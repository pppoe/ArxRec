<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240730.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene\n  Understanding", "author": "Hongjia Zhai and Gan Huang and Qirui Hu and Guanglin Li and Hujun Bao and Guofeng Zhang", "abstract": "  In recent years, the paradigm of neural implicit representations has gained\nsubstantial attention in the field of Simultaneous Localization and Mapping\n(SLAM). However, a notable gap exists in the existing approaches when it comes\nto scene understanding. In this paper, we introduce NIS-SLAM, an efficient\nneural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D\nsegmentation network to learn consistent semantic representations.\nSpecifically, for high-fidelity surface reconstruction and spatial consistent\nscene understanding, we combine high-frequency multi-resolution\ntetrahedron-based features and low-frequency positional encoding as the\nimplicit scene representations. Besides, to address the inconsistency of 2D\nsegmentation results from multiple views, we propose a fusion strategy that\nintegrates the semantic probabilities from previous non-keyframes into\nkeyframes to achieve consistent semantic learning. Furthermore, we implement a\nconfidence-based pixel sampling and progressive optimization weight function\nfor robust camera tracking. Extensive experimental results on various datasets\nshow the better or more competitive performance of our system when compared to\nother existing neural dense implicit RGB-D SLAM approaches. Finally, we also\nshow that our approach can be used in augmented reality applications. Project\npage:\n\\href{https://zju3dv.github.io/nis_slam}{https://zju3dv.github.io/nis\\_slam}.\n", "link": "http://arxiv.org/abs/2407.20853v1", "date": "2024-07-30", "relevancy": 3.2832, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7148}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6416}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NIS-SLAM%3A%20Neural%20Implicit%20Semantic%20RGB-D%20SLAM%20for%203D%20Consistent%20Scene%0A%20%20Understanding&body=Title%3A%20NIS-SLAM%3A%20Neural%20Implicit%20Semantic%20RGB-D%20SLAM%20for%203D%20Consistent%20Scene%0A%20%20Understanding%0AAuthor%3A%20Hongjia%20Zhai%20and%20Gan%20Huang%20and%20Qirui%20Hu%20and%20Guanglin%20Li%20and%20Hujun%20Bao%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20paradigm%20of%20neural%20implicit%20representations%20has%20gained%0Asubstantial%20attention%20in%20the%20field%20of%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29.%20However%2C%20a%20notable%20gap%20exists%20in%20the%20existing%20approaches%20when%20it%20comes%0Ato%20scene%20understanding.%20In%20this%20paper%2C%20we%20introduce%20NIS-SLAM%2C%20an%20efficient%0Aneural%20implicit%20semantic%20RGB-D%20SLAM%20system%2C%20that%20leverages%20a%20pre-trained%202D%0Asegmentation%20network%20to%20learn%20consistent%20semantic%20representations.%0ASpecifically%2C%20for%20high-fidelity%20surface%20reconstruction%20and%20spatial%20consistent%0Ascene%20understanding%2C%20we%20combine%20high-frequency%20multi-resolution%0Atetrahedron-based%20features%20and%20low-frequency%20positional%20encoding%20as%20the%0Aimplicit%20scene%20representations.%20Besides%2C%20to%20address%20the%20inconsistency%20of%202D%0Asegmentation%20results%20from%20multiple%20views%2C%20we%20propose%20a%20fusion%20strategy%20that%0Aintegrates%20the%20semantic%20probabilities%20from%20previous%20non-keyframes%20into%0Akeyframes%20to%20achieve%20consistent%20semantic%20learning.%20Furthermore%2C%20we%20implement%20a%0Aconfidence-based%20pixel%20sampling%20and%20progressive%20optimization%20weight%20function%0Afor%20robust%20camera%20tracking.%20Extensive%20experimental%20results%20on%20various%20datasets%0Ashow%20the%20better%20or%20more%20competitive%20performance%20of%20our%20system%20when%20compared%20to%0Aother%20existing%20neural%20dense%20implicit%20RGB-D%20SLAM%20approaches.%20Finally%2C%20we%20also%0Ashow%20that%20our%20approach%20can%20be%20used%20in%20augmented%20reality%20applications.%20Project%0Apage%3A%0A%5Chref%7Bhttps%3A//zju3dv.github.io/nis_slam%7D%7Bhttps%3A//zju3dv.github.io/nis%5C_slam%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNIS-SLAM%253A%2520Neural%2520Implicit%2520Semantic%2520RGB-D%2520SLAM%2520for%25203D%2520Consistent%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DHongjia%2520Zhai%2520and%2520Gan%2520Huang%2520and%2520Qirui%2520Hu%2520and%2520Guanglin%2520Li%2520and%2520Hujun%2520Bao%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520paradigm%2520of%2520neural%2520implicit%2520representations%2520has%2520gained%250Asubstantial%2520attention%2520in%2520the%2520field%2520of%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528SLAM%2529.%2520However%252C%2520a%2520notable%2520gap%2520exists%2520in%2520the%2520existing%2520approaches%2520when%2520it%2520comes%250Ato%2520scene%2520understanding.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520NIS-SLAM%252C%2520an%2520efficient%250Aneural%2520implicit%2520semantic%2520RGB-D%2520SLAM%2520system%252C%2520that%2520leverages%2520a%2520pre-trained%25202D%250Asegmentation%2520network%2520to%2520learn%2520consistent%2520semantic%2520representations.%250ASpecifically%252C%2520for%2520high-fidelity%2520surface%2520reconstruction%2520and%2520spatial%2520consistent%250Ascene%2520understanding%252C%2520we%2520combine%2520high-frequency%2520multi-resolution%250Atetrahedron-based%2520features%2520and%2520low-frequency%2520positional%2520encoding%2520as%2520the%250Aimplicit%2520scene%2520representations.%2520Besides%252C%2520to%2520address%2520the%2520inconsistency%2520of%25202D%250Asegmentation%2520results%2520from%2520multiple%2520views%252C%2520we%2520propose%2520a%2520fusion%2520strategy%2520that%250Aintegrates%2520the%2520semantic%2520probabilities%2520from%2520previous%2520non-keyframes%2520into%250Akeyframes%2520to%2520achieve%2520consistent%2520semantic%2520learning.%2520Furthermore%252C%2520we%2520implement%2520a%250Aconfidence-based%2520pixel%2520sampling%2520and%2520progressive%2520optimization%2520weight%2520function%250Afor%2520robust%2520camera%2520tracking.%2520Extensive%2520experimental%2520results%2520on%2520various%2520datasets%250Ashow%2520the%2520better%2520or%2520more%2520competitive%2520performance%2520of%2520our%2520system%2520when%2520compared%2520to%250Aother%2520existing%2520neural%2520dense%2520implicit%2520RGB-D%2520SLAM%2520approaches.%2520Finally%252C%2520we%2520also%250Ashow%2520that%2520our%2520approach%2520can%2520be%2520used%2520in%2520augmented%2520reality%2520applications.%2520Project%250Apage%253A%250A%255Chref%257Bhttps%253A//zju3dv.github.io/nis_slam%257D%257Bhttps%253A//zju3dv.github.io/nis%255C_slam%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NIS-SLAM%3A%20Neural%20Implicit%20Semantic%20RGB-D%20SLAM%20for%203D%20Consistent%20Scene%0A%20%20Understanding&entry.906535625=Hongjia%20Zhai%20and%20Gan%20Huang%20and%20Qirui%20Hu%20and%20Guanglin%20Li%20and%20Hujun%20Bao%20and%20Guofeng%20Zhang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20paradigm%20of%20neural%20implicit%20representations%20has%20gained%0Asubstantial%20attention%20in%20the%20field%20of%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29.%20However%2C%20a%20notable%20gap%20exists%20in%20the%20existing%20approaches%20when%20it%20comes%0Ato%20scene%20understanding.%20In%20this%20paper%2C%20we%20introduce%20NIS-SLAM%2C%20an%20efficient%0Aneural%20implicit%20semantic%20RGB-D%20SLAM%20system%2C%20that%20leverages%20a%20pre-trained%202D%0Asegmentation%20network%20to%20learn%20consistent%20semantic%20representations.%0ASpecifically%2C%20for%20high-fidelity%20surface%20reconstruction%20and%20spatial%20consistent%0Ascene%20understanding%2C%20we%20combine%20high-frequency%20multi-resolution%0Atetrahedron-based%20features%20and%20low-frequency%20positional%20encoding%20as%20the%0Aimplicit%20scene%20representations.%20Besides%2C%20to%20address%20the%20inconsistency%20of%202D%0Asegmentation%20results%20from%20multiple%20views%2C%20we%20propose%20a%20fusion%20strategy%20that%0Aintegrates%20the%20semantic%20probabilities%20from%20previous%20non-keyframes%20into%0Akeyframes%20to%20achieve%20consistent%20semantic%20learning.%20Furthermore%2C%20we%20implement%20a%0Aconfidence-based%20pixel%20sampling%20and%20progressive%20optimization%20weight%20function%0Afor%20robust%20camera%20tracking.%20Extensive%20experimental%20results%20on%20various%20datasets%0Ashow%20the%20better%20or%20more%20competitive%20performance%20of%20our%20system%20when%20compared%20to%0Aother%20existing%20neural%20dense%20implicit%20RGB-D%20SLAM%20approaches.%20Finally%2C%20we%20also%0Ashow%20that%20our%20approach%20can%20be%20used%20in%20augmented%20reality%20applications.%20Project%0Apage%3A%0A%5Chref%7Bhttps%3A//zju3dv.github.io/nis_slam%7D%7Bhttps%3A//zju3dv.github.io/nis%5C_slam%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20853v1&entry.124074799=Read"},
{"title": "latentSplat: Autoencoding Variational Gaussians for Fast Generalizable\n  3D Reconstruction", "author": "Christopher Wewer and Kevin Raj and Eddy Ilg and Bernt Schiele and Jan Eric Lenssen", "abstract": "  We present latentSplat, a method to predict semantic Gaussians in a 3D latent\nspace that can be splatted and decoded by a light-weight generative 2D\narchitecture. Existing methods for generalizable 3D reconstruction either do\nnot scale to large scenes and resolutions, or are limited to interpolation of\nclose input views. latentSplat combines the strengths of regression-based and\ngenerative approaches while being trained purely on readily available real\nvideo data. The core of our method are variational 3D Gaussians, a\nrepresentation that efficiently encodes varying uncertainty within a latent\nspace consisting of 3D feature Gaussians. From these Gaussians, specific\ninstances can be sampled and rendered via efficient splatting and a fast,\ngenerative decoder. We show that latentSplat outperforms previous works in\nreconstruction quality and generalization, while being fast and scalable to\nhigh-resolution data.\n", "link": "http://arxiv.org/abs/2403.16292v2", "date": "2024-07-30", "relevancy": 3.1806, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7262}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6424}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20latentSplat%3A%20Autoencoding%20Variational%20Gaussians%20for%20Fast%20Generalizable%0A%20%203D%20Reconstruction&body=Title%3A%20latentSplat%3A%20Autoencoding%20Variational%20Gaussians%20for%20Fast%20Generalizable%0A%20%203D%20Reconstruction%0AAuthor%3A%20Christopher%20Wewer%20and%20Kevin%20Raj%20and%20Eddy%20Ilg%20and%20Bernt%20Schiele%20and%20Jan%20Eric%20Lenssen%0AAbstract%3A%20%20%20We%20present%20latentSplat%2C%20a%20method%20to%20predict%20semantic%20Gaussians%20in%20a%203D%20latent%0Aspace%20that%20can%20be%20splatted%20and%20decoded%20by%20a%20light-weight%20generative%202D%0Aarchitecture.%20Existing%20methods%20for%20generalizable%203D%20reconstruction%20either%20do%0Anot%20scale%20to%20large%20scenes%20and%20resolutions%2C%20or%20are%20limited%20to%20interpolation%20of%0Aclose%20input%20views.%20latentSplat%20combines%20the%20strengths%20of%20regression-based%20and%0Agenerative%20approaches%20while%20being%20trained%20purely%20on%20readily%20available%20real%0Avideo%20data.%20The%20core%20of%20our%20method%20are%20variational%203D%20Gaussians%2C%20a%0Arepresentation%20that%20efficiently%20encodes%20varying%20uncertainty%20within%20a%20latent%0Aspace%20consisting%20of%203D%20feature%20Gaussians.%20From%20these%20Gaussians%2C%20specific%0Ainstances%20can%20be%20sampled%20and%20rendered%20via%20efficient%20splatting%20and%20a%20fast%2C%0Agenerative%20decoder.%20We%20show%20that%20latentSplat%20outperforms%20previous%20works%20in%0Areconstruction%20quality%20and%20generalization%2C%20while%20being%20fast%20and%20scalable%20to%0Ahigh-resolution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DlatentSplat%253A%2520Autoencoding%2520Variational%2520Gaussians%2520for%2520Fast%2520Generalizable%250A%2520%25203D%2520Reconstruction%26entry.906535625%3DChristopher%2520Wewer%2520and%2520Kevin%2520Raj%2520and%2520Eddy%2520Ilg%2520and%2520Bernt%2520Schiele%2520and%2520Jan%2520Eric%2520Lenssen%26entry.1292438233%3D%2520%2520We%2520present%2520latentSplat%252C%2520a%2520method%2520to%2520predict%2520semantic%2520Gaussians%2520in%2520a%25203D%2520latent%250Aspace%2520that%2520can%2520be%2520splatted%2520and%2520decoded%2520by%2520a%2520light-weight%2520generative%25202D%250Aarchitecture.%2520Existing%2520methods%2520for%2520generalizable%25203D%2520reconstruction%2520either%2520do%250Anot%2520scale%2520to%2520large%2520scenes%2520and%2520resolutions%252C%2520or%2520are%2520limited%2520to%2520interpolation%2520of%250Aclose%2520input%2520views.%2520latentSplat%2520combines%2520the%2520strengths%2520of%2520regression-based%2520and%250Agenerative%2520approaches%2520while%2520being%2520trained%2520purely%2520on%2520readily%2520available%2520real%250Avideo%2520data.%2520The%2520core%2520of%2520our%2520method%2520are%2520variational%25203D%2520Gaussians%252C%2520a%250Arepresentation%2520that%2520efficiently%2520encodes%2520varying%2520uncertainty%2520within%2520a%2520latent%250Aspace%2520consisting%2520of%25203D%2520feature%2520Gaussians.%2520From%2520these%2520Gaussians%252C%2520specific%250Ainstances%2520can%2520be%2520sampled%2520and%2520rendered%2520via%2520efficient%2520splatting%2520and%2520a%2520fast%252C%250Agenerative%2520decoder.%2520We%2520show%2520that%2520latentSplat%2520outperforms%2520previous%2520works%2520in%250Areconstruction%2520quality%2520and%2520generalization%252C%2520while%2520being%2520fast%2520and%2520scalable%2520to%250Ahigh-resolution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=latentSplat%3A%20Autoencoding%20Variational%20Gaussians%20for%20Fast%20Generalizable%0A%20%203D%20Reconstruction&entry.906535625=Christopher%20Wewer%20and%20Kevin%20Raj%20and%20Eddy%20Ilg%20and%20Bernt%20Schiele%20and%20Jan%20Eric%20Lenssen&entry.1292438233=%20%20We%20present%20latentSplat%2C%20a%20method%20to%20predict%20semantic%20Gaussians%20in%20a%203D%20latent%0Aspace%20that%20can%20be%20splatted%20and%20decoded%20by%20a%20light-weight%20generative%202D%0Aarchitecture.%20Existing%20methods%20for%20generalizable%203D%20reconstruction%20either%20do%0Anot%20scale%20to%20large%20scenes%20and%20resolutions%2C%20or%20are%20limited%20to%20interpolation%20of%0Aclose%20input%20views.%20latentSplat%20combines%20the%20strengths%20of%20regression-based%20and%0Agenerative%20approaches%20while%20being%20trained%20purely%20on%20readily%20available%20real%0Avideo%20data.%20The%20core%20of%20our%20method%20are%20variational%203D%20Gaussians%2C%20a%0Arepresentation%20that%20efficiently%20encodes%20varying%20uncertainty%20within%20a%20latent%0Aspace%20consisting%20of%203D%20feature%20Gaussians.%20From%20these%20Gaussians%2C%20specific%0Ainstances%20can%20be%20sampled%20and%20rendered%20via%20efficient%20splatting%20and%20a%20fast%2C%0Agenerative%20decoder.%20We%20show%20that%20latentSplat%20outperforms%20previous%20works%20in%0Areconstruction%20quality%20and%20generalization%2C%20while%20being%20fast%20and%20scalable%20to%0Ahigh-resolution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16292v2&entry.124074799=Read"},
{"title": "SceneTeller: Language-to-3D Scene Generation", "author": "Ba\u015fak Melis \u00d6cal and Maxim Tatarchenko and Sezer Karaoglu and Theo Gevers", "abstract": "  Designing high-quality indoor 3D scenes is important in many practical\napplications, such as room planning or game development. Conventionally, this\nhas been a time-consuming process which requires both artistic skill and\nfamiliarity with professional software, making it hardly accessible for layman\nusers. However, recent advances in generative AI have established solid\nfoundation for democratizing 3D design. In this paper, we propose a pioneering\napproach for text-based 3D room design. Given a prompt in natural language\ndescribing the object placement in the room, our method produces a high-quality\n3D scene corresponding to it. With an additional text prompt the users can\nchange the appearance of the entire scene or of individual objects in it. Built\nusing in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-based\nstylization, our turnkey pipeline produces state-of-the-art 3D scenes, while\nbeing easy to use even for novices. Our project page is available at\nhttps://sceneteller.github.io/.\n", "link": "http://arxiv.org/abs/2407.20727v1", "date": "2024-07-30", "relevancy": 3.1744, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6453}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6453}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneTeller%3A%20Language-to-3D%20Scene%20Generation&body=Title%3A%20SceneTeller%3A%20Language-to-3D%20Scene%20Generation%0AAuthor%3A%20Ba%C5%9Fak%20Melis%20%C3%96cal%20and%20Maxim%20Tatarchenko%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers%0AAbstract%3A%20%20%20Designing%20high-quality%20indoor%203D%20scenes%20is%20important%20in%20many%20practical%0Aapplications%2C%20such%20as%20room%20planning%20or%20game%20development.%20Conventionally%2C%20this%0Ahas%20been%20a%20time-consuming%20process%20which%20requires%20both%20artistic%20skill%20and%0Afamiliarity%20with%20professional%20software%2C%20making%20it%20hardly%20accessible%20for%20layman%0Ausers.%20However%2C%20recent%20advances%20in%20generative%20AI%20have%20established%20solid%0Afoundation%20for%20democratizing%203D%20design.%20In%20this%20paper%2C%20we%20propose%20a%20pioneering%0Aapproach%20for%20text-based%203D%20room%20design.%20Given%20a%20prompt%20in%20natural%20language%0Adescribing%20the%20object%20placement%20in%20the%20room%2C%20our%20method%20produces%20a%20high-quality%0A3D%20scene%20corresponding%20to%20it.%20With%20an%20additional%20text%20prompt%20the%20users%20can%0Achange%20the%20appearance%20of%20the%20entire%20scene%20or%20of%20individual%20objects%20in%20it.%20Built%0Ausing%20in-context%20learning%2C%20CAD%20model%20retrieval%20and%203D-Gaussian-Splatting-based%0Astylization%2C%20our%20turnkey%20pipeline%20produces%20state-of-the-art%203D%20scenes%2C%20while%0Abeing%20easy%20to%20use%20even%20for%20novices.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//sceneteller.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneTeller%253A%2520Language-to-3D%2520Scene%2520Generation%26entry.906535625%3DBa%25C5%259Fak%2520Melis%2520%25C3%2596cal%2520and%2520Maxim%2520Tatarchenko%2520and%2520Sezer%2520Karaoglu%2520and%2520Theo%2520Gevers%26entry.1292438233%3D%2520%2520Designing%2520high-quality%2520indoor%25203D%2520scenes%2520is%2520important%2520in%2520many%2520practical%250Aapplications%252C%2520such%2520as%2520room%2520planning%2520or%2520game%2520development.%2520Conventionally%252C%2520this%250Ahas%2520been%2520a%2520time-consuming%2520process%2520which%2520requires%2520both%2520artistic%2520skill%2520and%250Afamiliarity%2520with%2520professional%2520software%252C%2520making%2520it%2520hardly%2520accessible%2520for%2520layman%250Ausers.%2520However%252C%2520recent%2520advances%2520in%2520generative%2520AI%2520have%2520established%2520solid%250Afoundation%2520for%2520democratizing%25203D%2520design.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520pioneering%250Aapproach%2520for%2520text-based%25203D%2520room%2520design.%2520Given%2520a%2520prompt%2520in%2520natural%2520language%250Adescribing%2520the%2520object%2520placement%2520in%2520the%2520room%252C%2520our%2520method%2520produces%2520a%2520high-quality%250A3D%2520scene%2520corresponding%2520to%2520it.%2520With%2520an%2520additional%2520text%2520prompt%2520the%2520users%2520can%250Achange%2520the%2520appearance%2520of%2520the%2520entire%2520scene%2520or%2520of%2520individual%2520objects%2520in%2520it.%2520Built%250Ausing%2520in-context%2520learning%252C%2520CAD%2520model%2520retrieval%2520and%25203D-Gaussian-Splatting-based%250Astylization%252C%2520our%2520turnkey%2520pipeline%2520produces%2520state-of-the-art%25203D%2520scenes%252C%2520while%250Abeing%2520easy%2520to%2520use%2520even%2520for%2520novices.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//sceneteller.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneTeller%3A%20Language-to-3D%20Scene%20Generation&entry.906535625=Ba%C5%9Fak%20Melis%20%C3%96cal%20and%20Maxim%20Tatarchenko%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers&entry.1292438233=%20%20Designing%20high-quality%20indoor%203D%20scenes%20is%20important%20in%20many%20practical%0Aapplications%2C%20such%20as%20room%20planning%20or%20game%20development.%20Conventionally%2C%20this%0Ahas%20been%20a%20time-consuming%20process%20which%20requires%20both%20artistic%20skill%20and%0Afamiliarity%20with%20professional%20software%2C%20making%20it%20hardly%20accessible%20for%20layman%0Ausers.%20However%2C%20recent%20advances%20in%20generative%20AI%20have%20established%20solid%0Afoundation%20for%20democratizing%203D%20design.%20In%20this%20paper%2C%20we%20propose%20a%20pioneering%0Aapproach%20for%20text-based%203D%20room%20design.%20Given%20a%20prompt%20in%20natural%20language%0Adescribing%20the%20object%20placement%20in%20the%20room%2C%20our%20method%20produces%20a%20high-quality%0A3D%20scene%20corresponding%20to%20it.%20With%20an%20additional%20text%20prompt%20the%20users%20can%0Achange%20the%20appearance%20of%20the%20entire%20scene%20or%20of%20individual%20objects%20in%20it.%20Built%0Ausing%20in-context%20learning%2C%20CAD%20model%20retrieval%20and%203D-Gaussian-Splatting-based%0Astylization%2C%20our%20turnkey%20pipeline%20produces%20state-of-the-art%203D%20scenes%2C%20while%0Abeing%20easy%20to%20use%20even%20for%20novices.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//sceneteller.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20727v1&entry.124074799=Read"},
{"title": "Dynamic Scene Understanding through Object-Centric Voxelization and\n  Neural Rendering", "author": "Yanpeng Zhao and Yiwei Hao and Siyu Gao and Yunbo Wang and Xiaokang Yang", "abstract": "  Learning object-centric representations from unsupervised videos is\nchallenging. Unlike most previous approaches that focus on decomposing 2D\nimages, we present a 3D generative model named DynaVol-S for dynamic scenes\nthat enables object-centric learning within a differentiable volume rendering\nframework. The key idea is to perform object-centric voxelization to capture\nthe 3D nature of the scene, which infers per-object occupancy probabilities at\nindividual spatial locations. These voxel features evolve through a\ncanonical-space deformation function and are optimized in an inverse rendering\npipeline with a compositional NeRF. Additionally, our approach integrates 2D\nsemantic features to create 3D semantic grids, representing the scene through\nmultiple disentangled voxel grids. DynaVol-S significantly outperforms existing\nmodels in both novel view synthesis and unsupervised decomposition tasks for\ndynamic scenes. By jointly considering geometric structures and semantic\nfeatures, it effectively addresses challenging real-world scenarios involving\ncomplex object interactions. Furthermore, once trained, the explicitly\nmeaningful voxel features enable additional capabilities that 2D scene\ndecomposition methods cannot achieve, such as novel scene generation through\nediting geometric shapes or manipulating the motion trajectories of objects.\n", "link": "http://arxiv.org/abs/2407.20908v1", "date": "2024-07-30", "relevancy": 3.1137, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6326}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6178}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Scene%20Understanding%20through%20Object-Centric%20Voxelization%20and%0A%20%20Neural%20Rendering&body=Title%3A%20Dynamic%20Scene%20Understanding%20through%20Object-Centric%20Voxelization%20and%0A%20%20Neural%20Rendering%0AAuthor%3A%20Yanpeng%20Zhao%20and%20Yiwei%20Hao%20and%20Siyu%20Gao%20and%20Yunbo%20Wang%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Learning%20object-centric%20representations%20from%20unsupervised%20videos%20is%0Achallenging.%20Unlike%20most%20previous%20approaches%20that%20focus%20on%20decomposing%202D%0Aimages%2C%20we%20present%20a%203D%20generative%20model%20named%20DynaVol-S%20for%20dynamic%20scenes%0Athat%20enables%20object-centric%20learning%20within%20a%20differentiable%20volume%20rendering%0Aframework.%20The%20key%20idea%20is%20to%20perform%20object-centric%20voxelization%20to%20capture%0Athe%203D%20nature%20of%20the%20scene%2C%20which%20infers%20per-object%20occupancy%20probabilities%20at%0Aindividual%20spatial%20locations.%20These%20voxel%20features%20evolve%20through%20a%0Acanonical-space%20deformation%20function%20and%20are%20optimized%20in%20an%20inverse%20rendering%0Apipeline%20with%20a%20compositional%20NeRF.%20Additionally%2C%20our%20approach%20integrates%202D%0Asemantic%20features%20to%20create%203D%20semantic%20grids%2C%20representing%20the%20scene%20through%0Amultiple%20disentangled%20voxel%20grids.%20DynaVol-S%20significantly%20outperforms%20existing%0Amodels%20in%20both%20novel%20view%20synthesis%20and%20unsupervised%20decomposition%20tasks%20for%0Adynamic%20scenes.%20By%20jointly%20considering%20geometric%20structures%20and%20semantic%0Afeatures%2C%20it%20effectively%20addresses%20challenging%20real-world%20scenarios%20involving%0Acomplex%20object%20interactions.%20Furthermore%2C%20once%20trained%2C%20the%20explicitly%0Ameaningful%20voxel%20features%20enable%20additional%20capabilities%20that%202D%20scene%0Adecomposition%20methods%20cannot%20achieve%2C%20such%20as%20novel%20scene%20generation%20through%0Aediting%20geometric%20shapes%20or%20manipulating%20the%20motion%20trajectories%20of%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Scene%2520Understanding%2520through%2520Object-Centric%2520Voxelization%2520and%250A%2520%2520Neural%2520Rendering%26entry.906535625%3DYanpeng%2520Zhao%2520and%2520Yiwei%2520Hao%2520and%2520Siyu%2520Gao%2520and%2520Yunbo%2520Wang%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Learning%2520object-centric%2520representations%2520from%2520unsupervised%2520videos%2520is%250Achallenging.%2520Unlike%2520most%2520previous%2520approaches%2520that%2520focus%2520on%2520decomposing%25202D%250Aimages%252C%2520we%2520present%2520a%25203D%2520generative%2520model%2520named%2520DynaVol-S%2520for%2520dynamic%2520scenes%250Athat%2520enables%2520object-centric%2520learning%2520within%2520a%2520differentiable%2520volume%2520rendering%250Aframework.%2520The%2520key%2520idea%2520is%2520to%2520perform%2520object-centric%2520voxelization%2520to%2520capture%250Athe%25203D%2520nature%2520of%2520the%2520scene%252C%2520which%2520infers%2520per-object%2520occupancy%2520probabilities%2520at%250Aindividual%2520spatial%2520locations.%2520These%2520voxel%2520features%2520evolve%2520through%2520a%250Acanonical-space%2520deformation%2520function%2520and%2520are%2520optimized%2520in%2520an%2520inverse%2520rendering%250Apipeline%2520with%2520a%2520compositional%2520NeRF.%2520Additionally%252C%2520our%2520approach%2520integrates%25202D%250Asemantic%2520features%2520to%2520create%25203D%2520semantic%2520grids%252C%2520representing%2520the%2520scene%2520through%250Amultiple%2520disentangled%2520voxel%2520grids.%2520DynaVol-S%2520significantly%2520outperforms%2520existing%250Amodels%2520in%2520both%2520novel%2520view%2520synthesis%2520and%2520unsupervised%2520decomposition%2520tasks%2520for%250Adynamic%2520scenes.%2520By%2520jointly%2520considering%2520geometric%2520structures%2520and%2520semantic%250Afeatures%252C%2520it%2520effectively%2520addresses%2520challenging%2520real-world%2520scenarios%2520involving%250Acomplex%2520object%2520interactions.%2520Furthermore%252C%2520once%2520trained%252C%2520the%2520explicitly%250Ameaningful%2520voxel%2520features%2520enable%2520additional%2520capabilities%2520that%25202D%2520scene%250Adecomposition%2520methods%2520cannot%2520achieve%252C%2520such%2520as%2520novel%2520scene%2520generation%2520through%250Aediting%2520geometric%2520shapes%2520or%2520manipulating%2520the%2520motion%2520trajectories%2520of%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Scene%20Understanding%20through%20Object-Centric%20Voxelization%20and%0A%20%20Neural%20Rendering&entry.906535625=Yanpeng%20Zhao%20and%20Yiwei%20Hao%20and%20Siyu%20Gao%20and%20Yunbo%20Wang%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Learning%20object-centric%20representations%20from%20unsupervised%20videos%20is%0Achallenging.%20Unlike%20most%20previous%20approaches%20that%20focus%20on%20decomposing%202D%0Aimages%2C%20we%20present%20a%203D%20generative%20model%20named%20DynaVol-S%20for%20dynamic%20scenes%0Athat%20enables%20object-centric%20learning%20within%20a%20differentiable%20volume%20rendering%0Aframework.%20The%20key%20idea%20is%20to%20perform%20object-centric%20voxelization%20to%20capture%0Athe%203D%20nature%20of%20the%20scene%2C%20which%20infers%20per-object%20occupancy%20probabilities%20at%0Aindividual%20spatial%20locations.%20These%20voxel%20features%20evolve%20through%20a%0Acanonical-space%20deformation%20function%20and%20are%20optimized%20in%20an%20inverse%20rendering%0Apipeline%20with%20a%20compositional%20NeRF.%20Additionally%2C%20our%20approach%20integrates%202D%0Asemantic%20features%20to%20create%203D%20semantic%20grids%2C%20representing%20the%20scene%20through%0Amultiple%20disentangled%20voxel%20grids.%20DynaVol-S%20significantly%20outperforms%20existing%0Amodels%20in%20both%20novel%20view%20synthesis%20and%20unsupervised%20decomposition%20tasks%20for%0Adynamic%20scenes.%20By%20jointly%20considering%20geometric%20structures%20and%20semantic%0Afeatures%2C%20it%20effectively%20addresses%20challenging%20real-world%20scenarios%20involving%0Acomplex%20object%20interactions.%20Furthermore%2C%20once%20trained%2C%20the%20explicitly%0Ameaningful%20voxel%20features%20enable%20additional%20capabilities%20that%202D%20scene%0Adecomposition%20methods%20cannot%20achieve%2C%20such%20as%20novel%20scene%20generation%20through%0Aediting%20geometric%20shapes%20or%20manipulating%20the%20motion%20trajectories%20of%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20908v1&entry.124074799=Read"},
{"title": "Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D\n  Pose Estimation", "author": "Francesco Di Felice and Alberto Remus and Stefano Gasperini and Benjamin Busam and Lionel Ott and Federico Tombari and Roland Siegwart and Carlo Alberto Avizzano", "abstract": "  Estimating the pose of objects through vision is essential to make robotic\nplatforms interact with the environment. Yet, it presents many challenges,\noften related to the lack of flexibility and generalizability of\nstate-of-the-art solutions. Diffusion models are a cutting-edge neural\narchitecture transforming 2D and 3D computer vision, outlining remarkable\nperformances in zero-shot novel-view synthesis. Such a use case is particularly\nintriguing for reconstructing 3D objects. However, localizing objects in\nunstructured environments is rather unexplored. To this end, this work presents\nZero123-6D, the first work to demonstrate the utility of Diffusion Model-based\nnovel-view-synthesizers in enhancing RGB 6D pose estimation at category-level,\nby integrating them with feature extraction techniques. Novel View Synthesis\nallows to obtain a coarse pose that is refined through an online optimization\nmethod introduced in this work to deal with intra-category geometric\ndifferences. In such a way, the outlined method shows reduction in data\nrequirements, removal of the necessity of depth information in zero-shot\ncategory-level 6D pose estimation task, and increased performance,\nquantitatively demonstrated through experiments on the CO3D dataset.\n", "link": "http://arxiv.org/abs/2403.14279v2", "date": "2024-07-30", "relevancy": 3.0645, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6165}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6111}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero123-6D%3A%20Zero-shot%20Novel%20View%20Synthesis%20for%20RGB%20Category-level%206D%0A%20%20Pose%20Estimation&body=Title%3A%20Zero123-6D%3A%20Zero-shot%20Novel%20View%20Synthesis%20for%20RGB%20Category-level%206D%0A%20%20Pose%20Estimation%0AAuthor%3A%20Francesco%20Di%20Felice%20and%20Alberto%20Remus%20and%20Stefano%20Gasperini%20and%20Benjamin%20Busam%20and%20Lionel%20Ott%20and%20Federico%20Tombari%20and%20Roland%20Siegwart%20and%20Carlo%20Alberto%20Avizzano%0AAbstract%3A%20%20%20Estimating%20the%20pose%20of%20objects%20through%20vision%20is%20essential%20to%20make%20robotic%0Aplatforms%20interact%20with%20the%20environment.%20Yet%2C%20it%20presents%20many%20challenges%2C%0Aoften%20related%20to%20the%20lack%20of%20flexibility%20and%20generalizability%20of%0Astate-of-the-art%20solutions.%20Diffusion%20models%20are%20a%20cutting-edge%20neural%0Aarchitecture%20transforming%202D%20and%203D%20computer%20vision%2C%20outlining%20remarkable%0Aperformances%20in%20zero-shot%20novel-view%20synthesis.%20Such%20a%20use%20case%20is%20particularly%0Aintriguing%20for%20reconstructing%203D%20objects.%20However%2C%20localizing%20objects%20in%0Aunstructured%20environments%20is%20rather%20unexplored.%20To%20this%20end%2C%20this%20work%20presents%0AZero123-6D%2C%20the%20first%20work%20to%20demonstrate%20the%20utility%20of%20Diffusion%20Model-based%0Anovel-view-synthesizers%20in%20enhancing%20RGB%206D%20pose%20estimation%20at%20category-level%2C%0Aby%20integrating%20them%20with%20feature%20extraction%20techniques.%20Novel%20View%20Synthesis%0Aallows%20to%20obtain%20a%20coarse%20pose%20that%20is%20refined%20through%20an%20online%20optimization%0Amethod%20introduced%20in%20this%20work%20to%20deal%20with%20intra-category%20geometric%0Adifferences.%20In%20such%20a%20way%2C%20the%20outlined%20method%20shows%20reduction%20in%20data%0Arequirements%2C%20removal%20of%20the%20necessity%20of%20depth%20information%20in%20zero-shot%0Acategory-level%206D%20pose%20estimation%20task%2C%20and%20increased%20performance%2C%0Aquantitatively%20demonstrated%20through%20experiments%20on%20the%20CO3D%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero123-6D%253A%2520Zero-shot%2520Novel%2520View%2520Synthesis%2520for%2520RGB%2520Category-level%25206D%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DFrancesco%2520Di%2520Felice%2520and%2520Alberto%2520Remus%2520and%2520Stefano%2520Gasperini%2520and%2520Benjamin%2520Busam%2520and%2520Lionel%2520Ott%2520and%2520Federico%2520Tombari%2520and%2520Roland%2520Siegwart%2520and%2520Carlo%2520Alberto%2520Avizzano%26entry.1292438233%3D%2520%2520Estimating%2520the%2520pose%2520of%2520objects%2520through%2520vision%2520is%2520essential%2520to%2520make%2520robotic%250Aplatforms%2520interact%2520with%2520the%2520environment.%2520Yet%252C%2520it%2520presents%2520many%2520challenges%252C%250Aoften%2520related%2520to%2520the%2520lack%2520of%2520flexibility%2520and%2520generalizability%2520of%250Astate-of-the-art%2520solutions.%2520Diffusion%2520models%2520are%2520a%2520cutting-edge%2520neural%250Aarchitecture%2520transforming%25202D%2520and%25203D%2520computer%2520vision%252C%2520outlining%2520remarkable%250Aperformances%2520in%2520zero-shot%2520novel-view%2520synthesis.%2520Such%2520a%2520use%2520case%2520is%2520particularly%250Aintriguing%2520for%2520reconstructing%25203D%2520objects.%2520However%252C%2520localizing%2520objects%2520in%250Aunstructured%2520environments%2520is%2520rather%2520unexplored.%2520To%2520this%2520end%252C%2520this%2520work%2520presents%250AZero123-6D%252C%2520the%2520first%2520work%2520to%2520demonstrate%2520the%2520utility%2520of%2520Diffusion%2520Model-based%250Anovel-view-synthesizers%2520in%2520enhancing%2520RGB%25206D%2520pose%2520estimation%2520at%2520category-level%252C%250Aby%2520integrating%2520them%2520with%2520feature%2520extraction%2520techniques.%2520Novel%2520View%2520Synthesis%250Aallows%2520to%2520obtain%2520a%2520coarse%2520pose%2520that%2520is%2520refined%2520through%2520an%2520online%2520optimization%250Amethod%2520introduced%2520in%2520this%2520work%2520to%2520deal%2520with%2520intra-category%2520geometric%250Adifferences.%2520In%2520such%2520a%2520way%252C%2520the%2520outlined%2520method%2520shows%2520reduction%2520in%2520data%250Arequirements%252C%2520removal%2520of%2520the%2520necessity%2520of%2520depth%2520information%2520in%2520zero-shot%250Acategory-level%25206D%2520pose%2520estimation%2520task%252C%2520and%2520increased%2520performance%252C%250Aquantitatively%2520demonstrated%2520through%2520experiments%2520on%2520the%2520CO3D%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero123-6D%3A%20Zero-shot%20Novel%20View%20Synthesis%20for%20RGB%20Category-level%206D%0A%20%20Pose%20Estimation&entry.906535625=Francesco%20Di%20Felice%20and%20Alberto%20Remus%20and%20Stefano%20Gasperini%20and%20Benjamin%20Busam%20and%20Lionel%20Ott%20and%20Federico%20Tombari%20and%20Roland%20Siegwart%20and%20Carlo%20Alberto%20Avizzano&entry.1292438233=%20%20Estimating%20the%20pose%20of%20objects%20through%20vision%20is%20essential%20to%20make%20robotic%0Aplatforms%20interact%20with%20the%20environment.%20Yet%2C%20it%20presents%20many%20challenges%2C%0Aoften%20related%20to%20the%20lack%20of%20flexibility%20and%20generalizability%20of%0Astate-of-the-art%20solutions.%20Diffusion%20models%20are%20a%20cutting-edge%20neural%0Aarchitecture%20transforming%202D%20and%203D%20computer%20vision%2C%20outlining%20remarkable%0Aperformances%20in%20zero-shot%20novel-view%20synthesis.%20Such%20a%20use%20case%20is%20particularly%0Aintriguing%20for%20reconstructing%203D%20objects.%20However%2C%20localizing%20objects%20in%0Aunstructured%20environments%20is%20rather%20unexplored.%20To%20this%20end%2C%20this%20work%20presents%0AZero123-6D%2C%20the%20first%20work%20to%20demonstrate%20the%20utility%20of%20Diffusion%20Model-based%0Anovel-view-synthesizers%20in%20enhancing%20RGB%206D%20pose%20estimation%20at%20category-level%2C%0Aby%20integrating%20them%20with%20feature%20extraction%20techniques.%20Novel%20View%20Synthesis%0Aallows%20to%20obtain%20a%20coarse%20pose%20that%20is%20refined%20through%20an%20online%20optimization%0Amethod%20introduced%20in%20this%20work%20to%20deal%20with%20intra-category%20geometric%0Adifferences.%20In%20such%20a%20way%2C%20the%20outlined%20method%20shows%20reduction%20in%20data%0Arequirements%2C%20removal%20of%20the%20necessity%20of%20depth%20information%20in%20zero-shot%0Acategory-level%206D%20pose%20estimation%20task%2C%20and%20increased%20performance%2C%0Aquantitatively%20demonstrated%20through%20experiments%20on%20the%20CO3D%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14279v2&entry.124074799=Read"},
{"title": "XHand: Real-time Expressive Hand Avatar", "author": "Qijun Gan and Zijie Zhou and Jianke Zhu", "abstract": "  Hand avatars play a pivotal role in a wide array of digital interfaces,\nenhancing user immersion and facilitating natural interaction within virtual\nenvironments. While previous studies have focused on photo-realistic hand\nrendering, little attention has been paid to reconstruct the hand geometry with\nfine details, which is essential to rendering quality. In the realms of\nextended reality and gaming, on-the-fly rendering becomes imperative. To this\nend, we introduce an expressive hand avatar, named XHand, that is designed to\ncomprehensively generate hand shape, appearance, and deformations in real-time.\nTo obtain fine-grained hand meshes, we make use of three feature embedding\nmodules to predict hand deformation displacements, albedo, and linear blending\nskinning weights, respectively. To achieve photo-realistic hand rendering on\nfine-grained meshes, our method employs a mesh-based neural renderer by\nleveraging mesh topological consistency and latent codes from embedding\nmodules. During training, a part-aware Laplace smoothing strategy is proposed\nby incorporating the distinct levels of regularization to effectively maintain\nthe necessary details and eliminate the undesired artifacts. The experimental\nevaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy\nof XHand, which is able to recover high-fidelity geometry and texture for hand\nanimations across diverse poses in real-time. To reproduce our results, we will\nmake the full implementation publicly available at\nhttps://github.com/agnJason/XHand.\n", "link": "http://arxiv.org/abs/2407.21002v1", "date": "2024-07-30", "relevancy": 2.9604, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5979}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5979}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XHand%3A%20Real-time%20Expressive%20Hand%20Avatar&body=Title%3A%20XHand%3A%20Real-time%20Expressive%20Hand%20Avatar%0AAuthor%3A%20Qijun%20Gan%20and%20Zijie%20Zhou%20and%20Jianke%20Zhu%0AAbstract%3A%20%20%20Hand%20avatars%20play%20a%20pivotal%20role%20in%20a%20wide%20array%20of%20digital%20interfaces%2C%0Aenhancing%20user%20immersion%20and%20facilitating%20natural%20interaction%20within%20virtual%0Aenvironments.%20While%20previous%20studies%20have%20focused%20on%20photo-realistic%20hand%0Arendering%2C%20little%20attention%20has%20been%20paid%20to%20reconstruct%20the%20hand%20geometry%20with%0Afine%20details%2C%20which%20is%20essential%20to%20rendering%20quality.%20In%20the%20realms%20of%0Aextended%20reality%20and%20gaming%2C%20on-the-fly%20rendering%20becomes%20imperative.%20To%20this%0Aend%2C%20we%20introduce%20an%20expressive%20hand%20avatar%2C%20named%20XHand%2C%20that%20is%20designed%20to%0Acomprehensively%20generate%20hand%20shape%2C%20appearance%2C%20and%20deformations%20in%20real-time.%0ATo%20obtain%20fine-grained%20hand%20meshes%2C%20we%20make%20use%20of%20three%20feature%20embedding%0Amodules%20to%20predict%20hand%20deformation%20displacements%2C%20albedo%2C%20and%20linear%20blending%0Askinning%20weights%2C%20respectively.%20To%20achieve%20photo-realistic%20hand%20rendering%20on%0Afine-grained%20meshes%2C%20our%20method%20employs%20a%20mesh-based%20neural%20renderer%20by%0Aleveraging%20mesh%20topological%20consistency%20and%20latent%20codes%20from%20embedding%0Amodules.%20During%20training%2C%20a%20part-aware%20Laplace%20smoothing%20strategy%20is%20proposed%0Aby%20incorporating%20the%20distinct%20levels%20of%20regularization%20to%20effectively%20maintain%0Athe%20necessary%20details%20and%20eliminate%20the%20undesired%20artifacts.%20The%20experimental%0Aevaluations%20on%20InterHand2.6M%20and%20DeepHandMesh%20datasets%20demonstrate%20the%20efficacy%0Aof%20XHand%2C%20which%20is%20able%20to%20recover%20high-fidelity%20geometry%20and%20texture%20for%20hand%0Aanimations%20across%20diverse%20poses%20in%20real-time.%20To%20reproduce%20our%20results%2C%20we%20will%0Amake%20the%20full%20implementation%20publicly%20available%20at%0Ahttps%3A//github.com/agnJason/XHand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXHand%253A%2520Real-time%2520Expressive%2520Hand%2520Avatar%26entry.906535625%3DQijun%2520Gan%2520and%2520Zijie%2520Zhou%2520and%2520Jianke%2520Zhu%26entry.1292438233%3D%2520%2520Hand%2520avatars%2520play%2520a%2520pivotal%2520role%2520in%2520a%2520wide%2520array%2520of%2520digital%2520interfaces%252C%250Aenhancing%2520user%2520immersion%2520and%2520facilitating%2520natural%2520interaction%2520within%2520virtual%250Aenvironments.%2520While%2520previous%2520studies%2520have%2520focused%2520on%2520photo-realistic%2520hand%250Arendering%252C%2520little%2520attention%2520has%2520been%2520paid%2520to%2520reconstruct%2520the%2520hand%2520geometry%2520with%250Afine%2520details%252C%2520which%2520is%2520essential%2520to%2520rendering%2520quality.%2520In%2520the%2520realms%2520of%250Aextended%2520reality%2520and%2520gaming%252C%2520on-the-fly%2520rendering%2520becomes%2520imperative.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520an%2520expressive%2520hand%2520avatar%252C%2520named%2520XHand%252C%2520that%2520is%2520designed%2520to%250Acomprehensively%2520generate%2520hand%2520shape%252C%2520appearance%252C%2520and%2520deformations%2520in%2520real-time.%250ATo%2520obtain%2520fine-grained%2520hand%2520meshes%252C%2520we%2520make%2520use%2520of%2520three%2520feature%2520embedding%250Amodules%2520to%2520predict%2520hand%2520deformation%2520displacements%252C%2520albedo%252C%2520and%2520linear%2520blending%250Askinning%2520weights%252C%2520respectively.%2520To%2520achieve%2520photo-realistic%2520hand%2520rendering%2520on%250Afine-grained%2520meshes%252C%2520our%2520method%2520employs%2520a%2520mesh-based%2520neural%2520renderer%2520by%250Aleveraging%2520mesh%2520topological%2520consistency%2520and%2520latent%2520codes%2520from%2520embedding%250Amodules.%2520During%2520training%252C%2520a%2520part-aware%2520Laplace%2520smoothing%2520strategy%2520is%2520proposed%250Aby%2520incorporating%2520the%2520distinct%2520levels%2520of%2520regularization%2520to%2520effectively%2520maintain%250Athe%2520necessary%2520details%2520and%2520eliminate%2520the%2520undesired%2520artifacts.%2520The%2520experimental%250Aevaluations%2520on%2520InterHand2.6M%2520and%2520DeepHandMesh%2520datasets%2520demonstrate%2520the%2520efficacy%250Aof%2520XHand%252C%2520which%2520is%2520able%2520to%2520recover%2520high-fidelity%2520geometry%2520and%2520texture%2520for%2520hand%250Aanimations%2520across%2520diverse%2520poses%2520in%2520real-time.%2520To%2520reproduce%2520our%2520results%252C%2520we%2520will%250Amake%2520the%2520full%2520implementation%2520publicly%2520available%2520at%250Ahttps%253A//github.com/agnJason/XHand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XHand%3A%20Real-time%20Expressive%20Hand%20Avatar&entry.906535625=Qijun%20Gan%20and%20Zijie%20Zhou%20and%20Jianke%20Zhu&entry.1292438233=%20%20Hand%20avatars%20play%20a%20pivotal%20role%20in%20a%20wide%20array%20of%20digital%20interfaces%2C%0Aenhancing%20user%20immersion%20and%20facilitating%20natural%20interaction%20within%20virtual%0Aenvironments.%20While%20previous%20studies%20have%20focused%20on%20photo-realistic%20hand%0Arendering%2C%20little%20attention%20has%20been%20paid%20to%20reconstruct%20the%20hand%20geometry%20with%0Afine%20details%2C%20which%20is%20essential%20to%20rendering%20quality.%20In%20the%20realms%20of%0Aextended%20reality%20and%20gaming%2C%20on-the-fly%20rendering%20becomes%20imperative.%20To%20this%0Aend%2C%20we%20introduce%20an%20expressive%20hand%20avatar%2C%20named%20XHand%2C%20that%20is%20designed%20to%0Acomprehensively%20generate%20hand%20shape%2C%20appearance%2C%20and%20deformations%20in%20real-time.%0ATo%20obtain%20fine-grained%20hand%20meshes%2C%20we%20make%20use%20of%20three%20feature%20embedding%0Amodules%20to%20predict%20hand%20deformation%20displacements%2C%20albedo%2C%20and%20linear%20blending%0Askinning%20weights%2C%20respectively.%20To%20achieve%20photo-realistic%20hand%20rendering%20on%0Afine-grained%20meshes%2C%20our%20method%20employs%20a%20mesh-based%20neural%20renderer%20by%0Aleveraging%20mesh%20topological%20consistency%20and%20latent%20codes%20from%20embedding%0Amodules.%20During%20training%2C%20a%20part-aware%20Laplace%20smoothing%20strategy%20is%20proposed%0Aby%20incorporating%20the%20distinct%20levels%20of%20regularization%20to%20effectively%20maintain%0Athe%20necessary%20details%20and%20eliminate%20the%20undesired%20artifacts.%20The%20experimental%0Aevaluations%20on%20InterHand2.6M%20and%20DeepHandMesh%20datasets%20demonstrate%20the%20efficacy%0Aof%20XHand%2C%20which%20is%20able%20to%20recover%20high-fidelity%20geometry%20and%20texture%20for%20hand%0Aanimations%20across%20diverse%20poses%20in%20real-time.%20To%20reproduce%20our%20results%2C%20we%20will%0Amake%20the%20full%20implementation%20publicly%20available%20at%0Ahttps%3A//github.com/agnJason/XHand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21002v1&entry.124074799=Read"},
{"title": "SALSA: Swift Adaptive Lightweight Self-Attention for Enhanced LiDAR\n  Place Recognition", "author": "Raktim Gautam Goswami and Naman Patel and Prashanth Krishnamurthy and Farshad Khorrami", "abstract": "  Large-scale LiDAR mappings and localization leverage place recognition\ntechniques to mitigate odometry drifts, ensuring accurate mapping. These\ntechniques utilize scene representations from LiDAR point clouds to identify\npreviously visited sites within a database. Local descriptors, assigned to each\npoint within a point cloud, are aggregated to form a scene representation for\nthe point cloud. These descriptors are also used to re-rank the retrieved point\nclouds based on geometric fitness scores. We propose SALSA, a novel,\nlightweight, and efficient framework for LiDAR place recognition. It consists\nof a Sphereformer backbone that uses radial window attention to enable\ninformation aggregation for sparse distant points, an adaptive self-attention\nlayer to pool local descriptors into tokens, and a multi-layer-perceptron Mixer\nlayer for aggregating the tokens to generate a scene descriptor. The proposed\nframework outperforms existing methods on various LiDAR place recognition\ndatasets in terms of both retrieval and metric localization while operating in\nreal-time.\n", "link": "http://arxiv.org/abs/2407.08260v2", "date": "2024-07-30", "relevancy": 2.9371, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.606}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SALSA%3A%20Swift%20Adaptive%20Lightweight%20Self-Attention%20for%20Enhanced%20LiDAR%0A%20%20Place%20Recognition&body=Title%3A%20SALSA%3A%20Swift%20Adaptive%20Lightweight%20Self-Attention%20for%20Enhanced%20LiDAR%0A%20%20Place%20Recognition%0AAuthor%3A%20Raktim%20Gautam%20Goswami%20and%20Naman%20Patel%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami%0AAbstract%3A%20%20%20Large-scale%20LiDAR%20mappings%20and%20localization%20leverage%20place%20recognition%0Atechniques%20to%20mitigate%20odometry%20drifts%2C%20ensuring%20accurate%20mapping.%20These%0Atechniques%20utilize%20scene%20representations%20from%20LiDAR%20point%20clouds%20to%20identify%0Apreviously%20visited%20sites%20within%20a%20database.%20Local%20descriptors%2C%20assigned%20to%20each%0Apoint%20within%20a%20point%20cloud%2C%20are%20aggregated%20to%20form%20a%20scene%20representation%20for%0Athe%20point%20cloud.%20These%20descriptors%20are%20also%20used%20to%20re-rank%20the%20retrieved%20point%0Aclouds%20based%20on%20geometric%20fitness%20scores.%20We%20propose%20SALSA%2C%20a%20novel%2C%0Alightweight%2C%20and%20efficient%20framework%20for%20LiDAR%20place%20recognition.%20It%20consists%0Aof%20a%20Sphereformer%20backbone%20that%20uses%20radial%20window%20attention%20to%20enable%0Ainformation%20aggregation%20for%20sparse%20distant%20points%2C%20an%20adaptive%20self-attention%0Alayer%20to%20pool%20local%20descriptors%20into%20tokens%2C%20and%20a%20multi-layer-perceptron%20Mixer%0Alayer%20for%20aggregating%20the%20tokens%20to%20generate%20a%20scene%20descriptor.%20The%20proposed%0Aframework%20outperforms%20existing%20methods%20on%20various%20LiDAR%20place%20recognition%0Adatasets%20in%20terms%20of%20both%20retrieval%20and%20metric%20localization%20while%20operating%20in%0Areal-time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08260v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSALSA%253A%2520Swift%2520Adaptive%2520Lightweight%2520Self-Attention%2520for%2520Enhanced%2520LiDAR%250A%2520%2520Place%2520Recognition%26entry.906535625%3DRaktim%2520Gautam%2520Goswami%2520and%2520Naman%2520Patel%2520and%2520Prashanth%2520Krishnamurthy%2520and%2520Farshad%2520Khorrami%26entry.1292438233%3D%2520%2520Large-scale%2520LiDAR%2520mappings%2520and%2520localization%2520leverage%2520place%2520recognition%250Atechniques%2520to%2520mitigate%2520odometry%2520drifts%252C%2520ensuring%2520accurate%2520mapping.%2520These%250Atechniques%2520utilize%2520scene%2520representations%2520from%2520LiDAR%2520point%2520clouds%2520to%2520identify%250Apreviously%2520visited%2520sites%2520within%2520a%2520database.%2520Local%2520descriptors%252C%2520assigned%2520to%2520each%250Apoint%2520within%2520a%2520point%2520cloud%252C%2520are%2520aggregated%2520to%2520form%2520a%2520scene%2520representation%2520for%250Athe%2520point%2520cloud.%2520These%2520descriptors%2520are%2520also%2520used%2520to%2520re-rank%2520the%2520retrieved%2520point%250Aclouds%2520based%2520on%2520geometric%2520fitness%2520scores.%2520We%2520propose%2520SALSA%252C%2520a%2520novel%252C%250Alightweight%252C%2520and%2520efficient%2520framework%2520for%2520LiDAR%2520place%2520recognition.%2520It%2520consists%250Aof%2520a%2520Sphereformer%2520backbone%2520that%2520uses%2520radial%2520window%2520attention%2520to%2520enable%250Ainformation%2520aggregation%2520for%2520sparse%2520distant%2520points%252C%2520an%2520adaptive%2520self-attention%250Alayer%2520to%2520pool%2520local%2520descriptors%2520into%2520tokens%252C%2520and%2520a%2520multi-layer-perceptron%2520Mixer%250Alayer%2520for%2520aggregating%2520the%2520tokens%2520to%2520generate%2520a%2520scene%2520descriptor.%2520The%2520proposed%250Aframework%2520outperforms%2520existing%2520methods%2520on%2520various%2520LiDAR%2520place%2520recognition%250Adatasets%2520in%2520terms%2520of%2520both%2520retrieval%2520and%2520metric%2520localization%2520while%2520operating%2520in%250Areal-time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08260v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SALSA%3A%20Swift%20Adaptive%20Lightweight%20Self-Attention%20for%20Enhanced%20LiDAR%0A%20%20Place%20Recognition&entry.906535625=Raktim%20Gautam%20Goswami%20and%20Naman%20Patel%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami&entry.1292438233=%20%20Large-scale%20LiDAR%20mappings%20and%20localization%20leverage%20place%20recognition%0Atechniques%20to%20mitigate%20odometry%20drifts%2C%20ensuring%20accurate%20mapping.%20These%0Atechniques%20utilize%20scene%20representations%20from%20LiDAR%20point%20clouds%20to%20identify%0Apreviously%20visited%20sites%20within%20a%20database.%20Local%20descriptors%2C%20assigned%20to%20each%0Apoint%20within%20a%20point%20cloud%2C%20are%20aggregated%20to%20form%20a%20scene%20representation%20for%0Athe%20point%20cloud.%20These%20descriptors%20are%20also%20used%20to%20re-rank%20the%20retrieved%20point%0Aclouds%20based%20on%20geometric%20fitness%20scores.%20We%20propose%20SALSA%2C%20a%20novel%2C%0Alightweight%2C%20and%20efficient%20framework%20for%20LiDAR%20place%20recognition.%20It%20consists%0Aof%20a%20Sphereformer%20backbone%20that%20uses%20radial%20window%20attention%20to%20enable%0Ainformation%20aggregation%20for%20sparse%20distant%20points%2C%20an%20adaptive%20self-attention%0Alayer%20to%20pool%20local%20descriptors%20into%20tokens%2C%20and%20a%20multi-layer-perceptron%20Mixer%0Alayer%20for%20aggregating%20the%20tokens%20to%20generate%20a%20scene%20descriptor.%20The%20proposed%0Aframework%20outperforms%20existing%20methods%20on%20various%20LiDAR%20place%20recognition%0Adatasets%20in%20terms%20of%20both%20retrieval%20and%20metric%20localization%20while%20operating%20in%0Areal-time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08260v2&entry.124074799=Read"},
{"title": "NeRF-Supervised Feature Point Detection and Description", "author": "Ali Youssef and Francisco Vasconcelos", "abstract": "  Feature point detection and description is the backbone for various computer\nvision applications, such as Structure-from-Motion, visual SLAM, and visual\nplace recognition. While learning-based methods have surpassed traditional\nhandcrafted techniques, their training often relies on simplistic\nhomography-based simulations of multi-view perspectives, limiting model\ngeneralisability. This paper presents a novel approach leveraging Neural\nRadiance Fields (NeRFs) to generate a diverse and realistic dataset consisting\nof indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art\nfeature detectors and descriptors for training on multi-view NeRF-synthesised\ndata, with supervision achieved through perspective projective geometry.\nExperiments demonstrate that the proposed methodology achieves competitive or\nsuperior performance on standard benchmarks for relative pose estimation, point\ncloud registration, and homography estimation while requiring significantly\nless training data and time compared to existing approaches.\n", "link": "http://arxiv.org/abs/2403.08156v2", "date": "2024-07-30", "relevancy": 2.9199, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6195}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5774}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF-Supervised%20Feature%20Point%20Detection%20and%20Description&body=Title%3A%20NeRF-Supervised%20Feature%20Point%20Detection%20and%20Description%0AAuthor%3A%20Ali%20Youssef%20and%20Francisco%20Vasconcelos%0AAbstract%3A%20%20%20Feature%20point%20detection%20and%20description%20is%20the%20backbone%20for%20various%20computer%0Avision%20applications%2C%20such%20as%20Structure-from-Motion%2C%20visual%20SLAM%2C%20and%20visual%0Aplace%20recognition.%20While%20learning-based%20methods%20have%20surpassed%20traditional%0Ahandcrafted%20techniques%2C%20their%20training%20often%20relies%20on%20simplistic%0Ahomography-based%20simulations%20of%20multi-view%20perspectives%2C%20limiting%20model%0Ageneralisability.%20This%20paper%20presents%20a%20novel%20approach%20leveraging%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20to%20generate%20a%20diverse%20and%20realistic%20dataset%20consisting%0Aof%20indoor%20and%20outdoor%20scenes.%20Our%20proposed%20methodology%20adapts%20state-of-the-art%0Afeature%20detectors%20and%20descriptors%20for%20training%20on%20multi-view%20NeRF-synthesised%0Adata%2C%20with%20supervision%20achieved%20through%20perspective%20projective%20geometry.%0AExperiments%20demonstrate%20that%20the%20proposed%20methodology%20achieves%20competitive%20or%0Asuperior%20performance%20on%20standard%20benchmarks%20for%20relative%20pose%20estimation%2C%20point%0Acloud%20registration%2C%20and%20homography%20estimation%20while%20requiring%20significantly%0Aless%20training%20data%20and%20time%20compared%20to%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08156v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF-Supervised%2520Feature%2520Point%2520Detection%2520and%2520Description%26entry.906535625%3DAli%2520Youssef%2520and%2520Francisco%2520Vasconcelos%26entry.1292438233%3D%2520%2520Feature%2520point%2520detection%2520and%2520description%2520is%2520the%2520backbone%2520for%2520various%2520computer%250Avision%2520applications%252C%2520such%2520as%2520Structure-from-Motion%252C%2520visual%2520SLAM%252C%2520and%2520visual%250Aplace%2520recognition.%2520While%2520learning-based%2520methods%2520have%2520surpassed%2520traditional%250Ahandcrafted%2520techniques%252C%2520their%2520training%2520often%2520relies%2520on%2520simplistic%250Ahomography-based%2520simulations%2520of%2520multi-view%2520perspectives%252C%2520limiting%2520model%250Ageneralisability.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520leveraging%2520Neural%250ARadiance%2520Fields%2520%2528NeRFs%2529%2520to%2520generate%2520a%2520diverse%2520and%2520realistic%2520dataset%2520consisting%250Aof%2520indoor%2520and%2520outdoor%2520scenes.%2520Our%2520proposed%2520methodology%2520adapts%2520state-of-the-art%250Afeature%2520detectors%2520and%2520descriptors%2520for%2520training%2520on%2520multi-view%2520NeRF-synthesised%250Adata%252C%2520with%2520supervision%2520achieved%2520through%2520perspective%2520projective%2520geometry.%250AExperiments%2520demonstrate%2520that%2520the%2520proposed%2520methodology%2520achieves%2520competitive%2520or%250Asuperior%2520performance%2520on%2520standard%2520benchmarks%2520for%2520relative%2520pose%2520estimation%252C%2520point%250Acloud%2520registration%252C%2520and%2520homography%2520estimation%2520while%2520requiring%2520significantly%250Aless%2520training%2520data%2520and%2520time%2520compared%2520to%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08156v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-Supervised%20Feature%20Point%20Detection%20and%20Description&entry.906535625=Ali%20Youssef%20and%20Francisco%20Vasconcelos&entry.1292438233=%20%20Feature%20point%20detection%20and%20description%20is%20the%20backbone%20for%20various%20computer%0Avision%20applications%2C%20such%20as%20Structure-from-Motion%2C%20visual%20SLAM%2C%20and%20visual%0Aplace%20recognition.%20While%20learning-based%20methods%20have%20surpassed%20traditional%0Ahandcrafted%20techniques%2C%20their%20training%20often%20relies%20on%20simplistic%0Ahomography-based%20simulations%20of%20multi-view%20perspectives%2C%20limiting%20model%0Ageneralisability.%20This%20paper%20presents%20a%20novel%20approach%20leveraging%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20to%20generate%20a%20diverse%20and%20realistic%20dataset%20consisting%0Aof%20indoor%20and%20outdoor%20scenes.%20Our%20proposed%20methodology%20adapts%20state-of-the-art%0Afeature%20detectors%20and%20descriptors%20for%20training%20on%20multi-view%20NeRF-synthesised%0Adata%2C%20with%20supervision%20achieved%20through%20perspective%20projective%20geometry.%0AExperiments%20demonstrate%20that%20the%20proposed%20methodology%20achieves%20competitive%20or%0Asuperior%20performance%20on%20standard%20benchmarks%20for%20relative%20pose%20estimation%2C%20point%0Acloud%20registration%2C%20and%20homography%20estimation%20while%20requiring%20significantly%0Aless%20training%20data%20and%20time%20compared%20to%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08156v2&entry.124074799=Read"},
{"title": "Semantic Image Synthesis via Class-Adaptive Cross-Attention", "author": "Tomaso Fontanini and Claudio Ferrari and Giuseppe Lisanti and Massimo Bertozzi and Andrea Prati", "abstract": "  In semantic image synthesis the state of the art is dominated by methods that\nuse customized variants of the SPatially-Adaptive DE-normalization (SPADE)\nlayers, which allow for good visual generation quality and editing versatility.\nBy design, such layers learn pixel-wise modulation parameters to de-normalize\nthe generator activations based on the semantic class each pixel belongs to.\nThus, they tend to overlook global image statistics, ultimately leading to\nunconvincing local style editing and causing global inconsistencies such as\ncolor or illumination distribution shifts. Also, SPADE layers require the\nsemantic segmentation mask for mapping styles in the generator, preventing\nshape manipulations without manual intervention. In response, we designed a\nnovel architecture where cross-attention layers are used in place of SPADE for\nlearning shape-style correlations and so conditioning the image generation\nprocess. Our model inherits the versatility of SPADE, at the same time\nobtaining state-of-the-art generation quality, as well as improved global and\nlocal style transfer. Code and models available at\nhttps://github.com/TFonta/CA2SIS.\n", "link": "http://arxiv.org/abs/2308.16071v3", "date": "2024-07-30", "relevancy": 2.8162, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.568}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5676}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Image%20Synthesis%20via%20Class-Adaptive%20Cross-Attention&body=Title%3A%20Semantic%20Image%20Synthesis%20via%20Class-Adaptive%20Cross-Attention%0AAuthor%3A%20Tomaso%20Fontanini%20and%20Claudio%20Ferrari%20and%20Giuseppe%20Lisanti%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati%0AAbstract%3A%20%20%20In%20semantic%20image%20synthesis%20the%20state%20of%20the%20art%20is%20dominated%20by%20methods%20that%0Ause%20customized%20variants%20of%20the%20SPatially-Adaptive%20DE-normalization%20%28SPADE%29%0Alayers%2C%20which%20allow%20for%20good%20visual%20generation%20quality%20and%20editing%20versatility.%0ABy%20design%2C%20such%20layers%20learn%20pixel-wise%20modulation%20parameters%20to%20de-normalize%0Athe%20generator%20activations%20based%20on%20the%20semantic%20class%20each%20pixel%20belongs%20to.%0AThus%2C%20they%20tend%20to%20overlook%20global%20image%20statistics%2C%20ultimately%20leading%20to%0Aunconvincing%20local%20style%20editing%20and%20causing%20global%20inconsistencies%20such%20as%0Acolor%20or%20illumination%20distribution%20shifts.%20Also%2C%20SPADE%20layers%20require%20the%0Asemantic%20segmentation%20mask%20for%20mapping%20styles%20in%20the%20generator%2C%20preventing%0Ashape%20manipulations%20without%20manual%20intervention.%20In%20response%2C%20we%20designed%20a%0Anovel%20architecture%20where%20cross-attention%20layers%20are%20used%20in%20place%20of%20SPADE%20for%0Alearning%20shape-style%20correlations%20and%20so%20conditioning%20the%20image%20generation%0Aprocess.%20Our%20model%20inherits%20the%20versatility%20of%20SPADE%2C%20at%20the%20same%20time%0Aobtaining%20state-of-the-art%20generation%20quality%2C%20as%20well%20as%20improved%20global%20and%0Alocal%20style%20transfer.%20Code%20and%20models%20available%20at%0Ahttps%3A//github.com/TFonta/CA2SIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.16071v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Image%2520Synthesis%2520via%2520Class-Adaptive%2520Cross-Attention%26entry.906535625%3DTomaso%2520Fontanini%2520and%2520Claudio%2520Ferrari%2520and%2520Giuseppe%2520Lisanti%2520and%2520Massimo%2520Bertozzi%2520and%2520Andrea%2520Prati%26entry.1292438233%3D%2520%2520In%2520semantic%2520image%2520synthesis%2520the%2520state%2520of%2520the%2520art%2520is%2520dominated%2520by%2520methods%2520that%250Ause%2520customized%2520variants%2520of%2520the%2520SPatially-Adaptive%2520DE-normalization%2520%2528SPADE%2529%250Alayers%252C%2520which%2520allow%2520for%2520good%2520visual%2520generation%2520quality%2520and%2520editing%2520versatility.%250ABy%2520design%252C%2520such%2520layers%2520learn%2520pixel-wise%2520modulation%2520parameters%2520to%2520de-normalize%250Athe%2520generator%2520activations%2520based%2520on%2520the%2520semantic%2520class%2520each%2520pixel%2520belongs%2520to.%250AThus%252C%2520they%2520tend%2520to%2520overlook%2520global%2520image%2520statistics%252C%2520ultimately%2520leading%2520to%250Aunconvincing%2520local%2520style%2520editing%2520and%2520causing%2520global%2520inconsistencies%2520such%2520as%250Acolor%2520or%2520illumination%2520distribution%2520shifts.%2520Also%252C%2520SPADE%2520layers%2520require%2520the%250Asemantic%2520segmentation%2520mask%2520for%2520mapping%2520styles%2520in%2520the%2520generator%252C%2520preventing%250Ashape%2520manipulations%2520without%2520manual%2520intervention.%2520In%2520response%252C%2520we%2520designed%2520a%250Anovel%2520architecture%2520where%2520cross-attention%2520layers%2520are%2520used%2520in%2520place%2520of%2520SPADE%2520for%250Alearning%2520shape-style%2520correlations%2520and%2520so%2520conditioning%2520the%2520image%2520generation%250Aprocess.%2520Our%2520model%2520inherits%2520the%2520versatility%2520of%2520SPADE%252C%2520at%2520the%2520same%2520time%250Aobtaining%2520state-of-the-art%2520generation%2520quality%252C%2520as%2520well%2520as%2520improved%2520global%2520and%250Alocal%2520style%2520transfer.%2520Code%2520and%2520models%2520available%2520at%250Ahttps%253A//github.com/TFonta/CA2SIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.16071v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Image%20Synthesis%20via%20Class-Adaptive%20Cross-Attention&entry.906535625=Tomaso%20Fontanini%20and%20Claudio%20Ferrari%20and%20Giuseppe%20Lisanti%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati&entry.1292438233=%20%20In%20semantic%20image%20synthesis%20the%20state%20of%20the%20art%20is%20dominated%20by%20methods%20that%0Ause%20customized%20variants%20of%20the%20SPatially-Adaptive%20DE-normalization%20%28SPADE%29%0Alayers%2C%20which%20allow%20for%20good%20visual%20generation%20quality%20and%20editing%20versatility.%0ABy%20design%2C%20such%20layers%20learn%20pixel-wise%20modulation%20parameters%20to%20de-normalize%0Athe%20generator%20activations%20based%20on%20the%20semantic%20class%20each%20pixel%20belongs%20to.%0AThus%2C%20they%20tend%20to%20overlook%20global%20image%20statistics%2C%20ultimately%20leading%20to%0Aunconvincing%20local%20style%20editing%20and%20causing%20global%20inconsistencies%20such%20as%0Acolor%20or%20illumination%20distribution%20shifts.%20Also%2C%20SPADE%20layers%20require%20the%0Asemantic%20segmentation%20mask%20for%20mapping%20styles%20in%20the%20generator%2C%20preventing%0Ashape%20manipulations%20without%20manual%20intervention.%20In%20response%2C%20we%20designed%20a%0Anovel%20architecture%20where%20cross-attention%20layers%20are%20used%20in%20place%20of%20SPADE%20for%0Alearning%20shape-style%20correlations%20and%20so%20conditioning%20the%20image%20generation%0Aprocess.%20Our%20model%20inherits%20the%20versatility%20of%20SPADE%2C%20at%20the%20same%20time%0Aobtaining%20state-of-the-art%20generation%20quality%2C%20as%20well%20as%20improved%20global%20and%0Alocal%20style%20transfer.%20Code%20and%20models%20available%20at%0Ahttps%3A//github.com/TFonta/CA2SIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.16071v3&entry.124074799=Read"},
{"title": "Autogenic Language Embedding for Coherent Point Tracking", "author": "Zikai Song and Ying Tang and Run Luo and Lintao Ma and Junqing Yu and Yi-Ping Phoebe Chen and Wei Yang", "abstract": "  Point tracking is a challenging task in computer vision, aiming to establish\npoint-wise correspondence across long video sequences. Recent advancements have\nprimarily focused on temporal modeling techniques to improve local feature\nsimilarity, often overlooking the valuable semantic consistency inherent in\ntracked points. In this paper, we introduce a novel approach leveraging\nlanguage embeddings to enhance the coherence of frame-wise visual features\nrelated to the same object. Our proposed method, termed autogenic language\nembedding for visual feature enhancement, strengthens point correspondence in\nlong-term sequences. Unlike existing visual-language schemes, our approach\nlearns text embeddings from visual features through a dedicated mapping\nnetwork, enabling seamless adaptation to various tracking tasks without\nexplicit text annotations. Additionally, we introduce a consistency decoder\nthat efficiently integrates text tokens into visual features with minimal\ncomputational overhead. Through enhanced visual consistency, our approach\nsignificantly improves tracking trajectories in lengthy videos with substantial\nappearance variations. Extensive experiments on widely-used tracking benchmarks\ndemonstrate the superior performance of our method, showcasing notable\nenhancements compared to trackers relying solely on visual cues.\n", "link": "http://arxiv.org/abs/2407.20730v1", "date": "2024-07-30", "relevancy": 2.7516, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6021}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5297}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autogenic%20Language%20Embedding%20for%20Coherent%20Point%20Tracking&body=Title%3A%20Autogenic%20Language%20Embedding%20for%20Coherent%20Point%20Tracking%0AAuthor%3A%20Zikai%20Song%20and%20Ying%20Tang%20and%20Run%20Luo%20and%20Lintao%20Ma%20and%20Junqing%20Yu%20and%20Yi-Ping%20Phoebe%20Chen%20and%20Wei%20Yang%0AAbstract%3A%20%20%20Point%20tracking%20is%20a%20challenging%20task%20in%20computer%20vision%2C%20aiming%20to%20establish%0Apoint-wise%20correspondence%20across%20long%20video%20sequences.%20Recent%20advancements%20have%0Aprimarily%20focused%20on%20temporal%20modeling%20techniques%20to%20improve%20local%20feature%0Asimilarity%2C%20often%20overlooking%20the%20valuable%20semantic%20consistency%20inherent%20in%0Atracked%20points.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20leveraging%0Alanguage%20embeddings%20to%20enhance%20the%20coherence%20of%20frame-wise%20visual%20features%0Arelated%20to%20the%20same%20object.%20Our%20proposed%20method%2C%20termed%20autogenic%20language%0Aembedding%20for%20visual%20feature%20enhancement%2C%20strengthens%20point%20correspondence%20in%0Along-term%20sequences.%20Unlike%20existing%20visual-language%20schemes%2C%20our%20approach%0Alearns%20text%20embeddings%20from%20visual%20features%20through%20a%20dedicated%20mapping%0Anetwork%2C%20enabling%20seamless%20adaptation%20to%20various%20tracking%20tasks%20without%0Aexplicit%20text%20annotations.%20Additionally%2C%20we%20introduce%20a%20consistency%20decoder%0Athat%20efficiently%20integrates%20text%20tokens%20into%20visual%20features%20with%20minimal%0Acomputational%20overhead.%20Through%20enhanced%20visual%20consistency%2C%20our%20approach%0Asignificantly%20improves%20tracking%20trajectories%20in%20lengthy%20videos%20with%20substantial%0Aappearance%20variations.%20Extensive%20experiments%20on%20widely-used%20tracking%20benchmarks%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%2C%20showcasing%20notable%0Aenhancements%20compared%20to%20trackers%20relying%20solely%20on%20visual%20cues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutogenic%2520Language%2520Embedding%2520for%2520Coherent%2520Point%2520Tracking%26entry.906535625%3DZikai%2520Song%2520and%2520Ying%2520Tang%2520and%2520Run%2520Luo%2520and%2520Lintao%2520Ma%2520and%2520Junqing%2520Yu%2520and%2520Yi-Ping%2520Phoebe%2520Chen%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520Point%2520tracking%2520is%2520a%2520challenging%2520task%2520in%2520computer%2520vision%252C%2520aiming%2520to%2520establish%250Apoint-wise%2520correspondence%2520across%2520long%2520video%2520sequences.%2520Recent%2520advancements%2520have%250Aprimarily%2520focused%2520on%2520temporal%2520modeling%2520techniques%2520to%2520improve%2520local%2520feature%250Asimilarity%252C%2520often%2520overlooking%2520the%2520valuable%2520semantic%2520consistency%2520inherent%2520in%250Atracked%2520points.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520leveraging%250Alanguage%2520embeddings%2520to%2520enhance%2520the%2520coherence%2520of%2520frame-wise%2520visual%2520features%250Arelated%2520to%2520the%2520same%2520object.%2520Our%2520proposed%2520method%252C%2520termed%2520autogenic%2520language%250Aembedding%2520for%2520visual%2520feature%2520enhancement%252C%2520strengthens%2520point%2520correspondence%2520in%250Along-term%2520sequences.%2520Unlike%2520existing%2520visual-language%2520schemes%252C%2520our%2520approach%250Alearns%2520text%2520embeddings%2520from%2520visual%2520features%2520through%2520a%2520dedicated%2520mapping%250Anetwork%252C%2520enabling%2520seamless%2520adaptation%2520to%2520various%2520tracking%2520tasks%2520without%250Aexplicit%2520text%2520annotations.%2520Additionally%252C%2520we%2520introduce%2520a%2520consistency%2520decoder%250Athat%2520efficiently%2520integrates%2520text%2520tokens%2520into%2520visual%2520features%2520with%2520minimal%250Acomputational%2520overhead.%2520Through%2520enhanced%2520visual%2520consistency%252C%2520our%2520approach%250Asignificantly%2520improves%2520tracking%2520trajectories%2520in%2520lengthy%2520videos%2520with%2520substantial%250Aappearance%2520variations.%2520Extensive%2520experiments%2520on%2520widely-used%2520tracking%2520benchmarks%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%252C%2520showcasing%2520notable%250Aenhancements%2520compared%2520to%2520trackers%2520relying%2520solely%2520on%2520visual%2520cues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autogenic%20Language%20Embedding%20for%20Coherent%20Point%20Tracking&entry.906535625=Zikai%20Song%20and%20Ying%20Tang%20and%20Run%20Luo%20and%20Lintao%20Ma%20and%20Junqing%20Yu%20and%20Yi-Ping%20Phoebe%20Chen%20and%20Wei%20Yang&entry.1292438233=%20%20Point%20tracking%20is%20a%20challenging%20task%20in%20computer%20vision%2C%20aiming%20to%20establish%0Apoint-wise%20correspondence%20across%20long%20video%20sequences.%20Recent%20advancements%20have%0Aprimarily%20focused%20on%20temporal%20modeling%20techniques%20to%20improve%20local%20feature%0Asimilarity%2C%20often%20overlooking%20the%20valuable%20semantic%20consistency%20inherent%20in%0Atracked%20points.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20leveraging%0Alanguage%20embeddings%20to%20enhance%20the%20coherence%20of%20frame-wise%20visual%20features%0Arelated%20to%20the%20same%20object.%20Our%20proposed%20method%2C%20termed%20autogenic%20language%0Aembedding%20for%20visual%20feature%20enhancement%2C%20strengthens%20point%20correspondence%20in%0Along-term%20sequences.%20Unlike%20existing%20visual-language%20schemes%2C%20our%20approach%0Alearns%20text%20embeddings%20from%20visual%20features%20through%20a%20dedicated%20mapping%0Anetwork%2C%20enabling%20seamless%20adaptation%20to%20various%20tracking%20tasks%20without%0Aexplicit%20text%20annotations.%20Additionally%2C%20we%20introduce%20a%20consistency%20decoder%0Athat%20efficiently%20integrates%20text%20tokens%20into%20visual%20features%20with%20minimal%0Acomputational%20overhead.%20Through%20enhanced%20visual%20consistency%2C%20our%20approach%0Asignificantly%20improves%20tracking%20trajectories%20in%20lengthy%20videos%20with%20substantial%0Aappearance%20variations.%20Extensive%20experiments%20on%20widely-used%20tracking%20benchmarks%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%2C%20showcasing%20notable%0Aenhancements%20compared%20to%20trackers%20relying%20solely%20on%20visual%20cues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20730v1&entry.124074799=Read"},
{"title": "A Comparative Study of Neural Surface Reconstruction for Scientific\n  Visualization", "author": "Siyuan Yao and Weixi Song and Chaoli Wang", "abstract": "  This comparative study evaluates various neural surface reconstruction\nmethods, particularly focusing on their implications for scientific\nvisualization through reconstructing 3D surfaces via multi-view rendering\nimages. We categorize ten methods into neural radiance fields and neural\nimplicit surfaces, uncovering the benefits of leveraging distance functions\n(i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the\nreconstructed surfaces. Our findings highlight the efficiency and quality of\nNeuS2 for reconstructing closed surfaces and identify NeUDF as a promising\ncandidate for reconstructing open surfaces despite some limitations. By sharing\nour benchmark dataset, we invite researchers to test the performance of their\nmethods, contributing to the advancement of surface reconstruction solutions\nfor scientific visualization.\n", "link": "http://arxiv.org/abs/2407.20868v1", "date": "2024-07-30", "relevancy": 2.7508, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5531}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5531}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Study%20of%20Neural%20Surface%20Reconstruction%20for%20Scientific%0A%20%20Visualization&body=Title%3A%20A%20Comparative%20Study%20of%20Neural%20Surface%20Reconstruction%20for%20Scientific%0A%20%20Visualization%0AAuthor%3A%20Siyuan%20Yao%20and%20Weixi%20Song%20and%20Chaoli%20Wang%0AAbstract%3A%20%20%20This%20comparative%20study%20evaluates%20various%20neural%20surface%20reconstruction%0Amethods%2C%20particularly%20focusing%20on%20their%20implications%20for%20scientific%0Avisualization%20through%20reconstructing%203D%20surfaces%20via%20multi-view%20rendering%0Aimages.%20We%20categorize%20ten%20methods%20into%20neural%20radiance%20fields%20and%20neural%0Aimplicit%20surfaces%2C%20uncovering%20the%20benefits%20of%20leveraging%20distance%20functions%0A%28i.e.%2C%20SDFs%20and%20UDFs%29%20to%20enhance%20the%20accuracy%20and%20smoothness%20of%20the%0Areconstructed%20surfaces.%20Our%20findings%20highlight%20the%20efficiency%20and%20quality%20of%0ANeuS2%20for%20reconstructing%20closed%20surfaces%20and%20identify%20NeUDF%20as%20a%20promising%0Acandidate%20for%20reconstructing%20open%20surfaces%20despite%20some%20limitations.%20By%20sharing%0Aour%20benchmark%20dataset%2C%20we%20invite%20researchers%20to%20test%20the%20performance%20of%20their%0Amethods%2C%20contributing%20to%20the%20advancement%20of%20surface%20reconstruction%20solutions%0Afor%20scientific%20visualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Study%2520of%2520Neural%2520Surface%2520Reconstruction%2520for%2520Scientific%250A%2520%2520Visualization%26entry.906535625%3DSiyuan%2520Yao%2520and%2520Weixi%2520Song%2520and%2520Chaoli%2520Wang%26entry.1292438233%3D%2520%2520This%2520comparative%2520study%2520evaluates%2520various%2520neural%2520surface%2520reconstruction%250Amethods%252C%2520particularly%2520focusing%2520on%2520their%2520implications%2520for%2520scientific%250Avisualization%2520through%2520reconstructing%25203D%2520surfaces%2520via%2520multi-view%2520rendering%250Aimages.%2520We%2520categorize%2520ten%2520methods%2520into%2520neural%2520radiance%2520fields%2520and%2520neural%250Aimplicit%2520surfaces%252C%2520uncovering%2520the%2520benefits%2520of%2520leveraging%2520distance%2520functions%250A%2528i.e.%252C%2520SDFs%2520and%2520UDFs%2529%2520to%2520enhance%2520the%2520accuracy%2520and%2520smoothness%2520of%2520the%250Areconstructed%2520surfaces.%2520Our%2520findings%2520highlight%2520the%2520efficiency%2520and%2520quality%2520of%250ANeuS2%2520for%2520reconstructing%2520closed%2520surfaces%2520and%2520identify%2520NeUDF%2520as%2520a%2520promising%250Acandidate%2520for%2520reconstructing%2520open%2520surfaces%2520despite%2520some%2520limitations.%2520By%2520sharing%250Aour%2520benchmark%2520dataset%252C%2520we%2520invite%2520researchers%2520to%2520test%2520the%2520performance%2520of%2520their%250Amethods%252C%2520contributing%2520to%2520the%2520advancement%2520of%2520surface%2520reconstruction%2520solutions%250Afor%2520scientific%2520visualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Study%20of%20Neural%20Surface%20Reconstruction%20for%20Scientific%0A%20%20Visualization&entry.906535625=Siyuan%20Yao%20and%20Weixi%20Song%20and%20Chaoli%20Wang&entry.1292438233=%20%20This%20comparative%20study%20evaluates%20various%20neural%20surface%20reconstruction%0Amethods%2C%20particularly%20focusing%20on%20their%20implications%20for%20scientific%0Avisualization%20through%20reconstructing%203D%20surfaces%20via%20multi-view%20rendering%0Aimages.%20We%20categorize%20ten%20methods%20into%20neural%20radiance%20fields%20and%20neural%0Aimplicit%20surfaces%2C%20uncovering%20the%20benefits%20of%20leveraging%20distance%20functions%0A%28i.e.%2C%20SDFs%20and%20UDFs%29%20to%20enhance%20the%20accuracy%20and%20smoothness%20of%20the%0Areconstructed%20surfaces.%20Our%20findings%20highlight%20the%20efficiency%20and%20quality%20of%0ANeuS2%20for%20reconstructing%20closed%20surfaces%20and%20identify%20NeUDF%20as%20a%20promising%0Acandidate%20for%20reconstructing%20open%20surfaces%20despite%20some%20limitations.%20By%20sharing%0Aour%20benchmark%20dataset%2C%20we%20invite%20researchers%20to%20test%20the%20performance%20of%20their%0Amethods%2C%20contributing%20to%20the%20advancement%20of%20surface%20reconstruction%20solutions%0Afor%20scientific%20visualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20868v1&entry.124074799=Read"},
{"title": "Surgical Text-to-Image Generation", "author": "Chinedu Innocent Nwoye and Rupak Bose and Kareem Elgohary and Lorenzo Arboit and Giorgio Carlino and Jo\u00ebl L. Lavanchy and Pietro Mascagni and Nicolas Padoy", "abstract": "  Acquiring surgical data for research and development is significantly\nhindered by high annotation costs and practical and ethical constraints.\nUtilizing synthetically generated images could offer a valuable alternative. In\nthis work, we explore adapting text-to-image generative models for the surgical\ndomain using the CholecT50 dataset, which provides surgical images annotated\nwith action triplets (instrument, verb, target). We investigate several\nlanguage models and find T5 to offer more distinct features for differentiating\nsurgical actions on triplet-based textual inputs, and showcasing stronger\nalignment between long and triplet-based captions. To address challenges in\ntraining text-to-image models solely on triplet-based captions without\nadditional inputs and supervisory signals, we discover that triplet text\nembeddings are instrument-centric in the latent space. Leveraging this insight,\nwe design an instrument-based class balancing technique to counteract data\nimbalance and skewness, improving training convergence. Extending Imagen, a\ndiffusion-based generative model, we develop Surgical Imagen to generate\nphotorealistic and activity-aligned surgical images from triplet-based textual\nprompts. We assess the model on quality, alignment, reasoning, and knowledge,\nachieving FID and CLIP scores of 3.7 and 26.8% respectively. Human expert\nsurvey shows that participants were highly challenged by the realistic\ncharacteristics of the generated samples, demonstrating Surgical Imagen's\neffectiveness as a practical alternative to real data collection.\n", "link": "http://arxiv.org/abs/2407.09230v2", "date": "2024-07-30", "relevancy": 2.7312, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5562}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5413}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surgical%20Text-to-Image%20Generation&body=Title%3A%20Surgical%20Text-to-Image%20Generation%0AAuthor%3A%20Chinedu%20Innocent%20Nwoye%20and%20Rupak%20Bose%20and%20Kareem%20Elgohary%20and%20Lorenzo%20Arboit%20and%20Giorgio%20Carlino%20and%20Jo%C3%ABl%20L.%20Lavanchy%20and%20Pietro%20Mascagni%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Acquiring%20surgical%20data%20for%20research%20and%20development%20is%20significantly%0Ahindered%20by%20high%20annotation%20costs%20and%20practical%20and%20ethical%20constraints.%0AUtilizing%20synthetically%20generated%20images%20could%20offer%20a%20valuable%20alternative.%20In%0Athis%20work%2C%20we%20explore%20adapting%20text-to-image%20generative%20models%20for%20the%20surgical%0Adomain%20using%20the%20CholecT50%20dataset%2C%20which%20provides%20surgical%20images%20annotated%0Awith%20action%20triplets%20%28instrument%2C%20verb%2C%20target%29.%20We%20investigate%20several%0Alanguage%20models%20and%20find%20T5%20to%20offer%20more%20distinct%20features%20for%20differentiating%0Asurgical%20actions%20on%20triplet-based%20textual%20inputs%2C%20and%20showcasing%20stronger%0Aalignment%20between%20long%20and%20triplet-based%20captions.%20To%20address%20challenges%20in%0Atraining%20text-to-image%20models%20solely%20on%20triplet-based%20captions%20without%0Aadditional%20inputs%20and%20supervisory%20signals%2C%20we%20discover%20that%20triplet%20text%0Aembeddings%20are%20instrument-centric%20in%20the%20latent%20space.%20Leveraging%20this%20insight%2C%0Awe%20design%20an%20instrument-based%20class%20balancing%20technique%20to%20counteract%20data%0Aimbalance%20and%20skewness%2C%20improving%20training%20convergence.%20Extending%20Imagen%2C%20a%0Adiffusion-based%20generative%20model%2C%20we%20develop%20Surgical%20Imagen%20to%20generate%0Aphotorealistic%20and%20activity-aligned%20surgical%20images%20from%20triplet-based%20textual%0Aprompts.%20We%20assess%20the%20model%20on%20quality%2C%20alignment%2C%20reasoning%2C%20and%20knowledge%2C%0Aachieving%20FID%20and%20CLIP%20scores%20of%203.7%20and%2026.8%25%20respectively.%20Human%20expert%0Asurvey%20shows%20that%20participants%20were%20highly%20challenged%20by%20the%20realistic%0Acharacteristics%20of%20the%20generated%20samples%2C%20demonstrating%20Surgical%20Imagen%27s%0Aeffectiveness%20as%20a%20practical%20alternative%20to%20real%20data%20collection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgical%2520Text-to-Image%2520Generation%26entry.906535625%3DChinedu%2520Innocent%2520Nwoye%2520and%2520Rupak%2520Bose%2520and%2520Kareem%2520Elgohary%2520and%2520Lorenzo%2520Arboit%2520and%2520Giorgio%2520Carlino%2520and%2520Jo%25C3%25ABl%2520L.%2520Lavanchy%2520and%2520Pietro%2520Mascagni%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Acquiring%2520surgical%2520data%2520for%2520research%2520and%2520development%2520is%2520significantly%250Ahindered%2520by%2520high%2520annotation%2520costs%2520and%2520practical%2520and%2520ethical%2520constraints.%250AUtilizing%2520synthetically%2520generated%2520images%2520could%2520offer%2520a%2520valuable%2520alternative.%2520In%250Athis%2520work%252C%2520we%2520explore%2520adapting%2520text-to-image%2520generative%2520models%2520for%2520the%2520surgical%250Adomain%2520using%2520the%2520CholecT50%2520dataset%252C%2520which%2520provides%2520surgical%2520images%2520annotated%250Awith%2520action%2520triplets%2520%2528instrument%252C%2520verb%252C%2520target%2529.%2520We%2520investigate%2520several%250Alanguage%2520models%2520and%2520find%2520T5%2520to%2520offer%2520more%2520distinct%2520features%2520for%2520differentiating%250Asurgical%2520actions%2520on%2520triplet-based%2520textual%2520inputs%252C%2520and%2520showcasing%2520stronger%250Aalignment%2520between%2520long%2520and%2520triplet-based%2520captions.%2520To%2520address%2520challenges%2520in%250Atraining%2520text-to-image%2520models%2520solely%2520on%2520triplet-based%2520captions%2520without%250Aadditional%2520inputs%2520and%2520supervisory%2520signals%252C%2520we%2520discover%2520that%2520triplet%2520text%250Aembeddings%2520are%2520instrument-centric%2520in%2520the%2520latent%2520space.%2520Leveraging%2520this%2520insight%252C%250Awe%2520design%2520an%2520instrument-based%2520class%2520balancing%2520technique%2520to%2520counteract%2520data%250Aimbalance%2520and%2520skewness%252C%2520improving%2520training%2520convergence.%2520Extending%2520Imagen%252C%2520a%250Adiffusion-based%2520generative%2520model%252C%2520we%2520develop%2520Surgical%2520Imagen%2520to%2520generate%250Aphotorealistic%2520and%2520activity-aligned%2520surgical%2520images%2520from%2520triplet-based%2520textual%250Aprompts.%2520We%2520assess%2520the%2520model%2520on%2520quality%252C%2520alignment%252C%2520reasoning%252C%2520and%2520knowledge%252C%250Aachieving%2520FID%2520and%2520CLIP%2520scores%2520of%25203.7%2520and%252026.8%2525%2520respectively.%2520Human%2520expert%250Asurvey%2520shows%2520that%2520participants%2520were%2520highly%2520challenged%2520by%2520the%2520realistic%250Acharacteristics%2520of%2520the%2520generated%2520samples%252C%2520demonstrating%2520Surgical%2520Imagen%2527s%250Aeffectiveness%2520as%2520a%2520practical%2520alternative%2520to%2520real%2520data%2520collection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surgical%20Text-to-Image%20Generation&entry.906535625=Chinedu%20Innocent%20Nwoye%20and%20Rupak%20Bose%20and%20Kareem%20Elgohary%20and%20Lorenzo%20Arboit%20and%20Giorgio%20Carlino%20and%20Jo%C3%ABl%20L.%20Lavanchy%20and%20Pietro%20Mascagni%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Acquiring%20surgical%20data%20for%20research%20and%20development%20is%20significantly%0Ahindered%20by%20high%20annotation%20costs%20and%20practical%20and%20ethical%20constraints.%0AUtilizing%20synthetically%20generated%20images%20could%20offer%20a%20valuable%20alternative.%20In%0Athis%20work%2C%20we%20explore%20adapting%20text-to-image%20generative%20models%20for%20the%20surgical%0Adomain%20using%20the%20CholecT50%20dataset%2C%20which%20provides%20surgical%20images%20annotated%0Awith%20action%20triplets%20%28instrument%2C%20verb%2C%20target%29.%20We%20investigate%20several%0Alanguage%20models%20and%20find%20T5%20to%20offer%20more%20distinct%20features%20for%20differentiating%0Asurgical%20actions%20on%20triplet-based%20textual%20inputs%2C%20and%20showcasing%20stronger%0Aalignment%20between%20long%20and%20triplet-based%20captions.%20To%20address%20challenges%20in%0Atraining%20text-to-image%20models%20solely%20on%20triplet-based%20captions%20without%0Aadditional%20inputs%20and%20supervisory%20signals%2C%20we%20discover%20that%20triplet%20text%0Aembeddings%20are%20instrument-centric%20in%20the%20latent%20space.%20Leveraging%20this%20insight%2C%0Awe%20design%20an%20instrument-based%20class%20balancing%20technique%20to%20counteract%20data%0Aimbalance%20and%20skewness%2C%20improving%20training%20convergence.%20Extending%20Imagen%2C%20a%0Adiffusion-based%20generative%20model%2C%20we%20develop%20Surgical%20Imagen%20to%20generate%0Aphotorealistic%20and%20activity-aligned%20surgical%20images%20from%20triplet-based%20textual%0Aprompts.%20We%20assess%20the%20model%20on%20quality%2C%20alignment%2C%20reasoning%2C%20and%20knowledge%2C%0Aachieving%20FID%20and%20CLIP%20scores%20of%203.7%20and%2026.8%25%20respectively.%20Human%20expert%0Asurvey%20shows%20that%20participants%20were%20highly%20challenged%20by%20the%20realistic%0Acharacteristics%20of%20the%20generated%20samples%2C%20demonstrating%20Surgical%20Imagen%27s%0Aeffectiveness%20as%20a%20practical%20alternative%20to%20real%20data%20collection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09230v2&entry.124074799=Read"},
{"title": "Differentiable Voxelization and Mesh Morphing", "author": "Yihao Luo and Yikai Wang and Zhengrui Xiang and Yuliang Xiu and Guang Yang and ChoonHwai Yap", "abstract": "  In this paper, we propose the differentiable voxelization of 3D meshes via\nthe winding number and solid angles. The proposed approach achieves fast,\nflexible, and accurate voxelization of 3D meshes, admitting the computation of\ngradients with respect to the input mesh and GPU acceleration. We further\ndemonstrate the application of the proposed voxelization in mesh morphing,\nwhere the voxelized mesh is deformed by a neural network. The proposed method\nis evaluated on the ShapeNet dataset and achieves state-of-the-art performance\nin terms of both accuracy and efficiency.\n", "link": "http://arxiv.org/abs/2407.11272v2", "date": "2024-07-30", "relevancy": 2.5996, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5661}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4968}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Voxelization%20and%20Mesh%20Morphing&body=Title%3A%20Differentiable%20Voxelization%20and%20Mesh%20Morphing%0AAuthor%3A%20Yihao%20Luo%20and%20Yikai%20Wang%20and%20Zhengrui%20Xiang%20and%20Yuliang%20Xiu%20and%20Guang%20Yang%20and%20ChoonHwai%20Yap%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20the%20differentiable%20voxelization%20of%203D%20meshes%20via%0Athe%20winding%20number%20and%20solid%20angles.%20The%20proposed%20approach%20achieves%20fast%2C%0Aflexible%2C%20and%20accurate%20voxelization%20of%203D%20meshes%2C%20admitting%20the%20computation%20of%0Agradients%20with%20respect%20to%20the%20input%20mesh%20and%20GPU%20acceleration.%20We%20further%0Ademonstrate%20the%20application%20of%20the%20proposed%20voxelization%20in%20mesh%20morphing%2C%0Awhere%20the%20voxelized%20mesh%20is%20deformed%20by%20a%20neural%20network.%20The%20proposed%20method%0Ais%20evaluated%20on%20the%20ShapeNet%20dataset%20and%20achieves%20state-of-the-art%20performance%0Ain%20terms%20of%20both%20accuracy%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Voxelization%2520and%2520Mesh%2520Morphing%26entry.906535625%3DYihao%2520Luo%2520and%2520Yikai%2520Wang%2520and%2520Zhengrui%2520Xiang%2520and%2520Yuliang%2520Xiu%2520and%2520Guang%2520Yang%2520and%2520ChoonHwai%2520Yap%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520differentiable%2520voxelization%2520of%25203D%2520meshes%2520via%250Athe%2520winding%2520number%2520and%2520solid%2520angles.%2520The%2520proposed%2520approach%2520achieves%2520fast%252C%250Aflexible%252C%2520and%2520accurate%2520voxelization%2520of%25203D%2520meshes%252C%2520admitting%2520the%2520computation%2520of%250Agradients%2520with%2520respect%2520to%2520the%2520input%2520mesh%2520and%2520GPU%2520acceleration.%2520We%2520further%250Ademonstrate%2520the%2520application%2520of%2520the%2520proposed%2520voxelization%2520in%2520mesh%2520morphing%252C%250Awhere%2520the%2520voxelized%2520mesh%2520is%2520deformed%2520by%2520a%2520neural%2520network.%2520The%2520proposed%2520method%250Ais%2520evaluated%2520on%2520the%2520ShapeNet%2520dataset%2520and%2520achieves%2520state-of-the-art%2520performance%250Ain%2520terms%2520of%2520both%2520accuracy%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Voxelization%20and%20Mesh%20Morphing&entry.906535625=Yihao%20Luo%20and%20Yikai%20Wang%20and%20Zhengrui%20Xiang%20and%20Yuliang%20Xiu%20and%20Guang%20Yang%20and%20ChoonHwai%20Yap&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20the%20differentiable%20voxelization%20of%203D%20meshes%20via%0Athe%20winding%20number%20and%20solid%20angles.%20The%20proposed%20approach%20achieves%20fast%2C%0Aflexible%2C%20and%20accurate%20voxelization%20of%203D%20meshes%2C%20admitting%20the%20computation%20of%0Agradients%20with%20respect%20to%20the%20input%20mesh%20and%20GPU%20acceleration.%20We%20further%0Ademonstrate%20the%20application%20of%20the%20proposed%20voxelization%20in%20mesh%20morphing%2C%0Awhere%20the%20voxelized%20mesh%20is%20deformed%20by%20a%20neural%20network.%20The%20proposed%20method%0Ais%20evaluated%20on%20the%20ShapeNet%20dataset%20and%20achieves%20state-of-the-art%20performance%0Ain%20terms%20of%20both%20accuracy%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11272v2&entry.124074799=Read"},
{"title": "SOWA: Adapting Hierarchical Frozen Window Self-Attention to\n  Visual-Language Models for Better Anomaly Detection", "author": "Zongxiang Hu and Zhaosheng Zhang", "abstract": "  Visual anomaly detection is critical in industrial manufacturing, but\ntraditional methods often rely on extensive normal datasets and custom models,\nlimiting scalability. Recent advancements in large-scale visual-language models\nhave significantly improved zero/few-shot anomaly detection. However, these\napproaches may not fully utilize hierarchical features, potentially missing\nnuanced details. We introduce a window self-attention mechanism based on the\nCLIP model, combined with learnable prompts to process multi-level features\nwithin a Soldier-Offier Window self-Attention (SOWA) framework. Our method has\nbeen tested on five benchmark datasets, demonstrating superior performance by\nleading in 18 out of 20 metrics compared to existing state-of-the-art\ntechniques.\n", "link": "http://arxiv.org/abs/2407.03634v2", "date": "2024-07-30", "relevancy": 2.5663, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOWA%3A%20Adapting%20Hierarchical%20Frozen%20Window%20Self-Attention%20to%0A%20%20Visual-Language%20Models%20for%20Better%20Anomaly%20Detection&body=Title%3A%20SOWA%3A%20Adapting%20Hierarchical%20Frozen%20Window%20Self-Attention%20to%0A%20%20Visual-Language%20Models%20for%20Better%20Anomaly%20Detection%0AAuthor%3A%20Zongxiang%20Hu%20and%20Zhaosheng%20Zhang%0AAbstract%3A%20%20%20Visual%20anomaly%20detection%20is%20critical%20in%20industrial%20manufacturing%2C%20but%0Atraditional%20methods%20often%20rely%20on%20extensive%20normal%20datasets%20and%20custom%20models%2C%0Alimiting%20scalability.%20Recent%20advancements%20in%20large-scale%20visual-language%20models%0Ahave%20significantly%20improved%20zero/few-shot%20anomaly%20detection.%20However%2C%20these%0Aapproaches%20may%20not%20fully%20utilize%20hierarchical%20features%2C%20potentially%20missing%0Anuanced%20details.%20We%20introduce%20a%20window%20self-attention%20mechanism%20based%20on%20the%0ACLIP%20model%2C%20combined%20with%20learnable%20prompts%20to%20process%20multi-level%20features%0Awithin%20a%20Soldier-Offier%20Window%20self-Attention%20%28SOWA%29%20framework.%20Our%20method%20has%0Abeen%20tested%20on%20five%20benchmark%20datasets%2C%20demonstrating%20superior%20performance%20by%0Aleading%20in%2018%20out%20of%2020%20metrics%20compared%20to%20existing%20state-of-the-art%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOWA%253A%2520Adapting%2520Hierarchical%2520Frozen%2520Window%2520Self-Attention%2520to%250A%2520%2520Visual-Language%2520Models%2520for%2520Better%2520Anomaly%2520Detection%26entry.906535625%3DZongxiang%2520Hu%2520and%2520Zhaosheng%2520Zhang%26entry.1292438233%3D%2520%2520Visual%2520anomaly%2520detection%2520is%2520critical%2520in%2520industrial%2520manufacturing%252C%2520but%250Atraditional%2520methods%2520often%2520rely%2520on%2520extensive%2520normal%2520datasets%2520and%2520custom%2520models%252C%250Alimiting%2520scalability.%2520Recent%2520advancements%2520in%2520large-scale%2520visual-language%2520models%250Ahave%2520significantly%2520improved%2520zero/few-shot%2520anomaly%2520detection.%2520However%252C%2520these%250Aapproaches%2520may%2520not%2520fully%2520utilize%2520hierarchical%2520features%252C%2520potentially%2520missing%250Anuanced%2520details.%2520We%2520introduce%2520a%2520window%2520self-attention%2520mechanism%2520based%2520on%2520the%250ACLIP%2520model%252C%2520combined%2520with%2520learnable%2520prompts%2520to%2520process%2520multi-level%2520features%250Awithin%2520a%2520Soldier-Offier%2520Window%2520self-Attention%2520%2528SOWA%2529%2520framework.%2520Our%2520method%2520has%250Abeen%2520tested%2520on%2520five%2520benchmark%2520datasets%252C%2520demonstrating%2520superior%2520performance%2520by%250Aleading%2520in%252018%2520out%2520of%252020%2520metrics%2520compared%2520to%2520existing%2520state-of-the-art%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOWA%3A%20Adapting%20Hierarchical%20Frozen%20Window%20Self-Attention%20to%0A%20%20Visual-Language%20Models%20for%20Better%20Anomaly%20Detection&entry.906535625=Zongxiang%20Hu%20and%20Zhaosheng%20Zhang&entry.1292438233=%20%20Visual%20anomaly%20detection%20is%20critical%20in%20industrial%20manufacturing%2C%20but%0Atraditional%20methods%20often%20rely%20on%20extensive%20normal%20datasets%20and%20custom%20models%2C%0Alimiting%20scalability.%20Recent%20advancements%20in%20large-scale%20visual-language%20models%0Ahave%20significantly%20improved%20zero/few-shot%20anomaly%20detection.%20However%2C%20these%0Aapproaches%20may%20not%20fully%20utilize%20hierarchical%20features%2C%20potentially%20missing%0Anuanced%20details.%20We%20introduce%20a%20window%20self-attention%20mechanism%20based%20on%20the%0ACLIP%20model%2C%20combined%20with%20learnable%20prompts%20to%20process%20multi-level%20features%0Awithin%20a%20Soldier-Offier%20Window%20self-Attention%20%28SOWA%29%20framework.%20Our%20method%20has%0Abeen%20tested%20on%20five%20benchmark%20datasets%2C%20demonstrating%20superior%20performance%20by%0Aleading%20in%2018%20out%20of%2020%20metrics%20compared%20to%20existing%20state-of-the-art%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03634v2&entry.124074799=Read"},
{"title": "An Effective Dynamic Gradient Calibration Method for Continual Learning", "author": "Weichen Lin and Jiaxiang Chen and Ruomin Huang and Hu Ding", "abstract": "  Continual learning (CL) is a fundamental topic in machine learning, where the\ngoal is to train a model with continuously incoming data and tasks. Due to the\nmemory limit, we cannot store all the historical data, and therefore confront\nthe ``catastrophic forgetting'' problem, i.e., the performance on the previous\ntasks can substantially decrease because of the missing information in the\nlatter period. Though a number of elegant methods have been proposed, the\ncatastrophic forgetting phenomenon still cannot be well avoided in practice. In\nthis paper, we study the problem from the gradient perspective, where our aim\nis to develop an effective algorithm to calibrate the gradient in each updating\nstep of the model; namely, our goal is to guide the model to be updated in the\nright direction under the situation that a large amount of historical data are\nunavailable. Our idea is partly inspired by the seminal stochastic variance\nreduction methods (e.g., SVRG and SAGA) for reducing the variance of gradient\nestimation in stochastic gradient descent algorithms. Another benefit is that\nour approach can be used as a general tool, which is able to be incorporated\nwith several existing popular CL methods to achieve better performance. We also\nconduct a set of experiments on several benchmark datasets to evaluate the\nperformance in practice.\n", "link": "http://arxiv.org/abs/2407.20956v1", "date": "2024-07-30", "relevancy": 2.5601, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5497}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5009}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Effective%20Dynamic%20Gradient%20Calibration%20Method%20for%20Continual%20Learning&body=Title%3A%20An%20Effective%20Dynamic%20Gradient%20Calibration%20Method%20for%20Continual%20Learning%0AAuthor%3A%20Weichen%20Lin%20and%20Jiaxiang%20Chen%20and%20Ruomin%20Huang%20and%20Hu%20Ding%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20is%20a%20fundamental%20topic%20in%20machine%20learning%2C%20where%20the%0Agoal%20is%20to%20train%20a%20model%20with%20continuously%20incoming%20data%20and%20tasks.%20Due%20to%20the%0Amemory%20limit%2C%20we%20cannot%20store%20all%20the%20historical%20data%2C%20and%20therefore%20confront%0Athe%20%60%60catastrophic%20forgetting%27%27%20problem%2C%20i.e.%2C%20the%20performance%20on%20the%20previous%0Atasks%20can%20substantially%20decrease%20because%20of%20the%20missing%20information%20in%20the%0Alatter%20period.%20Though%20a%20number%20of%20elegant%20methods%20have%20been%20proposed%2C%20the%0Acatastrophic%20forgetting%20phenomenon%20still%20cannot%20be%20well%20avoided%20in%20practice.%20In%0Athis%20paper%2C%20we%20study%20the%20problem%20from%20the%20gradient%20perspective%2C%20where%20our%20aim%0Ais%20to%20develop%20an%20effective%20algorithm%20to%20calibrate%20the%20gradient%20in%20each%20updating%0Astep%20of%20the%20model%3B%20namely%2C%20our%20goal%20is%20to%20guide%20the%20model%20to%20be%20updated%20in%20the%0Aright%20direction%20under%20the%20situation%20that%20a%20large%20amount%20of%20historical%20data%20are%0Aunavailable.%20Our%20idea%20is%20partly%20inspired%20by%20the%20seminal%20stochastic%20variance%0Areduction%20methods%20%28e.g.%2C%20SVRG%20and%20SAGA%29%20for%20reducing%20the%20variance%20of%20gradient%0Aestimation%20in%20stochastic%20gradient%20descent%20algorithms.%20Another%20benefit%20is%20that%0Aour%20approach%20can%20be%20used%20as%20a%20general%20tool%2C%20which%20is%20able%20to%20be%20incorporated%0Awith%20several%20existing%20popular%20CL%20methods%20to%20achieve%20better%20performance.%20We%20also%0Aconduct%20a%20set%20of%20experiments%20on%20several%20benchmark%20datasets%20to%20evaluate%20the%0Aperformance%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Effective%2520Dynamic%2520Gradient%2520Calibration%2520Method%2520for%2520Continual%2520Learning%26entry.906535625%3DWeichen%2520Lin%2520and%2520Jiaxiang%2520Chen%2520and%2520Ruomin%2520Huang%2520and%2520Hu%2520Ding%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520is%2520a%2520fundamental%2520topic%2520in%2520machine%2520learning%252C%2520where%2520the%250Agoal%2520is%2520to%2520train%2520a%2520model%2520with%2520continuously%2520incoming%2520data%2520and%2520tasks.%2520Due%2520to%2520the%250Amemory%2520limit%252C%2520we%2520cannot%2520store%2520all%2520the%2520historical%2520data%252C%2520and%2520therefore%2520confront%250Athe%2520%2560%2560catastrophic%2520forgetting%2527%2527%2520problem%252C%2520i.e.%252C%2520the%2520performance%2520on%2520the%2520previous%250Atasks%2520can%2520substantially%2520decrease%2520because%2520of%2520the%2520missing%2520information%2520in%2520the%250Alatter%2520period.%2520Though%2520a%2520number%2520of%2520elegant%2520methods%2520have%2520been%2520proposed%252C%2520the%250Acatastrophic%2520forgetting%2520phenomenon%2520still%2520cannot%2520be%2520well%2520avoided%2520in%2520practice.%2520In%250Athis%2520paper%252C%2520we%2520study%2520the%2520problem%2520from%2520the%2520gradient%2520perspective%252C%2520where%2520our%2520aim%250Ais%2520to%2520develop%2520an%2520effective%2520algorithm%2520to%2520calibrate%2520the%2520gradient%2520in%2520each%2520updating%250Astep%2520of%2520the%2520model%253B%2520namely%252C%2520our%2520goal%2520is%2520to%2520guide%2520the%2520model%2520to%2520be%2520updated%2520in%2520the%250Aright%2520direction%2520under%2520the%2520situation%2520that%2520a%2520large%2520amount%2520of%2520historical%2520data%2520are%250Aunavailable.%2520Our%2520idea%2520is%2520partly%2520inspired%2520by%2520the%2520seminal%2520stochastic%2520variance%250Areduction%2520methods%2520%2528e.g.%252C%2520SVRG%2520and%2520SAGA%2529%2520for%2520reducing%2520the%2520variance%2520of%2520gradient%250Aestimation%2520in%2520stochastic%2520gradient%2520descent%2520algorithms.%2520Another%2520benefit%2520is%2520that%250Aour%2520approach%2520can%2520be%2520used%2520as%2520a%2520general%2520tool%252C%2520which%2520is%2520able%2520to%2520be%2520incorporated%250Awith%2520several%2520existing%2520popular%2520CL%2520methods%2520to%2520achieve%2520better%2520performance.%2520We%2520also%250Aconduct%2520a%2520set%2520of%2520experiments%2520on%2520several%2520benchmark%2520datasets%2520to%2520evaluate%2520the%250Aperformance%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Effective%20Dynamic%20Gradient%20Calibration%20Method%20for%20Continual%20Learning&entry.906535625=Weichen%20Lin%20and%20Jiaxiang%20Chen%20and%20Ruomin%20Huang%20and%20Hu%20Ding&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20is%20a%20fundamental%20topic%20in%20machine%20learning%2C%20where%20the%0Agoal%20is%20to%20train%20a%20model%20with%20continuously%20incoming%20data%20and%20tasks.%20Due%20to%20the%0Amemory%20limit%2C%20we%20cannot%20store%20all%20the%20historical%20data%2C%20and%20therefore%20confront%0Athe%20%60%60catastrophic%20forgetting%27%27%20problem%2C%20i.e.%2C%20the%20performance%20on%20the%20previous%0Atasks%20can%20substantially%20decrease%20because%20of%20the%20missing%20information%20in%20the%0Alatter%20period.%20Though%20a%20number%20of%20elegant%20methods%20have%20been%20proposed%2C%20the%0Acatastrophic%20forgetting%20phenomenon%20still%20cannot%20be%20well%20avoided%20in%20practice.%20In%0Athis%20paper%2C%20we%20study%20the%20problem%20from%20the%20gradient%20perspective%2C%20where%20our%20aim%0Ais%20to%20develop%20an%20effective%20algorithm%20to%20calibrate%20the%20gradient%20in%20each%20updating%0Astep%20of%20the%20model%3B%20namely%2C%20our%20goal%20is%20to%20guide%20the%20model%20to%20be%20updated%20in%20the%0Aright%20direction%20under%20the%20situation%20that%20a%20large%20amount%20of%20historical%20data%20are%0Aunavailable.%20Our%20idea%20is%20partly%20inspired%20by%20the%20seminal%20stochastic%20variance%0Areduction%20methods%20%28e.g.%2C%20SVRG%20and%20SAGA%29%20for%20reducing%20the%20variance%20of%20gradient%0Aestimation%20in%20stochastic%20gradient%20descent%20algorithms.%20Another%20benefit%20is%20that%0Aour%20approach%20can%20be%20used%20as%20a%20general%20tool%2C%20which%20is%20able%20to%20be%20incorporated%0Awith%20several%20existing%20popular%20CL%20methods%20to%20achieve%20better%20performance.%20We%20also%0Aconduct%20a%20set%20of%20experiments%20on%20several%20benchmark%20datasets%20to%20evaluate%20the%0Aperformance%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20956v1&entry.124074799=Read"},
{"title": "PIP: Prototypes-Injected Prompt for Federated Class Incremental Learning", "author": "Muhammad Anwar Ma'sum and Mahardhika Pratama and Savitha Ramasamy and Lin Liu and Habibullah Habibullah and Ryszard Kowalczyk", "abstract": "  Federated Class Incremental Learning (FCIL) is a new direction in continual\nlearning (CL) for addressing catastrophic forgetting and non-IID data\ndistribution simultaneously. Existing FCIL methods call for high communication\ncosts and exemplars from previous classes. We propose a novel rehearsal-free\nmethod for FCIL named prototypes-injected prompt (PIP) that involves 3 main\nideas: a) prototype injection on prompt learning, b) prototype augmentation,\nand c) weighted Gaussian aggregation on the server side. Our experiment result\nshows that the proposed method outperforms the current state of the arts\n(SOTAs) with a significant improvement (up to 33%) in CIFAR100, MiniImageNet\nand TinyImageNet datasets. Our extensive analysis demonstrates the robustness\nof PIP in different task sizes, and the advantage of requiring smaller\nparticipating local clients, and smaller global rounds. For further study,\nsource codes of PIP, baseline, and experimental logs are shared publicly in\nhttps://github.com/anwarmaxsum/PIP.\n", "link": "http://arxiv.org/abs/2407.20705v1", "date": "2024-07-30", "relevancy": 2.4673, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5009}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4963}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIP%3A%20Prototypes-Injected%20Prompt%20for%20Federated%20Class%20Incremental%20Learning&body=Title%3A%20PIP%3A%20Prototypes-Injected%20Prompt%20for%20Federated%20Class%20Incremental%20Learning%0AAuthor%3A%20Muhammad%20Anwar%20Ma%27sum%20and%20Mahardhika%20Pratama%20and%20Savitha%20Ramasamy%20and%20Lin%20Liu%20and%20Habibullah%20Habibullah%20and%20Ryszard%20Kowalczyk%0AAbstract%3A%20%20%20Federated%20Class%20Incremental%20Learning%20%28FCIL%29%20is%20a%20new%20direction%20in%20continual%0Alearning%20%28CL%29%20for%20addressing%20catastrophic%20forgetting%20and%20non-IID%20data%0Adistribution%20simultaneously.%20Existing%20FCIL%20methods%20call%20for%20high%20communication%0Acosts%20and%20exemplars%20from%20previous%20classes.%20We%20propose%20a%20novel%20rehearsal-free%0Amethod%20for%20FCIL%20named%20prototypes-injected%20prompt%20%28PIP%29%20that%20involves%203%20main%0Aideas%3A%20a%29%20prototype%20injection%20on%20prompt%20learning%2C%20b%29%20prototype%20augmentation%2C%0Aand%20c%29%20weighted%20Gaussian%20aggregation%20on%20the%20server%20side.%20Our%20experiment%20result%0Ashows%20that%20the%20proposed%20method%20outperforms%20the%20current%20state%20of%20the%20arts%0A%28SOTAs%29%20with%20a%20significant%20improvement%20%28up%20to%2033%25%29%20in%20CIFAR100%2C%20MiniImageNet%0Aand%20TinyImageNet%20datasets.%20Our%20extensive%20analysis%20demonstrates%20the%20robustness%0Aof%20PIP%20in%20different%20task%20sizes%2C%20and%20the%20advantage%20of%20requiring%20smaller%0Aparticipating%20local%20clients%2C%20and%20smaller%20global%20rounds.%20For%20further%20study%2C%0Asource%20codes%20of%20PIP%2C%20baseline%2C%20and%20experimental%20logs%20are%20shared%20publicly%20in%0Ahttps%3A//github.com/anwarmaxsum/PIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIP%253A%2520Prototypes-Injected%2520Prompt%2520for%2520Federated%2520Class%2520Incremental%2520Learning%26entry.906535625%3DMuhammad%2520Anwar%2520Ma%2527sum%2520and%2520Mahardhika%2520Pratama%2520and%2520Savitha%2520Ramasamy%2520and%2520Lin%2520Liu%2520and%2520Habibullah%2520Habibullah%2520and%2520Ryszard%2520Kowalczyk%26entry.1292438233%3D%2520%2520Federated%2520Class%2520Incremental%2520Learning%2520%2528FCIL%2529%2520is%2520a%2520new%2520direction%2520in%2520continual%250Alearning%2520%2528CL%2529%2520for%2520addressing%2520catastrophic%2520forgetting%2520and%2520non-IID%2520data%250Adistribution%2520simultaneously.%2520Existing%2520FCIL%2520methods%2520call%2520for%2520high%2520communication%250Acosts%2520and%2520exemplars%2520from%2520previous%2520classes.%2520We%2520propose%2520a%2520novel%2520rehearsal-free%250Amethod%2520for%2520FCIL%2520named%2520prototypes-injected%2520prompt%2520%2528PIP%2529%2520that%2520involves%25203%2520main%250Aideas%253A%2520a%2529%2520prototype%2520injection%2520on%2520prompt%2520learning%252C%2520b%2529%2520prototype%2520augmentation%252C%250Aand%2520c%2529%2520weighted%2520Gaussian%2520aggregation%2520on%2520the%2520server%2520side.%2520Our%2520experiment%2520result%250Ashows%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%2520current%2520state%2520of%2520the%2520arts%250A%2528SOTAs%2529%2520with%2520a%2520significant%2520improvement%2520%2528up%2520to%252033%2525%2529%2520in%2520CIFAR100%252C%2520MiniImageNet%250Aand%2520TinyImageNet%2520datasets.%2520Our%2520extensive%2520analysis%2520demonstrates%2520the%2520robustness%250Aof%2520PIP%2520in%2520different%2520task%2520sizes%252C%2520and%2520the%2520advantage%2520of%2520requiring%2520smaller%250Aparticipating%2520local%2520clients%252C%2520and%2520smaller%2520global%2520rounds.%2520For%2520further%2520study%252C%250Asource%2520codes%2520of%2520PIP%252C%2520baseline%252C%2520and%2520experimental%2520logs%2520are%2520shared%2520publicly%2520in%250Ahttps%253A//github.com/anwarmaxsum/PIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIP%3A%20Prototypes-Injected%20Prompt%20for%20Federated%20Class%20Incremental%20Learning&entry.906535625=Muhammad%20Anwar%20Ma%27sum%20and%20Mahardhika%20Pratama%20and%20Savitha%20Ramasamy%20and%20Lin%20Liu%20and%20Habibullah%20Habibullah%20and%20Ryszard%20Kowalczyk&entry.1292438233=%20%20Federated%20Class%20Incremental%20Learning%20%28FCIL%29%20is%20a%20new%20direction%20in%20continual%0Alearning%20%28CL%29%20for%20addressing%20catastrophic%20forgetting%20and%20non-IID%20data%0Adistribution%20simultaneously.%20Existing%20FCIL%20methods%20call%20for%20high%20communication%0Acosts%20and%20exemplars%20from%20previous%20classes.%20We%20propose%20a%20novel%20rehearsal-free%0Amethod%20for%20FCIL%20named%20prototypes-injected%20prompt%20%28PIP%29%20that%20involves%203%20main%0Aideas%3A%20a%29%20prototype%20injection%20on%20prompt%20learning%2C%20b%29%20prototype%20augmentation%2C%0Aand%20c%29%20weighted%20Gaussian%20aggregation%20on%20the%20server%20side.%20Our%20experiment%20result%0Ashows%20that%20the%20proposed%20method%20outperforms%20the%20current%20state%20of%20the%20arts%0A%28SOTAs%29%20with%20a%20significant%20improvement%20%28up%20to%2033%25%29%20in%20CIFAR100%2C%20MiniImageNet%0Aand%20TinyImageNet%20datasets.%20Our%20extensive%20analysis%20demonstrates%20the%20robustness%0Aof%20PIP%20in%20different%20task%20sizes%2C%20and%20the%20advantage%20of%20requiring%20smaller%0Aparticipating%20local%20clients%2C%20and%20smaller%20global%20rounds.%20For%20further%20study%2C%0Asource%20codes%20of%20PIP%2C%20baseline%2C%20and%20experimental%20logs%20are%20shared%20publicly%20in%0Ahttps%3A//github.com/anwarmaxsum/PIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20705v1&entry.124074799=Read"},
{"title": "Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for\n  Dynamic Link Prediction", "author": "Ke Cheng and Linzhi Peng and Junchen Ye and Leilei Sun and Bowen Du", "abstract": "  Structure encoding has proven to be the key feature to distinguishing links\nin a graph. However, Structure encoding in the temporal graph keeps changing as\nthe graph evolves, repeatedly computing such features can be time-consuming due\nto the high-order subgraph construction. We develop the Co-Neighbor Encoding\nSchema (CNES) to address this issue. Instead of recomputing the feature by the\nlink, CNES stores information in the memory to avoid redundant calculations.\nBesides, unlike the existing memory-based dynamic graph learning method that\nstores node hidden states, we introduce a hashtable-based memory to compress\nthe adjacency matrix for efficient structure feature construction and updating\nwith vector computation in parallel. Furthermore, CNES introduces a\nTemporal-Diverse Memory to generate long-term and short-term structure encoding\nfor neighbors with different structural information. A dynamic graph learning\nframework, Co-Neighbor Encoding Network (CNE-N), is proposed using the\naforementioned techniques. Extensive experiments on thirteen public datasets\nverify the effectiveness and efficiency of the proposed method.\n", "link": "http://arxiv.org/abs/2407.20871v1", "date": "2024-07-30", "relevancy": 2.4604, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4868}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Neighbor%20Encoding%20Schema%3A%20A%20Light-cost%20Structure%20Encoding%20Method%20for%0A%20%20Dynamic%20Link%20Prediction&body=Title%3A%20Co-Neighbor%20Encoding%20Schema%3A%20A%20Light-cost%20Structure%20Encoding%20Method%20for%0A%20%20Dynamic%20Link%20Prediction%0AAuthor%3A%20Ke%20Cheng%20and%20Linzhi%20Peng%20and%20Junchen%20Ye%20and%20Leilei%20Sun%20and%20Bowen%20Du%0AAbstract%3A%20%20%20Structure%20encoding%20has%20proven%20to%20be%20the%20key%20feature%20to%20distinguishing%20links%0Ain%20a%20graph.%20However%2C%20Structure%20encoding%20in%20the%20temporal%20graph%20keeps%20changing%20as%0Athe%20graph%20evolves%2C%20repeatedly%20computing%20such%20features%20can%20be%20time-consuming%20due%0Ato%20the%20high-order%20subgraph%20construction.%20We%20develop%20the%20Co-Neighbor%20Encoding%0ASchema%20%28CNES%29%20to%20address%20this%20issue.%20Instead%20of%20recomputing%20the%20feature%20by%20the%0Alink%2C%20CNES%20stores%20information%20in%20the%20memory%20to%20avoid%20redundant%20calculations.%0ABesides%2C%20unlike%20the%20existing%20memory-based%20dynamic%20graph%20learning%20method%20that%0Astores%20node%20hidden%20states%2C%20we%20introduce%20a%20hashtable-based%20memory%20to%20compress%0Athe%20adjacency%20matrix%20for%20efficient%20structure%20feature%20construction%20and%20updating%0Awith%20vector%20computation%20in%20parallel.%20Furthermore%2C%20CNES%20introduces%20a%0ATemporal-Diverse%20Memory%20to%20generate%20long-term%20and%20short-term%20structure%20encoding%0Afor%20neighbors%20with%20different%20structural%20information.%20A%20dynamic%20graph%20learning%0Aframework%2C%20Co-Neighbor%20Encoding%20Network%20%28CNE-N%29%2C%20is%20proposed%20using%20the%0Aaforementioned%20techniques.%20Extensive%20experiments%20on%20thirteen%20public%20datasets%0Averify%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Neighbor%2520Encoding%2520Schema%253A%2520A%2520Light-cost%2520Structure%2520Encoding%2520Method%2520for%250A%2520%2520Dynamic%2520Link%2520Prediction%26entry.906535625%3DKe%2520Cheng%2520and%2520Linzhi%2520Peng%2520and%2520Junchen%2520Ye%2520and%2520Leilei%2520Sun%2520and%2520Bowen%2520Du%26entry.1292438233%3D%2520%2520Structure%2520encoding%2520has%2520proven%2520to%2520be%2520the%2520key%2520feature%2520to%2520distinguishing%2520links%250Ain%2520a%2520graph.%2520However%252C%2520Structure%2520encoding%2520in%2520the%2520temporal%2520graph%2520keeps%2520changing%2520as%250Athe%2520graph%2520evolves%252C%2520repeatedly%2520computing%2520such%2520features%2520can%2520be%2520time-consuming%2520due%250Ato%2520the%2520high-order%2520subgraph%2520construction.%2520We%2520develop%2520the%2520Co-Neighbor%2520Encoding%250ASchema%2520%2528CNES%2529%2520to%2520address%2520this%2520issue.%2520Instead%2520of%2520recomputing%2520the%2520feature%2520by%2520the%250Alink%252C%2520CNES%2520stores%2520information%2520in%2520the%2520memory%2520to%2520avoid%2520redundant%2520calculations.%250ABesides%252C%2520unlike%2520the%2520existing%2520memory-based%2520dynamic%2520graph%2520learning%2520method%2520that%250Astores%2520node%2520hidden%2520states%252C%2520we%2520introduce%2520a%2520hashtable-based%2520memory%2520to%2520compress%250Athe%2520adjacency%2520matrix%2520for%2520efficient%2520structure%2520feature%2520construction%2520and%2520updating%250Awith%2520vector%2520computation%2520in%2520parallel.%2520Furthermore%252C%2520CNES%2520introduces%2520a%250ATemporal-Diverse%2520Memory%2520to%2520generate%2520long-term%2520and%2520short-term%2520structure%2520encoding%250Afor%2520neighbors%2520with%2520different%2520structural%2520information.%2520A%2520dynamic%2520graph%2520learning%250Aframework%252C%2520Co-Neighbor%2520Encoding%2520Network%2520%2528CNE-N%2529%252C%2520is%2520proposed%2520using%2520the%250Aaforementioned%2520techniques.%2520Extensive%2520experiments%2520on%2520thirteen%2520public%2520datasets%250Averify%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Neighbor%20Encoding%20Schema%3A%20A%20Light-cost%20Structure%20Encoding%20Method%20for%0A%20%20Dynamic%20Link%20Prediction&entry.906535625=Ke%20Cheng%20and%20Linzhi%20Peng%20and%20Junchen%20Ye%20and%20Leilei%20Sun%20and%20Bowen%20Du&entry.1292438233=%20%20Structure%20encoding%20has%20proven%20to%20be%20the%20key%20feature%20to%20distinguishing%20links%0Ain%20a%20graph.%20However%2C%20Structure%20encoding%20in%20the%20temporal%20graph%20keeps%20changing%20as%0Athe%20graph%20evolves%2C%20repeatedly%20computing%20such%20features%20can%20be%20time-consuming%20due%0Ato%20the%20high-order%20subgraph%20construction.%20We%20develop%20the%20Co-Neighbor%20Encoding%0ASchema%20%28CNES%29%20to%20address%20this%20issue.%20Instead%20of%20recomputing%20the%20feature%20by%20the%0Alink%2C%20CNES%20stores%20information%20in%20the%20memory%20to%20avoid%20redundant%20calculations.%0ABesides%2C%20unlike%20the%20existing%20memory-based%20dynamic%20graph%20learning%20method%20that%0Astores%20node%20hidden%20states%2C%20we%20introduce%20a%20hashtable-based%20memory%20to%20compress%0Athe%20adjacency%20matrix%20for%20efficient%20structure%20feature%20construction%20and%20updating%0Awith%20vector%20computation%20in%20parallel.%20Furthermore%2C%20CNES%20introduces%20a%0ATemporal-Diverse%20Memory%20to%20generate%20long-term%20and%20short-term%20structure%20encoding%0Afor%20neighbors%20with%20different%20structural%20information.%20A%20dynamic%20graph%20learning%0Aframework%2C%20Co-Neighbor%20Encoding%20Network%20%28CNE-N%29%2C%20is%20proposed%20using%20the%0Aaforementioned%20techniques.%20Extensive%20experiments%20on%20thirteen%20public%20datasets%0Averify%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20871v1&entry.124074799=Read"},
{"title": "DeTurb: Atmospheric Turbulence Mitigation with Deformable 3D\n  Convolutions and 3D Swin Transformers", "author": "Zhicheng Zou and Nantheera Anantrasirichai", "abstract": "  Atmospheric turbulence in long-range imaging significantly degrades the\nquality and fidelity of captured scenes due to random variations in both\nspatial and temporal dimensions. These distortions present a formidable\nchallenge across various applications, from surveillance to astronomy,\nnecessitating robust mitigation strategies. While model-based approaches\nachieve good results, they are very slow. Deep learning approaches show promise\nin image and video restoration but have struggled to address these\nspatiotemporal variant distortions effectively. This paper proposes a new\nframework that combines geometric restoration with an enhancement module.\nRandom perturbations and geometric distortion are removed using a pyramid\narchitecture with deformable 3D convolutions, resulting in aligned frames.\nThese frames are then used to reconstruct a sharp, clear image via a\nmulti-scale architecture of 3D Swin Transformers. The proposed framework\ndemonstrates superior performance over the state of the art for both synthetic\nand real atmospheric turbulence effects, with reasonable speed and model size.\n", "link": "http://arxiv.org/abs/2407.20855v1", "date": "2024-07-30", "relevancy": 2.4559, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6381}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6091}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeTurb%3A%20Atmospheric%20Turbulence%20Mitigation%20with%20Deformable%203D%0A%20%20Convolutions%20and%203D%20Swin%20Transformers&body=Title%3A%20DeTurb%3A%20Atmospheric%20Turbulence%20Mitigation%20with%20Deformable%203D%0A%20%20Convolutions%20and%203D%20Swin%20Transformers%0AAuthor%3A%20Zhicheng%20Zou%20and%20Nantheera%20Anantrasirichai%0AAbstract%3A%20%20%20Atmospheric%20turbulence%20in%20long-range%20imaging%20significantly%20degrades%20the%0Aquality%20and%20fidelity%20of%20captured%20scenes%20due%20to%20random%20variations%20in%20both%0Aspatial%20and%20temporal%20dimensions.%20These%20distortions%20present%20a%20formidable%0Achallenge%20across%20various%20applications%2C%20from%20surveillance%20to%20astronomy%2C%0Anecessitating%20robust%20mitigation%20strategies.%20While%20model-based%20approaches%0Aachieve%20good%20results%2C%20they%20are%20very%20slow.%20Deep%20learning%20approaches%20show%20promise%0Ain%20image%20and%20video%20restoration%20but%20have%20struggled%20to%20address%20these%0Aspatiotemporal%20variant%20distortions%20effectively.%20This%20paper%20proposes%20a%20new%0Aframework%20that%20combines%20geometric%20restoration%20with%20an%20enhancement%20module.%0ARandom%20perturbations%20and%20geometric%20distortion%20are%20removed%20using%20a%20pyramid%0Aarchitecture%20with%20deformable%203D%20convolutions%2C%20resulting%20in%20aligned%20frames.%0AThese%20frames%20are%20then%20used%20to%20reconstruct%20a%20sharp%2C%20clear%20image%20via%20a%0Amulti-scale%20architecture%20of%203D%20Swin%20Transformers.%20The%20proposed%20framework%0Ademonstrates%20superior%20performance%20over%20the%20state%20of%20the%20art%20for%20both%20synthetic%0Aand%20real%20atmospheric%20turbulence%20effects%2C%20with%20reasonable%20speed%20and%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeTurb%253A%2520Atmospheric%2520Turbulence%2520Mitigation%2520with%2520Deformable%25203D%250A%2520%2520Convolutions%2520and%25203D%2520Swin%2520Transformers%26entry.906535625%3DZhicheng%2520Zou%2520and%2520Nantheera%2520Anantrasirichai%26entry.1292438233%3D%2520%2520Atmospheric%2520turbulence%2520in%2520long-range%2520imaging%2520significantly%2520degrades%2520the%250Aquality%2520and%2520fidelity%2520of%2520captured%2520scenes%2520due%2520to%2520random%2520variations%2520in%2520both%250Aspatial%2520and%2520temporal%2520dimensions.%2520These%2520distortions%2520present%2520a%2520formidable%250Achallenge%2520across%2520various%2520applications%252C%2520from%2520surveillance%2520to%2520astronomy%252C%250Anecessitating%2520robust%2520mitigation%2520strategies.%2520While%2520model-based%2520approaches%250Aachieve%2520good%2520results%252C%2520they%2520are%2520very%2520slow.%2520Deep%2520learning%2520approaches%2520show%2520promise%250Ain%2520image%2520and%2520video%2520restoration%2520but%2520have%2520struggled%2520to%2520address%2520these%250Aspatiotemporal%2520variant%2520distortions%2520effectively.%2520This%2520paper%2520proposes%2520a%2520new%250Aframework%2520that%2520combines%2520geometric%2520restoration%2520with%2520an%2520enhancement%2520module.%250ARandom%2520perturbations%2520and%2520geometric%2520distortion%2520are%2520removed%2520using%2520a%2520pyramid%250Aarchitecture%2520with%2520deformable%25203D%2520convolutions%252C%2520resulting%2520in%2520aligned%2520frames.%250AThese%2520frames%2520are%2520then%2520used%2520to%2520reconstruct%2520a%2520sharp%252C%2520clear%2520image%2520via%2520a%250Amulti-scale%2520architecture%2520of%25203D%2520Swin%2520Transformers.%2520The%2520proposed%2520framework%250Ademonstrates%2520superior%2520performance%2520over%2520the%2520state%2520of%2520the%2520art%2520for%2520both%2520synthetic%250Aand%2520real%2520atmospheric%2520turbulence%2520effects%252C%2520with%2520reasonable%2520speed%2520and%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeTurb%3A%20Atmospheric%20Turbulence%20Mitigation%20with%20Deformable%203D%0A%20%20Convolutions%20and%203D%20Swin%20Transformers&entry.906535625=Zhicheng%20Zou%20and%20Nantheera%20Anantrasirichai&entry.1292438233=%20%20Atmospheric%20turbulence%20in%20long-range%20imaging%20significantly%20degrades%20the%0Aquality%20and%20fidelity%20of%20captured%20scenes%20due%20to%20random%20variations%20in%20both%0Aspatial%20and%20temporal%20dimensions.%20These%20distortions%20present%20a%20formidable%0Achallenge%20across%20various%20applications%2C%20from%20surveillance%20to%20astronomy%2C%0Anecessitating%20robust%20mitigation%20strategies.%20While%20model-based%20approaches%0Aachieve%20good%20results%2C%20they%20are%20very%20slow.%20Deep%20learning%20approaches%20show%20promise%0Ain%20image%20and%20video%20restoration%20but%20have%20struggled%20to%20address%20these%0Aspatiotemporal%20variant%20distortions%20effectively.%20This%20paper%20proposes%20a%20new%0Aframework%20that%20combines%20geometric%20restoration%20with%20an%20enhancement%20module.%0ARandom%20perturbations%20and%20geometric%20distortion%20are%20removed%20using%20a%20pyramid%0Aarchitecture%20with%20deformable%203D%20convolutions%2C%20resulting%20in%20aligned%20frames.%0AThese%20frames%20are%20then%20used%20to%20reconstruct%20a%20sharp%2C%20clear%20image%20via%20a%0Amulti-scale%20architecture%20of%203D%20Swin%20Transformers.%20The%20proposed%20framework%0Ademonstrates%20superior%20performance%20over%20the%20state%20of%20the%20art%20for%20both%20synthetic%0Aand%20real%20atmospheric%20turbulence%20effects%2C%20with%20reasonable%20speed%20and%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20855v1&entry.124074799=Read"},
{"title": "WARM-3D: A Weakly-Supervised Sim2Real Domain Adaptation Framework for\n  Roadside Monocular 3D Object Detection", "author": "Xingcheng Zhou and Deyu Fu and Walter Zimmer and Mingyu Liu and Venkatnarayanan Lakshminarasimhan and Leah Strand and Alois C. Knoll", "abstract": "  Existing roadside perception systems are limited by the absence of publicly\navailable, large-scale, high-quality 3D datasets. Exploring the use of\ncost-effective, extensive synthetic datasets offers a viable solution to tackle\nthis challenge and enhance the performance of roadside monocular 3D detection.\nIn this study, we introduce the TUMTraf Synthetic Dataset, offering a diverse\nand substantial collection of high-quality 3D data to augment scarce real-world\ndatasets. Besides, we present WARM-3D, a concise yet effective framework to aid\nthe Sim2Real domain transfer for roadside monocular 3D detection. Our method\nleverages cheap synthetic datasets and 2D labels from an off-the-shelf 2D\ndetector for weak supervision. We show that WARM-3D significantly enhances\nperformance, achieving a +12.40% increase in mAP 3D over the baseline with only\npseudo-2D supervision. With 2D GT as weak labels, WARM-3D even reaches\nperformance close to the Oracle baseline. Moreover, WARM-3D improves the\nability of 3D detectors to unseen sample recognition across various real-world\nenvironments, highlighting its potential for practical applications.\n", "link": "http://arxiv.org/abs/2407.20818v1", "date": "2024-07-30", "relevancy": 2.4005, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6202}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.589}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WARM-3D%3A%20A%20Weakly-Supervised%20Sim2Real%20Domain%20Adaptation%20Framework%20for%0A%20%20Roadside%20Monocular%203D%20Object%20Detection&body=Title%3A%20WARM-3D%3A%20A%20Weakly-Supervised%20Sim2Real%20Domain%20Adaptation%20Framework%20for%0A%20%20Roadside%20Monocular%203D%20Object%20Detection%0AAuthor%3A%20Xingcheng%20Zhou%20and%20Deyu%20Fu%20and%20Walter%20Zimmer%20and%20Mingyu%20Liu%20and%20Venkatnarayanan%20Lakshminarasimhan%20and%20Leah%20Strand%20and%20Alois%20C.%20Knoll%0AAbstract%3A%20%20%20Existing%20roadside%20perception%20systems%20are%20limited%20by%20the%20absence%20of%20publicly%0Aavailable%2C%20large-scale%2C%20high-quality%203D%20datasets.%20Exploring%20the%20use%20of%0Acost-effective%2C%20extensive%20synthetic%20datasets%20offers%20a%20viable%20solution%20to%20tackle%0Athis%20challenge%20and%20enhance%20the%20performance%20of%20roadside%20monocular%203D%20detection.%0AIn%20this%20study%2C%20we%20introduce%20the%20TUMTraf%20Synthetic%20Dataset%2C%20offering%20a%20diverse%0Aand%20substantial%20collection%20of%20high-quality%203D%20data%20to%20augment%20scarce%20real-world%0Adatasets.%20Besides%2C%20we%20present%20WARM-3D%2C%20a%20concise%20yet%20effective%20framework%20to%20aid%0Athe%20Sim2Real%20domain%20transfer%20for%20roadside%20monocular%203D%20detection.%20Our%20method%0Aleverages%20cheap%20synthetic%20datasets%20and%202D%20labels%20from%20an%20off-the-shelf%202D%0Adetector%20for%20weak%20supervision.%20We%20show%20that%20WARM-3D%20significantly%20enhances%0Aperformance%2C%20achieving%20a%20%2B12.40%25%20increase%20in%20mAP%203D%20over%20the%20baseline%20with%20only%0Apseudo-2D%20supervision.%20With%202D%20GT%20as%20weak%20labels%2C%20WARM-3D%20even%20reaches%0Aperformance%20close%20to%20the%20Oracle%20baseline.%20Moreover%2C%20WARM-3D%20improves%20the%0Aability%20of%203D%20detectors%20to%20unseen%20sample%20recognition%20across%20various%20real-world%0Aenvironments%2C%20highlighting%20its%20potential%20for%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWARM-3D%253A%2520A%2520Weakly-Supervised%2520Sim2Real%2520Domain%2520Adaptation%2520Framework%2520for%250A%2520%2520Roadside%2520Monocular%25203D%2520Object%2520Detection%26entry.906535625%3DXingcheng%2520Zhou%2520and%2520Deyu%2520Fu%2520and%2520Walter%2520Zimmer%2520and%2520Mingyu%2520Liu%2520and%2520Venkatnarayanan%2520Lakshminarasimhan%2520and%2520Leah%2520Strand%2520and%2520Alois%2520C.%2520Knoll%26entry.1292438233%3D%2520%2520Existing%2520roadside%2520perception%2520systems%2520are%2520limited%2520by%2520the%2520absence%2520of%2520publicly%250Aavailable%252C%2520large-scale%252C%2520high-quality%25203D%2520datasets.%2520Exploring%2520the%2520use%2520of%250Acost-effective%252C%2520extensive%2520synthetic%2520datasets%2520offers%2520a%2520viable%2520solution%2520to%2520tackle%250Athis%2520challenge%2520and%2520enhance%2520the%2520performance%2520of%2520roadside%2520monocular%25203D%2520detection.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520the%2520TUMTraf%2520Synthetic%2520Dataset%252C%2520offering%2520a%2520diverse%250Aand%2520substantial%2520collection%2520of%2520high-quality%25203D%2520data%2520to%2520augment%2520scarce%2520real-world%250Adatasets.%2520Besides%252C%2520we%2520present%2520WARM-3D%252C%2520a%2520concise%2520yet%2520effective%2520framework%2520to%2520aid%250Athe%2520Sim2Real%2520domain%2520transfer%2520for%2520roadside%2520monocular%25203D%2520detection.%2520Our%2520method%250Aleverages%2520cheap%2520synthetic%2520datasets%2520and%25202D%2520labels%2520from%2520an%2520off-the-shelf%25202D%250Adetector%2520for%2520weak%2520supervision.%2520We%2520show%2520that%2520WARM-3D%2520significantly%2520enhances%250Aperformance%252C%2520achieving%2520a%2520%252B12.40%2525%2520increase%2520in%2520mAP%25203D%2520over%2520the%2520baseline%2520with%2520only%250Apseudo-2D%2520supervision.%2520With%25202D%2520GT%2520as%2520weak%2520labels%252C%2520WARM-3D%2520even%2520reaches%250Aperformance%2520close%2520to%2520the%2520Oracle%2520baseline.%2520Moreover%252C%2520WARM-3D%2520improves%2520the%250Aability%2520of%25203D%2520detectors%2520to%2520unseen%2520sample%2520recognition%2520across%2520various%2520real-world%250Aenvironments%252C%2520highlighting%2520its%2520potential%2520for%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WARM-3D%3A%20A%20Weakly-Supervised%20Sim2Real%20Domain%20Adaptation%20Framework%20for%0A%20%20Roadside%20Monocular%203D%20Object%20Detection&entry.906535625=Xingcheng%20Zhou%20and%20Deyu%20Fu%20and%20Walter%20Zimmer%20and%20Mingyu%20Liu%20and%20Venkatnarayanan%20Lakshminarasimhan%20and%20Leah%20Strand%20and%20Alois%20C.%20Knoll&entry.1292438233=%20%20Existing%20roadside%20perception%20systems%20are%20limited%20by%20the%20absence%20of%20publicly%0Aavailable%2C%20large-scale%2C%20high-quality%203D%20datasets.%20Exploring%20the%20use%20of%0Acost-effective%2C%20extensive%20synthetic%20datasets%20offers%20a%20viable%20solution%20to%20tackle%0Athis%20challenge%20and%20enhance%20the%20performance%20of%20roadside%20monocular%203D%20detection.%0AIn%20this%20study%2C%20we%20introduce%20the%20TUMTraf%20Synthetic%20Dataset%2C%20offering%20a%20diverse%0Aand%20substantial%20collection%20of%20high-quality%203D%20data%20to%20augment%20scarce%20real-world%0Adatasets.%20Besides%2C%20we%20present%20WARM-3D%2C%20a%20concise%20yet%20effective%20framework%20to%20aid%0Athe%20Sim2Real%20domain%20transfer%20for%20roadside%20monocular%203D%20detection.%20Our%20method%0Aleverages%20cheap%20synthetic%20datasets%20and%202D%20labels%20from%20an%20off-the-shelf%202D%0Adetector%20for%20weak%20supervision.%20We%20show%20that%20WARM-3D%20significantly%20enhances%0Aperformance%2C%20achieving%20a%20%2B12.40%25%20increase%20in%20mAP%203D%20over%20the%20baseline%20with%20only%0Apseudo-2D%20supervision.%20With%202D%20GT%20as%20weak%20labels%2C%20WARM-3D%20even%20reaches%0Aperformance%20close%20to%20the%20Oracle%20baseline.%20Moreover%2C%20WARM-3D%20improves%20the%0Aability%20of%203D%20detectors%20to%20unseen%20sample%20recognition%20across%20various%20real-world%0Aenvironments%2C%20highlighting%20its%20potential%20for%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20818v1&entry.124074799=Read"},
{"title": "Mean of Means: A 10-dollar Solution for Human Localization with\n  Calibration-free and Unconstrained Camera Settings", "author": "Tianyi Zhang and Wengyu Zhang and Xulu Zhang and Jiaxin Wu and Xiao-Yong Wei and Jiannong Cao and Qing Li", "abstract": "  Accurate human localization is crucial for various applications, especially\nin the Metaverse era. Existing high precision solutions rely on expensive,\ntag-dependent hardware, while vision-based methods offer a cheaper, tag-free\nalternative. However, current vision solutions based on stereo vision face\nlimitations due to rigid perspective transformation principles and error\npropagation in multi-stage SVD solvers. These solutions also require multiple\nhigh-resolution cameras with strict setup constraints. To address these\nlimitations, we propose a probabilistic approach that considers all points on\nthe human body as observations generated by a distribution centered around the\nbody's geometric center. This enables us to improve sampling significantly,\nincreasing the number of samples for each point of interest from hundreds to\nbillions. By modeling the relation between the means of the distributions of\nworld coordinates and pixel coordinates, leveraging the Central Limit Theorem,\nwe ensure normality and facilitate the learning process. Experimental results\ndemonstrate human localization accuracy of 95% within a 0.3m range and nearly\n100% accuracy within a 0.5m range, achieved at a low cost of only 10 USD using\ntwo web cameras with a resolution of 640x480 pixels.\n", "link": "http://arxiv.org/abs/2407.20870v1", "date": "2024-07-30", "relevancy": 2.3977, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6042}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mean%20of%20Means%3A%20A%2010-dollar%20Solution%20for%20Human%20Localization%20with%0A%20%20Calibration-free%20and%20Unconstrained%20Camera%20Settings&body=Title%3A%20Mean%20of%20Means%3A%20A%2010-dollar%20Solution%20for%20Human%20Localization%20with%0A%20%20Calibration-free%20and%20Unconstrained%20Camera%20Settings%0AAuthor%3A%20Tianyi%20Zhang%20and%20Wengyu%20Zhang%20and%20Xulu%20Zhang%20and%20Jiaxin%20Wu%20and%20Xiao-Yong%20Wei%20and%20Jiannong%20Cao%20and%20Qing%20Li%0AAbstract%3A%20%20%20Accurate%20human%20localization%20is%20crucial%20for%20various%20applications%2C%20especially%0Ain%20the%20Metaverse%20era.%20Existing%20high%20precision%20solutions%20rely%20on%20expensive%2C%0Atag-dependent%20hardware%2C%20while%20vision-based%20methods%20offer%20a%20cheaper%2C%20tag-free%0Aalternative.%20However%2C%20current%20vision%20solutions%20based%20on%20stereo%20vision%20face%0Alimitations%20due%20to%20rigid%20perspective%20transformation%20principles%20and%20error%0Apropagation%20in%20multi-stage%20SVD%20solvers.%20These%20solutions%20also%20require%20multiple%0Ahigh-resolution%20cameras%20with%20strict%20setup%20constraints.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20probabilistic%20approach%20that%20considers%20all%20points%20on%0Athe%20human%20body%20as%20observations%20generated%20by%20a%20distribution%20centered%20around%20the%0Abody%27s%20geometric%20center.%20This%20enables%20us%20to%20improve%20sampling%20significantly%2C%0Aincreasing%20the%20number%20of%20samples%20for%20each%20point%20of%20interest%20from%20hundreds%20to%0Abillions.%20By%20modeling%20the%20relation%20between%20the%20means%20of%20the%20distributions%20of%0Aworld%20coordinates%20and%20pixel%20coordinates%2C%20leveraging%20the%20Central%20Limit%20Theorem%2C%0Awe%20ensure%20normality%20and%20facilitate%20the%20learning%20process.%20Experimental%20results%0Ademonstrate%20human%20localization%20accuracy%20of%2095%25%20within%20a%200.3m%20range%20and%20nearly%0A100%25%20accuracy%20within%20a%200.5m%20range%2C%20achieved%20at%20a%20low%20cost%20of%20only%2010%20USD%20using%0Atwo%20web%20cameras%20with%20a%20resolution%20of%20640x480%20pixels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMean%2520of%2520Means%253A%2520A%252010-dollar%2520Solution%2520for%2520Human%2520Localization%2520with%250A%2520%2520Calibration-free%2520and%2520Unconstrained%2520Camera%2520Settings%26entry.906535625%3DTianyi%2520Zhang%2520and%2520Wengyu%2520Zhang%2520and%2520Xulu%2520Zhang%2520and%2520Jiaxin%2520Wu%2520and%2520Xiao-Yong%2520Wei%2520and%2520Jiannong%2520Cao%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Accurate%2520human%2520localization%2520is%2520crucial%2520for%2520various%2520applications%252C%2520especially%250Ain%2520the%2520Metaverse%2520era.%2520Existing%2520high%2520precision%2520solutions%2520rely%2520on%2520expensive%252C%250Atag-dependent%2520hardware%252C%2520while%2520vision-based%2520methods%2520offer%2520a%2520cheaper%252C%2520tag-free%250Aalternative.%2520However%252C%2520current%2520vision%2520solutions%2520based%2520on%2520stereo%2520vision%2520face%250Alimitations%2520due%2520to%2520rigid%2520perspective%2520transformation%2520principles%2520and%2520error%250Apropagation%2520in%2520multi-stage%2520SVD%2520solvers.%2520These%2520solutions%2520also%2520require%2520multiple%250Ahigh-resolution%2520cameras%2520with%2520strict%2520setup%2520constraints.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520probabilistic%2520approach%2520that%2520considers%2520all%2520points%2520on%250Athe%2520human%2520body%2520as%2520observations%2520generated%2520by%2520a%2520distribution%2520centered%2520around%2520the%250Abody%2527s%2520geometric%2520center.%2520This%2520enables%2520us%2520to%2520improve%2520sampling%2520significantly%252C%250Aincreasing%2520the%2520number%2520of%2520samples%2520for%2520each%2520point%2520of%2520interest%2520from%2520hundreds%2520to%250Abillions.%2520By%2520modeling%2520the%2520relation%2520between%2520the%2520means%2520of%2520the%2520distributions%2520of%250Aworld%2520coordinates%2520and%2520pixel%2520coordinates%252C%2520leveraging%2520the%2520Central%2520Limit%2520Theorem%252C%250Awe%2520ensure%2520normality%2520and%2520facilitate%2520the%2520learning%2520process.%2520Experimental%2520results%250Ademonstrate%2520human%2520localization%2520accuracy%2520of%252095%2525%2520within%2520a%25200.3m%2520range%2520and%2520nearly%250A100%2525%2520accuracy%2520within%2520a%25200.5m%2520range%252C%2520achieved%2520at%2520a%2520low%2520cost%2520of%2520only%252010%2520USD%2520using%250Atwo%2520web%2520cameras%2520with%2520a%2520resolution%2520of%2520640x480%2520pixels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mean%20of%20Means%3A%20A%2010-dollar%20Solution%20for%20Human%20Localization%20with%0A%20%20Calibration-free%20and%20Unconstrained%20Camera%20Settings&entry.906535625=Tianyi%20Zhang%20and%20Wengyu%20Zhang%20and%20Xulu%20Zhang%20and%20Jiaxin%20Wu%20and%20Xiao-Yong%20Wei%20and%20Jiannong%20Cao%20and%20Qing%20Li&entry.1292438233=%20%20Accurate%20human%20localization%20is%20crucial%20for%20various%20applications%2C%20especially%0Ain%20the%20Metaverse%20era.%20Existing%20high%20precision%20solutions%20rely%20on%20expensive%2C%0Atag-dependent%20hardware%2C%20while%20vision-based%20methods%20offer%20a%20cheaper%2C%20tag-free%0Aalternative.%20However%2C%20current%20vision%20solutions%20based%20on%20stereo%20vision%20face%0Alimitations%20due%20to%20rigid%20perspective%20transformation%20principles%20and%20error%0Apropagation%20in%20multi-stage%20SVD%20solvers.%20These%20solutions%20also%20require%20multiple%0Ahigh-resolution%20cameras%20with%20strict%20setup%20constraints.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20probabilistic%20approach%20that%20considers%20all%20points%20on%0Athe%20human%20body%20as%20observations%20generated%20by%20a%20distribution%20centered%20around%20the%0Abody%27s%20geometric%20center.%20This%20enables%20us%20to%20improve%20sampling%20significantly%2C%0Aincreasing%20the%20number%20of%20samples%20for%20each%20point%20of%20interest%20from%20hundreds%20to%0Abillions.%20By%20modeling%20the%20relation%20between%20the%20means%20of%20the%20distributions%20of%0Aworld%20coordinates%20and%20pixel%20coordinates%2C%20leveraging%20the%20Central%20Limit%20Theorem%2C%0Awe%20ensure%20normality%20and%20facilitate%20the%20learning%20process.%20Experimental%20results%0Ademonstrate%20human%20localization%20accuracy%20of%2095%25%20within%20a%200.3m%20range%20and%20nearly%0A100%25%20accuracy%20within%20a%200.5m%20range%2C%20achieved%20at%20a%20low%20cost%20of%20only%2010%20USD%20using%0Atwo%20web%20cameras%20with%20a%20resolution%20of%20640x480%20pixels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20870v1&entry.124074799=Read"},
{"title": "SSPA: Split-and-Synthesize Prompting with Gated Alignments for\n  Multi-Label Image Recognition", "author": "Hao Tan and Zichang Tan and Jun Li and Jun Wan and Zhen Lei and Stan Z. Li", "abstract": "  Multi-label image recognition is a fundamental task in computer vision.\nRecently, Vision-Language Models (VLMs) have made notable advancements in this\narea. However, previous methods fail to effectively leverage the rich knowledge\nin language models and often incorporate label semantics into visual features\nunidirectionally. To overcome these problems, we propose a Split-and-Synthesize\nPrompting with Gated Alignments (SSPA) framework to amplify the potential of\nVLMs. Specifically, we develop an in-context learning approach to associate the\ninherent knowledge from LLMs. Then we propose a novel Split-and-Synthesize\nPrompting (SSP) strategy to first model the generic knowledge and downstream\nlabel semantics individually and then aggregate them carefully through the\nquaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) to\nbidirectionally interact visual and linguistic modalities while eliminating\nredundant cross-modal information, enabling more efficient region-level\nalignments. Rather than making the final prediction by a sharp manner in\nprevious works, we propose a soft aggregator to jointly consider results from\nall image regions. With the help of flexible prompting and gated alignments,\nSSPA is generalizable to specific domains. Extensive experiments on nine\ndatasets from three domains (i.e., natural, pedestrian attributes and remote\nsensing) demonstrate the state-of-the-art performance of SSPA. Further analyses\nverify the effectiveness of SSP and the interpretability of GDMA. The code will\nbe made public.\n", "link": "http://arxiv.org/abs/2407.20920v1", "date": "2024-07-30", "relevancy": 2.3795, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6173}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSPA%3A%20Split-and-Synthesize%20Prompting%20with%20Gated%20Alignments%20for%0A%20%20Multi-Label%20Image%20Recognition&body=Title%3A%20SSPA%3A%20Split-and-Synthesize%20Prompting%20with%20Gated%20Alignments%20for%0A%20%20Multi-Label%20Image%20Recognition%0AAuthor%3A%20Hao%20Tan%20and%20Zichang%20Tan%20and%20Jun%20Li%20and%20Jun%20Wan%20and%20Zhen%20Lei%20and%20Stan%20Z.%20Li%0AAbstract%3A%20%20%20Multi-label%20image%20recognition%20is%20a%20fundamental%20task%20in%20computer%20vision.%0ARecently%2C%20Vision-Language%20Models%20%28VLMs%29%20have%20made%20notable%20advancements%20in%20this%0Aarea.%20However%2C%20previous%20methods%20fail%20to%20effectively%20leverage%20the%20rich%20knowledge%0Ain%20language%20models%20and%20often%20incorporate%20label%20semantics%20into%20visual%20features%0Aunidirectionally.%20To%20overcome%20these%20problems%2C%20we%20propose%20a%20Split-and-Synthesize%0APrompting%20with%20Gated%20Alignments%20%28SSPA%29%20framework%20to%20amplify%20the%20potential%20of%0AVLMs.%20Specifically%2C%20we%20develop%20an%20in-context%20learning%20approach%20to%20associate%20the%0Ainherent%20knowledge%20from%20LLMs.%20Then%20we%20propose%20a%20novel%20Split-and-Synthesize%0APrompting%20%28SSP%29%20strategy%20to%20first%20model%20the%20generic%20knowledge%20and%20downstream%0Alabel%20semantics%20individually%20and%20then%20aggregate%20them%20carefully%20through%20the%0Aquaternion%20network.%20Moreover%2C%20we%20present%20Gated%20Dual-Modal%20Alignments%20%28GDMA%29%20to%0Abidirectionally%20interact%20visual%20and%20linguistic%20modalities%20while%20eliminating%0Aredundant%20cross-modal%20information%2C%20enabling%20more%20efficient%20region-level%0Aalignments.%20Rather%20than%20making%20the%20final%20prediction%20by%20a%20sharp%20manner%20in%0Aprevious%20works%2C%20we%20propose%20a%20soft%20aggregator%20to%20jointly%20consider%20results%20from%0Aall%20image%20regions.%20With%20the%20help%20of%20flexible%20prompting%20and%20gated%20alignments%2C%0ASSPA%20is%20generalizable%20to%20specific%20domains.%20Extensive%20experiments%20on%20nine%0Adatasets%20from%20three%20domains%20%28i.e.%2C%20natural%2C%20pedestrian%20attributes%20and%20remote%0Asensing%29%20demonstrate%20the%20state-of-the-art%20performance%20of%20SSPA.%20Further%20analyses%0Averify%20the%20effectiveness%20of%20SSP%20and%20the%20interpretability%20of%20GDMA.%20The%20code%20will%0Abe%20made%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSPA%253A%2520Split-and-Synthesize%2520Prompting%2520with%2520Gated%2520Alignments%2520for%250A%2520%2520Multi-Label%2520Image%2520Recognition%26entry.906535625%3DHao%2520Tan%2520and%2520Zichang%2520Tan%2520and%2520Jun%2520Li%2520and%2520Jun%2520Wan%2520and%2520Zhen%2520Lei%2520and%2520Stan%2520Z.%2520Li%26entry.1292438233%3D%2520%2520Multi-label%2520image%2520recognition%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision.%250ARecently%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520made%2520notable%2520advancements%2520in%2520this%250Aarea.%2520However%252C%2520previous%2520methods%2520fail%2520to%2520effectively%2520leverage%2520the%2520rich%2520knowledge%250Ain%2520language%2520models%2520and%2520often%2520incorporate%2520label%2520semantics%2520into%2520visual%2520features%250Aunidirectionally.%2520To%2520overcome%2520these%2520problems%252C%2520we%2520propose%2520a%2520Split-and-Synthesize%250APrompting%2520with%2520Gated%2520Alignments%2520%2528SSPA%2529%2520framework%2520to%2520amplify%2520the%2520potential%2520of%250AVLMs.%2520Specifically%252C%2520we%2520develop%2520an%2520in-context%2520learning%2520approach%2520to%2520associate%2520the%250Ainherent%2520knowledge%2520from%2520LLMs.%2520Then%2520we%2520propose%2520a%2520novel%2520Split-and-Synthesize%250APrompting%2520%2528SSP%2529%2520strategy%2520to%2520first%2520model%2520the%2520generic%2520knowledge%2520and%2520downstream%250Alabel%2520semantics%2520individually%2520and%2520then%2520aggregate%2520them%2520carefully%2520through%2520the%250Aquaternion%2520network.%2520Moreover%252C%2520we%2520present%2520Gated%2520Dual-Modal%2520Alignments%2520%2528GDMA%2529%2520to%250Abidirectionally%2520interact%2520visual%2520and%2520linguistic%2520modalities%2520while%2520eliminating%250Aredundant%2520cross-modal%2520information%252C%2520enabling%2520more%2520efficient%2520region-level%250Aalignments.%2520Rather%2520than%2520making%2520the%2520final%2520prediction%2520by%2520a%2520sharp%2520manner%2520in%250Aprevious%2520works%252C%2520we%2520propose%2520a%2520soft%2520aggregator%2520to%2520jointly%2520consider%2520results%2520from%250Aall%2520image%2520regions.%2520With%2520the%2520help%2520of%2520flexible%2520prompting%2520and%2520gated%2520alignments%252C%250ASSPA%2520is%2520generalizable%2520to%2520specific%2520domains.%2520Extensive%2520experiments%2520on%2520nine%250Adatasets%2520from%2520three%2520domains%2520%2528i.e.%252C%2520natural%252C%2520pedestrian%2520attributes%2520and%2520remote%250Asensing%2529%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520SSPA.%2520Further%2520analyses%250Averify%2520the%2520effectiveness%2520of%2520SSP%2520and%2520the%2520interpretability%2520of%2520GDMA.%2520The%2520code%2520will%250Abe%2520made%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSPA%3A%20Split-and-Synthesize%20Prompting%20with%20Gated%20Alignments%20for%0A%20%20Multi-Label%20Image%20Recognition&entry.906535625=Hao%20Tan%20and%20Zichang%20Tan%20and%20Jun%20Li%20and%20Jun%20Wan%20and%20Zhen%20Lei%20and%20Stan%20Z.%20Li&entry.1292438233=%20%20Multi-label%20image%20recognition%20is%20a%20fundamental%20task%20in%20computer%20vision.%0ARecently%2C%20Vision-Language%20Models%20%28VLMs%29%20have%20made%20notable%20advancements%20in%20this%0Aarea.%20However%2C%20previous%20methods%20fail%20to%20effectively%20leverage%20the%20rich%20knowledge%0Ain%20language%20models%20and%20often%20incorporate%20label%20semantics%20into%20visual%20features%0Aunidirectionally.%20To%20overcome%20these%20problems%2C%20we%20propose%20a%20Split-and-Synthesize%0APrompting%20with%20Gated%20Alignments%20%28SSPA%29%20framework%20to%20amplify%20the%20potential%20of%0AVLMs.%20Specifically%2C%20we%20develop%20an%20in-context%20learning%20approach%20to%20associate%20the%0Ainherent%20knowledge%20from%20LLMs.%20Then%20we%20propose%20a%20novel%20Split-and-Synthesize%0APrompting%20%28SSP%29%20strategy%20to%20first%20model%20the%20generic%20knowledge%20and%20downstream%0Alabel%20semantics%20individually%20and%20then%20aggregate%20them%20carefully%20through%20the%0Aquaternion%20network.%20Moreover%2C%20we%20present%20Gated%20Dual-Modal%20Alignments%20%28GDMA%29%20to%0Abidirectionally%20interact%20visual%20and%20linguistic%20modalities%20while%20eliminating%0Aredundant%20cross-modal%20information%2C%20enabling%20more%20efficient%20region-level%0Aalignments.%20Rather%20than%20making%20the%20final%20prediction%20by%20a%20sharp%20manner%20in%0Aprevious%20works%2C%20we%20propose%20a%20soft%20aggregator%20to%20jointly%20consider%20results%20from%0Aall%20image%20regions.%20With%20the%20help%20of%20flexible%20prompting%20and%20gated%20alignments%2C%0ASSPA%20is%20generalizable%20to%20specific%20domains.%20Extensive%20experiments%20on%20nine%0Adatasets%20from%20three%20domains%20%28i.e.%2C%20natural%2C%20pedestrian%20attributes%20and%20remote%0Asensing%29%20demonstrate%20the%20state-of-the-art%20performance%20of%20SSPA.%20Further%20analyses%0Averify%20the%20effectiveness%20of%20SSP%20and%20the%20interpretability%20of%20GDMA.%20The%20code%20will%0Abe%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20920v1&entry.124074799=Read"},
{"title": "WidthFormer: Toward Efficient Transformer-based BEV View Transformation", "author": "Chenhongyi Yang and Tianwei Lin and Lichao Huang and Elliot J. Crowley", "abstract": "  We present WidthFormer, a novel transformer-based module to compute\nBird's-Eye-View (BEV) representations from multi-view cameras for real-time\nautonomous-driving applications. WidthFormer is computationally efficient,\nrobust and does not require any special engineering effort to deploy. We first\nintroduce a novel 3D positional encoding mechanism capable of accurately\nencapsulating 3D geometric information, which enables our model to compute\nhigh-quality BEV representations with only a single transformer decoder layer.\nThis mechanism is also beneficial for existing sparse 3D object detectors.\nInspired by the recently proposed works, we further improve our model's\nefficiency by vertically compressing the image features when serving as\nattention keys and values, and then we develop two modules to compensate for\npotential information loss due to feature compression. Experimental evaluation\non the widely-used nuScenes 3D object detection benchmark demonstrates that our\nmethod outperforms previous approaches across different 3D detection\narchitectures. More importantly, our model is highly efficient. For example,\nwhen using $256\\times 704$ input images, it achieves 1.5 ms and 2.8 ms latency\non NVIDIA 3090 GPU and Horizon Journey-5 computation solutions. Furthermore,\nWidthFormer also exhibits strong robustness to different degrees of camera\nperturbations. Our study offers valuable insights into the deployment of BEV\ntransformation methods in real-world, complex road environments. Code is\navailable at https://github.com/ChenhongyiYang/WidthFormer .\n", "link": "http://arxiv.org/abs/2401.03836v5", "date": "2024-07-30", "relevancy": 2.2934, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5773}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5773}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WidthFormer%3A%20Toward%20Efficient%20Transformer-based%20BEV%20View%20Transformation&body=Title%3A%20WidthFormer%3A%20Toward%20Efficient%20Transformer-based%20BEV%20View%20Transformation%0AAuthor%3A%20Chenhongyi%20Yang%20and%20Tianwei%20Lin%20and%20Lichao%20Huang%20and%20Elliot%20J.%20Crowley%0AAbstract%3A%20%20%20We%20present%20WidthFormer%2C%20a%20novel%20transformer-based%20module%20to%20compute%0ABird%27s-Eye-View%20%28BEV%29%20representations%20from%20multi-view%20cameras%20for%20real-time%0Aautonomous-driving%20applications.%20WidthFormer%20is%20computationally%20efficient%2C%0Arobust%20and%20does%20not%20require%20any%20special%20engineering%20effort%20to%20deploy.%20We%20first%0Aintroduce%20a%20novel%203D%20positional%20encoding%20mechanism%20capable%20of%20accurately%0Aencapsulating%203D%20geometric%20information%2C%20which%20enables%20our%20model%20to%20compute%0Ahigh-quality%20BEV%20representations%20with%20only%20a%20single%20transformer%20decoder%20layer.%0AThis%20mechanism%20is%20also%20beneficial%20for%20existing%20sparse%203D%20object%20detectors.%0AInspired%20by%20the%20recently%20proposed%20works%2C%20we%20further%20improve%20our%20model%27s%0Aefficiency%20by%20vertically%20compressing%20the%20image%20features%20when%20serving%20as%0Aattention%20keys%20and%20values%2C%20and%20then%20we%20develop%20two%20modules%20to%20compensate%20for%0Apotential%20information%20loss%20due%20to%20feature%20compression.%20Experimental%20evaluation%0Aon%20the%20widely-used%20nuScenes%203D%20object%20detection%20benchmark%20demonstrates%20that%20our%0Amethod%20outperforms%20previous%20approaches%20across%20different%203D%20detection%0Aarchitectures.%20More%20importantly%2C%20our%20model%20is%20highly%20efficient.%20For%20example%2C%0Awhen%20using%20%24256%5Ctimes%20704%24%20input%20images%2C%20it%20achieves%201.5%20ms%20and%202.8%20ms%20latency%0Aon%20NVIDIA%203090%20GPU%20and%20Horizon%20Journey-5%20computation%20solutions.%20Furthermore%2C%0AWidthFormer%20also%20exhibits%20strong%20robustness%20to%20different%20degrees%20of%20camera%0Aperturbations.%20Our%20study%20offers%20valuable%20insights%20into%20the%20deployment%20of%20BEV%0Atransformation%20methods%20in%20real-world%2C%20complex%20road%20environments.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ChenhongyiYang/WidthFormer%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03836v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWidthFormer%253A%2520Toward%2520Efficient%2520Transformer-based%2520BEV%2520View%2520Transformation%26entry.906535625%3DChenhongyi%2520Yang%2520and%2520Tianwei%2520Lin%2520and%2520Lichao%2520Huang%2520and%2520Elliot%2520J.%2520Crowley%26entry.1292438233%3D%2520%2520We%2520present%2520WidthFormer%252C%2520a%2520novel%2520transformer-based%2520module%2520to%2520compute%250ABird%2527s-Eye-View%2520%2528BEV%2529%2520representations%2520from%2520multi-view%2520cameras%2520for%2520real-time%250Aautonomous-driving%2520applications.%2520WidthFormer%2520is%2520computationally%2520efficient%252C%250Arobust%2520and%2520does%2520not%2520require%2520any%2520special%2520engineering%2520effort%2520to%2520deploy.%2520We%2520first%250Aintroduce%2520a%2520novel%25203D%2520positional%2520encoding%2520mechanism%2520capable%2520of%2520accurately%250Aencapsulating%25203D%2520geometric%2520information%252C%2520which%2520enables%2520our%2520model%2520to%2520compute%250Ahigh-quality%2520BEV%2520representations%2520with%2520only%2520a%2520single%2520transformer%2520decoder%2520layer.%250AThis%2520mechanism%2520is%2520also%2520beneficial%2520for%2520existing%2520sparse%25203D%2520object%2520detectors.%250AInspired%2520by%2520the%2520recently%2520proposed%2520works%252C%2520we%2520further%2520improve%2520our%2520model%2527s%250Aefficiency%2520by%2520vertically%2520compressing%2520the%2520image%2520features%2520when%2520serving%2520as%250Aattention%2520keys%2520and%2520values%252C%2520and%2520then%2520we%2520develop%2520two%2520modules%2520to%2520compensate%2520for%250Apotential%2520information%2520loss%2520due%2520to%2520feature%2520compression.%2520Experimental%2520evaluation%250Aon%2520the%2520widely-used%2520nuScenes%25203D%2520object%2520detection%2520benchmark%2520demonstrates%2520that%2520our%250Amethod%2520outperforms%2520previous%2520approaches%2520across%2520different%25203D%2520detection%250Aarchitectures.%2520More%2520importantly%252C%2520our%2520model%2520is%2520highly%2520efficient.%2520For%2520example%252C%250Awhen%2520using%2520%2524256%255Ctimes%2520704%2524%2520input%2520images%252C%2520it%2520achieves%25201.5%2520ms%2520and%25202.8%2520ms%2520latency%250Aon%2520NVIDIA%25203090%2520GPU%2520and%2520Horizon%2520Journey-5%2520computation%2520solutions.%2520Furthermore%252C%250AWidthFormer%2520also%2520exhibits%2520strong%2520robustness%2520to%2520different%2520degrees%2520of%2520camera%250Aperturbations.%2520Our%2520study%2520offers%2520valuable%2520insights%2520into%2520the%2520deployment%2520of%2520BEV%250Atransformation%2520methods%2520in%2520real-world%252C%2520complex%2520road%2520environments.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/ChenhongyiYang/WidthFormer%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03836v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WidthFormer%3A%20Toward%20Efficient%20Transformer-based%20BEV%20View%20Transformation&entry.906535625=Chenhongyi%20Yang%20and%20Tianwei%20Lin%20and%20Lichao%20Huang%20and%20Elliot%20J.%20Crowley&entry.1292438233=%20%20We%20present%20WidthFormer%2C%20a%20novel%20transformer-based%20module%20to%20compute%0ABird%27s-Eye-View%20%28BEV%29%20representations%20from%20multi-view%20cameras%20for%20real-time%0Aautonomous-driving%20applications.%20WidthFormer%20is%20computationally%20efficient%2C%0Arobust%20and%20does%20not%20require%20any%20special%20engineering%20effort%20to%20deploy.%20We%20first%0Aintroduce%20a%20novel%203D%20positional%20encoding%20mechanism%20capable%20of%20accurately%0Aencapsulating%203D%20geometric%20information%2C%20which%20enables%20our%20model%20to%20compute%0Ahigh-quality%20BEV%20representations%20with%20only%20a%20single%20transformer%20decoder%20layer.%0AThis%20mechanism%20is%20also%20beneficial%20for%20existing%20sparse%203D%20object%20detectors.%0AInspired%20by%20the%20recently%20proposed%20works%2C%20we%20further%20improve%20our%20model%27s%0Aefficiency%20by%20vertically%20compressing%20the%20image%20features%20when%20serving%20as%0Aattention%20keys%20and%20values%2C%20and%20then%20we%20develop%20two%20modules%20to%20compensate%20for%0Apotential%20information%20loss%20due%20to%20feature%20compression.%20Experimental%20evaluation%0Aon%20the%20widely-used%20nuScenes%203D%20object%20detection%20benchmark%20demonstrates%20that%20our%0Amethod%20outperforms%20previous%20approaches%20across%20different%203D%20detection%0Aarchitectures.%20More%20importantly%2C%20our%20model%20is%20highly%20efficient.%20For%20example%2C%0Awhen%20using%20%24256%5Ctimes%20704%24%20input%20images%2C%20it%20achieves%201.5%20ms%20and%202.8%20ms%20latency%0Aon%20NVIDIA%203090%20GPU%20and%20Horizon%20Journey-5%20computation%20solutions.%20Furthermore%2C%0AWidthFormer%20also%20exhibits%20strong%20robustness%20to%20different%20degrees%20of%20camera%0Aperturbations.%20Our%20study%20offers%20valuable%20insights%20into%20the%20deployment%20of%20BEV%0Atransformation%20methods%20in%20real-world%2C%20complex%20road%20environments.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ChenhongyiYang/WidthFormer%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03836v5&entry.124074799=Read"},
{"title": "Learning with Alignments: Tackling the Inter- and Intra-domain Shifts\n  for Cross-multidomain Facial Expression Recognition", "author": "Yuxiang Yang and Lu Wen and Xinyi Zeng and Yuanyuan Xu and Xi Wu and Jiliu Zhou and Yan Wang", "abstract": "  Facial Expression Recognition (FER) holds significant importance in\nhuman-computer interactions. Existing cross-domain FER methods often transfer\nknowledge solely from a single labeled source domain to an unlabeled target\ndomain, neglecting the comprehensive information across multiple sources.\nNevertheless, cross-multidomain FER (CMFER) is very challenging for (i) the\ninherent inter-domain shifts across multiple domains and (ii) the intra-domain\nshifts stemming from the ambiguous expressions and low inter-class\ndistinctions. In this paper, we propose a novel Learning with Alignments CMFER\nframework, named LA-CMFER, to handle both inter- and intra-domain shifts.\nSpecifically, LA-CMFER is constructed with a global branch and a local branch\nto extract features from the full images and local subtle expressions,\nrespectively. Based on this, LA-CMFER presents a dual-level inter-domain\nalignment method to force the model to prioritize hard-to-align samples in\nknowledge transfer at a sample level while gradually generating a\nwell-clustered feature space with the guidance of class attributes at a cluster\nlevel, thus narrowing the inter-domain shifts. To address the intra-domain\nshifts, LA-CMFER introduces a multi-view intra-domain alignment method with a\nmulti-view clustering consistency constraint where a prediction similarity\nmatrix is built to pursue consistency between the global and local views, thus\nrefining pseudo labels and eliminating latent noise. Extensive experiments on\nsix benchmark datasets have validated the superiority of our LA-CMFER.\n", "link": "http://arxiv.org/abs/2407.05688v2", "date": "2024-07-30", "relevancy": 2.2893, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5965}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5671}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Alignments%3A%20Tackling%20the%20Inter-%20and%20Intra-domain%20Shifts%0A%20%20for%20Cross-multidomain%20Facial%20Expression%20Recognition&body=Title%3A%20Learning%20with%20Alignments%3A%20Tackling%20the%20Inter-%20and%20Intra-domain%20Shifts%0A%20%20for%20Cross-multidomain%20Facial%20Expression%20Recognition%0AAuthor%3A%20Yuxiang%20Yang%20and%20Lu%20Wen%20and%20Xinyi%20Zeng%20and%20Yuanyuan%20Xu%20and%20Xi%20Wu%20and%20Jiliu%20Zhou%20and%20Yan%20Wang%0AAbstract%3A%20%20%20Facial%20Expression%20Recognition%20%28FER%29%20holds%20significant%20importance%20in%0Ahuman-computer%20interactions.%20Existing%20cross-domain%20FER%20methods%20often%20transfer%0Aknowledge%20solely%20from%20a%20single%20labeled%20source%20domain%20to%20an%20unlabeled%20target%0Adomain%2C%20neglecting%20the%20comprehensive%20information%20across%20multiple%20sources.%0ANevertheless%2C%20cross-multidomain%20FER%20%28CMFER%29%20is%20very%20challenging%20for%20%28i%29%20the%0Ainherent%20inter-domain%20shifts%20across%20multiple%20domains%20and%20%28ii%29%20the%20intra-domain%0Ashifts%20stemming%20from%20the%20ambiguous%20expressions%20and%20low%20inter-class%0Adistinctions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Learning%20with%20Alignments%20CMFER%0Aframework%2C%20named%20LA-CMFER%2C%20to%20handle%20both%20inter-%20and%20intra-domain%20shifts.%0ASpecifically%2C%20LA-CMFER%20is%20constructed%20with%20a%20global%20branch%20and%20a%20local%20branch%0Ato%20extract%20features%20from%20the%20full%20images%20and%20local%20subtle%20expressions%2C%0Arespectively.%20Based%20on%20this%2C%20LA-CMFER%20presents%20a%20dual-level%20inter-domain%0Aalignment%20method%20to%20force%20the%20model%20to%20prioritize%20hard-to-align%20samples%20in%0Aknowledge%20transfer%20at%20a%20sample%20level%20while%20gradually%20generating%20a%0Awell-clustered%20feature%20space%20with%20the%20guidance%20of%20class%20attributes%20at%20a%20cluster%0Alevel%2C%20thus%20narrowing%20the%20inter-domain%20shifts.%20To%20address%20the%20intra-domain%0Ashifts%2C%20LA-CMFER%20introduces%20a%20multi-view%20intra-domain%20alignment%20method%20with%20a%0Amulti-view%20clustering%20consistency%20constraint%20where%20a%20prediction%20similarity%0Amatrix%20is%20built%20to%20pursue%20consistency%20between%20the%20global%20and%20local%20views%2C%20thus%0Arefining%20pseudo%20labels%20and%20eliminating%20latent%20noise.%20Extensive%20experiments%20on%0Asix%20benchmark%20datasets%20have%20validated%20the%20superiority%20of%20our%20LA-CMFER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520Alignments%253A%2520Tackling%2520the%2520Inter-%2520and%2520Intra-domain%2520Shifts%250A%2520%2520for%2520Cross-multidomain%2520Facial%2520Expression%2520Recognition%26entry.906535625%3DYuxiang%2520Yang%2520and%2520Lu%2520Wen%2520and%2520Xinyi%2520Zeng%2520and%2520Yuanyuan%2520Xu%2520and%2520Xi%2520Wu%2520and%2520Jiliu%2520Zhou%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520Facial%2520Expression%2520Recognition%2520%2528FER%2529%2520holds%2520significant%2520importance%2520in%250Ahuman-computer%2520interactions.%2520Existing%2520cross-domain%2520FER%2520methods%2520often%2520transfer%250Aknowledge%2520solely%2520from%2520a%2520single%2520labeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%250Adomain%252C%2520neglecting%2520the%2520comprehensive%2520information%2520across%2520multiple%2520sources.%250ANevertheless%252C%2520cross-multidomain%2520FER%2520%2528CMFER%2529%2520is%2520very%2520challenging%2520for%2520%2528i%2529%2520the%250Ainherent%2520inter-domain%2520shifts%2520across%2520multiple%2520domains%2520and%2520%2528ii%2529%2520the%2520intra-domain%250Ashifts%2520stemming%2520from%2520the%2520ambiguous%2520expressions%2520and%2520low%2520inter-class%250Adistinctions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Learning%2520with%2520Alignments%2520CMFER%250Aframework%252C%2520named%2520LA-CMFER%252C%2520to%2520handle%2520both%2520inter-%2520and%2520intra-domain%2520shifts.%250ASpecifically%252C%2520LA-CMFER%2520is%2520constructed%2520with%2520a%2520global%2520branch%2520and%2520a%2520local%2520branch%250Ato%2520extract%2520features%2520from%2520the%2520full%2520images%2520and%2520local%2520subtle%2520expressions%252C%250Arespectively.%2520Based%2520on%2520this%252C%2520LA-CMFER%2520presents%2520a%2520dual-level%2520inter-domain%250Aalignment%2520method%2520to%2520force%2520the%2520model%2520to%2520prioritize%2520hard-to-align%2520samples%2520in%250Aknowledge%2520transfer%2520at%2520a%2520sample%2520level%2520while%2520gradually%2520generating%2520a%250Awell-clustered%2520feature%2520space%2520with%2520the%2520guidance%2520of%2520class%2520attributes%2520at%2520a%2520cluster%250Alevel%252C%2520thus%2520narrowing%2520the%2520inter-domain%2520shifts.%2520To%2520address%2520the%2520intra-domain%250Ashifts%252C%2520LA-CMFER%2520introduces%2520a%2520multi-view%2520intra-domain%2520alignment%2520method%2520with%2520a%250Amulti-view%2520clustering%2520consistency%2520constraint%2520where%2520a%2520prediction%2520similarity%250Amatrix%2520is%2520built%2520to%2520pursue%2520consistency%2520between%2520the%2520global%2520and%2520local%2520views%252C%2520thus%250Arefining%2520pseudo%2520labels%2520and%2520eliminating%2520latent%2520noise.%2520Extensive%2520experiments%2520on%250Asix%2520benchmark%2520datasets%2520have%2520validated%2520the%2520superiority%2520of%2520our%2520LA-CMFER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Alignments%3A%20Tackling%20the%20Inter-%20and%20Intra-domain%20Shifts%0A%20%20for%20Cross-multidomain%20Facial%20Expression%20Recognition&entry.906535625=Yuxiang%20Yang%20and%20Lu%20Wen%20and%20Xinyi%20Zeng%20and%20Yuanyuan%20Xu%20and%20Xi%20Wu%20and%20Jiliu%20Zhou%20and%20Yan%20Wang&entry.1292438233=%20%20Facial%20Expression%20Recognition%20%28FER%29%20holds%20significant%20importance%20in%0Ahuman-computer%20interactions.%20Existing%20cross-domain%20FER%20methods%20often%20transfer%0Aknowledge%20solely%20from%20a%20single%20labeled%20source%20domain%20to%20an%20unlabeled%20target%0Adomain%2C%20neglecting%20the%20comprehensive%20information%20across%20multiple%20sources.%0ANevertheless%2C%20cross-multidomain%20FER%20%28CMFER%29%20is%20very%20challenging%20for%20%28i%29%20the%0Ainherent%20inter-domain%20shifts%20across%20multiple%20domains%20and%20%28ii%29%20the%20intra-domain%0Ashifts%20stemming%20from%20the%20ambiguous%20expressions%20and%20low%20inter-class%0Adistinctions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Learning%20with%20Alignments%20CMFER%0Aframework%2C%20named%20LA-CMFER%2C%20to%20handle%20both%20inter-%20and%20intra-domain%20shifts.%0ASpecifically%2C%20LA-CMFER%20is%20constructed%20with%20a%20global%20branch%20and%20a%20local%20branch%0Ato%20extract%20features%20from%20the%20full%20images%20and%20local%20subtle%20expressions%2C%0Arespectively.%20Based%20on%20this%2C%20LA-CMFER%20presents%20a%20dual-level%20inter-domain%0Aalignment%20method%20to%20force%20the%20model%20to%20prioritize%20hard-to-align%20samples%20in%0Aknowledge%20transfer%20at%20a%20sample%20level%20while%20gradually%20generating%20a%0Awell-clustered%20feature%20space%20with%20the%20guidance%20of%20class%20attributes%20at%20a%20cluster%0Alevel%2C%20thus%20narrowing%20the%20inter-domain%20shifts.%20To%20address%20the%20intra-domain%0Ashifts%2C%20LA-CMFER%20introduces%20a%20multi-view%20intra-domain%20alignment%20method%20with%20a%0Amulti-view%20clustering%20consistency%20constraint%20where%20a%20prediction%20similarity%0Amatrix%20is%20built%20to%20pursue%20consistency%20between%20the%20global%20and%20local%20views%2C%20thus%0Arefining%20pseudo%20labels%20and%20eliminating%20latent%20noise.%20Extensive%20experiments%20on%0Asix%20benchmark%20datasets%20have%20validated%20the%20superiority%20of%20our%20LA-CMFER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05688v2&entry.124074799=Read"},
{"title": "Matting by Generation", "author": "Zhixiang Wang and Baiang Li and Jian Wang and Yu-Lun Liu and Jinwei Gu and Yung-Yu Chuang and Shin'ichi Satoh", "abstract": "  This paper introduces an innovative approach for image matting that redefines\nthe traditional regression-based task as a generative modeling challenge. Our\nmethod harnesses the capabilities of latent diffusion models, enriched with\nextensive pre-trained knowledge, to regularize the matting process. We present\nnovel architectural innovations that empower our model to produce mattes with\nsuperior resolution and detail. The proposed method is versatile and can\nperform both guidance-free and guidance-based image matting, accommodating a\nvariety of additional cues. Our comprehensive evaluation across three benchmark\ndatasets demonstrates the superior performance of our approach, both\nquantitatively and qualitatively. The results not only reflect our method's\nrobust effectiveness but also highlight its ability to generate visually\ncompelling mattes that approach photorealistic quality. The project page for\nthis paper is available at\nhttps://lightchaserx.github.io/matting-by-generation/\n", "link": "http://arxiv.org/abs/2407.21017v1", "date": "2024-07-30", "relevancy": 2.2777, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5751}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5687}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matting%20by%20Generation&body=Title%3A%20Matting%20by%20Generation%0AAuthor%3A%20Zhixiang%20Wang%20and%20Baiang%20Li%20and%20Jian%20Wang%20and%20Yu-Lun%20Liu%20and%20Jinwei%20Gu%20and%20Yung-Yu%20Chuang%20and%20Shin%27ichi%20Satoh%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20innovative%20approach%20for%20image%20matting%20that%20redefines%0Athe%20traditional%20regression-based%20task%20as%20a%20generative%20modeling%20challenge.%20Our%0Amethod%20harnesses%20the%20capabilities%20of%20latent%20diffusion%20models%2C%20enriched%20with%0Aextensive%20pre-trained%20knowledge%2C%20to%20regularize%20the%20matting%20process.%20We%20present%0Anovel%20architectural%20innovations%20that%20empower%20our%20model%20to%20produce%20mattes%20with%0Asuperior%20resolution%20and%20detail.%20The%20proposed%20method%20is%20versatile%20and%20can%0Aperform%20both%20guidance-free%20and%20guidance-based%20image%20matting%2C%20accommodating%20a%0Avariety%20of%20additional%20cues.%20Our%20comprehensive%20evaluation%20across%20three%20benchmark%0Adatasets%20demonstrates%20the%20superior%20performance%20of%20our%20approach%2C%20both%0Aquantitatively%20and%20qualitatively.%20The%20results%20not%20only%20reflect%20our%20method%27s%0Arobust%20effectiveness%20but%20also%20highlight%20its%20ability%20to%20generate%20visually%0Acompelling%20mattes%20that%20approach%20photorealistic%20quality.%20The%20project%20page%20for%0Athis%20paper%20is%20available%20at%0Ahttps%3A//lightchaserx.github.io/matting-by-generation/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatting%2520by%2520Generation%26entry.906535625%3DZhixiang%2520Wang%2520and%2520Baiang%2520Li%2520and%2520Jian%2520Wang%2520and%2520Yu-Lun%2520Liu%2520and%2520Jinwei%2520Gu%2520and%2520Yung-Yu%2520Chuang%2520and%2520Shin%2527ichi%2520Satoh%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520innovative%2520approach%2520for%2520image%2520matting%2520that%2520redefines%250Athe%2520traditional%2520regression-based%2520task%2520as%2520a%2520generative%2520modeling%2520challenge.%2520Our%250Amethod%2520harnesses%2520the%2520capabilities%2520of%2520latent%2520diffusion%2520models%252C%2520enriched%2520with%250Aextensive%2520pre-trained%2520knowledge%252C%2520to%2520regularize%2520the%2520matting%2520process.%2520We%2520present%250Anovel%2520architectural%2520innovations%2520that%2520empower%2520our%2520model%2520to%2520produce%2520mattes%2520with%250Asuperior%2520resolution%2520and%2520detail.%2520The%2520proposed%2520method%2520is%2520versatile%2520and%2520can%250Aperform%2520both%2520guidance-free%2520and%2520guidance-based%2520image%2520matting%252C%2520accommodating%2520a%250Avariety%2520of%2520additional%2520cues.%2520Our%2520comprehensive%2520evaluation%2520across%2520three%2520benchmark%250Adatasets%2520demonstrates%2520the%2520superior%2520performance%2520of%2520our%2520approach%252C%2520both%250Aquantitatively%2520and%2520qualitatively.%2520The%2520results%2520not%2520only%2520reflect%2520our%2520method%2527s%250Arobust%2520effectiveness%2520but%2520also%2520highlight%2520its%2520ability%2520to%2520generate%2520visually%250Acompelling%2520mattes%2520that%2520approach%2520photorealistic%2520quality.%2520The%2520project%2520page%2520for%250Athis%2520paper%2520is%2520available%2520at%250Ahttps%253A//lightchaserx.github.io/matting-by-generation/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matting%20by%20Generation&entry.906535625=Zhixiang%20Wang%20and%20Baiang%20Li%20and%20Jian%20Wang%20and%20Yu-Lun%20Liu%20and%20Jinwei%20Gu%20and%20Yung-Yu%20Chuang%20and%20Shin%27ichi%20Satoh&entry.1292438233=%20%20This%20paper%20introduces%20an%20innovative%20approach%20for%20image%20matting%20that%20redefines%0Athe%20traditional%20regression-based%20task%20as%20a%20generative%20modeling%20challenge.%20Our%0Amethod%20harnesses%20the%20capabilities%20of%20latent%20diffusion%20models%2C%20enriched%20with%0Aextensive%20pre-trained%20knowledge%2C%20to%20regularize%20the%20matting%20process.%20We%20present%0Anovel%20architectural%20innovations%20that%20empower%20our%20model%20to%20produce%20mattes%20with%0Asuperior%20resolution%20and%20detail.%20The%20proposed%20method%20is%20versatile%20and%20can%0Aperform%20both%20guidance-free%20and%20guidance-based%20image%20matting%2C%20accommodating%20a%0Avariety%20of%20additional%20cues.%20Our%20comprehensive%20evaluation%20across%20three%20benchmark%0Adatasets%20demonstrates%20the%20superior%20performance%20of%20our%20approach%2C%20both%0Aquantitatively%20and%20qualitatively.%20The%20results%20not%20only%20reflect%20our%20method%27s%0Arobust%20effectiveness%20but%20also%20highlight%20its%20ability%20to%20generate%20visually%0Acompelling%20mattes%20that%20approach%20photorealistic%20quality.%20The%20project%20page%20for%0Athis%20paper%20is%20available%20at%0Ahttps%3A//lightchaserx.github.io/matting-by-generation/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21017v1&entry.124074799=Read"},
{"title": "Diffusion Augmented Agents: A Framework for Efficient Exploration and\n  Transfer Learning", "author": "Norman Di Palo and Leonard Hasenclever and Jan Humplik and Arunkumar Byravan", "abstract": "  We introduce Diffusion Augmented Agents (DAAG), a novel framework that\nleverages large language models, vision language models, and diffusion models\nto improve sample efficiency and transfer learning in reinforcement learning\nfor embodied agents. DAAG hindsight relabels the agent's past experience by\nusing diffusion models to transform videos in a temporally and geometrically\nconsistent way to align with target instructions with a technique we call\nHindsight Experience Augmentation. A large language model orchestrates this\nautonomous process without requiring human supervision, making it well-suited\nfor lifelong learning scenarios. The framework reduces the amount of\nreward-labeled data needed to 1) finetune a vision language model that acts as\na reward detector, and 2) train RL agents on new tasks. We demonstrate the\nsample efficiency gains of DAAG in simulated robotics environments involving\nmanipulation and navigation. Our results show that DAAG improves learning of\nreward detectors, transferring past experience, and acquiring new tasks - key\nabilities for developing efficient lifelong learning agents. Supplementary\nmaterial and visualizations are available on our website\nhttps://sites.google.com/view/diffusion-augmented-agents/\n", "link": "http://arxiv.org/abs/2407.20798v1", "date": "2024-07-30", "relevancy": 2.2707, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5853}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5651}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Augmented%20Agents%3A%20A%20Framework%20for%20Efficient%20Exploration%20and%0A%20%20Transfer%20Learning&body=Title%3A%20Diffusion%20Augmented%20Agents%3A%20A%20Framework%20for%20Efficient%20Exploration%20and%0A%20%20Transfer%20Learning%0AAuthor%3A%20Norman%20Di%20Palo%20and%20Leonard%20Hasenclever%20and%20Jan%20Humplik%20and%20Arunkumar%20Byravan%0AAbstract%3A%20%20%20We%20introduce%20Diffusion%20Augmented%20Agents%20%28DAAG%29%2C%20a%20novel%20framework%20that%0Aleverages%20large%20language%20models%2C%20vision%20language%20models%2C%20and%20diffusion%20models%0Ato%20improve%20sample%20efficiency%20and%20transfer%20learning%20in%20reinforcement%20learning%0Afor%20embodied%20agents.%20DAAG%20hindsight%20relabels%20the%20agent%27s%20past%20experience%20by%0Ausing%20diffusion%20models%20to%20transform%20videos%20in%20a%20temporally%20and%20geometrically%0Aconsistent%20way%20to%20align%20with%20target%20instructions%20with%20a%20technique%20we%20call%0AHindsight%20Experience%20Augmentation.%20A%20large%20language%20model%20orchestrates%20this%0Aautonomous%20process%20without%20requiring%20human%20supervision%2C%20making%20it%20well-suited%0Afor%20lifelong%20learning%20scenarios.%20The%20framework%20reduces%20the%20amount%20of%0Areward-labeled%20data%20needed%20to%201%29%20finetune%20a%20vision%20language%20model%20that%20acts%20as%0Aa%20reward%20detector%2C%20and%202%29%20train%20RL%20agents%20on%20new%20tasks.%20We%20demonstrate%20the%0Asample%20efficiency%20gains%20of%20DAAG%20in%20simulated%20robotics%20environments%20involving%0Amanipulation%20and%20navigation.%20Our%20results%20show%20that%20DAAG%20improves%20learning%20of%0Areward%20detectors%2C%20transferring%20past%20experience%2C%20and%20acquiring%20new%20tasks%20-%20key%0Aabilities%20for%20developing%20efficient%20lifelong%20learning%20agents.%20Supplementary%0Amaterial%20and%20visualizations%20are%20available%20on%20our%20website%0Ahttps%3A//sites.google.com/view/diffusion-augmented-agents/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Augmented%2520Agents%253A%2520A%2520Framework%2520for%2520Efficient%2520Exploration%2520and%250A%2520%2520Transfer%2520Learning%26entry.906535625%3DNorman%2520Di%2520Palo%2520and%2520Leonard%2520Hasenclever%2520and%2520Jan%2520Humplik%2520and%2520Arunkumar%2520Byravan%26entry.1292438233%3D%2520%2520We%2520introduce%2520Diffusion%2520Augmented%2520Agents%2520%2528DAAG%2529%252C%2520a%2520novel%2520framework%2520that%250Aleverages%2520large%2520language%2520models%252C%2520vision%2520language%2520models%252C%2520and%2520diffusion%2520models%250Ato%2520improve%2520sample%2520efficiency%2520and%2520transfer%2520learning%2520in%2520reinforcement%2520learning%250Afor%2520embodied%2520agents.%2520DAAG%2520hindsight%2520relabels%2520the%2520agent%2527s%2520past%2520experience%2520by%250Ausing%2520diffusion%2520models%2520to%2520transform%2520videos%2520in%2520a%2520temporally%2520and%2520geometrically%250Aconsistent%2520way%2520to%2520align%2520with%2520target%2520instructions%2520with%2520a%2520technique%2520we%2520call%250AHindsight%2520Experience%2520Augmentation.%2520A%2520large%2520language%2520model%2520orchestrates%2520this%250Aautonomous%2520process%2520without%2520requiring%2520human%2520supervision%252C%2520making%2520it%2520well-suited%250Afor%2520lifelong%2520learning%2520scenarios.%2520The%2520framework%2520reduces%2520the%2520amount%2520of%250Areward-labeled%2520data%2520needed%2520to%25201%2529%2520finetune%2520a%2520vision%2520language%2520model%2520that%2520acts%2520as%250Aa%2520reward%2520detector%252C%2520and%25202%2529%2520train%2520RL%2520agents%2520on%2520new%2520tasks.%2520We%2520demonstrate%2520the%250Asample%2520efficiency%2520gains%2520of%2520DAAG%2520in%2520simulated%2520robotics%2520environments%2520involving%250Amanipulation%2520and%2520navigation.%2520Our%2520results%2520show%2520that%2520DAAG%2520improves%2520learning%2520of%250Areward%2520detectors%252C%2520transferring%2520past%2520experience%252C%2520and%2520acquiring%2520new%2520tasks%2520-%2520key%250Aabilities%2520for%2520developing%2520efficient%2520lifelong%2520learning%2520agents.%2520Supplementary%250Amaterial%2520and%2520visualizations%2520are%2520available%2520on%2520our%2520website%250Ahttps%253A//sites.google.com/view/diffusion-augmented-agents/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Augmented%20Agents%3A%20A%20Framework%20for%20Efficient%20Exploration%20and%0A%20%20Transfer%20Learning&entry.906535625=Norman%20Di%20Palo%20and%20Leonard%20Hasenclever%20and%20Jan%20Humplik%20and%20Arunkumar%20Byravan&entry.1292438233=%20%20We%20introduce%20Diffusion%20Augmented%20Agents%20%28DAAG%29%2C%20a%20novel%20framework%20that%0Aleverages%20large%20language%20models%2C%20vision%20language%20models%2C%20and%20diffusion%20models%0Ato%20improve%20sample%20efficiency%20and%20transfer%20learning%20in%20reinforcement%20learning%0Afor%20embodied%20agents.%20DAAG%20hindsight%20relabels%20the%20agent%27s%20past%20experience%20by%0Ausing%20diffusion%20models%20to%20transform%20videos%20in%20a%20temporally%20and%20geometrically%0Aconsistent%20way%20to%20align%20with%20target%20instructions%20with%20a%20technique%20we%20call%0AHindsight%20Experience%20Augmentation.%20A%20large%20language%20model%20orchestrates%20this%0Aautonomous%20process%20without%20requiring%20human%20supervision%2C%20making%20it%20well-suited%0Afor%20lifelong%20learning%20scenarios.%20The%20framework%20reduces%20the%20amount%20of%0Areward-labeled%20data%20needed%20to%201%29%20finetune%20a%20vision%20language%20model%20that%20acts%20as%0Aa%20reward%20detector%2C%20and%202%29%20train%20RL%20agents%20on%20new%20tasks.%20We%20demonstrate%20the%0Asample%20efficiency%20gains%20of%20DAAG%20in%20simulated%20robotics%20environments%20involving%0Amanipulation%20and%20navigation.%20Our%20results%20show%20that%20DAAG%20improves%20learning%20of%0Areward%20detectors%2C%20transferring%20past%20experience%2C%20and%20acquiring%20new%20tasks%20-%20key%0Aabilities%20for%20developing%20efficient%20lifelong%20learning%20agents.%20Supplementary%0Amaterial%20and%20visualizations%20are%20available%20on%20our%20website%0Ahttps%3A//sites.google.com/view/diffusion-augmented-agents/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20798v1&entry.124074799=Read"},
{"title": "Versatile audio-visual learning for emotion recognition", "author": "Lucas Goncalves and Seong-Gyun Leem and Wei-Cheng Lin and Berrak Sisman and Carlos Busso", "abstract": "  Most current audio-visual emotion recognition models lack the flexibility\nneeded for deployment in practical applications. We envision a multimodal\nsystem that works even when only one modality is available and can be\nimplemented interchangeably for either predicting emotional attributes or\nrecognizing categorical emotions. Achieving such flexibility in a multimodal\nemotion recognition system is difficult due to the inherent challenges in\naccurately interpreting and integrating varied data sources. It is also a\nchallenge to robustly handle missing or partial information while allowing\ndirect switch between regression or classification tasks. This study proposes a\nversatile audio-visual learning (VAVL) framework for handling unimodal and\nmultimodal systems for emotion regression or emotion classification tasks. We\nimplement an audio-visual framework that can be trained even when audio and\nvisual paired data is not available for part of the training set (i.e., audio\nonly or only video is present). We achieve this effective representation\nlearning with audio-visual shared layers, residual connections over shared\nlayers, and a unimodal reconstruction task. Our experimental results reveal\nthat our architecture significantly outperforms strong baselines on the\nCREMA-D, MSP-IMPROV, and CMU-MOSEI corpora. Notably, VAVL attains a new\nstate-of-the-art performance in the emotional attribute prediction task on the\nMSP-IMPROV corpus.\n", "link": "http://arxiv.org/abs/2305.07216v2", "date": "2024-07-30", "relevancy": 2.2677, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5761}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5666}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Versatile%20audio-visual%20learning%20for%20emotion%20recognition&body=Title%3A%20Versatile%20audio-visual%20learning%20for%20emotion%20recognition%0AAuthor%3A%20Lucas%20Goncalves%20and%20Seong-Gyun%20Leem%20and%20Wei-Cheng%20Lin%20and%20Berrak%20Sisman%20and%20Carlos%20Busso%0AAbstract%3A%20%20%20Most%20current%20audio-visual%20emotion%20recognition%20models%20lack%20the%20flexibility%0Aneeded%20for%20deployment%20in%20practical%20applications.%20We%20envision%20a%20multimodal%0Asystem%20that%20works%20even%20when%20only%20one%20modality%20is%20available%20and%20can%20be%0Aimplemented%20interchangeably%20for%20either%20predicting%20emotional%20attributes%20or%0Arecognizing%20categorical%20emotions.%20Achieving%20such%20flexibility%20in%20a%20multimodal%0Aemotion%20recognition%20system%20is%20difficult%20due%20to%20the%20inherent%20challenges%20in%0Aaccurately%20interpreting%20and%20integrating%20varied%20data%20sources.%20It%20is%20also%20a%0Achallenge%20to%20robustly%20handle%20missing%20or%20partial%20information%20while%20allowing%0Adirect%20switch%20between%20regression%20or%20classification%20tasks.%20This%20study%20proposes%20a%0Aversatile%20audio-visual%20learning%20%28VAVL%29%20framework%20for%20handling%20unimodal%20and%0Amultimodal%20systems%20for%20emotion%20regression%20or%20emotion%20classification%20tasks.%20We%0Aimplement%20an%20audio-visual%20framework%20that%20can%20be%20trained%20even%20when%20audio%20and%0Avisual%20paired%20data%20is%20not%20available%20for%20part%20of%20the%20training%20set%20%28i.e.%2C%20audio%0Aonly%20or%20only%20video%20is%20present%29.%20We%20achieve%20this%20effective%20representation%0Alearning%20with%20audio-visual%20shared%20layers%2C%20residual%20connections%20over%20shared%0Alayers%2C%20and%20a%20unimodal%20reconstruction%20task.%20Our%20experimental%20results%20reveal%0Athat%20our%20architecture%20significantly%20outperforms%20strong%20baselines%20on%20the%0ACREMA-D%2C%20MSP-IMPROV%2C%20and%20CMU-MOSEI%20corpora.%20Notably%2C%20VAVL%20attains%20a%20new%0Astate-of-the-art%20performance%20in%20the%20emotional%20attribute%20prediction%20task%20on%20the%0AMSP-IMPROV%20corpus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.07216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVersatile%2520audio-visual%2520learning%2520for%2520emotion%2520recognition%26entry.906535625%3DLucas%2520Goncalves%2520and%2520Seong-Gyun%2520Leem%2520and%2520Wei-Cheng%2520Lin%2520and%2520Berrak%2520Sisman%2520and%2520Carlos%2520Busso%26entry.1292438233%3D%2520%2520Most%2520current%2520audio-visual%2520emotion%2520recognition%2520models%2520lack%2520the%2520flexibility%250Aneeded%2520for%2520deployment%2520in%2520practical%2520applications.%2520We%2520envision%2520a%2520multimodal%250Asystem%2520that%2520works%2520even%2520when%2520only%2520one%2520modality%2520is%2520available%2520and%2520can%2520be%250Aimplemented%2520interchangeably%2520for%2520either%2520predicting%2520emotional%2520attributes%2520or%250Arecognizing%2520categorical%2520emotions.%2520Achieving%2520such%2520flexibility%2520in%2520a%2520multimodal%250Aemotion%2520recognition%2520system%2520is%2520difficult%2520due%2520to%2520the%2520inherent%2520challenges%2520in%250Aaccurately%2520interpreting%2520and%2520integrating%2520varied%2520data%2520sources.%2520It%2520is%2520also%2520a%250Achallenge%2520to%2520robustly%2520handle%2520missing%2520or%2520partial%2520information%2520while%2520allowing%250Adirect%2520switch%2520between%2520regression%2520or%2520classification%2520tasks.%2520This%2520study%2520proposes%2520a%250Aversatile%2520audio-visual%2520learning%2520%2528VAVL%2529%2520framework%2520for%2520handling%2520unimodal%2520and%250Amultimodal%2520systems%2520for%2520emotion%2520regression%2520or%2520emotion%2520classification%2520tasks.%2520We%250Aimplement%2520an%2520audio-visual%2520framework%2520that%2520can%2520be%2520trained%2520even%2520when%2520audio%2520and%250Avisual%2520paired%2520data%2520is%2520not%2520available%2520for%2520part%2520of%2520the%2520training%2520set%2520%2528i.e.%252C%2520audio%250Aonly%2520or%2520only%2520video%2520is%2520present%2529.%2520We%2520achieve%2520this%2520effective%2520representation%250Alearning%2520with%2520audio-visual%2520shared%2520layers%252C%2520residual%2520connections%2520over%2520shared%250Alayers%252C%2520and%2520a%2520unimodal%2520reconstruction%2520task.%2520Our%2520experimental%2520results%2520reveal%250Athat%2520our%2520architecture%2520significantly%2520outperforms%2520strong%2520baselines%2520on%2520the%250ACREMA-D%252C%2520MSP-IMPROV%252C%2520and%2520CMU-MOSEI%2520corpora.%2520Notably%252C%2520VAVL%2520attains%2520a%2520new%250Astate-of-the-art%2520performance%2520in%2520the%2520emotional%2520attribute%2520prediction%2520task%2520on%2520the%250AMSP-IMPROV%2520corpus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.07216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Versatile%20audio-visual%20learning%20for%20emotion%20recognition&entry.906535625=Lucas%20Goncalves%20and%20Seong-Gyun%20Leem%20and%20Wei-Cheng%20Lin%20and%20Berrak%20Sisman%20and%20Carlos%20Busso&entry.1292438233=%20%20Most%20current%20audio-visual%20emotion%20recognition%20models%20lack%20the%20flexibility%0Aneeded%20for%20deployment%20in%20practical%20applications.%20We%20envision%20a%20multimodal%0Asystem%20that%20works%20even%20when%20only%20one%20modality%20is%20available%20and%20can%20be%0Aimplemented%20interchangeably%20for%20either%20predicting%20emotional%20attributes%20or%0Arecognizing%20categorical%20emotions.%20Achieving%20such%20flexibility%20in%20a%20multimodal%0Aemotion%20recognition%20system%20is%20difficult%20due%20to%20the%20inherent%20challenges%20in%0Aaccurately%20interpreting%20and%20integrating%20varied%20data%20sources.%20It%20is%20also%20a%0Achallenge%20to%20robustly%20handle%20missing%20or%20partial%20information%20while%20allowing%0Adirect%20switch%20between%20regression%20or%20classification%20tasks.%20This%20study%20proposes%20a%0Aversatile%20audio-visual%20learning%20%28VAVL%29%20framework%20for%20handling%20unimodal%20and%0Amultimodal%20systems%20for%20emotion%20regression%20or%20emotion%20classification%20tasks.%20We%0Aimplement%20an%20audio-visual%20framework%20that%20can%20be%20trained%20even%20when%20audio%20and%0Avisual%20paired%20data%20is%20not%20available%20for%20part%20of%20the%20training%20set%20%28i.e.%2C%20audio%0Aonly%20or%20only%20video%20is%20present%29.%20We%20achieve%20this%20effective%20representation%0Alearning%20with%20audio-visual%20shared%20layers%2C%20residual%20connections%20over%20shared%0Alayers%2C%20and%20a%20unimodal%20reconstruction%20task.%20Our%20experimental%20results%20reveal%0Athat%20our%20architecture%20significantly%20outperforms%20strong%20baselines%20on%20the%0ACREMA-D%2C%20MSP-IMPROV%2C%20and%20CMU-MOSEI%20corpora.%20Notably%2C%20VAVL%20attains%20a%20new%0Astate-of-the-art%20performance%20in%20the%20emotional%20attribute%20prediction%20task%20on%20the%0AMSP-IMPROV%20corpus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.07216v2&entry.124074799=Read"},
{"title": "Ponymation: Learning 3D Animal Motions from Unlabeled Online Videos", "author": "Keqiang Sun and Dor Litvak and Yunzhi Zhang and Hongsheng Li and Jiajun Wu and Shangzhe Wu", "abstract": "  We introduce Ponymation, a new method for learning a generative model of\narticulated 3D animal motions from raw, unlabeled online videos. Unlike\nexisting approaches for motion synthesis, our model does not require any pose\nannotations or parametric shape models for training, and is learned purely from\na collection of raw video clips obtained from the Internet. We build upon a\nrecent work, MagicPony, which learns articulated 3D animal shapes purely from\nsingle image collections, and extend it on two fronts. First, instead of\ntraining on static images, we augment the framework with a video training\npipeline that incorporates temporal regularizations, achieving more accurate\nand temporally consistent reconstructions. Second, we learn a generative model\nof the underlying articulated 3D motion sequences via a spatio-temporal\ntransformer VAE, simply using 2D reconstruction losses without relying on any\nexplicit pose annotations. At inference time, given a single 2D image of a new\nanimal instance, our model reconstructs an articulated, textured 3D mesh, and\ngenerates plausible 3D animations by sampling from the learned motion latent\nspace.\n", "link": "http://arxiv.org/abs/2312.13604v2", "date": "2024-07-30", "relevancy": 2.2665, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5713}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5697}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ponymation%3A%20Learning%203D%20Animal%20Motions%20from%20Unlabeled%20Online%20Videos&body=Title%3A%20Ponymation%3A%20Learning%203D%20Animal%20Motions%20from%20Unlabeled%20Online%20Videos%0AAuthor%3A%20Keqiang%20Sun%20and%20Dor%20Litvak%20and%20Yunzhi%20Zhang%20and%20Hongsheng%20Li%20and%20Jiajun%20Wu%20and%20Shangzhe%20Wu%0AAbstract%3A%20%20%20We%20introduce%20Ponymation%2C%20a%20new%20method%20for%20learning%20a%20generative%20model%20of%0Aarticulated%203D%20animal%20motions%20from%20raw%2C%20unlabeled%20online%20videos.%20Unlike%0Aexisting%20approaches%20for%20motion%20synthesis%2C%20our%20model%20does%20not%20require%20any%20pose%0Aannotations%20or%20parametric%20shape%20models%20for%20training%2C%20and%20is%20learned%20purely%20from%0Aa%20collection%20of%20raw%20video%20clips%20obtained%20from%20the%20Internet.%20We%20build%20upon%20a%0Arecent%20work%2C%20MagicPony%2C%20which%20learns%20articulated%203D%20animal%20shapes%20purely%20from%0Asingle%20image%20collections%2C%20and%20extend%20it%20on%20two%20fronts.%20First%2C%20instead%20of%0Atraining%20on%20static%20images%2C%20we%20augment%20the%20framework%20with%20a%20video%20training%0Apipeline%20that%20incorporates%20temporal%20regularizations%2C%20achieving%20more%20accurate%0Aand%20temporally%20consistent%20reconstructions.%20Second%2C%20we%20learn%20a%20generative%20model%0Aof%20the%20underlying%20articulated%203D%20motion%20sequences%20via%20a%20spatio-temporal%0Atransformer%20VAE%2C%20simply%20using%202D%20reconstruction%20losses%20without%20relying%20on%20any%0Aexplicit%20pose%20annotations.%20At%20inference%20time%2C%20given%20a%20single%202D%20image%20of%20a%20new%0Aanimal%20instance%2C%20our%20model%20reconstructs%20an%20articulated%2C%20textured%203D%20mesh%2C%20and%0Agenerates%20plausible%203D%20animations%20by%20sampling%20from%20the%20learned%20motion%20latent%0Aspace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPonymation%253A%2520Learning%25203D%2520Animal%2520Motions%2520from%2520Unlabeled%2520Online%2520Videos%26entry.906535625%3DKeqiang%2520Sun%2520and%2520Dor%2520Litvak%2520and%2520Yunzhi%2520Zhang%2520and%2520Hongsheng%2520Li%2520and%2520Jiajun%2520Wu%2520and%2520Shangzhe%2520Wu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Ponymation%252C%2520a%2520new%2520method%2520for%2520learning%2520a%2520generative%2520model%2520of%250Aarticulated%25203D%2520animal%2520motions%2520from%2520raw%252C%2520unlabeled%2520online%2520videos.%2520Unlike%250Aexisting%2520approaches%2520for%2520motion%2520synthesis%252C%2520our%2520model%2520does%2520not%2520require%2520any%2520pose%250Aannotations%2520or%2520parametric%2520shape%2520models%2520for%2520training%252C%2520and%2520is%2520learned%2520purely%2520from%250Aa%2520collection%2520of%2520raw%2520video%2520clips%2520obtained%2520from%2520the%2520Internet.%2520We%2520build%2520upon%2520a%250Arecent%2520work%252C%2520MagicPony%252C%2520which%2520learns%2520articulated%25203D%2520animal%2520shapes%2520purely%2520from%250Asingle%2520image%2520collections%252C%2520and%2520extend%2520it%2520on%2520two%2520fronts.%2520First%252C%2520instead%2520of%250Atraining%2520on%2520static%2520images%252C%2520we%2520augment%2520the%2520framework%2520with%2520a%2520video%2520training%250Apipeline%2520that%2520incorporates%2520temporal%2520regularizations%252C%2520achieving%2520more%2520accurate%250Aand%2520temporally%2520consistent%2520reconstructions.%2520Second%252C%2520we%2520learn%2520a%2520generative%2520model%250Aof%2520the%2520underlying%2520articulated%25203D%2520motion%2520sequences%2520via%2520a%2520spatio-temporal%250Atransformer%2520VAE%252C%2520simply%2520using%25202D%2520reconstruction%2520losses%2520without%2520relying%2520on%2520any%250Aexplicit%2520pose%2520annotations.%2520At%2520inference%2520time%252C%2520given%2520a%2520single%25202D%2520image%2520of%2520a%2520new%250Aanimal%2520instance%252C%2520our%2520model%2520reconstructs%2520an%2520articulated%252C%2520textured%25203D%2520mesh%252C%2520and%250Agenerates%2520plausible%25203D%2520animations%2520by%2520sampling%2520from%2520the%2520learned%2520motion%2520latent%250Aspace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ponymation%3A%20Learning%203D%20Animal%20Motions%20from%20Unlabeled%20Online%20Videos&entry.906535625=Keqiang%20Sun%20and%20Dor%20Litvak%20and%20Yunzhi%20Zhang%20and%20Hongsheng%20Li%20and%20Jiajun%20Wu%20and%20Shangzhe%20Wu&entry.1292438233=%20%20We%20introduce%20Ponymation%2C%20a%20new%20method%20for%20learning%20a%20generative%20model%20of%0Aarticulated%203D%20animal%20motions%20from%20raw%2C%20unlabeled%20online%20videos.%20Unlike%0Aexisting%20approaches%20for%20motion%20synthesis%2C%20our%20model%20does%20not%20require%20any%20pose%0Aannotations%20or%20parametric%20shape%20models%20for%20training%2C%20and%20is%20learned%20purely%20from%0Aa%20collection%20of%20raw%20video%20clips%20obtained%20from%20the%20Internet.%20We%20build%20upon%20a%0Arecent%20work%2C%20MagicPony%2C%20which%20learns%20articulated%203D%20animal%20shapes%20purely%20from%0Asingle%20image%20collections%2C%20and%20extend%20it%20on%20two%20fronts.%20First%2C%20instead%20of%0Atraining%20on%20static%20images%2C%20we%20augment%20the%20framework%20with%20a%20video%20training%0Apipeline%20that%20incorporates%20temporal%20regularizations%2C%20achieving%20more%20accurate%0Aand%20temporally%20consistent%20reconstructions.%20Second%2C%20we%20learn%20a%20generative%20model%0Aof%20the%20underlying%20articulated%203D%20motion%20sequences%20via%20a%20spatio-temporal%0Atransformer%20VAE%2C%20simply%20using%202D%20reconstruction%20losses%20without%20relying%20on%20any%0Aexplicit%20pose%20annotations.%20At%20inference%20time%2C%20given%20a%20single%202D%20image%20of%20a%20new%0Aanimal%20instance%2C%20our%20model%20reconstructs%20an%20articulated%2C%20textured%203D%20mesh%2C%20and%0Agenerates%20plausible%203D%20animations%20by%20sampling%20from%20the%20learned%20motion%20latent%0Aspace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13604v2&entry.124074799=Read"},
{"title": "An Adaptive Indoor Localization Approach Using WiFi RSSI Fingerprinting\n  with SLAM-Enabled Robotic Platform and Deep Neural Networks", "author": "Seyed Alireza Rahimi Azghadi and Atah Nuh Mih and Asfia Kawnine and Monica Wachowicz and Francis Palma and Hung Cao", "abstract": "  Indoor localization plays a vital role in the era of the IoT and robotics,\nwith WiFi technology being a prominent choice due to its ubiquity. We present a\nmethod for creating WiFi fingerprinting datasets to enhance indoor localization\nsystems and address the gap in WiFi fingerprinting dataset creation. We used\nthe Simultaneous Localization And Mapping (SLAM) algorithm and employed a\nrobotic platform to construct precise maps and localize robots in indoor\nenvironments. We developed software applications to facilitate data\nacquisition, fingerprinting dataset collection, and accurate ground truth map\nbuilding. Subsequently, we aligned the spatial information generated via the\nSLAM with the WiFi scans to create a comprehensive WiFi fingerprinting dataset.\nThe created dataset was used to train a deep neural network (DNN) for indoor\nlocalization, which can prove the usefulness of grid density. We conducted\nexperimental validation within our office environment to demonstrate the\nproposed method's effectiveness, including a heatmap from the dataset\nshowcasing the spatial distribution of WiFi signal strengths for the testing\naccess points placed within the environment. Notably, our method offers\ndistinct advantages over existing approaches as it eliminates the need for a\npredefined map of the environment, requires no preparatory steps, lessens human\nintervention, creates a denser fingerprinting dataset, and reduces the WiFi\nfingerprinting dataset creation time. Our method achieves 26% more accurate\nlocalization than the other methods and can create a six times denser\nfingerprinting dataset in one-third of the time compared to the traditional\nmethod. In summary, using WiFi RSSI Fingerprinting data surveyed by the\nSLAM-Enabled Robotic Platform, we can adapt our trained DNN model to indoor\nlocalization in any dynamic environment and enhance its scalability and\napplicability in real-world scenarios.\n", "link": "http://arxiv.org/abs/2407.09242v2", "date": "2024-07-30", "relevancy": 2.2487, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.591}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5449}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Indoor%20Localization%20Approach%20Using%20WiFi%20RSSI%20Fingerprinting%0A%20%20with%20SLAM-Enabled%20Robotic%20Platform%20and%20Deep%20Neural%20Networks&body=Title%3A%20An%20Adaptive%20Indoor%20Localization%20Approach%20Using%20WiFi%20RSSI%20Fingerprinting%0A%20%20with%20SLAM-Enabled%20Robotic%20Platform%20and%20Deep%20Neural%20Networks%0AAuthor%3A%20Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Atah%20Nuh%20Mih%20and%20Asfia%20Kawnine%20and%20Monica%20Wachowicz%20and%20Francis%20Palma%20and%20Hung%20Cao%0AAbstract%3A%20%20%20Indoor%20localization%20plays%20a%20vital%20role%20in%20the%20era%20of%20the%20IoT%20and%20robotics%2C%0Awith%20WiFi%20technology%20being%20a%20prominent%20choice%20due%20to%20its%20ubiquity.%20We%20present%20a%0Amethod%20for%20creating%20WiFi%20fingerprinting%20datasets%20to%20enhance%20indoor%20localization%0Asystems%20and%20address%20the%20gap%20in%20WiFi%20fingerprinting%20dataset%20creation.%20We%20used%0Athe%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20algorithm%20and%20employed%20a%0Arobotic%20platform%20to%20construct%20precise%20maps%20and%20localize%20robots%20in%20indoor%0Aenvironments.%20We%20developed%20software%20applications%20to%20facilitate%20data%0Aacquisition%2C%20fingerprinting%20dataset%20collection%2C%20and%20accurate%20ground%20truth%20map%0Abuilding.%20Subsequently%2C%20we%20aligned%20the%20spatial%20information%20generated%20via%20the%0ASLAM%20with%20the%20WiFi%20scans%20to%20create%20a%20comprehensive%20WiFi%20fingerprinting%20dataset.%0AThe%20created%20dataset%20was%20used%20to%20train%20a%20deep%20neural%20network%20%28DNN%29%20for%20indoor%0Alocalization%2C%20which%20can%20prove%20the%20usefulness%20of%20grid%20density.%20We%20conducted%0Aexperimental%20validation%20within%20our%20office%20environment%20to%20demonstrate%20the%0Aproposed%20method%27s%20effectiveness%2C%20including%20a%20heatmap%20from%20the%20dataset%0Ashowcasing%20the%20spatial%20distribution%20of%20WiFi%20signal%20strengths%20for%20the%20testing%0Aaccess%20points%20placed%20within%20the%20environment.%20Notably%2C%20our%20method%20offers%0Adistinct%20advantages%20over%20existing%20approaches%20as%20it%20eliminates%20the%20need%20for%20a%0Apredefined%20map%20of%20the%20environment%2C%20requires%20no%20preparatory%20steps%2C%20lessens%20human%0Aintervention%2C%20creates%20a%20denser%20fingerprinting%20dataset%2C%20and%20reduces%20the%20WiFi%0Afingerprinting%20dataset%20creation%20time.%20Our%20method%20achieves%2026%25%20more%20accurate%0Alocalization%20than%20the%20other%20methods%20and%20can%20create%20a%20six%20times%20denser%0Afingerprinting%20dataset%20in%20one-third%20of%20the%20time%20compared%20to%20the%20traditional%0Amethod.%20In%20summary%2C%20using%20WiFi%20RSSI%20Fingerprinting%20data%20surveyed%20by%20the%0ASLAM-Enabled%20Robotic%20Platform%2C%20we%20can%20adapt%20our%20trained%20DNN%20model%20to%20indoor%0Alocalization%20in%20any%20dynamic%20environment%20and%20enhance%20its%20scalability%20and%0Aapplicability%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Indoor%2520Localization%2520Approach%2520Using%2520WiFi%2520RSSI%2520Fingerprinting%250A%2520%2520with%2520SLAM-Enabled%2520Robotic%2520Platform%2520and%2520Deep%2520Neural%2520Networks%26entry.906535625%3DSeyed%2520Alireza%2520Rahimi%2520Azghadi%2520and%2520Atah%2520Nuh%2520Mih%2520and%2520Asfia%2520Kawnine%2520and%2520Monica%2520Wachowicz%2520and%2520Francis%2520Palma%2520and%2520Hung%2520Cao%26entry.1292438233%3D%2520%2520Indoor%2520localization%2520plays%2520a%2520vital%2520role%2520in%2520the%2520era%2520of%2520the%2520IoT%2520and%2520robotics%252C%250Awith%2520WiFi%2520technology%2520being%2520a%2520prominent%2520choice%2520due%2520to%2520its%2520ubiquity.%2520We%2520present%2520a%250Amethod%2520for%2520creating%2520WiFi%2520fingerprinting%2520datasets%2520to%2520enhance%2520indoor%2520localization%250Asystems%2520and%2520address%2520the%2520gap%2520in%2520WiFi%2520fingerprinting%2520dataset%2520creation.%2520We%2520used%250Athe%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528SLAM%2529%2520algorithm%2520and%2520employed%2520a%250Arobotic%2520platform%2520to%2520construct%2520precise%2520maps%2520and%2520localize%2520robots%2520in%2520indoor%250Aenvironments.%2520We%2520developed%2520software%2520applications%2520to%2520facilitate%2520data%250Aacquisition%252C%2520fingerprinting%2520dataset%2520collection%252C%2520and%2520accurate%2520ground%2520truth%2520map%250Abuilding.%2520Subsequently%252C%2520we%2520aligned%2520the%2520spatial%2520information%2520generated%2520via%2520the%250ASLAM%2520with%2520the%2520WiFi%2520scans%2520to%2520create%2520a%2520comprehensive%2520WiFi%2520fingerprinting%2520dataset.%250AThe%2520created%2520dataset%2520was%2520used%2520to%2520train%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%2520for%2520indoor%250Alocalization%252C%2520which%2520can%2520prove%2520the%2520usefulness%2520of%2520grid%2520density.%2520We%2520conducted%250Aexperimental%2520validation%2520within%2520our%2520office%2520environment%2520to%2520demonstrate%2520the%250Aproposed%2520method%2527s%2520effectiveness%252C%2520including%2520a%2520heatmap%2520from%2520the%2520dataset%250Ashowcasing%2520the%2520spatial%2520distribution%2520of%2520WiFi%2520signal%2520strengths%2520for%2520the%2520testing%250Aaccess%2520points%2520placed%2520within%2520the%2520environment.%2520Notably%252C%2520our%2520method%2520offers%250Adistinct%2520advantages%2520over%2520existing%2520approaches%2520as%2520it%2520eliminates%2520the%2520need%2520for%2520a%250Apredefined%2520map%2520of%2520the%2520environment%252C%2520requires%2520no%2520preparatory%2520steps%252C%2520lessens%2520human%250Aintervention%252C%2520creates%2520a%2520denser%2520fingerprinting%2520dataset%252C%2520and%2520reduces%2520the%2520WiFi%250Afingerprinting%2520dataset%2520creation%2520time.%2520Our%2520method%2520achieves%252026%2525%2520more%2520accurate%250Alocalization%2520than%2520the%2520other%2520methods%2520and%2520can%2520create%2520a%2520six%2520times%2520denser%250Afingerprinting%2520dataset%2520in%2520one-third%2520of%2520the%2520time%2520compared%2520to%2520the%2520traditional%250Amethod.%2520In%2520summary%252C%2520using%2520WiFi%2520RSSI%2520Fingerprinting%2520data%2520surveyed%2520by%2520the%250ASLAM-Enabled%2520Robotic%2520Platform%252C%2520we%2520can%2520adapt%2520our%2520trained%2520DNN%2520model%2520to%2520indoor%250Alocalization%2520in%2520any%2520dynamic%2520environment%2520and%2520enhance%2520its%2520scalability%2520and%250Aapplicability%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Indoor%20Localization%20Approach%20Using%20WiFi%20RSSI%20Fingerprinting%0A%20%20with%20SLAM-Enabled%20Robotic%20Platform%20and%20Deep%20Neural%20Networks&entry.906535625=Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Atah%20Nuh%20Mih%20and%20Asfia%20Kawnine%20and%20Monica%20Wachowicz%20and%20Francis%20Palma%20and%20Hung%20Cao&entry.1292438233=%20%20Indoor%20localization%20plays%20a%20vital%20role%20in%20the%20era%20of%20the%20IoT%20and%20robotics%2C%0Awith%20WiFi%20technology%20being%20a%20prominent%20choice%20due%20to%20its%20ubiquity.%20We%20present%20a%0Amethod%20for%20creating%20WiFi%20fingerprinting%20datasets%20to%20enhance%20indoor%20localization%0Asystems%20and%20address%20the%20gap%20in%20WiFi%20fingerprinting%20dataset%20creation.%20We%20used%0Athe%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20algorithm%20and%20employed%20a%0Arobotic%20platform%20to%20construct%20precise%20maps%20and%20localize%20robots%20in%20indoor%0Aenvironments.%20We%20developed%20software%20applications%20to%20facilitate%20data%0Aacquisition%2C%20fingerprinting%20dataset%20collection%2C%20and%20accurate%20ground%20truth%20map%0Abuilding.%20Subsequently%2C%20we%20aligned%20the%20spatial%20information%20generated%20via%20the%0ASLAM%20with%20the%20WiFi%20scans%20to%20create%20a%20comprehensive%20WiFi%20fingerprinting%20dataset.%0AThe%20created%20dataset%20was%20used%20to%20train%20a%20deep%20neural%20network%20%28DNN%29%20for%20indoor%0Alocalization%2C%20which%20can%20prove%20the%20usefulness%20of%20grid%20density.%20We%20conducted%0Aexperimental%20validation%20within%20our%20office%20environment%20to%20demonstrate%20the%0Aproposed%20method%27s%20effectiveness%2C%20including%20a%20heatmap%20from%20the%20dataset%0Ashowcasing%20the%20spatial%20distribution%20of%20WiFi%20signal%20strengths%20for%20the%20testing%0Aaccess%20points%20placed%20within%20the%20environment.%20Notably%2C%20our%20method%20offers%0Adistinct%20advantages%20over%20existing%20approaches%20as%20it%20eliminates%20the%20need%20for%20a%0Apredefined%20map%20of%20the%20environment%2C%20requires%20no%20preparatory%20steps%2C%20lessens%20human%0Aintervention%2C%20creates%20a%20denser%20fingerprinting%20dataset%2C%20and%20reduces%20the%20WiFi%0Afingerprinting%20dataset%20creation%20time.%20Our%20method%20achieves%2026%25%20more%20accurate%0Alocalization%20than%20the%20other%20methods%20and%20can%20create%20a%20six%20times%20denser%0Afingerprinting%20dataset%20in%20one-third%20of%20the%20time%20compared%20to%20the%20traditional%0Amethod.%20In%20summary%2C%20using%20WiFi%20RSSI%20Fingerprinting%20data%20surveyed%20by%20the%0ASLAM-Enabled%20Robotic%20Platform%2C%20we%20can%20adapt%20our%20trained%20DNN%20model%20to%20indoor%0Alocalization%20in%20any%20dynamic%20environment%20and%20enhance%20its%20scalability%20and%0Aapplicability%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09242v2&entry.124074799=Read"},
{"title": "SceneX:Procedural Controllable Large-scale Scene Generation via\n  Large-language Models", "author": "Mengqi Zhou and Yuxi Wang and Jun Hou and Chuanchen Luo and Zhaoxiang Zhang and Junran Peng", "abstract": "  Due to its great application potential, large-scale scene generation has\ndrawn extensive attention in academia and industry. Recent research employs\npowerful generative models to create desired scenes and achieves promising\nresults. However, most of these methods represent the scene using 3D primitives\n(e.g. point cloud or radiance field) incompatible with the industrial pipeline,\nwhich leads to a substantial gap between academic research and industrial\ndeployment. Procedural Controllable Generation (PCG) is an efficient technique\nfor creating scalable and high-quality assets, but it is unfriendly for\nordinary users as it demands profound domain expertise. To address these\nissues, we resort to using the large language model (LLM) to drive the\nprocedural modeling. In this paper, we introduce a large-scale scene generation\nframework, SceneX, which can automatically produce high-quality procedural\nmodels according to designers' textual descriptions.Specifically, the proposed\nmethod comprises two components, PCGBench and PCGPlanner. The former\nencompasses an extensive collection of accessible procedural assets and\nthousands of hand-craft API documents. The latter aims to generate executable\nactions for Blender to produce controllable and precise 3D assets guided by the\nuser's instructions. Our SceneX can generate a city spanning 2.5 km times 2.5\nkm with delicate layout and geometric structures, drastically reducing the time\ncost from several weeks for professional PCG engineers to just a few hours for\nan ordinary user. Extensive experiments demonstrated the capability of our\nmethod in controllable large-scale scene generation and editing, including\nasset placement and season translation.\n", "link": "http://arxiv.org/abs/2403.15698v2", "date": "2024-07-30", "relevancy": 2.2393, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6394}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5457}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneX%3AProcedural%20Controllable%20Large-scale%20Scene%20Generation%20via%0A%20%20Large-language%20Models&body=Title%3A%20SceneX%3AProcedural%20Controllable%20Large-scale%20Scene%20Generation%20via%0A%20%20Large-language%20Models%0AAuthor%3A%20Mengqi%20Zhou%20and%20Yuxi%20Wang%20and%20Jun%20Hou%20and%20Chuanchen%20Luo%20and%20Zhaoxiang%20Zhang%20and%20Junran%20Peng%0AAbstract%3A%20%20%20Due%20to%20its%20great%20application%20potential%2C%20large-scale%20scene%20generation%20has%0Adrawn%20extensive%20attention%20in%20academia%20and%20industry.%20Recent%20research%20employs%0Apowerful%20generative%20models%20to%20create%20desired%20scenes%20and%20achieves%20promising%0Aresults.%20However%2C%20most%20of%20these%20methods%20represent%20the%20scene%20using%203D%20primitives%0A%28e.g.%20point%20cloud%20or%20radiance%20field%29%20incompatible%20with%20the%20industrial%20pipeline%2C%0Awhich%20leads%20to%20a%20substantial%20gap%20between%20academic%20research%20and%20industrial%0Adeployment.%20Procedural%20Controllable%20Generation%20%28PCG%29%20is%20an%20efficient%20technique%0Afor%20creating%20scalable%20and%20high-quality%20assets%2C%20but%20it%20is%20unfriendly%20for%0Aordinary%20users%20as%20it%20demands%20profound%20domain%20expertise.%20To%20address%20these%0Aissues%2C%20we%20resort%20to%20using%20the%20large%20language%20model%20%28LLM%29%20to%20drive%20the%0Aprocedural%20modeling.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20scene%20generation%0Aframework%2C%20SceneX%2C%20which%20can%20automatically%20produce%20high-quality%20procedural%0Amodels%20according%20to%20designers%27%20textual%20descriptions.Specifically%2C%20the%20proposed%0Amethod%20comprises%20two%20components%2C%20PCGBench%20and%20PCGPlanner.%20The%20former%0Aencompasses%20an%20extensive%20collection%20of%20accessible%20procedural%20assets%20and%0Athousands%20of%20hand-craft%20API%20documents.%20The%20latter%20aims%20to%20generate%20executable%0Aactions%20for%20Blender%20to%20produce%20controllable%20and%20precise%203D%20assets%20guided%20by%20the%0Auser%27s%20instructions.%20Our%20SceneX%20can%20generate%20a%20city%20spanning%202.5%20km%20times%202.5%0Akm%20with%20delicate%20layout%20and%20geometric%20structures%2C%20drastically%20reducing%20the%20time%0Acost%20from%20several%20weeks%20for%20professional%20PCG%20engineers%20to%20just%20a%20few%20hours%20for%0Aan%20ordinary%20user.%20Extensive%20experiments%20demonstrated%20the%20capability%20of%20our%0Amethod%20in%20controllable%20large-scale%20scene%20generation%20and%20editing%2C%20including%0Aasset%20placement%20and%20season%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneX%253AProcedural%2520Controllable%2520Large-scale%2520Scene%2520Generation%2520via%250A%2520%2520Large-language%2520Models%26entry.906535625%3DMengqi%2520Zhou%2520and%2520Yuxi%2520Wang%2520and%2520Jun%2520Hou%2520and%2520Chuanchen%2520Luo%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Junran%2520Peng%26entry.1292438233%3D%2520%2520Due%2520to%2520its%2520great%2520application%2520potential%252C%2520large-scale%2520scene%2520generation%2520has%250Adrawn%2520extensive%2520attention%2520in%2520academia%2520and%2520industry.%2520Recent%2520research%2520employs%250Apowerful%2520generative%2520models%2520to%2520create%2520desired%2520scenes%2520and%2520achieves%2520promising%250Aresults.%2520However%252C%2520most%2520of%2520these%2520methods%2520represent%2520the%2520scene%2520using%25203D%2520primitives%250A%2528e.g.%2520point%2520cloud%2520or%2520radiance%2520field%2529%2520incompatible%2520with%2520the%2520industrial%2520pipeline%252C%250Awhich%2520leads%2520to%2520a%2520substantial%2520gap%2520between%2520academic%2520research%2520and%2520industrial%250Adeployment.%2520Procedural%2520Controllable%2520Generation%2520%2528PCG%2529%2520is%2520an%2520efficient%2520technique%250Afor%2520creating%2520scalable%2520and%2520high-quality%2520assets%252C%2520but%2520it%2520is%2520unfriendly%2520for%250Aordinary%2520users%2520as%2520it%2520demands%2520profound%2520domain%2520expertise.%2520To%2520address%2520these%250Aissues%252C%2520we%2520resort%2520to%2520using%2520the%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520drive%2520the%250Aprocedural%2520modeling.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520large-scale%2520scene%2520generation%250Aframework%252C%2520SceneX%252C%2520which%2520can%2520automatically%2520produce%2520high-quality%2520procedural%250Amodels%2520according%2520to%2520designers%2527%2520textual%2520descriptions.Specifically%252C%2520the%2520proposed%250Amethod%2520comprises%2520two%2520components%252C%2520PCGBench%2520and%2520PCGPlanner.%2520The%2520former%250Aencompasses%2520an%2520extensive%2520collection%2520of%2520accessible%2520procedural%2520assets%2520and%250Athousands%2520of%2520hand-craft%2520API%2520documents.%2520The%2520latter%2520aims%2520to%2520generate%2520executable%250Aactions%2520for%2520Blender%2520to%2520produce%2520controllable%2520and%2520precise%25203D%2520assets%2520guided%2520by%2520the%250Auser%2527s%2520instructions.%2520Our%2520SceneX%2520can%2520generate%2520a%2520city%2520spanning%25202.5%2520km%2520times%25202.5%250Akm%2520with%2520delicate%2520layout%2520and%2520geometric%2520structures%252C%2520drastically%2520reducing%2520the%2520time%250Acost%2520from%2520several%2520weeks%2520for%2520professional%2520PCG%2520engineers%2520to%2520just%2520a%2520few%2520hours%2520for%250Aan%2520ordinary%2520user.%2520Extensive%2520experiments%2520demonstrated%2520the%2520capability%2520of%2520our%250Amethod%2520in%2520controllable%2520large-scale%2520scene%2520generation%2520and%2520editing%252C%2520including%250Aasset%2520placement%2520and%2520season%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneX%3AProcedural%20Controllable%20Large-scale%20Scene%20Generation%20via%0A%20%20Large-language%20Models&entry.906535625=Mengqi%20Zhou%20and%20Yuxi%20Wang%20and%20Jun%20Hou%20and%20Chuanchen%20Luo%20and%20Zhaoxiang%20Zhang%20and%20Junran%20Peng&entry.1292438233=%20%20Due%20to%20its%20great%20application%20potential%2C%20large-scale%20scene%20generation%20has%0Adrawn%20extensive%20attention%20in%20academia%20and%20industry.%20Recent%20research%20employs%0Apowerful%20generative%20models%20to%20create%20desired%20scenes%20and%20achieves%20promising%0Aresults.%20However%2C%20most%20of%20these%20methods%20represent%20the%20scene%20using%203D%20primitives%0A%28e.g.%20point%20cloud%20or%20radiance%20field%29%20incompatible%20with%20the%20industrial%20pipeline%2C%0Awhich%20leads%20to%20a%20substantial%20gap%20between%20academic%20research%20and%20industrial%0Adeployment.%20Procedural%20Controllable%20Generation%20%28PCG%29%20is%20an%20efficient%20technique%0Afor%20creating%20scalable%20and%20high-quality%20assets%2C%20but%20it%20is%20unfriendly%20for%0Aordinary%20users%20as%20it%20demands%20profound%20domain%20expertise.%20To%20address%20these%0Aissues%2C%20we%20resort%20to%20using%20the%20large%20language%20model%20%28LLM%29%20to%20drive%20the%0Aprocedural%20modeling.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20scene%20generation%0Aframework%2C%20SceneX%2C%20which%20can%20automatically%20produce%20high-quality%20procedural%0Amodels%20according%20to%20designers%27%20textual%20descriptions.Specifically%2C%20the%20proposed%0Amethod%20comprises%20two%20components%2C%20PCGBench%20and%20PCGPlanner.%20The%20former%0Aencompasses%20an%20extensive%20collection%20of%20accessible%20procedural%20assets%20and%0Athousands%20of%20hand-craft%20API%20documents.%20The%20latter%20aims%20to%20generate%20executable%0Aactions%20for%20Blender%20to%20produce%20controllable%20and%20precise%203D%20assets%20guided%20by%20the%0Auser%27s%20instructions.%20Our%20SceneX%20can%20generate%20a%20city%20spanning%202.5%20km%20times%202.5%0Akm%20with%20delicate%20layout%20and%20geometric%20structures%2C%20drastically%20reducing%20the%20time%0Acost%20from%20several%20weeks%20for%20professional%20PCG%20engineers%20to%20just%20a%20few%20hours%20for%0Aan%20ordinary%20user.%20Extensive%20experiments%20demonstrated%20the%20capability%20of%20our%0Amethod%20in%20controllable%20large-scale%20scene%20generation%20and%20editing%2C%20including%0Aasset%20placement%20and%20season%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15698v2&entry.124074799=Read"},
{"title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens", "author": "Gagan Jain and Nidhi Hegde and Aditya Kusupati and Arsha Nagrani and Shyamal Buch and Prateek Jain and Anurag Arnab and Sujoy Paul", "abstract": "  The visual medium (images and videos) naturally contains a large amount of\ninformation redundancy, thereby providing a great opportunity for leveraging\nefficiency in processing. While Vision Transformer (ViT) based models scale\neffectively to large data regimes, they fail to capitalize on this inherent\nredundancy, leading to higher computational costs. Mixture of Experts (MoE)\nnetworks demonstrate scalability while maintaining same inference-time costs,\nbut they come with a larger parameter footprint. We present Mixture of Nested\nExperts (MoNE), which utilizes a nested structure for experts, wherein\nindividual experts fall on an increasing compute-accuracy curve. Given a\ncompute budget, MoNE learns to dynamically choose tokens in a priority order,\nand thus redundant tokens are processed through cheaper nested experts. Using\nthis framework, we achieve equivalent performance as the baseline models, while\nreducing inference time compute by over two-fold. We validate our approach on\nstandard image and video datasets - ImageNet-21K, Kinetics400, and\nSomething-Something-v2. We further highlight MoNE$'$s adaptability by\nshowcasing its ability to maintain strong performance across different\ninference-time compute budgets on videos, using only a single trained model.\n", "link": "http://arxiv.org/abs/2407.19985v2", "date": "2024-07-30", "relevancy": 2.2343, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5701}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5525}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Nested%20Experts%3A%20Adaptive%20Processing%20of%20Visual%20Tokens&body=Title%3A%20Mixture%20of%20Nested%20Experts%3A%20Adaptive%20Processing%20of%20Visual%20Tokens%0AAuthor%3A%20Gagan%20Jain%20and%20Nidhi%20Hegde%20and%20Aditya%20Kusupati%20and%20Arsha%20Nagrani%20and%20Shyamal%20Buch%20and%20Prateek%20Jain%20and%20Anurag%20Arnab%20and%20Sujoy%20Paul%0AAbstract%3A%20%20%20The%20visual%20medium%20%28images%20and%20videos%29%20naturally%20contains%20a%20large%20amount%20of%0Ainformation%20redundancy%2C%20thereby%20providing%20a%20great%20opportunity%20for%20leveraging%0Aefficiency%20in%20processing.%20While%20Vision%20Transformer%20%28ViT%29%20based%20models%20scale%0Aeffectively%20to%20large%20data%20regimes%2C%20they%20fail%20to%20capitalize%20on%20this%20inherent%0Aredundancy%2C%20leading%20to%20higher%20computational%20costs.%20Mixture%20of%20Experts%20%28MoE%29%0Anetworks%20demonstrate%20scalability%20while%20maintaining%20same%20inference-time%20costs%2C%0Abut%20they%20come%20with%20a%20larger%20parameter%20footprint.%20We%20present%20Mixture%20of%20Nested%0AExperts%20%28MoNE%29%2C%20which%20utilizes%20a%20nested%20structure%20for%20experts%2C%20wherein%0Aindividual%20experts%20fall%20on%20an%20increasing%20compute-accuracy%20curve.%20Given%20a%0Acompute%20budget%2C%20MoNE%20learns%20to%20dynamically%20choose%20tokens%20in%20a%20priority%20order%2C%0Aand%20thus%20redundant%20tokens%20are%20processed%20through%20cheaper%20nested%20experts.%20Using%0Athis%20framework%2C%20we%20achieve%20equivalent%20performance%20as%20the%20baseline%20models%2C%20while%0Areducing%20inference%20time%20compute%20by%20over%20two-fold.%20We%20validate%20our%20approach%20on%0Astandard%20image%20and%20video%20datasets%20-%20ImageNet-21K%2C%20Kinetics400%2C%20and%0ASomething-Something-v2.%20We%20further%20highlight%20MoNE%24%27%24s%20adaptability%20by%0Ashowcasing%20its%20ability%20to%20maintain%20strong%20performance%20across%20different%0Ainference-time%20compute%20budgets%20on%20videos%2C%20using%20only%20a%20single%20trained%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19985v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Nested%2520Experts%253A%2520Adaptive%2520Processing%2520of%2520Visual%2520Tokens%26entry.906535625%3DGagan%2520Jain%2520and%2520Nidhi%2520Hegde%2520and%2520Aditya%2520Kusupati%2520and%2520Arsha%2520Nagrani%2520and%2520Shyamal%2520Buch%2520and%2520Prateek%2520Jain%2520and%2520Anurag%2520Arnab%2520and%2520Sujoy%2520Paul%26entry.1292438233%3D%2520%2520The%2520visual%2520medium%2520%2528images%2520and%2520videos%2529%2520naturally%2520contains%2520a%2520large%2520amount%2520of%250Ainformation%2520redundancy%252C%2520thereby%2520providing%2520a%2520great%2520opportunity%2520for%2520leveraging%250Aefficiency%2520in%2520processing.%2520While%2520Vision%2520Transformer%2520%2528ViT%2529%2520based%2520models%2520scale%250Aeffectively%2520to%2520large%2520data%2520regimes%252C%2520they%2520fail%2520to%2520capitalize%2520on%2520this%2520inherent%250Aredundancy%252C%2520leading%2520to%2520higher%2520computational%2520costs.%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%250Anetworks%2520demonstrate%2520scalability%2520while%2520maintaining%2520same%2520inference-time%2520costs%252C%250Abut%2520they%2520come%2520with%2520a%2520larger%2520parameter%2520footprint.%2520We%2520present%2520Mixture%2520of%2520Nested%250AExperts%2520%2528MoNE%2529%252C%2520which%2520utilizes%2520a%2520nested%2520structure%2520for%2520experts%252C%2520wherein%250Aindividual%2520experts%2520fall%2520on%2520an%2520increasing%2520compute-accuracy%2520curve.%2520Given%2520a%250Acompute%2520budget%252C%2520MoNE%2520learns%2520to%2520dynamically%2520choose%2520tokens%2520in%2520a%2520priority%2520order%252C%250Aand%2520thus%2520redundant%2520tokens%2520are%2520processed%2520through%2520cheaper%2520nested%2520experts.%2520Using%250Athis%2520framework%252C%2520we%2520achieve%2520equivalent%2520performance%2520as%2520the%2520baseline%2520models%252C%2520while%250Areducing%2520inference%2520time%2520compute%2520by%2520over%2520two-fold.%2520We%2520validate%2520our%2520approach%2520on%250Astandard%2520image%2520and%2520video%2520datasets%2520-%2520ImageNet-21K%252C%2520Kinetics400%252C%2520and%250ASomething-Something-v2.%2520We%2520further%2520highlight%2520MoNE%2524%2527%2524s%2520adaptability%2520by%250Ashowcasing%2520its%2520ability%2520to%2520maintain%2520strong%2520performance%2520across%2520different%250Ainference-time%2520compute%2520budgets%2520on%2520videos%252C%2520using%2520only%2520a%2520single%2520trained%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19985v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Nested%20Experts%3A%20Adaptive%20Processing%20of%20Visual%20Tokens&entry.906535625=Gagan%20Jain%20and%20Nidhi%20Hegde%20and%20Aditya%20Kusupati%20and%20Arsha%20Nagrani%20and%20Shyamal%20Buch%20and%20Prateek%20Jain%20and%20Anurag%20Arnab%20and%20Sujoy%20Paul&entry.1292438233=%20%20The%20visual%20medium%20%28images%20and%20videos%29%20naturally%20contains%20a%20large%20amount%20of%0Ainformation%20redundancy%2C%20thereby%20providing%20a%20great%20opportunity%20for%20leveraging%0Aefficiency%20in%20processing.%20While%20Vision%20Transformer%20%28ViT%29%20based%20models%20scale%0Aeffectively%20to%20large%20data%20regimes%2C%20they%20fail%20to%20capitalize%20on%20this%20inherent%0Aredundancy%2C%20leading%20to%20higher%20computational%20costs.%20Mixture%20of%20Experts%20%28MoE%29%0Anetworks%20demonstrate%20scalability%20while%20maintaining%20same%20inference-time%20costs%2C%0Abut%20they%20come%20with%20a%20larger%20parameter%20footprint.%20We%20present%20Mixture%20of%20Nested%0AExperts%20%28MoNE%29%2C%20which%20utilizes%20a%20nested%20structure%20for%20experts%2C%20wherein%0Aindividual%20experts%20fall%20on%20an%20increasing%20compute-accuracy%20curve.%20Given%20a%0Acompute%20budget%2C%20MoNE%20learns%20to%20dynamically%20choose%20tokens%20in%20a%20priority%20order%2C%0Aand%20thus%20redundant%20tokens%20are%20processed%20through%20cheaper%20nested%20experts.%20Using%0Athis%20framework%2C%20we%20achieve%20equivalent%20performance%20as%20the%20baseline%20models%2C%20while%0Areducing%20inference%20time%20compute%20by%20over%20two-fold.%20We%20validate%20our%20approach%20on%0Astandard%20image%20and%20video%20datasets%20-%20ImageNet-21K%2C%20Kinetics400%2C%20and%0ASomething-Something-v2.%20We%20further%20highlight%20MoNE%24%27%24s%20adaptability%20by%0Ashowcasing%20its%20ability%20to%20maintain%20strong%20performance%20across%20different%0Ainference-time%20compute%20budgets%20on%20videos%2C%20using%20only%20a%20single%20trained%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19985v2&entry.124074799=Read"},
{"title": "A Unified Formulation of Geometry-aware Dynamic Movement Primitives", "author": "Fares J. Abu-Dakka and Matteo Saveriano and Ville Kyrki", "abstract": "  Learning from demonstration (LfD) is considered as an efficient way to\ntransfer skills from humans to robots. Traditionally, LfD has been used to\ntransfer Cartesian and joint positions and forces from human demonstrations.\nThe traditional approach works well for some robotic tasks, but for many tasks\nof interest, it is necessary to learn skills such as orientation, impedance,\nand/or manipulability that have specific geometric characteristics. An\neffective encoding of such skills can be only achieved if the underlying\ngeometric structure of the skill manifold is considered and the constrains\narising from this structure are fulfilled during both learning and execution.\nHowever, typical learned skill models such as dynamic movement primitives\n(DMPs) are limited to Euclidean data and fail in correctly embedding quantities\nwith geometric constraints. In this paper, we propose a novel and\nmathematically principled framework that uses concepts from Riemannian geometry\nto allow DMPs to properly embed geometric constrains. The resulting DMP\nformulation can deal with data sampled from any Riemannian manifold including,\nbut not limited to, unit quaternions and symmetric and positive definite\nmatrices. The proposed approach has been extensively evaluated both on\nsimulated data and real robot experiments. The performed evaluation\ndemonstrates that beneficial properties of DMPs, such as convergence to a given\ngoal and the possibility to change the goal during operation, apply also to the\nproposed formulation.\n", "link": "http://arxiv.org/abs/2203.03374v3", "date": "2024-07-30", "relevancy": 2.2226, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5615}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Formulation%20of%20Geometry-aware%20Dynamic%20Movement%20Primitives&body=Title%3A%20A%20Unified%20Formulation%20of%20Geometry-aware%20Dynamic%20Movement%20Primitives%0AAuthor%3A%20Fares%20J.%20Abu-Dakka%20and%20Matteo%20Saveriano%20and%20Ville%20Kyrki%0AAbstract%3A%20%20%20Learning%20from%20demonstration%20%28LfD%29%20is%20considered%20as%20an%20efficient%20way%20to%0Atransfer%20skills%20from%20humans%20to%20robots.%20Traditionally%2C%20LfD%20has%20been%20used%20to%0Atransfer%20Cartesian%20and%20joint%20positions%20and%20forces%20from%20human%20demonstrations.%0AThe%20traditional%20approach%20works%20well%20for%20some%20robotic%20tasks%2C%20but%20for%20many%20tasks%0Aof%20interest%2C%20it%20is%20necessary%20to%20learn%20skills%20such%20as%20orientation%2C%20impedance%2C%0Aand/or%20manipulability%20that%20have%20specific%20geometric%20characteristics.%20An%0Aeffective%20encoding%20of%20such%20skills%20can%20be%20only%20achieved%20if%20the%20underlying%0Ageometric%20structure%20of%20the%20skill%20manifold%20is%20considered%20and%20the%20constrains%0Aarising%20from%20this%20structure%20are%20fulfilled%20during%20both%20learning%20and%20execution.%0AHowever%2C%20typical%20learned%20skill%20models%20such%20as%20dynamic%20movement%20primitives%0A%28DMPs%29%20are%20limited%20to%20Euclidean%20data%20and%20fail%20in%20correctly%20embedding%20quantities%0Awith%20geometric%20constraints.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%0Amathematically%20principled%20framework%20that%20uses%20concepts%20from%20Riemannian%20geometry%0Ato%20allow%20DMPs%20to%20properly%20embed%20geometric%20constrains.%20The%20resulting%20DMP%0Aformulation%20can%20deal%20with%20data%20sampled%20from%20any%20Riemannian%20manifold%20including%2C%0Abut%20not%20limited%20to%2C%20unit%20quaternions%20and%20symmetric%20and%20positive%20definite%0Amatrices.%20The%20proposed%20approach%20has%20been%20extensively%20evaluated%20both%20on%0Asimulated%20data%20and%20real%20robot%20experiments.%20The%20performed%20evaluation%0Ademonstrates%20that%20beneficial%20properties%20of%20DMPs%2C%20such%20as%20convergence%20to%20a%20given%0Agoal%20and%20the%20possibility%20to%20change%20the%20goal%20during%20operation%2C%20apply%20also%20to%20the%0Aproposed%20formulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.03374v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Formulation%2520of%2520Geometry-aware%2520Dynamic%2520Movement%2520Primitives%26entry.906535625%3DFares%2520J.%2520Abu-Dakka%2520and%2520Matteo%2520Saveriano%2520and%2520Ville%2520Kyrki%26entry.1292438233%3D%2520%2520Learning%2520from%2520demonstration%2520%2528LfD%2529%2520is%2520considered%2520as%2520an%2520efficient%2520way%2520to%250Atransfer%2520skills%2520from%2520humans%2520to%2520robots.%2520Traditionally%252C%2520LfD%2520has%2520been%2520used%2520to%250Atransfer%2520Cartesian%2520and%2520joint%2520positions%2520and%2520forces%2520from%2520human%2520demonstrations.%250AThe%2520traditional%2520approach%2520works%2520well%2520for%2520some%2520robotic%2520tasks%252C%2520but%2520for%2520many%2520tasks%250Aof%2520interest%252C%2520it%2520is%2520necessary%2520to%2520learn%2520skills%2520such%2520as%2520orientation%252C%2520impedance%252C%250Aand/or%2520manipulability%2520that%2520have%2520specific%2520geometric%2520characteristics.%2520An%250Aeffective%2520encoding%2520of%2520such%2520skills%2520can%2520be%2520only%2520achieved%2520if%2520the%2520underlying%250Ageometric%2520structure%2520of%2520the%2520skill%2520manifold%2520is%2520considered%2520and%2520the%2520constrains%250Aarising%2520from%2520this%2520structure%2520are%2520fulfilled%2520during%2520both%2520learning%2520and%2520execution.%250AHowever%252C%2520typical%2520learned%2520skill%2520models%2520such%2520as%2520dynamic%2520movement%2520primitives%250A%2528DMPs%2529%2520are%2520limited%2520to%2520Euclidean%2520data%2520and%2520fail%2520in%2520correctly%2520embedding%2520quantities%250Awith%2520geometric%2520constraints.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%250Amathematically%2520principled%2520framework%2520that%2520uses%2520concepts%2520from%2520Riemannian%2520geometry%250Ato%2520allow%2520DMPs%2520to%2520properly%2520embed%2520geometric%2520constrains.%2520The%2520resulting%2520DMP%250Aformulation%2520can%2520deal%2520with%2520data%2520sampled%2520from%2520any%2520Riemannian%2520manifold%2520including%252C%250Abut%2520not%2520limited%2520to%252C%2520unit%2520quaternions%2520and%2520symmetric%2520and%2520positive%2520definite%250Amatrices.%2520The%2520proposed%2520approach%2520has%2520been%2520extensively%2520evaluated%2520both%2520on%250Asimulated%2520data%2520and%2520real%2520robot%2520experiments.%2520The%2520performed%2520evaluation%250Ademonstrates%2520that%2520beneficial%2520properties%2520of%2520DMPs%252C%2520such%2520as%2520convergence%2520to%2520a%2520given%250Agoal%2520and%2520the%2520possibility%2520to%2520change%2520the%2520goal%2520during%2520operation%252C%2520apply%2520also%2520to%2520the%250Aproposed%2520formulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.03374v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Formulation%20of%20Geometry-aware%20Dynamic%20Movement%20Primitives&entry.906535625=Fares%20J.%20Abu-Dakka%20and%20Matteo%20Saveriano%20and%20Ville%20Kyrki&entry.1292438233=%20%20Learning%20from%20demonstration%20%28LfD%29%20is%20considered%20as%20an%20efficient%20way%20to%0Atransfer%20skills%20from%20humans%20to%20robots.%20Traditionally%2C%20LfD%20has%20been%20used%20to%0Atransfer%20Cartesian%20and%20joint%20positions%20and%20forces%20from%20human%20demonstrations.%0AThe%20traditional%20approach%20works%20well%20for%20some%20robotic%20tasks%2C%20but%20for%20many%20tasks%0Aof%20interest%2C%20it%20is%20necessary%20to%20learn%20skills%20such%20as%20orientation%2C%20impedance%2C%0Aand/or%20manipulability%20that%20have%20specific%20geometric%20characteristics.%20An%0Aeffective%20encoding%20of%20such%20skills%20can%20be%20only%20achieved%20if%20the%20underlying%0Ageometric%20structure%20of%20the%20skill%20manifold%20is%20considered%20and%20the%20constrains%0Aarising%20from%20this%20structure%20are%20fulfilled%20during%20both%20learning%20and%20execution.%0AHowever%2C%20typical%20learned%20skill%20models%20such%20as%20dynamic%20movement%20primitives%0A%28DMPs%29%20are%20limited%20to%20Euclidean%20data%20and%20fail%20in%20correctly%20embedding%20quantities%0Awith%20geometric%20constraints.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%0Amathematically%20principled%20framework%20that%20uses%20concepts%20from%20Riemannian%20geometry%0Ato%20allow%20DMPs%20to%20properly%20embed%20geometric%20constrains.%20The%20resulting%20DMP%0Aformulation%20can%20deal%20with%20data%20sampled%20from%20any%20Riemannian%20manifold%20including%2C%0Abut%20not%20limited%20to%2C%20unit%20quaternions%20and%20symmetric%20and%20positive%20definite%0Amatrices.%20The%20proposed%20approach%20has%20been%20extensively%20evaluated%20both%20on%0Asimulated%20data%20and%20real%20robot%20experiments.%20The%20performed%20evaluation%0Ademonstrates%20that%20beneficial%20properties%20of%20DMPs%2C%20such%20as%20convergence%20to%20a%20given%0Agoal%20and%20the%20possibility%20to%20change%20the%20goal%20during%20operation%2C%20apply%20also%20to%20the%0Aproposed%20formulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.03374v3&entry.124074799=Read"},
{"title": "SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision\n  Language Models", "author": "Zheng Liu and Hao Liang and Wentao Xiong and Qinhan Yu and Conghui He and Bin Cui and Wentao Zhang", "abstract": "  Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).\n", "link": "http://arxiv.org/abs/2407.20756v1", "date": "2024-07-30", "relevancy": 2.2198, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5861}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.556}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthVLM%3A%20High-Efficiency%20and%20High-Quality%20Synthetic%20Data%20for%20Vision%0A%20%20Language%20Models&body=Title%3A%20SynthVLM%3A%20High-Efficiency%20and%20High-Quality%20Synthetic%20Data%20for%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Zheng%20Liu%20and%20Hao%20Liang%20and%20Wentao%20Xiong%20and%20Qinhan%20Yu%20and%20Conghui%20He%20and%20Bin%20Cui%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20with%20the%20rise%20of%20web%20images%2C%20managing%20and%20understanding%20large-scale%0Aimage%20datasets%20has%20become%20increasingly%20important.%20Vision%20Large%20Language%20Models%0A%28VLLMs%29%20have%20recently%20emerged%20due%20to%20their%20robust%20vision-understanding%0Acapabilities.%20However%2C%20training%20these%20models%20requires%20vast%20amounts%20of%20data%2C%0Aposing%20challenges%20to%20efficiency%2C%20effectiveness%2C%20data%20quality%2C%20and%20privacy.%20In%0Athis%20paper%2C%20we%20introduce%20SynthVLM%2C%20a%20novel%20data%20synthesis%20pipeline%20for%20VLLMs.%0AUnlike%20existing%20methods%20that%20generate%20captions%20from%20images%2C%20SynthVLM%20employs%0Aadvanced%20diffusion%20models%20and%20high-quality%20captions%20to%20automatically%20generate%0Aand%20select%20high-resolution%20images%20from%20captions%2C%20creating%20precisely%20aligned%0Aimage-text%20pairs.%20Leveraging%20these%20pairs%2C%20we%20achieve%20state-of-the-art%20%28SoTA%29%0Aperformance%20on%20various%20vision%20question%20answering%20tasks%2C%20maintaining%20high%0Aalignment%20quality%20and%20preserving%20advanced%20language%20abilities.%20Moreover%2C%0ASynthVLM%20surpasses%20traditional%20GPT-4%20Vision-based%20caption%20generation%20methods%20in%0Aperformance%20while%20significantly%20reducing%20computational%20overhead.%20Crucially%2C%20our%0Amethod%27s%20reliance%20on%20purely%20generated%20data%20ensures%20the%20preservation%20of%20privacy%2C%0Aachieving%20SoTA%20performance%20with%20just%20100k%20data%20points%20%28only%2018%25%20of%20the%20official%0Adataset%20size%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthVLM%253A%2520High-Efficiency%2520and%2520High-Quality%2520Synthetic%2520Data%2520for%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DZheng%2520Liu%2520and%2520Hao%2520Liang%2520and%2520Wentao%2520Xiong%2520and%2520Qinhan%2520Yu%2520and%2520Conghui%2520He%2520and%2520Bin%2520Cui%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520with%2520the%2520rise%2520of%2520web%2520images%252C%2520managing%2520and%2520understanding%2520large-scale%250Aimage%2520datasets%2520has%2520become%2520increasingly%2520important.%2520Vision%2520Large%2520Language%2520Models%250A%2528VLLMs%2529%2520have%2520recently%2520emerged%2520due%2520to%2520their%2520robust%2520vision-understanding%250Acapabilities.%2520However%252C%2520training%2520these%2520models%2520requires%2520vast%2520amounts%2520of%2520data%252C%250Aposing%2520challenges%2520to%2520efficiency%252C%2520effectiveness%252C%2520data%2520quality%252C%2520and%2520privacy.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520SynthVLM%252C%2520a%2520novel%2520data%2520synthesis%2520pipeline%2520for%2520VLLMs.%250AUnlike%2520existing%2520methods%2520that%2520generate%2520captions%2520from%2520images%252C%2520SynthVLM%2520employs%250Aadvanced%2520diffusion%2520models%2520and%2520high-quality%2520captions%2520to%2520automatically%2520generate%250Aand%2520select%2520high-resolution%2520images%2520from%2520captions%252C%2520creating%2520precisely%2520aligned%250Aimage-text%2520pairs.%2520Leveraging%2520these%2520pairs%252C%2520we%2520achieve%2520state-of-the-art%2520%2528SoTA%2529%250Aperformance%2520on%2520various%2520vision%2520question%2520answering%2520tasks%252C%2520maintaining%2520high%250Aalignment%2520quality%2520and%2520preserving%2520advanced%2520language%2520abilities.%2520Moreover%252C%250ASynthVLM%2520surpasses%2520traditional%2520GPT-4%2520Vision-based%2520caption%2520generation%2520methods%2520in%250Aperformance%2520while%2520significantly%2520reducing%2520computational%2520overhead.%2520Crucially%252C%2520our%250Amethod%2527s%2520reliance%2520on%2520purely%2520generated%2520data%2520ensures%2520the%2520preservation%2520of%2520privacy%252C%250Aachieving%2520SoTA%2520performance%2520with%2520just%2520100k%2520data%2520points%2520%2528only%252018%2525%2520of%2520the%2520official%250Adataset%2520size%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthVLM%3A%20High-Efficiency%20and%20High-Quality%20Synthetic%20Data%20for%20Vision%0A%20%20Language%20Models&entry.906535625=Zheng%20Liu%20and%20Hao%20Liang%20and%20Wentao%20Xiong%20and%20Qinhan%20Yu%20and%20Conghui%20He%20and%20Bin%20Cui%20and%20Wentao%20Zhang&entry.1292438233=%20%20Recently%2C%20with%20the%20rise%20of%20web%20images%2C%20managing%20and%20understanding%20large-scale%0Aimage%20datasets%20has%20become%20increasingly%20important.%20Vision%20Large%20Language%20Models%0A%28VLLMs%29%20have%20recently%20emerged%20due%20to%20their%20robust%20vision-understanding%0Acapabilities.%20However%2C%20training%20these%20models%20requires%20vast%20amounts%20of%20data%2C%0Aposing%20challenges%20to%20efficiency%2C%20effectiveness%2C%20data%20quality%2C%20and%20privacy.%20In%0Athis%20paper%2C%20we%20introduce%20SynthVLM%2C%20a%20novel%20data%20synthesis%20pipeline%20for%20VLLMs.%0AUnlike%20existing%20methods%20that%20generate%20captions%20from%20images%2C%20SynthVLM%20employs%0Aadvanced%20diffusion%20models%20and%20high-quality%20captions%20to%20automatically%20generate%0Aand%20select%20high-resolution%20images%20from%20captions%2C%20creating%20precisely%20aligned%0Aimage-text%20pairs.%20Leveraging%20these%20pairs%2C%20we%20achieve%20state-of-the-art%20%28SoTA%29%0Aperformance%20on%20various%20vision%20question%20answering%20tasks%2C%20maintaining%20high%0Aalignment%20quality%20and%20preserving%20advanced%20language%20abilities.%20Moreover%2C%0ASynthVLM%20surpasses%20traditional%20GPT-4%20Vision-based%20caption%20generation%20methods%20in%0Aperformance%20while%20significantly%20reducing%20computational%20overhead.%20Crucially%2C%20our%0Amethod%27s%20reliance%20on%20purely%20generated%20data%20ensures%20the%20preservation%20of%20privacy%2C%0Aachieving%20SoTA%20performance%20with%20just%20100k%20data%20points%20%28only%2018%25%20of%20the%20official%0Adataset%20size%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20756v1&entry.124074799=Read"},
{"title": "Highly Efficient No-reference 4K Video Quality Assessment with\n  Full-Pixel Covering Sampling and Training Strategy", "author": "Xiaoheng Tan and Jiabin Zhang and Yuhui Quan and Jing Li and Yajing Wu and Zilin Bian", "abstract": "  Deep Video Quality Assessment (VQA) methods have shown impressive\nhigh-performance capabilities. Notably, no-reference (NR) VQA methods play a\nvital role in situations where obtaining reference videos is restricted or not\nfeasible. Nevertheless, as more streaming videos are being created in\nultra-high definition (e.g., 4K) to enrich viewers' experiences, the current\ndeep VQA methods face unacceptable computational costs. Furthermore, the\nresizing, cropping, and local sampling techniques employed in these methods can\ncompromise the details and content of original 4K videos, thereby negatively\nimpacting quality assessment. In this paper, we propose a highly efficient and\nnovel NR 4K VQA technology. Specifically, first, a novel data sampling and\ntraining strategy is proposed to tackle the problem of excessive resolution.\nThis strategy allows the VQA Swin Transformer-based model to effectively train\nand make inferences using the full data of 4K videos on standard consumer-grade\nGPUs without compromising content or details. Second, a weighting and scoring\nscheme is developed to mimic the human subjective perception mode, which is\nachieved by considering the distinct impact of each sub-region within a 4K\nframe on the overall perception. Third, we incorporate the frequency domain\ninformation of video frames to better capture the details that affect video\nquality, consequently further improving the model's generalizability. To our\nknowledge, this is the first technology for the NR 4K VQA task. Thorough\nempirical studies demonstrate it not only significantly outperforms existing\nmethods on a specialized 4K VQA dataset but also achieves state-of-the-art\nperformance across multiple open-source NR video quality datasets.\n", "link": "http://arxiv.org/abs/2407.20766v1", "date": "2024-07-30", "relevancy": 2.2125, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5844}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.532}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highly%20Efficient%20No-reference%204K%20Video%20Quality%20Assessment%20with%0A%20%20Full-Pixel%20Covering%20Sampling%20and%20Training%20Strategy&body=Title%3A%20Highly%20Efficient%20No-reference%204K%20Video%20Quality%20Assessment%20with%0A%20%20Full-Pixel%20Covering%20Sampling%20and%20Training%20Strategy%0AAuthor%3A%20Xiaoheng%20Tan%20and%20Jiabin%20Zhang%20and%20Yuhui%20Quan%20and%20Jing%20Li%20and%20Yajing%20Wu%20and%20Zilin%20Bian%0AAbstract%3A%20%20%20Deep%20Video%20Quality%20Assessment%20%28VQA%29%20methods%20have%20shown%20impressive%0Ahigh-performance%20capabilities.%20Notably%2C%20no-reference%20%28NR%29%20VQA%20methods%20play%20a%0Avital%20role%20in%20situations%20where%20obtaining%20reference%20videos%20is%20restricted%20or%20not%0Afeasible.%20Nevertheless%2C%20as%20more%20streaming%20videos%20are%20being%20created%20in%0Aultra-high%20definition%20%28e.g.%2C%204K%29%20to%20enrich%20viewers%27%20experiences%2C%20the%20current%0Adeep%20VQA%20methods%20face%20unacceptable%20computational%20costs.%20Furthermore%2C%20the%0Aresizing%2C%20cropping%2C%20and%20local%20sampling%20techniques%20employed%20in%20these%20methods%20can%0Acompromise%20the%20details%20and%20content%20of%20original%204K%20videos%2C%20thereby%20negatively%0Aimpacting%20quality%20assessment.%20In%20this%20paper%2C%20we%20propose%20a%20highly%20efficient%20and%0Anovel%20NR%204K%20VQA%20technology.%20Specifically%2C%20first%2C%20a%20novel%20data%20sampling%20and%0Atraining%20strategy%20is%20proposed%20to%20tackle%20the%20problem%20of%20excessive%20resolution.%0AThis%20strategy%20allows%20the%20VQA%20Swin%20Transformer-based%20model%20to%20effectively%20train%0Aand%20make%20inferences%20using%20the%20full%20data%20of%204K%20videos%20on%20standard%20consumer-grade%0AGPUs%20without%20compromising%20content%20or%20details.%20Second%2C%20a%20weighting%20and%20scoring%0Ascheme%20is%20developed%20to%20mimic%20the%20human%20subjective%20perception%20mode%2C%20which%20is%0Aachieved%20by%20considering%20the%20distinct%20impact%20of%20each%20sub-region%20within%20a%204K%0Aframe%20on%20the%20overall%20perception.%20Third%2C%20we%20incorporate%20the%20frequency%20domain%0Ainformation%20of%20video%20frames%20to%20better%20capture%20the%20details%20that%20affect%20video%0Aquality%2C%20consequently%20further%20improving%20the%20model%27s%20generalizability.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20technology%20for%20the%20NR%204K%20VQA%20task.%20Thorough%0Aempirical%20studies%20demonstrate%20it%20not%20only%20significantly%20outperforms%20existing%0Amethods%20on%20a%20specialized%204K%20VQA%20dataset%20but%20also%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20open-source%20NR%20video%20quality%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighly%2520Efficient%2520No-reference%25204K%2520Video%2520Quality%2520Assessment%2520with%250A%2520%2520Full-Pixel%2520Covering%2520Sampling%2520and%2520Training%2520Strategy%26entry.906535625%3DXiaoheng%2520Tan%2520and%2520Jiabin%2520Zhang%2520and%2520Yuhui%2520Quan%2520and%2520Jing%2520Li%2520and%2520Yajing%2520Wu%2520and%2520Zilin%2520Bian%26entry.1292438233%3D%2520%2520Deep%2520Video%2520Quality%2520Assessment%2520%2528VQA%2529%2520methods%2520have%2520shown%2520impressive%250Ahigh-performance%2520capabilities.%2520Notably%252C%2520no-reference%2520%2528NR%2529%2520VQA%2520methods%2520play%2520a%250Avital%2520role%2520in%2520situations%2520where%2520obtaining%2520reference%2520videos%2520is%2520restricted%2520or%2520not%250Afeasible.%2520Nevertheless%252C%2520as%2520more%2520streaming%2520videos%2520are%2520being%2520created%2520in%250Aultra-high%2520definition%2520%2528e.g.%252C%25204K%2529%2520to%2520enrich%2520viewers%2527%2520experiences%252C%2520the%2520current%250Adeep%2520VQA%2520methods%2520face%2520unacceptable%2520computational%2520costs.%2520Furthermore%252C%2520the%250Aresizing%252C%2520cropping%252C%2520and%2520local%2520sampling%2520techniques%2520employed%2520in%2520these%2520methods%2520can%250Acompromise%2520the%2520details%2520and%2520content%2520of%2520original%25204K%2520videos%252C%2520thereby%2520negatively%250Aimpacting%2520quality%2520assessment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520highly%2520efficient%2520and%250Anovel%2520NR%25204K%2520VQA%2520technology.%2520Specifically%252C%2520first%252C%2520a%2520novel%2520data%2520sampling%2520and%250Atraining%2520strategy%2520is%2520proposed%2520to%2520tackle%2520the%2520problem%2520of%2520excessive%2520resolution.%250AThis%2520strategy%2520allows%2520the%2520VQA%2520Swin%2520Transformer-based%2520model%2520to%2520effectively%2520train%250Aand%2520make%2520inferences%2520using%2520the%2520full%2520data%2520of%25204K%2520videos%2520on%2520standard%2520consumer-grade%250AGPUs%2520without%2520compromising%2520content%2520or%2520details.%2520Second%252C%2520a%2520weighting%2520and%2520scoring%250Ascheme%2520is%2520developed%2520to%2520mimic%2520the%2520human%2520subjective%2520perception%2520mode%252C%2520which%2520is%250Aachieved%2520by%2520considering%2520the%2520distinct%2520impact%2520of%2520each%2520sub-region%2520within%2520a%25204K%250Aframe%2520on%2520the%2520overall%2520perception.%2520Third%252C%2520we%2520incorporate%2520the%2520frequency%2520domain%250Ainformation%2520of%2520video%2520frames%2520to%2520better%2520capture%2520the%2520details%2520that%2520affect%2520video%250Aquality%252C%2520consequently%2520further%2520improving%2520the%2520model%2527s%2520generalizability.%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520technology%2520for%2520the%2520NR%25204K%2520VQA%2520task.%2520Thorough%250Aempirical%2520studies%2520demonstrate%2520it%2520not%2520only%2520significantly%2520outperforms%2520existing%250Amethods%2520on%2520a%2520specialized%25204K%2520VQA%2520dataset%2520but%2520also%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520multiple%2520open-source%2520NR%2520video%2520quality%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highly%20Efficient%20No-reference%204K%20Video%20Quality%20Assessment%20with%0A%20%20Full-Pixel%20Covering%20Sampling%20and%20Training%20Strategy&entry.906535625=Xiaoheng%20Tan%20and%20Jiabin%20Zhang%20and%20Yuhui%20Quan%20and%20Jing%20Li%20and%20Yajing%20Wu%20and%20Zilin%20Bian&entry.1292438233=%20%20Deep%20Video%20Quality%20Assessment%20%28VQA%29%20methods%20have%20shown%20impressive%0Ahigh-performance%20capabilities.%20Notably%2C%20no-reference%20%28NR%29%20VQA%20methods%20play%20a%0Avital%20role%20in%20situations%20where%20obtaining%20reference%20videos%20is%20restricted%20or%20not%0Afeasible.%20Nevertheless%2C%20as%20more%20streaming%20videos%20are%20being%20created%20in%0Aultra-high%20definition%20%28e.g.%2C%204K%29%20to%20enrich%20viewers%27%20experiences%2C%20the%20current%0Adeep%20VQA%20methods%20face%20unacceptable%20computational%20costs.%20Furthermore%2C%20the%0Aresizing%2C%20cropping%2C%20and%20local%20sampling%20techniques%20employed%20in%20these%20methods%20can%0Acompromise%20the%20details%20and%20content%20of%20original%204K%20videos%2C%20thereby%20negatively%0Aimpacting%20quality%20assessment.%20In%20this%20paper%2C%20we%20propose%20a%20highly%20efficient%20and%0Anovel%20NR%204K%20VQA%20technology.%20Specifically%2C%20first%2C%20a%20novel%20data%20sampling%20and%0Atraining%20strategy%20is%20proposed%20to%20tackle%20the%20problem%20of%20excessive%20resolution.%0AThis%20strategy%20allows%20the%20VQA%20Swin%20Transformer-based%20model%20to%20effectively%20train%0Aand%20make%20inferences%20using%20the%20full%20data%20of%204K%20videos%20on%20standard%20consumer-grade%0AGPUs%20without%20compromising%20content%20or%20details.%20Second%2C%20a%20weighting%20and%20scoring%0Ascheme%20is%20developed%20to%20mimic%20the%20human%20subjective%20perception%20mode%2C%20which%20is%0Aachieved%20by%20considering%20the%20distinct%20impact%20of%20each%20sub-region%20within%20a%204K%0Aframe%20on%20the%20overall%20perception.%20Third%2C%20we%20incorporate%20the%20frequency%20domain%0Ainformation%20of%20video%20frames%20to%20better%20capture%20the%20details%20that%20affect%20video%0Aquality%2C%20consequently%20further%20improving%20the%20model%27s%20generalizability.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20technology%20for%20the%20NR%204K%20VQA%20task.%20Thorough%0Aempirical%20studies%20demonstrate%20it%20not%20only%20significantly%20outperforms%20existing%0Amethods%20on%20a%20specialized%204K%20VQA%20dataset%20but%20also%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20open-source%20NR%20video%20quality%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20766v1&entry.124074799=Read"},
{"title": "Re-localization acceleration with Medoid Silhouette Clustering", "author": "Hongyi Zhang and Walterio Mayol-Cuevas", "abstract": "  Two crucial performance criteria for the deployment of visual localization\nare speed and accuracy. Current research on visual localization with neural\nnetworks is limited to examining methods for enhancing the accuracy of networks\nacross various datasets. How to expedite the re-localization process within\ndeep neural network architectures still needs further investigation. In this\npaper, we present a novel approach for accelerating visual re-localization in\npractice. A tree-like search strategy, built on the keyframes extracted by a\nvisual clustering algorithm, is designed for matching acceleration. Our method\nhas been validated on two tasks across three public datasets, allowing for 50\nup to 90 percent time saving over the baseline while not reducing location\naccuracy.\n", "link": "http://arxiv.org/abs/2407.20749v1", "date": "2024-07-30", "relevancy": 2.2029, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5862}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5285}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-localization%20acceleration%20with%20Medoid%20Silhouette%20Clustering&body=Title%3A%20Re-localization%20acceleration%20with%20Medoid%20Silhouette%20Clustering%0AAuthor%3A%20Hongyi%20Zhang%20and%20Walterio%20Mayol-Cuevas%0AAbstract%3A%20%20%20Two%20crucial%20performance%20criteria%20for%20the%20deployment%20of%20visual%20localization%0Aare%20speed%20and%20accuracy.%20Current%20research%20on%20visual%20localization%20with%20neural%0Anetworks%20is%20limited%20to%20examining%20methods%20for%20enhancing%20the%20accuracy%20of%20networks%0Aacross%20various%20datasets.%20How%20to%20expedite%20the%20re-localization%20process%20within%0Adeep%20neural%20network%20architectures%20still%20needs%20further%20investigation.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20approach%20for%20accelerating%20visual%20re-localization%20in%0Apractice.%20A%20tree-like%20search%20strategy%2C%20built%20on%20the%20keyframes%20extracted%20by%20a%0Avisual%20clustering%20algorithm%2C%20is%20designed%20for%20matching%20acceleration.%20Our%20method%0Ahas%20been%20validated%20on%20two%20tasks%20across%20three%20public%20datasets%2C%20allowing%20for%2050%0Aup%20to%2090%20percent%20time%20saving%20over%20the%20baseline%20while%20not%20reducing%20location%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-localization%2520acceleration%2520with%2520Medoid%2520Silhouette%2520Clustering%26entry.906535625%3DHongyi%2520Zhang%2520and%2520Walterio%2520Mayol-Cuevas%26entry.1292438233%3D%2520%2520Two%2520crucial%2520performance%2520criteria%2520for%2520the%2520deployment%2520of%2520visual%2520localization%250Aare%2520speed%2520and%2520accuracy.%2520Current%2520research%2520on%2520visual%2520localization%2520with%2520neural%250Anetworks%2520is%2520limited%2520to%2520examining%2520methods%2520for%2520enhancing%2520the%2520accuracy%2520of%2520networks%250Aacross%2520various%2520datasets.%2520How%2520to%2520expedite%2520the%2520re-localization%2520process%2520within%250Adeep%2520neural%2520network%2520architectures%2520still%2520needs%2520further%2520investigation.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%2520accelerating%2520visual%2520re-localization%2520in%250Apractice.%2520A%2520tree-like%2520search%2520strategy%252C%2520built%2520on%2520the%2520keyframes%2520extracted%2520by%2520a%250Avisual%2520clustering%2520algorithm%252C%2520is%2520designed%2520for%2520matching%2520acceleration.%2520Our%2520method%250Ahas%2520been%2520validated%2520on%2520two%2520tasks%2520across%2520three%2520public%2520datasets%252C%2520allowing%2520for%252050%250Aup%2520to%252090%2520percent%2520time%2520saving%2520over%2520the%2520baseline%2520while%2520not%2520reducing%2520location%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-localization%20acceleration%20with%20Medoid%20Silhouette%20Clustering&entry.906535625=Hongyi%20Zhang%20and%20Walterio%20Mayol-Cuevas&entry.1292438233=%20%20Two%20crucial%20performance%20criteria%20for%20the%20deployment%20of%20visual%20localization%0Aare%20speed%20and%20accuracy.%20Current%20research%20on%20visual%20localization%20with%20neural%0Anetworks%20is%20limited%20to%20examining%20methods%20for%20enhancing%20the%20accuracy%20of%20networks%0Aacross%20various%20datasets.%20How%20to%20expedite%20the%20re-localization%20process%20within%0Adeep%20neural%20network%20architectures%20still%20needs%20further%20investigation.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20approach%20for%20accelerating%20visual%20re-localization%20in%0Apractice.%20A%20tree-like%20search%20strategy%2C%20built%20on%20the%20keyframes%20extracted%20by%20a%0Avisual%20clustering%20algorithm%2C%20is%20designed%20for%20matching%20acceleration.%20Our%20method%0Ahas%20been%20validated%20on%20two%20tasks%20across%20three%20public%20datasets%2C%20allowing%20for%2050%0Aup%20to%2090%20percent%20time%20saving%20over%20the%20baseline%20while%20not%20reducing%20location%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20749v1&entry.124074799=Read"},
{"title": "A Case Study on Visual-Audio-Tactile Cross-Modal Retrieval", "author": "Jagoda Wojcik and Jiaqi Jiang and Jiacheng Wu and Shan Luo", "abstract": "  Cross-Modal Retrieval (CMR), which retrieves relevant items from one modality\n(e.g., audio) given a query in another modality (e.g., visual), has undergone\nsignificant advancements in recent years. This capability is crucial for robots\nto integrate and interpret information across diverse sensory inputs. However,\nthe retrieval space in existing robotic CMR approaches often consists of only\none modality, which limits the robot's performance. In this paper, we propose a\nnovel CMR model that incorporates three different modalities, i.e., visual,\naudio and tactile, for enhanced multi-modal object retrieval, named as VAT-CMR.\nIn this model, multi-modal representations are first fused to provide a\nholistic view of object features. To mitigate the semantic gaps between\nrepresentations of different modalities, a dominant modality is then selected\nduring the classification training phase to improve the distinctiveness of the\nrepresentations, so as to improve the retrieval performance. To evaluate our\nproposed approach, we conducted a case study and the results demonstrate that\nour VAT-CMR model surpasses competing approaches. Further, our proposed\ndominant modality selection significantly enhances cross-retrieval accuracy.\n", "link": "http://arxiv.org/abs/2407.20709v1", "date": "2024-07-30", "relevancy": 2.1665, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5631}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Case%20Study%20on%20Visual-Audio-Tactile%20Cross-Modal%20Retrieval&body=Title%3A%20A%20Case%20Study%20on%20Visual-Audio-Tactile%20Cross-Modal%20Retrieval%0AAuthor%3A%20Jagoda%20Wojcik%20and%20Jiaqi%20Jiang%20and%20Jiacheng%20Wu%20and%20Shan%20Luo%0AAbstract%3A%20%20%20Cross-Modal%20Retrieval%20%28CMR%29%2C%20which%20retrieves%20relevant%20items%20from%20one%20modality%0A%28e.g.%2C%20audio%29%20given%20a%20query%20in%20another%20modality%20%28e.g.%2C%20visual%29%2C%20has%20undergone%0Asignificant%20advancements%20in%20recent%20years.%20This%20capability%20is%20crucial%20for%20robots%0Ato%20integrate%20and%20interpret%20information%20across%20diverse%20sensory%20inputs.%20However%2C%0Athe%20retrieval%20space%20in%20existing%20robotic%20CMR%20approaches%20often%20consists%20of%20only%0Aone%20modality%2C%20which%20limits%20the%20robot%27s%20performance.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20CMR%20model%20that%20incorporates%20three%20different%20modalities%2C%20i.e.%2C%20visual%2C%0Aaudio%20and%20tactile%2C%20for%20enhanced%20multi-modal%20object%20retrieval%2C%20named%20as%20VAT-CMR.%0AIn%20this%20model%2C%20multi-modal%20representations%20are%20first%20fused%20to%20provide%20a%0Aholistic%20view%20of%20object%20features.%20To%20mitigate%20the%20semantic%20gaps%20between%0Arepresentations%20of%20different%20modalities%2C%20a%20dominant%20modality%20is%20then%20selected%0Aduring%20the%20classification%20training%20phase%20to%20improve%20the%20distinctiveness%20of%20the%0Arepresentations%2C%20so%20as%20to%20improve%20the%20retrieval%20performance.%20To%20evaluate%20our%0Aproposed%20approach%2C%20we%20conducted%20a%20case%20study%20and%20the%20results%20demonstrate%20that%0Aour%20VAT-CMR%20model%20surpasses%20competing%20approaches.%20Further%2C%20our%20proposed%0Adominant%20modality%20selection%20significantly%20enhances%20cross-retrieval%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Case%2520Study%2520on%2520Visual-Audio-Tactile%2520Cross-Modal%2520Retrieval%26entry.906535625%3DJagoda%2520Wojcik%2520and%2520Jiaqi%2520Jiang%2520and%2520Jiacheng%2520Wu%2520and%2520Shan%2520Luo%26entry.1292438233%3D%2520%2520Cross-Modal%2520Retrieval%2520%2528CMR%2529%252C%2520which%2520retrieves%2520relevant%2520items%2520from%2520one%2520modality%250A%2528e.g.%252C%2520audio%2529%2520given%2520a%2520query%2520in%2520another%2520modality%2520%2528e.g.%252C%2520visual%2529%252C%2520has%2520undergone%250Asignificant%2520advancements%2520in%2520recent%2520years.%2520This%2520capability%2520is%2520crucial%2520for%2520robots%250Ato%2520integrate%2520and%2520interpret%2520information%2520across%2520diverse%2520sensory%2520inputs.%2520However%252C%250Athe%2520retrieval%2520space%2520in%2520existing%2520robotic%2520CMR%2520approaches%2520often%2520consists%2520of%2520only%250Aone%2520modality%252C%2520which%2520limits%2520the%2520robot%2527s%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520CMR%2520model%2520that%2520incorporates%2520three%2520different%2520modalities%252C%2520i.e.%252C%2520visual%252C%250Aaudio%2520and%2520tactile%252C%2520for%2520enhanced%2520multi-modal%2520object%2520retrieval%252C%2520named%2520as%2520VAT-CMR.%250AIn%2520this%2520model%252C%2520multi-modal%2520representations%2520are%2520first%2520fused%2520to%2520provide%2520a%250Aholistic%2520view%2520of%2520object%2520features.%2520To%2520mitigate%2520the%2520semantic%2520gaps%2520between%250Arepresentations%2520of%2520different%2520modalities%252C%2520a%2520dominant%2520modality%2520is%2520then%2520selected%250Aduring%2520the%2520classification%2520training%2520phase%2520to%2520improve%2520the%2520distinctiveness%2520of%2520the%250Arepresentations%252C%2520so%2520as%2520to%2520improve%2520the%2520retrieval%2520performance.%2520To%2520evaluate%2520our%250Aproposed%2520approach%252C%2520we%2520conducted%2520a%2520case%2520study%2520and%2520the%2520results%2520demonstrate%2520that%250Aour%2520VAT-CMR%2520model%2520surpasses%2520competing%2520approaches.%2520Further%252C%2520our%2520proposed%250Adominant%2520modality%2520selection%2520significantly%2520enhances%2520cross-retrieval%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Case%20Study%20on%20Visual-Audio-Tactile%20Cross-Modal%20Retrieval&entry.906535625=Jagoda%20Wojcik%20and%20Jiaqi%20Jiang%20and%20Jiacheng%20Wu%20and%20Shan%20Luo&entry.1292438233=%20%20Cross-Modal%20Retrieval%20%28CMR%29%2C%20which%20retrieves%20relevant%20items%20from%20one%20modality%0A%28e.g.%2C%20audio%29%20given%20a%20query%20in%20another%20modality%20%28e.g.%2C%20visual%29%2C%20has%20undergone%0Asignificant%20advancements%20in%20recent%20years.%20This%20capability%20is%20crucial%20for%20robots%0Ato%20integrate%20and%20interpret%20information%20across%20diverse%20sensory%20inputs.%20However%2C%0Athe%20retrieval%20space%20in%20existing%20robotic%20CMR%20approaches%20often%20consists%20of%20only%0Aone%20modality%2C%20which%20limits%20the%20robot%27s%20performance.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20CMR%20model%20that%20incorporates%20three%20different%20modalities%2C%20i.e.%2C%20visual%2C%0Aaudio%20and%20tactile%2C%20for%20enhanced%20multi-modal%20object%20retrieval%2C%20named%20as%20VAT-CMR.%0AIn%20this%20model%2C%20multi-modal%20representations%20are%20first%20fused%20to%20provide%20a%0Aholistic%20view%20of%20object%20features.%20To%20mitigate%20the%20semantic%20gaps%20between%0Arepresentations%20of%20different%20modalities%2C%20a%20dominant%20modality%20is%20then%20selected%0Aduring%20the%20classification%20training%20phase%20to%20improve%20the%20distinctiveness%20of%20the%0Arepresentations%2C%20so%20as%20to%20improve%20the%20retrieval%20performance.%20To%20evaluate%20our%0Aproposed%20approach%2C%20we%20conducted%20a%20case%20study%20and%20the%20results%20demonstrate%20that%0Aour%20VAT-CMR%20model%20surpasses%20competing%20approaches.%20Further%2C%20our%20proposed%0Adominant%20modality%20selection%20significantly%20enhances%20cross-retrieval%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20709v1&entry.124074799=Read"},
{"title": "A Scalable Tool For Analyzing Genomic Variants Of Humans Using Knowledge\n  Graphs and Machine Learning", "author": "Shivika Prasanna and Ajay Kumar and Deepthi Rao and Eduardo Simoes and Praveen Rao", "abstract": "  The integration of knowledge graphs and graph machine learning (GML) in\ngenomic data analysis offers several opportunities for understanding complex\ngenetic relationships, especially at the RNA level. We present a comprehensive\napproach for leveraging these technologies to analyze genomic variants,\nspecifically in the context of RNA sequencing (RNA-seq) data from COVID-19\npatient samples. The proposed method involves extracting variant-level genetic\ninformation, annotating the data with additional metadata using SnpEff, and\nconverting the enriched Variant Call Format (VCF) files into Resource\nDescription Framework (RDF) triples. The resulting knowledge graph is further\nenhanced with patient metadata and stored in a graph database, facilitating\nefficient querying and indexing. We utilize the Deep Graph Library (DGL) to\nperform graph machine learning tasks, including node classification with\nGraphSAGE and Graph Convolutional Networks (GCNs). Our approach demonstrates\nsignificant utility using our proposed tool, VariantKG, in three key scenarios:\nenriching graphs with new VCF data, creating subgraphs based on user-defined\nfeatures, and conducting graph machine learning for node classification.\n", "link": "http://arxiv.org/abs/2407.20879v1", "date": "2024-07-30", "relevancy": 2.1567, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4467}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4256}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Tool%20For%20Analyzing%20Genomic%20Variants%20Of%20Humans%20Using%20Knowledge%0A%20%20Graphs%20and%20Machine%20Learning&body=Title%3A%20A%20Scalable%20Tool%20For%20Analyzing%20Genomic%20Variants%20Of%20Humans%20Using%20Knowledge%0A%20%20Graphs%20and%20Machine%20Learning%0AAuthor%3A%20Shivika%20Prasanna%20and%20Ajay%20Kumar%20and%20Deepthi%20Rao%20and%20Eduardo%20Simoes%20and%20Praveen%20Rao%0AAbstract%3A%20%20%20The%20integration%20of%20knowledge%20graphs%20and%20graph%20machine%20learning%20%28GML%29%20in%0Agenomic%20data%20analysis%20offers%20several%20opportunities%20for%20understanding%20complex%0Agenetic%20relationships%2C%20especially%20at%20the%20RNA%20level.%20We%20present%20a%20comprehensive%0Aapproach%20for%20leveraging%20these%20technologies%20to%20analyze%20genomic%20variants%2C%0Aspecifically%20in%20the%20context%20of%20RNA%20sequencing%20%28RNA-seq%29%20data%20from%20COVID-19%0Apatient%20samples.%20The%20proposed%20method%20involves%20extracting%20variant-level%20genetic%0Ainformation%2C%20annotating%20the%20data%20with%20additional%20metadata%20using%20SnpEff%2C%20and%0Aconverting%20the%20enriched%20Variant%20Call%20Format%20%28VCF%29%20files%20into%20Resource%0ADescription%20Framework%20%28RDF%29%20triples.%20The%20resulting%20knowledge%20graph%20is%20further%0Aenhanced%20with%20patient%20metadata%20and%20stored%20in%20a%20graph%20database%2C%20facilitating%0Aefficient%20querying%20and%20indexing.%20We%20utilize%20the%20Deep%20Graph%20Library%20%28DGL%29%20to%0Aperform%20graph%20machine%20learning%20tasks%2C%20including%20node%20classification%20with%0AGraphSAGE%20and%20Graph%20Convolutional%20Networks%20%28GCNs%29.%20Our%20approach%20demonstrates%0Asignificant%20utility%20using%20our%20proposed%20tool%2C%20VariantKG%2C%20in%20three%20key%20scenarios%3A%0Aenriching%20graphs%20with%20new%20VCF%20data%2C%20creating%20subgraphs%20based%20on%20user-defined%0Afeatures%2C%20and%20conducting%20graph%20machine%20learning%20for%20node%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Tool%2520For%2520Analyzing%2520Genomic%2520Variants%2520Of%2520Humans%2520Using%2520Knowledge%250A%2520%2520Graphs%2520and%2520Machine%2520Learning%26entry.906535625%3DShivika%2520Prasanna%2520and%2520Ajay%2520Kumar%2520and%2520Deepthi%2520Rao%2520and%2520Eduardo%2520Simoes%2520and%2520Praveen%2520Rao%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520knowledge%2520graphs%2520and%2520graph%2520machine%2520learning%2520%2528GML%2529%2520in%250Agenomic%2520data%2520analysis%2520offers%2520several%2520opportunities%2520for%2520understanding%2520complex%250Agenetic%2520relationships%252C%2520especially%2520at%2520the%2520RNA%2520level.%2520We%2520present%2520a%2520comprehensive%250Aapproach%2520for%2520leveraging%2520these%2520technologies%2520to%2520analyze%2520genomic%2520variants%252C%250Aspecifically%2520in%2520the%2520context%2520of%2520RNA%2520sequencing%2520%2528RNA-seq%2529%2520data%2520from%2520COVID-19%250Apatient%2520samples.%2520The%2520proposed%2520method%2520involves%2520extracting%2520variant-level%2520genetic%250Ainformation%252C%2520annotating%2520the%2520data%2520with%2520additional%2520metadata%2520using%2520SnpEff%252C%2520and%250Aconverting%2520the%2520enriched%2520Variant%2520Call%2520Format%2520%2528VCF%2529%2520files%2520into%2520Resource%250ADescription%2520Framework%2520%2528RDF%2529%2520triples.%2520The%2520resulting%2520knowledge%2520graph%2520is%2520further%250Aenhanced%2520with%2520patient%2520metadata%2520and%2520stored%2520in%2520a%2520graph%2520database%252C%2520facilitating%250Aefficient%2520querying%2520and%2520indexing.%2520We%2520utilize%2520the%2520Deep%2520Graph%2520Library%2520%2528DGL%2529%2520to%250Aperform%2520graph%2520machine%2520learning%2520tasks%252C%2520including%2520node%2520classification%2520with%250AGraphSAGE%2520and%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529.%2520Our%2520approach%2520demonstrates%250Asignificant%2520utility%2520using%2520our%2520proposed%2520tool%252C%2520VariantKG%252C%2520in%2520three%2520key%2520scenarios%253A%250Aenriching%2520graphs%2520with%2520new%2520VCF%2520data%252C%2520creating%2520subgraphs%2520based%2520on%2520user-defined%250Afeatures%252C%2520and%2520conducting%2520graph%2520machine%2520learning%2520for%2520node%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Tool%20For%20Analyzing%20Genomic%20Variants%20Of%20Humans%20Using%20Knowledge%0A%20%20Graphs%20and%20Machine%20Learning&entry.906535625=Shivika%20Prasanna%20and%20Ajay%20Kumar%20and%20Deepthi%20Rao%20and%20Eduardo%20Simoes%20and%20Praveen%20Rao&entry.1292438233=%20%20The%20integration%20of%20knowledge%20graphs%20and%20graph%20machine%20learning%20%28GML%29%20in%0Agenomic%20data%20analysis%20offers%20several%20opportunities%20for%20understanding%20complex%0Agenetic%20relationships%2C%20especially%20at%20the%20RNA%20level.%20We%20present%20a%20comprehensive%0Aapproach%20for%20leveraging%20these%20technologies%20to%20analyze%20genomic%20variants%2C%0Aspecifically%20in%20the%20context%20of%20RNA%20sequencing%20%28RNA-seq%29%20data%20from%20COVID-19%0Apatient%20samples.%20The%20proposed%20method%20involves%20extracting%20variant-level%20genetic%0Ainformation%2C%20annotating%20the%20data%20with%20additional%20metadata%20using%20SnpEff%2C%20and%0Aconverting%20the%20enriched%20Variant%20Call%20Format%20%28VCF%29%20files%20into%20Resource%0ADescription%20Framework%20%28RDF%29%20triples.%20The%20resulting%20knowledge%20graph%20is%20further%0Aenhanced%20with%20patient%20metadata%20and%20stored%20in%20a%20graph%20database%2C%20facilitating%0Aefficient%20querying%20and%20indexing.%20We%20utilize%20the%20Deep%20Graph%20Library%20%28DGL%29%20to%0Aperform%20graph%20machine%20learning%20tasks%2C%20including%20node%20classification%20with%0AGraphSAGE%20and%20Graph%20Convolutional%20Networks%20%28GCNs%29.%20Our%20approach%20demonstrates%0Asignificant%20utility%20using%20our%20proposed%20tool%2C%20VariantKG%2C%20in%20three%20key%20scenarios%3A%0Aenriching%20graphs%20with%20new%20VCF%20data%2C%20creating%20subgraphs%20based%20on%20user-defined%0Afeatures%2C%20and%20conducting%20graph%20machine%20learning%20for%20node%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20879v1&entry.124074799=Read"},
{"title": "On the Exploitation of DCT-Traces in the Generative-AI Domain", "author": "Orazio Pontorno and Luca Guarnera and Sebastiano Battiato", "abstract": "  Deepfakes represent one of the toughest challenges in the world of\nCybersecurity and Digital Forensics, especially considering the high-quality\nresults obtained with recent generative AI-based solutions. Almost all\ngenerative models leave unique traces in synthetic data that, if analyzed and\nidentified in detail, can be exploited to improve the generalization\nlimitations of existing deepfake detectors. In this paper we analyzed deepfake\nimages in the frequency domain generated by both GAN and Diffusion Model\nengines, examining in detail the underlying statistical distribution of\nDiscrete Cosine Transform (DCT) coefficients. Recognizing that not all\ncoefficients contribute equally to image detection, we hypothesize the\nexistence of a unique ``discriminative fingerprint\", embedded in specific\ncombinations of coefficients. To identify them, Machine Learning classifiers\nwere trained on various combinations of coefficients. In addition, the\nExplainable AI (XAI) LIME algorithm was used to search for intrinsic\ndiscriminative combinations of coefficients. Finally, we performed a robustness\ntest to analyze the persistence of traces by applying JPEG compression. The\nexperimental results reveal the existence of traces left by the generative\nmodels that are more discriminative and persistent at JPEG attacks. Code and\ndataset are available at https://github.com/opontorno/dcts_analysis_deepfakes.\n", "link": "http://arxiv.org/abs/2402.02209v3", "date": "2024-07-30", "relevancy": 2.1461, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5644}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.518}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Exploitation%20of%20DCT-Traces%20in%20the%20Generative-AI%20Domain&body=Title%3A%20On%20the%20Exploitation%20of%20DCT-Traces%20in%20the%20Generative-AI%20Domain%0AAuthor%3A%20Orazio%20Pontorno%20and%20Luca%20Guarnera%20and%20Sebastiano%20Battiato%0AAbstract%3A%20%20%20Deepfakes%20represent%20one%20of%20the%20toughest%20challenges%20in%20the%20world%20of%0ACybersecurity%20and%20Digital%20Forensics%2C%20especially%20considering%20the%20high-quality%0Aresults%20obtained%20with%20recent%20generative%20AI-based%20solutions.%20Almost%20all%0Agenerative%20models%20leave%20unique%20traces%20in%20synthetic%20data%20that%2C%20if%20analyzed%20and%0Aidentified%20in%20detail%2C%20can%20be%20exploited%20to%20improve%20the%20generalization%0Alimitations%20of%20existing%20deepfake%20detectors.%20In%20this%20paper%20we%20analyzed%20deepfake%0Aimages%20in%20the%20frequency%20domain%20generated%20by%20both%20GAN%20and%20Diffusion%20Model%0Aengines%2C%20examining%20in%20detail%20the%20underlying%20statistical%20distribution%20of%0ADiscrete%20Cosine%20Transform%20%28DCT%29%20coefficients.%20Recognizing%20that%20not%20all%0Acoefficients%20contribute%20equally%20to%20image%20detection%2C%20we%20hypothesize%20the%0Aexistence%20of%20a%20unique%20%60%60discriminative%20fingerprint%22%2C%20embedded%20in%20specific%0Acombinations%20of%20coefficients.%20To%20identify%20them%2C%20Machine%20Learning%20classifiers%0Awere%20trained%20on%20various%20combinations%20of%20coefficients.%20In%20addition%2C%20the%0AExplainable%20AI%20%28XAI%29%20LIME%20algorithm%20was%20used%20to%20search%20for%20intrinsic%0Adiscriminative%20combinations%20of%20coefficients.%20Finally%2C%20we%20performed%20a%20robustness%0Atest%20to%20analyze%20the%20persistence%20of%20traces%20by%20applying%20JPEG%20compression.%20The%0Aexperimental%20results%20reveal%20the%20existence%20of%20traces%20left%20by%20the%20generative%0Amodels%20that%20are%20more%20discriminative%20and%20persistent%20at%20JPEG%20attacks.%20Code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/opontorno/dcts_analysis_deepfakes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02209v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Exploitation%2520of%2520DCT-Traces%2520in%2520the%2520Generative-AI%2520Domain%26entry.906535625%3DOrazio%2520Pontorno%2520and%2520Luca%2520Guarnera%2520and%2520Sebastiano%2520Battiato%26entry.1292438233%3D%2520%2520Deepfakes%2520represent%2520one%2520of%2520the%2520toughest%2520challenges%2520in%2520the%2520world%2520of%250ACybersecurity%2520and%2520Digital%2520Forensics%252C%2520especially%2520considering%2520the%2520high-quality%250Aresults%2520obtained%2520with%2520recent%2520generative%2520AI-based%2520solutions.%2520Almost%2520all%250Agenerative%2520models%2520leave%2520unique%2520traces%2520in%2520synthetic%2520data%2520that%252C%2520if%2520analyzed%2520and%250Aidentified%2520in%2520detail%252C%2520can%2520be%2520exploited%2520to%2520improve%2520the%2520generalization%250Alimitations%2520of%2520existing%2520deepfake%2520detectors.%2520In%2520this%2520paper%2520we%2520analyzed%2520deepfake%250Aimages%2520in%2520the%2520frequency%2520domain%2520generated%2520by%2520both%2520GAN%2520and%2520Diffusion%2520Model%250Aengines%252C%2520examining%2520in%2520detail%2520the%2520underlying%2520statistical%2520distribution%2520of%250ADiscrete%2520Cosine%2520Transform%2520%2528DCT%2529%2520coefficients.%2520Recognizing%2520that%2520not%2520all%250Acoefficients%2520contribute%2520equally%2520to%2520image%2520detection%252C%2520we%2520hypothesize%2520the%250Aexistence%2520of%2520a%2520unique%2520%2560%2560discriminative%2520fingerprint%2522%252C%2520embedded%2520in%2520specific%250Acombinations%2520of%2520coefficients.%2520To%2520identify%2520them%252C%2520Machine%2520Learning%2520classifiers%250Awere%2520trained%2520on%2520various%2520combinations%2520of%2520coefficients.%2520In%2520addition%252C%2520the%250AExplainable%2520AI%2520%2528XAI%2529%2520LIME%2520algorithm%2520was%2520used%2520to%2520search%2520for%2520intrinsic%250Adiscriminative%2520combinations%2520of%2520coefficients.%2520Finally%252C%2520we%2520performed%2520a%2520robustness%250Atest%2520to%2520analyze%2520the%2520persistence%2520of%2520traces%2520by%2520applying%2520JPEG%2520compression.%2520The%250Aexperimental%2520results%2520reveal%2520the%2520existence%2520of%2520traces%2520left%2520by%2520the%2520generative%250Amodels%2520that%2520are%2520more%2520discriminative%2520and%2520persistent%2520at%2520JPEG%2520attacks.%2520Code%2520and%250Adataset%2520are%2520available%2520at%2520https%253A//github.com/opontorno/dcts_analysis_deepfakes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02209v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Exploitation%20of%20DCT-Traces%20in%20the%20Generative-AI%20Domain&entry.906535625=Orazio%20Pontorno%20and%20Luca%20Guarnera%20and%20Sebastiano%20Battiato&entry.1292438233=%20%20Deepfakes%20represent%20one%20of%20the%20toughest%20challenges%20in%20the%20world%20of%0ACybersecurity%20and%20Digital%20Forensics%2C%20especially%20considering%20the%20high-quality%0Aresults%20obtained%20with%20recent%20generative%20AI-based%20solutions.%20Almost%20all%0Agenerative%20models%20leave%20unique%20traces%20in%20synthetic%20data%20that%2C%20if%20analyzed%20and%0Aidentified%20in%20detail%2C%20can%20be%20exploited%20to%20improve%20the%20generalization%0Alimitations%20of%20existing%20deepfake%20detectors.%20In%20this%20paper%20we%20analyzed%20deepfake%0Aimages%20in%20the%20frequency%20domain%20generated%20by%20both%20GAN%20and%20Diffusion%20Model%0Aengines%2C%20examining%20in%20detail%20the%20underlying%20statistical%20distribution%20of%0ADiscrete%20Cosine%20Transform%20%28DCT%29%20coefficients.%20Recognizing%20that%20not%20all%0Acoefficients%20contribute%20equally%20to%20image%20detection%2C%20we%20hypothesize%20the%0Aexistence%20of%20a%20unique%20%60%60discriminative%20fingerprint%22%2C%20embedded%20in%20specific%0Acombinations%20of%20coefficients.%20To%20identify%20them%2C%20Machine%20Learning%20classifiers%0Awere%20trained%20on%20various%20combinations%20of%20coefficients.%20In%20addition%2C%20the%0AExplainable%20AI%20%28XAI%29%20LIME%20algorithm%20was%20used%20to%20search%20for%20intrinsic%0Adiscriminative%20combinations%20of%20coefficients.%20Finally%2C%20we%20performed%20a%20robustness%0Atest%20to%20analyze%20the%20persistence%20of%20traces%20by%20applying%20JPEG%20compression.%20The%0Aexperimental%20results%20reveal%20the%20existence%20of%20traces%20left%20by%20the%20generative%0Amodels%20that%20are%20more%20discriminative%20and%20persistent%20at%20JPEG%20attacks.%20Code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/opontorno/dcts_analysis_deepfakes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02209v3&entry.124074799=Read"},
{"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language\n  Model and Prompt Fine-Tuning", "author": "Yuexi Du and Brian Chang and Nicha C. Dvornek", "abstract": "  Recent advancements in Contrastive Language-Image Pre-training (CLIP) have\ndemonstrated notable success in self-supervised representation learning across\nvarious tasks. However, the existing CLIP-like approaches often demand\nextensive GPU resources and prolonged training times due to the considerable\nsize of the model and dataset, making them poor for medical applications, in\nwhich large datasets are not always common. Meanwhile, the language model\nprompts are mainly manually derived from labels tied to images, potentially\noverlooking the richness of information within training samples. We introduce a\nnovel language-image Contrastive Learning method with an Efficient large\nlanguage model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of\nthe extensive pre-trained language and visual models. Furthermore, we present\nan efficient strategy for learning context-based prompts that mitigates the gap\nbetween informative clinical diagnostic data and simple class labels. Our\nmethod demonstrates state-of-the-art performance on multiple chest X-ray and\nmammography datasets compared with various baselines. The proposed parameter\nefficient framework can reduce the total trainable model size by 39% and reduce\nthe trainable language model to only 4% compared with the current BERT encoder.\n", "link": "http://arxiv.org/abs/2407.21011v1", "date": "2024-07-30", "relevancy": 2.1361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5078}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLEFT%3A%20Language-Image%20Contrastive%20Learning%20with%20Efficient%20Large%20Language%0A%20%20Model%20and%20Prompt%20Fine-Tuning&body=Title%3A%20CLEFT%3A%20Language-Image%20Contrastive%20Learning%20with%20Efficient%20Large%20Language%0A%20%20Model%20and%20Prompt%20Fine-Tuning%0AAuthor%3A%20Yuexi%20Du%20and%20Brian%20Chang%20and%20Nicha%20C.%20Dvornek%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20have%0Ademonstrated%20notable%20success%20in%20self-supervised%20representation%20learning%20across%0Avarious%20tasks.%20However%2C%20the%20existing%20CLIP-like%20approaches%20often%20demand%0Aextensive%20GPU%20resources%20and%20prolonged%20training%20times%20due%20to%20the%20considerable%0Asize%20of%20the%20model%20and%20dataset%2C%20making%20them%20poor%20for%20medical%20applications%2C%20in%0Awhich%20large%20datasets%20are%20not%20always%20common.%20Meanwhile%2C%20the%20language%20model%0Aprompts%20are%20mainly%20manually%20derived%20from%20labels%20tied%20to%20images%2C%20potentially%0Aoverlooking%20the%20richness%20of%20information%20within%20training%20samples.%20We%20introduce%20a%0Anovel%20language-image%20Contrastive%20Learning%20method%20with%20an%20Efficient%20large%0Alanguage%20model%20and%20prompt%20Fine-Tuning%20%28CLEFT%29%20that%20harnesses%20the%20strengths%20of%0Athe%20extensive%20pre-trained%20language%20and%20visual%20models.%20Furthermore%2C%20we%20present%0Aan%20efficient%20strategy%20for%20learning%20context-based%20prompts%20that%20mitigates%20the%20gap%0Abetween%20informative%20clinical%20diagnostic%20data%20and%20simple%20class%20labels.%20Our%0Amethod%20demonstrates%20state-of-the-art%20performance%20on%20multiple%20chest%20X-ray%20and%0Amammography%20datasets%20compared%20with%20various%20baselines.%20The%20proposed%20parameter%0Aefficient%20framework%20can%20reduce%20the%20total%20trainable%20model%20size%20by%2039%25%20and%20reduce%0Athe%20trainable%20language%20model%20to%20only%204%25%20compared%20with%20the%20current%20BERT%20encoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLEFT%253A%2520Language-Image%2520Contrastive%2520Learning%2520with%2520Efficient%2520Large%2520Language%250A%2520%2520Model%2520and%2520Prompt%2520Fine-Tuning%26entry.906535625%3DYuexi%2520Du%2520and%2520Brian%2520Chang%2520and%2520Nicha%2520C.%2520Dvornek%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520have%250Ademonstrated%2520notable%2520success%2520in%2520self-supervised%2520representation%2520learning%2520across%250Avarious%2520tasks.%2520However%252C%2520the%2520existing%2520CLIP-like%2520approaches%2520often%2520demand%250Aextensive%2520GPU%2520resources%2520and%2520prolonged%2520training%2520times%2520due%2520to%2520the%2520considerable%250Asize%2520of%2520the%2520model%2520and%2520dataset%252C%2520making%2520them%2520poor%2520for%2520medical%2520applications%252C%2520in%250Awhich%2520large%2520datasets%2520are%2520not%2520always%2520common.%2520Meanwhile%252C%2520the%2520language%2520model%250Aprompts%2520are%2520mainly%2520manually%2520derived%2520from%2520labels%2520tied%2520to%2520images%252C%2520potentially%250Aoverlooking%2520the%2520richness%2520of%2520information%2520within%2520training%2520samples.%2520We%2520introduce%2520a%250Anovel%2520language-image%2520Contrastive%2520Learning%2520method%2520with%2520an%2520Efficient%2520large%250Alanguage%2520model%2520and%2520prompt%2520Fine-Tuning%2520%2528CLEFT%2529%2520that%2520harnesses%2520the%2520strengths%2520of%250Athe%2520extensive%2520pre-trained%2520language%2520and%2520visual%2520models.%2520Furthermore%252C%2520we%2520present%250Aan%2520efficient%2520strategy%2520for%2520learning%2520context-based%2520prompts%2520that%2520mitigates%2520the%2520gap%250Abetween%2520informative%2520clinical%2520diagnostic%2520data%2520and%2520simple%2520class%2520labels.%2520Our%250Amethod%2520demonstrates%2520state-of-the-art%2520performance%2520on%2520multiple%2520chest%2520X-ray%2520and%250Amammography%2520datasets%2520compared%2520with%2520various%2520baselines.%2520The%2520proposed%2520parameter%250Aefficient%2520framework%2520can%2520reduce%2520the%2520total%2520trainable%2520model%2520size%2520by%252039%2525%2520and%2520reduce%250Athe%2520trainable%2520language%2520model%2520to%2520only%25204%2525%2520compared%2520with%2520the%2520current%2520BERT%2520encoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEFT%3A%20Language-Image%20Contrastive%20Learning%20with%20Efficient%20Large%20Language%0A%20%20Model%20and%20Prompt%20Fine-Tuning&entry.906535625=Yuexi%20Du%20and%20Brian%20Chang%20and%20Nicha%20C.%20Dvornek&entry.1292438233=%20%20Recent%20advancements%20in%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20have%0Ademonstrated%20notable%20success%20in%20self-supervised%20representation%20learning%20across%0Avarious%20tasks.%20However%2C%20the%20existing%20CLIP-like%20approaches%20often%20demand%0Aextensive%20GPU%20resources%20and%20prolonged%20training%20times%20due%20to%20the%20considerable%0Asize%20of%20the%20model%20and%20dataset%2C%20making%20them%20poor%20for%20medical%20applications%2C%20in%0Awhich%20large%20datasets%20are%20not%20always%20common.%20Meanwhile%2C%20the%20language%20model%0Aprompts%20are%20mainly%20manually%20derived%20from%20labels%20tied%20to%20images%2C%20potentially%0Aoverlooking%20the%20richness%20of%20information%20within%20training%20samples.%20We%20introduce%20a%0Anovel%20language-image%20Contrastive%20Learning%20method%20with%20an%20Efficient%20large%0Alanguage%20model%20and%20prompt%20Fine-Tuning%20%28CLEFT%29%20that%20harnesses%20the%20strengths%20of%0Athe%20extensive%20pre-trained%20language%20and%20visual%20models.%20Furthermore%2C%20we%20present%0Aan%20efficient%20strategy%20for%20learning%20context-based%20prompts%20that%20mitigates%20the%20gap%0Abetween%20informative%20clinical%20diagnostic%20data%20and%20simple%20class%20labels.%20Our%0Amethod%20demonstrates%20state-of-the-art%20performance%20on%20multiple%20chest%20X-ray%20and%0Amammography%20datasets%20compared%20with%20various%20baselines.%20The%20proposed%20parameter%0Aefficient%20framework%20can%20reduce%20the%20total%20trainable%20model%20size%20by%2039%25%20and%20reduce%0Athe%20trainable%20language%20model%20to%20only%204%25%20compared%20with%20the%20current%20BERT%20encoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21011v1&entry.124074799=Read"},
{"title": "Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for\n  Long Sequences", "author": "Yanming Kang and Giang Tran and Hans De Sterck", "abstract": "  Transformer-based models have achieved state-of-the-art performance in many\nareas. However, the quadratic complexity of self-attention with respect to the\ninput length hinders the applicability of Transformer-based models to long\nsequences. To address this, we present Fast Multipole Attention, a new\nattention mechanism that uses a divide-and-conquer strategy to reduce the time\nand memory complexity of attention for sequences of length $n$ from\n$\\mathcal{O}(n^2)$ to $\\mathcal{O}(n \\log n)$ or $O(n)$, while retaining a\nglobal receptive field. The hierarchical approach groups queries, keys, and\nvalues into $\\mathcal{O}( \\log n)$ levels of resolution, where groups at\ngreater distances are increasingly larger in size and the weights to compute\ngroup quantities are learned. As such, the interaction between tokens far from\neach other is considered in lower resolution in an efficient hierarchical\nmanner. The overall complexity of Fast Multipole Attention is $\\mathcal{O}(n)$\nor $\\mathcal{O}(n \\log n)$, depending on whether the queries are down-sampled\nor not. This multi-level divide-and-conquer strategy is inspired by fast\nsummation methods from $n$-body physics and the Fast Multipole Method. We\nperform evaluation on autoregressive and bidirectional language modeling tasks\nand compare our Fast Multipole Attention model with other efficient attention\nvariants on medium-size datasets. We find empirically that the Fast Multipole\nTransformer performs much better than other efficient transformers in terms of\nmemory size and accuracy. The Fast Multipole Attention mechanism has the\npotential to empower large language models with much greater sequence lengths,\ntaking the full context into account in an efficient, naturally hierarchical\nmanner during training and when generating long sequences.\n", "link": "http://arxiv.org/abs/2310.11960v3", "date": "2024-07-30", "relevancy": 2.1344, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5641}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Multipole%20Attention%3A%20A%20Divide-and-Conquer%20Attention%20Mechanism%20for%0A%20%20Long%20Sequences&body=Title%3A%20Fast%20Multipole%20Attention%3A%20A%20Divide-and-Conquer%20Attention%20Mechanism%20for%0A%20%20Long%20Sequences%0AAuthor%3A%20Yanming%20Kang%20and%20Giang%20Tran%20and%20Hans%20De%20Sterck%0AAbstract%3A%20%20%20Transformer-based%20models%20have%20achieved%20state-of-the-art%20performance%20in%20many%0Aareas.%20However%2C%20the%20quadratic%20complexity%20of%20self-attention%20with%20respect%20to%20the%0Ainput%20length%20hinders%20the%20applicability%20of%20Transformer-based%20models%20to%20long%0Asequences.%20To%20address%20this%2C%20we%20present%20Fast%20Multipole%20Attention%2C%20a%20new%0Aattention%20mechanism%20that%20uses%20a%20divide-and-conquer%20strategy%20to%20reduce%20the%20time%0Aand%20memory%20complexity%20of%20attention%20for%20sequences%20of%20length%20%24n%24%20from%0A%24%5Cmathcal%7BO%7D%28n%5E2%29%24%20to%20%24%5Cmathcal%7BO%7D%28n%20%5Clog%20n%29%24%20or%20%24O%28n%29%24%2C%20while%20retaining%20a%0Aglobal%20receptive%20field.%20The%20hierarchical%20approach%20groups%20queries%2C%20keys%2C%20and%0Avalues%20into%20%24%5Cmathcal%7BO%7D%28%20%5Clog%20n%29%24%20levels%20of%20resolution%2C%20where%20groups%20at%0Agreater%20distances%20are%20increasingly%20larger%20in%20size%20and%20the%20weights%20to%20compute%0Agroup%20quantities%20are%20learned.%20As%20such%2C%20the%20interaction%20between%20tokens%20far%20from%0Aeach%20other%20is%20considered%20in%20lower%20resolution%20in%20an%20efficient%20hierarchical%0Amanner.%20The%20overall%20complexity%20of%20Fast%20Multipole%20Attention%20is%20%24%5Cmathcal%7BO%7D%28n%29%24%0Aor%20%24%5Cmathcal%7BO%7D%28n%20%5Clog%20n%29%24%2C%20depending%20on%20whether%20the%20queries%20are%20down-sampled%0Aor%20not.%20This%20multi-level%20divide-and-conquer%20strategy%20is%20inspired%20by%20fast%0Asummation%20methods%20from%20%24n%24-body%20physics%20and%20the%20Fast%20Multipole%20Method.%20We%0Aperform%20evaluation%20on%20autoregressive%20and%20bidirectional%20language%20modeling%20tasks%0Aand%20compare%20our%20Fast%20Multipole%20Attention%20model%20with%20other%20efficient%20attention%0Avariants%20on%20medium-size%20datasets.%20We%20find%20empirically%20that%20the%20Fast%20Multipole%0ATransformer%20performs%20much%20better%20than%20other%20efficient%20transformers%20in%20terms%20of%0Amemory%20size%20and%20accuracy.%20The%20Fast%20Multipole%20Attention%20mechanism%20has%20the%0Apotential%20to%20empower%20large%20language%20models%20with%20much%20greater%20sequence%20lengths%2C%0Ataking%20the%20full%20context%20into%20account%20in%20an%20efficient%2C%20naturally%20hierarchical%0Amanner%20during%20training%20and%20when%20generating%20long%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11960v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Multipole%2520Attention%253A%2520A%2520Divide-and-Conquer%2520Attention%2520Mechanism%2520for%250A%2520%2520Long%2520Sequences%26entry.906535625%3DYanming%2520Kang%2520and%2520Giang%2520Tran%2520and%2520Hans%2520De%2520Sterck%26entry.1292438233%3D%2520%2520Transformer-based%2520models%2520have%2520achieved%2520state-of-the-art%2520performance%2520in%2520many%250Aareas.%2520However%252C%2520the%2520quadratic%2520complexity%2520of%2520self-attention%2520with%2520respect%2520to%2520the%250Ainput%2520length%2520hinders%2520the%2520applicability%2520of%2520Transformer-based%2520models%2520to%2520long%250Asequences.%2520To%2520address%2520this%252C%2520we%2520present%2520Fast%2520Multipole%2520Attention%252C%2520a%2520new%250Aattention%2520mechanism%2520that%2520uses%2520a%2520divide-and-conquer%2520strategy%2520to%2520reduce%2520the%2520time%250Aand%2520memory%2520complexity%2520of%2520attention%2520for%2520sequences%2520of%2520length%2520%2524n%2524%2520from%250A%2524%255Cmathcal%257BO%257D%2528n%255E2%2529%2524%2520to%2520%2524%255Cmathcal%257BO%257D%2528n%2520%255Clog%2520n%2529%2524%2520or%2520%2524O%2528n%2529%2524%252C%2520while%2520retaining%2520a%250Aglobal%2520receptive%2520field.%2520The%2520hierarchical%2520approach%2520groups%2520queries%252C%2520keys%252C%2520and%250Avalues%2520into%2520%2524%255Cmathcal%257BO%257D%2528%2520%255Clog%2520n%2529%2524%2520levels%2520of%2520resolution%252C%2520where%2520groups%2520at%250Agreater%2520distances%2520are%2520increasingly%2520larger%2520in%2520size%2520and%2520the%2520weights%2520to%2520compute%250Agroup%2520quantities%2520are%2520learned.%2520As%2520such%252C%2520the%2520interaction%2520between%2520tokens%2520far%2520from%250Aeach%2520other%2520is%2520considered%2520in%2520lower%2520resolution%2520in%2520an%2520efficient%2520hierarchical%250Amanner.%2520The%2520overall%2520complexity%2520of%2520Fast%2520Multipole%2520Attention%2520is%2520%2524%255Cmathcal%257BO%257D%2528n%2529%2524%250Aor%2520%2524%255Cmathcal%257BO%257D%2528n%2520%255Clog%2520n%2529%2524%252C%2520depending%2520on%2520whether%2520the%2520queries%2520are%2520down-sampled%250Aor%2520not.%2520This%2520multi-level%2520divide-and-conquer%2520strategy%2520is%2520inspired%2520by%2520fast%250Asummation%2520methods%2520from%2520%2524n%2524-body%2520physics%2520and%2520the%2520Fast%2520Multipole%2520Method.%2520We%250Aperform%2520evaluation%2520on%2520autoregressive%2520and%2520bidirectional%2520language%2520modeling%2520tasks%250Aand%2520compare%2520our%2520Fast%2520Multipole%2520Attention%2520model%2520with%2520other%2520efficient%2520attention%250Avariants%2520on%2520medium-size%2520datasets.%2520We%2520find%2520empirically%2520that%2520the%2520Fast%2520Multipole%250ATransformer%2520performs%2520much%2520better%2520than%2520other%2520efficient%2520transformers%2520in%2520terms%2520of%250Amemory%2520size%2520and%2520accuracy.%2520The%2520Fast%2520Multipole%2520Attention%2520mechanism%2520has%2520the%250Apotential%2520to%2520empower%2520large%2520language%2520models%2520with%2520much%2520greater%2520sequence%2520lengths%252C%250Ataking%2520the%2520full%2520context%2520into%2520account%2520in%2520an%2520efficient%252C%2520naturally%2520hierarchical%250Amanner%2520during%2520training%2520and%2520when%2520generating%2520long%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11960v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Multipole%20Attention%3A%20A%20Divide-and-Conquer%20Attention%20Mechanism%20for%0A%20%20Long%20Sequences&entry.906535625=Yanming%20Kang%20and%20Giang%20Tran%20and%20Hans%20De%20Sterck&entry.1292438233=%20%20Transformer-based%20models%20have%20achieved%20state-of-the-art%20performance%20in%20many%0Aareas.%20However%2C%20the%20quadratic%20complexity%20of%20self-attention%20with%20respect%20to%20the%0Ainput%20length%20hinders%20the%20applicability%20of%20Transformer-based%20models%20to%20long%0Asequences.%20To%20address%20this%2C%20we%20present%20Fast%20Multipole%20Attention%2C%20a%20new%0Aattention%20mechanism%20that%20uses%20a%20divide-and-conquer%20strategy%20to%20reduce%20the%20time%0Aand%20memory%20complexity%20of%20attention%20for%20sequences%20of%20length%20%24n%24%20from%0A%24%5Cmathcal%7BO%7D%28n%5E2%29%24%20to%20%24%5Cmathcal%7BO%7D%28n%20%5Clog%20n%29%24%20or%20%24O%28n%29%24%2C%20while%20retaining%20a%0Aglobal%20receptive%20field.%20The%20hierarchical%20approach%20groups%20queries%2C%20keys%2C%20and%0Avalues%20into%20%24%5Cmathcal%7BO%7D%28%20%5Clog%20n%29%24%20levels%20of%20resolution%2C%20where%20groups%20at%0Agreater%20distances%20are%20increasingly%20larger%20in%20size%20and%20the%20weights%20to%20compute%0Agroup%20quantities%20are%20learned.%20As%20such%2C%20the%20interaction%20between%20tokens%20far%20from%0Aeach%20other%20is%20considered%20in%20lower%20resolution%20in%20an%20efficient%20hierarchical%0Amanner.%20The%20overall%20complexity%20of%20Fast%20Multipole%20Attention%20is%20%24%5Cmathcal%7BO%7D%28n%29%24%0Aor%20%24%5Cmathcal%7BO%7D%28n%20%5Clog%20n%29%24%2C%20depending%20on%20whether%20the%20queries%20are%20down-sampled%0Aor%20not.%20This%20multi-level%20divide-and-conquer%20strategy%20is%20inspired%20by%20fast%0Asummation%20methods%20from%20%24n%24-body%20physics%20and%20the%20Fast%20Multipole%20Method.%20We%0Aperform%20evaluation%20on%20autoregressive%20and%20bidirectional%20language%20modeling%20tasks%0Aand%20compare%20our%20Fast%20Multipole%20Attention%20model%20with%20other%20efficient%20attention%0Avariants%20on%20medium-size%20datasets.%20We%20find%20empirically%20that%20the%20Fast%20Multipole%0ATransformer%20performs%20much%20better%20than%20other%20efficient%20transformers%20in%20terms%20of%0Amemory%20size%20and%20accuracy.%20The%20Fast%20Multipole%20Attention%20mechanism%20has%20the%0Apotential%20to%20empower%20large%20language%20models%20with%20much%20greater%20sequence%20lengths%2C%0Ataking%20the%20full%20context%20into%20account%20in%20an%20efficient%2C%20naturally%20hierarchical%0Amanner%20during%20training%20and%20when%20generating%20long%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11960v3&entry.124074799=Read"},
{"title": "Structure Unbiased Adversarial Model for Medical Image Segmentation", "author": "Tianyang Zhang and Shaoming Zheng and Jun Cheng and Xi Jia and Joseph Bartlett and Xinxing Cheng and Huazhu Fu and Zhaowen Qiu and Jiang Liu and Jinming Duan", "abstract": "  Generative models have been widely proposed in image recognition to generate\nmore images where the distribution is similar to that of the real ones. It\noften introduces a discriminator network to differentiate the real data from\nthe generated ones. Such models utilise a discriminator network tasked with\ndifferentiating style transferred data from data contained in the target\ndataset. However in doing so the network focuses on discrepancies in the\nintensity distribution and may overlook structural differences between the\ndatasets. In this paper we formulate a new image-to-image translation problem\nto ensure that the structure of the generated images is similar to that in the\ntarget dataset. We propose a simple, yet powerful Structure-Unbiased\nAdversarial (SUA) network which accounts for both intensity and structural\ndifferences between the training and test sets when performing image\nsegmentation. It consists of a spatial transformation block followed by an\nintensity distribution rendering module. The spatial transformation block is\nproposed to reduce the structure gap between the two images, and also produce\nan inverse deformation field to warp the final segmented image back. The\nintensity distribution rendering module then renders the deformed structure to\nan image with the target intensity distribution. Experimental results show that\nthe proposed SUA method has the capability to transfer both intensity\ndistribution and structural content between multiple datasets.\n", "link": "http://arxiv.org/abs/2205.12857v4", "date": "2024-07-30", "relevancy": 2.1204, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5313}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20Unbiased%20Adversarial%20Model%20for%20Medical%20Image%20Segmentation&body=Title%3A%20Structure%20Unbiased%20Adversarial%20Model%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Tianyang%20Zhang%20and%20Shaoming%20Zheng%20and%20Jun%20Cheng%20and%20Xi%20Jia%20and%20Joseph%20Bartlett%20and%20Xinxing%20Cheng%20and%20Huazhu%20Fu%20and%20Zhaowen%20Qiu%20and%20Jiang%20Liu%20and%20Jinming%20Duan%0AAbstract%3A%20%20%20Generative%20models%20have%20been%20widely%20proposed%20in%20image%20recognition%20to%20generate%0Amore%20images%20where%20the%20distribution%20is%20similar%20to%20that%20of%20the%20real%20ones.%20It%0Aoften%20introduces%20a%20discriminator%20network%20to%20differentiate%20the%20real%20data%20from%0Athe%20generated%20ones.%20Such%20models%20utilise%20a%20discriminator%20network%20tasked%20with%0Adifferentiating%20style%20transferred%20data%20from%20data%20contained%20in%20the%20target%0Adataset.%20However%20in%20doing%20so%20the%20network%20focuses%20on%20discrepancies%20in%20the%0Aintensity%20distribution%20and%20may%20overlook%20structural%20differences%20between%20the%0Adatasets.%20In%20this%20paper%20we%20formulate%20a%20new%20image-to-image%20translation%20problem%0Ato%20ensure%20that%20the%20structure%20of%20the%20generated%20images%20is%20similar%20to%20that%20in%20the%0Atarget%20dataset.%20We%20propose%20a%20simple%2C%20yet%20powerful%20Structure-Unbiased%0AAdversarial%20%28SUA%29%20network%20which%20accounts%20for%20both%20intensity%20and%20structural%0Adifferences%20between%20the%20training%20and%20test%20sets%20when%20performing%20image%0Asegmentation.%20It%20consists%20of%20a%20spatial%20transformation%20block%20followed%20by%20an%0Aintensity%20distribution%20rendering%20module.%20The%20spatial%20transformation%20block%20is%0Aproposed%20to%20reduce%20the%20structure%20gap%20between%20the%20two%20images%2C%20and%20also%20produce%0Aan%20inverse%20deformation%20field%20to%20warp%20the%20final%20segmented%20image%20back.%20The%0Aintensity%20distribution%20rendering%20module%20then%20renders%20the%20deformed%20structure%20to%0Aan%20image%20with%20the%20target%20intensity%20distribution.%20Experimental%20results%20show%20that%0Athe%20proposed%20SUA%20method%20has%20the%20capability%20to%20transfer%20both%20intensity%0Adistribution%20and%20structural%20content%20between%20multiple%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.12857v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520Unbiased%2520Adversarial%2520Model%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DTianyang%2520Zhang%2520and%2520Shaoming%2520Zheng%2520and%2520Jun%2520Cheng%2520and%2520Xi%2520Jia%2520and%2520Joseph%2520Bartlett%2520and%2520Xinxing%2520Cheng%2520and%2520Huazhu%2520Fu%2520and%2520Zhaowen%2520Qiu%2520and%2520Jiang%2520Liu%2520and%2520Jinming%2520Duan%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520been%2520widely%2520proposed%2520in%2520image%2520recognition%2520to%2520generate%250Amore%2520images%2520where%2520the%2520distribution%2520is%2520similar%2520to%2520that%2520of%2520the%2520real%2520ones.%2520It%250Aoften%2520introduces%2520a%2520discriminator%2520network%2520to%2520differentiate%2520the%2520real%2520data%2520from%250Athe%2520generated%2520ones.%2520Such%2520models%2520utilise%2520a%2520discriminator%2520network%2520tasked%2520with%250Adifferentiating%2520style%2520transferred%2520data%2520from%2520data%2520contained%2520in%2520the%2520target%250Adataset.%2520However%2520in%2520doing%2520so%2520the%2520network%2520focuses%2520on%2520discrepancies%2520in%2520the%250Aintensity%2520distribution%2520and%2520may%2520overlook%2520structural%2520differences%2520between%2520the%250Adatasets.%2520In%2520this%2520paper%2520we%2520formulate%2520a%2520new%2520image-to-image%2520translation%2520problem%250Ato%2520ensure%2520that%2520the%2520structure%2520of%2520the%2520generated%2520images%2520is%2520similar%2520to%2520that%2520in%2520the%250Atarget%2520dataset.%2520We%2520propose%2520a%2520simple%252C%2520yet%2520powerful%2520Structure-Unbiased%250AAdversarial%2520%2528SUA%2529%2520network%2520which%2520accounts%2520for%2520both%2520intensity%2520and%2520structural%250Adifferences%2520between%2520the%2520training%2520and%2520test%2520sets%2520when%2520performing%2520image%250Asegmentation.%2520It%2520consists%2520of%2520a%2520spatial%2520transformation%2520block%2520followed%2520by%2520an%250Aintensity%2520distribution%2520rendering%2520module.%2520The%2520spatial%2520transformation%2520block%2520is%250Aproposed%2520to%2520reduce%2520the%2520structure%2520gap%2520between%2520the%2520two%2520images%252C%2520and%2520also%2520produce%250Aan%2520inverse%2520deformation%2520field%2520to%2520warp%2520the%2520final%2520segmented%2520image%2520back.%2520The%250Aintensity%2520distribution%2520rendering%2520module%2520then%2520renders%2520the%2520deformed%2520structure%2520to%250Aan%2520image%2520with%2520the%2520target%2520intensity%2520distribution.%2520Experimental%2520results%2520show%2520that%250Athe%2520proposed%2520SUA%2520method%2520has%2520the%2520capability%2520to%2520transfer%2520both%2520intensity%250Adistribution%2520and%2520structural%2520content%2520between%2520multiple%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.12857v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20Unbiased%20Adversarial%20Model%20for%20Medical%20Image%20Segmentation&entry.906535625=Tianyang%20Zhang%20and%20Shaoming%20Zheng%20and%20Jun%20Cheng%20and%20Xi%20Jia%20and%20Joseph%20Bartlett%20and%20Xinxing%20Cheng%20and%20Huazhu%20Fu%20and%20Zhaowen%20Qiu%20and%20Jiang%20Liu%20and%20Jinming%20Duan&entry.1292438233=%20%20Generative%20models%20have%20been%20widely%20proposed%20in%20image%20recognition%20to%20generate%0Amore%20images%20where%20the%20distribution%20is%20similar%20to%20that%20of%20the%20real%20ones.%20It%0Aoften%20introduces%20a%20discriminator%20network%20to%20differentiate%20the%20real%20data%20from%0Athe%20generated%20ones.%20Such%20models%20utilise%20a%20discriminator%20network%20tasked%20with%0Adifferentiating%20style%20transferred%20data%20from%20data%20contained%20in%20the%20target%0Adataset.%20However%20in%20doing%20so%20the%20network%20focuses%20on%20discrepancies%20in%20the%0Aintensity%20distribution%20and%20may%20overlook%20structural%20differences%20between%20the%0Adatasets.%20In%20this%20paper%20we%20formulate%20a%20new%20image-to-image%20translation%20problem%0Ato%20ensure%20that%20the%20structure%20of%20the%20generated%20images%20is%20similar%20to%20that%20in%20the%0Atarget%20dataset.%20We%20propose%20a%20simple%2C%20yet%20powerful%20Structure-Unbiased%0AAdversarial%20%28SUA%29%20network%20which%20accounts%20for%20both%20intensity%20and%20structural%0Adifferences%20between%20the%20training%20and%20test%20sets%20when%20performing%20image%0Asegmentation.%20It%20consists%20of%20a%20spatial%20transformation%20block%20followed%20by%20an%0Aintensity%20distribution%20rendering%20module.%20The%20spatial%20transformation%20block%20is%0Aproposed%20to%20reduce%20the%20structure%20gap%20between%20the%20two%20images%2C%20and%20also%20produce%0Aan%20inverse%20deformation%20field%20to%20warp%20the%20final%20segmented%20image%20back.%20The%0Aintensity%20distribution%20rendering%20module%20then%20renders%20the%20deformed%20structure%20to%0Aan%20image%20with%20the%20target%20intensity%20distribution.%20Experimental%20results%20show%20that%0Athe%20proposed%20SUA%20method%20has%20the%20capability%20to%20transfer%20both%20intensity%0Adistribution%20and%20structural%20content%20between%20multiple%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.12857v4&entry.124074799=Read"},
{"title": "Efficient Face Super-Resolution via Wavelet-based Feature Enhancement\n  Network", "author": "Wenjie Li and Heng Guo and Xuannan Liu and Kongming Liang and Jiani Hu and Zhanyu Ma and Jun Guo", "abstract": "  Face super-resolution aims to reconstruct a high-resolution face image from a\nlow-resolution face image. Previous methods typically employ an encoder-decoder\nstructure to extract facial structural features, where the direct downsampling\ninevitably introduces distortions, especially to high-frequency features such\nas edges. To address this issue, we propose a wavelet-based feature enhancement\nnetwork, which mitigates feature distortion by losslessly decomposing the input\nfeature into high and low-frequency components using the wavelet transform and\nprocessing them separately. To improve the efficiency of facial feature\nextraction, a full domain Transformer is further proposed to enhance local,\nregional, and global facial features. Such designs allow our method to perform\nbetter without stacking many modules as previous methods did. Experiments show\nthat our method effectively balances performance, model size, and speed. Code\nlink: https://github.com/PRIS-CV/WFEN.\n", "link": "http://arxiv.org/abs/2407.19768v2", "date": "2024-07-30", "relevancy": 2.1193, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5402}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5244}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Face%20Super-Resolution%20via%20Wavelet-based%20Feature%20Enhancement%0A%20%20Network&body=Title%3A%20Efficient%20Face%20Super-Resolution%20via%20Wavelet-based%20Feature%20Enhancement%0A%20%20Network%0AAuthor%3A%20Wenjie%20Li%20and%20Heng%20Guo%20and%20Xuannan%20Liu%20and%20Kongming%20Liang%20and%20Jiani%20Hu%20and%20Zhanyu%20Ma%20and%20Jun%20Guo%0AAbstract%3A%20%20%20Face%20super-resolution%20aims%20to%20reconstruct%20a%20high-resolution%20face%20image%20from%20a%0Alow-resolution%20face%20image.%20Previous%20methods%20typically%20employ%20an%20encoder-decoder%0Astructure%20to%20extract%20facial%20structural%20features%2C%20where%20the%20direct%20downsampling%0Ainevitably%20introduces%20distortions%2C%20especially%20to%20high-frequency%20features%20such%0Aas%20edges.%20To%20address%20this%20issue%2C%20we%20propose%20a%20wavelet-based%20feature%20enhancement%0Anetwork%2C%20which%20mitigates%20feature%20distortion%20by%20losslessly%20decomposing%20the%20input%0Afeature%20into%20high%20and%20low-frequency%20components%20using%20the%20wavelet%20transform%20and%0Aprocessing%20them%20separately.%20To%20improve%20the%20efficiency%20of%20facial%20feature%0Aextraction%2C%20a%20full%20domain%20Transformer%20is%20further%20proposed%20to%20enhance%20local%2C%0Aregional%2C%20and%20global%20facial%20features.%20Such%20designs%20allow%20our%20method%20to%20perform%0Abetter%20without%20stacking%20many%20modules%20as%20previous%20methods%20did.%20Experiments%20show%0Athat%20our%20method%20effectively%20balances%20performance%2C%20model%20size%2C%20and%20speed.%20Code%0Alink%3A%20https%3A//github.com/PRIS-CV/WFEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Face%2520Super-Resolution%2520via%2520Wavelet-based%2520Feature%2520Enhancement%250A%2520%2520Network%26entry.906535625%3DWenjie%2520Li%2520and%2520Heng%2520Guo%2520and%2520Xuannan%2520Liu%2520and%2520Kongming%2520Liang%2520and%2520Jiani%2520Hu%2520and%2520Zhanyu%2520Ma%2520and%2520Jun%2520Guo%26entry.1292438233%3D%2520%2520Face%2520super-resolution%2520aims%2520to%2520reconstruct%2520a%2520high-resolution%2520face%2520image%2520from%2520a%250Alow-resolution%2520face%2520image.%2520Previous%2520methods%2520typically%2520employ%2520an%2520encoder-decoder%250Astructure%2520to%2520extract%2520facial%2520structural%2520features%252C%2520where%2520the%2520direct%2520downsampling%250Ainevitably%2520introduces%2520distortions%252C%2520especially%2520to%2520high-frequency%2520features%2520such%250Aas%2520edges.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520wavelet-based%2520feature%2520enhancement%250Anetwork%252C%2520which%2520mitigates%2520feature%2520distortion%2520by%2520losslessly%2520decomposing%2520the%2520input%250Afeature%2520into%2520high%2520and%2520low-frequency%2520components%2520using%2520the%2520wavelet%2520transform%2520and%250Aprocessing%2520them%2520separately.%2520To%2520improve%2520the%2520efficiency%2520of%2520facial%2520feature%250Aextraction%252C%2520a%2520full%2520domain%2520Transformer%2520is%2520further%2520proposed%2520to%2520enhance%2520local%252C%250Aregional%252C%2520and%2520global%2520facial%2520features.%2520Such%2520designs%2520allow%2520our%2520method%2520to%2520perform%250Abetter%2520without%2520stacking%2520many%2520modules%2520as%2520previous%2520methods%2520did.%2520Experiments%2520show%250Athat%2520our%2520method%2520effectively%2520balances%2520performance%252C%2520model%2520size%252C%2520and%2520speed.%2520Code%250Alink%253A%2520https%253A//github.com/PRIS-CV/WFEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Face%20Super-Resolution%20via%20Wavelet-based%20Feature%20Enhancement%0A%20%20Network&entry.906535625=Wenjie%20Li%20and%20Heng%20Guo%20and%20Xuannan%20Liu%20and%20Kongming%20Liang%20and%20Jiani%20Hu%20and%20Zhanyu%20Ma%20and%20Jun%20Guo&entry.1292438233=%20%20Face%20super-resolution%20aims%20to%20reconstruct%20a%20high-resolution%20face%20image%20from%20a%0Alow-resolution%20face%20image.%20Previous%20methods%20typically%20employ%20an%20encoder-decoder%0Astructure%20to%20extract%20facial%20structural%20features%2C%20where%20the%20direct%20downsampling%0Ainevitably%20introduces%20distortions%2C%20especially%20to%20high-frequency%20features%20such%0Aas%20edges.%20To%20address%20this%20issue%2C%20we%20propose%20a%20wavelet-based%20feature%20enhancement%0Anetwork%2C%20which%20mitigates%20feature%20distortion%20by%20losslessly%20decomposing%20the%20input%0Afeature%20into%20high%20and%20low-frequency%20components%20using%20the%20wavelet%20transform%20and%0Aprocessing%20them%20separately.%20To%20improve%20the%20efficiency%20of%20facial%20feature%0Aextraction%2C%20a%20full%20domain%20Transformer%20is%20further%20proposed%20to%20enhance%20local%2C%0Aregional%2C%20and%20global%20facial%20features.%20Such%20designs%20allow%20our%20method%20to%20perform%0Abetter%20without%20stacking%20many%20modules%20as%20previous%20methods%20did.%20Experiments%20show%0Athat%20our%20method%20effectively%20balances%20performance%2C%20model%20size%2C%20and%20speed.%20Code%0Alink%3A%20https%3A//github.com/PRIS-CV/WFEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19768v2&entry.124074799=Read"},
{"title": "Hilti SLAM Challenge 2023: Benchmarking Single + Multi-session SLAM\n  across Sensor Constellations in Construction", "author": "Ashish Devadas Nair and Julien Kindle and Plamen Levchev and Davide Scaramuzza", "abstract": "  Simultaneous Localization and Mapping systems are a key enabler for\npositioning in both handheld and robotic applications. The Hilti SLAM\nChallenges organized over the past years have been successful at benchmarking\nsome of the world's best SLAM Systems with high accuracy. However, more\ncapabilities of these systems are yet to be explored, such as platform\nagnosticism across varying sensor suites and multi-session SLAM. These factors\nindirectly serve as an indicator of robustness and ease of deployment in\nreal-world applications. There exists no dataset plus benchmark combination\npublicly available, which considers these factors combined. The Hilti SLAM\nChallenge 2023 Dataset and Benchmark addresses this issue. Additionally, we\npropose a novel fiducial marker design for a pre-surveyed point on the ground\nto be observable from an off-the-shelf LiDAR mounted on a robot, and an\nalgorithm to estimate its position at mm-level accuracy. Results from the\nchallenge show an increase in overall participation, single-session SLAM\nsystems getting increasingly accurate, successfully operating across varying\nsensor suites, but relatively few participants performing multi-session SLAM.\nDataset URL: https://www.hilti-challenge.com/dataset-2023.html\n", "link": "http://arxiv.org/abs/2404.09765v2", "date": "2024-07-30", "relevancy": 2.1127, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5384}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5257}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hilti%20SLAM%20Challenge%202023%3A%20Benchmarking%20Single%20%2B%20Multi-session%20SLAM%0A%20%20across%20Sensor%20Constellations%20in%20Construction&body=Title%3A%20Hilti%20SLAM%20Challenge%202023%3A%20Benchmarking%20Single%20%2B%20Multi-session%20SLAM%0A%20%20across%20Sensor%20Constellations%20in%20Construction%0AAuthor%3A%20Ashish%20Devadas%20Nair%20and%20Julien%20Kindle%20and%20Plamen%20Levchev%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20systems%20are%20a%20key%20enabler%20for%0Apositioning%20in%20both%20handheld%20and%20robotic%20applications.%20The%20Hilti%20SLAM%0AChallenges%20organized%20over%20the%20past%20years%20have%20been%20successful%20at%20benchmarking%0Asome%20of%20the%20world%27s%20best%20SLAM%20Systems%20with%20high%20accuracy.%20However%2C%20more%0Acapabilities%20of%20these%20systems%20are%20yet%20to%20be%20explored%2C%20such%20as%20platform%0Aagnosticism%20across%20varying%20sensor%20suites%20and%20multi-session%20SLAM.%20These%20factors%0Aindirectly%20serve%20as%20an%20indicator%20of%20robustness%20and%20ease%20of%20deployment%20in%0Areal-world%20applications.%20There%20exists%20no%20dataset%20plus%20benchmark%20combination%0Apublicly%20available%2C%20which%20considers%20these%20factors%20combined.%20The%20Hilti%20SLAM%0AChallenge%202023%20Dataset%20and%20Benchmark%20addresses%20this%20issue.%20Additionally%2C%20we%0Apropose%20a%20novel%20fiducial%20marker%20design%20for%20a%20pre-surveyed%20point%20on%20the%20ground%0Ato%20be%20observable%20from%20an%20off-the-shelf%20LiDAR%20mounted%20on%20a%20robot%2C%20and%20an%0Aalgorithm%20to%20estimate%20its%20position%20at%20mm-level%20accuracy.%20Results%20from%20the%0Achallenge%20show%20an%20increase%20in%20overall%20participation%2C%20single-session%20SLAM%0Asystems%20getting%20increasingly%20accurate%2C%20successfully%20operating%20across%20varying%0Asensor%20suites%2C%20but%20relatively%20few%20participants%20performing%20multi-session%20SLAM.%0ADataset%20URL%3A%20https%3A//www.hilti-challenge.com/dataset-2023.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHilti%2520SLAM%2520Challenge%25202023%253A%2520Benchmarking%2520Single%2520%252B%2520Multi-session%2520SLAM%250A%2520%2520across%2520Sensor%2520Constellations%2520in%2520Construction%26entry.906535625%3DAshish%2520Devadas%2520Nair%2520and%2520Julien%2520Kindle%2520and%2520Plamen%2520Levchev%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520systems%2520are%2520a%2520key%2520enabler%2520for%250Apositioning%2520in%2520both%2520handheld%2520and%2520robotic%2520applications.%2520The%2520Hilti%2520SLAM%250AChallenges%2520organized%2520over%2520the%2520past%2520years%2520have%2520been%2520successful%2520at%2520benchmarking%250Asome%2520of%2520the%2520world%2527s%2520best%2520SLAM%2520Systems%2520with%2520high%2520accuracy.%2520However%252C%2520more%250Acapabilities%2520of%2520these%2520systems%2520are%2520yet%2520to%2520be%2520explored%252C%2520such%2520as%2520platform%250Aagnosticism%2520across%2520varying%2520sensor%2520suites%2520and%2520multi-session%2520SLAM.%2520These%2520factors%250Aindirectly%2520serve%2520as%2520an%2520indicator%2520of%2520robustness%2520and%2520ease%2520of%2520deployment%2520in%250Areal-world%2520applications.%2520There%2520exists%2520no%2520dataset%2520plus%2520benchmark%2520combination%250Apublicly%2520available%252C%2520which%2520considers%2520these%2520factors%2520combined.%2520The%2520Hilti%2520SLAM%250AChallenge%25202023%2520Dataset%2520and%2520Benchmark%2520addresses%2520this%2520issue.%2520Additionally%252C%2520we%250Apropose%2520a%2520novel%2520fiducial%2520marker%2520design%2520for%2520a%2520pre-surveyed%2520point%2520on%2520the%2520ground%250Ato%2520be%2520observable%2520from%2520an%2520off-the-shelf%2520LiDAR%2520mounted%2520on%2520a%2520robot%252C%2520and%2520an%250Aalgorithm%2520to%2520estimate%2520its%2520position%2520at%2520mm-level%2520accuracy.%2520Results%2520from%2520the%250Achallenge%2520show%2520an%2520increase%2520in%2520overall%2520participation%252C%2520single-session%2520SLAM%250Asystems%2520getting%2520increasingly%2520accurate%252C%2520successfully%2520operating%2520across%2520varying%250Asensor%2520suites%252C%2520but%2520relatively%2520few%2520participants%2520performing%2520multi-session%2520SLAM.%250ADataset%2520URL%253A%2520https%253A//www.hilti-challenge.com/dataset-2023.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hilti%20SLAM%20Challenge%202023%3A%20Benchmarking%20Single%20%2B%20Multi-session%20SLAM%0A%20%20across%20Sensor%20Constellations%20in%20Construction&entry.906535625=Ashish%20Devadas%20Nair%20and%20Julien%20Kindle%20and%20Plamen%20Levchev%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20systems%20are%20a%20key%20enabler%20for%0Apositioning%20in%20both%20handheld%20and%20robotic%20applications.%20The%20Hilti%20SLAM%0AChallenges%20organized%20over%20the%20past%20years%20have%20been%20successful%20at%20benchmarking%0Asome%20of%20the%20world%27s%20best%20SLAM%20Systems%20with%20high%20accuracy.%20However%2C%20more%0Acapabilities%20of%20these%20systems%20are%20yet%20to%20be%20explored%2C%20such%20as%20platform%0Aagnosticism%20across%20varying%20sensor%20suites%20and%20multi-session%20SLAM.%20These%20factors%0Aindirectly%20serve%20as%20an%20indicator%20of%20robustness%20and%20ease%20of%20deployment%20in%0Areal-world%20applications.%20There%20exists%20no%20dataset%20plus%20benchmark%20combination%0Apublicly%20available%2C%20which%20considers%20these%20factors%20combined.%20The%20Hilti%20SLAM%0AChallenge%202023%20Dataset%20and%20Benchmark%20addresses%20this%20issue.%20Additionally%2C%20we%0Apropose%20a%20novel%20fiducial%20marker%20design%20for%20a%20pre-surveyed%20point%20on%20the%20ground%0Ato%20be%20observable%20from%20an%20off-the-shelf%20LiDAR%20mounted%20on%20a%20robot%2C%20and%20an%0Aalgorithm%20to%20estimate%20its%20position%20at%20mm-level%20accuracy.%20Results%20from%20the%0Achallenge%20show%20an%20increase%20in%20overall%20participation%2C%20single-session%20SLAM%0Asystems%20getting%20increasingly%20accurate%2C%20successfully%20operating%20across%20varying%0Asensor%20suites%2C%20but%20relatively%20few%20participants%20performing%20multi-session%20SLAM.%0ADataset%20URL%3A%20https%3A//www.hilti-challenge.com/dataset-2023.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09765v2&entry.124074799=Read"},
{"title": "KI-PMF: Knowledge Integrated Plausible Motion Forecasting", "author": "Abhishek Vivekanandan and Ahmed Abouelazm and Philip Sch\u00f6rner and J. Marius Z\u00f6llner", "abstract": "  Accurately forecasting the motion of traffic actors is crucial for the\ndeployment of autonomous vehicles at a large scale. Current trajectory\nforecasting approaches primarily concentrate on optimizing a loss function with\na specific metric, which can result in predictions that do not adhere to\nphysical laws or violate external constraints. Our objective is to incorporate\nexplicit knowledge priors that allow a network to forecast future trajectories\nin compliance with both the kinematic constraints of a vehicle and the geometry\nof the driving environment. To achieve this, we introduce a non-parametric\npruning layer and attention layers to integrate the defined knowledge priors.\nOur proposed method is designed to ensure reachability guarantees for traffic\nactors in both complex and dynamic situations. By conditioning the network to\nfollow physical laws, we can obtain accurate and safe predictions, essential\nfor maintaining autonomous vehicles' safety and efficiency in real-world\nsettings.In summary, this paper presents concepts that prevent off-road\npredictions for safe and reliable motion forecasting by incorporating knowledge\npriors into the training process.\n", "link": "http://arxiv.org/abs/2310.12007v3", "date": "2024-07-30", "relevancy": 2.1071, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KI-PMF%3A%20Knowledge%20Integrated%20Plausible%20Motion%20Forecasting&body=Title%3A%20KI-PMF%3A%20Knowledge%20Integrated%20Plausible%20Motion%20Forecasting%0AAuthor%3A%20Abhishek%20Vivekanandan%20and%20Ahmed%20Abouelazm%20and%20Philip%20Sch%C3%B6rner%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Accurately%20forecasting%20the%20motion%20of%20traffic%20actors%20is%20crucial%20for%20the%0Adeployment%20of%20autonomous%20vehicles%20at%20a%20large%20scale.%20Current%20trajectory%0Aforecasting%20approaches%20primarily%20concentrate%20on%20optimizing%20a%20loss%20function%20with%0Aa%20specific%20metric%2C%20which%20can%20result%20in%20predictions%20that%20do%20not%20adhere%20to%0Aphysical%20laws%20or%20violate%20external%20constraints.%20Our%20objective%20is%20to%20incorporate%0Aexplicit%20knowledge%20priors%20that%20allow%20a%20network%20to%20forecast%20future%20trajectories%0Ain%20compliance%20with%20both%20the%20kinematic%20constraints%20of%20a%20vehicle%20and%20the%20geometry%0Aof%20the%20driving%20environment.%20To%20achieve%20this%2C%20we%20introduce%20a%20non-parametric%0Apruning%20layer%20and%20attention%20layers%20to%20integrate%20the%20defined%20knowledge%20priors.%0AOur%20proposed%20method%20is%20designed%20to%20ensure%20reachability%20guarantees%20for%20traffic%0Aactors%20in%20both%20complex%20and%20dynamic%20situations.%20By%20conditioning%20the%20network%20to%0Afollow%20physical%20laws%2C%20we%20can%20obtain%20accurate%20and%20safe%20predictions%2C%20essential%0Afor%20maintaining%20autonomous%20vehicles%27%20safety%20and%20efficiency%20in%20real-world%0Asettings.In%20summary%2C%20this%20paper%20presents%20concepts%20that%20prevent%20off-road%0Apredictions%20for%20safe%20and%20reliable%20motion%20forecasting%20by%20incorporating%20knowledge%0Apriors%20into%20the%20training%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12007v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKI-PMF%253A%2520Knowledge%2520Integrated%2520Plausible%2520Motion%2520Forecasting%26entry.906535625%3DAbhishek%2520Vivekanandan%2520and%2520Ahmed%2520Abouelazm%2520and%2520Philip%2520Sch%25C3%25B6rner%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520Accurately%2520forecasting%2520the%2520motion%2520of%2520traffic%2520actors%2520is%2520crucial%2520for%2520the%250Adeployment%2520of%2520autonomous%2520vehicles%2520at%2520a%2520large%2520scale.%2520Current%2520trajectory%250Aforecasting%2520approaches%2520primarily%2520concentrate%2520on%2520optimizing%2520a%2520loss%2520function%2520with%250Aa%2520specific%2520metric%252C%2520which%2520can%2520result%2520in%2520predictions%2520that%2520do%2520not%2520adhere%2520to%250Aphysical%2520laws%2520or%2520violate%2520external%2520constraints.%2520Our%2520objective%2520is%2520to%2520incorporate%250Aexplicit%2520knowledge%2520priors%2520that%2520allow%2520a%2520network%2520to%2520forecast%2520future%2520trajectories%250Ain%2520compliance%2520with%2520both%2520the%2520kinematic%2520constraints%2520of%2520a%2520vehicle%2520and%2520the%2520geometry%250Aof%2520the%2520driving%2520environment.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520non-parametric%250Apruning%2520layer%2520and%2520attention%2520layers%2520to%2520integrate%2520the%2520defined%2520knowledge%2520priors.%250AOur%2520proposed%2520method%2520is%2520designed%2520to%2520ensure%2520reachability%2520guarantees%2520for%2520traffic%250Aactors%2520in%2520both%2520complex%2520and%2520dynamic%2520situations.%2520By%2520conditioning%2520the%2520network%2520to%250Afollow%2520physical%2520laws%252C%2520we%2520can%2520obtain%2520accurate%2520and%2520safe%2520predictions%252C%2520essential%250Afor%2520maintaining%2520autonomous%2520vehicles%2527%2520safety%2520and%2520efficiency%2520in%2520real-world%250Asettings.In%2520summary%252C%2520this%2520paper%2520presents%2520concepts%2520that%2520prevent%2520off-road%250Apredictions%2520for%2520safe%2520and%2520reliable%2520motion%2520forecasting%2520by%2520incorporating%2520knowledge%250Apriors%2520into%2520the%2520training%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12007v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KI-PMF%3A%20Knowledge%20Integrated%20Plausible%20Motion%20Forecasting&entry.906535625=Abhishek%20Vivekanandan%20and%20Ahmed%20Abouelazm%20and%20Philip%20Sch%C3%B6rner%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Accurately%20forecasting%20the%20motion%20of%20traffic%20actors%20is%20crucial%20for%20the%0Adeployment%20of%20autonomous%20vehicles%20at%20a%20large%20scale.%20Current%20trajectory%0Aforecasting%20approaches%20primarily%20concentrate%20on%20optimizing%20a%20loss%20function%20with%0Aa%20specific%20metric%2C%20which%20can%20result%20in%20predictions%20that%20do%20not%20adhere%20to%0Aphysical%20laws%20or%20violate%20external%20constraints.%20Our%20objective%20is%20to%20incorporate%0Aexplicit%20knowledge%20priors%20that%20allow%20a%20network%20to%20forecast%20future%20trajectories%0Ain%20compliance%20with%20both%20the%20kinematic%20constraints%20of%20a%20vehicle%20and%20the%20geometry%0Aof%20the%20driving%20environment.%20To%20achieve%20this%2C%20we%20introduce%20a%20non-parametric%0Apruning%20layer%20and%20attention%20layers%20to%20integrate%20the%20defined%20knowledge%20priors.%0AOur%20proposed%20method%20is%20designed%20to%20ensure%20reachability%20guarantees%20for%20traffic%0Aactors%20in%20both%20complex%20and%20dynamic%20situations.%20By%20conditioning%20the%20network%20to%0Afollow%20physical%20laws%2C%20we%20can%20obtain%20accurate%20and%20safe%20predictions%2C%20essential%0Afor%20maintaining%20autonomous%20vehicles%27%20safety%20and%20efficiency%20in%20real-world%0Asettings.In%20summary%2C%20this%20paper%20presents%20concepts%20that%20prevent%20off-road%0Apredictions%20for%20safe%20and%20reliable%20motion%20forecasting%20by%20incorporating%20knowledge%0Apriors%20into%20the%20training%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12007v3&entry.124074799=Read"},
{"title": "JSSL: Joint Supervised and Self-supervised Learning for MRI\n  Reconstruction", "author": "George Yiasemis and Nikita Moriakov and Clara I. S\u00e1nchez and Jan-Jakob Sonke and Jonas Teuwen", "abstract": "  Purpose: MRI represents an important diagnostic modality; however, its\ninherently slow acquisition process poses challenges in obtaining fully-sampled\nk-space data under motion. In the absence of fully-sampled acquisitions,\nserving as ground truths, training deep learning algorithms in a supervised\nmanner to predict the underlying ground truth image becomes challenging. To\naddress this limitation, self-supervised methods have emerged as a viable\nalternative, leveraging available subsampled k-space data to train deep neural\nnetworks for MRI reconstruction. Nevertheless, these approaches often fall\nshort when compared to supervised methods.\n  Methods: We propose Joint Supervised and Self-supervised Learning (JSSL), a\nnovel training approach for deep learning-based MRI reconstruction algorithms\naimed at enhancing reconstruction quality in cases where target datasets\ncontaining fully-sampled k-space measurements are unavailable. JSSL operates by\nsimultaneously training a model in a self-supervised learning setting, using\nsubsampled data from the target dataset(s), and in a supervised learning\nmanner, utilizing datasets with fully-sampled k-space data, referred to as\nproxy datasets. We demonstrate JSSL's efficacy using subsampled prostate or\ncardiac MRI data as the target datasets, with fully-sampled brain and knee, or\nbrain, knee and prostate k-space acquisitions, respectively, as proxy datasets.\n  Results: Our results showcase substantial improvements over conventional\nself-supervised methods, validated using common image quality metrics.\nFurthermore, we provide theoretical motivations for JSSL and establish\nrule-of-thumb guidelines for training MRI reconstruction models.\n  Conclusion: JSSL effectively enhances MRI reconstruction quality in scenarios\nwhere fully-sampled k-space data is not available, leveraging the strengths of\nsupervised learning by incorporating proxy datasets.\n", "link": "http://arxiv.org/abs/2311.15856v2", "date": "2024-07-30", "relevancy": 2.0845, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5374}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5221}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JSSL%3A%20Joint%20Supervised%20and%20Self-supervised%20Learning%20for%20MRI%0A%20%20Reconstruction&body=Title%3A%20JSSL%3A%20Joint%20Supervised%20and%20Self-supervised%20Learning%20for%20MRI%0A%20%20Reconstruction%0AAuthor%3A%20George%20Yiasemis%20and%20Nikita%20Moriakov%20and%20Clara%20I.%20S%C3%A1nchez%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen%0AAbstract%3A%20%20%20Purpose%3A%20MRI%20represents%20an%20important%20diagnostic%20modality%3B%20however%2C%20its%0Ainherently%20slow%20acquisition%20process%20poses%20challenges%20in%20obtaining%20fully-sampled%0Ak-space%20data%20under%20motion.%20In%20the%20absence%20of%20fully-sampled%20acquisitions%2C%0Aserving%20as%20ground%20truths%2C%20training%20deep%20learning%20algorithms%20in%20a%20supervised%0Amanner%20to%20predict%20the%20underlying%20ground%20truth%20image%20becomes%20challenging.%20To%0Aaddress%20this%20limitation%2C%20self-supervised%20methods%20have%20emerged%20as%20a%20viable%0Aalternative%2C%20leveraging%20available%20subsampled%20k-space%20data%20to%20train%20deep%20neural%0Anetworks%20for%20MRI%20reconstruction.%20Nevertheless%2C%20these%20approaches%20often%20fall%0Ashort%20when%20compared%20to%20supervised%20methods.%0A%20%20Methods%3A%20We%20propose%20Joint%20Supervised%20and%20Self-supervised%20Learning%20%28JSSL%29%2C%20a%0Anovel%20training%20approach%20for%20deep%20learning-based%20MRI%20reconstruction%20algorithms%0Aaimed%20at%20enhancing%20reconstruction%20quality%20in%20cases%20where%20target%20datasets%0Acontaining%20fully-sampled%20k-space%20measurements%20are%20unavailable.%20JSSL%20operates%20by%0Asimultaneously%20training%20a%20model%20in%20a%20self-supervised%20learning%20setting%2C%20using%0Asubsampled%20data%20from%20the%20target%20dataset%28s%29%2C%20and%20in%20a%20supervised%20learning%0Amanner%2C%20utilizing%20datasets%20with%20fully-sampled%20k-space%20data%2C%20referred%20to%20as%0Aproxy%20datasets.%20We%20demonstrate%20JSSL%27s%20efficacy%20using%20subsampled%20prostate%20or%0Acardiac%20MRI%20data%20as%20the%20target%20datasets%2C%20with%20fully-sampled%20brain%20and%20knee%2C%20or%0Abrain%2C%20knee%20and%20prostate%20k-space%20acquisitions%2C%20respectively%2C%20as%20proxy%20datasets.%0A%20%20Results%3A%20Our%20results%20showcase%20substantial%20improvements%20over%20conventional%0Aself-supervised%20methods%2C%20validated%20using%20common%20image%20quality%20metrics.%0AFurthermore%2C%20we%20provide%20theoretical%20motivations%20for%20JSSL%20and%20establish%0Arule-of-thumb%20guidelines%20for%20training%20MRI%20reconstruction%20models.%0A%20%20Conclusion%3A%20JSSL%20effectively%20enhances%20MRI%20reconstruction%20quality%20in%20scenarios%0Awhere%20fully-sampled%20k-space%20data%20is%20not%20available%2C%20leveraging%20the%20strengths%20of%0Asupervised%20learning%20by%20incorporating%20proxy%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJSSL%253A%2520Joint%2520Supervised%2520and%2520Self-supervised%2520Learning%2520for%2520MRI%250A%2520%2520Reconstruction%26entry.906535625%3DGeorge%2520Yiasemis%2520and%2520Nikita%2520Moriakov%2520and%2520Clara%2520I.%2520S%25C3%25A1nchez%2520and%2520Jan-Jakob%2520Sonke%2520and%2520Jonas%2520Teuwen%26entry.1292438233%3D%2520%2520Purpose%253A%2520MRI%2520represents%2520an%2520important%2520diagnostic%2520modality%253B%2520however%252C%2520its%250Ainherently%2520slow%2520acquisition%2520process%2520poses%2520challenges%2520in%2520obtaining%2520fully-sampled%250Ak-space%2520data%2520under%2520motion.%2520In%2520the%2520absence%2520of%2520fully-sampled%2520acquisitions%252C%250Aserving%2520as%2520ground%2520truths%252C%2520training%2520deep%2520learning%2520algorithms%2520in%2520a%2520supervised%250Amanner%2520to%2520predict%2520the%2520underlying%2520ground%2520truth%2520image%2520becomes%2520challenging.%2520To%250Aaddress%2520this%2520limitation%252C%2520self-supervised%2520methods%2520have%2520emerged%2520as%2520a%2520viable%250Aalternative%252C%2520leveraging%2520available%2520subsampled%2520k-space%2520data%2520to%2520train%2520deep%2520neural%250Anetworks%2520for%2520MRI%2520reconstruction.%2520Nevertheless%252C%2520these%2520approaches%2520often%2520fall%250Ashort%2520when%2520compared%2520to%2520supervised%2520methods.%250A%2520%2520Methods%253A%2520We%2520propose%2520Joint%2520Supervised%2520and%2520Self-supervised%2520Learning%2520%2528JSSL%2529%252C%2520a%250Anovel%2520training%2520approach%2520for%2520deep%2520learning-based%2520MRI%2520reconstruction%2520algorithms%250Aaimed%2520at%2520enhancing%2520reconstruction%2520quality%2520in%2520cases%2520where%2520target%2520datasets%250Acontaining%2520fully-sampled%2520k-space%2520measurements%2520are%2520unavailable.%2520JSSL%2520operates%2520by%250Asimultaneously%2520training%2520a%2520model%2520in%2520a%2520self-supervised%2520learning%2520setting%252C%2520using%250Asubsampled%2520data%2520from%2520the%2520target%2520dataset%2528s%2529%252C%2520and%2520in%2520a%2520supervised%2520learning%250Amanner%252C%2520utilizing%2520datasets%2520with%2520fully-sampled%2520k-space%2520data%252C%2520referred%2520to%2520as%250Aproxy%2520datasets.%2520We%2520demonstrate%2520JSSL%2527s%2520efficacy%2520using%2520subsampled%2520prostate%2520or%250Acardiac%2520MRI%2520data%2520as%2520the%2520target%2520datasets%252C%2520with%2520fully-sampled%2520brain%2520and%2520knee%252C%2520or%250Abrain%252C%2520knee%2520and%2520prostate%2520k-space%2520acquisitions%252C%2520respectively%252C%2520as%2520proxy%2520datasets.%250A%2520%2520Results%253A%2520Our%2520results%2520showcase%2520substantial%2520improvements%2520over%2520conventional%250Aself-supervised%2520methods%252C%2520validated%2520using%2520common%2520image%2520quality%2520metrics.%250AFurthermore%252C%2520we%2520provide%2520theoretical%2520motivations%2520for%2520JSSL%2520and%2520establish%250Arule-of-thumb%2520guidelines%2520for%2520training%2520MRI%2520reconstruction%2520models.%250A%2520%2520Conclusion%253A%2520JSSL%2520effectively%2520enhances%2520MRI%2520reconstruction%2520quality%2520in%2520scenarios%250Awhere%2520fully-sampled%2520k-space%2520data%2520is%2520not%2520available%252C%2520leveraging%2520the%2520strengths%2520of%250Asupervised%2520learning%2520by%2520incorporating%2520proxy%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JSSL%3A%20Joint%20Supervised%20and%20Self-supervised%20Learning%20for%20MRI%0A%20%20Reconstruction&entry.906535625=George%20Yiasemis%20and%20Nikita%20Moriakov%20and%20Clara%20I.%20S%C3%A1nchez%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen&entry.1292438233=%20%20Purpose%3A%20MRI%20represents%20an%20important%20diagnostic%20modality%3B%20however%2C%20its%0Ainherently%20slow%20acquisition%20process%20poses%20challenges%20in%20obtaining%20fully-sampled%0Ak-space%20data%20under%20motion.%20In%20the%20absence%20of%20fully-sampled%20acquisitions%2C%0Aserving%20as%20ground%20truths%2C%20training%20deep%20learning%20algorithms%20in%20a%20supervised%0Amanner%20to%20predict%20the%20underlying%20ground%20truth%20image%20becomes%20challenging.%20To%0Aaddress%20this%20limitation%2C%20self-supervised%20methods%20have%20emerged%20as%20a%20viable%0Aalternative%2C%20leveraging%20available%20subsampled%20k-space%20data%20to%20train%20deep%20neural%0Anetworks%20for%20MRI%20reconstruction.%20Nevertheless%2C%20these%20approaches%20often%20fall%0Ashort%20when%20compared%20to%20supervised%20methods.%0A%20%20Methods%3A%20We%20propose%20Joint%20Supervised%20and%20Self-supervised%20Learning%20%28JSSL%29%2C%20a%0Anovel%20training%20approach%20for%20deep%20learning-based%20MRI%20reconstruction%20algorithms%0Aaimed%20at%20enhancing%20reconstruction%20quality%20in%20cases%20where%20target%20datasets%0Acontaining%20fully-sampled%20k-space%20measurements%20are%20unavailable.%20JSSL%20operates%20by%0Asimultaneously%20training%20a%20model%20in%20a%20self-supervised%20learning%20setting%2C%20using%0Asubsampled%20data%20from%20the%20target%20dataset%28s%29%2C%20and%20in%20a%20supervised%20learning%0Amanner%2C%20utilizing%20datasets%20with%20fully-sampled%20k-space%20data%2C%20referred%20to%20as%0Aproxy%20datasets.%20We%20demonstrate%20JSSL%27s%20efficacy%20using%20subsampled%20prostate%20or%0Acardiac%20MRI%20data%20as%20the%20target%20datasets%2C%20with%20fully-sampled%20brain%20and%20knee%2C%20or%0Abrain%2C%20knee%20and%20prostate%20k-space%20acquisitions%2C%20respectively%2C%20as%20proxy%20datasets.%0A%20%20Results%3A%20Our%20results%20showcase%20substantial%20improvements%20over%20conventional%0Aself-supervised%20methods%2C%20validated%20using%20common%20image%20quality%20metrics.%0AFurthermore%2C%20we%20provide%20theoretical%20motivations%20for%20JSSL%20and%20establish%0Arule-of-thumb%20guidelines%20for%20training%20MRI%20reconstruction%20models.%0A%20%20Conclusion%3A%20JSSL%20effectively%20enhances%20MRI%20reconstruction%20quality%20in%20scenarios%0Awhere%20fully-sampled%20k-space%20data%20is%20not%20available%2C%20leveraging%20the%20strengths%20of%0Asupervised%20learning%20by%20incorporating%20proxy%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15856v2&entry.124074799=Read"},
{"title": "S3PET: Semi-supervised Standard-dose PET Image Reconstruction via\n  Dose-aware Token Swap", "author": "Jiaqi Cui and Pinxian Zeng and Yuanyuan Xu and Xi Wu and Jiliu Zhou and Yan Wang", "abstract": "  To acquire high-quality positron emission tomography (PET) images while\nreducing the radiation tracer dose, numerous efforts have been devoted to\nreconstructing standard-dose PET (SPET) images from low-dose PET (LPET).\nHowever, the success of current fully-supervised approaches relies on abundant\npaired LPET and SPET images, which are often unavailable in clinic. Moreover,\nthese methods often mix the dose-invariant content with dose level-related\ndose-specific details during reconstruction, resulting in distorted images. To\nalleviate these problems, in this paper, we propose a two-stage Semi-Supervised\nSPET reconstruction framework, namely S3PET, to accommodate the training of\nabundant unpaired and limited paired SPET and LPET images. Our S3PET involves\nan un-supervised pre-training stage (Stage I) to extract representations from\nunpaired images, and a supervised dose-aware reconstruction stage (Stage II) to\nachieve LPET-to-SPET reconstruction by transferring the dose-specific knowledge\nbetween paired images. Specifically, in stage I, two independent dose-specific\nmasked autoencoders (DsMAEs) are adopted to comprehensively understand the\nunpaired SPET and LPET images. Then, in Stage II, the pre-trained DsMAEs are\nfurther finetuned using paired images. To prevent distortions in both content\nand details, we introduce two elaborate modules, i.e., a dose knowledge\ndecouple module to disentangle the respective dose-specific and dose-invariant\nknowledge of LPET and SPET, and a dose-specific knowledge learning module to\ntransfer the dose-specific information from SPET to LPET, thereby achieving\nhigh-quality SPET reconstruction from LPET images. Experiments on two datasets\ndemonstrate that our S3PET achieves state-of-the-art performance quantitatively\nand qualitatively.\n", "link": "http://arxiv.org/abs/2407.20878v1", "date": "2024-07-30", "relevancy": 2.0821, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5331}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5183}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S3PET%3A%20Semi-supervised%20Standard-dose%20PET%20Image%20Reconstruction%20via%0A%20%20Dose-aware%20Token%20Swap&body=Title%3A%20S3PET%3A%20Semi-supervised%20Standard-dose%20PET%20Image%20Reconstruction%20via%0A%20%20Dose-aware%20Token%20Swap%0AAuthor%3A%20Jiaqi%20Cui%20and%20Pinxian%20Zeng%20and%20Yuanyuan%20Xu%20and%20Xi%20Wu%20and%20Jiliu%20Zhou%20and%20Yan%20Wang%0AAbstract%3A%20%20%20To%20acquire%20high-quality%20positron%20emission%20tomography%20%28PET%29%20images%20while%0Areducing%20the%20radiation%20tracer%20dose%2C%20numerous%20efforts%20have%20been%20devoted%20to%0Areconstructing%20standard-dose%20PET%20%28SPET%29%20images%20from%20low-dose%20PET%20%28LPET%29.%0AHowever%2C%20the%20success%20of%20current%20fully-supervised%20approaches%20relies%20on%20abundant%0Apaired%20LPET%20and%20SPET%20images%2C%20which%20are%20often%20unavailable%20in%20clinic.%20Moreover%2C%0Athese%20methods%20often%20mix%20the%20dose-invariant%20content%20with%20dose%20level-related%0Adose-specific%20details%20during%20reconstruction%2C%20resulting%20in%20distorted%20images.%20To%0Aalleviate%20these%20problems%2C%20in%20this%20paper%2C%20we%20propose%20a%20two-stage%20Semi-Supervised%0ASPET%20reconstruction%20framework%2C%20namely%20S3PET%2C%20to%20accommodate%20the%20training%20of%0Aabundant%20unpaired%20and%20limited%20paired%20SPET%20and%20LPET%20images.%20Our%20S3PET%20involves%0Aan%20un-supervised%20pre-training%20stage%20%28Stage%20I%29%20to%20extract%20representations%20from%0Aunpaired%20images%2C%20and%20a%20supervised%20dose-aware%20reconstruction%20stage%20%28Stage%20II%29%20to%0Aachieve%20LPET-to-SPET%20reconstruction%20by%20transferring%20the%20dose-specific%20knowledge%0Abetween%20paired%20images.%20Specifically%2C%20in%20stage%20I%2C%20two%20independent%20dose-specific%0Amasked%20autoencoders%20%28DsMAEs%29%20are%20adopted%20to%20comprehensively%20understand%20the%0Aunpaired%20SPET%20and%20LPET%20images.%20Then%2C%20in%20Stage%20II%2C%20the%20pre-trained%20DsMAEs%20are%0Afurther%20finetuned%20using%20paired%20images.%20To%20prevent%20distortions%20in%20both%20content%0Aand%20details%2C%20we%20introduce%20two%20elaborate%20modules%2C%20i.e.%2C%20a%20dose%20knowledge%0Adecouple%20module%20to%20disentangle%20the%20respective%20dose-specific%20and%20dose-invariant%0Aknowledge%20of%20LPET%20and%20SPET%2C%20and%20a%20dose-specific%20knowledge%20learning%20module%20to%0Atransfer%20the%20dose-specific%20information%20from%20SPET%20to%20LPET%2C%20thereby%20achieving%0Ahigh-quality%20SPET%20reconstruction%20from%20LPET%20images.%20Experiments%20on%20two%20datasets%0Ademonstrate%20that%20our%20S3PET%20achieves%20state-of-the-art%20performance%20quantitatively%0Aand%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS3PET%253A%2520Semi-supervised%2520Standard-dose%2520PET%2520Image%2520Reconstruction%2520via%250A%2520%2520Dose-aware%2520Token%2520Swap%26entry.906535625%3DJiaqi%2520Cui%2520and%2520Pinxian%2520Zeng%2520and%2520Yuanyuan%2520Xu%2520and%2520Xi%2520Wu%2520and%2520Jiliu%2520Zhou%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520To%2520acquire%2520high-quality%2520positron%2520emission%2520tomography%2520%2528PET%2529%2520images%2520while%250Areducing%2520the%2520radiation%2520tracer%2520dose%252C%2520numerous%2520efforts%2520have%2520been%2520devoted%2520to%250Areconstructing%2520standard-dose%2520PET%2520%2528SPET%2529%2520images%2520from%2520low-dose%2520PET%2520%2528LPET%2529.%250AHowever%252C%2520the%2520success%2520of%2520current%2520fully-supervised%2520approaches%2520relies%2520on%2520abundant%250Apaired%2520LPET%2520and%2520SPET%2520images%252C%2520which%2520are%2520often%2520unavailable%2520in%2520clinic.%2520Moreover%252C%250Athese%2520methods%2520often%2520mix%2520the%2520dose-invariant%2520content%2520with%2520dose%2520level-related%250Adose-specific%2520details%2520during%2520reconstruction%252C%2520resulting%2520in%2520distorted%2520images.%2520To%250Aalleviate%2520these%2520problems%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520two-stage%2520Semi-Supervised%250ASPET%2520reconstruction%2520framework%252C%2520namely%2520S3PET%252C%2520to%2520accommodate%2520the%2520training%2520of%250Aabundant%2520unpaired%2520and%2520limited%2520paired%2520SPET%2520and%2520LPET%2520images.%2520Our%2520S3PET%2520involves%250Aan%2520un-supervised%2520pre-training%2520stage%2520%2528Stage%2520I%2529%2520to%2520extract%2520representations%2520from%250Aunpaired%2520images%252C%2520and%2520a%2520supervised%2520dose-aware%2520reconstruction%2520stage%2520%2528Stage%2520II%2529%2520to%250Aachieve%2520LPET-to-SPET%2520reconstruction%2520by%2520transferring%2520the%2520dose-specific%2520knowledge%250Abetween%2520paired%2520images.%2520Specifically%252C%2520in%2520stage%2520I%252C%2520two%2520independent%2520dose-specific%250Amasked%2520autoencoders%2520%2528DsMAEs%2529%2520are%2520adopted%2520to%2520comprehensively%2520understand%2520the%250Aunpaired%2520SPET%2520and%2520LPET%2520images.%2520Then%252C%2520in%2520Stage%2520II%252C%2520the%2520pre-trained%2520DsMAEs%2520are%250Afurther%2520finetuned%2520using%2520paired%2520images.%2520To%2520prevent%2520distortions%2520in%2520both%2520content%250Aand%2520details%252C%2520we%2520introduce%2520two%2520elaborate%2520modules%252C%2520i.e.%252C%2520a%2520dose%2520knowledge%250Adecouple%2520module%2520to%2520disentangle%2520the%2520respective%2520dose-specific%2520and%2520dose-invariant%250Aknowledge%2520of%2520LPET%2520and%2520SPET%252C%2520and%2520a%2520dose-specific%2520knowledge%2520learning%2520module%2520to%250Atransfer%2520the%2520dose-specific%2520information%2520from%2520SPET%2520to%2520LPET%252C%2520thereby%2520achieving%250Ahigh-quality%2520SPET%2520reconstruction%2520from%2520LPET%2520images.%2520Experiments%2520on%2520two%2520datasets%250Ademonstrate%2520that%2520our%2520S3PET%2520achieves%2520state-of-the-art%2520performance%2520quantitatively%250Aand%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S3PET%3A%20Semi-supervised%20Standard-dose%20PET%20Image%20Reconstruction%20via%0A%20%20Dose-aware%20Token%20Swap&entry.906535625=Jiaqi%20Cui%20and%20Pinxian%20Zeng%20and%20Yuanyuan%20Xu%20and%20Xi%20Wu%20and%20Jiliu%20Zhou%20and%20Yan%20Wang&entry.1292438233=%20%20To%20acquire%20high-quality%20positron%20emission%20tomography%20%28PET%29%20images%20while%0Areducing%20the%20radiation%20tracer%20dose%2C%20numerous%20efforts%20have%20been%20devoted%20to%0Areconstructing%20standard-dose%20PET%20%28SPET%29%20images%20from%20low-dose%20PET%20%28LPET%29.%0AHowever%2C%20the%20success%20of%20current%20fully-supervised%20approaches%20relies%20on%20abundant%0Apaired%20LPET%20and%20SPET%20images%2C%20which%20are%20often%20unavailable%20in%20clinic.%20Moreover%2C%0Athese%20methods%20often%20mix%20the%20dose-invariant%20content%20with%20dose%20level-related%0Adose-specific%20details%20during%20reconstruction%2C%20resulting%20in%20distorted%20images.%20To%0Aalleviate%20these%20problems%2C%20in%20this%20paper%2C%20we%20propose%20a%20two-stage%20Semi-Supervised%0ASPET%20reconstruction%20framework%2C%20namely%20S3PET%2C%20to%20accommodate%20the%20training%20of%0Aabundant%20unpaired%20and%20limited%20paired%20SPET%20and%20LPET%20images.%20Our%20S3PET%20involves%0Aan%20un-supervised%20pre-training%20stage%20%28Stage%20I%29%20to%20extract%20representations%20from%0Aunpaired%20images%2C%20and%20a%20supervised%20dose-aware%20reconstruction%20stage%20%28Stage%20II%29%20to%0Aachieve%20LPET-to-SPET%20reconstruction%20by%20transferring%20the%20dose-specific%20knowledge%0Abetween%20paired%20images.%20Specifically%2C%20in%20stage%20I%2C%20two%20independent%20dose-specific%0Amasked%20autoencoders%20%28DsMAEs%29%20are%20adopted%20to%20comprehensively%20understand%20the%0Aunpaired%20SPET%20and%20LPET%20images.%20Then%2C%20in%20Stage%20II%2C%20the%20pre-trained%20DsMAEs%20are%0Afurther%20finetuned%20using%20paired%20images.%20To%20prevent%20distortions%20in%20both%20content%0Aand%20details%2C%20we%20introduce%20two%20elaborate%20modules%2C%20i.e.%2C%20a%20dose%20knowledge%0Adecouple%20module%20to%20disentangle%20the%20respective%20dose-specific%20and%20dose-invariant%0Aknowledge%20of%20LPET%20and%20SPET%2C%20and%20a%20dose-specific%20knowledge%20learning%20module%20to%0Atransfer%20the%20dose-specific%20information%20from%20SPET%20to%20LPET%2C%20thereby%20achieving%0Ahigh-quality%20SPET%20reconstruction%20from%20LPET%20images.%20Experiments%20on%20two%20datasets%0Ademonstrate%20that%20our%20S3PET%20achieves%20state-of-the-art%20performance%20quantitatively%0Aand%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20878v1&entry.124074799=Read"},
{"title": "The Sky's the Limit: Re-lightable Outdoor Scenes via a Sky-pixel\n  Constrained Illumination Prior and Outside-In Visibility", "author": "James A. D. Gardner and Evgenii Kashin and Bernhard Egger and William A. P. Smith", "abstract": "  Inverse rendering of outdoor scenes from unconstrained image collections is a\nchallenging task, particularly illumination/albedo ambiguities and occlusion of\nthe illumination environment (shadowing) caused by geometry. However, there are\nmany cues in an image that can aid in the disentanglement of geometry, albedo\nand shadows. Whilst sky is frequently masked out in state-of-the-art methods,\nwe exploit the fact that any sky pixel provides a direct observation of distant\nlighting in the corresponding direction and, via a neural illumination prior, a\nstatistical cue to derive the remaining illumination environment. The\nincorporation of our illumination prior is enabled by a novel `outside-in'\nmethod for computing differentiable sky visibility based on a neural\ndirectional distance function. This is highly efficient and can be trained in\nparallel with the neural scene representation, allowing gradients from\nappearance loss to flow from shadows to influence the estimation of\nillumination and geometry. Our method estimates high-quality albedo, geometry,\nillumination and sky visibility, achieving state-of-the-art results on the\nNeRF-OSR relighting benchmark. Our code and models can be found at\nhttps://github.com/JADGardner/neusky\n", "link": "http://arxiv.org/abs/2311.16937v2", "date": "2024-07-30", "relevancy": 2.0733, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5259}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5236}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Sky%27s%20the%20Limit%3A%20Re-lightable%20Outdoor%20Scenes%20via%20a%20Sky-pixel%0A%20%20Constrained%20Illumination%20Prior%20and%20Outside-In%20Visibility&body=Title%3A%20The%20Sky%27s%20the%20Limit%3A%20Re-lightable%20Outdoor%20Scenes%20via%20a%20Sky-pixel%0A%20%20Constrained%20Illumination%20Prior%20and%20Outside-In%20Visibility%0AAuthor%3A%20James%20A.%20D.%20Gardner%20and%20Evgenii%20Kashin%20and%20Bernhard%20Egger%20and%20William%20A.%20P.%20Smith%0AAbstract%3A%20%20%20Inverse%20rendering%20of%20outdoor%20scenes%20from%20unconstrained%20image%20collections%20is%20a%0Achallenging%20task%2C%20particularly%20illumination/albedo%20ambiguities%20and%20occlusion%20of%0Athe%20illumination%20environment%20%28shadowing%29%20caused%20by%20geometry.%20However%2C%20there%20are%0Amany%20cues%20in%20an%20image%20that%20can%20aid%20in%20the%20disentanglement%20of%20geometry%2C%20albedo%0Aand%20shadows.%20Whilst%20sky%20is%20frequently%20masked%20out%20in%20state-of-the-art%20methods%2C%0Awe%20exploit%20the%20fact%20that%20any%20sky%20pixel%20provides%20a%20direct%20observation%20of%20distant%0Alighting%20in%20the%20corresponding%20direction%20and%2C%20via%20a%20neural%20illumination%20prior%2C%20a%0Astatistical%20cue%20to%20derive%20the%20remaining%20illumination%20environment.%20The%0Aincorporation%20of%20our%20illumination%20prior%20is%20enabled%20by%20a%20novel%20%60outside-in%27%0Amethod%20for%20computing%20differentiable%20sky%20visibility%20based%20on%20a%20neural%0Adirectional%20distance%20function.%20This%20is%20highly%20efficient%20and%20can%20be%20trained%20in%0Aparallel%20with%20the%20neural%20scene%20representation%2C%20allowing%20gradients%20from%0Aappearance%20loss%20to%20flow%20from%20shadows%20to%20influence%20the%20estimation%20of%0Aillumination%20and%20geometry.%20Our%20method%20estimates%20high-quality%20albedo%2C%20geometry%2C%0Aillumination%20and%20sky%20visibility%2C%20achieving%20state-of-the-art%20results%20on%20the%0ANeRF-OSR%20relighting%20benchmark.%20Our%20code%20and%20models%20can%20be%20found%20at%0Ahttps%3A//github.com/JADGardner/neusky%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Sky%2527s%2520the%2520Limit%253A%2520Re-lightable%2520Outdoor%2520Scenes%2520via%2520a%2520Sky-pixel%250A%2520%2520Constrained%2520Illumination%2520Prior%2520and%2520Outside-In%2520Visibility%26entry.906535625%3DJames%2520A.%2520D.%2520Gardner%2520and%2520Evgenii%2520Kashin%2520and%2520Bernhard%2520Egger%2520and%2520William%2520A.%2520P.%2520Smith%26entry.1292438233%3D%2520%2520Inverse%2520rendering%2520of%2520outdoor%2520scenes%2520from%2520unconstrained%2520image%2520collections%2520is%2520a%250Achallenging%2520task%252C%2520particularly%2520illumination/albedo%2520ambiguities%2520and%2520occlusion%2520of%250Athe%2520illumination%2520environment%2520%2528shadowing%2529%2520caused%2520by%2520geometry.%2520However%252C%2520there%2520are%250Amany%2520cues%2520in%2520an%2520image%2520that%2520can%2520aid%2520in%2520the%2520disentanglement%2520of%2520geometry%252C%2520albedo%250Aand%2520shadows.%2520Whilst%2520sky%2520is%2520frequently%2520masked%2520out%2520in%2520state-of-the-art%2520methods%252C%250Awe%2520exploit%2520the%2520fact%2520that%2520any%2520sky%2520pixel%2520provides%2520a%2520direct%2520observation%2520of%2520distant%250Alighting%2520in%2520the%2520corresponding%2520direction%2520and%252C%2520via%2520a%2520neural%2520illumination%2520prior%252C%2520a%250Astatistical%2520cue%2520to%2520derive%2520the%2520remaining%2520illumination%2520environment.%2520The%250Aincorporation%2520of%2520our%2520illumination%2520prior%2520is%2520enabled%2520by%2520a%2520novel%2520%2560outside-in%2527%250Amethod%2520for%2520computing%2520differentiable%2520sky%2520visibility%2520based%2520on%2520a%2520neural%250Adirectional%2520distance%2520function.%2520This%2520is%2520highly%2520efficient%2520and%2520can%2520be%2520trained%2520in%250Aparallel%2520with%2520the%2520neural%2520scene%2520representation%252C%2520allowing%2520gradients%2520from%250Aappearance%2520loss%2520to%2520flow%2520from%2520shadows%2520to%2520influence%2520the%2520estimation%2520of%250Aillumination%2520and%2520geometry.%2520Our%2520method%2520estimates%2520high-quality%2520albedo%252C%2520geometry%252C%250Aillumination%2520and%2520sky%2520visibility%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520the%250ANeRF-OSR%2520relighting%2520benchmark.%2520Our%2520code%2520and%2520models%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/JADGardner/neusky%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Sky%27s%20the%20Limit%3A%20Re-lightable%20Outdoor%20Scenes%20via%20a%20Sky-pixel%0A%20%20Constrained%20Illumination%20Prior%20and%20Outside-In%20Visibility&entry.906535625=James%20A.%20D.%20Gardner%20and%20Evgenii%20Kashin%20and%20Bernhard%20Egger%20and%20William%20A.%20P.%20Smith&entry.1292438233=%20%20Inverse%20rendering%20of%20outdoor%20scenes%20from%20unconstrained%20image%20collections%20is%20a%0Achallenging%20task%2C%20particularly%20illumination/albedo%20ambiguities%20and%20occlusion%20of%0Athe%20illumination%20environment%20%28shadowing%29%20caused%20by%20geometry.%20However%2C%20there%20are%0Amany%20cues%20in%20an%20image%20that%20can%20aid%20in%20the%20disentanglement%20of%20geometry%2C%20albedo%0Aand%20shadows.%20Whilst%20sky%20is%20frequently%20masked%20out%20in%20state-of-the-art%20methods%2C%0Awe%20exploit%20the%20fact%20that%20any%20sky%20pixel%20provides%20a%20direct%20observation%20of%20distant%0Alighting%20in%20the%20corresponding%20direction%20and%2C%20via%20a%20neural%20illumination%20prior%2C%20a%0Astatistical%20cue%20to%20derive%20the%20remaining%20illumination%20environment.%20The%0Aincorporation%20of%20our%20illumination%20prior%20is%20enabled%20by%20a%20novel%20%60outside-in%27%0Amethod%20for%20computing%20differentiable%20sky%20visibility%20based%20on%20a%20neural%0Adirectional%20distance%20function.%20This%20is%20highly%20efficient%20and%20can%20be%20trained%20in%0Aparallel%20with%20the%20neural%20scene%20representation%2C%20allowing%20gradients%20from%0Aappearance%20loss%20to%20flow%20from%20shadows%20to%20influence%20the%20estimation%20of%0Aillumination%20and%20geometry.%20Our%20method%20estimates%20high-quality%20albedo%2C%20geometry%2C%0Aillumination%20and%20sky%20visibility%2C%20achieving%20state-of-the-art%20results%20on%20the%0ANeRF-OSR%20relighting%20benchmark.%20Our%20code%20and%20models%20can%20be%20found%20at%0Ahttps%3A//github.com/JADGardner/neusky%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16937v2&entry.124074799=Read"},
{"title": "DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain\n  Feature Extraction and Interaction Attention", "author": "Wei Wang and Jixing He and Xin Wang", "abstract": "  It is helpful in preventing colorectal cancer to detect and treat polyps in\nthe gastrointestinal tract early. However, there have been few studies to date\non designing polyp image classification networks that balance efficiency and\naccuracy. This challenge is mainly attributed to the fact that polyps are\nsimilar to other pathologies and have complex features influenced by texture,\ncolor, and morphology. In this paper, we propose a novel network DFE-IANet\nbased on both spectral transformation and feature interaction. Firstly, to\nextract detailed features and multi-scale features, the features are\ntransformed by the multi-scale frequency domain feature extraction (MSFD) block\nto extract texture details at the fine-grained level in the frequency domain.\nSecondly, the multi-scale interaction attention (MSIA) block is designed to\nenhance the network's capability of extracting critical features. This block\nintroduces multi-scale features into self-attention, aiming to adaptively guide\nthe network to concentrate on vital regions. Finally, with a compact parameter\nof only 4M, DFE-IANet outperforms the latest and classical networks in terms of\nefficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on\nthe challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of\n93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%,\nand VMamba by 1.88%. Our code is publicly available at\nhttps://github.com/PURSUETHESUN/DFE-IANet.\n", "link": "http://arxiv.org/abs/2407.20843v1", "date": "2024-07-30", "relevancy": 2.07, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5199}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5194}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFE-IANet%3A%20A%20Method%20for%20Polyp%20Image%20Classification%20Based%20on%20Dual-domain%0A%20%20Feature%20Extraction%20and%20Interaction%20Attention&body=Title%3A%20DFE-IANet%3A%20A%20Method%20for%20Polyp%20Image%20Classification%20Based%20on%20Dual-domain%0A%20%20Feature%20Extraction%20and%20Interaction%20Attention%0AAuthor%3A%20Wei%20Wang%20and%20Jixing%20He%20and%20Xin%20Wang%0AAbstract%3A%20%20%20It%20is%20helpful%20in%20preventing%20colorectal%20cancer%20to%20detect%20and%20treat%20polyps%20in%0Athe%20gastrointestinal%20tract%20early.%20However%2C%20there%20have%20been%20few%20studies%20to%20date%0Aon%20designing%20polyp%20image%20classification%20networks%20that%20balance%20efficiency%20and%0Aaccuracy.%20This%20challenge%20is%20mainly%20attributed%20to%20the%20fact%20that%20polyps%20are%0Asimilar%20to%20other%20pathologies%20and%20have%20complex%20features%20influenced%20by%20texture%2C%0Acolor%2C%20and%20morphology.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20network%20DFE-IANet%0Abased%20on%20both%20spectral%20transformation%20and%20feature%20interaction.%20Firstly%2C%20to%0Aextract%20detailed%20features%20and%20multi-scale%20features%2C%20the%20features%20are%0Atransformed%20by%20the%20multi-scale%20frequency%20domain%20feature%20extraction%20%28MSFD%29%20block%0Ato%20extract%20texture%20details%20at%20the%20fine-grained%20level%20in%20the%20frequency%20domain.%0ASecondly%2C%20the%20multi-scale%20interaction%20attention%20%28MSIA%29%20block%20is%20designed%20to%0Aenhance%20the%20network%27s%20capability%20of%20extracting%20critical%20features.%20This%20block%0Aintroduces%20multi-scale%20features%20into%20self-attention%2C%20aiming%20to%20adaptively%20guide%0Athe%20network%20to%20concentrate%20on%20vital%20regions.%20Finally%2C%20with%20a%20compact%20parameter%0Aof%20only%204M%2C%20DFE-IANet%20outperforms%20the%20latest%20and%20classical%20networks%20in%20terms%20of%0Aefficiency.%20Furthermore%2C%20DFE-IANet%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%0Athe%20challenging%20Kvasir%20dataset%2C%20demonstrating%20a%20remarkable%20Top-1%20accuracy%20of%0A93.94%25.%20This%20outstanding%20accuracy%20surpasses%20ViT%20by%208.94%25%2C%20ResNet50%20by%201.69%25%2C%0Aand%20VMamba%20by%201.88%25.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/PURSUETHESUN/DFE-IANet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFE-IANet%253A%2520A%2520Method%2520for%2520Polyp%2520Image%2520Classification%2520Based%2520on%2520Dual-domain%250A%2520%2520Feature%2520Extraction%2520and%2520Interaction%2520Attention%26entry.906535625%3DWei%2520Wang%2520and%2520Jixing%2520He%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%2520It%2520is%2520helpful%2520in%2520preventing%2520colorectal%2520cancer%2520to%2520detect%2520and%2520treat%2520polyps%2520in%250Athe%2520gastrointestinal%2520tract%2520early.%2520However%252C%2520there%2520have%2520been%2520few%2520studies%2520to%2520date%250Aon%2520designing%2520polyp%2520image%2520classification%2520networks%2520that%2520balance%2520efficiency%2520and%250Aaccuracy.%2520This%2520challenge%2520is%2520mainly%2520attributed%2520to%2520the%2520fact%2520that%2520polyps%2520are%250Asimilar%2520to%2520other%2520pathologies%2520and%2520have%2520complex%2520features%2520influenced%2520by%2520texture%252C%250Acolor%252C%2520and%2520morphology.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520network%2520DFE-IANet%250Abased%2520on%2520both%2520spectral%2520transformation%2520and%2520feature%2520interaction.%2520Firstly%252C%2520to%250Aextract%2520detailed%2520features%2520and%2520multi-scale%2520features%252C%2520the%2520features%2520are%250Atransformed%2520by%2520the%2520multi-scale%2520frequency%2520domain%2520feature%2520extraction%2520%2528MSFD%2529%2520block%250Ato%2520extract%2520texture%2520details%2520at%2520the%2520fine-grained%2520level%2520in%2520the%2520frequency%2520domain.%250ASecondly%252C%2520the%2520multi-scale%2520interaction%2520attention%2520%2528MSIA%2529%2520block%2520is%2520designed%2520to%250Aenhance%2520the%2520network%2527s%2520capability%2520of%2520extracting%2520critical%2520features.%2520This%2520block%250Aintroduces%2520multi-scale%2520features%2520into%2520self-attention%252C%2520aiming%2520to%2520adaptively%2520guide%250Athe%2520network%2520to%2520concentrate%2520on%2520vital%2520regions.%2520Finally%252C%2520with%2520a%2520compact%2520parameter%250Aof%2520only%25204M%252C%2520DFE-IANet%2520outperforms%2520the%2520latest%2520and%2520classical%2520networks%2520in%2520terms%2520of%250Aefficiency.%2520Furthermore%252C%2520DFE-IANet%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520on%250Athe%2520challenging%2520Kvasir%2520dataset%252C%2520demonstrating%2520a%2520remarkable%2520Top-1%2520accuracy%2520of%250A93.94%2525.%2520This%2520outstanding%2520accuracy%2520surpasses%2520ViT%2520by%25208.94%2525%252C%2520ResNet50%2520by%25201.69%2525%252C%250Aand%2520VMamba%2520by%25201.88%2525.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/PURSUETHESUN/DFE-IANet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFE-IANet%3A%20A%20Method%20for%20Polyp%20Image%20Classification%20Based%20on%20Dual-domain%0A%20%20Feature%20Extraction%20and%20Interaction%20Attention&entry.906535625=Wei%20Wang%20and%20Jixing%20He%20and%20Xin%20Wang&entry.1292438233=%20%20It%20is%20helpful%20in%20preventing%20colorectal%20cancer%20to%20detect%20and%20treat%20polyps%20in%0Athe%20gastrointestinal%20tract%20early.%20However%2C%20there%20have%20been%20few%20studies%20to%20date%0Aon%20designing%20polyp%20image%20classification%20networks%20that%20balance%20efficiency%20and%0Aaccuracy.%20This%20challenge%20is%20mainly%20attributed%20to%20the%20fact%20that%20polyps%20are%0Asimilar%20to%20other%20pathologies%20and%20have%20complex%20features%20influenced%20by%20texture%2C%0Acolor%2C%20and%20morphology.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20network%20DFE-IANet%0Abased%20on%20both%20spectral%20transformation%20and%20feature%20interaction.%20Firstly%2C%20to%0Aextract%20detailed%20features%20and%20multi-scale%20features%2C%20the%20features%20are%0Atransformed%20by%20the%20multi-scale%20frequency%20domain%20feature%20extraction%20%28MSFD%29%20block%0Ato%20extract%20texture%20details%20at%20the%20fine-grained%20level%20in%20the%20frequency%20domain.%0ASecondly%2C%20the%20multi-scale%20interaction%20attention%20%28MSIA%29%20block%20is%20designed%20to%0Aenhance%20the%20network%27s%20capability%20of%20extracting%20critical%20features.%20This%20block%0Aintroduces%20multi-scale%20features%20into%20self-attention%2C%20aiming%20to%20adaptively%20guide%0Athe%20network%20to%20concentrate%20on%20vital%20regions.%20Finally%2C%20with%20a%20compact%20parameter%0Aof%20only%204M%2C%20DFE-IANet%20outperforms%20the%20latest%20and%20classical%20networks%20in%20terms%20of%0Aefficiency.%20Furthermore%2C%20DFE-IANet%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%0Athe%20challenging%20Kvasir%20dataset%2C%20demonstrating%20a%20remarkable%20Top-1%20accuracy%20of%0A93.94%25.%20This%20outstanding%20accuracy%20surpasses%20ViT%20by%208.94%25%2C%20ResNet50%20by%201.69%25%2C%0Aand%20VMamba%20by%201.88%25.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/PURSUETHESUN/DFE-IANet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20843v1&entry.124074799=Read"},
{"title": "How Novice Programmers Use and Experience ChatGPT when Solving\n  Programming Exercises in an Introductory Course", "author": "Andreas Scholl and Natalie Kiesler", "abstract": "  This research paper contributes to the computing education research\ncommunity's understanding of Generative AI (GenAI) in the context of\nintroductory programming, and specifically, how students utilize related tools,\nsuch as ChatGPT. An increased understanding of students' use is mandatory for\neducators and higher education institutions, as GenAI is here to stay, and its\nperformance is likely to improve rapidly in the near future. Learning about\nstudents' use patterns is not only crucial to support their learning, but to\ndevelop adequate forms of instruction and assessment. With the rapid\nadvancement of AI, its broad availability, and ubiquitous presence in\neducational environments, elaborating how AI can enhance learning experiences,\nespecially in courses such as introductory programming is important. To date,\nmost studies have focused on the educator's perspective on GenAI, its\nperformance, characteristics, and limitations. However, the student\nperspective, and how they actually use GenAI tools in course contexts, has not\nbeen subject to a great number of studies. Therefore, this study is guided by\nthe following research questions: (1) What do students report on their use\npattern of ChatGPT in the context of introductory programming exercises? and\n(2) How do students perceive ChatGPT in the context of introductory programming\nexercises? To address these questions, computing students at a large German\nuniversity were asked to solve programming tasks with the assistance of ChatGPT\nas part of their introductory programming course. Students (n=298) provided\ninformation regarding the use of ChatGPT, and their evaluation of the tool via\nan online survey. This research provides a comprehensive evaluation of\nChatGPT-3.5's application by novice programmers in a higher education\ncontext...\n", "link": "http://arxiv.org/abs/2407.20792v1", "date": "2024-07-30", "relevancy": 2.0662, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5281}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5222}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Novice%20Programmers%20Use%20and%20Experience%20ChatGPT%20when%20Solving%0A%20%20Programming%20Exercises%20in%20an%20Introductory%20Course&body=Title%3A%20How%20Novice%20Programmers%20Use%20and%20Experience%20ChatGPT%20when%20Solving%0A%20%20Programming%20Exercises%20in%20an%20Introductory%20Course%0AAuthor%3A%20Andreas%20Scholl%20and%20Natalie%20Kiesler%0AAbstract%3A%20%20%20This%20research%20paper%20contributes%20to%20the%20computing%20education%20research%0Acommunity%27s%20understanding%20of%20Generative%20AI%20%28GenAI%29%20in%20the%20context%20of%0Aintroductory%20programming%2C%20and%20specifically%2C%20how%20students%20utilize%20related%20tools%2C%0Asuch%20as%20ChatGPT.%20An%20increased%20understanding%20of%20students%27%20use%20is%20mandatory%20for%0Aeducators%20and%20higher%20education%20institutions%2C%20as%20GenAI%20is%20here%20to%20stay%2C%20and%20its%0Aperformance%20is%20likely%20to%20improve%20rapidly%20in%20the%20near%20future.%20Learning%20about%0Astudents%27%20use%20patterns%20is%20not%20only%20crucial%20to%20support%20their%20learning%2C%20but%20to%0Adevelop%20adequate%20forms%20of%20instruction%20and%20assessment.%20With%20the%20rapid%0Aadvancement%20of%20AI%2C%20its%20broad%20availability%2C%20and%20ubiquitous%20presence%20in%0Aeducational%20environments%2C%20elaborating%20how%20AI%20can%20enhance%20learning%20experiences%2C%0Aespecially%20in%20courses%20such%20as%20introductory%20programming%20is%20important.%20To%20date%2C%0Amost%20studies%20have%20focused%20on%20the%20educator%27s%20perspective%20on%20GenAI%2C%20its%0Aperformance%2C%20characteristics%2C%20and%20limitations.%20However%2C%20the%20student%0Aperspective%2C%20and%20how%20they%20actually%20use%20GenAI%20tools%20in%20course%20contexts%2C%20has%20not%0Abeen%20subject%20to%20a%20great%20number%20of%20studies.%20Therefore%2C%20this%20study%20is%20guided%20by%0Athe%20following%20research%20questions%3A%20%281%29%20What%20do%20students%20report%20on%20their%20use%0Apattern%20of%20ChatGPT%20in%20the%20context%20of%20introductory%20programming%20exercises%3F%20and%0A%282%29%20How%20do%20students%20perceive%20ChatGPT%20in%20the%20context%20of%20introductory%20programming%0Aexercises%3F%20To%20address%20these%20questions%2C%20computing%20students%20at%20a%20large%20German%0Auniversity%20were%20asked%20to%20solve%20programming%20tasks%20with%20the%20assistance%20of%20ChatGPT%0Aas%20part%20of%20their%20introductory%20programming%20course.%20Students%20%28n%3D298%29%20provided%0Ainformation%20regarding%20the%20use%20of%20ChatGPT%2C%20and%20their%20evaluation%20of%20the%20tool%20via%0Aan%20online%20survey.%20This%20research%20provides%20a%20comprehensive%20evaluation%20of%0AChatGPT-3.5%27s%20application%20by%20novice%20programmers%20in%20a%20higher%20education%0Acontext...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Novice%2520Programmers%2520Use%2520and%2520Experience%2520ChatGPT%2520when%2520Solving%250A%2520%2520Programming%2520Exercises%2520in%2520an%2520Introductory%2520Course%26entry.906535625%3DAndreas%2520Scholl%2520and%2520Natalie%2520Kiesler%26entry.1292438233%3D%2520%2520This%2520research%2520paper%2520contributes%2520to%2520the%2520computing%2520education%2520research%250Acommunity%2527s%2520understanding%2520of%2520Generative%2520AI%2520%2528GenAI%2529%2520in%2520the%2520context%2520of%250Aintroductory%2520programming%252C%2520and%2520specifically%252C%2520how%2520students%2520utilize%2520related%2520tools%252C%250Asuch%2520as%2520ChatGPT.%2520An%2520increased%2520understanding%2520of%2520students%2527%2520use%2520is%2520mandatory%2520for%250Aeducators%2520and%2520higher%2520education%2520institutions%252C%2520as%2520GenAI%2520is%2520here%2520to%2520stay%252C%2520and%2520its%250Aperformance%2520is%2520likely%2520to%2520improve%2520rapidly%2520in%2520the%2520near%2520future.%2520Learning%2520about%250Astudents%2527%2520use%2520patterns%2520is%2520not%2520only%2520crucial%2520to%2520support%2520their%2520learning%252C%2520but%2520to%250Adevelop%2520adequate%2520forms%2520of%2520instruction%2520and%2520assessment.%2520With%2520the%2520rapid%250Aadvancement%2520of%2520AI%252C%2520its%2520broad%2520availability%252C%2520and%2520ubiquitous%2520presence%2520in%250Aeducational%2520environments%252C%2520elaborating%2520how%2520AI%2520can%2520enhance%2520learning%2520experiences%252C%250Aespecially%2520in%2520courses%2520such%2520as%2520introductory%2520programming%2520is%2520important.%2520To%2520date%252C%250Amost%2520studies%2520have%2520focused%2520on%2520the%2520educator%2527s%2520perspective%2520on%2520GenAI%252C%2520its%250Aperformance%252C%2520characteristics%252C%2520and%2520limitations.%2520However%252C%2520the%2520student%250Aperspective%252C%2520and%2520how%2520they%2520actually%2520use%2520GenAI%2520tools%2520in%2520course%2520contexts%252C%2520has%2520not%250Abeen%2520subject%2520to%2520a%2520great%2520number%2520of%2520studies.%2520Therefore%252C%2520this%2520study%2520is%2520guided%2520by%250Athe%2520following%2520research%2520questions%253A%2520%25281%2529%2520What%2520do%2520students%2520report%2520on%2520their%2520use%250Apattern%2520of%2520ChatGPT%2520in%2520the%2520context%2520of%2520introductory%2520programming%2520exercises%253F%2520and%250A%25282%2529%2520How%2520do%2520students%2520perceive%2520ChatGPT%2520in%2520the%2520context%2520of%2520introductory%2520programming%250Aexercises%253F%2520To%2520address%2520these%2520questions%252C%2520computing%2520students%2520at%2520a%2520large%2520German%250Auniversity%2520were%2520asked%2520to%2520solve%2520programming%2520tasks%2520with%2520the%2520assistance%2520of%2520ChatGPT%250Aas%2520part%2520of%2520their%2520introductory%2520programming%2520course.%2520Students%2520%2528n%253D298%2529%2520provided%250Ainformation%2520regarding%2520the%2520use%2520of%2520ChatGPT%252C%2520and%2520their%2520evaluation%2520of%2520the%2520tool%2520via%250Aan%2520online%2520survey.%2520This%2520research%2520provides%2520a%2520comprehensive%2520evaluation%2520of%250AChatGPT-3.5%2527s%2520application%2520by%2520novice%2520programmers%2520in%2520a%2520higher%2520education%250Acontext...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Novice%20Programmers%20Use%20and%20Experience%20ChatGPT%20when%20Solving%0A%20%20Programming%20Exercises%20in%20an%20Introductory%20Course&entry.906535625=Andreas%20Scholl%20and%20Natalie%20Kiesler&entry.1292438233=%20%20This%20research%20paper%20contributes%20to%20the%20computing%20education%20research%0Acommunity%27s%20understanding%20of%20Generative%20AI%20%28GenAI%29%20in%20the%20context%20of%0Aintroductory%20programming%2C%20and%20specifically%2C%20how%20students%20utilize%20related%20tools%2C%0Asuch%20as%20ChatGPT.%20An%20increased%20understanding%20of%20students%27%20use%20is%20mandatory%20for%0Aeducators%20and%20higher%20education%20institutions%2C%20as%20GenAI%20is%20here%20to%20stay%2C%20and%20its%0Aperformance%20is%20likely%20to%20improve%20rapidly%20in%20the%20near%20future.%20Learning%20about%0Astudents%27%20use%20patterns%20is%20not%20only%20crucial%20to%20support%20their%20learning%2C%20but%20to%0Adevelop%20adequate%20forms%20of%20instruction%20and%20assessment.%20With%20the%20rapid%0Aadvancement%20of%20AI%2C%20its%20broad%20availability%2C%20and%20ubiquitous%20presence%20in%0Aeducational%20environments%2C%20elaborating%20how%20AI%20can%20enhance%20learning%20experiences%2C%0Aespecially%20in%20courses%20such%20as%20introductory%20programming%20is%20important.%20To%20date%2C%0Amost%20studies%20have%20focused%20on%20the%20educator%27s%20perspective%20on%20GenAI%2C%20its%0Aperformance%2C%20characteristics%2C%20and%20limitations.%20However%2C%20the%20student%0Aperspective%2C%20and%20how%20they%20actually%20use%20GenAI%20tools%20in%20course%20contexts%2C%20has%20not%0Abeen%20subject%20to%20a%20great%20number%20of%20studies.%20Therefore%2C%20this%20study%20is%20guided%20by%0Athe%20following%20research%20questions%3A%20%281%29%20What%20do%20students%20report%20on%20their%20use%0Apattern%20of%20ChatGPT%20in%20the%20context%20of%20introductory%20programming%20exercises%3F%20and%0A%282%29%20How%20do%20students%20perceive%20ChatGPT%20in%20the%20context%20of%20introductory%20programming%0Aexercises%3F%20To%20address%20these%20questions%2C%20computing%20students%20at%20a%20large%20German%0Auniversity%20were%20asked%20to%20solve%20programming%20tasks%20with%20the%20assistance%20of%20ChatGPT%0Aas%20part%20of%20their%20introductory%20programming%20course.%20Students%20%28n%3D298%29%20provided%0Ainformation%20regarding%20the%20use%20of%20ChatGPT%2C%20and%20their%20evaluation%20of%20the%20tool%20via%0Aan%20online%20survey.%20This%20research%20provides%20a%20comprehensive%20evaluation%20of%0AChatGPT-3.5%27s%20application%20by%20novice%20programmers%20in%20a%20higher%20education%0Acontext...%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20792v1&entry.124074799=Read"},
{"title": "Assessing Graphical Perception of Image Embedding Models using Channel\n  Effectiveness", "author": "Soohyun Lee and Minsuk Chang and Seokhyeon Park and Jinwook Seo", "abstract": "  Recent advancements in vision models have greatly improved their ability to\nhandle complex chart understanding tasks, like chart captioning and question\nanswering. However, it remains challenging to assess how these models process\ncharts. Existing benchmarks only roughly evaluate model performance without\nevaluating the underlying mechanisms, such as how models extract image\nembeddings. This limits our understanding of the model's ability to perceive\nfundamental graphical components. To address this, we introduce a novel\nevaluation framework to assess the graphical perception of image embedding\nmodels. For chart comprehension, we examine two main aspects of channel\neffectiveness: accuracy and discriminability of various visual channels.\nChannel accuracy is assessed through the linearity of embeddings, measuring how\nwell the perceived magnitude aligns with the size of the stimulus.\nDiscriminability is evaluated based on the distances between embeddings,\nindicating their distinctness. Our experiments with the CLIP model show that it\nperceives channel accuracy differently from humans and shows unique\ndiscriminability in channels like length, tilt, and curvature. We aim to\ndevelop this work into a broader benchmark for reliable visual encoders,\nenhancing models for precise chart comprehension and human-like perception in\nfuture applications.\n", "link": "http://arxiv.org/abs/2407.20845v1", "date": "2024-07-30", "relevancy": 2.0656, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5098}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Graphical%20Perception%20of%20Image%20Embedding%20Models%20using%20Channel%0A%20%20Effectiveness&body=Title%3A%20Assessing%20Graphical%20Perception%20of%20Image%20Embedding%20Models%20using%20Channel%0A%20%20Effectiveness%0AAuthor%3A%20Soohyun%20Lee%20and%20Minsuk%20Chang%20and%20Seokhyeon%20Park%20and%20Jinwook%20Seo%0AAbstract%3A%20%20%20Recent%20advancements%20in%20vision%20models%20have%20greatly%20improved%20their%20ability%20to%0Ahandle%20complex%20chart%20understanding%20tasks%2C%20like%20chart%20captioning%20and%20question%0Aanswering.%20However%2C%20it%20remains%20challenging%20to%20assess%20how%20these%20models%20process%0Acharts.%20Existing%20benchmarks%20only%20roughly%20evaluate%20model%20performance%20without%0Aevaluating%20the%20underlying%20mechanisms%2C%20such%20as%20how%20models%20extract%20image%0Aembeddings.%20This%20limits%20our%20understanding%20of%20the%20model%27s%20ability%20to%20perceive%0Afundamental%20graphical%20components.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Aevaluation%20framework%20to%20assess%20the%20graphical%20perception%20of%20image%20embedding%0Amodels.%20For%20chart%20comprehension%2C%20we%20examine%20two%20main%20aspects%20of%20channel%0Aeffectiveness%3A%20accuracy%20and%20discriminability%20of%20various%20visual%20channels.%0AChannel%20accuracy%20is%20assessed%20through%20the%20linearity%20of%20embeddings%2C%20measuring%20how%0Awell%20the%20perceived%20magnitude%20aligns%20with%20the%20size%20of%20the%20stimulus.%0ADiscriminability%20is%20evaluated%20based%20on%20the%20distances%20between%20embeddings%2C%0Aindicating%20their%20distinctness.%20Our%20experiments%20with%20the%20CLIP%20model%20show%20that%20it%0Aperceives%20channel%20accuracy%20differently%20from%20humans%20and%20shows%20unique%0Adiscriminability%20in%20channels%20like%20length%2C%20tilt%2C%20and%20curvature.%20We%20aim%20to%0Adevelop%20this%20work%20into%20a%20broader%20benchmark%20for%20reliable%20visual%20encoders%2C%0Aenhancing%20models%20for%20precise%20chart%20comprehension%20and%20human-like%20perception%20in%0Afuture%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Graphical%2520Perception%2520of%2520Image%2520Embedding%2520Models%2520using%2520Channel%250A%2520%2520Effectiveness%26entry.906535625%3DSoohyun%2520Lee%2520and%2520Minsuk%2520Chang%2520and%2520Seokhyeon%2520Park%2520and%2520Jinwook%2520Seo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520vision%2520models%2520have%2520greatly%2520improved%2520their%2520ability%2520to%250Ahandle%2520complex%2520chart%2520understanding%2520tasks%252C%2520like%2520chart%2520captioning%2520and%2520question%250Aanswering.%2520However%252C%2520it%2520remains%2520challenging%2520to%2520assess%2520how%2520these%2520models%2520process%250Acharts.%2520Existing%2520benchmarks%2520only%2520roughly%2520evaluate%2520model%2520performance%2520without%250Aevaluating%2520the%2520underlying%2520mechanisms%252C%2520such%2520as%2520how%2520models%2520extract%2520image%250Aembeddings.%2520This%2520limits%2520our%2520understanding%2520of%2520the%2520model%2527s%2520ability%2520to%2520perceive%250Afundamental%2520graphical%2520components.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%250Aevaluation%2520framework%2520to%2520assess%2520the%2520graphical%2520perception%2520of%2520image%2520embedding%250Amodels.%2520For%2520chart%2520comprehension%252C%2520we%2520examine%2520two%2520main%2520aspects%2520of%2520channel%250Aeffectiveness%253A%2520accuracy%2520and%2520discriminability%2520of%2520various%2520visual%2520channels.%250AChannel%2520accuracy%2520is%2520assessed%2520through%2520the%2520linearity%2520of%2520embeddings%252C%2520measuring%2520how%250Awell%2520the%2520perceived%2520magnitude%2520aligns%2520with%2520the%2520size%2520of%2520the%2520stimulus.%250ADiscriminability%2520is%2520evaluated%2520based%2520on%2520the%2520distances%2520between%2520embeddings%252C%250Aindicating%2520their%2520distinctness.%2520Our%2520experiments%2520with%2520the%2520CLIP%2520model%2520show%2520that%2520it%250Aperceives%2520channel%2520accuracy%2520differently%2520from%2520humans%2520and%2520shows%2520unique%250Adiscriminability%2520in%2520channels%2520like%2520length%252C%2520tilt%252C%2520and%2520curvature.%2520We%2520aim%2520to%250Adevelop%2520this%2520work%2520into%2520a%2520broader%2520benchmark%2520for%2520reliable%2520visual%2520encoders%252C%250Aenhancing%2520models%2520for%2520precise%2520chart%2520comprehension%2520and%2520human-like%2520perception%2520in%250Afuture%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Graphical%20Perception%20of%20Image%20Embedding%20Models%20using%20Channel%0A%20%20Effectiveness&entry.906535625=Soohyun%20Lee%20and%20Minsuk%20Chang%20and%20Seokhyeon%20Park%20and%20Jinwook%20Seo&entry.1292438233=%20%20Recent%20advancements%20in%20vision%20models%20have%20greatly%20improved%20their%20ability%20to%0Ahandle%20complex%20chart%20understanding%20tasks%2C%20like%20chart%20captioning%20and%20question%0Aanswering.%20However%2C%20it%20remains%20challenging%20to%20assess%20how%20these%20models%20process%0Acharts.%20Existing%20benchmarks%20only%20roughly%20evaluate%20model%20performance%20without%0Aevaluating%20the%20underlying%20mechanisms%2C%20such%20as%20how%20models%20extract%20image%0Aembeddings.%20This%20limits%20our%20understanding%20of%20the%20model%27s%20ability%20to%20perceive%0Afundamental%20graphical%20components.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Aevaluation%20framework%20to%20assess%20the%20graphical%20perception%20of%20image%20embedding%0Amodels.%20For%20chart%20comprehension%2C%20we%20examine%20two%20main%20aspects%20of%20channel%0Aeffectiveness%3A%20accuracy%20and%20discriminability%20of%20various%20visual%20channels.%0AChannel%20accuracy%20is%20assessed%20through%20the%20linearity%20of%20embeddings%2C%20measuring%20how%0Awell%20the%20perceived%20magnitude%20aligns%20with%20the%20size%20of%20the%20stimulus.%0ADiscriminability%20is%20evaluated%20based%20on%20the%20distances%20between%20embeddings%2C%0Aindicating%20their%20distinctness.%20Our%20experiments%20with%20the%20CLIP%20model%20show%20that%20it%0Aperceives%20channel%20accuracy%20differently%20from%20humans%20and%20shows%20unique%0Adiscriminability%20in%20channels%20like%20length%2C%20tilt%2C%20and%20curvature.%20We%20aim%20to%0Adevelop%20this%20work%20into%20a%20broader%20benchmark%20for%20reliable%20visual%20encoders%2C%0Aenhancing%20models%20for%20precise%20chart%20comprehension%20and%20human-like%20perception%20in%0Afuture%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20845v1&entry.124074799=Read"},
{"title": "dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from\n  Multiple Humans", "author": "Marek Herde and Denis Huseljic and Lukas Rauch and Bernhard Sick", "abstract": "  Human annotators typically provide annotated data for training machine\nlearning models, such as neural networks. Yet, human annotations are subject to\nnoise, impairing generalization performances. Methodological research on\napproaches counteracting noisy annotations requires corresponding datasets for\na meaningful empirical evaluation. Consequently, we introduce a novel benchmark\ndataset, dopanim, consisting of about 15,750 animal images of 15 classes with\nground truth labels. For approximately 10,500 of these images, 20 humans\nprovided over 52,000 annotations with an accuracy of circa 67%. Its key\nattributes include (1) the challenging task of classifying doppelganger\nanimals, (2) human-estimated likelihoods as annotations, and (3) annotator\nmetadata. We benchmark well-known multi-annotator learning approaches using\nseven variants of this dataset and outline further evaluation use cases such as\nlearning beyond hard class labels and active learning. Our dataset and a\ncomprehensive codebase are publicly available to emulate the data collection\nprocess and to reproduce all empirical results.\n", "link": "http://arxiv.org/abs/2407.20950v1", "date": "2024-07-30", "relevancy": 2.0644, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5527}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5156}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20dopanim%3A%20A%20Dataset%20of%20Doppelganger%20Animals%20with%20Noisy%20Annotations%20from%0A%20%20Multiple%20Humans&body=Title%3A%20dopanim%3A%20A%20Dataset%20of%20Doppelganger%20Animals%20with%20Noisy%20Annotations%20from%0A%20%20Multiple%20Humans%0AAuthor%3A%20Marek%20Herde%20and%20Denis%20Huseljic%20and%20Lukas%20Rauch%20and%20Bernhard%20Sick%0AAbstract%3A%20%20%20Human%20annotators%20typically%20provide%20annotated%20data%20for%20training%20machine%0Alearning%20models%2C%20such%20as%20neural%20networks.%20Yet%2C%20human%20annotations%20are%20subject%20to%0Anoise%2C%20impairing%20generalization%20performances.%20Methodological%20research%20on%0Aapproaches%20counteracting%20noisy%20annotations%20requires%20corresponding%20datasets%20for%0Aa%20meaningful%20empirical%20evaluation.%20Consequently%2C%20we%20introduce%20a%20novel%20benchmark%0Adataset%2C%20dopanim%2C%20consisting%20of%20about%2015%2C750%20animal%20images%20of%2015%20classes%20with%0Aground%20truth%20labels.%20For%20approximately%2010%2C500%20of%20these%20images%2C%2020%20humans%0Aprovided%20over%2052%2C000%20annotations%20with%20an%20accuracy%20of%20circa%2067%25.%20Its%20key%0Aattributes%20include%20%281%29%20the%20challenging%20task%20of%20classifying%20doppelganger%0Aanimals%2C%20%282%29%20human-estimated%20likelihoods%20as%20annotations%2C%20and%20%283%29%20annotator%0Ametadata.%20We%20benchmark%20well-known%20multi-annotator%20learning%20approaches%20using%0Aseven%20variants%20of%20this%20dataset%20and%20outline%20further%20evaluation%20use%20cases%20such%20as%0Alearning%20beyond%20hard%20class%20labels%20and%20active%20learning.%20Our%20dataset%20and%20a%0Acomprehensive%20codebase%20are%20publicly%20available%20to%20emulate%20the%20data%20collection%0Aprocess%20and%20to%20reproduce%20all%20empirical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Ddopanim%253A%2520A%2520Dataset%2520of%2520Doppelganger%2520Animals%2520with%2520Noisy%2520Annotations%2520from%250A%2520%2520Multiple%2520Humans%26entry.906535625%3DMarek%2520Herde%2520and%2520Denis%2520Huseljic%2520and%2520Lukas%2520Rauch%2520and%2520Bernhard%2520Sick%26entry.1292438233%3D%2520%2520Human%2520annotators%2520typically%2520provide%2520annotated%2520data%2520for%2520training%2520machine%250Alearning%2520models%252C%2520such%2520as%2520neural%2520networks.%2520Yet%252C%2520human%2520annotations%2520are%2520subject%2520to%250Anoise%252C%2520impairing%2520generalization%2520performances.%2520Methodological%2520research%2520on%250Aapproaches%2520counteracting%2520noisy%2520annotations%2520requires%2520corresponding%2520datasets%2520for%250Aa%2520meaningful%2520empirical%2520evaluation.%2520Consequently%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%250Adataset%252C%2520dopanim%252C%2520consisting%2520of%2520about%252015%252C750%2520animal%2520images%2520of%252015%2520classes%2520with%250Aground%2520truth%2520labels.%2520For%2520approximately%252010%252C500%2520of%2520these%2520images%252C%252020%2520humans%250Aprovided%2520over%252052%252C000%2520annotations%2520with%2520an%2520accuracy%2520of%2520circa%252067%2525.%2520Its%2520key%250Aattributes%2520include%2520%25281%2529%2520the%2520challenging%2520task%2520of%2520classifying%2520doppelganger%250Aanimals%252C%2520%25282%2529%2520human-estimated%2520likelihoods%2520as%2520annotations%252C%2520and%2520%25283%2529%2520annotator%250Ametadata.%2520We%2520benchmark%2520well-known%2520multi-annotator%2520learning%2520approaches%2520using%250Aseven%2520variants%2520of%2520this%2520dataset%2520and%2520outline%2520further%2520evaluation%2520use%2520cases%2520such%2520as%250Alearning%2520beyond%2520hard%2520class%2520labels%2520and%2520active%2520learning.%2520Our%2520dataset%2520and%2520a%250Acomprehensive%2520codebase%2520are%2520publicly%2520available%2520to%2520emulate%2520the%2520data%2520collection%250Aprocess%2520and%2520to%2520reproduce%2520all%2520empirical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=dopanim%3A%20A%20Dataset%20of%20Doppelganger%20Animals%20with%20Noisy%20Annotations%20from%0A%20%20Multiple%20Humans&entry.906535625=Marek%20Herde%20and%20Denis%20Huseljic%20and%20Lukas%20Rauch%20and%20Bernhard%20Sick&entry.1292438233=%20%20Human%20annotators%20typically%20provide%20annotated%20data%20for%20training%20machine%0Alearning%20models%2C%20such%20as%20neural%20networks.%20Yet%2C%20human%20annotations%20are%20subject%20to%0Anoise%2C%20impairing%20generalization%20performances.%20Methodological%20research%20on%0Aapproaches%20counteracting%20noisy%20annotations%20requires%20corresponding%20datasets%20for%0Aa%20meaningful%20empirical%20evaluation.%20Consequently%2C%20we%20introduce%20a%20novel%20benchmark%0Adataset%2C%20dopanim%2C%20consisting%20of%20about%2015%2C750%20animal%20images%20of%2015%20classes%20with%0Aground%20truth%20labels.%20For%20approximately%2010%2C500%20of%20these%20images%2C%2020%20humans%0Aprovided%20over%2052%2C000%20annotations%20with%20an%20accuracy%20of%20circa%2067%25.%20Its%20key%0Aattributes%20include%20%281%29%20the%20challenging%20task%20of%20classifying%20doppelganger%0Aanimals%2C%20%282%29%20human-estimated%20likelihoods%20as%20annotations%2C%20and%20%283%29%20annotator%0Ametadata.%20We%20benchmark%20well-known%20multi-annotator%20learning%20approaches%20using%0Aseven%20variants%20of%20this%20dataset%20and%20outline%20further%20evaluation%20use%20cases%20such%20as%0Alearning%20beyond%20hard%20class%20labels%20and%20active%20learning.%20Our%20dataset%20and%20a%0Acomprehensive%20codebase%20are%20publicly%20available%20to%20emulate%20the%20data%20collection%0Aprocess%20and%20to%20reproduce%20all%20empirical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20950v1&entry.124074799=Read"},
{"title": "Forecasting Tropical Cyclones with Cascaded Diffusion Models", "author": "Pritthijit Nath and Pancham Shukla and Shuai Wang and C\u00e9sar Quilodr\u00e1n-Casas", "abstract": "  As tropical cyclones become more intense due to climate change, the rise of\nAl-based modelling provides a more affordable and accessible approach compared\nto traditional methods based on mathematical models. This work leverages\ngenerative diffusion models to forecast cyclone trajectories and precipitation\npatterns by integrating satellite imaging, remote sensing, and atmospheric\ndata. It employs a cascaded approach that incorporates three main tasks:\nforecasting, super-resolution, and precipitation modelling. The training\ndataset includes 51 cyclones from six major tropical cyclone basins from\nJanuary 2019 - March 2023. Experiments demonstrate that the final forecasts\nfrom the cascaded models show accurate predictions up to a 36-hour rollout,\nwith excellent Structural Similarity (SSIM) and Peak-Singal-To-Noise Ratio\n(PSNR) values exceeding 0.5 and 20 dB, respectively, for all three tasks. The\n36-hour forecasts can be produced in as little as 30 mins on a single Nvidia\nA30/RTX 2080 Ti. This work also highlights the promising efficiency of Al\nmethods such as diffusion models for high-performance needs in weather\nforecasting, such as tropical cyclone forecasting, while remaining\ncomputationally affordable, making them ideal for highly vulnerable regions\nwith critical forecasting needs and financial limitations. Code accessible at\nhttps://github.com/nathzi1505/forecast-diffmodels.\n", "link": "http://arxiv.org/abs/2310.01690v7", "date": "2024-07-30", "relevancy": 2.0572, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5417}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20Tropical%20Cyclones%20with%20Cascaded%20Diffusion%20Models&body=Title%3A%20Forecasting%20Tropical%20Cyclones%20with%20Cascaded%20Diffusion%20Models%0AAuthor%3A%20Pritthijit%20Nath%20and%20Pancham%20Shukla%20and%20Shuai%20Wang%20and%20C%C3%A9sar%20Quilodr%C3%A1n-Casas%0AAbstract%3A%20%20%20As%20tropical%20cyclones%20become%20more%20intense%20due%20to%20climate%20change%2C%20the%20rise%20of%0AAl-based%20modelling%20provides%20a%20more%20affordable%20and%20accessible%20approach%20compared%0Ato%20traditional%20methods%20based%20on%20mathematical%20models.%20This%20work%20leverages%0Agenerative%20diffusion%20models%20to%20forecast%20cyclone%20trajectories%20and%20precipitation%0Apatterns%20by%20integrating%20satellite%20imaging%2C%20remote%20sensing%2C%20and%20atmospheric%0Adata.%20It%20employs%20a%20cascaded%20approach%20that%20incorporates%20three%20main%20tasks%3A%0Aforecasting%2C%20super-resolution%2C%20and%20precipitation%20modelling.%20The%20training%0Adataset%20includes%2051%20cyclones%20from%20six%20major%20tropical%20cyclone%20basins%20from%0AJanuary%202019%20-%20March%202023.%20Experiments%20demonstrate%20that%20the%20final%20forecasts%0Afrom%20the%20cascaded%20models%20show%20accurate%20predictions%20up%20to%20a%2036-hour%20rollout%2C%0Awith%20excellent%20Structural%20Similarity%20%28SSIM%29%20and%20Peak-Singal-To-Noise%20Ratio%0A%28PSNR%29%20values%20exceeding%200.5%20and%2020%20dB%2C%20respectively%2C%20for%20all%20three%20tasks.%20The%0A36-hour%20forecasts%20can%20be%20produced%20in%20as%20little%20as%2030%20mins%20on%20a%20single%20Nvidia%0AA30/RTX%202080%20Ti.%20This%20work%20also%20highlights%20the%20promising%20efficiency%20of%20Al%0Amethods%20such%20as%20diffusion%20models%20for%20high-performance%20needs%20in%20weather%0Aforecasting%2C%20such%20as%20tropical%20cyclone%20forecasting%2C%20while%20remaining%0Acomputationally%20affordable%2C%20making%20them%20ideal%20for%20highly%20vulnerable%20regions%0Awith%20critical%20forecasting%20needs%20and%20financial%20limitations.%20Code%20accessible%20at%0Ahttps%3A//github.com/nathzi1505/forecast-diffmodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01690v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520Tropical%2520Cyclones%2520with%2520Cascaded%2520Diffusion%2520Models%26entry.906535625%3DPritthijit%2520Nath%2520and%2520Pancham%2520Shukla%2520and%2520Shuai%2520Wang%2520and%2520C%25C3%25A9sar%2520Quilodr%25C3%25A1n-Casas%26entry.1292438233%3D%2520%2520As%2520tropical%2520cyclones%2520become%2520more%2520intense%2520due%2520to%2520climate%2520change%252C%2520the%2520rise%2520of%250AAl-based%2520modelling%2520provides%2520a%2520more%2520affordable%2520and%2520accessible%2520approach%2520compared%250Ato%2520traditional%2520methods%2520based%2520on%2520mathematical%2520models.%2520This%2520work%2520leverages%250Agenerative%2520diffusion%2520models%2520to%2520forecast%2520cyclone%2520trajectories%2520and%2520precipitation%250Apatterns%2520by%2520integrating%2520satellite%2520imaging%252C%2520remote%2520sensing%252C%2520and%2520atmospheric%250Adata.%2520It%2520employs%2520a%2520cascaded%2520approach%2520that%2520incorporates%2520three%2520main%2520tasks%253A%250Aforecasting%252C%2520super-resolution%252C%2520and%2520precipitation%2520modelling.%2520The%2520training%250Adataset%2520includes%252051%2520cyclones%2520from%2520six%2520major%2520tropical%2520cyclone%2520basins%2520from%250AJanuary%25202019%2520-%2520March%25202023.%2520Experiments%2520demonstrate%2520that%2520the%2520final%2520forecasts%250Afrom%2520the%2520cascaded%2520models%2520show%2520accurate%2520predictions%2520up%2520to%2520a%252036-hour%2520rollout%252C%250Awith%2520excellent%2520Structural%2520Similarity%2520%2528SSIM%2529%2520and%2520Peak-Singal-To-Noise%2520Ratio%250A%2528PSNR%2529%2520values%2520exceeding%25200.5%2520and%252020%2520dB%252C%2520respectively%252C%2520for%2520all%2520three%2520tasks.%2520The%250A36-hour%2520forecasts%2520can%2520be%2520produced%2520in%2520as%2520little%2520as%252030%2520mins%2520on%2520a%2520single%2520Nvidia%250AA30/RTX%25202080%2520Ti.%2520This%2520work%2520also%2520highlights%2520the%2520promising%2520efficiency%2520of%2520Al%250Amethods%2520such%2520as%2520diffusion%2520models%2520for%2520high-performance%2520needs%2520in%2520weather%250Aforecasting%252C%2520such%2520as%2520tropical%2520cyclone%2520forecasting%252C%2520while%2520remaining%250Acomputationally%2520affordable%252C%2520making%2520them%2520ideal%2520for%2520highly%2520vulnerable%2520regions%250Awith%2520critical%2520forecasting%2520needs%2520and%2520financial%2520limitations.%2520Code%2520accessible%2520at%250Ahttps%253A//github.com/nathzi1505/forecast-diffmodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01690v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20Tropical%20Cyclones%20with%20Cascaded%20Diffusion%20Models&entry.906535625=Pritthijit%20Nath%20and%20Pancham%20Shukla%20and%20Shuai%20Wang%20and%20C%C3%A9sar%20Quilodr%C3%A1n-Casas&entry.1292438233=%20%20As%20tropical%20cyclones%20become%20more%20intense%20due%20to%20climate%20change%2C%20the%20rise%20of%0AAl-based%20modelling%20provides%20a%20more%20affordable%20and%20accessible%20approach%20compared%0Ato%20traditional%20methods%20based%20on%20mathematical%20models.%20This%20work%20leverages%0Agenerative%20diffusion%20models%20to%20forecast%20cyclone%20trajectories%20and%20precipitation%0Apatterns%20by%20integrating%20satellite%20imaging%2C%20remote%20sensing%2C%20and%20atmospheric%0Adata.%20It%20employs%20a%20cascaded%20approach%20that%20incorporates%20three%20main%20tasks%3A%0Aforecasting%2C%20super-resolution%2C%20and%20precipitation%20modelling.%20The%20training%0Adataset%20includes%2051%20cyclones%20from%20six%20major%20tropical%20cyclone%20basins%20from%0AJanuary%202019%20-%20March%202023.%20Experiments%20demonstrate%20that%20the%20final%20forecasts%0Afrom%20the%20cascaded%20models%20show%20accurate%20predictions%20up%20to%20a%2036-hour%20rollout%2C%0Awith%20excellent%20Structural%20Similarity%20%28SSIM%29%20and%20Peak-Singal-To-Noise%20Ratio%0A%28PSNR%29%20values%20exceeding%200.5%20and%2020%20dB%2C%20respectively%2C%20for%20all%20three%20tasks.%20The%0A36-hour%20forecasts%20can%20be%20produced%20in%20as%20little%20as%2030%20mins%20on%20a%20single%20Nvidia%0AA30/RTX%202080%20Ti.%20This%20work%20also%20highlights%20the%20promising%20efficiency%20of%20Al%0Amethods%20such%20as%20diffusion%20models%20for%20high-performance%20needs%20in%20weather%0Aforecasting%2C%20such%20as%20tropical%20cyclone%20forecasting%2C%20while%20remaining%0Acomputationally%20affordable%2C%20making%20them%20ideal%20for%20highly%20vulnerable%20regions%0Awith%20critical%20forecasting%20needs%20and%20financial%20limitations.%20Code%20accessible%20at%0Ahttps%3A//github.com/nathzi1505/forecast-diffmodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01690v7&entry.124074799=Read"},
{"title": "An Adaptive Graduated Nonconvexity Loss Function for Robust Nonlinear\n  Least Squares Solutions", "author": "Kyungmin Jung and Thomas Hitchcox and James Richard Forbes", "abstract": "  Many problems in robotics, such as estimating the state from noisy sensor\ndata or aligning two point clouds, can be posed and solved as least-squares\nproblems. Unfortunately, vanilla nonminimal solvers for least-squares problems\nare notoriously sensitive to outliers. As such, various robust loss functions\nhave been proposed to reduce the sensitivity to outliers. Examples of loss\nfunctions include pseudo-Huber, Cauchy, and Geman-McClure. Recently, these loss\nfunctions have been generalized into a single loss function that enables the\nbest loss function to be found adaptively based on the distribution of the\nresiduals. However, even with the generalized robust loss function, most\nnonminimal solvers can only be solved locally given a prior state estimate due\nto the nonconvexity of the problem. The first contribution of this paper is to\ncombine graduated nonconvexity (GNC) with the generalized robust loss function\nto solve least-squares problems without a prior state estimate and without the\nneed to specify a loss function. Moreover, existing loss functions, including\nthe generalized loss function, are based on Gaussian-like distribution.\nHowever, residuals are often defined as the squared norm of a multivariate\nerror and distributed in a Chi-like fashion. The second contribution of this\npaper is to apply a norm-aware adaptive robust loss function within a GNC\nframework. The proposed approach enables a GNC formulation of a generalized\nloss function such that GNC can be readily applied to a wider family of loss\nfunctions. Furthermore, simulations and experiments demonstrate that the\nproposed method is more robust compared to non-GNC counterparts, and yields\nfaster convergence times compared to other GNC formulations.\n", "link": "http://arxiv.org/abs/2305.06869v5", "date": "2024-07-30", "relevancy": 2.0563, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5367}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5239}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Graduated%20Nonconvexity%20Loss%20Function%20for%20Robust%20Nonlinear%0A%20%20Least%20Squares%20Solutions&body=Title%3A%20An%20Adaptive%20Graduated%20Nonconvexity%20Loss%20Function%20for%20Robust%20Nonlinear%0A%20%20Least%20Squares%20Solutions%0AAuthor%3A%20Kyungmin%20Jung%20and%20Thomas%20Hitchcox%20and%20James%20Richard%20Forbes%0AAbstract%3A%20%20%20Many%20problems%20in%20robotics%2C%20such%20as%20estimating%20the%20state%20from%20noisy%20sensor%0Adata%20or%20aligning%20two%20point%20clouds%2C%20can%20be%20posed%20and%20solved%20as%20least-squares%0Aproblems.%20Unfortunately%2C%20vanilla%20nonminimal%20solvers%20for%20least-squares%20problems%0Aare%20notoriously%20sensitive%20to%20outliers.%20As%20such%2C%20various%20robust%20loss%20functions%0Ahave%20been%20proposed%20to%20reduce%20the%20sensitivity%20to%20outliers.%20Examples%20of%20loss%0Afunctions%20include%20pseudo-Huber%2C%20Cauchy%2C%20and%20Geman-McClure.%20Recently%2C%20these%20loss%0Afunctions%20have%20been%20generalized%20into%20a%20single%20loss%20function%20that%20enables%20the%0Abest%20loss%20function%20to%20be%20found%20adaptively%20based%20on%20the%20distribution%20of%20the%0Aresiduals.%20However%2C%20even%20with%20the%20generalized%20robust%20loss%20function%2C%20most%0Anonminimal%20solvers%20can%20only%20be%20solved%20locally%20given%20a%20prior%20state%20estimate%20due%0Ato%20the%20nonconvexity%20of%20the%20problem.%20The%20first%20contribution%20of%20this%20paper%20is%20to%0Acombine%20graduated%20nonconvexity%20%28GNC%29%20with%20the%20generalized%20robust%20loss%20function%0Ato%20solve%20least-squares%20problems%20without%20a%20prior%20state%20estimate%20and%20without%20the%0Aneed%20to%20specify%20a%20loss%20function.%20Moreover%2C%20existing%20loss%20functions%2C%20including%0Athe%20generalized%20loss%20function%2C%20are%20based%20on%20Gaussian-like%20distribution.%0AHowever%2C%20residuals%20are%20often%20defined%20as%20the%20squared%20norm%20of%20a%20multivariate%0Aerror%20and%20distributed%20in%20a%20Chi-like%20fashion.%20The%20second%20contribution%20of%20this%0Apaper%20is%20to%20apply%20a%20norm-aware%20adaptive%20robust%20loss%20function%20within%20a%20GNC%0Aframework.%20The%20proposed%20approach%20enables%20a%20GNC%20formulation%20of%20a%20generalized%0Aloss%20function%20such%20that%20GNC%20can%20be%20readily%20applied%20to%20a%20wider%20family%20of%20loss%0Afunctions.%20Furthermore%2C%20simulations%20and%20experiments%20demonstrate%20that%20the%0Aproposed%20method%20is%20more%20robust%20compared%20to%20non-GNC%20counterparts%2C%20and%20yields%0Afaster%20convergence%20times%20compared%20to%20other%20GNC%20formulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.06869v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Graduated%2520Nonconvexity%2520Loss%2520Function%2520for%2520Robust%2520Nonlinear%250A%2520%2520Least%2520Squares%2520Solutions%26entry.906535625%3DKyungmin%2520Jung%2520and%2520Thomas%2520Hitchcox%2520and%2520James%2520Richard%2520Forbes%26entry.1292438233%3D%2520%2520Many%2520problems%2520in%2520robotics%252C%2520such%2520as%2520estimating%2520the%2520state%2520from%2520noisy%2520sensor%250Adata%2520or%2520aligning%2520two%2520point%2520clouds%252C%2520can%2520be%2520posed%2520and%2520solved%2520as%2520least-squares%250Aproblems.%2520Unfortunately%252C%2520vanilla%2520nonminimal%2520solvers%2520for%2520least-squares%2520problems%250Aare%2520notoriously%2520sensitive%2520to%2520outliers.%2520As%2520such%252C%2520various%2520robust%2520loss%2520functions%250Ahave%2520been%2520proposed%2520to%2520reduce%2520the%2520sensitivity%2520to%2520outliers.%2520Examples%2520of%2520loss%250Afunctions%2520include%2520pseudo-Huber%252C%2520Cauchy%252C%2520and%2520Geman-McClure.%2520Recently%252C%2520these%2520loss%250Afunctions%2520have%2520been%2520generalized%2520into%2520a%2520single%2520loss%2520function%2520that%2520enables%2520the%250Abest%2520loss%2520function%2520to%2520be%2520found%2520adaptively%2520based%2520on%2520the%2520distribution%2520of%2520the%250Aresiduals.%2520However%252C%2520even%2520with%2520the%2520generalized%2520robust%2520loss%2520function%252C%2520most%250Anonminimal%2520solvers%2520can%2520only%2520be%2520solved%2520locally%2520given%2520a%2520prior%2520state%2520estimate%2520due%250Ato%2520the%2520nonconvexity%2520of%2520the%2520problem.%2520The%2520first%2520contribution%2520of%2520this%2520paper%2520is%2520to%250Acombine%2520graduated%2520nonconvexity%2520%2528GNC%2529%2520with%2520the%2520generalized%2520robust%2520loss%2520function%250Ato%2520solve%2520least-squares%2520problems%2520without%2520a%2520prior%2520state%2520estimate%2520and%2520without%2520the%250Aneed%2520to%2520specify%2520a%2520loss%2520function.%2520Moreover%252C%2520existing%2520loss%2520functions%252C%2520including%250Athe%2520generalized%2520loss%2520function%252C%2520are%2520based%2520on%2520Gaussian-like%2520distribution.%250AHowever%252C%2520residuals%2520are%2520often%2520defined%2520as%2520the%2520squared%2520norm%2520of%2520a%2520multivariate%250Aerror%2520and%2520distributed%2520in%2520a%2520Chi-like%2520fashion.%2520The%2520second%2520contribution%2520of%2520this%250Apaper%2520is%2520to%2520apply%2520a%2520norm-aware%2520adaptive%2520robust%2520loss%2520function%2520within%2520a%2520GNC%250Aframework.%2520The%2520proposed%2520approach%2520enables%2520a%2520GNC%2520formulation%2520of%2520a%2520generalized%250Aloss%2520function%2520such%2520that%2520GNC%2520can%2520be%2520readily%2520applied%2520to%2520a%2520wider%2520family%2520of%2520loss%250Afunctions.%2520Furthermore%252C%2520simulations%2520and%2520experiments%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520is%2520more%2520robust%2520compared%2520to%2520non-GNC%2520counterparts%252C%2520and%2520yields%250Afaster%2520convergence%2520times%2520compared%2520to%2520other%2520GNC%2520formulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.06869v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Graduated%20Nonconvexity%20Loss%20Function%20for%20Robust%20Nonlinear%0A%20%20Least%20Squares%20Solutions&entry.906535625=Kyungmin%20Jung%20and%20Thomas%20Hitchcox%20and%20James%20Richard%20Forbes&entry.1292438233=%20%20Many%20problems%20in%20robotics%2C%20such%20as%20estimating%20the%20state%20from%20noisy%20sensor%0Adata%20or%20aligning%20two%20point%20clouds%2C%20can%20be%20posed%20and%20solved%20as%20least-squares%0Aproblems.%20Unfortunately%2C%20vanilla%20nonminimal%20solvers%20for%20least-squares%20problems%0Aare%20notoriously%20sensitive%20to%20outliers.%20As%20such%2C%20various%20robust%20loss%20functions%0Ahave%20been%20proposed%20to%20reduce%20the%20sensitivity%20to%20outliers.%20Examples%20of%20loss%0Afunctions%20include%20pseudo-Huber%2C%20Cauchy%2C%20and%20Geman-McClure.%20Recently%2C%20these%20loss%0Afunctions%20have%20been%20generalized%20into%20a%20single%20loss%20function%20that%20enables%20the%0Abest%20loss%20function%20to%20be%20found%20adaptively%20based%20on%20the%20distribution%20of%20the%0Aresiduals.%20However%2C%20even%20with%20the%20generalized%20robust%20loss%20function%2C%20most%0Anonminimal%20solvers%20can%20only%20be%20solved%20locally%20given%20a%20prior%20state%20estimate%20due%0Ato%20the%20nonconvexity%20of%20the%20problem.%20The%20first%20contribution%20of%20this%20paper%20is%20to%0Acombine%20graduated%20nonconvexity%20%28GNC%29%20with%20the%20generalized%20robust%20loss%20function%0Ato%20solve%20least-squares%20problems%20without%20a%20prior%20state%20estimate%20and%20without%20the%0Aneed%20to%20specify%20a%20loss%20function.%20Moreover%2C%20existing%20loss%20functions%2C%20including%0Athe%20generalized%20loss%20function%2C%20are%20based%20on%20Gaussian-like%20distribution.%0AHowever%2C%20residuals%20are%20often%20defined%20as%20the%20squared%20norm%20of%20a%20multivariate%0Aerror%20and%20distributed%20in%20a%20Chi-like%20fashion.%20The%20second%20contribution%20of%20this%0Apaper%20is%20to%20apply%20a%20norm-aware%20adaptive%20robust%20loss%20function%20within%20a%20GNC%0Aframework.%20The%20proposed%20approach%20enables%20a%20GNC%20formulation%20of%20a%20generalized%0Aloss%20function%20such%20that%20GNC%20can%20be%20readily%20applied%20to%20a%20wider%20family%20of%20loss%0Afunctions.%20Furthermore%2C%20simulations%20and%20experiments%20demonstrate%20that%20the%0Aproposed%20method%20is%20more%20robust%20compared%20to%20non-GNC%20counterparts%2C%20and%20yields%0Afaster%20convergence%20times%20compared%20to%20other%20GNC%20formulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.06869v5&entry.124074799=Read"},
{"title": "SpaER: Learning Spatio-temporal Equivariant Representations for Fetal\n  Brain Motion Tracking", "author": "Jian Wang and Razieh Faghihpirayesh and Polina Golland and Ali Ghoulipour", "abstract": "  In this paper, we introduce SpaER, a pioneering method for fetal motion\ntracking that leverages equivariant filters and self-attention mechanisms to\neffectively learn spatio-temporal representations. Different from conventional\napproaches that statically estimate fetal brain motions from pairs of images,\nour method dynamically tracks the rigid movement patterns of the fetal head\nacross temporal and spatial dimensions. Specifically, we first develop an\nequivariant neural network that efficiently learns rigid motion sequences\nthrough low-dimensional spatial representations of images. Subsequently, we\nlearn spatio-temporal representations by incorporating time encoding and\nself-attention neural network layers. This approach allows for the capture of\nlong-term dependencies of fetal brain motion and addresses alignment errors due\nto contrast changes and severe motion artifacts. Our model also provides a\ngeometric deformation estimation that properly addresses image distortions\namong all time frames. To the best of our knowledge, our approach is the first\nto learn spatial-temporal representations via deep neural networks for fetal\nmotion tracking without data augmentation. We validated our model using real\nfetal echo-planar images with simulated and real motions. Our method carries\nsignificant potential value in accurately measuring, tracking, and correcting\nfetal motion in fetal MRI sequences.\n", "link": "http://arxiv.org/abs/2407.20198v2", "date": "2024-07-30", "relevancy": 2.0429, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5122}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking&body=Title%3A%20SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking%0AAuthor%3A%20Jian%20Wang%20and%20Razieh%20Faghihpirayesh%20and%20Polina%20Golland%20and%20Ali%20Ghoulipour%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SpaER%2C%20a%20pioneering%20method%20for%20fetal%20motion%0Atracking%20that%20leverages%20equivariant%20filters%20and%20self-attention%20mechanisms%20to%0Aeffectively%20learn%20spatio-temporal%20representations.%20Different%20from%20conventional%0Aapproaches%20that%20statically%20estimate%20fetal%20brain%20motions%20from%20pairs%20of%20images%2C%0Aour%20method%20dynamically%20tracks%20the%20rigid%20movement%20patterns%20of%20the%20fetal%20head%0Aacross%20temporal%20and%20spatial%20dimensions.%20Specifically%2C%20we%20first%20develop%20an%0Aequivariant%20neural%20network%20that%20efficiently%20learns%20rigid%20motion%20sequences%0Athrough%20low-dimensional%20spatial%20representations%20of%20images.%20Subsequently%2C%20we%0Alearn%20spatio-temporal%20representations%20by%20incorporating%20time%20encoding%20and%0Aself-attention%20neural%20network%20layers.%20This%20approach%20allows%20for%20the%20capture%20of%0Along-term%20dependencies%20of%20fetal%20brain%20motion%20and%20addresses%20alignment%20errors%20due%0Ato%20contrast%20changes%20and%20severe%20motion%20artifacts.%20Our%20model%20also%20provides%20a%0Ageometric%20deformation%20estimation%20that%20properly%20addresses%20image%20distortions%0Aamong%20all%20time%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20approach%20is%20the%20first%0Ato%20learn%20spatial-temporal%20representations%20via%20deep%20neural%20networks%20for%20fetal%0Amotion%20tracking%20without%20data%20augmentation.%20We%20validated%20our%20model%20using%20real%0Afetal%20echo-planar%20images%20with%20simulated%20and%20real%20motions.%20Our%20method%20carries%0Asignificant%20potential%20value%20in%20accurately%20measuring%2C%20tracking%2C%20and%20correcting%0Afetal%20motion%20in%20fetal%20MRI%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20198v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaER%253A%2520Learning%2520Spatio-temporal%2520Equivariant%2520Representations%2520for%2520Fetal%250A%2520%2520Brain%2520Motion%2520Tracking%26entry.906535625%3DJian%2520Wang%2520and%2520Razieh%2520Faghihpirayesh%2520and%2520Polina%2520Golland%2520and%2520Ali%2520Ghoulipour%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SpaER%252C%2520a%2520pioneering%2520method%2520for%2520fetal%2520motion%250Atracking%2520that%2520leverages%2520equivariant%2520filters%2520and%2520self-attention%2520mechanisms%2520to%250Aeffectively%2520learn%2520spatio-temporal%2520representations.%2520Different%2520from%2520conventional%250Aapproaches%2520that%2520statically%2520estimate%2520fetal%2520brain%2520motions%2520from%2520pairs%2520of%2520images%252C%250Aour%2520method%2520dynamically%2520tracks%2520the%2520rigid%2520movement%2520patterns%2520of%2520the%2520fetal%2520head%250Aacross%2520temporal%2520and%2520spatial%2520dimensions.%2520Specifically%252C%2520we%2520first%2520develop%2520an%250Aequivariant%2520neural%2520network%2520that%2520efficiently%2520learns%2520rigid%2520motion%2520sequences%250Athrough%2520low-dimensional%2520spatial%2520representations%2520of%2520images.%2520Subsequently%252C%2520we%250Alearn%2520spatio-temporal%2520representations%2520by%2520incorporating%2520time%2520encoding%2520and%250Aself-attention%2520neural%2520network%2520layers.%2520This%2520approach%2520allows%2520for%2520the%2520capture%2520of%250Along-term%2520dependencies%2520of%2520fetal%2520brain%2520motion%2520and%2520addresses%2520alignment%2520errors%2520due%250Ato%2520contrast%2520changes%2520and%2520severe%2520motion%2520artifacts.%2520Our%2520model%2520also%2520provides%2520a%250Ageometric%2520deformation%2520estimation%2520that%2520properly%2520addresses%2520image%2520distortions%250Aamong%2520all%2520time%2520frames.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520approach%2520is%2520the%2520first%250Ato%2520learn%2520spatial-temporal%2520representations%2520via%2520deep%2520neural%2520networks%2520for%2520fetal%250Amotion%2520tracking%2520without%2520data%2520augmentation.%2520We%2520validated%2520our%2520model%2520using%2520real%250Afetal%2520echo-planar%2520images%2520with%2520simulated%2520and%2520real%2520motions.%2520Our%2520method%2520carries%250Asignificant%2520potential%2520value%2520in%2520accurately%2520measuring%252C%2520tracking%252C%2520and%2520correcting%250Afetal%2520motion%2520in%2520fetal%2520MRI%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20198v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking&entry.906535625=Jian%20Wang%20and%20Razieh%20Faghihpirayesh%20and%20Polina%20Golland%20and%20Ali%20Ghoulipour&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SpaER%2C%20a%20pioneering%20method%20for%20fetal%20motion%0Atracking%20that%20leverages%20equivariant%20filters%20and%20self-attention%20mechanisms%20to%0Aeffectively%20learn%20spatio-temporal%20representations.%20Different%20from%20conventional%0Aapproaches%20that%20statically%20estimate%20fetal%20brain%20motions%20from%20pairs%20of%20images%2C%0Aour%20method%20dynamically%20tracks%20the%20rigid%20movement%20patterns%20of%20the%20fetal%20head%0Aacross%20temporal%20and%20spatial%20dimensions.%20Specifically%2C%20we%20first%20develop%20an%0Aequivariant%20neural%20network%20that%20efficiently%20learns%20rigid%20motion%20sequences%0Athrough%20low-dimensional%20spatial%20representations%20of%20images.%20Subsequently%2C%20we%0Alearn%20spatio-temporal%20representations%20by%20incorporating%20time%20encoding%20and%0Aself-attention%20neural%20network%20layers.%20This%20approach%20allows%20for%20the%20capture%20of%0Along-term%20dependencies%20of%20fetal%20brain%20motion%20and%20addresses%20alignment%20errors%20due%0Ato%20contrast%20changes%20and%20severe%20motion%20artifacts.%20Our%20model%20also%20provides%20a%0Ageometric%20deformation%20estimation%20that%20properly%20addresses%20image%20distortions%0Aamong%20all%20time%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20approach%20is%20the%20first%0Ato%20learn%20spatial-temporal%20representations%20via%20deep%20neural%20networks%20for%20fetal%0Amotion%20tracking%20without%20data%20augmentation.%20We%20validated%20our%20model%20using%20real%0Afetal%20echo-planar%20images%20with%20simulated%20and%20real%20motions.%20Our%20method%20carries%0Asignificant%20potential%20value%20in%20accurately%20measuring%2C%20tracking%2C%20and%20correcting%0Afetal%20motion%20in%20fetal%20MRI%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20198v2&entry.124074799=Read"},
{"title": "Local-peak scale-invariant feature transform for fast and random image\n  stitching", "author": "Hao Li and Lipo Wang and Tianyun Zhao and Wei Zhao", "abstract": "  Image stitching aims to construct a wide field of view with high spatial\nresolution, which cannot be achieved in a single exposure. Typically,\nconventional image stitching techniques, other than deep learning, require\ncomplex computation and thus computational pricy, especially for stitching\nlarge raw images. In this study, inspired by the multiscale feature of fluid\nturbulence, we developed a fast feature point detection algorithm named\nlocal-peak scale-invariant feature transform (LP-SIFT), based on the multiscale\nlocal peaks and scale-invariant feature transform method. By combining LP-SIFT\nand RANSAC in image stitching, the stitching speed can be improved by orders,\ncompared with the original SIFT method. Nine large images (over 2600*1600\npixels), arranged randomly without prior knowledge, can be stitched within\n158.94 s. The algorithm is highly practical for applications requiring a wide\nfield of view in diverse application scenes, e.g., terrain mapping, biological\nanalysis, and even criminal investigation.\n", "link": "http://arxiv.org/abs/2405.08578v2", "date": "2024-07-30", "relevancy": 2.0271, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5307}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4933}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local-peak%20scale-invariant%20feature%20transform%20for%20fast%20and%20random%20image%0A%20%20stitching&body=Title%3A%20Local-peak%20scale-invariant%20feature%20transform%20for%20fast%20and%20random%20image%0A%20%20stitching%0AAuthor%3A%20Hao%20Li%20and%20Lipo%20Wang%20and%20Tianyun%20Zhao%20and%20Wei%20Zhao%0AAbstract%3A%20%20%20Image%20stitching%20aims%20to%20construct%20a%20wide%20field%20of%20view%20with%20high%20spatial%0Aresolution%2C%20which%20cannot%20be%20achieved%20in%20a%20single%20exposure.%20Typically%2C%0Aconventional%20image%20stitching%20techniques%2C%20other%20than%20deep%20learning%2C%20require%0Acomplex%20computation%20and%20thus%20computational%20pricy%2C%20especially%20for%20stitching%0Alarge%20raw%20images.%20In%20this%20study%2C%20inspired%20by%20the%20multiscale%20feature%20of%20fluid%0Aturbulence%2C%20we%20developed%20a%20fast%20feature%20point%20detection%20algorithm%20named%0Alocal-peak%20scale-invariant%20feature%20transform%20%28LP-SIFT%29%2C%20based%20on%20the%20multiscale%0Alocal%20peaks%20and%20scale-invariant%20feature%20transform%20method.%20By%20combining%20LP-SIFT%0Aand%20RANSAC%20in%20image%20stitching%2C%20the%20stitching%20speed%20can%20be%20improved%20by%20orders%2C%0Acompared%20with%20the%20original%20SIFT%20method.%20Nine%20large%20images%20%28over%202600%2A1600%0Apixels%29%2C%20arranged%20randomly%20without%20prior%20knowledge%2C%20can%20be%20stitched%20within%0A158.94%20s.%20The%20algorithm%20is%20highly%20practical%20for%20applications%20requiring%20a%20wide%0Afield%20of%20view%20in%20diverse%20application%20scenes%2C%20e.g.%2C%20terrain%20mapping%2C%20biological%0Aanalysis%2C%20and%20even%20criminal%20investigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal-peak%2520scale-invariant%2520feature%2520transform%2520for%2520fast%2520and%2520random%2520image%250A%2520%2520stitching%26entry.906535625%3DHao%2520Li%2520and%2520Lipo%2520Wang%2520and%2520Tianyun%2520Zhao%2520and%2520Wei%2520Zhao%26entry.1292438233%3D%2520%2520Image%2520stitching%2520aims%2520to%2520construct%2520a%2520wide%2520field%2520of%2520view%2520with%2520high%2520spatial%250Aresolution%252C%2520which%2520cannot%2520be%2520achieved%2520in%2520a%2520single%2520exposure.%2520Typically%252C%250Aconventional%2520image%2520stitching%2520techniques%252C%2520other%2520than%2520deep%2520learning%252C%2520require%250Acomplex%2520computation%2520and%2520thus%2520computational%2520pricy%252C%2520especially%2520for%2520stitching%250Alarge%2520raw%2520images.%2520In%2520this%2520study%252C%2520inspired%2520by%2520the%2520multiscale%2520feature%2520of%2520fluid%250Aturbulence%252C%2520we%2520developed%2520a%2520fast%2520feature%2520point%2520detection%2520algorithm%2520named%250Alocal-peak%2520scale-invariant%2520feature%2520transform%2520%2528LP-SIFT%2529%252C%2520based%2520on%2520the%2520multiscale%250Alocal%2520peaks%2520and%2520scale-invariant%2520feature%2520transform%2520method.%2520By%2520combining%2520LP-SIFT%250Aand%2520RANSAC%2520in%2520image%2520stitching%252C%2520the%2520stitching%2520speed%2520can%2520be%2520improved%2520by%2520orders%252C%250Acompared%2520with%2520the%2520original%2520SIFT%2520method.%2520Nine%2520large%2520images%2520%2528over%25202600%252A1600%250Apixels%2529%252C%2520arranged%2520randomly%2520without%2520prior%2520knowledge%252C%2520can%2520be%2520stitched%2520within%250A158.94%2520s.%2520The%2520algorithm%2520is%2520highly%2520practical%2520for%2520applications%2520requiring%2520a%2520wide%250Afield%2520of%2520view%2520in%2520diverse%2520application%2520scenes%252C%2520e.g.%252C%2520terrain%2520mapping%252C%2520biological%250Aanalysis%252C%2520and%2520even%2520criminal%2520investigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local-peak%20scale-invariant%20feature%20transform%20for%20fast%20and%20random%20image%0A%20%20stitching&entry.906535625=Hao%20Li%20and%20Lipo%20Wang%20and%20Tianyun%20Zhao%20and%20Wei%20Zhao&entry.1292438233=%20%20Image%20stitching%20aims%20to%20construct%20a%20wide%20field%20of%20view%20with%20high%20spatial%0Aresolution%2C%20which%20cannot%20be%20achieved%20in%20a%20single%20exposure.%20Typically%2C%0Aconventional%20image%20stitching%20techniques%2C%20other%20than%20deep%20learning%2C%20require%0Acomplex%20computation%20and%20thus%20computational%20pricy%2C%20especially%20for%20stitching%0Alarge%20raw%20images.%20In%20this%20study%2C%20inspired%20by%20the%20multiscale%20feature%20of%20fluid%0Aturbulence%2C%20we%20developed%20a%20fast%20feature%20point%20detection%20algorithm%20named%0Alocal-peak%20scale-invariant%20feature%20transform%20%28LP-SIFT%29%2C%20based%20on%20the%20multiscale%0Alocal%20peaks%20and%20scale-invariant%20feature%20transform%20method.%20By%20combining%20LP-SIFT%0Aand%20RANSAC%20in%20image%20stitching%2C%20the%20stitching%20speed%20can%20be%20improved%20by%20orders%2C%0Acompared%20with%20the%20original%20SIFT%20method.%20Nine%20large%20images%20%28over%202600%2A1600%0Apixels%29%2C%20arranged%20randomly%20without%20prior%20knowledge%2C%20can%20be%20stitched%20within%0A158.94%20s.%20The%20algorithm%20is%20highly%20practical%20for%20applications%20requiring%20a%20wide%0Afield%20of%20view%20in%20diverse%20application%20scenes%2C%20e.g.%2C%20terrain%20mapping%2C%20biological%0Aanalysis%2C%20and%20even%20criminal%20investigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08578v2&entry.124074799=Read"},
{"title": "Efficient Pareto Manifold Learning with Low-Rank Structure", "author": "Weiyu Chen and James T. Kwok", "abstract": "  Multi-task learning, which optimizes performance across multiple tasks, is\ninherently a multi-objective optimization problem. Various algorithms are\ndeveloped to provide discrete trade-off solutions on the Pareto front.\nRecently, continuous Pareto front approximations using a linear combination of\nbase networks have emerged as a compelling strategy. However, it suffers from\nscalability issues when the number of tasks is large. To address this issue, we\npropose a novel approach that integrates a main network with several low-rank\nmatrices to efficiently learn the Pareto manifold. It significantly reduces the\nnumber of parameters and facilitates the extraction of shared features. We also\nintroduce orthogonal regularization to further bolster performance. Extensive\nexperimental results demonstrate that the proposed approach outperforms\nstate-of-the-art baselines, especially on datasets with a large number of\ntasks.\n", "link": "http://arxiv.org/abs/2407.20734v1", "date": "2024-07-30", "relevancy": 2.0231, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5268}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4965}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Pareto%20Manifold%20Learning%20with%20Low-Rank%20Structure&body=Title%3A%20Efficient%20Pareto%20Manifold%20Learning%20with%20Low-Rank%20Structure%0AAuthor%3A%20Weiyu%20Chen%20and%20James%20T.%20Kwok%0AAbstract%3A%20%20%20Multi-task%20learning%2C%20which%20optimizes%20performance%20across%20multiple%20tasks%2C%20is%0Ainherently%20a%20multi-objective%20optimization%20problem.%20Various%20algorithms%20are%0Adeveloped%20to%20provide%20discrete%20trade-off%20solutions%20on%20the%20Pareto%20front.%0ARecently%2C%20continuous%20Pareto%20front%20approximations%20using%20a%20linear%20combination%20of%0Abase%20networks%20have%20emerged%20as%20a%20compelling%20strategy.%20However%2C%20it%20suffers%20from%0Ascalability%20issues%20when%20the%20number%20of%20tasks%20is%20large.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20approach%20that%20integrates%20a%20main%20network%20with%20several%20low-rank%0Amatrices%20to%20efficiently%20learn%20the%20Pareto%20manifold.%20It%20significantly%20reduces%20the%0Anumber%20of%20parameters%20and%20facilitates%20the%20extraction%20of%20shared%20features.%20We%20also%0Aintroduce%20orthogonal%20regularization%20to%20further%20bolster%20performance.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20approach%20outperforms%0Astate-of-the-art%20baselines%2C%20especially%20on%20datasets%20with%20a%20large%20number%20of%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Pareto%2520Manifold%2520Learning%2520with%2520Low-Rank%2520Structure%26entry.906535625%3DWeiyu%2520Chen%2520and%2520James%2520T.%2520Kwok%26entry.1292438233%3D%2520%2520Multi-task%2520learning%252C%2520which%2520optimizes%2520performance%2520across%2520multiple%2520tasks%252C%2520is%250Ainherently%2520a%2520multi-objective%2520optimization%2520problem.%2520Various%2520algorithms%2520are%250Adeveloped%2520to%2520provide%2520discrete%2520trade-off%2520solutions%2520on%2520the%2520Pareto%2520front.%250ARecently%252C%2520continuous%2520Pareto%2520front%2520approximations%2520using%2520a%2520linear%2520combination%2520of%250Abase%2520networks%2520have%2520emerged%2520as%2520a%2520compelling%2520strategy.%2520However%252C%2520it%2520suffers%2520from%250Ascalability%2520issues%2520when%2520the%2520number%2520of%2520tasks%2520is%2520large.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520that%2520integrates%2520a%2520main%2520network%2520with%2520several%2520low-rank%250Amatrices%2520to%2520efficiently%2520learn%2520the%2520Pareto%2520manifold.%2520It%2520significantly%2520reduces%2520the%250Anumber%2520of%2520parameters%2520and%2520facilitates%2520the%2520extraction%2520of%2520shared%2520features.%2520We%2520also%250Aintroduce%2520orthogonal%2520regularization%2520to%2520further%2520bolster%2520performance.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520outperforms%250Astate-of-the-art%2520baselines%252C%2520especially%2520on%2520datasets%2520with%2520a%2520large%2520number%2520of%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Pareto%20Manifold%20Learning%20with%20Low-Rank%20Structure&entry.906535625=Weiyu%20Chen%20and%20James%20T.%20Kwok&entry.1292438233=%20%20Multi-task%20learning%2C%20which%20optimizes%20performance%20across%20multiple%20tasks%2C%20is%0Ainherently%20a%20multi-objective%20optimization%20problem.%20Various%20algorithms%20are%0Adeveloped%20to%20provide%20discrete%20trade-off%20solutions%20on%20the%20Pareto%20front.%0ARecently%2C%20continuous%20Pareto%20front%20approximations%20using%20a%20linear%20combination%20of%0Abase%20networks%20have%20emerged%20as%20a%20compelling%20strategy.%20However%2C%20it%20suffers%20from%0Ascalability%20issues%20when%20the%20number%20of%20tasks%20is%20large.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20approach%20that%20integrates%20a%20main%20network%20with%20several%20low-rank%0Amatrices%20to%20efficiently%20learn%20the%20Pareto%20manifold.%20It%20significantly%20reduces%20the%0Anumber%20of%20parameters%20and%20facilitates%20the%20extraction%20of%20shared%20features.%20We%20also%0Aintroduce%20orthogonal%20regularization%20to%20further%20bolster%20performance.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20approach%20outperforms%0Astate-of-the-art%20baselines%2C%20especially%20on%20datasets%20with%20a%20large%20number%20of%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20734v1&entry.124074799=Read"},
{"title": "EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from\n  bi-planar X-ray images", "author": "Lixing Tan and Shuang Song and Yaofeng He and Kangneng Zhou and Tong Lu and Ruoxiu Xiao", "abstract": "  X-ray images ease the diagnosis and treatment process due to their rapid\nimaging speed and high resolution. However, due to the projection process of\nX-ray imaging, much spatial information has been lost. To accurately provide\nefficient spinal morphological and structural information, reconstructing the\n3-D structures of the spine from the 2-D X-ray images is essential. It is\nchallenging for current reconstruction methods to preserve the edge information\nand local shapes of the asymmetrical vertebrae structures. In this study, we\npropose a new Edge-Aware Reconstruction network (EAR) to focus on the\nperformance improvement of the edge information and vertebrae shapes. In our\nnetwork, by using the auto-encoder architecture as the backbone, the edge\nattention module and frequency enhancement module are proposed to strengthen\nthe perception of the edge reconstruction. Meanwhile, we also combine four loss\nterms, including reconstruction loss, edge loss, frequency loss and projection\nloss. The proposed method is evaluated using three publicly accessible datasets\nand compared with four state-of-the-art models. The proposed method is superior\nto other methods and achieves 25.32%, 15.32%, 86.44%, 80.13%, 23.7612 and\n0.3014 with regard to MSE, MAE, Dice, SSIM, PSNR and frequency distance. Due to\nthe end-to-end and accurate reconstruction process, EAR can provide sufficient\n3-D spatial information and precise preoperative surgical planning guidance.\n", "link": "http://arxiv.org/abs/2407.20937v1", "date": "2024-07-30", "relevancy": 2.0139, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5094}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EAR%3A%20Edge-Aware%20Reconstruction%20of%203-D%20vertebrae%20structures%20from%0A%20%20bi-planar%20X-ray%20images&body=Title%3A%20EAR%3A%20Edge-Aware%20Reconstruction%20of%203-D%20vertebrae%20structures%20from%0A%20%20bi-planar%20X-ray%20images%0AAuthor%3A%20Lixing%20Tan%20and%20Shuang%20Song%20and%20Yaofeng%20He%20and%20Kangneng%20Zhou%20and%20Tong%20Lu%20and%20Ruoxiu%20Xiao%0AAbstract%3A%20%20%20X-ray%20images%20ease%20the%20diagnosis%20and%20treatment%20process%20due%20to%20their%20rapid%0Aimaging%20speed%20and%20high%20resolution.%20However%2C%20due%20to%20the%20projection%20process%20of%0AX-ray%20imaging%2C%20much%20spatial%20information%20has%20been%20lost.%20To%20accurately%20provide%0Aefficient%20spinal%20morphological%20and%20structural%20information%2C%20reconstructing%20the%0A3-D%20structures%20of%20the%20spine%20from%20the%202-D%20X-ray%20images%20is%20essential.%20It%20is%0Achallenging%20for%20current%20reconstruction%20methods%20to%20preserve%20the%20edge%20information%0Aand%20local%20shapes%20of%20the%20asymmetrical%20vertebrae%20structures.%20In%20this%20study%2C%20we%0Apropose%20a%20new%20Edge-Aware%20Reconstruction%20network%20%28EAR%29%20to%20focus%20on%20the%0Aperformance%20improvement%20of%20the%20edge%20information%20and%20vertebrae%20shapes.%20In%20our%0Anetwork%2C%20by%20using%20the%20auto-encoder%20architecture%20as%20the%20backbone%2C%20the%20edge%0Aattention%20module%20and%20frequency%20enhancement%20module%20are%20proposed%20to%20strengthen%0Athe%20perception%20of%20the%20edge%20reconstruction.%20Meanwhile%2C%20we%20also%20combine%20four%20loss%0Aterms%2C%20including%20reconstruction%20loss%2C%20edge%20loss%2C%20frequency%20loss%20and%20projection%0Aloss.%20The%20proposed%20method%20is%20evaluated%20using%20three%20publicly%20accessible%20datasets%0Aand%20compared%20with%20four%20state-of-the-art%20models.%20The%20proposed%20method%20is%20superior%0Ato%20other%20methods%20and%20achieves%2025.32%25%2C%2015.32%25%2C%2086.44%25%2C%2080.13%25%2C%2023.7612%20and%0A0.3014%20with%20regard%20to%20MSE%2C%20MAE%2C%20Dice%2C%20SSIM%2C%20PSNR%20and%20frequency%20distance.%20Due%20to%0Athe%20end-to-end%20and%20accurate%20reconstruction%20process%2C%20EAR%20can%20provide%20sufficient%0A3-D%20spatial%20information%20and%20precise%20preoperative%20surgical%20planning%20guidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEAR%253A%2520Edge-Aware%2520Reconstruction%2520of%25203-D%2520vertebrae%2520structures%2520from%250A%2520%2520bi-planar%2520X-ray%2520images%26entry.906535625%3DLixing%2520Tan%2520and%2520Shuang%2520Song%2520and%2520Yaofeng%2520He%2520and%2520Kangneng%2520Zhou%2520and%2520Tong%2520Lu%2520and%2520Ruoxiu%2520Xiao%26entry.1292438233%3D%2520%2520X-ray%2520images%2520ease%2520the%2520diagnosis%2520and%2520treatment%2520process%2520due%2520to%2520their%2520rapid%250Aimaging%2520speed%2520and%2520high%2520resolution.%2520However%252C%2520due%2520to%2520the%2520projection%2520process%2520of%250AX-ray%2520imaging%252C%2520much%2520spatial%2520information%2520has%2520been%2520lost.%2520To%2520accurately%2520provide%250Aefficient%2520spinal%2520morphological%2520and%2520structural%2520information%252C%2520reconstructing%2520the%250A3-D%2520structures%2520of%2520the%2520spine%2520from%2520the%25202-D%2520X-ray%2520images%2520is%2520essential.%2520It%2520is%250Achallenging%2520for%2520current%2520reconstruction%2520methods%2520to%2520preserve%2520the%2520edge%2520information%250Aand%2520local%2520shapes%2520of%2520the%2520asymmetrical%2520vertebrae%2520structures.%2520In%2520this%2520study%252C%2520we%250Apropose%2520a%2520new%2520Edge-Aware%2520Reconstruction%2520network%2520%2528EAR%2529%2520to%2520focus%2520on%2520the%250Aperformance%2520improvement%2520of%2520the%2520edge%2520information%2520and%2520vertebrae%2520shapes.%2520In%2520our%250Anetwork%252C%2520by%2520using%2520the%2520auto-encoder%2520architecture%2520as%2520the%2520backbone%252C%2520the%2520edge%250Aattention%2520module%2520and%2520frequency%2520enhancement%2520module%2520are%2520proposed%2520to%2520strengthen%250Athe%2520perception%2520of%2520the%2520edge%2520reconstruction.%2520Meanwhile%252C%2520we%2520also%2520combine%2520four%2520loss%250Aterms%252C%2520including%2520reconstruction%2520loss%252C%2520edge%2520loss%252C%2520frequency%2520loss%2520and%2520projection%250Aloss.%2520The%2520proposed%2520method%2520is%2520evaluated%2520using%2520three%2520publicly%2520accessible%2520datasets%250Aand%2520compared%2520with%2520four%2520state-of-the-art%2520models.%2520The%2520proposed%2520method%2520is%2520superior%250Ato%2520other%2520methods%2520and%2520achieves%252025.32%2525%252C%252015.32%2525%252C%252086.44%2525%252C%252080.13%2525%252C%252023.7612%2520and%250A0.3014%2520with%2520regard%2520to%2520MSE%252C%2520MAE%252C%2520Dice%252C%2520SSIM%252C%2520PSNR%2520and%2520frequency%2520distance.%2520Due%2520to%250Athe%2520end-to-end%2520and%2520accurate%2520reconstruction%2520process%252C%2520EAR%2520can%2520provide%2520sufficient%250A3-D%2520spatial%2520information%2520and%2520precise%2520preoperative%2520surgical%2520planning%2520guidance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EAR%3A%20Edge-Aware%20Reconstruction%20of%203-D%20vertebrae%20structures%20from%0A%20%20bi-planar%20X-ray%20images&entry.906535625=Lixing%20Tan%20and%20Shuang%20Song%20and%20Yaofeng%20He%20and%20Kangneng%20Zhou%20and%20Tong%20Lu%20and%20Ruoxiu%20Xiao&entry.1292438233=%20%20X-ray%20images%20ease%20the%20diagnosis%20and%20treatment%20process%20due%20to%20their%20rapid%0Aimaging%20speed%20and%20high%20resolution.%20However%2C%20due%20to%20the%20projection%20process%20of%0AX-ray%20imaging%2C%20much%20spatial%20information%20has%20been%20lost.%20To%20accurately%20provide%0Aefficient%20spinal%20morphological%20and%20structural%20information%2C%20reconstructing%20the%0A3-D%20structures%20of%20the%20spine%20from%20the%202-D%20X-ray%20images%20is%20essential.%20It%20is%0Achallenging%20for%20current%20reconstruction%20methods%20to%20preserve%20the%20edge%20information%0Aand%20local%20shapes%20of%20the%20asymmetrical%20vertebrae%20structures.%20In%20this%20study%2C%20we%0Apropose%20a%20new%20Edge-Aware%20Reconstruction%20network%20%28EAR%29%20to%20focus%20on%20the%0Aperformance%20improvement%20of%20the%20edge%20information%20and%20vertebrae%20shapes.%20In%20our%0Anetwork%2C%20by%20using%20the%20auto-encoder%20architecture%20as%20the%20backbone%2C%20the%20edge%0Aattention%20module%20and%20frequency%20enhancement%20module%20are%20proposed%20to%20strengthen%0Athe%20perception%20of%20the%20edge%20reconstruction.%20Meanwhile%2C%20we%20also%20combine%20four%20loss%0Aterms%2C%20including%20reconstruction%20loss%2C%20edge%20loss%2C%20frequency%20loss%20and%20projection%0Aloss.%20The%20proposed%20method%20is%20evaluated%20using%20three%20publicly%20accessible%20datasets%0Aand%20compared%20with%20four%20state-of-the-art%20models.%20The%20proposed%20method%20is%20superior%0Ato%20other%20methods%20and%20achieves%2025.32%25%2C%2015.32%25%2C%2086.44%25%2C%2080.13%25%2C%2023.7612%20and%0A0.3014%20with%20regard%20to%20MSE%2C%20MAE%2C%20Dice%2C%20SSIM%2C%20PSNR%20and%20frequency%20distance.%20Due%20to%0Athe%20end-to-end%20and%20accurate%20reconstruction%20process%2C%20EAR%20can%20provide%20sufficient%0A3-D%20spatial%20information%20and%20precise%20preoperative%20surgical%20planning%20guidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20937v1&entry.124074799=Read"},
{"title": "GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language\n  Models", "author": "Ali Abdollahi and Mahdi Ghaznavi and Mohammad Reza Karimi Nejad and Arash Mari Oriyad and Reza Abbasi and Ali Salesi and Melika Behjati and Mohammad Hossein Rohban and Mahdieh Soleymani Baghshah", "abstract": "  Vision-language models (VLMs) are intensively used in many downstream tasks,\nincluding those requiring assessments of individuals appearing in the images.\nWhile VLMs perform well in simple single-person scenarios, in real-world\napplications, we often face complex situations in which there are persons of\ndifferent genders doing different activities. We show that in such cases, VLMs\nare biased towards identifying the individual with the expected gender\n(according to ingrained gender stereotypes in the model or other forms of\nsample selection bias) as the performer of the activity. We refer to this bias\nin associating an activity with the gender of its actual performer in an image\nor text as the Gender-Activity Binding (GAB) bias and analyze how this bias is\ninternalized in VLMs. To assess this bias, we have introduced the GAB dataset\nwith approximately 5500 AI-generated images that represent a variety of\nactivities, addressing the scarcity of real-world images for some scenarios. To\nhave extensive quality control, the generated images are evaluated for their\ndiversity, quality, and realism. We have tested 12 renowned pre-trained VLMs on\nthis dataset in the context of text-to-image and image-to-text retrieval to\nmeasure the effect of this bias on their predictions. Additionally, we have\ncarried out supplementary experiments to quantify the bias in VLMs' text\nencoders and to evaluate VLMs' capability to recognize activities. Our\nexperiments indicate that VLMs experience an average performance decline of\nabout 13.2% when confronted with gender-activity binding bias.\n", "link": "http://arxiv.org/abs/2407.21001v1", "date": "2024-07-30", "relevancy": 1.9945, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5042}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5011}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GABInsight%3A%20Exploring%20Gender-Activity%20Binding%20Bias%20in%20Vision-Language%0A%20%20Models&body=Title%3A%20GABInsight%3A%20Exploring%20Gender-Activity%20Binding%20Bias%20in%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Ali%20Abdollahi%20and%20Mahdi%20Ghaznavi%20and%20Mohammad%20Reza%20Karimi%20Nejad%20and%20Arash%20Mari%20Oriyad%20and%20Reza%20Abbasi%20and%20Ali%20Salesi%20and%20Melika%20Behjati%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20are%20intensively%20used%20in%20many%20downstream%20tasks%2C%0Aincluding%20those%20requiring%20assessments%20of%20individuals%20appearing%20in%20the%20images.%0AWhile%20VLMs%20perform%20well%20in%20simple%20single-person%20scenarios%2C%20in%20real-world%0Aapplications%2C%20we%20often%20face%20complex%20situations%20in%20which%20there%20are%20persons%20of%0Adifferent%20genders%20doing%20different%20activities.%20We%20show%20that%20in%20such%20cases%2C%20VLMs%0Aare%20biased%20towards%20identifying%20the%20individual%20with%20the%20expected%20gender%0A%28according%20to%20ingrained%20gender%20stereotypes%20in%20the%20model%20or%20other%20forms%20of%0Asample%20selection%20bias%29%20as%20the%20performer%20of%20the%20activity.%20We%20refer%20to%20this%20bias%0Ain%20associating%20an%20activity%20with%20the%20gender%20of%20its%20actual%20performer%20in%20an%20image%0Aor%20text%20as%20the%20Gender-Activity%20Binding%20%28GAB%29%20bias%20and%20analyze%20how%20this%20bias%20is%0Ainternalized%20in%20VLMs.%20To%20assess%20this%20bias%2C%20we%20have%20introduced%20the%20GAB%20dataset%0Awith%20approximately%205500%20AI-generated%20images%20that%20represent%20a%20variety%20of%0Aactivities%2C%20addressing%20the%20scarcity%20of%20real-world%20images%20for%20some%20scenarios.%20To%0Ahave%20extensive%20quality%20control%2C%20the%20generated%20images%20are%20evaluated%20for%20their%0Adiversity%2C%20quality%2C%20and%20realism.%20We%20have%20tested%2012%20renowned%20pre-trained%20VLMs%20on%0Athis%20dataset%20in%20the%20context%20of%20text-to-image%20and%20image-to-text%20retrieval%20to%0Ameasure%20the%20effect%20of%20this%20bias%20on%20their%20predictions.%20Additionally%2C%20we%20have%0Acarried%20out%20supplementary%20experiments%20to%20quantify%20the%20bias%20in%20VLMs%27%20text%0Aencoders%20and%20to%20evaluate%20VLMs%27%20capability%20to%20recognize%20activities.%20Our%0Aexperiments%20indicate%20that%20VLMs%20experience%20an%20average%20performance%20decline%20of%0Aabout%2013.2%25%20when%20confronted%20with%20gender-activity%20binding%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGABInsight%253A%2520Exploring%2520Gender-Activity%2520Binding%2520Bias%2520in%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DAli%2520Abdollahi%2520and%2520Mahdi%2520Ghaznavi%2520and%2520Mohammad%2520Reza%2520Karimi%2520Nejad%2520and%2520Arash%2520Mari%2520Oriyad%2520and%2520Reza%2520Abbasi%2520and%2520Ali%2520Salesi%2520and%2520Melika%2520Behjati%2520and%2520Mohammad%2520Hossein%2520Rohban%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520are%2520intensively%2520used%2520in%2520many%2520downstream%2520tasks%252C%250Aincluding%2520those%2520requiring%2520assessments%2520of%2520individuals%2520appearing%2520in%2520the%2520images.%250AWhile%2520VLMs%2520perform%2520well%2520in%2520simple%2520single-person%2520scenarios%252C%2520in%2520real-world%250Aapplications%252C%2520we%2520often%2520face%2520complex%2520situations%2520in%2520which%2520there%2520are%2520persons%2520of%250Adifferent%2520genders%2520doing%2520different%2520activities.%2520We%2520show%2520that%2520in%2520such%2520cases%252C%2520VLMs%250Aare%2520biased%2520towards%2520identifying%2520the%2520individual%2520with%2520the%2520expected%2520gender%250A%2528according%2520to%2520ingrained%2520gender%2520stereotypes%2520in%2520the%2520model%2520or%2520other%2520forms%2520of%250Asample%2520selection%2520bias%2529%2520as%2520the%2520performer%2520of%2520the%2520activity.%2520We%2520refer%2520to%2520this%2520bias%250Ain%2520associating%2520an%2520activity%2520with%2520the%2520gender%2520of%2520its%2520actual%2520performer%2520in%2520an%2520image%250Aor%2520text%2520as%2520the%2520Gender-Activity%2520Binding%2520%2528GAB%2529%2520bias%2520and%2520analyze%2520how%2520this%2520bias%2520is%250Ainternalized%2520in%2520VLMs.%2520To%2520assess%2520this%2520bias%252C%2520we%2520have%2520introduced%2520the%2520GAB%2520dataset%250Awith%2520approximately%25205500%2520AI-generated%2520images%2520that%2520represent%2520a%2520variety%2520of%250Aactivities%252C%2520addressing%2520the%2520scarcity%2520of%2520real-world%2520images%2520for%2520some%2520scenarios.%2520To%250Ahave%2520extensive%2520quality%2520control%252C%2520the%2520generated%2520images%2520are%2520evaluated%2520for%2520their%250Adiversity%252C%2520quality%252C%2520and%2520realism.%2520We%2520have%2520tested%252012%2520renowned%2520pre-trained%2520VLMs%2520on%250Athis%2520dataset%2520in%2520the%2520context%2520of%2520text-to-image%2520and%2520image-to-text%2520retrieval%2520to%250Ameasure%2520the%2520effect%2520of%2520this%2520bias%2520on%2520their%2520predictions.%2520Additionally%252C%2520we%2520have%250Acarried%2520out%2520supplementary%2520experiments%2520to%2520quantify%2520the%2520bias%2520in%2520VLMs%2527%2520text%250Aencoders%2520and%2520to%2520evaluate%2520VLMs%2527%2520capability%2520to%2520recognize%2520activities.%2520Our%250Aexperiments%2520indicate%2520that%2520VLMs%2520experience%2520an%2520average%2520performance%2520decline%2520of%250Aabout%252013.2%2525%2520when%2520confronted%2520with%2520gender-activity%2520binding%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GABInsight%3A%20Exploring%20Gender-Activity%20Binding%20Bias%20in%20Vision-Language%0A%20%20Models&entry.906535625=Ali%20Abdollahi%20and%20Mahdi%20Ghaznavi%20and%20Mohammad%20Reza%20Karimi%20Nejad%20and%20Arash%20Mari%20Oriyad%20and%20Reza%20Abbasi%20and%20Ali%20Salesi%20and%20Melika%20Behjati%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20are%20intensively%20used%20in%20many%20downstream%20tasks%2C%0Aincluding%20those%20requiring%20assessments%20of%20individuals%20appearing%20in%20the%20images.%0AWhile%20VLMs%20perform%20well%20in%20simple%20single-person%20scenarios%2C%20in%20real-world%0Aapplications%2C%20we%20often%20face%20complex%20situations%20in%20which%20there%20are%20persons%20of%0Adifferent%20genders%20doing%20different%20activities.%20We%20show%20that%20in%20such%20cases%2C%20VLMs%0Aare%20biased%20towards%20identifying%20the%20individual%20with%20the%20expected%20gender%0A%28according%20to%20ingrained%20gender%20stereotypes%20in%20the%20model%20or%20other%20forms%20of%0Asample%20selection%20bias%29%20as%20the%20performer%20of%20the%20activity.%20We%20refer%20to%20this%20bias%0Ain%20associating%20an%20activity%20with%20the%20gender%20of%20its%20actual%20performer%20in%20an%20image%0Aor%20text%20as%20the%20Gender-Activity%20Binding%20%28GAB%29%20bias%20and%20analyze%20how%20this%20bias%20is%0Ainternalized%20in%20VLMs.%20To%20assess%20this%20bias%2C%20we%20have%20introduced%20the%20GAB%20dataset%0Awith%20approximately%205500%20AI-generated%20images%20that%20represent%20a%20variety%20of%0Aactivities%2C%20addressing%20the%20scarcity%20of%20real-world%20images%20for%20some%20scenarios.%20To%0Ahave%20extensive%20quality%20control%2C%20the%20generated%20images%20are%20evaluated%20for%20their%0Adiversity%2C%20quality%2C%20and%20realism.%20We%20have%20tested%2012%20renowned%20pre-trained%20VLMs%20on%0Athis%20dataset%20in%20the%20context%20of%20text-to-image%20and%20image-to-text%20retrieval%20to%0Ameasure%20the%20effect%20of%20this%20bias%20on%20their%20predictions.%20Additionally%2C%20we%20have%0Acarried%20out%20supplementary%20experiments%20to%20quantify%20the%20bias%20in%20VLMs%27%20text%0Aencoders%20and%20to%20evaluate%20VLMs%27%20capability%20to%20recognize%20activities.%20Our%0Aexperiments%20indicate%20that%20VLMs%20experience%20an%20average%20performance%20decline%20of%0Aabout%2013.2%25%20when%20confronted%20with%20gender-activity%20binding%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21001v1&entry.124074799=Read"},
{"title": "Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian\n  Neural Networks", "author": "Bao Gia Doan and Afshar Shamsi and Xiao-Yu Guo and Arash Mohammadi and Hamid Alinejad-Rokny and Dino Sejdinovic and Damith C. Ranasinghe and Ehsan Abbasnejad", "abstract": "  Computational complexity of Bayesian learning is impeding its adoption in\npractical, large-scale tasks. Despite demonstrations of significant merits such\nas improved robustness and resilience to unseen or out-of-distribution inputs\nover their non- Bayesian counterparts, their practical use has faded to near\ninsignificance. In this study, we introduce an innovative framework to mitigate\nthe computational burden of Bayesian neural networks (BNNs). Our approach\nfollows the principle of Bayesian techniques based on deep ensembles, but\nsignificantly reduces their cost via multiple low-rank perturbations of\nparameters arising from a pre-trained neural network. Both vanilla version of\nensembles as well as more sophisticated schemes such as Bayesian learning with\nStein Variational Gradient Descent (SVGD), previously deemed impractical for\nlarge models, can be seamlessly implemented within the proposed framework,\ncalled Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a\ndramatic reduction in the number of trainable parameters required to\napproximate a Bayesian posterior; and ii) it not only maintains, but in some\ninstances, surpasses the performance of conventional Bayesian learning methods\nand non-Bayesian baselines. Our results with large-scale tasks such as\nImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the\neffectiveness and versatility of Bella in building highly scalable and\npractical Bayesian deep models for real-world applications.\n", "link": "http://arxiv.org/abs/2407.20891v1", "date": "2024-07-30", "relevancy": 1.9925, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4963}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29%3A%20A%20Practical%20Approach%20to%20Bayesian%0A%20%20Neural%20Networks&body=Title%3A%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29%3A%20A%20Practical%20Approach%20to%20Bayesian%0A%20%20Neural%20Networks%0AAuthor%3A%20Bao%20Gia%20Doan%20and%20Afshar%20Shamsi%20and%20Xiao-Yu%20Guo%20and%20Arash%20Mohammadi%20and%20Hamid%20Alinejad-Rokny%20and%20Dino%20Sejdinovic%20and%20Damith%20C.%20Ranasinghe%20and%20Ehsan%20Abbasnejad%0AAbstract%3A%20%20%20Computational%20complexity%20of%20Bayesian%20learning%20is%20impeding%20its%20adoption%20in%0Apractical%2C%20large-scale%20tasks.%20Despite%20demonstrations%20of%20significant%20merits%20such%0Aas%20improved%20robustness%20and%20resilience%20to%20unseen%20or%20out-of-distribution%20inputs%0Aover%20their%20non-%20Bayesian%20counterparts%2C%20their%20practical%20use%20has%20faded%20to%20near%0Ainsignificance.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20framework%20to%20mitigate%0Athe%20computational%20burden%20of%20Bayesian%20neural%20networks%20%28BNNs%29.%20Our%20approach%0Afollows%20the%20principle%20of%20Bayesian%20techniques%20based%20on%20deep%20ensembles%2C%20but%0Asignificantly%20reduces%20their%20cost%20via%20multiple%20low-rank%20perturbations%20of%0Aparameters%20arising%20from%20a%20pre-trained%20neural%20network.%20Both%20vanilla%20version%20of%0Aensembles%20as%20well%20as%20more%20sophisticated%20schemes%20such%20as%20Bayesian%20learning%20with%0AStein%20Variational%20Gradient%20Descent%20%28SVGD%29%2C%20previously%20deemed%20impractical%20for%0Alarge%20models%2C%20can%20be%20seamlessly%20implemented%20within%20the%20proposed%20framework%2C%0Acalled%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29.%20In%20a%20nutshell%2C%20i%29%20Bella%20achieves%20a%0Adramatic%20reduction%20in%20the%20number%20of%20trainable%20parameters%20required%20to%0Aapproximate%20a%20Bayesian%20posterior%3B%20and%20ii%29%20it%20not%20only%20maintains%2C%20but%20in%20some%0Ainstances%2C%20surpasses%20the%20performance%20of%20conventional%20Bayesian%20learning%20methods%0Aand%20non-Bayesian%20baselines.%20Our%20results%20with%20large-scale%20tasks%20such%20as%0AImageNet%2C%20CAMELYON17%2C%20DomainNet%2C%20VQA%20with%20CLIP%2C%20LLaVA%20demonstrate%20the%0Aeffectiveness%20and%20versatility%20of%20Bella%20in%20building%20highly%20scalable%20and%0Apractical%20Bayesian%20deep%20models%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Low-Rank%2520LeArning%2520%2528Bella%2529%253A%2520A%2520Practical%2520Approach%2520to%2520Bayesian%250A%2520%2520Neural%2520Networks%26entry.906535625%3DBao%2520Gia%2520Doan%2520and%2520Afshar%2520Shamsi%2520and%2520Xiao-Yu%2520Guo%2520and%2520Arash%2520Mohammadi%2520and%2520Hamid%2520Alinejad-Rokny%2520and%2520Dino%2520Sejdinovic%2520and%2520Damith%2520C.%2520Ranasinghe%2520and%2520Ehsan%2520Abbasnejad%26entry.1292438233%3D%2520%2520Computational%2520complexity%2520of%2520Bayesian%2520learning%2520is%2520impeding%2520its%2520adoption%2520in%250Apractical%252C%2520large-scale%2520tasks.%2520Despite%2520demonstrations%2520of%2520significant%2520merits%2520such%250Aas%2520improved%2520robustness%2520and%2520resilience%2520to%2520unseen%2520or%2520out-of-distribution%2520inputs%250Aover%2520their%2520non-%2520Bayesian%2520counterparts%252C%2520their%2520practical%2520use%2520has%2520faded%2520to%2520near%250Ainsignificance.%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%2520innovative%2520framework%2520to%2520mitigate%250Athe%2520computational%2520burden%2520of%2520Bayesian%2520neural%2520networks%2520%2528BNNs%2529.%2520Our%2520approach%250Afollows%2520the%2520principle%2520of%2520Bayesian%2520techniques%2520based%2520on%2520deep%2520ensembles%252C%2520but%250Asignificantly%2520reduces%2520their%2520cost%2520via%2520multiple%2520low-rank%2520perturbations%2520of%250Aparameters%2520arising%2520from%2520a%2520pre-trained%2520neural%2520network.%2520Both%2520vanilla%2520version%2520of%250Aensembles%2520as%2520well%2520as%2520more%2520sophisticated%2520schemes%2520such%2520as%2520Bayesian%2520learning%2520with%250AStein%2520Variational%2520Gradient%2520Descent%2520%2528SVGD%2529%252C%2520previously%2520deemed%2520impractical%2520for%250Alarge%2520models%252C%2520can%2520be%2520seamlessly%2520implemented%2520within%2520the%2520proposed%2520framework%252C%250Acalled%2520Bayesian%2520Low-Rank%2520LeArning%2520%2528Bella%2529.%2520In%2520a%2520nutshell%252C%2520i%2529%2520Bella%2520achieves%2520a%250Adramatic%2520reduction%2520in%2520the%2520number%2520of%2520trainable%2520parameters%2520required%2520to%250Aapproximate%2520a%2520Bayesian%2520posterior%253B%2520and%2520ii%2529%2520it%2520not%2520only%2520maintains%252C%2520but%2520in%2520some%250Ainstances%252C%2520surpasses%2520the%2520performance%2520of%2520conventional%2520Bayesian%2520learning%2520methods%250Aand%2520non-Bayesian%2520baselines.%2520Our%2520results%2520with%2520large-scale%2520tasks%2520such%2520as%250AImageNet%252C%2520CAMELYON17%252C%2520DomainNet%252C%2520VQA%2520with%2520CLIP%252C%2520LLaVA%2520demonstrate%2520the%250Aeffectiveness%2520and%2520versatility%2520of%2520Bella%2520in%2520building%2520highly%2520scalable%2520and%250Apractical%2520Bayesian%2520deep%2520models%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Low-Rank%20LeArning%20%28Bella%29%3A%20A%20Practical%20Approach%20to%20Bayesian%0A%20%20Neural%20Networks&entry.906535625=Bao%20Gia%20Doan%20and%20Afshar%20Shamsi%20and%20Xiao-Yu%20Guo%20and%20Arash%20Mohammadi%20and%20Hamid%20Alinejad-Rokny%20and%20Dino%20Sejdinovic%20and%20Damith%20C.%20Ranasinghe%20and%20Ehsan%20Abbasnejad&entry.1292438233=%20%20Computational%20complexity%20of%20Bayesian%20learning%20is%20impeding%20its%20adoption%20in%0Apractical%2C%20large-scale%20tasks.%20Despite%20demonstrations%20of%20significant%20merits%20such%0Aas%20improved%20robustness%20and%20resilience%20to%20unseen%20or%20out-of-distribution%20inputs%0Aover%20their%20non-%20Bayesian%20counterparts%2C%20their%20practical%20use%20has%20faded%20to%20near%0Ainsignificance.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20framework%20to%20mitigate%0Athe%20computational%20burden%20of%20Bayesian%20neural%20networks%20%28BNNs%29.%20Our%20approach%0Afollows%20the%20principle%20of%20Bayesian%20techniques%20based%20on%20deep%20ensembles%2C%20but%0Asignificantly%20reduces%20their%20cost%20via%20multiple%20low-rank%20perturbations%20of%0Aparameters%20arising%20from%20a%20pre-trained%20neural%20network.%20Both%20vanilla%20version%20of%0Aensembles%20as%20well%20as%20more%20sophisticated%20schemes%20such%20as%20Bayesian%20learning%20with%0AStein%20Variational%20Gradient%20Descent%20%28SVGD%29%2C%20previously%20deemed%20impractical%20for%0Alarge%20models%2C%20can%20be%20seamlessly%20implemented%20within%20the%20proposed%20framework%2C%0Acalled%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29.%20In%20a%20nutshell%2C%20i%29%20Bella%20achieves%20a%0Adramatic%20reduction%20in%20the%20number%20of%20trainable%20parameters%20required%20to%0Aapproximate%20a%20Bayesian%20posterior%3B%20and%20ii%29%20it%20not%20only%20maintains%2C%20but%20in%20some%0Ainstances%2C%20surpasses%20the%20performance%20of%20conventional%20Bayesian%20learning%20methods%0Aand%20non-Bayesian%20baselines.%20Our%20results%20with%20large-scale%20tasks%20such%20as%0AImageNet%2C%20CAMELYON17%2C%20DomainNet%2C%20VQA%20with%20CLIP%2C%20LLaVA%20demonstrate%20the%0Aeffectiveness%20and%20versatility%20of%20Bella%20in%20building%20highly%20scalable%20and%0Apractical%20Bayesian%20deep%20models%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20891v1&entry.124074799=Read"},
{"title": "Improving PINNs By Algebraic Inclusion of Boundary and Initial\n  Conditions", "author": "Mohan Ren and Zhihao Fang and Keren Li and Anirbit Mukherjee", "abstract": "  \"AI for Science\" aims to solve fundamental scientific problems using AI\ntechniques. As most physical phenomena can be described as Partial Differential\nEquations (PDEs) , approximating their solutions using neural networks has\nevolved as a central component of scientific-ML. Physics-Informed Neural\nNetworks (PINNs) is the general method that has evolved for this task but its\ntraining is well-known to be very unstable. In this work we explore the\npossibility of changing the model being trained from being just a neural\nnetwork to being a non-linear transformation of it - one that algebraically\nincludes the boundary/initial conditions. This reduces the number of terms in\nthe loss function than the standard PINN losses. We demonstrate that our\nmodification leads to significant performance gains across a range of benchmark\ntasks, in various dimensions and without having to tweak the training\nalgorithm. Our conclusions are based on conducting hundreds of experiments, in\nthe fully unsupervised setting, over multiple linear and non-linear PDEs set to\nexactly solvable scenarios, which lends to a concrete measurement of our\nperformance gains in terms of order(s) of magnitude lower fractional errors\nbeing achieved, than by standard PINNs. The code accompanying this manuscript\nis publicly available at,\nhttps://github.com/MorganREN/Improving-PINNs-By-Algebraic-Inclusion-of-Boundary-and-Initial-Conditions\n", "link": "http://arxiv.org/abs/2407.20741v1", "date": "2024-07-30", "relevancy": 1.9889, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20PINNs%20By%20Algebraic%20Inclusion%20of%20Boundary%20and%20Initial%0A%20%20Conditions&body=Title%3A%20Improving%20PINNs%20By%20Algebraic%20Inclusion%20of%20Boundary%20and%20Initial%0A%20%20Conditions%0AAuthor%3A%20Mohan%20Ren%20and%20Zhihao%20Fang%20and%20Keren%20Li%20and%20Anirbit%20Mukherjee%0AAbstract%3A%20%20%20%22AI%20for%20Science%22%20aims%20to%20solve%20fundamental%20scientific%20problems%20using%20AI%0Atechniques.%20As%20most%20physical%20phenomena%20can%20be%20described%20as%20Partial%20Differential%0AEquations%20%28PDEs%29%20%2C%20approximating%20their%20solutions%20using%20neural%20networks%20has%0Aevolved%20as%20a%20central%20component%20of%20scientific-ML.%20Physics-Informed%20Neural%0ANetworks%20%28PINNs%29%20is%20the%20general%20method%20that%20has%20evolved%20for%20this%20task%20but%20its%0Atraining%20is%20well-known%20to%20be%20very%20unstable.%20In%20this%20work%20we%20explore%20the%0Apossibility%20of%20changing%20the%20model%20being%20trained%20from%20being%20just%20a%20neural%0Anetwork%20to%20being%20a%20non-linear%20transformation%20of%20it%20-%20one%20that%20algebraically%0Aincludes%20the%20boundary/initial%20conditions.%20This%20reduces%20the%20number%20of%20terms%20in%0Athe%20loss%20function%20than%20the%20standard%20PINN%20losses.%20We%20demonstrate%20that%20our%0Amodification%20leads%20to%20significant%20performance%20gains%20across%20a%20range%20of%20benchmark%0Atasks%2C%20in%20various%20dimensions%20and%20without%20having%20to%20tweak%20the%20training%0Aalgorithm.%20Our%20conclusions%20are%20based%20on%20conducting%20hundreds%20of%20experiments%2C%20in%0Athe%20fully%20unsupervised%20setting%2C%20over%20multiple%20linear%20and%20non-linear%20PDEs%20set%20to%0Aexactly%20solvable%20scenarios%2C%20which%20lends%20to%20a%20concrete%20measurement%20of%20our%0Aperformance%20gains%20in%20terms%20of%20order%28s%29%20of%20magnitude%20lower%20fractional%20errors%0Abeing%20achieved%2C%20than%20by%20standard%20PINNs.%20The%20code%20accompanying%20this%20manuscript%0Ais%20publicly%20available%20at%2C%0Ahttps%3A//github.com/MorganREN/Improving-PINNs-By-Algebraic-Inclusion-of-Boundary-and-Initial-Conditions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520PINNs%2520By%2520Algebraic%2520Inclusion%2520of%2520Boundary%2520and%2520Initial%250A%2520%2520Conditions%26entry.906535625%3DMohan%2520Ren%2520and%2520Zhihao%2520Fang%2520and%2520Keren%2520Li%2520and%2520Anirbit%2520Mukherjee%26entry.1292438233%3D%2520%2520%2522AI%2520for%2520Science%2522%2520aims%2520to%2520solve%2520fundamental%2520scientific%2520problems%2520using%2520AI%250Atechniques.%2520As%2520most%2520physical%2520phenomena%2520can%2520be%2520described%2520as%2520Partial%2520Differential%250AEquations%2520%2528PDEs%2529%2520%252C%2520approximating%2520their%2520solutions%2520using%2520neural%2520networks%2520has%250Aevolved%2520as%2520a%2520central%2520component%2520of%2520scientific-ML.%2520Physics-Informed%2520Neural%250ANetworks%2520%2528PINNs%2529%2520is%2520the%2520general%2520method%2520that%2520has%2520evolved%2520for%2520this%2520task%2520but%2520its%250Atraining%2520is%2520well-known%2520to%2520be%2520very%2520unstable.%2520In%2520this%2520work%2520we%2520explore%2520the%250Apossibility%2520of%2520changing%2520the%2520model%2520being%2520trained%2520from%2520being%2520just%2520a%2520neural%250Anetwork%2520to%2520being%2520a%2520non-linear%2520transformation%2520of%2520it%2520-%2520one%2520that%2520algebraically%250Aincludes%2520the%2520boundary/initial%2520conditions.%2520This%2520reduces%2520the%2520number%2520of%2520terms%2520in%250Athe%2520loss%2520function%2520than%2520the%2520standard%2520PINN%2520losses.%2520We%2520demonstrate%2520that%2520our%250Amodification%2520leads%2520to%2520significant%2520performance%2520gains%2520across%2520a%2520range%2520of%2520benchmark%250Atasks%252C%2520in%2520various%2520dimensions%2520and%2520without%2520having%2520to%2520tweak%2520the%2520training%250Aalgorithm.%2520Our%2520conclusions%2520are%2520based%2520on%2520conducting%2520hundreds%2520of%2520experiments%252C%2520in%250Athe%2520fully%2520unsupervised%2520setting%252C%2520over%2520multiple%2520linear%2520and%2520non-linear%2520PDEs%2520set%2520to%250Aexactly%2520solvable%2520scenarios%252C%2520which%2520lends%2520to%2520a%2520concrete%2520measurement%2520of%2520our%250Aperformance%2520gains%2520in%2520terms%2520of%2520order%2528s%2529%2520of%2520magnitude%2520lower%2520fractional%2520errors%250Abeing%2520achieved%252C%2520than%2520by%2520standard%2520PINNs.%2520The%2520code%2520accompanying%2520this%2520manuscript%250Ais%2520publicly%2520available%2520at%252C%250Ahttps%253A//github.com/MorganREN/Improving-PINNs-By-Algebraic-Inclusion-of-Boundary-and-Initial-Conditions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20PINNs%20By%20Algebraic%20Inclusion%20of%20Boundary%20and%20Initial%0A%20%20Conditions&entry.906535625=Mohan%20Ren%20and%20Zhihao%20Fang%20and%20Keren%20Li%20and%20Anirbit%20Mukherjee&entry.1292438233=%20%20%22AI%20for%20Science%22%20aims%20to%20solve%20fundamental%20scientific%20problems%20using%20AI%0Atechniques.%20As%20most%20physical%20phenomena%20can%20be%20described%20as%20Partial%20Differential%0AEquations%20%28PDEs%29%20%2C%20approximating%20their%20solutions%20using%20neural%20networks%20has%0Aevolved%20as%20a%20central%20component%20of%20scientific-ML.%20Physics-Informed%20Neural%0ANetworks%20%28PINNs%29%20is%20the%20general%20method%20that%20has%20evolved%20for%20this%20task%20but%20its%0Atraining%20is%20well-known%20to%20be%20very%20unstable.%20In%20this%20work%20we%20explore%20the%0Apossibility%20of%20changing%20the%20model%20being%20trained%20from%20being%20just%20a%20neural%0Anetwork%20to%20being%20a%20non-linear%20transformation%20of%20it%20-%20one%20that%20algebraically%0Aincludes%20the%20boundary/initial%20conditions.%20This%20reduces%20the%20number%20of%20terms%20in%0Athe%20loss%20function%20than%20the%20standard%20PINN%20losses.%20We%20demonstrate%20that%20our%0Amodification%20leads%20to%20significant%20performance%20gains%20across%20a%20range%20of%20benchmark%0Atasks%2C%20in%20various%20dimensions%20and%20without%20having%20to%20tweak%20the%20training%0Aalgorithm.%20Our%20conclusions%20are%20based%20on%20conducting%20hundreds%20of%20experiments%2C%20in%0Athe%20fully%20unsupervised%20setting%2C%20over%20multiple%20linear%20and%20non-linear%20PDEs%20set%20to%0Aexactly%20solvable%20scenarios%2C%20which%20lends%20to%20a%20concrete%20measurement%20of%20our%0Aperformance%20gains%20in%20terms%20of%20order%28s%29%20of%20magnitude%20lower%20fractional%20errors%0Abeing%20achieved%2C%20than%20by%20standard%20PINNs.%20The%20code%20accompanying%20this%20manuscript%0Ais%20publicly%20available%20at%2C%0Ahttps%3A//github.com/MorganREN/Improving-PINNs-By-Algebraic-Inclusion-of-Boundary-and-Initial-Conditions%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20741v1&entry.124074799=Read"},
{"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised\n  Knowledge Distillation", "author": "Marco Mistretta and Alberto Baldrati and Marco Bertini and Andrew D. Bagdanov", "abstract": "  Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization\nto unseen tasks, but fall short of the performance of supervised methods in\ngeneralizing to downstream tasks with limited data. Prompt learning is emerging\nas a parameter-efficient method for adapting VLMs, but state-of-the-art\napproaches require annotated samples. In this paper we propose a novel approach\nto prompt learning based on unsupervised knowledge distillation from more\npowerful models. Our approach, which we call Knowledge Distillation Prompt\nLearning (KDPL), can be integrated into existing prompt learning techniques and\neliminates the need for labeled examples during adaptation. Our experiments on\nmore than ten standard benchmark datasets demonstrate that KDPL is very\neffective at improving generalization of learned prompts for zero-shot domain\ngeneralization, zero-shot cross-dataset generalization, and zero-shot\nbase-to-novel class generalization problems. KDPL requires no ground-truth\nlabels for adaptation, and moreover we show that even in the absence of any\nknowledge of training class names it can be used to effectively transfer\nknowledge. The code is publicly available at https://github.com/miccunifi/KDPL.\n", "link": "http://arxiv.org/abs/2407.03056v2", "date": "2024-07-30", "relevancy": 1.9821, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4925}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Zero-shot%20Generalization%20of%20Learned%20Prompts%20via%20Unsupervised%0A%20%20Knowledge%20Distillation&body=Title%3A%20Improving%20Zero-shot%20Generalization%20of%20Learned%20Prompts%20via%20Unsupervised%0A%20%20Knowledge%20Distillation%0AAuthor%3A%20Marco%20Mistretta%20and%20Alberto%20Baldrati%20and%20Marco%20Bertini%20and%20Andrew%20D.%20Bagdanov%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20remarkable%20zero-shot%20generalization%0Ato%20unseen%20tasks%2C%20but%20fall%20short%20of%20the%20performance%20of%20supervised%20methods%20in%0Ageneralizing%20to%20downstream%20tasks%20with%20limited%20data.%20Prompt%20learning%20is%20emerging%0Aas%20a%20parameter-efficient%20method%20for%20adapting%20VLMs%2C%20but%20state-of-the-art%0Aapproaches%20require%20annotated%20samples.%20In%20this%20paper%20we%20propose%20a%20novel%20approach%0Ato%20prompt%20learning%20based%20on%20unsupervised%20knowledge%20distillation%20from%20more%0Apowerful%20models.%20Our%20approach%2C%20which%20we%20call%20Knowledge%20Distillation%20Prompt%0ALearning%20%28KDPL%29%2C%20can%20be%20integrated%20into%20existing%20prompt%20learning%20techniques%20and%0Aeliminates%20the%20need%20for%20labeled%20examples%20during%20adaptation.%20Our%20experiments%20on%0Amore%20than%20ten%20standard%20benchmark%20datasets%20demonstrate%20that%20KDPL%20is%20very%0Aeffective%20at%20improving%20generalization%20of%20learned%20prompts%20for%20zero-shot%20domain%0Ageneralization%2C%20zero-shot%20cross-dataset%20generalization%2C%20and%20zero-shot%0Abase-to-novel%20class%20generalization%20problems.%20KDPL%20requires%20no%20ground-truth%0Alabels%20for%20adaptation%2C%20and%20moreover%20we%20show%20that%20even%20in%20the%20absence%20of%20any%0Aknowledge%20of%20training%20class%20names%20it%20can%20be%20used%20to%20effectively%20transfer%0Aknowledge.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/miccunifi/KDPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Zero-shot%2520Generalization%2520of%2520Learned%2520Prompts%2520via%2520Unsupervised%250A%2520%2520Knowledge%2520Distillation%26entry.906535625%3DMarco%2520Mistretta%2520and%2520Alberto%2520Baldrati%2520and%2520Marco%2520Bertini%2520and%2520Andrew%2520D.%2520Bagdanov%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520remarkable%2520zero-shot%2520generalization%250Ato%2520unseen%2520tasks%252C%2520but%2520fall%2520short%2520of%2520the%2520performance%2520of%2520supervised%2520methods%2520in%250Ageneralizing%2520to%2520downstream%2520tasks%2520with%2520limited%2520data.%2520Prompt%2520learning%2520is%2520emerging%250Aas%2520a%2520parameter-efficient%2520method%2520for%2520adapting%2520VLMs%252C%2520but%2520state-of-the-art%250Aapproaches%2520require%2520annotated%2520samples.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520novel%2520approach%250Ato%2520prompt%2520learning%2520based%2520on%2520unsupervised%2520knowledge%2520distillation%2520from%2520more%250Apowerful%2520models.%2520Our%2520approach%252C%2520which%2520we%2520call%2520Knowledge%2520Distillation%2520Prompt%250ALearning%2520%2528KDPL%2529%252C%2520can%2520be%2520integrated%2520into%2520existing%2520prompt%2520learning%2520techniques%2520and%250Aeliminates%2520the%2520need%2520for%2520labeled%2520examples%2520during%2520adaptation.%2520Our%2520experiments%2520on%250Amore%2520than%2520ten%2520standard%2520benchmark%2520datasets%2520demonstrate%2520that%2520KDPL%2520is%2520very%250Aeffective%2520at%2520improving%2520generalization%2520of%2520learned%2520prompts%2520for%2520zero-shot%2520domain%250Ageneralization%252C%2520zero-shot%2520cross-dataset%2520generalization%252C%2520and%2520zero-shot%250Abase-to-novel%2520class%2520generalization%2520problems.%2520KDPL%2520requires%2520no%2520ground-truth%250Alabels%2520for%2520adaptation%252C%2520and%2520moreover%2520we%2520show%2520that%2520even%2520in%2520the%2520absence%2520of%2520any%250Aknowledge%2520of%2520training%2520class%2520names%2520it%2520can%2520be%2520used%2520to%2520effectively%2520transfer%250Aknowledge.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/miccunifi/KDPL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Zero-shot%20Generalization%20of%20Learned%20Prompts%20via%20Unsupervised%0A%20%20Knowledge%20Distillation&entry.906535625=Marco%20Mistretta%20and%20Alberto%20Baldrati%20and%20Marco%20Bertini%20and%20Andrew%20D.%20Bagdanov&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20remarkable%20zero-shot%20generalization%0Ato%20unseen%20tasks%2C%20but%20fall%20short%20of%20the%20performance%20of%20supervised%20methods%20in%0Ageneralizing%20to%20downstream%20tasks%20with%20limited%20data.%20Prompt%20learning%20is%20emerging%0Aas%20a%20parameter-efficient%20method%20for%20adapting%20VLMs%2C%20but%20state-of-the-art%0Aapproaches%20require%20annotated%20samples.%20In%20this%20paper%20we%20propose%20a%20novel%20approach%0Ato%20prompt%20learning%20based%20on%20unsupervised%20knowledge%20distillation%20from%20more%0Apowerful%20models.%20Our%20approach%2C%20which%20we%20call%20Knowledge%20Distillation%20Prompt%0ALearning%20%28KDPL%29%2C%20can%20be%20integrated%20into%20existing%20prompt%20learning%20techniques%20and%0Aeliminates%20the%20need%20for%20labeled%20examples%20during%20adaptation.%20Our%20experiments%20on%0Amore%20than%20ten%20standard%20benchmark%20datasets%20demonstrate%20that%20KDPL%20is%20very%0Aeffective%20at%20improving%20generalization%20of%20learned%20prompts%20for%20zero-shot%20domain%0Ageneralization%2C%20zero-shot%20cross-dataset%20generalization%2C%20and%20zero-shot%0Abase-to-novel%20class%20generalization%20problems.%20KDPL%20requires%20no%20ground-truth%0Alabels%20for%20adaptation%2C%20and%20moreover%20we%20show%20that%20even%20in%20the%20absence%20of%20any%0Aknowledge%20of%20training%20class%20names%20it%20can%20be%20used%20to%20effectively%20transfer%0Aknowledge.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/miccunifi/KDPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03056v2&entry.124074799=Read"},
{"title": "Neural Fields for Continuous Periodic Motion Estimation in 4D\n  Cardiovascular Imaging", "author": "Simone Garzia and Patryk Rygiel and Sven Dummer and Filippo Cademartiri and Simona Celi and Jelmer M. Wolterink", "abstract": "  Time-resolved three-dimensional flow MRI (4D flow MRI) provides a unique\nnon-invasive solution to visualize and quantify hemodynamics in blood vessels\nsuch as the aortic arch. However, most current analysis methods for arterial 4D\nflow MRI use static artery walls because of the difficulty in obtaining a full\ncycle segmentation. To overcome this limitation, we propose a neural\nfields-based method that directly estimates continuous periodic wall\ndeformations throughout the cardiac cycle. For a 3D + time imaging dataset, we\noptimize an implicit neural representation (INR) that represents a\ntime-dependent velocity vector field (VVF). An ODE solver is used to integrate\nthe VVF into a deformation vector field (DVF), that can deform images,\nsegmentation masks, or meshes over time, thereby visualizing and quantifying\nlocal wall motion patterns. To properly reflect the periodic nature of 3D +\ntime cardiovascular data, we impose periodicity in two ways. First, by\nperiodically encoding the time input to the INR, and hence VVF. Second, by\nregularizing the DVF. We demonstrate the effectiveness of this approach on\nsynthetic data with different periodic patterns, ECG-gated CT, and 4D flow MRI\ndata. The obtained method could be used to improve 4D flow MRI analysis.\n", "link": "http://arxiv.org/abs/2407.20728v1", "date": "2024-07-30", "relevancy": 1.9791, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5148}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4932}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Fields%20for%20Continuous%20Periodic%20Motion%20Estimation%20in%204D%0A%20%20Cardiovascular%20Imaging&body=Title%3A%20Neural%20Fields%20for%20Continuous%20Periodic%20Motion%20Estimation%20in%204D%0A%20%20Cardiovascular%20Imaging%0AAuthor%3A%20Simone%20Garzia%20and%20Patryk%20Rygiel%20and%20Sven%20Dummer%20and%20Filippo%20Cademartiri%20and%20Simona%20Celi%20and%20Jelmer%20M.%20Wolterink%0AAbstract%3A%20%20%20Time-resolved%20three-dimensional%20flow%20MRI%20%284D%20flow%20MRI%29%20provides%20a%20unique%0Anon-invasive%20solution%20to%20visualize%20and%20quantify%20hemodynamics%20in%20blood%20vessels%0Asuch%20as%20the%20aortic%20arch.%20However%2C%20most%20current%20analysis%20methods%20for%20arterial%204D%0Aflow%20MRI%20use%20static%20artery%20walls%20because%20of%20the%20difficulty%20in%20obtaining%20a%20full%0Acycle%20segmentation.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20neural%0Afields-based%20method%20that%20directly%20estimates%20continuous%20periodic%20wall%0Adeformations%20throughout%20the%20cardiac%20cycle.%20For%20a%203D%20%2B%20time%20imaging%20dataset%2C%20we%0Aoptimize%20an%20implicit%20neural%20representation%20%28INR%29%20that%20represents%20a%0Atime-dependent%20velocity%20vector%20field%20%28VVF%29.%20An%20ODE%20solver%20is%20used%20to%20integrate%0Athe%20VVF%20into%20a%20deformation%20vector%20field%20%28DVF%29%2C%20that%20can%20deform%20images%2C%0Asegmentation%20masks%2C%20or%20meshes%20over%20time%2C%20thereby%20visualizing%20and%20quantifying%0Alocal%20wall%20motion%20patterns.%20To%20properly%20reflect%20the%20periodic%20nature%20of%203D%20%2B%0Atime%20cardiovascular%20data%2C%20we%20impose%20periodicity%20in%20two%20ways.%20First%2C%20by%0Aperiodically%20encoding%20the%20time%20input%20to%20the%20INR%2C%20and%20hence%20VVF.%20Second%2C%20by%0Aregularizing%20the%20DVF.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%20on%0Asynthetic%20data%20with%20different%20periodic%20patterns%2C%20ECG-gated%20CT%2C%20and%204D%20flow%20MRI%0Adata.%20The%20obtained%20method%20could%20be%20used%20to%20improve%204D%20flow%20MRI%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Fields%2520for%2520Continuous%2520Periodic%2520Motion%2520Estimation%2520in%25204D%250A%2520%2520Cardiovascular%2520Imaging%26entry.906535625%3DSimone%2520Garzia%2520and%2520Patryk%2520Rygiel%2520and%2520Sven%2520Dummer%2520and%2520Filippo%2520Cademartiri%2520and%2520Simona%2520Celi%2520and%2520Jelmer%2520M.%2520Wolterink%26entry.1292438233%3D%2520%2520Time-resolved%2520three-dimensional%2520flow%2520MRI%2520%25284D%2520flow%2520MRI%2529%2520provides%2520a%2520unique%250Anon-invasive%2520solution%2520to%2520visualize%2520and%2520quantify%2520hemodynamics%2520in%2520blood%2520vessels%250Asuch%2520as%2520the%2520aortic%2520arch.%2520However%252C%2520most%2520current%2520analysis%2520methods%2520for%2520arterial%25204D%250Aflow%2520MRI%2520use%2520static%2520artery%2520walls%2520because%2520of%2520the%2520difficulty%2520in%2520obtaining%2520a%2520full%250Acycle%2520segmentation.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520neural%250Afields-based%2520method%2520that%2520directly%2520estimates%2520continuous%2520periodic%2520wall%250Adeformations%2520throughout%2520the%2520cardiac%2520cycle.%2520For%2520a%25203D%2520%252B%2520time%2520imaging%2520dataset%252C%2520we%250Aoptimize%2520an%2520implicit%2520neural%2520representation%2520%2528INR%2529%2520that%2520represents%2520a%250Atime-dependent%2520velocity%2520vector%2520field%2520%2528VVF%2529.%2520An%2520ODE%2520solver%2520is%2520used%2520to%2520integrate%250Athe%2520VVF%2520into%2520a%2520deformation%2520vector%2520field%2520%2528DVF%2529%252C%2520that%2520can%2520deform%2520images%252C%250Asegmentation%2520masks%252C%2520or%2520meshes%2520over%2520time%252C%2520thereby%2520visualizing%2520and%2520quantifying%250Alocal%2520wall%2520motion%2520patterns.%2520To%2520properly%2520reflect%2520the%2520periodic%2520nature%2520of%25203D%2520%252B%250Atime%2520cardiovascular%2520data%252C%2520we%2520impose%2520periodicity%2520in%2520two%2520ways.%2520First%252C%2520by%250Aperiodically%2520encoding%2520the%2520time%2520input%2520to%2520the%2520INR%252C%2520and%2520hence%2520VVF.%2520Second%252C%2520by%250Aregularizing%2520the%2520DVF.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%2520on%250Asynthetic%2520data%2520with%2520different%2520periodic%2520patterns%252C%2520ECG-gated%2520CT%252C%2520and%25204D%2520flow%2520MRI%250Adata.%2520The%2520obtained%2520method%2520could%2520be%2520used%2520to%2520improve%25204D%2520flow%2520MRI%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Fields%20for%20Continuous%20Periodic%20Motion%20Estimation%20in%204D%0A%20%20Cardiovascular%20Imaging&entry.906535625=Simone%20Garzia%20and%20Patryk%20Rygiel%20and%20Sven%20Dummer%20and%20Filippo%20Cademartiri%20and%20Simona%20Celi%20and%20Jelmer%20M.%20Wolterink&entry.1292438233=%20%20Time-resolved%20three-dimensional%20flow%20MRI%20%284D%20flow%20MRI%29%20provides%20a%20unique%0Anon-invasive%20solution%20to%20visualize%20and%20quantify%20hemodynamics%20in%20blood%20vessels%0Asuch%20as%20the%20aortic%20arch.%20However%2C%20most%20current%20analysis%20methods%20for%20arterial%204D%0Aflow%20MRI%20use%20static%20artery%20walls%20because%20of%20the%20difficulty%20in%20obtaining%20a%20full%0Acycle%20segmentation.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20neural%0Afields-based%20method%20that%20directly%20estimates%20continuous%20periodic%20wall%0Adeformations%20throughout%20the%20cardiac%20cycle.%20For%20a%203D%20%2B%20time%20imaging%20dataset%2C%20we%0Aoptimize%20an%20implicit%20neural%20representation%20%28INR%29%20that%20represents%20a%0Atime-dependent%20velocity%20vector%20field%20%28VVF%29.%20An%20ODE%20solver%20is%20used%20to%20integrate%0Athe%20VVF%20into%20a%20deformation%20vector%20field%20%28DVF%29%2C%20that%20can%20deform%20images%2C%0Asegmentation%20masks%2C%20or%20meshes%20over%20time%2C%20thereby%20visualizing%20and%20quantifying%0Alocal%20wall%20motion%20patterns.%20To%20properly%20reflect%20the%20periodic%20nature%20of%203D%20%2B%0Atime%20cardiovascular%20data%2C%20we%20impose%20periodicity%20in%20two%20ways.%20First%2C%20by%0Aperiodically%20encoding%20the%20time%20input%20to%20the%20INR%2C%20and%20hence%20VVF.%20Second%2C%20by%0Aregularizing%20the%20DVF.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%20on%0Asynthetic%20data%20with%20different%20periodic%20patterns%2C%20ECG-gated%20CT%2C%20and%204D%20flow%20MRI%0Adata.%20The%20obtained%20method%20could%20be%20used%20to%20improve%204D%20flow%20MRI%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20728v1&entry.124074799=Read"},
{"title": "Interpretable Pre-Trained Transformers for Heart Time-Series Data", "author": "Harry J. Davies and James Monsen and Danilo P. Mandic", "abstract": "  Decoder-only transformers are the backbone of the popular generative\npre-trained transformer (GPT) series of large language models. In this work, we\napply the same framework to periodic heart time-series data to create two\npre-trained general purpose cardiac models, namely PPG-PT and ECG-PT. We\ndemonstrate that both such pre-trained models are fully interpretable. This is\nachieved firstly through aggregate attention maps which show that the model\nfocuses on similar points in previous cardiac cycles in order to make\npredictions and gradually broadens its attention in deeper layers. Next, tokens\nwith the same value, that occur at different distinct points in the ECG and PPG\ncycle, form separate clusters in high dimensional space based on their phase as\nthey propagate through the transformer blocks. Finally, we highlight that\nindividual attention heads respond to specific physiologically relevent\nfeatures, such as the dicrotic notch in PPG and the P-wave in ECG. It is also\ndemonstrated that these pre-trained models can be easily fine-tuned for tasks\nsuch as classification of atrial fibrillation. In this specific example, the\nfine-tuning took 11 minutes of computer time, and achieved a\nleave-one-subject-out AUCs of 0.99 and 0.93 for ECG and PPG respectively.\nImportantly, these fine-tuned models are also fully explainable, with attention\nshifting to regions in the context that are strongly indicative of atrial\nfibrillation.\n", "link": "http://arxiv.org/abs/2407.20775v1", "date": "2024-07-30", "relevancy": 1.9753, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5519}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4933}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Pre-Trained%20Transformers%20for%20Heart%20Time-Series%20Data&body=Title%3A%20Interpretable%20Pre-Trained%20Transformers%20for%20Heart%20Time-Series%20Data%0AAuthor%3A%20Harry%20J.%20Davies%20and%20James%20Monsen%20and%20Danilo%20P.%20Mandic%0AAbstract%3A%20%20%20Decoder-only%20transformers%20are%20the%20backbone%20of%20the%20popular%20generative%0Apre-trained%20transformer%20%28GPT%29%20series%20of%20large%20language%20models.%20In%20this%20work%2C%20we%0Aapply%20the%20same%20framework%20to%20periodic%20heart%20time-series%20data%20to%20create%20two%0Apre-trained%20general%20purpose%20cardiac%20models%2C%20namely%20PPG-PT%20and%20ECG-PT.%20We%0Ademonstrate%20that%20both%20such%20pre-trained%20models%20are%20fully%20interpretable.%20This%20is%0Aachieved%20firstly%20through%20aggregate%20attention%20maps%20which%20show%20that%20the%20model%0Afocuses%20on%20similar%20points%20in%20previous%20cardiac%20cycles%20in%20order%20to%20make%0Apredictions%20and%20gradually%20broadens%20its%20attention%20in%20deeper%20layers.%20Next%2C%20tokens%0Awith%20the%20same%20value%2C%20that%20occur%20at%20different%20distinct%20points%20in%20the%20ECG%20and%20PPG%0Acycle%2C%20form%20separate%20clusters%20in%20high%20dimensional%20space%20based%20on%20their%20phase%20as%0Athey%20propagate%20through%20the%20transformer%20blocks.%20Finally%2C%20we%20highlight%20that%0Aindividual%20attention%20heads%20respond%20to%20specific%20physiologically%20relevent%0Afeatures%2C%20such%20as%20the%20dicrotic%20notch%20in%20PPG%20and%20the%20P-wave%20in%20ECG.%20It%20is%20also%0Ademonstrated%20that%20these%20pre-trained%20models%20can%20be%20easily%20fine-tuned%20for%20tasks%0Asuch%20as%20classification%20of%20atrial%20fibrillation.%20In%20this%20specific%20example%2C%20the%0Afine-tuning%20took%2011%20minutes%20of%20computer%20time%2C%20and%20achieved%20a%0Aleave-one-subject-out%20AUCs%20of%200.99%20and%200.93%20for%20ECG%20and%20PPG%20respectively.%0AImportantly%2C%20these%20fine-tuned%20models%20are%20also%20fully%20explainable%2C%20with%20attention%0Ashifting%20to%20regions%20in%20the%20context%20that%20are%20strongly%20indicative%20of%20atrial%0Afibrillation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Pre-Trained%2520Transformers%2520for%2520Heart%2520Time-Series%2520Data%26entry.906535625%3DHarry%2520J.%2520Davies%2520and%2520James%2520Monsen%2520and%2520Danilo%2520P.%2520Mandic%26entry.1292438233%3D%2520%2520Decoder-only%2520transformers%2520are%2520the%2520backbone%2520of%2520the%2520popular%2520generative%250Apre-trained%2520transformer%2520%2528GPT%2529%2520series%2520of%2520large%2520language%2520models.%2520In%2520this%2520work%252C%2520we%250Aapply%2520the%2520same%2520framework%2520to%2520periodic%2520heart%2520time-series%2520data%2520to%2520create%2520two%250Apre-trained%2520general%2520purpose%2520cardiac%2520models%252C%2520namely%2520PPG-PT%2520and%2520ECG-PT.%2520We%250Ademonstrate%2520that%2520both%2520such%2520pre-trained%2520models%2520are%2520fully%2520interpretable.%2520This%2520is%250Aachieved%2520firstly%2520through%2520aggregate%2520attention%2520maps%2520which%2520show%2520that%2520the%2520model%250Afocuses%2520on%2520similar%2520points%2520in%2520previous%2520cardiac%2520cycles%2520in%2520order%2520to%2520make%250Apredictions%2520and%2520gradually%2520broadens%2520its%2520attention%2520in%2520deeper%2520layers.%2520Next%252C%2520tokens%250Awith%2520the%2520same%2520value%252C%2520that%2520occur%2520at%2520different%2520distinct%2520points%2520in%2520the%2520ECG%2520and%2520PPG%250Acycle%252C%2520form%2520separate%2520clusters%2520in%2520high%2520dimensional%2520space%2520based%2520on%2520their%2520phase%2520as%250Athey%2520propagate%2520through%2520the%2520transformer%2520blocks.%2520Finally%252C%2520we%2520highlight%2520that%250Aindividual%2520attention%2520heads%2520respond%2520to%2520specific%2520physiologically%2520relevent%250Afeatures%252C%2520such%2520as%2520the%2520dicrotic%2520notch%2520in%2520PPG%2520and%2520the%2520P-wave%2520in%2520ECG.%2520It%2520is%2520also%250Ademonstrated%2520that%2520these%2520pre-trained%2520models%2520can%2520be%2520easily%2520fine-tuned%2520for%2520tasks%250Asuch%2520as%2520classification%2520of%2520atrial%2520fibrillation.%2520In%2520this%2520specific%2520example%252C%2520the%250Afine-tuning%2520took%252011%2520minutes%2520of%2520computer%2520time%252C%2520and%2520achieved%2520a%250Aleave-one-subject-out%2520AUCs%2520of%25200.99%2520and%25200.93%2520for%2520ECG%2520and%2520PPG%2520respectively.%250AImportantly%252C%2520these%2520fine-tuned%2520models%2520are%2520also%2520fully%2520explainable%252C%2520with%2520attention%250Ashifting%2520to%2520regions%2520in%2520the%2520context%2520that%2520are%2520strongly%2520indicative%2520of%2520atrial%250Afibrillation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Pre-Trained%20Transformers%20for%20Heart%20Time-Series%20Data&entry.906535625=Harry%20J.%20Davies%20and%20James%20Monsen%20and%20Danilo%20P.%20Mandic&entry.1292438233=%20%20Decoder-only%20transformers%20are%20the%20backbone%20of%20the%20popular%20generative%0Apre-trained%20transformer%20%28GPT%29%20series%20of%20large%20language%20models.%20In%20this%20work%2C%20we%0Aapply%20the%20same%20framework%20to%20periodic%20heart%20time-series%20data%20to%20create%20two%0Apre-trained%20general%20purpose%20cardiac%20models%2C%20namely%20PPG-PT%20and%20ECG-PT.%20We%0Ademonstrate%20that%20both%20such%20pre-trained%20models%20are%20fully%20interpretable.%20This%20is%0Aachieved%20firstly%20through%20aggregate%20attention%20maps%20which%20show%20that%20the%20model%0Afocuses%20on%20similar%20points%20in%20previous%20cardiac%20cycles%20in%20order%20to%20make%0Apredictions%20and%20gradually%20broadens%20its%20attention%20in%20deeper%20layers.%20Next%2C%20tokens%0Awith%20the%20same%20value%2C%20that%20occur%20at%20different%20distinct%20points%20in%20the%20ECG%20and%20PPG%0Acycle%2C%20form%20separate%20clusters%20in%20high%20dimensional%20space%20based%20on%20their%20phase%20as%0Athey%20propagate%20through%20the%20transformer%20blocks.%20Finally%2C%20we%20highlight%20that%0Aindividual%20attention%20heads%20respond%20to%20specific%20physiologically%20relevent%0Afeatures%2C%20such%20as%20the%20dicrotic%20notch%20in%20PPG%20and%20the%20P-wave%20in%20ECG.%20It%20is%20also%0Ademonstrated%20that%20these%20pre-trained%20models%20can%20be%20easily%20fine-tuned%20for%20tasks%0Asuch%20as%20classification%20of%20atrial%20fibrillation.%20In%20this%20specific%20example%2C%20the%0Afine-tuning%20took%2011%20minutes%20of%20computer%20time%2C%20and%20achieved%20a%0Aleave-one-subject-out%20AUCs%20of%200.99%20and%200.93%20for%20ECG%20and%20PPG%20respectively.%0AImportantly%2C%20these%20fine-tuned%20models%20are%20also%20fully%20explainable%2C%20with%20attention%0Ashifting%20to%20regions%20in%20the%20context%20that%20are%20strongly%20indicative%20of%20atrial%0Afibrillation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20775v1&entry.124074799=Read"},
{"title": "Exploring Loss Landscapes through the Lens of Spin Glass Theory", "author": "Hao Liao and Wei Zhang and Zhanyi Huang and Zexiao Long and Mingyang Zhou and Xiaoqun Wu and Rui Mao and Chi Ho Yeung", "abstract": "  In the past decade, significant strides in deep learning have led to numerous\ngroundbreaking applications. Despite these advancements, the understanding of\nthe high generalizability of deep learning, especially in such an\nover-parametrized space, remains limited. Successful applications are often\nconsidered as empirical rather than scientific achievements. For instance, deep\nneural networks' (DNNs) internal representations, decision-making mechanism,\nabsence of overfitting in an over-parametrized space, high generalizability,\netc., remain less understood. This paper delves into the loss landscape of DNNs\nthrough the lens of spin glass in statistical physics, i.e. a system\ncharacterized by a complex energy landscape with numerous metastable states, to\nbetter understand how DNNs work. We investigated a single hidden layer\nRectified Linear Unit (ReLU) neural network model, and introduced several\nprotocols to examine the analogy between DNNs (trained with datasets including\nMNIST and CIFAR10) and spin glass. Specifically, we used (1) random walk in the\nparameter space of DNNs to unravel the structures in their loss landscape; (2)\na permutation-interpolation protocol to study the connection between copies of\nidentical regions in the loss landscape due to the permutation symmetry in the\nhidden layers; (3) hierarchical clustering to reveal the hierarchy among\ntrained solutions of DNNs, reminiscent of the so-called Replica Symmetry\nBreaking (RSB) phenomenon (i.e. the Parisi solution) in analogy to spin glass;\n(4) finally, we examine the relationship between the degree of the ruggedness\nof the loss landscape of the DNN and its generalizability, showing an\nimprovement of flattened minima.\n", "link": "http://arxiv.org/abs/2407.20724v1", "date": "2024-07-30", "relevancy": 1.9671, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4909}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Loss%20Landscapes%20through%20the%20Lens%20of%20Spin%20Glass%20Theory&body=Title%3A%20Exploring%20Loss%20Landscapes%20through%20the%20Lens%20of%20Spin%20Glass%20Theory%0AAuthor%3A%20Hao%20Liao%20and%20Wei%20Zhang%20and%20Zhanyi%20Huang%20and%20Zexiao%20Long%20and%20Mingyang%20Zhou%20and%20Xiaoqun%20Wu%20and%20Rui%20Mao%20and%20Chi%20Ho%20Yeung%0AAbstract%3A%20%20%20In%20the%20past%20decade%2C%20significant%20strides%20in%20deep%20learning%20have%20led%20to%20numerous%0Agroundbreaking%20applications.%20Despite%20these%20advancements%2C%20the%20understanding%20of%0Athe%20high%20generalizability%20of%20deep%20learning%2C%20especially%20in%20such%20an%0Aover-parametrized%20space%2C%20remains%20limited.%20Successful%20applications%20are%20often%0Aconsidered%20as%20empirical%20rather%20than%20scientific%20achievements.%20For%20instance%2C%20deep%0Aneural%20networks%27%20%28DNNs%29%20internal%20representations%2C%20decision-making%20mechanism%2C%0Aabsence%20of%20overfitting%20in%20an%20over-parametrized%20space%2C%20high%20generalizability%2C%0Aetc.%2C%20remain%20less%20understood.%20This%20paper%20delves%20into%20the%20loss%20landscape%20of%20DNNs%0Athrough%20the%20lens%20of%20spin%20glass%20in%20statistical%20physics%2C%20i.e.%20a%20system%0Acharacterized%20by%20a%20complex%20energy%20landscape%20with%20numerous%20metastable%20states%2C%20to%0Abetter%20understand%20how%20DNNs%20work.%20We%20investigated%20a%20single%20hidden%20layer%0ARectified%20Linear%20Unit%20%28ReLU%29%20neural%20network%20model%2C%20and%20introduced%20several%0Aprotocols%20to%20examine%20the%20analogy%20between%20DNNs%20%28trained%20with%20datasets%20including%0AMNIST%20and%20CIFAR10%29%20and%20spin%20glass.%20Specifically%2C%20we%20used%20%281%29%20random%20walk%20in%20the%0Aparameter%20space%20of%20DNNs%20to%20unravel%20the%20structures%20in%20their%20loss%20landscape%3B%20%282%29%0Aa%20permutation-interpolation%20protocol%20to%20study%20the%20connection%20between%20copies%20of%0Aidentical%20regions%20in%20the%20loss%20landscape%20due%20to%20the%20permutation%20symmetry%20in%20the%0Ahidden%20layers%3B%20%283%29%20hierarchical%20clustering%20to%20reveal%20the%20hierarchy%20among%0Atrained%20solutions%20of%20DNNs%2C%20reminiscent%20of%20the%20so-called%20Replica%20Symmetry%0ABreaking%20%28RSB%29%20phenomenon%20%28i.e.%20the%20Parisi%20solution%29%20in%20analogy%20to%20spin%20glass%3B%0A%284%29%20finally%2C%20we%20examine%20the%20relationship%20between%20the%20degree%20of%20the%20ruggedness%0Aof%20the%20loss%20landscape%20of%20the%20DNN%20and%20its%20generalizability%2C%20showing%20an%0Aimprovement%20of%20flattened%20minima.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Loss%2520Landscapes%2520through%2520the%2520Lens%2520of%2520Spin%2520Glass%2520Theory%26entry.906535625%3DHao%2520Liao%2520and%2520Wei%2520Zhang%2520and%2520Zhanyi%2520Huang%2520and%2520Zexiao%2520Long%2520and%2520Mingyang%2520Zhou%2520and%2520Xiaoqun%2520Wu%2520and%2520Rui%2520Mao%2520and%2520Chi%2520Ho%2520Yeung%26entry.1292438233%3D%2520%2520In%2520the%2520past%2520decade%252C%2520significant%2520strides%2520in%2520deep%2520learning%2520have%2520led%2520to%2520numerous%250Agroundbreaking%2520applications.%2520Despite%2520these%2520advancements%252C%2520the%2520understanding%2520of%250Athe%2520high%2520generalizability%2520of%2520deep%2520learning%252C%2520especially%2520in%2520such%2520an%250Aover-parametrized%2520space%252C%2520remains%2520limited.%2520Successful%2520applications%2520are%2520often%250Aconsidered%2520as%2520empirical%2520rather%2520than%2520scientific%2520achievements.%2520For%2520instance%252C%2520deep%250Aneural%2520networks%2527%2520%2528DNNs%2529%2520internal%2520representations%252C%2520decision-making%2520mechanism%252C%250Aabsence%2520of%2520overfitting%2520in%2520an%2520over-parametrized%2520space%252C%2520high%2520generalizability%252C%250Aetc.%252C%2520remain%2520less%2520understood.%2520This%2520paper%2520delves%2520into%2520the%2520loss%2520landscape%2520of%2520DNNs%250Athrough%2520the%2520lens%2520of%2520spin%2520glass%2520in%2520statistical%2520physics%252C%2520i.e.%2520a%2520system%250Acharacterized%2520by%2520a%2520complex%2520energy%2520landscape%2520with%2520numerous%2520metastable%2520states%252C%2520to%250Abetter%2520understand%2520how%2520DNNs%2520work.%2520We%2520investigated%2520a%2520single%2520hidden%2520layer%250ARectified%2520Linear%2520Unit%2520%2528ReLU%2529%2520neural%2520network%2520model%252C%2520and%2520introduced%2520several%250Aprotocols%2520to%2520examine%2520the%2520analogy%2520between%2520DNNs%2520%2528trained%2520with%2520datasets%2520including%250AMNIST%2520and%2520CIFAR10%2529%2520and%2520spin%2520glass.%2520Specifically%252C%2520we%2520used%2520%25281%2529%2520random%2520walk%2520in%2520the%250Aparameter%2520space%2520of%2520DNNs%2520to%2520unravel%2520the%2520structures%2520in%2520their%2520loss%2520landscape%253B%2520%25282%2529%250Aa%2520permutation-interpolation%2520protocol%2520to%2520study%2520the%2520connection%2520between%2520copies%2520of%250Aidentical%2520regions%2520in%2520the%2520loss%2520landscape%2520due%2520to%2520the%2520permutation%2520symmetry%2520in%2520the%250Ahidden%2520layers%253B%2520%25283%2529%2520hierarchical%2520clustering%2520to%2520reveal%2520the%2520hierarchy%2520among%250Atrained%2520solutions%2520of%2520DNNs%252C%2520reminiscent%2520of%2520the%2520so-called%2520Replica%2520Symmetry%250ABreaking%2520%2528RSB%2529%2520phenomenon%2520%2528i.e.%2520the%2520Parisi%2520solution%2529%2520in%2520analogy%2520to%2520spin%2520glass%253B%250A%25284%2529%2520finally%252C%2520we%2520examine%2520the%2520relationship%2520between%2520the%2520degree%2520of%2520the%2520ruggedness%250Aof%2520the%2520loss%2520landscape%2520of%2520the%2520DNN%2520and%2520its%2520generalizability%252C%2520showing%2520an%250Aimprovement%2520of%2520flattened%2520minima.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Loss%20Landscapes%20through%20the%20Lens%20of%20Spin%20Glass%20Theory&entry.906535625=Hao%20Liao%20and%20Wei%20Zhang%20and%20Zhanyi%20Huang%20and%20Zexiao%20Long%20and%20Mingyang%20Zhou%20and%20Xiaoqun%20Wu%20and%20Rui%20Mao%20and%20Chi%20Ho%20Yeung&entry.1292438233=%20%20In%20the%20past%20decade%2C%20significant%20strides%20in%20deep%20learning%20have%20led%20to%20numerous%0Agroundbreaking%20applications.%20Despite%20these%20advancements%2C%20the%20understanding%20of%0Athe%20high%20generalizability%20of%20deep%20learning%2C%20especially%20in%20such%20an%0Aover-parametrized%20space%2C%20remains%20limited.%20Successful%20applications%20are%20often%0Aconsidered%20as%20empirical%20rather%20than%20scientific%20achievements.%20For%20instance%2C%20deep%0Aneural%20networks%27%20%28DNNs%29%20internal%20representations%2C%20decision-making%20mechanism%2C%0Aabsence%20of%20overfitting%20in%20an%20over-parametrized%20space%2C%20high%20generalizability%2C%0Aetc.%2C%20remain%20less%20understood.%20This%20paper%20delves%20into%20the%20loss%20landscape%20of%20DNNs%0Athrough%20the%20lens%20of%20spin%20glass%20in%20statistical%20physics%2C%20i.e.%20a%20system%0Acharacterized%20by%20a%20complex%20energy%20landscape%20with%20numerous%20metastable%20states%2C%20to%0Abetter%20understand%20how%20DNNs%20work.%20We%20investigated%20a%20single%20hidden%20layer%0ARectified%20Linear%20Unit%20%28ReLU%29%20neural%20network%20model%2C%20and%20introduced%20several%0Aprotocols%20to%20examine%20the%20analogy%20between%20DNNs%20%28trained%20with%20datasets%20including%0AMNIST%20and%20CIFAR10%29%20and%20spin%20glass.%20Specifically%2C%20we%20used%20%281%29%20random%20walk%20in%20the%0Aparameter%20space%20of%20DNNs%20to%20unravel%20the%20structures%20in%20their%20loss%20landscape%3B%20%282%29%0Aa%20permutation-interpolation%20protocol%20to%20study%20the%20connection%20between%20copies%20of%0Aidentical%20regions%20in%20the%20loss%20landscape%20due%20to%20the%20permutation%20symmetry%20in%20the%0Ahidden%20layers%3B%20%283%29%20hierarchical%20clustering%20to%20reveal%20the%20hierarchy%20among%0Atrained%20solutions%20of%20DNNs%2C%20reminiscent%20of%20the%20so-called%20Replica%20Symmetry%0ABreaking%20%28RSB%29%20phenomenon%20%28i.e.%20the%20Parisi%20solution%29%20in%20analogy%20to%20spin%20glass%3B%0A%284%29%20finally%2C%20we%20examine%20the%20relationship%20between%20the%20degree%20of%20the%20ruggedness%0Aof%20the%20loss%20landscape%20of%20the%20DNN%20and%20its%20generalizability%2C%20showing%20an%0Aimprovement%20of%20flattened%20minima.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20724v1&entry.124074799=Read"},
{"title": "vSHARP: variable Splitting Half-quadratic Admm algorithm for\n  Reconstruction of inverse-Problems", "author": "George Yiasemis and Nikita Moriakov and Jan-Jakob Sonke and Jonas Teuwen", "abstract": "  Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance\nImaging (MRI), often involve reconstructing an image from noisy or incomplete\nmeasurements. This amounts to solving ill-posed inverse problems, where a\nsatisfactory closed-form analytical solution is not available. Traditional\nmethods such as Compressed Sensing (CS) in MRI reconstruction can be\ntime-consuming or prone to obtaining low-fidelity images. Recently, a plethora\nof Deep Learning (DL) approaches have demonstrated superior performance in\ninverse-problem solving, surpassing conventional methods. In this study, we\npropose vSHARP (variable Splitting Half-quadratic ADMM algorithm for\nReconstruction of inverse Problems), a novel DL-based method for solving\nill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic\nVariable Splitting method and employs the Alternating Direction Method of\nMultipliers (ADMM) to unroll the optimization process. For data consistency,\nvSHARP unrolls a differentiable gradient descent process in the image domain,\nwhile a DL-based denoiser, such as a U-Net architecture, is applied to enhance\nimage quality. vSHARP also employs a dilated-convolution DL-based model to\npredict the Lagrange multipliers for the ADMM initialization. We evaluate\nvSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct\ndatasets and on accelerated parallel dynamic MRI Reconstruction using another\ndataset. Our comparative analysis with state-of-the-art methods demonstrates\nthe superior performance of vSHARP in these applications.\n", "link": "http://arxiv.org/abs/2309.09954v2", "date": "2024-07-30", "relevancy": 1.9669, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4947}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4946}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vSHARP%3A%20variable%20Splitting%20Half-quadratic%20Admm%20algorithm%20for%0A%20%20Reconstruction%20of%20inverse-Problems&body=Title%3A%20vSHARP%3A%20variable%20Splitting%20Half-quadratic%20Admm%20algorithm%20for%0A%20%20Reconstruction%20of%20inverse-Problems%0AAuthor%3A%20George%20Yiasemis%20and%20Nikita%20Moriakov%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen%0AAbstract%3A%20%20%20Medical%20Imaging%20%28MI%29%20tasks%2C%20such%20as%20accelerated%20parallel%20Magnetic%20Resonance%0AImaging%20%28MRI%29%2C%20often%20involve%20reconstructing%20an%20image%20from%20noisy%20or%20incomplete%0Ameasurements.%20This%20amounts%20to%20solving%20ill-posed%20inverse%20problems%2C%20where%20a%0Asatisfactory%20closed-form%20analytical%20solution%20is%20not%20available.%20Traditional%0Amethods%20such%20as%20Compressed%20Sensing%20%28CS%29%20in%20MRI%20reconstruction%20can%20be%0Atime-consuming%20or%20prone%20to%20obtaining%20low-fidelity%20images.%20Recently%2C%20a%20plethora%0Aof%20Deep%20Learning%20%28DL%29%20approaches%20have%20demonstrated%20superior%20performance%20in%0Ainverse-problem%20solving%2C%20surpassing%20conventional%20methods.%20In%20this%20study%2C%20we%0Apropose%20vSHARP%20%28variable%20Splitting%20Half-quadratic%20ADMM%20algorithm%20for%0AReconstruction%20of%20inverse%20Problems%29%2C%20a%20novel%20DL-based%20method%20for%20solving%0Aill-posed%20inverse%20problems%20arising%20in%20MI.%20vSHARP%20utilizes%20the%20Half-Quadratic%0AVariable%20Splitting%20method%20and%20employs%20the%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29%20to%20unroll%20the%20optimization%20process.%20For%20data%20consistency%2C%0AvSHARP%20unrolls%20a%20differentiable%20gradient%20descent%20process%20in%20the%20image%20domain%2C%0Awhile%20a%20DL-based%20denoiser%2C%20such%20as%20a%20U-Net%20architecture%2C%20is%20applied%20to%20enhance%0Aimage%20quality.%20vSHARP%20also%20employs%20a%20dilated-convolution%20DL-based%20model%20to%0Apredict%20the%20Lagrange%20multipliers%20for%20the%20ADMM%20initialization.%20We%20evaluate%0AvSHARP%20on%20tasks%20of%20accelerated%20parallel%20MRI%20Reconstruction%20using%20two%20distinct%0Adatasets%20and%20on%20accelerated%20parallel%20dynamic%20MRI%20Reconstruction%20using%20another%0Adataset.%20Our%20comparative%20analysis%20with%20state-of-the-art%20methods%20demonstrates%0Athe%20superior%20performance%20of%20vSHARP%20in%20these%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvSHARP%253A%2520variable%2520Splitting%2520Half-quadratic%2520Admm%2520algorithm%2520for%250A%2520%2520Reconstruction%2520of%2520inverse-Problems%26entry.906535625%3DGeorge%2520Yiasemis%2520and%2520Nikita%2520Moriakov%2520and%2520Jan-Jakob%2520Sonke%2520and%2520Jonas%2520Teuwen%26entry.1292438233%3D%2520%2520Medical%2520Imaging%2520%2528MI%2529%2520tasks%252C%2520such%2520as%2520accelerated%2520parallel%2520Magnetic%2520Resonance%250AImaging%2520%2528MRI%2529%252C%2520often%2520involve%2520reconstructing%2520an%2520image%2520from%2520noisy%2520or%2520incomplete%250Ameasurements.%2520This%2520amounts%2520to%2520solving%2520ill-posed%2520inverse%2520problems%252C%2520where%2520a%250Asatisfactory%2520closed-form%2520analytical%2520solution%2520is%2520not%2520available.%2520Traditional%250Amethods%2520such%2520as%2520Compressed%2520Sensing%2520%2528CS%2529%2520in%2520MRI%2520reconstruction%2520can%2520be%250Atime-consuming%2520or%2520prone%2520to%2520obtaining%2520low-fidelity%2520images.%2520Recently%252C%2520a%2520plethora%250Aof%2520Deep%2520Learning%2520%2528DL%2529%2520approaches%2520have%2520demonstrated%2520superior%2520performance%2520in%250Ainverse-problem%2520solving%252C%2520surpassing%2520conventional%2520methods.%2520In%2520this%2520study%252C%2520we%250Apropose%2520vSHARP%2520%2528variable%2520Splitting%2520Half-quadratic%2520ADMM%2520algorithm%2520for%250AReconstruction%2520of%2520inverse%2520Problems%2529%252C%2520a%2520novel%2520DL-based%2520method%2520for%2520solving%250Aill-posed%2520inverse%2520problems%2520arising%2520in%2520MI.%2520vSHARP%2520utilizes%2520the%2520Half-Quadratic%250AVariable%2520Splitting%2520method%2520and%2520employs%2520the%2520Alternating%2520Direction%2520Method%2520of%250AMultipliers%2520%2528ADMM%2529%2520to%2520unroll%2520the%2520optimization%2520process.%2520For%2520data%2520consistency%252C%250AvSHARP%2520unrolls%2520a%2520differentiable%2520gradient%2520descent%2520process%2520in%2520the%2520image%2520domain%252C%250Awhile%2520a%2520DL-based%2520denoiser%252C%2520such%2520as%2520a%2520U-Net%2520architecture%252C%2520is%2520applied%2520to%2520enhance%250Aimage%2520quality.%2520vSHARP%2520also%2520employs%2520a%2520dilated-convolution%2520DL-based%2520model%2520to%250Apredict%2520the%2520Lagrange%2520multipliers%2520for%2520the%2520ADMM%2520initialization.%2520We%2520evaluate%250AvSHARP%2520on%2520tasks%2520of%2520accelerated%2520parallel%2520MRI%2520Reconstruction%2520using%2520two%2520distinct%250Adatasets%2520and%2520on%2520accelerated%2520parallel%2520dynamic%2520MRI%2520Reconstruction%2520using%2520another%250Adataset.%2520Our%2520comparative%2520analysis%2520with%2520state-of-the-art%2520methods%2520demonstrates%250Athe%2520superior%2520performance%2520of%2520vSHARP%2520in%2520these%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vSHARP%3A%20variable%20Splitting%20Half-quadratic%20Admm%20algorithm%20for%0A%20%20Reconstruction%20of%20inverse-Problems&entry.906535625=George%20Yiasemis%20and%20Nikita%20Moriakov%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen&entry.1292438233=%20%20Medical%20Imaging%20%28MI%29%20tasks%2C%20such%20as%20accelerated%20parallel%20Magnetic%20Resonance%0AImaging%20%28MRI%29%2C%20often%20involve%20reconstructing%20an%20image%20from%20noisy%20or%20incomplete%0Ameasurements.%20This%20amounts%20to%20solving%20ill-posed%20inverse%20problems%2C%20where%20a%0Asatisfactory%20closed-form%20analytical%20solution%20is%20not%20available.%20Traditional%0Amethods%20such%20as%20Compressed%20Sensing%20%28CS%29%20in%20MRI%20reconstruction%20can%20be%0Atime-consuming%20or%20prone%20to%20obtaining%20low-fidelity%20images.%20Recently%2C%20a%20plethora%0Aof%20Deep%20Learning%20%28DL%29%20approaches%20have%20demonstrated%20superior%20performance%20in%0Ainverse-problem%20solving%2C%20surpassing%20conventional%20methods.%20In%20this%20study%2C%20we%0Apropose%20vSHARP%20%28variable%20Splitting%20Half-quadratic%20ADMM%20algorithm%20for%0AReconstruction%20of%20inverse%20Problems%29%2C%20a%20novel%20DL-based%20method%20for%20solving%0Aill-posed%20inverse%20problems%20arising%20in%20MI.%20vSHARP%20utilizes%20the%20Half-Quadratic%0AVariable%20Splitting%20method%20and%20employs%20the%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29%20to%20unroll%20the%20optimization%20process.%20For%20data%20consistency%2C%0AvSHARP%20unrolls%20a%20differentiable%20gradient%20descent%20process%20in%20the%20image%20domain%2C%0Awhile%20a%20DL-based%20denoiser%2C%20such%20as%20a%20U-Net%20architecture%2C%20is%20applied%20to%20enhance%0Aimage%20quality.%20vSHARP%20also%20employs%20a%20dilated-convolution%20DL-based%20model%20to%0Apredict%20the%20Lagrange%20multipliers%20for%20the%20ADMM%20initialization.%20We%20evaluate%0AvSHARP%20on%20tasks%20of%20accelerated%20parallel%20MRI%20Reconstruction%20using%20two%20distinct%0Adatasets%20and%20on%20accelerated%20parallel%20dynamic%20MRI%20Reconstruction%20using%20another%0Adataset.%20Our%20comparative%20analysis%20with%20state-of-the-art%20methods%20demonstrates%0Athe%20superior%20performance%20of%20vSHARP%20in%20these%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09954v2&entry.124074799=Read"},
{"title": "What Are Good Positional Encodings for Directed Graphs?", "author": "Yinan Huang and Haoyu Wang and Pan Li", "abstract": "  Positional encodings (PE) for graphs are essential in constructing powerful\nand expressive graph neural networks and graph transformers as they effectively\ncapture relative spatial relations between nodes. While PEs for undirected\ngraphs have been extensively studied, those for directed graphs remain largely\nunexplored, despite the fundamental role of directed graphs in representing\nentities with strong logical dependencies, such as those in program analysis\nand circuit designs. This work studies the design of PEs for directed graphs\nthat are expressive to represent desired directed spatial relations. We first\npropose walk profile, a generalization of walk counting sequence to directed\ngraphs. We identify limitations in existing PE methods, including symmetrized\nLaplacian PE, Singular Value Decomposition PE, and Magnetic Laplacian PE, in\ntheir ability to express walk profiles. To address these limitations, we\npropose the Multi-q Magnetic Laplacian PE, which extends Magnetic Laplacian PE\nwith multiple potential factors. This simple variant turns out to be capable of\nprovably expressing walk profiles. Furthermore, we generalize previous\nbasis-invariant and stable networks to handle complex-domain PEs decomposed\nfrom Magnetic Laplacians. Our numerical experiments demonstrate the\neffectiveness of Multi-q Magnetic Laplacian PE with a stable neural\narchitecture, outperforming previous PE methods (with stable networks) on\npredicting directed distances/walk profiles, sorting network satisfiability,\nand on general circuit benchmarks. Our code is available at\nhttps://github.com/Graph-COM/Multi-q-Maglap.\n", "link": "http://arxiv.org/abs/2407.20912v1", "date": "2024-07-30", "relevancy": 1.9609, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4771}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Are%20Good%20Positional%20Encodings%20for%20Directed%20Graphs%3F&body=Title%3A%20What%20Are%20Good%20Positional%20Encodings%20for%20Directed%20Graphs%3F%0AAuthor%3A%20Yinan%20Huang%20and%20Haoyu%20Wang%20and%20Pan%20Li%0AAbstract%3A%20%20%20Positional%20encodings%20%28PE%29%20for%20graphs%20are%20essential%20in%20constructing%20powerful%0Aand%20expressive%20graph%20neural%20networks%20and%20graph%20transformers%20as%20they%20effectively%0Acapture%20relative%20spatial%20relations%20between%20nodes.%20While%20PEs%20for%20undirected%0Agraphs%20have%20been%20extensively%20studied%2C%20those%20for%20directed%20graphs%20remain%20largely%0Aunexplored%2C%20despite%20the%20fundamental%20role%20of%20directed%20graphs%20in%20representing%0Aentities%20with%20strong%20logical%20dependencies%2C%20such%20as%20those%20in%20program%20analysis%0Aand%20circuit%20designs.%20This%20work%20studies%20the%20design%20of%20PEs%20for%20directed%20graphs%0Athat%20are%20expressive%20to%20represent%20desired%20directed%20spatial%20relations.%20We%20first%0Apropose%20walk%20profile%2C%20a%20generalization%20of%20walk%20counting%20sequence%20to%20directed%0Agraphs.%20We%20identify%20limitations%20in%20existing%20PE%20methods%2C%20including%20symmetrized%0ALaplacian%20PE%2C%20Singular%20Value%20Decomposition%20PE%2C%20and%20Magnetic%20Laplacian%20PE%2C%20in%0Atheir%20ability%20to%20express%20walk%20profiles.%20To%20address%20these%20limitations%2C%20we%0Apropose%20the%20Multi-q%20Magnetic%20Laplacian%20PE%2C%20which%20extends%20Magnetic%20Laplacian%20PE%0Awith%20multiple%20potential%20factors.%20This%20simple%20variant%20turns%20out%20to%20be%20capable%20of%0Aprovably%20expressing%20walk%20profiles.%20Furthermore%2C%20we%20generalize%20previous%0Abasis-invariant%20and%20stable%20networks%20to%20handle%20complex-domain%20PEs%20decomposed%0Afrom%20Magnetic%20Laplacians.%20Our%20numerical%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20Multi-q%20Magnetic%20Laplacian%20PE%20with%20a%20stable%20neural%0Aarchitecture%2C%20outperforming%20previous%20PE%20methods%20%28with%20stable%20networks%29%20on%0Apredicting%20directed%20distances/walk%20profiles%2C%20sorting%20network%20satisfiability%2C%0Aand%20on%20general%20circuit%20benchmarks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Graph-COM/Multi-q-Maglap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Are%2520Good%2520Positional%2520Encodings%2520for%2520Directed%2520Graphs%253F%26entry.906535625%3DYinan%2520Huang%2520and%2520Haoyu%2520Wang%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520Positional%2520encodings%2520%2528PE%2529%2520for%2520graphs%2520are%2520essential%2520in%2520constructing%2520powerful%250Aand%2520expressive%2520graph%2520neural%2520networks%2520and%2520graph%2520transformers%2520as%2520they%2520effectively%250Acapture%2520relative%2520spatial%2520relations%2520between%2520nodes.%2520While%2520PEs%2520for%2520undirected%250Agraphs%2520have%2520been%2520extensively%2520studied%252C%2520those%2520for%2520directed%2520graphs%2520remain%2520largely%250Aunexplored%252C%2520despite%2520the%2520fundamental%2520role%2520of%2520directed%2520graphs%2520in%2520representing%250Aentities%2520with%2520strong%2520logical%2520dependencies%252C%2520such%2520as%2520those%2520in%2520program%2520analysis%250Aand%2520circuit%2520designs.%2520This%2520work%2520studies%2520the%2520design%2520of%2520PEs%2520for%2520directed%2520graphs%250Athat%2520are%2520expressive%2520to%2520represent%2520desired%2520directed%2520spatial%2520relations.%2520We%2520first%250Apropose%2520walk%2520profile%252C%2520a%2520generalization%2520of%2520walk%2520counting%2520sequence%2520to%2520directed%250Agraphs.%2520We%2520identify%2520limitations%2520in%2520existing%2520PE%2520methods%252C%2520including%2520symmetrized%250ALaplacian%2520PE%252C%2520Singular%2520Value%2520Decomposition%2520PE%252C%2520and%2520Magnetic%2520Laplacian%2520PE%252C%2520in%250Atheir%2520ability%2520to%2520express%2520walk%2520profiles.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520the%2520Multi-q%2520Magnetic%2520Laplacian%2520PE%252C%2520which%2520extends%2520Magnetic%2520Laplacian%2520PE%250Awith%2520multiple%2520potential%2520factors.%2520This%2520simple%2520variant%2520turns%2520out%2520to%2520be%2520capable%2520of%250Aprovably%2520expressing%2520walk%2520profiles.%2520Furthermore%252C%2520we%2520generalize%2520previous%250Abasis-invariant%2520and%2520stable%2520networks%2520to%2520handle%2520complex-domain%2520PEs%2520decomposed%250Afrom%2520Magnetic%2520Laplacians.%2520Our%2520numerical%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520Multi-q%2520Magnetic%2520Laplacian%2520PE%2520with%2520a%2520stable%2520neural%250Aarchitecture%252C%2520outperforming%2520previous%2520PE%2520methods%2520%2528with%2520stable%2520networks%2529%2520on%250Apredicting%2520directed%2520distances/walk%2520profiles%252C%2520sorting%2520network%2520satisfiability%252C%250Aand%2520on%2520general%2520circuit%2520benchmarks.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Graph-COM/Multi-q-Maglap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Are%20Good%20Positional%20Encodings%20for%20Directed%20Graphs%3F&entry.906535625=Yinan%20Huang%20and%20Haoyu%20Wang%20and%20Pan%20Li&entry.1292438233=%20%20Positional%20encodings%20%28PE%29%20for%20graphs%20are%20essential%20in%20constructing%20powerful%0Aand%20expressive%20graph%20neural%20networks%20and%20graph%20transformers%20as%20they%20effectively%0Acapture%20relative%20spatial%20relations%20between%20nodes.%20While%20PEs%20for%20undirected%0Agraphs%20have%20been%20extensively%20studied%2C%20those%20for%20directed%20graphs%20remain%20largely%0Aunexplored%2C%20despite%20the%20fundamental%20role%20of%20directed%20graphs%20in%20representing%0Aentities%20with%20strong%20logical%20dependencies%2C%20such%20as%20those%20in%20program%20analysis%0Aand%20circuit%20designs.%20This%20work%20studies%20the%20design%20of%20PEs%20for%20directed%20graphs%0Athat%20are%20expressive%20to%20represent%20desired%20directed%20spatial%20relations.%20We%20first%0Apropose%20walk%20profile%2C%20a%20generalization%20of%20walk%20counting%20sequence%20to%20directed%0Agraphs.%20We%20identify%20limitations%20in%20existing%20PE%20methods%2C%20including%20symmetrized%0ALaplacian%20PE%2C%20Singular%20Value%20Decomposition%20PE%2C%20and%20Magnetic%20Laplacian%20PE%2C%20in%0Atheir%20ability%20to%20express%20walk%20profiles.%20To%20address%20these%20limitations%2C%20we%0Apropose%20the%20Multi-q%20Magnetic%20Laplacian%20PE%2C%20which%20extends%20Magnetic%20Laplacian%20PE%0Awith%20multiple%20potential%20factors.%20This%20simple%20variant%20turns%20out%20to%20be%20capable%20of%0Aprovably%20expressing%20walk%20profiles.%20Furthermore%2C%20we%20generalize%20previous%0Abasis-invariant%20and%20stable%20networks%20to%20handle%20complex-domain%20PEs%20decomposed%0Afrom%20Magnetic%20Laplacians.%20Our%20numerical%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20Multi-q%20Magnetic%20Laplacian%20PE%20with%20a%20stable%20neural%0Aarchitecture%2C%20outperforming%20previous%20PE%20methods%20%28with%20stable%20networks%29%20on%0Apredicting%20directed%20distances/walk%20profiles%2C%20sorting%20network%20satisfiability%2C%0Aand%20on%20general%20circuit%20benchmarks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Graph-COM/Multi-q-Maglap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20912v1&entry.124074799=Read"},
{"title": "Vulnerabilities in AI-generated Image Detection: The Challenge of\n  Adversarial Attacks", "author": "Yunfeng Diao and Naixin Zhai and Changtao Miao and Xun Yang and Meng Wang", "abstract": "  Recent advancements in image synthesis, particularly with the advent of GAN\nand Diffusion models, have amplified public concerns regarding the\ndissemination of disinformation. To address such concerns, numerous\nAI-generated Image (AIGI) Detectors have been proposed and achieved promising\nperformance in identifying fake images. However, there still lacks a systematic\nunderstanding of the adversarial robustness of these AIGI detectors. In this\npaper, we examine the vulnerability of state-of-the-art AIGI detectors against\nadversarial attack under white-box and black-box settings, which has been\nrarely investigated so far. For the task of AIGI detection, we propose a new\nattack containing two main parts. First, inspired by the obvious difference\nbetween real images and fake images in the frequency domain, we add\nperturbations under the frequency domain to push the image away from its\noriginal frequency distribution. Second, we explore the full posterior\ndistribution of the surrogate model to further narrow this gap between\nheterogeneous models, e.g. transferring adversarial examples across CNNs and\nViTs. This is achieved by introducing a novel post-train Bayesian strategy that\nturns a single surrogate into a Bayesian one, capable of simulating diverse\nvictim models using one pre-trained surrogate, without the need for\nre-training. We name our method as frequency-based post-train Bayesian attack,\nor FPBA. Through FPBA, we show that adversarial attack is truly a real threat\nto AIGI detectors, because FPBA can deliver successful black-box attacks across\nmodels, generators, defense methods, and even evade cross-generator detection,\nwhich is a crucial real-world detection scenario.\n", "link": "http://arxiv.org/abs/2407.20836v1", "date": "2024-07-30", "relevancy": 1.956, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4987}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4828}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vulnerabilities%20in%20AI-generated%20Image%20Detection%3A%20The%20Challenge%20of%0A%20%20Adversarial%20Attacks&body=Title%3A%20Vulnerabilities%20in%20AI-generated%20Image%20Detection%3A%20The%20Challenge%20of%0A%20%20Adversarial%20Attacks%0AAuthor%3A%20Yunfeng%20Diao%20and%20Naixin%20Zhai%20and%20Changtao%20Miao%20and%20Xun%20Yang%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image%20synthesis%2C%20particularly%20with%20the%20advent%20of%20GAN%0Aand%20Diffusion%20models%2C%20have%20amplified%20public%20concerns%20regarding%20the%0Adissemination%20of%20disinformation.%20To%20address%20such%20concerns%2C%20numerous%0AAI-generated%20Image%20%28AIGI%29%20Detectors%20have%20been%20proposed%20and%20achieved%20promising%0Aperformance%20in%20identifying%20fake%20images.%20However%2C%20there%20still%20lacks%20a%20systematic%0Aunderstanding%20of%20the%20adversarial%20robustness%20of%20these%20AIGI%20detectors.%20In%20this%0Apaper%2C%20we%20examine%20the%20vulnerability%20of%20state-of-the-art%20AIGI%20detectors%20against%0Aadversarial%20attack%20under%20white-box%20and%20black-box%20settings%2C%20which%20has%20been%0Ararely%20investigated%20so%20far.%20For%20the%20task%20of%20AIGI%20detection%2C%20we%20propose%20a%20new%0Aattack%20containing%20two%20main%20parts.%20First%2C%20inspired%20by%20the%20obvious%20difference%0Abetween%20real%20images%20and%20fake%20images%20in%20the%20frequency%20domain%2C%20we%20add%0Aperturbations%20under%20the%20frequency%20domain%20to%20push%20the%20image%20away%20from%20its%0Aoriginal%20frequency%20distribution.%20Second%2C%20we%20explore%20the%20full%20posterior%0Adistribution%20of%20the%20surrogate%20model%20to%20further%20narrow%20this%20gap%20between%0Aheterogeneous%20models%2C%20e.g.%20transferring%20adversarial%20examples%20across%20CNNs%20and%0AViTs.%20This%20is%20achieved%20by%20introducing%20a%20novel%20post-train%20Bayesian%20strategy%20that%0Aturns%20a%20single%20surrogate%20into%20a%20Bayesian%20one%2C%20capable%20of%20simulating%20diverse%0Avictim%20models%20using%20one%20pre-trained%20surrogate%2C%20without%20the%20need%20for%0Are-training.%20We%20name%20our%20method%20as%20frequency-based%20post-train%20Bayesian%20attack%2C%0Aor%20FPBA.%20Through%20FPBA%2C%20we%20show%20that%20adversarial%20attack%20is%20truly%20a%20real%20threat%0Ato%20AIGI%20detectors%2C%20because%20FPBA%20can%20deliver%20successful%20black-box%20attacks%20across%0Amodels%2C%20generators%2C%20defense%20methods%2C%20and%20even%20evade%20cross-generator%20detection%2C%0Awhich%20is%20a%20crucial%20real-world%20detection%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVulnerabilities%2520in%2520AI-generated%2520Image%2520Detection%253A%2520The%2520Challenge%2520of%250A%2520%2520Adversarial%2520Attacks%26entry.906535625%3DYunfeng%2520Diao%2520and%2520Naixin%2520Zhai%2520and%2520Changtao%2520Miao%2520and%2520Xun%2520Yang%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image%2520synthesis%252C%2520particularly%2520with%2520the%2520advent%2520of%2520GAN%250Aand%2520Diffusion%2520models%252C%2520have%2520amplified%2520public%2520concerns%2520regarding%2520the%250Adissemination%2520of%2520disinformation.%2520To%2520address%2520such%2520concerns%252C%2520numerous%250AAI-generated%2520Image%2520%2528AIGI%2529%2520Detectors%2520have%2520been%2520proposed%2520and%2520achieved%2520promising%250Aperformance%2520in%2520identifying%2520fake%2520images.%2520However%252C%2520there%2520still%2520lacks%2520a%2520systematic%250Aunderstanding%2520of%2520the%2520adversarial%2520robustness%2520of%2520these%2520AIGI%2520detectors.%2520In%2520this%250Apaper%252C%2520we%2520examine%2520the%2520vulnerability%2520of%2520state-of-the-art%2520AIGI%2520detectors%2520against%250Aadversarial%2520attack%2520under%2520white-box%2520and%2520black-box%2520settings%252C%2520which%2520has%2520been%250Ararely%2520investigated%2520so%2520far.%2520For%2520the%2520task%2520of%2520AIGI%2520detection%252C%2520we%2520propose%2520a%2520new%250Aattack%2520containing%2520two%2520main%2520parts.%2520First%252C%2520inspired%2520by%2520the%2520obvious%2520difference%250Abetween%2520real%2520images%2520and%2520fake%2520images%2520in%2520the%2520frequency%2520domain%252C%2520we%2520add%250Aperturbations%2520under%2520the%2520frequency%2520domain%2520to%2520push%2520the%2520image%2520away%2520from%2520its%250Aoriginal%2520frequency%2520distribution.%2520Second%252C%2520we%2520explore%2520the%2520full%2520posterior%250Adistribution%2520of%2520the%2520surrogate%2520model%2520to%2520further%2520narrow%2520this%2520gap%2520between%250Aheterogeneous%2520models%252C%2520e.g.%2520transferring%2520adversarial%2520examples%2520across%2520CNNs%2520and%250AViTs.%2520This%2520is%2520achieved%2520by%2520introducing%2520a%2520novel%2520post-train%2520Bayesian%2520strategy%2520that%250Aturns%2520a%2520single%2520surrogate%2520into%2520a%2520Bayesian%2520one%252C%2520capable%2520of%2520simulating%2520diverse%250Avictim%2520models%2520using%2520one%2520pre-trained%2520surrogate%252C%2520without%2520the%2520need%2520for%250Are-training.%2520We%2520name%2520our%2520method%2520as%2520frequency-based%2520post-train%2520Bayesian%2520attack%252C%250Aor%2520FPBA.%2520Through%2520FPBA%252C%2520we%2520show%2520that%2520adversarial%2520attack%2520is%2520truly%2520a%2520real%2520threat%250Ato%2520AIGI%2520detectors%252C%2520because%2520FPBA%2520can%2520deliver%2520successful%2520black-box%2520attacks%2520across%250Amodels%252C%2520generators%252C%2520defense%2520methods%252C%2520and%2520even%2520evade%2520cross-generator%2520detection%252C%250Awhich%2520is%2520a%2520crucial%2520real-world%2520detection%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vulnerabilities%20in%20AI-generated%20Image%20Detection%3A%20The%20Challenge%20of%0A%20%20Adversarial%20Attacks&entry.906535625=Yunfeng%20Diao%20and%20Naixin%20Zhai%20and%20Changtao%20Miao%20and%20Xun%20Yang%20and%20Meng%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20image%20synthesis%2C%20particularly%20with%20the%20advent%20of%20GAN%0Aand%20Diffusion%20models%2C%20have%20amplified%20public%20concerns%20regarding%20the%0Adissemination%20of%20disinformation.%20To%20address%20such%20concerns%2C%20numerous%0AAI-generated%20Image%20%28AIGI%29%20Detectors%20have%20been%20proposed%20and%20achieved%20promising%0Aperformance%20in%20identifying%20fake%20images.%20However%2C%20there%20still%20lacks%20a%20systematic%0Aunderstanding%20of%20the%20adversarial%20robustness%20of%20these%20AIGI%20detectors.%20In%20this%0Apaper%2C%20we%20examine%20the%20vulnerability%20of%20state-of-the-art%20AIGI%20detectors%20against%0Aadversarial%20attack%20under%20white-box%20and%20black-box%20settings%2C%20which%20has%20been%0Ararely%20investigated%20so%20far.%20For%20the%20task%20of%20AIGI%20detection%2C%20we%20propose%20a%20new%0Aattack%20containing%20two%20main%20parts.%20First%2C%20inspired%20by%20the%20obvious%20difference%0Abetween%20real%20images%20and%20fake%20images%20in%20the%20frequency%20domain%2C%20we%20add%0Aperturbations%20under%20the%20frequency%20domain%20to%20push%20the%20image%20away%20from%20its%0Aoriginal%20frequency%20distribution.%20Second%2C%20we%20explore%20the%20full%20posterior%0Adistribution%20of%20the%20surrogate%20model%20to%20further%20narrow%20this%20gap%20between%0Aheterogeneous%20models%2C%20e.g.%20transferring%20adversarial%20examples%20across%20CNNs%20and%0AViTs.%20This%20is%20achieved%20by%20introducing%20a%20novel%20post-train%20Bayesian%20strategy%20that%0Aturns%20a%20single%20surrogate%20into%20a%20Bayesian%20one%2C%20capable%20of%20simulating%20diverse%0Avictim%20models%20using%20one%20pre-trained%20surrogate%2C%20without%20the%20need%20for%0Are-training.%20We%20name%20our%20method%20as%20frequency-based%20post-train%20Bayesian%20attack%2C%0Aor%20FPBA.%20Through%20FPBA%2C%20we%20show%20that%20adversarial%20attack%20is%20truly%20a%20real%20threat%0Ato%20AIGI%20detectors%2C%20because%20FPBA%20can%20deliver%20successful%20black-box%20attacks%20across%0Amodels%2C%20generators%2C%20defense%20methods%2C%20and%20even%20evade%20cross-generator%20detection%2C%0Awhich%20is%20a%20crucial%20real-world%20detection%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20836v1&entry.124074799=Read"},
{"title": "Generative Learning for Simulation of Vehicle Faults", "author": "Patrick Kuiper and Sirui Lin and Jose Blanchet and Vahid Tarokh", "abstract": "  We develop a novel generative model to simulate vehicle health and forecast\nfaults, conditioned on practical operational considerations. The model, trained\non data from the US Army's Predictive Logistics program, aims to support\npredictive maintenance. It forecasts faults far enough in advance to execute a\nmaintenance intervention before a breakdown occurs. The model incorporates\nreal-world factors that affect vehicle health. It also allows us to understand\nthe vehicle's condition by analyzing operating data, and characterizing each\nvehicle into discrete states. Importantly, the model predicts the time to first\nfault with high accuracy. We compare its performance to other models and\ndemonstrate its successful training.\n", "link": "http://arxiv.org/abs/2407.17654v2", "date": "2024-07-30", "relevancy": 1.9538, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5043}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Learning%20for%20Simulation%20of%20Vehicle%20Faults&body=Title%3A%20Generative%20Learning%20for%20Simulation%20of%20Vehicle%20Faults%0AAuthor%3A%20Patrick%20Kuiper%20and%20Sirui%20Lin%20and%20Jose%20Blanchet%20and%20Vahid%20Tarokh%0AAbstract%3A%20%20%20We%20develop%20a%20novel%20generative%20model%20to%20simulate%20vehicle%20health%20and%20forecast%0Afaults%2C%20conditioned%20on%20practical%20operational%20considerations.%20The%20model%2C%20trained%0Aon%20data%20from%20the%20US%20Army%27s%20Predictive%20Logistics%20program%2C%20aims%20to%20support%0Apredictive%20maintenance.%20It%20forecasts%20faults%20far%20enough%20in%20advance%20to%20execute%20a%0Amaintenance%20intervention%20before%20a%20breakdown%20occurs.%20The%20model%20incorporates%0Areal-world%20factors%20that%20affect%20vehicle%20health.%20It%20also%20allows%20us%20to%20understand%0Athe%20vehicle%27s%20condition%20by%20analyzing%20operating%20data%2C%20and%20characterizing%20each%0Avehicle%20into%20discrete%20states.%20Importantly%2C%20the%20model%20predicts%20the%20time%20to%20first%0Afault%20with%20high%20accuracy.%20We%20compare%20its%20performance%20to%20other%20models%20and%0Ademonstrate%20its%20successful%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Learning%2520for%2520Simulation%2520of%2520Vehicle%2520Faults%26entry.906535625%3DPatrick%2520Kuiper%2520and%2520Sirui%2520Lin%2520and%2520Jose%2520Blanchet%2520and%2520Vahid%2520Tarokh%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520novel%2520generative%2520model%2520to%2520simulate%2520vehicle%2520health%2520and%2520forecast%250Afaults%252C%2520conditioned%2520on%2520practical%2520operational%2520considerations.%2520The%2520model%252C%2520trained%250Aon%2520data%2520from%2520the%2520US%2520Army%2527s%2520Predictive%2520Logistics%2520program%252C%2520aims%2520to%2520support%250Apredictive%2520maintenance.%2520It%2520forecasts%2520faults%2520far%2520enough%2520in%2520advance%2520to%2520execute%2520a%250Amaintenance%2520intervention%2520before%2520a%2520breakdown%2520occurs.%2520The%2520model%2520incorporates%250Areal-world%2520factors%2520that%2520affect%2520vehicle%2520health.%2520It%2520also%2520allows%2520us%2520to%2520understand%250Athe%2520vehicle%2527s%2520condition%2520by%2520analyzing%2520operating%2520data%252C%2520and%2520characterizing%2520each%250Avehicle%2520into%2520discrete%2520states.%2520Importantly%252C%2520the%2520model%2520predicts%2520the%2520time%2520to%2520first%250Afault%2520with%2520high%2520accuracy.%2520We%2520compare%2520its%2520performance%2520to%2520other%2520models%2520and%250Ademonstrate%2520its%2520successful%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Learning%20for%20Simulation%20of%20Vehicle%20Faults&entry.906535625=Patrick%20Kuiper%20and%20Sirui%20Lin%20and%20Jose%20Blanchet%20and%20Vahid%20Tarokh&entry.1292438233=%20%20We%20develop%20a%20novel%20generative%20model%20to%20simulate%20vehicle%20health%20and%20forecast%0Afaults%2C%20conditioned%20on%20practical%20operational%20considerations.%20The%20model%2C%20trained%0Aon%20data%20from%20the%20US%20Army%27s%20Predictive%20Logistics%20program%2C%20aims%20to%20support%0Apredictive%20maintenance.%20It%20forecasts%20faults%20far%20enough%20in%20advance%20to%20execute%20a%0Amaintenance%20intervention%20before%20a%20breakdown%20occurs.%20The%20model%20incorporates%0Areal-world%20factors%20that%20affect%20vehicle%20health.%20It%20also%20allows%20us%20to%20understand%0Athe%20vehicle%27s%20condition%20by%20analyzing%20operating%20data%2C%20and%20characterizing%20each%0Avehicle%20into%20discrete%20states.%20Importantly%2C%20the%20model%20predicts%20the%20time%20to%20first%0Afault%20with%20high%20accuracy.%20We%20compare%20its%20performance%20to%20other%20models%20and%0Ademonstrate%20its%20successful%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17654v2&entry.124074799=Read"},
{"title": "Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial\n  Minority Influence", "author": "Simin Li and Jun Guo and Jingqiao Xiu and Yuwei Zheng and Pu Feng and Xin Yu and Aishan Liu and Yaodong Yang and Bo An and Wenjun Wu and Xianglong Liu", "abstract": "  This study probes the vulnerabilities of cooperative multi-agent\nreinforcement learning (c-MARL) under adversarial attacks, a critical\ndeterminant of c-MARL's worst-case performance prior to real-world\nimplementation. Current observation-based attacks, constrained by white-box\nassumptions, overlook c-MARL's complex multi-agent interactions and cooperative\nobjectives, resulting in impractical and limited attack capabilities. To\naddress these shortcomes, we propose Adversarial Minority Influence (AMI), a\npractical and strong for c-MARL. AMI is a practical black-box attack and can be\nlaunched without knowing victim parameters. AMI is also strong by considering\nthe complex multi-agent interaction and the cooperative goal of agents,\nenabling a single adversarial agent to unilaterally misleads majority victims\nto form targeted worst-case cooperation. This mirrors minority influence\nphenomena in social psychology. To achieve maximum deviation in victim policies\nunder complex agent-wise interactions, our unilateral attack aims to\ncharacterize and maximize the impact of the adversary on the victims. This is\nachieved by adapting a unilateral agent-wise relation metric derived from\nmutual information, thereby mitigating the adverse effects of victim influence\non the adversary. To lead the victims into a jointly detrimental scenario, our\ntargeted attack deceives victims into a long-term, cooperatively harmful\nsituation by guiding each victim towards a specific target, determined through\na trial-and-error process executed by a reinforcement learning agent. Through\nAMI, we achieve the first successful attack against real-world robot swarms and\neffectively fool agents in simulated environments into collectively worst-case\nscenarios, including Starcraft II and Multi-agent Mujoco. The source code and\ndemonstrations can be found at: https://github.com/DIG-Beihang/AMI.\n", "link": "http://arxiv.org/abs/2302.03322v3", "date": "2024-07-30", "relevancy": 1.9497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5121}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4978}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attacking%20Cooperative%20Multi-Agent%20Reinforcement%20Learning%20by%20Adversarial%0A%20%20Minority%20Influence&body=Title%3A%20Attacking%20Cooperative%20Multi-Agent%20Reinforcement%20Learning%20by%20Adversarial%0A%20%20Minority%20Influence%0AAuthor%3A%20Simin%20Li%20and%20Jun%20Guo%20and%20Jingqiao%20Xiu%20and%20Yuwei%20Zheng%20and%20Pu%20Feng%20and%20Xin%20Yu%20and%20Aishan%20Liu%20and%20Yaodong%20Yang%20and%20Bo%20An%20and%20Wenjun%20Wu%20and%20Xianglong%20Liu%0AAbstract%3A%20%20%20This%20study%20probes%20the%20vulnerabilities%20of%20cooperative%20multi-agent%0Areinforcement%20learning%20%28c-MARL%29%20under%20adversarial%20attacks%2C%20a%20critical%0Adeterminant%20of%20c-MARL%27s%20worst-case%20performance%20prior%20to%20real-world%0Aimplementation.%20Current%20observation-based%20attacks%2C%20constrained%20by%20white-box%0Aassumptions%2C%20overlook%20c-MARL%27s%20complex%20multi-agent%20interactions%20and%20cooperative%0Aobjectives%2C%20resulting%20in%20impractical%20and%20limited%20attack%20capabilities.%20To%0Aaddress%20these%20shortcomes%2C%20we%20propose%20Adversarial%20Minority%20Influence%20%28AMI%29%2C%20a%0Apractical%20and%20strong%20for%20c-MARL.%20AMI%20is%20a%20practical%20black-box%20attack%20and%20can%20be%0Alaunched%20without%20knowing%20victim%20parameters.%20AMI%20is%20also%20strong%20by%20considering%0Athe%20complex%20multi-agent%20interaction%20and%20the%20cooperative%20goal%20of%20agents%2C%0Aenabling%20a%20single%20adversarial%20agent%20to%20unilaterally%20misleads%20majority%20victims%0Ato%20form%20targeted%20worst-case%20cooperation.%20This%20mirrors%20minority%20influence%0Aphenomena%20in%20social%20psychology.%20To%20achieve%20maximum%20deviation%20in%20victim%20policies%0Aunder%20complex%20agent-wise%20interactions%2C%20our%20unilateral%20attack%20aims%20to%0Acharacterize%20and%20maximize%20the%20impact%20of%20the%20adversary%20on%20the%20victims.%20This%20is%0Aachieved%20by%20adapting%20a%20unilateral%20agent-wise%20relation%20metric%20derived%20from%0Amutual%20information%2C%20thereby%20mitigating%20the%20adverse%20effects%20of%20victim%20influence%0Aon%20the%20adversary.%20To%20lead%20the%20victims%20into%20a%20jointly%20detrimental%20scenario%2C%20our%0Atargeted%20attack%20deceives%20victims%20into%20a%20long-term%2C%20cooperatively%20harmful%0Asituation%20by%20guiding%20each%20victim%20towards%20a%20specific%20target%2C%20determined%20through%0Aa%20trial-and-error%20process%20executed%20by%20a%20reinforcement%20learning%20agent.%20Through%0AAMI%2C%20we%20achieve%20the%20first%20successful%20attack%20against%20real-world%20robot%20swarms%20and%0Aeffectively%20fool%20agents%20in%20simulated%20environments%20into%20collectively%20worst-case%0Ascenarios%2C%20including%20Starcraft%20II%20and%20Multi-agent%20Mujoco.%20The%20source%20code%20and%0Ademonstrations%20can%20be%20found%20at%3A%20https%3A//github.com/DIG-Beihang/AMI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttacking%2520Cooperative%2520Multi-Agent%2520Reinforcement%2520Learning%2520by%2520Adversarial%250A%2520%2520Minority%2520Influence%26entry.906535625%3DSimin%2520Li%2520and%2520Jun%2520Guo%2520and%2520Jingqiao%2520Xiu%2520and%2520Yuwei%2520Zheng%2520and%2520Pu%2520Feng%2520and%2520Xin%2520Yu%2520and%2520Aishan%2520Liu%2520and%2520Yaodong%2520Yang%2520and%2520Bo%2520An%2520and%2520Wenjun%2520Wu%2520and%2520Xianglong%2520Liu%26entry.1292438233%3D%2520%2520This%2520study%2520probes%2520the%2520vulnerabilities%2520of%2520cooperative%2520multi-agent%250Areinforcement%2520learning%2520%2528c-MARL%2529%2520under%2520adversarial%2520attacks%252C%2520a%2520critical%250Adeterminant%2520of%2520c-MARL%2527s%2520worst-case%2520performance%2520prior%2520to%2520real-world%250Aimplementation.%2520Current%2520observation-based%2520attacks%252C%2520constrained%2520by%2520white-box%250Aassumptions%252C%2520overlook%2520c-MARL%2527s%2520complex%2520multi-agent%2520interactions%2520and%2520cooperative%250Aobjectives%252C%2520resulting%2520in%2520impractical%2520and%2520limited%2520attack%2520capabilities.%2520To%250Aaddress%2520these%2520shortcomes%252C%2520we%2520propose%2520Adversarial%2520Minority%2520Influence%2520%2528AMI%2529%252C%2520a%250Apractical%2520and%2520strong%2520for%2520c-MARL.%2520AMI%2520is%2520a%2520practical%2520black-box%2520attack%2520and%2520can%2520be%250Alaunched%2520without%2520knowing%2520victim%2520parameters.%2520AMI%2520is%2520also%2520strong%2520by%2520considering%250Athe%2520complex%2520multi-agent%2520interaction%2520and%2520the%2520cooperative%2520goal%2520of%2520agents%252C%250Aenabling%2520a%2520single%2520adversarial%2520agent%2520to%2520unilaterally%2520misleads%2520majority%2520victims%250Ato%2520form%2520targeted%2520worst-case%2520cooperation.%2520This%2520mirrors%2520minority%2520influence%250Aphenomena%2520in%2520social%2520psychology.%2520To%2520achieve%2520maximum%2520deviation%2520in%2520victim%2520policies%250Aunder%2520complex%2520agent-wise%2520interactions%252C%2520our%2520unilateral%2520attack%2520aims%2520to%250Acharacterize%2520and%2520maximize%2520the%2520impact%2520of%2520the%2520adversary%2520on%2520the%2520victims.%2520This%2520is%250Aachieved%2520by%2520adapting%2520a%2520unilateral%2520agent-wise%2520relation%2520metric%2520derived%2520from%250Amutual%2520information%252C%2520thereby%2520mitigating%2520the%2520adverse%2520effects%2520of%2520victim%2520influence%250Aon%2520the%2520adversary.%2520To%2520lead%2520the%2520victims%2520into%2520a%2520jointly%2520detrimental%2520scenario%252C%2520our%250Atargeted%2520attack%2520deceives%2520victims%2520into%2520a%2520long-term%252C%2520cooperatively%2520harmful%250Asituation%2520by%2520guiding%2520each%2520victim%2520towards%2520a%2520specific%2520target%252C%2520determined%2520through%250Aa%2520trial-and-error%2520process%2520executed%2520by%2520a%2520reinforcement%2520learning%2520agent.%2520Through%250AAMI%252C%2520we%2520achieve%2520the%2520first%2520successful%2520attack%2520against%2520real-world%2520robot%2520swarms%2520and%250Aeffectively%2520fool%2520agents%2520in%2520simulated%2520environments%2520into%2520collectively%2520worst-case%250Ascenarios%252C%2520including%2520Starcraft%2520II%2520and%2520Multi-agent%2520Mujoco.%2520The%2520source%2520code%2520and%250Ademonstrations%2520can%2520be%2520found%2520at%253A%2520https%253A//github.com/DIG-Beihang/AMI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.03322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attacking%20Cooperative%20Multi-Agent%20Reinforcement%20Learning%20by%20Adversarial%0A%20%20Minority%20Influence&entry.906535625=Simin%20Li%20and%20Jun%20Guo%20and%20Jingqiao%20Xiu%20and%20Yuwei%20Zheng%20and%20Pu%20Feng%20and%20Xin%20Yu%20and%20Aishan%20Liu%20and%20Yaodong%20Yang%20and%20Bo%20An%20and%20Wenjun%20Wu%20and%20Xianglong%20Liu&entry.1292438233=%20%20This%20study%20probes%20the%20vulnerabilities%20of%20cooperative%20multi-agent%0Areinforcement%20learning%20%28c-MARL%29%20under%20adversarial%20attacks%2C%20a%20critical%0Adeterminant%20of%20c-MARL%27s%20worst-case%20performance%20prior%20to%20real-world%0Aimplementation.%20Current%20observation-based%20attacks%2C%20constrained%20by%20white-box%0Aassumptions%2C%20overlook%20c-MARL%27s%20complex%20multi-agent%20interactions%20and%20cooperative%0Aobjectives%2C%20resulting%20in%20impractical%20and%20limited%20attack%20capabilities.%20To%0Aaddress%20these%20shortcomes%2C%20we%20propose%20Adversarial%20Minority%20Influence%20%28AMI%29%2C%20a%0Apractical%20and%20strong%20for%20c-MARL.%20AMI%20is%20a%20practical%20black-box%20attack%20and%20can%20be%0Alaunched%20without%20knowing%20victim%20parameters.%20AMI%20is%20also%20strong%20by%20considering%0Athe%20complex%20multi-agent%20interaction%20and%20the%20cooperative%20goal%20of%20agents%2C%0Aenabling%20a%20single%20adversarial%20agent%20to%20unilaterally%20misleads%20majority%20victims%0Ato%20form%20targeted%20worst-case%20cooperation.%20This%20mirrors%20minority%20influence%0Aphenomena%20in%20social%20psychology.%20To%20achieve%20maximum%20deviation%20in%20victim%20policies%0Aunder%20complex%20agent-wise%20interactions%2C%20our%20unilateral%20attack%20aims%20to%0Acharacterize%20and%20maximize%20the%20impact%20of%20the%20adversary%20on%20the%20victims.%20This%20is%0Aachieved%20by%20adapting%20a%20unilateral%20agent-wise%20relation%20metric%20derived%20from%0Amutual%20information%2C%20thereby%20mitigating%20the%20adverse%20effects%20of%20victim%20influence%0Aon%20the%20adversary.%20To%20lead%20the%20victims%20into%20a%20jointly%20detrimental%20scenario%2C%20our%0Atargeted%20attack%20deceives%20victims%20into%20a%20long-term%2C%20cooperatively%20harmful%0Asituation%20by%20guiding%20each%20victim%20towards%20a%20specific%20target%2C%20determined%20through%0Aa%20trial-and-error%20process%20executed%20by%20a%20reinforcement%20learning%20agent.%20Through%0AAMI%2C%20we%20achieve%20the%20first%20successful%20attack%20against%20real-world%20robot%20swarms%20and%0Aeffectively%20fool%20agents%20in%20simulated%20environments%20into%20collectively%20worst-case%0Ascenarios%2C%20including%20Starcraft%20II%20and%20Multi-agent%20Mujoco.%20The%20source%20code%20and%0Ademonstrations%20can%20be%20found%20at%3A%20https%3A//github.com/DIG-Beihang/AMI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03322v3&entry.124074799=Read"},
{"title": "ThinK: Thinner Key Cache by Query-Driven Pruning", "author": "Yuhui Xu and Zhanming Jie and Hanze Dong and Lei Wang and Xudong Lu and Aojun Zhou and Amrita Saha and Caiming Xiong and Doyen Sahoo", "abstract": "  Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.\n", "link": "http://arxiv.org/abs/2407.21018v1", "date": "2024-07-30", "relevancy": 1.9353, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5094}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThinK%3A%20Thinner%20Key%20Cache%20by%20Query-Driven%20Pruning&body=Title%3A%20ThinK%3A%20Thinner%20Key%20Cache%20by%20Query-Driven%20Pruning%0AAuthor%3A%20Yuhui%20Xu%20and%20Zhanming%20Jie%20and%20Hanze%20Dong%20and%20Lei%20Wang%20and%20Xudong%20Lu%20and%20Aojun%20Zhou%20and%20Amrita%20Saha%20and%20Caiming%20Xiong%20and%20Doyen%20Sahoo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20field%20of%20natural%0Alanguage%20processing%2C%20achieving%20unprecedented%20performance%20across%20a%20variety%20of%0Aapplications%20by%20leveraging%20increased%20model%20sizes%20and%20sequence%20lengths.%20However%2C%0Athe%20associated%20rise%20in%20computational%20and%20memory%20costs%20poses%20significant%0Achallenges%2C%20particularly%20in%20managing%20long%20sequences%20due%20to%20the%20quadratic%0Acomplexity%20of%20the%20transformer%20attention%20mechanism.%20This%20paper%20focuses%20on%20the%0Along-context%20scenario%2C%20addressing%20the%20inefficiencies%20in%20KV%20cache%20memory%0Aconsumption%20during%20inference.%20Unlike%20existing%20approaches%20that%20optimize%20the%0Amemory%20based%20on%20the%20sequence%20lengths%2C%20we%20uncover%20that%20the%20channel%20dimension%20of%0Athe%20KV%20cache%20exhibits%20significant%20redundancy%2C%20characterized%20by%20unbalanced%0Amagnitude%20distribution%20and%20low-rank%20structure%20in%20attention%20weights.%20Based%20on%0Athese%20observations%2C%20we%20propose%20ThinK%2C%20a%20novel%20query-dependent%20KV%20cache%20pruning%0Amethod%20designed%20to%20minimize%20attention%20weight%20loss%20while%20selectively%20pruning%20the%0Aleast%20significant%20channels.%20Our%20approach%20not%20only%20maintains%20or%20enhances%20model%0Aaccuracy%20but%20also%20achieves%20a%20reduction%20in%20memory%20costs%20by%20over%2020%25%20compared%0Awith%20vanilla%20KV%20cache%20eviction%20methods.%20Extensive%20evaluations%20on%20the%20LLaMA3%20and%0AMistral%20models%20across%20various%20long-sequence%20datasets%20confirm%20the%20efficacy%20of%0AThinK%2C%20setting%20a%20new%20precedent%20for%20efficient%20LLM%20deployment%20without%0Acompromising%20performance.%20We%20also%20outline%20the%20potential%20of%20extending%20our%20method%0Ato%20value%20cache%20pruning%2C%20demonstrating%20ThinK%27s%20versatility%20and%20broad%0Aapplicability%20in%20reducing%20both%20memory%20and%20computational%20overheads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinK%253A%2520Thinner%2520Key%2520Cache%2520by%2520Query-Driven%2520Pruning%26entry.906535625%3DYuhui%2520Xu%2520and%2520Zhanming%2520Jie%2520and%2520Hanze%2520Dong%2520and%2520Lei%2520Wang%2520and%2520Xudong%2520Lu%2520and%2520Aojun%2520Zhou%2520and%2520Amrita%2520Saha%2520and%2520Caiming%2520Xiong%2520and%2520Doyen%2520Sahoo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520the%2520field%2520of%2520natural%250Alanguage%2520processing%252C%2520achieving%2520unprecedented%2520performance%2520across%2520a%2520variety%2520of%250Aapplications%2520by%2520leveraging%2520increased%2520model%2520sizes%2520and%2520sequence%2520lengths.%2520However%252C%250Athe%2520associated%2520rise%2520in%2520computational%2520and%2520memory%2520costs%2520poses%2520significant%250Achallenges%252C%2520particularly%2520in%2520managing%2520long%2520sequences%2520due%2520to%2520the%2520quadratic%250Acomplexity%2520of%2520the%2520transformer%2520attention%2520mechanism.%2520This%2520paper%2520focuses%2520on%2520the%250Along-context%2520scenario%252C%2520addressing%2520the%2520inefficiencies%2520in%2520KV%2520cache%2520memory%250Aconsumption%2520during%2520inference.%2520Unlike%2520existing%2520approaches%2520that%2520optimize%2520the%250Amemory%2520based%2520on%2520the%2520sequence%2520lengths%252C%2520we%2520uncover%2520that%2520the%2520channel%2520dimension%2520of%250Athe%2520KV%2520cache%2520exhibits%2520significant%2520redundancy%252C%2520characterized%2520by%2520unbalanced%250Amagnitude%2520distribution%2520and%2520low-rank%2520structure%2520in%2520attention%2520weights.%2520Based%2520on%250Athese%2520observations%252C%2520we%2520propose%2520ThinK%252C%2520a%2520novel%2520query-dependent%2520KV%2520cache%2520pruning%250Amethod%2520designed%2520to%2520minimize%2520attention%2520weight%2520loss%2520while%2520selectively%2520pruning%2520the%250Aleast%2520significant%2520channels.%2520Our%2520approach%2520not%2520only%2520maintains%2520or%2520enhances%2520model%250Aaccuracy%2520but%2520also%2520achieves%2520a%2520reduction%2520in%2520memory%2520costs%2520by%2520over%252020%2525%2520compared%250Awith%2520vanilla%2520KV%2520cache%2520eviction%2520methods.%2520Extensive%2520evaluations%2520on%2520the%2520LLaMA3%2520and%250AMistral%2520models%2520across%2520various%2520long-sequence%2520datasets%2520confirm%2520the%2520efficacy%2520of%250AThinK%252C%2520setting%2520a%2520new%2520precedent%2520for%2520efficient%2520LLM%2520deployment%2520without%250Acompromising%2520performance.%2520We%2520also%2520outline%2520the%2520potential%2520of%2520extending%2520our%2520method%250Ato%2520value%2520cache%2520pruning%252C%2520demonstrating%2520ThinK%2527s%2520versatility%2520and%2520broad%250Aapplicability%2520in%2520reducing%2520both%2520memory%2520and%2520computational%2520overheads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThinK%3A%20Thinner%20Key%20Cache%20by%20Query-Driven%20Pruning&entry.906535625=Yuhui%20Xu%20and%20Zhanming%20Jie%20and%20Hanze%20Dong%20and%20Lei%20Wang%20and%20Xudong%20Lu%20and%20Aojun%20Zhou%20and%20Amrita%20Saha%20and%20Caiming%20Xiong%20and%20Doyen%20Sahoo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20field%20of%20natural%0Alanguage%20processing%2C%20achieving%20unprecedented%20performance%20across%20a%20variety%20of%0Aapplications%20by%20leveraging%20increased%20model%20sizes%20and%20sequence%20lengths.%20However%2C%0Athe%20associated%20rise%20in%20computational%20and%20memory%20costs%20poses%20significant%0Achallenges%2C%20particularly%20in%20managing%20long%20sequences%20due%20to%20the%20quadratic%0Acomplexity%20of%20the%20transformer%20attention%20mechanism.%20This%20paper%20focuses%20on%20the%0Along-context%20scenario%2C%20addressing%20the%20inefficiencies%20in%20KV%20cache%20memory%0Aconsumption%20during%20inference.%20Unlike%20existing%20approaches%20that%20optimize%20the%0Amemory%20based%20on%20the%20sequence%20lengths%2C%20we%20uncover%20that%20the%20channel%20dimension%20of%0Athe%20KV%20cache%20exhibits%20significant%20redundancy%2C%20characterized%20by%20unbalanced%0Amagnitude%20distribution%20and%20low-rank%20structure%20in%20attention%20weights.%20Based%20on%0Athese%20observations%2C%20we%20propose%20ThinK%2C%20a%20novel%20query-dependent%20KV%20cache%20pruning%0Amethod%20designed%20to%20minimize%20attention%20weight%20loss%20while%20selectively%20pruning%20the%0Aleast%20significant%20channels.%20Our%20approach%20not%20only%20maintains%20or%20enhances%20model%0Aaccuracy%20but%20also%20achieves%20a%20reduction%20in%20memory%20costs%20by%20over%2020%25%20compared%0Awith%20vanilla%20KV%20cache%20eviction%20methods.%20Extensive%20evaluations%20on%20the%20LLaMA3%20and%0AMistral%20models%20across%20various%20long-sequence%20datasets%20confirm%20the%20efficacy%20of%0AThinK%2C%20setting%20a%20new%20precedent%20for%20efficient%20LLM%20deployment%20without%0Acompromising%20performance.%20We%20also%20outline%20the%20potential%20of%20extending%20our%20method%0Ato%20value%20cache%20pruning%2C%20demonstrating%20ThinK%27s%20versatility%20and%20broad%0Aapplicability%20in%20reducing%20both%20memory%20and%20computational%20overheads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21018v1&entry.124074799=Read"},
{"title": "Learn by Selling: Equipping Large Language Models with Product Knowledge\n  for Context-Driven Recommendations", "author": "Sarthak Anand and Yutong Jiang and Giorgi Kokaia", "abstract": "  The rapid evolution of large language models (LLMs) has opened up new\npossibilities for applications such as context-driven product recommendations.\nHowever, the effectiveness of these models in this context is heavily reliant\non their comprehensive understanding of the product inventory. This paper\npresents a novel approach to equipping LLMs with product knowledge by training\nthem to respond contextually to synthetic search queries that include product\nIDs. We delve into an extensive analysis of this method, evaluating its\neffectiveness, outlining its benefits, and highlighting its constraints. The\npaper also discusses the potential improvements and future directions for this\napproach, providing a comprehensive understanding of the role of LLMs in\nproduct recommendations.\n", "link": "http://arxiv.org/abs/2407.20856v1", "date": "2024-07-30", "relevancy": 1.9075, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4618}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20by%20Selling%3A%20Equipping%20Large%20Language%20Models%20with%20Product%20Knowledge%0A%20%20for%20Context-Driven%20Recommendations&body=Title%3A%20Learn%20by%20Selling%3A%20Equipping%20Large%20Language%20Models%20with%20Product%20Knowledge%0A%20%20for%20Context-Driven%20Recommendations%0AAuthor%3A%20Sarthak%20Anand%20and%20Yutong%20Jiang%20and%20Giorgi%20Kokaia%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20large%20language%20models%20%28LLMs%29%20has%20opened%20up%20new%0Apossibilities%20for%20applications%20such%20as%20context-driven%20product%20recommendations.%0AHowever%2C%20the%20effectiveness%20of%20these%20models%20in%20this%20context%20is%20heavily%20reliant%0Aon%20their%20comprehensive%20understanding%20of%20the%20product%20inventory.%20This%20paper%0Apresents%20a%20novel%20approach%20to%20equipping%20LLMs%20with%20product%20knowledge%20by%20training%0Athem%20to%20respond%20contextually%20to%20synthetic%20search%20queries%20that%20include%20product%0AIDs.%20We%20delve%20into%20an%20extensive%20analysis%20of%20this%20method%2C%20evaluating%20its%0Aeffectiveness%2C%20outlining%20its%20benefits%2C%20and%20highlighting%20its%20constraints.%20The%0Apaper%20also%20discusses%20the%20potential%20improvements%20and%20future%20directions%20for%20this%0Aapproach%2C%20providing%20a%20comprehensive%20understanding%20of%20the%20role%20of%20LLMs%20in%0Aproduct%20recommendations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520by%2520Selling%253A%2520Equipping%2520Large%2520Language%2520Models%2520with%2520Product%2520Knowledge%250A%2520%2520for%2520Context-Driven%2520Recommendations%26entry.906535625%3DSarthak%2520Anand%2520and%2520Yutong%2520Jiang%2520and%2520Giorgi%2520Kokaia%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520opened%2520up%2520new%250Apossibilities%2520for%2520applications%2520such%2520as%2520context-driven%2520product%2520recommendations.%250AHowever%252C%2520the%2520effectiveness%2520of%2520these%2520models%2520in%2520this%2520context%2520is%2520heavily%2520reliant%250Aon%2520their%2520comprehensive%2520understanding%2520of%2520the%2520product%2520inventory.%2520This%2520paper%250Apresents%2520a%2520novel%2520approach%2520to%2520equipping%2520LLMs%2520with%2520product%2520knowledge%2520by%2520training%250Athem%2520to%2520respond%2520contextually%2520to%2520synthetic%2520search%2520queries%2520that%2520include%2520product%250AIDs.%2520We%2520delve%2520into%2520an%2520extensive%2520analysis%2520of%2520this%2520method%252C%2520evaluating%2520its%250Aeffectiveness%252C%2520outlining%2520its%2520benefits%252C%2520and%2520highlighting%2520its%2520constraints.%2520The%250Apaper%2520also%2520discusses%2520the%2520potential%2520improvements%2520and%2520future%2520directions%2520for%2520this%250Aapproach%252C%2520providing%2520a%2520comprehensive%2520understanding%2520of%2520the%2520role%2520of%2520LLMs%2520in%250Aproduct%2520recommendations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20by%20Selling%3A%20Equipping%20Large%20Language%20Models%20with%20Product%20Knowledge%0A%20%20for%20Context-Driven%20Recommendations&entry.906535625=Sarthak%20Anand%20and%20Yutong%20Jiang%20and%20Giorgi%20Kokaia&entry.1292438233=%20%20The%20rapid%20evolution%20of%20large%20language%20models%20%28LLMs%29%20has%20opened%20up%20new%0Apossibilities%20for%20applications%20such%20as%20context-driven%20product%20recommendations.%0AHowever%2C%20the%20effectiveness%20of%20these%20models%20in%20this%20context%20is%20heavily%20reliant%0Aon%20their%20comprehensive%20understanding%20of%20the%20product%20inventory.%20This%20paper%0Apresents%20a%20novel%20approach%20to%20equipping%20LLMs%20with%20product%20knowledge%20by%20training%0Athem%20to%20respond%20contextually%20to%20synthetic%20search%20queries%20that%20include%20product%0AIDs.%20We%20delve%20into%20an%20extensive%20analysis%20of%20this%20method%2C%20evaluating%20its%0Aeffectiveness%2C%20outlining%20its%20benefits%2C%20and%20highlighting%20its%20constraints.%20The%0Apaper%20also%20discusses%20the%20potential%20improvements%20and%20future%20directions%20for%20this%0Aapproach%2C%20providing%20a%20comprehensive%20understanding%20of%20the%20role%20of%20LLMs%20in%0Aproduct%20recommendations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20856v1&entry.124074799=Read"},
{"title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM\n  Fine-Tuning", "author": "Yupeng Chen and Senmiao Wang and Zhihang Lin and Zeyu Qin and Yushun Zhang and Tian Ding and Ruoyu Sun", "abstract": "  Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in a wide range of tasks. Typically, an LLM is pre-trained on\nlarge corpora and subsequently fine-tuned on task-specific datasets. However,\nduring finetuning, LLMs may forget the knowledge acquired in the pretraining\nstage, leading to a decline in general capabilities. To address this issue, we\npropose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).\nThe key idea of MoFO is to iteratively select and update the model parameters\nwith the largest momentum magnitudes. Compared to full-parameter training, MoFO\nachieves similar fine-tuning performance while keeping parameters closer to the\npre-trained model, thereby mitigating knowledge forgetting. Unlike most\nexisting methods for forgetting mitigation, MoFO combines the following two\nadvantages. First, MoFO does not require access to pre-training data. This\nmakes MoFO particularly suitable for fine-tuning scenarios where pre-training\ndata is unavailable, such as fine-tuning checkpoint-only open-source LLMs.\nSecond, MoFO does not alter the original loss function. This could avoid\nimpairing the model performance on the fine-tuning tasks. We validate MoFO\nthrough rigorous convergence analysis and extensive experiments, demonstrating\nits superiority over existing methods in mitigating forgetting and enhancing\nfine-tuning performance.\n", "link": "http://arxiv.org/abs/2407.20999v1", "date": "2024-07-30", "relevancy": 1.889, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.473}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoFO%3A%20Momentum-Filtered%20Optimizer%20for%20Mitigating%20Forgetting%20in%20LLM%0A%20%20Fine-Tuning&body=Title%3A%20MoFO%3A%20Momentum-Filtered%20Optimizer%20for%20Mitigating%20Forgetting%20in%20LLM%0A%20%20Fine-Tuning%0AAuthor%3A%20Yupeng%20Chen%20and%20Senmiao%20Wang%20and%20Zhihang%20Lin%20and%20Zeyu%20Qin%20and%20Yushun%20Zhang%20and%20Tian%20Ding%20and%20Ruoyu%20Sun%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20a%20wide%20range%20of%20tasks.%20Typically%2C%20an%20LLM%20is%20pre-trained%20on%0Alarge%20corpora%20and%20subsequently%20fine-tuned%20on%20task-specific%20datasets.%20However%2C%0Aduring%20finetuning%2C%20LLMs%20may%20forget%20the%20knowledge%20acquired%20in%20the%20pretraining%0Astage%2C%20leading%20to%20a%20decline%20in%20general%20capabilities.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20new%20fine-tuning%20algorithm%20termed%20Momentum-Filtered%20Optimizer%20%28MoFO%29.%0AThe%20key%20idea%20of%20MoFO%20is%20to%20iteratively%20select%20and%20update%20the%20model%20parameters%0Awith%20the%20largest%20momentum%20magnitudes.%20Compared%20to%20full-parameter%20training%2C%20MoFO%0Aachieves%20similar%20fine-tuning%20performance%20while%20keeping%20parameters%20closer%20to%20the%0Apre-trained%20model%2C%20thereby%20mitigating%20knowledge%20forgetting.%20Unlike%20most%0Aexisting%20methods%20for%20forgetting%20mitigation%2C%20MoFO%20combines%20the%20following%20two%0Aadvantages.%20First%2C%20MoFO%20does%20not%20require%20access%20to%20pre-training%20data.%20This%0Amakes%20MoFO%20particularly%20suitable%20for%20fine-tuning%20scenarios%20where%20pre-training%0Adata%20is%20unavailable%2C%20such%20as%20fine-tuning%20checkpoint-only%20open-source%20LLMs.%0ASecond%2C%20MoFO%20does%20not%20alter%20the%20original%20loss%20function.%20This%20could%20avoid%0Aimpairing%20the%20model%20performance%20on%20the%20fine-tuning%20tasks.%20We%20validate%20MoFO%0Athrough%20rigorous%20convergence%20analysis%20and%20extensive%20experiments%2C%20demonstrating%0Aits%20superiority%20over%20existing%20methods%20in%20mitigating%20forgetting%20and%20enhancing%0Afine-tuning%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoFO%253A%2520Momentum-Filtered%2520Optimizer%2520for%2520Mitigating%2520Forgetting%2520in%2520LLM%250A%2520%2520Fine-Tuning%26entry.906535625%3DYupeng%2520Chen%2520and%2520Senmiao%2520Wang%2520and%2520Zhihang%2520Lin%2520and%2520Zeyu%2520Qin%2520and%2520Yushun%2520Zhang%2520and%2520Tian%2520Ding%2520and%2520Ruoyu%2520Sun%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520a%2520wide%2520range%2520of%2520tasks.%2520Typically%252C%2520an%2520LLM%2520is%2520pre-trained%2520on%250Alarge%2520corpora%2520and%2520subsequently%2520fine-tuned%2520on%2520task-specific%2520datasets.%2520However%252C%250Aduring%2520finetuning%252C%2520LLMs%2520may%2520forget%2520the%2520knowledge%2520acquired%2520in%2520the%2520pretraining%250Astage%252C%2520leading%2520to%2520a%2520decline%2520in%2520general%2520capabilities.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520new%2520fine-tuning%2520algorithm%2520termed%2520Momentum-Filtered%2520Optimizer%2520%2528MoFO%2529.%250AThe%2520key%2520idea%2520of%2520MoFO%2520is%2520to%2520iteratively%2520select%2520and%2520update%2520the%2520model%2520parameters%250Awith%2520the%2520largest%2520momentum%2520magnitudes.%2520Compared%2520to%2520full-parameter%2520training%252C%2520MoFO%250Aachieves%2520similar%2520fine-tuning%2520performance%2520while%2520keeping%2520parameters%2520closer%2520to%2520the%250Apre-trained%2520model%252C%2520thereby%2520mitigating%2520knowledge%2520forgetting.%2520Unlike%2520most%250Aexisting%2520methods%2520for%2520forgetting%2520mitigation%252C%2520MoFO%2520combines%2520the%2520following%2520two%250Aadvantages.%2520First%252C%2520MoFO%2520does%2520not%2520require%2520access%2520to%2520pre-training%2520data.%2520This%250Amakes%2520MoFO%2520particularly%2520suitable%2520for%2520fine-tuning%2520scenarios%2520where%2520pre-training%250Adata%2520is%2520unavailable%252C%2520such%2520as%2520fine-tuning%2520checkpoint-only%2520open-source%2520LLMs.%250ASecond%252C%2520MoFO%2520does%2520not%2520alter%2520the%2520original%2520loss%2520function.%2520This%2520could%2520avoid%250Aimpairing%2520the%2520model%2520performance%2520on%2520the%2520fine-tuning%2520tasks.%2520We%2520validate%2520MoFO%250Athrough%2520rigorous%2520convergence%2520analysis%2520and%2520extensive%2520experiments%252C%2520demonstrating%250Aits%2520superiority%2520over%2520existing%2520methods%2520in%2520mitigating%2520forgetting%2520and%2520enhancing%250Afine-tuning%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoFO%3A%20Momentum-Filtered%20Optimizer%20for%20Mitigating%20Forgetting%20in%20LLM%0A%20%20Fine-Tuning&entry.906535625=Yupeng%20Chen%20and%20Senmiao%20Wang%20and%20Zhihang%20Lin%20and%20Zeyu%20Qin%20and%20Yushun%20Zhang%20and%20Tian%20Ding%20and%20Ruoyu%20Sun&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20a%20wide%20range%20of%20tasks.%20Typically%2C%20an%20LLM%20is%20pre-trained%20on%0Alarge%20corpora%20and%20subsequently%20fine-tuned%20on%20task-specific%20datasets.%20However%2C%0Aduring%20finetuning%2C%20LLMs%20may%20forget%20the%20knowledge%20acquired%20in%20the%20pretraining%0Astage%2C%20leading%20to%20a%20decline%20in%20general%20capabilities.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20new%20fine-tuning%20algorithm%20termed%20Momentum-Filtered%20Optimizer%20%28MoFO%29.%0AThe%20key%20idea%20of%20MoFO%20is%20to%20iteratively%20select%20and%20update%20the%20model%20parameters%0Awith%20the%20largest%20momentum%20magnitudes.%20Compared%20to%20full-parameter%20training%2C%20MoFO%0Aachieves%20similar%20fine-tuning%20performance%20while%20keeping%20parameters%20closer%20to%20the%0Apre-trained%20model%2C%20thereby%20mitigating%20knowledge%20forgetting.%20Unlike%20most%0Aexisting%20methods%20for%20forgetting%20mitigation%2C%20MoFO%20combines%20the%20following%20two%0Aadvantages.%20First%2C%20MoFO%20does%20not%20require%20access%20to%20pre-training%20data.%20This%0Amakes%20MoFO%20particularly%20suitable%20for%20fine-tuning%20scenarios%20where%20pre-training%0Adata%20is%20unavailable%2C%20such%20as%20fine-tuning%20checkpoint-only%20open-source%20LLMs.%0ASecond%2C%20MoFO%20does%20not%20alter%20the%20original%20loss%20function.%20This%20could%20avoid%0Aimpairing%20the%20model%20performance%20on%20the%20fine-tuning%20tasks.%20We%20validate%20MoFO%0Athrough%20rigorous%20convergence%20analysis%20and%20extensive%20experiments%2C%20demonstrating%0Aits%20superiority%20over%20existing%20methods%20in%20mitigating%20forgetting%20and%20enhancing%0Afine-tuning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20999v1&entry.124074799=Read"},
{"title": "DyGKT: Dynamic Graph Learning for Knowledge Tracing", "author": "Ke Cheng and Linzhi Peng and Pengyang Wang and Junchen Ye and Leilei Sun and Bowen Du", "abstract": "  Knowledge Tracing aims to assess student learning states by predicting their\nperformance in answering questions. Different from the existing research which\nutilizes fixed-length learning sequence to obtain the student states and\nregards KT as a static problem, this work is motivated by three dynamical\ncharacteristics: 1) The scales of students answering records are constantly\ngrowing; 2) The semantics of time intervals between the records vary; 3) The\nrelationships between students, questions and concepts are evolving. The three\ndynamical characteristics above contain the great potential to revolutionize\nthe existing knowledge tracing methods. Along this line, we propose a Dynamic\nGraph-based Knowledge Tracing model, namely DyGKT. In particular, a\ncontinuous-time dynamic question-answering graph for knowledge tracing is\nconstructed to deal with the infinitely growing answering behaviors, and it is\nworth mentioning that it is the first time dynamic graph learning technology is\nused in this field. Then, a dual time encoder is proposed to capture long-term\nand short-term semantics among the different time intervals. Finally, a\nmultiset indicator is utilized to model the evolving relationships between\nstudents, questions, and concepts via the graph structural feature. Numerous\nexperiments are conducted on five real-world datasets, and the results\ndemonstrate the superiority of our model. All the used resources are publicly\navailable at https://github.com/PengLinzhi/DyGKT.\n", "link": "http://arxiv.org/abs/2407.20824v1", "date": "2024-07-30", "relevancy": 1.8809, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4855}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4603}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyGKT%3A%20Dynamic%20Graph%20Learning%20for%20Knowledge%20Tracing&body=Title%3A%20DyGKT%3A%20Dynamic%20Graph%20Learning%20for%20Knowledge%20Tracing%0AAuthor%3A%20Ke%20Cheng%20and%20Linzhi%20Peng%20and%20Pengyang%20Wang%20and%20Junchen%20Ye%20and%20Leilei%20Sun%20and%20Bowen%20Du%0AAbstract%3A%20%20%20Knowledge%20Tracing%20aims%20to%20assess%20student%20learning%20states%20by%20predicting%20their%0Aperformance%20in%20answering%20questions.%20Different%20from%20the%20existing%20research%20which%0Autilizes%20fixed-length%20learning%20sequence%20to%20obtain%20the%20student%20states%20and%0Aregards%20KT%20as%20a%20static%20problem%2C%20this%20work%20is%20motivated%20by%20three%20dynamical%0Acharacteristics%3A%201%29%20The%20scales%20of%20students%20answering%20records%20are%20constantly%0Agrowing%3B%202%29%20The%20semantics%20of%20time%20intervals%20between%20the%20records%20vary%3B%203%29%20The%0Arelationships%20between%20students%2C%20questions%20and%20concepts%20are%20evolving.%20The%20three%0Adynamical%20characteristics%20above%20contain%20the%20great%20potential%20to%20revolutionize%0Athe%20existing%20knowledge%20tracing%20methods.%20Along%20this%20line%2C%20we%20propose%20a%20Dynamic%0AGraph-based%20Knowledge%20Tracing%20model%2C%20namely%20DyGKT.%20In%20particular%2C%20a%0Acontinuous-time%20dynamic%20question-answering%20graph%20for%20knowledge%20tracing%20is%0Aconstructed%20to%20deal%20with%20the%20infinitely%20growing%20answering%20behaviors%2C%20and%20it%20is%0Aworth%20mentioning%20that%20it%20is%20the%20first%20time%20dynamic%20graph%20learning%20technology%20is%0Aused%20in%20this%20field.%20Then%2C%20a%20dual%20time%20encoder%20is%20proposed%20to%20capture%20long-term%0Aand%20short-term%20semantics%20among%20the%20different%20time%20intervals.%20Finally%2C%20a%0Amultiset%20indicator%20is%20utilized%20to%20model%20the%20evolving%20relationships%20between%0Astudents%2C%20questions%2C%20and%20concepts%20via%20the%20graph%20structural%20feature.%20Numerous%0Aexperiments%20are%20conducted%20on%20five%20real-world%20datasets%2C%20and%20the%20results%0Ademonstrate%20the%20superiority%20of%20our%20model.%20All%20the%20used%20resources%20are%20publicly%0Aavailable%20at%20https%3A//github.com/PengLinzhi/DyGKT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyGKT%253A%2520Dynamic%2520Graph%2520Learning%2520for%2520Knowledge%2520Tracing%26entry.906535625%3DKe%2520Cheng%2520and%2520Linzhi%2520Peng%2520and%2520Pengyang%2520Wang%2520and%2520Junchen%2520Ye%2520and%2520Leilei%2520Sun%2520and%2520Bowen%2520Du%26entry.1292438233%3D%2520%2520Knowledge%2520Tracing%2520aims%2520to%2520assess%2520student%2520learning%2520states%2520by%2520predicting%2520their%250Aperformance%2520in%2520answering%2520questions.%2520Different%2520from%2520the%2520existing%2520research%2520which%250Autilizes%2520fixed-length%2520learning%2520sequence%2520to%2520obtain%2520the%2520student%2520states%2520and%250Aregards%2520KT%2520as%2520a%2520static%2520problem%252C%2520this%2520work%2520is%2520motivated%2520by%2520three%2520dynamical%250Acharacteristics%253A%25201%2529%2520The%2520scales%2520of%2520students%2520answering%2520records%2520are%2520constantly%250Agrowing%253B%25202%2529%2520The%2520semantics%2520of%2520time%2520intervals%2520between%2520the%2520records%2520vary%253B%25203%2529%2520The%250Arelationships%2520between%2520students%252C%2520questions%2520and%2520concepts%2520are%2520evolving.%2520The%2520three%250Adynamical%2520characteristics%2520above%2520contain%2520the%2520great%2520potential%2520to%2520revolutionize%250Athe%2520existing%2520knowledge%2520tracing%2520methods.%2520Along%2520this%2520line%252C%2520we%2520propose%2520a%2520Dynamic%250AGraph-based%2520Knowledge%2520Tracing%2520model%252C%2520namely%2520DyGKT.%2520In%2520particular%252C%2520a%250Acontinuous-time%2520dynamic%2520question-answering%2520graph%2520for%2520knowledge%2520tracing%2520is%250Aconstructed%2520to%2520deal%2520with%2520the%2520infinitely%2520growing%2520answering%2520behaviors%252C%2520and%2520it%2520is%250Aworth%2520mentioning%2520that%2520it%2520is%2520the%2520first%2520time%2520dynamic%2520graph%2520learning%2520technology%2520is%250Aused%2520in%2520this%2520field.%2520Then%252C%2520a%2520dual%2520time%2520encoder%2520is%2520proposed%2520to%2520capture%2520long-term%250Aand%2520short-term%2520semantics%2520among%2520the%2520different%2520time%2520intervals.%2520Finally%252C%2520a%250Amultiset%2520indicator%2520is%2520utilized%2520to%2520model%2520the%2520evolving%2520relationships%2520between%250Astudents%252C%2520questions%252C%2520and%2520concepts%2520via%2520the%2520graph%2520structural%2520feature.%2520Numerous%250Aexperiments%2520are%2520conducted%2520on%2520five%2520real-world%2520datasets%252C%2520and%2520the%2520results%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520model.%2520All%2520the%2520used%2520resources%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/PengLinzhi/DyGKT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyGKT%3A%20Dynamic%20Graph%20Learning%20for%20Knowledge%20Tracing&entry.906535625=Ke%20Cheng%20and%20Linzhi%20Peng%20and%20Pengyang%20Wang%20and%20Junchen%20Ye%20and%20Leilei%20Sun%20and%20Bowen%20Du&entry.1292438233=%20%20Knowledge%20Tracing%20aims%20to%20assess%20student%20learning%20states%20by%20predicting%20their%0Aperformance%20in%20answering%20questions.%20Different%20from%20the%20existing%20research%20which%0Autilizes%20fixed-length%20learning%20sequence%20to%20obtain%20the%20student%20states%20and%0Aregards%20KT%20as%20a%20static%20problem%2C%20this%20work%20is%20motivated%20by%20three%20dynamical%0Acharacteristics%3A%201%29%20The%20scales%20of%20students%20answering%20records%20are%20constantly%0Agrowing%3B%202%29%20The%20semantics%20of%20time%20intervals%20between%20the%20records%20vary%3B%203%29%20The%0Arelationships%20between%20students%2C%20questions%20and%20concepts%20are%20evolving.%20The%20three%0Adynamical%20characteristics%20above%20contain%20the%20great%20potential%20to%20revolutionize%0Athe%20existing%20knowledge%20tracing%20methods.%20Along%20this%20line%2C%20we%20propose%20a%20Dynamic%0AGraph-based%20Knowledge%20Tracing%20model%2C%20namely%20DyGKT.%20In%20particular%2C%20a%0Acontinuous-time%20dynamic%20question-answering%20graph%20for%20knowledge%20tracing%20is%0Aconstructed%20to%20deal%20with%20the%20infinitely%20growing%20answering%20behaviors%2C%20and%20it%20is%0Aworth%20mentioning%20that%20it%20is%20the%20first%20time%20dynamic%20graph%20learning%20technology%20is%0Aused%20in%20this%20field.%20Then%2C%20a%20dual%20time%20encoder%20is%20proposed%20to%20capture%20long-term%0Aand%20short-term%20semantics%20among%20the%20different%20time%20intervals.%20Finally%2C%20a%0Amultiset%20indicator%20is%20utilized%20to%20model%20the%20evolving%20relationships%20between%0Astudents%2C%20questions%2C%20and%20concepts%20via%20the%20graph%20structural%20feature.%20Numerous%0Aexperiments%20are%20conducted%20on%20five%20real-world%20datasets%2C%20and%20the%20results%0Ademonstrate%20the%20superiority%20of%20our%20model.%20All%20the%20used%20resources%20are%20publicly%0Aavailable%20at%20https%3A//github.com/PengLinzhi/DyGKT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20824v1&entry.124074799=Read"},
{"title": "Large Language Models (LLMs) as Agents for Augmented Democracy", "author": "Jairo Gudi\u00f1o-Rosero and Umberto Grandi and C\u00e9sar A. Hidalgo", "abstract": "  We explore an augmented democracy system built on off-the-shelf LLMs\nfine-tuned to augment data on citizen's preferences elicited over policies\nextracted from the government programs of the two main candidates of Brazil's\n2022 presidential election. We use a train-test cross-validation setup to\nestimate the accuracy with which the LLMs predict both: a subject's individual\npolitical choices and the aggregate preferences of the full sample of\nparticipants. At the individual level, we find that LLMs predict out of sample\npreferences more accurately than a \"bundle rule\", which would assume that\ncitizens always vote for the proposals of the candidate aligned with their\nself-reported political orientation. At the population level, we show that a\nprobabilistic sample augmented by an LLM provides a more accurate estimate of\nthe aggregate preferences of a population than the non-augmented probabilistic\nsample alone. Together, these results indicates that policy preference data\naugmented using LLMs can capture nuances that transcend party lines and\nrepresents a promising avenue of research for data augmentation.\n", "link": "http://arxiv.org/abs/2405.03452v3", "date": "2024-07-30", "relevancy": 1.8709, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4907}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4702}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20%28LLMs%29%20as%20Agents%20for%20Augmented%20Democracy&body=Title%3A%20Large%20Language%20Models%20%28LLMs%29%20as%20Agents%20for%20Augmented%20Democracy%0AAuthor%3A%20Jairo%20Gudi%C3%B1o-Rosero%20and%20Umberto%20Grandi%20and%20C%C3%A9sar%20A.%20Hidalgo%0AAbstract%3A%20%20%20We%20explore%20an%20augmented%20democracy%20system%20built%20on%20off-the-shelf%20LLMs%0Afine-tuned%20to%20augment%20data%20on%20citizen%27s%20preferences%20elicited%20over%20policies%0Aextracted%20from%20the%20government%20programs%20of%20the%20two%20main%20candidates%20of%20Brazil%27s%0A2022%20presidential%20election.%20We%20use%20a%20train-test%20cross-validation%20setup%20to%0Aestimate%20the%20accuracy%20with%20which%20the%20LLMs%20predict%20both%3A%20a%20subject%27s%20individual%0Apolitical%20choices%20and%20the%20aggregate%20preferences%20of%20the%20full%20sample%20of%0Aparticipants.%20At%20the%20individual%20level%2C%20we%20find%20that%20LLMs%20predict%20out%20of%20sample%0Apreferences%20more%20accurately%20than%20a%20%22bundle%20rule%22%2C%20which%20would%20assume%20that%0Acitizens%20always%20vote%20for%20the%20proposals%20of%20the%20candidate%20aligned%20with%20their%0Aself-reported%20political%20orientation.%20At%20the%20population%20level%2C%20we%20show%20that%20a%0Aprobabilistic%20sample%20augmented%20by%20an%20LLM%20provides%20a%20more%20accurate%20estimate%20of%0Athe%20aggregate%20preferences%20of%20a%20population%20than%20the%20non-augmented%20probabilistic%0Asample%20alone.%20Together%2C%20these%20results%20indicates%20that%20policy%20preference%20data%0Aaugmented%20using%20LLMs%20can%20capture%20nuances%20that%20transcend%20party%20lines%20and%0Arepresents%20a%20promising%20avenue%20of%20research%20for%20data%20augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03452v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520as%2520Agents%2520for%2520Augmented%2520Democracy%26entry.906535625%3DJairo%2520Gudi%25C3%25B1o-Rosero%2520and%2520Umberto%2520Grandi%2520and%2520C%25C3%25A9sar%2520A.%2520Hidalgo%26entry.1292438233%3D%2520%2520We%2520explore%2520an%2520augmented%2520democracy%2520system%2520built%2520on%2520off-the-shelf%2520LLMs%250Afine-tuned%2520to%2520augment%2520data%2520on%2520citizen%2527s%2520preferences%2520elicited%2520over%2520policies%250Aextracted%2520from%2520the%2520government%2520programs%2520of%2520the%2520two%2520main%2520candidates%2520of%2520Brazil%2527s%250A2022%2520presidential%2520election.%2520We%2520use%2520a%2520train-test%2520cross-validation%2520setup%2520to%250Aestimate%2520the%2520accuracy%2520with%2520which%2520the%2520LLMs%2520predict%2520both%253A%2520a%2520subject%2527s%2520individual%250Apolitical%2520choices%2520and%2520the%2520aggregate%2520preferences%2520of%2520the%2520full%2520sample%2520of%250Aparticipants.%2520At%2520the%2520individual%2520level%252C%2520we%2520find%2520that%2520LLMs%2520predict%2520out%2520of%2520sample%250Apreferences%2520more%2520accurately%2520than%2520a%2520%2522bundle%2520rule%2522%252C%2520which%2520would%2520assume%2520that%250Acitizens%2520always%2520vote%2520for%2520the%2520proposals%2520of%2520the%2520candidate%2520aligned%2520with%2520their%250Aself-reported%2520political%2520orientation.%2520At%2520the%2520population%2520level%252C%2520we%2520show%2520that%2520a%250Aprobabilistic%2520sample%2520augmented%2520by%2520an%2520LLM%2520provides%2520a%2520more%2520accurate%2520estimate%2520of%250Athe%2520aggregate%2520preferences%2520of%2520a%2520population%2520than%2520the%2520non-augmented%2520probabilistic%250Asample%2520alone.%2520Together%252C%2520these%2520results%2520indicates%2520that%2520policy%2520preference%2520data%250Aaugmented%2520using%2520LLMs%2520can%2520capture%2520nuances%2520that%2520transcend%2520party%2520lines%2520and%250Arepresents%2520a%2520promising%2520avenue%2520of%2520research%2520for%2520data%2520augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03452v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20%28LLMs%29%20as%20Agents%20for%20Augmented%20Democracy&entry.906535625=Jairo%20Gudi%C3%B1o-Rosero%20and%20Umberto%20Grandi%20and%20C%C3%A9sar%20A.%20Hidalgo&entry.1292438233=%20%20We%20explore%20an%20augmented%20democracy%20system%20built%20on%20off-the-shelf%20LLMs%0Afine-tuned%20to%20augment%20data%20on%20citizen%27s%20preferences%20elicited%20over%20policies%0Aextracted%20from%20the%20government%20programs%20of%20the%20two%20main%20candidates%20of%20Brazil%27s%0A2022%20presidential%20election.%20We%20use%20a%20train-test%20cross-validation%20setup%20to%0Aestimate%20the%20accuracy%20with%20which%20the%20LLMs%20predict%20both%3A%20a%20subject%27s%20individual%0Apolitical%20choices%20and%20the%20aggregate%20preferences%20of%20the%20full%20sample%20of%0Aparticipants.%20At%20the%20individual%20level%2C%20we%20find%20that%20LLMs%20predict%20out%20of%20sample%0Apreferences%20more%20accurately%20than%20a%20%22bundle%20rule%22%2C%20which%20would%20assume%20that%0Acitizens%20always%20vote%20for%20the%20proposals%20of%20the%20candidate%20aligned%20with%20their%0Aself-reported%20political%20orientation.%20At%20the%20population%20level%2C%20we%20show%20that%20a%0Aprobabilistic%20sample%20augmented%20by%20an%20LLM%20provides%20a%20more%20accurate%20estimate%20of%0Athe%20aggregate%20preferences%20of%20a%20population%20than%20the%20non-augmented%20probabilistic%0Asample%20alone.%20Together%2C%20these%20results%20indicates%20that%20policy%20preference%20data%0Aaugmented%20using%20LLMs%20can%20capture%20nuances%20that%20transcend%20party%20lines%20and%0Arepresents%20a%20promising%20avenue%20of%20research%20for%20data%20augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03452v3&entry.124074799=Read"},
{"title": "Add-SD: Rational Generation without Manual Reference", "author": "Lingfeng Yang and Xinyu Zhang and Xiang Li and Jinwen Chen and Kun Yao and Gang Zhang and Errui Ding and Lingqiao Liu and Jingdong Wang and Jian Yang", "abstract": "  Diffusion models have exhibited remarkable prowess in visual generalization.\nBuilding on this success, we introduce an instruction-based object addition\npipeline, named Add-SD, which automatically inserts objects into realistic\nscenes with rational sizes and positions. Different from layout-conditioned\nmethods, Add-SD is solely conditioned on simple text prompts rather than any\nother human-costly references like bounding boxes. Our work contributes in\nthree aspects: proposing a dataset containing numerous instructed image pairs;\nfine-tuning a diffusion model for rational generation; and generating synthetic\ndata to boost downstream tasks. The first aspect involves creating a\nRemovalDataset consisting of original-edited image pairs with textual\ninstructions, where an object has been removed from the original image while\nmaintaining strong pixel consistency in the background. These data pairs are\nthen used for fine-tuning the Stable Diffusion (SD) model. Subsequently, the\npretrained Add-SD model allows for the insertion of expected objects into an\nimage with good rationale. Additionally, we generate synthetic instances for\ndownstream task datasets at scale, particularly for tail classes, to alleviate\nthe long-tailed problem. Downstream tasks benefit from the enriched dataset\nwith enhanced diversity and rationale. Experiments on LVIS val demonstrate that\nAdd-SD yields an improvement of 4.3 mAP on rare classes over the baseline. Code\nand models are available at https://github.com/ylingfeng/Add-SD.\n", "link": "http://arxiv.org/abs/2407.21016v1", "date": "2024-07-30", "relevancy": 1.8629, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6734}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6084}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Add-SD%3A%20Rational%20Generation%20without%20Manual%20Reference&body=Title%3A%20Add-SD%3A%20Rational%20Generation%20without%20Manual%20Reference%0AAuthor%3A%20Lingfeng%20Yang%20and%20Xinyu%20Zhang%20and%20Xiang%20Li%20and%20Jinwen%20Chen%20and%20Kun%20Yao%20and%20Gang%20Zhang%20and%20Errui%20Ding%20and%20Lingqiao%20Liu%20and%20Jingdong%20Wang%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20exhibited%20remarkable%20prowess%20in%20visual%20generalization.%0ABuilding%20on%20this%20success%2C%20we%20introduce%20an%20instruction-based%20object%20addition%0Apipeline%2C%20named%20Add-SD%2C%20which%20automatically%20inserts%20objects%20into%20realistic%0Ascenes%20with%20rational%20sizes%20and%20positions.%20Different%20from%20layout-conditioned%0Amethods%2C%20Add-SD%20is%20solely%20conditioned%20on%20simple%20text%20prompts%20rather%20than%20any%0Aother%20human-costly%20references%20like%20bounding%20boxes.%20Our%20work%20contributes%20in%0Athree%20aspects%3A%20proposing%20a%20dataset%20containing%20numerous%20instructed%20image%20pairs%3B%0Afine-tuning%20a%20diffusion%20model%20for%20rational%20generation%3B%20and%20generating%20synthetic%0Adata%20to%20boost%20downstream%20tasks.%20The%20first%20aspect%20involves%20creating%20a%0ARemovalDataset%20consisting%20of%20original-edited%20image%20pairs%20with%20textual%0Ainstructions%2C%20where%20an%20object%20has%20been%20removed%20from%20the%20original%20image%20while%0Amaintaining%20strong%20pixel%20consistency%20in%20the%20background.%20These%20data%20pairs%20are%0Athen%20used%20for%20fine-tuning%20the%20Stable%20Diffusion%20%28SD%29%20model.%20Subsequently%2C%20the%0Apretrained%20Add-SD%20model%20allows%20for%20the%20insertion%20of%20expected%20objects%20into%20an%0Aimage%20with%20good%20rationale.%20Additionally%2C%20we%20generate%20synthetic%20instances%20for%0Adownstream%20task%20datasets%20at%20scale%2C%20particularly%20for%20tail%20classes%2C%20to%20alleviate%0Athe%20long-tailed%20problem.%20Downstream%20tasks%20benefit%20from%20the%20enriched%20dataset%0Awith%20enhanced%20diversity%20and%20rationale.%20Experiments%20on%20LVIS%20val%20demonstrate%20that%0AAdd-SD%20yields%20an%20improvement%20of%204.3%20mAP%20on%20rare%20classes%20over%20the%20baseline.%20Code%0Aand%20models%20are%20available%20at%20https%3A//github.com/ylingfeng/Add-SD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdd-SD%253A%2520Rational%2520Generation%2520without%2520Manual%2520Reference%26entry.906535625%3DLingfeng%2520Yang%2520and%2520Xinyu%2520Zhang%2520and%2520Xiang%2520Li%2520and%2520Jinwen%2520Chen%2520and%2520Kun%2520Yao%2520and%2520Gang%2520Zhang%2520and%2520Errui%2520Ding%2520and%2520Lingqiao%2520Liu%2520and%2520Jingdong%2520Wang%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520exhibited%2520remarkable%2520prowess%2520in%2520visual%2520generalization.%250ABuilding%2520on%2520this%2520success%252C%2520we%2520introduce%2520an%2520instruction-based%2520object%2520addition%250Apipeline%252C%2520named%2520Add-SD%252C%2520which%2520automatically%2520inserts%2520objects%2520into%2520realistic%250Ascenes%2520with%2520rational%2520sizes%2520and%2520positions.%2520Different%2520from%2520layout-conditioned%250Amethods%252C%2520Add-SD%2520is%2520solely%2520conditioned%2520on%2520simple%2520text%2520prompts%2520rather%2520than%2520any%250Aother%2520human-costly%2520references%2520like%2520bounding%2520boxes.%2520Our%2520work%2520contributes%2520in%250Athree%2520aspects%253A%2520proposing%2520a%2520dataset%2520containing%2520numerous%2520instructed%2520image%2520pairs%253B%250Afine-tuning%2520a%2520diffusion%2520model%2520for%2520rational%2520generation%253B%2520and%2520generating%2520synthetic%250Adata%2520to%2520boost%2520downstream%2520tasks.%2520The%2520first%2520aspect%2520involves%2520creating%2520a%250ARemovalDataset%2520consisting%2520of%2520original-edited%2520image%2520pairs%2520with%2520textual%250Ainstructions%252C%2520where%2520an%2520object%2520has%2520been%2520removed%2520from%2520the%2520original%2520image%2520while%250Amaintaining%2520strong%2520pixel%2520consistency%2520in%2520the%2520background.%2520These%2520data%2520pairs%2520are%250Athen%2520used%2520for%2520fine-tuning%2520the%2520Stable%2520Diffusion%2520%2528SD%2529%2520model.%2520Subsequently%252C%2520the%250Apretrained%2520Add-SD%2520model%2520allows%2520for%2520the%2520insertion%2520of%2520expected%2520objects%2520into%2520an%250Aimage%2520with%2520good%2520rationale.%2520Additionally%252C%2520we%2520generate%2520synthetic%2520instances%2520for%250Adownstream%2520task%2520datasets%2520at%2520scale%252C%2520particularly%2520for%2520tail%2520classes%252C%2520to%2520alleviate%250Athe%2520long-tailed%2520problem.%2520Downstream%2520tasks%2520benefit%2520from%2520the%2520enriched%2520dataset%250Awith%2520enhanced%2520diversity%2520and%2520rationale.%2520Experiments%2520on%2520LVIS%2520val%2520demonstrate%2520that%250AAdd-SD%2520yields%2520an%2520improvement%2520of%25204.3%2520mAP%2520on%2520rare%2520classes%2520over%2520the%2520baseline.%2520Code%250Aand%2520models%2520are%2520available%2520at%2520https%253A//github.com/ylingfeng/Add-SD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Add-SD%3A%20Rational%20Generation%20without%20Manual%20Reference&entry.906535625=Lingfeng%20Yang%20and%20Xinyu%20Zhang%20and%20Xiang%20Li%20and%20Jinwen%20Chen%20and%20Kun%20Yao%20and%20Gang%20Zhang%20and%20Errui%20Ding%20and%20Lingqiao%20Liu%20and%20Jingdong%20Wang%20and%20Jian%20Yang&entry.1292438233=%20%20Diffusion%20models%20have%20exhibited%20remarkable%20prowess%20in%20visual%20generalization.%0ABuilding%20on%20this%20success%2C%20we%20introduce%20an%20instruction-based%20object%20addition%0Apipeline%2C%20named%20Add-SD%2C%20which%20automatically%20inserts%20objects%20into%20realistic%0Ascenes%20with%20rational%20sizes%20and%20positions.%20Different%20from%20layout-conditioned%0Amethods%2C%20Add-SD%20is%20solely%20conditioned%20on%20simple%20text%20prompts%20rather%20than%20any%0Aother%20human-costly%20references%20like%20bounding%20boxes.%20Our%20work%20contributes%20in%0Athree%20aspects%3A%20proposing%20a%20dataset%20containing%20numerous%20instructed%20image%20pairs%3B%0Afine-tuning%20a%20diffusion%20model%20for%20rational%20generation%3B%20and%20generating%20synthetic%0Adata%20to%20boost%20downstream%20tasks.%20The%20first%20aspect%20involves%20creating%20a%0ARemovalDataset%20consisting%20of%20original-edited%20image%20pairs%20with%20textual%0Ainstructions%2C%20where%20an%20object%20has%20been%20removed%20from%20the%20original%20image%20while%0Amaintaining%20strong%20pixel%20consistency%20in%20the%20background.%20These%20data%20pairs%20are%0Athen%20used%20for%20fine-tuning%20the%20Stable%20Diffusion%20%28SD%29%20model.%20Subsequently%2C%20the%0Apretrained%20Add-SD%20model%20allows%20for%20the%20insertion%20of%20expected%20objects%20into%20an%0Aimage%20with%20good%20rationale.%20Additionally%2C%20we%20generate%20synthetic%20instances%20for%0Adownstream%20task%20datasets%20at%20scale%2C%20particularly%20for%20tail%20classes%2C%20to%20alleviate%0Athe%20long-tailed%20problem.%20Downstream%20tasks%20benefit%20from%20the%20enriched%20dataset%0Awith%20enhanced%20diversity%20and%20rationale.%20Experiments%20on%20LVIS%20val%20demonstrate%20that%0AAdd-SD%20yields%20an%20improvement%20of%204.3%20mAP%20on%20rare%20classes%20over%20the%20baseline.%20Code%0Aand%20models%20are%20available%20at%20https%3A//github.com/ylingfeng/Add-SD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21016v1&entry.124074799=Read"},
{"title": "Knowledge Graph Structure as Prompt: Improving Small Language Models\n  Capabilities for Knowledge-based Causal Discovery", "author": "Yuni Susanti and Michael F\u00e4rber", "abstract": "  Causal discovery aims to estimate causal structures among variables based on\nobservational data. Large Language Models (LLMs) offer a fresh perspective to\ntackle the causal discovery problem by reasoning on the metadata associated\nwith variables rather than their actual data values, an approach referred to as\nknowledge-based causal discovery. In this paper, we investigate the\ncapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1\nbillion parameters) with prompt-based learning for knowledge-based causal\ndiscovery. Specifically, we present KG Structure as Prompt, a novel approach\nfor integrating structural information from a knowledge graph, such as common\nneighbor nodes and metapaths, into prompt-based learning to enhance the\ncapabilities of SLMs. Experimental results on three types of biomedical and\nopen-domain datasets under few-shot settings demonstrate the effectiveness of\nour approach, surpassing most baselines and even conventional fine-tuning\napproaches trained on full datasets. Our findings further highlight the strong\ncapabilities of SLMs: in combination with knowledge graphs and prompt-based\nlearning, SLMs demonstrate the potential to surpass LLMs with larger number of\nparameters. Our code and datasets are available on GitHub.\n", "link": "http://arxiv.org/abs/2407.18752v3", "date": "2024-07-30", "relevancy": 1.8514, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4687}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graph%20Structure%20as%20Prompt%3A%20Improving%20Small%20Language%20Models%0A%20%20Capabilities%20for%20Knowledge-based%20Causal%20Discovery&body=Title%3A%20Knowledge%20Graph%20Structure%20as%20Prompt%3A%20Improving%20Small%20Language%20Models%0A%20%20Capabilities%20for%20Knowledge-based%20Causal%20Discovery%0AAuthor%3A%20Yuni%20Susanti%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20%20%20Causal%20discovery%20aims%20to%20estimate%20causal%20structures%20among%20variables%20based%20on%0Aobservational%20data.%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20fresh%20perspective%20to%0Atackle%20the%20causal%20discovery%20problem%20by%20reasoning%20on%20the%20metadata%20associated%0Awith%20variables%20rather%20than%20their%20actual%20data%20values%2C%20an%20approach%20referred%20to%20as%0Aknowledge-based%20causal%20discovery.%20In%20this%20paper%2C%20we%20investigate%20the%0Acapabilities%20of%20Small%20Language%20Models%20%28SLMs%2C%20defined%20as%20LLMs%20with%20fewer%20than%201%0Abillion%20parameters%29%20with%20prompt-based%20learning%20for%20knowledge-based%20causal%0Adiscovery.%20Specifically%2C%20we%20present%20KG%20Structure%20as%20Prompt%2C%20a%20novel%20approach%0Afor%20integrating%20structural%20information%20from%20a%20knowledge%20graph%2C%20such%20as%20common%0Aneighbor%20nodes%20and%20metapaths%2C%20into%20prompt-based%20learning%20to%20enhance%20the%0Acapabilities%20of%20SLMs.%20Experimental%20results%20on%20three%20types%20of%20biomedical%20and%0Aopen-domain%20datasets%20under%20few-shot%20settings%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%2C%20surpassing%20most%20baselines%20and%20even%20conventional%20fine-tuning%0Aapproaches%20trained%20on%20full%20datasets.%20Our%20findings%20further%20highlight%20the%20strong%0Acapabilities%20of%20SLMs%3A%20in%20combination%20with%20knowledge%20graphs%20and%20prompt-based%0Alearning%2C%20SLMs%20demonstrate%20the%20potential%20to%20surpass%20LLMs%20with%20larger%20number%20of%0Aparameters.%20Our%20code%20and%20datasets%20are%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18752v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graph%2520Structure%2520as%2520Prompt%253A%2520Improving%2520Small%2520Language%2520Models%250A%2520%2520Capabilities%2520for%2520Knowledge-based%2520Causal%2520Discovery%26entry.906535625%3DYuni%2520Susanti%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3D%2520%2520Causal%2520discovery%2520aims%2520to%2520estimate%2520causal%2520structures%2520among%2520variables%2520based%2520on%250Aobservational%2520data.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520a%2520fresh%2520perspective%2520to%250Atackle%2520the%2520causal%2520discovery%2520problem%2520by%2520reasoning%2520on%2520the%2520metadata%2520associated%250Awith%2520variables%2520rather%2520than%2520their%2520actual%2520data%2520values%252C%2520an%2520approach%2520referred%2520to%2520as%250Aknowledge-based%2520causal%2520discovery.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Acapabilities%2520of%2520Small%2520Language%2520Models%2520%2528SLMs%252C%2520defined%2520as%2520LLMs%2520with%2520fewer%2520than%25201%250Abillion%2520parameters%2529%2520with%2520prompt-based%2520learning%2520for%2520knowledge-based%2520causal%250Adiscovery.%2520Specifically%252C%2520we%2520present%2520KG%2520Structure%2520as%2520Prompt%252C%2520a%2520novel%2520approach%250Afor%2520integrating%2520structural%2520information%2520from%2520a%2520knowledge%2520graph%252C%2520such%2520as%2520common%250Aneighbor%2520nodes%2520and%2520metapaths%252C%2520into%2520prompt-based%2520learning%2520to%2520enhance%2520the%250Acapabilities%2520of%2520SLMs.%2520Experimental%2520results%2520on%2520three%2520types%2520of%2520biomedical%2520and%250Aopen-domain%2520datasets%2520under%2520few-shot%2520settings%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520approach%252C%2520surpassing%2520most%2520baselines%2520and%2520even%2520conventional%2520fine-tuning%250Aapproaches%2520trained%2520on%2520full%2520datasets.%2520Our%2520findings%2520further%2520highlight%2520the%2520strong%250Acapabilities%2520of%2520SLMs%253A%2520in%2520combination%2520with%2520knowledge%2520graphs%2520and%2520prompt-based%250Alearning%252C%2520SLMs%2520demonstrate%2520the%2520potential%2520to%2520surpass%2520LLMs%2520with%2520larger%2520number%2520of%250Aparameters.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18752v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graph%20Structure%20as%20Prompt%3A%20Improving%20Small%20Language%20Models%0A%20%20Capabilities%20for%20Knowledge-based%20Causal%20Discovery&entry.906535625=Yuni%20Susanti%20and%20Michael%20F%C3%A4rber&entry.1292438233=%20%20Causal%20discovery%20aims%20to%20estimate%20causal%20structures%20among%20variables%20based%20on%0Aobservational%20data.%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20fresh%20perspective%20to%0Atackle%20the%20causal%20discovery%20problem%20by%20reasoning%20on%20the%20metadata%20associated%0Awith%20variables%20rather%20than%20their%20actual%20data%20values%2C%20an%20approach%20referred%20to%20as%0Aknowledge-based%20causal%20discovery.%20In%20this%20paper%2C%20we%20investigate%20the%0Acapabilities%20of%20Small%20Language%20Models%20%28SLMs%2C%20defined%20as%20LLMs%20with%20fewer%20than%201%0Abillion%20parameters%29%20with%20prompt-based%20learning%20for%20knowledge-based%20causal%0Adiscovery.%20Specifically%2C%20we%20present%20KG%20Structure%20as%20Prompt%2C%20a%20novel%20approach%0Afor%20integrating%20structural%20information%20from%20a%20knowledge%20graph%2C%20such%20as%20common%0Aneighbor%20nodes%20and%20metapaths%2C%20into%20prompt-based%20learning%20to%20enhance%20the%0Acapabilities%20of%20SLMs.%20Experimental%20results%20on%20three%20types%20of%20biomedical%20and%0Aopen-domain%20datasets%20under%20few-shot%20settings%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%2C%20surpassing%20most%20baselines%20and%20even%20conventional%20fine-tuning%0Aapproaches%20trained%20on%20full%20datasets.%20Our%20findings%20further%20highlight%20the%20strong%0Acapabilities%20of%20SLMs%3A%20in%20combination%20with%20knowledge%20graphs%20and%20prompt-based%0Alearning%2C%20SLMs%20demonstrate%20the%20potential%20to%20surpass%20LLMs%20with%20larger%20number%20of%0Aparameters.%20Our%20code%20and%20datasets%20are%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18752v3&entry.124074799=Read"},
{"title": "AI-Assisted Generation of Difficult Math Questions", "author": "Vedant Shah and Dingli Yu and Kaifeng Lyu and Simon Park and Nan Rosemary Ke and Michael Mozer and Yoshua Bengio and Sanjeev Arora and Anirudh Goyal", "abstract": "  Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.\n", "link": "http://arxiv.org/abs/2407.21009v1", "date": "2024-07-30", "relevancy": 1.8423, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4944}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4616}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Assisted%20Generation%20of%20Difficult%20Math%20Questions&body=Title%3A%20AI-Assisted%20Generation%20of%20Difficult%20Math%20Questions%0AAuthor%3A%20Vedant%20Shah%20and%20Dingli%20Yu%20and%20Kaifeng%20Lyu%20and%20Simon%20Park%20and%20Nan%20Rosemary%20Ke%20and%20Michael%20Mozer%20and%20Yoshua%20Bengio%20and%20Sanjeev%20Arora%20and%20Anirudh%20Goyal%0AAbstract%3A%20%20%20Current%20LLM%20training%20positions%20mathematical%20reasoning%20as%20a%20core%20capability.%0AWith%20publicly%20available%20sources%20fully%20tapped%2C%20there%20is%20unmet%20demand%20for%20diverse%0Aand%20challenging%20math%20questions.%20Relying%20solely%20on%20human%20experts%20is%20both%0Atime-consuming%20and%20costly%2C%20while%20LLM-generated%20questions%20often%20lack%20the%0Arequisite%20diversity%20and%20difficulty.%20We%20present%20a%20design%20framework%20that%20combines%0Athe%20strengths%20of%20LLMs%20with%20a%20human-in-the-loop%20approach%20to%20generate%20a%20diverse%0Aarray%20of%20challenging%20math%20questions.%20We%20leverage%20LLM%20metacognition%20skills%0A%5BDidolkar%20et%20al.%2C%202024%5D%20of%20a%20strong%20LLM%20to%20extract%20core%20%22skills%22%20from%20existing%0Amath%20datasets.%20These%20skills%20serve%20as%20the%20basis%20for%20generating%20novel%20and%0Adifficult%20questions%20by%20prompting%20the%20LLM%20with%20random%20pairs%20of%20core%20skills.%20The%0Ause%20of%20two%20different%20skills%20within%20each%20question%20makes%20finding%20such%20questions%0Aan%20%22out%20of%20distribution%22%20task%20for%20both%20LLMs%20and%20humans.%20Our%20pipeline%20employs%0ALLMs%20to%20iteratively%20generate%20and%20refine%20questions%20and%20solutions%20through%0Amultiturn%20prompting.%20Human%20annotators%20then%20verify%20and%20further%20refine%20the%0Aquestions%2C%20with%20their%20efficiency%20enhanced%20via%20further%20LLM%20interactions.%0AApplying%20this%20pipeline%20on%20skills%20extracted%20from%20the%20MATH%20dataset%20%5BHendrycks%20et%0Aal.%2C%202021%5D%20resulted%20in%20MATH%24%5E2%24%20-%20a%20dataset%20of%20higher-quality%20math%20questions%2C%0Aas%20evidenced%20by%3A%20%28a%29%20Lower%20performance%20of%20all%20models%20on%20MATH%24%5E2%24%20than%20on%20MATH%0A%28b%29%20Higher%20performance%20on%20MATH%20when%20using%20MATH%24%5E2%24%20questions%20as%20in-context%0Aexamples.%20Although%20focused%20on%20mathematics%2C%20our%20methodology%20seems%20applicable%20to%0Aother%20domains%20requiring%20structured%20reasoning%2C%20and%20potentially%20as%20a%20component%20of%0Ascalable%20oversight.%20Also%20of%20interest%20is%20a%20striking%20relationship%20observed%0Abetween%20models%27%20performance%20on%20the%20new%20dataset%3A%20the%20success%20rate%20on%20MATH%24%5E2%24%20is%0Athe%20square%20on%20MATH%2C%20suggesting%20that%20successfully%20solving%20the%20question%20in%0AMATH%24%5E2%24%20requires%20a%20nontrivial%20combination%20of%20two%20distinct%20math%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Assisted%2520Generation%2520of%2520Difficult%2520Math%2520Questions%26entry.906535625%3DVedant%2520Shah%2520and%2520Dingli%2520Yu%2520and%2520Kaifeng%2520Lyu%2520and%2520Simon%2520Park%2520and%2520Nan%2520Rosemary%2520Ke%2520and%2520Michael%2520Mozer%2520and%2520Yoshua%2520Bengio%2520and%2520Sanjeev%2520Arora%2520and%2520Anirudh%2520Goyal%26entry.1292438233%3D%2520%2520Current%2520LLM%2520training%2520positions%2520mathematical%2520reasoning%2520as%2520a%2520core%2520capability.%250AWith%2520publicly%2520available%2520sources%2520fully%2520tapped%252C%2520there%2520is%2520unmet%2520demand%2520for%2520diverse%250Aand%2520challenging%2520math%2520questions.%2520Relying%2520solely%2520on%2520human%2520experts%2520is%2520both%250Atime-consuming%2520and%2520costly%252C%2520while%2520LLM-generated%2520questions%2520often%2520lack%2520the%250Arequisite%2520diversity%2520and%2520difficulty.%2520We%2520present%2520a%2520design%2520framework%2520that%2520combines%250Athe%2520strengths%2520of%2520LLMs%2520with%2520a%2520human-in-the-loop%2520approach%2520to%2520generate%2520a%2520diverse%250Aarray%2520of%2520challenging%2520math%2520questions.%2520We%2520leverage%2520LLM%2520metacognition%2520skills%250A%255BDidolkar%2520et%2520al.%252C%25202024%255D%2520of%2520a%2520strong%2520LLM%2520to%2520extract%2520core%2520%2522skills%2522%2520from%2520existing%250Amath%2520datasets.%2520These%2520skills%2520serve%2520as%2520the%2520basis%2520for%2520generating%2520novel%2520and%250Adifficult%2520questions%2520by%2520prompting%2520the%2520LLM%2520with%2520random%2520pairs%2520of%2520core%2520skills.%2520The%250Ause%2520of%2520two%2520different%2520skills%2520within%2520each%2520question%2520makes%2520finding%2520such%2520questions%250Aan%2520%2522out%2520of%2520distribution%2522%2520task%2520for%2520both%2520LLMs%2520and%2520humans.%2520Our%2520pipeline%2520employs%250ALLMs%2520to%2520iteratively%2520generate%2520and%2520refine%2520questions%2520and%2520solutions%2520through%250Amultiturn%2520prompting.%2520Human%2520annotators%2520then%2520verify%2520and%2520further%2520refine%2520the%250Aquestions%252C%2520with%2520their%2520efficiency%2520enhanced%2520via%2520further%2520LLM%2520interactions.%250AApplying%2520this%2520pipeline%2520on%2520skills%2520extracted%2520from%2520the%2520MATH%2520dataset%2520%255BHendrycks%2520et%250Aal.%252C%25202021%255D%2520resulted%2520in%2520MATH%2524%255E2%2524%2520-%2520a%2520dataset%2520of%2520higher-quality%2520math%2520questions%252C%250Aas%2520evidenced%2520by%253A%2520%2528a%2529%2520Lower%2520performance%2520of%2520all%2520models%2520on%2520MATH%2524%255E2%2524%2520than%2520on%2520MATH%250A%2528b%2529%2520Higher%2520performance%2520on%2520MATH%2520when%2520using%2520MATH%2524%255E2%2524%2520questions%2520as%2520in-context%250Aexamples.%2520Although%2520focused%2520on%2520mathematics%252C%2520our%2520methodology%2520seems%2520applicable%2520to%250Aother%2520domains%2520requiring%2520structured%2520reasoning%252C%2520and%2520potentially%2520as%2520a%2520component%2520of%250Ascalable%2520oversight.%2520Also%2520of%2520interest%2520is%2520a%2520striking%2520relationship%2520observed%250Abetween%2520models%2527%2520performance%2520on%2520the%2520new%2520dataset%253A%2520the%2520success%2520rate%2520on%2520MATH%2524%255E2%2524%2520is%250Athe%2520square%2520on%2520MATH%252C%2520suggesting%2520that%2520successfully%2520solving%2520the%2520question%2520in%250AMATH%2524%255E2%2524%2520requires%2520a%2520nontrivial%2520combination%2520of%2520two%2520distinct%2520math%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Assisted%20Generation%20of%20Difficult%20Math%20Questions&entry.906535625=Vedant%20Shah%20and%20Dingli%20Yu%20and%20Kaifeng%20Lyu%20and%20Simon%20Park%20and%20Nan%20Rosemary%20Ke%20and%20Michael%20Mozer%20and%20Yoshua%20Bengio%20and%20Sanjeev%20Arora%20and%20Anirudh%20Goyal&entry.1292438233=%20%20Current%20LLM%20training%20positions%20mathematical%20reasoning%20as%20a%20core%20capability.%0AWith%20publicly%20available%20sources%20fully%20tapped%2C%20there%20is%20unmet%20demand%20for%20diverse%0Aand%20challenging%20math%20questions.%20Relying%20solely%20on%20human%20experts%20is%20both%0Atime-consuming%20and%20costly%2C%20while%20LLM-generated%20questions%20often%20lack%20the%0Arequisite%20diversity%20and%20difficulty.%20We%20present%20a%20design%20framework%20that%20combines%0Athe%20strengths%20of%20LLMs%20with%20a%20human-in-the-loop%20approach%20to%20generate%20a%20diverse%0Aarray%20of%20challenging%20math%20questions.%20We%20leverage%20LLM%20metacognition%20skills%0A%5BDidolkar%20et%20al.%2C%202024%5D%20of%20a%20strong%20LLM%20to%20extract%20core%20%22skills%22%20from%20existing%0Amath%20datasets.%20These%20skills%20serve%20as%20the%20basis%20for%20generating%20novel%20and%0Adifficult%20questions%20by%20prompting%20the%20LLM%20with%20random%20pairs%20of%20core%20skills.%20The%0Ause%20of%20two%20different%20skills%20within%20each%20question%20makes%20finding%20such%20questions%0Aan%20%22out%20of%20distribution%22%20task%20for%20both%20LLMs%20and%20humans.%20Our%20pipeline%20employs%0ALLMs%20to%20iteratively%20generate%20and%20refine%20questions%20and%20solutions%20through%0Amultiturn%20prompting.%20Human%20annotators%20then%20verify%20and%20further%20refine%20the%0Aquestions%2C%20with%20their%20efficiency%20enhanced%20via%20further%20LLM%20interactions.%0AApplying%20this%20pipeline%20on%20skills%20extracted%20from%20the%20MATH%20dataset%20%5BHendrycks%20et%0Aal.%2C%202021%5D%20resulted%20in%20MATH%24%5E2%24%20-%20a%20dataset%20of%20higher-quality%20math%20questions%2C%0Aas%20evidenced%20by%3A%20%28a%29%20Lower%20performance%20of%20all%20models%20on%20MATH%24%5E2%24%20than%20on%20MATH%0A%28b%29%20Higher%20performance%20on%20MATH%20when%20using%20MATH%24%5E2%24%20questions%20as%20in-context%0Aexamples.%20Although%20focused%20on%20mathematics%2C%20our%20methodology%20seems%20applicable%20to%0Aother%20domains%20requiring%20structured%20reasoning%2C%20and%20potentially%20as%20a%20component%20of%0Ascalable%20oversight.%20Also%20of%20interest%20is%20a%20striking%20relationship%20observed%0Abetween%20models%27%20performance%20on%20the%20new%20dataset%3A%20the%20success%20rate%20on%20MATH%24%5E2%24%20is%0Athe%20square%20on%20MATH%2C%20suggesting%20that%20successfully%20solving%20the%20question%20in%0AMATH%24%5E2%24%20requires%20a%20nontrivial%20combination%20of%20two%20distinct%20math%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21009v1&entry.124074799=Read"},
{"title": "Persistent Sampling: Unleashing the Potential of Sequential Monte Carlo", "author": "Minas Karamanis and Uro\u0161 Seljak", "abstract": "  Sequential Monte Carlo (SMC) methods are powerful tools for Bayesian\ninference but suffer from requiring many particles for accurate estimates,\nleading to high computational costs. We introduce persistent sampling (PS), an\nextension of SMC that mitigates this issue by allowing particles from previous\niterations to persist. This generates a growing, weighted ensemble of particles\ndistributed across iterations. In each iteration, PS utilizes multiple\nimportance sampling and resampling from the mixture of all previous\ndistributions to produce the next generation of particles. This addresses\nparticle impoverishment and mode collapse, resulting in more accurate posterior\napproximations. Furthermore, this approach provides lower-variance marginal\nlikelihood estimates for model comparison. Additionally, the persistent\nparticles improve transition kernel adaptation for efficient exploration.\nExperiments on complex distributions show that PS consistently outperforms\nstandard methods, achieving lower squared bias in posterior moment estimation\nand significantly reduced marginal likelihood errors, all at a lower\ncomputational cost. PS offers a robust, efficient, and scalable framework for\nBayesian inference.\n", "link": "http://arxiv.org/abs/2407.20722v1", "date": "2024-07-30", "relevancy": 1.8416, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4642}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Sampling%3A%20Unleashing%20the%20Potential%20of%20Sequential%20Monte%20Carlo&body=Title%3A%20Persistent%20Sampling%3A%20Unleashing%20the%20Potential%20of%20Sequential%20Monte%20Carlo%0AAuthor%3A%20Minas%20Karamanis%20and%20Uro%C5%A1%20Seljak%0AAbstract%3A%20%20%20Sequential%20Monte%20Carlo%20%28SMC%29%20methods%20are%20powerful%20tools%20for%20Bayesian%0Ainference%20but%20suffer%20from%20requiring%20many%20particles%20for%20accurate%20estimates%2C%0Aleading%20to%20high%20computational%20costs.%20We%20introduce%20persistent%20sampling%20%28PS%29%2C%20an%0Aextension%20of%20SMC%20that%20mitigates%20this%20issue%20by%20allowing%20particles%20from%20previous%0Aiterations%20to%20persist.%20This%20generates%20a%20growing%2C%20weighted%20ensemble%20of%20particles%0Adistributed%20across%20iterations.%20In%20each%20iteration%2C%20PS%20utilizes%20multiple%0Aimportance%20sampling%20and%20resampling%20from%20the%20mixture%20of%20all%20previous%0Adistributions%20to%20produce%20the%20next%20generation%20of%20particles.%20This%20addresses%0Aparticle%20impoverishment%20and%20mode%20collapse%2C%20resulting%20in%20more%20accurate%20posterior%0Aapproximations.%20Furthermore%2C%20this%20approach%20provides%20lower-variance%20marginal%0Alikelihood%20estimates%20for%20model%20comparison.%20Additionally%2C%20the%20persistent%0Aparticles%20improve%20transition%20kernel%20adaptation%20for%20efficient%20exploration.%0AExperiments%20on%20complex%20distributions%20show%20that%20PS%20consistently%20outperforms%0Astandard%20methods%2C%20achieving%20lower%20squared%20bias%20in%20posterior%20moment%20estimation%0Aand%20significantly%20reduced%20marginal%20likelihood%20errors%2C%20all%20at%20a%20lower%0Acomputational%20cost.%20PS%20offers%20a%20robust%2C%20efficient%2C%20and%20scalable%20framework%20for%0ABayesian%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Sampling%253A%2520Unleashing%2520the%2520Potential%2520of%2520Sequential%2520Monte%2520Carlo%26entry.906535625%3DMinas%2520Karamanis%2520and%2520Uro%25C5%25A1%2520Seljak%26entry.1292438233%3D%2520%2520Sequential%2520Monte%2520Carlo%2520%2528SMC%2529%2520methods%2520are%2520powerful%2520tools%2520for%2520Bayesian%250Ainference%2520but%2520suffer%2520from%2520requiring%2520many%2520particles%2520for%2520accurate%2520estimates%252C%250Aleading%2520to%2520high%2520computational%2520costs.%2520We%2520introduce%2520persistent%2520sampling%2520%2528PS%2529%252C%2520an%250Aextension%2520of%2520SMC%2520that%2520mitigates%2520this%2520issue%2520by%2520allowing%2520particles%2520from%2520previous%250Aiterations%2520to%2520persist.%2520This%2520generates%2520a%2520growing%252C%2520weighted%2520ensemble%2520of%2520particles%250Adistributed%2520across%2520iterations.%2520In%2520each%2520iteration%252C%2520PS%2520utilizes%2520multiple%250Aimportance%2520sampling%2520and%2520resampling%2520from%2520the%2520mixture%2520of%2520all%2520previous%250Adistributions%2520to%2520produce%2520the%2520next%2520generation%2520of%2520particles.%2520This%2520addresses%250Aparticle%2520impoverishment%2520and%2520mode%2520collapse%252C%2520resulting%2520in%2520more%2520accurate%2520posterior%250Aapproximations.%2520Furthermore%252C%2520this%2520approach%2520provides%2520lower-variance%2520marginal%250Alikelihood%2520estimates%2520for%2520model%2520comparison.%2520Additionally%252C%2520the%2520persistent%250Aparticles%2520improve%2520transition%2520kernel%2520adaptation%2520for%2520efficient%2520exploration.%250AExperiments%2520on%2520complex%2520distributions%2520show%2520that%2520PS%2520consistently%2520outperforms%250Astandard%2520methods%252C%2520achieving%2520lower%2520squared%2520bias%2520in%2520posterior%2520moment%2520estimation%250Aand%2520significantly%2520reduced%2520marginal%2520likelihood%2520errors%252C%2520all%2520at%2520a%2520lower%250Acomputational%2520cost.%2520PS%2520offers%2520a%2520robust%252C%2520efficient%252C%2520and%2520scalable%2520framework%2520for%250ABayesian%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Sampling%3A%20Unleashing%20the%20Potential%20of%20Sequential%20Monte%20Carlo&entry.906535625=Minas%20Karamanis%20and%20Uro%C5%A1%20Seljak&entry.1292438233=%20%20Sequential%20Monte%20Carlo%20%28SMC%29%20methods%20are%20powerful%20tools%20for%20Bayesian%0Ainference%20but%20suffer%20from%20requiring%20many%20particles%20for%20accurate%20estimates%2C%0Aleading%20to%20high%20computational%20costs.%20We%20introduce%20persistent%20sampling%20%28PS%29%2C%20an%0Aextension%20of%20SMC%20that%20mitigates%20this%20issue%20by%20allowing%20particles%20from%20previous%0Aiterations%20to%20persist.%20This%20generates%20a%20growing%2C%20weighted%20ensemble%20of%20particles%0Adistributed%20across%20iterations.%20In%20each%20iteration%2C%20PS%20utilizes%20multiple%0Aimportance%20sampling%20and%20resampling%20from%20the%20mixture%20of%20all%20previous%0Adistributions%20to%20produce%20the%20next%20generation%20of%20particles.%20This%20addresses%0Aparticle%20impoverishment%20and%20mode%20collapse%2C%20resulting%20in%20more%20accurate%20posterior%0Aapproximations.%20Furthermore%2C%20this%20approach%20provides%20lower-variance%20marginal%0Alikelihood%20estimates%20for%20model%20comparison.%20Additionally%2C%20the%20persistent%0Aparticles%20improve%20transition%20kernel%20adaptation%20for%20efficient%20exploration.%0AExperiments%20on%20complex%20distributions%20show%20that%20PS%20consistently%20outperforms%0Astandard%20methods%2C%20achieving%20lower%20squared%20bias%20in%20posterior%20moment%20estimation%0Aand%20significantly%20reduced%20marginal%20likelihood%20errors%2C%20all%20at%20a%20lower%0Acomputational%20cost.%20PS%20offers%20a%20robust%2C%20efficient%2C%20and%20scalable%20framework%20for%0ABayesian%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20722v1&entry.124074799=Read"},
{"title": "Emotion-driven Piano Music Generation via Two-stage Disentanglement and\n  Functional Representation", "author": "Jingyue Huang and Ke Chen and Yi-Hsuan Yang", "abstract": "  Managing the emotional aspect remains a challenge in automatic music\ngeneration. Prior works aim to learn various emotions at once, leading to\ninadequate modeling. This paper explores the disentanglement of emotions in\npiano performance generation through a two-stage framework. The first stage\nfocuses on valence modeling of lead sheet, and the second stage addresses\narousal modeling by introducing performance-level attributes. To further\ncapture features that shape valence, an aspect less explored by previous\napproaches, we introduce a novel functional representation of symbolic music.\nThis representation aims to capture the emotional impact of major-minor\ntonality, as well as the interactions among notes, chords, and key signatures.\nObjective and subjective experiments validate the effectiveness of our\nframework in both emotional valence and arousal modeling. We further leverage\nour framework in a novel application of emotional controls, showing a broad\npotential in emotion-driven music generation.\n", "link": "http://arxiv.org/abs/2407.20955v1", "date": "2024-07-30", "relevancy": 1.834, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4654}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emotion-driven%20Piano%20Music%20Generation%20via%20Two-stage%20Disentanglement%20and%0A%20%20Functional%20Representation&body=Title%3A%20Emotion-driven%20Piano%20Music%20Generation%20via%20Two-stage%20Disentanglement%20and%0A%20%20Functional%20Representation%0AAuthor%3A%20Jingyue%20Huang%20and%20Ke%20Chen%20and%20Yi-Hsuan%20Yang%0AAbstract%3A%20%20%20Managing%20the%20emotional%20aspect%20remains%20a%20challenge%20in%20automatic%20music%0Ageneration.%20Prior%20works%20aim%20to%20learn%20various%20emotions%20at%20once%2C%20leading%20to%0Ainadequate%20modeling.%20This%20paper%20explores%20the%20disentanglement%20of%20emotions%20in%0Apiano%20performance%20generation%20through%20a%20two-stage%20framework.%20The%20first%20stage%0Afocuses%20on%20valence%20modeling%20of%20lead%20sheet%2C%20and%20the%20second%20stage%20addresses%0Aarousal%20modeling%20by%20introducing%20performance-level%20attributes.%20To%20further%0Acapture%20features%20that%20shape%20valence%2C%20an%20aspect%20less%20explored%20by%20previous%0Aapproaches%2C%20we%20introduce%20a%20novel%20functional%20representation%20of%20symbolic%20music.%0AThis%20representation%20aims%20to%20capture%20the%20emotional%20impact%20of%20major-minor%0Atonality%2C%20as%20well%20as%20the%20interactions%20among%20notes%2C%20chords%2C%20and%20key%20signatures.%0AObjective%20and%20subjective%20experiments%20validate%20the%20effectiveness%20of%20our%0Aframework%20in%20both%20emotional%20valence%20and%20arousal%20modeling.%20We%20further%20leverage%0Aour%20framework%20in%20a%20novel%20application%20of%20emotional%20controls%2C%20showing%20a%20broad%0Apotential%20in%20emotion-driven%20music%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotion-driven%2520Piano%2520Music%2520Generation%2520via%2520Two-stage%2520Disentanglement%2520and%250A%2520%2520Functional%2520Representation%26entry.906535625%3DJingyue%2520Huang%2520and%2520Ke%2520Chen%2520and%2520Yi-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Managing%2520the%2520emotional%2520aspect%2520remains%2520a%2520challenge%2520in%2520automatic%2520music%250Ageneration.%2520Prior%2520works%2520aim%2520to%2520learn%2520various%2520emotions%2520at%2520once%252C%2520leading%2520to%250Ainadequate%2520modeling.%2520This%2520paper%2520explores%2520the%2520disentanglement%2520of%2520emotions%2520in%250Apiano%2520performance%2520generation%2520through%2520a%2520two-stage%2520framework.%2520The%2520first%2520stage%250Afocuses%2520on%2520valence%2520modeling%2520of%2520lead%2520sheet%252C%2520and%2520the%2520second%2520stage%2520addresses%250Aarousal%2520modeling%2520by%2520introducing%2520performance-level%2520attributes.%2520To%2520further%250Acapture%2520features%2520that%2520shape%2520valence%252C%2520an%2520aspect%2520less%2520explored%2520by%2520previous%250Aapproaches%252C%2520we%2520introduce%2520a%2520novel%2520functional%2520representation%2520of%2520symbolic%2520music.%250AThis%2520representation%2520aims%2520to%2520capture%2520the%2520emotional%2520impact%2520of%2520major-minor%250Atonality%252C%2520as%2520well%2520as%2520the%2520interactions%2520among%2520notes%252C%2520chords%252C%2520and%2520key%2520signatures.%250AObjective%2520and%2520subjective%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%250Aframework%2520in%2520both%2520emotional%2520valence%2520and%2520arousal%2520modeling.%2520We%2520further%2520leverage%250Aour%2520framework%2520in%2520a%2520novel%2520application%2520of%2520emotional%2520controls%252C%2520showing%2520a%2520broad%250Apotential%2520in%2520emotion-driven%2520music%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotion-driven%20Piano%20Music%20Generation%20via%20Two-stage%20Disentanglement%20and%0A%20%20Functional%20Representation&entry.906535625=Jingyue%20Huang%20and%20Ke%20Chen%20and%20Yi-Hsuan%20Yang&entry.1292438233=%20%20Managing%20the%20emotional%20aspect%20remains%20a%20challenge%20in%20automatic%20music%0Ageneration.%20Prior%20works%20aim%20to%20learn%20various%20emotions%20at%20once%2C%20leading%20to%0Ainadequate%20modeling.%20This%20paper%20explores%20the%20disentanglement%20of%20emotions%20in%0Apiano%20performance%20generation%20through%20a%20two-stage%20framework.%20The%20first%20stage%0Afocuses%20on%20valence%20modeling%20of%20lead%20sheet%2C%20and%20the%20second%20stage%20addresses%0Aarousal%20modeling%20by%20introducing%20performance-level%20attributes.%20To%20further%0Acapture%20features%20that%20shape%20valence%2C%20an%20aspect%20less%20explored%20by%20previous%0Aapproaches%2C%20we%20introduce%20a%20novel%20functional%20representation%20of%20symbolic%20music.%0AThis%20representation%20aims%20to%20capture%20the%20emotional%20impact%20of%20major-minor%0Atonality%2C%20as%20well%20as%20the%20interactions%20among%20notes%2C%20chords%2C%20and%20key%20signatures.%0AObjective%20and%20subjective%20experiments%20validate%20the%20effectiveness%20of%20our%0Aframework%20in%20both%20emotional%20valence%20and%20arousal%20modeling.%20We%20further%20leverage%0Aour%20framework%20in%20a%20novel%20application%20of%20emotional%20controls%2C%20showing%20a%20broad%0Apotential%20in%20emotion-driven%20music%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20955v1&entry.124074799=Read"},
{"title": "Metaheuristic Enhanced with Feature-Based Guidance and Diversity\n  Management for Solving the Capacitated Vehicle Routing Problem", "author": "Bachtiar Herdianto and Romain Billot and Flavien Lucas and Marc Sevaux", "abstract": "  We propose a metaheuristic algorithm enhanced with feature-based guidance\nthat is designed to solve the Capacitated Vehicle Routing Problem (CVRP). To\nformulate the proposed guidance, we developed and explained a supervised\nMachine Learning (ML) model, that is used to formulate the guidance and control\nthe diversity of the solution during the optimization process. We propose a\nmetaheuristic algorithm combining neighborhood search and a novel mechanism of\nhybrid split and path relinking to implement the proposed guidance. The\nproposed guidance has proven to give a statistically significant improvement to\nthe proposed metaheuristic algorithm when solving CVRP. Moreover, the proposed\nguided metaheuristic is also capable of producing competitive solutions among\nstate-of-the-art metaheuristic algorithms.\n", "link": "http://arxiv.org/abs/2407.20777v1", "date": "2024-07-30", "relevancy": 1.8221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.469}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metaheuristic%20Enhanced%20with%20Feature-Based%20Guidance%20and%20Diversity%0A%20%20Management%20for%20Solving%20the%20Capacitated%20Vehicle%20Routing%20Problem&body=Title%3A%20Metaheuristic%20Enhanced%20with%20Feature-Based%20Guidance%20and%20Diversity%0A%20%20Management%20for%20Solving%20the%20Capacitated%20Vehicle%20Routing%20Problem%0AAuthor%3A%20Bachtiar%20Herdianto%20and%20Romain%20Billot%20and%20Flavien%20Lucas%20and%20Marc%20Sevaux%0AAbstract%3A%20%20%20We%20propose%20a%20metaheuristic%20algorithm%20enhanced%20with%20feature-based%20guidance%0Athat%20is%20designed%20to%20solve%20the%20Capacitated%20Vehicle%20Routing%20Problem%20%28CVRP%29.%20To%0Aformulate%20the%20proposed%20guidance%2C%20we%20developed%20and%20explained%20a%20supervised%0AMachine%20Learning%20%28ML%29%20model%2C%20that%20is%20used%20to%20formulate%20the%20guidance%20and%20control%0Athe%20diversity%20of%20the%20solution%20during%20the%20optimization%20process.%20We%20propose%20a%0Ametaheuristic%20algorithm%20combining%20neighborhood%20search%20and%20a%20novel%20mechanism%20of%0Ahybrid%20split%20and%20path%20relinking%20to%20implement%20the%20proposed%20guidance.%20The%0Aproposed%20guidance%20has%20proven%20to%20give%20a%20statistically%20significant%20improvement%20to%0Athe%20proposed%20metaheuristic%20algorithm%20when%20solving%20CVRP.%20Moreover%2C%20the%20proposed%0Aguided%20metaheuristic%20is%20also%20capable%20of%20producing%20competitive%20solutions%20among%0Astate-of-the-art%20metaheuristic%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaheuristic%2520Enhanced%2520with%2520Feature-Based%2520Guidance%2520and%2520Diversity%250A%2520%2520Management%2520for%2520Solving%2520the%2520Capacitated%2520Vehicle%2520Routing%2520Problem%26entry.906535625%3DBachtiar%2520Herdianto%2520and%2520Romain%2520Billot%2520and%2520Flavien%2520Lucas%2520and%2520Marc%2520Sevaux%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520metaheuristic%2520algorithm%2520enhanced%2520with%2520feature-based%2520guidance%250Athat%2520is%2520designed%2520to%2520solve%2520the%2520Capacitated%2520Vehicle%2520Routing%2520Problem%2520%2528CVRP%2529.%2520To%250Aformulate%2520the%2520proposed%2520guidance%252C%2520we%2520developed%2520and%2520explained%2520a%2520supervised%250AMachine%2520Learning%2520%2528ML%2529%2520model%252C%2520that%2520is%2520used%2520to%2520formulate%2520the%2520guidance%2520and%2520control%250Athe%2520diversity%2520of%2520the%2520solution%2520during%2520the%2520optimization%2520process.%2520We%2520propose%2520a%250Ametaheuristic%2520algorithm%2520combining%2520neighborhood%2520search%2520and%2520a%2520novel%2520mechanism%2520of%250Ahybrid%2520split%2520and%2520path%2520relinking%2520to%2520implement%2520the%2520proposed%2520guidance.%2520The%250Aproposed%2520guidance%2520has%2520proven%2520to%2520give%2520a%2520statistically%2520significant%2520improvement%2520to%250Athe%2520proposed%2520metaheuristic%2520algorithm%2520when%2520solving%2520CVRP.%2520Moreover%252C%2520the%2520proposed%250Aguided%2520metaheuristic%2520is%2520also%2520capable%2520of%2520producing%2520competitive%2520solutions%2520among%250Astate-of-the-art%2520metaheuristic%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metaheuristic%20Enhanced%20with%20Feature-Based%20Guidance%20and%20Diversity%0A%20%20Management%20for%20Solving%20the%20Capacitated%20Vehicle%20Routing%20Problem&entry.906535625=Bachtiar%20Herdianto%20and%20Romain%20Billot%20and%20Flavien%20Lucas%20and%20Marc%20Sevaux&entry.1292438233=%20%20We%20propose%20a%20metaheuristic%20algorithm%20enhanced%20with%20feature-based%20guidance%0Athat%20is%20designed%20to%20solve%20the%20Capacitated%20Vehicle%20Routing%20Problem%20%28CVRP%29.%20To%0Aformulate%20the%20proposed%20guidance%2C%20we%20developed%20and%20explained%20a%20supervised%0AMachine%20Learning%20%28ML%29%20model%2C%20that%20is%20used%20to%20formulate%20the%20guidance%20and%20control%0Athe%20diversity%20of%20the%20solution%20during%20the%20optimization%20process.%20We%20propose%20a%0Ametaheuristic%20algorithm%20combining%20neighborhood%20search%20and%20a%20novel%20mechanism%20of%0Ahybrid%20split%20and%20path%20relinking%20to%20implement%20the%20proposed%20guidance.%20The%0Aproposed%20guidance%20has%20proven%20to%20give%20a%20statistically%20significant%20improvement%20to%0Athe%20proposed%20metaheuristic%20algorithm%20when%20solving%20CVRP.%20Moreover%2C%20the%20proposed%0Aguided%20metaheuristic%20is%20also%20capable%20of%20producing%20competitive%20solutions%20among%0Astate-of-the-art%20metaheuristic%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20777v1&entry.124074799=Read"},
{"title": "What is YOLOv5: A deep look into the internal features of the popular\n  object detector", "author": "Rahima Khanam and Muhammad Hussain", "abstract": "  This study presents a comprehensive analysis of the YOLOv5 object detection\nmodel, examining its architecture, training methodologies, and performance. Key\ncomponents, including the Cross Stage Partial backbone and Path\nAggregation-Network, are explored in detail. The paper reviews the model's\nperformance across various metrics and hardware platforms. Additionally, the\nstudy discusses the transition from Darknet to PyTorch and its impact on model\ndevelopment. Overall, this research provides insights into YOLOv5's\ncapabilities and its position within the broader landscape of object detection\nand why it is a popular choice for constrained edge deployment scenarios.\n", "link": "http://arxiv.org/abs/2407.20892v1", "date": "2024-07-30", "relevancy": 1.8125, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4526}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20is%20YOLOv5%3A%20A%20deep%20look%20into%20the%20internal%20features%20of%20the%20popular%0A%20%20object%20detector&body=Title%3A%20What%20is%20YOLOv5%3A%20A%20deep%20look%20into%20the%20internal%20features%20of%20the%20popular%0A%20%20object%20detector%0AAuthor%3A%20Rahima%20Khanam%20and%20Muhammad%20Hussain%0AAbstract%3A%20%20%20This%20study%20presents%20a%20comprehensive%20analysis%20of%20the%20YOLOv5%20object%20detection%0Amodel%2C%20examining%20its%20architecture%2C%20training%20methodologies%2C%20and%20performance.%20Key%0Acomponents%2C%20including%20the%20Cross%20Stage%20Partial%20backbone%20and%20Path%0AAggregation-Network%2C%20are%20explored%20in%20detail.%20The%20paper%20reviews%20the%20model%27s%0Aperformance%20across%20various%20metrics%20and%20hardware%20platforms.%20Additionally%2C%20the%0Astudy%20discusses%20the%20transition%20from%20Darknet%20to%20PyTorch%20and%20its%20impact%20on%20model%0Adevelopment.%20Overall%2C%20this%20research%20provides%20insights%20into%20YOLOv5%27s%0Acapabilities%20and%20its%20position%20within%20the%20broader%20landscape%20of%20object%20detection%0Aand%20why%20it%20is%20a%20popular%20choice%20for%20constrained%20edge%20deployment%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520is%2520YOLOv5%253A%2520A%2520deep%2520look%2520into%2520the%2520internal%2520features%2520of%2520the%2520popular%250A%2520%2520object%2520detector%26entry.906535625%3DRahima%2520Khanam%2520and%2520Muhammad%2520Hussain%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520comprehensive%2520analysis%2520of%2520the%2520YOLOv5%2520object%2520detection%250Amodel%252C%2520examining%2520its%2520architecture%252C%2520training%2520methodologies%252C%2520and%2520performance.%2520Key%250Acomponents%252C%2520including%2520the%2520Cross%2520Stage%2520Partial%2520backbone%2520and%2520Path%250AAggregation-Network%252C%2520are%2520explored%2520in%2520detail.%2520The%2520paper%2520reviews%2520the%2520model%2527s%250Aperformance%2520across%2520various%2520metrics%2520and%2520hardware%2520platforms.%2520Additionally%252C%2520the%250Astudy%2520discusses%2520the%2520transition%2520from%2520Darknet%2520to%2520PyTorch%2520and%2520its%2520impact%2520on%2520model%250Adevelopment.%2520Overall%252C%2520this%2520research%2520provides%2520insights%2520into%2520YOLOv5%2527s%250Acapabilities%2520and%2520its%2520position%2520within%2520the%2520broader%2520landscape%2520of%2520object%2520detection%250Aand%2520why%2520it%2520is%2520a%2520popular%2520choice%2520for%2520constrained%2520edge%2520deployment%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20is%20YOLOv5%3A%20A%20deep%20look%20into%20the%20internal%20features%20of%20the%20popular%0A%20%20object%20detector&entry.906535625=Rahima%20Khanam%20and%20Muhammad%20Hussain&entry.1292438233=%20%20This%20study%20presents%20a%20comprehensive%20analysis%20of%20the%20YOLOv5%20object%20detection%0Amodel%2C%20examining%20its%20architecture%2C%20training%20methodologies%2C%20and%20performance.%20Key%0Acomponents%2C%20including%20the%20Cross%20Stage%20Partial%20backbone%20and%20Path%0AAggregation-Network%2C%20are%20explored%20in%20detail.%20The%20paper%20reviews%20the%20model%27s%0Aperformance%20across%20various%20metrics%20and%20hardware%20platforms.%20Additionally%2C%20the%0Astudy%20discusses%20the%20transition%20from%20Darknet%20to%20PyTorch%20and%20its%20impact%20on%20model%0Adevelopment.%20Overall%2C%20this%20research%20provides%20insights%20into%20YOLOv5%27s%0Acapabilities%20and%20its%20position%20within%20the%20broader%20landscape%20of%20object%20detection%0Aand%20why%20it%20is%20a%20popular%20choice%20for%20constrained%20edge%20deployment%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20892v1&entry.124074799=Read"},
{"title": "Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction", "author": "Alexander Timans and Christoph-Nikolas Straehle and Kaspar Sakmann and Eric Nalisnick", "abstract": "  Quantifying a model's predictive uncertainty is essential for safety-critical\napplications such as autonomous driving. We consider quantifying such\nuncertainty for multi-object detection. In particular, we leverage conformal\nprediction to obtain uncertainty intervals with guaranteed coverage for object\nbounding boxes. One challenge in doing so is that bounding box predictions are\nconditioned on the object's class label. Thus, we develop a novel two-step\nconformal approach that propagates uncertainty in predicted class labels into\nthe uncertainty intervals of bounding boxes. This broadens the validity of our\nconformal coverage guarantees to include incorrectly classified objects, thus\noffering more actionable safety assurances. Moreover, we investigate novel\nensemble and quantile regression formulations to ensure the bounding box\nintervals are adaptive to object size, leading to a more balanced coverage.\nValidating our two-step approach on real-world datasets for 2D bounding box\nlocalization, we find that desired coverage levels are satisfied with\npractically tight predictive uncertainty intervals.\n", "link": "http://arxiv.org/abs/2403.07263v2", "date": "2024-07-30", "relevancy": 1.7997, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6076}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Bounding%20Box%20Uncertainties%20via%20Two-Step%20Conformal%20Prediction&body=Title%3A%20Adaptive%20Bounding%20Box%20Uncertainties%20via%20Two-Step%20Conformal%20Prediction%0AAuthor%3A%20Alexander%20Timans%20and%20Christoph-Nikolas%20Straehle%20and%20Kaspar%20Sakmann%20and%20Eric%20Nalisnick%0AAbstract%3A%20%20%20Quantifying%20a%20model%27s%20predictive%20uncertainty%20is%20essential%20for%20safety-critical%0Aapplications%20such%20as%20autonomous%20driving.%20We%20consider%20quantifying%20such%0Auncertainty%20for%20multi-object%20detection.%20In%20particular%2C%20we%20leverage%20conformal%0Aprediction%20to%20obtain%20uncertainty%20intervals%20with%20guaranteed%20coverage%20for%20object%0Abounding%20boxes.%20One%20challenge%20in%20doing%20so%20is%20that%20bounding%20box%20predictions%20are%0Aconditioned%20on%20the%20object%27s%20class%20label.%20Thus%2C%20we%20develop%20a%20novel%20two-step%0Aconformal%20approach%20that%20propagates%20uncertainty%20in%20predicted%20class%20labels%20into%0Athe%20uncertainty%20intervals%20of%20bounding%20boxes.%20This%20broadens%20the%20validity%20of%20our%0Aconformal%20coverage%20guarantees%20to%20include%20incorrectly%20classified%20objects%2C%20thus%0Aoffering%20more%20actionable%20safety%20assurances.%20Moreover%2C%20we%20investigate%20novel%0Aensemble%20and%20quantile%20regression%20formulations%20to%20ensure%20the%20bounding%20box%0Aintervals%20are%20adaptive%20to%20object%20size%2C%20leading%20to%20a%20more%20balanced%20coverage.%0AValidating%20our%20two-step%20approach%20on%20real-world%20datasets%20for%202D%20bounding%20box%0Alocalization%2C%20we%20find%20that%20desired%20coverage%20levels%20are%20satisfied%20with%0Apractically%20tight%20predictive%20uncertainty%20intervals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Bounding%2520Box%2520Uncertainties%2520via%2520Two-Step%2520Conformal%2520Prediction%26entry.906535625%3DAlexander%2520Timans%2520and%2520Christoph-Nikolas%2520Straehle%2520and%2520Kaspar%2520Sakmann%2520and%2520Eric%2520Nalisnick%26entry.1292438233%3D%2520%2520Quantifying%2520a%2520model%2527s%2520predictive%2520uncertainty%2520is%2520essential%2520for%2520safety-critical%250Aapplications%2520such%2520as%2520autonomous%2520driving.%2520We%2520consider%2520quantifying%2520such%250Auncertainty%2520for%2520multi-object%2520detection.%2520In%2520particular%252C%2520we%2520leverage%2520conformal%250Aprediction%2520to%2520obtain%2520uncertainty%2520intervals%2520with%2520guaranteed%2520coverage%2520for%2520object%250Abounding%2520boxes.%2520One%2520challenge%2520in%2520doing%2520so%2520is%2520that%2520bounding%2520box%2520predictions%2520are%250Aconditioned%2520on%2520the%2520object%2527s%2520class%2520label.%2520Thus%252C%2520we%2520develop%2520a%2520novel%2520two-step%250Aconformal%2520approach%2520that%2520propagates%2520uncertainty%2520in%2520predicted%2520class%2520labels%2520into%250Athe%2520uncertainty%2520intervals%2520of%2520bounding%2520boxes.%2520This%2520broadens%2520the%2520validity%2520of%2520our%250Aconformal%2520coverage%2520guarantees%2520to%2520include%2520incorrectly%2520classified%2520objects%252C%2520thus%250Aoffering%2520more%2520actionable%2520safety%2520assurances.%2520Moreover%252C%2520we%2520investigate%2520novel%250Aensemble%2520and%2520quantile%2520regression%2520formulations%2520to%2520ensure%2520the%2520bounding%2520box%250Aintervals%2520are%2520adaptive%2520to%2520object%2520size%252C%2520leading%2520to%2520a%2520more%2520balanced%2520coverage.%250AValidating%2520our%2520two-step%2520approach%2520on%2520real-world%2520datasets%2520for%25202D%2520bounding%2520box%250Alocalization%252C%2520we%2520find%2520that%2520desired%2520coverage%2520levels%2520are%2520satisfied%2520with%250Apractically%2520tight%2520predictive%2520uncertainty%2520intervals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Bounding%20Box%20Uncertainties%20via%20Two-Step%20Conformal%20Prediction&entry.906535625=Alexander%20Timans%20and%20Christoph-Nikolas%20Straehle%20and%20Kaspar%20Sakmann%20and%20Eric%20Nalisnick&entry.1292438233=%20%20Quantifying%20a%20model%27s%20predictive%20uncertainty%20is%20essential%20for%20safety-critical%0Aapplications%20such%20as%20autonomous%20driving.%20We%20consider%20quantifying%20such%0Auncertainty%20for%20multi-object%20detection.%20In%20particular%2C%20we%20leverage%20conformal%0Aprediction%20to%20obtain%20uncertainty%20intervals%20with%20guaranteed%20coverage%20for%20object%0Abounding%20boxes.%20One%20challenge%20in%20doing%20so%20is%20that%20bounding%20box%20predictions%20are%0Aconditioned%20on%20the%20object%27s%20class%20label.%20Thus%2C%20we%20develop%20a%20novel%20two-step%0Aconformal%20approach%20that%20propagates%20uncertainty%20in%20predicted%20class%20labels%20into%0Athe%20uncertainty%20intervals%20of%20bounding%20boxes.%20This%20broadens%20the%20validity%20of%20our%0Aconformal%20coverage%20guarantees%20to%20include%20incorrectly%20classified%20objects%2C%20thus%0Aoffering%20more%20actionable%20safety%20assurances.%20Moreover%2C%20we%20investigate%20novel%0Aensemble%20and%20quantile%20regression%20formulations%20to%20ensure%20the%20bounding%20box%0Aintervals%20are%20adaptive%20to%20object%20size%2C%20leading%20to%20a%20more%20balanced%20coverage.%0AValidating%20our%20two-step%20approach%20on%20real-world%20datasets%20for%202D%20bounding%20box%0Alocalization%2C%20we%20find%20that%20desired%20coverage%20levels%20are%20satisfied%20with%0Apractically%20tight%20predictive%20uncertainty%20intervals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07263v2&entry.124074799=Read"},
{"title": "Graph Reinforcement Learning in Power Grids: A Survey", "author": "Mohamed Hassouna and Clara Holzh\u00fcter and Pawel Lytaev and Josephine Thomas and Bernhard Sick and Christoph Scholz", "abstract": "  The challenges posed by renewable energy and distributed electricity\ngeneration motivate the development of deep learning approaches to overcome the\nlack of flexibility of traditional methods in power grids use cases. The\napplication of GNNs is particularly promising due to their ability to learn\nfrom graph-structured data present in power grids. Combined with RL, they can\nserve as control approaches to determine remedial grid actions. This review\nanalyses the ability of GRL to capture the inherent graph structure of power\ngrids to improve representation learning and decision making in different power\ngrid use cases. It distinguishes between common problems in transmission and\ndistribution grids and explores the synergy between RL and GNNs. In\ntransmission grids, GRL typically addresses automated grid management and\ntopology control, whereas on the distribution side, GRL concentrates more on\nvoltage regulation. We analyzed the selected papers based on their graph\nstructure and GNN model, the applied RL algorithm, and their overall\ncontributions. Although GRL demonstrate adaptability in the face of\nunpredictable events and noisy or incomplete data, it primarily serves as a\nproof of concept at this stage. There are multiple open challenges and\nlimitations that need to be addressed when considering the application of RL to\nreal power grid operation.\n", "link": "http://arxiv.org/abs/2407.04522v2", "date": "2024-07-30", "relevancy": 1.7918, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.459}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4546}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Reinforcement%20Learning%20in%20Power%20Grids%3A%20A%20Survey&body=Title%3A%20Graph%20Reinforcement%20Learning%20in%20Power%20Grids%3A%20A%20Survey%0AAuthor%3A%20Mohamed%20Hassouna%20and%20Clara%20Holzh%C3%BCter%20and%20Pawel%20Lytaev%20and%20Josephine%20Thomas%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20The%20challenges%20posed%20by%20renewable%20energy%20and%20distributed%20electricity%0Ageneration%20motivate%20the%20development%20of%20deep%20learning%20approaches%20to%20overcome%20the%0Alack%20of%20flexibility%20of%20traditional%20methods%20in%20power%20grids%20use%20cases.%20The%0Aapplication%20of%20GNNs%20is%20particularly%20promising%20due%20to%20their%20ability%20to%20learn%0Afrom%20graph-structured%20data%20present%20in%20power%20grids.%20Combined%20with%20RL%2C%20they%20can%0Aserve%20as%20control%20approaches%20to%20determine%20remedial%20grid%20actions.%20This%20review%0Aanalyses%20the%20ability%20of%20GRL%20to%20capture%20the%20inherent%20graph%20structure%20of%20power%0Agrids%20to%20improve%20representation%20learning%20and%20decision%20making%20in%20different%20power%0Agrid%20use%20cases.%20It%20distinguishes%20between%20common%20problems%20in%20transmission%20and%0Adistribution%20grids%20and%20explores%20the%20synergy%20between%20RL%20and%20GNNs.%20In%0Atransmission%20grids%2C%20GRL%20typically%20addresses%20automated%20grid%20management%20and%0Atopology%20control%2C%20whereas%20on%20the%20distribution%20side%2C%20GRL%20concentrates%20more%20on%0Avoltage%20regulation.%20We%20analyzed%20the%20selected%20papers%20based%20on%20their%20graph%0Astructure%20and%20GNN%20model%2C%20the%20applied%20RL%20algorithm%2C%20and%20their%20overall%0Acontributions.%20Although%20GRL%20demonstrate%20adaptability%20in%20the%20face%20of%0Aunpredictable%20events%20and%20noisy%20or%20incomplete%20data%2C%20it%20primarily%20serves%20as%20a%0Aproof%20of%20concept%20at%20this%20stage.%20There%20are%20multiple%20open%20challenges%20and%0Alimitations%20that%20need%20to%20be%20addressed%20when%20considering%20the%20application%20of%20RL%20to%0Areal%20power%20grid%20operation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Reinforcement%2520Learning%2520in%2520Power%2520Grids%253A%2520A%2520Survey%26entry.906535625%3DMohamed%2520Hassouna%2520and%2520Clara%2520Holzh%25C3%25BCter%2520and%2520Pawel%2520Lytaev%2520and%2520Josephine%2520Thomas%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520The%2520challenges%2520posed%2520by%2520renewable%2520energy%2520and%2520distributed%2520electricity%250Ageneration%2520motivate%2520the%2520development%2520of%2520deep%2520learning%2520approaches%2520to%2520overcome%2520the%250Alack%2520of%2520flexibility%2520of%2520traditional%2520methods%2520in%2520power%2520grids%2520use%2520cases.%2520The%250Aapplication%2520of%2520GNNs%2520is%2520particularly%2520promising%2520due%2520to%2520their%2520ability%2520to%2520learn%250Afrom%2520graph-structured%2520data%2520present%2520in%2520power%2520grids.%2520Combined%2520with%2520RL%252C%2520they%2520can%250Aserve%2520as%2520control%2520approaches%2520to%2520determine%2520remedial%2520grid%2520actions.%2520This%2520review%250Aanalyses%2520the%2520ability%2520of%2520GRL%2520to%2520capture%2520the%2520inherent%2520graph%2520structure%2520of%2520power%250Agrids%2520to%2520improve%2520representation%2520learning%2520and%2520decision%2520making%2520in%2520different%2520power%250Agrid%2520use%2520cases.%2520It%2520distinguishes%2520between%2520common%2520problems%2520in%2520transmission%2520and%250Adistribution%2520grids%2520and%2520explores%2520the%2520synergy%2520between%2520RL%2520and%2520GNNs.%2520In%250Atransmission%2520grids%252C%2520GRL%2520typically%2520addresses%2520automated%2520grid%2520management%2520and%250Atopology%2520control%252C%2520whereas%2520on%2520the%2520distribution%2520side%252C%2520GRL%2520concentrates%2520more%2520on%250Avoltage%2520regulation.%2520We%2520analyzed%2520the%2520selected%2520papers%2520based%2520on%2520their%2520graph%250Astructure%2520and%2520GNN%2520model%252C%2520the%2520applied%2520RL%2520algorithm%252C%2520and%2520their%2520overall%250Acontributions.%2520Although%2520GRL%2520demonstrate%2520adaptability%2520in%2520the%2520face%2520of%250Aunpredictable%2520events%2520and%2520noisy%2520or%2520incomplete%2520data%252C%2520it%2520primarily%2520serves%2520as%2520a%250Aproof%2520of%2520concept%2520at%2520this%2520stage.%2520There%2520are%2520multiple%2520open%2520challenges%2520and%250Alimitations%2520that%2520need%2520to%2520be%2520addressed%2520when%2520considering%2520the%2520application%2520of%2520RL%2520to%250Areal%2520power%2520grid%2520operation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Reinforcement%20Learning%20in%20Power%20Grids%3A%20A%20Survey&entry.906535625=Mohamed%20Hassouna%20and%20Clara%20Holzh%C3%BCter%20and%20Pawel%20Lytaev%20and%20Josephine%20Thomas%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20The%20challenges%20posed%20by%20renewable%20energy%20and%20distributed%20electricity%0Ageneration%20motivate%20the%20development%20of%20deep%20learning%20approaches%20to%20overcome%20the%0Alack%20of%20flexibility%20of%20traditional%20methods%20in%20power%20grids%20use%20cases.%20The%0Aapplication%20of%20GNNs%20is%20particularly%20promising%20due%20to%20their%20ability%20to%20learn%0Afrom%20graph-structured%20data%20present%20in%20power%20grids.%20Combined%20with%20RL%2C%20they%20can%0Aserve%20as%20control%20approaches%20to%20determine%20remedial%20grid%20actions.%20This%20review%0Aanalyses%20the%20ability%20of%20GRL%20to%20capture%20the%20inherent%20graph%20structure%20of%20power%0Agrids%20to%20improve%20representation%20learning%20and%20decision%20making%20in%20different%20power%0Agrid%20use%20cases.%20It%20distinguishes%20between%20common%20problems%20in%20transmission%20and%0Adistribution%20grids%20and%20explores%20the%20synergy%20between%20RL%20and%20GNNs.%20In%0Atransmission%20grids%2C%20GRL%20typically%20addresses%20automated%20grid%20management%20and%0Atopology%20control%2C%20whereas%20on%20the%20distribution%20side%2C%20GRL%20concentrates%20more%20on%0Avoltage%20regulation.%20We%20analyzed%20the%20selected%20papers%20based%20on%20their%20graph%0Astructure%20and%20GNN%20model%2C%20the%20applied%20RL%20algorithm%2C%20and%20their%20overall%0Acontributions.%20Although%20GRL%20demonstrate%20adaptability%20in%20the%20face%20of%0Aunpredictable%20events%20and%20noisy%20or%20incomplete%20data%2C%20it%20primarily%20serves%20as%20a%0Aproof%20of%20concept%20at%20this%20stage.%20There%20are%20multiple%20open%20challenges%20and%0Alimitations%20that%20need%20to%20be%20addressed%20when%20considering%20the%20application%20of%20RL%20to%0Areal%20power%20grid%20operation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04522v2&entry.124074799=Read"},
{"title": "Rethinking Radiology Report Generation via Causal Inspired\n  Counterfactual Augmentation", "author": "Xiao Song and Jiafan Liu and Yun Li and Yan Liu and Wenbin Lei and Ruxin Wang", "abstract": "  Radiology Report Generation (RRG) draws attention as a vision-and-language\ninteraction of biomedical fields. Previous works inherited the ideology of\ntraditional language generation tasks, aiming to generate paragraphs with high\nreadability as reports. Despite significant progress, the independence between\ndiseases-a specific property of RRG-was neglected, yielding the models being\nconfused by the co-occurrence of diseases brought on by the biased data\ndistribution, thus generating inaccurate reports. In this paper, to rethink\nthis issue, we first model the causal effects between the variables from a\ncausal perspective, through which we prove that the co-occurrence relationships\nbetween diseases on the biased distribution function as confounders, confusing\nthe accuracy through two backdoor paths, i.e. the Joint Vision Coupling and the\nConditional Sequential Coupling. Then, we proposed a novel model-agnostic\ncounterfactual augmentation method that contains two strategies, i.e. the\nPrototype-based Counterfactual Sample Synthesis (P-CSS) and the Magic-Cube-like\nCounterfactual Report Reconstruction (Cube), to intervene the backdoor paths,\nthus enhancing the accuracy and generalization of RRG models. Experimental\nresults on the widely used MIMIC-CXR dataset demonstrate the effectiveness of\nour proposed method. Additionally, a generalization performance is evaluated on\nIU X-Ray dataset, which verifies our work can effectively reduce the impact of\nco-occurrences caused by different distributions on the results.\n", "link": "http://arxiv.org/abs/2311.13307v3", "date": "2024-07-30", "relevancy": 1.7837, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4461}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Radiology%20Report%20Generation%20via%20Causal%20Inspired%0A%20%20Counterfactual%20Augmentation&body=Title%3A%20Rethinking%20Radiology%20Report%20Generation%20via%20Causal%20Inspired%0A%20%20Counterfactual%20Augmentation%0AAuthor%3A%20Xiao%20Song%20and%20Jiafan%20Liu%20and%20Yun%20Li%20and%20Yan%20Liu%20and%20Wenbin%20Lei%20and%20Ruxin%20Wang%0AAbstract%3A%20%20%20Radiology%20Report%20Generation%20%28RRG%29%20draws%20attention%20as%20a%20vision-and-language%0Ainteraction%20of%20biomedical%20fields.%20Previous%20works%20inherited%20the%20ideology%20of%0Atraditional%20language%20generation%20tasks%2C%20aiming%20to%20generate%20paragraphs%20with%20high%0Areadability%20as%20reports.%20Despite%20significant%20progress%2C%20the%20independence%20between%0Adiseases-a%20specific%20property%20of%20RRG-was%20neglected%2C%20yielding%20the%20models%20being%0Aconfused%20by%20the%20co-occurrence%20of%20diseases%20brought%20on%20by%20the%20biased%20data%0Adistribution%2C%20thus%20generating%20inaccurate%20reports.%20In%20this%20paper%2C%20to%20rethink%0Athis%20issue%2C%20we%20first%20model%20the%20causal%20effects%20between%20the%20variables%20from%20a%0Acausal%20perspective%2C%20through%20which%20we%20prove%20that%20the%20co-occurrence%20relationships%0Abetween%20diseases%20on%20the%20biased%20distribution%20function%20as%20confounders%2C%20confusing%0Athe%20accuracy%20through%20two%20backdoor%20paths%2C%20i.e.%20the%20Joint%20Vision%20Coupling%20and%20the%0AConditional%20Sequential%20Coupling.%20Then%2C%20we%20proposed%20a%20novel%20model-agnostic%0Acounterfactual%20augmentation%20method%20that%20contains%20two%20strategies%2C%20i.e.%20the%0APrototype-based%20Counterfactual%20Sample%20Synthesis%20%28P-CSS%29%20and%20the%20Magic-Cube-like%0ACounterfactual%20Report%20Reconstruction%20%28Cube%29%2C%20to%20intervene%20the%20backdoor%20paths%2C%0Athus%20enhancing%20the%20accuracy%20and%20generalization%20of%20RRG%20models.%20Experimental%0Aresults%20on%20the%20widely%20used%20MIMIC-CXR%20dataset%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20method.%20Additionally%2C%20a%20generalization%20performance%20is%20evaluated%20on%0AIU%20X-Ray%20dataset%2C%20which%20verifies%20our%20work%20can%20effectively%20reduce%20the%20impact%20of%0Aco-occurrences%20caused%20by%20different%20distributions%20on%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13307v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Radiology%2520Report%2520Generation%2520via%2520Causal%2520Inspired%250A%2520%2520Counterfactual%2520Augmentation%26entry.906535625%3DXiao%2520Song%2520and%2520Jiafan%2520Liu%2520and%2520Yun%2520Li%2520and%2520Yan%2520Liu%2520and%2520Wenbin%2520Lei%2520and%2520Ruxin%2520Wang%26entry.1292438233%3D%2520%2520Radiology%2520Report%2520Generation%2520%2528RRG%2529%2520draws%2520attention%2520as%2520a%2520vision-and-language%250Ainteraction%2520of%2520biomedical%2520fields.%2520Previous%2520works%2520inherited%2520the%2520ideology%2520of%250Atraditional%2520language%2520generation%2520tasks%252C%2520aiming%2520to%2520generate%2520paragraphs%2520with%2520high%250Areadability%2520as%2520reports.%2520Despite%2520significant%2520progress%252C%2520the%2520independence%2520between%250Adiseases-a%2520specific%2520property%2520of%2520RRG-was%2520neglected%252C%2520yielding%2520the%2520models%2520being%250Aconfused%2520by%2520the%2520co-occurrence%2520of%2520diseases%2520brought%2520on%2520by%2520the%2520biased%2520data%250Adistribution%252C%2520thus%2520generating%2520inaccurate%2520reports.%2520In%2520this%2520paper%252C%2520to%2520rethink%250Athis%2520issue%252C%2520we%2520first%2520model%2520the%2520causal%2520effects%2520between%2520the%2520variables%2520from%2520a%250Acausal%2520perspective%252C%2520through%2520which%2520we%2520prove%2520that%2520the%2520co-occurrence%2520relationships%250Abetween%2520diseases%2520on%2520the%2520biased%2520distribution%2520function%2520as%2520confounders%252C%2520confusing%250Athe%2520accuracy%2520through%2520two%2520backdoor%2520paths%252C%2520i.e.%2520the%2520Joint%2520Vision%2520Coupling%2520and%2520the%250AConditional%2520Sequential%2520Coupling.%2520Then%252C%2520we%2520proposed%2520a%2520novel%2520model-agnostic%250Acounterfactual%2520augmentation%2520method%2520that%2520contains%2520two%2520strategies%252C%2520i.e.%2520the%250APrototype-based%2520Counterfactual%2520Sample%2520Synthesis%2520%2528P-CSS%2529%2520and%2520the%2520Magic-Cube-like%250ACounterfactual%2520Report%2520Reconstruction%2520%2528Cube%2529%252C%2520to%2520intervene%2520the%2520backdoor%2520paths%252C%250Athus%2520enhancing%2520the%2520accuracy%2520and%2520generalization%2520of%2520RRG%2520models.%2520Experimental%250Aresults%2520on%2520the%2520widely%2520used%2520MIMIC-CXR%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520proposed%2520method.%2520Additionally%252C%2520a%2520generalization%2520performance%2520is%2520evaluated%2520on%250AIU%2520X-Ray%2520dataset%252C%2520which%2520verifies%2520our%2520work%2520can%2520effectively%2520reduce%2520the%2520impact%2520of%250Aco-occurrences%2520caused%2520by%2520different%2520distributions%2520on%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13307v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Radiology%20Report%20Generation%20via%20Causal%20Inspired%0A%20%20Counterfactual%20Augmentation&entry.906535625=Xiao%20Song%20and%20Jiafan%20Liu%20and%20Yun%20Li%20and%20Yan%20Liu%20and%20Wenbin%20Lei%20and%20Ruxin%20Wang&entry.1292438233=%20%20Radiology%20Report%20Generation%20%28RRG%29%20draws%20attention%20as%20a%20vision-and-language%0Ainteraction%20of%20biomedical%20fields.%20Previous%20works%20inherited%20the%20ideology%20of%0Atraditional%20language%20generation%20tasks%2C%20aiming%20to%20generate%20paragraphs%20with%20high%0Areadability%20as%20reports.%20Despite%20significant%20progress%2C%20the%20independence%20between%0Adiseases-a%20specific%20property%20of%20RRG-was%20neglected%2C%20yielding%20the%20models%20being%0Aconfused%20by%20the%20co-occurrence%20of%20diseases%20brought%20on%20by%20the%20biased%20data%0Adistribution%2C%20thus%20generating%20inaccurate%20reports.%20In%20this%20paper%2C%20to%20rethink%0Athis%20issue%2C%20we%20first%20model%20the%20causal%20effects%20between%20the%20variables%20from%20a%0Acausal%20perspective%2C%20through%20which%20we%20prove%20that%20the%20co-occurrence%20relationships%0Abetween%20diseases%20on%20the%20biased%20distribution%20function%20as%20confounders%2C%20confusing%0Athe%20accuracy%20through%20two%20backdoor%20paths%2C%20i.e.%20the%20Joint%20Vision%20Coupling%20and%20the%0AConditional%20Sequential%20Coupling.%20Then%2C%20we%20proposed%20a%20novel%20model-agnostic%0Acounterfactual%20augmentation%20method%20that%20contains%20two%20strategies%2C%20i.e.%20the%0APrototype-based%20Counterfactual%20Sample%20Synthesis%20%28P-CSS%29%20and%20the%20Magic-Cube-like%0ACounterfactual%20Report%20Reconstruction%20%28Cube%29%2C%20to%20intervene%20the%20backdoor%20paths%2C%0Athus%20enhancing%20the%20accuracy%20and%20generalization%20of%20RRG%20models.%20Experimental%0Aresults%20on%20the%20widely%20used%20MIMIC-CXR%20dataset%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20method.%20Additionally%2C%20a%20generalization%20performance%20is%20evaluated%20on%0AIU%20X-Ray%20dataset%2C%20which%20verifies%20our%20work%20can%20effectively%20reduce%20the%20impact%20of%0Aco-occurrences%20caused%20by%20different%20distributions%20on%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13307v3&entry.124074799=Read"},
{"title": "Look Around and Learn: Self-Training Object Detection by Exploration", "author": "Gianluca Scarpellini and Stefano Rosa and Pietro Morerio and Lorenzo Natale and Alessio Del Bue", "abstract": "  When an object detector is deployed in a novel setting it often experiences a\ndrop in performance. This paper studies how an embodied agent can automatically\nfine-tune a pre-existing object detector while exploring and acquiring images\nin a new environment without relying on human intervention, i.e., a fully\nself-supervised approach. In our setting, an agent initially learns to explore\nthe environment using a pre-trained off-the-shelf detector to locate objects\nand associate pseudo-labels. By assuming that pseudo-labels for the same object\nmust be consistent across different views, we learn the exploration policy Look\nAround to mine hard samples, and we devise a novel mechanism called\nDisagreement Reconciliation for producing refined pseudo-labels from the\nconsensus among observations. We implement a unified benchmark of the current\nstate-of-the-art and compare our approach with pre-existing exploration\npolicies and perception mechanisms. Our method is shown to outperform existing\napproaches, improving the object detector by 6.2% in a simulated scenario, a\n3.59% advancement over other state-of-the-art methods, and by 9.97% in the real\nrobotic test without relying on ground-truth. Code for the proposed approach\nand baselines are available at\nhttps://iit-pavis.github.io/Look_Around_And_Learn/.\n", "link": "http://arxiv.org/abs/2302.03566v4", "date": "2024-07-30", "relevancy": 1.7709, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6078}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5943}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Around%20and%20Learn%3A%20Self-Training%20Object%20Detection%20by%20Exploration&body=Title%3A%20Look%20Around%20and%20Learn%3A%20Self-Training%20Object%20Detection%20by%20Exploration%0AAuthor%3A%20Gianluca%20Scarpellini%20and%20Stefano%20Rosa%20and%20Pietro%20Morerio%20and%20Lorenzo%20Natale%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20When%20an%20object%20detector%20is%20deployed%20in%20a%20novel%20setting%20it%20often%20experiences%20a%0Adrop%20in%20performance.%20This%20paper%20studies%20how%20an%20embodied%20agent%20can%20automatically%0Afine-tune%20a%20pre-existing%20object%20detector%20while%20exploring%20and%20acquiring%20images%0Ain%20a%20new%20environment%20without%20relying%20on%20human%20intervention%2C%20i.e.%2C%20a%20fully%0Aself-supervised%20approach.%20In%20our%20setting%2C%20an%20agent%20initially%20learns%20to%20explore%0Athe%20environment%20using%20a%20pre-trained%20off-the-shelf%20detector%20to%20locate%20objects%0Aand%20associate%20pseudo-labels.%20By%20assuming%20that%20pseudo-labels%20for%20the%20same%20object%0Amust%20be%20consistent%20across%20different%20views%2C%20we%20learn%20the%20exploration%20policy%20Look%0AAround%20to%20mine%20hard%20samples%2C%20and%20we%20devise%20a%20novel%20mechanism%20called%0ADisagreement%20Reconciliation%20for%20producing%20refined%20pseudo-labels%20from%20the%0Aconsensus%20among%20observations.%20We%20implement%20a%20unified%20benchmark%20of%20the%20current%0Astate-of-the-art%20and%20compare%20our%20approach%20with%20pre-existing%20exploration%0Apolicies%20and%20perception%20mechanisms.%20Our%20method%20is%20shown%20to%20outperform%20existing%0Aapproaches%2C%20improving%20the%20object%20detector%20by%206.2%25%20in%20a%20simulated%20scenario%2C%20a%0A3.59%25%20advancement%20over%20other%20state-of-the-art%20methods%2C%20and%20by%209.97%25%20in%20the%20real%0Arobotic%20test%20without%20relying%20on%20ground-truth.%20Code%20for%20the%20proposed%20approach%0Aand%20baselines%20are%20available%20at%0Ahttps%3A//iit-pavis.github.io/Look_Around_And_Learn/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03566v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Around%2520and%2520Learn%253A%2520Self-Training%2520Object%2520Detection%2520by%2520Exploration%26entry.906535625%3DGianluca%2520Scarpellini%2520and%2520Stefano%2520Rosa%2520and%2520Pietro%2520Morerio%2520and%2520Lorenzo%2520Natale%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520When%2520an%2520object%2520detector%2520is%2520deployed%2520in%2520a%2520novel%2520setting%2520it%2520often%2520experiences%2520a%250Adrop%2520in%2520performance.%2520This%2520paper%2520studies%2520how%2520an%2520embodied%2520agent%2520can%2520automatically%250Afine-tune%2520a%2520pre-existing%2520object%2520detector%2520while%2520exploring%2520and%2520acquiring%2520images%250Ain%2520a%2520new%2520environment%2520without%2520relying%2520on%2520human%2520intervention%252C%2520i.e.%252C%2520a%2520fully%250Aself-supervised%2520approach.%2520In%2520our%2520setting%252C%2520an%2520agent%2520initially%2520learns%2520to%2520explore%250Athe%2520environment%2520using%2520a%2520pre-trained%2520off-the-shelf%2520detector%2520to%2520locate%2520objects%250Aand%2520associate%2520pseudo-labels.%2520By%2520assuming%2520that%2520pseudo-labels%2520for%2520the%2520same%2520object%250Amust%2520be%2520consistent%2520across%2520different%2520views%252C%2520we%2520learn%2520the%2520exploration%2520policy%2520Look%250AAround%2520to%2520mine%2520hard%2520samples%252C%2520and%2520we%2520devise%2520a%2520novel%2520mechanism%2520called%250ADisagreement%2520Reconciliation%2520for%2520producing%2520refined%2520pseudo-labels%2520from%2520the%250Aconsensus%2520among%2520observations.%2520We%2520implement%2520a%2520unified%2520benchmark%2520of%2520the%2520current%250Astate-of-the-art%2520and%2520compare%2520our%2520approach%2520with%2520pre-existing%2520exploration%250Apolicies%2520and%2520perception%2520mechanisms.%2520Our%2520method%2520is%2520shown%2520to%2520outperform%2520existing%250Aapproaches%252C%2520improving%2520the%2520object%2520detector%2520by%25206.2%2525%2520in%2520a%2520simulated%2520scenario%252C%2520a%250A3.59%2525%2520advancement%2520over%2520other%2520state-of-the-art%2520methods%252C%2520and%2520by%25209.97%2525%2520in%2520the%2520real%250Arobotic%2520test%2520without%2520relying%2520on%2520ground-truth.%2520Code%2520for%2520the%2520proposed%2520approach%250Aand%2520baselines%2520are%2520available%2520at%250Ahttps%253A//iit-pavis.github.io/Look_Around_And_Learn/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.03566v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Around%20and%20Learn%3A%20Self-Training%20Object%20Detection%20by%20Exploration&entry.906535625=Gianluca%20Scarpellini%20and%20Stefano%20Rosa%20and%20Pietro%20Morerio%20and%20Lorenzo%20Natale%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20When%20an%20object%20detector%20is%20deployed%20in%20a%20novel%20setting%20it%20often%20experiences%20a%0Adrop%20in%20performance.%20This%20paper%20studies%20how%20an%20embodied%20agent%20can%20automatically%0Afine-tune%20a%20pre-existing%20object%20detector%20while%20exploring%20and%20acquiring%20images%0Ain%20a%20new%20environment%20without%20relying%20on%20human%20intervention%2C%20i.e.%2C%20a%20fully%0Aself-supervised%20approach.%20In%20our%20setting%2C%20an%20agent%20initially%20learns%20to%20explore%0Athe%20environment%20using%20a%20pre-trained%20off-the-shelf%20detector%20to%20locate%20objects%0Aand%20associate%20pseudo-labels.%20By%20assuming%20that%20pseudo-labels%20for%20the%20same%20object%0Amust%20be%20consistent%20across%20different%20views%2C%20we%20learn%20the%20exploration%20policy%20Look%0AAround%20to%20mine%20hard%20samples%2C%20and%20we%20devise%20a%20novel%20mechanism%20called%0ADisagreement%20Reconciliation%20for%20producing%20refined%20pseudo-labels%20from%20the%0Aconsensus%20among%20observations.%20We%20implement%20a%20unified%20benchmark%20of%20the%20current%0Astate-of-the-art%20and%20compare%20our%20approach%20with%20pre-existing%20exploration%0Apolicies%20and%20perception%20mechanisms.%20Our%20method%20is%20shown%20to%20outperform%20existing%0Aapproaches%2C%20improving%20the%20object%20detector%20by%206.2%25%20in%20a%20simulated%20scenario%2C%20a%0A3.59%25%20advancement%20over%20other%20state-of-the-art%20methods%2C%20and%20by%209.97%25%20in%20the%20real%0Arobotic%20test%20without%20relying%20on%20ground-truth.%20Code%20for%20the%20proposed%20approach%0Aand%20baselines%20are%20available%20at%0Ahttps%3A//iit-pavis.github.io/Look_Around_And_Learn/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03566v4&entry.124074799=Read"},
{"title": "F-KANs: Federated Kolmogorov-Arnold Networks", "author": "Engin Zeydan and Cristian J. Vaca-Rubio and Luis Blanco and Roberto Pereira and Marius Caus and Abdullah Aydeger", "abstract": "  In this paper, we present an innovative federated learning (FL) approach that\nutilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. By\nutilizing the adaptive activation capabilities of KANs in a federated\nframework, we aim to improve classification capabilities while preserving\nprivacy. The study evaluates the performance of federated KANs (F- KANs)\ncompared to traditional Multi-Layer Perceptrons (MLPs) on classification task.\nThe results show that the F-KANs model significantly outperforms the federated\nMLP model in terms of accuracy, precision, recall, F1 score and stability, and\nachieves better performance, paving the way for more efficient and\nprivacy-preserving predictive analytics.\n", "link": "http://arxiv.org/abs/2407.20100v2", "date": "2024-07-30", "relevancy": 1.7547, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4485}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4386}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F-KANs%3A%20Federated%20Kolmogorov-Arnold%20Networks&body=Title%3A%20F-KANs%3A%20Federated%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Engin%20Zeydan%20and%20Cristian%20J.%20Vaca-Rubio%20and%20Luis%20Blanco%20and%20Roberto%20Pereira%20and%20Marius%20Caus%20and%20Abdullah%20Aydeger%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20an%20innovative%20federated%20learning%20%28FL%29%20approach%20that%0Autilizes%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20for%20classification%20tasks.%20By%0Autilizing%20the%20adaptive%20activation%20capabilities%20of%20KANs%20in%20a%20federated%0Aframework%2C%20we%20aim%20to%20improve%20classification%20capabilities%20while%20preserving%0Aprivacy.%20The%20study%20evaluates%20the%20performance%20of%20federated%20KANs%20%28F-%20KANs%29%0Acompared%20to%20traditional%20Multi-Layer%20Perceptrons%20%28MLPs%29%20on%20classification%20task.%0AThe%20results%20show%20that%20the%20F-KANs%20model%20significantly%20outperforms%20the%20federated%0AMLP%20model%20in%20terms%20of%20accuracy%2C%20precision%2C%20recall%2C%20F1%20score%20and%20stability%2C%20and%0Aachieves%20better%20performance%2C%20paving%20the%20way%20for%20more%20efficient%20and%0Aprivacy-preserving%20predictive%20analytics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF-KANs%253A%2520Federated%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DEngin%2520Zeydan%2520and%2520Cristian%2520J.%2520Vaca-Rubio%2520and%2520Luis%2520Blanco%2520and%2520Roberto%2520Pereira%2520and%2520Marius%2520Caus%2520and%2520Abdullah%2520Aydeger%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520innovative%2520federated%2520learning%2520%2528FL%2529%2520approach%2520that%250Autilizes%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520for%2520classification%2520tasks.%2520By%250Autilizing%2520the%2520adaptive%2520activation%2520capabilities%2520of%2520KANs%2520in%2520a%2520federated%250Aframework%252C%2520we%2520aim%2520to%2520improve%2520classification%2520capabilities%2520while%2520preserving%250Aprivacy.%2520The%2520study%2520evaluates%2520the%2520performance%2520of%2520federated%2520KANs%2520%2528F-%2520KANs%2529%250Acompared%2520to%2520traditional%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%2520on%2520classification%2520task.%250AThe%2520results%2520show%2520that%2520the%2520F-KANs%2520model%2520significantly%2520outperforms%2520the%2520federated%250AMLP%2520model%2520in%2520terms%2520of%2520accuracy%252C%2520precision%252C%2520recall%252C%2520F1%2520score%2520and%2520stability%252C%2520and%250Aachieves%2520better%2520performance%252C%2520paving%2520the%2520way%2520for%2520more%2520efficient%2520and%250Aprivacy-preserving%2520predictive%2520analytics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F-KANs%3A%20Federated%20Kolmogorov-Arnold%20Networks&entry.906535625=Engin%20Zeydan%20and%20Cristian%20J.%20Vaca-Rubio%20and%20Luis%20Blanco%20and%20Roberto%20Pereira%20and%20Marius%20Caus%20and%20Abdullah%20Aydeger&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20an%20innovative%20federated%20learning%20%28FL%29%20approach%20that%0Autilizes%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20for%20classification%20tasks.%20By%0Autilizing%20the%20adaptive%20activation%20capabilities%20of%20KANs%20in%20a%20federated%0Aframework%2C%20we%20aim%20to%20improve%20classification%20capabilities%20while%20preserving%0Aprivacy.%20The%20study%20evaluates%20the%20performance%20of%20federated%20KANs%20%28F-%20KANs%29%0Acompared%20to%20traditional%20Multi-Layer%20Perceptrons%20%28MLPs%29%20on%20classification%20task.%0AThe%20results%20show%20that%20the%20F-KANs%20model%20significantly%20outperforms%20the%20federated%0AMLP%20model%20in%20terms%20of%20accuracy%2C%20precision%2C%20recall%2C%20F1%20score%20and%20stability%2C%20and%0Aachieves%20better%20performance%2C%20paving%20the%20way%20for%20more%20efficient%20and%0Aprivacy-preserving%20predictive%20analytics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20100v2&entry.124074799=Read"},
{"title": "Automatic Die Studies for Ancient Numismatics", "author": "Cl\u00e9ment Cornet and H\u00e9lo\u00efse Auma\u00eetre and Romaric Besan\u00e7on and Julien Olivier and Thomas Faucher and Herv\u00e9 Le Borgne", "abstract": "  Die studies are fundamental to quantifying ancient monetary production,\nproviding insights into the relationship between coinage, politics, and\nhistory. The process requires tedious manual work, which limits the size of the\ncorpora that can be studied. Few works have attempted to automate this task,\nand none have been properly released and evaluated from a computer vision\nperspective. We propose a fully automatic approach that introduces several\ninnovations compared to previous methods. We rely on fast and robust local\ndescriptors matching that is set automatically. Second, the core of our\nproposal is a clustering-based approach that uses an intrinsic metric (that\ndoes not need the ground truth labels) to determine its critical\nhyper-parameters. We validate the approach on two corpora of Greek coins,\npropose an automatic implementation and evaluation of previous baselines, and\nshow that our approach significantly outperforms them.\n", "link": "http://arxiv.org/abs/2407.20876v1", "date": "2024-07-30", "relevancy": 1.7539, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4393}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Die%20Studies%20for%20Ancient%20Numismatics&body=Title%3A%20Automatic%20Die%20Studies%20for%20Ancient%20Numismatics%0AAuthor%3A%20Cl%C3%A9ment%20Cornet%20and%20H%C3%A9lo%C3%AFse%20Auma%C3%AEtre%20and%20Romaric%20Besan%C3%A7on%20and%20Julien%20Olivier%20and%20Thomas%20Faucher%20and%20Herv%C3%A9%20Le%20Borgne%0AAbstract%3A%20%20%20Die%20studies%20are%20fundamental%20to%20quantifying%20ancient%20monetary%20production%2C%0Aproviding%20insights%20into%20the%20relationship%20between%20coinage%2C%20politics%2C%20and%0Ahistory.%20The%20process%20requires%20tedious%20manual%20work%2C%20which%20limits%20the%20size%20of%20the%0Acorpora%20that%20can%20be%20studied.%20Few%20works%20have%20attempted%20to%20automate%20this%20task%2C%0Aand%20none%20have%20been%20properly%20released%20and%20evaluated%20from%20a%20computer%20vision%0Aperspective.%20We%20propose%20a%20fully%20automatic%20approach%20that%20introduces%20several%0Ainnovations%20compared%20to%20previous%20methods.%20We%20rely%20on%20fast%20and%20robust%20local%0Adescriptors%20matching%20that%20is%20set%20automatically.%20Second%2C%20the%20core%20of%20our%0Aproposal%20is%20a%20clustering-based%20approach%20that%20uses%20an%20intrinsic%20metric%20%28that%0Adoes%20not%20need%20the%20ground%20truth%20labels%29%20to%20determine%20its%20critical%0Ahyper-parameters.%20We%20validate%20the%20approach%20on%20two%20corpora%20of%20Greek%20coins%2C%0Apropose%20an%20automatic%20implementation%20and%20evaluation%20of%20previous%20baselines%2C%20and%0Ashow%20that%20our%20approach%20significantly%20outperforms%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Die%2520Studies%2520for%2520Ancient%2520Numismatics%26entry.906535625%3DCl%25C3%25A9ment%2520Cornet%2520and%2520H%25C3%25A9lo%25C3%25AFse%2520Auma%25C3%25AEtre%2520and%2520Romaric%2520Besan%25C3%25A7on%2520and%2520Julien%2520Olivier%2520and%2520Thomas%2520Faucher%2520and%2520Herv%25C3%25A9%2520Le%2520Borgne%26entry.1292438233%3D%2520%2520Die%2520studies%2520are%2520fundamental%2520to%2520quantifying%2520ancient%2520monetary%2520production%252C%250Aproviding%2520insights%2520into%2520the%2520relationship%2520between%2520coinage%252C%2520politics%252C%2520and%250Ahistory.%2520The%2520process%2520requires%2520tedious%2520manual%2520work%252C%2520which%2520limits%2520the%2520size%2520of%2520the%250Acorpora%2520that%2520can%2520be%2520studied.%2520Few%2520works%2520have%2520attempted%2520to%2520automate%2520this%2520task%252C%250Aand%2520none%2520have%2520been%2520properly%2520released%2520and%2520evaluated%2520from%2520a%2520computer%2520vision%250Aperspective.%2520We%2520propose%2520a%2520fully%2520automatic%2520approach%2520that%2520introduces%2520several%250Ainnovations%2520compared%2520to%2520previous%2520methods.%2520We%2520rely%2520on%2520fast%2520and%2520robust%2520local%250Adescriptors%2520matching%2520that%2520is%2520set%2520automatically.%2520Second%252C%2520the%2520core%2520of%2520our%250Aproposal%2520is%2520a%2520clustering-based%2520approach%2520that%2520uses%2520an%2520intrinsic%2520metric%2520%2528that%250Adoes%2520not%2520need%2520the%2520ground%2520truth%2520labels%2529%2520to%2520determine%2520its%2520critical%250Ahyper-parameters.%2520We%2520validate%2520the%2520approach%2520on%2520two%2520corpora%2520of%2520Greek%2520coins%252C%250Apropose%2520an%2520automatic%2520implementation%2520and%2520evaluation%2520of%2520previous%2520baselines%252C%2520and%250Ashow%2520that%2520our%2520approach%2520significantly%2520outperforms%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Die%20Studies%20for%20Ancient%20Numismatics&entry.906535625=Cl%C3%A9ment%20Cornet%20and%20H%C3%A9lo%C3%AFse%20Auma%C3%AEtre%20and%20Romaric%20Besan%C3%A7on%20and%20Julien%20Olivier%20and%20Thomas%20Faucher%20and%20Herv%C3%A9%20Le%20Borgne&entry.1292438233=%20%20Die%20studies%20are%20fundamental%20to%20quantifying%20ancient%20monetary%20production%2C%0Aproviding%20insights%20into%20the%20relationship%20between%20coinage%2C%20politics%2C%20and%0Ahistory.%20The%20process%20requires%20tedious%20manual%20work%2C%20which%20limits%20the%20size%20of%20the%0Acorpora%20that%20can%20be%20studied.%20Few%20works%20have%20attempted%20to%20automate%20this%20task%2C%0Aand%20none%20have%20been%20properly%20released%20and%20evaluated%20from%20a%20computer%20vision%0Aperspective.%20We%20propose%20a%20fully%20automatic%20approach%20that%20introduces%20several%0Ainnovations%20compared%20to%20previous%20methods.%20We%20rely%20on%20fast%20and%20robust%20local%0Adescriptors%20matching%20that%20is%20set%20automatically.%20Second%2C%20the%20core%20of%20our%0Aproposal%20is%20a%20clustering-based%20approach%20that%20uses%20an%20intrinsic%20metric%20%28that%0Adoes%20not%20need%20the%20ground%20truth%20labels%29%20to%20determine%20its%20critical%0Ahyper-parameters.%20We%20validate%20the%20approach%20on%20two%20corpora%20of%20Greek%20coins%2C%0Apropose%20an%20automatic%20implementation%20and%20evaluation%20of%20previous%20baselines%2C%20and%0Ashow%20that%20our%20approach%20significantly%20outperforms%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20876v1&entry.124074799=Read"},
{"title": "Accounting for shared covariates in semi-parametric Bayesian additive\n  regression trees", "author": "Estev\u00e3o B. Prado and Andrew C. Parnell and Keefe Murphy and Nathan McJames and Ann O'Shea and Rafael A. Moral", "abstract": "  We propose some extensions to semi-parametric models based on Bayesian\nadditive regression trees (BART). In the semi-parametric BART paradigm, the\nresponse variable is approximated by a linear predictor and a BART model, where\nthe linear component is responsible for estimating the main effects and BART\naccounts for non-specified interactions and non-linearities. Previous\nsemi-parametric models based on BART have assumed that the set of covariates in\nthe linear predictor and the BART model are mutually exclusive in an attempt to\navoid poor coverage properties and reduce bias in the estimates of the\nparameters in the linear predictor. The main novelty in our approach lies in\nthe way we change the tree-generation moves in BART to deal with this bias and\nresolve non-identifiability issues between the parametric and non-parametric\ncomponents, even when they have covariates in common. This allows us to model\ncomplex interactions involving the covariates of primary interest, both among\nthemselves and with those in the BART component. Our novel method is developed\nwith a view to analysing data from an international education assessment, where\ncertain predictors of students' achievements in mathematics are of particular\ninterpretational interest. Through additional simulation studies and another\napplication to a well-known benchmark dataset, we also show competitive\nperformance when compared to regression models, alternative formulations of\nsemi-parametric BART, and other tree-based methods. The implementation of the\nproposed method is available at \\url{https://github.com/ebprado/CSP-BART}.\n", "link": "http://arxiv.org/abs/2108.07636v7", "date": "2024-07-30", "relevancy": 1.7468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4285}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accounting%20for%20shared%20covariates%20in%20semi-parametric%20Bayesian%20additive%0A%20%20regression%20trees&body=Title%3A%20Accounting%20for%20shared%20covariates%20in%20semi-parametric%20Bayesian%20additive%0A%20%20regression%20trees%0AAuthor%3A%20Estev%C3%A3o%20B.%20Prado%20and%20Andrew%20C.%20Parnell%20and%20Keefe%20Murphy%20and%20Nathan%20McJames%20and%20Ann%20O%27Shea%20and%20Rafael%20A.%20Moral%0AAbstract%3A%20%20%20We%20propose%20some%20extensions%20to%20semi-parametric%20models%20based%20on%20Bayesian%0Aadditive%20regression%20trees%20%28BART%29.%20In%20the%20semi-parametric%20BART%20paradigm%2C%20the%0Aresponse%20variable%20is%20approximated%20by%20a%20linear%20predictor%20and%20a%20BART%20model%2C%20where%0Athe%20linear%20component%20is%20responsible%20for%20estimating%20the%20main%20effects%20and%20BART%0Aaccounts%20for%20non-specified%20interactions%20and%20non-linearities.%20Previous%0Asemi-parametric%20models%20based%20on%20BART%20have%20assumed%20that%20the%20set%20of%20covariates%20in%0Athe%20linear%20predictor%20and%20the%20BART%20model%20are%20mutually%20exclusive%20in%20an%20attempt%20to%0Aavoid%20poor%20coverage%20properties%20and%20reduce%20bias%20in%20the%20estimates%20of%20the%0Aparameters%20in%20the%20linear%20predictor.%20The%20main%20novelty%20in%20our%20approach%20lies%20in%0Athe%20way%20we%20change%20the%20tree-generation%20moves%20in%20BART%20to%20deal%20with%20this%20bias%20and%0Aresolve%20non-identifiability%20issues%20between%20the%20parametric%20and%20non-parametric%0Acomponents%2C%20even%20when%20they%20have%20covariates%20in%20common.%20This%20allows%20us%20to%20model%0Acomplex%20interactions%20involving%20the%20covariates%20of%20primary%20interest%2C%20both%20among%0Athemselves%20and%20with%20those%20in%20the%20BART%20component.%20Our%20novel%20method%20is%20developed%0Awith%20a%20view%20to%20analysing%20data%20from%20an%20international%20education%20assessment%2C%20where%0Acertain%20predictors%20of%20students%27%20achievements%20in%20mathematics%20are%20of%20particular%0Ainterpretational%20interest.%20Through%20additional%20simulation%20studies%20and%20another%0Aapplication%20to%20a%20well-known%20benchmark%20dataset%2C%20we%20also%20show%20competitive%0Aperformance%20when%20compared%20to%20regression%20models%2C%20alternative%20formulations%20of%0Asemi-parametric%20BART%2C%20and%20other%20tree-based%20methods.%20The%20implementation%20of%20the%0Aproposed%20method%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/ebprado/CSP-BART%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.07636v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccounting%2520for%2520shared%2520covariates%2520in%2520semi-parametric%2520Bayesian%2520additive%250A%2520%2520regression%2520trees%26entry.906535625%3DEstev%25C3%25A3o%2520B.%2520Prado%2520and%2520Andrew%2520C.%2520Parnell%2520and%2520Keefe%2520Murphy%2520and%2520Nathan%2520McJames%2520and%2520Ann%2520O%2527Shea%2520and%2520Rafael%2520A.%2520Moral%26entry.1292438233%3D%2520%2520We%2520propose%2520some%2520extensions%2520to%2520semi-parametric%2520models%2520based%2520on%2520Bayesian%250Aadditive%2520regression%2520trees%2520%2528BART%2529.%2520In%2520the%2520semi-parametric%2520BART%2520paradigm%252C%2520the%250Aresponse%2520variable%2520is%2520approximated%2520by%2520a%2520linear%2520predictor%2520and%2520a%2520BART%2520model%252C%2520where%250Athe%2520linear%2520component%2520is%2520responsible%2520for%2520estimating%2520the%2520main%2520effects%2520and%2520BART%250Aaccounts%2520for%2520non-specified%2520interactions%2520and%2520non-linearities.%2520Previous%250Asemi-parametric%2520models%2520based%2520on%2520BART%2520have%2520assumed%2520that%2520the%2520set%2520of%2520covariates%2520in%250Athe%2520linear%2520predictor%2520and%2520the%2520BART%2520model%2520are%2520mutually%2520exclusive%2520in%2520an%2520attempt%2520to%250Aavoid%2520poor%2520coverage%2520properties%2520and%2520reduce%2520bias%2520in%2520the%2520estimates%2520of%2520the%250Aparameters%2520in%2520the%2520linear%2520predictor.%2520The%2520main%2520novelty%2520in%2520our%2520approach%2520lies%2520in%250Athe%2520way%2520we%2520change%2520the%2520tree-generation%2520moves%2520in%2520BART%2520to%2520deal%2520with%2520this%2520bias%2520and%250Aresolve%2520non-identifiability%2520issues%2520between%2520the%2520parametric%2520and%2520non-parametric%250Acomponents%252C%2520even%2520when%2520they%2520have%2520covariates%2520in%2520common.%2520This%2520allows%2520us%2520to%2520model%250Acomplex%2520interactions%2520involving%2520the%2520covariates%2520of%2520primary%2520interest%252C%2520both%2520among%250Athemselves%2520and%2520with%2520those%2520in%2520the%2520BART%2520component.%2520Our%2520novel%2520method%2520is%2520developed%250Awith%2520a%2520view%2520to%2520analysing%2520data%2520from%2520an%2520international%2520education%2520assessment%252C%2520where%250Acertain%2520predictors%2520of%2520students%2527%2520achievements%2520in%2520mathematics%2520are%2520of%2520particular%250Ainterpretational%2520interest.%2520Through%2520additional%2520simulation%2520studies%2520and%2520another%250Aapplication%2520to%2520a%2520well-known%2520benchmark%2520dataset%252C%2520we%2520also%2520show%2520competitive%250Aperformance%2520when%2520compared%2520to%2520regression%2520models%252C%2520alternative%2520formulations%2520of%250Asemi-parametric%2520BART%252C%2520and%2520other%2520tree-based%2520methods.%2520The%2520implementation%2520of%2520the%250Aproposed%2520method%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/ebprado/CSP-BART%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.07636v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accounting%20for%20shared%20covariates%20in%20semi-parametric%20Bayesian%20additive%0A%20%20regression%20trees&entry.906535625=Estev%C3%A3o%20B.%20Prado%20and%20Andrew%20C.%20Parnell%20and%20Keefe%20Murphy%20and%20Nathan%20McJames%20and%20Ann%20O%27Shea%20and%20Rafael%20A.%20Moral&entry.1292438233=%20%20We%20propose%20some%20extensions%20to%20semi-parametric%20models%20based%20on%20Bayesian%0Aadditive%20regression%20trees%20%28BART%29.%20In%20the%20semi-parametric%20BART%20paradigm%2C%20the%0Aresponse%20variable%20is%20approximated%20by%20a%20linear%20predictor%20and%20a%20BART%20model%2C%20where%0Athe%20linear%20component%20is%20responsible%20for%20estimating%20the%20main%20effects%20and%20BART%0Aaccounts%20for%20non-specified%20interactions%20and%20non-linearities.%20Previous%0Asemi-parametric%20models%20based%20on%20BART%20have%20assumed%20that%20the%20set%20of%20covariates%20in%0Athe%20linear%20predictor%20and%20the%20BART%20model%20are%20mutually%20exclusive%20in%20an%20attempt%20to%0Aavoid%20poor%20coverage%20properties%20and%20reduce%20bias%20in%20the%20estimates%20of%20the%0Aparameters%20in%20the%20linear%20predictor.%20The%20main%20novelty%20in%20our%20approach%20lies%20in%0Athe%20way%20we%20change%20the%20tree-generation%20moves%20in%20BART%20to%20deal%20with%20this%20bias%20and%0Aresolve%20non-identifiability%20issues%20between%20the%20parametric%20and%20non-parametric%0Acomponents%2C%20even%20when%20they%20have%20covariates%20in%20common.%20This%20allows%20us%20to%20model%0Acomplex%20interactions%20involving%20the%20covariates%20of%20primary%20interest%2C%20both%20among%0Athemselves%20and%20with%20those%20in%20the%20BART%20component.%20Our%20novel%20method%20is%20developed%0Awith%20a%20view%20to%20analysing%20data%20from%20an%20international%20education%20assessment%2C%20where%0Acertain%20predictors%20of%20students%27%20achievements%20in%20mathematics%20are%20of%20particular%0Ainterpretational%20interest.%20Through%20additional%20simulation%20studies%20and%20another%0Aapplication%20to%20a%20well-known%20benchmark%20dataset%2C%20we%20also%20show%20competitive%0Aperformance%20when%20compared%20to%20regression%20models%2C%20alternative%20formulations%20of%0Asemi-parametric%20BART%2C%20and%20other%20tree-based%20methods.%20The%20implementation%20of%20the%0Aproposed%20method%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/ebprado/CSP-BART%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.07636v7&entry.124074799=Read"},
{"title": "Neural networks for bifurcation and linear stability analysis of steady\n  states in partial differential equations", "author": "Muhammad Luthfi Shahab and Hadi Susanto", "abstract": "  This research introduces an extended application of neural networks for\nsolving nonlinear partial differential equations (PDEs). A neural network,\ncombined with a pseudo-arclength continuation, is proposed to construct\nbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural\nnetwork approach is also presented for solving eigenvalue problems to analyze\nsolution linear stability, focusing on identifying the largest eigenvalue. The\neffectiveness of the proposed neural network is examined through experiments on\nthe Bratu equation and the Burgers equation. Results from a finite difference\nmethod are also presented as comparison. Varying numbers of grid points are\nemployed in each case to assess the behavior and accuracy of both the neural\nnetwork and the finite difference method. The experimental results demonstrate\nthat the proposed neural network produces better solutions, generates more\naccurate bifurcation diagrams, has reasonable computational times, and proves\neffective for linear stability analysis.\n", "link": "http://arxiv.org/abs/2407.19707v2", "date": "2024-07-30", "relevancy": 1.7366, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4259}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20networks%20for%20bifurcation%20and%20linear%20stability%20analysis%20of%20steady%0A%20%20states%20in%20partial%20differential%20equations&body=Title%3A%20Neural%20networks%20for%20bifurcation%20and%20linear%20stability%20analysis%20of%20steady%0A%20%20states%20in%20partial%20differential%20equations%0AAuthor%3A%20Muhammad%20Luthfi%20Shahab%20and%20Hadi%20Susanto%0AAbstract%3A%20%20%20This%20research%20introduces%20an%20extended%20application%20of%20neural%20networks%20for%0Asolving%20nonlinear%20partial%20differential%20equations%20%28PDEs%29.%20A%20neural%20network%2C%0Acombined%20with%20a%20pseudo-arclength%20continuation%2C%20is%20proposed%20to%20construct%0Abifurcation%20diagrams%20from%20parameterized%20nonlinear%20PDEs.%20Additionally%2C%20a%20neural%0Anetwork%20approach%20is%20also%20presented%20for%20solving%20eigenvalue%20problems%20to%20analyze%0Asolution%20linear%20stability%2C%20focusing%20on%20identifying%20the%20largest%20eigenvalue.%20The%0Aeffectiveness%20of%20the%20proposed%20neural%20network%20is%20examined%20through%20experiments%20on%0Athe%20Bratu%20equation%20and%20the%20Burgers%20equation.%20Results%20from%20a%20finite%20difference%0Amethod%20are%20also%20presented%20as%20comparison.%20Varying%20numbers%20of%20grid%20points%20are%0Aemployed%20in%20each%20case%20to%20assess%20the%20behavior%20and%20accuracy%20of%20both%20the%20neural%0Anetwork%20and%20the%20finite%20difference%20method.%20The%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20neural%20network%20produces%20better%20solutions%2C%20generates%20more%0Aaccurate%20bifurcation%20diagrams%2C%20has%20reasonable%20computational%20times%2C%20and%20proves%0Aeffective%20for%20linear%20stability%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520networks%2520for%2520bifurcation%2520and%2520linear%2520stability%2520analysis%2520of%2520steady%250A%2520%2520states%2520in%2520partial%2520differential%2520equations%26entry.906535625%3DMuhammad%2520Luthfi%2520Shahab%2520and%2520Hadi%2520Susanto%26entry.1292438233%3D%2520%2520This%2520research%2520introduces%2520an%2520extended%2520application%2520of%2520neural%2520networks%2520for%250Asolving%2520nonlinear%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520A%2520neural%2520network%252C%250Acombined%2520with%2520a%2520pseudo-arclength%2520continuation%252C%2520is%2520proposed%2520to%2520construct%250Abifurcation%2520diagrams%2520from%2520parameterized%2520nonlinear%2520PDEs.%2520Additionally%252C%2520a%2520neural%250Anetwork%2520approach%2520is%2520also%2520presented%2520for%2520solving%2520eigenvalue%2520problems%2520to%2520analyze%250Asolution%2520linear%2520stability%252C%2520focusing%2520on%2520identifying%2520the%2520largest%2520eigenvalue.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520neural%2520network%2520is%2520examined%2520through%2520experiments%2520on%250Athe%2520Bratu%2520equation%2520and%2520the%2520Burgers%2520equation.%2520Results%2520from%2520a%2520finite%2520difference%250Amethod%2520are%2520also%2520presented%2520as%2520comparison.%2520Varying%2520numbers%2520of%2520grid%2520points%2520are%250Aemployed%2520in%2520each%2520case%2520to%2520assess%2520the%2520behavior%2520and%2520accuracy%2520of%2520both%2520the%2520neural%250Anetwork%2520and%2520the%2520finite%2520difference%2520method.%2520The%2520experimental%2520results%2520demonstrate%250Athat%2520the%2520proposed%2520neural%2520network%2520produces%2520better%2520solutions%252C%2520generates%2520more%250Aaccurate%2520bifurcation%2520diagrams%252C%2520has%2520reasonable%2520computational%2520times%252C%2520and%2520proves%250Aeffective%2520for%2520linear%2520stability%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20networks%20for%20bifurcation%20and%20linear%20stability%20analysis%20of%20steady%0A%20%20states%20in%20partial%20differential%20equations&entry.906535625=Muhammad%20Luthfi%20Shahab%20and%20Hadi%20Susanto&entry.1292438233=%20%20This%20research%20introduces%20an%20extended%20application%20of%20neural%20networks%20for%0Asolving%20nonlinear%20partial%20differential%20equations%20%28PDEs%29.%20A%20neural%20network%2C%0Acombined%20with%20a%20pseudo-arclength%20continuation%2C%20is%20proposed%20to%20construct%0Abifurcation%20diagrams%20from%20parameterized%20nonlinear%20PDEs.%20Additionally%2C%20a%20neural%0Anetwork%20approach%20is%20also%20presented%20for%20solving%20eigenvalue%20problems%20to%20analyze%0Asolution%20linear%20stability%2C%20focusing%20on%20identifying%20the%20largest%20eigenvalue.%20The%0Aeffectiveness%20of%20the%20proposed%20neural%20network%20is%20examined%20through%20experiments%20on%0Athe%20Bratu%20equation%20and%20the%20Burgers%20equation.%20Results%20from%20a%20finite%20difference%0Amethod%20are%20also%20presented%20as%20comparison.%20Varying%20numbers%20of%20grid%20points%20are%0Aemployed%20in%20each%20case%20to%20assess%20the%20behavior%20and%20accuracy%20of%20both%20the%20neural%0Anetwork%20and%20the%20finite%20difference%20method.%20The%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20neural%20network%20produces%20better%20solutions%2C%20generates%20more%0Aaccurate%20bifurcation%20diagrams%2C%20has%20reasonable%20computational%20times%2C%20and%20proves%0Aeffective%20for%20linear%20stability%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19707v2&entry.124074799=Read"},
{"title": "DualTime: A Dual-Adapter Multimodal Language Model for Time Series\n  Representation", "author": "Weiqi Zhang and Jiexia Ye and Ziyue Li and Jia Li and Fugee Tsung", "abstract": "  The recent rapid development of language models (LMs) has attracted attention\nin the field of time series, including multimodal time series modeling.\nHowever, we note that current time series multimodal methods are biased, often\nassigning a primary role to one modality while the other assumes a secondary\nrole. They overlook the mutual benefits and complementary of different\nmodalities. For example, in seizure diagnosis, relying solely on textual\nclinical reports makes it difficult to pinpoint the area and type of the\ndisease, while electroencephalograms (EEGs) alone cannot provide an accurate\ndiagnosis without considering the symptoms. In this study, based on the\ncomplementary information mining of time series multimodal data, we propose\nDualTime, a Dual-adapter multimodal language model for Time series\nrepresentation implementing temporal-primary and textual-primary modeling\nsimultaneously. By injecting lightweight adaption tokens, the LM pipeline\nshared by dual adapters encourages embedding alignment and achieves efficient\nfine-tuning. Empirically, our method outperforms state-of-the-art models in\nboth supervised and unsupervised settings, highlighting the complementary\nbenefits of different modalities. In addition, we conduct few-shot label\ntransfer experiments, which further verifies the transferability and\nexpressiveness of our proposed DualTime.\n", "link": "http://arxiv.org/abs/2406.06620v2", "date": "2024-07-30", "relevancy": 1.7137, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.614}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5281}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualTime%3A%20A%20Dual-Adapter%20Multimodal%20Language%20Model%20for%20Time%20Series%0A%20%20Representation&body=Title%3A%20DualTime%3A%20A%20Dual-Adapter%20Multimodal%20Language%20Model%20for%20Time%20Series%0A%20%20Representation%0AAuthor%3A%20Weiqi%20Zhang%20and%20Jiexia%20Ye%20and%20Ziyue%20Li%20and%20Jia%20Li%20and%20Fugee%20Tsung%0AAbstract%3A%20%20%20The%20recent%20rapid%20development%20of%20language%20models%20%28LMs%29%20has%20attracted%20attention%0Ain%20the%20field%20of%20time%20series%2C%20including%20multimodal%20time%20series%20modeling.%0AHowever%2C%20we%20note%20that%20current%20time%20series%20multimodal%20methods%20are%20biased%2C%20often%0Aassigning%20a%20primary%20role%20to%20one%20modality%20while%20the%20other%20assumes%20a%20secondary%0Arole.%20They%20overlook%20the%20mutual%20benefits%20and%20complementary%20of%20different%0Amodalities.%20For%20example%2C%20in%20seizure%20diagnosis%2C%20relying%20solely%20on%20textual%0Aclinical%20reports%20makes%20it%20difficult%20to%20pinpoint%20the%20area%20and%20type%20of%20the%0Adisease%2C%20while%20electroencephalograms%20%28EEGs%29%20alone%20cannot%20provide%20an%20accurate%0Adiagnosis%20without%20considering%20the%20symptoms.%20In%20this%20study%2C%20based%20on%20the%0Acomplementary%20information%20mining%20of%20time%20series%20multimodal%20data%2C%20we%20propose%0ADualTime%2C%20a%20Dual-adapter%20multimodal%20language%20model%20for%20Time%20series%0Arepresentation%20implementing%20temporal-primary%20and%20textual-primary%20modeling%0Asimultaneously.%20By%20injecting%20lightweight%20adaption%20tokens%2C%20the%20LM%20pipeline%0Ashared%20by%20dual%20adapters%20encourages%20embedding%20alignment%20and%20achieves%20efficient%0Afine-tuning.%20Empirically%2C%20our%20method%20outperforms%20state-of-the-art%20models%20in%0Aboth%20supervised%20and%20unsupervised%20settings%2C%20highlighting%20the%20complementary%0Abenefits%20of%20different%20modalities.%20In%20addition%2C%20we%20conduct%20few-shot%20label%0Atransfer%20experiments%2C%20which%20further%20verifies%20the%20transferability%20and%0Aexpressiveness%20of%20our%20proposed%20DualTime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualTime%253A%2520A%2520Dual-Adapter%2520Multimodal%2520Language%2520Model%2520for%2520Time%2520Series%250A%2520%2520Representation%26entry.906535625%3DWeiqi%2520Zhang%2520and%2520Jiexia%2520Ye%2520and%2520Ziyue%2520Li%2520and%2520Jia%2520Li%2520and%2520Fugee%2520Tsung%26entry.1292438233%3D%2520%2520The%2520recent%2520rapid%2520development%2520of%2520language%2520models%2520%2528LMs%2529%2520has%2520attracted%2520attention%250Ain%2520the%2520field%2520of%2520time%2520series%252C%2520including%2520multimodal%2520time%2520series%2520modeling.%250AHowever%252C%2520we%2520note%2520that%2520current%2520time%2520series%2520multimodal%2520methods%2520are%2520biased%252C%2520often%250Aassigning%2520a%2520primary%2520role%2520to%2520one%2520modality%2520while%2520the%2520other%2520assumes%2520a%2520secondary%250Arole.%2520They%2520overlook%2520the%2520mutual%2520benefits%2520and%2520complementary%2520of%2520different%250Amodalities.%2520For%2520example%252C%2520in%2520seizure%2520diagnosis%252C%2520relying%2520solely%2520on%2520textual%250Aclinical%2520reports%2520makes%2520it%2520difficult%2520to%2520pinpoint%2520the%2520area%2520and%2520type%2520of%2520the%250Adisease%252C%2520while%2520electroencephalograms%2520%2528EEGs%2529%2520alone%2520cannot%2520provide%2520an%2520accurate%250Adiagnosis%2520without%2520considering%2520the%2520symptoms.%2520In%2520this%2520study%252C%2520based%2520on%2520the%250Acomplementary%2520information%2520mining%2520of%2520time%2520series%2520multimodal%2520data%252C%2520we%2520propose%250ADualTime%252C%2520a%2520Dual-adapter%2520multimodal%2520language%2520model%2520for%2520Time%2520series%250Arepresentation%2520implementing%2520temporal-primary%2520and%2520textual-primary%2520modeling%250Asimultaneously.%2520By%2520injecting%2520lightweight%2520adaption%2520tokens%252C%2520the%2520LM%2520pipeline%250Ashared%2520by%2520dual%2520adapters%2520encourages%2520embedding%2520alignment%2520and%2520achieves%2520efficient%250Afine-tuning.%2520Empirically%252C%2520our%2520method%2520outperforms%2520state-of-the-art%2520models%2520in%250Aboth%2520supervised%2520and%2520unsupervised%2520settings%252C%2520highlighting%2520the%2520complementary%250Abenefits%2520of%2520different%2520modalities.%2520In%2520addition%252C%2520we%2520conduct%2520few-shot%2520label%250Atransfer%2520experiments%252C%2520which%2520further%2520verifies%2520the%2520transferability%2520and%250Aexpressiveness%2520of%2520our%2520proposed%2520DualTime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualTime%3A%20A%20Dual-Adapter%20Multimodal%20Language%20Model%20for%20Time%20Series%0A%20%20Representation&entry.906535625=Weiqi%20Zhang%20and%20Jiexia%20Ye%20and%20Ziyue%20Li%20and%20Jia%20Li%20and%20Fugee%20Tsung&entry.1292438233=%20%20The%20recent%20rapid%20development%20of%20language%20models%20%28LMs%29%20has%20attracted%20attention%0Ain%20the%20field%20of%20time%20series%2C%20including%20multimodal%20time%20series%20modeling.%0AHowever%2C%20we%20note%20that%20current%20time%20series%20multimodal%20methods%20are%20biased%2C%20often%0Aassigning%20a%20primary%20role%20to%20one%20modality%20while%20the%20other%20assumes%20a%20secondary%0Arole.%20They%20overlook%20the%20mutual%20benefits%20and%20complementary%20of%20different%0Amodalities.%20For%20example%2C%20in%20seizure%20diagnosis%2C%20relying%20solely%20on%20textual%0Aclinical%20reports%20makes%20it%20difficult%20to%20pinpoint%20the%20area%20and%20type%20of%20the%0Adisease%2C%20while%20electroencephalograms%20%28EEGs%29%20alone%20cannot%20provide%20an%20accurate%0Adiagnosis%20without%20considering%20the%20symptoms.%20In%20this%20study%2C%20based%20on%20the%0Acomplementary%20information%20mining%20of%20time%20series%20multimodal%20data%2C%20we%20propose%0ADualTime%2C%20a%20Dual-adapter%20multimodal%20language%20model%20for%20Time%20series%0Arepresentation%20implementing%20temporal-primary%20and%20textual-primary%20modeling%0Asimultaneously.%20By%20injecting%20lightweight%20adaption%20tokens%2C%20the%20LM%20pipeline%0Ashared%20by%20dual%20adapters%20encourages%20embedding%20alignment%20and%20achieves%20efficient%0Afine-tuning.%20Empirically%2C%20our%20method%20outperforms%20state-of-the-art%20models%20in%0Aboth%20supervised%20and%20unsupervised%20settings%2C%20highlighting%20the%20complementary%0Abenefits%20of%20different%20modalities.%20In%20addition%2C%20we%20conduct%20few-shot%20label%0Atransfer%20experiments%2C%20which%20further%20verifies%20the%20transferability%20and%0Aexpressiveness%20of%20our%20proposed%20DualTime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06620v2&entry.124074799=Read"},
{"title": "AhmedML: High-Fidelity Computational Fluid Dynamics Dataset for\n  Incompressible, Low-Speed Bluff Body Aerodynamics", "author": "Neil Ashton and Danielle C. Maddix and Samuel Gundry and Parisa M. Shabestari", "abstract": "  The development of Machine Learning (ML) methods for Computational Fluid\nDynamics (CFD) is currently limited by the lack of openly available training\ndata. This paper presents a new open-source dataset comprising of high\nfidelity, scale-resolving CFD simulations of 500 geometric variations of the\nAhmed Car Body - a simplified car-like shape that exhibits many of the flow\ntopologies that are present on bluff bodies such as road vehicles. The dataset\ncontains simulation results that exhibit a broad set of fundamental flow\nphysics such as geometry and pressure-induced flow separation as well as 3D\nvortical structures. Each variation of the Ahmed car body were run using a\nhigh-fidelity, time-accurate, hybrid Reynolds-Averaged Navier-Stokes (RANS) -\nLarge-Eddy Simulation (LES) turbulence modelling approach using the open-source\nCFD code OpenFOAM. The dataset contains boundary, volume, geometry, and\ntime-averaged forces/moments in widely used open-source formats. In addition,\nthe OpenFOAM case setup is provided so that others can reproduce or extend the\ndataset. This represents to the authors knowledge, the first open-source\nlarge-scale dataset using high-fidelity CFD methods for the widely used Ahmed\ncar body that is available to freely download with a permissive license\n(CC-BY-SA).\n", "link": "http://arxiv.org/abs/2407.20801v1", "date": "2024-07-30", "relevancy": 1.6933, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4235}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4235}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AhmedML%3A%20High-Fidelity%20Computational%20Fluid%20Dynamics%20Dataset%20for%0A%20%20Incompressible%2C%20Low-Speed%20Bluff%20Body%20Aerodynamics&body=Title%3A%20AhmedML%3A%20High-Fidelity%20Computational%20Fluid%20Dynamics%20Dataset%20for%0A%20%20Incompressible%2C%20Low-Speed%20Bluff%20Body%20Aerodynamics%0AAuthor%3A%20Neil%20Ashton%20and%20Danielle%20C.%20Maddix%20and%20Samuel%20Gundry%20and%20Parisa%20M.%20Shabestari%0AAbstract%3A%20%20%20The%20development%20of%20Machine%20Learning%20%28ML%29%20methods%20for%20Computational%20Fluid%0ADynamics%20%28CFD%29%20is%20currently%20limited%20by%20the%20lack%20of%20openly%20available%20training%0Adata.%20This%20paper%20presents%20a%20new%20open-source%20dataset%20comprising%20of%20high%0Afidelity%2C%20scale-resolving%20CFD%20simulations%20of%20500%20geometric%20variations%20of%20the%0AAhmed%20Car%20Body%20-%20a%20simplified%20car-like%20shape%20that%20exhibits%20many%20of%20the%20flow%0Atopologies%20that%20are%20present%20on%20bluff%20bodies%20such%20as%20road%20vehicles.%20The%20dataset%0Acontains%20simulation%20results%20that%20exhibit%20a%20broad%20set%20of%20fundamental%20flow%0Aphysics%20such%20as%20geometry%20and%20pressure-induced%20flow%20separation%20as%20well%20as%203D%0Avortical%20structures.%20Each%20variation%20of%20the%20Ahmed%20car%20body%20were%20run%20using%20a%0Ahigh-fidelity%2C%20time-accurate%2C%20hybrid%20Reynolds-Averaged%20Navier-Stokes%20%28RANS%29%20-%0ALarge-Eddy%20Simulation%20%28LES%29%20turbulence%20modelling%20approach%20using%20the%20open-source%0ACFD%20code%20OpenFOAM.%20The%20dataset%20contains%20boundary%2C%20volume%2C%20geometry%2C%20and%0Atime-averaged%20forces/moments%20in%20widely%20used%20open-source%20formats.%20In%20addition%2C%0Athe%20OpenFOAM%20case%20setup%20is%20provided%20so%20that%20others%20can%20reproduce%20or%20extend%20the%0Adataset.%20This%20represents%20to%20the%20authors%20knowledge%2C%20the%20first%20open-source%0Alarge-scale%20dataset%20using%20high-fidelity%20CFD%20methods%20for%20the%20widely%20used%20Ahmed%0Acar%20body%20that%20is%20available%20to%20freely%20download%20with%20a%20permissive%20license%0A%28CC-BY-SA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAhmedML%253A%2520High-Fidelity%2520Computational%2520Fluid%2520Dynamics%2520Dataset%2520for%250A%2520%2520Incompressible%252C%2520Low-Speed%2520Bluff%2520Body%2520Aerodynamics%26entry.906535625%3DNeil%2520Ashton%2520and%2520Danielle%2520C.%2520Maddix%2520and%2520Samuel%2520Gundry%2520and%2520Parisa%2520M.%2520Shabestari%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520Machine%2520Learning%2520%2528ML%2529%2520methods%2520for%2520Computational%2520Fluid%250ADynamics%2520%2528CFD%2529%2520is%2520currently%2520limited%2520by%2520the%2520lack%2520of%2520openly%2520available%2520training%250Adata.%2520This%2520paper%2520presents%2520a%2520new%2520open-source%2520dataset%2520comprising%2520of%2520high%250Afidelity%252C%2520scale-resolving%2520CFD%2520simulations%2520of%2520500%2520geometric%2520variations%2520of%2520the%250AAhmed%2520Car%2520Body%2520-%2520a%2520simplified%2520car-like%2520shape%2520that%2520exhibits%2520many%2520of%2520the%2520flow%250Atopologies%2520that%2520are%2520present%2520on%2520bluff%2520bodies%2520such%2520as%2520road%2520vehicles.%2520The%2520dataset%250Acontains%2520simulation%2520results%2520that%2520exhibit%2520a%2520broad%2520set%2520of%2520fundamental%2520flow%250Aphysics%2520such%2520as%2520geometry%2520and%2520pressure-induced%2520flow%2520separation%2520as%2520well%2520as%25203D%250Avortical%2520structures.%2520Each%2520variation%2520of%2520the%2520Ahmed%2520car%2520body%2520were%2520run%2520using%2520a%250Ahigh-fidelity%252C%2520time-accurate%252C%2520hybrid%2520Reynolds-Averaged%2520Navier-Stokes%2520%2528RANS%2529%2520-%250ALarge-Eddy%2520Simulation%2520%2528LES%2529%2520turbulence%2520modelling%2520approach%2520using%2520the%2520open-source%250ACFD%2520code%2520OpenFOAM.%2520The%2520dataset%2520contains%2520boundary%252C%2520volume%252C%2520geometry%252C%2520and%250Atime-averaged%2520forces/moments%2520in%2520widely%2520used%2520open-source%2520formats.%2520In%2520addition%252C%250Athe%2520OpenFOAM%2520case%2520setup%2520is%2520provided%2520so%2520that%2520others%2520can%2520reproduce%2520or%2520extend%2520the%250Adataset.%2520This%2520represents%2520to%2520the%2520authors%2520knowledge%252C%2520the%2520first%2520open-source%250Alarge-scale%2520dataset%2520using%2520high-fidelity%2520CFD%2520methods%2520for%2520the%2520widely%2520used%2520Ahmed%250Acar%2520body%2520that%2520is%2520available%2520to%2520freely%2520download%2520with%2520a%2520permissive%2520license%250A%2528CC-BY-SA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AhmedML%3A%20High-Fidelity%20Computational%20Fluid%20Dynamics%20Dataset%20for%0A%20%20Incompressible%2C%20Low-Speed%20Bluff%20Body%20Aerodynamics&entry.906535625=Neil%20Ashton%20and%20Danielle%20C.%20Maddix%20and%20Samuel%20Gundry%20and%20Parisa%20M.%20Shabestari&entry.1292438233=%20%20The%20development%20of%20Machine%20Learning%20%28ML%29%20methods%20for%20Computational%20Fluid%0ADynamics%20%28CFD%29%20is%20currently%20limited%20by%20the%20lack%20of%20openly%20available%20training%0Adata.%20This%20paper%20presents%20a%20new%20open-source%20dataset%20comprising%20of%20high%0Afidelity%2C%20scale-resolving%20CFD%20simulations%20of%20500%20geometric%20variations%20of%20the%0AAhmed%20Car%20Body%20-%20a%20simplified%20car-like%20shape%20that%20exhibits%20many%20of%20the%20flow%0Atopologies%20that%20are%20present%20on%20bluff%20bodies%20such%20as%20road%20vehicles.%20The%20dataset%0Acontains%20simulation%20results%20that%20exhibit%20a%20broad%20set%20of%20fundamental%20flow%0Aphysics%20such%20as%20geometry%20and%20pressure-induced%20flow%20separation%20as%20well%20as%203D%0Avortical%20structures.%20Each%20variation%20of%20the%20Ahmed%20car%20body%20were%20run%20using%20a%0Ahigh-fidelity%2C%20time-accurate%2C%20hybrid%20Reynolds-Averaged%20Navier-Stokes%20%28RANS%29%20-%0ALarge-Eddy%20Simulation%20%28LES%29%20turbulence%20modelling%20approach%20using%20the%20open-source%0ACFD%20code%20OpenFOAM.%20The%20dataset%20contains%20boundary%2C%20volume%2C%20geometry%2C%20and%0Atime-averaged%20forces/moments%20in%20widely%20used%20open-source%20formats.%20In%20addition%2C%0Athe%20OpenFOAM%20case%20setup%20is%20provided%20so%20that%20others%20can%20reproduce%20or%20extend%20the%0Adataset.%20This%20represents%20to%20the%20authors%20knowledge%2C%20the%20first%20open-source%0Alarge-scale%20dataset%20using%20high-fidelity%20CFD%20methods%20for%20the%20widely%20used%20Ahmed%0Acar%20body%20that%20is%20available%20to%20freely%20download%20with%20a%20permissive%20license%0A%28CC-BY-SA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20801v1&entry.124074799=Read"},
{"title": "Industrial-Grade Smart Troubleshooting through Causal Technical Language\n  Processing: a Proof of Concept", "author": "Alexandre Trilla and Ossee Yiboe and Nenad Mijatovic and Jordi Vitri\u00e0", "abstract": "  This paper describes the development of a causal diagnosis approach for\ntroubleshooting an industrial environment on the basis of the technical\nlanguage expressed in Return on Experience records. The proposed method\nleverages the vectorized linguistic knowledge contained in the distributed\nrepresentation of a Large Language Model, and the causal associations entailed\nby the embedded failure modes and mechanisms of the industrial assets. The\npaper presents the elementary but essential concepts of the solution, which is\nconceived as a causality-aware retrieval augmented generation system, and\nillustrates them experimentally on a real-world Predictive Maintenance setting.\nFinally, it discusses avenues of improvement for the maturity of the utilized\ncausal technology to meet the robustness challenges of increasingly complex\nscenarios in the industry.\n", "link": "http://arxiv.org/abs/2407.20700v1", "date": "2024-07-30", "relevancy": 1.2621, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4515}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4383}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Industrial-Grade%20Smart%20Troubleshooting%20through%20Causal%20Technical%20Language%0A%20%20Processing%3A%20a%20Proof%20of%20Concept&body=Title%3A%20Industrial-Grade%20Smart%20Troubleshooting%20through%20Causal%20Technical%20Language%0A%20%20Processing%3A%20a%20Proof%20of%20Concept%0AAuthor%3A%20Alexandre%20Trilla%20and%20Ossee%20Yiboe%20and%20Nenad%20Mijatovic%20and%20Jordi%20Vitri%C3%A0%0AAbstract%3A%20%20%20This%20paper%20describes%20the%20development%20of%20a%20causal%20diagnosis%20approach%20for%0Atroubleshooting%20an%20industrial%20environment%20on%20the%20basis%20of%20the%20technical%0Alanguage%20expressed%20in%20Return%20on%20Experience%20records.%20The%20proposed%20method%0Aleverages%20the%20vectorized%20linguistic%20knowledge%20contained%20in%20the%20distributed%0Arepresentation%20of%20a%20Large%20Language%20Model%2C%20and%20the%20causal%20associations%20entailed%0Aby%20the%20embedded%20failure%20modes%20and%20mechanisms%20of%20the%20industrial%20assets.%20The%0Apaper%20presents%20the%20elementary%20but%20essential%20concepts%20of%20the%20solution%2C%20which%20is%0Aconceived%20as%20a%20causality-aware%20retrieval%20augmented%20generation%20system%2C%20and%0Aillustrates%20them%20experimentally%20on%20a%20real-world%20Predictive%20Maintenance%20setting.%0AFinally%2C%20it%20discusses%20avenues%20of%20improvement%20for%20the%20maturity%20of%20the%20utilized%0Acausal%20technology%20to%20meet%20the%20robustness%20challenges%20of%20increasingly%20complex%0Ascenarios%20in%20the%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndustrial-Grade%2520Smart%2520Troubleshooting%2520through%2520Causal%2520Technical%2520Language%250A%2520%2520Processing%253A%2520a%2520Proof%2520of%2520Concept%26entry.906535625%3DAlexandre%2520Trilla%2520and%2520Ossee%2520Yiboe%2520and%2520Nenad%2520Mijatovic%2520and%2520Jordi%2520Vitri%25C3%25A0%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520the%2520development%2520of%2520a%2520causal%2520diagnosis%2520approach%2520for%250Atroubleshooting%2520an%2520industrial%2520environment%2520on%2520the%2520basis%2520of%2520the%2520technical%250Alanguage%2520expressed%2520in%2520Return%2520on%2520Experience%2520records.%2520The%2520proposed%2520method%250Aleverages%2520the%2520vectorized%2520linguistic%2520knowledge%2520contained%2520in%2520the%2520distributed%250Arepresentation%2520of%2520a%2520Large%2520Language%2520Model%252C%2520and%2520the%2520causal%2520associations%2520entailed%250Aby%2520the%2520embedded%2520failure%2520modes%2520and%2520mechanisms%2520of%2520the%2520industrial%2520assets.%2520The%250Apaper%2520presents%2520the%2520elementary%2520but%2520essential%2520concepts%2520of%2520the%2520solution%252C%2520which%2520is%250Aconceived%2520as%2520a%2520causality-aware%2520retrieval%2520augmented%2520generation%2520system%252C%2520and%250Aillustrates%2520them%2520experimentally%2520on%2520a%2520real-world%2520Predictive%2520Maintenance%2520setting.%250AFinally%252C%2520it%2520discusses%2520avenues%2520of%2520improvement%2520for%2520the%2520maturity%2520of%2520the%2520utilized%250Acausal%2520technology%2520to%2520meet%2520the%2520robustness%2520challenges%2520of%2520increasingly%2520complex%250Ascenarios%2520in%2520the%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Industrial-Grade%20Smart%20Troubleshooting%20through%20Causal%20Technical%20Language%0A%20%20Processing%3A%20a%20Proof%20of%20Concept&entry.906535625=Alexandre%20Trilla%20and%20Ossee%20Yiboe%20and%20Nenad%20Mijatovic%20and%20Jordi%20Vitri%C3%A0&entry.1292438233=%20%20This%20paper%20describes%20the%20development%20of%20a%20causal%20diagnosis%20approach%20for%0Atroubleshooting%20an%20industrial%20environment%20on%20the%20basis%20of%20the%20technical%0Alanguage%20expressed%20in%20Return%20on%20Experience%20records.%20The%20proposed%20method%0Aleverages%20the%20vectorized%20linguistic%20knowledge%20contained%20in%20the%20distributed%0Arepresentation%20of%20a%20Large%20Language%20Model%2C%20and%20the%20causal%20associations%20entailed%0Aby%20the%20embedded%20failure%20modes%20and%20mechanisms%20of%20the%20industrial%20assets.%20The%0Apaper%20presents%20the%20elementary%20but%20essential%20concepts%20of%20the%20solution%2C%20which%20is%0Aconceived%20as%20a%20causality-aware%20retrieval%20augmented%20generation%20system%2C%20and%0Aillustrates%20them%20experimentally%20on%20a%20real-world%20Predictive%20Maintenance%20setting.%0AFinally%2C%20it%20discusses%20avenues%20of%20improvement%20for%20the%20maturity%20of%20the%20utilized%0Acausal%20technology%20to%20meet%20the%20robustness%20challenges%20of%20increasingly%20complex%0Ascenarios%20in%20the%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20700v1&entry.124074799=Read"},
{"title": "Be aware of overfitting by hyperparameter optimization!", "author": "Igor V. Tetko and Ruud van Deursen and Guillaume Godin", "abstract": "  Hyperparameter optimization is very frequently employed in machine learning.\nHowever, an optimization of a large space of parameters could result in\noverfitting of models. In recent studies on solubility prediction the authors\ncollected seven thermodynamic and kinetic solubility datasets from different\ndata sources. They used state-of-the-art graph-based methods and compared\nmodels developed for each dataset using different data cleaning protocols and\nhyperparameter optimization. In our study we showed that hyperparameter\noptimization did not always result in better models, possibly due to\noverfitting when using the same statistical measures. Similar results could be\ncalculated using pre-set hyperparameters, reducing the computational effort by\naround 10,000 times. We also extended the previous analysis by adding a\nrepresentation learning method based on Natural Language Processing of smiles\ncalled Transformer CNN. We show that across all analyzed sets using exactly the\nsame protocol, Transformer CNN provided better results than graph-based methods\nfor 26 out of 28 pairwise comparisons by using only a tiny fraction of time as\ncompared to other methods. Last but not least we stressed the importance of\ncomparing calculation results using exactly the same statistical measures.\n", "link": "http://arxiv.org/abs/2407.20786v1", "date": "2024-07-30", "relevancy": 1.3272, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4638}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4399}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Be%20aware%20of%20overfitting%20by%20hyperparameter%20optimization%21&body=Title%3A%20Be%20aware%20of%20overfitting%20by%20hyperparameter%20optimization%21%0AAuthor%3A%20Igor%20V.%20Tetko%20and%20Ruud%20van%20Deursen%20and%20Guillaume%20Godin%0AAbstract%3A%20%20%20Hyperparameter%20optimization%20is%20very%20frequently%20employed%20in%20machine%20learning.%0AHowever%2C%20an%20optimization%20of%20a%20large%20space%20of%20parameters%20could%20result%20in%0Aoverfitting%20of%20models.%20In%20recent%20studies%20on%20solubility%20prediction%20the%20authors%0Acollected%20seven%20thermodynamic%20and%20kinetic%20solubility%20datasets%20from%20different%0Adata%20sources.%20They%20used%20state-of-the-art%20graph-based%20methods%20and%20compared%0Amodels%20developed%20for%20each%20dataset%20using%20different%20data%20cleaning%20protocols%20and%0Ahyperparameter%20optimization.%20In%20our%20study%20we%20showed%20that%20hyperparameter%0Aoptimization%20did%20not%20always%20result%20in%20better%20models%2C%20possibly%20due%20to%0Aoverfitting%20when%20using%20the%20same%20statistical%20measures.%20Similar%20results%20could%20be%0Acalculated%20using%20pre-set%20hyperparameters%2C%20reducing%20the%20computational%20effort%20by%0Aaround%2010%2C000%20times.%20We%20also%20extended%20the%20previous%20analysis%20by%20adding%20a%0Arepresentation%20learning%20method%20based%20on%20Natural%20Language%20Processing%20of%20smiles%0Acalled%20Transformer%20CNN.%20We%20show%20that%20across%20all%20analyzed%20sets%20using%20exactly%20the%0Asame%20protocol%2C%20Transformer%20CNN%20provided%20better%20results%20than%20graph-based%20methods%0Afor%2026%20out%20of%2028%20pairwise%20comparisons%20by%20using%20only%20a%20tiny%20fraction%20of%20time%20as%0Acompared%20to%20other%20methods.%20Last%20but%20not%20least%20we%20stressed%20the%20importance%20of%0Acomparing%20calculation%20results%20using%20exactly%20the%20same%20statistical%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBe%2520aware%2520of%2520overfitting%2520by%2520hyperparameter%2520optimization%2521%26entry.906535625%3DIgor%2520V.%2520Tetko%2520and%2520Ruud%2520van%2520Deursen%2520and%2520Guillaume%2520Godin%26entry.1292438233%3D%2520%2520Hyperparameter%2520optimization%2520is%2520very%2520frequently%2520employed%2520in%2520machine%2520learning.%250AHowever%252C%2520an%2520optimization%2520of%2520a%2520large%2520space%2520of%2520parameters%2520could%2520result%2520in%250Aoverfitting%2520of%2520models.%2520In%2520recent%2520studies%2520on%2520solubility%2520prediction%2520the%2520authors%250Acollected%2520seven%2520thermodynamic%2520and%2520kinetic%2520solubility%2520datasets%2520from%2520different%250Adata%2520sources.%2520They%2520used%2520state-of-the-art%2520graph-based%2520methods%2520and%2520compared%250Amodels%2520developed%2520for%2520each%2520dataset%2520using%2520different%2520data%2520cleaning%2520protocols%2520and%250Ahyperparameter%2520optimization.%2520In%2520our%2520study%2520we%2520showed%2520that%2520hyperparameter%250Aoptimization%2520did%2520not%2520always%2520result%2520in%2520better%2520models%252C%2520possibly%2520due%2520to%250Aoverfitting%2520when%2520using%2520the%2520same%2520statistical%2520measures.%2520Similar%2520results%2520could%2520be%250Acalculated%2520using%2520pre-set%2520hyperparameters%252C%2520reducing%2520the%2520computational%2520effort%2520by%250Aaround%252010%252C000%2520times.%2520We%2520also%2520extended%2520the%2520previous%2520analysis%2520by%2520adding%2520a%250Arepresentation%2520learning%2520method%2520based%2520on%2520Natural%2520Language%2520Processing%2520of%2520smiles%250Acalled%2520Transformer%2520CNN.%2520We%2520show%2520that%2520across%2520all%2520analyzed%2520sets%2520using%2520exactly%2520the%250Asame%2520protocol%252C%2520Transformer%2520CNN%2520provided%2520better%2520results%2520than%2520graph-based%2520methods%250Afor%252026%2520out%2520of%252028%2520pairwise%2520comparisons%2520by%2520using%2520only%2520a%2520tiny%2520fraction%2520of%2520time%2520as%250Acompared%2520to%2520other%2520methods.%2520Last%2520but%2520not%2520least%2520we%2520stressed%2520the%2520importance%2520of%250Acomparing%2520calculation%2520results%2520using%2520exactly%2520the%2520same%2520statistical%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Be%20aware%20of%20overfitting%20by%20hyperparameter%20optimization%21&entry.906535625=Igor%20V.%20Tetko%20and%20Ruud%20van%20Deursen%20and%20Guillaume%20Godin&entry.1292438233=%20%20Hyperparameter%20optimization%20is%20very%20frequently%20employed%20in%20machine%20learning.%0AHowever%2C%20an%20optimization%20of%20a%20large%20space%20of%20parameters%20could%20result%20in%0Aoverfitting%20of%20models.%20In%20recent%20studies%20on%20solubility%20prediction%20the%20authors%0Acollected%20seven%20thermodynamic%20and%20kinetic%20solubility%20datasets%20from%20different%0Adata%20sources.%20They%20used%20state-of-the-art%20graph-based%20methods%20and%20compared%0Amodels%20developed%20for%20each%20dataset%20using%20different%20data%20cleaning%20protocols%20and%0Ahyperparameter%20optimization.%20In%20our%20study%20we%20showed%20that%20hyperparameter%0Aoptimization%20did%20not%20always%20result%20in%20better%20models%2C%20possibly%20due%20to%0Aoverfitting%20when%20using%20the%20same%20statistical%20measures.%20Similar%20results%20could%20be%0Acalculated%20using%20pre-set%20hyperparameters%2C%20reducing%20the%20computational%20effort%20by%0Aaround%2010%2C000%20times.%20We%20also%20extended%20the%20previous%20analysis%20by%20adding%20a%0Arepresentation%20learning%20method%20based%20on%20Natural%20Language%20Processing%20of%20smiles%0Acalled%20Transformer%20CNN.%20We%20show%20that%20across%20all%20analyzed%20sets%20using%20exactly%20the%0Asame%20protocol%2C%20Transformer%20CNN%20provided%20better%20results%20than%20graph-based%20methods%0Afor%2026%20out%20of%2028%20pairwise%20comparisons%20by%20using%20only%20a%20tiny%20fraction%20of%20time%20as%0Acompared%20to%20other%20methods.%20Last%20but%20not%20least%20we%20stressed%20the%20importance%20of%0Acomparing%20calculation%20results%20using%20exactly%20the%20same%20statistical%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20786v1&entry.124074799=Read"},
{"title": "Cocobo: Exploring Large Language Models as the Engine for End-User Robot\n  Programming", "author": "Yate Ge and Yi Dai and Run Shan and Kechun Li and Yuanda Hu and Xiaohua Sun", "abstract": "  End-user development allows everyday users to tailor service robots or\napplications to their needs. One user-friendly approach is natural language\nprogramming. However, it encounters challenges such as an expansive user\nexpression space and limited support for debugging and editing, which restrict\nits application in end-user programming. The emergence of large language models\n(LLMs) offers promising avenues for the translation and interpretation between\nhuman language instructions and the code executed by robots, but their\napplication in end-user programming systems requires further study. We\nintroduce Cocobo, a natural language programming system with interactive\ndiagrams powered by LLMs. Cocobo employs LLMs to understand users' authoring\nintentions, generate and explain robot programs, and facilitate the conversion\nbetween executable code and flowchart representations. Our user study shows\nthat Cocobo has a low learning curve, enabling even users with zero coding\nexperience to customize robot programs successfully.\n", "link": "http://arxiv.org/abs/2407.20712v1", "date": "2024-07-30", "relevancy": 1.6145, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6274}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5169}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cocobo%3A%20Exploring%20Large%20Language%20Models%20as%20the%20Engine%20for%20End-User%20Robot%0A%20%20Programming&body=Title%3A%20Cocobo%3A%20Exploring%20Large%20Language%20Models%20as%20the%20Engine%20for%20End-User%20Robot%0A%20%20Programming%0AAuthor%3A%20Yate%20Ge%20and%20Yi%20Dai%20and%20Run%20Shan%20and%20Kechun%20Li%20and%20Yuanda%20Hu%20and%20Xiaohua%20Sun%0AAbstract%3A%20%20%20End-user%20development%20allows%20everyday%20users%20to%20tailor%20service%20robots%20or%0Aapplications%20to%20their%20needs.%20One%20user-friendly%20approach%20is%20natural%20language%0Aprogramming.%20However%2C%20it%20encounters%20challenges%20such%20as%20an%20expansive%20user%0Aexpression%20space%20and%20limited%20support%20for%20debugging%20and%20editing%2C%20which%20restrict%0Aits%20application%20in%20end-user%20programming.%20The%20emergence%20of%20large%20language%20models%0A%28LLMs%29%20offers%20promising%20avenues%20for%20the%20translation%20and%20interpretation%20between%0Ahuman%20language%20instructions%20and%20the%20code%20executed%20by%20robots%2C%20but%20their%0Aapplication%20in%20end-user%20programming%20systems%20requires%20further%20study.%20We%0Aintroduce%20Cocobo%2C%20a%20natural%20language%20programming%20system%20with%20interactive%0Adiagrams%20powered%20by%20LLMs.%20Cocobo%20employs%20LLMs%20to%20understand%20users%27%20authoring%0Aintentions%2C%20generate%20and%20explain%20robot%20programs%2C%20and%20facilitate%20the%20conversion%0Abetween%20executable%20code%20and%20flowchart%20representations.%20Our%20user%20study%20shows%0Athat%20Cocobo%20has%20a%20low%20learning%20curve%2C%20enabling%20even%20users%20with%20zero%20coding%0Aexperience%20to%20customize%20robot%20programs%20successfully.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCocobo%253A%2520Exploring%2520Large%2520Language%2520Models%2520as%2520the%2520Engine%2520for%2520End-User%2520Robot%250A%2520%2520Programming%26entry.906535625%3DYate%2520Ge%2520and%2520Yi%2520Dai%2520and%2520Run%2520Shan%2520and%2520Kechun%2520Li%2520and%2520Yuanda%2520Hu%2520and%2520Xiaohua%2520Sun%26entry.1292438233%3D%2520%2520End-user%2520development%2520allows%2520everyday%2520users%2520to%2520tailor%2520service%2520robots%2520or%250Aapplications%2520to%2520their%2520needs.%2520One%2520user-friendly%2520approach%2520is%2520natural%2520language%250Aprogramming.%2520However%252C%2520it%2520encounters%2520challenges%2520such%2520as%2520an%2520expansive%2520user%250Aexpression%2520space%2520and%2520limited%2520support%2520for%2520debugging%2520and%2520editing%252C%2520which%2520restrict%250Aits%2520application%2520in%2520end-user%2520programming.%2520The%2520emergence%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520offers%2520promising%2520avenues%2520for%2520the%2520translation%2520and%2520interpretation%2520between%250Ahuman%2520language%2520instructions%2520and%2520the%2520code%2520executed%2520by%2520robots%252C%2520but%2520their%250Aapplication%2520in%2520end-user%2520programming%2520systems%2520requires%2520further%2520study.%2520We%250Aintroduce%2520Cocobo%252C%2520a%2520natural%2520language%2520programming%2520system%2520with%2520interactive%250Adiagrams%2520powered%2520by%2520LLMs.%2520Cocobo%2520employs%2520LLMs%2520to%2520understand%2520users%2527%2520authoring%250Aintentions%252C%2520generate%2520and%2520explain%2520robot%2520programs%252C%2520and%2520facilitate%2520the%2520conversion%250Abetween%2520executable%2520code%2520and%2520flowchart%2520representations.%2520Our%2520user%2520study%2520shows%250Athat%2520Cocobo%2520has%2520a%2520low%2520learning%2520curve%252C%2520enabling%2520even%2520users%2520with%2520zero%2520coding%250Aexperience%2520to%2520customize%2520robot%2520programs%2520successfully.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cocobo%3A%20Exploring%20Large%20Language%20Models%20as%20the%20Engine%20for%20End-User%20Robot%0A%20%20Programming&entry.906535625=Yate%20Ge%20and%20Yi%20Dai%20and%20Run%20Shan%20and%20Kechun%20Li%20and%20Yuanda%20Hu%20and%20Xiaohua%20Sun&entry.1292438233=%20%20End-user%20development%20allows%20everyday%20users%20to%20tailor%20service%20robots%20or%0Aapplications%20to%20their%20needs.%20One%20user-friendly%20approach%20is%20natural%20language%0Aprogramming.%20However%2C%20it%20encounters%20challenges%20such%20as%20an%20expansive%20user%0Aexpression%20space%20and%20limited%20support%20for%20debugging%20and%20editing%2C%20which%20restrict%0Aits%20application%20in%20end-user%20programming.%20The%20emergence%20of%20large%20language%20models%0A%28LLMs%29%20offers%20promising%20avenues%20for%20the%20translation%20and%20interpretation%20between%0Ahuman%20language%20instructions%20and%20the%20code%20executed%20by%20robots%2C%20but%20their%0Aapplication%20in%20end-user%20programming%20systems%20requires%20further%20study.%20We%0Aintroduce%20Cocobo%2C%20a%20natural%20language%20programming%20system%20with%20interactive%0Adiagrams%20powered%20by%20LLMs.%20Cocobo%20employs%20LLMs%20to%20understand%20users%27%20authoring%0Aintentions%2C%20generate%20and%20explain%20robot%20programs%2C%20and%20facilitate%20the%20conversion%0Abetween%20executable%20code%20and%20flowchart%20representations.%20Our%20user%20study%20shows%0Athat%20Cocobo%20has%20a%20low%20learning%20curve%2C%20enabling%20even%20users%20with%20zero%20coding%0Aexperience%20to%20customize%20robot%20programs%20successfully.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20712v1&entry.124074799=Read"},
{"title": "A Comparative Analysis of YOLOv5, YOLOv8, and YOLOv10 in Kitchen Safety", "author": "Athulya Sundaresan Geetha and Muhammad Hussain", "abstract": "  Knife safety in the kitchen is essential for preventing accidents or injuries\nwith an emphasis on proper handling, maintenance, and storage methods. This\nresearch presents a comparative analysis of three YOLO models, YOLOv5, YOLOv8,\nand YOLOv10, to detect the hazards involved in handling knife, concentrating\nmainly on ensuring fingers are curled while holding items to be cut and that\nhands should only be in contact with knife handle avoiding the blade.\nPrecision, recall, F-score, and normalized confusion matrix are used to\nevaluate the performance of the models. The results indicate that YOLOv5\nperformed better than the other two models in identifying the hazard of\nensuring hands only touch the blade, while YOLOv8 excelled in detecting the\nhazard of curled fingers while holding items. YOLOv5 and YOLOv8 performed\nalmost identically in recognizing classes such as hand, knife, and vegetable,\nwhereas YOLOv5, YOLOv8, and YOLOv10 accurately identified the cutting board.\nThis paper provides insights into the advantages and shortcomings of these\nmodels in real-world settings. Moreover, by detailing the optimization of YOLO\narchitectures for safe knife handling, this study promotes the development of\nincreased accuracy and efficiency in safety surveillance systems.\n", "link": "http://arxiv.org/abs/2407.20872v1", "date": "2024-07-30", "relevancy": 1.229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4182}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Analysis%20of%20YOLOv5%2C%20YOLOv8%2C%20and%20YOLOv10%20in%20Kitchen%20Safety&body=Title%3A%20A%20Comparative%20Analysis%20of%20YOLOv5%2C%20YOLOv8%2C%20and%20YOLOv10%20in%20Kitchen%20Safety%0AAuthor%3A%20Athulya%20Sundaresan%20Geetha%20and%20Muhammad%20Hussain%0AAbstract%3A%20%20%20Knife%20safety%20in%20the%20kitchen%20is%20essential%20for%20preventing%20accidents%20or%20injuries%0Awith%20an%20emphasis%20on%20proper%20handling%2C%20maintenance%2C%20and%20storage%20methods.%20This%0Aresearch%20presents%20a%20comparative%20analysis%20of%20three%20YOLO%20models%2C%20YOLOv5%2C%20YOLOv8%2C%0Aand%20YOLOv10%2C%20to%20detect%20the%20hazards%20involved%20in%20handling%20knife%2C%20concentrating%0Amainly%20on%20ensuring%20fingers%20are%20curled%20while%20holding%20items%20to%20be%20cut%20and%20that%0Ahands%20should%20only%20be%20in%20contact%20with%20knife%20handle%20avoiding%20the%20blade.%0APrecision%2C%20recall%2C%20F-score%2C%20and%20normalized%20confusion%20matrix%20are%20used%20to%0Aevaluate%20the%20performance%20of%20the%20models.%20The%20results%20indicate%20that%20YOLOv5%0Aperformed%20better%20than%20the%20other%20two%20models%20in%20identifying%20the%20hazard%20of%0Aensuring%20hands%20only%20touch%20the%20blade%2C%20while%20YOLOv8%20excelled%20in%20detecting%20the%0Ahazard%20of%20curled%20fingers%20while%20holding%20items.%20YOLOv5%20and%20YOLOv8%20performed%0Aalmost%20identically%20in%20recognizing%20classes%20such%20as%20hand%2C%20knife%2C%20and%20vegetable%2C%0Awhereas%20YOLOv5%2C%20YOLOv8%2C%20and%20YOLOv10%20accurately%20identified%20the%20cutting%20board.%0AThis%20paper%20provides%20insights%20into%20the%20advantages%20and%20shortcomings%20of%20these%0Amodels%20in%20real-world%20settings.%20Moreover%2C%20by%20detailing%20the%20optimization%20of%20YOLO%0Aarchitectures%20for%20safe%20knife%20handling%2C%20this%20study%20promotes%20the%20development%20of%0Aincreased%20accuracy%20and%20efficiency%20in%20safety%20surveillance%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Analysis%2520of%2520YOLOv5%252C%2520YOLOv8%252C%2520and%2520YOLOv10%2520in%2520Kitchen%2520Safety%26entry.906535625%3DAthulya%2520Sundaresan%2520Geetha%2520and%2520Muhammad%2520Hussain%26entry.1292438233%3D%2520%2520Knife%2520safety%2520in%2520the%2520kitchen%2520is%2520essential%2520for%2520preventing%2520accidents%2520or%2520injuries%250Awith%2520an%2520emphasis%2520on%2520proper%2520handling%252C%2520maintenance%252C%2520and%2520storage%2520methods.%2520This%250Aresearch%2520presents%2520a%2520comparative%2520analysis%2520of%2520three%2520YOLO%2520models%252C%2520YOLOv5%252C%2520YOLOv8%252C%250Aand%2520YOLOv10%252C%2520to%2520detect%2520the%2520hazards%2520involved%2520in%2520handling%2520knife%252C%2520concentrating%250Amainly%2520on%2520ensuring%2520fingers%2520are%2520curled%2520while%2520holding%2520items%2520to%2520be%2520cut%2520and%2520that%250Ahands%2520should%2520only%2520be%2520in%2520contact%2520with%2520knife%2520handle%2520avoiding%2520the%2520blade.%250APrecision%252C%2520recall%252C%2520F-score%252C%2520and%2520normalized%2520confusion%2520matrix%2520are%2520used%2520to%250Aevaluate%2520the%2520performance%2520of%2520the%2520models.%2520The%2520results%2520indicate%2520that%2520YOLOv5%250Aperformed%2520better%2520than%2520the%2520other%2520two%2520models%2520in%2520identifying%2520the%2520hazard%2520of%250Aensuring%2520hands%2520only%2520touch%2520the%2520blade%252C%2520while%2520YOLOv8%2520excelled%2520in%2520detecting%2520the%250Ahazard%2520of%2520curled%2520fingers%2520while%2520holding%2520items.%2520YOLOv5%2520and%2520YOLOv8%2520performed%250Aalmost%2520identically%2520in%2520recognizing%2520classes%2520such%2520as%2520hand%252C%2520knife%252C%2520and%2520vegetable%252C%250Awhereas%2520YOLOv5%252C%2520YOLOv8%252C%2520and%2520YOLOv10%2520accurately%2520identified%2520the%2520cutting%2520board.%250AThis%2520paper%2520provides%2520insights%2520into%2520the%2520advantages%2520and%2520shortcomings%2520of%2520these%250Amodels%2520in%2520real-world%2520settings.%2520Moreover%252C%2520by%2520detailing%2520the%2520optimization%2520of%2520YOLO%250Aarchitectures%2520for%2520safe%2520knife%2520handling%252C%2520this%2520study%2520promotes%2520the%2520development%2520of%250Aincreased%2520accuracy%2520and%2520efficiency%2520in%2520safety%2520surveillance%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Analysis%20of%20YOLOv5%2C%20YOLOv8%2C%20and%20YOLOv10%20in%20Kitchen%20Safety&entry.906535625=Athulya%20Sundaresan%20Geetha%20and%20Muhammad%20Hussain&entry.1292438233=%20%20Knife%20safety%20in%20the%20kitchen%20is%20essential%20for%20preventing%20accidents%20or%20injuries%0Awith%20an%20emphasis%20on%20proper%20handling%2C%20maintenance%2C%20and%20storage%20methods.%20This%0Aresearch%20presents%20a%20comparative%20analysis%20of%20three%20YOLO%20models%2C%20YOLOv5%2C%20YOLOv8%2C%0Aand%20YOLOv10%2C%20to%20detect%20the%20hazards%20involved%20in%20handling%20knife%2C%20concentrating%0Amainly%20on%20ensuring%20fingers%20are%20curled%20while%20holding%20items%20to%20be%20cut%20and%20that%0Ahands%20should%20only%20be%20in%20contact%20with%20knife%20handle%20avoiding%20the%20blade.%0APrecision%2C%20recall%2C%20F-score%2C%20and%20normalized%20confusion%20matrix%20are%20used%20to%0Aevaluate%20the%20performance%20of%20the%20models.%20The%20results%20indicate%20that%20YOLOv5%0Aperformed%20better%20than%20the%20other%20two%20models%20in%20identifying%20the%20hazard%20of%0Aensuring%20hands%20only%20touch%20the%20blade%2C%20while%20YOLOv8%20excelled%20in%20detecting%20the%0Ahazard%20of%20curled%20fingers%20while%20holding%20items.%20YOLOv5%20and%20YOLOv8%20performed%0Aalmost%20identically%20in%20recognizing%20classes%20such%20as%20hand%2C%20knife%2C%20and%20vegetable%2C%0Awhereas%20YOLOv5%2C%20YOLOv8%2C%20and%20YOLOv10%20accurately%20identified%20the%20cutting%20board.%0AThis%20paper%20provides%20insights%20into%20the%20advantages%20and%20shortcomings%20of%20these%0Amodels%20in%20real-world%20settings.%20Moreover%2C%20by%20detailing%20the%20optimization%20of%20YOLO%0Aarchitectures%20for%20safe%20knife%20handling%2C%20this%20study%20promotes%20the%20development%20of%0Aincreased%20accuracy%20and%20efficiency%20in%20safety%20surveillance%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20872v1&entry.124074799=Read"},
{"title": "Transfer learning for conflict and duplicate detection in software\n  requirement pairs", "author": "Garima Malik and Savas Yildirim and Mucahit Cevik and Ayse Bener and Devang Parikh", "abstract": "  Consistent and holistic expression of software requirements is important for\nthe success of software projects. In this study, we aim to enhance the\nefficiency of the software development processes by automatically identifying\nconflicting and duplicate software requirement specifications. We formulate the\nconflict and duplicate detection problem as a requirement pair classification\ntask. We design a novel transformers-based architecture, SR-BERT, which\nincorporates Sentence-BERT and Bi-encoders for the conflict and duplicate\nidentification task. Furthermore, we apply supervised multi-stage fine-tuning\nto the pre-trained transformer models. We test the performance of different\ntransfer models using four different datasets. We find that sequentially\ntrained and fine-tuned transformer models perform well across the datasets with\nSR-BERT achieving the best performance for larger datasets. We also explore the\ncross-domain performance of conflict detection models and adopt a rule-based\nfiltering approach to validate the model classifications. Our analysis\nindicates that the sentence pair classification approach and the proposed\ntransformer-based natural language processing strategies can contribute\nsignificantly to achieving automation in conflict and duplicate detection\n", "link": "http://arxiv.org/abs/2301.03709v2", "date": "2024-07-30", "relevancy": 1.449, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4901}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20learning%20for%20conflict%20and%20duplicate%20detection%20in%20software%0A%20%20requirement%20pairs&body=Title%3A%20Transfer%20learning%20for%20conflict%20and%20duplicate%20detection%20in%20software%0A%20%20requirement%20pairs%0AAuthor%3A%20Garima%20Malik%20and%20Savas%20Yildirim%20and%20Mucahit%20Cevik%20and%20Ayse%20Bener%20and%20Devang%20Parikh%0AAbstract%3A%20%20%20Consistent%20and%20holistic%20expression%20of%20software%20requirements%20is%20important%20for%0Athe%20success%20of%20software%20projects.%20In%20this%20study%2C%20we%20aim%20to%20enhance%20the%0Aefficiency%20of%20the%20software%20development%20processes%20by%20automatically%20identifying%0Aconflicting%20and%20duplicate%20software%20requirement%20specifications.%20We%20formulate%20the%0Aconflict%20and%20duplicate%20detection%20problem%20as%20a%20requirement%20pair%20classification%0Atask.%20We%20design%20a%20novel%20transformers-based%20architecture%2C%20SR-BERT%2C%20which%0Aincorporates%20Sentence-BERT%20and%20Bi-encoders%20for%20the%20conflict%20and%20duplicate%0Aidentification%20task.%20Furthermore%2C%20we%20apply%20supervised%20multi-stage%20fine-tuning%0Ato%20the%20pre-trained%20transformer%20models.%20We%20test%20the%20performance%20of%20different%0Atransfer%20models%20using%20four%20different%20datasets.%20We%20find%20that%20sequentially%0Atrained%20and%20fine-tuned%20transformer%20models%20perform%20well%20across%20the%20datasets%20with%0ASR-BERT%20achieving%20the%20best%20performance%20for%20larger%20datasets.%20We%20also%20explore%20the%0Across-domain%20performance%20of%20conflict%20detection%20models%20and%20adopt%20a%20rule-based%0Afiltering%20approach%20to%20validate%20the%20model%20classifications.%20Our%20analysis%0Aindicates%20that%20the%20sentence%20pair%20classification%20approach%20and%20the%20proposed%0Atransformer-based%20natural%20language%20processing%20strategies%20can%20contribute%0Asignificantly%20to%20achieving%20automation%20in%20conflict%20and%20duplicate%20detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.03709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520learning%2520for%2520conflict%2520and%2520duplicate%2520detection%2520in%2520software%250A%2520%2520requirement%2520pairs%26entry.906535625%3DGarima%2520Malik%2520and%2520Savas%2520Yildirim%2520and%2520Mucahit%2520Cevik%2520and%2520Ayse%2520Bener%2520and%2520Devang%2520Parikh%26entry.1292438233%3D%2520%2520Consistent%2520and%2520holistic%2520expression%2520of%2520software%2520requirements%2520is%2520important%2520for%250Athe%2520success%2520of%2520software%2520projects.%2520In%2520this%2520study%252C%2520we%2520aim%2520to%2520enhance%2520the%250Aefficiency%2520of%2520the%2520software%2520development%2520processes%2520by%2520automatically%2520identifying%250Aconflicting%2520and%2520duplicate%2520software%2520requirement%2520specifications.%2520We%2520formulate%2520the%250Aconflict%2520and%2520duplicate%2520detection%2520problem%2520as%2520a%2520requirement%2520pair%2520classification%250Atask.%2520We%2520design%2520a%2520novel%2520transformers-based%2520architecture%252C%2520SR-BERT%252C%2520which%250Aincorporates%2520Sentence-BERT%2520and%2520Bi-encoders%2520for%2520the%2520conflict%2520and%2520duplicate%250Aidentification%2520task.%2520Furthermore%252C%2520we%2520apply%2520supervised%2520multi-stage%2520fine-tuning%250Ato%2520the%2520pre-trained%2520transformer%2520models.%2520We%2520test%2520the%2520performance%2520of%2520different%250Atransfer%2520models%2520using%2520four%2520different%2520datasets.%2520We%2520find%2520that%2520sequentially%250Atrained%2520and%2520fine-tuned%2520transformer%2520models%2520perform%2520well%2520across%2520the%2520datasets%2520with%250ASR-BERT%2520achieving%2520the%2520best%2520performance%2520for%2520larger%2520datasets.%2520We%2520also%2520explore%2520the%250Across-domain%2520performance%2520of%2520conflict%2520detection%2520models%2520and%2520adopt%2520a%2520rule-based%250Afiltering%2520approach%2520to%2520validate%2520the%2520model%2520classifications.%2520Our%2520analysis%250Aindicates%2520that%2520the%2520sentence%2520pair%2520classification%2520approach%2520and%2520the%2520proposed%250Atransformer-based%2520natural%2520language%2520processing%2520strategies%2520can%2520contribute%250Asignificantly%2520to%2520achieving%2520automation%2520in%2520conflict%2520and%2520duplicate%2520detection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.03709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20learning%20for%20conflict%20and%20duplicate%20detection%20in%20software%0A%20%20requirement%20pairs&entry.906535625=Garima%20Malik%20and%20Savas%20Yildirim%20and%20Mucahit%20Cevik%20and%20Ayse%20Bener%20and%20Devang%20Parikh&entry.1292438233=%20%20Consistent%20and%20holistic%20expression%20of%20software%20requirements%20is%20important%20for%0Athe%20success%20of%20software%20projects.%20In%20this%20study%2C%20we%20aim%20to%20enhance%20the%0Aefficiency%20of%20the%20software%20development%20processes%20by%20automatically%20identifying%0Aconflicting%20and%20duplicate%20software%20requirement%20specifications.%20We%20formulate%20the%0Aconflict%20and%20duplicate%20detection%20problem%20as%20a%20requirement%20pair%20classification%0Atask.%20We%20design%20a%20novel%20transformers-based%20architecture%2C%20SR-BERT%2C%20which%0Aincorporates%20Sentence-BERT%20and%20Bi-encoders%20for%20the%20conflict%20and%20duplicate%0Aidentification%20task.%20Furthermore%2C%20we%20apply%20supervised%20multi-stage%20fine-tuning%0Ato%20the%20pre-trained%20transformer%20models.%20We%20test%20the%20performance%20of%20different%0Atransfer%20models%20using%20four%20different%20datasets.%20We%20find%20that%20sequentially%0Atrained%20and%20fine-tuned%20transformer%20models%20perform%20well%20across%20the%20datasets%20with%0ASR-BERT%20achieving%20the%20best%20performance%20for%20larger%20datasets.%20We%20also%20explore%20the%0Across-domain%20performance%20of%20conflict%20detection%20models%20and%20adopt%20a%20rule-based%0Afiltering%20approach%20to%20validate%20the%20model%20classifications.%20Our%20analysis%0Aindicates%20that%20the%20sentence%20pair%20classification%20approach%20and%20the%20proposed%0Atransformer-based%20natural%20language%20processing%20strategies%20can%20contribute%0Asignificantly%20to%20achieving%20automation%20in%20conflict%20and%20duplicate%20detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.03709v2&entry.124074799=Read"},
{"title": "A Role-specific Guided Large Language Model for Ophthalmic Consultation\n  Based on Stylistic Differentiation", "author": "Laiyi Fu and Binbin Fan and Hongkai Du and Yanxiang Feng and Chunhua Li and Huping Song", "abstract": "  Ophthalmology consultations are crucial for diagnosing, treating, and\npreventing eye diseases. However, the growing demand for consultations exceeds\nthe availability of ophthalmologists. By leveraging large pre-trained language\nmodels, we can design effective dialogues for specific scenarios, aiding in\nconsultations. Traditional fine-tuning strategies for question-answering tasks\nare impractical due to increasing model size and often ignoring patient-doctor\nrole function during consultations. In this paper, we propose EyeDoctor, an\nophthalmic medical questioning large language model that enhances accuracy\nthrough doctor-patient role perception guided and an augmented knowledge base\nwith external disease information. Experimental results show EyeDoctor achieves\nhigher question-answering precision in ophthalmology consultations. Notably,\nEyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%\nimprovement in F1 scores on multi-round datasets compared to second best model\nChatGPT, highlighting the importance of doctor-patient role differentiation and\ndynamic knowledge base expansion for intelligent medical consultations. EyeDoc\nalso serves as a free available web based service and souce code is available\nat https://github.com/sperfu/EyeDoc.\n", "link": "http://arxiv.org/abs/2407.18483v3", "date": "2024-07-30", "relevancy": 1.4831, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4949}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Role-specific%20Guided%20Large%20Language%20Model%20for%20Ophthalmic%20Consultation%0A%20%20Based%20on%20Stylistic%20Differentiation&body=Title%3A%20A%20Role-specific%20Guided%20Large%20Language%20Model%20for%20Ophthalmic%20Consultation%0A%20%20Based%20on%20Stylistic%20Differentiation%0AAuthor%3A%20Laiyi%20Fu%20and%20Binbin%20Fan%20and%20Hongkai%20Du%20and%20Yanxiang%20Feng%20and%20Chunhua%20Li%20and%20Huping%20Song%0AAbstract%3A%20%20%20Ophthalmology%20consultations%20are%20crucial%20for%20diagnosing%2C%20treating%2C%20and%0Apreventing%20eye%20diseases.%20However%2C%20the%20growing%20demand%20for%20consultations%20exceeds%0Athe%20availability%20of%20ophthalmologists.%20By%20leveraging%20large%20pre-trained%20language%0Amodels%2C%20we%20can%20design%20effective%20dialogues%20for%20specific%20scenarios%2C%20aiding%20in%0Aconsultations.%20Traditional%20fine-tuning%20strategies%20for%20question-answering%20tasks%0Aare%20impractical%20due%20to%20increasing%20model%20size%20and%20often%20ignoring%20patient-doctor%0Arole%20function%20during%20consultations.%20In%20this%20paper%2C%20we%20propose%20EyeDoctor%2C%20an%0Aophthalmic%20medical%20questioning%20large%20language%20model%20that%20enhances%20accuracy%0Athrough%20doctor-patient%20role%20perception%20guided%20and%20an%20augmented%20knowledge%20base%0Awith%20external%20disease%20information.%20Experimental%20results%20show%20EyeDoctor%20achieves%0Ahigher%20question-answering%20precision%20in%20ophthalmology%20consultations.%20Notably%2C%0AEyeDoctor%20demonstrated%20a%207.25%25%20improvement%20in%20Rouge-1%20scores%20and%20a%2010.16%25%0Aimprovement%20in%20F1%20scores%20on%20multi-round%20datasets%20compared%20to%20second%20best%20model%0AChatGPT%2C%20highlighting%20the%20importance%20of%20doctor-patient%20role%20differentiation%20and%0Adynamic%20knowledge%20base%20expansion%20for%20intelligent%20medical%20consultations.%20EyeDoc%0Aalso%20serves%20as%20a%20free%20available%20web%20based%20service%20and%20souce%20code%20is%20available%0Aat%20https%3A//github.com/sperfu/EyeDoc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18483v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Role-specific%2520Guided%2520Large%2520Language%2520Model%2520for%2520Ophthalmic%2520Consultation%250A%2520%2520Based%2520on%2520Stylistic%2520Differentiation%26entry.906535625%3DLaiyi%2520Fu%2520and%2520Binbin%2520Fan%2520and%2520Hongkai%2520Du%2520and%2520Yanxiang%2520Feng%2520and%2520Chunhua%2520Li%2520and%2520Huping%2520Song%26entry.1292438233%3D%2520%2520Ophthalmology%2520consultations%2520are%2520crucial%2520for%2520diagnosing%252C%2520treating%252C%2520and%250Apreventing%2520eye%2520diseases.%2520However%252C%2520the%2520growing%2520demand%2520for%2520consultations%2520exceeds%250Athe%2520availability%2520of%2520ophthalmologists.%2520By%2520leveraging%2520large%2520pre-trained%2520language%250Amodels%252C%2520we%2520can%2520design%2520effective%2520dialogues%2520for%2520specific%2520scenarios%252C%2520aiding%2520in%250Aconsultations.%2520Traditional%2520fine-tuning%2520strategies%2520for%2520question-answering%2520tasks%250Aare%2520impractical%2520due%2520to%2520increasing%2520model%2520size%2520and%2520often%2520ignoring%2520patient-doctor%250Arole%2520function%2520during%2520consultations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520EyeDoctor%252C%2520an%250Aophthalmic%2520medical%2520questioning%2520large%2520language%2520model%2520that%2520enhances%2520accuracy%250Athrough%2520doctor-patient%2520role%2520perception%2520guided%2520and%2520an%2520augmented%2520knowledge%2520base%250Awith%2520external%2520disease%2520information.%2520Experimental%2520results%2520show%2520EyeDoctor%2520achieves%250Ahigher%2520question-answering%2520precision%2520in%2520ophthalmology%2520consultations.%2520Notably%252C%250AEyeDoctor%2520demonstrated%2520a%25207.25%2525%2520improvement%2520in%2520Rouge-1%2520scores%2520and%2520a%252010.16%2525%250Aimprovement%2520in%2520F1%2520scores%2520on%2520multi-round%2520datasets%2520compared%2520to%2520second%2520best%2520model%250AChatGPT%252C%2520highlighting%2520the%2520importance%2520of%2520doctor-patient%2520role%2520differentiation%2520and%250Adynamic%2520knowledge%2520base%2520expansion%2520for%2520intelligent%2520medical%2520consultations.%2520EyeDoc%250Aalso%2520serves%2520as%2520a%2520free%2520available%2520web%2520based%2520service%2520and%2520souce%2520code%2520is%2520available%250Aat%2520https%253A//github.com/sperfu/EyeDoc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18483v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Role-specific%20Guided%20Large%20Language%20Model%20for%20Ophthalmic%20Consultation%0A%20%20Based%20on%20Stylistic%20Differentiation&entry.906535625=Laiyi%20Fu%20and%20Binbin%20Fan%20and%20Hongkai%20Du%20and%20Yanxiang%20Feng%20and%20Chunhua%20Li%20and%20Huping%20Song&entry.1292438233=%20%20Ophthalmology%20consultations%20are%20crucial%20for%20diagnosing%2C%20treating%2C%20and%0Apreventing%20eye%20diseases.%20However%2C%20the%20growing%20demand%20for%20consultations%20exceeds%0Athe%20availability%20of%20ophthalmologists.%20By%20leveraging%20large%20pre-trained%20language%0Amodels%2C%20we%20can%20design%20effective%20dialogues%20for%20specific%20scenarios%2C%20aiding%20in%0Aconsultations.%20Traditional%20fine-tuning%20strategies%20for%20question-answering%20tasks%0Aare%20impractical%20due%20to%20increasing%20model%20size%20and%20often%20ignoring%20patient-doctor%0Arole%20function%20during%20consultations.%20In%20this%20paper%2C%20we%20propose%20EyeDoctor%2C%20an%0Aophthalmic%20medical%20questioning%20large%20language%20model%20that%20enhances%20accuracy%0Athrough%20doctor-patient%20role%20perception%20guided%20and%20an%20augmented%20knowledge%20base%0Awith%20external%20disease%20information.%20Experimental%20results%20show%20EyeDoctor%20achieves%0Ahigher%20question-answering%20precision%20in%20ophthalmology%20consultations.%20Notably%2C%0AEyeDoctor%20demonstrated%20a%207.25%25%20improvement%20in%20Rouge-1%20scores%20and%20a%2010.16%25%0Aimprovement%20in%20F1%20scores%20on%20multi-round%20datasets%20compared%20to%20second%20best%20model%0AChatGPT%2C%20highlighting%20the%20importance%20of%20doctor-patient%20role%20differentiation%20and%0Adynamic%20knowledge%20base%20expansion%20for%20intelligent%20medical%20consultations.%20EyeDoc%0Aalso%20serves%20as%20a%20free%20available%20web%20based%20service%20and%20souce%20code%20is%20available%0Aat%20https%3A//github.com/sperfu/EyeDoc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18483v3&entry.124074799=Read"},
{"title": "Complete Approximations of Incomplete Queries", "author": "Julien Corman and Werner Nutt and Ognjen Savkovi\u0107", "abstract": "  This paper studies the completeness of conjunctive queries over a partially\ncomplete database and the approximation of incomplete queries. Given a query\nand a set of completeness rules (a special kind of tuple generating\ndependencies) that specify which parts of the database are complete, we\ninvestigate whether the query can be fully answered, as if all data were\navailable. If not, we explore reformulating the query into either Maximal\nComplete Specializations (MCSs) or the (unique up to equivalence) Minimal\nComplete Generalization (MCG) that can be fully answered, that is, the best\ncomplete approximations of the query from below or above in the sense of query\ncontainment. We show that the MSG can be characterized as the least fixed-point\nof a monotonic operator in a preorder. Then, we show that an MCS can be\ncomputed by recursive backward application of completeness rules. We study the\ncomplexity of both problems and discuss implementation techniques that rely on\nan ASP and Prolog engines, respectively.\n", "link": "http://arxiv.org/abs/2407.20932v1", "date": "2024-07-30", "relevancy": 1.0565, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3588}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complete%20Approximations%20of%20Incomplete%20Queries&body=Title%3A%20Complete%20Approximations%20of%20Incomplete%20Queries%0AAuthor%3A%20Julien%20Corman%20and%20Werner%20Nutt%20and%20Ognjen%20Savkovi%C4%87%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20completeness%20of%20conjunctive%20queries%20over%20a%20partially%0Acomplete%20database%20and%20the%20approximation%20of%20incomplete%20queries.%20Given%20a%20query%0Aand%20a%20set%20of%20completeness%20rules%20%28a%20special%20kind%20of%20tuple%20generating%0Adependencies%29%20that%20specify%20which%20parts%20of%20the%20database%20are%20complete%2C%20we%0Ainvestigate%20whether%20the%20query%20can%20be%20fully%20answered%2C%20as%20if%20all%20data%20were%0Aavailable.%20If%20not%2C%20we%20explore%20reformulating%20the%20query%20into%20either%20Maximal%0AComplete%20Specializations%20%28MCSs%29%20or%20the%20%28unique%20up%20to%20equivalence%29%20Minimal%0AComplete%20Generalization%20%28MCG%29%20that%20can%20be%20fully%20answered%2C%20that%20is%2C%20the%20best%0Acomplete%20approximations%20of%20the%20query%20from%20below%20or%20above%20in%20the%20sense%20of%20query%0Acontainment.%20We%20show%20that%20the%20MSG%20can%20be%20characterized%20as%20the%20least%20fixed-point%0Aof%20a%20monotonic%20operator%20in%20a%20preorder.%20Then%2C%20we%20show%20that%20an%20MCS%20can%20be%0Acomputed%20by%20recursive%20backward%20application%20of%20completeness%20rules.%20We%20study%20the%0Acomplexity%20of%20both%20problems%20and%20discuss%20implementation%20techniques%20that%20rely%20on%0Aan%20ASP%20and%20Prolog%20engines%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplete%2520Approximations%2520of%2520Incomplete%2520Queries%26entry.906535625%3DJulien%2520Corman%2520and%2520Werner%2520Nutt%2520and%2520Ognjen%2520Savkovi%25C4%2587%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520completeness%2520of%2520conjunctive%2520queries%2520over%2520a%2520partially%250Acomplete%2520database%2520and%2520the%2520approximation%2520of%2520incomplete%2520queries.%2520Given%2520a%2520query%250Aand%2520a%2520set%2520of%2520completeness%2520rules%2520%2528a%2520special%2520kind%2520of%2520tuple%2520generating%250Adependencies%2529%2520that%2520specify%2520which%2520parts%2520of%2520the%2520database%2520are%2520complete%252C%2520we%250Ainvestigate%2520whether%2520the%2520query%2520can%2520be%2520fully%2520answered%252C%2520as%2520if%2520all%2520data%2520were%250Aavailable.%2520If%2520not%252C%2520we%2520explore%2520reformulating%2520the%2520query%2520into%2520either%2520Maximal%250AComplete%2520Specializations%2520%2528MCSs%2529%2520or%2520the%2520%2528unique%2520up%2520to%2520equivalence%2529%2520Minimal%250AComplete%2520Generalization%2520%2528MCG%2529%2520that%2520can%2520be%2520fully%2520answered%252C%2520that%2520is%252C%2520the%2520best%250Acomplete%2520approximations%2520of%2520the%2520query%2520from%2520below%2520or%2520above%2520in%2520the%2520sense%2520of%2520query%250Acontainment.%2520We%2520show%2520that%2520the%2520MSG%2520can%2520be%2520characterized%2520as%2520the%2520least%2520fixed-point%250Aof%2520a%2520monotonic%2520operator%2520in%2520a%2520preorder.%2520Then%252C%2520we%2520show%2520that%2520an%2520MCS%2520can%2520be%250Acomputed%2520by%2520recursive%2520backward%2520application%2520of%2520completeness%2520rules.%2520We%2520study%2520the%250Acomplexity%2520of%2520both%2520problems%2520and%2520discuss%2520implementation%2520techniques%2520that%2520rely%2520on%250Aan%2520ASP%2520and%2520Prolog%2520engines%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complete%20Approximations%20of%20Incomplete%20Queries&entry.906535625=Julien%20Corman%20and%20Werner%20Nutt%20and%20Ognjen%20Savkovi%C4%87&entry.1292438233=%20%20This%20paper%20studies%20the%20completeness%20of%20conjunctive%20queries%20over%20a%20partially%0Acomplete%20database%20and%20the%20approximation%20of%20incomplete%20queries.%20Given%20a%20query%0Aand%20a%20set%20of%20completeness%20rules%20%28a%20special%20kind%20of%20tuple%20generating%0Adependencies%29%20that%20specify%20which%20parts%20of%20the%20database%20are%20complete%2C%20we%0Ainvestigate%20whether%20the%20query%20can%20be%20fully%20answered%2C%20as%20if%20all%20data%20were%0Aavailable.%20If%20not%2C%20we%20explore%20reformulating%20the%20query%20into%20either%20Maximal%0AComplete%20Specializations%20%28MCSs%29%20or%20the%20%28unique%20up%20to%20equivalence%29%20Minimal%0AComplete%20Generalization%20%28MCG%29%20that%20can%20be%20fully%20answered%2C%20that%20is%2C%20the%20best%0Acomplete%20approximations%20of%20the%20query%20from%20below%20or%20above%20in%20the%20sense%20of%20query%0Acontainment.%20We%20show%20that%20the%20MSG%20can%20be%20characterized%20as%20the%20least%20fixed-point%0Aof%20a%20monotonic%20operator%20in%20a%20preorder.%20Then%2C%20we%20show%20that%20an%20MCS%20can%20be%0Acomputed%20by%20recursive%20backward%20application%20of%20completeness%20rules.%20We%20study%20the%0Acomplexity%20of%20both%20problems%20and%20discuss%20implementation%20techniques%20that%20rely%20on%0Aan%20ASP%20and%20Prolog%20engines%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20932v1&entry.124074799=Read"},
{"title": "MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with\n  Electrocardiography Using Mamba Capsule Network", "author": "Yinlong Xu and Xiaoqiang Liu and Zitai Kong and Yixuan Wu and Yue Wang and Yingzhou Lu and Honghao Gao and Jian Wu and Hongxia Xu", "abstract": "  Cardiac arrhythmia, a condition characterized by irregular heartbeats, often\nserves as an early indication of various heart ailments. With the advent of\ndeep learning, numerous innovative models have been introduced for diagnosing\narrhythmias using Electrocardiogram (ECG) signals. However, recent studies\nsolely focus on the performance of models, neglecting the interpretation of\ntheir results. This leads to a considerable lack of transparency, posing a\nsignificant risk in the actual diagnostic process. To solve this problem, this\npaper introduces MambaCapsule, a deep neural networks for ECG arrhythmias\nclassification, which increases the explainability of the model while enhancing\nthe accuracy.Our model utilizes Mamba for feature extraction and Capsule\nnetworks for prediction, providing not only a confidence score but also signal\nfeatures. Akin to the processing mechanism of human brain, the model learns\nsignal features and their relationship between them by reconstructing ECG\nsignals in the predicted selection. The model evaluation was conducted on\nMIT-BIH and PTB dataset, following the AAMI standard. MambaCapsule has achieved\na total accuracy of 99.54% and 99.59% on the test sets respectively. These\nresults demonstrate the promising performance of under the standard test\nprotocol.\n", "link": "http://arxiv.org/abs/2407.20893v1", "date": "2024-07-30", "relevancy": 1.4069, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4737}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4649}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaCapsule%3A%20Towards%20Transparent%20Cardiac%20Disease%20Diagnosis%20with%0A%20%20Electrocardiography%20Using%20Mamba%20Capsule%20Network&body=Title%3A%20MambaCapsule%3A%20Towards%20Transparent%20Cardiac%20Disease%20Diagnosis%20with%0A%20%20Electrocardiography%20Using%20Mamba%20Capsule%20Network%0AAuthor%3A%20Yinlong%20Xu%20and%20Xiaoqiang%20Liu%20and%20Zitai%20Kong%20and%20Yixuan%20Wu%20and%20Yue%20Wang%20and%20Yingzhou%20Lu%20and%20Honghao%20Gao%20and%20Jian%20Wu%20and%20Hongxia%20Xu%0AAbstract%3A%20%20%20Cardiac%20arrhythmia%2C%20a%20condition%20characterized%20by%20irregular%20heartbeats%2C%20often%0Aserves%20as%20an%20early%20indication%20of%20various%20heart%20ailments.%20With%20the%20advent%20of%0Adeep%20learning%2C%20numerous%20innovative%20models%20have%20been%20introduced%20for%20diagnosing%0Aarrhythmias%20using%20Electrocardiogram%20%28ECG%29%20signals.%20However%2C%20recent%20studies%0Asolely%20focus%20on%20the%20performance%20of%20models%2C%20neglecting%20the%20interpretation%20of%0Atheir%20results.%20This%20leads%20to%20a%20considerable%20lack%20of%20transparency%2C%20posing%20a%0Asignificant%20risk%20in%20the%20actual%20diagnostic%20process.%20To%20solve%20this%20problem%2C%20this%0Apaper%20introduces%20MambaCapsule%2C%20a%20deep%20neural%20networks%20for%20ECG%20arrhythmias%0Aclassification%2C%20which%20increases%20the%20explainability%20of%20the%20model%20while%20enhancing%0Athe%20accuracy.Our%20model%20utilizes%20Mamba%20for%20feature%20extraction%20and%20Capsule%0Anetworks%20for%20prediction%2C%20providing%20not%20only%20a%20confidence%20score%20but%20also%20signal%0Afeatures.%20Akin%20to%20the%20processing%20mechanism%20of%20human%20brain%2C%20the%20model%20learns%0Asignal%20features%20and%20their%20relationship%20between%20them%20by%20reconstructing%20ECG%0Asignals%20in%20the%20predicted%20selection.%20The%20model%20evaluation%20was%20conducted%20on%0AMIT-BIH%20and%20PTB%20dataset%2C%20following%20the%20AAMI%20standard.%20MambaCapsule%20has%20achieved%0Aa%20total%20accuracy%20of%2099.54%25%20and%2099.59%25%20on%20the%20test%20sets%20respectively.%20These%0Aresults%20demonstrate%20the%20promising%20performance%20of%20under%20the%20standard%20test%0Aprotocol.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaCapsule%253A%2520Towards%2520Transparent%2520Cardiac%2520Disease%2520Diagnosis%2520with%250A%2520%2520Electrocardiography%2520Using%2520Mamba%2520Capsule%2520Network%26entry.906535625%3DYinlong%2520Xu%2520and%2520Xiaoqiang%2520Liu%2520and%2520Zitai%2520Kong%2520and%2520Yixuan%2520Wu%2520and%2520Yue%2520Wang%2520and%2520Yingzhou%2520Lu%2520and%2520Honghao%2520Gao%2520and%2520Jian%2520Wu%2520and%2520Hongxia%2520Xu%26entry.1292438233%3D%2520%2520Cardiac%2520arrhythmia%252C%2520a%2520condition%2520characterized%2520by%2520irregular%2520heartbeats%252C%2520often%250Aserves%2520as%2520an%2520early%2520indication%2520of%2520various%2520heart%2520ailments.%2520With%2520the%2520advent%2520of%250Adeep%2520learning%252C%2520numerous%2520innovative%2520models%2520have%2520been%2520introduced%2520for%2520diagnosing%250Aarrhythmias%2520using%2520Electrocardiogram%2520%2528ECG%2529%2520signals.%2520However%252C%2520recent%2520studies%250Asolely%2520focus%2520on%2520the%2520performance%2520of%2520models%252C%2520neglecting%2520the%2520interpretation%2520of%250Atheir%2520results.%2520This%2520leads%2520to%2520a%2520considerable%2520lack%2520of%2520transparency%252C%2520posing%2520a%250Asignificant%2520risk%2520in%2520the%2520actual%2520diagnostic%2520process.%2520To%2520solve%2520this%2520problem%252C%2520this%250Apaper%2520introduces%2520MambaCapsule%252C%2520a%2520deep%2520neural%2520networks%2520for%2520ECG%2520arrhythmias%250Aclassification%252C%2520which%2520increases%2520the%2520explainability%2520of%2520the%2520model%2520while%2520enhancing%250Athe%2520accuracy.Our%2520model%2520utilizes%2520Mamba%2520for%2520feature%2520extraction%2520and%2520Capsule%250Anetworks%2520for%2520prediction%252C%2520providing%2520not%2520only%2520a%2520confidence%2520score%2520but%2520also%2520signal%250Afeatures.%2520Akin%2520to%2520the%2520processing%2520mechanism%2520of%2520human%2520brain%252C%2520the%2520model%2520learns%250Asignal%2520features%2520and%2520their%2520relationship%2520between%2520them%2520by%2520reconstructing%2520ECG%250Asignals%2520in%2520the%2520predicted%2520selection.%2520The%2520model%2520evaluation%2520was%2520conducted%2520on%250AMIT-BIH%2520and%2520PTB%2520dataset%252C%2520following%2520the%2520AAMI%2520standard.%2520MambaCapsule%2520has%2520achieved%250Aa%2520total%2520accuracy%2520of%252099.54%2525%2520and%252099.59%2525%2520on%2520the%2520test%2520sets%2520respectively.%2520These%250Aresults%2520demonstrate%2520the%2520promising%2520performance%2520of%2520under%2520the%2520standard%2520test%250Aprotocol.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaCapsule%3A%20Towards%20Transparent%20Cardiac%20Disease%20Diagnosis%20with%0A%20%20Electrocardiography%20Using%20Mamba%20Capsule%20Network&entry.906535625=Yinlong%20Xu%20and%20Xiaoqiang%20Liu%20and%20Zitai%20Kong%20and%20Yixuan%20Wu%20and%20Yue%20Wang%20and%20Yingzhou%20Lu%20and%20Honghao%20Gao%20and%20Jian%20Wu%20and%20Hongxia%20Xu&entry.1292438233=%20%20Cardiac%20arrhythmia%2C%20a%20condition%20characterized%20by%20irregular%20heartbeats%2C%20often%0Aserves%20as%20an%20early%20indication%20of%20various%20heart%20ailments.%20With%20the%20advent%20of%0Adeep%20learning%2C%20numerous%20innovative%20models%20have%20been%20introduced%20for%20diagnosing%0Aarrhythmias%20using%20Electrocardiogram%20%28ECG%29%20signals.%20However%2C%20recent%20studies%0Asolely%20focus%20on%20the%20performance%20of%20models%2C%20neglecting%20the%20interpretation%20of%0Atheir%20results.%20This%20leads%20to%20a%20considerable%20lack%20of%20transparency%2C%20posing%20a%0Asignificant%20risk%20in%20the%20actual%20diagnostic%20process.%20To%20solve%20this%20problem%2C%20this%0Apaper%20introduces%20MambaCapsule%2C%20a%20deep%20neural%20networks%20for%20ECG%20arrhythmias%0Aclassification%2C%20which%20increases%20the%20explainability%20of%20the%20model%20while%20enhancing%0Athe%20accuracy.Our%20model%20utilizes%20Mamba%20for%20feature%20extraction%20and%20Capsule%0Anetworks%20for%20prediction%2C%20providing%20not%20only%20a%20confidence%20score%20but%20also%20signal%0Afeatures.%20Akin%20to%20the%20processing%20mechanism%20of%20human%20brain%2C%20the%20model%20learns%0Asignal%20features%20and%20their%20relationship%20between%20them%20by%20reconstructing%20ECG%0Asignals%20in%20the%20predicted%20selection.%20The%20model%20evaluation%20was%20conducted%20on%0AMIT-BIH%20and%20PTB%20dataset%2C%20following%20the%20AAMI%20standard.%20MambaCapsule%20has%20achieved%0Aa%20total%20accuracy%20of%2099.54%25%20and%2099.59%25%20on%20the%20test%20sets%20respectively.%20These%0Aresults%20demonstrate%20the%20promising%20performance%20of%20under%20the%20standard%20test%0Aprotocol.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20893v1&entry.124074799=Read"},
{"title": "Large Language Models Assume People are More Rational than We Really are", "author": "Ryan Liu and Jiayi Geng and Joshua C. Peterson and Ilia Sucholutsky and Thomas L. Griffiths", "abstract": "  In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.\n", "link": "http://arxiv.org/abs/2406.17055v3", "date": "2024-07-30", "relevancy": 1.4131, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4767}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4689}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Assume%20People%20are%20More%20Rational%20than%20We%20Really%20are&body=Title%3A%20Large%20Language%20Models%20Assume%20People%20are%20More%20Rational%20than%20We%20Really%20are%0AAuthor%3A%20Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Joshua%20C.%20Peterson%20and%20Ilia%20Sucholutsky%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20In%20order%20for%20AI%20systems%20to%20communicate%20effectively%20with%20people%2C%20they%20must%0Aunderstand%20how%20we%20make%20decisions.%20However%2C%20people%27s%20decisions%20are%20not%20always%0Arational%2C%20so%20the%20implicit%20internal%20models%20of%20human%20decision-making%20in%20Large%0ALanguage%20Models%20%28LLMs%29%20must%20account%20for%20this.%20Previous%20empirical%20evidence%20seems%0Ato%20suggest%20that%20these%20implicit%20models%20are%20accurate%20--%20LLMs%20offer%20believable%0Aproxies%20of%20human%20behavior%2C%20acting%20how%20we%20expect%20humans%20would%20in%20everyday%0Ainteractions.%20However%2C%20by%20comparing%20LLM%20behavior%20and%20predictions%20to%20a%20large%0Adataset%20of%20human%20decisions%2C%20we%20find%20that%20this%20is%20actually%20not%20the%20case%3A%20when%0Aboth%20simulating%20and%20predicting%20people%27s%20choices%2C%20a%20suite%20of%20cutting-edge%20LLMs%0A%28GPT-4o%20%26%204-Turbo%2C%20Llama-3-8B%20%26%2070B%2C%20Claude%203%20Opus%29%20assume%20that%20people%20are%20more%0Arational%20than%20we%20really%20are.%20Specifically%2C%20these%20models%20deviate%20from%20human%0Abehavior%20and%20align%20more%20closely%20with%20a%20classic%20model%20of%20rational%20choice%20--%0Aexpected%20value%20theory.%20Interestingly%2C%20people%20also%20tend%20to%20assume%20that%20other%0Apeople%20are%20rational%20when%20interpreting%20their%20behavior.%20As%20a%20consequence%2C%20when%20we%0Acompare%20the%20inferences%20that%20LLMs%20and%20people%20draw%20from%20the%20decisions%20of%20others%0Ausing%20another%20psychological%20dataset%2C%20we%20find%20that%20these%20inferences%20are%20highly%0Acorrelated.%20Thus%2C%20the%20implicit%20decision-making%20models%20of%20LLMs%20appear%20to%20be%0Aaligned%20with%20the%20human%20expectation%20that%20other%20people%20will%20act%20rationally%2C%0Arather%20than%20with%20how%20people%20actually%20act.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17055v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Assume%2520People%2520are%2520More%2520Rational%2520than%2520We%2520Really%2520are%26entry.906535625%3DRyan%2520Liu%2520and%2520Jiayi%2520Geng%2520and%2520Joshua%2520C.%2520Peterson%2520and%2520Ilia%2520Sucholutsky%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520In%2520order%2520for%2520AI%2520systems%2520to%2520communicate%2520effectively%2520with%2520people%252C%2520they%2520must%250Aunderstand%2520how%2520we%2520make%2520decisions.%2520However%252C%2520people%2527s%2520decisions%2520are%2520not%2520always%250Arational%252C%2520so%2520the%2520implicit%2520internal%2520models%2520of%2520human%2520decision-making%2520in%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520must%2520account%2520for%2520this.%2520Previous%2520empirical%2520evidence%2520seems%250Ato%2520suggest%2520that%2520these%2520implicit%2520models%2520are%2520accurate%2520--%2520LLMs%2520offer%2520believable%250Aproxies%2520of%2520human%2520behavior%252C%2520acting%2520how%2520we%2520expect%2520humans%2520would%2520in%2520everyday%250Ainteractions.%2520However%252C%2520by%2520comparing%2520LLM%2520behavior%2520and%2520predictions%2520to%2520a%2520large%250Adataset%2520of%2520human%2520decisions%252C%2520we%2520find%2520that%2520this%2520is%2520actually%2520not%2520the%2520case%253A%2520when%250Aboth%2520simulating%2520and%2520predicting%2520people%2527s%2520choices%252C%2520a%2520suite%2520of%2520cutting-edge%2520LLMs%250A%2528GPT-4o%2520%2526%25204-Turbo%252C%2520Llama-3-8B%2520%2526%252070B%252C%2520Claude%25203%2520Opus%2529%2520assume%2520that%2520people%2520are%2520more%250Arational%2520than%2520we%2520really%2520are.%2520Specifically%252C%2520these%2520models%2520deviate%2520from%2520human%250Abehavior%2520and%2520align%2520more%2520closely%2520with%2520a%2520classic%2520model%2520of%2520rational%2520choice%2520--%250Aexpected%2520value%2520theory.%2520Interestingly%252C%2520people%2520also%2520tend%2520to%2520assume%2520that%2520other%250Apeople%2520are%2520rational%2520when%2520interpreting%2520their%2520behavior.%2520As%2520a%2520consequence%252C%2520when%2520we%250Acompare%2520the%2520inferences%2520that%2520LLMs%2520and%2520people%2520draw%2520from%2520the%2520decisions%2520of%2520others%250Ausing%2520another%2520psychological%2520dataset%252C%2520we%2520find%2520that%2520these%2520inferences%2520are%2520highly%250Acorrelated.%2520Thus%252C%2520the%2520implicit%2520decision-making%2520models%2520of%2520LLMs%2520appear%2520to%2520be%250Aaligned%2520with%2520the%2520human%2520expectation%2520that%2520other%2520people%2520will%2520act%2520rationally%252C%250Arather%2520than%2520with%2520how%2520people%2520actually%2520act.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17055v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Assume%20People%20are%20More%20Rational%20than%20We%20Really%20are&entry.906535625=Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Joshua%20C.%20Peterson%20and%20Ilia%20Sucholutsky%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20In%20order%20for%20AI%20systems%20to%20communicate%20effectively%20with%20people%2C%20they%20must%0Aunderstand%20how%20we%20make%20decisions.%20However%2C%20people%27s%20decisions%20are%20not%20always%0Arational%2C%20so%20the%20implicit%20internal%20models%20of%20human%20decision-making%20in%20Large%0ALanguage%20Models%20%28LLMs%29%20must%20account%20for%20this.%20Previous%20empirical%20evidence%20seems%0Ato%20suggest%20that%20these%20implicit%20models%20are%20accurate%20--%20LLMs%20offer%20believable%0Aproxies%20of%20human%20behavior%2C%20acting%20how%20we%20expect%20humans%20would%20in%20everyday%0Ainteractions.%20However%2C%20by%20comparing%20LLM%20behavior%20and%20predictions%20to%20a%20large%0Adataset%20of%20human%20decisions%2C%20we%20find%20that%20this%20is%20actually%20not%20the%20case%3A%20when%0Aboth%20simulating%20and%20predicting%20people%27s%20choices%2C%20a%20suite%20of%20cutting-edge%20LLMs%0A%28GPT-4o%20%26%204-Turbo%2C%20Llama-3-8B%20%26%2070B%2C%20Claude%203%20Opus%29%20assume%20that%20people%20are%20more%0Arational%20than%20we%20really%20are.%20Specifically%2C%20these%20models%20deviate%20from%20human%0Abehavior%20and%20align%20more%20closely%20with%20a%20classic%20model%20of%20rational%20choice%20--%0Aexpected%20value%20theory.%20Interestingly%2C%20people%20also%20tend%20to%20assume%20that%20other%0Apeople%20are%20rational%20when%20interpreting%20their%20behavior.%20As%20a%20consequence%2C%20when%20we%0Acompare%20the%20inferences%20that%20LLMs%20and%20people%20draw%20from%20the%20decisions%20of%20others%0Ausing%20another%20psychological%20dataset%2C%20we%20find%20that%20these%20inferences%20are%20highly%0Acorrelated.%20Thus%2C%20the%20implicit%20decision-making%20models%20of%20LLMs%20appear%20to%20be%0Aaligned%20with%20the%20human%20expectation%20that%20other%20people%20will%20act%20rationally%2C%0Arather%20than%20with%20how%20people%20actually%20act.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17055v3&entry.124074799=Read"},
{"title": "PV-OSIMr: A Lowest Order Complexity Algorithm for Computing the Delassus\n  Matrix", "author": "Ajay Suresha Sathya and Wilm Decre and Jan Swevers", "abstract": "  We present PV-OSIMr, an efficient algorithm for computing the Delassus matrix\n(also known as the inverse operational space inertia matrix) for a kinematic\ntree, with the lowest order computational complexity known in literature.\nPV-OSIMr is derived by optimizing the Popov-Vereshchagin (PV) solver\ncomputations using the compositionality of the force and motion propagators. It\nhas a computational complexity of O(n + m^2 ) compared to O(n + m^2d) of the\noriginal PV-OSIM algorithm and O(n+md+m^2 ) of the extended force propagator\nalgorithm (EFPA), where n is the number of joints, m is the number of\nconstraints and d is the depth of the kinematic tree. Since Delassus matrix\ncomputation requires constructing an m x m sized matrix and must consider all\nthe n joints at least once, the asymptotic computational complexity of PV-OSIMr\nis optimal. We further benchmark our algorithm and find it to be often more\nefficient than the PV-OSIM and EFPA in practice.\n", "link": "http://arxiv.org/abs/2310.03676v2", "date": "2024-07-30", "relevancy": 1.2694, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4379}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4064}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PV-OSIMr%3A%20A%20Lowest%20Order%20Complexity%20Algorithm%20for%20Computing%20the%20Delassus%0A%20%20Matrix&body=Title%3A%20PV-OSIMr%3A%20A%20Lowest%20Order%20Complexity%20Algorithm%20for%20Computing%20the%20Delassus%0A%20%20Matrix%0AAuthor%3A%20Ajay%20Suresha%20Sathya%20and%20Wilm%20Decre%20and%20Jan%20Swevers%0AAbstract%3A%20%20%20We%20present%20PV-OSIMr%2C%20an%20efficient%20algorithm%20for%20computing%20the%20Delassus%20matrix%0A%28also%20known%20as%20the%20inverse%20operational%20space%20inertia%20matrix%29%20for%20a%20kinematic%0Atree%2C%20with%20the%20lowest%20order%20computational%20complexity%20known%20in%20literature.%0APV-OSIMr%20is%20derived%20by%20optimizing%20the%20Popov-Vereshchagin%20%28PV%29%20solver%0Acomputations%20using%20the%20compositionality%20of%20the%20force%20and%20motion%20propagators.%20It%0Ahas%20a%20computational%20complexity%20of%20O%28n%20%2B%20m%5E2%20%29%20compared%20to%20O%28n%20%2B%20m%5E2d%29%20of%20the%0Aoriginal%20PV-OSIM%20algorithm%20and%20O%28n%2Bmd%2Bm%5E2%20%29%20of%20the%20extended%20force%20propagator%0Aalgorithm%20%28EFPA%29%2C%20where%20n%20is%20the%20number%20of%20joints%2C%20m%20is%20the%20number%20of%0Aconstraints%20and%20d%20is%20the%20depth%20of%20the%20kinematic%20tree.%20Since%20Delassus%20matrix%0Acomputation%20requires%20constructing%20an%20m%20x%20m%20sized%20matrix%20and%20must%20consider%20all%0Athe%20n%20joints%20at%20least%20once%2C%20the%20asymptotic%20computational%20complexity%20of%20PV-OSIMr%0Ais%20optimal.%20We%20further%20benchmark%20our%20algorithm%20and%20find%20it%20to%20be%20often%20more%0Aefficient%20than%20the%20PV-OSIM%20and%20EFPA%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03676v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPV-OSIMr%253A%2520A%2520Lowest%2520Order%2520Complexity%2520Algorithm%2520for%2520Computing%2520the%2520Delassus%250A%2520%2520Matrix%26entry.906535625%3DAjay%2520Suresha%2520Sathya%2520and%2520Wilm%2520Decre%2520and%2520Jan%2520Swevers%26entry.1292438233%3D%2520%2520We%2520present%2520PV-OSIMr%252C%2520an%2520efficient%2520algorithm%2520for%2520computing%2520the%2520Delassus%2520matrix%250A%2528also%2520known%2520as%2520the%2520inverse%2520operational%2520space%2520inertia%2520matrix%2529%2520for%2520a%2520kinematic%250Atree%252C%2520with%2520the%2520lowest%2520order%2520computational%2520complexity%2520known%2520in%2520literature.%250APV-OSIMr%2520is%2520derived%2520by%2520optimizing%2520the%2520Popov-Vereshchagin%2520%2528PV%2529%2520solver%250Acomputations%2520using%2520the%2520compositionality%2520of%2520the%2520force%2520and%2520motion%2520propagators.%2520It%250Ahas%2520a%2520computational%2520complexity%2520of%2520O%2528n%2520%252B%2520m%255E2%2520%2529%2520compared%2520to%2520O%2528n%2520%252B%2520m%255E2d%2529%2520of%2520the%250Aoriginal%2520PV-OSIM%2520algorithm%2520and%2520O%2528n%252Bmd%252Bm%255E2%2520%2529%2520of%2520the%2520extended%2520force%2520propagator%250Aalgorithm%2520%2528EFPA%2529%252C%2520where%2520n%2520is%2520the%2520number%2520of%2520joints%252C%2520m%2520is%2520the%2520number%2520of%250Aconstraints%2520and%2520d%2520is%2520the%2520depth%2520of%2520the%2520kinematic%2520tree.%2520Since%2520Delassus%2520matrix%250Acomputation%2520requires%2520constructing%2520an%2520m%2520x%2520m%2520sized%2520matrix%2520and%2520must%2520consider%2520all%250Athe%2520n%2520joints%2520at%2520least%2520once%252C%2520the%2520asymptotic%2520computational%2520complexity%2520of%2520PV-OSIMr%250Ais%2520optimal.%2520We%2520further%2520benchmark%2520our%2520algorithm%2520and%2520find%2520it%2520to%2520be%2520often%2520more%250Aefficient%2520than%2520the%2520PV-OSIM%2520and%2520EFPA%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03676v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PV-OSIMr%3A%20A%20Lowest%20Order%20Complexity%20Algorithm%20for%20Computing%20the%20Delassus%0A%20%20Matrix&entry.906535625=Ajay%20Suresha%20Sathya%20and%20Wilm%20Decre%20and%20Jan%20Swevers&entry.1292438233=%20%20We%20present%20PV-OSIMr%2C%20an%20efficient%20algorithm%20for%20computing%20the%20Delassus%20matrix%0A%28also%20known%20as%20the%20inverse%20operational%20space%20inertia%20matrix%29%20for%20a%20kinematic%0Atree%2C%20with%20the%20lowest%20order%20computational%20complexity%20known%20in%20literature.%0APV-OSIMr%20is%20derived%20by%20optimizing%20the%20Popov-Vereshchagin%20%28PV%29%20solver%0Acomputations%20using%20the%20compositionality%20of%20the%20force%20and%20motion%20propagators.%20It%0Ahas%20a%20computational%20complexity%20of%20O%28n%20%2B%20m%5E2%20%29%20compared%20to%20O%28n%20%2B%20m%5E2d%29%20of%20the%0Aoriginal%20PV-OSIM%20algorithm%20and%20O%28n%2Bmd%2Bm%5E2%20%29%20of%20the%20extended%20force%20propagator%0Aalgorithm%20%28EFPA%29%2C%20where%20n%20is%20the%20number%20of%20joints%2C%20m%20is%20the%20number%20of%0Aconstraints%20and%20d%20is%20the%20depth%20of%20the%20kinematic%20tree.%20Since%20Delassus%20matrix%0Acomputation%20requires%20constructing%20an%20m%20x%20m%20sized%20matrix%20and%20must%20consider%20all%0Athe%20n%20joints%20at%20least%20once%2C%20the%20asymptotic%20computational%20complexity%20of%20PV-OSIMr%0Ais%20optimal.%20We%20further%20benchmark%20our%20algorithm%20and%20find%20it%20to%20be%20often%20more%0Aefficient%20than%20the%20PV-OSIM%20and%20EFPA%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03676v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


