<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251221.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "G3Splat: Geometrically Consistent Generalizable Gaussian Splatting", "author": "Mehdi Hosseinzadeh and Shin-Fang Chng and Yi Xu and Simon Lucey and Ian Reid and Ravi Garg", "abstract": "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).", "link": "http://arxiv.org/abs/2512.17547v1", "date": "2025-12-19", "relevancy": 3.5936, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.749}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7407}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G3Splat%3A%20Geometrically%20Consistent%20Generalizable%20Gaussian%20Splatting&body=Title%3A%20G3Splat%3A%20Geometrically%20Consistent%20Generalizable%20Gaussian%20Splatting%0AAuthor%3A%20Mehdi%20Hosseinzadeh%20and%20Shin-Fang%20Chng%20and%20Yi%20Xu%20and%20Simon%20Lucey%20and%20Ian%20Reid%20and%20Ravi%20Garg%0AAbstract%3A%203D%20Gaussians%20have%20recently%20emerged%20as%20an%20effective%20scene%20representation%20for%20real-time%20splatting%20and%20accurate%20novel-view%20synthesis%2C%20motivating%20several%20works%20to%20adapt%20multi-view%20structure%20prediction%20networks%20to%20regress%20per-pixel%203D%20Gaussians%20from%20images.%20However%2C%20most%20prior%20work%20extends%20these%20networks%20to%20predict%20additional%20Gaussian%20parameters%20--%20orientation%2C%20scale%2C%20opacity%2C%20and%20appearance%20--%20while%20relying%20almost%20exclusively%20on%20view-synthesis%20supervision.%20We%20show%20that%20a%20view-synthesis%20loss%20alone%20is%20insufficient%20to%20recover%20geometrically%20meaningful%20splats%20in%20this%20setting.%20We%20analyze%20and%20address%20the%20ambiguities%20of%20learning%203D%20Gaussian%20splats%20under%20self-supervision%20for%20pose-free%20generalizable%20splatting%2C%20and%20introduce%20G3Splat%2C%20which%20enforces%20geometric%20priors%20to%20obtain%20geometrically%20consistent%203D%20scene%20representations.%20Trained%20on%20RE10K%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20in%20%28i%29%20geometrically%20consistent%20reconstruction%2C%20%28ii%29%20relative%20pose%20estimation%2C%20and%20%28iii%29%20novel-view%20synthesis.%20We%20further%20demonstrate%20strong%20zero-shot%20generalization%20on%20ScanNet%2C%20substantially%20outperforming%20prior%20work%20in%20both%20geometry%20recovery%20and%20relative%20pose%20estimation.%20Code%20and%20pretrained%20models%20are%20released%20on%20our%20project%20page%20%28https%3A//m80hz.github.io/g3splat/%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG3Splat%253A%2520Geometrically%2520Consistent%2520Generalizable%2520Gaussian%2520Splatting%26entry.906535625%3DMehdi%2520Hosseinzadeh%2520and%2520Shin-Fang%2520Chng%2520and%2520Yi%2520Xu%2520and%2520Simon%2520Lucey%2520and%2520Ian%2520Reid%2520and%2520Ravi%2520Garg%26entry.1292438233%3D3D%2520Gaussians%2520have%2520recently%2520emerged%2520as%2520an%2520effective%2520scene%2520representation%2520for%2520real-time%2520splatting%2520and%2520accurate%2520novel-view%2520synthesis%252C%2520motivating%2520several%2520works%2520to%2520adapt%2520multi-view%2520structure%2520prediction%2520networks%2520to%2520regress%2520per-pixel%25203D%2520Gaussians%2520from%2520images.%2520However%252C%2520most%2520prior%2520work%2520extends%2520these%2520networks%2520to%2520predict%2520additional%2520Gaussian%2520parameters%2520--%2520orientation%252C%2520scale%252C%2520opacity%252C%2520and%2520appearance%2520--%2520while%2520relying%2520almost%2520exclusively%2520on%2520view-synthesis%2520supervision.%2520We%2520show%2520that%2520a%2520view-synthesis%2520loss%2520alone%2520is%2520insufficient%2520to%2520recover%2520geometrically%2520meaningful%2520splats%2520in%2520this%2520setting.%2520We%2520analyze%2520and%2520address%2520the%2520ambiguities%2520of%2520learning%25203D%2520Gaussian%2520splats%2520under%2520self-supervision%2520for%2520pose-free%2520generalizable%2520splatting%252C%2520and%2520introduce%2520G3Splat%252C%2520which%2520enforces%2520geometric%2520priors%2520to%2520obtain%2520geometrically%2520consistent%25203D%2520scene%2520representations.%2520Trained%2520on%2520RE10K%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520in%2520%2528i%2529%2520geometrically%2520consistent%2520reconstruction%252C%2520%2528ii%2529%2520relative%2520pose%2520estimation%252C%2520and%2520%2528iii%2529%2520novel-view%2520synthesis.%2520We%2520further%2520demonstrate%2520strong%2520zero-shot%2520generalization%2520on%2520ScanNet%252C%2520substantially%2520outperforming%2520prior%2520work%2520in%2520both%2520geometry%2520recovery%2520and%2520relative%2520pose%2520estimation.%2520Code%2520and%2520pretrained%2520models%2520are%2520released%2520on%2520our%2520project%2520page%2520%2528https%253A//m80hz.github.io/g3splat/%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G3Splat%3A%20Geometrically%20Consistent%20Generalizable%20Gaussian%20Splatting&entry.906535625=Mehdi%20Hosseinzadeh%20and%20Shin-Fang%20Chng%20and%20Yi%20Xu%20and%20Simon%20Lucey%20and%20Ian%20Reid%20and%20Ravi%20Garg&entry.1292438233=3D%20Gaussians%20have%20recently%20emerged%20as%20an%20effective%20scene%20representation%20for%20real-time%20splatting%20and%20accurate%20novel-view%20synthesis%2C%20motivating%20several%20works%20to%20adapt%20multi-view%20structure%20prediction%20networks%20to%20regress%20per-pixel%203D%20Gaussians%20from%20images.%20However%2C%20most%20prior%20work%20extends%20these%20networks%20to%20predict%20additional%20Gaussian%20parameters%20--%20orientation%2C%20scale%2C%20opacity%2C%20and%20appearance%20--%20while%20relying%20almost%20exclusively%20on%20view-synthesis%20supervision.%20We%20show%20that%20a%20view-synthesis%20loss%20alone%20is%20insufficient%20to%20recover%20geometrically%20meaningful%20splats%20in%20this%20setting.%20We%20analyze%20and%20address%20the%20ambiguities%20of%20learning%203D%20Gaussian%20splats%20under%20self-supervision%20for%20pose-free%20generalizable%20splatting%2C%20and%20introduce%20G3Splat%2C%20which%20enforces%20geometric%20priors%20to%20obtain%20geometrically%20consistent%203D%20scene%20representations.%20Trained%20on%20RE10K%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20in%20%28i%29%20geometrically%20consistent%20reconstruction%2C%20%28ii%29%20relative%20pose%20estimation%2C%20and%20%28iii%29%20novel-view%20synthesis.%20We%20further%20demonstrate%20strong%20zero-shot%20generalization%20on%20ScanNet%2C%20substantially%20outperforming%20prior%20work%20in%20both%20geometry%20recovery%20and%20relative%20pose%20estimation.%20Code%20and%20pretrained%20models%20are%20released%20on%20our%20project%20page%20%28https%3A//m80hz.github.io/g3splat/%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.17547v1&entry.124074799=Read"},
{"title": "FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation", "author": "Cheng Peng and Zhuo Su and Liao Wang and Chen Guo and Zhaohu Li and Chengjiang Long and Zheng Lv and Jingxiang Sun and Chenyangguang Zhang and Yebin Liu", "abstract": "We present FlexAvatar, a flexible large reconstruction model for high-fidelity 3D head avatars with detailed dynamic deformation from single or sparse images, without requiring camera poses or expression labels. It leverages a transformer-based reconstruction model with structured head query tokens as canonical anchor to aggregate flexible input-number-agnostic, camera-pose-free and expression-free inputs into a robust canonical 3D representation. For detailed dynamic deformation, we introduce a lightweight UNet decoder conditioned on UV-space position maps, which can produce detailed expression-dependent deformations in real time. To better capture rare but critical expressions like wrinkles and bared teeth, we also adopt a data distribution adjustment strategy during training to balance the distribution of these expressions in the training set. Moreover, a lightweight 10-second refinement can further enhances identity-specific details in extreme identities without affecting deformation quality. Extensive experiments demonstrate that our FlexAvatar achieves superior 3D consistency, detailed dynamic realism compared with previous methods, providing a practical solution for animatable 3D avatar creation.", "link": "http://arxiv.org/abs/2512.17717v1", "date": "2025-12-19", "relevancy": 3.4313, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7271}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7271}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexAvatar%3A%20Flexible%20Large%20Reconstruction%20Model%20for%20Animatable%20Gaussian%20Head%20Avatars%20with%20Detailed%20Deformation&body=Title%3A%20FlexAvatar%3A%20Flexible%20Large%20Reconstruction%20Model%20for%20Animatable%20Gaussian%20Head%20Avatars%20with%20Detailed%20Deformation%0AAuthor%3A%20Cheng%20Peng%20and%20Zhuo%20Su%20and%20Liao%20Wang%20and%20Chen%20Guo%20and%20Zhaohu%20Li%20and%20Chengjiang%20Long%20and%20Zheng%20Lv%20and%20Jingxiang%20Sun%20and%20Chenyangguang%20Zhang%20and%20Yebin%20Liu%0AAbstract%3A%20We%20present%20FlexAvatar%2C%20a%20flexible%20large%20reconstruction%20model%20for%20high-fidelity%203D%20head%20avatars%20with%20detailed%20dynamic%20deformation%20from%20single%20or%20sparse%20images%2C%20without%20requiring%20camera%20poses%20or%20expression%20labels.%20It%20leverages%20a%20transformer-based%20reconstruction%20model%20with%20structured%20head%20query%20tokens%20as%20canonical%20anchor%20to%20aggregate%20flexible%20input-number-agnostic%2C%20camera-pose-free%20and%20expression-free%20inputs%20into%20a%20robust%20canonical%203D%20representation.%20For%20detailed%20dynamic%20deformation%2C%20we%20introduce%20a%20lightweight%20UNet%20decoder%20conditioned%20on%20UV-space%20position%20maps%2C%20which%20can%20produce%20detailed%20expression-dependent%20deformations%20in%20real%20time.%20To%20better%20capture%20rare%20but%20critical%20expressions%20like%20wrinkles%20and%20bared%20teeth%2C%20we%20also%20adopt%20a%20data%20distribution%20adjustment%20strategy%20during%20training%20to%20balance%20the%20distribution%20of%20these%20expressions%20in%20the%20training%20set.%20Moreover%2C%20a%20lightweight%2010-second%20refinement%20can%20further%20enhances%20identity-specific%20details%20in%20extreme%20identities%20without%20affecting%20deformation%20quality.%20Extensive%20experiments%20demonstrate%20that%20our%20FlexAvatar%20achieves%20superior%203D%20consistency%2C%20detailed%20dynamic%20realism%20compared%20with%20previous%20methods%2C%20providing%20a%20practical%20solution%20for%20animatable%203D%20avatar%20creation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexAvatar%253A%2520Flexible%2520Large%2520Reconstruction%2520Model%2520for%2520Animatable%2520Gaussian%2520Head%2520Avatars%2520with%2520Detailed%2520Deformation%26entry.906535625%3DCheng%2520Peng%2520and%2520Zhuo%2520Su%2520and%2520Liao%2520Wang%2520and%2520Chen%2520Guo%2520and%2520Zhaohu%2520Li%2520and%2520Chengjiang%2520Long%2520and%2520Zheng%2520Lv%2520and%2520Jingxiang%2520Sun%2520and%2520Chenyangguang%2520Zhang%2520and%2520Yebin%2520Liu%26entry.1292438233%3DWe%2520present%2520FlexAvatar%252C%2520a%2520flexible%2520large%2520reconstruction%2520model%2520for%2520high-fidelity%25203D%2520head%2520avatars%2520with%2520detailed%2520dynamic%2520deformation%2520from%2520single%2520or%2520sparse%2520images%252C%2520without%2520requiring%2520camera%2520poses%2520or%2520expression%2520labels.%2520It%2520leverages%2520a%2520transformer-based%2520reconstruction%2520model%2520with%2520structured%2520head%2520query%2520tokens%2520as%2520canonical%2520anchor%2520to%2520aggregate%2520flexible%2520input-number-agnostic%252C%2520camera-pose-free%2520and%2520expression-free%2520inputs%2520into%2520a%2520robust%2520canonical%25203D%2520representation.%2520For%2520detailed%2520dynamic%2520deformation%252C%2520we%2520introduce%2520a%2520lightweight%2520UNet%2520decoder%2520conditioned%2520on%2520UV-space%2520position%2520maps%252C%2520which%2520can%2520produce%2520detailed%2520expression-dependent%2520deformations%2520in%2520real%2520time.%2520To%2520better%2520capture%2520rare%2520but%2520critical%2520expressions%2520like%2520wrinkles%2520and%2520bared%2520teeth%252C%2520we%2520also%2520adopt%2520a%2520data%2520distribution%2520adjustment%2520strategy%2520during%2520training%2520to%2520balance%2520the%2520distribution%2520of%2520these%2520expressions%2520in%2520the%2520training%2520set.%2520Moreover%252C%2520a%2520lightweight%252010-second%2520refinement%2520can%2520further%2520enhances%2520identity-specific%2520details%2520in%2520extreme%2520identities%2520without%2520affecting%2520deformation%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520FlexAvatar%2520achieves%2520superior%25203D%2520consistency%252C%2520detailed%2520dynamic%2520realism%2520compared%2520with%2520previous%2520methods%252C%2520providing%2520a%2520practical%2520solution%2520for%2520animatable%25203D%2520avatar%2520creation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexAvatar%3A%20Flexible%20Large%20Reconstruction%20Model%20for%20Animatable%20Gaussian%20Head%20Avatars%20with%20Detailed%20Deformation&entry.906535625=Cheng%20Peng%20and%20Zhuo%20Su%20and%20Liao%20Wang%20and%20Chen%20Guo%20and%20Zhaohu%20Li%20and%20Chengjiang%20Long%20and%20Zheng%20Lv%20and%20Jingxiang%20Sun%20and%20Chenyangguang%20Zhang%20and%20Yebin%20Liu&entry.1292438233=We%20present%20FlexAvatar%2C%20a%20flexible%20large%20reconstruction%20model%20for%20high-fidelity%203D%20head%20avatars%20with%20detailed%20dynamic%20deformation%20from%20single%20or%20sparse%20images%2C%20without%20requiring%20camera%20poses%20or%20expression%20labels.%20It%20leverages%20a%20transformer-based%20reconstruction%20model%20with%20structured%20head%20query%20tokens%20as%20canonical%20anchor%20to%20aggregate%20flexible%20input-number-agnostic%2C%20camera-pose-free%20and%20expression-free%20inputs%20into%20a%20robust%20canonical%203D%20representation.%20For%20detailed%20dynamic%20deformation%2C%20we%20introduce%20a%20lightweight%20UNet%20decoder%20conditioned%20on%20UV-space%20position%20maps%2C%20which%20can%20produce%20detailed%20expression-dependent%20deformations%20in%20real%20time.%20To%20better%20capture%20rare%20but%20critical%20expressions%20like%20wrinkles%20and%20bared%20teeth%2C%20we%20also%20adopt%20a%20data%20distribution%20adjustment%20strategy%20during%20training%20to%20balance%20the%20distribution%20of%20these%20expressions%20in%20the%20training%20set.%20Moreover%2C%20a%20lightweight%2010-second%20refinement%20can%20further%20enhances%20identity-specific%20details%20in%20extreme%20identities%20without%20affecting%20deformation%20quality.%20Extensive%20experiments%20demonstrate%20that%20our%20FlexAvatar%20achieves%20superior%203D%20consistency%2C%20detailed%20dynamic%20realism%20compared%20with%20previous%20methods%2C%20providing%20a%20practical%20solution%20for%20animatable%203D%20avatar%20creation.&entry.1838667208=http%3A//arxiv.org/abs/2512.17717v1&entry.124074799=Read"},
{"title": "FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views", "author": "Qijian Tian and Xin Tan and Jiayu Ying and Xuhong Wang and Yuan Xie and Lizhuang Ma", "abstract": "We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.", "link": "http://arxiv.org/abs/2512.17541v1", "date": "2025-12-19", "relevancy": 3.115, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6375}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6194}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLEG%3A%20Feed-Forward%20Language%20Embedded%20Gaussian%20Splatting%20from%20Any%20Views&body=Title%3A%20FLEG%3A%20Feed-Forward%20Language%20Embedded%20Gaussian%20Splatting%20from%20Any%20Views%0AAuthor%3A%20Qijian%20Tian%20and%20Xin%20Tan%20and%20Jiayu%20Ying%20and%20Xuhong%20Wang%20and%20Yuan%20Xie%20and%20Lizhuang%20Ma%0AAbstract%3A%20We%20present%20FLEG%2C%20a%20feed-forward%20network%20that%20reconstructs%20language-embedded%203D%20Gaussians%20from%20any%20views.%20Previous%20straightforward%20solutions%20combine%20feed-forward%20reconstruction%20with%20Gaussian%20heads%20but%20suffer%20from%20fixed%20input%20views%20and%20insufficient%203D%20training%20data.%20In%20contrast%2C%20we%20propose%20a%203D-annotation-free%20training%20framework%20for%202D-to-3D%20lifting%20from%20arbitrary%20uncalibrated%20and%20unposed%20multi-view%20images.%20Since%20the%20framework%20does%20not%20require%203D%20annotations%2C%20we%20can%20leverage%20large-scale%20video%20data%20with%20easily%20obtained%202D%20instance%20information%20to%20enrich%20semantic%20embedding.%20We%20also%20propose%20an%20instance-guided%20contrastive%20learning%20to%20align%202D%20semantics%20with%20the%203D%20representations.%20In%20addition%2C%20to%20mitigate%20the%20high%20memory%20and%20computational%20cost%20of%20dense%20views%2C%20we%20further%20propose%20a%20geometry-semantic%20hierarchical%20sparsification%20strategy.%20Our%20FLEG%20efficiently%20reconstructs%20language-embedded%203D%20Gaussian%20representation%20in%20a%20feed-forward%20manner%20from%20arbitrary%20sparse%20or%20dense%20views%2C%20jointly%20producing%20accurate%20geometry%2C%20high-fidelity%20appearance%2C%20and%20language-aligned%20semantics.%20Extensive%20experiments%20show%20that%20it%20outperforms%20existing%20methods%20on%20various%20related%20tasks.%20Project%20page%3A%20https%3A//fangzhou2000.github.io/projects/fleg.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLEG%253A%2520Feed-Forward%2520Language%2520Embedded%2520Gaussian%2520Splatting%2520from%2520Any%2520Views%26entry.906535625%3DQijian%2520Tian%2520and%2520Xin%2520Tan%2520and%2520Jiayu%2520Ying%2520and%2520Xuhong%2520Wang%2520and%2520Yuan%2520Xie%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3DWe%2520present%2520FLEG%252C%2520a%2520feed-forward%2520network%2520that%2520reconstructs%2520language-embedded%25203D%2520Gaussians%2520from%2520any%2520views.%2520Previous%2520straightforward%2520solutions%2520combine%2520feed-forward%2520reconstruction%2520with%2520Gaussian%2520heads%2520but%2520suffer%2520from%2520fixed%2520input%2520views%2520and%2520insufficient%25203D%2520training%2520data.%2520In%2520contrast%252C%2520we%2520propose%2520a%25203D-annotation-free%2520training%2520framework%2520for%25202D-to-3D%2520lifting%2520from%2520arbitrary%2520uncalibrated%2520and%2520unposed%2520multi-view%2520images.%2520Since%2520the%2520framework%2520does%2520not%2520require%25203D%2520annotations%252C%2520we%2520can%2520leverage%2520large-scale%2520video%2520data%2520with%2520easily%2520obtained%25202D%2520instance%2520information%2520to%2520enrich%2520semantic%2520embedding.%2520We%2520also%2520propose%2520an%2520instance-guided%2520contrastive%2520learning%2520to%2520align%25202D%2520semantics%2520with%2520the%25203D%2520representations.%2520In%2520addition%252C%2520to%2520mitigate%2520the%2520high%2520memory%2520and%2520computational%2520cost%2520of%2520dense%2520views%252C%2520we%2520further%2520propose%2520a%2520geometry-semantic%2520hierarchical%2520sparsification%2520strategy.%2520Our%2520FLEG%2520efficiently%2520reconstructs%2520language-embedded%25203D%2520Gaussian%2520representation%2520in%2520a%2520feed-forward%2520manner%2520from%2520arbitrary%2520sparse%2520or%2520dense%2520views%252C%2520jointly%2520producing%2520accurate%2520geometry%252C%2520high-fidelity%2520appearance%252C%2520and%2520language-aligned%2520semantics.%2520Extensive%2520experiments%2520show%2520that%2520it%2520outperforms%2520existing%2520methods%2520on%2520various%2520related%2520tasks.%2520Project%2520page%253A%2520https%253A//fangzhou2000.github.io/projects/fleg.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLEG%3A%20Feed-Forward%20Language%20Embedded%20Gaussian%20Splatting%20from%20Any%20Views&entry.906535625=Qijian%20Tian%20and%20Xin%20Tan%20and%20Jiayu%20Ying%20and%20Xuhong%20Wang%20and%20Yuan%20Xie%20and%20Lizhuang%20Ma&entry.1292438233=We%20present%20FLEG%2C%20a%20feed-forward%20network%20that%20reconstructs%20language-embedded%203D%20Gaussians%20from%20any%20views.%20Previous%20straightforward%20solutions%20combine%20feed-forward%20reconstruction%20with%20Gaussian%20heads%20but%20suffer%20from%20fixed%20input%20views%20and%20insufficient%203D%20training%20data.%20In%20contrast%2C%20we%20propose%20a%203D-annotation-free%20training%20framework%20for%202D-to-3D%20lifting%20from%20arbitrary%20uncalibrated%20and%20unposed%20multi-view%20images.%20Since%20the%20framework%20does%20not%20require%203D%20annotations%2C%20we%20can%20leverage%20large-scale%20video%20data%20with%20easily%20obtained%202D%20instance%20information%20to%20enrich%20semantic%20embedding.%20We%20also%20propose%20an%20instance-guided%20contrastive%20learning%20to%20align%202D%20semantics%20with%20the%203D%20representations.%20In%20addition%2C%20to%20mitigate%20the%20high%20memory%20and%20computational%20cost%20of%20dense%20views%2C%20we%20further%20propose%20a%20geometry-semantic%20hierarchical%20sparsification%20strategy.%20Our%20FLEG%20efficiently%20reconstructs%20language-embedded%203D%20Gaussian%20representation%20in%20a%20feed-forward%20manner%20from%20arbitrary%20sparse%20or%20dense%20views%2C%20jointly%20producing%20accurate%20geometry%2C%20high-fidelity%20appearance%2C%20and%20language-aligned%20semantics.%20Extensive%20experiments%20show%20that%20it%20outperforms%20existing%20methods%20on%20various%20related%20tasks.%20Project%20page%3A%20https%3A//fangzhou2000.github.io/projects/fleg.&entry.1838667208=http%3A//arxiv.org/abs/2512.17541v1&entry.124074799=Read"},
{"title": "SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing", "author": "Chaolei Wang and Yang Luo and Jing Du and Siyu Chen and Yiping Chen and Ting Han", "abstract": "Accurate 3D instance segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D instance segmentation based on 2D-to-3D lifting approaches struggle to produce precise instance-level segmentation, due to accumulated errors introduced during the lifting process from ambiguous semantic guidance and insufficient depth constraints. To tackle these challenges, we propose splitting and growing reliable semantic mask for high-fidelity 3D instance segmentation (SGS-3D), a novel \"split-then-grow\" framework that first purifies and splits ambiguous lifted masks using geometric primitives, and then grows them into complete instances within the scene. Unlike existing approaches that directly rely on raw lifted masks and sacrifice segmentation accuracy, SGS-3D serves as a training-free refinement method that jointly fuses semantic and geometric information, enabling effective cooperation between the two levels of representation. Specifically, for semantic guidance, we introduce a mask filtering strategy that leverages the co-occurrence of 3D geometry primitives to identify and remove ambiguous masks, thereby ensuring more reliable semantic consistency with the 3D object instances. For the geometric refinement, we construct fine-grained object instances by exploiting both spatial continuity and high-level features, particularly in the case of semantic ambiguity between distinct objects. Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that SGS-3D substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained models, yielding high-fidelity object instances while maintaining strong generalization across diverse indoor and outdoor environments. Code is available at https://github.com/wangchaolei7/SGS-3D.", "link": "http://arxiv.org/abs/2509.05144v2", "date": "2025-12-19", "relevancy": 3.0246, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6197}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGS-3D%3A%20High-Fidelity%203D%20Instance%20Segmentation%20via%20Reliable%20Semantic%20Mask%20Splitting%20and%20Growing&body=Title%3A%20SGS-3D%3A%20High-Fidelity%203D%20Instance%20Segmentation%20via%20Reliable%20Semantic%20Mask%20Splitting%20and%20Growing%0AAuthor%3A%20Chaolei%20Wang%20and%20Yang%20Luo%20and%20Jing%20Du%20and%20Siyu%20Chen%20and%20Yiping%20Chen%20and%20Ting%20Han%0AAbstract%3A%20Accurate%203D%20instance%20segmentation%20is%20crucial%20for%20high-quality%20scene%20understanding%20in%20the%203D%20vision%20domain.%20However%2C%203D%20instance%20segmentation%20based%20on%202D-to-3D%20lifting%20approaches%20struggle%20to%20produce%20precise%20instance-level%20segmentation%2C%20due%20to%20accumulated%20errors%20introduced%20during%20the%20lifting%20process%20from%20ambiguous%20semantic%20guidance%20and%20insufficient%20depth%20constraints.%20To%20tackle%20these%20challenges%2C%20we%20propose%20splitting%20and%20growing%20reliable%20semantic%20mask%20for%20high-fidelity%203D%20instance%20segmentation%20%28SGS-3D%29%2C%20a%20novel%20%22split-then-grow%22%20framework%20that%20first%20purifies%20and%20splits%20ambiguous%20lifted%20masks%20using%20geometric%20primitives%2C%20and%20then%20grows%20them%20into%20complete%20instances%20within%20the%20scene.%20Unlike%20existing%20approaches%20that%20directly%20rely%20on%20raw%20lifted%20masks%20and%20sacrifice%20segmentation%20accuracy%2C%20SGS-3D%20serves%20as%20a%20training-free%20refinement%20method%20that%20jointly%20fuses%20semantic%20and%20geometric%20information%2C%20enabling%20effective%20cooperation%20between%20the%20two%20levels%20of%20representation.%20Specifically%2C%20for%20semantic%20guidance%2C%20we%20introduce%20a%20mask%20filtering%20strategy%20that%20leverages%20the%20co-occurrence%20of%203D%20geometry%20primitives%20to%20identify%20and%20remove%20ambiguous%20masks%2C%20thereby%20ensuring%20more%20reliable%20semantic%20consistency%20with%20the%203D%20object%20instances.%20For%20the%20geometric%20refinement%2C%20we%20construct%20fine-grained%20object%20instances%20by%20exploiting%20both%20spatial%20continuity%20and%20high-level%20features%2C%20particularly%20in%20the%20case%20of%20semantic%20ambiguity%20between%20distinct%20objects.%20Experimental%20results%20on%20ScanNet200%2C%20ScanNet%2B%2B%2C%20and%20KITTI-360%20demonstrate%20that%20SGS-3D%20substantially%20improves%20segmentation%20accuracy%20and%20robustness%20against%20inaccurate%20masks%20from%20pre-trained%20models%2C%20yielding%20high-fidelity%20object%20instances%20while%20maintaining%20strong%20generalization%20across%20diverse%20indoor%20and%20outdoor%20environments.%20Code%20is%20available%20at%20https%3A//github.com/wangchaolei7/SGS-3D.%0ALink%3A%20http%3A//arxiv.org/abs/2509.05144v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGS-3D%253A%2520High-Fidelity%25203D%2520Instance%2520Segmentation%2520via%2520Reliable%2520Semantic%2520Mask%2520Splitting%2520and%2520Growing%26entry.906535625%3DChaolei%2520Wang%2520and%2520Yang%2520Luo%2520and%2520Jing%2520Du%2520and%2520Siyu%2520Chen%2520and%2520Yiping%2520Chen%2520and%2520Ting%2520Han%26entry.1292438233%3DAccurate%25203D%2520instance%2520segmentation%2520is%2520crucial%2520for%2520high-quality%2520scene%2520understanding%2520in%2520the%25203D%2520vision%2520domain.%2520However%252C%25203D%2520instance%2520segmentation%2520based%2520on%25202D-to-3D%2520lifting%2520approaches%2520struggle%2520to%2520produce%2520precise%2520instance-level%2520segmentation%252C%2520due%2520to%2520accumulated%2520errors%2520introduced%2520during%2520the%2520lifting%2520process%2520from%2520ambiguous%2520semantic%2520guidance%2520and%2520insufficient%2520depth%2520constraints.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520splitting%2520and%2520growing%2520reliable%2520semantic%2520mask%2520for%2520high-fidelity%25203D%2520instance%2520segmentation%2520%2528SGS-3D%2529%252C%2520a%2520novel%2520%2522split-then-grow%2522%2520framework%2520that%2520first%2520purifies%2520and%2520splits%2520ambiguous%2520lifted%2520masks%2520using%2520geometric%2520primitives%252C%2520and%2520then%2520grows%2520them%2520into%2520complete%2520instances%2520within%2520the%2520scene.%2520Unlike%2520existing%2520approaches%2520that%2520directly%2520rely%2520on%2520raw%2520lifted%2520masks%2520and%2520sacrifice%2520segmentation%2520accuracy%252C%2520SGS-3D%2520serves%2520as%2520a%2520training-free%2520refinement%2520method%2520that%2520jointly%2520fuses%2520semantic%2520and%2520geometric%2520information%252C%2520enabling%2520effective%2520cooperation%2520between%2520the%2520two%2520levels%2520of%2520representation.%2520Specifically%252C%2520for%2520semantic%2520guidance%252C%2520we%2520introduce%2520a%2520mask%2520filtering%2520strategy%2520that%2520leverages%2520the%2520co-occurrence%2520of%25203D%2520geometry%2520primitives%2520to%2520identify%2520and%2520remove%2520ambiguous%2520masks%252C%2520thereby%2520ensuring%2520more%2520reliable%2520semantic%2520consistency%2520with%2520the%25203D%2520object%2520instances.%2520For%2520the%2520geometric%2520refinement%252C%2520we%2520construct%2520fine-grained%2520object%2520instances%2520by%2520exploiting%2520both%2520spatial%2520continuity%2520and%2520high-level%2520features%252C%2520particularly%2520in%2520the%2520case%2520of%2520semantic%2520ambiguity%2520between%2520distinct%2520objects.%2520Experimental%2520results%2520on%2520ScanNet200%252C%2520ScanNet%252B%252B%252C%2520and%2520KITTI-360%2520demonstrate%2520that%2520SGS-3D%2520substantially%2520improves%2520segmentation%2520accuracy%2520and%2520robustness%2520against%2520inaccurate%2520masks%2520from%2520pre-trained%2520models%252C%2520yielding%2520high-fidelity%2520object%2520instances%2520while%2520maintaining%2520strong%2520generalization%2520across%2520diverse%2520indoor%2520and%2520outdoor%2520environments.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/wangchaolei7/SGS-3D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05144v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGS-3D%3A%20High-Fidelity%203D%20Instance%20Segmentation%20via%20Reliable%20Semantic%20Mask%20Splitting%20and%20Growing&entry.906535625=Chaolei%20Wang%20and%20Yang%20Luo%20and%20Jing%20Du%20and%20Siyu%20Chen%20and%20Yiping%20Chen%20and%20Ting%20Han&entry.1292438233=Accurate%203D%20instance%20segmentation%20is%20crucial%20for%20high-quality%20scene%20understanding%20in%20the%203D%20vision%20domain.%20However%2C%203D%20instance%20segmentation%20based%20on%202D-to-3D%20lifting%20approaches%20struggle%20to%20produce%20precise%20instance-level%20segmentation%2C%20due%20to%20accumulated%20errors%20introduced%20during%20the%20lifting%20process%20from%20ambiguous%20semantic%20guidance%20and%20insufficient%20depth%20constraints.%20To%20tackle%20these%20challenges%2C%20we%20propose%20splitting%20and%20growing%20reliable%20semantic%20mask%20for%20high-fidelity%203D%20instance%20segmentation%20%28SGS-3D%29%2C%20a%20novel%20%22split-then-grow%22%20framework%20that%20first%20purifies%20and%20splits%20ambiguous%20lifted%20masks%20using%20geometric%20primitives%2C%20and%20then%20grows%20them%20into%20complete%20instances%20within%20the%20scene.%20Unlike%20existing%20approaches%20that%20directly%20rely%20on%20raw%20lifted%20masks%20and%20sacrifice%20segmentation%20accuracy%2C%20SGS-3D%20serves%20as%20a%20training-free%20refinement%20method%20that%20jointly%20fuses%20semantic%20and%20geometric%20information%2C%20enabling%20effective%20cooperation%20between%20the%20two%20levels%20of%20representation.%20Specifically%2C%20for%20semantic%20guidance%2C%20we%20introduce%20a%20mask%20filtering%20strategy%20that%20leverages%20the%20co-occurrence%20of%203D%20geometry%20primitives%20to%20identify%20and%20remove%20ambiguous%20masks%2C%20thereby%20ensuring%20more%20reliable%20semantic%20consistency%20with%20the%203D%20object%20instances.%20For%20the%20geometric%20refinement%2C%20we%20construct%20fine-grained%20object%20instances%20by%20exploiting%20both%20spatial%20continuity%20and%20high-level%20features%2C%20particularly%20in%20the%20case%20of%20semantic%20ambiguity%20between%20distinct%20objects.%20Experimental%20results%20on%20ScanNet200%2C%20ScanNet%2B%2B%2C%20and%20KITTI-360%20demonstrate%20that%20SGS-3D%20substantially%20improves%20segmentation%20accuracy%20and%20robustness%20against%20inaccurate%20masks%20from%20pre-trained%20models%2C%20yielding%20high-fidelity%20object%20instances%20while%20maintaining%20strong%20generalization%20across%20diverse%20indoor%20and%20outdoor%20environments.%20Code%20is%20available%20at%20https%3A//github.com/wangchaolei7/SGS-3D.&entry.1838667208=http%3A//arxiv.org/abs/2509.05144v2&entry.124074799=Read"},
{"title": "Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding", "author": "Yue Li and Qi Ma and Runyi Yang and Mengjiao Ma and Bin Ren and Nikola Popovic and Nicu Sebe and Theo Gevers and Luc Van Gool and Danda Pani Paudel and Martin R. Oswald", "abstract": "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.\n  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.", "link": "http://arxiv.org/abs/2512.17817v1", "date": "2025-12-19", "relevancy": 2.9956, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chorus%3A%20Multi-Teacher%20Pretraining%20for%20Holistic%203D%20Gaussian%20Scene%20Encoding&body=Title%3A%20Chorus%3A%20Multi-Teacher%20Pretraining%20for%20Holistic%203D%20Gaussian%20Scene%20Encoding%0AAuthor%3A%20Yue%20Li%20and%20Qi%20Ma%20and%20Runyi%20Yang%20and%20Mengjiao%20Ma%20and%20Bin%20Ren%20and%20Nikola%20Popovic%20and%20Nicu%20Sebe%20and%20Theo%20Gevers%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20While%203DGS%20has%20emerged%20as%20a%20high-fidelity%20scene%20representation%2C%20encoding%20rich%2C%20general-purpose%20features%20directly%20from%20its%20primitives%20remains%20under-explored.%20We%20address%20this%20gap%20by%20introducing%20Chorus%2C%20a%20multi-teacher%20pretraining%20framework%20that%20learns%20a%20holistic%20feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20scene%20encoder%20by%20distilling%20complementary%20signals%20from%202D%20foundation%20models.%20Chorus%20employs%20a%20shared%203D%20encoder%20and%20teacher-specific%20projectors%20to%20learn%20from%20language-aligned%2C%20generalist%2C%20and%20object-aware%20teachers%2C%20encouraging%20a%20shared%20embedding%20space%20that%20captures%20signals%20from%20high-level%20semantics%20to%20fine-grained%20structure.%0A%20%20We%20evaluate%20Chorus%20on%20a%20wide%20range%20of%20tasks%3A%20open-vocabulary%20semantic%20and%20instance%20segmentation%2C%20linear%20and%20decoder%20probing%2C%20as%20well%20as%20data-efficient%20supervision.%20Besides%203DGS%2C%20we%20also%20test%20Chorus%20on%20several%20benchmarks%20that%20only%20support%20point%20clouds%20by%20pretraining%20a%20variant%20using%20only%20Gaussians%27%20centers%2C%20colors%2C%20estimated%20normals%20as%20inputs.%20Interestingly%2C%20this%20encoder%20shows%20strong%20transfer%20and%20outperforms%20the%20point%20clouds%20baseline%20while%20using%2039.9%20times%20fewer%20training%20scenes.%20Finally%2C%20we%20propose%20a%20render-and-distill%20adaptation%20that%20facilitates%20out-of-domain%20finetuning.%20Our%20code%20and%20model%20will%20be%20released%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChorus%253A%2520Multi-Teacher%2520Pretraining%2520for%2520Holistic%25203D%2520Gaussian%2520Scene%2520Encoding%26entry.906535625%3DYue%2520Li%2520and%2520Qi%2520Ma%2520and%2520Runyi%2520Yang%2520and%2520Mengjiao%2520Ma%2520and%2520Bin%2520Ren%2520and%2520Nikola%2520Popovic%2520and%2520Nicu%2520Sebe%2520and%2520Theo%2520Gevers%2520and%2520Luc%2520Van%2520Gool%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3DWhile%25203DGS%2520has%2520emerged%2520as%2520a%2520high-fidelity%2520scene%2520representation%252C%2520encoding%2520rich%252C%2520general-purpose%2520features%2520directly%2520from%2520its%2520primitives%2520remains%2520under-explored.%2520We%2520address%2520this%2520gap%2520by%2520introducing%2520Chorus%252C%2520a%2520multi-teacher%2520pretraining%2520framework%2520that%2520learns%2520a%2520holistic%2520feed-forward%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520scene%2520encoder%2520by%2520distilling%2520complementary%2520signals%2520from%25202D%2520foundation%2520models.%2520Chorus%2520employs%2520a%2520shared%25203D%2520encoder%2520and%2520teacher-specific%2520projectors%2520to%2520learn%2520from%2520language-aligned%252C%2520generalist%252C%2520and%2520object-aware%2520teachers%252C%2520encouraging%2520a%2520shared%2520embedding%2520space%2520that%2520captures%2520signals%2520from%2520high-level%2520semantics%2520to%2520fine-grained%2520structure.%250A%2520%2520We%2520evaluate%2520Chorus%2520on%2520a%2520wide%2520range%2520of%2520tasks%253A%2520open-vocabulary%2520semantic%2520and%2520instance%2520segmentation%252C%2520linear%2520and%2520decoder%2520probing%252C%2520as%2520well%2520as%2520data-efficient%2520supervision.%2520Besides%25203DGS%252C%2520we%2520also%2520test%2520Chorus%2520on%2520several%2520benchmarks%2520that%2520only%2520support%2520point%2520clouds%2520by%2520pretraining%2520a%2520variant%2520using%2520only%2520Gaussians%2527%2520centers%252C%2520colors%252C%2520estimated%2520normals%2520as%2520inputs.%2520Interestingly%252C%2520this%2520encoder%2520shows%2520strong%2520transfer%2520and%2520outperforms%2520the%2520point%2520clouds%2520baseline%2520while%2520using%252039.9%2520times%2520fewer%2520training%2520scenes.%2520Finally%252C%2520we%2520propose%2520a%2520render-and-distill%2520adaptation%2520that%2520facilitates%2520out-of-domain%2520finetuning.%2520Our%2520code%2520and%2520model%2520will%2520be%2520released%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chorus%3A%20Multi-Teacher%20Pretraining%20for%20Holistic%203D%20Gaussian%20Scene%20Encoding&entry.906535625=Yue%20Li%20and%20Qi%20Ma%20and%20Runyi%20Yang%20and%20Mengjiao%20Ma%20and%20Bin%20Ren%20and%20Nikola%20Popovic%20and%20Nicu%20Sebe%20and%20Theo%20Gevers%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%20and%20Martin%20R.%20Oswald&entry.1292438233=While%203DGS%20has%20emerged%20as%20a%20high-fidelity%20scene%20representation%2C%20encoding%20rich%2C%20general-purpose%20features%20directly%20from%20its%20primitives%20remains%20under-explored.%20We%20address%20this%20gap%20by%20introducing%20Chorus%2C%20a%20multi-teacher%20pretraining%20framework%20that%20learns%20a%20holistic%20feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20scene%20encoder%20by%20distilling%20complementary%20signals%20from%202D%20foundation%20models.%20Chorus%20employs%20a%20shared%203D%20encoder%20and%20teacher-specific%20projectors%20to%20learn%20from%20language-aligned%2C%20generalist%2C%20and%20object-aware%20teachers%2C%20encouraging%20a%20shared%20embedding%20space%20that%20captures%20signals%20from%20high-level%20semantics%20to%20fine-grained%20structure.%0A%20%20We%20evaluate%20Chorus%20on%20a%20wide%20range%20of%20tasks%3A%20open-vocabulary%20semantic%20and%20instance%20segmentation%2C%20linear%20and%20decoder%20probing%2C%20as%20well%20as%20data-efficient%20supervision.%20Besides%203DGS%2C%20we%20also%20test%20Chorus%20on%20several%20benchmarks%20that%20only%20support%20point%20clouds%20by%20pretraining%20a%20variant%20using%20only%20Gaussians%27%20centers%2C%20colors%2C%20estimated%20normals%20as%20inputs.%20Interestingly%2C%20this%20encoder%20shows%20strong%20transfer%20and%20outperforms%20the%20point%20clouds%20baseline%20while%20using%2039.9%20times%20fewer%20training%20scenes.%20Finally%2C%20we%20propose%20a%20render-and-distill%20adaptation%20that%20facilitates%20out-of-domain%20finetuning.%20Our%20code%20and%20model%20will%20be%20released%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2512.17817v1&entry.124074799=Read"},
{"title": "SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds", "author": "Alexander Dow and Manduhu Manduhu and Matheus Santos and Ben Bartlett and Gerard Dooly and James Riordan", "abstract": "This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.", "link": "http://arxiv.org/abs/2512.08557v2", "date": "2025-12-19", "relevancy": 2.9775, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6275}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5795}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds&body=Title%3A%20SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Alexander%20Dow%20and%20Manduhu%20Manduhu%20and%20Matheus%20Santos%20and%20Ben%20Bartlett%20and%20Gerard%20Dooly%20and%20James%20Riordan%0AAbstract%3A%20This%20work%20leverages%20the%20continuous%20sweeping%20motion%20of%20LiDAR%20scanning%20to%20concentrate%20object%20detection%20efforts%20on%20specific%20regions%20that%20receive%20a%20change%20in%20point%20data%20from%20one%20frame%20to%20another.%20We%20achieve%20this%20by%20using%20a%20sliding%20time%20window%20with%20short%20strides%20and%20consider%20the%20temporal%20dimension%20by%20storing%20convolution%20results%20between%20passes.%20This%20allows%20us%20to%20ignore%20unchanged%20regions%2C%20significantly%20reducing%20the%20number%20of%20convolution%20operations%20per%20forward%20pass%20without%20sacrificing%20accuracy.%20This%20data%20reuse%20scheme%20introduces%20extreme%20sparsity%20to%20detection%20data.%20To%20exploit%20this%20sparsity%2C%20we%20extend%20our%20previous%20work%20on%20scatter-based%20convolutions%20to%20allow%20for%20data%20reuse%2C%20and%20as%20such%20propose%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20%28SSCATeR%29.%20This%20operation%20treats%20incoming%20LiDAR%20data%20as%20a%20continuous%20stream%20and%20acts%20only%20on%20the%20changing%20parts%20of%20the%20point%20cloud.%20By%20doing%20so%2C%20we%20achieve%20the%20same%20results%20with%20as%20much%20as%20a%206.61-fold%20reduction%20in%20processing%20time.%20Our%20test%20results%20show%20that%20the%20feature%20maps%20output%20by%20our%20method%20are%20identical%20to%20those%20produced%20by%20traditional%20sparse%20convolution%20techniques%2C%20whilst%20greatly%20increasing%20the%20computational%20efficiency%20of%20the%20network.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSCATeR%253A%2520Sparse%2520Scatter-Based%2520Convolution%2520Algorithm%2520with%2520Temporal%2520Data%2520Recycling%2520for%2520Real-Time%25203D%2520Object%2520Detection%2520in%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DAlexander%2520Dow%2520and%2520Manduhu%2520Manduhu%2520and%2520Matheus%2520Santos%2520and%2520Ben%2520Bartlett%2520and%2520Gerard%2520Dooly%2520and%2520James%2520Riordan%26entry.1292438233%3DThis%2520work%2520leverages%2520the%2520continuous%2520sweeping%2520motion%2520of%2520LiDAR%2520scanning%2520to%2520concentrate%2520object%2520detection%2520efforts%2520on%2520specific%2520regions%2520that%2520receive%2520a%2520change%2520in%2520point%2520data%2520from%2520one%2520frame%2520to%2520another.%2520We%2520achieve%2520this%2520by%2520using%2520a%2520sliding%2520time%2520window%2520with%2520short%2520strides%2520and%2520consider%2520the%2520temporal%2520dimension%2520by%2520storing%2520convolution%2520results%2520between%2520passes.%2520This%2520allows%2520us%2520to%2520ignore%2520unchanged%2520regions%252C%2520significantly%2520reducing%2520the%2520number%2520of%2520convolution%2520operations%2520per%2520forward%2520pass%2520without%2520sacrificing%2520accuracy.%2520This%2520data%2520reuse%2520scheme%2520introduces%2520extreme%2520sparsity%2520to%2520detection%2520data.%2520To%2520exploit%2520this%2520sparsity%252C%2520we%2520extend%2520our%2520previous%2520work%2520on%2520scatter-based%2520convolutions%2520to%2520allow%2520for%2520data%2520reuse%252C%2520and%2520as%2520such%2520propose%2520Sparse%2520Scatter-Based%2520Convolution%2520Algorithm%2520with%2520Temporal%2520Data%2520Recycling%2520%2528SSCATeR%2529.%2520This%2520operation%2520treats%2520incoming%2520LiDAR%2520data%2520as%2520a%2520continuous%2520stream%2520and%2520acts%2520only%2520on%2520the%2520changing%2520parts%2520of%2520the%2520point%2520cloud.%2520By%2520doing%2520so%252C%2520we%2520achieve%2520the%2520same%2520results%2520with%2520as%2520much%2520as%2520a%25206.61-fold%2520reduction%2520in%2520processing%2520time.%2520Our%2520test%2520results%2520show%2520that%2520the%2520feature%2520maps%2520output%2520by%2520our%2520method%2520are%2520identical%2520to%2520those%2520produced%2520by%2520traditional%2520sparse%2520convolution%2520techniques%252C%2520whilst%2520greatly%2520increasing%2520the%2520computational%2520efficiency%2520of%2520the%2520network.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds&entry.906535625=Alexander%20Dow%20and%20Manduhu%20Manduhu%20and%20Matheus%20Santos%20and%20Ben%20Bartlett%20and%20Gerard%20Dooly%20and%20James%20Riordan&entry.1292438233=This%20work%20leverages%20the%20continuous%20sweeping%20motion%20of%20LiDAR%20scanning%20to%20concentrate%20object%20detection%20efforts%20on%20specific%20regions%20that%20receive%20a%20change%20in%20point%20data%20from%20one%20frame%20to%20another.%20We%20achieve%20this%20by%20using%20a%20sliding%20time%20window%20with%20short%20strides%20and%20consider%20the%20temporal%20dimension%20by%20storing%20convolution%20results%20between%20passes.%20This%20allows%20us%20to%20ignore%20unchanged%20regions%2C%20significantly%20reducing%20the%20number%20of%20convolution%20operations%20per%20forward%20pass%20without%20sacrificing%20accuracy.%20This%20data%20reuse%20scheme%20introduces%20extreme%20sparsity%20to%20detection%20data.%20To%20exploit%20this%20sparsity%2C%20we%20extend%20our%20previous%20work%20on%20scatter-based%20convolutions%20to%20allow%20for%20data%20reuse%2C%20and%20as%20such%20propose%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20%28SSCATeR%29.%20This%20operation%20treats%20incoming%20LiDAR%20data%20as%20a%20continuous%20stream%20and%20acts%20only%20on%20the%20changing%20parts%20of%20the%20point%20cloud.%20By%20doing%20so%2C%20we%20achieve%20the%20same%20results%20with%20as%20much%20as%20a%206.61-fold%20reduction%20in%20processing%20time.%20Our%20test%20results%20show%20that%20the%20feature%20maps%20output%20by%20our%20method%20are%20identical%20to%20those%20produced%20by%20traditional%20sparse%20convolution%20techniques%2C%20whilst%20greatly%20increasing%20the%20computational%20efficiency%20of%20the%20network.&entry.1838667208=http%3A//arxiv.org/abs/2512.08557v2&entry.124074799=Read"},
{"title": "Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image", "author": "Simon Giebenhain and Tobias Kirschstein and Liam Schoneveld and Davide Davoli and Zhe Chen and Matthias Nie\u00dfner", "abstract": "Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.", "link": "http://arxiv.org/abs/2512.17773v1", "date": "2025-12-19", "relevancy": 2.9707, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6011}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5942}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pix2NPHM%3A%20Learning%20to%20Regress%20NPHM%20Reconstructions%20From%20a%20Single%20Image&body=Title%3A%20Pix2NPHM%3A%20Learning%20to%20Regress%20NPHM%20Reconstructions%20From%20a%20Single%20Image%0AAuthor%3A%20Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Liam%20Schoneveld%20and%20Davide%20Davoli%20and%20Zhe%20Chen%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20Neural%20Parametric%20Head%20Models%20%28NPHMs%29%20are%20a%20recent%20advancement%20over%20mesh-based%203d%20morphable%20models%20%283DMMs%29%20to%20facilitate%20high-fidelity%20geometric%20detail.%20However%2C%20fitting%20NPHMs%20to%20visual%20inputs%20is%20notoriously%20challenging%20due%20to%20the%20expressive%20nature%20of%20their%20underlying%20latent%20space.%20To%20this%20end%2C%20we%20propose%20Pix2NPHM%2C%20a%20vision%20transformer%20%28ViT%29%20network%20that%20directly%20regresses%20NPHM%20parameters%2C%20given%20a%20single%20image%20as%20input.%20Compared%20to%20existing%20approaches%2C%20the%20neural%20parametric%20space%20allows%20our%20method%20to%20reconstruct%20more%20recognizable%20facial%20geometry%20and%20accurate%20facial%20expressions.%20For%20broad%20generalization%2C%20we%20exploit%20domain-specific%20ViTs%20as%20backbones%2C%20which%20are%20pretrained%20on%20geometric%20prediction%20tasks.%20We%20train%20Pix2NPHM%20on%20a%20mixture%20of%203D%20data%2C%20including%20a%20total%20of%20over%20100K%20NPHM%20registrations%20that%20enable%20direct%20supervision%20in%20SDF%20space%2C%20and%20large-scale%202D%20video%20datasets%2C%20for%20which%20normal%20estimates%20serve%20as%20pseudo%20ground%20truth%20geometry.%20Pix2NPHM%20not%20only%20allows%20for%203D%20reconstructions%20at%20interactive%20frame%20rates%2C%20it%20is%20also%20possible%20to%20improve%20geometric%20fidelity%20by%20a%20subsequent%20inference-time%20optimization%20against%20estimated%20surface%20normals%20and%20canonical%20point%20maps.%20As%20a%20result%2C%20we%20achieve%20unprecedented%20face%20reconstruction%20quality%20that%20can%20run%20at%20scale%20on%20in-the-wild%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPix2NPHM%253A%2520Learning%2520to%2520Regress%2520NPHM%2520Reconstructions%2520From%2520a%2520Single%2520Image%26entry.906535625%3DSimon%2520Giebenhain%2520and%2520Tobias%2520Kirschstein%2520and%2520Liam%2520Schoneveld%2520and%2520Davide%2520Davoli%2520and%2520Zhe%2520Chen%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3DNeural%2520Parametric%2520Head%2520Models%2520%2528NPHMs%2529%2520are%2520a%2520recent%2520advancement%2520over%2520mesh-based%25203d%2520morphable%2520models%2520%25283DMMs%2529%2520to%2520facilitate%2520high-fidelity%2520geometric%2520detail.%2520However%252C%2520fitting%2520NPHMs%2520to%2520visual%2520inputs%2520is%2520notoriously%2520challenging%2520due%2520to%2520the%2520expressive%2520nature%2520of%2520their%2520underlying%2520latent%2520space.%2520To%2520this%2520end%252C%2520we%2520propose%2520Pix2NPHM%252C%2520a%2520vision%2520transformer%2520%2528ViT%2529%2520network%2520that%2520directly%2520regresses%2520NPHM%2520parameters%252C%2520given%2520a%2520single%2520image%2520as%2520input.%2520Compared%2520to%2520existing%2520approaches%252C%2520the%2520neural%2520parametric%2520space%2520allows%2520our%2520method%2520to%2520reconstruct%2520more%2520recognizable%2520facial%2520geometry%2520and%2520accurate%2520facial%2520expressions.%2520For%2520broad%2520generalization%252C%2520we%2520exploit%2520domain-specific%2520ViTs%2520as%2520backbones%252C%2520which%2520are%2520pretrained%2520on%2520geometric%2520prediction%2520tasks.%2520We%2520train%2520Pix2NPHM%2520on%2520a%2520mixture%2520of%25203D%2520data%252C%2520including%2520a%2520total%2520of%2520over%2520100K%2520NPHM%2520registrations%2520that%2520enable%2520direct%2520supervision%2520in%2520SDF%2520space%252C%2520and%2520large-scale%25202D%2520video%2520datasets%252C%2520for%2520which%2520normal%2520estimates%2520serve%2520as%2520pseudo%2520ground%2520truth%2520geometry.%2520Pix2NPHM%2520not%2520only%2520allows%2520for%25203D%2520reconstructions%2520at%2520interactive%2520frame%2520rates%252C%2520it%2520is%2520also%2520possible%2520to%2520improve%2520geometric%2520fidelity%2520by%2520a%2520subsequent%2520inference-time%2520optimization%2520against%2520estimated%2520surface%2520normals%2520and%2520canonical%2520point%2520maps.%2520As%2520a%2520result%252C%2520we%2520achieve%2520unprecedented%2520face%2520reconstruction%2520quality%2520that%2520can%2520run%2520at%2520scale%2520on%2520in-the-wild%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pix2NPHM%3A%20Learning%20to%20Regress%20NPHM%20Reconstructions%20From%20a%20Single%20Image&entry.906535625=Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Liam%20Schoneveld%20and%20Davide%20Davoli%20and%20Zhe%20Chen%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=Neural%20Parametric%20Head%20Models%20%28NPHMs%29%20are%20a%20recent%20advancement%20over%20mesh-based%203d%20morphable%20models%20%283DMMs%29%20to%20facilitate%20high-fidelity%20geometric%20detail.%20However%2C%20fitting%20NPHMs%20to%20visual%20inputs%20is%20notoriously%20challenging%20due%20to%20the%20expressive%20nature%20of%20their%20underlying%20latent%20space.%20To%20this%20end%2C%20we%20propose%20Pix2NPHM%2C%20a%20vision%20transformer%20%28ViT%29%20network%20that%20directly%20regresses%20NPHM%20parameters%2C%20given%20a%20single%20image%20as%20input.%20Compared%20to%20existing%20approaches%2C%20the%20neural%20parametric%20space%20allows%20our%20method%20to%20reconstruct%20more%20recognizable%20facial%20geometry%20and%20accurate%20facial%20expressions.%20For%20broad%20generalization%2C%20we%20exploit%20domain-specific%20ViTs%20as%20backbones%2C%20which%20are%20pretrained%20on%20geometric%20prediction%20tasks.%20We%20train%20Pix2NPHM%20on%20a%20mixture%20of%203D%20data%2C%20including%20a%20total%20of%20over%20100K%20NPHM%20registrations%20that%20enable%20direct%20supervision%20in%20SDF%20space%2C%20and%20large-scale%202D%20video%20datasets%2C%20for%20which%20normal%20estimates%20serve%20as%20pseudo%20ground%20truth%20geometry.%20Pix2NPHM%20not%20only%20allows%20for%203D%20reconstructions%20at%20interactive%20frame%20rates%2C%20it%20is%20also%20possible%20to%20improve%20geometric%20fidelity%20by%20a%20subsequent%20inference-time%20optimization%20against%20estimated%20surface%20normals%20and%20canonical%20point%20maps.%20As%20a%20result%2C%20we%20achieve%20unprecedented%20face%20reconstruction%20quality%20that%20can%20run%20at%20scale%20on%20in-the-wild%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.17773v1&entry.124074799=Read"},
{"title": "Human-like Content Analysis for Generative AI with Language-Grounded Sparse Encoders", "author": "Yiming Tang and Arash Lagzian and Srinivas Anumasa and Qiran Zou and Yingtao Zhu and Ye Zhang and Trang Nguyen and Yih-Chung Tham and Ehsan Adeli and Ching-Yu Cheng and Yilun Du and Dianbo Liu", "abstract": "The rapid development of generative AI has transformed content creation, communication, and human development. However, this technology raises profound concerns in high-stakes domains, demanding rigorous methods to analyze and evaluate AI-generated content. While existing analytic methods often treat images as indivisible wholes, real-world AI failures generally manifest as specific visual patterns that can evade holistic detection and suit more granular and decomposed analysis. Here we introduce a content analysis tool, Language-Grounded Sparse Encoders (LanSE), which decompose images into interpretable visual patterns with natural language descriptions. Utilizing interpretability modules and large multimodal models, LanSE can automatically identify visual patterns within data modalities. Our method discovers more than 5,000 visual patterns with 93\\% human agreement, provides decomposed evaluation outperforming existing methods, establishes the first systematic evaluation of physical plausibility, and extends to medical imaging settings. Our method's capability to extract language-grounded patterns can be naturally adapted to numerous fields, including biology and geography, as well as other data modalities such as protein structures and time series, thereby advancing content analysis for generative AI.", "link": "http://arxiv.org/abs/2508.18236v3", "date": "2025-12-19", "relevancy": 2.9491, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-like%20Content%20Analysis%20for%20Generative%20AI%20with%20Language-Grounded%20Sparse%20Encoders&body=Title%3A%20Human-like%20Content%20Analysis%20for%20Generative%20AI%20with%20Language-Grounded%20Sparse%20Encoders%0AAuthor%3A%20Yiming%20Tang%20and%20Arash%20Lagzian%20and%20Srinivas%20Anumasa%20and%20Qiran%20Zou%20and%20Yingtao%20Zhu%20and%20Ye%20Zhang%20and%20Trang%20Nguyen%20and%20Yih-Chung%20Tham%20and%20Ehsan%20Adeli%20and%20Ching-Yu%20Cheng%20and%20Yilun%20Du%20and%20Dianbo%20Liu%0AAbstract%3A%20The%20rapid%20development%20of%20generative%20AI%20has%20transformed%20content%20creation%2C%20communication%2C%20and%20human%20development.%20However%2C%20this%20technology%20raises%20profound%20concerns%20in%20high-stakes%20domains%2C%20demanding%20rigorous%20methods%20to%20analyze%20and%20evaluate%20AI-generated%20content.%20While%20existing%20analytic%20methods%20often%20treat%20images%20as%20indivisible%20wholes%2C%20real-world%20AI%20failures%20generally%20manifest%20as%20specific%20visual%20patterns%20that%20can%20evade%20holistic%20detection%20and%20suit%20more%20granular%20and%20decomposed%20analysis.%20Here%20we%20introduce%20a%20content%20analysis%20tool%2C%20Language-Grounded%20Sparse%20Encoders%20%28LanSE%29%2C%20which%20decompose%20images%20into%20interpretable%20visual%20patterns%20with%20natural%20language%20descriptions.%20Utilizing%20interpretability%20modules%20and%20large%20multimodal%20models%2C%20LanSE%20can%20automatically%20identify%20visual%20patterns%20within%20data%20modalities.%20Our%20method%20discovers%20more%20than%205%2C000%20visual%20patterns%20with%2093%5C%25%20human%20agreement%2C%20provides%20decomposed%20evaluation%20outperforming%20existing%20methods%2C%20establishes%20the%20first%20systematic%20evaluation%20of%20physical%20plausibility%2C%20and%20extends%20to%20medical%20imaging%20settings.%20Our%20method%27s%20capability%20to%20extract%20language-grounded%20patterns%20can%20be%20naturally%20adapted%20to%20numerous%20fields%2C%20including%20biology%20and%20geography%2C%20as%20well%20as%20other%20data%20modalities%20such%20as%20protein%20structures%20and%20time%20series%2C%20thereby%20advancing%20content%20analysis%20for%20generative%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18236v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-like%2520Content%2520Analysis%2520for%2520Generative%2520AI%2520with%2520Language-Grounded%2520Sparse%2520Encoders%26entry.906535625%3DYiming%2520Tang%2520and%2520Arash%2520Lagzian%2520and%2520Srinivas%2520Anumasa%2520and%2520Qiran%2520Zou%2520and%2520Yingtao%2520Zhu%2520and%2520Ye%2520Zhang%2520and%2520Trang%2520Nguyen%2520and%2520Yih-Chung%2520Tham%2520and%2520Ehsan%2520Adeli%2520and%2520Ching-Yu%2520Cheng%2520and%2520Yilun%2520Du%2520and%2520Dianbo%2520Liu%26entry.1292438233%3DThe%2520rapid%2520development%2520of%2520generative%2520AI%2520has%2520transformed%2520content%2520creation%252C%2520communication%252C%2520and%2520human%2520development.%2520However%252C%2520this%2520technology%2520raises%2520profound%2520concerns%2520in%2520high-stakes%2520domains%252C%2520demanding%2520rigorous%2520methods%2520to%2520analyze%2520and%2520evaluate%2520AI-generated%2520content.%2520While%2520existing%2520analytic%2520methods%2520often%2520treat%2520images%2520as%2520indivisible%2520wholes%252C%2520real-world%2520AI%2520failures%2520generally%2520manifest%2520as%2520specific%2520visual%2520patterns%2520that%2520can%2520evade%2520holistic%2520detection%2520and%2520suit%2520more%2520granular%2520and%2520decomposed%2520analysis.%2520Here%2520we%2520introduce%2520a%2520content%2520analysis%2520tool%252C%2520Language-Grounded%2520Sparse%2520Encoders%2520%2528LanSE%2529%252C%2520which%2520decompose%2520images%2520into%2520interpretable%2520visual%2520patterns%2520with%2520natural%2520language%2520descriptions.%2520Utilizing%2520interpretability%2520modules%2520and%2520large%2520multimodal%2520models%252C%2520LanSE%2520can%2520automatically%2520identify%2520visual%2520patterns%2520within%2520data%2520modalities.%2520Our%2520method%2520discovers%2520more%2520than%25205%252C000%2520visual%2520patterns%2520with%252093%255C%2525%2520human%2520agreement%252C%2520provides%2520decomposed%2520evaluation%2520outperforming%2520existing%2520methods%252C%2520establishes%2520the%2520first%2520systematic%2520evaluation%2520of%2520physical%2520plausibility%252C%2520and%2520extends%2520to%2520medical%2520imaging%2520settings.%2520Our%2520method%2527s%2520capability%2520to%2520extract%2520language-grounded%2520patterns%2520can%2520be%2520naturally%2520adapted%2520to%2520numerous%2520fields%252C%2520including%2520biology%2520and%2520geography%252C%2520as%2520well%2520as%2520other%2520data%2520modalities%2520such%2520as%2520protein%2520structures%2520and%2520time%2520series%252C%2520thereby%2520advancing%2520content%2520analysis%2520for%2520generative%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18236v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-like%20Content%20Analysis%20for%20Generative%20AI%20with%20Language-Grounded%20Sparse%20Encoders&entry.906535625=Yiming%20Tang%20and%20Arash%20Lagzian%20and%20Srinivas%20Anumasa%20and%20Qiran%20Zou%20and%20Yingtao%20Zhu%20and%20Ye%20Zhang%20and%20Trang%20Nguyen%20and%20Yih-Chung%20Tham%20and%20Ehsan%20Adeli%20and%20Ching-Yu%20Cheng%20and%20Yilun%20Du%20and%20Dianbo%20Liu&entry.1292438233=The%20rapid%20development%20of%20generative%20AI%20has%20transformed%20content%20creation%2C%20communication%2C%20and%20human%20development.%20However%2C%20this%20technology%20raises%20profound%20concerns%20in%20high-stakes%20domains%2C%20demanding%20rigorous%20methods%20to%20analyze%20and%20evaluate%20AI-generated%20content.%20While%20existing%20analytic%20methods%20often%20treat%20images%20as%20indivisible%20wholes%2C%20real-world%20AI%20failures%20generally%20manifest%20as%20specific%20visual%20patterns%20that%20can%20evade%20holistic%20detection%20and%20suit%20more%20granular%20and%20decomposed%20analysis.%20Here%20we%20introduce%20a%20content%20analysis%20tool%2C%20Language-Grounded%20Sparse%20Encoders%20%28LanSE%29%2C%20which%20decompose%20images%20into%20interpretable%20visual%20patterns%20with%20natural%20language%20descriptions.%20Utilizing%20interpretability%20modules%20and%20large%20multimodal%20models%2C%20LanSE%20can%20automatically%20identify%20visual%20patterns%20within%20data%20modalities.%20Our%20method%20discovers%20more%20than%205%2C000%20visual%20patterns%20with%2093%5C%25%20human%20agreement%2C%20provides%20decomposed%20evaluation%20outperforming%20existing%20methods%2C%20establishes%20the%20first%20systematic%20evaluation%20of%20physical%20plausibility%2C%20and%20extends%20to%20medical%20imaging%20settings.%20Our%20method%27s%20capability%20to%20extract%20language-grounded%20patterns%20can%20be%20naturally%20adapted%20to%20numerous%20fields%2C%20including%20biology%20and%20geography%2C%20as%20well%20as%20other%20data%20modalities%20such%20as%20protein%20structures%20and%20time%20series%2C%20thereby%20advancing%20content%20analysis%20for%20generative%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2508.18236v3&entry.124074799=Read"},
{"title": "Human Mesh Modeling for Anny Body", "author": "Romain Br\u00e9gier and Gu\u00e9nol\u00e9 Fiche and Laura Bravo-S\u00e1nchez and Thomas Lucas and Matthieu Armando and Philippe Weinzaepfel and Gr\u00e9gory Rogez and Fabien Baradel", "abstract": "Parametric body models provide the structural basis for many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms--across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling--supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic images generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.", "link": "http://arxiv.org/abs/2511.03589v2", "date": "2025-12-19", "relevancy": 2.9379, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6178}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5937}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Mesh%20Modeling%20for%20Anny%20Body&body=Title%3A%20Human%20Mesh%20Modeling%20for%20Anny%20Body%0AAuthor%3A%20Romain%20Br%C3%A9gier%20and%20Gu%C3%A9nol%C3%A9%20Fiche%20and%20Laura%20Bravo-S%C3%A1nchez%20and%20Thomas%20Lucas%20and%20Matthieu%20Armando%20and%20Philippe%20Weinzaepfel%20and%20Gr%C3%A9gory%20Rogez%20and%20Fabien%20Baradel%0AAbstract%3A%20Parametric%20body%20models%20provide%20the%20structural%20basis%20for%20many%20human-centric%20tasks%2C%20yet%20existing%20models%20often%20rely%20on%20costly%203D%20scans%20and%20learned%20shape%20spaces%20that%20are%20proprietary%20and%20demographically%20narrow.%20We%20introduce%20Anny%2C%20a%20simple%2C%20fully%20differentiable%2C%20and%20scan-free%20human%20body%20model%20grounded%20in%20anthropometric%20knowledge%20from%20the%20MakeHuman%20community.%20Anny%20defines%20a%20continuous%2C%20interpretable%20shape%20space%2C%20where%20phenotype%20parameters%20%28e.g.%20gender%2C%20age%2C%20height%2C%20weight%29%20control%20blendshapes%20spanning%20a%20wide%20range%20of%20human%20forms--across%20ages%20%28from%20infants%20to%20elders%29%2C%20body%20types%2C%20and%20proportions.%20Calibrated%20using%20WHO%20population%20statistics%2C%20it%20provides%20realistic%20and%20demographically%20grounded%20human%20shape%20variation%20within%20a%20single%20unified%20model.%20Thanks%20to%20its%20openness%20and%20semantic%20control%2C%20Anny%20serves%20as%20a%20versatile%20foundation%20for%203D%20human%20modeling--supporting%20millimeter-accurate%20scan%20fitting%2C%20controlled%20synthetic%20data%20generation%2C%20and%20Human%20Mesh%20Recovery%20%28HMR%29.%20We%20further%20introduce%20Anny-One%2C%20a%20collection%20of%20800k%20photorealistic%20images%20generated%20with%20Anny%2C%20showing%20that%20despite%20its%20simplicity%2C%20HMR%20models%20trained%20with%20Anny%20can%20match%20the%20performance%20of%20those%20trained%20with%20scan-based%20body%20models.%20The%20Anny%20body%20model%20and%20its%20code%20are%20released%20under%20the%20Apache%202.0%20license%2C%20making%20Anny%20an%20accessible%20foundation%20for%20human-centric%203D%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.03589v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Mesh%2520Modeling%2520for%2520Anny%2520Body%26entry.906535625%3DRomain%2520Br%25C3%25A9gier%2520and%2520Gu%25C3%25A9nol%25C3%25A9%2520Fiche%2520and%2520Laura%2520Bravo-S%25C3%25A1nchez%2520and%2520Thomas%2520Lucas%2520and%2520Matthieu%2520Armando%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Gr%25C3%25A9gory%2520Rogez%2520and%2520Fabien%2520Baradel%26entry.1292438233%3DParametric%2520body%2520models%2520provide%2520the%2520structural%2520basis%2520for%2520many%2520human-centric%2520tasks%252C%2520yet%2520existing%2520models%2520often%2520rely%2520on%2520costly%25203D%2520scans%2520and%2520learned%2520shape%2520spaces%2520that%2520are%2520proprietary%2520and%2520demographically%2520narrow.%2520We%2520introduce%2520Anny%252C%2520a%2520simple%252C%2520fully%2520differentiable%252C%2520and%2520scan-free%2520human%2520body%2520model%2520grounded%2520in%2520anthropometric%2520knowledge%2520from%2520the%2520MakeHuman%2520community.%2520Anny%2520defines%2520a%2520continuous%252C%2520interpretable%2520shape%2520space%252C%2520where%2520phenotype%2520parameters%2520%2528e.g.%2520gender%252C%2520age%252C%2520height%252C%2520weight%2529%2520control%2520blendshapes%2520spanning%2520a%2520wide%2520range%2520of%2520human%2520forms--across%2520ages%2520%2528from%2520infants%2520to%2520elders%2529%252C%2520body%2520types%252C%2520and%2520proportions.%2520Calibrated%2520using%2520WHO%2520population%2520statistics%252C%2520it%2520provides%2520realistic%2520and%2520demographically%2520grounded%2520human%2520shape%2520variation%2520within%2520a%2520single%2520unified%2520model.%2520Thanks%2520to%2520its%2520openness%2520and%2520semantic%2520control%252C%2520Anny%2520serves%2520as%2520a%2520versatile%2520foundation%2520for%25203D%2520human%2520modeling--supporting%2520millimeter-accurate%2520scan%2520fitting%252C%2520controlled%2520synthetic%2520data%2520generation%252C%2520and%2520Human%2520Mesh%2520Recovery%2520%2528HMR%2529.%2520We%2520further%2520introduce%2520Anny-One%252C%2520a%2520collection%2520of%2520800k%2520photorealistic%2520images%2520generated%2520with%2520Anny%252C%2520showing%2520that%2520despite%2520its%2520simplicity%252C%2520HMR%2520models%2520trained%2520with%2520Anny%2520can%2520match%2520the%2520performance%2520of%2520those%2520trained%2520with%2520scan-based%2520body%2520models.%2520The%2520Anny%2520body%2520model%2520and%2520its%2520code%2520are%2520released%2520under%2520the%2520Apache%25202.0%2520license%252C%2520making%2520Anny%2520an%2520accessible%2520foundation%2520for%2520human-centric%25203D%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03589v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Mesh%20Modeling%20for%20Anny%20Body&entry.906535625=Romain%20Br%C3%A9gier%20and%20Gu%C3%A9nol%C3%A9%20Fiche%20and%20Laura%20Bravo-S%C3%A1nchez%20and%20Thomas%20Lucas%20and%20Matthieu%20Armando%20and%20Philippe%20Weinzaepfel%20and%20Gr%C3%A9gory%20Rogez%20and%20Fabien%20Baradel&entry.1292438233=Parametric%20body%20models%20provide%20the%20structural%20basis%20for%20many%20human-centric%20tasks%2C%20yet%20existing%20models%20often%20rely%20on%20costly%203D%20scans%20and%20learned%20shape%20spaces%20that%20are%20proprietary%20and%20demographically%20narrow.%20We%20introduce%20Anny%2C%20a%20simple%2C%20fully%20differentiable%2C%20and%20scan-free%20human%20body%20model%20grounded%20in%20anthropometric%20knowledge%20from%20the%20MakeHuman%20community.%20Anny%20defines%20a%20continuous%2C%20interpretable%20shape%20space%2C%20where%20phenotype%20parameters%20%28e.g.%20gender%2C%20age%2C%20height%2C%20weight%29%20control%20blendshapes%20spanning%20a%20wide%20range%20of%20human%20forms--across%20ages%20%28from%20infants%20to%20elders%29%2C%20body%20types%2C%20and%20proportions.%20Calibrated%20using%20WHO%20population%20statistics%2C%20it%20provides%20realistic%20and%20demographically%20grounded%20human%20shape%20variation%20within%20a%20single%20unified%20model.%20Thanks%20to%20its%20openness%20and%20semantic%20control%2C%20Anny%20serves%20as%20a%20versatile%20foundation%20for%203D%20human%20modeling--supporting%20millimeter-accurate%20scan%20fitting%2C%20controlled%20synthetic%20data%20generation%2C%20and%20Human%20Mesh%20Recovery%20%28HMR%29.%20We%20further%20introduce%20Anny-One%2C%20a%20collection%20of%20800k%20photorealistic%20images%20generated%20with%20Anny%2C%20showing%20that%20despite%20its%20simplicity%2C%20HMR%20models%20trained%20with%20Anny%20can%20match%20the%20performance%20of%20those%20trained%20with%20scan-based%20body%20models.%20The%20Anny%20body%20model%20and%20its%20code%20are%20released%20under%20the%20Apache%202.0%20license%2C%20making%20Anny%20an%20accessible%20foundation%20for%20human-centric%203D%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2511.03589v2&entry.124074799=Read"},
{"title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras", "author": "Tomer Borreda and Fangqiang Ding and Sanja Fidler and Shengyu Huang and Or Litany", "abstract": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.", "link": "http://arxiv.org/abs/2512.17897v1", "date": "2025-12-19", "relevancy": 2.9295, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5877}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5877}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadarGen%3A%20Automotive%20Radar%20Point%20Cloud%20Generation%20from%20Cameras&body=Title%3A%20RadarGen%3A%20Automotive%20Radar%20Point%20Cloud%20Generation%20from%20Cameras%0AAuthor%3A%20Tomer%20Borreda%20and%20Fangqiang%20Ding%20and%20Sanja%20Fidler%20and%20Shengyu%20Huang%20and%20Or%20Litany%0AAbstract%3A%20We%20present%20RadarGen%2C%20a%20diffusion%20model%20for%20synthesizing%20realistic%20automotive%20radar%20point%20clouds%20from%20multi-view%20camera%20imagery.%20RadarGen%20adapts%20efficient%20image-latent%20diffusion%20to%20the%20radar%20domain%20by%20representing%20radar%20measurements%20in%20bird%27s-eye-view%20form%20that%20encodes%20spatial%20structure%20together%20with%20radar%20cross%20section%20%28RCS%29%20and%20Doppler%20attributes.%20A%20lightweight%20recovery%20step%20reconstructs%20point%20clouds%20from%20the%20generated%20maps.%20To%20better%20align%20generation%20with%20the%20visual%20scene%2C%20RadarGen%20incorporates%20BEV-aligned%20depth%2C%20semantic%2C%20and%20motion%20cues%20extracted%20from%20pretrained%20foundation%20models%2C%20which%20guide%20the%20stochastic%20generation%20process%20toward%20physically%20plausible%20radar%20patterns.%20Conditioning%20on%20images%20makes%20the%20approach%20broadly%20compatible%2C%20in%20principle%2C%20with%20existing%20visual%20datasets%20and%20simulation%20frameworks%2C%20offering%20a%20scalable%20direction%20for%20multimodal%20generative%20simulation.%20Evaluations%20on%20large-scale%20driving%20data%20show%20that%20RadarGen%20captures%20characteristic%20radar%20measurement%20distributions%20and%20reduces%20the%20gap%20to%20perception%20models%20trained%20on%20real%20data%2C%20marking%20a%20step%20toward%20unified%20generative%20simulation%20across%20sensing%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadarGen%253A%2520Automotive%2520Radar%2520Point%2520Cloud%2520Generation%2520from%2520Cameras%26entry.906535625%3DTomer%2520Borreda%2520and%2520Fangqiang%2520Ding%2520and%2520Sanja%2520Fidler%2520and%2520Shengyu%2520Huang%2520and%2520Or%2520Litany%26entry.1292438233%3DWe%2520present%2520RadarGen%252C%2520a%2520diffusion%2520model%2520for%2520synthesizing%2520realistic%2520automotive%2520radar%2520point%2520clouds%2520from%2520multi-view%2520camera%2520imagery.%2520RadarGen%2520adapts%2520efficient%2520image-latent%2520diffusion%2520to%2520the%2520radar%2520domain%2520by%2520representing%2520radar%2520measurements%2520in%2520bird%2527s-eye-view%2520form%2520that%2520encodes%2520spatial%2520structure%2520together%2520with%2520radar%2520cross%2520section%2520%2528RCS%2529%2520and%2520Doppler%2520attributes.%2520A%2520lightweight%2520recovery%2520step%2520reconstructs%2520point%2520clouds%2520from%2520the%2520generated%2520maps.%2520To%2520better%2520align%2520generation%2520with%2520the%2520visual%2520scene%252C%2520RadarGen%2520incorporates%2520BEV-aligned%2520depth%252C%2520semantic%252C%2520and%2520motion%2520cues%2520extracted%2520from%2520pretrained%2520foundation%2520models%252C%2520which%2520guide%2520the%2520stochastic%2520generation%2520process%2520toward%2520physically%2520plausible%2520radar%2520patterns.%2520Conditioning%2520on%2520images%2520makes%2520the%2520approach%2520broadly%2520compatible%252C%2520in%2520principle%252C%2520with%2520existing%2520visual%2520datasets%2520and%2520simulation%2520frameworks%252C%2520offering%2520a%2520scalable%2520direction%2520for%2520multimodal%2520generative%2520simulation.%2520Evaluations%2520on%2520large-scale%2520driving%2520data%2520show%2520that%2520RadarGen%2520captures%2520characteristic%2520radar%2520measurement%2520distributions%2520and%2520reduces%2520the%2520gap%2520to%2520perception%2520models%2520trained%2520on%2520real%2520data%252C%2520marking%2520a%2520step%2520toward%2520unified%2520generative%2520simulation%2520across%2520sensing%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadarGen%3A%20Automotive%20Radar%20Point%20Cloud%20Generation%20from%20Cameras&entry.906535625=Tomer%20Borreda%20and%20Fangqiang%20Ding%20and%20Sanja%20Fidler%20and%20Shengyu%20Huang%20and%20Or%20Litany&entry.1292438233=We%20present%20RadarGen%2C%20a%20diffusion%20model%20for%20synthesizing%20realistic%20automotive%20radar%20point%20clouds%20from%20multi-view%20camera%20imagery.%20RadarGen%20adapts%20efficient%20image-latent%20diffusion%20to%20the%20radar%20domain%20by%20representing%20radar%20measurements%20in%20bird%27s-eye-view%20form%20that%20encodes%20spatial%20structure%20together%20with%20radar%20cross%20section%20%28RCS%29%20and%20Doppler%20attributes.%20A%20lightweight%20recovery%20step%20reconstructs%20point%20clouds%20from%20the%20generated%20maps.%20To%20better%20align%20generation%20with%20the%20visual%20scene%2C%20RadarGen%20incorporates%20BEV-aligned%20depth%2C%20semantic%2C%20and%20motion%20cues%20extracted%20from%20pretrained%20foundation%20models%2C%20which%20guide%20the%20stochastic%20generation%20process%20toward%20physically%20plausible%20radar%20patterns.%20Conditioning%20on%20images%20makes%20the%20approach%20broadly%20compatible%2C%20in%20principle%2C%20with%20existing%20visual%20datasets%20and%20simulation%20frameworks%2C%20offering%20a%20scalable%20direction%20for%20multimodal%20generative%20simulation.%20Evaluations%20on%20large-scale%20driving%20data%20show%20that%20RadarGen%20captures%20characteristic%20radar%20measurement%20distributions%20and%20reduces%20the%20gap%20to%20perception%20models%20trained%20on%20real%20data%2C%20marking%20a%20step%20toward%20unified%20generative%20simulation%20across%20sensing%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2512.17897v1&entry.124074799=Read"},
{"title": "MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation", "author": "Jinda Du and Jieji Ren and Qiaojun Yu and Ningbin Zhang and Yu Deng and Xingyu Wei and Yufei Liu and Guoying Gu and Xiangyang Zhu", "abstract": "Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.", "link": "http://arxiv.org/abs/2512.00324v2", "date": "2025-12-19", "relevancy": 2.8319, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5806}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5603}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MILE%3A%20A%20Mechanically%20Isomorphic%20Exoskeleton%20Data%20Collection%20System%20with%20Fingertip%20Visuotactile%20Sensing%20for%20Dexterous%20Manipulation&body=Title%3A%20MILE%3A%20A%20Mechanically%20Isomorphic%20Exoskeleton%20Data%20Collection%20System%20with%20Fingertip%20Visuotactile%20Sensing%20for%20Dexterous%20Manipulation%0AAuthor%3A%20Jinda%20Du%20and%20Jieji%20Ren%20and%20Qiaojun%20Yu%20and%20Ningbin%20Zhang%20and%20Yu%20Deng%20and%20Xingyu%20Wei%20and%20Yufei%20Liu%20and%20Guoying%20Gu%20and%20Xiangyang%20Zhu%0AAbstract%3A%20Imitation%20learning%20provides%20a%20promising%20approach%20to%20dexterous%20hand%20manipulation%2C%20but%20its%20effectiveness%20is%20limited%20by%20the%20lack%20of%20large-scale%2C%20high-fidelity%20data.%20Existing%20data-collection%20pipelines%20suffer%20from%20inaccurate%20motion%20retargeting%2C%20low%20data-collection%20efficiency%2C%20and%20missing%20high-resolution%20fingertip%20tactile%20sensing.%20We%20address%20this%20gap%20with%20MILE%2C%20a%20mechanically%20isomorphic%20teleoperation%20and%20data-collection%20system%20co-designed%20from%20human%20hand%20to%20exoskeleton%20to%20robotic%20hand.%20The%20exoskeleton%20is%20anthropometrically%20derived%20from%20the%20human%20hand%2C%20and%20the%20robotic%20hand%20preserves%20one-to-one%20joint-position%20isomorphism%2C%20eliminating%20nonlinear%20retargeting%20and%20enabling%20precise%2C%20natural%20control.%20The%20exoskeleton%20achieves%20a%20multi-joint%20mean%20absolute%20angular%20error%20below%20one%20degree%2C%20while%20the%20robotic%20hand%20integrates%20compact%20fingertip%20visuotactile%20modules%20that%20provide%20high-resolution%20tactile%20observations.%20Built%20on%20this%20retargeting-free%20interface%2C%20we%20teleoperate%20complex%2C%20contact-rich%20in-hand%20manipulation%20and%20efficiently%20collect%20a%20multimodal%20dataset%20comprising%20high-resolution%20fingertip%20visuotactile%20signals%2C%20RGB-D%20images%2C%20and%20joint%20positions.%20The%20teleoperation%20pipeline%20achieves%20a%20mean%20success%20rate%20improvement%20of%2064%25.%20Incorporating%20fingertip%20tactile%20observations%20further%20increases%20the%20success%20rate%20by%20an%20average%20of%2025%25%20over%20the%20vision-only%20baseline%2C%20validating%20the%20fidelity%20and%20utility%20of%20the%20dataset.%20Further%20details%20are%20available%20at%3A%20https%3A//sites.google.com/view/mile-system.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMILE%253A%2520A%2520Mechanically%2520Isomorphic%2520Exoskeleton%2520Data%2520Collection%2520System%2520with%2520Fingertip%2520Visuotactile%2520Sensing%2520for%2520Dexterous%2520Manipulation%26entry.906535625%3DJinda%2520Du%2520and%2520Jieji%2520Ren%2520and%2520Qiaojun%2520Yu%2520and%2520Ningbin%2520Zhang%2520and%2520Yu%2520Deng%2520and%2520Xingyu%2520Wei%2520and%2520Yufei%2520Liu%2520and%2520Guoying%2520Gu%2520and%2520Xiangyang%2520Zhu%26entry.1292438233%3DImitation%2520learning%2520provides%2520a%2520promising%2520approach%2520to%2520dexterous%2520hand%2520manipulation%252C%2520but%2520its%2520effectiveness%2520is%2520limited%2520by%2520the%2520lack%2520of%2520large-scale%252C%2520high-fidelity%2520data.%2520Existing%2520data-collection%2520pipelines%2520suffer%2520from%2520inaccurate%2520motion%2520retargeting%252C%2520low%2520data-collection%2520efficiency%252C%2520and%2520missing%2520high-resolution%2520fingertip%2520tactile%2520sensing.%2520We%2520address%2520this%2520gap%2520with%2520MILE%252C%2520a%2520mechanically%2520isomorphic%2520teleoperation%2520and%2520data-collection%2520system%2520co-designed%2520from%2520human%2520hand%2520to%2520exoskeleton%2520to%2520robotic%2520hand.%2520The%2520exoskeleton%2520is%2520anthropometrically%2520derived%2520from%2520the%2520human%2520hand%252C%2520and%2520the%2520robotic%2520hand%2520preserves%2520one-to-one%2520joint-position%2520isomorphism%252C%2520eliminating%2520nonlinear%2520retargeting%2520and%2520enabling%2520precise%252C%2520natural%2520control.%2520The%2520exoskeleton%2520achieves%2520a%2520multi-joint%2520mean%2520absolute%2520angular%2520error%2520below%2520one%2520degree%252C%2520while%2520the%2520robotic%2520hand%2520integrates%2520compact%2520fingertip%2520visuotactile%2520modules%2520that%2520provide%2520high-resolution%2520tactile%2520observations.%2520Built%2520on%2520this%2520retargeting-free%2520interface%252C%2520we%2520teleoperate%2520complex%252C%2520contact-rich%2520in-hand%2520manipulation%2520and%2520efficiently%2520collect%2520a%2520multimodal%2520dataset%2520comprising%2520high-resolution%2520fingertip%2520visuotactile%2520signals%252C%2520RGB-D%2520images%252C%2520and%2520joint%2520positions.%2520The%2520teleoperation%2520pipeline%2520achieves%2520a%2520mean%2520success%2520rate%2520improvement%2520of%252064%2525.%2520Incorporating%2520fingertip%2520tactile%2520observations%2520further%2520increases%2520the%2520success%2520rate%2520by%2520an%2520average%2520of%252025%2525%2520over%2520the%2520vision-only%2520baseline%252C%2520validating%2520the%2520fidelity%2520and%2520utility%2520of%2520the%2520dataset.%2520Further%2520details%2520are%2520available%2520at%253A%2520https%253A//sites.google.com/view/mile-system.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MILE%3A%20A%20Mechanically%20Isomorphic%20Exoskeleton%20Data%20Collection%20System%20with%20Fingertip%20Visuotactile%20Sensing%20for%20Dexterous%20Manipulation&entry.906535625=Jinda%20Du%20and%20Jieji%20Ren%20and%20Qiaojun%20Yu%20and%20Ningbin%20Zhang%20and%20Yu%20Deng%20and%20Xingyu%20Wei%20and%20Yufei%20Liu%20and%20Guoying%20Gu%20and%20Xiangyang%20Zhu&entry.1292438233=Imitation%20learning%20provides%20a%20promising%20approach%20to%20dexterous%20hand%20manipulation%2C%20but%20its%20effectiveness%20is%20limited%20by%20the%20lack%20of%20large-scale%2C%20high-fidelity%20data.%20Existing%20data-collection%20pipelines%20suffer%20from%20inaccurate%20motion%20retargeting%2C%20low%20data-collection%20efficiency%2C%20and%20missing%20high-resolution%20fingertip%20tactile%20sensing.%20We%20address%20this%20gap%20with%20MILE%2C%20a%20mechanically%20isomorphic%20teleoperation%20and%20data-collection%20system%20co-designed%20from%20human%20hand%20to%20exoskeleton%20to%20robotic%20hand.%20The%20exoskeleton%20is%20anthropometrically%20derived%20from%20the%20human%20hand%2C%20and%20the%20robotic%20hand%20preserves%20one-to-one%20joint-position%20isomorphism%2C%20eliminating%20nonlinear%20retargeting%20and%20enabling%20precise%2C%20natural%20control.%20The%20exoskeleton%20achieves%20a%20multi-joint%20mean%20absolute%20angular%20error%20below%20one%20degree%2C%20while%20the%20robotic%20hand%20integrates%20compact%20fingertip%20visuotactile%20modules%20that%20provide%20high-resolution%20tactile%20observations.%20Built%20on%20this%20retargeting-free%20interface%2C%20we%20teleoperate%20complex%2C%20contact-rich%20in-hand%20manipulation%20and%20efficiently%20collect%20a%20multimodal%20dataset%20comprising%20high-resolution%20fingertip%20visuotactile%20signals%2C%20RGB-D%20images%2C%20and%20joint%20positions.%20The%20teleoperation%20pipeline%20achieves%20a%20mean%20success%20rate%20improvement%20of%2064%25.%20Incorporating%20fingertip%20tactile%20observations%20further%20increases%20the%20success%20rate%20by%20an%20average%20of%2025%25%20over%20the%20vision-only%20baseline%2C%20validating%20the%20fidelity%20and%20utility%20of%20the%20dataset.%20Further%20details%20are%20available%20at%3A%20https%3A//sites.google.com/view/mile-system.&entry.1838667208=http%3A//arxiv.org/abs/2512.00324v2&entry.124074799=Read"},
{"title": "LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence", "author": "Yohanes Yudhi Adikusuma and Qixing Huang and Ying He", "abstract": "Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.", "link": "http://arxiv.org/abs/2512.17781v1", "date": "2025-12-19", "relevancy": 2.8266, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5827}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5788}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteGE%3A%20Lightweight%20Geodesic%20Embedding%20for%20Efficient%20Geodesics%20Computation%20and%20Non-Isometric%20Shape%20Correspondence&body=Title%3A%20LiteGE%3A%20Lightweight%20Geodesic%20Embedding%20for%20Efficient%20Geodesics%20Computation%20and%20Non-Isometric%20Shape%20Correspondence%0AAuthor%3A%20Yohanes%20Yudhi%20Adikusuma%20and%20Qixing%20Huang%20and%20Ying%20He%0AAbstract%3A%20Computing%20geodesic%20distances%20on%203D%20surfaces%20is%20fundamental%20to%20many%20tasks%20in%203D%20vision%20and%20geometry%20processing%2C%20with%20deep%20connections%20to%20tasks%20such%20as%20shape%20correspondence.%20Recent%20learning-based%20methods%20achieve%20strong%20performance%20but%20rely%20on%20large%203D%20backbones%2C%20leading%20to%20high%20memory%20usage%20and%20latency%2C%20which%20limit%20their%20use%20in%20interactive%20or%20resource-constrained%20settings.%20We%20introduce%20LiteGE%2C%20a%20lightweight%20approach%20that%20constructs%20compact%2C%20category-aware%20shape%20descriptors%20by%20applying%20PCA%20to%20unsigned%20distance%20field%20%28UDFs%29%20samples%20at%20informative%20voxels.%20This%20descriptor%20is%20efficient%20to%20compute%20and%20removes%20the%20need%20for%20high-capacity%20networks.%20LiteGE%20remains%20robust%20on%20sparse%20point%20clouds%2C%20supporting%20inputs%20with%20as%20few%20as%20300%20points%2C%20where%20prior%20methods%20fail.%20Extensive%20experiments%20show%20that%20LiteGE%20reduces%20memory%20usage%20and%20inference%20time%20by%20up%20to%20300%24%5Ctimes%24%20compared%20to%20existing%20neural%20approaches.%20In%20addition%2C%20by%20exploiting%20the%20intrinsic%20relationship%20between%20geodesic%20distance%20and%20shape%20correspondence%2C%20LiteGE%20enables%20fast%20and%20accurate%20shape%20matching.%20Our%20method%20achieves%20up%20to%201000%24%5Ctimes%24%20speedup%20over%20state-of-the-art%20mesh-based%20approaches%20while%20maintaining%20comparable%20accuracy%20on%20non-isometric%20shape%20pairs%2C%20including%20evaluations%20on%20point-cloud%20inputs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteGE%253A%2520Lightweight%2520Geodesic%2520Embedding%2520for%2520Efficient%2520Geodesics%2520Computation%2520and%2520Non-Isometric%2520Shape%2520Correspondence%26entry.906535625%3DYohanes%2520Yudhi%2520Adikusuma%2520and%2520Qixing%2520Huang%2520and%2520Ying%2520He%26entry.1292438233%3DComputing%2520geodesic%2520distances%2520on%25203D%2520surfaces%2520is%2520fundamental%2520to%2520many%2520tasks%2520in%25203D%2520vision%2520and%2520geometry%2520processing%252C%2520with%2520deep%2520connections%2520to%2520tasks%2520such%2520as%2520shape%2520correspondence.%2520Recent%2520learning-based%2520methods%2520achieve%2520strong%2520performance%2520but%2520rely%2520on%2520large%25203D%2520backbones%252C%2520leading%2520to%2520high%2520memory%2520usage%2520and%2520latency%252C%2520which%2520limit%2520their%2520use%2520in%2520interactive%2520or%2520resource-constrained%2520settings.%2520We%2520introduce%2520LiteGE%252C%2520a%2520lightweight%2520approach%2520that%2520constructs%2520compact%252C%2520category-aware%2520shape%2520descriptors%2520by%2520applying%2520PCA%2520to%2520unsigned%2520distance%2520field%2520%2528UDFs%2529%2520samples%2520at%2520informative%2520voxels.%2520This%2520descriptor%2520is%2520efficient%2520to%2520compute%2520and%2520removes%2520the%2520need%2520for%2520high-capacity%2520networks.%2520LiteGE%2520remains%2520robust%2520on%2520sparse%2520point%2520clouds%252C%2520supporting%2520inputs%2520with%2520as%2520few%2520as%2520300%2520points%252C%2520where%2520prior%2520methods%2520fail.%2520Extensive%2520experiments%2520show%2520that%2520LiteGE%2520reduces%2520memory%2520usage%2520and%2520inference%2520time%2520by%2520up%2520to%2520300%2524%255Ctimes%2524%2520compared%2520to%2520existing%2520neural%2520approaches.%2520In%2520addition%252C%2520by%2520exploiting%2520the%2520intrinsic%2520relationship%2520between%2520geodesic%2520distance%2520and%2520shape%2520correspondence%252C%2520LiteGE%2520enables%2520fast%2520and%2520accurate%2520shape%2520matching.%2520Our%2520method%2520achieves%2520up%2520to%25201000%2524%255Ctimes%2524%2520speedup%2520over%2520state-of-the-art%2520mesh-based%2520approaches%2520while%2520maintaining%2520comparable%2520accuracy%2520on%2520non-isometric%2520shape%2520pairs%252C%2520including%2520evaluations%2520on%2520point-cloud%2520inputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteGE%3A%20Lightweight%20Geodesic%20Embedding%20for%20Efficient%20Geodesics%20Computation%20and%20Non-Isometric%20Shape%20Correspondence&entry.906535625=Yohanes%20Yudhi%20Adikusuma%20and%20Qixing%20Huang%20and%20Ying%20He&entry.1292438233=Computing%20geodesic%20distances%20on%203D%20surfaces%20is%20fundamental%20to%20many%20tasks%20in%203D%20vision%20and%20geometry%20processing%2C%20with%20deep%20connections%20to%20tasks%20such%20as%20shape%20correspondence.%20Recent%20learning-based%20methods%20achieve%20strong%20performance%20but%20rely%20on%20large%203D%20backbones%2C%20leading%20to%20high%20memory%20usage%20and%20latency%2C%20which%20limit%20their%20use%20in%20interactive%20or%20resource-constrained%20settings.%20We%20introduce%20LiteGE%2C%20a%20lightweight%20approach%20that%20constructs%20compact%2C%20category-aware%20shape%20descriptors%20by%20applying%20PCA%20to%20unsigned%20distance%20field%20%28UDFs%29%20samples%20at%20informative%20voxels.%20This%20descriptor%20is%20efficient%20to%20compute%20and%20removes%20the%20need%20for%20high-capacity%20networks.%20LiteGE%20remains%20robust%20on%20sparse%20point%20clouds%2C%20supporting%20inputs%20with%20as%20few%20as%20300%20points%2C%20where%20prior%20methods%20fail.%20Extensive%20experiments%20show%20that%20LiteGE%20reduces%20memory%20usage%20and%20inference%20time%20by%20up%20to%20300%24%5Ctimes%24%20compared%20to%20existing%20neural%20approaches.%20In%20addition%2C%20by%20exploiting%20the%20intrinsic%20relationship%20between%20geodesic%20distance%20and%20shape%20correspondence%2C%20LiteGE%20enables%20fast%20and%20accurate%20shape%20matching.%20Our%20method%20achieves%20up%20to%201000%24%5Ctimes%24%20speedup%20over%20state-of-the-art%20mesh-based%20approaches%20while%20maintaining%20comparable%20accuracy%20on%20non-isometric%20shape%20pairs%2C%20including%20evaluations%20on%20point-cloud%20inputs.&entry.1838667208=http%3A//arxiv.org/abs/2512.17781v1&entry.124074799=Read"},
{"title": "On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness", "author": "Pablo Hern\u00e1ndez-C\u00e1mara and Jose Manuel Ja\u00e9n-Lorites and Alexandra G\u00f3mez-Villa and Jorge Vila-Tom\u00e1s and Valero Laparra and Jesus Malo", "abstract": "Contrastive language-image models such as CLIP have demonstrated remarkable generalization capabilities. However, how their internal visual representations evolve during training and how this evolution relates to human perception remains poorly understood. Most existing analysis characterize fully trained models, leaving the dynamics of representational biases and perceptual alignment largely unexplored. In this work, we present an epoch-by-epoch analysis of CLIP models throughout training, focusing on the evolution of texture-shape bias, alignment with human perceptual judgements, and sensitivity to image noise. Using multiple perceptual benchmarks spanning low-level image quality assessment, mid-level perceptual similarity, saliency correspondence, and noisy robustness, we identify a consistent, training-stage-dependent representational transition. Early training stages exhibit strong texture bias, elevated alignment with low-level human perceptual measures, and increased sensitivity to Gaussian noise perturbations. As training progresses, this texture bias gradually diminishes in favor of more shape-based representations, coinciding with improved robustness to noise and a decline in low-level perceptual alignment. Importantly, these dynamics are consistently observed across multiple CLIP model scales, indicating that the phenomenon is not specific to a particular architecture size. Our findings provide an empirical characterization of how perceptual alignment, feature bias, and robustness co-evolve during multimodal model training. This work reveals a systematic trade-off between early low-level perceptual alignment and later robustness, offering new insights into the representational dynamics of vision-language models and their relationship to human visual processing.", "link": "http://arxiv.org/abs/2508.09814v2", "date": "2025-12-19", "relevancy": 2.8179, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20dynamic%20evolution%20of%20CLIP%20texture-shape%20bias%20and%20its%20relationship%20to%20human%20alignment%20and%20model%20robustness&body=Title%3A%20On%20the%20dynamic%20evolution%20of%20CLIP%20texture-shape%20bias%20and%20its%20relationship%20to%20human%20alignment%20and%20model%20robustness%0AAuthor%3A%20Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Jose%20Manuel%20Ja%C3%A9n-Lorites%20and%20Alexandra%20G%C3%B3mez-Villa%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Valero%20Laparra%20and%20Jesus%20Malo%0AAbstract%3A%20Contrastive%20language-image%20models%20such%20as%20CLIP%20have%20demonstrated%20remarkable%20generalization%20capabilities.%20However%2C%20how%20their%20internal%20visual%20representations%20evolve%20during%20training%20and%20how%20this%20evolution%20relates%20to%20human%20perception%20remains%20poorly%20understood.%20Most%20existing%20analysis%20characterize%20fully%20trained%20models%2C%20leaving%20the%20dynamics%20of%20representational%20biases%20and%20perceptual%20alignment%20largely%20unexplored.%20In%20this%20work%2C%20we%20present%20an%20epoch-by-epoch%20analysis%20of%20CLIP%20models%20throughout%20training%2C%20focusing%20on%20the%20evolution%20of%20texture-shape%20bias%2C%20alignment%20with%20human%20perceptual%20judgements%2C%20and%20sensitivity%20to%20image%20noise.%20Using%20multiple%20perceptual%20benchmarks%20spanning%20low-level%20image%20quality%20assessment%2C%20mid-level%20perceptual%20similarity%2C%20saliency%20correspondence%2C%20and%20noisy%20robustness%2C%20we%20identify%20a%20consistent%2C%20training-stage-dependent%20representational%20transition.%20Early%20training%20stages%20exhibit%20strong%20texture%20bias%2C%20elevated%20alignment%20with%20low-level%20human%20perceptual%20measures%2C%20and%20increased%20sensitivity%20to%20Gaussian%20noise%20perturbations.%20As%20training%20progresses%2C%20this%20texture%20bias%20gradually%20diminishes%20in%20favor%20of%20more%20shape-based%20representations%2C%20coinciding%20with%20improved%20robustness%20to%20noise%20and%20a%20decline%20in%20low-level%20perceptual%20alignment.%20Importantly%2C%20these%20dynamics%20are%20consistently%20observed%20across%20multiple%20CLIP%20model%20scales%2C%20indicating%20that%20the%20phenomenon%20is%20not%20specific%20to%20a%20particular%20architecture%20size.%20Our%20findings%20provide%20an%20empirical%20characterization%20of%20how%20perceptual%20alignment%2C%20feature%20bias%2C%20and%20robustness%20co-evolve%20during%20multimodal%20model%20training.%20This%20work%20reveals%20a%20systematic%20trade-off%20between%20early%20low-level%20perceptual%20alignment%20and%20later%20robustness%2C%20offering%20new%20insights%20into%20the%20representational%20dynamics%20of%20vision-language%20models%20and%20their%20relationship%20to%20human%20visual%20processing.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520dynamic%2520evolution%2520of%2520CLIP%2520texture-shape%2520bias%2520and%2520its%2520relationship%2520to%2520human%2520alignment%2520and%2520model%2520robustness%26entry.906535625%3DPablo%2520Hern%25C3%25A1ndez-C%25C3%25A1mara%2520and%2520Jose%2520Manuel%2520Ja%25C3%25A9n-Lorites%2520and%2520Alexandra%2520G%25C3%25B3mez-Villa%2520and%2520Jorge%2520Vila-Tom%25C3%25A1s%2520and%2520Valero%2520Laparra%2520and%2520Jesus%2520Malo%26entry.1292438233%3DContrastive%2520language-image%2520models%2520such%2520as%2520CLIP%2520have%2520demonstrated%2520remarkable%2520generalization%2520capabilities.%2520However%252C%2520how%2520their%2520internal%2520visual%2520representations%2520evolve%2520during%2520training%2520and%2520how%2520this%2520evolution%2520relates%2520to%2520human%2520perception%2520remains%2520poorly%2520understood.%2520Most%2520existing%2520analysis%2520characterize%2520fully%2520trained%2520models%252C%2520leaving%2520the%2520dynamics%2520of%2520representational%2520biases%2520and%2520perceptual%2520alignment%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520epoch-by-epoch%2520analysis%2520of%2520CLIP%2520models%2520throughout%2520training%252C%2520focusing%2520on%2520the%2520evolution%2520of%2520texture-shape%2520bias%252C%2520alignment%2520with%2520human%2520perceptual%2520judgements%252C%2520and%2520sensitivity%2520to%2520image%2520noise.%2520Using%2520multiple%2520perceptual%2520benchmarks%2520spanning%2520low-level%2520image%2520quality%2520assessment%252C%2520mid-level%2520perceptual%2520similarity%252C%2520saliency%2520correspondence%252C%2520and%2520noisy%2520robustness%252C%2520we%2520identify%2520a%2520consistent%252C%2520training-stage-dependent%2520representational%2520transition.%2520Early%2520training%2520stages%2520exhibit%2520strong%2520texture%2520bias%252C%2520elevated%2520alignment%2520with%2520low-level%2520human%2520perceptual%2520measures%252C%2520and%2520increased%2520sensitivity%2520to%2520Gaussian%2520noise%2520perturbations.%2520As%2520training%2520progresses%252C%2520this%2520texture%2520bias%2520gradually%2520diminishes%2520in%2520favor%2520of%2520more%2520shape-based%2520representations%252C%2520coinciding%2520with%2520improved%2520robustness%2520to%2520noise%2520and%2520a%2520decline%2520in%2520low-level%2520perceptual%2520alignment.%2520Importantly%252C%2520these%2520dynamics%2520are%2520consistently%2520observed%2520across%2520multiple%2520CLIP%2520model%2520scales%252C%2520indicating%2520that%2520the%2520phenomenon%2520is%2520not%2520specific%2520to%2520a%2520particular%2520architecture%2520size.%2520Our%2520findings%2520provide%2520an%2520empirical%2520characterization%2520of%2520how%2520perceptual%2520alignment%252C%2520feature%2520bias%252C%2520and%2520robustness%2520co-evolve%2520during%2520multimodal%2520model%2520training.%2520This%2520work%2520reveals%2520a%2520systematic%2520trade-off%2520between%2520early%2520low-level%2520perceptual%2520alignment%2520and%2520later%2520robustness%252C%2520offering%2520new%2520insights%2520into%2520the%2520representational%2520dynamics%2520of%2520vision-language%2520models%2520and%2520their%2520relationship%2520to%2520human%2520visual%2520processing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20dynamic%20evolution%20of%20CLIP%20texture-shape%20bias%20and%20its%20relationship%20to%20human%20alignment%20and%20model%20robustness&entry.906535625=Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Jose%20Manuel%20Ja%C3%A9n-Lorites%20and%20Alexandra%20G%C3%B3mez-Villa%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Valero%20Laparra%20and%20Jesus%20Malo&entry.1292438233=Contrastive%20language-image%20models%20such%20as%20CLIP%20have%20demonstrated%20remarkable%20generalization%20capabilities.%20However%2C%20how%20their%20internal%20visual%20representations%20evolve%20during%20training%20and%20how%20this%20evolution%20relates%20to%20human%20perception%20remains%20poorly%20understood.%20Most%20existing%20analysis%20characterize%20fully%20trained%20models%2C%20leaving%20the%20dynamics%20of%20representational%20biases%20and%20perceptual%20alignment%20largely%20unexplored.%20In%20this%20work%2C%20we%20present%20an%20epoch-by-epoch%20analysis%20of%20CLIP%20models%20throughout%20training%2C%20focusing%20on%20the%20evolution%20of%20texture-shape%20bias%2C%20alignment%20with%20human%20perceptual%20judgements%2C%20and%20sensitivity%20to%20image%20noise.%20Using%20multiple%20perceptual%20benchmarks%20spanning%20low-level%20image%20quality%20assessment%2C%20mid-level%20perceptual%20similarity%2C%20saliency%20correspondence%2C%20and%20noisy%20robustness%2C%20we%20identify%20a%20consistent%2C%20training-stage-dependent%20representational%20transition.%20Early%20training%20stages%20exhibit%20strong%20texture%20bias%2C%20elevated%20alignment%20with%20low-level%20human%20perceptual%20measures%2C%20and%20increased%20sensitivity%20to%20Gaussian%20noise%20perturbations.%20As%20training%20progresses%2C%20this%20texture%20bias%20gradually%20diminishes%20in%20favor%20of%20more%20shape-based%20representations%2C%20coinciding%20with%20improved%20robustness%20to%20noise%20and%20a%20decline%20in%20low-level%20perceptual%20alignment.%20Importantly%2C%20these%20dynamics%20are%20consistently%20observed%20across%20multiple%20CLIP%20model%20scales%2C%20indicating%20that%20the%20phenomenon%20is%20not%20specific%20to%20a%20particular%20architecture%20size.%20Our%20findings%20provide%20an%20empirical%20characterization%20of%20how%20perceptual%20alignment%2C%20feature%20bias%2C%20and%20robustness%20co-evolve%20during%20multimodal%20model%20training.%20This%20work%20reveals%20a%20systematic%20trade-off%20between%20early%20low-level%20perceptual%20alignment%20and%20later%20robustness%2C%20offering%20new%20insights%20into%20the%20representational%20dynamics%20of%20vision-language%20models%20and%20their%20relationship%20to%20human%20visual%20processing.&entry.1838667208=http%3A//arxiv.org/abs/2508.09814v2&entry.124074799=Read"},
{"title": "Visually Prompted Benchmarks Are Surprisingly Fragile", "author": "Haiwen Feng and Long Lian and Lisa Dunlap and Jiahao Shu and XuDong Wang and Renhao Wang and Trevor Darrell and Alane Suhr and Angjoo Kanazawa", "abstract": "A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.", "link": "http://arxiv.org/abs/2512.17875v1", "date": "2025-12-19", "relevancy": 2.7805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visually%20Prompted%20Benchmarks%20Are%20Surprisingly%20Fragile&body=Title%3A%20Visually%20Prompted%20Benchmarks%20Are%20Surprisingly%20Fragile%0AAuthor%3A%20Haiwen%20Feng%20and%20Long%20Lian%20and%20Lisa%20Dunlap%20and%20Jiahao%20Shu%20and%20XuDong%20Wang%20and%20Renhao%20Wang%20and%20Trevor%20Darrell%20and%20Alane%20Suhr%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20A%20key%20challenge%20in%20evaluating%20VLMs%20is%20testing%20models%27%20ability%20to%20analyze%20visual%20content%20independently%20from%20their%20textual%20priors.%20Recent%20benchmarks%20such%20as%20BLINK%20probe%20visual%20perception%20through%20visual%20prompting%2C%20where%20questions%20about%20visual%20content%20are%20paired%20with%20coordinates%20to%20which%20the%20question%20refers%2C%20with%20the%20coordinates%20explicitly%20marked%20in%20the%20image%20itself.%20While%20these%20benchmarks%20are%20an%20important%20part%20of%20VLM%20evaluation%2C%20we%20find%20that%20existing%20models%20are%20surprisingly%20fragile%20to%20seemingly%20irrelevant%20details%20of%20visual%20prompting%3A%20simply%20changing%20a%20visual%20marker%20from%20red%20to%20blue%20can%20completely%20change%20rankings%20among%20models%20on%20a%20leaderboard.%20By%20evaluating%20nine%20commonly-used%20open-%20and%20closed-source%20VLMs%20on%20two%20visually%20prompted%20tasks%2C%20we%20demonstrate%20how%20details%20in%20benchmark%20setup%2C%20including%20visual%20marker%20design%20and%20dataset%20size%2C%20have%20a%20significant%20influence%20on%20model%20performance%20and%20leaderboard%20rankings.%20These%20effects%20can%20even%20be%20exploited%20to%20lift%20weaker%20models%20above%20stronger%20ones%3B%20for%20instance%2C%20slightly%20increasing%20the%20size%20of%20the%20visual%20marker%20results%20in%20open-source%20InternVL3-8B%20ranking%20alongside%20or%20better%20than%20much%20larger%20proprietary%20models%20like%20Gemini%202.5%20Pro.%20We%20further%20show%20that%20low-level%20inference%20choices%20that%20are%20often%20ignored%20in%20benchmarking%2C%20such%20as%20JPEG%20compression%20levels%20in%20API%20calls%2C%20can%20also%20cause%20model%20lineup%20changes.%20These%20details%20have%20substantially%20larger%20impacts%20on%20visually%20prompted%20benchmarks%20than%20on%20conventional%20semantic%20VLM%20evaluations.%20To%20mitigate%20this%20instability%2C%20we%20curate%20existing%20datasets%20to%20create%20VPBench%2C%20a%20larger%20visually%20prompted%20benchmark%20with%2016%20visual%20marker%20variants.%20VPBench%20and%20additional%20analysis%20tools%20are%20released%20at%20https%3A//lisadunlap.github.io/vpbench/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisually%2520Prompted%2520Benchmarks%2520Are%2520Surprisingly%2520Fragile%26entry.906535625%3DHaiwen%2520Feng%2520and%2520Long%2520Lian%2520and%2520Lisa%2520Dunlap%2520and%2520Jiahao%2520Shu%2520and%2520XuDong%2520Wang%2520and%2520Renhao%2520Wang%2520and%2520Trevor%2520Darrell%2520and%2520Alane%2520Suhr%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3DA%2520key%2520challenge%2520in%2520evaluating%2520VLMs%2520is%2520testing%2520models%2527%2520ability%2520to%2520analyze%2520visual%2520content%2520independently%2520from%2520their%2520textual%2520priors.%2520Recent%2520benchmarks%2520such%2520as%2520BLINK%2520probe%2520visual%2520perception%2520through%2520visual%2520prompting%252C%2520where%2520questions%2520about%2520visual%2520content%2520are%2520paired%2520with%2520coordinates%2520to%2520which%2520the%2520question%2520refers%252C%2520with%2520the%2520coordinates%2520explicitly%2520marked%2520in%2520the%2520image%2520itself.%2520While%2520these%2520benchmarks%2520are%2520an%2520important%2520part%2520of%2520VLM%2520evaluation%252C%2520we%2520find%2520that%2520existing%2520models%2520are%2520surprisingly%2520fragile%2520to%2520seemingly%2520irrelevant%2520details%2520of%2520visual%2520prompting%253A%2520simply%2520changing%2520a%2520visual%2520marker%2520from%2520red%2520to%2520blue%2520can%2520completely%2520change%2520rankings%2520among%2520models%2520on%2520a%2520leaderboard.%2520By%2520evaluating%2520nine%2520commonly-used%2520open-%2520and%2520closed-source%2520VLMs%2520on%2520two%2520visually%2520prompted%2520tasks%252C%2520we%2520demonstrate%2520how%2520details%2520in%2520benchmark%2520setup%252C%2520including%2520visual%2520marker%2520design%2520and%2520dataset%2520size%252C%2520have%2520a%2520significant%2520influence%2520on%2520model%2520performance%2520and%2520leaderboard%2520rankings.%2520These%2520effects%2520can%2520even%2520be%2520exploited%2520to%2520lift%2520weaker%2520models%2520above%2520stronger%2520ones%253B%2520for%2520instance%252C%2520slightly%2520increasing%2520the%2520size%2520of%2520the%2520visual%2520marker%2520results%2520in%2520open-source%2520InternVL3-8B%2520ranking%2520alongside%2520or%2520better%2520than%2520much%2520larger%2520proprietary%2520models%2520like%2520Gemini%25202.5%2520Pro.%2520We%2520further%2520show%2520that%2520low-level%2520inference%2520choices%2520that%2520are%2520often%2520ignored%2520in%2520benchmarking%252C%2520such%2520as%2520JPEG%2520compression%2520levels%2520in%2520API%2520calls%252C%2520can%2520also%2520cause%2520model%2520lineup%2520changes.%2520These%2520details%2520have%2520substantially%2520larger%2520impacts%2520on%2520visually%2520prompted%2520benchmarks%2520than%2520on%2520conventional%2520semantic%2520VLM%2520evaluations.%2520To%2520mitigate%2520this%2520instability%252C%2520we%2520curate%2520existing%2520datasets%2520to%2520create%2520VPBench%252C%2520a%2520larger%2520visually%2520prompted%2520benchmark%2520with%252016%2520visual%2520marker%2520variants.%2520VPBench%2520and%2520additional%2520analysis%2520tools%2520are%2520released%2520at%2520https%253A//lisadunlap.github.io/vpbench/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visually%20Prompted%20Benchmarks%20Are%20Surprisingly%20Fragile&entry.906535625=Haiwen%20Feng%20and%20Long%20Lian%20and%20Lisa%20Dunlap%20and%20Jiahao%20Shu%20and%20XuDong%20Wang%20and%20Renhao%20Wang%20and%20Trevor%20Darrell%20and%20Alane%20Suhr%20and%20Angjoo%20Kanazawa&entry.1292438233=A%20key%20challenge%20in%20evaluating%20VLMs%20is%20testing%20models%27%20ability%20to%20analyze%20visual%20content%20independently%20from%20their%20textual%20priors.%20Recent%20benchmarks%20such%20as%20BLINK%20probe%20visual%20perception%20through%20visual%20prompting%2C%20where%20questions%20about%20visual%20content%20are%20paired%20with%20coordinates%20to%20which%20the%20question%20refers%2C%20with%20the%20coordinates%20explicitly%20marked%20in%20the%20image%20itself.%20While%20these%20benchmarks%20are%20an%20important%20part%20of%20VLM%20evaluation%2C%20we%20find%20that%20existing%20models%20are%20surprisingly%20fragile%20to%20seemingly%20irrelevant%20details%20of%20visual%20prompting%3A%20simply%20changing%20a%20visual%20marker%20from%20red%20to%20blue%20can%20completely%20change%20rankings%20among%20models%20on%20a%20leaderboard.%20By%20evaluating%20nine%20commonly-used%20open-%20and%20closed-source%20VLMs%20on%20two%20visually%20prompted%20tasks%2C%20we%20demonstrate%20how%20details%20in%20benchmark%20setup%2C%20including%20visual%20marker%20design%20and%20dataset%20size%2C%20have%20a%20significant%20influence%20on%20model%20performance%20and%20leaderboard%20rankings.%20These%20effects%20can%20even%20be%20exploited%20to%20lift%20weaker%20models%20above%20stronger%20ones%3B%20for%20instance%2C%20slightly%20increasing%20the%20size%20of%20the%20visual%20marker%20results%20in%20open-source%20InternVL3-8B%20ranking%20alongside%20or%20better%20than%20much%20larger%20proprietary%20models%20like%20Gemini%202.5%20Pro.%20We%20further%20show%20that%20low-level%20inference%20choices%20that%20are%20often%20ignored%20in%20benchmarking%2C%20such%20as%20JPEG%20compression%20levels%20in%20API%20calls%2C%20can%20also%20cause%20model%20lineup%20changes.%20These%20details%20have%20substantially%20larger%20impacts%20on%20visually%20prompted%20benchmarks%20than%20on%20conventional%20semantic%20VLM%20evaluations.%20To%20mitigate%20this%20instability%2C%20we%20curate%20existing%20datasets%20to%20create%20VPBench%2C%20a%20larger%20visually%20prompted%20benchmark%20with%2016%20visual%20marker%20variants.%20VPBench%20and%20additional%20analysis%20tools%20are%20released%20at%20https%3A//lisadunlap.github.io/vpbench/.&entry.1838667208=http%3A//arxiv.org/abs/2512.17875v1&entry.124074799=Read"},
{"title": "MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding", "author": "Oskar Kristoffersen and Alba R. S\u00e1nchez and Morten R. Hannemose and Anders B. Dahl and Dim P. Papadopoulos", "abstract": "Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.", "link": "http://arxiv.org/abs/2512.17492v1", "date": "2025-12-19", "relevancy": 2.7724, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5695}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMLANDMARKS%3A%20a%20Cross-View%20Instance-Level%20Benchmark%20for%20Geo-Spatial%20Understanding&body=Title%3A%20MMLANDMARKS%3A%20a%20Cross-View%20Instance-Level%20Benchmark%20for%20Geo-Spatial%20Understanding%0AAuthor%3A%20Oskar%20Kristoffersen%20and%20Alba%20R.%20S%C3%A1nchez%20and%20Morten%20R.%20Hannemose%20and%20Anders%20B.%20Dahl%20and%20Dim%20P.%20Papadopoulos%0AAbstract%3A%20Geo-spatial%20analysis%20of%20our%20world%20benefits%20from%20a%20multimodal%20approach%2C%20as%20every%20single%20geographic%20location%20can%20be%20described%20in%20numerous%20ways%20%28images%20from%20various%20viewpoints%2C%20textual%20descriptions%2C%20and%20geographic%20coordinates%29.%20Current%20geo-spatial%20benchmarks%20have%20limited%20coverage%20across%20modalities%2C%20considerably%20restricting%20progress%20in%20the%20field%2C%20as%20current%20approaches%20cannot%20integrate%20all%20relevant%20modalities%20within%20a%20unified%20framework.%20We%20introduce%20the%20Multi-Modal%20Landmark%20dataset%20%28MMLANDMARKS%29%2C%20a%20benchmark%20composed%20of%20four%20modalities%3A%20197k%20highresolution%20aerial%20images%2C%20329k%20ground-view%20images%2C%20textual%20information%2C%20and%20geographic%20coordinates%20for%2018%2C557%20distinct%20landmarks%20in%20the%20United%20States.%20The%20MMLANDMARKS%20dataset%20has%20a%20one-to-one%20correspondence%20across%20every%20modality%2C%20which%20enables%20training%20and%20benchmarking%20models%20for%20various%20geo-spatial%20tasks%2C%20including%20cross-view%20Ground-to-Satellite%20retrieval%2C%20ground%20and%20satellite%20geolocalization%2C%20Text-to-Image%2C%20and%20Text-to-GPS%20retrieval.%20We%20demonstrate%20broad%20generalization%20and%20competitive%20performance%20against%20off-the-shelf%20foundational%20models%20and%20specialized%20state-of-the-art%20models%20across%20different%20tasks%20by%20employing%20a%20simple%20CLIP-inspired%20baseline%2C%20illustrating%20the%20necessity%20for%20multimodal%20datasets%20to%20achieve%20broad%20geo-spatial%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMLANDMARKS%253A%2520a%2520Cross-View%2520Instance-Level%2520Benchmark%2520for%2520Geo-Spatial%2520Understanding%26entry.906535625%3DOskar%2520Kristoffersen%2520and%2520Alba%2520R.%2520S%25C3%25A1nchez%2520and%2520Morten%2520R.%2520Hannemose%2520and%2520Anders%2520B.%2520Dahl%2520and%2520Dim%2520P.%2520Papadopoulos%26entry.1292438233%3DGeo-spatial%2520analysis%2520of%2520our%2520world%2520benefits%2520from%2520a%2520multimodal%2520approach%252C%2520as%2520every%2520single%2520geographic%2520location%2520can%2520be%2520described%2520in%2520numerous%2520ways%2520%2528images%2520from%2520various%2520viewpoints%252C%2520textual%2520descriptions%252C%2520and%2520geographic%2520coordinates%2529.%2520Current%2520geo-spatial%2520benchmarks%2520have%2520limited%2520coverage%2520across%2520modalities%252C%2520considerably%2520restricting%2520progress%2520in%2520the%2520field%252C%2520as%2520current%2520approaches%2520cannot%2520integrate%2520all%2520relevant%2520modalities%2520within%2520a%2520unified%2520framework.%2520We%2520introduce%2520the%2520Multi-Modal%2520Landmark%2520dataset%2520%2528MMLANDMARKS%2529%252C%2520a%2520benchmark%2520composed%2520of%2520four%2520modalities%253A%2520197k%2520highresolution%2520aerial%2520images%252C%2520329k%2520ground-view%2520images%252C%2520textual%2520information%252C%2520and%2520geographic%2520coordinates%2520for%252018%252C557%2520distinct%2520landmarks%2520in%2520the%2520United%2520States.%2520The%2520MMLANDMARKS%2520dataset%2520has%2520a%2520one-to-one%2520correspondence%2520across%2520every%2520modality%252C%2520which%2520enables%2520training%2520and%2520benchmarking%2520models%2520for%2520various%2520geo-spatial%2520tasks%252C%2520including%2520cross-view%2520Ground-to-Satellite%2520retrieval%252C%2520ground%2520and%2520satellite%2520geolocalization%252C%2520Text-to-Image%252C%2520and%2520Text-to-GPS%2520retrieval.%2520We%2520demonstrate%2520broad%2520generalization%2520and%2520competitive%2520performance%2520against%2520off-the-shelf%2520foundational%2520models%2520and%2520specialized%2520state-of-the-art%2520models%2520across%2520different%2520tasks%2520by%2520employing%2520a%2520simple%2520CLIP-inspired%2520baseline%252C%2520illustrating%2520the%2520necessity%2520for%2520multimodal%2520datasets%2520to%2520achieve%2520broad%2520geo-spatial%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMLANDMARKS%3A%20a%20Cross-View%20Instance-Level%20Benchmark%20for%20Geo-Spatial%20Understanding&entry.906535625=Oskar%20Kristoffersen%20and%20Alba%20R.%20S%C3%A1nchez%20and%20Morten%20R.%20Hannemose%20and%20Anders%20B.%20Dahl%20and%20Dim%20P.%20Papadopoulos&entry.1292438233=Geo-spatial%20analysis%20of%20our%20world%20benefits%20from%20a%20multimodal%20approach%2C%20as%20every%20single%20geographic%20location%20can%20be%20described%20in%20numerous%20ways%20%28images%20from%20various%20viewpoints%2C%20textual%20descriptions%2C%20and%20geographic%20coordinates%29.%20Current%20geo-spatial%20benchmarks%20have%20limited%20coverage%20across%20modalities%2C%20considerably%20restricting%20progress%20in%20the%20field%2C%20as%20current%20approaches%20cannot%20integrate%20all%20relevant%20modalities%20within%20a%20unified%20framework.%20We%20introduce%20the%20Multi-Modal%20Landmark%20dataset%20%28MMLANDMARKS%29%2C%20a%20benchmark%20composed%20of%20four%20modalities%3A%20197k%20highresolution%20aerial%20images%2C%20329k%20ground-view%20images%2C%20textual%20information%2C%20and%20geographic%20coordinates%20for%2018%2C557%20distinct%20landmarks%20in%20the%20United%20States.%20The%20MMLANDMARKS%20dataset%20has%20a%20one-to-one%20correspondence%20across%20every%20modality%2C%20which%20enables%20training%20and%20benchmarking%20models%20for%20various%20geo-spatial%20tasks%2C%20including%20cross-view%20Ground-to-Satellite%20retrieval%2C%20ground%20and%20satellite%20geolocalization%2C%20Text-to-Image%2C%20and%20Text-to-GPS%20retrieval.%20We%20demonstrate%20broad%20generalization%20and%20competitive%20performance%20against%20off-the-shelf%20foundational%20models%20and%20specialized%20state-of-the-art%20models%20across%20different%20tasks%20by%20employing%20a%20simple%20CLIP-inspired%20baseline%2C%20illustrating%20the%20necessity%20for%20multimodal%20datasets%20to%20achieve%20broad%20geo-spatial%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2512.17492v1&entry.124074799=Read"},
{"title": "RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis", "author": "Qilong Wang and Xiaofan Ming and Zhenyi Lin and Jinwen Li and Dongwei Ren and Wangmeng Zuo and Qinghua Hu", "abstract": "Virtual furniture synthesis, which seamlessly integrates reference objects into indoor scenes while maintaining geometric coherence and visual realism, holds substantial promise for home design and e-commerce applications. However, this field remains underexplored due to the scarcity of reproducible benchmarks and the limitations of existing image composition methods in achieving high-fidelity furniture synthesis while preserving background integrity. To overcome these challenges, we first present RoomBench++, a comprehensive and publicly available benchmark dataset tailored for this task. It consists of 112,851 training pairs and 1,832 testing pairs drawn from both real-world indoor videos and realistic home design renderings, thereby supporting robust training and evaluation under practical conditions. Then, we propose RoomEditor++, a versatile diffusion-based architecture featuring a parameter-sharing dual diffusion backbone, which is compatible with both U-Net and DiT architectures. This design unifies the feature extraction and inpainting processes for reference and background images. Our in-depth analysis reveals that the parameter-sharing mechanism enforces aligned feature representations, facilitating precise geometric transformations, texture preservation, and seamless integration. Extensive experiments validate that RoomEditor++ is superior over state-of-the-art approaches in terms of quantitative metrics, qualitative assessments, and human preference studies, while highlighting its strong generalization to unseen indoor scenes and general scenes without task-specific fine-tuning. The dataset and source code are available at \\url{https://github.com/stonecutter-21/roomeditor}.", "link": "http://arxiv.org/abs/2512.17573v1", "date": "2025-12-19", "relevancy": 2.7683, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5647}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5489}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoomEditor%2B%2B%3A%20A%20Parameter-Sharing%20Diffusion%20Architecture%20for%20High-Fidelity%20Furniture%20Synthesis&body=Title%3A%20RoomEditor%2B%2B%3A%20A%20Parameter-Sharing%20Diffusion%20Architecture%20for%20High-Fidelity%20Furniture%20Synthesis%0AAuthor%3A%20Qilong%20Wang%20and%20Xiaofan%20Ming%20and%20Zhenyi%20Lin%20and%20Jinwen%20Li%20and%20Dongwei%20Ren%20and%20Wangmeng%20Zuo%20and%20Qinghua%20Hu%0AAbstract%3A%20Virtual%20furniture%20synthesis%2C%20which%20seamlessly%20integrates%20reference%20objects%20into%20indoor%20scenes%20while%20maintaining%20geometric%20coherence%20and%20visual%20realism%2C%20holds%20substantial%20promise%20for%20home%20design%20and%20e-commerce%20applications.%20However%2C%20this%20field%20remains%20underexplored%20due%20to%20the%20scarcity%20of%20reproducible%20benchmarks%20and%20the%20limitations%20of%20existing%20image%20composition%20methods%20in%20achieving%20high-fidelity%20furniture%20synthesis%20while%20preserving%20background%20integrity.%20To%20overcome%20these%20challenges%2C%20we%20first%20present%20RoomBench%2B%2B%2C%20a%20comprehensive%20and%20publicly%20available%20benchmark%20dataset%20tailored%20for%20this%20task.%20It%20consists%20of%20112%2C851%20training%20pairs%20and%201%2C832%20testing%20pairs%20drawn%20from%20both%20real-world%20indoor%20videos%20and%20realistic%20home%20design%20renderings%2C%20thereby%20supporting%20robust%20training%20and%20evaluation%20under%20practical%20conditions.%20Then%2C%20we%20propose%20RoomEditor%2B%2B%2C%20a%20versatile%20diffusion-based%20architecture%20featuring%20a%20parameter-sharing%20dual%20diffusion%20backbone%2C%20which%20is%20compatible%20with%20both%20U-Net%20and%20DiT%20architectures.%20This%20design%20unifies%20the%20feature%20extraction%20and%20inpainting%20processes%20for%20reference%20and%20background%20images.%20Our%20in-depth%20analysis%20reveals%20that%20the%20parameter-sharing%20mechanism%20enforces%20aligned%20feature%20representations%2C%20facilitating%20precise%20geometric%20transformations%2C%20texture%20preservation%2C%20and%20seamless%20integration.%20Extensive%20experiments%20validate%20that%20RoomEditor%2B%2B%20is%20superior%20over%20state-of-the-art%20approaches%20in%20terms%20of%20quantitative%20metrics%2C%20qualitative%20assessments%2C%20and%20human%20preference%20studies%2C%20while%20highlighting%20its%20strong%20generalization%20to%20unseen%20indoor%20scenes%20and%20general%20scenes%20without%20task-specific%20fine-tuning.%20The%20dataset%20and%20source%20code%20are%20available%20at%20%5Curl%7Bhttps%3A//github.com/stonecutter-21/roomeditor%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoomEditor%252B%252B%253A%2520A%2520Parameter-Sharing%2520Diffusion%2520Architecture%2520for%2520High-Fidelity%2520Furniture%2520Synthesis%26entry.906535625%3DQilong%2520Wang%2520and%2520Xiaofan%2520Ming%2520and%2520Zhenyi%2520Lin%2520and%2520Jinwen%2520Li%2520and%2520Dongwei%2520Ren%2520and%2520Wangmeng%2520Zuo%2520and%2520Qinghua%2520Hu%26entry.1292438233%3DVirtual%2520furniture%2520synthesis%252C%2520which%2520seamlessly%2520integrates%2520reference%2520objects%2520into%2520indoor%2520scenes%2520while%2520maintaining%2520geometric%2520coherence%2520and%2520visual%2520realism%252C%2520holds%2520substantial%2520promise%2520for%2520home%2520design%2520and%2520e-commerce%2520applications.%2520However%252C%2520this%2520field%2520remains%2520underexplored%2520due%2520to%2520the%2520scarcity%2520of%2520reproducible%2520benchmarks%2520and%2520the%2520limitations%2520of%2520existing%2520image%2520composition%2520methods%2520in%2520achieving%2520high-fidelity%2520furniture%2520synthesis%2520while%2520preserving%2520background%2520integrity.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520first%2520present%2520RoomBench%252B%252B%252C%2520a%2520comprehensive%2520and%2520publicly%2520available%2520benchmark%2520dataset%2520tailored%2520for%2520this%2520task.%2520It%2520consists%2520of%2520112%252C851%2520training%2520pairs%2520and%25201%252C832%2520testing%2520pairs%2520drawn%2520from%2520both%2520real-world%2520indoor%2520videos%2520and%2520realistic%2520home%2520design%2520renderings%252C%2520thereby%2520supporting%2520robust%2520training%2520and%2520evaluation%2520under%2520practical%2520conditions.%2520Then%252C%2520we%2520propose%2520RoomEditor%252B%252B%252C%2520a%2520versatile%2520diffusion-based%2520architecture%2520featuring%2520a%2520parameter-sharing%2520dual%2520diffusion%2520backbone%252C%2520which%2520is%2520compatible%2520with%2520both%2520U-Net%2520and%2520DiT%2520architectures.%2520This%2520design%2520unifies%2520the%2520feature%2520extraction%2520and%2520inpainting%2520processes%2520for%2520reference%2520and%2520background%2520images.%2520Our%2520in-depth%2520analysis%2520reveals%2520that%2520the%2520parameter-sharing%2520mechanism%2520enforces%2520aligned%2520feature%2520representations%252C%2520facilitating%2520precise%2520geometric%2520transformations%252C%2520texture%2520preservation%252C%2520and%2520seamless%2520integration.%2520Extensive%2520experiments%2520validate%2520that%2520RoomEditor%252B%252B%2520is%2520superior%2520over%2520state-of-the-art%2520approaches%2520in%2520terms%2520of%2520quantitative%2520metrics%252C%2520qualitative%2520assessments%252C%2520and%2520human%2520preference%2520studies%252C%2520while%2520highlighting%2520its%2520strong%2520generalization%2520to%2520unseen%2520indoor%2520scenes%2520and%2520general%2520scenes%2520without%2520task-specific%2520fine-tuning.%2520The%2520dataset%2520and%2520source%2520code%2520are%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/stonecutter-21/roomeditor%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoomEditor%2B%2B%3A%20A%20Parameter-Sharing%20Diffusion%20Architecture%20for%20High-Fidelity%20Furniture%20Synthesis&entry.906535625=Qilong%20Wang%20and%20Xiaofan%20Ming%20and%20Zhenyi%20Lin%20and%20Jinwen%20Li%20and%20Dongwei%20Ren%20and%20Wangmeng%20Zuo%20and%20Qinghua%20Hu&entry.1292438233=Virtual%20furniture%20synthesis%2C%20which%20seamlessly%20integrates%20reference%20objects%20into%20indoor%20scenes%20while%20maintaining%20geometric%20coherence%20and%20visual%20realism%2C%20holds%20substantial%20promise%20for%20home%20design%20and%20e-commerce%20applications.%20However%2C%20this%20field%20remains%20underexplored%20due%20to%20the%20scarcity%20of%20reproducible%20benchmarks%20and%20the%20limitations%20of%20existing%20image%20composition%20methods%20in%20achieving%20high-fidelity%20furniture%20synthesis%20while%20preserving%20background%20integrity.%20To%20overcome%20these%20challenges%2C%20we%20first%20present%20RoomBench%2B%2B%2C%20a%20comprehensive%20and%20publicly%20available%20benchmark%20dataset%20tailored%20for%20this%20task.%20It%20consists%20of%20112%2C851%20training%20pairs%20and%201%2C832%20testing%20pairs%20drawn%20from%20both%20real-world%20indoor%20videos%20and%20realistic%20home%20design%20renderings%2C%20thereby%20supporting%20robust%20training%20and%20evaluation%20under%20practical%20conditions.%20Then%2C%20we%20propose%20RoomEditor%2B%2B%2C%20a%20versatile%20diffusion-based%20architecture%20featuring%20a%20parameter-sharing%20dual%20diffusion%20backbone%2C%20which%20is%20compatible%20with%20both%20U-Net%20and%20DiT%20architectures.%20This%20design%20unifies%20the%20feature%20extraction%20and%20inpainting%20processes%20for%20reference%20and%20background%20images.%20Our%20in-depth%20analysis%20reveals%20that%20the%20parameter-sharing%20mechanism%20enforces%20aligned%20feature%20representations%2C%20facilitating%20precise%20geometric%20transformations%2C%20texture%20preservation%2C%20and%20seamless%20integration.%20Extensive%20experiments%20validate%20that%20RoomEditor%2B%2B%20is%20superior%20over%20state-of-the-art%20approaches%20in%20terms%20of%20quantitative%20metrics%2C%20qualitative%20assessments%2C%20and%20human%20preference%20studies%2C%20while%20highlighting%20its%20strong%20generalization%20to%20unseen%20indoor%20scenes%20and%20general%20scenes%20without%20task-specific%20fine-tuning.%20The%20dataset%20and%20source%20code%20are%20available%20at%20%5Curl%7Bhttps%3A//github.com/stonecutter-21/roomeditor%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.17573v1&entry.124074799=Read"},
{"title": "CLUENet: Cluster Attention Makes Neural Networks Have Eyes", "author": "Xiangshuai Song and Jun-Jie Huang and Tianrui Liu and Ke Liang and Chang Tang", "abstract": "Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.", "link": "http://arxiv.org/abs/2512.06345v2", "date": "2025-12-19", "relevancy": 2.7468, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLUENet%3A%20Cluster%20Attention%20Makes%20Neural%20Networks%20Have%20Eyes&body=Title%3A%20CLUENet%3A%20Cluster%20Attention%20Makes%20Neural%20Networks%20Have%20Eyes%0AAuthor%3A%20Xiangshuai%20Song%20and%20Jun-Jie%20Huang%20and%20Tianrui%20Liu%20and%20Ke%20Liang%20and%20Chang%20Tang%0AAbstract%3A%20Despite%20the%20success%20of%20convolution-%20and%20attention-based%20models%20in%20vision%20tasks%2C%20their%20rigid%20receptive%20fields%20and%20complex%20architectures%20limit%20their%20ability%20to%20model%20irregular%20spatial%20patterns%20and%20hinder%20interpretability%2C%20therefore%20posing%20challenges%20for%20tasks%20requiring%20high%20model%20transparency.%20Clustering%20paradigms%20offer%20promising%20interpretability%20and%20flexible%20semantic%20modeling%2C%20but%20suffer%20from%20limited%20accuracy%2C%20low%20efficiency%2C%20and%20gradient%20vanishing%20during%20training.%20To%20address%20these%20issues%2C%20we%20propose%20CLUster%20attEntion%20Network%20%28CLUENet%29%2C%20an%20transparent%20deep%20architecture%20for%20visual%20semantic%20understanding.%20We%20propose%20three%20key%20innovations%20include%20%28i%29%20a%20Global%20Soft%20Aggregation%20and%20Hard%20Assignment%20with%20a%20Temperature-Scaled%20Cosin%20Attention%20and%20gated%20residual%20connections%20for%20enhanced%20local%20modeling%2C%20%28ii%29%20inter-block%20Hard%20and%20Shared%20Feature%20Dispatching%2C%20and%20%28iii%29%20an%20improved%20cluster%20pooling%20strategy.%20These%20enhancements%20significantly%20improve%20both%20classification%20performance%20and%20visual%20interpretability.%20Experiments%20on%20CIFAR-100%20and%20Mini-ImageNet%20demonstrate%20that%20CLUENet%20outperforms%20existing%20clustering%20methods%20and%20mainstream%20visual%20models%2C%20offering%20a%20compelling%20balance%20of%20accuracy%2C%20efficiency%2C%20and%20transparency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLUENet%253A%2520Cluster%2520Attention%2520Makes%2520Neural%2520Networks%2520Have%2520Eyes%26entry.906535625%3DXiangshuai%2520Song%2520and%2520Jun-Jie%2520Huang%2520and%2520Tianrui%2520Liu%2520and%2520Ke%2520Liang%2520and%2520Chang%2520Tang%26entry.1292438233%3DDespite%2520the%2520success%2520of%2520convolution-%2520and%2520attention-based%2520models%2520in%2520vision%2520tasks%252C%2520their%2520rigid%2520receptive%2520fields%2520and%2520complex%2520architectures%2520limit%2520their%2520ability%2520to%2520model%2520irregular%2520spatial%2520patterns%2520and%2520hinder%2520interpretability%252C%2520therefore%2520posing%2520challenges%2520for%2520tasks%2520requiring%2520high%2520model%2520transparency.%2520Clustering%2520paradigms%2520offer%2520promising%2520interpretability%2520and%2520flexible%2520semantic%2520modeling%252C%2520but%2520suffer%2520from%2520limited%2520accuracy%252C%2520low%2520efficiency%252C%2520and%2520gradient%2520vanishing%2520during%2520training.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520CLUster%2520attEntion%2520Network%2520%2528CLUENet%2529%252C%2520an%2520transparent%2520deep%2520architecture%2520for%2520visual%2520semantic%2520understanding.%2520We%2520propose%2520three%2520key%2520innovations%2520include%2520%2528i%2529%2520a%2520Global%2520Soft%2520Aggregation%2520and%2520Hard%2520Assignment%2520with%2520a%2520Temperature-Scaled%2520Cosin%2520Attention%2520and%2520gated%2520residual%2520connections%2520for%2520enhanced%2520local%2520modeling%252C%2520%2528ii%2529%2520inter-block%2520Hard%2520and%2520Shared%2520Feature%2520Dispatching%252C%2520and%2520%2528iii%2529%2520an%2520improved%2520cluster%2520pooling%2520strategy.%2520These%2520enhancements%2520significantly%2520improve%2520both%2520classification%2520performance%2520and%2520visual%2520interpretability.%2520Experiments%2520on%2520CIFAR-100%2520and%2520Mini-ImageNet%2520demonstrate%2520that%2520CLUENet%2520outperforms%2520existing%2520clustering%2520methods%2520and%2520mainstream%2520visual%2520models%252C%2520offering%2520a%2520compelling%2520balance%2520of%2520accuracy%252C%2520efficiency%252C%2520and%2520transparency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLUENet%3A%20Cluster%20Attention%20Makes%20Neural%20Networks%20Have%20Eyes&entry.906535625=Xiangshuai%20Song%20and%20Jun-Jie%20Huang%20and%20Tianrui%20Liu%20and%20Ke%20Liang%20and%20Chang%20Tang&entry.1292438233=Despite%20the%20success%20of%20convolution-%20and%20attention-based%20models%20in%20vision%20tasks%2C%20their%20rigid%20receptive%20fields%20and%20complex%20architectures%20limit%20their%20ability%20to%20model%20irregular%20spatial%20patterns%20and%20hinder%20interpretability%2C%20therefore%20posing%20challenges%20for%20tasks%20requiring%20high%20model%20transparency.%20Clustering%20paradigms%20offer%20promising%20interpretability%20and%20flexible%20semantic%20modeling%2C%20but%20suffer%20from%20limited%20accuracy%2C%20low%20efficiency%2C%20and%20gradient%20vanishing%20during%20training.%20To%20address%20these%20issues%2C%20we%20propose%20CLUster%20attEntion%20Network%20%28CLUENet%29%2C%20an%20transparent%20deep%20architecture%20for%20visual%20semantic%20understanding.%20We%20propose%20three%20key%20innovations%20include%20%28i%29%20a%20Global%20Soft%20Aggregation%20and%20Hard%20Assignment%20with%20a%20Temperature-Scaled%20Cosin%20Attention%20and%20gated%20residual%20connections%20for%20enhanced%20local%20modeling%2C%20%28ii%29%20inter-block%20Hard%20and%20Shared%20Feature%20Dispatching%2C%20and%20%28iii%29%20an%20improved%20cluster%20pooling%20strategy.%20These%20enhancements%20significantly%20improve%20both%20classification%20performance%20and%20visual%20interpretability.%20Experiments%20on%20CIFAR-100%20and%20Mini-ImageNet%20demonstrate%20that%20CLUENet%20outperforms%20existing%20clustering%20methods%20and%20mainstream%20visual%20models%2C%20offering%20a%20compelling%20balance%20of%20accuracy%2C%20efficiency%2C%20and%20transparency.&entry.1838667208=http%3A//arxiv.org/abs/2512.06345v2&entry.124074799=Read"},
{"title": "PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology", "author": "Fengchun Liu and Songhan Jiang and Linghan Cai and Ziyue Wang and Yongbing Zhang", "abstract": "While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.", "link": "http://arxiv.org/abs/2512.17621v1", "date": "2025-12-19", "relevancy": 2.738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PathFLIP%3A%20Fine-grained%20Language-Image%20Pretraining%20for%20Versatile%20Computational%20Pathology&body=Title%3A%20PathFLIP%3A%20Fine-grained%20Language-Image%20Pretraining%20for%20Versatile%20Computational%20Pathology%0AAuthor%3A%20Fengchun%20Liu%20and%20Songhan%20Jiang%20and%20Linghan%20Cai%20and%20Ziyue%20Wang%20and%20Yongbing%20Zhang%0AAbstract%3A%20While%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20notable%20progress%20in%20computational%20pathology%20%28CPath%29%2C%20the%20gigapixel%20scale%20and%20spatial%20heterogeneity%20of%20Whole%20Slide%20Images%20%28WSIs%29%20continue%20to%20pose%20challenges%20for%20multimodal%20understanding.%20Existing%20alignment%20methods%20struggle%20to%20capture%20fine-grained%20correspondences%20between%20textual%20descriptions%20and%20visual%20cues%20across%20thousands%20of%20patches%20from%20a%20slide%2C%20compromising%20their%20performance%20on%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20PathFLIP%20%28Pathology%20Fine-grained%20Language-Image%20Pretraining%29%2C%20a%20novel%20framework%20for%20holistic%20WSI%20interpretation.%20PathFLIP%20decomposes%20slide-level%20captions%20into%20region-level%20subcaptions%20and%20generates%20text-conditioned%20region%20embeddings%20to%20facilitate%20precise%20visual-language%20grounding.%20By%20harnessing%20Large%20Language%20Models%20%28LLMs%29%2C%20PathFLIP%20can%20seamlessly%20follow%20diverse%20clinical%20instructions%20and%20adapt%20to%20varied%20diagnostic%20contexts.%20Furthermore%2C%20it%20exhibits%20versatile%20capabilities%20across%20multiple%20paradigms%2C%20efficiently%20handling%20slide-level%20classification%20and%20retrieval%2C%20fine-grained%20lesion%20localization%2C%20and%20instruction%20following.%20Extensive%20experiments%20demonstrate%20that%20PathFLIP%20outperforms%20existing%20large-scale%20pathological%20VLMs%20on%20four%20representative%20benchmarks%20while%20requiring%20significantly%20less%20training%20data%2C%20paving%20the%20way%20for%20fine-grained%2C%20instruction-aware%20WSI%20interpretation%20in%20clinical%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathFLIP%253A%2520Fine-grained%2520Language-Image%2520Pretraining%2520for%2520Versatile%2520Computational%2520Pathology%26entry.906535625%3DFengchun%2520Liu%2520and%2520Songhan%2520Jiang%2520and%2520Linghan%2520Cai%2520and%2520Ziyue%2520Wang%2520and%2520Yongbing%2520Zhang%26entry.1292438233%3DWhile%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520notable%2520progress%2520in%2520computational%2520pathology%2520%2528CPath%2529%252C%2520the%2520gigapixel%2520scale%2520and%2520spatial%2520heterogeneity%2520of%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520continue%2520to%2520pose%2520challenges%2520for%2520multimodal%2520understanding.%2520Existing%2520alignment%2520methods%2520struggle%2520to%2520capture%2520fine-grained%2520correspondences%2520between%2520textual%2520descriptions%2520and%2520visual%2520cues%2520across%2520thousands%2520of%2520patches%2520from%2520a%2520slide%252C%2520compromising%2520their%2520performance%2520on%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PathFLIP%2520%2528Pathology%2520Fine-grained%2520Language-Image%2520Pretraining%2529%252C%2520a%2520novel%2520framework%2520for%2520holistic%2520WSI%2520interpretation.%2520PathFLIP%2520decomposes%2520slide-level%2520captions%2520into%2520region-level%2520subcaptions%2520and%2520generates%2520text-conditioned%2520region%2520embeddings%2520to%2520facilitate%2520precise%2520visual-language%2520grounding.%2520By%2520harnessing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520PathFLIP%2520can%2520seamlessly%2520follow%2520diverse%2520clinical%2520instructions%2520and%2520adapt%2520to%2520varied%2520diagnostic%2520contexts.%2520Furthermore%252C%2520it%2520exhibits%2520versatile%2520capabilities%2520across%2520multiple%2520paradigms%252C%2520efficiently%2520handling%2520slide-level%2520classification%2520and%2520retrieval%252C%2520fine-grained%2520lesion%2520localization%252C%2520and%2520instruction%2520following.%2520Extensive%2520experiments%2520demonstrate%2520that%2520PathFLIP%2520outperforms%2520existing%2520large-scale%2520pathological%2520VLMs%2520on%2520four%2520representative%2520benchmarks%2520while%2520requiring%2520significantly%2520less%2520training%2520data%252C%2520paving%2520the%2520way%2520for%2520fine-grained%252C%2520instruction-aware%2520WSI%2520interpretation%2520in%2520clinical%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PathFLIP%3A%20Fine-grained%20Language-Image%20Pretraining%20for%20Versatile%20Computational%20Pathology&entry.906535625=Fengchun%20Liu%20and%20Songhan%20Jiang%20and%20Linghan%20Cai%20and%20Ziyue%20Wang%20and%20Yongbing%20Zhang&entry.1292438233=While%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20notable%20progress%20in%20computational%20pathology%20%28CPath%29%2C%20the%20gigapixel%20scale%20and%20spatial%20heterogeneity%20of%20Whole%20Slide%20Images%20%28WSIs%29%20continue%20to%20pose%20challenges%20for%20multimodal%20understanding.%20Existing%20alignment%20methods%20struggle%20to%20capture%20fine-grained%20correspondences%20between%20textual%20descriptions%20and%20visual%20cues%20across%20thousands%20of%20patches%20from%20a%20slide%2C%20compromising%20their%20performance%20on%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20PathFLIP%20%28Pathology%20Fine-grained%20Language-Image%20Pretraining%29%2C%20a%20novel%20framework%20for%20holistic%20WSI%20interpretation.%20PathFLIP%20decomposes%20slide-level%20captions%20into%20region-level%20subcaptions%20and%20generates%20text-conditioned%20region%20embeddings%20to%20facilitate%20precise%20visual-language%20grounding.%20By%20harnessing%20Large%20Language%20Models%20%28LLMs%29%2C%20PathFLIP%20can%20seamlessly%20follow%20diverse%20clinical%20instructions%20and%20adapt%20to%20varied%20diagnostic%20contexts.%20Furthermore%2C%20it%20exhibits%20versatile%20capabilities%20across%20multiple%20paradigms%2C%20efficiently%20handling%20slide-level%20classification%20and%20retrieval%2C%20fine-grained%20lesion%20localization%2C%20and%20instruction%20following.%20Extensive%20experiments%20demonstrate%20that%20PathFLIP%20outperforms%20existing%20large-scale%20pathological%20VLMs%20on%20four%20representative%20benchmarks%20while%20requiring%20significantly%20less%20training%20data%2C%20paving%20the%20way%20for%20fine-grained%2C%20instruction-aware%20WSI%20interpretation%20in%20clinical%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2512.17621v1&entry.124074799=Read"},
{"title": "Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection", "author": "Sairam VCR and Rishabh Lalla and Aveen Dayal and Tejal Kulkarni and Anuj Lalla and Vineeth N Balasubramanian and Muhammad Haris Khan", "abstract": "Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.", "link": "http://arxiv.org/abs/2512.17514v1", "date": "2025-12-19", "relevancy": 2.7292, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Model%20Priors%20Enhance%20Object%20Focus%20in%20Feature%20Space%20for%20Source-Free%20Object%20Detection&body=Title%3A%20Foundation%20Model%20Priors%20Enhance%20Object%20Focus%20in%20Feature%20Space%20for%20Source-Free%20Object%20Detection%0AAuthor%3A%20Sairam%20VCR%20and%20Rishabh%20Lalla%20and%20Aveen%20Dayal%20and%20Tejal%20Kulkarni%20and%20Anuj%20Lalla%20and%20Vineeth%20N%20Balasubramanian%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20Current%20state-of-the-art%20approaches%20in%20Source-Free%20Object%20Detection%20%28SFOD%29%20typically%20rely%20on%20Mean-Teacher%20self-labeling.%20However%2C%20domain%20shift%20often%20reduces%20the%20detector%27s%20ability%20to%20maintain%20strong%20object-focused%20representations%2C%20causing%20high-confidence%20activations%20over%20background%20clutter.%20This%20weak%20object%20focus%20results%20in%20unreliable%20pseudo-labels%20from%20the%20detection%20head.%20While%20prior%20works%20mainly%20refine%20these%20pseudo-labels%2C%20they%20overlook%20the%20underlying%20need%20to%20strengthen%20the%20feature%20space%20itself.%20We%20propose%20FALCON-SFOD%20%28Foundation-Aligned%20Learning%20with%20Clutter%20suppression%20and%20Noise%20robustness%29%2C%20a%20framework%20designed%20to%20enhance%20object-focused%20adaptation%20under%20domain%20shift.%20It%20consists%20of%20two%20complementary%20components.%20SPAR%20%28Spatial%20Prior-Aware%20Regularization%29%20leverages%20the%20generalization%20strength%20of%20vision%20foundation%20models%20to%20regularize%20the%20detector%27s%20feature%20space.%20Using%20class-agnostic%20binary%20masks%20derived%20from%20OV-SAM%2C%20SPAR%20promotes%20structured%20and%20foreground-focused%20activations%20by%20guiding%20the%20network%20toward%20object%20regions.%20IRPL%20%28Imbalance-aware%20Noise%20Robust%20Pseudo-Labeling%29%20complements%20SPAR%20by%20promoting%20balanced%20and%20noise-tolerant%20learning%20under%20severe%20foreground-background%20imbalance.%20Guided%20by%20a%20theoretical%20analysis%20that%20connects%20these%20designs%20to%20tighter%20localization%20and%20classification%20error%20bounds%2C%20FALCON-SFOD%20achieves%20competitive%20performance%20across%20SFOD%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Model%2520Priors%2520Enhance%2520Object%2520Focus%2520in%2520Feature%2520Space%2520for%2520Source-Free%2520Object%2520Detection%26entry.906535625%3DSairam%2520VCR%2520and%2520Rishabh%2520Lalla%2520and%2520Aveen%2520Dayal%2520and%2520Tejal%2520Kulkarni%2520and%2520Anuj%2520Lalla%2520and%2520Vineeth%2520N%2520Balasubramanian%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3DCurrent%2520state-of-the-art%2520approaches%2520in%2520Source-Free%2520Object%2520Detection%2520%2528SFOD%2529%2520typically%2520rely%2520on%2520Mean-Teacher%2520self-labeling.%2520However%252C%2520domain%2520shift%2520often%2520reduces%2520the%2520detector%2527s%2520ability%2520to%2520maintain%2520strong%2520object-focused%2520representations%252C%2520causing%2520high-confidence%2520activations%2520over%2520background%2520clutter.%2520This%2520weak%2520object%2520focus%2520results%2520in%2520unreliable%2520pseudo-labels%2520from%2520the%2520detection%2520head.%2520While%2520prior%2520works%2520mainly%2520refine%2520these%2520pseudo-labels%252C%2520they%2520overlook%2520the%2520underlying%2520need%2520to%2520strengthen%2520the%2520feature%2520space%2520itself.%2520We%2520propose%2520FALCON-SFOD%2520%2528Foundation-Aligned%2520Learning%2520with%2520Clutter%2520suppression%2520and%2520Noise%2520robustness%2529%252C%2520a%2520framework%2520designed%2520to%2520enhance%2520object-focused%2520adaptation%2520under%2520domain%2520shift.%2520It%2520consists%2520of%2520two%2520complementary%2520components.%2520SPAR%2520%2528Spatial%2520Prior-Aware%2520Regularization%2529%2520leverages%2520the%2520generalization%2520strength%2520of%2520vision%2520foundation%2520models%2520to%2520regularize%2520the%2520detector%2527s%2520feature%2520space.%2520Using%2520class-agnostic%2520binary%2520masks%2520derived%2520from%2520OV-SAM%252C%2520SPAR%2520promotes%2520structured%2520and%2520foreground-focused%2520activations%2520by%2520guiding%2520the%2520network%2520toward%2520object%2520regions.%2520IRPL%2520%2528Imbalance-aware%2520Noise%2520Robust%2520Pseudo-Labeling%2529%2520complements%2520SPAR%2520by%2520promoting%2520balanced%2520and%2520noise-tolerant%2520learning%2520under%2520severe%2520foreground-background%2520imbalance.%2520Guided%2520by%2520a%2520theoretical%2520analysis%2520that%2520connects%2520these%2520designs%2520to%2520tighter%2520localization%2520and%2520classification%2520error%2520bounds%252C%2520FALCON-SFOD%2520achieves%2520competitive%2520performance%2520across%2520SFOD%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Model%20Priors%20Enhance%20Object%20Focus%20in%20Feature%20Space%20for%20Source-Free%20Object%20Detection&entry.906535625=Sairam%20VCR%20and%20Rishabh%20Lalla%20and%20Aveen%20Dayal%20and%20Tejal%20Kulkarni%20and%20Anuj%20Lalla%20and%20Vineeth%20N%20Balasubramanian%20and%20Muhammad%20Haris%20Khan&entry.1292438233=Current%20state-of-the-art%20approaches%20in%20Source-Free%20Object%20Detection%20%28SFOD%29%20typically%20rely%20on%20Mean-Teacher%20self-labeling.%20However%2C%20domain%20shift%20often%20reduces%20the%20detector%27s%20ability%20to%20maintain%20strong%20object-focused%20representations%2C%20causing%20high-confidence%20activations%20over%20background%20clutter.%20This%20weak%20object%20focus%20results%20in%20unreliable%20pseudo-labels%20from%20the%20detection%20head.%20While%20prior%20works%20mainly%20refine%20these%20pseudo-labels%2C%20they%20overlook%20the%20underlying%20need%20to%20strengthen%20the%20feature%20space%20itself.%20We%20propose%20FALCON-SFOD%20%28Foundation-Aligned%20Learning%20with%20Clutter%20suppression%20and%20Noise%20robustness%29%2C%20a%20framework%20designed%20to%20enhance%20object-focused%20adaptation%20under%20domain%20shift.%20It%20consists%20of%20two%20complementary%20components.%20SPAR%20%28Spatial%20Prior-Aware%20Regularization%29%20leverages%20the%20generalization%20strength%20of%20vision%20foundation%20models%20to%20regularize%20the%20detector%27s%20feature%20space.%20Using%20class-agnostic%20binary%20masks%20derived%20from%20OV-SAM%2C%20SPAR%20promotes%20structured%20and%20foreground-focused%20activations%20by%20guiding%20the%20network%20toward%20object%20regions.%20IRPL%20%28Imbalance-aware%20Noise%20Robust%20Pseudo-Labeling%29%20complements%20SPAR%20by%20promoting%20balanced%20and%20noise-tolerant%20learning%20under%20severe%20foreground-background%20imbalance.%20Guided%20by%20a%20theoretical%20analysis%20that%20connects%20these%20designs%20to%20tighter%20localization%20and%20classification%20error%20bounds%2C%20FALCON-SFOD%20achieves%20competitive%20performance%20across%20SFOD%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.17514v1&entry.124074799=Read"},
{"title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework", "author": "Tobias Sautter and Jan-Niklas Dihlmann and Hendrik P. A. Lensch", "abstract": "Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.\n  Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.", "link": "http://arxiv.org/abs/2512.17459v1", "date": "2025-12-19", "relevancy": 2.7103, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6789}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6789}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-RE-GEN%3A%203D%20Reconstruction%20of%20Indoor%20Scenes%20with%20a%20Generative%20Framework&body=Title%3A%203D-RE-GEN%3A%203D%20Reconstruction%20of%20Indoor%20Scenes%20with%20a%20Generative%20Framework%0AAuthor%3A%20Tobias%20Sautter%20and%20Jan-Niklas%20Dihlmann%20and%20Hendrik%20P.%20A.%20Lensch%0AAbstract%3A%20Recent%20advances%20in%203D%20scene%20generation%20produce%20visually%20appealing%20output%2C%20but%20current%20representations%20hinder%20artists%27%20workflows%20that%20require%20modifiable%203D%20textured%20mesh%20scenes%20for%20visual%20effects%20and%20game%20development.%20Despite%20significant%20advances%2C%20current%20textured%20mesh%20scene%20reconstruction%20methods%20are%20far%20from%20artist%20ready%2C%20suffering%20from%20incorrect%20object%20decomposition%2C%20inaccurate%20spatial%20relationships%2C%20and%20missing%20backgrounds.%20We%20present%203D-RE-GEN%2C%20a%20compositional%20framework%20that%20reconstructs%20a%20single%20image%20into%20textured%203D%20objects%20and%20a%20background.%20We%20show%20that%20combining%20state%20of%20the%20art%20models%20from%20specific%20domains%20achieves%20state%20of%20the%20art%20scene%20reconstruction%20performance%2C%20addressing%20artists%27%20requirements.%0A%20%20Our%20reconstruction%20pipeline%20integrates%20models%20for%20asset%20detection%2C%20reconstruction%2C%20and%20placement%2C%20pushing%20certain%20models%20beyond%20their%20originally%20intended%20domains.%20Obtaining%20occluded%20objects%20is%20treated%20as%20an%20image%20editing%20task%20with%20generative%20models%20to%20infer%20and%20reconstruct%20with%20scene%20level%20reasoning%20under%20consistent%20lighting%20and%20geometry.%20Unlike%20current%20methods%2C%203D-RE-GEN%20generates%20a%20comprehensive%20background%20that%20spatially%20constrains%20objects%20during%20optimization%20and%20provides%20a%20foundation%20for%20realistic%20lighting%20and%20simulation%20tasks%20in%20visual%20effects%20and%20games.%20To%20obtain%20physically%20realistic%20layouts%2C%20we%20employ%20a%20novel%204-DoF%20differentiable%20optimization%20that%20aligns%20reconstructed%20objects%20with%20the%20estimated%20ground%20plane.%203D-RE-GEN~achieves%20state%20of%20the%20art%20performance%20in%20single%20image%203D%20scene%20reconstruction%2C%20producing%20coherent%2C%20modifiable%20scenes%20through%20compositional%20generation%20guided%20by%20precise%20camera%20recovery%20and%20spatial%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-RE-GEN%253A%25203D%2520Reconstruction%2520of%2520Indoor%2520Scenes%2520with%2520a%2520Generative%2520Framework%26entry.906535625%3DTobias%2520Sautter%2520and%2520Jan-Niklas%2520Dihlmann%2520and%2520Hendrik%2520P.%2520A.%2520Lensch%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520scene%2520generation%2520produce%2520visually%2520appealing%2520output%252C%2520but%2520current%2520representations%2520hinder%2520artists%2527%2520workflows%2520that%2520require%2520modifiable%25203D%2520textured%2520mesh%2520scenes%2520for%2520visual%2520effects%2520and%2520game%2520development.%2520Despite%2520significant%2520advances%252C%2520current%2520textured%2520mesh%2520scene%2520reconstruction%2520methods%2520are%2520far%2520from%2520artist%2520ready%252C%2520suffering%2520from%2520incorrect%2520object%2520decomposition%252C%2520inaccurate%2520spatial%2520relationships%252C%2520and%2520missing%2520backgrounds.%2520We%2520present%25203D-RE-GEN%252C%2520a%2520compositional%2520framework%2520that%2520reconstructs%2520a%2520single%2520image%2520into%2520textured%25203D%2520objects%2520and%2520a%2520background.%2520We%2520show%2520that%2520combining%2520state%2520of%2520the%2520art%2520models%2520from%2520specific%2520domains%2520achieves%2520state%2520of%2520the%2520art%2520scene%2520reconstruction%2520performance%252C%2520addressing%2520artists%2527%2520requirements.%250A%2520%2520Our%2520reconstruction%2520pipeline%2520integrates%2520models%2520for%2520asset%2520detection%252C%2520reconstruction%252C%2520and%2520placement%252C%2520pushing%2520certain%2520models%2520beyond%2520their%2520originally%2520intended%2520domains.%2520Obtaining%2520occluded%2520objects%2520is%2520treated%2520as%2520an%2520image%2520editing%2520task%2520with%2520generative%2520models%2520to%2520infer%2520and%2520reconstruct%2520with%2520scene%2520level%2520reasoning%2520under%2520consistent%2520lighting%2520and%2520geometry.%2520Unlike%2520current%2520methods%252C%25203D-RE-GEN%2520generates%2520a%2520comprehensive%2520background%2520that%2520spatially%2520constrains%2520objects%2520during%2520optimization%2520and%2520provides%2520a%2520foundation%2520for%2520realistic%2520lighting%2520and%2520simulation%2520tasks%2520in%2520visual%2520effects%2520and%2520games.%2520To%2520obtain%2520physically%2520realistic%2520layouts%252C%2520we%2520employ%2520a%2520novel%25204-DoF%2520differentiable%2520optimization%2520that%2520aligns%2520reconstructed%2520objects%2520with%2520the%2520estimated%2520ground%2520plane.%25203D-RE-GEN~achieves%2520state%2520of%2520the%2520art%2520performance%2520in%2520single%2520image%25203D%2520scene%2520reconstruction%252C%2520producing%2520coherent%252C%2520modifiable%2520scenes%2520through%2520compositional%2520generation%2520guided%2520by%2520precise%2520camera%2520recovery%2520and%2520spatial%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-RE-GEN%3A%203D%20Reconstruction%20of%20Indoor%20Scenes%20with%20a%20Generative%20Framework&entry.906535625=Tobias%20Sautter%20and%20Jan-Niklas%20Dihlmann%20and%20Hendrik%20P.%20A.%20Lensch&entry.1292438233=Recent%20advances%20in%203D%20scene%20generation%20produce%20visually%20appealing%20output%2C%20but%20current%20representations%20hinder%20artists%27%20workflows%20that%20require%20modifiable%203D%20textured%20mesh%20scenes%20for%20visual%20effects%20and%20game%20development.%20Despite%20significant%20advances%2C%20current%20textured%20mesh%20scene%20reconstruction%20methods%20are%20far%20from%20artist%20ready%2C%20suffering%20from%20incorrect%20object%20decomposition%2C%20inaccurate%20spatial%20relationships%2C%20and%20missing%20backgrounds.%20We%20present%203D-RE-GEN%2C%20a%20compositional%20framework%20that%20reconstructs%20a%20single%20image%20into%20textured%203D%20objects%20and%20a%20background.%20We%20show%20that%20combining%20state%20of%20the%20art%20models%20from%20specific%20domains%20achieves%20state%20of%20the%20art%20scene%20reconstruction%20performance%2C%20addressing%20artists%27%20requirements.%0A%20%20Our%20reconstruction%20pipeline%20integrates%20models%20for%20asset%20detection%2C%20reconstruction%2C%20and%20placement%2C%20pushing%20certain%20models%20beyond%20their%20originally%20intended%20domains.%20Obtaining%20occluded%20objects%20is%20treated%20as%20an%20image%20editing%20task%20with%20generative%20models%20to%20infer%20and%20reconstruct%20with%20scene%20level%20reasoning%20under%20consistent%20lighting%20and%20geometry.%20Unlike%20current%20methods%2C%203D-RE-GEN%20generates%20a%20comprehensive%20background%20that%20spatially%20constrains%20objects%20during%20optimization%20and%20provides%20a%20foundation%20for%20realistic%20lighting%20and%20simulation%20tasks%20in%20visual%20effects%20and%20games.%20To%20obtain%20physically%20realistic%20layouts%2C%20we%20employ%20a%20novel%204-DoF%20differentiable%20optimization%20that%20aligns%20reconstructed%20objects%20with%20the%20estimated%20ground%20plane.%203D-RE-GEN~achieves%20state%20of%20the%20art%20performance%20in%20single%20image%203D%20scene%20reconstruction%2C%20producing%20coherent%2C%20modifiable%20scenes%20through%20compositional%20generation%20guided%20by%20precise%20camera%20recovery%20and%20spatial%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2512.17459v1&entry.124074799=Read"},
{"title": "MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation", "author": "Saikat Roy and Yannick Kirchhoff and Constantin Ulrich and Maximillian Rokuss and Tassilo Wald and Fabian Isensee and Klaus Maier-Hein", "abstract": "Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet", "link": "http://arxiv.org/abs/2512.17774v1", "date": "2025-12-19", "relevancy": 2.7042, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedNeXt-v2%3A%20Scaling%203D%20ConvNeXts%20for%20Large-Scale%20Supervised%20Representation%20Learning%20in%20Medical%20Image%20Segmentation&body=Title%3A%20MedNeXt-v2%3A%20Scaling%203D%20ConvNeXts%20for%20Large-Scale%20Supervised%20Representation%20Learning%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Saikat%20Roy%20and%20Yannick%20Kirchhoff%20and%20Constantin%20Ulrich%20and%20Maximillian%20Rokuss%20and%20Tassilo%20Wald%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20Large-scale%20supervised%20pretraining%20is%20rapidly%20reshaping%203D%20medical%20image%20segmentation.%20However%2C%20existing%20efforts%20focus%20primarily%20on%20increasing%20dataset%20size%20and%20overlook%20the%20question%20of%20whether%20the%20backbone%20network%20is%20an%20effective%20representation%20learner%20at%20scale.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20revisiting%20ConvNeXt-based%20architectures%20for%20volumetric%20segmentation%20and%20introducing%20MedNeXt-v2%2C%20a%20compound-scaled%203D%20ConvNeXt%20that%20leverages%20improved%20micro-architecture%20and%20data%20scaling%20to%20deliver%20state-of-the-art%20performance.%20First%2C%20we%20show%20that%20routinely%20used%20backbones%20in%20large-scale%20pretraining%20pipelines%20are%20often%20suboptimal.%20Subsequently%2C%20we%20use%20comprehensive%20backbone%20benchmarking%20prior%20to%20scaling%20and%20demonstrate%20that%20stronger%20from%20scratch%20performance%20reliably%20predicts%20stronger%20downstream%20performance%20after%20pretraining.%20Guided%20by%20these%20findings%2C%20we%20incorporate%20a%203D%20Global%20Response%20Normalization%20module%20and%20use%20depth%2C%20width%2C%20and%20context%20scaling%20to%20improve%20our%20architecture%20for%20effective%20representation%20learning.%20We%20pretrain%20MedNeXt-v2%20on%2018k%20CT%20volumes%20and%20demonstrate%20state-of-the-art%20performance%20when%20fine-tuning%20across%20six%20challenging%20CT%20and%20MR%20benchmarks%20%28144%20structures%29%2C%20showing%20consistent%20gains%20over%20seven%20publicly%20released%20pretrained%20models.%20Beyond%20improvements%2C%20our%20benchmarking%20of%20these%20models%20also%20reveals%20that%20stronger%20backbones%20yield%20better%20results%20on%20similar%20data%2C%20representation%20scaling%20disproportionately%20benefits%20pathological%20segmentation%2C%20and%20that%20modality-specific%20pretraining%20offers%20negligible%20benefit%20once%20full%20finetuning%20is%20applied.%20In%20conclusion%2C%20our%20results%20establish%20MedNeXt-v2%20as%20a%20strong%20backbone%20for%20large-scale%20supervised%20representation%20learning%20in%203D%20Medical%20Image%20Segmentation.%20Our%20code%20and%20pretrained%20models%20are%20made%20available%20with%20the%20official%20nnUNet%20repository%20at%3A%20https%3A//www.github.com/MIC-DKFZ/nnUNet%0ALink%3A%20http%3A//arxiv.org/abs/2512.17774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedNeXt-v2%253A%2520Scaling%25203D%2520ConvNeXts%2520for%2520Large-Scale%2520Supervised%2520Representation%2520Learning%2520in%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DSaikat%2520Roy%2520and%2520Yannick%2520Kirchhoff%2520and%2520Constantin%2520Ulrich%2520and%2520Maximillian%2520Rokuss%2520and%2520Tassilo%2520Wald%2520and%2520Fabian%2520Isensee%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3DLarge-scale%2520supervised%2520pretraining%2520is%2520rapidly%2520reshaping%25203D%2520medical%2520image%2520segmentation.%2520However%252C%2520existing%2520efforts%2520focus%2520primarily%2520on%2520increasing%2520dataset%2520size%2520and%2520overlook%2520the%2520question%2520of%2520whether%2520the%2520backbone%2520network%2520is%2520an%2520effective%2520representation%2520learner%2520at%2520scale.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520revisiting%2520ConvNeXt-based%2520architectures%2520for%2520volumetric%2520segmentation%2520and%2520introducing%2520MedNeXt-v2%252C%2520a%2520compound-scaled%25203D%2520ConvNeXt%2520that%2520leverages%2520improved%2520micro-architecture%2520and%2520data%2520scaling%2520to%2520deliver%2520state-of-the-art%2520performance.%2520First%252C%2520we%2520show%2520that%2520routinely%2520used%2520backbones%2520in%2520large-scale%2520pretraining%2520pipelines%2520are%2520often%2520suboptimal.%2520Subsequently%252C%2520we%2520use%2520comprehensive%2520backbone%2520benchmarking%2520prior%2520to%2520scaling%2520and%2520demonstrate%2520that%2520stronger%2520from%2520scratch%2520performance%2520reliably%2520predicts%2520stronger%2520downstream%2520performance%2520after%2520pretraining.%2520Guided%2520by%2520these%2520findings%252C%2520we%2520incorporate%2520a%25203D%2520Global%2520Response%2520Normalization%2520module%2520and%2520use%2520depth%252C%2520width%252C%2520and%2520context%2520scaling%2520to%2520improve%2520our%2520architecture%2520for%2520effective%2520representation%2520learning.%2520We%2520pretrain%2520MedNeXt-v2%2520on%252018k%2520CT%2520volumes%2520and%2520demonstrate%2520state-of-the-art%2520performance%2520when%2520fine-tuning%2520across%2520six%2520challenging%2520CT%2520and%2520MR%2520benchmarks%2520%2528144%2520structures%2529%252C%2520showing%2520consistent%2520gains%2520over%2520seven%2520publicly%2520released%2520pretrained%2520models.%2520Beyond%2520improvements%252C%2520our%2520benchmarking%2520of%2520these%2520models%2520also%2520reveals%2520that%2520stronger%2520backbones%2520yield%2520better%2520results%2520on%2520similar%2520data%252C%2520representation%2520scaling%2520disproportionately%2520benefits%2520pathological%2520segmentation%252C%2520and%2520that%2520modality-specific%2520pretraining%2520offers%2520negligible%2520benefit%2520once%2520full%2520finetuning%2520is%2520applied.%2520In%2520conclusion%252C%2520our%2520results%2520establish%2520MedNeXt-v2%2520as%2520a%2520strong%2520backbone%2520for%2520large-scale%2520supervised%2520representation%2520learning%2520in%25203D%2520Medical%2520Image%2520Segmentation.%2520Our%2520code%2520and%2520pretrained%2520models%2520are%2520made%2520available%2520with%2520the%2520official%2520nnUNet%2520repository%2520at%253A%2520https%253A//www.github.com/MIC-DKFZ/nnUNet%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedNeXt-v2%3A%20Scaling%203D%20ConvNeXts%20for%20Large-Scale%20Supervised%20Representation%20Learning%20in%20Medical%20Image%20Segmentation&entry.906535625=Saikat%20Roy%20and%20Yannick%20Kirchhoff%20and%20Constantin%20Ulrich%20and%20Maximillian%20Rokuss%20and%20Tassilo%20Wald%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein&entry.1292438233=Large-scale%20supervised%20pretraining%20is%20rapidly%20reshaping%203D%20medical%20image%20segmentation.%20However%2C%20existing%20efforts%20focus%20primarily%20on%20increasing%20dataset%20size%20and%20overlook%20the%20question%20of%20whether%20the%20backbone%20network%20is%20an%20effective%20representation%20learner%20at%20scale.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20revisiting%20ConvNeXt-based%20architectures%20for%20volumetric%20segmentation%20and%20introducing%20MedNeXt-v2%2C%20a%20compound-scaled%203D%20ConvNeXt%20that%20leverages%20improved%20micro-architecture%20and%20data%20scaling%20to%20deliver%20state-of-the-art%20performance.%20First%2C%20we%20show%20that%20routinely%20used%20backbones%20in%20large-scale%20pretraining%20pipelines%20are%20often%20suboptimal.%20Subsequently%2C%20we%20use%20comprehensive%20backbone%20benchmarking%20prior%20to%20scaling%20and%20demonstrate%20that%20stronger%20from%20scratch%20performance%20reliably%20predicts%20stronger%20downstream%20performance%20after%20pretraining.%20Guided%20by%20these%20findings%2C%20we%20incorporate%20a%203D%20Global%20Response%20Normalization%20module%20and%20use%20depth%2C%20width%2C%20and%20context%20scaling%20to%20improve%20our%20architecture%20for%20effective%20representation%20learning.%20We%20pretrain%20MedNeXt-v2%20on%2018k%20CT%20volumes%20and%20demonstrate%20state-of-the-art%20performance%20when%20fine-tuning%20across%20six%20challenging%20CT%20and%20MR%20benchmarks%20%28144%20structures%29%2C%20showing%20consistent%20gains%20over%20seven%20publicly%20released%20pretrained%20models.%20Beyond%20improvements%2C%20our%20benchmarking%20of%20these%20models%20also%20reveals%20that%20stronger%20backbones%20yield%20better%20results%20on%20similar%20data%2C%20representation%20scaling%20disproportionately%20benefits%20pathological%20segmentation%2C%20and%20that%20modality-specific%20pretraining%20offers%20negligible%20benefit%20once%20full%20finetuning%20is%20applied.%20In%20conclusion%2C%20our%20results%20establish%20MedNeXt-v2%20as%20a%20strong%20backbone%20for%20large-scale%20supervised%20representation%20learning%20in%203D%20Medical%20Image%20Segmentation.%20Our%20code%20and%20pretrained%20models%20are%20made%20available%20with%20the%20official%20nnUNet%20repository%20at%3A%20https%3A//www.github.com/MIC-DKFZ/nnUNet&entry.1838667208=http%3A//arxiv.org/abs/2512.17774v1&entry.124074799=Read"},
{"title": "Adversarial Robustness of Vision in Open Foundation Models", "author": "Jonathon Fox and William J Buchanan and Pavlos Papadopoulos", "abstract": "With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.", "link": "http://arxiv.org/abs/2512.17902v1", "date": "2025-12-19", "relevancy": 2.6992, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5458}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustness%20of%20Vision%20in%20Open%20Foundation%20Models&body=Title%3A%20Adversarial%20Robustness%20of%20Vision%20in%20Open%20Foundation%20Models%0AAuthor%3A%20Jonathon%20Fox%20and%20William%20J%20Buchanan%20and%20Pavlos%20Papadopoulos%0AAbstract%3A%20With%20the%20increase%20in%20deep%20learning%2C%20it%20becomes%20increasingly%20difficult%20to%20understand%20the%20model%20in%20which%20AI%20systems%20can%20identify%20objects.%20Thus%2C%20an%20adversary%20could%20aim%20to%20modify%20an%20image%20by%20adding%20unseen%20elements%2C%20which%20will%20confuse%20the%20AI%20in%20its%20recognition%20of%20an%20entity.%20This%20paper%20thus%20investigates%20the%20adversarial%20robustness%20of%20LLaVA-1.5-13B%20and%20Meta%27s%20Llama%203.2%20Vision-8B-2.%20These%20are%20tested%20for%20untargeted%20PGD%20%28Projected%20Gradient%20Descent%29%20against%20the%20visual%20input%20modality%2C%20and%20empirically%20evaluated%20on%20the%20Visual%20Question%20Answering%20%28VQA%29%20v2%20dataset%20subset.%20The%20results%20of%20these%20adversarial%20attacks%20are%20then%20quantified%20using%20the%20standard%20VQA%20accuracy%20metric.%20This%20evaluation%20is%20then%20compared%20with%20the%20accuracy%20degradation%20%28accuracy%20drop%29%20of%20LLaVA%20and%20Llama%203.2%20Vision.%20A%20key%20finding%20is%20that%20Llama%203.2%20Vision%2C%20despite%20a%20lower%20baseline%20accuracy%20in%20this%20setup%2C%20exhibited%20a%20smaller%20drop%20in%20performance%20under%20attack%20compared%20to%20LLaVA%2C%20particularly%20at%20higher%20perturbation%20levels.%20Overall%2C%20the%20findings%20confirm%20that%20the%20vision%20modality%20represents%20a%20viable%20attack%20vector%20for%20degrading%20the%20performance%20of%20contemporary%20open-weight%20VLMs%2C%20including%20Meta%27s%20Llama%203.2%20Vision.%20Furthermore%2C%20they%20highlight%20that%20adversarial%20robustness%20does%20not%20necessarily%20correlate%20directly%20with%20standard%20benchmark%20performance%20and%20may%20be%20influenced%20by%20underlying%20architectural%20and%20training%20factors.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustness%2520of%2520Vision%2520in%2520Open%2520Foundation%2520Models%26entry.906535625%3DJonathon%2520Fox%2520and%2520William%2520J%2520Buchanan%2520and%2520Pavlos%2520Papadopoulos%26entry.1292438233%3DWith%2520the%2520increase%2520in%2520deep%2520learning%252C%2520it%2520becomes%2520increasingly%2520difficult%2520to%2520understand%2520the%2520model%2520in%2520which%2520AI%2520systems%2520can%2520identify%2520objects.%2520Thus%252C%2520an%2520adversary%2520could%2520aim%2520to%2520modify%2520an%2520image%2520by%2520adding%2520unseen%2520elements%252C%2520which%2520will%2520confuse%2520the%2520AI%2520in%2520its%2520recognition%2520of%2520an%2520entity.%2520This%2520paper%2520thus%2520investigates%2520the%2520adversarial%2520robustness%2520of%2520LLaVA-1.5-13B%2520and%2520Meta%2527s%2520Llama%25203.2%2520Vision-8B-2.%2520These%2520are%2520tested%2520for%2520untargeted%2520PGD%2520%2528Projected%2520Gradient%2520Descent%2529%2520against%2520the%2520visual%2520input%2520modality%252C%2520and%2520empirically%2520evaluated%2520on%2520the%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520v2%2520dataset%2520subset.%2520The%2520results%2520of%2520these%2520adversarial%2520attacks%2520are%2520then%2520quantified%2520using%2520the%2520standard%2520VQA%2520accuracy%2520metric.%2520This%2520evaluation%2520is%2520then%2520compared%2520with%2520the%2520accuracy%2520degradation%2520%2528accuracy%2520drop%2529%2520of%2520LLaVA%2520and%2520Llama%25203.2%2520Vision.%2520A%2520key%2520finding%2520is%2520that%2520Llama%25203.2%2520Vision%252C%2520despite%2520a%2520lower%2520baseline%2520accuracy%2520in%2520this%2520setup%252C%2520exhibited%2520a%2520smaller%2520drop%2520in%2520performance%2520under%2520attack%2520compared%2520to%2520LLaVA%252C%2520particularly%2520at%2520higher%2520perturbation%2520levels.%2520Overall%252C%2520the%2520findings%2520confirm%2520that%2520the%2520vision%2520modality%2520represents%2520a%2520viable%2520attack%2520vector%2520for%2520degrading%2520the%2520performance%2520of%2520contemporary%2520open-weight%2520VLMs%252C%2520including%2520Meta%2527s%2520Llama%25203.2%2520Vision.%2520Furthermore%252C%2520they%2520highlight%2520that%2520adversarial%2520robustness%2520does%2520not%2520necessarily%2520correlate%2520directly%2520with%2520standard%2520benchmark%2520performance%2520and%2520may%2520be%2520influenced%2520by%2520underlying%2520architectural%2520and%2520training%2520factors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20of%20Vision%20in%20Open%20Foundation%20Models&entry.906535625=Jonathon%20Fox%20and%20William%20J%20Buchanan%20and%20Pavlos%20Papadopoulos&entry.1292438233=With%20the%20increase%20in%20deep%20learning%2C%20it%20becomes%20increasingly%20difficult%20to%20understand%20the%20model%20in%20which%20AI%20systems%20can%20identify%20objects.%20Thus%2C%20an%20adversary%20could%20aim%20to%20modify%20an%20image%20by%20adding%20unseen%20elements%2C%20which%20will%20confuse%20the%20AI%20in%20its%20recognition%20of%20an%20entity.%20This%20paper%20thus%20investigates%20the%20adversarial%20robustness%20of%20LLaVA-1.5-13B%20and%20Meta%27s%20Llama%203.2%20Vision-8B-2.%20These%20are%20tested%20for%20untargeted%20PGD%20%28Projected%20Gradient%20Descent%29%20against%20the%20visual%20input%20modality%2C%20and%20empirically%20evaluated%20on%20the%20Visual%20Question%20Answering%20%28VQA%29%20v2%20dataset%20subset.%20The%20results%20of%20these%20adversarial%20attacks%20are%20then%20quantified%20using%20the%20standard%20VQA%20accuracy%20metric.%20This%20evaluation%20is%20then%20compared%20with%20the%20accuracy%20degradation%20%28accuracy%20drop%29%20of%20LLaVA%20and%20Llama%203.2%20Vision.%20A%20key%20finding%20is%20that%20Llama%203.2%20Vision%2C%20despite%20a%20lower%20baseline%20accuracy%20in%20this%20setup%2C%20exhibited%20a%20smaller%20drop%20in%20performance%20under%20attack%20compared%20to%20LLaVA%2C%20particularly%20at%20higher%20perturbation%20levels.%20Overall%2C%20the%20findings%20confirm%20that%20the%20vision%20modality%20represents%20a%20viable%20attack%20vector%20for%20degrading%20the%20performance%20of%20contemporary%20open-weight%20VLMs%2C%20including%20Meta%27s%20Llama%203.2%20Vision.%20Furthermore%2C%20they%20highlight%20that%20adversarial%20robustness%20does%20not%20necessarily%20correlate%20directly%20with%20standard%20benchmark%20performance%20and%20may%20be%20influenced%20by%20underlying%20architectural%20and%20training%20factors.&entry.1838667208=http%3A//arxiv.org/abs/2512.17902v1&entry.124074799=Read"},
{"title": "Equivariant symmetry-aware head pose estimation for fetal MRI", "author": "Ramya Muthukrishnan and Borjan Gagoski and Aryn Lee and P. Ellen Grant and Elfar Adalsteinsson and Polina Golland and Benjamin Billot", "abstract": "We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.", "link": "http://arxiv.org/abs/2512.04890v4", "date": "2025-12-19", "relevancy": 2.6916, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5514}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5318}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20symmetry-aware%20head%20pose%20estimation%20for%20fetal%20MRI&body=Title%3A%20Equivariant%20symmetry-aware%20head%20pose%20estimation%20for%20fetal%20MRI%0AAuthor%3A%20Ramya%20Muthukrishnan%20and%20Borjan%20Gagoski%20and%20Aryn%20Lee%20and%20P.%20Ellen%20Grant%20and%20Elfar%20Adalsteinsson%20and%20Polina%20Golland%20and%20Benjamin%20Billot%0AAbstract%3A%20We%20present%20E%283%29-Pose%2C%20a%20novel%20fast%20pose%20estimation%20method%20that%20jointly%20and%20explicitly%20models%20rotation%20equivariance%20and%20object%20symmetry.%20Our%20work%20is%20motivated%20by%20the%20challenging%20problem%20of%20accounting%20for%20fetal%20head%20motion%20during%20a%20diagnostic%20MRI%20scan.%20We%20aim%20to%20enable%20automatic%20adaptive%20prescription%20of%202D%20diagnostic%20MRI%20slices%20with%206-DoF%20head%20pose%20estimation%2C%20supported%20by%203D%20MRI%20volumes%20rapidly%20acquired%20before%20each%202D%20slice.%20Existing%20methods%20struggle%20to%20generalize%20to%20clinical%20volumes%2C%20due%20to%20pose%20ambiguities%20induced%20by%20inherent%20anatomical%20symmetries%2C%20as%20well%20as%20low%20resolution%2C%20noise%2C%20and%20artifacts.%20In%20contrast%2C%20E%283%29-Pose%20captures%20anatomical%20symmetries%20and%20rigid%20pose%20equivariance%20by%20construction%2C%20and%20yields%20robust%20estimates%20of%20the%20fetal%20head%20pose.%20Our%20experiments%20on%20publicly%20available%20and%20representative%20clinical%20fetal%20MRI%20datasets%20demonstrate%20the%20superior%20robustness%20and%20generalization%20of%20our%20method%20across%20domains.%20Crucially%2C%20E%283%29-Pose%20achieves%20state-of-the-art%20accuracy%20on%20clinical%20MRI%20volumes%2C%20paving%20the%20way%20for%20clinical%20translation.%20Our%20implementation%20is%20available%20at%20github.com/ramyamut/E3-Pose.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04890v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520symmetry-aware%2520head%2520pose%2520estimation%2520for%2520fetal%2520MRI%26entry.906535625%3DRamya%2520Muthukrishnan%2520and%2520Borjan%2520Gagoski%2520and%2520Aryn%2520Lee%2520and%2520P.%2520Ellen%2520Grant%2520and%2520Elfar%2520Adalsteinsson%2520and%2520Polina%2520Golland%2520and%2520Benjamin%2520Billot%26entry.1292438233%3DWe%2520present%2520E%25283%2529-Pose%252C%2520a%2520novel%2520fast%2520pose%2520estimation%2520method%2520that%2520jointly%2520and%2520explicitly%2520models%2520rotation%2520equivariance%2520and%2520object%2520symmetry.%2520Our%2520work%2520is%2520motivated%2520by%2520the%2520challenging%2520problem%2520of%2520accounting%2520for%2520fetal%2520head%2520motion%2520during%2520a%2520diagnostic%2520MRI%2520scan.%2520We%2520aim%2520to%2520enable%2520automatic%2520adaptive%2520prescription%2520of%25202D%2520diagnostic%2520MRI%2520slices%2520with%25206-DoF%2520head%2520pose%2520estimation%252C%2520supported%2520by%25203D%2520MRI%2520volumes%2520rapidly%2520acquired%2520before%2520each%25202D%2520slice.%2520Existing%2520methods%2520struggle%2520to%2520generalize%2520to%2520clinical%2520volumes%252C%2520due%2520to%2520pose%2520ambiguities%2520induced%2520by%2520inherent%2520anatomical%2520symmetries%252C%2520as%2520well%2520as%2520low%2520resolution%252C%2520noise%252C%2520and%2520artifacts.%2520In%2520contrast%252C%2520E%25283%2529-Pose%2520captures%2520anatomical%2520symmetries%2520and%2520rigid%2520pose%2520equivariance%2520by%2520construction%252C%2520and%2520yields%2520robust%2520estimates%2520of%2520the%2520fetal%2520head%2520pose.%2520Our%2520experiments%2520on%2520publicly%2520available%2520and%2520representative%2520clinical%2520fetal%2520MRI%2520datasets%2520demonstrate%2520the%2520superior%2520robustness%2520and%2520generalization%2520of%2520our%2520method%2520across%2520domains.%2520Crucially%252C%2520E%25283%2529-Pose%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520clinical%2520MRI%2520volumes%252C%2520paving%2520the%2520way%2520for%2520clinical%2520translation.%2520Our%2520implementation%2520is%2520available%2520at%2520github.com/ramyamut/E3-Pose.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04890v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20symmetry-aware%20head%20pose%20estimation%20for%20fetal%20MRI&entry.906535625=Ramya%20Muthukrishnan%20and%20Borjan%20Gagoski%20and%20Aryn%20Lee%20and%20P.%20Ellen%20Grant%20and%20Elfar%20Adalsteinsson%20and%20Polina%20Golland%20and%20Benjamin%20Billot&entry.1292438233=We%20present%20E%283%29-Pose%2C%20a%20novel%20fast%20pose%20estimation%20method%20that%20jointly%20and%20explicitly%20models%20rotation%20equivariance%20and%20object%20symmetry.%20Our%20work%20is%20motivated%20by%20the%20challenging%20problem%20of%20accounting%20for%20fetal%20head%20motion%20during%20a%20diagnostic%20MRI%20scan.%20We%20aim%20to%20enable%20automatic%20adaptive%20prescription%20of%202D%20diagnostic%20MRI%20slices%20with%206-DoF%20head%20pose%20estimation%2C%20supported%20by%203D%20MRI%20volumes%20rapidly%20acquired%20before%20each%202D%20slice.%20Existing%20methods%20struggle%20to%20generalize%20to%20clinical%20volumes%2C%20due%20to%20pose%20ambiguities%20induced%20by%20inherent%20anatomical%20symmetries%2C%20as%20well%20as%20low%20resolution%2C%20noise%2C%20and%20artifacts.%20In%20contrast%2C%20E%283%29-Pose%20captures%20anatomical%20symmetries%20and%20rigid%20pose%20equivariance%20by%20construction%2C%20and%20yields%20robust%20estimates%20of%20the%20fetal%20head%20pose.%20Our%20experiments%20on%20publicly%20available%20and%20representative%20clinical%20fetal%20MRI%20datasets%20demonstrate%20the%20superior%20robustness%20and%20generalization%20of%20our%20method%20across%20domains.%20Crucially%2C%20E%283%29-Pose%20achieves%20state-of-the-art%20accuracy%20on%20clinical%20MRI%20volumes%2C%20paving%20the%20way%20for%20clinical%20translation.%20Our%20implementation%20is%20available%20at%20github.com/ramyamut/E3-Pose.&entry.1838667208=http%3A//arxiv.org/abs/2512.04890v4&entry.124074799=Read"},
{"title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars", "author": "Xiaosheng Zhao and Yang Huang and Guirong Xue and Xiao Kong and Jifeng Liu and Xiaoyu Tang and Timothy C. Beers and Yuan-Sen Ting and A-Li Luo", "abstract": "In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP", "link": "http://arxiv.org/abs/2507.01939v4", "date": "2025-12-19", "relevancy": 2.6758, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5348}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars&body=Title%3A%20SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars%0AAuthor%3A%20Xiaosheng%20Zhao%20and%20Yang%20Huang%20and%20Guirong%20Xue%20and%20Xiao%20Kong%20and%20Jifeng%20Liu%20and%20Xiaoyu%20Tang%20and%20Timothy%20C.%20Beers%20and%20Yuan-Sen%20Ting%20and%20A-Li%20Luo%0AAbstract%3A%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20transformed%20natural%20language%20understanding%20through%20vast%20datasets%20and%20large-scale%20parameterization.%20Inspired%20by%20this%20success%2C%20we%20present%20SpecCLIP%2C%20a%20foundation%20model%20framework%20that%20extends%20LLM-inspired%20methodologies%20to%20stellar%20spectral%20analysis.%20Stellar%20spectra%2C%20akin%20to%20structured%20language%2C%20encode%20rich%20physical%20and%20chemical%20information%20about%20stars.%20By%20training%20foundation%20models%20on%20large-scale%20spectral%20datasets%2C%20our%20goal%20is%20to%20learn%20robust%20and%20informative%20embeddings%20that%20support%20diverse%20downstream%20applications.%20As%20a%20proof%20of%20concept%2C%20SpecCLIP%20involves%20pre-training%20on%20two%20spectral%20types--LAMOST%20low-resolution%20and%20Gaia%20XP--followed%20by%20contrastive%20alignment%20using%20the%20CLIP%20%28Contrastive%20Language-Image%20Pre-training%29%20framework%2C%20adapted%20to%20associate%20spectra%20from%20different%20instruments.%20This%20alignment%20is%20complemented%20by%20auxiliary%20decoders%20that%20preserve%20spectrum-specific%20information%20and%20enable%20translation%20%28prediction%29%20between%20spectral%20types%2C%20with%20the%20former%20achieved%20by%20maximizing%20mutual%20information%20between%20embeddings%20and%20input%20spectra.%20The%20result%20is%20a%20cross-spectrum%20framework%20enabling%20intrinsic%20calibration%20and%20flexible%20applications%20across%20instruments.%20We%20demonstrate%20that%20fine-tuning%20these%20models%20on%20moderate-sized%20labeled%20datasets%20improves%20adaptability%20to%20tasks%20such%20as%20stellar-parameter%20estimation%20and%20chemical-abundance%20determination.%20SpecCLIP%20also%20enhances%20the%20accuracy%20and%20precision%20of%20parameter%20estimates%20benchmarked%20against%20external%20survey%20data.%20Additionally%2C%20its%20similarity%20search%20and%20cross-spectrum%20prediction%20capabilities%20offer%20potential%20for%20anomaly%20detection.%20Our%20results%20suggest%20that%20contrastively%20trained%20foundation%20models%20enriched%20with%20spectrum-aware%20decoders%20can%20advance%20precision%20stellar%20spectroscopy.%20Our%20code%20SpecCLIP%20is%20publicly%20available%20at%20https%3A//github.com/Xiaosheng-Zhao/SpecCLIP%0ALink%3A%20http%3A//arxiv.org/abs/2507.01939v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecCLIP%253A%2520Aligning%2520and%2520Translating%2520Spectroscopic%2520Measurements%2520for%2520Stars%26entry.906535625%3DXiaosheng%2520Zhao%2520and%2520Yang%2520Huang%2520and%2520Guirong%2520Xue%2520and%2520Xiao%2520Kong%2520and%2520Jifeng%2520Liu%2520and%2520Xiaoyu%2520Tang%2520and%2520Timothy%2520C.%2520Beers%2520and%2520Yuan-Sen%2520Ting%2520and%2520A-Li%2520Luo%26entry.1292438233%3DIn%2520recent%2520years%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520transformed%2520natural%2520language%2520understanding%2520through%2520vast%2520datasets%2520and%2520large-scale%2520parameterization.%2520Inspired%2520by%2520this%2520success%252C%2520we%2520present%2520SpecCLIP%252C%2520a%2520foundation%2520model%2520framework%2520that%2520extends%2520LLM-inspired%2520methodologies%2520to%2520stellar%2520spectral%2520analysis.%2520Stellar%2520spectra%252C%2520akin%2520to%2520structured%2520language%252C%2520encode%2520rich%2520physical%2520and%2520chemical%2520information%2520about%2520stars.%2520By%2520training%2520foundation%2520models%2520on%2520large-scale%2520spectral%2520datasets%252C%2520our%2520goal%2520is%2520to%2520learn%2520robust%2520and%2520informative%2520embeddings%2520that%2520support%2520diverse%2520downstream%2520applications.%2520As%2520a%2520proof%2520of%2520concept%252C%2520SpecCLIP%2520involves%2520pre-training%2520on%2520two%2520spectral%2520types--LAMOST%2520low-resolution%2520and%2520Gaia%2520XP--followed%2520by%2520contrastive%2520alignment%2520using%2520the%2520CLIP%2520%2528Contrastive%2520Language-Image%2520Pre-training%2529%2520framework%252C%2520adapted%2520to%2520associate%2520spectra%2520from%2520different%2520instruments.%2520This%2520alignment%2520is%2520complemented%2520by%2520auxiliary%2520decoders%2520that%2520preserve%2520spectrum-specific%2520information%2520and%2520enable%2520translation%2520%2528prediction%2529%2520between%2520spectral%2520types%252C%2520with%2520the%2520former%2520achieved%2520by%2520maximizing%2520mutual%2520information%2520between%2520embeddings%2520and%2520input%2520spectra.%2520The%2520result%2520is%2520a%2520cross-spectrum%2520framework%2520enabling%2520intrinsic%2520calibration%2520and%2520flexible%2520applications%2520across%2520instruments.%2520We%2520demonstrate%2520that%2520fine-tuning%2520these%2520models%2520on%2520moderate-sized%2520labeled%2520datasets%2520improves%2520adaptability%2520to%2520tasks%2520such%2520as%2520stellar-parameter%2520estimation%2520and%2520chemical-abundance%2520determination.%2520SpecCLIP%2520also%2520enhances%2520the%2520accuracy%2520and%2520precision%2520of%2520parameter%2520estimates%2520benchmarked%2520against%2520external%2520survey%2520data.%2520Additionally%252C%2520its%2520similarity%2520search%2520and%2520cross-spectrum%2520prediction%2520capabilities%2520offer%2520potential%2520for%2520anomaly%2520detection.%2520Our%2520results%2520suggest%2520that%2520contrastively%2520trained%2520foundation%2520models%2520enriched%2520with%2520spectrum-aware%2520decoders%2520can%2520advance%2520precision%2520stellar%2520spectroscopy.%2520Our%2520code%2520SpecCLIP%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Xiaosheng-Zhao/SpecCLIP%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01939v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars&entry.906535625=Xiaosheng%20Zhao%20and%20Yang%20Huang%20and%20Guirong%20Xue%20and%20Xiao%20Kong%20and%20Jifeng%20Liu%20and%20Xiaoyu%20Tang%20and%20Timothy%20C.%20Beers%20and%20Yuan-Sen%20Ting%20and%20A-Li%20Luo&entry.1292438233=In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20transformed%20natural%20language%20understanding%20through%20vast%20datasets%20and%20large-scale%20parameterization.%20Inspired%20by%20this%20success%2C%20we%20present%20SpecCLIP%2C%20a%20foundation%20model%20framework%20that%20extends%20LLM-inspired%20methodologies%20to%20stellar%20spectral%20analysis.%20Stellar%20spectra%2C%20akin%20to%20structured%20language%2C%20encode%20rich%20physical%20and%20chemical%20information%20about%20stars.%20By%20training%20foundation%20models%20on%20large-scale%20spectral%20datasets%2C%20our%20goal%20is%20to%20learn%20robust%20and%20informative%20embeddings%20that%20support%20diverse%20downstream%20applications.%20As%20a%20proof%20of%20concept%2C%20SpecCLIP%20involves%20pre-training%20on%20two%20spectral%20types--LAMOST%20low-resolution%20and%20Gaia%20XP--followed%20by%20contrastive%20alignment%20using%20the%20CLIP%20%28Contrastive%20Language-Image%20Pre-training%29%20framework%2C%20adapted%20to%20associate%20spectra%20from%20different%20instruments.%20This%20alignment%20is%20complemented%20by%20auxiliary%20decoders%20that%20preserve%20spectrum-specific%20information%20and%20enable%20translation%20%28prediction%29%20between%20spectral%20types%2C%20with%20the%20former%20achieved%20by%20maximizing%20mutual%20information%20between%20embeddings%20and%20input%20spectra.%20The%20result%20is%20a%20cross-spectrum%20framework%20enabling%20intrinsic%20calibration%20and%20flexible%20applications%20across%20instruments.%20We%20demonstrate%20that%20fine-tuning%20these%20models%20on%20moderate-sized%20labeled%20datasets%20improves%20adaptability%20to%20tasks%20such%20as%20stellar-parameter%20estimation%20and%20chemical-abundance%20determination.%20SpecCLIP%20also%20enhances%20the%20accuracy%20and%20precision%20of%20parameter%20estimates%20benchmarked%20against%20external%20survey%20data.%20Additionally%2C%20its%20similarity%20search%20and%20cross-spectrum%20prediction%20capabilities%20offer%20potential%20for%20anomaly%20detection.%20Our%20results%20suggest%20that%20contrastively%20trained%20foundation%20models%20enriched%20with%20spectrum-aware%20decoders%20can%20advance%20precision%20stellar%20spectroscopy.%20Our%20code%20SpecCLIP%20is%20publicly%20available%20at%20https%3A//github.com/Xiaosheng-Zhao/SpecCLIP&entry.1838667208=http%3A//arxiv.org/abs/2507.01939v4&entry.124074799=Read"},
{"title": "Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training", "author": "Kristoffer Wickstr\u00f8m and Teresa Dorszewski and Siyan Chen and Michael Kampffmeyer and Elisabeth Wetzer and Robert Jenssen", "abstract": "Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.", "link": "http://arxiv.org/abs/2512.17891v1", "date": "2025-12-19", "relevancy": 2.6672, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keypoint%20Counting%20Classifiers%3A%20Turning%20Vision%20Transformers%20into%20Self-Explainable%20Models%20Without%20Training&body=Title%3A%20Keypoint%20Counting%20Classifiers%3A%20Turning%20Vision%20Transformers%20into%20Self-Explainable%20Models%20Without%20Training%0AAuthor%3A%20Kristoffer%20Wickstr%C3%B8m%20and%20Teresa%20Dorszewski%20and%20Siyan%20Chen%20and%20Michael%20Kampffmeyer%20and%20Elisabeth%20Wetzer%20and%20Robert%20Jenssen%0AAbstract%3A%20Current%20approaches%20for%20designing%20self-explainable%20models%20%28SEMs%29%20require%20complicated%20training%20procedures%20and%20specific%20architectures%20which%20makes%20them%20impractical.%20With%20the%20advance%20of%20general%20purpose%20foundation%20models%20based%20on%20Vision%20Transformers%20%28ViTs%29%2C%20this%20impracticability%20becomes%20even%20more%20problematic.%20Therefore%2C%20new%20methods%20are%20necessary%20to%20provide%20transparency%20and%20reliability%20to%20ViT-based%20foundation%20models.%20In%20this%20work%2C%20we%20present%20a%20new%20method%20for%20turning%20any%20well-trained%20ViT-based%20model%20into%20a%20SEM%20without%20retraining%2C%20which%20we%20call%20Keypoint%20Counting%20Classifiers%20%28KCCs%29.%20Recent%20works%20have%20shown%20that%20ViTs%20can%20automatically%20identify%20matching%20keypoints%20between%20images%20with%20high%20precision%2C%20and%20we%20build%20on%20these%20results%20to%20create%20an%20easily%20interpretable%20decision%20process%20that%20is%20inherently%20visualizable%20in%20the%20input.%20We%20perform%20an%20extensive%20evaluation%20which%20show%20that%20KCCs%20improve%20the%20human-machine%20communication%20compared%20to%20recent%20baselines.%20We%20believe%20that%20KCCs%20constitute%20an%20important%20step%20towards%20making%20ViT-based%20foundation%20models%20more%20transparent%20and%20reliable.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeypoint%2520Counting%2520Classifiers%253A%2520Turning%2520Vision%2520Transformers%2520into%2520Self-Explainable%2520Models%2520Without%2520Training%26entry.906535625%3DKristoffer%2520Wickstr%25C3%25B8m%2520and%2520Teresa%2520Dorszewski%2520and%2520Siyan%2520Chen%2520and%2520Michael%2520Kampffmeyer%2520and%2520Elisabeth%2520Wetzer%2520and%2520Robert%2520Jenssen%26entry.1292438233%3DCurrent%2520approaches%2520for%2520designing%2520self-explainable%2520models%2520%2528SEMs%2529%2520require%2520complicated%2520training%2520procedures%2520and%2520specific%2520architectures%2520which%2520makes%2520them%2520impractical.%2520With%2520the%2520advance%2520of%2520general%2520purpose%2520foundation%2520models%2520based%2520on%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520this%2520impracticability%2520becomes%2520even%2520more%2520problematic.%2520Therefore%252C%2520new%2520methods%2520are%2520necessary%2520to%2520provide%2520transparency%2520and%2520reliability%2520to%2520ViT-based%2520foundation%2520models.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%2520method%2520for%2520turning%2520any%2520well-trained%2520ViT-based%2520model%2520into%2520a%2520SEM%2520without%2520retraining%252C%2520which%2520we%2520call%2520Keypoint%2520Counting%2520Classifiers%2520%2528KCCs%2529.%2520Recent%2520works%2520have%2520shown%2520that%2520ViTs%2520can%2520automatically%2520identify%2520matching%2520keypoints%2520between%2520images%2520with%2520high%2520precision%252C%2520and%2520we%2520build%2520on%2520these%2520results%2520to%2520create%2520an%2520easily%2520interpretable%2520decision%2520process%2520that%2520is%2520inherently%2520visualizable%2520in%2520the%2520input.%2520We%2520perform%2520an%2520extensive%2520evaluation%2520which%2520show%2520that%2520KCCs%2520improve%2520the%2520human-machine%2520communication%2520compared%2520to%2520recent%2520baselines.%2520We%2520believe%2520that%2520KCCs%2520constitute%2520an%2520important%2520step%2520towards%2520making%2520ViT-based%2520foundation%2520models%2520more%2520transparent%2520and%2520reliable.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keypoint%20Counting%20Classifiers%3A%20Turning%20Vision%20Transformers%20into%20Self-Explainable%20Models%20Without%20Training&entry.906535625=Kristoffer%20Wickstr%C3%B8m%20and%20Teresa%20Dorszewski%20and%20Siyan%20Chen%20and%20Michael%20Kampffmeyer%20and%20Elisabeth%20Wetzer%20and%20Robert%20Jenssen&entry.1292438233=Current%20approaches%20for%20designing%20self-explainable%20models%20%28SEMs%29%20require%20complicated%20training%20procedures%20and%20specific%20architectures%20which%20makes%20them%20impractical.%20With%20the%20advance%20of%20general%20purpose%20foundation%20models%20based%20on%20Vision%20Transformers%20%28ViTs%29%2C%20this%20impracticability%20becomes%20even%20more%20problematic.%20Therefore%2C%20new%20methods%20are%20necessary%20to%20provide%20transparency%20and%20reliability%20to%20ViT-based%20foundation%20models.%20In%20this%20work%2C%20we%20present%20a%20new%20method%20for%20turning%20any%20well-trained%20ViT-based%20model%20into%20a%20SEM%20without%20retraining%2C%20which%20we%20call%20Keypoint%20Counting%20Classifiers%20%28KCCs%29.%20Recent%20works%20have%20shown%20that%20ViTs%20can%20automatically%20identify%20matching%20keypoints%20between%20images%20with%20high%20precision%2C%20and%20we%20build%20on%20these%20results%20to%20create%20an%20easily%20interpretable%20decision%20process%20that%20is%20inherently%20visualizable%20in%20the%20input.%20We%20perform%20an%20extensive%20evaluation%20which%20show%20that%20KCCs%20improve%20the%20human-machine%20communication%20compared%20to%20recent%20baselines.%20We%20believe%20that%20KCCs%20constitute%20an%20important%20step%20towards%20making%20ViT-based%20foundation%20models%20more%20transparent%20and%20reliable.&entry.1838667208=http%3A//arxiv.org/abs/2512.17891v1&entry.124074799=Read"},
{"title": "ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image", "author": "Yunqi Gao and Leyuan Liu and Yuhan Li and Changxin Gao and Yuanyuan Liu and Jingying Chen", "abstract": "With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \\url{https://github.com/starVisionTeam/ClothHMR}.", "link": "http://arxiv.org/abs/2512.17545v1", "date": "2025-12-19", "relevancy": 2.6354, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6741}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6527}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClothHMR%3A%203D%20Mesh%20Recovery%20of%20Humans%20in%20Diverse%20Clothing%20from%20Single%20Image&body=Title%3A%20ClothHMR%3A%203D%20Mesh%20Recovery%20of%20Humans%20in%20Diverse%20Clothing%20from%20Single%20Image%0AAuthor%3A%20Yunqi%20Gao%20and%20Leyuan%20Liu%20and%20Yuhan%20Li%20and%20Changxin%20Gao%20and%20Yuanyuan%20Liu%20and%20Jingying%20Chen%0AAbstract%3A%20With%203D%20data%20rapidly%20emerging%20as%20an%20important%20form%20of%20multimedia%20information%2C%203D%20human%20mesh%20recovery%20technology%20has%20also%20advanced%20accordingly.%20However%2C%20current%20methods%20mainly%20focus%20on%20handling%20humans%20wearing%20tight%20clothing%20and%20perform%20poorly%20when%20estimating%20body%20shapes%20and%20poses%20under%20diverse%20clothing%2C%20especially%20loose%20garments.%20To%20this%20end%2C%20we%20make%20two%20key%20insights%3A%20%281%29%20tailoring%20clothing%20to%20fit%20the%20human%20body%20can%20mitigate%20the%20adverse%20impact%20of%20clothing%20on%203D%20human%20mesh%20recovery%2C%20and%20%282%29%20utilizing%20human%20visual%20information%20from%20large%20foundational%20models%20can%20enhance%20the%20generalization%20ability%20of%20the%20estimation.%20Based%20on%20these%20insights%2C%20we%20propose%20ClothHMR%2C%20to%20accurately%20recover%203D%20meshes%20of%20humans%20in%20diverse%20clothing.%20ClothHMR%20primarily%20consists%20of%20two%20modules%3A%20clothing%20tailoring%20%28CT%29%20and%20FHVM-based%20mesh%20recovering%20%28MR%29.%20The%20CT%20module%20employs%20body%20semantic%20estimation%20and%20body%20edge%20prediction%20to%20tailor%20the%20clothing%2C%20ensuring%20it%20fits%20the%20body%20silhouette.%20The%20MR%20module%20optimizes%20the%20initial%20parameters%20of%20the%203D%20human%20mesh%20by%20continuously%20aligning%20the%20intermediate%20representations%20of%20the%203D%20mesh%20with%20those%20inferred%20from%20the%20foundational%20human%20visual%20model%20%28FHVM%29.%20ClothHMR%20can%20accurately%20recover%203D%20meshes%20of%20humans%20wearing%20diverse%20clothing%2C%20precisely%20estimating%20their%20body%20shapes%20and%20poses.%20Experimental%20results%20demonstrate%20that%20ClothHMR%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20across%20benchmark%20datasets%20and%20in-the-wild%20images.%20Additionally%2C%20a%20web%20application%20for%20online%20fashion%20and%20shopping%20powered%20by%20ClothHMR%20is%20developed%2C%20illustrating%20that%20ClothHMR%20can%20effectively%20serve%20real-world%20usage%20scenarios.%20The%20code%20and%20model%20for%20ClothHMR%20are%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/starVisionTeam/ClothHMR%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClothHMR%253A%25203D%2520Mesh%2520Recovery%2520of%2520Humans%2520in%2520Diverse%2520Clothing%2520from%2520Single%2520Image%26entry.906535625%3DYunqi%2520Gao%2520and%2520Leyuan%2520Liu%2520and%2520Yuhan%2520Li%2520and%2520Changxin%2520Gao%2520and%2520Yuanyuan%2520Liu%2520and%2520Jingying%2520Chen%26entry.1292438233%3DWith%25203D%2520data%2520rapidly%2520emerging%2520as%2520an%2520important%2520form%2520of%2520multimedia%2520information%252C%25203D%2520human%2520mesh%2520recovery%2520technology%2520has%2520also%2520advanced%2520accordingly.%2520However%252C%2520current%2520methods%2520mainly%2520focus%2520on%2520handling%2520humans%2520wearing%2520tight%2520clothing%2520and%2520perform%2520poorly%2520when%2520estimating%2520body%2520shapes%2520and%2520poses%2520under%2520diverse%2520clothing%252C%2520especially%2520loose%2520garments.%2520To%2520this%2520end%252C%2520we%2520make%2520two%2520key%2520insights%253A%2520%25281%2529%2520tailoring%2520clothing%2520to%2520fit%2520the%2520human%2520body%2520can%2520mitigate%2520the%2520adverse%2520impact%2520of%2520clothing%2520on%25203D%2520human%2520mesh%2520recovery%252C%2520and%2520%25282%2529%2520utilizing%2520human%2520visual%2520information%2520from%2520large%2520foundational%2520models%2520can%2520enhance%2520the%2520generalization%2520ability%2520of%2520the%2520estimation.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520ClothHMR%252C%2520to%2520accurately%2520recover%25203D%2520meshes%2520of%2520humans%2520in%2520diverse%2520clothing.%2520ClothHMR%2520primarily%2520consists%2520of%2520two%2520modules%253A%2520clothing%2520tailoring%2520%2528CT%2529%2520and%2520FHVM-based%2520mesh%2520recovering%2520%2528MR%2529.%2520The%2520CT%2520module%2520employs%2520body%2520semantic%2520estimation%2520and%2520body%2520edge%2520prediction%2520to%2520tailor%2520the%2520clothing%252C%2520ensuring%2520it%2520fits%2520the%2520body%2520silhouette.%2520The%2520MR%2520module%2520optimizes%2520the%2520initial%2520parameters%2520of%2520the%25203D%2520human%2520mesh%2520by%2520continuously%2520aligning%2520the%2520intermediate%2520representations%2520of%2520the%25203D%2520mesh%2520with%2520those%2520inferred%2520from%2520the%2520foundational%2520human%2520visual%2520model%2520%2528FHVM%2529.%2520ClothHMR%2520can%2520accurately%2520recover%25203D%2520meshes%2520of%2520humans%2520wearing%2520diverse%2520clothing%252C%2520precisely%2520estimating%2520their%2520body%2520shapes%2520and%2520poses.%2520Experimental%2520results%2520demonstrate%2520that%2520ClothHMR%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520across%2520benchmark%2520datasets%2520and%2520in-the-wild%2520images.%2520Additionally%252C%2520a%2520web%2520application%2520for%2520online%2520fashion%2520and%2520shopping%2520powered%2520by%2520ClothHMR%2520is%2520developed%252C%2520illustrating%2520that%2520ClothHMR%2520can%2520effectively%2520serve%2520real-world%2520usage%2520scenarios.%2520The%2520code%2520and%2520model%2520for%2520ClothHMR%2520are%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/starVisionTeam/ClothHMR%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClothHMR%3A%203D%20Mesh%20Recovery%20of%20Humans%20in%20Diverse%20Clothing%20from%20Single%20Image&entry.906535625=Yunqi%20Gao%20and%20Leyuan%20Liu%20and%20Yuhan%20Li%20and%20Changxin%20Gao%20and%20Yuanyuan%20Liu%20and%20Jingying%20Chen&entry.1292438233=With%203D%20data%20rapidly%20emerging%20as%20an%20important%20form%20of%20multimedia%20information%2C%203D%20human%20mesh%20recovery%20technology%20has%20also%20advanced%20accordingly.%20However%2C%20current%20methods%20mainly%20focus%20on%20handling%20humans%20wearing%20tight%20clothing%20and%20perform%20poorly%20when%20estimating%20body%20shapes%20and%20poses%20under%20diverse%20clothing%2C%20especially%20loose%20garments.%20To%20this%20end%2C%20we%20make%20two%20key%20insights%3A%20%281%29%20tailoring%20clothing%20to%20fit%20the%20human%20body%20can%20mitigate%20the%20adverse%20impact%20of%20clothing%20on%203D%20human%20mesh%20recovery%2C%20and%20%282%29%20utilizing%20human%20visual%20information%20from%20large%20foundational%20models%20can%20enhance%20the%20generalization%20ability%20of%20the%20estimation.%20Based%20on%20these%20insights%2C%20we%20propose%20ClothHMR%2C%20to%20accurately%20recover%203D%20meshes%20of%20humans%20in%20diverse%20clothing.%20ClothHMR%20primarily%20consists%20of%20two%20modules%3A%20clothing%20tailoring%20%28CT%29%20and%20FHVM-based%20mesh%20recovering%20%28MR%29.%20The%20CT%20module%20employs%20body%20semantic%20estimation%20and%20body%20edge%20prediction%20to%20tailor%20the%20clothing%2C%20ensuring%20it%20fits%20the%20body%20silhouette.%20The%20MR%20module%20optimizes%20the%20initial%20parameters%20of%20the%203D%20human%20mesh%20by%20continuously%20aligning%20the%20intermediate%20representations%20of%20the%203D%20mesh%20with%20those%20inferred%20from%20the%20foundational%20human%20visual%20model%20%28FHVM%29.%20ClothHMR%20can%20accurately%20recover%203D%20meshes%20of%20humans%20wearing%20diverse%20clothing%2C%20precisely%20estimating%20their%20body%20shapes%20and%20poses.%20Experimental%20results%20demonstrate%20that%20ClothHMR%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20across%20benchmark%20datasets%20and%20in-the-wild%20images.%20Additionally%2C%20a%20web%20application%20for%20online%20fashion%20and%20shopping%20powered%20by%20ClothHMR%20is%20developed%2C%20illustrating%20that%20ClothHMR%20can%20effectively%20serve%20real-world%20usage%20scenarios.%20The%20code%20and%20model%20for%20ClothHMR%20are%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/starVisionTeam/ClothHMR%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.17545v1&entry.124074799=Read"},
{"title": "Dexterous World Models", "author": "Byungjun Kim and Taeksoo Kim and Junyoung Lee and Hanbyul Joo", "abstract": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.\n  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.\n  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.", "link": "http://arxiv.org/abs/2512.17907v1", "date": "2025-12-19", "relevancy": 2.5752, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6825}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6431}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dexterous%20World%20Models&body=Title%3A%20Dexterous%20World%20Models%0AAuthor%3A%20Byungjun%20Kim%20and%20Taeksoo%20Kim%20and%20Junyoung%20Lee%20and%20Hanbyul%20Joo%0AAbstract%3A%20Recent%20progress%20in%203D%20reconstruction%20has%20made%20it%20easy%20to%20create%20realistic%20digital%20twins%20from%20everyday%20environments.%20However%2C%20current%20digital%20twins%20remain%20largely%20static%20and%20are%20limited%20to%20navigation%20and%20view%20synthesis%20without%20embodied%20interactivity.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Dexterous%20World%20Model%20%28DWM%29%2C%20a%20scene-action-conditioned%20video%20diffusion%20framework%20that%20models%20how%20dexterous%20human%20actions%20induce%20dynamic%20changes%20in%20static%203D%20scenes.%0A%20%20Given%20a%20static%203D%20scene%20rendering%20and%20an%20egocentric%20hand%20motion%20sequence%2C%20DWM%20generates%20temporally%20coherent%20videos%20depicting%20plausible%20human-scene%20interactions.%20Our%20approach%20conditions%20video%20generation%20on%20%281%29%20static%20scene%20renderings%20following%20a%20specified%20camera%20trajectory%20to%20ensure%20spatial%20consistency%2C%20and%20%282%29%20egocentric%20hand%20mesh%20renderings%20that%20encode%20both%20geometry%20and%20motion%20cues%20to%20model%20action-conditioned%20dynamics%20directly.%20To%20train%20DWM%2C%20we%20construct%20a%20hybrid%20interaction%20video%20dataset.%20Synthetic%20egocentric%20interactions%20provide%20fully%20aligned%20supervision%20for%20joint%20locomotion%20and%20manipulation%20learning%2C%20while%20fixed-camera%20real-world%20videos%20contribute%20diverse%20and%20realistic%20object%20dynamics.%0A%20%20Experiments%20demonstrate%20that%20DWM%20enables%20realistic%20and%20physically%20plausible%20interactions%2C%20such%20as%20grasping%2C%20opening%2C%20and%20moving%20objects%2C%20while%20maintaining%20camera%20and%20scene%20consistency.%20This%20framework%20represents%20a%20first%20step%20toward%20video%20diffusion-based%20interactive%20digital%20twins%20and%20enables%20embodied%20simulation%20from%20egocentric%20actions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexterous%2520World%2520Models%26entry.906535625%3DByungjun%2520Kim%2520and%2520Taeksoo%2520Kim%2520and%2520Junyoung%2520Lee%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3DRecent%2520progress%2520in%25203D%2520reconstruction%2520has%2520made%2520it%2520easy%2520to%2520create%2520realistic%2520digital%2520twins%2520from%2520everyday%2520environments.%2520However%252C%2520current%2520digital%2520twins%2520remain%2520largely%2520static%2520and%2520are%2520limited%2520to%2520navigation%2520and%2520view%2520synthesis%2520without%2520embodied%2520interactivity.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Dexterous%2520World%2520Model%2520%2528DWM%2529%252C%2520a%2520scene-action-conditioned%2520video%2520diffusion%2520framework%2520that%2520models%2520how%2520dexterous%2520human%2520actions%2520induce%2520dynamic%2520changes%2520in%2520static%25203D%2520scenes.%250A%2520%2520Given%2520a%2520static%25203D%2520scene%2520rendering%2520and%2520an%2520egocentric%2520hand%2520motion%2520sequence%252C%2520DWM%2520generates%2520temporally%2520coherent%2520videos%2520depicting%2520plausible%2520human-scene%2520interactions.%2520Our%2520approach%2520conditions%2520video%2520generation%2520on%2520%25281%2529%2520static%2520scene%2520renderings%2520following%2520a%2520specified%2520camera%2520trajectory%2520to%2520ensure%2520spatial%2520consistency%252C%2520and%2520%25282%2529%2520egocentric%2520hand%2520mesh%2520renderings%2520that%2520encode%2520both%2520geometry%2520and%2520motion%2520cues%2520to%2520model%2520action-conditioned%2520dynamics%2520directly.%2520To%2520train%2520DWM%252C%2520we%2520construct%2520a%2520hybrid%2520interaction%2520video%2520dataset.%2520Synthetic%2520egocentric%2520interactions%2520provide%2520fully%2520aligned%2520supervision%2520for%2520joint%2520locomotion%2520and%2520manipulation%2520learning%252C%2520while%2520fixed-camera%2520real-world%2520videos%2520contribute%2520diverse%2520and%2520realistic%2520object%2520dynamics.%250A%2520%2520Experiments%2520demonstrate%2520that%2520DWM%2520enables%2520realistic%2520and%2520physically%2520plausible%2520interactions%252C%2520such%2520as%2520grasping%252C%2520opening%252C%2520and%2520moving%2520objects%252C%2520while%2520maintaining%2520camera%2520and%2520scene%2520consistency.%2520This%2520framework%2520represents%2520a%2520first%2520step%2520toward%2520video%2520diffusion-based%2520interactive%2520digital%2520twins%2520and%2520enables%2520embodied%2520simulation%2520from%2520egocentric%2520actions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dexterous%20World%20Models&entry.906535625=Byungjun%20Kim%20and%20Taeksoo%20Kim%20and%20Junyoung%20Lee%20and%20Hanbyul%20Joo&entry.1292438233=Recent%20progress%20in%203D%20reconstruction%20has%20made%20it%20easy%20to%20create%20realistic%20digital%20twins%20from%20everyday%20environments.%20However%2C%20current%20digital%20twins%20remain%20largely%20static%20and%20are%20limited%20to%20navigation%20and%20view%20synthesis%20without%20embodied%20interactivity.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Dexterous%20World%20Model%20%28DWM%29%2C%20a%20scene-action-conditioned%20video%20diffusion%20framework%20that%20models%20how%20dexterous%20human%20actions%20induce%20dynamic%20changes%20in%20static%203D%20scenes.%0A%20%20Given%20a%20static%203D%20scene%20rendering%20and%20an%20egocentric%20hand%20motion%20sequence%2C%20DWM%20generates%20temporally%20coherent%20videos%20depicting%20plausible%20human-scene%20interactions.%20Our%20approach%20conditions%20video%20generation%20on%20%281%29%20static%20scene%20renderings%20following%20a%20specified%20camera%20trajectory%20to%20ensure%20spatial%20consistency%2C%20and%20%282%29%20egocentric%20hand%20mesh%20renderings%20that%20encode%20both%20geometry%20and%20motion%20cues%20to%20model%20action-conditioned%20dynamics%20directly.%20To%20train%20DWM%2C%20we%20construct%20a%20hybrid%20interaction%20video%20dataset.%20Synthetic%20egocentric%20interactions%20provide%20fully%20aligned%20supervision%20for%20joint%20locomotion%20and%20manipulation%20learning%2C%20while%20fixed-camera%20real-world%20videos%20contribute%20diverse%20and%20realistic%20object%20dynamics.%0A%20%20Experiments%20demonstrate%20that%20DWM%20enables%20realistic%20and%20physically%20plausible%20interactions%2C%20such%20as%20grasping%2C%20opening%2C%20and%20moving%20objects%2C%20while%20maintaining%20camera%20and%20scene%20consistency.%20This%20framework%20represents%20a%20first%20step%20toward%20video%20diffusion-based%20interactive%20digital%20twins%20and%20enables%20embodied%20simulation%20from%20egocentric%20actions.&entry.1838667208=http%3A//arxiv.org/abs/2512.17907v1&entry.124074799=Read"},
{"title": "FakeParts: a New Family of AI-Generated DeepFakes", "author": "Ziyi Liu and Firas Gabetni and Awais Hussain Sani and Xi Wang and Soobash Daiboo and Gaetan Brison and Gianni Franchi and Vicky Kalogeiton", "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.", "link": "http://arxiv.org/abs/2508.21052v2", "date": "2025-12-19", "relevancy": 2.5405, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5284}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5054}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakeParts%3A%20a%20New%20Family%20of%20AI-Generated%20DeepFakes&body=Title%3A%20FakeParts%3A%20a%20New%20Family%20of%20AI-Generated%20DeepFakes%0AAuthor%3A%20Ziyi%20Liu%20and%20Firas%20Gabetni%20and%20Awais%20Hussain%20Sani%20and%20Xi%20Wang%20and%20Soobash%20Daiboo%20and%20Gaetan%20Brison%20and%20Gianni%20Franchi%20and%20Vicky%20Kalogeiton%0AAbstract%3A%20We%20introduce%20FakeParts%2C%20a%20new%20class%20of%20deepfakes%20characterized%20by%20subtle%2C%20localized%20manipulations%20to%20specific%20spatial%20regions%20or%20temporal%20segments%20of%20otherwise%20authentic%20videos.%20Unlike%20fully%20synthetic%20content%2C%20these%20partial%20manipulations%20-%20ranging%20from%20altered%20facial%20expressions%20to%20object%20substitutions%20and%20background%20modifications%20-%20blend%20seamlessly%20with%20real%20elements%2C%20making%20them%20particularly%20deceptive%20and%20difficult%20to%20detect.%20To%20address%20the%20critical%20gap%20in%20detection%2C%20we%20present%20FakePartsBench%2C%20the%20first%20large-scale%20benchmark%20specifically%20designed%20to%20capture%20the%20full%20spectrum%20of%20partial%20deepfakes.%20Comprising%20over%2081K%20%28including%2044K%20FakeParts%29%20videos%20with%20pixel-%20and%20frame-level%20manipulation%20annotations%2C%20our%20dataset%20enables%20comprehensive%20evaluation%20of%20detection%20methods.%20Our%20user%20studies%20demonstrate%20that%20FakeParts%20reduces%20human%20detection%20accuracy%20by%20up%20to%2026%25%20compared%20to%20traditional%20deepfakes%2C%20with%20similar%20performance%20degradation%20observed%20in%20state-of-the-art%20detection%20models.%20This%20work%20identifies%20an%20urgent%20vulnerability%20in%20current%20detectors%20and%20provides%20the%20necessary%20resources%20to%20develop%20methods%20robust%20to%20partial%20manipulations.%0ALink%3A%20http%3A//arxiv.org/abs/2508.21052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakeParts%253A%2520a%2520New%2520Family%2520of%2520AI-Generated%2520DeepFakes%26entry.906535625%3DZiyi%2520Liu%2520and%2520Firas%2520Gabetni%2520and%2520Awais%2520Hussain%2520Sani%2520and%2520Xi%2520Wang%2520and%2520Soobash%2520Daiboo%2520and%2520Gaetan%2520Brison%2520and%2520Gianni%2520Franchi%2520and%2520Vicky%2520Kalogeiton%26entry.1292438233%3DWe%2520introduce%2520FakeParts%252C%2520a%2520new%2520class%2520of%2520deepfakes%2520characterized%2520by%2520subtle%252C%2520localized%2520manipulations%2520to%2520specific%2520spatial%2520regions%2520or%2520temporal%2520segments%2520of%2520otherwise%2520authentic%2520videos.%2520Unlike%2520fully%2520synthetic%2520content%252C%2520these%2520partial%2520manipulations%2520-%2520ranging%2520from%2520altered%2520facial%2520expressions%2520to%2520object%2520substitutions%2520and%2520background%2520modifications%2520-%2520blend%2520seamlessly%2520with%2520real%2520elements%252C%2520making%2520them%2520particularly%2520deceptive%2520and%2520difficult%2520to%2520detect.%2520To%2520address%2520the%2520critical%2520gap%2520in%2520detection%252C%2520we%2520present%2520FakePartsBench%252C%2520the%2520first%2520large-scale%2520benchmark%2520specifically%2520designed%2520to%2520capture%2520the%2520full%2520spectrum%2520of%2520partial%2520deepfakes.%2520Comprising%2520over%252081K%2520%2528including%252044K%2520FakeParts%2529%2520videos%2520with%2520pixel-%2520and%2520frame-level%2520manipulation%2520annotations%252C%2520our%2520dataset%2520enables%2520comprehensive%2520evaluation%2520of%2520detection%2520methods.%2520Our%2520user%2520studies%2520demonstrate%2520that%2520FakeParts%2520reduces%2520human%2520detection%2520accuracy%2520by%2520up%2520to%252026%2525%2520compared%2520to%2520traditional%2520deepfakes%252C%2520with%2520similar%2520performance%2520degradation%2520observed%2520in%2520state-of-the-art%2520detection%2520models.%2520This%2520work%2520identifies%2520an%2520urgent%2520vulnerability%2520in%2520current%2520detectors%2520and%2520provides%2520the%2520necessary%2520resources%2520to%2520develop%2520methods%2520robust%2520to%2520partial%2520manipulations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakeParts%3A%20a%20New%20Family%20of%20AI-Generated%20DeepFakes&entry.906535625=Ziyi%20Liu%20and%20Firas%20Gabetni%20and%20Awais%20Hussain%20Sani%20and%20Xi%20Wang%20and%20Soobash%20Daiboo%20and%20Gaetan%20Brison%20and%20Gianni%20Franchi%20and%20Vicky%20Kalogeiton&entry.1292438233=We%20introduce%20FakeParts%2C%20a%20new%20class%20of%20deepfakes%20characterized%20by%20subtle%2C%20localized%20manipulations%20to%20specific%20spatial%20regions%20or%20temporal%20segments%20of%20otherwise%20authentic%20videos.%20Unlike%20fully%20synthetic%20content%2C%20these%20partial%20manipulations%20-%20ranging%20from%20altered%20facial%20expressions%20to%20object%20substitutions%20and%20background%20modifications%20-%20blend%20seamlessly%20with%20real%20elements%2C%20making%20them%20particularly%20deceptive%20and%20difficult%20to%20detect.%20To%20address%20the%20critical%20gap%20in%20detection%2C%20we%20present%20FakePartsBench%2C%20the%20first%20large-scale%20benchmark%20specifically%20designed%20to%20capture%20the%20full%20spectrum%20of%20partial%20deepfakes.%20Comprising%20over%2081K%20%28including%2044K%20FakeParts%29%20videos%20with%20pixel-%20and%20frame-level%20manipulation%20annotations%2C%20our%20dataset%20enables%20comprehensive%20evaluation%20of%20detection%20methods.%20Our%20user%20studies%20demonstrate%20that%20FakeParts%20reduces%20human%20detection%20accuracy%20by%20up%20to%2026%25%20compared%20to%20traditional%20deepfakes%2C%20with%20similar%20performance%20degradation%20observed%20in%20state-of-the-art%20detection%20models.%20This%20work%20identifies%20an%20urgent%20vulnerability%20in%20current%20detectors%20and%20provides%20the%20necessary%20resources%20to%20develop%20methods%20robust%20to%20partial%20manipulations.&entry.1838667208=http%3A//arxiv.org/abs/2508.21052v2&entry.124074799=Read"},
{"title": "Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation", "author": "Liam Collins and Bhuvesh Kumar and Clark Mingxuan Ju and Tong Zhao and Donald Loveland and Leonardo Neves and Neil Shah", "abstract": "Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.", "link": "http://arxiv.org/abs/2512.17820v1", "date": "2025-12-19", "relevancy": 2.5229, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20ID-Text%20Complementarity%20via%20Ensembling%20for%20Sequential%20Recommendation&body=Title%3A%20Exploiting%20ID-Text%20Complementarity%20via%20Ensembling%20for%20Sequential%20Recommendation%0AAuthor%3A%20Liam%20Collins%20and%20Bhuvesh%20Kumar%20and%20Clark%20Mingxuan%20Ju%20and%20Tong%20Zhao%20and%20Donald%20Loveland%20and%20Leonardo%20Neves%20and%20Neil%20Shah%0AAbstract%3A%20Modern%20Sequential%20Recommendation%20%28SR%29%20models%20commonly%20utilize%20modality%20features%20to%20represent%20items%2C%20motivated%20in%20large%20part%20by%20recent%20advancements%20in%20language%20and%20vision%20modeling.%20To%20do%20so%2C%20several%20works%20completely%20replace%20ID%20embeddings%20with%20modality%20embeddings%2C%20claiming%20that%20modality%20embeddings%20render%20ID%20embeddings%20unnecessary%20because%20they%20can%20match%20or%20even%20exceed%20ID%20embedding%20performance.%20On%20the%20other%20hand%2C%20many%20works%20jointly%20utilize%20ID%20and%20modality%20features%2C%20but%20posit%20that%20complex%20fusion%20strategies%2C%20such%20as%20multi-stage%20training%20and/or%20intricate%20alignment%20architectures%2C%20are%20necessary%20for%20this%20joint%20utilization.%20However%2C%20underlying%20both%20these%20lines%20of%20work%20is%20a%20lack%20of%20understanding%20of%20the%20complementarity%20of%20ID%20and%20modality%20features.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20studying%20the%20complementarity%20of%20ID-%20and%20text-based%20SR%20models.%20We%20show%20that%20these%20models%20do%20learn%20complementary%20signals%2C%20meaning%20that%20either%20should%20provide%20performance%20gain%20when%20used%20properly%20alongside%20the%20other.%20Motivated%20by%20this%2C%20we%20propose%20a%20new%20SR%20method%20that%20preserves%20ID-text%20complementarity%20through%20independent%20model%20training%2C%20then%20harnesses%20it%20through%20a%20simple%20ensembling%20strategy.%20Despite%20this%20method%27s%20simplicity%2C%20we%20show%20it%20outperforms%20several%20competitive%20SR%20baselines%2C%20implying%20that%20both%20ID%20and%20text%20features%20are%20necessary%20to%20achieve%20state-of-the-art%20SR%20performance%20but%20complex%20fusion%20architectures%20are%20not.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520ID-Text%2520Complementarity%2520via%2520Ensembling%2520for%2520Sequential%2520Recommendation%26entry.906535625%3DLiam%2520Collins%2520and%2520Bhuvesh%2520Kumar%2520and%2520Clark%2520Mingxuan%2520Ju%2520and%2520Tong%2520Zhao%2520and%2520Donald%2520Loveland%2520and%2520Leonardo%2520Neves%2520and%2520Neil%2520Shah%26entry.1292438233%3DModern%2520Sequential%2520Recommendation%2520%2528SR%2529%2520models%2520commonly%2520utilize%2520modality%2520features%2520to%2520represent%2520items%252C%2520motivated%2520in%2520large%2520part%2520by%2520recent%2520advancements%2520in%2520language%2520and%2520vision%2520modeling.%2520To%2520do%2520so%252C%2520several%2520works%2520completely%2520replace%2520ID%2520embeddings%2520with%2520modality%2520embeddings%252C%2520claiming%2520that%2520modality%2520embeddings%2520render%2520ID%2520embeddings%2520unnecessary%2520because%2520they%2520can%2520match%2520or%2520even%2520exceed%2520ID%2520embedding%2520performance.%2520On%2520the%2520other%2520hand%252C%2520many%2520works%2520jointly%2520utilize%2520ID%2520and%2520modality%2520features%252C%2520but%2520posit%2520that%2520complex%2520fusion%2520strategies%252C%2520such%2520as%2520multi-stage%2520training%2520and/or%2520intricate%2520alignment%2520architectures%252C%2520are%2520necessary%2520for%2520this%2520joint%2520utilization.%2520However%252C%2520underlying%2520both%2520these%2520lines%2520of%2520work%2520is%2520a%2520lack%2520of%2520understanding%2520of%2520the%2520complementarity%2520of%2520ID%2520and%2520modality%2520features.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520studying%2520the%2520complementarity%2520of%2520ID-%2520and%2520text-based%2520SR%2520models.%2520We%2520show%2520that%2520these%2520models%2520do%2520learn%2520complementary%2520signals%252C%2520meaning%2520that%2520either%2520should%2520provide%2520performance%2520gain%2520when%2520used%2520properly%2520alongside%2520the%2520other.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520new%2520SR%2520method%2520that%2520preserves%2520ID-text%2520complementarity%2520through%2520independent%2520model%2520training%252C%2520then%2520harnesses%2520it%2520through%2520a%2520simple%2520ensembling%2520strategy.%2520Despite%2520this%2520method%2527s%2520simplicity%252C%2520we%2520show%2520it%2520outperforms%2520several%2520competitive%2520SR%2520baselines%252C%2520implying%2520that%2520both%2520ID%2520and%2520text%2520features%2520are%2520necessary%2520to%2520achieve%2520state-of-the-art%2520SR%2520performance%2520but%2520complex%2520fusion%2520architectures%2520are%2520not.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20ID-Text%20Complementarity%20via%20Ensembling%20for%20Sequential%20Recommendation&entry.906535625=Liam%20Collins%20and%20Bhuvesh%20Kumar%20and%20Clark%20Mingxuan%20Ju%20and%20Tong%20Zhao%20and%20Donald%20Loveland%20and%20Leonardo%20Neves%20and%20Neil%20Shah&entry.1292438233=Modern%20Sequential%20Recommendation%20%28SR%29%20models%20commonly%20utilize%20modality%20features%20to%20represent%20items%2C%20motivated%20in%20large%20part%20by%20recent%20advancements%20in%20language%20and%20vision%20modeling.%20To%20do%20so%2C%20several%20works%20completely%20replace%20ID%20embeddings%20with%20modality%20embeddings%2C%20claiming%20that%20modality%20embeddings%20render%20ID%20embeddings%20unnecessary%20because%20they%20can%20match%20or%20even%20exceed%20ID%20embedding%20performance.%20On%20the%20other%20hand%2C%20many%20works%20jointly%20utilize%20ID%20and%20modality%20features%2C%20but%20posit%20that%20complex%20fusion%20strategies%2C%20such%20as%20multi-stage%20training%20and/or%20intricate%20alignment%20architectures%2C%20are%20necessary%20for%20this%20joint%20utilization.%20However%2C%20underlying%20both%20these%20lines%20of%20work%20is%20a%20lack%20of%20understanding%20of%20the%20complementarity%20of%20ID%20and%20modality%20features.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20studying%20the%20complementarity%20of%20ID-%20and%20text-based%20SR%20models.%20We%20show%20that%20these%20models%20do%20learn%20complementary%20signals%2C%20meaning%20that%20either%20should%20provide%20performance%20gain%20when%20used%20properly%20alongside%20the%20other.%20Motivated%20by%20this%2C%20we%20propose%20a%20new%20SR%20method%20that%20preserves%20ID-text%20complementarity%20through%20independent%20model%20training%2C%20then%20harnesses%20it%20through%20a%20simple%20ensembling%20strategy.%20Despite%20this%20method%27s%20simplicity%2C%20we%20show%20it%20outperforms%20several%20competitive%20SR%20baselines%2C%20implying%20that%20both%20ID%20and%20text%20features%20are%20necessary%20to%20achieve%20state-of-the-art%20SR%20performance%20but%20complex%20fusion%20architectures%20are%20not.&entry.1838667208=http%3A//arxiv.org/abs/2512.17820v1&entry.124074799=Read"},
{"title": "Machine Learning for Static and Single-Event Dynamic Complex Network Analysis", "author": "Nikolaos Nakis", "abstract": "The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.", "link": "http://arxiv.org/abs/2512.17577v1", "date": "2025-12-19", "relevancy": 2.5212, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5219}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5034}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20for%20Static%20and%20Single-Event%20Dynamic%20Complex%20Network%20Analysis&body=Title%3A%20Machine%20Learning%20for%20Static%20and%20Single-Event%20Dynamic%20Complex%20Network%20Analysis%0AAuthor%3A%20Nikolaos%20Nakis%0AAbstract%3A%20The%20primary%20objective%20of%20this%20thesis%20is%20to%20develop%20novel%20algorithmic%20approaches%20for%20Graph%20Representation%20Learning%20of%20static%20and%20single-event%20dynamic%20networks.%20In%20such%20a%20direction%2C%20we%20focus%20on%20the%20family%20of%20Latent%20Space%20Models%2C%20and%20more%20specifically%20on%20the%20Latent%20Distance%20Model%20which%20naturally%20conveys%20important%20network%20characteristics%20such%20as%20homophily%2C%20transitivity%2C%20and%20the%20balance%20theory.%20Furthermore%2C%20this%20thesis%20aims%20to%20create%20structural-aware%20network%20representations%2C%20which%20lead%20to%20hierarchical%20expressions%20of%20network%20structure%2C%20community%20characterization%2C%20the%20identification%20of%20extreme%20profiles%20in%20networks%2C%20and%20impact%20dynamics%20quantification%20in%20temporal%20networks.%20Crucially%2C%20the%20methods%20presented%20are%20designed%20to%20define%20unified%20learning%20processes%2C%20eliminating%20the%20need%20for%20heuristics%20and%20multi-stage%20processes%20like%20post-processing%20steps.%20Our%20aim%20is%20to%20delve%20into%20a%20journey%20towards%20unified%20network%20embeddings%20that%20are%20both%20comprehensive%20and%20powerful%2C%20capable%20of%20characterizing%20network%20structures%20and%20adeptly%20handling%20the%20diverse%20tasks%20that%20graph%20analysis%20offers.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520for%2520Static%2520and%2520Single-Event%2520Dynamic%2520Complex%2520Network%2520Analysis%26entry.906535625%3DNikolaos%2520Nakis%26entry.1292438233%3DThe%2520primary%2520objective%2520of%2520this%2520thesis%2520is%2520to%2520develop%2520novel%2520algorithmic%2520approaches%2520for%2520Graph%2520Representation%2520Learning%2520of%2520static%2520and%2520single-event%2520dynamic%2520networks.%2520In%2520such%2520a%2520direction%252C%2520we%2520focus%2520on%2520the%2520family%2520of%2520Latent%2520Space%2520Models%252C%2520and%2520more%2520specifically%2520on%2520the%2520Latent%2520Distance%2520Model%2520which%2520naturally%2520conveys%2520important%2520network%2520characteristics%2520such%2520as%2520homophily%252C%2520transitivity%252C%2520and%2520the%2520balance%2520theory.%2520Furthermore%252C%2520this%2520thesis%2520aims%2520to%2520create%2520structural-aware%2520network%2520representations%252C%2520which%2520lead%2520to%2520hierarchical%2520expressions%2520of%2520network%2520structure%252C%2520community%2520characterization%252C%2520the%2520identification%2520of%2520extreme%2520profiles%2520in%2520networks%252C%2520and%2520impact%2520dynamics%2520quantification%2520in%2520temporal%2520networks.%2520Crucially%252C%2520the%2520methods%2520presented%2520are%2520designed%2520to%2520define%2520unified%2520learning%2520processes%252C%2520eliminating%2520the%2520need%2520for%2520heuristics%2520and%2520multi-stage%2520processes%2520like%2520post-processing%2520steps.%2520Our%2520aim%2520is%2520to%2520delve%2520into%2520a%2520journey%2520towards%2520unified%2520network%2520embeddings%2520that%2520are%2520both%2520comprehensive%2520and%2520powerful%252C%2520capable%2520of%2520characterizing%2520network%2520structures%2520and%2520adeptly%2520handling%2520the%2520diverse%2520tasks%2520that%2520graph%2520analysis%2520offers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20for%20Static%20and%20Single-Event%20Dynamic%20Complex%20Network%20Analysis&entry.906535625=Nikolaos%20Nakis&entry.1292438233=The%20primary%20objective%20of%20this%20thesis%20is%20to%20develop%20novel%20algorithmic%20approaches%20for%20Graph%20Representation%20Learning%20of%20static%20and%20single-event%20dynamic%20networks.%20In%20such%20a%20direction%2C%20we%20focus%20on%20the%20family%20of%20Latent%20Space%20Models%2C%20and%20more%20specifically%20on%20the%20Latent%20Distance%20Model%20which%20naturally%20conveys%20important%20network%20characteristics%20such%20as%20homophily%2C%20transitivity%2C%20and%20the%20balance%20theory.%20Furthermore%2C%20this%20thesis%20aims%20to%20create%20structural-aware%20network%20representations%2C%20which%20lead%20to%20hierarchical%20expressions%20of%20network%20structure%2C%20community%20characterization%2C%20the%20identification%20of%20extreme%20profiles%20in%20networks%2C%20and%20impact%20dynamics%20quantification%20in%20temporal%20networks.%20Crucially%2C%20the%20methods%20presented%20are%20designed%20to%20define%20unified%20learning%20processes%2C%20eliminating%20the%20need%20for%20heuristics%20and%20multi-stage%20processes%20like%20post-processing%20steps.%20Our%20aim%20is%20to%20delve%20into%20a%20journey%20towards%20unified%20network%20embeddings%20that%20are%20both%20comprehensive%20and%20powerful%2C%20capable%20of%20characterizing%20network%20structures%20and%20adeptly%20handling%20the%20diverse%20tasks%20that%20graph%20analysis%20offers.&entry.1838667208=http%3A//arxiv.org/abs/2512.17577v1&entry.124074799=Read"},
{"title": "SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neural Networks", "author": "Yimeng Fan and Changsong Liu and Mingyang Li and Dongze Liu and Yanyan Liu and Wei Zhang", "abstract": "Spiking Neural Networks (SNNs) are the third generation of neural networks. They have gained widespread attention in object detection due to their low power consumption and biological interpretability. However, existing SNN-based object detection methods suffer from local firing saturation, where adjacent neurons concurrently reach maximum firing rates, especially in object-centric regions. This abnormal neuron firing pattern reduces the feature discrimination capability and detection accuracy, while also increasing the firing rates that prevent SNNs from achieving their potential energy efficiency. To address this problem, we propose SpikeDet, a novel spiking object detector that optimizes firing patterns for accurate and energy-efficient detection. Specifically, we design a spiking backbone network, MDSNet, which effectively adjusts the membrane synaptic input distribution at each layer, achieving better neuron firing patterns during spiking feature extraction. For the neck, to better utilize and preserve these high-quality backbone features, we introduce the Spiking Multi-direction Fusion Module (SMFM), which realizes multi-direction fusion of spiking features, enhancing the multi-scale detection capability of the model. Furthermore, we propose the Local Firing Saturation Index (LFSI) to quantitatively measure local firing saturation. Experimental results validate the effectiveness of our method, with SpikeDet achieving superior performance. On the COCO 2017 dataset, it achieves 52.2% AP, outperforming previous SNN-based methods by 3.3% AP while requiring only half the power consumption. On object detection sub-tasks, including event-based GEN1, underwater URPC 2019, low-light ExDARK, and dense scene CrowdHuman datasets, SpikeDet also achieves the best performance.", "link": "http://arxiv.org/abs/2501.15151v4", "date": "2025-12-19", "relevancy": 2.5062, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5409}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4917}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeDet%3A%20Better%20Firing%20Patterns%20for%20Accurate%20and%20Energy-Efficient%20Object%20Detection%20with%20Spiking%20Neural%20Networks&body=Title%3A%20SpikeDet%3A%20Better%20Firing%20Patterns%20for%20Accurate%20and%20Energy-Efficient%20Object%20Detection%20with%20Spiking%20Neural%20Networks%0AAuthor%3A%20Yimeng%20Fan%20and%20Changsong%20Liu%20and%20Mingyang%20Li%20and%20Dongze%20Liu%20and%20Yanyan%20Liu%20and%20Wei%20Zhang%0AAbstract%3A%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20the%20third%20generation%20of%20neural%20networks.%20They%20have%20gained%20widespread%20attention%20in%20object%20detection%20due%20to%20their%20low%20power%20consumption%20and%20biological%20interpretability.%20However%2C%20existing%20SNN-based%20object%20detection%20methods%20suffer%20from%20local%20firing%20saturation%2C%20where%20adjacent%20neurons%20concurrently%20reach%20maximum%20firing%20rates%2C%20especially%20in%20object-centric%20regions.%20This%20abnormal%20neuron%20firing%20pattern%20reduces%20the%20feature%20discrimination%20capability%20and%20detection%20accuracy%2C%20while%20also%20increasing%20the%20firing%20rates%20that%20prevent%20SNNs%20from%20achieving%20their%20potential%20energy%20efficiency.%20To%20address%20this%20problem%2C%20we%20propose%20SpikeDet%2C%20a%20novel%20spiking%20object%20detector%20that%20optimizes%20firing%20patterns%20for%20accurate%20and%20energy-efficient%20detection.%20Specifically%2C%20we%20design%20a%20spiking%20backbone%20network%2C%20MDSNet%2C%20which%20effectively%20adjusts%20the%20membrane%20synaptic%20input%20distribution%20at%20each%20layer%2C%20achieving%20better%20neuron%20firing%20patterns%20during%20spiking%20feature%20extraction.%20For%20the%20neck%2C%20to%20better%20utilize%20and%20preserve%20these%20high-quality%20backbone%20features%2C%20we%20introduce%20the%20Spiking%20Multi-direction%20Fusion%20Module%20%28SMFM%29%2C%20which%20realizes%20multi-direction%20fusion%20of%20spiking%20features%2C%20enhancing%20the%20multi-scale%20detection%20capability%20of%20the%20model.%20Furthermore%2C%20we%20propose%20the%20Local%20Firing%20Saturation%20Index%20%28LFSI%29%20to%20quantitatively%20measure%20local%20firing%20saturation.%20Experimental%20results%20validate%20the%20effectiveness%20of%20our%20method%2C%20with%20SpikeDet%20achieving%20superior%20performance.%20On%20the%20COCO%202017%20dataset%2C%20it%20achieves%2052.2%25%20AP%2C%20outperforming%20previous%20SNN-based%20methods%20by%203.3%25%20AP%20while%20requiring%20only%20half%20the%20power%20consumption.%20On%20object%20detection%20sub-tasks%2C%20including%20event-based%20GEN1%2C%20underwater%20URPC%202019%2C%20low-light%20ExDARK%2C%20and%20dense%20scene%20CrowdHuman%20datasets%2C%20SpikeDet%20also%20achieves%20the%20best%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2501.15151v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeDet%253A%2520Better%2520Firing%2520Patterns%2520for%2520Accurate%2520and%2520Energy-Efficient%2520Object%2520Detection%2520with%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DYimeng%2520Fan%2520and%2520Changsong%2520Liu%2520and%2520Mingyang%2520Li%2520and%2520Dongze%2520Liu%2520and%2520Yanyan%2520Liu%2520and%2520Wei%2520Zhang%26entry.1292438233%3DSpiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520the%2520third%2520generation%2520of%2520neural%2520networks.%2520They%2520have%2520gained%2520widespread%2520attention%2520in%2520object%2520detection%2520due%2520to%2520their%2520low%2520power%2520consumption%2520and%2520biological%2520interpretability.%2520However%252C%2520existing%2520SNN-based%2520object%2520detection%2520methods%2520suffer%2520from%2520local%2520firing%2520saturation%252C%2520where%2520adjacent%2520neurons%2520concurrently%2520reach%2520maximum%2520firing%2520rates%252C%2520especially%2520in%2520object-centric%2520regions.%2520This%2520abnormal%2520neuron%2520firing%2520pattern%2520reduces%2520the%2520feature%2520discrimination%2520capability%2520and%2520detection%2520accuracy%252C%2520while%2520also%2520increasing%2520the%2520firing%2520rates%2520that%2520prevent%2520SNNs%2520from%2520achieving%2520their%2520potential%2520energy%2520efficiency.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520SpikeDet%252C%2520a%2520novel%2520spiking%2520object%2520detector%2520that%2520optimizes%2520firing%2520patterns%2520for%2520accurate%2520and%2520energy-efficient%2520detection.%2520Specifically%252C%2520we%2520design%2520a%2520spiking%2520backbone%2520network%252C%2520MDSNet%252C%2520which%2520effectively%2520adjusts%2520the%2520membrane%2520synaptic%2520input%2520distribution%2520at%2520each%2520layer%252C%2520achieving%2520better%2520neuron%2520firing%2520patterns%2520during%2520spiking%2520feature%2520extraction.%2520For%2520the%2520neck%252C%2520to%2520better%2520utilize%2520and%2520preserve%2520these%2520high-quality%2520backbone%2520features%252C%2520we%2520introduce%2520the%2520Spiking%2520Multi-direction%2520Fusion%2520Module%2520%2528SMFM%2529%252C%2520which%2520realizes%2520multi-direction%2520fusion%2520of%2520spiking%2520features%252C%2520enhancing%2520the%2520multi-scale%2520detection%2520capability%2520of%2520the%2520model.%2520Furthermore%252C%2520we%2520propose%2520the%2520Local%2520Firing%2520Saturation%2520Index%2520%2528LFSI%2529%2520to%2520quantitatively%2520measure%2520local%2520firing%2520saturation.%2520Experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520with%2520SpikeDet%2520achieving%2520superior%2520performance.%2520On%2520the%2520COCO%25202017%2520dataset%252C%2520it%2520achieves%252052.2%2525%2520AP%252C%2520outperforming%2520previous%2520SNN-based%2520methods%2520by%25203.3%2525%2520AP%2520while%2520requiring%2520only%2520half%2520the%2520power%2520consumption.%2520On%2520object%2520detection%2520sub-tasks%252C%2520including%2520event-based%2520GEN1%252C%2520underwater%2520URPC%25202019%252C%2520low-light%2520ExDARK%252C%2520and%2520dense%2520scene%2520CrowdHuman%2520datasets%252C%2520SpikeDet%2520also%2520achieves%2520the%2520best%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15151v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeDet%3A%20Better%20Firing%20Patterns%20for%20Accurate%20and%20Energy-Efficient%20Object%20Detection%20with%20Spiking%20Neural%20Networks&entry.906535625=Yimeng%20Fan%20and%20Changsong%20Liu%20and%20Mingyang%20Li%20and%20Dongze%20Liu%20and%20Yanyan%20Liu%20and%20Wei%20Zhang&entry.1292438233=Spiking%20Neural%20Networks%20%28SNNs%29%20are%20the%20third%20generation%20of%20neural%20networks.%20They%20have%20gained%20widespread%20attention%20in%20object%20detection%20due%20to%20their%20low%20power%20consumption%20and%20biological%20interpretability.%20However%2C%20existing%20SNN-based%20object%20detection%20methods%20suffer%20from%20local%20firing%20saturation%2C%20where%20adjacent%20neurons%20concurrently%20reach%20maximum%20firing%20rates%2C%20especially%20in%20object-centric%20regions.%20This%20abnormal%20neuron%20firing%20pattern%20reduces%20the%20feature%20discrimination%20capability%20and%20detection%20accuracy%2C%20while%20also%20increasing%20the%20firing%20rates%20that%20prevent%20SNNs%20from%20achieving%20their%20potential%20energy%20efficiency.%20To%20address%20this%20problem%2C%20we%20propose%20SpikeDet%2C%20a%20novel%20spiking%20object%20detector%20that%20optimizes%20firing%20patterns%20for%20accurate%20and%20energy-efficient%20detection.%20Specifically%2C%20we%20design%20a%20spiking%20backbone%20network%2C%20MDSNet%2C%20which%20effectively%20adjusts%20the%20membrane%20synaptic%20input%20distribution%20at%20each%20layer%2C%20achieving%20better%20neuron%20firing%20patterns%20during%20spiking%20feature%20extraction.%20For%20the%20neck%2C%20to%20better%20utilize%20and%20preserve%20these%20high-quality%20backbone%20features%2C%20we%20introduce%20the%20Spiking%20Multi-direction%20Fusion%20Module%20%28SMFM%29%2C%20which%20realizes%20multi-direction%20fusion%20of%20spiking%20features%2C%20enhancing%20the%20multi-scale%20detection%20capability%20of%20the%20model.%20Furthermore%2C%20we%20propose%20the%20Local%20Firing%20Saturation%20Index%20%28LFSI%29%20to%20quantitatively%20measure%20local%20firing%20saturation.%20Experimental%20results%20validate%20the%20effectiveness%20of%20our%20method%2C%20with%20SpikeDet%20achieving%20superior%20performance.%20On%20the%20COCO%202017%20dataset%2C%20it%20achieves%2052.2%25%20AP%2C%20outperforming%20previous%20SNN-based%20methods%20by%203.3%25%20AP%20while%20requiring%20only%20half%20the%20power%20consumption.%20On%20object%20detection%20sub-tasks%2C%20including%20event-based%20GEN1%2C%20underwater%20URPC%202019%2C%20low-light%20ExDARK%2C%20and%20dense%20scene%20CrowdHuman%20datasets%2C%20SpikeDet%20also%20achieves%20the%20best%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2501.15151v4&entry.124074799=Read"},
{"title": "Enhancing Blind Face Restoration through Online Reinforcement Learning", "author": "Bin Wu and Yahui Liu and Chi Zhang and Yao Zhao and Wei Wang", "abstract": "Blind Face Restoration (BFR) encounters inherent challenges in exploring its large solution space, leading to common artifacts like missing details and identity ambiguity in the restored images. To tackle these challenges, we propose a Likelihood-Regularized Policy Optimization (LRPO) framework, the first to apply online reinforcement learning (RL) to the BFR task. LRPO leverages rewards from sampled candidates to refine the policy network, increasing the likelihood of high-quality outputs while improving restoration performance on low-quality inputs. However, directly applying RL to BFR creates incompatibility issues, producing restoration results that deviate significantly from the ground truth. To balance perceptual quality and fidelity, we propose three key strategies: 1) a composite reward function tailored for face restoration assessment, 2) ground-truth guided likelihood regularization, and 3) noise-level advantage assignment. Extensive experiments demonstrate that our proposed LRPO significantly improves the face restoration quality over baseline methods and achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2509.23339v2", "date": "2025-12-19", "relevancy": 2.4988, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5051}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4974}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Blind%20Face%20Restoration%20through%20Online%20Reinforcement%20Learning&body=Title%3A%20Enhancing%20Blind%20Face%20Restoration%20through%20Online%20Reinforcement%20Learning%0AAuthor%3A%20Bin%20Wu%20and%20Yahui%20Liu%20and%20Chi%20Zhang%20and%20Yao%20Zhao%20and%20Wei%20Wang%0AAbstract%3A%20Blind%20Face%20Restoration%20%28BFR%29%20encounters%20inherent%20challenges%20in%20exploring%20its%20large%20solution%20space%2C%20leading%20to%20common%20artifacts%20like%20missing%20details%20and%20identity%20ambiguity%20in%20the%20restored%20images.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20Likelihood-Regularized%20Policy%20Optimization%20%28LRPO%29%20framework%2C%20the%20first%20to%20apply%20online%20reinforcement%20learning%20%28RL%29%20to%20the%20BFR%20task.%20LRPO%20leverages%20rewards%20from%20sampled%20candidates%20to%20refine%20the%20policy%20network%2C%20increasing%20the%20likelihood%20of%20high-quality%20outputs%20while%20improving%20restoration%20performance%20on%20low-quality%20inputs.%20However%2C%20directly%20applying%20RL%20to%20BFR%20creates%20incompatibility%20issues%2C%20producing%20restoration%20results%20that%20deviate%20significantly%20from%20the%20ground%20truth.%20To%20balance%20perceptual%20quality%20and%20fidelity%2C%20we%20propose%20three%20key%20strategies%3A%201%29%20a%20composite%20reward%20function%20tailored%20for%20face%20restoration%20assessment%2C%202%29%20ground-truth%20guided%20likelihood%20regularization%2C%20and%203%29%20noise-level%20advantage%20assignment.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20LRPO%20significantly%20improves%20the%20face%20restoration%20quality%20over%20baseline%20methods%20and%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Blind%2520Face%2520Restoration%2520through%2520Online%2520Reinforcement%2520Learning%26entry.906535625%3DBin%2520Wu%2520and%2520Yahui%2520Liu%2520and%2520Chi%2520Zhang%2520and%2520Yao%2520Zhao%2520and%2520Wei%2520Wang%26entry.1292438233%3DBlind%2520Face%2520Restoration%2520%2528BFR%2529%2520encounters%2520inherent%2520challenges%2520in%2520exploring%2520its%2520large%2520solution%2520space%252C%2520leading%2520to%2520common%2520artifacts%2520like%2520missing%2520details%2520and%2520identity%2520ambiguity%2520in%2520the%2520restored%2520images.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Likelihood-Regularized%2520Policy%2520Optimization%2520%2528LRPO%2529%2520framework%252C%2520the%2520first%2520to%2520apply%2520online%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520the%2520BFR%2520task.%2520LRPO%2520leverages%2520rewards%2520from%2520sampled%2520candidates%2520to%2520refine%2520the%2520policy%2520network%252C%2520increasing%2520the%2520likelihood%2520of%2520high-quality%2520outputs%2520while%2520improving%2520restoration%2520performance%2520on%2520low-quality%2520inputs.%2520However%252C%2520directly%2520applying%2520RL%2520to%2520BFR%2520creates%2520incompatibility%2520issues%252C%2520producing%2520restoration%2520results%2520that%2520deviate%2520significantly%2520from%2520the%2520ground%2520truth.%2520To%2520balance%2520perceptual%2520quality%2520and%2520fidelity%252C%2520we%2520propose%2520three%2520key%2520strategies%253A%25201%2529%2520a%2520composite%2520reward%2520function%2520tailored%2520for%2520face%2520restoration%2520assessment%252C%25202%2529%2520ground-truth%2520guided%2520likelihood%2520regularization%252C%2520and%25203%2529%2520noise-level%2520advantage%2520assignment.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520LRPO%2520significantly%2520improves%2520the%2520face%2520restoration%2520quality%2520over%2520baseline%2520methods%2520and%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Blind%20Face%20Restoration%20through%20Online%20Reinforcement%20Learning&entry.906535625=Bin%20Wu%20and%20Yahui%20Liu%20and%20Chi%20Zhang%20and%20Yao%20Zhao%20and%20Wei%20Wang&entry.1292438233=Blind%20Face%20Restoration%20%28BFR%29%20encounters%20inherent%20challenges%20in%20exploring%20its%20large%20solution%20space%2C%20leading%20to%20common%20artifacts%20like%20missing%20details%20and%20identity%20ambiguity%20in%20the%20restored%20images.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20Likelihood-Regularized%20Policy%20Optimization%20%28LRPO%29%20framework%2C%20the%20first%20to%20apply%20online%20reinforcement%20learning%20%28RL%29%20to%20the%20BFR%20task.%20LRPO%20leverages%20rewards%20from%20sampled%20candidates%20to%20refine%20the%20policy%20network%2C%20increasing%20the%20likelihood%20of%20high-quality%20outputs%20while%20improving%20restoration%20performance%20on%20low-quality%20inputs.%20However%2C%20directly%20applying%20RL%20to%20BFR%20creates%20incompatibility%20issues%2C%20producing%20restoration%20results%20that%20deviate%20significantly%20from%20the%20ground%20truth.%20To%20balance%20perceptual%20quality%20and%20fidelity%2C%20we%20propose%20three%20key%20strategies%3A%201%29%20a%20composite%20reward%20function%20tailored%20for%20face%20restoration%20assessment%2C%202%29%20ground-truth%20guided%20likelihood%20regularization%2C%20and%203%29%20noise-level%20advantage%20assignment.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20LRPO%20significantly%20improves%20the%20face%20restoration%20quality%20over%20baseline%20methods%20and%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2509.23339v2&entry.124074799=Read"},
{"title": "MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration", "author": "Svetlana Krasnova and Emiliya Starikova and Ilia Naletov and Andrey Krylov and Dmitry Sorokin", "abstract": "Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.", "link": "http://arxiv.org/abs/2512.17605v1", "date": "2025-12-19", "relevancy": 2.4966, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5081}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGRegBench%3A%20A%20Novel%20Benchmark%20Dataset%20with%20Anatomical%20Landmarks%20for%20Mammography%20Image%20Registration&body=Title%3A%20MGRegBench%3A%20A%20Novel%20Benchmark%20Dataset%20with%20Anatomical%20Landmarks%20for%20Mammography%20Image%20Registration%0AAuthor%3A%20Svetlana%20Krasnova%20and%20Emiliya%20Starikova%20and%20Ilia%20Naletov%20and%20Andrey%20Krylov%20and%20Dmitry%20Sorokin%0AAbstract%3A%20Robust%20mammography%20registration%20is%20essential%20for%20clinical%20applications%20like%20tracking%20disease%20progression%20and%20monitoring%20longitudinal%20changes%20in%20breast%20tissue.%20However%2C%20progress%20has%20been%20limited%20by%20the%20absence%20of%20public%20datasets%20and%20standardized%20benchmarks.%20Existing%20studies%20are%20often%20not%20directly%20comparable%2C%20as%20they%20use%20private%20data%20and%20inconsistent%20evaluation%20frameworks.%20To%20address%20this%2C%20we%20present%20MGRegBench%2C%20a%20public%20benchmark%20dataset%20for%20mammogram%20registration.%20It%20comprises%20over%205%2C000%20image%20pairs%2C%20with%20100%20containing%20manual%20anatomical%20landmarks%20and%20segmentation%20masks%20for%20rigorous%20evaluation.%20This%20makes%20MGRegBench%20one%20of%20the%20largest%20public%202D%20registration%20datasets%20with%20manual%20annotations.%20Using%20this%20resource%2C%20we%20benchmarked%20diverse%20registration%20methods%20including%20classical%20%28ANTs%29%2C%20learning-based%20%28VoxelMorph%2C%20TransMorph%29%2C%20implicit%20neural%20representation%20%28IDIR%29%2C%20a%20classic%20mammography-specific%20approach%2C%20and%20a%20recent%20state-of-the-art%20deep%20learning%20method%20MammoRegNet.%20The%20implementations%20were%20adapted%20to%20this%20modality%20from%20the%20authors%27%20implementations%20or%20re-implemented%20from%20scratch.%20Our%20contributions%20are%3A%20%281%29%20the%20first%20public%20dataset%20of%20this%20scale%20with%20manual%20landmarks%20and%20masks%20for%20mammography%20registration%3B%20%282%29%20the%20first%20like-for-like%20comparison%20of%20diverse%20methods%20on%20this%20modality%3B%20and%20%283%29%20an%20extensive%20analysis%20of%20deep%20learning-based%20registration.%20We%20publicly%20release%20our%20code%20and%20data%20to%20establish%20a%20foundational%20resource%20for%20fair%20comparisons%20and%20catalyze%20future%20research.%20The%20source%20code%20and%20data%20are%20at%20https%3A//github.com/KourtKardash/MGRegBench.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGRegBench%253A%2520A%2520Novel%2520Benchmark%2520Dataset%2520with%2520Anatomical%2520Landmarks%2520for%2520Mammography%2520Image%2520Registration%26entry.906535625%3DSvetlana%2520Krasnova%2520and%2520Emiliya%2520Starikova%2520and%2520Ilia%2520Naletov%2520and%2520Andrey%2520Krylov%2520and%2520Dmitry%2520Sorokin%26entry.1292438233%3DRobust%2520mammography%2520registration%2520is%2520essential%2520for%2520clinical%2520applications%2520like%2520tracking%2520disease%2520progression%2520and%2520monitoring%2520longitudinal%2520changes%2520in%2520breast%2520tissue.%2520However%252C%2520progress%2520has%2520been%2520limited%2520by%2520the%2520absence%2520of%2520public%2520datasets%2520and%2520standardized%2520benchmarks.%2520Existing%2520studies%2520are%2520often%2520not%2520directly%2520comparable%252C%2520as%2520they%2520use%2520private%2520data%2520and%2520inconsistent%2520evaluation%2520frameworks.%2520To%2520address%2520this%252C%2520we%2520present%2520MGRegBench%252C%2520a%2520public%2520benchmark%2520dataset%2520for%2520mammogram%2520registration.%2520It%2520comprises%2520over%25205%252C000%2520image%2520pairs%252C%2520with%2520100%2520containing%2520manual%2520anatomical%2520landmarks%2520and%2520segmentation%2520masks%2520for%2520rigorous%2520evaluation.%2520This%2520makes%2520MGRegBench%2520one%2520of%2520the%2520largest%2520public%25202D%2520registration%2520datasets%2520with%2520manual%2520annotations.%2520Using%2520this%2520resource%252C%2520we%2520benchmarked%2520diverse%2520registration%2520methods%2520including%2520classical%2520%2528ANTs%2529%252C%2520learning-based%2520%2528VoxelMorph%252C%2520TransMorph%2529%252C%2520implicit%2520neural%2520representation%2520%2528IDIR%2529%252C%2520a%2520classic%2520mammography-specific%2520approach%252C%2520and%2520a%2520recent%2520state-of-the-art%2520deep%2520learning%2520method%2520MammoRegNet.%2520The%2520implementations%2520were%2520adapted%2520to%2520this%2520modality%2520from%2520the%2520authors%2527%2520implementations%2520or%2520re-implemented%2520from%2520scratch.%2520Our%2520contributions%2520are%253A%2520%25281%2529%2520the%2520first%2520public%2520dataset%2520of%2520this%2520scale%2520with%2520manual%2520landmarks%2520and%2520masks%2520for%2520mammography%2520registration%253B%2520%25282%2529%2520the%2520first%2520like-for-like%2520comparison%2520of%2520diverse%2520methods%2520on%2520this%2520modality%253B%2520and%2520%25283%2529%2520an%2520extensive%2520analysis%2520of%2520deep%2520learning-based%2520registration.%2520We%2520publicly%2520release%2520our%2520code%2520and%2520data%2520to%2520establish%2520a%2520foundational%2520resource%2520for%2520fair%2520comparisons%2520and%2520catalyze%2520future%2520research.%2520The%2520source%2520code%2520and%2520data%2520are%2520at%2520https%253A//github.com/KourtKardash/MGRegBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGRegBench%3A%20A%20Novel%20Benchmark%20Dataset%20with%20Anatomical%20Landmarks%20for%20Mammography%20Image%20Registration&entry.906535625=Svetlana%20Krasnova%20and%20Emiliya%20Starikova%20and%20Ilia%20Naletov%20and%20Andrey%20Krylov%20and%20Dmitry%20Sorokin&entry.1292438233=Robust%20mammography%20registration%20is%20essential%20for%20clinical%20applications%20like%20tracking%20disease%20progression%20and%20monitoring%20longitudinal%20changes%20in%20breast%20tissue.%20However%2C%20progress%20has%20been%20limited%20by%20the%20absence%20of%20public%20datasets%20and%20standardized%20benchmarks.%20Existing%20studies%20are%20often%20not%20directly%20comparable%2C%20as%20they%20use%20private%20data%20and%20inconsistent%20evaluation%20frameworks.%20To%20address%20this%2C%20we%20present%20MGRegBench%2C%20a%20public%20benchmark%20dataset%20for%20mammogram%20registration.%20It%20comprises%20over%205%2C000%20image%20pairs%2C%20with%20100%20containing%20manual%20anatomical%20landmarks%20and%20segmentation%20masks%20for%20rigorous%20evaluation.%20This%20makes%20MGRegBench%20one%20of%20the%20largest%20public%202D%20registration%20datasets%20with%20manual%20annotations.%20Using%20this%20resource%2C%20we%20benchmarked%20diverse%20registration%20methods%20including%20classical%20%28ANTs%29%2C%20learning-based%20%28VoxelMorph%2C%20TransMorph%29%2C%20implicit%20neural%20representation%20%28IDIR%29%2C%20a%20classic%20mammography-specific%20approach%2C%20and%20a%20recent%20state-of-the-art%20deep%20learning%20method%20MammoRegNet.%20The%20implementations%20were%20adapted%20to%20this%20modality%20from%20the%20authors%27%20implementations%20or%20re-implemented%20from%20scratch.%20Our%20contributions%20are%3A%20%281%29%20the%20first%20public%20dataset%20of%20this%20scale%20with%20manual%20landmarks%20and%20masks%20for%20mammography%20registration%3B%20%282%29%20the%20first%20like-for-like%20comparison%20of%20diverse%20methods%20on%20this%20modality%3B%20and%20%283%29%20an%20extensive%20analysis%20of%20deep%20learning-based%20registration.%20We%20publicly%20release%20our%20code%20and%20data%20to%20establish%20a%20foundational%20resource%20for%20fair%20comparisons%20and%20catalyze%20future%20research.%20The%20source%20code%20and%20data%20are%20at%20https%3A//github.com/KourtKardash/MGRegBench.&entry.1838667208=http%3A//arxiv.org/abs/2512.17605v1&entry.124074799=Read"},
{"title": "AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora", "author": "Zhihan Zhou and Daqian Shi and Rui Song and Lida Shi and Xiaolei Diao and Hao Xu", "abstract": "Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.", "link": "http://arxiv.org/abs/2512.17756v1", "date": "2025-12-19", "relevancy": 2.4799, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AncientBench%3A%20Towards%20Comprehensive%20Evaluation%20on%20Excavated%20and%20Transmitted%20Chinese%20Corpora&body=Title%3A%20AncientBench%3A%20Towards%20Comprehensive%20Evaluation%20on%20Excavated%20and%20Transmitted%20Chinese%20Corpora%0AAuthor%3A%20Zhihan%20Zhou%20and%20Daqian%20Shi%20and%20Rui%20Song%20and%20Lida%20Shi%20and%20Xiaolei%20Diao%20and%20Hao%20Xu%0AAbstract%3A%20Comprehension%20of%20ancient%20texts%20plays%20an%20important%20role%20in%20archaeology%20and%20understanding%20of%20Chinese%20history%20and%20civilization.%20The%20rapid%20development%20of%20large%20language%20models%20needs%20benchmarks%20that%20can%20evaluate%20their%20comprehension%20of%20ancient%20characters.%20Existing%20Chinese%20benchmarks%20are%20mostly%20targeted%20at%20modern%20Chinese%20and%20transmitted%20documents%20in%20ancient%20Chinese%2C%20but%20the%20part%20of%20excavated%20documents%20in%20ancient%20Chinese%20is%20not%20covered.%20To%20meet%20this%20need%2C%20we%20propose%20the%20AncientBench%2C%20which%20aims%20to%20evaluate%20the%20comprehension%20of%20ancient%20characters%2C%20especially%20in%20the%20scenario%20of%20excavated%20documents.%20The%20AncientBench%20is%20divided%20into%20four%20dimensions%2C%20which%20correspond%20to%20the%20four%20competencies%20of%20ancient%20character%20comprehension%3A%20glyph%20comprehension%2C%20pronunciation%20comprehension%2C%20meaning%20comprehension%2C%20and%20contextual%20comprehension.%20The%20benchmark%20also%20contains%20ten%20tasks%2C%20including%20radical%2C%20phonetic%20radical%2C%20homophone%2C%20cloze%2C%20translation%2C%20and%20more%2C%20providing%20a%20comprehensive%20framework%20for%20evaluation.%20We%20convened%20archaeological%20researchers%20to%20conduct%20experimental%20evaluations%2C%20proposed%20an%20ancient%20model%20as%20baseline%2C%20and%20conducted%20extensive%20experiments%20on%20the%20currently%20best-performing%20large%20language%20models.%20The%20experimental%20results%20reveal%20the%20great%20potential%20of%20large%20language%20models%20in%20ancient%20textual%20scenarios%20as%20well%20as%20the%20gap%20with%20humans.%20Our%20research%20aims%20to%20promote%20the%20development%20and%20application%20of%20large%20language%20models%20in%20the%20field%20of%20archaeology%20and%20ancient%20Chinese%20language.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAncientBench%253A%2520Towards%2520Comprehensive%2520Evaluation%2520on%2520Excavated%2520and%2520Transmitted%2520Chinese%2520Corpora%26entry.906535625%3DZhihan%2520Zhou%2520and%2520Daqian%2520Shi%2520and%2520Rui%2520Song%2520and%2520Lida%2520Shi%2520and%2520Xiaolei%2520Diao%2520and%2520Hao%2520Xu%26entry.1292438233%3DComprehension%2520of%2520ancient%2520texts%2520plays%2520an%2520important%2520role%2520in%2520archaeology%2520and%2520understanding%2520of%2520Chinese%2520history%2520and%2520civilization.%2520The%2520rapid%2520development%2520of%2520large%2520language%2520models%2520needs%2520benchmarks%2520that%2520can%2520evaluate%2520their%2520comprehension%2520of%2520ancient%2520characters.%2520Existing%2520Chinese%2520benchmarks%2520are%2520mostly%2520targeted%2520at%2520modern%2520Chinese%2520and%2520transmitted%2520documents%2520in%2520ancient%2520Chinese%252C%2520but%2520the%2520part%2520of%2520excavated%2520documents%2520in%2520ancient%2520Chinese%2520is%2520not%2520covered.%2520To%2520meet%2520this%2520need%252C%2520we%2520propose%2520the%2520AncientBench%252C%2520which%2520aims%2520to%2520evaluate%2520the%2520comprehension%2520of%2520ancient%2520characters%252C%2520especially%2520in%2520the%2520scenario%2520of%2520excavated%2520documents.%2520The%2520AncientBench%2520is%2520divided%2520into%2520four%2520dimensions%252C%2520which%2520correspond%2520to%2520the%2520four%2520competencies%2520of%2520ancient%2520character%2520comprehension%253A%2520glyph%2520comprehension%252C%2520pronunciation%2520comprehension%252C%2520meaning%2520comprehension%252C%2520and%2520contextual%2520comprehension.%2520The%2520benchmark%2520also%2520contains%2520ten%2520tasks%252C%2520including%2520radical%252C%2520phonetic%2520radical%252C%2520homophone%252C%2520cloze%252C%2520translation%252C%2520and%2520more%252C%2520providing%2520a%2520comprehensive%2520framework%2520for%2520evaluation.%2520We%2520convened%2520archaeological%2520researchers%2520to%2520conduct%2520experimental%2520evaluations%252C%2520proposed%2520an%2520ancient%2520model%2520as%2520baseline%252C%2520and%2520conducted%2520extensive%2520experiments%2520on%2520the%2520currently%2520best-performing%2520large%2520language%2520models.%2520The%2520experimental%2520results%2520reveal%2520the%2520great%2520potential%2520of%2520large%2520language%2520models%2520in%2520ancient%2520textual%2520scenarios%2520as%2520well%2520as%2520the%2520gap%2520with%2520humans.%2520Our%2520research%2520aims%2520to%2520promote%2520the%2520development%2520and%2520application%2520of%2520large%2520language%2520models%2520in%2520the%2520field%2520of%2520archaeology%2520and%2520ancient%2520Chinese%2520language.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AncientBench%3A%20Towards%20Comprehensive%20Evaluation%20on%20Excavated%20and%20Transmitted%20Chinese%20Corpora&entry.906535625=Zhihan%20Zhou%20and%20Daqian%20Shi%20and%20Rui%20Song%20and%20Lida%20Shi%20and%20Xiaolei%20Diao%20and%20Hao%20Xu&entry.1292438233=Comprehension%20of%20ancient%20texts%20plays%20an%20important%20role%20in%20archaeology%20and%20understanding%20of%20Chinese%20history%20and%20civilization.%20The%20rapid%20development%20of%20large%20language%20models%20needs%20benchmarks%20that%20can%20evaluate%20their%20comprehension%20of%20ancient%20characters.%20Existing%20Chinese%20benchmarks%20are%20mostly%20targeted%20at%20modern%20Chinese%20and%20transmitted%20documents%20in%20ancient%20Chinese%2C%20but%20the%20part%20of%20excavated%20documents%20in%20ancient%20Chinese%20is%20not%20covered.%20To%20meet%20this%20need%2C%20we%20propose%20the%20AncientBench%2C%20which%20aims%20to%20evaluate%20the%20comprehension%20of%20ancient%20characters%2C%20especially%20in%20the%20scenario%20of%20excavated%20documents.%20The%20AncientBench%20is%20divided%20into%20four%20dimensions%2C%20which%20correspond%20to%20the%20four%20competencies%20of%20ancient%20character%20comprehension%3A%20glyph%20comprehension%2C%20pronunciation%20comprehension%2C%20meaning%20comprehension%2C%20and%20contextual%20comprehension.%20The%20benchmark%20also%20contains%20ten%20tasks%2C%20including%20radical%2C%20phonetic%20radical%2C%20homophone%2C%20cloze%2C%20translation%2C%20and%20more%2C%20providing%20a%20comprehensive%20framework%20for%20evaluation.%20We%20convened%20archaeological%20researchers%20to%20conduct%20experimental%20evaluations%2C%20proposed%20an%20ancient%20model%20as%20baseline%2C%20and%20conducted%20extensive%20experiments%20on%20the%20currently%20best-performing%20large%20language%20models.%20The%20experimental%20results%20reveal%20the%20great%20potential%20of%20large%20language%20models%20in%20ancient%20textual%20scenarios%20as%20well%20as%20the%20gap%20with%20humans.%20Our%20research%20aims%20to%20promote%20the%20development%20and%20application%20of%20large%20language%20models%20in%20the%20field%20of%20archaeology%20and%20ancient%20Chinese%20language.&entry.1838667208=http%3A//arxiv.org/abs/2512.17756v1&entry.124074799=Read"},
{"title": "More Consistent Accuracy PINN via Alternating Easy-Hard Training", "author": "Zhaoqian Gao and Min Yanga", "abstract": "Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.", "link": "http://arxiv.org/abs/2512.17607v1", "date": "2025-12-19", "relevancy": 2.4797, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5513}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Consistent%20Accuracy%20PINN%20via%20Alternating%20Easy-Hard%20Training&body=Title%3A%20More%20Consistent%20Accuracy%20PINN%20via%20Alternating%20Easy-Hard%20Training%0AAuthor%3A%20Zhaoqian%20Gao%20and%20Min%20Yanga%0AAbstract%3A%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20recently%20emerged%20as%20a%20prominent%20paradigm%20for%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20yet%20their%20training%20strategies%20remain%20underexplored.%20While%20hard%20prioritization%20methods%20inspired%20by%20finite%20element%20methods%20are%20widely%20adopted%2C%20recent%20research%20suggests%20that%20easy%20prioritization%20can%20also%20be%20effective.%20Nevertheless%2C%20we%20find%20that%20both%20approaches%20exhibit%20notable%20trade-offs%20and%20inconsistent%20performance%20across%20PDE%20types.%20To%20address%20this%20issue%2C%20we%20develop%20a%20hybrid%20strategy%20that%20combines%20the%20strengths%20of%20hard%20and%20easy%20prioritization%20through%20an%20alternating%20training%20algorithm.%20On%20PDEs%20with%20steep%20gradients%2C%20nonlinearity%2C%20and%20high%20dimensionality%2C%20the%20proposed%20method%20achieves%20consistently%20high%20accuracy%2C%20with%20relative%20L2%20errors%20mostly%20in%20the%20range%20of%20O%2810%5E-5%29%20to%20O%2810%5E-6%29%2C%20significantly%20surpassing%20baseline%20methods.%20Moreover%2C%20it%20offers%20greater%20reliability%20across%20diverse%20problems%2C%20whereas%20compared%20approaches%20often%20suffer%20from%20variable%20accuracy%20depending%20on%20the%20PDE.%20This%20work%20provides%20new%20insights%20into%20designing%20hybrid%20training%20strategies%20to%20enhance%20the%20performance%20and%20robustness%20of%20PINNs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Consistent%2520Accuracy%2520PINN%2520via%2520Alternating%2520Easy-Hard%2520Training%26entry.906535625%3DZhaoqian%2520Gao%2520and%2520Min%2520Yanga%26entry.1292438233%3DPhysics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520prominent%2520paradigm%2520for%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520yet%2520their%2520training%2520strategies%2520remain%2520underexplored.%2520While%2520hard%2520prioritization%2520methods%2520inspired%2520by%2520finite%2520element%2520methods%2520are%2520widely%2520adopted%252C%2520recent%2520research%2520suggests%2520that%2520easy%2520prioritization%2520can%2520also%2520be%2520effective.%2520Nevertheless%252C%2520we%2520find%2520that%2520both%2520approaches%2520exhibit%2520notable%2520trade-offs%2520and%2520inconsistent%2520performance%2520across%2520PDE%2520types.%2520To%2520address%2520this%2520issue%252C%2520we%2520develop%2520a%2520hybrid%2520strategy%2520that%2520combines%2520the%2520strengths%2520of%2520hard%2520and%2520easy%2520prioritization%2520through%2520an%2520alternating%2520training%2520algorithm.%2520On%2520PDEs%2520with%2520steep%2520gradients%252C%2520nonlinearity%252C%2520and%2520high%2520dimensionality%252C%2520the%2520proposed%2520method%2520achieves%2520consistently%2520high%2520accuracy%252C%2520with%2520relative%2520L2%2520errors%2520mostly%2520in%2520the%2520range%2520of%2520O%252810%255E-5%2529%2520to%2520O%252810%255E-6%2529%252C%2520significantly%2520surpassing%2520baseline%2520methods.%2520Moreover%252C%2520it%2520offers%2520greater%2520reliability%2520across%2520diverse%2520problems%252C%2520whereas%2520compared%2520approaches%2520often%2520suffer%2520from%2520variable%2520accuracy%2520depending%2520on%2520the%2520PDE.%2520This%2520work%2520provides%2520new%2520insights%2520into%2520designing%2520hybrid%2520training%2520strategies%2520to%2520enhance%2520the%2520performance%2520and%2520robustness%2520of%2520PINNs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Consistent%20Accuracy%20PINN%20via%20Alternating%20Easy-Hard%20Training&entry.906535625=Zhaoqian%20Gao%20and%20Min%20Yanga&entry.1292438233=Physics-informed%20neural%20networks%20%28PINNs%29%20have%20recently%20emerged%20as%20a%20prominent%20paradigm%20for%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20yet%20their%20training%20strategies%20remain%20underexplored.%20While%20hard%20prioritization%20methods%20inspired%20by%20finite%20element%20methods%20are%20widely%20adopted%2C%20recent%20research%20suggests%20that%20easy%20prioritization%20can%20also%20be%20effective.%20Nevertheless%2C%20we%20find%20that%20both%20approaches%20exhibit%20notable%20trade-offs%20and%20inconsistent%20performance%20across%20PDE%20types.%20To%20address%20this%20issue%2C%20we%20develop%20a%20hybrid%20strategy%20that%20combines%20the%20strengths%20of%20hard%20and%20easy%20prioritization%20through%20an%20alternating%20training%20algorithm.%20On%20PDEs%20with%20steep%20gradients%2C%20nonlinearity%2C%20and%20high%20dimensionality%2C%20the%20proposed%20method%20achieves%20consistently%20high%20accuracy%2C%20with%20relative%20L2%20errors%20mostly%20in%20the%20range%20of%20O%2810%5E-5%29%20to%20O%2810%5E-6%29%2C%20significantly%20surpassing%20baseline%20methods.%20Moreover%2C%20it%20offers%20greater%20reliability%20across%20diverse%20problems%2C%20whereas%20compared%20approaches%20often%20suffer%20from%20variable%20accuracy%20depending%20on%20the%20PDE.%20This%20work%20provides%20new%20insights%20into%20designing%20hybrid%20training%20strategies%20to%20enhance%20the%20performance%20and%20robustness%20of%20PINNs.&entry.1838667208=http%3A//arxiv.org/abs/2512.17607v1&entry.124074799=Read"},
{"title": "Unified Acoustic Representations for Screening Neurological and Respiratory Pathologies from Voice", "author": "Ran Piao and Yuan Lu and Hareld Kemps and Tong Xia and Aaqib Saeed", "abstract": "Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.", "link": "http://arxiv.org/abs/2508.20717v2", "date": "2025-12-19", "relevancy": 2.4752, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Acoustic%20Representations%20for%20Screening%20Neurological%20and%20Respiratory%20Pathologies%20from%20Voice&body=Title%3A%20Unified%20Acoustic%20Representations%20for%20Screening%20Neurological%20and%20Respiratory%20Pathologies%20from%20Voice%0AAuthor%3A%20Ran%20Piao%20and%20Yuan%20Lu%20and%20Hareld%20Kemps%20and%20Tong%20Xia%20and%20Aaqib%20Saeed%0AAbstract%3A%20Voice-based%20health%20assessment%20offers%20unprecedented%20opportunities%20for%20scalable%2C%20non-invasive%20disease%20screening%2C%20yet%20existing%20approaches%20typically%20focus%20on%20single%20conditions%20and%20fail%20to%20leverage%20the%20rich%2C%20multi-faceted%20information%20embedded%20in%20speech.%20We%20present%20MARVEL%20%28Multi-task%20Acoustic%20Representations%20for%20Voice-based%20Health%20Analysis%29%2C%20a%20privacy-conscious%20multitask%20learning%20framework%20that%20simultaneously%20detects%20nine%20distinct%20neurological%2C%20respiratory%2C%20and%20voice%20disorders%20using%20only%20derived%20acoustic%20features%2C%20eliminating%20the%20need%20for%20raw%20audio%20transmission.%20Our%20dual-branch%20architecture%20employs%20specialized%20encoders%20with%20task-specific%20heads%20sharing%20a%20common%20acoustic%20backbone%2C%20enabling%20effective%20cross-condition%20knowledge%20transfer.%20Evaluated%20on%20the%20large-scale%20Bridge2AI-Voice%20v2.0%20dataset%2C%20MARVEL%20achieves%20an%20overall%20AUROC%20of%200.78%2C%20with%20exceptional%20performance%20on%20neurological%20disorders%20%28AUROC%20%3D%200.89%29%2C%20particularly%20for%20Alzheimer%27s%20disease/mild%20cognitive%20impairment%20%28AUROC%20%3D%200.97%29.%20Our%20framework%20consistently%20outperforms%20single-modal%20baselines%20by%205-19%25%20and%20surpasses%20state-of-the-art%20self-supervised%20models%20on%207%20of%209%20tasks%2C%20while%20correlation%20analysis%20reveals%20that%20the%20learned%20representations%20exhibit%20meaningful%20similarities%20with%20established%20acoustic%20features%2C%20indicating%20that%20the%20model%27s%20internal%20representations%20are%20consistent%20with%20clinically%20recognized%20acoustic%20patterns.%20By%20demonstrating%20that%20a%20single%20unified%20model%20can%20effectively%20screen%20for%20diverse%20conditions%2C%20this%20work%20establishes%20a%20foundation%20for%20deployable%20voice-based%20diagnostics%20in%20resource-constrained%20and%20remote%20healthcare%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2508.20717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Acoustic%2520Representations%2520for%2520Screening%2520Neurological%2520and%2520Respiratory%2520Pathologies%2520from%2520Voice%26entry.906535625%3DRan%2520Piao%2520and%2520Yuan%2520Lu%2520and%2520Hareld%2520Kemps%2520and%2520Tong%2520Xia%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3DVoice-based%2520health%2520assessment%2520offers%2520unprecedented%2520opportunities%2520for%2520scalable%252C%2520non-invasive%2520disease%2520screening%252C%2520yet%2520existing%2520approaches%2520typically%2520focus%2520on%2520single%2520conditions%2520and%2520fail%2520to%2520leverage%2520the%2520rich%252C%2520multi-faceted%2520information%2520embedded%2520in%2520speech.%2520We%2520present%2520MARVEL%2520%2528Multi-task%2520Acoustic%2520Representations%2520for%2520Voice-based%2520Health%2520Analysis%2529%252C%2520a%2520privacy-conscious%2520multitask%2520learning%2520framework%2520that%2520simultaneously%2520detects%2520nine%2520distinct%2520neurological%252C%2520respiratory%252C%2520and%2520voice%2520disorders%2520using%2520only%2520derived%2520acoustic%2520features%252C%2520eliminating%2520the%2520need%2520for%2520raw%2520audio%2520transmission.%2520Our%2520dual-branch%2520architecture%2520employs%2520specialized%2520encoders%2520with%2520task-specific%2520heads%2520sharing%2520a%2520common%2520acoustic%2520backbone%252C%2520enabling%2520effective%2520cross-condition%2520knowledge%2520transfer.%2520Evaluated%2520on%2520the%2520large-scale%2520Bridge2AI-Voice%2520v2.0%2520dataset%252C%2520MARVEL%2520achieves%2520an%2520overall%2520AUROC%2520of%25200.78%252C%2520with%2520exceptional%2520performance%2520on%2520neurological%2520disorders%2520%2528AUROC%2520%253D%25200.89%2529%252C%2520particularly%2520for%2520Alzheimer%2527s%2520disease/mild%2520cognitive%2520impairment%2520%2528AUROC%2520%253D%25200.97%2529.%2520Our%2520framework%2520consistently%2520outperforms%2520single-modal%2520baselines%2520by%25205-19%2525%2520and%2520surpasses%2520state-of-the-art%2520self-supervised%2520models%2520on%25207%2520of%25209%2520tasks%252C%2520while%2520correlation%2520analysis%2520reveals%2520that%2520the%2520learned%2520representations%2520exhibit%2520meaningful%2520similarities%2520with%2520established%2520acoustic%2520features%252C%2520indicating%2520that%2520the%2520model%2527s%2520internal%2520representations%2520are%2520consistent%2520with%2520clinically%2520recognized%2520acoustic%2520patterns.%2520By%2520demonstrating%2520that%2520a%2520single%2520unified%2520model%2520can%2520effectively%2520screen%2520for%2520diverse%2520conditions%252C%2520this%2520work%2520establishes%2520a%2520foundation%2520for%2520deployable%2520voice-based%2520diagnostics%2520in%2520resource-constrained%2520and%2520remote%2520healthcare%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Acoustic%20Representations%20for%20Screening%20Neurological%20and%20Respiratory%20Pathologies%20from%20Voice&entry.906535625=Ran%20Piao%20and%20Yuan%20Lu%20and%20Hareld%20Kemps%20and%20Tong%20Xia%20and%20Aaqib%20Saeed&entry.1292438233=Voice-based%20health%20assessment%20offers%20unprecedented%20opportunities%20for%20scalable%2C%20non-invasive%20disease%20screening%2C%20yet%20existing%20approaches%20typically%20focus%20on%20single%20conditions%20and%20fail%20to%20leverage%20the%20rich%2C%20multi-faceted%20information%20embedded%20in%20speech.%20We%20present%20MARVEL%20%28Multi-task%20Acoustic%20Representations%20for%20Voice-based%20Health%20Analysis%29%2C%20a%20privacy-conscious%20multitask%20learning%20framework%20that%20simultaneously%20detects%20nine%20distinct%20neurological%2C%20respiratory%2C%20and%20voice%20disorders%20using%20only%20derived%20acoustic%20features%2C%20eliminating%20the%20need%20for%20raw%20audio%20transmission.%20Our%20dual-branch%20architecture%20employs%20specialized%20encoders%20with%20task-specific%20heads%20sharing%20a%20common%20acoustic%20backbone%2C%20enabling%20effective%20cross-condition%20knowledge%20transfer.%20Evaluated%20on%20the%20large-scale%20Bridge2AI-Voice%20v2.0%20dataset%2C%20MARVEL%20achieves%20an%20overall%20AUROC%20of%200.78%2C%20with%20exceptional%20performance%20on%20neurological%20disorders%20%28AUROC%20%3D%200.89%29%2C%20particularly%20for%20Alzheimer%27s%20disease/mild%20cognitive%20impairment%20%28AUROC%20%3D%200.97%29.%20Our%20framework%20consistently%20outperforms%20single-modal%20baselines%20by%205-19%25%20and%20surpasses%20state-of-the-art%20self-supervised%20models%20on%207%20of%209%20tasks%2C%20while%20correlation%20analysis%20reveals%20that%20the%20learned%20representations%20exhibit%20meaningful%20similarities%20with%20established%20acoustic%20features%2C%20indicating%20that%20the%20model%27s%20internal%20representations%20are%20consistent%20with%20clinically%20recognized%20acoustic%20patterns.%20By%20demonstrating%20that%20a%20single%20unified%20model%20can%20effectively%20screen%20for%20diverse%20conditions%2C%20this%20work%20establishes%20a%20foundation%20for%20deployable%20voice-based%20diagnostics%20in%20resource-constrained%20and%20remote%20healthcare%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2508.20717v2&entry.124074799=Read"},
{"title": "Deep Gaussian Process Proximal Policy Optimization", "author": "Matthijs van der Lende and Juan Cardenas-Cartagena", "abstract": "Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.", "link": "http://arxiv.org/abs/2511.18214v2", "date": "2025-12-19", "relevancy": 2.4691, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5002}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4961}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Gaussian%20Process%20Proximal%20Policy%20Optimization&body=Title%3A%20Deep%20Gaussian%20Process%20Proximal%20Policy%20Optimization%0AAuthor%3A%20Matthijs%20van%20der%20Lende%20and%20Juan%20Cardenas-Cartagena%0AAbstract%3A%20Uncertainty%20estimation%20for%20Reinforcement%20Learning%20%28RL%29%20is%20a%20critical%20component%20in%20control%20tasks%20where%20agents%20must%20balance%20safe%20exploration%20and%20efficient%20learning.%20While%20deep%20neural%20networks%20have%20enabled%20breakthroughs%20in%20RL%2C%20they%20often%20lack%20calibrated%20uncertainty%20estimates.%20We%20introduce%20Deep%20Gaussian%20Process%20Proximal%20Policy%20Optimization%20%28GPPO%29%2C%20a%20scalable%2C%20model-free%20actor-critic%20algorithm%20that%20leverages%20Deep%20Gaussian%20Processes%20%28DGPs%29%20to%20approximate%20both%20the%20policy%20and%20value%20function.%20GPPO%20maintains%20competitive%20performance%20with%20respect%20to%20Proximal%20Policy%20Optimization%20on%20standard%20high-dimensional%20continuous%20control%20benchmarks%20while%20providing%20well-calibrated%20uncertainty%20estimates%20that%20can%20inform%20safer%20and%20more%20effective%20exploration.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Gaussian%2520Process%2520Proximal%2520Policy%2520Optimization%26entry.906535625%3DMatthijs%2520van%2520der%2520Lende%2520and%2520Juan%2520Cardenas-Cartagena%26entry.1292438233%3DUncertainty%2520estimation%2520for%2520Reinforcement%2520Learning%2520%2528RL%2529%2520is%2520a%2520critical%2520component%2520in%2520control%2520tasks%2520where%2520agents%2520must%2520balance%2520safe%2520exploration%2520and%2520efficient%2520learning.%2520While%2520deep%2520neural%2520networks%2520have%2520enabled%2520breakthroughs%2520in%2520RL%252C%2520they%2520often%2520lack%2520calibrated%2520uncertainty%2520estimates.%2520We%2520introduce%2520Deep%2520Gaussian%2520Process%2520Proximal%2520Policy%2520Optimization%2520%2528GPPO%2529%252C%2520a%2520scalable%252C%2520model-free%2520actor-critic%2520algorithm%2520that%2520leverages%2520Deep%2520Gaussian%2520Processes%2520%2528DGPs%2529%2520to%2520approximate%2520both%2520the%2520policy%2520and%2520value%2520function.%2520GPPO%2520maintains%2520competitive%2520performance%2520with%2520respect%2520to%2520Proximal%2520Policy%2520Optimization%2520on%2520standard%2520high-dimensional%2520continuous%2520control%2520benchmarks%2520while%2520providing%2520well-calibrated%2520uncertainty%2520estimates%2520that%2520can%2520inform%2520safer%2520and%2520more%2520effective%2520exploration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Gaussian%20Process%20Proximal%20Policy%20Optimization&entry.906535625=Matthijs%20van%20der%20Lende%20and%20Juan%20Cardenas-Cartagena&entry.1292438233=Uncertainty%20estimation%20for%20Reinforcement%20Learning%20%28RL%29%20is%20a%20critical%20component%20in%20control%20tasks%20where%20agents%20must%20balance%20safe%20exploration%20and%20efficient%20learning.%20While%20deep%20neural%20networks%20have%20enabled%20breakthroughs%20in%20RL%2C%20they%20often%20lack%20calibrated%20uncertainty%20estimates.%20We%20introduce%20Deep%20Gaussian%20Process%20Proximal%20Policy%20Optimization%20%28GPPO%29%2C%20a%20scalable%2C%20model-free%20actor-critic%20algorithm%20that%20leverages%20Deep%20Gaussian%20Processes%20%28DGPs%29%20to%20approximate%20both%20the%20policy%20and%20value%20function.%20GPPO%20maintains%20competitive%20performance%20with%20respect%20to%20Proximal%20Policy%20Optimization%20on%20standard%20high-dimensional%20continuous%20control%20benchmarks%20while%20providing%20well-calibrated%20uncertainty%20estimates%20that%20can%20inform%20safer%20and%20more%20effective%20exploration.&entry.1838667208=http%3A//arxiv.org/abs/2511.18214v2&entry.124074799=Read"},
{"title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination", "author": "Teng Wang and Xinxin Zhao and Wenzhe Cai and Changyin Sun", "abstract": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.", "link": "http://arxiv.org/abs/2512.17435v1", "date": "2025-12-19", "relevancy": 2.4663, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6369}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImagineNav%2B%2B%3A%20Prompting%20Vision-Language%20Models%20as%20Embodied%20Navigator%20through%20Scene%20Imagination&body=Title%3A%20ImagineNav%2B%2B%3A%20Prompting%20Vision-Language%20Models%20as%20Embodied%20Navigator%20through%20Scene%20Imagination%0AAuthor%3A%20Teng%20Wang%20and%20Xinxin%20Zhao%20and%20Wenzhe%20Cai%20and%20Changyin%20Sun%0AAbstract%3A%20Visual%20navigation%20is%20a%20fundamental%20capability%20for%20autonomous%20home-assistance%20robots%2C%20enabling%20long-horizon%20tasks%20such%20as%20object%20search.%20While%20recent%20methods%20have%20leveraged%20Large%20Language%20Models%20%28LLMs%29%20to%20incorporate%20commonsense%20reasoning%20and%20improve%20exploration%20efficiency%2C%20their%20planning%20remains%20constrained%20by%20textual%20representations%2C%20which%20cannot%20adequately%20capture%20spatial%20occupancy%20or%20scene%20geometry--critical%20factors%20for%20navigation%20decisions.%20We%20explore%20whether%20Vision-Language%20Models%20%28VLMs%29%20can%20achieve%20mapless%20visual%20navigation%20using%20only%20onboard%20RGB/RGB-D%20streams%2C%20unlocking%20their%20potential%20for%20spatial%20perception%20and%20planning.%20We%20achieve%20this%20through%20an%20imagination-powered%20navigation%20framework%2C%20ImagineNav%2B%2B%2C%20which%20imagines%20future%20observation%20images%20from%20candidate%20robot%20views%20and%20translates%20navigation%20planning%20into%20a%20simple%20best-view%20image%20selection%20problem%20for%20VLMs.%20First%2C%20a%20future-view%20imagination%20module%20distills%20human%20navigation%20preferences%20to%20generate%20semantically%20meaningful%20viewpoints%20with%20high%20exploration%20potential.%20These%20imagined%20views%20then%20serve%20as%20visual%20prompts%20for%20the%20VLM%20to%20identify%20the%20most%20informative%20viewpoint.%20To%20maintain%20spatial%20consistency%2C%20we%20develop%20a%20selective%20foveation%20memory%20mechanism%2C%20which%20hierarchically%20integrates%20keyframe%20observations%20via%20a%20sparse-to-dense%20framework%2C%20constructing%20a%20compact%20yet%20comprehensive%20memory%20for%20long-term%20spatial%20reasoning.%20This%20approach%20transforms%20goal-oriented%20navigation%20into%20a%20series%20of%20tractable%20point-goal%20navigation%20tasks.%20Extensive%20experiments%20on%20open-vocabulary%20object%20and%20instance%20navigation%20benchmarks%20show%20that%20ImagineNav%2B%2B%20achieves%20SOTA%20performance%20in%20mapless%20settings%2C%20even%20surpassing%20most%20map-based%20methods%2C%20highlighting%20the%20importance%20of%20scene%20imagination%20and%20memory%20in%20VLM-based%20spatial%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagineNav%252B%252B%253A%2520Prompting%2520Vision-Language%2520Models%2520as%2520Embodied%2520Navigator%2520through%2520Scene%2520Imagination%26entry.906535625%3DTeng%2520Wang%2520and%2520Xinxin%2520Zhao%2520and%2520Wenzhe%2520Cai%2520and%2520Changyin%2520Sun%26entry.1292438233%3DVisual%2520navigation%2520is%2520a%2520fundamental%2520capability%2520for%2520autonomous%2520home-assistance%2520robots%252C%2520enabling%2520long-horizon%2520tasks%2520such%2520as%2520object%2520search.%2520While%2520recent%2520methods%2520have%2520leveraged%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520incorporate%2520commonsense%2520reasoning%2520and%2520improve%2520exploration%2520efficiency%252C%2520their%2520planning%2520remains%2520constrained%2520by%2520textual%2520representations%252C%2520which%2520cannot%2520adequately%2520capture%2520spatial%2520occupancy%2520or%2520scene%2520geometry--critical%2520factors%2520for%2520navigation%2520decisions.%2520We%2520explore%2520whether%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520can%2520achieve%2520mapless%2520visual%2520navigation%2520using%2520only%2520onboard%2520RGB/RGB-D%2520streams%252C%2520unlocking%2520their%2520potential%2520for%2520spatial%2520perception%2520and%2520planning.%2520We%2520achieve%2520this%2520through%2520an%2520imagination-powered%2520navigation%2520framework%252C%2520ImagineNav%252B%252B%252C%2520which%2520imagines%2520future%2520observation%2520images%2520from%2520candidate%2520robot%2520views%2520and%2520translates%2520navigation%2520planning%2520into%2520a%2520simple%2520best-view%2520image%2520selection%2520problem%2520for%2520VLMs.%2520First%252C%2520a%2520future-view%2520imagination%2520module%2520distills%2520human%2520navigation%2520preferences%2520to%2520generate%2520semantically%2520meaningful%2520viewpoints%2520with%2520high%2520exploration%2520potential.%2520These%2520imagined%2520views%2520then%2520serve%2520as%2520visual%2520prompts%2520for%2520the%2520VLM%2520to%2520identify%2520the%2520most%2520informative%2520viewpoint.%2520To%2520maintain%2520spatial%2520consistency%252C%2520we%2520develop%2520a%2520selective%2520foveation%2520memory%2520mechanism%252C%2520which%2520hierarchically%2520integrates%2520keyframe%2520observations%2520via%2520a%2520sparse-to-dense%2520framework%252C%2520constructing%2520a%2520compact%2520yet%2520comprehensive%2520memory%2520for%2520long-term%2520spatial%2520reasoning.%2520This%2520approach%2520transforms%2520goal-oriented%2520navigation%2520into%2520a%2520series%2520of%2520tractable%2520point-goal%2520navigation%2520tasks.%2520Extensive%2520experiments%2520on%2520open-vocabulary%2520object%2520and%2520instance%2520navigation%2520benchmarks%2520show%2520that%2520ImagineNav%252B%252B%2520achieves%2520SOTA%2520performance%2520in%2520mapless%2520settings%252C%2520even%2520surpassing%2520most%2520map-based%2520methods%252C%2520highlighting%2520the%2520importance%2520of%2520scene%2520imagination%2520and%2520memory%2520in%2520VLM-based%2520spatial%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImagineNav%2B%2B%3A%20Prompting%20Vision-Language%20Models%20as%20Embodied%20Navigator%20through%20Scene%20Imagination&entry.906535625=Teng%20Wang%20and%20Xinxin%20Zhao%20and%20Wenzhe%20Cai%20and%20Changyin%20Sun&entry.1292438233=Visual%20navigation%20is%20a%20fundamental%20capability%20for%20autonomous%20home-assistance%20robots%2C%20enabling%20long-horizon%20tasks%20such%20as%20object%20search.%20While%20recent%20methods%20have%20leveraged%20Large%20Language%20Models%20%28LLMs%29%20to%20incorporate%20commonsense%20reasoning%20and%20improve%20exploration%20efficiency%2C%20their%20planning%20remains%20constrained%20by%20textual%20representations%2C%20which%20cannot%20adequately%20capture%20spatial%20occupancy%20or%20scene%20geometry--critical%20factors%20for%20navigation%20decisions.%20We%20explore%20whether%20Vision-Language%20Models%20%28VLMs%29%20can%20achieve%20mapless%20visual%20navigation%20using%20only%20onboard%20RGB/RGB-D%20streams%2C%20unlocking%20their%20potential%20for%20spatial%20perception%20and%20planning.%20We%20achieve%20this%20through%20an%20imagination-powered%20navigation%20framework%2C%20ImagineNav%2B%2B%2C%20which%20imagines%20future%20observation%20images%20from%20candidate%20robot%20views%20and%20translates%20navigation%20planning%20into%20a%20simple%20best-view%20image%20selection%20problem%20for%20VLMs.%20First%2C%20a%20future-view%20imagination%20module%20distills%20human%20navigation%20preferences%20to%20generate%20semantically%20meaningful%20viewpoints%20with%20high%20exploration%20potential.%20These%20imagined%20views%20then%20serve%20as%20visual%20prompts%20for%20the%20VLM%20to%20identify%20the%20most%20informative%20viewpoint.%20To%20maintain%20spatial%20consistency%2C%20we%20develop%20a%20selective%20foveation%20memory%20mechanism%2C%20which%20hierarchically%20integrates%20keyframe%20observations%20via%20a%20sparse-to-dense%20framework%2C%20constructing%20a%20compact%20yet%20comprehensive%20memory%20for%20long-term%20spatial%20reasoning.%20This%20approach%20transforms%20goal-oriented%20navigation%20into%20a%20series%20of%20tractable%20point-goal%20navigation%20tasks.%20Extensive%20experiments%20on%20open-vocabulary%20object%20and%20instance%20navigation%20benchmarks%20show%20that%20ImagineNav%2B%2B%20achieves%20SOTA%20performance%20in%20mapless%20settings%2C%20even%20surpassing%20most%20map-based%20methods%2C%20highlighting%20the%20importance%20of%20scene%20imagination%20and%20memory%20in%20VLM-based%20spatial%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.17435v1&entry.124074799=Read"},
{"title": "Trust-Region Adaptive Policy Optimization", "author": "Mingyu Su and Jian Guan and Yuxian Gu and Minlie Huang and Hongning Wang", "abstract": "Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.", "link": "http://arxiv.org/abs/2512.17636v1", "date": "2025-12-19", "relevancy": 2.4557, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust-Region%20Adaptive%20Policy%20Optimization&body=Title%3A%20Trust-Region%20Adaptive%20Policy%20Optimization%0AAuthor%3A%20Mingyu%20Su%20and%20Jian%20Guan%20and%20Yuxian%20Gu%20and%20Minlie%20Huang%20and%20Hongning%20Wang%0AAbstract%3A%20Post-training%20methods%2C%20especially%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%2C%20play%20an%20important%20role%20in%20improving%20large%20language%20models%27%20%28LLMs%29%20complex%20reasoning%20abilities.%20However%2C%20the%20dominant%20two-stage%20pipeline%20%28SFT%20then%20RL%29%20suffers%20from%20a%20key%20inconsistency%3A%20SFT%20enforces%20rigid%20imitation%20that%20suppresses%20exploration%20and%20induces%20forgetting%2C%20limiting%20RL%27s%20potential%20for%20improvements.%20We%20address%20this%20inefficiency%20with%20TRAPO%20%28%5Ctextbf%7BT%7Drust-%5Ctextbf%7BR%7Degion%20%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BP%7Dolicy%20%5Ctextbf%7BO%7Dptimization%29%2C%20a%20hybrid%20framework%20that%20interleaves%20SFT%20and%20RL%20within%20each%20training%20instance%20by%20optimizing%20SFT%20loss%20on%20expert%20prefixes%20and%20RL%20loss%20on%20the%20model%27s%20own%20completions%2C%20unifying%20external%20supervision%20and%20self-exploration.%20To%20stabilize%20training%2C%20we%20introduce%20Trust-Region%20SFT%20%28TrSFT%29%2C%20which%20minimizes%20forward%20KL%20divergence%20inside%20a%20trust%20region%20but%20attenuates%20optimization%20outside%2C%20effectively%20shifting%20toward%20reverse%20KL%20and%20yielding%20stable%2C%20mode-seeking%20updates%20favorable%20for%20RL.%20An%20adaptive%20prefix-selection%20mechanism%20further%20allocates%20expert%20guidance%20based%20on%20measured%20utility.%20Experiments%20on%20five%20mathematical%20reasoning%20benchmarks%20show%20that%20TRAPO%20consistently%20surpasses%20standard%20SFT%2C%20RL%2C%20and%20SFT-then-RL%20pipelines%2C%20as%20well%20as%20recent%20state-of-the-art%20approaches%2C%20establishing%20a%20strong%20new%20paradigm%20for%20reasoning-enhanced%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust-Region%2520Adaptive%2520Policy%2520Optimization%26entry.906535625%3DMingyu%2520Su%2520and%2520Jian%2520Guan%2520and%2520Yuxian%2520Gu%2520and%2520Minlie%2520Huang%2520and%2520Hongning%2520Wang%26entry.1292438233%3DPost-training%2520methods%252C%2520especially%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520play%2520an%2520important%2520role%2520in%2520improving%2520large%2520language%2520models%2527%2520%2528LLMs%2529%2520complex%2520reasoning%2520abilities.%2520However%252C%2520the%2520dominant%2520two-stage%2520pipeline%2520%2528SFT%2520then%2520RL%2529%2520suffers%2520from%2520a%2520key%2520inconsistency%253A%2520SFT%2520enforces%2520rigid%2520imitation%2520that%2520suppresses%2520exploration%2520and%2520induces%2520forgetting%252C%2520limiting%2520RL%2527s%2520potential%2520for%2520improvements.%2520We%2520address%2520this%2520inefficiency%2520with%2520TRAPO%2520%2528%255Ctextbf%257BT%257Drust-%255Ctextbf%257BR%257Degion%2520%255Ctextbf%257BA%257Ddaptive%2520%255Ctextbf%257BP%257Dolicy%2520%255Ctextbf%257BO%257Dptimization%2529%252C%2520a%2520hybrid%2520framework%2520that%2520interleaves%2520SFT%2520and%2520RL%2520within%2520each%2520training%2520instance%2520by%2520optimizing%2520SFT%2520loss%2520on%2520expert%2520prefixes%2520and%2520RL%2520loss%2520on%2520the%2520model%2527s%2520own%2520completions%252C%2520unifying%2520external%2520supervision%2520and%2520self-exploration.%2520To%2520stabilize%2520training%252C%2520we%2520introduce%2520Trust-Region%2520SFT%2520%2528TrSFT%2529%252C%2520which%2520minimizes%2520forward%2520KL%2520divergence%2520inside%2520a%2520trust%2520region%2520but%2520attenuates%2520optimization%2520outside%252C%2520effectively%2520shifting%2520toward%2520reverse%2520KL%2520and%2520yielding%2520stable%252C%2520mode-seeking%2520updates%2520favorable%2520for%2520RL.%2520An%2520adaptive%2520prefix-selection%2520mechanism%2520further%2520allocates%2520expert%2520guidance%2520based%2520on%2520measured%2520utility.%2520Experiments%2520on%2520five%2520mathematical%2520reasoning%2520benchmarks%2520show%2520that%2520TRAPO%2520consistently%2520surpasses%2520standard%2520SFT%252C%2520RL%252C%2520and%2520SFT-then-RL%2520pipelines%252C%2520as%2520well%2520as%2520recent%2520state-of-the-art%2520approaches%252C%2520establishing%2520a%2520strong%2520new%2520paradigm%2520for%2520reasoning-enhanced%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust-Region%20Adaptive%20Policy%20Optimization&entry.906535625=Mingyu%20Su%20and%20Jian%20Guan%20and%20Yuxian%20Gu%20and%20Minlie%20Huang%20and%20Hongning%20Wang&entry.1292438233=Post-training%20methods%2C%20especially%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%2C%20play%20an%20important%20role%20in%20improving%20large%20language%20models%27%20%28LLMs%29%20complex%20reasoning%20abilities.%20However%2C%20the%20dominant%20two-stage%20pipeline%20%28SFT%20then%20RL%29%20suffers%20from%20a%20key%20inconsistency%3A%20SFT%20enforces%20rigid%20imitation%20that%20suppresses%20exploration%20and%20induces%20forgetting%2C%20limiting%20RL%27s%20potential%20for%20improvements.%20We%20address%20this%20inefficiency%20with%20TRAPO%20%28%5Ctextbf%7BT%7Drust-%5Ctextbf%7BR%7Degion%20%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BP%7Dolicy%20%5Ctextbf%7BO%7Dptimization%29%2C%20a%20hybrid%20framework%20that%20interleaves%20SFT%20and%20RL%20within%20each%20training%20instance%20by%20optimizing%20SFT%20loss%20on%20expert%20prefixes%20and%20RL%20loss%20on%20the%20model%27s%20own%20completions%2C%20unifying%20external%20supervision%20and%20self-exploration.%20To%20stabilize%20training%2C%20we%20introduce%20Trust-Region%20SFT%20%28TrSFT%29%2C%20which%20minimizes%20forward%20KL%20divergence%20inside%20a%20trust%20region%20but%20attenuates%20optimization%20outside%2C%20effectively%20shifting%20toward%20reverse%20KL%20and%20yielding%20stable%2C%20mode-seeking%20updates%20favorable%20for%20RL.%20An%20adaptive%20prefix-selection%20mechanism%20further%20allocates%20expert%20guidance%20based%20on%20measured%20utility.%20Experiments%20on%20five%20mathematical%20reasoning%20benchmarks%20show%20that%20TRAPO%20consistently%20surpasses%20standard%20SFT%2C%20RL%2C%20and%20SFT-then-RL%20pipelines%2C%20as%20well%20as%20recent%20state-of-the-art%20approaches%2C%20establishing%20a%20strong%20new%20paradigm%20for%20reasoning-enhanced%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.17636v1&entry.124074799=Read"},
{"title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion", "author": "Hoiyeong Jin and Hyojin Jang and Jeongho Kim and Junha Hyung and Kinam Kim and Dongjin Kim and Huijin Choi and Hyeonji Kim and Jaegul Choo", "abstract": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.", "link": "http://arxiv.org/abs/2512.17504v1", "date": "2025-12-19", "relevancy": 2.4527, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6336}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.614}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsertAnywhere%3A%20Bridging%204D%20Scene%20Geometry%20and%20Diffusion%20Models%20for%20Realistic%20Video%20Object%20Insertion&body=Title%3A%20InsertAnywhere%3A%20Bridging%204D%20Scene%20Geometry%20and%20Diffusion%20Models%20for%20Realistic%20Video%20Object%20Insertion%0AAuthor%3A%20Hoiyeong%20Jin%20and%20Hyojin%20Jang%20and%20Jeongho%20Kim%20and%20Junha%20Hyung%20and%20Kinam%20Kim%20and%20Dongjin%20Kim%20and%20Huijin%20Choi%20and%20Hyeonji%20Kim%20and%20Jaegul%20Choo%0AAbstract%3A%20Recent%20advances%20in%20diffusion-based%20video%20generation%20have%20opened%20new%20possibilities%20for%20controllable%20video%20editing%2C%20yet%20realistic%20video%20object%20insertion%20%28VOI%29%20remains%20challenging%20due%20to%20limited%204D%20scene%20understanding%20and%20inadequate%20handling%20of%20occlusion%20and%20lighting%20effects.%20We%20present%20InsertAnywhere%2C%20a%20new%20VOI%20framework%20that%20achieves%20geometrically%20consistent%20object%20placement%20and%20appearance-faithful%20video%20synthesis.%20Our%20method%20begins%20with%20a%204D%20aware%20mask%20generation%20module%20that%20reconstructs%20the%20scene%20geometry%20and%20propagates%20user%20specified%20object%20placement%20across%20frames%20while%20maintaining%20temporal%20coherence%20and%20occlusion%20consistency.%20Building%20upon%20this%20spatial%20foundation%2C%20we%20extend%20a%20diffusion%20based%20video%20generation%20model%20to%20jointly%20synthesize%20the%20inserted%20object%20and%20its%20surrounding%20local%20variations%20such%20as%20illumination%20and%20shading.%20To%20enable%20supervised%20training%2C%20we%20introduce%20ROSE%2B%2B%2C%20an%20illumination%20aware%20synthetic%20dataset%20constructed%20by%20transforming%20the%20ROSE%20object%20removal%20dataset%20into%20triplets%20of%20object%20removed%20video%2C%20object%20present%20video%2C%20and%20a%20VLM%20generated%20reference%20image.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20framework%20produces%20geometrically%20plausible%20and%20visually%20coherent%20object%20insertions%20across%20diverse%20real%20world%20scenarios%2C%20significantly%20outperforming%20existing%20research%20and%20commercial%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsertAnywhere%253A%2520Bridging%25204D%2520Scene%2520Geometry%2520and%2520Diffusion%2520Models%2520for%2520Realistic%2520Video%2520Object%2520Insertion%26entry.906535625%3DHoiyeong%2520Jin%2520and%2520Hyojin%2520Jang%2520and%2520Jeongho%2520Kim%2520and%2520Junha%2520Hyung%2520and%2520Kinam%2520Kim%2520and%2520Dongjin%2520Kim%2520and%2520Huijin%2520Choi%2520and%2520Hyeonji%2520Kim%2520and%2520Jaegul%2520Choo%26entry.1292438233%3DRecent%2520advances%2520in%2520diffusion-based%2520video%2520generation%2520have%2520opened%2520new%2520possibilities%2520for%2520controllable%2520video%2520editing%252C%2520yet%2520realistic%2520video%2520object%2520insertion%2520%2528VOI%2529%2520remains%2520challenging%2520due%2520to%2520limited%25204D%2520scene%2520understanding%2520and%2520inadequate%2520handling%2520of%2520occlusion%2520and%2520lighting%2520effects.%2520We%2520present%2520InsertAnywhere%252C%2520a%2520new%2520VOI%2520framework%2520that%2520achieves%2520geometrically%2520consistent%2520object%2520placement%2520and%2520appearance-faithful%2520video%2520synthesis.%2520Our%2520method%2520begins%2520with%2520a%25204D%2520aware%2520mask%2520generation%2520module%2520that%2520reconstructs%2520the%2520scene%2520geometry%2520and%2520propagates%2520user%2520specified%2520object%2520placement%2520across%2520frames%2520while%2520maintaining%2520temporal%2520coherence%2520and%2520occlusion%2520consistency.%2520Building%2520upon%2520this%2520spatial%2520foundation%252C%2520we%2520extend%2520a%2520diffusion%2520based%2520video%2520generation%2520model%2520to%2520jointly%2520synthesize%2520the%2520inserted%2520object%2520and%2520its%2520surrounding%2520local%2520variations%2520such%2520as%2520illumination%2520and%2520shading.%2520To%2520enable%2520supervised%2520training%252C%2520we%2520introduce%2520ROSE%252B%252B%252C%2520an%2520illumination%2520aware%2520synthetic%2520dataset%2520constructed%2520by%2520transforming%2520the%2520ROSE%2520object%2520removal%2520dataset%2520into%2520triplets%2520of%2520object%2520removed%2520video%252C%2520object%2520present%2520video%252C%2520and%2520a%2520VLM%2520generated%2520reference%2520image.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520framework%2520produces%2520geometrically%2520plausible%2520and%2520visually%2520coherent%2520object%2520insertions%2520across%2520diverse%2520real%2520world%2520scenarios%252C%2520significantly%2520outperforming%2520existing%2520research%2520and%2520commercial%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsertAnywhere%3A%20Bridging%204D%20Scene%20Geometry%20and%20Diffusion%20Models%20for%20Realistic%20Video%20Object%20Insertion&entry.906535625=Hoiyeong%20Jin%20and%20Hyojin%20Jang%20and%20Jeongho%20Kim%20and%20Junha%20Hyung%20and%20Kinam%20Kim%20and%20Dongjin%20Kim%20and%20Huijin%20Choi%20and%20Hyeonji%20Kim%20and%20Jaegul%20Choo&entry.1292438233=Recent%20advances%20in%20diffusion-based%20video%20generation%20have%20opened%20new%20possibilities%20for%20controllable%20video%20editing%2C%20yet%20realistic%20video%20object%20insertion%20%28VOI%29%20remains%20challenging%20due%20to%20limited%204D%20scene%20understanding%20and%20inadequate%20handling%20of%20occlusion%20and%20lighting%20effects.%20We%20present%20InsertAnywhere%2C%20a%20new%20VOI%20framework%20that%20achieves%20geometrically%20consistent%20object%20placement%20and%20appearance-faithful%20video%20synthesis.%20Our%20method%20begins%20with%20a%204D%20aware%20mask%20generation%20module%20that%20reconstructs%20the%20scene%20geometry%20and%20propagates%20user%20specified%20object%20placement%20across%20frames%20while%20maintaining%20temporal%20coherence%20and%20occlusion%20consistency.%20Building%20upon%20this%20spatial%20foundation%2C%20we%20extend%20a%20diffusion%20based%20video%20generation%20model%20to%20jointly%20synthesize%20the%20inserted%20object%20and%20its%20surrounding%20local%20variations%20such%20as%20illumination%20and%20shading.%20To%20enable%20supervised%20training%2C%20we%20introduce%20ROSE%2B%2B%2C%20an%20illumination%20aware%20synthetic%20dataset%20constructed%20by%20transforming%20the%20ROSE%20object%20removal%20dataset%20into%20triplets%20of%20object%20removed%20video%2C%20object%20present%20video%2C%20and%20a%20VLM%20generated%20reference%20image.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20framework%20produces%20geometrically%20plausible%20and%20visually%20coherent%20object%20insertions%20across%20diverse%20real%20world%20scenarios%2C%20significantly%20outperforming%20existing%20research%20and%20commercial%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.17504v1&entry.124074799=Read"},
{"title": "Adaptive Focus Memory for Language Models", "author": "Christopher Cruz", "abstract": "Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.\n  We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.\n  We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.\n  These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.", "link": "http://arxiv.org/abs/2511.12712v2", "date": "2025-12-19", "relevancy": 2.4526, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Focus%20Memory%20for%20Language%20Models&body=Title%3A%20Adaptive%20Focus%20Memory%20for%20Language%20Models%0AAuthor%3A%20Christopher%20Cruz%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20multi-turn%20dialogue%20settings%2C%20yet%20their%20behavior%20remains%20bottlenecked%20by%20naive%20history%20management%20strategies.%20Replaying%20the%20full%20conversation%20at%20every%20turn%20is%20simple%20but%20costly%2C%20while%20recency-based%20truncation%20or%20static%20summarization%20often%20causes%20early%2C%20high-impact%20user%20constraints%20to%20drift%20out%20of%20effective%20context.%20As%20a%20result%2C%20models%20may%20retain%20text%20without%20reliably%20applying%20it%20when%20it%20matters.%0A%20%20We%20present%20Adaptive%20Focus%20Memory%20%28AFM%29%2C%20a%20lightweight%20context%20management%20system%20that%20dynamically%20assigns%20each%20past%20message%20one%20of%20three%20fidelity%20levels%3A%20Full%2C%20Compressed%2C%20or%20Placeholder%2C%20based%20on%20semantic%20relevance%2C%20temporal%20decay%2C%20and%20importance%20classification.%20AFM%20packs%20messages%20chronologically%20under%20a%20fixed%20token%20budget%2C%20preserving%20critical%20constraints%20at%20high%20fidelity%20while%20allowing%20low-importance%20context%20to%20degrade%20gracefully.%0A%20%20We%20evaluate%20AFM%20on%20two%20multi-turn%20dialogue%20benchmarks%20designed%20to%20stress%20long-horizon%20constraint%20preservation%3A%20a%20safety-critical%20travel%20scenario%20involving%20a%20user%20with%20a%20severe%20peanut%20allergy%2C%20and%20a%20policy-critical%20tax%20compliance%20scenario%20involving%20an%20illegal%20evasion%20request.%20Under%20strict%20grading%20that%20requires%20both%20explicit%20constraint%20recall%20and%20appropriately%20conditioned%20generation%2C%20AFM%20succeeds%20in%2083.3%20percent%20of%20allergy%20runs%20where%20all%20baseline%20strategies%20fail%2C%20and%20preserves%20correct%20refusal%20behavior%20on%20the%20tax%20benchmark.%0A%20%20These%20results%20demonstrate%20that%20effective%20dialogue%20memory%20requires%20more%20than%20retaining%20prior%20text.%20Selectively%20allocating%20fidelity%20across%20past%20messages%20enables%20reliable%20constraint%20preservation%20under%20bounded%20context%20growth%2C%20without%20modifying%20model%20weights%20or%20introducing%20external%20retrieval%20infrastructure.%20We%20release%20an%20open-source%20implementation%20of%20AFM%20compatible%20with%20OpenAI-style%20chat%20APIs%20to%20support%20reproducible%20research%20and%20practical%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Focus%2520Memory%2520for%2520Language%2520Models%26entry.906535625%3DChristopher%2520Cruz%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520multi-turn%2520dialogue%2520settings%252C%2520yet%2520their%2520behavior%2520remains%2520bottlenecked%2520by%2520naive%2520history%2520management%2520strategies.%2520Replaying%2520the%2520full%2520conversation%2520at%2520every%2520turn%2520is%2520simple%2520but%2520costly%252C%2520while%2520recency-based%2520truncation%2520or%2520static%2520summarization%2520often%2520causes%2520early%252C%2520high-impact%2520user%2520constraints%2520to%2520drift%2520out%2520of%2520effective%2520context.%2520As%2520a%2520result%252C%2520models%2520may%2520retain%2520text%2520without%2520reliably%2520applying%2520it%2520when%2520it%2520matters.%250A%2520%2520We%2520present%2520Adaptive%2520Focus%2520Memory%2520%2528AFM%2529%252C%2520a%2520lightweight%2520context%2520management%2520system%2520that%2520dynamically%2520assigns%2520each%2520past%2520message%2520one%2520of%2520three%2520fidelity%2520levels%253A%2520Full%252C%2520Compressed%252C%2520or%2520Placeholder%252C%2520based%2520on%2520semantic%2520relevance%252C%2520temporal%2520decay%252C%2520and%2520importance%2520classification.%2520AFM%2520packs%2520messages%2520chronologically%2520under%2520a%2520fixed%2520token%2520budget%252C%2520preserving%2520critical%2520constraints%2520at%2520high%2520fidelity%2520while%2520allowing%2520low-importance%2520context%2520to%2520degrade%2520gracefully.%250A%2520%2520We%2520evaluate%2520AFM%2520on%2520two%2520multi-turn%2520dialogue%2520benchmarks%2520designed%2520to%2520stress%2520long-horizon%2520constraint%2520preservation%253A%2520a%2520safety-critical%2520travel%2520scenario%2520involving%2520a%2520user%2520with%2520a%2520severe%2520peanut%2520allergy%252C%2520and%2520a%2520policy-critical%2520tax%2520compliance%2520scenario%2520involving%2520an%2520illegal%2520evasion%2520request.%2520Under%2520strict%2520grading%2520that%2520requires%2520both%2520explicit%2520constraint%2520recall%2520and%2520appropriately%2520conditioned%2520generation%252C%2520AFM%2520succeeds%2520in%252083.3%2520percent%2520of%2520allergy%2520runs%2520where%2520all%2520baseline%2520strategies%2520fail%252C%2520and%2520preserves%2520correct%2520refusal%2520behavior%2520on%2520the%2520tax%2520benchmark.%250A%2520%2520These%2520results%2520demonstrate%2520that%2520effective%2520dialogue%2520memory%2520requires%2520more%2520than%2520retaining%2520prior%2520text.%2520Selectively%2520allocating%2520fidelity%2520across%2520past%2520messages%2520enables%2520reliable%2520constraint%2520preservation%2520under%2520bounded%2520context%2520growth%252C%2520without%2520modifying%2520model%2520weights%2520or%2520introducing%2520external%2520retrieval%2520infrastructure.%2520We%2520release%2520an%2520open-source%2520implementation%2520of%2520AFM%2520compatible%2520with%2520OpenAI-style%2520chat%2520APIs%2520to%2520support%2520reproducible%2520research%2520and%2520practical%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Focus%20Memory%20for%20Language%20Models&entry.906535625=Christopher%20Cruz&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20multi-turn%20dialogue%20settings%2C%20yet%20their%20behavior%20remains%20bottlenecked%20by%20naive%20history%20management%20strategies.%20Replaying%20the%20full%20conversation%20at%20every%20turn%20is%20simple%20but%20costly%2C%20while%20recency-based%20truncation%20or%20static%20summarization%20often%20causes%20early%2C%20high-impact%20user%20constraints%20to%20drift%20out%20of%20effective%20context.%20As%20a%20result%2C%20models%20may%20retain%20text%20without%20reliably%20applying%20it%20when%20it%20matters.%0A%20%20We%20present%20Adaptive%20Focus%20Memory%20%28AFM%29%2C%20a%20lightweight%20context%20management%20system%20that%20dynamically%20assigns%20each%20past%20message%20one%20of%20three%20fidelity%20levels%3A%20Full%2C%20Compressed%2C%20or%20Placeholder%2C%20based%20on%20semantic%20relevance%2C%20temporal%20decay%2C%20and%20importance%20classification.%20AFM%20packs%20messages%20chronologically%20under%20a%20fixed%20token%20budget%2C%20preserving%20critical%20constraints%20at%20high%20fidelity%20while%20allowing%20low-importance%20context%20to%20degrade%20gracefully.%0A%20%20We%20evaluate%20AFM%20on%20two%20multi-turn%20dialogue%20benchmarks%20designed%20to%20stress%20long-horizon%20constraint%20preservation%3A%20a%20safety-critical%20travel%20scenario%20involving%20a%20user%20with%20a%20severe%20peanut%20allergy%2C%20and%20a%20policy-critical%20tax%20compliance%20scenario%20involving%20an%20illegal%20evasion%20request.%20Under%20strict%20grading%20that%20requires%20both%20explicit%20constraint%20recall%20and%20appropriately%20conditioned%20generation%2C%20AFM%20succeeds%20in%2083.3%20percent%20of%20allergy%20runs%20where%20all%20baseline%20strategies%20fail%2C%20and%20preserves%20correct%20refusal%20behavior%20on%20the%20tax%20benchmark.%0A%20%20These%20results%20demonstrate%20that%20effective%20dialogue%20memory%20requires%20more%20than%20retaining%20prior%20text.%20Selectively%20allocating%20fidelity%20across%20past%20messages%20enables%20reliable%20constraint%20preservation%20under%20bounded%20context%20growth%2C%20without%20modifying%20model%20weights%20or%20introducing%20external%20retrieval%20infrastructure.%20We%20release%20an%20open-source%20implementation%20of%20AFM%20compatible%20with%20OpenAI-style%20chat%20APIs%20to%20support%20reproducible%20research%20and%20practical%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2511.12712v2&entry.124074799=Read"},
{"title": "UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation", "author": "Kangchen Lv and Mingrui Yu and Shihefeng Wang and Xiangyang Ji and Xiang Li", "abstract": "Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.", "link": "http://arxiv.org/abs/2512.17764v1", "date": "2025-12-19", "relevancy": 2.4208, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6243}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5966}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniStateDLO%3A%20Unified%20Generative%20State%20Estimation%20and%20Tracking%20of%20Deformable%20Linear%20Objects%20Under%20Occlusion%20for%20Constrained%20Manipulation&body=Title%3A%20UniStateDLO%3A%20Unified%20Generative%20State%20Estimation%20and%20Tracking%20of%20Deformable%20Linear%20Objects%20Under%20Occlusion%20for%20Constrained%20Manipulation%0AAuthor%3A%20Kangchen%20Lv%20and%20Mingrui%20Yu%20and%20Shihefeng%20Wang%20and%20Xiangyang%20Ji%20and%20Xiang%20Li%0AAbstract%3A%20Perception%20of%20deformable%20linear%20objects%20%28DLOs%29%2C%20such%20as%20cables%2C%20ropes%2C%20and%20wires%2C%20is%20the%20cornerstone%20for%20successful%20downstream%20manipulation.%20Although%20vision-based%20methods%20have%20been%20extensively%20explored%2C%20they%20remain%20highly%20vulnerable%20to%20occlusions%20that%20commonly%20arise%20in%20constrained%20manipulation%20environments%20due%20to%20surrounding%20obstacles%2C%20large%20and%20varying%20deformations%2C%20and%20limited%20viewpoints.%20Moreover%2C%20the%20high%20dimensionality%20of%20the%20state%20space%2C%20the%20lack%20of%20distinctive%20visual%20features%2C%20and%20the%20presence%20of%20sensor%20noises%20further%20compound%20the%20challenges%20of%20reliable%20DLO%20perception.%20To%20address%20these%20open%20issues%2C%20this%20paper%20presents%20UniStateDLO%2C%20the%20first%20complete%20DLO%20perception%20pipeline%20with%20deep-learning%20methods%20that%20achieves%20robust%20performance%20under%20severe%20occlusion%2C%20covering%20both%20single-frame%20state%20estimation%20and%20cross-frame%20state%20tracking%20from%20partial%20point%20clouds.%20Both%20tasks%20are%20formulated%20as%20conditional%20generative%20problems%2C%20leveraging%20the%20strong%20capability%20of%20diffusion%20models%20to%20capture%20the%20complex%20mapping%20between%20highly%20partial%20observations%20and%20high-dimensional%20DLO%20states.%20UniStateDLO%20effectively%20handles%20a%20wide%20range%20of%20occlusion%20patterns%2C%20including%20initial%20occlusion%2C%20self-occlusion%2C%20and%20occlusion%20caused%20by%20multiple%20objects.%20In%20addition%2C%20it%20exhibits%20strong%20data%20efficiency%20as%20the%20entire%20network%20is%20trained%20solely%20on%20a%20large-scale%20synthetic%20dataset%2C%20enabling%20zero-shot%20sim-to-real%20generalization%20without%20any%20real-world%20training%20data.%20Comprehensive%20simulation%20and%20real-world%20experiments%20demonstrate%20that%20UniStateDLO%20outperforms%20all%20state-of-the-art%20baselines%20in%20both%20estimation%20and%20tracking%2C%20producing%20globally%20smooth%20yet%20locally%20precise%20DLO%20state%20predictions%20in%20real%20time%2C%20even%20under%20substantial%20occlusions.%20Its%20integration%20as%20the%20front-end%20module%20in%20a%20closed-loop%20DLO%20manipulation%20system%20further%20demonstrates%20its%20ability%20to%20support%20stable%20feedback%20control%20in%20complex%2C%20constrained%203-D%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniStateDLO%253A%2520Unified%2520Generative%2520State%2520Estimation%2520and%2520Tracking%2520of%2520Deformable%2520Linear%2520Objects%2520Under%2520Occlusion%2520for%2520Constrained%2520Manipulation%26entry.906535625%3DKangchen%2520Lv%2520and%2520Mingrui%2520Yu%2520and%2520Shihefeng%2520Wang%2520and%2520Xiangyang%2520Ji%2520and%2520Xiang%2520Li%26entry.1292438233%3DPerception%2520of%2520deformable%2520linear%2520objects%2520%2528DLOs%2529%252C%2520such%2520as%2520cables%252C%2520ropes%252C%2520and%2520wires%252C%2520is%2520the%2520cornerstone%2520for%2520successful%2520downstream%2520manipulation.%2520Although%2520vision-based%2520methods%2520have%2520been%2520extensively%2520explored%252C%2520they%2520remain%2520highly%2520vulnerable%2520to%2520occlusions%2520that%2520commonly%2520arise%2520in%2520constrained%2520manipulation%2520environments%2520due%2520to%2520surrounding%2520obstacles%252C%2520large%2520and%2520varying%2520deformations%252C%2520and%2520limited%2520viewpoints.%2520Moreover%252C%2520the%2520high%2520dimensionality%2520of%2520the%2520state%2520space%252C%2520the%2520lack%2520of%2520distinctive%2520visual%2520features%252C%2520and%2520the%2520presence%2520of%2520sensor%2520noises%2520further%2520compound%2520the%2520challenges%2520of%2520reliable%2520DLO%2520perception.%2520To%2520address%2520these%2520open%2520issues%252C%2520this%2520paper%2520presents%2520UniStateDLO%252C%2520the%2520first%2520complete%2520DLO%2520perception%2520pipeline%2520with%2520deep-learning%2520methods%2520that%2520achieves%2520robust%2520performance%2520under%2520severe%2520occlusion%252C%2520covering%2520both%2520single-frame%2520state%2520estimation%2520and%2520cross-frame%2520state%2520tracking%2520from%2520partial%2520point%2520clouds.%2520Both%2520tasks%2520are%2520formulated%2520as%2520conditional%2520generative%2520problems%252C%2520leveraging%2520the%2520strong%2520capability%2520of%2520diffusion%2520models%2520to%2520capture%2520the%2520complex%2520mapping%2520between%2520highly%2520partial%2520observations%2520and%2520high-dimensional%2520DLO%2520states.%2520UniStateDLO%2520effectively%2520handles%2520a%2520wide%2520range%2520of%2520occlusion%2520patterns%252C%2520including%2520initial%2520occlusion%252C%2520self-occlusion%252C%2520and%2520occlusion%2520caused%2520by%2520multiple%2520objects.%2520In%2520addition%252C%2520it%2520exhibits%2520strong%2520data%2520efficiency%2520as%2520the%2520entire%2520network%2520is%2520trained%2520solely%2520on%2520a%2520large-scale%2520synthetic%2520dataset%252C%2520enabling%2520zero-shot%2520sim-to-real%2520generalization%2520without%2520any%2520real-world%2520training%2520data.%2520Comprehensive%2520simulation%2520and%2520real-world%2520experiments%2520demonstrate%2520that%2520UniStateDLO%2520outperforms%2520all%2520state-of-the-art%2520baselines%2520in%2520both%2520estimation%2520and%2520tracking%252C%2520producing%2520globally%2520smooth%2520yet%2520locally%2520precise%2520DLO%2520state%2520predictions%2520in%2520real%2520time%252C%2520even%2520under%2520substantial%2520occlusions.%2520Its%2520integration%2520as%2520the%2520front-end%2520module%2520in%2520a%2520closed-loop%2520DLO%2520manipulation%2520system%2520further%2520demonstrates%2520its%2520ability%2520to%2520support%2520stable%2520feedback%2520control%2520in%2520complex%252C%2520constrained%25203-D%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniStateDLO%3A%20Unified%20Generative%20State%20Estimation%20and%20Tracking%20of%20Deformable%20Linear%20Objects%20Under%20Occlusion%20for%20Constrained%20Manipulation&entry.906535625=Kangchen%20Lv%20and%20Mingrui%20Yu%20and%20Shihefeng%20Wang%20and%20Xiangyang%20Ji%20and%20Xiang%20Li&entry.1292438233=Perception%20of%20deformable%20linear%20objects%20%28DLOs%29%2C%20such%20as%20cables%2C%20ropes%2C%20and%20wires%2C%20is%20the%20cornerstone%20for%20successful%20downstream%20manipulation.%20Although%20vision-based%20methods%20have%20been%20extensively%20explored%2C%20they%20remain%20highly%20vulnerable%20to%20occlusions%20that%20commonly%20arise%20in%20constrained%20manipulation%20environments%20due%20to%20surrounding%20obstacles%2C%20large%20and%20varying%20deformations%2C%20and%20limited%20viewpoints.%20Moreover%2C%20the%20high%20dimensionality%20of%20the%20state%20space%2C%20the%20lack%20of%20distinctive%20visual%20features%2C%20and%20the%20presence%20of%20sensor%20noises%20further%20compound%20the%20challenges%20of%20reliable%20DLO%20perception.%20To%20address%20these%20open%20issues%2C%20this%20paper%20presents%20UniStateDLO%2C%20the%20first%20complete%20DLO%20perception%20pipeline%20with%20deep-learning%20methods%20that%20achieves%20robust%20performance%20under%20severe%20occlusion%2C%20covering%20both%20single-frame%20state%20estimation%20and%20cross-frame%20state%20tracking%20from%20partial%20point%20clouds.%20Both%20tasks%20are%20formulated%20as%20conditional%20generative%20problems%2C%20leveraging%20the%20strong%20capability%20of%20diffusion%20models%20to%20capture%20the%20complex%20mapping%20between%20highly%20partial%20observations%20and%20high-dimensional%20DLO%20states.%20UniStateDLO%20effectively%20handles%20a%20wide%20range%20of%20occlusion%20patterns%2C%20including%20initial%20occlusion%2C%20self-occlusion%2C%20and%20occlusion%20caused%20by%20multiple%20objects.%20In%20addition%2C%20it%20exhibits%20strong%20data%20efficiency%20as%20the%20entire%20network%20is%20trained%20solely%20on%20a%20large-scale%20synthetic%20dataset%2C%20enabling%20zero-shot%20sim-to-real%20generalization%20without%20any%20real-world%20training%20data.%20Comprehensive%20simulation%20and%20real-world%20experiments%20demonstrate%20that%20UniStateDLO%20outperforms%20all%20state-of-the-art%20baselines%20in%20both%20estimation%20and%20tracking%2C%20producing%20globally%20smooth%20yet%20locally%20precise%20DLO%20state%20predictions%20in%20real%20time%2C%20even%20under%20substantial%20occlusions.%20Its%20integration%20as%20the%20front-end%20module%20in%20a%20closed-loop%20DLO%20manipulation%20system%20further%20demonstrates%20its%20ability%20to%20support%20stable%20feedback%20control%20in%20complex%2C%20constrained%203-D%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.17764v1&entry.124074799=Read"},
{"title": "AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments", "author": "Georgios Simantiris and Konstantinos Bacharidis and Apostolos Papanikolaou and Petros Giannakakis and Costas Panagiotakis", "abstract": "Accurate flood detection from visual data is a critical step toward improving disaster response and risk assessment, yet datasets for flood segmentation remain scarce due to the challenges of collecting and annotating large-scale imagery. Existing resources are often limited in geographic scope and annotation detail, hindering the development of robust, generalized computer vision methods. To bridge this gap, we introduce AIFloodSense, a comprehensive, publicly available aerial imagery dataset comprising 470 high-resolution images from 230 distinct flood events across 64 countries and six continents. Unlike prior benchmarks, AIFloodSense ensures global diversity and temporal relevance (2022-2024), supporting three complementary tasks: (i) Image Classification with novel sub-tasks for environment type, camera angle, and continent recognition; (ii) Semantic Segmentation providing precise pixel-level masks for flood, sky, and buildings; and (iii) Visual Question Answering (VQA) to enable natural language reasoning for disaster assessment. We establish baseline benchmarks for all tasks using state-of-the-art architectures, demonstrating the dataset's complexity and its value in advancing domain-generalized AI tools for climate resilience.", "link": "http://arxiv.org/abs/2512.17432v1", "date": "2025-12-19", "relevancy": 2.4155, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIFloodSense%3A%20A%20Global%20Aerial%20Imagery%20Dataset%20for%20Semantic%20Segmentation%20and%20Understanding%20of%20Flooded%20Environments&body=Title%3A%20AIFloodSense%3A%20A%20Global%20Aerial%20Imagery%20Dataset%20for%20Semantic%20Segmentation%20and%20Understanding%20of%20Flooded%20Environments%0AAuthor%3A%20Georgios%20Simantiris%20and%20Konstantinos%20Bacharidis%20and%20Apostolos%20Papanikolaou%20and%20Petros%20Giannakakis%20and%20Costas%20Panagiotakis%0AAbstract%3A%20Accurate%20flood%20detection%20from%20visual%20data%20is%20a%20critical%20step%20toward%20improving%20disaster%20response%20and%20risk%20assessment%2C%20yet%20datasets%20for%20flood%20segmentation%20remain%20scarce%20due%20to%20the%20challenges%20of%20collecting%20and%20annotating%20large-scale%20imagery.%20Existing%20resources%20are%20often%20limited%20in%20geographic%20scope%20and%20annotation%20detail%2C%20hindering%20the%20development%20of%20robust%2C%20generalized%20computer%20vision%20methods.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AIFloodSense%2C%20a%20comprehensive%2C%20publicly%20available%20aerial%20imagery%20dataset%20comprising%20470%20high-resolution%20images%20from%20230%20distinct%20flood%20events%20across%2064%20countries%20and%20six%20continents.%20Unlike%20prior%20benchmarks%2C%20AIFloodSense%20ensures%20global%20diversity%20and%20temporal%20relevance%20%282022-2024%29%2C%20supporting%20three%20complementary%20tasks%3A%20%28i%29%20Image%20Classification%20with%20novel%20sub-tasks%20for%20environment%20type%2C%20camera%20angle%2C%20and%20continent%20recognition%3B%20%28ii%29%20Semantic%20Segmentation%20providing%20precise%20pixel-level%20masks%20for%20flood%2C%20sky%2C%20and%20buildings%3B%20and%20%28iii%29%20Visual%20Question%20Answering%20%28VQA%29%20to%20enable%20natural%20language%20reasoning%20for%20disaster%20assessment.%20We%20establish%20baseline%20benchmarks%20for%20all%20tasks%20using%20state-of-the-art%20architectures%2C%20demonstrating%20the%20dataset%27s%20complexity%20and%20its%20value%20in%20advancing%20domain-generalized%20AI%20tools%20for%20climate%20resilience.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIFloodSense%253A%2520A%2520Global%2520Aerial%2520Imagery%2520Dataset%2520for%2520Semantic%2520Segmentation%2520and%2520Understanding%2520of%2520Flooded%2520Environments%26entry.906535625%3DGeorgios%2520Simantiris%2520and%2520Konstantinos%2520Bacharidis%2520and%2520Apostolos%2520Papanikolaou%2520and%2520Petros%2520Giannakakis%2520and%2520Costas%2520Panagiotakis%26entry.1292438233%3DAccurate%2520flood%2520detection%2520from%2520visual%2520data%2520is%2520a%2520critical%2520step%2520toward%2520improving%2520disaster%2520response%2520and%2520risk%2520assessment%252C%2520yet%2520datasets%2520for%2520flood%2520segmentation%2520remain%2520scarce%2520due%2520to%2520the%2520challenges%2520of%2520collecting%2520and%2520annotating%2520large-scale%2520imagery.%2520Existing%2520resources%2520are%2520often%2520limited%2520in%2520geographic%2520scope%2520and%2520annotation%2520detail%252C%2520hindering%2520the%2520development%2520of%2520robust%252C%2520generalized%2520computer%2520vision%2520methods.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520AIFloodSense%252C%2520a%2520comprehensive%252C%2520publicly%2520available%2520aerial%2520imagery%2520dataset%2520comprising%2520470%2520high-resolution%2520images%2520from%2520230%2520distinct%2520flood%2520events%2520across%252064%2520countries%2520and%2520six%2520continents.%2520Unlike%2520prior%2520benchmarks%252C%2520AIFloodSense%2520ensures%2520global%2520diversity%2520and%2520temporal%2520relevance%2520%25282022-2024%2529%252C%2520supporting%2520three%2520complementary%2520tasks%253A%2520%2528i%2529%2520Image%2520Classification%2520with%2520novel%2520sub-tasks%2520for%2520environment%2520type%252C%2520camera%2520angle%252C%2520and%2520continent%2520recognition%253B%2520%2528ii%2529%2520Semantic%2520Segmentation%2520providing%2520precise%2520pixel-level%2520masks%2520for%2520flood%252C%2520sky%252C%2520and%2520buildings%253B%2520and%2520%2528iii%2529%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520to%2520enable%2520natural%2520language%2520reasoning%2520for%2520disaster%2520assessment.%2520We%2520establish%2520baseline%2520benchmarks%2520for%2520all%2520tasks%2520using%2520state-of-the-art%2520architectures%252C%2520demonstrating%2520the%2520dataset%2527s%2520complexity%2520and%2520its%2520value%2520in%2520advancing%2520domain-generalized%2520AI%2520tools%2520for%2520climate%2520resilience.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIFloodSense%3A%20A%20Global%20Aerial%20Imagery%20Dataset%20for%20Semantic%20Segmentation%20and%20Understanding%20of%20Flooded%20Environments&entry.906535625=Georgios%20Simantiris%20and%20Konstantinos%20Bacharidis%20and%20Apostolos%20Papanikolaou%20and%20Petros%20Giannakakis%20and%20Costas%20Panagiotakis&entry.1292438233=Accurate%20flood%20detection%20from%20visual%20data%20is%20a%20critical%20step%20toward%20improving%20disaster%20response%20and%20risk%20assessment%2C%20yet%20datasets%20for%20flood%20segmentation%20remain%20scarce%20due%20to%20the%20challenges%20of%20collecting%20and%20annotating%20large-scale%20imagery.%20Existing%20resources%20are%20often%20limited%20in%20geographic%20scope%20and%20annotation%20detail%2C%20hindering%20the%20development%20of%20robust%2C%20generalized%20computer%20vision%20methods.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AIFloodSense%2C%20a%20comprehensive%2C%20publicly%20available%20aerial%20imagery%20dataset%20comprising%20470%20high-resolution%20images%20from%20230%20distinct%20flood%20events%20across%2064%20countries%20and%20six%20continents.%20Unlike%20prior%20benchmarks%2C%20AIFloodSense%20ensures%20global%20diversity%20and%20temporal%20relevance%20%282022-2024%29%2C%20supporting%20three%20complementary%20tasks%3A%20%28i%29%20Image%20Classification%20with%20novel%20sub-tasks%20for%20environment%20type%2C%20camera%20angle%2C%20and%20continent%20recognition%3B%20%28ii%29%20Semantic%20Segmentation%20providing%20precise%20pixel-level%20masks%20for%20flood%2C%20sky%2C%20and%20buildings%3B%20and%20%28iii%29%20Visual%20Question%20Answering%20%28VQA%29%20to%20enable%20natural%20language%20reasoning%20for%20disaster%20assessment.%20We%20establish%20baseline%20benchmarks%20for%20all%20tasks%20using%20state-of-the-art%20architectures%2C%20demonstrating%20the%20dataset%27s%20complexity%20and%20its%20value%20in%20advancing%20domain-generalized%20AI%20tools%20for%20climate%20resilience.&entry.1838667208=http%3A//arxiv.org/abs/2512.17432v1&entry.124074799=Read"},
{"title": "Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras", "author": "Ami Pandat and Punna Rajasekhar and G. Aravamuthan and Gopika Vinod and Rohit Shukla", "abstract": "Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances. Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range. This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens. This paper presents a framework for modeling a suitable distortion model that can be used for localizing the objects at longer distances. It is well known that neural networks can be a better alternative to model a highly complex non-linear lens distortion function; on contrary, it is observed that a direct application of neural networks to distortion models fails to converge to estimate the camera parameters. To resolve this, a hybrid approach is presented in this paper where the conventional distortion models are initially extended to incorporate higher-order terms and then enhanced using neural network based residual correction model. This hybrid approach has substantially improved long-range localization performance and is capable of estimating the 3D position of objects at distances up to 5 kilometres. The estimated 3D coordinates are transformed to GIS coordinates and are plotted on a GIS map for visualization. Experimental validation demonstrates the robustness and effectiveness of proposed framework, offering a practical solution to calibrate CCTV cameras for long-range photogrammetry applications.", "link": "http://arxiv.org/abs/2512.17784v1", "date": "2025-12-19", "relevancy": 2.4152, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6419}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5813}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Range%20depth%20estimation%20using%20learning%20based%20Hybrid%20Distortion%20Model%20for%20CCTV%20cameras&body=Title%3A%20Long-Range%20depth%20estimation%20using%20learning%20based%20Hybrid%20Distortion%20Model%20for%20CCTV%20cameras%0AAuthor%3A%20Ami%20Pandat%20and%20Punna%20Rajasekhar%20and%20G.%20Aravamuthan%20and%20Gopika%20Vinod%20and%20Rohit%20Shukla%0AAbstract%3A%20Accurate%20camera%20models%20are%20essential%20for%20photogrammetry%20applications%20such%20as%203D%20mapping%20and%20object%20localization%2C%20particularly%20for%20long%20distances.%20Various%20stereo-camera%20based%203D%20localization%20methods%20are%20available%20but%20are%20limited%20to%20few%20hundreds%20of%20meters%27%20range.%20This%20is%20majorly%20due%20to%20the%20limitation%20of%20the%20distortion%20models%20assumed%20for%20the%20non-linearities%20present%20in%20the%20camera%20lens.%20This%20paper%20presents%20a%20framework%20for%20modeling%20a%20suitable%20distortion%20model%20that%20can%20be%20used%20for%20localizing%20the%20objects%20at%20longer%20distances.%20It%20is%20well%20known%20that%20neural%20networks%20can%20be%20a%20better%20alternative%20to%20model%20a%20highly%20complex%20non-linear%20lens%20distortion%20function%3B%20on%20contrary%2C%20it%20is%20observed%20that%20a%20direct%20application%20of%20neural%20networks%20to%20distortion%20models%20fails%20to%20converge%20to%20estimate%20the%20camera%20parameters.%20To%20resolve%20this%2C%20a%20hybrid%20approach%20is%20presented%20in%20this%20paper%20where%20the%20conventional%20distortion%20models%20are%20initially%20extended%20to%20incorporate%20higher-order%20terms%20and%20then%20enhanced%20using%20neural%20network%20based%20residual%20correction%20model.%20This%20hybrid%20approach%20has%20substantially%20improved%20long-range%20localization%20performance%20and%20is%20capable%20of%20estimating%20the%203D%20position%20of%20objects%20at%20distances%20up%20to%205%20kilometres.%20The%20estimated%203D%20coordinates%20are%20transformed%20to%20GIS%20coordinates%20and%20are%20plotted%20on%20a%20GIS%20map%20for%20visualization.%20Experimental%20validation%20demonstrates%20the%20robustness%20and%20effectiveness%20of%20proposed%20framework%2C%20offering%20a%20practical%20solution%20to%20calibrate%20CCTV%20cameras%20for%20long-range%20photogrammetry%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Range%2520depth%2520estimation%2520using%2520learning%2520based%2520Hybrid%2520Distortion%2520Model%2520for%2520CCTV%2520cameras%26entry.906535625%3DAmi%2520Pandat%2520and%2520Punna%2520Rajasekhar%2520and%2520G.%2520Aravamuthan%2520and%2520Gopika%2520Vinod%2520and%2520Rohit%2520Shukla%26entry.1292438233%3DAccurate%2520camera%2520models%2520are%2520essential%2520for%2520photogrammetry%2520applications%2520such%2520as%25203D%2520mapping%2520and%2520object%2520localization%252C%2520particularly%2520for%2520long%2520distances.%2520Various%2520stereo-camera%2520based%25203D%2520localization%2520methods%2520are%2520available%2520but%2520are%2520limited%2520to%2520few%2520hundreds%2520of%2520meters%2527%2520range.%2520This%2520is%2520majorly%2520due%2520to%2520the%2520limitation%2520of%2520the%2520distortion%2520models%2520assumed%2520for%2520the%2520non-linearities%2520present%2520in%2520the%2520camera%2520lens.%2520This%2520paper%2520presents%2520a%2520framework%2520for%2520modeling%2520a%2520suitable%2520distortion%2520model%2520that%2520can%2520be%2520used%2520for%2520localizing%2520the%2520objects%2520at%2520longer%2520distances.%2520It%2520is%2520well%2520known%2520that%2520neural%2520networks%2520can%2520be%2520a%2520better%2520alternative%2520to%2520model%2520a%2520highly%2520complex%2520non-linear%2520lens%2520distortion%2520function%253B%2520on%2520contrary%252C%2520it%2520is%2520observed%2520that%2520a%2520direct%2520application%2520of%2520neural%2520networks%2520to%2520distortion%2520models%2520fails%2520to%2520converge%2520to%2520estimate%2520the%2520camera%2520parameters.%2520To%2520resolve%2520this%252C%2520a%2520hybrid%2520approach%2520is%2520presented%2520in%2520this%2520paper%2520where%2520the%2520conventional%2520distortion%2520models%2520are%2520initially%2520extended%2520to%2520incorporate%2520higher-order%2520terms%2520and%2520then%2520enhanced%2520using%2520neural%2520network%2520based%2520residual%2520correction%2520model.%2520This%2520hybrid%2520approach%2520has%2520substantially%2520improved%2520long-range%2520localization%2520performance%2520and%2520is%2520capable%2520of%2520estimating%2520the%25203D%2520position%2520of%2520objects%2520at%2520distances%2520up%2520to%25205%2520kilometres.%2520The%2520estimated%25203D%2520coordinates%2520are%2520transformed%2520to%2520GIS%2520coordinates%2520and%2520are%2520plotted%2520on%2520a%2520GIS%2520map%2520for%2520visualization.%2520Experimental%2520validation%2520demonstrates%2520the%2520robustness%2520and%2520effectiveness%2520of%2520proposed%2520framework%252C%2520offering%2520a%2520practical%2520solution%2520to%2520calibrate%2520CCTV%2520cameras%2520for%2520long-range%2520photogrammetry%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Range%20depth%20estimation%20using%20learning%20based%20Hybrid%20Distortion%20Model%20for%20CCTV%20cameras&entry.906535625=Ami%20Pandat%20and%20Punna%20Rajasekhar%20and%20G.%20Aravamuthan%20and%20Gopika%20Vinod%20and%20Rohit%20Shukla&entry.1292438233=Accurate%20camera%20models%20are%20essential%20for%20photogrammetry%20applications%20such%20as%203D%20mapping%20and%20object%20localization%2C%20particularly%20for%20long%20distances.%20Various%20stereo-camera%20based%203D%20localization%20methods%20are%20available%20but%20are%20limited%20to%20few%20hundreds%20of%20meters%27%20range.%20This%20is%20majorly%20due%20to%20the%20limitation%20of%20the%20distortion%20models%20assumed%20for%20the%20non-linearities%20present%20in%20the%20camera%20lens.%20This%20paper%20presents%20a%20framework%20for%20modeling%20a%20suitable%20distortion%20model%20that%20can%20be%20used%20for%20localizing%20the%20objects%20at%20longer%20distances.%20It%20is%20well%20known%20that%20neural%20networks%20can%20be%20a%20better%20alternative%20to%20model%20a%20highly%20complex%20non-linear%20lens%20distortion%20function%3B%20on%20contrary%2C%20it%20is%20observed%20that%20a%20direct%20application%20of%20neural%20networks%20to%20distortion%20models%20fails%20to%20converge%20to%20estimate%20the%20camera%20parameters.%20To%20resolve%20this%2C%20a%20hybrid%20approach%20is%20presented%20in%20this%20paper%20where%20the%20conventional%20distortion%20models%20are%20initially%20extended%20to%20incorporate%20higher-order%20terms%20and%20then%20enhanced%20using%20neural%20network%20based%20residual%20correction%20model.%20This%20hybrid%20approach%20has%20substantially%20improved%20long-range%20localization%20performance%20and%20is%20capable%20of%20estimating%20the%203D%20position%20of%20objects%20at%20distances%20up%20to%205%20kilometres.%20The%20estimated%203D%20coordinates%20are%20transformed%20to%20GIS%20coordinates%20and%20are%20plotted%20on%20a%20GIS%20map%20for%20visualization.%20Experimental%20validation%20demonstrates%20the%20robustness%20and%20effectiveness%20of%20proposed%20framework%2C%20offering%20a%20practical%20solution%20to%20calibrate%20CCTV%20cameras%20for%20long-range%20photogrammetry%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.17784v1&entry.124074799=Read"},
{"title": "Generalized infinite dimensional Alpha-Procrustes based geometries", "author": "Salvish Goomanee and Andi Han and Pratik Jawanpuria and Bamdev Mishra", "abstract": "This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.", "link": "http://arxiv.org/abs/2511.09801v2", "date": "2025-12-19", "relevancy": 2.397, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4989}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4734}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20infinite%20dimensional%20Alpha-Procrustes%20based%20geometries&body=Title%3A%20Generalized%20infinite%20dimensional%20Alpha-Procrustes%20based%20geometries%0AAuthor%3A%20Salvish%20Goomanee%20and%20Andi%20Han%20and%20Pratik%20Jawanpuria%20and%20Bamdev%20Mishra%0AAbstract%3A%20This%20work%20extends%20the%20recently%20introduced%20Alpha-Procrustes%20family%20of%20Riemannian%20metrics%20for%20symmetric%20positive%20definite%20%28SPD%29%20matrices%20by%20incorporating%20generalized%20versions%20of%20the%20Bures-Wasserstein%20%28GBW%29%2C%20Log-Euclidean%2C%20and%20Wasserstein%20distances.%20While%20the%20Alpha-Procrustes%20framework%20has%20unified%20many%20classical%20metrics%20in%20both%20finite-%20and%20infinite-%20dimensional%20settings%2C%20it%20previously%20lacked%20the%20structural%20components%20necessary%20to%20realize%20these%20generalized%20forms.%20We%20introduce%20a%20formalism%20based%20on%20unitized%20Hilbert-Schmidt%20operators%20and%20an%20extended%20Mahalanobis%20norm%20that%20allows%20the%20construction%20of%20robust%2C%20infinite-dimensional%20generalizations%20of%20GBW%20and%20Log-Hilbert-Schmidt%20distances.%20Our%20approach%20also%20incorporates%20a%20learnable%20regularization%20parameter%20that%20enhances%20geometric%20stability%20in%20high-dimensional%20comparisons.%20Preliminary%20experiments%20reproducing%20benchmarks%20from%20the%20literature%20demonstrate%20the%20improved%20performance%20of%20our%20generalized%20metrics%2C%20particularly%20in%20scenarios%20involving%20comparisons%20between%20datasets%20of%20varying%20dimension%20and%20scale.%20This%20work%20lays%20a%20theoretical%20and%20computational%20foundation%20for%20advancing%20robust%20geometric%20methods%20in%20machine%20learning%2C%20statistical%20inference%2C%20and%20functional%20data%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520infinite%2520dimensional%2520Alpha-Procrustes%2520based%2520geometries%26entry.906535625%3DSalvish%2520Goomanee%2520and%2520Andi%2520Han%2520and%2520Pratik%2520Jawanpuria%2520and%2520Bamdev%2520Mishra%26entry.1292438233%3DThis%2520work%2520extends%2520the%2520recently%2520introduced%2520Alpha-Procrustes%2520family%2520of%2520Riemannian%2520metrics%2520for%2520symmetric%2520positive%2520definite%2520%2528SPD%2529%2520matrices%2520by%2520incorporating%2520generalized%2520versions%2520of%2520the%2520Bures-Wasserstein%2520%2528GBW%2529%252C%2520Log-Euclidean%252C%2520and%2520Wasserstein%2520distances.%2520While%2520the%2520Alpha-Procrustes%2520framework%2520has%2520unified%2520many%2520classical%2520metrics%2520in%2520both%2520finite-%2520and%2520infinite-%2520dimensional%2520settings%252C%2520it%2520previously%2520lacked%2520the%2520structural%2520components%2520necessary%2520to%2520realize%2520these%2520generalized%2520forms.%2520We%2520introduce%2520a%2520formalism%2520based%2520on%2520unitized%2520Hilbert-Schmidt%2520operators%2520and%2520an%2520extended%2520Mahalanobis%2520norm%2520that%2520allows%2520the%2520construction%2520of%2520robust%252C%2520infinite-dimensional%2520generalizations%2520of%2520GBW%2520and%2520Log-Hilbert-Schmidt%2520distances.%2520Our%2520approach%2520also%2520incorporates%2520a%2520learnable%2520regularization%2520parameter%2520that%2520enhances%2520geometric%2520stability%2520in%2520high-dimensional%2520comparisons.%2520Preliminary%2520experiments%2520reproducing%2520benchmarks%2520from%2520the%2520literature%2520demonstrate%2520the%2520improved%2520performance%2520of%2520our%2520generalized%2520metrics%252C%2520particularly%2520in%2520scenarios%2520involving%2520comparisons%2520between%2520datasets%2520of%2520varying%2520dimension%2520and%2520scale.%2520This%2520work%2520lays%2520a%2520theoretical%2520and%2520computational%2520foundation%2520for%2520advancing%2520robust%2520geometric%2520methods%2520in%2520machine%2520learning%252C%2520statistical%2520inference%252C%2520and%2520functional%2520data%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20infinite%20dimensional%20Alpha-Procrustes%20based%20geometries&entry.906535625=Salvish%20Goomanee%20and%20Andi%20Han%20and%20Pratik%20Jawanpuria%20and%20Bamdev%20Mishra&entry.1292438233=This%20work%20extends%20the%20recently%20introduced%20Alpha-Procrustes%20family%20of%20Riemannian%20metrics%20for%20symmetric%20positive%20definite%20%28SPD%29%20matrices%20by%20incorporating%20generalized%20versions%20of%20the%20Bures-Wasserstein%20%28GBW%29%2C%20Log-Euclidean%2C%20and%20Wasserstein%20distances.%20While%20the%20Alpha-Procrustes%20framework%20has%20unified%20many%20classical%20metrics%20in%20both%20finite-%20and%20infinite-%20dimensional%20settings%2C%20it%20previously%20lacked%20the%20structural%20components%20necessary%20to%20realize%20these%20generalized%20forms.%20We%20introduce%20a%20formalism%20based%20on%20unitized%20Hilbert-Schmidt%20operators%20and%20an%20extended%20Mahalanobis%20norm%20that%20allows%20the%20construction%20of%20robust%2C%20infinite-dimensional%20generalizations%20of%20GBW%20and%20Log-Hilbert-Schmidt%20distances.%20Our%20approach%20also%20incorporates%20a%20learnable%20regularization%20parameter%20that%20enhances%20geometric%20stability%20in%20high-dimensional%20comparisons.%20Preliminary%20experiments%20reproducing%20benchmarks%20from%20the%20literature%20demonstrate%20the%20improved%20performance%20of%20our%20generalized%20metrics%2C%20particularly%20in%20scenarios%20involving%20comparisons%20between%20datasets%20of%20varying%20dimension%20and%20scale.%20This%20work%20lays%20a%20theoretical%20and%20computational%20foundation%20for%20advancing%20robust%20geometric%20methods%20in%20machine%20learning%2C%20statistical%20inference%2C%20and%20functional%20data%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2511.09801v2&entry.124074799=Read"},
{"title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling", "author": "Vongani H. Maluleke and Kie Horiuchi and Lea Wilken and Evonne Ng and Jitendra Malik and Angjoo Kanazawa", "abstract": "Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/", "link": "http://arxiv.org/abs/2512.17900v1", "date": "2025-12-19", "relevancy": 2.3929, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6397}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5711}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Forcing%20for%20Multi-Agent%20Interaction%20Sequence%20Modeling&body=Title%3A%20Diffusion%20Forcing%20for%20Multi-Agent%20Interaction%20Sequence%20Modeling%0AAuthor%3A%20Vongani%20H.%20Maluleke%20and%20Kie%20Horiuchi%20and%20Lea%20Wilken%20and%20Evonne%20Ng%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20Understanding%20and%20generating%20multi-person%20interactions%20is%20a%20fundamental%20challenge%20with%20broad%20implications%20for%20robotics%20and%20social%20computing.%20While%20humans%20naturally%20coordinate%20in%20groups%2C%20modeling%20such%20interactions%20remains%20difficult%20due%20to%20long%20temporal%20horizons%2C%20strong%20inter-agent%20dependencies%2C%20and%20variable%20group%20sizes.%20Existing%20motion%20generation%20methods%20are%20largely%20task-specific%20and%20do%20not%20generalize%20to%20flexible%20multi-agent%20generation.%20We%20introduce%20MAGNet%20%28Multi-Agent%20Diffusion%20Forcing%20Transformer%29%2C%20a%20unified%20autoregressive%20diffusion%20framework%20for%20multi-agent%20motion%20generation%20that%20supports%20a%20wide%20range%20of%20interaction%20tasks%20through%20flexible%20conditioning%20and%20sampling.%20MAGNet%20performs%20dyadic%20prediction%2C%20partner%20inpainting%2C%20and%20full%20multi-agent%20motion%20generation%20within%20a%20single%20model%2C%20and%20can%20autoregressively%20generate%20ultra-long%20sequences%20spanning%20hundreds%20of%20v.%20Building%20on%20Diffusion%20Forcing%2C%20we%20introduce%20key%20modifications%20that%20explicitly%20model%20inter-agent%20coupling%20during%20autoregressive%20denoising%2C%20enabling%20coherent%20coordination%20across%20agents.%20As%20a%20result%2C%20MAGNet%20captures%20both%20tightly%20synchronized%20activities%20%28e.g%2C%20dancing%2C%20boxing%29%20and%20loosely%20structured%20social%20interactions.%20Our%20approach%20performs%20on%20par%20with%20specialized%20methods%20on%20dyadic%20benchmarks%20while%20naturally%20extending%20to%20polyadic%20scenarios%20involving%20three%20or%20more%20interacting%20people%2C%20enabled%20by%20a%20scalable%20architecture%20that%20is%20agnostic%20to%20the%20number%20of%20agents.%20We%20refer%20readers%20to%20the%20supplemental%20video%2C%20where%20the%20temporal%20dynamics%20and%20spatial%20coordination%20of%20generated%20interactions%20are%20best%20appreciated.%20Project%20page%3A%20https%3A//von31.github.io/MAGNet/%0ALink%3A%20http%3A//arxiv.org/abs/2512.17900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Forcing%2520for%2520Multi-Agent%2520Interaction%2520Sequence%2520Modeling%26entry.906535625%3DVongani%2520H.%2520Maluleke%2520and%2520Kie%2520Horiuchi%2520and%2520Lea%2520Wilken%2520and%2520Evonne%2520Ng%2520and%2520Jitendra%2520Malik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3DUnderstanding%2520and%2520generating%2520multi-person%2520interactions%2520is%2520a%2520fundamental%2520challenge%2520with%2520broad%2520implications%2520for%2520robotics%2520and%2520social%2520computing.%2520While%2520humans%2520naturally%2520coordinate%2520in%2520groups%252C%2520modeling%2520such%2520interactions%2520remains%2520difficult%2520due%2520to%2520long%2520temporal%2520horizons%252C%2520strong%2520inter-agent%2520dependencies%252C%2520and%2520variable%2520group%2520sizes.%2520Existing%2520motion%2520generation%2520methods%2520are%2520largely%2520task-specific%2520and%2520do%2520not%2520generalize%2520to%2520flexible%2520multi-agent%2520generation.%2520We%2520introduce%2520MAGNet%2520%2528Multi-Agent%2520Diffusion%2520Forcing%2520Transformer%2529%252C%2520a%2520unified%2520autoregressive%2520diffusion%2520framework%2520for%2520multi-agent%2520motion%2520generation%2520that%2520supports%2520a%2520wide%2520range%2520of%2520interaction%2520tasks%2520through%2520flexible%2520conditioning%2520and%2520sampling.%2520MAGNet%2520performs%2520dyadic%2520prediction%252C%2520partner%2520inpainting%252C%2520and%2520full%2520multi-agent%2520motion%2520generation%2520within%2520a%2520single%2520model%252C%2520and%2520can%2520autoregressively%2520generate%2520ultra-long%2520sequences%2520spanning%2520hundreds%2520of%2520v.%2520Building%2520on%2520Diffusion%2520Forcing%252C%2520we%2520introduce%2520key%2520modifications%2520that%2520explicitly%2520model%2520inter-agent%2520coupling%2520during%2520autoregressive%2520denoising%252C%2520enabling%2520coherent%2520coordination%2520across%2520agents.%2520As%2520a%2520result%252C%2520MAGNet%2520captures%2520both%2520tightly%2520synchronized%2520activities%2520%2528e.g%252C%2520dancing%252C%2520boxing%2529%2520and%2520loosely%2520structured%2520social%2520interactions.%2520Our%2520approach%2520performs%2520on%2520par%2520with%2520specialized%2520methods%2520on%2520dyadic%2520benchmarks%2520while%2520naturally%2520extending%2520to%2520polyadic%2520scenarios%2520involving%2520three%2520or%2520more%2520interacting%2520people%252C%2520enabled%2520by%2520a%2520scalable%2520architecture%2520that%2520is%2520agnostic%2520to%2520the%2520number%2520of%2520agents.%2520We%2520refer%2520readers%2520to%2520the%2520supplemental%2520video%252C%2520where%2520the%2520temporal%2520dynamics%2520and%2520spatial%2520coordination%2520of%2520generated%2520interactions%2520are%2520best%2520appreciated.%2520Project%2520page%253A%2520https%253A//von31.github.io/MAGNet/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Forcing%20for%20Multi-Agent%20Interaction%20Sequence%20Modeling&entry.906535625=Vongani%20H.%20Maluleke%20and%20Kie%20Horiuchi%20and%20Lea%20Wilken%20and%20Evonne%20Ng%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa&entry.1292438233=Understanding%20and%20generating%20multi-person%20interactions%20is%20a%20fundamental%20challenge%20with%20broad%20implications%20for%20robotics%20and%20social%20computing.%20While%20humans%20naturally%20coordinate%20in%20groups%2C%20modeling%20such%20interactions%20remains%20difficult%20due%20to%20long%20temporal%20horizons%2C%20strong%20inter-agent%20dependencies%2C%20and%20variable%20group%20sizes.%20Existing%20motion%20generation%20methods%20are%20largely%20task-specific%20and%20do%20not%20generalize%20to%20flexible%20multi-agent%20generation.%20We%20introduce%20MAGNet%20%28Multi-Agent%20Diffusion%20Forcing%20Transformer%29%2C%20a%20unified%20autoregressive%20diffusion%20framework%20for%20multi-agent%20motion%20generation%20that%20supports%20a%20wide%20range%20of%20interaction%20tasks%20through%20flexible%20conditioning%20and%20sampling.%20MAGNet%20performs%20dyadic%20prediction%2C%20partner%20inpainting%2C%20and%20full%20multi-agent%20motion%20generation%20within%20a%20single%20model%2C%20and%20can%20autoregressively%20generate%20ultra-long%20sequences%20spanning%20hundreds%20of%20v.%20Building%20on%20Diffusion%20Forcing%2C%20we%20introduce%20key%20modifications%20that%20explicitly%20model%20inter-agent%20coupling%20during%20autoregressive%20denoising%2C%20enabling%20coherent%20coordination%20across%20agents.%20As%20a%20result%2C%20MAGNet%20captures%20both%20tightly%20synchronized%20activities%20%28e.g%2C%20dancing%2C%20boxing%29%20and%20loosely%20structured%20social%20interactions.%20Our%20approach%20performs%20on%20par%20with%20specialized%20methods%20on%20dyadic%20benchmarks%20while%20naturally%20extending%20to%20polyadic%20scenarios%20involving%20three%20or%20more%20interacting%20people%2C%20enabled%20by%20a%20scalable%20architecture%20that%20is%20agnostic%20to%20the%20number%20of%20agents.%20We%20refer%20readers%20to%20the%20supplemental%20video%2C%20where%20the%20temporal%20dynamics%20and%20spatial%20coordination%20of%20generated%20interactions%20are%20best%20appreciated.%20Project%20page%3A%20https%3A//von31.github.io/MAGNet/&entry.1838667208=http%3A//arxiv.org/abs/2512.17900v1&entry.124074799=Read"},
{"title": "StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection", "author": "Di Wu and Feng Yang and Wenhui Zhao and Jinwen Yu and Pan Liao and Benlian Xu and Dingwen Zhang", "abstract": "Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.", "link": "http://arxiv.org/abs/2512.17620v1", "date": "2025-12-19", "relevancy": 2.3696, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5976}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5897}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StereoMV2D%3A%20A%20Sparse%20Temporal%20Stereo-Enhanced%20Framework%20for%20Robust%20Multi-View%203D%20Object%20Detection&body=Title%3A%20StereoMV2D%3A%20A%20Sparse%20Temporal%20Stereo-Enhanced%20Framework%20for%20Robust%20Multi-View%203D%20Object%20Detection%0AAuthor%3A%20Di%20Wu%20and%20Feng%20Yang%20and%20Wenhui%20Zhao%20and%20Jinwen%20Yu%20and%20Pan%20Liao%20and%20Benlian%20Xu%20and%20Dingwen%20Zhang%0AAbstract%3A%20Multi-view%203D%20object%20detection%20is%20a%20fundamental%20task%20in%20autonomous%20driving%20perception%2C%20where%20achieving%20a%20balance%20between%20detection%20accuracy%20and%20computational%20efficiency%20remains%20crucial.%20Sparse%20query-based%203D%20detectors%20efficiently%20aggregate%20object-relevant%20features%20from%20multi-view%20images%20through%20a%20set%20of%20learnable%20queries%2C%20offering%20a%20concise%20and%20end-to-end%20detection%20paradigm.%20Building%20on%20this%20foundation%2C%20MV2D%20leverages%202D%20detection%20results%20to%20provide%20high-quality%20object%20priors%20for%20query%20initialization%2C%20enabling%20higher%20precision%20and%20recall.%20However%2C%20the%20inherent%20depth%20ambiguity%20in%20single-frame%202D%20detections%20still%20limits%20the%20accuracy%20of%203D%20query%20generation.%20To%20address%20this%20issue%2C%20we%20propose%20StereoMV2D%2C%20a%20unified%20framework%20that%20integrates%20temporal%20stereo%20modeling%20into%20the%202D%20detection-guided%20multi-view%203D%20detector.%20By%20exploiting%20cross-temporal%20disparities%20of%20the%20same%20object%20across%20adjacent%20frames%2C%20StereoMV2D%20enhances%20depth%20perception%20and%20refines%20the%20query%20priors%2C%20while%20performing%20all%20computations%20efficiently%20within%202D%20regions%20of%20interest%20%28RoIs%29.%20Furthermore%2C%20a%20dynamic%20confidence%20gating%20mechanism%20adaptively%20evaluates%20the%20reliability%20of%20temporal%20stereo%20cues%20through%20learning%20statistical%20patterns%20derived%20from%20the%20inter-frame%20matching%20matrix%20together%20with%20appearance%20consistency%2C%20ensuring%20robust%20detection%20under%20object%20appearance%20and%20occlusion.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Argoverse%202%20datasets%20demonstrate%20that%20StereoMV2D%20achieves%20superior%20detection%20performance%20without%20incurring%20significant%20computational%20overhead.%20Code%20will%20be%20available%20at%20https%3A//github.com/Uddd821/StereoMV2D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereoMV2D%253A%2520A%2520Sparse%2520Temporal%2520Stereo-Enhanced%2520Framework%2520for%2520Robust%2520Multi-View%25203D%2520Object%2520Detection%26entry.906535625%3DDi%2520Wu%2520and%2520Feng%2520Yang%2520and%2520Wenhui%2520Zhao%2520and%2520Jinwen%2520Yu%2520and%2520Pan%2520Liao%2520and%2520Benlian%2520Xu%2520and%2520Dingwen%2520Zhang%26entry.1292438233%3DMulti-view%25203D%2520object%2520detection%2520is%2520a%2520fundamental%2520task%2520in%2520autonomous%2520driving%2520perception%252C%2520where%2520achieving%2520a%2520balance%2520between%2520detection%2520accuracy%2520and%2520computational%2520efficiency%2520remains%2520crucial.%2520Sparse%2520query-based%25203D%2520detectors%2520efficiently%2520aggregate%2520object-relevant%2520features%2520from%2520multi-view%2520images%2520through%2520a%2520set%2520of%2520learnable%2520queries%252C%2520offering%2520a%2520concise%2520and%2520end-to-end%2520detection%2520paradigm.%2520Building%2520on%2520this%2520foundation%252C%2520MV2D%2520leverages%25202D%2520detection%2520results%2520to%2520provide%2520high-quality%2520object%2520priors%2520for%2520query%2520initialization%252C%2520enabling%2520higher%2520precision%2520and%2520recall.%2520However%252C%2520the%2520inherent%2520depth%2520ambiguity%2520in%2520single-frame%25202D%2520detections%2520still%2520limits%2520the%2520accuracy%2520of%25203D%2520query%2520generation.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520StereoMV2D%252C%2520a%2520unified%2520framework%2520that%2520integrates%2520temporal%2520stereo%2520modeling%2520into%2520the%25202D%2520detection-guided%2520multi-view%25203D%2520detector.%2520By%2520exploiting%2520cross-temporal%2520disparities%2520of%2520the%2520same%2520object%2520across%2520adjacent%2520frames%252C%2520StereoMV2D%2520enhances%2520depth%2520perception%2520and%2520refines%2520the%2520query%2520priors%252C%2520while%2520performing%2520all%2520computations%2520efficiently%2520within%25202D%2520regions%2520of%2520interest%2520%2528RoIs%2529.%2520Furthermore%252C%2520a%2520dynamic%2520confidence%2520gating%2520mechanism%2520adaptively%2520evaluates%2520the%2520reliability%2520of%2520temporal%2520stereo%2520cues%2520through%2520learning%2520statistical%2520patterns%2520derived%2520from%2520the%2520inter-frame%2520matching%2520matrix%2520together%2520with%2520appearance%2520consistency%252C%2520ensuring%2520robust%2520detection%2520under%2520object%2520appearance%2520and%2520occlusion.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520and%2520Argoverse%25202%2520datasets%2520demonstrate%2520that%2520StereoMV2D%2520achieves%2520superior%2520detection%2520performance%2520without%2520incurring%2520significant%2520computational%2520overhead.%2520Code%2520will%2520be%2520available%2520at%2520https%253A//github.com/Uddd821/StereoMV2D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StereoMV2D%3A%20A%20Sparse%20Temporal%20Stereo-Enhanced%20Framework%20for%20Robust%20Multi-View%203D%20Object%20Detection&entry.906535625=Di%20Wu%20and%20Feng%20Yang%20and%20Wenhui%20Zhao%20and%20Jinwen%20Yu%20and%20Pan%20Liao%20and%20Benlian%20Xu%20and%20Dingwen%20Zhang&entry.1292438233=Multi-view%203D%20object%20detection%20is%20a%20fundamental%20task%20in%20autonomous%20driving%20perception%2C%20where%20achieving%20a%20balance%20between%20detection%20accuracy%20and%20computational%20efficiency%20remains%20crucial.%20Sparse%20query-based%203D%20detectors%20efficiently%20aggregate%20object-relevant%20features%20from%20multi-view%20images%20through%20a%20set%20of%20learnable%20queries%2C%20offering%20a%20concise%20and%20end-to-end%20detection%20paradigm.%20Building%20on%20this%20foundation%2C%20MV2D%20leverages%202D%20detection%20results%20to%20provide%20high-quality%20object%20priors%20for%20query%20initialization%2C%20enabling%20higher%20precision%20and%20recall.%20However%2C%20the%20inherent%20depth%20ambiguity%20in%20single-frame%202D%20detections%20still%20limits%20the%20accuracy%20of%203D%20query%20generation.%20To%20address%20this%20issue%2C%20we%20propose%20StereoMV2D%2C%20a%20unified%20framework%20that%20integrates%20temporal%20stereo%20modeling%20into%20the%202D%20detection-guided%20multi-view%203D%20detector.%20By%20exploiting%20cross-temporal%20disparities%20of%20the%20same%20object%20across%20adjacent%20frames%2C%20StereoMV2D%20enhances%20depth%20perception%20and%20refines%20the%20query%20priors%2C%20while%20performing%20all%20computations%20efficiently%20within%202D%20regions%20of%20interest%20%28RoIs%29.%20Furthermore%2C%20a%20dynamic%20confidence%20gating%20mechanism%20adaptively%20evaluates%20the%20reliability%20of%20temporal%20stereo%20cues%20through%20learning%20statistical%20patterns%20derived%20from%20the%20inter-frame%20matching%20matrix%20together%20with%20appearance%20consistency%2C%20ensuring%20robust%20detection%20under%20object%20appearance%20and%20occlusion.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Argoverse%202%20datasets%20demonstrate%20that%20StereoMV2D%20achieves%20superior%20detection%20performance%20without%20incurring%20significant%20computational%20overhead.%20Code%20will%20be%20available%20at%20https%3A//github.com/Uddd821/StereoMV2D.&entry.1838667208=http%3A//arxiv.org/abs/2512.17620v1&entry.124074799=Read"},
{"title": "AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection", "author": "Yichen Jiang and Mohammed Talha Alam and Sohail Ahmed Khan and Duc-Tien Dang-Nguyen and Fakhri Karray", "abstract": "Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.", "link": "http://arxiv.org/abs/2512.17730v1", "date": "2025-12-19", "relevancy": 2.3591, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6086}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5781}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaptPrompt%3A%20Parameter-Efficient%20Adaptation%20of%20VLMs%20for%20Generalizable%20Deepfake%20Detection&body=Title%3A%20AdaptPrompt%3A%20Parameter-Efficient%20Adaptation%20of%20VLMs%20for%20Generalizable%20Deepfake%20Detection%0AAuthor%3A%20Yichen%20Jiang%20and%20Mohammed%20Talha%20Alam%20and%20Sohail%20Ahmed%20Khan%20and%20Duc-Tien%20Dang-Nguyen%20and%20Fakhri%20Karray%0AAbstract%3A%20Recent%20advances%20in%20image%20generation%20have%20led%20to%20the%20widespread%20availability%20of%20highly%20realistic%20synthetic%20media%2C%20increasing%20the%20difficulty%20of%20reliable%20deepfake%20detection.%20A%20key%20challenge%20is%20generalization%2C%20as%20detectors%20trained%20on%20a%20narrow%20class%20of%20generators%20often%20fail%20when%20confronted%20with%20unseen%20models.%20In%20this%20work%2C%20we%20address%20the%20pressing%20need%20for%20generalizable%20detection%20by%20leveraging%20large%20vision-language%20models%2C%20specifically%20CLIP%2C%20to%20identify%20synthetic%20content%20across%20diverse%20generative%20techniques.%20First%2C%20we%20introduce%20Diff-Gen%2C%20a%20large-scale%20benchmark%20dataset%20comprising%20100k%20diffusion-generated%20fakes%20that%20capture%20broad%20spectral%20artifacts%20unlike%20traditional%20GAN%20datasets.%20Models%20trained%20on%20Diff-Gen%20demonstrate%20stronger%20cross-domain%20generalization%2C%20particularly%20on%20previously%20unseen%20image%20generators.%20Second%2C%20we%20propose%20AdaptPrompt%2C%20a%20parameter-efficient%20transfer%20learning%20framework%20that%20jointly%20learns%20task-specific%20textual%20prompts%20and%20visual%20adapters%20while%20keeping%20the%20CLIP%20backbone%20frozen.%20We%20further%20show%20via%20layer%20ablation%20that%20pruning%20the%20final%20transformer%20block%20of%20the%20vision%20encoder%20enhances%20the%20retention%20of%20high-frequency%20generative%20artifacts%2C%20significantly%20boosting%20detection%20accuracy.%20Our%20evaluation%20spans%2025%20challenging%20test%20sets%2C%20covering%20synthetic%20content%20generated%20by%20GANs%2C%20diffusion%20models%2C%20and%20commercial%20tools%2C%20establishing%20a%20new%20state-of-the-art%20in%20both%20standard%20and%20cross-domain%20scenarios.%20We%20further%20demonstrate%20the%20framework%27s%20versatility%20through%20few-shot%20generalization%20%28using%20as%20few%20as%20320%20images%29%20and%20source%20attribution%2C%20enabling%20the%20precise%20identification%20of%20generator%20architectures%20in%20closed-set%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptPrompt%253A%2520Parameter-Efficient%2520Adaptation%2520of%2520VLMs%2520for%2520Generalizable%2520Deepfake%2520Detection%26entry.906535625%3DYichen%2520Jiang%2520and%2520Mohammed%2520Talha%2520Alam%2520and%2520Sohail%2520Ahmed%2520Khan%2520and%2520Duc-Tien%2520Dang-Nguyen%2520and%2520Fakhri%2520Karray%26entry.1292438233%3DRecent%2520advances%2520in%2520image%2520generation%2520have%2520led%2520to%2520the%2520widespread%2520availability%2520of%2520highly%2520realistic%2520synthetic%2520media%252C%2520increasing%2520the%2520difficulty%2520of%2520reliable%2520deepfake%2520detection.%2520A%2520key%2520challenge%2520is%2520generalization%252C%2520as%2520detectors%2520trained%2520on%2520a%2520narrow%2520class%2520of%2520generators%2520often%2520fail%2520when%2520confronted%2520with%2520unseen%2520models.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520pressing%2520need%2520for%2520generalizable%2520detection%2520by%2520leveraging%2520large%2520vision-language%2520models%252C%2520specifically%2520CLIP%252C%2520to%2520identify%2520synthetic%2520content%2520across%2520diverse%2520generative%2520techniques.%2520First%252C%2520we%2520introduce%2520Diff-Gen%252C%2520a%2520large-scale%2520benchmark%2520dataset%2520comprising%2520100k%2520diffusion-generated%2520fakes%2520that%2520capture%2520broad%2520spectral%2520artifacts%2520unlike%2520traditional%2520GAN%2520datasets.%2520Models%2520trained%2520on%2520Diff-Gen%2520demonstrate%2520stronger%2520cross-domain%2520generalization%252C%2520particularly%2520on%2520previously%2520unseen%2520image%2520generators.%2520Second%252C%2520we%2520propose%2520AdaptPrompt%252C%2520a%2520parameter-efficient%2520transfer%2520learning%2520framework%2520that%2520jointly%2520learns%2520task-specific%2520textual%2520prompts%2520and%2520visual%2520adapters%2520while%2520keeping%2520the%2520CLIP%2520backbone%2520frozen.%2520We%2520further%2520show%2520via%2520layer%2520ablation%2520that%2520pruning%2520the%2520final%2520transformer%2520block%2520of%2520the%2520vision%2520encoder%2520enhances%2520the%2520retention%2520of%2520high-frequency%2520generative%2520artifacts%252C%2520significantly%2520boosting%2520detection%2520accuracy.%2520Our%2520evaluation%2520spans%252025%2520challenging%2520test%2520sets%252C%2520covering%2520synthetic%2520content%2520generated%2520by%2520GANs%252C%2520diffusion%2520models%252C%2520and%2520commercial%2520tools%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%2520both%2520standard%2520and%2520cross-domain%2520scenarios.%2520We%2520further%2520demonstrate%2520the%2520framework%2527s%2520versatility%2520through%2520few-shot%2520generalization%2520%2528using%2520as%2520few%2520as%2520320%2520images%2529%2520and%2520source%2520attribution%252C%2520enabling%2520the%2520precise%2520identification%2520of%2520generator%2520architectures%2520in%2520closed-set%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaptPrompt%3A%20Parameter-Efficient%20Adaptation%20of%20VLMs%20for%20Generalizable%20Deepfake%20Detection&entry.906535625=Yichen%20Jiang%20and%20Mohammed%20Talha%20Alam%20and%20Sohail%20Ahmed%20Khan%20and%20Duc-Tien%20Dang-Nguyen%20and%20Fakhri%20Karray&entry.1292438233=Recent%20advances%20in%20image%20generation%20have%20led%20to%20the%20widespread%20availability%20of%20highly%20realistic%20synthetic%20media%2C%20increasing%20the%20difficulty%20of%20reliable%20deepfake%20detection.%20A%20key%20challenge%20is%20generalization%2C%20as%20detectors%20trained%20on%20a%20narrow%20class%20of%20generators%20often%20fail%20when%20confronted%20with%20unseen%20models.%20In%20this%20work%2C%20we%20address%20the%20pressing%20need%20for%20generalizable%20detection%20by%20leveraging%20large%20vision-language%20models%2C%20specifically%20CLIP%2C%20to%20identify%20synthetic%20content%20across%20diverse%20generative%20techniques.%20First%2C%20we%20introduce%20Diff-Gen%2C%20a%20large-scale%20benchmark%20dataset%20comprising%20100k%20diffusion-generated%20fakes%20that%20capture%20broad%20spectral%20artifacts%20unlike%20traditional%20GAN%20datasets.%20Models%20trained%20on%20Diff-Gen%20demonstrate%20stronger%20cross-domain%20generalization%2C%20particularly%20on%20previously%20unseen%20image%20generators.%20Second%2C%20we%20propose%20AdaptPrompt%2C%20a%20parameter-efficient%20transfer%20learning%20framework%20that%20jointly%20learns%20task-specific%20textual%20prompts%20and%20visual%20adapters%20while%20keeping%20the%20CLIP%20backbone%20frozen.%20We%20further%20show%20via%20layer%20ablation%20that%20pruning%20the%20final%20transformer%20block%20of%20the%20vision%20encoder%20enhances%20the%20retention%20of%20high-frequency%20generative%20artifacts%2C%20significantly%20boosting%20detection%20accuracy.%20Our%20evaluation%20spans%2025%20challenging%20test%20sets%2C%20covering%20synthetic%20content%20generated%20by%20GANs%2C%20diffusion%20models%2C%20and%20commercial%20tools%2C%20establishing%20a%20new%20state-of-the-art%20in%20both%20standard%20and%20cross-domain%20scenarios.%20We%20further%20demonstrate%20the%20framework%27s%20versatility%20through%20few-shot%20generalization%20%28using%20as%20few%20as%20320%20images%29%20and%20source%20attribution%2C%20enabling%20the%20precise%20identification%20of%20generator%20architectures%20in%20closed-set%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.17730v1&entry.124074799=Read"},
{"title": "Digital and Web Forensics Model Cards, V1", "author": "Paola Di Maio", "abstract": "This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.", "link": "http://arxiv.org/abs/2512.17722v1", "date": "2025-12-19", "relevancy": 2.3587, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4632}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20and%20Web%20Forensics%20Model%20Cards%2C%20V1&body=Title%3A%20Digital%20and%20Web%20Forensics%20Model%20Cards%2C%20V1%0AAuthor%3A%20Paola%20Di%20Maio%0AAbstract%3A%20This%20paper%20introduces%20a%20standardized%20model%20card%20framework%20specifically%20designed%20for%20digital%20and%20web%20forensics.%20Building%20upon%20established%20model%20card%20methodologies%20and%20recent%20work%20on%20abstract%20models%20for%20digital%20forensic%20analysis%2C%20this%20paper%20presents%20a%20web%20based%20framework%20that%20generates%20model%20cards%20specifically%20designed%20to%20represent%20knowledge%20in%20the%20forensic%20domain.%20The%20framework%20includes%20controlled%20vocabularies%20for%20classification%2C%20reasoning%20types%2C%20bias%20identification%2C%20and%20error%20categorization%2C%20along%20with%20a%20web-based%20generator%20tool%20to%20facilitate%20adoption.%20The%20paper%20describes%20the%20model%20card%20structure%2C%20presents%20the%20controlled%20vocabularies%2C%20and%20introduces%20the%20beta%20version%20of%20the%20generator%20tool%2C%20inviting%20community%20feedback%20to%20refine%20this%20emerging%20standard.%20Ultimately%2C%20the%20systemic%20risk%20is%20that%20that%20the%20anti%20fraud%20and%20digital%20and%20web%20forensics%20processes%20are%20controlled%20by%20the%20mobs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520and%2520Web%2520Forensics%2520Model%2520Cards%252C%2520V1%26entry.906535625%3DPaola%2520Di%2520Maio%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520standardized%2520model%2520card%2520framework%2520specifically%2520designed%2520for%2520digital%2520and%2520web%2520forensics.%2520Building%2520upon%2520established%2520model%2520card%2520methodologies%2520and%2520recent%2520work%2520on%2520abstract%2520models%2520for%2520digital%2520forensic%2520analysis%252C%2520this%2520paper%2520presents%2520a%2520web%2520based%2520framework%2520that%2520generates%2520model%2520cards%2520specifically%2520designed%2520to%2520represent%2520knowledge%2520in%2520the%2520forensic%2520domain.%2520The%2520framework%2520includes%2520controlled%2520vocabularies%2520for%2520classification%252C%2520reasoning%2520types%252C%2520bias%2520identification%252C%2520and%2520error%2520categorization%252C%2520along%2520with%2520a%2520web-based%2520generator%2520tool%2520to%2520facilitate%2520adoption.%2520The%2520paper%2520describes%2520the%2520model%2520card%2520structure%252C%2520presents%2520the%2520controlled%2520vocabularies%252C%2520and%2520introduces%2520the%2520beta%2520version%2520of%2520the%2520generator%2520tool%252C%2520inviting%2520community%2520feedback%2520to%2520refine%2520this%2520emerging%2520standard.%2520Ultimately%252C%2520the%2520systemic%2520risk%2520is%2520that%2520that%2520the%2520anti%2520fraud%2520and%2520digital%2520and%2520web%2520forensics%2520processes%2520are%2520controlled%2520by%2520the%2520mobs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20and%20Web%20Forensics%20Model%20Cards%2C%20V1&entry.906535625=Paola%20Di%20Maio&entry.1292438233=This%20paper%20introduces%20a%20standardized%20model%20card%20framework%20specifically%20designed%20for%20digital%20and%20web%20forensics.%20Building%20upon%20established%20model%20card%20methodologies%20and%20recent%20work%20on%20abstract%20models%20for%20digital%20forensic%20analysis%2C%20this%20paper%20presents%20a%20web%20based%20framework%20that%20generates%20model%20cards%20specifically%20designed%20to%20represent%20knowledge%20in%20the%20forensic%20domain.%20The%20framework%20includes%20controlled%20vocabularies%20for%20classification%2C%20reasoning%20types%2C%20bias%20identification%2C%20and%20error%20categorization%2C%20along%20with%20a%20web-based%20generator%20tool%20to%20facilitate%20adoption.%20The%20paper%20describes%20the%20model%20card%20structure%2C%20presents%20the%20controlled%20vocabularies%2C%20and%20introduces%20the%20beta%20version%20of%20the%20generator%20tool%2C%20inviting%20community%20feedback%20to%20refine%20this%20emerging%20standard.%20Ultimately%2C%20the%20systemic%20risk%20is%20that%20that%20the%20anti%20fraud%20and%20digital%20and%20web%20forensics%20processes%20are%20controlled%20by%20the%20mobs.&entry.1838667208=http%3A//arxiv.org/abs/2512.17722v1&entry.124074799=Read"},
{"title": "Generating Samples to Probe Trained Models", "author": "Eren Mehmet K\u0131ral and Nur\u015fen Ayd\u0131n and \u015e. \u0130lker Birbil", "abstract": "There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.", "link": "http://arxiv.org/abs/2502.06658v3", "date": "2025-12-19", "relevancy": 2.3503, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4702}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Samples%20to%20Probe%20Trained%20Models&body=Title%3A%20Generating%20Samples%20to%20Probe%20Trained%20Models%0AAuthor%3A%20Eren%20Mehmet%20K%C4%B1ral%20and%20Nur%C5%9Fen%20Ayd%C4%B1n%20and%20%C5%9E.%20%C4%B0lker%20Birbil%0AAbstract%3A%20There%20is%20a%20growing%20need%20for%20investigating%20how%20machine%20learning%20models%20operate.%20With%20this%20work%2C%20we%20aim%20to%20understand%20trained%20machine%20learning%20models%20by%20questioning%20their%20data%20preferences.%20We%20propose%20a%20mathematical%20framework%20that%20allows%20us%20to%20probe%20trained%20models%20and%20identify%20their%20preferred%20samples%20in%20various%20scenarios%20including%20prediction-risky%2C%20parameter-sensitive%2C%20or%20model-contrastive%20samples.%20To%20showcase%20our%20framework%2C%20we%20pose%20these%20queries%20to%20a%20range%20of%20models%20trained%20on%20a%20range%20of%20classification%20and%20regression%20tasks%2C%20and%20receive%20answers%20in%20the%20form%20of%20generated%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2502.06658v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Samples%2520to%2520Probe%2520Trained%2520Models%26entry.906535625%3DEren%2520Mehmet%2520K%25C4%25B1ral%2520and%2520Nur%25C5%259Fen%2520Ayd%25C4%25B1n%2520and%2520%25C5%259E.%2520%25C4%25B0lker%2520Birbil%26entry.1292438233%3DThere%2520is%2520a%2520growing%2520need%2520for%2520investigating%2520how%2520machine%2520learning%2520models%2520operate.%2520With%2520this%2520work%252C%2520we%2520aim%2520to%2520understand%2520trained%2520machine%2520learning%2520models%2520by%2520questioning%2520their%2520data%2520preferences.%2520We%2520propose%2520a%2520mathematical%2520framework%2520that%2520allows%2520us%2520to%2520probe%2520trained%2520models%2520and%2520identify%2520their%2520preferred%2520samples%2520in%2520various%2520scenarios%2520including%2520prediction-risky%252C%2520parameter-sensitive%252C%2520or%2520model-contrastive%2520samples.%2520To%2520showcase%2520our%2520framework%252C%2520we%2520pose%2520these%2520queries%2520to%2520a%2520range%2520of%2520models%2520trained%2520on%2520a%2520range%2520of%2520classification%2520and%2520regression%2520tasks%252C%2520and%2520receive%2520answers%2520in%2520the%2520form%2520of%2520generated%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06658v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Samples%20to%20Probe%20Trained%20Models&entry.906535625=Eren%20Mehmet%20K%C4%B1ral%20and%20Nur%C5%9Fen%20Ayd%C4%B1n%20and%20%C5%9E.%20%C4%B0lker%20Birbil&entry.1292438233=There%20is%20a%20growing%20need%20for%20investigating%20how%20machine%20learning%20models%20operate.%20With%20this%20work%2C%20we%20aim%20to%20understand%20trained%20machine%20learning%20models%20by%20questioning%20their%20data%20preferences.%20We%20propose%20a%20mathematical%20framework%20that%20allows%20us%20to%20probe%20trained%20models%20and%20identify%20their%20preferred%20samples%20in%20various%20scenarios%20including%20prediction-risky%2C%20parameter-sensitive%2C%20or%20model-contrastive%20samples.%20To%20showcase%20our%20framework%2C%20we%20pose%20these%20queries%20to%20a%20range%20of%20models%20trained%20on%20a%20range%20of%20classification%20and%20regression%20tasks%2C%20and%20receive%20answers%20in%20the%20form%20of%20generated%20data.&entry.1838667208=http%3A//arxiv.org/abs/2502.06658v3&entry.124074799=Read"},
{"title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning", "author": "Ran Gong and Xiaohan Zhang and Jinghuan Shang and Maria Vittoria Minniti and Jigarkumar Patel and Valerio Pepe and Riedana Yan and Ahmet Gundogdu and Ivan Kapelyukh and Ali Abbas and Xiaoqiang Yan and Harsh Patel and Laura Herlant and Karl Schmeckpeper", "abstract": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .", "link": "http://arxiv.org/abs/2512.17853v1", "date": "2025-12-19", "relevancy": 2.3499, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6158}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5875}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyTask%3A%20an%20Automated%20Task%20and%20Data%20Generation%20Framework%20for%20Advancing%20Sim-to-Real%20Policy%20Learning&body=Title%3A%20AnyTask%3A%20an%20Automated%20Task%20and%20Data%20Generation%20Framework%20for%20Advancing%20Sim-to-Real%20Policy%20Learning%0AAuthor%3A%20Ran%20Gong%20and%20Xiaohan%20Zhang%20and%20Jinghuan%20Shang%20and%20Maria%20Vittoria%20Minniti%20and%20Jigarkumar%20Patel%20and%20Valerio%20Pepe%20and%20Riedana%20Yan%20and%20Ahmet%20Gundogdu%20and%20Ivan%20Kapelyukh%20and%20Ali%20Abbas%20and%20Xiaoqiang%20Yan%20and%20Harsh%20Patel%20and%20Laura%20Herlant%20and%20Karl%20Schmeckpeper%0AAbstract%3A%20Generalist%20robot%20learning%20remains%20constrained%20by%20data%3A%20large-scale%2C%20diverse%2C%20and%20high-quality%20interaction%20data%20are%20expensive%20to%20collect%20in%20the%20real%20world.%20While%20simulation%20has%20become%20a%20promising%20way%20for%20scaling%20up%20data%20collection%2C%20the%20related%20tasks%2C%20including%20simulation%20task%20design%2C%20task-aware%20scene%20generation%2C%20expert%20demonstration%20synthesis%2C%20and%20sim-to-real%20transfer%2C%20still%20demand%20substantial%20human%20effort.%20We%20present%20AnyTask%2C%20an%20automated%20framework%20that%20pairs%20massively%20parallel%20GPU%20simulation%20with%20foundation%20models%20to%20design%20diverse%20manipulation%20tasks%20and%20synthesize%20robot%20data.%20We%20introduce%20three%20AnyTask%20agents%20for%20generating%20expert%20demonstrations%20aiming%20to%20solve%20as%20many%20tasks%20as%20possible%3A%201%29%20ViPR%2C%20a%20novel%20task%20and%20motion%20planning%20agent%20with%20VLM-in-the-loop%20Parallel%20Refinement%3B%202%29%20ViPR-Eureka%2C%20a%20reinforcement%20learning%20agent%20with%20generated%20dense%20rewards%20and%20LLM-guided%20contact%20sampling%3B%203%29%20ViPR-RL%2C%20a%20hybrid%20planning%20and%20learning%20approach%20that%20jointly%20produces%20high-quality%20demonstrations%20with%20only%20sparse%20rewards.%20We%20train%20behavior%20cloning%20policies%20on%20generated%20data%2C%20validate%20them%20in%20simulation%2C%20and%20deploy%20them%20directly%20on%20real%20robot%20hardware.%20The%20policies%20generalize%20to%20novel%20object%20poses%2C%20achieving%2044%25%20average%20success%20across%20a%20suite%20of%20real-world%20pick-and-place%2C%20drawer%20opening%2C%20contact-rich%20pushing%2C%20and%20long-horizon%20manipulation%20tasks.%20Our%20project%20website%20is%20at%20https%3A//anytask.rai-inst.com%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyTask%253A%2520an%2520Automated%2520Task%2520and%2520Data%2520Generation%2520Framework%2520for%2520Advancing%2520Sim-to-Real%2520Policy%2520Learning%26entry.906535625%3DRan%2520Gong%2520and%2520Xiaohan%2520Zhang%2520and%2520Jinghuan%2520Shang%2520and%2520Maria%2520Vittoria%2520Minniti%2520and%2520Jigarkumar%2520Patel%2520and%2520Valerio%2520Pepe%2520and%2520Riedana%2520Yan%2520and%2520Ahmet%2520Gundogdu%2520and%2520Ivan%2520Kapelyukh%2520and%2520Ali%2520Abbas%2520and%2520Xiaoqiang%2520Yan%2520and%2520Harsh%2520Patel%2520and%2520Laura%2520Herlant%2520and%2520Karl%2520Schmeckpeper%26entry.1292438233%3DGeneralist%2520robot%2520learning%2520remains%2520constrained%2520by%2520data%253A%2520large-scale%252C%2520diverse%252C%2520and%2520high-quality%2520interaction%2520data%2520are%2520expensive%2520to%2520collect%2520in%2520the%2520real%2520world.%2520While%2520simulation%2520has%2520become%2520a%2520promising%2520way%2520for%2520scaling%2520up%2520data%2520collection%252C%2520the%2520related%2520tasks%252C%2520including%2520simulation%2520task%2520design%252C%2520task-aware%2520scene%2520generation%252C%2520expert%2520demonstration%2520synthesis%252C%2520and%2520sim-to-real%2520transfer%252C%2520still%2520demand%2520substantial%2520human%2520effort.%2520We%2520present%2520AnyTask%252C%2520an%2520automated%2520framework%2520that%2520pairs%2520massively%2520parallel%2520GPU%2520simulation%2520with%2520foundation%2520models%2520to%2520design%2520diverse%2520manipulation%2520tasks%2520and%2520synthesize%2520robot%2520data.%2520We%2520introduce%2520three%2520AnyTask%2520agents%2520for%2520generating%2520expert%2520demonstrations%2520aiming%2520to%2520solve%2520as%2520many%2520tasks%2520as%2520possible%253A%25201%2529%2520ViPR%252C%2520a%2520novel%2520task%2520and%2520motion%2520planning%2520agent%2520with%2520VLM-in-the-loop%2520Parallel%2520Refinement%253B%25202%2529%2520ViPR-Eureka%252C%2520a%2520reinforcement%2520learning%2520agent%2520with%2520generated%2520dense%2520rewards%2520and%2520LLM-guided%2520contact%2520sampling%253B%25203%2529%2520ViPR-RL%252C%2520a%2520hybrid%2520planning%2520and%2520learning%2520approach%2520that%2520jointly%2520produces%2520high-quality%2520demonstrations%2520with%2520only%2520sparse%2520rewards.%2520We%2520train%2520behavior%2520cloning%2520policies%2520on%2520generated%2520data%252C%2520validate%2520them%2520in%2520simulation%252C%2520and%2520deploy%2520them%2520directly%2520on%2520real%2520robot%2520hardware.%2520The%2520policies%2520generalize%2520to%2520novel%2520object%2520poses%252C%2520achieving%252044%2525%2520average%2520success%2520across%2520a%2520suite%2520of%2520real-world%2520pick-and-place%252C%2520drawer%2520opening%252C%2520contact-rich%2520pushing%252C%2520and%2520long-horizon%2520manipulation%2520tasks.%2520Our%2520project%2520website%2520is%2520at%2520https%253A//anytask.rai-inst.com%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyTask%3A%20an%20Automated%20Task%20and%20Data%20Generation%20Framework%20for%20Advancing%20Sim-to-Real%20Policy%20Learning&entry.906535625=Ran%20Gong%20and%20Xiaohan%20Zhang%20and%20Jinghuan%20Shang%20and%20Maria%20Vittoria%20Minniti%20and%20Jigarkumar%20Patel%20and%20Valerio%20Pepe%20and%20Riedana%20Yan%20and%20Ahmet%20Gundogdu%20and%20Ivan%20Kapelyukh%20and%20Ali%20Abbas%20and%20Xiaoqiang%20Yan%20and%20Harsh%20Patel%20and%20Laura%20Herlant%20and%20Karl%20Schmeckpeper&entry.1292438233=Generalist%20robot%20learning%20remains%20constrained%20by%20data%3A%20large-scale%2C%20diverse%2C%20and%20high-quality%20interaction%20data%20are%20expensive%20to%20collect%20in%20the%20real%20world.%20While%20simulation%20has%20become%20a%20promising%20way%20for%20scaling%20up%20data%20collection%2C%20the%20related%20tasks%2C%20including%20simulation%20task%20design%2C%20task-aware%20scene%20generation%2C%20expert%20demonstration%20synthesis%2C%20and%20sim-to-real%20transfer%2C%20still%20demand%20substantial%20human%20effort.%20We%20present%20AnyTask%2C%20an%20automated%20framework%20that%20pairs%20massively%20parallel%20GPU%20simulation%20with%20foundation%20models%20to%20design%20diverse%20manipulation%20tasks%20and%20synthesize%20robot%20data.%20We%20introduce%20three%20AnyTask%20agents%20for%20generating%20expert%20demonstrations%20aiming%20to%20solve%20as%20many%20tasks%20as%20possible%3A%201%29%20ViPR%2C%20a%20novel%20task%20and%20motion%20planning%20agent%20with%20VLM-in-the-loop%20Parallel%20Refinement%3B%202%29%20ViPR-Eureka%2C%20a%20reinforcement%20learning%20agent%20with%20generated%20dense%20rewards%20and%20LLM-guided%20contact%20sampling%3B%203%29%20ViPR-RL%2C%20a%20hybrid%20planning%20and%20learning%20approach%20that%20jointly%20produces%20high-quality%20demonstrations%20with%20only%20sparse%20rewards.%20We%20train%20behavior%20cloning%20policies%20on%20generated%20data%2C%20validate%20them%20in%20simulation%2C%20and%20deploy%20them%20directly%20on%20real%20robot%20hardware.%20The%20policies%20generalize%20to%20novel%20object%20poses%2C%20achieving%2044%25%20average%20success%20across%20a%20suite%20of%20real-world%20pick-and-place%2C%20drawer%20opening%2C%20contact-rich%20pushing%2C%20and%20long-horizon%20manipulation%20tasks.%20Our%20project%20website%20is%20at%20https%3A//anytask.rai-inst.com%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.17853v1&entry.124074799=Read"},
{"title": "Mitigating Forgetting in Low Rank Adaptation", "author": "Joanna Sliwa and Frank Schneider and Philipp Hennig and Jose Miguel Hernandez-Lobato", "abstract": "Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.", "link": "http://arxiv.org/abs/2512.17720v1", "date": "2025-12-19", "relevancy": 2.3402, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4762}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4715}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Forgetting%20in%20Low%20Rank%20Adaptation&body=Title%3A%20Mitigating%20Forgetting%20in%20Low%20Rank%20Adaptation%0AAuthor%3A%20Joanna%20Sliwa%20and%20Frank%20Schneider%20and%20Philipp%20Hennig%20and%20Jose%20Miguel%20Hernandez-Lobato%0AAbstract%3A%20Parameter-efficient%20fine-tuning%20methods%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20enable%20fast%20specialization%20of%20large%20pre-trained%20models%20to%20different%20downstream%20applications.%20However%2C%20this%20process%20often%20leads%20to%20catastrophic%20forgetting%20of%20the%20model%27s%20prior%20domain%20knowledge.%20We%20address%20this%20issue%20with%20LaLoRA%2C%20a%20weight-space%20regularization%20technique%20that%20applies%20a%20Laplace%20approximation%20to%20Low-Rank%20Adaptation.%20Our%20approach%20estimates%20the%20model%27s%20confidence%20in%20each%20parameter%20and%20constrains%20updates%20in%20high-curvature%20directions%2C%20preserving%20prior%20knowledge%20while%20enabling%20efficient%20target-domain%20learning.%20By%20applying%20the%20Laplace%20approximation%20only%20to%20the%20LoRA%20weights%2C%20the%20method%20remains%20lightweight.%20We%20evaluate%20LaLoRA%20by%20fine-tuning%20a%20Llama%20model%20for%20mathematical%20reasoning%20and%20demonstrate%20an%20improved%20learning-forgetting%20trade-off%2C%20which%20can%20be%20directly%20controlled%20via%20the%20method%27s%20regularization%20strength.%20We%20further%20explore%20different%20loss%20landscape%20curvature%20approximations%20for%20estimating%20parameter%20confidence%2C%20analyze%20the%20effect%20of%20the%20data%20used%20for%20the%20Laplace%20approximation%2C%20and%20study%20robustness%20across%20hyperparameters.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Forgetting%2520in%2520Low%2520Rank%2520Adaptation%26entry.906535625%3DJoanna%2520Sliwa%2520and%2520Frank%2520Schneider%2520and%2520Philipp%2520Hennig%2520and%2520Jose%2520Miguel%2520Hernandez-Lobato%26entry.1292438233%3DParameter-efficient%2520fine-tuning%2520methods%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520enable%2520fast%2520specialization%2520of%2520large%2520pre-trained%2520models%2520to%2520different%2520downstream%2520applications.%2520However%252C%2520this%2520process%2520often%2520leads%2520to%2520catastrophic%2520forgetting%2520of%2520the%2520model%2527s%2520prior%2520domain%2520knowledge.%2520We%2520address%2520this%2520issue%2520with%2520LaLoRA%252C%2520a%2520weight-space%2520regularization%2520technique%2520that%2520applies%2520a%2520Laplace%2520approximation%2520to%2520Low-Rank%2520Adaptation.%2520Our%2520approach%2520estimates%2520the%2520model%2527s%2520confidence%2520in%2520each%2520parameter%2520and%2520constrains%2520updates%2520in%2520high-curvature%2520directions%252C%2520preserving%2520prior%2520knowledge%2520while%2520enabling%2520efficient%2520target-domain%2520learning.%2520By%2520applying%2520the%2520Laplace%2520approximation%2520only%2520to%2520the%2520LoRA%2520weights%252C%2520the%2520method%2520remains%2520lightweight.%2520We%2520evaluate%2520LaLoRA%2520by%2520fine-tuning%2520a%2520Llama%2520model%2520for%2520mathematical%2520reasoning%2520and%2520demonstrate%2520an%2520improved%2520learning-forgetting%2520trade-off%252C%2520which%2520can%2520be%2520directly%2520controlled%2520via%2520the%2520method%2527s%2520regularization%2520strength.%2520We%2520further%2520explore%2520different%2520loss%2520landscape%2520curvature%2520approximations%2520for%2520estimating%2520parameter%2520confidence%252C%2520analyze%2520the%2520effect%2520of%2520the%2520data%2520used%2520for%2520the%2520Laplace%2520approximation%252C%2520and%2520study%2520robustness%2520across%2520hyperparameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Forgetting%20in%20Low%20Rank%20Adaptation&entry.906535625=Joanna%20Sliwa%20and%20Frank%20Schneider%20and%20Philipp%20Hennig%20and%20Jose%20Miguel%20Hernandez-Lobato&entry.1292438233=Parameter-efficient%20fine-tuning%20methods%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20enable%20fast%20specialization%20of%20large%20pre-trained%20models%20to%20different%20downstream%20applications.%20However%2C%20this%20process%20often%20leads%20to%20catastrophic%20forgetting%20of%20the%20model%27s%20prior%20domain%20knowledge.%20We%20address%20this%20issue%20with%20LaLoRA%2C%20a%20weight-space%20regularization%20technique%20that%20applies%20a%20Laplace%20approximation%20to%20Low-Rank%20Adaptation.%20Our%20approach%20estimates%20the%20model%27s%20confidence%20in%20each%20parameter%20and%20constrains%20updates%20in%20high-curvature%20directions%2C%20preserving%20prior%20knowledge%20while%20enabling%20efficient%20target-domain%20learning.%20By%20applying%20the%20Laplace%20approximation%20only%20to%20the%20LoRA%20weights%2C%20the%20method%20remains%20lightweight.%20We%20evaluate%20LaLoRA%20by%20fine-tuning%20a%20Llama%20model%20for%20mathematical%20reasoning%20and%20demonstrate%20an%20improved%20learning-forgetting%20trade-off%2C%20which%20can%20be%20directly%20controlled%20via%20the%20method%27s%20regularization%20strength.%20We%20further%20explore%20different%20loss%20landscape%20curvature%20approximations%20for%20estimating%20parameter%20confidence%2C%20analyze%20the%20effect%20of%20the%20data%20used%20for%20the%20Laplace%20approximation%2C%20and%20study%20robustness%20across%20hyperparameters.&entry.1838667208=http%3A//arxiv.org/abs/2512.17720v1&entry.124074799=Read"},
{"title": "Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?", "author": "Guiomar Pescador-Barrios and Sarah Filippi and Mark van der Wilk", "abstract": "Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question \"How big is big enough?\" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.", "link": "http://arxiv.org/abs/2408.07588v5", "date": "2025-12-19", "relevancy": 2.3195, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4674}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4624}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adjusting%20Model%20Size%20in%20Continual%20Gaussian%20Processes%3A%20How%20Big%20is%20Big%20Enough%3F&body=Title%3A%20Adjusting%20Model%20Size%20in%20Continual%20Gaussian%20Processes%3A%20How%20Big%20is%20Big%20Enough%3F%0AAuthor%3A%20Guiomar%20Pescador-Barrios%20and%20Sarah%20Filippi%20and%20Mark%20van%20der%20Wilk%0AAbstract%3A%20Many%20machine%20learning%20models%20require%20setting%20a%20parameter%20that%20controls%20their%20size%20before%20training%2C%20e.g.%20number%20of%20neurons%20in%20DNNs%2C%20or%20inducing%20points%20in%20GPs.%20Increasing%20capacity%20typically%20improves%20performance%20until%20all%20the%20information%20from%20the%20dataset%20is%20captured.%20After%20this%20point%2C%20computational%20cost%20keeps%20increasing%2C%20without%20improved%20performance.%20This%20leads%20to%20the%20question%20%22How%20big%20is%20big%20enough%3F%22%20We%20investigate%20this%20problem%20for%20Gaussian%20processes%20%28single-layer%20neural%20networks%29%20in%20continual%20learning.%20Here%2C%20data%20becomes%20available%20incrementally%2C%20and%20the%20final%20dataset%20size%20will%20therefore%20not%20be%20known%20before%20training%2C%20preventing%20the%20use%20of%20heuristics%20for%20setting%20a%20fixed%20model%20size.%20We%20develop%20a%20method%20to%20automatically%20adjust%20model%20size%20while%20maintaining%20near-optimal%20performance.%20Our%20experimental%20procedure%20follows%20the%20constraint%20that%20any%20hyperparameters%20must%20be%20set%20without%20seeing%20dataset%20properties%2C%20and%20we%20show%20that%20our%20method%20performs%20well%20across%20diverse%20datasets%20without%20the%20need%20to%20adjust%20its%20hyperparameter%2C%20showing%20it%20requires%20less%20tuning%20than%20others.%0ALink%3A%20http%3A//arxiv.org/abs/2408.07588v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdjusting%2520Model%2520Size%2520in%2520Continual%2520Gaussian%2520Processes%253A%2520How%2520Big%2520is%2520Big%2520Enough%253F%26entry.906535625%3DGuiomar%2520Pescador-Barrios%2520and%2520Sarah%2520Filippi%2520and%2520Mark%2520van%2520der%2520Wilk%26entry.1292438233%3DMany%2520machine%2520learning%2520models%2520require%2520setting%2520a%2520parameter%2520that%2520controls%2520their%2520size%2520before%2520training%252C%2520e.g.%2520number%2520of%2520neurons%2520in%2520DNNs%252C%2520or%2520inducing%2520points%2520in%2520GPs.%2520Increasing%2520capacity%2520typically%2520improves%2520performance%2520until%2520all%2520the%2520information%2520from%2520the%2520dataset%2520is%2520captured.%2520After%2520this%2520point%252C%2520computational%2520cost%2520keeps%2520increasing%252C%2520without%2520improved%2520performance.%2520This%2520leads%2520to%2520the%2520question%2520%2522How%2520big%2520is%2520big%2520enough%253F%2522%2520We%2520investigate%2520this%2520problem%2520for%2520Gaussian%2520processes%2520%2528single-layer%2520neural%2520networks%2529%2520in%2520continual%2520learning.%2520Here%252C%2520data%2520becomes%2520available%2520incrementally%252C%2520and%2520the%2520final%2520dataset%2520size%2520will%2520therefore%2520not%2520be%2520known%2520before%2520training%252C%2520preventing%2520the%2520use%2520of%2520heuristics%2520for%2520setting%2520a%2520fixed%2520model%2520size.%2520We%2520develop%2520a%2520method%2520to%2520automatically%2520adjust%2520model%2520size%2520while%2520maintaining%2520near-optimal%2520performance.%2520Our%2520experimental%2520procedure%2520follows%2520the%2520constraint%2520that%2520any%2520hyperparameters%2520must%2520be%2520set%2520without%2520seeing%2520dataset%2520properties%252C%2520and%2520we%2520show%2520that%2520our%2520method%2520performs%2520well%2520across%2520diverse%2520datasets%2520without%2520the%2520need%2520to%2520adjust%2520its%2520hyperparameter%252C%2520showing%2520it%2520requires%2520less%2520tuning%2520than%2520others.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07588v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adjusting%20Model%20Size%20in%20Continual%20Gaussian%20Processes%3A%20How%20Big%20is%20Big%20Enough%3F&entry.906535625=Guiomar%20Pescador-Barrios%20and%20Sarah%20Filippi%20and%20Mark%20van%20der%20Wilk&entry.1292438233=Many%20machine%20learning%20models%20require%20setting%20a%20parameter%20that%20controls%20their%20size%20before%20training%2C%20e.g.%20number%20of%20neurons%20in%20DNNs%2C%20or%20inducing%20points%20in%20GPs.%20Increasing%20capacity%20typically%20improves%20performance%20until%20all%20the%20information%20from%20the%20dataset%20is%20captured.%20After%20this%20point%2C%20computational%20cost%20keeps%20increasing%2C%20without%20improved%20performance.%20This%20leads%20to%20the%20question%20%22How%20big%20is%20big%20enough%3F%22%20We%20investigate%20this%20problem%20for%20Gaussian%20processes%20%28single-layer%20neural%20networks%29%20in%20continual%20learning.%20Here%2C%20data%20becomes%20available%20incrementally%2C%20and%20the%20final%20dataset%20size%20will%20therefore%20not%20be%20known%20before%20training%2C%20preventing%20the%20use%20of%20heuristics%20for%20setting%20a%20fixed%20model%20size.%20We%20develop%20a%20method%20to%20automatically%20adjust%20model%20size%20while%20maintaining%20near-optimal%20performance.%20Our%20experimental%20procedure%20follows%20the%20constraint%20that%20any%20hyperparameters%20must%20be%20set%20without%20seeing%20dataset%20properties%2C%20and%20we%20show%20that%20our%20method%20performs%20well%20across%20diverse%20datasets%20without%20the%20need%20to%20adjust%20its%20hyperparameter%2C%20showing%20it%20requires%20less%20tuning%20than%20others.&entry.1838667208=http%3A//arxiv.org/abs/2408.07588v5&entry.124074799=Read"},
{"title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation", "author": "Rang Li and Lei Li and Shuhuai Ren and Hao Tian and Shuhao Gu and Shicheng Li and Zihao Yue and Yudong Wang and Wenhan Ma and Zhe Yang and Jingyuan Ma and Zhifang Sui and Fuli Luo", "abstract": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.", "link": "http://arxiv.org/abs/2512.17495v1", "date": "2025-12-19", "relevancy": 2.3107, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GroundingME%3A%20Exposing%20the%20Visual%20Grounding%20Gap%20in%20MLLMs%20through%20Multi-Dimensional%20Evaluation&body=Title%3A%20GroundingME%3A%20Exposing%20the%20Visual%20Grounding%20Gap%20in%20MLLMs%20through%20Multi-Dimensional%20Evaluation%0AAuthor%3A%20Rang%20Li%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Hao%20Tian%20and%20Shuhao%20Gu%20and%20Shicheng%20Li%20and%20Zihao%20Yue%20and%20Yudong%20Wang%20and%20Wenhan%20Ma%20and%20Zhe%20Yang%20and%20Jingyuan%20Ma%20and%20Zhifang%20Sui%20and%20Fuli%20Luo%0AAbstract%3A%20Visual%20grounding%2C%20localizing%20objects%20from%20natural%20language%20descriptions%2C%20represents%20a%20critical%20bridge%20between%20language%20and%20vision%20understanding.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20achieve%20impressive%20scores%20on%20existing%20benchmarks%2C%20a%20fundamental%20question%20remains%3A%20can%20MLLMs%20truly%20ground%20language%20in%20vision%20with%20human-like%20sophistication%2C%20or%20are%20they%20merely%20pattern-matching%20on%20simplified%20datasets%3F%20Current%20benchmarks%20fail%20to%20capture%20real-world%20complexity%20where%20humans%20effortlessly%20navigate%20ambiguous%20references%20and%20recognize%20when%20grounding%20is%20impossible.%20To%20rigorously%20assess%20MLLMs%27%20true%20capabilities%2C%20we%20introduce%20GroundingME%2C%20a%20benchmark%20that%20systematically%20challenges%20models%20across%20four%20critical%20dimensions%3A%20%281%29%20Discriminative%2C%20distinguishing%20highly%20similar%20objects%2C%20%282%29%20Spatial%2C%20understanding%20complex%20relational%20descriptions%2C%20%283%29%20Limited%2C%20handling%20occlusions%20or%20tiny%20objects%2C%20and%20%284%29%20Rejection%2C%20recognizing%20ungroundable%20queries.%20Through%20careful%20curation%20combining%20automated%20generation%20with%20human%20verification%2C%20we%20create%201%2C005%20challenging%20examples%20mirroring%20real-world%20complexity.%20Evaluating%2025%20state-of-the-art%20MLLMs%20reveals%20a%20profound%20capability%20gap%3A%20the%20best%20model%20achieves%20only%2045.1%25%20accuracy%2C%20while%20most%20score%200%25%20on%20rejection%20tasks%2C%20reflexively%20hallucinating%20objects%20rather%20than%20acknowledging%20their%20absence%2C%20raising%20critical%20safety%20concerns%20for%20deployment.%20We%20explore%20two%20strategies%20for%20improvements%3A%20%281%29%20test-time%20scaling%20selects%20optimal%20response%20by%20thinking%20trajectory%20to%20improve%20complex%20grounding%20by%20up%20to%202.9%25%2C%20and%20%282%29%20data-mixture%20training%20teaches%20models%20to%20recognize%20ungroundable%20queries%2C%20boosting%20rejection%20accuracy%20from%200%25%20to%2027.9%25.%20GroundingME%20thus%20serves%20as%20both%20a%20diagnostic%20tool%20revealing%20current%20limitations%20in%20MLLMs%20and%20a%20roadmap%20toward%20human-level%20visual%20grounding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroundingME%253A%2520Exposing%2520the%2520Visual%2520Grounding%2520Gap%2520in%2520MLLMs%2520through%2520Multi-Dimensional%2520Evaluation%26entry.906535625%3DRang%2520Li%2520and%2520Lei%2520Li%2520and%2520Shuhuai%2520Ren%2520and%2520Hao%2520Tian%2520and%2520Shuhao%2520Gu%2520and%2520Shicheng%2520Li%2520and%2520Zihao%2520Yue%2520and%2520Yudong%2520Wang%2520and%2520Wenhan%2520Ma%2520and%2520Zhe%2520Yang%2520and%2520Jingyuan%2520Ma%2520and%2520Zhifang%2520Sui%2520and%2520Fuli%2520Luo%26entry.1292438233%3DVisual%2520grounding%252C%2520localizing%2520objects%2520from%2520natural%2520language%2520descriptions%252C%2520represents%2520a%2520critical%2520bridge%2520between%2520language%2520and%2520vision%2520understanding.%2520While%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520achieve%2520impressive%2520scores%2520on%2520existing%2520benchmarks%252C%2520a%2520fundamental%2520question%2520remains%253A%2520can%2520MLLMs%2520truly%2520ground%2520language%2520in%2520vision%2520with%2520human-like%2520sophistication%252C%2520or%2520are%2520they%2520merely%2520pattern-matching%2520on%2520simplified%2520datasets%253F%2520Current%2520benchmarks%2520fail%2520to%2520capture%2520real-world%2520complexity%2520where%2520humans%2520effortlessly%2520navigate%2520ambiguous%2520references%2520and%2520recognize%2520when%2520grounding%2520is%2520impossible.%2520To%2520rigorously%2520assess%2520MLLMs%2527%2520true%2520capabilities%252C%2520we%2520introduce%2520GroundingME%252C%2520a%2520benchmark%2520that%2520systematically%2520challenges%2520models%2520across%2520four%2520critical%2520dimensions%253A%2520%25281%2529%2520Discriminative%252C%2520distinguishing%2520highly%2520similar%2520objects%252C%2520%25282%2529%2520Spatial%252C%2520understanding%2520complex%2520relational%2520descriptions%252C%2520%25283%2529%2520Limited%252C%2520handling%2520occlusions%2520or%2520tiny%2520objects%252C%2520and%2520%25284%2529%2520Rejection%252C%2520recognizing%2520ungroundable%2520queries.%2520Through%2520careful%2520curation%2520combining%2520automated%2520generation%2520with%2520human%2520verification%252C%2520we%2520create%25201%252C005%2520challenging%2520examples%2520mirroring%2520real-world%2520complexity.%2520Evaluating%252025%2520state-of-the-art%2520MLLMs%2520reveals%2520a%2520profound%2520capability%2520gap%253A%2520the%2520best%2520model%2520achieves%2520only%252045.1%2525%2520accuracy%252C%2520while%2520most%2520score%25200%2525%2520on%2520rejection%2520tasks%252C%2520reflexively%2520hallucinating%2520objects%2520rather%2520than%2520acknowledging%2520their%2520absence%252C%2520raising%2520critical%2520safety%2520concerns%2520for%2520deployment.%2520We%2520explore%2520two%2520strategies%2520for%2520improvements%253A%2520%25281%2529%2520test-time%2520scaling%2520selects%2520optimal%2520response%2520by%2520thinking%2520trajectory%2520to%2520improve%2520complex%2520grounding%2520by%2520up%2520to%25202.9%2525%252C%2520and%2520%25282%2529%2520data-mixture%2520training%2520teaches%2520models%2520to%2520recognize%2520ungroundable%2520queries%252C%2520boosting%2520rejection%2520accuracy%2520from%25200%2525%2520to%252027.9%2525.%2520GroundingME%2520thus%2520serves%2520as%2520both%2520a%2520diagnostic%2520tool%2520revealing%2520current%2520limitations%2520in%2520MLLMs%2520and%2520a%2520roadmap%2520toward%2520human-level%2520visual%2520grounding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroundingME%3A%20Exposing%20the%20Visual%20Grounding%20Gap%20in%20MLLMs%20through%20Multi-Dimensional%20Evaluation&entry.906535625=Rang%20Li%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Hao%20Tian%20and%20Shuhao%20Gu%20and%20Shicheng%20Li%20and%20Zihao%20Yue%20and%20Yudong%20Wang%20and%20Wenhan%20Ma%20and%20Zhe%20Yang%20and%20Jingyuan%20Ma%20and%20Zhifang%20Sui%20and%20Fuli%20Luo&entry.1292438233=Visual%20grounding%2C%20localizing%20objects%20from%20natural%20language%20descriptions%2C%20represents%20a%20critical%20bridge%20between%20language%20and%20vision%20understanding.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20achieve%20impressive%20scores%20on%20existing%20benchmarks%2C%20a%20fundamental%20question%20remains%3A%20can%20MLLMs%20truly%20ground%20language%20in%20vision%20with%20human-like%20sophistication%2C%20or%20are%20they%20merely%20pattern-matching%20on%20simplified%20datasets%3F%20Current%20benchmarks%20fail%20to%20capture%20real-world%20complexity%20where%20humans%20effortlessly%20navigate%20ambiguous%20references%20and%20recognize%20when%20grounding%20is%20impossible.%20To%20rigorously%20assess%20MLLMs%27%20true%20capabilities%2C%20we%20introduce%20GroundingME%2C%20a%20benchmark%20that%20systematically%20challenges%20models%20across%20four%20critical%20dimensions%3A%20%281%29%20Discriminative%2C%20distinguishing%20highly%20similar%20objects%2C%20%282%29%20Spatial%2C%20understanding%20complex%20relational%20descriptions%2C%20%283%29%20Limited%2C%20handling%20occlusions%20or%20tiny%20objects%2C%20and%20%284%29%20Rejection%2C%20recognizing%20ungroundable%20queries.%20Through%20careful%20curation%20combining%20automated%20generation%20with%20human%20verification%2C%20we%20create%201%2C005%20challenging%20examples%20mirroring%20real-world%20complexity.%20Evaluating%2025%20state-of-the-art%20MLLMs%20reveals%20a%20profound%20capability%20gap%3A%20the%20best%20model%20achieves%20only%2045.1%25%20accuracy%2C%20while%20most%20score%200%25%20on%20rejection%20tasks%2C%20reflexively%20hallucinating%20objects%20rather%20than%20acknowledging%20their%20absence%2C%20raising%20critical%20safety%20concerns%20for%20deployment.%20We%20explore%20two%20strategies%20for%20improvements%3A%20%281%29%20test-time%20scaling%20selects%20optimal%20response%20by%20thinking%20trajectory%20to%20improve%20complex%20grounding%20by%20up%20to%202.9%25%2C%20and%20%282%29%20data-mixture%20training%20teaches%20models%20to%20recognize%20ungroundable%20queries%2C%20boosting%20rejection%20accuracy%20from%200%25%20to%2027.9%25.%20GroundingME%20thus%20serves%20as%20both%20a%20diagnostic%20tool%20revealing%20current%20limitations%20in%20MLLMs%20and%20a%20roadmap%20toward%20human-level%20visual%20grounding.&entry.1838667208=http%3A//arxiv.org/abs/2512.17495v1&entry.124074799=Read"},
{"title": "Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests", "author": "Guglielmo Del Col and V\u00e4in\u00f6 Karjalainen and Teemu Hakala and Yibo Zhang and Eija Honkavaara", "abstract": "Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz.\n  The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.", "link": "http://arxiv.org/abs/2512.17553v1", "date": "2025-12-19", "relevancy": 2.3059, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5804}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Robust%20Autonomous%20Navigation%20of%20Aerial%20Robots%20in%20Dense%20Forests&body=Title%3A%20Deep%20Learning-based%20Robust%20Autonomous%20Navigation%20of%20Aerial%20Robots%20in%20Dense%20Forests%0AAuthor%3A%20Guglielmo%20Del%20Col%20and%20V%C3%A4in%C3%B6%20Karjalainen%20and%20Teemu%20Hakala%20and%20Yibo%20Zhang%20and%20Eija%20Honkavaara%0AAbstract%3A%20Autonomous%20aerial%20navigation%20in%20dense%20natural%20environments%20remains%20challenging%20due%20to%20limited%20visibility%2C%20thin%20and%20irregular%20obstacles%2C%20GNSS-denied%20operation%2C%20and%20frequent%20perceptual%20degradation.%20This%20work%20presents%20an%20improved%20deep%20learning-based%20navigation%20framework%20that%20integrates%20semantically%20enhanced%20depth%20encoding%20with%20neural%20motion-primitive%20evaluation%20for%20robust%20flight%20in%20cluttered%20forests.%20Several%20modules%20are%20incorporated%20on%20top%20of%20the%20original%20sevae-ORACLE%20algorithm%20to%20address%20limitations%20observed%20during%20real-world%20deployment%2C%20including%20lateral%20control%20for%20sharper%20maneuvering%2C%20a%20temporal%20consistency%20mechanism%20to%20suppress%20oscillatory%20planning%20decisions%2C%20a%20stereo-based%20visual-inertial%20odometry%20solution%20for%20drift-resilient%20state%20estimation%2C%20and%20a%20supervisory%20safety%20layer%20that%20filters%20unsafe%20actions%20in%20real%20time.%20A%20depth%20refinement%20stage%20is%20included%20to%20improve%20the%20representation%20of%20thin%20branches%20and%20reduce%20stereo%20noise%2C%20while%20GPU%20optimization%20increases%20onboard%20inference%20throughput%20from%204%20Hz%20to%2010%20Hz.%0A%20%20The%20proposed%20approach%20is%20evaluated%20against%20several%20existing%20learning-based%20navigation%20methods%20under%20identical%20environmental%20conditions%20and%20hardware%20constraints.%20It%20demonstrates%20higher%20success%20rates%2C%20more%20stable%20trajectories%2C%20and%20improved%20collision%20avoidance%2C%20particularly%20in%20highly%20cluttered%20forest%20settings.%20The%20system%20is%20deployed%20on%20a%20custom%20quadrotor%20in%20three%20boreal%20forest%20environments%2C%20achieving%20fully%20autonomous%20completion%20in%20all%20flights%20in%20moderate%20and%20dense%20clutter%2C%20and%2012%20out%20of%2015%20flights%20in%20highly%20dense%20underbrush.%20These%20results%20demonstrate%20improved%20reliability%20and%20safety%20over%20existing%20navigation%20methods%20in%20complex%20natural%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Robust%2520Autonomous%2520Navigation%2520of%2520Aerial%2520Robots%2520in%2520Dense%2520Forests%26entry.906535625%3DGuglielmo%2520Del%2520Col%2520and%2520V%25C3%25A4in%25C3%25B6%2520Karjalainen%2520and%2520Teemu%2520Hakala%2520and%2520Yibo%2520Zhang%2520and%2520Eija%2520Honkavaara%26entry.1292438233%3DAutonomous%2520aerial%2520navigation%2520in%2520dense%2520natural%2520environments%2520remains%2520challenging%2520due%2520to%2520limited%2520visibility%252C%2520thin%2520and%2520irregular%2520obstacles%252C%2520GNSS-denied%2520operation%252C%2520and%2520frequent%2520perceptual%2520degradation.%2520This%2520work%2520presents%2520an%2520improved%2520deep%2520learning-based%2520navigation%2520framework%2520that%2520integrates%2520semantically%2520enhanced%2520depth%2520encoding%2520with%2520neural%2520motion-primitive%2520evaluation%2520for%2520robust%2520flight%2520in%2520cluttered%2520forests.%2520Several%2520modules%2520are%2520incorporated%2520on%2520top%2520of%2520the%2520original%2520sevae-ORACLE%2520algorithm%2520to%2520address%2520limitations%2520observed%2520during%2520real-world%2520deployment%252C%2520including%2520lateral%2520control%2520for%2520sharper%2520maneuvering%252C%2520a%2520temporal%2520consistency%2520mechanism%2520to%2520suppress%2520oscillatory%2520planning%2520decisions%252C%2520a%2520stereo-based%2520visual-inertial%2520odometry%2520solution%2520for%2520drift-resilient%2520state%2520estimation%252C%2520and%2520a%2520supervisory%2520safety%2520layer%2520that%2520filters%2520unsafe%2520actions%2520in%2520real%2520time.%2520A%2520depth%2520refinement%2520stage%2520is%2520included%2520to%2520improve%2520the%2520representation%2520of%2520thin%2520branches%2520and%2520reduce%2520stereo%2520noise%252C%2520while%2520GPU%2520optimization%2520increases%2520onboard%2520inference%2520throughput%2520from%25204%2520Hz%2520to%252010%2520Hz.%250A%2520%2520The%2520proposed%2520approach%2520is%2520evaluated%2520against%2520several%2520existing%2520learning-based%2520navigation%2520methods%2520under%2520identical%2520environmental%2520conditions%2520and%2520hardware%2520constraints.%2520It%2520demonstrates%2520higher%2520success%2520rates%252C%2520more%2520stable%2520trajectories%252C%2520and%2520improved%2520collision%2520avoidance%252C%2520particularly%2520in%2520highly%2520cluttered%2520forest%2520settings.%2520The%2520system%2520is%2520deployed%2520on%2520a%2520custom%2520quadrotor%2520in%2520three%2520boreal%2520forest%2520environments%252C%2520achieving%2520fully%2520autonomous%2520completion%2520in%2520all%2520flights%2520in%2520moderate%2520and%2520dense%2520clutter%252C%2520and%252012%2520out%2520of%252015%2520flights%2520in%2520highly%2520dense%2520underbrush.%2520These%2520results%2520demonstrate%2520improved%2520reliability%2520and%2520safety%2520over%2520existing%2520navigation%2520methods%2520in%2520complex%2520natural%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Robust%20Autonomous%20Navigation%20of%20Aerial%20Robots%20in%20Dense%20Forests&entry.906535625=Guglielmo%20Del%20Col%20and%20V%C3%A4in%C3%B6%20Karjalainen%20and%20Teemu%20Hakala%20and%20Yibo%20Zhang%20and%20Eija%20Honkavaara&entry.1292438233=Autonomous%20aerial%20navigation%20in%20dense%20natural%20environments%20remains%20challenging%20due%20to%20limited%20visibility%2C%20thin%20and%20irregular%20obstacles%2C%20GNSS-denied%20operation%2C%20and%20frequent%20perceptual%20degradation.%20This%20work%20presents%20an%20improved%20deep%20learning-based%20navigation%20framework%20that%20integrates%20semantically%20enhanced%20depth%20encoding%20with%20neural%20motion-primitive%20evaluation%20for%20robust%20flight%20in%20cluttered%20forests.%20Several%20modules%20are%20incorporated%20on%20top%20of%20the%20original%20sevae-ORACLE%20algorithm%20to%20address%20limitations%20observed%20during%20real-world%20deployment%2C%20including%20lateral%20control%20for%20sharper%20maneuvering%2C%20a%20temporal%20consistency%20mechanism%20to%20suppress%20oscillatory%20planning%20decisions%2C%20a%20stereo-based%20visual-inertial%20odometry%20solution%20for%20drift-resilient%20state%20estimation%2C%20and%20a%20supervisory%20safety%20layer%20that%20filters%20unsafe%20actions%20in%20real%20time.%20A%20depth%20refinement%20stage%20is%20included%20to%20improve%20the%20representation%20of%20thin%20branches%20and%20reduce%20stereo%20noise%2C%20while%20GPU%20optimization%20increases%20onboard%20inference%20throughput%20from%204%20Hz%20to%2010%20Hz.%0A%20%20The%20proposed%20approach%20is%20evaluated%20against%20several%20existing%20learning-based%20navigation%20methods%20under%20identical%20environmental%20conditions%20and%20hardware%20constraints.%20It%20demonstrates%20higher%20success%20rates%2C%20more%20stable%20trajectories%2C%20and%20improved%20collision%20avoidance%2C%20particularly%20in%20highly%20cluttered%20forest%20settings.%20The%20system%20is%20deployed%20on%20a%20custom%20quadrotor%20in%20three%20boreal%20forest%20environments%2C%20achieving%20fully%20autonomous%20completion%20in%20all%20flights%20in%20moderate%20and%20dense%20clutter%2C%20and%2012%20out%20of%2015%20flights%20in%20highly%20dense%20underbrush.%20These%20results%20demonstrate%20improved%20reliability%20and%20safety%20over%20existing%20navigation%20methods%20in%20complex%20natural%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.17553v1&entry.124074799=Read"},
{"title": "Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data", "author": "Rahul Ravi and Ruizhe Li and Tarek Abdelfatah and Stephen Chan and Xin Chen", "abstract": "Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.", "link": "http://arxiv.org/abs/2512.17759v1", "date": "2025-12-19", "relevancy": 2.3047, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.464}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4609}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breast%20Cancer%20Neoadjuvant%20Chemotherapy%20Treatment%20Response%20Prediction%20Using%20Aligned%20Longitudinal%20MRI%20and%20Clinical%20Data&body=Title%3A%20Breast%20Cancer%20Neoadjuvant%20Chemotherapy%20Treatment%20Response%20Prediction%20Using%20Aligned%20Longitudinal%20MRI%20and%20Clinical%20Data%0AAuthor%3A%20Rahul%20Ravi%20and%20Ruizhe%20Li%20and%20Tarek%20Abdelfatah%20and%20Stephen%20Chan%20and%20Xin%20Chen%0AAbstract%3A%20Aim%3A%20This%20study%20investigates%20treatment%20response%20prediction%20to%20neoadjuvant%20chemotherapy%20%28NACT%29%20in%20breast%20cancer%20patients%2C%20using%20longitudinal%20contrast-enhanced%20magnetic%20resonance%20images%20%28CE-MRI%29%20and%20clinical%20data.%20The%20goal%20is%20to%20develop%20machine%20learning%20%28ML%29%20models%20to%20predict%20pathologic%20complete%20response%20%28PCR%20binary%20classification%29%20and%205-year%20relapse-free%20survival%20status%20%28RFS%20binary%20classification%29.%20Method%3A%20The%20proposed%20framework%20includes%20tumour%20segmentation%2C%20image%20registration%2C%20feature%20extraction%2C%20and%20predictive%20modelling.%20Using%20the%20image%20registration%20method%2C%20MRI%20image%20features%20can%20be%20extracted%20and%20compared%20from%20the%20original%20tumour%20site%20at%20different%20time%20points%2C%20therefore%20monitoring%20the%20intratumor%20changes%20during%20NACT%20process.%20Four%20feature%20extractors%2C%20including%20one%20radiomics%20and%20three%20deep%20learning-based%20%28MedicalNet%2C%20Segformer3D%2C%20SAM-Med3D%29%20were%20implemented%20and%20compared.%20In%20combination%20with%20three%20feature%20selection%20methods%20and%20four%20ML%20models%2C%20predictive%20models%20are%20built%20and%20compared.%20Results%3A%20The%20proposed%20image%20registration-based%20feature%20extraction%20consistently%20improves%20the%20predictive%20models.%20In%20the%20PCR%20and%20RFS%20classification%20tasks%20logistic%20regression%20model%20trained%20on%20radiomic%20features%20performed%20the%20best%20with%20an%20AUC%20of%200.88%20and%20classification%20accuracy%20of%200.85%20for%20PCR%20classification%2C%20and%20AUC%20of%200.78%20and%20classification%20accuracy%20of%200.72%20for%20RFS%20classification.%20Conclusions%3A%20It%20is%20evidenced%20that%20the%20image%20registration%20method%20has%20significantly%20improved%20performance%20in%20longitudinal%20feature%20learning%20in%20predicting%20PCR%20and%20RFS.%20The%20radiomics%20feature%20extractor%20is%20more%20effective%20than%20the%20pre-trained%20deep%20learning%20feature%20extractors%2C%20with%20higher%20performance%20and%20better%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreast%2520Cancer%2520Neoadjuvant%2520Chemotherapy%2520Treatment%2520Response%2520Prediction%2520Using%2520Aligned%2520Longitudinal%2520MRI%2520and%2520Clinical%2520Data%26entry.906535625%3DRahul%2520Ravi%2520and%2520Ruizhe%2520Li%2520and%2520Tarek%2520Abdelfatah%2520and%2520Stephen%2520Chan%2520and%2520Xin%2520Chen%26entry.1292438233%3DAim%253A%2520This%2520study%2520investigates%2520treatment%2520response%2520prediction%2520to%2520neoadjuvant%2520chemotherapy%2520%2528NACT%2529%2520in%2520breast%2520cancer%2520patients%252C%2520using%2520longitudinal%2520contrast-enhanced%2520magnetic%2520resonance%2520images%2520%2528CE-MRI%2529%2520and%2520clinical%2520data.%2520The%2520goal%2520is%2520to%2520develop%2520machine%2520learning%2520%2528ML%2529%2520models%2520to%2520predict%2520pathologic%2520complete%2520response%2520%2528PCR%2520binary%2520classification%2529%2520and%25205-year%2520relapse-free%2520survival%2520status%2520%2528RFS%2520binary%2520classification%2529.%2520Method%253A%2520The%2520proposed%2520framework%2520includes%2520tumour%2520segmentation%252C%2520image%2520registration%252C%2520feature%2520extraction%252C%2520and%2520predictive%2520modelling.%2520Using%2520the%2520image%2520registration%2520method%252C%2520MRI%2520image%2520features%2520can%2520be%2520extracted%2520and%2520compared%2520from%2520the%2520original%2520tumour%2520site%2520at%2520different%2520time%2520points%252C%2520therefore%2520monitoring%2520the%2520intratumor%2520changes%2520during%2520NACT%2520process.%2520Four%2520feature%2520extractors%252C%2520including%2520one%2520radiomics%2520and%2520three%2520deep%2520learning-based%2520%2528MedicalNet%252C%2520Segformer3D%252C%2520SAM-Med3D%2529%2520were%2520implemented%2520and%2520compared.%2520In%2520combination%2520with%2520three%2520feature%2520selection%2520methods%2520and%2520four%2520ML%2520models%252C%2520predictive%2520models%2520are%2520built%2520and%2520compared.%2520Results%253A%2520The%2520proposed%2520image%2520registration-based%2520feature%2520extraction%2520consistently%2520improves%2520the%2520predictive%2520models.%2520In%2520the%2520PCR%2520and%2520RFS%2520classification%2520tasks%2520logistic%2520regression%2520model%2520trained%2520on%2520radiomic%2520features%2520performed%2520the%2520best%2520with%2520an%2520AUC%2520of%25200.88%2520and%2520classification%2520accuracy%2520of%25200.85%2520for%2520PCR%2520classification%252C%2520and%2520AUC%2520of%25200.78%2520and%2520classification%2520accuracy%2520of%25200.72%2520for%2520RFS%2520classification.%2520Conclusions%253A%2520It%2520is%2520evidenced%2520that%2520the%2520image%2520registration%2520method%2520has%2520significantly%2520improved%2520performance%2520in%2520longitudinal%2520feature%2520learning%2520in%2520predicting%2520PCR%2520and%2520RFS.%2520The%2520radiomics%2520feature%2520extractor%2520is%2520more%2520effective%2520than%2520the%2520pre-trained%2520deep%2520learning%2520feature%2520extractors%252C%2520with%2520higher%2520performance%2520and%2520better%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Cancer%20Neoadjuvant%20Chemotherapy%20Treatment%20Response%20Prediction%20Using%20Aligned%20Longitudinal%20MRI%20and%20Clinical%20Data&entry.906535625=Rahul%20Ravi%20and%20Ruizhe%20Li%20and%20Tarek%20Abdelfatah%20and%20Stephen%20Chan%20and%20Xin%20Chen&entry.1292438233=Aim%3A%20This%20study%20investigates%20treatment%20response%20prediction%20to%20neoadjuvant%20chemotherapy%20%28NACT%29%20in%20breast%20cancer%20patients%2C%20using%20longitudinal%20contrast-enhanced%20magnetic%20resonance%20images%20%28CE-MRI%29%20and%20clinical%20data.%20The%20goal%20is%20to%20develop%20machine%20learning%20%28ML%29%20models%20to%20predict%20pathologic%20complete%20response%20%28PCR%20binary%20classification%29%20and%205-year%20relapse-free%20survival%20status%20%28RFS%20binary%20classification%29.%20Method%3A%20The%20proposed%20framework%20includes%20tumour%20segmentation%2C%20image%20registration%2C%20feature%20extraction%2C%20and%20predictive%20modelling.%20Using%20the%20image%20registration%20method%2C%20MRI%20image%20features%20can%20be%20extracted%20and%20compared%20from%20the%20original%20tumour%20site%20at%20different%20time%20points%2C%20therefore%20monitoring%20the%20intratumor%20changes%20during%20NACT%20process.%20Four%20feature%20extractors%2C%20including%20one%20radiomics%20and%20three%20deep%20learning-based%20%28MedicalNet%2C%20Segformer3D%2C%20SAM-Med3D%29%20were%20implemented%20and%20compared.%20In%20combination%20with%20three%20feature%20selection%20methods%20and%20four%20ML%20models%2C%20predictive%20models%20are%20built%20and%20compared.%20Results%3A%20The%20proposed%20image%20registration-based%20feature%20extraction%20consistently%20improves%20the%20predictive%20models.%20In%20the%20PCR%20and%20RFS%20classification%20tasks%20logistic%20regression%20model%20trained%20on%20radiomic%20features%20performed%20the%20best%20with%20an%20AUC%20of%200.88%20and%20classification%20accuracy%20of%200.85%20for%20PCR%20classification%2C%20and%20AUC%20of%200.78%20and%20classification%20accuracy%20of%200.72%20for%20RFS%20classification.%20Conclusions%3A%20It%20is%20evidenced%20that%20the%20image%20registration%20method%20has%20significantly%20improved%20performance%20in%20longitudinal%20feature%20learning%20in%20predicting%20PCR%20and%20RFS.%20The%20radiomics%20feature%20extractor%20is%20more%20effective%20than%20the%20pre-trained%20deep%20learning%20feature%20extractors%2C%20with%20higher%20performance%20and%20better%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2512.17759v1&entry.124074799=Read"},
{"title": "MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation", "author": "Jon Muhovi\u010d and Janez Per\u0161", "abstract": "Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.", "link": "http://arxiv.org/abs/2512.17450v1", "date": "2025-12-19", "relevancy": 2.2965, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5761}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MULTIAQUA%3A%20A%20multimodal%20maritime%20dataset%20and%20robust%20training%20strategies%20for%20multimodal%20semantic%20segmentation&body=Title%3A%20MULTIAQUA%3A%20A%20multimodal%20maritime%20dataset%20and%20robust%20training%20strategies%20for%20multimodal%20semantic%20segmentation%0AAuthor%3A%20Jon%20Muhovi%C4%8D%20and%20Janez%20Per%C5%A1%0AAbstract%3A%20Unmanned%20surface%20vehicles%20can%20encounter%20a%20number%20of%20varied%20visual%20circumstances%20during%20operation%2C%20some%20of%20which%20can%20be%20very%20difficult%20to%20interpret.%20While%20most%20cases%20can%20be%20solved%20only%20using%20color%20camera%20images%2C%20some%20weather%20and%20lighting%20conditions%20require%20additional%20information.%20To%20expand%20the%20available%20maritime%20data%2C%20we%20present%20a%20novel%20multimodal%20maritime%20dataset%20MULTIAQUA%20%28Multimodal%20Aquatic%20Dataset%29.%20Our%20dataset%20contains%20synchronized%2C%20calibrated%20and%20annotated%20data%20captured%20by%20sensors%20of%20different%20modalities%2C%20such%20as%20RGB%2C%20thermal%2C%20IR%2C%20LIDAR%2C%20etc.%20The%20dataset%20is%20aimed%20at%20developing%20supervised%20methods%20that%20can%20extract%20useful%20information%20from%20these%20modalities%20in%20order%20to%20provide%20a%20high%20quality%20of%20scene%20interpretation%20regardless%20of%20potentially%20poor%20visibility%20conditions.%20To%20illustrate%20the%20benefits%20of%20the%20proposed%20dataset%2C%20we%20evaluate%20several%20multimodal%20methods%20on%20our%20difficult%20nighttime%20test%20set.%20We%20present%20training%20approaches%20that%20enable%20multimodal%20methods%20to%20be%20trained%20in%20a%20more%20robust%20way%2C%20thus%20enabling%20them%20to%20retain%20reliable%20performance%20even%20in%20near-complete%20darkness.%20Our%20approach%20allows%20for%20training%20a%20robust%20deep%20neural%20network%20only%20using%20daytime%20images%2C%20thus%20significantly%20simplifying%20data%20acquisition%2C%20annotation%2C%20and%20the%20training%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMULTIAQUA%253A%2520A%2520multimodal%2520maritime%2520dataset%2520and%2520robust%2520training%2520strategies%2520for%2520multimodal%2520semantic%2520segmentation%26entry.906535625%3DJon%2520Muhovi%25C4%258D%2520and%2520Janez%2520Per%25C5%25A1%26entry.1292438233%3DUnmanned%2520surface%2520vehicles%2520can%2520encounter%2520a%2520number%2520of%2520varied%2520visual%2520circumstances%2520during%2520operation%252C%2520some%2520of%2520which%2520can%2520be%2520very%2520difficult%2520to%2520interpret.%2520While%2520most%2520cases%2520can%2520be%2520solved%2520only%2520using%2520color%2520camera%2520images%252C%2520some%2520weather%2520and%2520lighting%2520conditions%2520require%2520additional%2520information.%2520To%2520expand%2520the%2520available%2520maritime%2520data%252C%2520we%2520present%2520a%2520novel%2520multimodal%2520maritime%2520dataset%2520MULTIAQUA%2520%2528Multimodal%2520Aquatic%2520Dataset%2529.%2520Our%2520dataset%2520contains%2520synchronized%252C%2520calibrated%2520and%2520annotated%2520data%2520captured%2520by%2520sensors%2520of%2520different%2520modalities%252C%2520such%2520as%2520RGB%252C%2520thermal%252C%2520IR%252C%2520LIDAR%252C%2520etc.%2520The%2520dataset%2520is%2520aimed%2520at%2520developing%2520supervised%2520methods%2520that%2520can%2520extract%2520useful%2520information%2520from%2520these%2520modalities%2520in%2520order%2520to%2520provide%2520a%2520high%2520quality%2520of%2520scene%2520interpretation%2520regardless%2520of%2520potentially%2520poor%2520visibility%2520conditions.%2520To%2520illustrate%2520the%2520benefits%2520of%2520the%2520proposed%2520dataset%252C%2520we%2520evaluate%2520several%2520multimodal%2520methods%2520on%2520our%2520difficult%2520nighttime%2520test%2520set.%2520We%2520present%2520training%2520approaches%2520that%2520enable%2520multimodal%2520methods%2520to%2520be%2520trained%2520in%2520a%2520more%2520robust%2520way%252C%2520thus%2520enabling%2520them%2520to%2520retain%2520reliable%2520performance%2520even%2520in%2520near-complete%2520darkness.%2520Our%2520approach%2520allows%2520for%2520training%2520a%2520robust%2520deep%2520neural%2520network%2520only%2520using%2520daytime%2520images%252C%2520thus%2520significantly%2520simplifying%2520data%2520acquisition%252C%2520annotation%252C%2520and%2520the%2520training%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MULTIAQUA%3A%20A%20multimodal%20maritime%20dataset%20and%20robust%20training%20strategies%20for%20multimodal%20semantic%20segmentation&entry.906535625=Jon%20Muhovi%C4%8D%20and%20Janez%20Per%C5%A1&entry.1292438233=Unmanned%20surface%20vehicles%20can%20encounter%20a%20number%20of%20varied%20visual%20circumstances%20during%20operation%2C%20some%20of%20which%20can%20be%20very%20difficult%20to%20interpret.%20While%20most%20cases%20can%20be%20solved%20only%20using%20color%20camera%20images%2C%20some%20weather%20and%20lighting%20conditions%20require%20additional%20information.%20To%20expand%20the%20available%20maritime%20data%2C%20we%20present%20a%20novel%20multimodal%20maritime%20dataset%20MULTIAQUA%20%28Multimodal%20Aquatic%20Dataset%29.%20Our%20dataset%20contains%20synchronized%2C%20calibrated%20and%20annotated%20data%20captured%20by%20sensors%20of%20different%20modalities%2C%20such%20as%20RGB%2C%20thermal%2C%20IR%2C%20LIDAR%2C%20etc.%20The%20dataset%20is%20aimed%20at%20developing%20supervised%20methods%20that%20can%20extract%20useful%20information%20from%20these%20modalities%20in%20order%20to%20provide%20a%20high%20quality%20of%20scene%20interpretation%20regardless%20of%20potentially%20poor%20visibility%20conditions.%20To%20illustrate%20the%20benefits%20of%20the%20proposed%20dataset%2C%20we%20evaluate%20several%20multimodal%20methods%20on%20our%20difficult%20nighttime%20test%20set.%20We%20present%20training%20approaches%20that%20enable%20multimodal%20methods%20to%20be%20trained%20in%20a%20more%20robust%20way%2C%20thus%20enabling%20them%20to%20retain%20reliable%20performance%20even%20in%20near-complete%20darkness.%20Our%20approach%20allows%20for%20training%20a%20robust%20deep%20neural%20network%20only%20using%20daytime%20images%2C%20thus%20significantly%20simplifying%20data%20acquisition%2C%20annotation%2C%20and%20the%20training%20process.&entry.1838667208=http%3A//arxiv.org/abs/2512.17450v1&entry.124074799=Read"},
{"title": "Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks", "author": "Irched Chafaa and Giacomo Bacci and Luca Sanguinetti", "abstract": "Optimal AP clustering and power allocation are critical in user-centric cell-free massive MIMO systems. Existing deep learning models lack flexibility to handle dynamic network configurations. Furthermore, many approaches overlook pilot contamination and suffer from high computational complexity. In this paper, we propose a lightweight transformer model that overcomes these limitations by jointly predicting AP clusters and powers solely from spatial coordinates of user devices and AP. Our model is architecture-agnostic to users load, handles both clustering and power allocation without channel estimation overhead, and eliminates pilot contamination by assigning users to AP within a pilot reuse constraint. We also incorporate a customized linear attention mechanism to capture user-AP interactions efficiently and enable linear scalability with respect to the number of users. Numerical results confirm the model's effectiveness in maximizing the minimum spectral efficiency and providing near-optimal performance while ensuring adaptability and scalability in dynamic scenarios.", "link": "http://arxiv.org/abs/2512.17466v1", "date": "2025-12-19", "relevancy": 2.2955, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4621}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4585}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Attention%20for%20Joint%20Power%20Optimization%20and%20User-Centric%20Clustering%20in%20Cell-Free%20Networks&body=Title%3A%20Linear%20Attention%20for%20Joint%20Power%20Optimization%20and%20User-Centric%20Clustering%20in%20Cell-Free%20Networks%0AAuthor%3A%20Irched%20Chafaa%20and%20Giacomo%20Bacci%20and%20Luca%20Sanguinetti%0AAbstract%3A%20Optimal%20AP%20clustering%20and%20power%20allocation%20are%20critical%20in%20user-centric%20cell-free%20massive%20MIMO%20systems.%20Existing%20deep%20learning%20models%20lack%20flexibility%20to%20handle%20dynamic%20network%20configurations.%20Furthermore%2C%20many%20approaches%20overlook%20pilot%20contamination%20and%20suffer%20from%20high%20computational%20complexity.%20In%20this%20paper%2C%20we%20propose%20a%20lightweight%20transformer%20model%20that%20overcomes%20these%20limitations%20by%20jointly%20predicting%20AP%20clusters%20and%20powers%20solely%20from%20spatial%20coordinates%20of%20user%20devices%20and%20AP.%20Our%20model%20is%20architecture-agnostic%20to%20users%20load%2C%20handles%20both%20clustering%20and%20power%20allocation%20without%20channel%20estimation%20overhead%2C%20and%20eliminates%20pilot%20contamination%20by%20assigning%20users%20to%20AP%20within%20a%20pilot%20reuse%20constraint.%20We%20also%20incorporate%20a%20customized%20linear%20attention%20mechanism%20to%20capture%20user-AP%20interactions%20efficiently%20and%20enable%20linear%20scalability%20with%20respect%20to%20the%20number%20of%20users.%20Numerical%20results%20confirm%20the%20model%27s%20effectiveness%20in%20maximizing%20the%20minimum%20spectral%20efficiency%20and%20providing%20near-optimal%20performance%20while%20ensuring%20adaptability%20and%20scalability%20in%20dynamic%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Attention%2520for%2520Joint%2520Power%2520Optimization%2520and%2520User-Centric%2520Clustering%2520in%2520Cell-Free%2520Networks%26entry.906535625%3DIrched%2520Chafaa%2520and%2520Giacomo%2520Bacci%2520and%2520Luca%2520Sanguinetti%26entry.1292438233%3DOptimal%2520AP%2520clustering%2520and%2520power%2520allocation%2520are%2520critical%2520in%2520user-centric%2520cell-free%2520massive%2520MIMO%2520systems.%2520Existing%2520deep%2520learning%2520models%2520lack%2520flexibility%2520to%2520handle%2520dynamic%2520network%2520configurations.%2520Furthermore%252C%2520many%2520approaches%2520overlook%2520pilot%2520contamination%2520and%2520suffer%2520from%2520high%2520computational%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520lightweight%2520transformer%2520model%2520that%2520overcomes%2520these%2520limitations%2520by%2520jointly%2520predicting%2520AP%2520clusters%2520and%2520powers%2520solely%2520from%2520spatial%2520coordinates%2520of%2520user%2520devices%2520and%2520AP.%2520Our%2520model%2520is%2520architecture-agnostic%2520to%2520users%2520load%252C%2520handles%2520both%2520clustering%2520and%2520power%2520allocation%2520without%2520channel%2520estimation%2520overhead%252C%2520and%2520eliminates%2520pilot%2520contamination%2520by%2520assigning%2520users%2520to%2520AP%2520within%2520a%2520pilot%2520reuse%2520constraint.%2520We%2520also%2520incorporate%2520a%2520customized%2520linear%2520attention%2520mechanism%2520to%2520capture%2520user-AP%2520interactions%2520efficiently%2520and%2520enable%2520linear%2520scalability%2520with%2520respect%2520to%2520the%2520number%2520of%2520users.%2520Numerical%2520results%2520confirm%2520the%2520model%2527s%2520effectiveness%2520in%2520maximizing%2520the%2520minimum%2520spectral%2520efficiency%2520and%2520providing%2520near-optimal%2520performance%2520while%2520ensuring%2520adaptability%2520and%2520scalability%2520in%2520dynamic%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Attention%20for%20Joint%20Power%20Optimization%20and%20User-Centric%20Clustering%20in%20Cell-Free%20Networks&entry.906535625=Irched%20Chafaa%20and%20Giacomo%20Bacci%20and%20Luca%20Sanguinetti&entry.1292438233=Optimal%20AP%20clustering%20and%20power%20allocation%20are%20critical%20in%20user-centric%20cell-free%20massive%20MIMO%20systems.%20Existing%20deep%20learning%20models%20lack%20flexibility%20to%20handle%20dynamic%20network%20configurations.%20Furthermore%2C%20many%20approaches%20overlook%20pilot%20contamination%20and%20suffer%20from%20high%20computational%20complexity.%20In%20this%20paper%2C%20we%20propose%20a%20lightweight%20transformer%20model%20that%20overcomes%20these%20limitations%20by%20jointly%20predicting%20AP%20clusters%20and%20powers%20solely%20from%20spatial%20coordinates%20of%20user%20devices%20and%20AP.%20Our%20model%20is%20architecture-agnostic%20to%20users%20load%2C%20handles%20both%20clustering%20and%20power%20allocation%20without%20channel%20estimation%20overhead%2C%20and%20eliminates%20pilot%20contamination%20by%20assigning%20users%20to%20AP%20within%20a%20pilot%20reuse%20constraint.%20We%20also%20incorporate%20a%20customized%20linear%20attention%20mechanism%20to%20capture%20user-AP%20interactions%20efficiently%20and%20enable%20linear%20scalability%20with%20respect%20to%20the%20number%20of%20users.%20Numerical%20results%20confirm%20the%20model%27s%20effectiveness%20in%20maximizing%20the%20minimum%20spectral%20efficiency%20and%20providing%20near-optimal%20performance%20while%20ensuring%20adaptability%20and%20scalability%20in%20dynamic%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.17466v1&entry.124074799=Read"},
{"title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities", "author": "Gheorghe Comanici and Eric Bieber and Mike Schaekermann and Ice Pasupat and Noveen Sachdeva and Inderjit Dhillon and Marcel Blistein and Ori Ram and Dan Zhang and Evan Rosen and Luke Marris and Sam Petulla and Colin Gaffney and Asaf Aharoni and Nathan Lintz and Tiago Cardal Pais and Henrik Jacobsson and Idan Szpektor and Nan-Jiang Jiang and Krishna Haridasan and Ahmed Omran and Nikunj Saunshi and Dara Bahri and Gaurav Mishra and Eric Chu and Toby Boyd and Brad Hekman and Aaron Parisi and Chaoyi Zhang and Kornraphop Kawintiranon and Tania Bedrax-Weiss and Oliver Wang and Ya Xu and Ollie Purkiss and Uri Mendlovic and Ila\u00ef Deutel and Nam Nguyen and Adam Langley and Flip Korn and Lucia Rossazza and Alexandre Ram\u00e9 and Sagar Waghmare and Helen Miller and Nathan Byrd and Ashrith Sheshan and Raia Hadsell and Sangnie Bhardwaj and Pawel Janus and Tero Rissa and Dan Horgan and Alvin Abdagic and Lior Belenki and James Allingham and Anima Singh and Theo Guidroz and Srivatsan Srinivasan and Herman Schmit and Kristen Chiafullo and Andre Elisseeff and Nilpa Jha and Prateek Kolhar and Leonard Berrada and Frank Ding and Xiance Si and Shrestha Basu Mallick and Franz Och and Sofia Erell and Eric Ni and Tejasi Latkar and Sherry Yang and Petar Sirkovic and Ziqiang Feng and Robert Leland and Rachel Hornung and Gang Wu and Charles Blundell and Hamidreza Alvari and Po-Sen Huang and Cathy Yip and Sanja Deur and Li Liu and Gabriela Surita and Pablo Duque and Dima Damen and Johnson Jia and Arthur Guez and Markus Mircea and Animesh Sinha and Alberto Magni and Pawe\u0142 Stradomski and Tal Marian and Vlado Gali\u0107 and Wenhu Chen and Hisham Husain and Achintya Singhal and Dominik Grewe and Fran\u00e7ois-Xavier Aubet and Shuang Song and Lorenzo Blanco and Leland Rechis and Lewis Ho and Rich Munoz and Kelvin Zheng and Jessica Hamrick and Kevin Mather and Hagai Taitelbaum and Eliza Rutherford and Yun Lei and Kuangyuan Chen and Anand Shukla and Erica Moreira and Eric Doi and Berivan Isik and Nir Shabat and Dominika Rogozi\u0144ska and Kashyap Kolipaka and Jason Chang and Eugen Vu\u0161ak and Srinivasan Venkatachary and Shadi Noghabi and Tarun Bharti and Younghoon Jun and Aleksandr Zaks and Simon Green and Jeshwanth Challagundla and William Wong and Muqthar Mohammad and Dean Hirsch and Yong Cheng and Iftekhar Naim and Lev Proleev and Damien Vincent and Aayush Singh and Maxim Krikun and Dilip Krishnan and Zoubin Ghahramani and Aviel Atias and Rajeev Aggarwal and Christo Kirov and Dimitrios Vytiniotis and Christy Koh and Alexandra Chronopoulou and Pawan Dogra and Vlad-Doru Ion and Gladys Tyen and Jason Lee and Felix Weissenberger and Trevor Strohman and Ashwin Balakrishna and Jack Rae and Marko Velic and Raoul de Liedekerke and Oded Elyada and Wentao Yuan and Canoee Liu and Lior Shani and Sergey Kishchenko and Bea Alessio and Yandong Li and Richard Song and Sam Kwei and Orion Jankowski and Aneesh Pappu and Youhei Namiki and Yenai Ma and Nilesh Tripuraneni and Colin Cherry and Marissa Ikonomidis and Yu-Cheng Ling and Colin Ji and Beka Westberg and Auriel Wright and Da Yu and David Parkinson and Swaroop Ramaswamy and Jerome Connor and Soheil Hassas Yeganeh and Snchit Grover and George Kenwright and Lubo Litchev and Chris Apps and Alex Tomala and Felix Halim and Alex Castro-Ros and Zefei Li and Anudhyan Boral and Pauline Sho and Michal Yarom and Eric Malmi and David Klinghoffer and Rebecca Lin and Alan Ansell and Pradeep Kumar S and Shubin Zhao and Siqi Zuo and Adam Santoro and Heng-Tze Cheng and Solomon Demmessie and Yuchi Liu and Nicole Brichtova and Allie Culp and Nathaniel Braun and Dan Graur and Will Ng and Nikhil Mehta and Aaron Phillips and Patrik Sundberg and Varun Godbole and Fangyu Liu and Yash Katariya and David Rim and Mojtaba Seyedhosseini and Sean Ammirati and Jonas Valfridsson and Mahan Malihi and Timothy Knight and Andeep Toor and Thomas Lampe and Abe Ittycheriah and Lewis Chiang and Chak Yeung and Alexandre Fr\u00e9chette and Jinmeng Rao and Huisheng Wang and Himanshu Srivastava and Richard Zhang and Rocky Rhodes and Ariel Brand and Dean Weesner and Ilya Figotin and Felix Gimeno and Rachana Fellinger and Pierre Marcenac and Jos\u00e9 Leal and Eyal Marcus and Victor Cotruta and Rodrigo Cabrera and Sheryl Luo and Dan Garrette and Vera Axelrod and Sorin Baltateanu and David Barker and Dongkai Chen and Horia Toma and Ben Ingram and Jason Riesa and Chinmay Kulkarni and Yujing Zhang and Hongbin Liu and Chao Wang and Martin Polacek and Will Wu and Kai Hui and Adrian N Reyes and Yi Su and Megan Barnes and Ishaan Malhi and Anfal Siddiqui and Qixuan Feng and Mihai Damaschin and Daniele Pighin and Andreas Steiner and Samuel Yang and Ramya Sree Boppana and Simeon Ivanov and Arun Kandoor and Aditya Shah and Asier Mujika and Da Huang and Christopher A. Choquette-Choo and Mohak Patel and Tianhe Yu and Toni Creswell and  Jerry and  Liu and Catarina Barros and Yasaman Razeghi and Aurko Roy and Phil Culliton and Binbin Xiong and Jiaqi Pan and Thomas Strohmann and Tolly Powell and Babi Seal and Doug DeCarlo and Pranav Shyam and Kaan Katircioglu and Xuezhi Wang and Cassidy Hardin and Immanuel Odisho and Josef Broder and Oscar Chang and Arun Nair and Artem Shtefan and Maura O'Brien and Manu Agarwal and Sahitya Potluri and Siddharth Goyal and Amit Jhindal and Saksham Thakur and Yury Stuken and James Lyon and Kristina Toutanova and Fangxiaoyu Feng and Austin Wu and Ben Horn and Alek Wang and Alex Cullum and Gabe Taubman and Disha Shrivastava and Chongyang Shi and Hamish Tomlinson and Roma Patel and Tao Tu and Ada Maksutaj Oflazer and Francesco Pongetti and Mingyao Yang and Adrien Ali Ta\u00efga and Vincent Perot and Nuo Wang Pierse and Feng Han and Yoel Drori and I\u00f1aki Iturrate and Ayan Chakrabarti and Legg Yeung and Dave Dopson and Yi-ting Chen and Apoorv Kulshreshtha and Tongfei Guo and Philip Pham and Tal Schuster and Junquan Chen and Alex Polozov and Jinwei Xing and Huanjie Zhou and Praneeth Kacham and Doron Kukliansky and Antoine Miech and Sergey Yaroshenko and Ed Chi and Sholto Douglas and Hongliang Fei and Mathieu Blondel and Preethi Myla and Lior Madmoni and Xing Wu and Daniel Keysers and Kristian Kjems and Isabela Albuquerque and Lijun Yu and Joel D'sa and Michelle Plantan and Vlad Ionescu and Jaume Sanchez Elias and Abhirut Gupta and Manish Reddy Vuyyuru and Fred Alcober and Tong Zhou and Kaiyang Ji and Florian Hartmann and Subha Puttagunta and Hugo Song and Ehsan Amid and Anca Stefanoiu and Andrew Lee and Paul Pucciarelli and Emma Wang and Amit Raul and Slav Petrov and Isaac Tian and Valentin Anklin and Nana Nti and Victor Gomes and Max Schumacher and Grace Vesom and Alex Panagopoulos and Konstantinos Bousmalis and Daniel Andor and Josh Jacob and Yuan Zhang and Bill Rosgen and Matija Kecman and Matthew Tung and Alexandra Belias and Noah Goodman and Paul Covington and Brian Wieder and Nikita Saxena and Elnaz Davoodi and Muhuan Huang and Sharath Maddineni and Vincent Roulet and Folawiyo Campbell-Ajala and Pier Giuseppe Sessa and  Xintian and  Wu and Guangda Lai and Paul Collins and Alex Haig and Vytenis Sakenas and Xiaowei Xu and Marissa Giustina and Laurent El Shafey and Pichi Charoenpanit and Shefali Garg and Joshua Ainslie and Boone Severson and Montse Gonzalez Arenas and Shreya Pathak and Sujee Rajayogam and Jie Feng and Michiel Bakker and Sheng Li and Nevan Wichers and Jamie Rogers and Xinyang Geng and Yeqing Li and Rolf Jagerman and Chao Jia and Nadav Olmert and David Sharon and Matthew Mauger and Sandeep Mariserla and Hongxu Ma and Megha Mohabey and Kyuyeun Kim and Alek Andreev and Scott Pollom and Juliette Love and Vihan Jain and Priyanka Agrawal and Yannick Schroecker and Alisa Fortin and Manfred Warmuth and Ji Liu and Andrew Leach and Irina Blok and Ganesh Poomal Girirajan and Roee Aharoni and Benigno Uria and Andrei Sozanschi and Dan Goldberg and Lucian Ionita and Marco Tulio Ribeiro and Martin Zlocha and Vighnesh Birodkar and Sami Lachgar and Liangzhe Yuan and Himadri Choudhury and Matt Ginsberg and Fei Zheng and Gregory Dibb and Emily Graves and Swachhand Lokhande and Gabriel Rasskin and George-Cristian Muraru and Corbin Quick and Sandeep Tata and Pierre Sermanet and Aditya Chawla and Itay Karo and Yan Wang and Susan Zhang and Orgad Keller and Anca Dragan and Guolong Su and Ian Chou and Xi Liu and Yiqing Tao and Shruthi Prabhakara and Marc Wilson and Ruibo Liu and Shibo Wang and Georgie Evans and David Du and Alfonso Casta\u00f1o and Gautam Prasad and Mona El Mahdy and Sebastian Gerlach and Machel Reid and Jarrod Kahn and Amir Zait and Thanumalayan Sankaranarayana Pillai and Thatcher Ulrich and Guanyu Wang and Jan Wassenberg and Efrat Farkash and Kiran Yalasangi and Congchao Wang and Maria Bauza and Simon Bucher and Ting Liu and Jun Yan and Gary Leung and Vikas Sindhwani and Parker Barnes and Avi Singh and Ivan Jurin and Jichuan Chang and Niket Kumar Bhumihar and Sivan Eiger and Gui Citovsky and Ben Withbroe and Zhang Li and Siyang Xue and Niccol\u00f2 Dal Santo and Georgi Stoyanov and Yves Raimond and Steven Zheng and Yilin Gao and V\u00edt List\u00edk and S\u0142awek Kwasiborski and Rachel Saputro and Adnan Ozturel and Ganesh Mallya and Kushal Majmundar and Ross West and Paul Caron and Jinliang Wei and Lluis Castrejon and Sharad Vikram and Deepak Ramachandran and Nikhil Dhawan and Jiho Park and Sara Smoot and George van den Driessche and Yochai Blau and Chase Malik and Wei Liang and Roy Hirsch and Cicero Nogueira dos Santos and Eugene Weinstein and A\u00e4ron van den Oord and Sid Lall and Nicholas FitzGerald and Zixuan Jiang and Xuan Yang and Dale Webster and Ali Elqursh and Aedan Pope and Georges Rotival and David Raposo and Wanzheng Zhu and Jeff Dean and Sami Alabed and Dustin Tran and Arushi Gupta and Zach Gleicher and Jessica Austin and Edouard Rosseel and Megh Umekar and Dipanjan Das and Yinghao Sun and Kai Chen and Karolis Misiunas and Xiang Zhou and Yixian Di and Alyssa Loo and Josh Newlan and Bo Li and Vinay Ramasesh and Ying Xu and Alex Chen and Sudeep Gandhe and Radu Soricut and Nikita Gupta and Shuguang Hu and Seliem El-Sayed and Xavier Garcia and Idan Brusilovsky and Pu-Chin Chen and Andrew Bolt and Lu Huang and Alex Gurney and Zhiying Zhang and Alexander Pritzel and Jarek Wilkiewicz and Bryan Seybold and Bhargav Kanagal Shamanna and Felix Fischer and Josef Dean and Karan Gill and Ross Mcilroy and Abhishek Bhowmick and Jeremy Selier and Antoine Yang and Derek Cheng and Vladimir Magay and Jie Tan and Dhriti Varma and Christian Walder and Tomas Kocisky and Ryo Nakashima and Paul Natsev and Mike Kwong and Ionel Gog and Chiyuan Zhang and Sander Dieleman and Thomas Jimma and Andrey Ryabtsev and Siddhartha Brahma and David Steiner and Dayou Du and Ante \u017du\u017eul and Mislav \u017dani\u0107 and Mukund Raghavachari and Willi Gierke and Zeyu Zheng and Dessie Petrova and Yann Dauphin and Yuchuan Liu and Ido Kessler and Steven Hand and Chris Duvarney and Seokhwan Kim and Hyo Lee and L\u00e9onard Hussenot and Jeffrey Hui and Josh Smith and Deepali Jain and Jiawei Xia and Gaurav Singh Tomar and Keyvan Amiri and Du Phan and Fabian Fuchs and Tobias Weyand and Nenad Tomasev and Alexandra Cordell and Xin Liu and Jonathan Mallinson and Pankaj Joshi and Andy Crawford and Arun Suggala and Steve Chien and Nick Fernando and Mariella Sanchez-Vargas and Duncan Williams and Phil Crone and Xiyang Luo and Igor Karpov and Jyn Shan and Terry Thurk and Robin Strudel and Paul Voigtlaender and Piyush Patil and Tim Dozat and Ali Khodaei and Sahil Singla and Piotr Ambroszczyk and Qiyin Wu and Yifan Chang and Brian Roark and Chaitra Hegde and Tianli Ding and Angelos Filos and Zhongru Wu and Andr\u00e9 Susano Pinto and Shuang Liu and Saarthak Khanna and Aditya Pandey and Siobhan Mcloughlin and Qiujia Li and Sam Haves and Allan Zhou and Elena Buchatskaya and Isabel Leal and Peter de Boursac and Nami Akazawa and Nina Anderson and Terry Chen and Krishna Somandepalli and Chen Liang and Sheela Goenka and Stephanie Winkler and Alexander Grushetsky and Yifan Ding and Jamie Smith and Fan Ye and Jordi Pont-Tuset and Eric Li and Ruichao Li and Tomer Golany and Dawid Wegner and Tao Jiang and Omer Barak and Yuan Shangguan and Eszter V\u00e9rtes and Renee Wong and J\u00f6rg Bornschein and Alex Tudor and Michele Bevilacqua and Tom Schaul and Ankit Singh Rawat and Yang Zhao and Kyriakos Axiotis and Lei Meng and Cory McLean and Jonathan Lai and Jennifer Beattie and Nate Kushman and Yaxin Liu and Blair Kutzman and Fiona Lang and Jingchen Ye and Praneeth Netrapalli and Pushkar Mishra and Myriam Khan and Megha Goel and Rob Willoughby and David Tian and Honglei Zhuang and JD Chen and Zak Tsai and Tasos Kementsietsidis and Arjun Khare and James Keeling and Keyang Xu and Nathan Waters and Florent Altch\u00e9 and Ashok Popat and Bhavishya Mittal and David Saxton and Dalia El Badawy and Michael Mathieu and Zheng Zheng and Hao Zhou and Nishant Ranka and Richard Shin and Qingnan Duan and Tim Salimans and Ioana Mihailescu and Uri Shaham and Ming-Wei Chang and Yannis Assael and Nishanth Dikkala and Martin Izzard and Vincent Cohen-Addad and Cat Graves and Vlad Feinberg and Grace Chung and DJ Strouse and Danny Karmon and Sahand Sharifzadeh and Zoe Ashwood and Khiem Pham and Jon Blanton and Alex Vasiloff and Jarred Barber and Mark Geller and Aurick Zhou and Fedir Zubach and Tzu-Kuo Huang and Lei Zhang and Himanshu Gupta and Matt Young and Julia Proskurnia and Ronny Votel and Valentin Gabeur and Gabriel Barcik and Aditya Tripathi and Hongkun Yu and Geng Yan and Beer Changpinyo and Filip Paveti\u0107 and Amy Coyle and Yasuhisa Fujii and Jorge Gonzalez Mendez and Tianhao Zhou and Harish Rajamani and Blake Hechtman and Eddie Cao and Da-Cheng Juan and Yi-Xuan Tan and Valentin Dalibard and Yilun Du and Natalie Clay and Kaisheng Yao and Wenhao Jia and Dimple Vijaykumar and Yuxiang Zhou and Xinyi Bai and Wei-Chih Hung and Steven Pecht and Georgi Todorov and Nikhil Khadke and Pramod Gupta and Preethi Lahoti and Arnaud Autef and Karthik Duddu and James Lee-Thorp and Alexander Bykovsky and Tautvydas Misiunas and Sebastian Flennerhag and Santhosh Thangaraj and Jed McGiffin and Zack Nado and Markus Kunesch and Andreas Noever and Amir Hertz and Marco Liang and Victor Stone and Evan Palmer and Samira Daruki and Arijit Pramanik and Siim P\u00f5der and Austin Kyker and Mina Khan and Evgeny Sluzhaev and Marvin Ritter and Avraham Ruderman and Wenlei Zhou and Chirag Nagpal and Kiran Vodrahalli and George Necula and Paul Barham and Ellie Pavlick and Jay Hartford and Izhak Shafran and Long Zhao and Maciej Miku\u0142a and Tom Eccles and Hidetoshi Shimokawa and Kanav Garg and Luke Vilnis and Hanwen Chen and Ilia Shumailov and Kuang-Huei Lee and Abdelrahman Abdelhamed and Meiyan Xie and Vered Cohen and Ester Hlavnova and Dan Malkin and Chawin Sitawarin and James Lottes and Pauline Coquinot and Tianli Yu and Sandeep Kumar and Jingwei Zhang and Aroma Mahendru and Zafarali Ahmed and James Martens and Tao Chen and Aviel Boag and Daiyi Peng and Coline Devin and Arseniy Klimovskiy and Mary Phuong and Danny Vainstein and Jin Xie and Bhuvana Ramabhadran and Nathan Howard and Xinxin Yu and Gitartha Goswami and Jingyu Cui and Sam Shleifer and Mario Pinto and Chih-Kuan Yeh and Ming-Hsuan Yang and Sara Javanmardi and Dan Ethier and Chace Lee and Jordi Orbay and Suyog Kotecha and Carla Bromberg and Pete Shaw and James Thornton and Adi Gerzi Rosenthal and Shane Gu and Matt Thomas and Ian Gemp and Aditya Ayyar and Asahi Ushio and Aarush Selvan and Joel Wee and Chenxi Liu and Maryam Majzoubi and Weiren Yu and Jake Abernethy and Tyler Liechty and Renke Pan and Hoang Nguyen and  Qiong and  Hu and Sarah Perrin and Abhinav Arora and Emily Pitler and Weiyi Wang and Kaushik Shivakumar and Flavien Prost and Ben Limonchik and Jing Wang and Yi Gao and Timothee Cour and Shyamal Buch and Huan Gui and Maria Ivanova and Philipp Neubeck and Kelvin Chan and Lucy Kim and Huizhong Chen and Naman Goyal and Da-Woon Chung and Lu Liu and Yao Su and Anastasia Petrushkina and Jiajun Shen and Armand Joulin and Yuanzhong Xu and Stein Xudong Lin and Yana Kulizhskaya and Ciprian Chelba and Shobha Vasudevan and Eli Collins and Vasilisa Bashlovkina and Tony Lu and Doug Fritz and Jongbin Park and Yanqi Zhou and Chen Su and Richard Tanburn and Mikhail Sushkov and Mitchelle Rasquinha and Jinning Li and Jennifer Prendki and Yiming Li and Pallavi LV and Shriya Sharma and Hen Fitoussi and Hui Huang and Andrew Dai and Phuong Dao and Mike Burrows and Henry Prior and Danfeng Qin and Golan Pundak and Lars Lowe Sjoesund and Art Khurshudov and Zhenkai Zhu and Albert Webson and Elizabeth Kemp and Tat Tan and Saurabh Agrawal and Susie Sargsyan and Liqun Cheng and Jim Stephan and Tom Kwiatkowski and David Reid and Arunkumar Byravan and Assaf Hurwitz Michaely and Nicolas Heess and Luowei Zhou and Sonam Goenka and Viral Carpenter and Anselm Levskaya and Bo Wang and Reed Roberts and R\u00e9mi Leblond and Sharat Chikkerur and Stav Ginzburg and Max Chang and Robert Riachi and  Chuqiao and  Xu and Zal\u00e1n Borsos and Michael Pliskin and Julia Pawar and Morgane Lustman and Hannah Kirkwood and Ankit Anand and Aditi Chaudhary and Norbert Kalb and Kieran Milan and Sean Augenstein and Anna Goldie and Laurel Prince and Karthik Raman and Yanhua Sun and Vivian Xia and Aaron Cohen and Zhouyuan Huo and Josh Camp and Seher Ellis and Lukas Zilka and David Vilar Torres and Lisa Patel and Sho Arora and Betty Chan and Jonas Adler and Kareem Ayoub and Jacky Liang and Fayaz Jamil and Jiepu Jiang and Simon Baumgartner and Haitian Sun and Yael Karov and Yaroslav Akulov and Hui Zheng and Irene Cai and Claudio Fantacci and James Rubin and Alex Rav Acha and Mengchao Wang and Nina D'Souza and Rohit Sathyanarayana and Shengyang Dai and Simon Rowe and Andrey Simanovsky and Omer Goldman and Yuheng Kuang and Xiaoyue Pan and Andrew Rosenberg and Tania Rojas-Esponda and Praneet Dutta and Amy Zeng and Irina Jurenka and Greg Farquhar and Yamini Bansal and Shariq Iqbal and Becca Roelofs and Ga-Young Joung and Parker Beak and Changwan Ryu and Ryan Poplin and Yan Wu and Jean-Baptiste Alayrac and Senaka Buthpitiya and Olaf Ronneberger and Caleb Habtegebriel and Wei Li and Paul Cavallaro and Aurora Wei and Guy Bensky and Timo Denk and Harish Ganapathy and Jeff Stanway and Pratik Joshi and Francesco Bertolini and Jessica Lo and Olivia Ma and Zachary Charles and Geta Sampemane and Himanshu Sahni and Xu Chen and Harry Askham and David Gaddy and Peter Young and Jiewen Tan and Matan Eyal and Arthur Bra\u017einskas and Li Zhong and Zhichun Wu and Mark Epstein and Kai Bailey and Andrew Hard and Kamyu Lee and Sasha Goldshtein and Alex Ruiz and Mohammed Badawi and Matthias Lochbrunner and JK Kearns and Ashley Brown and Fabio Pardo and Theophane Weber and Haichuan Yang and Pan-Pan Jiang and Berkin Akin and Zhao Fu and Marcus Wainwright and Chi Zou and Meenu Gaba and Pierre-Antoine Manzagol and Wendy Kan and Yang Song and Karina Zainullina and Rui Lin and Jeongwoo Ko and Salil Deshmukh and Apoorv Jindal and James Svensson and Divya Tyam and Heri Zhao and Christine Kaeser-Chen and Scott Baird and Pooya Moradi and Jamie Hall and Qiuchen Guo and Vincent Tsang and Bowen Liang and Fernando Pereira and Suhas Ganesh and Ivan Korotkov and Jakub Adamek and Sridhar Thiagarajan and Vinh Tran and Charles Chen and Chris Tar and Sanil Jain and Ishita Dasgupta and Taylan Bilal and David Reitter and Kai Zhao and Giulia Vezzani and Yasmin Gehman and Pulkit Mehta and Lauren Beltrone and Xerxes Dotiwalla and Sergio Guadarrama and Zaheer Abbas and Stefani Karp and Petko Georgiev and Chun-Sung Ferng and Marc Brockschmidt and Liqian Peng and Christoph Hirnschall and Vikas Verma and Yingying Bi and Ying Xiao and Avigail Dabush and Kelvin Xu and Phil Wallis and Randall Parker and Qifei Wang and Yang Xu and Ilkin Safarli and Dinesh Tewari and Yin Zhang and Seungyeon Kim and Andrea Gesmundo and Mackenzie Thomas and Sergey Levi and Ahmed Chowdhury and Kanishka Rao and Peter Garst and Sam Conway-Rahman and Helen Ran and Kay McKinney and Zhisheng Xiao and Wenhao Yu and Rohan Agrawal and Axel Stjerngren and Catalin Ionescu and Jingjing Chen and Vivek Sharma and Justin Chiu and Fei Liu and Ken Franko and Clayton Sanford and Xingyu Cai and Paul Michel and Sanjay Ganapathy and Jane Labanowski and Zachary Garrett and Ben Vargas and Sean Sun and Bryan Gale and Thomas Buschmann and Guillaume Desjardins and Nimesh Ghelani and Palak Jain and Mudit Verma and Chulayuth Asawaroengchai and Julian Eisenschlos and Jitendra Harlalka and Hideto Kazawa and Don Metzler and Joshua Howland and Ying Jian and Jake Ades and Viral Shah and Tynan Gangwani and Seungji Lee and Roman Ring and Steven M. Hernandez and Dean Reich and Amer Sinha and Ashutosh Sathe and Joe Kovac and Ashleah Gill and Ajay Kannan and Andrea D'olimpio and Martin Sevenich and Jay Whang and Been Kim and Khe Chai Sim and Jilin Chen and Jiageng Zhang and Shuba Lall and Yossi Matias and Bill Jia and Abe Friesen and Sara Nasso and Ashish Thapliyal and Bryan Perozzi and Ting Yu and Anna Shekhawat and Safeen Huda and Peter Grabowski and Eric Wang and Ashwin Sreevatsa and Hilal Dib and Mehadi Hassen and Parker Schuh and Vedrana Milutinovic and Chris Welty and Michael Quinn and Ali Shah and Bangju Wang and Gabe Barth-Maron and Justin Frye and Natalie Axelsson and Tao Zhu and Yukun Ma and Irene Giannoumis and Hanie Sedghi and Chang Ye and Yi Luan and Kevin Aydin and Bilva Chandra and Vivek Sampathkumar and Ronny Huang and Victor Lavrenko and Ahmed Eleryan and Zhi Hong and Steven Hansen and Sara Mc Carthy and Bidisha Samanta and Domagoj \u0106evid and Xin Wang and Fangtao Li and Michael Voznesensky and Matt Hoffman and Andreas Terzis and Vikash Sehwag and Gil Fidel and Luheng He and Mu Cai and Yanzhang He and Alex Feng and Martin Nikoltchev and Samrat Phatale and Jason Chase and Rory Lawton and Ming Zhang and Tom Ouyang and Manuel Tragut and Mehdi Hafezi Manshadi and Arjun Narayanan and Jiaming Shen and Xu Gao and Tolga Bolukbasi and Nick Roy and Xin Li and Daniel Golovin and Liviu Panait and Zhen Qin and Guangxing Han and Thomas Anthony and Sneha Kudugunta and Viorica Patraucean and Aniket Ray and Xinyun Chen and Xiaochen Yang and Tanuj Bhatia and Pranav Talluri and Alex Morris and Andrija Ra\u017enatovi\u0107 and Bethanie Brownfield and James An and Sheng Peng and Patrick Kane and Ce Zheng and Nico Duduta and Joshua Kessinger and James Noraky and Siqi Liu and Keran Rong and Petar Veli\u010dkovi\u0107 and Keith Rush and Alex Goldin and Fanny Wei and Shiva Mohan Reddy Garlapati and Caroline Pantofaru and Okwan Kwon and Jianmo Ni and Eric Noland and Julia Di Trapani and Fran\u00e7oise Beaufays and Abhijit Guha Roy and Yinlam Chow and Aybuke Turker and Geoffrey Cideron and Lantao Mei and Jon Clark and Qingyun Dou and Matko Bo\u0161njak and Ralph Leith and Yuqing Du and Amir Yazdanbakhsh and Milad Nasr and Chester Kwak and Suraj Satishkumar Sheth and Alex Kaskasoli and Ankesh Anand and Balaji Lakshminarayanan and Sammy Jerome and David Bieber and Chun-Te Chu and Alexandre Senges and Tianxiao Shen and Mukund Sridhar and Ndaba Ndebele and Benjamin Beyret and Shakir Mohamed and Mia Chen and Markus Freitag and Jiaxian Guo and Luyang Liu and Paul Roit and Heng Chen and Shen Yan and Tom Stone and JD Co-Reyes and Jeremy Cole and Salvatore Scellato and Shekoofeh Azizi and Hadi Hashemi and Alicia Jin and Anand Iyer and Marcella Valentine and Andr\u00e1s Gy\u00f6rgy and Arun Ahuja and Daniel Hernandez Diaz and Chen-Yu Lee and Nathan Clement and Weize Kong and Drew Garmon and Ishaan Watts and Kush Bhatia and Khyatti Gupta and Matt Miecnikowski and Hugo Vallet and Ankur Taly and Edward Loper and Saket Joshi and James Atwood and Jo Chick and Mark Collier and Fotis Iliopoulos and Ryan Trostle and Beliz Gunel and Ramiro Leal-Cavazos and Arnar Mar Hrafnkelsson and Michael Guzman and Xiaoen Ju and Andy Forbes and Jesse Emond and Kushal Chauhan and Ben Caine and Li Xiao and Wenjun Zeng and Alexandre Moufarek and Daniel Murphy and Maya Meng and Nitish Gupta and Felix Riedel and Anil Das and Elijah Lawal and Shashi Narayan and Tiberiu Sosea and James Swirhun and Linda Friso and Behnam Neyshabur and Jing Lu and Sertan Girgin and Michael Wunder and Edouard Yvinec and Aroonalok Pyne and Victor Carbune and Shruti Rijhwani and Yang Guo and Tulsee Doshi and Anton Briukhov and Max Bain and Ayal Hitron and Xuanhui Wang and Ashish Gupta and Ke Chen and Cosmo Du and Weiyang Zhang and Dhruv Shah and Arjun Akula and Max Dylla and Ashyana Kachra and Weicheng Kuo and Tingting Zou and Lily Wang and Luyao Xu and Jifan Zhu and Justin Snyder and Sachit Menon and Orhan Firat and Igor Mordatch and Yuan Yuan and Natalia Ponomareva and Rory Blevins and Lawrence Moore and Weijun Wang and Phil Chen and Martin Scholz and Artur Dwornik and Jason Lin and Sicheng Li and Diego Antognini and Te I and Xiaodan Song and Matt Miller and Uday Kalra and Adam Raveret and Oscar Akerlund and Felix Wu and Andrew Nystrom and Namrata Godbole and Tianqi Liu and Hannah DeBalsi and Jewel Zhao and Buhuang Liu and Avi Caciularu and Lauren Lax and Urvashi Khandelwal and Victoria Langston and Eric Bailey and Silvio Lattanzi and Yufei Wang and Neel Kovelamudi and Sneha Mondal and Guru Guruganesh and Nan Hua and Ofir Roval and Pawe\u0142 Weso\u0142owski and Rishikesh Ingale and Jonathan Halcrow and Tim Sohn and Christof Angermueller and Bahram Raad and Eli Stickgold and Eva Lu and Alec Kosik and Jing Xie and Timothy Lillicrap and Austin Huang and Lydia Lihui Zhang and Dominik Paulus and Clement Farabet and Alex Wertheim and Bing Wang and Rishabh Joshi and Chu-ling Ko and Yonghui Wu and Shubham Agrawal and Lily Lin and XiangHai Sheng and Peter Sung and Tyler Breland-King and Christina Butterfield and Swapnil Gawde and Sumeet Singh and Qiao Zhang and Raj Apte and Shilpa Shetty and Adrian Hutter and Tao Li and Elizabeth Salesky and Federico Lebron and Jonni Kanerva and Michela Paganini and Arthur Nguyen and Rohith Vallu and Jan-Thorsten Peter and Sarmishta Velury and David Kao and Jay Hoover and Anna Bortsova and Colton Bishop and Shoshana Jakobovits and Alessandro Agostini and Alekh Agarwal and Chang Liu and Charles Kwong and Sasan Tavakkol and Ioana Bica and Alex Greve and Anirudh GP and Jake Marcus and Le Hou and Tom Duerig and Rivka Moroshko and Dave Lacey and Andy Davis and Julien Amelot and Guohui Wang and Frank Kim and Theofilos Strinopoulos and Hui Wan and Charline Le Lan and Shankar Krishnan and Haotian Tang and Peter Humphreys and Junwen Bai and Idan Heimlich Shtacher and Diego Machado and Chenxi Pang and Ken Burke and Dangyi Liu and Renga Aravamudhan and Yue Song and Ed Hirst and Abhimanyu Singh and Brendan Jou and Liang Bai and Francesco Piccinno and Chuyuan Kelly Fu and Robin Alazard and Barak Meiri and Daniel Winter and Charlie Chen and Mingda Zhang and Jens Heitkaemper and John Lambert and Jinhyuk Lee and Alexander Fr\u00f6mmgen and Sergey Rogulenko and Pranav Nair and Paul Niemczyk and Anton Bulyenov and Bibo Xu and Hadar Shemtov and Morteza Zadimoghaddam and Serge Toropov and Mateo Wirth and Hanjun Dai and Sreenivas Gollapudi and Daniel Zheng and Alex Kurakin and Chansoo Lee and Kalesha Bullard and Nicolas Serrano and Ivana Balazevic and Yang Li and Johan Schalkwyk and Mark Murphy and Mingyang Zhang and Kevin Sequeira and Romina Datta and Nishant Agrawal and Charles Sutton and Nithya Attaluri and Mencher Chiang and Wael Farhan and Gregory Thornton and Kate Lin and Travis Choma and Hung Nguyen and Kingshuk Dasgupta and Dirk Robinson and Iulia Com\u015fa and Michael Riley and Arjun Pillai and Basil Mustafa and Ben Golan and Amir Zandieh and Jean-Baptiste Lespiau and Billy Porter and David Ross and Sujeevan Rajayogam and Mohit Agarwal and Subhashini Venugopalan and Bobak Shahriari and Qiqi Yan and Hao Xu and Taylor Tobin and Pavel Dubov and Hongzhi Shi and Adri\u00e0 Recasens and Anton Kovsharov and Sebastian Borgeaud and Lucio Dery and Shanthal Vasanth and Elena Gribovskaya and Linhai Qiu and Mahdis Mahdieh and Wojtek Skut and Elizabeth Nielsen and CJ Zheng and Adams Yu and Carrie Grimes Bostock and Shaleen Gupta and Aaron Archer and Chris Rawles and Elinor Davies and Alexey Svyatkovskiy and Tomy Tsai and Yoni Halpern and Christian Reisswig and Bartek Wydrowski and Bo Chang and Joan Puigcerver and Mor Hazan Taege and Jian Li and Eva Schnider and Xinjian Li and Dragos Dena and Yunhan Xu and Umesh Telang and Tianze Shi and Heiga Zen and Kyle Kastner and Yeongil Ko and Neesha Subramaniam and Aviral Kumar and Pete Blois and Zhuyun Dai and John Wieting and Yifeng Lu and Yoel Zeldes and Tian Xie and Anja Hauth and Alexandru \u0162ifrea and Yuqi Li and Sam El-Husseini and Dan Abolafia and Howard Zhou and Wen Ding and Sahra Ghalebikesabi and Carlos Gu\u00eda and Andrii Maksai and \u00c1goston Weisz and Sercan Arik and Nick Sukhanov and Aga \u015awietlik and Xuhui Jia and Luo Yu and Weiyue Wang and Mark Brand and Dawn Bloxwich and Sean Kirmani and Zhe Chen and Alec Go and Pablo Sprechmann and Nithish Kannen and Alen Carin and Paramjit Sandhu and Isabel Edkins and Leslie Nooteboom and Jai Gupta and Loren Maggiore and Javad Azizi and Yael Pritch and Pengcheng Yin and Mansi Gupta and Danny Tarlow and Duncan Smith and Desi Ivanov and Mohammad Babaeizadeh and Ankita Goel and Satish Kambala and Grace Chu and Matej Kastelic and Michelle Liu and Hagen Soltau and Austin Stone and Shivani Agrawal and Min Kim and Kedar Soparkar and Srinivas Tadepalli and Oskar Bunyan and Rachel Soh and Arvind Kannan and DY Kim and Blake JianHang Chen and Afief Halumi and Sudeshna Roy and Yulong Wang and Olcan Sercinoglu and Gena Gibson and Sijal Bhatnagar and Motoki Sano and Daniel von Dincklage and Qingchun Ren and Blagoj Mitrevski and Mirek Ol\u0161\u00e1k and Jennifer She and Carl Doersch and  Jilei and  Wang and Bingyuan Liu and Qijun Tan and Tamar Yakar and Tris Warkentin and Alex Ramirez and Carl Lebsack and Josh Dillon and Rajiv Mathews and Tom Cobley and Zelin Wu and Zhuoyuan Chen and Jon Simon and Swaroop Nath and Tara Sainath and Alexei Bendebury and Ryan Julian and Bharath Mankalale and Daria \u0106urko and Paulo Zacchello and Adam R. Brown and Kiranbir Sodhia and Heidi Howard and Sergi Caelles and Abhinav Gupta and Gareth Evans and Anna Bulanova and Lesley Katzen and Roman Goldenberg and Anton Tsitsulin and Joe Stanton and Benoit Schillings and Vitaly Kovalev and Corey Fry and Rushin Shah and Kuo Lin and Shyam Upadhyay and Cheng Li and Soroush Radpour and Marcello Maggioni and Jing Xiong and Lukas Haas and Jenny Brennan and Aishwarya Kamath and Nikolay Savinov and Arsha Nagrani and Trevor Yacovone and Ryan Kappedal and Kostas Andriopoulos and Li Lao and YaGuang Li and Grigory Rozhdestvenskiy and Kazuma Hashimoto and Andrew Audibert and Sophia Austin and Daniel Rodriguez and Anian Ruoss and Garrett Honke and Deep Karkhanis and Xi Xiong and Qing Wei and James Huang and Zhaoqi Leng and Vittal Premachandran and Stan Bileschi and Georgios Evangelopoulos and Thomas Mensink and Jay Pavagadhi and Denis Teplyashin and Paul Chang and Linting Xue and Garrett Tanzer and Sally Goldman and Kaushal Patel and Shixin Li and Jeremy Wiesner and Ivy Zheng and Ian Stewart-Binks and Jie Han and Zhi Li and Liangchen Luo and Karel Lenc and Mario Lu\u010di\u0107 and Fuzhao Xue and Ryan Mullins and Alexey Guseynov and Chung-Ching Chang and Isaac Galatzer-Levy and Adam Zhang and Garrett Bingham and Grace Hu and Ale Hartman and Yue Ma and Jordan Griffith and Alex Irpan and Carey Radebaugh and Summer Yue and Lijie Fan and Victor Ungureanu and Christina Sorokin and Hannah Teufel and Peiran Li and Rohan Anil and Dimitris Paparas and Todd Wang and Chu-Cheng Lin and Hui Peng and Megan Shum and Goran Petrovic and Demetra Brady and Richard Nguyen and Klaus Macherey and Zhihao Li and Harman Singh and Madhavi Yenugula and Mariko Iinuma and Xinyi Chen and Kavya Kopparapu and Alexey Stern and Shachi Dave and Chandu Thekkath and Florence Perot and Anurag Kumar and Fangda Li and Yang Xiao and Matthew Bilotti and Mohammad Hossein Bateni and Isaac Noble and Lisa Lee and Amelio V\u00e1zquez-Reina and Julian Salazar and Xiaomeng Yang and Boyu Wang and Ela Gruzewska and Anand Rao and Sindhu Raghuram and Zheng Xu and Eyal Ben-David and Jieru Mei and Sid Dalmia and Zhaoyi Zhang and Yuchen Liu and Gagan Bansal and Helena Pankov and Steven Schwarcz and Andrea Burns and Christine Chan and Sumit Sanghai and Ricky Liang and Ethan Liang and Antoine He and Amy Stuart and Arun Narayanan and Yukun Zhu and Christian Frank and Bahar Fatemi and Amit Sabne and Oran Lang and Indro Bhattacharya and Shane Settle and Maria Wang and Brendan McMahan and Andrea Tacchetti and Livio Baldini Soares and Majid Hadian and Serkan Cabi and Timothy Chung and Nikita Putikhin and Gang Li and Jeremy Chen and Austin Tarango and Henryk Michalewski and Mehran Kazemi and Hussain Masoom and Hila Sheftel and Rakesh Shivanna and Archita Vadali and Ramona Comanescu and Doug Reid and Joss Moore and Arvind Neelakantan and Micha\u00ebl Sander and Jonathan Herzig and Aviv Rosenberg and Mostafa Dehghani and JD Choi and Michael Fink and Reid Hayes and Eric Ge and Shitao Weng and Chia-Hua Ho and John Karro and Kalpesh Krishna and Lam Nguyen Thiet and Amy Skerry-Ryan and Daniel Eppens and Marco Andreetto and Navin Sarma and Silvano Bonacina and Burcu Karagol Ayan and Megha Nawhal and Zhihao Shan and Mike Dusenberry and Shantanu Thakoor and Sagar Gubbi and Duc Dung Nguyen and Reut Tsarfaty and Samuel Albanie and Jovana Mitrovi\u0107 and Meet Gandhi and Bo-Juen Chen and Alessandro Epasto and Georgi Stephanov and Ye Jin and Samuel Gehman and Aida Amini and Jack Weber and Feryal Behbahani and Shawn Xu and Miltos Allamanis and Xi Chen and Myle Ott and Claire Sha and Michal Jastrzebski and Hang Qi and David Greene and Xinyi Wu and Abodunrinwa Toki and Daniel Vlasic and Jane Shapiro and Ragha Kotikalapudi and Zhe Shen and Takaaki Saeki and Sirui Xie and Albin Cassirer and Shikhar Bharadwaj and Tatsuya Kiyono and Srinadh Bhojanapalli and Elan Rosenfeld and Sam Ritter and Jieming Mao and Jo\u00e3o Gabriel Oliveira and Zoltan Egyed and Bernd Bandemer and Emilio Parisotto and Keisuke Kinoshita and Juliette Pluto and Petros Maniatis and Steve Li and Yaohui Guo and Golnaz Ghiasi and Jean Tarbouriech and Srimon Chatterjee and Julie Jin and  Katrina and  Xu and Jennimaria Palomaki and S\u00e9b Arnold and Madhavi Sewak and Federico Piccinini and Mohit Sharma and Ben Albrecht and Sean Purser-haskell and Ashwin Vaswani and Chongyan Chen and Matheus Wisniewski and Qin Cao and John Aslanides and Nguyet Minh Phu and Maximilian Sieb and Lauren Agubuzu and Anne Zheng and Daniel Sohn and Marco Selvi and Anders Andreassen and Krishan Subudhi and Prem Eruvbetine and Oliver Woodman and Tomas Mery and Sebastian Krause and Xiaoqi Ren and Xiao Ma and Jincheng Luo and Dawn Chen and Wei Fan and Henry Griffiths and Christian Schuler and Alice Li and Shujian Zhang and Jean-Michel Sarr and Shixin Luo and Riccardo Patana and Matthew Watson and Dani Naboulsi and Michael Collins and Sailesh Sidhwani and Emiel Hoogeboom and Sharon Silver and Emily Caveness and Xiaokai Zhao and Mikel Rodriguez and Maxine Deines and Libin Bai and Patrick Griffin and Marco Tagliasacchi and Emily Xue and Spandana Raj Babbula and Bo Pang and Nan Ding and Gloria Shen and Elijah Peake and Remi Crocker and Shubha Srinivas Raghvendra and Danny Swisher and Woohyun Han and Richa Singh and Ling Wu and Vladimir Pchelin and Tsendsuren Munkhdalai and Dana Alon and Geoff Bacon and Efren Robles and Jannis Bulian and Melvin Johnson and George Powell and Felipe Tiengo Ferreira and Yaoyiran Li and Frederik Benzing and Mihajlo Velimirovi\u0107 and Hubert Soyer and William Kong and  Tony and  Nguy\u00ean and Zhen Yang and Jeremiah Liu and Joost van Amersfoort and Daniel Gillick and Baochen Sun and Nathalie Rauschmayr and Katie Zhang and Serena Zhan and Tao Zhou and Alexey Frolov and Chengrun Yang and Denis Vnukov and Louis Rouillard and Hongji Li and Amol Mandhane and Nova Fallen and Rajesh Venkataraman and Clara Huiyi Hu and Jennifer Brennan and Jenny Lee and Jerry Chang and Martin Sundermeyer and Zhufeng Pan and Rosemary Ke and Simon Tong and Alex Fabrikant and William Bono and Jindong Gu and Ryan Foley and Yiran Mao and Manolis Delakis and Dhruva Bhaswar and Roy Frostig and Nick Li and Avital Zipori and Cath Hope and Olga Kozlova and Swaroop Mishra and Josip Djolonga and Craig Schiff and Majd Al Merey and Eleftheria Briakou and Peter Morgan and Andy Wan and Avinatan Hassidim and RJ Skerry-Ryan and Kuntal Sengupta and Mary Jasarevic and Praveen Kallakuri and Paige Kunkle and Hannah Brennan and Tom Lieber and Hassan Mansoor and Julian Walker and Bing Zhang and Annie Xie and Goran \u017du\u017ei\u0107 and Adaeze Chukwuka and Alex Druinsky and Donghyun Cho and Rui Yao and Ferjad Naeem and Shiraz Butt and Eunyoung Kim and Zhipeng Jia and Mandy Jordan and Adam Lelkes and Mark Kurzeja and Sophie Wang and James Zhao and Andrew Over and Abhishek Chakladar and Marcel Prasetya and Neha Jha and Sriram Ganapathy and Yale Cong and Prakash Shroff and Carl Saroufim and Sobhan Miryoosefi and Mohamed Hammad and Tajwar Nasir and Weijuan Xi and Yang Gao and Young Maeng and Ben Hora and Chin-Yi Cheng and Parisa Haghani and Yoad Lewenberg and Caden Lu and Martin Matysiak and Naina Raisinghani and Huiyu Wang and Lexi Baugher and Rahul Sukthankar and Minh Giang and John Schultz and Noah Fiedel and Minmin Chen and Cheng-Chun Lee and Tapomay Dey and Hao Zheng and Shachi Paul and Celine Smith and Andy Ly and Yicheng Wang and Rishabh Bansal and Bartek Perz and Susanna Ricco and Stasha Blank and Vaishakh Keshava and Deepak Sharma and Marvin Chow and Kunal Lad and Komal Jalan and Simon Osindero and Craig Swanson and Jacob Scott and Anastasija Ili\u0107 and Xiaowei Li and Siddhartha Reddy Jonnalagadda and Afzal Shama Soudagar and Yan Xiong and Bat-Orgil Batsaikhan and Daniel Jarrett and Naveen Kumar and Maulik Shah and Matt Lawlor and Austin Waters and Mark Graham and Rhys May and Sabela Ramos and Sandra Lefdal and Zeynep Cankara and Nacho Cano and Brendan O'Donoghue and Jed Borovik and Frederick Liu and Jordan Grimstad and Mahmoud Alnahlawi and Katerina Tsihlas and Tom Hudson and Nikolai Grigorev and Yiling Jia and Terry Huang and Tobenna Peter Igwe and Sergei Lebedev and Xiaodan Tang and Igor Krivokon and Frankie Garcia and Melissa Tan and Eric Jia and Peter Stys and Shikhar Vashishth and Yu Liang and Balaji Venkatraman and Chenjie Gu and Anastasios Kementsietsidis and Chen Zhu and Junehyuk Jung and Yunfei Bai and Mohammad Javad Hosseini and Faruk Ahmed and Aditya Gupta and Xin Yuan and Shereen Ashraf and Shitij Nigam and Gautam Vasudevan and Pranjal Awasthi and Adi Mayrav Gilady and Zelda Mariet and Ramy Eskander and Haiguang Li and Hexiang Hu and Guillermo Garrido and Philippe Schlattner and George Zhang and Rohun Saxena and Petar Devi\u0107 and Kritika Muralidharan and Ashwin Murthy and Yiqian Zhou and Min Choi and Arissa Wongpanich and Zhengdong Wang and Premal Shah and Yuntao Xu and Yiling Huang and Stephen Spencer and Alice Chen and James Cohan and Junjie Wang and Jonathan Tompson and Junru Wu and Ruba Haroun and Haiqiong Li and Blanca Huergo and Fan Yang and Tongxin Yin and James Wendt and Michael Bendersky and Rahma Chaabouni and Javier Snaider and Johan Ferret and Abhishek Jindal and Tara Thompson and Andrew Xue and Will Bishop and Shubham Milind Phal and Archit Sharma and Yunhsuan Sung and Prabakar Radhakrishnan and Mo Shomrat and Reeve Ingle and Roopali Vij and Justin Gilmer and Mihai Dorin Istin and Sam Sobell and Yang Lu and Emily Nottage and Dorsa Sadigh and Jeremiah Willcock and Tingnan Zhang and Steve Xu and Sasha Brown and Katherine Lee and Gary Wang and Yun Zhu and Yi Tay and Cheolmin Kim and Audrey Gutierrez and Abhanshu Sharma and Yongqin Xian and Sungyong Seo and Claire Cui and Elena Pochernina and Cip Baetu and Krzysztof Jastrz\u0119bski and Mimi Ly and Mohamed Elhawaty and Dan Suh and Eren Sezener and Pidong Wang and Nancy Yuen and George Tucker and Jiahao Cai and Zuguang Yang and Cindy Wang and Alex Muzio and Hai Qian and Jae Yoo and Derek Lockhart and Kevin R. McKee and Mandy Guo and Malika Mehrotra and Artur Mendon\u00e7a and Sanket Vaibhav Mehta and Sherry Ben and Chetan Tekur and Jiaqi Mu and Muye Zhu and Victoria Krakovna and Hongrae Lee and AJ Maschinot and S\u00e9bastien Cevey and HyunJeong Choe and Aijun Bai and Hansa Srinivasan and Derek Gasaway and Nick Young and Patrick Siegler and Dan Holtmann-Rice and Vihari Piratla and Kate Baumli and Roey Yogev and Alex Hofer and Hado van Hasselt and Svetlana Grant and Yuri Chervonyi and David Silver and Andrew Hogue and Ayushi Agarwal and Kathie Wang and Preeti Singh and Four Flynn and Josh Lipschultz and Robert David and Lizzetth Bellot and Yao-Yuan Yang and Long Le and Filippo Graziano and Kate Olszewska and Kevin Hui and Akanksha Maurya and Nikos Parotsidis and Weijie Chen and Tayo Oguntebi and Joe Kelley and Anirudh Baddepudi and Johannes Mauerer and Gregory Shaw and Alex Siegman and Lin Yang and Shravya Shetty and Subhrajit Roy and Yunting Song and Wojciech Stokowiec and Ryan Burnell and Omkar Savant and Robert Busa-Fekete and Jin Miao and Samrat Ghosh and Liam MacDermed and Phillip Lippe and Mikhail Dektiarev and Zach Behrman and Fabian Mentzer and Kelvin Nguyen and Meng Wei and Siddharth Verma and Chris Knutsen and Sudeep Dasari and Zhipeng Yan and Petr Mitrichev and Xingyu Wang and Virat Shejwalkar and Jacob Austin and Srinivas Sunkara and Navneet Potti and Yan Virin and Christian Wright and Ga\u00ebl Liu and Oriana Riva and Etienne Pot and Greg Kochanski and Quoc Le and Gargi Balasubramaniam and Arka Dhar and Yuguo Liao and Adam Bloniarz and Divyansh Shukla and Elizabeth Cole and Jong Lee and Sheng Zhang and Sushant Kafle and Siddharth Vashishtha and Parsa Mahmoudieh and Grace Chen and Raphael Hoffmann and Pranesh Srinivasan and Agustin Dal Lago and Yoav Ben Shalom and Zi Wang and Michael Elabd and Anuj Sharma and Junhyuk Oh and Suraj Kothawade and Maigo Le and Marianne Monteiro and Shentao Yang and Kaiz Alarakyia and Robert Geirhos and Diana Mincu and H\u00e5vard Garnes and Hayato Kobayashi and Soroosh Mariooryad and Kacper Krasowiak and  Zhixin and  Lai and Shibl Mourad and Mingqiu Wang and Fan Bu and Ophir Aharoni and Guanjie Chen and Abhimanyu Goyal and Vadim Zubov and Ankur Bapna and Elahe Dabir and Nisarg Kothari and Kay Lamerigts and Nicola De Cao and Jeremy Shar and Christopher Yew and Nitish Kulkarni and Dre Mahaarachchi and Mandar Joshi and Zhenhai Zhu and Jared Lichtarge and Yichao Zhou and Hannah Muckenhirn and Vittorio Selo and Oriol Vinyals and Peter Chen and Anthony Brohan and Vaibhav Mehta and Sarah Cogan and Ruth Wang and Ty Geri and Wei-Jen Ko and Wei Chen and Fabio Viola and Keshav Shivam and Lisa Wang and Madeleine Clare Elish and Raluca Ada Popa and S\u00e9bastien Pereira and Jianqiao Liu and Raphael Koster and Donnie Kim and Gufeng Zhang and Sayna Ebrahimi and Partha Talukdar and Yanyan Zheng and Petra Poklukar and Ales Mikhalap and Dale Johnson and Anitha Vijayakumar and Mark Omernick and Matt Dibb and Ayush Dubey and Qiong Hu and Apurv Suman and Vaibhav Aggarwal and Ilya Kornakov and Fei Xia and Wing Lowe and Alexey Kolganov and Ted Xiao and Vitaly Nikolaev and Steven Hemingray and Bonnie Li and Joana Iljazi and Miko\u0142aj Rybi\u0144ski and Ballie Sandhu and Peggy Lu and Thang Luong and Rodolphe Jenatton and Vineetha Govindaraj and  Hui and  Li and Gabriel Dulac-Arnold and Wonpyo Park and Henry Wang and Abhinit Modi and Jean Pouget-Abadie and Kristina Greller and Rahul Gupta and Robert Berry and Prajit Ramachandran and Jinyu Xie and Liam McCafferty and Jianling Wang and Kilol Gupta and Hyeontaek Lim and Bla\u017e Bratani\u010d and Andy Brock and Ilia Akolzin and Jim Sproch and Dan Karliner and Duhyeon Kim and Adrian Goedeckemeyer and Noam Shazeer and Cordelia Schmid and Daniele Calandriello and Parul Bhatia and Krzysztof Choromanski and Ceslee Montgomery and Dheeru Dua and Ana Ramalho and Helen King and Yue Gao and Lynn Nguyen and David Lindner and Divya Pitta and Oleaser Johnson and Khalid Salama and Diego Ardila and Michael Han and Erin Farnese and Seth Odoom and Ziyue Wang and Xiangzhuo Ding and Norman Rink and Ray Smith and Harshal Tushar Lehri and Eden Cohen and Neera Vats and Tong He and Parthasarathy Gopavarapu and Adam Paszke and Miteyan Patel and Wouter Van Gansbeke and Lucia Loher and Luis Castro and Maria Voitovich and Tamara von Glehn and Nelson George and Simon Niklaus and Zach Eaton-Rosen and Nemanja Raki\u0107evi\u0107 and Erik Jue and Sagi Perel and Carrie Zhang and Yuval Bahat and Ang\u00e9line Pouget and Zhi Xing and Fantine Huot and Ashish Shenoy and Taylor Bos and Vincent Coriou and Bryan Richter and Natasha Noy and Yaqing Wang and Santiago Ontanon and Siyang Qin and Gleb Makarchuk and Demis Hassabis and Zhuowan Li and Mandar Sharma and Kumaran Venkatesan and Iurii Kemaev and Roxanne Daniel and Shiyu Huang and Saloni Shah and Octavio Ponce and  Warren and  Chen and Manaal Faruqui and Jialin Wu and Slavica Anda\u010di\u0107 and Szabolcs Payrits and Daniel McDuff and Tom Hume and Yuan Cao and MH Tessler and Qingze Wang and Yinan Wang and Ivor Rendulic and Eirikur Agustsson and Matthew Johnson and Tanya Lando and Andrew Howard and Sri Gayatri Sundara Padmanabhan and Mayank Daswani and Andrea Banino and Michael Kilgore and Jonathan Heek and Ziwei Ji and Alvaro Caceres and Conglong Li and Nora Kassner and Alexey Vlaskin and Zeyu Liu and Alex Grills and Yanhan Hou and Roykrong Sukkerd and Gowoon Cheon and Nishita Shetty and Larisa Markeeva and Piotr Stanczyk and Tejas Iyer and Yuan Gong and Shawn Gao and Keerthana Gopalakrishnan and Tim Blyth and Malcolm Reynolds and Avishkar Bhoopchand and Misha Bilenko and Dero Gharibian and Vicky Zayats and Aleksandra Faust and Abhinav Singh and Min Ma and Hongyang Jiao and Sudheendra Vijayanarasimhan and Lora Aroyo and Vikas Yadav and Sarah Chakera and Ashwin Kakarla and Vilobh Meshram and Karol Gregor and Gabriela Botea and Evan Senter and Dawei Jia and Geza Kovacs and Neha Sharma and Sebastien Baur and Kai Kang and Yifan He and Lin Zhuo and Marija Kostelac and Itay Laish and Songyou Peng and Louis O'Bryan and Daniel Kasenberg and Girish Ramchandra Rao and Edouard Leurent and Biao Zhang and Sage Stevens and Ana Salazar and Ye Zhang and Ivan Lobov and Jake Walker and Allen Porter and Morgan Redshaw and Han Ke and Abhishek Rao and Alex Lee and Hoi Lam and Michael Moffitt and Jaeyoun Kim and Siyuan Qiao and Terry Koo and Robert Dadashi and Xinying Song and Mukund Sundararajan and Peng Xu and Chizu Kawamoto and Yan Zhong and Clara Barbu and Apoorv Reddy and Mauro Verzetti and Leon Li and George Papamakarios and Hanna Klimczak-Pluci\u0144ska and Mary Cassin and Koray Kavukcuoglu and Rigel Swavely and Alain Vaucher and Jeffrey Zhao and Ross Hemsley and Michael Tschannen and Heming Ge and Gaurav Menghani and Yang Yu and Natalie Ha and Wei He and Xiao Wu and Maggie Song and Rachel Sterneck and Stefan Zinke and Dan A. Calian and Annie Marsden and Alejandro Cruzado Ruiz and Matteo Hessel and Almog Gueta and Benjamin Lee and Brian Farris and Manish Gupta and Yunjie Li and Mohammad Saleh and Vedant Misra and Kefan Xiao and Piermaria Mendolicchio and Gavin Buttimore and Varvara Krayvanova and Nigamaa Nayakanti and Matthew Wiethoff and Yash Pande and Azalia Mirhoseini and Ni Lao and Jasmine Liu and Yiqing Hua and Angie Chen and Yury Malkov and Dmitry Kalashnikov and Shubham Gupta and Kartik Audhkhasi and Yuexiang Zhai and Sudhindra Kopalle and Prateek Jain and Eran Ofek and Clemens Meyer and Khuslen Baatarsukh and Hana Strej\u010dek and Jun Qian and James Freedman and Ricardo Figueira and Michal Sokolik and Olivier Bachem and Raymond Lin and Dia Kharrat and Chris Hidey and Pingmei Xu and Dennis Duan and Yin Li and Muge Ersoy and Richard Everett and Kevin Cen and Rebeca Santamaria-Fernandez and Amir Taubenfeld and Ian Mackinnon and Linda Deng and Polina Zablotskaia and Shashank Viswanadha and Shivanker Goel and Damion Yates and Yunxiao Deng and Peter Choy and Mingqing Chen and Abhishek Sinha and Alex Mossin and Yiming Wang and Arthur Szlam and Susan Hao and Paul Kishan Rubenstein and Metin Toksoz-Exley and Miranda Aperghis and Yin Zhong and Junwhan Ahn and Michael Isard and Olivier Lacombe and Florian Luisier and Chrysovalantis Anastasiou and Yogesh Kalley and Utsav Prabhu and Emma Dunleavy and Shaan Bijwadia and Justin Mao-Jones and Kelly Chen and Rama Pasumarthi and Emily Wood and Adil Dostmohamed and Nate Hurley and Jiri Simsa and Alicia Parrish and Mantas Pajarskas and Matt Harvey and Ondrej Skopek and Yony Kochinski and Javier Rey and Verena Rieser and Denny Zhou and Sun Jae Lee and Trilok Acharya and Guowang Li and Joe Jiang and Xiaofan Zhang and Bryant Gipson and Ethan Mahintorabi and Marco Gelmi and Nima Khajehnouri and Angel Yeh and Kayi Lee and Loic Matthey and Leslie Baker and Trang Pham and Han Fu and Alex Pak and Prakhar Gupta and Cristina Vasconcelos and Adam Sadovsky and Brian Walker and Sissie Hsiao and Patrik Zochbauer and Andreea Marzoca and Noam Velan and Junhao Zeng and Gilles Baechler and Danny Driess and Divya Jain and Yanping Huang and Lizzie Tao and John Maggs and Nir Levine and Jon Schneider and Erika Gemzer and Samuel Petit and Shan Han and Zach Fisher and Dustin Zelle and Courtney Biles and Eugene Ie and Asya Fadeeva and Casper Liu and Juliana Vicente Franco and Adrian Collister and Hao Zhang and Renshen Wang and Ruizhe Zhao and Leandro Kieliger and Kurt Shuster and Rui Zhu and Boqing Gong and Lawrence Chan and Ruoxi Sun and Sujoy Basu and Roland Zimmermann and Jamie Hayes and Abhishek Bapna and Jasper Snoek and Weel Yang and Puranjay Datta and Jad Al Abdallah and Kevin Kilgour and Lu Li and SQ Mah and Yennie Jun and Morgane Rivi\u00e8re and Abhijit Karmarkar and Tammo Spalink and Tao Huang and Lucas Gonzalez and Duc-Hieu Tran and Averi Nowak and John Palowitch and Martin Chadwick and Ellie Talius and Harsh Mehta and Thibault Sellam and Philipp Fr\u00e4nken and Massimo Nicosia and Kyle He and Aditya Kini and David Amos and Sugato Basu and Harrison Jobe and Eleni Shaw and Qiantong Xu and Colin Evans and Daisuke Ikeda and Chaochao Yan and Larry Jin and Lun Wang and Sachin Yadav and Ilia Labzovsky and Ramesh Sampath and Ada Ma and Candice Schumann and Aditya Siddhant and Rohin Shah and John Youssef and Rishabh Agarwal and Natalie Dabney and Alessio Tonioni and Moran Ambar and Jing Li and Isabelle Guyon and Benny Li and David Soergel and Boya Fang and Georgi Karadzhov and Cristian Udrescu and Trieu Trinh and Vikas Raunak and Seb Noury and Dee Guo and Sonal Gupta and Mara Finkelstein and Denis Petek and Lihao Liang and Greg Billock and Pei Sun and David Wood and Yiwen Song and Xiaobin Yu and Tatiana Matejovicova and Regev Cohen and Kalyan Andra and David D'Ambrosio and Zhiwei Deng and Vincent Nallatamby and Ebrahim Songhori and Rumen Dangovski and Andrew Lampinen and Pankil Botadra and Adam Hillier and Jiawei Cao and Nagabhushan Baddi and Adhi Kuncoro and Toshihiro Yoshino and Ankit Bhagatwala and Marc\u00e1urelio Ranzato and Rylan Schaeffer and Tianlin Liu and Shuai Ye and Obaid Sarvana and John Nham and Chenkai Kuang and Isabel Gao and Jinoo Baek and Shubham Mittal and Ayzaan Wahid and Anita Gergely and Bin Ni and Josh Feldman and Carrie Muir and Pascal Lamblin and Wolfgang Macherey and Ethan Dyer and Logan Kilpatrick and V\u00edctor Campos and Mukul Bhutani and Stanislav Fort and Yanif Ahmad and Aliaksei Severyn and Kleopatra Chatziprimou and Oleksandr Ferludin and Mason Dimarco and Aditya Kusupati and Joe Heyward and Dan Bahir and Kevin Villela and Katie Millican and Dror Marcus and Sanaz Bahargam and Caglar Unlu and Nicholas Roth and Zichuan Wei and Siddharth Gopal and Deepanway Ghoshal and Edward Lee and Sharon Lin and Jennie Lees and Dayeong Lee and Anahita Hosseini and Connie Fan and Seth Neel and Marcus Wu and Yasemin Altun and Honglong Cai and Enrique Piqueras and Josh Woodward and Alessandro Bissacco and Salem Haykal and Mahyar Bordbar and Prasha Sundaram and Sarah Hodkinson and Daniel Toyama and George Polovets and Austin Myers and Anu Sinha and Tomer Levinboim and Kashyap Krishnakumar and Rachita Chhaparia and Tatiana Sholokhova and Nitesh Bharadwaj Gundavarapu and Ganesh Jawahar and Haroon Qureshi and Jieru Hu and Nikola Momchev and Matthew Rahtz and Renjie Wu and Aishwarya P S and Kedar Dhamdhere and Meiqi Guo and Umang Gupta and Ali Eslami and Mariano Schain and Michiel Blokzijl and David Welling and Dave Orr and Levent Bolelli and Nicolas Perez-Nieves and Mikhail Sirotenko and Aman Prasad and Arjun Kar and Borja De Balle Pigem and Tayfun Terzi and Gell\u00e9rt Weisz and Dipankar Ghosh and Aditi Mavalankar and Dhruv Madeka and Kaspar Daugaard and Hartwig Adam and Viraj Shah and Dana Berman and Maggie Tran and Steven Baker and Ewa Andrejczuk and Grishma Chole and Ganna Raboshchuk and Mahdi Mirzazadeh and Thais Kagohara and Shimu Wu and Christian Schallhart and Bernett Orlando and Chen Wang and Alban Rrustemi and Hao Xiong and Hao Liu and Arpi Vezer and Nolan Ramsden and Shuo-yiin Chang and Sidharth Mudgal and Yan Li and Nino Vieillard and Yedid Hoshen and Farooq Ahmad and Ambrose Slone and Amy Hua and Natan Potikha and Mirko Rossini and Jon Stritar and Sushant Prakash and Zifeng Wang and Xuanyi Dong and Alireza Nazari and Efrat Nehoran and Kaan Tekelioglu and Yinxiao Li and Kartikeya Badola and Tom Funkhouser and Yuanzhen Li and Varun Yerram and Ramya Ganeshan and Daniel Formoso and Karol Langner and Tian Shi and Huijian Li and Yumeya Yamamori and Amayika Panda and Alaa Saade and Angelo Scorza Scarpati and Chris Breaux and CJ Carey and Zongwei Zhou and Cho-Jui Hsieh and Sophie Bridgers and Alena Butryna and Nishesh Gupta and Vaibhav Tulsyan and Sanghyun Woo and Evgenii Eltyshev and Will Grathwohl and Chanel Parks and Seth Benjamin and Rina Panigrahy and Shenil Dodhia and Daniel De Freitas and Chris Sauer and Will Song and Ferran Alet and Jackson Tolins and Cosmin Paduraru and Xingyi Zhou and Brian Albert and Zizhao Zhang and Lei Shu and Mudit Bansal and Sarah Nguyen and Amir Globerson and Owen Xiao and James Manyika and Tom Hennigan and Rong Rong and Josip Matak and Anton Bakalov and Ankur Sharma and Danila Sinopalnikov and Andrew Pierson and Stephen Roller and Geoff Brown and Mingcen Gao and Toshiyuki Fukuzawa and Amin Ghafouri and Kenny Vassigh and Iain Barr and Zhicheng Wang and Anna Korsun and Rajesh Jayaram and Lijie Ren and Tim Zaman and Samira Khan and Yana Lunts and Dan Deutsch and Dave Uthus and Nitzan Katz and Masha Samsikova and Amr Khalifa and Nikhil Sethi and Jiao Sun and Luming Tang and Uri Alon and Xianghong Luo and Dian Yu and Abhishek Nayyar and Bryce Petrini and Will Truong and Vincent Hellendoorn and Nikolai Chinaev and Chris Alberti and Wei Wang and Jingcao Hu and Vahab Mirrokni and Ananth Balashankar and Avia Aharon and Aahil Mehta and Ahmet Iscen and Joseph Kready and Lucas Manning and Anhad Mohananey and Yuankai Chen and Anshuman Tripathi and Allen Wu and Igor Petrovski and Dawsen Hwang and Martin Baeuml and Shreyas Chandrakaladharan and Yuan Liu and Rey Coaguila and Maxwell Chen and Sally Ma and Pouya Tafti and Susheel Tatineni and Terry Spitz and Jiayu Ye and Paul Vicol and Mihaela Rosca and Adri\u00e0 Puigdom\u00e8nech and Zohar Yahav and Sanjay Ghemawat and Hanzhao Lin and Phoebe Kirk and Zaid Nabulsi and Sergey Brin and Bernd Bohnet and Ken Caluwaerts and Aditya Srikanth Veerubhotla and Dan Zheng and Zihang Dai and Petre Petrov and Yichong Xu and Ramin Mehran and Zhuo Xu and Luisa Zintgraf and Jiho Choi and Spurthi Amba Hombaiah and Romal Thoppilan and Sashank Reddi and Lukasz Lew and Li Li and Kellie Webster and KP Sawhney and Lampros Lamprou and Siamak Shakeri and Mayank Lunayach and Jianmin Chen and Sumit Bagri and Alex Salcianu and Ying Chen and Yani Donchev and Charlotte Magister and Signe N\u00f8rly and Vitor Rodrigues and Tomas Izo and Hila Noga and Joe Zou and Thomas K\u00f6ppe and Wenxuan Zhou and Kenton Lee and Xiangzhu Long and Danielle Eisenbud and Anthony Chen and Connor Schenck and Chi Ming To and Peilin Zhong and Emanuel Taropa and Minh Truong and Omer Levy and Danilo Martins and Zhiyuan Zhang and Christopher Semturs and Kelvin Zhang and Alex Yakubovich and Pol Moreno and Lara McConnaughey and Di Lu and Sam Redmond and Lotte Weerts and Yonatan Bitton and Tiziana Refice and Nicolas Lacasse and Arthur Conmy and Corentin Tallec and Julian Odell and Hannah Forbes-Pollard and Arkadiusz Socala and Jonathan Hoech and Pushmeet Kohli and Alanna Walton and Rui Wang and Mikita Sazanovich and Kexin Zhu and Andrei Kapishnikov and Rich Galt and Matthew Denton and Ben Murdoch and Caitlin Sikora and Kareem Mohamed and Wei Wei and Uri First and Tim McConnell and Luis C. Cobo and James Qin and Thi Avrahami and Daniel Balle and Yu Watanabe and Annie Louis and Adam Kraft and Setareh Ariafar and Yiming Gu and Eug\u00e9nie Rives and Charles Yoon and Andrei Rusu and James Cobon-Kerr and Chris Hahn and Jiaming Luo and  Yuvein and  Zhu and Niharika Ahuja and Rodrigo Benenson and Rapha\u00ebl Lopez Kaufman and Honglin Yu and Lloyd Hightower and Junlin Zhang and Darren Ni and Lisa Anne Hendricks and Gabby Wang and Gal Yona and Lalit Jain and Pablo Barrio and Surya Bhupatiraju and Siva Velusamy and Allan Dafoe and Sebastian Riedel and Tara Thomas and Zhe Yuan and Mathias Bellaiche and Sheena Panthaplackel and Klemen Kloboves and Sarthak Jauhari and Canfer Akbulut and Todor Davchev and Evgeny Gladchenko and David Madras and Aleksandr Chuklin and Tyrone Hill and Quan Yuan and Mukundan Madhavan and Luke Leonhard and Dylan Scandinaro and Qihang Chen and Ning Niu and Arthur Douillard and Bogdan Damoc and Yasumasa Onoe and Fabian Pedregosa and Fred Bertsch and Chas Leichner and Joseph Pagadora and Jonathan Malmaud and Sameera Ponda and Andy Twigg and Oleksii Duzhyi and Jingwei Shen and Miaosen Wang and Roopal Garg and Jing Chen and Utku Evci and Jonathan Lee and Leon Liu and Koji Kojima and Masa Yamaguchi and Arunkumar Rajendran and AJ Piergiovanni and Vinodh Kumar Rajendran and Marco Fornoni and Gabriel Ibagon and Harry Ragan and Sadh MNM Khan and John Blitzer and Andrew Bunner and Guan Sun and Takahiro Kosakai and Scott Lundberg and Ndidi Elue and Kelvin Guu and SK Park and Jane Park and Arunachalam Narayanaswamy and Chengda Wu and Jayaram Mudigonda and Trevor Cohn and Hairong Mu and Ravi Kumar and Laura Graesser and Yichi Zhang and Richard Killam and Vincent Zhuang and Mai Gim\u00e9nez and Wael Al Jishi and Ruy Ley-Wild and Alex Zhai and Kazuki Osawa and Diego Cedillo and Jialu Liu and Mayank Upadhyay and Marcin Sieniek and Roshan Sharma and Tom Paine and Anelia Angelova and Sravanti Addepalli and Carolina Parada and Kingshuk Majumder and Avery Lamp and Sanjiv Kumar and Xiang Deng and Artiom Myaskovsky and Tea Saboli\u0107 and Jeffrey Dudek and Sarah York and F\u00e9lix de Chaumont Quitry and Jiazhong Nie and Dee Cattle and Alok Gunjan and Bilal Piot and Waleed Khawaja and Seojin Bang and Simon Wang and Siavash Khodadadeh and Raghavender R and Praynaa Rawlani and Richard Powell and Kevin Lee and Johannes Griesser and GS Oh and Cesar Magalhaes and Yujia Li and Simon Tokumine and Hadas Natalie Vogel and Dennis Hsu and Arturo BC and Disha Jindal and Matan Cohen and Zi Yang and Junwei Yuan and Dario de Cesare and Tony Bruguier and Jun Xu and Monica Roy and Alon Jacovi and Dan Belov and Rahul Arya and Phoenix Meadowlark and Shlomi Cohen-Ganor and Wenting Ye and Patrick Morris-Suzuki and Praseem Banzal and Gan Song and Pranavaraj Ponnuramu and Fred Zhang and George Scrivener and Salah Zaiem and Alif Raditya Rochman and Kehang Han and Badih Ghazi and Kate Lee and Shahar Drath and Daniel Suo and Antonious Girgis and Pradeep Shenoy and Duy Nguyen and Douglas Eck and Somit Gupta and Le Yan and Joao Carreira and Anmol Gulati and Ruoxin Sang and Daniil Mirylenka and Emma Cooney and Edward Chou and Mingyang Ling and Cindy Fan and Ben Coleman and Guilherme Tubone and Ravin Kumar and Jason Baldridge and Felix Hernandez-Campos and Angeliki Lazaridou and James Besley and Itay Yona and Neslihan Bulut and Quentin Wellens and AJ Pierigiovanni and Jasmine George and Richard Green and Pu Han and Connie Tao and Geoff Clark and Chong You and Abbas Abdolmaleki and Justin Fu and Tongzhou Chen and Ashwin Chaugule and Angad Chandorkar and Altaf Rahman and Will Thompson and Penporn Koanantakool and Mike Bernico and Jie Ren and Andrey Vlasov and Sergei Vassilvitskii and Maciej Kula and Yizhong Liang and Dahun Kim and Yangsibo Huang and Chengxi Ye and Dmitry Lepikhin and Wesley Helmholz", "abstract": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.", "link": "http://arxiv.org/abs/2507.06261v6", "date": "2025-12-19", "relevancy": 2.2911, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities&body=Title%3A%20Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities%0AAuthor%3A%20Gheorghe%20Comanici%20and%20Eric%20Bieber%20and%20Mike%20Schaekermann%20and%20Ice%20Pasupat%20and%20Noveen%20Sachdeva%20and%20Inderjit%20Dhillon%20and%20Marcel%20Blistein%20and%20Ori%20Ram%20and%20Dan%20Zhang%20and%20Evan%20Rosen%20and%20Luke%20Marris%20and%20Sam%20Petulla%20and%20Colin%20Gaffney%20and%20Asaf%20Aharoni%20and%20Nathan%20Lintz%20and%20Tiago%20Cardal%20Pais%20and%20Henrik%20Jacobsson%20and%20Idan%20Szpektor%20and%20Nan-Jiang%20Jiang%20and%20Krishna%20Haridasan%20and%20Ahmed%20Omran%20and%20Nikunj%20Saunshi%20and%20Dara%20Bahri%20and%20Gaurav%20Mishra%20and%20Eric%20Chu%20and%20Toby%20Boyd%20and%20Brad%20Hekman%20and%20Aaron%20Parisi%20and%20Chaoyi%20Zhang%20and%20Kornraphop%20Kawintiranon%20and%20Tania%20Bedrax-Weiss%20and%20Oliver%20Wang%20and%20Ya%20Xu%20and%20Ollie%20Purkiss%20and%20Uri%20Mendlovic%20and%20Ila%C3%AF%20Deutel%20and%20Nam%20Nguyen%20and%20Adam%20Langley%20and%20Flip%20Korn%20and%20Lucia%20Rossazza%20and%20Alexandre%20Ram%C3%A9%20and%20Sagar%20Waghmare%20and%20Helen%20Miller%20and%20Nathan%20Byrd%20and%20Ashrith%20Sheshan%20and%20Raia%20Hadsell%20and%20Sangnie%20Bhardwaj%20and%20Pawel%20Janus%20and%20Tero%20Rissa%20and%20Dan%20Horgan%20and%20Alvin%20Abdagic%20and%20Lior%20Belenki%20and%20James%20Allingham%20and%20Anima%20Singh%20and%20Theo%20Guidroz%20and%20Srivatsan%20Srinivasan%20and%20Herman%20Schmit%20and%20Kristen%20Chiafullo%20and%20Andre%20Elisseeff%20and%20Nilpa%20Jha%20and%20Prateek%20Kolhar%20and%20Leonard%20Berrada%20and%20Frank%20Ding%20and%20Xiance%20Si%20and%20Shrestha%20Basu%20Mallick%20and%20Franz%20Och%20and%20Sofia%20Erell%20and%20Eric%20Ni%20and%20Tejasi%20Latkar%20and%20Sherry%20Yang%20and%20Petar%20Sirkovic%20and%20Ziqiang%20Feng%20and%20Robert%20Leland%20and%20Rachel%20Hornung%20and%20Gang%20Wu%20and%20Charles%20Blundell%20and%20Hamidreza%20Alvari%20and%20Po-Sen%20Huang%20and%20Cathy%20Yip%20and%20Sanja%20Deur%20and%20Li%20Liu%20and%20Gabriela%20Surita%20and%20Pablo%20Duque%20and%20Dima%20Damen%20and%20Johnson%20Jia%20and%20Arthur%20Guez%20and%20Markus%20Mircea%20and%20Animesh%20Sinha%20and%20Alberto%20Magni%20and%20Pawe%C5%82%20Stradomski%20and%20Tal%20Marian%20and%20Vlado%20Gali%C4%87%20and%20Wenhu%20Chen%20and%20Hisham%20Husain%20and%20Achintya%20Singhal%20and%20Dominik%20Grewe%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Shuang%20Song%20and%20Lorenzo%20Blanco%20and%20Leland%20Rechis%20and%20Lewis%20Ho%20and%20Rich%20Munoz%20and%20Kelvin%20Zheng%20and%20Jessica%20Hamrick%20and%20Kevin%20Mather%20and%20Hagai%20Taitelbaum%20and%20Eliza%20Rutherford%20and%20Yun%20Lei%20and%20Kuangyuan%20Chen%20and%20Anand%20Shukla%20and%20Erica%20Moreira%20and%20Eric%20Doi%20and%20Berivan%20Isik%20and%20Nir%20Shabat%20and%20Dominika%20Rogozi%C5%84ska%20and%20Kashyap%20Kolipaka%20and%20Jason%20Chang%20and%20Eugen%20Vu%C5%A1ak%20and%20Srinivasan%20Venkatachary%20and%20Shadi%20Noghabi%20and%20Tarun%20Bharti%20and%20Younghoon%20Jun%20and%20Aleksandr%20Zaks%20and%20Simon%20Green%20and%20Jeshwanth%20Challagundla%20and%20William%20Wong%20and%20Muqthar%20Mohammad%20and%20Dean%20Hirsch%20and%20Yong%20Cheng%20and%20Iftekhar%20Naim%20and%20Lev%20Proleev%20and%20Damien%20Vincent%20and%20Aayush%20Singh%20and%20Maxim%20Krikun%20and%20Dilip%20Krishnan%20and%20Zoubin%20Ghahramani%20and%20Aviel%20Atias%20and%20Rajeev%20Aggarwal%20and%20Christo%20Kirov%20and%20Dimitrios%20Vytiniotis%20and%20Christy%20Koh%20and%20Alexandra%20Chronopoulou%20and%20Pawan%20Dogra%20and%20Vlad-Doru%20Ion%20and%20Gladys%20Tyen%20and%20Jason%20Lee%20and%20Felix%20Weissenberger%20and%20Trevor%20Strohman%20and%20Ashwin%20Balakrishna%20and%20Jack%20Rae%20and%20Marko%20Velic%20and%20Raoul%20de%20Liedekerke%20and%20Oded%20Elyada%20and%20Wentao%20Yuan%20and%20Canoee%20Liu%20and%20Lior%20Shani%20and%20Sergey%20Kishchenko%20and%20Bea%20Alessio%20and%20Yandong%20Li%20and%20Richard%20Song%20and%20Sam%20Kwei%20and%20Orion%20Jankowski%20and%20Aneesh%20Pappu%20and%20Youhei%20Namiki%20and%20Yenai%20Ma%20and%20Nilesh%20Tripuraneni%20and%20Colin%20Cherry%20and%20Marissa%20Ikonomidis%20and%20Yu-Cheng%20Ling%20and%20Colin%20Ji%20and%20Beka%20Westberg%20and%20Auriel%20Wright%20and%20Da%20Yu%20and%20David%20Parkinson%20and%20Swaroop%20Ramaswamy%20and%20Jerome%20Connor%20and%20Soheil%20Hassas%20Yeganeh%20and%20Snchit%20Grover%20and%20George%20Kenwright%20and%20Lubo%20Litchev%20and%20Chris%20Apps%20and%20Alex%20Tomala%20and%20Felix%20Halim%20and%20Alex%20Castro-Ros%20and%20Zefei%20Li%20and%20Anudhyan%20Boral%20and%20Pauline%20Sho%20and%20Michal%20Yarom%20and%20Eric%20Malmi%20and%20David%20Klinghoffer%20and%20Rebecca%20Lin%20and%20Alan%20Ansell%20and%20Pradeep%20Kumar%20S%20and%20Shubin%20Zhao%20and%20Siqi%20Zuo%20and%20Adam%20Santoro%20and%20Heng-Tze%20Cheng%20and%20Solomon%20Demmessie%20and%20Yuchi%20Liu%20and%20Nicole%20Brichtova%20and%20Allie%20Culp%20and%20Nathaniel%20Braun%20and%20Dan%20Graur%20and%20Will%20Ng%20and%20Nikhil%20Mehta%20and%20Aaron%20Phillips%20and%20Patrik%20Sundberg%20and%20Varun%20Godbole%20and%20Fangyu%20Liu%20and%20Yash%20Katariya%20and%20David%20Rim%20and%20Mojtaba%20Seyedhosseini%20and%20Sean%20Ammirati%20and%20Jonas%20Valfridsson%20and%20Mahan%20Malihi%20and%20Timothy%20Knight%20and%20Andeep%20Toor%20and%20Thomas%20Lampe%20and%20Abe%20Ittycheriah%20and%20Lewis%20Chiang%20and%20Chak%20Yeung%20and%20Alexandre%20Fr%C3%A9chette%20and%20Jinmeng%20Rao%20and%20Huisheng%20Wang%20and%20Himanshu%20Srivastava%20and%20Richard%20Zhang%20and%20Rocky%20Rhodes%20and%20Ariel%20Brand%20and%20Dean%20Weesner%20and%20Ilya%20Figotin%20and%20Felix%20Gimeno%20and%20Rachana%20Fellinger%20and%20Pierre%20Marcenac%20and%20Jos%C3%A9%20Leal%20and%20Eyal%20Marcus%20and%20Victor%20Cotruta%20and%20Rodrigo%20Cabrera%20and%20Sheryl%20Luo%20and%20Dan%20Garrette%20and%20Vera%20Axelrod%20and%20Sorin%20Baltateanu%20and%20David%20Barker%20and%20Dongkai%20Chen%20and%20Horia%20Toma%20and%20Ben%20Ingram%20and%20Jason%20Riesa%20and%20Chinmay%20Kulkarni%20and%20Yujing%20Zhang%20and%20Hongbin%20Liu%20and%20Chao%20Wang%20and%20Martin%20Polacek%20and%20Will%20Wu%20and%20Kai%20Hui%20and%20Adrian%20N%20Reyes%20and%20Yi%20Su%20and%20Megan%20Barnes%20and%20Ishaan%20Malhi%20and%20Anfal%20Siddiqui%20and%20Qixuan%20Feng%20and%20Mihai%20Damaschin%20and%20Daniele%20Pighin%20and%20Andreas%20Steiner%20and%20Samuel%20Yang%20and%20Ramya%20Sree%20Boppana%20and%20Simeon%20Ivanov%20and%20Arun%20Kandoor%20and%20Aditya%20Shah%20and%20Asier%20Mujika%20and%20Da%20Huang%20and%20Christopher%20A.%20Choquette-Choo%20and%20Mohak%20Patel%20and%20Tianhe%20Yu%20and%20Toni%20Creswell%20and%20%20Jerry%20and%20%20Liu%20and%20Catarina%20Barros%20and%20Yasaman%20Razeghi%20and%20Aurko%20Roy%20and%20Phil%20Culliton%20and%20Binbin%20Xiong%20and%20Jiaqi%20Pan%20and%20Thomas%20Strohmann%20and%20Tolly%20Powell%20and%20Babi%20Seal%20and%20Doug%20DeCarlo%20and%20Pranav%20Shyam%20and%20Kaan%20Katircioglu%20and%20Xuezhi%20Wang%20and%20Cassidy%20Hardin%20and%20Immanuel%20Odisho%20and%20Josef%20Broder%20and%20Oscar%20Chang%20and%20Arun%20Nair%20and%20Artem%20Shtefan%20and%20Maura%20O%27Brien%20and%20Manu%20Agarwal%20and%20Sahitya%20Potluri%20and%20Siddharth%20Goyal%20and%20Amit%20Jhindal%20and%20Saksham%20Thakur%20and%20Yury%20Stuken%20and%20James%20Lyon%20and%20Kristina%20Toutanova%20and%20Fangxiaoyu%20Feng%20and%20Austin%20Wu%20and%20Ben%20Horn%20and%20Alek%20Wang%20and%20Alex%20Cullum%20and%20Gabe%20Taubman%20and%20Disha%20Shrivastava%20and%20Chongyang%20Shi%20and%20Hamish%20Tomlinson%20and%20Roma%20Patel%20and%20Tao%20Tu%20and%20Ada%20Maksutaj%20Oflazer%20and%20Francesco%20Pongetti%20and%20Mingyao%20Yang%20and%20Adrien%20Ali%20Ta%C3%AFga%20and%20Vincent%20Perot%20and%20Nuo%20Wang%20Pierse%20and%20Feng%20Han%20and%20Yoel%20Drori%20and%20I%C3%B1aki%20Iturrate%20and%20Ayan%20Chakrabarti%20and%20Legg%20Yeung%20and%20Dave%20Dopson%20and%20Yi-ting%20Chen%20and%20Apoorv%20Kulshreshtha%20and%20Tongfei%20Guo%20and%20Philip%20Pham%20and%20Tal%20Schuster%20and%20Junquan%20Chen%20and%20Alex%20Polozov%20and%20Jinwei%20Xing%20and%20Huanjie%20Zhou%20and%20Praneeth%20Kacham%20and%20Doron%20Kukliansky%20and%20Antoine%20Miech%20and%20Sergey%20Yaroshenko%20and%20Ed%20Chi%20and%20Sholto%20Douglas%20and%20Hongliang%20Fei%20and%20Mathieu%20Blondel%20and%20Preethi%20Myla%20and%20Lior%20Madmoni%20and%20Xing%20Wu%20and%20Daniel%20Keysers%20and%20Kristian%20Kjems%20and%20Isabela%20Albuquerque%20and%20Lijun%20Yu%20and%20Joel%20D%27sa%20and%20Michelle%20Plantan%20and%20Vlad%20Ionescu%20and%20Jaume%20Sanchez%20Elias%20and%20Abhirut%20Gupta%20and%20Manish%20Reddy%20Vuyyuru%20and%20Fred%20Alcober%20and%20Tong%20Zhou%20and%20Kaiyang%20Ji%20and%20Florian%20Hartmann%20and%20Subha%20Puttagunta%20and%20Hugo%20Song%20and%20Ehsan%20Amid%20and%20Anca%20Stefanoiu%20and%20Andrew%20Lee%20and%20Paul%20Pucciarelli%20and%20Emma%20Wang%20and%20Amit%20Raul%20and%20Slav%20Petrov%20and%20Isaac%20Tian%20and%20Valentin%20Anklin%20and%20Nana%20Nti%20and%20Victor%20Gomes%20and%20Max%20Schumacher%20and%20Grace%20Vesom%20and%20Alex%20Panagopoulos%20and%20Konstantinos%20Bousmalis%20and%20Daniel%20Andor%20and%20Josh%20Jacob%20and%20Yuan%20Zhang%20and%20Bill%20Rosgen%20and%20Matija%20Kecman%20and%20Matthew%20Tung%20and%20Alexandra%20Belias%20and%20Noah%20Goodman%20and%20Paul%20Covington%20and%20Brian%20Wieder%20and%20Nikita%20Saxena%20and%20Elnaz%20Davoodi%20and%20Muhuan%20Huang%20and%20Sharath%20Maddineni%20and%20Vincent%20Roulet%20and%20Folawiyo%20Campbell-Ajala%20and%20Pier%20Giuseppe%20Sessa%20and%20%20Xintian%20and%20%20Wu%20and%20Guangda%20Lai%20and%20Paul%20Collins%20and%20Alex%20Haig%20and%20Vytenis%20Sakenas%20and%20Xiaowei%20Xu%20and%20Marissa%20Giustina%20and%20Laurent%20El%20Shafey%20and%20Pichi%20Charoenpanit%20and%20Shefali%20Garg%20and%20Joshua%20Ainslie%20and%20Boone%20Severson%20and%20Montse%20Gonzalez%20Arenas%20and%20Shreya%20Pathak%20and%20Sujee%20Rajayogam%20and%20Jie%20Feng%20and%20Michiel%20Bakker%20and%20Sheng%20Li%20and%20Nevan%20Wichers%20and%20Jamie%20Rogers%20and%20Xinyang%20Geng%20and%20Yeqing%20Li%20and%20Rolf%20Jagerman%20and%20Chao%20Jia%20and%20Nadav%20Olmert%20and%20David%20Sharon%20and%20Matthew%20Mauger%20and%20Sandeep%20Mariserla%20and%20Hongxu%20Ma%20and%20Megha%20Mohabey%20and%20Kyuyeun%20Kim%20and%20Alek%20Andreev%20and%20Scott%20Pollom%20and%20Juliette%20Love%20and%20Vihan%20Jain%20and%20Priyanka%20Agrawal%20and%20Yannick%20Schroecker%20and%20Alisa%20Fortin%20and%20Manfred%20Warmuth%20and%20Ji%20Liu%20and%20Andrew%20Leach%20and%20Irina%20Blok%20and%20Ganesh%20Poomal%20Girirajan%20and%20Roee%20Aharoni%20and%20Benigno%20Uria%20and%20Andrei%20Sozanschi%20and%20Dan%20Goldberg%20and%20Lucian%20Ionita%20and%20Marco%20Tulio%20Ribeiro%20and%20Martin%20Zlocha%20and%20Vighnesh%20Birodkar%20and%20Sami%20Lachgar%20and%20Liangzhe%20Yuan%20and%20Himadri%20Choudhury%20and%20Matt%20Ginsberg%20and%20Fei%20Zheng%20and%20Gregory%20Dibb%20and%20Emily%20Graves%20and%20Swachhand%20Lokhande%20and%20Gabriel%20Rasskin%20and%20George-Cristian%20Muraru%20and%20Corbin%20Quick%20and%20Sandeep%20Tata%20and%20Pierre%20Sermanet%20and%20Aditya%20Chawla%20and%20Itay%20Karo%20and%20Yan%20Wang%20and%20Susan%20Zhang%20and%20Orgad%20Keller%20and%20Anca%20Dragan%20and%20Guolong%20Su%20and%20Ian%20Chou%20and%20Xi%20Liu%20and%20Yiqing%20Tao%20and%20Shruthi%20Prabhakara%20and%20Marc%20Wilson%20and%20Ruibo%20Liu%20and%20Shibo%20Wang%20and%20Georgie%20Evans%20and%20David%20Du%20and%20Alfonso%20Casta%C3%B1o%20and%20Gautam%20Prasad%20and%20Mona%20El%20Mahdy%20and%20Sebastian%20Gerlach%20and%20Machel%20Reid%20and%20Jarrod%20Kahn%20and%20Amir%20Zait%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Thatcher%20Ulrich%20and%20Guanyu%20Wang%20and%20Jan%20Wassenberg%20and%20Efrat%20Farkash%20and%20Kiran%20Yalasangi%20and%20Congchao%20Wang%20and%20Maria%20Bauza%20and%20Simon%20Bucher%20and%20Ting%20Liu%20and%20Jun%20Yan%20and%20Gary%20Leung%20and%20Vikas%20Sindhwani%20and%20Parker%20Barnes%20and%20Avi%20Singh%20and%20Ivan%20Jurin%20and%20Jichuan%20Chang%20and%20Niket%20Kumar%20Bhumihar%20and%20Sivan%20Eiger%20and%20Gui%20Citovsky%20and%20Ben%20Withbroe%20and%20Zhang%20Li%20and%20Siyang%20Xue%20and%20Niccol%C3%B2%20Dal%20Santo%20and%20Georgi%20Stoyanov%20and%20Yves%20Raimond%20and%20Steven%20Zheng%20and%20Yilin%20Gao%20and%20V%C3%ADt%20List%C3%ADk%20and%20S%C5%82awek%20Kwasiborski%20and%20Rachel%20Saputro%20and%20Adnan%20Ozturel%20and%20Ganesh%20Mallya%20and%20Kushal%20Majmundar%20and%20Ross%20West%20and%20Paul%20Caron%20and%20Jinliang%20Wei%20and%20Lluis%20Castrejon%20and%20Sharad%20Vikram%20and%20Deepak%20Ramachandran%20and%20Nikhil%20Dhawan%20and%20Jiho%20Park%20and%20Sara%20Smoot%20and%20George%20van%20den%20Driessche%20and%20Yochai%20Blau%20and%20Chase%20Malik%20and%20Wei%20Liang%20and%20Roy%20Hirsch%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Eugene%20Weinstein%20and%20A%C3%A4ron%20van%20den%20Oord%20and%20Sid%20Lall%20and%20Nicholas%20FitzGerald%20and%20Zixuan%20Jiang%20and%20Xuan%20Yang%20and%20Dale%20Webster%20and%20Ali%20Elqursh%20and%20Aedan%20Pope%20and%20Georges%20Rotival%20and%20David%20Raposo%20and%20Wanzheng%20Zhu%20and%20Jeff%20Dean%20and%20Sami%20Alabed%20and%20Dustin%20Tran%20and%20Arushi%20Gupta%20and%20Zach%20Gleicher%20and%20Jessica%20Austin%20and%20Edouard%20Rosseel%20and%20Megh%20Umekar%20and%20Dipanjan%20Das%20and%20Yinghao%20Sun%20and%20Kai%20Chen%20and%20Karolis%20Misiunas%20and%20Xiang%20Zhou%20and%20Yixian%20Di%20and%20Alyssa%20Loo%20and%20Josh%20Newlan%20and%20Bo%20Li%20and%20Vinay%20Ramasesh%20and%20Ying%20Xu%20and%20Alex%20Chen%20and%20Sudeep%20Gandhe%20and%20Radu%20Soricut%20and%20Nikita%20Gupta%20and%20Shuguang%20Hu%20and%20Seliem%20El-Sayed%20and%20Xavier%20Garcia%20and%20Idan%20Brusilovsky%20and%20Pu-Chin%20Chen%20and%20Andrew%20Bolt%20and%20Lu%20Huang%20and%20Alex%20Gurney%20and%20Zhiying%20Zhang%20and%20Alexander%20Pritzel%20and%20Jarek%20Wilkiewicz%20and%20Bryan%20Seybold%20and%20Bhargav%20Kanagal%20Shamanna%20and%20Felix%20Fischer%20and%20Josef%20Dean%20and%20Karan%20Gill%20and%20Ross%20Mcilroy%20and%20Abhishek%20Bhowmick%20and%20Jeremy%20Selier%20and%20Antoine%20Yang%20and%20Derek%20Cheng%20and%20Vladimir%20Magay%20and%20Jie%20Tan%20and%20Dhriti%20Varma%20and%20Christian%20Walder%20and%20Tomas%20Kocisky%20and%20Ryo%20Nakashima%20and%20Paul%20Natsev%20and%20Mike%20Kwong%20and%20Ionel%20Gog%20and%20Chiyuan%20Zhang%20and%20Sander%20Dieleman%20and%20Thomas%20Jimma%20and%20Andrey%20Ryabtsev%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Dayou%20Du%20and%20Ante%20%C5%BDu%C5%BEul%20and%20Mislav%20%C5%BDani%C4%87%20and%20Mukund%20Raghavachari%20and%20Willi%20Gierke%20and%20Zeyu%20Zheng%20and%20Dessie%20Petrova%20and%20Yann%20Dauphin%20and%20Yuchuan%20Liu%20and%20Ido%20Kessler%20and%20Steven%20Hand%20and%20Chris%20Duvarney%20and%20Seokhwan%20Kim%20and%20Hyo%20Lee%20and%20L%C3%A9onard%20Hussenot%20and%20Jeffrey%20Hui%20and%20Josh%20Smith%20and%20Deepali%20Jain%20and%20Jiawei%20Xia%20and%20Gaurav%20Singh%20Tomar%20and%20Keyvan%20Amiri%20and%20Du%20Phan%20and%20Fabian%20Fuchs%20and%20Tobias%20Weyand%20and%20Nenad%20Tomasev%20and%20Alexandra%20Cordell%20and%20Xin%20Liu%20and%20Jonathan%20Mallinson%20and%20Pankaj%20Joshi%20and%20Andy%20Crawford%20and%20Arun%20Suggala%20and%20Steve%20Chien%20and%20Nick%20Fernando%20and%20Mariella%20Sanchez-Vargas%20and%20Duncan%20Williams%20and%20Phil%20Crone%20and%20Xiyang%20Luo%20and%20Igor%20Karpov%20and%20Jyn%20Shan%20and%20Terry%20Thurk%20and%20Robin%20Strudel%20and%20Paul%20Voigtlaender%20and%20Piyush%20Patil%20and%20Tim%20Dozat%20and%20Ali%20Khodaei%20and%20Sahil%20Singla%20and%20Piotr%20Ambroszczyk%20and%20Qiyin%20Wu%20and%20Yifan%20Chang%20and%20Brian%20Roark%20and%20Chaitra%20Hegde%20and%20Tianli%20Ding%20and%20Angelos%20Filos%20and%20Zhongru%20Wu%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Shuang%20Liu%20and%20Saarthak%20Khanna%20and%20Aditya%20Pandey%20and%20Siobhan%20Mcloughlin%20and%20Qiujia%20Li%20and%20Sam%20Haves%20and%20Allan%20Zhou%20and%20Elena%20Buchatskaya%20and%20Isabel%20Leal%20and%20Peter%20de%20Boursac%20and%20Nami%20Akazawa%20and%20Nina%20Anderson%20and%20Terry%20Chen%20and%20Krishna%20Somandepalli%20and%20Chen%20Liang%20and%20Sheela%20Goenka%20and%20Stephanie%20Winkler%20and%20Alexander%20Grushetsky%20and%20Yifan%20Ding%20and%20Jamie%20Smith%20and%20Fan%20Ye%20and%20Jordi%20Pont-Tuset%20and%20Eric%20Li%20and%20Ruichao%20Li%20and%20Tomer%20Golany%20and%20Dawid%20Wegner%20and%20Tao%20Jiang%20and%20Omer%20Barak%20and%20Yuan%20Shangguan%20and%20Eszter%20V%C3%A9rtes%20and%20Renee%20Wong%20and%20J%C3%B6rg%20Bornschein%20and%20Alex%20Tudor%20and%20Michele%20Bevilacqua%20and%20Tom%20Schaul%20and%20Ankit%20Singh%20Rawat%20and%20Yang%20Zhao%20and%20Kyriakos%20Axiotis%20and%20Lei%20Meng%20and%20Cory%20McLean%20and%20Jonathan%20Lai%20and%20Jennifer%20Beattie%20and%20Nate%20Kushman%20and%20Yaxin%20Liu%20and%20Blair%20Kutzman%20and%20Fiona%20Lang%20and%20Jingchen%20Ye%20and%20Praneeth%20Netrapalli%20and%20Pushkar%20Mishra%20and%20Myriam%20Khan%20and%20Megha%20Goel%20and%20Rob%20Willoughby%20and%20David%20Tian%20and%20Honglei%20Zhuang%20and%20JD%20Chen%20and%20Zak%20Tsai%20and%20Tasos%20Kementsietsidis%20and%20Arjun%20Khare%20and%20James%20Keeling%20and%20Keyang%20Xu%20and%20Nathan%20Waters%20and%20Florent%20Altch%C3%A9%20and%20Ashok%20Popat%20and%20Bhavishya%20Mittal%20and%20David%20Saxton%20and%20Dalia%20El%20Badawy%20and%20Michael%20Mathieu%20and%20Zheng%20Zheng%20and%20Hao%20Zhou%20and%20Nishant%20Ranka%20and%20Richard%20Shin%20and%20Qingnan%20Duan%20and%20Tim%20Salimans%20and%20Ioana%20Mihailescu%20and%20Uri%20Shaham%20and%20Ming-Wei%20Chang%20and%20Yannis%20Assael%20and%20Nishanth%20Dikkala%20and%20Martin%20Izzard%20and%20Vincent%20Cohen-Addad%20and%20Cat%20Graves%20and%20Vlad%20Feinberg%20and%20Grace%20Chung%20and%20DJ%20Strouse%20and%20Danny%20Karmon%20and%20Sahand%20Sharifzadeh%20and%20Zoe%20Ashwood%20and%20Khiem%20Pham%20and%20Jon%20Blanton%20and%20Alex%20Vasiloff%20and%20Jarred%20Barber%20and%20Mark%20Geller%20and%20Aurick%20Zhou%20and%20Fedir%20Zubach%20and%20Tzu-Kuo%20Huang%20and%20Lei%20Zhang%20and%20Himanshu%20Gupta%20and%20Matt%20Young%20and%20Julia%20Proskurnia%20and%20Ronny%20Votel%20and%20Valentin%20Gabeur%20and%20Gabriel%20Barcik%20and%20Aditya%20Tripathi%20and%20Hongkun%20Yu%20and%20Geng%20Yan%20and%20Beer%20Changpinyo%20and%20Filip%20Paveti%C4%87%20and%20Amy%20Coyle%20and%20Yasuhisa%20Fujii%20and%20Jorge%20Gonzalez%20Mendez%20and%20Tianhao%20Zhou%20and%20Harish%20Rajamani%20and%20Blake%20Hechtman%20and%20Eddie%20Cao%20and%20Da-Cheng%20Juan%20and%20Yi-Xuan%20Tan%20and%20Valentin%20Dalibard%20and%20Yilun%20Du%20and%20Natalie%20Clay%20and%20Kaisheng%20Yao%20and%20Wenhao%20Jia%20and%20Dimple%20Vijaykumar%20and%20Yuxiang%20Zhou%20and%20Xinyi%20Bai%20and%20Wei-Chih%20Hung%20and%20Steven%20Pecht%20and%20Georgi%20Todorov%20and%20Nikhil%20Khadke%20and%20Pramod%20Gupta%20and%20Preethi%20Lahoti%20and%20Arnaud%20Autef%20and%20Karthik%20Duddu%20and%20James%20Lee-Thorp%20and%20Alexander%20Bykovsky%20and%20Tautvydas%20Misiunas%20and%20Sebastian%20Flennerhag%20and%20Santhosh%20Thangaraj%20and%20Jed%20McGiffin%20and%20Zack%20Nado%20and%20Markus%20Kunesch%20and%20Andreas%20Noever%20and%20Amir%20Hertz%20and%20Marco%20Liang%20and%20Victor%20Stone%20and%20Evan%20Palmer%20and%20Samira%20Daruki%20and%20Arijit%20Pramanik%20and%20Siim%20P%C3%B5der%20and%20Austin%20Kyker%20and%20Mina%20Khan%20and%20Evgeny%20Sluzhaev%20and%20Marvin%20Ritter%20and%20Avraham%20Ruderman%20and%20Wenlei%20Zhou%20and%20Chirag%20Nagpal%20and%20Kiran%20Vodrahalli%20and%20George%20Necula%20and%20Paul%20Barham%20and%20Ellie%20Pavlick%20and%20Jay%20Hartford%20and%20Izhak%20Shafran%20and%20Long%20Zhao%20and%20Maciej%20Miku%C5%82a%20and%20Tom%20Eccles%20and%20Hidetoshi%20Shimokawa%20and%20Kanav%20Garg%20and%20Luke%20Vilnis%20and%20Hanwen%20Chen%20and%20Ilia%20Shumailov%20and%20Kuang-Huei%20Lee%20and%20Abdelrahman%20Abdelhamed%20and%20Meiyan%20Xie%20and%20Vered%20Cohen%20and%20Ester%20Hlavnova%20and%20Dan%20Malkin%20and%20Chawin%20Sitawarin%20and%20James%20Lottes%20and%20Pauline%20Coquinot%20and%20Tianli%20Yu%20and%20Sandeep%20Kumar%20and%20Jingwei%20Zhang%20and%20Aroma%20Mahendru%20and%20Zafarali%20Ahmed%20and%20James%20Martens%20and%20Tao%20Chen%20and%20Aviel%20Boag%20and%20Daiyi%20Peng%20and%20Coline%20Devin%20and%20Arseniy%20Klimovskiy%20and%20Mary%20Phuong%20and%20Danny%20Vainstein%20and%20Jin%20Xie%20and%20Bhuvana%20Ramabhadran%20and%20Nathan%20Howard%20and%20Xinxin%20Yu%20and%20Gitartha%20Goswami%20and%20Jingyu%20Cui%20and%20Sam%20Shleifer%20and%20Mario%20Pinto%20and%20Chih-Kuan%20Yeh%20and%20Ming-Hsuan%20Yang%20and%20Sara%20Javanmardi%20and%20Dan%20Ethier%20and%20Chace%20Lee%20and%20Jordi%20Orbay%20and%20Suyog%20Kotecha%20and%20Carla%20Bromberg%20and%20Pete%20Shaw%20and%20James%20Thornton%20and%20Adi%20Gerzi%20Rosenthal%20and%20Shane%20Gu%20and%20Matt%20Thomas%20and%20Ian%20Gemp%20and%20Aditya%20Ayyar%20and%20Asahi%20Ushio%20and%20Aarush%20Selvan%20and%20Joel%20Wee%20and%20Chenxi%20Liu%20and%20Maryam%20Majzoubi%20and%20Weiren%20Yu%20and%20Jake%20Abernethy%20and%20Tyler%20Liechty%20and%20Renke%20Pan%20and%20Hoang%20Nguyen%20and%20%20Qiong%20and%20%20Hu%20and%20Sarah%20Perrin%20and%20Abhinav%20Arora%20and%20Emily%20Pitler%20and%20Weiyi%20Wang%20and%20Kaushik%20Shivakumar%20and%20Flavien%20Prost%20and%20Ben%20Limonchik%20and%20Jing%20Wang%20and%20Yi%20Gao%20and%20Timothee%20Cour%20and%20Shyamal%20Buch%20and%20Huan%20Gui%20and%20Maria%20Ivanova%20and%20Philipp%20Neubeck%20and%20Kelvin%20Chan%20and%20Lucy%20Kim%20and%20Huizhong%20Chen%20and%20Naman%20Goyal%20and%20Da-Woon%20Chung%20and%20Lu%20Liu%20and%20Yao%20Su%20and%20Anastasia%20Petrushkina%20and%20Jiajun%20Shen%20and%20Armand%20Joulin%20and%20Yuanzhong%20Xu%20and%20Stein%20Xudong%20Lin%20and%20Yana%20Kulizhskaya%20and%20Ciprian%20Chelba%20and%20Shobha%20Vasudevan%20and%20Eli%20Collins%20and%20Vasilisa%20Bashlovkina%20and%20Tony%20Lu%20and%20Doug%20Fritz%20and%20Jongbin%20Park%20and%20Yanqi%20Zhou%20and%20Chen%20Su%20and%20Richard%20Tanburn%20and%20Mikhail%20Sushkov%20and%20Mitchelle%20Rasquinha%20and%20Jinning%20Li%20and%20Jennifer%20Prendki%20and%20Yiming%20Li%20and%20Pallavi%20LV%20and%20Shriya%20Sharma%20and%20Hen%20Fitoussi%20and%20Hui%20Huang%20and%20Andrew%20Dai%20and%20Phuong%20Dao%20and%20Mike%20Burrows%20and%20Henry%20Prior%20and%20Danfeng%20Qin%20and%20Golan%20Pundak%20and%20Lars%20Lowe%20Sjoesund%20and%20Art%20Khurshudov%20and%20Zhenkai%20Zhu%20and%20Albert%20Webson%20and%20Elizabeth%20Kemp%20and%20Tat%20Tan%20and%20Saurabh%20Agrawal%20and%20Susie%20Sargsyan%20and%20Liqun%20Cheng%20and%20Jim%20Stephan%20and%20Tom%20Kwiatkowski%20and%20David%20Reid%20and%20Arunkumar%20Byravan%20and%20Assaf%20Hurwitz%20Michaely%20and%20Nicolas%20Heess%20and%20Luowei%20Zhou%20and%20Sonam%20Goenka%20and%20Viral%20Carpenter%20and%20Anselm%20Levskaya%20and%20Bo%20Wang%20and%20Reed%20Roberts%20and%20R%C3%A9mi%20Leblond%20and%20Sharat%20Chikkerur%20and%20Stav%20Ginzburg%20and%20Max%20Chang%20and%20Robert%20Riachi%20and%20%20Chuqiao%20and%20%20Xu%20and%20Zal%C3%A1n%20Borsos%20and%20Michael%20Pliskin%20and%20Julia%20Pawar%20and%20Morgane%20Lustman%20and%20Hannah%20Kirkwood%20and%20Ankit%20Anand%20and%20Aditi%20Chaudhary%20and%20Norbert%20Kalb%20and%20Kieran%20Milan%20and%20Sean%20Augenstein%20and%20Anna%20Goldie%20and%20Laurel%20Prince%20and%20Karthik%20Raman%20and%20Yanhua%20Sun%20and%20Vivian%20Xia%20and%20Aaron%20Cohen%20and%20Zhouyuan%20Huo%20and%20Josh%20Camp%20and%20Seher%20Ellis%20and%20Lukas%20Zilka%20and%20David%20Vilar%20Torres%20and%20Lisa%20Patel%20and%20Sho%20Arora%20and%20Betty%20Chan%20and%20Jonas%20Adler%20and%20Kareem%20Ayoub%20and%20Jacky%20Liang%20and%20Fayaz%20Jamil%20and%20Jiepu%20Jiang%20and%20Simon%20Baumgartner%20and%20Haitian%20Sun%20and%20Yael%20Karov%20and%20Yaroslav%20Akulov%20and%20Hui%20Zheng%20and%20Irene%20Cai%20and%20Claudio%20Fantacci%20and%20James%20Rubin%20and%20Alex%20Rav%20Acha%20and%20Mengchao%20Wang%20and%20Nina%20D%27Souza%20and%20Rohit%20Sathyanarayana%20and%20Shengyang%20Dai%20and%20Simon%20Rowe%20and%20Andrey%20Simanovsky%20and%20Omer%20Goldman%20and%20Yuheng%20Kuang%20and%20Xiaoyue%20Pan%20and%20Andrew%20Rosenberg%20and%20Tania%20Rojas-Esponda%20and%20Praneet%20Dutta%20and%20Amy%20Zeng%20and%20Irina%20Jurenka%20and%20Greg%20Farquhar%20and%20Yamini%20Bansal%20and%20Shariq%20Iqbal%20and%20Becca%20Roelofs%20and%20Ga-Young%20Joung%20and%20Parker%20Beak%20and%20Changwan%20Ryu%20and%20Ryan%20Poplin%20and%20Yan%20Wu%20and%20Jean-Baptiste%20Alayrac%20and%20Senaka%20Buthpitiya%20and%20Olaf%20Ronneberger%20and%20Caleb%20Habtegebriel%20and%20Wei%20Li%20and%20Paul%20Cavallaro%20and%20Aurora%20Wei%20and%20Guy%20Bensky%20and%20Timo%20Denk%20and%20Harish%20Ganapathy%20and%20Jeff%20Stanway%20and%20Pratik%20Joshi%20and%20Francesco%20Bertolini%20and%20Jessica%20Lo%20and%20Olivia%20Ma%20and%20Zachary%20Charles%20and%20Geta%20Sampemane%20and%20Himanshu%20Sahni%20and%20Xu%20Chen%20and%20Harry%20Askham%20and%20David%20Gaddy%20and%20Peter%20Young%20and%20Jiewen%20Tan%20and%20Matan%20Eyal%20and%20Arthur%20Bra%C5%BEinskas%20and%20Li%20Zhong%20and%20Zhichun%20Wu%20and%20Mark%20Epstein%20and%20Kai%20Bailey%20and%20Andrew%20Hard%20and%20Kamyu%20Lee%20and%20Sasha%20Goldshtein%20and%20Alex%20Ruiz%20and%20Mohammed%20Badawi%20and%20Matthias%20Lochbrunner%20and%20JK%20Kearns%20and%20Ashley%20Brown%20and%20Fabio%20Pardo%20and%20Theophane%20Weber%20and%20Haichuan%20Yang%20and%20Pan-Pan%20Jiang%20and%20Berkin%20Akin%20and%20Zhao%20Fu%20and%20Marcus%20Wainwright%20and%20Chi%20Zou%20and%20Meenu%20Gaba%20and%20Pierre-Antoine%20Manzagol%20and%20Wendy%20Kan%20and%20Yang%20Song%20and%20Karina%20Zainullina%20and%20Rui%20Lin%20and%20Jeongwoo%20Ko%20and%20Salil%20Deshmukh%20and%20Apoorv%20Jindal%20and%20James%20Svensson%20and%20Divya%20Tyam%20and%20Heri%20Zhao%20and%20Christine%20Kaeser-Chen%20and%20Scott%20Baird%20and%20Pooya%20Moradi%20and%20Jamie%20Hall%20and%20Qiuchen%20Guo%20and%20Vincent%20Tsang%20and%20Bowen%20Liang%20and%20Fernando%20Pereira%20and%20Suhas%20Ganesh%20and%20Ivan%20Korotkov%20and%20Jakub%20Adamek%20and%20Sridhar%20Thiagarajan%20and%20Vinh%20Tran%20and%20Charles%20Chen%20and%20Chris%20Tar%20and%20Sanil%20Jain%20and%20Ishita%20Dasgupta%20and%20Taylan%20Bilal%20and%20David%20Reitter%20and%20Kai%20Zhao%20and%20Giulia%20Vezzani%20and%20Yasmin%20Gehman%20and%20Pulkit%20Mehta%20and%20Lauren%20Beltrone%20and%20Xerxes%20Dotiwalla%20and%20Sergio%20Guadarrama%20and%20Zaheer%20Abbas%20and%20Stefani%20Karp%20and%20Petko%20Georgiev%20and%20Chun-Sung%20Ferng%20and%20Marc%20Brockschmidt%20and%20Liqian%20Peng%20and%20Christoph%20Hirnschall%20and%20Vikas%20Verma%20and%20Yingying%20Bi%20and%20Ying%20Xiao%20and%20Avigail%20Dabush%20and%20Kelvin%20Xu%20and%20Phil%20Wallis%20and%20Randall%20Parker%20and%20Qifei%20Wang%20and%20Yang%20Xu%20and%20Ilkin%20Safarli%20and%20Dinesh%20Tewari%20and%20Yin%20Zhang%20and%20Seungyeon%20Kim%20and%20Andrea%20Gesmundo%20and%20Mackenzie%20Thomas%20and%20Sergey%20Levi%20and%20Ahmed%20Chowdhury%20and%20Kanishka%20Rao%20and%20Peter%20Garst%20and%20Sam%20Conway-Rahman%20and%20Helen%20Ran%20and%20Kay%20McKinney%20and%20Zhisheng%20Xiao%20and%20Wenhao%20Yu%20and%20Rohan%20Agrawal%20and%20Axel%20Stjerngren%20and%20Catalin%20Ionescu%20and%20Jingjing%20Chen%20and%20Vivek%20Sharma%20and%20Justin%20Chiu%20and%20Fei%20Liu%20and%20Ken%20Franko%20and%20Clayton%20Sanford%20and%20Xingyu%20Cai%20and%20Paul%20Michel%20and%20Sanjay%20Ganapathy%20and%20Jane%20Labanowski%20and%20Zachary%20Garrett%20and%20Ben%20Vargas%20and%20Sean%20Sun%20and%20Bryan%20Gale%20and%20Thomas%20Buschmann%20and%20Guillaume%20Desjardins%20and%20Nimesh%20Ghelani%20and%20Palak%20Jain%20and%20Mudit%20Verma%20and%20Chulayuth%20Asawaroengchai%20and%20Julian%20Eisenschlos%20and%20Jitendra%20Harlalka%20and%20Hideto%20Kazawa%20and%20Don%20Metzler%20and%20Joshua%20Howland%20and%20Ying%20Jian%20and%20Jake%20Ades%20and%20Viral%20Shah%20and%20Tynan%20Gangwani%20and%20Seungji%20Lee%20and%20Roman%20Ring%20and%20Steven%20M.%20Hernandez%20and%20Dean%20Reich%20and%20Amer%20Sinha%20and%20Ashutosh%20Sathe%20and%20Joe%20Kovac%20and%20Ashleah%20Gill%20and%20Ajay%20Kannan%20and%20Andrea%20D%27olimpio%20and%20Martin%20Sevenich%20and%20Jay%20Whang%20and%20Been%20Kim%20and%20Khe%20Chai%20Sim%20and%20Jilin%20Chen%20and%20Jiageng%20Zhang%20and%20Shuba%20Lall%20and%20Yossi%20Matias%20and%20Bill%20Jia%20and%20Abe%20Friesen%20and%20Sara%20Nasso%20and%20Ashish%20Thapliyal%20and%20Bryan%20Perozzi%20and%20Ting%20Yu%20and%20Anna%20Shekhawat%20and%20Safeen%20Huda%20and%20Peter%20Grabowski%20and%20Eric%20Wang%20and%20Ashwin%20Sreevatsa%20and%20Hilal%20Dib%20and%20Mehadi%20Hassen%20and%20Parker%20Schuh%20and%20Vedrana%20Milutinovic%20and%20Chris%20Welty%20and%20Michael%20Quinn%20and%20Ali%20Shah%20and%20Bangju%20Wang%20and%20Gabe%20Barth-Maron%20and%20Justin%20Frye%20and%20Natalie%20Axelsson%20and%20Tao%20Zhu%20and%20Yukun%20Ma%20and%20Irene%20Giannoumis%20and%20Hanie%20Sedghi%20and%20Chang%20Ye%20and%20Yi%20Luan%20and%20Kevin%20Aydin%20and%20Bilva%20Chandra%20and%20Vivek%20Sampathkumar%20and%20Ronny%20Huang%20and%20Victor%20Lavrenko%20and%20Ahmed%20Eleryan%20and%20Zhi%20Hong%20and%20Steven%20Hansen%20and%20Sara%20Mc%20Carthy%20and%20Bidisha%20Samanta%20and%20Domagoj%20%C4%86evid%20and%20Xin%20Wang%20and%20Fangtao%20Li%20and%20Michael%20Voznesensky%20and%20Matt%20Hoffman%20and%20Andreas%20Terzis%20and%20Vikash%20Sehwag%20and%20Gil%20Fidel%20and%20Luheng%20He%20and%20Mu%20Cai%20and%20Yanzhang%20He%20and%20Alex%20Feng%20and%20Martin%20Nikoltchev%20and%20Samrat%20Phatale%20and%20Jason%20Chase%20and%20Rory%20Lawton%20and%20Ming%20Zhang%20and%20Tom%20Ouyang%20and%20Manuel%20Tragut%20and%20Mehdi%20Hafezi%20Manshadi%20and%20Arjun%20Narayanan%20and%20Jiaming%20Shen%20and%20Xu%20Gao%20and%20Tolga%20Bolukbasi%20and%20Nick%20Roy%20and%20Xin%20Li%20and%20Daniel%20Golovin%20and%20Liviu%20Panait%20and%20Zhen%20Qin%20and%20Guangxing%20Han%20and%20Thomas%20Anthony%20and%20Sneha%20Kudugunta%20and%20Viorica%20Patraucean%20and%20Aniket%20Ray%20and%20Xinyun%20Chen%20and%20Xiaochen%20Yang%20and%20Tanuj%20Bhatia%20and%20Pranav%20Talluri%20and%20Alex%20Morris%20and%20Andrija%20Ra%C5%BEnatovi%C4%87%20and%20Bethanie%20Brownfield%20and%20James%20An%20and%20Sheng%20Peng%20and%20Patrick%20Kane%20and%20Ce%20Zheng%20and%20Nico%20Duduta%20and%20Joshua%20Kessinger%20and%20James%20Noraky%20and%20Siqi%20Liu%20and%20Keran%20Rong%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Keith%20Rush%20and%20Alex%20Goldin%20and%20Fanny%20Wei%20and%20Shiva%20Mohan%20Reddy%20Garlapati%20and%20Caroline%20Pantofaru%20and%20Okwan%20Kwon%20and%20Jianmo%20Ni%20and%20Eric%20Noland%20and%20Julia%20Di%20Trapani%20and%20Fran%C3%A7oise%20Beaufays%20and%20Abhijit%20Guha%20Roy%20and%20Yinlam%20Chow%20and%20Aybuke%20Turker%20and%20Geoffrey%20Cideron%20and%20Lantao%20Mei%20and%20Jon%20Clark%20and%20Qingyun%20Dou%20and%20Matko%20Bo%C5%A1njak%20and%20Ralph%20Leith%20and%20Yuqing%20Du%20and%20Amir%20Yazdanbakhsh%20and%20Milad%20Nasr%20and%20Chester%20Kwak%20and%20Suraj%20Satishkumar%20Sheth%20and%20Alex%20Kaskasoli%20and%20Ankesh%20Anand%20and%20Balaji%20Lakshminarayanan%20and%20Sammy%20Jerome%20and%20David%20Bieber%20and%20Chun-Te%20Chu%20and%20Alexandre%20Senges%20and%20Tianxiao%20Shen%20and%20Mukund%20Sridhar%20and%20Ndaba%20Ndebele%20and%20Benjamin%20Beyret%20and%20Shakir%20Mohamed%20and%20Mia%20Chen%20and%20Markus%20Freitag%20and%20Jiaxian%20Guo%20and%20Luyang%20Liu%20and%20Paul%20Roit%20and%20Heng%20Chen%20and%20Shen%20Yan%20and%20Tom%20Stone%20and%20JD%20Co-Reyes%20and%20Jeremy%20Cole%20and%20Salvatore%20Scellato%20and%20Shekoofeh%20Azizi%20and%20Hadi%20Hashemi%20and%20Alicia%20Jin%20and%20Anand%20Iyer%20and%20Marcella%20Valentine%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Arun%20Ahuja%20and%20Daniel%20Hernandez%20Diaz%20and%20Chen-Yu%20Lee%20and%20Nathan%20Clement%20and%20Weize%20Kong%20and%20Drew%20Garmon%20and%20Ishaan%20Watts%20and%20Kush%20Bhatia%20and%20Khyatti%20Gupta%20and%20Matt%20Miecnikowski%20and%20Hugo%20Vallet%20and%20Ankur%20Taly%20and%20Edward%20Loper%20and%20Saket%20Joshi%20and%20James%20Atwood%20and%20Jo%20Chick%20and%20Mark%20Collier%20and%20Fotis%20Iliopoulos%20and%20Ryan%20Trostle%20and%20Beliz%20Gunel%20and%20Ramiro%20Leal-Cavazos%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Michael%20Guzman%20and%20Xiaoen%20Ju%20and%20Andy%20Forbes%20and%20Jesse%20Emond%20and%20Kushal%20Chauhan%20and%20Ben%20Caine%20and%20Li%20Xiao%20and%20Wenjun%20Zeng%20and%20Alexandre%20Moufarek%20and%20Daniel%20Murphy%20and%20Maya%20Meng%20and%20Nitish%20Gupta%20and%20Felix%20Riedel%20and%20Anil%20Das%20and%20Elijah%20Lawal%20and%20Shashi%20Narayan%20and%20Tiberiu%20Sosea%20and%20James%20Swirhun%20and%20Linda%20Friso%20and%20Behnam%20Neyshabur%20and%20Jing%20Lu%20and%20Sertan%20Girgin%20and%20Michael%20Wunder%20and%20Edouard%20Yvinec%20and%20Aroonalok%20Pyne%20and%20Victor%20Carbune%20and%20Shruti%20Rijhwani%20and%20Yang%20Guo%20and%20Tulsee%20Doshi%20and%20Anton%20Briukhov%20and%20Max%20Bain%20and%20Ayal%20Hitron%20and%20Xuanhui%20Wang%20and%20Ashish%20Gupta%20and%20Ke%20Chen%20and%20Cosmo%20Du%20and%20Weiyang%20Zhang%20and%20Dhruv%20Shah%20and%20Arjun%20Akula%20and%20Max%20Dylla%20and%20Ashyana%20Kachra%20and%20Weicheng%20Kuo%20and%20Tingting%20Zou%20and%20Lily%20Wang%20and%20Luyao%20Xu%20and%20Jifan%20Zhu%20and%20Justin%20Snyder%20and%20Sachit%20Menon%20and%20Orhan%20Firat%20and%20Igor%20Mordatch%20and%20Yuan%20Yuan%20and%20Natalia%20Ponomareva%20and%20Rory%20Blevins%20and%20Lawrence%20Moore%20and%20Weijun%20Wang%20and%20Phil%20Chen%20and%20Martin%20Scholz%20and%20Artur%20Dwornik%20and%20Jason%20Lin%20and%20Sicheng%20Li%20and%20Diego%20Antognini%20and%20Te%20I%20and%20Xiaodan%20Song%20and%20Matt%20Miller%20and%20Uday%20Kalra%20and%20Adam%20Raveret%20and%20Oscar%20Akerlund%20and%20Felix%20Wu%20and%20Andrew%20Nystrom%20and%20Namrata%20Godbole%20and%20Tianqi%20Liu%20and%20Hannah%20DeBalsi%20and%20Jewel%20Zhao%20and%20Buhuang%20Liu%20and%20Avi%20Caciularu%20and%20Lauren%20Lax%20and%20Urvashi%20Khandelwal%20and%20Victoria%20Langston%20and%20Eric%20Bailey%20and%20Silvio%20Lattanzi%20and%20Yufei%20Wang%20and%20Neel%20Kovelamudi%20and%20Sneha%20Mondal%20and%20Guru%20Guruganesh%20and%20Nan%20Hua%20and%20Ofir%20Roval%20and%20Pawe%C5%82%20Weso%C5%82owski%20and%20Rishikesh%20Ingale%20and%20Jonathan%20Halcrow%20and%20Tim%20Sohn%20and%20Christof%20Angermueller%20and%20Bahram%20Raad%20and%20Eli%20Stickgold%20and%20Eva%20Lu%20and%20Alec%20Kosik%20and%20Jing%20Xie%20and%20Timothy%20Lillicrap%20and%20Austin%20Huang%20and%20Lydia%20Lihui%20Zhang%20and%20Dominik%20Paulus%20and%20Clement%20Farabet%20and%20Alex%20Wertheim%20and%20Bing%20Wang%20and%20Rishabh%20Joshi%20and%20Chu-ling%20Ko%20and%20Yonghui%20Wu%20and%20Shubham%20Agrawal%20and%20Lily%20Lin%20and%20XiangHai%20Sheng%20and%20Peter%20Sung%20and%20Tyler%20Breland-King%20and%20Christina%20Butterfield%20and%20Swapnil%20Gawde%20and%20Sumeet%20Singh%20and%20Qiao%20Zhang%20and%20Raj%20Apte%20and%20Shilpa%20Shetty%20and%20Adrian%20Hutter%20and%20Tao%20Li%20and%20Elizabeth%20Salesky%20and%20Federico%20Lebron%20and%20Jonni%20Kanerva%20and%20Michela%20Paganini%20and%20Arthur%20Nguyen%20and%20Rohith%20Vallu%20and%20Jan-Thorsten%20Peter%20and%20Sarmishta%20Velury%20and%20David%20Kao%20and%20Jay%20Hoover%20and%20Anna%20Bortsova%20and%20Colton%20Bishop%20and%20Shoshana%20Jakobovits%20and%20Alessandro%20Agostini%20and%20Alekh%20Agarwal%20and%20Chang%20Liu%20and%20Charles%20Kwong%20and%20Sasan%20Tavakkol%20and%20Ioana%20Bica%20and%20Alex%20Greve%20and%20Anirudh%20GP%20and%20Jake%20Marcus%20and%20Le%20Hou%20and%20Tom%20Duerig%20and%20Rivka%20Moroshko%20and%20Dave%20Lacey%20and%20Andy%20Davis%20and%20Julien%20Amelot%20and%20Guohui%20Wang%20and%20Frank%20Kim%20and%20Theofilos%20Strinopoulos%20and%20Hui%20Wan%20and%20Charline%20Le%20Lan%20and%20Shankar%20Krishnan%20and%20Haotian%20Tang%20and%20Peter%20Humphreys%20and%20Junwen%20Bai%20and%20Idan%20Heimlich%20Shtacher%20and%20Diego%20Machado%20and%20Chenxi%20Pang%20and%20Ken%20Burke%20and%20Dangyi%20Liu%20and%20Renga%20Aravamudhan%20and%20Yue%20Song%20and%20Ed%20Hirst%20and%20Abhimanyu%20Singh%20and%20Brendan%20Jou%20and%20Liang%20Bai%20and%20Francesco%20Piccinno%20and%20Chuyuan%20Kelly%20Fu%20and%20Robin%20Alazard%20and%20Barak%20Meiri%20and%20Daniel%20Winter%20and%20Charlie%20Chen%20and%20Mingda%20Zhang%20and%20Jens%20Heitkaemper%20and%20John%20Lambert%20and%20Jinhyuk%20Lee%20and%20Alexander%20Fr%C3%B6mmgen%20and%20Sergey%20Rogulenko%20and%20Pranav%20Nair%20and%20Paul%20Niemczyk%20and%20Anton%20Bulyenov%20and%20Bibo%20Xu%20and%20Hadar%20Shemtov%20and%20Morteza%20Zadimoghaddam%20and%20Serge%20Toropov%20and%20Mateo%20Wirth%20and%20Hanjun%20Dai%20and%20Sreenivas%20Gollapudi%20and%20Daniel%20Zheng%20and%20Alex%20Kurakin%20and%20Chansoo%20Lee%20and%20Kalesha%20Bullard%20and%20Nicolas%20Serrano%20and%20Ivana%20Balazevic%20and%20Yang%20Li%20and%20Johan%20Schalkwyk%20and%20Mark%20Murphy%20and%20Mingyang%20Zhang%20and%20Kevin%20Sequeira%20and%20Romina%20Datta%20and%20Nishant%20Agrawal%20and%20Charles%20Sutton%20and%20Nithya%20Attaluri%20and%20Mencher%20Chiang%20and%20Wael%20Farhan%20and%20Gregory%20Thornton%20and%20Kate%20Lin%20and%20Travis%20Choma%20and%20Hung%20Nguyen%20and%20Kingshuk%20Dasgupta%20and%20Dirk%20Robinson%20and%20Iulia%20Com%C5%9Fa%20and%20Michael%20Riley%20and%20Arjun%20Pillai%20and%20Basil%20Mustafa%20and%20Ben%20Golan%20and%20Amir%20Zandieh%20and%20Jean-Baptiste%20Lespiau%20and%20Billy%20Porter%20and%20David%20Ross%20and%20Sujeevan%20Rajayogam%20and%20Mohit%20Agarwal%20and%20Subhashini%20Venugopalan%20and%20Bobak%20Shahriari%20and%20Qiqi%20Yan%20and%20Hao%20Xu%20and%20Taylor%20Tobin%20and%20Pavel%20Dubov%20and%20Hongzhi%20Shi%20and%20Adri%C3%A0%20Recasens%20and%20Anton%20Kovsharov%20and%20Sebastian%20Borgeaud%20and%20Lucio%20Dery%20and%20Shanthal%20Vasanth%20and%20Elena%20Gribovskaya%20and%20Linhai%20Qiu%20and%20Mahdis%20Mahdieh%20and%20Wojtek%20Skut%20and%20Elizabeth%20Nielsen%20and%20CJ%20Zheng%20and%20Adams%20Yu%20and%20Carrie%20Grimes%20Bostock%20and%20Shaleen%20Gupta%20and%20Aaron%20Archer%20and%20Chris%20Rawles%20and%20Elinor%20Davies%20and%20Alexey%20Svyatkovskiy%20and%20Tomy%20Tsai%20and%20Yoni%20Halpern%20and%20Christian%20Reisswig%20and%20Bartek%20Wydrowski%20and%20Bo%20Chang%20and%20Joan%20Puigcerver%20and%20Mor%20Hazan%20Taege%20and%20Jian%20Li%20and%20Eva%20Schnider%20and%20Xinjian%20Li%20and%20Dragos%20Dena%20and%20Yunhan%20Xu%20and%20Umesh%20Telang%20and%20Tianze%20Shi%20and%20Heiga%20Zen%20and%20Kyle%20Kastner%20and%20Yeongil%20Ko%20and%20Neesha%20Subramaniam%20and%20Aviral%20Kumar%20and%20Pete%20Blois%20and%20Zhuyun%20Dai%20and%20John%20Wieting%20and%20Yifeng%20Lu%20and%20Yoel%20Zeldes%20and%20Tian%20Xie%20and%20Anja%20Hauth%20and%20Alexandru%20%C5%A2ifrea%20and%20Yuqi%20Li%20and%20Sam%20El-Husseini%20and%20Dan%20Abolafia%20and%20Howard%20Zhou%20and%20Wen%20Ding%20and%20Sahra%20Ghalebikesabi%20and%20Carlos%20Gu%C3%ADa%20and%20Andrii%20Maksai%20and%20%C3%81goston%20Weisz%20and%20Sercan%20Arik%20and%20Nick%20Sukhanov%20and%20Aga%20%C5%9Awietlik%20and%20Xuhui%20Jia%20and%20Luo%20Yu%20and%20Weiyue%20Wang%20and%20Mark%20Brand%20and%20Dawn%20Bloxwich%20and%20Sean%20Kirmani%20and%20Zhe%20Chen%20and%20Alec%20Go%20and%20Pablo%20Sprechmann%20and%20Nithish%20Kannen%20and%20Alen%20Carin%20and%20Paramjit%20Sandhu%20and%20Isabel%20Edkins%20and%20Leslie%20Nooteboom%20and%20Jai%20Gupta%20and%20Loren%20Maggiore%20and%20Javad%20Azizi%20and%20Yael%20Pritch%20and%20Pengcheng%20Yin%20and%20Mansi%20Gupta%20and%20Danny%20Tarlow%20and%20Duncan%20Smith%20and%20Desi%20Ivanov%20and%20Mohammad%20Babaeizadeh%20and%20Ankita%20Goel%20and%20Satish%20Kambala%20and%20Grace%20Chu%20and%20Matej%20Kastelic%20and%20Michelle%20Liu%20and%20Hagen%20Soltau%20and%20Austin%20Stone%20and%20Shivani%20Agrawal%20and%20Min%20Kim%20and%20Kedar%20Soparkar%20and%20Srinivas%20Tadepalli%20and%20Oskar%20Bunyan%20and%20Rachel%20Soh%20and%20Arvind%20Kannan%20and%20DY%20Kim%20and%20Blake%20JianHang%20Chen%20and%20Afief%20Halumi%20and%20Sudeshna%20Roy%20and%20Yulong%20Wang%20and%20Olcan%20Sercinoglu%20and%20Gena%20Gibson%20and%20Sijal%20Bhatnagar%20and%20Motoki%20Sano%20and%20Daniel%20von%20Dincklage%20and%20Qingchun%20Ren%20and%20Blagoj%20Mitrevski%20and%20Mirek%20Ol%C5%A1%C3%A1k%20and%20Jennifer%20She%20and%20Carl%20Doersch%20and%20%20Jilei%20and%20%20Wang%20and%20Bingyuan%20Liu%20and%20Qijun%20Tan%20and%20Tamar%20Yakar%20and%20Tris%20Warkentin%20and%20Alex%20Ramirez%20and%20Carl%20Lebsack%20and%20Josh%20Dillon%20and%20Rajiv%20Mathews%20and%20Tom%20Cobley%20and%20Zelin%20Wu%20and%20Zhuoyuan%20Chen%20and%20Jon%20Simon%20and%20Swaroop%20Nath%20and%20Tara%20Sainath%20and%20Alexei%20Bendebury%20and%20Ryan%20Julian%20and%20Bharath%20Mankalale%20and%20Daria%20%C4%86urko%20and%20Paulo%20Zacchello%20and%20Adam%20R.%20Brown%20and%20Kiranbir%20Sodhia%20and%20Heidi%20Howard%20and%20Sergi%20Caelles%20and%20Abhinav%20Gupta%20and%20Gareth%20Evans%20and%20Anna%20Bulanova%20and%20Lesley%20Katzen%20and%20Roman%20Goldenberg%20and%20Anton%20Tsitsulin%20and%20Joe%20Stanton%20and%20Benoit%20Schillings%20and%20Vitaly%20Kovalev%20and%20Corey%20Fry%20and%20Rushin%20Shah%20and%20Kuo%20Lin%20and%20Shyam%20Upadhyay%20and%20Cheng%20Li%20and%20Soroush%20Radpour%20and%20Marcello%20Maggioni%20and%20Jing%20Xiong%20and%20Lukas%20Haas%20and%20Jenny%20Brennan%20and%20Aishwarya%20Kamath%20and%20Nikolay%20Savinov%20and%20Arsha%20Nagrani%20and%20Trevor%20Yacovone%20and%20Ryan%20Kappedal%20and%20Kostas%20Andriopoulos%20and%20Li%20Lao%20and%20YaGuang%20Li%20and%20Grigory%20Rozhdestvenskiy%20and%20Kazuma%20Hashimoto%20and%20Andrew%20Audibert%20and%20Sophia%20Austin%20and%20Daniel%20Rodriguez%20and%20Anian%20Ruoss%20and%20Garrett%20Honke%20and%20Deep%20Karkhanis%20and%20Xi%20Xiong%20and%20Qing%20Wei%20and%20James%20Huang%20and%20Zhaoqi%20Leng%20and%20Vittal%20Premachandran%20and%20Stan%20Bileschi%20and%20Georgios%20Evangelopoulos%20and%20Thomas%20Mensink%20and%20Jay%20Pavagadhi%20and%20Denis%20Teplyashin%20and%20Paul%20Chang%20and%20Linting%20Xue%20and%20Garrett%20Tanzer%20and%20Sally%20Goldman%20and%20Kaushal%20Patel%20and%20Shixin%20Li%20and%20Jeremy%20Wiesner%20and%20Ivy%20Zheng%20and%20Ian%20Stewart-Binks%20and%20Jie%20Han%20and%20Zhi%20Li%20and%20Liangchen%20Luo%20and%20Karel%20Lenc%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Fuzhao%20Xue%20and%20Ryan%20Mullins%20and%20Alexey%20Guseynov%20and%20Chung-Ching%20Chang%20and%20Isaac%20Galatzer-Levy%20and%20Adam%20Zhang%20and%20Garrett%20Bingham%20and%20Grace%20Hu%20and%20Ale%20Hartman%20and%20Yue%20Ma%20and%20Jordan%20Griffith%20and%20Alex%20Irpan%20and%20Carey%20Radebaugh%20and%20Summer%20Yue%20and%20Lijie%20Fan%20and%20Victor%20Ungureanu%20and%20Christina%20Sorokin%20and%20Hannah%20Teufel%20and%20Peiran%20Li%20and%20Rohan%20Anil%20and%20Dimitris%20Paparas%20and%20Todd%20Wang%20and%20Chu-Cheng%20Lin%20and%20Hui%20Peng%20and%20Megan%20Shum%20and%20Goran%20Petrovic%20and%20Demetra%20Brady%20and%20Richard%20Nguyen%20and%20Klaus%20Macherey%20and%20Zhihao%20Li%20and%20Harman%20Singh%20and%20Madhavi%20Yenugula%20and%20Mariko%20Iinuma%20and%20Xinyi%20Chen%20and%20Kavya%20Kopparapu%20and%20Alexey%20Stern%20and%20Shachi%20Dave%20and%20Chandu%20Thekkath%20and%20Florence%20Perot%20and%20Anurag%20Kumar%20and%20Fangda%20Li%20and%20Yang%20Xiao%20and%20Matthew%20Bilotti%20and%20Mohammad%20Hossein%20Bateni%20and%20Isaac%20Noble%20and%20Lisa%20Lee%20and%20Amelio%20V%C3%A1zquez-Reina%20and%20Julian%20Salazar%20and%20Xiaomeng%20Yang%20and%20Boyu%20Wang%20and%20Ela%20Gruzewska%20and%20Anand%20Rao%20and%20Sindhu%20Raghuram%20and%20Zheng%20Xu%20and%20Eyal%20Ben-David%20and%20Jieru%20Mei%20and%20Sid%20Dalmia%20and%20Zhaoyi%20Zhang%20and%20Yuchen%20Liu%20and%20Gagan%20Bansal%20and%20Helena%20Pankov%20and%20Steven%20Schwarcz%20and%20Andrea%20Burns%20and%20Christine%20Chan%20and%20Sumit%20Sanghai%20and%20Ricky%20Liang%20and%20Ethan%20Liang%20and%20Antoine%20He%20and%20Amy%20Stuart%20and%20Arun%20Narayanan%20and%20Yukun%20Zhu%20and%20Christian%20Frank%20and%20Bahar%20Fatemi%20and%20Amit%20Sabne%20and%20Oran%20Lang%20and%20Indro%20Bhattacharya%20and%20Shane%20Settle%20and%20Maria%20Wang%20and%20Brendan%20McMahan%20and%20Andrea%20Tacchetti%20and%20Livio%20Baldini%20Soares%20and%20Majid%20Hadian%20and%20Serkan%20Cabi%20and%20Timothy%20Chung%20and%20Nikita%20Putikhin%20and%20Gang%20Li%20and%20Jeremy%20Chen%20and%20Austin%20Tarango%20and%20Henryk%20Michalewski%20and%20Mehran%20Kazemi%20and%20Hussain%20Masoom%20and%20Hila%20Sheftel%20and%20Rakesh%20Shivanna%20and%20Archita%20Vadali%20and%20Ramona%20Comanescu%20and%20Doug%20Reid%20and%20Joss%20Moore%20and%20Arvind%20Neelakantan%20and%20Micha%C3%ABl%20Sander%20and%20Jonathan%20Herzig%20and%20Aviv%20Rosenberg%20and%20Mostafa%20Dehghani%20and%20JD%20Choi%20and%20Michael%20Fink%20and%20Reid%20Hayes%20and%20Eric%20Ge%20and%20Shitao%20Weng%20and%20Chia-Hua%20Ho%20and%20John%20Karro%20and%20Kalpesh%20Krishna%20and%20Lam%20Nguyen%20Thiet%20and%20Amy%20Skerry-Ryan%20and%20Daniel%20Eppens%20and%20Marco%20Andreetto%20and%20Navin%20Sarma%20and%20Silvano%20Bonacina%20and%20Burcu%20Karagol%20Ayan%20and%20Megha%20Nawhal%20and%20Zhihao%20Shan%20and%20Mike%20Dusenberry%20and%20Shantanu%20Thakoor%20and%20Sagar%20Gubbi%20and%20Duc%20Dung%20Nguyen%20and%20Reut%20Tsarfaty%20and%20Samuel%20Albanie%20and%20Jovana%20Mitrovi%C4%87%20and%20Meet%20Gandhi%20and%20Bo-Juen%20Chen%20and%20Alessandro%20Epasto%20and%20Georgi%20Stephanov%20and%20Ye%20Jin%20and%20Samuel%20Gehman%20and%20Aida%20Amini%20and%20Jack%20Weber%20and%20Feryal%20Behbahani%20and%20Shawn%20Xu%20and%20Miltos%20Allamanis%20and%20Xi%20Chen%20and%20Myle%20Ott%20and%20Claire%20Sha%20and%20Michal%20Jastrzebski%20and%20Hang%20Qi%20and%20David%20Greene%20and%20Xinyi%20Wu%20and%20Abodunrinwa%20Toki%20and%20Daniel%20Vlasic%20and%20Jane%20Shapiro%20and%20Ragha%20Kotikalapudi%20and%20Zhe%20Shen%20and%20Takaaki%20Saeki%20and%20Sirui%20Xie%20and%20Albin%20Cassirer%20and%20Shikhar%20Bharadwaj%20and%20Tatsuya%20Kiyono%20and%20Srinadh%20Bhojanapalli%20and%20Elan%20Rosenfeld%20and%20Sam%20Ritter%20and%20Jieming%20Mao%20and%20Jo%C3%A3o%20Gabriel%20Oliveira%20and%20Zoltan%20Egyed%20and%20Bernd%20Bandemer%20and%20Emilio%20Parisotto%20and%20Keisuke%20Kinoshita%20and%20Juliette%20Pluto%20and%20Petros%20Maniatis%20and%20Steve%20Li%20and%20Yaohui%20Guo%20and%20Golnaz%20Ghiasi%20and%20Jean%20Tarbouriech%20and%20Srimon%20Chatterjee%20and%20Julie%20Jin%20and%20%20Katrina%20and%20%20Xu%20and%20Jennimaria%20Palomaki%20and%20S%C3%A9b%20Arnold%20and%20Madhavi%20Sewak%20and%20Federico%20Piccinini%20and%20Mohit%20Sharma%20and%20Ben%20Albrecht%20and%20Sean%20Purser-haskell%20and%20Ashwin%20Vaswani%20and%20Chongyan%20Chen%20and%20Matheus%20Wisniewski%20and%20Qin%20Cao%20and%20John%20Aslanides%20and%20Nguyet%20Minh%20Phu%20and%20Maximilian%20Sieb%20and%20Lauren%20Agubuzu%20and%20Anne%20Zheng%20and%20Daniel%20Sohn%20and%20Marco%20Selvi%20and%20Anders%20Andreassen%20and%20Krishan%20Subudhi%20and%20Prem%20Eruvbetine%20and%20Oliver%20Woodman%20and%20Tomas%20Mery%20and%20Sebastian%20Krause%20and%20Xiaoqi%20Ren%20and%20Xiao%20Ma%20and%20Jincheng%20Luo%20and%20Dawn%20Chen%20and%20Wei%20Fan%20and%20Henry%20Griffiths%20and%20Christian%20Schuler%20and%20Alice%20Li%20and%20Shujian%20Zhang%20and%20Jean-Michel%20Sarr%20and%20Shixin%20Luo%20and%20Riccardo%20Patana%20and%20Matthew%20Watson%20and%20Dani%20Naboulsi%20and%20Michael%20Collins%20and%20Sailesh%20Sidhwani%20and%20Emiel%20Hoogeboom%20and%20Sharon%20Silver%20and%20Emily%20Caveness%20and%20Xiaokai%20Zhao%20and%20Mikel%20Rodriguez%20and%20Maxine%20Deines%20and%20Libin%20Bai%20and%20Patrick%20Griffin%20and%20Marco%20Tagliasacchi%20and%20Emily%20Xue%20and%20Spandana%20Raj%20Babbula%20and%20Bo%20Pang%20and%20Nan%20Ding%20and%20Gloria%20Shen%20and%20Elijah%20Peake%20and%20Remi%20Crocker%20and%20Shubha%20Srinivas%20Raghvendra%20and%20Danny%20Swisher%20and%20Woohyun%20Han%20and%20Richa%20Singh%20and%20Ling%20Wu%20and%20Vladimir%20Pchelin%20and%20Tsendsuren%20Munkhdalai%20and%20Dana%20Alon%20and%20Geoff%20Bacon%20and%20Efren%20Robles%20and%20Jannis%20Bulian%20and%20Melvin%20Johnson%20and%20George%20Powell%20and%20Felipe%20Tiengo%20Ferreira%20and%20Yaoyiran%20Li%20and%20Frederik%20Benzing%20and%20Mihajlo%20Velimirovi%C4%87%20and%20Hubert%20Soyer%20and%20William%20Kong%20and%20%20Tony%20and%20%20Nguy%C3%AAn%20and%20Zhen%20Yang%20and%20Jeremiah%20Liu%20and%20Joost%20van%20Amersfoort%20and%20Daniel%20Gillick%20and%20Baochen%20Sun%20and%20Nathalie%20Rauschmayr%20and%20Katie%20Zhang%20and%20Serena%20Zhan%20and%20Tao%20Zhou%20and%20Alexey%20Frolov%20and%20Chengrun%20Yang%20and%20Denis%20Vnukov%20and%20Louis%20Rouillard%20and%20Hongji%20Li%20and%20Amol%20Mandhane%20and%20Nova%20Fallen%20and%20Rajesh%20Venkataraman%20and%20Clara%20Huiyi%20Hu%20and%20Jennifer%20Brennan%20and%20Jenny%20Lee%20and%20Jerry%20Chang%20and%20Martin%20Sundermeyer%20and%20Zhufeng%20Pan%20and%20Rosemary%20Ke%20and%20Simon%20Tong%20and%20Alex%20Fabrikant%20and%20William%20Bono%20and%20Jindong%20Gu%20and%20Ryan%20Foley%20and%20Yiran%20Mao%20and%20Manolis%20Delakis%20and%20Dhruva%20Bhaswar%20and%20Roy%20Frostig%20and%20Nick%20Li%20and%20Avital%20Zipori%20and%20Cath%20Hope%20and%20Olga%20Kozlova%20and%20Swaroop%20Mishra%20and%20Josip%20Djolonga%20and%20Craig%20Schiff%20and%20Majd%20Al%20Merey%20and%20Eleftheria%20Briakou%20and%20Peter%20Morgan%20and%20Andy%20Wan%20and%20Avinatan%20Hassidim%20and%20RJ%20Skerry-Ryan%20and%20Kuntal%20Sengupta%20and%20Mary%20Jasarevic%20and%20Praveen%20Kallakuri%20and%20Paige%20Kunkle%20and%20Hannah%20Brennan%20and%20Tom%20Lieber%20and%20Hassan%20Mansoor%20and%20Julian%20Walker%20and%20Bing%20Zhang%20and%20Annie%20Xie%20and%20Goran%20%C5%BDu%C5%BEi%C4%87%20and%20Adaeze%20Chukwuka%20and%20Alex%20Druinsky%20and%20Donghyun%20Cho%20and%20Rui%20Yao%20and%20Ferjad%20Naeem%20and%20Shiraz%20Butt%20and%20Eunyoung%20Kim%20and%20Zhipeng%20Jia%20and%20Mandy%20Jordan%20and%20Adam%20Lelkes%20and%20Mark%20Kurzeja%20and%20Sophie%20Wang%20and%20James%20Zhao%20and%20Andrew%20Over%20and%20Abhishek%20Chakladar%20and%20Marcel%20Prasetya%20and%20Neha%20Jha%20and%20Sriram%20Ganapathy%20and%20Yale%20Cong%20and%20Prakash%20Shroff%20and%20Carl%20Saroufim%20and%20Sobhan%20Miryoosefi%20and%20Mohamed%20Hammad%20and%20Tajwar%20Nasir%20and%20Weijuan%20Xi%20and%20Yang%20Gao%20and%20Young%20Maeng%20and%20Ben%20Hora%20and%20Chin-Yi%20Cheng%20and%20Parisa%20Haghani%20and%20Yoad%20Lewenberg%20and%20Caden%20Lu%20and%20Martin%20Matysiak%20and%20Naina%20Raisinghani%20and%20Huiyu%20Wang%20and%20Lexi%20Baugher%20and%20Rahul%20Sukthankar%20and%20Minh%20Giang%20and%20John%20Schultz%20and%20Noah%20Fiedel%20and%20Minmin%20Chen%20and%20Cheng-Chun%20Lee%20and%20Tapomay%20Dey%20and%20Hao%20Zheng%20and%20Shachi%20Paul%20and%20Celine%20Smith%20and%20Andy%20Ly%20and%20Yicheng%20Wang%20and%20Rishabh%20Bansal%20and%20Bartek%20Perz%20and%20Susanna%20Ricco%20and%20Stasha%20Blank%20and%20Vaishakh%20Keshava%20and%20Deepak%20Sharma%20and%20Marvin%20Chow%20and%20Kunal%20Lad%20and%20Komal%20Jalan%20and%20Simon%20Osindero%20and%20Craig%20Swanson%20and%20Jacob%20Scott%20and%20Anastasija%20Ili%C4%87%20and%20Xiaowei%20Li%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Afzal%20Shama%20Soudagar%20and%20Yan%20Xiong%20and%20Bat-Orgil%20Batsaikhan%20and%20Daniel%20Jarrett%20and%20Naveen%20Kumar%20and%20Maulik%20Shah%20and%20Matt%20Lawlor%20and%20Austin%20Waters%20and%20Mark%20Graham%20and%20Rhys%20May%20and%20Sabela%20Ramos%20and%20Sandra%20Lefdal%20and%20Zeynep%20Cankara%20and%20Nacho%20Cano%20and%20Brendan%20O%27Donoghue%20and%20Jed%20Borovik%20and%20Frederick%20Liu%20and%20Jordan%20Grimstad%20and%20Mahmoud%20Alnahlawi%20and%20Katerina%20Tsihlas%20and%20Tom%20Hudson%20and%20Nikolai%20Grigorev%20and%20Yiling%20Jia%20and%20Terry%20Huang%20and%20Tobenna%20Peter%20Igwe%20and%20Sergei%20Lebedev%20and%20Xiaodan%20Tang%20and%20Igor%20Krivokon%20and%20Frankie%20Garcia%20and%20Melissa%20Tan%20and%20Eric%20Jia%20and%20Peter%20Stys%20and%20Shikhar%20Vashishth%20and%20Yu%20Liang%20and%20Balaji%20Venkatraman%20and%20Chenjie%20Gu%20and%20Anastasios%20Kementsietsidis%20and%20Chen%20Zhu%20and%20Junehyuk%20Jung%20and%20Yunfei%20Bai%20and%20Mohammad%20Javad%20Hosseini%20and%20Faruk%20Ahmed%20and%20Aditya%20Gupta%20and%20Xin%20Yuan%20and%20Shereen%20Ashraf%20and%20Shitij%20Nigam%20and%20Gautam%20Vasudevan%20and%20Pranjal%20Awasthi%20and%20Adi%20Mayrav%20Gilady%20and%20Zelda%20Mariet%20and%20Ramy%20Eskander%20and%20Haiguang%20Li%20and%20Hexiang%20Hu%20and%20Guillermo%20Garrido%20and%20Philippe%20Schlattner%20and%20George%20Zhang%20and%20Rohun%20Saxena%20and%20Petar%20Devi%C4%87%20and%20Kritika%20Muralidharan%20and%20Ashwin%20Murthy%20and%20Yiqian%20Zhou%20and%20Min%20Choi%20and%20Arissa%20Wongpanich%20and%20Zhengdong%20Wang%20and%20Premal%20Shah%20and%20Yuntao%20Xu%20and%20Yiling%20Huang%20and%20Stephen%20Spencer%20and%20Alice%20Chen%20and%20James%20Cohan%20and%20Junjie%20Wang%20and%20Jonathan%20Tompson%20and%20Junru%20Wu%20and%20Ruba%20Haroun%20and%20Haiqiong%20Li%20and%20Blanca%20Huergo%20and%20Fan%20Yang%20and%20Tongxin%20Yin%20and%20James%20Wendt%20and%20Michael%20Bendersky%20and%20Rahma%20Chaabouni%20and%20Javier%20Snaider%20and%20Johan%20Ferret%20and%20Abhishek%20Jindal%20and%20Tara%20Thompson%20and%20Andrew%20Xue%20and%20Will%20Bishop%20and%20Shubham%20Milind%20Phal%20and%20Archit%20Sharma%20and%20Yunhsuan%20Sung%20and%20Prabakar%20Radhakrishnan%20and%20Mo%20Shomrat%20and%20Reeve%20Ingle%20and%20Roopali%20Vij%20and%20Justin%20Gilmer%20and%20Mihai%20Dorin%20Istin%20and%20Sam%20Sobell%20and%20Yang%20Lu%20and%20Emily%20Nottage%20and%20Dorsa%20Sadigh%20and%20Jeremiah%20Willcock%20and%20Tingnan%20Zhang%20and%20Steve%20Xu%20and%20Sasha%20Brown%20and%20Katherine%20Lee%20and%20Gary%20Wang%20and%20Yun%20Zhu%20and%20Yi%20Tay%20and%20Cheolmin%20Kim%20and%20Audrey%20Gutierrez%20and%20Abhanshu%20Sharma%20and%20Yongqin%20Xian%20and%20Sungyong%20Seo%20and%20Claire%20Cui%20and%20Elena%20Pochernina%20and%20Cip%20Baetu%20and%20Krzysztof%20Jastrz%C4%99bski%20and%20Mimi%20Ly%20and%20Mohamed%20Elhawaty%20and%20Dan%20Suh%20and%20Eren%20Sezener%20and%20Pidong%20Wang%20and%20Nancy%20Yuen%20and%20George%20Tucker%20and%20Jiahao%20Cai%20and%20Zuguang%20Yang%20and%20Cindy%20Wang%20and%20Alex%20Muzio%20and%20Hai%20Qian%20and%20Jae%20Yoo%20and%20Derek%20Lockhart%20and%20Kevin%20R.%20McKee%20and%20Mandy%20Guo%20and%20Malika%20Mehrotra%20and%20Artur%20Mendon%C3%A7a%20and%20Sanket%20Vaibhav%20Mehta%20and%20Sherry%20Ben%20and%20Chetan%20Tekur%20and%20Jiaqi%20Mu%20and%20Muye%20Zhu%20and%20Victoria%20Krakovna%20and%20Hongrae%20Lee%20and%20AJ%20Maschinot%20and%20S%C3%A9bastien%20Cevey%20and%20HyunJeong%20Choe%20and%20Aijun%20Bai%20and%20Hansa%20Srinivasan%20and%20Derek%20Gasaway%20and%20Nick%20Young%20and%20Patrick%20Siegler%20and%20Dan%20Holtmann-Rice%20and%20Vihari%20Piratla%20and%20Kate%20Baumli%20and%20Roey%20Yogev%20and%20Alex%20Hofer%20and%20Hado%20van%20Hasselt%20and%20Svetlana%20Grant%20and%20Yuri%20Chervonyi%20and%20David%20Silver%20and%20Andrew%20Hogue%20and%20Ayushi%20Agarwal%20and%20Kathie%20Wang%20and%20Preeti%20Singh%20and%20Four%20Flynn%20and%20Josh%20Lipschultz%20and%20Robert%20David%20and%20Lizzetth%20Bellot%20and%20Yao-Yuan%20Yang%20and%20Long%20Le%20and%20Filippo%20Graziano%20and%20Kate%20Olszewska%20and%20Kevin%20Hui%20and%20Akanksha%20Maurya%20and%20Nikos%20Parotsidis%20and%20Weijie%20Chen%20and%20Tayo%20Oguntebi%20and%20Joe%20Kelley%20and%20Anirudh%20Baddepudi%20and%20Johannes%20Mauerer%20and%20Gregory%20Shaw%20and%20Alex%20Siegman%20and%20Lin%20Yang%20and%20Shravya%20Shetty%20and%20Subhrajit%20Roy%20and%20Yunting%20Song%20and%20Wojciech%20Stokowiec%20and%20Ryan%20Burnell%20and%20Omkar%20Savant%20and%20Robert%20Busa-Fekete%20and%20Jin%20Miao%20and%20Samrat%20Ghosh%20and%20Liam%20MacDermed%20and%20Phillip%20Lippe%20and%20Mikhail%20Dektiarev%20and%20Zach%20Behrman%20and%20Fabian%20Mentzer%20and%20Kelvin%20Nguyen%20and%20Meng%20Wei%20and%20Siddharth%20Verma%20and%20Chris%20Knutsen%20and%20Sudeep%20Dasari%20and%20Zhipeng%20Yan%20and%20Petr%20Mitrichev%20and%20Xingyu%20Wang%20and%20Virat%20Shejwalkar%20and%20Jacob%20Austin%20and%20Srinivas%20Sunkara%20and%20Navneet%20Potti%20and%20Yan%20Virin%20and%20Christian%20Wright%20and%20Ga%C3%ABl%20Liu%20and%20Oriana%20Riva%20and%20Etienne%20Pot%20and%20Greg%20Kochanski%20and%20Quoc%20Le%20and%20Gargi%20Balasubramaniam%20and%20Arka%20Dhar%20and%20Yuguo%20Liao%20and%20Adam%20Bloniarz%20and%20Divyansh%20Shukla%20and%20Elizabeth%20Cole%20and%20Jong%20Lee%20and%20Sheng%20Zhang%20and%20Sushant%20Kafle%20and%20Siddharth%20Vashishtha%20and%20Parsa%20Mahmoudieh%20and%20Grace%20Chen%20and%20Raphael%20Hoffmann%20and%20Pranesh%20Srinivasan%20and%20Agustin%20Dal%20Lago%20and%20Yoav%20Ben%20Shalom%20and%20Zi%20Wang%20and%20Michael%20Elabd%20and%20Anuj%20Sharma%20and%20Junhyuk%20Oh%20and%20Suraj%20Kothawade%20and%20Maigo%20Le%20and%20Marianne%20Monteiro%20and%20Shentao%20Yang%20and%20Kaiz%20Alarakyia%20and%20Robert%20Geirhos%20and%20Diana%20Mincu%20and%20H%C3%A5vard%20Garnes%20and%20Hayato%20Kobayashi%20and%20Soroosh%20Mariooryad%20and%20Kacper%20Krasowiak%20and%20%20Zhixin%20and%20%20Lai%20and%20Shibl%20Mourad%20and%20Mingqiu%20Wang%20and%20Fan%20Bu%20and%20Ophir%20Aharoni%20and%20Guanjie%20Chen%20and%20Abhimanyu%20Goyal%20and%20Vadim%20Zubov%20and%20Ankur%20Bapna%20and%20Elahe%20Dabir%20and%20Nisarg%20Kothari%20and%20Kay%20Lamerigts%20and%20Nicola%20De%20Cao%20and%20Jeremy%20Shar%20and%20Christopher%20Yew%20and%20Nitish%20Kulkarni%20and%20Dre%20Mahaarachchi%20and%20Mandar%20Joshi%20and%20Zhenhai%20Zhu%20and%20Jared%20Lichtarge%20and%20Yichao%20Zhou%20and%20Hannah%20Muckenhirn%20and%20Vittorio%20Selo%20and%20Oriol%20Vinyals%20and%20Peter%20Chen%20and%20Anthony%20Brohan%20and%20Vaibhav%20Mehta%20and%20Sarah%20Cogan%20and%20Ruth%20Wang%20and%20Ty%20Geri%20and%20Wei-Jen%20Ko%20and%20Wei%20Chen%20and%20Fabio%20Viola%20and%20Keshav%20Shivam%20and%20Lisa%20Wang%20and%20Madeleine%20Clare%20Elish%20and%20Raluca%20Ada%20Popa%20and%20S%C3%A9bastien%20Pereira%20and%20Jianqiao%20Liu%20and%20Raphael%20Koster%20and%20Donnie%20Kim%20and%20Gufeng%20Zhang%20and%20Sayna%20Ebrahimi%20and%20Partha%20Talukdar%20and%20Yanyan%20Zheng%20and%20Petra%20Poklukar%20and%20Ales%20Mikhalap%20and%20Dale%20Johnson%20and%20Anitha%20Vijayakumar%20and%20Mark%20Omernick%20and%20Matt%20Dibb%20and%20Ayush%20Dubey%20and%20Qiong%20Hu%20and%20Apurv%20Suman%20and%20Vaibhav%20Aggarwal%20and%20Ilya%20Kornakov%20and%20Fei%20Xia%20and%20Wing%20Lowe%20and%20Alexey%20Kolganov%20and%20Ted%20Xiao%20and%20Vitaly%20Nikolaev%20and%20Steven%20Hemingray%20and%20Bonnie%20Li%20and%20Joana%20Iljazi%20and%20Miko%C5%82aj%20Rybi%C5%84ski%20and%20Ballie%20Sandhu%20and%20Peggy%20Lu%20and%20Thang%20Luong%20and%20Rodolphe%20Jenatton%20and%20Vineetha%20Govindaraj%20and%20%20Hui%20and%20%20Li%20and%20Gabriel%20Dulac-Arnold%20and%20Wonpyo%20Park%20and%20Henry%20Wang%20and%20Abhinit%20Modi%20and%20Jean%20Pouget-Abadie%20and%20Kristina%20Greller%20and%20Rahul%20Gupta%20and%20Robert%20Berry%20and%20Prajit%20Ramachandran%20and%20Jinyu%20Xie%20and%20Liam%20McCafferty%20and%20Jianling%20Wang%20and%20Kilol%20Gupta%20and%20Hyeontaek%20Lim%20and%20Bla%C5%BE%20Bratani%C4%8D%20and%20Andy%20Brock%20and%20Ilia%20Akolzin%20and%20Jim%20Sproch%20and%20Dan%20Karliner%20and%20Duhyeon%20Kim%20and%20Adrian%20Goedeckemeyer%20and%20Noam%20Shazeer%20and%20Cordelia%20Schmid%20and%20Daniele%20Calandriello%20and%20Parul%20Bhatia%20and%20Krzysztof%20Choromanski%20and%20Ceslee%20Montgomery%20and%20Dheeru%20Dua%20and%20Ana%20Ramalho%20and%20Helen%20King%20and%20Yue%20Gao%20and%20Lynn%20Nguyen%20and%20David%20Lindner%20and%20Divya%20Pitta%20and%20Oleaser%20Johnson%20and%20Khalid%20Salama%20and%20Diego%20Ardila%20and%20Michael%20Han%20and%20Erin%20Farnese%20and%20Seth%20Odoom%20and%20Ziyue%20Wang%20and%20Xiangzhuo%20Ding%20and%20Norman%20Rink%20and%20Ray%20Smith%20and%20Harshal%20Tushar%20Lehri%20and%20Eden%20Cohen%20and%20Neera%20Vats%20and%20Tong%20He%20and%20Parthasarathy%20Gopavarapu%20and%20Adam%20Paszke%20and%20Miteyan%20Patel%20and%20Wouter%20Van%20Gansbeke%20and%20Lucia%20Loher%20and%20Luis%20Castro%20and%20Maria%20Voitovich%20and%20Tamara%20von%20Glehn%20and%20Nelson%20George%20and%20Simon%20Niklaus%20and%20Zach%20Eaton-Rosen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Erik%20Jue%20and%20Sagi%20Perel%20and%20Carrie%20Zhang%20and%20Yuval%20Bahat%20and%20Ang%C3%A9line%20Pouget%20and%20Zhi%20Xing%20and%20Fantine%20Huot%20and%20Ashish%20Shenoy%20and%20Taylor%20Bos%20and%20Vincent%20Coriou%20and%20Bryan%20Richter%20and%20Natasha%20Noy%20and%20Yaqing%20Wang%20and%20Santiago%20Ontanon%20and%20Siyang%20Qin%20and%20Gleb%20Makarchuk%20and%20Demis%20Hassabis%20and%20Zhuowan%20Li%20and%20Mandar%20Sharma%20and%20Kumaran%20Venkatesan%20and%20Iurii%20Kemaev%20and%20Roxanne%20Daniel%20and%20Shiyu%20Huang%20and%20Saloni%20Shah%20and%20Octavio%20Ponce%20and%20%20Warren%20and%20%20Chen%20and%20Manaal%20Faruqui%20and%20Jialin%20Wu%20and%20Slavica%20Anda%C4%8Di%C4%87%20and%20Szabolcs%20Payrits%20and%20Daniel%20McDuff%20and%20Tom%20Hume%20and%20Yuan%20Cao%20and%20MH%20Tessler%20and%20Qingze%20Wang%20and%20Yinan%20Wang%20and%20Ivor%20Rendulic%20and%20Eirikur%20Agustsson%20and%20Matthew%20Johnson%20and%20Tanya%20Lando%20and%20Andrew%20Howard%20and%20Sri%20Gayatri%20Sundara%20Padmanabhan%20and%20Mayank%20Daswani%20and%20Andrea%20Banino%20and%20Michael%20Kilgore%20and%20Jonathan%20Heek%20and%20Ziwei%20Ji%20and%20Alvaro%20Caceres%20and%20Conglong%20Li%20and%20Nora%20Kassner%20and%20Alexey%20Vlaskin%20and%20Zeyu%20Liu%20and%20Alex%20Grills%20and%20Yanhan%20Hou%20and%20Roykrong%20Sukkerd%20and%20Gowoon%20Cheon%20and%20Nishita%20Shetty%20and%20Larisa%20Markeeva%20and%20Piotr%20Stanczyk%20and%20Tejas%20Iyer%20and%20Yuan%20Gong%20and%20Shawn%20Gao%20and%20Keerthana%20Gopalakrishnan%20and%20Tim%20Blyth%20and%20Malcolm%20Reynolds%20and%20Avishkar%20Bhoopchand%20and%20Misha%20Bilenko%20and%20Dero%20Gharibian%20and%20Vicky%20Zayats%20and%20Aleksandra%20Faust%20and%20Abhinav%20Singh%20and%20Min%20Ma%20and%20Hongyang%20Jiao%20and%20Sudheendra%20Vijayanarasimhan%20and%20Lora%20Aroyo%20and%20Vikas%20Yadav%20and%20Sarah%20Chakera%20and%20Ashwin%20Kakarla%20and%20Vilobh%20Meshram%20and%20Karol%20Gregor%20and%20Gabriela%20Botea%20and%20Evan%20Senter%20and%20Dawei%20Jia%20and%20Geza%20Kovacs%20and%20Neha%20Sharma%20and%20Sebastien%20Baur%20and%20Kai%20Kang%20and%20Yifan%20He%20and%20Lin%20Zhuo%20and%20Marija%20Kostelac%20and%20Itay%20Laish%20and%20Songyou%20Peng%20and%20Louis%20O%27Bryan%20and%20Daniel%20Kasenberg%20and%20Girish%20Ramchandra%20Rao%20and%20Edouard%20Leurent%20and%20Biao%20Zhang%20and%20Sage%20Stevens%20and%20Ana%20Salazar%20and%20Ye%20Zhang%20and%20Ivan%20Lobov%20and%20Jake%20Walker%20and%20Allen%20Porter%20and%20Morgan%20Redshaw%20and%20Han%20Ke%20and%20Abhishek%20Rao%20and%20Alex%20Lee%20and%20Hoi%20Lam%20and%20Michael%20Moffitt%20and%20Jaeyoun%20Kim%20and%20Siyuan%20Qiao%20and%20Terry%20Koo%20and%20Robert%20Dadashi%20and%20Xinying%20Song%20and%20Mukund%20Sundararajan%20and%20Peng%20Xu%20and%20Chizu%20Kawamoto%20and%20Yan%20Zhong%20and%20Clara%20Barbu%20and%20Apoorv%20Reddy%20and%20Mauro%20Verzetti%20and%20Leon%20Li%20and%20George%20Papamakarios%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Mary%20Cassin%20and%20Koray%20Kavukcuoglu%20and%20Rigel%20Swavely%20and%20Alain%20Vaucher%20and%20Jeffrey%20Zhao%20and%20Ross%20Hemsley%20and%20Michael%20Tschannen%20and%20Heming%20Ge%20and%20Gaurav%20Menghani%20and%20Yang%20Yu%20and%20Natalie%20Ha%20and%20Wei%20He%20and%20Xiao%20Wu%20and%20Maggie%20Song%20and%20Rachel%20Sterneck%20and%20Stefan%20Zinke%20and%20Dan%20A.%20Calian%20and%20Annie%20Marsden%20and%20Alejandro%20Cruzado%20Ruiz%20and%20Matteo%20Hessel%20and%20Almog%20Gueta%20and%20Benjamin%20Lee%20and%20Brian%20Farris%20and%20Manish%20Gupta%20and%20Yunjie%20Li%20and%20Mohammad%20Saleh%20and%20Vedant%20Misra%20and%20Kefan%20Xiao%20and%20Piermaria%20Mendolicchio%20and%20Gavin%20Buttimore%20and%20Varvara%20Krayvanova%20and%20Nigamaa%20Nayakanti%20and%20Matthew%20Wiethoff%20and%20Yash%20Pande%20and%20Azalia%20Mirhoseini%20and%20Ni%20Lao%20and%20Jasmine%20Liu%20and%20Yiqing%20Hua%20and%20Angie%20Chen%20and%20Yury%20Malkov%20and%20Dmitry%20Kalashnikov%20and%20Shubham%20Gupta%20and%20Kartik%20Audhkhasi%20and%20Yuexiang%20Zhai%20and%20Sudhindra%20Kopalle%20and%20Prateek%20Jain%20and%20Eran%20Ofek%20and%20Clemens%20Meyer%20and%20Khuslen%20Baatarsukh%20and%20Hana%20Strej%C4%8Dek%20and%20Jun%20Qian%20and%20James%20Freedman%20and%20Ricardo%20Figueira%20and%20Michal%20Sokolik%20and%20Olivier%20Bachem%20and%20Raymond%20Lin%20and%20Dia%20Kharrat%20and%20Chris%20Hidey%20and%20Pingmei%20Xu%20and%20Dennis%20Duan%20and%20Yin%20Li%20and%20Muge%20Ersoy%20and%20Richard%20Everett%20and%20Kevin%20Cen%20and%20Rebeca%20Santamaria-Fernandez%20and%20Amir%20Taubenfeld%20and%20Ian%20Mackinnon%20and%20Linda%20Deng%20and%20Polina%20Zablotskaia%20and%20Shashank%20Viswanadha%20and%20Shivanker%20Goel%20and%20Damion%20Yates%20and%20Yunxiao%20Deng%20and%20Peter%20Choy%20and%20Mingqing%20Chen%20and%20Abhishek%20Sinha%20and%20Alex%20Mossin%20and%20Yiming%20Wang%20and%20Arthur%20Szlam%20and%20Susan%20Hao%20and%20Paul%20Kishan%20Rubenstein%20and%20Metin%20Toksoz-Exley%20and%20Miranda%20Aperghis%20and%20Yin%20Zhong%20and%20Junwhan%20Ahn%20and%20Michael%20Isard%20and%20Olivier%20Lacombe%20and%20Florian%20Luisier%20and%20Chrysovalantis%20Anastasiou%20and%20Yogesh%20Kalley%20and%20Utsav%20Prabhu%20and%20Emma%20Dunleavy%20and%20Shaan%20Bijwadia%20and%20Justin%20Mao-Jones%20and%20Kelly%20Chen%20and%20Rama%20Pasumarthi%20and%20Emily%20Wood%20and%20Adil%20Dostmohamed%20and%20Nate%20Hurley%20and%20Jiri%20Simsa%20and%20Alicia%20Parrish%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Ondrej%20Skopek%20and%20Yony%20Kochinski%20and%20Javier%20Rey%20and%20Verena%20Rieser%20and%20Denny%20Zhou%20and%20Sun%20Jae%20Lee%20and%20Trilok%20Acharya%20and%20Guowang%20Li%20and%20Joe%20Jiang%20and%20Xiaofan%20Zhang%20and%20Bryant%20Gipson%20and%20Ethan%20Mahintorabi%20and%20Marco%20Gelmi%20and%20Nima%20Khajehnouri%20and%20Angel%20Yeh%20and%20Kayi%20Lee%20and%20Loic%20Matthey%20and%20Leslie%20Baker%20and%20Trang%20Pham%20and%20Han%20Fu%20and%20Alex%20Pak%20and%20Prakhar%20Gupta%20and%20Cristina%20Vasconcelos%20and%20Adam%20Sadovsky%20and%20Brian%20Walker%20and%20Sissie%20Hsiao%20and%20Patrik%20Zochbauer%20and%20Andreea%20Marzoca%20and%20Noam%20Velan%20and%20Junhao%20Zeng%20and%20Gilles%20Baechler%20and%20Danny%20Driess%20and%20Divya%20Jain%20and%20Yanping%20Huang%20and%20Lizzie%20Tao%20and%20John%20Maggs%20and%20Nir%20Levine%20and%20Jon%20Schneider%20and%20Erika%20Gemzer%20and%20Samuel%20Petit%20and%20Shan%20Han%20and%20Zach%20Fisher%20and%20Dustin%20Zelle%20and%20Courtney%20Biles%20and%20Eugene%20Ie%20and%20Asya%20Fadeeva%20and%20Casper%20Liu%20and%20Juliana%20Vicente%20Franco%20and%20Adrian%20Collister%20and%20Hao%20Zhang%20and%20Renshen%20Wang%20and%20Ruizhe%20Zhao%20and%20Leandro%20Kieliger%20and%20Kurt%20Shuster%20and%20Rui%20Zhu%20and%20Boqing%20Gong%20and%20Lawrence%20Chan%20and%20Ruoxi%20Sun%20and%20Sujoy%20Basu%20and%20Roland%20Zimmermann%20and%20Jamie%20Hayes%20and%20Abhishek%20Bapna%20and%20Jasper%20Snoek%20and%20Weel%20Yang%20and%20Puranjay%20Datta%20and%20Jad%20Al%20Abdallah%20and%20Kevin%20Kilgour%20and%20Lu%20Li%20and%20SQ%20Mah%20and%20Yennie%20Jun%20and%20Morgane%20Rivi%C3%A8re%20and%20Abhijit%20Karmarkar%20and%20Tammo%20Spalink%20and%20Tao%20Huang%20and%20Lucas%20Gonzalez%20and%20Duc-Hieu%20Tran%20and%20Averi%20Nowak%20and%20John%20Palowitch%20and%20Martin%20Chadwick%20and%20Ellie%20Talius%20and%20Harsh%20Mehta%20and%20Thibault%20Sellam%20and%20Philipp%20Fr%C3%A4nken%20and%20Massimo%20Nicosia%20and%20Kyle%20He%20and%20Aditya%20Kini%20and%20David%20Amos%20and%20Sugato%20Basu%20and%20Harrison%20Jobe%20and%20Eleni%20Shaw%20and%20Qiantong%20Xu%20and%20Colin%20Evans%20and%20Daisuke%20Ikeda%20and%20Chaochao%20Yan%20and%20Larry%20Jin%20and%20Lun%20Wang%20and%20Sachin%20Yadav%20and%20Ilia%20Labzovsky%20and%20Ramesh%20Sampath%20and%20Ada%20Ma%20and%20Candice%20Schumann%20and%20Aditya%20Siddhant%20and%20Rohin%20Shah%20and%20John%20Youssef%20and%20Rishabh%20Agarwal%20and%20Natalie%20Dabney%20and%20Alessio%20Tonioni%20and%20Moran%20Ambar%20and%20Jing%20Li%20and%20Isabelle%20Guyon%20and%20Benny%20Li%20and%20David%20Soergel%20and%20Boya%20Fang%20and%20Georgi%20Karadzhov%20and%20Cristian%20Udrescu%20and%20Trieu%20Trinh%20and%20Vikas%20Raunak%20and%20Seb%20Noury%20and%20Dee%20Guo%20and%20Sonal%20Gupta%20and%20Mara%20Finkelstein%20and%20Denis%20Petek%20and%20Lihao%20Liang%20and%20Greg%20Billock%20and%20Pei%20Sun%20and%20David%20Wood%20and%20Yiwen%20Song%20and%20Xiaobin%20Yu%20and%20Tatiana%20Matejovicova%20and%20Regev%20Cohen%20and%20Kalyan%20Andra%20and%20David%20D%27Ambrosio%20and%20Zhiwei%20Deng%20and%20Vincent%20Nallatamby%20and%20Ebrahim%20Songhori%20and%20Rumen%20Dangovski%20and%20Andrew%20Lampinen%20and%20Pankil%20Botadra%20and%20Adam%20Hillier%20and%20Jiawei%20Cao%20and%20Nagabhushan%20Baddi%20and%20Adhi%20Kuncoro%20and%20Toshihiro%20Yoshino%20and%20Ankit%20Bhagatwala%20and%20Marc%C3%A1urelio%20Ranzato%20and%20Rylan%20Schaeffer%20and%20Tianlin%20Liu%20and%20Shuai%20Ye%20and%20Obaid%20Sarvana%20and%20John%20Nham%20and%20Chenkai%20Kuang%20and%20Isabel%20Gao%20and%20Jinoo%20Baek%20and%20Shubham%20Mittal%20and%20Ayzaan%20Wahid%20and%20Anita%20Gergely%20and%20Bin%20Ni%20and%20Josh%20Feldman%20and%20Carrie%20Muir%20and%20Pascal%20Lamblin%20and%20Wolfgang%20Macherey%20and%20Ethan%20Dyer%20and%20Logan%20Kilpatrick%20and%20V%C3%ADctor%20Campos%20and%20Mukul%20Bhutani%20and%20Stanislav%20Fort%20and%20Yanif%20Ahmad%20and%20Aliaksei%20Severyn%20and%20Kleopatra%20Chatziprimou%20and%20Oleksandr%20Ferludin%20and%20Mason%20Dimarco%20and%20Aditya%20Kusupati%20and%20Joe%20Heyward%20and%20Dan%20Bahir%20and%20Kevin%20Villela%20and%20Katie%20Millican%20and%20Dror%20Marcus%20and%20Sanaz%20Bahargam%20and%20Caglar%20Unlu%20and%20Nicholas%20Roth%20and%20Zichuan%20Wei%20and%20Siddharth%20Gopal%20and%20Deepanway%20Ghoshal%20and%20Edward%20Lee%20and%20Sharon%20Lin%20and%20Jennie%20Lees%20and%20Dayeong%20Lee%20and%20Anahita%20Hosseini%20and%20Connie%20Fan%20and%20Seth%20Neel%20and%20Marcus%20Wu%20and%20Yasemin%20Altun%20and%20Honglong%20Cai%20and%20Enrique%20Piqueras%20and%20Josh%20Woodward%20and%20Alessandro%20Bissacco%20and%20Salem%20Haykal%20and%20Mahyar%20Bordbar%20and%20Prasha%20Sundaram%20and%20Sarah%20Hodkinson%20and%20Daniel%20Toyama%20and%20George%20Polovets%20and%20Austin%20Myers%20and%20Anu%20Sinha%20and%20Tomer%20Levinboim%20and%20Kashyap%20Krishnakumar%20and%20Rachita%20Chhaparia%20and%20Tatiana%20Sholokhova%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Ganesh%20Jawahar%20and%20Haroon%20Qureshi%20and%20Jieru%20Hu%20and%20Nikola%20Momchev%20and%20Matthew%20Rahtz%20and%20Renjie%20Wu%20and%20Aishwarya%20P%20S%20and%20Kedar%20Dhamdhere%20and%20Meiqi%20Guo%20and%20Umang%20Gupta%20and%20Ali%20Eslami%20and%20Mariano%20Schain%20and%20Michiel%20Blokzijl%20and%20David%20Welling%20and%20Dave%20Orr%20and%20Levent%20Bolelli%20and%20Nicolas%20Perez-Nieves%20and%20Mikhail%20Sirotenko%20and%20Aman%20Prasad%20and%20Arjun%20Kar%20and%20Borja%20De%20Balle%20Pigem%20and%20Tayfun%20Terzi%20and%20Gell%C3%A9rt%20Weisz%20and%20Dipankar%20Ghosh%20and%20Aditi%20Mavalankar%20and%20Dhruv%20Madeka%20and%20Kaspar%20Daugaard%20and%20Hartwig%20Adam%20and%20Viraj%20Shah%20and%20Dana%20Berman%20and%20Maggie%20Tran%20and%20Steven%20Baker%20and%20Ewa%20Andrejczuk%20and%20Grishma%20Chole%20and%20Ganna%20Raboshchuk%20and%20Mahdi%20Mirzazadeh%20and%20Thais%20Kagohara%20and%20Shimu%20Wu%20and%20Christian%20Schallhart%20and%20Bernett%20Orlando%20and%20Chen%20Wang%20and%20Alban%20Rrustemi%20and%20Hao%20Xiong%20and%20Hao%20Liu%20and%20Arpi%20Vezer%20and%20Nolan%20Ramsden%20and%20Shuo-yiin%20Chang%20and%20Sidharth%20Mudgal%20and%20Yan%20Li%20and%20Nino%20Vieillard%20and%20Yedid%20Hoshen%20and%20Farooq%20Ahmad%20and%20Ambrose%20Slone%20and%20Amy%20Hua%20and%20Natan%20Potikha%20and%20Mirko%20Rossini%20and%20Jon%20Stritar%20and%20Sushant%20Prakash%20and%20Zifeng%20Wang%20and%20Xuanyi%20Dong%20and%20Alireza%20Nazari%20and%20Efrat%20Nehoran%20and%20Kaan%20Tekelioglu%20and%20Yinxiao%20Li%20and%20Kartikeya%20Badola%20and%20Tom%20Funkhouser%20and%20Yuanzhen%20Li%20and%20Varun%20Yerram%20and%20Ramya%20Ganeshan%20and%20Daniel%20Formoso%20and%20Karol%20Langner%20and%20Tian%20Shi%20and%20Huijian%20Li%20and%20Yumeya%20Yamamori%20and%20Amayika%20Panda%20and%20Alaa%20Saade%20and%20Angelo%20Scorza%20Scarpati%20and%20Chris%20Breaux%20and%20CJ%20Carey%20and%20Zongwei%20Zhou%20and%20Cho-Jui%20Hsieh%20and%20Sophie%20Bridgers%20and%20Alena%20Butryna%20and%20Nishesh%20Gupta%20and%20Vaibhav%20Tulsyan%20and%20Sanghyun%20Woo%20and%20Evgenii%20Eltyshev%20and%20Will%20Grathwohl%20and%20Chanel%20Parks%20and%20Seth%20Benjamin%20and%20Rina%20Panigrahy%20and%20Shenil%20Dodhia%20and%20Daniel%20De%20Freitas%20and%20Chris%20Sauer%20and%20Will%20Song%20and%20Ferran%20Alet%20and%20Jackson%20Tolins%20and%20Cosmin%20Paduraru%20and%20Xingyi%20Zhou%20and%20Brian%20Albert%20and%20Zizhao%20Zhang%20and%20Lei%20Shu%20and%20Mudit%20Bansal%20and%20Sarah%20Nguyen%20and%20Amir%20Globerson%20and%20Owen%20Xiao%20and%20James%20Manyika%20and%20Tom%20Hennigan%20and%20Rong%20Rong%20and%20Josip%20Matak%20and%20Anton%20Bakalov%20and%20Ankur%20Sharma%20and%20Danila%20Sinopalnikov%20and%20Andrew%20Pierson%20and%20Stephen%20Roller%20and%20Geoff%20Brown%20and%20Mingcen%20Gao%20and%20Toshiyuki%20Fukuzawa%20and%20Amin%20Ghafouri%20and%20Kenny%20Vassigh%20and%20Iain%20Barr%20and%20Zhicheng%20Wang%20and%20Anna%20Korsun%20and%20Rajesh%20Jayaram%20and%20Lijie%20Ren%20and%20Tim%20Zaman%20and%20Samira%20Khan%20and%20Yana%20Lunts%20and%20Dan%20Deutsch%20and%20Dave%20Uthus%20and%20Nitzan%20Katz%20and%20Masha%20Samsikova%20and%20Amr%20Khalifa%20and%20Nikhil%20Sethi%20and%20Jiao%20Sun%20and%20Luming%20Tang%20and%20Uri%20Alon%20and%20Xianghong%20Luo%20and%20Dian%20Yu%20and%20Abhishek%20Nayyar%20and%20Bryce%20Petrini%20and%20Will%20Truong%20and%20Vincent%20Hellendoorn%20and%20Nikolai%20Chinaev%20and%20Chris%20Alberti%20and%20Wei%20Wang%20and%20Jingcao%20Hu%20and%20Vahab%20Mirrokni%20and%20Ananth%20Balashankar%20and%20Avia%20Aharon%20and%20Aahil%20Mehta%20and%20Ahmet%20Iscen%20and%20Joseph%20Kready%20and%20Lucas%20Manning%20and%20Anhad%20Mohananey%20and%20Yuankai%20Chen%20and%20Anshuman%20Tripathi%20and%20Allen%20Wu%20and%20Igor%20Petrovski%20and%20Dawsen%20Hwang%20and%20Martin%20Baeuml%20and%20Shreyas%20Chandrakaladharan%20and%20Yuan%20Liu%20and%20Rey%20Coaguila%20and%20Maxwell%20Chen%20and%20Sally%20Ma%20and%20Pouya%20Tafti%20and%20Susheel%20Tatineni%20and%20Terry%20Spitz%20and%20Jiayu%20Ye%20and%20Paul%20Vicol%20and%20Mihaela%20Rosca%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20and%20Zohar%20Yahav%20and%20Sanjay%20Ghemawat%20and%20Hanzhao%20Lin%20and%20Phoebe%20Kirk%20and%20Zaid%20Nabulsi%20and%20Sergey%20Brin%20and%20Bernd%20Bohnet%20and%20Ken%20Caluwaerts%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Dan%20Zheng%20and%20Zihang%20Dai%20and%20Petre%20Petrov%20and%20Yichong%20Xu%20and%20Ramin%20Mehran%20and%20Zhuo%20Xu%20and%20Luisa%20Zintgraf%20and%20Jiho%20Choi%20and%20Spurthi%20Amba%20Hombaiah%20and%20Romal%20Thoppilan%20and%20Sashank%20Reddi%20and%20Lukasz%20Lew%20and%20Li%20Li%20and%20Kellie%20Webster%20and%20KP%20Sawhney%20and%20Lampros%20Lamprou%20and%20Siamak%20Shakeri%20and%20Mayank%20Lunayach%20and%20Jianmin%20Chen%20and%20Sumit%20Bagri%20and%20Alex%20Salcianu%20and%20Ying%20Chen%20and%20Yani%20Donchev%20and%20Charlotte%20Magister%20and%20Signe%20N%C3%B8rly%20and%20Vitor%20Rodrigues%20and%20Tomas%20Izo%20and%20Hila%20Noga%20and%20Joe%20Zou%20and%20Thomas%20K%C3%B6ppe%20and%20Wenxuan%20Zhou%20and%20Kenton%20Lee%20and%20Xiangzhu%20Long%20and%20Danielle%20Eisenbud%20and%20Anthony%20Chen%20and%20Connor%20Schenck%20and%20Chi%20Ming%20To%20and%20Peilin%20Zhong%20and%20Emanuel%20Taropa%20and%20Minh%20Truong%20and%20Omer%20Levy%20and%20Danilo%20Martins%20and%20Zhiyuan%20Zhang%20and%20Christopher%20Semturs%20and%20Kelvin%20Zhang%20and%20Alex%20Yakubovich%20and%20Pol%20Moreno%20and%20Lara%20McConnaughey%20and%20Di%20Lu%20and%20Sam%20Redmond%20and%20Lotte%20Weerts%20and%20Yonatan%20Bitton%20and%20Tiziana%20Refice%20and%20Nicolas%20Lacasse%20and%20Arthur%20Conmy%20and%20Corentin%20Tallec%20and%20Julian%20Odell%20and%20Hannah%20Forbes-Pollard%20and%20Arkadiusz%20Socala%20and%20Jonathan%20Hoech%20and%20Pushmeet%20Kohli%20and%20Alanna%20Walton%20and%20Rui%20Wang%20and%20Mikita%20Sazanovich%20and%20Kexin%20Zhu%20and%20Andrei%20Kapishnikov%20and%20Rich%20Galt%20and%20Matthew%20Denton%20and%20Ben%20Murdoch%20and%20Caitlin%20Sikora%20and%20Kareem%20Mohamed%20and%20Wei%20Wei%20and%20Uri%20First%20and%20Tim%20McConnell%20and%20Luis%20C.%20Cobo%20and%20James%20Qin%20and%20Thi%20Avrahami%20and%20Daniel%20Balle%20and%20Yu%20Watanabe%20and%20Annie%20Louis%20and%20Adam%20Kraft%20and%20Setareh%20Ariafar%20and%20Yiming%20Gu%20and%20Eug%C3%A9nie%20Rives%20and%20Charles%20Yoon%20and%20Andrei%20Rusu%20and%20James%20Cobon-Kerr%20and%20Chris%20Hahn%20and%20Jiaming%20Luo%20and%20%20Yuvein%20and%20%20Zhu%20and%20Niharika%20Ahuja%20and%20Rodrigo%20Benenson%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Honglin%20Yu%20and%20Lloyd%20Hightower%20and%20Junlin%20Zhang%20and%20Darren%20Ni%20and%20Lisa%20Anne%20Hendricks%20and%20Gabby%20Wang%20and%20Gal%20Yona%20and%20Lalit%20Jain%20and%20Pablo%20Barrio%20and%20Surya%20Bhupatiraju%20and%20Siva%20Velusamy%20and%20Allan%20Dafoe%20and%20Sebastian%20Riedel%20and%20Tara%20Thomas%20and%20Zhe%20Yuan%20and%20Mathias%20Bellaiche%20and%20Sheena%20Panthaplackel%20and%20Klemen%20Kloboves%20and%20Sarthak%20Jauhari%20and%20Canfer%20Akbulut%20and%20Todor%20Davchev%20and%20Evgeny%20Gladchenko%20and%20David%20Madras%20and%20Aleksandr%20Chuklin%20and%20Tyrone%20Hill%20and%20Quan%20Yuan%20and%20Mukundan%20Madhavan%20and%20Luke%20Leonhard%20and%20Dylan%20Scandinaro%20and%20Qihang%20Chen%20and%20Ning%20Niu%20and%20Arthur%20Douillard%20and%20Bogdan%20Damoc%20and%20Yasumasa%20Onoe%20and%20Fabian%20Pedregosa%20and%20Fred%20Bertsch%20and%20Chas%20Leichner%20and%20Joseph%20Pagadora%20and%20Jonathan%20Malmaud%20and%20Sameera%20Ponda%20and%20Andy%20Twigg%20and%20Oleksii%20Duzhyi%20and%20Jingwei%20Shen%20and%20Miaosen%20Wang%20and%20Roopal%20Garg%20and%20Jing%20Chen%20and%20Utku%20Evci%20and%20Jonathan%20Lee%20and%20Leon%20Liu%20and%20Koji%20Kojima%20and%20Masa%20Yamaguchi%20and%20Arunkumar%20Rajendran%20and%20AJ%20Piergiovanni%20and%20Vinodh%20Kumar%20Rajendran%20and%20Marco%20Fornoni%20and%20Gabriel%20Ibagon%20and%20Harry%20Ragan%20and%20Sadh%20MNM%20Khan%20and%20John%20Blitzer%20and%20Andrew%20Bunner%20and%20Guan%20Sun%20and%20Takahiro%20Kosakai%20and%20Scott%20Lundberg%20and%20Ndidi%20Elue%20and%20Kelvin%20Guu%20and%20SK%20Park%20and%20Jane%20Park%20and%20Arunachalam%20Narayanaswamy%20and%20Chengda%20Wu%20and%20Jayaram%20Mudigonda%20and%20Trevor%20Cohn%20and%20Hairong%20Mu%20and%20Ravi%20Kumar%20and%20Laura%20Graesser%20and%20Yichi%20Zhang%20and%20Richard%20Killam%20and%20Vincent%20Zhuang%20and%20Mai%20Gim%C3%A9nez%20and%20Wael%20Al%20Jishi%20and%20Ruy%20Ley-Wild%20and%20Alex%20Zhai%20and%20Kazuki%20Osawa%20and%20Diego%20Cedillo%20and%20Jialu%20Liu%20and%20Mayank%20Upadhyay%20and%20Marcin%20Sieniek%20and%20Roshan%20Sharma%20and%20Tom%20Paine%20and%20Anelia%20Angelova%20and%20Sravanti%20Addepalli%20and%20Carolina%20Parada%20and%20Kingshuk%20Majumder%20and%20Avery%20Lamp%20and%20Sanjiv%20Kumar%20and%20Xiang%20Deng%20and%20Artiom%20Myaskovsky%20and%20Tea%20Saboli%C4%87%20and%20Jeffrey%20Dudek%20and%20Sarah%20York%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Jiazhong%20Nie%20and%20Dee%20Cattle%20and%20Alok%20Gunjan%20and%20Bilal%20Piot%20and%20Waleed%20Khawaja%20and%20Seojin%20Bang%20and%20Simon%20Wang%20and%20Siavash%20Khodadadeh%20and%20Raghavender%20R%20and%20Praynaa%20Rawlani%20and%20Richard%20Powell%20and%20Kevin%20Lee%20and%20Johannes%20Griesser%20and%20GS%20Oh%20and%20Cesar%20Magalhaes%20and%20Yujia%20Li%20and%20Simon%20Tokumine%20and%20Hadas%20Natalie%20Vogel%20and%20Dennis%20Hsu%20and%20Arturo%20BC%20and%20Disha%20Jindal%20and%20Matan%20Cohen%20and%20Zi%20Yang%20and%20Junwei%20Yuan%20and%20Dario%20de%20Cesare%20and%20Tony%20Bruguier%20and%20Jun%20Xu%20and%20Monica%20Roy%20and%20Alon%20Jacovi%20and%20Dan%20Belov%20and%20Rahul%20Arya%20and%20Phoenix%20Meadowlark%20and%20Shlomi%20Cohen-Ganor%20and%20Wenting%20Ye%20and%20Patrick%20Morris-Suzuki%20and%20Praseem%20Banzal%20and%20Gan%20Song%20and%20Pranavaraj%20Ponnuramu%20and%20Fred%20Zhang%20and%20George%20Scrivener%20and%20Salah%20Zaiem%20and%20Alif%20Raditya%20Rochman%20and%20Kehang%20Han%20and%20Badih%20Ghazi%20and%20Kate%20Lee%20and%20Shahar%20Drath%20and%20Daniel%20Suo%20and%20Antonious%20Girgis%20and%20Pradeep%20Shenoy%20and%20Duy%20Nguyen%20and%20Douglas%20Eck%20and%20Somit%20Gupta%20and%20Le%20Yan%20and%20Joao%20Carreira%20and%20Anmol%20Gulati%20and%20Ruoxin%20Sang%20and%20Daniil%20Mirylenka%20and%20Emma%20Cooney%20and%20Edward%20Chou%20and%20Mingyang%20Ling%20and%20Cindy%20Fan%20and%20Ben%20Coleman%20and%20Guilherme%20Tubone%20and%20Ravin%20Kumar%20and%20Jason%20Baldridge%20and%20Felix%20Hernandez-Campos%20and%20Angeliki%20Lazaridou%20and%20James%20Besley%20and%20Itay%20Yona%20and%20Neslihan%20Bulut%20and%20Quentin%20Wellens%20and%20AJ%20Pierigiovanni%20and%20Jasmine%20George%20and%20Richard%20Green%20and%20Pu%20Han%20and%20Connie%20Tao%20and%20Geoff%20Clark%20and%20Chong%20You%20and%20Abbas%20Abdolmaleki%20and%20Justin%20Fu%20and%20Tongzhou%20Chen%20and%20Ashwin%20Chaugule%20and%20Angad%20Chandorkar%20and%20Altaf%20Rahman%20and%20Will%20Thompson%20and%20Penporn%20Koanantakool%20and%20Mike%20Bernico%20and%20Jie%20Ren%20and%20Andrey%20Vlasov%20and%20Sergei%20Vassilvitskii%20and%20Maciej%20Kula%20and%20Yizhong%20Liang%20and%20Dahun%20Kim%20and%20Yangsibo%20Huang%20and%20Chengxi%20Ye%20and%20Dmitry%20Lepikhin%20and%20Wesley%20Helmholz%0AAbstract%3A%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%202.X%20model%20family%3A%20Gemini%202.5%20Pro%20and%20Gemini%202.5%20Flash%2C%20as%20well%20as%20our%20earlier%20Gemini%202.0%20Flash%20and%20Flash-Lite%20models.%20Gemini%202.5%20Pro%20is%20our%20most%20capable%20model%20yet%2C%20achieving%20SoTA%20performance%20on%20frontier%20coding%20and%20reasoning%20benchmarks.%20In%20addition%20to%20its%20incredible%20coding%20and%20reasoning%20skills%2C%20Gemini%202.5%20Pro%20is%20a%20thinking%20model%20that%20excels%20at%20multimodal%20understanding%20and%20it%20is%20now%20able%20to%20process%20up%20to%203%20hours%20of%20video%20content.%20Its%20unique%20combination%20of%20long%20context%2C%20multimodal%20and%20reasoning%20capabilities%20can%20be%20combined%20to%20unlock%20new%20agentic%20workflows.%20Gemini%202.5%20Flash%20provides%20excellent%20reasoning%20abilities%20at%20a%20fraction%20of%20the%20compute%20and%20latency%20requirements%20and%20Gemini%202.0%20Flash%20and%20Flash-Lite%20provide%20high%20performance%20at%20low%20latency%20and%20cost.%20Taken%20together%2C%20the%20Gemini%202.X%20model%20generation%20spans%20the%20full%20Pareto%20frontier%20of%20model%20capability%20vs%20cost%2C%20allowing%20users%20to%20explore%20the%20boundaries%20of%20what%20is%20possible%20with%20complex%20agentic%20problem%20solving.%0ALink%3A%20http%3A//arxiv.org/abs/2507.06261v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25202.5%253A%2520Pushing%2520the%2520Frontier%2520with%2520Advanced%2520Reasoning%252C%2520Multimodality%252C%2520Long%2520Context%252C%2520and%2520Next%2520Generation%2520Agentic%2520Capabilities%26entry.906535625%3DGheorghe%2520Comanici%2520and%2520Eric%2520Bieber%2520and%2520Mike%2520Schaekermann%2520and%2520Ice%2520Pasupat%2520and%2520Noveen%2520Sachdeva%2520and%2520Inderjit%2520Dhillon%2520and%2520Marcel%2520Blistein%2520and%2520Ori%2520Ram%2520and%2520Dan%2520Zhang%2520and%2520Evan%2520Rosen%2520and%2520Luke%2520Marris%2520and%2520Sam%2520Petulla%2520and%2520Colin%2520Gaffney%2520and%2520Asaf%2520Aharoni%2520and%2520Nathan%2520Lintz%2520and%2520Tiago%2520Cardal%2520Pais%2520and%2520Henrik%2520Jacobsson%2520and%2520Idan%2520Szpektor%2520and%2520Nan-Jiang%2520Jiang%2520and%2520Krishna%2520Haridasan%2520and%2520Ahmed%2520Omran%2520and%2520Nikunj%2520Saunshi%2520and%2520Dara%2520Bahri%2520and%2520Gaurav%2520Mishra%2520and%2520Eric%2520Chu%2520and%2520Toby%2520Boyd%2520and%2520Brad%2520Hekman%2520and%2520Aaron%2520Parisi%2520and%2520Chaoyi%2520Zhang%2520and%2520Kornraphop%2520Kawintiranon%2520and%2520Tania%2520Bedrax-Weiss%2520and%2520Oliver%2520Wang%2520and%2520Ya%2520Xu%2520and%2520Ollie%2520Purkiss%2520and%2520Uri%2520Mendlovic%2520and%2520Ila%25C3%25AF%2520Deutel%2520and%2520Nam%2520Nguyen%2520and%2520Adam%2520Langley%2520and%2520Flip%2520Korn%2520and%2520Lucia%2520Rossazza%2520and%2520Alexandre%2520Ram%25C3%25A9%2520and%2520Sagar%2520Waghmare%2520and%2520Helen%2520Miller%2520and%2520Nathan%2520Byrd%2520and%2520Ashrith%2520Sheshan%2520and%2520Raia%2520Hadsell%2520and%2520Sangnie%2520Bhardwaj%2520and%2520Pawel%2520Janus%2520and%2520Tero%2520Rissa%2520and%2520Dan%2520Horgan%2520and%2520Alvin%2520Abdagic%2520and%2520Lior%2520Belenki%2520and%2520James%2520Allingham%2520and%2520Anima%2520Singh%2520and%2520Theo%2520Guidroz%2520and%2520Srivatsan%2520Srinivasan%2520and%2520Herman%2520Schmit%2520and%2520Kristen%2520Chiafullo%2520and%2520Andre%2520Elisseeff%2520and%2520Nilpa%2520Jha%2520and%2520Prateek%2520Kolhar%2520and%2520Leonard%2520Berrada%2520and%2520Frank%2520Ding%2520and%2520Xiance%2520Si%2520and%2520Shrestha%2520Basu%2520Mallick%2520and%2520Franz%2520Och%2520and%2520Sofia%2520Erell%2520and%2520Eric%2520Ni%2520and%2520Tejasi%2520Latkar%2520and%2520Sherry%2520Yang%2520and%2520Petar%2520Sirkovic%2520and%2520Ziqiang%2520Feng%2520and%2520Robert%2520Leland%2520and%2520Rachel%2520Hornung%2520and%2520Gang%2520Wu%2520and%2520Charles%2520Blundell%2520and%2520Hamidreza%2520Alvari%2520and%2520Po-Sen%2520Huang%2520and%2520Cathy%2520Yip%2520and%2520Sanja%2520Deur%2520and%2520Li%2520Liu%2520and%2520Gabriela%2520Surita%2520and%2520Pablo%2520Duque%2520and%2520Dima%2520Damen%2520and%2520Johnson%2520Jia%2520and%2520Arthur%2520Guez%2520and%2520Markus%2520Mircea%2520and%2520Animesh%2520Sinha%2520and%2520Alberto%2520Magni%2520and%2520Pawe%25C5%2582%2520Stradomski%2520and%2520Tal%2520Marian%2520and%2520Vlado%2520Gali%25C4%2587%2520and%2520Wenhu%2520Chen%2520and%2520Hisham%2520Husain%2520and%2520Achintya%2520Singhal%2520and%2520Dominik%2520Grewe%2520and%2520Fran%25C3%25A7ois-Xavier%2520Aubet%2520and%2520Shuang%2520Song%2520and%2520Lorenzo%2520Blanco%2520and%2520Leland%2520Rechis%2520and%2520Lewis%2520Ho%2520and%2520Rich%2520Munoz%2520and%2520Kelvin%2520Zheng%2520and%2520Jessica%2520Hamrick%2520and%2520Kevin%2520Mather%2520and%2520Hagai%2520Taitelbaum%2520and%2520Eliza%2520Rutherford%2520and%2520Yun%2520Lei%2520and%2520Kuangyuan%2520Chen%2520and%2520Anand%2520Shukla%2520and%2520Erica%2520Moreira%2520and%2520Eric%2520Doi%2520and%2520Berivan%2520Isik%2520and%2520Nir%2520Shabat%2520and%2520Dominika%2520Rogozi%25C5%2584ska%2520and%2520Kashyap%2520Kolipaka%2520and%2520Jason%2520Chang%2520and%2520Eugen%2520Vu%25C5%25A1ak%2520and%2520Srinivasan%2520Venkatachary%2520and%2520Shadi%2520Noghabi%2520and%2520Tarun%2520Bharti%2520and%2520Younghoon%2520Jun%2520and%2520Aleksandr%2520Zaks%2520and%2520Simon%2520Green%2520and%2520Jeshwanth%2520Challagundla%2520and%2520William%2520Wong%2520and%2520Muqthar%2520Mohammad%2520and%2520Dean%2520Hirsch%2520and%2520Yong%2520Cheng%2520and%2520Iftekhar%2520Naim%2520and%2520Lev%2520Proleev%2520and%2520Damien%2520Vincent%2520and%2520Aayush%2520Singh%2520and%2520Maxim%2520Krikun%2520and%2520Dilip%2520Krishnan%2520and%2520Zoubin%2520Ghahramani%2520and%2520Aviel%2520Atias%2520and%2520Rajeev%2520Aggarwal%2520and%2520Christo%2520Kirov%2520and%2520Dimitrios%2520Vytiniotis%2520and%2520Christy%2520Koh%2520and%2520Alexandra%2520Chronopoulou%2520and%2520Pawan%2520Dogra%2520and%2520Vlad-Doru%2520Ion%2520and%2520Gladys%2520Tyen%2520and%2520Jason%2520Lee%2520and%2520Felix%2520Weissenberger%2520and%2520Trevor%2520Strohman%2520and%2520Ashwin%2520Balakrishna%2520and%2520Jack%2520Rae%2520and%2520Marko%2520Velic%2520and%2520Raoul%2520de%2520Liedekerke%2520and%2520Oded%2520Elyada%2520and%2520Wentao%2520Yuan%2520and%2520Canoee%2520Liu%2520and%2520Lior%2520Shani%2520and%2520Sergey%2520Kishchenko%2520and%2520Bea%2520Alessio%2520and%2520Yandong%2520Li%2520and%2520Richard%2520Song%2520and%2520Sam%2520Kwei%2520and%2520Orion%2520Jankowski%2520and%2520Aneesh%2520Pappu%2520and%2520Youhei%2520Namiki%2520and%2520Yenai%2520Ma%2520and%2520Nilesh%2520Tripuraneni%2520and%2520Colin%2520Cherry%2520and%2520Marissa%2520Ikonomidis%2520and%2520Yu-Cheng%2520Ling%2520and%2520Colin%2520Ji%2520and%2520Beka%2520Westberg%2520and%2520Auriel%2520Wright%2520and%2520Da%2520Yu%2520and%2520David%2520Parkinson%2520and%2520Swaroop%2520Ramaswamy%2520and%2520Jerome%2520Connor%2520and%2520Soheil%2520Hassas%2520Yeganeh%2520and%2520Snchit%2520Grover%2520and%2520George%2520Kenwright%2520and%2520Lubo%2520Litchev%2520and%2520Chris%2520Apps%2520and%2520Alex%2520Tomala%2520and%2520Felix%2520Halim%2520and%2520Alex%2520Castro-Ros%2520and%2520Zefei%2520Li%2520and%2520Anudhyan%2520Boral%2520and%2520Pauline%2520Sho%2520and%2520Michal%2520Yarom%2520and%2520Eric%2520Malmi%2520and%2520David%2520Klinghoffer%2520and%2520Rebecca%2520Lin%2520and%2520Alan%2520Ansell%2520and%2520Pradeep%2520Kumar%2520S%2520and%2520Shubin%2520Zhao%2520and%2520Siqi%2520Zuo%2520and%2520Adam%2520Santoro%2520and%2520Heng-Tze%2520Cheng%2520and%2520Solomon%2520Demmessie%2520and%2520Yuchi%2520Liu%2520and%2520Nicole%2520Brichtova%2520and%2520Allie%2520Culp%2520and%2520Nathaniel%2520Braun%2520and%2520Dan%2520Graur%2520and%2520Will%2520Ng%2520and%2520Nikhil%2520Mehta%2520and%2520Aaron%2520Phillips%2520and%2520Patrik%2520Sundberg%2520and%2520Varun%2520Godbole%2520and%2520Fangyu%2520Liu%2520and%2520Yash%2520Katariya%2520and%2520David%2520Rim%2520and%2520Mojtaba%2520Seyedhosseini%2520and%2520Sean%2520Ammirati%2520and%2520Jonas%2520Valfridsson%2520and%2520Mahan%2520Malihi%2520and%2520Timothy%2520Knight%2520and%2520Andeep%2520Toor%2520and%2520Thomas%2520Lampe%2520and%2520Abe%2520Ittycheriah%2520and%2520Lewis%2520Chiang%2520and%2520Chak%2520Yeung%2520and%2520Alexandre%2520Fr%25C3%25A9chette%2520and%2520Jinmeng%2520Rao%2520and%2520Huisheng%2520Wang%2520and%2520Himanshu%2520Srivastava%2520and%2520Richard%2520Zhang%2520and%2520Rocky%2520Rhodes%2520and%2520Ariel%2520Brand%2520and%2520Dean%2520Weesner%2520and%2520Ilya%2520Figotin%2520and%2520Felix%2520Gimeno%2520and%2520Rachana%2520Fellinger%2520and%2520Pierre%2520Marcenac%2520and%2520Jos%25C3%25A9%2520Leal%2520and%2520Eyal%2520Marcus%2520and%2520Victor%2520Cotruta%2520and%2520Rodrigo%2520Cabrera%2520and%2520Sheryl%2520Luo%2520and%2520Dan%2520Garrette%2520and%2520Vera%2520Axelrod%2520and%2520Sorin%2520Baltateanu%2520and%2520David%2520Barker%2520and%2520Dongkai%2520Chen%2520and%2520Horia%2520Toma%2520and%2520Ben%2520Ingram%2520and%2520Jason%2520Riesa%2520and%2520Chinmay%2520Kulkarni%2520and%2520Yujing%2520Zhang%2520and%2520Hongbin%2520Liu%2520and%2520Chao%2520Wang%2520and%2520Martin%2520Polacek%2520and%2520Will%2520Wu%2520and%2520Kai%2520Hui%2520and%2520Adrian%2520N%2520Reyes%2520and%2520Yi%2520Su%2520and%2520Megan%2520Barnes%2520and%2520Ishaan%2520Malhi%2520and%2520Anfal%2520Siddiqui%2520and%2520Qixuan%2520Feng%2520and%2520Mihai%2520Damaschin%2520and%2520Daniele%2520Pighin%2520and%2520Andreas%2520Steiner%2520and%2520Samuel%2520Yang%2520and%2520Ramya%2520Sree%2520Boppana%2520and%2520Simeon%2520Ivanov%2520and%2520Arun%2520Kandoor%2520and%2520Aditya%2520Shah%2520and%2520Asier%2520Mujika%2520and%2520Da%2520Huang%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Mohak%2520Patel%2520and%2520Tianhe%2520Yu%2520and%2520Toni%2520Creswell%2520and%2520%2520Jerry%2520and%2520%2520Liu%2520and%2520Catarina%2520Barros%2520and%2520Yasaman%2520Razeghi%2520and%2520Aurko%2520Roy%2520and%2520Phil%2520Culliton%2520and%2520Binbin%2520Xiong%2520and%2520Jiaqi%2520Pan%2520and%2520Thomas%2520Strohmann%2520and%2520Tolly%2520Powell%2520and%2520Babi%2520Seal%2520and%2520Doug%2520DeCarlo%2520and%2520Pranav%2520Shyam%2520and%2520Kaan%2520Katircioglu%2520and%2520Xuezhi%2520Wang%2520and%2520Cassidy%2520Hardin%2520and%2520Immanuel%2520Odisho%2520and%2520Josef%2520Broder%2520and%2520Oscar%2520Chang%2520and%2520Arun%2520Nair%2520and%2520Artem%2520Shtefan%2520and%2520Maura%2520O%2527Brien%2520and%2520Manu%2520Agarwal%2520and%2520Sahitya%2520Potluri%2520and%2520Siddharth%2520Goyal%2520and%2520Amit%2520Jhindal%2520and%2520Saksham%2520Thakur%2520and%2520Yury%2520Stuken%2520and%2520James%2520Lyon%2520and%2520Kristina%2520Toutanova%2520and%2520Fangxiaoyu%2520Feng%2520and%2520Austin%2520Wu%2520and%2520Ben%2520Horn%2520and%2520Alek%2520Wang%2520and%2520Alex%2520Cullum%2520and%2520Gabe%2520Taubman%2520and%2520Disha%2520Shrivastava%2520and%2520Chongyang%2520Shi%2520and%2520Hamish%2520Tomlinson%2520and%2520Roma%2520Patel%2520and%2520Tao%2520Tu%2520and%2520Ada%2520Maksutaj%2520Oflazer%2520and%2520Francesco%2520Pongetti%2520and%2520Mingyao%2520Yang%2520and%2520Adrien%2520Ali%2520Ta%25C3%25AFga%2520and%2520Vincent%2520Perot%2520and%2520Nuo%2520Wang%2520Pierse%2520and%2520Feng%2520Han%2520and%2520Yoel%2520Drori%2520and%2520I%25C3%25B1aki%2520Iturrate%2520and%2520Ayan%2520Chakrabarti%2520and%2520Legg%2520Yeung%2520and%2520Dave%2520Dopson%2520and%2520Yi-ting%2520Chen%2520and%2520Apoorv%2520Kulshreshtha%2520and%2520Tongfei%2520Guo%2520and%2520Philip%2520Pham%2520and%2520Tal%2520Schuster%2520and%2520Junquan%2520Chen%2520and%2520Alex%2520Polozov%2520and%2520Jinwei%2520Xing%2520and%2520Huanjie%2520Zhou%2520and%2520Praneeth%2520Kacham%2520and%2520Doron%2520Kukliansky%2520and%2520Antoine%2520Miech%2520and%2520Sergey%2520Yaroshenko%2520and%2520Ed%2520Chi%2520and%2520Sholto%2520Douglas%2520and%2520Hongliang%2520Fei%2520and%2520Mathieu%2520Blondel%2520and%2520Preethi%2520Myla%2520and%2520Lior%2520Madmoni%2520and%2520Xing%2520Wu%2520and%2520Daniel%2520Keysers%2520and%2520Kristian%2520Kjems%2520and%2520Isabela%2520Albuquerque%2520and%2520Lijun%2520Yu%2520and%2520Joel%2520D%2527sa%2520and%2520Michelle%2520Plantan%2520and%2520Vlad%2520Ionescu%2520and%2520Jaume%2520Sanchez%2520Elias%2520and%2520Abhirut%2520Gupta%2520and%2520Manish%2520Reddy%2520Vuyyuru%2520and%2520Fred%2520Alcober%2520and%2520Tong%2520Zhou%2520and%2520Kaiyang%2520Ji%2520and%2520Florian%2520Hartmann%2520and%2520Subha%2520Puttagunta%2520and%2520Hugo%2520Song%2520and%2520Ehsan%2520Amid%2520and%2520Anca%2520Stefanoiu%2520and%2520Andrew%2520Lee%2520and%2520Paul%2520Pucciarelli%2520and%2520Emma%2520Wang%2520and%2520Amit%2520Raul%2520and%2520Slav%2520Petrov%2520and%2520Isaac%2520Tian%2520and%2520Valentin%2520Anklin%2520and%2520Nana%2520Nti%2520and%2520Victor%2520Gomes%2520and%2520Max%2520Schumacher%2520and%2520Grace%2520Vesom%2520and%2520Alex%2520Panagopoulos%2520and%2520Konstantinos%2520Bousmalis%2520and%2520Daniel%2520Andor%2520and%2520Josh%2520Jacob%2520and%2520Yuan%2520Zhang%2520and%2520Bill%2520Rosgen%2520and%2520Matija%2520Kecman%2520and%2520Matthew%2520Tung%2520and%2520Alexandra%2520Belias%2520and%2520Noah%2520Goodman%2520and%2520Paul%2520Covington%2520and%2520Brian%2520Wieder%2520and%2520Nikita%2520Saxena%2520and%2520Elnaz%2520Davoodi%2520and%2520Muhuan%2520Huang%2520and%2520Sharath%2520Maddineni%2520and%2520Vincent%2520Roulet%2520and%2520Folawiyo%2520Campbell-Ajala%2520and%2520Pier%2520Giuseppe%2520Sessa%2520and%2520%2520Xintian%2520and%2520%2520Wu%2520and%2520Guangda%2520Lai%2520and%2520Paul%2520Collins%2520and%2520Alex%2520Haig%2520and%2520Vytenis%2520Sakenas%2520and%2520Xiaowei%2520Xu%2520and%2520Marissa%2520Giustina%2520and%2520Laurent%2520El%2520Shafey%2520and%2520Pichi%2520Charoenpanit%2520and%2520Shefali%2520Garg%2520and%2520Joshua%2520Ainslie%2520and%2520Boone%2520Severson%2520and%2520Montse%2520Gonzalez%2520Arenas%2520and%2520Shreya%2520Pathak%2520and%2520Sujee%2520Rajayogam%2520and%2520Jie%2520Feng%2520and%2520Michiel%2520Bakker%2520and%2520Sheng%2520Li%2520and%2520Nevan%2520Wichers%2520and%2520Jamie%2520Rogers%2520and%2520Xinyang%2520Geng%2520and%2520Yeqing%2520Li%2520and%2520Rolf%2520Jagerman%2520and%2520Chao%2520Jia%2520and%2520Nadav%2520Olmert%2520and%2520David%2520Sharon%2520and%2520Matthew%2520Mauger%2520and%2520Sandeep%2520Mariserla%2520and%2520Hongxu%2520Ma%2520and%2520Megha%2520Mohabey%2520and%2520Kyuyeun%2520Kim%2520and%2520Alek%2520Andreev%2520and%2520Scott%2520Pollom%2520and%2520Juliette%2520Love%2520and%2520Vihan%2520Jain%2520and%2520Priyanka%2520Agrawal%2520and%2520Yannick%2520Schroecker%2520and%2520Alisa%2520Fortin%2520and%2520Manfred%2520Warmuth%2520and%2520Ji%2520Liu%2520and%2520Andrew%2520Leach%2520and%2520Irina%2520Blok%2520and%2520Ganesh%2520Poomal%2520Girirajan%2520and%2520Roee%2520Aharoni%2520and%2520Benigno%2520Uria%2520and%2520Andrei%2520Sozanschi%2520and%2520Dan%2520Goldberg%2520and%2520Lucian%2520Ionita%2520and%2520Marco%2520Tulio%2520Ribeiro%2520and%2520Martin%2520Zlocha%2520and%2520Vighnesh%2520Birodkar%2520and%2520Sami%2520Lachgar%2520and%2520Liangzhe%2520Yuan%2520and%2520Himadri%2520Choudhury%2520and%2520Matt%2520Ginsberg%2520and%2520Fei%2520Zheng%2520and%2520Gregory%2520Dibb%2520and%2520Emily%2520Graves%2520and%2520Swachhand%2520Lokhande%2520and%2520Gabriel%2520Rasskin%2520and%2520George-Cristian%2520Muraru%2520and%2520Corbin%2520Quick%2520and%2520Sandeep%2520Tata%2520and%2520Pierre%2520Sermanet%2520and%2520Aditya%2520Chawla%2520and%2520Itay%2520Karo%2520and%2520Yan%2520Wang%2520and%2520Susan%2520Zhang%2520and%2520Orgad%2520Keller%2520and%2520Anca%2520Dragan%2520and%2520Guolong%2520Su%2520and%2520Ian%2520Chou%2520and%2520Xi%2520Liu%2520and%2520Yiqing%2520Tao%2520and%2520Shruthi%2520Prabhakara%2520and%2520Marc%2520Wilson%2520and%2520Ruibo%2520Liu%2520and%2520Shibo%2520Wang%2520and%2520Georgie%2520Evans%2520and%2520David%2520Du%2520and%2520Alfonso%2520Casta%25C3%25B1o%2520and%2520Gautam%2520Prasad%2520and%2520Mona%2520El%2520Mahdy%2520and%2520Sebastian%2520Gerlach%2520and%2520Machel%2520Reid%2520and%2520Jarrod%2520Kahn%2520and%2520Amir%2520Zait%2520and%2520Thanumalayan%2520Sankaranarayana%2520Pillai%2520and%2520Thatcher%2520Ulrich%2520and%2520Guanyu%2520Wang%2520and%2520Jan%2520Wassenberg%2520and%2520Efrat%2520Farkash%2520and%2520Kiran%2520Yalasangi%2520and%2520Congchao%2520Wang%2520and%2520Maria%2520Bauza%2520and%2520Simon%2520Bucher%2520and%2520Ting%2520Liu%2520and%2520Jun%2520Yan%2520and%2520Gary%2520Leung%2520and%2520Vikas%2520Sindhwani%2520and%2520Parker%2520Barnes%2520and%2520Avi%2520Singh%2520and%2520Ivan%2520Jurin%2520and%2520Jichuan%2520Chang%2520and%2520Niket%2520Kumar%2520Bhumihar%2520and%2520Sivan%2520Eiger%2520and%2520Gui%2520Citovsky%2520and%2520Ben%2520Withbroe%2520and%2520Zhang%2520Li%2520and%2520Siyang%2520Xue%2520and%2520Niccol%25C3%25B2%2520Dal%2520Santo%2520and%2520Georgi%2520Stoyanov%2520and%2520Yves%2520Raimond%2520and%2520Steven%2520Zheng%2520and%2520Yilin%2520Gao%2520and%2520V%25C3%25ADt%2520List%25C3%25ADk%2520and%2520S%25C5%2582awek%2520Kwasiborski%2520and%2520Rachel%2520Saputro%2520and%2520Adnan%2520Ozturel%2520and%2520Ganesh%2520Mallya%2520and%2520Kushal%2520Majmundar%2520and%2520Ross%2520West%2520and%2520Paul%2520Caron%2520and%2520Jinliang%2520Wei%2520and%2520Lluis%2520Castrejon%2520and%2520Sharad%2520Vikram%2520and%2520Deepak%2520Ramachandran%2520and%2520Nikhil%2520Dhawan%2520and%2520Jiho%2520Park%2520and%2520Sara%2520Smoot%2520and%2520George%2520van%2520den%2520Driessche%2520and%2520Yochai%2520Blau%2520and%2520Chase%2520Malik%2520and%2520Wei%2520Liang%2520and%2520Roy%2520Hirsch%2520and%2520Cicero%2520Nogueira%2520dos%2520Santos%2520and%2520Eugene%2520Weinstein%2520and%2520A%25C3%25A4ron%2520van%2520den%2520Oord%2520and%2520Sid%2520Lall%2520and%2520Nicholas%2520FitzGerald%2520and%2520Zixuan%2520Jiang%2520and%2520Xuan%2520Yang%2520and%2520Dale%2520Webster%2520and%2520Ali%2520Elqursh%2520and%2520Aedan%2520Pope%2520and%2520Georges%2520Rotival%2520and%2520David%2520Raposo%2520and%2520Wanzheng%2520Zhu%2520and%2520Jeff%2520Dean%2520and%2520Sami%2520Alabed%2520and%2520Dustin%2520Tran%2520and%2520Arushi%2520Gupta%2520and%2520Zach%2520Gleicher%2520and%2520Jessica%2520Austin%2520and%2520Edouard%2520Rosseel%2520and%2520Megh%2520Umekar%2520and%2520Dipanjan%2520Das%2520and%2520Yinghao%2520Sun%2520and%2520Kai%2520Chen%2520and%2520Karolis%2520Misiunas%2520and%2520Xiang%2520Zhou%2520and%2520Yixian%2520Di%2520and%2520Alyssa%2520Loo%2520and%2520Josh%2520Newlan%2520and%2520Bo%2520Li%2520and%2520Vinay%2520Ramasesh%2520and%2520Ying%2520Xu%2520and%2520Alex%2520Chen%2520and%2520Sudeep%2520Gandhe%2520and%2520Radu%2520Soricut%2520and%2520Nikita%2520Gupta%2520and%2520Shuguang%2520Hu%2520and%2520Seliem%2520El-Sayed%2520and%2520Xavier%2520Garcia%2520and%2520Idan%2520Brusilovsky%2520and%2520Pu-Chin%2520Chen%2520and%2520Andrew%2520Bolt%2520and%2520Lu%2520Huang%2520and%2520Alex%2520Gurney%2520and%2520Zhiying%2520Zhang%2520and%2520Alexander%2520Pritzel%2520and%2520Jarek%2520Wilkiewicz%2520and%2520Bryan%2520Seybold%2520and%2520Bhargav%2520Kanagal%2520Shamanna%2520and%2520Felix%2520Fischer%2520and%2520Josef%2520Dean%2520and%2520Karan%2520Gill%2520and%2520Ross%2520Mcilroy%2520and%2520Abhishek%2520Bhowmick%2520and%2520Jeremy%2520Selier%2520and%2520Antoine%2520Yang%2520and%2520Derek%2520Cheng%2520and%2520Vladimir%2520Magay%2520and%2520Jie%2520Tan%2520and%2520Dhriti%2520Varma%2520and%2520Christian%2520Walder%2520and%2520Tomas%2520Kocisky%2520and%2520Ryo%2520Nakashima%2520and%2520Paul%2520Natsev%2520and%2520Mike%2520Kwong%2520and%2520Ionel%2520Gog%2520and%2520Chiyuan%2520Zhang%2520and%2520Sander%2520Dieleman%2520and%2520Thomas%2520Jimma%2520and%2520Andrey%2520Ryabtsev%2520and%2520Siddhartha%2520Brahma%2520and%2520David%2520Steiner%2520and%2520Dayou%2520Du%2520and%2520Ante%2520%25C5%25BDu%25C5%25BEul%2520and%2520Mislav%2520%25C5%25BDani%25C4%2587%2520and%2520Mukund%2520Raghavachari%2520and%2520Willi%2520Gierke%2520and%2520Zeyu%2520Zheng%2520and%2520Dessie%2520Petrova%2520and%2520Yann%2520Dauphin%2520and%2520Yuchuan%2520Liu%2520and%2520Ido%2520Kessler%2520and%2520Steven%2520Hand%2520and%2520Chris%2520Duvarney%2520and%2520Seokhwan%2520Kim%2520and%2520Hyo%2520Lee%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Jeffrey%2520Hui%2520and%2520Josh%2520Smith%2520and%2520Deepali%2520Jain%2520and%2520Jiawei%2520Xia%2520and%2520Gaurav%2520Singh%2520Tomar%2520and%2520Keyvan%2520Amiri%2520and%2520Du%2520Phan%2520and%2520Fabian%2520Fuchs%2520and%2520Tobias%2520Weyand%2520and%2520Nenad%2520Tomasev%2520and%2520Alexandra%2520Cordell%2520and%2520Xin%2520Liu%2520and%2520Jonathan%2520Mallinson%2520and%2520Pankaj%2520Joshi%2520and%2520Andy%2520Crawford%2520and%2520Arun%2520Suggala%2520and%2520Steve%2520Chien%2520and%2520Nick%2520Fernando%2520and%2520Mariella%2520Sanchez-Vargas%2520and%2520Duncan%2520Williams%2520and%2520Phil%2520Crone%2520and%2520Xiyang%2520Luo%2520and%2520Igor%2520Karpov%2520and%2520Jyn%2520Shan%2520and%2520Terry%2520Thurk%2520and%2520Robin%2520Strudel%2520and%2520Paul%2520Voigtlaender%2520and%2520Piyush%2520Patil%2520and%2520Tim%2520Dozat%2520and%2520Ali%2520Khodaei%2520and%2520Sahil%2520Singla%2520and%2520Piotr%2520Ambroszczyk%2520and%2520Qiyin%2520Wu%2520and%2520Yifan%2520Chang%2520and%2520Brian%2520Roark%2520and%2520Chaitra%2520Hegde%2520and%2520Tianli%2520Ding%2520and%2520Angelos%2520Filos%2520and%2520Zhongru%2520Wu%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Shuang%2520Liu%2520and%2520Saarthak%2520Khanna%2520and%2520Aditya%2520Pandey%2520and%2520Siobhan%2520Mcloughlin%2520and%2520Qiujia%2520Li%2520and%2520Sam%2520Haves%2520and%2520Allan%2520Zhou%2520and%2520Elena%2520Buchatskaya%2520and%2520Isabel%2520Leal%2520and%2520Peter%2520de%2520Boursac%2520and%2520Nami%2520Akazawa%2520and%2520Nina%2520Anderson%2520and%2520Terry%2520Chen%2520and%2520Krishna%2520Somandepalli%2520and%2520Chen%2520Liang%2520and%2520Sheela%2520Goenka%2520and%2520Stephanie%2520Winkler%2520and%2520Alexander%2520Grushetsky%2520and%2520Yifan%2520Ding%2520and%2520Jamie%2520Smith%2520and%2520Fan%2520Ye%2520and%2520Jordi%2520Pont-Tuset%2520and%2520Eric%2520Li%2520and%2520Ruichao%2520Li%2520and%2520Tomer%2520Golany%2520and%2520Dawid%2520Wegner%2520and%2520Tao%2520Jiang%2520and%2520Omer%2520Barak%2520and%2520Yuan%2520Shangguan%2520and%2520Eszter%2520V%25C3%25A9rtes%2520and%2520Renee%2520Wong%2520and%2520J%25C3%25B6rg%2520Bornschein%2520and%2520Alex%2520Tudor%2520and%2520Michele%2520Bevilacqua%2520and%2520Tom%2520Schaul%2520and%2520Ankit%2520Singh%2520Rawat%2520and%2520Yang%2520Zhao%2520and%2520Kyriakos%2520Axiotis%2520and%2520Lei%2520Meng%2520and%2520Cory%2520McLean%2520and%2520Jonathan%2520Lai%2520and%2520Jennifer%2520Beattie%2520and%2520Nate%2520Kushman%2520and%2520Yaxin%2520Liu%2520and%2520Blair%2520Kutzman%2520and%2520Fiona%2520Lang%2520and%2520Jingchen%2520Ye%2520and%2520Praneeth%2520Netrapalli%2520and%2520Pushkar%2520Mishra%2520and%2520Myriam%2520Khan%2520and%2520Megha%2520Goel%2520and%2520Rob%2520Willoughby%2520and%2520David%2520Tian%2520and%2520Honglei%2520Zhuang%2520and%2520JD%2520Chen%2520and%2520Zak%2520Tsai%2520and%2520Tasos%2520Kementsietsidis%2520and%2520Arjun%2520Khare%2520and%2520James%2520Keeling%2520and%2520Keyang%2520Xu%2520and%2520Nathan%2520Waters%2520and%2520Florent%2520Altch%25C3%25A9%2520and%2520Ashok%2520Popat%2520and%2520Bhavishya%2520Mittal%2520and%2520David%2520Saxton%2520and%2520Dalia%2520El%2520Badawy%2520and%2520Michael%2520Mathieu%2520and%2520Zheng%2520Zheng%2520and%2520Hao%2520Zhou%2520and%2520Nishant%2520Ranka%2520and%2520Richard%2520Shin%2520and%2520Qingnan%2520Duan%2520and%2520Tim%2520Salimans%2520and%2520Ioana%2520Mihailescu%2520and%2520Uri%2520Shaham%2520and%2520Ming-Wei%2520Chang%2520and%2520Yannis%2520Assael%2520and%2520Nishanth%2520Dikkala%2520and%2520Martin%2520Izzard%2520and%2520Vincent%2520Cohen-Addad%2520and%2520Cat%2520Graves%2520and%2520Vlad%2520Feinberg%2520and%2520Grace%2520Chung%2520and%2520DJ%2520Strouse%2520and%2520Danny%2520Karmon%2520and%2520Sahand%2520Sharifzadeh%2520and%2520Zoe%2520Ashwood%2520and%2520Khiem%2520Pham%2520and%2520Jon%2520Blanton%2520and%2520Alex%2520Vasiloff%2520and%2520Jarred%2520Barber%2520and%2520Mark%2520Geller%2520and%2520Aurick%2520Zhou%2520and%2520Fedir%2520Zubach%2520and%2520Tzu-Kuo%2520Huang%2520and%2520Lei%2520Zhang%2520and%2520Himanshu%2520Gupta%2520and%2520Matt%2520Young%2520and%2520Julia%2520Proskurnia%2520and%2520Ronny%2520Votel%2520and%2520Valentin%2520Gabeur%2520and%2520Gabriel%2520Barcik%2520and%2520Aditya%2520Tripathi%2520and%2520Hongkun%2520Yu%2520and%2520Geng%2520Yan%2520and%2520Beer%2520Changpinyo%2520and%2520Filip%2520Paveti%25C4%2587%2520and%2520Amy%2520Coyle%2520and%2520Yasuhisa%2520Fujii%2520and%2520Jorge%2520Gonzalez%2520Mendez%2520and%2520Tianhao%2520Zhou%2520and%2520Harish%2520Rajamani%2520and%2520Blake%2520Hechtman%2520and%2520Eddie%2520Cao%2520and%2520Da-Cheng%2520Juan%2520and%2520Yi-Xuan%2520Tan%2520and%2520Valentin%2520Dalibard%2520and%2520Yilun%2520Du%2520and%2520Natalie%2520Clay%2520and%2520Kaisheng%2520Yao%2520and%2520Wenhao%2520Jia%2520and%2520Dimple%2520Vijaykumar%2520and%2520Yuxiang%2520Zhou%2520and%2520Xinyi%2520Bai%2520and%2520Wei-Chih%2520Hung%2520and%2520Steven%2520Pecht%2520and%2520Georgi%2520Todorov%2520and%2520Nikhil%2520Khadke%2520and%2520Pramod%2520Gupta%2520and%2520Preethi%2520Lahoti%2520and%2520Arnaud%2520Autef%2520and%2520Karthik%2520Duddu%2520and%2520James%2520Lee-Thorp%2520and%2520Alexander%2520Bykovsky%2520and%2520Tautvydas%2520Misiunas%2520and%2520Sebastian%2520Flennerhag%2520and%2520Santhosh%2520Thangaraj%2520and%2520Jed%2520McGiffin%2520and%2520Zack%2520Nado%2520and%2520Markus%2520Kunesch%2520and%2520Andreas%2520Noever%2520and%2520Amir%2520Hertz%2520and%2520Marco%2520Liang%2520and%2520Victor%2520Stone%2520and%2520Evan%2520Palmer%2520and%2520Samira%2520Daruki%2520and%2520Arijit%2520Pramanik%2520and%2520Siim%2520P%25C3%25B5der%2520and%2520Austin%2520Kyker%2520and%2520Mina%2520Khan%2520and%2520Evgeny%2520Sluzhaev%2520and%2520Marvin%2520Ritter%2520and%2520Avraham%2520Ruderman%2520and%2520Wenlei%2520Zhou%2520and%2520Chirag%2520Nagpal%2520and%2520Kiran%2520Vodrahalli%2520and%2520George%2520Necula%2520and%2520Paul%2520Barham%2520and%2520Ellie%2520Pavlick%2520and%2520Jay%2520Hartford%2520and%2520Izhak%2520Shafran%2520and%2520Long%2520Zhao%2520and%2520Maciej%2520Miku%25C5%2582a%2520and%2520Tom%2520Eccles%2520and%2520Hidetoshi%2520Shimokawa%2520and%2520Kanav%2520Garg%2520and%2520Luke%2520Vilnis%2520and%2520Hanwen%2520Chen%2520and%2520Ilia%2520Shumailov%2520and%2520Kuang-Huei%2520Lee%2520and%2520Abdelrahman%2520Abdelhamed%2520and%2520Meiyan%2520Xie%2520and%2520Vered%2520Cohen%2520and%2520Ester%2520Hlavnova%2520and%2520Dan%2520Malkin%2520and%2520Chawin%2520Sitawarin%2520and%2520James%2520Lottes%2520and%2520Pauline%2520Coquinot%2520and%2520Tianli%2520Yu%2520and%2520Sandeep%2520Kumar%2520and%2520Jingwei%2520Zhang%2520and%2520Aroma%2520Mahendru%2520and%2520Zafarali%2520Ahmed%2520and%2520James%2520Martens%2520and%2520Tao%2520Chen%2520and%2520Aviel%2520Boag%2520and%2520Daiyi%2520Peng%2520and%2520Coline%2520Devin%2520and%2520Arseniy%2520Klimovskiy%2520and%2520Mary%2520Phuong%2520and%2520Danny%2520Vainstein%2520and%2520Jin%2520Xie%2520and%2520Bhuvana%2520Ramabhadran%2520and%2520Nathan%2520Howard%2520and%2520Xinxin%2520Yu%2520and%2520Gitartha%2520Goswami%2520and%2520Jingyu%2520Cui%2520and%2520Sam%2520Shleifer%2520and%2520Mario%2520Pinto%2520and%2520Chih-Kuan%2520Yeh%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Sara%2520Javanmardi%2520and%2520Dan%2520Ethier%2520and%2520Chace%2520Lee%2520and%2520Jordi%2520Orbay%2520and%2520Suyog%2520Kotecha%2520and%2520Carla%2520Bromberg%2520and%2520Pete%2520Shaw%2520and%2520James%2520Thornton%2520and%2520Adi%2520Gerzi%2520Rosenthal%2520and%2520Shane%2520Gu%2520and%2520Matt%2520Thomas%2520and%2520Ian%2520Gemp%2520and%2520Aditya%2520Ayyar%2520and%2520Asahi%2520Ushio%2520and%2520Aarush%2520Selvan%2520and%2520Joel%2520Wee%2520and%2520Chenxi%2520Liu%2520and%2520Maryam%2520Majzoubi%2520and%2520Weiren%2520Yu%2520and%2520Jake%2520Abernethy%2520and%2520Tyler%2520Liechty%2520and%2520Renke%2520Pan%2520and%2520Hoang%2520Nguyen%2520and%2520%2520Qiong%2520and%2520%2520Hu%2520and%2520Sarah%2520Perrin%2520and%2520Abhinav%2520Arora%2520and%2520Emily%2520Pitler%2520and%2520Weiyi%2520Wang%2520and%2520Kaushik%2520Shivakumar%2520and%2520Flavien%2520Prost%2520and%2520Ben%2520Limonchik%2520and%2520Jing%2520Wang%2520and%2520Yi%2520Gao%2520and%2520Timothee%2520Cour%2520and%2520Shyamal%2520Buch%2520and%2520Huan%2520Gui%2520and%2520Maria%2520Ivanova%2520and%2520Philipp%2520Neubeck%2520and%2520Kelvin%2520Chan%2520and%2520Lucy%2520Kim%2520and%2520Huizhong%2520Chen%2520and%2520Naman%2520Goyal%2520and%2520Da-Woon%2520Chung%2520and%2520Lu%2520Liu%2520and%2520Yao%2520Su%2520and%2520Anastasia%2520Petrushkina%2520and%2520Jiajun%2520Shen%2520and%2520Armand%2520Joulin%2520and%2520Yuanzhong%2520Xu%2520and%2520Stein%2520Xudong%2520Lin%2520and%2520Yana%2520Kulizhskaya%2520and%2520Ciprian%2520Chelba%2520and%2520Shobha%2520Vasudevan%2520and%2520Eli%2520Collins%2520and%2520Vasilisa%2520Bashlovkina%2520and%2520Tony%2520Lu%2520and%2520Doug%2520Fritz%2520and%2520Jongbin%2520Park%2520and%2520Yanqi%2520Zhou%2520and%2520Chen%2520Su%2520and%2520Richard%2520Tanburn%2520and%2520Mikhail%2520Sushkov%2520and%2520Mitchelle%2520Rasquinha%2520and%2520Jinning%2520Li%2520and%2520Jennifer%2520Prendki%2520and%2520Yiming%2520Li%2520and%2520Pallavi%2520LV%2520and%2520Shriya%2520Sharma%2520and%2520Hen%2520Fitoussi%2520and%2520Hui%2520Huang%2520and%2520Andrew%2520Dai%2520and%2520Phuong%2520Dao%2520and%2520Mike%2520Burrows%2520and%2520Henry%2520Prior%2520and%2520Danfeng%2520Qin%2520and%2520Golan%2520Pundak%2520and%2520Lars%2520Lowe%2520Sjoesund%2520and%2520Art%2520Khurshudov%2520and%2520Zhenkai%2520Zhu%2520and%2520Albert%2520Webson%2520and%2520Elizabeth%2520Kemp%2520and%2520Tat%2520Tan%2520and%2520Saurabh%2520Agrawal%2520and%2520Susie%2520Sargsyan%2520and%2520Liqun%2520Cheng%2520and%2520Jim%2520Stephan%2520and%2520Tom%2520Kwiatkowski%2520and%2520David%2520Reid%2520and%2520Arunkumar%2520Byravan%2520and%2520Assaf%2520Hurwitz%2520Michaely%2520and%2520Nicolas%2520Heess%2520and%2520Luowei%2520Zhou%2520and%2520Sonam%2520Goenka%2520and%2520Viral%2520Carpenter%2520and%2520Anselm%2520Levskaya%2520and%2520Bo%2520Wang%2520and%2520Reed%2520Roberts%2520and%2520R%25C3%25A9mi%2520Leblond%2520and%2520Sharat%2520Chikkerur%2520and%2520Stav%2520Ginzburg%2520and%2520Max%2520Chang%2520and%2520Robert%2520Riachi%2520and%2520%2520Chuqiao%2520and%2520%2520Xu%2520and%2520Zal%25C3%25A1n%2520Borsos%2520and%2520Michael%2520Pliskin%2520and%2520Julia%2520Pawar%2520and%2520Morgane%2520Lustman%2520and%2520Hannah%2520Kirkwood%2520and%2520Ankit%2520Anand%2520and%2520Aditi%2520Chaudhary%2520and%2520Norbert%2520Kalb%2520and%2520Kieran%2520Milan%2520and%2520Sean%2520Augenstein%2520and%2520Anna%2520Goldie%2520and%2520Laurel%2520Prince%2520and%2520Karthik%2520Raman%2520and%2520Yanhua%2520Sun%2520and%2520Vivian%2520Xia%2520and%2520Aaron%2520Cohen%2520and%2520Zhouyuan%2520Huo%2520and%2520Josh%2520Camp%2520and%2520Seher%2520Ellis%2520and%2520Lukas%2520Zilka%2520and%2520David%2520Vilar%2520Torres%2520and%2520Lisa%2520Patel%2520and%2520Sho%2520Arora%2520and%2520Betty%2520Chan%2520and%2520Jonas%2520Adler%2520and%2520Kareem%2520Ayoub%2520and%2520Jacky%2520Liang%2520and%2520Fayaz%2520Jamil%2520and%2520Jiepu%2520Jiang%2520and%2520Simon%2520Baumgartner%2520and%2520Haitian%2520Sun%2520and%2520Yael%2520Karov%2520and%2520Yaroslav%2520Akulov%2520and%2520Hui%2520Zheng%2520and%2520Irene%2520Cai%2520and%2520Claudio%2520Fantacci%2520and%2520James%2520Rubin%2520and%2520Alex%2520Rav%2520Acha%2520and%2520Mengchao%2520Wang%2520and%2520Nina%2520D%2527Souza%2520and%2520Rohit%2520Sathyanarayana%2520and%2520Shengyang%2520Dai%2520and%2520Simon%2520Rowe%2520and%2520Andrey%2520Simanovsky%2520and%2520Omer%2520Goldman%2520and%2520Yuheng%2520Kuang%2520and%2520Xiaoyue%2520Pan%2520and%2520Andrew%2520Rosenberg%2520and%2520Tania%2520Rojas-Esponda%2520and%2520Praneet%2520Dutta%2520and%2520Amy%2520Zeng%2520and%2520Irina%2520Jurenka%2520and%2520Greg%2520Farquhar%2520and%2520Yamini%2520Bansal%2520and%2520Shariq%2520Iqbal%2520and%2520Becca%2520Roelofs%2520and%2520Ga-Young%2520Joung%2520and%2520Parker%2520Beak%2520and%2520Changwan%2520Ryu%2520and%2520Ryan%2520Poplin%2520and%2520Yan%2520Wu%2520and%2520Jean-Baptiste%2520Alayrac%2520and%2520Senaka%2520Buthpitiya%2520and%2520Olaf%2520Ronneberger%2520and%2520Caleb%2520Habtegebriel%2520and%2520Wei%2520Li%2520and%2520Paul%2520Cavallaro%2520and%2520Aurora%2520Wei%2520and%2520Guy%2520Bensky%2520and%2520Timo%2520Denk%2520and%2520Harish%2520Ganapathy%2520and%2520Jeff%2520Stanway%2520and%2520Pratik%2520Joshi%2520and%2520Francesco%2520Bertolini%2520and%2520Jessica%2520Lo%2520and%2520Olivia%2520Ma%2520and%2520Zachary%2520Charles%2520and%2520Geta%2520Sampemane%2520and%2520Himanshu%2520Sahni%2520and%2520Xu%2520Chen%2520and%2520Harry%2520Askham%2520and%2520David%2520Gaddy%2520and%2520Peter%2520Young%2520and%2520Jiewen%2520Tan%2520and%2520Matan%2520Eyal%2520and%2520Arthur%2520Bra%25C5%25BEinskas%2520and%2520Li%2520Zhong%2520and%2520Zhichun%2520Wu%2520and%2520Mark%2520Epstein%2520and%2520Kai%2520Bailey%2520and%2520Andrew%2520Hard%2520and%2520Kamyu%2520Lee%2520and%2520Sasha%2520Goldshtein%2520and%2520Alex%2520Ruiz%2520and%2520Mohammed%2520Badawi%2520and%2520Matthias%2520Lochbrunner%2520and%2520JK%2520Kearns%2520and%2520Ashley%2520Brown%2520and%2520Fabio%2520Pardo%2520and%2520Theophane%2520Weber%2520and%2520Haichuan%2520Yang%2520and%2520Pan-Pan%2520Jiang%2520and%2520Berkin%2520Akin%2520and%2520Zhao%2520Fu%2520and%2520Marcus%2520Wainwright%2520and%2520Chi%2520Zou%2520and%2520Meenu%2520Gaba%2520and%2520Pierre-Antoine%2520Manzagol%2520and%2520Wendy%2520Kan%2520and%2520Yang%2520Song%2520and%2520Karina%2520Zainullina%2520and%2520Rui%2520Lin%2520and%2520Jeongwoo%2520Ko%2520and%2520Salil%2520Deshmukh%2520and%2520Apoorv%2520Jindal%2520and%2520James%2520Svensson%2520and%2520Divya%2520Tyam%2520and%2520Heri%2520Zhao%2520and%2520Christine%2520Kaeser-Chen%2520and%2520Scott%2520Baird%2520and%2520Pooya%2520Moradi%2520and%2520Jamie%2520Hall%2520and%2520Qiuchen%2520Guo%2520and%2520Vincent%2520Tsang%2520and%2520Bowen%2520Liang%2520and%2520Fernando%2520Pereira%2520and%2520Suhas%2520Ganesh%2520and%2520Ivan%2520Korotkov%2520and%2520Jakub%2520Adamek%2520and%2520Sridhar%2520Thiagarajan%2520and%2520Vinh%2520Tran%2520and%2520Charles%2520Chen%2520and%2520Chris%2520Tar%2520and%2520Sanil%2520Jain%2520and%2520Ishita%2520Dasgupta%2520and%2520Taylan%2520Bilal%2520and%2520David%2520Reitter%2520and%2520Kai%2520Zhao%2520and%2520Giulia%2520Vezzani%2520and%2520Yasmin%2520Gehman%2520and%2520Pulkit%2520Mehta%2520and%2520Lauren%2520Beltrone%2520and%2520Xerxes%2520Dotiwalla%2520and%2520Sergio%2520Guadarrama%2520and%2520Zaheer%2520Abbas%2520and%2520Stefani%2520Karp%2520and%2520Petko%2520Georgiev%2520and%2520Chun-Sung%2520Ferng%2520and%2520Marc%2520Brockschmidt%2520and%2520Liqian%2520Peng%2520and%2520Christoph%2520Hirnschall%2520and%2520Vikas%2520Verma%2520and%2520Yingying%2520Bi%2520and%2520Ying%2520Xiao%2520and%2520Avigail%2520Dabush%2520and%2520Kelvin%2520Xu%2520and%2520Phil%2520Wallis%2520and%2520Randall%2520Parker%2520and%2520Qifei%2520Wang%2520and%2520Yang%2520Xu%2520and%2520Ilkin%2520Safarli%2520and%2520Dinesh%2520Tewari%2520and%2520Yin%2520Zhang%2520and%2520Seungyeon%2520Kim%2520and%2520Andrea%2520Gesmundo%2520and%2520Mackenzie%2520Thomas%2520and%2520Sergey%2520Levi%2520and%2520Ahmed%2520Chowdhury%2520and%2520Kanishka%2520Rao%2520and%2520Peter%2520Garst%2520and%2520Sam%2520Conway-Rahman%2520and%2520Helen%2520Ran%2520and%2520Kay%2520McKinney%2520and%2520Zhisheng%2520Xiao%2520and%2520Wenhao%2520Yu%2520and%2520Rohan%2520Agrawal%2520and%2520Axel%2520Stjerngren%2520and%2520Catalin%2520Ionescu%2520and%2520Jingjing%2520Chen%2520and%2520Vivek%2520Sharma%2520and%2520Justin%2520Chiu%2520and%2520Fei%2520Liu%2520and%2520Ken%2520Franko%2520and%2520Clayton%2520Sanford%2520and%2520Xingyu%2520Cai%2520and%2520Paul%2520Michel%2520and%2520Sanjay%2520Ganapathy%2520and%2520Jane%2520Labanowski%2520and%2520Zachary%2520Garrett%2520and%2520Ben%2520Vargas%2520and%2520Sean%2520Sun%2520and%2520Bryan%2520Gale%2520and%2520Thomas%2520Buschmann%2520and%2520Guillaume%2520Desjardins%2520and%2520Nimesh%2520Ghelani%2520and%2520Palak%2520Jain%2520and%2520Mudit%2520Verma%2520and%2520Chulayuth%2520Asawaroengchai%2520and%2520Julian%2520Eisenschlos%2520and%2520Jitendra%2520Harlalka%2520and%2520Hideto%2520Kazawa%2520and%2520Don%2520Metzler%2520and%2520Joshua%2520Howland%2520and%2520Ying%2520Jian%2520and%2520Jake%2520Ades%2520and%2520Viral%2520Shah%2520and%2520Tynan%2520Gangwani%2520and%2520Seungji%2520Lee%2520and%2520Roman%2520Ring%2520and%2520Steven%2520M.%2520Hernandez%2520and%2520Dean%2520Reich%2520and%2520Amer%2520Sinha%2520and%2520Ashutosh%2520Sathe%2520and%2520Joe%2520Kovac%2520and%2520Ashleah%2520Gill%2520and%2520Ajay%2520Kannan%2520and%2520Andrea%2520D%2527olimpio%2520and%2520Martin%2520Sevenich%2520and%2520Jay%2520Whang%2520and%2520Been%2520Kim%2520and%2520Khe%2520Chai%2520Sim%2520and%2520Jilin%2520Chen%2520and%2520Jiageng%2520Zhang%2520and%2520Shuba%2520Lall%2520and%2520Yossi%2520Matias%2520and%2520Bill%2520Jia%2520and%2520Abe%2520Friesen%2520and%2520Sara%2520Nasso%2520and%2520Ashish%2520Thapliyal%2520and%2520Bryan%2520Perozzi%2520and%2520Ting%2520Yu%2520and%2520Anna%2520Shekhawat%2520and%2520Safeen%2520Huda%2520and%2520Peter%2520Grabowski%2520and%2520Eric%2520Wang%2520and%2520Ashwin%2520Sreevatsa%2520and%2520Hilal%2520Dib%2520and%2520Mehadi%2520Hassen%2520and%2520Parker%2520Schuh%2520and%2520Vedrana%2520Milutinovic%2520and%2520Chris%2520Welty%2520and%2520Michael%2520Quinn%2520and%2520Ali%2520Shah%2520and%2520Bangju%2520Wang%2520and%2520Gabe%2520Barth-Maron%2520and%2520Justin%2520Frye%2520and%2520Natalie%2520Axelsson%2520and%2520Tao%2520Zhu%2520and%2520Yukun%2520Ma%2520and%2520Irene%2520Giannoumis%2520and%2520Hanie%2520Sedghi%2520and%2520Chang%2520Ye%2520and%2520Yi%2520Luan%2520and%2520Kevin%2520Aydin%2520and%2520Bilva%2520Chandra%2520and%2520Vivek%2520Sampathkumar%2520and%2520Ronny%2520Huang%2520and%2520Victor%2520Lavrenko%2520and%2520Ahmed%2520Eleryan%2520and%2520Zhi%2520Hong%2520and%2520Steven%2520Hansen%2520and%2520Sara%2520Mc%2520Carthy%2520and%2520Bidisha%2520Samanta%2520and%2520Domagoj%2520%25C4%2586evid%2520and%2520Xin%2520Wang%2520and%2520Fangtao%2520Li%2520and%2520Michael%2520Voznesensky%2520and%2520Matt%2520Hoffman%2520and%2520Andreas%2520Terzis%2520and%2520Vikash%2520Sehwag%2520and%2520Gil%2520Fidel%2520and%2520Luheng%2520He%2520and%2520Mu%2520Cai%2520and%2520Yanzhang%2520He%2520and%2520Alex%2520Feng%2520and%2520Martin%2520Nikoltchev%2520and%2520Samrat%2520Phatale%2520and%2520Jason%2520Chase%2520and%2520Rory%2520Lawton%2520and%2520Ming%2520Zhang%2520and%2520Tom%2520Ouyang%2520and%2520Manuel%2520Tragut%2520and%2520Mehdi%2520Hafezi%2520Manshadi%2520and%2520Arjun%2520Narayanan%2520and%2520Jiaming%2520Shen%2520and%2520Xu%2520Gao%2520and%2520Tolga%2520Bolukbasi%2520and%2520Nick%2520Roy%2520and%2520Xin%2520Li%2520and%2520Daniel%2520Golovin%2520and%2520Liviu%2520Panait%2520and%2520Zhen%2520Qin%2520and%2520Guangxing%2520Han%2520and%2520Thomas%2520Anthony%2520and%2520Sneha%2520Kudugunta%2520and%2520Viorica%2520Patraucean%2520and%2520Aniket%2520Ray%2520and%2520Xinyun%2520Chen%2520and%2520Xiaochen%2520Yang%2520and%2520Tanuj%2520Bhatia%2520and%2520Pranav%2520Talluri%2520and%2520Alex%2520Morris%2520and%2520Andrija%2520Ra%25C5%25BEnatovi%25C4%2587%2520and%2520Bethanie%2520Brownfield%2520and%2520James%2520An%2520and%2520Sheng%2520Peng%2520and%2520Patrick%2520Kane%2520and%2520Ce%2520Zheng%2520and%2520Nico%2520Duduta%2520and%2520Joshua%2520Kessinger%2520and%2520James%2520Noraky%2520and%2520Siqi%2520Liu%2520and%2520Keran%2520Rong%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%2520and%2520Keith%2520Rush%2520and%2520Alex%2520Goldin%2520and%2520Fanny%2520Wei%2520and%2520Shiva%2520Mohan%2520Reddy%2520Garlapati%2520and%2520Caroline%2520Pantofaru%2520and%2520Okwan%2520Kwon%2520and%2520Jianmo%2520Ni%2520and%2520Eric%2520Noland%2520and%2520Julia%2520Di%2520Trapani%2520and%2520Fran%25C3%25A7oise%2520Beaufays%2520and%2520Abhijit%2520Guha%2520Roy%2520and%2520Yinlam%2520Chow%2520and%2520Aybuke%2520Turker%2520and%2520Geoffrey%2520Cideron%2520and%2520Lantao%2520Mei%2520and%2520Jon%2520Clark%2520and%2520Qingyun%2520Dou%2520and%2520Matko%2520Bo%25C5%25A1njak%2520and%2520Ralph%2520Leith%2520and%2520Yuqing%2520Du%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520Milad%2520Nasr%2520and%2520Chester%2520Kwak%2520and%2520Suraj%2520Satishkumar%2520Sheth%2520and%2520Alex%2520Kaskasoli%2520and%2520Ankesh%2520Anand%2520and%2520Balaji%2520Lakshminarayanan%2520and%2520Sammy%2520Jerome%2520and%2520David%2520Bieber%2520and%2520Chun-Te%2520Chu%2520and%2520Alexandre%2520Senges%2520and%2520Tianxiao%2520Shen%2520and%2520Mukund%2520Sridhar%2520and%2520Ndaba%2520Ndebele%2520and%2520Benjamin%2520Beyret%2520and%2520Shakir%2520Mohamed%2520and%2520Mia%2520Chen%2520and%2520Markus%2520Freitag%2520and%2520Jiaxian%2520Guo%2520and%2520Luyang%2520Liu%2520and%2520Paul%2520Roit%2520and%2520Heng%2520Chen%2520and%2520Shen%2520Yan%2520and%2520Tom%2520Stone%2520and%2520JD%2520Co-Reyes%2520and%2520Jeremy%2520Cole%2520and%2520Salvatore%2520Scellato%2520and%2520Shekoofeh%2520Azizi%2520and%2520Hadi%2520Hashemi%2520and%2520Alicia%2520Jin%2520and%2520Anand%2520Iyer%2520and%2520Marcella%2520Valentine%2520and%2520Andr%25C3%25A1s%2520Gy%25C3%25B6rgy%2520and%2520Arun%2520Ahuja%2520and%2520Daniel%2520Hernandez%2520Diaz%2520and%2520Chen-Yu%2520Lee%2520and%2520Nathan%2520Clement%2520and%2520Weize%2520Kong%2520and%2520Drew%2520Garmon%2520and%2520Ishaan%2520Watts%2520and%2520Kush%2520Bhatia%2520and%2520Khyatti%2520Gupta%2520and%2520Matt%2520Miecnikowski%2520and%2520Hugo%2520Vallet%2520and%2520Ankur%2520Taly%2520and%2520Edward%2520Loper%2520and%2520Saket%2520Joshi%2520and%2520James%2520Atwood%2520and%2520Jo%2520Chick%2520and%2520Mark%2520Collier%2520and%2520Fotis%2520Iliopoulos%2520and%2520Ryan%2520Trostle%2520and%2520Beliz%2520Gunel%2520and%2520Ramiro%2520Leal-Cavazos%2520and%2520Arnar%2520Mar%2520Hrafnkelsson%2520and%2520Michael%2520Guzman%2520and%2520Xiaoen%2520Ju%2520and%2520Andy%2520Forbes%2520and%2520Jesse%2520Emond%2520and%2520Kushal%2520Chauhan%2520and%2520Ben%2520Caine%2520and%2520Li%2520Xiao%2520and%2520Wenjun%2520Zeng%2520and%2520Alexandre%2520Moufarek%2520and%2520Daniel%2520Murphy%2520and%2520Maya%2520Meng%2520and%2520Nitish%2520Gupta%2520and%2520Felix%2520Riedel%2520and%2520Anil%2520Das%2520and%2520Elijah%2520Lawal%2520and%2520Shashi%2520Narayan%2520and%2520Tiberiu%2520Sosea%2520and%2520James%2520Swirhun%2520and%2520Linda%2520Friso%2520and%2520Behnam%2520Neyshabur%2520and%2520Jing%2520Lu%2520and%2520Sertan%2520Girgin%2520and%2520Michael%2520Wunder%2520and%2520Edouard%2520Yvinec%2520and%2520Aroonalok%2520Pyne%2520and%2520Victor%2520Carbune%2520and%2520Shruti%2520Rijhwani%2520and%2520Yang%2520Guo%2520and%2520Tulsee%2520Doshi%2520and%2520Anton%2520Briukhov%2520and%2520Max%2520Bain%2520and%2520Ayal%2520Hitron%2520and%2520Xuanhui%2520Wang%2520and%2520Ashish%2520Gupta%2520and%2520Ke%2520Chen%2520and%2520Cosmo%2520Du%2520and%2520Weiyang%2520Zhang%2520and%2520Dhruv%2520Shah%2520and%2520Arjun%2520Akula%2520and%2520Max%2520Dylla%2520and%2520Ashyana%2520Kachra%2520and%2520Weicheng%2520Kuo%2520and%2520Tingting%2520Zou%2520and%2520Lily%2520Wang%2520and%2520Luyao%2520Xu%2520and%2520Jifan%2520Zhu%2520and%2520Justin%2520Snyder%2520and%2520Sachit%2520Menon%2520and%2520Orhan%2520Firat%2520and%2520Igor%2520Mordatch%2520and%2520Yuan%2520Yuan%2520and%2520Natalia%2520Ponomareva%2520and%2520Rory%2520Blevins%2520and%2520Lawrence%2520Moore%2520and%2520Weijun%2520Wang%2520and%2520Phil%2520Chen%2520and%2520Martin%2520Scholz%2520and%2520Artur%2520Dwornik%2520and%2520Jason%2520Lin%2520and%2520Sicheng%2520Li%2520and%2520Diego%2520Antognini%2520and%2520Te%2520I%2520and%2520Xiaodan%2520Song%2520and%2520Matt%2520Miller%2520and%2520Uday%2520Kalra%2520and%2520Adam%2520Raveret%2520and%2520Oscar%2520Akerlund%2520and%2520Felix%2520Wu%2520and%2520Andrew%2520Nystrom%2520and%2520Namrata%2520Godbole%2520and%2520Tianqi%2520Liu%2520and%2520Hannah%2520DeBalsi%2520and%2520Jewel%2520Zhao%2520and%2520Buhuang%2520Liu%2520and%2520Avi%2520Caciularu%2520and%2520Lauren%2520Lax%2520and%2520Urvashi%2520Khandelwal%2520and%2520Victoria%2520Langston%2520and%2520Eric%2520Bailey%2520and%2520Silvio%2520Lattanzi%2520and%2520Yufei%2520Wang%2520and%2520Neel%2520Kovelamudi%2520and%2520Sneha%2520Mondal%2520and%2520Guru%2520Guruganesh%2520and%2520Nan%2520Hua%2520and%2520Ofir%2520Roval%2520and%2520Pawe%25C5%2582%2520Weso%25C5%2582owski%2520and%2520Rishikesh%2520Ingale%2520and%2520Jonathan%2520Halcrow%2520and%2520Tim%2520Sohn%2520and%2520Christof%2520Angermueller%2520and%2520Bahram%2520Raad%2520and%2520Eli%2520Stickgold%2520and%2520Eva%2520Lu%2520and%2520Alec%2520Kosik%2520and%2520Jing%2520Xie%2520and%2520Timothy%2520Lillicrap%2520and%2520Austin%2520Huang%2520and%2520Lydia%2520Lihui%2520Zhang%2520and%2520Dominik%2520Paulus%2520and%2520Clement%2520Farabet%2520and%2520Alex%2520Wertheim%2520and%2520Bing%2520Wang%2520and%2520Rishabh%2520Joshi%2520and%2520Chu-ling%2520Ko%2520and%2520Yonghui%2520Wu%2520and%2520Shubham%2520Agrawal%2520and%2520Lily%2520Lin%2520and%2520XiangHai%2520Sheng%2520and%2520Peter%2520Sung%2520and%2520Tyler%2520Breland-King%2520and%2520Christina%2520Butterfield%2520and%2520Swapnil%2520Gawde%2520and%2520Sumeet%2520Singh%2520and%2520Qiao%2520Zhang%2520and%2520Raj%2520Apte%2520and%2520Shilpa%2520Shetty%2520and%2520Adrian%2520Hutter%2520and%2520Tao%2520Li%2520and%2520Elizabeth%2520Salesky%2520and%2520Federico%2520Lebron%2520and%2520Jonni%2520Kanerva%2520and%2520Michela%2520Paganini%2520and%2520Arthur%2520Nguyen%2520and%2520Rohith%2520Vallu%2520and%2520Jan-Thorsten%2520Peter%2520and%2520Sarmishta%2520Velury%2520and%2520David%2520Kao%2520and%2520Jay%2520Hoover%2520and%2520Anna%2520Bortsova%2520and%2520Colton%2520Bishop%2520and%2520Shoshana%2520Jakobovits%2520and%2520Alessandro%2520Agostini%2520and%2520Alekh%2520Agarwal%2520and%2520Chang%2520Liu%2520and%2520Charles%2520Kwong%2520and%2520Sasan%2520Tavakkol%2520and%2520Ioana%2520Bica%2520and%2520Alex%2520Greve%2520and%2520Anirudh%2520GP%2520and%2520Jake%2520Marcus%2520and%2520Le%2520Hou%2520and%2520Tom%2520Duerig%2520and%2520Rivka%2520Moroshko%2520and%2520Dave%2520Lacey%2520and%2520Andy%2520Davis%2520and%2520Julien%2520Amelot%2520and%2520Guohui%2520Wang%2520and%2520Frank%2520Kim%2520and%2520Theofilos%2520Strinopoulos%2520and%2520Hui%2520Wan%2520and%2520Charline%2520Le%2520Lan%2520and%2520Shankar%2520Krishnan%2520and%2520Haotian%2520Tang%2520and%2520Peter%2520Humphreys%2520and%2520Junwen%2520Bai%2520and%2520Idan%2520Heimlich%2520Shtacher%2520and%2520Diego%2520Machado%2520and%2520Chenxi%2520Pang%2520and%2520Ken%2520Burke%2520and%2520Dangyi%2520Liu%2520and%2520Renga%2520Aravamudhan%2520and%2520Yue%2520Song%2520and%2520Ed%2520Hirst%2520and%2520Abhimanyu%2520Singh%2520and%2520Brendan%2520Jou%2520and%2520Liang%2520Bai%2520and%2520Francesco%2520Piccinno%2520and%2520Chuyuan%2520Kelly%2520Fu%2520and%2520Robin%2520Alazard%2520and%2520Barak%2520Meiri%2520and%2520Daniel%2520Winter%2520and%2520Charlie%2520Chen%2520and%2520Mingda%2520Zhang%2520and%2520Jens%2520Heitkaemper%2520and%2520John%2520Lambert%2520and%2520Jinhyuk%2520Lee%2520and%2520Alexander%2520Fr%25C3%25B6mmgen%2520and%2520Sergey%2520Rogulenko%2520and%2520Pranav%2520Nair%2520and%2520Paul%2520Niemczyk%2520and%2520Anton%2520Bulyenov%2520and%2520Bibo%2520Xu%2520and%2520Hadar%2520Shemtov%2520and%2520Morteza%2520Zadimoghaddam%2520and%2520Serge%2520Toropov%2520and%2520Mateo%2520Wirth%2520and%2520Hanjun%2520Dai%2520and%2520Sreenivas%2520Gollapudi%2520and%2520Daniel%2520Zheng%2520and%2520Alex%2520Kurakin%2520and%2520Chansoo%2520Lee%2520and%2520Kalesha%2520Bullard%2520and%2520Nicolas%2520Serrano%2520and%2520Ivana%2520Balazevic%2520and%2520Yang%2520Li%2520and%2520Johan%2520Schalkwyk%2520and%2520Mark%2520Murphy%2520and%2520Mingyang%2520Zhang%2520and%2520Kevin%2520Sequeira%2520and%2520Romina%2520Datta%2520and%2520Nishant%2520Agrawal%2520and%2520Charles%2520Sutton%2520and%2520Nithya%2520Attaluri%2520and%2520Mencher%2520Chiang%2520and%2520Wael%2520Farhan%2520and%2520Gregory%2520Thornton%2520and%2520Kate%2520Lin%2520and%2520Travis%2520Choma%2520and%2520Hung%2520Nguyen%2520and%2520Kingshuk%2520Dasgupta%2520and%2520Dirk%2520Robinson%2520and%2520Iulia%2520Com%25C5%259Fa%2520and%2520Michael%2520Riley%2520and%2520Arjun%2520Pillai%2520and%2520Basil%2520Mustafa%2520and%2520Ben%2520Golan%2520and%2520Amir%2520Zandieh%2520and%2520Jean-Baptiste%2520Lespiau%2520and%2520Billy%2520Porter%2520and%2520David%2520Ross%2520and%2520Sujeevan%2520Rajayogam%2520and%2520Mohit%2520Agarwal%2520and%2520Subhashini%2520Venugopalan%2520and%2520Bobak%2520Shahriari%2520and%2520Qiqi%2520Yan%2520and%2520Hao%2520Xu%2520and%2520Taylor%2520Tobin%2520and%2520Pavel%2520Dubov%2520and%2520Hongzhi%2520Shi%2520and%2520Adri%25C3%25A0%2520Recasens%2520and%2520Anton%2520Kovsharov%2520and%2520Sebastian%2520Borgeaud%2520and%2520Lucio%2520Dery%2520and%2520Shanthal%2520Vasanth%2520and%2520Elena%2520Gribovskaya%2520and%2520Linhai%2520Qiu%2520and%2520Mahdis%2520Mahdieh%2520and%2520Wojtek%2520Skut%2520and%2520Elizabeth%2520Nielsen%2520and%2520CJ%2520Zheng%2520and%2520Adams%2520Yu%2520and%2520Carrie%2520Grimes%2520Bostock%2520and%2520Shaleen%2520Gupta%2520and%2520Aaron%2520Archer%2520and%2520Chris%2520Rawles%2520and%2520Elinor%2520Davies%2520and%2520Alexey%2520Svyatkovskiy%2520and%2520Tomy%2520Tsai%2520and%2520Yoni%2520Halpern%2520and%2520Christian%2520Reisswig%2520and%2520Bartek%2520Wydrowski%2520and%2520Bo%2520Chang%2520and%2520Joan%2520Puigcerver%2520and%2520Mor%2520Hazan%2520Taege%2520and%2520Jian%2520Li%2520and%2520Eva%2520Schnider%2520and%2520Xinjian%2520Li%2520and%2520Dragos%2520Dena%2520and%2520Yunhan%2520Xu%2520and%2520Umesh%2520Telang%2520and%2520Tianze%2520Shi%2520and%2520Heiga%2520Zen%2520and%2520Kyle%2520Kastner%2520and%2520Yeongil%2520Ko%2520and%2520Neesha%2520Subramaniam%2520and%2520Aviral%2520Kumar%2520and%2520Pete%2520Blois%2520and%2520Zhuyun%2520Dai%2520and%2520John%2520Wieting%2520and%2520Yifeng%2520Lu%2520and%2520Yoel%2520Zeldes%2520and%2520Tian%2520Xie%2520and%2520Anja%2520Hauth%2520and%2520Alexandru%2520%25C5%25A2ifrea%2520and%2520Yuqi%2520Li%2520and%2520Sam%2520El-Husseini%2520and%2520Dan%2520Abolafia%2520and%2520Howard%2520Zhou%2520and%2520Wen%2520Ding%2520and%2520Sahra%2520Ghalebikesabi%2520and%2520Carlos%2520Gu%25C3%25ADa%2520and%2520Andrii%2520Maksai%2520and%2520%25C3%2581goston%2520Weisz%2520and%2520Sercan%2520Arik%2520and%2520Nick%2520Sukhanov%2520and%2520Aga%2520%25C5%259Awietlik%2520and%2520Xuhui%2520Jia%2520and%2520Luo%2520Yu%2520and%2520Weiyue%2520Wang%2520and%2520Mark%2520Brand%2520and%2520Dawn%2520Bloxwich%2520and%2520Sean%2520Kirmani%2520and%2520Zhe%2520Chen%2520and%2520Alec%2520Go%2520and%2520Pablo%2520Sprechmann%2520and%2520Nithish%2520Kannen%2520and%2520Alen%2520Carin%2520and%2520Paramjit%2520Sandhu%2520and%2520Isabel%2520Edkins%2520and%2520Leslie%2520Nooteboom%2520and%2520Jai%2520Gupta%2520and%2520Loren%2520Maggiore%2520and%2520Javad%2520Azizi%2520and%2520Yael%2520Pritch%2520and%2520Pengcheng%2520Yin%2520and%2520Mansi%2520Gupta%2520and%2520Danny%2520Tarlow%2520and%2520Duncan%2520Smith%2520and%2520Desi%2520Ivanov%2520and%2520Mohammad%2520Babaeizadeh%2520and%2520Ankita%2520Goel%2520and%2520Satish%2520Kambala%2520and%2520Grace%2520Chu%2520and%2520Matej%2520Kastelic%2520and%2520Michelle%2520Liu%2520and%2520Hagen%2520Soltau%2520and%2520Austin%2520Stone%2520and%2520Shivani%2520Agrawal%2520and%2520Min%2520Kim%2520and%2520Kedar%2520Soparkar%2520and%2520Srinivas%2520Tadepalli%2520and%2520Oskar%2520Bunyan%2520and%2520Rachel%2520Soh%2520and%2520Arvind%2520Kannan%2520and%2520DY%2520Kim%2520and%2520Blake%2520JianHang%2520Chen%2520and%2520Afief%2520Halumi%2520and%2520Sudeshna%2520Roy%2520and%2520Yulong%2520Wang%2520and%2520Olcan%2520Sercinoglu%2520and%2520Gena%2520Gibson%2520and%2520Sijal%2520Bhatnagar%2520and%2520Motoki%2520Sano%2520and%2520Daniel%2520von%2520Dincklage%2520and%2520Qingchun%2520Ren%2520and%2520Blagoj%2520Mitrevski%2520and%2520Mirek%2520Ol%25C5%25A1%25C3%25A1k%2520and%2520Jennifer%2520She%2520and%2520Carl%2520Doersch%2520and%2520%2520Jilei%2520and%2520%2520Wang%2520and%2520Bingyuan%2520Liu%2520and%2520Qijun%2520Tan%2520and%2520Tamar%2520Yakar%2520and%2520Tris%2520Warkentin%2520and%2520Alex%2520Ramirez%2520and%2520Carl%2520Lebsack%2520and%2520Josh%2520Dillon%2520and%2520Rajiv%2520Mathews%2520and%2520Tom%2520Cobley%2520and%2520Zelin%2520Wu%2520and%2520Zhuoyuan%2520Chen%2520and%2520Jon%2520Simon%2520and%2520Swaroop%2520Nath%2520and%2520Tara%2520Sainath%2520and%2520Alexei%2520Bendebury%2520and%2520Ryan%2520Julian%2520and%2520Bharath%2520Mankalale%2520and%2520Daria%2520%25C4%2586urko%2520and%2520Paulo%2520Zacchello%2520and%2520Adam%2520R.%2520Brown%2520and%2520Kiranbir%2520Sodhia%2520and%2520Heidi%2520Howard%2520and%2520Sergi%2520Caelles%2520and%2520Abhinav%2520Gupta%2520and%2520Gareth%2520Evans%2520and%2520Anna%2520Bulanova%2520and%2520Lesley%2520Katzen%2520and%2520Roman%2520Goldenberg%2520and%2520Anton%2520Tsitsulin%2520and%2520Joe%2520Stanton%2520and%2520Benoit%2520Schillings%2520and%2520Vitaly%2520Kovalev%2520and%2520Corey%2520Fry%2520and%2520Rushin%2520Shah%2520and%2520Kuo%2520Lin%2520and%2520Shyam%2520Upadhyay%2520and%2520Cheng%2520Li%2520and%2520Soroush%2520Radpour%2520and%2520Marcello%2520Maggioni%2520and%2520Jing%2520Xiong%2520and%2520Lukas%2520Haas%2520and%2520Jenny%2520Brennan%2520and%2520Aishwarya%2520Kamath%2520and%2520Nikolay%2520Savinov%2520and%2520Arsha%2520Nagrani%2520and%2520Trevor%2520Yacovone%2520and%2520Ryan%2520Kappedal%2520and%2520Kostas%2520Andriopoulos%2520and%2520Li%2520Lao%2520and%2520YaGuang%2520Li%2520and%2520Grigory%2520Rozhdestvenskiy%2520and%2520Kazuma%2520Hashimoto%2520and%2520Andrew%2520Audibert%2520and%2520Sophia%2520Austin%2520and%2520Daniel%2520Rodriguez%2520and%2520Anian%2520Ruoss%2520and%2520Garrett%2520Honke%2520and%2520Deep%2520Karkhanis%2520and%2520Xi%2520Xiong%2520and%2520Qing%2520Wei%2520and%2520James%2520Huang%2520and%2520Zhaoqi%2520Leng%2520and%2520Vittal%2520Premachandran%2520and%2520Stan%2520Bileschi%2520and%2520Georgios%2520Evangelopoulos%2520and%2520Thomas%2520Mensink%2520and%2520Jay%2520Pavagadhi%2520and%2520Denis%2520Teplyashin%2520and%2520Paul%2520Chang%2520and%2520Linting%2520Xue%2520and%2520Garrett%2520Tanzer%2520and%2520Sally%2520Goldman%2520and%2520Kaushal%2520Patel%2520and%2520Shixin%2520Li%2520and%2520Jeremy%2520Wiesner%2520and%2520Ivy%2520Zheng%2520and%2520Ian%2520Stewart-Binks%2520and%2520Jie%2520Han%2520and%2520Zhi%2520Li%2520and%2520Liangchen%2520Luo%2520and%2520Karel%2520Lenc%2520and%2520Mario%2520Lu%25C4%258Di%25C4%2587%2520and%2520Fuzhao%2520Xue%2520and%2520Ryan%2520Mullins%2520and%2520Alexey%2520Guseynov%2520and%2520Chung-Ching%2520Chang%2520and%2520Isaac%2520Galatzer-Levy%2520and%2520Adam%2520Zhang%2520and%2520Garrett%2520Bingham%2520and%2520Grace%2520Hu%2520and%2520Ale%2520Hartman%2520and%2520Yue%2520Ma%2520and%2520Jordan%2520Griffith%2520and%2520Alex%2520Irpan%2520and%2520Carey%2520Radebaugh%2520and%2520Summer%2520Yue%2520and%2520Lijie%2520Fan%2520and%2520Victor%2520Ungureanu%2520and%2520Christina%2520Sorokin%2520and%2520Hannah%2520Teufel%2520and%2520Peiran%2520Li%2520and%2520Rohan%2520Anil%2520and%2520Dimitris%2520Paparas%2520and%2520Todd%2520Wang%2520and%2520Chu-Cheng%2520Lin%2520and%2520Hui%2520Peng%2520and%2520Megan%2520Shum%2520and%2520Goran%2520Petrovic%2520and%2520Demetra%2520Brady%2520and%2520Richard%2520Nguyen%2520and%2520Klaus%2520Macherey%2520and%2520Zhihao%2520Li%2520and%2520Harman%2520Singh%2520and%2520Madhavi%2520Yenugula%2520and%2520Mariko%2520Iinuma%2520and%2520Xinyi%2520Chen%2520and%2520Kavya%2520Kopparapu%2520and%2520Alexey%2520Stern%2520and%2520Shachi%2520Dave%2520and%2520Chandu%2520Thekkath%2520and%2520Florence%2520Perot%2520and%2520Anurag%2520Kumar%2520and%2520Fangda%2520Li%2520and%2520Yang%2520Xiao%2520and%2520Matthew%2520Bilotti%2520and%2520Mohammad%2520Hossein%2520Bateni%2520and%2520Isaac%2520Noble%2520and%2520Lisa%2520Lee%2520and%2520Amelio%2520V%25C3%25A1zquez-Reina%2520and%2520Julian%2520Salazar%2520and%2520Xiaomeng%2520Yang%2520and%2520Boyu%2520Wang%2520and%2520Ela%2520Gruzewska%2520and%2520Anand%2520Rao%2520and%2520Sindhu%2520Raghuram%2520and%2520Zheng%2520Xu%2520and%2520Eyal%2520Ben-David%2520and%2520Jieru%2520Mei%2520and%2520Sid%2520Dalmia%2520and%2520Zhaoyi%2520Zhang%2520and%2520Yuchen%2520Liu%2520and%2520Gagan%2520Bansal%2520and%2520Helena%2520Pankov%2520and%2520Steven%2520Schwarcz%2520and%2520Andrea%2520Burns%2520and%2520Christine%2520Chan%2520and%2520Sumit%2520Sanghai%2520and%2520Ricky%2520Liang%2520and%2520Ethan%2520Liang%2520and%2520Antoine%2520He%2520and%2520Amy%2520Stuart%2520and%2520Arun%2520Narayanan%2520and%2520Yukun%2520Zhu%2520and%2520Christian%2520Frank%2520and%2520Bahar%2520Fatemi%2520and%2520Amit%2520Sabne%2520and%2520Oran%2520Lang%2520and%2520Indro%2520Bhattacharya%2520and%2520Shane%2520Settle%2520and%2520Maria%2520Wang%2520and%2520Brendan%2520McMahan%2520and%2520Andrea%2520Tacchetti%2520and%2520Livio%2520Baldini%2520Soares%2520and%2520Majid%2520Hadian%2520and%2520Serkan%2520Cabi%2520and%2520Timothy%2520Chung%2520and%2520Nikita%2520Putikhin%2520and%2520Gang%2520Li%2520and%2520Jeremy%2520Chen%2520and%2520Austin%2520Tarango%2520and%2520Henryk%2520Michalewski%2520and%2520Mehran%2520Kazemi%2520and%2520Hussain%2520Masoom%2520and%2520Hila%2520Sheftel%2520and%2520Rakesh%2520Shivanna%2520and%2520Archita%2520Vadali%2520and%2520Ramona%2520Comanescu%2520and%2520Doug%2520Reid%2520and%2520Joss%2520Moore%2520and%2520Arvind%2520Neelakantan%2520and%2520Micha%25C3%25ABl%2520Sander%2520and%2520Jonathan%2520Herzig%2520and%2520Aviv%2520Rosenberg%2520and%2520Mostafa%2520Dehghani%2520and%2520JD%2520Choi%2520and%2520Michael%2520Fink%2520and%2520Reid%2520Hayes%2520and%2520Eric%2520Ge%2520and%2520Shitao%2520Weng%2520and%2520Chia-Hua%2520Ho%2520and%2520John%2520Karro%2520and%2520Kalpesh%2520Krishna%2520and%2520Lam%2520Nguyen%2520Thiet%2520and%2520Amy%2520Skerry-Ryan%2520and%2520Daniel%2520Eppens%2520and%2520Marco%2520Andreetto%2520and%2520Navin%2520Sarma%2520and%2520Silvano%2520Bonacina%2520and%2520Burcu%2520Karagol%2520Ayan%2520and%2520Megha%2520Nawhal%2520and%2520Zhihao%2520Shan%2520and%2520Mike%2520Dusenberry%2520and%2520Shantanu%2520Thakoor%2520and%2520Sagar%2520Gubbi%2520and%2520Duc%2520Dung%2520Nguyen%2520and%2520Reut%2520Tsarfaty%2520and%2520Samuel%2520Albanie%2520and%2520Jovana%2520Mitrovi%25C4%2587%2520and%2520Meet%2520Gandhi%2520and%2520Bo-Juen%2520Chen%2520and%2520Alessandro%2520Epasto%2520and%2520Georgi%2520Stephanov%2520and%2520Ye%2520Jin%2520and%2520Samuel%2520Gehman%2520and%2520Aida%2520Amini%2520and%2520Jack%2520Weber%2520and%2520Feryal%2520Behbahani%2520and%2520Shawn%2520Xu%2520and%2520Miltos%2520Allamanis%2520and%2520Xi%2520Chen%2520and%2520Myle%2520Ott%2520and%2520Claire%2520Sha%2520and%2520Michal%2520Jastrzebski%2520and%2520Hang%2520Qi%2520and%2520David%2520Greene%2520and%2520Xinyi%2520Wu%2520and%2520Abodunrinwa%2520Toki%2520and%2520Daniel%2520Vlasic%2520and%2520Jane%2520Shapiro%2520and%2520Ragha%2520Kotikalapudi%2520and%2520Zhe%2520Shen%2520and%2520Takaaki%2520Saeki%2520and%2520Sirui%2520Xie%2520and%2520Albin%2520Cassirer%2520and%2520Shikhar%2520Bharadwaj%2520and%2520Tatsuya%2520Kiyono%2520and%2520Srinadh%2520Bhojanapalli%2520and%2520Elan%2520Rosenfeld%2520and%2520Sam%2520Ritter%2520and%2520Jieming%2520Mao%2520and%2520Jo%25C3%25A3o%2520Gabriel%2520Oliveira%2520and%2520Zoltan%2520Egyed%2520and%2520Bernd%2520Bandemer%2520and%2520Emilio%2520Parisotto%2520and%2520Keisuke%2520Kinoshita%2520and%2520Juliette%2520Pluto%2520and%2520Petros%2520Maniatis%2520and%2520Steve%2520Li%2520and%2520Yaohui%2520Guo%2520and%2520Golnaz%2520Ghiasi%2520and%2520Jean%2520Tarbouriech%2520and%2520Srimon%2520Chatterjee%2520and%2520Julie%2520Jin%2520and%2520%2520Katrina%2520and%2520%2520Xu%2520and%2520Jennimaria%2520Palomaki%2520and%2520S%25C3%25A9b%2520Arnold%2520and%2520Madhavi%2520Sewak%2520and%2520Federico%2520Piccinini%2520and%2520Mohit%2520Sharma%2520and%2520Ben%2520Albrecht%2520and%2520Sean%2520Purser-haskell%2520and%2520Ashwin%2520Vaswani%2520and%2520Chongyan%2520Chen%2520and%2520Matheus%2520Wisniewski%2520and%2520Qin%2520Cao%2520and%2520John%2520Aslanides%2520and%2520Nguyet%2520Minh%2520Phu%2520and%2520Maximilian%2520Sieb%2520and%2520Lauren%2520Agubuzu%2520and%2520Anne%2520Zheng%2520and%2520Daniel%2520Sohn%2520and%2520Marco%2520Selvi%2520and%2520Anders%2520Andreassen%2520and%2520Krishan%2520Subudhi%2520and%2520Prem%2520Eruvbetine%2520and%2520Oliver%2520Woodman%2520and%2520Tomas%2520Mery%2520and%2520Sebastian%2520Krause%2520and%2520Xiaoqi%2520Ren%2520and%2520Xiao%2520Ma%2520and%2520Jincheng%2520Luo%2520and%2520Dawn%2520Chen%2520and%2520Wei%2520Fan%2520and%2520Henry%2520Griffiths%2520and%2520Christian%2520Schuler%2520and%2520Alice%2520Li%2520and%2520Shujian%2520Zhang%2520and%2520Jean-Michel%2520Sarr%2520and%2520Shixin%2520Luo%2520and%2520Riccardo%2520Patana%2520and%2520Matthew%2520Watson%2520and%2520Dani%2520Naboulsi%2520and%2520Michael%2520Collins%2520and%2520Sailesh%2520Sidhwani%2520and%2520Emiel%2520Hoogeboom%2520and%2520Sharon%2520Silver%2520and%2520Emily%2520Caveness%2520and%2520Xiaokai%2520Zhao%2520and%2520Mikel%2520Rodriguez%2520and%2520Maxine%2520Deines%2520and%2520Libin%2520Bai%2520and%2520Patrick%2520Griffin%2520and%2520Marco%2520Tagliasacchi%2520and%2520Emily%2520Xue%2520and%2520Spandana%2520Raj%2520Babbula%2520and%2520Bo%2520Pang%2520and%2520Nan%2520Ding%2520and%2520Gloria%2520Shen%2520and%2520Elijah%2520Peake%2520and%2520Remi%2520Crocker%2520and%2520Shubha%2520Srinivas%2520Raghvendra%2520and%2520Danny%2520Swisher%2520and%2520Woohyun%2520Han%2520and%2520Richa%2520Singh%2520and%2520Ling%2520Wu%2520and%2520Vladimir%2520Pchelin%2520and%2520Tsendsuren%2520Munkhdalai%2520and%2520Dana%2520Alon%2520and%2520Geoff%2520Bacon%2520and%2520Efren%2520Robles%2520and%2520Jannis%2520Bulian%2520and%2520Melvin%2520Johnson%2520and%2520George%2520Powell%2520and%2520Felipe%2520Tiengo%2520Ferreira%2520and%2520Yaoyiran%2520Li%2520and%2520Frederik%2520Benzing%2520and%2520Mihajlo%2520Velimirovi%25C4%2587%2520and%2520Hubert%2520Soyer%2520and%2520William%2520Kong%2520and%2520%2520Tony%2520and%2520%2520Nguy%25C3%25AAn%2520and%2520Zhen%2520Yang%2520and%2520Jeremiah%2520Liu%2520and%2520Joost%2520van%2520Amersfoort%2520and%2520Daniel%2520Gillick%2520and%2520Baochen%2520Sun%2520and%2520Nathalie%2520Rauschmayr%2520and%2520Katie%2520Zhang%2520and%2520Serena%2520Zhan%2520and%2520Tao%2520Zhou%2520and%2520Alexey%2520Frolov%2520and%2520Chengrun%2520Yang%2520and%2520Denis%2520Vnukov%2520and%2520Louis%2520Rouillard%2520and%2520Hongji%2520Li%2520and%2520Amol%2520Mandhane%2520and%2520Nova%2520Fallen%2520and%2520Rajesh%2520Venkataraman%2520and%2520Clara%2520Huiyi%2520Hu%2520and%2520Jennifer%2520Brennan%2520and%2520Jenny%2520Lee%2520and%2520Jerry%2520Chang%2520and%2520Martin%2520Sundermeyer%2520and%2520Zhufeng%2520Pan%2520and%2520Rosemary%2520Ke%2520and%2520Simon%2520Tong%2520and%2520Alex%2520Fabrikant%2520and%2520William%2520Bono%2520and%2520Jindong%2520Gu%2520and%2520Ryan%2520Foley%2520and%2520Yiran%2520Mao%2520and%2520Manolis%2520Delakis%2520and%2520Dhruva%2520Bhaswar%2520and%2520Roy%2520Frostig%2520and%2520Nick%2520Li%2520and%2520Avital%2520Zipori%2520and%2520Cath%2520Hope%2520and%2520Olga%2520Kozlova%2520and%2520Swaroop%2520Mishra%2520and%2520Josip%2520Djolonga%2520and%2520Craig%2520Schiff%2520and%2520Majd%2520Al%2520Merey%2520and%2520Eleftheria%2520Briakou%2520and%2520Peter%2520Morgan%2520and%2520Andy%2520Wan%2520and%2520Avinatan%2520Hassidim%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Kuntal%2520Sengupta%2520and%2520Mary%2520Jasarevic%2520and%2520Praveen%2520Kallakuri%2520and%2520Paige%2520Kunkle%2520and%2520Hannah%2520Brennan%2520and%2520Tom%2520Lieber%2520and%2520Hassan%2520Mansoor%2520and%2520Julian%2520Walker%2520and%2520Bing%2520Zhang%2520and%2520Annie%2520Xie%2520and%2520Goran%2520%25C5%25BDu%25C5%25BEi%25C4%2587%2520and%2520Adaeze%2520Chukwuka%2520and%2520Alex%2520Druinsky%2520and%2520Donghyun%2520Cho%2520and%2520Rui%2520Yao%2520and%2520Ferjad%2520Naeem%2520and%2520Shiraz%2520Butt%2520and%2520Eunyoung%2520Kim%2520and%2520Zhipeng%2520Jia%2520and%2520Mandy%2520Jordan%2520and%2520Adam%2520Lelkes%2520and%2520Mark%2520Kurzeja%2520and%2520Sophie%2520Wang%2520and%2520James%2520Zhao%2520and%2520Andrew%2520Over%2520and%2520Abhishek%2520Chakladar%2520and%2520Marcel%2520Prasetya%2520and%2520Neha%2520Jha%2520and%2520Sriram%2520Ganapathy%2520and%2520Yale%2520Cong%2520and%2520Prakash%2520Shroff%2520and%2520Carl%2520Saroufim%2520and%2520Sobhan%2520Miryoosefi%2520and%2520Mohamed%2520Hammad%2520and%2520Tajwar%2520Nasir%2520and%2520Weijuan%2520Xi%2520and%2520Yang%2520Gao%2520and%2520Young%2520Maeng%2520and%2520Ben%2520Hora%2520and%2520Chin-Yi%2520Cheng%2520and%2520Parisa%2520Haghani%2520and%2520Yoad%2520Lewenberg%2520and%2520Caden%2520Lu%2520and%2520Martin%2520Matysiak%2520and%2520Naina%2520Raisinghani%2520and%2520Huiyu%2520Wang%2520and%2520Lexi%2520Baugher%2520and%2520Rahul%2520Sukthankar%2520and%2520Minh%2520Giang%2520and%2520John%2520Schultz%2520and%2520Noah%2520Fiedel%2520and%2520Minmin%2520Chen%2520and%2520Cheng-Chun%2520Lee%2520and%2520Tapomay%2520Dey%2520and%2520Hao%2520Zheng%2520and%2520Shachi%2520Paul%2520and%2520Celine%2520Smith%2520and%2520Andy%2520Ly%2520and%2520Yicheng%2520Wang%2520and%2520Rishabh%2520Bansal%2520and%2520Bartek%2520Perz%2520and%2520Susanna%2520Ricco%2520and%2520Stasha%2520Blank%2520and%2520Vaishakh%2520Keshava%2520and%2520Deepak%2520Sharma%2520and%2520Marvin%2520Chow%2520and%2520Kunal%2520Lad%2520and%2520Komal%2520Jalan%2520and%2520Simon%2520Osindero%2520and%2520Craig%2520Swanson%2520and%2520Jacob%2520Scott%2520and%2520Anastasija%2520Ili%25C4%2587%2520and%2520Xiaowei%2520Li%2520and%2520Siddhartha%2520Reddy%2520Jonnalagadda%2520and%2520Afzal%2520Shama%2520Soudagar%2520and%2520Yan%2520Xiong%2520and%2520Bat-Orgil%2520Batsaikhan%2520and%2520Daniel%2520Jarrett%2520and%2520Naveen%2520Kumar%2520and%2520Maulik%2520Shah%2520and%2520Matt%2520Lawlor%2520and%2520Austin%2520Waters%2520and%2520Mark%2520Graham%2520and%2520Rhys%2520May%2520and%2520Sabela%2520Ramos%2520and%2520Sandra%2520Lefdal%2520and%2520Zeynep%2520Cankara%2520and%2520Nacho%2520Cano%2520and%2520Brendan%2520O%2527Donoghue%2520and%2520Jed%2520Borovik%2520and%2520Frederick%2520Liu%2520and%2520Jordan%2520Grimstad%2520and%2520Mahmoud%2520Alnahlawi%2520and%2520Katerina%2520Tsihlas%2520and%2520Tom%2520Hudson%2520and%2520Nikolai%2520Grigorev%2520and%2520Yiling%2520Jia%2520and%2520Terry%2520Huang%2520and%2520Tobenna%2520Peter%2520Igwe%2520and%2520Sergei%2520Lebedev%2520and%2520Xiaodan%2520Tang%2520and%2520Igor%2520Krivokon%2520and%2520Frankie%2520Garcia%2520and%2520Melissa%2520Tan%2520and%2520Eric%2520Jia%2520and%2520Peter%2520Stys%2520and%2520Shikhar%2520Vashishth%2520and%2520Yu%2520Liang%2520and%2520Balaji%2520Venkatraman%2520and%2520Chenjie%2520Gu%2520and%2520Anastasios%2520Kementsietsidis%2520and%2520Chen%2520Zhu%2520and%2520Junehyuk%2520Jung%2520and%2520Yunfei%2520Bai%2520and%2520Mohammad%2520Javad%2520Hosseini%2520and%2520Faruk%2520Ahmed%2520and%2520Aditya%2520Gupta%2520and%2520Xin%2520Yuan%2520and%2520Shereen%2520Ashraf%2520and%2520Shitij%2520Nigam%2520and%2520Gautam%2520Vasudevan%2520and%2520Pranjal%2520Awasthi%2520and%2520Adi%2520Mayrav%2520Gilady%2520and%2520Zelda%2520Mariet%2520and%2520Ramy%2520Eskander%2520and%2520Haiguang%2520Li%2520and%2520Hexiang%2520Hu%2520and%2520Guillermo%2520Garrido%2520and%2520Philippe%2520Schlattner%2520and%2520George%2520Zhang%2520and%2520Rohun%2520Saxena%2520and%2520Petar%2520Devi%25C4%2587%2520and%2520Kritika%2520Muralidharan%2520and%2520Ashwin%2520Murthy%2520and%2520Yiqian%2520Zhou%2520and%2520Min%2520Choi%2520and%2520Arissa%2520Wongpanich%2520and%2520Zhengdong%2520Wang%2520and%2520Premal%2520Shah%2520and%2520Yuntao%2520Xu%2520and%2520Yiling%2520Huang%2520and%2520Stephen%2520Spencer%2520and%2520Alice%2520Chen%2520and%2520James%2520Cohan%2520and%2520Junjie%2520Wang%2520and%2520Jonathan%2520Tompson%2520and%2520Junru%2520Wu%2520and%2520Ruba%2520Haroun%2520and%2520Haiqiong%2520Li%2520and%2520Blanca%2520Huergo%2520and%2520Fan%2520Yang%2520and%2520Tongxin%2520Yin%2520and%2520James%2520Wendt%2520and%2520Michael%2520Bendersky%2520and%2520Rahma%2520Chaabouni%2520and%2520Javier%2520Snaider%2520and%2520Johan%2520Ferret%2520and%2520Abhishek%2520Jindal%2520and%2520Tara%2520Thompson%2520and%2520Andrew%2520Xue%2520and%2520Will%2520Bishop%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Archit%2520Sharma%2520and%2520Yunhsuan%2520Sung%2520and%2520Prabakar%2520Radhakrishnan%2520and%2520Mo%2520Shomrat%2520and%2520Reeve%2520Ingle%2520and%2520Roopali%2520Vij%2520and%2520Justin%2520Gilmer%2520and%2520Mihai%2520Dorin%2520Istin%2520and%2520Sam%2520Sobell%2520and%2520Yang%2520Lu%2520and%2520Emily%2520Nottage%2520and%2520Dorsa%2520Sadigh%2520and%2520Jeremiah%2520Willcock%2520and%2520Tingnan%2520Zhang%2520and%2520Steve%2520Xu%2520and%2520Sasha%2520Brown%2520and%2520Katherine%2520Lee%2520and%2520Gary%2520Wang%2520and%2520Yun%2520Zhu%2520and%2520Yi%2520Tay%2520and%2520Cheolmin%2520Kim%2520and%2520Audrey%2520Gutierrez%2520and%2520Abhanshu%2520Sharma%2520and%2520Yongqin%2520Xian%2520and%2520Sungyong%2520Seo%2520and%2520Claire%2520Cui%2520and%2520Elena%2520Pochernina%2520and%2520Cip%2520Baetu%2520and%2520Krzysztof%2520Jastrz%25C4%2599bski%2520and%2520Mimi%2520Ly%2520and%2520Mohamed%2520Elhawaty%2520and%2520Dan%2520Suh%2520and%2520Eren%2520Sezener%2520and%2520Pidong%2520Wang%2520and%2520Nancy%2520Yuen%2520and%2520George%2520Tucker%2520and%2520Jiahao%2520Cai%2520and%2520Zuguang%2520Yang%2520and%2520Cindy%2520Wang%2520and%2520Alex%2520Muzio%2520and%2520Hai%2520Qian%2520and%2520Jae%2520Yoo%2520and%2520Derek%2520Lockhart%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Mandy%2520Guo%2520and%2520Malika%2520Mehrotra%2520and%2520Artur%2520Mendon%25C3%25A7a%2520and%2520Sanket%2520Vaibhav%2520Mehta%2520and%2520Sherry%2520Ben%2520and%2520Chetan%2520Tekur%2520and%2520Jiaqi%2520Mu%2520and%2520Muye%2520Zhu%2520and%2520Victoria%2520Krakovna%2520and%2520Hongrae%2520Lee%2520and%2520AJ%2520Maschinot%2520and%2520S%25C3%25A9bastien%2520Cevey%2520and%2520HyunJeong%2520Choe%2520and%2520Aijun%2520Bai%2520and%2520Hansa%2520Srinivasan%2520and%2520Derek%2520Gasaway%2520and%2520Nick%2520Young%2520and%2520Patrick%2520Siegler%2520and%2520Dan%2520Holtmann-Rice%2520and%2520Vihari%2520Piratla%2520and%2520Kate%2520Baumli%2520and%2520Roey%2520Yogev%2520and%2520Alex%2520Hofer%2520and%2520Hado%2520van%2520Hasselt%2520and%2520Svetlana%2520Grant%2520and%2520Yuri%2520Chervonyi%2520and%2520David%2520Silver%2520and%2520Andrew%2520Hogue%2520and%2520Ayushi%2520Agarwal%2520and%2520Kathie%2520Wang%2520and%2520Preeti%2520Singh%2520and%2520Four%2520Flynn%2520and%2520Josh%2520Lipschultz%2520and%2520Robert%2520David%2520and%2520Lizzetth%2520Bellot%2520and%2520Yao-Yuan%2520Yang%2520and%2520Long%2520Le%2520and%2520Filippo%2520Graziano%2520and%2520Kate%2520Olszewska%2520and%2520Kevin%2520Hui%2520and%2520Akanksha%2520Maurya%2520and%2520Nikos%2520Parotsidis%2520and%2520Weijie%2520Chen%2520and%2520Tayo%2520Oguntebi%2520and%2520Joe%2520Kelley%2520and%2520Anirudh%2520Baddepudi%2520and%2520Johannes%2520Mauerer%2520and%2520Gregory%2520Shaw%2520and%2520Alex%2520Siegman%2520and%2520Lin%2520Yang%2520and%2520Shravya%2520Shetty%2520and%2520Subhrajit%2520Roy%2520and%2520Yunting%2520Song%2520and%2520Wojciech%2520Stokowiec%2520and%2520Ryan%2520Burnell%2520and%2520Omkar%2520Savant%2520and%2520Robert%2520Busa-Fekete%2520and%2520Jin%2520Miao%2520and%2520Samrat%2520Ghosh%2520and%2520Liam%2520MacDermed%2520and%2520Phillip%2520Lippe%2520and%2520Mikhail%2520Dektiarev%2520and%2520Zach%2520Behrman%2520and%2520Fabian%2520Mentzer%2520and%2520Kelvin%2520Nguyen%2520and%2520Meng%2520Wei%2520and%2520Siddharth%2520Verma%2520and%2520Chris%2520Knutsen%2520and%2520Sudeep%2520Dasari%2520and%2520Zhipeng%2520Yan%2520and%2520Petr%2520Mitrichev%2520and%2520Xingyu%2520Wang%2520and%2520Virat%2520Shejwalkar%2520and%2520Jacob%2520Austin%2520and%2520Srinivas%2520Sunkara%2520and%2520Navneet%2520Potti%2520and%2520Yan%2520Virin%2520and%2520Christian%2520Wright%2520and%2520Ga%25C3%25ABl%2520Liu%2520and%2520Oriana%2520Riva%2520and%2520Etienne%2520Pot%2520and%2520Greg%2520Kochanski%2520and%2520Quoc%2520Le%2520and%2520Gargi%2520Balasubramaniam%2520and%2520Arka%2520Dhar%2520and%2520Yuguo%2520Liao%2520and%2520Adam%2520Bloniarz%2520and%2520Divyansh%2520Shukla%2520and%2520Elizabeth%2520Cole%2520and%2520Jong%2520Lee%2520and%2520Sheng%2520Zhang%2520and%2520Sushant%2520Kafle%2520and%2520Siddharth%2520Vashishtha%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Grace%2520Chen%2520and%2520Raphael%2520Hoffmann%2520and%2520Pranesh%2520Srinivasan%2520and%2520Agustin%2520Dal%2520Lago%2520and%2520Yoav%2520Ben%2520Shalom%2520and%2520Zi%2520Wang%2520and%2520Michael%2520Elabd%2520and%2520Anuj%2520Sharma%2520and%2520Junhyuk%2520Oh%2520and%2520Suraj%2520Kothawade%2520and%2520Maigo%2520Le%2520and%2520Marianne%2520Monteiro%2520and%2520Shentao%2520Yang%2520and%2520Kaiz%2520Alarakyia%2520and%2520Robert%2520Geirhos%2520and%2520Diana%2520Mincu%2520and%2520H%25C3%25A5vard%2520Garnes%2520and%2520Hayato%2520Kobayashi%2520and%2520Soroosh%2520Mariooryad%2520and%2520Kacper%2520Krasowiak%2520and%2520%2520Zhixin%2520and%2520%2520Lai%2520and%2520Shibl%2520Mourad%2520and%2520Mingqiu%2520Wang%2520and%2520Fan%2520Bu%2520and%2520Ophir%2520Aharoni%2520and%2520Guanjie%2520Chen%2520and%2520Abhimanyu%2520Goyal%2520and%2520Vadim%2520Zubov%2520and%2520Ankur%2520Bapna%2520and%2520Elahe%2520Dabir%2520and%2520Nisarg%2520Kothari%2520and%2520Kay%2520Lamerigts%2520and%2520Nicola%2520De%2520Cao%2520and%2520Jeremy%2520Shar%2520and%2520Christopher%2520Yew%2520and%2520Nitish%2520Kulkarni%2520and%2520Dre%2520Mahaarachchi%2520and%2520Mandar%2520Joshi%2520and%2520Zhenhai%2520Zhu%2520and%2520Jared%2520Lichtarge%2520and%2520Yichao%2520Zhou%2520and%2520Hannah%2520Muckenhirn%2520and%2520Vittorio%2520Selo%2520and%2520Oriol%2520Vinyals%2520and%2520Peter%2520Chen%2520and%2520Anthony%2520Brohan%2520and%2520Vaibhav%2520Mehta%2520and%2520Sarah%2520Cogan%2520and%2520Ruth%2520Wang%2520and%2520Ty%2520Geri%2520and%2520Wei-Jen%2520Ko%2520and%2520Wei%2520Chen%2520and%2520Fabio%2520Viola%2520and%2520Keshav%2520Shivam%2520and%2520Lisa%2520Wang%2520and%2520Madeleine%2520Clare%2520Elish%2520and%2520Raluca%2520Ada%2520Popa%2520and%2520S%25C3%25A9bastien%2520Pereira%2520and%2520Jianqiao%2520Liu%2520and%2520Raphael%2520Koster%2520and%2520Donnie%2520Kim%2520and%2520Gufeng%2520Zhang%2520and%2520Sayna%2520Ebrahimi%2520and%2520Partha%2520Talukdar%2520and%2520Yanyan%2520Zheng%2520and%2520Petra%2520Poklukar%2520and%2520Ales%2520Mikhalap%2520and%2520Dale%2520Johnson%2520and%2520Anitha%2520Vijayakumar%2520and%2520Mark%2520Omernick%2520and%2520Matt%2520Dibb%2520and%2520Ayush%2520Dubey%2520and%2520Qiong%2520Hu%2520and%2520Apurv%2520Suman%2520and%2520Vaibhav%2520Aggarwal%2520and%2520Ilya%2520Kornakov%2520and%2520Fei%2520Xia%2520and%2520Wing%2520Lowe%2520and%2520Alexey%2520Kolganov%2520and%2520Ted%2520Xiao%2520and%2520Vitaly%2520Nikolaev%2520and%2520Steven%2520Hemingray%2520and%2520Bonnie%2520Li%2520and%2520Joana%2520Iljazi%2520and%2520Miko%25C5%2582aj%2520Rybi%25C5%2584ski%2520and%2520Ballie%2520Sandhu%2520and%2520Peggy%2520Lu%2520and%2520Thang%2520Luong%2520and%2520Rodolphe%2520Jenatton%2520and%2520Vineetha%2520Govindaraj%2520and%2520%2520Hui%2520and%2520%2520Li%2520and%2520Gabriel%2520Dulac-Arnold%2520and%2520Wonpyo%2520Park%2520and%2520Henry%2520Wang%2520and%2520Abhinit%2520Modi%2520and%2520Jean%2520Pouget-Abadie%2520and%2520Kristina%2520Greller%2520and%2520Rahul%2520Gupta%2520and%2520Robert%2520Berry%2520and%2520Prajit%2520Ramachandran%2520and%2520Jinyu%2520Xie%2520and%2520Liam%2520McCafferty%2520and%2520Jianling%2520Wang%2520and%2520Kilol%2520Gupta%2520and%2520Hyeontaek%2520Lim%2520and%2520Bla%25C5%25BE%2520Bratani%25C4%258D%2520and%2520Andy%2520Brock%2520and%2520Ilia%2520Akolzin%2520and%2520Jim%2520Sproch%2520and%2520Dan%2520Karliner%2520and%2520Duhyeon%2520Kim%2520and%2520Adrian%2520Goedeckemeyer%2520and%2520Noam%2520Shazeer%2520and%2520Cordelia%2520Schmid%2520and%2520Daniele%2520Calandriello%2520and%2520Parul%2520Bhatia%2520and%2520Krzysztof%2520Choromanski%2520and%2520Ceslee%2520Montgomery%2520and%2520Dheeru%2520Dua%2520and%2520Ana%2520Ramalho%2520and%2520Helen%2520King%2520and%2520Yue%2520Gao%2520and%2520Lynn%2520Nguyen%2520and%2520David%2520Lindner%2520and%2520Divya%2520Pitta%2520and%2520Oleaser%2520Johnson%2520and%2520Khalid%2520Salama%2520and%2520Diego%2520Ardila%2520and%2520Michael%2520Han%2520and%2520Erin%2520Farnese%2520and%2520Seth%2520Odoom%2520and%2520Ziyue%2520Wang%2520and%2520Xiangzhuo%2520Ding%2520and%2520Norman%2520Rink%2520and%2520Ray%2520Smith%2520and%2520Harshal%2520Tushar%2520Lehri%2520and%2520Eden%2520Cohen%2520and%2520Neera%2520Vats%2520and%2520Tong%2520He%2520and%2520Parthasarathy%2520Gopavarapu%2520and%2520Adam%2520Paszke%2520and%2520Miteyan%2520Patel%2520and%2520Wouter%2520Van%2520Gansbeke%2520and%2520Lucia%2520Loher%2520and%2520Luis%2520Castro%2520and%2520Maria%2520Voitovich%2520and%2520Tamara%2520von%2520Glehn%2520and%2520Nelson%2520George%2520and%2520Simon%2520Niklaus%2520and%2520Zach%2520Eaton-Rosen%2520and%2520Nemanja%2520Raki%25C4%2587evi%25C4%2587%2520and%2520Erik%2520Jue%2520and%2520Sagi%2520Perel%2520and%2520Carrie%2520Zhang%2520and%2520Yuval%2520Bahat%2520and%2520Ang%25C3%25A9line%2520Pouget%2520and%2520Zhi%2520Xing%2520and%2520Fantine%2520Huot%2520and%2520Ashish%2520Shenoy%2520and%2520Taylor%2520Bos%2520and%2520Vincent%2520Coriou%2520and%2520Bryan%2520Richter%2520and%2520Natasha%2520Noy%2520and%2520Yaqing%2520Wang%2520and%2520Santiago%2520Ontanon%2520and%2520Siyang%2520Qin%2520and%2520Gleb%2520Makarchuk%2520and%2520Demis%2520Hassabis%2520and%2520Zhuowan%2520Li%2520and%2520Mandar%2520Sharma%2520and%2520Kumaran%2520Venkatesan%2520and%2520Iurii%2520Kemaev%2520and%2520Roxanne%2520Daniel%2520and%2520Shiyu%2520Huang%2520and%2520Saloni%2520Shah%2520and%2520Octavio%2520Ponce%2520and%2520%2520Warren%2520and%2520%2520Chen%2520and%2520Manaal%2520Faruqui%2520and%2520Jialin%2520Wu%2520and%2520Slavica%2520Anda%25C4%258Di%25C4%2587%2520and%2520Szabolcs%2520Payrits%2520and%2520Daniel%2520McDuff%2520and%2520Tom%2520Hume%2520and%2520Yuan%2520Cao%2520and%2520MH%2520Tessler%2520and%2520Qingze%2520Wang%2520and%2520Yinan%2520Wang%2520and%2520Ivor%2520Rendulic%2520and%2520Eirikur%2520Agustsson%2520and%2520Matthew%2520Johnson%2520and%2520Tanya%2520Lando%2520and%2520Andrew%2520Howard%2520and%2520Sri%2520Gayatri%2520Sundara%2520Padmanabhan%2520and%2520Mayank%2520Daswani%2520and%2520Andrea%2520Banino%2520and%2520Michael%2520Kilgore%2520and%2520Jonathan%2520Heek%2520and%2520Ziwei%2520Ji%2520and%2520Alvaro%2520Caceres%2520and%2520Conglong%2520Li%2520and%2520Nora%2520Kassner%2520and%2520Alexey%2520Vlaskin%2520and%2520Zeyu%2520Liu%2520and%2520Alex%2520Grills%2520and%2520Yanhan%2520Hou%2520and%2520Roykrong%2520Sukkerd%2520and%2520Gowoon%2520Cheon%2520and%2520Nishita%2520Shetty%2520and%2520Larisa%2520Markeeva%2520and%2520Piotr%2520Stanczyk%2520and%2520Tejas%2520Iyer%2520and%2520Yuan%2520Gong%2520and%2520Shawn%2520Gao%2520and%2520Keerthana%2520Gopalakrishnan%2520and%2520Tim%2520Blyth%2520and%2520Malcolm%2520Reynolds%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Misha%2520Bilenko%2520and%2520Dero%2520Gharibian%2520and%2520Vicky%2520Zayats%2520and%2520Aleksandra%2520Faust%2520and%2520Abhinav%2520Singh%2520and%2520Min%2520Ma%2520and%2520Hongyang%2520Jiao%2520and%2520Sudheendra%2520Vijayanarasimhan%2520and%2520Lora%2520Aroyo%2520and%2520Vikas%2520Yadav%2520and%2520Sarah%2520Chakera%2520and%2520Ashwin%2520Kakarla%2520and%2520Vilobh%2520Meshram%2520and%2520Karol%2520Gregor%2520and%2520Gabriela%2520Botea%2520and%2520Evan%2520Senter%2520and%2520Dawei%2520Jia%2520and%2520Geza%2520Kovacs%2520and%2520Neha%2520Sharma%2520and%2520Sebastien%2520Baur%2520and%2520Kai%2520Kang%2520and%2520Yifan%2520He%2520and%2520Lin%2520Zhuo%2520and%2520Marija%2520Kostelac%2520and%2520Itay%2520Laish%2520and%2520Songyou%2520Peng%2520and%2520Louis%2520O%2527Bryan%2520and%2520Daniel%2520Kasenberg%2520and%2520Girish%2520Ramchandra%2520Rao%2520and%2520Edouard%2520Leurent%2520and%2520Biao%2520Zhang%2520and%2520Sage%2520Stevens%2520and%2520Ana%2520Salazar%2520and%2520Ye%2520Zhang%2520and%2520Ivan%2520Lobov%2520and%2520Jake%2520Walker%2520and%2520Allen%2520Porter%2520and%2520Morgan%2520Redshaw%2520and%2520Han%2520Ke%2520and%2520Abhishek%2520Rao%2520and%2520Alex%2520Lee%2520and%2520Hoi%2520Lam%2520and%2520Michael%2520Moffitt%2520and%2520Jaeyoun%2520Kim%2520and%2520Siyuan%2520Qiao%2520and%2520Terry%2520Koo%2520and%2520Robert%2520Dadashi%2520and%2520Xinying%2520Song%2520and%2520Mukund%2520Sundararajan%2520and%2520Peng%2520Xu%2520and%2520Chizu%2520Kawamoto%2520and%2520Yan%2520Zhong%2520and%2520Clara%2520Barbu%2520and%2520Apoorv%2520Reddy%2520and%2520Mauro%2520Verzetti%2520and%2520Leon%2520Li%2520and%2520George%2520Papamakarios%2520and%2520Hanna%2520Klimczak-Pluci%25C5%2584ska%2520and%2520Mary%2520Cassin%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Rigel%2520Swavely%2520and%2520Alain%2520Vaucher%2520and%2520Jeffrey%2520Zhao%2520and%2520Ross%2520Hemsley%2520and%2520Michael%2520Tschannen%2520and%2520Heming%2520Ge%2520and%2520Gaurav%2520Menghani%2520and%2520Yang%2520Yu%2520and%2520Natalie%2520Ha%2520and%2520Wei%2520He%2520and%2520Xiao%2520Wu%2520and%2520Maggie%2520Song%2520and%2520Rachel%2520Sterneck%2520and%2520Stefan%2520Zinke%2520and%2520Dan%2520A.%2520Calian%2520and%2520Annie%2520Marsden%2520and%2520Alejandro%2520Cruzado%2520Ruiz%2520and%2520Matteo%2520Hessel%2520and%2520Almog%2520Gueta%2520and%2520Benjamin%2520Lee%2520and%2520Brian%2520Farris%2520and%2520Manish%2520Gupta%2520and%2520Yunjie%2520Li%2520and%2520Mohammad%2520Saleh%2520and%2520Vedant%2520Misra%2520and%2520Kefan%2520Xiao%2520and%2520Piermaria%2520Mendolicchio%2520and%2520Gavin%2520Buttimore%2520and%2520Varvara%2520Krayvanova%2520and%2520Nigamaa%2520Nayakanti%2520and%2520Matthew%2520Wiethoff%2520and%2520Yash%2520Pande%2520and%2520Azalia%2520Mirhoseini%2520and%2520Ni%2520Lao%2520and%2520Jasmine%2520Liu%2520and%2520Yiqing%2520Hua%2520and%2520Angie%2520Chen%2520and%2520Yury%2520Malkov%2520and%2520Dmitry%2520Kalashnikov%2520and%2520Shubham%2520Gupta%2520and%2520Kartik%2520Audhkhasi%2520and%2520Yuexiang%2520Zhai%2520and%2520Sudhindra%2520Kopalle%2520and%2520Prateek%2520Jain%2520and%2520Eran%2520Ofek%2520and%2520Clemens%2520Meyer%2520and%2520Khuslen%2520Baatarsukh%2520and%2520Hana%2520Strej%25C4%258Dek%2520and%2520Jun%2520Qian%2520and%2520James%2520Freedman%2520and%2520Ricardo%2520Figueira%2520and%2520Michal%2520Sokolik%2520and%2520Olivier%2520Bachem%2520and%2520Raymond%2520Lin%2520and%2520Dia%2520Kharrat%2520and%2520Chris%2520Hidey%2520and%2520Pingmei%2520Xu%2520and%2520Dennis%2520Duan%2520and%2520Yin%2520Li%2520and%2520Muge%2520Ersoy%2520and%2520Richard%2520Everett%2520and%2520Kevin%2520Cen%2520and%2520Rebeca%2520Santamaria-Fernandez%2520and%2520Amir%2520Taubenfeld%2520and%2520Ian%2520Mackinnon%2520and%2520Linda%2520Deng%2520and%2520Polina%2520Zablotskaia%2520and%2520Shashank%2520Viswanadha%2520and%2520Shivanker%2520Goel%2520and%2520Damion%2520Yates%2520and%2520Yunxiao%2520Deng%2520and%2520Peter%2520Choy%2520and%2520Mingqing%2520Chen%2520and%2520Abhishek%2520Sinha%2520and%2520Alex%2520Mossin%2520and%2520Yiming%2520Wang%2520and%2520Arthur%2520Szlam%2520and%2520Susan%2520Hao%2520and%2520Paul%2520Kishan%2520Rubenstein%2520and%2520Metin%2520Toksoz-Exley%2520and%2520Miranda%2520Aperghis%2520and%2520Yin%2520Zhong%2520and%2520Junwhan%2520Ahn%2520and%2520Michael%2520Isard%2520and%2520Olivier%2520Lacombe%2520and%2520Florian%2520Luisier%2520and%2520Chrysovalantis%2520Anastasiou%2520and%2520Yogesh%2520Kalley%2520and%2520Utsav%2520Prabhu%2520and%2520Emma%2520Dunleavy%2520and%2520Shaan%2520Bijwadia%2520and%2520Justin%2520Mao-Jones%2520and%2520Kelly%2520Chen%2520and%2520Rama%2520Pasumarthi%2520and%2520Emily%2520Wood%2520and%2520Adil%2520Dostmohamed%2520and%2520Nate%2520Hurley%2520and%2520Jiri%2520Simsa%2520and%2520Alicia%2520Parrish%2520and%2520Mantas%2520Pajarskas%2520and%2520Matt%2520Harvey%2520and%2520Ondrej%2520Skopek%2520and%2520Yony%2520Kochinski%2520and%2520Javier%2520Rey%2520and%2520Verena%2520Rieser%2520and%2520Denny%2520Zhou%2520and%2520Sun%2520Jae%2520Lee%2520and%2520Trilok%2520Acharya%2520and%2520Guowang%2520Li%2520and%2520Joe%2520Jiang%2520and%2520Xiaofan%2520Zhang%2520and%2520Bryant%2520Gipson%2520and%2520Ethan%2520Mahintorabi%2520and%2520Marco%2520Gelmi%2520and%2520Nima%2520Khajehnouri%2520and%2520Angel%2520Yeh%2520and%2520Kayi%2520Lee%2520and%2520Loic%2520Matthey%2520and%2520Leslie%2520Baker%2520and%2520Trang%2520Pham%2520and%2520Han%2520Fu%2520and%2520Alex%2520Pak%2520and%2520Prakhar%2520Gupta%2520and%2520Cristina%2520Vasconcelos%2520and%2520Adam%2520Sadovsky%2520and%2520Brian%2520Walker%2520and%2520Sissie%2520Hsiao%2520and%2520Patrik%2520Zochbauer%2520and%2520Andreea%2520Marzoca%2520and%2520Noam%2520Velan%2520and%2520Junhao%2520Zeng%2520and%2520Gilles%2520Baechler%2520and%2520Danny%2520Driess%2520and%2520Divya%2520Jain%2520and%2520Yanping%2520Huang%2520and%2520Lizzie%2520Tao%2520and%2520John%2520Maggs%2520and%2520Nir%2520Levine%2520and%2520Jon%2520Schneider%2520and%2520Erika%2520Gemzer%2520and%2520Samuel%2520Petit%2520and%2520Shan%2520Han%2520and%2520Zach%2520Fisher%2520and%2520Dustin%2520Zelle%2520and%2520Courtney%2520Biles%2520and%2520Eugene%2520Ie%2520and%2520Asya%2520Fadeeva%2520and%2520Casper%2520Liu%2520and%2520Juliana%2520Vicente%2520Franco%2520and%2520Adrian%2520Collister%2520and%2520Hao%2520Zhang%2520and%2520Renshen%2520Wang%2520and%2520Ruizhe%2520Zhao%2520and%2520Leandro%2520Kieliger%2520and%2520Kurt%2520Shuster%2520and%2520Rui%2520Zhu%2520and%2520Boqing%2520Gong%2520and%2520Lawrence%2520Chan%2520and%2520Ruoxi%2520Sun%2520and%2520Sujoy%2520Basu%2520and%2520Roland%2520Zimmermann%2520and%2520Jamie%2520Hayes%2520and%2520Abhishek%2520Bapna%2520and%2520Jasper%2520Snoek%2520and%2520Weel%2520Yang%2520and%2520Puranjay%2520Datta%2520and%2520Jad%2520Al%2520Abdallah%2520and%2520Kevin%2520Kilgour%2520and%2520Lu%2520Li%2520and%2520SQ%2520Mah%2520and%2520Yennie%2520Jun%2520and%2520Morgane%2520Rivi%25C3%25A8re%2520and%2520Abhijit%2520Karmarkar%2520and%2520Tammo%2520Spalink%2520and%2520Tao%2520Huang%2520and%2520Lucas%2520Gonzalez%2520and%2520Duc-Hieu%2520Tran%2520and%2520Averi%2520Nowak%2520and%2520John%2520Palowitch%2520and%2520Martin%2520Chadwick%2520and%2520Ellie%2520Talius%2520and%2520Harsh%2520Mehta%2520and%2520Thibault%2520Sellam%2520and%2520Philipp%2520Fr%25C3%25A4nken%2520and%2520Massimo%2520Nicosia%2520and%2520Kyle%2520He%2520and%2520Aditya%2520Kini%2520and%2520David%2520Amos%2520and%2520Sugato%2520Basu%2520and%2520Harrison%2520Jobe%2520and%2520Eleni%2520Shaw%2520and%2520Qiantong%2520Xu%2520and%2520Colin%2520Evans%2520and%2520Daisuke%2520Ikeda%2520and%2520Chaochao%2520Yan%2520and%2520Larry%2520Jin%2520and%2520Lun%2520Wang%2520and%2520Sachin%2520Yadav%2520and%2520Ilia%2520Labzovsky%2520and%2520Ramesh%2520Sampath%2520and%2520Ada%2520Ma%2520and%2520Candice%2520Schumann%2520and%2520Aditya%2520Siddhant%2520and%2520Rohin%2520Shah%2520and%2520John%2520Youssef%2520and%2520Rishabh%2520Agarwal%2520and%2520Natalie%2520Dabney%2520and%2520Alessio%2520Tonioni%2520and%2520Moran%2520Ambar%2520and%2520Jing%2520Li%2520and%2520Isabelle%2520Guyon%2520and%2520Benny%2520Li%2520and%2520David%2520Soergel%2520and%2520Boya%2520Fang%2520and%2520Georgi%2520Karadzhov%2520and%2520Cristian%2520Udrescu%2520and%2520Trieu%2520Trinh%2520and%2520Vikas%2520Raunak%2520and%2520Seb%2520Noury%2520and%2520Dee%2520Guo%2520and%2520Sonal%2520Gupta%2520and%2520Mara%2520Finkelstein%2520and%2520Denis%2520Petek%2520and%2520Lihao%2520Liang%2520and%2520Greg%2520Billock%2520and%2520Pei%2520Sun%2520and%2520David%2520Wood%2520and%2520Yiwen%2520Song%2520and%2520Xiaobin%2520Yu%2520and%2520Tatiana%2520Matejovicova%2520and%2520Regev%2520Cohen%2520and%2520Kalyan%2520Andra%2520and%2520David%2520D%2527Ambrosio%2520and%2520Zhiwei%2520Deng%2520and%2520Vincent%2520Nallatamby%2520and%2520Ebrahim%2520Songhori%2520and%2520Rumen%2520Dangovski%2520and%2520Andrew%2520Lampinen%2520and%2520Pankil%2520Botadra%2520and%2520Adam%2520Hillier%2520and%2520Jiawei%2520Cao%2520and%2520Nagabhushan%2520Baddi%2520and%2520Adhi%2520Kuncoro%2520and%2520Toshihiro%2520Yoshino%2520and%2520Ankit%2520Bhagatwala%2520and%2520Marc%25C3%25A1urelio%2520Ranzato%2520and%2520Rylan%2520Schaeffer%2520and%2520Tianlin%2520Liu%2520and%2520Shuai%2520Ye%2520and%2520Obaid%2520Sarvana%2520and%2520John%2520Nham%2520and%2520Chenkai%2520Kuang%2520and%2520Isabel%2520Gao%2520and%2520Jinoo%2520Baek%2520and%2520Shubham%2520Mittal%2520and%2520Ayzaan%2520Wahid%2520and%2520Anita%2520Gergely%2520and%2520Bin%2520Ni%2520and%2520Josh%2520Feldman%2520and%2520Carrie%2520Muir%2520and%2520Pascal%2520Lamblin%2520and%2520Wolfgang%2520Macherey%2520and%2520Ethan%2520Dyer%2520and%2520Logan%2520Kilpatrick%2520and%2520V%25C3%25ADctor%2520Campos%2520and%2520Mukul%2520Bhutani%2520and%2520Stanislav%2520Fort%2520and%2520Yanif%2520Ahmad%2520and%2520Aliaksei%2520Severyn%2520and%2520Kleopatra%2520Chatziprimou%2520and%2520Oleksandr%2520Ferludin%2520and%2520Mason%2520Dimarco%2520and%2520Aditya%2520Kusupati%2520and%2520Joe%2520Heyward%2520and%2520Dan%2520Bahir%2520and%2520Kevin%2520Villela%2520and%2520Katie%2520Millican%2520and%2520Dror%2520Marcus%2520and%2520Sanaz%2520Bahargam%2520and%2520Caglar%2520Unlu%2520and%2520Nicholas%2520Roth%2520and%2520Zichuan%2520Wei%2520and%2520Siddharth%2520Gopal%2520and%2520Deepanway%2520Ghoshal%2520and%2520Edward%2520Lee%2520and%2520Sharon%2520Lin%2520and%2520Jennie%2520Lees%2520and%2520Dayeong%2520Lee%2520and%2520Anahita%2520Hosseini%2520and%2520Connie%2520Fan%2520and%2520Seth%2520Neel%2520and%2520Marcus%2520Wu%2520and%2520Yasemin%2520Altun%2520and%2520Honglong%2520Cai%2520and%2520Enrique%2520Piqueras%2520and%2520Josh%2520Woodward%2520and%2520Alessandro%2520Bissacco%2520and%2520Salem%2520Haykal%2520and%2520Mahyar%2520Bordbar%2520and%2520Prasha%2520Sundaram%2520and%2520Sarah%2520Hodkinson%2520and%2520Daniel%2520Toyama%2520and%2520George%2520Polovets%2520and%2520Austin%2520Myers%2520and%2520Anu%2520Sinha%2520and%2520Tomer%2520Levinboim%2520and%2520Kashyap%2520Krishnakumar%2520and%2520Rachita%2520Chhaparia%2520and%2520Tatiana%2520Sholokhova%2520and%2520Nitesh%2520Bharadwaj%2520Gundavarapu%2520and%2520Ganesh%2520Jawahar%2520and%2520Haroon%2520Qureshi%2520and%2520Jieru%2520Hu%2520and%2520Nikola%2520Momchev%2520and%2520Matthew%2520Rahtz%2520and%2520Renjie%2520Wu%2520and%2520Aishwarya%2520P%2520S%2520and%2520Kedar%2520Dhamdhere%2520and%2520Meiqi%2520Guo%2520and%2520Umang%2520Gupta%2520and%2520Ali%2520Eslami%2520and%2520Mariano%2520Schain%2520and%2520Michiel%2520Blokzijl%2520and%2520David%2520Welling%2520and%2520Dave%2520Orr%2520and%2520Levent%2520Bolelli%2520and%2520Nicolas%2520Perez-Nieves%2520and%2520Mikhail%2520Sirotenko%2520and%2520Aman%2520Prasad%2520and%2520Arjun%2520Kar%2520and%2520Borja%2520De%2520Balle%2520Pigem%2520and%2520Tayfun%2520Terzi%2520and%2520Gell%25C3%25A9rt%2520Weisz%2520and%2520Dipankar%2520Ghosh%2520and%2520Aditi%2520Mavalankar%2520and%2520Dhruv%2520Madeka%2520and%2520Kaspar%2520Daugaard%2520and%2520Hartwig%2520Adam%2520and%2520Viraj%2520Shah%2520and%2520Dana%2520Berman%2520and%2520Maggie%2520Tran%2520and%2520Steven%2520Baker%2520and%2520Ewa%2520Andrejczuk%2520and%2520Grishma%2520Chole%2520and%2520Ganna%2520Raboshchuk%2520and%2520Mahdi%2520Mirzazadeh%2520and%2520Thais%2520Kagohara%2520and%2520Shimu%2520Wu%2520and%2520Christian%2520Schallhart%2520and%2520Bernett%2520Orlando%2520and%2520Chen%2520Wang%2520and%2520Alban%2520Rrustemi%2520and%2520Hao%2520Xiong%2520and%2520Hao%2520Liu%2520and%2520Arpi%2520Vezer%2520and%2520Nolan%2520Ramsden%2520and%2520Shuo-yiin%2520Chang%2520and%2520Sidharth%2520Mudgal%2520and%2520Yan%2520Li%2520and%2520Nino%2520Vieillard%2520and%2520Yedid%2520Hoshen%2520and%2520Farooq%2520Ahmad%2520and%2520Ambrose%2520Slone%2520and%2520Amy%2520Hua%2520and%2520Natan%2520Potikha%2520and%2520Mirko%2520Rossini%2520and%2520Jon%2520Stritar%2520and%2520Sushant%2520Prakash%2520and%2520Zifeng%2520Wang%2520and%2520Xuanyi%2520Dong%2520and%2520Alireza%2520Nazari%2520and%2520Efrat%2520Nehoran%2520and%2520Kaan%2520Tekelioglu%2520and%2520Yinxiao%2520Li%2520and%2520Kartikeya%2520Badola%2520and%2520Tom%2520Funkhouser%2520and%2520Yuanzhen%2520Li%2520and%2520Varun%2520Yerram%2520and%2520Ramya%2520Ganeshan%2520and%2520Daniel%2520Formoso%2520and%2520Karol%2520Langner%2520and%2520Tian%2520Shi%2520and%2520Huijian%2520Li%2520and%2520Yumeya%2520Yamamori%2520and%2520Amayika%2520Panda%2520and%2520Alaa%2520Saade%2520and%2520Angelo%2520Scorza%2520Scarpati%2520and%2520Chris%2520Breaux%2520and%2520CJ%2520Carey%2520and%2520Zongwei%2520Zhou%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Sophie%2520Bridgers%2520and%2520Alena%2520Butryna%2520and%2520Nishesh%2520Gupta%2520and%2520Vaibhav%2520Tulsyan%2520and%2520Sanghyun%2520Woo%2520and%2520Evgenii%2520Eltyshev%2520and%2520Will%2520Grathwohl%2520and%2520Chanel%2520Parks%2520and%2520Seth%2520Benjamin%2520and%2520Rina%2520Panigrahy%2520and%2520Shenil%2520Dodhia%2520and%2520Daniel%2520De%2520Freitas%2520and%2520Chris%2520Sauer%2520and%2520Will%2520Song%2520and%2520Ferran%2520Alet%2520and%2520Jackson%2520Tolins%2520and%2520Cosmin%2520Paduraru%2520and%2520Xingyi%2520Zhou%2520and%2520Brian%2520Albert%2520and%2520Zizhao%2520Zhang%2520and%2520Lei%2520Shu%2520and%2520Mudit%2520Bansal%2520and%2520Sarah%2520Nguyen%2520and%2520Amir%2520Globerson%2520and%2520Owen%2520Xiao%2520and%2520James%2520Manyika%2520and%2520Tom%2520Hennigan%2520and%2520Rong%2520Rong%2520and%2520Josip%2520Matak%2520and%2520Anton%2520Bakalov%2520and%2520Ankur%2520Sharma%2520and%2520Danila%2520Sinopalnikov%2520and%2520Andrew%2520Pierson%2520and%2520Stephen%2520Roller%2520and%2520Geoff%2520Brown%2520and%2520Mingcen%2520Gao%2520and%2520Toshiyuki%2520Fukuzawa%2520and%2520Amin%2520Ghafouri%2520and%2520Kenny%2520Vassigh%2520and%2520Iain%2520Barr%2520and%2520Zhicheng%2520Wang%2520and%2520Anna%2520Korsun%2520and%2520Rajesh%2520Jayaram%2520and%2520Lijie%2520Ren%2520and%2520Tim%2520Zaman%2520and%2520Samira%2520Khan%2520and%2520Yana%2520Lunts%2520and%2520Dan%2520Deutsch%2520and%2520Dave%2520Uthus%2520and%2520Nitzan%2520Katz%2520and%2520Masha%2520Samsikova%2520and%2520Amr%2520Khalifa%2520and%2520Nikhil%2520Sethi%2520and%2520Jiao%2520Sun%2520and%2520Luming%2520Tang%2520and%2520Uri%2520Alon%2520and%2520Xianghong%2520Luo%2520and%2520Dian%2520Yu%2520and%2520Abhishek%2520Nayyar%2520and%2520Bryce%2520Petrini%2520and%2520Will%2520Truong%2520and%2520Vincent%2520Hellendoorn%2520and%2520Nikolai%2520Chinaev%2520and%2520Chris%2520Alberti%2520and%2520Wei%2520Wang%2520and%2520Jingcao%2520Hu%2520and%2520Vahab%2520Mirrokni%2520and%2520Ananth%2520Balashankar%2520and%2520Avia%2520Aharon%2520and%2520Aahil%2520Mehta%2520and%2520Ahmet%2520Iscen%2520and%2520Joseph%2520Kready%2520and%2520Lucas%2520Manning%2520and%2520Anhad%2520Mohananey%2520and%2520Yuankai%2520Chen%2520and%2520Anshuman%2520Tripathi%2520and%2520Allen%2520Wu%2520and%2520Igor%2520Petrovski%2520and%2520Dawsen%2520Hwang%2520and%2520Martin%2520Baeuml%2520and%2520Shreyas%2520Chandrakaladharan%2520and%2520Yuan%2520Liu%2520and%2520Rey%2520Coaguila%2520and%2520Maxwell%2520Chen%2520and%2520Sally%2520Ma%2520and%2520Pouya%2520Tafti%2520and%2520Susheel%2520Tatineni%2520and%2520Terry%2520Spitz%2520and%2520Jiayu%2520Ye%2520and%2520Paul%2520Vicol%2520and%2520Mihaela%2520Rosca%2520and%2520Adri%25C3%25A0%2520Puigdom%25C3%25A8nech%2520and%2520Zohar%2520Yahav%2520and%2520Sanjay%2520Ghemawat%2520and%2520Hanzhao%2520Lin%2520and%2520Phoebe%2520Kirk%2520and%2520Zaid%2520Nabulsi%2520and%2520Sergey%2520Brin%2520and%2520Bernd%2520Bohnet%2520and%2520Ken%2520Caluwaerts%2520and%2520Aditya%2520Srikanth%2520Veerubhotla%2520and%2520Dan%2520Zheng%2520and%2520Zihang%2520Dai%2520and%2520Petre%2520Petrov%2520and%2520Yichong%2520Xu%2520and%2520Ramin%2520Mehran%2520and%2520Zhuo%2520Xu%2520and%2520Luisa%2520Zintgraf%2520and%2520Jiho%2520Choi%2520and%2520Spurthi%2520Amba%2520Hombaiah%2520and%2520Romal%2520Thoppilan%2520and%2520Sashank%2520Reddi%2520and%2520Lukasz%2520Lew%2520and%2520Li%2520Li%2520and%2520Kellie%2520Webster%2520and%2520KP%2520Sawhney%2520and%2520Lampros%2520Lamprou%2520and%2520Siamak%2520Shakeri%2520and%2520Mayank%2520Lunayach%2520and%2520Jianmin%2520Chen%2520and%2520Sumit%2520Bagri%2520and%2520Alex%2520Salcianu%2520and%2520Ying%2520Chen%2520and%2520Yani%2520Donchev%2520and%2520Charlotte%2520Magister%2520and%2520Signe%2520N%25C3%25B8rly%2520and%2520Vitor%2520Rodrigues%2520and%2520Tomas%2520Izo%2520and%2520Hila%2520Noga%2520and%2520Joe%2520Zou%2520and%2520Thomas%2520K%25C3%25B6ppe%2520and%2520Wenxuan%2520Zhou%2520and%2520Kenton%2520Lee%2520and%2520Xiangzhu%2520Long%2520and%2520Danielle%2520Eisenbud%2520and%2520Anthony%2520Chen%2520and%2520Connor%2520Schenck%2520and%2520Chi%2520Ming%2520To%2520and%2520Peilin%2520Zhong%2520and%2520Emanuel%2520Taropa%2520and%2520Minh%2520Truong%2520and%2520Omer%2520Levy%2520and%2520Danilo%2520Martins%2520and%2520Zhiyuan%2520Zhang%2520and%2520Christopher%2520Semturs%2520and%2520Kelvin%2520Zhang%2520and%2520Alex%2520Yakubovich%2520and%2520Pol%2520Moreno%2520and%2520Lara%2520McConnaughey%2520and%2520Di%2520Lu%2520and%2520Sam%2520Redmond%2520and%2520Lotte%2520Weerts%2520and%2520Yonatan%2520Bitton%2520and%2520Tiziana%2520Refice%2520and%2520Nicolas%2520Lacasse%2520and%2520Arthur%2520Conmy%2520and%2520Corentin%2520Tallec%2520and%2520Julian%2520Odell%2520and%2520Hannah%2520Forbes-Pollard%2520and%2520Arkadiusz%2520Socala%2520and%2520Jonathan%2520Hoech%2520and%2520Pushmeet%2520Kohli%2520and%2520Alanna%2520Walton%2520and%2520Rui%2520Wang%2520and%2520Mikita%2520Sazanovich%2520and%2520Kexin%2520Zhu%2520and%2520Andrei%2520Kapishnikov%2520and%2520Rich%2520Galt%2520and%2520Matthew%2520Denton%2520and%2520Ben%2520Murdoch%2520and%2520Caitlin%2520Sikora%2520and%2520Kareem%2520Mohamed%2520and%2520Wei%2520Wei%2520and%2520Uri%2520First%2520and%2520Tim%2520McConnell%2520and%2520Luis%2520C.%2520Cobo%2520and%2520James%2520Qin%2520and%2520Thi%2520Avrahami%2520and%2520Daniel%2520Balle%2520and%2520Yu%2520Watanabe%2520and%2520Annie%2520Louis%2520and%2520Adam%2520Kraft%2520and%2520Setareh%2520Ariafar%2520and%2520Yiming%2520Gu%2520and%2520Eug%25C3%25A9nie%2520Rives%2520and%2520Charles%2520Yoon%2520and%2520Andrei%2520Rusu%2520and%2520James%2520Cobon-Kerr%2520and%2520Chris%2520Hahn%2520and%2520Jiaming%2520Luo%2520and%2520%2520Yuvein%2520and%2520%2520Zhu%2520and%2520Niharika%2520Ahuja%2520and%2520Rodrigo%2520Benenson%2520and%2520Rapha%25C3%25ABl%2520Lopez%2520Kaufman%2520and%2520Honglin%2520Yu%2520and%2520Lloyd%2520Hightower%2520and%2520Junlin%2520Zhang%2520and%2520Darren%2520Ni%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Gabby%2520Wang%2520and%2520Gal%2520Yona%2520and%2520Lalit%2520Jain%2520and%2520Pablo%2520Barrio%2520and%2520Surya%2520Bhupatiraju%2520and%2520Siva%2520Velusamy%2520and%2520Allan%2520Dafoe%2520and%2520Sebastian%2520Riedel%2520and%2520Tara%2520Thomas%2520and%2520Zhe%2520Yuan%2520and%2520Mathias%2520Bellaiche%2520and%2520Sheena%2520Panthaplackel%2520and%2520Klemen%2520Kloboves%2520and%2520Sarthak%2520Jauhari%2520and%2520Canfer%2520Akbulut%2520and%2520Todor%2520Davchev%2520and%2520Evgeny%2520Gladchenko%2520and%2520David%2520Madras%2520and%2520Aleksandr%2520Chuklin%2520and%2520Tyrone%2520Hill%2520and%2520Quan%2520Yuan%2520and%2520Mukundan%2520Madhavan%2520and%2520Luke%2520Leonhard%2520and%2520Dylan%2520Scandinaro%2520and%2520Qihang%2520Chen%2520and%2520Ning%2520Niu%2520and%2520Arthur%2520Douillard%2520and%2520Bogdan%2520Damoc%2520and%2520Yasumasa%2520Onoe%2520and%2520Fabian%2520Pedregosa%2520and%2520Fred%2520Bertsch%2520and%2520Chas%2520Leichner%2520and%2520Joseph%2520Pagadora%2520and%2520Jonathan%2520Malmaud%2520and%2520Sameera%2520Ponda%2520and%2520Andy%2520Twigg%2520and%2520Oleksii%2520Duzhyi%2520and%2520Jingwei%2520Shen%2520and%2520Miaosen%2520Wang%2520and%2520Roopal%2520Garg%2520and%2520Jing%2520Chen%2520and%2520Utku%2520Evci%2520and%2520Jonathan%2520Lee%2520and%2520Leon%2520Liu%2520and%2520Koji%2520Kojima%2520and%2520Masa%2520Yamaguchi%2520and%2520Arunkumar%2520Rajendran%2520and%2520AJ%2520Piergiovanni%2520and%2520Vinodh%2520Kumar%2520Rajendran%2520and%2520Marco%2520Fornoni%2520and%2520Gabriel%2520Ibagon%2520and%2520Harry%2520Ragan%2520and%2520Sadh%2520MNM%2520Khan%2520and%2520John%2520Blitzer%2520and%2520Andrew%2520Bunner%2520and%2520Guan%2520Sun%2520and%2520Takahiro%2520Kosakai%2520and%2520Scott%2520Lundberg%2520and%2520Ndidi%2520Elue%2520and%2520Kelvin%2520Guu%2520and%2520SK%2520Park%2520and%2520Jane%2520Park%2520and%2520Arunachalam%2520Narayanaswamy%2520and%2520Chengda%2520Wu%2520and%2520Jayaram%2520Mudigonda%2520and%2520Trevor%2520Cohn%2520and%2520Hairong%2520Mu%2520and%2520Ravi%2520Kumar%2520and%2520Laura%2520Graesser%2520and%2520Yichi%2520Zhang%2520and%2520Richard%2520Killam%2520and%2520Vincent%2520Zhuang%2520and%2520Mai%2520Gim%25C3%25A9nez%2520and%2520Wael%2520Al%2520Jishi%2520and%2520Ruy%2520Ley-Wild%2520and%2520Alex%2520Zhai%2520and%2520Kazuki%2520Osawa%2520and%2520Diego%2520Cedillo%2520and%2520Jialu%2520Liu%2520and%2520Mayank%2520Upadhyay%2520and%2520Marcin%2520Sieniek%2520and%2520Roshan%2520Sharma%2520and%2520Tom%2520Paine%2520and%2520Anelia%2520Angelova%2520and%2520Sravanti%2520Addepalli%2520and%2520Carolina%2520Parada%2520and%2520Kingshuk%2520Majumder%2520and%2520Avery%2520Lamp%2520and%2520Sanjiv%2520Kumar%2520and%2520Xiang%2520Deng%2520and%2520Artiom%2520Myaskovsky%2520and%2520Tea%2520Saboli%25C4%2587%2520and%2520Jeffrey%2520Dudek%2520and%2520Sarah%2520York%2520and%2520F%25C3%25A9lix%2520de%2520Chaumont%2520Quitry%2520and%2520Jiazhong%2520Nie%2520and%2520Dee%2520Cattle%2520and%2520Alok%2520Gunjan%2520and%2520Bilal%2520Piot%2520and%2520Waleed%2520Khawaja%2520and%2520Seojin%2520Bang%2520and%2520Simon%2520Wang%2520and%2520Siavash%2520Khodadadeh%2520and%2520Raghavender%2520R%2520and%2520Praynaa%2520Rawlani%2520and%2520Richard%2520Powell%2520and%2520Kevin%2520Lee%2520and%2520Johannes%2520Griesser%2520and%2520GS%2520Oh%2520and%2520Cesar%2520Magalhaes%2520and%2520Yujia%2520Li%2520and%2520Simon%2520Tokumine%2520and%2520Hadas%2520Natalie%2520Vogel%2520and%2520Dennis%2520Hsu%2520and%2520Arturo%2520BC%2520and%2520Disha%2520Jindal%2520and%2520Matan%2520Cohen%2520and%2520Zi%2520Yang%2520and%2520Junwei%2520Yuan%2520and%2520Dario%2520de%2520Cesare%2520and%2520Tony%2520Bruguier%2520and%2520Jun%2520Xu%2520and%2520Monica%2520Roy%2520and%2520Alon%2520Jacovi%2520and%2520Dan%2520Belov%2520and%2520Rahul%2520Arya%2520and%2520Phoenix%2520Meadowlark%2520and%2520Shlomi%2520Cohen-Ganor%2520and%2520Wenting%2520Ye%2520and%2520Patrick%2520Morris-Suzuki%2520and%2520Praseem%2520Banzal%2520and%2520Gan%2520Song%2520and%2520Pranavaraj%2520Ponnuramu%2520and%2520Fred%2520Zhang%2520and%2520George%2520Scrivener%2520and%2520Salah%2520Zaiem%2520and%2520Alif%2520Raditya%2520Rochman%2520and%2520Kehang%2520Han%2520and%2520Badih%2520Ghazi%2520and%2520Kate%2520Lee%2520and%2520Shahar%2520Drath%2520and%2520Daniel%2520Suo%2520and%2520Antonious%2520Girgis%2520and%2520Pradeep%2520Shenoy%2520and%2520Duy%2520Nguyen%2520and%2520Douglas%2520Eck%2520and%2520Somit%2520Gupta%2520and%2520Le%2520Yan%2520and%2520Joao%2520Carreira%2520and%2520Anmol%2520Gulati%2520and%2520Ruoxin%2520Sang%2520and%2520Daniil%2520Mirylenka%2520and%2520Emma%2520Cooney%2520and%2520Edward%2520Chou%2520and%2520Mingyang%2520Ling%2520and%2520Cindy%2520Fan%2520and%2520Ben%2520Coleman%2520and%2520Guilherme%2520Tubone%2520and%2520Ravin%2520Kumar%2520and%2520Jason%2520Baldridge%2520and%2520Felix%2520Hernandez-Campos%2520and%2520Angeliki%2520Lazaridou%2520and%2520James%2520Besley%2520and%2520Itay%2520Yona%2520and%2520Neslihan%2520Bulut%2520and%2520Quentin%2520Wellens%2520and%2520AJ%2520Pierigiovanni%2520and%2520Jasmine%2520George%2520and%2520Richard%2520Green%2520and%2520Pu%2520Han%2520and%2520Connie%2520Tao%2520and%2520Geoff%2520Clark%2520and%2520Chong%2520You%2520and%2520Abbas%2520Abdolmaleki%2520and%2520Justin%2520Fu%2520and%2520Tongzhou%2520Chen%2520and%2520Ashwin%2520Chaugule%2520and%2520Angad%2520Chandorkar%2520and%2520Altaf%2520Rahman%2520and%2520Will%2520Thompson%2520and%2520Penporn%2520Koanantakool%2520and%2520Mike%2520Bernico%2520and%2520Jie%2520Ren%2520and%2520Andrey%2520Vlasov%2520and%2520Sergei%2520Vassilvitskii%2520and%2520Maciej%2520Kula%2520and%2520Yizhong%2520Liang%2520and%2520Dahun%2520Kim%2520and%2520Yangsibo%2520Huang%2520and%2520Chengxi%2520Ye%2520and%2520Dmitry%2520Lepikhin%2520and%2520Wesley%2520Helmholz%26entry.1292438233%3DIn%2520this%2520report%252C%2520we%2520introduce%2520the%2520Gemini%25202.X%2520model%2520family%253A%2520Gemini%25202.5%2520Pro%2520and%2520Gemini%25202.5%2520Flash%252C%2520as%2520well%2520as%2520our%2520earlier%2520Gemini%25202.0%2520Flash%2520and%2520Flash-Lite%2520models.%2520Gemini%25202.5%2520Pro%2520is%2520our%2520most%2520capable%2520model%2520yet%252C%2520achieving%2520SoTA%2520performance%2520on%2520frontier%2520coding%2520and%2520reasoning%2520benchmarks.%2520In%2520addition%2520to%2520its%2520incredible%2520coding%2520and%2520reasoning%2520skills%252C%2520Gemini%25202.5%2520Pro%2520is%2520a%2520thinking%2520model%2520that%2520excels%2520at%2520multimodal%2520understanding%2520and%2520it%2520is%2520now%2520able%2520to%2520process%2520up%2520to%25203%2520hours%2520of%2520video%2520content.%2520Its%2520unique%2520combination%2520of%2520long%2520context%252C%2520multimodal%2520and%2520reasoning%2520capabilities%2520can%2520be%2520combined%2520to%2520unlock%2520new%2520agentic%2520workflows.%2520Gemini%25202.5%2520Flash%2520provides%2520excellent%2520reasoning%2520abilities%2520at%2520a%2520fraction%2520of%2520the%2520compute%2520and%2520latency%2520requirements%2520and%2520Gemini%25202.0%2520Flash%2520and%2520Flash-Lite%2520provide%2520high%2520performance%2520at%2520low%2520latency%2520and%2520cost.%2520Taken%2520together%252C%2520the%2520Gemini%25202.X%2520model%2520generation%2520spans%2520the%2520full%2520Pareto%2520frontier%2520of%2520model%2520capability%2520vs%2520cost%252C%2520allowing%2520users%2520to%2520explore%2520the%2520boundaries%2520of%2520what%2520is%2520possible%2520with%2520complex%2520agentic%2520problem%2520solving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06261v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities&entry.906535625=Gheorghe%20Comanici%20and%20Eric%20Bieber%20and%20Mike%20Schaekermann%20and%20Ice%20Pasupat%20and%20Noveen%20Sachdeva%20and%20Inderjit%20Dhillon%20and%20Marcel%20Blistein%20and%20Ori%20Ram%20and%20Dan%20Zhang%20and%20Evan%20Rosen%20and%20Luke%20Marris%20and%20Sam%20Petulla%20and%20Colin%20Gaffney%20and%20Asaf%20Aharoni%20and%20Nathan%20Lintz%20and%20Tiago%20Cardal%20Pais%20and%20Henrik%20Jacobsson%20and%20Idan%20Szpektor%20and%20Nan-Jiang%20Jiang%20and%20Krishna%20Haridasan%20and%20Ahmed%20Omran%20and%20Nikunj%20Saunshi%20and%20Dara%20Bahri%20and%20Gaurav%20Mishra%20and%20Eric%20Chu%20and%20Toby%20Boyd%20and%20Brad%20Hekman%20and%20Aaron%20Parisi%20and%20Chaoyi%20Zhang%20and%20Kornraphop%20Kawintiranon%20and%20Tania%20Bedrax-Weiss%20and%20Oliver%20Wang%20and%20Ya%20Xu%20and%20Ollie%20Purkiss%20and%20Uri%20Mendlovic%20and%20Ila%C3%AF%20Deutel%20and%20Nam%20Nguyen%20and%20Adam%20Langley%20and%20Flip%20Korn%20and%20Lucia%20Rossazza%20and%20Alexandre%20Ram%C3%A9%20and%20Sagar%20Waghmare%20and%20Helen%20Miller%20and%20Nathan%20Byrd%20and%20Ashrith%20Sheshan%20and%20Raia%20Hadsell%20and%20Sangnie%20Bhardwaj%20and%20Pawel%20Janus%20and%20Tero%20Rissa%20and%20Dan%20Horgan%20and%20Alvin%20Abdagic%20and%20Lior%20Belenki%20and%20James%20Allingham%20and%20Anima%20Singh%20and%20Theo%20Guidroz%20and%20Srivatsan%20Srinivasan%20and%20Herman%20Schmit%20and%20Kristen%20Chiafullo%20and%20Andre%20Elisseeff%20and%20Nilpa%20Jha%20and%20Prateek%20Kolhar%20and%20Leonard%20Berrada%20and%20Frank%20Ding%20and%20Xiance%20Si%20and%20Shrestha%20Basu%20Mallick%20and%20Franz%20Och%20and%20Sofia%20Erell%20and%20Eric%20Ni%20and%20Tejasi%20Latkar%20and%20Sherry%20Yang%20and%20Petar%20Sirkovic%20and%20Ziqiang%20Feng%20and%20Robert%20Leland%20and%20Rachel%20Hornung%20and%20Gang%20Wu%20and%20Charles%20Blundell%20and%20Hamidreza%20Alvari%20and%20Po-Sen%20Huang%20and%20Cathy%20Yip%20and%20Sanja%20Deur%20and%20Li%20Liu%20and%20Gabriela%20Surita%20and%20Pablo%20Duque%20and%20Dima%20Damen%20and%20Johnson%20Jia%20and%20Arthur%20Guez%20and%20Markus%20Mircea%20and%20Animesh%20Sinha%20and%20Alberto%20Magni%20and%20Pawe%C5%82%20Stradomski%20and%20Tal%20Marian%20and%20Vlado%20Gali%C4%87%20and%20Wenhu%20Chen%20and%20Hisham%20Husain%20and%20Achintya%20Singhal%20and%20Dominik%20Grewe%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Shuang%20Song%20and%20Lorenzo%20Blanco%20and%20Leland%20Rechis%20and%20Lewis%20Ho%20and%20Rich%20Munoz%20and%20Kelvin%20Zheng%20and%20Jessica%20Hamrick%20and%20Kevin%20Mather%20and%20Hagai%20Taitelbaum%20and%20Eliza%20Rutherford%20and%20Yun%20Lei%20and%20Kuangyuan%20Chen%20and%20Anand%20Shukla%20and%20Erica%20Moreira%20and%20Eric%20Doi%20and%20Berivan%20Isik%20and%20Nir%20Shabat%20and%20Dominika%20Rogozi%C5%84ska%20and%20Kashyap%20Kolipaka%20and%20Jason%20Chang%20and%20Eugen%20Vu%C5%A1ak%20and%20Srinivasan%20Venkatachary%20and%20Shadi%20Noghabi%20and%20Tarun%20Bharti%20and%20Younghoon%20Jun%20and%20Aleksandr%20Zaks%20and%20Simon%20Green%20and%20Jeshwanth%20Challagundla%20and%20William%20Wong%20and%20Muqthar%20Mohammad%20and%20Dean%20Hirsch%20and%20Yong%20Cheng%20and%20Iftekhar%20Naim%20and%20Lev%20Proleev%20and%20Damien%20Vincent%20and%20Aayush%20Singh%20and%20Maxim%20Krikun%20and%20Dilip%20Krishnan%20and%20Zoubin%20Ghahramani%20and%20Aviel%20Atias%20and%20Rajeev%20Aggarwal%20and%20Christo%20Kirov%20and%20Dimitrios%20Vytiniotis%20and%20Christy%20Koh%20and%20Alexandra%20Chronopoulou%20and%20Pawan%20Dogra%20and%20Vlad-Doru%20Ion%20and%20Gladys%20Tyen%20and%20Jason%20Lee%20and%20Felix%20Weissenberger%20and%20Trevor%20Strohman%20and%20Ashwin%20Balakrishna%20and%20Jack%20Rae%20and%20Marko%20Velic%20and%20Raoul%20de%20Liedekerke%20and%20Oded%20Elyada%20and%20Wentao%20Yuan%20and%20Canoee%20Liu%20and%20Lior%20Shani%20and%20Sergey%20Kishchenko%20and%20Bea%20Alessio%20and%20Yandong%20Li%20and%20Richard%20Song%20and%20Sam%20Kwei%20and%20Orion%20Jankowski%20and%20Aneesh%20Pappu%20and%20Youhei%20Namiki%20and%20Yenai%20Ma%20and%20Nilesh%20Tripuraneni%20and%20Colin%20Cherry%20and%20Marissa%20Ikonomidis%20and%20Yu-Cheng%20Ling%20and%20Colin%20Ji%20and%20Beka%20Westberg%20and%20Auriel%20Wright%20and%20Da%20Yu%20and%20David%20Parkinson%20and%20Swaroop%20Ramaswamy%20and%20Jerome%20Connor%20and%20Soheil%20Hassas%20Yeganeh%20and%20Snchit%20Grover%20and%20George%20Kenwright%20and%20Lubo%20Litchev%20and%20Chris%20Apps%20and%20Alex%20Tomala%20and%20Felix%20Halim%20and%20Alex%20Castro-Ros%20and%20Zefei%20Li%20and%20Anudhyan%20Boral%20and%20Pauline%20Sho%20and%20Michal%20Yarom%20and%20Eric%20Malmi%20and%20David%20Klinghoffer%20and%20Rebecca%20Lin%20and%20Alan%20Ansell%20and%20Pradeep%20Kumar%20S%20and%20Shubin%20Zhao%20and%20Siqi%20Zuo%20and%20Adam%20Santoro%20and%20Heng-Tze%20Cheng%20and%20Solomon%20Demmessie%20and%20Yuchi%20Liu%20and%20Nicole%20Brichtova%20and%20Allie%20Culp%20and%20Nathaniel%20Braun%20and%20Dan%20Graur%20and%20Will%20Ng%20and%20Nikhil%20Mehta%20and%20Aaron%20Phillips%20and%20Patrik%20Sundberg%20and%20Varun%20Godbole%20and%20Fangyu%20Liu%20and%20Yash%20Katariya%20and%20David%20Rim%20and%20Mojtaba%20Seyedhosseini%20and%20Sean%20Ammirati%20and%20Jonas%20Valfridsson%20and%20Mahan%20Malihi%20and%20Timothy%20Knight%20and%20Andeep%20Toor%20and%20Thomas%20Lampe%20and%20Abe%20Ittycheriah%20and%20Lewis%20Chiang%20and%20Chak%20Yeung%20and%20Alexandre%20Fr%C3%A9chette%20and%20Jinmeng%20Rao%20and%20Huisheng%20Wang%20and%20Himanshu%20Srivastava%20and%20Richard%20Zhang%20and%20Rocky%20Rhodes%20and%20Ariel%20Brand%20and%20Dean%20Weesner%20and%20Ilya%20Figotin%20and%20Felix%20Gimeno%20and%20Rachana%20Fellinger%20and%20Pierre%20Marcenac%20and%20Jos%C3%A9%20Leal%20and%20Eyal%20Marcus%20and%20Victor%20Cotruta%20and%20Rodrigo%20Cabrera%20and%20Sheryl%20Luo%20and%20Dan%20Garrette%20and%20Vera%20Axelrod%20and%20Sorin%20Baltateanu%20and%20David%20Barker%20and%20Dongkai%20Chen%20and%20Horia%20Toma%20and%20Ben%20Ingram%20and%20Jason%20Riesa%20and%20Chinmay%20Kulkarni%20and%20Yujing%20Zhang%20and%20Hongbin%20Liu%20and%20Chao%20Wang%20and%20Martin%20Polacek%20and%20Will%20Wu%20and%20Kai%20Hui%20and%20Adrian%20N%20Reyes%20and%20Yi%20Su%20and%20Megan%20Barnes%20and%20Ishaan%20Malhi%20and%20Anfal%20Siddiqui%20and%20Qixuan%20Feng%20and%20Mihai%20Damaschin%20and%20Daniele%20Pighin%20and%20Andreas%20Steiner%20and%20Samuel%20Yang%20and%20Ramya%20Sree%20Boppana%20and%20Simeon%20Ivanov%20and%20Arun%20Kandoor%20and%20Aditya%20Shah%20and%20Asier%20Mujika%20and%20Da%20Huang%20and%20Christopher%20A.%20Choquette-Choo%20and%20Mohak%20Patel%20and%20Tianhe%20Yu%20and%20Toni%20Creswell%20and%20%20Jerry%20and%20%20Liu%20and%20Catarina%20Barros%20and%20Yasaman%20Razeghi%20and%20Aurko%20Roy%20and%20Phil%20Culliton%20and%20Binbin%20Xiong%20and%20Jiaqi%20Pan%20and%20Thomas%20Strohmann%20and%20Tolly%20Powell%20and%20Babi%20Seal%20and%20Doug%20DeCarlo%20and%20Pranav%20Shyam%20and%20Kaan%20Katircioglu%20and%20Xuezhi%20Wang%20and%20Cassidy%20Hardin%20and%20Immanuel%20Odisho%20and%20Josef%20Broder%20and%20Oscar%20Chang%20and%20Arun%20Nair%20and%20Artem%20Shtefan%20and%20Maura%20O%27Brien%20and%20Manu%20Agarwal%20and%20Sahitya%20Potluri%20and%20Siddharth%20Goyal%20and%20Amit%20Jhindal%20and%20Saksham%20Thakur%20and%20Yury%20Stuken%20and%20James%20Lyon%20and%20Kristina%20Toutanova%20and%20Fangxiaoyu%20Feng%20and%20Austin%20Wu%20and%20Ben%20Horn%20and%20Alek%20Wang%20and%20Alex%20Cullum%20and%20Gabe%20Taubman%20and%20Disha%20Shrivastava%20and%20Chongyang%20Shi%20and%20Hamish%20Tomlinson%20and%20Roma%20Patel%20and%20Tao%20Tu%20and%20Ada%20Maksutaj%20Oflazer%20and%20Francesco%20Pongetti%20and%20Mingyao%20Yang%20and%20Adrien%20Ali%20Ta%C3%AFga%20and%20Vincent%20Perot%20and%20Nuo%20Wang%20Pierse%20and%20Feng%20Han%20and%20Yoel%20Drori%20and%20I%C3%B1aki%20Iturrate%20and%20Ayan%20Chakrabarti%20and%20Legg%20Yeung%20and%20Dave%20Dopson%20and%20Yi-ting%20Chen%20and%20Apoorv%20Kulshreshtha%20and%20Tongfei%20Guo%20and%20Philip%20Pham%20and%20Tal%20Schuster%20and%20Junquan%20Chen%20and%20Alex%20Polozov%20and%20Jinwei%20Xing%20and%20Huanjie%20Zhou%20and%20Praneeth%20Kacham%20and%20Doron%20Kukliansky%20and%20Antoine%20Miech%20and%20Sergey%20Yaroshenko%20and%20Ed%20Chi%20and%20Sholto%20Douglas%20and%20Hongliang%20Fei%20and%20Mathieu%20Blondel%20and%20Preethi%20Myla%20and%20Lior%20Madmoni%20and%20Xing%20Wu%20and%20Daniel%20Keysers%20and%20Kristian%20Kjems%20and%20Isabela%20Albuquerque%20and%20Lijun%20Yu%20and%20Joel%20D%27sa%20and%20Michelle%20Plantan%20and%20Vlad%20Ionescu%20and%20Jaume%20Sanchez%20Elias%20and%20Abhirut%20Gupta%20and%20Manish%20Reddy%20Vuyyuru%20and%20Fred%20Alcober%20and%20Tong%20Zhou%20and%20Kaiyang%20Ji%20and%20Florian%20Hartmann%20and%20Subha%20Puttagunta%20and%20Hugo%20Song%20and%20Ehsan%20Amid%20and%20Anca%20Stefanoiu%20and%20Andrew%20Lee%20and%20Paul%20Pucciarelli%20and%20Emma%20Wang%20and%20Amit%20Raul%20and%20Slav%20Petrov%20and%20Isaac%20Tian%20and%20Valentin%20Anklin%20and%20Nana%20Nti%20and%20Victor%20Gomes%20and%20Max%20Schumacher%20and%20Grace%20Vesom%20and%20Alex%20Panagopoulos%20and%20Konstantinos%20Bousmalis%20and%20Daniel%20Andor%20and%20Josh%20Jacob%20and%20Yuan%20Zhang%20and%20Bill%20Rosgen%20and%20Matija%20Kecman%20and%20Matthew%20Tung%20and%20Alexandra%20Belias%20and%20Noah%20Goodman%20and%20Paul%20Covington%20and%20Brian%20Wieder%20and%20Nikita%20Saxena%20and%20Elnaz%20Davoodi%20and%20Muhuan%20Huang%20and%20Sharath%20Maddineni%20and%20Vincent%20Roulet%20and%20Folawiyo%20Campbell-Ajala%20and%20Pier%20Giuseppe%20Sessa%20and%20%20Xintian%20and%20%20Wu%20and%20Guangda%20Lai%20and%20Paul%20Collins%20and%20Alex%20Haig%20and%20Vytenis%20Sakenas%20and%20Xiaowei%20Xu%20and%20Marissa%20Giustina%20and%20Laurent%20El%20Shafey%20and%20Pichi%20Charoenpanit%20and%20Shefali%20Garg%20and%20Joshua%20Ainslie%20and%20Boone%20Severson%20and%20Montse%20Gonzalez%20Arenas%20and%20Shreya%20Pathak%20and%20Sujee%20Rajayogam%20and%20Jie%20Feng%20and%20Michiel%20Bakker%20and%20Sheng%20Li%20and%20Nevan%20Wichers%20and%20Jamie%20Rogers%20and%20Xinyang%20Geng%20and%20Yeqing%20Li%20and%20Rolf%20Jagerman%20and%20Chao%20Jia%20and%20Nadav%20Olmert%20and%20David%20Sharon%20and%20Matthew%20Mauger%20and%20Sandeep%20Mariserla%20and%20Hongxu%20Ma%20and%20Megha%20Mohabey%20and%20Kyuyeun%20Kim%20and%20Alek%20Andreev%20and%20Scott%20Pollom%20and%20Juliette%20Love%20and%20Vihan%20Jain%20and%20Priyanka%20Agrawal%20and%20Yannick%20Schroecker%20and%20Alisa%20Fortin%20and%20Manfred%20Warmuth%20and%20Ji%20Liu%20and%20Andrew%20Leach%20and%20Irina%20Blok%20and%20Ganesh%20Poomal%20Girirajan%20and%20Roee%20Aharoni%20and%20Benigno%20Uria%20and%20Andrei%20Sozanschi%20and%20Dan%20Goldberg%20and%20Lucian%20Ionita%20and%20Marco%20Tulio%20Ribeiro%20and%20Martin%20Zlocha%20and%20Vighnesh%20Birodkar%20and%20Sami%20Lachgar%20and%20Liangzhe%20Yuan%20and%20Himadri%20Choudhury%20and%20Matt%20Ginsberg%20and%20Fei%20Zheng%20and%20Gregory%20Dibb%20and%20Emily%20Graves%20and%20Swachhand%20Lokhande%20and%20Gabriel%20Rasskin%20and%20George-Cristian%20Muraru%20and%20Corbin%20Quick%20and%20Sandeep%20Tata%20and%20Pierre%20Sermanet%20and%20Aditya%20Chawla%20and%20Itay%20Karo%20and%20Yan%20Wang%20and%20Susan%20Zhang%20and%20Orgad%20Keller%20and%20Anca%20Dragan%20and%20Guolong%20Su%20and%20Ian%20Chou%20and%20Xi%20Liu%20and%20Yiqing%20Tao%20and%20Shruthi%20Prabhakara%20and%20Marc%20Wilson%20and%20Ruibo%20Liu%20and%20Shibo%20Wang%20and%20Georgie%20Evans%20and%20David%20Du%20and%20Alfonso%20Casta%C3%B1o%20and%20Gautam%20Prasad%20and%20Mona%20El%20Mahdy%20and%20Sebastian%20Gerlach%20and%20Machel%20Reid%20and%20Jarrod%20Kahn%20and%20Amir%20Zait%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Thatcher%20Ulrich%20and%20Guanyu%20Wang%20and%20Jan%20Wassenberg%20and%20Efrat%20Farkash%20and%20Kiran%20Yalasangi%20and%20Congchao%20Wang%20and%20Maria%20Bauza%20and%20Simon%20Bucher%20and%20Ting%20Liu%20and%20Jun%20Yan%20and%20Gary%20Leung%20and%20Vikas%20Sindhwani%20and%20Parker%20Barnes%20and%20Avi%20Singh%20and%20Ivan%20Jurin%20and%20Jichuan%20Chang%20and%20Niket%20Kumar%20Bhumihar%20and%20Sivan%20Eiger%20and%20Gui%20Citovsky%20and%20Ben%20Withbroe%20and%20Zhang%20Li%20and%20Siyang%20Xue%20and%20Niccol%C3%B2%20Dal%20Santo%20and%20Georgi%20Stoyanov%20and%20Yves%20Raimond%20and%20Steven%20Zheng%20and%20Yilin%20Gao%20and%20V%C3%ADt%20List%C3%ADk%20and%20S%C5%82awek%20Kwasiborski%20and%20Rachel%20Saputro%20and%20Adnan%20Ozturel%20and%20Ganesh%20Mallya%20and%20Kushal%20Majmundar%20and%20Ross%20West%20and%20Paul%20Caron%20and%20Jinliang%20Wei%20and%20Lluis%20Castrejon%20and%20Sharad%20Vikram%20and%20Deepak%20Ramachandran%20and%20Nikhil%20Dhawan%20and%20Jiho%20Park%20and%20Sara%20Smoot%20and%20George%20van%20den%20Driessche%20and%20Yochai%20Blau%20and%20Chase%20Malik%20and%20Wei%20Liang%20and%20Roy%20Hirsch%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Eugene%20Weinstein%20and%20A%C3%A4ron%20van%20den%20Oord%20and%20Sid%20Lall%20and%20Nicholas%20FitzGerald%20and%20Zixuan%20Jiang%20and%20Xuan%20Yang%20and%20Dale%20Webster%20and%20Ali%20Elqursh%20and%20Aedan%20Pope%20and%20Georges%20Rotival%20and%20David%20Raposo%20and%20Wanzheng%20Zhu%20and%20Jeff%20Dean%20and%20Sami%20Alabed%20and%20Dustin%20Tran%20and%20Arushi%20Gupta%20and%20Zach%20Gleicher%20and%20Jessica%20Austin%20and%20Edouard%20Rosseel%20and%20Megh%20Umekar%20and%20Dipanjan%20Das%20and%20Yinghao%20Sun%20and%20Kai%20Chen%20and%20Karolis%20Misiunas%20and%20Xiang%20Zhou%20and%20Yixian%20Di%20and%20Alyssa%20Loo%20and%20Josh%20Newlan%20and%20Bo%20Li%20and%20Vinay%20Ramasesh%20and%20Ying%20Xu%20and%20Alex%20Chen%20and%20Sudeep%20Gandhe%20and%20Radu%20Soricut%20and%20Nikita%20Gupta%20and%20Shuguang%20Hu%20and%20Seliem%20El-Sayed%20and%20Xavier%20Garcia%20and%20Idan%20Brusilovsky%20and%20Pu-Chin%20Chen%20and%20Andrew%20Bolt%20and%20Lu%20Huang%20and%20Alex%20Gurney%20and%20Zhiying%20Zhang%20and%20Alexander%20Pritzel%20and%20Jarek%20Wilkiewicz%20and%20Bryan%20Seybold%20and%20Bhargav%20Kanagal%20Shamanna%20and%20Felix%20Fischer%20and%20Josef%20Dean%20and%20Karan%20Gill%20and%20Ross%20Mcilroy%20and%20Abhishek%20Bhowmick%20and%20Jeremy%20Selier%20and%20Antoine%20Yang%20and%20Derek%20Cheng%20and%20Vladimir%20Magay%20and%20Jie%20Tan%20and%20Dhriti%20Varma%20and%20Christian%20Walder%20and%20Tomas%20Kocisky%20and%20Ryo%20Nakashima%20and%20Paul%20Natsev%20and%20Mike%20Kwong%20and%20Ionel%20Gog%20and%20Chiyuan%20Zhang%20and%20Sander%20Dieleman%20and%20Thomas%20Jimma%20and%20Andrey%20Ryabtsev%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Dayou%20Du%20and%20Ante%20%C5%BDu%C5%BEul%20and%20Mislav%20%C5%BDani%C4%87%20and%20Mukund%20Raghavachari%20and%20Willi%20Gierke%20and%20Zeyu%20Zheng%20and%20Dessie%20Petrova%20and%20Yann%20Dauphin%20and%20Yuchuan%20Liu%20and%20Ido%20Kessler%20and%20Steven%20Hand%20and%20Chris%20Duvarney%20and%20Seokhwan%20Kim%20and%20Hyo%20Lee%20and%20L%C3%A9onard%20Hussenot%20and%20Jeffrey%20Hui%20and%20Josh%20Smith%20and%20Deepali%20Jain%20and%20Jiawei%20Xia%20and%20Gaurav%20Singh%20Tomar%20and%20Keyvan%20Amiri%20and%20Du%20Phan%20and%20Fabian%20Fuchs%20and%20Tobias%20Weyand%20and%20Nenad%20Tomasev%20and%20Alexandra%20Cordell%20and%20Xin%20Liu%20and%20Jonathan%20Mallinson%20and%20Pankaj%20Joshi%20and%20Andy%20Crawford%20and%20Arun%20Suggala%20and%20Steve%20Chien%20and%20Nick%20Fernando%20and%20Mariella%20Sanchez-Vargas%20and%20Duncan%20Williams%20and%20Phil%20Crone%20and%20Xiyang%20Luo%20and%20Igor%20Karpov%20and%20Jyn%20Shan%20and%20Terry%20Thurk%20and%20Robin%20Strudel%20and%20Paul%20Voigtlaender%20and%20Piyush%20Patil%20and%20Tim%20Dozat%20and%20Ali%20Khodaei%20and%20Sahil%20Singla%20and%20Piotr%20Ambroszczyk%20and%20Qiyin%20Wu%20and%20Yifan%20Chang%20and%20Brian%20Roark%20and%20Chaitra%20Hegde%20and%20Tianli%20Ding%20and%20Angelos%20Filos%20and%20Zhongru%20Wu%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Shuang%20Liu%20and%20Saarthak%20Khanna%20and%20Aditya%20Pandey%20and%20Siobhan%20Mcloughlin%20and%20Qiujia%20Li%20and%20Sam%20Haves%20and%20Allan%20Zhou%20and%20Elena%20Buchatskaya%20and%20Isabel%20Leal%20and%20Peter%20de%20Boursac%20and%20Nami%20Akazawa%20and%20Nina%20Anderson%20and%20Terry%20Chen%20and%20Krishna%20Somandepalli%20and%20Chen%20Liang%20and%20Sheela%20Goenka%20and%20Stephanie%20Winkler%20and%20Alexander%20Grushetsky%20and%20Yifan%20Ding%20and%20Jamie%20Smith%20and%20Fan%20Ye%20and%20Jordi%20Pont-Tuset%20and%20Eric%20Li%20and%20Ruichao%20Li%20and%20Tomer%20Golany%20and%20Dawid%20Wegner%20and%20Tao%20Jiang%20and%20Omer%20Barak%20and%20Yuan%20Shangguan%20and%20Eszter%20V%C3%A9rtes%20and%20Renee%20Wong%20and%20J%C3%B6rg%20Bornschein%20and%20Alex%20Tudor%20and%20Michele%20Bevilacqua%20and%20Tom%20Schaul%20and%20Ankit%20Singh%20Rawat%20and%20Yang%20Zhao%20and%20Kyriakos%20Axiotis%20and%20Lei%20Meng%20and%20Cory%20McLean%20and%20Jonathan%20Lai%20and%20Jennifer%20Beattie%20and%20Nate%20Kushman%20and%20Yaxin%20Liu%20and%20Blair%20Kutzman%20and%20Fiona%20Lang%20and%20Jingchen%20Ye%20and%20Praneeth%20Netrapalli%20and%20Pushkar%20Mishra%20and%20Myriam%20Khan%20and%20Megha%20Goel%20and%20Rob%20Willoughby%20and%20David%20Tian%20and%20Honglei%20Zhuang%20and%20JD%20Chen%20and%20Zak%20Tsai%20and%20Tasos%20Kementsietsidis%20and%20Arjun%20Khare%20and%20James%20Keeling%20and%20Keyang%20Xu%20and%20Nathan%20Waters%20and%20Florent%20Altch%C3%A9%20and%20Ashok%20Popat%20and%20Bhavishya%20Mittal%20and%20David%20Saxton%20and%20Dalia%20El%20Badawy%20and%20Michael%20Mathieu%20and%20Zheng%20Zheng%20and%20Hao%20Zhou%20and%20Nishant%20Ranka%20and%20Richard%20Shin%20and%20Qingnan%20Duan%20and%20Tim%20Salimans%20and%20Ioana%20Mihailescu%20and%20Uri%20Shaham%20and%20Ming-Wei%20Chang%20and%20Yannis%20Assael%20and%20Nishanth%20Dikkala%20and%20Martin%20Izzard%20and%20Vincent%20Cohen-Addad%20and%20Cat%20Graves%20and%20Vlad%20Feinberg%20and%20Grace%20Chung%20and%20DJ%20Strouse%20and%20Danny%20Karmon%20and%20Sahand%20Sharifzadeh%20and%20Zoe%20Ashwood%20and%20Khiem%20Pham%20and%20Jon%20Blanton%20and%20Alex%20Vasiloff%20and%20Jarred%20Barber%20and%20Mark%20Geller%20and%20Aurick%20Zhou%20and%20Fedir%20Zubach%20and%20Tzu-Kuo%20Huang%20and%20Lei%20Zhang%20and%20Himanshu%20Gupta%20and%20Matt%20Young%20and%20Julia%20Proskurnia%20and%20Ronny%20Votel%20and%20Valentin%20Gabeur%20and%20Gabriel%20Barcik%20and%20Aditya%20Tripathi%20and%20Hongkun%20Yu%20and%20Geng%20Yan%20and%20Beer%20Changpinyo%20and%20Filip%20Paveti%C4%87%20and%20Amy%20Coyle%20and%20Yasuhisa%20Fujii%20and%20Jorge%20Gonzalez%20Mendez%20and%20Tianhao%20Zhou%20and%20Harish%20Rajamani%20and%20Blake%20Hechtman%20and%20Eddie%20Cao%20and%20Da-Cheng%20Juan%20and%20Yi-Xuan%20Tan%20and%20Valentin%20Dalibard%20and%20Yilun%20Du%20and%20Natalie%20Clay%20and%20Kaisheng%20Yao%20and%20Wenhao%20Jia%20and%20Dimple%20Vijaykumar%20and%20Yuxiang%20Zhou%20and%20Xinyi%20Bai%20and%20Wei-Chih%20Hung%20and%20Steven%20Pecht%20and%20Georgi%20Todorov%20and%20Nikhil%20Khadke%20and%20Pramod%20Gupta%20and%20Preethi%20Lahoti%20and%20Arnaud%20Autef%20and%20Karthik%20Duddu%20and%20James%20Lee-Thorp%20and%20Alexander%20Bykovsky%20and%20Tautvydas%20Misiunas%20and%20Sebastian%20Flennerhag%20and%20Santhosh%20Thangaraj%20and%20Jed%20McGiffin%20and%20Zack%20Nado%20and%20Markus%20Kunesch%20and%20Andreas%20Noever%20and%20Amir%20Hertz%20and%20Marco%20Liang%20and%20Victor%20Stone%20and%20Evan%20Palmer%20and%20Samira%20Daruki%20and%20Arijit%20Pramanik%20and%20Siim%20P%C3%B5der%20and%20Austin%20Kyker%20and%20Mina%20Khan%20and%20Evgeny%20Sluzhaev%20and%20Marvin%20Ritter%20and%20Avraham%20Ruderman%20and%20Wenlei%20Zhou%20and%20Chirag%20Nagpal%20and%20Kiran%20Vodrahalli%20and%20George%20Necula%20and%20Paul%20Barham%20and%20Ellie%20Pavlick%20and%20Jay%20Hartford%20and%20Izhak%20Shafran%20and%20Long%20Zhao%20and%20Maciej%20Miku%C5%82a%20and%20Tom%20Eccles%20and%20Hidetoshi%20Shimokawa%20and%20Kanav%20Garg%20and%20Luke%20Vilnis%20and%20Hanwen%20Chen%20and%20Ilia%20Shumailov%20and%20Kuang-Huei%20Lee%20and%20Abdelrahman%20Abdelhamed%20and%20Meiyan%20Xie%20and%20Vered%20Cohen%20and%20Ester%20Hlavnova%20and%20Dan%20Malkin%20and%20Chawin%20Sitawarin%20and%20James%20Lottes%20and%20Pauline%20Coquinot%20and%20Tianli%20Yu%20and%20Sandeep%20Kumar%20and%20Jingwei%20Zhang%20and%20Aroma%20Mahendru%20and%20Zafarali%20Ahmed%20and%20James%20Martens%20and%20Tao%20Chen%20and%20Aviel%20Boag%20and%20Daiyi%20Peng%20and%20Coline%20Devin%20and%20Arseniy%20Klimovskiy%20and%20Mary%20Phuong%20and%20Danny%20Vainstein%20and%20Jin%20Xie%20and%20Bhuvana%20Ramabhadran%20and%20Nathan%20Howard%20and%20Xinxin%20Yu%20and%20Gitartha%20Goswami%20and%20Jingyu%20Cui%20and%20Sam%20Shleifer%20and%20Mario%20Pinto%20and%20Chih-Kuan%20Yeh%20and%20Ming-Hsuan%20Yang%20and%20Sara%20Javanmardi%20and%20Dan%20Ethier%20and%20Chace%20Lee%20and%20Jordi%20Orbay%20and%20Suyog%20Kotecha%20and%20Carla%20Bromberg%20and%20Pete%20Shaw%20and%20James%20Thornton%20and%20Adi%20Gerzi%20Rosenthal%20and%20Shane%20Gu%20and%20Matt%20Thomas%20and%20Ian%20Gemp%20and%20Aditya%20Ayyar%20and%20Asahi%20Ushio%20and%20Aarush%20Selvan%20and%20Joel%20Wee%20and%20Chenxi%20Liu%20and%20Maryam%20Majzoubi%20and%20Weiren%20Yu%20and%20Jake%20Abernethy%20and%20Tyler%20Liechty%20and%20Renke%20Pan%20and%20Hoang%20Nguyen%20and%20%20Qiong%20and%20%20Hu%20and%20Sarah%20Perrin%20and%20Abhinav%20Arora%20and%20Emily%20Pitler%20and%20Weiyi%20Wang%20and%20Kaushik%20Shivakumar%20and%20Flavien%20Prost%20and%20Ben%20Limonchik%20and%20Jing%20Wang%20and%20Yi%20Gao%20and%20Timothee%20Cour%20and%20Shyamal%20Buch%20and%20Huan%20Gui%20and%20Maria%20Ivanova%20and%20Philipp%20Neubeck%20and%20Kelvin%20Chan%20and%20Lucy%20Kim%20and%20Huizhong%20Chen%20and%20Naman%20Goyal%20and%20Da-Woon%20Chung%20and%20Lu%20Liu%20and%20Yao%20Su%20and%20Anastasia%20Petrushkina%20and%20Jiajun%20Shen%20and%20Armand%20Joulin%20and%20Yuanzhong%20Xu%20and%20Stein%20Xudong%20Lin%20and%20Yana%20Kulizhskaya%20and%20Ciprian%20Chelba%20and%20Shobha%20Vasudevan%20and%20Eli%20Collins%20and%20Vasilisa%20Bashlovkina%20and%20Tony%20Lu%20and%20Doug%20Fritz%20and%20Jongbin%20Park%20and%20Yanqi%20Zhou%20and%20Chen%20Su%20and%20Richard%20Tanburn%20and%20Mikhail%20Sushkov%20and%20Mitchelle%20Rasquinha%20and%20Jinning%20Li%20and%20Jennifer%20Prendki%20and%20Yiming%20Li%20and%20Pallavi%20LV%20and%20Shriya%20Sharma%20and%20Hen%20Fitoussi%20and%20Hui%20Huang%20and%20Andrew%20Dai%20and%20Phuong%20Dao%20and%20Mike%20Burrows%20and%20Henry%20Prior%20and%20Danfeng%20Qin%20and%20Golan%20Pundak%20and%20Lars%20Lowe%20Sjoesund%20and%20Art%20Khurshudov%20and%20Zhenkai%20Zhu%20and%20Albert%20Webson%20and%20Elizabeth%20Kemp%20and%20Tat%20Tan%20and%20Saurabh%20Agrawal%20and%20Susie%20Sargsyan%20and%20Liqun%20Cheng%20and%20Jim%20Stephan%20and%20Tom%20Kwiatkowski%20and%20David%20Reid%20and%20Arunkumar%20Byravan%20and%20Assaf%20Hurwitz%20Michaely%20and%20Nicolas%20Heess%20and%20Luowei%20Zhou%20and%20Sonam%20Goenka%20and%20Viral%20Carpenter%20and%20Anselm%20Levskaya%20and%20Bo%20Wang%20and%20Reed%20Roberts%20and%20R%C3%A9mi%20Leblond%20and%20Sharat%20Chikkerur%20and%20Stav%20Ginzburg%20and%20Max%20Chang%20and%20Robert%20Riachi%20and%20%20Chuqiao%20and%20%20Xu%20and%20Zal%C3%A1n%20Borsos%20and%20Michael%20Pliskin%20and%20Julia%20Pawar%20and%20Morgane%20Lustman%20and%20Hannah%20Kirkwood%20and%20Ankit%20Anand%20and%20Aditi%20Chaudhary%20and%20Norbert%20Kalb%20and%20Kieran%20Milan%20and%20Sean%20Augenstein%20and%20Anna%20Goldie%20and%20Laurel%20Prince%20and%20Karthik%20Raman%20and%20Yanhua%20Sun%20and%20Vivian%20Xia%20and%20Aaron%20Cohen%20and%20Zhouyuan%20Huo%20and%20Josh%20Camp%20and%20Seher%20Ellis%20and%20Lukas%20Zilka%20and%20David%20Vilar%20Torres%20and%20Lisa%20Patel%20and%20Sho%20Arora%20and%20Betty%20Chan%20and%20Jonas%20Adler%20and%20Kareem%20Ayoub%20and%20Jacky%20Liang%20and%20Fayaz%20Jamil%20and%20Jiepu%20Jiang%20and%20Simon%20Baumgartner%20and%20Haitian%20Sun%20and%20Yael%20Karov%20and%20Yaroslav%20Akulov%20and%20Hui%20Zheng%20and%20Irene%20Cai%20and%20Claudio%20Fantacci%20and%20James%20Rubin%20and%20Alex%20Rav%20Acha%20and%20Mengchao%20Wang%20and%20Nina%20D%27Souza%20and%20Rohit%20Sathyanarayana%20and%20Shengyang%20Dai%20and%20Simon%20Rowe%20and%20Andrey%20Simanovsky%20and%20Omer%20Goldman%20and%20Yuheng%20Kuang%20and%20Xiaoyue%20Pan%20and%20Andrew%20Rosenberg%20and%20Tania%20Rojas-Esponda%20and%20Praneet%20Dutta%20and%20Amy%20Zeng%20and%20Irina%20Jurenka%20and%20Greg%20Farquhar%20and%20Yamini%20Bansal%20and%20Shariq%20Iqbal%20and%20Becca%20Roelofs%20and%20Ga-Young%20Joung%20and%20Parker%20Beak%20and%20Changwan%20Ryu%20and%20Ryan%20Poplin%20and%20Yan%20Wu%20and%20Jean-Baptiste%20Alayrac%20and%20Senaka%20Buthpitiya%20and%20Olaf%20Ronneberger%20and%20Caleb%20Habtegebriel%20and%20Wei%20Li%20and%20Paul%20Cavallaro%20and%20Aurora%20Wei%20and%20Guy%20Bensky%20and%20Timo%20Denk%20and%20Harish%20Ganapathy%20and%20Jeff%20Stanway%20and%20Pratik%20Joshi%20and%20Francesco%20Bertolini%20and%20Jessica%20Lo%20and%20Olivia%20Ma%20and%20Zachary%20Charles%20and%20Geta%20Sampemane%20and%20Himanshu%20Sahni%20and%20Xu%20Chen%20and%20Harry%20Askham%20and%20David%20Gaddy%20and%20Peter%20Young%20and%20Jiewen%20Tan%20and%20Matan%20Eyal%20and%20Arthur%20Bra%C5%BEinskas%20and%20Li%20Zhong%20and%20Zhichun%20Wu%20and%20Mark%20Epstein%20and%20Kai%20Bailey%20and%20Andrew%20Hard%20and%20Kamyu%20Lee%20and%20Sasha%20Goldshtein%20and%20Alex%20Ruiz%20and%20Mohammed%20Badawi%20and%20Matthias%20Lochbrunner%20and%20JK%20Kearns%20and%20Ashley%20Brown%20and%20Fabio%20Pardo%20and%20Theophane%20Weber%20and%20Haichuan%20Yang%20and%20Pan-Pan%20Jiang%20and%20Berkin%20Akin%20and%20Zhao%20Fu%20and%20Marcus%20Wainwright%20and%20Chi%20Zou%20and%20Meenu%20Gaba%20and%20Pierre-Antoine%20Manzagol%20and%20Wendy%20Kan%20and%20Yang%20Song%20and%20Karina%20Zainullina%20and%20Rui%20Lin%20and%20Jeongwoo%20Ko%20and%20Salil%20Deshmukh%20and%20Apoorv%20Jindal%20and%20James%20Svensson%20and%20Divya%20Tyam%20and%20Heri%20Zhao%20and%20Christine%20Kaeser-Chen%20and%20Scott%20Baird%20and%20Pooya%20Moradi%20and%20Jamie%20Hall%20and%20Qiuchen%20Guo%20and%20Vincent%20Tsang%20and%20Bowen%20Liang%20and%20Fernando%20Pereira%20and%20Suhas%20Ganesh%20and%20Ivan%20Korotkov%20and%20Jakub%20Adamek%20and%20Sridhar%20Thiagarajan%20and%20Vinh%20Tran%20and%20Charles%20Chen%20and%20Chris%20Tar%20and%20Sanil%20Jain%20and%20Ishita%20Dasgupta%20and%20Taylan%20Bilal%20and%20David%20Reitter%20and%20Kai%20Zhao%20and%20Giulia%20Vezzani%20and%20Yasmin%20Gehman%20and%20Pulkit%20Mehta%20and%20Lauren%20Beltrone%20and%20Xerxes%20Dotiwalla%20and%20Sergio%20Guadarrama%20and%20Zaheer%20Abbas%20and%20Stefani%20Karp%20and%20Petko%20Georgiev%20and%20Chun-Sung%20Ferng%20and%20Marc%20Brockschmidt%20and%20Liqian%20Peng%20and%20Christoph%20Hirnschall%20and%20Vikas%20Verma%20and%20Yingying%20Bi%20and%20Ying%20Xiao%20and%20Avigail%20Dabush%20and%20Kelvin%20Xu%20and%20Phil%20Wallis%20and%20Randall%20Parker%20and%20Qifei%20Wang%20and%20Yang%20Xu%20and%20Ilkin%20Safarli%20and%20Dinesh%20Tewari%20and%20Yin%20Zhang%20and%20Seungyeon%20Kim%20and%20Andrea%20Gesmundo%20and%20Mackenzie%20Thomas%20and%20Sergey%20Levi%20and%20Ahmed%20Chowdhury%20and%20Kanishka%20Rao%20and%20Peter%20Garst%20and%20Sam%20Conway-Rahman%20and%20Helen%20Ran%20and%20Kay%20McKinney%20and%20Zhisheng%20Xiao%20and%20Wenhao%20Yu%20and%20Rohan%20Agrawal%20and%20Axel%20Stjerngren%20and%20Catalin%20Ionescu%20and%20Jingjing%20Chen%20and%20Vivek%20Sharma%20and%20Justin%20Chiu%20and%20Fei%20Liu%20and%20Ken%20Franko%20and%20Clayton%20Sanford%20and%20Xingyu%20Cai%20and%20Paul%20Michel%20and%20Sanjay%20Ganapathy%20and%20Jane%20Labanowski%20and%20Zachary%20Garrett%20and%20Ben%20Vargas%20and%20Sean%20Sun%20and%20Bryan%20Gale%20and%20Thomas%20Buschmann%20and%20Guillaume%20Desjardins%20and%20Nimesh%20Ghelani%20and%20Palak%20Jain%20and%20Mudit%20Verma%20and%20Chulayuth%20Asawaroengchai%20and%20Julian%20Eisenschlos%20and%20Jitendra%20Harlalka%20and%20Hideto%20Kazawa%20and%20Don%20Metzler%20and%20Joshua%20Howland%20and%20Ying%20Jian%20and%20Jake%20Ades%20and%20Viral%20Shah%20and%20Tynan%20Gangwani%20and%20Seungji%20Lee%20and%20Roman%20Ring%20and%20Steven%20M.%20Hernandez%20and%20Dean%20Reich%20and%20Amer%20Sinha%20and%20Ashutosh%20Sathe%20and%20Joe%20Kovac%20and%20Ashleah%20Gill%20and%20Ajay%20Kannan%20and%20Andrea%20D%27olimpio%20and%20Martin%20Sevenich%20and%20Jay%20Whang%20and%20Been%20Kim%20and%20Khe%20Chai%20Sim%20and%20Jilin%20Chen%20and%20Jiageng%20Zhang%20and%20Shuba%20Lall%20and%20Yossi%20Matias%20and%20Bill%20Jia%20and%20Abe%20Friesen%20and%20Sara%20Nasso%20and%20Ashish%20Thapliyal%20and%20Bryan%20Perozzi%20and%20Ting%20Yu%20and%20Anna%20Shekhawat%20and%20Safeen%20Huda%20and%20Peter%20Grabowski%20and%20Eric%20Wang%20and%20Ashwin%20Sreevatsa%20and%20Hilal%20Dib%20and%20Mehadi%20Hassen%20and%20Parker%20Schuh%20and%20Vedrana%20Milutinovic%20and%20Chris%20Welty%20and%20Michael%20Quinn%20and%20Ali%20Shah%20and%20Bangju%20Wang%20and%20Gabe%20Barth-Maron%20and%20Justin%20Frye%20and%20Natalie%20Axelsson%20and%20Tao%20Zhu%20and%20Yukun%20Ma%20and%20Irene%20Giannoumis%20and%20Hanie%20Sedghi%20and%20Chang%20Ye%20and%20Yi%20Luan%20and%20Kevin%20Aydin%20and%20Bilva%20Chandra%20and%20Vivek%20Sampathkumar%20and%20Ronny%20Huang%20and%20Victor%20Lavrenko%20and%20Ahmed%20Eleryan%20and%20Zhi%20Hong%20and%20Steven%20Hansen%20and%20Sara%20Mc%20Carthy%20and%20Bidisha%20Samanta%20and%20Domagoj%20%C4%86evid%20and%20Xin%20Wang%20and%20Fangtao%20Li%20and%20Michael%20Voznesensky%20and%20Matt%20Hoffman%20and%20Andreas%20Terzis%20and%20Vikash%20Sehwag%20and%20Gil%20Fidel%20and%20Luheng%20He%20and%20Mu%20Cai%20and%20Yanzhang%20He%20and%20Alex%20Feng%20and%20Martin%20Nikoltchev%20and%20Samrat%20Phatale%20and%20Jason%20Chase%20and%20Rory%20Lawton%20and%20Ming%20Zhang%20and%20Tom%20Ouyang%20and%20Manuel%20Tragut%20and%20Mehdi%20Hafezi%20Manshadi%20and%20Arjun%20Narayanan%20and%20Jiaming%20Shen%20and%20Xu%20Gao%20and%20Tolga%20Bolukbasi%20and%20Nick%20Roy%20and%20Xin%20Li%20and%20Daniel%20Golovin%20and%20Liviu%20Panait%20and%20Zhen%20Qin%20and%20Guangxing%20Han%20and%20Thomas%20Anthony%20and%20Sneha%20Kudugunta%20and%20Viorica%20Patraucean%20and%20Aniket%20Ray%20and%20Xinyun%20Chen%20and%20Xiaochen%20Yang%20and%20Tanuj%20Bhatia%20and%20Pranav%20Talluri%20and%20Alex%20Morris%20and%20Andrija%20Ra%C5%BEnatovi%C4%87%20and%20Bethanie%20Brownfield%20and%20James%20An%20and%20Sheng%20Peng%20and%20Patrick%20Kane%20and%20Ce%20Zheng%20and%20Nico%20Duduta%20and%20Joshua%20Kessinger%20and%20James%20Noraky%20and%20Siqi%20Liu%20and%20Keran%20Rong%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Keith%20Rush%20and%20Alex%20Goldin%20and%20Fanny%20Wei%20and%20Shiva%20Mohan%20Reddy%20Garlapati%20and%20Caroline%20Pantofaru%20and%20Okwan%20Kwon%20and%20Jianmo%20Ni%20and%20Eric%20Noland%20and%20Julia%20Di%20Trapani%20and%20Fran%C3%A7oise%20Beaufays%20and%20Abhijit%20Guha%20Roy%20and%20Yinlam%20Chow%20and%20Aybuke%20Turker%20and%20Geoffrey%20Cideron%20and%20Lantao%20Mei%20and%20Jon%20Clark%20and%20Qingyun%20Dou%20and%20Matko%20Bo%C5%A1njak%20and%20Ralph%20Leith%20and%20Yuqing%20Du%20and%20Amir%20Yazdanbakhsh%20and%20Milad%20Nasr%20and%20Chester%20Kwak%20and%20Suraj%20Satishkumar%20Sheth%20and%20Alex%20Kaskasoli%20and%20Ankesh%20Anand%20and%20Balaji%20Lakshminarayanan%20and%20Sammy%20Jerome%20and%20David%20Bieber%20and%20Chun-Te%20Chu%20and%20Alexandre%20Senges%20and%20Tianxiao%20Shen%20and%20Mukund%20Sridhar%20and%20Ndaba%20Ndebele%20and%20Benjamin%20Beyret%20and%20Shakir%20Mohamed%20and%20Mia%20Chen%20and%20Markus%20Freitag%20and%20Jiaxian%20Guo%20and%20Luyang%20Liu%20and%20Paul%20Roit%20and%20Heng%20Chen%20and%20Shen%20Yan%20and%20Tom%20Stone%20and%20JD%20Co-Reyes%20and%20Jeremy%20Cole%20and%20Salvatore%20Scellato%20and%20Shekoofeh%20Azizi%20and%20Hadi%20Hashemi%20and%20Alicia%20Jin%20and%20Anand%20Iyer%20and%20Marcella%20Valentine%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Arun%20Ahuja%20and%20Daniel%20Hernandez%20Diaz%20and%20Chen-Yu%20Lee%20and%20Nathan%20Clement%20and%20Weize%20Kong%20and%20Drew%20Garmon%20and%20Ishaan%20Watts%20and%20Kush%20Bhatia%20and%20Khyatti%20Gupta%20and%20Matt%20Miecnikowski%20and%20Hugo%20Vallet%20and%20Ankur%20Taly%20and%20Edward%20Loper%20and%20Saket%20Joshi%20and%20James%20Atwood%20and%20Jo%20Chick%20and%20Mark%20Collier%20and%20Fotis%20Iliopoulos%20and%20Ryan%20Trostle%20and%20Beliz%20Gunel%20and%20Ramiro%20Leal-Cavazos%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Michael%20Guzman%20and%20Xiaoen%20Ju%20and%20Andy%20Forbes%20and%20Jesse%20Emond%20and%20Kushal%20Chauhan%20and%20Ben%20Caine%20and%20Li%20Xiao%20and%20Wenjun%20Zeng%20and%20Alexandre%20Moufarek%20and%20Daniel%20Murphy%20and%20Maya%20Meng%20and%20Nitish%20Gupta%20and%20Felix%20Riedel%20and%20Anil%20Das%20and%20Elijah%20Lawal%20and%20Shashi%20Narayan%20and%20Tiberiu%20Sosea%20and%20James%20Swirhun%20and%20Linda%20Friso%20and%20Behnam%20Neyshabur%20and%20Jing%20Lu%20and%20Sertan%20Girgin%20and%20Michael%20Wunder%20and%20Edouard%20Yvinec%20and%20Aroonalok%20Pyne%20and%20Victor%20Carbune%20and%20Shruti%20Rijhwani%20and%20Yang%20Guo%20and%20Tulsee%20Doshi%20and%20Anton%20Briukhov%20and%20Max%20Bain%20and%20Ayal%20Hitron%20and%20Xuanhui%20Wang%20and%20Ashish%20Gupta%20and%20Ke%20Chen%20and%20Cosmo%20Du%20and%20Weiyang%20Zhang%20and%20Dhruv%20Shah%20and%20Arjun%20Akula%20and%20Max%20Dylla%20and%20Ashyana%20Kachra%20and%20Weicheng%20Kuo%20and%20Tingting%20Zou%20and%20Lily%20Wang%20and%20Luyao%20Xu%20and%20Jifan%20Zhu%20and%20Justin%20Snyder%20and%20Sachit%20Menon%20and%20Orhan%20Firat%20and%20Igor%20Mordatch%20and%20Yuan%20Yuan%20and%20Natalia%20Ponomareva%20and%20Rory%20Blevins%20and%20Lawrence%20Moore%20and%20Weijun%20Wang%20and%20Phil%20Chen%20and%20Martin%20Scholz%20and%20Artur%20Dwornik%20and%20Jason%20Lin%20and%20Sicheng%20Li%20and%20Diego%20Antognini%20and%20Te%20I%20and%20Xiaodan%20Song%20and%20Matt%20Miller%20and%20Uday%20Kalra%20and%20Adam%20Raveret%20and%20Oscar%20Akerlund%20and%20Felix%20Wu%20and%20Andrew%20Nystrom%20and%20Namrata%20Godbole%20and%20Tianqi%20Liu%20and%20Hannah%20DeBalsi%20and%20Jewel%20Zhao%20and%20Buhuang%20Liu%20and%20Avi%20Caciularu%20and%20Lauren%20Lax%20and%20Urvashi%20Khandelwal%20and%20Victoria%20Langston%20and%20Eric%20Bailey%20and%20Silvio%20Lattanzi%20and%20Yufei%20Wang%20and%20Neel%20Kovelamudi%20and%20Sneha%20Mondal%20and%20Guru%20Guruganesh%20and%20Nan%20Hua%20and%20Ofir%20Roval%20and%20Pawe%C5%82%20Weso%C5%82owski%20and%20Rishikesh%20Ingale%20and%20Jonathan%20Halcrow%20and%20Tim%20Sohn%20and%20Christof%20Angermueller%20and%20Bahram%20Raad%20and%20Eli%20Stickgold%20and%20Eva%20Lu%20and%20Alec%20Kosik%20and%20Jing%20Xie%20and%20Timothy%20Lillicrap%20and%20Austin%20Huang%20and%20Lydia%20Lihui%20Zhang%20and%20Dominik%20Paulus%20and%20Clement%20Farabet%20and%20Alex%20Wertheim%20and%20Bing%20Wang%20and%20Rishabh%20Joshi%20and%20Chu-ling%20Ko%20and%20Yonghui%20Wu%20and%20Shubham%20Agrawal%20and%20Lily%20Lin%20and%20XiangHai%20Sheng%20and%20Peter%20Sung%20and%20Tyler%20Breland-King%20and%20Christina%20Butterfield%20and%20Swapnil%20Gawde%20and%20Sumeet%20Singh%20and%20Qiao%20Zhang%20and%20Raj%20Apte%20and%20Shilpa%20Shetty%20and%20Adrian%20Hutter%20and%20Tao%20Li%20and%20Elizabeth%20Salesky%20and%20Federico%20Lebron%20and%20Jonni%20Kanerva%20and%20Michela%20Paganini%20and%20Arthur%20Nguyen%20and%20Rohith%20Vallu%20and%20Jan-Thorsten%20Peter%20and%20Sarmishta%20Velury%20and%20David%20Kao%20and%20Jay%20Hoover%20and%20Anna%20Bortsova%20and%20Colton%20Bishop%20and%20Shoshana%20Jakobovits%20and%20Alessandro%20Agostini%20and%20Alekh%20Agarwal%20and%20Chang%20Liu%20and%20Charles%20Kwong%20and%20Sasan%20Tavakkol%20and%20Ioana%20Bica%20and%20Alex%20Greve%20and%20Anirudh%20GP%20and%20Jake%20Marcus%20and%20Le%20Hou%20and%20Tom%20Duerig%20and%20Rivka%20Moroshko%20and%20Dave%20Lacey%20and%20Andy%20Davis%20and%20Julien%20Amelot%20and%20Guohui%20Wang%20and%20Frank%20Kim%20and%20Theofilos%20Strinopoulos%20and%20Hui%20Wan%20and%20Charline%20Le%20Lan%20and%20Shankar%20Krishnan%20and%20Haotian%20Tang%20and%20Peter%20Humphreys%20and%20Junwen%20Bai%20and%20Idan%20Heimlich%20Shtacher%20and%20Diego%20Machado%20and%20Chenxi%20Pang%20and%20Ken%20Burke%20and%20Dangyi%20Liu%20and%20Renga%20Aravamudhan%20and%20Yue%20Song%20and%20Ed%20Hirst%20and%20Abhimanyu%20Singh%20and%20Brendan%20Jou%20and%20Liang%20Bai%20and%20Francesco%20Piccinno%20and%20Chuyuan%20Kelly%20Fu%20and%20Robin%20Alazard%20and%20Barak%20Meiri%20and%20Daniel%20Winter%20and%20Charlie%20Chen%20and%20Mingda%20Zhang%20and%20Jens%20Heitkaemper%20and%20John%20Lambert%20and%20Jinhyuk%20Lee%20and%20Alexander%20Fr%C3%B6mmgen%20and%20Sergey%20Rogulenko%20and%20Pranav%20Nair%20and%20Paul%20Niemczyk%20and%20Anton%20Bulyenov%20and%20Bibo%20Xu%20and%20Hadar%20Shemtov%20and%20Morteza%20Zadimoghaddam%20and%20Serge%20Toropov%20and%20Mateo%20Wirth%20and%20Hanjun%20Dai%20and%20Sreenivas%20Gollapudi%20and%20Daniel%20Zheng%20and%20Alex%20Kurakin%20and%20Chansoo%20Lee%20and%20Kalesha%20Bullard%20and%20Nicolas%20Serrano%20and%20Ivana%20Balazevic%20and%20Yang%20Li%20and%20Johan%20Schalkwyk%20and%20Mark%20Murphy%20and%20Mingyang%20Zhang%20and%20Kevin%20Sequeira%20and%20Romina%20Datta%20and%20Nishant%20Agrawal%20and%20Charles%20Sutton%20and%20Nithya%20Attaluri%20and%20Mencher%20Chiang%20and%20Wael%20Farhan%20and%20Gregory%20Thornton%20and%20Kate%20Lin%20and%20Travis%20Choma%20and%20Hung%20Nguyen%20and%20Kingshuk%20Dasgupta%20and%20Dirk%20Robinson%20and%20Iulia%20Com%C5%9Fa%20and%20Michael%20Riley%20and%20Arjun%20Pillai%20and%20Basil%20Mustafa%20and%20Ben%20Golan%20and%20Amir%20Zandieh%20and%20Jean-Baptiste%20Lespiau%20and%20Billy%20Porter%20and%20David%20Ross%20and%20Sujeevan%20Rajayogam%20and%20Mohit%20Agarwal%20and%20Subhashini%20Venugopalan%20and%20Bobak%20Shahriari%20and%20Qiqi%20Yan%20and%20Hao%20Xu%20and%20Taylor%20Tobin%20and%20Pavel%20Dubov%20and%20Hongzhi%20Shi%20and%20Adri%C3%A0%20Recasens%20and%20Anton%20Kovsharov%20and%20Sebastian%20Borgeaud%20and%20Lucio%20Dery%20and%20Shanthal%20Vasanth%20and%20Elena%20Gribovskaya%20and%20Linhai%20Qiu%20and%20Mahdis%20Mahdieh%20and%20Wojtek%20Skut%20and%20Elizabeth%20Nielsen%20and%20CJ%20Zheng%20and%20Adams%20Yu%20and%20Carrie%20Grimes%20Bostock%20and%20Shaleen%20Gupta%20and%20Aaron%20Archer%20and%20Chris%20Rawles%20and%20Elinor%20Davies%20and%20Alexey%20Svyatkovskiy%20and%20Tomy%20Tsai%20and%20Yoni%20Halpern%20and%20Christian%20Reisswig%20and%20Bartek%20Wydrowski%20and%20Bo%20Chang%20and%20Joan%20Puigcerver%20and%20Mor%20Hazan%20Taege%20and%20Jian%20Li%20and%20Eva%20Schnider%20and%20Xinjian%20Li%20and%20Dragos%20Dena%20and%20Yunhan%20Xu%20and%20Umesh%20Telang%20and%20Tianze%20Shi%20and%20Heiga%20Zen%20and%20Kyle%20Kastner%20and%20Yeongil%20Ko%20and%20Neesha%20Subramaniam%20and%20Aviral%20Kumar%20and%20Pete%20Blois%20and%20Zhuyun%20Dai%20and%20John%20Wieting%20and%20Yifeng%20Lu%20and%20Yoel%20Zeldes%20and%20Tian%20Xie%20and%20Anja%20Hauth%20and%20Alexandru%20%C5%A2ifrea%20and%20Yuqi%20Li%20and%20Sam%20El-Husseini%20and%20Dan%20Abolafia%20and%20Howard%20Zhou%20and%20Wen%20Ding%20and%20Sahra%20Ghalebikesabi%20and%20Carlos%20Gu%C3%ADa%20and%20Andrii%20Maksai%20and%20%C3%81goston%20Weisz%20and%20Sercan%20Arik%20and%20Nick%20Sukhanov%20and%20Aga%20%C5%9Awietlik%20and%20Xuhui%20Jia%20and%20Luo%20Yu%20and%20Weiyue%20Wang%20and%20Mark%20Brand%20and%20Dawn%20Bloxwich%20and%20Sean%20Kirmani%20and%20Zhe%20Chen%20and%20Alec%20Go%20and%20Pablo%20Sprechmann%20and%20Nithish%20Kannen%20and%20Alen%20Carin%20and%20Paramjit%20Sandhu%20and%20Isabel%20Edkins%20and%20Leslie%20Nooteboom%20and%20Jai%20Gupta%20and%20Loren%20Maggiore%20and%20Javad%20Azizi%20and%20Yael%20Pritch%20and%20Pengcheng%20Yin%20and%20Mansi%20Gupta%20and%20Danny%20Tarlow%20and%20Duncan%20Smith%20and%20Desi%20Ivanov%20and%20Mohammad%20Babaeizadeh%20and%20Ankita%20Goel%20and%20Satish%20Kambala%20and%20Grace%20Chu%20and%20Matej%20Kastelic%20and%20Michelle%20Liu%20and%20Hagen%20Soltau%20and%20Austin%20Stone%20and%20Shivani%20Agrawal%20and%20Min%20Kim%20and%20Kedar%20Soparkar%20and%20Srinivas%20Tadepalli%20and%20Oskar%20Bunyan%20and%20Rachel%20Soh%20and%20Arvind%20Kannan%20and%20DY%20Kim%20and%20Blake%20JianHang%20Chen%20and%20Afief%20Halumi%20and%20Sudeshna%20Roy%20and%20Yulong%20Wang%20and%20Olcan%20Sercinoglu%20and%20Gena%20Gibson%20and%20Sijal%20Bhatnagar%20and%20Motoki%20Sano%20and%20Daniel%20von%20Dincklage%20and%20Qingchun%20Ren%20and%20Blagoj%20Mitrevski%20and%20Mirek%20Ol%C5%A1%C3%A1k%20and%20Jennifer%20She%20and%20Carl%20Doersch%20and%20%20Jilei%20and%20%20Wang%20and%20Bingyuan%20Liu%20and%20Qijun%20Tan%20and%20Tamar%20Yakar%20and%20Tris%20Warkentin%20and%20Alex%20Ramirez%20and%20Carl%20Lebsack%20and%20Josh%20Dillon%20and%20Rajiv%20Mathews%20and%20Tom%20Cobley%20and%20Zelin%20Wu%20and%20Zhuoyuan%20Chen%20and%20Jon%20Simon%20and%20Swaroop%20Nath%20and%20Tara%20Sainath%20and%20Alexei%20Bendebury%20and%20Ryan%20Julian%20and%20Bharath%20Mankalale%20and%20Daria%20%C4%86urko%20and%20Paulo%20Zacchello%20and%20Adam%20R.%20Brown%20and%20Kiranbir%20Sodhia%20and%20Heidi%20Howard%20and%20Sergi%20Caelles%20and%20Abhinav%20Gupta%20and%20Gareth%20Evans%20and%20Anna%20Bulanova%20and%20Lesley%20Katzen%20and%20Roman%20Goldenberg%20and%20Anton%20Tsitsulin%20and%20Joe%20Stanton%20and%20Benoit%20Schillings%20and%20Vitaly%20Kovalev%20and%20Corey%20Fry%20and%20Rushin%20Shah%20and%20Kuo%20Lin%20and%20Shyam%20Upadhyay%20and%20Cheng%20Li%20and%20Soroush%20Radpour%20and%20Marcello%20Maggioni%20and%20Jing%20Xiong%20and%20Lukas%20Haas%20and%20Jenny%20Brennan%20and%20Aishwarya%20Kamath%20and%20Nikolay%20Savinov%20and%20Arsha%20Nagrani%20and%20Trevor%20Yacovone%20and%20Ryan%20Kappedal%20and%20Kostas%20Andriopoulos%20and%20Li%20Lao%20and%20YaGuang%20Li%20and%20Grigory%20Rozhdestvenskiy%20and%20Kazuma%20Hashimoto%20and%20Andrew%20Audibert%20and%20Sophia%20Austin%20and%20Daniel%20Rodriguez%20and%20Anian%20Ruoss%20and%20Garrett%20Honke%20and%20Deep%20Karkhanis%20and%20Xi%20Xiong%20and%20Qing%20Wei%20and%20James%20Huang%20and%20Zhaoqi%20Leng%20and%20Vittal%20Premachandran%20and%20Stan%20Bileschi%20and%20Georgios%20Evangelopoulos%20and%20Thomas%20Mensink%20and%20Jay%20Pavagadhi%20and%20Denis%20Teplyashin%20and%20Paul%20Chang%20and%20Linting%20Xue%20and%20Garrett%20Tanzer%20and%20Sally%20Goldman%20and%20Kaushal%20Patel%20and%20Shixin%20Li%20and%20Jeremy%20Wiesner%20and%20Ivy%20Zheng%20and%20Ian%20Stewart-Binks%20and%20Jie%20Han%20and%20Zhi%20Li%20and%20Liangchen%20Luo%20and%20Karel%20Lenc%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Fuzhao%20Xue%20and%20Ryan%20Mullins%20and%20Alexey%20Guseynov%20and%20Chung-Ching%20Chang%20and%20Isaac%20Galatzer-Levy%20and%20Adam%20Zhang%20and%20Garrett%20Bingham%20and%20Grace%20Hu%20and%20Ale%20Hartman%20and%20Yue%20Ma%20and%20Jordan%20Griffith%20and%20Alex%20Irpan%20and%20Carey%20Radebaugh%20and%20Summer%20Yue%20and%20Lijie%20Fan%20and%20Victor%20Ungureanu%20and%20Christina%20Sorokin%20and%20Hannah%20Teufel%20and%20Peiran%20Li%20and%20Rohan%20Anil%20and%20Dimitris%20Paparas%20and%20Todd%20Wang%20and%20Chu-Cheng%20Lin%20and%20Hui%20Peng%20and%20Megan%20Shum%20and%20Goran%20Petrovic%20and%20Demetra%20Brady%20and%20Richard%20Nguyen%20and%20Klaus%20Macherey%20and%20Zhihao%20Li%20and%20Harman%20Singh%20and%20Madhavi%20Yenugula%20and%20Mariko%20Iinuma%20and%20Xinyi%20Chen%20and%20Kavya%20Kopparapu%20and%20Alexey%20Stern%20and%20Shachi%20Dave%20and%20Chandu%20Thekkath%20and%20Florence%20Perot%20and%20Anurag%20Kumar%20and%20Fangda%20Li%20and%20Yang%20Xiao%20and%20Matthew%20Bilotti%20and%20Mohammad%20Hossein%20Bateni%20and%20Isaac%20Noble%20and%20Lisa%20Lee%20and%20Amelio%20V%C3%A1zquez-Reina%20and%20Julian%20Salazar%20and%20Xiaomeng%20Yang%20and%20Boyu%20Wang%20and%20Ela%20Gruzewska%20and%20Anand%20Rao%20and%20Sindhu%20Raghuram%20and%20Zheng%20Xu%20and%20Eyal%20Ben-David%20and%20Jieru%20Mei%20and%20Sid%20Dalmia%20and%20Zhaoyi%20Zhang%20and%20Yuchen%20Liu%20and%20Gagan%20Bansal%20and%20Helena%20Pankov%20and%20Steven%20Schwarcz%20and%20Andrea%20Burns%20and%20Christine%20Chan%20and%20Sumit%20Sanghai%20and%20Ricky%20Liang%20and%20Ethan%20Liang%20and%20Antoine%20He%20and%20Amy%20Stuart%20and%20Arun%20Narayanan%20and%20Yukun%20Zhu%20and%20Christian%20Frank%20and%20Bahar%20Fatemi%20and%20Amit%20Sabne%20and%20Oran%20Lang%20and%20Indro%20Bhattacharya%20and%20Shane%20Settle%20and%20Maria%20Wang%20and%20Brendan%20McMahan%20and%20Andrea%20Tacchetti%20and%20Livio%20Baldini%20Soares%20and%20Majid%20Hadian%20and%20Serkan%20Cabi%20and%20Timothy%20Chung%20and%20Nikita%20Putikhin%20and%20Gang%20Li%20and%20Jeremy%20Chen%20and%20Austin%20Tarango%20and%20Henryk%20Michalewski%20and%20Mehran%20Kazemi%20and%20Hussain%20Masoom%20and%20Hila%20Sheftel%20and%20Rakesh%20Shivanna%20and%20Archita%20Vadali%20and%20Ramona%20Comanescu%20and%20Doug%20Reid%20and%20Joss%20Moore%20and%20Arvind%20Neelakantan%20and%20Micha%C3%ABl%20Sander%20and%20Jonathan%20Herzig%20and%20Aviv%20Rosenberg%20and%20Mostafa%20Dehghani%20and%20JD%20Choi%20and%20Michael%20Fink%20and%20Reid%20Hayes%20and%20Eric%20Ge%20and%20Shitao%20Weng%20and%20Chia-Hua%20Ho%20and%20John%20Karro%20and%20Kalpesh%20Krishna%20and%20Lam%20Nguyen%20Thiet%20and%20Amy%20Skerry-Ryan%20and%20Daniel%20Eppens%20and%20Marco%20Andreetto%20and%20Navin%20Sarma%20and%20Silvano%20Bonacina%20and%20Burcu%20Karagol%20Ayan%20and%20Megha%20Nawhal%20and%20Zhihao%20Shan%20and%20Mike%20Dusenberry%20and%20Shantanu%20Thakoor%20and%20Sagar%20Gubbi%20and%20Duc%20Dung%20Nguyen%20and%20Reut%20Tsarfaty%20and%20Samuel%20Albanie%20and%20Jovana%20Mitrovi%C4%87%20and%20Meet%20Gandhi%20and%20Bo-Juen%20Chen%20and%20Alessandro%20Epasto%20and%20Georgi%20Stephanov%20and%20Ye%20Jin%20and%20Samuel%20Gehman%20and%20Aida%20Amini%20and%20Jack%20Weber%20and%20Feryal%20Behbahani%20and%20Shawn%20Xu%20and%20Miltos%20Allamanis%20and%20Xi%20Chen%20and%20Myle%20Ott%20and%20Claire%20Sha%20and%20Michal%20Jastrzebski%20and%20Hang%20Qi%20and%20David%20Greene%20and%20Xinyi%20Wu%20and%20Abodunrinwa%20Toki%20and%20Daniel%20Vlasic%20and%20Jane%20Shapiro%20and%20Ragha%20Kotikalapudi%20and%20Zhe%20Shen%20and%20Takaaki%20Saeki%20and%20Sirui%20Xie%20and%20Albin%20Cassirer%20and%20Shikhar%20Bharadwaj%20and%20Tatsuya%20Kiyono%20and%20Srinadh%20Bhojanapalli%20and%20Elan%20Rosenfeld%20and%20Sam%20Ritter%20and%20Jieming%20Mao%20and%20Jo%C3%A3o%20Gabriel%20Oliveira%20and%20Zoltan%20Egyed%20and%20Bernd%20Bandemer%20and%20Emilio%20Parisotto%20and%20Keisuke%20Kinoshita%20and%20Juliette%20Pluto%20and%20Petros%20Maniatis%20and%20Steve%20Li%20and%20Yaohui%20Guo%20and%20Golnaz%20Ghiasi%20and%20Jean%20Tarbouriech%20and%20Srimon%20Chatterjee%20and%20Julie%20Jin%20and%20%20Katrina%20and%20%20Xu%20and%20Jennimaria%20Palomaki%20and%20S%C3%A9b%20Arnold%20and%20Madhavi%20Sewak%20and%20Federico%20Piccinini%20and%20Mohit%20Sharma%20and%20Ben%20Albrecht%20and%20Sean%20Purser-haskell%20and%20Ashwin%20Vaswani%20and%20Chongyan%20Chen%20and%20Matheus%20Wisniewski%20and%20Qin%20Cao%20and%20John%20Aslanides%20and%20Nguyet%20Minh%20Phu%20and%20Maximilian%20Sieb%20and%20Lauren%20Agubuzu%20and%20Anne%20Zheng%20and%20Daniel%20Sohn%20and%20Marco%20Selvi%20and%20Anders%20Andreassen%20and%20Krishan%20Subudhi%20and%20Prem%20Eruvbetine%20and%20Oliver%20Woodman%20and%20Tomas%20Mery%20and%20Sebastian%20Krause%20and%20Xiaoqi%20Ren%20and%20Xiao%20Ma%20and%20Jincheng%20Luo%20and%20Dawn%20Chen%20and%20Wei%20Fan%20and%20Henry%20Griffiths%20and%20Christian%20Schuler%20and%20Alice%20Li%20and%20Shujian%20Zhang%20and%20Jean-Michel%20Sarr%20and%20Shixin%20Luo%20and%20Riccardo%20Patana%20and%20Matthew%20Watson%20and%20Dani%20Naboulsi%20and%20Michael%20Collins%20and%20Sailesh%20Sidhwani%20and%20Emiel%20Hoogeboom%20and%20Sharon%20Silver%20and%20Emily%20Caveness%20and%20Xiaokai%20Zhao%20and%20Mikel%20Rodriguez%20and%20Maxine%20Deines%20and%20Libin%20Bai%20and%20Patrick%20Griffin%20and%20Marco%20Tagliasacchi%20and%20Emily%20Xue%20and%20Spandana%20Raj%20Babbula%20and%20Bo%20Pang%20and%20Nan%20Ding%20and%20Gloria%20Shen%20and%20Elijah%20Peake%20and%20Remi%20Crocker%20and%20Shubha%20Srinivas%20Raghvendra%20and%20Danny%20Swisher%20and%20Woohyun%20Han%20and%20Richa%20Singh%20and%20Ling%20Wu%20and%20Vladimir%20Pchelin%20and%20Tsendsuren%20Munkhdalai%20and%20Dana%20Alon%20and%20Geoff%20Bacon%20and%20Efren%20Robles%20and%20Jannis%20Bulian%20and%20Melvin%20Johnson%20and%20George%20Powell%20and%20Felipe%20Tiengo%20Ferreira%20and%20Yaoyiran%20Li%20and%20Frederik%20Benzing%20and%20Mihajlo%20Velimirovi%C4%87%20and%20Hubert%20Soyer%20and%20William%20Kong%20and%20%20Tony%20and%20%20Nguy%C3%AAn%20and%20Zhen%20Yang%20and%20Jeremiah%20Liu%20and%20Joost%20van%20Amersfoort%20and%20Daniel%20Gillick%20and%20Baochen%20Sun%20and%20Nathalie%20Rauschmayr%20and%20Katie%20Zhang%20and%20Serena%20Zhan%20and%20Tao%20Zhou%20and%20Alexey%20Frolov%20and%20Chengrun%20Yang%20and%20Denis%20Vnukov%20and%20Louis%20Rouillard%20and%20Hongji%20Li%20and%20Amol%20Mandhane%20and%20Nova%20Fallen%20and%20Rajesh%20Venkataraman%20and%20Clara%20Huiyi%20Hu%20and%20Jennifer%20Brennan%20and%20Jenny%20Lee%20and%20Jerry%20Chang%20and%20Martin%20Sundermeyer%20and%20Zhufeng%20Pan%20and%20Rosemary%20Ke%20and%20Simon%20Tong%20and%20Alex%20Fabrikant%20and%20William%20Bono%20and%20Jindong%20Gu%20and%20Ryan%20Foley%20and%20Yiran%20Mao%20and%20Manolis%20Delakis%20and%20Dhruva%20Bhaswar%20and%20Roy%20Frostig%20and%20Nick%20Li%20and%20Avital%20Zipori%20and%20Cath%20Hope%20and%20Olga%20Kozlova%20and%20Swaroop%20Mishra%20and%20Josip%20Djolonga%20and%20Craig%20Schiff%20and%20Majd%20Al%20Merey%20and%20Eleftheria%20Briakou%20and%20Peter%20Morgan%20and%20Andy%20Wan%20and%20Avinatan%20Hassidim%20and%20RJ%20Skerry-Ryan%20and%20Kuntal%20Sengupta%20and%20Mary%20Jasarevic%20and%20Praveen%20Kallakuri%20and%20Paige%20Kunkle%20and%20Hannah%20Brennan%20and%20Tom%20Lieber%20and%20Hassan%20Mansoor%20and%20Julian%20Walker%20and%20Bing%20Zhang%20and%20Annie%20Xie%20and%20Goran%20%C5%BDu%C5%BEi%C4%87%20and%20Adaeze%20Chukwuka%20and%20Alex%20Druinsky%20and%20Donghyun%20Cho%20and%20Rui%20Yao%20and%20Ferjad%20Naeem%20and%20Shiraz%20Butt%20and%20Eunyoung%20Kim%20and%20Zhipeng%20Jia%20and%20Mandy%20Jordan%20and%20Adam%20Lelkes%20and%20Mark%20Kurzeja%20and%20Sophie%20Wang%20and%20James%20Zhao%20and%20Andrew%20Over%20and%20Abhishek%20Chakladar%20and%20Marcel%20Prasetya%20and%20Neha%20Jha%20and%20Sriram%20Ganapathy%20and%20Yale%20Cong%20and%20Prakash%20Shroff%20and%20Carl%20Saroufim%20and%20Sobhan%20Miryoosefi%20and%20Mohamed%20Hammad%20and%20Tajwar%20Nasir%20and%20Weijuan%20Xi%20and%20Yang%20Gao%20and%20Young%20Maeng%20and%20Ben%20Hora%20and%20Chin-Yi%20Cheng%20and%20Parisa%20Haghani%20and%20Yoad%20Lewenberg%20and%20Caden%20Lu%20and%20Martin%20Matysiak%20and%20Naina%20Raisinghani%20and%20Huiyu%20Wang%20and%20Lexi%20Baugher%20and%20Rahul%20Sukthankar%20and%20Minh%20Giang%20and%20John%20Schultz%20and%20Noah%20Fiedel%20and%20Minmin%20Chen%20and%20Cheng-Chun%20Lee%20and%20Tapomay%20Dey%20and%20Hao%20Zheng%20and%20Shachi%20Paul%20and%20Celine%20Smith%20and%20Andy%20Ly%20and%20Yicheng%20Wang%20and%20Rishabh%20Bansal%20and%20Bartek%20Perz%20and%20Susanna%20Ricco%20and%20Stasha%20Blank%20and%20Vaishakh%20Keshava%20and%20Deepak%20Sharma%20and%20Marvin%20Chow%20and%20Kunal%20Lad%20and%20Komal%20Jalan%20and%20Simon%20Osindero%20and%20Craig%20Swanson%20and%20Jacob%20Scott%20and%20Anastasija%20Ili%C4%87%20and%20Xiaowei%20Li%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Afzal%20Shama%20Soudagar%20and%20Yan%20Xiong%20and%20Bat-Orgil%20Batsaikhan%20and%20Daniel%20Jarrett%20and%20Naveen%20Kumar%20and%20Maulik%20Shah%20and%20Matt%20Lawlor%20and%20Austin%20Waters%20and%20Mark%20Graham%20and%20Rhys%20May%20and%20Sabela%20Ramos%20and%20Sandra%20Lefdal%20and%20Zeynep%20Cankara%20and%20Nacho%20Cano%20and%20Brendan%20O%27Donoghue%20and%20Jed%20Borovik%20and%20Frederick%20Liu%20and%20Jordan%20Grimstad%20and%20Mahmoud%20Alnahlawi%20and%20Katerina%20Tsihlas%20and%20Tom%20Hudson%20and%20Nikolai%20Grigorev%20and%20Yiling%20Jia%20and%20Terry%20Huang%20and%20Tobenna%20Peter%20Igwe%20and%20Sergei%20Lebedev%20and%20Xiaodan%20Tang%20and%20Igor%20Krivokon%20and%20Frankie%20Garcia%20and%20Melissa%20Tan%20and%20Eric%20Jia%20and%20Peter%20Stys%20and%20Shikhar%20Vashishth%20and%20Yu%20Liang%20and%20Balaji%20Venkatraman%20and%20Chenjie%20Gu%20and%20Anastasios%20Kementsietsidis%20and%20Chen%20Zhu%20and%20Junehyuk%20Jung%20and%20Yunfei%20Bai%20and%20Mohammad%20Javad%20Hosseini%20and%20Faruk%20Ahmed%20and%20Aditya%20Gupta%20and%20Xin%20Yuan%20and%20Shereen%20Ashraf%20and%20Shitij%20Nigam%20and%20Gautam%20Vasudevan%20and%20Pranjal%20Awasthi%20and%20Adi%20Mayrav%20Gilady%20and%20Zelda%20Mariet%20and%20Ramy%20Eskander%20and%20Haiguang%20Li%20and%20Hexiang%20Hu%20and%20Guillermo%20Garrido%20and%20Philippe%20Schlattner%20and%20George%20Zhang%20and%20Rohun%20Saxena%20and%20Petar%20Devi%C4%87%20and%20Kritika%20Muralidharan%20and%20Ashwin%20Murthy%20and%20Yiqian%20Zhou%20and%20Min%20Choi%20and%20Arissa%20Wongpanich%20and%20Zhengdong%20Wang%20and%20Premal%20Shah%20and%20Yuntao%20Xu%20and%20Yiling%20Huang%20and%20Stephen%20Spencer%20and%20Alice%20Chen%20and%20James%20Cohan%20and%20Junjie%20Wang%20and%20Jonathan%20Tompson%20and%20Junru%20Wu%20and%20Ruba%20Haroun%20and%20Haiqiong%20Li%20and%20Blanca%20Huergo%20and%20Fan%20Yang%20and%20Tongxin%20Yin%20and%20James%20Wendt%20and%20Michael%20Bendersky%20and%20Rahma%20Chaabouni%20and%20Javier%20Snaider%20and%20Johan%20Ferret%20and%20Abhishek%20Jindal%20and%20Tara%20Thompson%20and%20Andrew%20Xue%20and%20Will%20Bishop%20and%20Shubham%20Milind%20Phal%20and%20Archit%20Sharma%20and%20Yunhsuan%20Sung%20and%20Prabakar%20Radhakrishnan%20and%20Mo%20Shomrat%20and%20Reeve%20Ingle%20and%20Roopali%20Vij%20and%20Justin%20Gilmer%20and%20Mihai%20Dorin%20Istin%20and%20Sam%20Sobell%20and%20Yang%20Lu%20and%20Emily%20Nottage%20and%20Dorsa%20Sadigh%20and%20Jeremiah%20Willcock%20and%20Tingnan%20Zhang%20and%20Steve%20Xu%20and%20Sasha%20Brown%20and%20Katherine%20Lee%20and%20Gary%20Wang%20and%20Yun%20Zhu%20and%20Yi%20Tay%20and%20Cheolmin%20Kim%20and%20Audrey%20Gutierrez%20and%20Abhanshu%20Sharma%20and%20Yongqin%20Xian%20and%20Sungyong%20Seo%20and%20Claire%20Cui%20and%20Elena%20Pochernina%20and%20Cip%20Baetu%20and%20Krzysztof%20Jastrz%C4%99bski%20and%20Mimi%20Ly%20and%20Mohamed%20Elhawaty%20and%20Dan%20Suh%20and%20Eren%20Sezener%20and%20Pidong%20Wang%20and%20Nancy%20Yuen%20and%20George%20Tucker%20and%20Jiahao%20Cai%20and%20Zuguang%20Yang%20and%20Cindy%20Wang%20and%20Alex%20Muzio%20and%20Hai%20Qian%20and%20Jae%20Yoo%20and%20Derek%20Lockhart%20and%20Kevin%20R.%20McKee%20and%20Mandy%20Guo%20and%20Malika%20Mehrotra%20and%20Artur%20Mendon%C3%A7a%20and%20Sanket%20Vaibhav%20Mehta%20and%20Sherry%20Ben%20and%20Chetan%20Tekur%20and%20Jiaqi%20Mu%20and%20Muye%20Zhu%20and%20Victoria%20Krakovna%20and%20Hongrae%20Lee%20and%20AJ%20Maschinot%20and%20S%C3%A9bastien%20Cevey%20and%20HyunJeong%20Choe%20and%20Aijun%20Bai%20and%20Hansa%20Srinivasan%20and%20Derek%20Gasaway%20and%20Nick%20Young%20and%20Patrick%20Siegler%20and%20Dan%20Holtmann-Rice%20and%20Vihari%20Piratla%20and%20Kate%20Baumli%20and%20Roey%20Yogev%20and%20Alex%20Hofer%20and%20Hado%20van%20Hasselt%20and%20Svetlana%20Grant%20and%20Yuri%20Chervonyi%20and%20David%20Silver%20and%20Andrew%20Hogue%20and%20Ayushi%20Agarwal%20and%20Kathie%20Wang%20and%20Preeti%20Singh%20and%20Four%20Flynn%20and%20Josh%20Lipschultz%20and%20Robert%20David%20and%20Lizzetth%20Bellot%20and%20Yao-Yuan%20Yang%20and%20Long%20Le%20and%20Filippo%20Graziano%20and%20Kate%20Olszewska%20and%20Kevin%20Hui%20and%20Akanksha%20Maurya%20and%20Nikos%20Parotsidis%20and%20Weijie%20Chen%20and%20Tayo%20Oguntebi%20and%20Joe%20Kelley%20and%20Anirudh%20Baddepudi%20and%20Johannes%20Mauerer%20and%20Gregory%20Shaw%20and%20Alex%20Siegman%20and%20Lin%20Yang%20and%20Shravya%20Shetty%20and%20Subhrajit%20Roy%20and%20Yunting%20Song%20and%20Wojciech%20Stokowiec%20and%20Ryan%20Burnell%20and%20Omkar%20Savant%20and%20Robert%20Busa-Fekete%20and%20Jin%20Miao%20and%20Samrat%20Ghosh%20and%20Liam%20MacDermed%20and%20Phillip%20Lippe%20and%20Mikhail%20Dektiarev%20and%20Zach%20Behrman%20and%20Fabian%20Mentzer%20and%20Kelvin%20Nguyen%20and%20Meng%20Wei%20and%20Siddharth%20Verma%20and%20Chris%20Knutsen%20and%20Sudeep%20Dasari%20and%20Zhipeng%20Yan%20and%20Petr%20Mitrichev%20and%20Xingyu%20Wang%20and%20Virat%20Shejwalkar%20and%20Jacob%20Austin%20and%20Srinivas%20Sunkara%20and%20Navneet%20Potti%20and%20Yan%20Virin%20and%20Christian%20Wright%20and%20Ga%C3%ABl%20Liu%20and%20Oriana%20Riva%20and%20Etienne%20Pot%20and%20Greg%20Kochanski%20and%20Quoc%20Le%20and%20Gargi%20Balasubramaniam%20and%20Arka%20Dhar%20and%20Yuguo%20Liao%20and%20Adam%20Bloniarz%20and%20Divyansh%20Shukla%20and%20Elizabeth%20Cole%20and%20Jong%20Lee%20and%20Sheng%20Zhang%20and%20Sushant%20Kafle%20and%20Siddharth%20Vashishtha%20and%20Parsa%20Mahmoudieh%20and%20Grace%20Chen%20and%20Raphael%20Hoffmann%20and%20Pranesh%20Srinivasan%20and%20Agustin%20Dal%20Lago%20and%20Yoav%20Ben%20Shalom%20and%20Zi%20Wang%20and%20Michael%20Elabd%20and%20Anuj%20Sharma%20and%20Junhyuk%20Oh%20and%20Suraj%20Kothawade%20and%20Maigo%20Le%20and%20Marianne%20Monteiro%20and%20Shentao%20Yang%20and%20Kaiz%20Alarakyia%20and%20Robert%20Geirhos%20and%20Diana%20Mincu%20and%20H%C3%A5vard%20Garnes%20and%20Hayato%20Kobayashi%20and%20Soroosh%20Mariooryad%20and%20Kacper%20Krasowiak%20and%20%20Zhixin%20and%20%20Lai%20and%20Shibl%20Mourad%20and%20Mingqiu%20Wang%20and%20Fan%20Bu%20and%20Ophir%20Aharoni%20and%20Guanjie%20Chen%20and%20Abhimanyu%20Goyal%20and%20Vadim%20Zubov%20and%20Ankur%20Bapna%20and%20Elahe%20Dabir%20and%20Nisarg%20Kothari%20and%20Kay%20Lamerigts%20and%20Nicola%20De%20Cao%20and%20Jeremy%20Shar%20and%20Christopher%20Yew%20and%20Nitish%20Kulkarni%20and%20Dre%20Mahaarachchi%20and%20Mandar%20Joshi%20and%20Zhenhai%20Zhu%20and%20Jared%20Lichtarge%20and%20Yichao%20Zhou%20and%20Hannah%20Muckenhirn%20and%20Vittorio%20Selo%20and%20Oriol%20Vinyals%20and%20Peter%20Chen%20and%20Anthony%20Brohan%20and%20Vaibhav%20Mehta%20and%20Sarah%20Cogan%20and%20Ruth%20Wang%20and%20Ty%20Geri%20and%20Wei-Jen%20Ko%20and%20Wei%20Chen%20and%20Fabio%20Viola%20and%20Keshav%20Shivam%20and%20Lisa%20Wang%20and%20Madeleine%20Clare%20Elish%20and%20Raluca%20Ada%20Popa%20and%20S%C3%A9bastien%20Pereira%20and%20Jianqiao%20Liu%20and%20Raphael%20Koster%20and%20Donnie%20Kim%20and%20Gufeng%20Zhang%20and%20Sayna%20Ebrahimi%20and%20Partha%20Talukdar%20and%20Yanyan%20Zheng%20and%20Petra%20Poklukar%20and%20Ales%20Mikhalap%20and%20Dale%20Johnson%20and%20Anitha%20Vijayakumar%20and%20Mark%20Omernick%20and%20Matt%20Dibb%20and%20Ayush%20Dubey%20and%20Qiong%20Hu%20and%20Apurv%20Suman%20and%20Vaibhav%20Aggarwal%20and%20Ilya%20Kornakov%20and%20Fei%20Xia%20and%20Wing%20Lowe%20and%20Alexey%20Kolganov%20and%20Ted%20Xiao%20and%20Vitaly%20Nikolaev%20and%20Steven%20Hemingray%20and%20Bonnie%20Li%20and%20Joana%20Iljazi%20and%20Miko%C5%82aj%20Rybi%C5%84ski%20and%20Ballie%20Sandhu%20and%20Peggy%20Lu%20and%20Thang%20Luong%20and%20Rodolphe%20Jenatton%20and%20Vineetha%20Govindaraj%20and%20%20Hui%20and%20%20Li%20and%20Gabriel%20Dulac-Arnold%20and%20Wonpyo%20Park%20and%20Henry%20Wang%20and%20Abhinit%20Modi%20and%20Jean%20Pouget-Abadie%20and%20Kristina%20Greller%20and%20Rahul%20Gupta%20and%20Robert%20Berry%20and%20Prajit%20Ramachandran%20and%20Jinyu%20Xie%20and%20Liam%20McCafferty%20and%20Jianling%20Wang%20and%20Kilol%20Gupta%20and%20Hyeontaek%20Lim%20and%20Bla%C5%BE%20Bratani%C4%8D%20and%20Andy%20Brock%20and%20Ilia%20Akolzin%20and%20Jim%20Sproch%20and%20Dan%20Karliner%20and%20Duhyeon%20Kim%20and%20Adrian%20Goedeckemeyer%20and%20Noam%20Shazeer%20and%20Cordelia%20Schmid%20and%20Daniele%20Calandriello%20and%20Parul%20Bhatia%20and%20Krzysztof%20Choromanski%20and%20Ceslee%20Montgomery%20and%20Dheeru%20Dua%20and%20Ana%20Ramalho%20and%20Helen%20King%20and%20Yue%20Gao%20and%20Lynn%20Nguyen%20and%20David%20Lindner%20and%20Divya%20Pitta%20and%20Oleaser%20Johnson%20and%20Khalid%20Salama%20and%20Diego%20Ardila%20and%20Michael%20Han%20and%20Erin%20Farnese%20and%20Seth%20Odoom%20and%20Ziyue%20Wang%20and%20Xiangzhuo%20Ding%20and%20Norman%20Rink%20and%20Ray%20Smith%20and%20Harshal%20Tushar%20Lehri%20and%20Eden%20Cohen%20and%20Neera%20Vats%20and%20Tong%20He%20and%20Parthasarathy%20Gopavarapu%20and%20Adam%20Paszke%20and%20Miteyan%20Patel%20and%20Wouter%20Van%20Gansbeke%20and%20Lucia%20Loher%20and%20Luis%20Castro%20and%20Maria%20Voitovich%20and%20Tamara%20von%20Glehn%20and%20Nelson%20George%20and%20Simon%20Niklaus%20and%20Zach%20Eaton-Rosen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Erik%20Jue%20and%20Sagi%20Perel%20and%20Carrie%20Zhang%20and%20Yuval%20Bahat%20and%20Ang%C3%A9line%20Pouget%20and%20Zhi%20Xing%20and%20Fantine%20Huot%20and%20Ashish%20Shenoy%20and%20Taylor%20Bos%20and%20Vincent%20Coriou%20and%20Bryan%20Richter%20and%20Natasha%20Noy%20and%20Yaqing%20Wang%20and%20Santiago%20Ontanon%20and%20Siyang%20Qin%20and%20Gleb%20Makarchuk%20and%20Demis%20Hassabis%20and%20Zhuowan%20Li%20and%20Mandar%20Sharma%20and%20Kumaran%20Venkatesan%20and%20Iurii%20Kemaev%20and%20Roxanne%20Daniel%20and%20Shiyu%20Huang%20and%20Saloni%20Shah%20and%20Octavio%20Ponce%20and%20%20Warren%20and%20%20Chen%20and%20Manaal%20Faruqui%20and%20Jialin%20Wu%20and%20Slavica%20Anda%C4%8Di%C4%87%20and%20Szabolcs%20Payrits%20and%20Daniel%20McDuff%20and%20Tom%20Hume%20and%20Yuan%20Cao%20and%20MH%20Tessler%20and%20Qingze%20Wang%20and%20Yinan%20Wang%20and%20Ivor%20Rendulic%20and%20Eirikur%20Agustsson%20and%20Matthew%20Johnson%20and%20Tanya%20Lando%20and%20Andrew%20Howard%20and%20Sri%20Gayatri%20Sundara%20Padmanabhan%20and%20Mayank%20Daswani%20and%20Andrea%20Banino%20and%20Michael%20Kilgore%20and%20Jonathan%20Heek%20and%20Ziwei%20Ji%20and%20Alvaro%20Caceres%20and%20Conglong%20Li%20and%20Nora%20Kassner%20and%20Alexey%20Vlaskin%20and%20Zeyu%20Liu%20and%20Alex%20Grills%20and%20Yanhan%20Hou%20and%20Roykrong%20Sukkerd%20and%20Gowoon%20Cheon%20and%20Nishita%20Shetty%20and%20Larisa%20Markeeva%20and%20Piotr%20Stanczyk%20and%20Tejas%20Iyer%20and%20Yuan%20Gong%20and%20Shawn%20Gao%20and%20Keerthana%20Gopalakrishnan%20and%20Tim%20Blyth%20and%20Malcolm%20Reynolds%20and%20Avishkar%20Bhoopchand%20and%20Misha%20Bilenko%20and%20Dero%20Gharibian%20and%20Vicky%20Zayats%20and%20Aleksandra%20Faust%20and%20Abhinav%20Singh%20and%20Min%20Ma%20and%20Hongyang%20Jiao%20and%20Sudheendra%20Vijayanarasimhan%20and%20Lora%20Aroyo%20and%20Vikas%20Yadav%20and%20Sarah%20Chakera%20and%20Ashwin%20Kakarla%20and%20Vilobh%20Meshram%20and%20Karol%20Gregor%20and%20Gabriela%20Botea%20and%20Evan%20Senter%20and%20Dawei%20Jia%20and%20Geza%20Kovacs%20and%20Neha%20Sharma%20and%20Sebastien%20Baur%20and%20Kai%20Kang%20and%20Yifan%20He%20and%20Lin%20Zhuo%20and%20Marija%20Kostelac%20and%20Itay%20Laish%20and%20Songyou%20Peng%20and%20Louis%20O%27Bryan%20and%20Daniel%20Kasenberg%20and%20Girish%20Ramchandra%20Rao%20and%20Edouard%20Leurent%20and%20Biao%20Zhang%20and%20Sage%20Stevens%20and%20Ana%20Salazar%20and%20Ye%20Zhang%20and%20Ivan%20Lobov%20and%20Jake%20Walker%20and%20Allen%20Porter%20and%20Morgan%20Redshaw%20and%20Han%20Ke%20and%20Abhishek%20Rao%20and%20Alex%20Lee%20and%20Hoi%20Lam%20and%20Michael%20Moffitt%20and%20Jaeyoun%20Kim%20and%20Siyuan%20Qiao%20and%20Terry%20Koo%20and%20Robert%20Dadashi%20and%20Xinying%20Song%20and%20Mukund%20Sundararajan%20and%20Peng%20Xu%20and%20Chizu%20Kawamoto%20and%20Yan%20Zhong%20and%20Clara%20Barbu%20and%20Apoorv%20Reddy%20and%20Mauro%20Verzetti%20and%20Leon%20Li%20and%20George%20Papamakarios%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Mary%20Cassin%20and%20Koray%20Kavukcuoglu%20and%20Rigel%20Swavely%20and%20Alain%20Vaucher%20and%20Jeffrey%20Zhao%20and%20Ross%20Hemsley%20and%20Michael%20Tschannen%20and%20Heming%20Ge%20and%20Gaurav%20Menghani%20and%20Yang%20Yu%20and%20Natalie%20Ha%20and%20Wei%20He%20and%20Xiao%20Wu%20and%20Maggie%20Song%20and%20Rachel%20Sterneck%20and%20Stefan%20Zinke%20and%20Dan%20A.%20Calian%20and%20Annie%20Marsden%20and%20Alejandro%20Cruzado%20Ruiz%20and%20Matteo%20Hessel%20and%20Almog%20Gueta%20and%20Benjamin%20Lee%20and%20Brian%20Farris%20and%20Manish%20Gupta%20and%20Yunjie%20Li%20and%20Mohammad%20Saleh%20and%20Vedant%20Misra%20and%20Kefan%20Xiao%20and%20Piermaria%20Mendolicchio%20and%20Gavin%20Buttimore%20and%20Varvara%20Krayvanova%20and%20Nigamaa%20Nayakanti%20and%20Matthew%20Wiethoff%20and%20Yash%20Pande%20and%20Azalia%20Mirhoseini%20and%20Ni%20Lao%20and%20Jasmine%20Liu%20and%20Yiqing%20Hua%20and%20Angie%20Chen%20and%20Yury%20Malkov%20and%20Dmitry%20Kalashnikov%20and%20Shubham%20Gupta%20and%20Kartik%20Audhkhasi%20and%20Yuexiang%20Zhai%20and%20Sudhindra%20Kopalle%20and%20Prateek%20Jain%20and%20Eran%20Ofek%20and%20Clemens%20Meyer%20and%20Khuslen%20Baatarsukh%20and%20Hana%20Strej%C4%8Dek%20and%20Jun%20Qian%20and%20James%20Freedman%20and%20Ricardo%20Figueira%20and%20Michal%20Sokolik%20and%20Olivier%20Bachem%20and%20Raymond%20Lin%20and%20Dia%20Kharrat%20and%20Chris%20Hidey%20and%20Pingmei%20Xu%20and%20Dennis%20Duan%20and%20Yin%20Li%20and%20Muge%20Ersoy%20and%20Richard%20Everett%20and%20Kevin%20Cen%20and%20Rebeca%20Santamaria-Fernandez%20and%20Amir%20Taubenfeld%20and%20Ian%20Mackinnon%20and%20Linda%20Deng%20and%20Polina%20Zablotskaia%20and%20Shashank%20Viswanadha%20and%20Shivanker%20Goel%20and%20Damion%20Yates%20and%20Yunxiao%20Deng%20and%20Peter%20Choy%20and%20Mingqing%20Chen%20and%20Abhishek%20Sinha%20and%20Alex%20Mossin%20and%20Yiming%20Wang%20and%20Arthur%20Szlam%20and%20Susan%20Hao%20and%20Paul%20Kishan%20Rubenstein%20and%20Metin%20Toksoz-Exley%20and%20Miranda%20Aperghis%20and%20Yin%20Zhong%20and%20Junwhan%20Ahn%20and%20Michael%20Isard%20and%20Olivier%20Lacombe%20and%20Florian%20Luisier%20and%20Chrysovalantis%20Anastasiou%20and%20Yogesh%20Kalley%20and%20Utsav%20Prabhu%20and%20Emma%20Dunleavy%20and%20Shaan%20Bijwadia%20and%20Justin%20Mao-Jones%20and%20Kelly%20Chen%20and%20Rama%20Pasumarthi%20and%20Emily%20Wood%20and%20Adil%20Dostmohamed%20and%20Nate%20Hurley%20and%20Jiri%20Simsa%20and%20Alicia%20Parrish%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Ondrej%20Skopek%20and%20Yony%20Kochinski%20and%20Javier%20Rey%20and%20Verena%20Rieser%20and%20Denny%20Zhou%20and%20Sun%20Jae%20Lee%20and%20Trilok%20Acharya%20and%20Guowang%20Li%20and%20Joe%20Jiang%20and%20Xiaofan%20Zhang%20and%20Bryant%20Gipson%20and%20Ethan%20Mahintorabi%20and%20Marco%20Gelmi%20and%20Nima%20Khajehnouri%20and%20Angel%20Yeh%20and%20Kayi%20Lee%20and%20Loic%20Matthey%20and%20Leslie%20Baker%20and%20Trang%20Pham%20and%20Han%20Fu%20and%20Alex%20Pak%20and%20Prakhar%20Gupta%20and%20Cristina%20Vasconcelos%20and%20Adam%20Sadovsky%20and%20Brian%20Walker%20and%20Sissie%20Hsiao%20and%20Patrik%20Zochbauer%20and%20Andreea%20Marzoca%20and%20Noam%20Velan%20and%20Junhao%20Zeng%20and%20Gilles%20Baechler%20and%20Danny%20Driess%20and%20Divya%20Jain%20and%20Yanping%20Huang%20and%20Lizzie%20Tao%20and%20John%20Maggs%20and%20Nir%20Levine%20and%20Jon%20Schneider%20and%20Erika%20Gemzer%20and%20Samuel%20Petit%20and%20Shan%20Han%20and%20Zach%20Fisher%20and%20Dustin%20Zelle%20and%20Courtney%20Biles%20and%20Eugene%20Ie%20and%20Asya%20Fadeeva%20and%20Casper%20Liu%20and%20Juliana%20Vicente%20Franco%20and%20Adrian%20Collister%20and%20Hao%20Zhang%20and%20Renshen%20Wang%20and%20Ruizhe%20Zhao%20and%20Leandro%20Kieliger%20and%20Kurt%20Shuster%20and%20Rui%20Zhu%20and%20Boqing%20Gong%20and%20Lawrence%20Chan%20and%20Ruoxi%20Sun%20and%20Sujoy%20Basu%20and%20Roland%20Zimmermann%20and%20Jamie%20Hayes%20and%20Abhishek%20Bapna%20and%20Jasper%20Snoek%20and%20Weel%20Yang%20and%20Puranjay%20Datta%20and%20Jad%20Al%20Abdallah%20and%20Kevin%20Kilgour%20and%20Lu%20Li%20and%20SQ%20Mah%20and%20Yennie%20Jun%20and%20Morgane%20Rivi%C3%A8re%20and%20Abhijit%20Karmarkar%20and%20Tammo%20Spalink%20and%20Tao%20Huang%20and%20Lucas%20Gonzalez%20and%20Duc-Hieu%20Tran%20and%20Averi%20Nowak%20and%20John%20Palowitch%20and%20Martin%20Chadwick%20and%20Ellie%20Talius%20and%20Harsh%20Mehta%20and%20Thibault%20Sellam%20and%20Philipp%20Fr%C3%A4nken%20and%20Massimo%20Nicosia%20and%20Kyle%20He%20and%20Aditya%20Kini%20and%20David%20Amos%20and%20Sugato%20Basu%20and%20Harrison%20Jobe%20and%20Eleni%20Shaw%20and%20Qiantong%20Xu%20and%20Colin%20Evans%20and%20Daisuke%20Ikeda%20and%20Chaochao%20Yan%20and%20Larry%20Jin%20and%20Lun%20Wang%20and%20Sachin%20Yadav%20and%20Ilia%20Labzovsky%20and%20Ramesh%20Sampath%20and%20Ada%20Ma%20and%20Candice%20Schumann%20and%20Aditya%20Siddhant%20and%20Rohin%20Shah%20and%20John%20Youssef%20and%20Rishabh%20Agarwal%20and%20Natalie%20Dabney%20and%20Alessio%20Tonioni%20and%20Moran%20Ambar%20and%20Jing%20Li%20and%20Isabelle%20Guyon%20and%20Benny%20Li%20and%20David%20Soergel%20and%20Boya%20Fang%20and%20Georgi%20Karadzhov%20and%20Cristian%20Udrescu%20and%20Trieu%20Trinh%20and%20Vikas%20Raunak%20and%20Seb%20Noury%20and%20Dee%20Guo%20and%20Sonal%20Gupta%20and%20Mara%20Finkelstein%20and%20Denis%20Petek%20and%20Lihao%20Liang%20and%20Greg%20Billock%20and%20Pei%20Sun%20and%20David%20Wood%20and%20Yiwen%20Song%20and%20Xiaobin%20Yu%20and%20Tatiana%20Matejovicova%20and%20Regev%20Cohen%20and%20Kalyan%20Andra%20and%20David%20D%27Ambrosio%20and%20Zhiwei%20Deng%20and%20Vincent%20Nallatamby%20and%20Ebrahim%20Songhori%20and%20Rumen%20Dangovski%20and%20Andrew%20Lampinen%20and%20Pankil%20Botadra%20and%20Adam%20Hillier%20and%20Jiawei%20Cao%20and%20Nagabhushan%20Baddi%20and%20Adhi%20Kuncoro%20and%20Toshihiro%20Yoshino%20and%20Ankit%20Bhagatwala%20and%20Marc%C3%A1urelio%20Ranzato%20and%20Rylan%20Schaeffer%20and%20Tianlin%20Liu%20and%20Shuai%20Ye%20and%20Obaid%20Sarvana%20and%20John%20Nham%20and%20Chenkai%20Kuang%20and%20Isabel%20Gao%20and%20Jinoo%20Baek%20and%20Shubham%20Mittal%20and%20Ayzaan%20Wahid%20and%20Anita%20Gergely%20and%20Bin%20Ni%20and%20Josh%20Feldman%20and%20Carrie%20Muir%20and%20Pascal%20Lamblin%20and%20Wolfgang%20Macherey%20and%20Ethan%20Dyer%20and%20Logan%20Kilpatrick%20and%20V%C3%ADctor%20Campos%20and%20Mukul%20Bhutani%20and%20Stanislav%20Fort%20and%20Yanif%20Ahmad%20and%20Aliaksei%20Severyn%20and%20Kleopatra%20Chatziprimou%20and%20Oleksandr%20Ferludin%20and%20Mason%20Dimarco%20and%20Aditya%20Kusupati%20and%20Joe%20Heyward%20and%20Dan%20Bahir%20and%20Kevin%20Villela%20and%20Katie%20Millican%20and%20Dror%20Marcus%20and%20Sanaz%20Bahargam%20and%20Caglar%20Unlu%20and%20Nicholas%20Roth%20and%20Zichuan%20Wei%20and%20Siddharth%20Gopal%20and%20Deepanway%20Ghoshal%20and%20Edward%20Lee%20and%20Sharon%20Lin%20and%20Jennie%20Lees%20and%20Dayeong%20Lee%20and%20Anahita%20Hosseini%20and%20Connie%20Fan%20and%20Seth%20Neel%20and%20Marcus%20Wu%20and%20Yasemin%20Altun%20and%20Honglong%20Cai%20and%20Enrique%20Piqueras%20and%20Josh%20Woodward%20and%20Alessandro%20Bissacco%20and%20Salem%20Haykal%20and%20Mahyar%20Bordbar%20and%20Prasha%20Sundaram%20and%20Sarah%20Hodkinson%20and%20Daniel%20Toyama%20and%20George%20Polovets%20and%20Austin%20Myers%20and%20Anu%20Sinha%20and%20Tomer%20Levinboim%20and%20Kashyap%20Krishnakumar%20and%20Rachita%20Chhaparia%20and%20Tatiana%20Sholokhova%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Ganesh%20Jawahar%20and%20Haroon%20Qureshi%20and%20Jieru%20Hu%20and%20Nikola%20Momchev%20and%20Matthew%20Rahtz%20and%20Renjie%20Wu%20and%20Aishwarya%20P%20S%20and%20Kedar%20Dhamdhere%20and%20Meiqi%20Guo%20and%20Umang%20Gupta%20and%20Ali%20Eslami%20and%20Mariano%20Schain%20and%20Michiel%20Blokzijl%20and%20David%20Welling%20and%20Dave%20Orr%20and%20Levent%20Bolelli%20and%20Nicolas%20Perez-Nieves%20and%20Mikhail%20Sirotenko%20and%20Aman%20Prasad%20and%20Arjun%20Kar%20and%20Borja%20De%20Balle%20Pigem%20and%20Tayfun%20Terzi%20and%20Gell%C3%A9rt%20Weisz%20and%20Dipankar%20Ghosh%20and%20Aditi%20Mavalankar%20and%20Dhruv%20Madeka%20and%20Kaspar%20Daugaard%20and%20Hartwig%20Adam%20and%20Viraj%20Shah%20and%20Dana%20Berman%20and%20Maggie%20Tran%20and%20Steven%20Baker%20and%20Ewa%20Andrejczuk%20and%20Grishma%20Chole%20and%20Ganna%20Raboshchuk%20and%20Mahdi%20Mirzazadeh%20and%20Thais%20Kagohara%20and%20Shimu%20Wu%20and%20Christian%20Schallhart%20and%20Bernett%20Orlando%20and%20Chen%20Wang%20and%20Alban%20Rrustemi%20and%20Hao%20Xiong%20and%20Hao%20Liu%20and%20Arpi%20Vezer%20and%20Nolan%20Ramsden%20and%20Shuo-yiin%20Chang%20and%20Sidharth%20Mudgal%20and%20Yan%20Li%20and%20Nino%20Vieillard%20and%20Yedid%20Hoshen%20and%20Farooq%20Ahmad%20and%20Ambrose%20Slone%20and%20Amy%20Hua%20and%20Natan%20Potikha%20and%20Mirko%20Rossini%20and%20Jon%20Stritar%20and%20Sushant%20Prakash%20and%20Zifeng%20Wang%20and%20Xuanyi%20Dong%20and%20Alireza%20Nazari%20and%20Efrat%20Nehoran%20and%20Kaan%20Tekelioglu%20and%20Yinxiao%20Li%20and%20Kartikeya%20Badola%20and%20Tom%20Funkhouser%20and%20Yuanzhen%20Li%20and%20Varun%20Yerram%20and%20Ramya%20Ganeshan%20and%20Daniel%20Formoso%20and%20Karol%20Langner%20and%20Tian%20Shi%20and%20Huijian%20Li%20and%20Yumeya%20Yamamori%20and%20Amayika%20Panda%20and%20Alaa%20Saade%20and%20Angelo%20Scorza%20Scarpati%20and%20Chris%20Breaux%20and%20CJ%20Carey%20and%20Zongwei%20Zhou%20and%20Cho-Jui%20Hsieh%20and%20Sophie%20Bridgers%20and%20Alena%20Butryna%20and%20Nishesh%20Gupta%20and%20Vaibhav%20Tulsyan%20and%20Sanghyun%20Woo%20and%20Evgenii%20Eltyshev%20and%20Will%20Grathwohl%20and%20Chanel%20Parks%20and%20Seth%20Benjamin%20and%20Rina%20Panigrahy%20and%20Shenil%20Dodhia%20and%20Daniel%20De%20Freitas%20and%20Chris%20Sauer%20and%20Will%20Song%20and%20Ferran%20Alet%20and%20Jackson%20Tolins%20and%20Cosmin%20Paduraru%20and%20Xingyi%20Zhou%20and%20Brian%20Albert%20and%20Zizhao%20Zhang%20and%20Lei%20Shu%20and%20Mudit%20Bansal%20and%20Sarah%20Nguyen%20and%20Amir%20Globerson%20and%20Owen%20Xiao%20and%20James%20Manyika%20and%20Tom%20Hennigan%20and%20Rong%20Rong%20and%20Josip%20Matak%20and%20Anton%20Bakalov%20and%20Ankur%20Sharma%20and%20Danila%20Sinopalnikov%20and%20Andrew%20Pierson%20and%20Stephen%20Roller%20and%20Geoff%20Brown%20and%20Mingcen%20Gao%20and%20Toshiyuki%20Fukuzawa%20and%20Amin%20Ghafouri%20and%20Kenny%20Vassigh%20and%20Iain%20Barr%20and%20Zhicheng%20Wang%20and%20Anna%20Korsun%20and%20Rajesh%20Jayaram%20and%20Lijie%20Ren%20and%20Tim%20Zaman%20and%20Samira%20Khan%20and%20Yana%20Lunts%20and%20Dan%20Deutsch%20and%20Dave%20Uthus%20and%20Nitzan%20Katz%20and%20Masha%20Samsikova%20and%20Amr%20Khalifa%20and%20Nikhil%20Sethi%20and%20Jiao%20Sun%20and%20Luming%20Tang%20and%20Uri%20Alon%20and%20Xianghong%20Luo%20and%20Dian%20Yu%20and%20Abhishek%20Nayyar%20and%20Bryce%20Petrini%20and%20Will%20Truong%20and%20Vincent%20Hellendoorn%20and%20Nikolai%20Chinaev%20and%20Chris%20Alberti%20and%20Wei%20Wang%20and%20Jingcao%20Hu%20and%20Vahab%20Mirrokni%20and%20Ananth%20Balashankar%20and%20Avia%20Aharon%20and%20Aahil%20Mehta%20and%20Ahmet%20Iscen%20and%20Joseph%20Kready%20and%20Lucas%20Manning%20and%20Anhad%20Mohananey%20and%20Yuankai%20Chen%20and%20Anshuman%20Tripathi%20and%20Allen%20Wu%20and%20Igor%20Petrovski%20and%20Dawsen%20Hwang%20and%20Martin%20Baeuml%20and%20Shreyas%20Chandrakaladharan%20and%20Yuan%20Liu%20and%20Rey%20Coaguila%20and%20Maxwell%20Chen%20and%20Sally%20Ma%20and%20Pouya%20Tafti%20and%20Susheel%20Tatineni%20and%20Terry%20Spitz%20and%20Jiayu%20Ye%20and%20Paul%20Vicol%20and%20Mihaela%20Rosca%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20and%20Zohar%20Yahav%20and%20Sanjay%20Ghemawat%20and%20Hanzhao%20Lin%20and%20Phoebe%20Kirk%20and%20Zaid%20Nabulsi%20and%20Sergey%20Brin%20and%20Bernd%20Bohnet%20and%20Ken%20Caluwaerts%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Dan%20Zheng%20and%20Zihang%20Dai%20and%20Petre%20Petrov%20and%20Yichong%20Xu%20and%20Ramin%20Mehran%20and%20Zhuo%20Xu%20and%20Luisa%20Zintgraf%20and%20Jiho%20Choi%20and%20Spurthi%20Amba%20Hombaiah%20and%20Romal%20Thoppilan%20and%20Sashank%20Reddi%20and%20Lukasz%20Lew%20and%20Li%20Li%20and%20Kellie%20Webster%20and%20KP%20Sawhney%20and%20Lampros%20Lamprou%20and%20Siamak%20Shakeri%20and%20Mayank%20Lunayach%20and%20Jianmin%20Chen%20and%20Sumit%20Bagri%20and%20Alex%20Salcianu%20and%20Ying%20Chen%20and%20Yani%20Donchev%20and%20Charlotte%20Magister%20and%20Signe%20N%C3%B8rly%20and%20Vitor%20Rodrigues%20and%20Tomas%20Izo%20and%20Hila%20Noga%20and%20Joe%20Zou%20and%20Thomas%20K%C3%B6ppe%20and%20Wenxuan%20Zhou%20and%20Kenton%20Lee%20and%20Xiangzhu%20Long%20and%20Danielle%20Eisenbud%20and%20Anthony%20Chen%20and%20Connor%20Schenck%20and%20Chi%20Ming%20To%20and%20Peilin%20Zhong%20and%20Emanuel%20Taropa%20and%20Minh%20Truong%20and%20Omer%20Levy%20and%20Danilo%20Martins%20and%20Zhiyuan%20Zhang%20and%20Christopher%20Semturs%20and%20Kelvin%20Zhang%20and%20Alex%20Yakubovich%20and%20Pol%20Moreno%20and%20Lara%20McConnaughey%20and%20Di%20Lu%20and%20Sam%20Redmond%20and%20Lotte%20Weerts%20and%20Yonatan%20Bitton%20and%20Tiziana%20Refice%20and%20Nicolas%20Lacasse%20and%20Arthur%20Conmy%20and%20Corentin%20Tallec%20and%20Julian%20Odell%20and%20Hannah%20Forbes-Pollard%20and%20Arkadiusz%20Socala%20and%20Jonathan%20Hoech%20and%20Pushmeet%20Kohli%20and%20Alanna%20Walton%20and%20Rui%20Wang%20and%20Mikita%20Sazanovich%20and%20Kexin%20Zhu%20and%20Andrei%20Kapishnikov%20and%20Rich%20Galt%20and%20Matthew%20Denton%20and%20Ben%20Murdoch%20and%20Caitlin%20Sikora%20and%20Kareem%20Mohamed%20and%20Wei%20Wei%20and%20Uri%20First%20and%20Tim%20McConnell%20and%20Luis%20C.%20Cobo%20and%20James%20Qin%20and%20Thi%20Avrahami%20and%20Daniel%20Balle%20and%20Yu%20Watanabe%20and%20Annie%20Louis%20and%20Adam%20Kraft%20and%20Setareh%20Ariafar%20and%20Yiming%20Gu%20and%20Eug%C3%A9nie%20Rives%20and%20Charles%20Yoon%20and%20Andrei%20Rusu%20and%20James%20Cobon-Kerr%20and%20Chris%20Hahn%20and%20Jiaming%20Luo%20and%20%20Yuvein%20and%20%20Zhu%20and%20Niharika%20Ahuja%20and%20Rodrigo%20Benenson%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Honglin%20Yu%20and%20Lloyd%20Hightower%20and%20Junlin%20Zhang%20and%20Darren%20Ni%20and%20Lisa%20Anne%20Hendricks%20and%20Gabby%20Wang%20and%20Gal%20Yona%20and%20Lalit%20Jain%20and%20Pablo%20Barrio%20and%20Surya%20Bhupatiraju%20and%20Siva%20Velusamy%20and%20Allan%20Dafoe%20and%20Sebastian%20Riedel%20and%20Tara%20Thomas%20and%20Zhe%20Yuan%20and%20Mathias%20Bellaiche%20and%20Sheena%20Panthaplackel%20and%20Klemen%20Kloboves%20and%20Sarthak%20Jauhari%20and%20Canfer%20Akbulut%20and%20Todor%20Davchev%20and%20Evgeny%20Gladchenko%20and%20David%20Madras%20and%20Aleksandr%20Chuklin%20and%20Tyrone%20Hill%20and%20Quan%20Yuan%20and%20Mukundan%20Madhavan%20and%20Luke%20Leonhard%20and%20Dylan%20Scandinaro%20and%20Qihang%20Chen%20and%20Ning%20Niu%20and%20Arthur%20Douillard%20and%20Bogdan%20Damoc%20and%20Yasumasa%20Onoe%20and%20Fabian%20Pedregosa%20and%20Fred%20Bertsch%20and%20Chas%20Leichner%20and%20Joseph%20Pagadora%20and%20Jonathan%20Malmaud%20and%20Sameera%20Ponda%20and%20Andy%20Twigg%20and%20Oleksii%20Duzhyi%20and%20Jingwei%20Shen%20and%20Miaosen%20Wang%20and%20Roopal%20Garg%20and%20Jing%20Chen%20and%20Utku%20Evci%20and%20Jonathan%20Lee%20and%20Leon%20Liu%20and%20Koji%20Kojima%20and%20Masa%20Yamaguchi%20and%20Arunkumar%20Rajendran%20and%20AJ%20Piergiovanni%20and%20Vinodh%20Kumar%20Rajendran%20and%20Marco%20Fornoni%20and%20Gabriel%20Ibagon%20and%20Harry%20Ragan%20and%20Sadh%20MNM%20Khan%20and%20John%20Blitzer%20and%20Andrew%20Bunner%20and%20Guan%20Sun%20and%20Takahiro%20Kosakai%20and%20Scott%20Lundberg%20and%20Ndidi%20Elue%20and%20Kelvin%20Guu%20and%20SK%20Park%20and%20Jane%20Park%20and%20Arunachalam%20Narayanaswamy%20and%20Chengda%20Wu%20and%20Jayaram%20Mudigonda%20and%20Trevor%20Cohn%20and%20Hairong%20Mu%20and%20Ravi%20Kumar%20and%20Laura%20Graesser%20and%20Yichi%20Zhang%20and%20Richard%20Killam%20and%20Vincent%20Zhuang%20and%20Mai%20Gim%C3%A9nez%20and%20Wael%20Al%20Jishi%20and%20Ruy%20Ley-Wild%20and%20Alex%20Zhai%20and%20Kazuki%20Osawa%20and%20Diego%20Cedillo%20and%20Jialu%20Liu%20and%20Mayank%20Upadhyay%20and%20Marcin%20Sieniek%20and%20Roshan%20Sharma%20and%20Tom%20Paine%20and%20Anelia%20Angelova%20and%20Sravanti%20Addepalli%20and%20Carolina%20Parada%20and%20Kingshuk%20Majumder%20and%20Avery%20Lamp%20and%20Sanjiv%20Kumar%20and%20Xiang%20Deng%20and%20Artiom%20Myaskovsky%20and%20Tea%20Saboli%C4%87%20and%20Jeffrey%20Dudek%20and%20Sarah%20York%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Jiazhong%20Nie%20and%20Dee%20Cattle%20and%20Alok%20Gunjan%20and%20Bilal%20Piot%20and%20Waleed%20Khawaja%20and%20Seojin%20Bang%20and%20Simon%20Wang%20and%20Siavash%20Khodadadeh%20and%20Raghavender%20R%20and%20Praynaa%20Rawlani%20and%20Richard%20Powell%20and%20Kevin%20Lee%20and%20Johannes%20Griesser%20and%20GS%20Oh%20and%20Cesar%20Magalhaes%20and%20Yujia%20Li%20and%20Simon%20Tokumine%20and%20Hadas%20Natalie%20Vogel%20and%20Dennis%20Hsu%20and%20Arturo%20BC%20and%20Disha%20Jindal%20and%20Matan%20Cohen%20and%20Zi%20Yang%20and%20Junwei%20Yuan%20and%20Dario%20de%20Cesare%20and%20Tony%20Bruguier%20and%20Jun%20Xu%20and%20Monica%20Roy%20and%20Alon%20Jacovi%20and%20Dan%20Belov%20and%20Rahul%20Arya%20and%20Phoenix%20Meadowlark%20and%20Shlomi%20Cohen-Ganor%20and%20Wenting%20Ye%20and%20Patrick%20Morris-Suzuki%20and%20Praseem%20Banzal%20and%20Gan%20Song%20and%20Pranavaraj%20Ponnuramu%20and%20Fred%20Zhang%20and%20George%20Scrivener%20and%20Salah%20Zaiem%20and%20Alif%20Raditya%20Rochman%20and%20Kehang%20Han%20and%20Badih%20Ghazi%20and%20Kate%20Lee%20and%20Shahar%20Drath%20and%20Daniel%20Suo%20and%20Antonious%20Girgis%20and%20Pradeep%20Shenoy%20and%20Duy%20Nguyen%20and%20Douglas%20Eck%20and%20Somit%20Gupta%20and%20Le%20Yan%20and%20Joao%20Carreira%20and%20Anmol%20Gulati%20and%20Ruoxin%20Sang%20and%20Daniil%20Mirylenka%20and%20Emma%20Cooney%20and%20Edward%20Chou%20and%20Mingyang%20Ling%20and%20Cindy%20Fan%20and%20Ben%20Coleman%20and%20Guilherme%20Tubone%20and%20Ravin%20Kumar%20and%20Jason%20Baldridge%20and%20Felix%20Hernandez-Campos%20and%20Angeliki%20Lazaridou%20and%20James%20Besley%20and%20Itay%20Yona%20and%20Neslihan%20Bulut%20and%20Quentin%20Wellens%20and%20AJ%20Pierigiovanni%20and%20Jasmine%20George%20and%20Richard%20Green%20and%20Pu%20Han%20and%20Connie%20Tao%20and%20Geoff%20Clark%20and%20Chong%20You%20and%20Abbas%20Abdolmaleki%20and%20Justin%20Fu%20and%20Tongzhou%20Chen%20and%20Ashwin%20Chaugule%20and%20Angad%20Chandorkar%20and%20Altaf%20Rahman%20and%20Will%20Thompson%20and%20Penporn%20Koanantakool%20and%20Mike%20Bernico%20and%20Jie%20Ren%20and%20Andrey%20Vlasov%20and%20Sergei%20Vassilvitskii%20and%20Maciej%20Kula%20and%20Yizhong%20Liang%20and%20Dahun%20Kim%20and%20Yangsibo%20Huang%20and%20Chengxi%20Ye%20and%20Dmitry%20Lepikhin%20and%20Wesley%20Helmholz&entry.1292438233=In%20this%20report%2C%20we%20introduce%20the%20Gemini%202.X%20model%20family%3A%20Gemini%202.5%20Pro%20and%20Gemini%202.5%20Flash%2C%20as%20well%20as%20our%20earlier%20Gemini%202.0%20Flash%20and%20Flash-Lite%20models.%20Gemini%202.5%20Pro%20is%20our%20most%20capable%20model%20yet%2C%20achieving%20SoTA%20performance%20on%20frontier%20coding%20and%20reasoning%20benchmarks.%20In%20addition%20to%20its%20incredible%20coding%20and%20reasoning%20skills%2C%20Gemini%202.5%20Pro%20is%20a%20thinking%20model%20that%20excels%20at%20multimodal%20understanding%20and%20it%20is%20now%20able%20to%20process%20up%20to%203%20hours%20of%20video%20content.%20Its%20unique%20combination%20of%20long%20context%2C%20multimodal%20and%20reasoning%20capabilities%20can%20be%20combined%20to%20unlock%20new%20agentic%20workflows.%20Gemini%202.5%20Flash%20provides%20excellent%20reasoning%20abilities%20at%20a%20fraction%20of%20the%20compute%20and%20latency%20requirements%20and%20Gemini%202.0%20Flash%20and%20Flash-Lite%20provide%20high%20performance%20at%20low%20latency%20and%20cost.%20Taken%20together%2C%20the%20Gemini%202.X%20model%20generation%20spans%20the%20full%20Pareto%20frontier%20of%20model%20capability%20vs%20cost%2C%20allowing%20users%20to%20explore%20the%20boundaries%20of%20what%20is%20possible%20with%20complex%20agentic%20problem%20solving.&entry.1838667208=http%3A//arxiv.org/abs/2507.06261v6&entry.124074799=Read"},
{"title": "A Unified Representation of Neural Networks Architectures", "author": "Christophe Prieur and Mircea Lazar and Bogdan Robu", "abstract": "In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.", "link": "http://arxiv.org/abs/2512.17593v1", "date": "2025-12-19", "relevancy": 2.2834, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4868}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4465}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Representation%20of%20Neural%20Networks%20Architectures&body=Title%3A%20A%20Unified%20Representation%20of%20Neural%20Networks%20Architectures%0AAuthor%3A%20Christophe%20Prieur%20and%20Mircea%20Lazar%20and%20Bogdan%20Robu%0AAbstract%3A%20In%20this%20paper%20we%20consider%20the%20limiting%20case%20of%20neural%20networks%20%28NNs%29%20architectures%20when%20the%20number%20of%20neurons%20in%20each%20hidden%20layer%20and%20the%20number%20of%20hidden%20layers%20tend%20to%20infinity%20thus%20forming%20a%20continuum%2C%20and%20we%20derive%20approximation%20errors%20as%20a%20function%20of%20the%20number%20of%20neurons%20and/or%20hidden%20layers.%20Firstly%2C%20we%20consider%20the%20case%20of%20neural%20networks%20with%20a%20single%20hidden%20layer%20and%20we%20derive%20an%20integral%20infinite%20width%20neural%20representation%20that%20generalizes%20existing%20continuous%20neural%20networks%20%28CNNs%29%20representations.%20Then%20we%20extend%20this%20to%20deep%20residual%20CNNs%20that%20have%20a%20finite%20number%20of%20integral%20hidden%20layers%20and%20residual%20connections.%20Secondly%2C%20we%20revisit%20the%20relation%20between%20neural%20ODEs%20and%20deep%20residual%20NNs%20and%20we%20formalize%20approximation%20errors%20via%20discretization%20techniques.%20Then%2C%20we%20merge%20these%20two%20approaches%20into%20a%20unified%20homogeneous%20representation%20of%20NNs%20as%20a%20Distributed%20Parameter%20neural%20Network%20%28DiPaNet%29%20and%20we%20show%20that%20most%20of%20the%20existing%20finite%20and%20infinite-dimensional%20NNs%20architectures%20are%20related%20via%20homogeneization/discretization%20with%20the%20DiPaNet%20representation.%20Our%20approach%20is%20purely%20deterministic%20and%20applies%20to%20general%2C%20uniformly%20continuous%20matrix%20weight%20functions.%20Differences%20and%20similarities%20with%20neural%20fields%20are%20discussed%20along%20with%20further%20possible%20generalizations%20and%20applications%20of%20the%20DiPaNet%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Representation%2520of%2520Neural%2520Networks%2520Architectures%26entry.906535625%3DChristophe%2520Prieur%2520and%2520Mircea%2520Lazar%2520and%2520Bogdan%2520Robu%26entry.1292438233%3DIn%2520this%2520paper%2520we%2520consider%2520the%2520limiting%2520case%2520of%2520neural%2520networks%2520%2528NNs%2529%2520architectures%2520when%2520the%2520number%2520of%2520neurons%2520in%2520each%2520hidden%2520layer%2520and%2520the%2520number%2520of%2520hidden%2520layers%2520tend%2520to%2520infinity%2520thus%2520forming%2520a%2520continuum%252C%2520and%2520we%2520derive%2520approximation%2520errors%2520as%2520a%2520function%2520of%2520the%2520number%2520of%2520neurons%2520and/or%2520hidden%2520layers.%2520Firstly%252C%2520we%2520consider%2520the%2520case%2520of%2520neural%2520networks%2520with%2520a%2520single%2520hidden%2520layer%2520and%2520we%2520derive%2520an%2520integral%2520infinite%2520width%2520neural%2520representation%2520that%2520generalizes%2520existing%2520continuous%2520neural%2520networks%2520%2528CNNs%2529%2520representations.%2520Then%2520we%2520extend%2520this%2520to%2520deep%2520residual%2520CNNs%2520that%2520have%2520a%2520finite%2520number%2520of%2520integral%2520hidden%2520layers%2520and%2520residual%2520connections.%2520Secondly%252C%2520we%2520revisit%2520the%2520relation%2520between%2520neural%2520ODEs%2520and%2520deep%2520residual%2520NNs%2520and%2520we%2520formalize%2520approximation%2520errors%2520via%2520discretization%2520techniques.%2520Then%252C%2520we%2520merge%2520these%2520two%2520approaches%2520into%2520a%2520unified%2520homogeneous%2520representation%2520of%2520NNs%2520as%2520a%2520Distributed%2520Parameter%2520neural%2520Network%2520%2528DiPaNet%2529%2520and%2520we%2520show%2520that%2520most%2520of%2520the%2520existing%2520finite%2520and%2520infinite-dimensional%2520NNs%2520architectures%2520are%2520related%2520via%2520homogeneization/discretization%2520with%2520the%2520DiPaNet%2520representation.%2520Our%2520approach%2520is%2520purely%2520deterministic%2520and%2520applies%2520to%2520general%252C%2520uniformly%2520continuous%2520matrix%2520weight%2520functions.%2520Differences%2520and%2520similarities%2520with%2520neural%2520fields%2520are%2520discussed%2520along%2520with%2520further%2520possible%2520generalizations%2520and%2520applications%2520of%2520the%2520DiPaNet%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Representation%20of%20Neural%20Networks%20Architectures&entry.906535625=Christophe%20Prieur%20and%20Mircea%20Lazar%20and%20Bogdan%20Robu&entry.1292438233=In%20this%20paper%20we%20consider%20the%20limiting%20case%20of%20neural%20networks%20%28NNs%29%20architectures%20when%20the%20number%20of%20neurons%20in%20each%20hidden%20layer%20and%20the%20number%20of%20hidden%20layers%20tend%20to%20infinity%20thus%20forming%20a%20continuum%2C%20and%20we%20derive%20approximation%20errors%20as%20a%20function%20of%20the%20number%20of%20neurons%20and/or%20hidden%20layers.%20Firstly%2C%20we%20consider%20the%20case%20of%20neural%20networks%20with%20a%20single%20hidden%20layer%20and%20we%20derive%20an%20integral%20infinite%20width%20neural%20representation%20that%20generalizes%20existing%20continuous%20neural%20networks%20%28CNNs%29%20representations.%20Then%20we%20extend%20this%20to%20deep%20residual%20CNNs%20that%20have%20a%20finite%20number%20of%20integral%20hidden%20layers%20and%20residual%20connections.%20Secondly%2C%20we%20revisit%20the%20relation%20between%20neural%20ODEs%20and%20deep%20residual%20NNs%20and%20we%20formalize%20approximation%20errors%20via%20discretization%20techniques.%20Then%2C%20we%20merge%20these%20two%20approaches%20into%20a%20unified%20homogeneous%20representation%20of%20NNs%20as%20a%20Distributed%20Parameter%20neural%20Network%20%28DiPaNet%29%20and%20we%20show%20that%20most%20of%20the%20existing%20finite%20and%20infinite-dimensional%20NNs%20architectures%20are%20related%20via%20homogeneization/discretization%20with%20the%20DiPaNet%20representation.%20Our%20approach%20is%20purely%20deterministic%20and%20applies%20to%20general%2C%20uniformly%20continuous%20matrix%20weight%20functions.%20Differences%20and%20similarities%20with%20neural%20fields%20are%20discussed%20along%20with%20further%20possible%20generalizations%20and%20applications%20of%20the%20DiPaNet%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.17593v1&entry.124074799=Read"},
{"title": "Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting", "author": "Ananta R. Bhattarai and Helge Rhodin", "abstract": "Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.", "link": "http://arxiv.org/abs/2512.17908v1", "date": "2025-12-19", "relevancy": 2.2829, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5819}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5685}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-Depth%20Anything%3A%20Test-Time%20Depth%20Refinement%20via%20Self-Supervised%20Re-lighting&body=Title%3A%20Re-Depth%20Anything%3A%20Test-Time%20Depth%20Refinement%20via%20Self-Supervised%20Re-lighting%0AAuthor%3A%20Ananta%20R.%20Bhattarai%20and%20Helge%20Rhodin%0AAbstract%3A%20Monocular%20depth%20estimation%20remains%20challenging%20as%20recent%20foundation%20models%2C%20such%20as%20Depth%20Anything%20V2%20%28DA-V2%29%2C%20struggle%20with%20real-world%20images%20that%20are%20far%20from%20the%20training%20distribution.%20We%20introduce%20Re-Depth%20Anything%2C%20a%20test-time%20self-supervision%20framework%20that%20bridges%20this%20domain%20gap%20by%20fusing%20DA-V2%20with%20the%20powerful%20priors%20of%20large-scale%202D%20diffusion%20models.%20Our%20method%20performs%20label-free%20refinement%20directly%20on%20the%20input%20image%20by%20re-lighting%20predicted%20depth%20maps%20and%20augmenting%20the%20input.%20This%20re-synthesis%20method%20replaces%20classical%20photometric%20reconstruction%20by%20leveraging%20shape%20from%20shading%20%28SfS%29%20cues%20in%20a%20new%2C%20generative%20context%20with%20Score%20Distillation%20Sampling%20%28SDS%29.%20To%20prevent%20optimization%20collapse%2C%20our%20framework%20employs%20a%20targeted%20optimization%20strategy%3A%20rather%20than%20optimizing%20depth%20directly%20or%20fine-tuning%20the%20full%20model%2C%20we%20freeze%20the%20encoder%20and%20only%20update%20intermediate%20embeddings%20while%20also%20fine-tuning%20the%20decoder.%20Across%20diverse%20benchmarks%2C%20Re-Depth%20Anything%20yields%20substantial%20gains%20in%20depth%20accuracy%20and%20realism%20over%20the%20DA-V2%2C%20showcasing%20new%20avenues%20for%20self-supervision%20by%20augmenting%20geometric%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-Depth%2520Anything%253A%2520Test-Time%2520Depth%2520Refinement%2520via%2520Self-Supervised%2520Re-lighting%26entry.906535625%3DAnanta%2520R.%2520Bhattarai%2520and%2520Helge%2520Rhodin%26entry.1292438233%3DMonocular%2520depth%2520estimation%2520remains%2520challenging%2520as%2520recent%2520foundation%2520models%252C%2520such%2520as%2520Depth%2520Anything%2520V2%2520%2528DA-V2%2529%252C%2520struggle%2520with%2520real-world%2520images%2520that%2520are%2520far%2520from%2520the%2520training%2520distribution.%2520We%2520introduce%2520Re-Depth%2520Anything%252C%2520a%2520test-time%2520self-supervision%2520framework%2520that%2520bridges%2520this%2520domain%2520gap%2520by%2520fusing%2520DA-V2%2520with%2520the%2520powerful%2520priors%2520of%2520large-scale%25202D%2520diffusion%2520models.%2520Our%2520method%2520performs%2520label-free%2520refinement%2520directly%2520on%2520the%2520input%2520image%2520by%2520re-lighting%2520predicted%2520depth%2520maps%2520and%2520augmenting%2520the%2520input.%2520This%2520re-synthesis%2520method%2520replaces%2520classical%2520photometric%2520reconstruction%2520by%2520leveraging%2520shape%2520from%2520shading%2520%2528SfS%2529%2520cues%2520in%2520a%2520new%252C%2520generative%2520context%2520with%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529.%2520To%2520prevent%2520optimization%2520collapse%252C%2520our%2520framework%2520employs%2520a%2520targeted%2520optimization%2520strategy%253A%2520rather%2520than%2520optimizing%2520depth%2520directly%2520or%2520fine-tuning%2520the%2520full%2520model%252C%2520we%2520freeze%2520the%2520encoder%2520and%2520only%2520update%2520intermediate%2520embeddings%2520while%2520also%2520fine-tuning%2520the%2520decoder.%2520Across%2520diverse%2520benchmarks%252C%2520Re-Depth%2520Anything%2520yields%2520substantial%2520gains%2520in%2520depth%2520accuracy%2520and%2520realism%2520over%2520the%2520DA-V2%252C%2520showcasing%2520new%2520avenues%2520for%2520self-supervision%2520by%2520augmenting%2520geometric%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-Depth%20Anything%3A%20Test-Time%20Depth%20Refinement%20via%20Self-Supervised%20Re-lighting&entry.906535625=Ananta%20R.%20Bhattarai%20and%20Helge%20Rhodin&entry.1292438233=Monocular%20depth%20estimation%20remains%20challenging%20as%20recent%20foundation%20models%2C%20such%20as%20Depth%20Anything%20V2%20%28DA-V2%29%2C%20struggle%20with%20real-world%20images%20that%20are%20far%20from%20the%20training%20distribution.%20We%20introduce%20Re-Depth%20Anything%2C%20a%20test-time%20self-supervision%20framework%20that%20bridges%20this%20domain%20gap%20by%20fusing%20DA-V2%20with%20the%20powerful%20priors%20of%20large-scale%202D%20diffusion%20models.%20Our%20method%20performs%20label-free%20refinement%20directly%20on%20the%20input%20image%20by%20re-lighting%20predicted%20depth%20maps%20and%20augmenting%20the%20input.%20This%20re-synthesis%20method%20replaces%20classical%20photometric%20reconstruction%20by%20leveraging%20shape%20from%20shading%20%28SfS%29%20cues%20in%20a%20new%2C%20generative%20context%20with%20Score%20Distillation%20Sampling%20%28SDS%29.%20To%20prevent%20optimization%20collapse%2C%20our%20framework%20employs%20a%20targeted%20optimization%20strategy%3A%20rather%20than%20optimizing%20depth%20directly%20or%20fine-tuning%20the%20full%20model%2C%20we%20freeze%20the%20encoder%20and%20only%20update%20intermediate%20embeddings%20while%20also%20fine-tuning%20the%20decoder.%20Across%20diverse%20benchmarks%2C%20Re-Depth%20Anything%20yields%20substantial%20gains%20in%20depth%20accuracy%20and%20realism%20over%20the%20DA-V2%2C%20showcasing%20new%20avenues%20for%20self-supervision%20by%20augmenting%20geometric%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.17908v1&entry.124074799=Read"},
{"title": "Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR", "author": "Denis Mikhailapov and Vladimir Berikov", "abstract": "Convolutional neural networks (CNN) for multi-class segmentation of medical images are widely used today. Especially models with multiple outputs that can separately predict segmentation classes (regions) without relying on a probabilistic formulation of the segmentation of regions. These models allow for more precise segmentation by tailoring the network's components to each class (region). They have a common encoder part of the architecture but branch out at the output layers, leading to improved accuracy.\n  These methods are used to diagnose type B aortic dissection (TBAD), which requires accurate segmentation of aortic structures based on the ImageTBDA dataset, which contains 100 3D computed tomography angiography (CTA) images. These images identify three key classes: true lumen (TL), false lumen (FL), and false lumen thrombus (FLT) of the aorta, which is critical for diagnosis and treatment decisions. In the dataset, 68 examples have a false lumen, while the remaining 32 do not, creating additional complexity for pathology detection.\n  However, implementing these CNN methods requires a large amount of high-quality labeled data. Obtaining accurate labels for the regions of interest can be an expensive and time-consuming process, particularly for 3D data. Semi-supervised learning methods allow models to be trained by using both labeled and unlabeled data, which is a promising approach for overcoming the challenge of obtaining accurate labels. However, these learning methods are not well understood for models with multiple outputs.\n  This paper presents a semi-supervised learning method for models with multiple outputs. The method is based on the additional rotations and flipping, and does not assume the probabilistic nature of the model's responses. This makes it a universal approach, which is especially important for architectures that involve separate segmentation.", "link": "http://arxiv.org/abs/2512.17610v1", "date": "2025-12-19", "relevancy": 2.2744, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5744}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5712}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%203D%20Segmentation%20for%20Type-B%20Aortic%20Dissection%20with%20Slim%20UNETR&body=Title%3A%20Semi-Supervised%203D%20Segmentation%20for%20Type-B%20Aortic%20Dissection%20with%20Slim%20UNETR%0AAuthor%3A%20Denis%20Mikhailapov%20and%20Vladimir%20Berikov%0AAbstract%3A%20Convolutional%20neural%20networks%20%28CNN%29%20for%20multi-class%20segmentation%20of%20medical%20images%20are%20widely%20used%20today.%20Especially%20models%20with%20multiple%20outputs%20that%20can%20separately%20predict%20segmentation%20classes%20%28regions%29%20without%20relying%20on%20a%20probabilistic%20formulation%20of%20the%20segmentation%20of%20regions.%20These%20models%20allow%20for%20more%20precise%20segmentation%20by%20tailoring%20the%20network%27s%20components%20to%20each%20class%20%28region%29.%20They%20have%20a%20common%20encoder%20part%20of%20the%20architecture%20but%20branch%20out%20at%20the%20output%20layers%2C%20leading%20to%20improved%20accuracy.%0A%20%20These%20methods%20are%20used%20to%20diagnose%20type%20B%20aortic%20dissection%20%28TBAD%29%2C%20which%20requires%20accurate%20segmentation%20of%20aortic%20structures%20based%20on%20the%20ImageTBDA%20dataset%2C%20which%20contains%20100%203D%20computed%20tomography%20angiography%20%28CTA%29%20images.%20These%20images%20identify%20three%20key%20classes%3A%20true%20lumen%20%28TL%29%2C%20false%20lumen%20%28FL%29%2C%20and%20false%20lumen%20thrombus%20%28FLT%29%20of%20the%20aorta%2C%20which%20is%20critical%20for%20diagnosis%20and%20treatment%20decisions.%20In%20the%20dataset%2C%2068%20examples%20have%20a%20false%20lumen%2C%20while%20the%20remaining%2032%20do%20not%2C%20creating%20additional%20complexity%20for%20pathology%20detection.%0A%20%20However%2C%20implementing%20these%20CNN%20methods%20requires%20a%20large%20amount%20of%20high-quality%20labeled%20data.%20Obtaining%20accurate%20labels%20for%20the%20regions%20of%20interest%20can%20be%20an%20expensive%20and%20time-consuming%20process%2C%20particularly%20for%203D%20data.%20Semi-supervised%20learning%20methods%20allow%20models%20to%20be%20trained%20by%20using%20both%20labeled%20and%20unlabeled%20data%2C%20which%20is%20a%20promising%20approach%20for%20overcoming%20the%20challenge%20of%20obtaining%20accurate%20labels.%20However%2C%20these%20learning%20methods%20are%20not%20well%20understood%20for%20models%20with%20multiple%20outputs.%0A%20%20This%20paper%20presents%20a%20semi-supervised%20learning%20method%20for%20models%20with%20multiple%20outputs.%20The%20method%20is%20based%20on%20the%20additional%20rotations%20and%20flipping%2C%20and%20does%20not%20assume%20the%20probabilistic%20nature%20of%20the%20model%27s%20responses.%20This%20makes%20it%20a%20universal%20approach%2C%20which%20is%20especially%20important%20for%20architectures%20that%20involve%20separate%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%25203D%2520Segmentation%2520for%2520Type-B%2520Aortic%2520Dissection%2520with%2520Slim%2520UNETR%26entry.906535625%3DDenis%2520Mikhailapov%2520and%2520Vladimir%2520Berikov%26entry.1292438233%3DConvolutional%2520neural%2520networks%2520%2528CNN%2529%2520for%2520multi-class%2520segmentation%2520of%2520medical%2520images%2520are%2520widely%2520used%2520today.%2520Especially%2520models%2520with%2520multiple%2520outputs%2520that%2520can%2520separately%2520predict%2520segmentation%2520classes%2520%2528regions%2529%2520without%2520relying%2520on%2520a%2520probabilistic%2520formulation%2520of%2520the%2520segmentation%2520of%2520regions.%2520These%2520models%2520allow%2520for%2520more%2520precise%2520segmentation%2520by%2520tailoring%2520the%2520network%2527s%2520components%2520to%2520each%2520class%2520%2528region%2529.%2520They%2520have%2520a%2520common%2520encoder%2520part%2520of%2520the%2520architecture%2520but%2520branch%2520out%2520at%2520the%2520output%2520layers%252C%2520leading%2520to%2520improved%2520accuracy.%250A%2520%2520These%2520methods%2520are%2520used%2520to%2520diagnose%2520type%2520B%2520aortic%2520dissection%2520%2528TBAD%2529%252C%2520which%2520requires%2520accurate%2520segmentation%2520of%2520aortic%2520structures%2520based%2520on%2520the%2520ImageTBDA%2520dataset%252C%2520which%2520contains%2520100%25203D%2520computed%2520tomography%2520angiography%2520%2528CTA%2529%2520images.%2520These%2520images%2520identify%2520three%2520key%2520classes%253A%2520true%2520lumen%2520%2528TL%2529%252C%2520false%2520lumen%2520%2528FL%2529%252C%2520and%2520false%2520lumen%2520thrombus%2520%2528FLT%2529%2520of%2520the%2520aorta%252C%2520which%2520is%2520critical%2520for%2520diagnosis%2520and%2520treatment%2520decisions.%2520In%2520the%2520dataset%252C%252068%2520examples%2520have%2520a%2520false%2520lumen%252C%2520while%2520the%2520remaining%252032%2520do%2520not%252C%2520creating%2520additional%2520complexity%2520for%2520pathology%2520detection.%250A%2520%2520However%252C%2520implementing%2520these%2520CNN%2520methods%2520requires%2520a%2520large%2520amount%2520of%2520high-quality%2520labeled%2520data.%2520Obtaining%2520accurate%2520labels%2520for%2520the%2520regions%2520of%2520interest%2520can%2520be%2520an%2520expensive%2520and%2520time-consuming%2520process%252C%2520particularly%2520for%25203D%2520data.%2520Semi-supervised%2520learning%2520methods%2520allow%2520models%2520to%2520be%2520trained%2520by%2520using%2520both%2520labeled%2520and%2520unlabeled%2520data%252C%2520which%2520is%2520a%2520promising%2520approach%2520for%2520overcoming%2520the%2520challenge%2520of%2520obtaining%2520accurate%2520labels.%2520However%252C%2520these%2520learning%2520methods%2520are%2520not%2520well%2520understood%2520for%2520models%2520with%2520multiple%2520outputs.%250A%2520%2520This%2520paper%2520presents%2520a%2520semi-supervised%2520learning%2520method%2520for%2520models%2520with%2520multiple%2520outputs.%2520The%2520method%2520is%2520based%2520on%2520the%2520additional%2520rotations%2520and%2520flipping%252C%2520and%2520does%2520not%2520assume%2520the%2520probabilistic%2520nature%2520of%2520the%2520model%2527s%2520responses.%2520This%2520makes%2520it%2520a%2520universal%2520approach%252C%2520which%2520is%2520especially%2520important%2520for%2520architectures%2520that%2520involve%2520separate%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%203D%20Segmentation%20for%20Type-B%20Aortic%20Dissection%20with%20Slim%20UNETR&entry.906535625=Denis%20Mikhailapov%20and%20Vladimir%20Berikov&entry.1292438233=Convolutional%20neural%20networks%20%28CNN%29%20for%20multi-class%20segmentation%20of%20medical%20images%20are%20widely%20used%20today.%20Especially%20models%20with%20multiple%20outputs%20that%20can%20separately%20predict%20segmentation%20classes%20%28regions%29%20without%20relying%20on%20a%20probabilistic%20formulation%20of%20the%20segmentation%20of%20regions.%20These%20models%20allow%20for%20more%20precise%20segmentation%20by%20tailoring%20the%20network%27s%20components%20to%20each%20class%20%28region%29.%20They%20have%20a%20common%20encoder%20part%20of%20the%20architecture%20but%20branch%20out%20at%20the%20output%20layers%2C%20leading%20to%20improved%20accuracy.%0A%20%20These%20methods%20are%20used%20to%20diagnose%20type%20B%20aortic%20dissection%20%28TBAD%29%2C%20which%20requires%20accurate%20segmentation%20of%20aortic%20structures%20based%20on%20the%20ImageTBDA%20dataset%2C%20which%20contains%20100%203D%20computed%20tomography%20angiography%20%28CTA%29%20images.%20These%20images%20identify%20three%20key%20classes%3A%20true%20lumen%20%28TL%29%2C%20false%20lumen%20%28FL%29%2C%20and%20false%20lumen%20thrombus%20%28FLT%29%20of%20the%20aorta%2C%20which%20is%20critical%20for%20diagnosis%20and%20treatment%20decisions.%20In%20the%20dataset%2C%2068%20examples%20have%20a%20false%20lumen%2C%20while%20the%20remaining%2032%20do%20not%2C%20creating%20additional%20complexity%20for%20pathology%20detection.%0A%20%20However%2C%20implementing%20these%20CNN%20methods%20requires%20a%20large%20amount%20of%20high-quality%20labeled%20data.%20Obtaining%20accurate%20labels%20for%20the%20regions%20of%20interest%20can%20be%20an%20expensive%20and%20time-consuming%20process%2C%20particularly%20for%203D%20data.%20Semi-supervised%20learning%20methods%20allow%20models%20to%20be%20trained%20by%20using%20both%20labeled%20and%20unlabeled%20data%2C%20which%20is%20a%20promising%20approach%20for%20overcoming%20the%20challenge%20of%20obtaining%20accurate%20labels.%20However%2C%20these%20learning%20methods%20are%20not%20well%20understood%20for%20models%20with%20multiple%20outputs.%0A%20%20This%20paper%20presents%20a%20semi-supervised%20learning%20method%20for%20models%20with%20multiple%20outputs.%20The%20method%20is%20based%20on%20the%20additional%20rotations%20and%20flipping%2C%20and%20does%20not%20assume%20the%20probabilistic%20nature%20of%20the%20model%27s%20responses.%20This%20makes%20it%20a%20universal%20approach%2C%20which%20is%20especially%20important%20for%20architectures%20that%20involve%20separate%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2512.17610v1&entry.124074799=Read"},
{"title": "You Only Train Once: Differentiable Subset Selection for Omics Data", "author": "Daphn\u00e9 Chopard and Jorge da Silva Gon\u00e7alves and Irene Cannistraci and Thomas M. Sutter and Julia E. Vogt", "abstract": "Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.", "link": "http://arxiv.org/abs/2512.17678v1", "date": "2025-12-19", "relevancy": 2.2739, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4866}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4391}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Only%20Train%20Once%3A%20Differentiable%20Subset%20Selection%20for%20Omics%20Data&body=Title%3A%20You%20Only%20Train%20Once%3A%20Differentiable%20Subset%20Selection%20for%20Omics%20Data%0AAuthor%3A%20Daphn%C3%A9%20Chopard%20and%20Jorge%20da%20Silva%20Gon%C3%A7alves%20and%20Irene%20Cannistraci%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt%0AAbstract%3A%20Selecting%20compact%20and%20informative%20gene%20subsets%20from%20single-cell%20transcriptomic%20data%20is%20essential%20for%20biomarker%20discovery%2C%20improving%20interpretability%2C%20and%20cost-effective%20profiling.%20However%2C%20most%20existing%20feature%20selection%20approaches%20either%20operate%20as%20multi-stage%20pipelines%20or%20rely%20on%20post%20hoc%20feature%20attribution%2C%20making%20selection%20and%20prediction%20weakly%20coupled.%20In%20this%20work%2C%20we%20present%20YOTO%20%28you%20only%20train%20once%29%2C%20an%20end-to-end%20framework%20that%20jointly%20identifies%20discrete%20gene%20subsets%20and%20performs%20prediction%20within%20a%20single%20differentiable%20architecture.%20In%20our%20model%2C%20the%20prediction%20task%20directly%20guides%20which%20genes%20are%20selected%2C%20while%20the%20learned%20subsets%2C%20in%20turn%2C%20shape%20the%20predictive%20representation.%20This%20closed%20feedback%20loop%20enables%20the%20model%20to%20iteratively%20refine%20both%20what%20it%20selects%20and%20how%20it%20predicts%20during%20training.%20Unlike%20existing%20approaches%2C%20YOTO%20enforces%20sparsity%20so%20that%20only%20the%20selected%20genes%20contribute%20to%20inference%2C%20eliminating%20the%20need%20to%20train%20additional%20downstream%20classifiers.%20Through%20a%20multi-task%20learning%20design%2C%20the%20model%20learns%20shared%20representations%20across%20related%20objectives%2C%20allowing%20partially%20labeled%20datasets%20to%20inform%20one%20another%2C%20and%20discovering%20gene%20subsets%20that%20generalize%20across%20tasks%20without%20additional%20training%20steps.%20We%20evaluate%20YOTO%20on%20two%20representative%20single-cell%20RNA-seq%20datasets%2C%20showing%20that%20it%20consistently%20outperforms%20state-of-the-art%20baselines.%20These%20results%20demonstrate%20that%20sparse%2C%20end-to-end%2C%20multi-task%20gene%20subset%20selection%20improves%20predictive%20performance%20and%20yields%20compact%20and%20meaningful%20gene%20subsets%2C%20advancing%20biomarker%20discovery%20and%20single-cell%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Only%2520Train%2520Once%253A%2520Differentiable%2520Subset%2520Selection%2520for%2520Omics%2520Data%26entry.906535625%3DDaphn%25C3%25A9%2520Chopard%2520and%2520Jorge%2520da%2520Silva%2520Gon%25C3%25A7alves%2520and%2520Irene%2520Cannistraci%2520and%2520Thomas%2520M.%2520Sutter%2520and%2520Julia%2520E.%2520Vogt%26entry.1292438233%3DSelecting%2520compact%2520and%2520informative%2520gene%2520subsets%2520from%2520single-cell%2520transcriptomic%2520data%2520is%2520essential%2520for%2520biomarker%2520discovery%252C%2520improving%2520interpretability%252C%2520and%2520cost-effective%2520profiling.%2520However%252C%2520most%2520existing%2520feature%2520selection%2520approaches%2520either%2520operate%2520as%2520multi-stage%2520pipelines%2520or%2520rely%2520on%2520post%2520hoc%2520feature%2520attribution%252C%2520making%2520selection%2520and%2520prediction%2520weakly%2520coupled.%2520In%2520this%2520work%252C%2520we%2520present%2520YOTO%2520%2528you%2520only%2520train%2520once%2529%252C%2520an%2520end-to-end%2520framework%2520that%2520jointly%2520identifies%2520discrete%2520gene%2520subsets%2520and%2520performs%2520prediction%2520within%2520a%2520single%2520differentiable%2520architecture.%2520In%2520our%2520model%252C%2520the%2520prediction%2520task%2520directly%2520guides%2520which%2520genes%2520are%2520selected%252C%2520while%2520the%2520learned%2520subsets%252C%2520in%2520turn%252C%2520shape%2520the%2520predictive%2520representation.%2520This%2520closed%2520feedback%2520loop%2520enables%2520the%2520model%2520to%2520iteratively%2520refine%2520both%2520what%2520it%2520selects%2520and%2520how%2520it%2520predicts%2520during%2520training.%2520Unlike%2520existing%2520approaches%252C%2520YOTO%2520enforces%2520sparsity%2520so%2520that%2520only%2520the%2520selected%2520genes%2520contribute%2520to%2520inference%252C%2520eliminating%2520the%2520need%2520to%2520train%2520additional%2520downstream%2520classifiers.%2520Through%2520a%2520multi-task%2520learning%2520design%252C%2520the%2520model%2520learns%2520shared%2520representations%2520across%2520related%2520objectives%252C%2520allowing%2520partially%2520labeled%2520datasets%2520to%2520inform%2520one%2520another%252C%2520and%2520discovering%2520gene%2520subsets%2520that%2520generalize%2520across%2520tasks%2520without%2520additional%2520training%2520steps.%2520We%2520evaluate%2520YOTO%2520on%2520two%2520representative%2520single-cell%2520RNA-seq%2520datasets%252C%2520showing%2520that%2520it%2520consistently%2520outperforms%2520state-of-the-art%2520baselines.%2520These%2520results%2520demonstrate%2520that%2520sparse%252C%2520end-to-end%252C%2520multi-task%2520gene%2520subset%2520selection%2520improves%2520predictive%2520performance%2520and%2520yields%2520compact%2520and%2520meaningful%2520gene%2520subsets%252C%2520advancing%2520biomarker%2520discovery%2520and%2520single-cell%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Only%20Train%20Once%3A%20Differentiable%20Subset%20Selection%20for%20Omics%20Data&entry.906535625=Daphn%C3%A9%20Chopard%20and%20Jorge%20da%20Silva%20Gon%C3%A7alves%20and%20Irene%20Cannistraci%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt&entry.1292438233=Selecting%20compact%20and%20informative%20gene%20subsets%20from%20single-cell%20transcriptomic%20data%20is%20essential%20for%20biomarker%20discovery%2C%20improving%20interpretability%2C%20and%20cost-effective%20profiling.%20However%2C%20most%20existing%20feature%20selection%20approaches%20either%20operate%20as%20multi-stage%20pipelines%20or%20rely%20on%20post%20hoc%20feature%20attribution%2C%20making%20selection%20and%20prediction%20weakly%20coupled.%20In%20this%20work%2C%20we%20present%20YOTO%20%28you%20only%20train%20once%29%2C%20an%20end-to-end%20framework%20that%20jointly%20identifies%20discrete%20gene%20subsets%20and%20performs%20prediction%20within%20a%20single%20differentiable%20architecture.%20In%20our%20model%2C%20the%20prediction%20task%20directly%20guides%20which%20genes%20are%20selected%2C%20while%20the%20learned%20subsets%2C%20in%20turn%2C%20shape%20the%20predictive%20representation.%20This%20closed%20feedback%20loop%20enables%20the%20model%20to%20iteratively%20refine%20both%20what%20it%20selects%20and%20how%20it%20predicts%20during%20training.%20Unlike%20existing%20approaches%2C%20YOTO%20enforces%20sparsity%20so%20that%20only%20the%20selected%20genes%20contribute%20to%20inference%2C%20eliminating%20the%20need%20to%20train%20additional%20downstream%20classifiers.%20Through%20a%20multi-task%20learning%20design%2C%20the%20model%20learns%20shared%20representations%20across%20related%20objectives%2C%20allowing%20partially%20labeled%20datasets%20to%20inform%20one%20another%2C%20and%20discovering%20gene%20subsets%20that%20generalize%20across%20tasks%20without%20additional%20training%20steps.%20We%20evaluate%20YOTO%20on%20two%20representative%20single-cell%20RNA-seq%20datasets%2C%20showing%20that%20it%20consistently%20outperforms%20state-of-the-art%20baselines.%20These%20results%20demonstrate%20that%20sparse%2C%20end-to-end%2C%20multi-task%20gene%20subset%20selection%20improves%20predictive%20performance%20and%20yields%20compact%20and%20meaningful%20gene%20subsets%2C%20advancing%20biomarker%20discovery%20and%20single-cell%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.17678v1&entry.124074799=Read"},
{"title": "Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs", "author": "Zhaolin Cai and Huiyu Duan and Zitong Xu and Fan Li and Zhi Liu and Jing Liu and Wei Shen and Xiongkuo Min and Guangtao Zhai", "abstract": "Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \\GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.", "link": "http://arxiv.org/abs/2512.17640v1", "date": "2025-12-19", "relevancy": 2.2664, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5904}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5744}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Human-Object%20Interaction%20Detection%20via%20Differentiable%20Cognitive%20Steering%20of%20Multi-modal%20LLMs&body=Title%3A%20Generative%20Human-Object%20Interaction%20Detection%20via%20Differentiable%20Cognitive%20Steering%20of%20Multi-modal%20LLMs%0AAuthor%3A%20Zhaolin%20Cai%20and%20Huiyu%20Duan%20and%20Zitong%20Xu%20and%20Fan%20Li%20and%20Zhi%20Liu%20and%20Jing%20Liu%20and%20Wei%20Shen%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20Human-object%20interaction%20%28HOI%29%20detection%20aims%20to%20localize%20human-object%20pairs%20and%20the%20interactions%20between%20them.%20Existing%20methods%20operate%20under%20a%20closed-world%20assumption%2C%20treating%20the%20task%20as%20a%20classification%20problem%20over%20a%20small%2C%20predefined%20verb%20set%2C%20which%20struggles%20to%20generalize%20to%20the%20long-tail%20of%20unseen%20or%20ambiguous%20interactions%20in%20the%20wild.%20While%20recent%20multi-modal%20large%20language%20models%20%28MLLMs%29%20possess%20the%20rich%20world%20knowledge%20required%20for%20open-vocabulary%20understanding%2C%20they%20remain%20decoupled%20from%20existing%20HOI%20detectors%20since%20fine-tuning%20them%20is%20computationally%20prohibitive.%20To%20address%20these%20constraints%2C%20we%20propose%20%5CGRASP-HO%7D%2C%20a%20novel%20Generative%20Reasoning%20And%20Steerable%20Perception%20framework%20that%20reformulates%20HOI%20detection%20from%20the%20closed-set%20classification%20task%20to%20the%20open-vocabulary%20generation%20problem.%20To%20bridge%20the%20vision%20and%20cognitive%2C%20we%20first%20extract%20hybrid%20interaction%20representations%2C%20then%20design%20a%20lightweight%20learnable%20cognitive%20steering%20conduit%20%28CSC%29%20module%20to%20inject%20the%20fine-grained%20visual%20evidence%20into%20a%20frozen%20MLLM%20for%20effective%20reasoning.%20To%20address%20the%20supervision%20mismatch%20between%20classification-based%20HOI%20datasets%20and%20open-vocabulary%20generative%20models%2C%20we%20introduce%20a%20hybrid%20guidance%20strategy%20that%20coupling%20the%20language%20modeling%20loss%20and%20auxiliary%20classification%20loss%2C%20enabling%20discriminative%20grounding%20without%20sacrificing%20generative%20flexibility.%20Experiments%20demonstrate%20state-of-the-art%20closed-set%20performance%20and%20strong%20zero-shot%20generalization%2C%20achieving%20a%20unified%20paradigm%20that%20seamlessly%20bridges%20discriminative%20perception%20and%20generative%20reasoning%20for%20open-world%20HOI%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Human-Object%2520Interaction%2520Detection%2520via%2520Differentiable%2520Cognitive%2520Steering%2520of%2520Multi-modal%2520LLMs%26entry.906535625%3DZhaolin%2520Cai%2520and%2520Huiyu%2520Duan%2520and%2520Zitong%2520Xu%2520and%2520Fan%2520Li%2520and%2520Zhi%2520Liu%2520and%2520Jing%2520Liu%2520and%2520Wei%2520Shen%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DHuman-object%2520interaction%2520%2528HOI%2529%2520detection%2520aims%2520to%2520localize%2520human-object%2520pairs%2520and%2520the%2520interactions%2520between%2520them.%2520Existing%2520methods%2520operate%2520under%2520a%2520closed-world%2520assumption%252C%2520treating%2520the%2520task%2520as%2520a%2520classification%2520problem%2520over%2520a%2520small%252C%2520predefined%2520verb%2520set%252C%2520which%2520struggles%2520to%2520generalize%2520to%2520the%2520long-tail%2520of%2520unseen%2520or%2520ambiguous%2520interactions%2520in%2520the%2520wild.%2520While%2520recent%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520possess%2520the%2520rich%2520world%2520knowledge%2520required%2520for%2520open-vocabulary%2520understanding%252C%2520they%2520remain%2520decoupled%2520from%2520existing%2520HOI%2520detectors%2520since%2520fine-tuning%2520them%2520is%2520computationally%2520prohibitive.%2520To%2520address%2520these%2520constraints%252C%2520we%2520propose%2520%255CGRASP-HO%257D%252C%2520a%2520novel%2520Generative%2520Reasoning%2520And%2520Steerable%2520Perception%2520framework%2520that%2520reformulates%2520HOI%2520detection%2520from%2520the%2520closed-set%2520classification%2520task%2520to%2520the%2520open-vocabulary%2520generation%2520problem.%2520To%2520bridge%2520the%2520vision%2520and%2520cognitive%252C%2520we%2520first%2520extract%2520hybrid%2520interaction%2520representations%252C%2520then%2520design%2520a%2520lightweight%2520learnable%2520cognitive%2520steering%2520conduit%2520%2528CSC%2529%2520module%2520to%2520inject%2520the%2520fine-grained%2520visual%2520evidence%2520into%2520a%2520frozen%2520MLLM%2520for%2520effective%2520reasoning.%2520To%2520address%2520the%2520supervision%2520mismatch%2520between%2520classification-based%2520HOI%2520datasets%2520and%2520open-vocabulary%2520generative%2520models%252C%2520we%2520introduce%2520a%2520hybrid%2520guidance%2520strategy%2520that%2520coupling%2520the%2520language%2520modeling%2520loss%2520and%2520auxiliary%2520classification%2520loss%252C%2520enabling%2520discriminative%2520grounding%2520without%2520sacrificing%2520generative%2520flexibility.%2520Experiments%2520demonstrate%2520state-of-the-art%2520closed-set%2520performance%2520and%2520strong%2520zero-shot%2520generalization%252C%2520achieving%2520a%2520unified%2520paradigm%2520that%2520seamlessly%2520bridges%2520discriminative%2520perception%2520and%2520generative%2520reasoning%2520for%2520open-world%2520HOI%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Human-Object%20Interaction%20Detection%20via%20Differentiable%20Cognitive%20Steering%20of%20Multi-modal%20LLMs&entry.906535625=Zhaolin%20Cai%20and%20Huiyu%20Duan%20and%20Zitong%20Xu%20and%20Fan%20Li%20and%20Zhi%20Liu%20and%20Jing%20Liu%20and%20Wei%20Shen%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=Human-object%20interaction%20%28HOI%29%20detection%20aims%20to%20localize%20human-object%20pairs%20and%20the%20interactions%20between%20them.%20Existing%20methods%20operate%20under%20a%20closed-world%20assumption%2C%20treating%20the%20task%20as%20a%20classification%20problem%20over%20a%20small%2C%20predefined%20verb%20set%2C%20which%20struggles%20to%20generalize%20to%20the%20long-tail%20of%20unseen%20or%20ambiguous%20interactions%20in%20the%20wild.%20While%20recent%20multi-modal%20large%20language%20models%20%28MLLMs%29%20possess%20the%20rich%20world%20knowledge%20required%20for%20open-vocabulary%20understanding%2C%20they%20remain%20decoupled%20from%20existing%20HOI%20detectors%20since%20fine-tuning%20them%20is%20computationally%20prohibitive.%20To%20address%20these%20constraints%2C%20we%20propose%20%5CGRASP-HO%7D%2C%20a%20novel%20Generative%20Reasoning%20And%20Steerable%20Perception%20framework%20that%20reformulates%20HOI%20detection%20from%20the%20closed-set%20classification%20task%20to%20the%20open-vocabulary%20generation%20problem.%20To%20bridge%20the%20vision%20and%20cognitive%2C%20we%20first%20extract%20hybrid%20interaction%20representations%2C%20then%20design%20a%20lightweight%20learnable%20cognitive%20steering%20conduit%20%28CSC%29%20module%20to%20inject%20the%20fine-grained%20visual%20evidence%20into%20a%20frozen%20MLLM%20for%20effective%20reasoning.%20To%20address%20the%20supervision%20mismatch%20between%20classification-based%20HOI%20datasets%20and%20open-vocabulary%20generative%20models%2C%20we%20introduce%20a%20hybrid%20guidance%20strategy%20that%20coupling%20the%20language%20modeling%20loss%20and%20auxiliary%20classification%20loss%2C%20enabling%20discriminative%20grounding%20without%20sacrificing%20generative%20flexibility.%20Experiments%20demonstrate%20state-of-the-art%20closed-set%20performance%20and%20strong%20zero-shot%20generalization%2C%20achieving%20a%20unified%20paradigm%20that%20seamlessly%20bridges%20discriminative%20perception%20and%20generative%20reasoning%20for%20open-world%20HOI%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.17640v1&entry.124074799=Read"},
{"title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments", "author": "Yuze Wu and Mo Zhu and Xingxing Li and Yuheng Du and Yuxin Fan and Wenjun Li and Zhichao Han and Xin Zhou and Fei Gao", "abstract": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.", "link": "http://arxiv.org/abs/2512.15258v2", "date": "2025-12-19", "relevancy": 2.2627, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.576}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.564}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA-AN%3A%20An%20Efficient%20and%20Onboard%20Vision-Language-Action%20Framework%20for%20Aerial%20Navigation%20in%20Complex%20Environments&body=Title%3A%20VLA-AN%3A%20An%20Efficient%20and%20Onboard%20Vision-Language-Action%20Framework%20for%20Aerial%20Navigation%20in%20Complex%20Environments%0AAuthor%3A%20Yuze%20Wu%20and%20Mo%20Zhu%20and%20Xingxing%20Li%20and%20Yuheng%20Du%20and%20Yuxin%20Fan%20and%20Wenjun%20Li%20and%20Zhichao%20Han%20and%20Xin%20Zhou%20and%20Fei%20Gao%0AAbstract%3A%20This%20paper%20proposes%20VLA-AN%2C%20an%20efficient%20and%20onboard%20Vision-Language-Action%20%28VLA%29%20framework%20dedicated%20to%20autonomous%20drone%20navigation%20in%20complex%20environments.%20VLA-AN%20addresses%20four%20major%20limitations%20of%20existing%20large%20aerial%20navigation%20models%3A%20the%20data%20domain%20gap%2C%20insufficient%20temporal%20navigation%20with%20reasoning%2C%20safety%20issues%20with%20generative%20action%20policies%2C%20and%20onboard%20deployment%20constraints.%20First%2C%20we%20construct%20a%20high-fidelity%20dataset%20utilizing%203D%20Gaussian%20Splatting%20%283D-GS%29%20to%20effectively%20bridge%20the%20domain%20gap.%20Second%2C%20we%20introduce%20a%20progressive%20three-stage%20training%20framework%20that%20sequentially%20reinforces%20scene%20comprehension%2C%20core%20flight%20skills%2C%20and%20complex%20navigation%20capabilities.%20Third%2C%20we%20design%20a%20lightweight%2C%20real-time%20action%20module%20coupled%20with%20geometric%20safety%20correction.%20This%20module%20ensures%20fast%2C%20collision-free%2C%20and%20stable%20command%20generation%2C%20mitigating%20the%20safety%20risks%20inherent%20in%20stochastic%20generative%20policies.%20Finally%2C%20through%20deep%20optimization%20of%20the%20onboard%20deployment%20pipeline%2C%20VLA-AN%20achieves%20a%20robust%20real-time%208.3x%20improvement%20in%20inference%20throughput%20on%20resource-constrained%20UAVs.%20Extensive%20experiments%20demonstrate%20that%20VLA-AN%20significantly%20improves%20spatial%20grounding%2C%20scene%20reasoning%2C%20and%20long-horizon%20navigation%2C%20achieving%20a%20maximum%20single-task%20success%20rate%20of%2098.1%25%2C%20and%20providing%20an%20efficient%2C%20practical%20solution%20for%20realizing%20full-chain%20closed-loop%20autonomy%20in%20lightweight%20aerial%20robots.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA-AN%253A%2520An%2520Efficient%2520and%2520Onboard%2520Vision-Language-Action%2520Framework%2520for%2520Aerial%2520Navigation%2520in%2520Complex%2520Environments%26entry.906535625%3DYuze%2520Wu%2520and%2520Mo%2520Zhu%2520and%2520Xingxing%2520Li%2520and%2520Yuheng%2520Du%2520and%2520Yuxin%2520Fan%2520and%2520Wenjun%2520Li%2520and%2520Zhichao%2520Han%2520and%2520Xin%2520Zhou%2520and%2520Fei%2520Gao%26entry.1292438233%3DThis%2520paper%2520proposes%2520VLA-AN%252C%2520an%2520efficient%2520and%2520onboard%2520Vision-Language-Action%2520%2528VLA%2529%2520framework%2520dedicated%2520to%2520autonomous%2520drone%2520navigation%2520in%2520complex%2520environments.%2520VLA-AN%2520addresses%2520four%2520major%2520limitations%2520of%2520existing%2520large%2520aerial%2520navigation%2520models%253A%2520the%2520data%2520domain%2520gap%252C%2520insufficient%2520temporal%2520navigation%2520with%2520reasoning%252C%2520safety%2520issues%2520with%2520generative%2520action%2520policies%252C%2520and%2520onboard%2520deployment%2520constraints.%2520First%252C%2520we%2520construct%2520a%2520high-fidelity%2520dataset%2520utilizing%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520to%2520effectively%2520bridge%2520the%2520domain%2520gap.%2520Second%252C%2520we%2520introduce%2520a%2520progressive%2520three-stage%2520training%2520framework%2520that%2520sequentially%2520reinforces%2520scene%2520comprehension%252C%2520core%2520flight%2520skills%252C%2520and%2520complex%2520navigation%2520capabilities.%2520Third%252C%2520we%2520design%2520a%2520lightweight%252C%2520real-time%2520action%2520module%2520coupled%2520with%2520geometric%2520safety%2520correction.%2520This%2520module%2520ensures%2520fast%252C%2520collision-free%252C%2520and%2520stable%2520command%2520generation%252C%2520mitigating%2520the%2520safety%2520risks%2520inherent%2520in%2520stochastic%2520generative%2520policies.%2520Finally%252C%2520through%2520deep%2520optimization%2520of%2520the%2520onboard%2520deployment%2520pipeline%252C%2520VLA-AN%2520achieves%2520a%2520robust%2520real-time%25208.3x%2520improvement%2520in%2520inference%2520throughput%2520on%2520resource-constrained%2520UAVs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520VLA-AN%2520significantly%2520improves%2520spatial%2520grounding%252C%2520scene%2520reasoning%252C%2520and%2520long-horizon%2520navigation%252C%2520achieving%2520a%2520maximum%2520single-task%2520success%2520rate%2520of%252098.1%2525%252C%2520and%2520providing%2520an%2520efficient%252C%2520practical%2520solution%2520for%2520realizing%2520full-chain%2520closed-loop%2520autonomy%2520in%2520lightweight%2520aerial%2520robots.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA-AN%3A%20An%20Efficient%20and%20Onboard%20Vision-Language-Action%20Framework%20for%20Aerial%20Navigation%20in%20Complex%20Environments&entry.906535625=Yuze%20Wu%20and%20Mo%20Zhu%20and%20Xingxing%20Li%20and%20Yuheng%20Du%20and%20Yuxin%20Fan%20and%20Wenjun%20Li%20and%20Zhichao%20Han%20and%20Xin%20Zhou%20and%20Fei%20Gao&entry.1292438233=This%20paper%20proposes%20VLA-AN%2C%20an%20efficient%20and%20onboard%20Vision-Language-Action%20%28VLA%29%20framework%20dedicated%20to%20autonomous%20drone%20navigation%20in%20complex%20environments.%20VLA-AN%20addresses%20four%20major%20limitations%20of%20existing%20large%20aerial%20navigation%20models%3A%20the%20data%20domain%20gap%2C%20insufficient%20temporal%20navigation%20with%20reasoning%2C%20safety%20issues%20with%20generative%20action%20policies%2C%20and%20onboard%20deployment%20constraints.%20First%2C%20we%20construct%20a%20high-fidelity%20dataset%20utilizing%203D%20Gaussian%20Splatting%20%283D-GS%29%20to%20effectively%20bridge%20the%20domain%20gap.%20Second%2C%20we%20introduce%20a%20progressive%20three-stage%20training%20framework%20that%20sequentially%20reinforces%20scene%20comprehension%2C%20core%20flight%20skills%2C%20and%20complex%20navigation%20capabilities.%20Third%2C%20we%20design%20a%20lightweight%2C%20real-time%20action%20module%20coupled%20with%20geometric%20safety%20correction.%20This%20module%20ensures%20fast%2C%20collision-free%2C%20and%20stable%20command%20generation%2C%20mitigating%20the%20safety%20risks%20inherent%20in%20stochastic%20generative%20policies.%20Finally%2C%20through%20deep%20optimization%20of%20the%20onboard%20deployment%20pipeline%2C%20VLA-AN%20achieves%20a%20robust%20real-time%208.3x%20improvement%20in%20inference%20throughput%20on%20resource-constrained%20UAVs.%20Extensive%20experiments%20demonstrate%20that%20VLA-AN%20significantly%20improves%20spatial%20grounding%2C%20scene%20reasoning%2C%20and%20long-horizon%20navigation%2C%20achieving%20a%20maximum%20single-task%20success%20rate%20of%2098.1%25%2C%20and%20providing%20an%20efficient%2C%20practical%20solution%20for%20realizing%20full-chain%20closed-loop%20autonomy%20in%20lightweight%20aerial%20robots.&entry.1838667208=http%3A//arxiv.org/abs/2512.15258v2&entry.124074799=Read"},
{"title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding", "author": "Jiaqi Tang and Jianmin Chen and Wei Wei and Xiaogang Xu and Runtao Liu and Xiangyu Wu and Qipeng Xie and Jiafei Wu and Lei Zhang and Qifeng Chen", "abstract": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.", "link": "http://arxiv.org/abs/2512.17532v1", "date": "2025-12-19", "relevancy": 2.262, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust-R1%3A%20Degradation-Aware%20Reasoning%20for%20Robust%20Visual%20Understanding&body=Title%3A%20Robust-R1%3A%20Degradation-Aware%20Reasoning%20for%20Robust%20Visual%20Understanding%0AAuthor%3A%20Jiaqi%20Tang%20and%20Jianmin%20Chen%20and%20Wei%20Wei%20and%20Xiaogang%20Xu%20and%20Runtao%20Liu%20and%20Xiangyu%20Wu%20and%20Qipeng%20Xie%20and%20Jiafei%20Wu%20and%20Lei%20Zhang%20and%20Qifeng%20Chen%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20struggle%20to%20maintain%20reliable%20performance%20under%20extreme%20real-world%20visual%20degradations%2C%20which%20impede%20their%20practical%20robustness.%20Existing%20robust%20MLLMs%20predominantly%20rely%20on%20implicit%20training/adaptation%20that%20focuses%20solely%20on%20visual%20encoder%20generalization%2C%20suffering%20from%20limited%20interpretability%20and%20isolated%20optimization.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Robust-R1%2C%20a%20novel%20framework%20that%20explicitly%20models%20visual%20degradations%20through%20structured%20reasoning%20chains.%20Our%20approach%20integrates%3A%20%28i%29%20supervised%20fine-tuning%20for%20degradation-aware%20reasoning%20foundations%2C%20%28ii%29%20reward-driven%20alignment%20for%20accurately%20perceiving%20degradation%20parameters%2C%20and%20%28iii%29%20dynamic%20reasoning%20depth%20scaling%20adapted%20to%20degradation%20intensity.%20To%20facilitate%20this%20approach%2C%20we%20introduce%20a%20specialized%2011K%20dataset%20featuring%20realistic%20degradations%20synthesized%20across%20four%20critical%20real-world%20visual%20processing%20stages%2C%20each%20annotated%20with%20structured%20chains%20connecting%20degradation%20parameters%2C%20perceptual%20influence%2C%20pristine%20semantic%20reasoning%20chain%2C%20and%20conclusion.%20Comprehensive%20evaluations%20demonstrate%20state-of-the-art%20robustness%3A%20Robust-R1%20outperforms%20all%20general%20and%20robust%20baselines%20on%20the%20real-world%20degradation%20benchmark%20R-Bench%2C%20while%20maintaining%20superior%20anti-degradation%20performance%20under%20multi-intensity%20adversarial%20degradations%20on%20MMMB%2C%20MMStar%2C%20and%20RealWorldQA.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust-R1%253A%2520Degradation-Aware%2520Reasoning%2520for%2520Robust%2520Visual%2520Understanding%26entry.906535625%3DJiaqi%2520Tang%2520and%2520Jianmin%2520Chen%2520and%2520Wei%2520Wei%2520and%2520Xiaogang%2520Xu%2520and%2520Runtao%2520Liu%2520and%2520Xiangyu%2520Wu%2520and%2520Qipeng%2520Xie%2520and%2520Jiafei%2520Wu%2520and%2520Lei%2520Zhang%2520and%2520Qifeng%2520Chen%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520struggle%2520to%2520maintain%2520reliable%2520performance%2520under%2520extreme%2520real-world%2520visual%2520degradations%252C%2520which%2520impede%2520their%2520practical%2520robustness.%2520Existing%2520robust%2520MLLMs%2520predominantly%2520rely%2520on%2520implicit%2520training/adaptation%2520that%2520focuses%2520solely%2520on%2520visual%2520encoder%2520generalization%252C%2520suffering%2520from%2520limited%2520interpretability%2520and%2520isolated%2520optimization.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Robust-R1%252C%2520a%2520novel%2520framework%2520that%2520explicitly%2520models%2520visual%2520degradations%2520through%2520structured%2520reasoning%2520chains.%2520Our%2520approach%2520integrates%253A%2520%2528i%2529%2520supervised%2520fine-tuning%2520for%2520degradation-aware%2520reasoning%2520foundations%252C%2520%2528ii%2529%2520reward-driven%2520alignment%2520for%2520accurately%2520perceiving%2520degradation%2520parameters%252C%2520and%2520%2528iii%2529%2520dynamic%2520reasoning%2520depth%2520scaling%2520adapted%2520to%2520degradation%2520intensity.%2520To%2520facilitate%2520this%2520approach%252C%2520we%2520introduce%2520a%2520specialized%252011K%2520dataset%2520featuring%2520realistic%2520degradations%2520synthesized%2520across%2520four%2520critical%2520real-world%2520visual%2520processing%2520stages%252C%2520each%2520annotated%2520with%2520structured%2520chains%2520connecting%2520degradation%2520parameters%252C%2520perceptual%2520influence%252C%2520pristine%2520semantic%2520reasoning%2520chain%252C%2520and%2520conclusion.%2520Comprehensive%2520evaluations%2520demonstrate%2520state-of-the-art%2520robustness%253A%2520Robust-R1%2520outperforms%2520all%2520general%2520and%2520robust%2520baselines%2520on%2520the%2520real-world%2520degradation%2520benchmark%2520R-Bench%252C%2520while%2520maintaining%2520superior%2520anti-degradation%2520performance%2520under%2520multi-intensity%2520adversarial%2520degradations%2520on%2520MMMB%252C%2520MMStar%252C%2520and%2520RealWorldQA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust-R1%3A%20Degradation-Aware%20Reasoning%20for%20Robust%20Visual%20Understanding&entry.906535625=Jiaqi%20Tang%20and%20Jianmin%20Chen%20and%20Wei%20Wei%20and%20Xiaogang%20Xu%20and%20Runtao%20Liu%20and%20Xiangyu%20Wu%20and%20Qipeng%20Xie%20and%20Jiafei%20Wu%20and%20Lei%20Zhang%20and%20Qifeng%20Chen&entry.1292438233=Multimodal%20Large%20Language%20Models%20struggle%20to%20maintain%20reliable%20performance%20under%20extreme%20real-world%20visual%20degradations%2C%20which%20impede%20their%20practical%20robustness.%20Existing%20robust%20MLLMs%20predominantly%20rely%20on%20implicit%20training/adaptation%20that%20focuses%20solely%20on%20visual%20encoder%20generalization%2C%20suffering%20from%20limited%20interpretability%20and%20isolated%20optimization.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Robust-R1%2C%20a%20novel%20framework%20that%20explicitly%20models%20visual%20degradations%20through%20structured%20reasoning%20chains.%20Our%20approach%20integrates%3A%20%28i%29%20supervised%20fine-tuning%20for%20degradation-aware%20reasoning%20foundations%2C%20%28ii%29%20reward-driven%20alignment%20for%20accurately%20perceiving%20degradation%20parameters%2C%20and%20%28iii%29%20dynamic%20reasoning%20depth%20scaling%20adapted%20to%20degradation%20intensity.%20To%20facilitate%20this%20approach%2C%20we%20introduce%20a%20specialized%2011K%20dataset%20featuring%20realistic%20degradations%20synthesized%20across%20four%20critical%20real-world%20visual%20processing%20stages%2C%20each%20annotated%20with%20structured%20chains%20connecting%20degradation%20parameters%2C%20perceptual%20influence%2C%20pristine%20semantic%20reasoning%20chain%2C%20and%20conclusion.%20Comprehensive%20evaluations%20demonstrate%20state-of-the-art%20robustness%3A%20Robust-R1%20outperforms%20all%20general%20and%20robust%20baselines%20on%20the%20real-world%20degradation%20benchmark%20R-Bench%2C%20while%20maintaining%20superior%20anti-degradation%20performance%20under%20multi-intensity%20adversarial%20degradations%20on%20MMMB%2C%20MMStar%2C%20and%20RealWorldQA.&entry.1838667208=http%3A//arxiv.org/abs/2512.17532v1&entry.124074799=Read"},
{"title": "A Survey on Archetypal Analysis", "author": "Aleix Alcacer and Irene Epifanio and Sebastian Mair and Morten M\u00f8rup", "abstract": "Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and Leo Breiman as a computational procedure for extracting distinct aspects, so-called archetypes, from observations, with each observational record approximated as a mixture (i.e., convex combination) of these archetypes. AA thereby provides straightforward, interpretable, and explainable representations for feature extraction and dimensionality reduction, facilitating the understanding of the structure of high-dimensional data and enabling wide applications across the sciences. However, AA also faces challenges, particularly as the associated optimization problem is non-convex. This is the first survey that provides researchers and data mining practitioners with an overview of the methodologies and opportunities that AA offers, surveying the many applications of AA across disparate fields of science, as well as best practices for modeling data with AA and its limitations. The survey concludes by explaining crucial future research directions concerning AA.", "link": "http://arxiv.org/abs/2504.12392v2", "date": "2025-12-19", "relevancy": 2.246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4601}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Archetypal%20Analysis&body=Title%3A%20A%20Survey%20on%20Archetypal%20Analysis%0AAuthor%3A%20Aleix%20Alcacer%20and%20Irene%20Epifanio%20and%20Sebastian%20Mair%20and%20Morten%20M%C3%B8rup%0AAbstract%3A%20Archetypal%20analysis%20%28AA%29%20was%20originally%20proposed%20in%201994%20by%20Adele%20Cutler%20and%20Leo%20Breiman%20as%20a%20computational%20procedure%20for%20extracting%20distinct%20aspects%2C%20so-called%20archetypes%2C%20from%20observations%2C%20with%20each%20observational%20record%20approximated%20as%20a%20mixture%20%28i.e.%2C%20convex%20combination%29%20of%20these%20archetypes.%20AA%20thereby%20provides%20straightforward%2C%20interpretable%2C%20and%20explainable%20representations%20for%20feature%20extraction%20and%20dimensionality%20reduction%2C%20facilitating%20the%20understanding%20of%20the%20structure%20of%20high-dimensional%20data%20and%20enabling%20wide%20applications%20across%20the%20sciences.%20However%2C%20AA%20also%20faces%20challenges%2C%20particularly%20as%20the%20associated%20optimization%20problem%20is%20non-convex.%20This%20is%20the%20first%20survey%20that%20provides%20researchers%20and%20data%20mining%20practitioners%20with%20an%20overview%20of%20the%20methodologies%20and%20opportunities%20that%20AA%20offers%2C%20surveying%20the%20many%20applications%20of%20AA%20across%20disparate%20fields%20of%20science%2C%20as%20well%20as%20best%20practices%20for%20modeling%20data%20with%20AA%20and%20its%20limitations.%20The%20survey%20concludes%20by%20explaining%20crucial%20future%20research%20directions%20concerning%20AA.%0ALink%3A%20http%3A//arxiv.org/abs/2504.12392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Archetypal%2520Analysis%26entry.906535625%3DAleix%2520Alcacer%2520and%2520Irene%2520Epifanio%2520and%2520Sebastian%2520Mair%2520and%2520Morten%2520M%25C3%25B8rup%26entry.1292438233%3DArchetypal%2520analysis%2520%2528AA%2529%2520was%2520originally%2520proposed%2520in%25201994%2520by%2520Adele%2520Cutler%2520and%2520Leo%2520Breiman%2520as%2520a%2520computational%2520procedure%2520for%2520extracting%2520distinct%2520aspects%252C%2520so-called%2520archetypes%252C%2520from%2520observations%252C%2520with%2520each%2520observational%2520record%2520approximated%2520as%2520a%2520mixture%2520%2528i.e.%252C%2520convex%2520combination%2529%2520of%2520these%2520archetypes.%2520AA%2520thereby%2520provides%2520straightforward%252C%2520interpretable%252C%2520and%2520explainable%2520representations%2520for%2520feature%2520extraction%2520and%2520dimensionality%2520reduction%252C%2520facilitating%2520the%2520understanding%2520of%2520the%2520structure%2520of%2520high-dimensional%2520data%2520and%2520enabling%2520wide%2520applications%2520across%2520the%2520sciences.%2520However%252C%2520AA%2520also%2520faces%2520challenges%252C%2520particularly%2520as%2520the%2520associated%2520optimization%2520problem%2520is%2520non-convex.%2520This%2520is%2520the%2520first%2520survey%2520that%2520provides%2520researchers%2520and%2520data%2520mining%2520practitioners%2520with%2520an%2520overview%2520of%2520the%2520methodologies%2520and%2520opportunities%2520that%2520AA%2520offers%252C%2520surveying%2520the%2520many%2520applications%2520of%2520AA%2520across%2520disparate%2520fields%2520of%2520science%252C%2520as%2520well%2520as%2520best%2520practices%2520for%2520modeling%2520data%2520with%2520AA%2520and%2520its%2520limitations.%2520The%2520survey%2520concludes%2520by%2520explaining%2520crucial%2520future%2520research%2520directions%2520concerning%2520AA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Archetypal%20Analysis&entry.906535625=Aleix%20Alcacer%20and%20Irene%20Epifanio%20and%20Sebastian%20Mair%20and%20Morten%20M%C3%B8rup&entry.1292438233=Archetypal%20analysis%20%28AA%29%20was%20originally%20proposed%20in%201994%20by%20Adele%20Cutler%20and%20Leo%20Breiman%20as%20a%20computational%20procedure%20for%20extracting%20distinct%20aspects%2C%20so-called%20archetypes%2C%20from%20observations%2C%20with%20each%20observational%20record%20approximated%20as%20a%20mixture%20%28i.e.%2C%20convex%20combination%29%20of%20these%20archetypes.%20AA%20thereby%20provides%20straightforward%2C%20interpretable%2C%20and%20explainable%20representations%20for%20feature%20extraction%20and%20dimensionality%20reduction%2C%20facilitating%20the%20understanding%20of%20the%20structure%20of%20high-dimensional%20data%20and%20enabling%20wide%20applications%20across%20the%20sciences.%20However%2C%20AA%20also%20faces%20challenges%2C%20particularly%20as%20the%20associated%20optimization%20problem%20is%20non-convex.%20This%20is%20the%20first%20survey%20that%20provides%20researchers%20and%20data%20mining%20practitioners%20with%20an%20overview%20of%20the%20methodologies%20and%20opportunities%20that%20AA%20offers%2C%20surveying%20the%20many%20applications%20of%20AA%20across%20disparate%20fields%20of%20science%2C%20as%20well%20as%20best%20practices%20for%20modeling%20data%20with%20AA%20and%20its%20limitations.%20The%20survey%20concludes%20by%20explaining%20crucial%20future%20research%20directions%20concerning%20AA.&entry.1838667208=http%3A//arxiv.org/abs/2504.12392v2&entry.124074799=Read"},
{"title": "SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning", "author": "Paul Mangold and Sergey Samsonov and Safwan Labbi and Ilya Levin and Reda Alami and Alexey Naumov and Eric Moulines", "abstract": "In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy $\u03b5$. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.", "link": "http://arxiv.org/abs/2402.04114v3", "date": "2025-12-19", "relevancy": 2.2378, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4529}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4496}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAFFLSA%3A%20Taming%20Heterogeneity%20in%20Federated%20Linear%20Stochastic%20Approximation%20and%20TD%20Learning&body=Title%3A%20SCAFFLSA%3A%20Taming%20Heterogeneity%20in%20Federated%20Linear%20Stochastic%20Approximation%20and%20TD%20Learning%0AAuthor%3A%20Paul%20Mangold%20and%20Sergey%20Samsonov%20and%20Safwan%20Labbi%20and%20Ilya%20Levin%20and%20Reda%20Alami%20and%20Alexey%20Naumov%20and%20Eric%20Moulines%0AAbstract%3A%20In%20this%20paper%2C%20we%20analyze%20the%20sample%20and%20communication%20complexity%20of%20the%20federated%20linear%20stochastic%20approximation%20%28FedLSA%29%20algorithm.%20We%20explicitly%20quantify%20the%20effects%20of%20local%20training%20with%20agent%20heterogeneity.%20We%20show%20that%20the%20communication%20complexity%20of%20FedLSA%20scales%20polynomially%20with%20the%20inverse%20of%20the%20desired%20accuracy%20%24%CE%B5%24.%20To%20overcome%20this%2C%20we%20propose%20SCAFFLSA%20a%20new%20variant%20of%20FedLSA%20that%20uses%20control%20variates%20to%20correct%20for%20client%20drift%2C%20and%20establish%20its%20sample%20and%20communication%20complexities.%20We%20show%20that%20for%20statistically%20heterogeneous%20agents%2C%20its%20communication%20complexity%20scales%20logarithmically%20with%20the%20desired%20accuracy%2C%20similar%20to%20Scaffnew.%20An%20important%20finding%20is%20that%2C%20compared%20to%20the%20existing%20results%20for%20Scaffnew%2C%20the%20sample%20complexity%20scales%20with%20the%20inverse%20of%20the%20number%20of%20agents%2C%20a%20property%20referred%20to%20as%20linear%20speed-up.%20Achieving%20this%20linear%20speed-up%20requires%20completely%20new%20theoretical%20arguments.%20We%20apply%20the%20proposed%20method%20to%20federated%20temporal%20difference%20learning%20with%20linear%20function%20approximation%20and%20analyze%20the%20corresponding%20complexity%20improvements.%0ALink%3A%20http%3A//arxiv.org/abs/2402.04114v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAFFLSA%253A%2520Taming%2520Heterogeneity%2520in%2520Federated%2520Linear%2520Stochastic%2520Approximation%2520and%2520TD%2520Learning%26entry.906535625%3DPaul%2520Mangold%2520and%2520Sergey%2520Samsonov%2520and%2520Safwan%2520Labbi%2520and%2520Ilya%2520Levin%2520and%2520Reda%2520Alami%2520and%2520Alexey%2520Naumov%2520and%2520Eric%2520Moulines%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520analyze%2520the%2520sample%2520and%2520communication%2520complexity%2520of%2520the%2520federated%2520linear%2520stochastic%2520approximation%2520%2528FedLSA%2529%2520algorithm.%2520We%2520explicitly%2520quantify%2520the%2520effects%2520of%2520local%2520training%2520with%2520agent%2520heterogeneity.%2520We%2520show%2520that%2520the%2520communication%2520complexity%2520of%2520FedLSA%2520scales%2520polynomially%2520with%2520the%2520inverse%2520of%2520the%2520desired%2520accuracy%2520%2524%25CE%25B5%2524.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520SCAFFLSA%2520a%2520new%2520variant%2520of%2520FedLSA%2520that%2520uses%2520control%2520variates%2520to%2520correct%2520for%2520client%2520drift%252C%2520and%2520establish%2520its%2520sample%2520and%2520communication%2520complexities.%2520We%2520show%2520that%2520for%2520statistically%2520heterogeneous%2520agents%252C%2520its%2520communication%2520complexity%2520scales%2520logarithmically%2520with%2520the%2520desired%2520accuracy%252C%2520similar%2520to%2520Scaffnew.%2520An%2520important%2520finding%2520is%2520that%252C%2520compared%2520to%2520the%2520existing%2520results%2520for%2520Scaffnew%252C%2520the%2520sample%2520complexity%2520scales%2520with%2520the%2520inverse%2520of%2520the%2520number%2520of%2520agents%252C%2520a%2520property%2520referred%2520to%2520as%2520linear%2520speed-up.%2520Achieving%2520this%2520linear%2520speed-up%2520requires%2520completely%2520new%2520theoretical%2520arguments.%2520We%2520apply%2520the%2520proposed%2520method%2520to%2520federated%2520temporal%2520difference%2520learning%2520with%2520linear%2520function%2520approximation%2520and%2520analyze%2520the%2520corresponding%2520complexity%2520improvements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04114v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAFFLSA%3A%20Taming%20Heterogeneity%20in%20Federated%20Linear%20Stochastic%20Approximation%20and%20TD%20Learning&entry.906535625=Paul%20Mangold%20and%20Sergey%20Samsonov%20and%20Safwan%20Labbi%20and%20Ilya%20Levin%20and%20Reda%20Alami%20and%20Alexey%20Naumov%20and%20Eric%20Moulines&entry.1292438233=In%20this%20paper%2C%20we%20analyze%20the%20sample%20and%20communication%20complexity%20of%20the%20federated%20linear%20stochastic%20approximation%20%28FedLSA%29%20algorithm.%20We%20explicitly%20quantify%20the%20effects%20of%20local%20training%20with%20agent%20heterogeneity.%20We%20show%20that%20the%20communication%20complexity%20of%20FedLSA%20scales%20polynomially%20with%20the%20inverse%20of%20the%20desired%20accuracy%20%24%CE%B5%24.%20To%20overcome%20this%2C%20we%20propose%20SCAFFLSA%20a%20new%20variant%20of%20FedLSA%20that%20uses%20control%20variates%20to%20correct%20for%20client%20drift%2C%20and%20establish%20its%20sample%20and%20communication%20complexities.%20We%20show%20that%20for%20statistically%20heterogeneous%20agents%2C%20its%20communication%20complexity%20scales%20logarithmically%20with%20the%20desired%20accuracy%2C%20similar%20to%20Scaffnew.%20An%20important%20finding%20is%20that%2C%20compared%20to%20the%20existing%20results%20for%20Scaffnew%2C%20the%20sample%20complexity%20scales%20with%20the%20inverse%20of%20the%20number%20of%20agents%2C%20a%20property%20referred%20to%20as%20linear%20speed-up.%20Achieving%20this%20linear%20speed-up%20requires%20completely%20new%20theoretical%20arguments.%20We%20apply%20the%20proposed%20method%20to%20federated%20temporal%20difference%20learning%20with%20linear%20function%20approximation%20and%20analyze%20the%20corresponding%20complexity%20improvements.&entry.1838667208=http%3A//arxiv.org/abs/2402.04114v3&entry.124074799=Read"},
{"title": "The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics", "author": " Inamullah and Imran Razzak and Shoaib Jameel", "abstract": "The unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. The retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. The advancement in imaging technology leveraging Artificial Intelligence has seized this opportunity to bridge the gap between the eye and human health. This track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. The new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. In this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of AI-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. We also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.", "link": "http://arxiv.org/abs/2505.04006v2", "date": "2025-12-19", "relevancy": 2.2232, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4601}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4383}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Eye%20as%20a%20Window%20to%20Systemic%20Health%3A%20A%20Survey%20of%20Retinal%20Imaging%20from%20Classical%20Techniques%20to%20Oculomics&body=Title%3A%20The%20Eye%20as%20a%20Window%20to%20Systemic%20Health%3A%20A%20Survey%20of%20Retinal%20Imaging%20from%20Classical%20Techniques%20to%20Oculomics%0AAuthor%3A%20%20Inamullah%20and%20Imran%20Razzak%20and%20Shoaib%20Jameel%0AAbstract%3A%20The%20unique%20vascularized%20anatomy%20of%20the%20human%20eye%2C%20encased%20in%20the%20retina%2C%20provides%20an%20opportunity%20to%20act%20as%20a%20window%20for%20human%20health.%20The%20retinal%20structure%20assists%20in%20assessing%20the%20early%20detection%2C%20monitoring%20of%20disease%20progression%20and%20intervention%20for%20both%20ocular%20and%20non-ocular%20diseases.%20The%20advancement%20in%20imaging%20technology%20leveraging%20Artificial%20Intelligence%20has%20seized%20this%20opportunity%20to%20bridge%20the%20gap%20between%20the%20eye%20and%20human%20health.%20This%20track%20paves%20the%20way%20for%20unveiling%20systemic%20health%20insight%20from%20the%20ocular%20system%20and%20surrogating%20non-invasive%20markers%20for%20timely%20intervention%20and%20identification.%20The%20new%20frontiers%20of%20oculomics%20in%20ophthalmology%20cover%20both%20ocular%20and%20systemic%20diseases%2C%20and%20getting%20more%20attention%20to%20explore%20them.%20In%20this%20survey%20paper%2C%20we%20explore%20the%20evolution%20of%20retinal%20imaging%20techniques%2C%20the%20dire%20need%20for%20the%20integration%20of%20AI-driven%20analysis%2C%20and%20the%20shift%20of%20retinal%20imaging%20from%20classical%20techniques%20to%20oculomics.%20We%20also%20discuss%20some%20hurdles%20that%20may%20be%20faced%20in%20the%20progression%20of%20oculomics%2C%20highlighting%20the%20research%20gaps%20and%20future%20directions.%0ALink%3A%20http%3A//arxiv.org/abs/2505.04006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Eye%2520as%2520a%2520Window%2520to%2520Systemic%2520Health%253A%2520A%2520Survey%2520of%2520Retinal%2520Imaging%2520from%2520Classical%2520Techniques%2520to%2520Oculomics%26entry.906535625%3D%2520Inamullah%2520and%2520Imran%2520Razzak%2520and%2520Shoaib%2520Jameel%26entry.1292438233%3DThe%2520unique%2520vascularized%2520anatomy%2520of%2520the%2520human%2520eye%252C%2520encased%2520in%2520the%2520retina%252C%2520provides%2520an%2520opportunity%2520to%2520act%2520as%2520a%2520window%2520for%2520human%2520health.%2520The%2520retinal%2520structure%2520assists%2520in%2520assessing%2520the%2520early%2520detection%252C%2520monitoring%2520of%2520disease%2520progression%2520and%2520intervention%2520for%2520both%2520ocular%2520and%2520non-ocular%2520diseases.%2520The%2520advancement%2520in%2520imaging%2520technology%2520leveraging%2520Artificial%2520Intelligence%2520has%2520seized%2520this%2520opportunity%2520to%2520bridge%2520the%2520gap%2520between%2520the%2520eye%2520and%2520human%2520health.%2520This%2520track%2520paves%2520the%2520way%2520for%2520unveiling%2520systemic%2520health%2520insight%2520from%2520the%2520ocular%2520system%2520and%2520surrogating%2520non-invasive%2520markers%2520for%2520timely%2520intervention%2520and%2520identification.%2520The%2520new%2520frontiers%2520of%2520oculomics%2520in%2520ophthalmology%2520cover%2520both%2520ocular%2520and%2520systemic%2520diseases%252C%2520and%2520getting%2520more%2520attention%2520to%2520explore%2520them.%2520In%2520this%2520survey%2520paper%252C%2520we%2520explore%2520the%2520evolution%2520of%2520retinal%2520imaging%2520techniques%252C%2520the%2520dire%2520need%2520for%2520the%2520integration%2520of%2520AI-driven%2520analysis%252C%2520and%2520the%2520shift%2520of%2520retinal%2520imaging%2520from%2520classical%2520techniques%2520to%2520oculomics.%2520We%2520also%2520discuss%2520some%2520hurdles%2520that%2520may%2520be%2520faced%2520in%2520the%2520progression%2520of%2520oculomics%252C%2520highlighting%2520the%2520research%2520gaps%2520and%2520future%2520directions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Eye%20as%20a%20Window%20to%20Systemic%20Health%3A%20A%20Survey%20of%20Retinal%20Imaging%20from%20Classical%20Techniques%20to%20Oculomics&entry.906535625=%20Inamullah%20and%20Imran%20Razzak%20and%20Shoaib%20Jameel&entry.1292438233=The%20unique%20vascularized%20anatomy%20of%20the%20human%20eye%2C%20encased%20in%20the%20retina%2C%20provides%20an%20opportunity%20to%20act%20as%20a%20window%20for%20human%20health.%20The%20retinal%20structure%20assists%20in%20assessing%20the%20early%20detection%2C%20monitoring%20of%20disease%20progression%20and%20intervention%20for%20both%20ocular%20and%20non-ocular%20diseases.%20The%20advancement%20in%20imaging%20technology%20leveraging%20Artificial%20Intelligence%20has%20seized%20this%20opportunity%20to%20bridge%20the%20gap%20between%20the%20eye%20and%20human%20health.%20This%20track%20paves%20the%20way%20for%20unveiling%20systemic%20health%20insight%20from%20the%20ocular%20system%20and%20surrogating%20non-invasive%20markers%20for%20timely%20intervention%20and%20identification.%20The%20new%20frontiers%20of%20oculomics%20in%20ophthalmology%20cover%20both%20ocular%20and%20systemic%20diseases%2C%20and%20getting%20more%20attention%20to%20explore%20them.%20In%20this%20survey%20paper%2C%20we%20explore%20the%20evolution%20of%20retinal%20imaging%20techniques%2C%20the%20dire%20need%20for%20the%20integration%20of%20AI-driven%20analysis%2C%20and%20the%20shift%20of%20retinal%20imaging%20from%20classical%20techniques%20to%20oculomics.%20We%20also%20discuss%20some%20hurdles%20that%20may%20be%20faced%20in%20the%20progression%20of%20oculomics%2C%20highlighting%20the%20research%20gaps%20and%20future%20directions.&entry.1838667208=http%3A//arxiv.org/abs/2505.04006v2&entry.124074799=Read"},
{"title": "Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry", "author": "Ufuk Asil and Efendi Nasibov", "abstract": "This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.", "link": "http://arxiv.org/abs/2512.17505v1", "date": "2025-12-19", "relevancy": 2.2212, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Covariance%20and%20Quaternion-Focused%20Hybrid%20Error-State%20EKF/UKF%20for%20Visual-Inertial%20Odometry&body=Title%3A%20Adaptive%20Covariance%20and%20Quaternion-Focused%20Hybrid%20Error-State%20EKF/UKF%20for%20Visual-Inertial%20Odometry%0AAuthor%3A%20Ufuk%20Asil%20and%20Efendi%20Nasibov%0AAbstract%3A%20This%20study%20presents%20an%20innovative%20hybrid%20Visual-Inertial%20Odometry%20%28VIO%29%20method%20for%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20that%20is%20resilient%20to%20environmental%20challenges%20and%20capable%20of%20dynamically%20assessing%20sensor%20reliability.%20Built%20upon%20a%20loosely%20coupled%20sensor%20fusion%20architecture%2C%20the%20system%20utilizes%20a%20novel%20hybrid%20Quaternion-focused%20Error-State%20EKF/UKF%20%28Qf-ES-EKF/UKF%29%20architecture%20to%20process%20inertial%20measurement%20unit%20%28IMU%29%20data.%20This%20architecture%20first%20propagates%20the%20entire%20state%20using%20an%20Error-State%20Extended%20Kalman%20Filter%20%28ESKF%29%20and%20then%20applies%20a%20targeted%20Scaled%20Unscented%20Kalman%20Filter%20%28SUKF%29%20step%20to%20refine%20only%20the%20orientation.%20This%20sequential%20process%20blends%20the%20accuracy%20of%20SUKF%20in%20quaternion%20estimation%20with%20the%20overall%20computational%20efficiency%20of%20ESKF.%20The%20reliability%20of%20visual%20measurements%20is%20assessed%20via%20a%20dynamic%20sensor%20confidence%20score%20based%20on%20metrics%2C%20such%20as%20image%20entropy%2C%20intensity%20variation%2C%20motion%20blur%2C%20and%20inference%20quality%2C%20adapting%20the%20measurement%20noise%20covariance%20to%20ensure%20stable%20pose%20estimation%20even%20under%20challenging%20conditions.%20Comprehensive%20experimental%20analyses%20on%20the%20EuRoC%20MAV%20dataset%20demonstrate%20key%20advantages%3A%20an%20average%20improvement%20of%2049%25%20in%20position%20accuracy%20in%20challenging%20scenarios%2C%20an%20average%20of%2057%25%20in%20rotation%20accuracy%20over%20ESKF-based%20methods%2C%20and%20SUKF-comparable%20accuracy%20achieved%20with%20approximately%2048%25%20lower%20computational%20cost%20than%20a%20full%20SUKF%20implementation.%20These%20findings%20demonstrate%20that%20the%20presented%20approach%20strikes%20an%20effective%20balance%20between%20computational%20efficiency%20and%20estimation%20accuracy%2C%20and%20significantly%20enhances%20UAV%20pose%20estimation%20performance%20in%20complex%20environments%20with%20varying%20sensor%20reliability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Covariance%2520and%2520Quaternion-Focused%2520Hybrid%2520Error-State%2520EKF/UKF%2520for%2520Visual-Inertial%2520Odometry%26entry.906535625%3DUfuk%2520Asil%2520and%2520Efendi%2520Nasibov%26entry.1292438233%3DThis%2520study%2520presents%2520an%2520innovative%2520hybrid%2520Visual-Inertial%2520Odometry%2520%2528VIO%2529%2520method%2520for%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520that%2520is%2520resilient%2520to%2520environmental%2520challenges%2520and%2520capable%2520of%2520dynamically%2520assessing%2520sensor%2520reliability.%2520Built%2520upon%2520a%2520loosely%2520coupled%2520sensor%2520fusion%2520architecture%252C%2520the%2520system%2520utilizes%2520a%2520novel%2520hybrid%2520Quaternion-focused%2520Error-State%2520EKF/UKF%2520%2528Qf-ES-EKF/UKF%2529%2520architecture%2520to%2520process%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%2520data.%2520This%2520architecture%2520first%2520propagates%2520the%2520entire%2520state%2520using%2520an%2520Error-State%2520Extended%2520Kalman%2520Filter%2520%2528ESKF%2529%2520and%2520then%2520applies%2520a%2520targeted%2520Scaled%2520Unscented%2520Kalman%2520Filter%2520%2528SUKF%2529%2520step%2520to%2520refine%2520only%2520the%2520orientation.%2520This%2520sequential%2520process%2520blends%2520the%2520accuracy%2520of%2520SUKF%2520in%2520quaternion%2520estimation%2520with%2520the%2520overall%2520computational%2520efficiency%2520of%2520ESKF.%2520The%2520reliability%2520of%2520visual%2520measurements%2520is%2520assessed%2520via%2520a%2520dynamic%2520sensor%2520confidence%2520score%2520based%2520on%2520metrics%252C%2520such%2520as%2520image%2520entropy%252C%2520intensity%2520variation%252C%2520motion%2520blur%252C%2520and%2520inference%2520quality%252C%2520adapting%2520the%2520measurement%2520noise%2520covariance%2520to%2520ensure%2520stable%2520pose%2520estimation%2520even%2520under%2520challenging%2520conditions.%2520Comprehensive%2520experimental%2520analyses%2520on%2520the%2520EuRoC%2520MAV%2520dataset%2520demonstrate%2520key%2520advantages%253A%2520an%2520average%2520improvement%2520of%252049%2525%2520in%2520position%2520accuracy%2520in%2520challenging%2520scenarios%252C%2520an%2520average%2520of%252057%2525%2520in%2520rotation%2520accuracy%2520over%2520ESKF-based%2520methods%252C%2520and%2520SUKF-comparable%2520accuracy%2520achieved%2520with%2520approximately%252048%2525%2520lower%2520computational%2520cost%2520than%2520a%2520full%2520SUKF%2520implementation.%2520These%2520findings%2520demonstrate%2520that%2520the%2520presented%2520approach%2520strikes%2520an%2520effective%2520balance%2520between%2520computational%2520efficiency%2520and%2520estimation%2520accuracy%252C%2520and%2520significantly%2520enhances%2520UAV%2520pose%2520estimation%2520performance%2520in%2520complex%2520environments%2520with%2520varying%2520sensor%2520reliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Covariance%20and%20Quaternion-Focused%20Hybrid%20Error-State%20EKF/UKF%20for%20Visual-Inertial%20Odometry&entry.906535625=Ufuk%20Asil%20and%20Efendi%20Nasibov&entry.1292438233=This%20study%20presents%20an%20innovative%20hybrid%20Visual-Inertial%20Odometry%20%28VIO%29%20method%20for%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20that%20is%20resilient%20to%20environmental%20challenges%20and%20capable%20of%20dynamically%20assessing%20sensor%20reliability.%20Built%20upon%20a%20loosely%20coupled%20sensor%20fusion%20architecture%2C%20the%20system%20utilizes%20a%20novel%20hybrid%20Quaternion-focused%20Error-State%20EKF/UKF%20%28Qf-ES-EKF/UKF%29%20architecture%20to%20process%20inertial%20measurement%20unit%20%28IMU%29%20data.%20This%20architecture%20first%20propagates%20the%20entire%20state%20using%20an%20Error-State%20Extended%20Kalman%20Filter%20%28ESKF%29%20and%20then%20applies%20a%20targeted%20Scaled%20Unscented%20Kalman%20Filter%20%28SUKF%29%20step%20to%20refine%20only%20the%20orientation.%20This%20sequential%20process%20blends%20the%20accuracy%20of%20SUKF%20in%20quaternion%20estimation%20with%20the%20overall%20computational%20efficiency%20of%20ESKF.%20The%20reliability%20of%20visual%20measurements%20is%20assessed%20via%20a%20dynamic%20sensor%20confidence%20score%20based%20on%20metrics%2C%20such%20as%20image%20entropy%2C%20intensity%20variation%2C%20motion%20blur%2C%20and%20inference%20quality%2C%20adapting%20the%20measurement%20noise%20covariance%20to%20ensure%20stable%20pose%20estimation%20even%20under%20challenging%20conditions.%20Comprehensive%20experimental%20analyses%20on%20the%20EuRoC%20MAV%20dataset%20demonstrate%20key%20advantages%3A%20an%20average%20improvement%20of%2049%25%20in%20position%20accuracy%20in%20challenging%20scenarios%2C%20an%20average%20of%2057%25%20in%20rotation%20accuracy%20over%20ESKF-based%20methods%2C%20and%20SUKF-comparable%20accuracy%20achieved%20with%20approximately%2048%25%20lower%20computational%20cost%20than%20a%20full%20SUKF%20implementation.%20These%20findings%20demonstrate%20that%20the%20presented%20approach%20strikes%20an%20effective%20balance%20between%20computational%20efficiency%20and%20estimation%20accuracy%2C%20and%20significantly%20enhances%20UAV%20pose%20estimation%20performance%20in%20complex%20environments%20with%20varying%20sensor%20reliability.&entry.1838667208=http%3A//arxiv.org/abs/2512.17505v1&entry.124074799=Read"},
{"title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning", "author": "Haoran Sun and Chen Cai and Huiping Zhuang and Kong Aik Lee and Lap-Pui Chau and Yi Wang", "abstract": "The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.", "link": "http://arxiv.org/abs/2510.16442v2", "date": "2025-12-19", "relevancy": 2.2201, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5881}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5674}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDVD-LLaMA%3A%20Explainable%20Deepfake%20Video%20Detection%20via%20Multimodal%20Large%20Language%20Model%20Reasoning&body=Title%3A%20EDVD-LLaMA%3A%20Explainable%20Deepfake%20Video%20Detection%20via%20Multimodal%20Large%20Language%20Model%20Reasoning%0AAuthor%3A%20Haoran%20Sun%20and%20Chen%20Cai%20and%20Huiping%20Zhuang%20and%20Kong%20Aik%20Lee%20and%20Lap-Pui%20Chau%20and%20Yi%20Wang%0AAbstract%3A%20The%20rapid%20development%20of%20deepfake%20video%20technology%20has%20not%20only%20facilitated%20artistic%20creation%20but%20also%20made%20it%20easier%20to%20spread%20misinformation.%20Traditional%20deepfake%20video%20detection%20%28DVD%29%20methods%20face%20issues%20such%20as%20a%20lack%20of%20transparency%20in%20their%20principles%20and%20insufficient%20generalization%20capabilities%20to%20cope%20with%20evolving%20forgery%20techniques.%20This%20highlights%20an%20urgent%20need%20for%20detectors%20that%20can%20identify%20forged%20content%20and%20provide%20verifiable%20reasoning%20explanations.%20This%20paper%20proposes%20the%20explainable%20deepfake%20video%20detection%20%28EDVD%29%20task%20and%20designs%20the%20EDVD-LLaMA%20multimodal%2C%20a%20large%20language%20model%20%28MLLM%29%20reasoning%20framework%2C%20which%20provides%20traceable%20reasoning%20processes%20alongside%20accurate%20detection%20results%20and%20trustworthy%20explanations.%20Our%20approach%20first%20incorporates%20a%20Spatio-Temporal%20Subtle%20Information%20Tokenization%20%28ST-SIT%29%20to%20extract%20and%20fuse%20global%20and%20local%20cross-frame%20deepfake%20features%2C%20providing%20rich%20spatio-temporal%20semantic%20information%20input%20for%20MLLM%20reasoning.%20Second%2C%20we%20construct%20a%20Fine-grained%20Multimodal%20Chain-of-Thought%20%28Fg-MCoT%29%20mechanism%2C%20which%20introduces%20facial%20feature%20data%20as%20hard%20constraints%20during%20the%20reasoning%20process%20to%20achieve%20pixel-level%20spatio-temporal%20video%20localization%2C%20suppress%20hallucinated%20outputs%2C%20and%20enhance%20the%20reliability%20of%20the%20chain%20of%20thought.%20In%20addition%2C%20we%20build%20an%20Explainable%20Reasoning%20FF%2B%2B%20dataset%20%28ER-FF%2B%2Bset%29%2C%20leveraging%20structured%20data%20to%20annotate%20videos%20and%20ensure%20quality%20control%2C%20thereby%20supporting%20dual%20supervision%20for%20reasoning%20and%20detection.%20Extensive%20experiments%20demonstrate%20that%20EDVD-LLaMA%20achieves%20outstanding%20performance%20and%20robustness%20in%20terms%20of%20detection%20accuracy%2C%20explainability%2C%20and%20its%20ability%20to%20handle%20cross-forgery%20methods%20and%20cross-dataset%20scenarios.%20Compared%20to%20previous%20DVD%20methods%2C%20it%20provides%20a%20more%20explainable%20and%20superior%20solution.%20The%20project%20page%20is%20available%20at%3A%20https%3A//11ouo1.github.io/edvd-llama/.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDVD-LLaMA%253A%2520Explainable%2520Deepfake%2520Video%2520Detection%2520via%2520Multimodal%2520Large%2520Language%2520Model%2520Reasoning%26entry.906535625%3DHaoran%2520Sun%2520and%2520Chen%2520Cai%2520and%2520Huiping%2520Zhuang%2520and%2520Kong%2520Aik%2520Lee%2520and%2520Lap-Pui%2520Chau%2520and%2520Yi%2520Wang%26entry.1292438233%3DThe%2520rapid%2520development%2520of%2520deepfake%2520video%2520technology%2520has%2520not%2520only%2520facilitated%2520artistic%2520creation%2520but%2520also%2520made%2520it%2520easier%2520to%2520spread%2520misinformation.%2520Traditional%2520deepfake%2520video%2520detection%2520%2528DVD%2529%2520methods%2520face%2520issues%2520such%2520as%2520a%2520lack%2520of%2520transparency%2520in%2520their%2520principles%2520and%2520insufficient%2520generalization%2520capabilities%2520to%2520cope%2520with%2520evolving%2520forgery%2520techniques.%2520This%2520highlights%2520an%2520urgent%2520need%2520for%2520detectors%2520that%2520can%2520identify%2520forged%2520content%2520and%2520provide%2520verifiable%2520reasoning%2520explanations.%2520This%2520paper%2520proposes%2520the%2520explainable%2520deepfake%2520video%2520detection%2520%2528EDVD%2529%2520task%2520and%2520designs%2520the%2520EDVD-LLaMA%2520multimodal%252C%2520a%2520large%2520language%2520model%2520%2528MLLM%2529%2520reasoning%2520framework%252C%2520which%2520provides%2520traceable%2520reasoning%2520processes%2520alongside%2520accurate%2520detection%2520results%2520and%2520trustworthy%2520explanations.%2520Our%2520approach%2520first%2520incorporates%2520a%2520Spatio-Temporal%2520Subtle%2520Information%2520Tokenization%2520%2528ST-SIT%2529%2520to%2520extract%2520and%2520fuse%2520global%2520and%2520local%2520cross-frame%2520deepfake%2520features%252C%2520providing%2520rich%2520spatio-temporal%2520semantic%2520information%2520input%2520for%2520MLLM%2520reasoning.%2520Second%252C%2520we%2520construct%2520a%2520Fine-grained%2520Multimodal%2520Chain-of-Thought%2520%2528Fg-MCoT%2529%2520mechanism%252C%2520which%2520introduces%2520facial%2520feature%2520data%2520as%2520hard%2520constraints%2520during%2520the%2520reasoning%2520process%2520to%2520achieve%2520pixel-level%2520spatio-temporal%2520video%2520localization%252C%2520suppress%2520hallucinated%2520outputs%252C%2520and%2520enhance%2520the%2520reliability%2520of%2520the%2520chain%2520of%2520thought.%2520In%2520addition%252C%2520we%2520build%2520an%2520Explainable%2520Reasoning%2520FF%252B%252B%2520dataset%2520%2528ER-FF%252B%252Bset%2529%252C%2520leveraging%2520structured%2520data%2520to%2520annotate%2520videos%2520and%2520ensure%2520quality%2520control%252C%2520thereby%2520supporting%2520dual%2520supervision%2520for%2520reasoning%2520and%2520detection.%2520Extensive%2520experiments%2520demonstrate%2520that%2520EDVD-LLaMA%2520achieves%2520outstanding%2520performance%2520and%2520robustness%2520in%2520terms%2520of%2520detection%2520accuracy%252C%2520explainability%252C%2520and%2520its%2520ability%2520to%2520handle%2520cross-forgery%2520methods%2520and%2520cross-dataset%2520scenarios.%2520Compared%2520to%2520previous%2520DVD%2520methods%252C%2520it%2520provides%2520a%2520more%2520explainable%2520and%2520superior%2520solution.%2520The%2520project%2520page%2520is%2520available%2520at%253A%2520https%253A//11ouo1.github.io/edvd-llama/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDVD-LLaMA%3A%20Explainable%20Deepfake%20Video%20Detection%20via%20Multimodal%20Large%20Language%20Model%20Reasoning&entry.906535625=Haoran%20Sun%20and%20Chen%20Cai%20and%20Huiping%20Zhuang%20and%20Kong%20Aik%20Lee%20and%20Lap-Pui%20Chau%20and%20Yi%20Wang&entry.1292438233=The%20rapid%20development%20of%20deepfake%20video%20technology%20has%20not%20only%20facilitated%20artistic%20creation%20but%20also%20made%20it%20easier%20to%20spread%20misinformation.%20Traditional%20deepfake%20video%20detection%20%28DVD%29%20methods%20face%20issues%20such%20as%20a%20lack%20of%20transparency%20in%20their%20principles%20and%20insufficient%20generalization%20capabilities%20to%20cope%20with%20evolving%20forgery%20techniques.%20This%20highlights%20an%20urgent%20need%20for%20detectors%20that%20can%20identify%20forged%20content%20and%20provide%20verifiable%20reasoning%20explanations.%20This%20paper%20proposes%20the%20explainable%20deepfake%20video%20detection%20%28EDVD%29%20task%20and%20designs%20the%20EDVD-LLaMA%20multimodal%2C%20a%20large%20language%20model%20%28MLLM%29%20reasoning%20framework%2C%20which%20provides%20traceable%20reasoning%20processes%20alongside%20accurate%20detection%20results%20and%20trustworthy%20explanations.%20Our%20approach%20first%20incorporates%20a%20Spatio-Temporal%20Subtle%20Information%20Tokenization%20%28ST-SIT%29%20to%20extract%20and%20fuse%20global%20and%20local%20cross-frame%20deepfake%20features%2C%20providing%20rich%20spatio-temporal%20semantic%20information%20input%20for%20MLLM%20reasoning.%20Second%2C%20we%20construct%20a%20Fine-grained%20Multimodal%20Chain-of-Thought%20%28Fg-MCoT%29%20mechanism%2C%20which%20introduces%20facial%20feature%20data%20as%20hard%20constraints%20during%20the%20reasoning%20process%20to%20achieve%20pixel-level%20spatio-temporal%20video%20localization%2C%20suppress%20hallucinated%20outputs%2C%20and%20enhance%20the%20reliability%20of%20the%20chain%20of%20thought.%20In%20addition%2C%20we%20build%20an%20Explainable%20Reasoning%20FF%2B%2B%20dataset%20%28ER-FF%2B%2Bset%29%2C%20leveraging%20structured%20data%20to%20annotate%20videos%20and%20ensure%20quality%20control%2C%20thereby%20supporting%20dual%20supervision%20for%20reasoning%20and%20detection.%20Extensive%20experiments%20demonstrate%20that%20EDVD-LLaMA%20achieves%20outstanding%20performance%20and%20robustness%20in%20terms%20of%20detection%20accuracy%2C%20explainability%2C%20and%20its%20ability%20to%20handle%20cross-forgery%20methods%20and%20cross-dataset%20scenarios.%20Compared%20to%20previous%20DVD%20methods%2C%20it%20provides%20a%20more%20explainable%20and%20superior%20solution.%20The%20project%20page%20is%20available%20at%3A%20https%3A//11ouo1.github.io/edvd-llama/.&entry.1838667208=http%3A//arxiv.org/abs/2510.16442v2&entry.124074799=Read"},
{"title": "ShareChat: A Dataset of Chatbot Conversations in the Wild", "author": "Yueru Yan and Tuc Nguyen and Bo Su and Melissa Lieffers and Thai Le", "abstract": "While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.", "link": "http://arxiv.org/abs/2512.17843v1", "date": "2025-12-19", "relevancy": 2.1962, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.461}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShareChat%3A%20A%20Dataset%20of%20Chatbot%20Conversations%20in%20the%20Wild&body=Title%3A%20ShareChat%3A%20A%20Dataset%20of%20Chatbot%20Conversations%20in%20the%20Wild%0AAuthor%3A%20Yueru%20Yan%20and%20Tuc%20Nguyen%20and%20Bo%20Su%20and%20Melissa%20Lieffers%20and%20Thai%20Le%0AAbstract%3A%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20evolved%20into%20distinct%20platforms%20with%20unique%20interface%20designs%20and%20capabilities%2C%20existing%20public%20datasets%20treat%20models%20as%20generic%20text%20generators%2C%20stripping%20away%20the%20interface%20context%20that%20actively%20shapes%20user%20interaction.%20To%20address%20this%20limitation%2C%20we%20present%20ShareChat%2C%20a%20large-scale%2C%20cross-platform%20corpus%20comprising%20142%2C808%20conversations%20and%20over%20660%2C000%20turns%20collected%20from%20publicly%20shared%20URLs%20across%20five%20major%20platforms%3A%20ChatGPT%2C%20Claude%2C%20Gemini%2C%20Perplexity%2C%20and%20Grok.%20ShareChat%20distinguishes%20itself%20by%20preserving%20native%20platform%20affordances%20often%20lost%20in%20standard%20logs%2C%20including%20reasoning%20traces%2C%20source%20links%2C%20and%20code%20artifacts%2C%20while%20spanning%20101%20languages%20over%20the%20period%20from%20April%202023%20to%20October%202025.%20Furthermore%2C%20ShareChat%20offers%20substantially%20longer%20context%20windows%20and%20greater%20interaction%20depth%20than%20prior%20datasets.%20We%20demonstrate%20the%20dataset%27s%20multifaceted%20utility%20through%20three%20representative%20analyses%3A%20%281%29%20analyzing%20conversation%20completeness%20to%20measure%20user%20intent%20satisfaction%3B%20%282%29%20evaluating%20source%20citation%20behaviors%20in%20content%20generation%3B%20and%20%283%29%20conducting%20temporal%20analysis%20to%20track%20evolving%20usage%20patterns.%20This%20work%20provides%20the%20community%20with%20a%20vital%20and%20timely%20resource%20for%20understanding%20authentic%20user-LLM%20chatbot%20interactions%20in%20the%20wild.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShareChat%253A%2520A%2520Dataset%2520of%2520Chatbot%2520Conversations%2520in%2520the%2520Wild%26entry.906535625%3DYueru%2520Yan%2520and%2520Tuc%2520Nguyen%2520and%2520Bo%2520Su%2520and%2520Melissa%2520Lieffers%2520and%2520Thai%2520Le%26entry.1292438233%3DWhile%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520evolved%2520into%2520distinct%2520platforms%2520with%2520unique%2520interface%2520designs%2520and%2520capabilities%252C%2520existing%2520public%2520datasets%2520treat%2520models%2520as%2520generic%2520text%2520generators%252C%2520stripping%2520away%2520the%2520interface%2520context%2520that%2520actively%2520shapes%2520user%2520interaction.%2520To%2520address%2520this%2520limitation%252C%2520we%2520present%2520ShareChat%252C%2520a%2520large-scale%252C%2520cross-platform%2520corpus%2520comprising%2520142%252C808%2520conversations%2520and%2520over%2520660%252C000%2520turns%2520collected%2520from%2520publicly%2520shared%2520URLs%2520across%2520five%2520major%2520platforms%253A%2520ChatGPT%252C%2520Claude%252C%2520Gemini%252C%2520Perplexity%252C%2520and%2520Grok.%2520ShareChat%2520distinguishes%2520itself%2520by%2520preserving%2520native%2520platform%2520affordances%2520often%2520lost%2520in%2520standard%2520logs%252C%2520including%2520reasoning%2520traces%252C%2520source%2520links%252C%2520and%2520code%2520artifacts%252C%2520while%2520spanning%2520101%2520languages%2520over%2520the%2520period%2520from%2520April%25202023%2520to%2520October%25202025.%2520Furthermore%252C%2520ShareChat%2520offers%2520substantially%2520longer%2520context%2520windows%2520and%2520greater%2520interaction%2520depth%2520than%2520prior%2520datasets.%2520We%2520demonstrate%2520the%2520dataset%2527s%2520multifaceted%2520utility%2520through%2520three%2520representative%2520analyses%253A%2520%25281%2529%2520analyzing%2520conversation%2520completeness%2520to%2520measure%2520user%2520intent%2520satisfaction%253B%2520%25282%2529%2520evaluating%2520source%2520citation%2520behaviors%2520in%2520content%2520generation%253B%2520and%2520%25283%2529%2520conducting%2520temporal%2520analysis%2520to%2520track%2520evolving%2520usage%2520patterns.%2520This%2520work%2520provides%2520the%2520community%2520with%2520a%2520vital%2520and%2520timely%2520resource%2520for%2520understanding%2520authentic%2520user-LLM%2520chatbot%2520interactions%2520in%2520the%2520wild.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShareChat%3A%20A%20Dataset%20of%20Chatbot%20Conversations%20in%20the%20Wild&entry.906535625=Yueru%20Yan%20and%20Tuc%20Nguyen%20and%20Bo%20Su%20and%20Melissa%20Lieffers%20and%20Thai%20Le&entry.1292438233=While%20Large%20Language%20Models%20%28LLMs%29%20have%20evolved%20into%20distinct%20platforms%20with%20unique%20interface%20designs%20and%20capabilities%2C%20existing%20public%20datasets%20treat%20models%20as%20generic%20text%20generators%2C%20stripping%20away%20the%20interface%20context%20that%20actively%20shapes%20user%20interaction.%20To%20address%20this%20limitation%2C%20we%20present%20ShareChat%2C%20a%20large-scale%2C%20cross-platform%20corpus%20comprising%20142%2C808%20conversations%20and%20over%20660%2C000%20turns%20collected%20from%20publicly%20shared%20URLs%20across%20five%20major%20platforms%3A%20ChatGPT%2C%20Claude%2C%20Gemini%2C%20Perplexity%2C%20and%20Grok.%20ShareChat%20distinguishes%20itself%20by%20preserving%20native%20platform%20affordances%20often%20lost%20in%20standard%20logs%2C%20including%20reasoning%20traces%2C%20source%20links%2C%20and%20code%20artifacts%2C%20while%20spanning%20101%20languages%20over%20the%20period%20from%20April%202023%20to%20October%202025.%20Furthermore%2C%20ShareChat%20offers%20substantially%20longer%20context%20windows%20and%20greater%20interaction%20depth%20than%20prior%20datasets.%20We%20demonstrate%20the%20dataset%27s%20multifaceted%20utility%20through%20three%20representative%20analyses%3A%20%281%29%20analyzing%20conversation%20completeness%20to%20measure%20user%20intent%20satisfaction%3B%20%282%29%20evaluating%20source%20citation%20behaviors%20in%20content%20generation%3B%20and%20%283%29%20conducting%20temporal%20analysis%20to%20track%20evolving%20usage%20patterns.%20This%20work%20provides%20the%20community%20with%20a%20vital%20and%20timely%20resource%20for%20understanding%20authentic%20user-LLM%20chatbot%20interactions%20in%20the%20wild.&entry.1838667208=http%3A//arxiv.org/abs/2512.17843v1&entry.124074799=Read"},
{"title": "Step-GUI Technical Report", "author": "Haolong Yan and Jia Wang and Xin Huang and Yeqing Shen and Ziyang Meng and Zhimin Fan and Kaijun Tan and Jin Gao and Lieyu Shi and Mi Yang and Shiliang Yang and Zhirui Wang and Brian Li and Kang An and Chenyang Li and Lei Lei and Mengmeng Duan and Danxun Liang and Guodong Liu and Hang Cheng and Hao Wu and Jie Dong and Junhao Huang and Mei Chen and Renjie Yu and Shunshan Li and Xu Zhou and Yiting Dai and Yineng Deng and Yingdan Liang and Zelin Chen and Wen Sun and Chengxu Yan and Chunqin Xu and Dong Li and Fengqiong Xiao and Guanghao Fan and Guopeng Li and Guozhen Peng and Hongbing Li and Hang Li and Hongming Chen and Jingjing Xie and Jianyong Li and Jingyang Zhang and Jiaju Ren and Jiayu Yuan and Jianpeng Yin and Kai Cao and Liang Zhao and Liguo Tan and Liying Shi and Mengqiang Ren and Min Xu and Manjiao Liu and Mao Luo and Mingxin Wan and Na Wang and Nan Wu and Ning Wang and Peiyao Ma and Qingzhou Zhang and Qiao Wang and Qinlin Zeng and Qiong Gao and Qiongyao Li and Shangwu Zhong and Shuli Gao and Shaofan Liu and Shisi Gao and Shuang Luo and Xingbin Liu and Xiaojia Liu and Xiaojie Hou and Xin Liu and Xuanti Feng and Xuedan Cai and Xuan Wen and Xianwei Zhu and Xin Liang and Xin Liu and Xin Zhou and Yifan Sui and Yingxiu Zhao and Yukang Shi and Yunfang Xu and Yuqing Zeng and Yixun Zhang and Zejia Weng and Zhonghao Yan and Zhiguo Huang and Zhuoyu Wang and Zihan Yan and Zheng Ge and Jing Li and Yibo Zhu and Binxing Jiao and Xiangyu Zhang and Daxin Jiang", "abstract": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "link": "http://arxiv.org/abs/2512.15431v2", "date": "2025-12-19", "relevancy": 2.1936, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5636}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5377}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-GUI%20Technical%20Report&body=Title%3A%20Step-GUI%20Technical%20Report%0AAuthor%3A%20Haolong%20Yan%20and%20Jia%20Wang%20and%20Xin%20Huang%20and%20Yeqing%20Shen%20and%20Ziyang%20Meng%20and%20Zhimin%20Fan%20and%20Kaijun%20Tan%20and%20Jin%20Gao%20and%20Lieyu%20Shi%20and%20Mi%20Yang%20and%20Shiliang%20Yang%20and%20Zhirui%20Wang%20and%20Brian%20Li%20and%20Kang%20An%20and%20Chenyang%20Li%20and%20Lei%20Lei%20and%20Mengmeng%20Duan%20and%20Danxun%20Liang%20and%20Guodong%20Liu%20and%20Hang%20Cheng%20and%20Hao%20Wu%20and%20Jie%20Dong%20and%20Junhao%20Huang%20and%20Mei%20Chen%20and%20Renjie%20Yu%20and%20Shunshan%20Li%20and%20Xu%20Zhou%20and%20Yiting%20Dai%20and%20Yineng%20Deng%20and%20Yingdan%20Liang%20and%20Zelin%20Chen%20and%20Wen%20Sun%20and%20Chengxu%20Yan%20and%20Chunqin%20Xu%20and%20Dong%20Li%20and%20Fengqiong%20Xiao%20and%20Guanghao%20Fan%20and%20Guopeng%20Li%20and%20Guozhen%20Peng%20and%20Hongbing%20Li%20and%20Hang%20Li%20and%20Hongming%20Chen%20and%20Jingjing%20Xie%20and%20Jianyong%20Li%20and%20Jingyang%20Zhang%20and%20Jiaju%20Ren%20and%20Jiayu%20Yuan%20and%20Jianpeng%20Yin%20and%20Kai%20Cao%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liying%20Shi%20and%20Mengqiang%20Ren%20and%20Min%20Xu%20and%20Manjiao%20Liu%20and%20Mao%20Luo%20and%20Mingxin%20Wan%20and%20Na%20Wang%20and%20Nan%20Wu%20and%20Ning%20Wang%20and%20Peiyao%20Ma%20and%20Qingzhou%20Zhang%20and%20Qiao%20Wang%20and%20Qinlin%20Zeng%20and%20Qiong%20Gao%20and%20Qiongyao%20Li%20and%20Shangwu%20Zhong%20and%20Shuli%20Gao%20and%20Shaofan%20Liu%20and%20Shisi%20Gao%20and%20Shuang%20Luo%20and%20Xingbin%20Liu%20and%20Xiaojia%20Liu%20and%20Xiaojie%20Hou%20and%20Xin%20Liu%20and%20Xuanti%20Feng%20and%20Xuedan%20Cai%20and%20Xuan%20Wen%20and%20Xianwei%20Zhu%20and%20Xin%20Liang%20and%20Xin%20Liu%20and%20Xin%20Zhou%20and%20Yifan%20Sui%20and%20Yingxiu%20Zhao%20and%20Yukang%20Shi%20and%20Yunfang%20Xu%20and%20Yuqing%20Zeng%20and%20Yixun%20Zhang%20and%20Zejia%20Weng%20and%20Zhonghao%20Yan%20and%20Zhiguo%20Huang%20and%20Zhuoyu%20Wang%20and%20Zihan%20Yan%20and%20Zheng%20Ge%20and%20Jing%20Li%20and%20Yibo%20Zhu%20and%20Binxing%20Jiao%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20unlock%20unprecedented%20opportunities%20for%20GUI%20automation.%20However%2C%20a%20fundamental%20challenge%20remains%3A%20how%20to%20efficiently%20acquire%20high-quality%20training%20data%20while%20maintaining%20annotation%20reliability%3F%20We%20introduce%20a%20self-evolving%20training%20pipeline%20powered%20by%20the%20Calibrated%20Step%20Reward%20System%2C%20which%20converts%20model-generated%20trajectories%20into%20reliable%20training%20signals%20through%20trajectory-level%20calibration%2C%20achieving%20%3E90%25%20annotation%20accuracy%20with%2010-100x%20lower%20cost.%20Leveraging%20this%20pipeline%2C%20we%20introduce%20Step-GUI%2C%20a%20family%20of%20models%20%284B/8B%29%20that%20achieves%20state-of-the-art%20GUI%20performance%20%288B%3A%2080.2%25%20AndroidWorld%2C%2048.5%25%20OSWorld%2C%2062.6%25%20ScreenShot-Pro%29%20while%20maintaining%20robust%20general%20capabilities.%20As%20GUI%20agent%20capabilities%20improve%2C%20practical%20deployment%20demands%20standardized%20interfaces%20across%20heterogeneous%20devices%20while%20protecting%20user%20privacy.%20To%20this%20end%2C%20we%20propose%20GUI-MCP%2C%20the%20first%20Model%20Context%20Protocol%20for%20GUI%20automation%20with%20hierarchical%20architecture%20that%20combines%20low-level%20atomic%20operations%20and%20high-level%20task%20delegation%20to%20local%20specialist%20models%2C%20enabling%20high-privacy%20execution%20where%20sensitive%20data%20stays%20on-device.%20Finally%2C%20to%20assess%20whether%20agents%20can%20handle%20authentic%20everyday%20usage%2C%20we%20introduce%20AndroidDaily%2C%20a%20benchmark%20grounded%20in%20real-world%20mobile%20usage%20patterns%20with%203146%20static%20actions%20and%20235%20end-to-end%20tasks%20across%20high-frequency%20daily%20scenarios%20%288B%3A%20static%2089.91%25%2C%20end-to-end%2052.50%25%29.%20Our%20work%20advances%20the%20development%20of%20practical%20GUI%20agents%20and%20demonstrates%20strong%20potential%20for%20real-world%20deployment%20in%20everyday%20digital%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15431v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-GUI%2520Technical%2520Report%26entry.906535625%3DHaolong%2520Yan%2520and%2520Jia%2520Wang%2520and%2520Xin%2520Huang%2520and%2520Yeqing%2520Shen%2520and%2520Ziyang%2520Meng%2520and%2520Zhimin%2520Fan%2520and%2520Kaijun%2520Tan%2520and%2520Jin%2520Gao%2520and%2520Lieyu%2520Shi%2520and%2520Mi%2520Yang%2520and%2520Shiliang%2520Yang%2520and%2520Zhirui%2520Wang%2520and%2520Brian%2520Li%2520and%2520Kang%2520An%2520and%2520Chenyang%2520Li%2520and%2520Lei%2520Lei%2520and%2520Mengmeng%2520Duan%2520and%2520Danxun%2520Liang%2520and%2520Guodong%2520Liu%2520and%2520Hang%2520Cheng%2520and%2520Hao%2520Wu%2520and%2520Jie%2520Dong%2520and%2520Junhao%2520Huang%2520and%2520Mei%2520Chen%2520and%2520Renjie%2520Yu%2520and%2520Shunshan%2520Li%2520and%2520Xu%2520Zhou%2520and%2520Yiting%2520Dai%2520and%2520Yineng%2520Deng%2520and%2520Yingdan%2520Liang%2520and%2520Zelin%2520Chen%2520and%2520Wen%2520Sun%2520and%2520Chengxu%2520Yan%2520and%2520Chunqin%2520Xu%2520and%2520Dong%2520Li%2520and%2520Fengqiong%2520Xiao%2520and%2520Guanghao%2520Fan%2520and%2520Guopeng%2520Li%2520and%2520Guozhen%2520Peng%2520and%2520Hongbing%2520Li%2520and%2520Hang%2520Li%2520and%2520Hongming%2520Chen%2520and%2520Jingjing%2520Xie%2520and%2520Jianyong%2520Li%2520and%2520Jingyang%2520Zhang%2520and%2520Jiaju%2520Ren%2520and%2520Jiayu%2520Yuan%2520and%2520Jianpeng%2520Yin%2520and%2520Kai%2520Cao%2520and%2520Liang%2520Zhao%2520and%2520Liguo%2520Tan%2520and%2520Liying%2520Shi%2520and%2520Mengqiang%2520Ren%2520and%2520Min%2520Xu%2520and%2520Manjiao%2520Liu%2520and%2520Mao%2520Luo%2520and%2520Mingxin%2520Wan%2520and%2520Na%2520Wang%2520and%2520Nan%2520Wu%2520and%2520Ning%2520Wang%2520and%2520Peiyao%2520Ma%2520and%2520Qingzhou%2520Zhang%2520and%2520Qiao%2520Wang%2520and%2520Qinlin%2520Zeng%2520and%2520Qiong%2520Gao%2520and%2520Qiongyao%2520Li%2520and%2520Shangwu%2520Zhong%2520and%2520Shuli%2520Gao%2520and%2520Shaofan%2520Liu%2520and%2520Shisi%2520Gao%2520and%2520Shuang%2520Luo%2520and%2520Xingbin%2520Liu%2520and%2520Xiaojia%2520Liu%2520and%2520Xiaojie%2520Hou%2520and%2520Xin%2520Liu%2520and%2520Xuanti%2520Feng%2520and%2520Xuedan%2520Cai%2520and%2520Xuan%2520Wen%2520and%2520Xianwei%2520Zhu%2520and%2520Xin%2520Liang%2520and%2520Xin%2520Liu%2520and%2520Xin%2520Zhou%2520and%2520Yifan%2520Sui%2520and%2520Yingxiu%2520Zhao%2520and%2520Yukang%2520Shi%2520and%2520Yunfang%2520Xu%2520and%2520Yuqing%2520Zeng%2520and%2520Yixun%2520Zhang%2520and%2520Zejia%2520Weng%2520and%2520Zhonghao%2520Yan%2520and%2520Zhiguo%2520Huang%2520and%2520Zhuoyu%2520Wang%2520and%2520Zihan%2520Yan%2520and%2520Zheng%2520Ge%2520and%2520Jing%2520Li%2520and%2520Yibo%2520Zhu%2520and%2520Binxing%2520Jiao%2520and%2520Xiangyu%2520Zhang%2520and%2520Daxin%2520Jiang%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520unlock%2520unprecedented%2520opportunities%2520for%2520GUI%2520automation.%2520However%252C%2520a%2520fundamental%2520challenge%2520remains%253A%2520how%2520to%2520efficiently%2520acquire%2520high-quality%2520training%2520data%2520while%2520maintaining%2520annotation%2520reliability%253F%2520We%2520introduce%2520a%2520self-evolving%2520training%2520pipeline%2520powered%2520by%2520the%2520Calibrated%2520Step%2520Reward%2520System%252C%2520which%2520converts%2520model-generated%2520trajectories%2520into%2520reliable%2520training%2520signals%2520through%2520trajectory-level%2520calibration%252C%2520achieving%2520%253E90%2525%2520annotation%2520accuracy%2520with%252010-100x%2520lower%2520cost.%2520Leveraging%2520this%2520pipeline%252C%2520we%2520introduce%2520Step-GUI%252C%2520a%2520family%2520of%2520models%2520%25284B/8B%2529%2520that%2520achieves%2520state-of-the-art%2520GUI%2520performance%2520%25288B%253A%252080.2%2525%2520AndroidWorld%252C%252048.5%2525%2520OSWorld%252C%252062.6%2525%2520ScreenShot-Pro%2529%2520while%2520maintaining%2520robust%2520general%2520capabilities.%2520As%2520GUI%2520agent%2520capabilities%2520improve%252C%2520practical%2520deployment%2520demands%2520standardized%2520interfaces%2520across%2520heterogeneous%2520devices%2520while%2520protecting%2520user%2520privacy.%2520To%2520this%2520end%252C%2520we%2520propose%2520GUI-MCP%252C%2520the%2520first%2520Model%2520Context%2520Protocol%2520for%2520GUI%2520automation%2520with%2520hierarchical%2520architecture%2520that%2520combines%2520low-level%2520atomic%2520operations%2520and%2520high-level%2520task%2520delegation%2520to%2520local%2520specialist%2520models%252C%2520enabling%2520high-privacy%2520execution%2520where%2520sensitive%2520data%2520stays%2520on-device.%2520Finally%252C%2520to%2520assess%2520whether%2520agents%2520can%2520handle%2520authentic%2520everyday%2520usage%252C%2520we%2520introduce%2520AndroidDaily%252C%2520a%2520benchmark%2520grounded%2520in%2520real-world%2520mobile%2520usage%2520patterns%2520with%25203146%2520static%2520actions%2520and%2520235%2520end-to-end%2520tasks%2520across%2520high-frequency%2520daily%2520scenarios%2520%25288B%253A%2520static%252089.91%2525%252C%2520end-to-end%252052.50%2525%2529.%2520Our%2520work%2520advances%2520the%2520development%2520of%2520practical%2520GUI%2520agents%2520and%2520demonstrates%2520strong%2520potential%2520for%2520real-world%2520deployment%2520in%2520everyday%2520digital%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15431v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-GUI%20Technical%20Report&entry.906535625=Haolong%20Yan%20and%20Jia%20Wang%20and%20Xin%20Huang%20and%20Yeqing%20Shen%20and%20Ziyang%20Meng%20and%20Zhimin%20Fan%20and%20Kaijun%20Tan%20and%20Jin%20Gao%20and%20Lieyu%20Shi%20and%20Mi%20Yang%20and%20Shiliang%20Yang%20and%20Zhirui%20Wang%20and%20Brian%20Li%20and%20Kang%20An%20and%20Chenyang%20Li%20and%20Lei%20Lei%20and%20Mengmeng%20Duan%20and%20Danxun%20Liang%20and%20Guodong%20Liu%20and%20Hang%20Cheng%20and%20Hao%20Wu%20and%20Jie%20Dong%20and%20Junhao%20Huang%20and%20Mei%20Chen%20and%20Renjie%20Yu%20and%20Shunshan%20Li%20and%20Xu%20Zhou%20and%20Yiting%20Dai%20and%20Yineng%20Deng%20and%20Yingdan%20Liang%20and%20Zelin%20Chen%20and%20Wen%20Sun%20and%20Chengxu%20Yan%20and%20Chunqin%20Xu%20and%20Dong%20Li%20and%20Fengqiong%20Xiao%20and%20Guanghao%20Fan%20and%20Guopeng%20Li%20and%20Guozhen%20Peng%20and%20Hongbing%20Li%20and%20Hang%20Li%20and%20Hongming%20Chen%20and%20Jingjing%20Xie%20and%20Jianyong%20Li%20and%20Jingyang%20Zhang%20and%20Jiaju%20Ren%20and%20Jiayu%20Yuan%20and%20Jianpeng%20Yin%20and%20Kai%20Cao%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liying%20Shi%20and%20Mengqiang%20Ren%20and%20Min%20Xu%20and%20Manjiao%20Liu%20and%20Mao%20Luo%20and%20Mingxin%20Wan%20and%20Na%20Wang%20and%20Nan%20Wu%20and%20Ning%20Wang%20and%20Peiyao%20Ma%20and%20Qingzhou%20Zhang%20and%20Qiao%20Wang%20and%20Qinlin%20Zeng%20and%20Qiong%20Gao%20and%20Qiongyao%20Li%20and%20Shangwu%20Zhong%20and%20Shuli%20Gao%20and%20Shaofan%20Liu%20and%20Shisi%20Gao%20and%20Shuang%20Luo%20and%20Xingbin%20Liu%20and%20Xiaojia%20Liu%20and%20Xiaojie%20Hou%20and%20Xin%20Liu%20and%20Xuanti%20Feng%20and%20Xuedan%20Cai%20and%20Xuan%20Wen%20and%20Xianwei%20Zhu%20and%20Xin%20Liang%20and%20Xin%20Liu%20and%20Xin%20Zhou%20and%20Yifan%20Sui%20and%20Yingxiu%20Zhao%20and%20Yukang%20Shi%20and%20Yunfang%20Xu%20and%20Yuqing%20Zeng%20and%20Yixun%20Zhang%20and%20Zejia%20Weng%20and%20Zhonghao%20Yan%20and%20Zhiguo%20Huang%20and%20Zhuoyu%20Wang%20and%20Zihan%20Yan%20and%20Zheng%20Ge%20and%20Jing%20Li%20and%20Yibo%20Zhu%20and%20Binxing%20Jiao%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20unlock%20unprecedented%20opportunities%20for%20GUI%20automation.%20However%2C%20a%20fundamental%20challenge%20remains%3A%20how%20to%20efficiently%20acquire%20high-quality%20training%20data%20while%20maintaining%20annotation%20reliability%3F%20We%20introduce%20a%20self-evolving%20training%20pipeline%20powered%20by%20the%20Calibrated%20Step%20Reward%20System%2C%20which%20converts%20model-generated%20trajectories%20into%20reliable%20training%20signals%20through%20trajectory-level%20calibration%2C%20achieving%20%3E90%25%20annotation%20accuracy%20with%2010-100x%20lower%20cost.%20Leveraging%20this%20pipeline%2C%20we%20introduce%20Step-GUI%2C%20a%20family%20of%20models%20%284B/8B%29%20that%20achieves%20state-of-the-art%20GUI%20performance%20%288B%3A%2080.2%25%20AndroidWorld%2C%2048.5%25%20OSWorld%2C%2062.6%25%20ScreenShot-Pro%29%20while%20maintaining%20robust%20general%20capabilities.%20As%20GUI%20agent%20capabilities%20improve%2C%20practical%20deployment%20demands%20standardized%20interfaces%20across%20heterogeneous%20devices%20while%20protecting%20user%20privacy.%20To%20this%20end%2C%20we%20propose%20GUI-MCP%2C%20the%20first%20Model%20Context%20Protocol%20for%20GUI%20automation%20with%20hierarchical%20architecture%20that%20combines%20low-level%20atomic%20operations%20and%20high-level%20task%20delegation%20to%20local%20specialist%20models%2C%20enabling%20high-privacy%20execution%20where%20sensitive%20data%20stays%20on-device.%20Finally%2C%20to%20assess%20whether%20agents%20can%20handle%20authentic%20everyday%20usage%2C%20we%20introduce%20AndroidDaily%2C%20a%20benchmark%20grounded%20in%20real-world%20mobile%20usage%20patterns%20with%203146%20static%20actions%20and%20235%20end-to-end%20tasks%20across%20high-frequency%20daily%20scenarios%20%288B%3A%20static%2089.91%25%2C%20end-to-end%2052.50%25%29.%20Our%20work%20advances%20the%20development%20of%20practical%20GUI%20agents%20and%20demonstrates%20strong%20potential%20for%20real-world%20deployment%20in%20everyday%20digital%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2512.15431v2&entry.124074799=Read"},
{"title": "Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation", "author": "Alexandre Personnic and Mihai B\u00e2ce", "abstract": "Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.", "link": "http://arxiv.org/abs/2512.17673v1", "date": "2025-12-19", "relevancy": 2.1936, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5801}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5433}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Spatio-Temporal%20Feature%20Representations%20for%20Video-Based%20Gaze%20Estimation&body=Title%3A%20Learning%20Spatio-Temporal%20Feature%20Representations%20for%20Video-Based%20Gaze%20Estimation%0AAuthor%3A%20Alexandre%20Personnic%20and%20Mihai%20B%C3%A2ce%0AAbstract%3A%20Video-based%20gaze%20estimation%20methods%20aim%20to%20capture%20the%20inherently%20temporal%20dynamics%20of%20human%20eye%20gaze%20from%20multiple%20image%20frames.%20However%2C%20since%20models%20must%20capture%20both%20spatial%20and%20temporal%20relationships%2C%20performance%20is%20limited%20by%20the%20feature%20representations%20within%20a%20frame%20but%20also%20between%20multiple%20frames.%20We%20propose%20the%20Spatio-Temporal%20Gaze%20Network%20%28ST-Gaze%29%2C%20a%20model%20that%20combines%20a%20CNN%20backbone%20with%20dedicated%20channel%20attention%20and%20self-attention%20modules%20to%20fuse%20eye%20and%20face%20features%20optimally.%20The%20fused%20features%20are%20then%20treated%20as%20a%20spatial%20sequence%2C%20allowing%20for%20the%20capture%20of%20an%20intra-frame%20context%2C%20which%20is%20then%20propagated%20through%20time%20to%20model%20inter-frame%20dynamics.%20We%20evaluated%20our%20method%20on%20the%20EVE%20dataset%20and%20show%20that%20ST-Gaze%20achieves%20state-of-the-art%20performance%20both%20with%20and%20without%20person-specific%20adaptation.%20Additionally%2C%20our%20ablation%20study%20provides%20further%20insights%20into%20the%20model%20performance%2C%20showing%20that%20preserving%20and%20modelling%20intra-frame%20spatial%20context%20with%20our%20spatio-temporal%20recurrence%20is%20fundamentally%20superior%20to%20premature%20spatial%20pooling.%20As%20such%2C%20our%20results%20pave%20the%20way%20towards%20more%20robust%20video-based%20gaze%20estimation%20using%20commonly%20available%20cameras.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Spatio-Temporal%2520Feature%2520Representations%2520for%2520Video-Based%2520Gaze%2520Estimation%26entry.906535625%3DAlexandre%2520Personnic%2520and%2520Mihai%2520B%25C3%25A2ce%26entry.1292438233%3DVideo-based%2520gaze%2520estimation%2520methods%2520aim%2520to%2520capture%2520the%2520inherently%2520temporal%2520dynamics%2520of%2520human%2520eye%2520gaze%2520from%2520multiple%2520image%2520frames.%2520However%252C%2520since%2520models%2520must%2520capture%2520both%2520spatial%2520and%2520temporal%2520relationships%252C%2520performance%2520is%2520limited%2520by%2520the%2520feature%2520representations%2520within%2520a%2520frame%2520but%2520also%2520between%2520multiple%2520frames.%2520We%2520propose%2520the%2520Spatio-Temporal%2520Gaze%2520Network%2520%2528ST-Gaze%2529%252C%2520a%2520model%2520that%2520combines%2520a%2520CNN%2520backbone%2520with%2520dedicated%2520channel%2520attention%2520and%2520self-attention%2520modules%2520to%2520fuse%2520eye%2520and%2520face%2520features%2520optimally.%2520The%2520fused%2520features%2520are%2520then%2520treated%2520as%2520a%2520spatial%2520sequence%252C%2520allowing%2520for%2520the%2520capture%2520of%2520an%2520intra-frame%2520context%252C%2520which%2520is%2520then%2520propagated%2520through%2520time%2520to%2520model%2520inter-frame%2520dynamics.%2520We%2520evaluated%2520our%2520method%2520on%2520the%2520EVE%2520dataset%2520and%2520show%2520that%2520ST-Gaze%2520achieves%2520state-of-the-art%2520performance%2520both%2520with%2520and%2520without%2520person-specific%2520adaptation.%2520Additionally%252C%2520our%2520ablation%2520study%2520provides%2520further%2520insights%2520into%2520the%2520model%2520performance%252C%2520showing%2520that%2520preserving%2520and%2520modelling%2520intra-frame%2520spatial%2520context%2520with%2520our%2520spatio-temporal%2520recurrence%2520is%2520fundamentally%2520superior%2520to%2520premature%2520spatial%2520pooling.%2520As%2520such%252C%2520our%2520results%2520pave%2520the%2520way%2520towards%2520more%2520robust%2520video-based%2520gaze%2520estimation%2520using%2520commonly%2520available%2520cameras.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Spatio-Temporal%20Feature%20Representations%20for%20Video-Based%20Gaze%20Estimation&entry.906535625=Alexandre%20Personnic%20and%20Mihai%20B%C3%A2ce&entry.1292438233=Video-based%20gaze%20estimation%20methods%20aim%20to%20capture%20the%20inherently%20temporal%20dynamics%20of%20human%20eye%20gaze%20from%20multiple%20image%20frames.%20However%2C%20since%20models%20must%20capture%20both%20spatial%20and%20temporal%20relationships%2C%20performance%20is%20limited%20by%20the%20feature%20representations%20within%20a%20frame%20but%20also%20between%20multiple%20frames.%20We%20propose%20the%20Spatio-Temporal%20Gaze%20Network%20%28ST-Gaze%29%2C%20a%20model%20that%20combines%20a%20CNN%20backbone%20with%20dedicated%20channel%20attention%20and%20self-attention%20modules%20to%20fuse%20eye%20and%20face%20features%20optimally.%20The%20fused%20features%20are%20then%20treated%20as%20a%20spatial%20sequence%2C%20allowing%20for%20the%20capture%20of%20an%20intra-frame%20context%2C%20which%20is%20then%20propagated%20through%20time%20to%20model%20inter-frame%20dynamics.%20We%20evaluated%20our%20method%20on%20the%20EVE%20dataset%20and%20show%20that%20ST-Gaze%20achieves%20state-of-the-art%20performance%20both%20with%20and%20without%20person-specific%20adaptation.%20Additionally%2C%20our%20ablation%20study%20provides%20further%20insights%20into%20the%20model%20performance%2C%20showing%20that%20preserving%20and%20modelling%20intra-frame%20spatial%20context%20with%20our%20spatio-temporal%20recurrence%20is%20fundamentally%20superior%20to%20premature%20spatial%20pooling.%20As%20such%2C%20our%20results%20pave%20the%20way%20towards%20more%20robust%20video-based%20gaze%20estimation%20using%20commonly%20available%20cameras.&entry.1838667208=http%3A//arxiv.org/abs/2512.17673v1&entry.124074799=Read"},
{"title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection", "author": "Zhaolin Cai and Fan Li and Ziwei Zheng and Haixia Bi and Lijun He", "abstract": "Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.", "link": "http://arxiv.org/abs/2512.17601v1", "date": "2025-12-19", "relevancy": 2.1664, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5513}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.537}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadHunt-VAD%3A%20Hunting%20Robust%20Anomaly-Sensitive%20Heads%20in%20MLLM%20for%20Tuning-Free%20Video%20Anomaly%20Detection&body=Title%3A%20HeadHunt-VAD%3A%20Hunting%20Robust%20Anomaly-Sensitive%20Heads%20in%20MLLM%20for%20Tuning-Free%20Video%20Anomaly%20Detection%0AAuthor%3A%20Zhaolin%20Cai%20and%20Fan%20Li%20and%20Ziwei%20Zheng%20and%20Haixia%20Bi%20and%20Lijun%20He%0AAbstract%3A%20Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20locate%20events%20that%20deviate%20from%20normal%20patterns%20in%20videos.%20Traditional%20approaches%20often%20rely%20on%20extensive%20labeled%20data%20and%20incur%20high%20computational%20costs.%20Recent%20tuning-free%20methods%20based%20on%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20a%20promising%20alternative%20by%20leveraging%20their%20rich%20world%20knowledge.%20However%2C%20these%20methods%20typically%20rely%20on%20textual%20outputs%2C%20which%20introduces%20information%20loss%2C%20exhibits%20normalcy%20bias%2C%20and%20suffers%20from%20prompt%20sensitivity%2C%20making%20them%20insufficient%20for%20capturing%20subtle%20anomalous%20cues.%20To%20address%20these%20constraints%2C%20we%20propose%20HeadHunt-VAD%2C%20a%20novel%20tuning-free%20VAD%20paradigm%20that%20bypasses%20textual%20generation%20by%20directly%20hunting%20robust%20anomaly-sensitive%20internal%20attention%20heads%20within%20the%20frozen%20MLLM.%20Central%20to%20our%20method%20is%20a%20Robust%20Head%20Identification%20module%20that%20systematically%20evaluates%20all%20attention%20heads%20using%20a%20multi-criteria%20analysis%20of%20saliency%20and%20stability%2C%20identifying%20a%20sparse%20subset%20of%20heads%20that%20are%20consistently%20discriminative%20across%20diverse%20prompts.%20Features%20from%20these%20expert%20heads%20are%20then%20fed%20into%20a%20lightweight%20anomaly%20scorer%20and%20a%20temporal%20locator%2C%20enabling%20efficient%20and%20accurate%20anomaly%20detection%20with%20interpretable%20outputs.%20Extensive%20experiments%20show%20that%20HeadHunt-VAD%20achieves%20state-of-the-art%20performance%20among%20tuning-free%20methods%20on%20two%20major%20VAD%20benchmarks%20while%20maintaining%20high%20efficiency%2C%20validating%20head-level%20probing%20in%20MLLMs%20as%20a%20powerful%20and%20practical%20solution%20for%20real-world%20anomaly%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadHunt-VAD%253A%2520Hunting%2520Robust%2520Anomaly-Sensitive%2520Heads%2520in%2520MLLM%2520for%2520Tuning-Free%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DZhaolin%2520Cai%2520and%2520Fan%2520Li%2520and%2520Ziwei%2520Zheng%2520and%2520Haixia%2520Bi%2520and%2520Lijun%2520He%26entry.1292438233%3DVideo%2520Anomaly%2520Detection%2520%2528VAD%2529%2520aims%2520to%2520locate%2520events%2520that%2520deviate%2520from%2520normal%2520patterns%2520in%2520videos.%2520Traditional%2520approaches%2520often%2520rely%2520on%2520extensive%2520labeled%2520data%2520and%2520incur%2520high%2520computational%2520costs.%2520Recent%2520tuning-free%2520methods%2520based%2520on%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offer%2520a%2520promising%2520alternative%2520by%2520leveraging%2520their%2520rich%2520world%2520knowledge.%2520However%252C%2520these%2520methods%2520typically%2520rely%2520on%2520textual%2520outputs%252C%2520which%2520introduces%2520information%2520loss%252C%2520exhibits%2520normalcy%2520bias%252C%2520and%2520suffers%2520from%2520prompt%2520sensitivity%252C%2520making%2520them%2520insufficient%2520for%2520capturing%2520subtle%2520anomalous%2520cues.%2520To%2520address%2520these%2520constraints%252C%2520we%2520propose%2520HeadHunt-VAD%252C%2520a%2520novel%2520tuning-free%2520VAD%2520paradigm%2520that%2520bypasses%2520textual%2520generation%2520by%2520directly%2520hunting%2520robust%2520anomaly-sensitive%2520internal%2520attention%2520heads%2520within%2520the%2520frozen%2520MLLM.%2520Central%2520to%2520our%2520method%2520is%2520a%2520Robust%2520Head%2520Identification%2520module%2520that%2520systematically%2520evaluates%2520all%2520attention%2520heads%2520using%2520a%2520multi-criteria%2520analysis%2520of%2520saliency%2520and%2520stability%252C%2520identifying%2520a%2520sparse%2520subset%2520of%2520heads%2520that%2520are%2520consistently%2520discriminative%2520across%2520diverse%2520prompts.%2520Features%2520from%2520these%2520expert%2520heads%2520are%2520then%2520fed%2520into%2520a%2520lightweight%2520anomaly%2520scorer%2520and%2520a%2520temporal%2520locator%252C%2520enabling%2520efficient%2520and%2520accurate%2520anomaly%2520detection%2520with%2520interpretable%2520outputs.%2520Extensive%2520experiments%2520show%2520that%2520HeadHunt-VAD%2520achieves%2520state-of-the-art%2520performance%2520among%2520tuning-free%2520methods%2520on%2520two%2520major%2520VAD%2520benchmarks%2520while%2520maintaining%2520high%2520efficiency%252C%2520validating%2520head-level%2520probing%2520in%2520MLLMs%2520as%2520a%2520powerful%2520and%2520practical%2520solution%2520for%2520real-world%2520anomaly%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadHunt-VAD%3A%20Hunting%20Robust%20Anomaly-Sensitive%20Heads%20in%20MLLM%20for%20Tuning-Free%20Video%20Anomaly%20Detection&entry.906535625=Zhaolin%20Cai%20and%20Fan%20Li%20and%20Ziwei%20Zheng%20and%20Haixia%20Bi%20and%20Lijun%20He&entry.1292438233=Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20locate%20events%20that%20deviate%20from%20normal%20patterns%20in%20videos.%20Traditional%20approaches%20often%20rely%20on%20extensive%20labeled%20data%20and%20incur%20high%20computational%20costs.%20Recent%20tuning-free%20methods%20based%20on%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20a%20promising%20alternative%20by%20leveraging%20their%20rich%20world%20knowledge.%20However%2C%20these%20methods%20typically%20rely%20on%20textual%20outputs%2C%20which%20introduces%20information%20loss%2C%20exhibits%20normalcy%20bias%2C%20and%20suffers%20from%20prompt%20sensitivity%2C%20making%20them%20insufficient%20for%20capturing%20subtle%20anomalous%20cues.%20To%20address%20these%20constraints%2C%20we%20propose%20HeadHunt-VAD%2C%20a%20novel%20tuning-free%20VAD%20paradigm%20that%20bypasses%20textual%20generation%20by%20directly%20hunting%20robust%20anomaly-sensitive%20internal%20attention%20heads%20within%20the%20frozen%20MLLM.%20Central%20to%20our%20method%20is%20a%20Robust%20Head%20Identification%20module%20that%20systematically%20evaluates%20all%20attention%20heads%20using%20a%20multi-criteria%20analysis%20of%20saliency%20and%20stability%2C%20identifying%20a%20sparse%20subset%20of%20heads%20that%20are%20consistently%20discriminative%20across%20diverse%20prompts.%20Features%20from%20these%20expert%20heads%20are%20then%20fed%20into%20a%20lightweight%20anomaly%20scorer%20and%20a%20temporal%20locator%2C%20enabling%20efficient%20and%20accurate%20anomaly%20detection%20with%20interpretable%20outputs.%20Extensive%20experiments%20show%20that%20HeadHunt-VAD%20achieves%20state-of-the-art%20performance%20among%20tuning-free%20methods%20on%20two%20major%20VAD%20benchmarks%20while%20maintaining%20high%20efficiency%2C%20validating%20head-level%20probing%20in%20MLLMs%20as%20a%20powerful%20and%20practical%20solution%20for%20real-world%20anomaly%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.17601v1&entry.124074799=Read"},
{"title": "Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models", "author": "Muhammad Haris Khan", "abstract": "We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.", "link": "http://arxiv.org/abs/2512.17519v1", "date": "2025-12-19", "relevancy": 2.1648, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4414}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4336}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Key-Conditioned%20Orthonormal%20Transform%20Gating%20%28K-OTG%29%3A%20Multi-Key%20Access%20Control%20with%20Hidden-State%20Scrambling%20for%20LoRA-Tuned%20Models&body=Title%3A%20Key-Conditioned%20Orthonormal%20Transform%20Gating%20%28K-OTG%29%3A%20Multi-Key%20Access%20Control%20with%20Hidden-State%20Scrambling%20for%20LoRA-Tuned%20Models%0AAuthor%3A%20Muhammad%20Haris%20Khan%0AAbstract%3A%20We%20present%20a%20simple%2C%20PEFT-compatible%20mechanism%20that%20enforces%20secret-key%20access%20control%20in%20instruction-tuned%20language%20models.%20K-OTG%20trains%20on%20a%20dual-path%20corpus%3A%20authorized%20examples%20%28prefixed%20with%20a%20role%20key%29%20learn%20the%20task%20output%2C%20while%20unauthorized%20examples%20learn%20a%20visible%20block%20token.%20At%20inference%2C%20a%20pre-lm_head%20hook%20applies%20an%20orthonormal%20transform%20to%20the%20hidden%20state%3A%20with%20the%20correct%20key/role%20the%20inverse%20map%20restores%20the%20model%27s%20native%20basis%3B%20otherwise%20a%20session-ephemeral%20scrambler%20%28permutation%2C%20sign%20flips%2C%20Householders%29%20makes%20logits%20uninformative%20and%20the%20system%20short-circuits%20to%20BLOCK.%20Keys%20are%20not%20added%20as%20special%20tokens%2C%20and%20the%20method%20composes%20cleanly%20with%20LoRA%20on%204-bit%20bases.%20We%20evaluate%20an%20hour-scale%20protocol%20on%201-3B-class%20instruction%20models%20%28Llama%203.2%2C%20Qwen2.5%201.5B%29%20across%20utility%20%28XSum%20ROUGE/BLEU%2C%20GSM8K%20accuracy%2C%20WikiText-2%20perplexity%29%2C%20selectivity%20%283by3%20role-key%20unlock%20matrices%29%2C%20nonce%20invariance%2C%20block%20suppression%2C%20and%20throughput.%20Authorized%20utility%20remains%20close%20to%20the%20base%20on%20summarization%20with%20the%20expected%20modest%20PPL%20increase%20from%20instruction%20tuning%3B%20unauthorized%20utility%20collapses%20%28near-zero%20sequence%20metrics%20with%20exploding%20PPL%29%2C%20indicating%20practical%20unusability%20without%20the%20key.%20Unlock%20matrices%20are%20diagonally%20dominant%20%28high%20on-target%20unlock%2C%20low%20cross-unlock%29%2C%20authorized%20block%20emission%20is%200%20per%20N%20via%20robust%20bad-word%20lists%2C%20and%20greedy%20outputs%20match%20exactly%20across%20nonces%2C%20confirming%20correct%20inverse%20cancellation.%20The%20runtime%20overhead%20of%20the%20Python-level%20hook%20is%2040%25%20tokens%20per%20sec%20versus%20the%20base.%20K-OTG%20therefore%20provides%20a%20pragmatic%2C%20model-agnostic%20way%20to%20prevent%20unauthorized%20use%20while%20preserving%20authorized%20utility.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKey-Conditioned%2520Orthonormal%2520Transform%2520Gating%2520%2528K-OTG%2529%253A%2520Multi-Key%2520Access%2520Control%2520with%2520Hidden-State%2520Scrambling%2520for%2520LoRA-Tuned%2520Models%26entry.906535625%3DMuhammad%2520Haris%2520Khan%26entry.1292438233%3DWe%2520present%2520a%2520simple%252C%2520PEFT-compatible%2520mechanism%2520that%2520enforces%2520secret-key%2520access%2520control%2520in%2520instruction-tuned%2520language%2520models.%2520K-OTG%2520trains%2520on%2520a%2520dual-path%2520corpus%253A%2520authorized%2520examples%2520%2528prefixed%2520with%2520a%2520role%2520key%2529%2520learn%2520the%2520task%2520output%252C%2520while%2520unauthorized%2520examples%2520learn%2520a%2520visible%2520block%2520token.%2520At%2520inference%252C%2520a%2520pre-lm_head%2520hook%2520applies%2520an%2520orthonormal%2520transform%2520to%2520the%2520hidden%2520state%253A%2520with%2520the%2520correct%2520key/role%2520the%2520inverse%2520map%2520restores%2520the%2520model%2527s%2520native%2520basis%253B%2520otherwise%2520a%2520session-ephemeral%2520scrambler%2520%2528permutation%252C%2520sign%2520flips%252C%2520Householders%2529%2520makes%2520logits%2520uninformative%2520and%2520the%2520system%2520short-circuits%2520to%2520BLOCK.%2520Keys%2520are%2520not%2520added%2520as%2520special%2520tokens%252C%2520and%2520the%2520method%2520composes%2520cleanly%2520with%2520LoRA%2520on%25204-bit%2520bases.%2520We%2520evaluate%2520an%2520hour-scale%2520protocol%2520on%25201-3B-class%2520instruction%2520models%2520%2528Llama%25203.2%252C%2520Qwen2.5%25201.5B%2529%2520across%2520utility%2520%2528XSum%2520ROUGE/BLEU%252C%2520GSM8K%2520accuracy%252C%2520WikiText-2%2520perplexity%2529%252C%2520selectivity%2520%25283by3%2520role-key%2520unlock%2520matrices%2529%252C%2520nonce%2520invariance%252C%2520block%2520suppression%252C%2520and%2520throughput.%2520Authorized%2520utility%2520remains%2520close%2520to%2520the%2520base%2520on%2520summarization%2520with%2520the%2520expected%2520modest%2520PPL%2520increase%2520from%2520instruction%2520tuning%253B%2520unauthorized%2520utility%2520collapses%2520%2528near-zero%2520sequence%2520metrics%2520with%2520exploding%2520PPL%2529%252C%2520indicating%2520practical%2520unusability%2520without%2520the%2520key.%2520Unlock%2520matrices%2520are%2520diagonally%2520dominant%2520%2528high%2520on-target%2520unlock%252C%2520low%2520cross-unlock%2529%252C%2520authorized%2520block%2520emission%2520is%25200%2520per%2520N%2520via%2520robust%2520bad-word%2520lists%252C%2520and%2520greedy%2520outputs%2520match%2520exactly%2520across%2520nonces%252C%2520confirming%2520correct%2520inverse%2520cancellation.%2520The%2520runtime%2520overhead%2520of%2520the%2520Python-level%2520hook%2520is%252040%2525%2520tokens%2520per%2520sec%2520versus%2520the%2520base.%2520K-OTG%2520therefore%2520provides%2520a%2520pragmatic%252C%2520model-agnostic%2520way%2520to%2520prevent%2520unauthorized%2520use%2520while%2520preserving%2520authorized%2520utility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Key-Conditioned%20Orthonormal%20Transform%20Gating%20%28K-OTG%29%3A%20Multi-Key%20Access%20Control%20with%20Hidden-State%20Scrambling%20for%20LoRA-Tuned%20Models&entry.906535625=Muhammad%20Haris%20Khan&entry.1292438233=We%20present%20a%20simple%2C%20PEFT-compatible%20mechanism%20that%20enforces%20secret-key%20access%20control%20in%20instruction-tuned%20language%20models.%20K-OTG%20trains%20on%20a%20dual-path%20corpus%3A%20authorized%20examples%20%28prefixed%20with%20a%20role%20key%29%20learn%20the%20task%20output%2C%20while%20unauthorized%20examples%20learn%20a%20visible%20block%20token.%20At%20inference%2C%20a%20pre-lm_head%20hook%20applies%20an%20orthonormal%20transform%20to%20the%20hidden%20state%3A%20with%20the%20correct%20key/role%20the%20inverse%20map%20restores%20the%20model%27s%20native%20basis%3B%20otherwise%20a%20session-ephemeral%20scrambler%20%28permutation%2C%20sign%20flips%2C%20Householders%29%20makes%20logits%20uninformative%20and%20the%20system%20short-circuits%20to%20BLOCK.%20Keys%20are%20not%20added%20as%20special%20tokens%2C%20and%20the%20method%20composes%20cleanly%20with%20LoRA%20on%204-bit%20bases.%20We%20evaluate%20an%20hour-scale%20protocol%20on%201-3B-class%20instruction%20models%20%28Llama%203.2%2C%20Qwen2.5%201.5B%29%20across%20utility%20%28XSum%20ROUGE/BLEU%2C%20GSM8K%20accuracy%2C%20WikiText-2%20perplexity%29%2C%20selectivity%20%283by3%20role-key%20unlock%20matrices%29%2C%20nonce%20invariance%2C%20block%20suppression%2C%20and%20throughput.%20Authorized%20utility%20remains%20close%20to%20the%20base%20on%20summarization%20with%20the%20expected%20modest%20PPL%20increase%20from%20instruction%20tuning%3B%20unauthorized%20utility%20collapses%20%28near-zero%20sequence%20metrics%20with%20exploding%20PPL%29%2C%20indicating%20practical%20unusability%20without%20the%20key.%20Unlock%20matrices%20are%20diagonally%20dominant%20%28high%20on-target%20unlock%2C%20low%20cross-unlock%29%2C%20authorized%20block%20emission%20is%200%20per%20N%20via%20robust%20bad-word%20lists%2C%20and%20greedy%20outputs%20match%20exactly%20across%20nonces%2C%20confirming%20correct%20inverse%20cancellation.%20The%20runtime%20overhead%20of%20the%20Python-level%20hook%20is%2040%25%20tokens%20per%20sec%20versus%20the%20base.%20K-OTG%20therefore%20provides%20a%20pragmatic%2C%20model-agnostic%20way%20to%20prevent%20unauthorized%20use%20while%20preserving%20authorized%20utility.&entry.1838667208=http%3A//arxiv.org/abs/2512.17519v1&entry.124074799=Read"},
{"title": "Xiaomi MiMo-VL-Miloco Technical Report", "author": "Jiaze Li and Jingyang Chen and Yuxun Qu and Jianzhong Ju and Zhenbo Luo and Jian Luan and Shijie Xu and Zhenru Lin and Junyou Zhu and Boshen Xu and Wenhui Tan and Pei Fu", "abstract": "We open-source \\textbf{MiMo-VL-Miloco-7B} and its quantized variant \\textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \\href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.", "link": "http://arxiv.org/abs/2512.17436v1", "date": "2025-12-19", "relevancy": 2.1641, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Xiaomi%20MiMo-VL-Miloco%20Technical%20Report&body=Title%3A%20Xiaomi%20MiMo-VL-Miloco%20Technical%20Report%0AAuthor%3A%20Jiaze%20Li%20and%20Jingyang%20Chen%20and%20Yuxun%20Qu%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Shijie%20Xu%20and%20Zhenru%20Lin%20and%20Junyou%20Zhu%20and%20Boshen%20Xu%20and%20Wenhui%20Tan%20and%20Pei%20Fu%0AAbstract%3A%20We%20open-source%20%5Ctextbf%7BMiMo-VL-Miloco-7B%7D%20and%20its%20quantized%20variant%20%5Ctextbf%7BMiMo-VL-Miloco-7B-GGUF%7D%2C%20a%20pair%20of%20home-centric%20vision-language%20models%20that%20achieve%20strong%20performance%20on%20both%20home-scenario%20understanding%20and%20general%20multimodal%20reasoning.%20Built%20on%20the%20MiMo-VL-7B%20backbone%2C%20MiMo-VL-Miloco-7B%20is%20specialized%20for%20smart-home%20environments%2C%20attaining%20leading%20F1%20scores%20on%20gesture%20recognition%20and%20common%20home-scenario%20understanding%2C%20while%20also%20delivering%20consistent%20gains%20across%20video%20benchmarks%20such%20as%20Video-MME%2C%20Video-MMMU%2C%20and%20Charades-STA%2C%20as%20well%20as%20language%20understanding%20benchmarks%20including%20MMMU-Pro%20and%20MMLU-Pro.%20In%20our%20experiments%2C%20MiMo-VL-Miloco-7B%20outperforms%20strong%20closed-source%20and%20open-source%20baselines%20on%20home-scenario%20understanding%20and%20several%20multimodal%20reasoning%20benchmarks.%20To%20balance%20specialization%20and%20generality%2C%20we%20design%20a%20two-stage%20training%20pipeline%20that%20combines%20supervised%20fine-tuning%20with%20reinforcement%20learning%20based%20on%20Group%20Relative%20Policy%20Optimization%2C%20leveraging%20efficient%20multi-domain%20data.%20We%20further%20incorporate%20chain-of-thought%20supervision%20and%20token-budget-aware%20reasoning%2C%20enabling%20the%20model%20to%20learn%20knowledge%20in%20a%20data-efficient%20manner%20while%20also%20performing%20reasoning%20efficiently.%20Our%20analysis%20shows%20that%20targeted%20home-scenario%20training%20not%20only%20enhances%20activity%20and%20gesture%20understanding%2C%20but%20also%20improves%20text-only%20reasoning%20with%20only%20modest%20trade-offs%20on%20document-centric%20tasks.%20Model%20checkpoints%2C%20quantized%20GGUF%20weights%2C%20and%20our%20home-scenario%20evaluation%20toolkit%20are%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%7D%7Bhttps%3A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%7D%20to%20support%20research%20and%20deployment%20in%20real-world%20smart-home%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXiaomi%2520MiMo-VL-Miloco%2520Technical%2520Report%26entry.906535625%3DJiaze%2520Li%2520and%2520Jingyang%2520Chen%2520and%2520Yuxun%2520Qu%2520and%2520Jianzhong%2520Ju%2520and%2520Zhenbo%2520Luo%2520and%2520Jian%2520Luan%2520and%2520Shijie%2520Xu%2520and%2520Zhenru%2520Lin%2520and%2520Junyou%2520Zhu%2520and%2520Boshen%2520Xu%2520and%2520Wenhui%2520Tan%2520and%2520Pei%2520Fu%26entry.1292438233%3DWe%2520open-source%2520%255Ctextbf%257BMiMo-VL-Miloco-7B%257D%2520and%2520its%2520quantized%2520variant%2520%255Ctextbf%257BMiMo-VL-Miloco-7B-GGUF%257D%252C%2520a%2520pair%2520of%2520home-centric%2520vision-language%2520models%2520that%2520achieve%2520strong%2520performance%2520on%2520both%2520home-scenario%2520understanding%2520and%2520general%2520multimodal%2520reasoning.%2520Built%2520on%2520the%2520MiMo-VL-7B%2520backbone%252C%2520MiMo-VL-Miloco-7B%2520is%2520specialized%2520for%2520smart-home%2520environments%252C%2520attaining%2520leading%2520F1%2520scores%2520on%2520gesture%2520recognition%2520and%2520common%2520home-scenario%2520understanding%252C%2520while%2520also%2520delivering%2520consistent%2520gains%2520across%2520video%2520benchmarks%2520such%2520as%2520Video-MME%252C%2520Video-MMMU%252C%2520and%2520Charades-STA%252C%2520as%2520well%2520as%2520language%2520understanding%2520benchmarks%2520including%2520MMMU-Pro%2520and%2520MMLU-Pro.%2520In%2520our%2520experiments%252C%2520MiMo-VL-Miloco-7B%2520outperforms%2520strong%2520closed-source%2520and%2520open-source%2520baselines%2520on%2520home-scenario%2520understanding%2520and%2520several%2520multimodal%2520reasoning%2520benchmarks.%2520To%2520balance%2520specialization%2520and%2520generality%252C%2520we%2520design%2520a%2520two-stage%2520training%2520pipeline%2520that%2520combines%2520supervised%2520fine-tuning%2520with%2520reinforcement%2520learning%2520based%2520on%2520Group%2520Relative%2520Policy%2520Optimization%252C%2520leveraging%2520efficient%2520multi-domain%2520data.%2520We%2520further%2520incorporate%2520chain-of-thought%2520supervision%2520and%2520token-budget-aware%2520reasoning%252C%2520enabling%2520the%2520model%2520to%2520learn%2520knowledge%2520in%2520a%2520data-efficient%2520manner%2520while%2520also%2520performing%2520reasoning%2520efficiently.%2520Our%2520analysis%2520shows%2520that%2520targeted%2520home-scenario%2520training%2520not%2520only%2520enhances%2520activity%2520and%2520gesture%2520understanding%252C%2520but%2520also%2520improves%2520text-only%2520reasoning%2520with%2520only%2520modest%2520trade-offs%2520on%2520document-centric%2520tasks.%2520Model%2520checkpoints%252C%2520quantized%2520GGUF%2520weights%252C%2520and%2520our%2520home-scenario%2520evaluation%2520toolkit%2520are%2520publicly%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%257D%257Bhttps%253A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%257D%2520to%2520support%2520research%2520and%2520deployment%2520in%2520real-world%2520smart-home%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Xiaomi%20MiMo-VL-Miloco%20Technical%20Report&entry.906535625=Jiaze%20Li%20and%20Jingyang%20Chen%20and%20Yuxun%20Qu%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Shijie%20Xu%20and%20Zhenru%20Lin%20and%20Junyou%20Zhu%20and%20Boshen%20Xu%20and%20Wenhui%20Tan%20and%20Pei%20Fu&entry.1292438233=We%20open-source%20%5Ctextbf%7BMiMo-VL-Miloco-7B%7D%20and%20its%20quantized%20variant%20%5Ctextbf%7BMiMo-VL-Miloco-7B-GGUF%7D%2C%20a%20pair%20of%20home-centric%20vision-language%20models%20that%20achieve%20strong%20performance%20on%20both%20home-scenario%20understanding%20and%20general%20multimodal%20reasoning.%20Built%20on%20the%20MiMo-VL-7B%20backbone%2C%20MiMo-VL-Miloco-7B%20is%20specialized%20for%20smart-home%20environments%2C%20attaining%20leading%20F1%20scores%20on%20gesture%20recognition%20and%20common%20home-scenario%20understanding%2C%20while%20also%20delivering%20consistent%20gains%20across%20video%20benchmarks%20such%20as%20Video-MME%2C%20Video-MMMU%2C%20and%20Charades-STA%2C%20as%20well%20as%20language%20understanding%20benchmarks%20including%20MMMU-Pro%20and%20MMLU-Pro.%20In%20our%20experiments%2C%20MiMo-VL-Miloco-7B%20outperforms%20strong%20closed-source%20and%20open-source%20baselines%20on%20home-scenario%20understanding%20and%20several%20multimodal%20reasoning%20benchmarks.%20To%20balance%20specialization%20and%20generality%2C%20we%20design%20a%20two-stage%20training%20pipeline%20that%20combines%20supervised%20fine-tuning%20with%20reinforcement%20learning%20based%20on%20Group%20Relative%20Policy%20Optimization%2C%20leveraging%20efficient%20multi-domain%20data.%20We%20further%20incorporate%20chain-of-thought%20supervision%20and%20token-budget-aware%20reasoning%2C%20enabling%20the%20model%20to%20learn%20knowledge%20in%20a%20data-efficient%20manner%20while%20also%20performing%20reasoning%20efficiently.%20Our%20analysis%20shows%20that%20targeted%20home-scenario%20training%20not%20only%20enhances%20activity%20and%20gesture%20understanding%2C%20but%20also%20improves%20text-only%20reasoning%20with%20only%20modest%20trade-offs%20on%20document-centric%20tasks.%20Model%20checkpoints%2C%20quantized%20GGUF%20weights%2C%20and%20our%20home-scenario%20evaluation%20toolkit%20are%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%7D%7Bhttps%3A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%7D%20to%20support%20research%20and%20deployment%20in%20real-world%20smart-home%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.17436v1&entry.124074799=Read"},
{"title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing", "author": "Lingxiao Zhao and Haoran Zhou and Yuezhi Che and Dazhao Cheng", "abstract": "Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.\n  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.", "link": "http://arxiv.org/abs/2512.17574v1", "date": "2025-12-19", "relevancy": 2.1357, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5695}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Disaggregated%20Multi-Stage%20MLLM%20Inference%20via%20GPU-Internal%20Scheduling%20and%20Resource%20Sharing&body=Title%3A%20Enabling%20Disaggregated%20Multi-Stage%20MLLM%20Inference%20via%20GPU-Internal%20Scheduling%20and%20Resource%20Sharing%0AAuthor%3A%20Lingxiao%20Zhao%20and%20Haoran%20Zhou%20and%20Yuezhi%20Che%20and%20Dazhao%20Cheng%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20extend%20LLMs%20with%20visual%20understanding%20through%20a%20three-stage%20pipeline%3A%20multimodal%20preprocessing%2C%20vision%20encoding%2C%20and%20LLM%20inference.%20While%20these%20stages%20enhance%20capability%2C%20they%20introduce%20significant%20system%20bottlenecks.%20First%2C%20multimodal%20preprocessing-especially%20video%20decoding-often%20dominates%20Time-to-First-Token%20%28TTFT%29.%20Most%20systems%20rely%20on%20CPU-based%20decoding%2C%20which%20severely%20limits%20throughput%2C%20while%20existing%20GPU-based%20approaches%20prioritize%20throughput-oriented%20parallelism%20and%20fail%20to%20meet%20the%20latency-sensitive%20requirements%20of%20MLLM%20inference.%20Second%2C%20the%20vision%20encoder%20is%20a%20standalone%2C%20compute-intensive%20stage%20that%20produces%20visual%20embeddings%20and%20cannot%20be%20co-batched%20with%20LLM%20prefill%20or%20decoding.%20This%20heterogeneity%20forces%20inter-stage%20blocking%20and%20increases%20token-generation%20latency.%20Even%20when%20deployed%20on%20separate%20GPUs%2C%20these%20stages%20underutilize%20available%20compute%20and%20memory%20resources%2C%20reducing%20overall%20utilization%20and%20constraining%20system%20throughput.%0A%20%20To%20address%20these%20challenges%2C%20we%20present%20FlashCodec%20and%20UnifiedServe%2C%20two%20complementary%20designs%20that%20jointly%20optimize%20the%20end-to-end%20MLLM%20pipeline.%20FlashCodec%20accelerates%20the%20multimodal%20preprocessing%20stage%20through%20collaborative%20multi-GPU%20video%20decoding%2C%20reducing%20decoding%20latency%20while%20preserving%20high%20throughput.%20UnifiedServe%20optimizes%20the%20vision-to-text%20and%20inference%20stages%20using%20a%20logically%20decoupled%20their%20execution%20to%20eliminate%20inter-stage%20blocking%2C%20yet%20physically%20sharing%20GPU%20resources%20to%20maximize%20GPU%20system%20utilization.%20By%20carefully%20orchestrating%20execution%20across%20stages%20and%20minimizing%20interference%2C%20UnifiedServe%20Together%2C%20our%20proposed%20framework%20forms%20an%20end-to-end%20optimized%20stack%20that%20can%20serve%20up%20to%203.0%24%5Ctimes%24%20more%20requests%20or%20enforce%201.5%24%5Ctimes%24%20tighter%20SLOs%2C%20while%20achieving%20up%20to%204.4%24%5Ctimes%24%20higher%20throughput%20compared%20to%20state-of-the-art%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Disaggregated%2520Multi-Stage%2520MLLM%2520Inference%2520via%2520GPU-Internal%2520Scheduling%2520and%2520Resource%2520Sharing%26entry.906535625%3DLingxiao%2520Zhao%2520and%2520Haoran%2520Zhou%2520and%2520Yuezhi%2520Che%2520and%2520Dazhao%2520Cheng%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520extend%2520LLMs%2520with%2520visual%2520understanding%2520through%2520a%2520three-stage%2520pipeline%253A%2520multimodal%2520preprocessing%252C%2520vision%2520encoding%252C%2520and%2520LLM%2520inference.%2520While%2520these%2520stages%2520enhance%2520capability%252C%2520they%2520introduce%2520significant%2520system%2520bottlenecks.%2520First%252C%2520multimodal%2520preprocessing-especially%2520video%2520decoding-often%2520dominates%2520Time-to-First-Token%2520%2528TTFT%2529.%2520Most%2520systems%2520rely%2520on%2520CPU-based%2520decoding%252C%2520which%2520severely%2520limits%2520throughput%252C%2520while%2520existing%2520GPU-based%2520approaches%2520prioritize%2520throughput-oriented%2520parallelism%2520and%2520fail%2520to%2520meet%2520the%2520latency-sensitive%2520requirements%2520of%2520MLLM%2520inference.%2520Second%252C%2520the%2520vision%2520encoder%2520is%2520a%2520standalone%252C%2520compute-intensive%2520stage%2520that%2520produces%2520visual%2520embeddings%2520and%2520cannot%2520be%2520co-batched%2520with%2520LLM%2520prefill%2520or%2520decoding.%2520This%2520heterogeneity%2520forces%2520inter-stage%2520blocking%2520and%2520increases%2520token-generation%2520latency.%2520Even%2520when%2520deployed%2520on%2520separate%2520GPUs%252C%2520these%2520stages%2520underutilize%2520available%2520compute%2520and%2520memory%2520resources%252C%2520reducing%2520overall%2520utilization%2520and%2520constraining%2520system%2520throughput.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520FlashCodec%2520and%2520UnifiedServe%252C%2520two%2520complementary%2520designs%2520that%2520jointly%2520optimize%2520the%2520end-to-end%2520MLLM%2520pipeline.%2520FlashCodec%2520accelerates%2520the%2520multimodal%2520preprocessing%2520stage%2520through%2520collaborative%2520multi-GPU%2520video%2520decoding%252C%2520reducing%2520decoding%2520latency%2520while%2520preserving%2520high%2520throughput.%2520UnifiedServe%2520optimizes%2520the%2520vision-to-text%2520and%2520inference%2520stages%2520using%2520a%2520logically%2520decoupled%2520their%2520execution%2520to%2520eliminate%2520inter-stage%2520blocking%252C%2520yet%2520physically%2520sharing%2520GPU%2520resources%2520to%2520maximize%2520GPU%2520system%2520utilization.%2520By%2520carefully%2520orchestrating%2520execution%2520across%2520stages%2520and%2520minimizing%2520interference%252C%2520UnifiedServe%2520Together%252C%2520our%2520proposed%2520framework%2520forms%2520an%2520end-to-end%2520optimized%2520stack%2520that%2520can%2520serve%2520up%2520to%25203.0%2524%255Ctimes%2524%2520more%2520requests%2520or%2520enforce%25201.5%2524%255Ctimes%2524%2520tighter%2520SLOs%252C%2520while%2520achieving%2520up%2520to%25204.4%2524%255Ctimes%2524%2520higher%2520throughput%2520compared%2520to%2520state-of-the-art%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Disaggregated%20Multi-Stage%20MLLM%20Inference%20via%20GPU-Internal%20Scheduling%20and%20Resource%20Sharing&entry.906535625=Lingxiao%20Zhao%20and%20Haoran%20Zhou%20and%20Yuezhi%20Che%20and%20Dazhao%20Cheng&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20extend%20LLMs%20with%20visual%20understanding%20through%20a%20three-stage%20pipeline%3A%20multimodal%20preprocessing%2C%20vision%20encoding%2C%20and%20LLM%20inference.%20While%20these%20stages%20enhance%20capability%2C%20they%20introduce%20significant%20system%20bottlenecks.%20First%2C%20multimodal%20preprocessing-especially%20video%20decoding-often%20dominates%20Time-to-First-Token%20%28TTFT%29.%20Most%20systems%20rely%20on%20CPU-based%20decoding%2C%20which%20severely%20limits%20throughput%2C%20while%20existing%20GPU-based%20approaches%20prioritize%20throughput-oriented%20parallelism%20and%20fail%20to%20meet%20the%20latency-sensitive%20requirements%20of%20MLLM%20inference.%20Second%2C%20the%20vision%20encoder%20is%20a%20standalone%2C%20compute-intensive%20stage%20that%20produces%20visual%20embeddings%20and%20cannot%20be%20co-batched%20with%20LLM%20prefill%20or%20decoding.%20This%20heterogeneity%20forces%20inter-stage%20blocking%20and%20increases%20token-generation%20latency.%20Even%20when%20deployed%20on%20separate%20GPUs%2C%20these%20stages%20underutilize%20available%20compute%20and%20memory%20resources%2C%20reducing%20overall%20utilization%20and%20constraining%20system%20throughput.%0A%20%20To%20address%20these%20challenges%2C%20we%20present%20FlashCodec%20and%20UnifiedServe%2C%20two%20complementary%20designs%20that%20jointly%20optimize%20the%20end-to-end%20MLLM%20pipeline.%20FlashCodec%20accelerates%20the%20multimodal%20preprocessing%20stage%20through%20collaborative%20multi-GPU%20video%20decoding%2C%20reducing%20decoding%20latency%20while%20preserving%20high%20throughput.%20UnifiedServe%20optimizes%20the%20vision-to-text%20and%20inference%20stages%20using%20a%20logically%20decoupled%20their%20execution%20to%20eliminate%20inter-stage%20blocking%2C%20yet%20physically%20sharing%20GPU%20resources%20to%20maximize%20GPU%20system%20utilization.%20By%20carefully%20orchestrating%20execution%20across%20stages%20and%20minimizing%20interference%2C%20UnifiedServe%20Together%2C%20our%20proposed%20framework%20forms%20an%20end-to-end%20optimized%20stack%20that%20can%20serve%20up%20to%203.0%24%5Ctimes%24%20more%20requests%20or%20enforce%201.5%24%5Ctimes%24%20tighter%20SLOs%2C%20while%20achieving%20up%20to%204.4%24%5Ctimes%24%20higher%20throughput%20compared%20to%20state-of-the-art%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.17574v1&entry.124074799=Read"},
{"title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories", "author": "Lilin Wang and Lucas Ramalho and Alan Celestino and Phuc Anthony Pham and Yu Liu and Umang Kumar Sinha and Andres Portillo and Onassis Osunwa and Gabriel Maduekwe", "abstract": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.", "link": "http://arxiv.org/abs/2512.17419v1", "date": "2025-12-19", "relevancy": 2.1292, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4373}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4329}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWE-Bench%2B%2B%3A%20A%20Framework%20for%20the%20Scalable%20Generation%20of%20Software%20Engineering%20Benchmarks%20from%20Open-Source%20Repositories&body=Title%3A%20SWE-Bench%2B%2B%3A%20A%20Framework%20for%20the%20Scalable%20Generation%20of%20Software%20Engineering%20Benchmarks%20from%20Open-Source%20Repositories%0AAuthor%3A%20Lilin%20Wang%20and%20Lucas%20Ramalho%20and%20Alan%20Celestino%20and%20Phuc%20Anthony%20Pham%20and%20Yu%20Liu%20and%20Umang%20Kumar%20Sinha%20and%20Andres%20Portillo%20and%20Onassis%20Osunwa%20and%20Gabriel%20Maduekwe%0AAbstract%3A%20Benchmarks%20like%20SWE-bench%20have%20standardized%20the%20evaluation%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20repository-level%20software%20engineering%20tasks.%20However%2C%20these%20efforts%20remain%20limited%20by%20manual%20curation%2C%20static%20datasets%2C%20and%20a%20focus%20on%20Python-based%20bug%20fixes.%20We%20introduce%20SWE-Bench%2B%2B%2C%20an%20automated%20framework%20that%20generates%20repository-level%20coding%20tasks%20from%20open-source%20GitHub%20projects.%20Unlike%20synthetic%20approaches%2C%20our%20pipeline%20harvests%20live%20pull%20requests%20to%20cover%20both%20bug%20fixes%20and%20feature%20requests%20across%2011%20languages.%20SWE-Bench%2B%2B%20turns%20GitHub%20pull%20requests%20%28PRs%29%20into%20reproducible%2C%20execution-based%20tasks%20via%20four%20stages%3A%20programmatic%20sourcing%2C%20environment%20synthesis%2C%20test%20oracle%20extraction%2C%20and%20quality%20assurance.%20A%20final%20hint-guided%20trajectory%20synthesis%20step%20converts%20instances%20that%20strong%20models%20fail%20on%20into%20training%20trajectories.%20Our%20initial%20benchmark%20consists%20of%2011%2C133%20instances%20from%203%2C971%20repositories%20across%2011%20languages.%20On%20a%20subset%20of%201%2C782%20instances%20of%20this%20benchmark%2C%20today%27s%20strongest%20models%20perform%20as%20follows%3A%20claude-sonnet-4.5%20achieves%2036.20%25%20pass%4010%2C%20gpt-5-2025-08-07%2034.57%25%2C%20gemini/gemini-2.5-pro%2024.92%25%2C%20and%20gpt-4o%2016.89%25.%20We%20further%20demonstrate%20the%20utility%20of%20our%20dataset%20by%20showing%20that%20fine-tuning%20on%20SWE-Bench%2B%2B%20instances%20yields%20measurable%20improvements%20on%20the%20SWE-bench%20Multilingual%20benchmark.%20SWE-Bench%2B%2B%20provides%20a%20scalable%2C%20multilingual%20benchmark%20for%20evaluating%20and%20improving%20repository-level%20code%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWE-Bench%252B%252B%253A%2520A%2520Framework%2520for%2520the%2520Scalable%2520Generation%2520of%2520Software%2520Engineering%2520Benchmarks%2520from%2520Open-Source%2520Repositories%26entry.906535625%3DLilin%2520Wang%2520and%2520Lucas%2520Ramalho%2520and%2520Alan%2520Celestino%2520and%2520Phuc%2520Anthony%2520Pham%2520and%2520Yu%2520Liu%2520and%2520Umang%2520Kumar%2520Sinha%2520and%2520Andres%2520Portillo%2520and%2520Onassis%2520Osunwa%2520and%2520Gabriel%2520Maduekwe%26entry.1292438233%3DBenchmarks%2520like%2520SWE-bench%2520have%2520standardized%2520the%2520evaluation%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520repository-level%2520software%2520engineering%2520tasks.%2520However%252C%2520these%2520efforts%2520remain%2520limited%2520by%2520manual%2520curation%252C%2520static%2520datasets%252C%2520and%2520a%2520focus%2520on%2520Python-based%2520bug%2520fixes.%2520We%2520introduce%2520SWE-Bench%252B%252B%252C%2520an%2520automated%2520framework%2520that%2520generates%2520repository-level%2520coding%2520tasks%2520from%2520open-source%2520GitHub%2520projects.%2520Unlike%2520synthetic%2520approaches%252C%2520our%2520pipeline%2520harvests%2520live%2520pull%2520requests%2520to%2520cover%2520both%2520bug%2520fixes%2520and%2520feature%2520requests%2520across%252011%2520languages.%2520SWE-Bench%252B%252B%2520turns%2520GitHub%2520pull%2520requests%2520%2528PRs%2529%2520into%2520reproducible%252C%2520execution-based%2520tasks%2520via%2520four%2520stages%253A%2520programmatic%2520sourcing%252C%2520environment%2520synthesis%252C%2520test%2520oracle%2520extraction%252C%2520and%2520quality%2520assurance.%2520A%2520final%2520hint-guided%2520trajectory%2520synthesis%2520step%2520converts%2520instances%2520that%2520strong%2520models%2520fail%2520on%2520into%2520training%2520trajectories.%2520Our%2520initial%2520benchmark%2520consists%2520of%252011%252C133%2520instances%2520from%25203%252C971%2520repositories%2520across%252011%2520languages.%2520On%2520a%2520subset%2520of%25201%252C782%2520instances%2520of%2520this%2520benchmark%252C%2520today%2527s%2520strongest%2520models%2520perform%2520as%2520follows%253A%2520claude-sonnet-4.5%2520achieves%252036.20%2525%2520pass%254010%252C%2520gpt-5-2025-08-07%252034.57%2525%252C%2520gemini/gemini-2.5-pro%252024.92%2525%252C%2520and%2520gpt-4o%252016.89%2525.%2520We%2520further%2520demonstrate%2520the%2520utility%2520of%2520our%2520dataset%2520by%2520showing%2520that%2520fine-tuning%2520on%2520SWE-Bench%252B%252B%2520instances%2520yields%2520measurable%2520improvements%2520on%2520the%2520SWE-bench%2520Multilingual%2520benchmark.%2520SWE-Bench%252B%252B%2520provides%2520a%2520scalable%252C%2520multilingual%2520benchmark%2520for%2520evaluating%2520and%2520improving%2520repository-level%2520code%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWE-Bench%2B%2B%3A%20A%20Framework%20for%20the%20Scalable%20Generation%20of%20Software%20Engineering%20Benchmarks%20from%20Open-Source%20Repositories&entry.906535625=Lilin%20Wang%20and%20Lucas%20Ramalho%20and%20Alan%20Celestino%20and%20Phuc%20Anthony%20Pham%20and%20Yu%20Liu%20and%20Umang%20Kumar%20Sinha%20and%20Andres%20Portillo%20and%20Onassis%20Osunwa%20and%20Gabriel%20Maduekwe&entry.1292438233=Benchmarks%20like%20SWE-bench%20have%20standardized%20the%20evaluation%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20repository-level%20software%20engineering%20tasks.%20However%2C%20these%20efforts%20remain%20limited%20by%20manual%20curation%2C%20static%20datasets%2C%20and%20a%20focus%20on%20Python-based%20bug%20fixes.%20We%20introduce%20SWE-Bench%2B%2B%2C%20an%20automated%20framework%20that%20generates%20repository-level%20coding%20tasks%20from%20open-source%20GitHub%20projects.%20Unlike%20synthetic%20approaches%2C%20our%20pipeline%20harvests%20live%20pull%20requests%20to%20cover%20both%20bug%20fixes%20and%20feature%20requests%20across%2011%20languages.%20SWE-Bench%2B%2B%20turns%20GitHub%20pull%20requests%20%28PRs%29%20into%20reproducible%2C%20execution-based%20tasks%20via%20four%20stages%3A%20programmatic%20sourcing%2C%20environment%20synthesis%2C%20test%20oracle%20extraction%2C%20and%20quality%20assurance.%20A%20final%20hint-guided%20trajectory%20synthesis%20step%20converts%20instances%20that%20strong%20models%20fail%20on%20into%20training%20trajectories.%20Our%20initial%20benchmark%20consists%20of%2011%2C133%20instances%20from%203%2C971%20repositories%20across%2011%20languages.%20On%20a%20subset%20of%201%2C782%20instances%20of%20this%20benchmark%2C%20today%27s%20strongest%20models%20perform%20as%20follows%3A%20claude-sonnet-4.5%20achieves%2036.20%25%20pass%4010%2C%20gpt-5-2025-08-07%2034.57%25%2C%20gemini/gemini-2.5-pro%2024.92%25%2C%20and%20gpt-4o%2016.89%25.%20We%20further%20demonstrate%20the%20utility%20of%20our%20dataset%20by%20showing%20that%20fine-tuning%20on%20SWE-Bench%2B%2B%20instances%20yields%20measurable%20improvements%20on%20the%20SWE-bench%20Multilingual%20benchmark.%20SWE-Bench%2B%2B%20provides%20a%20scalable%2C%20multilingual%20benchmark%20for%20evaluating%20and%20improving%20repository-level%20code%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.17419v1&entry.124074799=Read"},
{"title": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification", "author": "Xiaoyu Tao and Tingyue Pan and Mingyue Cheng and Yucong Luo and Qi Liu and Enhong Chen", "abstract": "Time series classification plays a fundamental role in a wide range of real-world applications. Recently, large language models (LLMs) have demonstrated strong generalization and reasoning capacities, but directly applying them to time series classification remains non-trivial due to the representation gap between numerical sequences and linguistic semantics. In this paper, we propose HiTime, a hierarchical LLM-based framework for multimodal time series classification that bridges structured temporal representations with semantic reasoning in a generative paradigm. Specifically, we design a hierarchical sequence feature encoding module composed of a data-specific encoder and a task-specific encoder to extract complementary temporal features. To mitigate the embedding gap between time series representations and textual semantics, we further introduce a semantic space alignment module that jointly performs coarse-grained global modeling and fine-grained cross-modal correspondence. Building upon the above representations, we employ a parameter-efficient supervised fine-tuning strategy to activate the generative classification capability of the algined LLMs, thereby transforming conventional discriminative time series classification into a generative task. Extensive experiments on multiple benchmarks demonstrate that the proposed framework consistently outperforms state-of-the-art baselines. The code is publicly available at https://github.com/Xiaoyu-Tao/HiTime.", "link": "http://arxiv.org/abs/2410.18686v2", "date": "2025-12-19", "relevancy": 2.1254, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.531}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Multimodal%20LLMs%20with%20Semantic%20Space%20Alignment%20for%20Enhanced%20Time%20Series%20Classification&body=Title%3A%20Hierarchical%20Multimodal%20LLMs%20with%20Semantic%20Space%20Alignment%20for%20Enhanced%20Time%20Series%20Classification%0AAuthor%3A%20Xiaoyu%20Tao%20and%20Tingyue%20Pan%20and%20Mingyue%20Cheng%20and%20Yucong%20Luo%20and%20Qi%20Liu%20and%20Enhong%20Chen%0AAbstract%3A%20Time%20series%20classification%20plays%20a%20fundamental%20role%20in%20a%20wide%20range%20of%20real-world%20applications.%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20generalization%20and%20reasoning%20capacities%2C%20but%20directly%20applying%20them%20to%20time%20series%20classification%20remains%20non-trivial%20due%20to%20the%20representation%20gap%20between%20numerical%20sequences%20and%20linguistic%20semantics.%20In%20this%20paper%2C%20we%20propose%20HiTime%2C%20a%20hierarchical%20LLM-based%20framework%20for%20multimodal%20time%20series%20classification%20that%20bridges%20structured%20temporal%20representations%20with%20semantic%20reasoning%20in%20a%20generative%20paradigm.%20Specifically%2C%20we%20design%20a%20hierarchical%20sequence%20feature%20encoding%20module%20composed%20of%20a%20data-specific%20encoder%20and%20a%20task-specific%20encoder%20to%20extract%20complementary%20temporal%20features.%20To%20mitigate%20the%20embedding%20gap%20between%20time%20series%20representations%20and%20textual%20semantics%2C%20we%20further%20introduce%20a%20semantic%20space%20alignment%20module%20that%20jointly%20performs%20coarse-grained%20global%20modeling%20and%20fine-grained%20cross-modal%20correspondence.%20Building%20upon%20the%20above%20representations%2C%20we%20employ%20a%20parameter-efficient%20supervised%20fine-tuning%20strategy%20to%20activate%20the%20generative%20classification%20capability%20of%20the%20algined%20LLMs%2C%20thereby%20transforming%20conventional%20discriminative%20time%20series%20classification%20into%20a%20generative%20task.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20the%20proposed%20framework%20consistently%20outperforms%20state-of-the-art%20baselines.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/Xiaoyu-Tao/HiTime.%0ALink%3A%20http%3A//arxiv.org/abs/2410.18686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Multimodal%2520LLMs%2520with%2520Semantic%2520Space%2520Alignment%2520for%2520Enhanced%2520Time%2520Series%2520Classification%26entry.906535625%3DXiaoyu%2520Tao%2520and%2520Tingyue%2520Pan%2520and%2520Mingyue%2520Cheng%2520and%2520Yucong%2520Luo%2520and%2520Qi%2520Liu%2520and%2520Enhong%2520Chen%26entry.1292438233%3DTime%2520series%2520classification%2520plays%2520a%2520fundamental%2520role%2520in%2520a%2520wide%2520range%2520of%2520real-world%2520applications.%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520generalization%2520and%2520reasoning%2520capacities%252C%2520but%2520directly%2520applying%2520them%2520to%2520time%2520series%2520classification%2520remains%2520non-trivial%2520due%2520to%2520the%2520representation%2520gap%2520between%2520numerical%2520sequences%2520and%2520linguistic%2520semantics.%2520In%2520this%2520paper%252C%2520we%2520propose%2520HiTime%252C%2520a%2520hierarchical%2520LLM-based%2520framework%2520for%2520multimodal%2520time%2520series%2520classification%2520that%2520bridges%2520structured%2520temporal%2520representations%2520with%2520semantic%2520reasoning%2520in%2520a%2520generative%2520paradigm.%2520Specifically%252C%2520we%2520design%2520a%2520hierarchical%2520sequence%2520feature%2520encoding%2520module%2520composed%2520of%2520a%2520data-specific%2520encoder%2520and%2520a%2520task-specific%2520encoder%2520to%2520extract%2520complementary%2520temporal%2520features.%2520To%2520mitigate%2520the%2520embedding%2520gap%2520between%2520time%2520series%2520representations%2520and%2520textual%2520semantics%252C%2520we%2520further%2520introduce%2520a%2520semantic%2520space%2520alignment%2520module%2520that%2520jointly%2520performs%2520coarse-grained%2520global%2520modeling%2520and%2520fine-grained%2520cross-modal%2520correspondence.%2520Building%2520upon%2520the%2520above%2520representations%252C%2520we%2520employ%2520a%2520parameter-efficient%2520supervised%2520fine-tuning%2520strategy%2520to%2520activate%2520the%2520generative%2520classification%2520capability%2520of%2520the%2520algined%2520LLMs%252C%2520thereby%2520transforming%2520conventional%2520discriminative%2520time%2520series%2520classification%2520into%2520a%2520generative%2520task.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520the%2520proposed%2520framework%2520consistently%2520outperforms%2520state-of-the-art%2520baselines.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Xiaoyu-Tao/HiTime.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Multimodal%20LLMs%20with%20Semantic%20Space%20Alignment%20for%20Enhanced%20Time%20Series%20Classification&entry.906535625=Xiaoyu%20Tao%20and%20Tingyue%20Pan%20and%20Mingyue%20Cheng%20and%20Yucong%20Luo%20and%20Qi%20Liu%20and%20Enhong%20Chen&entry.1292438233=Time%20series%20classification%20plays%20a%20fundamental%20role%20in%20a%20wide%20range%20of%20real-world%20applications.%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20generalization%20and%20reasoning%20capacities%2C%20but%20directly%20applying%20them%20to%20time%20series%20classification%20remains%20non-trivial%20due%20to%20the%20representation%20gap%20between%20numerical%20sequences%20and%20linguistic%20semantics.%20In%20this%20paper%2C%20we%20propose%20HiTime%2C%20a%20hierarchical%20LLM-based%20framework%20for%20multimodal%20time%20series%20classification%20that%20bridges%20structured%20temporal%20representations%20with%20semantic%20reasoning%20in%20a%20generative%20paradigm.%20Specifically%2C%20we%20design%20a%20hierarchical%20sequence%20feature%20encoding%20module%20composed%20of%20a%20data-specific%20encoder%20and%20a%20task-specific%20encoder%20to%20extract%20complementary%20temporal%20features.%20To%20mitigate%20the%20embedding%20gap%20between%20time%20series%20representations%20and%20textual%20semantics%2C%20we%20further%20introduce%20a%20semantic%20space%20alignment%20module%20that%20jointly%20performs%20coarse-grained%20global%20modeling%20and%20fine-grained%20cross-modal%20correspondence.%20Building%20upon%20the%20above%20representations%2C%20we%20employ%20a%20parameter-efficient%20supervised%20fine-tuning%20strategy%20to%20activate%20the%20generative%20classification%20capability%20of%20the%20algined%20LLMs%2C%20thereby%20transforming%20conventional%20discriminative%20time%20series%20classification%20into%20a%20generative%20task.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20the%20proposed%20framework%20consistently%20outperforms%20state-of-the-art%20baselines.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/Xiaoyu-Tao/HiTime.&entry.1838667208=http%3A//arxiv.org/abs/2410.18686v2&entry.124074799=Read"},
{"title": "MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image", "author": "Qian Zeng and Yihui Wang and Shu Yang and Yingxue Xu and Fengtao Zhou and Jiabo Ma and Dejia Cai and Zhengyu Zhang and Lijuan Qu and Yu Wang and Li Liang and Hao Chen", "abstract": "Whole-slide images (WSIs) are an important data modality in computational pathology, yet their gigapixel resolution and lack of fine-grained annotations challenge conventional deep learning models. Multiple instance learning (MIL) offers a solution by treating each WSI as a bag of patch-level instances, but effectively modeling ultra-long sequences with rich spatial context remains difficult. Recently, Mamba has emerged as a promising alternative for long sequence learning, scaling linearly to thousands of tokens. However, despite its efficiency, it still suffers from limited spatial context modeling and memory decay, constraining its effectiveness to WSI analysis. To address these limitations, we propose MambaMIL+, a new MIL framework that explicitly integrates spatial context while maintaining long-range dependency modeling without memory forgetting. Specifically, MambaMIL+ introduces 1) overlapping scanning, which restructures the patch sequence to embed spatial continuity and instance correlations; 2) a selective stripe position encoder (S2PE) that encodes positional information while mitigating the biases of fixed scanning orders; and 3) a contextual token selection (CTS) mechanism, which leverages supervisory knowledge to dynamically enlarge the contextual memory for stable long-range modeling. Extensive experiments on 20 benchmarks across diagnostic classification, molecular prediction, and survival analysis demonstrate that MambaMIL+ consistently achieves state-of-the-art performance under three feature extractors (ResNet-50, PLIP, and CONCH), highlighting its effectiveness and robustness for large-scale computational pathology", "link": "http://arxiv.org/abs/2512.17726v1", "date": "2025-12-19", "relevancy": 2.1225, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5273}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaMIL%2B%3A%20Modeling%20Long-Term%20Contextual%20Patterns%20for%20Gigapixel%20Whole%20Slide%20Image&body=Title%3A%20MambaMIL%2B%3A%20Modeling%20Long-Term%20Contextual%20Patterns%20for%20Gigapixel%20Whole%20Slide%20Image%0AAuthor%3A%20Qian%20Zeng%20and%20Yihui%20Wang%20and%20Shu%20Yang%20and%20Yingxue%20Xu%20and%20Fengtao%20Zhou%20and%20Jiabo%20Ma%20and%20Dejia%20Cai%20and%20Zhengyu%20Zhang%20and%20Lijuan%20Qu%20and%20Yu%20Wang%20and%20Li%20Liang%20and%20Hao%20Chen%0AAbstract%3A%20Whole-slide%20images%20%28WSIs%29%20are%20an%20important%20data%20modality%20in%20computational%20pathology%2C%20yet%20their%20gigapixel%20resolution%20and%20lack%20of%20fine-grained%20annotations%20challenge%20conventional%20deep%20learning%20models.%20Multiple%20instance%20learning%20%28MIL%29%20offers%20a%20solution%20by%20treating%20each%20WSI%20as%20a%20bag%20of%20patch-level%20instances%2C%20but%20effectively%20modeling%20ultra-long%20sequences%20with%20rich%20spatial%20context%20remains%20difficult.%20Recently%2C%20Mamba%20has%20emerged%20as%20a%20promising%20alternative%20for%20long%20sequence%20learning%2C%20scaling%20linearly%20to%20thousands%20of%20tokens.%20However%2C%20despite%20its%20efficiency%2C%20it%20still%20suffers%20from%20limited%20spatial%20context%20modeling%20and%20memory%20decay%2C%20constraining%20its%20effectiveness%20to%20WSI%20analysis.%20To%20address%20these%20limitations%2C%20we%20propose%20MambaMIL%2B%2C%20a%20new%20MIL%20framework%20that%20explicitly%20integrates%20spatial%20context%20while%20maintaining%20long-range%20dependency%20modeling%20without%20memory%20forgetting.%20Specifically%2C%20MambaMIL%2B%20introduces%201%29%20overlapping%20scanning%2C%20which%20restructures%20the%20patch%20sequence%20to%20embed%20spatial%20continuity%20and%20instance%20correlations%3B%202%29%20a%20selective%20stripe%20position%20encoder%20%28S2PE%29%20that%20encodes%20positional%20information%20while%20mitigating%20the%20biases%20of%20fixed%20scanning%20orders%3B%20and%203%29%20a%20contextual%20token%20selection%20%28CTS%29%20mechanism%2C%20which%20leverages%20supervisory%20knowledge%20to%20dynamically%20enlarge%20the%20contextual%20memory%20for%20stable%20long-range%20modeling.%20Extensive%20experiments%20on%2020%20benchmarks%20across%20diagnostic%20classification%2C%20molecular%20prediction%2C%20and%20survival%20analysis%20demonstrate%20that%20MambaMIL%2B%20consistently%20achieves%20state-of-the-art%20performance%20under%20three%20feature%20extractors%20%28ResNet-50%2C%20PLIP%2C%20and%20CONCH%29%2C%20highlighting%20its%20effectiveness%20and%20robustness%20for%20large-scale%20computational%20pathology%0ALink%3A%20http%3A//arxiv.org/abs/2512.17726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaMIL%252B%253A%2520Modeling%2520Long-Term%2520Contextual%2520Patterns%2520for%2520Gigapixel%2520Whole%2520Slide%2520Image%26entry.906535625%3DQian%2520Zeng%2520and%2520Yihui%2520Wang%2520and%2520Shu%2520Yang%2520and%2520Yingxue%2520Xu%2520and%2520Fengtao%2520Zhou%2520and%2520Jiabo%2520Ma%2520and%2520Dejia%2520Cai%2520and%2520Zhengyu%2520Zhang%2520and%2520Lijuan%2520Qu%2520and%2520Yu%2520Wang%2520and%2520Li%2520Liang%2520and%2520Hao%2520Chen%26entry.1292438233%3DWhole-slide%2520images%2520%2528WSIs%2529%2520are%2520an%2520important%2520data%2520modality%2520in%2520computational%2520pathology%252C%2520yet%2520their%2520gigapixel%2520resolution%2520and%2520lack%2520of%2520fine-grained%2520annotations%2520challenge%2520conventional%2520deep%2520learning%2520models.%2520Multiple%2520instance%2520learning%2520%2528MIL%2529%2520offers%2520a%2520solution%2520by%2520treating%2520each%2520WSI%2520as%2520a%2520bag%2520of%2520patch-level%2520instances%252C%2520but%2520effectively%2520modeling%2520ultra-long%2520sequences%2520with%2520rich%2520spatial%2520context%2520remains%2520difficult.%2520Recently%252C%2520Mamba%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%2520for%2520long%2520sequence%2520learning%252C%2520scaling%2520linearly%2520to%2520thousands%2520of%2520tokens.%2520However%252C%2520despite%2520its%2520efficiency%252C%2520it%2520still%2520suffers%2520from%2520limited%2520spatial%2520context%2520modeling%2520and%2520memory%2520decay%252C%2520constraining%2520its%2520effectiveness%2520to%2520WSI%2520analysis.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MambaMIL%252B%252C%2520a%2520new%2520MIL%2520framework%2520that%2520explicitly%2520integrates%2520spatial%2520context%2520while%2520maintaining%2520long-range%2520dependency%2520modeling%2520without%2520memory%2520forgetting.%2520Specifically%252C%2520MambaMIL%252B%2520introduces%25201%2529%2520overlapping%2520scanning%252C%2520which%2520restructures%2520the%2520patch%2520sequence%2520to%2520embed%2520spatial%2520continuity%2520and%2520instance%2520correlations%253B%25202%2529%2520a%2520selective%2520stripe%2520position%2520encoder%2520%2528S2PE%2529%2520that%2520encodes%2520positional%2520information%2520while%2520mitigating%2520the%2520biases%2520of%2520fixed%2520scanning%2520orders%253B%2520and%25203%2529%2520a%2520contextual%2520token%2520selection%2520%2528CTS%2529%2520mechanism%252C%2520which%2520leverages%2520supervisory%2520knowledge%2520to%2520dynamically%2520enlarge%2520the%2520contextual%2520memory%2520for%2520stable%2520long-range%2520modeling.%2520Extensive%2520experiments%2520on%252020%2520benchmarks%2520across%2520diagnostic%2520classification%252C%2520molecular%2520prediction%252C%2520and%2520survival%2520analysis%2520demonstrate%2520that%2520MambaMIL%252B%2520consistently%2520achieves%2520state-of-the-art%2520performance%2520under%2520three%2520feature%2520extractors%2520%2528ResNet-50%252C%2520PLIP%252C%2520and%2520CONCH%2529%252C%2520highlighting%2520its%2520effectiveness%2520and%2520robustness%2520for%2520large-scale%2520computational%2520pathology%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaMIL%2B%3A%20Modeling%20Long-Term%20Contextual%20Patterns%20for%20Gigapixel%20Whole%20Slide%20Image&entry.906535625=Qian%20Zeng%20and%20Yihui%20Wang%20and%20Shu%20Yang%20and%20Yingxue%20Xu%20and%20Fengtao%20Zhou%20and%20Jiabo%20Ma%20and%20Dejia%20Cai%20and%20Zhengyu%20Zhang%20and%20Lijuan%20Qu%20and%20Yu%20Wang%20and%20Li%20Liang%20and%20Hao%20Chen&entry.1292438233=Whole-slide%20images%20%28WSIs%29%20are%20an%20important%20data%20modality%20in%20computational%20pathology%2C%20yet%20their%20gigapixel%20resolution%20and%20lack%20of%20fine-grained%20annotations%20challenge%20conventional%20deep%20learning%20models.%20Multiple%20instance%20learning%20%28MIL%29%20offers%20a%20solution%20by%20treating%20each%20WSI%20as%20a%20bag%20of%20patch-level%20instances%2C%20but%20effectively%20modeling%20ultra-long%20sequences%20with%20rich%20spatial%20context%20remains%20difficult.%20Recently%2C%20Mamba%20has%20emerged%20as%20a%20promising%20alternative%20for%20long%20sequence%20learning%2C%20scaling%20linearly%20to%20thousands%20of%20tokens.%20However%2C%20despite%20its%20efficiency%2C%20it%20still%20suffers%20from%20limited%20spatial%20context%20modeling%20and%20memory%20decay%2C%20constraining%20its%20effectiveness%20to%20WSI%20analysis.%20To%20address%20these%20limitations%2C%20we%20propose%20MambaMIL%2B%2C%20a%20new%20MIL%20framework%20that%20explicitly%20integrates%20spatial%20context%20while%20maintaining%20long-range%20dependency%20modeling%20without%20memory%20forgetting.%20Specifically%2C%20MambaMIL%2B%20introduces%201%29%20overlapping%20scanning%2C%20which%20restructures%20the%20patch%20sequence%20to%20embed%20spatial%20continuity%20and%20instance%20correlations%3B%202%29%20a%20selective%20stripe%20position%20encoder%20%28S2PE%29%20that%20encodes%20positional%20information%20while%20mitigating%20the%20biases%20of%20fixed%20scanning%20orders%3B%20and%203%29%20a%20contextual%20token%20selection%20%28CTS%29%20mechanism%2C%20which%20leverages%20supervisory%20knowledge%20to%20dynamically%20enlarge%20the%20contextual%20memory%20for%20stable%20long-range%20modeling.%20Extensive%20experiments%20on%2020%20benchmarks%20across%20diagnostic%20classification%2C%20molecular%20prediction%2C%20and%20survival%20analysis%20demonstrate%20that%20MambaMIL%2B%20consistently%20achieves%20state-of-the-art%20performance%20under%20three%20feature%20extractors%20%28ResNet-50%2C%20PLIP%2C%20and%20CONCH%29%2C%20highlighting%20its%20effectiveness%20and%20robustness%20for%20large-scale%20computational%20pathology&entry.1838667208=http%3A//arxiv.org/abs/2512.17726v1&entry.124074799=Read"},
{"title": "xGR: Efficient Generative Recommendation Serving at Scale", "author": "Qingxiao Sun and Tongxuan Liu and Shen Zhang and Siyu Wu and Peijun Yang and Haotian Liang and Menxin Li and Xiaolong Ma and Zhiwei Liang and Ziyi Ren and Minchao Zhang and Xinyu Liu and Ke Zhang and Depei Qian and Hailong Yang", "abstract": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.", "link": "http://arxiv.org/abs/2512.11529v2", "date": "2025-12-19", "relevancy": 2.0977, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5341}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5319}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xGR%3A%20Efficient%20Generative%20Recommendation%20Serving%20at%20Scale&body=Title%3A%20xGR%3A%20Efficient%20Generative%20Recommendation%20Serving%20at%20Scale%0AAuthor%3A%20Qingxiao%20Sun%20and%20Tongxuan%20Liu%20and%20Shen%20Zhang%20and%20Siyu%20Wu%20and%20Peijun%20Yang%20and%20Haotian%20Liang%20and%20Menxin%20Li%20and%20Xiaolong%20Ma%20and%20Zhiwei%20Liang%20and%20Ziyi%20Ren%20and%20Minchao%20Zhang%20and%20Xinyu%20Liu%20and%20Ke%20Zhang%20and%20Depei%20Qian%20and%20Hailong%20Yang%0AAbstract%3A%20Recommendation%20system%20delivers%20substantial%20economic%20benefits%20by%20providing%20personalized%20predictions.%20Generative%20recommendation%20%28GR%29%20integrates%20LLMs%20to%20enhance%20the%20understanding%20of%20long%20user-item%20sequences.%20Despite%20employing%20attention-based%20architectures%2C%20GR%27s%20workload%20differs%20markedly%20from%20that%20of%20LLM%20serving.%20GR%20typically%20processes%20long%20prompt%20while%20producing%20short%2C%20fixed-length%20outputs%2C%20yet%20the%20computational%20cost%20of%20each%20decode%20phase%20is%20especially%20high%20due%20to%20the%20large%20beam%20width.%20In%20addition%2C%20since%20the%20beam%20search%20involves%20a%20vast%20item%20space%2C%20the%20sorting%20overhead%20becomes%20particularly%20time-consuming.%20We%20propose%20xGR%2C%20a%20GR-oriented%20serving%20system%20that%20meets%20strict%20low-latency%20requirements%20under%20highconcurrency%20scenarios.%20First%2C%20xGR%20unifies%20the%20processing%20of%20prefill%20and%20decode%20phases%20through%20staged%20computation%20and%20separated%20KV%20cache.%20Second%2C%20xGR%20enables%20early%20sorting%20termination%20and%20mask-based%20item%20filtering%20with%20data%20structure%20reuse.%20Third%2C%20xGR%20reconstructs%20the%20overall%20pipeline%20to%20exploit%20multilevel%20overlap%20and%20multi-stream%20parallelism.%20Our%20experiments%20with%20real-world%20recommendation%20service%20datasets%20demonstrate%20that%20xGR%20achieves%20at%20least%203.49x%20throughput%20compared%20to%20the%20state-of-the-art%20baseline%20under%20strict%20latency%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxGR%253A%2520Efficient%2520Generative%2520Recommendation%2520Serving%2520at%2520Scale%26entry.906535625%3DQingxiao%2520Sun%2520and%2520Tongxuan%2520Liu%2520and%2520Shen%2520Zhang%2520and%2520Siyu%2520Wu%2520and%2520Peijun%2520Yang%2520and%2520Haotian%2520Liang%2520and%2520Menxin%2520Li%2520and%2520Xiaolong%2520Ma%2520and%2520Zhiwei%2520Liang%2520and%2520Ziyi%2520Ren%2520and%2520Minchao%2520Zhang%2520and%2520Xinyu%2520Liu%2520and%2520Ke%2520Zhang%2520and%2520Depei%2520Qian%2520and%2520Hailong%2520Yang%26entry.1292438233%3DRecommendation%2520system%2520delivers%2520substantial%2520economic%2520benefits%2520by%2520providing%2520personalized%2520predictions.%2520Generative%2520recommendation%2520%2528GR%2529%2520integrates%2520LLMs%2520to%2520enhance%2520the%2520understanding%2520of%2520long%2520user-item%2520sequences.%2520Despite%2520employing%2520attention-based%2520architectures%252C%2520GR%2527s%2520workload%2520differs%2520markedly%2520from%2520that%2520of%2520LLM%2520serving.%2520GR%2520typically%2520processes%2520long%2520prompt%2520while%2520producing%2520short%252C%2520fixed-length%2520outputs%252C%2520yet%2520the%2520computational%2520cost%2520of%2520each%2520decode%2520phase%2520is%2520especially%2520high%2520due%2520to%2520the%2520large%2520beam%2520width.%2520In%2520addition%252C%2520since%2520the%2520beam%2520search%2520involves%2520a%2520vast%2520item%2520space%252C%2520the%2520sorting%2520overhead%2520becomes%2520particularly%2520time-consuming.%2520We%2520propose%2520xGR%252C%2520a%2520GR-oriented%2520serving%2520system%2520that%2520meets%2520strict%2520low-latency%2520requirements%2520under%2520highconcurrency%2520scenarios.%2520First%252C%2520xGR%2520unifies%2520the%2520processing%2520of%2520prefill%2520and%2520decode%2520phases%2520through%2520staged%2520computation%2520and%2520separated%2520KV%2520cache.%2520Second%252C%2520xGR%2520enables%2520early%2520sorting%2520termination%2520and%2520mask-based%2520item%2520filtering%2520with%2520data%2520structure%2520reuse.%2520Third%252C%2520xGR%2520reconstructs%2520the%2520overall%2520pipeline%2520to%2520exploit%2520multilevel%2520overlap%2520and%2520multi-stream%2520parallelism.%2520Our%2520experiments%2520with%2520real-world%2520recommendation%2520service%2520datasets%2520demonstrate%2520that%2520xGR%2520achieves%2520at%2520least%25203.49x%2520throughput%2520compared%2520to%2520the%2520state-of-the-art%2520baseline%2520under%2520strict%2520latency%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xGR%3A%20Efficient%20Generative%20Recommendation%20Serving%20at%20Scale&entry.906535625=Qingxiao%20Sun%20and%20Tongxuan%20Liu%20and%20Shen%20Zhang%20and%20Siyu%20Wu%20and%20Peijun%20Yang%20and%20Haotian%20Liang%20and%20Menxin%20Li%20and%20Xiaolong%20Ma%20and%20Zhiwei%20Liang%20and%20Ziyi%20Ren%20and%20Minchao%20Zhang%20and%20Xinyu%20Liu%20and%20Ke%20Zhang%20and%20Depei%20Qian%20and%20Hailong%20Yang&entry.1292438233=Recommendation%20system%20delivers%20substantial%20economic%20benefits%20by%20providing%20personalized%20predictions.%20Generative%20recommendation%20%28GR%29%20integrates%20LLMs%20to%20enhance%20the%20understanding%20of%20long%20user-item%20sequences.%20Despite%20employing%20attention-based%20architectures%2C%20GR%27s%20workload%20differs%20markedly%20from%20that%20of%20LLM%20serving.%20GR%20typically%20processes%20long%20prompt%20while%20producing%20short%2C%20fixed-length%20outputs%2C%20yet%20the%20computational%20cost%20of%20each%20decode%20phase%20is%20especially%20high%20due%20to%20the%20large%20beam%20width.%20In%20addition%2C%20since%20the%20beam%20search%20involves%20a%20vast%20item%20space%2C%20the%20sorting%20overhead%20becomes%20particularly%20time-consuming.%20We%20propose%20xGR%2C%20a%20GR-oriented%20serving%20system%20that%20meets%20strict%20low-latency%20requirements%20under%20highconcurrency%20scenarios.%20First%2C%20xGR%20unifies%20the%20processing%20of%20prefill%20and%20decode%20phases%20through%20staged%20computation%20and%20separated%20KV%20cache.%20Second%2C%20xGR%20enables%20early%20sorting%20termination%20and%20mask-based%20item%20filtering%20with%20data%20structure%20reuse.%20Third%2C%20xGR%20reconstructs%20the%20overall%20pipeline%20to%20exploit%20multilevel%20overlap%20and%20multi-stream%20parallelism.%20Our%20experiments%20with%20real-world%20recommendation%20service%20datasets%20demonstrate%20that%20xGR%20achieves%20at%20least%203.49x%20throughput%20compared%20to%20the%20state-of-the-art%20baseline%20under%20strict%20latency%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2512.11529v2&entry.124074799=Read"},
{"title": "Learning Safe Autonomous Driving Policies Using Predictive Safety Representations", "author": "Mahesh Keswani and Raunak Bhattacharyya", "abstract": "Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.", "link": "http://arxiv.org/abs/2512.17586v1", "date": "2025-12-19", "relevancy": 2.0905, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5349}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Safe%20Autonomous%20Driving%20Policies%20Using%20Predictive%20Safety%20Representations&body=Title%3A%20Learning%20Safe%20Autonomous%20Driving%20Policies%20Using%20Predictive%20Safety%20Representations%0AAuthor%3A%20Mahesh%20Keswani%20and%20Raunak%20Bhattacharyya%0AAbstract%3A%20Safe%20reinforcement%20learning%20%28SafeRL%29%20is%20a%20prominent%20paradigm%20for%20autonomous%20driving%2C%20where%20agents%20are%20required%20to%20optimize%20performance%20under%20strict%20safety%20requirements.%20This%20dual%20objective%20creates%20a%20fundamental%20tension%2C%20as%20overly%20conservative%20policies%20limit%20driving%20efficiency%20while%20aggressive%20exploration%20risks%20safety%20violations.%20The%20Safety%20Representations%20for%20Safer%20Policy%20Learning%20%28SRPL%29%20framework%20addresses%20this%20challenge%20by%20equipping%20agents%20with%20a%20predictive%20model%20of%20future%20constraint%20violations%20and%20has%20shown%20promise%20in%20controlled%20environments.%20This%20paper%20investigates%20whether%20SRPL%20extends%20to%20real-world%20autonomous%20driving%20scenarios.%20Systematic%20experiments%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29%20and%20NuPlan%20demonstrate%20that%20SRPL%20can%20improve%20the%20reward-safety%20tradeoff%2C%20achieving%20statistically%20significant%20improvements%20in%20success%20rate%20%28effect%20sizes%20r%20%3D%200.65-0.86%29%20and%20cost%20reduction%20%28effect%20sizes%20r%20%3D%200.70-0.83%29%2C%20with%20p%20%3C%200.05%20for%20observed%20improvements.%20However%2C%20its%20effectiveness%20depends%20on%20the%20underlying%20policy%20optimizer%20and%20the%20dataset%20distribution.%20The%20results%20further%20show%20that%20predictive%20safety%20representations%20play%20a%20critical%20role%20in%20improving%20robustness%20to%20observation%20noise.%20Additionally%2C%20in%20zero-shot%20cross-dataset%20evaluation%2C%20SRPL-augmented%20agents%20demonstrate%20improved%20generalization%20compared%20to%20non-SRPL%20methods.%20These%20findings%20collectively%20demonstrate%20the%20potential%20of%20predictive%20safety%20representations%20to%20strengthen%20SafeRL%20for%20autonomous%20driving.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Safe%2520Autonomous%2520Driving%2520Policies%2520Using%2520Predictive%2520Safety%2520Representations%26entry.906535625%3DMahesh%2520Keswani%2520and%2520Raunak%2520Bhattacharyya%26entry.1292438233%3DSafe%2520reinforcement%2520learning%2520%2528SafeRL%2529%2520is%2520a%2520prominent%2520paradigm%2520for%2520autonomous%2520driving%252C%2520where%2520agents%2520are%2520required%2520to%2520optimize%2520performance%2520under%2520strict%2520safety%2520requirements.%2520This%2520dual%2520objective%2520creates%2520a%2520fundamental%2520tension%252C%2520as%2520overly%2520conservative%2520policies%2520limit%2520driving%2520efficiency%2520while%2520aggressive%2520exploration%2520risks%2520safety%2520violations.%2520The%2520Safety%2520Representations%2520for%2520Safer%2520Policy%2520Learning%2520%2528SRPL%2529%2520framework%2520addresses%2520this%2520challenge%2520by%2520equipping%2520agents%2520with%2520a%2520predictive%2520model%2520of%2520future%2520constraint%2520violations%2520and%2520has%2520shown%2520promise%2520in%2520controlled%2520environments.%2520This%2520paper%2520investigates%2520whether%2520SRPL%2520extends%2520to%2520real-world%2520autonomous%2520driving%2520scenarios.%2520Systematic%2520experiments%2520on%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%2520%2528WOMD%2529%2520and%2520NuPlan%2520demonstrate%2520that%2520SRPL%2520can%2520improve%2520the%2520reward-safety%2520tradeoff%252C%2520achieving%2520statistically%2520significant%2520improvements%2520in%2520success%2520rate%2520%2528effect%2520sizes%2520r%2520%253D%25200.65-0.86%2529%2520and%2520cost%2520reduction%2520%2528effect%2520sizes%2520r%2520%253D%25200.70-0.83%2529%252C%2520with%2520p%2520%253C%25200.05%2520for%2520observed%2520improvements.%2520However%252C%2520its%2520effectiveness%2520depends%2520on%2520the%2520underlying%2520policy%2520optimizer%2520and%2520the%2520dataset%2520distribution.%2520The%2520results%2520further%2520show%2520that%2520predictive%2520safety%2520representations%2520play%2520a%2520critical%2520role%2520in%2520improving%2520robustness%2520to%2520observation%2520noise.%2520Additionally%252C%2520in%2520zero-shot%2520cross-dataset%2520evaluation%252C%2520SRPL-augmented%2520agents%2520demonstrate%2520improved%2520generalization%2520compared%2520to%2520non-SRPL%2520methods.%2520These%2520findings%2520collectively%2520demonstrate%2520the%2520potential%2520of%2520predictive%2520safety%2520representations%2520to%2520strengthen%2520SafeRL%2520for%2520autonomous%2520driving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Safe%20Autonomous%20Driving%20Policies%20Using%20Predictive%20Safety%20Representations&entry.906535625=Mahesh%20Keswani%20and%20Raunak%20Bhattacharyya&entry.1292438233=Safe%20reinforcement%20learning%20%28SafeRL%29%20is%20a%20prominent%20paradigm%20for%20autonomous%20driving%2C%20where%20agents%20are%20required%20to%20optimize%20performance%20under%20strict%20safety%20requirements.%20This%20dual%20objective%20creates%20a%20fundamental%20tension%2C%20as%20overly%20conservative%20policies%20limit%20driving%20efficiency%20while%20aggressive%20exploration%20risks%20safety%20violations.%20The%20Safety%20Representations%20for%20Safer%20Policy%20Learning%20%28SRPL%29%20framework%20addresses%20this%20challenge%20by%20equipping%20agents%20with%20a%20predictive%20model%20of%20future%20constraint%20violations%20and%20has%20shown%20promise%20in%20controlled%20environments.%20This%20paper%20investigates%20whether%20SRPL%20extends%20to%20real-world%20autonomous%20driving%20scenarios.%20Systematic%20experiments%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29%20and%20NuPlan%20demonstrate%20that%20SRPL%20can%20improve%20the%20reward-safety%20tradeoff%2C%20achieving%20statistically%20significant%20improvements%20in%20success%20rate%20%28effect%20sizes%20r%20%3D%200.65-0.86%29%20and%20cost%20reduction%20%28effect%20sizes%20r%20%3D%200.70-0.83%29%2C%20with%20p%20%3C%200.05%20for%20observed%20improvements.%20However%2C%20its%20effectiveness%20depends%20on%20the%20underlying%20policy%20optimizer%20and%20the%20dataset%20distribution.%20The%20results%20further%20show%20that%20predictive%20safety%20representations%20play%20a%20critical%20role%20in%20improving%20robustness%20to%20observation%20noise.%20Additionally%2C%20in%20zero-shot%20cross-dataset%20evaluation%2C%20SRPL-augmented%20agents%20demonstrate%20improved%20generalization%20compared%20to%20non-SRPL%20methods.%20These%20findings%20collectively%20demonstrate%20the%20potential%20of%20predictive%20safety%20representations%20to%20strengthen%20SafeRL%20for%20autonomous%20driving.&entry.1838667208=http%3A//arxiv.org/abs/2512.17586v1&entry.124074799=Read"},
{"title": "On Using Neural Networks to Learn Safety Speed Reduction in Human-Robot Collaboration: A Comparative Analysis", "author": "Marco Faroni and Alessio Span\u00f2 and Andrea M. Zanchettin and Paolo Rocco", "abstract": "In Human-Robot Collaboration, safety mechanisms such as Speed and Separation Monitoring and Power and Force Limitation dynamically adjust the robot's speed based on human proximity. While essential for risk reduction, these mechanisms introduce slowdowns that makes cycle time estimation a hard task and impact job scheduling efficiency. Existing methods for estimating cycle times or designing schedulers often rely on predefined safety models, which may not accurately reflect real-world safety implementations, as these depend on case-specific risk assessments. In this paper, we propose a deep learning approach to predict the robot's safety scaling factor directly from process execution data. We analyze multiple neural network architectures and demonstrate that a simple feed-forward network effectively estimates the robot's slowdown. This capability is crucial for improving cycle time predictions and designing more effective scheduling algorithms in collaborative robotic environments.", "link": "http://arxiv.org/abs/2512.17579v1", "date": "2025-12-19", "relevancy": 2.0885, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5262}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Using%20Neural%20Networks%20to%20Learn%20Safety%20Speed%20Reduction%20in%20Human-Robot%20Collaboration%3A%20A%20Comparative%20Analysis&body=Title%3A%20On%20Using%20Neural%20Networks%20to%20Learn%20Safety%20Speed%20Reduction%20in%20Human-Robot%20Collaboration%3A%20A%20Comparative%20Analysis%0AAuthor%3A%20Marco%20Faroni%20and%20Alessio%20Span%C3%B2%20and%20Andrea%20M.%20Zanchettin%20and%20Paolo%20Rocco%0AAbstract%3A%20In%20Human-Robot%20Collaboration%2C%20safety%20mechanisms%20such%20as%20Speed%20and%20Separation%20Monitoring%20and%20Power%20and%20Force%20Limitation%20dynamically%20adjust%20the%20robot%27s%20speed%20based%20on%20human%20proximity.%20While%20essential%20for%20risk%20reduction%2C%20these%20mechanisms%20introduce%20slowdowns%20that%20makes%20cycle%20time%20estimation%20a%20hard%20task%20and%20impact%20job%20scheduling%20efficiency.%20Existing%20methods%20for%20estimating%20cycle%20times%20or%20designing%20schedulers%20often%20rely%20on%20predefined%20safety%20models%2C%20which%20may%20not%20accurately%20reflect%20real-world%20safety%20implementations%2C%20as%20these%20depend%20on%20case-specific%20risk%20assessments.%20In%20this%20paper%2C%20we%20propose%20a%20deep%20learning%20approach%20to%20predict%20the%20robot%27s%20safety%20scaling%20factor%20directly%20from%20process%20execution%20data.%20We%20analyze%20multiple%20neural%20network%20architectures%20and%20demonstrate%20that%20a%20simple%20feed-forward%20network%20effectively%20estimates%20the%20robot%27s%20slowdown.%20This%20capability%20is%20crucial%20for%20improving%20cycle%20time%20predictions%20and%20designing%20more%20effective%20scheduling%20algorithms%20in%20collaborative%20robotic%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Using%2520Neural%2520Networks%2520to%2520Learn%2520Safety%2520Speed%2520Reduction%2520in%2520Human-Robot%2520Collaboration%253A%2520A%2520Comparative%2520Analysis%26entry.906535625%3DMarco%2520Faroni%2520and%2520Alessio%2520Span%25C3%25B2%2520and%2520Andrea%2520M.%2520Zanchettin%2520and%2520Paolo%2520Rocco%26entry.1292438233%3DIn%2520Human-Robot%2520Collaboration%252C%2520safety%2520mechanisms%2520such%2520as%2520Speed%2520and%2520Separation%2520Monitoring%2520and%2520Power%2520and%2520Force%2520Limitation%2520dynamically%2520adjust%2520the%2520robot%2527s%2520speed%2520based%2520on%2520human%2520proximity.%2520While%2520essential%2520for%2520risk%2520reduction%252C%2520these%2520mechanisms%2520introduce%2520slowdowns%2520that%2520makes%2520cycle%2520time%2520estimation%2520a%2520hard%2520task%2520and%2520impact%2520job%2520scheduling%2520efficiency.%2520Existing%2520methods%2520for%2520estimating%2520cycle%2520times%2520or%2520designing%2520schedulers%2520often%2520rely%2520on%2520predefined%2520safety%2520models%252C%2520which%2520may%2520not%2520accurately%2520reflect%2520real-world%2520safety%2520implementations%252C%2520as%2520these%2520depend%2520on%2520case-specific%2520risk%2520assessments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520deep%2520learning%2520approach%2520to%2520predict%2520the%2520robot%2527s%2520safety%2520scaling%2520factor%2520directly%2520from%2520process%2520execution%2520data.%2520We%2520analyze%2520multiple%2520neural%2520network%2520architectures%2520and%2520demonstrate%2520that%2520a%2520simple%2520feed-forward%2520network%2520effectively%2520estimates%2520the%2520robot%2527s%2520slowdown.%2520This%2520capability%2520is%2520crucial%2520for%2520improving%2520cycle%2520time%2520predictions%2520and%2520designing%2520more%2520effective%2520scheduling%2520algorithms%2520in%2520collaborative%2520robotic%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Using%20Neural%20Networks%20to%20Learn%20Safety%20Speed%20Reduction%20in%20Human-Robot%20Collaboration%3A%20A%20Comparative%20Analysis&entry.906535625=Marco%20Faroni%20and%20Alessio%20Span%C3%B2%20and%20Andrea%20M.%20Zanchettin%20and%20Paolo%20Rocco&entry.1292438233=In%20Human-Robot%20Collaboration%2C%20safety%20mechanisms%20such%20as%20Speed%20and%20Separation%20Monitoring%20and%20Power%20and%20Force%20Limitation%20dynamically%20adjust%20the%20robot%27s%20speed%20based%20on%20human%20proximity.%20While%20essential%20for%20risk%20reduction%2C%20these%20mechanisms%20introduce%20slowdowns%20that%20makes%20cycle%20time%20estimation%20a%20hard%20task%20and%20impact%20job%20scheduling%20efficiency.%20Existing%20methods%20for%20estimating%20cycle%20times%20or%20designing%20schedulers%20often%20rely%20on%20predefined%20safety%20models%2C%20which%20may%20not%20accurately%20reflect%20real-world%20safety%20implementations%2C%20as%20these%20depend%20on%20case-specific%20risk%20assessments.%20In%20this%20paper%2C%20we%20propose%20a%20deep%20learning%20approach%20to%20predict%20the%20robot%27s%20safety%20scaling%20factor%20directly%20from%20process%20execution%20data.%20We%20analyze%20multiple%20neural%20network%20architectures%20and%20demonstrate%20that%20a%20simple%20feed-forward%20network%20effectively%20estimates%20the%20robot%27s%20slowdown.%20This%20capability%20is%20crucial%20for%20improving%20cycle%20time%20predictions%20and%20designing%20more%20effective%20scheduling%20algorithms%20in%20collaborative%20robotic%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.17579v1&entry.124074799=Read"},
{"title": "Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning", "author": "Wei Tang and Yin-Fang Yang and Weijia Zhang and Min-Ling Zhang", "abstract": "Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.", "link": "http://arxiv.org/abs/2512.17788v1", "date": "2025-12-19", "relevancy": 2.0859, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5624}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5236}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibratable%20Disambiguation%20Loss%20for%20Multi-Instance%20Partial-Label%20Learning&body=Title%3A%20Calibratable%20Disambiguation%20Loss%20for%20Multi-Instance%20Partial-Label%20Learning%0AAuthor%3A%20Wei%20Tang%20and%20Yin-Fang%20Yang%20and%20Weijia%20Zhang%20and%20Min-Ling%20Zhang%0AAbstract%3A%20Multi-instance%20partial-label%20learning%20%28MIPL%29%20is%20a%20weakly%20supervised%20framework%20that%20extends%20the%20principles%20of%20multi-instance%20learning%20%28MIL%29%20and%20partial-label%20learning%20%28PLL%29%20to%20address%20the%20challenges%20of%20inexact%20supervision%20in%20both%20instance%20and%20label%20spaces.%20However%2C%20existing%20MIPL%20approaches%20often%20suffer%20from%20poor%20calibration%2C%20undermining%20classifier%20reliability.%20In%20this%20work%2C%20we%20propose%20a%20plug-and-play%20calibratable%20disambiguation%20loss%20%28CDL%29%20that%20simultaneously%20improves%20classification%20accuracy%20and%20calibration%20performance.%20The%20loss%20has%20two%20instantiations%3A%20the%20first%20one%20calibrates%20predictions%20based%20on%20probabilities%20from%20the%20candidate%20label%20set%2C%20while%20the%20second%20one%20integrates%20probabilities%20from%20both%20candidate%20and%20non-candidate%20label%20sets.%20The%20proposed%20CDL%20can%20be%20seamlessly%20incorporated%20into%20existing%20MIPL%20and%20PLL%20frameworks.%20We%20provide%20a%20theoretical%20analysis%20that%20establishes%20the%20lower%20bound%20and%20regularization%20properties%20of%20CDL%2C%20demonstrating%20its%20superiority%20over%20conventional%20disambiguation%20losses.%20Experimental%20results%20on%20benchmark%20and%20real-world%20datasets%20confirm%20that%20our%20CDL%20significantly%20enhances%20both%20classification%20and%20calibration%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibratable%2520Disambiguation%2520Loss%2520for%2520Multi-Instance%2520Partial-Label%2520Learning%26entry.906535625%3DWei%2520Tang%2520and%2520Yin-Fang%2520Yang%2520and%2520Weijia%2520Zhang%2520and%2520Min-Ling%2520Zhang%26entry.1292438233%3DMulti-instance%2520partial-label%2520learning%2520%2528MIPL%2529%2520is%2520a%2520weakly%2520supervised%2520framework%2520that%2520extends%2520the%2520principles%2520of%2520multi-instance%2520learning%2520%2528MIL%2529%2520and%2520partial-label%2520learning%2520%2528PLL%2529%2520to%2520address%2520the%2520challenges%2520of%2520inexact%2520supervision%2520in%2520both%2520instance%2520and%2520label%2520spaces.%2520However%252C%2520existing%2520MIPL%2520approaches%2520often%2520suffer%2520from%2520poor%2520calibration%252C%2520undermining%2520classifier%2520reliability.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520plug-and-play%2520calibratable%2520disambiguation%2520loss%2520%2528CDL%2529%2520that%2520simultaneously%2520improves%2520classification%2520accuracy%2520and%2520calibration%2520performance.%2520The%2520loss%2520has%2520two%2520instantiations%253A%2520the%2520first%2520one%2520calibrates%2520predictions%2520based%2520on%2520probabilities%2520from%2520the%2520candidate%2520label%2520set%252C%2520while%2520the%2520second%2520one%2520integrates%2520probabilities%2520from%2520both%2520candidate%2520and%2520non-candidate%2520label%2520sets.%2520The%2520proposed%2520CDL%2520can%2520be%2520seamlessly%2520incorporated%2520into%2520existing%2520MIPL%2520and%2520PLL%2520frameworks.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520that%2520establishes%2520the%2520lower%2520bound%2520and%2520regularization%2520properties%2520of%2520CDL%252C%2520demonstrating%2520its%2520superiority%2520over%2520conventional%2520disambiguation%2520losses.%2520Experimental%2520results%2520on%2520benchmark%2520and%2520real-world%2520datasets%2520confirm%2520that%2520our%2520CDL%2520significantly%2520enhances%2520both%2520classification%2520and%2520calibration%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibratable%20Disambiguation%20Loss%20for%20Multi-Instance%20Partial-Label%20Learning&entry.906535625=Wei%20Tang%20and%20Yin-Fang%20Yang%20and%20Weijia%20Zhang%20and%20Min-Ling%20Zhang&entry.1292438233=Multi-instance%20partial-label%20learning%20%28MIPL%29%20is%20a%20weakly%20supervised%20framework%20that%20extends%20the%20principles%20of%20multi-instance%20learning%20%28MIL%29%20and%20partial-label%20learning%20%28PLL%29%20to%20address%20the%20challenges%20of%20inexact%20supervision%20in%20both%20instance%20and%20label%20spaces.%20However%2C%20existing%20MIPL%20approaches%20often%20suffer%20from%20poor%20calibration%2C%20undermining%20classifier%20reliability.%20In%20this%20work%2C%20we%20propose%20a%20plug-and-play%20calibratable%20disambiguation%20loss%20%28CDL%29%20that%20simultaneously%20improves%20classification%20accuracy%20and%20calibration%20performance.%20The%20loss%20has%20two%20instantiations%3A%20the%20first%20one%20calibrates%20predictions%20based%20on%20probabilities%20from%20the%20candidate%20label%20set%2C%20while%20the%20second%20one%20integrates%20probabilities%20from%20both%20candidate%20and%20non-candidate%20label%20sets.%20The%20proposed%20CDL%20can%20be%20seamlessly%20incorporated%20into%20existing%20MIPL%20and%20PLL%20frameworks.%20We%20provide%20a%20theoretical%20analysis%20that%20establishes%20the%20lower%20bound%20and%20regularization%20properties%20of%20CDL%2C%20demonstrating%20its%20superiority%20over%20conventional%20disambiguation%20losses.%20Experimental%20results%20on%20benchmark%20and%20real-world%20datasets%20confirm%20that%20our%20CDL%20significantly%20enhances%20both%20classification%20and%20calibration%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.17788v1&entry.124074799=Read"},
{"title": "HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs", "author": "Chang Sun and Zhiqiang Que and Thea K. \u00c5rrestad and Vladimir Loncar and Jennifer Ngadiuba and Wayne Luk and Maria Spiropulu", "abstract": "Neural networks with sub-microsecond inference latency are required by many critical applications. Targeting such applications deployed on FPGAs, we present High Granularity Quantization (HGQ), a quantization-aware training framework that optimizes parameter bit-widths through gradient descent. Unlike conventional methods, HGQ determines the optimal bit-width for each parameter independently, making it suitable for hardware platforms supporting heterogeneous arbitrary precision arithmetic. In our experiments, HGQ shows superior performance compared to existing network compression methods, achieving orders of magnitude reduction in resource consumption and latency while maintaining the accuracy on several benchmark tasks. These improvements enable the deployment of complex models previously infeasible due to resource or latency constraints. HGQ is open-source and is used for developing next-generation trigger systems at the CERN ATLAS and CMS experiments for particle physics, enabling the use of advanced machine learning models for real-time data selection with sub-microsecond latency.", "link": "http://arxiv.org/abs/2405.00645v3", "date": "2025-12-19", "relevancy": 2.0851, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5356}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5348}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGQ%3A%20High%20Granularity%20Quantization%20for%20Real-time%20Neural%20Networks%20on%20FPGAs&body=Title%3A%20HGQ%3A%20High%20Granularity%20Quantization%20for%20Real-time%20Neural%20Networks%20on%20FPGAs%0AAuthor%3A%20Chang%20Sun%20and%20Zhiqiang%20Que%20and%20Thea%20K.%20%C3%85rrestad%20and%20Vladimir%20Loncar%20and%20Jennifer%20Ngadiuba%20and%20Wayne%20Luk%20and%20Maria%20Spiropulu%0AAbstract%3A%20Neural%20networks%20with%20sub-microsecond%20inference%20latency%20are%20required%20by%20many%20critical%20applications.%20Targeting%20such%20applications%20deployed%20on%20FPGAs%2C%20we%20present%20High%20Granularity%20Quantization%20%28HGQ%29%2C%20a%20quantization-aware%20training%20framework%20that%20optimizes%20parameter%20bit-widths%20through%20gradient%20descent.%20Unlike%20conventional%20methods%2C%20HGQ%20determines%20the%20optimal%20bit-width%20for%20each%20parameter%20independently%2C%20making%20it%20suitable%20for%20hardware%20platforms%20supporting%20heterogeneous%20arbitrary%20precision%20arithmetic.%20In%20our%20experiments%2C%20HGQ%20shows%20superior%20performance%20compared%20to%20existing%20network%20compression%20methods%2C%20achieving%20orders%20of%20magnitude%20reduction%20in%20resource%20consumption%20and%20latency%20while%20maintaining%20the%20accuracy%20on%20several%20benchmark%20tasks.%20These%20improvements%20enable%20the%20deployment%20of%20complex%20models%20previously%20infeasible%20due%20to%20resource%20or%20latency%20constraints.%20HGQ%20is%20open-source%20and%20is%20used%20for%20developing%20next-generation%20trigger%20systems%20at%20the%20CERN%20ATLAS%20and%20CMS%20experiments%20for%20particle%20physics%2C%20enabling%20the%20use%20of%20advanced%20machine%20learning%20models%20for%20real-time%20data%20selection%20with%20sub-microsecond%20latency.%0ALink%3A%20http%3A//arxiv.org/abs/2405.00645v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGQ%253A%2520High%2520Granularity%2520Quantization%2520for%2520Real-time%2520Neural%2520Networks%2520on%2520FPGAs%26entry.906535625%3DChang%2520Sun%2520and%2520Zhiqiang%2520Que%2520and%2520Thea%2520K.%2520%25C3%2585rrestad%2520and%2520Vladimir%2520Loncar%2520and%2520Jennifer%2520Ngadiuba%2520and%2520Wayne%2520Luk%2520and%2520Maria%2520Spiropulu%26entry.1292438233%3DNeural%2520networks%2520with%2520sub-microsecond%2520inference%2520latency%2520are%2520required%2520by%2520many%2520critical%2520applications.%2520Targeting%2520such%2520applications%2520deployed%2520on%2520FPGAs%252C%2520we%2520present%2520High%2520Granularity%2520Quantization%2520%2528HGQ%2529%252C%2520a%2520quantization-aware%2520training%2520framework%2520that%2520optimizes%2520parameter%2520bit-widths%2520through%2520gradient%2520descent.%2520Unlike%2520conventional%2520methods%252C%2520HGQ%2520determines%2520the%2520optimal%2520bit-width%2520for%2520each%2520parameter%2520independently%252C%2520making%2520it%2520suitable%2520for%2520hardware%2520platforms%2520supporting%2520heterogeneous%2520arbitrary%2520precision%2520arithmetic.%2520In%2520our%2520experiments%252C%2520HGQ%2520shows%2520superior%2520performance%2520compared%2520to%2520existing%2520network%2520compression%2520methods%252C%2520achieving%2520orders%2520of%2520magnitude%2520reduction%2520in%2520resource%2520consumption%2520and%2520latency%2520while%2520maintaining%2520the%2520accuracy%2520on%2520several%2520benchmark%2520tasks.%2520These%2520improvements%2520enable%2520the%2520deployment%2520of%2520complex%2520models%2520previously%2520infeasible%2520due%2520to%2520resource%2520or%2520latency%2520constraints.%2520HGQ%2520is%2520open-source%2520and%2520is%2520used%2520for%2520developing%2520next-generation%2520trigger%2520systems%2520at%2520the%2520CERN%2520ATLAS%2520and%2520CMS%2520experiments%2520for%2520particle%2520physics%252C%2520enabling%2520the%2520use%2520of%2520advanced%2520machine%2520learning%2520models%2520for%2520real-time%2520data%2520selection%2520with%2520sub-microsecond%2520latency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00645v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGQ%3A%20High%20Granularity%20Quantization%20for%20Real-time%20Neural%20Networks%20on%20FPGAs&entry.906535625=Chang%20Sun%20and%20Zhiqiang%20Que%20and%20Thea%20K.%20%C3%85rrestad%20and%20Vladimir%20Loncar%20and%20Jennifer%20Ngadiuba%20and%20Wayne%20Luk%20and%20Maria%20Spiropulu&entry.1292438233=Neural%20networks%20with%20sub-microsecond%20inference%20latency%20are%20required%20by%20many%20critical%20applications.%20Targeting%20such%20applications%20deployed%20on%20FPGAs%2C%20we%20present%20High%20Granularity%20Quantization%20%28HGQ%29%2C%20a%20quantization-aware%20training%20framework%20that%20optimizes%20parameter%20bit-widths%20through%20gradient%20descent.%20Unlike%20conventional%20methods%2C%20HGQ%20determines%20the%20optimal%20bit-width%20for%20each%20parameter%20independently%2C%20making%20it%20suitable%20for%20hardware%20platforms%20supporting%20heterogeneous%20arbitrary%20precision%20arithmetic.%20In%20our%20experiments%2C%20HGQ%20shows%20superior%20performance%20compared%20to%20existing%20network%20compression%20methods%2C%20achieving%20orders%20of%20magnitude%20reduction%20in%20resource%20consumption%20and%20latency%20while%20maintaining%20the%20accuracy%20on%20several%20benchmark%20tasks.%20These%20improvements%20enable%20the%20deployment%20of%20complex%20models%20previously%20infeasible%20due%20to%20resource%20or%20latency%20constraints.%20HGQ%20is%20open-source%20and%20is%20used%20for%20developing%20next-generation%20trigger%20systems%20at%20the%20CERN%20ATLAS%20and%20CMS%20experiments%20for%20particle%20physics%2C%20enabling%20the%20use%20of%20advanced%20machine%20learning%20models%20for%20real-time%20data%20selection%20with%20sub-microsecond%20latency.&entry.1838667208=http%3A//arxiv.org/abs/2405.00645v3&entry.124074799=Read"},
{"title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy", "author": "Aditya Gahlawat and Ahmed Aboudonia and Sandeep Banik and Naira Hovakimyan and Nikolai Matni and Aaron D. Ames and Gioele Zardini and Alberto Speranzon", "abstract": "Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \\ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \\textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.", "link": "http://arxiv.org/abs/2512.17899v1", "date": "2025-12-19", "relevancy": 2.0816, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5459}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5043}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributionally%20Robust%20Imitation%20Learning%3A%20Layered%20Control%20Architecture%20for%20Certifiable%20Autonomy&body=Title%3A%20Distributionally%20Robust%20Imitation%20Learning%3A%20Layered%20Control%20Architecture%20for%20Certifiable%20Autonomy%0AAuthor%3A%20Aditya%20Gahlawat%20and%20Ahmed%20Aboudonia%20and%20Sandeep%20Banik%20and%20Naira%20Hovakimyan%20and%20Nikolai%20Matni%20and%20Aaron%20D.%20Ames%20and%20Gioele%20Zardini%20and%20Alberto%20Speranzon%0AAbstract%3A%20Imitation%20learning%20%28IL%29%20enables%20autonomous%20behavior%20by%20learning%20from%20expert%20demonstrations.%20While%20more%20sample-efficient%20than%20comparative%20alternatives%20like%20reinforcement%20learning%2C%20IL%20is%20sensitive%20to%20compounding%20errors%20induced%20by%20distribution%20shifts.%20There%20are%20two%20significant%20sources%20of%20distribution%20shifts%20when%20using%20IL-based%20feedback%20laws%20on%20systems%3A%20distribution%20shifts%20caused%20by%20policy%20error%20and%20distribution%20shifts%20due%20to%20exogenous%20disturbances%20and%20endogenous%20model%20errors%20due%20to%20lack%20of%20learning.%20Our%20previously%20developed%20approaches%2C%20Taylor%20Series%20Imitation%20Learning%20%28TaSIL%29%20and%20%24%5Cmathcal%7BL%7D_1%24%20-Distributionally%20Robust%20Adaptive%20Control%20%28%5Cellonedrac%29%2C%20address%20the%20challenge%20of%20distribution%20shifts%20in%20complementary%20ways.%20While%20TaSIL%20offers%20robustness%20against%20policy%20error-induced%20distribution%20shifts%2C%20%5Cellonedrac%20offers%20robustness%20against%20distribution%20shifts%20due%20to%20aleatoric%20and%20epistemic%20uncertainties.%20To%20enable%20certifiable%20IL%20for%20learned%20and/or%20uncertain%20dynamical%20systems%2C%20we%20formulate%20%5Ctextit%7BDistributionally%20Robust%20Imitation%20Policy%20%28DRIP%29%7D%20architecture%2C%20a%20Layered%20Control%20Architecture%20%28LCA%29%20that%20integrates%20TaSIL%20and~%5Cellonedrac.%20By%20judiciously%20designing%20individual%20layer-centric%20input%20and%20output%20requirements%2C%20we%20show%20how%20we%20can%20guarantee%20certificates%20for%20the%20entire%20control%20pipeline.%20Our%20solution%20paves%20the%20path%20for%20designing%20fully%20certifiable%20autonomy%20pipelines%2C%20by%20integrating%20learning-based%20components%2C%20such%20as%20perception%2C%20with%20certifiable%20model-based%20decision-making%20through%20the%20proposed%20LCA%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributionally%2520Robust%2520Imitation%2520Learning%253A%2520Layered%2520Control%2520Architecture%2520for%2520Certifiable%2520Autonomy%26entry.906535625%3DAditya%2520Gahlawat%2520and%2520Ahmed%2520Aboudonia%2520and%2520Sandeep%2520Banik%2520and%2520Naira%2520Hovakimyan%2520and%2520Nikolai%2520Matni%2520and%2520Aaron%2520D.%2520Ames%2520and%2520Gioele%2520Zardini%2520and%2520Alberto%2520Speranzon%26entry.1292438233%3DImitation%2520learning%2520%2528IL%2529%2520enables%2520autonomous%2520behavior%2520by%2520learning%2520from%2520expert%2520demonstrations.%2520While%2520more%2520sample-efficient%2520than%2520comparative%2520alternatives%2520like%2520reinforcement%2520learning%252C%2520IL%2520is%2520sensitive%2520to%2520compounding%2520errors%2520induced%2520by%2520distribution%2520shifts.%2520There%2520are%2520two%2520significant%2520sources%2520of%2520distribution%2520shifts%2520when%2520using%2520IL-based%2520feedback%2520laws%2520on%2520systems%253A%2520distribution%2520shifts%2520caused%2520by%2520policy%2520error%2520and%2520distribution%2520shifts%2520due%2520to%2520exogenous%2520disturbances%2520and%2520endogenous%2520model%2520errors%2520due%2520to%2520lack%2520of%2520learning.%2520Our%2520previously%2520developed%2520approaches%252C%2520Taylor%2520Series%2520Imitation%2520Learning%2520%2528TaSIL%2529%2520and%2520%2524%255Cmathcal%257BL%257D_1%2524%2520-Distributionally%2520Robust%2520Adaptive%2520Control%2520%2528%255Cellonedrac%2529%252C%2520address%2520the%2520challenge%2520of%2520distribution%2520shifts%2520in%2520complementary%2520ways.%2520While%2520TaSIL%2520offers%2520robustness%2520against%2520policy%2520error-induced%2520distribution%2520shifts%252C%2520%255Cellonedrac%2520offers%2520robustness%2520against%2520distribution%2520shifts%2520due%2520to%2520aleatoric%2520and%2520epistemic%2520uncertainties.%2520To%2520enable%2520certifiable%2520IL%2520for%2520learned%2520and/or%2520uncertain%2520dynamical%2520systems%252C%2520we%2520formulate%2520%255Ctextit%257BDistributionally%2520Robust%2520Imitation%2520Policy%2520%2528DRIP%2529%257D%2520architecture%252C%2520a%2520Layered%2520Control%2520Architecture%2520%2528LCA%2529%2520that%2520integrates%2520TaSIL%2520and~%255Cellonedrac.%2520By%2520judiciously%2520designing%2520individual%2520layer-centric%2520input%2520and%2520output%2520requirements%252C%2520we%2520show%2520how%2520we%2520can%2520guarantee%2520certificates%2520for%2520the%2520entire%2520control%2520pipeline.%2520Our%2520solution%2520paves%2520the%2520path%2520for%2520designing%2520fully%2520certifiable%2520autonomy%2520pipelines%252C%2520by%2520integrating%2520learning-based%2520components%252C%2520such%2520as%2520perception%252C%2520with%2520certifiable%2520model-based%2520decision-making%2520through%2520the%2520proposed%2520LCA%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributionally%20Robust%20Imitation%20Learning%3A%20Layered%20Control%20Architecture%20for%20Certifiable%20Autonomy&entry.906535625=Aditya%20Gahlawat%20and%20Ahmed%20Aboudonia%20and%20Sandeep%20Banik%20and%20Naira%20Hovakimyan%20and%20Nikolai%20Matni%20and%20Aaron%20D.%20Ames%20and%20Gioele%20Zardini%20and%20Alberto%20Speranzon&entry.1292438233=Imitation%20learning%20%28IL%29%20enables%20autonomous%20behavior%20by%20learning%20from%20expert%20demonstrations.%20While%20more%20sample-efficient%20than%20comparative%20alternatives%20like%20reinforcement%20learning%2C%20IL%20is%20sensitive%20to%20compounding%20errors%20induced%20by%20distribution%20shifts.%20There%20are%20two%20significant%20sources%20of%20distribution%20shifts%20when%20using%20IL-based%20feedback%20laws%20on%20systems%3A%20distribution%20shifts%20caused%20by%20policy%20error%20and%20distribution%20shifts%20due%20to%20exogenous%20disturbances%20and%20endogenous%20model%20errors%20due%20to%20lack%20of%20learning.%20Our%20previously%20developed%20approaches%2C%20Taylor%20Series%20Imitation%20Learning%20%28TaSIL%29%20and%20%24%5Cmathcal%7BL%7D_1%24%20-Distributionally%20Robust%20Adaptive%20Control%20%28%5Cellonedrac%29%2C%20address%20the%20challenge%20of%20distribution%20shifts%20in%20complementary%20ways.%20While%20TaSIL%20offers%20robustness%20against%20policy%20error-induced%20distribution%20shifts%2C%20%5Cellonedrac%20offers%20robustness%20against%20distribution%20shifts%20due%20to%20aleatoric%20and%20epistemic%20uncertainties.%20To%20enable%20certifiable%20IL%20for%20learned%20and/or%20uncertain%20dynamical%20systems%2C%20we%20formulate%20%5Ctextit%7BDistributionally%20Robust%20Imitation%20Policy%20%28DRIP%29%7D%20architecture%2C%20a%20Layered%20Control%20Architecture%20%28LCA%29%20that%20integrates%20TaSIL%20and~%5Cellonedrac.%20By%20judiciously%20designing%20individual%20layer-centric%20input%20and%20output%20requirements%2C%20we%20show%20how%20we%20can%20guarantee%20certificates%20for%20the%20entire%20control%20pipeline.%20Our%20solution%20paves%20the%20path%20for%20designing%20fully%20certifiable%20autonomy%20pipelines%2C%20by%20integrating%20learning-based%20components%2C%20such%20as%20perception%2C%20with%20certifiable%20model-based%20decision-making%20through%20the%20proposed%20LCA%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2512.17899v1&entry.124074799=Read"},
{"title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "author": "Ko Watanabe and Stanislav Frolov and Aya Hassan and David Dembinsky and Adriano Lucieri and Andreas Dengel", "abstract": "Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.", "link": "http://arxiv.org/abs/2507.17860v3", "date": "2025-12-19", "relevancy": 2.0759, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5215}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5215}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Facilitated%20Fairness%20Assessment%20of%20AI-based%20Skin%20Lesion%20Classifiers%20Through%20GenAI-based%20Image%20Synthesis&body=Title%3A%20Towards%20Facilitated%20Fairness%20Assessment%20of%20AI-based%20Skin%20Lesion%20Classifiers%20Through%20GenAI-based%20Image%20Synthesis%0AAuthor%3A%20Ko%20Watanabe%20and%20Stanislav%20Frolov%20and%20Aya%20Hassan%20and%20David%20Dembinsky%20and%20Adriano%20Lucieri%20and%20Andreas%20Dengel%0AAbstract%3A%20Recent%20advances%20in%20deep%20learning%20and%20on-device%20inference%20could%20transform%20routine%20screening%20for%20skin%20cancers.%20Along%20with%20the%20anticipated%20benefits%20of%20this%20technology%2C%20potential%20dangers%20arise%20from%20unforeseen%20and%20inherent%20biases.%20A%20significant%20obstacle%20is%20building%20evaluation%20datasets%20that%20accurately%20reflect%20key%20demographics%2C%20including%20sex%2C%20age%2C%20and%20race%2C%20as%20well%20as%20other%20underrepresented%20groups.%20To%20address%20this%2C%20we%20train%20a%20state-of-the-art%20generative%20model%20to%20generate%20synthetic%20data%20in%20a%20controllable%20manner%20to%20assess%20the%20fairness%20of%20publicly%20available%20skin%20cancer%20classifiers.%20To%20evaluate%20whether%20synthetic%20images%20can%20be%20used%20as%20a%20fairness%20testing%20dataset%2C%20we%20prepare%20a%20real-image%20dataset%20%28MILK10K%29%20as%20a%20benchmark%20and%20compare%20the%20True%20Positive%20Rate%20result%20of%20three%20models%20%28DeepGuide%2C%20MelaNet%2C%20and%20SkinLesionDensnet%29.%20As%20a%20result%2C%20the%20classification%20tendencies%20observed%20in%20each%20model%20when%20tested%20on%20real%20and%20generated%20images%20showed%20similar%20patterns%20across%20different%20attribute%20data%20sets.%20We%20confirm%20that%20highly%20realistic%20synthetic%20images%20facilitate%20model%20fairness%20verification.%0ALink%3A%20http%3A//arxiv.org/abs/2507.17860v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Facilitated%2520Fairness%2520Assessment%2520of%2520AI-based%2520Skin%2520Lesion%2520Classifiers%2520Through%2520GenAI-based%2520Image%2520Synthesis%26entry.906535625%3DKo%2520Watanabe%2520and%2520Stanislav%2520Frolov%2520and%2520Aya%2520Hassan%2520and%2520David%2520Dembinsky%2520and%2520Adriano%2520Lucieri%2520and%2520Andreas%2520Dengel%26entry.1292438233%3DRecent%2520advances%2520in%2520deep%2520learning%2520and%2520on-device%2520inference%2520could%2520transform%2520routine%2520screening%2520for%2520skin%2520cancers.%2520Along%2520with%2520the%2520anticipated%2520benefits%2520of%2520this%2520technology%252C%2520potential%2520dangers%2520arise%2520from%2520unforeseen%2520and%2520inherent%2520biases.%2520A%2520significant%2520obstacle%2520is%2520building%2520evaluation%2520datasets%2520that%2520accurately%2520reflect%2520key%2520demographics%252C%2520including%2520sex%252C%2520age%252C%2520and%2520race%252C%2520as%2520well%2520as%2520other%2520underrepresented%2520groups.%2520To%2520address%2520this%252C%2520we%2520train%2520a%2520state-of-the-art%2520generative%2520model%2520to%2520generate%2520synthetic%2520data%2520in%2520a%2520controllable%2520manner%2520to%2520assess%2520the%2520fairness%2520of%2520publicly%2520available%2520skin%2520cancer%2520classifiers.%2520To%2520evaluate%2520whether%2520synthetic%2520images%2520can%2520be%2520used%2520as%2520a%2520fairness%2520testing%2520dataset%252C%2520we%2520prepare%2520a%2520real-image%2520dataset%2520%2528MILK10K%2529%2520as%2520a%2520benchmark%2520and%2520compare%2520the%2520True%2520Positive%2520Rate%2520result%2520of%2520three%2520models%2520%2528DeepGuide%252C%2520MelaNet%252C%2520and%2520SkinLesionDensnet%2529.%2520As%2520a%2520result%252C%2520the%2520classification%2520tendencies%2520observed%2520in%2520each%2520model%2520when%2520tested%2520on%2520real%2520and%2520generated%2520images%2520showed%2520similar%2520patterns%2520across%2520different%2520attribute%2520data%2520sets.%2520We%2520confirm%2520that%2520highly%2520realistic%2520synthetic%2520images%2520facilitate%2520model%2520fairness%2520verification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17860v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Facilitated%20Fairness%20Assessment%20of%20AI-based%20Skin%20Lesion%20Classifiers%20Through%20GenAI-based%20Image%20Synthesis&entry.906535625=Ko%20Watanabe%20and%20Stanislav%20Frolov%20and%20Aya%20Hassan%20and%20David%20Dembinsky%20and%20Adriano%20Lucieri%20and%20Andreas%20Dengel&entry.1292438233=Recent%20advances%20in%20deep%20learning%20and%20on-device%20inference%20could%20transform%20routine%20screening%20for%20skin%20cancers.%20Along%20with%20the%20anticipated%20benefits%20of%20this%20technology%2C%20potential%20dangers%20arise%20from%20unforeseen%20and%20inherent%20biases.%20A%20significant%20obstacle%20is%20building%20evaluation%20datasets%20that%20accurately%20reflect%20key%20demographics%2C%20including%20sex%2C%20age%2C%20and%20race%2C%20as%20well%20as%20other%20underrepresented%20groups.%20To%20address%20this%2C%20we%20train%20a%20state-of-the-art%20generative%20model%20to%20generate%20synthetic%20data%20in%20a%20controllable%20manner%20to%20assess%20the%20fairness%20of%20publicly%20available%20skin%20cancer%20classifiers.%20To%20evaluate%20whether%20synthetic%20images%20can%20be%20used%20as%20a%20fairness%20testing%20dataset%2C%20we%20prepare%20a%20real-image%20dataset%20%28MILK10K%29%20as%20a%20benchmark%20and%20compare%20the%20True%20Positive%20Rate%20result%20of%20three%20models%20%28DeepGuide%2C%20MelaNet%2C%20and%20SkinLesionDensnet%29.%20As%20a%20result%2C%20the%20classification%20tendencies%20observed%20in%20each%20model%20when%20tested%20on%20real%20and%20generated%20images%20showed%20similar%20patterns%20across%20different%20attribute%20data%20sets.%20We%20confirm%20that%20highly%20realistic%20synthetic%20images%20facilitate%20model%20fairness%20verification.&entry.1838667208=http%3A//arxiv.org/abs/2507.17860v3&entry.124074799=Read"},
{"title": "Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework", "author": "Md. Irtiza Hossain and Junaid Ahmed Sifat and Abir Chowdhury", "abstract": "The digitization of structured handwritten documents, such as academic marksheets, remains a significant challenge due to the dual complexity of irregular table structures and diverse handwriting styles. While recent Transformer-based approaches like TableNet and TrOCR achieve state-of-the-art accuracy, their high computational cost renders them unsuitable for resource-constrained edge deployments. This paper introduces a resource-efficient hybrid framework that integrates a heuristic OpenCV-based pipeline for rapid table structure detection with a modified lightweight YOLOv8 architecture for handwritten character recognition. By strategically removing the SPPF and deep C2f layers from the standard YOLOv8 backbone, we reduce computational overhead while maintaining high recognition fidelity. Experimental results on the EMNIST digit benchmark demonstrate that our Modified YOLOv8 model achieves 97.5% accuracy. Furthermore, we provide a comprehensive efficiency analysis showing that our framework offers a 95 times inference speedup over standard OCR pipelines and massive efficiency gains over emerging Large Multimodal Models (LMMs) like Qwen2.5-VL, achieving real-time performance 29 FPS on standard CPU hardware. A qualitative and quantitative evaluation on the AMES dataset, a challenging subset of real-world marksheets, confirms the system's robustness in handling mixed alphanumeric content, bridging the gap between high-performance deep learning and practical, scalable document automation.", "link": "http://arxiv.org/abs/2508.16295v2", "date": "2025-12-19", "relevancy": 2.0756, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5231}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Native%20Digitization%20of%20Handwritten%20Marksheets%3A%20A%20Hybrid%20Heuristic-Deep%20Learning%20Framework&body=Title%3A%20Edge-Native%20Digitization%20of%20Handwritten%20Marksheets%3A%20A%20Hybrid%20Heuristic-Deep%20Learning%20Framework%0AAuthor%3A%20Md.%20Irtiza%20Hossain%20and%20Junaid%20Ahmed%20Sifat%20and%20Abir%20Chowdhury%0AAbstract%3A%20The%20digitization%20of%20structured%20handwritten%20documents%2C%20such%20as%20academic%20marksheets%2C%20remains%20a%20significant%20challenge%20due%20to%20the%20dual%20complexity%20of%20irregular%20table%20structures%20and%20diverse%20handwriting%20styles.%20While%20recent%20Transformer-based%20approaches%20like%20TableNet%20and%20TrOCR%20achieve%20state-of-the-art%20accuracy%2C%20their%20high%20computational%20cost%20renders%20them%20unsuitable%20for%20resource-constrained%20edge%20deployments.%20This%20paper%20introduces%20a%20resource-efficient%20hybrid%20framework%20that%20integrates%20a%20heuristic%20OpenCV-based%20pipeline%20for%20rapid%20table%20structure%20detection%20with%20a%20modified%20lightweight%20YOLOv8%20architecture%20for%20handwritten%20character%20recognition.%20By%20strategically%20removing%20the%20SPPF%20and%20deep%20C2f%20layers%20from%20the%20standard%20YOLOv8%20backbone%2C%20we%20reduce%20computational%20overhead%20while%20maintaining%20high%20recognition%20fidelity.%20Experimental%20results%20on%20the%20EMNIST%20digit%20benchmark%20demonstrate%20that%20our%20Modified%20YOLOv8%20model%20achieves%2097.5%25%20accuracy.%20Furthermore%2C%20we%20provide%20a%20comprehensive%20efficiency%20analysis%20showing%20that%20our%20framework%20offers%20a%2095%20times%20inference%20speedup%20over%20standard%20OCR%20pipelines%20and%20massive%20efficiency%20gains%20over%20emerging%20Large%20Multimodal%20Models%20%28LMMs%29%20like%20Qwen2.5-VL%2C%20achieving%20real-time%20performance%2029%20FPS%20on%20standard%20CPU%20hardware.%20A%20qualitative%20and%20quantitative%20evaluation%20on%20the%20AMES%20dataset%2C%20a%20challenging%20subset%20of%20real-world%20marksheets%2C%20confirms%20the%20system%27s%20robustness%20in%20handling%20mixed%20alphanumeric%20content%2C%20bridging%20the%20gap%20between%20high-performance%20deep%20learning%20and%20practical%2C%20scalable%20document%20automation.%0ALink%3A%20http%3A//arxiv.org/abs/2508.16295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Native%2520Digitization%2520of%2520Handwritten%2520Marksheets%253A%2520A%2520Hybrid%2520Heuristic-Deep%2520Learning%2520Framework%26entry.906535625%3DMd.%2520Irtiza%2520Hossain%2520and%2520Junaid%2520Ahmed%2520Sifat%2520and%2520Abir%2520Chowdhury%26entry.1292438233%3DThe%2520digitization%2520of%2520structured%2520handwritten%2520documents%252C%2520such%2520as%2520academic%2520marksheets%252C%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520dual%2520complexity%2520of%2520irregular%2520table%2520structures%2520and%2520diverse%2520handwriting%2520styles.%2520While%2520recent%2520Transformer-based%2520approaches%2520like%2520TableNet%2520and%2520TrOCR%2520achieve%2520state-of-the-art%2520accuracy%252C%2520their%2520high%2520computational%2520cost%2520renders%2520them%2520unsuitable%2520for%2520resource-constrained%2520edge%2520deployments.%2520This%2520paper%2520introduces%2520a%2520resource-efficient%2520hybrid%2520framework%2520that%2520integrates%2520a%2520heuristic%2520OpenCV-based%2520pipeline%2520for%2520rapid%2520table%2520structure%2520detection%2520with%2520a%2520modified%2520lightweight%2520YOLOv8%2520architecture%2520for%2520handwritten%2520character%2520recognition.%2520By%2520strategically%2520removing%2520the%2520SPPF%2520and%2520deep%2520C2f%2520layers%2520from%2520the%2520standard%2520YOLOv8%2520backbone%252C%2520we%2520reduce%2520computational%2520overhead%2520while%2520maintaining%2520high%2520recognition%2520fidelity.%2520Experimental%2520results%2520on%2520the%2520EMNIST%2520digit%2520benchmark%2520demonstrate%2520that%2520our%2520Modified%2520YOLOv8%2520model%2520achieves%252097.5%2525%2520accuracy.%2520Furthermore%252C%2520we%2520provide%2520a%2520comprehensive%2520efficiency%2520analysis%2520showing%2520that%2520our%2520framework%2520offers%2520a%252095%2520times%2520inference%2520speedup%2520over%2520standard%2520OCR%2520pipelines%2520and%2520massive%2520efficiency%2520gains%2520over%2520emerging%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520like%2520Qwen2.5-VL%252C%2520achieving%2520real-time%2520performance%252029%2520FPS%2520on%2520standard%2520CPU%2520hardware.%2520A%2520qualitative%2520and%2520quantitative%2520evaluation%2520on%2520the%2520AMES%2520dataset%252C%2520a%2520challenging%2520subset%2520of%2520real-world%2520marksheets%252C%2520confirms%2520the%2520system%2527s%2520robustness%2520in%2520handling%2520mixed%2520alphanumeric%2520content%252C%2520bridging%2520the%2520gap%2520between%2520high-performance%2520deep%2520learning%2520and%2520practical%252C%2520scalable%2520document%2520automation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Native%20Digitization%20of%20Handwritten%20Marksheets%3A%20A%20Hybrid%20Heuristic-Deep%20Learning%20Framework&entry.906535625=Md.%20Irtiza%20Hossain%20and%20Junaid%20Ahmed%20Sifat%20and%20Abir%20Chowdhury&entry.1292438233=The%20digitization%20of%20structured%20handwritten%20documents%2C%20such%20as%20academic%20marksheets%2C%20remains%20a%20significant%20challenge%20due%20to%20the%20dual%20complexity%20of%20irregular%20table%20structures%20and%20diverse%20handwriting%20styles.%20While%20recent%20Transformer-based%20approaches%20like%20TableNet%20and%20TrOCR%20achieve%20state-of-the-art%20accuracy%2C%20their%20high%20computational%20cost%20renders%20them%20unsuitable%20for%20resource-constrained%20edge%20deployments.%20This%20paper%20introduces%20a%20resource-efficient%20hybrid%20framework%20that%20integrates%20a%20heuristic%20OpenCV-based%20pipeline%20for%20rapid%20table%20structure%20detection%20with%20a%20modified%20lightweight%20YOLOv8%20architecture%20for%20handwritten%20character%20recognition.%20By%20strategically%20removing%20the%20SPPF%20and%20deep%20C2f%20layers%20from%20the%20standard%20YOLOv8%20backbone%2C%20we%20reduce%20computational%20overhead%20while%20maintaining%20high%20recognition%20fidelity.%20Experimental%20results%20on%20the%20EMNIST%20digit%20benchmark%20demonstrate%20that%20our%20Modified%20YOLOv8%20model%20achieves%2097.5%25%20accuracy.%20Furthermore%2C%20we%20provide%20a%20comprehensive%20efficiency%20analysis%20showing%20that%20our%20framework%20offers%20a%2095%20times%20inference%20speedup%20over%20standard%20OCR%20pipelines%20and%20massive%20efficiency%20gains%20over%20emerging%20Large%20Multimodal%20Models%20%28LMMs%29%20like%20Qwen2.5-VL%2C%20achieving%20real-time%20performance%2029%20FPS%20on%20standard%20CPU%20hardware.%20A%20qualitative%20and%20quantitative%20evaluation%20on%20the%20AMES%20dataset%2C%20a%20challenging%20subset%20of%20real-world%20marksheets%2C%20confirms%20the%20system%27s%20robustness%20in%20handling%20mixed%20alphanumeric%20content%2C%20bridging%20the%20gap%20between%20high-performance%20deep%20learning%20and%20practical%2C%20scalable%20document%20automation.&entry.1838667208=http%3A//arxiv.org/abs/2508.16295v2&entry.124074799=Read"},
{"title": "STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation", "author": "Vipooshan Vipulananthan and Kumudu Mohottala and Kavindu Chinthana and Nimsara Paramulla and Charith D Chitraranjan", "abstract": "Accident prediction and timely preventive actions improve road safety by reducing the risk of injury to road users and minimizing property damage. Hence, they are critical components of advanced driver assistance systems (ADAS) and autonomous vehicles. While many existing systems depend on multiple sensors such as LiDAR, radar, and GPS, relying solely on dash-cam videos presents a more challenging, yet more cost-effective and easily deployable solution. In this work, we incorporate improved spatio-temporal features and aggregate them through a recurrent network to enhance state-of-the-art graph neural networks for predicting accidents from dash-cam videos. Experiments using three publicly available datasets (DAD, DoTA and DADA) show that our proposed STAGNet model achieves higher average precision and mean time-to-accident scores than previous methods, both when cross-validated on a given dataset and when trained and tested on different datasets.", "link": "http://arxiv.org/abs/2508.15216v3", "date": "2025-12-19", "relevancy": 2.0625, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5203}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAGNet%3A%20A%20Spatio-Temporal%20Graph%20and%20LSTM%20Framework%20for%20Accident%20Anticipation&body=Title%3A%20STAGNet%3A%20A%20Spatio-Temporal%20Graph%20and%20LSTM%20Framework%20for%20Accident%20Anticipation%0AAuthor%3A%20Vipooshan%20Vipulananthan%20and%20Kumudu%20Mohottala%20and%20Kavindu%20Chinthana%20and%20Nimsara%20Paramulla%20and%20Charith%20D%20Chitraranjan%0AAbstract%3A%20Accident%20prediction%20and%20timely%20preventive%20actions%20improve%20road%20safety%20by%20reducing%20the%20risk%20of%20injury%20to%20road%20users%20and%20minimizing%20property%20damage.%20Hence%2C%20they%20are%20critical%20components%20of%20advanced%20driver%20assistance%20systems%20%28ADAS%29%20and%20autonomous%20vehicles.%20While%20many%20existing%20systems%20depend%20on%20multiple%20sensors%20such%20as%20LiDAR%2C%20radar%2C%20and%20GPS%2C%20relying%20solely%20on%20dash-cam%20videos%20presents%20a%20more%20challenging%2C%20yet%20more%20cost-effective%20and%20easily%20deployable%20solution.%20In%20this%20work%2C%20we%20incorporate%20improved%20spatio-temporal%20features%20and%20aggregate%20them%20through%20a%20recurrent%20network%20to%20enhance%20state-of-the-art%20graph%20neural%20networks%20for%20predicting%20accidents%20from%20dash-cam%20videos.%20Experiments%20using%20three%20publicly%20available%20datasets%20%28DAD%2C%20DoTA%20and%20DADA%29%20show%20that%20our%20proposed%20STAGNet%20model%20achieves%20higher%20average%20precision%20and%20mean%20time-to-accident%20scores%20than%20previous%20methods%2C%20both%20when%20cross-validated%20on%20a%20given%20dataset%20and%20when%20trained%20and%20tested%20on%20different%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2508.15216v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAGNet%253A%2520A%2520Spatio-Temporal%2520Graph%2520and%2520LSTM%2520Framework%2520for%2520Accident%2520Anticipation%26entry.906535625%3DVipooshan%2520Vipulananthan%2520and%2520Kumudu%2520Mohottala%2520and%2520Kavindu%2520Chinthana%2520and%2520Nimsara%2520Paramulla%2520and%2520Charith%2520D%2520Chitraranjan%26entry.1292438233%3DAccident%2520prediction%2520and%2520timely%2520preventive%2520actions%2520improve%2520road%2520safety%2520by%2520reducing%2520the%2520risk%2520of%2520injury%2520to%2520road%2520users%2520and%2520minimizing%2520property%2520damage.%2520Hence%252C%2520they%2520are%2520critical%2520components%2520of%2520advanced%2520driver%2520assistance%2520systems%2520%2528ADAS%2529%2520and%2520autonomous%2520vehicles.%2520While%2520many%2520existing%2520systems%2520depend%2520on%2520multiple%2520sensors%2520such%2520as%2520LiDAR%252C%2520radar%252C%2520and%2520GPS%252C%2520relying%2520solely%2520on%2520dash-cam%2520videos%2520presents%2520a%2520more%2520challenging%252C%2520yet%2520more%2520cost-effective%2520and%2520easily%2520deployable%2520solution.%2520In%2520this%2520work%252C%2520we%2520incorporate%2520improved%2520spatio-temporal%2520features%2520and%2520aggregate%2520them%2520through%2520a%2520recurrent%2520network%2520to%2520enhance%2520state-of-the-art%2520graph%2520neural%2520networks%2520for%2520predicting%2520accidents%2520from%2520dash-cam%2520videos.%2520Experiments%2520using%2520three%2520publicly%2520available%2520datasets%2520%2528DAD%252C%2520DoTA%2520and%2520DADA%2529%2520show%2520that%2520our%2520proposed%2520STAGNet%2520model%2520achieves%2520higher%2520average%2520precision%2520and%2520mean%2520time-to-accident%2520scores%2520than%2520previous%2520methods%252C%2520both%2520when%2520cross-validated%2520on%2520a%2520given%2520dataset%2520and%2520when%2520trained%2520and%2520tested%2520on%2520different%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15216v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAGNet%3A%20A%20Spatio-Temporal%20Graph%20and%20LSTM%20Framework%20for%20Accident%20Anticipation&entry.906535625=Vipooshan%20Vipulananthan%20and%20Kumudu%20Mohottala%20and%20Kavindu%20Chinthana%20and%20Nimsara%20Paramulla%20and%20Charith%20D%20Chitraranjan&entry.1292438233=Accident%20prediction%20and%20timely%20preventive%20actions%20improve%20road%20safety%20by%20reducing%20the%20risk%20of%20injury%20to%20road%20users%20and%20minimizing%20property%20damage.%20Hence%2C%20they%20are%20critical%20components%20of%20advanced%20driver%20assistance%20systems%20%28ADAS%29%20and%20autonomous%20vehicles.%20While%20many%20existing%20systems%20depend%20on%20multiple%20sensors%20such%20as%20LiDAR%2C%20radar%2C%20and%20GPS%2C%20relying%20solely%20on%20dash-cam%20videos%20presents%20a%20more%20challenging%2C%20yet%20more%20cost-effective%20and%20easily%20deployable%20solution.%20In%20this%20work%2C%20we%20incorporate%20improved%20spatio-temporal%20features%20and%20aggregate%20them%20through%20a%20recurrent%20network%20to%20enhance%20state-of-the-art%20graph%20neural%20networks%20for%20predicting%20accidents%20from%20dash-cam%20videos.%20Experiments%20using%20three%20publicly%20available%20datasets%20%28DAD%2C%20DoTA%20and%20DADA%29%20show%20that%20our%20proposed%20STAGNet%20model%20achieves%20higher%20average%20precision%20and%20mean%20time-to-accident%20scores%20than%20previous%20methods%2C%20both%20when%20cross-validated%20on%20a%20given%20dataset%20and%20when%20trained%20and%20tested%20on%20different%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2508.15216v3&entry.124074799=Read"},
{"title": "Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents", "author": "Paul Mangold and Elo\u00efse Berthier and Eric Moulines", "abstract": "We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.", "link": "http://arxiv.org/abs/2512.17688v1", "date": "2025-12-19", "relevancy": 1.797, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4829}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4525}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20Guarantees%20for%20Federated%20SARSA%20with%20Local%20Training%20and%20Heterogeneous%20Agents&body=Title%3A%20Convergence%20Guarantees%20for%20Federated%20SARSA%20with%20Local%20Training%20and%20Heterogeneous%20Agents%0AAuthor%3A%20Paul%20Mangold%20and%20Elo%C3%AFse%20Berthier%20and%20Eric%20Moulines%0AAbstract%3A%20We%20present%20a%20novel%20theoretical%20analysis%20of%20Federated%20SARSA%20%28FedSARSA%29%20with%20linear%20function%20approximation%20and%20local%20training.%20We%20establish%20convergence%20guarantees%20for%20FedSARSA%20in%20the%20presence%20of%20heterogeneity%2C%20both%20in%20local%20transitions%20and%20rewards%2C%20providing%20the%20first%20sample%20and%20communication%20complexity%20bounds%20in%20this%20setting.%20At%20the%20core%20of%20our%20analysis%20is%20a%20new%2C%20exact%20multi-step%20error%20expansion%20for%20single-agent%20SARSA%2C%20which%20is%20of%20independent%20interest.%20Our%20analysis%20precisely%20quantifies%20the%20impact%20of%20heterogeneity%2C%20demonstrating%20the%20convergence%20of%20FedSARSA%20with%20multiple%20local%20updates.%20Crucially%2C%20we%20show%20that%20FedSARSA%20achieves%20linear%20speed-up%20with%20respect%20to%20the%20number%20of%20agents%2C%20up%20to%20higher-order%20terms%20due%20to%20Markovian%20sampling.%20Numerical%20experiments%20support%20our%20theoretical%20findings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520Guarantees%2520for%2520Federated%2520SARSA%2520with%2520Local%2520Training%2520and%2520Heterogeneous%2520Agents%26entry.906535625%3DPaul%2520Mangold%2520and%2520Elo%25C3%25AFse%2520Berthier%2520and%2520Eric%2520Moulines%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520theoretical%2520analysis%2520of%2520Federated%2520SARSA%2520%2528FedSARSA%2529%2520with%2520linear%2520function%2520approximation%2520and%2520local%2520training.%2520We%2520establish%2520convergence%2520guarantees%2520for%2520FedSARSA%2520in%2520the%2520presence%2520of%2520heterogeneity%252C%2520both%2520in%2520local%2520transitions%2520and%2520rewards%252C%2520providing%2520the%2520first%2520sample%2520and%2520communication%2520complexity%2520bounds%2520in%2520this%2520setting.%2520At%2520the%2520core%2520of%2520our%2520analysis%2520is%2520a%2520new%252C%2520exact%2520multi-step%2520error%2520expansion%2520for%2520single-agent%2520SARSA%252C%2520which%2520is%2520of%2520independent%2520interest.%2520Our%2520analysis%2520precisely%2520quantifies%2520the%2520impact%2520of%2520heterogeneity%252C%2520demonstrating%2520the%2520convergence%2520of%2520FedSARSA%2520with%2520multiple%2520local%2520updates.%2520Crucially%252C%2520we%2520show%2520that%2520FedSARSA%2520achieves%2520linear%2520speed-up%2520with%2520respect%2520to%2520the%2520number%2520of%2520agents%252C%2520up%2520to%2520higher-order%2520terms%2520due%2520to%2520Markovian%2520sampling.%2520Numerical%2520experiments%2520support%2520our%2520theoretical%2520findings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Guarantees%20for%20Federated%20SARSA%20with%20Local%20Training%20and%20Heterogeneous%20Agents&entry.906535625=Paul%20Mangold%20and%20Elo%C3%AFse%20Berthier%20and%20Eric%20Moulines&entry.1292438233=We%20present%20a%20novel%20theoretical%20analysis%20of%20Federated%20SARSA%20%28FedSARSA%29%20with%20linear%20function%20approximation%20and%20local%20training.%20We%20establish%20convergence%20guarantees%20for%20FedSARSA%20in%20the%20presence%20of%20heterogeneity%2C%20both%20in%20local%20transitions%20and%20rewards%2C%20providing%20the%20first%20sample%20and%20communication%20complexity%20bounds%20in%20this%20setting.%20At%20the%20core%20of%20our%20analysis%20is%20a%20new%2C%20exact%20multi-step%20error%20expansion%20for%20single-agent%20SARSA%2C%20which%20is%20of%20independent%20interest.%20Our%20analysis%20precisely%20quantifies%20the%20impact%20of%20heterogeneity%2C%20demonstrating%20the%20convergence%20of%20FedSARSA%20with%20multiple%20local%20updates.%20Crucially%2C%20we%20show%20that%20FedSARSA%20achieves%20linear%20speed-up%20with%20respect%20to%20the%20number%20of%20agents%2C%20up%20to%20higher-order%20terms%20due%20to%20Markovian%20sampling.%20Numerical%20experiments%20support%20our%20theoretical%20findings.&entry.1838667208=http%3A//arxiv.org/abs/2512.17688v1&entry.124074799=Read"},
{"title": "Preconditioned Inexact Stochastic ADMM for Deep Model", "author": "Shenglong Zhou and Ouya Wang and Ziyan Luo and Yongxu Zhu and Geoffrey Ye Li", "abstract": "The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers). Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables the proposed algorithm to tackle the challenge of data heterogeneity effectively. Moreover, the algorithmic architecture enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Incorporating the latter two preconditions in PISA yields two computationally efficient variants: SISA and NSISA. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate superior numerical performance of SISA and NSISA compared to various state-of-the-art optimizers.", "link": "http://arxiv.org/abs/2502.10784v5", "date": "2025-12-19", "relevancy": 1.5904, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5677}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5263}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preconditioned%20Inexact%20Stochastic%20ADMM%20for%20Deep%20Model&body=Title%3A%20Preconditioned%20Inexact%20Stochastic%20ADMM%20for%20Deep%20Model%0AAuthor%3A%20Shenglong%20Zhou%20and%20Ouya%20Wang%20and%20Ziyan%20Luo%20and%20Yongxu%20Zhu%20and%20Geoffrey%20Ye%20Li%0AAbstract%3A%20The%20recent%20advancement%20of%20foundation%20models%20%28FMs%29%20has%20brought%20about%20a%20paradigm%20shift%2C%20revolutionizing%20various%20sectors%20worldwide.%20The%20popular%20optimizers%20used%20to%20train%20these%20models%20are%20stochastic%20gradient%20descent-based%20algorithms%2C%20which%20face%20inherent%20limitations%2C%20such%20as%20slow%20convergence%20and%20stringent%20assumptions%20for%20convergence.%20In%20particular%2C%20data%20heterogeneity%20arising%20from%20distributed%20settings%20poses%20significant%20challenges%20to%20their%20theoretical%20and%20numerical%20performance.%20This%20paper%20develops%20an%20algorithm%2C%20PISA%20%28Preconditioned%20Inexact%20Stochastic%20Alternating%20Direction%20Method%20of%20Multipliers%29.%20Grounded%20in%20rigorous%20theoretical%20guarantees%2C%20the%20algorithm%20converges%20under%20the%20sole%20assumption%20of%20Lipschitz%20continuity%20of%20the%20gradient%20on%20a%20bounded%20region%2C%20thereby%20removing%20the%20need%20for%20other%20conditions%20commonly%20imposed%20by%20stochastic%20methods.%20This%20capability%20enables%20the%20proposed%20algorithm%20to%20tackle%20the%20challenge%20of%20data%20heterogeneity%20effectively.%20Moreover%2C%20the%20algorithmic%20architecture%20enables%20scalable%20parallel%20computing%20and%20supports%20various%20preconditions%2C%20such%20as%20second-order%20information%2C%20second%20moment%2C%20and%20orthogonalized%20momentum%20by%20Newton-Schulz%20iterations.%20Incorporating%20the%20latter%20two%20preconditions%20in%20PISA%20yields%20two%20computationally%20efficient%20variants%3A%20SISA%20and%20NSISA.%20Comprehensive%20experimental%20evaluations%20for%20training%20or%20fine-tuning%20diverse%20deep%20models%2C%20including%20vision%20models%2C%20large%20language%20models%2C%20reinforcement%20learning%20models%2C%20generative%20adversarial%20networks%2C%20and%20recurrent%20neural%20networks%2C%20demonstrate%20superior%20numerical%20performance%20of%20SISA%20and%20NSISA%20compared%20to%20various%20state-of-the-art%20optimizers.%0ALink%3A%20http%3A//arxiv.org/abs/2502.10784v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreconditioned%2520Inexact%2520Stochastic%2520ADMM%2520for%2520Deep%2520Model%26entry.906535625%3DShenglong%2520Zhou%2520and%2520Ouya%2520Wang%2520and%2520Ziyan%2520Luo%2520and%2520Yongxu%2520Zhu%2520and%2520Geoffrey%2520Ye%2520Li%26entry.1292438233%3DThe%2520recent%2520advancement%2520of%2520foundation%2520models%2520%2528FMs%2529%2520has%2520brought%2520about%2520a%2520paradigm%2520shift%252C%2520revolutionizing%2520various%2520sectors%2520worldwide.%2520The%2520popular%2520optimizers%2520used%2520to%2520train%2520these%2520models%2520are%2520stochastic%2520gradient%2520descent-based%2520algorithms%252C%2520which%2520face%2520inherent%2520limitations%252C%2520such%2520as%2520slow%2520convergence%2520and%2520stringent%2520assumptions%2520for%2520convergence.%2520In%2520particular%252C%2520data%2520heterogeneity%2520arising%2520from%2520distributed%2520settings%2520poses%2520significant%2520challenges%2520to%2520their%2520theoretical%2520and%2520numerical%2520performance.%2520This%2520paper%2520develops%2520an%2520algorithm%252C%2520PISA%2520%2528Preconditioned%2520Inexact%2520Stochastic%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%2529.%2520Grounded%2520in%2520rigorous%2520theoretical%2520guarantees%252C%2520the%2520algorithm%2520converges%2520under%2520the%2520sole%2520assumption%2520of%2520Lipschitz%2520continuity%2520of%2520the%2520gradient%2520on%2520a%2520bounded%2520region%252C%2520thereby%2520removing%2520the%2520need%2520for%2520other%2520conditions%2520commonly%2520imposed%2520by%2520stochastic%2520methods.%2520This%2520capability%2520enables%2520the%2520proposed%2520algorithm%2520to%2520tackle%2520the%2520challenge%2520of%2520data%2520heterogeneity%2520effectively.%2520Moreover%252C%2520the%2520algorithmic%2520architecture%2520enables%2520scalable%2520parallel%2520computing%2520and%2520supports%2520various%2520preconditions%252C%2520such%2520as%2520second-order%2520information%252C%2520second%2520moment%252C%2520and%2520orthogonalized%2520momentum%2520by%2520Newton-Schulz%2520iterations.%2520Incorporating%2520the%2520latter%2520two%2520preconditions%2520in%2520PISA%2520yields%2520two%2520computationally%2520efficient%2520variants%253A%2520SISA%2520and%2520NSISA.%2520Comprehensive%2520experimental%2520evaluations%2520for%2520training%2520or%2520fine-tuning%2520diverse%2520deep%2520models%252C%2520including%2520vision%2520models%252C%2520large%2520language%2520models%252C%2520reinforcement%2520learning%2520models%252C%2520generative%2520adversarial%2520networks%252C%2520and%2520recurrent%2520neural%2520networks%252C%2520demonstrate%2520superior%2520numerical%2520performance%2520of%2520SISA%2520and%2520NSISA%2520compared%2520to%2520various%2520state-of-the-art%2520optimizers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10784v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preconditioned%20Inexact%20Stochastic%20ADMM%20for%20Deep%20Model&entry.906535625=Shenglong%20Zhou%20and%20Ouya%20Wang%20and%20Ziyan%20Luo%20and%20Yongxu%20Zhu%20and%20Geoffrey%20Ye%20Li&entry.1292438233=The%20recent%20advancement%20of%20foundation%20models%20%28FMs%29%20has%20brought%20about%20a%20paradigm%20shift%2C%20revolutionizing%20various%20sectors%20worldwide.%20The%20popular%20optimizers%20used%20to%20train%20these%20models%20are%20stochastic%20gradient%20descent-based%20algorithms%2C%20which%20face%20inherent%20limitations%2C%20such%20as%20slow%20convergence%20and%20stringent%20assumptions%20for%20convergence.%20In%20particular%2C%20data%20heterogeneity%20arising%20from%20distributed%20settings%20poses%20significant%20challenges%20to%20their%20theoretical%20and%20numerical%20performance.%20This%20paper%20develops%20an%20algorithm%2C%20PISA%20%28Preconditioned%20Inexact%20Stochastic%20Alternating%20Direction%20Method%20of%20Multipliers%29.%20Grounded%20in%20rigorous%20theoretical%20guarantees%2C%20the%20algorithm%20converges%20under%20the%20sole%20assumption%20of%20Lipschitz%20continuity%20of%20the%20gradient%20on%20a%20bounded%20region%2C%20thereby%20removing%20the%20need%20for%20other%20conditions%20commonly%20imposed%20by%20stochastic%20methods.%20This%20capability%20enables%20the%20proposed%20algorithm%20to%20tackle%20the%20challenge%20of%20data%20heterogeneity%20effectively.%20Moreover%2C%20the%20algorithmic%20architecture%20enables%20scalable%20parallel%20computing%20and%20supports%20various%20preconditions%2C%20such%20as%20second-order%20information%2C%20second%20moment%2C%20and%20orthogonalized%20momentum%20by%20Newton-Schulz%20iterations.%20Incorporating%20the%20latter%20two%20preconditions%20in%20PISA%20yields%20two%20computationally%20efficient%20variants%3A%20SISA%20and%20NSISA.%20Comprehensive%20experimental%20evaluations%20for%20training%20or%20fine-tuning%20diverse%20deep%20models%2C%20including%20vision%20models%2C%20large%20language%20models%2C%20reinforcement%20learning%20models%2C%20generative%20adversarial%20networks%2C%20and%20recurrent%20neural%20networks%2C%20demonstrate%20superior%20numerical%20performance%20of%20SISA%20and%20NSISA%20compared%20to%20various%20state-of-the-art%20optimizers.&entry.1838667208=http%3A//arxiv.org/abs/2502.10784v5&entry.124074799=Read"},
{"title": "Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life", "author": "Corey M. Abramson", "abstract": "This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.", "link": "http://arxiv.org/abs/2512.17850v1", "date": "2025-12-19", "relevancy": 1.5685, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4006}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3863}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Computational%20Methods%20and%20AI%20into%20Qualitative%20Studies%20of%20Aging%20and%20Later%20Life&body=Title%3A%20Integrating%20Computational%20Methods%20and%20AI%20into%20Qualitative%20Studies%20of%20Aging%20and%20Later%20Life%0AAuthor%3A%20Corey%20M.%20Abramson%0AAbstract%3A%20This%20chapter%20demonstrates%20how%20computational%20social%20science%20%28CSS%29%20tools%20are%20extending%20and%20expanding%20research%20on%20aging.%20The%20depth%20and%20context%20from%20traditionally%20qualitative%20methods%20such%20as%20participant%20observation%2C%20in-depth%20interviews%2C%20and%20historical%20documents%20are%20increasingly%20employed%20alongside%20scalable%20data%20management%2C%20computational%20text%20analysis%2C%20and%20open-science%20practices.%20Machine%20learning%20%28ML%29%20and%20natural%20language%20processing%20%28NLP%29%2C%20provide%20resources%20to%20aggregate%20and%20systematically%20index%20large%20volumes%20of%20qualitative%20data%2C%20identify%20patterns%2C%20and%20maintain%20clear%20links%20to%20in-depth%20accounts.%20Drawing%20on%20case%20studies%20of%20projects%20that%20examine%20later%20life--including%20examples%20with%20original%20data%20from%20the%20DISCERN%20study%20%28a%20team-based%20ethnography%20of%20life%20with%20dementia%29%20and%20secondary%20analyses%20of%20the%20American%20Voices%20Project%20%28nationally%20representative%20interview%29--the%20chapter%20highlights%20both%20uses%20and%20challenges%20of%20bringing%20CSS%20tools%20into%20more%20meaningful%20dialogue%20with%20qualitative%20aging%20research.%20The%20chapter%20argues%20such%20work%20has%20potential%20for%20%281%29%20streamlining%20and%20augmenting%20existing%20workflows%2C%20%282%29%20scaling%20up%20samples%20and%20projects%2C%20and%20%283%29%20generating%20multi-method%20approaches%20to%20address%20important%20questions%20in%20new%20ways%2C%20before%20turning%20to%20practices%20useful%20for%20individuals%20and%20teams%20seeking%20to%20understand%20current%20possibilities%20or%20refine%20their%20workflow%20processes.%20The%20chapter%20concludes%20that%20current%20developments%20are%20not%20without%20peril%2C%20but%20offer%20potential%20for%20new%20insights%20into%20aging%20and%20the%20life%20course%20by%20broadening--rather%20than%20replacing--the%20methodological%20foundations%20of%20qualitative%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Computational%2520Methods%2520and%2520AI%2520into%2520Qualitative%2520Studies%2520of%2520Aging%2520and%2520Later%2520Life%26entry.906535625%3DCorey%2520M.%2520Abramson%26entry.1292438233%3DThis%2520chapter%2520demonstrates%2520how%2520computational%2520social%2520science%2520%2528CSS%2529%2520tools%2520are%2520extending%2520and%2520expanding%2520research%2520on%2520aging.%2520The%2520depth%2520and%2520context%2520from%2520traditionally%2520qualitative%2520methods%2520such%2520as%2520participant%2520observation%252C%2520in-depth%2520interviews%252C%2520and%2520historical%2520documents%2520are%2520increasingly%2520employed%2520alongside%2520scalable%2520data%2520management%252C%2520computational%2520text%2520analysis%252C%2520and%2520open-science%2520practices.%2520Machine%2520learning%2520%2528ML%2529%2520and%2520natural%2520language%2520processing%2520%2528NLP%2529%252C%2520provide%2520resources%2520to%2520aggregate%2520and%2520systematically%2520index%2520large%2520volumes%2520of%2520qualitative%2520data%252C%2520identify%2520patterns%252C%2520and%2520maintain%2520clear%2520links%2520to%2520in-depth%2520accounts.%2520Drawing%2520on%2520case%2520studies%2520of%2520projects%2520that%2520examine%2520later%2520life--including%2520examples%2520with%2520original%2520data%2520from%2520the%2520DISCERN%2520study%2520%2528a%2520team-based%2520ethnography%2520of%2520life%2520with%2520dementia%2529%2520and%2520secondary%2520analyses%2520of%2520the%2520American%2520Voices%2520Project%2520%2528nationally%2520representative%2520interview%2529--the%2520chapter%2520highlights%2520both%2520uses%2520and%2520challenges%2520of%2520bringing%2520CSS%2520tools%2520into%2520more%2520meaningful%2520dialogue%2520with%2520qualitative%2520aging%2520research.%2520The%2520chapter%2520argues%2520such%2520work%2520has%2520potential%2520for%2520%25281%2529%2520streamlining%2520and%2520augmenting%2520existing%2520workflows%252C%2520%25282%2529%2520scaling%2520up%2520samples%2520and%2520projects%252C%2520and%2520%25283%2529%2520generating%2520multi-method%2520approaches%2520to%2520address%2520important%2520questions%2520in%2520new%2520ways%252C%2520before%2520turning%2520to%2520practices%2520useful%2520for%2520individuals%2520and%2520teams%2520seeking%2520to%2520understand%2520current%2520possibilities%2520or%2520refine%2520their%2520workflow%2520processes.%2520The%2520chapter%2520concludes%2520that%2520current%2520developments%2520are%2520not%2520without%2520peril%252C%2520but%2520offer%2520potential%2520for%2520new%2520insights%2520into%2520aging%2520and%2520the%2520life%2520course%2520by%2520broadening--rather%2520than%2520replacing--the%2520methodological%2520foundations%2520of%2520qualitative%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Computational%20Methods%20and%20AI%20into%20Qualitative%20Studies%20of%20Aging%20and%20Later%20Life&entry.906535625=Corey%20M.%20Abramson&entry.1292438233=This%20chapter%20demonstrates%20how%20computational%20social%20science%20%28CSS%29%20tools%20are%20extending%20and%20expanding%20research%20on%20aging.%20The%20depth%20and%20context%20from%20traditionally%20qualitative%20methods%20such%20as%20participant%20observation%2C%20in-depth%20interviews%2C%20and%20historical%20documents%20are%20increasingly%20employed%20alongside%20scalable%20data%20management%2C%20computational%20text%20analysis%2C%20and%20open-science%20practices.%20Machine%20learning%20%28ML%29%20and%20natural%20language%20processing%20%28NLP%29%2C%20provide%20resources%20to%20aggregate%20and%20systematically%20index%20large%20volumes%20of%20qualitative%20data%2C%20identify%20patterns%2C%20and%20maintain%20clear%20links%20to%20in-depth%20accounts.%20Drawing%20on%20case%20studies%20of%20projects%20that%20examine%20later%20life--including%20examples%20with%20original%20data%20from%20the%20DISCERN%20study%20%28a%20team-based%20ethnography%20of%20life%20with%20dementia%29%20and%20secondary%20analyses%20of%20the%20American%20Voices%20Project%20%28nationally%20representative%20interview%29--the%20chapter%20highlights%20both%20uses%20and%20challenges%20of%20bringing%20CSS%20tools%20into%20more%20meaningful%20dialogue%20with%20qualitative%20aging%20research.%20The%20chapter%20argues%20such%20work%20has%20potential%20for%20%281%29%20streamlining%20and%20augmenting%20existing%20workflows%2C%20%282%29%20scaling%20up%20samples%20and%20projects%2C%20and%20%283%29%20generating%20multi-method%20approaches%20to%20address%20important%20questions%20in%20new%20ways%2C%20before%20turning%20to%20practices%20useful%20for%20individuals%20and%20teams%20seeking%20to%20understand%20current%20possibilities%20or%20refine%20their%20workflow%20processes.%20The%20chapter%20concludes%20that%20current%20developments%20are%20not%20without%20peril%2C%20but%20offer%20potential%20for%20new%20insights%20into%20aging%20and%20the%20life%20course%20by%20broadening--rather%20than%20replacing--the%20methodological%20foundations%20of%20qualitative%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.17850v1&entry.124074799=Read"},
{"title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features", "author": "Linghui Zhu and Yiming Li and Haiqin Weng and Yan Liu and Tianwei Zhang and Shu-Tao Xia and Zhi Wang", "abstract": "Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.", "link": "http://arxiv.org/abs/2507.00724v2", "date": "2025-12-19", "relevancy": 1.9889, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5088}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holmes%3A%20Towards%20Effective%20and%20Harmless%20Model%20Ownership%20Verification%20to%20Personalized%20Large%20Vision%20Models%20via%20Decoupling%20Common%20Features&body=Title%3A%20Holmes%3A%20Towards%20Effective%20and%20Harmless%20Model%20Ownership%20Verification%20to%20Personalized%20Large%20Vision%20Models%20via%20Decoupling%20Common%20Features%0AAuthor%3A%20Linghui%20Zhu%20and%20Yiming%20Li%20and%20Haiqin%20Weng%20and%20Yan%20Liu%20and%20Tianwei%20Zhang%20and%20Shu-Tao%20Xia%20and%20Zhi%20Wang%0AAbstract%3A%20Large%20vision%20models%20%28LVMs%29%20achieve%20remarkable%20performance%20in%20various%20downstream%20tasks%2C%20primarily%20by%20personalizing%20pre-trained%20models%20through%20fine-tuning%20with%20private%20and%20valuable%20local%20data%2C%20which%20makes%20the%20personalized%20model%20a%20valuable%20intellectual%20property.%20Similar%20to%20the%20era%20of%20traditional%20DNNs%2C%20model%20stealing%20attacks%20also%20pose%20significant%20risks%20to%20LVMs.%20However%2C%20this%20paper%20reveals%20that%20most%20existing%20defense%20methods%20%28developed%20for%20traditional%20DNNs%29%2C%20typically%20designed%20for%20models%20trained%20from%20scratch%2C%20either%20introduce%20additional%20security%20risks%2C%20are%20prone%20to%20misjudgment%2C%20or%20are%20even%20ineffective%20for%20fine-tuned%20models.%20To%20alleviate%20these%20problems%2C%20this%20paper%20proposes%20a%20harmless%20model%20ownership%20verification%20method%20for%20personalized%20LVMs%20by%20decoupling%20similar%20common%20features.%20In%20general%2C%20our%20method%20consists%20of%20three%20main%20stages.%20In%20the%20first%20stage%2C%20we%20create%20shadow%20models%20that%20retain%20common%20features%20of%20the%20victim%20model%20while%20disrupting%20dataset-specific%20features.%20We%20represent%20the%20dataset-specific%20features%20of%20the%20victim%20model%20by%20computing%20the%20output%20differences%20between%20the%20shadow%20and%20victim%20models%2C%20without%20altering%20the%20victim%20model%20or%20its%20training%20process.%20After%20that%2C%20a%20meta-classifier%20is%20trained%20to%20identify%20stolen%20models%20by%20determining%20whether%20suspicious%20models%20contain%20the%20dataset-specific%20features%20of%20the%20victim.%20In%20the%20third%20stage%2C%20we%20conduct%20model%20ownership%20verification%20by%20hypothesis%20test%20to%20mitigate%20randomness%20and%20enhance%20robustness.%20Extensive%20experiments%20on%20benchmark%20datasets%20verify%20the%20effectiveness%20of%20the%20proposed%20method%20in%20detecting%20different%20types%20of%20model%20stealing%20simultaneously.%20Our%20codes%20are%20available%20at%20https%3A//github.com/zlh-thu/Holmes.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolmes%253A%2520Towards%2520Effective%2520and%2520Harmless%2520Model%2520Ownership%2520Verification%2520to%2520Personalized%2520Large%2520Vision%2520Models%2520via%2520Decoupling%2520Common%2520Features%26entry.906535625%3DLinghui%2520Zhu%2520and%2520Yiming%2520Li%2520and%2520Haiqin%2520Weng%2520and%2520Yan%2520Liu%2520and%2520Tianwei%2520Zhang%2520and%2520Shu-Tao%2520Xia%2520and%2520Zhi%2520Wang%26entry.1292438233%3DLarge%2520vision%2520models%2520%2528LVMs%2529%2520achieve%2520remarkable%2520performance%2520in%2520various%2520downstream%2520tasks%252C%2520primarily%2520by%2520personalizing%2520pre-trained%2520models%2520through%2520fine-tuning%2520with%2520private%2520and%2520valuable%2520local%2520data%252C%2520which%2520makes%2520the%2520personalized%2520model%2520a%2520valuable%2520intellectual%2520property.%2520Similar%2520to%2520the%2520era%2520of%2520traditional%2520DNNs%252C%2520model%2520stealing%2520attacks%2520also%2520pose%2520significant%2520risks%2520to%2520LVMs.%2520However%252C%2520this%2520paper%2520reveals%2520that%2520most%2520existing%2520defense%2520methods%2520%2528developed%2520for%2520traditional%2520DNNs%2529%252C%2520typically%2520designed%2520for%2520models%2520trained%2520from%2520scratch%252C%2520either%2520introduce%2520additional%2520security%2520risks%252C%2520are%2520prone%2520to%2520misjudgment%252C%2520or%2520are%2520even%2520ineffective%2520for%2520fine-tuned%2520models.%2520To%2520alleviate%2520these%2520problems%252C%2520this%2520paper%2520proposes%2520a%2520harmless%2520model%2520ownership%2520verification%2520method%2520for%2520personalized%2520LVMs%2520by%2520decoupling%2520similar%2520common%2520features.%2520In%2520general%252C%2520our%2520method%2520consists%2520of%2520three%2520main%2520stages.%2520In%2520the%2520first%2520stage%252C%2520we%2520create%2520shadow%2520models%2520that%2520retain%2520common%2520features%2520of%2520the%2520victim%2520model%2520while%2520disrupting%2520dataset-specific%2520features.%2520We%2520represent%2520the%2520dataset-specific%2520features%2520of%2520the%2520victim%2520model%2520by%2520computing%2520the%2520output%2520differences%2520between%2520the%2520shadow%2520and%2520victim%2520models%252C%2520without%2520altering%2520the%2520victim%2520model%2520or%2520its%2520training%2520process.%2520After%2520that%252C%2520a%2520meta-classifier%2520is%2520trained%2520to%2520identify%2520stolen%2520models%2520by%2520determining%2520whether%2520suspicious%2520models%2520contain%2520the%2520dataset-specific%2520features%2520of%2520the%2520victim.%2520In%2520the%2520third%2520stage%252C%2520we%2520conduct%2520model%2520ownership%2520verification%2520by%2520hypothesis%2520test%2520to%2520mitigate%2520randomness%2520and%2520enhance%2520robustness.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520in%2520detecting%2520different%2520types%2520of%2520model%2520stealing%2520simultaneously.%2520Our%2520codes%2520are%2520available%2520at%2520https%253A//github.com/zlh-thu/Holmes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holmes%3A%20Towards%20Effective%20and%20Harmless%20Model%20Ownership%20Verification%20to%20Personalized%20Large%20Vision%20Models%20via%20Decoupling%20Common%20Features&entry.906535625=Linghui%20Zhu%20and%20Yiming%20Li%20and%20Haiqin%20Weng%20and%20Yan%20Liu%20and%20Tianwei%20Zhang%20and%20Shu-Tao%20Xia%20and%20Zhi%20Wang&entry.1292438233=Large%20vision%20models%20%28LVMs%29%20achieve%20remarkable%20performance%20in%20various%20downstream%20tasks%2C%20primarily%20by%20personalizing%20pre-trained%20models%20through%20fine-tuning%20with%20private%20and%20valuable%20local%20data%2C%20which%20makes%20the%20personalized%20model%20a%20valuable%20intellectual%20property.%20Similar%20to%20the%20era%20of%20traditional%20DNNs%2C%20model%20stealing%20attacks%20also%20pose%20significant%20risks%20to%20LVMs.%20However%2C%20this%20paper%20reveals%20that%20most%20existing%20defense%20methods%20%28developed%20for%20traditional%20DNNs%29%2C%20typically%20designed%20for%20models%20trained%20from%20scratch%2C%20either%20introduce%20additional%20security%20risks%2C%20are%20prone%20to%20misjudgment%2C%20or%20are%20even%20ineffective%20for%20fine-tuned%20models.%20To%20alleviate%20these%20problems%2C%20this%20paper%20proposes%20a%20harmless%20model%20ownership%20verification%20method%20for%20personalized%20LVMs%20by%20decoupling%20similar%20common%20features.%20In%20general%2C%20our%20method%20consists%20of%20three%20main%20stages.%20In%20the%20first%20stage%2C%20we%20create%20shadow%20models%20that%20retain%20common%20features%20of%20the%20victim%20model%20while%20disrupting%20dataset-specific%20features.%20We%20represent%20the%20dataset-specific%20features%20of%20the%20victim%20model%20by%20computing%20the%20output%20differences%20between%20the%20shadow%20and%20victim%20models%2C%20without%20altering%20the%20victim%20model%20or%20its%20training%20process.%20After%20that%2C%20a%20meta-classifier%20is%20trained%20to%20identify%20stolen%20models%20by%20determining%20whether%20suspicious%20models%20contain%20the%20dataset-specific%20features%20of%20the%20victim.%20In%20the%20third%20stage%2C%20we%20conduct%20model%20ownership%20verification%20by%20hypothesis%20test%20to%20mitigate%20randomness%20and%20enhance%20robustness.%20Extensive%20experiments%20on%20benchmark%20datasets%20verify%20the%20effectiveness%20of%20the%20proposed%20method%20in%20detecting%20different%20types%20of%20model%20stealing%20simultaneously.%20Our%20codes%20are%20available%20at%20https%3A//github.com/zlh-thu/Holmes.&entry.1838667208=http%3A//arxiv.org/abs/2507.00724v2&entry.124074799=Read"},
{"title": "Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation", "author": "Binh Vu", "abstract": "The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.", "link": "http://arxiv.org/abs/2512.17795v1", "date": "2025-12-19", "relevancy": 1.7885, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Knowledge%20Mining%20Framework%3A%20Bridging%20AI%20Analysis%20and%20Trustworthy%20Preservation&body=Title%3A%20Intelligent%20Knowledge%20Mining%20Framework%3A%20Bridging%20AI%20Analysis%20and%20Trustworthy%20Preservation%0AAuthor%3A%20Binh%20Vu%0AAbstract%3A%20The%20unprecedented%20proliferation%20of%20digital%20data%20presents%20significant%20challenges%20in%20access%2C%20integration%2C%20and%20value%20creation%20across%20all%20data-intensive%20sectors.%20Valuable%20information%20is%20frequently%20encapsulated%20within%20disparate%20systems%2C%20unstructured%20documents%2C%20and%20heterogeneous%20formats%2C%20creating%20silos%20that%20impede%20efficient%20utilization%20and%20collaborative%20decision-making.%20This%20paper%20introduces%20the%20Intelligent%20Knowledge%20Mining%20Framework%20%28IKMF%29%2C%20a%20comprehensive%20conceptual%20model%20designed%20to%20bridge%20the%20critical%20gap%20between%20dynamic%20AI-driven%20analysis%20and%20trustworthy%20long-term%20preservation.%20The%20framework%20proposes%20a%20dual-stream%20architecture%3A%20a%20horizontal%20Mining%20Process%20that%20systematically%20transforms%20raw%20data%20into%20semantically%20rich%2C%20machine-actionable%20knowledge%2C%20and%20a%20parallel%20Trustworthy%20Archiving%20Stream%20that%20ensures%20the%20integrity%2C%20provenance%2C%20and%20computational%20reproducibility%20of%20these%20assets.%20By%20defining%20a%20blueprint%20for%20this%20symbiotic%20relationship%2C%20the%20paper%20provides%20a%20foundational%20model%20for%20transforming%20static%20repositories%20into%20living%20ecosystems%20that%20facilitate%20the%20flow%20of%20actionable%20intelligence%20from%20producers%20to%20consumers.%20This%20paper%20outlines%20the%20motivation%2C%20problem%20statement%2C%20and%20key%20research%20questions%20guiding%20the%20research%20and%20development%20of%20the%20framework%2C%20presents%20the%20underlying%20scientific%20methodology%2C%20and%20details%20its%20conceptual%20design%20and%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Knowledge%2520Mining%2520Framework%253A%2520Bridging%2520AI%2520Analysis%2520and%2520Trustworthy%2520Preservation%26entry.906535625%3DBinh%2520Vu%26entry.1292438233%3DThe%2520unprecedented%2520proliferation%2520of%2520digital%2520data%2520presents%2520significant%2520challenges%2520in%2520access%252C%2520integration%252C%2520and%2520value%2520creation%2520across%2520all%2520data-intensive%2520sectors.%2520Valuable%2520information%2520is%2520frequently%2520encapsulated%2520within%2520disparate%2520systems%252C%2520unstructured%2520documents%252C%2520and%2520heterogeneous%2520formats%252C%2520creating%2520silos%2520that%2520impede%2520efficient%2520utilization%2520and%2520collaborative%2520decision-making.%2520This%2520paper%2520introduces%2520the%2520Intelligent%2520Knowledge%2520Mining%2520Framework%2520%2528IKMF%2529%252C%2520a%2520comprehensive%2520conceptual%2520model%2520designed%2520to%2520bridge%2520the%2520critical%2520gap%2520between%2520dynamic%2520AI-driven%2520analysis%2520and%2520trustworthy%2520long-term%2520preservation.%2520The%2520framework%2520proposes%2520a%2520dual-stream%2520architecture%253A%2520a%2520horizontal%2520Mining%2520Process%2520that%2520systematically%2520transforms%2520raw%2520data%2520into%2520semantically%2520rich%252C%2520machine-actionable%2520knowledge%252C%2520and%2520a%2520parallel%2520Trustworthy%2520Archiving%2520Stream%2520that%2520ensures%2520the%2520integrity%252C%2520provenance%252C%2520and%2520computational%2520reproducibility%2520of%2520these%2520assets.%2520By%2520defining%2520a%2520blueprint%2520for%2520this%2520symbiotic%2520relationship%252C%2520the%2520paper%2520provides%2520a%2520foundational%2520model%2520for%2520transforming%2520static%2520repositories%2520into%2520living%2520ecosystems%2520that%2520facilitate%2520the%2520flow%2520of%2520actionable%2520intelligence%2520from%2520producers%2520to%2520consumers.%2520This%2520paper%2520outlines%2520the%2520motivation%252C%2520problem%2520statement%252C%2520and%2520key%2520research%2520questions%2520guiding%2520the%2520research%2520and%2520development%2520of%2520the%2520framework%252C%2520presents%2520the%2520underlying%2520scientific%2520methodology%252C%2520and%2520details%2520its%2520conceptual%2520design%2520and%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Knowledge%20Mining%20Framework%3A%20Bridging%20AI%20Analysis%20and%20Trustworthy%20Preservation&entry.906535625=Binh%20Vu&entry.1292438233=The%20unprecedented%20proliferation%20of%20digital%20data%20presents%20significant%20challenges%20in%20access%2C%20integration%2C%20and%20value%20creation%20across%20all%20data-intensive%20sectors.%20Valuable%20information%20is%20frequently%20encapsulated%20within%20disparate%20systems%2C%20unstructured%20documents%2C%20and%20heterogeneous%20formats%2C%20creating%20silos%20that%20impede%20efficient%20utilization%20and%20collaborative%20decision-making.%20This%20paper%20introduces%20the%20Intelligent%20Knowledge%20Mining%20Framework%20%28IKMF%29%2C%20a%20comprehensive%20conceptual%20model%20designed%20to%20bridge%20the%20critical%20gap%20between%20dynamic%20AI-driven%20analysis%20and%20trustworthy%20long-term%20preservation.%20The%20framework%20proposes%20a%20dual-stream%20architecture%3A%20a%20horizontal%20Mining%20Process%20that%20systematically%20transforms%20raw%20data%20into%20semantically%20rich%2C%20machine-actionable%20knowledge%2C%20and%20a%20parallel%20Trustworthy%20Archiving%20Stream%20that%20ensures%20the%20integrity%2C%20provenance%2C%20and%20computational%20reproducibility%20of%20these%20assets.%20By%20defining%20a%20blueprint%20for%20this%20symbiotic%20relationship%2C%20the%20paper%20provides%20a%20foundational%20model%20for%20transforming%20static%20repositories%20into%20living%20ecosystems%20that%20facilitate%20the%20flow%20of%20actionable%20intelligence%20from%20producers%20to%20consumers.%20This%20paper%20outlines%20the%20motivation%2C%20problem%20statement%2C%20and%20key%20research%20questions%20guiding%20the%20research%20and%20development%20of%20the%20framework%2C%20presents%20the%20underlying%20scientific%20methodology%2C%20and%20details%20its%20conceptual%20design%20and%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.17795v1&entry.124074799=Read"},
{"title": "InSPECT: Invariant Spectral Features Preservation of Diffusion Models", "author": "Baohua Yan and Qingyuan Liu and Jennifer Kava and Xuan Di", "abstract": "Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.", "link": "http://arxiv.org/abs/2512.17873v1", "date": "2025-12-19", "relevancy": 1.8178, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6761}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5865}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InSPECT%3A%20Invariant%20Spectral%20Features%20Preservation%20of%20Diffusion%20Models&body=Title%3A%20InSPECT%3A%20Invariant%20Spectral%20Features%20Preservation%20of%20Diffusion%20Models%0AAuthor%3A%20Baohua%20Yan%20and%20Qingyuan%20Liu%20and%20Jennifer%20Kava%20and%20Xuan%20Di%0AAbstract%3A%20Modern%20diffusion%20models%20%28DMs%29%20have%20achieved%20state-of-the-art%20image%20generation.%20However%2C%20the%20fundamental%20design%20choice%20of%20diffusing%20data%20all%20the%20way%20to%20white%20noise%20and%20then%20reconstructing%20it%20leads%20to%20an%20extremely%20difficult%20and%20computationally%20intractable%20prediction%20task.%20To%20overcome%20this%20limitation%2C%20we%20propose%20InSPECT%20%28Invariant%20Spectral%20Feature-Preserving%20Diffusion%20Model%29%2C%20a%20novel%20diffusion%20model%20that%20keeps%20invariant%20spectral%20features%20during%20both%20the%20forward%20and%20backward%20processes.%20At%20the%20end%20of%20the%20forward%20process%2C%20the%20Fourier%20coefficients%20smoothly%20converge%20to%20a%20specified%20random%20noise%2C%20enabling%20features%20preservation%20while%20maintaining%20diversity%20and%20randomness.%20By%20preserving%20invariant%20features%2C%20InSPECT%20demonstrates%20enhanced%20visual%20diversity%2C%20faster%20convergence%20rate%2C%20and%20a%20smoother%20diffusion%20process.%20Experiments%20on%20CIFAR-10%2C%20Celeb-A%2C%20and%20LSUN%20demonstrate%20that%20InSPECT%20achieves%20on%20average%20a%2039.23%25%20reduction%20in%20FID%20and%2045.80%25%20improvement%20in%20IS%20against%20DDPM%20for%2010K%20iterations%20under%20specified%20parameter%20settings%2C%20which%20demonstrates%20the%20significant%20advantages%20of%20preserving%20invariant%20features%3A%20achieving%20superior%20generation%20quality%20and%20diversity%2C%20while%20enhancing%20computational%20efficiency%20and%20enabling%20faster%20convergence%20rate.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20attempt%20to%20analyze%20and%20preserve%20invariant%20spectral%20features%20in%20diffusion%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInSPECT%253A%2520Invariant%2520Spectral%2520Features%2520Preservation%2520of%2520Diffusion%2520Models%26entry.906535625%3DBaohua%2520Yan%2520and%2520Qingyuan%2520Liu%2520and%2520Jennifer%2520Kava%2520and%2520Xuan%2520Di%26entry.1292438233%3DModern%2520diffusion%2520models%2520%2528DMs%2529%2520have%2520achieved%2520state-of-the-art%2520image%2520generation.%2520However%252C%2520the%2520fundamental%2520design%2520choice%2520of%2520diffusing%2520data%2520all%2520the%2520way%2520to%2520white%2520noise%2520and%2520then%2520reconstructing%2520it%2520leads%2520to%2520an%2520extremely%2520difficult%2520and%2520computationally%2520intractable%2520prediction%2520task.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520InSPECT%2520%2528Invariant%2520Spectral%2520Feature-Preserving%2520Diffusion%2520Model%2529%252C%2520a%2520novel%2520diffusion%2520model%2520that%2520keeps%2520invariant%2520spectral%2520features%2520during%2520both%2520the%2520forward%2520and%2520backward%2520processes.%2520At%2520the%2520end%2520of%2520the%2520forward%2520process%252C%2520the%2520Fourier%2520coefficients%2520smoothly%2520converge%2520to%2520a%2520specified%2520random%2520noise%252C%2520enabling%2520features%2520preservation%2520while%2520maintaining%2520diversity%2520and%2520randomness.%2520By%2520preserving%2520invariant%2520features%252C%2520InSPECT%2520demonstrates%2520enhanced%2520visual%2520diversity%252C%2520faster%2520convergence%2520rate%252C%2520and%2520a%2520smoother%2520diffusion%2520process.%2520Experiments%2520on%2520CIFAR-10%252C%2520Celeb-A%252C%2520and%2520LSUN%2520demonstrate%2520that%2520InSPECT%2520achieves%2520on%2520average%2520a%252039.23%2525%2520reduction%2520in%2520FID%2520and%252045.80%2525%2520improvement%2520in%2520IS%2520against%2520DDPM%2520for%252010K%2520iterations%2520under%2520specified%2520parameter%2520settings%252C%2520which%2520demonstrates%2520the%2520significant%2520advantages%2520of%2520preserving%2520invariant%2520features%253A%2520achieving%2520superior%2520generation%2520quality%2520and%2520diversity%252C%2520while%2520enhancing%2520computational%2520efficiency%2520and%2520enabling%2520faster%2520convergence%2520rate.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520attempt%2520to%2520analyze%2520and%2520preserve%2520invariant%2520spectral%2520features%2520in%2520diffusion%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InSPECT%3A%20Invariant%20Spectral%20Features%20Preservation%20of%20Diffusion%20Models&entry.906535625=Baohua%20Yan%20and%20Qingyuan%20Liu%20and%20Jennifer%20Kava%20and%20Xuan%20Di&entry.1292438233=Modern%20diffusion%20models%20%28DMs%29%20have%20achieved%20state-of-the-art%20image%20generation.%20However%2C%20the%20fundamental%20design%20choice%20of%20diffusing%20data%20all%20the%20way%20to%20white%20noise%20and%20then%20reconstructing%20it%20leads%20to%20an%20extremely%20difficult%20and%20computationally%20intractable%20prediction%20task.%20To%20overcome%20this%20limitation%2C%20we%20propose%20InSPECT%20%28Invariant%20Spectral%20Feature-Preserving%20Diffusion%20Model%29%2C%20a%20novel%20diffusion%20model%20that%20keeps%20invariant%20spectral%20features%20during%20both%20the%20forward%20and%20backward%20processes.%20At%20the%20end%20of%20the%20forward%20process%2C%20the%20Fourier%20coefficients%20smoothly%20converge%20to%20a%20specified%20random%20noise%2C%20enabling%20features%20preservation%20while%20maintaining%20diversity%20and%20randomness.%20By%20preserving%20invariant%20features%2C%20InSPECT%20demonstrates%20enhanced%20visual%20diversity%2C%20faster%20convergence%20rate%2C%20and%20a%20smoother%20diffusion%20process.%20Experiments%20on%20CIFAR-10%2C%20Celeb-A%2C%20and%20LSUN%20demonstrate%20that%20InSPECT%20achieves%20on%20average%20a%2039.23%25%20reduction%20in%20FID%20and%2045.80%25%20improvement%20in%20IS%20against%20DDPM%20for%2010K%20iterations%20under%20specified%20parameter%20settings%2C%20which%20demonstrates%20the%20significant%20advantages%20of%20preserving%20invariant%20features%3A%20achieving%20superior%20generation%20quality%20and%20diversity%2C%20while%20enhancing%20computational%20efficiency%20and%20enabling%20faster%20convergence%20rate.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20attempt%20to%20analyze%20and%20preserve%20invariant%20spectral%20features%20in%20diffusion%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.17873v1&entry.124074799=Read"},
{"title": "Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow", "author": "Herlock Rahimi", "abstract": "Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.\n  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.", "link": "http://arxiv.org/abs/2512.17878v1", "date": "2025-12-19", "relevancy": 1.5503, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.593}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5074}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weighted%20Stochastic%20Differential%20Equation%20to%20Implement%20Wasserstein-Fisher-Rao%20Gradient%20Flow&body=Title%3A%20Weighted%20Stochastic%20Differential%20Equation%20to%20Implement%20Wasserstein-Fisher-Rao%20Gradient%20Flow%0AAuthor%3A%20Herlock%20Rahimi%0AAbstract%3A%20Score-based%20diffusion%20models%20currently%20constitute%20the%20state%20of%20the%20art%20in%20continuous%20generative%20modeling.%20These%20methods%20are%20typically%20formulated%20via%20overdamped%20or%20underdamped%20Ornstein--Uhlenbeck-type%20stochastic%20differential%20equations%2C%20in%20which%20sampling%20is%20driven%20by%20a%20combination%20of%20deterministic%20drift%20and%20Brownian%20diffusion%2C%20resulting%20in%20continuous%20particle%20trajectories%20in%20the%20ambient%20space.%20While%20such%20dynamics%20enjoy%20exponential%20convergence%20guarantees%20for%20strongly%20log-concave%20target%20distributions%2C%20it%20is%20well%20known%20that%20their%20mixing%20rates%20deteriorate%20exponentially%20in%20the%20presence%20of%20nonconvex%20or%20multimodal%20landscapes%2C%20such%20as%20double-well%20potentials.%20Since%20many%20practical%20generative%20modeling%20tasks%20involve%20highly%20non-log-concave%20target%20distributions%2C%20considerable%20recent%20effort%20has%20been%20devoted%20to%20developing%20sampling%20schemes%20that%20improve%20exploration%20beyond%20classical%20diffusion%20dynamics.%0A%20%20A%20promising%20line%20of%20work%20leverages%20tools%20from%20information%20geometry%20to%20augment%20diffusion-based%20samplers%20with%20controlled%20mass%20reweighting%20mechanisms.%20This%20perspective%20leads%20naturally%20to%20Wasserstein--Fisher--Rao%20%28WFR%29%20geometries%2C%20which%20couple%20transport%20in%20the%20sample%20space%20with%20vertical%20%28reaction%29%20dynamics%20on%20the%20space%20of%20probability%20measures.%20In%20this%20work%2C%20we%20formulate%20such%20reweighting%20mechanisms%20through%20the%20introduction%20of%20explicit%20correction%20terms%20and%20show%20how%20they%20can%20be%20implemented%20via%20weighted%20stochastic%20differential%20equations%20using%20the%20Feynman--Kac%20representation.%20Our%20study%20provides%20a%20preliminary%20but%20rigorous%20investigation%20of%20WFR-based%20sampling%20dynamics%2C%20and%20aims%20to%20clarify%20their%20geometric%20and%20operator-theoretic%20structure%20as%20a%20foundation%20for%20future%20theoretical%20and%20algorithmic%20developments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeighted%2520Stochastic%2520Differential%2520Equation%2520to%2520Implement%2520Wasserstein-Fisher-Rao%2520Gradient%2520Flow%26entry.906535625%3DHerlock%2520Rahimi%26entry.1292438233%3DScore-based%2520diffusion%2520models%2520currently%2520constitute%2520the%2520state%2520of%2520the%2520art%2520in%2520continuous%2520generative%2520modeling.%2520These%2520methods%2520are%2520typically%2520formulated%2520via%2520overdamped%2520or%2520underdamped%2520Ornstein--Uhlenbeck-type%2520stochastic%2520differential%2520equations%252C%2520in%2520which%2520sampling%2520is%2520driven%2520by%2520a%2520combination%2520of%2520deterministic%2520drift%2520and%2520Brownian%2520diffusion%252C%2520resulting%2520in%2520continuous%2520particle%2520trajectories%2520in%2520the%2520ambient%2520space.%2520While%2520such%2520dynamics%2520enjoy%2520exponential%2520convergence%2520guarantees%2520for%2520strongly%2520log-concave%2520target%2520distributions%252C%2520it%2520is%2520well%2520known%2520that%2520their%2520mixing%2520rates%2520deteriorate%2520exponentially%2520in%2520the%2520presence%2520of%2520nonconvex%2520or%2520multimodal%2520landscapes%252C%2520such%2520as%2520double-well%2520potentials.%2520Since%2520many%2520practical%2520generative%2520modeling%2520tasks%2520involve%2520highly%2520non-log-concave%2520target%2520distributions%252C%2520considerable%2520recent%2520effort%2520has%2520been%2520devoted%2520to%2520developing%2520sampling%2520schemes%2520that%2520improve%2520exploration%2520beyond%2520classical%2520diffusion%2520dynamics.%250A%2520%2520A%2520promising%2520line%2520of%2520work%2520leverages%2520tools%2520from%2520information%2520geometry%2520to%2520augment%2520diffusion-based%2520samplers%2520with%2520controlled%2520mass%2520reweighting%2520mechanisms.%2520This%2520perspective%2520leads%2520naturally%2520to%2520Wasserstein--Fisher--Rao%2520%2528WFR%2529%2520geometries%252C%2520which%2520couple%2520transport%2520in%2520the%2520sample%2520space%2520with%2520vertical%2520%2528reaction%2529%2520dynamics%2520on%2520the%2520space%2520of%2520probability%2520measures.%2520In%2520this%2520work%252C%2520we%2520formulate%2520such%2520reweighting%2520mechanisms%2520through%2520the%2520introduction%2520of%2520explicit%2520correction%2520terms%2520and%2520show%2520how%2520they%2520can%2520be%2520implemented%2520via%2520weighted%2520stochastic%2520differential%2520equations%2520using%2520the%2520Feynman--Kac%2520representation.%2520Our%2520study%2520provides%2520a%2520preliminary%2520but%2520rigorous%2520investigation%2520of%2520WFR-based%2520sampling%2520dynamics%252C%2520and%2520aims%2520to%2520clarify%2520their%2520geometric%2520and%2520operator-theoretic%2520structure%2520as%2520a%2520foundation%2520for%2520future%2520theoretical%2520and%2520algorithmic%2520developments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20Stochastic%20Differential%20Equation%20to%20Implement%20Wasserstein-Fisher-Rao%20Gradient%20Flow&entry.906535625=Herlock%20Rahimi&entry.1292438233=Score-based%20diffusion%20models%20currently%20constitute%20the%20state%20of%20the%20art%20in%20continuous%20generative%20modeling.%20These%20methods%20are%20typically%20formulated%20via%20overdamped%20or%20underdamped%20Ornstein--Uhlenbeck-type%20stochastic%20differential%20equations%2C%20in%20which%20sampling%20is%20driven%20by%20a%20combination%20of%20deterministic%20drift%20and%20Brownian%20diffusion%2C%20resulting%20in%20continuous%20particle%20trajectories%20in%20the%20ambient%20space.%20While%20such%20dynamics%20enjoy%20exponential%20convergence%20guarantees%20for%20strongly%20log-concave%20target%20distributions%2C%20it%20is%20well%20known%20that%20their%20mixing%20rates%20deteriorate%20exponentially%20in%20the%20presence%20of%20nonconvex%20or%20multimodal%20landscapes%2C%20such%20as%20double-well%20potentials.%20Since%20many%20practical%20generative%20modeling%20tasks%20involve%20highly%20non-log-concave%20target%20distributions%2C%20considerable%20recent%20effort%20has%20been%20devoted%20to%20developing%20sampling%20schemes%20that%20improve%20exploration%20beyond%20classical%20diffusion%20dynamics.%0A%20%20A%20promising%20line%20of%20work%20leverages%20tools%20from%20information%20geometry%20to%20augment%20diffusion-based%20samplers%20with%20controlled%20mass%20reweighting%20mechanisms.%20This%20perspective%20leads%20naturally%20to%20Wasserstein--Fisher--Rao%20%28WFR%29%20geometries%2C%20which%20couple%20transport%20in%20the%20sample%20space%20with%20vertical%20%28reaction%29%20dynamics%20on%20the%20space%20of%20probability%20measures.%20In%20this%20work%2C%20we%20formulate%20such%20reweighting%20mechanisms%20through%20the%20introduction%20of%20explicit%20correction%20terms%20and%20show%20how%20they%20can%20be%20implemented%20via%20weighted%20stochastic%20differential%20equations%20using%20the%20Feynman--Kac%20representation.%20Our%20study%20provides%20a%20preliminary%20but%20rigorous%20investigation%20of%20WFR-based%20sampling%20dynamics%2C%20and%20aims%20to%20clarify%20their%20geometric%20and%20operator-theoretic%20structure%20as%20a%20foundation%20for%20future%20theoretical%20and%20algorithmic%20developments.&entry.1838667208=http%3A//arxiv.org/abs/2512.17878v1&entry.124074799=Read"},
{"title": "SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses", "author": "Shaoyan Zhai and Mohamed Abdel-Aty and Chenzhu Wang and Rodrigo Vena Garcia", "abstract": "The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.", "link": "http://arxiv.org/abs/2512.17724v1", "date": "2025-12-19", "relevancy": 1.6269, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5534}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5316}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAVeD%3A%20A%20First-Person%20Social%20Media%20Video%20Dataset%20for%20ADAS-equipped%20vehicle%20Near-Miss%20and%20Crash%20Event%20Analyses&body=Title%3A%20SAVeD%3A%20A%20First-Person%20Social%20Media%20Video%20Dataset%20for%20ADAS-equipped%20vehicle%20Near-Miss%20and%20Crash%20Event%20Analyses%0AAuthor%3A%20Shaoyan%20Zhai%20and%20Mohamed%20Abdel-Aty%20and%20Chenzhu%20Wang%20and%20Rodrigo%20Vena%20Garcia%0AAbstract%3A%20The%20advancement%20of%20safety-critical%20research%20in%20driving%20behavior%20in%20ADAS-equipped%20vehicles%20require%20real-world%20datasets%20that%20not%20only%20include%20diverse%20traffic%20scenarios%20but%20also%20capture%20high-risk%20edge%20cases%20such%20as%20near-miss%20events%20and%20system%20failures.%20However%2C%20existing%20datasets%20are%20largely%20limited%20to%20either%20simulated%20environments%20or%20human-driven%20vehicle%20data%2C%20lacking%20authentic%20ADAS%20%28Advanced%20Driver%20Assistance%20System%29%20vehicle%20behavior%20under%20risk%20conditions.%20To%20address%20this%20gap%2C%20this%20paper%20introduces%20SAVeD%2C%20a%20large-scale%20video%20dataset%20curated%20from%20publicly%20available%20social%20media%20content%2C%20explicitly%20focused%20on%20ADAS%20vehicle-related%20crashes%2C%20near-miss%20incidents%2C%20and%20disengagements.%20SAVeD%20features%202%2C119%20first-person%20videos%2C%20capturing%20ADAS%20vehicle%20operations%20in%20diverse%20locations%2C%20lighting%20conditions%2C%20and%20weather%20scenarios.%20The%20dataset%20includes%20video%20frame-level%20annotations%20for%20collisions%2C%20evasive%20maneuvers%2C%20and%20disengagements%2C%20enabling%20analysis%20of%20both%20perception%20and%20decision-making%20failures.%20We%20demonstrate%20SAVeD%27s%20utility%20through%20multiple%20analyses%20and%20contributions%3A%20%281%29%20We%20propose%20a%20novel%20framework%20integrating%20semantic%20segmentation%20and%20monocular%20depth%20estimation%20to%20compute%20real-time%20Time-to-Collision%20%28TTC%29%20for%20dynamic%20objects.%20%282%29%20We%20utilize%20the%20Generalized%20Extreme%20Value%20%28GEV%29%20distribution%20to%20model%20and%20quantify%20the%20extreme%20risk%20in%20crash%20and%20near-miss%20events%20across%20different%20roadway%20types.%20%283%29%20We%20establish%20benchmarks%20for%20state-of-the-art%20VLLMs%20%28VideoLLaMA2%20and%20InternVL2.5%20HiCo%20R16%29%2C%20showing%20that%20SAVeD%27s%20detailed%20annotations%20significantly%20enhance%20model%20performance%20through%20domain%20adaptation%20in%20complex%20near-miss%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAVeD%253A%2520A%2520First-Person%2520Social%2520Media%2520Video%2520Dataset%2520for%2520ADAS-equipped%2520vehicle%2520Near-Miss%2520and%2520Crash%2520Event%2520Analyses%26entry.906535625%3DShaoyan%2520Zhai%2520and%2520Mohamed%2520Abdel-Aty%2520and%2520Chenzhu%2520Wang%2520and%2520Rodrigo%2520Vena%2520Garcia%26entry.1292438233%3DThe%2520advancement%2520of%2520safety-critical%2520research%2520in%2520driving%2520behavior%2520in%2520ADAS-equipped%2520vehicles%2520require%2520real-world%2520datasets%2520that%2520not%2520only%2520include%2520diverse%2520traffic%2520scenarios%2520but%2520also%2520capture%2520high-risk%2520edge%2520cases%2520such%2520as%2520near-miss%2520events%2520and%2520system%2520failures.%2520However%252C%2520existing%2520datasets%2520are%2520largely%2520limited%2520to%2520either%2520simulated%2520environments%2520or%2520human-driven%2520vehicle%2520data%252C%2520lacking%2520authentic%2520ADAS%2520%2528Advanced%2520Driver%2520Assistance%2520System%2529%2520vehicle%2520behavior%2520under%2520risk%2520conditions.%2520To%2520address%2520this%2520gap%252C%2520this%2520paper%2520introduces%2520SAVeD%252C%2520a%2520large-scale%2520video%2520dataset%2520curated%2520from%2520publicly%2520available%2520social%2520media%2520content%252C%2520explicitly%2520focused%2520on%2520ADAS%2520vehicle-related%2520crashes%252C%2520near-miss%2520incidents%252C%2520and%2520disengagements.%2520SAVeD%2520features%25202%252C119%2520first-person%2520videos%252C%2520capturing%2520ADAS%2520vehicle%2520operations%2520in%2520diverse%2520locations%252C%2520lighting%2520conditions%252C%2520and%2520weather%2520scenarios.%2520The%2520dataset%2520includes%2520video%2520frame-level%2520annotations%2520for%2520collisions%252C%2520evasive%2520maneuvers%252C%2520and%2520disengagements%252C%2520enabling%2520analysis%2520of%2520both%2520perception%2520and%2520decision-making%2520failures.%2520We%2520demonstrate%2520SAVeD%2527s%2520utility%2520through%2520multiple%2520analyses%2520and%2520contributions%253A%2520%25281%2529%2520We%2520propose%2520a%2520novel%2520framework%2520integrating%2520semantic%2520segmentation%2520and%2520monocular%2520depth%2520estimation%2520to%2520compute%2520real-time%2520Time-to-Collision%2520%2528TTC%2529%2520for%2520dynamic%2520objects.%2520%25282%2529%2520We%2520utilize%2520the%2520Generalized%2520Extreme%2520Value%2520%2528GEV%2529%2520distribution%2520to%2520model%2520and%2520quantify%2520the%2520extreme%2520risk%2520in%2520crash%2520and%2520near-miss%2520events%2520across%2520different%2520roadway%2520types.%2520%25283%2529%2520We%2520establish%2520benchmarks%2520for%2520state-of-the-art%2520VLLMs%2520%2528VideoLLaMA2%2520and%2520InternVL2.5%2520HiCo%2520R16%2529%252C%2520showing%2520that%2520SAVeD%2527s%2520detailed%2520annotations%2520significantly%2520enhance%2520model%2520performance%2520through%2520domain%2520adaptation%2520in%2520complex%2520near-miss%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAVeD%3A%20A%20First-Person%20Social%20Media%20Video%20Dataset%20for%20ADAS-equipped%20vehicle%20Near-Miss%20and%20Crash%20Event%20Analyses&entry.906535625=Shaoyan%20Zhai%20and%20Mohamed%20Abdel-Aty%20and%20Chenzhu%20Wang%20and%20Rodrigo%20Vena%20Garcia&entry.1292438233=The%20advancement%20of%20safety-critical%20research%20in%20driving%20behavior%20in%20ADAS-equipped%20vehicles%20require%20real-world%20datasets%20that%20not%20only%20include%20diverse%20traffic%20scenarios%20but%20also%20capture%20high-risk%20edge%20cases%20such%20as%20near-miss%20events%20and%20system%20failures.%20However%2C%20existing%20datasets%20are%20largely%20limited%20to%20either%20simulated%20environments%20or%20human-driven%20vehicle%20data%2C%20lacking%20authentic%20ADAS%20%28Advanced%20Driver%20Assistance%20System%29%20vehicle%20behavior%20under%20risk%20conditions.%20To%20address%20this%20gap%2C%20this%20paper%20introduces%20SAVeD%2C%20a%20large-scale%20video%20dataset%20curated%20from%20publicly%20available%20social%20media%20content%2C%20explicitly%20focused%20on%20ADAS%20vehicle-related%20crashes%2C%20near-miss%20incidents%2C%20and%20disengagements.%20SAVeD%20features%202%2C119%20first-person%20videos%2C%20capturing%20ADAS%20vehicle%20operations%20in%20diverse%20locations%2C%20lighting%20conditions%2C%20and%20weather%20scenarios.%20The%20dataset%20includes%20video%20frame-level%20annotations%20for%20collisions%2C%20evasive%20maneuvers%2C%20and%20disengagements%2C%20enabling%20analysis%20of%20both%20perception%20and%20decision-making%20failures.%20We%20demonstrate%20SAVeD%27s%20utility%20through%20multiple%20analyses%20and%20contributions%3A%20%281%29%20We%20propose%20a%20novel%20framework%20integrating%20semantic%20segmentation%20and%20monocular%20depth%20estimation%20to%20compute%20real-time%20Time-to-Collision%20%28TTC%29%20for%20dynamic%20objects.%20%282%29%20We%20utilize%20the%20Generalized%20Extreme%20Value%20%28GEV%29%20distribution%20to%20model%20and%20quantify%20the%20extreme%20risk%20in%20crash%20and%20near-miss%20events%20across%20different%20roadway%20types.%20%283%29%20We%20establish%20benchmarks%20for%20state-of-the-art%20VLLMs%20%28VideoLLaMA2%20and%20InternVL2.5%20HiCo%20R16%29%2C%20showing%20that%20SAVeD%27s%20detailed%20annotations%20significantly%20enhance%20model%20performance%20through%20domain%20adaptation%20in%20complex%20near-miss%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.17724v1&entry.124074799=Read"},
{"title": "Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques", "author": "Xingyu Feng", "abstract": "Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.", "link": "http://arxiv.org/abs/2512.17411v1", "date": "2025-12-19", "relevancy": 1.7156, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4342}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4292}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20and%20Analysis%20of%20Sensitive%20and%20Illegal%20Content%20on%20the%20Ethereum%20Blockchain%20Using%20Machine%20Learning%20Techniques&body=Title%3A%20Detection%20and%20Analysis%20of%20Sensitive%20and%20Illegal%20Content%20on%20the%20Ethereum%20Blockchain%20Using%20Machine%20Learning%20Techniques%0AAuthor%3A%20Xingyu%20Feng%0AAbstract%3A%20Blockchain%20technology%2C%20lauded%20for%20its%20transparent%20and%20immutable%20nature%2C%20introduces%20a%20novel%20trust%20model.%20However%2C%20its%20decentralized%20structure%20raises%20concerns%20about%20potential%20inclusion%20of%20malicious%20or%20illegal%20content.%20This%20study%20focuses%20on%20Ethereum%2C%20presenting%20a%20data%20identification%20and%20restoration%20algorithm.%20Successfully%20recovering%20175%20common%20files%2C%20296%20images%2C%20and%2091%2C206%20texts%2C%20we%20employed%20the%20FastText%20algorithm%20for%20sentiment%20analysis%2C%20achieving%20a%200.9%20accuracy%20after%20parameter%20tuning.%20Classification%20revealed%2070%2C189%20neutral%2C%205%2C208%20positive%2C%20and%2015%2C810%20negative%20texts%2C%20aiding%20in%20identifying%20sensitive%20or%20illicit%20information.%20Leveraging%20the%20NSFWJS%20library%2C%20we%20detected%20seven%20indecent%20images%20with%20100%25%20accuracy.%20Our%20findings%20expose%20the%20coexistence%20of%20benign%20and%20harmful%20content%20on%20the%20Ethereum%20blockchain%2C%20including%20personal%20data%2C%20explicit%20images%2C%20divisive%20language%2C%20and%20racial%20discrimination.%20Notably%2C%20sensitive%20information%20targeted%20Chinese%20government%20officials.%20Proposing%20preventative%20measures%2C%20our%20study%20offers%20valuable%20insights%20for%20public%20comprehension%20of%20blockchain%20technology%20and%20regulatory%20agency%20guidance.%20The%20algorithms%20employed%20present%20innovative%20solutions%20to%20address%20blockchain%20data%20privacy%20and%20security%20concerns.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520and%2520Analysis%2520of%2520Sensitive%2520and%2520Illegal%2520Content%2520on%2520the%2520Ethereum%2520Blockchain%2520Using%2520Machine%2520Learning%2520Techniques%26entry.906535625%3DXingyu%2520Feng%26entry.1292438233%3DBlockchain%2520technology%252C%2520lauded%2520for%2520its%2520transparent%2520and%2520immutable%2520nature%252C%2520introduces%2520a%2520novel%2520trust%2520model.%2520However%252C%2520its%2520decentralized%2520structure%2520raises%2520concerns%2520about%2520potential%2520inclusion%2520of%2520malicious%2520or%2520illegal%2520content.%2520This%2520study%2520focuses%2520on%2520Ethereum%252C%2520presenting%2520a%2520data%2520identification%2520and%2520restoration%2520algorithm.%2520Successfully%2520recovering%2520175%2520common%2520files%252C%2520296%2520images%252C%2520and%252091%252C206%2520texts%252C%2520we%2520employed%2520the%2520FastText%2520algorithm%2520for%2520sentiment%2520analysis%252C%2520achieving%2520a%25200.9%2520accuracy%2520after%2520parameter%2520tuning.%2520Classification%2520revealed%252070%252C189%2520neutral%252C%25205%252C208%2520positive%252C%2520and%252015%252C810%2520negative%2520texts%252C%2520aiding%2520in%2520identifying%2520sensitive%2520or%2520illicit%2520information.%2520Leveraging%2520the%2520NSFWJS%2520library%252C%2520we%2520detected%2520seven%2520indecent%2520images%2520with%2520100%2525%2520accuracy.%2520Our%2520findings%2520expose%2520the%2520coexistence%2520of%2520benign%2520and%2520harmful%2520content%2520on%2520the%2520Ethereum%2520blockchain%252C%2520including%2520personal%2520data%252C%2520explicit%2520images%252C%2520divisive%2520language%252C%2520and%2520racial%2520discrimination.%2520Notably%252C%2520sensitive%2520information%2520targeted%2520Chinese%2520government%2520officials.%2520Proposing%2520preventative%2520measures%252C%2520our%2520study%2520offers%2520valuable%2520insights%2520for%2520public%2520comprehension%2520of%2520blockchain%2520technology%2520and%2520regulatory%2520agency%2520guidance.%2520The%2520algorithms%2520employed%2520present%2520innovative%2520solutions%2520to%2520address%2520blockchain%2520data%2520privacy%2520and%2520security%2520concerns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20and%20Analysis%20of%20Sensitive%20and%20Illegal%20Content%20on%20the%20Ethereum%20Blockchain%20Using%20Machine%20Learning%20Techniques&entry.906535625=Xingyu%20Feng&entry.1292438233=Blockchain%20technology%2C%20lauded%20for%20its%20transparent%20and%20immutable%20nature%2C%20introduces%20a%20novel%20trust%20model.%20However%2C%20its%20decentralized%20structure%20raises%20concerns%20about%20potential%20inclusion%20of%20malicious%20or%20illegal%20content.%20This%20study%20focuses%20on%20Ethereum%2C%20presenting%20a%20data%20identification%20and%20restoration%20algorithm.%20Successfully%20recovering%20175%20common%20files%2C%20296%20images%2C%20and%2091%2C206%20texts%2C%20we%20employed%20the%20FastText%20algorithm%20for%20sentiment%20analysis%2C%20achieving%20a%200.9%20accuracy%20after%20parameter%20tuning.%20Classification%20revealed%2070%2C189%20neutral%2C%205%2C208%20positive%2C%20and%2015%2C810%20negative%20texts%2C%20aiding%20in%20identifying%20sensitive%20or%20illicit%20information.%20Leveraging%20the%20NSFWJS%20library%2C%20we%20detected%20seven%20indecent%20images%20with%20100%25%20accuracy.%20Our%20findings%20expose%20the%20coexistence%20of%20benign%20and%20harmful%20content%20on%20the%20Ethereum%20blockchain%2C%20including%20personal%20data%2C%20explicit%20images%2C%20divisive%20language%2C%20and%20racial%20discrimination.%20Notably%2C%20sensitive%20information%20targeted%20Chinese%20government%20officials.%20Proposing%20preventative%20measures%2C%20our%20study%20offers%20valuable%20insights%20for%20public%20comprehension%20of%20blockchain%20technology%20and%20regulatory%20agency%20guidance.%20The%20algorithms%20employed%20present%20innovative%20solutions%20to%20address%20blockchain%20data%20privacy%20and%20security%20concerns.&entry.1838667208=http%3A//arxiv.org/abs/2512.17411v1&entry.124074799=Read"},
{"title": "Best Practices For Empirical Meta-Algorithmic Research: Guidelines from the COSEAL Research Network", "author": "Theresa Eimer and Lennart Sch\u00e4permeier and Andr\u00e9 Biedenkapp and Alexander Tornede and Lars Kotthoff and Pieter Leyman and Matthias Feurer and Katharina Eggensperger and Kaitlin Maile and Tanja Tornede and Anna Kozak and Ke Xue and Marcel Wever and Mitra Baratchi and Damir Pulatov and Heike Trautmann and Haniye Kashgarani and Marius Lindauer", "abstract": "Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing experiments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.", "link": "http://arxiv.org/abs/2512.16491v2", "date": "2025-12-19", "relevancy": 1.1616, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4259}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Best%20Practices%20For%20Empirical%20Meta-Algorithmic%20Research%3A%20Guidelines%20from%20the%20COSEAL%20Research%20Network&body=Title%3A%20Best%20Practices%20For%20Empirical%20Meta-Algorithmic%20Research%3A%20Guidelines%20from%20the%20COSEAL%20Research%20Network%0AAuthor%3A%20Theresa%20Eimer%20and%20Lennart%20Sch%C3%A4permeier%20and%20Andr%C3%A9%20Biedenkapp%20and%20Alexander%20Tornede%20and%20Lars%20Kotthoff%20and%20Pieter%20Leyman%20and%20Matthias%20Feurer%20and%20Katharina%20Eggensperger%20and%20Kaitlin%20Maile%20and%20Tanja%20Tornede%20and%20Anna%20Kozak%20and%20Ke%20Xue%20and%20Marcel%20Wever%20and%20Mitra%20Baratchi%20and%20Damir%20Pulatov%20and%20Heike%20Trautmann%20and%20Haniye%20Kashgarani%20and%20Marius%20Lindauer%0AAbstract%3A%20Empirical%20research%20on%20meta-algorithmics%2C%20such%20as%20algorithm%20selection%2C%20configuration%2C%20and%20scheduling%2C%20often%20relies%20on%20extensive%20and%20thus%20computationally%20expensive%20experiments.%20With%20the%20large%20degree%20of%20freedom%20we%20have%20over%20our%20experimental%20setup%20and%20design%20comes%20a%20plethora%20of%20possible%20error%20sources%20that%20threaten%20the%20scalability%20and%20validity%20of%20our%20scientific%20insights.%20Best%20practices%20for%20meta-algorithmic%20research%20exist%2C%20but%20they%20are%20scattered%20between%20different%20publications%20and%20fields%2C%20and%20continue%20to%20evolve%20separately%20from%20each%20other.%20In%20this%20report%2C%20we%20collect%20good%20practices%20for%20empirical%20meta-algorithmic%20research%20across%20the%20subfields%20of%20the%20COSEAL%20community%2C%20encompassing%20the%20entire%20experimental%20cycle%3A%20from%20formulating%20research%20questions%20and%20selecting%20an%20experimental%20design%2C%20to%20executing%20experiments%2C%20and%20ultimately%2C%20analyzing%20and%20presenting%20results%20impartially.%20It%20establishes%20the%20current%20state-of-the-art%20practices%20within%20meta-algorithmic%20research%20and%20serves%20as%20a%20guideline%20to%20both%20new%20researchers%20and%20practitioners%20in%20meta-algorithmic%20fields.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBest%2520Practices%2520For%2520Empirical%2520Meta-Algorithmic%2520Research%253A%2520Guidelines%2520from%2520the%2520COSEAL%2520Research%2520Network%26entry.906535625%3DTheresa%2520Eimer%2520and%2520Lennart%2520Sch%25C3%25A4permeier%2520and%2520Andr%25C3%25A9%2520Biedenkapp%2520and%2520Alexander%2520Tornede%2520and%2520Lars%2520Kotthoff%2520and%2520Pieter%2520Leyman%2520and%2520Matthias%2520Feurer%2520and%2520Katharina%2520Eggensperger%2520and%2520Kaitlin%2520Maile%2520and%2520Tanja%2520Tornede%2520and%2520Anna%2520Kozak%2520and%2520Ke%2520Xue%2520and%2520Marcel%2520Wever%2520and%2520Mitra%2520Baratchi%2520and%2520Damir%2520Pulatov%2520and%2520Heike%2520Trautmann%2520and%2520Haniye%2520Kashgarani%2520and%2520Marius%2520Lindauer%26entry.1292438233%3DEmpirical%2520research%2520on%2520meta-algorithmics%252C%2520such%2520as%2520algorithm%2520selection%252C%2520configuration%252C%2520and%2520scheduling%252C%2520often%2520relies%2520on%2520extensive%2520and%2520thus%2520computationally%2520expensive%2520experiments.%2520With%2520the%2520large%2520degree%2520of%2520freedom%2520we%2520have%2520over%2520our%2520experimental%2520setup%2520and%2520design%2520comes%2520a%2520plethora%2520of%2520possible%2520error%2520sources%2520that%2520threaten%2520the%2520scalability%2520and%2520validity%2520of%2520our%2520scientific%2520insights.%2520Best%2520practices%2520for%2520meta-algorithmic%2520research%2520exist%252C%2520but%2520they%2520are%2520scattered%2520between%2520different%2520publications%2520and%2520fields%252C%2520and%2520continue%2520to%2520evolve%2520separately%2520from%2520each%2520other.%2520In%2520this%2520report%252C%2520we%2520collect%2520good%2520practices%2520for%2520empirical%2520meta-algorithmic%2520research%2520across%2520the%2520subfields%2520of%2520the%2520COSEAL%2520community%252C%2520encompassing%2520the%2520entire%2520experimental%2520cycle%253A%2520from%2520formulating%2520research%2520questions%2520and%2520selecting%2520an%2520experimental%2520design%252C%2520to%2520executing%2520experiments%252C%2520and%2520ultimately%252C%2520analyzing%2520and%2520presenting%2520results%2520impartially.%2520It%2520establishes%2520the%2520current%2520state-of-the-art%2520practices%2520within%2520meta-algorithmic%2520research%2520and%2520serves%2520as%2520a%2520guideline%2520to%2520both%2520new%2520researchers%2520and%2520practitioners%2520in%2520meta-algorithmic%2520fields.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best%20Practices%20For%20Empirical%20Meta-Algorithmic%20Research%3A%20Guidelines%20from%20the%20COSEAL%20Research%20Network&entry.906535625=Theresa%20Eimer%20and%20Lennart%20Sch%C3%A4permeier%20and%20Andr%C3%A9%20Biedenkapp%20and%20Alexander%20Tornede%20and%20Lars%20Kotthoff%20and%20Pieter%20Leyman%20and%20Matthias%20Feurer%20and%20Katharina%20Eggensperger%20and%20Kaitlin%20Maile%20and%20Tanja%20Tornede%20and%20Anna%20Kozak%20and%20Ke%20Xue%20and%20Marcel%20Wever%20and%20Mitra%20Baratchi%20and%20Damir%20Pulatov%20and%20Heike%20Trautmann%20and%20Haniye%20Kashgarani%20and%20Marius%20Lindauer&entry.1292438233=Empirical%20research%20on%20meta-algorithmics%2C%20such%20as%20algorithm%20selection%2C%20configuration%2C%20and%20scheduling%2C%20often%20relies%20on%20extensive%20and%20thus%20computationally%20expensive%20experiments.%20With%20the%20large%20degree%20of%20freedom%20we%20have%20over%20our%20experimental%20setup%20and%20design%20comes%20a%20plethora%20of%20possible%20error%20sources%20that%20threaten%20the%20scalability%20and%20validity%20of%20our%20scientific%20insights.%20Best%20practices%20for%20meta-algorithmic%20research%20exist%2C%20but%20they%20are%20scattered%20between%20different%20publications%20and%20fields%2C%20and%20continue%20to%20evolve%20separately%20from%20each%20other.%20In%20this%20report%2C%20we%20collect%20good%20practices%20for%20empirical%20meta-algorithmic%20research%20across%20the%20subfields%20of%20the%20COSEAL%20community%2C%20encompassing%20the%20entire%20experimental%20cycle%3A%20from%20formulating%20research%20questions%20and%20selecting%20an%20experimental%20design%2C%20to%20executing%20experiments%2C%20and%20ultimately%2C%20analyzing%20and%20presenting%20results%20impartially.%20It%20establishes%20the%20current%20state-of-the-art%20practices%20within%20meta-algorithmic%20research%20and%20serves%20as%20a%20guideline%20to%20both%20new%20researchers%20and%20practitioners%20in%20meta-algorithmic%20fields.&entry.1838667208=http%3A//arxiv.org/abs/2512.16491v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


