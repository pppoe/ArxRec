<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250527.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Generalizable and Relightable Gaussian Splatting for Human Novel View\n  Synthesis", "author": "Yipengjing Sun and Chenyang Wang and Shunyuan Zheng and Zonglin Li and Shengping Zhang and Xiangyang Ji", "abstract": "  We propose GRGS, a generalizable and relightable 3D Gaussian framework for\nhigh-fidelity human novel view synthesis under diverse lighting conditions.\nUnlike existing methods that rely on per-character optimization or ignore\nphysical constraints, GRGS adopts a feed-forward, fully supervised strategy\nthat projects geometry, material, and illumination cues from multi-view 2D\nobservations into 3D Gaussian representations. Specifically, to reconstruct\nlighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement\n(LGR) module trained on synthetically relit data to predict accurate depth and\nsurface normals. Based on the high-quality geometry, a Physically Grounded\nNeural Rendering (PGNR) module is further proposed to integrate neural\nprediction with physics-based shading, supporting editable relighting with\nshadows and indirect illumination. Besides, we design a 2D-to-3D projection\ntraining scheme that leverages differentiable supervision from ambient\nocclusion, direct, and indirect lighting maps, which alleviates the\ncomputational cost of explicit ray tracing. Extensive experiments demonstrate\nthat GRGS achieves superior visual quality, geometric consistency, and\ngeneralization across characters and lighting conditions.\n", "link": "http://arxiv.org/abs/2505.21502v1", "date": "2025-05-27", "relevancy": 3.4249, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7133}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6994}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20and%20Relightable%20Gaussian%20Splatting%20for%20Human%20Novel%20View%0A%20%20Synthesis&body=Title%3A%20Generalizable%20and%20Relightable%20Gaussian%20Splatting%20for%20Human%20Novel%20View%0A%20%20Synthesis%0AAuthor%3A%20Yipengjing%20Sun%20and%20Chenyang%20Wang%20and%20Shunyuan%20Zheng%20and%20Zonglin%20Li%20and%20Shengping%20Zhang%20and%20Xiangyang%20Ji%0AAbstract%3A%20%20%20We%20propose%20GRGS%2C%20a%20generalizable%20and%20relightable%203D%20Gaussian%20framework%20for%0Ahigh-fidelity%20human%20novel%20view%20synthesis%20under%20diverse%20lighting%20conditions.%0AUnlike%20existing%20methods%20that%20rely%20on%20per-character%20optimization%20or%20ignore%0Aphysical%20constraints%2C%20GRGS%20adopts%20a%20feed-forward%2C%20fully%20supervised%20strategy%0Athat%20projects%20geometry%2C%20material%2C%20and%20illumination%20cues%20from%20multi-view%202D%0Aobservations%20into%203D%20Gaussian%20representations.%20Specifically%2C%20to%20reconstruct%0Alighting-invariant%20geometry%2C%20we%20introduce%20a%20Lighting-aware%20Geometry%20Refinement%0A%28LGR%29%20module%20trained%20on%20synthetically%20relit%20data%20to%20predict%20accurate%20depth%20and%0Asurface%20normals.%20Based%20on%20the%20high-quality%20geometry%2C%20a%20Physically%20Grounded%0ANeural%20Rendering%20%28PGNR%29%20module%20is%20further%20proposed%20to%20integrate%20neural%0Aprediction%20with%20physics-based%20shading%2C%20supporting%20editable%20relighting%20with%0Ashadows%20and%20indirect%20illumination.%20Besides%2C%20we%20design%20a%202D-to-3D%20projection%0Atraining%20scheme%20that%20leverages%20differentiable%20supervision%20from%20ambient%0Aocclusion%2C%20direct%2C%20and%20indirect%20lighting%20maps%2C%20which%20alleviates%20the%0Acomputational%20cost%20of%20explicit%20ray%20tracing.%20Extensive%20experiments%20demonstrate%0Athat%20GRGS%20achieves%20superior%20visual%20quality%2C%20geometric%20consistency%2C%20and%0Ageneralization%20across%20characters%20and%20lighting%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520and%2520Relightable%2520Gaussian%2520Splatting%2520for%2520Human%2520Novel%2520View%250A%2520%2520Synthesis%26entry.906535625%3DYipengjing%2520Sun%2520and%2520Chenyang%2520Wang%2520and%2520Shunyuan%2520Zheng%2520and%2520Zonglin%2520Li%2520and%2520Shengping%2520Zhang%2520and%2520Xiangyang%2520Ji%26entry.1292438233%3D%2520%2520We%2520propose%2520GRGS%252C%2520a%2520generalizable%2520and%2520relightable%25203D%2520Gaussian%2520framework%2520for%250Ahigh-fidelity%2520human%2520novel%2520view%2520synthesis%2520under%2520diverse%2520lighting%2520conditions.%250AUnlike%2520existing%2520methods%2520that%2520rely%2520on%2520per-character%2520optimization%2520or%2520ignore%250Aphysical%2520constraints%252C%2520GRGS%2520adopts%2520a%2520feed-forward%252C%2520fully%2520supervised%2520strategy%250Athat%2520projects%2520geometry%252C%2520material%252C%2520and%2520illumination%2520cues%2520from%2520multi-view%25202D%250Aobservations%2520into%25203D%2520Gaussian%2520representations.%2520Specifically%252C%2520to%2520reconstruct%250Alighting-invariant%2520geometry%252C%2520we%2520introduce%2520a%2520Lighting-aware%2520Geometry%2520Refinement%250A%2528LGR%2529%2520module%2520trained%2520on%2520synthetically%2520relit%2520data%2520to%2520predict%2520accurate%2520depth%2520and%250Asurface%2520normals.%2520Based%2520on%2520the%2520high-quality%2520geometry%252C%2520a%2520Physically%2520Grounded%250ANeural%2520Rendering%2520%2528PGNR%2529%2520module%2520is%2520further%2520proposed%2520to%2520integrate%2520neural%250Aprediction%2520with%2520physics-based%2520shading%252C%2520supporting%2520editable%2520relighting%2520with%250Ashadows%2520and%2520indirect%2520illumination.%2520Besides%252C%2520we%2520design%2520a%25202D-to-3D%2520projection%250Atraining%2520scheme%2520that%2520leverages%2520differentiable%2520supervision%2520from%2520ambient%250Aocclusion%252C%2520direct%252C%2520and%2520indirect%2520lighting%2520maps%252C%2520which%2520alleviates%2520the%250Acomputational%2520cost%2520of%2520explicit%2520ray%2520tracing.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520GRGS%2520achieves%2520superior%2520visual%2520quality%252C%2520geometric%2520consistency%252C%2520and%250Ageneralization%2520across%2520characters%2520and%2520lighting%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20and%20Relightable%20Gaussian%20Splatting%20for%20Human%20Novel%20View%0A%20%20Synthesis&entry.906535625=Yipengjing%20Sun%20and%20Chenyang%20Wang%20and%20Shunyuan%20Zheng%20and%20Zonglin%20Li%20and%20Shengping%20Zhang%20and%20Xiangyang%20Ji&entry.1292438233=%20%20We%20propose%20GRGS%2C%20a%20generalizable%20and%20relightable%203D%20Gaussian%20framework%20for%0Ahigh-fidelity%20human%20novel%20view%20synthesis%20under%20diverse%20lighting%20conditions.%0AUnlike%20existing%20methods%20that%20rely%20on%20per-character%20optimization%20or%20ignore%0Aphysical%20constraints%2C%20GRGS%20adopts%20a%20feed-forward%2C%20fully%20supervised%20strategy%0Athat%20projects%20geometry%2C%20material%2C%20and%20illumination%20cues%20from%20multi-view%202D%0Aobservations%20into%203D%20Gaussian%20representations.%20Specifically%2C%20to%20reconstruct%0Alighting-invariant%20geometry%2C%20we%20introduce%20a%20Lighting-aware%20Geometry%20Refinement%0A%28LGR%29%20module%20trained%20on%20synthetically%20relit%20data%20to%20predict%20accurate%20depth%20and%0Asurface%20normals.%20Based%20on%20the%20high-quality%20geometry%2C%20a%20Physically%20Grounded%0ANeural%20Rendering%20%28PGNR%29%20module%20is%20further%20proposed%20to%20integrate%20neural%0Aprediction%20with%20physics-based%20shading%2C%20supporting%20editable%20relighting%20with%0Ashadows%20and%20indirect%20illumination.%20Besides%2C%20we%20design%20a%202D-to-3D%20projection%0Atraining%20scheme%20that%20leverages%20differentiable%20supervision%20from%20ambient%0Aocclusion%2C%20direct%2C%20and%20indirect%20lighting%20maps%2C%20which%20alleviates%20the%0Acomputational%20cost%20of%20explicit%20ray%20tracing.%20Extensive%20experiments%20demonstrate%0Athat%20GRGS%20achieves%20superior%20visual%20quality%2C%20geometric%20consistency%2C%20and%0Ageneralization%20across%20characters%20and%20lighting%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21502v1&entry.124074799=Read"},
{"title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles", "author": "Peng Wang and Xiang Liu and Peidong Liu", "abstract": "  Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.\n", "link": "http://arxiv.org/abs/2505.21060v1", "date": "2025-05-27", "relevancy": 3.2051, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6461}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6461}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Styl3R%3A%20Instant%203D%20Stylized%20Reconstruction%20for%20Arbitrary%20Scenes%20and%0A%20%20Styles&body=Title%3A%20Styl3R%3A%20Instant%203D%20Stylized%20Reconstruction%20for%20Arbitrary%20Scenes%20and%0A%20%20Styles%0AAuthor%3A%20Peng%20Wang%20and%20Xiang%20Liu%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Stylizing%203D%20scenes%20instantly%20while%20maintaining%20multi-view%20consistency%20and%0Afaithfully%20resembling%20a%20style%20image%20remains%20a%20significant%20challenge.%20Current%0Astate-of-the-art%203D%20stylization%20methods%20typically%20involve%20computationally%0Aintensive%20test-time%20optimization%20to%20transfer%20artistic%20features%20into%20a%0Apretrained%203D%20representation%2C%20often%20requiring%20dense%20posed%20input%20images.%20In%0Acontrast%2C%20leveraging%20recent%20advances%20in%20feed-forward%20reconstruction%20models%2C%20we%0Ademonstrate%20a%20novel%20approach%20to%20achieve%20direct%203D%20stylization%20in%20less%20than%20a%0Asecond%20using%20unposed%20sparse-view%20scene%20images%20and%20an%20arbitrary%20style%20image.%20To%0Aaddress%20the%20inherent%20decoupling%20between%20reconstruction%20and%20stylization%2C%20we%0Aintroduce%20a%20branched%20architecture%20that%20separates%20structure%20modeling%20and%0Aappearance%20shading%2C%20effectively%20preventing%20stylistic%20transfer%20from%20distorting%0Athe%20underlying%203D%20scene%20structure.%20Furthermore%2C%20we%20adapt%20an%20identity%20loss%20to%0Afacilitate%20pre-training%20our%20stylization%20model%20through%20the%20novel%20view%20synthesis%0Atask.%20This%20strategy%20also%20allows%20our%20model%20to%20retain%20its%20original%20reconstruction%0Acapabilities%20while%20being%20fine-tuned%20for%20stylization.%20Comprehensive%20evaluations%2C%0Ausing%20both%20in-domain%20and%20out-of-domain%20datasets%2C%20demonstrate%20that%20our%20approach%0Aproduces%20high-quality%20stylized%203D%20content%20that%20achieve%20a%20superior%20blend%20of%0Astyle%20and%20scene%20appearance%2C%20while%20also%20outperforming%20existing%20methods%20in%20terms%0Aof%20multi-view%20consistency%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyl3R%253A%2520Instant%25203D%2520Stylized%2520Reconstruction%2520for%2520Arbitrary%2520Scenes%2520and%250A%2520%2520Styles%26entry.906535625%3DPeng%2520Wang%2520and%2520Xiang%2520Liu%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Stylizing%25203D%2520scenes%2520instantly%2520while%2520maintaining%2520multi-view%2520consistency%2520and%250Afaithfully%2520resembling%2520a%2520style%2520image%2520remains%2520a%2520significant%2520challenge.%2520Current%250Astate-of-the-art%25203D%2520stylization%2520methods%2520typically%2520involve%2520computationally%250Aintensive%2520test-time%2520optimization%2520to%2520transfer%2520artistic%2520features%2520into%2520a%250Apretrained%25203D%2520representation%252C%2520often%2520requiring%2520dense%2520posed%2520input%2520images.%2520In%250Acontrast%252C%2520leveraging%2520recent%2520advances%2520in%2520feed-forward%2520reconstruction%2520models%252C%2520we%250Ademonstrate%2520a%2520novel%2520approach%2520to%2520achieve%2520direct%25203D%2520stylization%2520in%2520less%2520than%2520a%250Asecond%2520using%2520unposed%2520sparse-view%2520scene%2520images%2520and%2520an%2520arbitrary%2520style%2520image.%2520To%250Aaddress%2520the%2520inherent%2520decoupling%2520between%2520reconstruction%2520and%2520stylization%252C%2520we%250Aintroduce%2520a%2520branched%2520architecture%2520that%2520separates%2520structure%2520modeling%2520and%250Aappearance%2520shading%252C%2520effectively%2520preventing%2520stylistic%2520transfer%2520from%2520distorting%250Athe%2520underlying%25203D%2520scene%2520structure.%2520Furthermore%252C%2520we%2520adapt%2520an%2520identity%2520loss%2520to%250Afacilitate%2520pre-training%2520our%2520stylization%2520model%2520through%2520the%2520novel%2520view%2520synthesis%250Atask.%2520This%2520strategy%2520also%2520allows%2520our%2520model%2520to%2520retain%2520its%2520original%2520reconstruction%250Acapabilities%2520while%2520being%2520fine-tuned%2520for%2520stylization.%2520Comprehensive%2520evaluations%252C%250Ausing%2520both%2520in-domain%2520and%2520out-of-domain%2520datasets%252C%2520demonstrate%2520that%2520our%2520approach%250Aproduces%2520high-quality%2520stylized%25203D%2520content%2520that%2520achieve%2520a%2520superior%2520blend%2520of%250Astyle%2520and%2520scene%2520appearance%252C%2520while%2520also%2520outperforming%2520existing%2520methods%2520in%2520terms%250Aof%2520multi-view%2520consistency%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Styl3R%3A%20Instant%203D%20Stylized%20Reconstruction%20for%20Arbitrary%20Scenes%20and%0A%20%20Styles&entry.906535625=Peng%20Wang%20and%20Xiang%20Liu%20and%20Peidong%20Liu&entry.1292438233=%20%20Stylizing%203D%20scenes%20instantly%20while%20maintaining%20multi-view%20consistency%20and%0Afaithfully%20resembling%20a%20style%20image%20remains%20a%20significant%20challenge.%20Current%0Astate-of-the-art%203D%20stylization%20methods%20typically%20involve%20computationally%0Aintensive%20test-time%20optimization%20to%20transfer%20artistic%20features%20into%20a%0Apretrained%203D%20representation%2C%20often%20requiring%20dense%20posed%20input%20images.%20In%0Acontrast%2C%20leveraging%20recent%20advances%20in%20feed-forward%20reconstruction%20models%2C%20we%0Ademonstrate%20a%20novel%20approach%20to%20achieve%20direct%203D%20stylization%20in%20less%20than%20a%0Asecond%20using%20unposed%20sparse-view%20scene%20images%20and%20an%20arbitrary%20style%20image.%20To%0Aaddress%20the%20inherent%20decoupling%20between%20reconstruction%20and%20stylization%2C%20we%0Aintroduce%20a%20branched%20architecture%20that%20separates%20structure%20modeling%20and%0Aappearance%20shading%2C%20effectively%20preventing%20stylistic%20transfer%20from%20distorting%0Athe%20underlying%203D%20scene%20structure.%20Furthermore%2C%20we%20adapt%20an%20identity%20loss%20to%0Afacilitate%20pre-training%20our%20stylization%20model%20through%20the%20novel%20view%20synthesis%0Atask.%20This%20strategy%20also%20allows%20our%20model%20to%20retain%20its%20original%20reconstruction%0Acapabilities%20while%20being%20fine-tuned%20for%20stylization.%20Comprehensive%20evaluations%2C%0Ausing%20both%20in-domain%20and%20out-of-domain%20datasets%2C%20demonstrate%20that%20our%20approach%0Aproduces%20high-quality%20stylized%203D%20content%20that%20achieve%20a%20superior%20blend%20of%0Astyle%20and%20scene%20appearance%2C%20while%20also%20outperforming%20existing%20methods%20in%20terms%0Aof%20multi-view%20consistency%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21060v1&entry.124074799=Read"},
{"title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in\n  Vision-Language Models", "author": "Dingming Li and Hongxing Li and Zixuan Wang and Yuchen Yan and Hang Zhang and Siqi Chen and Guiyang Hou and Shengpei Jiang and Wenqi Zhang and Yongliang Shen and Weiming Lu and Yueting Zhuang", "abstract": "  Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.\n", "link": "http://arxiv.org/abs/2505.21500v1", "date": "2025-05-27", "relevancy": 3.166, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewSpatial-Bench%3A%20Evaluating%20Multi-perspective%20Spatial%20Localization%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20ViewSpatial-Bench%3A%20Evaluating%20Multi-perspective%20Spatial%20Localization%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Dingming%20Li%20and%20Hongxing%20Li%20and%20Zixuan%20Wang%20and%20Yuchen%20Yan%20and%20Hang%20Zhang%20and%20Siqi%20Chen%20and%20Guiyang%20Hou%20and%20Shengpei%20Jiang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Aunderstanding%20and%20reasoning%20about%20visual%20content%2C%20but%20significant%20challenges%0Apersist%20in%20tasks%20requiring%20cross-viewpoint%20understanding%20and%20spatial%20reasoning.%0AWe%20identify%20a%20critical%20limitation%3A%20current%20VLMs%20excel%20primarily%20at%20egocentric%0Aspatial%20reasoning%20%28from%20the%20camera%27s%20perspective%29%20but%20fail%20to%20generalize%20to%0Aallocentric%20viewpoints%20when%20required%20to%20adopt%20another%20entity%27s%20spatial%20frame%20of%0Areference.%20We%20introduce%20ViewSpatial-Bench%2C%20the%20first%20comprehensive%20benchmark%0Adesigned%20specifically%20for%20multi-viewpoint%20spatial%20localization%20recognition%0Aevaluation%20across%20five%20distinct%20task%20types%2C%20supported%20by%20an%20automated%203D%0Aannotation%20pipeline%20that%20generates%20precise%20directional%20labels.%20Comprehensive%0Aevaluation%20of%20diverse%20VLMs%20on%20ViewSpatial-Bench%20reveals%20a%20significant%0Aperformance%20disparity%3A%20models%20demonstrate%20reasonable%20performance%20on%0Acamera-perspective%20tasks%20but%20exhibit%20reduced%20accuracy%20when%20reasoning%20from%20a%0Ahuman%20viewpoint.%20By%20fine-tuning%20VLMs%20on%20our%20multi-perspective%20spatial%20dataset%2C%0Awe%20achieve%20an%20overall%20performance%20improvement%20of%2046.24%25%20across%20tasks%2C%0Ahighlighting%20the%20efficacy%20of%20our%20approach.%20Our%20work%20establishes%20a%20crucial%0Abenchmark%20for%20spatial%20intelligence%20in%20embodied%20AI%20systems%20and%20provides%0Aempirical%20evidence%20that%20modeling%203D%20spatial%20relationships%20enhances%20VLMs%27%0Acorresponding%20spatial%20comprehension%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewSpatial-Bench%253A%2520Evaluating%2520Multi-perspective%2520Spatial%2520Localization%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DDingming%2520Li%2520and%2520Hongxing%2520Li%2520and%2520Zixuan%2520Wang%2520and%2520Yuchen%2520Yan%2520and%2520Hang%2520Zhang%2520and%2520Siqi%2520Chen%2520and%2520Guiyang%2520Hou%2520and%2520Shengpei%2520Jiang%2520and%2520Wenqi%2520Zhang%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Aunderstanding%2520and%2520reasoning%2520about%2520visual%2520content%252C%2520but%2520significant%2520challenges%250Apersist%2520in%2520tasks%2520requiring%2520cross-viewpoint%2520understanding%2520and%2520spatial%2520reasoning.%250AWe%2520identify%2520a%2520critical%2520limitation%253A%2520current%2520VLMs%2520excel%2520primarily%2520at%2520egocentric%250Aspatial%2520reasoning%2520%2528from%2520the%2520camera%2527s%2520perspective%2529%2520but%2520fail%2520to%2520generalize%2520to%250Aallocentric%2520viewpoints%2520when%2520required%2520to%2520adopt%2520another%2520entity%2527s%2520spatial%2520frame%2520of%250Areference.%2520We%2520introduce%2520ViewSpatial-Bench%252C%2520the%2520first%2520comprehensive%2520benchmark%250Adesigned%2520specifically%2520for%2520multi-viewpoint%2520spatial%2520localization%2520recognition%250Aevaluation%2520across%2520five%2520distinct%2520task%2520types%252C%2520supported%2520by%2520an%2520automated%25203D%250Aannotation%2520pipeline%2520that%2520generates%2520precise%2520directional%2520labels.%2520Comprehensive%250Aevaluation%2520of%2520diverse%2520VLMs%2520on%2520ViewSpatial-Bench%2520reveals%2520a%2520significant%250Aperformance%2520disparity%253A%2520models%2520demonstrate%2520reasonable%2520performance%2520on%250Acamera-perspective%2520tasks%2520but%2520exhibit%2520reduced%2520accuracy%2520when%2520reasoning%2520from%2520a%250Ahuman%2520viewpoint.%2520By%2520fine-tuning%2520VLMs%2520on%2520our%2520multi-perspective%2520spatial%2520dataset%252C%250Awe%2520achieve%2520an%2520overall%2520performance%2520improvement%2520of%252046.24%2525%2520across%2520tasks%252C%250Ahighlighting%2520the%2520efficacy%2520of%2520our%2520approach.%2520Our%2520work%2520establishes%2520a%2520crucial%250Abenchmark%2520for%2520spatial%2520intelligence%2520in%2520embodied%2520AI%2520systems%2520and%2520provides%250Aempirical%2520evidence%2520that%2520modeling%25203D%2520spatial%2520relationships%2520enhances%2520VLMs%2527%250Acorresponding%2520spatial%2520comprehension%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewSpatial-Bench%3A%20Evaluating%20Multi-perspective%20Spatial%20Localization%20in%0A%20%20Vision-Language%20Models&entry.906535625=Dingming%20Li%20and%20Hongxing%20Li%20and%20Zixuan%20Wang%20and%20Yuchen%20Yan%20and%20Hang%20Zhang%20and%20Siqi%20Chen%20and%20Guiyang%20Hou%20and%20Shengpei%20Jiang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Aunderstanding%20and%20reasoning%20about%20visual%20content%2C%20but%20significant%20challenges%0Apersist%20in%20tasks%20requiring%20cross-viewpoint%20understanding%20and%20spatial%20reasoning.%0AWe%20identify%20a%20critical%20limitation%3A%20current%20VLMs%20excel%20primarily%20at%20egocentric%0Aspatial%20reasoning%20%28from%20the%20camera%27s%20perspective%29%20but%20fail%20to%20generalize%20to%0Aallocentric%20viewpoints%20when%20required%20to%20adopt%20another%20entity%27s%20spatial%20frame%20of%0Areference.%20We%20introduce%20ViewSpatial-Bench%2C%20the%20first%20comprehensive%20benchmark%0Adesigned%20specifically%20for%20multi-viewpoint%20spatial%20localization%20recognition%0Aevaluation%20across%20five%20distinct%20task%20types%2C%20supported%20by%20an%20automated%203D%0Aannotation%20pipeline%20that%20generates%20precise%20directional%20labels.%20Comprehensive%0Aevaluation%20of%20diverse%20VLMs%20on%20ViewSpatial-Bench%20reveals%20a%20significant%0Aperformance%20disparity%3A%20models%20demonstrate%20reasonable%20performance%20on%0Acamera-perspective%20tasks%20but%20exhibit%20reduced%20accuracy%20when%20reasoning%20from%20a%0Ahuman%20viewpoint.%20By%20fine-tuning%20VLMs%20on%20our%20multi-perspective%20spatial%20dataset%2C%0Awe%20achieve%20an%20overall%20performance%20improvement%20of%2046.24%25%20across%20tasks%2C%0Ahighlighting%20the%20efficacy%20of%20our%20approach.%20Our%20work%20establishes%20a%20crucial%0Abenchmark%20for%20spatial%20intelligence%20in%20embodied%20AI%20systems%20and%20provides%0Aempirical%20evidence%20that%20modeling%203D%20spatial%20relationships%20enhances%20VLMs%27%0Acorresponding%20spatial%20comprehension%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21500v1&entry.124074799=Read"},
{"title": "MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D\n  Scene Relighting", "author": "Yumeng He and Yunbo Wang and Xiaokang Yang", "abstract": "  Out-of-distribution (OOD) 3D relighting requires novel view synthesis under\nunseen lighting conditions that differ significantly from the observed images.\nExisting relighting methods, which assume consistent light source distributions\nbetween training and testing, often degrade in OOD scenarios. We introduce\nMetaGS to tackle this challenge from two perspectives. First, we propose a\nmeta-learning approach to train 3D Gaussian splatting, which explicitly\npromotes learning generalizable Gaussian geometries and appearance attributes\nacross diverse lighting conditions, even with biased training data. Second, we\nembed fundamental physical priors from the Blinn-Phong reflection model into\nGaussian splatting, which enhances the decoupling of shading components and\nleads to more accurate 3D scene reconstruction. Results on both synthetic and\nreal-world datasets demonstrate the effectiveness of MetaGS in challenging OOD\nrelighting tasks, supporting efficient point-light relighting and generalizing\nwell to unseen environment lighting maps.\n", "link": "http://arxiv.org/abs/2405.20791v2", "date": "2025-05-27", "relevancy": 3.0707, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6162}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6156}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaGS%3A%20A%20Meta-Learned%20Gaussian-Phong%20Model%20for%20Out-of-Distribution%203D%0A%20%20Scene%20Relighting&body=Title%3A%20MetaGS%3A%20A%20Meta-Learned%20Gaussian-Phong%20Model%20for%20Out-of-Distribution%203D%0A%20%20Scene%20Relighting%0AAuthor%3A%20Yumeng%20He%20and%20Yunbo%20Wang%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%203D%20relighting%20requires%20novel%20view%20synthesis%20under%0Aunseen%20lighting%20conditions%20that%20differ%20significantly%20from%20the%20observed%20images.%0AExisting%20relighting%20methods%2C%20which%20assume%20consistent%20light%20source%20distributions%0Abetween%20training%20and%20testing%2C%20often%20degrade%20in%20OOD%20scenarios.%20We%20introduce%0AMetaGS%20to%20tackle%20this%20challenge%20from%20two%20perspectives.%20First%2C%20we%20propose%20a%0Ameta-learning%20approach%20to%20train%203D%20Gaussian%20splatting%2C%20which%20explicitly%0Apromotes%20learning%20generalizable%20Gaussian%20geometries%20and%20appearance%20attributes%0Aacross%20diverse%20lighting%20conditions%2C%20even%20with%20biased%20training%20data.%20Second%2C%20we%0Aembed%20fundamental%20physical%20priors%20from%20the%20Blinn-Phong%20reflection%20model%20into%0AGaussian%20splatting%2C%20which%20enhances%20the%20decoupling%20of%20shading%20components%20and%0Aleads%20to%20more%20accurate%203D%20scene%20reconstruction.%20Results%20on%20both%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20the%20effectiveness%20of%20MetaGS%20in%20challenging%20OOD%0Arelighting%20tasks%2C%20supporting%20efficient%20point-light%20relighting%20and%20generalizing%0Awell%20to%20unseen%20environment%20lighting%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaGS%253A%2520A%2520Meta-Learned%2520Gaussian-Phong%2520Model%2520for%2520Out-of-Distribution%25203D%250A%2520%2520Scene%2520Relighting%26entry.906535625%3DYumeng%2520He%2520and%2520Yunbo%2520Wang%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%25203D%2520relighting%2520requires%2520novel%2520view%2520synthesis%2520under%250Aunseen%2520lighting%2520conditions%2520that%2520differ%2520significantly%2520from%2520the%2520observed%2520images.%250AExisting%2520relighting%2520methods%252C%2520which%2520assume%2520consistent%2520light%2520source%2520distributions%250Abetween%2520training%2520and%2520testing%252C%2520often%2520degrade%2520in%2520OOD%2520scenarios.%2520We%2520introduce%250AMetaGS%2520to%2520tackle%2520this%2520challenge%2520from%2520two%2520perspectives.%2520First%252C%2520we%2520propose%2520a%250Ameta-learning%2520approach%2520to%2520train%25203D%2520Gaussian%2520splatting%252C%2520which%2520explicitly%250Apromotes%2520learning%2520generalizable%2520Gaussian%2520geometries%2520and%2520appearance%2520attributes%250Aacross%2520diverse%2520lighting%2520conditions%252C%2520even%2520with%2520biased%2520training%2520data.%2520Second%252C%2520we%250Aembed%2520fundamental%2520physical%2520priors%2520from%2520the%2520Blinn-Phong%2520reflection%2520model%2520into%250AGaussian%2520splatting%252C%2520which%2520enhances%2520the%2520decoupling%2520of%2520shading%2520components%2520and%250Aleads%2520to%2520more%2520accurate%25203D%2520scene%2520reconstruction.%2520Results%2520on%2520both%2520synthetic%2520and%250Areal-world%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520MetaGS%2520in%2520challenging%2520OOD%250Arelighting%2520tasks%252C%2520supporting%2520efficient%2520point-light%2520relighting%2520and%2520generalizing%250Awell%2520to%2520unseen%2520environment%2520lighting%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaGS%3A%20A%20Meta-Learned%20Gaussian-Phong%20Model%20for%20Out-of-Distribution%203D%0A%20%20Scene%20Relighting&entry.906535625=Yumeng%20He%20and%20Yunbo%20Wang%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%203D%20relighting%20requires%20novel%20view%20synthesis%20under%0Aunseen%20lighting%20conditions%20that%20differ%20significantly%20from%20the%20observed%20images.%0AExisting%20relighting%20methods%2C%20which%20assume%20consistent%20light%20source%20distributions%0Abetween%20training%20and%20testing%2C%20often%20degrade%20in%20OOD%20scenarios.%20We%20introduce%0AMetaGS%20to%20tackle%20this%20challenge%20from%20two%20perspectives.%20First%2C%20we%20propose%20a%0Ameta-learning%20approach%20to%20train%203D%20Gaussian%20splatting%2C%20which%20explicitly%0Apromotes%20learning%20generalizable%20Gaussian%20geometries%20and%20appearance%20attributes%0Aacross%20diverse%20lighting%20conditions%2C%20even%20with%20biased%20training%20data.%20Second%2C%20we%0Aembed%20fundamental%20physical%20priors%20from%20the%20Blinn-Phong%20reflection%20model%20into%0AGaussian%20splatting%2C%20which%20enhances%20the%20decoupling%20of%20shading%20components%20and%0Aleads%20to%20more%20accurate%203D%20scene%20reconstruction.%20Results%20on%20both%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20the%20effectiveness%20of%20MetaGS%20in%20challenging%20OOD%0Arelighting%20tasks%2C%20supporting%20efficient%20point-light%20relighting%20and%20generalizing%0Awell%20to%20unseen%20environment%20lighting%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20791v2&entry.124074799=Read"},
{"title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via\n  Physics-Based Appearance-Medium Decouplin", "author": "Jieyu Yuan and Yujun Li and Yuanlin Zhang and Chunle Guo and Xiongxin Tang and Ruixing Wang and Chongyi Li", "abstract": "  Novel view synthesis for underwater scene reconstruction presents unique\nchallenges due to complex light-media interactions. Optical scattering and\nabsorption in water body bring inhomogeneous medium attenuation interference\nthat disrupts conventional volume rendering assumptions of uniform propagation\nmedium. While 3D Gaussian Splatting (3DGS) offers real-time rendering\ncapabilities, it struggles with underwater inhomogeneous environments where\nscattering media introduce artifacts and inconsistent appearance. In this\nstudy, we propose a physics-based framework that disentangles object appearance\nfrom water medium effects through tailored Gaussian modeling. Our approach\nintroduces appearance embeddings, which are explicit medium representations for\nbackscatter and attenuation, enhancing scene consistency. In addition, we\npropose a distance-guided optimization strategy that leverages pseudo-depth\nmaps as supervision with depth regularization and scale penalty terms to\nimprove geometric fidelity. By integrating the proposed appearance and medium\nmodeling components via an underwater imaging model, our approach achieves both\nhigh-quality novel view synthesis and physically accurate scene restoration.\nExperiments demonstrate our significant improvements in rendering quality and\nrestoration accuracy over existing methods. The project page is available at\n\\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR\n", "link": "http://arxiv.org/abs/2505.21238v1", "date": "2025-05-27", "relevancy": 3.0582, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6188}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.612}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-UIR%3A%203D%20Gaussian%20for%20Underwater%203D%20Scene%20Reconstruction%20via%0A%20%20Physics-Based%20Appearance-Medium%20Decouplin&body=Title%3A%203D-UIR%3A%203D%20Gaussian%20for%20Underwater%203D%20Scene%20Reconstruction%20via%0A%20%20Physics-Based%20Appearance-Medium%20Decouplin%0AAuthor%3A%20Jieyu%20Yuan%20and%20Yujun%20Li%20and%20Yuanlin%20Zhang%20and%20Chunle%20Guo%20and%20Xiongxin%20Tang%20and%20Ruixing%20Wang%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20for%20underwater%20scene%20reconstruction%20presents%20unique%0Achallenges%20due%20to%20complex%20light-media%20interactions.%20Optical%20scattering%20and%0Aabsorption%20in%20water%20body%20bring%20inhomogeneous%20medium%20attenuation%20interference%0Athat%20disrupts%20conventional%20volume%20rendering%20assumptions%20of%20uniform%20propagation%0Amedium.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%20real-time%20rendering%0Acapabilities%2C%20it%20struggles%20with%20underwater%20inhomogeneous%20environments%20where%0Ascattering%20media%20introduce%20artifacts%20and%20inconsistent%20appearance.%20In%20this%0Astudy%2C%20we%20propose%20a%20physics-based%20framework%20that%20disentangles%20object%20appearance%0Afrom%20water%20medium%20effects%20through%20tailored%20Gaussian%20modeling.%20Our%20approach%0Aintroduces%20appearance%20embeddings%2C%20which%20are%20explicit%20medium%20representations%20for%0Abackscatter%20and%20attenuation%2C%20enhancing%20scene%20consistency.%20In%20addition%2C%20we%0Apropose%20a%20distance-guided%20optimization%20strategy%20that%20leverages%20pseudo-depth%0Amaps%20as%20supervision%20with%20depth%20regularization%20and%20scale%20penalty%20terms%20to%0Aimprove%20geometric%20fidelity.%20By%20integrating%20the%20proposed%20appearance%20and%20medium%0Amodeling%20components%20via%20an%20underwater%20imaging%20model%2C%20our%20approach%20achieves%20both%0Ahigh-quality%20novel%20view%20synthesis%20and%20physically%20accurate%20scene%20restoration.%0AExperiments%20demonstrate%20our%20significant%20improvements%20in%20rendering%20quality%20and%0Arestoration%20accuracy%20over%20existing%20methods.%20The%20project%20page%20is%20available%20at%0A%5Chref%7Bhttps%3A//bilityniu.github.io/3D-UIR%7D%7Bhttps%3A//bilityniu.github.io/3D-UIR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-UIR%253A%25203D%2520Gaussian%2520for%2520Underwater%25203D%2520Scene%2520Reconstruction%2520via%250A%2520%2520Physics-Based%2520Appearance-Medium%2520Decouplin%26entry.906535625%3DJieyu%2520Yuan%2520and%2520Yujun%2520Li%2520and%2520Yuanlin%2520Zhang%2520and%2520Chunle%2520Guo%2520and%2520Xiongxin%2520Tang%2520and%2520Ruixing%2520Wang%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520for%2520underwater%2520scene%2520reconstruction%2520presents%2520unique%250Achallenges%2520due%2520to%2520complex%2520light-media%2520interactions.%2520Optical%2520scattering%2520and%250Aabsorption%2520in%2520water%2520body%2520bring%2520inhomogeneous%2520medium%2520attenuation%2520interference%250Athat%2520disrupts%2520conventional%2520volume%2520rendering%2520assumptions%2520of%2520uniform%2520propagation%250Amedium.%2520While%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520offers%2520real-time%2520rendering%250Acapabilities%252C%2520it%2520struggles%2520with%2520underwater%2520inhomogeneous%2520environments%2520where%250Ascattering%2520media%2520introduce%2520artifacts%2520and%2520inconsistent%2520appearance.%2520In%2520this%250Astudy%252C%2520we%2520propose%2520a%2520physics-based%2520framework%2520that%2520disentangles%2520object%2520appearance%250Afrom%2520water%2520medium%2520effects%2520through%2520tailored%2520Gaussian%2520modeling.%2520Our%2520approach%250Aintroduces%2520appearance%2520embeddings%252C%2520which%2520are%2520explicit%2520medium%2520representations%2520for%250Abackscatter%2520and%2520attenuation%252C%2520enhancing%2520scene%2520consistency.%2520In%2520addition%252C%2520we%250Apropose%2520a%2520distance-guided%2520optimization%2520strategy%2520that%2520leverages%2520pseudo-depth%250Amaps%2520as%2520supervision%2520with%2520depth%2520regularization%2520and%2520scale%2520penalty%2520terms%2520to%250Aimprove%2520geometric%2520fidelity.%2520By%2520integrating%2520the%2520proposed%2520appearance%2520and%2520medium%250Amodeling%2520components%2520via%2520an%2520underwater%2520imaging%2520model%252C%2520our%2520approach%2520achieves%2520both%250Ahigh-quality%2520novel%2520view%2520synthesis%2520and%2520physically%2520accurate%2520scene%2520restoration.%250AExperiments%2520demonstrate%2520our%2520significant%2520improvements%2520in%2520rendering%2520quality%2520and%250Arestoration%2520accuracy%2520over%2520existing%2520methods.%2520The%2520project%2520page%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//bilityniu.github.io/3D-UIR%257D%257Bhttps%253A//bilityniu.github.io/3D-UIR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-UIR%3A%203D%20Gaussian%20for%20Underwater%203D%20Scene%20Reconstruction%20via%0A%20%20Physics-Based%20Appearance-Medium%20Decouplin&entry.906535625=Jieyu%20Yuan%20and%20Yujun%20Li%20and%20Yuanlin%20Zhang%20and%20Chunle%20Guo%20and%20Xiongxin%20Tang%20and%20Ruixing%20Wang%20and%20Chongyi%20Li&entry.1292438233=%20%20Novel%20view%20synthesis%20for%20underwater%20scene%20reconstruction%20presents%20unique%0Achallenges%20due%20to%20complex%20light-media%20interactions.%20Optical%20scattering%20and%0Aabsorption%20in%20water%20body%20bring%20inhomogeneous%20medium%20attenuation%20interference%0Athat%20disrupts%20conventional%20volume%20rendering%20assumptions%20of%20uniform%20propagation%0Amedium.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%20real-time%20rendering%0Acapabilities%2C%20it%20struggles%20with%20underwater%20inhomogeneous%20environments%20where%0Ascattering%20media%20introduce%20artifacts%20and%20inconsistent%20appearance.%20In%20this%0Astudy%2C%20we%20propose%20a%20physics-based%20framework%20that%20disentangles%20object%20appearance%0Afrom%20water%20medium%20effects%20through%20tailored%20Gaussian%20modeling.%20Our%20approach%0Aintroduces%20appearance%20embeddings%2C%20which%20are%20explicit%20medium%20representations%20for%0Abackscatter%20and%20attenuation%2C%20enhancing%20scene%20consistency.%20In%20addition%2C%20we%0Apropose%20a%20distance-guided%20optimization%20strategy%20that%20leverages%20pseudo-depth%0Amaps%20as%20supervision%20with%20depth%20regularization%20and%20scale%20penalty%20terms%20to%0Aimprove%20geometric%20fidelity.%20By%20integrating%20the%20proposed%20appearance%20and%20medium%0Amodeling%20components%20via%20an%20underwater%20imaging%20model%2C%20our%20approach%20achieves%20both%0Ahigh-quality%20novel%20view%20synthesis%20and%20physically%20accurate%20scene%20restoration.%0AExperiments%20demonstrate%20our%20significant%20improvements%20in%20rendering%20quality%20and%0Arestoration%20accuracy%20over%20existing%20methods.%20The%20project%20page%20is%20available%20at%0A%5Chref%7Bhttps%3A//bilityniu.github.io/3D-UIR%7D%7Bhttps%3A//bilityniu.github.io/3D-UIR%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21238v1&entry.124074799=Read"},
{"title": "Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium\n  Representation", "author": "Changguanng Wu and Jiangxin Dong and Chengjian Li and Jinhui Tang", "abstract": "  We present Plenodium (plenoptic medium), an effective and efficient 3D\nrepresentation framework capable of jointly modeling both objects and\nparticipating media. In contrast to existing medium representations that rely\nsolely on view-dependent modeling, our novel plenoptic medium representation\nincorporates both directional and positional information through spherical\nharmonics encoding, enabling highly accurate underwater scene reconstruction.\nTo address the initialization challenge in degraded underwater environments, we\npropose the pseudo-depth Gaussian complementation to augment COLMAP-derived\npoint clouds with robust depth priors. In addition, a depth ranking regularized\nloss is developed to optimize the geometry of the scene and improve the ordinal\nconsistency of the depth maps. Extensive experiments on real-world underwater\ndatasets demonstrate that our method achieves significant improvements in 3D\nreconstruction. Furthermore, we conduct a simulated dataset with ground truth\nand the controllable scattering medium to demonstrate the restoration\ncapability of our method in underwater scenarios. Our code and dataset are\navailable at https://plenodium.github.io/.\n", "link": "http://arxiv.org/abs/2505.21258v1", "date": "2025-05-27", "relevancy": 2.917, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5905}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plenodium%3A%20UnderWater%203D%20Scene%20Reconstruction%20with%20Plenoptic%20Medium%0A%20%20Representation&body=Title%3A%20Plenodium%3A%20UnderWater%203D%20Scene%20Reconstruction%20with%20Plenoptic%20Medium%0A%20%20Representation%0AAuthor%3A%20Changguanng%20Wu%20and%20Jiangxin%20Dong%20and%20Chengjian%20Li%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20We%20present%20Plenodium%20%28plenoptic%20medium%29%2C%20an%20effective%20and%20efficient%203D%0Arepresentation%20framework%20capable%20of%20jointly%20modeling%20both%20objects%20and%0Aparticipating%20media.%20In%20contrast%20to%20existing%20medium%20representations%20that%20rely%0Asolely%20on%20view-dependent%20modeling%2C%20our%20novel%20plenoptic%20medium%20representation%0Aincorporates%20both%20directional%20and%20positional%20information%20through%20spherical%0Aharmonics%20encoding%2C%20enabling%20highly%20accurate%20underwater%20scene%20reconstruction.%0ATo%20address%20the%20initialization%20challenge%20in%20degraded%20underwater%20environments%2C%20we%0Apropose%20the%20pseudo-depth%20Gaussian%20complementation%20to%20augment%20COLMAP-derived%0Apoint%20clouds%20with%20robust%20depth%20priors.%20In%20addition%2C%20a%20depth%20ranking%20regularized%0Aloss%20is%20developed%20to%20optimize%20the%20geometry%20of%20the%20scene%20and%20improve%20the%20ordinal%0Aconsistency%20of%20the%20depth%20maps.%20Extensive%20experiments%20on%20real-world%20underwater%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20significant%20improvements%20in%203D%0Areconstruction.%20Furthermore%2C%20we%20conduct%20a%20simulated%20dataset%20with%20ground%20truth%0Aand%20the%20controllable%20scattering%20medium%20to%20demonstrate%20the%20restoration%0Acapability%20of%20our%20method%20in%20underwater%20scenarios.%20Our%20code%20and%20dataset%20are%0Aavailable%20at%20https%3A//plenodium.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlenodium%253A%2520UnderWater%25203D%2520Scene%2520Reconstruction%2520with%2520Plenoptic%2520Medium%250A%2520%2520Representation%26entry.906535625%3DChangguanng%2520Wu%2520and%2520Jiangxin%2520Dong%2520and%2520Chengjian%2520Li%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520We%2520present%2520Plenodium%2520%2528plenoptic%2520medium%2529%252C%2520an%2520effective%2520and%2520efficient%25203D%250Arepresentation%2520framework%2520capable%2520of%2520jointly%2520modeling%2520both%2520objects%2520and%250Aparticipating%2520media.%2520In%2520contrast%2520to%2520existing%2520medium%2520representations%2520that%2520rely%250Asolely%2520on%2520view-dependent%2520modeling%252C%2520our%2520novel%2520plenoptic%2520medium%2520representation%250Aincorporates%2520both%2520directional%2520and%2520positional%2520information%2520through%2520spherical%250Aharmonics%2520encoding%252C%2520enabling%2520highly%2520accurate%2520underwater%2520scene%2520reconstruction.%250ATo%2520address%2520the%2520initialization%2520challenge%2520in%2520degraded%2520underwater%2520environments%252C%2520we%250Apropose%2520the%2520pseudo-depth%2520Gaussian%2520complementation%2520to%2520augment%2520COLMAP-derived%250Apoint%2520clouds%2520with%2520robust%2520depth%2520priors.%2520In%2520addition%252C%2520a%2520depth%2520ranking%2520regularized%250Aloss%2520is%2520developed%2520to%2520optimize%2520the%2520geometry%2520of%2520the%2520scene%2520and%2520improve%2520the%2520ordinal%250Aconsistency%2520of%2520the%2520depth%2520maps.%2520Extensive%2520experiments%2520on%2520real-world%2520underwater%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520significant%2520improvements%2520in%25203D%250Areconstruction.%2520Furthermore%252C%2520we%2520conduct%2520a%2520simulated%2520dataset%2520with%2520ground%2520truth%250Aand%2520the%2520controllable%2520scattering%2520medium%2520to%2520demonstrate%2520the%2520restoration%250Acapability%2520of%2520our%2520method%2520in%2520underwater%2520scenarios.%2520Our%2520code%2520and%2520dataset%2520are%250Aavailable%2520at%2520https%253A//plenodium.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plenodium%3A%20UnderWater%203D%20Scene%20Reconstruction%20with%20Plenoptic%20Medium%0A%20%20Representation&entry.906535625=Changguanng%20Wu%20and%20Jiangxin%20Dong%20and%20Chengjian%20Li%20and%20Jinhui%20Tang&entry.1292438233=%20%20We%20present%20Plenodium%20%28plenoptic%20medium%29%2C%20an%20effective%20and%20efficient%203D%0Arepresentation%20framework%20capable%20of%20jointly%20modeling%20both%20objects%20and%0Aparticipating%20media.%20In%20contrast%20to%20existing%20medium%20representations%20that%20rely%0Asolely%20on%20view-dependent%20modeling%2C%20our%20novel%20plenoptic%20medium%20representation%0Aincorporates%20both%20directional%20and%20positional%20information%20through%20spherical%0Aharmonics%20encoding%2C%20enabling%20highly%20accurate%20underwater%20scene%20reconstruction.%0ATo%20address%20the%20initialization%20challenge%20in%20degraded%20underwater%20environments%2C%20we%0Apropose%20the%20pseudo-depth%20Gaussian%20complementation%20to%20augment%20COLMAP-derived%0Apoint%20clouds%20with%20robust%20depth%20priors.%20In%20addition%2C%20a%20depth%20ranking%20regularized%0Aloss%20is%20developed%20to%20optimize%20the%20geometry%20of%20the%20scene%20and%20improve%20the%20ordinal%0Aconsistency%20of%20the%20depth%20maps.%20Extensive%20experiments%20on%20real-world%20underwater%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20significant%20improvements%20in%203D%0Areconstruction.%20Furthermore%2C%20we%20conduct%20a%20simulated%20dataset%20with%20ground%20truth%0Aand%20the%20controllable%20scattering%20medium%20to%20demonstrate%20the%20restoration%0Acapability%20of%20our%20method%20in%20underwater%20scenarios.%20Our%20code%20and%20dataset%20are%0Aavailable%20at%20https%3A//plenodium.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21258v1&entry.124074799=Read"},
{"title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents", "author": "Han Xiao and Guozhi Wang and Yuxiang Chai and Zimu Lu and Weifeng Lin and Hao He and Lue Fan and Liuyang Bian and Rui Hu and Liang Liu and Shuai Ren and Yafei Wen and Xiaoxin Chen and Aojun Zhou and Hongsheng Li", "abstract": "  In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.\n", "link": "http://arxiv.org/abs/2505.21496v1", "date": "2025-05-27", "relevancy": 2.9163, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5778}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-Genie%3A%20A%20Self-Improving%20Approach%20for%20Iteratively%20Boosting%20MLLM-based%0A%20%20Mobile%20GUI%20Agents&body=Title%3A%20UI-Genie%3A%20A%20Self-Improving%20Approach%20for%20Iteratively%20Boosting%20MLLM-based%0A%20%20Mobile%20GUI%20Agents%0AAuthor%3A%20Han%20Xiao%20and%20Guozhi%20Wang%20and%20Yuxiang%20Chai%20and%20Zimu%20Lu%20and%20Weifeng%20Lin%20and%20Hao%20He%20and%20Lue%20Fan%20and%20Liuyang%20Bian%20and%20Rui%20Hu%20and%20Liang%20Liu%20and%20Shuai%20Ren%20and%20Yafei%20Wen%20and%20Xiaoxin%20Chen%20and%20Aojun%20Zhou%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20UI-Genie%2C%20a%20self-improving%20framework%20addressing%0Atwo%20key%20challenges%20in%20GUI%20agents%3A%20verification%20of%20trajectory%20outcome%20is%0Achallenging%20and%20high-quality%20training%20data%20are%20not%20scalable.%20These%20challenges%0Aare%20addressed%20by%20a%20reward%20model%20and%20a%20self-improving%20pipeline%2C%20respectively.%0AThe%20reward%20model%2C%20UI-Genie-RM%2C%20features%20an%20image-text%20interleaved%20architecture%0Athat%20efficiently%20pro-%20cesses%20historical%20context%20and%20unifies%20action-level%20and%0Atask-level%20rewards.%20To%20sup-%20port%20the%20training%20of%20UI-Genie-RM%2C%20we%20develop%0Adeliberately-designed%20data%20genera-%20tion%20strategies%20including%20rule-based%0Averification%2C%20controlled%20trajectory%20corruption%2C%20and%20hard%20negative%20mining.%20To%0Aaddress%20the%20second%20challenge%2C%20a%20self-improvement%20pipeline%20progressively%20expands%0Asolvable%20complex%20GUI%20tasks%20by%20enhancing%20both%20the%20agent%20and%20reward%20models%0Athrough%20reward-guided%20exploration%20and%20outcome%20verification%20in%20dynamic%0Aenvironments.%20For%20training%20the%20model%2C%20we%20generate%20UI-%20Genie-RM-517k%20and%0AUI-Genie-Agent-16k%2C%20establishing%20the%20first%20reward-specific%20dataset%20for%20GUI%0Aagents%20while%20demonstrating%20high-quality%20synthetic%20trajectory%20gen-%20eration%0Awithout%20manual%20annotation.%20Experimental%20results%20show%20that%20UI-Genie%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20GUI%20agent%20benchmarks%20with%20three%0Agenerations%20of%20data-model%20self-improvement.%20We%20open-source%20our%20complete%0Aframework%20implementation%20and%20generated%20datasets%20to%20facilitate%20further%20research%0Ain%20https%3A//github.com/Euphoria16/UI-Genie.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-Genie%253A%2520A%2520Self-Improving%2520Approach%2520for%2520Iteratively%2520Boosting%2520MLLM-based%250A%2520%2520Mobile%2520GUI%2520Agents%26entry.906535625%3DHan%2520Xiao%2520and%2520Guozhi%2520Wang%2520and%2520Yuxiang%2520Chai%2520and%2520Zimu%2520Lu%2520and%2520Weifeng%2520Lin%2520and%2520Hao%2520He%2520and%2520Lue%2520Fan%2520and%2520Liuyang%2520Bian%2520and%2520Rui%2520Hu%2520and%2520Liang%2520Liu%2520and%2520Shuai%2520Ren%2520and%2520Yafei%2520Wen%2520and%2520Xiaoxin%2520Chen%2520and%2520Aojun%2520Zhou%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520UI-Genie%252C%2520a%2520self-improving%2520framework%2520addressing%250Atwo%2520key%2520challenges%2520in%2520GUI%2520agents%253A%2520verification%2520of%2520trajectory%2520outcome%2520is%250Achallenging%2520and%2520high-quality%2520training%2520data%2520are%2520not%2520scalable.%2520These%2520challenges%250Aare%2520addressed%2520by%2520a%2520reward%2520model%2520and%2520a%2520self-improving%2520pipeline%252C%2520respectively.%250AThe%2520reward%2520model%252C%2520UI-Genie-RM%252C%2520features%2520an%2520image-text%2520interleaved%2520architecture%250Athat%2520efficiently%2520pro-%2520cesses%2520historical%2520context%2520and%2520unifies%2520action-level%2520and%250Atask-level%2520rewards.%2520To%2520sup-%2520port%2520the%2520training%2520of%2520UI-Genie-RM%252C%2520we%2520develop%250Adeliberately-designed%2520data%2520genera-%2520tion%2520strategies%2520including%2520rule-based%250Averification%252C%2520controlled%2520trajectory%2520corruption%252C%2520and%2520hard%2520negative%2520mining.%2520To%250Aaddress%2520the%2520second%2520challenge%252C%2520a%2520self-improvement%2520pipeline%2520progressively%2520expands%250Asolvable%2520complex%2520GUI%2520tasks%2520by%2520enhancing%2520both%2520the%2520agent%2520and%2520reward%2520models%250Athrough%2520reward-guided%2520exploration%2520and%2520outcome%2520verification%2520in%2520dynamic%250Aenvironments.%2520For%2520training%2520the%2520model%252C%2520we%2520generate%2520UI-%2520Genie-RM-517k%2520and%250AUI-Genie-Agent-16k%252C%2520establishing%2520the%2520first%2520reward-specific%2520dataset%2520for%2520GUI%250Aagents%2520while%2520demonstrating%2520high-quality%2520synthetic%2520trajectory%2520gen-%2520eration%250Awithout%2520manual%2520annotation.%2520Experimental%2520results%2520show%2520that%2520UI-Genie%2520achieves%250Astate-of-the-art%2520performance%2520across%2520multiple%2520GUI%2520agent%2520benchmarks%2520with%2520three%250Agenerations%2520of%2520data-model%2520self-improvement.%2520We%2520open-source%2520our%2520complete%250Aframework%2520implementation%2520and%2520generated%2520datasets%2520to%2520facilitate%2520further%2520research%250Ain%2520https%253A//github.com/Euphoria16/UI-Genie.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-Genie%3A%20A%20Self-Improving%20Approach%20for%20Iteratively%20Boosting%20MLLM-based%0A%20%20Mobile%20GUI%20Agents&entry.906535625=Han%20Xiao%20and%20Guozhi%20Wang%20and%20Yuxiang%20Chai%20and%20Zimu%20Lu%20and%20Weifeng%20Lin%20and%20Hao%20He%20and%20Lue%20Fan%20and%20Liuyang%20Bian%20and%20Rui%20Hu%20and%20Liang%20Liu%20and%20Shuai%20Ren%20and%20Yafei%20Wen%20and%20Xiaoxin%20Chen%20and%20Aojun%20Zhou%20and%20Hongsheng%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20UI-Genie%2C%20a%20self-improving%20framework%20addressing%0Atwo%20key%20challenges%20in%20GUI%20agents%3A%20verification%20of%20trajectory%20outcome%20is%0Achallenging%20and%20high-quality%20training%20data%20are%20not%20scalable.%20These%20challenges%0Aare%20addressed%20by%20a%20reward%20model%20and%20a%20self-improving%20pipeline%2C%20respectively.%0AThe%20reward%20model%2C%20UI-Genie-RM%2C%20features%20an%20image-text%20interleaved%20architecture%0Athat%20efficiently%20pro-%20cesses%20historical%20context%20and%20unifies%20action-level%20and%0Atask-level%20rewards.%20To%20sup-%20port%20the%20training%20of%20UI-Genie-RM%2C%20we%20develop%0Adeliberately-designed%20data%20genera-%20tion%20strategies%20including%20rule-based%0Averification%2C%20controlled%20trajectory%20corruption%2C%20and%20hard%20negative%20mining.%20To%0Aaddress%20the%20second%20challenge%2C%20a%20self-improvement%20pipeline%20progressively%20expands%0Asolvable%20complex%20GUI%20tasks%20by%20enhancing%20both%20the%20agent%20and%20reward%20models%0Athrough%20reward-guided%20exploration%20and%20outcome%20verification%20in%20dynamic%0Aenvironments.%20For%20training%20the%20model%2C%20we%20generate%20UI-%20Genie-RM-517k%20and%0AUI-Genie-Agent-16k%2C%20establishing%20the%20first%20reward-specific%20dataset%20for%20GUI%0Aagents%20while%20demonstrating%20high-quality%20synthetic%20trajectory%20gen-%20eration%0Awithout%20manual%20annotation.%20Experimental%20results%20show%20that%20UI-Genie%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20GUI%20agent%20benchmarks%20with%20three%0Agenerations%20of%20data-model%20self-improvement.%20We%20open-source%20our%20complete%0Aframework%20implementation%20and%20generated%20datasets%20to%20facilitate%20further%20research%0Ain%20https%3A//github.com/Euphoria16/UI-Genie.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21496v1&entry.124074799=Read"},
{"title": "FastFace: Tuning Identity Preservation in Distilled Diffusion via\n  Guidance and Attention", "author": "Sergey Karpukhin and Vadim Titov and Andrey Kuznetsov and Aibek Alanov", "abstract": "  In latest years plethora of identity-preserving adapters for a personalized\ngeneration with diffusion models have been released. Their main disadvantage is\nthat they are dominantly trained jointly with base diffusion models, which\nsuffer from slow multi-step inference. This work aims to tackle the challenge\nof training-free adaptation of pretrained ID-adapters to diffusion models\naccelerated via distillation - through careful re-design of classifier-free\nguidance for few-step stylistic generation and attention manipulation\nmechanisms in decoupled blocks to improve identity similarity and fidelity, we\npropose universal FastFace framework. Additionally, we develop a disentangled\npublic evaluation protocol for id-preserving adapters.\n", "link": "http://arxiv.org/abs/2505.21144v1", "date": "2025-05-27", "relevancy": 2.9072, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5992}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.587}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastFace%3A%20Tuning%20Identity%20Preservation%20in%20Distilled%20Diffusion%20via%0A%20%20Guidance%20and%20Attention&body=Title%3A%20FastFace%3A%20Tuning%20Identity%20Preservation%20in%20Distilled%20Diffusion%20via%0A%20%20Guidance%20and%20Attention%0AAuthor%3A%20Sergey%20Karpukhin%20and%20Vadim%20Titov%20and%20Andrey%20Kuznetsov%20and%20Aibek%20Alanov%0AAbstract%3A%20%20%20In%20latest%20years%20plethora%20of%20identity-preserving%20adapters%20for%20a%20personalized%0Ageneration%20with%20diffusion%20models%20have%20been%20released.%20Their%20main%20disadvantage%20is%0Athat%20they%20are%20dominantly%20trained%20jointly%20with%20base%20diffusion%20models%2C%20which%0Asuffer%20from%20slow%20multi-step%20inference.%20This%20work%20aims%20to%20tackle%20the%20challenge%0Aof%20training-free%20adaptation%20of%20pretrained%20ID-adapters%20to%20diffusion%20models%0Aaccelerated%20via%20distillation%20-%20through%20careful%20re-design%20of%20classifier-free%0Aguidance%20for%20few-step%20stylistic%20generation%20and%20attention%20manipulation%0Amechanisms%20in%20decoupled%20blocks%20to%20improve%20identity%20similarity%20and%20fidelity%2C%20we%0Apropose%20universal%20FastFace%20framework.%20Additionally%2C%20we%20develop%20a%20disentangled%0Apublic%20evaluation%20protocol%20for%20id-preserving%20adapters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastFace%253A%2520Tuning%2520Identity%2520Preservation%2520in%2520Distilled%2520Diffusion%2520via%250A%2520%2520Guidance%2520and%2520Attention%26entry.906535625%3DSergey%2520Karpukhin%2520and%2520Vadim%2520Titov%2520and%2520Andrey%2520Kuznetsov%2520and%2520Aibek%2520Alanov%26entry.1292438233%3D%2520%2520In%2520latest%2520years%2520plethora%2520of%2520identity-preserving%2520adapters%2520for%2520a%2520personalized%250Ageneration%2520with%2520diffusion%2520models%2520have%2520been%2520released.%2520Their%2520main%2520disadvantage%2520is%250Athat%2520they%2520are%2520dominantly%2520trained%2520jointly%2520with%2520base%2520diffusion%2520models%252C%2520which%250Asuffer%2520from%2520slow%2520multi-step%2520inference.%2520This%2520work%2520aims%2520to%2520tackle%2520the%2520challenge%250Aof%2520training-free%2520adaptation%2520of%2520pretrained%2520ID-adapters%2520to%2520diffusion%2520models%250Aaccelerated%2520via%2520distillation%2520-%2520through%2520careful%2520re-design%2520of%2520classifier-free%250Aguidance%2520for%2520few-step%2520stylistic%2520generation%2520and%2520attention%2520manipulation%250Amechanisms%2520in%2520decoupled%2520blocks%2520to%2520improve%2520identity%2520similarity%2520and%2520fidelity%252C%2520we%250Apropose%2520universal%2520FastFace%2520framework.%2520Additionally%252C%2520we%2520develop%2520a%2520disentangled%250Apublic%2520evaluation%2520protocol%2520for%2520id-preserving%2520adapters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastFace%3A%20Tuning%20Identity%20Preservation%20in%20Distilled%20Diffusion%20via%0A%20%20Guidance%20and%20Attention&entry.906535625=Sergey%20Karpukhin%20and%20Vadim%20Titov%20and%20Andrey%20Kuznetsov%20and%20Aibek%20Alanov&entry.1292438233=%20%20In%20latest%20years%20plethora%20of%20identity-preserving%20adapters%20for%20a%20personalized%0Ageneration%20with%20diffusion%20models%20have%20been%20released.%20Their%20main%20disadvantage%20is%0Athat%20they%20are%20dominantly%20trained%20jointly%20with%20base%20diffusion%20models%2C%20which%0Asuffer%20from%20slow%20multi-step%20inference.%20This%20work%20aims%20to%20tackle%20the%20challenge%0Aof%20training-free%20adaptation%20of%20pretrained%20ID-adapters%20to%20diffusion%20models%0Aaccelerated%20via%20distillation%20-%20through%20careful%20re-design%20of%20classifier-free%0Aguidance%20for%20few-step%20stylistic%20generation%20and%20attention%20manipulation%0Amechanisms%20in%20decoupled%20blocks%20to%20improve%20identity%20similarity%20and%20fidelity%2C%20we%0Apropose%20universal%20FastFace%20framework.%20Additionally%2C%20we%20develop%20a%20disentangled%0Apublic%20evaluation%20protocol%20for%20id-preserving%20adapters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21144v1&entry.124074799=Read"},
{"title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and\n  View-dependent Visibility", "author": "Yidi Li and Jun Xiao and Zhengda Lu and Yiqun Wang and Haiyong Jiang", "abstract": "  This work presents a novel text-to-vector graphics generation approach,\nDream3DVG, allowing for arbitrary viewpoint viewing, progressive detail\noptimization, and view-dependent occlusion awareness. Our approach is a\ndual-branch optimization framework, consisting of an auxiliary 3D Gaussian\nSplatting optimization branch and a 3D vector graphics optimization branch. The\nintroduced 3DGS branch can bridge the domain gaps between text prompts and\nvector graphics with more consistent guidance. Moreover, 3DGS allows for\nprogressive detail control by scheduling classifier-free guidance, facilitating\nguiding vector graphics with coarse shapes at the initial stages and finer\ndetails at later stages. We also improve the view-dependent occlusions by\ndevising a visibility-awareness rendering module. Extensive results on 3D\nsketches and 3D iconographies, demonstrate the superiority of the method on\ndifferent abstraction levels of details, cross-view consistency, and\nocclusion-aware stroke culling.\n", "link": "http://arxiv.org/abs/2505.21377v1", "date": "2025-05-27", "relevancy": 2.8992, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5821}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5787}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Vector%20Graphics%20with%20Consistently%20Arbitrary%20Viewing%20and%0A%20%20View-dependent%20Visibility&body=Title%3A%20Empowering%20Vector%20Graphics%20with%20Consistently%20Arbitrary%20Viewing%20and%0A%20%20View-dependent%20Visibility%0AAuthor%3A%20Yidi%20Li%20and%20Jun%20Xiao%20and%20Zhengda%20Lu%20and%20Yiqun%20Wang%20and%20Haiyong%20Jiang%0AAbstract%3A%20%20%20This%20work%20presents%20a%20novel%20text-to-vector%20graphics%20generation%20approach%2C%0ADream3DVG%2C%20allowing%20for%20arbitrary%20viewpoint%20viewing%2C%20progressive%20detail%0Aoptimization%2C%20and%20view-dependent%20occlusion%20awareness.%20Our%20approach%20is%20a%0Adual-branch%20optimization%20framework%2C%20consisting%20of%20an%20auxiliary%203D%20Gaussian%0ASplatting%20optimization%20branch%20and%20a%203D%20vector%20graphics%20optimization%20branch.%20The%0Aintroduced%203DGS%20branch%20can%20bridge%20the%20domain%20gaps%20between%20text%20prompts%20and%0Avector%20graphics%20with%20more%20consistent%20guidance.%20Moreover%2C%203DGS%20allows%20for%0Aprogressive%20detail%20control%20by%20scheduling%20classifier-free%20guidance%2C%20facilitating%0Aguiding%20vector%20graphics%20with%20coarse%20shapes%20at%20the%20initial%20stages%20and%20finer%0Adetails%20at%20later%20stages.%20We%20also%20improve%20the%20view-dependent%20occlusions%20by%0Adevising%20a%20visibility-awareness%20rendering%20module.%20Extensive%20results%20on%203D%0Asketches%20and%203D%20iconographies%2C%20demonstrate%20the%20superiority%20of%20the%20method%20on%0Adifferent%20abstraction%20levels%20of%20details%2C%20cross-view%20consistency%2C%20and%0Aocclusion-aware%20stroke%20culling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Vector%2520Graphics%2520with%2520Consistently%2520Arbitrary%2520Viewing%2520and%250A%2520%2520View-dependent%2520Visibility%26entry.906535625%3DYidi%2520Li%2520and%2520Jun%2520Xiao%2520and%2520Zhengda%2520Lu%2520and%2520Yiqun%2520Wang%2520and%2520Haiyong%2520Jiang%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520novel%2520text-to-vector%2520graphics%2520generation%2520approach%252C%250ADream3DVG%252C%2520allowing%2520for%2520arbitrary%2520viewpoint%2520viewing%252C%2520progressive%2520detail%250Aoptimization%252C%2520and%2520view-dependent%2520occlusion%2520awareness.%2520Our%2520approach%2520is%2520a%250Adual-branch%2520optimization%2520framework%252C%2520consisting%2520of%2520an%2520auxiliary%25203D%2520Gaussian%250ASplatting%2520optimization%2520branch%2520and%2520a%25203D%2520vector%2520graphics%2520optimization%2520branch.%2520The%250Aintroduced%25203DGS%2520branch%2520can%2520bridge%2520the%2520domain%2520gaps%2520between%2520text%2520prompts%2520and%250Avector%2520graphics%2520with%2520more%2520consistent%2520guidance.%2520Moreover%252C%25203DGS%2520allows%2520for%250Aprogressive%2520detail%2520control%2520by%2520scheduling%2520classifier-free%2520guidance%252C%2520facilitating%250Aguiding%2520vector%2520graphics%2520with%2520coarse%2520shapes%2520at%2520the%2520initial%2520stages%2520and%2520finer%250Adetails%2520at%2520later%2520stages.%2520We%2520also%2520improve%2520the%2520view-dependent%2520occlusions%2520by%250Adevising%2520a%2520visibility-awareness%2520rendering%2520module.%2520Extensive%2520results%2520on%25203D%250Asketches%2520and%25203D%2520iconographies%252C%2520demonstrate%2520the%2520superiority%2520of%2520the%2520method%2520on%250Adifferent%2520abstraction%2520levels%2520of%2520details%252C%2520cross-view%2520consistency%252C%2520and%250Aocclusion-aware%2520stroke%2520culling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Vector%20Graphics%20with%20Consistently%20Arbitrary%20Viewing%20and%0A%20%20View-dependent%20Visibility&entry.906535625=Yidi%20Li%20and%20Jun%20Xiao%20and%20Zhengda%20Lu%20and%20Yiqun%20Wang%20and%20Haiyong%20Jiang&entry.1292438233=%20%20This%20work%20presents%20a%20novel%20text-to-vector%20graphics%20generation%20approach%2C%0ADream3DVG%2C%20allowing%20for%20arbitrary%20viewpoint%20viewing%2C%20progressive%20detail%0Aoptimization%2C%20and%20view-dependent%20occlusion%20awareness.%20Our%20approach%20is%20a%0Adual-branch%20optimization%20framework%2C%20consisting%20of%20an%20auxiliary%203D%20Gaussian%0ASplatting%20optimization%20branch%20and%20a%203D%20vector%20graphics%20optimization%20branch.%20The%0Aintroduced%203DGS%20branch%20can%20bridge%20the%20domain%20gaps%20between%20text%20prompts%20and%0Avector%20graphics%20with%20more%20consistent%20guidance.%20Moreover%2C%203DGS%20allows%20for%0Aprogressive%20detail%20control%20by%20scheduling%20classifier-free%20guidance%2C%20facilitating%0Aguiding%20vector%20graphics%20with%20coarse%20shapes%20at%20the%20initial%20stages%20and%20finer%0Adetails%20at%20later%20stages.%20We%20also%20improve%20the%20view-dependent%20occlusions%20by%0Adevising%20a%20visibility-awareness%20rendering%20module.%20Extensive%20results%20on%203D%0Asketches%20and%203D%20iconographies%2C%20demonstrate%20the%20superiority%20of%20the%20method%20on%0Adifferent%20abstraction%20levels%20of%20details%2C%20cross-view%20consistency%2C%20and%0Aocclusion-aware%20stroke%20culling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21377v1&entry.124074799=Read"},
{"title": "ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution\n  Adaptation in Vision-Language Models", "author": "Bozhou Li and Wentao Zhang", "abstract": "  Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)\nperformance is to encode both the high-resolution version and the thumbnail of\nan image simultaneously. While effective, this method generates a large number\nof image tokens. When combined with the widely used Rotary Position Embedding\n(RoPE), its long-term decay property hinders the interaction between\nhigh-resolution tokens and thumbnail tokens, as well as between text and image.\nTo address these issues, we propose ID-Align, which alleviates these problems\nby reordering position IDs. In this method, high-resolution tokens inherit IDs\nfrom their corresponding thumbnail token while constraining the overexpansion\nof positional indices. Our experiments conducted within the LLaVA-Next\nframework demonstrate that ID-Align achieves significant improvements,\nincluding a 6.09% enhancement on MMBench's relation reasoning tasks and notable\ngains across multiple benchmarks. Our code is available at the following link:\nhttps://github.com/zooblastlbz/ID-Align.\n", "link": "http://arxiv.org/abs/2505.21465v1", "date": "2025-05-27", "relevancy": 2.8983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ID-Align%3A%20RoPE-Conscious%20Position%20Remapping%20for%20Dynamic%20High-Resolution%0A%20%20Adaptation%20in%20Vision-Language%20Models&body=Title%3A%20ID-Align%3A%20RoPE-Conscious%20Position%20Remapping%20for%20Dynamic%20High-Resolution%0A%20%20Adaptation%20in%20Vision-Language%20Models%0AAuthor%3A%20Bozhou%20Li%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Currently%2C%20a%20prevalent%20approach%20for%20enhancing%20Vision-Language%20Models%20%28VLMs%29%0Aperformance%20is%20to%20encode%20both%20the%20high-resolution%20version%20and%20the%20thumbnail%20of%0Aan%20image%20simultaneously.%20While%20effective%2C%20this%20method%20generates%20a%20large%20number%0Aof%20image%20tokens.%20When%20combined%20with%20the%20widely%20used%20Rotary%20Position%20Embedding%0A%28RoPE%29%2C%20its%20long-term%20decay%20property%20hinders%20the%20interaction%20between%0Ahigh-resolution%20tokens%20and%20thumbnail%20tokens%2C%20as%20well%20as%20between%20text%20and%20image.%0ATo%20address%20these%20issues%2C%20we%20propose%20ID-Align%2C%20which%20alleviates%20these%20problems%0Aby%20reordering%20position%20IDs.%20In%20this%20method%2C%20high-resolution%20tokens%20inherit%20IDs%0Afrom%20their%20corresponding%20thumbnail%20token%20while%20constraining%20the%20overexpansion%0Aof%20positional%20indices.%20Our%20experiments%20conducted%20within%20the%20LLaVA-Next%0Aframework%20demonstrate%20that%20ID-Align%20achieves%20significant%20improvements%2C%0Aincluding%20a%206.09%25%20enhancement%20on%20MMBench%27s%20relation%20reasoning%20tasks%20and%20notable%0Agains%20across%20multiple%20benchmarks.%20Our%20code%20is%20available%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/zooblastlbz/ID-Align.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DID-Align%253A%2520RoPE-Conscious%2520Position%2520Remapping%2520for%2520Dynamic%2520High-Resolution%250A%2520%2520Adaptation%2520in%2520Vision-Language%2520Models%26entry.906535625%3DBozhou%2520Li%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Currently%252C%2520a%2520prevalent%2520approach%2520for%2520enhancing%2520Vision-Language%2520Models%2520%2528VLMs%2529%250Aperformance%2520is%2520to%2520encode%2520both%2520the%2520high-resolution%2520version%2520and%2520the%2520thumbnail%2520of%250Aan%2520image%2520simultaneously.%2520While%2520effective%252C%2520this%2520method%2520generates%2520a%2520large%2520number%250Aof%2520image%2520tokens.%2520When%2520combined%2520with%2520the%2520widely%2520used%2520Rotary%2520Position%2520Embedding%250A%2528RoPE%2529%252C%2520its%2520long-term%2520decay%2520property%2520hinders%2520the%2520interaction%2520between%250Ahigh-resolution%2520tokens%2520and%2520thumbnail%2520tokens%252C%2520as%2520well%2520as%2520between%2520text%2520and%2520image.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520ID-Align%252C%2520which%2520alleviates%2520these%2520problems%250Aby%2520reordering%2520position%2520IDs.%2520In%2520this%2520method%252C%2520high-resolution%2520tokens%2520inherit%2520IDs%250Afrom%2520their%2520corresponding%2520thumbnail%2520token%2520while%2520constraining%2520the%2520overexpansion%250Aof%2520positional%2520indices.%2520Our%2520experiments%2520conducted%2520within%2520the%2520LLaVA-Next%250Aframework%2520demonstrate%2520that%2520ID-Align%2520achieves%2520significant%2520improvements%252C%250Aincluding%2520a%25206.09%2525%2520enhancement%2520on%2520MMBench%2527s%2520relation%2520reasoning%2520tasks%2520and%2520notable%250Agains%2520across%2520multiple%2520benchmarks.%2520Our%2520code%2520is%2520available%2520at%2520the%2520following%2520link%253A%250Ahttps%253A//github.com/zooblastlbz/ID-Align.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ID-Align%3A%20RoPE-Conscious%20Position%20Remapping%20for%20Dynamic%20High-Resolution%0A%20%20Adaptation%20in%20Vision-Language%20Models&entry.906535625=Bozhou%20Li%20and%20Wentao%20Zhang&entry.1292438233=%20%20Currently%2C%20a%20prevalent%20approach%20for%20enhancing%20Vision-Language%20Models%20%28VLMs%29%0Aperformance%20is%20to%20encode%20both%20the%20high-resolution%20version%20and%20the%20thumbnail%20of%0Aan%20image%20simultaneously.%20While%20effective%2C%20this%20method%20generates%20a%20large%20number%0Aof%20image%20tokens.%20When%20combined%20with%20the%20widely%20used%20Rotary%20Position%20Embedding%0A%28RoPE%29%2C%20its%20long-term%20decay%20property%20hinders%20the%20interaction%20between%0Ahigh-resolution%20tokens%20and%20thumbnail%20tokens%2C%20as%20well%20as%20between%20text%20and%20image.%0ATo%20address%20these%20issues%2C%20we%20propose%20ID-Align%2C%20which%20alleviates%20these%20problems%0Aby%20reordering%20position%20IDs.%20In%20this%20method%2C%20high-resolution%20tokens%20inherit%20IDs%0Afrom%20their%20corresponding%20thumbnail%20token%20while%20constraining%20the%20overexpansion%0Aof%20positional%20indices.%20Our%20experiments%20conducted%20within%20the%20LLaVA-Next%0Aframework%20demonstrate%20that%20ID-Align%20achieves%20significant%20improvements%2C%0Aincluding%20a%206.09%25%20enhancement%20on%20MMBench%27s%20relation%20reasoning%20tasks%20and%20notable%0Agains%20across%20multiple%20benchmarks.%20Our%20code%20is%20available%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/zooblastlbz/ID-Align.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21465v1&entry.124074799=Read"},
{"title": "DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage\n  Assessment and Response", "author": "Junjue Wang and Weihao Xuan and Heli Qi and Zhihao Liu and Kunyi Liu and Yuhan Wu and Hongruixuan Chen and Jian Song and Junshi Xia and Zhuo Zheng and Naoto Yokoya", "abstract": "  Large vision-language models (VLMs) have made great achievements in Earth\nvision. However, complex disaster scenes with diverse disaster types,\ngeographic regions, and satellite sensors have posed new challenges for VLM\napplications. To fill this gap, we curate a remote sensing vision-language\ndataset (DisasterM3) for global-scale disaster assessment and response.\nDisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction\npairs across 5 continents, with three characteristics: 1) Multi-hazard:\nDisasterM3 involves 36 historical disaster events with significant impacts,\nwhich are categorized into 10 common natural and man-made disasters.\n2)Multi-sensor: Extreme weather during disasters often hinders optical sensor\nimaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery\nfor post-disaster scenes. 3) Multi-task: Based on real-world scenarios,\nDisasterM3 includes 9 disaster-related visual perception and reasoning tasks,\nharnessing the full potential of VLM's reasoning ability with progressing from\ndisaster-bearing body recognition to structural damage assessment and object\nrelational reasoning, culminating in the generation of long-form disaster\nreports. We extensively evaluated 14 generic and remote sensing VLMs on our\nbenchmark, revealing that state-of-the-art models struggle with the disaster\ntasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap,\nand damage object counting insensitivity. Focusing on these issues, we\nfine-tune four VLMs using our dataset and achieve stable improvements across\nall tasks, with robust cross-sensor and cross-disaster generalization\ncapabilities.\n", "link": "http://arxiv.org/abs/2505.21089v1", "date": "2025-05-27", "relevancy": 2.8957, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5935}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisasterM3%3A%20A%20Remote%20Sensing%20Vision-Language%20Dataset%20for%20Disaster%20Damage%0A%20%20Assessment%20and%20Response&body=Title%3A%20DisasterM3%3A%20A%20Remote%20Sensing%20Vision-Language%20Dataset%20for%20Disaster%20Damage%0A%20%20Assessment%20and%20Response%0AAuthor%3A%20Junjue%20Wang%20and%20Weihao%20Xuan%20and%20Heli%20Qi%20and%20Zhihao%20Liu%20and%20Kunyi%20Liu%20and%20Yuhan%20Wu%20and%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Junshi%20Xia%20and%20Zhuo%20Zheng%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20made%20great%20achievements%20in%20Earth%0Avision.%20However%2C%20complex%20disaster%20scenes%20with%20diverse%20disaster%20types%2C%0Ageographic%20regions%2C%20and%20satellite%20sensors%20have%20posed%20new%20challenges%20for%20VLM%0Aapplications.%20To%20fill%20this%20gap%2C%20we%20curate%20a%20remote%20sensing%20vision-language%0Adataset%20%28DisasterM3%29%20for%20global-scale%20disaster%20assessment%20and%20response.%0ADisasterM3%20includes%2026%2C988%20bi-temporal%20satellite%20images%20and%20123k%20instruction%0Apairs%20across%205%20continents%2C%20with%20three%20characteristics%3A%201%29%20Multi-hazard%3A%0ADisasterM3%20involves%2036%20historical%20disaster%20events%20with%20significant%20impacts%2C%0Awhich%20are%20categorized%20into%2010%20common%20natural%20and%20man-made%20disasters.%0A2%29Multi-sensor%3A%20Extreme%20weather%20during%20disasters%20often%20hinders%20optical%20sensor%0Aimaging%2C%20making%20it%20necessary%20to%20combine%20Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%0Afor%20post-disaster%20scenes.%203%29%20Multi-task%3A%20Based%20on%20real-world%20scenarios%2C%0ADisasterM3%20includes%209%20disaster-related%20visual%20perception%20and%20reasoning%20tasks%2C%0Aharnessing%20the%20full%20potential%20of%20VLM%27s%20reasoning%20ability%20with%20progressing%20from%0Adisaster-bearing%20body%20recognition%20to%20structural%20damage%20assessment%20and%20object%0Arelational%20reasoning%2C%20culminating%20in%20the%20generation%20of%20long-form%20disaster%0Areports.%20We%20extensively%20evaluated%2014%20generic%20and%20remote%20sensing%20VLMs%20on%20our%0Abenchmark%2C%20revealing%20that%20state-of-the-art%20models%20struggle%20with%20the%20disaster%0Atasks%2C%20largely%20due%20to%20the%20lack%20of%20a%20disaster-specific%20corpus%2C%20cross-sensor%20gap%2C%0Aand%20damage%20object%20counting%20insensitivity.%20Focusing%20on%20these%20issues%2C%20we%0Afine-tune%20four%20VLMs%20using%20our%20dataset%20and%20achieve%20stable%20improvements%20across%0Aall%20tasks%2C%20with%20robust%20cross-sensor%20and%20cross-disaster%20generalization%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisasterM3%253A%2520A%2520Remote%2520Sensing%2520Vision-Language%2520Dataset%2520for%2520Disaster%2520Damage%250A%2520%2520Assessment%2520and%2520Response%26entry.906535625%3DJunjue%2520Wang%2520and%2520Weihao%2520Xuan%2520and%2520Heli%2520Qi%2520and%2520Zhihao%2520Liu%2520and%2520Kunyi%2520Liu%2520and%2520Yuhan%2520Wu%2520and%2520Hongruixuan%2520Chen%2520and%2520Jian%2520Song%2520and%2520Junshi%2520Xia%2520and%2520Zhuo%2520Zheng%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520great%2520achievements%2520in%2520Earth%250Avision.%2520However%252C%2520complex%2520disaster%2520scenes%2520with%2520diverse%2520disaster%2520types%252C%250Ageographic%2520regions%252C%2520and%2520satellite%2520sensors%2520have%2520posed%2520new%2520challenges%2520for%2520VLM%250Aapplications.%2520To%2520fill%2520this%2520gap%252C%2520we%2520curate%2520a%2520remote%2520sensing%2520vision-language%250Adataset%2520%2528DisasterM3%2529%2520for%2520global-scale%2520disaster%2520assessment%2520and%2520response.%250ADisasterM3%2520includes%252026%252C988%2520bi-temporal%2520satellite%2520images%2520and%2520123k%2520instruction%250Apairs%2520across%25205%2520continents%252C%2520with%2520three%2520characteristics%253A%25201%2529%2520Multi-hazard%253A%250ADisasterM3%2520involves%252036%2520historical%2520disaster%2520events%2520with%2520significant%2520impacts%252C%250Awhich%2520are%2520categorized%2520into%252010%2520common%2520natural%2520and%2520man-made%2520disasters.%250A2%2529Multi-sensor%253A%2520Extreme%2520weather%2520during%2520disasters%2520often%2520hinders%2520optical%2520sensor%250Aimaging%252C%2520making%2520it%2520necessary%2520to%2520combine%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520imagery%250Afor%2520post-disaster%2520scenes.%25203%2529%2520Multi-task%253A%2520Based%2520on%2520real-world%2520scenarios%252C%250ADisasterM3%2520includes%25209%2520disaster-related%2520visual%2520perception%2520and%2520reasoning%2520tasks%252C%250Aharnessing%2520the%2520full%2520potential%2520of%2520VLM%2527s%2520reasoning%2520ability%2520with%2520progressing%2520from%250Adisaster-bearing%2520body%2520recognition%2520to%2520structural%2520damage%2520assessment%2520and%2520object%250Arelational%2520reasoning%252C%2520culminating%2520in%2520the%2520generation%2520of%2520long-form%2520disaster%250Areports.%2520We%2520extensively%2520evaluated%252014%2520generic%2520and%2520remote%2520sensing%2520VLMs%2520on%2520our%250Abenchmark%252C%2520revealing%2520that%2520state-of-the-art%2520models%2520struggle%2520with%2520the%2520disaster%250Atasks%252C%2520largely%2520due%2520to%2520the%2520lack%2520of%2520a%2520disaster-specific%2520corpus%252C%2520cross-sensor%2520gap%252C%250Aand%2520damage%2520object%2520counting%2520insensitivity.%2520Focusing%2520on%2520these%2520issues%252C%2520we%250Afine-tune%2520four%2520VLMs%2520using%2520our%2520dataset%2520and%2520achieve%2520stable%2520improvements%2520across%250Aall%2520tasks%252C%2520with%2520robust%2520cross-sensor%2520and%2520cross-disaster%2520generalization%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisasterM3%3A%20A%20Remote%20Sensing%20Vision-Language%20Dataset%20for%20Disaster%20Damage%0A%20%20Assessment%20and%20Response&entry.906535625=Junjue%20Wang%20and%20Weihao%20Xuan%20and%20Heli%20Qi%20and%20Zhihao%20Liu%20and%20Kunyi%20Liu%20and%20Yuhan%20Wu%20and%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Junshi%20Xia%20and%20Zhuo%20Zheng%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20made%20great%20achievements%20in%20Earth%0Avision.%20However%2C%20complex%20disaster%20scenes%20with%20diverse%20disaster%20types%2C%0Ageographic%20regions%2C%20and%20satellite%20sensors%20have%20posed%20new%20challenges%20for%20VLM%0Aapplications.%20To%20fill%20this%20gap%2C%20we%20curate%20a%20remote%20sensing%20vision-language%0Adataset%20%28DisasterM3%29%20for%20global-scale%20disaster%20assessment%20and%20response.%0ADisasterM3%20includes%2026%2C988%20bi-temporal%20satellite%20images%20and%20123k%20instruction%0Apairs%20across%205%20continents%2C%20with%20three%20characteristics%3A%201%29%20Multi-hazard%3A%0ADisasterM3%20involves%2036%20historical%20disaster%20events%20with%20significant%20impacts%2C%0Awhich%20are%20categorized%20into%2010%20common%20natural%20and%20man-made%20disasters.%0A2%29Multi-sensor%3A%20Extreme%20weather%20during%20disasters%20often%20hinders%20optical%20sensor%0Aimaging%2C%20making%20it%20necessary%20to%20combine%20Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%0Afor%20post-disaster%20scenes.%203%29%20Multi-task%3A%20Based%20on%20real-world%20scenarios%2C%0ADisasterM3%20includes%209%20disaster-related%20visual%20perception%20and%20reasoning%20tasks%2C%0Aharnessing%20the%20full%20potential%20of%20VLM%27s%20reasoning%20ability%20with%20progressing%20from%0Adisaster-bearing%20body%20recognition%20to%20structural%20damage%20assessment%20and%20object%0Arelational%20reasoning%2C%20culminating%20in%20the%20generation%20of%20long-form%20disaster%0Areports.%20We%20extensively%20evaluated%2014%20generic%20and%20remote%20sensing%20VLMs%20on%20our%0Abenchmark%2C%20revealing%20that%20state-of-the-art%20models%20struggle%20with%20the%20disaster%0Atasks%2C%20largely%20due%20to%20the%20lack%20of%20a%20disaster-specific%20corpus%2C%20cross-sensor%20gap%2C%0Aand%20damage%20object%20counting%20insensitivity.%20Focusing%20on%20these%20issues%2C%20we%0Afine-tune%20four%20VLMs%20using%20our%20dataset%20and%20achieve%20stable%20improvements%20across%0Aall%20tasks%2C%20with%20robust%20cross-sensor%20and%20cross-disaster%20generalization%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21089v1&entry.124074799=Read"},
{"title": "A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D\n  Riemann Problems from Sparse Data", "author": "Rami Cassia and Rich Kerswell", "abstract": "  In compressible fluid flow, reconstructing shocks, discontinuities,\nrarefactions, and their interactions from sparse measurements is an important\ninverse problem with practical applications. Moreover, physics-informed machine\nlearning has recently become an increasingly popular approach for performing\nreconstructions tasks. In this work we explore a machine learning recipe, known\nas GraphGPS, for reconstructing canonical compressible flows known as 3D\nRiemann problems from sparse observations, in a physics-informed manner. The\nGraphGPS framework combines the benefits of positional encodings, local\nmessage-passing of graphs, and global contextual awareness, and we explore the\nlatter two components through an ablation study. Furthermore, we modify the\naggregation step of message-passing such that it is aware of shocks and\ndiscontinuities, resulting in sharper reconstructions of these features.\nAdditionally, we modify message-passing such that information flows strictly\nfrom known nodes only, which results in computational savings, better training\nconvergence, and no degradation of reconstruction accuracy. We also show that\nthe GraphGPS framework outperforms numerous machine learning benchmarks.\n", "link": "http://arxiv.org/abs/2505.21421v1", "date": "2025-05-27", "relevancy": 2.8716, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5833}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.577}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Physics-Augmented%20GraphGPS%20Framework%20for%20the%20Reconstruction%20of%203D%0A%20%20Riemann%20Problems%20from%20Sparse%20Data&body=Title%3A%20A%20Physics-Augmented%20GraphGPS%20Framework%20for%20the%20Reconstruction%20of%203D%0A%20%20Riemann%20Problems%20from%20Sparse%20Data%0AAuthor%3A%20Rami%20Cassia%20and%20Rich%20Kerswell%0AAbstract%3A%20%20%20In%20compressible%20fluid%20flow%2C%20reconstructing%20shocks%2C%20discontinuities%2C%0Ararefactions%2C%20and%20their%20interactions%20from%20sparse%20measurements%20is%20an%20important%0Ainverse%20problem%20with%20practical%20applications.%20Moreover%2C%20physics-informed%20machine%0Alearning%20has%20recently%20become%20an%20increasingly%20popular%20approach%20for%20performing%0Areconstructions%20tasks.%20In%20this%20work%20we%20explore%20a%20machine%20learning%20recipe%2C%20known%0Aas%20GraphGPS%2C%20for%20reconstructing%20canonical%20compressible%20flows%20known%20as%203D%0ARiemann%20problems%20from%20sparse%20observations%2C%20in%20a%20physics-informed%20manner.%20The%0AGraphGPS%20framework%20combines%20the%20benefits%20of%20positional%20encodings%2C%20local%0Amessage-passing%20of%20graphs%2C%20and%20global%20contextual%20awareness%2C%20and%20we%20explore%20the%0Alatter%20two%20components%20through%20an%20ablation%20study.%20Furthermore%2C%20we%20modify%20the%0Aaggregation%20step%20of%20message-passing%20such%20that%20it%20is%20aware%20of%20shocks%20and%0Adiscontinuities%2C%20resulting%20in%20sharper%20reconstructions%20of%20these%20features.%0AAdditionally%2C%20we%20modify%20message-passing%20such%20that%20information%20flows%20strictly%0Afrom%20known%20nodes%20only%2C%20which%20results%20in%20computational%20savings%2C%20better%20training%0Aconvergence%2C%20and%20no%20degradation%20of%20reconstruction%20accuracy.%20We%20also%20show%20that%0Athe%20GraphGPS%20framework%20outperforms%20numerous%20machine%20learning%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Physics-Augmented%2520GraphGPS%2520Framework%2520for%2520the%2520Reconstruction%2520of%25203D%250A%2520%2520Riemann%2520Problems%2520from%2520Sparse%2520Data%26entry.906535625%3DRami%2520Cassia%2520and%2520Rich%2520Kerswell%26entry.1292438233%3D%2520%2520In%2520compressible%2520fluid%2520flow%252C%2520reconstructing%2520shocks%252C%2520discontinuities%252C%250Ararefactions%252C%2520and%2520their%2520interactions%2520from%2520sparse%2520measurements%2520is%2520an%2520important%250Ainverse%2520problem%2520with%2520practical%2520applications.%2520Moreover%252C%2520physics-informed%2520machine%250Alearning%2520has%2520recently%2520become%2520an%2520increasingly%2520popular%2520approach%2520for%2520performing%250Areconstructions%2520tasks.%2520In%2520this%2520work%2520we%2520explore%2520a%2520machine%2520learning%2520recipe%252C%2520known%250Aas%2520GraphGPS%252C%2520for%2520reconstructing%2520canonical%2520compressible%2520flows%2520known%2520as%25203D%250ARiemann%2520problems%2520from%2520sparse%2520observations%252C%2520in%2520a%2520physics-informed%2520manner.%2520The%250AGraphGPS%2520framework%2520combines%2520the%2520benefits%2520of%2520positional%2520encodings%252C%2520local%250Amessage-passing%2520of%2520graphs%252C%2520and%2520global%2520contextual%2520awareness%252C%2520and%2520we%2520explore%2520the%250Alatter%2520two%2520components%2520through%2520an%2520ablation%2520study.%2520Furthermore%252C%2520we%2520modify%2520the%250Aaggregation%2520step%2520of%2520message-passing%2520such%2520that%2520it%2520is%2520aware%2520of%2520shocks%2520and%250Adiscontinuities%252C%2520resulting%2520in%2520sharper%2520reconstructions%2520of%2520these%2520features.%250AAdditionally%252C%2520we%2520modify%2520message-passing%2520such%2520that%2520information%2520flows%2520strictly%250Afrom%2520known%2520nodes%2520only%252C%2520which%2520results%2520in%2520computational%2520savings%252C%2520better%2520training%250Aconvergence%252C%2520and%2520no%2520degradation%2520of%2520reconstruction%2520accuracy.%2520We%2520also%2520show%2520that%250Athe%2520GraphGPS%2520framework%2520outperforms%2520numerous%2520machine%2520learning%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Physics-Augmented%20GraphGPS%20Framework%20for%20the%20Reconstruction%20of%203D%0A%20%20Riemann%20Problems%20from%20Sparse%20Data&entry.906535625=Rami%20Cassia%20and%20Rich%20Kerswell&entry.1292438233=%20%20In%20compressible%20fluid%20flow%2C%20reconstructing%20shocks%2C%20discontinuities%2C%0Ararefactions%2C%20and%20their%20interactions%20from%20sparse%20measurements%20is%20an%20important%0Ainverse%20problem%20with%20practical%20applications.%20Moreover%2C%20physics-informed%20machine%0Alearning%20has%20recently%20become%20an%20increasingly%20popular%20approach%20for%20performing%0Areconstructions%20tasks.%20In%20this%20work%20we%20explore%20a%20machine%20learning%20recipe%2C%20known%0Aas%20GraphGPS%2C%20for%20reconstructing%20canonical%20compressible%20flows%20known%20as%203D%0ARiemann%20problems%20from%20sparse%20observations%2C%20in%20a%20physics-informed%20manner.%20The%0AGraphGPS%20framework%20combines%20the%20benefits%20of%20positional%20encodings%2C%20local%0Amessage-passing%20of%20graphs%2C%20and%20global%20contextual%20awareness%2C%20and%20we%20explore%20the%0Alatter%20two%20components%20through%20an%20ablation%20study.%20Furthermore%2C%20we%20modify%20the%0Aaggregation%20step%20of%20message-passing%20such%20that%20it%20is%20aware%20of%20shocks%20and%0Adiscontinuities%2C%20resulting%20in%20sharper%20reconstructions%20of%20these%20features.%0AAdditionally%2C%20we%20modify%20message-passing%20such%20that%20information%20flows%20strictly%0Afrom%20known%20nodes%20only%2C%20which%20results%20in%20computational%20savings%2C%20better%20training%0Aconvergence%2C%20and%20no%20degradation%20of%20reconstruction%20accuracy.%20We%20also%20show%20that%0Athe%20GraphGPS%20framework%20outperforms%20numerous%20machine%20learning%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21421v1&entry.124074799=Read"},
{"title": "Structure from Collision", "author": "Takuhiro Kaneko", "abstract": "  Recent advancements in neural 3D representations, such as neural radiance\nfields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate\nestimation of 3D structures from multiview images. However, this capability is\nlimited to estimating the visible external structure, and identifying the\ninvisible internal structure hidden behind the surface is difficult. To\novercome this limitation, we address a new task called Structure from Collision\n(SfC), which aims to estimate the structure (including the invisible internal\nstructure) of an object from appearance changes during collision. To solve this\nproblem, we propose a novel model called SfC-NeRF that optimizes the invisible\ninternal structure of an object through a video sequence under physical,\nappearance (i.e., visible external structure)-preserving, and keyframe\nconstraints. In particular, to avoid falling into undesirable local optima\nowing to its ill-posed nature, we propose volume annealing; that is, searching\nfor global optima by repeatedly reducing and expanding the volume. Extensive\nexperiments on 115 objects involving diverse structures (i.e., various cavity\nshapes, locations, and sizes) and material properties revealed the properties\nof SfC and demonstrated the effectiveness of the proposed SfC-NeRF.\n", "link": "http://arxiv.org/abs/2505.21335v1", "date": "2025-05-27", "relevancy": 2.8429, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20from%20Collision&body=Title%3A%20Structure%20from%20Collision%0AAuthor%3A%20Takuhiro%20Kaneko%0AAbstract%3A%20%20%20Recent%20advancements%20in%20neural%203D%20representations%2C%20such%20as%20neural%20radiance%0Afields%20%28NeRF%29%20and%203D%20Gaussian%20splatting%20%283DGS%29%2C%20have%20enabled%20the%20accurate%0Aestimation%20of%203D%20structures%20from%20multiview%20images.%20However%2C%20this%20capability%20is%0Alimited%20to%20estimating%20the%20visible%20external%20structure%2C%20and%20identifying%20the%0Ainvisible%20internal%20structure%20hidden%20behind%20the%20surface%20is%20difficult.%20To%0Aovercome%20this%20limitation%2C%20we%20address%20a%20new%20task%20called%20Structure%20from%20Collision%0A%28SfC%29%2C%20which%20aims%20to%20estimate%20the%20structure%20%28including%20the%20invisible%20internal%0Astructure%29%20of%20an%20object%20from%20appearance%20changes%20during%20collision.%20To%20solve%20this%0Aproblem%2C%20we%20propose%20a%20novel%20model%20called%20SfC-NeRF%20that%20optimizes%20the%20invisible%0Ainternal%20structure%20of%20an%20object%20through%20a%20video%20sequence%20under%20physical%2C%0Aappearance%20%28i.e.%2C%20visible%20external%20structure%29-preserving%2C%20and%20keyframe%0Aconstraints.%20In%20particular%2C%20to%20avoid%20falling%20into%20undesirable%20local%20optima%0Aowing%20to%20its%20ill-posed%20nature%2C%20we%20propose%20volume%20annealing%3B%20that%20is%2C%20searching%0Afor%20global%20optima%20by%20repeatedly%20reducing%20and%20expanding%20the%20volume.%20Extensive%0Aexperiments%20on%20115%20objects%20involving%20diverse%20structures%20%28i.e.%2C%20various%20cavity%0Ashapes%2C%20locations%2C%20and%20sizes%29%20and%20material%20properties%20revealed%20the%20properties%0Aof%20SfC%20and%20demonstrated%20the%20effectiveness%20of%20the%20proposed%20SfC-NeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520from%2520Collision%26entry.906535625%3DTakuhiro%2520Kaneko%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520neural%25203D%2520representations%252C%2520such%2520as%2520neural%2520radiance%250Afields%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%252C%2520have%2520enabled%2520the%2520accurate%250Aestimation%2520of%25203D%2520structures%2520from%2520multiview%2520images.%2520However%252C%2520this%2520capability%2520is%250Alimited%2520to%2520estimating%2520the%2520visible%2520external%2520structure%252C%2520and%2520identifying%2520the%250Ainvisible%2520internal%2520structure%2520hidden%2520behind%2520the%2520surface%2520is%2520difficult.%2520To%250Aovercome%2520this%2520limitation%252C%2520we%2520address%2520a%2520new%2520task%2520called%2520Structure%2520from%2520Collision%250A%2528SfC%2529%252C%2520which%2520aims%2520to%2520estimate%2520the%2520structure%2520%2528including%2520the%2520invisible%2520internal%250Astructure%2529%2520of%2520an%2520object%2520from%2520appearance%2520changes%2520during%2520collision.%2520To%2520solve%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520novel%2520model%2520called%2520SfC-NeRF%2520that%2520optimizes%2520the%2520invisible%250Ainternal%2520structure%2520of%2520an%2520object%2520through%2520a%2520video%2520sequence%2520under%2520physical%252C%250Aappearance%2520%2528i.e.%252C%2520visible%2520external%2520structure%2529-preserving%252C%2520and%2520keyframe%250Aconstraints.%2520In%2520particular%252C%2520to%2520avoid%2520falling%2520into%2520undesirable%2520local%2520optima%250Aowing%2520to%2520its%2520ill-posed%2520nature%252C%2520we%2520propose%2520volume%2520annealing%253B%2520that%2520is%252C%2520searching%250Afor%2520global%2520optima%2520by%2520repeatedly%2520reducing%2520and%2520expanding%2520the%2520volume.%2520Extensive%250Aexperiments%2520on%2520115%2520objects%2520involving%2520diverse%2520structures%2520%2528i.e.%252C%2520various%2520cavity%250Ashapes%252C%2520locations%252C%2520and%2520sizes%2529%2520and%2520material%2520properties%2520revealed%2520the%2520properties%250Aof%2520SfC%2520and%2520demonstrated%2520the%2520effectiveness%2520of%2520the%2520proposed%2520SfC-NeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20from%20Collision&entry.906535625=Takuhiro%20Kaneko&entry.1292438233=%20%20Recent%20advancements%20in%20neural%203D%20representations%2C%20such%20as%20neural%20radiance%0Afields%20%28NeRF%29%20and%203D%20Gaussian%20splatting%20%283DGS%29%2C%20have%20enabled%20the%20accurate%0Aestimation%20of%203D%20structures%20from%20multiview%20images.%20However%2C%20this%20capability%20is%0Alimited%20to%20estimating%20the%20visible%20external%20structure%2C%20and%20identifying%20the%0Ainvisible%20internal%20structure%20hidden%20behind%20the%20surface%20is%20difficult.%20To%0Aovercome%20this%20limitation%2C%20we%20address%20a%20new%20task%20called%20Structure%20from%20Collision%0A%28SfC%29%2C%20which%20aims%20to%20estimate%20the%20structure%20%28including%20the%20invisible%20internal%0Astructure%29%20of%20an%20object%20from%20appearance%20changes%20during%20collision.%20To%20solve%20this%0Aproblem%2C%20we%20propose%20a%20novel%20model%20called%20SfC-NeRF%20that%20optimizes%20the%20invisible%0Ainternal%20structure%20of%20an%20object%20through%20a%20video%20sequence%20under%20physical%2C%0Aappearance%20%28i.e.%2C%20visible%20external%20structure%29-preserving%2C%20and%20keyframe%0Aconstraints.%20In%20particular%2C%20to%20avoid%20falling%20into%20undesirable%20local%20optima%0Aowing%20to%20its%20ill-posed%20nature%2C%20we%20propose%20volume%20annealing%3B%20that%20is%2C%20searching%0Afor%20global%20optima%20by%20repeatedly%20reducing%20and%20expanding%20the%20volume.%20Extensive%0Aexperiments%20on%20115%20objects%20involving%20diverse%20structures%20%28i.e.%2C%20various%20cavity%0Ashapes%2C%20locations%2C%20and%20sizes%29%20and%20material%20properties%20revealed%20the%20properties%0Aof%20SfC%20and%20demonstrated%20the%20effectiveness%20of%20the%20proposed%20SfC-NeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21335v1&entry.124074799=Read"},
{"title": "SURDS: Benchmarking Spatial Understanding and Reasoning in Driving\n  Scenarios with Vision Language Models", "author": "Xianda Guo and Ruijun Zhang and Yiqun Duan and Yuhang He and Dujun Nie and Wenke Huang and Chenming Zhang and Shuai Liu and Hao Zhao and Long Chen", "abstract": "  Accurate spatial reasoning in outdoor environments - covering geometry,\nobject pose, and inter-object relationships - is fundamental to downstream\ntasks such as mapping, motion forecasting, and high-level planning in\nautonomous driving. We introduce SURDS, a large-scale benchmark designed to\nsystematically evaluate the spatial reasoning capabilities of vision language\nmodels (VLMs). Built on the nuScenes dataset, SURDS comprises 41,080\nvision-question-answer training instances and 9,250 evaluation samples,\nspanning six spatial categories: orientation, depth estimation, pixel-level\nlocalization, pairwise distance, lateral ordering, and front-behind relations.\nWe benchmark leading general-purpose VLMs, including GPT, Gemini, and Qwen,\nrevealing persistent limitations in fine-grained spatial understanding. To\naddress these deficiencies, we go beyond static evaluation and explore whether\nalignment techniques can improve spatial reasoning performance. Specifically,\nwe propose a reinforcement learning-based alignment scheme leveraging spatially\ngrounded reward signals - capturing both perception-level accuracy (location)\nand reasoning consistency (logic). We further incorporate final-answer\ncorrectness and output-format rewards to guide fine-grained policy adaptation.\nOur GRPO-aligned variant achieves an overall score of 40.80 in the SURDS\nbenchmark. Notably, it outperforms proprietary systems such as GPT-4o (13.30)\nand Gemini-2.0-flash (35.71). To our best knowledge, this is the first study to\ndemonstrate that reinforcement learning-based alignment can significantly and\nconsistently enhance the spatial reasoning capabilities of VLMs in real-world\ndriving contexts. We release the SURDS benchmark, evaluation toolkit, and GRPO\nalignment code through: https://github.com/XiandaGuo/Drive-MLLM.\n", "link": "http://arxiv.org/abs/2411.13112v3", "date": "2025-05-27", "relevancy": 2.8388, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SURDS%3A%20Benchmarking%20Spatial%20Understanding%20and%20Reasoning%20in%20Driving%0A%20%20Scenarios%20with%20Vision%20Language%20Models&body=Title%3A%20SURDS%3A%20Benchmarking%20Spatial%20Understanding%20and%20Reasoning%20in%20Driving%0A%20%20Scenarios%20with%20Vision%20Language%20Models%0AAuthor%3A%20Xianda%20Guo%20and%20Ruijun%20Zhang%20and%20Yiqun%20Duan%20and%20Yuhang%20He%20and%20Dujun%20Nie%20and%20Wenke%20Huang%20and%20Chenming%20Zhang%20and%20Shuai%20Liu%20and%20Hao%20Zhao%20and%20Long%20Chen%0AAbstract%3A%20%20%20Accurate%20spatial%20reasoning%20in%20outdoor%20environments%20-%20covering%20geometry%2C%0Aobject%20pose%2C%20and%20inter-object%20relationships%20-%20is%20fundamental%20to%20downstream%0Atasks%20such%20as%20mapping%2C%20motion%20forecasting%2C%20and%20high-level%20planning%20in%0Aautonomous%20driving.%20We%20introduce%20SURDS%2C%20a%20large-scale%20benchmark%20designed%20to%0Asystematically%20evaluate%20the%20spatial%20reasoning%20capabilities%20of%20vision%20language%0Amodels%20%28VLMs%29.%20Built%20on%20the%20nuScenes%20dataset%2C%20SURDS%20comprises%2041%2C080%0Avision-question-answer%20training%20instances%20and%209%2C250%20evaluation%20samples%2C%0Aspanning%20six%20spatial%20categories%3A%20orientation%2C%20depth%20estimation%2C%20pixel-level%0Alocalization%2C%20pairwise%20distance%2C%20lateral%20ordering%2C%20and%20front-behind%20relations.%0AWe%20benchmark%20leading%20general-purpose%20VLMs%2C%20including%20GPT%2C%20Gemini%2C%20and%20Qwen%2C%0Arevealing%20persistent%20limitations%20in%20fine-grained%20spatial%20understanding.%20To%0Aaddress%20these%20deficiencies%2C%20we%20go%20beyond%20static%20evaluation%20and%20explore%20whether%0Aalignment%20techniques%20can%20improve%20spatial%20reasoning%20performance.%20Specifically%2C%0Awe%20propose%20a%20reinforcement%20learning-based%20alignment%20scheme%20leveraging%20spatially%0Agrounded%20reward%20signals%20-%20capturing%20both%20perception-level%20accuracy%20%28location%29%0Aand%20reasoning%20consistency%20%28logic%29.%20We%20further%20incorporate%20final-answer%0Acorrectness%20and%20output-format%20rewards%20to%20guide%20fine-grained%20policy%20adaptation.%0AOur%20GRPO-aligned%20variant%20achieves%20an%20overall%20score%20of%2040.80%20in%20the%20SURDS%0Abenchmark.%20Notably%2C%20it%20outperforms%20proprietary%20systems%20such%20as%20GPT-4o%20%2813.30%29%0Aand%20Gemini-2.0-flash%20%2835.71%29.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20study%20to%0Ademonstrate%20that%20reinforcement%20learning-based%20alignment%20can%20significantly%20and%0Aconsistently%20enhance%20the%20spatial%20reasoning%20capabilities%20of%20VLMs%20in%20real-world%0Adriving%20contexts.%20We%20release%20the%20SURDS%20benchmark%2C%20evaluation%20toolkit%2C%20and%20GRPO%0Aalignment%20code%20through%3A%20https%3A//github.com/XiandaGuo/Drive-MLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSURDS%253A%2520Benchmarking%2520Spatial%2520Understanding%2520and%2520Reasoning%2520in%2520Driving%250A%2520%2520Scenarios%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DXianda%2520Guo%2520and%2520Ruijun%2520Zhang%2520and%2520Yiqun%2520Duan%2520and%2520Yuhang%2520He%2520and%2520Dujun%2520Nie%2520and%2520Wenke%2520Huang%2520and%2520Chenming%2520Zhang%2520and%2520Shuai%2520Liu%2520and%2520Hao%2520Zhao%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Accurate%2520spatial%2520reasoning%2520in%2520outdoor%2520environments%2520-%2520covering%2520geometry%252C%250Aobject%2520pose%252C%2520and%2520inter-object%2520relationships%2520-%2520is%2520fundamental%2520to%2520downstream%250Atasks%2520such%2520as%2520mapping%252C%2520motion%2520forecasting%252C%2520and%2520high-level%2520planning%2520in%250Aautonomous%2520driving.%2520We%2520introduce%2520SURDS%252C%2520a%2520large-scale%2520benchmark%2520designed%2520to%250Asystematically%2520evaluate%2520the%2520spatial%2520reasoning%2520capabilities%2520of%2520vision%2520language%250Amodels%2520%2528VLMs%2529.%2520Built%2520on%2520the%2520nuScenes%2520dataset%252C%2520SURDS%2520comprises%252041%252C080%250Avision-question-answer%2520training%2520instances%2520and%25209%252C250%2520evaluation%2520samples%252C%250Aspanning%2520six%2520spatial%2520categories%253A%2520orientation%252C%2520depth%2520estimation%252C%2520pixel-level%250Alocalization%252C%2520pairwise%2520distance%252C%2520lateral%2520ordering%252C%2520and%2520front-behind%2520relations.%250AWe%2520benchmark%2520leading%2520general-purpose%2520VLMs%252C%2520including%2520GPT%252C%2520Gemini%252C%2520and%2520Qwen%252C%250Arevealing%2520persistent%2520limitations%2520in%2520fine-grained%2520spatial%2520understanding.%2520To%250Aaddress%2520these%2520deficiencies%252C%2520we%2520go%2520beyond%2520static%2520evaluation%2520and%2520explore%2520whether%250Aalignment%2520techniques%2520can%2520improve%2520spatial%2520reasoning%2520performance.%2520Specifically%252C%250Awe%2520propose%2520a%2520reinforcement%2520learning-based%2520alignment%2520scheme%2520leveraging%2520spatially%250Agrounded%2520reward%2520signals%2520-%2520capturing%2520both%2520perception-level%2520accuracy%2520%2528location%2529%250Aand%2520reasoning%2520consistency%2520%2528logic%2529.%2520We%2520further%2520incorporate%2520final-answer%250Acorrectness%2520and%2520output-format%2520rewards%2520to%2520guide%2520fine-grained%2520policy%2520adaptation.%250AOur%2520GRPO-aligned%2520variant%2520achieves%2520an%2520overall%2520score%2520of%252040.80%2520in%2520the%2520SURDS%250Abenchmark.%2520Notably%252C%2520it%2520outperforms%2520proprietary%2520systems%2520such%2520as%2520GPT-4o%2520%252813.30%2529%250Aand%2520Gemini-2.0-flash%2520%252835.71%2529.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%250Ademonstrate%2520that%2520reinforcement%2520learning-based%2520alignment%2520can%2520significantly%2520and%250Aconsistently%2520enhance%2520the%2520spatial%2520reasoning%2520capabilities%2520of%2520VLMs%2520in%2520real-world%250Adriving%2520contexts.%2520We%2520release%2520the%2520SURDS%2520benchmark%252C%2520evaluation%2520toolkit%252C%2520and%2520GRPO%250Aalignment%2520code%2520through%253A%2520https%253A//github.com/XiandaGuo/Drive-MLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SURDS%3A%20Benchmarking%20Spatial%20Understanding%20and%20Reasoning%20in%20Driving%0A%20%20Scenarios%20with%20Vision%20Language%20Models&entry.906535625=Xianda%20Guo%20and%20Ruijun%20Zhang%20and%20Yiqun%20Duan%20and%20Yuhang%20He%20and%20Dujun%20Nie%20and%20Wenke%20Huang%20and%20Chenming%20Zhang%20and%20Shuai%20Liu%20and%20Hao%20Zhao%20and%20Long%20Chen&entry.1292438233=%20%20Accurate%20spatial%20reasoning%20in%20outdoor%20environments%20-%20covering%20geometry%2C%0Aobject%20pose%2C%20and%20inter-object%20relationships%20-%20is%20fundamental%20to%20downstream%0Atasks%20such%20as%20mapping%2C%20motion%20forecasting%2C%20and%20high-level%20planning%20in%0Aautonomous%20driving.%20We%20introduce%20SURDS%2C%20a%20large-scale%20benchmark%20designed%20to%0Asystematically%20evaluate%20the%20spatial%20reasoning%20capabilities%20of%20vision%20language%0Amodels%20%28VLMs%29.%20Built%20on%20the%20nuScenes%20dataset%2C%20SURDS%20comprises%2041%2C080%0Avision-question-answer%20training%20instances%20and%209%2C250%20evaluation%20samples%2C%0Aspanning%20six%20spatial%20categories%3A%20orientation%2C%20depth%20estimation%2C%20pixel-level%0Alocalization%2C%20pairwise%20distance%2C%20lateral%20ordering%2C%20and%20front-behind%20relations.%0AWe%20benchmark%20leading%20general-purpose%20VLMs%2C%20including%20GPT%2C%20Gemini%2C%20and%20Qwen%2C%0Arevealing%20persistent%20limitations%20in%20fine-grained%20spatial%20understanding.%20To%0Aaddress%20these%20deficiencies%2C%20we%20go%20beyond%20static%20evaluation%20and%20explore%20whether%0Aalignment%20techniques%20can%20improve%20spatial%20reasoning%20performance.%20Specifically%2C%0Awe%20propose%20a%20reinforcement%20learning-based%20alignment%20scheme%20leveraging%20spatially%0Agrounded%20reward%20signals%20-%20capturing%20both%20perception-level%20accuracy%20%28location%29%0Aand%20reasoning%20consistency%20%28logic%29.%20We%20further%20incorporate%20final-answer%0Acorrectness%20and%20output-format%20rewards%20to%20guide%20fine-grained%20policy%20adaptation.%0AOur%20GRPO-aligned%20variant%20achieves%20an%20overall%20score%20of%2040.80%20in%20the%20SURDS%0Abenchmark.%20Notably%2C%20it%20outperforms%20proprietary%20systems%20such%20as%20GPT-4o%20%2813.30%29%0Aand%20Gemini-2.0-flash%20%2835.71%29.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20study%20to%0Ademonstrate%20that%20reinforcement%20learning-based%20alignment%20can%20significantly%20and%0Aconsistently%20enhance%20the%20spatial%20reasoning%20capabilities%20of%20VLMs%20in%20real-world%0Adriving%20contexts.%20We%20release%20the%20SURDS%20benchmark%2C%20evaluation%20toolkit%2C%20and%20GRPO%0Aalignment%20code%20through%3A%20https%3A//github.com/XiandaGuo/Drive-MLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13112v3&entry.124074799=Read"},
{"title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities", "author": "Junyan Zhang and Yubo Gao and Yibo Yan and Jungang Li and Zhaorui Hou and Sicheng Tao and Shuliang Liu and Song Dai and Yonghua Hei and Junzhuo Li and Xuming Hu", "abstract": "  The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.\n", "link": "http://arxiv.org/abs/2505.21191v1", "date": "2025-05-27", "relevancy": 2.7943, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Instruction-Specific%20Neurons%20%26%20Experts%3A%20An%20Analytical%0A%20%20Framework%20for%20LLM%27s%20Instruction-Following%20Capabilities&body=Title%3A%20Unveiling%20Instruction-Specific%20Neurons%20%26%20Experts%3A%20An%20Analytical%0A%20%20Framework%20for%20LLM%27s%20Instruction-Following%20Capabilities%0AAuthor%3A%20Junyan%20Zhang%20and%20Yubo%20Gao%20and%20Yibo%20Yan%20and%20Jungang%20Li%20and%20Zhaorui%20Hou%20and%20Sicheng%20Tao%20and%20Shuliang%20Liu%20and%20Song%20Dai%20and%20Yonghua%20Hei%20and%20Junzhuo%20Li%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20The%20finetuning%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%20advanced%0Atheir%20instruction-following%20capabilities%2C%20yet%20the%20underlying%20computational%0Amechanisms%20driving%20these%20improvements%20remain%20poorly%20understood.%20This%20study%0Asystematically%20examines%20how%20fine-tuning%20reconfigures%20LLM%20computations%20by%0Aisolating%20and%20analyzing%20instruction-specific%20sparse%20components%2C%20i.e.%2C%20neurons%0Ain%20dense%20models%20and%20both%20neurons%20and%20experts%20in%20Mixture-of-Experts%20%28MoE%29%0Aarchitectures.%20In%20particular%2C%20we%20introduce%20HexaInst%2C%20a%20carefully%20curated%20and%0Abalanced%20instructional%20dataset%20spanning%20six%20distinct%20categories%2C%20and%20propose%0ASPARCOM%2C%20a%20novel%20analytical%20framework%20comprising%20three%20key%20contributions%3A%20%281%29%20a%0Amethod%20for%20identifying%20these%20sparse%20components%2C%20%282%29%20an%20evaluation%20of%20their%0Afunctional%20generality%20and%20uniqueness%2C%20and%20%283%29%20a%20systematic%20comparison%20of%20their%0Aalterations.%20Through%20experiments%2C%20we%20demonstrate%20functional%20generality%2C%0Auniqueness%2C%20and%20the%20critical%20role%20of%20these%20components%20in%20instruction%20execution.%0ABy%20elucidating%20the%20relationship%20between%20fine-tuning-induced%20adaptations%20and%0Asparse%20computational%20substrates%2C%20this%20work%20provides%20deeper%20insights%20into%20how%0ALLMs%20internalize%20instruction-following%20behavior%20for%20the%20trustworthy%20LLM%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Instruction-Specific%2520Neurons%2520%2526%2520Experts%253A%2520An%2520Analytical%250A%2520%2520Framework%2520for%2520LLM%2527s%2520Instruction-Following%2520Capabilities%26entry.906535625%3DJunyan%2520Zhang%2520and%2520Yubo%2520Gao%2520and%2520Yibo%2520Yan%2520and%2520Jungang%2520Li%2520and%2520Zhaorui%2520Hou%2520and%2520Sicheng%2520Tao%2520and%2520Shuliang%2520Liu%2520and%2520Song%2520Dai%2520and%2520Yonghua%2520Hei%2520and%2520Junzhuo%2520Li%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520The%2520finetuning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520significantly%2520advanced%250Atheir%2520instruction-following%2520capabilities%252C%2520yet%2520the%2520underlying%2520computational%250Amechanisms%2520driving%2520these%2520improvements%2520remain%2520poorly%2520understood.%2520This%2520study%250Asystematically%2520examines%2520how%2520fine-tuning%2520reconfigures%2520LLM%2520computations%2520by%250Aisolating%2520and%2520analyzing%2520instruction-specific%2520sparse%2520components%252C%2520i.e.%252C%2520neurons%250Ain%2520dense%2520models%2520and%2520both%2520neurons%2520and%2520experts%2520in%2520Mixture-of-Experts%2520%2528MoE%2529%250Aarchitectures.%2520In%2520particular%252C%2520we%2520introduce%2520HexaInst%252C%2520a%2520carefully%2520curated%2520and%250Abalanced%2520instructional%2520dataset%2520spanning%2520six%2520distinct%2520categories%252C%2520and%2520propose%250ASPARCOM%252C%2520a%2520novel%2520analytical%2520framework%2520comprising%2520three%2520key%2520contributions%253A%2520%25281%2529%2520a%250Amethod%2520for%2520identifying%2520these%2520sparse%2520components%252C%2520%25282%2529%2520an%2520evaluation%2520of%2520their%250Afunctional%2520generality%2520and%2520uniqueness%252C%2520and%2520%25283%2529%2520a%2520systematic%2520comparison%2520of%2520their%250Aalterations.%2520Through%2520experiments%252C%2520we%2520demonstrate%2520functional%2520generality%252C%250Auniqueness%252C%2520and%2520the%2520critical%2520role%2520of%2520these%2520components%2520in%2520instruction%2520execution.%250ABy%2520elucidating%2520the%2520relationship%2520between%2520fine-tuning-induced%2520adaptations%2520and%250Asparse%2520computational%2520substrates%252C%2520this%2520work%2520provides%2520deeper%2520insights%2520into%2520how%250ALLMs%2520internalize%2520instruction-following%2520behavior%2520for%2520the%2520trustworthy%2520LLM%250Acommunity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Instruction-Specific%20Neurons%20%26%20Experts%3A%20An%20Analytical%0A%20%20Framework%20for%20LLM%27s%20Instruction-Following%20Capabilities&entry.906535625=Junyan%20Zhang%20and%20Yubo%20Gao%20and%20Yibo%20Yan%20and%20Jungang%20Li%20and%20Zhaorui%20Hou%20and%20Sicheng%20Tao%20and%20Shuliang%20Liu%20and%20Song%20Dai%20and%20Yonghua%20Hei%20and%20Junzhuo%20Li%20and%20Xuming%20Hu&entry.1292438233=%20%20The%20finetuning%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%20advanced%0Atheir%20instruction-following%20capabilities%2C%20yet%20the%20underlying%20computational%0Amechanisms%20driving%20these%20improvements%20remain%20poorly%20understood.%20This%20study%0Asystematically%20examines%20how%20fine-tuning%20reconfigures%20LLM%20computations%20by%0Aisolating%20and%20analyzing%20instruction-specific%20sparse%20components%2C%20i.e.%2C%20neurons%0Ain%20dense%20models%20and%20both%20neurons%20and%20experts%20in%20Mixture-of-Experts%20%28MoE%29%0Aarchitectures.%20In%20particular%2C%20we%20introduce%20HexaInst%2C%20a%20carefully%20curated%20and%0Abalanced%20instructional%20dataset%20spanning%20six%20distinct%20categories%2C%20and%20propose%0ASPARCOM%2C%20a%20novel%20analytical%20framework%20comprising%20three%20key%20contributions%3A%20%281%29%20a%0Amethod%20for%20identifying%20these%20sparse%20components%2C%20%282%29%20an%20evaluation%20of%20their%0Afunctional%20generality%20and%20uniqueness%2C%20and%20%283%29%20a%20systematic%20comparison%20of%20their%0Aalterations.%20Through%20experiments%2C%20we%20demonstrate%20functional%20generality%2C%0Auniqueness%2C%20and%20the%20critical%20role%20of%20these%20components%20in%20instruction%20execution.%0ABy%20elucidating%20the%20relationship%20between%20fine-tuning-induced%20adaptations%20and%0Asparse%20computational%20substrates%2C%20this%20work%20provides%20deeper%20insights%20into%20how%0ALLMs%20internalize%20instruction-following%20behavior%20for%20the%20trustworthy%20LLM%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21191v1&entry.124074799=Read"},
{"title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco\n  Reconstruction", "author": "Adeela Islam and Stefano Fiorini and Stuart James and Pietro Morerio and Alessio Del Bue", "abstract": "  The task of reassembly is a significant challenge across multiple domains,\nincluding archaeology, genomics, and molecular docking, requiring the precise\nplacement and orientation of elements to reconstruct an original structure. In\nthis work, we address key limitations in state-of-the-art Deep Learning methods\nfor reassembly, namely i) scalability; ii) multimodality; and iii) real-world\napplicability: beyond square or simple geometric shapes, realistic and complex\nerosion, or other real-world problems. We propose ReassembleNet, a method that\nreduces complexity by representing each input piece as a set of contour\nkeypoints and learning to select the most informative ones by Graph Neural\nNetworks pooling inspired techniques. ReassembleNet effectively lowers\ncomputational complexity while enabling the integration of features from\nmultiple modalities, including both geometric and texture data. Further\nenhanced through pretraining on a semi-synthetic dataset. We then apply\ndiffusion-based pose estimation to recover the original structure. We improve\non prior methods by 55% and 86% for RMSE Rotation and Translation,\nrespectively.\n", "link": "http://arxiv.org/abs/2505.21117v1", "date": "2025-05-27", "relevancy": 2.7875, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5905}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5446}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%0A%20%20Reconstruction&body=Title%3A%20ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%0A%20%20Reconstruction%0AAuthor%3A%20Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20The%20task%20of%20reassembly%20is%20a%20significant%20challenge%20across%20multiple%20domains%2C%0Aincluding%20archaeology%2C%20genomics%2C%20and%20molecular%20docking%2C%20requiring%20the%20precise%0Aplacement%20and%20orientation%20of%20elements%20to%20reconstruct%20an%20original%20structure.%20In%0Athis%20work%2C%20we%20address%20key%20limitations%20in%20state-of-the-art%20Deep%20Learning%20methods%0Afor%20reassembly%2C%20namely%20i%29%20scalability%3B%20ii%29%20multimodality%3B%20and%20iii%29%20real-world%0Aapplicability%3A%20beyond%20square%20or%20simple%20geometric%20shapes%2C%20realistic%20and%20complex%0Aerosion%2C%20or%20other%20real-world%20problems.%20We%20propose%20ReassembleNet%2C%20a%20method%20that%0Areduces%20complexity%20by%20representing%20each%20input%20piece%20as%20a%20set%20of%20contour%0Akeypoints%20and%20learning%20to%20select%20the%20most%20informative%20ones%20by%20Graph%20Neural%0ANetworks%20pooling%20inspired%20techniques.%20ReassembleNet%20effectively%20lowers%0Acomputational%20complexity%20while%20enabling%20the%20integration%20of%20features%20from%0Amultiple%20modalities%2C%20including%20both%20geometric%20and%20texture%20data.%20Further%0Aenhanced%20through%20pretraining%20on%20a%20semi-synthetic%20dataset.%20We%20then%20apply%0Adiffusion-based%20pose%20estimation%20to%20recover%20the%20original%20structure.%20We%20improve%0Aon%20prior%20methods%20by%2055%25%20and%2086%25%20for%20RMSE%20Rotation%20and%20Translation%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReassembleNet%253A%2520Learnable%2520Keypoints%2520and%2520Diffusion%2520for%25202D%2520Fresco%250A%2520%2520Reconstruction%26entry.906535625%3DAdeela%2520Islam%2520and%2520Stefano%2520Fiorini%2520and%2520Stuart%2520James%2520and%2520Pietro%2520Morerio%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520reassembly%2520is%2520a%2520significant%2520challenge%2520across%2520multiple%2520domains%252C%250Aincluding%2520archaeology%252C%2520genomics%252C%2520and%2520molecular%2520docking%252C%2520requiring%2520the%2520precise%250Aplacement%2520and%2520orientation%2520of%2520elements%2520to%2520reconstruct%2520an%2520original%2520structure.%2520In%250Athis%2520work%252C%2520we%2520address%2520key%2520limitations%2520in%2520state-of-the-art%2520Deep%2520Learning%2520methods%250Afor%2520reassembly%252C%2520namely%2520i%2529%2520scalability%253B%2520ii%2529%2520multimodality%253B%2520and%2520iii%2529%2520real-world%250Aapplicability%253A%2520beyond%2520square%2520or%2520simple%2520geometric%2520shapes%252C%2520realistic%2520and%2520complex%250Aerosion%252C%2520or%2520other%2520real-world%2520problems.%2520We%2520propose%2520ReassembleNet%252C%2520a%2520method%2520that%250Areduces%2520complexity%2520by%2520representing%2520each%2520input%2520piece%2520as%2520a%2520set%2520of%2520contour%250Akeypoints%2520and%2520learning%2520to%2520select%2520the%2520most%2520informative%2520ones%2520by%2520Graph%2520Neural%250ANetworks%2520pooling%2520inspired%2520techniques.%2520ReassembleNet%2520effectively%2520lowers%250Acomputational%2520complexity%2520while%2520enabling%2520the%2520integration%2520of%2520features%2520from%250Amultiple%2520modalities%252C%2520including%2520both%2520geometric%2520and%2520texture%2520data.%2520Further%250Aenhanced%2520through%2520pretraining%2520on%2520a%2520semi-synthetic%2520dataset.%2520We%2520then%2520apply%250Adiffusion-based%2520pose%2520estimation%2520to%2520recover%2520the%2520original%2520structure.%2520We%2520improve%250Aon%2520prior%2520methods%2520by%252055%2525%2520and%252086%2525%2520for%2520RMSE%2520Rotation%2520and%2520Translation%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%0A%20%20Reconstruction&entry.906535625=Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20The%20task%20of%20reassembly%20is%20a%20significant%20challenge%20across%20multiple%20domains%2C%0Aincluding%20archaeology%2C%20genomics%2C%20and%20molecular%20docking%2C%20requiring%20the%20precise%0Aplacement%20and%20orientation%20of%20elements%20to%20reconstruct%20an%20original%20structure.%20In%0Athis%20work%2C%20we%20address%20key%20limitations%20in%20state-of-the-art%20Deep%20Learning%20methods%0Afor%20reassembly%2C%20namely%20i%29%20scalability%3B%20ii%29%20multimodality%3B%20and%20iii%29%20real-world%0Aapplicability%3A%20beyond%20square%20or%20simple%20geometric%20shapes%2C%20realistic%20and%20complex%0Aerosion%2C%20or%20other%20real-world%20problems.%20We%20propose%20ReassembleNet%2C%20a%20method%20that%0Areduces%20complexity%20by%20representing%20each%20input%20piece%20as%20a%20set%20of%20contour%0Akeypoints%20and%20learning%20to%20select%20the%20most%20informative%20ones%20by%20Graph%20Neural%0ANetworks%20pooling%20inspired%20techniques.%20ReassembleNet%20effectively%20lowers%0Acomputational%20complexity%20while%20enabling%20the%20integration%20of%20features%20from%0Amultiple%20modalities%2C%20including%20both%20geometric%20and%20texture%20data.%20Further%0Aenhanced%20through%20pretraining%20on%20a%20semi-synthetic%20dataset.%20We%20then%20apply%0Adiffusion-based%20pose%20estimation%20to%20recover%20the%20original%20structure.%20We%20improve%0Aon%20prior%20methods%20by%2055%25%20and%2086%25%20for%20RMSE%20Rotation%20and%20Translation%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21117v1&entry.124074799=Read"},
{"title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to\n  8K Resolution", "author": "Fengxiang Wang and Mingshuo Chen and Yueying Li and Di Wang and Haotian Wang and Zonghao Guo and Zefan Wang and Boqi Shan and Long Lan and Yulin Wang and Hongzhen Wang and Wenjing Yang and Bo Du and Jing Zhang", "abstract": "  Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench.\n", "link": "http://arxiv.org/abs/2505.21375v1", "date": "2025-05-27", "relevancy": 2.7677, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoLLaVA-8K%3A%20Scaling%20Remote-Sensing%20Multimodal%20Large%20Language%20Models%20to%0A%20%208K%20Resolution&body=Title%3A%20GeoLLaVA-8K%3A%20Scaling%20Remote-Sensing%20Multimodal%20Large%20Language%20Models%20to%0A%20%208K%20Resolution%0AAuthor%3A%20Fengxiang%20Wang%20and%20Mingshuo%20Chen%20and%20Yueying%20Li%20and%20Di%20Wang%20and%20Haotian%20Wang%20and%20Zonghao%20Guo%20and%20Zefan%20Wang%20and%20Boqi%20Shan%20and%20Long%20Lan%20and%20Yulin%20Wang%20and%20Hongzhen%20Wang%20and%20Wenjing%20Yang%20and%20Bo%20Du%20and%20Jing%20Zhang%0AAbstract%3A%20%20%20Ultra-high-resolution%20%28UHR%29%20remote%20sensing%20%28RS%29%20imagery%20offers%20valuable%20data%0Afor%20Earth%20observation%20but%20pose%20challenges%20for%20existing%20multimodal%20foundation%0Amodels%20due%20to%20two%20key%20bottlenecks%3A%20%281%29%20limited%20availability%20of%20UHR%20training%0Adata%2C%20and%20%282%29%20token%20explosion%20caused%20by%20the%20large%20image%20size.%20To%20address%20data%0Ascarcity%2C%20we%20introduce%20SuperRS-VQA%20%28avg.%208%2C376%24%5Ctimes%248%2C376%29%20and%20HighRS-VQA%0A%28avg.%202%2C000%24%5Ctimes%241%2C912%29%2C%20the%20highest-resolution%20vision-language%20datasets%20in%0ARS%20to%20date%2C%20covering%2022%20real-world%20dialogue%20tasks.%20To%20mitigate%20token%20explosion%2C%0Aour%20pilot%20studies%20reveal%20significant%20redundancy%20in%20RS%20images%3A%20crucial%0Ainformation%20is%20concentrated%20in%20a%20small%20subset%20of%20object-centric%20tokens%2C%20while%0Apruning%20background%20tokens%20%28e.g.%2C%20ocean%20or%20forest%29%20can%20even%20improve%20performance.%0AMotivated%20by%20these%20findings%2C%20we%20propose%20two%20strategies%3A%20Background%20Token%0APruning%20and%20Anchored%20Token%20Selection%2C%20to%20reduce%20the%20memory%20footprint%20while%0Apreserving%20key%20semantics.Integrating%20these%20techniques%2C%20we%20introduce%0AGeoLLaVA-8K%2C%20the%20first%20RS-focused%20multimodal%20large%20language%20model%20capable%20of%0Ahandling%20inputs%20up%20to%208K%24%5Ctimes%248K%20resolution%2C%20built%20on%20the%20LLaVA%20framework.%0ATrained%20on%20SuperRS-VQA%20and%20HighRS-VQA%2C%20GeoLLaVA-8K%20sets%20a%20new%20state-of-the-art%0Aon%20the%20XLRS-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoLLaVA-8K%253A%2520Scaling%2520Remote-Sensing%2520Multimodal%2520Large%2520Language%2520Models%2520to%250A%2520%25208K%2520Resolution%26entry.906535625%3DFengxiang%2520Wang%2520and%2520Mingshuo%2520Chen%2520and%2520Yueying%2520Li%2520and%2520Di%2520Wang%2520and%2520Haotian%2520Wang%2520and%2520Zonghao%2520Guo%2520and%2520Zefan%2520Wang%2520and%2520Boqi%2520Shan%2520and%2520Long%2520Lan%2520and%2520Yulin%2520Wang%2520and%2520Hongzhen%2520Wang%2520and%2520Wenjing%2520Yang%2520and%2520Bo%2520Du%2520and%2520Jing%2520Zhang%26entry.1292438233%3D%2520%2520Ultra-high-resolution%2520%2528UHR%2529%2520remote%2520sensing%2520%2528RS%2529%2520imagery%2520offers%2520valuable%2520data%250Afor%2520Earth%2520observation%2520but%2520pose%2520challenges%2520for%2520existing%2520multimodal%2520foundation%250Amodels%2520due%2520to%2520two%2520key%2520bottlenecks%253A%2520%25281%2529%2520limited%2520availability%2520of%2520UHR%2520training%250Adata%252C%2520and%2520%25282%2529%2520token%2520explosion%2520caused%2520by%2520the%2520large%2520image%2520size.%2520To%2520address%2520data%250Ascarcity%252C%2520we%2520introduce%2520SuperRS-VQA%2520%2528avg.%25208%252C376%2524%255Ctimes%25248%252C376%2529%2520and%2520HighRS-VQA%250A%2528avg.%25202%252C000%2524%255Ctimes%25241%252C912%2529%252C%2520the%2520highest-resolution%2520vision-language%2520datasets%2520in%250ARS%2520to%2520date%252C%2520covering%252022%2520real-world%2520dialogue%2520tasks.%2520To%2520mitigate%2520token%2520explosion%252C%250Aour%2520pilot%2520studies%2520reveal%2520significant%2520redundancy%2520in%2520RS%2520images%253A%2520crucial%250Ainformation%2520is%2520concentrated%2520in%2520a%2520small%2520subset%2520of%2520object-centric%2520tokens%252C%2520while%250Apruning%2520background%2520tokens%2520%2528e.g.%252C%2520ocean%2520or%2520forest%2529%2520can%2520even%2520improve%2520performance.%250AMotivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520two%2520strategies%253A%2520Background%2520Token%250APruning%2520and%2520Anchored%2520Token%2520Selection%252C%2520to%2520reduce%2520the%2520memory%2520footprint%2520while%250Apreserving%2520key%2520semantics.Integrating%2520these%2520techniques%252C%2520we%2520introduce%250AGeoLLaVA-8K%252C%2520the%2520first%2520RS-focused%2520multimodal%2520large%2520language%2520model%2520capable%2520of%250Ahandling%2520inputs%2520up%2520to%25208K%2524%255Ctimes%25248K%2520resolution%252C%2520built%2520on%2520the%2520LLaVA%2520framework.%250ATrained%2520on%2520SuperRS-VQA%2520and%2520HighRS-VQA%252C%2520GeoLLaVA-8K%2520sets%2520a%2520new%2520state-of-the-art%250Aon%2520the%2520XLRS-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoLLaVA-8K%3A%20Scaling%20Remote-Sensing%20Multimodal%20Large%20Language%20Models%20to%0A%20%208K%20Resolution&entry.906535625=Fengxiang%20Wang%20and%20Mingshuo%20Chen%20and%20Yueying%20Li%20and%20Di%20Wang%20and%20Haotian%20Wang%20and%20Zonghao%20Guo%20and%20Zefan%20Wang%20and%20Boqi%20Shan%20and%20Long%20Lan%20and%20Yulin%20Wang%20and%20Hongzhen%20Wang%20and%20Wenjing%20Yang%20and%20Bo%20Du%20and%20Jing%20Zhang&entry.1292438233=%20%20Ultra-high-resolution%20%28UHR%29%20remote%20sensing%20%28RS%29%20imagery%20offers%20valuable%20data%0Afor%20Earth%20observation%20but%20pose%20challenges%20for%20existing%20multimodal%20foundation%0Amodels%20due%20to%20two%20key%20bottlenecks%3A%20%281%29%20limited%20availability%20of%20UHR%20training%0Adata%2C%20and%20%282%29%20token%20explosion%20caused%20by%20the%20large%20image%20size.%20To%20address%20data%0Ascarcity%2C%20we%20introduce%20SuperRS-VQA%20%28avg.%208%2C376%24%5Ctimes%248%2C376%29%20and%20HighRS-VQA%0A%28avg.%202%2C000%24%5Ctimes%241%2C912%29%2C%20the%20highest-resolution%20vision-language%20datasets%20in%0ARS%20to%20date%2C%20covering%2022%20real-world%20dialogue%20tasks.%20To%20mitigate%20token%20explosion%2C%0Aour%20pilot%20studies%20reveal%20significant%20redundancy%20in%20RS%20images%3A%20crucial%0Ainformation%20is%20concentrated%20in%20a%20small%20subset%20of%20object-centric%20tokens%2C%20while%0Apruning%20background%20tokens%20%28e.g.%2C%20ocean%20or%20forest%29%20can%20even%20improve%20performance.%0AMotivated%20by%20these%20findings%2C%20we%20propose%20two%20strategies%3A%20Background%20Token%0APruning%20and%20Anchored%20Token%20Selection%2C%20to%20reduce%20the%20memory%20footprint%20while%0Apreserving%20key%20semantics.Integrating%20these%20techniques%2C%20we%20introduce%0AGeoLLaVA-8K%2C%20the%20first%20RS-focused%20multimodal%20large%20language%20model%20capable%20of%0Ahandling%20inputs%20up%20to%208K%24%5Ctimes%248K%20resolution%2C%20built%20on%20the%20LLaVA%20framework.%0ATrained%20on%20SuperRS-VQA%20and%20HighRS-VQA%2C%20GeoLLaVA-8K%20sets%20a%20new%20state-of-the-art%0Aon%20the%20XLRS-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21375v1&entry.124074799=Read"},
{"title": "EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction", "author": "Ryosei Hara and Wataru Ikeda and Masashi Hatano and Mariko Isogawa", "abstract": "  Reconstructing 3D hand mesh is challenging but an important task for\nhuman-computer interaction and AR/VR applications. In particular, RGB and/or\ndepth cameras have been widely used in this task. However, methods using these\nconventional cameras face challenges in low-light environments and during\nmotion blur. Thus, to address these limitations, event cameras have been\nattracting attention in recent years for their high dynamic range and high\ntemporal resolution. Despite their advantages, event cameras are sensitive to\nbackground noise or camera motion, which has limited existing studies to static\nbackgrounds and fixed cameras. In this study, we propose EventEgoHands, a novel\nmethod for event-based 3D hand mesh reconstruction in an egocentric view. Our\napproach introduces a Hand Segmentation Module that extracts hand regions,\neffectively mitigating the influence of dynamic background events. We evaluated\nour approach and demonstrated its effectiveness on the N-HOT3D dataset,\nimproving MPJPE by approximately more than 4.5 cm (43%).\n", "link": "http://arxiv.org/abs/2505.19169v2", "date": "2025-05-27", "relevancy": 2.7667, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5694}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5514}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventEgoHands%3A%20Event-based%20Egocentric%203D%20Hand%20Mesh%20Reconstruction&body=Title%3A%20EventEgoHands%3A%20Event-based%20Egocentric%203D%20Hand%20Mesh%20Reconstruction%0AAuthor%3A%20Ryosei%20Hara%20and%20Wataru%20Ikeda%20and%20Masashi%20Hatano%20and%20Mariko%20Isogawa%0AAbstract%3A%20%20%20Reconstructing%203D%20hand%20mesh%20is%20challenging%20but%20an%20important%20task%20for%0Ahuman-computer%20interaction%20and%20AR/VR%20applications.%20In%20particular%2C%20RGB%20and/or%0Adepth%20cameras%20have%20been%20widely%20used%20in%20this%20task.%20However%2C%20methods%20using%20these%0Aconventional%20cameras%20face%20challenges%20in%20low-light%20environments%20and%20during%0Amotion%20blur.%20Thus%2C%20to%20address%20these%20limitations%2C%20event%20cameras%20have%20been%0Aattracting%20attention%20in%20recent%20years%20for%20their%20high%20dynamic%20range%20and%20high%0Atemporal%20resolution.%20Despite%20their%20advantages%2C%20event%20cameras%20are%20sensitive%20to%0Abackground%20noise%20or%20camera%20motion%2C%20which%20has%20limited%20existing%20studies%20to%20static%0Abackgrounds%20and%20fixed%20cameras.%20In%20this%20study%2C%20we%20propose%20EventEgoHands%2C%20a%20novel%0Amethod%20for%20event-based%203D%20hand%20mesh%20reconstruction%20in%20an%20egocentric%20view.%20Our%0Aapproach%20introduces%20a%20Hand%20Segmentation%20Module%20that%20extracts%20hand%20regions%2C%0Aeffectively%20mitigating%20the%20influence%20of%20dynamic%20background%20events.%20We%20evaluated%0Aour%20approach%20and%20demonstrated%20its%20effectiveness%20on%20the%20N-HOT3D%20dataset%2C%0Aimproving%20MPJPE%20by%20approximately%20more%20than%204.5%20cm%20%2843%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19169v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventEgoHands%253A%2520Event-based%2520Egocentric%25203D%2520Hand%2520Mesh%2520Reconstruction%26entry.906535625%3DRyosei%2520Hara%2520and%2520Wataru%2520Ikeda%2520and%2520Masashi%2520Hatano%2520and%2520Mariko%2520Isogawa%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520hand%2520mesh%2520is%2520challenging%2520but%2520an%2520important%2520task%2520for%250Ahuman-computer%2520interaction%2520and%2520AR/VR%2520applications.%2520In%2520particular%252C%2520RGB%2520and/or%250Adepth%2520cameras%2520have%2520been%2520widely%2520used%2520in%2520this%2520task.%2520However%252C%2520methods%2520using%2520these%250Aconventional%2520cameras%2520face%2520challenges%2520in%2520low-light%2520environments%2520and%2520during%250Amotion%2520blur.%2520Thus%252C%2520to%2520address%2520these%2520limitations%252C%2520event%2520cameras%2520have%2520been%250Aattracting%2520attention%2520in%2520recent%2520years%2520for%2520their%2520high%2520dynamic%2520range%2520and%2520high%250Atemporal%2520resolution.%2520Despite%2520their%2520advantages%252C%2520event%2520cameras%2520are%2520sensitive%2520to%250Abackground%2520noise%2520or%2520camera%2520motion%252C%2520which%2520has%2520limited%2520existing%2520studies%2520to%2520static%250Abackgrounds%2520and%2520fixed%2520cameras.%2520In%2520this%2520study%252C%2520we%2520propose%2520EventEgoHands%252C%2520a%2520novel%250Amethod%2520for%2520event-based%25203D%2520hand%2520mesh%2520reconstruction%2520in%2520an%2520egocentric%2520view.%2520Our%250Aapproach%2520introduces%2520a%2520Hand%2520Segmentation%2520Module%2520that%2520extracts%2520hand%2520regions%252C%250Aeffectively%2520mitigating%2520the%2520influence%2520of%2520dynamic%2520background%2520events.%2520We%2520evaluated%250Aour%2520approach%2520and%2520demonstrated%2520its%2520effectiveness%2520on%2520the%2520N-HOT3D%2520dataset%252C%250Aimproving%2520MPJPE%2520by%2520approximately%2520more%2520than%25204.5%2520cm%2520%252843%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19169v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventEgoHands%3A%20Event-based%20Egocentric%203D%20Hand%20Mesh%20Reconstruction&entry.906535625=Ryosei%20Hara%20and%20Wataru%20Ikeda%20and%20Masashi%20Hatano%20and%20Mariko%20Isogawa&entry.1292438233=%20%20Reconstructing%203D%20hand%20mesh%20is%20challenging%20but%20an%20important%20task%20for%0Ahuman-computer%20interaction%20and%20AR/VR%20applications.%20In%20particular%2C%20RGB%20and/or%0Adepth%20cameras%20have%20been%20widely%20used%20in%20this%20task.%20However%2C%20methods%20using%20these%0Aconventional%20cameras%20face%20challenges%20in%20low-light%20environments%20and%20during%0Amotion%20blur.%20Thus%2C%20to%20address%20these%20limitations%2C%20event%20cameras%20have%20been%0Aattracting%20attention%20in%20recent%20years%20for%20their%20high%20dynamic%20range%20and%20high%0Atemporal%20resolution.%20Despite%20their%20advantages%2C%20event%20cameras%20are%20sensitive%20to%0Abackground%20noise%20or%20camera%20motion%2C%20which%20has%20limited%20existing%20studies%20to%20static%0Abackgrounds%20and%20fixed%20cameras.%20In%20this%20study%2C%20we%20propose%20EventEgoHands%2C%20a%20novel%0Amethod%20for%20event-based%203D%20hand%20mesh%20reconstruction%20in%20an%20egocentric%20view.%20Our%0Aapproach%20introduces%20a%20Hand%20Segmentation%20Module%20that%20extracts%20hand%20regions%2C%0Aeffectively%20mitigating%20the%20influence%20of%20dynamic%20background%20events.%20We%20evaluated%0Aour%20approach%20and%20demonstrated%20its%20effectiveness%20on%20the%20N-HOT3D%20dataset%2C%0Aimproving%20MPJPE%20by%20approximately%20more%20than%204.5%20cm%20%2843%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19169v2&entry.124074799=Read"},
{"title": "Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices", "author": "Thibault de Surrel and Fabien Lotte and Sylvain Chevallier and Florian Yger", "abstract": "  Circular and non-flat data distributions are prevalent across diverse domains\nof data science, yet their specific geometric structures often remain\nunderutilized in machine learning frameworks. A principled approach to\naccounting for the underlying geometry of such data is pivotal, particularly\nwhen extending statistical models, like the pervasive Gaussian distribution. In\nthis work, we tackle those issue by focusing on the manifold of symmetric\npositive definite (SPD) matrices, a key focus in information geometry. We\nintroduce a non-isotropic wrapped Gaussian by leveraging the exponential map,\nwe derive theoretical properties of this distribution and propose a maximum\nlikelihood framework for parameter estimation. Furthermore, we reinterpret\nestablished classifiers on SPD through a probabilistic lens and introduce new\nclassifiers based on the wrapped Gaussian model. Experiments on synthetic and\nreal-world datasets demonstrate the robustness and flexibility of this\ngeometry-aware distribution, underscoring its potential to advance\nmanifold-based data analysis. This work lays the groundwork for extending\nclassical machine learning and statistical methods to more complex and\nstructured data.\n", "link": "http://arxiv.org/abs/2502.01512v3", "date": "2025-05-27", "relevancy": 2.7566, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.564}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5487}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wrapped%20Gaussian%20on%20the%20manifold%20of%20Symmetric%20Positive%20Definite%20Matrices&body=Title%3A%20Wrapped%20Gaussian%20on%20the%20manifold%20of%20Symmetric%20Positive%20Definite%20Matrices%0AAuthor%3A%20Thibault%20de%20Surrel%20and%20Fabien%20Lotte%20and%20Sylvain%20Chevallier%20and%20Florian%20Yger%0AAbstract%3A%20%20%20Circular%20and%20non-flat%20data%20distributions%20are%20prevalent%20across%20diverse%20domains%0Aof%20data%20science%2C%20yet%20their%20specific%20geometric%20structures%20often%20remain%0Aunderutilized%20in%20machine%20learning%20frameworks.%20A%20principled%20approach%20to%0Aaccounting%20for%20the%20underlying%20geometry%20of%20such%20data%20is%20pivotal%2C%20particularly%0Awhen%20extending%20statistical%20models%2C%20like%20the%20pervasive%20Gaussian%20distribution.%20In%0Athis%20work%2C%20we%20tackle%20those%20issue%20by%20focusing%20on%20the%20manifold%20of%20symmetric%0Apositive%20definite%20%28SPD%29%20matrices%2C%20a%20key%20focus%20in%20information%20geometry.%20We%0Aintroduce%20a%20non-isotropic%20wrapped%20Gaussian%20by%20leveraging%20the%20exponential%20map%2C%0Awe%20derive%20theoretical%20properties%20of%20this%20distribution%20and%20propose%20a%20maximum%0Alikelihood%20framework%20for%20parameter%20estimation.%20Furthermore%2C%20we%20reinterpret%0Aestablished%20classifiers%20on%20SPD%20through%20a%20probabilistic%20lens%20and%20introduce%20new%0Aclassifiers%20based%20on%20the%20wrapped%20Gaussian%20model.%20Experiments%20on%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20the%20robustness%20and%20flexibility%20of%20this%0Ageometry-aware%20distribution%2C%20underscoring%20its%20potential%20to%20advance%0Amanifold-based%20data%20analysis.%20This%20work%20lays%20the%20groundwork%20for%20extending%0Aclassical%20machine%20learning%20and%20statistical%20methods%20to%20more%20complex%20and%0Astructured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01512v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWrapped%2520Gaussian%2520on%2520the%2520manifold%2520of%2520Symmetric%2520Positive%2520Definite%2520Matrices%26entry.906535625%3DThibault%2520de%2520Surrel%2520and%2520Fabien%2520Lotte%2520and%2520Sylvain%2520Chevallier%2520and%2520Florian%2520Yger%26entry.1292438233%3D%2520%2520Circular%2520and%2520non-flat%2520data%2520distributions%2520are%2520prevalent%2520across%2520diverse%2520domains%250Aof%2520data%2520science%252C%2520yet%2520their%2520specific%2520geometric%2520structures%2520often%2520remain%250Aunderutilized%2520in%2520machine%2520learning%2520frameworks.%2520A%2520principled%2520approach%2520to%250Aaccounting%2520for%2520the%2520underlying%2520geometry%2520of%2520such%2520data%2520is%2520pivotal%252C%2520particularly%250Awhen%2520extending%2520statistical%2520models%252C%2520like%2520the%2520pervasive%2520Gaussian%2520distribution.%2520In%250Athis%2520work%252C%2520we%2520tackle%2520those%2520issue%2520by%2520focusing%2520on%2520the%2520manifold%2520of%2520symmetric%250Apositive%2520definite%2520%2528SPD%2529%2520matrices%252C%2520a%2520key%2520focus%2520in%2520information%2520geometry.%2520We%250Aintroduce%2520a%2520non-isotropic%2520wrapped%2520Gaussian%2520by%2520leveraging%2520the%2520exponential%2520map%252C%250Awe%2520derive%2520theoretical%2520properties%2520of%2520this%2520distribution%2520and%2520propose%2520a%2520maximum%250Alikelihood%2520framework%2520for%2520parameter%2520estimation.%2520Furthermore%252C%2520we%2520reinterpret%250Aestablished%2520classifiers%2520on%2520SPD%2520through%2520a%2520probabilistic%2520lens%2520and%2520introduce%2520new%250Aclassifiers%2520based%2520on%2520the%2520wrapped%2520Gaussian%2520model.%2520Experiments%2520on%2520synthetic%2520and%250Areal-world%2520datasets%2520demonstrate%2520the%2520robustness%2520and%2520flexibility%2520of%2520this%250Ageometry-aware%2520distribution%252C%2520underscoring%2520its%2520potential%2520to%2520advance%250Amanifold-based%2520data%2520analysis.%2520This%2520work%2520lays%2520the%2520groundwork%2520for%2520extending%250Aclassical%2520machine%2520learning%2520and%2520statistical%2520methods%2520to%2520more%2520complex%2520and%250Astructured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01512v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wrapped%20Gaussian%20on%20the%20manifold%20of%20Symmetric%20Positive%20Definite%20Matrices&entry.906535625=Thibault%20de%20Surrel%20and%20Fabien%20Lotte%20and%20Sylvain%20Chevallier%20and%20Florian%20Yger&entry.1292438233=%20%20Circular%20and%20non-flat%20data%20distributions%20are%20prevalent%20across%20diverse%20domains%0Aof%20data%20science%2C%20yet%20their%20specific%20geometric%20structures%20often%20remain%0Aunderutilized%20in%20machine%20learning%20frameworks.%20A%20principled%20approach%20to%0Aaccounting%20for%20the%20underlying%20geometry%20of%20such%20data%20is%20pivotal%2C%20particularly%0Awhen%20extending%20statistical%20models%2C%20like%20the%20pervasive%20Gaussian%20distribution.%20In%0Athis%20work%2C%20we%20tackle%20those%20issue%20by%20focusing%20on%20the%20manifold%20of%20symmetric%0Apositive%20definite%20%28SPD%29%20matrices%2C%20a%20key%20focus%20in%20information%20geometry.%20We%0Aintroduce%20a%20non-isotropic%20wrapped%20Gaussian%20by%20leveraging%20the%20exponential%20map%2C%0Awe%20derive%20theoretical%20properties%20of%20this%20distribution%20and%20propose%20a%20maximum%0Alikelihood%20framework%20for%20parameter%20estimation.%20Furthermore%2C%20we%20reinterpret%0Aestablished%20classifiers%20on%20SPD%20through%20a%20probabilistic%20lens%20and%20introduce%20new%0Aclassifiers%20based%20on%20the%20wrapped%20Gaussian%20model.%20Experiments%20on%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20the%20robustness%20and%20flexibility%20of%20this%0Ageometry-aware%20distribution%2C%20underscoring%20its%20potential%20to%20advance%0Amanifold-based%20data%20analysis.%20This%20work%20lays%20the%20groundwork%20for%20extending%0Aclassical%20machine%20learning%20and%20statistical%20methods%20to%20more%20complex%20and%0Astructured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01512v3&entry.124074799=Read"},
{"title": "Bringing Objects to Life: training-free 4D generation from 3D objects\n  through view consistent noise", "author": "Ohad Rahamim and Ori Malca and Dvir Samuel and Gal Chechik", "abstract": "  Recent advancements in generative models have enabled the creation of dynamic\n4D content - 3D objects in motion - based on text prompts, which holds\npotential for applications in virtual worlds, media, and gaming. Existing\nmethods provide control over the appearance of generated content, including the\nability to animate 3D objects. However, their ability to generate dynamics is\nlimited to the mesh datasets they were trained on, lacking any growth or\nstructural development capability. In this work, we introduce a training-free\nmethod for animating 3D objects by conditioning on textual prompts to guide 4D\ngeneration, enabling custom general scenes while maintaining the original\nobject's identity. We first convert a 3D mesh into a static 4D Neural Radiance\nField (NeRF) that preserves the object's visual attributes. Then, we animate\nthe object using an Image-to-Video diffusion model driven by text. To improve\nmotion realism, we introduce a view-consistent noising protocol that aligns\nobject perspectives with the noising process to promote lifelike movement, and\na masked Score Distillation Sampling (SDS) loss that leverages attention maps\nto focus optimization on relevant regions, better preserving the original\nobject. We evaluate our model on two different 3D object datasets for temporal\ncoherence, prompt adherence, and visual fidelity, and find that our method\noutperforms the baseline based on multiview training, achieving better\nconsistency with the textual prompt in hard scenarios.\n", "link": "http://arxiv.org/abs/2412.20422v2", "date": "2025-05-27", "relevancy": 2.7471, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6972}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6876}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bringing%20Objects%20to%20Life%3A%20training-free%204D%20generation%20from%203D%20objects%0A%20%20through%20view%20consistent%20noise&body=Title%3A%20Bringing%20Objects%20to%20Life%3A%20training-free%204D%20generation%20from%203D%20objects%0A%20%20through%20view%20consistent%20noise%0AAuthor%3A%20Ohad%20Rahamim%20and%20Ori%20Malca%20and%20Dvir%20Samuel%20and%20Gal%20Chechik%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20models%20have%20enabled%20the%20creation%20of%20dynamic%0A4D%20content%20-%203D%20objects%20in%20motion%20-%20based%20on%20text%20prompts%2C%20which%20holds%0Apotential%20for%20applications%20in%20virtual%20worlds%2C%20media%2C%20and%20gaming.%20Existing%0Amethods%20provide%20control%20over%20the%20appearance%20of%20generated%20content%2C%20including%20the%0Aability%20to%20animate%203D%20objects.%20However%2C%20their%20ability%20to%20generate%20dynamics%20is%0Alimited%20to%20the%20mesh%20datasets%20they%20were%20trained%20on%2C%20lacking%20any%20growth%20or%0Astructural%20development%20capability.%20In%20this%20work%2C%20we%20introduce%20a%20training-free%0Amethod%20for%20animating%203D%20objects%20by%20conditioning%20on%20textual%20prompts%20to%20guide%204D%0Ageneration%2C%20enabling%20custom%20general%20scenes%20while%20maintaining%20the%20original%0Aobject%27s%20identity.%20We%20first%20convert%20a%203D%20mesh%20into%20a%20static%204D%20Neural%20Radiance%0AField%20%28NeRF%29%20that%20preserves%20the%20object%27s%20visual%20attributes.%20Then%2C%20we%20animate%0Athe%20object%20using%20an%20Image-to-Video%20diffusion%20model%20driven%20by%20text.%20To%20improve%0Amotion%20realism%2C%20we%20introduce%20a%20view-consistent%20noising%20protocol%20that%20aligns%0Aobject%20perspectives%20with%20the%20noising%20process%20to%20promote%20lifelike%20movement%2C%20and%0Aa%20masked%20Score%20Distillation%20Sampling%20%28SDS%29%20loss%20that%20leverages%20attention%20maps%0Ato%20focus%20optimization%20on%20relevant%20regions%2C%20better%20preserving%20the%20original%0Aobject.%20We%20evaluate%20our%20model%20on%20two%20different%203D%20object%20datasets%20for%20temporal%0Acoherence%2C%20prompt%20adherence%2C%20and%20visual%20fidelity%2C%20and%20find%20that%20our%20method%0Aoutperforms%20the%20baseline%20based%20on%20multiview%20training%2C%20achieving%20better%0Aconsistency%20with%20the%20textual%20prompt%20in%20hard%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBringing%2520Objects%2520to%2520Life%253A%2520training-free%25204D%2520generation%2520from%25203D%2520objects%250A%2520%2520through%2520view%2520consistent%2520noise%26entry.906535625%3DOhad%2520Rahamim%2520and%2520Ori%2520Malca%2520and%2520Dvir%2520Samuel%2520and%2520Gal%2520Chechik%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520models%2520have%2520enabled%2520the%2520creation%2520of%2520dynamic%250A4D%2520content%2520-%25203D%2520objects%2520in%2520motion%2520-%2520based%2520on%2520text%2520prompts%252C%2520which%2520holds%250Apotential%2520for%2520applications%2520in%2520virtual%2520worlds%252C%2520media%252C%2520and%2520gaming.%2520Existing%250Amethods%2520provide%2520control%2520over%2520the%2520appearance%2520of%2520generated%2520content%252C%2520including%2520the%250Aability%2520to%2520animate%25203D%2520objects.%2520However%252C%2520their%2520ability%2520to%2520generate%2520dynamics%2520is%250Alimited%2520to%2520the%2520mesh%2520datasets%2520they%2520were%2520trained%2520on%252C%2520lacking%2520any%2520growth%2520or%250Astructural%2520development%2520capability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520training-free%250Amethod%2520for%2520animating%25203D%2520objects%2520by%2520conditioning%2520on%2520textual%2520prompts%2520to%2520guide%25204D%250Ageneration%252C%2520enabling%2520custom%2520general%2520scenes%2520while%2520maintaining%2520the%2520original%250Aobject%2527s%2520identity.%2520We%2520first%2520convert%2520a%25203D%2520mesh%2520into%2520a%2520static%25204D%2520Neural%2520Radiance%250AField%2520%2528NeRF%2529%2520that%2520preserves%2520the%2520object%2527s%2520visual%2520attributes.%2520Then%252C%2520we%2520animate%250Athe%2520object%2520using%2520an%2520Image-to-Video%2520diffusion%2520model%2520driven%2520by%2520text.%2520To%2520improve%250Amotion%2520realism%252C%2520we%2520introduce%2520a%2520view-consistent%2520noising%2520protocol%2520that%2520aligns%250Aobject%2520perspectives%2520with%2520the%2520noising%2520process%2520to%2520promote%2520lifelike%2520movement%252C%2520and%250Aa%2520masked%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520loss%2520that%2520leverages%2520attention%2520maps%250Ato%2520focus%2520optimization%2520on%2520relevant%2520regions%252C%2520better%2520preserving%2520the%2520original%250Aobject.%2520We%2520evaluate%2520our%2520model%2520on%2520two%2520different%25203D%2520object%2520datasets%2520for%2520temporal%250Acoherence%252C%2520prompt%2520adherence%252C%2520and%2520visual%2520fidelity%252C%2520and%2520find%2520that%2520our%2520method%250Aoutperforms%2520the%2520baseline%2520based%2520on%2520multiview%2520training%252C%2520achieving%2520better%250Aconsistency%2520with%2520the%2520textual%2520prompt%2520in%2520hard%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bringing%20Objects%20to%20Life%3A%20training-free%204D%20generation%20from%203D%20objects%0A%20%20through%20view%20consistent%20noise&entry.906535625=Ohad%20Rahamim%20and%20Ori%20Malca%20and%20Dvir%20Samuel%20and%20Gal%20Chechik&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20models%20have%20enabled%20the%20creation%20of%20dynamic%0A4D%20content%20-%203D%20objects%20in%20motion%20-%20based%20on%20text%20prompts%2C%20which%20holds%0Apotential%20for%20applications%20in%20virtual%20worlds%2C%20media%2C%20and%20gaming.%20Existing%0Amethods%20provide%20control%20over%20the%20appearance%20of%20generated%20content%2C%20including%20the%0Aability%20to%20animate%203D%20objects.%20However%2C%20their%20ability%20to%20generate%20dynamics%20is%0Alimited%20to%20the%20mesh%20datasets%20they%20were%20trained%20on%2C%20lacking%20any%20growth%20or%0Astructural%20development%20capability.%20In%20this%20work%2C%20we%20introduce%20a%20training-free%0Amethod%20for%20animating%203D%20objects%20by%20conditioning%20on%20textual%20prompts%20to%20guide%204D%0Ageneration%2C%20enabling%20custom%20general%20scenes%20while%20maintaining%20the%20original%0Aobject%27s%20identity.%20We%20first%20convert%20a%203D%20mesh%20into%20a%20static%204D%20Neural%20Radiance%0AField%20%28NeRF%29%20that%20preserves%20the%20object%27s%20visual%20attributes.%20Then%2C%20we%20animate%0Athe%20object%20using%20an%20Image-to-Video%20diffusion%20model%20driven%20by%20text.%20To%20improve%0Amotion%20realism%2C%20we%20introduce%20a%20view-consistent%20noising%20protocol%20that%20aligns%0Aobject%20perspectives%20with%20the%20noising%20process%20to%20promote%20lifelike%20movement%2C%20and%0Aa%20masked%20Score%20Distillation%20Sampling%20%28SDS%29%20loss%20that%20leverages%20attention%20maps%0Ato%20focus%20optimization%20on%20relevant%20regions%2C%20better%20preserving%20the%20original%0Aobject.%20We%20evaluate%20our%20model%20on%20two%20different%203D%20object%20datasets%20for%20temporal%0Acoherence%2C%20prompt%20adherence%2C%20and%20visual%20fidelity%2C%20and%20find%20that%20our%20method%0Aoutperforms%20the%20baseline%20based%20on%20multiview%20training%2C%20achieving%20better%0Aconsistency%20with%20the%20textual%20prompt%20in%20hard%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20422v2&entry.124074799=Read"},
{"title": "Mitigating Hallucination in Large Vision-Language Models via Adaptive\n  Attention Calibration", "author": "Mehrdad Fazli and Bowen Wei and Ziwei Zhu", "abstract": "  Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination.\n", "link": "http://arxiv.org/abs/2505.21472v1", "date": "2025-05-27", "relevancy": 2.7158, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20via%20Adaptive%0A%20%20Attention%20Calibration&body=Title%3A%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20via%20Adaptive%0A%20%20Attention%20Calibration%0AAuthor%3A%20Mehrdad%20Fazli%20and%20Bowen%20Wei%20and%20Ziwei%20Zhu%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20achieve%20impressive%20performance%20on%0Amultimodal%20tasks%20but%20often%20suffer%20from%20hallucination%2C%20and%20confidently%20describe%0Aobjects%20or%20attributes%20not%20present%20in%20the%20image.%20Current%20inference-time%0Ainterventions%2C%20while%20training-free%2C%20struggle%20to%20maintain%20accuracy%20in%20open-ended%0Aand%20long-form%20generation%20scenarios.%20We%20introduce%20the%20Confidence-Aware%20Attention%0ACalibration%20%28CAAC%29%20framework%20to%20address%20this%20challenge%20by%20targeting%20two%20key%0Abiases%3A%20spatial%20perception%20bias%2C%20which%20distributes%20attention%20disproportionately%0Aacross%20image%20tokens%2C%20and%20modality%20bias%2C%20which%20shifts%20focus%20from%20visual%20to%0Atextual%20inputs%20over%20time.%20CAAC%20employs%20a%20two-step%20approach%3A%20Visual-Token%0ACalibration%20%28VTC%29%20to%20balance%20attention%20across%20visual%20tokens%2C%20and%20Adaptive%0AAttention%20Re-Scaling%20%28AAR%29%20to%20reinforce%20visual%20grounding%20based%20on%20the%20model%27s%0Aconfidence.%20This%20confidence-driven%20adjustment%20ensures%20consistent%20visual%0Aalignment%20during%20generation.%20Experiments%20on%20CHAIR%2C%20AMBER%2C%20and%20POPE%20benchmarks%0Ademonstrate%20that%20CAAC%20outperforms%20baselines%2C%20particularly%20in%20long-form%0Agenerations%2C%20effectively%20reducing%20hallucination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucination%2520in%2520Large%2520Vision-Language%2520Models%2520via%2520Adaptive%250A%2520%2520Attention%2520Calibration%26entry.906535625%3DMehrdad%2520Fazli%2520and%2520Bowen%2520Wei%2520and%2520Ziwei%2520Zhu%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520achieve%2520impressive%2520performance%2520on%250Amultimodal%2520tasks%2520but%2520often%2520suffer%2520from%2520hallucination%252C%2520and%2520confidently%2520describe%250Aobjects%2520or%2520attributes%2520not%2520present%2520in%2520the%2520image.%2520Current%2520inference-time%250Ainterventions%252C%2520while%2520training-free%252C%2520struggle%2520to%2520maintain%2520accuracy%2520in%2520open-ended%250Aand%2520long-form%2520generation%2520scenarios.%2520We%2520introduce%2520the%2520Confidence-Aware%2520Attention%250ACalibration%2520%2528CAAC%2529%2520framework%2520to%2520address%2520this%2520challenge%2520by%2520targeting%2520two%2520key%250Abiases%253A%2520spatial%2520perception%2520bias%252C%2520which%2520distributes%2520attention%2520disproportionately%250Aacross%2520image%2520tokens%252C%2520and%2520modality%2520bias%252C%2520which%2520shifts%2520focus%2520from%2520visual%2520to%250Atextual%2520inputs%2520over%2520time.%2520CAAC%2520employs%2520a%2520two-step%2520approach%253A%2520Visual-Token%250ACalibration%2520%2528VTC%2529%2520to%2520balance%2520attention%2520across%2520visual%2520tokens%252C%2520and%2520Adaptive%250AAttention%2520Re-Scaling%2520%2528AAR%2529%2520to%2520reinforce%2520visual%2520grounding%2520based%2520on%2520the%2520model%2527s%250Aconfidence.%2520This%2520confidence-driven%2520adjustment%2520ensures%2520consistent%2520visual%250Aalignment%2520during%2520generation.%2520Experiments%2520on%2520CHAIR%252C%2520AMBER%252C%2520and%2520POPE%2520benchmarks%250Ademonstrate%2520that%2520CAAC%2520outperforms%2520baselines%252C%2520particularly%2520in%2520long-form%250Agenerations%252C%2520effectively%2520reducing%2520hallucination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20via%20Adaptive%0A%20%20Attention%20Calibration&entry.906535625=Mehrdad%20Fazli%20and%20Bowen%20Wei%20and%20Ziwei%20Zhu&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20achieve%20impressive%20performance%20on%0Amultimodal%20tasks%20but%20often%20suffer%20from%20hallucination%2C%20and%20confidently%20describe%0Aobjects%20or%20attributes%20not%20present%20in%20the%20image.%20Current%20inference-time%0Ainterventions%2C%20while%20training-free%2C%20struggle%20to%20maintain%20accuracy%20in%20open-ended%0Aand%20long-form%20generation%20scenarios.%20We%20introduce%20the%20Confidence-Aware%20Attention%0ACalibration%20%28CAAC%29%20framework%20to%20address%20this%20challenge%20by%20targeting%20two%20key%0Abiases%3A%20spatial%20perception%20bias%2C%20which%20distributes%20attention%20disproportionately%0Aacross%20image%20tokens%2C%20and%20modality%20bias%2C%20which%20shifts%20focus%20from%20visual%20to%0Atextual%20inputs%20over%20time.%20CAAC%20employs%20a%20two-step%20approach%3A%20Visual-Token%0ACalibration%20%28VTC%29%20to%20balance%20attention%20across%20visual%20tokens%2C%20and%20Adaptive%0AAttention%20Re-Scaling%20%28AAR%29%20to%20reinforce%20visual%20grounding%20based%20on%20the%20model%27s%0Aconfidence.%20This%20confidence-driven%20adjustment%20ensures%20consistent%20visual%0Aalignment%20during%20generation.%20Experiments%20on%20CHAIR%2C%20AMBER%2C%20and%20POPE%20benchmarks%0Ademonstrate%20that%20CAAC%20outperforms%20baselines%2C%20particularly%20in%20long-form%0Agenerations%2C%20effectively%20reducing%20hallucination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21472v1&entry.124074799=Read"},
{"title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal\n  Alignment", "author": "Xiaojun Jia and Sensen Gao and Simeng Qin and Tianyu Pang and Chao Du and Yihao Huang and Xinfeng Li and Yiming Li and Bo Li and Yang Liu", "abstract": "  Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.\n", "link": "http://arxiv.org/abs/2505.21494v1", "date": "2025-05-27", "relevancy": 2.7026, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5805}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5442}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20against%20Closed-Source%20MLLMs%20via%20Feature%20Optimal%0A%20%20Alignment&body=Title%3A%20Adversarial%20Attacks%20against%20Closed-Source%20MLLMs%20via%20Feature%20Optimal%0A%20%20Alignment%0AAuthor%3A%20Xiaojun%20Jia%20and%20Sensen%20Gao%20and%20Simeng%20Qin%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Yihao%20Huang%20and%20Xinfeng%20Li%20and%20Yiming%20Li%20and%20Bo%20Li%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20remain%20vulnerable%20to%20transferable%0Aadversarial%20examples.%20While%20existing%20methods%20typically%20achieve%20targeted%20attacks%0Aby%20aligning%20global%20features-such%20as%20CLIP%27s%20%5BCLS%5D%20token-between%20adversarial%20and%0Atarget%20samples%2C%20they%20often%20overlook%20the%20rich%20local%20information%20encoded%20in%20patch%0Atokens.%20This%20leads%20to%20suboptimal%20alignment%20and%20limited%20transferability%2C%0Aparticularly%20for%20closed-source%20models.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Atargeted%20transferable%20adversarial%20attack%20method%20based%20on%20feature%20optimal%0Aalignment%2C%20called%20FOA-Attack%2C%20to%20improve%20adversarial%20transfer%20capability.%0ASpecifically%2C%20at%20the%20global%20level%2C%20we%20introduce%20a%20global%20feature%20loss%20based%20on%0Acosine%20similarity%20to%20align%20the%20coarse-grained%20features%20of%20adversarial%20samples%0Awith%20those%20of%20target%20samples.%20At%20the%20local%20level%2C%20given%20the%20rich%20local%0Arepresentations%20within%20Transformers%2C%20we%20leverage%20clustering%20techniques%20to%0Aextract%20compact%20local%20patterns%20to%20alleviate%20redundant%20local%20features.%20We%20then%0Aformulate%20local%20feature%20alignment%20between%20adversarial%20and%20target%20samples%20as%20an%0Aoptimal%20transport%20%28OT%29%20problem%20and%20propose%20a%20local%20clustering%20optimal%20transport%0Aloss%20to%20refine%20fine-grained%20feature%20alignment.%20Additionally%2C%20we%20propose%20a%0Adynamic%20ensemble%20model%20weighting%20strategy%20to%20adaptively%20balance%20the%20influence%0Aof%20multiple%20models%20during%20adversarial%20example%20generation%2C%20thereby%20further%0Aimproving%20transferability.%20Extensive%20experiments%20across%20various%20models%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%2C%20outperforming%0Astate-of-the-art%20methods%2C%20especially%20in%20transferring%20to%20closed-source%20MLLMs.%0AThe%20code%20is%20released%20at%20https%3A//github.com/jiaxiaojunQAQ/FOA-Attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attacks%2520against%2520Closed-Source%2520MLLMs%2520via%2520Feature%2520Optimal%250A%2520%2520Alignment%26entry.906535625%3DXiaojun%2520Jia%2520and%2520Sensen%2520Gao%2520and%2520Simeng%2520Qin%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Yihao%2520Huang%2520and%2520Xinfeng%2520Li%2520and%2520Yiming%2520Li%2520and%2520Bo%2520Li%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520remain%2520vulnerable%2520to%2520transferable%250Aadversarial%2520examples.%2520While%2520existing%2520methods%2520typically%2520achieve%2520targeted%2520attacks%250Aby%2520aligning%2520global%2520features-such%2520as%2520CLIP%2527s%2520%255BCLS%255D%2520token-between%2520adversarial%2520and%250Atarget%2520samples%252C%2520they%2520often%2520overlook%2520the%2520rich%2520local%2520information%2520encoded%2520in%2520patch%250Atokens.%2520This%2520leads%2520to%2520suboptimal%2520alignment%2520and%2520limited%2520transferability%252C%250Aparticularly%2520for%2520closed-source%2520models.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%250Atargeted%2520transferable%2520adversarial%2520attack%2520method%2520based%2520on%2520feature%2520optimal%250Aalignment%252C%2520called%2520FOA-Attack%252C%2520to%2520improve%2520adversarial%2520transfer%2520capability.%250ASpecifically%252C%2520at%2520the%2520global%2520level%252C%2520we%2520introduce%2520a%2520global%2520feature%2520loss%2520based%2520on%250Acosine%2520similarity%2520to%2520align%2520the%2520coarse-grained%2520features%2520of%2520adversarial%2520samples%250Awith%2520those%2520of%2520target%2520samples.%2520At%2520the%2520local%2520level%252C%2520given%2520the%2520rich%2520local%250Arepresentations%2520within%2520Transformers%252C%2520we%2520leverage%2520clustering%2520techniques%2520to%250Aextract%2520compact%2520local%2520patterns%2520to%2520alleviate%2520redundant%2520local%2520features.%2520We%2520then%250Aformulate%2520local%2520feature%2520alignment%2520between%2520adversarial%2520and%2520target%2520samples%2520as%2520an%250Aoptimal%2520transport%2520%2528OT%2529%2520problem%2520and%2520propose%2520a%2520local%2520clustering%2520optimal%2520transport%250Aloss%2520to%2520refine%2520fine-grained%2520feature%2520alignment.%2520Additionally%252C%2520we%2520propose%2520a%250Adynamic%2520ensemble%2520model%2520weighting%2520strategy%2520to%2520adaptively%2520balance%2520the%2520influence%250Aof%2520multiple%2520models%2520during%2520adversarial%2520example%2520generation%252C%2520thereby%2520further%250Aimproving%2520transferability.%2520Extensive%2520experiments%2520across%2520various%2520models%250Ademonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520method%252C%2520outperforming%250Astate-of-the-art%2520methods%252C%2520especially%2520in%2520transferring%2520to%2520closed-source%2520MLLMs.%250AThe%2520code%2520is%2520released%2520at%2520https%253A//github.com/jiaxiaojunQAQ/FOA-Attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20against%20Closed-Source%20MLLMs%20via%20Feature%20Optimal%0A%20%20Alignment&entry.906535625=Xiaojun%20Jia%20and%20Sensen%20Gao%20and%20Simeng%20Qin%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Yihao%20Huang%20and%20Xinfeng%20Li%20and%20Yiming%20Li%20and%20Bo%20Li%20and%20Yang%20Liu&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20remain%20vulnerable%20to%20transferable%0Aadversarial%20examples.%20While%20existing%20methods%20typically%20achieve%20targeted%20attacks%0Aby%20aligning%20global%20features-such%20as%20CLIP%27s%20%5BCLS%5D%20token-between%20adversarial%20and%0Atarget%20samples%2C%20they%20often%20overlook%20the%20rich%20local%20information%20encoded%20in%20patch%0Atokens.%20This%20leads%20to%20suboptimal%20alignment%20and%20limited%20transferability%2C%0Aparticularly%20for%20closed-source%20models.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Atargeted%20transferable%20adversarial%20attack%20method%20based%20on%20feature%20optimal%0Aalignment%2C%20called%20FOA-Attack%2C%20to%20improve%20adversarial%20transfer%20capability.%0ASpecifically%2C%20at%20the%20global%20level%2C%20we%20introduce%20a%20global%20feature%20loss%20based%20on%0Acosine%20similarity%20to%20align%20the%20coarse-grained%20features%20of%20adversarial%20samples%0Awith%20those%20of%20target%20samples.%20At%20the%20local%20level%2C%20given%20the%20rich%20local%0Arepresentations%20within%20Transformers%2C%20we%20leverage%20clustering%20techniques%20to%0Aextract%20compact%20local%20patterns%20to%20alleviate%20redundant%20local%20features.%20We%20then%0Aformulate%20local%20feature%20alignment%20between%20adversarial%20and%20target%20samples%20as%20an%0Aoptimal%20transport%20%28OT%29%20problem%20and%20propose%20a%20local%20clustering%20optimal%20transport%0Aloss%20to%20refine%20fine-grained%20feature%20alignment.%20Additionally%2C%20we%20propose%20a%0Adynamic%20ensemble%20model%20weighting%20strategy%20to%20adaptively%20balance%20the%20influence%0Aof%20multiple%20models%20during%20adversarial%20example%20generation%2C%20thereby%20further%0Aimproving%20transferability.%20Extensive%20experiments%20across%20various%20models%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%2C%20outperforming%0Astate-of-the-art%20methods%2C%20especially%20in%20transferring%20to%20closed-source%20MLLMs.%0AThe%20code%20is%20released%20at%20https%3A//github.com/jiaxiaojunQAQ/FOA-Attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21494v1&entry.124074799=Read"},
{"title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving\n  Video Virtual Try-on", "author": "Guangyuan Li and Siming Zheng and Hao Zhang and Jinwei Chen and Junsheng Luan and Binkai Ou and Lei Zhao and Bo Li and Peng-Tao Jiang", "abstract": "  Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer.We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios.\n", "link": "http://arxiv.org/abs/2505.21325v1", "date": "2025-05-27", "relevancy": 2.6996, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6774}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6768}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicTryOn%3A%20Harnessing%20Diffusion%20Transformer%20for%20Garment-Preserving%0A%20%20Video%20Virtual%20Try-on&body=Title%3A%20MagicTryOn%3A%20Harnessing%20Diffusion%20Transformer%20for%20Garment-Preserving%0A%20%20Video%20Virtual%20Try-on%0AAuthor%3A%20Guangyuan%20Li%20and%20Siming%20Zheng%20and%20Hao%20Zhang%20and%20Jinwei%20Chen%20and%20Junsheng%20Luan%20and%20Binkai%20Ou%20and%20Lei%20Zhao%20and%20Bo%20Li%20and%20Peng-Tao%20Jiang%0AAbstract%3A%20%20%20Video%20Virtual%20Try-On%20%28VVT%29%20aims%20to%20simulate%20the%20natural%20appearance%20of%0Agarments%20across%20consecutive%20video%20frames%2C%20capturing%20their%20dynamic%20variations%0Aand%20interactions%20with%20human%20body%20motion.%20However%2C%20current%20VVT%20methods%20still%0Aface%20challenges%20in%20terms%20of%20spatiotemporal%20consistency%20and%20garment%20content%0Apreservation.%20First%2C%20they%20use%20diffusion%20models%20based%20on%20the%20U-Net%2C%20which%20are%0Alimited%20in%20their%20expressive%20capability%20and%20struggle%20to%20reconstruct%20complex%0Adetails.%20Second%2C%20they%20adopt%20a%20separative%20modeling%20approach%20for%20spatial%20and%0Atemporal%20attention%2C%20which%20hinders%20the%20effective%20capture%20of%20structural%0Arelationships%20and%20dynamic%20consistency%20across%20frames.%20Third%2C%20their%20expression%20of%0Agarment%20details%20remains%20insufficient%2C%20affecting%20the%20realism%20and%20stability%20of%0Athe%20overall%20synthesized%20results%2C%20especially%20during%20human%20motion.%20To%20address%20the%0Aabove%20challenges%2C%20we%20propose%20MagicTryOn%2C%20a%20video%20virtual%20try-on%20framework%20built%0Aupon%20the%20large-scale%20video%20diffusion%20Transformer.We%20replace%20the%20U-Net%0Aarchitecture%20with%20a%20diffusion%20Transformer%20and%20combine%20full%20self-attention%20to%0Ajointly%20model%20the%20spatiotemporal%20consistency%20of%20videos.%20We%20design%20a%0Acoarse-to-fine%20garment%20preservation%20strategy.%20The%20coarse%20strategy%20integrates%0Agarment%20tokens%20during%20the%20embedding%20stage%2C%20while%20the%20fine%20strategy%20incorporates%0Amultiple%20garment-based%20conditions%2C%20such%20as%20semantics%2C%20textures%2C%20and%20contour%0Alines%20during%20the%20denoising%20stage.%20Moreover%2C%20we%20introduce%20a%20mask-aware%20loss%20to%0Afurther%20optimize%20garment%20region%20fidelity.%20Extensive%20experiments%20on%20both%20image%0Aand%20video%20try-on%20datasets%20demonstrate%20that%20our%20method%20outperforms%20existing%20SOTA%0Amethods%20in%20comprehensive%20evaluations%20and%20generalizes%20to%20in-the-wild%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicTryOn%253A%2520Harnessing%2520Diffusion%2520Transformer%2520for%2520Garment-Preserving%250A%2520%2520Video%2520Virtual%2520Try-on%26entry.906535625%3DGuangyuan%2520Li%2520and%2520Siming%2520Zheng%2520and%2520Hao%2520Zhang%2520and%2520Jinwei%2520Chen%2520and%2520Junsheng%2520Luan%2520and%2520Binkai%2520Ou%2520and%2520Lei%2520Zhao%2520and%2520Bo%2520Li%2520and%2520Peng-Tao%2520Jiang%26entry.1292438233%3D%2520%2520Video%2520Virtual%2520Try-On%2520%2528VVT%2529%2520aims%2520to%2520simulate%2520the%2520natural%2520appearance%2520of%250Agarments%2520across%2520consecutive%2520video%2520frames%252C%2520capturing%2520their%2520dynamic%2520variations%250Aand%2520interactions%2520with%2520human%2520body%2520motion.%2520However%252C%2520current%2520VVT%2520methods%2520still%250Aface%2520challenges%2520in%2520terms%2520of%2520spatiotemporal%2520consistency%2520and%2520garment%2520content%250Apreservation.%2520First%252C%2520they%2520use%2520diffusion%2520models%2520based%2520on%2520the%2520U-Net%252C%2520which%2520are%250Alimited%2520in%2520their%2520expressive%2520capability%2520and%2520struggle%2520to%2520reconstruct%2520complex%250Adetails.%2520Second%252C%2520they%2520adopt%2520a%2520separative%2520modeling%2520approach%2520for%2520spatial%2520and%250Atemporal%2520attention%252C%2520which%2520hinders%2520the%2520effective%2520capture%2520of%2520structural%250Arelationships%2520and%2520dynamic%2520consistency%2520across%2520frames.%2520Third%252C%2520their%2520expression%2520of%250Agarment%2520details%2520remains%2520insufficient%252C%2520affecting%2520the%2520realism%2520and%2520stability%2520of%250Athe%2520overall%2520synthesized%2520results%252C%2520especially%2520during%2520human%2520motion.%2520To%2520address%2520the%250Aabove%2520challenges%252C%2520we%2520propose%2520MagicTryOn%252C%2520a%2520video%2520virtual%2520try-on%2520framework%2520built%250Aupon%2520the%2520large-scale%2520video%2520diffusion%2520Transformer.We%2520replace%2520the%2520U-Net%250Aarchitecture%2520with%2520a%2520diffusion%2520Transformer%2520and%2520combine%2520full%2520self-attention%2520to%250Ajointly%2520model%2520the%2520spatiotemporal%2520consistency%2520of%2520videos.%2520We%2520design%2520a%250Acoarse-to-fine%2520garment%2520preservation%2520strategy.%2520The%2520coarse%2520strategy%2520integrates%250Agarment%2520tokens%2520during%2520the%2520embedding%2520stage%252C%2520while%2520the%2520fine%2520strategy%2520incorporates%250Amultiple%2520garment-based%2520conditions%252C%2520such%2520as%2520semantics%252C%2520textures%252C%2520and%2520contour%250Alines%2520during%2520the%2520denoising%2520stage.%2520Moreover%252C%2520we%2520introduce%2520a%2520mask-aware%2520loss%2520to%250Afurther%2520optimize%2520garment%2520region%2520fidelity.%2520Extensive%2520experiments%2520on%2520both%2520image%250Aand%2520video%2520try-on%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520SOTA%250Amethods%2520in%2520comprehensive%2520evaluations%2520and%2520generalizes%2520to%2520in-the-wild%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicTryOn%3A%20Harnessing%20Diffusion%20Transformer%20for%20Garment-Preserving%0A%20%20Video%20Virtual%20Try-on&entry.906535625=Guangyuan%20Li%20and%20Siming%20Zheng%20and%20Hao%20Zhang%20and%20Jinwei%20Chen%20and%20Junsheng%20Luan%20and%20Binkai%20Ou%20and%20Lei%20Zhao%20and%20Bo%20Li%20and%20Peng-Tao%20Jiang&entry.1292438233=%20%20Video%20Virtual%20Try-On%20%28VVT%29%20aims%20to%20simulate%20the%20natural%20appearance%20of%0Agarments%20across%20consecutive%20video%20frames%2C%20capturing%20their%20dynamic%20variations%0Aand%20interactions%20with%20human%20body%20motion.%20However%2C%20current%20VVT%20methods%20still%0Aface%20challenges%20in%20terms%20of%20spatiotemporal%20consistency%20and%20garment%20content%0Apreservation.%20First%2C%20they%20use%20diffusion%20models%20based%20on%20the%20U-Net%2C%20which%20are%0Alimited%20in%20their%20expressive%20capability%20and%20struggle%20to%20reconstruct%20complex%0Adetails.%20Second%2C%20they%20adopt%20a%20separative%20modeling%20approach%20for%20spatial%20and%0Atemporal%20attention%2C%20which%20hinders%20the%20effective%20capture%20of%20structural%0Arelationships%20and%20dynamic%20consistency%20across%20frames.%20Third%2C%20their%20expression%20of%0Agarment%20details%20remains%20insufficient%2C%20affecting%20the%20realism%20and%20stability%20of%0Athe%20overall%20synthesized%20results%2C%20especially%20during%20human%20motion.%20To%20address%20the%0Aabove%20challenges%2C%20we%20propose%20MagicTryOn%2C%20a%20video%20virtual%20try-on%20framework%20built%0Aupon%20the%20large-scale%20video%20diffusion%20Transformer.We%20replace%20the%20U-Net%0Aarchitecture%20with%20a%20diffusion%20Transformer%20and%20combine%20full%20self-attention%20to%0Ajointly%20model%20the%20spatiotemporal%20consistency%20of%20videos.%20We%20design%20a%0Acoarse-to-fine%20garment%20preservation%20strategy.%20The%20coarse%20strategy%20integrates%0Agarment%20tokens%20during%20the%20embedding%20stage%2C%20while%20the%20fine%20strategy%20incorporates%0Amultiple%20garment-based%20conditions%2C%20such%20as%20semantics%2C%20textures%2C%20and%20contour%0Alines%20during%20the%20denoising%20stage.%20Moreover%2C%20we%20introduce%20a%20mask-aware%20loss%20to%0Afurther%20optimize%20garment%20region%20fidelity.%20Extensive%20experiments%20on%20both%20image%0Aand%20video%20try-on%20datasets%20demonstrate%20that%20our%20method%20outperforms%20existing%20SOTA%0Amethods%20in%20comprehensive%20evaluations%20and%20generalizes%20to%20in-the-wild%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21325v1&entry.124074799=Read"},
{"title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud\n  Learning", "author": "Song Wang and Xiaolu Liu and Lingdong Kong and Jianyun Xu and Chunyong Hu and Gongfan Fang and Wentong Li and Jianke Zhu and Xinchao Wang", "abstract": "  Self-supervised representation learning for point cloud has demonstrated\neffectiveness in improving pre-trained model performance across diverse tasks.\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\ndownstream applications demands substantial computational and storage\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\nsolution to mitigate these resource requirements, yet most current approaches\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\nIn this paper, we propose PointLoRA, a simple yet effective method that\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\nwithin the most parameter-intensive components of point cloud transformers,\nreducing the need for tunable parameters while enhancing global feature\ncapture. Additionally, multi-scale token selection extracts critical local\ninformation to serve as prompts for downstream fine-tuning, effectively\ncomplementing the global context captured by LoRA. The experimental results\nacross various pre-trained models and three challenging public datasets\ndemonstrate that our approach achieves competitive performance with only 3.43%\nof the trainable parameters, making it highly effective for\nresource-constrained applications. Source code is available at:\nhttps://github.com/songw-zju/PointLoRA.\n", "link": "http://arxiv.org/abs/2504.16023v2", "date": "2025-05-27", "relevancy": 2.6982, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5517}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5436}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointLoRA%3A%20Low-Rank%20Adaptation%20with%20Token%20Selection%20for%20Point%20Cloud%0A%20%20Learning&body=Title%3A%20PointLoRA%3A%20Low-Rank%20Adaptation%20with%20Token%20Selection%20for%20Point%20Cloud%0A%20%20Learning%0AAuthor%3A%20Song%20Wang%20and%20Xiaolu%20Liu%20and%20Lingdong%20Kong%20and%20Jianyun%20Xu%20and%20Chunyong%20Hu%20and%20Gongfan%20Fang%20and%20Wentong%20Li%20and%20Jianke%20Zhu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Self-supervised%20representation%20learning%20for%20point%20cloud%20has%20demonstrated%0Aeffectiveness%20in%20improving%20pre-trained%20model%20performance%20across%20diverse%20tasks.%0AHowever%2C%20as%20pre-trained%20models%20grow%20in%20complexity%2C%20fully%20fine-tuning%20them%20for%0Adownstream%20applications%20demands%20substantial%20computational%20and%20storage%0Aresources.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20offer%20a%20promising%0Asolution%20to%20mitigate%20these%20resource%20requirements%2C%20yet%20most%20current%20approaches%0Arely%20on%20complex%20adapter%20and%20prompt%20mechanisms%20that%20increase%20tunable%20parameters.%0AIn%20this%20paper%2C%20we%20propose%20PointLoRA%2C%20a%20simple%20yet%20effective%20method%20that%0Acombines%20low-rank%20adaptation%20%28LoRA%29%20with%20multi-scale%20token%20selection%20to%0Aefficiently%20fine-tune%20point%20cloud%20models.%20Our%20approach%20embeds%20LoRA%20layers%0Awithin%20the%20most%20parameter-intensive%20components%20of%20point%20cloud%20transformers%2C%0Areducing%20the%20need%20for%20tunable%20parameters%20while%20enhancing%20global%20feature%0Acapture.%20Additionally%2C%20multi-scale%20token%20selection%20extracts%20critical%20local%0Ainformation%20to%20serve%20as%20prompts%20for%20downstream%20fine-tuning%2C%20effectively%0Acomplementing%20the%20global%20context%20captured%20by%20LoRA.%20The%20experimental%20results%0Aacross%20various%20pre-trained%20models%20and%20three%20challenging%20public%20datasets%0Ademonstrate%20that%20our%20approach%20achieves%20competitive%20performance%20with%20only%203.43%25%0Aof%20the%20trainable%20parameters%2C%20making%20it%20highly%20effective%20for%0Aresource-constrained%20applications.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/songw-zju/PointLoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16023v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointLoRA%253A%2520Low-Rank%2520Adaptation%2520with%2520Token%2520Selection%2520for%2520Point%2520Cloud%250A%2520%2520Learning%26entry.906535625%3DSong%2520Wang%2520and%2520Xiaolu%2520Liu%2520and%2520Lingdong%2520Kong%2520and%2520Jianyun%2520Xu%2520and%2520Chunyong%2520Hu%2520and%2520Gongfan%2520Fang%2520and%2520Wentong%2520Li%2520and%2520Jianke%2520Zhu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Self-supervised%2520representation%2520learning%2520for%2520point%2520cloud%2520has%2520demonstrated%250Aeffectiveness%2520in%2520improving%2520pre-trained%2520model%2520performance%2520across%2520diverse%2520tasks.%250AHowever%252C%2520as%2520pre-trained%2520models%2520grow%2520in%2520complexity%252C%2520fully%2520fine-tuning%2520them%2520for%250Adownstream%2520applications%2520demands%2520substantial%2520computational%2520and%2520storage%250Aresources.%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520offer%2520a%2520promising%250Asolution%2520to%2520mitigate%2520these%2520resource%2520requirements%252C%2520yet%2520most%2520current%2520approaches%250Arely%2520on%2520complex%2520adapter%2520and%2520prompt%2520mechanisms%2520that%2520increase%2520tunable%2520parameters.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520PointLoRA%252C%2520a%2520simple%2520yet%2520effective%2520method%2520that%250Acombines%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520with%2520multi-scale%2520token%2520selection%2520to%250Aefficiently%2520fine-tune%2520point%2520cloud%2520models.%2520Our%2520approach%2520embeds%2520LoRA%2520layers%250Awithin%2520the%2520most%2520parameter-intensive%2520components%2520of%2520point%2520cloud%2520transformers%252C%250Areducing%2520the%2520need%2520for%2520tunable%2520parameters%2520while%2520enhancing%2520global%2520feature%250Acapture.%2520Additionally%252C%2520multi-scale%2520token%2520selection%2520extracts%2520critical%2520local%250Ainformation%2520to%2520serve%2520as%2520prompts%2520for%2520downstream%2520fine-tuning%252C%2520effectively%250Acomplementing%2520the%2520global%2520context%2520captured%2520by%2520LoRA.%2520The%2520experimental%2520results%250Aacross%2520various%2520pre-trained%2520models%2520and%2520three%2520challenging%2520public%2520datasets%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520competitive%2520performance%2520with%2520only%25203.43%2525%250Aof%2520the%2520trainable%2520parameters%252C%2520making%2520it%2520highly%2520effective%2520for%250Aresource-constrained%2520applications.%2520Source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/songw-zju/PointLoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16023v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointLoRA%3A%20Low-Rank%20Adaptation%20with%20Token%20Selection%20for%20Point%20Cloud%0A%20%20Learning&entry.906535625=Song%20Wang%20and%20Xiaolu%20Liu%20and%20Lingdong%20Kong%20and%20Jianyun%20Xu%20and%20Chunyong%20Hu%20and%20Gongfan%20Fang%20and%20Wentong%20Li%20and%20Jianke%20Zhu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Self-supervised%20representation%20learning%20for%20point%20cloud%20has%20demonstrated%0Aeffectiveness%20in%20improving%20pre-trained%20model%20performance%20across%20diverse%20tasks.%0AHowever%2C%20as%20pre-trained%20models%20grow%20in%20complexity%2C%20fully%20fine-tuning%20them%20for%0Adownstream%20applications%20demands%20substantial%20computational%20and%20storage%0Aresources.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20offer%20a%20promising%0Asolution%20to%20mitigate%20these%20resource%20requirements%2C%20yet%20most%20current%20approaches%0Arely%20on%20complex%20adapter%20and%20prompt%20mechanisms%20that%20increase%20tunable%20parameters.%0AIn%20this%20paper%2C%20we%20propose%20PointLoRA%2C%20a%20simple%20yet%20effective%20method%20that%0Acombines%20low-rank%20adaptation%20%28LoRA%29%20with%20multi-scale%20token%20selection%20to%0Aefficiently%20fine-tune%20point%20cloud%20models.%20Our%20approach%20embeds%20LoRA%20layers%0Awithin%20the%20most%20parameter-intensive%20components%20of%20point%20cloud%20transformers%2C%0Areducing%20the%20need%20for%20tunable%20parameters%20while%20enhancing%20global%20feature%0Acapture.%20Additionally%2C%20multi-scale%20token%20selection%20extracts%20critical%20local%0Ainformation%20to%20serve%20as%20prompts%20for%20downstream%20fine-tuning%2C%20effectively%0Acomplementing%20the%20global%20context%20captured%20by%20LoRA.%20The%20experimental%20results%0Aacross%20various%20pre-trained%20models%20and%20three%20challenging%20public%20datasets%0Ademonstrate%20that%20our%20approach%20achieves%20competitive%20performance%20with%20only%203.43%25%0Aof%20the%20trainable%20parameters%2C%20making%20it%20highly%20effective%20for%0Aresource-constrained%20applications.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/songw-zju/PointLoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16023v2&entry.124074799=Read"},
{"title": "CROP: Contextual Region-Oriented Visual Token Pruning", "author": "Jiawei Guo and Feifei Zhai and Pu Jian and Qianrun Wei and Yu Zhou", "abstract": "  Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable.\n", "link": "http://arxiv.org/abs/2505.21233v1", "date": "2025-05-27", "relevancy": 2.6975, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CROP%3A%20Contextual%20Region-Oriented%20Visual%20Token%20Pruning&body=Title%3A%20CROP%3A%20Contextual%20Region-Oriented%20Visual%20Token%20Pruning%0AAuthor%3A%20Jiawei%20Guo%20and%20Feifei%20Zhai%20and%20Pu%20Jian%20and%20Qianrun%20Wei%20and%20Yu%20Zhou%0AAbstract%3A%20%20%20Current%20VLM-based%20VQA%20methods%20often%20process%20entire%20images%2C%20leading%20to%0Aexcessive%20visual%20tokens%20that%20include%20redundant%20information%20irrelevant%20to%20the%0Aposed%20question.%20This%20abundance%20of%20unnecessary%20image%20details%20creates%20numerous%0Avisual%20tokens%2C%20drastically%20increasing%20memory%20and%20computational%20requirements%20in%0AVLMs.%20To%20address%20this%2C%20we%20propose%20Contextual%20Region-Oriented%20Visual%20Token%0APruning%20%28CROP%29%2C%20a%20novel%20framework%20to%20compress%20visual%20tokens%20through%20a%20two-step%0Aprocess%3A%20Localization%20and%20Pruning.%20Specifically%2C%20CROP%20first%20employs%20an%0Aefficient%20model%20to%20identify%20the%20contextual%20region%20relevant%20to%20the%20input%20query.%0ASubsequently%2C%20two%20distinct%20strategies%20are%20introduced%20for%20pruning%3A%20%281%29%20Pre-LLM%0ACompression%20%28PLC%29%2C%20which%20adaptively%20compresses%20different%20image%20regions%20with%0Avarying%20ratios%2C%20and%20%282%29%20Inner-LLM%20Pruning%20%28ILP%29%2C%20a%20training-free%20method%20that%0Aprunes%20tokens%20within%20early%20LLM%20layers%20guided%20by%20the%20identified%20contextual%0Aregion.%20Extensive%20experiments%20on%20a%20wide%20range%20of%20VQA%20tasks%20demonstrate%20that%0ACROP%20significantly%20outperforms%20existing%20visual%20token%20pruning%20methods%20and%0Aachieves%20state-of-the-art%20performance.%20Our%20code%20and%20datasets%20will%20be%20made%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCROP%253A%2520Contextual%2520Region-Oriented%2520Visual%2520Token%2520Pruning%26entry.906535625%3DJiawei%2520Guo%2520and%2520Feifei%2520Zhai%2520and%2520Pu%2520Jian%2520and%2520Qianrun%2520Wei%2520and%2520Yu%2520Zhou%26entry.1292438233%3D%2520%2520Current%2520VLM-based%2520VQA%2520methods%2520often%2520process%2520entire%2520images%252C%2520leading%2520to%250Aexcessive%2520visual%2520tokens%2520that%2520include%2520redundant%2520information%2520irrelevant%2520to%2520the%250Aposed%2520question.%2520This%2520abundance%2520of%2520unnecessary%2520image%2520details%2520creates%2520numerous%250Avisual%2520tokens%252C%2520drastically%2520increasing%2520memory%2520and%2520computational%2520requirements%2520in%250AVLMs.%2520To%2520address%2520this%252C%2520we%2520propose%2520Contextual%2520Region-Oriented%2520Visual%2520Token%250APruning%2520%2528CROP%2529%252C%2520a%2520novel%2520framework%2520to%2520compress%2520visual%2520tokens%2520through%2520a%2520two-step%250Aprocess%253A%2520Localization%2520and%2520Pruning.%2520Specifically%252C%2520CROP%2520first%2520employs%2520an%250Aefficient%2520model%2520to%2520identify%2520the%2520contextual%2520region%2520relevant%2520to%2520the%2520input%2520query.%250ASubsequently%252C%2520two%2520distinct%2520strategies%2520are%2520introduced%2520for%2520pruning%253A%2520%25281%2529%2520Pre-LLM%250ACompression%2520%2528PLC%2529%252C%2520which%2520adaptively%2520compresses%2520different%2520image%2520regions%2520with%250Avarying%2520ratios%252C%2520and%2520%25282%2529%2520Inner-LLM%2520Pruning%2520%2528ILP%2529%252C%2520a%2520training-free%2520method%2520that%250Aprunes%2520tokens%2520within%2520early%2520LLM%2520layers%2520guided%2520by%2520the%2520identified%2520contextual%250Aregion.%2520Extensive%2520experiments%2520on%2520a%2520wide%2520range%2520of%2520VQA%2520tasks%2520demonstrate%2520that%250ACROP%2520significantly%2520outperforms%2520existing%2520visual%2520token%2520pruning%2520methods%2520and%250Aachieves%2520state-of-the-art%2520performance.%2520Our%2520code%2520and%2520datasets%2520will%2520be%2520made%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CROP%3A%20Contextual%20Region-Oriented%20Visual%20Token%20Pruning&entry.906535625=Jiawei%20Guo%20and%20Feifei%20Zhai%20and%20Pu%20Jian%20and%20Qianrun%20Wei%20and%20Yu%20Zhou&entry.1292438233=%20%20Current%20VLM-based%20VQA%20methods%20often%20process%20entire%20images%2C%20leading%20to%0Aexcessive%20visual%20tokens%20that%20include%20redundant%20information%20irrelevant%20to%20the%0Aposed%20question.%20This%20abundance%20of%20unnecessary%20image%20details%20creates%20numerous%0Avisual%20tokens%2C%20drastically%20increasing%20memory%20and%20computational%20requirements%20in%0AVLMs.%20To%20address%20this%2C%20we%20propose%20Contextual%20Region-Oriented%20Visual%20Token%0APruning%20%28CROP%29%2C%20a%20novel%20framework%20to%20compress%20visual%20tokens%20through%20a%20two-step%0Aprocess%3A%20Localization%20and%20Pruning.%20Specifically%2C%20CROP%20first%20employs%20an%0Aefficient%20model%20to%20identify%20the%20contextual%20region%20relevant%20to%20the%20input%20query.%0ASubsequently%2C%20two%20distinct%20strategies%20are%20introduced%20for%20pruning%3A%20%281%29%20Pre-LLM%0ACompression%20%28PLC%29%2C%20which%20adaptively%20compresses%20different%20image%20regions%20with%0Avarying%20ratios%2C%20and%20%282%29%20Inner-LLM%20Pruning%20%28ILP%29%2C%20a%20training-free%20method%20that%0Aprunes%20tokens%20within%20early%20LLM%20layers%20guided%20by%20the%20identified%20contextual%0Aregion.%20Extensive%20experiments%20on%20a%20wide%20range%20of%20VQA%20tasks%20demonstrate%20that%0ACROP%20significantly%20outperforms%20existing%20visual%20token%20pruning%20methods%20and%0Aachieves%20state-of-the-art%20performance.%20Our%20code%20and%20datasets%20will%20be%20made%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21233v1&entry.124074799=Read"},
{"title": "Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of\n  Experts", "author": "Yue Zhang and Yingzhao Jian and Hehe Fan and Yi Yang and Roger Zimmermann", "abstract": "  Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated considerable potential for comprehensive 3D scene understanding.\nHowever, existing approaches typically utilize only one or a limited subset of\n3D modalities, resulting in incomplete representations of 3D scenes and reduced\ninterpretive accuracy. Furthermore, different types of queries inherently\ndepend on distinct modalities, indicating that uniform processing of all\nmodality tokens may fail to effectively capture query-specific context. To\naddress these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts\n(MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion.\nSpecifically, Uni3D-MoE integrates a comprehensive set of 3D modalities,\nincluding multi-view RGB and depth images, bird's-eye-view (BEV) maps, point\nclouds, and voxel representations. At its core, our framework employs a\nlearnable routing mechanism within the sparse MoE-based large language model,\ndynamically selecting appropriate experts at the token level. Each expert\nspecializes in processing multimodal tokens based on learned modality\npreferences, thus facilitating flexible collaboration tailored to diverse\ntask-specific requirements. Extensive evaluations on standard 3D scene\nunderstanding benchmarks and specialized datasets demonstrate the efficacy of\nUni3D-MoE.\n", "link": "http://arxiv.org/abs/2505.21079v1", "date": "2025-05-27", "relevancy": 2.6863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6762}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni3D-MoE%3A%20Scalable%20Multimodal%203D%20Scene%20Understanding%20via%20Mixture%20of%0A%20%20Experts&body=Title%3A%20Uni3D-MoE%3A%20Scalable%20Multimodal%203D%20Scene%20Understanding%20via%20Mixture%20of%0A%20%20Experts%0AAuthor%3A%20Yue%20Zhang%20and%20Yingzhao%20Jian%20and%20Hehe%20Fan%20and%20Yi%20Yang%20and%20Roger%20Zimmermann%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20considerable%20potential%20for%20comprehensive%203D%20scene%20understanding.%0AHowever%2C%20existing%20approaches%20typically%20utilize%20only%20one%20or%20a%20limited%20subset%20of%0A3D%20modalities%2C%20resulting%20in%20incomplete%20representations%20of%203D%20scenes%20and%20reduced%0Ainterpretive%20accuracy.%20Furthermore%2C%20different%20types%20of%20queries%20inherently%0Adepend%20on%20distinct%20modalities%2C%20indicating%20that%20uniform%20processing%20of%20all%0Amodality%20tokens%20may%20fail%20to%20effectively%20capture%20query-specific%20context.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Uni3D-MoE%2C%20a%20sparse%20Mixture-of-Experts%0A%28MoE%29-based%203D%20MLLM%20designed%20to%20enable%20adaptive%203D%20multimodal%20fusion.%0ASpecifically%2C%20Uni3D-MoE%20integrates%20a%20comprehensive%20set%20of%203D%20modalities%2C%0Aincluding%20multi-view%20RGB%20and%20depth%20images%2C%20bird%27s-eye-view%20%28BEV%29%20maps%2C%20point%0Aclouds%2C%20and%20voxel%20representations.%20At%20its%20core%2C%20our%20framework%20employs%20a%0Alearnable%20routing%20mechanism%20within%20the%20sparse%20MoE-based%20large%20language%20model%2C%0Adynamically%20selecting%20appropriate%20experts%20at%20the%20token%20level.%20Each%20expert%0Aspecializes%20in%20processing%20multimodal%20tokens%20based%20on%20learned%20modality%0Apreferences%2C%20thus%20facilitating%20flexible%20collaboration%20tailored%20to%20diverse%0Atask-specific%20requirements.%20Extensive%20evaluations%20on%20standard%203D%20scene%0Aunderstanding%20benchmarks%20and%20specialized%20datasets%20demonstrate%20the%20efficacy%20of%0AUni3D-MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni3D-MoE%253A%2520Scalable%2520Multimodal%25203D%2520Scene%2520Understanding%2520via%2520Mixture%2520of%250A%2520%2520Experts%26entry.906535625%3DYue%2520Zhang%2520and%2520Yingzhao%2520Jian%2520and%2520Hehe%2520Fan%2520and%2520Yi%2520Yang%2520and%2520Roger%2520Zimmermann%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%250Ademonstrated%2520considerable%2520potential%2520for%2520comprehensive%25203D%2520scene%2520understanding.%250AHowever%252C%2520existing%2520approaches%2520typically%2520utilize%2520only%2520one%2520or%2520a%2520limited%2520subset%2520of%250A3D%2520modalities%252C%2520resulting%2520in%2520incomplete%2520representations%2520of%25203D%2520scenes%2520and%2520reduced%250Ainterpretive%2520accuracy.%2520Furthermore%252C%2520different%2520types%2520of%2520queries%2520inherently%250Adepend%2520on%2520distinct%2520modalities%252C%2520indicating%2520that%2520uniform%2520processing%2520of%2520all%250Amodality%2520tokens%2520may%2520fail%2520to%2520effectively%2520capture%2520query-specific%2520context.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520Uni3D-MoE%252C%2520a%2520sparse%2520Mixture-of-Experts%250A%2528MoE%2529-based%25203D%2520MLLM%2520designed%2520to%2520enable%2520adaptive%25203D%2520multimodal%2520fusion.%250ASpecifically%252C%2520Uni3D-MoE%2520integrates%2520a%2520comprehensive%2520set%2520of%25203D%2520modalities%252C%250Aincluding%2520multi-view%2520RGB%2520and%2520depth%2520images%252C%2520bird%2527s-eye-view%2520%2528BEV%2529%2520maps%252C%2520point%250Aclouds%252C%2520and%2520voxel%2520representations.%2520At%2520its%2520core%252C%2520our%2520framework%2520employs%2520a%250Alearnable%2520routing%2520mechanism%2520within%2520the%2520sparse%2520MoE-based%2520large%2520language%2520model%252C%250Adynamically%2520selecting%2520appropriate%2520experts%2520at%2520the%2520token%2520level.%2520Each%2520expert%250Aspecializes%2520in%2520processing%2520multimodal%2520tokens%2520based%2520on%2520learned%2520modality%250Apreferences%252C%2520thus%2520facilitating%2520flexible%2520collaboration%2520tailored%2520to%2520diverse%250Atask-specific%2520requirements.%2520Extensive%2520evaluations%2520on%2520standard%25203D%2520scene%250Aunderstanding%2520benchmarks%2520and%2520specialized%2520datasets%2520demonstrate%2520the%2520efficacy%2520of%250AUni3D-MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni3D-MoE%3A%20Scalable%20Multimodal%203D%20Scene%20Understanding%20via%20Mixture%20of%0A%20%20Experts&entry.906535625=Yue%20Zhang%20and%20Yingzhao%20Jian%20and%20Hehe%20Fan%20and%20Yi%20Yang%20and%20Roger%20Zimmermann&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20considerable%20potential%20for%20comprehensive%203D%20scene%20understanding.%0AHowever%2C%20existing%20approaches%20typically%20utilize%20only%20one%20or%20a%20limited%20subset%20of%0A3D%20modalities%2C%20resulting%20in%20incomplete%20representations%20of%203D%20scenes%20and%20reduced%0Ainterpretive%20accuracy.%20Furthermore%2C%20different%20types%20of%20queries%20inherently%0Adepend%20on%20distinct%20modalities%2C%20indicating%20that%20uniform%20processing%20of%20all%0Amodality%20tokens%20may%20fail%20to%20effectively%20capture%20query-specific%20context.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Uni3D-MoE%2C%20a%20sparse%20Mixture-of-Experts%0A%28MoE%29-based%203D%20MLLM%20designed%20to%20enable%20adaptive%203D%20multimodal%20fusion.%0ASpecifically%2C%20Uni3D-MoE%20integrates%20a%20comprehensive%20set%20of%203D%20modalities%2C%0Aincluding%20multi-view%20RGB%20and%20depth%20images%2C%20bird%27s-eye-view%20%28BEV%29%20maps%2C%20point%0Aclouds%2C%20and%20voxel%20representations.%20At%20its%20core%2C%20our%20framework%20employs%20a%0Alearnable%20routing%20mechanism%20within%20the%20sparse%20MoE-based%20large%20language%20model%2C%0Adynamically%20selecting%20appropriate%20experts%20at%20the%20token%20level.%20Each%20expert%0Aspecializes%20in%20processing%20multimodal%20tokens%20based%20on%20learned%20modality%0Apreferences%2C%20thus%20facilitating%20flexible%20collaboration%20tailored%20to%20diverse%0Atask-specific%20requirements.%20Extensive%20evaluations%20on%20standard%203D%20scene%0Aunderstanding%20benchmarks%20and%20specialized%20datasets%20demonstrate%20the%20efficacy%20of%0AUni3D-MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21079v1&entry.124074799=Read"},
{"title": "Spectral Compression Transformer with Line Pose Graph for Monocular 3D\n  Human Pose Estimation", "author": "Zenghao Zheng and Lianping Yang and Hegui Zhu and Mingrui Ye", "abstract": "  Transformer-based 3D human pose estimation methods suffer from high\ncomputational costs due to the quadratic complexity of self-attention with\nrespect to sequence length. Additionally, pose sequences often contain\nsignificant redundancy between frames. However, recent methods typically fail\nto improve model capacity while effectively eliminating sequence redundancy. In\nthis work, we introduce the Spectral Compression Transformer (SCT) to reduce\nsequence length and accelerate computation. The SCT encoder treats hidden\nfeatures between blocks as Temporal Feature Signals (TFS) and applies the\nDiscrete Cosine Transform, a Fourier transform-based technique, to determine\nthe spectral components to be retained. By filtering out certain high-frequency\nnoise components, SCT compresses the sequence length and reduces redundancy. To\nfurther enrich the input sequence with prior structural information, we propose\nthe Line Pose Graph (LPG) based on line graph theory. The LPG generates\nskeletal position information that complements the input 2D joint positions,\nthereby improving the model's performance. Finally, we design a dual-stream\nnetwork architecture to effectively model spatial joint relationships and the\ncompressed motion trajectory within the pose sequence. Extensive experiments on\ntwo benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our\nmodel achieves state-of-the-art performance with improved computational\nefficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE\nof 37.7mm while maintaining a low computational cost. Furthermore, we perform\nablation studies on each module to assess its effectiveness. The code and\nmodels will be released.\n", "link": "http://arxiv.org/abs/2505.21309v1", "date": "2025-05-27", "relevancy": 2.6792, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7027}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6645}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Compression%20Transformer%20with%20Line%20Pose%20Graph%20for%20Monocular%203D%0A%20%20Human%20Pose%20Estimation&body=Title%3A%20Spectral%20Compression%20Transformer%20with%20Line%20Pose%20Graph%20for%20Monocular%203D%0A%20%20Human%20Pose%20Estimation%0AAuthor%3A%20Zenghao%20Zheng%20and%20Lianping%20Yang%20and%20Hegui%20Zhu%20and%20Mingrui%20Ye%0AAbstract%3A%20%20%20Transformer-based%203D%20human%20pose%20estimation%20methods%20suffer%20from%20high%0Acomputational%20costs%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%20with%0Arespect%20to%20sequence%20length.%20Additionally%2C%20pose%20sequences%20often%20contain%0Asignificant%20redundancy%20between%20frames.%20However%2C%20recent%20methods%20typically%20fail%0Ato%20improve%20model%20capacity%20while%20effectively%20eliminating%20sequence%20redundancy.%20In%0Athis%20work%2C%20we%20introduce%20the%20Spectral%20Compression%20Transformer%20%28SCT%29%20to%20reduce%0Asequence%20length%20and%20accelerate%20computation.%20The%20SCT%20encoder%20treats%20hidden%0Afeatures%20between%20blocks%20as%20Temporal%20Feature%20Signals%20%28TFS%29%20and%20applies%20the%0ADiscrete%20Cosine%20Transform%2C%20a%20Fourier%20transform-based%20technique%2C%20to%20determine%0Athe%20spectral%20components%20to%20be%20retained.%20By%20filtering%20out%20certain%20high-frequency%0Anoise%20components%2C%20SCT%20compresses%20the%20sequence%20length%20and%20reduces%20redundancy.%20To%0Afurther%20enrich%20the%20input%20sequence%20with%20prior%20structural%20information%2C%20we%20propose%0Athe%20Line%20Pose%20Graph%20%28LPG%29%20based%20on%20line%20graph%20theory.%20The%20LPG%20generates%0Askeletal%20position%20information%20that%20complements%20the%20input%202D%20joint%20positions%2C%0Athereby%20improving%20the%20model%27s%20performance.%20Finally%2C%20we%20design%20a%20dual-stream%0Anetwork%20architecture%20to%20effectively%20model%20spatial%20joint%20relationships%20and%20the%0Acompressed%20motion%20trajectory%20within%20the%20pose%20sequence.%20Extensive%20experiments%20on%0Atwo%20benchmark%20datasets%20%28i.e.%2C%20Human3.6M%20and%20MPI-INF-3DHP%29%20demonstrate%20that%20our%0Amodel%20achieves%20state-of-the-art%20performance%20with%20improved%20computational%0Aefficiency.%20For%20example%2C%20on%20the%20Human3.6M%20dataset%2C%20our%20method%20achieves%20an%20MPJPE%0Aof%2037.7mm%20while%20maintaining%20a%20low%20computational%20cost.%20Furthermore%2C%20we%20perform%0Aablation%20studies%20on%20each%20module%20to%20assess%20its%20effectiveness.%20The%20code%20and%0Amodels%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Compression%2520Transformer%2520with%2520Line%2520Pose%2520Graph%2520for%2520Monocular%25203D%250A%2520%2520Human%2520Pose%2520Estimation%26entry.906535625%3DZenghao%2520Zheng%2520and%2520Lianping%2520Yang%2520and%2520Hegui%2520Zhu%2520and%2520Mingrui%2520Ye%26entry.1292438233%3D%2520%2520Transformer-based%25203D%2520human%2520pose%2520estimation%2520methods%2520suffer%2520from%2520high%250Acomputational%2520costs%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520self-attention%2520with%250Arespect%2520to%2520sequence%2520length.%2520Additionally%252C%2520pose%2520sequences%2520often%2520contain%250Asignificant%2520redundancy%2520between%2520frames.%2520However%252C%2520recent%2520methods%2520typically%2520fail%250Ato%2520improve%2520model%2520capacity%2520while%2520effectively%2520eliminating%2520sequence%2520redundancy.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520the%2520Spectral%2520Compression%2520Transformer%2520%2528SCT%2529%2520to%2520reduce%250Asequence%2520length%2520and%2520accelerate%2520computation.%2520The%2520SCT%2520encoder%2520treats%2520hidden%250Afeatures%2520between%2520blocks%2520as%2520Temporal%2520Feature%2520Signals%2520%2528TFS%2529%2520and%2520applies%2520the%250ADiscrete%2520Cosine%2520Transform%252C%2520a%2520Fourier%2520transform-based%2520technique%252C%2520to%2520determine%250Athe%2520spectral%2520components%2520to%2520be%2520retained.%2520By%2520filtering%2520out%2520certain%2520high-frequency%250Anoise%2520components%252C%2520SCT%2520compresses%2520the%2520sequence%2520length%2520and%2520reduces%2520redundancy.%2520To%250Afurther%2520enrich%2520the%2520input%2520sequence%2520with%2520prior%2520structural%2520information%252C%2520we%2520propose%250Athe%2520Line%2520Pose%2520Graph%2520%2528LPG%2529%2520based%2520on%2520line%2520graph%2520theory.%2520The%2520LPG%2520generates%250Askeletal%2520position%2520information%2520that%2520complements%2520the%2520input%25202D%2520joint%2520positions%252C%250Athereby%2520improving%2520the%2520model%2527s%2520performance.%2520Finally%252C%2520we%2520design%2520a%2520dual-stream%250Anetwork%2520architecture%2520to%2520effectively%2520model%2520spatial%2520joint%2520relationships%2520and%2520the%250Acompressed%2520motion%2520trajectory%2520within%2520the%2520pose%2520sequence.%2520Extensive%2520experiments%2520on%250Atwo%2520benchmark%2520datasets%2520%2528i.e.%252C%2520Human3.6M%2520and%2520MPI-INF-3DHP%2529%2520demonstrate%2520that%2520our%250Amodel%2520achieves%2520state-of-the-art%2520performance%2520with%2520improved%2520computational%250Aefficiency.%2520For%2520example%252C%2520on%2520the%2520Human3.6M%2520dataset%252C%2520our%2520method%2520achieves%2520an%2520MPJPE%250Aof%252037.7mm%2520while%2520maintaining%2520a%2520low%2520computational%2520cost.%2520Furthermore%252C%2520we%2520perform%250Aablation%2520studies%2520on%2520each%2520module%2520to%2520assess%2520its%2520effectiveness.%2520The%2520code%2520and%250Amodels%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Compression%20Transformer%20with%20Line%20Pose%20Graph%20for%20Monocular%203D%0A%20%20Human%20Pose%20Estimation&entry.906535625=Zenghao%20Zheng%20and%20Lianping%20Yang%20and%20Hegui%20Zhu%20and%20Mingrui%20Ye&entry.1292438233=%20%20Transformer-based%203D%20human%20pose%20estimation%20methods%20suffer%20from%20high%0Acomputational%20costs%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%20with%0Arespect%20to%20sequence%20length.%20Additionally%2C%20pose%20sequences%20often%20contain%0Asignificant%20redundancy%20between%20frames.%20However%2C%20recent%20methods%20typically%20fail%0Ato%20improve%20model%20capacity%20while%20effectively%20eliminating%20sequence%20redundancy.%20In%0Athis%20work%2C%20we%20introduce%20the%20Spectral%20Compression%20Transformer%20%28SCT%29%20to%20reduce%0Asequence%20length%20and%20accelerate%20computation.%20The%20SCT%20encoder%20treats%20hidden%0Afeatures%20between%20blocks%20as%20Temporal%20Feature%20Signals%20%28TFS%29%20and%20applies%20the%0ADiscrete%20Cosine%20Transform%2C%20a%20Fourier%20transform-based%20technique%2C%20to%20determine%0Athe%20spectral%20components%20to%20be%20retained.%20By%20filtering%20out%20certain%20high-frequency%0Anoise%20components%2C%20SCT%20compresses%20the%20sequence%20length%20and%20reduces%20redundancy.%20To%0Afurther%20enrich%20the%20input%20sequence%20with%20prior%20structural%20information%2C%20we%20propose%0Athe%20Line%20Pose%20Graph%20%28LPG%29%20based%20on%20line%20graph%20theory.%20The%20LPG%20generates%0Askeletal%20position%20information%20that%20complements%20the%20input%202D%20joint%20positions%2C%0Athereby%20improving%20the%20model%27s%20performance.%20Finally%2C%20we%20design%20a%20dual-stream%0Anetwork%20architecture%20to%20effectively%20model%20spatial%20joint%20relationships%20and%20the%0Acompressed%20motion%20trajectory%20within%20the%20pose%20sequence.%20Extensive%20experiments%20on%0Atwo%20benchmark%20datasets%20%28i.e.%2C%20Human3.6M%20and%20MPI-INF-3DHP%29%20demonstrate%20that%20our%0Amodel%20achieves%20state-of-the-art%20performance%20with%20improved%20computational%0Aefficiency.%20For%20example%2C%20on%20the%20Human3.6M%20dataset%2C%20our%20method%20achieves%20an%20MPJPE%0Aof%2037.7mm%20while%20maintaining%20a%20low%20computational%20cost.%20Furthermore%2C%20we%20perform%0Aablation%20studies%20on%20each%20module%20to%20assess%20its%20effectiveness.%20The%20code%20and%0Amodels%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21309v1&entry.124074799=Read"},
{"title": "ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding", "author": "Linshuang Diao and Dayong Ren and Sensen Song and Yurong Qian", "abstract": "  State Space models (SSMs) such as PointMamba enable efficient feature\nextraction for point cloud self-supervised learning with linear complexity,\noutperforming Transformers in computational efficiency. However, existing\nPointMamba-based methods depend on complex token ordering and random masking,\nwhich disrupt spatial continuity and local semantic correlations. We propose\nZigzagPointMamba to tackle these challenges. The core of our approach is a\nsimple zigzag scan path that globally sequences point cloud tokens, enhancing\nspatial continuity by preserving the proximity of spatially adjacent point\ntokens. Nevertheless, random masking undermines local semantic modeling in\nself-supervised learning. To address this, we introduce a Semantic-Siamese\nMasking Strategy (SMS), which masks semantically similar tokens to facilitate\nreconstruction by integrating local features of original and similar tokens.\nThis overcomes the dependence on isolated local features and enables robust\nglobal semantic modeling. Our pre-trained ZigzagPointMamba weights\nsignificantly improve downstream tasks, achieving a 1.59% mIoU gain on\nShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for\nclassification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for\nthe classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of\nScanObjectNN. The code is available at:\nhttps://anonymous.4open.science/r/ZigzagPointMamba-1800/\n", "link": "http://arxiv.org/abs/2505.21381v1", "date": "2025-05-27", "relevancy": 2.6788, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZigzagPointMamba%3A%20Spatial-Semantic%20Mamba%20for%20Point%20Cloud%20Understanding&body=Title%3A%20ZigzagPointMamba%3A%20Spatial-Semantic%20Mamba%20for%20Point%20Cloud%20Understanding%0AAuthor%3A%20Linshuang%20Diao%20and%20Dayong%20Ren%20and%20Sensen%20Song%20and%20Yurong%20Qian%0AAbstract%3A%20%20%20State%20Space%20models%20%28SSMs%29%20such%20as%20PointMamba%20enable%20efficient%20feature%0Aextraction%20for%20point%20cloud%20self-supervised%20learning%20with%20linear%20complexity%2C%0Aoutperforming%20Transformers%20in%20computational%20efficiency.%20However%2C%20existing%0APointMamba-based%20methods%20depend%20on%20complex%20token%20ordering%20and%20random%20masking%2C%0Awhich%20disrupt%20spatial%20continuity%20and%20local%20semantic%20correlations.%20We%20propose%0AZigzagPointMamba%20to%20tackle%20these%20challenges.%20The%20core%20of%20our%20approach%20is%20a%0Asimple%20zigzag%20scan%20path%20that%20globally%20sequences%20point%20cloud%20tokens%2C%20enhancing%0Aspatial%20continuity%20by%20preserving%20the%20proximity%20of%20spatially%20adjacent%20point%0Atokens.%20Nevertheless%2C%20random%20masking%20undermines%20local%20semantic%20modeling%20in%0Aself-supervised%20learning.%20To%20address%20this%2C%20we%20introduce%20a%20Semantic-Siamese%0AMasking%20Strategy%20%28SMS%29%2C%20which%20masks%20semantically%20similar%20tokens%20to%20facilitate%0Areconstruction%20by%20integrating%20local%20features%20of%20original%20and%20similar%20tokens.%0AThis%20overcomes%20the%20dependence%20on%20isolated%20local%20features%20and%20enables%20robust%0Aglobal%20semantic%20modeling.%20Our%20pre-trained%20ZigzagPointMamba%20weights%0Asignificantly%20improve%20downstream%20tasks%2C%20achieving%20a%201.59%25%20mIoU%20gain%20on%0AShapeNetPart%20for%20part%20segmentation%2C%20a%200.4%25%20higher%20accuracy%20on%20ModelNet40%20for%0Aclassification%2C%20and%200.19%25%2C%201.22%25%2C%20and%200.72%25%20higher%20accuracies%20respectively%20for%0Athe%20classification%20tasks%20on%20the%20OBJ-BG%2C%20OBJ-ONLY%2C%20and%20PB-T50-RS%20subsets%20of%0AScanObjectNN.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/ZigzagPointMamba-1800/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZigzagPointMamba%253A%2520Spatial-Semantic%2520Mamba%2520for%2520Point%2520Cloud%2520Understanding%26entry.906535625%3DLinshuang%2520Diao%2520and%2520Dayong%2520Ren%2520and%2520Sensen%2520Song%2520and%2520Yurong%2520Qian%26entry.1292438233%3D%2520%2520State%2520Space%2520models%2520%2528SSMs%2529%2520such%2520as%2520PointMamba%2520enable%2520efficient%2520feature%250Aextraction%2520for%2520point%2520cloud%2520self-supervised%2520learning%2520with%2520linear%2520complexity%252C%250Aoutperforming%2520Transformers%2520in%2520computational%2520efficiency.%2520However%252C%2520existing%250APointMamba-based%2520methods%2520depend%2520on%2520complex%2520token%2520ordering%2520and%2520random%2520masking%252C%250Awhich%2520disrupt%2520spatial%2520continuity%2520and%2520local%2520semantic%2520correlations.%2520We%2520propose%250AZigzagPointMamba%2520to%2520tackle%2520these%2520challenges.%2520The%2520core%2520of%2520our%2520approach%2520is%2520a%250Asimple%2520zigzag%2520scan%2520path%2520that%2520globally%2520sequences%2520point%2520cloud%2520tokens%252C%2520enhancing%250Aspatial%2520continuity%2520by%2520preserving%2520the%2520proximity%2520of%2520spatially%2520adjacent%2520point%250Atokens.%2520Nevertheless%252C%2520random%2520masking%2520undermines%2520local%2520semantic%2520modeling%2520in%250Aself-supervised%2520learning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520Semantic-Siamese%250AMasking%2520Strategy%2520%2528SMS%2529%252C%2520which%2520masks%2520semantically%2520similar%2520tokens%2520to%2520facilitate%250Areconstruction%2520by%2520integrating%2520local%2520features%2520of%2520original%2520and%2520similar%2520tokens.%250AThis%2520overcomes%2520the%2520dependence%2520on%2520isolated%2520local%2520features%2520and%2520enables%2520robust%250Aglobal%2520semantic%2520modeling.%2520Our%2520pre-trained%2520ZigzagPointMamba%2520weights%250Asignificantly%2520improve%2520downstream%2520tasks%252C%2520achieving%2520a%25201.59%2525%2520mIoU%2520gain%2520on%250AShapeNetPart%2520for%2520part%2520segmentation%252C%2520a%25200.4%2525%2520higher%2520accuracy%2520on%2520ModelNet40%2520for%250Aclassification%252C%2520and%25200.19%2525%252C%25201.22%2525%252C%2520and%25200.72%2525%2520higher%2520accuracies%2520respectively%2520for%250Athe%2520classification%2520tasks%2520on%2520the%2520OBJ-BG%252C%2520OBJ-ONLY%252C%2520and%2520PB-T50-RS%2520subsets%2520of%250AScanObjectNN.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/ZigzagPointMamba-1800/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZigzagPointMamba%3A%20Spatial-Semantic%20Mamba%20for%20Point%20Cloud%20Understanding&entry.906535625=Linshuang%20Diao%20and%20Dayong%20Ren%20and%20Sensen%20Song%20and%20Yurong%20Qian&entry.1292438233=%20%20State%20Space%20models%20%28SSMs%29%20such%20as%20PointMamba%20enable%20efficient%20feature%0Aextraction%20for%20point%20cloud%20self-supervised%20learning%20with%20linear%20complexity%2C%0Aoutperforming%20Transformers%20in%20computational%20efficiency.%20However%2C%20existing%0APointMamba-based%20methods%20depend%20on%20complex%20token%20ordering%20and%20random%20masking%2C%0Awhich%20disrupt%20spatial%20continuity%20and%20local%20semantic%20correlations.%20We%20propose%0AZigzagPointMamba%20to%20tackle%20these%20challenges.%20The%20core%20of%20our%20approach%20is%20a%0Asimple%20zigzag%20scan%20path%20that%20globally%20sequences%20point%20cloud%20tokens%2C%20enhancing%0Aspatial%20continuity%20by%20preserving%20the%20proximity%20of%20spatially%20adjacent%20point%0Atokens.%20Nevertheless%2C%20random%20masking%20undermines%20local%20semantic%20modeling%20in%0Aself-supervised%20learning.%20To%20address%20this%2C%20we%20introduce%20a%20Semantic-Siamese%0AMasking%20Strategy%20%28SMS%29%2C%20which%20masks%20semantically%20similar%20tokens%20to%20facilitate%0Areconstruction%20by%20integrating%20local%20features%20of%20original%20and%20similar%20tokens.%0AThis%20overcomes%20the%20dependence%20on%20isolated%20local%20features%20and%20enables%20robust%0Aglobal%20semantic%20modeling.%20Our%20pre-trained%20ZigzagPointMamba%20weights%0Asignificantly%20improve%20downstream%20tasks%2C%20achieving%20a%201.59%25%20mIoU%20gain%20on%0AShapeNetPart%20for%20part%20segmentation%2C%20a%200.4%25%20higher%20accuracy%20on%20ModelNet40%20for%0Aclassification%2C%20and%200.19%25%2C%201.22%25%2C%20and%200.72%25%20higher%20accuracies%20respectively%20for%0Athe%20classification%20tasks%20on%20the%20OBJ-BG%2C%20OBJ-ONLY%2C%20and%20PB-T50-RS%20subsets%20of%0AScanObjectNN.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/ZigzagPointMamba-1800/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21381v1&entry.124074799=Read"},
{"title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object\n  Detection in Infographics", "author": "Jiangning Zhu and Yuxing Zhou and Zheng Wang and Juntao Yao and Yima Gu and Yuhui Yuan and Shixia Liu", "abstract": "  Given the central role of charts in scientific, business, and communication\ncontexts, enhancing the chart understanding capabilities of vision-language\nmodels (VLMs) has become increasingly critical. A key limitation of existing\nVLMs lies in their inaccurate visual grounding of infographic elements,\nincluding charts and human-recognizable objects (HROs) such as icons and\nimages. However, chart understanding often requires identifying relevant\nelements and reasoning over them. To address this limitation, we introduce\nOrionBench, a benchmark designed to support the development of accurate object\ndetection models for charts and HROs in infographics. It contains 26,250 real\nand 78,750 synthetic infographics, with over 6.9 million bounding box\nannotations. These annotations are created by combining the model-in-the-loop\nand programmatic methods. We demonstrate the usefulness of OrionBench through\nthree applications: 1) constructing a Thinking-with-Boxes scheme to boost the\nchart understanding performance of VLMs, 2) comparing existing object detection\nmodels, and 3) applying the developed detection model to document layout and UI\nelement detection.\n", "link": "http://arxiv.org/abs/2505.17473v2", "date": "2025-05-27", "relevancy": 2.6741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OrionBench%3A%20A%20Benchmark%20for%20Chart%20and%20Human-Recognizable%20Object%0A%20%20Detection%20in%20Infographics&body=Title%3A%20OrionBench%3A%20A%20Benchmark%20for%20Chart%20and%20Human-Recognizable%20Object%0A%20%20Detection%20in%20Infographics%0AAuthor%3A%20Jiangning%20Zhu%20and%20Yuxing%20Zhou%20and%20Zheng%20Wang%20and%20Juntao%20Yao%20and%20Yima%20Gu%20and%20Yuhui%20Yuan%20and%20Shixia%20Liu%0AAbstract%3A%20%20%20Given%20the%20central%20role%20of%20charts%20in%20scientific%2C%20business%2C%20and%20communication%0Acontexts%2C%20enhancing%20the%20chart%20understanding%20capabilities%20of%20vision-language%0Amodels%20%28VLMs%29%20has%20become%20increasingly%20critical.%20A%20key%20limitation%20of%20existing%0AVLMs%20lies%20in%20their%20inaccurate%20visual%20grounding%20of%20infographic%20elements%2C%0Aincluding%20charts%20and%20human-recognizable%20objects%20%28HROs%29%20such%20as%20icons%20and%0Aimages.%20However%2C%20chart%20understanding%20often%20requires%20identifying%20relevant%0Aelements%20and%20reasoning%20over%20them.%20To%20address%20this%20limitation%2C%20we%20introduce%0AOrionBench%2C%20a%20benchmark%20designed%20to%20support%20the%20development%20of%20accurate%20object%0Adetection%20models%20for%20charts%20and%20HROs%20in%20infographics.%20It%20contains%2026%2C250%20real%0Aand%2078%2C750%20synthetic%20infographics%2C%20with%20over%206.9%20million%20bounding%20box%0Aannotations.%20These%20annotations%20are%20created%20by%20combining%20the%20model-in-the-loop%0Aand%20programmatic%20methods.%20We%20demonstrate%20the%20usefulness%20of%20OrionBench%20through%0Athree%20applications%3A%201%29%20constructing%20a%20Thinking-with-Boxes%20scheme%20to%20boost%20the%0Achart%20understanding%20performance%20of%20VLMs%2C%202%29%20comparing%20existing%20object%20detection%0Amodels%2C%20and%203%29%20applying%20the%20developed%20detection%20model%20to%20document%20layout%20and%20UI%0Aelement%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrionBench%253A%2520A%2520Benchmark%2520for%2520Chart%2520and%2520Human-Recognizable%2520Object%250A%2520%2520Detection%2520in%2520Infographics%26entry.906535625%3DJiangning%2520Zhu%2520and%2520Yuxing%2520Zhou%2520and%2520Zheng%2520Wang%2520and%2520Juntao%2520Yao%2520and%2520Yima%2520Gu%2520and%2520Yuhui%2520Yuan%2520and%2520Shixia%2520Liu%26entry.1292438233%3D%2520%2520Given%2520the%2520central%2520role%2520of%2520charts%2520in%2520scientific%252C%2520business%252C%2520and%2520communication%250Acontexts%252C%2520enhancing%2520the%2520chart%2520understanding%2520capabilities%2520of%2520vision-language%250Amodels%2520%2528VLMs%2529%2520has%2520become%2520increasingly%2520critical.%2520A%2520key%2520limitation%2520of%2520existing%250AVLMs%2520lies%2520in%2520their%2520inaccurate%2520visual%2520grounding%2520of%2520infographic%2520elements%252C%250Aincluding%2520charts%2520and%2520human-recognizable%2520objects%2520%2528HROs%2529%2520such%2520as%2520icons%2520and%250Aimages.%2520However%252C%2520chart%2520understanding%2520often%2520requires%2520identifying%2520relevant%250Aelements%2520and%2520reasoning%2520over%2520them.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250AOrionBench%252C%2520a%2520benchmark%2520designed%2520to%2520support%2520the%2520development%2520of%2520accurate%2520object%250Adetection%2520models%2520for%2520charts%2520and%2520HROs%2520in%2520infographics.%2520It%2520contains%252026%252C250%2520real%250Aand%252078%252C750%2520synthetic%2520infographics%252C%2520with%2520over%25206.9%2520million%2520bounding%2520box%250Aannotations.%2520These%2520annotations%2520are%2520created%2520by%2520combining%2520the%2520model-in-the-loop%250Aand%2520programmatic%2520methods.%2520We%2520demonstrate%2520the%2520usefulness%2520of%2520OrionBench%2520through%250Athree%2520applications%253A%25201%2529%2520constructing%2520a%2520Thinking-with-Boxes%2520scheme%2520to%2520boost%2520the%250Achart%2520understanding%2520performance%2520of%2520VLMs%252C%25202%2529%2520comparing%2520existing%2520object%2520detection%250Amodels%252C%2520and%25203%2529%2520applying%2520the%2520developed%2520detection%2520model%2520to%2520document%2520layout%2520and%2520UI%250Aelement%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OrionBench%3A%20A%20Benchmark%20for%20Chart%20and%20Human-Recognizable%20Object%0A%20%20Detection%20in%20Infographics&entry.906535625=Jiangning%20Zhu%20and%20Yuxing%20Zhou%20and%20Zheng%20Wang%20and%20Juntao%20Yao%20and%20Yima%20Gu%20and%20Yuhui%20Yuan%20and%20Shixia%20Liu&entry.1292438233=%20%20Given%20the%20central%20role%20of%20charts%20in%20scientific%2C%20business%2C%20and%20communication%0Acontexts%2C%20enhancing%20the%20chart%20understanding%20capabilities%20of%20vision-language%0Amodels%20%28VLMs%29%20has%20become%20increasingly%20critical.%20A%20key%20limitation%20of%20existing%0AVLMs%20lies%20in%20their%20inaccurate%20visual%20grounding%20of%20infographic%20elements%2C%0Aincluding%20charts%20and%20human-recognizable%20objects%20%28HROs%29%20such%20as%20icons%20and%0Aimages.%20However%2C%20chart%20understanding%20often%20requires%20identifying%20relevant%0Aelements%20and%20reasoning%20over%20them.%20To%20address%20this%20limitation%2C%20we%20introduce%0AOrionBench%2C%20a%20benchmark%20designed%20to%20support%20the%20development%20of%20accurate%20object%0Adetection%20models%20for%20charts%20and%20HROs%20in%20infographics.%20It%20contains%2026%2C250%20real%0Aand%2078%2C750%20synthetic%20infographics%2C%20with%20over%206.9%20million%20bounding%20box%0Aannotations.%20These%20annotations%20are%20created%20by%20combining%20the%20model-in-the-loop%0Aand%20programmatic%20methods.%20We%20demonstrate%20the%20usefulness%20of%20OrionBench%20through%0Athree%20applications%3A%201%29%20constructing%20a%20Thinking-with-Boxes%20scheme%20to%20boost%20the%0Achart%20understanding%20performance%20of%20VLMs%2C%202%29%20comparing%20existing%20object%20detection%0Amodels%2C%20and%203%29%20applying%20the%20developed%20detection%20model%20to%20document%20layout%20and%20UI%0Aelement%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17473v2&entry.124074799=Read"},
{"title": "PLGSLAM: Progressive Neural Scene Represenation with Local to Global\n  Bundle Adjustment", "author": "Tianchen Deng and Guole Shen and Tong Qin and Jianyu Wang and Wentao Zhao and Jingchuan Wang and Danwei Wang and Weidong Chen", "abstract": "  Neural implicit scene representations have recently shown encouraging results\nin dense visual SLAM. However, existing methods produce low-quality scene\nreconstruction and low-accuracy localization performance when scaling up to\nlarge indoor scenes and long sequences. These limitations are mainly due to\ntheir single, global radiance field with finite capacity, which does not adapt\nto large scenarios. Their end-to-end pose networks are also not robust enough\nwith the growth of cumulative errors in large scenes. To this end, we introduce\nPLGSLAM, a neural visual SLAM system capable of high-fidelity surface\nreconstruction and robust camera tracking in real-time. To handle large-scale\nindoor scenes, PLGSLAM proposes a progressive scene representation method which\ndynamically allocates new local scene representation trained with frames within\na local sliding window. This allows us to scale up to larger indoor scenes and\nimproves robustness (even under pose drifts). In local scene representation,\nPLGSLAM utilizes tri-planes for local high-frequency features with multi-layer\nperceptron (MLP) networks for the low-frequency feature, achieving smoothness\nand scene completion in unobserved areas. Moreover, we propose local-to-global\nbundle adjustment method with a global keyframe database to address the\nincreased pose drifts on long sequences. Experimental results demonstrate that\nPLGSLAM achieves state-of-the-art scene reconstruction results and tracking\nperformance across various datasets and scenarios (both in small and\nlarge-scale indoor environments). The code is open-sourced at\nhttps://github.com/dtc111111/plgslam.\n", "link": "http://arxiv.org/abs/2312.09866v3", "date": "2025-05-27", "relevancy": 2.6689, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7086}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6502}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLGSLAM%3A%20Progressive%20Neural%20Scene%20Represenation%20with%20Local%20to%20Global%0A%20%20Bundle%20Adjustment&body=Title%3A%20PLGSLAM%3A%20Progressive%20Neural%20Scene%20Represenation%20with%20Local%20to%20Global%0A%20%20Bundle%20Adjustment%0AAuthor%3A%20Tianchen%20Deng%20and%20Guole%20Shen%20and%20Tong%20Qin%20and%20Jianyu%20Wang%20and%20Wentao%20Zhao%20and%20Jingchuan%20Wang%20and%20Danwei%20Wang%20and%20Weidong%20Chen%0AAbstract%3A%20%20%20Neural%20implicit%20scene%20representations%20have%20recently%20shown%20encouraging%20results%0Ain%20dense%20visual%20SLAM.%20However%2C%20existing%20methods%20produce%20low-quality%20scene%0Areconstruction%20and%20low-accuracy%20localization%20performance%20when%20scaling%20up%20to%0Alarge%20indoor%20scenes%20and%20long%20sequences.%20These%20limitations%20are%20mainly%20due%20to%0Atheir%20single%2C%20global%20radiance%20field%20with%20finite%20capacity%2C%20which%20does%20not%20adapt%0Ato%20large%20scenarios.%20Their%20end-to-end%20pose%20networks%20are%20also%20not%20robust%20enough%0Awith%20the%20growth%20of%20cumulative%20errors%20in%20large%20scenes.%20To%20this%20end%2C%20we%20introduce%0APLGSLAM%2C%20a%20neural%20visual%20SLAM%20system%20capable%20of%20high-fidelity%20surface%0Areconstruction%20and%20robust%20camera%20tracking%20in%20real-time.%20To%20handle%20large-scale%0Aindoor%20scenes%2C%20PLGSLAM%20proposes%20a%20progressive%20scene%20representation%20method%20which%0Adynamically%20allocates%20new%20local%20scene%20representation%20trained%20with%20frames%20within%0Aa%20local%20sliding%20window.%20This%20allows%20us%20to%20scale%20up%20to%20larger%20indoor%20scenes%20and%0Aimproves%20robustness%20%28even%20under%20pose%20drifts%29.%20In%20local%20scene%20representation%2C%0APLGSLAM%20utilizes%20tri-planes%20for%20local%20high-frequency%20features%20with%20multi-layer%0Aperceptron%20%28MLP%29%20networks%20for%20the%20low-frequency%20feature%2C%20achieving%20smoothness%0Aand%20scene%20completion%20in%20unobserved%20areas.%20Moreover%2C%20we%20propose%20local-to-global%0Abundle%20adjustment%20method%20with%20a%20global%20keyframe%20database%20to%20address%20the%0Aincreased%20pose%20drifts%20on%20long%20sequences.%20Experimental%20results%20demonstrate%20that%0APLGSLAM%20achieves%20state-of-the-art%20scene%20reconstruction%20results%20and%20tracking%0Aperformance%20across%20various%20datasets%20and%20scenarios%20%28both%20in%20small%20and%0Alarge-scale%20indoor%20environments%29.%20The%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/dtc111111/plgslam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09866v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLGSLAM%253A%2520Progressive%2520Neural%2520Scene%2520Represenation%2520with%2520Local%2520to%2520Global%250A%2520%2520Bundle%2520Adjustment%26entry.906535625%3DTianchen%2520Deng%2520and%2520Guole%2520Shen%2520and%2520Tong%2520Qin%2520and%2520Jianyu%2520Wang%2520and%2520Wentao%2520Zhao%2520and%2520Jingchuan%2520Wang%2520and%2520Danwei%2520Wang%2520and%2520Weidong%2520Chen%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520scene%2520representations%2520have%2520recently%2520shown%2520encouraging%2520results%250Ain%2520dense%2520visual%2520SLAM.%2520However%252C%2520existing%2520methods%2520produce%2520low-quality%2520scene%250Areconstruction%2520and%2520low-accuracy%2520localization%2520performance%2520when%2520scaling%2520up%2520to%250Alarge%2520indoor%2520scenes%2520and%2520long%2520sequences.%2520These%2520limitations%2520are%2520mainly%2520due%2520to%250Atheir%2520single%252C%2520global%2520radiance%2520field%2520with%2520finite%2520capacity%252C%2520which%2520does%2520not%2520adapt%250Ato%2520large%2520scenarios.%2520Their%2520end-to-end%2520pose%2520networks%2520are%2520also%2520not%2520robust%2520enough%250Awith%2520the%2520growth%2520of%2520cumulative%2520errors%2520in%2520large%2520scenes.%2520To%2520this%2520end%252C%2520we%2520introduce%250APLGSLAM%252C%2520a%2520neural%2520visual%2520SLAM%2520system%2520capable%2520of%2520high-fidelity%2520surface%250Areconstruction%2520and%2520robust%2520camera%2520tracking%2520in%2520real-time.%2520To%2520handle%2520large-scale%250Aindoor%2520scenes%252C%2520PLGSLAM%2520proposes%2520a%2520progressive%2520scene%2520representation%2520method%2520which%250Adynamically%2520allocates%2520new%2520local%2520scene%2520representation%2520trained%2520with%2520frames%2520within%250Aa%2520local%2520sliding%2520window.%2520This%2520allows%2520us%2520to%2520scale%2520up%2520to%2520larger%2520indoor%2520scenes%2520and%250Aimproves%2520robustness%2520%2528even%2520under%2520pose%2520drifts%2529.%2520In%2520local%2520scene%2520representation%252C%250APLGSLAM%2520utilizes%2520tri-planes%2520for%2520local%2520high-frequency%2520features%2520with%2520multi-layer%250Aperceptron%2520%2528MLP%2529%2520networks%2520for%2520the%2520low-frequency%2520feature%252C%2520achieving%2520smoothness%250Aand%2520scene%2520completion%2520in%2520unobserved%2520areas.%2520Moreover%252C%2520we%2520propose%2520local-to-global%250Abundle%2520adjustment%2520method%2520with%2520a%2520global%2520keyframe%2520database%2520to%2520address%2520the%250Aincreased%2520pose%2520drifts%2520on%2520long%2520sequences.%2520Experimental%2520results%2520demonstrate%2520that%250APLGSLAM%2520achieves%2520state-of-the-art%2520scene%2520reconstruction%2520results%2520and%2520tracking%250Aperformance%2520across%2520various%2520datasets%2520and%2520scenarios%2520%2528both%2520in%2520small%2520and%250Alarge-scale%2520indoor%2520environments%2529.%2520The%2520code%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/dtc111111/plgslam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09866v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLGSLAM%3A%20Progressive%20Neural%20Scene%20Represenation%20with%20Local%20to%20Global%0A%20%20Bundle%20Adjustment&entry.906535625=Tianchen%20Deng%20and%20Guole%20Shen%20and%20Tong%20Qin%20and%20Jianyu%20Wang%20and%20Wentao%20Zhao%20and%20Jingchuan%20Wang%20and%20Danwei%20Wang%20and%20Weidong%20Chen&entry.1292438233=%20%20Neural%20implicit%20scene%20representations%20have%20recently%20shown%20encouraging%20results%0Ain%20dense%20visual%20SLAM.%20However%2C%20existing%20methods%20produce%20low-quality%20scene%0Areconstruction%20and%20low-accuracy%20localization%20performance%20when%20scaling%20up%20to%0Alarge%20indoor%20scenes%20and%20long%20sequences.%20These%20limitations%20are%20mainly%20due%20to%0Atheir%20single%2C%20global%20radiance%20field%20with%20finite%20capacity%2C%20which%20does%20not%20adapt%0Ato%20large%20scenarios.%20Their%20end-to-end%20pose%20networks%20are%20also%20not%20robust%20enough%0Awith%20the%20growth%20of%20cumulative%20errors%20in%20large%20scenes.%20To%20this%20end%2C%20we%20introduce%0APLGSLAM%2C%20a%20neural%20visual%20SLAM%20system%20capable%20of%20high-fidelity%20surface%0Areconstruction%20and%20robust%20camera%20tracking%20in%20real-time.%20To%20handle%20large-scale%0Aindoor%20scenes%2C%20PLGSLAM%20proposes%20a%20progressive%20scene%20representation%20method%20which%0Adynamically%20allocates%20new%20local%20scene%20representation%20trained%20with%20frames%20within%0Aa%20local%20sliding%20window.%20This%20allows%20us%20to%20scale%20up%20to%20larger%20indoor%20scenes%20and%0Aimproves%20robustness%20%28even%20under%20pose%20drifts%29.%20In%20local%20scene%20representation%2C%0APLGSLAM%20utilizes%20tri-planes%20for%20local%20high-frequency%20features%20with%20multi-layer%0Aperceptron%20%28MLP%29%20networks%20for%20the%20low-frequency%20feature%2C%20achieving%20smoothness%0Aand%20scene%20completion%20in%20unobserved%20areas.%20Moreover%2C%20we%20propose%20local-to-global%0Abundle%20adjustment%20method%20with%20a%20global%20keyframe%20database%20to%20address%20the%0Aincreased%20pose%20drifts%20on%20long%20sequences.%20Experimental%20results%20demonstrate%20that%0APLGSLAM%20achieves%20state-of-the-art%20scene%20reconstruction%20results%20and%20tracking%0Aperformance%20across%20various%20datasets%20and%20scenarios%20%28both%20in%20small%20and%0Alarge-scale%20indoor%20environments%29.%20The%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/dtc111111/plgslam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09866v3&entry.124074799=Read"},
{"title": "Multi-Granularity Class Prototype Topology Distillation for\n  Class-Incremental Source-Free Unsupervised Domain Adaptation", "author": "Peihua Deng and Jiehua Zhang and Xichun Sheng and Chenggang Yan and Yaoqi Sun and Ying Fu and Liang Li", "abstract": "  This paper explores the Class-Incremental Source-Free Unsupervised Domain\nAdaptation (CI-SFUDA) problem, where the unlabeled target data come\nincrementally without access to labeled source instances. This problem poses\ntwo challenges, the interference of similar source-class knowledge in\ntarget-class representation learning and the shocks of new target knowledge to\nold ones. To address them, we propose the Multi-Granularity Class Prototype\nTopology Distillation (GROTO) algorithm, which effectively transfers the source\nknowledge to the class-incremental target domain. Concretely, we design the\nmulti-granularity class prototype self-organization module and the prototype\ntopology distillation module. First, we mine the positive classes by modeling\naccumulation distributions. Next, we introduce multi-granularity class\nprototypes to generate reliable pseudo-labels, and exploit them to promote the\npositive-class target feature self-organization. Second, the positive-class\nprototypes are leveraged to construct the topological structures of source and\ntarget feature spaces. Then, we perform the topology distillation to\ncontinually mitigate the shocks of new target knowledge to old ones. Extensive\nexperiments demonstrate that our proposed method achieves state-of-the-art\nperformance on three public datasets. Code is available at\nhttps://github.com/dengpeihua/GROTO.\n", "link": "http://arxiv.org/abs/2411.16064v4", "date": "2025-05-27", "relevancy": 2.6553, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Granularity%20Class%20Prototype%20Topology%20Distillation%20for%0A%20%20Class-Incremental%20Source-Free%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20Multi-Granularity%20Class%20Prototype%20Topology%20Distillation%20for%0A%20%20Class-Incremental%20Source-Free%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Peihua%20Deng%20and%20Jiehua%20Zhang%20and%20Xichun%20Sheng%20and%20Chenggang%20Yan%20and%20Yaoqi%20Sun%20and%20Ying%20Fu%20and%20Liang%20Li%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20Class-Incremental%20Source-Free%20Unsupervised%20Domain%0AAdaptation%20%28CI-SFUDA%29%20problem%2C%20where%20the%20unlabeled%20target%20data%20come%0Aincrementally%20without%20access%20to%20labeled%20source%20instances.%20This%20problem%20poses%0Atwo%20challenges%2C%20the%20interference%20of%20similar%20source-class%20knowledge%20in%0Atarget-class%20representation%20learning%20and%20the%20shocks%20of%20new%20target%20knowledge%20to%0Aold%20ones.%20To%20address%20them%2C%20we%20propose%20the%20Multi-Granularity%20Class%20Prototype%0ATopology%20Distillation%20%28GROTO%29%20algorithm%2C%20which%20effectively%20transfers%20the%20source%0Aknowledge%20to%20the%20class-incremental%20target%20domain.%20Concretely%2C%20we%20design%20the%0Amulti-granularity%20class%20prototype%20self-organization%20module%20and%20the%20prototype%0Atopology%20distillation%20module.%20First%2C%20we%20mine%20the%20positive%20classes%20by%20modeling%0Aaccumulation%20distributions.%20Next%2C%20we%20introduce%20multi-granularity%20class%0Aprototypes%20to%20generate%20reliable%20pseudo-labels%2C%20and%20exploit%20them%20to%20promote%20the%0Apositive-class%20target%20feature%20self-organization.%20Second%2C%20the%20positive-class%0Aprototypes%20are%20leveraged%20to%20construct%20the%20topological%20structures%20of%20source%20and%0Atarget%20feature%20spaces.%20Then%2C%20we%20perform%20the%20topology%20distillation%20to%0Acontinually%20mitigate%20the%20shocks%20of%20new%20target%20knowledge%20to%20old%20ones.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20on%20three%20public%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/dengpeihua/GROTO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16064v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Granularity%2520Class%2520Prototype%2520Topology%2520Distillation%2520for%250A%2520%2520Class-Incremental%2520Source-Free%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DPeihua%2520Deng%2520and%2520Jiehua%2520Zhang%2520and%2520Xichun%2520Sheng%2520and%2520Chenggang%2520Yan%2520and%2520Yaoqi%2520Sun%2520and%2520Ying%2520Fu%2520and%2520Liang%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520Class-Incremental%2520Source-Free%2520Unsupervised%2520Domain%250AAdaptation%2520%2528CI-SFUDA%2529%2520problem%252C%2520where%2520the%2520unlabeled%2520target%2520data%2520come%250Aincrementally%2520without%2520access%2520to%2520labeled%2520source%2520instances.%2520This%2520problem%2520poses%250Atwo%2520challenges%252C%2520the%2520interference%2520of%2520similar%2520source-class%2520knowledge%2520in%250Atarget-class%2520representation%2520learning%2520and%2520the%2520shocks%2520of%2520new%2520target%2520knowledge%2520to%250Aold%2520ones.%2520To%2520address%2520them%252C%2520we%2520propose%2520the%2520Multi-Granularity%2520Class%2520Prototype%250ATopology%2520Distillation%2520%2528GROTO%2529%2520algorithm%252C%2520which%2520effectively%2520transfers%2520the%2520source%250Aknowledge%2520to%2520the%2520class-incremental%2520target%2520domain.%2520Concretely%252C%2520we%2520design%2520the%250Amulti-granularity%2520class%2520prototype%2520self-organization%2520module%2520and%2520the%2520prototype%250Atopology%2520distillation%2520module.%2520First%252C%2520we%2520mine%2520the%2520positive%2520classes%2520by%2520modeling%250Aaccumulation%2520distributions.%2520Next%252C%2520we%2520introduce%2520multi-granularity%2520class%250Aprototypes%2520to%2520generate%2520reliable%2520pseudo-labels%252C%2520and%2520exploit%2520them%2520to%2520promote%2520the%250Apositive-class%2520target%2520feature%2520self-organization.%2520Second%252C%2520the%2520positive-class%250Aprototypes%2520are%2520leveraged%2520to%2520construct%2520the%2520topological%2520structures%2520of%2520source%2520and%250Atarget%2520feature%2520spaces.%2520Then%252C%2520we%2520perform%2520the%2520topology%2520distillation%2520to%250Acontinually%2520mitigate%2520the%2520shocks%2520of%2520new%2520target%2520knowledge%2520to%2520old%2520ones.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520three%2520public%2520datasets.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/dengpeihua/GROTO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16064v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Granularity%20Class%20Prototype%20Topology%20Distillation%20for%0A%20%20Class-Incremental%20Source-Free%20Unsupervised%20Domain%20Adaptation&entry.906535625=Peihua%20Deng%20and%20Jiehua%20Zhang%20and%20Xichun%20Sheng%20and%20Chenggang%20Yan%20and%20Yaoqi%20Sun%20and%20Ying%20Fu%20and%20Liang%20Li&entry.1292438233=%20%20This%20paper%20explores%20the%20Class-Incremental%20Source-Free%20Unsupervised%20Domain%0AAdaptation%20%28CI-SFUDA%29%20problem%2C%20where%20the%20unlabeled%20target%20data%20come%0Aincrementally%20without%20access%20to%20labeled%20source%20instances.%20This%20problem%20poses%0Atwo%20challenges%2C%20the%20interference%20of%20similar%20source-class%20knowledge%20in%0Atarget-class%20representation%20learning%20and%20the%20shocks%20of%20new%20target%20knowledge%20to%0Aold%20ones.%20To%20address%20them%2C%20we%20propose%20the%20Multi-Granularity%20Class%20Prototype%0ATopology%20Distillation%20%28GROTO%29%20algorithm%2C%20which%20effectively%20transfers%20the%20source%0Aknowledge%20to%20the%20class-incremental%20target%20domain.%20Concretely%2C%20we%20design%20the%0Amulti-granularity%20class%20prototype%20self-organization%20module%20and%20the%20prototype%0Atopology%20distillation%20module.%20First%2C%20we%20mine%20the%20positive%20classes%20by%20modeling%0Aaccumulation%20distributions.%20Next%2C%20we%20introduce%20multi-granularity%20class%0Aprototypes%20to%20generate%20reliable%20pseudo-labels%2C%20and%20exploit%20them%20to%20promote%20the%0Apositive-class%20target%20feature%20self-organization.%20Second%2C%20the%20positive-class%0Aprototypes%20are%20leveraged%20to%20construct%20the%20topological%20structures%20of%20source%20and%0Atarget%20feature%20spaces.%20Then%2C%20we%20perform%20the%20topology%20distillation%20to%0Acontinually%20mitigate%20the%20shocks%20of%20new%20target%20knowledge%20to%20old%20ones.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20on%20three%20public%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/dengpeihua/GROTO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16064v4&entry.124074799=Read"},
{"title": "Multilingual Pretraining for Pixel Language Models", "author": "Ilker Kesen and Jonas F. Lotz and Ingo Ziegler and Phillip Rust and Desmond Elliott", "abstract": "  Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.\n", "link": "http://arxiv.org/abs/2505.21265v1", "date": "2025-05-27", "relevancy": 2.6459, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Pretraining%20for%20Pixel%20Language%20Models&body=Title%3A%20Multilingual%20Pretraining%20for%20Pixel%20Language%20Models%0AAuthor%3A%20Ilker%20Kesen%20and%20Jonas%20F.%20Lotz%20and%20Ingo%20Ziegler%20and%20Phillip%20Rust%20and%20Desmond%20Elliott%0AAbstract%3A%20%20%20Pixel%20language%20models%20operate%20directly%20on%20images%20of%20rendered%20text%2C%0Aeliminating%20the%20need%20for%20a%20fixed%20vocabulary.%20While%20these%20models%20have%0Ademonstrated%20strong%20capabilities%20for%20downstream%20cross-lingual%20transfer%2C%0Amultilingual%20pretraining%20remains%20underexplored.%20We%20introduce%20PIXEL-M4%2C%20a%20model%0Apretrained%20on%20four%20visually%20and%20linguistically%20diverse%20languages%3A%20English%2C%0AHindi%2C%20Ukrainian%2C%20and%20Simplified%20Chinese.%20Multilingual%20evaluations%20on%20semantic%0Aand%20syntactic%20tasks%20show%20that%20PIXEL-M4%20outperforms%20an%20English-only%20counterpart%0Aon%20non-Latin%20scripts.%20Word-level%20probing%20analyses%20confirm%20that%20PIXEL-M4%0Acaptures%20rich%20linguistic%20features%2C%20even%20in%20languages%20not%20seen%20during%0Apretraining.%20Furthermore%2C%20an%20analysis%20of%20its%20hidden%20representations%20shows%20that%0Amultilingual%20pretraining%20yields%20a%20semantic%20embedding%20space%20closely%20aligned%0Aacross%20the%20languages%20used%20for%20pretraining.%20This%20work%20demonstrates%20that%0Amultilingual%20pretraining%20substantially%20enhances%20the%20capability%20of%20pixel%0Alanguage%20models%20to%20effectively%20support%20a%20diverse%20set%20of%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Pretraining%2520for%2520Pixel%2520Language%2520Models%26entry.906535625%3DIlker%2520Kesen%2520and%2520Jonas%2520F.%2520Lotz%2520and%2520Ingo%2520Ziegler%2520and%2520Phillip%2520Rust%2520and%2520Desmond%2520Elliott%26entry.1292438233%3D%2520%2520Pixel%2520language%2520models%2520operate%2520directly%2520on%2520images%2520of%2520rendered%2520text%252C%250Aeliminating%2520the%2520need%2520for%2520a%2520fixed%2520vocabulary.%2520While%2520these%2520models%2520have%250Ademonstrated%2520strong%2520capabilities%2520for%2520downstream%2520cross-lingual%2520transfer%252C%250Amultilingual%2520pretraining%2520remains%2520underexplored.%2520We%2520introduce%2520PIXEL-M4%252C%2520a%2520model%250Apretrained%2520on%2520four%2520visually%2520and%2520linguistically%2520diverse%2520languages%253A%2520English%252C%250AHindi%252C%2520Ukrainian%252C%2520and%2520Simplified%2520Chinese.%2520Multilingual%2520evaluations%2520on%2520semantic%250Aand%2520syntactic%2520tasks%2520show%2520that%2520PIXEL-M4%2520outperforms%2520an%2520English-only%2520counterpart%250Aon%2520non-Latin%2520scripts.%2520Word-level%2520probing%2520analyses%2520confirm%2520that%2520PIXEL-M4%250Acaptures%2520rich%2520linguistic%2520features%252C%2520even%2520in%2520languages%2520not%2520seen%2520during%250Apretraining.%2520Furthermore%252C%2520an%2520analysis%2520of%2520its%2520hidden%2520representations%2520shows%2520that%250Amultilingual%2520pretraining%2520yields%2520a%2520semantic%2520embedding%2520space%2520closely%2520aligned%250Aacross%2520the%2520languages%2520used%2520for%2520pretraining.%2520This%2520work%2520demonstrates%2520that%250Amultilingual%2520pretraining%2520substantially%2520enhances%2520the%2520capability%2520of%2520pixel%250Alanguage%2520models%2520to%2520effectively%2520support%2520a%2520diverse%2520set%2520of%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Pretraining%20for%20Pixel%20Language%20Models&entry.906535625=Ilker%20Kesen%20and%20Jonas%20F.%20Lotz%20and%20Ingo%20Ziegler%20and%20Phillip%20Rust%20and%20Desmond%20Elliott&entry.1292438233=%20%20Pixel%20language%20models%20operate%20directly%20on%20images%20of%20rendered%20text%2C%0Aeliminating%20the%20need%20for%20a%20fixed%20vocabulary.%20While%20these%20models%20have%0Ademonstrated%20strong%20capabilities%20for%20downstream%20cross-lingual%20transfer%2C%0Amultilingual%20pretraining%20remains%20underexplored.%20We%20introduce%20PIXEL-M4%2C%20a%20model%0Apretrained%20on%20four%20visually%20and%20linguistically%20diverse%20languages%3A%20English%2C%0AHindi%2C%20Ukrainian%2C%20and%20Simplified%20Chinese.%20Multilingual%20evaluations%20on%20semantic%0Aand%20syntactic%20tasks%20show%20that%20PIXEL-M4%20outperforms%20an%20English-only%20counterpart%0Aon%20non-Latin%20scripts.%20Word-level%20probing%20analyses%20confirm%20that%20PIXEL-M4%0Acaptures%20rich%20linguistic%20features%2C%20even%20in%20languages%20not%20seen%20during%0Apretraining.%20Furthermore%2C%20an%20analysis%20of%20its%20hidden%20representations%20shows%20that%0Amultilingual%20pretraining%20yields%20a%20semantic%20embedding%20space%20closely%20aligned%0Aacross%20the%20languages%20used%20for%20pretraining.%20This%20work%20demonstrates%20that%0Amultilingual%20pretraining%20substantially%20enhances%20the%20capability%20of%20pixel%0Alanguage%20models%20to%20effectively%20support%20a%20diverse%20set%20of%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21265v1&entry.124074799=Read"},
{"title": "DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing", "author": "Jingyi Yang and Xun Lin and Zitong Yu and Liepiao Zhang and Xin Liu and Hui Li and Xiaochen Yuan and Xiaochun Cao", "abstract": "  With the availability of diverse sensor modalities (i.e., RGB, Depth,\nInfrared) and the success of multi-modal learning, multi-modal face\nanti-spoofing (FAS) has emerged as a prominent research focus. The intuition\nbehind it is that leveraging multiple modalities can uncover more intrinsic\nspoofing traces. However, this approach presents more risk of misalignment. We\nidentify two main types of misalignment: (1) \\textbf{Intra-domain modality\nmisalignment}, where the importance of each modality varies across different\nattacks. For instance, certain modalities (e.g., Depth) may be non-defensive\nagainst specific attacks (e.g., 3D mask), indicating that each modality has\nunique strengths and weaknesses in countering particular attacks. Consequently,\nsimple fusion strategies may fall short. (2) \\textbf{Inter-domain modality\nmisalignment}, where the introduction of additional modalities exacerbates\ndomain shifts, potentially overshadowing the benefits of complementary fusion.\nTo tackle (1), we propose a alignment module between modalities based on mutual\ninformation, which adaptively enhances favorable modalities while suppressing\nunfavorable ones. To address (2), we employ a dual alignment optimization\nmethod that aligns both sub-domain hyperplanes and modality angle margins,\nthereby mitigating domain gaps. Our method, dubbed \\textbf{D}ual\n\\textbf{A}lignment of \\textbf{D}omain and \\textbf{M}odality (DADM), achieves\nstate-of-the-art performance in extensive experiments across four challenging\nprotocols demonstrating its robustness in multi-modal domain generalization\nscenarios. The codes will be released soon.\n", "link": "http://arxiv.org/abs/2503.00429v2", "date": "2025-05-27", "relevancy": 2.6453, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5741}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5127}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DADM%3A%20Dual%20Alignment%20of%20Domain%20and%20Modality%20for%20Face%20Anti-spoofing&body=Title%3A%20DADM%3A%20Dual%20Alignment%20of%20Domain%20and%20Modality%20for%20Face%20Anti-spoofing%0AAuthor%3A%20Jingyi%20Yang%20and%20Xun%20Lin%20and%20Zitong%20Yu%20and%20Liepiao%20Zhang%20and%20Xin%20Liu%20and%20Hui%20Li%20and%20Xiaochen%20Yuan%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20With%20the%20availability%20of%20diverse%20sensor%20modalities%20%28i.e.%2C%20RGB%2C%20Depth%2C%0AInfrared%29%20and%20the%20success%20of%20multi-modal%20learning%2C%20multi-modal%20face%0Aanti-spoofing%20%28FAS%29%20has%20emerged%20as%20a%20prominent%20research%20focus.%20The%20intuition%0Abehind%20it%20is%20that%20leveraging%20multiple%20modalities%20can%20uncover%20more%20intrinsic%0Aspoofing%20traces.%20However%2C%20this%20approach%20presents%20more%20risk%20of%20misalignment.%20We%0Aidentify%20two%20main%20types%20of%20misalignment%3A%20%281%29%20%5Ctextbf%7BIntra-domain%20modality%0Amisalignment%7D%2C%20where%20the%20importance%20of%20each%20modality%20varies%20across%20different%0Aattacks.%20For%20instance%2C%20certain%20modalities%20%28e.g.%2C%20Depth%29%20may%20be%20non-defensive%0Aagainst%20specific%20attacks%20%28e.g.%2C%203D%20mask%29%2C%20indicating%20that%20each%20modality%20has%0Aunique%20strengths%20and%20weaknesses%20in%20countering%20particular%20attacks.%20Consequently%2C%0Asimple%20fusion%20strategies%20may%20fall%20short.%20%282%29%20%5Ctextbf%7BInter-domain%20modality%0Amisalignment%7D%2C%20where%20the%20introduction%20of%20additional%20modalities%20exacerbates%0Adomain%20shifts%2C%20potentially%20overshadowing%20the%20benefits%20of%20complementary%20fusion.%0ATo%20tackle%20%281%29%2C%20we%20propose%20a%20alignment%20module%20between%20modalities%20based%20on%20mutual%0Ainformation%2C%20which%20adaptively%20enhances%20favorable%20modalities%20while%20suppressing%0Aunfavorable%20ones.%20To%20address%20%282%29%2C%20we%20employ%20a%20dual%20alignment%20optimization%0Amethod%20that%20aligns%20both%20sub-domain%20hyperplanes%20and%20modality%20angle%20margins%2C%0Athereby%20mitigating%20domain%20gaps.%20Our%20method%2C%20dubbed%20%5Ctextbf%7BD%7Dual%0A%5Ctextbf%7BA%7Dlignment%20of%20%5Ctextbf%7BD%7Domain%20and%20%5Ctextbf%7BM%7Dodality%20%28DADM%29%2C%20achieves%0Astate-of-the-art%20performance%20in%20extensive%20experiments%20across%20four%20challenging%0Aprotocols%20demonstrating%20its%20robustness%20in%20multi-modal%20domain%20generalization%0Ascenarios.%20The%20codes%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00429v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDADM%253A%2520Dual%2520Alignment%2520of%2520Domain%2520and%2520Modality%2520for%2520Face%2520Anti-spoofing%26entry.906535625%3DJingyi%2520Yang%2520and%2520Xun%2520Lin%2520and%2520Zitong%2520Yu%2520and%2520Liepiao%2520Zhang%2520and%2520Xin%2520Liu%2520and%2520Hui%2520Li%2520and%2520Xiaochen%2520Yuan%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520With%2520the%2520availability%2520of%2520diverse%2520sensor%2520modalities%2520%2528i.e.%252C%2520RGB%252C%2520Depth%252C%250AInfrared%2529%2520and%2520the%2520success%2520of%2520multi-modal%2520learning%252C%2520multi-modal%2520face%250Aanti-spoofing%2520%2528FAS%2529%2520has%2520emerged%2520as%2520a%2520prominent%2520research%2520focus.%2520The%2520intuition%250Abehind%2520it%2520is%2520that%2520leveraging%2520multiple%2520modalities%2520can%2520uncover%2520more%2520intrinsic%250Aspoofing%2520traces.%2520However%252C%2520this%2520approach%2520presents%2520more%2520risk%2520of%2520misalignment.%2520We%250Aidentify%2520two%2520main%2520types%2520of%2520misalignment%253A%2520%25281%2529%2520%255Ctextbf%257BIntra-domain%2520modality%250Amisalignment%257D%252C%2520where%2520the%2520importance%2520of%2520each%2520modality%2520varies%2520across%2520different%250Aattacks.%2520For%2520instance%252C%2520certain%2520modalities%2520%2528e.g.%252C%2520Depth%2529%2520may%2520be%2520non-defensive%250Aagainst%2520specific%2520attacks%2520%2528e.g.%252C%25203D%2520mask%2529%252C%2520indicating%2520that%2520each%2520modality%2520has%250Aunique%2520strengths%2520and%2520weaknesses%2520in%2520countering%2520particular%2520attacks.%2520Consequently%252C%250Asimple%2520fusion%2520strategies%2520may%2520fall%2520short.%2520%25282%2529%2520%255Ctextbf%257BInter-domain%2520modality%250Amisalignment%257D%252C%2520where%2520the%2520introduction%2520of%2520additional%2520modalities%2520exacerbates%250Adomain%2520shifts%252C%2520potentially%2520overshadowing%2520the%2520benefits%2520of%2520complementary%2520fusion.%250ATo%2520tackle%2520%25281%2529%252C%2520we%2520propose%2520a%2520alignment%2520module%2520between%2520modalities%2520based%2520on%2520mutual%250Ainformation%252C%2520which%2520adaptively%2520enhances%2520favorable%2520modalities%2520while%2520suppressing%250Aunfavorable%2520ones.%2520To%2520address%2520%25282%2529%252C%2520we%2520employ%2520a%2520dual%2520alignment%2520optimization%250Amethod%2520that%2520aligns%2520both%2520sub-domain%2520hyperplanes%2520and%2520modality%2520angle%2520margins%252C%250Athereby%2520mitigating%2520domain%2520gaps.%2520Our%2520method%252C%2520dubbed%2520%255Ctextbf%257BD%257Dual%250A%255Ctextbf%257BA%257Dlignment%2520of%2520%255Ctextbf%257BD%257Domain%2520and%2520%255Ctextbf%257BM%257Dodality%2520%2528DADM%2529%252C%2520achieves%250Astate-of-the-art%2520performance%2520in%2520extensive%2520experiments%2520across%2520four%2520challenging%250Aprotocols%2520demonstrating%2520its%2520robustness%2520in%2520multi-modal%2520domain%2520generalization%250Ascenarios.%2520The%2520codes%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00429v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DADM%3A%20Dual%20Alignment%20of%20Domain%20and%20Modality%20for%20Face%20Anti-spoofing&entry.906535625=Jingyi%20Yang%20and%20Xun%20Lin%20and%20Zitong%20Yu%20and%20Liepiao%20Zhang%20and%20Xin%20Liu%20and%20Hui%20Li%20and%20Xiaochen%20Yuan%20and%20Xiaochun%20Cao&entry.1292438233=%20%20With%20the%20availability%20of%20diverse%20sensor%20modalities%20%28i.e.%2C%20RGB%2C%20Depth%2C%0AInfrared%29%20and%20the%20success%20of%20multi-modal%20learning%2C%20multi-modal%20face%0Aanti-spoofing%20%28FAS%29%20has%20emerged%20as%20a%20prominent%20research%20focus.%20The%20intuition%0Abehind%20it%20is%20that%20leveraging%20multiple%20modalities%20can%20uncover%20more%20intrinsic%0Aspoofing%20traces.%20However%2C%20this%20approach%20presents%20more%20risk%20of%20misalignment.%20We%0Aidentify%20two%20main%20types%20of%20misalignment%3A%20%281%29%20%5Ctextbf%7BIntra-domain%20modality%0Amisalignment%7D%2C%20where%20the%20importance%20of%20each%20modality%20varies%20across%20different%0Aattacks.%20For%20instance%2C%20certain%20modalities%20%28e.g.%2C%20Depth%29%20may%20be%20non-defensive%0Aagainst%20specific%20attacks%20%28e.g.%2C%203D%20mask%29%2C%20indicating%20that%20each%20modality%20has%0Aunique%20strengths%20and%20weaknesses%20in%20countering%20particular%20attacks.%20Consequently%2C%0Asimple%20fusion%20strategies%20may%20fall%20short.%20%282%29%20%5Ctextbf%7BInter-domain%20modality%0Amisalignment%7D%2C%20where%20the%20introduction%20of%20additional%20modalities%20exacerbates%0Adomain%20shifts%2C%20potentially%20overshadowing%20the%20benefits%20of%20complementary%20fusion.%0ATo%20tackle%20%281%29%2C%20we%20propose%20a%20alignment%20module%20between%20modalities%20based%20on%20mutual%0Ainformation%2C%20which%20adaptively%20enhances%20favorable%20modalities%20while%20suppressing%0Aunfavorable%20ones.%20To%20address%20%282%29%2C%20we%20employ%20a%20dual%20alignment%20optimization%0Amethod%20that%20aligns%20both%20sub-domain%20hyperplanes%20and%20modality%20angle%20margins%2C%0Athereby%20mitigating%20domain%20gaps.%20Our%20method%2C%20dubbed%20%5Ctextbf%7BD%7Dual%0A%5Ctextbf%7BA%7Dlignment%20of%20%5Ctextbf%7BD%7Domain%20and%20%5Ctextbf%7BM%7Dodality%20%28DADM%29%2C%20achieves%0Astate-of-the-art%20performance%20in%20extensive%20experiments%20across%20four%20challenging%0Aprotocols%20demonstrating%20its%20robustness%20in%20multi-modal%20domain%20generalization%0Ascenarios.%20The%20codes%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00429v2&entry.124074799=Read"},
{"title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with\n  Limited Participants", "author": "Louis Jalouzot and Alexis Thual and Yair Lakretz and Christophe Pallier and Bertrand Thirion", "abstract": "  We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort.\n", "link": "http://arxiv.org/abs/2505.21304v1", "date": "2025-05-27", "relevancy": 2.6421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20fMRI%20Data%20Acquisition%20for%20Decoding%20Natural%20Speech%20with%0A%20%20Limited%20Participants&body=Title%3A%20Optimizing%20fMRI%20Data%20Acquisition%20for%20Decoding%20Natural%20Speech%20with%0A%20%20Limited%20Participants%0AAuthor%3A%20Louis%20Jalouzot%20and%20Alexis%20Thual%20and%20Yair%20Lakretz%20and%20Christophe%20Pallier%20and%20Bertrand%20Thirion%0AAbstract%3A%20%20%20We%20investigate%20optimal%20strategies%20for%20decoding%20perceived%20natural%20speech%20from%0AfMRI%20data%20acquired%20from%20a%20limited%20number%20of%20participants.%20Leveraging%20Lebel%20et%0Aal.%20%282023%29%27s%20dataset%20of%208%20participants%2C%20we%20first%20demonstrate%20the%20effectiveness%0Aof%20training%20deep%20neural%20networks%20to%20predict%20LLM-derived%20text%20representations%0Afrom%20fMRI%20activity.%20Then%2C%20in%20this%20data%20regime%2C%20we%20observe%20that%20multi-subject%0Atraining%20does%20not%20improve%20decoding%20accuracy%20compared%20to%20single-subject%0Aapproach.%20Furthermore%2C%20training%20on%20similar%20or%20different%20stimuli%20across%20subjects%0Ahas%20a%20negligible%20effect%20on%20decoding%20accuracy.%20Finally%2C%20we%20find%20that%20our%0Adecoders%20better%20model%20syntactic%20than%20semantic%20features%2C%20and%20that%20stories%0Acontaining%20sentences%20with%20complex%20syntax%20or%20rich%20semantic%20content%20are%20more%0Achallenging%20to%20decode.%20While%20our%20results%20demonstrate%20the%20benefits%20of%20having%0Aextensive%20data%20per%20participant%20%28deep%20phenotyping%29%2C%20they%20suggest%20that%20leveraging%0Amulti-subject%20for%20natural%20speech%20decoding%20likely%20requires%20deeper%20phenotyping%20or%0Aa%20substantially%20larger%20cohort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520fMRI%2520Data%2520Acquisition%2520for%2520Decoding%2520Natural%2520Speech%2520with%250A%2520%2520Limited%2520Participants%26entry.906535625%3DLouis%2520Jalouzot%2520and%2520Alexis%2520Thual%2520and%2520Yair%2520Lakretz%2520and%2520Christophe%2520Pallier%2520and%2520Bertrand%2520Thirion%26entry.1292438233%3D%2520%2520We%2520investigate%2520optimal%2520strategies%2520for%2520decoding%2520perceived%2520natural%2520speech%2520from%250AfMRI%2520data%2520acquired%2520from%2520a%2520limited%2520number%2520of%2520participants.%2520Leveraging%2520Lebel%2520et%250Aal.%2520%25282023%2529%2527s%2520dataset%2520of%25208%2520participants%252C%2520we%2520first%2520demonstrate%2520the%2520effectiveness%250Aof%2520training%2520deep%2520neural%2520networks%2520to%2520predict%2520LLM-derived%2520text%2520representations%250Afrom%2520fMRI%2520activity.%2520Then%252C%2520in%2520this%2520data%2520regime%252C%2520we%2520observe%2520that%2520multi-subject%250Atraining%2520does%2520not%2520improve%2520decoding%2520accuracy%2520compared%2520to%2520single-subject%250Aapproach.%2520Furthermore%252C%2520training%2520on%2520similar%2520or%2520different%2520stimuli%2520across%2520subjects%250Ahas%2520a%2520negligible%2520effect%2520on%2520decoding%2520accuracy.%2520Finally%252C%2520we%2520find%2520that%2520our%250Adecoders%2520better%2520model%2520syntactic%2520than%2520semantic%2520features%252C%2520and%2520that%2520stories%250Acontaining%2520sentences%2520with%2520complex%2520syntax%2520or%2520rich%2520semantic%2520content%2520are%2520more%250Achallenging%2520to%2520decode.%2520While%2520our%2520results%2520demonstrate%2520the%2520benefits%2520of%2520having%250Aextensive%2520data%2520per%2520participant%2520%2528deep%2520phenotyping%2529%252C%2520they%2520suggest%2520that%2520leveraging%250Amulti-subject%2520for%2520natural%2520speech%2520decoding%2520likely%2520requires%2520deeper%2520phenotyping%2520or%250Aa%2520substantially%2520larger%2520cohort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20fMRI%20Data%20Acquisition%20for%20Decoding%20Natural%20Speech%20with%0A%20%20Limited%20Participants&entry.906535625=Louis%20Jalouzot%20and%20Alexis%20Thual%20and%20Yair%20Lakretz%20and%20Christophe%20Pallier%20and%20Bertrand%20Thirion&entry.1292438233=%20%20We%20investigate%20optimal%20strategies%20for%20decoding%20perceived%20natural%20speech%20from%0AfMRI%20data%20acquired%20from%20a%20limited%20number%20of%20participants.%20Leveraging%20Lebel%20et%0Aal.%20%282023%29%27s%20dataset%20of%208%20participants%2C%20we%20first%20demonstrate%20the%20effectiveness%0Aof%20training%20deep%20neural%20networks%20to%20predict%20LLM-derived%20text%20representations%0Afrom%20fMRI%20activity.%20Then%2C%20in%20this%20data%20regime%2C%20we%20observe%20that%20multi-subject%0Atraining%20does%20not%20improve%20decoding%20accuracy%20compared%20to%20single-subject%0Aapproach.%20Furthermore%2C%20training%20on%20similar%20or%20different%20stimuli%20across%20subjects%0Ahas%20a%20negligible%20effect%20on%20decoding%20accuracy.%20Finally%2C%20we%20find%20that%20our%0Adecoders%20better%20model%20syntactic%20than%20semantic%20features%2C%20and%20that%20stories%0Acontaining%20sentences%20with%20complex%20syntax%20or%20rich%20semantic%20content%20are%20more%0Achallenging%20to%20decode.%20While%20our%20results%20demonstrate%20the%20benefits%20of%20having%0Aextensive%20data%20per%20participant%20%28deep%20phenotyping%29%2C%20they%20suggest%20that%20leveraging%0Amulti-subject%20for%20natural%20speech%20decoding%20likely%20requires%20deeper%20phenotyping%20or%0Aa%20substantially%20larger%20cohort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21304v1&entry.124074799=Read"},
{"title": "YOLO-SPCI: Enhancing Remote Sensing Object Detection via\n  Selective-Perspective-Class Integration", "author": "Xinyuan Wang and Lian Peng and Xiangcheng Li and Yilin He and KinTak U", "abstract": "  Object detection in remote sensing imagery remains a challenging task due to\nextreme scale variation, dense object distributions, and cluttered backgrounds.\nWhile recent detectors such as YOLOv8 have shown promising results, their\nbackbone architectures lack explicit mechanisms to guide multi-scale feature\nrefinement, limiting performance on high-resolution aerial data. In this work,\nwe propose YOLO-SPCI, an attention-enhanced detection framework that introduces\na lightweight Selective-Perspective-Class Integration (SPCI) module to improve\nfeature representation. The SPCI module integrates three components: a\nSelective Stream Gate (SSG) for adaptive regulation of global feature flow, a\nPerspective Fusion Module (PFM) for context-aware multi-scale integration, and\na Class Discrimination Module (CDM) to enhance inter-class separability. We\nembed two SPCI blocks into the P3 and P5 stages of the YOLOv8 backbone,\nenabling effective refinement while preserving compatibility with the original\nneck and head. Experiments on the NWPU VHR-10 dataset demonstrate that\nYOLO-SPCI achieves superior performance compared to state-of-the-art detectors.\n", "link": "http://arxiv.org/abs/2505.21370v1", "date": "2025-05-27", "relevancy": 2.6392, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLO-SPCI%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20via%0A%20%20Selective-Perspective-Class%20Integration&body=Title%3A%20YOLO-SPCI%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20via%0A%20%20Selective-Perspective-Class%20Integration%0AAuthor%3A%20Xinyuan%20Wang%20and%20Lian%20Peng%20and%20Xiangcheng%20Li%20and%20Yilin%20He%20and%20KinTak%20U%0AAbstract%3A%20%20%20Object%20detection%20in%20remote%20sensing%20imagery%20remains%20a%20challenging%20task%20due%20to%0Aextreme%20scale%20variation%2C%20dense%20object%20distributions%2C%20and%20cluttered%20backgrounds.%0AWhile%20recent%20detectors%20such%20as%20YOLOv8%20have%20shown%20promising%20results%2C%20their%0Abackbone%20architectures%20lack%20explicit%20mechanisms%20to%20guide%20multi-scale%20feature%0Arefinement%2C%20limiting%20performance%20on%20high-resolution%20aerial%20data.%20In%20this%20work%2C%0Awe%20propose%20YOLO-SPCI%2C%20an%20attention-enhanced%20detection%20framework%20that%20introduces%0Aa%20lightweight%20Selective-Perspective-Class%20Integration%20%28SPCI%29%20module%20to%20improve%0Afeature%20representation.%20The%20SPCI%20module%20integrates%20three%20components%3A%20a%0ASelective%20Stream%20Gate%20%28SSG%29%20for%20adaptive%20regulation%20of%20global%20feature%20flow%2C%20a%0APerspective%20Fusion%20Module%20%28PFM%29%20for%20context-aware%20multi-scale%20integration%2C%20and%0Aa%20Class%20Discrimination%20Module%20%28CDM%29%20to%20enhance%20inter-class%20separability.%20We%0Aembed%20two%20SPCI%20blocks%20into%20the%20P3%20and%20P5%20stages%20of%20the%20YOLOv8%20backbone%2C%0Aenabling%20effective%20refinement%20while%20preserving%20compatibility%20with%20the%20original%0Aneck%20and%20head.%20Experiments%20on%20the%20NWPU%20VHR-10%20dataset%20demonstrate%20that%0AYOLO-SPCI%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLO-SPCI%253A%2520Enhancing%2520Remote%2520Sensing%2520Object%2520Detection%2520via%250A%2520%2520Selective-Perspective-Class%2520Integration%26entry.906535625%3DXinyuan%2520Wang%2520and%2520Lian%2520Peng%2520and%2520Xiangcheng%2520Li%2520and%2520Yilin%2520He%2520and%2520KinTak%2520U%26entry.1292438233%3D%2520%2520Object%2520detection%2520in%2520remote%2520sensing%2520imagery%2520remains%2520a%2520challenging%2520task%2520due%2520to%250Aextreme%2520scale%2520variation%252C%2520dense%2520object%2520distributions%252C%2520and%2520cluttered%2520backgrounds.%250AWhile%2520recent%2520detectors%2520such%2520as%2520YOLOv8%2520have%2520shown%2520promising%2520results%252C%2520their%250Abackbone%2520architectures%2520lack%2520explicit%2520mechanisms%2520to%2520guide%2520multi-scale%2520feature%250Arefinement%252C%2520limiting%2520performance%2520on%2520high-resolution%2520aerial%2520data.%2520In%2520this%2520work%252C%250Awe%2520propose%2520YOLO-SPCI%252C%2520an%2520attention-enhanced%2520detection%2520framework%2520that%2520introduces%250Aa%2520lightweight%2520Selective-Perspective-Class%2520Integration%2520%2528SPCI%2529%2520module%2520to%2520improve%250Afeature%2520representation.%2520The%2520SPCI%2520module%2520integrates%2520three%2520components%253A%2520a%250ASelective%2520Stream%2520Gate%2520%2528SSG%2529%2520for%2520adaptive%2520regulation%2520of%2520global%2520feature%2520flow%252C%2520a%250APerspective%2520Fusion%2520Module%2520%2528PFM%2529%2520for%2520context-aware%2520multi-scale%2520integration%252C%2520and%250Aa%2520Class%2520Discrimination%2520Module%2520%2528CDM%2529%2520to%2520enhance%2520inter-class%2520separability.%2520We%250Aembed%2520two%2520SPCI%2520blocks%2520into%2520the%2520P3%2520and%2520P5%2520stages%2520of%2520the%2520YOLOv8%2520backbone%252C%250Aenabling%2520effective%2520refinement%2520while%2520preserving%2520compatibility%2520with%2520the%2520original%250Aneck%2520and%2520head.%2520Experiments%2520on%2520the%2520NWPU%2520VHR-10%2520dataset%2520demonstrate%2520that%250AYOLO-SPCI%2520achieves%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO-SPCI%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20via%0A%20%20Selective-Perspective-Class%20Integration&entry.906535625=Xinyuan%20Wang%20and%20Lian%20Peng%20and%20Xiangcheng%20Li%20and%20Yilin%20He%20and%20KinTak%20U&entry.1292438233=%20%20Object%20detection%20in%20remote%20sensing%20imagery%20remains%20a%20challenging%20task%20due%20to%0Aextreme%20scale%20variation%2C%20dense%20object%20distributions%2C%20and%20cluttered%20backgrounds.%0AWhile%20recent%20detectors%20such%20as%20YOLOv8%20have%20shown%20promising%20results%2C%20their%0Abackbone%20architectures%20lack%20explicit%20mechanisms%20to%20guide%20multi-scale%20feature%0Arefinement%2C%20limiting%20performance%20on%20high-resolution%20aerial%20data.%20In%20this%20work%2C%0Awe%20propose%20YOLO-SPCI%2C%20an%20attention-enhanced%20detection%20framework%20that%20introduces%0Aa%20lightweight%20Selective-Perspective-Class%20Integration%20%28SPCI%29%20module%20to%20improve%0Afeature%20representation.%20The%20SPCI%20module%20integrates%20three%20components%3A%20a%0ASelective%20Stream%20Gate%20%28SSG%29%20for%20adaptive%20regulation%20of%20global%20feature%20flow%2C%20a%0APerspective%20Fusion%20Module%20%28PFM%29%20for%20context-aware%20multi-scale%20integration%2C%20and%0Aa%20Class%20Discrimination%20Module%20%28CDM%29%20to%20enhance%20inter-class%20separability.%20We%0Aembed%20two%20SPCI%20blocks%20into%20the%20P3%20and%20P5%20stages%20of%20the%20YOLOv8%20backbone%2C%0Aenabling%20effective%20refinement%20while%20preserving%20compatibility%20with%20the%20original%0Aneck%20and%20head.%20Experiments%20on%20the%20NWPU%20VHR-10%20dataset%20demonstrate%20that%0AYOLO-SPCI%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21370v1&entry.124074799=Read"},
{"title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models", "author": "Kele Shao and Keda Tao and Can Qin and Haoxuan You and Yang Sui and Huan Wang", "abstract": "  Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.\n", "link": "http://arxiv.org/abs/2505.21334v1", "date": "2025-05-27", "relevancy": 2.6119, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5322}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoliTom%3A%20Holistic%20Token%20Merging%20for%20Fast%20Video%20Large%20Language%20Models&body=Title%3A%20HoliTom%3A%20Holistic%20Token%20Merging%20for%20Fast%20Video%20Large%20Language%20Models%0AAuthor%3A%20Kele%20Shao%20and%20Keda%20Tao%20and%20Can%20Qin%20and%20Haoxuan%20You%20and%20Yang%20Sui%20and%20Huan%20Wang%0AAbstract%3A%20%20%20Video%20large%20language%20models%20%28video%20LLMs%29%20excel%20at%20video%20comprehension%20but%0Aface%20significant%20computational%20inefficiency%20due%20to%20redundant%20video%20tokens.%0AExisting%20token%20pruning%20methods%20offer%20solutions.%20However%2C%20approaches%20operating%0Awithin%20the%20LLM%20%28inner-LLM%20pruning%29%2C%20such%20as%20FastV%2C%20incur%20intrinsic%0Acomputational%20overhead%20in%20shallow%20layers.%20In%20contrast%2C%20methods%20performing%20token%0Apruning%20before%20the%20LLM%20%28outer-LLM%20pruning%29%20primarily%20address%20spatial%20redundancy%0Awithin%20individual%20frames%20or%20limited%20temporal%20windows%2C%20neglecting%20the%20crucial%0Aglobal%20temporal%20dynamics%20and%20correlations%20across%20longer%20video%20sequences.%20This%0Aleads%20to%20sub-optimal%20spatio-temporal%20reduction%20and%20does%20not%20leverage%20video%0Acompressibility%20fully.%20Crucially%2C%20the%20synergistic%20potential%20and%20mutual%0Ainfluence%20of%20combining%20these%20strategies%20remain%20unexplored.%20To%20further%20reduce%0Aredundancy%2C%20we%20introduce%20HoliTom%2C%20a%20novel%20training-free%20holistic%20token%20merging%0Aframework.%20HoliTom%20employs%20outer-LLM%20pruning%20through%20global%20redundancy-aware%0Atemporal%20segmentation%2C%20followed%20by%20spatial-temporal%20merging%20to%20reduce%20visual%0Atokens%20by%20over%2090%25%2C%20significantly%20alleviating%20the%20LLM%27s%20computational%20burden.%0AComplementing%20this%2C%20we%20introduce%20a%20robust%20inner-LLM%20token%20similarity-based%0Amerging%20approach%2C%20designed%20for%20superior%20performance%20and%20compatibility%20with%0Aouter-LLM%20pruning.%20Evaluations%20demonstrate%20our%20method%27s%20promising%0Aefficiency-performance%20trade-off%20on%20LLaVA-OneVision-7B%2C%20reducing%20computational%0Acosts%20to%206.9%25%20of%20FLOPs%20while%20maintaining%2099.1%25%20of%20the%20original%20performance.%0AFurthermore%2C%20we%20achieve%20a%202.28x%20reduction%20in%20Time-To-First-Token%20%28TTFT%29%20and%20a%0A1.32x%20acceleration%20in%20decoding%20throughput%2C%20highlighting%20the%20practical%20benefits%0Aof%20our%20integrated%20pruning%20approach%20for%20efficient%20video%20LLMs%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoliTom%253A%2520Holistic%2520Token%2520Merging%2520for%2520Fast%2520Video%2520Large%2520Language%2520Models%26entry.906535625%3DKele%2520Shao%2520and%2520Keda%2520Tao%2520and%2520Can%2520Qin%2520and%2520Haoxuan%2520You%2520and%2520Yang%2520Sui%2520and%2520Huan%2520Wang%26entry.1292438233%3D%2520%2520Video%2520large%2520language%2520models%2520%2528video%2520LLMs%2529%2520excel%2520at%2520video%2520comprehension%2520but%250Aface%2520significant%2520computational%2520inefficiency%2520due%2520to%2520redundant%2520video%2520tokens.%250AExisting%2520token%2520pruning%2520methods%2520offer%2520solutions.%2520However%252C%2520approaches%2520operating%250Awithin%2520the%2520LLM%2520%2528inner-LLM%2520pruning%2529%252C%2520such%2520as%2520FastV%252C%2520incur%2520intrinsic%250Acomputational%2520overhead%2520in%2520shallow%2520layers.%2520In%2520contrast%252C%2520methods%2520performing%2520token%250Apruning%2520before%2520the%2520LLM%2520%2528outer-LLM%2520pruning%2529%2520primarily%2520address%2520spatial%2520redundancy%250Awithin%2520individual%2520frames%2520or%2520limited%2520temporal%2520windows%252C%2520neglecting%2520the%2520crucial%250Aglobal%2520temporal%2520dynamics%2520and%2520correlations%2520across%2520longer%2520video%2520sequences.%2520This%250Aleads%2520to%2520sub-optimal%2520spatio-temporal%2520reduction%2520and%2520does%2520not%2520leverage%2520video%250Acompressibility%2520fully.%2520Crucially%252C%2520the%2520synergistic%2520potential%2520and%2520mutual%250Ainfluence%2520of%2520combining%2520these%2520strategies%2520remain%2520unexplored.%2520To%2520further%2520reduce%250Aredundancy%252C%2520we%2520introduce%2520HoliTom%252C%2520a%2520novel%2520training-free%2520holistic%2520token%2520merging%250Aframework.%2520HoliTom%2520employs%2520outer-LLM%2520pruning%2520through%2520global%2520redundancy-aware%250Atemporal%2520segmentation%252C%2520followed%2520by%2520spatial-temporal%2520merging%2520to%2520reduce%2520visual%250Atokens%2520by%2520over%252090%2525%252C%2520significantly%2520alleviating%2520the%2520LLM%2527s%2520computational%2520burden.%250AComplementing%2520this%252C%2520we%2520introduce%2520a%2520robust%2520inner-LLM%2520token%2520similarity-based%250Amerging%2520approach%252C%2520designed%2520for%2520superior%2520performance%2520and%2520compatibility%2520with%250Aouter-LLM%2520pruning.%2520Evaluations%2520demonstrate%2520our%2520method%2527s%2520promising%250Aefficiency-performance%2520trade-off%2520on%2520LLaVA-OneVision-7B%252C%2520reducing%2520computational%250Acosts%2520to%25206.9%2525%2520of%2520FLOPs%2520while%2520maintaining%252099.1%2525%2520of%2520the%2520original%2520performance.%250AFurthermore%252C%2520we%2520achieve%2520a%25202.28x%2520reduction%2520in%2520Time-To-First-Token%2520%2528TTFT%2529%2520and%2520a%250A1.32x%2520acceleration%2520in%2520decoding%2520throughput%252C%2520highlighting%2520the%2520practical%2520benefits%250Aof%2520our%2520integrated%2520pruning%2520approach%2520for%2520efficient%2520video%2520LLMs%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoliTom%3A%20Holistic%20Token%20Merging%20for%20Fast%20Video%20Large%20Language%20Models&entry.906535625=Kele%20Shao%20and%20Keda%20Tao%20and%20Can%20Qin%20and%20Haoxuan%20You%20and%20Yang%20Sui%20and%20Huan%20Wang&entry.1292438233=%20%20Video%20large%20language%20models%20%28video%20LLMs%29%20excel%20at%20video%20comprehension%20but%0Aface%20significant%20computational%20inefficiency%20due%20to%20redundant%20video%20tokens.%0AExisting%20token%20pruning%20methods%20offer%20solutions.%20However%2C%20approaches%20operating%0Awithin%20the%20LLM%20%28inner-LLM%20pruning%29%2C%20such%20as%20FastV%2C%20incur%20intrinsic%0Acomputational%20overhead%20in%20shallow%20layers.%20In%20contrast%2C%20methods%20performing%20token%0Apruning%20before%20the%20LLM%20%28outer-LLM%20pruning%29%20primarily%20address%20spatial%20redundancy%0Awithin%20individual%20frames%20or%20limited%20temporal%20windows%2C%20neglecting%20the%20crucial%0Aglobal%20temporal%20dynamics%20and%20correlations%20across%20longer%20video%20sequences.%20This%0Aleads%20to%20sub-optimal%20spatio-temporal%20reduction%20and%20does%20not%20leverage%20video%0Acompressibility%20fully.%20Crucially%2C%20the%20synergistic%20potential%20and%20mutual%0Ainfluence%20of%20combining%20these%20strategies%20remain%20unexplored.%20To%20further%20reduce%0Aredundancy%2C%20we%20introduce%20HoliTom%2C%20a%20novel%20training-free%20holistic%20token%20merging%0Aframework.%20HoliTom%20employs%20outer-LLM%20pruning%20through%20global%20redundancy-aware%0Atemporal%20segmentation%2C%20followed%20by%20spatial-temporal%20merging%20to%20reduce%20visual%0Atokens%20by%20over%2090%25%2C%20significantly%20alleviating%20the%20LLM%27s%20computational%20burden.%0AComplementing%20this%2C%20we%20introduce%20a%20robust%20inner-LLM%20token%20similarity-based%0Amerging%20approach%2C%20designed%20for%20superior%20performance%20and%20compatibility%20with%0Aouter-LLM%20pruning.%20Evaluations%20demonstrate%20our%20method%27s%20promising%0Aefficiency-performance%20trade-off%20on%20LLaVA-OneVision-7B%2C%20reducing%20computational%0Acosts%20to%206.9%25%20of%20FLOPs%20while%20maintaining%2099.1%25%20of%20the%20original%20performance.%0AFurthermore%2C%20we%20achieve%20a%202.28x%20reduction%20in%20Time-To-First-Token%20%28TTFT%29%20and%20a%0A1.32x%20acceleration%20in%20decoding%20throughput%2C%20highlighting%20the%20practical%20benefits%0Aof%20our%20integrated%20pruning%20approach%20for%20efficient%20video%20LLMs%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21334v1&entry.124074799=Read"},
{"title": "Copresheaf Topological Neural Networks: A Generalized Deep Learning\n  Framework", "author": "Mustafa Hajij and Lennart Bastian and Sarah Osentoski and Hardik Kabaria and John L. Davenport and Sheik Dawood and Balaji Cherukuri and Joseph G. Kocheemoolayil and Nastaran Shahmansouri and Adrian Lew and Theodore Papamarkou and Tolga Birdal", "abstract": "  We introduce copresheaf topological neural networks (CTNNs), a powerful and\nunifying framework that encapsulates a wide spectrum of deep learning\narchitectures, designed to operate on structured data: including images, point\nclouds, graphs, meshes, and topological manifolds. While deep learning has\nprofoundly impacted domains ranging from digital assistants to autonomous\nsystems, the principled design of neural architectures tailored to specific\ntasks and data types remains one of the field's most persistent open\nchallenges. CTNNs address this gap by grounding model design in the language of\ncopresheaves, a concept from algebraic topology that generalizes and subsumes\nmost practical deep learning models in use today. This abstract yet\nconstructive formulation yields a rich design space from which theoretically\nsound and practically effective solutions can be derived to tackle core\nchallenges in representation learning: long-range dependencies, oversmoothing,\nheterophily, and non-Euclidean domains. Our empirical results on structured\ndata benchmarks demonstrate that CTNNs consistently outperform conventional\nbaselines, particularly in tasks requiring hierarchical or localized\nsensitivity. These results underscore CTNNs as a principled, multi-scale\nfoundation for the next generation of deep learning architectures.\n", "link": "http://arxiv.org/abs/2505.21251v1", "date": "2025-05-27", "relevancy": 2.6092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5202}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Copresheaf%20Topological%20Neural%20Networks%3A%20A%20Generalized%20Deep%20Learning%0A%20%20Framework&body=Title%3A%20Copresheaf%20Topological%20Neural%20Networks%3A%20A%20Generalized%20Deep%20Learning%0A%20%20Framework%0AAuthor%3A%20Mustafa%20Hajij%20and%20Lennart%20Bastian%20and%20Sarah%20Osentoski%20and%20Hardik%20Kabaria%20and%20John%20L.%20Davenport%20and%20Sheik%20Dawood%20and%20Balaji%20Cherukuri%20and%20Joseph%20G.%20Kocheemoolayil%20and%20Nastaran%20Shahmansouri%20and%20Adrian%20Lew%20and%20Theodore%20Papamarkou%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20We%20introduce%20copresheaf%20topological%20neural%20networks%20%28CTNNs%29%2C%20a%20powerful%20and%0Aunifying%20framework%20that%20encapsulates%20a%20wide%20spectrum%20of%20deep%20learning%0Aarchitectures%2C%20designed%20to%20operate%20on%20structured%20data%3A%20including%20images%2C%20point%0Aclouds%2C%20graphs%2C%20meshes%2C%20and%20topological%20manifolds.%20While%20deep%20learning%20has%0Aprofoundly%20impacted%20domains%20ranging%20from%20digital%20assistants%20to%20autonomous%0Asystems%2C%20the%20principled%20design%20of%20neural%20architectures%20tailored%20to%20specific%0Atasks%20and%20data%20types%20remains%20one%20of%20the%20field%27s%20most%20persistent%20open%0Achallenges.%20CTNNs%20address%20this%20gap%20by%20grounding%20model%20design%20in%20the%20language%20of%0Acopresheaves%2C%20a%20concept%20from%20algebraic%20topology%20that%20generalizes%20and%20subsumes%0Amost%20practical%20deep%20learning%20models%20in%20use%20today.%20This%20abstract%20yet%0Aconstructive%20formulation%20yields%20a%20rich%20design%20space%20from%20which%20theoretically%0Asound%20and%20practically%20effective%20solutions%20can%20be%20derived%20to%20tackle%20core%0Achallenges%20in%20representation%20learning%3A%20long-range%20dependencies%2C%20oversmoothing%2C%0Aheterophily%2C%20and%20non-Euclidean%20domains.%20Our%20empirical%20results%20on%20structured%0Adata%20benchmarks%20demonstrate%20that%20CTNNs%20consistently%20outperform%20conventional%0Abaselines%2C%20particularly%20in%20tasks%20requiring%20hierarchical%20or%20localized%0Asensitivity.%20These%20results%20underscore%20CTNNs%20as%20a%20principled%2C%20multi-scale%0Afoundation%20for%20the%20next%20generation%20of%20deep%20learning%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCopresheaf%2520Topological%2520Neural%2520Networks%253A%2520A%2520Generalized%2520Deep%2520Learning%250A%2520%2520Framework%26entry.906535625%3DMustafa%2520Hajij%2520and%2520Lennart%2520Bastian%2520and%2520Sarah%2520Osentoski%2520and%2520Hardik%2520Kabaria%2520and%2520John%2520L.%2520Davenport%2520and%2520Sheik%2520Dawood%2520and%2520Balaji%2520Cherukuri%2520and%2520Joseph%2520G.%2520Kocheemoolayil%2520and%2520Nastaran%2520Shahmansouri%2520and%2520Adrian%2520Lew%2520and%2520Theodore%2520Papamarkou%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520We%2520introduce%2520copresheaf%2520topological%2520neural%2520networks%2520%2528CTNNs%2529%252C%2520a%2520powerful%2520and%250Aunifying%2520framework%2520that%2520encapsulates%2520a%2520wide%2520spectrum%2520of%2520deep%2520learning%250Aarchitectures%252C%2520designed%2520to%2520operate%2520on%2520structured%2520data%253A%2520including%2520images%252C%2520point%250Aclouds%252C%2520graphs%252C%2520meshes%252C%2520and%2520topological%2520manifolds.%2520While%2520deep%2520learning%2520has%250Aprofoundly%2520impacted%2520domains%2520ranging%2520from%2520digital%2520assistants%2520to%2520autonomous%250Asystems%252C%2520the%2520principled%2520design%2520of%2520neural%2520architectures%2520tailored%2520to%2520specific%250Atasks%2520and%2520data%2520types%2520remains%2520one%2520of%2520the%2520field%2527s%2520most%2520persistent%2520open%250Achallenges.%2520CTNNs%2520address%2520this%2520gap%2520by%2520grounding%2520model%2520design%2520in%2520the%2520language%2520of%250Acopresheaves%252C%2520a%2520concept%2520from%2520algebraic%2520topology%2520that%2520generalizes%2520and%2520subsumes%250Amost%2520practical%2520deep%2520learning%2520models%2520in%2520use%2520today.%2520This%2520abstract%2520yet%250Aconstructive%2520formulation%2520yields%2520a%2520rich%2520design%2520space%2520from%2520which%2520theoretically%250Asound%2520and%2520practically%2520effective%2520solutions%2520can%2520be%2520derived%2520to%2520tackle%2520core%250Achallenges%2520in%2520representation%2520learning%253A%2520long-range%2520dependencies%252C%2520oversmoothing%252C%250Aheterophily%252C%2520and%2520non-Euclidean%2520domains.%2520Our%2520empirical%2520results%2520on%2520structured%250Adata%2520benchmarks%2520demonstrate%2520that%2520CTNNs%2520consistently%2520outperform%2520conventional%250Abaselines%252C%2520particularly%2520in%2520tasks%2520requiring%2520hierarchical%2520or%2520localized%250Asensitivity.%2520These%2520results%2520underscore%2520CTNNs%2520as%2520a%2520principled%252C%2520multi-scale%250Afoundation%2520for%2520the%2520next%2520generation%2520of%2520deep%2520learning%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Copresheaf%20Topological%20Neural%20Networks%3A%20A%20Generalized%20Deep%20Learning%0A%20%20Framework&entry.906535625=Mustafa%20Hajij%20and%20Lennart%20Bastian%20and%20Sarah%20Osentoski%20and%20Hardik%20Kabaria%20and%20John%20L.%20Davenport%20and%20Sheik%20Dawood%20and%20Balaji%20Cherukuri%20and%20Joseph%20G.%20Kocheemoolayil%20and%20Nastaran%20Shahmansouri%20and%20Adrian%20Lew%20and%20Theodore%20Papamarkou%20and%20Tolga%20Birdal&entry.1292438233=%20%20We%20introduce%20copresheaf%20topological%20neural%20networks%20%28CTNNs%29%2C%20a%20powerful%20and%0Aunifying%20framework%20that%20encapsulates%20a%20wide%20spectrum%20of%20deep%20learning%0Aarchitectures%2C%20designed%20to%20operate%20on%20structured%20data%3A%20including%20images%2C%20point%0Aclouds%2C%20graphs%2C%20meshes%2C%20and%20topological%20manifolds.%20While%20deep%20learning%20has%0Aprofoundly%20impacted%20domains%20ranging%20from%20digital%20assistants%20to%20autonomous%0Asystems%2C%20the%20principled%20design%20of%20neural%20architectures%20tailored%20to%20specific%0Atasks%20and%20data%20types%20remains%20one%20of%20the%20field%27s%20most%20persistent%20open%0Achallenges.%20CTNNs%20address%20this%20gap%20by%20grounding%20model%20design%20in%20the%20language%20of%0Acopresheaves%2C%20a%20concept%20from%20algebraic%20topology%20that%20generalizes%20and%20subsumes%0Amost%20practical%20deep%20learning%20models%20in%20use%20today.%20This%20abstract%20yet%0Aconstructive%20formulation%20yields%20a%20rich%20design%20space%20from%20which%20theoretically%0Asound%20and%20practically%20effective%20solutions%20can%20be%20derived%20to%20tackle%20core%0Achallenges%20in%20representation%20learning%3A%20long-range%20dependencies%2C%20oversmoothing%2C%0Aheterophily%2C%20and%20non-Euclidean%20domains.%20Our%20empirical%20results%20on%20structured%0Adata%20benchmarks%20demonstrate%20that%20CTNNs%20consistently%20outperform%20conventional%0Abaselines%2C%20particularly%20in%20tasks%20requiring%20hierarchical%20or%20localized%0Asensitivity.%20These%20results%20underscore%20CTNNs%20as%20a%20principled%2C%20multi-scale%0Afoundation%20for%20the%20next%20generation%20of%20deep%20learning%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21251v1&entry.124074799=Read"},
{"title": "PUSSM: Point Cloud Upsampling as Implicit Statistical Shape Model", "author": "Tongxu Zhang and Bei Wang", "abstract": "  This paper proposes a framework for high-fidelity reconstruction of pelvic\nstructures by integrating medical image segmentation and point cloud\nupsampling. By point cloud upsampling to learn shape priors from MedShapePelvic\nwithout requiring landmarks or PCA, our method functions as an implicit\nstatistical shape model. Evaluations on Pelvic1k show significant improvements\nin surface quality and anatomical accuracy. This approach is generalizable and\napplicable to other skeletal regions.\n", "link": "http://arxiv.org/abs/2501.16716v3", "date": "2025-05-27", "relevancy": 2.6037, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5334}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5289}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PUSSM%3A%20Point%20Cloud%20Upsampling%20as%20Implicit%20Statistical%20Shape%20Model&body=Title%3A%20PUSSM%3A%20Point%20Cloud%20Upsampling%20as%20Implicit%20Statistical%20Shape%20Model%0AAuthor%3A%20Tongxu%20Zhang%20and%20Bei%20Wang%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20framework%20for%20high-fidelity%20reconstruction%20of%20pelvic%0Astructures%20by%20integrating%20medical%20image%20segmentation%20and%20point%20cloud%0Aupsampling.%20By%20point%20cloud%20upsampling%20to%20learn%20shape%20priors%20from%20MedShapePelvic%0Awithout%20requiring%20landmarks%20or%20PCA%2C%20our%20method%20functions%20as%20an%20implicit%0Astatistical%20shape%20model.%20Evaluations%20on%20Pelvic1k%20show%20significant%20improvements%0Ain%20surface%20quality%20and%20anatomical%20accuracy.%20This%20approach%20is%20generalizable%20and%0Aapplicable%20to%20other%20skeletal%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16716v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPUSSM%253A%2520Point%2520Cloud%2520Upsampling%2520as%2520Implicit%2520Statistical%2520Shape%2520Model%26entry.906535625%3DTongxu%2520Zhang%2520and%2520Bei%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520framework%2520for%2520high-fidelity%2520reconstruction%2520of%2520pelvic%250Astructures%2520by%2520integrating%2520medical%2520image%2520segmentation%2520and%2520point%2520cloud%250Aupsampling.%2520By%2520point%2520cloud%2520upsampling%2520to%2520learn%2520shape%2520priors%2520from%2520MedShapePelvic%250Awithout%2520requiring%2520landmarks%2520or%2520PCA%252C%2520our%2520method%2520functions%2520as%2520an%2520implicit%250Astatistical%2520shape%2520model.%2520Evaluations%2520on%2520Pelvic1k%2520show%2520significant%2520improvements%250Ain%2520surface%2520quality%2520and%2520anatomical%2520accuracy.%2520This%2520approach%2520is%2520generalizable%2520and%250Aapplicable%2520to%2520other%2520skeletal%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16716v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PUSSM%3A%20Point%20Cloud%20Upsampling%20as%20Implicit%20Statistical%20Shape%20Model&entry.906535625=Tongxu%20Zhang%20and%20Bei%20Wang&entry.1292438233=%20%20This%20paper%20proposes%20a%20framework%20for%20high-fidelity%20reconstruction%20of%20pelvic%0Astructures%20by%20integrating%20medical%20image%20segmentation%20and%20point%20cloud%0Aupsampling.%20By%20point%20cloud%20upsampling%20to%20learn%20shape%20priors%20from%20MedShapePelvic%0Awithout%20requiring%20landmarks%20or%20PCA%2C%20our%20method%20functions%20as%20an%20implicit%0Astatistical%20shape%20model.%20Evaluations%20on%20Pelvic1k%20show%20significant%20improvements%0Ain%20surface%20quality%20and%20anatomical%20accuracy.%20This%20approach%20is%20generalizable%20and%0Aapplicable%20to%20other%20skeletal%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16716v3&entry.124074799=Read"},
{"title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images\n  from Clothed Individuals", "author": "Davide Lobba and Fulvio Sanguigni and Bin Ren and Marcella Cornia and Rita Cucchiara and Nicu Sebe", "abstract": "  While virtual try-on (VTON) systems aim to render a garment onto a target\nperson image, this paper tackles the novel task of virtual try-off (VTOFF),\nwhich addresses the inverse problem: generating standardized product images of\ngarments from real-world photos of clothed individuals. Unlike VTON, which must\nresolve diverse pose and style variations, VTOFF benefits from a consistent and\nwell-defined output format -- typically a flat, lay-down-style representation\nof the garment -- making it a promising tool for data generation and dataset\nenhancement. However, existing VTOFF approaches face two major limitations: (i)\ndifficulty in disentangling garment features from occlusions and complex poses,\noften leading to visual artifacts, and (ii) restricted applicability to\nsingle-category garments (e.g., upper-body clothes only), limiting\ngeneralization. To address these challenges, we present Text-Enhanced\nMUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a\ndual DiT-based backbone with a modified multimodal attention mechanism for\nrobust garment feature extraction. Our architecture is designed to receive\ngarment information from multiple modalities like images, text, and masks to\nwork in a multi-category setting. Finally, we propose an additional alignment\nmodule to further refine the generated visual details. Experiments on VITON-HD\nand Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the\nVTOFF task, significantly improving both visual quality and fidelity to the\ntarget garments.\n", "link": "http://arxiv.org/abs/2505.21062v1", "date": "2025-05-27", "relevancy": 2.5989, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6761}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6525}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Virtual%20Try-On%3A%20Generating%20Multi-Category%20Product-Style%20Images%0A%20%20from%20Clothed%20Individuals&body=Title%3A%20Inverse%20Virtual%20Try-On%3A%20Generating%20Multi-Category%20Product-Style%20Images%0A%20%20from%20Clothed%20Individuals%0AAuthor%3A%20Davide%20Lobba%20and%20Fulvio%20Sanguigni%20and%20Bin%20Ren%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20While%20virtual%20try-on%20%28VTON%29%20systems%20aim%20to%20render%20a%20garment%20onto%20a%20target%0Aperson%20image%2C%20this%20paper%20tackles%20the%20novel%20task%20of%20virtual%20try-off%20%28VTOFF%29%2C%0Awhich%20addresses%20the%20inverse%20problem%3A%20generating%20standardized%20product%20images%20of%0Agarments%20from%20real-world%20photos%20of%20clothed%20individuals.%20Unlike%20VTON%2C%20which%20must%0Aresolve%20diverse%20pose%20and%20style%20variations%2C%20VTOFF%20benefits%20from%20a%20consistent%20and%0Awell-defined%20output%20format%20--%20typically%20a%20flat%2C%20lay-down-style%20representation%0Aof%20the%20garment%20--%20making%20it%20a%20promising%20tool%20for%20data%20generation%20and%20dataset%0Aenhancement.%20However%2C%20existing%20VTOFF%20approaches%20face%20two%20major%20limitations%3A%20%28i%29%0Adifficulty%20in%20disentangling%20garment%20features%20from%20occlusions%20and%20complex%20poses%2C%0Aoften%20leading%20to%20visual%20artifacts%2C%20and%20%28ii%29%20restricted%20applicability%20to%0Asingle-category%20garments%20%28e.g.%2C%20upper-body%20clothes%20only%29%2C%20limiting%0Ageneralization.%20To%20address%20these%20challenges%2C%20we%20present%20Text-Enhanced%0AMUlti-category%20Virtual%20Try-Off%20%28TEMU-VTOFF%29%2C%20a%20novel%20architecture%20featuring%20a%0Adual%20DiT-based%20backbone%20with%20a%20modified%20multimodal%20attention%20mechanism%20for%0Arobust%20garment%20feature%20extraction.%20Our%20architecture%20is%20designed%20to%20receive%0Agarment%20information%20from%20multiple%20modalities%20like%20images%2C%20text%2C%20and%20masks%20to%0Awork%20in%20a%20multi-category%20setting.%20Finally%2C%20we%20propose%20an%20additional%20alignment%0Amodule%20to%20further%20refine%20the%20generated%20visual%20details.%20Experiments%20on%20VITON-HD%0Aand%20Dress%20Code%20datasets%20show%20that%20TEMU-VTOFF%20sets%20a%20new%20state-of-the-art%20on%20the%0AVTOFF%20task%2C%20significantly%20improving%20both%20visual%20quality%20and%20fidelity%20to%20the%0Atarget%20garments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Virtual%2520Try-On%253A%2520Generating%2520Multi-Category%2520Product-Style%2520Images%250A%2520%2520from%2520Clothed%2520Individuals%26entry.906535625%3DDavide%2520Lobba%2520and%2520Fulvio%2520Sanguigni%2520and%2520Bin%2520Ren%2520and%2520Marcella%2520Cornia%2520and%2520Rita%2520Cucchiara%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520While%2520virtual%2520try-on%2520%2528VTON%2529%2520systems%2520aim%2520to%2520render%2520a%2520garment%2520onto%2520a%2520target%250Aperson%2520image%252C%2520this%2520paper%2520tackles%2520the%2520novel%2520task%2520of%2520virtual%2520try-off%2520%2528VTOFF%2529%252C%250Awhich%2520addresses%2520the%2520inverse%2520problem%253A%2520generating%2520standardized%2520product%2520images%2520of%250Agarments%2520from%2520real-world%2520photos%2520of%2520clothed%2520individuals.%2520Unlike%2520VTON%252C%2520which%2520must%250Aresolve%2520diverse%2520pose%2520and%2520style%2520variations%252C%2520VTOFF%2520benefits%2520from%2520a%2520consistent%2520and%250Awell-defined%2520output%2520format%2520--%2520typically%2520a%2520flat%252C%2520lay-down-style%2520representation%250Aof%2520the%2520garment%2520--%2520making%2520it%2520a%2520promising%2520tool%2520for%2520data%2520generation%2520and%2520dataset%250Aenhancement.%2520However%252C%2520existing%2520VTOFF%2520approaches%2520face%2520two%2520major%2520limitations%253A%2520%2528i%2529%250Adifficulty%2520in%2520disentangling%2520garment%2520features%2520from%2520occlusions%2520and%2520complex%2520poses%252C%250Aoften%2520leading%2520to%2520visual%2520artifacts%252C%2520and%2520%2528ii%2529%2520restricted%2520applicability%2520to%250Asingle-category%2520garments%2520%2528e.g.%252C%2520upper-body%2520clothes%2520only%2529%252C%2520limiting%250Ageneralization.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520Text-Enhanced%250AMUlti-category%2520Virtual%2520Try-Off%2520%2528TEMU-VTOFF%2529%252C%2520a%2520novel%2520architecture%2520featuring%2520a%250Adual%2520DiT-based%2520backbone%2520with%2520a%2520modified%2520multimodal%2520attention%2520mechanism%2520for%250Arobust%2520garment%2520feature%2520extraction.%2520Our%2520architecture%2520is%2520designed%2520to%2520receive%250Agarment%2520information%2520from%2520multiple%2520modalities%2520like%2520images%252C%2520text%252C%2520and%2520masks%2520to%250Awork%2520in%2520a%2520multi-category%2520setting.%2520Finally%252C%2520we%2520propose%2520an%2520additional%2520alignment%250Amodule%2520to%2520further%2520refine%2520the%2520generated%2520visual%2520details.%2520Experiments%2520on%2520VITON-HD%250Aand%2520Dress%2520Code%2520datasets%2520show%2520that%2520TEMU-VTOFF%2520sets%2520a%2520new%2520state-of-the-art%2520on%2520the%250AVTOFF%2520task%252C%2520significantly%2520improving%2520both%2520visual%2520quality%2520and%2520fidelity%2520to%2520the%250Atarget%2520garments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Virtual%20Try-On%3A%20Generating%20Multi-Category%20Product-Style%20Images%0A%20%20from%20Clothed%20Individuals&entry.906535625=Davide%20Lobba%20and%20Fulvio%20Sanguigni%20and%20Bin%20Ren%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara%20and%20Nicu%20Sebe&entry.1292438233=%20%20While%20virtual%20try-on%20%28VTON%29%20systems%20aim%20to%20render%20a%20garment%20onto%20a%20target%0Aperson%20image%2C%20this%20paper%20tackles%20the%20novel%20task%20of%20virtual%20try-off%20%28VTOFF%29%2C%0Awhich%20addresses%20the%20inverse%20problem%3A%20generating%20standardized%20product%20images%20of%0Agarments%20from%20real-world%20photos%20of%20clothed%20individuals.%20Unlike%20VTON%2C%20which%20must%0Aresolve%20diverse%20pose%20and%20style%20variations%2C%20VTOFF%20benefits%20from%20a%20consistent%20and%0Awell-defined%20output%20format%20--%20typically%20a%20flat%2C%20lay-down-style%20representation%0Aof%20the%20garment%20--%20making%20it%20a%20promising%20tool%20for%20data%20generation%20and%20dataset%0Aenhancement.%20However%2C%20existing%20VTOFF%20approaches%20face%20two%20major%20limitations%3A%20%28i%29%0Adifficulty%20in%20disentangling%20garment%20features%20from%20occlusions%20and%20complex%20poses%2C%0Aoften%20leading%20to%20visual%20artifacts%2C%20and%20%28ii%29%20restricted%20applicability%20to%0Asingle-category%20garments%20%28e.g.%2C%20upper-body%20clothes%20only%29%2C%20limiting%0Ageneralization.%20To%20address%20these%20challenges%2C%20we%20present%20Text-Enhanced%0AMUlti-category%20Virtual%20Try-Off%20%28TEMU-VTOFF%29%2C%20a%20novel%20architecture%20featuring%20a%0Adual%20DiT-based%20backbone%20with%20a%20modified%20multimodal%20attention%20mechanism%20for%0Arobust%20garment%20feature%20extraction.%20Our%20architecture%20is%20designed%20to%20receive%0Agarment%20information%20from%20multiple%20modalities%20like%20images%2C%20text%2C%20and%20masks%20to%0Awork%20in%20a%20multi-category%20setting.%20Finally%2C%20we%20propose%20an%20additional%20alignment%0Amodule%20to%20further%20refine%20the%20generated%20visual%20details.%20Experiments%20on%20VITON-HD%0Aand%20Dress%20Code%20datasets%20show%20that%20TEMU-VTOFF%20sets%20a%20new%20state-of-the-art%20on%20the%0AVTOFF%20task%2C%20significantly%20improving%20both%20visual%20quality%20and%20fidelity%20to%20the%0Atarget%20garments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21062v1&entry.124074799=Read"},
{"title": "Guide your favorite protein sequence generative model", "author": "Junhao Xiong and Hunter Nisonoff and Maria Lukarska and Ishan Gaur and Luke M. Oltrogge and David F. Savage and Jennifer Listgarten", "abstract": "  Generative machine learning models on sequences are transforming protein\nengineering. However, no principled framework exists for conditioning these\nmodels on auxiliary information, such as experimental data, in a plug-and-play\nmanner. Herein, we present ProteinGuide -- a principled and general method for\nconditioning -- by unifying a broad class of protein generative models under a\nsingle framework. We demonstrate the applicability of ProteinGuide by guiding\ntwo protein generative models, ProteinMPNN and ESM3, to generate amino acid and\nstructure token sequences, conditioned on several user-specified properties\nsuch as enhanced stability, enzyme classes, and CATH-labeled folds. We also\nused ProteinGuide with inverse folding models and our own experimental assay to\ndesign adenine base editor sequences for high activity.\n", "link": "http://arxiv.org/abs/2505.04823v2", "date": "2025-05-27", "relevancy": 2.5919, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.544}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5144}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guide%20your%20favorite%20protein%20sequence%20generative%20model&body=Title%3A%20Guide%20your%20favorite%20protein%20sequence%20generative%20model%0AAuthor%3A%20Junhao%20Xiong%20and%20Hunter%20Nisonoff%20and%20Maria%20Lukarska%20and%20Ishan%20Gaur%20and%20Luke%20M.%20Oltrogge%20and%20David%20F.%20Savage%20and%20Jennifer%20Listgarten%0AAbstract%3A%20%20%20Generative%20machine%20learning%20models%20on%20sequences%20are%20transforming%20protein%0Aengineering.%20However%2C%20no%20principled%20framework%20exists%20for%20conditioning%20these%0Amodels%20on%20auxiliary%20information%2C%20such%20as%20experimental%20data%2C%20in%20a%20plug-and-play%0Amanner.%20Herein%2C%20we%20present%20ProteinGuide%20--%20a%20principled%20and%20general%20method%20for%0Aconditioning%20--%20by%20unifying%20a%20broad%20class%20of%20protein%20generative%20models%20under%20a%0Asingle%20framework.%20We%20demonstrate%20the%20applicability%20of%20ProteinGuide%20by%20guiding%0Atwo%20protein%20generative%20models%2C%20ProteinMPNN%20and%20ESM3%2C%20to%20generate%20amino%20acid%20and%0Astructure%20token%20sequences%2C%20conditioned%20on%20several%20user-specified%20properties%0Asuch%20as%20enhanced%20stability%2C%20enzyme%20classes%2C%20and%20CATH-labeled%20folds.%20We%20also%0Aused%20ProteinGuide%20with%20inverse%20folding%20models%20and%20our%20own%20experimental%20assay%20to%0Adesign%20adenine%20base%20editor%20sequences%20for%20high%20activity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuide%2520your%2520favorite%2520protein%2520sequence%2520generative%2520model%26entry.906535625%3DJunhao%2520Xiong%2520and%2520Hunter%2520Nisonoff%2520and%2520Maria%2520Lukarska%2520and%2520Ishan%2520Gaur%2520and%2520Luke%2520M.%2520Oltrogge%2520and%2520David%2520F.%2520Savage%2520and%2520Jennifer%2520Listgarten%26entry.1292438233%3D%2520%2520Generative%2520machine%2520learning%2520models%2520on%2520sequences%2520are%2520transforming%2520protein%250Aengineering.%2520However%252C%2520no%2520principled%2520framework%2520exists%2520for%2520conditioning%2520these%250Amodels%2520on%2520auxiliary%2520information%252C%2520such%2520as%2520experimental%2520data%252C%2520in%2520a%2520plug-and-play%250Amanner.%2520Herein%252C%2520we%2520present%2520ProteinGuide%2520--%2520a%2520principled%2520and%2520general%2520method%2520for%250Aconditioning%2520--%2520by%2520unifying%2520a%2520broad%2520class%2520of%2520protein%2520generative%2520models%2520under%2520a%250Asingle%2520framework.%2520We%2520demonstrate%2520the%2520applicability%2520of%2520ProteinGuide%2520by%2520guiding%250Atwo%2520protein%2520generative%2520models%252C%2520ProteinMPNN%2520and%2520ESM3%252C%2520to%2520generate%2520amino%2520acid%2520and%250Astructure%2520token%2520sequences%252C%2520conditioned%2520on%2520several%2520user-specified%2520properties%250Asuch%2520as%2520enhanced%2520stability%252C%2520enzyme%2520classes%252C%2520and%2520CATH-labeled%2520folds.%2520We%2520also%250Aused%2520ProteinGuide%2520with%2520inverse%2520folding%2520models%2520and%2520our%2520own%2520experimental%2520assay%2520to%250Adesign%2520adenine%2520base%2520editor%2520sequences%2520for%2520high%2520activity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guide%20your%20favorite%20protein%20sequence%20generative%20model&entry.906535625=Junhao%20Xiong%20and%20Hunter%20Nisonoff%20and%20Maria%20Lukarska%20and%20Ishan%20Gaur%20and%20Luke%20M.%20Oltrogge%20and%20David%20F.%20Savage%20and%20Jennifer%20Listgarten&entry.1292438233=%20%20Generative%20machine%20learning%20models%20on%20sequences%20are%20transforming%20protein%0Aengineering.%20However%2C%20no%20principled%20framework%20exists%20for%20conditioning%20these%0Amodels%20on%20auxiliary%20information%2C%20such%20as%20experimental%20data%2C%20in%20a%20plug-and-play%0Amanner.%20Herein%2C%20we%20present%20ProteinGuide%20--%20a%20principled%20and%20general%20method%20for%0Aconditioning%20--%20by%20unifying%20a%20broad%20class%20of%20protein%20generative%20models%20under%20a%0Asingle%20framework.%20We%20demonstrate%20the%20applicability%20of%20ProteinGuide%20by%20guiding%0Atwo%20protein%20generative%20models%2C%20ProteinMPNN%20and%20ESM3%2C%20to%20generate%20amino%20acid%20and%0Astructure%20token%20sequences%2C%20conditioned%20on%20several%20user-specified%20properties%0Asuch%20as%20enhanced%20stability%2C%20enzyme%20classes%2C%20and%20CATH-labeled%20folds.%20We%20also%0Aused%20ProteinGuide%20with%20inverse%20folding%20models%20and%20our%20own%20experimental%20assay%20to%0Adesign%20adenine%20base%20editor%20sequences%20for%20high%20activity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04823v2&entry.124074799=Read"},
{"title": "SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded\n  Scenarios", "author": "Simon Malzard and Nitish Mital and Richard Walters and Victoria Nockles and Raghuveer Rao and Celso M. De Melo", "abstract": "  Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS).\n", "link": "http://arxiv.org/abs/2505.18048v2", "date": "2025-05-27", "relevancy": 2.5746, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHARDeg%3A%20A%20Benchmark%20for%20Skeletal%20Human%20Action%20Recognition%20in%20Degraded%0A%20%20Scenarios&body=Title%3A%20SHARDeg%3A%20A%20Benchmark%20for%20Skeletal%20Human%20Action%20Recognition%20in%20Degraded%0A%20%20Scenarios%0AAuthor%3A%20Simon%20Malzard%20and%20Nitish%20Mital%20and%20Richard%20Walters%20and%20Victoria%20Nockles%20and%20Raghuveer%20Rao%20and%20Celso%20M.%20De%20Melo%0AAbstract%3A%20%20%20Computer%20vision%20%28CV%29%20models%20for%20detection%2C%20prediction%20or%20classification%20tasks%0Aoperate%20on%20video%20data-streams%20that%20are%20often%20degraded%20in%20the%20real%20world%2C%20due%20to%0Adeployment%20in%20real-time%20or%20on%20resource-constrained%20hardware.%20It%20is%20therefore%0Acritical%20that%20these%20models%20are%20robust%20to%20degraded%20data%2C%20but%20state%20of%20the%20art%0A%28SoTA%29%20models%20are%20often%20insufficiently%20assessed%20with%20these%20real-world%0Aconstraints%20in%20mind.%20This%20is%20exemplified%20by%20Skeletal%20Human%20Action%20Recognition%0A%28SHAR%29%2C%20which%20is%20critical%20in%20many%20CV%20pipelines%20operating%20in%20real-time%20and%20at%0Athe%20edge%2C%20but%20robustness%20to%20degraded%20data%20has%20previously%20only%20been%20shallowly%0Aand%20inconsistently%20assessed.%20Here%20we%20address%20this%20issue%20for%20SHAR%20by%20providing%0Aan%20important%20first%20data%20degradation%20benchmark%20on%20the%20most%20detailed%20and%20largest%0A3D%20open%20dataset%2C%20NTU-RGB%2BD-120%2C%20and%20assess%20the%20robustness%20of%20five%20leading%20SHAR%0Amodels%20to%20three%20forms%20of%20degradation%20that%20represent%20real-world%20issues.%20We%0Ademonstrate%20the%20need%20for%20this%20benchmark%20by%20showing%20that%20the%20form%20of%0Adegradation%2C%20which%20has%20not%20previously%20been%20considered%2C%20has%20a%20large%20impact%20on%0Amodel%20accuracy%3B%20at%20the%20same%20effective%20frame%20rate%2C%20model%20accuracy%20can%20vary%20by%0A%3E40%25%20depending%20on%20degradation%20type.%20We%20also%20identify%20that%20temporal%20regularity%0Aof%20frames%20in%20degraded%20SHAR%20data%20is%20likely%20a%20major%20driver%20of%20differences%20in%0Amodel%20performance%2C%20and%20harness%20this%20to%20improve%20performance%20of%20existing%20models%0Aby%20up%20to%20%3E40%25%2C%20through%20employing%20a%20simple%20mitigation%20approach%20based%20on%0Ainterpolation.%20Finally%2C%20we%20highlight%20how%20our%20benchmark%20has%20helped%20identify%20an%0Aimportant%20degradation-resistant%20SHAR%20model%20based%20in%20Rough%20Path%20Theory%3B%20the%0ALogSigRNN%20SHAR%20model%20outperforms%20the%20SoTA%20DeGCN%20model%20in%20five%20out%20of%20six%20cases%0Aat%20low%20frame%20rates%20by%20an%20average%20accuracy%20of%206%25%2C%20despite%20trailing%20the%20SoTA%0Amodel%20by%2011-12%25%20on%20un-degraded%20data%20at%20high%20frame%20rates%20%2830%20FPS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHARDeg%253A%2520A%2520Benchmark%2520for%2520Skeletal%2520Human%2520Action%2520Recognition%2520in%2520Degraded%250A%2520%2520Scenarios%26entry.906535625%3DSimon%2520Malzard%2520and%2520Nitish%2520Mital%2520and%2520Richard%2520Walters%2520and%2520Victoria%2520Nockles%2520and%2520Raghuveer%2520Rao%2520and%2520Celso%2520M.%2520De%2520Melo%26entry.1292438233%3D%2520%2520Computer%2520vision%2520%2528CV%2529%2520models%2520for%2520detection%252C%2520prediction%2520or%2520classification%2520tasks%250Aoperate%2520on%2520video%2520data-streams%2520that%2520are%2520often%2520degraded%2520in%2520the%2520real%2520world%252C%2520due%2520to%250Adeployment%2520in%2520real-time%2520or%2520on%2520resource-constrained%2520hardware.%2520It%2520is%2520therefore%250Acritical%2520that%2520these%2520models%2520are%2520robust%2520to%2520degraded%2520data%252C%2520but%2520state%2520of%2520the%2520art%250A%2528SoTA%2529%2520models%2520are%2520often%2520insufficiently%2520assessed%2520with%2520these%2520real-world%250Aconstraints%2520in%2520mind.%2520This%2520is%2520exemplified%2520by%2520Skeletal%2520Human%2520Action%2520Recognition%250A%2528SHAR%2529%252C%2520which%2520is%2520critical%2520in%2520many%2520CV%2520pipelines%2520operating%2520in%2520real-time%2520and%2520at%250Athe%2520edge%252C%2520but%2520robustness%2520to%2520degraded%2520data%2520has%2520previously%2520only%2520been%2520shallowly%250Aand%2520inconsistently%2520assessed.%2520Here%2520we%2520address%2520this%2520issue%2520for%2520SHAR%2520by%2520providing%250Aan%2520important%2520first%2520data%2520degradation%2520benchmark%2520on%2520the%2520most%2520detailed%2520and%2520largest%250A3D%2520open%2520dataset%252C%2520NTU-RGB%252BD-120%252C%2520and%2520assess%2520the%2520robustness%2520of%2520five%2520leading%2520SHAR%250Amodels%2520to%2520three%2520forms%2520of%2520degradation%2520that%2520represent%2520real-world%2520issues.%2520We%250Ademonstrate%2520the%2520need%2520for%2520this%2520benchmark%2520by%2520showing%2520that%2520the%2520form%2520of%250Adegradation%252C%2520which%2520has%2520not%2520previously%2520been%2520considered%252C%2520has%2520a%2520large%2520impact%2520on%250Amodel%2520accuracy%253B%2520at%2520the%2520same%2520effective%2520frame%2520rate%252C%2520model%2520accuracy%2520can%2520vary%2520by%250A%253E40%2525%2520depending%2520on%2520degradation%2520type.%2520We%2520also%2520identify%2520that%2520temporal%2520regularity%250Aof%2520frames%2520in%2520degraded%2520SHAR%2520data%2520is%2520likely%2520a%2520major%2520driver%2520of%2520differences%2520in%250Amodel%2520performance%252C%2520and%2520harness%2520this%2520to%2520improve%2520performance%2520of%2520existing%2520models%250Aby%2520up%2520to%2520%253E40%2525%252C%2520through%2520employing%2520a%2520simple%2520mitigation%2520approach%2520based%2520on%250Ainterpolation.%2520Finally%252C%2520we%2520highlight%2520how%2520our%2520benchmark%2520has%2520helped%2520identify%2520an%250Aimportant%2520degradation-resistant%2520SHAR%2520model%2520based%2520in%2520Rough%2520Path%2520Theory%253B%2520the%250ALogSigRNN%2520SHAR%2520model%2520outperforms%2520the%2520SoTA%2520DeGCN%2520model%2520in%2520five%2520out%2520of%2520six%2520cases%250Aat%2520low%2520frame%2520rates%2520by%2520an%2520average%2520accuracy%2520of%25206%2525%252C%2520despite%2520trailing%2520the%2520SoTA%250Amodel%2520by%252011-12%2525%2520on%2520un-degraded%2520data%2520at%2520high%2520frame%2520rates%2520%252830%2520FPS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHARDeg%3A%20A%20Benchmark%20for%20Skeletal%20Human%20Action%20Recognition%20in%20Degraded%0A%20%20Scenarios&entry.906535625=Simon%20Malzard%20and%20Nitish%20Mital%20and%20Richard%20Walters%20and%20Victoria%20Nockles%20and%20Raghuveer%20Rao%20and%20Celso%20M.%20De%20Melo&entry.1292438233=%20%20Computer%20vision%20%28CV%29%20models%20for%20detection%2C%20prediction%20or%20classification%20tasks%0Aoperate%20on%20video%20data-streams%20that%20are%20often%20degraded%20in%20the%20real%20world%2C%20due%20to%0Adeployment%20in%20real-time%20or%20on%20resource-constrained%20hardware.%20It%20is%20therefore%0Acritical%20that%20these%20models%20are%20robust%20to%20degraded%20data%2C%20but%20state%20of%20the%20art%0A%28SoTA%29%20models%20are%20often%20insufficiently%20assessed%20with%20these%20real-world%0Aconstraints%20in%20mind.%20This%20is%20exemplified%20by%20Skeletal%20Human%20Action%20Recognition%0A%28SHAR%29%2C%20which%20is%20critical%20in%20many%20CV%20pipelines%20operating%20in%20real-time%20and%20at%0Athe%20edge%2C%20but%20robustness%20to%20degraded%20data%20has%20previously%20only%20been%20shallowly%0Aand%20inconsistently%20assessed.%20Here%20we%20address%20this%20issue%20for%20SHAR%20by%20providing%0Aan%20important%20first%20data%20degradation%20benchmark%20on%20the%20most%20detailed%20and%20largest%0A3D%20open%20dataset%2C%20NTU-RGB%2BD-120%2C%20and%20assess%20the%20robustness%20of%20five%20leading%20SHAR%0Amodels%20to%20three%20forms%20of%20degradation%20that%20represent%20real-world%20issues.%20We%0Ademonstrate%20the%20need%20for%20this%20benchmark%20by%20showing%20that%20the%20form%20of%0Adegradation%2C%20which%20has%20not%20previously%20been%20considered%2C%20has%20a%20large%20impact%20on%0Amodel%20accuracy%3B%20at%20the%20same%20effective%20frame%20rate%2C%20model%20accuracy%20can%20vary%20by%0A%3E40%25%20depending%20on%20degradation%20type.%20We%20also%20identify%20that%20temporal%20regularity%0Aof%20frames%20in%20degraded%20SHAR%20data%20is%20likely%20a%20major%20driver%20of%20differences%20in%0Amodel%20performance%2C%20and%20harness%20this%20to%20improve%20performance%20of%20existing%20models%0Aby%20up%20to%20%3E40%25%2C%20through%20employing%20a%20simple%20mitigation%20approach%20based%20on%0Ainterpolation.%20Finally%2C%20we%20highlight%20how%20our%20benchmark%20has%20helped%20identify%20an%0Aimportant%20degradation-resistant%20SHAR%20model%20based%20in%20Rough%20Path%20Theory%3B%20the%0ALogSigRNN%20SHAR%20model%20outperforms%20the%20SoTA%20DeGCN%20model%20in%20five%20out%20of%20six%20cases%0Aat%20low%20frame%20rates%20by%20an%20average%20accuracy%20of%206%25%2C%20despite%20trailing%20the%20SoTA%0Amodel%20by%2011-12%25%20on%20un-degraded%20data%20at%20high%20frame%20rates%20%2830%20FPS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18048v2&entry.124074799=Read"},
{"title": "IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned\n  Motion Diffusion Model", "author": "Yang Zhao and Yan Zhang and Xubo Yang", "abstract": "  Existing human motion generation methods with trajectory and pose inputs\noperate global processing on both modalities, leading to suboptimal outputs. In\nthis paper, we propose IKMo, an image-keyframed motion generation method based\non the diffusion model with trajectory and pose being decoupled. The trajectory\nand pose inputs go through a two-stage conditioning framework. In the first\nstage, the dedicated optimization module is applied to refine inputs. In the\nsecond stage, trajectory and pose are encoded via a Trajectory Encoder and a\nPose Encoder in parallel. Then, motion with high spatial and semantic fidelity\nis guided by a motion ControlNet, which processes the fused trajectory and pose\ndata. Experiment results based on HumanML3D and KIT-ML datasets demonstrate\nthat the proposed method outperforms state-of-the-art on all metrics under\ntrajectory-keyframe constraints. In addition, MLLM-based agents are implemented\nto pre-process model inputs. Given texts and keyframe images from users, the\nagents extract motion descriptions, keyframe poses, and trajectories as the\noptimized inputs into the motion generation model. We conducts a user study\nwith 10 participants. The experiment results prove that the MLLM-based agents\npre-processing makes generated motion more in line with users' expectation. We\nbelieve that the proposed method improves both the fidelity and controllability\nof motion generation by the diffusion model.\n", "link": "http://arxiv.org/abs/2505.21146v1", "date": "2025-05-27", "relevancy": 2.5617, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6941}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6155}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IKMo%3A%20Image-Keyframed%20Motion%20Generation%20with%20Trajectory-Pose%20Conditioned%0A%20%20Motion%20Diffusion%20Model&body=Title%3A%20IKMo%3A%20Image-Keyframed%20Motion%20Generation%20with%20Trajectory-Pose%20Conditioned%0A%20%20Motion%20Diffusion%20Model%0AAuthor%3A%20Yang%20Zhao%20and%20Yan%20Zhang%20and%20Xubo%20Yang%0AAbstract%3A%20%20%20Existing%20human%20motion%20generation%20methods%20with%20trajectory%20and%20pose%20inputs%0Aoperate%20global%20processing%20on%20both%20modalities%2C%20leading%20to%20suboptimal%20outputs.%20In%0Athis%20paper%2C%20we%20propose%20IKMo%2C%20an%20image-keyframed%20motion%20generation%20method%20based%0Aon%20the%20diffusion%20model%20with%20trajectory%20and%20pose%20being%20decoupled.%20The%20trajectory%0Aand%20pose%20inputs%20go%20through%20a%20two-stage%20conditioning%20framework.%20In%20the%20first%0Astage%2C%20the%20dedicated%20optimization%20module%20is%20applied%20to%20refine%20inputs.%20In%20the%0Asecond%20stage%2C%20trajectory%20and%20pose%20are%20encoded%20via%20a%20Trajectory%20Encoder%20and%20a%0APose%20Encoder%20in%20parallel.%20Then%2C%20motion%20with%20high%20spatial%20and%20semantic%20fidelity%0Ais%20guided%20by%20a%20motion%20ControlNet%2C%20which%20processes%20the%20fused%20trajectory%20and%20pose%0Adata.%20Experiment%20results%20based%20on%20HumanML3D%20and%20KIT-ML%20datasets%20demonstrate%0Athat%20the%20proposed%20method%20outperforms%20state-of-the-art%20on%20all%20metrics%20under%0Atrajectory-keyframe%20constraints.%20In%20addition%2C%20MLLM-based%20agents%20are%20implemented%0Ato%20pre-process%20model%20inputs.%20Given%20texts%20and%20keyframe%20images%20from%20users%2C%20the%0Aagents%20extract%20motion%20descriptions%2C%20keyframe%20poses%2C%20and%20trajectories%20as%20the%0Aoptimized%20inputs%20into%20the%20motion%20generation%20model.%20We%20conducts%20a%20user%20study%0Awith%2010%20participants.%20The%20experiment%20results%20prove%20that%20the%20MLLM-based%20agents%0Apre-processing%20makes%20generated%20motion%20more%20in%20line%20with%20users%27%20expectation.%20We%0Abelieve%20that%20the%20proposed%20method%20improves%20both%20the%20fidelity%20and%20controllability%0Aof%20motion%20generation%20by%20the%20diffusion%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIKMo%253A%2520Image-Keyframed%2520Motion%2520Generation%2520with%2520Trajectory-Pose%2520Conditioned%250A%2520%2520Motion%2520Diffusion%2520Model%26entry.906535625%3DYang%2520Zhao%2520and%2520Yan%2520Zhang%2520and%2520Xubo%2520Yang%26entry.1292438233%3D%2520%2520Existing%2520human%2520motion%2520generation%2520methods%2520with%2520trajectory%2520and%2520pose%2520inputs%250Aoperate%2520global%2520processing%2520on%2520both%2520modalities%252C%2520leading%2520to%2520suboptimal%2520outputs.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520IKMo%252C%2520an%2520image-keyframed%2520motion%2520generation%2520method%2520based%250Aon%2520the%2520diffusion%2520model%2520with%2520trajectory%2520and%2520pose%2520being%2520decoupled.%2520The%2520trajectory%250Aand%2520pose%2520inputs%2520go%2520through%2520a%2520two-stage%2520conditioning%2520framework.%2520In%2520the%2520first%250Astage%252C%2520the%2520dedicated%2520optimization%2520module%2520is%2520applied%2520to%2520refine%2520inputs.%2520In%2520the%250Asecond%2520stage%252C%2520trajectory%2520and%2520pose%2520are%2520encoded%2520via%2520a%2520Trajectory%2520Encoder%2520and%2520a%250APose%2520Encoder%2520in%2520parallel.%2520Then%252C%2520motion%2520with%2520high%2520spatial%2520and%2520semantic%2520fidelity%250Ais%2520guided%2520by%2520a%2520motion%2520ControlNet%252C%2520which%2520processes%2520the%2520fused%2520trajectory%2520and%2520pose%250Adata.%2520Experiment%2520results%2520based%2520on%2520HumanML3D%2520and%2520KIT-ML%2520datasets%2520demonstrate%250Athat%2520the%2520proposed%2520method%2520outperforms%2520state-of-the-art%2520on%2520all%2520metrics%2520under%250Atrajectory-keyframe%2520constraints.%2520In%2520addition%252C%2520MLLM-based%2520agents%2520are%2520implemented%250Ato%2520pre-process%2520model%2520inputs.%2520Given%2520texts%2520and%2520keyframe%2520images%2520from%2520users%252C%2520the%250Aagents%2520extract%2520motion%2520descriptions%252C%2520keyframe%2520poses%252C%2520and%2520trajectories%2520as%2520the%250Aoptimized%2520inputs%2520into%2520the%2520motion%2520generation%2520model.%2520We%2520conducts%2520a%2520user%2520study%250Awith%252010%2520participants.%2520The%2520experiment%2520results%2520prove%2520that%2520the%2520MLLM-based%2520agents%250Apre-processing%2520makes%2520generated%2520motion%2520more%2520in%2520line%2520with%2520users%2527%2520expectation.%2520We%250Abelieve%2520that%2520the%2520proposed%2520method%2520improves%2520both%2520the%2520fidelity%2520and%2520controllability%250Aof%2520motion%2520generation%2520by%2520the%2520diffusion%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IKMo%3A%20Image-Keyframed%20Motion%20Generation%20with%20Trajectory-Pose%20Conditioned%0A%20%20Motion%20Diffusion%20Model&entry.906535625=Yang%20Zhao%20and%20Yan%20Zhang%20and%20Xubo%20Yang&entry.1292438233=%20%20Existing%20human%20motion%20generation%20methods%20with%20trajectory%20and%20pose%20inputs%0Aoperate%20global%20processing%20on%20both%20modalities%2C%20leading%20to%20suboptimal%20outputs.%20In%0Athis%20paper%2C%20we%20propose%20IKMo%2C%20an%20image-keyframed%20motion%20generation%20method%20based%0Aon%20the%20diffusion%20model%20with%20trajectory%20and%20pose%20being%20decoupled.%20The%20trajectory%0Aand%20pose%20inputs%20go%20through%20a%20two-stage%20conditioning%20framework.%20In%20the%20first%0Astage%2C%20the%20dedicated%20optimization%20module%20is%20applied%20to%20refine%20inputs.%20In%20the%0Asecond%20stage%2C%20trajectory%20and%20pose%20are%20encoded%20via%20a%20Trajectory%20Encoder%20and%20a%0APose%20Encoder%20in%20parallel.%20Then%2C%20motion%20with%20high%20spatial%20and%20semantic%20fidelity%0Ais%20guided%20by%20a%20motion%20ControlNet%2C%20which%20processes%20the%20fused%20trajectory%20and%20pose%0Adata.%20Experiment%20results%20based%20on%20HumanML3D%20and%20KIT-ML%20datasets%20demonstrate%0Athat%20the%20proposed%20method%20outperforms%20state-of-the-art%20on%20all%20metrics%20under%0Atrajectory-keyframe%20constraints.%20In%20addition%2C%20MLLM-based%20agents%20are%20implemented%0Ato%20pre-process%20model%20inputs.%20Given%20texts%20and%20keyframe%20images%20from%20users%2C%20the%0Aagents%20extract%20motion%20descriptions%2C%20keyframe%20poses%2C%20and%20trajectories%20as%20the%0Aoptimized%20inputs%20into%20the%20motion%20generation%20model.%20We%20conducts%20a%20user%20study%0Awith%2010%20participants.%20The%20experiment%20results%20prove%20that%20the%20MLLM-based%20agents%0Apre-processing%20makes%20generated%20motion%20more%20in%20line%20with%20users%27%20expectation.%20We%0Abelieve%20that%20the%20proposed%20method%20improves%20both%20the%20fidelity%20and%20controllability%0Aof%20motion%20generation%20by%20the%20diffusion%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21146v1&entry.124074799=Read"},
{"title": "Thinker: Learning to Think Fast and Slow", "author": "Stephen Chung and Wenyu Du and Jie Fu", "abstract": "  Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.\n", "link": "http://arxiv.org/abs/2505.21097v1", "date": "2025-05-27", "relevancy": 2.5567, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinker%3A%20Learning%20to%20Think%20Fast%20and%20Slow&body=Title%3A%20Thinker%3A%20Learning%20to%20Think%20Fast%20and%20Slow%0AAuthor%3A%20Stephen%20Chung%20and%20Wenyu%20Du%20and%20Jie%20Fu%0AAbstract%3A%20%20%20Recent%20studies%20show%20that%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%0A%28LLMs%29%20can%20be%20improved%20by%20applying%20Reinforcement%20Learning%20%28RL%29%20to%0Aquestion-answering%20%28QA%29%20tasks%20in%20areas%20such%20as%20math%20and%20coding.%20With%20a%20long%0Acontext%20length%2C%20LLMs%20may%20learn%20to%20perform%20search%2C%20as%20indicated%20by%20the%0Aself-correction%20behavior%20observed%20in%20DeepSeek%20R1.%20However%2C%20this%20search%20behavior%0Ais%20often%20imprecise%20and%20lacks%20confidence%2C%20resulting%20in%20long%2C%20redundant%20responses%0Aand%20highlighting%20deficiencies%20in%20intuition%20and%20verification.%20Inspired%20by%20the%0ADual%20Process%20Theory%20in%20psychology%2C%20we%20introduce%20a%20simple%20modification%20to%20the%20QA%0Atask%20that%20includes%20four%20stages%3A%20Fast%20Thinking%2C%20where%20the%20LLM%20must%20answer%20within%0Aa%20strict%20token%20budget%3B%20Verification%2C%20where%20the%20model%20evaluates%20its%20initial%0Aresponse%3B%20Slow%20Thinking%2C%20where%20it%20refines%20the%20initial%20response%20with%20more%0Adeliberation%3B%20and%20Summarization%2C%20where%20it%20distills%20the%20refinement%20from%20the%0Aprevious%20stage%20into%20precise%20steps.%20Our%20proposed%20task%20improves%20average%20accuracy%0Afrom%2024.9%25%20to%2027.9%25%20for%20Qwen2.5-1.5B%2C%20and%20from%2045.9%25%20to%2049.8%25%20for%0ADeepSeek-R1-Qwen-1.5B.%20Notably%2C%20for%20Qwen2.5-1.5B%2C%20the%20Fast%20Thinking%20mode%20alone%0Aachieves%2026.8%25%20accuracy%20using%20fewer%20than%201000%20tokens%2C%20demonstrating%20substantial%0Ainference%20efficiency%20gains.%20These%20findings%20suggest%20that%20intuition%20and%0Adeliberative%20reasoning%20are%20distinct%2C%20complementary%20systems%20benefiting%20from%0Atargeted%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinker%253A%2520Learning%2520to%2520Think%2520Fast%2520and%2520Slow%26entry.906535625%3DStephen%2520Chung%2520and%2520Wenyu%2520Du%2520and%2520Jie%2520Fu%26entry.1292438233%3D%2520%2520Recent%2520studies%2520show%2520that%2520the%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520can%2520be%2520improved%2520by%2520applying%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%250Aquestion-answering%2520%2528QA%2529%2520tasks%2520in%2520areas%2520such%2520as%2520math%2520and%2520coding.%2520With%2520a%2520long%250Acontext%2520length%252C%2520LLMs%2520may%2520learn%2520to%2520perform%2520search%252C%2520as%2520indicated%2520by%2520the%250Aself-correction%2520behavior%2520observed%2520in%2520DeepSeek%2520R1.%2520However%252C%2520this%2520search%2520behavior%250Ais%2520often%2520imprecise%2520and%2520lacks%2520confidence%252C%2520resulting%2520in%2520long%252C%2520redundant%2520responses%250Aand%2520highlighting%2520deficiencies%2520in%2520intuition%2520and%2520verification.%2520Inspired%2520by%2520the%250ADual%2520Process%2520Theory%2520in%2520psychology%252C%2520we%2520introduce%2520a%2520simple%2520modification%2520to%2520the%2520QA%250Atask%2520that%2520includes%2520four%2520stages%253A%2520Fast%2520Thinking%252C%2520where%2520the%2520LLM%2520must%2520answer%2520within%250Aa%2520strict%2520token%2520budget%253B%2520Verification%252C%2520where%2520the%2520model%2520evaluates%2520its%2520initial%250Aresponse%253B%2520Slow%2520Thinking%252C%2520where%2520it%2520refines%2520the%2520initial%2520response%2520with%2520more%250Adeliberation%253B%2520and%2520Summarization%252C%2520where%2520it%2520distills%2520the%2520refinement%2520from%2520the%250Aprevious%2520stage%2520into%2520precise%2520steps.%2520Our%2520proposed%2520task%2520improves%2520average%2520accuracy%250Afrom%252024.9%2525%2520to%252027.9%2525%2520for%2520Qwen2.5-1.5B%252C%2520and%2520from%252045.9%2525%2520to%252049.8%2525%2520for%250ADeepSeek-R1-Qwen-1.5B.%2520Notably%252C%2520for%2520Qwen2.5-1.5B%252C%2520the%2520Fast%2520Thinking%2520mode%2520alone%250Aachieves%252026.8%2525%2520accuracy%2520using%2520fewer%2520than%25201000%2520tokens%252C%2520demonstrating%2520substantial%250Ainference%2520efficiency%2520gains.%2520These%2520findings%2520suggest%2520that%2520intuition%2520and%250Adeliberative%2520reasoning%2520are%2520distinct%252C%2520complementary%2520systems%2520benefiting%2520from%250Atargeted%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinker%3A%20Learning%20to%20Think%20Fast%20and%20Slow&entry.906535625=Stephen%20Chung%20and%20Wenyu%20Du%20and%20Jie%20Fu&entry.1292438233=%20%20Recent%20studies%20show%20that%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%0A%28LLMs%29%20can%20be%20improved%20by%20applying%20Reinforcement%20Learning%20%28RL%29%20to%0Aquestion-answering%20%28QA%29%20tasks%20in%20areas%20such%20as%20math%20and%20coding.%20With%20a%20long%0Acontext%20length%2C%20LLMs%20may%20learn%20to%20perform%20search%2C%20as%20indicated%20by%20the%0Aself-correction%20behavior%20observed%20in%20DeepSeek%20R1.%20However%2C%20this%20search%20behavior%0Ais%20often%20imprecise%20and%20lacks%20confidence%2C%20resulting%20in%20long%2C%20redundant%20responses%0Aand%20highlighting%20deficiencies%20in%20intuition%20and%20verification.%20Inspired%20by%20the%0ADual%20Process%20Theory%20in%20psychology%2C%20we%20introduce%20a%20simple%20modification%20to%20the%20QA%0Atask%20that%20includes%20four%20stages%3A%20Fast%20Thinking%2C%20where%20the%20LLM%20must%20answer%20within%0Aa%20strict%20token%20budget%3B%20Verification%2C%20where%20the%20model%20evaluates%20its%20initial%0Aresponse%3B%20Slow%20Thinking%2C%20where%20it%20refines%20the%20initial%20response%20with%20more%0Adeliberation%3B%20and%20Summarization%2C%20where%20it%20distills%20the%20refinement%20from%20the%0Aprevious%20stage%20into%20precise%20steps.%20Our%20proposed%20task%20improves%20average%20accuracy%0Afrom%2024.9%25%20to%2027.9%25%20for%20Qwen2.5-1.5B%2C%20and%20from%2045.9%25%20to%2049.8%25%20for%0ADeepSeek-R1-Qwen-1.5B.%20Notably%2C%20for%20Qwen2.5-1.5B%2C%20the%20Fast%20Thinking%20mode%20alone%0Aachieves%2026.8%25%20accuracy%20using%20fewer%20than%201000%20tokens%2C%20demonstrating%20substantial%0Ainference%20efficiency%20gains.%20These%20findings%20suggest%20that%20intuition%20and%0Adeliberative%20reasoning%20are%20distinct%2C%20complementary%20systems%20benefiting%20from%0Atargeted%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21097v1&entry.124074799=Read"},
{"title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding", "author": "Ziyin Zhang and Hang Yu and Shijie Li and Peng Di and Jianguo Li and Rui Wang", "abstract": "  Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.\n", "link": "http://arxiv.org/abs/2409.04183v2", "date": "2025-05-27", "relevancy": 2.5563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding&body=Title%3A%20GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding%0AAuthor%3A%20Ziyin%20Zhang%20and%20Hang%20Yu%20and%20Shijie%20Li%20and%20Peng%20Di%20and%20Jianguo%20Li%20and%20Rui%20Wang%0AAbstract%3A%20%20%20Programming%20languages%20possess%20rich%20semantic%20information%20-%20such%20as%20data%20flow%20-%0Athat%20is%20represented%20by%20graphs%20and%20not%20available%20from%20the%20surface%20form%20of%20source%0Acode.%20Recent%20code%20language%20models%20have%20scaled%20to%20billions%20of%20parameters%2C%20but%0Amodel%20source%20code%20solely%20as%20text%20tokens%20while%20ignoring%20any%20other%20structural%0Ainformation.%20Conversely%2C%20models%20that%20do%20encode%20structural%20information%20of%20code%0Amake%20modifications%20to%20the%20Transformer%20architecture%2C%20limiting%20their%20scale%20and%0Acompatibility%20with%20pretrained%20LLMs.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%0Aworlds%20with%20GALLa%20-%20Graph%20Aligned%20Large%20Language%20Models.%20GALLa%20utilizes%20graph%0Aneural%20networks%20and%20cross-modal%20alignment%20technologies%20to%20inject%20the%20structural%0Ainformation%20of%20code%20into%20LLMs%20as%20an%20auxiliary%20task%20during%20finetuning.%20This%0Aframework%20is%20both%20model-agnostic%20and%20task-agnostic%2C%20as%20it%20can%20be%20applied%20to%20any%0Acode%20LLM%20for%20any%20code%20downstream%20task%2C%20and%20requires%20the%20structural%20graph%20data%0Aonly%20at%20training%20time%20from%20a%20corpus%20unrelated%20to%20the%20finetuning%20data%2C%20while%0Aincurring%20no%20cost%20at%20inference%20time%20over%20the%20baseline%20LLM.%20Experiments%20on%20five%0Acode%20tasks%20with%20seven%20different%20baseline%20LLMs%20ranging%20in%20size%20from%20350M%20to%2014B%0Avalidate%20the%20effectiveness%20of%20GALLa%2C%20demonstrating%20consistent%20improvement%20over%0Athe%20baseline%2C%20even%20for%20powerful%20models%20such%20as%20LLaMA3%20and%20Qwen2.5-Coder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGALLa%253A%2520Graph%2520Aligned%2520Large%2520Language%2520Models%2520for%2520Improved%2520Source%2520Code%250A%2520%2520Understanding%26entry.906535625%3DZiyin%2520Zhang%2520and%2520Hang%2520Yu%2520and%2520Shijie%2520Li%2520and%2520Peng%2520Di%2520and%2520Jianguo%2520Li%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520Programming%2520languages%2520possess%2520rich%2520semantic%2520information%2520-%2520such%2520as%2520data%2520flow%2520-%250Athat%2520is%2520represented%2520by%2520graphs%2520and%2520not%2520available%2520from%2520the%2520surface%2520form%2520of%2520source%250Acode.%2520Recent%2520code%2520language%2520models%2520have%2520scaled%2520to%2520billions%2520of%2520parameters%252C%2520but%250Amodel%2520source%2520code%2520solely%2520as%2520text%2520tokens%2520while%2520ignoring%2520any%2520other%2520structural%250Ainformation.%2520Conversely%252C%2520models%2520that%2520do%2520encode%2520structural%2520information%2520of%2520code%250Amake%2520modifications%2520to%2520the%2520Transformer%2520architecture%252C%2520limiting%2520their%2520scale%2520and%250Acompatibility%2520with%2520pretrained%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520take%2520the%2520best%2520of%2520both%250Aworlds%2520with%2520GALLa%2520-%2520Graph%2520Aligned%2520Large%2520Language%2520Models.%2520GALLa%2520utilizes%2520graph%250Aneural%2520networks%2520and%2520cross-modal%2520alignment%2520technologies%2520to%2520inject%2520the%2520structural%250Ainformation%2520of%2520code%2520into%2520LLMs%2520as%2520an%2520auxiliary%2520task%2520during%2520finetuning.%2520This%250Aframework%2520is%2520both%2520model-agnostic%2520and%2520task-agnostic%252C%2520as%2520it%2520can%2520be%2520applied%2520to%2520any%250Acode%2520LLM%2520for%2520any%2520code%2520downstream%2520task%252C%2520and%2520requires%2520the%2520structural%2520graph%2520data%250Aonly%2520at%2520training%2520time%2520from%2520a%2520corpus%2520unrelated%2520to%2520the%2520finetuning%2520data%252C%2520while%250Aincurring%2520no%2520cost%2520at%2520inference%2520time%2520over%2520the%2520baseline%2520LLM.%2520Experiments%2520on%2520five%250Acode%2520tasks%2520with%2520seven%2520different%2520baseline%2520LLMs%2520ranging%2520in%2520size%2520from%2520350M%2520to%252014B%250Avalidate%2520the%2520effectiveness%2520of%2520GALLa%252C%2520demonstrating%2520consistent%2520improvement%2520over%250Athe%2520baseline%252C%2520even%2520for%2520powerful%2520models%2520such%2520as%2520LLaMA3%2520and%2520Qwen2.5-Coder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GALLa%3A%20Graph%20Aligned%20Large%20Language%20Models%20for%20Improved%20Source%20Code%0A%20%20Understanding&entry.906535625=Ziyin%20Zhang%20and%20Hang%20Yu%20and%20Shijie%20Li%20and%20Peng%20Di%20and%20Jianguo%20Li%20and%20Rui%20Wang&entry.1292438233=%20%20Programming%20languages%20possess%20rich%20semantic%20information%20-%20such%20as%20data%20flow%20-%0Athat%20is%20represented%20by%20graphs%20and%20not%20available%20from%20the%20surface%20form%20of%20source%0Acode.%20Recent%20code%20language%20models%20have%20scaled%20to%20billions%20of%20parameters%2C%20but%0Amodel%20source%20code%20solely%20as%20text%20tokens%20while%20ignoring%20any%20other%20structural%0Ainformation.%20Conversely%2C%20models%20that%20do%20encode%20structural%20information%20of%20code%0Amake%20modifications%20to%20the%20Transformer%20architecture%2C%20limiting%20their%20scale%20and%0Acompatibility%20with%20pretrained%20LLMs.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%0Aworlds%20with%20GALLa%20-%20Graph%20Aligned%20Large%20Language%20Models.%20GALLa%20utilizes%20graph%0Aneural%20networks%20and%20cross-modal%20alignment%20technologies%20to%20inject%20the%20structural%0Ainformation%20of%20code%20into%20LLMs%20as%20an%20auxiliary%20task%20during%20finetuning.%20This%0Aframework%20is%20both%20model-agnostic%20and%20task-agnostic%2C%20as%20it%20can%20be%20applied%20to%20any%0Acode%20LLM%20for%20any%20code%20downstream%20task%2C%20and%20requires%20the%20structural%20graph%20data%0Aonly%20at%20training%20time%20from%20a%20corpus%20unrelated%20to%20the%20finetuning%20data%2C%20while%0Aincurring%20no%20cost%20at%20inference%20time%20over%20the%20baseline%20LLM.%20Experiments%20on%20five%0Acode%20tasks%20with%20seven%20different%20baseline%20LLMs%20ranging%20in%20size%20from%20350M%20to%2014B%0Avalidate%20the%20effectiveness%20of%20GALLa%2C%20demonstrating%20consistent%20improvement%20over%0Athe%20baseline%2C%20even%20for%20powerful%20models%20such%20as%20LLaMA3%20and%20Qwen2.5-Coder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04183v2&entry.124074799=Read"},
{"title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution\n  Generalization", "author": "Haoyu Zhang and Wentao Zhang and Hao Miao and Xinke Jiang and Yuchen Fang and Yifan Zhang", "abstract": "  Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful\ntool for modeling dynamic graph-structured data across diverse domains.\nHowever, they often fail to generalize in Spatio-Temporal Out-of-Distribution\n(STOOD) scenarios, where both temporal dynamics and spatial structures evolve\nbeyond the training distribution. To address this problem, we propose an\ninnovative Spatio-Temporal Retrieval-Augmented Pattern Learning\nframework,STRAP, which enhances model generalization by integrating\nretrieval-augmented learning into the STGNN continue learning pipeline. The\ncore of STRAP is a compact and expressive pattern library that stores\nrepresentative spatio-temporal patterns enriched with historical, structural,\nand semantic information, which is obtained and optimized during the training\nphase. During inference, STRAP retrieves relevant patterns from this library\nbased on similarity to the current input and injects them into the model via a\nplug-and-play prompting mechanism. This not only strengthens spatio-temporal\nrepresentations but also mitigates catastrophic forgetting. Moreover, STRAP\nintroduces a knowledge-balancing objective to harmonize new information with\nretrieved knowledge. Extensive experiments across multiple real-world streaming\ngraph datasets show that STRAP consistently outperforms state-of-the-art STGNN\nbaselines on STOOD tasks, demonstrating its robustness, adaptability, and\nstrong generalization capability without task-specific fine-tuning.\n", "link": "http://arxiv.org/abs/2505.19547v2", "date": "2025-05-27", "relevancy": 2.5461, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5205}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5079}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STRAP%3A%20Spatio-Temporal%20Pattern%20Retrieval%20for%20Out-of-Distribution%0A%20%20Generalization&body=Title%3A%20STRAP%3A%20Spatio-Temporal%20Pattern%20Retrieval%20for%20Out-of-Distribution%0A%20%20Generalization%0AAuthor%3A%20Haoyu%20Zhang%20and%20Wentao%20Zhang%20and%20Hao%20Miao%20and%20Xinke%20Jiang%20and%20Yuchen%20Fang%20and%20Yifan%20Zhang%0AAbstract%3A%20%20%20Spatio-Temporal%20Graph%20Neural%20Networks%20%28STGNNs%29%20have%20emerged%20as%20a%20powerful%0Atool%20for%20modeling%20dynamic%20graph-structured%20data%20across%20diverse%20domains.%0AHowever%2C%20they%20often%20fail%20to%20generalize%20in%20Spatio-Temporal%20Out-of-Distribution%0A%28STOOD%29%20scenarios%2C%20where%20both%20temporal%20dynamics%20and%20spatial%20structures%20evolve%0Abeyond%20the%20training%20distribution.%20To%20address%20this%20problem%2C%20we%20propose%20an%0Ainnovative%20Spatio-Temporal%20Retrieval-Augmented%20Pattern%20Learning%0Aframework%2CSTRAP%2C%20which%20enhances%20model%20generalization%20by%20integrating%0Aretrieval-augmented%20learning%20into%20the%20STGNN%20continue%20learning%20pipeline.%20The%0Acore%20of%20STRAP%20is%20a%20compact%20and%20expressive%20pattern%20library%20that%20stores%0Arepresentative%20spatio-temporal%20patterns%20enriched%20with%20historical%2C%20structural%2C%0Aand%20semantic%20information%2C%20which%20is%20obtained%20and%20optimized%20during%20the%20training%0Aphase.%20During%20inference%2C%20STRAP%20retrieves%20relevant%20patterns%20from%20this%20library%0Abased%20on%20similarity%20to%20the%20current%20input%20and%20injects%20them%20into%20the%20model%20via%20a%0Aplug-and-play%20prompting%20mechanism.%20This%20not%20only%20strengthens%20spatio-temporal%0Arepresentations%20but%20also%20mitigates%20catastrophic%20forgetting.%20Moreover%2C%20STRAP%0Aintroduces%20a%20knowledge-balancing%20objective%20to%20harmonize%20new%20information%20with%0Aretrieved%20knowledge.%20Extensive%20experiments%20across%20multiple%20real-world%20streaming%0Agraph%20datasets%20show%20that%20STRAP%20consistently%20outperforms%20state-of-the-art%20STGNN%0Abaselines%20on%20STOOD%20tasks%2C%20demonstrating%20its%20robustness%2C%20adaptability%2C%20and%0Astrong%20generalization%20capability%20without%20task-specific%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTRAP%253A%2520Spatio-Temporal%2520Pattern%2520Retrieval%2520for%2520Out-of-Distribution%250A%2520%2520Generalization%26entry.906535625%3DHaoyu%2520Zhang%2520and%2520Wentao%2520Zhang%2520and%2520Hao%2520Miao%2520and%2520Xinke%2520Jiang%2520and%2520Yuchen%2520Fang%2520and%2520Yifan%2520Zhang%26entry.1292438233%3D%2520%2520Spatio-Temporal%2520Graph%2520Neural%2520Networks%2520%2528STGNNs%2529%2520have%2520emerged%2520as%2520a%2520powerful%250Atool%2520for%2520modeling%2520dynamic%2520graph-structured%2520data%2520across%2520diverse%2520domains.%250AHowever%252C%2520they%2520often%2520fail%2520to%2520generalize%2520in%2520Spatio-Temporal%2520Out-of-Distribution%250A%2528STOOD%2529%2520scenarios%252C%2520where%2520both%2520temporal%2520dynamics%2520and%2520spatial%2520structures%2520evolve%250Abeyond%2520the%2520training%2520distribution.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520an%250Ainnovative%2520Spatio-Temporal%2520Retrieval-Augmented%2520Pattern%2520Learning%250Aframework%252CSTRAP%252C%2520which%2520enhances%2520model%2520generalization%2520by%2520integrating%250Aretrieval-augmented%2520learning%2520into%2520the%2520STGNN%2520continue%2520learning%2520pipeline.%2520The%250Acore%2520of%2520STRAP%2520is%2520a%2520compact%2520and%2520expressive%2520pattern%2520library%2520that%2520stores%250Arepresentative%2520spatio-temporal%2520patterns%2520enriched%2520with%2520historical%252C%2520structural%252C%250Aand%2520semantic%2520information%252C%2520which%2520is%2520obtained%2520and%2520optimized%2520during%2520the%2520training%250Aphase.%2520During%2520inference%252C%2520STRAP%2520retrieves%2520relevant%2520patterns%2520from%2520this%2520library%250Abased%2520on%2520similarity%2520to%2520the%2520current%2520input%2520and%2520injects%2520them%2520into%2520the%2520model%2520via%2520a%250Aplug-and-play%2520prompting%2520mechanism.%2520This%2520not%2520only%2520strengthens%2520spatio-temporal%250Arepresentations%2520but%2520also%2520mitigates%2520catastrophic%2520forgetting.%2520Moreover%252C%2520STRAP%250Aintroduces%2520a%2520knowledge-balancing%2520objective%2520to%2520harmonize%2520new%2520information%2520with%250Aretrieved%2520knowledge.%2520Extensive%2520experiments%2520across%2520multiple%2520real-world%2520streaming%250Agraph%2520datasets%2520show%2520that%2520STRAP%2520consistently%2520outperforms%2520state-of-the-art%2520STGNN%250Abaselines%2520on%2520STOOD%2520tasks%252C%2520demonstrating%2520its%2520robustness%252C%2520adaptability%252C%2520and%250Astrong%2520generalization%2520capability%2520without%2520task-specific%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STRAP%3A%20Spatio-Temporal%20Pattern%20Retrieval%20for%20Out-of-Distribution%0A%20%20Generalization&entry.906535625=Haoyu%20Zhang%20and%20Wentao%20Zhang%20and%20Hao%20Miao%20and%20Xinke%20Jiang%20and%20Yuchen%20Fang%20and%20Yifan%20Zhang&entry.1292438233=%20%20Spatio-Temporal%20Graph%20Neural%20Networks%20%28STGNNs%29%20have%20emerged%20as%20a%20powerful%0Atool%20for%20modeling%20dynamic%20graph-structured%20data%20across%20diverse%20domains.%0AHowever%2C%20they%20often%20fail%20to%20generalize%20in%20Spatio-Temporal%20Out-of-Distribution%0A%28STOOD%29%20scenarios%2C%20where%20both%20temporal%20dynamics%20and%20spatial%20structures%20evolve%0Abeyond%20the%20training%20distribution.%20To%20address%20this%20problem%2C%20we%20propose%20an%0Ainnovative%20Spatio-Temporal%20Retrieval-Augmented%20Pattern%20Learning%0Aframework%2CSTRAP%2C%20which%20enhances%20model%20generalization%20by%20integrating%0Aretrieval-augmented%20learning%20into%20the%20STGNN%20continue%20learning%20pipeline.%20The%0Acore%20of%20STRAP%20is%20a%20compact%20and%20expressive%20pattern%20library%20that%20stores%0Arepresentative%20spatio-temporal%20patterns%20enriched%20with%20historical%2C%20structural%2C%0Aand%20semantic%20information%2C%20which%20is%20obtained%20and%20optimized%20during%20the%20training%0Aphase.%20During%20inference%2C%20STRAP%20retrieves%20relevant%20patterns%20from%20this%20library%0Abased%20on%20similarity%20to%20the%20current%20input%20and%20injects%20them%20into%20the%20model%20via%20a%0Aplug-and-play%20prompting%20mechanism.%20This%20not%20only%20strengthens%20spatio-temporal%0Arepresentations%20but%20also%20mitigates%20catastrophic%20forgetting.%20Moreover%2C%20STRAP%0Aintroduces%20a%20knowledge-balancing%20objective%20to%20harmonize%20new%20information%20with%0Aretrieved%20knowledge.%20Extensive%20experiments%20across%20multiple%20real-world%20streaming%0Agraph%20datasets%20show%20that%20STRAP%20consistently%20outperforms%20state-of-the-art%20STGNN%0Abaselines%20on%20STOOD%20tasks%2C%20demonstrating%20its%20robustness%2C%20adaptability%2C%20and%0Astrong%20generalization%20capability%20without%20task-specific%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19547v2&entry.124074799=Read"},
{"title": "FlexiReg: Flexible Urban Region Representation Learning", "author": "Fengze Sun and Yanchuan Chang and Egemen Tanin and Shanika Karunasekera and Jianzhong Qi", "abstract": "  The increasing availability of urban data offers new opportunities for\nlearning region representations, which can be used as input to machine learning\nmodels for downstream tasks such as check-in or crime prediction. While\nexisting solutions have produced promising results, an issue is their fixed\nformation of regions and fixed input region features, which may not suit the\nneeds of different downstream tasks. To address this limitation, we propose a\nmodel named FlexiReg for urban region representation learning that is flexible\nwith both the formation of urban regions and the input region features.\nFlexiReg is based on a spatial grid partitioning over the spatial area of\ninterest. It learns representations for the grid cells, leveraging publicly\naccessible data, including POI, land use, satellite imagery, and street view\nimagery. We propose adaptive aggregation to fuse the cell representations and\nprompt learning techniques to tailor the representations towards different\ntasks, addressing the needs of varying formations of urban regions and\ndownstream tasks. Extensive experiments on five real-world datasets demonstrate\nthat FlexiReg outperforms state-of-the-art models by up to 202% in term of the\naccuracy of four diverse downstream tasks using the produced urban region\nrepresentations.\n", "link": "http://arxiv.org/abs/2503.09128v2", "date": "2025-05-27", "relevancy": 2.5432, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5164}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiReg%3A%20Flexible%20Urban%20Region%20Representation%20Learning&body=Title%3A%20FlexiReg%3A%20Flexible%20Urban%20Region%20Representation%20Learning%0AAuthor%3A%20Fengze%20Sun%20and%20Yanchuan%20Chang%20and%20Egemen%20Tanin%20and%20Shanika%20Karunasekera%20and%20Jianzhong%20Qi%0AAbstract%3A%20%20%20The%20increasing%20availability%20of%20urban%20data%20offers%20new%20opportunities%20for%0Alearning%20region%20representations%2C%20which%20can%20be%20used%20as%20input%20to%20machine%20learning%0Amodels%20for%20downstream%20tasks%20such%20as%20check-in%20or%20crime%20prediction.%20While%0Aexisting%20solutions%20have%20produced%20promising%20results%2C%20an%20issue%20is%20their%20fixed%0Aformation%20of%20regions%20and%20fixed%20input%20region%20features%2C%20which%20may%20not%20suit%20the%0Aneeds%20of%20different%20downstream%20tasks.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Amodel%20named%20FlexiReg%20for%20urban%20region%20representation%20learning%20that%20is%20flexible%0Awith%20both%20the%20formation%20of%20urban%20regions%20and%20the%20input%20region%20features.%0AFlexiReg%20is%20based%20on%20a%20spatial%20grid%20partitioning%20over%20the%20spatial%20area%20of%0Ainterest.%20It%20learns%20representations%20for%20the%20grid%20cells%2C%20leveraging%20publicly%0Aaccessible%20data%2C%20including%20POI%2C%20land%20use%2C%20satellite%20imagery%2C%20and%20street%20view%0Aimagery.%20We%20propose%20adaptive%20aggregation%20to%20fuse%20the%20cell%20representations%20and%0Aprompt%20learning%20techniques%20to%20tailor%20the%20representations%20towards%20different%0Atasks%2C%20addressing%20the%20needs%20of%20varying%20formations%20of%20urban%20regions%20and%0Adownstream%20tasks.%20Extensive%20experiments%20on%20five%20real-world%20datasets%20demonstrate%0Athat%20FlexiReg%20outperforms%20state-of-the-art%20models%20by%20up%20to%20202%25%20in%20term%20of%20the%0Aaccuracy%20of%20four%20diverse%20downstream%20tasks%20using%20the%20produced%20urban%20region%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiReg%253A%2520Flexible%2520Urban%2520Region%2520Representation%2520Learning%26entry.906535625%3DFengze%2520Sun%2520and%2520Yanchuan%2520Chang%2520and%2520Egemen%2520Tanin%2520and%2520Shanika%2520Karunasekera%2520and%2520Jianzhong%2520Qi%26entry.1292438233%3D%2520%2520The%2520increasing%2520availability%2520of%2520urban%2520data%2520offers%2520new%2520opportunities%2520for%250Alearning%2520region%2520representations%252C%2520which%2520can%2520be%2520used%2520as%2520input%2520to%2520machine%2520learning%250Amodels%2520for%2520downstream%2520tasks%2520such%2520as%2520check-in%2520or%2520crime%2520prediction.%2520While%250Aexisting%2520solutions%2520have%2520produced%2520promising%2520results%252C%2520an%2520issue%2520is%2520their%2520fixed%250Aformation%2520of%2520regions%2520and%2520fixed%2520input%2520region%2520features%252C%2520which%2520may%2520not%2520suit%2520the%250Aneeds%2520of%2520different%2520downstream%2520tasks.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%250Amodel%2520named%2520FlexiReg%2520for%2520urban%2520region%2520representation%2520learning%2520that%2520is%2520flexible%250Awith%2520both%2520the%2520formation%2520of%2520urban%2520regions%2520and%2520the%2520input%2520region%2520features.%250AFlexiReg%2520is%2520based%2520on%2520a%2520spatial%2520grid%2520partitioning%2520over%2520the%2520spatial%2520area%2520of%250Ainterest.%2520It%2520learns%2520representations%2520for%2520the%2520grid%2520cells%252C%2520leveraging%2520publicly%250Aaccessible%2520data%252C%2520including%2520POI%252C%2520land%2520use%252C%2520satellite%2520imagery%252C%2520and%2520street%2520view%250Aimagery.%2520We%2520propose%2520adaptive%2520aggregation%2520to%2520fuse%2520the%2520cell%2520representations%2520and%250Aprompt%2520learning%2520techniques%2520to%2520tailor%2520the%2520representations%2520towards%2520different%250Atasks%252C%2520addressing%2520the%2520needs%2520of%2520varying%2520formations%2520of%2520urban%2520regions%2520and%250Adownstream%2520tasks.%2520Extensive%2520experiments%2520on%2520five%2520real-world%2520datasets%2520demonstrate%250Athat%2520FlexiReg%2520outperforms%2520state-of-the-art%2520models%2520by%2520up%2520to%2520202%2525%2520in%2520term%2520of%2520the%250Aaccuracy%2520of%2520four%2520diverse%2520downstream%2520tasks%2520using%2520the%2520produced%2520urban%2520region%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiReg%3A%20Flexible%20Urban%20Region%20Representation%20Learning&entry.906535625=Fengze%20Sun%20and%20Yanchuan%20Chang%20and%20Egemen%20Tanin%20and%20Shanika%20Karunasekera%20and%20Jianzhong%20Qi&entry.1292438233=%20%20The%20increasing%20availability%20of%20urban%20data%20offers%20new%20opportunities%20for%0Alearning%20region%20representations%2C%20which%20can%20be%20used%20as%20input%20to%20machine%20learning%0Amodels%20for%20downstream%20tasks%20such%20as%20check-in%20or%20crime%20prediction.%20While%0Aexisting%20solutions%20have%20produced%20promising%20results%2C%20an%20issue%20is%20their%20fixed%0Aformation%20of%20regions%20and%20fixed%20input%20region%20features%2C%20which%20may%20not%20suit%20the%0Aneeds%20of%20different%20downstream%20tasks.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Amodel%20named%20FlexiReg%20for%20urban%20region%20representation%20learning%20that%20is%20flexible%0Awith%20both%20the%20formation%20of%20urban%20regions%20and%20the%20input%20region%20features.%0AFlexiReg%20is%20based%20on%20a%20spatial%20grid%20partitioning%20over%20the%20spatial%20area%20of%0Ainterest.%20It%20learns%20representations%20for%20the%20grid%20cells%2C%20leveraging%20publicly%0Aaccessible%20data%2C%20including%20POI%2C%20land%20use%2C%20satellite%20imagery%2C%20and%20street%20view%0Aimagery.%20We%20propose%20adaptive%20aggregation%20to%20fuse%20the%20cell%20representations%20and%0Aprompt%20learning%20techniques%20to%20tailor%20the%20representations%20towards%20different%0Atasks%2C%20addressing%20the%20needs%20of%20varying%20formations%20of%20urban%20regions%20and%0Adownstream%20tasks.%20Extensive%20experiments%20on%20five%20real-world%20datasets%20demonstrate%0Athat%20FlexiReg%20outperforms%20state-of-the-art%20models%20by%20up%20to%20202%25%20in%20term%20of%20the%0Aaccuracy%20of%20four%20diverse%20downstream%20tasks%20using%20the%20produced%20urban%20region%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09128v2&entry.124074799=Read"},
{"title": "Advancing high-fidelity 3D and Texture Generation with 2.5D latents", "author": "Xin Yang and Jiantao Lin and Yingjie Xu and Haodong Li and Yingcong Chen", "abstract": "  Despite the availability of large-scale 3D datasets and advancements in 3D\ngenerative models, the complexity and uneven quality of 3D geometry and texture\ndata continue to hinder the performance of 3D generation techniques. In most\nexisting approaches, 3D geometry and texture are generated in separate stages\nusing different models and non-unified representations, frequently leading to\nunsatisfactory coherence between geometry and texture. To address these\nchallenges, we propose a novel framework for joint generation of 3D geometry\nand texture. Specifically, we focus in generate a versatile 2.5D\nrepresentations that can be seamlessly transformed between 2D and 3D. Our\napproach begins by integrating multiview RGB, normal, and coordinate images\ninto a unified representation, termed as 2.5D latents. Next, we adapt\npre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing\nboth text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D\nrefiner-decoder framework that efficiently generates detailed 3D\nrepresentations from 2.5D images. Extensive experiments demonstrate that our\nmodel not only excels in generating high-quality 3D objects with coherent\nstructure and color from text and image inputs but also significantly\noutperforms existing methods in geometry-conditioned texture generation.\n", "link": "http://arxiv.org/abs/2505.21050v1", "date": "2025-05-27", "relevancy": 2.5423, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6376}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6376}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20high-fidelity%203D%20and%20Texture%20Generation%20with%202.5D%20latents&body=Title%3A%20Advancing%20high-fidelity%203D%20and%20Texture%20Generation%20with%202.5D%20latents%0AAuthor%3A%20Xin%20Yang%20and%20Jiantao%20Lin%20and%20Yingjie%20Xu%20and%20Haodong%20Li%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20Despite%20the%20availability%20of%20large-scale%203D%20datasets%20and%20advancements%20in%203D%0Agenerative%20models%2C%20the%20complexity%20and%20uneven%20quality%20of%203D%20geometry%20and%20texture%0Adata%20continue%20to%20hinder%20the%20performance%20of%203D%20generation%20techniques.%20In%20most%0Aexisting%20approaches%2C%203D%20geometry%20and%20texture%20are%20generated%20in%20separate%20stages%0Ausing%20different%20models%20and%20non-unified%20representations%2C%20frequently%20leading%20to%0Aunsatisfactory%20coherence%20between%20geometry%20and%20texture.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20framework%20for%20joint%20generation%20of%203D%20geometry%0Aand%20texture.%20Specifically%2C%20we%20focus%20in%20generate%20a%20versatile%202.5D%0Arepresentations%20that%20can%20be%20seamlessly%20transformed%20between%202D%20and%203D.%20Our%0Aapproach%20begins%20by%20integrating%20multiview%20RGB%2C%20normal%2C%20and%20coordinate%20images%0Ainto%20a%20unified%20representation%2C%20termed%20as%202.5D%20latents.%20Next%2C%20we%20adapt%0Apre-trained%202D%20foundation%20models%20for%20high-fidelity%202.5D%20generation%2C%20utilizing%0Aboth%20text%20and%20image%20conditions.%20Finally%2C%20we%20introduce%20a%20lightweight%202.5D-to-3D%0Arefiner-decoder%20framework%20that%20efficiently%20generates%20detailed%203D%0Arepresentations%20from%202.5D%20images.%20Extensive%20experiments%20demonstrate%20that%20our%0Amodel%20not%20only%20excels%20in%20generating%20high-quality%203D%20objects%20with%20coherent%0Astructure%20and%20color%20from%20text%20and%20image%20inputs%20but%20also%20significantly%0Aoutperforms%20existing%20methods%20in%20geometry-conditioned%20texture%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520high-fidelity%25203D%2520and%2520Texture%2520Generation%2520with%25202.5D%2520latents%26entry.906535625%3DXin%2520Yang%2520and%2520Jiantao%2520Lin%2520and%2520Yingjie%2520Xu%2520and%2520Haodong%2520Li%2520and%2520Yingcong%2520Chen%26entry.1292438233%3D%2520%2520Despite%2520the%2520availability%2520of%2520large-scale%25203D%2520datasets%2520and%2520advancements%2520in%25203D%250Agenerative%2520models%252C%2520the%2520complexity%2520and%2520uneven%2520quality%2520of%25203D%2520geometry%2520and%2520texture%250Adata%2520continue%2520to%2520hinder%2520the%2520performance%2520of%25203D%2520generation%2520techniques.%2520In%2520most%250Aexisting%2520approaches%252C%25203D%2520geometry%2520and%2520texture%2520are%2520generated%2520in%2520separate%2520stages%250Ausing%2520different%2520models%2520and%2520non-unified%2520representations%252C%2520frequently%2520leading%2520to%250Aunsatisfactory%2520coherence%2520between%2520geometry%2520and%2520texture.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520joint%2520generation%2520of%25203D%2520geometry%250Aand%2520texture.%2520Specifically%252C%2520we%2520focus%2520in%2520generate%2520a%2520versatile%25202.5D%250Arepresentations%2520that%2520can%2520be%2520seamlessly%2520transformed%2520between%25202D%2520and%25203D.%2520Our%250Aapproach%2520begins%2520by%2520integrating%2520multiview%2520RGB%252C%2520normal%252C%2520and%2520coordinate%2520images%250Ainto%2520a%2520unified%2520representation%252C%2520termed%2520as%25202.5D%2520latents.%2520Next%252C%2520we%2520adapt%250Apre-trained%25202D%2520foundation%2520models%2520for%2520high-fidelity%25202.5D%2520generation%252C%2520utilizing%250Aboth%2520text%2520and%2520image%2520conditions.%2520Finally%252C%2520we%2520introduce%2520a%2520lightweight%25202.5D-to-3D%250Arefiner-decoder%2520framework%2520that%2520efficiently%2520generates%2520detailed%25203D%250Arepresentations%2520from%25202.5D%2520images.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amodel%2520not%2520only%2520excels%2520in%2520generating%2520high-quality%25203D%2520objects%2520with%2520coherent%250Astructure%2520and%2520color%2520from%2520text%2520and%2520image%2520inputs%2520but%2520also%2520significantly%250Aoutperforms%2520existing%2520methods%2520in%2520geometry-conditioned%2520texture%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20high-fidelity%203D%20and%20Texture%20Generation%20with%202.5D%20latents&entry.906535625=Xin%20Yang%20and%20Jiantao%20Lin%20and%20Yingjie%20Xu%20and%20Haodong%20Li%20and%20Yingcong%20Chen&entry.1292438233=%20%20Despite%20the%20availability%20of%20large-scale%203D%20datasets%20and%20advancements%20in%203D%0Agenerative%20models%2C%20the%20complexity%20and%20uneven%20quality%20of%203D%20geometry%20and%20texture%0Adata%20continue%20to%20hinder%20the%20performance%20of%203D%20generation%20techniques.%20In%20most%0Aexisting%20approaches%2C%203D%20geometry%20and%20texture%20are%20generated%20in%20separate%20stages%0Ausing%20different%20models%20and%20non-unified%20representations%2C%20frequently%20leading%20to%0Aunsatisfactory%20coherence%20between%20geometry%20and%20texture.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20framework%20for%20joint%20generation%20of%203D%20geometry%0Aand%20texture.%20Specifically%2C%20we%20focus%20in%20generate%20a%20versatile%202.5D%0Arepresentations%20that%20can%20be%20seamlessly%20transformed%20between%202D%20and%203D.%20Our%0Aapproach%20begins%20by%20integrating%20multiview%20RGB%2C%20normal%2C%20and%20coordinate%20images%0Ainto%20a%20unified%20representation%2C%20termed%20as%202.5D%20latents.%20Next%2C%20we%20adapt%0Apre-trained%202D%20foundation%20models%20for%20high-fidelity%202.5D%20generation%2C%20utilizing%0Aboth%20text%20and%20image%20conditions.%20Finally%2C%20we%20introduce%20a%20lightweight%202.5D-to-3D%0Arefiner-decoder%20framework%20that%20efficiently%20generates%20detailed%203D%0Arepresentations%20from%202.5D%20images.%20Extensive%20experiments%20demonstrate%20that%20our%0Amodel%20not%20only%20excels%20in%20generating%20high-quality%203D%20objects%20with%20coherent%0Astructure%20and%20color%20from%20text%20and%20image%20inputs%20but%20also%20significantly%0Aoutperforms%20existing%20methods%20in%20geometry-conditioned%20texture%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21050v1&entry.124074799=Read"},
{"title": "Towards Robust Automated Perceptual Voice Quality Assessment with Deep\n  Learning", "author": "Whenty Ariyanti and Kuan-Yu Chen and Sabato Marco Siniscalchi and Hsin-Min Wang and Yu Tsao", "abstract": "  Objective: Perceptual voice quality assessment plays a critical role in\ndiagnosing and monitoring voice disorders by providing standardized evaluation\nof vocal function. Traditionally, this process relies on expert raters\nutilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation\nof Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain\n(GRBAS). However, these metrics are inherently subjective and susceptible to\ninter-rater variability, motivating the need for automated and objective\nassessment methods. Methods: We propose Voice Quality Assessment Network\n(VOQANet), a deep learning-based framework with an attention mechanism that\nleverages a Speech Foundation Model (SFM) to capture high-level acoustic and\nprosodic information from raw speech. To enhance robustness and\ninterpretability, we present VOQANet+, which integrates handcrafted acoustic\nfeatures such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM\nembeddings. Results: Sentence-based input yields stronger performance than\nvowel-based input, especially at the patient level. VOQANet consistently\noutperforms baseline methods in RMSE and PCC, while VOQANet+ performs even\nbetter and maintains robustness under noisy conditions. Conclusion: Combining\nSFM embeddings with domain-informed acoustic features improves interpretability\nand resilience. Significance: VOQANet+ shows strong potential for deployment in\nreal-world and telehealth settings, addressing the limitations of subjective\nperceptual assessments with an interpretable and noise-resilient solution.\n", "link": "http://arxiv.org/abs/2505.21356v1", "date": "2025-05-27", "relevancy": 2.5269, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Automated%20Perceptual%20Voice%20Quality%20Assessment%20with%20Deep%0A%20%20Learning&body=Title%3A%20Towards%20Robust%20Automated%20Perceptual%20Voice%20Quality%20Assessment%20with%20Deep%0A%20%20Learning%0AAuthor%3A%20Whenty%20Ariyanti%20and%20Kuan-Yu%20Chen%20and%20Sabato%20Marco%20Siniscalchi%20and%20Hsin-Min%20Wang%20and%20Yu%20Tsao%0AAbstract%3A%20%20%20Objective%3A%20Perceptual%20voice%20quality%20assessment%20plays%20a%20critical%20role%20in%0Adiagnosing%20and%20monitoring%20voice%20disorders%20by%20providing%20standardized%20evaluation%0Aof%20vocal%20function.%20Traditionally%2C%20this%20process%20relies%20on%20expert%20raters%0Autilizing%20standard%20scales%2C%20such%20as%20the%20Consensus%20Auditory-Perceptual%20Evaluation%0Aof%20Voice%20%28CAPE-V%29%20and%20Grade%2C%20Roughness%2C%20Breathiness%2C%20Asthenia%2C%20and%20Strain%0A%28GRBAS%29.%20However%2C%20these%20metrics%20are%20inherently%20subjective%20and%20susceptible%20to%0Ainter-rater%20variability%2C%20motivating%20the%20need%20for%20automated%20and%20objective%0Aassessment%20methods.%20Methods%3A%20We%20propose%20Voice%20Quality%20Assessment%20Network%0A%28VOQANet%29%2C%20a%20deep%20learning-based%20framework%20with%20an%20attention%20mechanism%20that%0Aleverages%20a%20Speech%20Foundation%20Model%20%28SFM%29%20to%20capture%20high-level%20acoustic%20and%0Aprosodic%20information%20from%20raw%20speech.%20To%20enhance%20robustness%20and%0Ainterpretability%2C%20we%20present%20VOQANet%2B%2C%20which%20integrates%20handcrafted%20acoustic%0Afeatures%20such%20as%20jitter%2C%20shimmer%2C%20and%20harmonics-to-noise%20ratio%20%28HNR%29%20with%20SFM%0Aembeddings.%20Results%3A%20Sentence-based%20input%20yields%20stronger%20performance%20than%0Avowel-based%20input%2C%20especially%20at%20the%20patient%20level.%20VOQANet%20consistently%0Aoutperforms%20baseline%20methods%20in%20RMSE%20and%20PCC%2C%20while%20VOQANet%2B%20performs%20even%0Abetter%20and%20maintains%20robustness%20under%20noisy%20conditions.%20Conclusion%3A%20Combining%0ASFM%20embeddings%20with%20domain-informed%20acoustic%20features%20improves%20interpretability%0Aand%20resilience.%20Significance%3A%20VOQANet%2B%20shows%20strong%20potential%20for%20deployment%20in%0Areal-world%20and%20telehealth%20settings%2C%20addressing%20the%20limitations%20of%20subjective%0Aperceptual%20assessments%20with%20an%20interpretable%20and%20noise-resilient%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Automated%2520Perceptual%2520Voice%2520Quality%2520Assessment%2520with%2520Deep%250A%2520%2520Learning%26entry.906535625%3DWhenty%2520Ariyanti%2520and%2520Kuan-Yu%2520Chen%2520and%2520Sabato%2520Marco%2520Siniscalchi%2520and%2520Hsin-Min%2520Wang%2520and%2520Yu%2520Tsao%26entry.1292438233%3D%2520%2520Objective%253A%2520Perceptual%2520voice%2520quality%2520assessment%2520plays%2520a%2520critical%2520role%2520in%250Adiagnosing%2520and%2520monitoring%2520voice%2520disorders%2520by%2520providing%2520standardized%2520evaluation%250Aof%2520vocal%2520function.%2520Traditionally%252C%2520this%2520process%2520relies%2520on%2520expert%2520raters%250Autilizing%2520standard%2520scales%252C%2520such%2520as%2520the%2520Consensus%2520Auditory-Perceptual%2520Evaluation%250Aof%2520Voice%2520%2528CAPE-V%2529%2520and%2520Grade%252C%2520Roughness%252C%2520Breathiness%252C%2520Asthenia%252C%2520and%2520Strain%250A%2528GRBAS%2529.%2520However%252C%2520these%2520metrics%2520are%2520inherently%2520subjective%2520and%2520susceptible%2520to%250Ainter-rater%2520variability%252C%2520motivating%2520the%2520need%2520for%2520automated%2520and%2520objective%250Aassessment%2520methods.%2520Methods%253A%2520We%2520propose%2520Voice%2520Quality%2520Assessment%2520Network%250A%2528VOQANet%2529%252C%2520a%2520deep%2520learning-based%2520framework%2520with%2520an%2520attention%2520mechanism%2520that%250Aleverages%2520a%2520Speech%2520Foundation%2520Model%2520%2528SFM%2529%2520to%2520capture%2520high-level%2520acoustic%2520and%250Aprosodic%2520information%2520from%2520raw%2520speech.%2520To%2520enhance%2520robustness%2520and%250Ainterpretability%252C%2520we%2520present%2520VOQANet%252B%252C%2520which%2520integrates%2520handcrafted%2520acoustic%250Afeatures%2520such%2520as%2520jitter%252C%2520shimmer%252C%2520and%2520harmonics-to-noise%2520ratio%2520%2528HNR%2529%2520with%2520SFM%250Aembeddings.%2520Results%253A%2520Sentence-based%2520input%2520yields%2520stronger%2520performance%2520than%250Avowel-based%2520input%252C%2520especially%2520at%2520the%2520patient%2520level.%2520VOQANet%2520consistently%250Aoutperforms%2520baseline%2520methods%2520in%2520RMSE%2520and%2520PCC%252C%2520while%2520VOQANet%252B%2520performs%2520even%250Abetter%2520and%2520maintains%2520robustness%2520under%2520noisy%2520conditions.%2520Conclusion%253A%2520Combining%250ASFM%2520embeddings%2520with%2520domain-informed%2520acoustic%2520features%2520improves%2520interpretability%250Aand%2520resilience.%2520Significance%253A%2520VOQANet%252B%2520shows%2520strong%2520potential%2520for%2520deployment%2520in%250Areal-world%2520and%2520telehealth%2520settings%252C%2520addressing%2520the%2520limitations%2520of%2520subjective%250Aperceptual%2520assessments%2520with%2520an%2520interpretable%2520and%2520noise-resilient%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Automated%20Perceptual%20Voice%20Quality%20Assessment%20with%20Deep%0A%20%20Learning&entry.906535625=Whenty%20Ariyanti%20and%20Kuan-Yu%20Chen%20and%20Sabato%20Marco%20Siniscalchi%20and%20Hsin-Min%20Wang%20and%20Yu%20Tsao&entry.1292438233=%20%20Objective%3A%20Perceptual%20voice%20quality%20assessment%20plays%20a%20critical%20role%20in%0Adiagnosing%20and%20monitoring%20voice%20disorders%20by%20providing%20standardized%20evaluation%0Aof%20vocal%20function.%20Traditionally%2C%20this%20process%20relies%20on%20expert%20raters%0Autilizing%20standard%20scales%2C%20such%20as%20the%20Consensus%20Auditory-Perceptual%20Evaluation%0Aof%20Voice%20%28CAPE-V%29%20and%20Grade%2C%20Roughness%2C%20Breathiness%2C%20Asthenia%2C%20and%20Strain%0A%28GRBAS%29.%20However%2C%20these%20metrics%20are%20inherently%20subjective%20and%20susceptible%20to%0Ainter-rater%20variability%2C%20motivating%20the%20need%20for%20automated%20and%20objective%0Aassessment%20methods.%20Methods%3A%20We%20propose%20Voice%20Quality%20Assessment%20Network%0A%28VOQANet%29%2C%20a%20deep%20learning-based%20framework%20with%20an%20attention%20mechanism%20that%0Aleverages%20a%20Speech%20Foundation%20Model%20%28SFM%29%20to%20capture%20high-level%20acoustic%20and%0Aprosodic%20information%20from%20raw%20speech.%20To%20enhance%20robustness%20and%0Ainterpretability%2C%20we%20present%20VOQANet%2B%2C%20which%20integrates%20handcrafted%20acoustic%0Afeatures%20such%20as%20jitter%2C%20shimmer%2C%20and%20harmonics-to-noise%20ratio%20%28HNR%29%20with%20SFM%0Aembeddings.%20Results%3A%20Sentence-based%20input%20yields%20stronger%20performance%20than%0Avowel-based%20input%2C%20especially%20at%20the%20patient%20level.%20VOQANet%20consistently%0Aoutperforms%20baseline%20methods%20in%20RMSE%20and%20PCC%2C%20while%20VOQANet%2B%20performs%20even%0Abetter%20and%20maintains%20robustness%20under%20noisy%20conditions.%20Conclusion%3A%20Combining%0ASFM%20embeddings%20with%20domain-informed%20acoustic%20features%20improves%20interpretability%0Aand%20resilience.%20Significance%3A%20VOQANet%2B%20shows%20strong%20potential%20for%20deployment%20in%0Areal-world%20and%20telehealth%20settings%2C%20addressing%20the%20limitations%20of%20subjective%0Aperceptual%20assessments%20with%20an%20interpretable%20and%20noise-resilient%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21356v1&entry.124074799=Read"},
{"title": "Joint Learning in the Gaussian Single Index Model", "author": "Loucas Pillaud-Vivien and Adrien Schertzer", "abstract": "  We consider the problem of jointly learning a one-dimensional projection and\na univariate function in high-dimensional Gaussian models. Specifically, we\nstudy predictors of the form $f(x)=\\varphi^\\star(\\langle w^\\star, x \\rangle)$,\nwhere both the direction $w^\\star \\in \\mathcal{S}_{d-1}$, the sphere of\n$\\mathbb{R}^d$, and the function $\\varphi^\\star: \\mathbb{R} \\to \\mathbb{R}$ are\nlearned from Gaussian data. This setting captures a fundamental non-convex\nproblem at the intersection of representation learning and nonlinear\nregression. We analyze the gradient flow dynamics of a natural alternating\nscheme and prove convergence, with a rate controlled by the information\nexponent reflecting the \\textit{Gaussian regularity} of the function\n$\\varphi^\\star$. Strikingly, our analysis shows that convergence still occurs\neven when the initial direction is negatively correlated with the target. On\nthe practical side, we demonstrate that such joint learning can be effectively\nimplemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the\nstructure of the problem, enabling efficient and flexible estimation of the\nunivariate function. Our results offer both theoretical insight and practical\nmethodology for learning low-dimensional structure in high-dimensional\nsettings.\n", "link": "http://arxiv.org/abs/2505.21336v1", "date": "2025-05-27", "relevancy": 2.4982, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5228}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4908}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Learning%20in%20the%20Gaussian%20Single%20Index%20Model&body=Title%3A%20Joint%20Learning%20in%20the%20Gaussian%20Single%20Index%20Model%0AAuthor%3A%20Loucas%20Pillaud-Vivien%20and%20Adrien%20Schertzer%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20jointly%20learning%20a%20one-dimensional%20projection%20and%0Aa%20univariate%20function%20in%20high-dimensional%20Gaussian%20models.%20Specifically%2C%20we%0Astudy%20predictors%20of%20the%20form%20%24f%28x%29%3D%5Cvarphi%5E%5Cstar%28%5Clangle%20w%5E%5Cstar%2C%20x%20%5Crangle%29%24%2C%0Awhere%20both%20the%20direction%20%24w%5E%5Cstar%20%5Cin%20%5Cmathcal%7BS%7D_%7Bd-1%7D%24%2C%20the%20sphere%20of%0A%24%5Cmathbb%7BR%7D%5Ed%24%2C%20and%20the%20function%20%24%5Cvarphi%5E%5Cstar%3A%20%5Cmathbb%7BR%7D%20%5Cto%20%5Cmathbb%7BR%7D%24%20are%0Alearned%20from%20Gaussian%20data.%20This%20setting%20captures%20a%20fundamental%20non-convex%0Aproblem%20at%20the%20intersection%20of%20representation%20learning%20and%20nonlinear%0Aregression.%20We%20analyze%20the%20gradient%20flow%20dynamics%20of%20a%20natural%20alternating%0Ascheme%20and%20prove%20convergence%2C%20with%20a%20rate%20controlled%20by%20the%20information%0Aexponent%20reflecting%20the%20%5Ctextit%7BGaussian%20regularity%7D%20of%20the%20function%0A%24%5Cvarphi%5E%5Cstar%24.%20Strikingly%2C%20our%20analysis%20shows%20that%20convergence%20still%20occurs%0Aeven%20when%20the%20initial%20direction%20is%20negatively%20correlated%20with%20the%20target.%20On%0Athe%20practical%20side%2C%20we%20demonstrate%20that%20such%20joint%20learning%20can%20be%20effectively%0Aimplemented%20using%20a%20Reproducing%20Kernel%20Hilbert%20Space%20%28RKHS%29%20adapted%20to%20the%0Astructure%20of%20the%20problem%2C%20enabling%20efficient%20and%20flexible%20estimation%20of%20the%0Aunivariate%20function.%20Our%20results%20offer%20both%20theoretical%20insight%20and%20practical%0Amethodology%20for%20learning%20low-dimensional%20structure%20in%20high-dimensional%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Learning%2520in%2520the%2520Gaussian%2520Single%2520Index%2520Model%26entry.906535625%3DLoucas%2520Pillaud-Vivien%2520and%2520Adrien%2520Schertzer%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520jointly%2520learning%2520a%2520one-dimensional%2520projection%2520and%250Aa%2520univariate%2520function%2520in%2520high-dimensional%2520Gaussian%2520models.%2520Specifically%252C%2520we%250Astudy%2520predictors%2520of%2520the%2520form%2520%2524f%2528x%2529%253D%255Cvarphi%255E%255Cstar%2528%255Clangle%2520w%255E%255Cstar%252C%2520x%2520%255Crangle%2529%2524%252C%250Awhere%2520both%2520the%2520direction%2520%2524w%255E%255Cstar%2520%255Cin%2520%255Cmathcal%257BS%257D_%257Bd-1%257D%2524%252C%2520the%2520sphere%2520of%250A%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520and%2520the%2520function%2520%2524%255Cvarphi%255E%255Cstar%253A%2520%255Cmathbb%257BR%257D%2520%255Cto%2520%255Cmathbb%257BR%257D%2524%2520are%250Alearned%2520from%2520Gaussian%2520data.%2520This%2520setting%2520captures%2520a%2520fundamental%2520non-convex%250Aproblem%2520at%2520the%2520intersection%2520of%2520representation%2520learning%2520and%2520nonlinear%250Aregression.%2520We%2520analyze%2520the%2520gradient%2520flow%2520dynamics%2520of%2520a%2520natural%2520alternating%250Ascheme%2520and%2520prove%2520convergence%252C%2520with%2520a%2520rate%2520controlled%2520by%2520the%2520information%250Aexponent%2520reflecting%2520the%2520%255Ctextit%257BGaussian%2520regularity%257D%2520of%2520the%2520function%250A%2524%255Cvarphi%255E%255Cstar%2524.%2520Strikingly%252C%2520our%2520analysis%2520shows%2520that%2520convergence%2520still%2520occurs%250Aeven%2520when%2520the%2520initial%2520direction%2520is%2520negatively%2520correlated%2520with%2520the%2520target.%2520On%250Athe%2520practical%2520side%252C%2520we%2520demonstrate%2520that%2520such%2520joint%2520learning%2520can%2520be%2520effectively%250Aimplemented%2520using%2520a%2520Reproducing%2520Kernel%2520Hilbert%2520Space%2520%2528RKHS%2529%2520adapted%2520to%2520the%250Astructure%2520of%2520the%2520problem%252C%2520enabling%2520efficient%2520and%2520flexible%2520estimation%2520of%2520the%250Aunivariate%2520function.%2520Our%2520results%2520offer%2520both%2520theoretical%2520insight%2520and%2520practical%250Amethodology%2520for%2520learning%2520low-dimensional%2520structure%2520in%2520high-dimensional%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Learning%20in%20the%20Gaussian%20Single%20Index%20Model&entry.906535625=Loucas%20Pillaud-Vivien%20and%20Adrien%20Schertzer&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20jointly%20learning%20a%20one-dimensional%20projection%20and%0Aa%20univariate%20function%20in%20high-dimensional%20Gaussian%20models.%20Specifically%2C%20we%0Astudy%20predictors%20of%20the%20form%20%24f%28x%29%3D%5Cvarphi%5E%5Cstar%28%5Clangle%20w%5E%5Cstar%2C%20x%20%5Crangle%29%24%2C%0Awhere%20both%20the%20direction%20%24w%5E%5Cstar%20%5Cin%20%5Cmathcal%7BS%7D_%7Bd-1%7D%24%2C%20the%20sphere%20of%0A%24%5Cmathbb%7BR%7D%5Ed%24%2C%20and%20the%20function%20%24%5Cvarphi%5E%5Cstar%3A%20%5Cmathbb%7BR%7D%20%5Cto%20%5Cmathbb%7BR%7D%24%20are%0Alearned%20from%20Gaussian%20data.%20This%20setting%20captures%20a%20fundamental%20non-convex%0Aproblem%20at%20the%20intersection%20of%20representation%20learning%20and%20nonlinear%0Aregression.%20We%20analyze%20the%20gradient%20flow%20dynamics%20of%20a%20natural%20alternating%0Ascheme%20and%20prove%20convergence%2C%20with%20a%20rate%20controlled%20by%20the%20information%0Aexponent%20reflecting%20the%20%5Ctextit%7BGaussian%20regularity%7D%20of%20the%20function%0A%24%5Cvarphi%5E%5Cstar%24.%20Strikingly%2C%20our%20analysis%20shows%20that%20convergence%20still%20occurs%0Aeven%20when%20the%20initial%20direction%20is%20negatively%20correlated%20with%20the%20target.%20On%0Athe%20practical%20side%2C%20we%20demonstrate%20that%20such%20joint%20learning%20can%20be%20effectively%0Aimplemented%20using%20a%20Reproducing%20Kernel%20Hilbert%20Space%20%28RKHS%29%20adapted%20to%20the%0Astructure%20of%20the%20problem%2C%20enabling%20efficient%20and%20flexible%20estimation%20of%20the%0Aunivariate%20function.%20Our%20results%20offer%20both%20theoretical%20insight%20and%20practical%0Amethodology%20for%20learning%20low-dimensional%20structure%20in%20high-dimensional%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21336v1&entry.124074799=Read"},
{"title": "LPOI: Listwise Preference Optimization for Vision Language Models", "author": "Fatemeh Pesaran Zadeh and Yoojin Oh and Gunhee Kim", "abstract": "  Aligning large VLMs with human preferences is a challenging task, as methods\nlike RLHF and DPO often overfit to textual information or exacerbate\nhallucinations. Although augmenting negative image samples partially addresses\nthese pitfalls, no prior work has employed listwise preference optimization for\nVLMs, due to the complexity and cost of constructing listwise image samples. In\nthis work, we propose LPOI, the first object-aware listwise preference\noptimization developed for reducing hallucinations in VLMs. LPOI identifies and\nmasks a critical object in the image, and then interpolates the masked region\nbetween the positive and negative images to form a sequence of incrementally\nmore complete images. The model is trained to rank these images in ascending\norder of object visibility, effectively reducing hallucinations while retaining\nvisual fidelity. LPOI requires no extra annotations beyond standard pairwise\npreference data, as it automatically constructs the ranked lists through object\nmasking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and\nObject HalBench confirm that LPOI outperforms existing preference optimization\nmethods in reducing hallucinations and enhancing VLM performance. We make the\ncode available at https://github.com/fatemehpesaran310/lpoi.\n", "link": "http://arxiv.org/abs/2505.21061v1", "date": "2025-05-27", "relevancy": 2.4937, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5028}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPOI%3A%20Listwise%20Preference%20Optimization%20for%20Vision%20Language%20Models&body=Title%3A%20LPOI%3A%20Listwise%20Preference%20Optimization%20for%20Vision%20Language%20Models%0AAuthor%3A%20Fatemeh%20Pesaran%20Zadeh%20and%20Yoojin%20Oh%20and%20Gunhee%20Kim%0AAbstract%3A%20%20%20Aligning%20large%20VLMs%20with%20human%20preferences%20is%20a%20challenging%20task%2C%20as%20methods%0Alike%20RLHF%20and%20DPO%20often%20overfit%20to%20textual%20information%20or%20exacerbate%0Ahallucinations.%20Although%20augmenting%20negative%20image%20samples%20partially%20addresses%0Athese%20pitfalls%2C%20no%20prior%20work%20has%20employed%20listwise%20preference%20optimization%20for%0AVLMs%2C%20due%20to%20the%20complexity%20and%20cost%20of%20constructing%20listwise%20image%20samples.%20In%0Athis%20work%2C%20we%20propose%20LPOI%2C%20the%20first%20object-aware%20listwise%20preference%0Aoptimization%20developed%20for%20reducing%20hallucinations%20in%20VLMs.%20LPOI%20identifies%20and%0Amasks%20a%20critical%20object%20in%20the%20image%2C%20and%20then%20interpolates%20the%20masked%20region%0Abetween%20the%20positive%20and%20negative%20images%20to%20form%20a%20sequence%20of%20incrementally%0Amore%20complete%20images.%20The%20model%20is%20trained%20to%20rank%20these%20images%20in%20ascending%0Aorder%20of%20object%20visibility%2C%20effectively%20reducing%20hallucinations%20while%20retaining%0Avisual%20fidelity.%20LPOI%20requires%20no%20extra%20annotations%20beyond%20standard%20pairwise%0Apreference%20data%2C%20as%20it%20automatically%20constructs%20the%20ranked%20lists%20through%20object%0Amasking%20and%20interpolation.%20Comprehensive%20experiments%20on%20MMHalBench%2C%20AMBER%2C%20and%0AObject%20HalBench%20confirm%20that%20LPOI%20outperforms%20existing%20preference%20optimization%0Amethods%20in%20reducing%20hallucinations%20and%20enhancing%20VLM%20performance.%20We%20make%20the%0Acode%20available%20at%20https%3A//github.com/fatemehpesaran310/lpoi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPOI%253A%2520Listwise%2520Preference%2520Optimization%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DFatemeh%2520Pesaran%2520Zadeh%2520and%2520Yoojin%2520Oh%2520and%2520Gunhee%2520Kim%26entry.1292438233%3D%2520%2520Aligning%2520large%2520VLMs%2520with%2520human%2520preferences%2520is%2520a%2520challenging%2520task%252C%2520as%2520methods%250Alike%2520RLHF%2520and%2520DPO%2520often%2520overfit%2520to%2520textual%2520information%2520or%2520exacerbate%250Ahallucinations.%2520Although%2520augmenting%2520negative%2520image%2520samples%2520partially%2520addresses%250Athese%2520pitfalls%252C%2520no%2520prior%2520work%2520has%2520employed%2520listwise%2520preference%2520optimization%2520for%250AVLMs%252C%2520due%2520to%2520the%2520complexity%2520and%2520cost%2520of%2520constructing%2520listwise%2520image%2520samples.%2520In%250Athis%2520work%252C%2520we%2520propose%2520LPOI%252C%2520the%2520first%2520object-aware%2520listwise%2520preference%250Aoptimization%2520developed%2520for%2520reducing%2520hallucinations%2520in%2520VLMs.%2520LPOI%2520identifies%2520and%250Amasks%2520a%2520critical%2520object%2520in%2520the%2520image%252C%2520and%2520then%2520interpolates%2520the%2520masked%2520region%250Abetween%2520the%2520positive%2520and%2520negative%2520images%2520to%2520form%2520a%2520sequence%2520of%2520incrementally%250Amore%2520complete%2520images.%2520The%2520model%2520is%2520trained%2520to%2520rank%2520these%2520images%2520in%2520ascending%250Aorder%2520of%2520object%2520visibility%252C%2520effectively%2520reducing%2520hallucinations%2520while%2520retaining%250Avisual%2520fidelity.%2520LPOI%2520requires%2520no%2520extra%2520annotations%2520beyond%2520standard%2520pairwise%250Apreference%2520data%252C%2520as%2520it%2520automatically%2520constructs%2520the%2520ranked%2520lists%2520through%2520object%250Amasking%2520and%2520interpolation.%2520Comprehensive%2520experiments%2520on%2520MMHalBench%252C%2520AMBER%252C%2520and%250AObject%2520HalBench%2520confirm%2520that%2520LPOI%2520outperforms%2520existing%2520preference%2520optimization%250Amethods%2520in%2520reducing%2520hallucinations%2520and%2520enhancing%2520VLM%2520performance.%2520We%2520make%2520the%250Acode%2520available%2520at%2520https%253A//github.com/fatemehpesaran310/lpoi.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPOI%3A%20Listwise%20Preference%20Optimization%20for%20Vision%20Language%20Models&entry.906535625=Fatemeh%20Pesaran%20Zadeh%20and%20Yoojin%20Oh%20and%20Gunhee%20Kim&entry.1292438233=%20%20Aligning%20large%20VLMs%20with%20human%20preferences%20is%20a%20challenging%20task%2C%20as%20methods%0Alike%20RLHF%20and%20DPO%20often%20overfit%20to%20textual%20information%20or%20exacerbate%0Ahallucinations.%20Although%20augmenting%20negative%20image%20samples%20partially%20addresses%0Athese%20pitfalls%2C%20no%20prior%20work%20has%20employed%20listwise%20preference%20optimization%20for%0AVLMs%2C%20due%20to%20the%20complexity%20and%20cost%20of%20constructing%20listwise%20image%20samples.%20In%0Athis%20work%2C%20we%20propose%20LPOI%2C%20the%20first%20object-aware%20listwise%20preference%0Aoptimization%20developed%20for%20reducing%20hallucinations%20in%20VLMs.%20LPOI%20identifies%20and%0Amasks%20a%20critical%20object%20in%20the%20image%2C%20and%20then%20interpolates%20the%20masked%20region%0Abetween%20the%20positive%20and%20negative%20images%20to%20form%20a%20sequence%20of%20incrementally%0Amore%20complete%20images.%20The%20model%20is%20trained%20to%20rank%20these%20images%20in%20ascending%0Aorder%20of%20object%20visibility%2C%20effectively%20reducing%20hallucinations%20while%20retaining%0Avisual%20fidelity.%20LPOI%20requires%20no%20extra%20annotations%20beyond%20standard%20pairwise%0Apreference%20data%2C%20as%20it%20automatically%20constructs%20the%20ranked%20lists%20through%20object%0Amasking%20and%20interpolation.%20Comprehensive%20experiments%20on%20MMHalBench%2C%20AMBER%2C%20and%0AObject%20HalBench%20confirm%20that%20LPOI%20outperforms%20existing%20preference%20optimization%0Amethods%20in%20reducing%20hallucinations%20and%20enhancing%20VLM%20performance.%20We%20make%20the%0Acode%20available%20at%20https%3A//github.com/fatemehpesaran310/lpoi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21061v1&entry.124074799=Read"},
{"title": "Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation", "author": "Prashant Shivaram Bhat and Shakib Yazdani and Elahe Arani and Bahram Zonooz", "abstract": "  Catastrophic forgetting has remained a critical challenge for deep neural\nnetworks in Continual Learning (CL) as it undermines consolidated knowledge\nwhen learning new tasks. Parameter efficient fine tuning CL techniques are\ngaining traction for their effectiveness in addressing catastrophic forgetting\nwith a lightweight training schedule while avoiding degradation of consolidated\nknowledge in pre-trained models. However, low rank adapters (LoRA) in these\napproaches are highly sensitive to rank selection which can lead to sub-optimal\nresource allocation and performance. To this end, we introduce PEARL, a\nrehearsal-free CL framework that entails dynamic rank allocation for LoRA\ncomponents during CL training. Specifically, PEARL leverages reference task\nweights and adaptively determines the rank of task-specific LoRA components\nbased on the current tasks' proximity to reference task weights in parameter\nspace. To demonstrate the versatility of PEARL, we evaluate it across three\nvision architectures (ResNet, Separable Convolutional Network and Vision\nTransformer) and a multitude of CL scenarios, and show that PEARL outperforms\nall considered baselines by a large margin.\n", "link": "http://arxiv.org/abs/2505.11998v2", "date": "2025-05-27", "relevancy": 2.4878, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5122}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter%20Efficient%20Continual%20Learning%20with%20Dynamic%20Low-Rank%20Adaptation&body=Title%3A%20Parameter%20Efficient%20Continual%20Learning%20with%20Dynamic%20Low-Rank%20Adaptation%0AAuthor%3A%20Prashant%20Shivaram%20Bhat%20and%20Shakib%20Yazdani%20and%20Elahe%20Arani%20and%20Bahram%20Zonooz%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20has%20remained%20a%20critical%20challenge%20for%20deep%20neural%0Anetworks%20in%20Continual%20Learning%20%28CL%29%20as%20it%20undermines%20consolidated%20knowledge%0Awhen%20learning%20new%20tasks.%20Parameter%20efficient%20fine%20tuning%20CL%20techniques%20are%0Againing%20traction%20for%20their%20effectiveness%20in%20addressing%20catastrophic%20forgetting%0Awith%20a%20lightweight%20training%20schedule%20while%20avoiding%20degradation%20of%20consolidated%0Aknowledge%20in%20pre-trained%20models.%20However%2C%20low%20rank%20adapters%20%28LoRA%29%20in%20these%0Aapproaches%20are%20highly%20sensitive%20to%20rank%20selection%20which%20can%20lead%20to%20sub-optimal%0Aresource%20allocation%20and%20performance.%20To%20this%20end%2C%20we%20introduce%20PEARL%2C%20a%0Arehearsal-free%20CL%20framework%20that%20entails%20dynamic%20rank%20allocation%20for%20LoRA%0Acomponents%20during%20CL%20training.%20Specifically%2C%20PEARL%20leverages%20reference%20task%0Aweights%20and%20adaptively%20determines%20the%20rank%20of%20task-specific%20LoRA%20components%0Abased%20on%20the%20current%20tasks%27%20proximity%20to%20reference%20task%20weights%20in%20parameter%0Aspace.%20To%20demonstrate%20the%20versatility%20of%20PEARL%2C%20we%20evaluate%20it%20across%20three%0Avision%20architectures%20%28ResNet%2C%20Separable%20Convolutional%20Network%20and%20Vision%0ATransformer%29%20and%20a%20multitude%20of%20CL%20scenarios%2C%20and%20show%20that%20PEARL%20outperforms%0Aall%20considered%20baselines%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter%2520Efficient%2520Continual%2520Learning%2520with%2520Dynamic%2520Low-Rank%2520Adaptation%26entry.906535625%3DPrashant%2520Shivaram%2520Bhat%2520and%2520Shakib%2520Yazdani%2520and%2520Elahe%2520Arani%2520and%2520Bahram%2520Zonooz%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520has%2520remained%2520a%2520critical%2520challenge%2520for%2520deep%2520neural%250Anetworks%2520in%2520Continual%2520Learning%2520%2528CL%2529%2520as%2520it%2520undermines%2520consolidated%2520knowledge%250Awhen%2520learning%2520new%2520tasks.%2520Parameter%2520efficient%2520fine%2520tuning%2520CL%2520techniques%2520are%250Againing%2520traction%2520for%2520their%2520effectiveness%2520in%2520addressing%2520catastrophic%2520forgetting%250Awith%2520a%2520lightweight%2520training%2520schedule%2520while%2520avoiding%2520degradation%2520of%2520consolidated%250Aknowledge%2520in%2520pre-trained%2520models.%2520However%252C%2520low%2520rank%2520adapters%2520%2528LoRA%2529%2520in%2520these%250Aapproaches%2520are%2520highly%2520sensitive%2520to%2520rank%2520selection%2520which%2520can%2520lead%2520to%2520sub-optimal%250Aresource%2520allocation%2520and%2520performance.%2520To%2520this%2520end%252C%2520we%2520introduce%2520PEARL%252C%2520a%250Arehearsal-free%2520CL%2520framework%2520that%2520entails%2520dynamic%2520rank%2520allocation%2520for%2520LoRA%250Acomponents%2520during%2520CL%2520training.%2520Specifically%252C%2520PEARL%2520leverages%2520reference%2520task%250Aweights%2520and%2520adaptively%2520determines%2520the%2520rank%2520of%2520task-specific%2520LoRA%2520components%250Abased%2520on%2520the%2520current%2520tasks%2527%2520proximity%2520to%2520reference%2520task%2520weights%2520in%2520parameter%250Aspace.%2520To%2520demonstrate%2520the%2520versatility%2520of%2520PEARL%252C%2520we%2520evaluate%2520it%2520across%2520three%250Avision%2520architectures%2520%2528ResNet%252C%2520Separable%2520Convolutional%2520Network%2520and%2520Vision%250ATransformer%2529%2520and%2520a%2520multitude%2520of%2520CL%2520scenarios%252C%2520and%2520show%2520that%2520PEARL%2520outperforms%250Aall%2520considered%2520baselines%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Efficient%20Continual%20Learning%20with%20Dynamic%20Low-Rank%20Adaptation&entry.906535625=Prashant%20Shivaram%20Bhat%20and%20Shakib%20Yazdani%20and%20Elahe%20Arani%20and%20Bahram%20Zonooz&entry.1292438233=%20%20Catastrophic%20forgetting%20has%20remained%20a%20critical%20challenge%20for%20deep%20neural%0Anetworks%20in%20Continual%20Learning%20%28CL%29%20as%20it%20undermines%20consolidated%20knowledge%0Awhen%20learning%20new%20tasks.%20Parameter%20efficient%20fine%20tuning%20CL%20techniques%20are%0Againing%20traction%20for%20their%20effectiveness%20in%20addressing%20catastrophic%20forgetting%0Awith%20a%20lightweight%20training%20schedule%20while%20avoiding%20degradation%20of%20consolidated%0Aknowledge%20in%20pre-trained%20models.%20However%2C%20low%20rank%20adapters%20%28LoRA%29%20in%20these%0Aapproaches%20are%20highly%20sensitive%20to%20rank%20selection%20which%20can%20lead%20to%20sub-optimal%0Aresource%20allocation%20and%20performance.%20To%20this%20end%2C%20we%20introduce%20PEARL%2C%20a%0Arehearsal-free%20CL%20framework%20that%20entails%20dynamic%20rank%20allocation%20for%20LoRA%0Acomponents%20during%20CL%20training.%20Specifically%2C%20PEARL%20leverages%20reference%20task%0Aweights%20and%20adaptively%20determines%20the%20rank%20of%20task-specific%20LoRA%20components%0Abased%20on%20the%20current%20tasks%27%20proximity%20to%20reference%20task%20weights%20in%20parameter%0Aspace.%20To%20demonstrate%20the%20versatility%20of%20PEARL%2C%20we%20evaluate%20it%20across%20three%0Avision%20architectures%20%28ResNet%2C%20Separable%20Convolutional%20Network%20and%20Vision%0ATransformer%29%20and%20a%20multitude%20of%20CL%20scenarios%2C%20and%20show%20that%20PEARL%20outperforms%0Aall%20considered%20baselines%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11998v2&entry.124074799=Read"},
{"title": "Recurrent Memory for Online Interdomain Gaussian Processes", "author": "Wenlong Chen and Naoki Kiyohara and Harrison Bo Hua Zhu and Jacob Curran-Sebastian and Samir Bhatt and Yingzhen Li", "abstract": "  We propose a novel online Gaussian process (GP) model that is capable of\ncapturing long-term memory in sequential data in an online learning setting.\nOur model, Online HiPPO Sparse Variational Gaussian Process (OHSVGP), leverages\nthe HiPPO (High-order Polynomial Projection Operators) framework, which is\npopularized in the RNN domain due to its long-range memory modeling\ncapabilities. We interpret the HiPPO time-varying orthogonal projections as\ninducing variables with time-dependent orthogonal polynomial basis functions,\nwhich allows the SVGP inducing variables to memorize the process history. We\nshow that the HiPPO framework fits naturally into the interdomain GP framework\nand demonstrate that the kernel matrices can also be updated online in a\nrecurrence form based on the ODE evolution of HiPPO. We evaluate OHSVGP with\nonline prediction for 1D time series, continual learning in discriminative GP\nmodel for data with multidimensional inputs, and deep generative modeling with\nsparse Gaussian process variational autoencoder, showing that it outperforms\nexisting online GP methods in terms of predictive performance, long-term memory\npreservation, and computational efficiency.\n", "link": "http://arxiv.org/abs/2502.08736v3", "date": "2025-05-27", "relevancy": 2.4754, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5158}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5022}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Memory%20for%20Online%20Interdomain%20Gaussian%20Processes&body=Title%3A%20Recurrent%20Memory%20for%20Online%20Interdomain%20Gaussian%20Processes%0AAuthor%3A%20Wenlong%20Chen%20and%20Naoki%20Kiyohara%20and%20Harrison%20Bo%20Hua%20Zhu%20and%20Jacob%20Curran-Sebastian%20and%20Samir%20Bhatt%20and%20Yingzhen%20Li%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20online%20Gaussian%20process%20%28GP%29%20model%20that%20is%20capable%20of%0Acapturing%20long-term%20memory%20in%20sequential%20data%20in%20an%20online%20learning%20setting.%0AOur%20model%2C%20Online%20HiPPO%20Sparse%20Variational%20Gaussian%20Process%20%28OHSVGP%29%2C%20leverages%0Athe%20HiPPO%20%28High-order%20Polynomial%20Projection%20Operators%29%20framework%2C%20which%20is%0Apopularized%20in%20the%20RNN%20domain%20due%20to%20its%20long-range%20memory%20modeling%0Acapabilities.%20We%20interpret%20the%20HiPPO%20time-varying%20orthogonal%20projections%20as%0Ainducing%20variables%20with%20time-dependent%20orthogonal%20polynomial%20basis%20functions%2C%0Awhich%20allows%20the%20SVGP%20inducing%20variables%20to%20memorize%20the%20process%20history.%20We%0Ashow%20that%20the%20HiPPO%20framework%20fits%20naturally%20into%20the%20interdomain%20GP%20framework%0Aand%20demonstrate%20that%20the%20kernel%20matrices%20can%20also%20be%20updated%20online%20in%20a%0Arecurrence%20form%20based%20on%20the%20ODE%20evolution%20of%20HiPPO.%20We%20evaluate%20OHSVGP%20with%0Aonline%20prediction%20for%201D%20time%20series%2C%20continual%20learning%20in%20discriminative%20GP%0Amodel%20for%20data%20with%20multidimensional%20inputs%2C%20and%20deep%20generative%20modeling%20with%0Asparse%20Gaussian%20process%20variational%20autoencoder%2C%20showing%20that%20it%20outperforms%0Aexisting%20online%20GP%20methods%20in%20terms%20of%20predictive%20performance%2C%20long-term%20memory%0Apreservation%2C%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08736v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Memory%2520for%2520Online%2520Interdomain%2520Gaussian%2520Processes%26entry.906535625%3DWenlong%2520Chen%2520and%2520Naoki%2520Kiyohara%2520and%2520Harrison%2520Bo%2520Hua%2520Zhu%2520and%2520Jacob%2520Curran-Sebastian%2520and%2520Samir%2520Bhatt%2520and%2520Yingzhen%2520Li%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520online%2520Gaussian%2520process%2520%2528GP%2529%2520model%2520that%2520is%2520capable%2520of%250Acapturing%2520long-term%2520memory%2520in%2520sequential%2520data%2520in%2520an%2520online%2520learning%2520setting.%250AOur%2520model%252C%2520Online%2520HiPPO%2520Sparse%2520Variational%2520Gaussian%2520Process%2520%2528OHSVGP%2529%252C%2520leverages%250Athe%2520HiPPO%2520%2528High-order%2520Polynomial%2520Projection%2520Operators%2529%2520framework%252C%2520which%2520is%250Apopularized%2520in%2520the%2520RNN%2520domain%2520due%2520to%2520its%2520long-range%2520memory%2520modeling%250Acapabilities.%2520We%2520interpret%2520the%2520HiPPO%2520time-varying%2520orthogonal%2520projections%2520as%250Ainducing%2520variables%2520with%2520time-dependent%2520orthogonal%2520polynomial%2520basis%2520functions%252C%250Awhich%2520allows%2520the%2520SVGP%2520inducing%2520variables%2520to%2520memorize%2520the%2520process%2520history.%2520We%250Ashow%2520that%2520the%2520HiPPO%2520framework%2520fits%2520naturally%2520into%2520the%2520interdomain%2520GP%2520framework%250Aand%2520demonstrate%2520that%2520the%2520kernel%2520matrices%2520can%2520also%2520be%2520updated%2520online%2520in%2520a%250Arecurrence%2520form%2520based%2520on%2520the%2520ODE%2520evolution%2520of%2520HiPPO.%2520We%2520evaluate%2520OHSVGP%2520with%250Aonline%2520prediction%2520for%25201D%2520time%2520series%252C%2520continual%2520learning%2520in%2520discriminative%2520GP%250Amodel%2520for%2520data%2520with%2520multidimensional%2520inputs%252C%2520and%2520deep%2520generative%2520modeling%2520with%250Asparse%2520Gaussian%2520process%2520variational%2520autoencoder%252C%2520showing%2520that%2520it%2520outperforms%250Aexisting%2520online%2520GP%2520methods%2520in%2520terms%2520of%2520predictive%2520performance%252C%2520long-term%2520memory%250Apreservation%252C%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08736v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Memory%20for%20Online%20Interdomain%20Gaussian%20Processes&entry.906535625=Wenlong%20Chen%20and%20Naoki%20Kiyohara%20and%20Harrison%20Bo%20Hua%20Zhu%20and%20Jacob%20Curran-Sebastian%20and%20Samir%20Bhatt%20and%20Yingzhen%20Li&entry.1292438233=%20%20We%20propose%20a%20novel%20online%20Gaussian%20process%20%28GP%29%20model%20that%20is%20capable%20of%0Acapturing%20long-term%20memory%20in%20sequential%20data%20in%20an%20online%20learning%20setting.%0AOur%20model%2C%20Online%20HiPPO%20Sparse%20Variational%20Gaussian%20Process%20%28OHSVGP%29%2C%20leverages%0Athe%20HiPPO%20%28High-order%20Polynomial%20Projection%20Operators%29%20framework%2C%20which%20is%0Apopularized%20in%20the%20RNN%20domain%20due%20to%20its%20long-range%20memory%20modeling%0Acapabilities.%20We%20interpret%20the%20HiPPO%20time-varying%20orthogonal%20projections%20as%0Ainducing%20variables%20with%20time-dependent%20orthogonal%20polynomial%20basis%20functions%2C%0Awhich%20allows%20the%20SVGP%20inducing%20variables%20to%20memorize%20the%20process%20history.%20We%0Ashow%20that%20the%20HiPPO%20framework%20fits%20naturally%20into%20the%20interdomain%20GP%20framework%0Aand%20demonstrate%20that%20the%20kernel%20matrices%20can%20also%20be%20updated%20online%20in%20a%0Arecurrence%20form%20based%20on%20the%20ODE%20evolution%20of%20HiPPO.%20We%20evaluate%20OHSVGP%20with%0Aonline%20prediction%20for%201D%20time%20series%2C%20continual%20learning%20in%20discriminative%20GP%0Amodel%20for%20data%20with%20multidimensional%20inputs%2C%20and%20deep%20generative%20modeling%20with%0Asparse%20Gaussian%20process%20variational%20autoencoder%2C%20showing%20that%20it%20outperforms%0Aexisting%20online%20GP%20methods%20in%20terms%20of%20predictive%20performance%2C%20long-term%20memory%0Apreservation%2C%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08736v3&entry.124074799=Read"},
{"title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate\n  Categories in Italian", "author": "Andrea Pedrotti and Giulia Rambelli and Caterina Villani and Marianna Bolognesi", "abstract": "  People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.\n", "link": "http://arxiv.org/abs/2505.21301v1", "date": "2025-05-27", "relevancy": 2.4712, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Humans%20and%20LLMs%20Organize%20Conceptual%20Knowledge%3A%20Exploring%20Subordinate%0A%20%20Categories%20in%20Italian&body=Title%3A%20How%20Humans%20and%20LLMs%20Organize%20Conceptual%20Knowledge%3A%20Exploring%20Subordinate%0A%20%20Categories%20in%20Italian%0AAuthor%3A%20Andrea%20Pedrotti%20and%20Giulia%20Rambelli%20and%20Caterina%20Villani%20and%20Marianna%20Bolognesi%0AAbstract%3A%20%20%20People%20can%20categorize%20the%20same%20entity%20at%20multiple%20taxonomic%20levels%2C%20such%20as%0Abasic%20%28bear%29%2C%20superordinate%20%28animal%29%2C%20and%20subordinate%20%28grizzly%20bear%29.%20While%0Aprior%20research%20has%20focused%20on%20basic-level%20categories%2C%20this%20study%20is%20the%20first%0Aattempt%20to%20examine%20the%20organization%20of%20categories%20by%20analyzing%20exemplars%0Aproduced%20at%20the%20subordinate%20level.%20We%20present%20a%20new%20Italian%20psycholinguistic%0Adataset%20of%20human-generated%20exemplars%20for%20187%20concrete%20words.%20We%20then%20use%20these%0Adata%20to%20evaluate%20whether%20textual%20and%20vision%20LLMs%20produce%20meaningful%20exemplars%0Athat%20align%20with%20human%20category%20organization%20across%20three%20key%20tasks%3A%20exemplar%0Ageneration%2C%20category%20induction%2C%20and%20typicality%20judgment.%20Our%20findings%20show%20a%0Alow%20alignment%20between%20humans%20and%20LLMs%2C%20consistent%20with%20previous%20studies.%0AHowever%2C%20their%20performance%20varies%20notably%20across%20different%20semantic%20domains.%0AUltimately%2C%20this%20study%20highlights%20both%20the%20promises%20and%20the%20constraints%20of%0Ausing%20AI-generated%20exemplars%20to%20support%20psychological%20and%20linguistic%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Humans%2520and%2520LLMs%2520Organize%2520Conceptual%2520Knowledge%253A%2520Exploring%2520Subordinate%250A%2520%2520Categories%2520in%2520Italian%26entry.906535625%3DAndrea%2520Pedrotti%2520and%2520Giulia%2520Rambelli%2520and%2520Caterina%2520Villani%2520and%2520Marianna%2520Bolognesi%26entry.1292438233%3D%2520%2520People%2520can%2520categorize%2520the%2520same%2520entity%2520at%2520multiple%2520taxonomic%2520levels%252C%2520such%2520as%250Abasic%2520%2528bear%2529%252C%2520superordinate%2520%2528animal%2529%252C%2520and%2520subordinate%2520%2528grizzly%2520bear%2529.%2520While%250Aprior%2520research%2520has%2520focused%2520on%2520basic-level%2520categories%252C%2520this%2520study%2520is%2520the%2520first%250Aattempt%2520to%2520examine%2520the%2520organization%2520of%2520categories%2520by%2520analyzing%2520exemplars%250Aproduced%2520at%2520the%2520subordinate%2520level.%2520We%2520present%2520a%2520new%2520Italian%2520psycholinguistic%250Adataset%2520of%2520human-generated%2520exemplars%2520for%2520187%2520concrete%2520words.%2520We%2520then%2520use%2520these%250Adata%2520to%2520evaluate%2520whether%2520textual%2520and%2520vision%2520LLMs%2520produce%2520meaningful%2520exemplars%250Athat%2520align%2520with%2520human%2520category%2520organization%2520across%2520three%2520key%2520tasks%253A%2520exemplar%250Ageneration%252C%2520category%2520induction%252C%2520and%2520typicality%2520judgment.%2520Our%2520findings%2520show%2520a%250Alow%2520alignment%2520between%2520humans%2520and%2520LLMs%252C%2520consistent%2520with%2520previous%2520studies.%250AHowever%252C%2520their%2520performance%2520varies%2520notably%2520across%2520different%2520semantic%2520domains.%250AUltimately%252C%2520this%2520study%2520highlights%2520both%2520the%2520promises%2520and%2520the%2520constraints%2520of%250Ausing%2520AI-generated%2520exemplars%2520to%2520support%2520psychological%2520and%2520linguistic%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Humans%20and%20LLMs%20Organize%20Conceptual%20Knowledge%3A%20Exploring%20Subordinate%0A%20%20Categories%20in%20Italian&entry.906535625=Andrea%20Pedrotti%20and%20Giulia%20Rambelli%20and%20Caterina%20Villani%20and%20Marianna%20Bolognesi&entry.1292438233=%20%20People%20can%20categorize%20the%20same%20entity%20at%20multiple%20taxonomic%20levels%2C%20such%20as%0Abasic%20%28bear%29%2C%20superordinate%20%28animal%29%2C%20and%20subordinate%20%28grizzly%20bear%29.%20While%0Aprior%20research%20has%20focused%20on%20basic-level%20categories%2C%20this%20study%20is%20the%20first%0Aattempt%20to%20examine%20the%20organization%20of%20categories%20by%20analyzing%20exemplars%0Aproduced%20at%20the%20subordinate%20level.%20We%20present%20a%20new%20Italian%20psycholinguistic%0Adataset%20of%20human-generated%20exemplars%20for%20187%20concrete%20words.%20We%20then%20use%20these%0Adata%20to%20evaluate%20whether%20textual%20and%20vision%20LLMs%20produce%20meaningful%20exemplars%0Athat%20align%20with%20human%20category%20organization%20across%20three%20key%20tasks%3A%20exemplar%0Ageneration%2C%20category%20induction%2C%20and%20typicality%20judgment.%20Our%20findings%20show%20a%0Alow%20alignment%20between%20humans%20and%20LLMs%2C%20consistent%20with%20previous%20studies.%0AHowever%2C%20their%20performance%20varies%20notably%20across%20different%20semantic%20domains.%0AUltimately%2C%20this%20study%20highlights%20both%20the%20promises%20and%20the%20constraints%20of%0Ausing%20AI-generated%20exemplars%20to%20support%20psychological%20and%20linguistic%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21301v1&entry.124074799=Read"},
{"title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling", "author": "Yichuan Cao and Yibo Miao and Xiao-Shan Gao and Yinpeng Dong", "abstract": "  Text-to-image (T2I) models raise ethical and safety concerns due to their\npotential to generate inappropriate or harmful images. Evaluating these models'\nsecurity through red-teaming is vital, yet white-box approaches are limited by\ntheir need for internal access, complicating their use with closed-source\nmodels. Moreover, existing black-box methods often assume knowledge about the\nmodel's specific defense mechanisms, limiting their utility in real-world\ncommercial API scenarios. A significant challenge is how to evade unknown and\ndiverse defense mechanisms. To overcome this difficulty, we propose a novel\nRule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively\nemploys LLM to modify prompts to query and leverages feedback from T2I systems\nfor fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a\nprior, enabling the LLM to dynamically adapt to unknown defense mechanisms.\nGiven that the feedback is often labeled and coarse-grained, making it\ndifficult to utilize directly, we further propose rule-based preference\nmodeling, which employs a set of rules to evaluate desired or undesired\nfeedback, facilitating finer-grained control over the LLM's dynamic adaptation\nprocess. Extensive experiments on nineteen T2I systems with varied safety\nmechanisms, three online commercial API services, and T2V models verify the\nsuperiority and practicality of our approach.\n", "link": "http://arxiv.org/abs/2505.21074v1", "date": "2025-05-27", "relevancy": 2.4695, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Red-Teaming%20Text-to-Image%20Systems%20by%20Rule-based%20Preference%20Modeling&body=Title%3A%20Red-Teaming%20Text-to-Image%20Systems%20by%20Rule-based%20Preference%20Modeling%0AAuthor%3A%20Yichuan%20Cao%20and%20Yibo%20Miao%20and%20Xiao-Shan%20Gao%20and%20Yinpeng%20Dong%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20models%20raise%20ethical%20and%20safety%20concerns%20due%20to%20their%0Apotential%20to%20generate%20inappropriate%20or%20harmful%20images.%20Evaluating%20these%20models%27%0Asecurity%20through%20red-teaming%20is%20vital%2C%20yet%20white-box%20approaches%20are%20limited%20by%0Atheir%20need%20for%20internal%20access%2C%20complicating%20their%20use%20with%20closed-source%0Amodels.%20Moreover%2C%20existing%20black-box%20methods%20often%20assume%20knowledge%20about%20the%0Amodel%27s%20specific%20defense%20mechanisms%2C%20limiting%20their%20utility%20in%20real-world%0Acommercial%20API%20scenarios.%20A%20significant%20challenge%20is%20how%20to%20evade%20unknown%20and%0Adiverse%20defense%20mechanisms.%20To%20overcome%20this%20difficulty%2C%20we%20propose%20a%20novel%0ARule-based%20Preference%20modeling%20Guided%20Red-Teaming%20%28RPG-RT%29%2C%20which%20iteratively%0Aemploys%20LLM%20to%20modify%20prompts%20to%20query%20and%20leverages%20feedback%20from%20T2I%20systems%0Afor%20fine-tuning%20the%20LLM.%20RPG-RT%20treats%20the%20feedback%20from%20each%20iteration%20as%20a%0Aprior%2C%20enabling%20the%20LLM%20to%20dynamically%20adapt%20to%20unknown%20defense%20mechanisms.%0AGiven%20that%20the%20feedback%20is%20often%20labeled%20and%20coarse-grained%2C%20making%20it%0Adifficult%20to%20utilize%20directly%2C%20we%20further%20propose%20rule-based%20preference%0Amodeling%2C%20which%20employs%20a%20set%20of%20rules%20to%20evaluate%20desired%20or%20undesired%0Afeedback%2C%20facilitating%20finer-grained%20control%20over%20the%20LLM%27s%20dynamic%20adaptation%0Aprocess.%20Extensive%20experiments%20on%20nineteen%20T2I%20systems%20with%20varied%20safety%0Amechanisms%2C%20three%20online%20commercial%20API%20services%2C%20and%20T2V%20models%20verify%20the%0Asuperiority%20and%20practicality%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRed-Teaming%2520Text-to-Image%2520Systems%2520by%2520Rule-based%2520Preference%2520Modeling%26entry.906535625%3DYichuan%2520Cao%2520and%2520Yibo%2520Miao%2520and%2520Xiao-Shan%2520Gao%2520and%2520Yinpeng%2520Dong%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520models%2520raise%2520ethical%2520and%2520safety%2520concerns%2520due%2520to%2520their%250Apotential%2520to%2520generate%2520inappropriate%2520or%2520harmful%2520images.%2520Evaluating%2520these%2520models%2527%250Asecurity%2520through%2520red-teaming%2520is%2520vital%252C%2520yet%2520white-box%2520approaches%2520are%2520limited%2520by%250Atheir%2520need%2520for%2520internal%2520access%252C%2520complicating%2520their%2520use%2520with%2520closed-source%250Amodels.%2520Moreover%252C%2520existing%2520black-box%2520methods%2520often%2520assume%2520knowledge%2520about%2520the%250Amodel%2527s%2520specific%2520defense%2520mechanisms%252C%2520limiting%2520their%2520utility%2520in%2520real-world%250Acommercial%2520API%2520scenarios.%2520A%2520significant%2520challenge%2520is%2520how%2520to%2520evade%2520unknown%2520and%250Adiverse%2520defense%2520mechanisms.%2520To%2520overcome%2520this%2520difficulty%252C%2520we%2520propose%2520a%2520novel%250ARule-based%2520Preference%2520modeling%2520Guided%2520Red-Teaming%2520%2528RPG-RT%2529%252C%2520which%2520iteratively%250Aemploys%2520LLM%2520to%2520modify%2520prompts%2520to%2520query%2520and%2520leverages%2520feedback%2520from%2520T2I%2520systems%250Afor%2520fine-tuning%2520the%2520LLM.%2520RPG-RT%2520treats%2520the%2520feedback%2520from%2520each%2520iteration%2520as%2520a%250Aprior%252C%2520enabling%2520the%2520LLM%2520to%2520dynamically%2520adapt%2520to%2520unknown%2520defense%2520mechanisms.%250AGiven%2520that%2520the%2520feedback%2520is%2520often%2520labeled%2520and%2520coarse-grained%252C%2520making%2520it%250Adifficult%2520to%2520utilize%2520directly%252C%2520we%2520further%2520propose%2520rule-based%2520preference%250Amodeling%252C%2520which%2520employs%2520a%2520set%2520of%2520rules%2520to%2520evaluate%2520desired%2520or%2520undesired%250Afeedback%252C%2520facilitating%2520finer-grained%2520control%2520over%2520the%2520LLM%2527s%2520dynamic%2520adaptation%250Aprocess.%2520Extensive%2520experiments%2520on%2520nineteen%2520T2I%2520systems%2520with%2520varied%2520safety%250Amechanisms%252C%2520three%2520online%2520commercial%2520API%2520services%252C%2520and%2520T2V%2520models%2520verify%2520the%250Asuperiority%2520and%2520practicality%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Red-Teaming%20Text-to-Image%20Systems%20by%20Rule-based%20Preference%20Modeling&entry.906535625=Yichuan%20Cao%20and%20Yibo%20Miao%20and%20Xiao-Shan%20Gao%20and%20Yinpeng%20Dong&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20models%20raise%20ethical%20and%20safety%20concerns%20due%20to%20their%0Apotential%20to%20generate%20inappropriate%20or%20harmful%20images.%20Evaluating%20these%20models%27%0Asecurity%20through%20red-teaming%20is%20vital%2C%20yet%20white-box%20approaches%20are%20limited%20by%0Atheir%20need%20for%20internal%20access%2C%20complicating%20their%20use%20with%20closed-source%0Amodels.%20Moreover%2C%20existing%20black-box%20methods%20often%20assume%20knowledge%20about%20the%0Amodel%27s%20specific%20defense%20mechanisms%2C%20limiting%20their%20utility%20in%20real-world%0Acommercial%20API%20scenarios.%20A%20significant%20challenge%20is%20how%20to%20evade%20unknown%20and%0Adiverse%20defense%20mechanisms.%20To%20overcome%20this%20difficulty%2C%20we%20propose%20a%20novel%0ARule-based%20Preference%20modeling%20Guided%20Red-Teaming%20%28RPG-RT%29%2C%20which%20iteratively%0Aemploys%20LLM%20to%20modify%20prompts%20to%20query%20and%20leverages%20feedback%20from%20T2I%20systems%0Afor%20fine-tuning%20the%20LLM.%20RPG-RT%20treats%20the%20feedback%20from%20each%20iteration%20as%20a%0Aprior%2C%20enabling%20the%20LLM%20to%20dynamically%20adapt%20to%20unknown%20defense%20mechanisms.%0AGiven%20that%20the%20feedback%20is%20often%20labeled%20and%20coarse-grained%2C%20making%20it%0Adifficult%20to%20utilize%20directly%2C%20we%20further%20propose%20rule-based%20preference%0Amodeling%2C%20which%20employs%20a%20set%20of%20rules%20to%20evaluate%20desired%20or%20undesired%0Afeedback%2C%20facilitating%20finer-grained%20control%20over%20the%20LLM%27s%20dynamic%20adaptation%0Aprocess.%20Extensive%20experiments%20on%20nineteen%20T2I%20systems%20with%20varied%20safety%0Amechanisms%2C%20three%20online%20commercial%20API%20services%2C%20and%20T2V%20models%20verify%20the%0Asuperiority%20and%20practicality%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21074v1&entry.124074799=Read"},
{"title": "A domain adaptation neural network for digital twin-supported fault\n  diagnosis", "author": "Zhenling Chen and Haiwei Fu and Zhiguo Zeng", "abstract": "  Digital twins offer a promising solution to the lack of sufficient labeled\ndata in deep learning-based fault diagnosis by generating simulated data for\nmodel training. However, discrepancies between simulation and real-world\nsystems can lead to a significant drop in performance when models are applied\nin real scenarios. To address this issue, we propose a fault diagnosis\nframework based on Domain-Adversarial Neural Networks (DANN), which enables\nknowledge transfer from simulated (source domain) to real-world (target domain)\ndata. We evaluate the proposed framework using a publicly available robotics\nfault diagnosis dataset, which includes 3,600 sequences generated by a digital\ntwin model and 90 real sequences collected from physical systems. The DANN\nmethod is compared with commonly used lightweight deep learning models such as\nCNN, TCN, Transformer, and LSTM. Experimental results show that incorporating\ndomain adaptation significantly improves the diagnostic performance. For\nexample, applying DANN to a baseline CNN model improves its accuracy from\n70.00% to 80.22% on real-world test data, demonstrating the effectiveness of\ndomain adaptation in bridging the sim-to-real gap.\n", "link": "http://arxiv.org/abs/2505.21046v1", "date": "2025-05-27", "relevancy": 2.4583, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5013}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4902}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20domain%20adaptation%20neural%20network%20for%20digital%20twin-supported%20fault%0A%20%20diagnosis&body=Title%3A%20A%20domain%20adaptation%20neural%20network%20for%20digital%20twin-supported%20fault%0A%20%20diagnosis%0AAuthor%3A%20Zhenling%20Chen%20and%20Haiwei%20Fu%20and%20Zhiguo%20Zeng%0AAbstract%3A%20%20%20Digital%20twins%20offer%20a%20promising%20solution%20to%20the%20lack%20of%20sufficient%20labeled%0Adata%20in%20deep%20learning-based%20fault%20diagnosis%20by%20generating%20simulated%20data%20for%0Amodel%20training.%20However%2C%20discrepancies%20between%20simulation%20and%20real-world%0Asystems%20can%20lead%20to%20a%20significant%20drop%20in%20performance%20when%20models%20are%20applied%0Ain%20real%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20a%20fault%20diagnosis%0Aframework%20based%20on%20Domain-Adversarial%20Neural%20Networks%20%28DANN%29%2C%20which%20enables%0Aknowledge%20transfer%20from%20simulated%20%28source%20domain%29%20to%20real-world%20%28target%20domain%29%0Adata.%20We%20evaluate%20the%20proposed%20framework%20using%20a%20publicly%20available%20robotics%0Afault%20diagnosis%20dataset%2C%20which%20includes%203%2C600%20sequences%20generated%20by%20a%20digital%0Atwin%20model%20and%2090%20real%20sequences%20collected%20from%20physical%20systems.%20The%20DANN%0Amethod%20is%20compared%20with%20commonly%20used%20lightweight%20deep%20learning%20models%20such%20as%0ACNN%2C%20TCN%2C%20Transformer%2C%20and%20LSTM.%20Experimental%20results%20show%20that%20incorporating%0Adomain%20adaptation%20significantly%20improves%20the%20diagnostic%20performance.%20For%0Aexample%2C%20applying%20DANN%20to%20a%20baseline%20CNN%20model%20improves%20its%20accuracy%20from%0A70.00%25%20to%2080.22%25%20on%20real-world%20test%20data%2C%20demonstrating%20the%20effectiveness%20of%0Adomain%20adaptation%20in%20bridging%20the%20sim-to-real%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520domain%2520adaptation%2520neural%2520network%2520for%2520digital%2520twin-supported%2520fault%250A%2520%2520diagnosis%26entry.906535625%3DZhenling%2520Chen%2520and%2520Haiwei%2520Fu%2520and%2520Zhiguo%2520Zeng%26entry.1292438233%3D%2520%2520Digital%2520twins%2520offer%2520a%2520promising%2520solution%2520to%2520the%2520lack%2520of%2520sufficient%2520labeled%250Adata%2520in%2520deep%2520learning-based%2520fault%2520diagnosis%2520by%2520generating%2520simulated%2520data%2520for%250Amodel%2520training.%2520However%252C%2520discrepancies%2520between%2520simulation%2520and%2520real-world%250Asystems%2520can%2520lead%2520to%2520a%2520significant%2520drop%2520in%2520performance%2520when%2520models%2520are%2520applied%250Ain%2520real%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520fault%2520diagnosis%250Aframework%2520based%2520on%2520Domain-Adversarial%2520Neural%2520Networks%2520%2528DANN%2529%252C%2520which%2520enables%250Aknowledge%2520transfer%2520from%2520simulated%2520%2528source%2520domain%2529%2520to%2520real-world%2520%2528target%2520domain%2529%250Adata.%2520We%2520evaluate%2520the%2520proposed%2520framework%2520using%2520a%2520publicly%2520available%2520robotics%250Afault%2520diagnosis%2520dataset%252C%2520which%2520includes%25203%252C600%2520sequences%2520generated%2520by%2520a%2520digital%250Atwin%2520model%2520and%252090%2520real%2520sequences%2520collected%2520from%2520physical%2520systems.%2520The%2520DANN%250Amethod%2520is%2520compared%2520with%2520commonly%2520used%2520lightweight%2520deep%2520learning%2520models%2520such%2520as%250ACNN%252C%2520TCN%252C%2520Transformer%252C%2520and%2520LSTM.%2520Experimental%2520results%2520show%2520that%2520incorporating%250Adomain%2520adaptation%2520significantly%2520improves%2520the%2520diagnostic%2520performance.%2520For%250Aexample%252C%2520applying%2520DANN%2520to%2520a%2520baseline%2520CNN%2520model%2520improves%2520its%2520accuracy%2520from%250A70.00%2525%2520to%252080.22%2525%2520on%2520real-world%2520test%2520data%252C%2520demonstrating%2520the%2520effectiveness%2520of%250Adomain%2520adaptation%2520in%2520bridging%2520the%2520sim-to-real%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20domain%20adaptation%20neural%20network%20for%20digital%20twin-supported%20fault%0A%20%20diagnosis&entry.906535625=Zhenling%20Chen%20and%20Haiwei%20Fu%20and%20Zhiguo%20Zeng&entry.1292438233=%20%20Digital%20twins%20offer%20a%20promising%20solution%20to%20the%20lack%20of%20sufficient%20labeled%0Adata%20in%20deep%20learning-based%20fault%20diagnosis%20by%20generating%20simulated%20data%20for%0Amodel%20training.%20However%2C%20discrepancies%20between%20simulation%20and%20real-world%0Asystems%20can%20lead%20to%20a%20significant%20drop%20in%20performance%20when%20models%20are%20applied%0Ain%20real%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20a%20fault%20diagnosis%0Aframework%20based%20on%20Domain-Adversarial%20Neural%20Networks%20%28DANN%29%2C%20which%20enables%0Aknowledge%20transfer%20from%20simulated%20%28source%20domain%29%20to%20real-world%20%28target%20domain%29%0Adata.%20We%20evaluate%20the%20proposed%20framework%20using%20a%20publicly%20available%20robotics%0Afault%20diagnosis%20dataset%2C%20which%20includes%203%2C600%20sequences%20generated%20by%20a%20digital%0Atwin%20model%20and%2090%20real%20sequences%20collected%20from%20physical%20systems.%20The%20DANN%0Amethod%20is%20compared%20with%20commonly%20used%20lightweight%20deep%20learning%20models%20such%20as%0ACNN%2C%20TCN%2C%20Transformer%2C%20and%20LSTM.%20Experimental%20results%20show%20that%20incorporating%0Adomain%20adaptation%20significantly%20improves%20the%20diagnostic%20performance.%20For%0Aexample%2C%20applying%20DANN%20to%20a%20baseline%20CNN%20model%20improves%20its%20accuracy%20from%0A70.00%25%20to%2080.22%25%20on%20real-world%20test%20data%2C%20demonstrating%20the%20effectiveness%20of%0Adomain%20adaptation%20in%20bridging%20the%20sim-to-real%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21046v1&entry.124074799=Read"},
{"title": "MV-CoLight: Efficient Object Compositing with Consistent Lighting and\n  Shadow Generation", "author": "Kerui Ren and Jiayang Bai and Linning Xu and Lihan Jiang and Jiangmiao Pang and Mulin Yu and Bo Dai", "abstract": "  Object compositing offers significant promise for augmented reality (AR) and\nembodied intelligence applications. Existing approaches predominantly focus on\nsingle-image scenarios or intrinsic decomposition techniques, facing challenges\nwith multi-view consistency, complex scenes, and diverse lighting conditions.\nRecent inverse rendering advancements, such as 3D Gaussian and diffusion-based\nmethods, have enhanced consistency but are limited by scalability, heavy data\nrequirements, or prolonged reconstruction time per scene. To broaden its\napplicability, we introduce MV-CoLight, a two-stage framework for\nillumination-consistent object compositing in both 2D images and 3D scenes. Our\nnovel feed-forward architecture models lighting and shadows directly, avoiding\nthe iterative biases of diffusion-based methods. We employ a Hilbert\ncurve-based mapping to align 2D image inputs with 3D Gaussian scene\nrepresentations seamlessly. To facilitate training and evaluation, we further\nintroduce a large-scale 3D compositing dataset. Experiments demonstrate\nstate-of-the-art harmonized results across standard benchmarks and our dataset,\nas well as casually captured real-world scenes demonstrate the framework's\nrobustness and wide generalization.\n", "link": "http://arxiv.org/abs/2505.21483v1", "date": "2025-05-27", "relevancy": 2.4578, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.7006}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5972}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-CoLight%3A%20Efficient%20Object%20Compositing%20with%20Consistent%20Lighting%20and%0A%20%20Shadow%20Generation&body=Title%3A%20MV-CoLight%3A%20Efficient%20Object%20Compositing%20with%20Consistent%20Lighting%20and%0A%20%20Shadow%20Generation%0AAuthor%3A%20Kerui%20Ren%20and%20Jiayang%20Bai%20and%20Linning%20Xu%20and%20Lihan%20Jiang%20and%20Jiangmiao%20Pang%20and%20Mulin%20Yu%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Object%20compositing%20offers%20significant%20promise%20for%20augmented%20reality%20%28AR%29%20and%0Aembodied%20intelligence%20applications.%20Existing%20approaches%20predominantly%20focus%20on%0Asingle-image%20scenarios%20or%20intrinsic%20decomposition%20techniques%2C%20facing%20challenges%0Awith%20multi-view%20consistency%2C%20complex%20scenes%2C%20and%20diverse%20lighting%20conditions.%0ARecent%20inverse%20rendering%20advancements%2C%20such%20as%203D%20Gaussian%20and%20diffusion-based%0Amethods%2C%20have%20enhanced%20consistency%20but%20are%20limited%20by%20scalability%2C%20heavy%20data%0Arequirements%2C%20or%20prolonged%20reconstruction%20time%20per%20scene.%20To%20broaden%20its%0Aapplicability%2C%20we%20introduce%20MV-CoLight%2C%20a%20two-stage%20framework%20for%0Aillumination-consistent%20object%20compositing%20in%20both%202D%20images%20and%203D%20scenes.%20Our%0Anovel%20feed-forward%20architecture%20models%20lighting%20and%20shadows%20directly%2C%20avoiding%0Athe%20iterative%20biases%20of%20diffusion-based%20methods.%20We%20employ%20a%20Hilbert%0Acurve-based%20mapping%20to%20align%202D%20image%20inputs%20with%203D%20Gaussian%20scene%0Arepresentations%20seamlessly.%20To%20facilitate%20training%20and%20evaluation%2C%20we%20further%0Aintroduce%20a%20large-scale%203D%20compositing%20dataset.%20Experiments%20demonstrate%0Astate-of-the-art%20harmonized%20results%20across%20standard%20benchmarks%20and%20our%20dataset%2C%0Aas%20well%20as%20casually%20captured%20real-world%20scenes%20demonstrate%20the%20framework%27s%0Arobustness%20and%20wide%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-CoLight%253A%2520Efficient%2520Object%2520Compositing%2520with%2520Consistent%2520Lighting%2520and%250A%2520%2520Shadow%2520Generation%26entry.906535625%3DKerui%2520Ren%2520and%2520Jiayang%2520Bai%2520and%2520Linning%2520Xu%2520and%2520Lihan%2520Jiang%2520and%2520Jiangmiao%2520Pang%2520and%2520Mulin%2520Yu%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520Object%2520compositing%2520offers%2520significant%2520promise%2520for%2520augmented%2520reality%2520%2528AR%2529%2520and%250Aembodied%2520intelligence%2520applications.%2520Existing%2520approaches%2520predominantly%2520focus%2520on%250Asingle-image%2520scenarios%2520or%2520intrinsic%2520decomposition%2520techniques%252C%2520facing%2520challenges%250Awith%2520multi-view%2520consistency%252C%2520complex%2520scenes%252C%2520and%2520diverse%2520lighting%2520conditions.%250ARecent%2520inverse%2520rendering%2520advancements%252C%2520such%2520as%25203D%2520Gaussian%2520and%2520diffusion-based%250Amethods%252C%2520have%2520enhanced%2520consistency%2520but%2520are%2520limited%2520by%2520scalability%252C%2520heavy%2520data%250Arequirements%252C%2520or%2520prolonged%2520reconstruction%2520time%2520per%2520scene.%2520To%2520broaden%2520its%250Aapplicability%252C%2520we%2520introduce%2520MV-CoLight%252C%2520a%2520two-stage%2520framework%2520for%250Aillumination-consistent%2520object%2520compositing%2520in%2520both%25202D%2520images%2520and%25203D%2520scenes.%2520Our%250Anovel%2520feed-forward%2520architecture%2520models%2520lighting%2520and%2520shadows%2520directly%252C%2520avoiding%250Athe%2520iterative%2520biases%2520of%2520diffusion-based%2520methods.%2520We%2520employ%2520a%2520Hilbert%250Acurve-based%2520mapping%2520to%2520align%25202D%2520image%2520inputs%2520with%25203D%2520Gaussian%2520scene%250Arepresentations%2520seamlessly.%2520To%2520facilitate%2520training%2520and%2520evaluation%252C%2520we%2520further%250Aintroduce%2520a%2520large-scale%25203D%2520compositing%2520dataset.%2520Experiments%2520demonstrate%250Astate-of-the-art%2520harmonized%2520results%2520across%2520standard%2520benchmarks%2520and%2520our%2520dataset%252C%250Aas%2520well%2520as%2520casually%2520captured%2520real-world%2520scenes%2520demonstrate%2520the%2520framework%2527s%250Arobustness%2520and%2520wide%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-CoLight%3A%20Efficient%20Object%20Compositing%20with%20Consistent%20Lighting%20and%0A%20%20Shadow%20Generation&entry.906535625=Kerui%20Ren%20and%20Jiayang%20Bai%20and%20Linning%20Xu%20and%20Lihan%20Jiang%20and%20Jiangmiao%20Pang%20and%20Mulin%20Yu%20and%20Bo%20Dai&entry.1292438233=%20%20Object%20compositing%20offers%20significant%20promise%20for%20augmented%20reality%20%28AR%29%20and%0Aembodied%20intelligence%20applications.%20Existing%20approaches%20predominantly%20focus%20on%0Asingle-image%20scenarios%20or%20intrinsic%20decomposition%20techniques%2C%20facing%20challenges%0Awith%20multi-view%20consistency%2C%20complex%20scenes%2C%20and%20diverse%20lighting%20conditions.%0ARecent%20inverse%20rendering%20advancements%2C%20such%20as%203D%20Gaussian%20and%20diffusion-based%0Amethods%2C%20have%20enhanced%20consistency%20but%20are%20limited%20by%20scalability%2C%20heavy%20data%0Arequirements%2C%20or%20prolonged%20reconstruction%20time%20per%20scene.%20To%20broaden%20its%0Aapplicability%2C%20we%20introduce%20MV-CoLight%2C%20a%20two-stage%20framework%20for%0Aillumination-consistent%20object%20compositing%20in%20both%202D%20images%20and%203D%20scenes.%20Our%0Anovel%20feed-forward%20architecture%20models%20lighting%20and%20shadows%20directly%2C%20avoiding%0Athe%20iterative%20biases%20of%20diffusion-based%20methods.%20We%20employ%20a%20Hilbert%0Acurve-based%20mapping%20to%20align%202D%20image%20inputs%20with%203D%20Gaussian%20scene%0Arepresentations%20seamlessly.%20To%20facilitate%20training%20and%20evaluation%2C%20we%20further%0Aintroduce%20a%20large-scale%203D%20compositing%20dataset.%20Experiments%20demonstrate%0Astate-of-the-art%20harmonized%20results%20across%20standard%20benchmarks%20and%20our%20dataset%2C%0Aas%20well%20as%20casually%20captured%20real-world%20scenes%20demonstrate%20the%20framework%27s%0Arobustness%20and%20wide%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21483v1&entry.124074799=Read"},
{"title": "DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic\n  City Understanding", "author": "Weihao Xuan and Junjue Wang and Heli Qi and Zihang Chen and Zhuo Zheng and Yanfei Zhong and Junshi Xia and Naoto Yokoya", "abstract": "  Multimodal large language models have demonstrated remarkable capabilities in\nvisual understanding, but their application to long-term Earth observation\nanalysis remains limited, primarily focusing on single-temporal or bi-temporal\nimagery. To address this gap, we introduce DVL-Suite, a comprehensive framework\nfor analyzing long-term urban dynamics through remote sensing imagery. Our\nsuite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42\nmegacities in the U.S. from 2005 to 2023, organized into two components:\nDVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding\ntasks, from fundamental change detection (pixel-level) to quantitative analyses\n(regional-level) and comprehensive urban narratives (scene-level), capturing\ndiverse urban dynamics including expansion/transformation patterns, disaster\nassessment, and environmental challenges. We evaluate 17 state-of-the-art\nmultimodal large language models and reveal their limitations in long-term\ntemporal understanding and quantitative analysis. These challenges motivate the\ncreation of DVL-Instruct, a specialized instruction-tuning dataset designed to\nenhance models' capabilities in multi-temporal Earth observation. Building upon\nthis dataset, we develop DVLChat, a baseline model capable of both image-level\nquestion-answering and pixel-level segmentation, facilitating a comprehensive\nunderstanding of city dynamics through language interactions.\n", "link": "http://arxiv.org/abs/2505.21076v1", "date": "2025-05-27", "relevancy": 2.4484, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.62}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.62}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicVL%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Dynamic%0A%20%20City%20Understanding&body=Title%3A%20DynamicVL%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Dynamic%0A%20%20City%20Understanding%0AAuthor%3A%20Weihao%20Xuan%20and%20Junjue%20Wang%20and%20Heli%20Qi%20and%20Zihang%20Chen%20and%20Zhuo%20Zheng%20and%20Yanfei%20Zhong%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20have%20demonstrated%20remarkable%20capabilities%20in%0Avisual%20understanding%2C%20but%20their%20application%20to%20long-term%20Earth%20observation%0Aanalysis%20remains%20limited%2C%20primarily%20focusing%20on%20single-temporal%20or%20bi-temporal%0Aimagery.%20To%20address%20this%20gap%2C%20we%20introduce%20DVL-Suite%2C%20a%20comprehensive%20framework%0Afor%20analyzing%20long-term%20urban%20dynamics%20through%20remote%20sensing%20imagery.%20Our%0Asuite%20comprises%2015%2C063%20high-resolution%20%281.0m%29%20multi-temporal%20images%20spanning%2042%0Amegacities%20in%20the%20U.S.%20from%202005%20to%202023%2C%20organized%20into%20two%20components%3A%0ADVL-Bench%20and%20DVL-Instruct.%20The%20DVL-Bench%20includes%20seven%20urban%20understanding%0Atasks%2C%20from%20fundamental%20change%20detection%20%28pixel-level%29%20to%20quantitative%20analyses%0A%28regional-level%29%20and%20comprehensive%20urban%20narratives%20%28scene-level%29%2C%20capturing%0Adiverse%20urban%20dynamics%20including%20expansion/transformation%20patterns%2C%20disaster%0Aassessment%2C%20and%20environmental%20challenges.%20We%20evaluate%2017%20state-of-the-art%0Amultimodal%20large%20language%20models%20and%20reveal%20their%20limitations%20in%20long-term%0Atemporal%20understanding%20and%20quantitative%20analysis.%20These%20challenges%20motivate%20the%0Acreation%20of%20DVL-Instruct%2C%20a%20specialized%20instruction-tuning%20dataset%20designed%20to%0Aenhance%20models%27%20capabilities%20in%20multi-temporal%20Earth%20observation.%20Building%20upon%0Athis%20dataset%2C%20we%20develop%20DVLChat%2C%20a%20baseline%20model%20capable%20of%20both%20image-level%0Aquestion-answering%20and%20pixel-level%20segmentation%2C%20facilitating%20a%20comprehensive%0Aunderstanding%20of%20city%20dynamics%20through%20language%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicVL%253A%2520Benchmarking%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520Dynamic%250A%2520%2520City%2520Understanding%26entry.906535625%3DWeihao%2520Xuan%2520and%2520Junjue%2520Wang%2520and%2520Heli%2520Qi%2520and%2520Zihang%2520Chen%2520and%2520Zhuo%2520Zheng%2520and%2520Yanfei%2520Zhong%2520and%2520Junshi%2520Xia%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Avisual%2520understanding%252C%2520but%2520their%2520application%2520to%2520long-term%2520Earth%2520observation%250Aanalysis%2520remains%2520limited%252C%2520primarily%2520focusing%2520on%2520single-temporal%2520or%2520bi-temporal%250Aimagery.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520DVL-Suite%252C%2520a%2520comprehensive%2520framework%250Afor%2520analyzing%2520long-term%2520urban%2520dynamics%2520through%2520remote%2520sensing%2520imagery.%2520Our%250Asuite%2520comprises%252015%252C063%2520high-resolution%2520%25281.0m%2529%2520multi-temporal%2520images%2520spanning%252042%250Amegacities%2520in%2520the%2520U.S.%2520from%25202005%2520to%25202023%252C%2520organized%2520into%2520two%2520components%253A%250ADVL-Bench%2520and%2520DVL-Instruct.%2520The%2520DVL-Bench%2520includes%2520seven%2520urban%2520understanding%250Atasks%252C%2520from%2520fundamental%2520change%2520detection%2520%2528pixel-level%2529%2520to%2520quantitative%2520analyses%250A%2528regional-level%2529%2520and%2520comprehensive%2520urban%2520narratives%2520%2528scene-level%2529%252C%2520capturing%250Adiverse%2520urban%2520dynamics%2520including%2520expansion/transformation%2520patterns%252C%2520disaster%250Aassessment%252C%2520and%2520environmental%2520challenges.%2520We%2520evaluate%252017%2520state-of-the-art%250Amultimodal%2520large%2520language%2520models%2520and%2520reveal%2520their%2520limitations%2520in%2520long-term%250Atemporal%2520understanding%2520and%2520quantitative%2520analysis.%2520These%2520challenges%2520motivate%2520the%250Acreation%2520of%2520DVL-Instruct%252C%2520a%2520specialized%2520instruction-tuning%2520dataset%2520designed%2520to%250Aenhance%2520models%2527%2520capabilities%2520in%2520multi-temporal%2520Earth%2520observation.%2520Building%2520upon%250Athis%2520dataset%252C%2520we%2520develop%2520DVLChat%252C%2520a%2520baseline%2520model%2520capable%2520of%2520both%2520image-level%250Aquestion-answering%2520and%2520pixel-level%2520segmentation%252C%2520facilitating%2520a%2520comprehensive%250Aunderstanding%2520of%2520city%2520dynamics%2520through%2520language%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicVL%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Dynamic%0A%20%20City%20Understanding&entry.906535625=Weihao%20Xuan%20and%20Junjue%20Wang%20and%20Heli%20Qi%20and%20Zihang%20Chen%20and%20Zhuo%20Zheng%20and%20Yanfei%20Zhong%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Multimodal%20large%20language%20models%20have%20demonstrated%20remarkable%20capabilities%20in%0Avisual%20understanding%2C%20but%20their%20application%20to%20long-term%20Earth%20observation%0Aanalysis%20remains%20limited%2C%20primarily%20focusing%20on%20single-temporal%20or%20bi-temporal%0Aimagery.%20To%20address%20this%20gap%2C%20we%20introduce%20DVL-Suite%2C%20a%20comprehensive%20framework%0Afor%20analyzing%20long-term%20urban%20dynamics%20through%20remote%20sensing%20imagery.%20Our%0Asuite%20comprises%2015%2C063%20high-resolution%20%281.0m%29%20multi-temporal%20images%20spanning%2042%0Amegacities%20in%20the%20U.S.%20from%202005%20to%202023%2C%20organized%20into%20two%20components%3A%0ADVL-Bench%20and%20DVL-Instruct.%20The%20DVL-Bench%20includes%20seven%20urban%20understanding%0Atasks%2C%20from%20fundamental%20change%20detection%20%28pixel-level%29%20to%20quantitative%20analyses%0A%28regional-level%29%20and%20comprehensive%20urban%20narratives%20%28scene-level%29%2C%20capturing%0Adiverse%20urban%20dynamics%20including%20expansion/transformation%20patterns%2C%20disaster%0Aassessment%2C%20and%20environmental%20challenges.%20We%20evaluate%2017%20state-of-the-art%0Amultimodal%20large%20language%20models%20and%20reveal%20their%20limitations%20in%20long-term%0Atemporal%20understanding%20and%20quantitative%20analysis.%20These%20challenges%20motivate%20the%0Acreation%20of%20DVL-Instruct%2C%20a%20specialized%20instruction-tuning%20dataset%20designed%20to%0Aenhance%20models%27%20capabilities%20in%20multi-temporal%20Earth%20observation.%20Building%20upon%0Athis%20dataset%2C%20we%20develop%20DVLChat%2C%20a%20baseline%20model%20capable%20of%20both%20image-level%0Aquestion-answering%20and%20pixel-level%20segmentation%2C%20facilitating%20a%20comprehensive%0Aunderstanding%20of%20city%20dynamics%20through%20language%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21076v1&entry.124074799=Read"},
{"title": "Reinforcing General Reasoning without Verifiers", "author": "Xiangxin Zhou and Zichen Liu and Anya Sims and Haonan Wang and Tianyu Pang and Chongxuan Li and Liang Wang and Min Lin and Chao Du", "abstract": "  The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.\n", "link": "http://arxiv.org/abs/2505.21493v1", "date": "2025-05-27", "relevancy": 2.4421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcing%20General%20Reasoning%20without%20Verifiers&body=Title%3A%20Reinforcing%20General%20Reasoning%20without%20Verifiers%0AAuthor%3A%20Xiangxin%20Zhou%20and%20Zichen%20Liu%20and%20Anya%20Sims%20and%20Haonan%20Wang%20and%20Tianyu%20Pang%20and%20Chongxuan%20Li%20and%20Liang%20Wang%20and%20Min%20Lin%20and%20Chao%20Du%0AAbstract%3A%20%20%20The%20recent%20paradigm%20shift%20towards%20training%20large%20language%20models%20%28LLMs%29%20using%0ADeepSeek-R1-Zero-style%20reinforcement%20learning%20%28RL%29%20on%20verifiable%20rewards%20has%0Aled%20to%20impressive%20advancements%20in%20code%20and%20mathematical%20reasoning.%20However%2C%0Athis%20methodology%20is%20limited%20to%20tasks%20where%20rule-based%20answer%20verification%20is%0Apossible%20and%20does%20not%20naturally%20extend%20to%20real-world%20domains%20such%20as%20chemistry%2C%0Ahealthcare%2C%20engineering%2C%20law%2C%20biology%2C%20business%2C%20and%20economics.%20Current%0Apractical%20workarounds%20use%20an%20additional%20LLM%20as%20a%20model-based%20verifier%3B%20however%2C%0Athis%20introduces%20issues%20such%20as%20reliance%20on%20a%20strong%20verifier%20LLM%2C%0Asusceptibility%20to%20reward%20hacking%2C%20and%20the%20practical%20burden%20of%20maintaining%20the%0Averifier%20model%20in%20memory%20during%20training.%20To%20address%20this%20and%20extend%0ADeepSeek-R1-Zero-style%20training%20to%20general%20reasoning%20domains%2C%20we%20propose%20a%0Averifier-free%20method%20%28VeriFree%29%20that%20bypasses%20answer%20verification%20and%20instead%0Auses%20RL%20to%20directly%20maximize%20the%20probability%20of%20generating%20the%20reference%0Aanswer.%20We%20compare%20VeriFree%20with%20verifier-based%20methods%20and%20demonstrate%20that%2C%0Ain%20addition%20to%20its%20significant%20practical%20benefits%20and%20reduced%20compute%0Arequirements%2C%20VeriFree%20matches%20and%20even%20surpasses%20verifier-based%20methods%20on%0Aextensive%20evaluations%20across%20MMLU-Pro%2C%20GPQA%2C%20SuperGPQA%2C%20and%20math-related%0Abenchmarks.%20Moreover%2C%20we%20provide%20insights%20into%20this%20method%20from%20multiple%0Aperspectives%3A%20as%20an%20elegant%20integration%20of%20training%20both%20the%20policy%20and%0Aimplicit%20verifier%20in%20a%20unified%20model%2C%20and%20as%20a%20variational%20optimization%0Aapproach.%20Code%20is%20available%20at%20https%3A//github.com/sail-sg/VeriFree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcing%2520General%2520Reasoning%2520without%2520Verifiers%26entry.906535625%3DXiangxin%2520Zhou%2520and%2520Zichen%2520Liu%2520and%2520Anya%2520Sims%2520and%2520Haonan%2520Wang%2520and%2520Tianyu%2520Pang%2520and%2520Chongxuan%2520Li%2520and%2520Liang%2520Wang%2520and%2520Min%2520Lin%2520and%2520Chao%2520Du%26entry.1292438233%3D%2520%2520The%2520recent%2520paradigm%2520shift%2520towards%2520training%2520large%2520language%2520models%2520%2528LLMs%2529%2520using%250ADeepSeek-R1-Zero-style%2520reinforcement%2520learning%2520%2528RL%2529%2520on%2520verifiable%2520rewards%2520has%250Aled%2520to%2520impressive%2520advancements%2520in%2520code%2520and%2520mathematical%2520reasoning.%2520However%252C%250Athis%2520methodology%2520is%2520limited%2520to%2520tasks%2520where%2520rule-based%2520answer%2520verification%2520is%250Apossible%2520and%2520does%2520not%2520naturally%2520extend%2520to%2520real-world%2520domains%2520such%2520as%2520chemistry%252C%250Ahealthcare%252C%2520engineering%252C%2520law%252C%2520biology%252C%2520business%252C%2520and%2520economics.%2520Current%250Apractical%2520workarounds%2520use%2520an%2520additional%2520LLM%2520as%2520a%2520model-based%2520verifier%253B%2520however%252C%250Athis%2520introduces%2520issues%2520such%2520as%2520reliance%2520on%2520a%2520strong%2520verifier%2520LLM%252C%250Asusceptibility%2520to%2520reward%2520hacking%252C%2520and%2520the%2520practical%2520burden%2520of%2520maintaining%2520the%250Averifier%2520model%2520in%2520memory%2520during%2520training.%2520To%2520address%2520this%2520and%2520extend%250ADeepSeek-R1-Zero-style%2520training%2520to%2520general%2520reasoning%2520domains%252C%2520we%2520propose%2520a%250Averifier-free%2520method%2520%2528VeriFree%2529%2520that%2520bypasses%2520answer%2520verification%2520and%2520instead%250Auses%2520RL%2520to%2520directly%2520maximize%2520the%2520probability%2520of%2520generating%2520the%2520reference%250Aanswer.%2520We%2520compare%2520VeriFree%2520with%2520verifier-based%2520methods%2520and%2520demonstrate%2520that%252C%250Ain%2520addition%2520to%2520its%2520significant%2520practical%2520benefits%2520and%2520reduced%2520compute%250Arequirements%252C%2520VeriFree%2520matches%2520and%2520even%2520surpasses%2520verifier-based%2520methods%2520on%250Aextensive%2520evaluations%2520across%2520MMLU-Pro%252C%2520GPQA%252C%2520SuperGPQA%252C%2520and%2520math-related%250Abenchmarks.%2520Moreover%252C%2520we%2520provide%2520insights%2520into%2520this%2520method%2520from%2520multiple%250Aperspectives%253A%2520as%2520an%2520elegant%2520integration%2520of%2520training%2520both%2520the%2520policy%2520and%250Aimplicit%2520verifier%2520in%2520a%2520unified%2520model%252C%2520and%2520as%2520a%2520variational%2520optimization%250Aapproach.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/sail-sg/VeriFree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcing%20General%20Reasoning%20without%20Verifiers&entry.906535625=Xiangxin%20Zhou%20and%20Zichen%20Liu%20and%20Anya%20Sims%20and%20Haonan%20Wang%20and%20Tianyu%20Pang%20and%20Chongxuan%20Li%20and%20Liang%20Wang%20and%20Min%20Lin%20and%20Chao%20Du&entry.1292438233=%20%20The%20recent%20paradigm%20shift%20towards%20training%20large%20language%20models%20%28LLMs%29%20using%0ADeepSeek-R1-Zero-style%20reinforcement%20learning%20%28RL%29%20on%20verifiable%20rewards%20has%0Aled%20to%20impressive%20advancements%20in%20code%20and%20mathematical%20reasoning.%20However%2C%0Athis%20methodology%20is%20limited%20to%20tasks%20where%20rule-based%20answer%20verification%20is%0Apossible%20and%20does%20not%20naturally%20extend%20to%20real-world%20domains%20such%20as%20chemistry%2C%0Ahealthcare%2C%20engineering%2C%20law%2C%20biology%2C%20business%2C%20and%20economics.%20Current%0Apractical%20workarounds%20use%20an%20additional%20LLM%20as%20a%20model-based%20verifier%3B%20however%2C%0Athis%20introduces%20issues%20such%20as%20reliance%20on%20a%20strong%20verifier%20LLM%2C%0Asusceptibility%20to%20reward%20hacking%2C%20and%20the%20practical%20burden%20of%20maintaining%20the%0Averifier%20model%20in%20memory%20during%20training.%20To%20address%20this%20and%20extend%0ADeepSeek-R1-Zero-style%20training%20to%20general%20reasoning%20domains%2C%20we%20propose%20a%0Averifier-free%20method%20%28VeriFree%29%20that%20bypasses%20answer%20verification%20and%20instead%0Auses%20RL%20to%20directly%20maximize%20the%20probability%20of%20generating%20the%20reference%0Aanswer.%20We%20compare%20VeriFree%20with%20verifier-based%20methods%20and%20demonstrate%20that%2C%0Ain%20addition%20to%20its%20significant%20practical%20benefits%20and%20reduced%20compute%0Arequirements%2C%20VeriFree%20matches%20and%20even%20surpasses%20verifier-based%20methods%20on%0Aextensive%20evaluations%20across%20MMLU-Pro%2C%20GPQA%2C%20SuperGPQA%2C%20and%20math-related%0Abenchmarks.%20Moreover%2C%20we%20provide%20insights%20into%20this%20method%20from%20multiple%0Aperspectives%3A%20as%20an%20elegant%20integration%20of%20training%20both%20the%20policy%20and%0Aimplicit%20verifier%20in%20a%20unified%20model%2C%20and%20as%20a%20variational%20optimization%0Aapproach.%20Code%20is%20available%20at%20https%3A//github.com/sail-sg/VeriFree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21493v1&entry.124074799=Read"},
{"title": "Learnable Kernel Density Estimation for Graphs", "author": "Xudong Wang and Ziheng Sun and Chris Ding and Jicong Fan", "abstract": "  This work proposes a framework LGKDE that learns kernel density estimation\nfor graphs. The key challenge in graph density estimation lies in effectively\ncapturing both structural patterns and semantic variations while maintaining\ntheoretical guarantees. Combining graph kernels and kernel density estimation\n(KDE) is a standard approach to graph density estimation, but has\nunsatisfactory performance due to the handcrafted and fixed features of\nkernels. Our method LGKDE leverages graph neural networks to represent each\ngraph as a discrete distribution and utilizes maximum mean discrepancy to learn\nthe graph metric for multi-scale KDE, where all parameters are learned by\nmaximizing the density of graphs relative to the density of their well-designed\nperturbed counterparts. The perturbations are conducted on both node features\nand graph spectra, which helps better characterize the boundary of normal\ndensity regions. Theoretically, we establish consistency and convergence\nguarantees for LGKDE, including bounds on the mean integrated squared error,\nrobustness, and complexity. We validate LGKDE by demonstrating its\neffectiveness in recovering the underlying density of synthetic graph\ndistributions and applying it to graph anomaly detection across diverse\nbenchmark datasets. Extensive empirical evaluation shows that LGKDE\ndemonstrates superior performance compared to state-of-the-art baselines on\nmost benchmark datasets.\n", "link": "http://arxiv.org/abs/2505.21285v1", "date": "2025-05-27", "relevancy": 2.4349, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5156}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4756}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Kernel%20Density%20Estimation%20for%20Graphs&body=Title%3A%20Learnable%20Kernel%20Density%20Estimation%20for%20Graphs%0AAuthor%3A%20Xudong%20Wang%20and%20Ziheng%20Sun%20and%20Chris%20Ding%20and%20Jicong%20Fan%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20framework%20LGKDE%20that%20learns%20kernel%20density%20estimation%0Afor%20graphs.%20The%20key%20challenge%20in%20graph%20density%20estimation%20lies%20in%20effectively%0Acapturing%20both%20structural%20patterns%20and%20semantic%20variations%20while%20maintaining%0Atheoretical%20guarantees.%20Combining%20graph%20kernels%20and%20kernel%20density%20estimation%0A%28KDE%29%20is%20a%20standard%20approach%20to%20graph%20density%20estimation%2C%20but%20has%0Aunsatisfactory%20performance%20due%20to%20the%20handcrafted%20and%20fixed%20features%20of%0Akernels.%20Our%20method%20LGKDE%20leverages%20graph%20neural%20networks%20to%20represent%20each%0Agraph%20as%20a%20discrete%20distribution%20and%20utilizes%20maximum%20mean%20discrepancy%20to%20learn%0Athe%20graph%20metric%20for%20multi-scale%20KDE%2C%20where%20all%20parameters%20are%20learned%20by%0Amaximizing%20the%20density%20of%20graphs%20relative%20to%20the%20density%20of%20their%20well-designed%0Aperturbed%20counterparts.%20The%20perturbations%20are%20conducted%20on%20both%20node%20features%0Aand%20graph%20spectra%2C%20which%20helps%20better%20characterize%20the%20boundary%20of%20normal%0Adensity%20regions.%20Theoretically%2C%20we%20establish%20consistency%20and%20convergence%0Aguarantees%20for%20LGKDE%2C%20including%20bounds%20on%20the%20mean%20integrated%20squared%20error%2C%0Arobustness%2C%20and%20complexity.%20We%20validate%20LGKDE%20by%20demonstrating%20its%0Aeffectiveness%20in%20recovering%20the%20underlying%20density%20of%20synthetic%20graph%0Adistributions%20and%20applying%20it%20to%20graph%20anomaly%20detection%20across%20diverse%0Abenchmark%20datasets.%20Extensive%20empirical%20evaluation%20shows%20that%20LGKDE%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Amost%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Kernel%2520Density%2520Estimation%2520for%2520Graphs%26entry.906535625%3DXudong%2520Wang%2520and%2520Ziheng%2520Sun%2520and%2520Chris%2520Ding%2520and%2520Jicong%2520Fan%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520framework%2520LGKDE%2520that%2520learns%2520kernel%2520density%2520estimation%250Afor%2520graphs.%2520The%2520key%2520challenge%2520in%2520graph%2520density%2520estimation%2520lies%2520in%2520effectively%250Acapturing%2520both%2520structural%2520patterns%2520and%2520semantic%2520variations%2520while%2520maintaining%250Atheoretical%2520guarantees.%2520Combining%2520graph%2520kernels%2520and%2520kernel%2520density%2520estimation%250A%2528KDE%2529%2520is%2520a%2520standard%2520approach%2520to%2520graph%2520density%2520estimation%252C%2520but%2520has%250Aunsatisfactory%2520performance%2520due%2520to%2520the%2520handcrafted%2520and%2520fixed%2520features%2520of%250Akernels.%2520Our%2520method%2520LGKDE%2520leverages%2520graph%2520neural%2520networks%2520to%2520represent%2520each%250Agraph%2520as%2520a%2520discrete%2520distribution%2520and%2520utilizes%2520maximum%2520mean%2520discrepancy%2520to%2520learn%250Athe%2520graph%2520metric%2520for%2520multi-scale%2520KDE%252C%2520where%2520all%2520parameters%2520are%2520learned%2520by%250Amaximizing%2520the%2520density%2520of%2520graphs%2520relative%2520to%2520the%2520density%2520of%2520their%2520well-designed%250Aperturbed%2520counterparts.%2520The%2520perturbations%2520are%2520conducted%2520on%2520both%2520node%2520features%250Aand%2520graph%2520spectra%252C%2520which%2520helps%2520better%2520characterize%2520the%2520boundary%2520of%2520normal%250Adensity%2520regions.%2520Theoretically%252C%2520we%2520establish%2520consistency%2520and%2520convergence%250Aguarantees%2520for%2520LGKDE%252C%2520including%2520bounds%2520on%2520the%2520mean%2520integrated%2520squared%2520error%252C%250Arobustness%252C%2520and%2520complexity.%2520We%2520validate%2520LGKDE%2520by%2520demonstrating%2520its%250Aeffectiveness%2520in%2520recovering%2520the%2520underlying%2520density%2520of%2520synthetic%2520graph%250Adistributions%2520and%2520applying%2520it%2520to%2520graph%2520anomaly%2520detection%2520across%2520diverse%250Abenchmark%2520datasets.%2520Extensive%2520empirical%2520evaluation%2520shows%2520that%2520LGKDE%250Ademonstrates%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520baselines%2520on%250Amost%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Kernel%20Density%20Estimation%20for%20Graphs&entry.906535625=Xudong%20Wang%20and%20Ziheng%20Sun%20and%20Chris%20Ding%20and%20Jicong%20Fan&entry.1292438233=%20%20This%20work%20proposes%20a%20framework%20LGKDE%20that%20learns%20kernel%20density%20estimation%0Afor%20graphs.%20The%20key%20challenge%20in%20graph%20density%20estimation%20lies%20in%20effectively%0Acapturing%20both%20structural%20patterns%20and%20semantic%20variations%20while%20maintaining%0Atheoretical%20guarantees.%20Combining%20graph%20kernels%20and%20kernel%20density%20estimation%0A%28KDE%29%20is%20a%20standard%20approach%20to%20graph%20density%20estimation%2C%20but%20has%0Aunsatisfactory%20performance%20due%20to%20the%20handcrafted%20and%20fixed%20features%20of%0Akernels.%20Our%20method%20LGKDE%20leverages%20graph%20neural%20networks%20to%20represent%20each%0Agraph%20as%20a%20discrete%20distribution%20and%20utilizes%20maximum%20mean%20discrepancy%20to%20learn%0Athe%20graph%20metric%20for%20multi-scale%20KDE%2C%20where%20all%20parameters%20are%20learned%20by%0Amaximizing%20the%20density%20of%20graphs%20relative%20to%20the%20density%20of%20their%20well-designed%0Aperturbed%20counterparts.%20The%20perturbations%20are%20conducted%20on%20both%20node%20features%0Aand%20graph%20spectra%2C%20which%20helps%20better%20characterize%20the%20boundary%20of%20normal%0Adensity%20regions.%20Theoretically%2C%20we%20establish%20consistency%20and%20convergence%0Aguarantees%20for%20LGKDE%2C%20including%20bounds%20on%20the%20mean%20integrated%20squared%20error%2C%0Arobustness%2C%20and%20complexity.%20We%20validate%20LGKDE%20by%20demonstrating%20its%0Aeffectiveness%20in%20recovering%20the%20underlying%20density%20of%20synthetic%20graph%0Adistributions%20and%20applying%20it%20to%20graph%20anomaly%20detection%20across%20diverse%0Abenchmark%20datasets.%20Extensive%20empirical%20evaluation%20shows%20that%20LGKDE%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Amost%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21285v1&entry.124074799=Read"},
{"title": "efunc: An Efficient Function Representation without Neural Networks", "author": "Biao Zhang and Peter Wonka", "abstract": "  Function fitting/approximation plays a fundamental role in computer graphics\nand other engineering applications. While recent advances have explored neural\nnetworks to address this task, these methods often rely on architectures with\nmany parameters, limiting their practical applicability. In contrast, we pursue\nhigh-quality function approximation using parameter-efficient representations\nthat eliminate the dependency on neural networks entirely. We first propose a\nnovel framework for continuous function modeling. Most existing works can be\nformulated using this framework. We then introduce a compact function\nrepresentation, which is based on polynomials interpolated using radial basis\nfunctions, bypassing both neural networks and complex/hierarchical data\nstructures. We also develop memory-efficient CUDA-optimized algorithms that\nreduce computational time and memory consumption to less than 10% compared to\nconventional automatic differentiation frameworks. Finally, we validate our\nrepresentation and optimization pipeline through extensive experiments on 3D\nsigned distance functions (SDFs). The proposed representation achieves\ncomparable or superior performance to state-of-the-art techniques (e.g.,\noctree/hash-grid techniques) with significantly fewer parameters.\n", "link": "http://arxiv.org/abs/2505.21319v1", "date": "2025-05-27", "relevancy": 2.4316, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.487}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4864}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20efunc%3A%20An%20Efficient%20Function%20Representation%20without%20Neural%20Networks&body=Title%3A%20efunc%3A%20An%20Efficient%20Function%20Representation%20without%20Neural%20Networks%0AAuthor%3A%20Biao%20Zhang%20and%20Peter%20Wonka%0AAbstract%3A%20%20%20Function%20fitting/approximation%20plays%20a%20fundamental%20role%20in%20computer%20graphics%0Aand%20other%20engineering%20applications.%20While%20recent%20advances%20have%20explored%20neural%0Anetworks%20to%20address%20this%20task%2C%20these%20methods%20often%20rely%20on%20architectures%20with%0Amany%20parameters%2C%20limiting%20their%20practical%20applicability.%20In%20contrast%2C%20we%20pursue%0Ahigh-quality%20function%20approximation%20using%20parameter-efficient%20representations%0Athat%20eliminate%20the%20dependency%20on%20neural%20networks%20entirely.%20We%20first%20propose%20a%0Anovel%20framework%20for%20continuous%20function%20modeling.%20Most%20existing%20works%20can%20be%0Aformulated%20using%20this%20framework.%20We%20then%20introduce%20a%20compact%20function%0Arepresentation%2C%20which%20is%20based%20on%20polynomials%20interpolated%20using%20radial%20basis%0Afunctions%2C%20bypassing%20both%20neural%20networks%20and%20complex/hierarchical%20data%0Astructures.%20We%20also%20develop%20memory-efficient%20CUDA-optimized%20algorithms%20that%0Areduce%20computational%20time%20and%20memory%20consumption%20to%20less%20than%2010%25%20compared%20to%0Aconventional%20automatic%20differentiation%20frameworks.%20Finally%2C%20we%20validate%20our%0Arepresentation%20and%20optimization%20pipeline%20through%20extensive%20experiments%20on%203D%0Asigned%20distance%20functions%20%28SDFs%29.%20The%20proposed%20representation%20achieves%0Acomparable%20or%20superior%20performance%20to%20state-of-the-art%20techniques%20%28e.g.%2C%0Aoctree/hash-grid%20techniques%29%20with%20significantly%20fewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Defunc%253A%2520An%2520Efficient%2520Function%2520Representation%2520without%2520Neural%2520Networks%26entry.906535625%3DBiao%2520Zhang%2520and%2520Peter%2520Wonka%26entry.1292438233%3D%2520%2520Function%2520fitting/approximation%2520plays%2520a%2520fundamental%2520role%2520in%2520computer%2520graphics%250Aand%2520other%2520engineering%2520applications.%2520While%2520recent%2520advances%2520have%2520explored%2520neural%250Anetworks%2520to%2520address%2520this%2520task%252C%2520these%2520methods%2520often%2520rely%2520on%2520architectures%2520with%250Amany%2520parameters%252C%2520limiting%2520their%2520practical%2520applicability.%2520In%2520contrast%252C%2520we%2520pursue%250Ahigh-quality%2520function%2520approximation%2520using%2520parameter-efficient%2520representations%250Athat%2520eliminate%2520the%2520dependency%2520on%2520neural%2520networks%2520entirely.%2520We%2520first%2520propose%2520a%250Anovel%2520framework%2520for%2520continuous%2520function%2520modeling.%2520Most%2520existing%2520works%2520can%2520be%250Aformulated%2520using%2520this%2520framework.%2520We%2520then%2520introduce%2520a%2520compact%2520function%250Arepresentation%252C%2520which%2520is%2520based%2520on%2520polynomials%2520interpolated%2520using%2520radial%2520basis%250Afunctions%252C%2520bypassing%2520both%2520neural%2520networks%2520and%2520complex/hierarchical%2520data%250Astructures.%2520We%2520also%2520develop%2520memory-efficient%2520CUDA-optimized%2520algorithms%2520that%250Areduce%2520computational%2520time%2520and%2520memory%2520consumption%2520to%2520less%2520than%252010%2525%2520compared%2520to%250Aconventional%2520automatic%2520differentiation%2520frameworks.%2520Finally%252C%2520we%2520validate%2520our%250Arepresentation%2520and%2520optimization%2520pipeline%2520through%2520extensive%2520experiments%2520on%25203D%250Asigned%2520distance%2520functions%2520%2528SDFs%2529.%2520The%2520proposed%2520representation%2520achieves%250Acomparable%2520or%2520superior%2520performance%2520to%2520state-of-the-art%2520techniques%2520%2528e.g.%252C%250Aoctree/hash-grid%2520techniques%2529%2520with%2520significantly%2520fewer%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=efunc%3A%20An%20Efficient%20Function%20Representation%20without%20Neural%20Networks&entry.906535625=Biao%20Zhang%20and%20Peter%20Wonka&entry.1292438233=%20%20Function%20fitting/approximation%20plays%20a%20fundamental%20role%20in%20computer%20graphics%0Aand%20other%20engineering%20applications.%20While%20recent%20advances%20have%20explored%20neural%0Anetworks%20to%20address%20this%20task%2C%20these%20methods%20often%20rely%20on%20architectures%20with%0Amany%20parameters%2C%20limiting%20their%20practical%20applicability.%20In%20contrast%2C%20we%20pursue%0Ahigh-quality%20function%20approximation%20using%20parameter-efficient%20representations%0Athat%20eliminate%20the%20dependency%20on%20neural%20networks%20entirely.%20We%20first%20propose%20a%0Anovel%20framework%20for%20continuous%20function%20modeling.%20Most%20existing%20works%20can%20be%0Aformulated%20using%20this%20framework.%20We%20then%20introduce%20a%20compact%20function%0Arepresentation%2C%20which%20is%20based%20on%20polynomials%20interpolated%20using%20radial%20basis%0Afunctions%2C%20bypassing%20both%20neural%20networks%20and%20complex/hierarchical%20data%0Astructures.%20We%20also%20develop%20memory-efficient%20CUDA-optimized%20algorithms%20that%0Areduce%20computational%20time%20and%20memory%20consumption%20to%20less%20than%2010%25%20compared%20to%0Aconventional%20automatic%20differentiation%20frameworks.%20Finally%2C%20we%20validate%20our%0Arepresentation%20and%20optimization%20pipeline%20through%20extensive%20experiments%20on%203D%0Asigned%20distance%20functions%20%28SDFs%29.%20The%20proposed%20representation%20achieves%0Acomparable%20or%20superior%20performance%20to%20state-of-the-art%20techniques%20%28e.g.%2C%0Aoctree/hash-grid%20techniques%29%20with%20significantly%20fewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21319v1&entry.124074799=Read"},
{"title": "Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via\n  Multi-modality Mentor Learning", "author": "Jinbao Wang and Hanzhe Liang and Can Gao and Chenxi Hu and Jie Zhou and Yunkang Cao and Linlin Shen and Weiming Shen", "abstract": "  Multimodal feature reconstruction is a promising approach for 3D anomaly\ndetection, leveraging the complementary information from dual modalities. We\nfurther advance this paradigm by utilizing multi-modal mentor learning, which\nfuses intermediate features to further distinguish normal from feature\ndifferences. To address these challenges, we propose a novel method called\nMentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared\nfeatures of different modalities, Mentor3AD can extract more effective features\nand guide feature reconstruction, ultimately improving detection performance.\nSpecifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges\nfeatures extracted from RGB and 3D modalities to create a mentor feature.\nAdditionally, we have designed a Mentor of Guidance Module (MGM) to facilitate\ncross-modal reconstruction, supported by the mentor feature. Lastly, we\nintroduce a Voting Module (VM) to more accurately generate the final anomaly\nscore. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies\nhave verified the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2505.21420v1", "date": "2025-05-27", "relevancy": 2.4236, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6412}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5845}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mentor3AD%3A%20Feature%20Reconstruction-based%203D%20Anomaly%20Detection%20via%0A%20%20Multi-modality%20Mentor%20Learning&body=Title%3A%20Mentor3AD%3A%20Feature%20Reconstruction-based%203D%20Anomaly%20Detection%20via%0A%20%20Multi-modality%20Mentor%20Learning%0AAuthor%3A%20Jinbao%20Wang%20and%20Hanzhe%20Liang%20and%20Can%20Gao%20and%20Chenxi%20Hu%20and%20Jie%20Zhou%20and%20Yunkang%20Cao%20and%20Linlin%20Shen%20and%20Weiming%20Shen%0AAbstract%3A%20%20%20Multimodal%20feature%20reconstruction%20is%20a%20promising%20approach%20for%203D%20anomaly%0Adetection%2C%20leveraging%20the%20complementary%20information%20from%20dual%20modalities.%20We%0Afurther%20advance%20this%20paradigm%20by%20utilizing%20multi-modal%20mentor%20learning%2C%20which%0Afuses%20intermediate%20features%20to%20further%20distinguish%20normal%20from%20feature%0Adifferences.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20method%20called%0AMentor3AD%2C%20which%20utilizes%20multi-modal%20mentor%20learning.%20By%20leveraging%20the%20shared%0Afeatures%20of%20different%20modalities%2C%20Mentor3AD%20can%20extract%20more%20effective%20features%0Aand%20guide%20feature%20reconstruction%2C%20ultimately%20improving%20detection%20performance.%0ASpecifically%2C%20Mentor3AD%20includes%20a%20Mentor%20of%20Fusion%20Module%20%28MFM%29%20that%20merges%0Afeatures%20extracted%20from%20RGB%20and%203D%20modalities%20to%20create%20a%20mentor%20feature.%0AAdditionally%2C%20we%20have%20designed%20a%20Mentor%20of%20Guidance%20Module%20%28MGM%29%20to%20facilitate%0Across-modal%20reconstruction%2C%20supported%20by%20the%20mentor%20feature.%20Lastly%2C%20we%0Aintroduce%20a%20Voting%20Module%20%28VM%29%20to%20more%20accurately%20generate%20the%20final%20anomaly%0Ascore.%20Extensive%20comparative%20and%20ablation%20studies%20on%20MVTec%203D-AD%20and%20Eyecandies%0Ahave%20verified%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMentor3AD%253A%2520Feature%2520Reconstruction-based%25203D%2520Anomaly%2520Detection%2520via%250A%2520%2520Multi-modality%2520Mentor%2520Learning%26entry.906535625%3DJinbao%2520Wang%2520and%2520Hanzhe%2520Liang%2520and%2520Can%2520Gao%2520and%2520Chenxi%2520Hu%2520and%2520Jie%2520Zhou%2520and%2520Yunkang%2520Cao%2520and%2520Linlin%2520Shen%2520and%2520Weiming%2520Shen%26entry.1292438233%3D%2520%2520Multimodal%2520feature%2520reconstruction%2520is%2520a%2520promising%2520approach%2520for%25203D%2520anomaly%250Adetection%252C%2520leveraging%2520the%2520complementary%2520information%2520from%2520dual%2520modalities.%2520We%250Afurther%2520advance%2520this%2520paradigm%2520by%2520utilizing%2520multi-modal%2520mentor%2520learning%252C%2520which%250Afuses%2520intermediate%2520features%2520to%2520further%2520distinguish%2520normal%2520from%2520feature%250Adifferences.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520method%2520called%250AMentor3AD%252C%2520which%2520utilizes%2520multi-modal%2520mentor%2520learning.%2520By%2520leveraging%2520the%2520shared%250Afeatures%2520of%2520different%2520modalities%252C%2520Mentor3AD%2520can%2520extract%2520more%2520effective%2520features%250Aand%2520guide%2520feature%2520reconstruction%252C%2520ultimately%2520improving%2520detection%2520performance.%250ASpecifically%252C%2520Mentor3AD%2520includes%2520a%2520Mentor%2520of%2520Fusion%2520Module%2520%2528MFM%2529%2520that%2520merges%250Afeatures%2520extracted%2520from%2520RGB%2520and%25203D%2520modalities%2520to%2520create%2520a%2520mentor%2520feature.%250AAdditionally%252C%2520we%2520have%2520designed%2520a%2520Mentor%2520of%2520Guidance%2520Module%2520%2528MGM%2529%2520to%2520facilitate%250Across-modal%2520reconstruction%252C%2520supported%2520by%2520the%2520mentor%2520feature.%2520Lastly%252C%2520we%250Aintroduce%2520a%2520Voting%2520Module%2520%2528VM%2529%2520to%2520more%2520accurately%2520generate%2520the%2520final%2520anomaly%250Ascore.%2520Extensive%2520comparative%2520and%2520ablation%2520studies%2520on%2520MVTec%25203D-AD%2520and%2520Eyecandies%250Ahave%2520verified%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mentor3AD%3A%20Feature%20Reconstruction-based%203D%20Anomaly%20Detection%20via%0A%20%20Multi-modality%20Mentor%20Learning&entry.906535625=Jinbao%20Wang%20and%20Hanzhe%20Liang%20and%20Can%20Gao%20and%20Chenxi%20Hu%20and%20Jie%20Zhou%20and%20Yunkang%20Cao%20and%20Linlin%20Shen%20and%20Weiming%20Shen&entry.1292438233=%20%20Multimodal%20feature%20reconstruction%20is%20a%20promising%20approach%20for%203D%20anomaly%0Adetection%2C%20leveraging%20the%20complementary%20information%20from%20dual%20modalities.%20We%0Afurther%20advance%20this%20paradigm%20by%20utilizing%20multi-modal%20mentor%20learning%2C%20which%0Afuses%20intermediate%20features%20to%20further%20distinguish%20normal%20from%20feature%0Adifferences.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20method%20called%0AMentor3AD%2C%20which%20utilizes%20multi-modal%20mentor%20learning.%20By%20leveraging%20the%20shared%0Afeatures%20of%20different%20modalities%2C%20Mentor3AD%20can%20extract%20more%20effective%20features%0Aand%20guide%20feature%20reconstruction%2C%20ultimately%20improving%20detection%20performance.%0ASpecifically%2C%20Mentor3AD%20includes%20a%20Mentor%20of%20Fusion%20Module%20%28MFM%29%20that%20merges%0Afeatures%20extracted%20from%20RGB%20and%203D%20modalities%20to%20create%20a%20mentor%20feature.%0AAdditionally%2C%20we%20have%20designed%20a%20Mentor%20of%20Guidance%20Module%20%28MGM%29%20to%20facilitate%0Across-modal%20reconstruction%2C%20supported%20by%20the%20mentor%20feature.%20Lastly%2C%20we%0Aintroduce%20a%20Voting%20Module%20%28VM%29%20to%20more%20accurately%20generate%20the%20final%20anomaly%0Ascore.%20Extensive%20comparative%20and%20ablation%20studies%20on%20MVTec%203D-AD%20and%20Eyecandies%0Ahave%20verified%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21420v1&entry.124074799=Read"},
{"title": "Controlling Participation in Federated Learning with Feedback", "author": "Michael Cummins and Guner Dilsad Er and Michael Muehlebach", "abstract": "  We address the problem of client participation in federated learning, where\ntraditional methods typically rely on a random selection of a small subset of\nclients for each training round. In contrast, we propose FedBack, a\ndeterministic approach that leverages control-theoretic principles to manage\nclient participation in ADMM-based federated learning. FedBack models client\nparticipation as a discrete-time dynamical system and employs an integral\nfeedback controller to adjust each client's participation rate individually,\nbased on the client's optimization dynamics. We provide global convergence\nguarantees for our approach by building on the recent federated learning\nresearch. Numerical experiments on federated image classification demonstrate\nthat FedBack achieves up to 50\\% improvement in communication and computational\nefficiency over algorithms that rely on a random selection of clients.\n", "link": "http://arxiv.org/abs/2411.19242v2", "date": "2025-05-27", "relevancy": 2.408, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5003}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4856}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20Participation%20in%20Federated%20Learning%20with%20Feedback&body=Title%3A%20Controlling%20Participation%20in%20Federated%20Learning%20with%20Feedback%0AAuthor%3A%20Michael%20Cummins%20and%20Guner%20Dilsad%20Er%20and%20Michael%20Muehlebach%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20client%20participation%20in%20federated%20learning%2C%20where%0Atraditional%20methods%20typically%20rely%20on%20a%20random%20selection%20of%20a%20small%20subset%20of%0Aclients%20for%20each%20training%20round.%20In%20contrast%2C%20we%20propose%20FedBack%2C%20a%0Adeterministic%20approach%20that%20leverages%20control-theoretic%20principles%20to%20manage%0Aclient%20participation%20in%20ADMM-based%20federated%20learning.%20FedBack%20models%20client%0Aparticipation%20as%20a%20discrete-time%20dynamical%20system%20and%20employs%20an%20integral%0Afeedback%20controller%20to%20adjust%20each%20client%27s%20participation%20rate%20individually%2C%0Abased%20on%20the%20client%27s%20optimization%20dynamics.%20We%20provide%20global%20convergence%0Aguarantees%20for%20our%20approach%20by%20building%20on%20the%20recent%20federated%20learning%0Aresearch.%20Numerical%20experiments%20on%20federated%20image%20classification%20demonstrate%0Athat%20FedBack%20achieves%20up%20to%2050%5C%25%20improvement%20in%20communication%20and%20computational%0Aefficiency%20over%20algorithms%20that%20rely%20on%20a%20random%20selection%20of%20clients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520Participation%2520in%2520Federated%2520Learning%2520with%2520Feedback%26entry.906535625%3DMichael%2520Cummins%2520and%2520Guner%2520Dilsad%2520Er%2520and%2520Michael%2520Muehlebach%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520client%2520participation%2520in%2520federated%2520learning%252C%2520where%250Atraditional%2520methods%2520typically%2520rely%2520on%2520a%2520random%2520selection%2520of%2520a%2520small%2520subset%2520of%250Aclients%2520for%2520each%2520training%2520round.%2520In%2520contrast%252C%2520we%2520propose%2520FedBack%252C%2520a%250Adeterministic%2520approach%2520that%2520leverages%2520control-theoretic%2520principles%2520to%2520manage%250Aclient%2520participation%2520in%2520ADMM-based%2520federated%2520learning.%2520FedBack%2520models%2520client%250Aparticipation%2520as%2520a%2520discrete-time%2520dynamical%2520system%2520and%2520employs%2520an%2520integral%250Afeedback%2520controller%2520to%2520adjust%2520each%2520client%2527s%2520participation%2520rate%2520individually%252C%250Abased%2520on%2520the%2520client%2527s%2520optimization%2520dynamics.%2520We%2520provide%2520global%2520convergence%250Aguarantees%2520for%2520our%2520approach%2520by%2520building%2520on%2520the%2520recent%2520federated%2520learning%250Aresearch.%2520Numerical%2520experiments%2520on%2520federated%2520image%2520classification%2520demonstrate%250Athat%2520FedBack%2520achieves%2520up%2520to%252050%255C%2525%2520improvement%2520in%2520communication%2520and%2520computational%250Aefficiency%2520over%2520algorithms%2520that%2520rely%2520on%2520a%2520random%2520selection%2520of%2520clients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Participation%20in%20Federated%20Learning%20with%20Feedback&entry.906535625=Michael%20Cummins%20and%20Guner%20Dilsad%20Er%20and%20Michael%20Muehlebach&entry.1292438233=%20%20We%20address%20the%20problem%20of%20client%20participation%20in%20federated%20learning%2C%20where%0Atraditional%20methods%20typically%20rely%20on%20a%20random%20selection%20of%20a%20small%20subset%20of%0Aclients%20for%20each%20training%20round.%20In%20contrast%2C%20we%20propose%20FedBack%2C%20a%0Adeterministic%20approach%20that%20leverages%20control-theoretic%20principles%20to%20manage%0Aclient%20participation%20in%20ADMM-based%20federated%20learning.%20FedBack%20models%20client%0Aparticipation%20as%20a%20discrete-time%20dynamical%20system%20and%20employs%20an%20integral%0Afeedback%20controller%20to%20adjust%20each%20client%27s%20participation%20rate%20individually%2C%0Abased%20on%20the%20client%27s%20optimization%20dynamics.%20We%20provide%20global%20convergence%0Aguarantees%20for%20our%20approach%20by%20building%20on%20the%20recent%20federated%20learning%0Aresearch.%20Numerical%20experiments%20on%20federated%20image%20classification%20demonstrate%0Athat%20FedBack%20achieves%20up%20to%2050%5C%25%20improvement%20in%20communication%20and%20computational%0Aefficiency%20over%20algorithms%20that%20rely%20on%20a%20random%20selection%20of%20clients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19242v2&entry.124074799=Read"},
{"title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction", "author": "Yiheng Liu and Liao Qu and Huichao Zhang and Xu Wang and Yi Jiang and Yiming Gao and Hu Ye and Xian Li and Shuai Wang and Daniel K. Du and Shu Cheng and Zehuan Yuan and Xinglong Wu", "abstract": "  This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image\ngeneration method that models images through a novel next-detail prediction\nstrategy. By learning a resolution-aware token sequence supervised with\nprogressively degraded images, DetailFlow enables the generation process to\nstart from the global structure and incrementally refine details. This\ncoarse-to-fine 1D token sequence aligns well with the autoregressive inference\nmechanism, providing a more natural and efficient way for the AR model to\ngenerate complex visual content. Our compact 1D AR model achieves high-quality\nimage synthesis with significantly fewer tokens than previous approaches, i.e.\nVAR/VQGAN. We further propose a parallel inference mechanism with\nself-correction that accelerates generation speed by approximately 8x while\nreducing accumulation sampling error inherent in teacher-forcing supervision.\nOn the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128\ntokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require\n680 tokens in their AR models. Moreover, due to the significantly reduced token\ncount and parallel inference mechanism, our method runs nearly 2x faster\ninference speed compared to VAR and FlexVAR. Extensive experimental results\ndemonstrate DetailFlow's superior generation quality and efficiency compared to\nexisting state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.21473v1", "date": "2025-05-27", "relevancy": 2.4069, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6361}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6159}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DetailFlow%3A%201D%20Coarse-to-Fine%20Autoregressive%20Image%20Generation%20via%0A%20%20Next-Detail%20Prediction&body=Title%3A%20DetailFlow%3A%201D%20Coarse-to-Fine%20Autoregressive%20Image%20Generation%20via%0A%20%20Next-Detail%20Prediction%0AAuthor%3A%20Yiheng%20Liu%20and%20Liao%20Qu%20and%20Huichao%20Zhang%20and%20Xu%20Wang%20and%20Yi%20Jiang%20and%20Yiming%20Gao%20and%20Hu%20Ye%20and%20Xian%20Li%20and%20Shuai%20Wang%20and%20Daniel%20K.%20Du%20and%20Shu%20Cheng%20and%20Zehuan%20Yuan%20and%20Xinglong%20Wu%0AAbstract%3A%20%20%20This%20paper%20presents%20DetailFlow%2C%20a%20coarse-to-fine%201D%20autoregressive%20%28AR%29%20image%0Ageneration%20method%20that%20models%20images%20through%20a%20novel%20next-detail%20prediction%0Astrategy.%20By%20learning%20a%20resolution-aware%20token%20sequence%20supervised%20with%0Aprogressively%20degraded%20images%2C%20DetailFlow%20enables%20the%20generation%20process%20to%0Astart%20from%20the%20global%20structure%20and%20incrementally%20refine%20details.%20This%0Acoarse-to-fine%201D%20token%20sequence%20aligns%20well%20with%20the%20autoregressive%20inference%0Amechanism%2C%20providing%20a%20more%20natural%20and%20efficient%20way%20for%20the%20AR%20model%20to%0Agenerate%20complex%20visual%20content.%20Our%20compact%201D%20AR%20model%20achieves%20high-quality%0Aimage%20synthesis%20with%20significantly%20fewer%20tokens%20than%20previous%20approaches%2C%20i.e.%0AVAR/VQGAN.%20We%20further%20propose%20a%20parallel%20inference%20mechanism%20with%0Aself-correction%20that%20accelerates%20generation%20speed%20by%20approximately%208x%20while%0Areducing%20accumulation%20sampling%20error%20inherent%20in%20teacher-forcing%20supervision.%0AOn%20the%20ImageNet%20256x256%20benchmark%2C%20our%20method%20achieves%202.96%20gFID%20with%20128%0Atokens%2C%20outperforming%20VAR%20%283.3%20FID%29%20and%20FlexVAR%20%283.05%20FID%29%2C%20which%20both%20require%0A680%20tokens%20in%20their%20AR%20models.%20Moreover%2C%20due%20to%20the%20significantly%20reduced%20token%0Acount%20and%20parallel%20inference%20mechanism%2C%20our%20method%20runs%20nearly%202x%20faster%0Ainference%20speed%20compared%20to%20VAR%20and%20FlexVAR.%20Extensive%20experimental%20results%0Ademonstrate%20DetailFlow%27s%20superior%20generation%20quality%20and%20efficiency%20compared%20to%0Aexisting%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetailFlow%253A%25201D%2520Coarse-to-Fine%2520Autoregressive%2520Image%2520Generation%2520via%250A%2520%2520Next-Detail%2520Prediction%26entry.906535625%3DYiheng%2520Liu%2520and%2520Liao%2520Qu%2520and%2520Huichao%2520Zhang%2520and%2520Xu%2520Wang%2520and%2520Yi%2520Jiang%2520and%2520Yiming%2520Gao%2520and%2520Hu%2520Ye%2520and%2520Xian%2520Li%2520and%2520Shuai%2520Wang%2520and%2520Daniel%2520K.%2520Du%2520and%2520Shu%2520Cheng%2520and%2520Zehuan%2520Yuan%2520and%2520Xinglong%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DetailFlow%252C%2520a%2520coarse-to-fine%25201D%2520autoregressive%2520%2528AR%2529%2520image%250Ageneration%2520method%2520that%2520models%2520images%2520through%2520a%2520novel%2520next-detail%2520prediction%250Astrategy.%2520By%2520learning%2520a%2520resolution-aware%2520token%2520sequence%2520supervised%2520with%250Aprogressively%2520degraded%2520images%252C%2520DetailFlow%2520enables%2520the%2520generation%2520process%2520to%250Astart%2520from%2520the%2520global%2520structure%2520and%2520incrementally%2520refine%2520details.%2520This%250Acoarse-to-fine%25201D%2520token%2520sequence%2520aligns%2520well%2520with%2520the%2520autoregressive%2520inference%250Amechanism%252C%2520providing%2520a%2520more%2520natural%2520and%2520efficient%2520way%2520for%2520the%2520AR%2520model%2520to%250Agenerate%2520complex%2520visual%2520content.%2520Our%2520compact%25201D%2520AR%2520model%2520achieves%2520high-quality%250Aimage%2520synthesis%2520with%2520significantly%2520fewer%2520tokens%2520than%2520previous%2520approaches%252C%2520i.e.%250AVAR/VQGAN.%2520We%2520further%2520propose%2520a%2520parallel%2520inference%2520mechanism%2520with%250Aself-correction%2520that%2520accelerates%2520generation%2520speed%2520by%2520approximately%25208x%2520while%250Areducing%2520accumulation%2520sampling%2520error%2520inherent%2520in%2520teacher-forcing%2520supervision.%250AOn%2520the%2520ImageNet%2520256x256%2520benchmark%252C%2520our%2520method%2520achieves%25202.96%2520gFID%2520with%2520128%250Atokens%252C%2520outperforming%2520VAR%2520%25283.3%2520FID%2529%2520and%2520FlexVAR%2520%25283.05%2520FID%2529%252C%2520which%2520both%2520require%250A680%2520tokens%2520in%2520their%2520AR%2520models.%2520Moreover%252C%2520due%2520to%2520the%2520significantly%2520reduced%2520token%250Acount%2520and%2520parallel%2520inference%2520mechanism%252C%2520our%2520method%2520runs%2520nearly%25202x%2520faster%250Ainference%2520speed%2520compared%2520to%2520VAR%2520and%2520FlexVAR.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520DetailFlow%2527s%2520superior%2520generation%2520quality%2520and%2520efficiency%2520compared%2520to%250Aexisting%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DetailFlow%3A%201D%20Coarse-to-Fine%20Autoregressive%20Image%20Generation%20via%0A%20%20Next-Detail%20Prediction&entry.906535625=Yiheng%20Liu%20and%20Liao%20Qu%20and%20Huichao%20Zhang%20and%20Xu%20Wang%20and%20Yi%20Jiang%20and%20Yiming%20Gao%20and%20Hu%20Ye%20and%20Xian%20Li%20and%20Shuai%20Wang%20and%20Daniel%20K.%20Du%20and%20Shu%20Cheng%20and%20Zehuan%20Yuan%20and%20Xinglong%20Wu&entry.1292438233=%20%20This%20paper%20presents%20DetailFlow%2C%20a%20coarse-to-fine%201D%20autoregressive%20%28AR%29%20image%0Ageneration%20method%20that%20models%20images%20through%20a%20novel%20next-detail%20prediction%0Astrategy.%20By%20learning%20a%20resolution-aware%20token%20sequence%20supervised%20with%0Aprogressively%20degraded%20images%2C%20DetailFlow%20enables%20the%20generation%20process%20to%0Astart%20from%20the%20global%20structure%20and%20incrementally%20refine%20details.%20This%0Acoarse-to-fine%201D%20token%20sequence%20aligns%20well%20with%20the%20autoregressive%20inference%0Amechanism%2C%20providing%20a%20more%20natural%20and%20efficient%20way%20for%20the%20AR%20model%20to%0Agenerate%20complex%20visual%20content.%20Our%20compact%201D%20AR%20model%20achieves%20high-quality%0Aimage%20synthesis%20with%20significantly%20fewer%20tokens%20than%20previous%20approaches%2C%20i.e.%0AVAR/VQGAN.%20We%20further%20propose%20a%20parallel%20inference%20mechanism%20with%0Aself-correction%20that%20accelerates%20generation%20speed%20by%20approximately%208x%20while%0Areducing%20accumulation%20sampling%20error%20inherent%20in%20teacher-forcing%20supervision.%0AOn%20the%20ImageNet%20256x256%20benchmark%2C%20our%20method%20achieves%202.96%20gFID%20with%20128%0Atokens%2C%20outperforming%20VAR%20%283.3%20FID%29%20and%20FlexVAR%20%283.05%20FID%29%2C%20which%20both%20require%0A680%20tokens%20in%20their%20AR%20models.%20Moreover%2C%20due%20to%20the%20significantly%20reduced%20token%0Acount%20and%20parallel%20inference%20mechanism%2C%20our%20method%20runs%20nearly%202x%20faster%0Ainference%20speed%20compared%20to%20VAR%20and%20FlexVAR.%20Extensive%20experimental%20results%0Ademonstrate%20DetailFlow%27s%20superior%20generation%20quality%20and%20efficiency%20compared%20to%0Aexisting%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21473v1&entry.124074799=Read"},
{"title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing", "author": "Xiangyu Zhao and Peiyuan Zhang and Kexian Tang and Xiaorong Zhu and Hao Li and Wenhao Chai and Zicheng Zhang and Renqiu Xia and Guangtao Zhai and Junchi Yan and Hua Yang and Xue Yang and Haodong Duan", "abstract": "  Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To study this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning categories:\nTemporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test\ncases for each category and propose an robust evaluation framework that\nassesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility\nwith both human judges and the LMM-as-a-judge approach. We conducted\nexperiments evaluating nine prominent visual editing models, comprising both\nopen-source and proprietary models. The evaluation results demonstrate that\ncurrent models face significant challenges in reasoning-based editing tasks.\nEven the most powerful model evaluated, GPT-4o-Image, achieves an accuracy of\nmerely 28.8%. RISEBench effectively highlights the limitations of contemporary\nediting models, provides valuable insights, and indicates potential future\ndirections for the field of reasoning-aware visual editing. Our code and data\nhave been released at https://github.com/PhoenixZ810/RISEBench.\n", "link": "http://arxiv.org/abs/2504.02826v4", "date": "2025-05-27", "relevancy": 2.4015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6084}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6084}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Envisioning%20Beyond%20the%20Pixels%3A%20Benchmarking%20Reasoning-Informed%20Visual%0A%20%20Editing&body=Title%3A%20Envisioning%20Beyond%20the%20Pixels%3A%20Benchmarking%20Reasoning-Informed%20Visual%0A%20%20Editing%0AAuthor%3A%20Xiangyu%20Zhao%20and%20Peiyuan%20Zhang%20and%20Kexian%20Tang%20and%20Xiaorong%20Zhu%20and%20Hao%20Li%20and%20Wenhao%20Chai%20and%20Zicheng%20Zhang%20and%20Renqiu%20Xia%20and%20Guangtao%20Zhai%20and%20Junchi%20Yan%20and%20Hua%20Yang%20and%20Xue%20Yang%20and%20Haodong%20Duan%0AAbstract%3A%20%20%20Large%20Multi-modality%20Models%20%28LMMs%29%20have%20made%20significant%20progress%20in%20visual%0Aunderstanding%20and%20generation%2C%20but%20they%20still%20face%20challenges%20in%20General%20Visual%0AEditing%2C%20particularly%20in%20following%20complex%20instructions%2C%20preserving%20appearance%0Aconsistency%2C%20and%20supporting%20flexible%20input%20formats.%20To%20study%20this%20gap%2C%20we%0Aintroduce%20RISEBench%2C%20the%20first%20benchmark%20for%20evaluating%20Reasoning-Informed%0AviSual%20Editing%20%28RISE%29.%20RISEBench%20focuses%20on%20four%20key%20reasoning%20categories%3A%0ATemporal%2C%20Causal%2C%20Spatial%2C%20and%20Logical%20Reasoning.%20We%20curate%20high-quality%20test%0Acases%20for%20each%20category%20and%20propose%20an%20robust%20evaluation%20framework%20that%0Aassesses%20Instruction%20Reasoning%2C%20Appearance%20Consistency%2C%20and%20Visual%20Plausibility%0Awith%20both%20human%20judges%20and%20the%20LMM-as-a-judge%20approach.%20We%20conducted%0Aexperiments%20evaluating%20nine%20prominent%20visual%20editing%20models%2C%20comprising%20both%0Aopen-source%20and%20proprietary%20models.%20The%20evaluation%20results%20demonstrate%20that%0Acurrent%20models%20face%20significant%20challenges%20in%20reasoning-based%20editing%20tasks.%0AEven%20the%20most%20powerful%20model%20evaluated%2C%20GPT-4o-Image%2C%20achieves%20an%20accuracy%20of%0Amerely%2028.8%25.%20RISEBench%20effectively%20highlights%20the%20limitations%20of%20contemporary%0Aediting%20models%2C%20provides%20valuable%20insights%2C%20and%20indicates%20potential%20future%0Adirections%20for%20the%20field%20of%20reasoning-aware%20visual%20editing.%20Our%20code%20and%20data%0Ahave%20been%20released%20at%20https%3A//github.com/PhoenixZ810/RISEBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02826v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvisioning%2520Beyond%2520the%2520Pixels%253A%2520Benchmarking%2520Reasoning-Informed%2520Visual%250A%2520%2520Editing%26entry.906535625%3DXiangyu%2520Zhao%2520and%2520Peiyuan%2520Zhang%2520and%2520Kexian%2520Tang%2520and%2520Xiaorong%2520Zhu%2520and%2520Hao%2520Li%2520and%2520Wenhao%2520Chai%2520and%2520Zicheng%2520Zhang%2520and%2520Renqiu%2520Xia%2520and%2520Guangtao%2520Zhai%2520and%2520Junchi%2520Yan%2520and%2520Hua%2520Yang%2520and%2520Xue%2520Yang%2520and%2520Haodong%2520Duan%26entry.1292438233%3D%2520%2520Large%2520Multi-modality%2520Models%2520%2528LMMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520visual%250Aunderstanding%2520and%2520generation%252C%2520but%2520they%2520still%2520face%2520challenges%2520in%2520General%2520Visual%250AEditing%252C%2520particularly%2520in%2520following%2520complex%2520instructions%252C%2520preserving%2520appearance%250Aconsistency%252C%2520and%2520supporting%2520flexible%2520input%2520formats.%2520To%2520study%2520this%2520gap%252C%2520we%250Aintroduce%2520RISEBench%252C%2520the%2520first%2520benchmark%2520for%2520evaluating%2520Reasoning-Informed%250AviSual%2520Editing%2520%2528RISE%2529.%2520RISEBench%2520focuses%2520on%2520four%2520key%2520reasoning%2520categories%253A%250ATemporal%252C%2520Causal%252C%2520Spatial%252C%2520and%2520Logical%2520Reasoning.%2520We%2520curate%2520high-quality%2520test%250Acases%2520for%2520each%2520category%2520and%2520propose%2520an%2520robust%2520evaluation%2520framework%2520that%250Aassesses%2520Instruction%2520Reasoning%252C%2520Appearance%2520Consistency%252C%2520and%2520Visual%2520Plausibility%250Awith%2520both%2520human%2520judges%2520and%2520the%2520LMM-as-a-judge%2520approach.%2520We%2520conducted%250Aexperiments%2520evaluating%2520nine%2520prominent%2520visual%2520editing%2520models%252C%2520comprising%2520both%250Aopen-source%2520and%2520proprietary%2520models.%2520The%2520evaluation%2520results%2520demonstrate%2520that%250Acurrent%2520models%2520face%2520significant%2520challenges%2520in%2520reasoning-based%2520editing%2520tasks.%250AEven%2520the%2520most%2520powerful%2520model%2520evaluated%252C%2520GPT-4o-Image%252C%2520achieves%2520an%2520accuracy%2520of%250Amerely%252028.8%2525.%2520RISEBench%2520effectively%2520highlights%2520the%2520limitations%2520of%2520contemporary%250Aediting%2520models%252C%2520provides%2520valuable%2520insights%252C%2520and%2520indicates%2520potential%2520future%250Adirections%2520for%2520the%2520field%2520of%2520reasoning-aware%2520visual%2520editing.%2520Our%2520code%2520and%2520data%250Ahave%2520been%2520released%2520at%2520https%253A//github.com/PhoenixZ810/RISEBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02826v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Envisioning%20Beyond%20the%20Pixels%3A%20Benchmarking%20Reasoning-Informed%20Visual%0A%20%20Editing&entry.906535625=Xiangyu%20Zhao%20and%20Peiyuan%20Zhang%20and%20Kexian%20Tang%20and%20Xiaorong%20Zhu%20and%20Hao%20Li%20and%20Wenhao%20Chai%20and%20Zicheng%20Zhang%20and%20Renqiu%20Xia%20and%20Guangtao%20Zhai%20and%20Junchi%20Yan%20and%20Hua%20Yang%20and%20Xue%20Yang%20and%20Haodong%20Duan&entry.1292438233=%20%20Large%20Multi-modality%20Models%20%28LMMs%29%20have%20made%20significant%20progress%20in%20visual%0Aunderstanding%20and%20generation%2C%20but%20they%20still%20face%20challenges%20in%20General%20Visual%0AEditing%2C%20particularly%20in%20following%20complex%20instructions%2C%20preserving%20appearance%0Aconsistency%2C%20and%20supporting%20flexible%20input%20formats.%20To%20study%20this%20gap%2C%20we%0Aintroduce%20RISEBench%2C%20the%20first%20benchmark%20for%20evaluating%20Reasoning-Informed%0AviSual%20Editing%20%28RISE%29.%20RISEBench%20focuses%20on%20four%20key%20reasoning%20categories%3A%0ATemporal%2C%20Causal%2C%20Spatial%2C%20and%20Logical%20Reasoning.%20We%20curate%20high-quality%20test%0Acases%20for%20each%20category%20and%20propose%20an%20robust%20evaluation%20framework%20that%0Aassesses%20Instruction%20Reasoning%2C%20Appearance%20Consistency%2C%20and%20Visual%20Plausibility%0Awith%20both%20human%20judges%20and%20the%20LMM-as-a-judge%20approach.%20We%20conducted%0Aexperiments%20evaluating%20nine%20prominent%20visual%20editing%20models%2C%20comprising%20both%0Aopen-source%20and%20proprietary%20models.%20The%20evaluation%20results%20demonstrate%20that%0Acurrent%20models%20face%20significant%20challenges%20in%20reasoning-based%20editing%20tasks.%0AEven%20the%20most%20powerful%20model%20evaluated%2C%20GPT-4o-Image%2C%20achieves%20an%20accuracy%20of%0Amerely%2028.8%25.%20RISEBench%20effectively%20highlights%20the%20limitations%20of%20contemporary%0Aediting%20models%2C%20provides%20valuable%20insights%2C%20and%20indicates%20potential%20future%0Adirections%20for%20the%20field%20of%20reasoning-aware%20visual%20editing.%20Our%20code%20and%20data%0Ahave%20been%20released%20at%20https%3A//github.com/PhoenixZ810/RISEBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02826v4&entry.124074799=Read"},
{"title": "Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO", "author": "Muzhi Zhu and Hao Zhong and Canyu Zhao and Zongze Du and Zheng Huang and Mingyu Liu and Hao Chen and Cheng Zou and Jingdong Chen and Ming Yang and Chunhua Shen", "abstract": "  Active vision, also known as active perception, refers to the process of\nactively selecting where and how to look in order to gather task-relevant\ninformation. It is a critical component of efficient perception and\ndecision-making in humans and advanced embodied agents. Recently, the use of\nMultimodal Large Language Models (MLLMs) as central planning and\ndecision-making modules in robotic systems has gained extensive attention.\nHowever, despite the importance of active perception in embodied intelligence,\nthere is little to no exploration of how MLLMs can be equipped with or learn\nactive perception capabilities. In this paper, we first provide a systematic\ndefinition of MLLM-based active perception tasks. We point out that the\nrecently proposed GPT-o3 model's zoom-in search strategy can be regarded as a\nspecial case of active perception; however, it still suffers from low search\nefficiency and inaccurate region selection. To address these issues, we propose\nACTIVE-O3, a purely reinforcement learning based training framework built on\ntop of GRPO, designed to equip MLLMs with active perception capabilities. We\nfurther establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across\nboth general open-world tasks, such as small-object and dense object grounding,\nand domain-specific scenarios, including small object detection in remote\nsensing and autonomous driving, as well as fine-grained interactive\nsegmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot\nreasoning abilities on the V* Benchmark, without relying on any explicit\nreasoning data. We hope that our work can provide a simple codebase and\nevaluation protocol to facilitate future research on active perception in\nMLLMs.\n", "link": "http://arxiv.org/abs/2505.21457v1", "date": "2025-05-27", "relevancy": 2.3976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active-O3%3A%20Empowering%20Multimodal%20Large%20Language%20Models%20with%20Active%0A%20%20Perception%20via%20GRPO&body=Title%3A%20Active-O3%3A%20Empowering%20Multimodal%20Large%20Language%20Models%20with%20Active%0A%20%20Perception%20via%20GRPO%0AAuthor%3A%20Muzhi%20Zhu%20and%20Hao%20Zhong%20and%20Canyu%20Zhao%20and%20Zongze%20Du%20and%20Zheng%20Huang%20and%20Mingyu%20Liu%20and%20Hao%20Chen%20and%20Cheng%20Zou%20and%20Jingdong%20Chen%20and%20Ming%20Yang%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Active%20vision%2C%20also%20known%20as%20active%20perception%2C%20refers%20to%20the%20process%20of%0Aactively%20selecting%20where%20and%20how%20to%20look%20in%20order%20to%20gather%20task-relevant%0Ainformation.%20It%20is%20a%20critical%20component%20of%20efficient%20perception%20and%0Adecision-making%20in%20humans%20and%20advanced%20embodied%20agents.%20Recently%2C%20the%20use%20of%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20as%20central%20planning%20and%0Adecision-making%20modules%20in%20robotic%20systems%20has%20gained%20extensive%20attention.%0AHowever%2C%20despite%20the%20importance%20of%20active%20perception%20in%20embodied%20intelligence%2C%0Athere%20is%20little%20to%20no%20exploration%20of%20how%20MLLMs%20can%20be%20equipped%20with%20or%20learn%0Aactive%20perception%20capabilities.%20In%20this%20paper%2C%20we%20first%20provide%20a%20systematic%0Adefinition%20of%20MLLM-based%20active%20perception%20tasks.%20We%20point%20out%20that%20the%0Arecently%20proposed%20GPT-o3%20model%27s%20zoom-in%20search%20strategy%20can%20be%20regarded%20as%20a%0Aspecial%20case%20of%20active%20perception%3B%20however%2C%20it%20still%20suffers%20from%20low%20search%0Aefficiency%20and%20inaccurate%20region%20selection.%20To%20address%20these%20issues%2C%20we%20propose%0AACTIVE-O3%2C%20a%20purely%20reinforcement%20learning%20based%20training%20framework%20built%20on%0Atop%20of%20GRPO%2C%20designed%20to%20equip%20MLLMs%20with%20active%20perception%20capabilities.%20We%0Afurther%20establish%20a%20comprehensive%20benchmark%20suite%20to%20evaluate%20ACTIVE-O3%20across%0Aboth%20general%20open-world%20tasks%2C%20such%20as%20small-object%20and%20dense%20object%20grounding%2C%0Aand%20domain-specific%20scenarios%2C%20including%20small%20object%20detection%20in%20remote%0Asensing%20and%20autonomous%20driving%2C%20as%20well%20as%20fine-grained%20interactive%0Asegmentation.%20In%20addition%2C%20ACTIVE-O3%20also%20demonstrates%20strong%20zero-shot%0Areasoning%20abilities%20on%20the%20V%2A%20Benchmark%2C%20without%20relying%20on%20any%20explicit%0Areasoning%20data.%20We%20hope%20that%20our%20work%20can%20provide%20a%20simple%20codebase%20and%0Aevaluation%20protocol%20to%20facilitate%20future%20research%20on%20active%20perception%20in%0AMLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive-O3%253A%2520Empowering%2520Multimodal%2520Large%2520Language%2520Models%2520with%2520Active%250A%2520%2520Perception%2520via%2520GRPO%26entry.906535625%3DMuzhi%2520Zhu%2520and%2520Hao%2520Zhong%2520and%2520Canyu%2520Zhao%2520and%2520Zongze%2520Du%2520and%2520Zheng%2520Huang%2520and%2520Mingyu%2520Liu%2520and%2520Hao%2520Chen%2520and%2520Cheng%2520Zou%2520and%2520Jingdong%2520Chen%2520and%2520Ming%2520Yang%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Active%2520vision%252C%2520also%2520known%2520as%2520active%2520perception%252C%2520refers%2520to%2520the%2520process%2520of%250Aactively%2520selecting%2520where%2520and%2520how%2520to%2520look%2520in%2520order%2520to%2520gather%2520task-relevant%250Ainformation.%2520It%2520is%2520a%2520critical%2520component%2520of%2520efficient%2520perception%2520and%250Adecision-making%2520in%2520humans%2520and%2520advanced%2520embodied%2520agents.%2520Recently%252C%2520the%2520use%2520of%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520as%2520central%2520planning%2520and%250Adecision-making%2520modules%2520in%2520robotic%2520systems%2520has%2520gained%2520extensive%2520attention.%250AHowever%252C%2520despite%2520the%2520importance%2520of%2520active%2520perception%2520in%2520embodied%2520intelligence%252C%250Athere%2520is%2520little%2520to%2520no%2520exploration%2520of%2520how%2520MLLMs%2520can%2520be%2520equipped%2520with%2520or%2520learn%250Aactive%2520perception%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520first%2520provide%2520a%2520systematic%250Adefinition%2520of%2520MLLM-based%2520active%2520perception%2520tasks.%2520We%2520point%2520out%2520that%2520the%250Arecently%2520proposed%2520GPT-o3%2520model%2527s%2520zoom-in%2520search%2520strategy%2520can%2520be%2520regarded%2520as%2520a%250Aspecial%2520case%2520of%2520active%2520perception%253B%2520however%252C%2520it%2520still%2520suffers%2520from%2520low%2520search%250Aefficiency%2520and%2520inaccurate%2520region%2520selection.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AACTIVE-O3%252C%2520a%2520purely%2520reinforcement%2520learning%2520based%2520training%2520framework%2520built%2520on%250Atop%2520of%2520GRPO%252C%2520designed%2520to%2520equip%2520MLLMs%2520with%2520active%2520perception%2520capabilities.%2520We%250Afurther%2520establish%2520a%2520comprehensive%2520benchmark%2520suite%2520to%2520evaluate%2520ACTIVE-O3%2520across%250Aboth%2520general%2520open-world%2520tasks%252C%2520such%2520as%2520small-object%2520and%2520dense%2520object%2520grounding%252C%250Aand%2520domain-specific%2520scenarios%252C%2520including%2520small%2520object%2520detection%2520in%2520remote%250Asensing%2520and%2520autonomous%2520driving%252C%2520as%2520well%2520as%2520fine-grained%2520interactive%250Asegmentation.%2520In%2520addition%252C%2520ACTIVE-O3%2520also%2520demonstrates%2520strong%2520zero-shot%250Areasoning%2520abilities%2520on%2520the%2520V%252A%2520Benchmark%252C%2520without%2520relying%2520on%2520any%2520explicit%250Areasoning%2520data.%2520We%2520hope%2520that%2520our%2520work%2520can%2520provide%2520a%2520simple%2520codebase%2520and%250Aevaluation%2520protocol%2520to%2520facilitate%2520future%2520research%2520on%2520active%2520perception%2520in%250AMLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active-O3%3A%20Empowering%20Multimodal%20Large%20Language%20Models%20with%20Active%0A%20%20Perception%20via%20GRPO&entry.906535625=Muzhi%20Zhu%20and%20Hao%20Zhong%20and%20Canyu%20Zhao%20and%20Zongze%20Du%20and%20Zheng%20Huang%20and%20Mingyu%20Liu%20and%20Hao%20Chen%20and%20Cheng%20Zou%20and%20Jingdong%20Chen%20and%20Ming%20Yang%20and%20Chunhua%20Shen&entry.1292438233=%20%20Active%20vision%2C%20also%20known%20as%20active%20perception%2C%20refers%20to%20the%20process%20of%0Aactively%20selecting%20where%20and%20how%20to%20look%20in%20order%20to%20gather%20task-relevant%0Ainformation.%20It%20is%20a%20critical%20component%20of%20efficient%20perception%20and%0Adecision-making%20in%20humans%20and%20advanced%20embodied%20agents.%20Recently%2C%20the%20use%20of%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20as%20central%20planning%20and%0Adecision-making%20modules%20in%20robotic%20systems%20has%20gained%20extensive%20attention.%0AHowever%2C%20despite%20the%20importance%20of%20active%20perception%20in%20embodied%20intelligence%2C%0Athere%20is%20little%20to%20no%20exploration%20of%20how%20MLLMs%20can%20be%20equipped%20with%20or%20learn%0Aactive%20perception%20capabilities.%20In%20this%20paper%2C%20we%20first%20provide%20a%20systematic%0Adefinition%20of%20MLLM-based%20active%20perception%20tasks.%20We%20point%20out%20that%20the%0Arecently%20proposed%20GPT-o3%20model%27s%20zoom-in%20search%20strategy%20can%20be%20regarded%20as%20a%0Aspecial%20case%20of%20active%20perception%3B%20however%2C%20it%20still%20suffers%20from%20low%20search%0Aefficiency%20and%20inaccurate%20region%20selection.%20To%20address%20these%20issues%2C%20we%20propose%0AACTIVE-O3%2C%20a%20purely%20reinforcement%20learning%20based%20training%20framework%20built%20on%0Atop%20of%20GRPO%2C%20designed%20to%20equip%20MLLMs%20with%20active%20perception%20capabilities.%20We%0Afurther%20establish%20a%20comprehensive%20benchmark%20suite%20to%20evaluate%20ACTIVE-O3%20across%0Aboth%20general%20open-world%20tasks%2C%20such%20as%20small-object%20and%20dense%20object%20grounding%2C%0Aand%20domain-specific%20scenarios%2C%20including%20small%20object%20detection%20in%20remote%0Asensing%20and%20autonomous%20driving%2C%20as%20well%20as%20fine-grained%20interactive%0Asegmentation.%20In%20addition%2C%20ACTIVE-O3%20also%20demonstrates%20strong%20zero-shot%0Areasoning%20abilities%20on%20the%20V%2A%20Benchmark%2C%20without%20relying%20on%20any%20explicit%0Areasoning%20data.%20We%20hope%20that%20our%20work%20can%20provide%20a%20simple%20codebase%20and%0Aevaluation%20protocol%20to%20facilitate%20future%20research%20on%20active%20perception%20in%0AMLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21457v1&entry.124074799=Read"},
{"title": "Can Large Reasoning Models Self-Train?", "author": "Sheikh Shafayat and Fahim Tajwar and Ruslan Salakhutdinov and Jeff Schneider and Andrea Zanette", "abstract": "  Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges.\n", "link": "http://arxiv.org/abs/2505.21444v1", "date": "2025-05-27", "relevancy": 2.3953, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4835}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Reasoning%20Models%20Self-Train%3F&body=Title%3A%20Can%20Large%20Reasoning%20Models%20Self-Train%3F%0AAuthor%3A%20Sheikh%20Shafayat%20and%20Fahim%20Tajwar%20and%20Ruslan%20Salakhutdinov%20and%20Jeff%20Schneider%20and%20Andrea%20Zanette%0AAbstract%3A%20%20%20Scaling%20the%20performance%20of%20large%20language%20models%20%28LLMs%29%20increasingly%20depends%0Aon%20methods%20that%20reduce%20reliance%20on%20human%20supervision.%20Reinforcement%20learning%0Afrom%20automated%20verification%20offers%20an%20alternative%2C%20but%20it%20incurs%20scalability%0Alimitations%20due%20to%20dependency%20upon%20human-designed%20verifiers.%20Self-training%2C%0Awhere%20the%20model%27s%20own%20judgment%20provides%20the%20supervisory%20signal%2C%20presents%20a%0Acompelling%20direction.%20We%20propose%20an%20online%20self-training%20reinforcement%20learning%0Aalgorithm%20that%20leverages%20the%20model%27s%20self-consistency%20to%20infer%20correctness%0Asignals%20and%20train%20without%20any%20ground-truth%20supervision.%20We%20apply%20the%20algorithm%0Ato%20challenging%20mathematical%20reasoning%20tasks%20and%20show%20that%20it%20quickly%20reaches%0Aperformance%20levels%20rivaling%20reinforcement-learning%20methods%20trained%20explicitly%0Aon%20gold-standard%20answers.%20Additionally%2C%20we%20analyze%20inherent%20limitations%20of%20the%0Aalgorithm%2C%20highlighting%20how%20the%20self-generated%20proxy%20reward%20initially%0Acorrelated%20with%20correctness%20can%20incentivize%20reward%20hacking%2C%20where%20confidently%0Aincorrect%20outputs%20are%20favored.%20Our%20results%20illustrate%20how%20self-supervised%0Aimprovement%20can%20achieve%20significant%20performance%20gains%20without%20external%20labels%2C%0Awhile%20also%20revealing%20its%20fundamental%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Reasoning%2520Models%2520Self-Train%253F%26entry.906535625%3DSheikh%2520Shafayat%2520and%2520Fahim%2520Tajwar%2520and%2520Ruslan%2520Salakhutdinov%2520and%2520Jeff%2520Schneider%2520and%2520Andrea%2520Zanette%26entry.1292438233%3D%2520%2520Scaling%2520the%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520depends%250Aon%2520methods%2520that%2520reduce%2520reliance%2520on%2520human%2520supervision.%2520Reinforcement%2520learning%250Afrom%2520automated%2520verification%2520offers%2520an%2520alternative%252C%2520but%2520it%2520incurs%2520scalability%250Alimitations%2520due%2520to%2520dependency%2520upon%2520human-designed%2520verifiers.%2520Self-training%252C%250Awhere%2520the%2520model%2527s%2520own%2520judgment%2520provides%2520the%2520supervisory%2520signal%252C%2520presents%2520a%250Acompelling%2520direction.%2520We%2520propose%2520an%2520online%2520self-training%2520reinforcement%2520learning%250Aalgorithm%2520that%2520leverages%2520the%2520model%2527s%2520self-consistency%2520to%2520infer%2520correctness%250Asignals%2520and%2520train%2520without%2520any%2520ground-truth%2520supervision.%2520We%2520apply%2520the%2520algorithm%250Ato%2520challenging%2520mathematical%2520reasoning%2520tasks%2520and%2520show%2520that%2520it%2520quickly%2520reaches%250Aperformance%2520levels%2520rivaling%2520reinforcement-learning%2520methods%2520trained%2520explicitly%250Aon%2520gold-standard%2520answers.%2520Additionally%252C%2520we%2520analyze%2520inherent%2520limitations%2520of%2520the%250Aalgorithm%252C%2520highlighting%2520how%2520the%2520self-generated%2520proxy%2520reward%2520initially%250Acorrelated%2520with%2520correctness%2520can%2520incentivize%2520reward%2520hacking%252C%2520where%2520confidently%250Aincorrect%2520outputs%2520are%2520favored.%2520Our%2520results%2520illustrate%2520how%2520self-supervised%250Aimprovement%2520can%2520achieve%2520significant%2520performance%2520gains%2520without%2520external%2520labels%252C%250Awhile%2520also%2520revealing%2520its%2520fundamental%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Reasoning%20Models%20Self-Train%3F&entry.906535625=Sheikh%20Shafayat%20and%20Fahim%20Tajwar%20and%20Ruslan%20Salakhutdinov%20and%20Jeff%20Schneider%20and%20Andrea%20Zanette&entry.1292438233=%20%20Scaling%20the%20performance%20of%20large%20language%20models%20%28LLMs%29%20increasingly%20depends%0Aon%20methods%20that%20reduce%20reliance%20on%20human%20supervision.%20Reinforcement%20learning%0Afrom%20automated%20verification%20offers%20an%20alternative%2C%20but%20it%20incurs%20scalability%0Alimitations%20due%20to%20dependency%20upon%20human-designed%20verifiers.%20Self-training%2C%0Awhere%20the%20model%27s%20own%20judgment%20provides%20the%20supervisory%20signal%2C%20presents%20a%0Acompelling%20direction.%20We%20propose%20an%20online%20self-training%20reinforcement%20learning%0Aalgorithm%20that%20leverages%20the%20model%27s%20self-consistency%20to%20infer%20correctness%0Asignals%20and%20train%20without%20any%20ground-truth%20supervision.%20We%20apply%20the%20algorithm%0Ato%20challenging%20mathematical%20reasoning%20tasks%20and%20show%20that%20it%20quickly%20reaches%0Aperformance%20levels%20rivaling%20reinforcement-learning%20methods%20trained%20explicitly%0Aon%20gold-standard%20answers.%20Additionally%2C%20we%20analyze%20inherent%20limitations%20of%20the%0Aalgorithm%2C%20highlighting%20how%20the%20self-generated%20proxy%20reward%20initially%0Acorrelated%20with%20correctness%20can%20incentivize%20reward%20hacking%2C%20where%20confidently%0Aincorrect%20outputs%20are%20favored.%20Our%20results%20illustrate%20how%20self-supervised%0Aimprovement%20can%20achieve%20significant%20performance%20gains%20without%20external%20labels%2C%0Awhile%20also%20revealing%20its%20fundamental%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21444v1&entry.124074799=Read"},
{"title": "GSAT: Graph Structure Attention Networks", "author": "Farshad Noravesh and Reza Haffari and Layki Soon and Arghya Pal", "abstract": "  Graph Neural Networks (GNNs) have emerged as a powerful tool for processing\ndata represented in graph structures, achieving remarkable success across a\nwide range of applications. However, to further improve the performance on\ngraph classification benchmarks, structural representation of each node that\nencodes rich local topological information in the neighbourhood of nodes is an\nimportant type of feature that is often overlooked in the modeling. The\nconsequence of neglecting the structural information has resulted high number\nof layers to connect messages from distant nodes which by itself produces other\nproblems such as oversmoothing. In the present paper, we leverage these\nstructural information that are modeled by anonymous random walks (ARWs) and\nintroduce graph structure attention network (GSAT) which is a generalization of\ngraph attention network(GAT) to integrate the original attribute and the\nstructural representation to enforce the model to automatically find patterns\nfor attending to different edges in the node neighbourhood to enrich graph\nrepresentation. Our experiments show GSAT slightly improves SOTA on some graph\nclassification benchmarks.\n", "link": "http://arxiv.org/abs/2505.21288v1", "date": "2025-05-27", "relevancy": 2.3943, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4891}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4845}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSAT%3A%20Graph%20Structure%20Attention%20Networks&body=Title%3A%20GSAT%3A%20Graph%20Structure%20Attention%20Networks%0AAuthor%3A%20Farshad%20Noravesh%20and%20Reza%20Haffari%20and%20Layki%20Soon%20and%20Arghya%20Pal%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20processing%0Adata%20represented%20in%20graph%20structures%2C%20achieving%20remarkable%20success%20across%20a%0Awide%20range%20of%20applications.%20However%2C%20to%20further%20improve%20the%20performance%20on%0Agraph%20classification%20benchmarks%2C%20structural%20representation%20of%20each%20node%20that%0Aencodes%20rich%20local%20topological%20information%20in%20the%20neighbourhood%20of%20nodes%20is%20an%0Aimportant%20type%20of%20feature%20that%20is%20often%20overlooked%20in%20the%20modeling.%20The%0Aconsequence%20of%20neglecting%20the%20structural%20information%20has%20resulted%20high%20number%0Aof%20layers%20to%20connect%20messages%20from%20distant%20nodes%20which%20by%20itself%20produces%20other%0Aproblems%20such%20as%20oversmoothing.%20In%20the%20present%20paper%2C%20we%20leverage%20these%0Astructural%20information%20that%20are%20modeled%20by%20anonymous%20random%20walks%20%28ARWs%29%20and%0Aintroduce%20graph%20structure%20attention%20network%20%28GSAT%29%20which%20is%20a%20generalization%20of%0Agraph%20attention%20network%28GAT%29%20to%20integrate%20the%20original%20attribute%20and%20the%0Astructural%20representation%20to%20enforce%20the%20model%20to%20automatically%20find%20patterns%0Afor%20attending%20to%20different%20edges%20in%20the%20node%20neighbourhood%20to%20enrich%20graph%0Arepresentation.%20Our%20experiments%20show%20GSAT%20slightly%20improves%20SOTA%20on%20some%20graph%0Aclassification%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSAT%253A%2520Graph%2520Structure%2520Attention%2520Networks%26entry.906535625%3DFarshad%2520Noravesh%2520and%2520Reza%2520Haffari%2520and%2520Layki%2520Soon%2520and%2520Arghya%2520Pal%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520processing%250Adata%2520represented%2520in%2520graph%2520structures%252C%2520achieving%2520remarkable%2520success%2520across%2520a%250Awide%2520range%2520of%2520applications.%2520However%252C%2520to%2520further%2520improve%2520the%2520performance%2520on%250Agraph%2520classification%2520benchmarks%252C%2520structural%2520representation%2520of%2520each%2520node%2520that%250Aencodes%2520rich%2520local%2520topological%2520information%2520in%2520the%2520neighbourhood%2520of%2520nodes%2520is%2520an%250Aimportant%2520type%2520of%2520feature%2520that%2520is%2520often%2520overlooked%2520in%2520the%2520modeling.%2520The%250Aconsequence%2520of%2520neglecting%2520the%2520structural%2520information%2520has%2520resulted%2520high%2520number%250Aof%2520layers%2520to%2520connect%2520messages%2520from%2520distant%2520nodes%2520which%2520by%2520itself%2520produces%2520other%250Aproblems%2520such%2520as%2520oversmoothing.%2520In%2520the%2520present%2520paper%252C%2520we%2520leverage%2520these%250Astructural%2520information%2520that%2520are%2520modeled%2520by%2520anonymous%2520random%2520walks%2520%2528ARWs%2529%2520and%250Aintroduce%2520graph%2520structure%2520attention%2520network%2520%2528GSAT%2529%2520which%2520is%2520a%2520generalization%2520of%250Agraph%2520attention%2520network%2528GAT%2529%2520to%2520integrate%2520the%2520original%2520attribute%2520and%2520the%250Astructural%2520representation%2520to%2520enforce%2520the%2520model%2520to%2520automatically%2520find%2520patterns%250Afor%2520attending%2520to%2520different%2520edges%2520in%2520the%2520node%2520neighbourhood%2520to%2520enrich%2520graph%250Arepresentation.%2520Our%2520experiments%2520show%2520GSAT%2520slightly%2520improves%2520SOTA%2520on%2520some%2520graph%250Aclassification%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSAT%3A%20Graph%20Structure%20Attention%20Networks&entry.906535625=Farshad%20Noravesh%20and%20Reza%20Haffari%20and%20Layki%20Soon%20and%20Arghya%20Pal&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20processing%0Adata%20represented%20in%20graph%20structures%2C%20achieving%20remarkable%20success%20across%20a%0Awide%20range%20of%20applications.%20However%2C%20to%20further%20improve%20the%20performance%20on%0Agraph%20classification%20benchmarks%2C%20structural%20representation%20of%20each%20node%20that%0Aencodes%20rich%20local%20topological%20information%20in%20the%20neighbourhood%20of%20nodes%20is%20an%0Aimportant%20type%20of%20feature%20that%20is%20often%20overlooked%20in%20the%20modeling.%20The%0Aconsequence%20of%20neglecting%20the%20structural%20information%20has%20resulted%20high%20number%0Aof%20layers%20to%20connect%20messages%20from%20distant%20nodes%20which%20by%20itself%20produces%20other%0Aproblems%20such%20as%20oversmoothing.%20In%20the%20present%20paper%2C%20we%20leverage%20these%0Astructural%20information%20that%20are%20modeled%20by%20anonymous%20random%20walks%20%28ARWs%29%20and%0Aintroduce%20graph%20structure%20attention%20network%20%28GSAT%29%20which%20is%20a%20generalization%20of%0Agraph%20attention%20network%28GAT%29%20to%20integrate%20the%20original%20attribute%20and%20the%0Astructural%20representation%20to%20enforce%20the%20model%20to%20automatically%20find%20patterns%0Afor%20attending%20to%20different%20edges%20in%20the%20node%20neighbourhood%20to%20enrich%20graph%0Arepresentation.%20Our%20experiments%20show%20GSAT%20slightly%20improves%20SOTA%20on%20some%20graph%0Aclassification%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21288v1&entry.124074799=Read"},
{"title": "LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning", "author": "Nurbek Tastan and Stefanos Laskaridis and Martin Takac and Karthik Nandakumar and Samuel Horvath", "abstract": "  Large pre-trained models are commonly adapted to downstream tasks using\nparameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),\nwhich injects small trainable low-rank matrices instead of updating all\nweights. While LoRA dramatically reduces trainable parameters with little\noverhead, it can still underperform full fine-tuning in accuracy and often\nconverges more slowly. We introduce LoFT, a novel low-rank adaptation method\nthat behaves like full fine-tuning by aligning the optimizer's internal\ndynamics with those of updating all model weights. LoFT not only learns weight\nupdates in a low-rank subspace (like LoRA) but also properly projects the\noptimizer's first and second moments (Adam's momentum and variance) into the\nsame subspace, mirroring full-model updates. By aligning the low-rank update\nitself with the full update, LoFT eliminates the need for tuning extra\nhyperparameters, e.g., LoRA scaling factor $\\alpha$. Empirically, this approach\nsubstantially narrows the performance gap between adapter-based tuning and full\nfine-tuning and consistently outperforms standard LoRA-style methods, all\nwithout increasing inference cost.\n", "link": "http://arxiv.org/abs/2505.21289v1", "date": "2025-05-27", "relevancy": 2.3926, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.485}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4819}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoFT%3A%20Low-Rank%20Adaptation%20That%20Behaves%20Like%20Full%20Fine-Tuning&body=Title%3A%20LoFT%3A%20Low-Rank%20Adaptation%20That%20Behaves%20Like%20Full%20Fine-Tuning%0AAuthor%3A%20Nurbek%20Tastan%20and%20Stefanos%20Laskaridis%20and%20Martin%20Takac%20and%20Karthik%20Nandakumar%20and%20Samuel%20Horvath%0AAbstract%3A%20%20%20Large%20pre-trained%20models%20are%20commonly%20adapted%20to%20downstream%20tasks%20using%0Aparameter-efficient%20fine-tuning%20methods%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%0Awhich%20injects%20small%20trainable%20low-rank%20matrices%20instead%20of%20updating%20all%0Aweights.%20While%20LoRA%20dramatically%20reduces%20trainable%20parameters%20with%20little%0Aoverhead%2C%20it%20can%20still%20underperform%20full%20fine-tuning%20in%20accuracy%20and%20often%0Aconverges%20more%20slowly.%20We%20introduce%20LoFT%2C%20a%20novel%20low-rank%20adaptation%20method%0Athat%20behaves%20like%20full%20fine-tuning%20by%20aligning%20the%20optimizer%27s%20internal%0Adynamics%20with%20those%20of%20updating%20all%20model%20weights.%20LoFT%20not%20only%20learns%20weight%0Aupdates%20in%20a%20low-rank%20subspace%20%28like%20LoRA%29%20but%20also%20properly%20projects%20the%0Aoptimizer%27s%20first%20and%20second%20moments%20%28Adam%27s%20momentum%20and%20variance%29%20into%20the%0Asame%20subspace%2C%20mirroring%20full-model%20updates.%20By%20aligning%20the%20low-rank%20update%0Aitself%20with%20the%20full%20update%2C%20LoFT%20eliminates%20the%20need%20for%20tuning%20extra%0Ahyperparameters%2C%20e.g.%2C%20LoRA%20scaling%20factor%20%24%5Calpha%24.%20Empirically%2C%20this%20approach%0Asubstantially%20narrows%20the%20performance%20gap%20between%20adapter-based%20tuning%20and%20full%0Afine-tuning%20and%20consistently%20outperforms%20standard%20LoRA-style%20methods%2C%20all%0Awithout%20increasing%20inference%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoFT%253A%2520Low-Rank%2520Adaptation%2520That%2520Behaves%2520Like%2520Full%2520Fine-Tuning%26entry.906535625%3DNurbek%2520Tastan%2520and%2520Stefanos%2520Laskaridis%2520and%2520Martin%2520Takac%2520and%2520Karthik%2520Nandakumar%2520and%2520Samuel%2520Horvath%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520models%2520are%2520commonly%2520adapted%2520to%2520downstream%2520tasks%2520using%250Aparameter-efficient%2520fine-tuning%2520methods%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%250Awhich%2520injects%2520small%2520trainable%2520low-rank%2520matrices%2520instead%2520of%2520updating%2520all%250Aweights.%2520While%2520LoRA%2520dramatically%2520reduces%2520trainable%2520parameters%2520with%2520little%250Aoverhead%252C%2520it%2520can%2520still%2520underperform%2520full%2520fine-tuning%2520in%2520accuracy%2520and%2520often%250Aconverges%2520more%2520slowly.%2520We%2520introduce%2520LoFT%252C%2520a%2520novel%2520low-rank%2520adaptation%2520method%250Athat%2520behaves%2520like%2520full%2520fine-tuning%2520by%2520aligning%2520the%2520optimizer%2527s%2520internal%250Adynamics%2520with%2520those%2520of%2520updating%2520all%2520model%2520weights.%2520LoFT%2520not%2520only%2520learns%2520weight%250Aupdates%2520in%2520a%2520low-rank%2520subspace%2520%2528like%2520LoRA%2529%2520but%2520also%2520properly%2520projects%2520the%250Aoptimizer%2527s%2520first%2520and%2520second%2520moments%2520%2528Adam%2527s%2520momentum%2520and%2520variance%2529%2520into%2520the%250Asame%2520subspace%252C%2520mirroring%2520full-model%2520updates.%2520By%2520aligning%2520the%2520low-rank%2520update%250Aitself%2520with%2520the%2520full%2520update%252C%2520LoFT%2520eliminates%2520the%2520need%2520for%2520tuning%2520extra%250Ahyperparameters%252C%2520e.g.%252C%2520LoRA%2520scaling%2520factor%2520%2524%255Calpha%2524.%2520Empirically%252C%2520this%2520approach%250Asubstantially%2520narrows%2520the%2520performance%2520gap%2520between%2520adapter-based%2520tuning%2520and%2520full%250Afine-tuning%2520and%2520consistently%2520outperforms%2520standard%2520LoRA-style%2520methods%252C%2520all%250Awithout%2520increasing%2520inference%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoFT%3A%20Low-Rank%20Adaptation%20That%20Behaves%20Like%20Full%20Fine-Tuning&entry.906535625=Nurbek%20Tastan%20and%20Stefanos%20Laskaridis%20and%20Martin%20Takac%20and%20Karthik%20Nandakumar%20and%20Samuel%20Horvath&entry.1292438233=%20%20Large%20pre-trained%20models%20are%20commonly%20adapted%20to%20downstream%20tasks%20using%0Aparameter-efficient%20fine-tuning%20methods%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%0Awhich%20injects%20small%20trainable%20low-rank%20matrices%20instead%20of%20updating%20all%0Aweights.%20While%20LoRA%20dramatically%20reduces%20trainable%20parameters%20with%20little%0Aoverhead%2C%20it%20can%20still%20underperform%20full%20fine-tuning%20in%20accuracy%20and%20often%0Aconverges%20more%20slowly.%20We%20introduce%20LoFT%2C%20a%20novel%20low-rank%20adaptation%20method%0Athat%20behaves%20like%20full%20fine-tuning%20by%20aligning%20the%20optimizer%27s%20internal%0Adynamics%20with%20those%20of%20updating%20all%20model%20weights.%20LoFT%20not%20only%20learns%20weight%0Aupdates%20in%20a%20low-rank%20subspace%20%28like%20LoRA%29%20but%20also%20properly%20projects%20the%0Aoptimizer%27s%20first%20and%20second%20moments%20%28Adam%27s%20momentum%20and%20variance%29%20into%20the%0Asame%20subspace%2C%20mirroring%20full-model%20updates.%20By%20aligning%20the%20low-rank%20update%0Aitself%20with%20the%20full%20update%2C%20LoFT%20eliminates%20the%20need%20for%20tuning%20extra%0Ahyperparameters%2C%20e.g.%2C%20LoRA%20scaling%20factor%20%24%5Calpha%24.%20Empirically%2C%20this%20approach%0Asubstantially%20narrows%20the%20performance%20gap%20between%20adapter-based%20tuning%20and%20full%0Afine-tuning%20and%20consistently%20outperforms%20standard%20LoRA-style%20methods%2C%20all%0Awithout%20increasing%20inference%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21289v1&entry.124074799=Read"},
{"title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT\n  Reasoning", "author": "Cehao Yang and Xueyuan Lin and Chengjin Xu and Xuhui Jiang and Xiaojun Wu and Honghao Liu and Hui Xiong and Jian Guo", "abstract": "  A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost.\n", "link": "http://arxiv.org/abs/2505.17266v2", "date": "2025-05-27", "relevancy": 2.3805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Select2Reason%3A%20Efficient%20Instruction-Tuning%20Data%20Selection%20for%20Long-CoT%0A%20%20Reasoning&body=Title%3A%20Select2Reason%3A%20Efficient%20Instruction-Tuning%20Data%20Selection%20for%20Long-CoT%0A%20%20Reasoning%0AAuthor%3A%20Cehao%20Yang%20and%20Xueyuan%20Lin%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Xiaojun%20Wu%20and%20Honghao%20Liu%20and%20Hui%20Xiong%20and%20Jian%20Guo%0AAbstract%3A%20%20%20A%20practical%20approach%20to%20activate%20long%20chain-of-thoughts%20reasoning%20ability%20in%0Apre-trained%20large%20language%20models%20is%20to%20perform%20supervised%20fine-tuning%20on%0Ainstruction%20datasets%20synthesized%20by%20strong%20Large%20Reasoning%20Models%20such%20as%0ADeepSeek-R1%2C%20offering%20a%20cost-effective%20alternative%20to%20reinforcement%20learning.%0AHowever%2C%20large-scale%20instruction%20sets%20with%20more%20than%20100k%20samples%20incur%0Asignificant%20training%20overhead%2C%20while%20effective%20strategies%20for%20automatic%0Along-CoT%20instruction%20selection%20still%20remain%20unexplored.%20In%20this%20work%2C%20we%0Apropose%20Select2Reason%2C%20a%20novel%20and%20efficient%20instruction-tuning%20data%20selection%0Aframework%20for%20long-CoT%20reasoning.%20From%20the%20perspective%20of%20emergence%20of%0Arethinking%20behaviors%20like%20self-correction%20and%20backtracking%2C%20we%20investigate%0Acommon%20metrics%20that%20may%20determine%20the%20quality%20of%20long-CoT%20reasoning%0Ainstructions.%20Select2Reason%20leverages%20a%20quantifier%20to%20estimate%20difficulty%20of%0Aquestion%20and%20jointly%20incorporates%20a%20reasoning%20trace%20length-based%20heuristic%0Athrough%20a%20weighted%20scheme%20for%20ranking%20to%20prioritize%20high-utility%20examples.%0AEmpirical%20results%20on%20OpenR1-Math-220k%20demonstrate%20that%20fine-tuning%20LLM%20on%20only%0A10%25%20of%20the%20data%20selected%20by%20Select2Reason%20achieves%20performance%20competitive%20with%0Aor%20superior%20to%20full-data%20tuning%20and%20open-source%20baseline%20OpenR1-Qwen-7B%20across%0Athree%20competition-level%20and%20six%20comprehensive%20mathematical%20benchmarks.%20Further%0Aexperiments%20highlight%20the%20scalability%20in%20varying%20data%20size%2C%20efficiency%20during%0Ainference%2C%20and%20its%20adaptability%20to%20other%20instruction%20pools%20with%20minimal%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17266v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelect2Reason%253A%2520Efficient%2520Instruction-Tuning%2520Data%2520Selection%2520for%2520Long-CoT%250A%2520%2520Reasoning%26entry.906535625%3DCehao%2520Yang%2520and%2520Xueyuan%2520Lin%2520and%2520Chengjin%2520Xu%2520and%2520Xuhui%2520Jiang%2520and%2520Xiaojun%2520Wu%2520and%2520Honghao%2520Liu%2520and%2520Hui%2520Xiong%2520and%2520Jian%2520Guo%26entry.1292438233%3D%2520%2520A%2520practical%2520approach%2520to%2520activate%2520long%2520chain-of-thoughts%2520reasoning%2520ability%2520in%250Apre-trained%2520large%2520language%2520models%2520is%2520to%2520perform%2520supervised%2520fine-tuning%2520on%250Ainstruction%2520datasets%2520synthesized%2520by%2520strong%2520Large%2520Reasoning%2520Models%2520such%2520as%250ADeepSeek-R1%252C%2520offering%2520a%2520cost-effective%2520alternative%2520to%2520reinforcement%2520learning.%250AHowever%252C%2520large-scale%2520instruction%2520sets%2520with%2520more%2520than%2520100k%2520samples%2520incur%250Asignificant%2520training%2520overhead%252C%2520while%2520effective%2520strategies%2520for%2520automatic%250Along-CoT%2520instruction%2520selection%2520still%2520remain%2520unexplored.%2520In%2520this%2520work%252C%2520we%250Apropose%2520Select2Reason%252C%2520a%2520novel%2520and%2520efficient%2520instruction-tuning%2520data%2520selection%250Aframework%2520for%2520long-CoT%2520reasoning.%2520From%2520the%2520perspective%2520of%2520emergence%2520of%250Arethinking%2520behaviors%2520like%2520self-correction%2520and%2520backtracking%252C%2520we%2520investigate%250Acommon%2520metrics%2520that%2520may%2520determine%2520the%2520quality%2520of%2520long-CoT%2520reasoning%250Ainstructions.%2520Select2Reason%2520leverages%2520a%2520quantifier%2520to%2520estimate%2520difficulty%2520of%250Aquestion%2520and%2520jointly%2520incorporates%2520a%2520reasoning%2520trace%2520length-based%2520heuristic%250Athrough%2520a%2520weighted%2520scheme%2520for%2520ranking%2520to%2520prioritize%2520high-utility%2520examples.%250AEmpirical%2520results%2520on%2520OpenR1-Math-220k%2520demonstrate%2520that%2520fine-tuning%2520LLM%2520on%2520only%250A10%2525%2520of%2520the%2520data%2520selected%2520by%2520Select2Reason%2520achieves%2520performance%2520competitive%2520with%250Aor%2520superior%2520to%2520full-data%2520tuning%2520and%2520open-source%2520baseline%2520OpenR1-Qwen-7B%2520across%250Athree%2520competition-level%2520and%2520six%2520comprehensive%2520mathematical%2520benchmarks.%2520Further%250Aexperiments%2520highlight%2520the%2520scalability%2520in%2520varying%2520data%2520size%252C%2520efficiency%2520during%250Ainference%252C%2520and%2520its%2520adaptability%2520to%2520other%2520instruction%2520pools%2520with%2520minimal%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17266v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Select2Reason%3A%20Efficient%20Instruction-Tuning%20Data%20Selection%20for%20Long-CoT%0A%20%20Reasoning&entry.906535625=Cehao%20Yang%20and%20Xueyuan%20Lin%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Xiaojun%20Wu%20and%20Honghao%20Liu%20and%20Hui%20Xiong%20and%20Jian%20Guo&entry.1292438233=%20%20A%20practical%20approach%20to%20activate%20long%20chain-of-thoughts%20reasoning%20ability%20in%0Apre-trained%20large%20language%20models%20is%20to%20perform%20supervised%20fine-tuning%20on%0Ainstruction%20datasets%20synthesized%20by%20strong%20Large%20Reasoning%20Models%20such%20as%0ADeepSeek-R1%2C%20offering%20a%20cost-effective%20alternative%20to%20reinforcement%20learning.%0AHowever%2C%20large-scale%20instruction%20sets%20with%20more%20than%20100k%20samples%20incur%0Asignificant%20training%20overhead%2C%20while%20effective%20strategies%20for%20automatic%0Along-CoT%20instruction%20selection%20still%20remain%20unexplored.%20In%20this%20work%2C%20we%0Apropose%20Select2Reason%2C%20a%20novel%20and%20efficient%20instruction-tuning%20data%20selection%0Aframework%20for%20long-CoT%20reasoning.%20From%20the%20perspective%20of%20emergence%20of%0Arethinking%20behaviors%20like%20self-correction%20and%20backtracking%2C%20we%20investigate%0Acommon%20metrics%20that%20may%20determine%20the%20quality%20of%20long-CoT%20reasoning%0Ainstructions.%20Select2Reason%20leverages%20a%20quantifier%20to%20estimate%20difficulty%20of%0Aquestion%20and%20jointly%20incorporates%20a%20reasoning%20trace%20length-based%20heuristic%0Athrough%20a%20weighted%20scheme%20for%20ranking%20to%20prioritize%20high-utility%20examples.%0AEmpirical%20results%20on%20OpenR1-Math-220k%20demonstrate%20that%20fine-tuning%20LLM%20on%20only%0A10%25%20of%20the%20data%20selected%20by%20Select2Reason%20achieves%20performance%20competitive%20with%0Aor%20superior%20to%20full-data%20tuning%20and%20open-source%20baseline%20OpenR1-Qwen-7B%20across%0Athree%20competition-level%20and%20six%20comprehensive%20mathematical%20benchmarks.%20Further%0Aexperiments%20highlight%20the%20scalability%20in%20varying%20data%20size%2C%20efficiency%20during%0Ainference%2C%20and%20its%20adaptability%20to%20other%20instruction%20pools%20with%20minimal%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17266v2&entry.124074799=Read"},
{"title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray\n  Interpretation", "author": "Jong Hak Moon and Geon Choi and Paloma Rabaey and Min Gwan Kim and Hyuk Gi Hong and Jung-Oh Lee and Hangyul Yoon and Eun Woo Doe and Jiyoun Kim and Harshita Sharma and Daniel C. Castro and Javier Alvarez-Valle and Edward Choi", "abstract": "  Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage\n", "link": "http://arxiv.org/abs/2505.21190v1", "date": "2025-05-27", "relevancy": 2.3715, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lunguage%3A%20A%20Benchmark%20for%20Structured%20and%20Sequential%20Chest%20X-ray%0A%20%20Interpretation&body=Title%3A%20Lunguage%3A%20A%20Benchmark%20for%20Structured%20and%20Sequential%20Chest%20X-ray%0A%20%20Interpretation%0AAuthor%3A%20Jong%20Hak%20Moon%20and%20Geon%20Choi%20and%20Paloma%20Rabaey%20and%20Min%20Gwan%20Kim%20and%20Hyuk%20Gi%20Hong%20and%20Jung-Oh%20Lee%20and%20Hangyul%20Yoon%20and%20Eun%20Woo%20Doe%20and%20Jiyoun%20Kim%20and%20Harshita%20Sharma%20and%20Daniel%20C.%20Castro%20and%20Javier%20Alvarez-Valle%20and%20Edward%20Choi%0AAbstract%3A%20%20%20Radiology%20reports%20convey%20detailed%20clinical%20observations%20and%20capture%0Adiagnostic%20reasoning%20that%20evolves%20over%20time.%20However%2C%20existing%20evaluation%0Amethods%20are%20limited%20to%20single-report%20settings%20and%20rely%20on%20coarse%20metrics%20that%0Afail%20to%20capture%20fine-grained%20clinical%20semantics%20and%20temporal%20dependencies.%20We%0Aintroduce%20LUNGUAGE%2Ca%20benchmark%20dataset%20for%20structured%20radiology%20report%0Ageneration%20that%20supports%20both%20single-report%20evaluation%20and%20longitudinal%0Apatient-level%20assessment%20across%20multiple%20studies.%20It%20contains%201%2C473%20annotated%0Achest%20X-ray%20reports%2C%20each%20reviewed%20by%20experts%2C%20and%2080%20of%20them%20contain%0Alongitudinal%20annotations%20to%20capture%20disease%20progression%20and%20inter-study%0Aintervals%2C%20also%20reviewed%20by%20experts.%20Using%20this%20benchmark%2C%20we%20develop%20a%0Atwo-stage%20framework%20that%20transforms%20generated%20reports%20into%20fine-grained%2C%0Aschema-aligned%20structured%20representations%2C%20enabling%20longitudinal%0Ainterpretation.%20We%20also%20propose%20LUNGUAGESCORE%2C%20an%20interpretable%20metric%20that%0Acompares%20structured%20outputs%20at%20the%20entity%2C%20relation%2C%20and%20attribute%20level%20while%0Amodeling%20temporal%20consistency%20across%20patient%20timelines.%20These%20contributions%0Aestablish%20the%20first%20benchmark%20dataset%2C%20structuring%20framework%2C%20and%20evaluation%0Ametric%20for%20sequential%20radiology%20reporting%2C%20with%20empirical%20results%20demonstrating%0Athat%20LUNGUAGESCORE%20effectively%20supports%20structured%20report%20evaluation.%20The%20code%0Ais%20available%20at%3A%20https%3A//github.com/SuperSupermoon/Lunguage%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLunguage%253A%2520A%2520Benchmark%2520for%2520Structured%2520and%2520Sequential%2520Chest%2520X-ray%250A%2520%2520Interpretation%26entry.906535625%3DJong%2520Hak%2520Moon%2520and%2520Geon%2520Choi%2520and%2520Paloma%2520Rabaey%2520and%2520Min%2520Gwan%2520Kim%2520and%2520Hyuk%2520Gi%2520Hong%2520and%2520Jung-Oh%2520Lee%2520and%2520Hangyul%2520Yoon%2520and%2520Eun%2520Woo%2520Doe%2520and%2520Jiyoun%2520Kim%2520and%2520Harshita%2520Sharma%2520and%2520Daniel%2520C.%2520Castro%2520and%2520Javier%2520Alvarez-Valle%2520and%2520Edward%2520Choi%26entry.1292438233%3D%2520%2520Radiology%2520reports%2520convey%2520detailed%2520clinical%2520observations%2520and%2520capture%250Adiagnostic%2520reasoning%2520that%2520evolves%2520over%2520time.%2520However%252C%2520existing%2520evaluation%250Amethods%2520are%2520limited%2520to%2520single-report%2520settings%2520and%2520rely%2520on%2520coarse%2520metrics%2520that%250Afail%2520to%2520capture%2520fine-grained%2520clinical%2520semantics%2520and%2520temporal%2520dependencies.%2520We%250Aintroduce%2520LUNGUAGE%252Ca%2520benchmark%2520dataset%2520for%2520structured%2520radiology%2520report%250Ageneration%2520that%2520supports%2520both%2520single-report%2520evaluation%2520and%2520longitudinal%250Apatient-level%2520assessment%2520across%2520multiple%2520studies.%2520It%2520contains%25201%252C473%2520annotated%250Achest%2520X-ray%2520reports%252C%2520each%2520reviewed%2520by%2520experts%252C%2520and%252080%2520of%2520them%2520contain%250Alongitudinal%2520annotations%2520to%2520capture%2520disease%2520progression%2520and%2520inter-study%250Aintervals%252C%2520also%2520reviewed%2520by%2520experts.%2520Using%2520this%2520benchmark%252C%2520we%2520develop%2520a%250Atwo-stage%2520framework%2520that%2520transforms%2520generated%2520reports%2520into%2520fine-grained%252C%250Aschema-aligned%2520structured%2520representations%252C%2520enabling%2520longitudinal%250Ainterpretation.%2520We%2520also%2520propose%2520LUNGUAGESCORE%252C%2520an%2520interpretable%2520metric%2520that%250Acompares%2520structured%2520outputs%2520at%2520the%2520entity%252C%2520relation%252C%2520and%2520attribute%2520level%2520while%250Amodeling%2520temporal%2520consistency%2520across%2520patient%2520timelines.%2520These%2520contributions%250Aestablish%2520the%2520first%2520benchmark%2520dataset%252C%2520structuring%2520framework%252C%2520and%2520evaluation%250Ametric%2520for%2520sequential%2520radiology%2520reporting%252C%2520with%2520empirical%2520results%2520demonstrating%250Athat%2520LUNGUAGESCORE%2520effectively%2520supports%2520structured%2520report%2520evaluation.%2520The%2520code%250Ais%2520available%2520at%253A%2520https%253A//github.com/SuperSupermoon/Lunguage%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lunguage%3A%20A%20Benchmark%20for%20Structured%20and%20Sequential%20Chest%20X-ray%0A%20%20Interpretation&entry.906535625=Jong%20Hak%20Moon%20and%20Geon%20Choi%20and%20Paloma%20Rabaey%20and%20Min%20Gwan%20Kim%20and%20Hyuk%20Gi%20Hong%20and%20Jung-Oh%20Lee%20and%20Hangyul%20Yoon%20and%20Eun%20Woo%20Doe%20and%20Jiyoun%20Kim%20and%20Harshita%20Sharma%20and%20Daniel%20C.%20Castro%20and%20Javier%20Alvarez-Valle%20and%20Edward%20Choi&entry.1292438233=%20%20Radiology%20reports%20convey%20detailed%20clinical%20observations%20and%20capture%0Adiagnostic%20reasoning%20that%20evolves%20over%20time.%20However%2C%20existing%20evaluation%0Amethods%20are%20limited%20to%20single-report%20settings%20and%20rely%20on%20coarse%20metrics%20that%0Afail%20to%20capture%20fine-grained%20clinical%20semantics%20and%20temporal%20dependencies.%20We%0Aintroduce%20LUNGUAGE%2Ca%20benchmark%20dataset%20for%20structured%20radiology%20report%0Ageneration%20that%20supports%20both%20single-report%20evaluation%20and%20longitudinal%0Apatient-level%20assessment%20across%20multiple%20studies.%20It%20contains%201%2C473%20annotated%0Achest%20X-ray%20reports%2C%20each%20reviewed%20by%20experts%2C%20and%2080%20of%20them%20contain%0Alongitudinal%20annotations%20to%20capture%20disease%20progression%20and%20inter-study%0Aintervals%2C%20also%20reviewed%20by%20experts.%20Using%20this%20benchmark%2C%20we%20develop%20a%0Atwo-stage%20framework%20that%20transforms%20generated%20reports%20into%20fine-grained%2C%0Aschema-aligned%20structured%20representations%2C%20enabling%20longitudinal%0Ainterpretation.%20We%20also%20propose%20LUNGUAGESCORE%2C%20an%20interpretable%20metric%20that%0Acompares%20structured%20outputs%20at%20the%20entity%2C%20relation%2C%20and%20attribute%20level%20while%0Amodeling%20temporal%20consistency%20across%20patient%20timelines.%20These%20contributions%0Aestablish%20the%20first%20benchmark%20dataset%2C%20structuring%20framework%2C%20and%20evaluation%0Ametric%20for%20sequential%20radiology%20reporting%2C%20with%20empirical%20results%20demonstrating%0Athat%20LUNGUAGESCORE%20effectively%20supports%20structured%20report%20evaluation.%20The%20code%0Ais%20available%20at%3A%20https%3A//github.com/SuperSupermoon/Lunguage%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21190v1&entry.124074799=Read"},
{"title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective", "author": "Shimao Zhang and Zhejian Lai and Xiang Liu and Shuaijie She and Xiao Liu and Yeyun Gong and Shujian Huang and Jiajun Chen", "abstract": "  Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.\n", "link": "http://arxiv.org/abs/2505.21505v1", "date": "2025-05-27", "relevancy": 2.3705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20does%20Alignment%20Enhance%20LLMs%27%20Multilingual%20Capabilities%3F%20A%20Language%0A%20%20Neurons%20Perspective&body=Title%3A%20How%20does%20Alignment%20Enhance%20LLMs%27%20Multilingual%20Capabilities%3F%20A%20Language%0A%20%20Neurons%20Perspective%0AAuthor%3A%20Shimao%20Zhang%20and%20Zhejian%20Lai%20and%20Xiang%20Liu%20and%20Shuaijie%20She%20and%20Xiao%20Liu%20and%20Yeyun%20Gong%20and%20Shujian%20Huang%20and%20Jiajun%20Chen%0AAbstract%3A%20%20%20Multilingual%20Alignment%20is%20an%20effective%20and%20representative%20paradigm%20to%20enhance%0ALLMs%27%20multilingual%20capabilities%2C%20which%20transfers%20the%20capabilities%20from%20the%0Ahigh-resource%20languages%20to%20the%20low-resource%20languages.%20Meanwhile%2C%20some%0Aresearches%20on%20language-specific%20neurons%20reveal%20that%20there%20are%20language-specific%0Aneurons%20that%20are%20selectively%20activated%20in%20LLMs%20when%20processing%20different%0Alanguages.%20This%20provides%20a%20new%20perspective%20to%20analyze%20and%20understand%20LLMs%27%0Amechanisms%20more%20specifically%20in%20multilingual%20scenarios.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20finer-grained%20neuron%20identification%20algorithm%2C%20which%20detects%0Alanguage%20neurons~%28including%20language-specific%20neurons%20and%20language-related%0Aneurons%29%20and%20language-agnostic%20neurons.%20Furthermore%2C%20based%20on%20the%0Adistributional%20characteristics%20of%20different%20types%20of%20neurons%2C%20we%20divide%20the%0ALLMs%27%20internal%20process%20for%20multilingual%20inference%20into%20four%20parts%3A%20%281%29%0Amultilingual%20understanding%2C%20%282%29%20shared%20semantic%20space%20reasoning%2C%20%283%29%0Amultilingual%20output%20space%20transformation%2C%20and%20%284%29%20vocabulary%20space%20outputting.%0AAdditionally%2C%20we%20systematically%20analyze%20the%20models%20before%20and%20after%20alignment%0Awith%20a%20focus%20on%20different%20types%20of%20neurons.%20We%20also%20analyze%20the%20phenomenon%20of%0A%27%27Spontaneous%20Multilingual%20Alignment%27%27.%20Overall%2C%20our%20work%20conducts%20a%0Acomprehensive%20investigation%20based%20on%20different%20types%20of%20neurons%2C%20providing%0Aempirical%20results%20and%20valuable%20insights%20for%20better%20understanding%20multilingual%0Aalignment%20and%20multilingual%20capabilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520does%2520Alignment%2520Enhance%2520LLMs%2527%2520Multilingual%2520Capabilities%253F%2520A%2520Language%250A%2520%2520Neurons%2520Perspective%26entry.906535625%3DShimao%2520Zhang%2520and%2520Zhejian%2520Lai%2520and%2520Xiang%2520Liu%2520and%2520Shuaijie%2520She%2520and%2520Xiao%2520Liu%2520and%2520Yeyun%2520Gong%2520and%2520Shujian%2520Huang%2520and%2520Jiajun%2520Chen%26entry.1292438233%3D%2520%2520Multilingual%2520Alignment%2520is%2520an%2520effective%2520and%2520representative%2520paradigm%2520to%2520enhance%250ALLMs%2527%2520multilingual%2520capabilities%252C%2520which%2520transfers%2520the%2520capabilities%2520from%2520the%250Ahigh-resource%2520languages%2520to%2520the%2520low-resource%2520languages.%2520Meanwhile%252C%2520some%250Aresearches%2520on%2520language-specific%2520neurons%2520reveal%2520that%2520there%2520are%2520language-specific%250Aneurons%2520that%2520are%2520selectively%2520activated%2520in%2520LLMs%2520when%2520processing%2520different%250Alanguages.%2520This%2520provides%2520a%2520new%2520perspective%2520to%2520analyze%2520and%2520understand%2520LLMs%2527%250Amechanisms%2520more%2520specifically%2520in%2520multilingual%2520scenarios.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520new%2520finer-grained%2520neuron%2520identification%2520algorithm%252C%2520which%2520detects%250Alanguage%2520neurons~%2528including%2520language-specific%2520neurons%2520and%2520language-related%250Aneurons%2529%2520and%2520language-agnostic%2520neurons.%2520Furthermore%252C%2520based%2520on%2520the%250Adistributional%2520characteristics%2520of%2520different%2520types%2520of%2520neurons%252C%2520we%2520divide%2520the%250ALLMs%2527%2520internal%2520process%2520for%2520multilingual%2520inference%2520into%2520four%2520parts%253A%2520%25281%2529%250Amultilingual%2520understanding%252C%2520%25282%2529%2520shared%2520semantic%2520space%2520reasoning%252C%2520%25283%2529%250Amultilingual%2520output%2520space%2520transformation%252C%2520and%2520%25284%2529%2520vocabulary%2520space%2520outputting.%250AAdditionally%252C%2520we%2520systematically%2520analyze%2520the%2520models%2520before%2520and%2520after%2520alignment%250Awith%2520a%2520focus%2520on%2520different%2520types%2520of%2520neurons.%2520We%2520also%2520analyze%2520the%2520phenomenon%2520of%250A%2527%2527Spontaneous%2520Multilingual%2520Alignment%2527%2527.%2520Overall%252C%2520our%2520work%2520conducts%2520a%250Acomprehensive%2520investigation%2520based%2520on%2520different%2520types%2520of%2520neurons%252C%2520providing%250Aempirical%2520results%2520and%2520valuable%2520insights%2520for%2520better%2520understanding%2520multilingual%250Aalignment%2520and%2520multilingual%2520capabilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20does%20Alignment%20Enhance%20LLMs%27%20Multilingual%20Capabilities%3F%20A%20Language%0A%20%20Neurons%20Perspective&entry.906535625=Shimao%20Zhang%20and%20Zhejian%20Lai%20and%20Xiang%20Liu%20and%20Shuaijie%20She%20and%20Xiao%20Liu%20and%20Yeyun%20Gong%20and%20Shujian%20Huang%20and%20Jiajun%20Chen&entry.1292438233=%20%20Multilingual%20Alignment%20is%20an%20effective%20and%20representative%20paradigm%20to%20enhance%0ALLMs%27%20multilingual%20capabilities%2C%20which%20transfers%20the%20capabilities%20from%20the%0Ahigh-resource%20languages%20to%20the%20low-resource%20languages.%20Meanwhile%2C%20some%0Aresearches%20on%20language-specific%20neurons%20reveal%20that%20there%20are%20language-specific%0Aneurons%20that%20are%20selectively%20activated%20in%20LLMs%20when%20processing%20different%0Alanguages.%20This%20provides%20a%20new%20perspective%20to%20analyze%20and%20understand%20LLMs%27%0Amechanisms%20more%20specifically%20in%20multilingual%20scenarios.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20finer-grained%20neuron%20identification%20algorithm%2C%20which%20detects%0Alanguage%20neurons~%28including%20language-specific%20neurons%20and%20language-related%0Aneurons%29%20and%20language-agnostic%20neurons.%20Furthermore%2C%20based%20on%20the%0Adistributional%20characteristics%20of%20different%20types%20of%20neurons%2C%20we%20divide%20the%0ALLMs%27%20internal%20process%20for%20multilingual%20inference%20into%20four%20parts%3A%20%281%29%0Amultilingual%20understanding%2C%20%282%29%20shared%20semantic%20space%20reasoning%2C%20%283%29%0Amultilingual%20output%20space%20transformation%2C%20and%20%284%29%20vocabulary%20space%20outputting.%0AAdditionally%2C%20we%20systematically%20analyze%20the%20models%20before%20and%20after%20alignment%0Awith%20a%20focus%20on%20different%20types%20of%20neurons.%20We%20also%20analyze%20the%20phenomenon%20of%0A%27%27Spontaneous%20Multilingual%20Alignment%27%27.%20Overall%2C%20our%20work%20conducts%20a%0Acomprehensive%20investigation%20based%20on%20different%20types%20of%20neurons%2C%20providing%0Aempirical%20results%20and%20valuable%20insights%20for%20better%20understanding%20multilingual%0Aalignment%20and%20multilingual%20capabilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21505v1&entry.124074799=Read"},
{"title": "Does quantization affect models' performance on long-context tasks?", "author": "Anmol Mekala and Anirudh Atmakuru and Yixiao Song and Marzena Karpinska and Mohit Iyyer", "abstract": "  Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English.\n", "link": "http://arxiv.org/abs/2505.20276v2", "date": "2025-05-27", "relevancy": 2.3642, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20quantization%20affect%20models%27%20performance%20on%20long-context%20tasks%3F&body=Title%3A%20Does%20quantization%20affect%20models%27%20performance%20on%20long-context%20tasks%3F%0AAuthor%3A%20Anmol%20Mekala%20and%20Anirudh%20Atmakuru%20and%20Yixiao%20Song%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20now%20support%20context%20windows%20exceeding%20128K%0Atokens%2C%20but%20this%20comes%20with%20significant%20memory%20requirements%20and%20high%20inference%0Alatency.%20Quantization%20can%20mitigate%20these%20costs%2C%20but%20may%20degrade%20performance.%20In%0Athis%20work%2C%20we%20present%20the%20first%20systematic%20evaluation%20of%20quantized%20LLMs%20on%0Atasks%20with%20long-inputs%20%28%3E64K%20tokens%29%20and%20long-form%20outputs.%20Our%20evaluation%0Aspans%209.7K%20test%20examples%2C%20five%20quantization%20methods%20%28FP8%2C%20GPTQ-int8%2C%20AWQ-int4%2C%0AGPTQ-int4%2C%20BNB-nf4%29%2C%20and%20five%20models%20%28Llama-3.1%208B%20and%2070B%3B%20Qwen-2.5%207B%2C%2032B%2C%0Aand%2072B%29.%20We%20find%20that%2C%20on%20average%2C%208-bit%20quantization%20preserves%20accuracy%0A%28~0.8%25%20drop%29%2C%20whereas%204-bit%20methods%20lead%20to%20substantial%20losses%2C%20especially%20for%0Atasks%20involving%20long%20context%20inputs%20%28drops%20of%20up%20to%2059%25%29.%20This%20degradation%0Atends%20to%20worsen%20when%20the%20input%20is%20in%20a%20language%20other%20than%20English.%20Crucially%2C%0Athe%20effects%20of%20quantization%20depend%20heavily%20on%20the%20quantization%20method%2C%20model%2C%0Aand%20task.%20For%20instance%2C%20while%20Qwen-2.5%2072B%20remains%20robust%20under%20BNB-nf4%2C%0ALlama-3.1%2070B%20experiences%20a%2032%25%20performance%20drop%20on%20the%20same%20task.%20These%0Afindings%20highlight%20the%20importance%20of%20a%20careful%2C%20task-specific%20evaluation%20before%0Adeploying%20quantized%20LLMs%2C%20particularly%20in%20long-context%20scenarios%20and%20with%0Alanguages%20other%20than%20English.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520quantization%2520affect%2520models%2527%2520performance%2520on%2520long-context%2520tasks%253F%26entry.906535625%3DAnmol%2520Mekala%2520and%2520Anirudh%2520Atmakuru%2520and%2520Yixiao%2520Song%2520and%2520Marzena%2520Karpinska%2520and%2520Mohit%2520Iyyer%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520now%2520support%2520context%2520windows%2520exceeding%2520128K%250Atokens%252C%2520but%2520this%2520comes%2520with%2520significant%2520memory%2520requirements%2520and%2520high%2520inference%250Alatency.%2520Quantization%2520can%2520mitigate%2520these%2520costs%252C%2520but%2520may%2520degrade%2520performance.%2520In%250Athis%2520work%252C%2520we%2520present%2520the%2520first%2520systematic%2520evaluation%2520of%2520quantized%2520LLMs%2520on%250Atasks%2520with%2520long-inputs%2520%2528%253E64K%2520tokens%2529%2520and%2520long-form%2520outputs.%2520Our%2520evaluation%250Aspans%25209.7K%2520test%2520examples%252C%2520five%2520quantization%2520methods%2520%2528FP8%252C%2520GPTQ-int8%252C%2520AWQ-int4%252C%250AGPTQ-int4%252C%2520BNB-nf4%2529%252C%2520and%2520five%2520models%2520%2528Llama-3.1%25208B%2520and%252070B%253B%2520Qwen-2.5%25207B%252C%252032B%252C%250Aand%252072B%2529.%2520We%2520find%2520that%252C%2520on%2520average%252C%25208-bit%2520quantization%2520preserves%2520accuracy%250A%2528~0.8%2525%2520drop%2529%252C%2520whereas%25204-bit%2520methods%2520lead%2520to%2520substantial%2520losses%252C%2520especially%2520for%250Atasks%2520involving%2520long%2520context%2520inputs%2520%2528drops%2520of%2520up%2520to%252059%2525%2529.%2520This%2520degradation%250Atends%2520to%2520worsen%2520when%2520the%2520input%2520is%2520in%2520a%2520language%2520other%2520than%2520English.%2520Crucially%252C%250Athe%2520effects%2520of%2520quantization%2520depend%2520heavily%2520on%2520the%2520quantization%2520method%252C%2520model%252C%250Aand%2520task.%2520For%2520instance%252C%2520while%2520Qwen-2.5%252072B%2520remains%2520robust%2520under%2520BNB-nf4%252C%250ALlama-3.1%252070B%2520experiences%2520a%252032%2525%2520performance%2520drop%2520on%2520the%2520same%2520task.%2520These%250Afindings%2520highlight%2520the%2520importance%2520of%2520a%2520careful%252C%2520task-specific%2520evaluation%2520before%250Adeploying%2520quantized%2520LLMs%252C%2520particularly%2520in%2520long-context%2520scenarios%2520and%2520with%250Alanguages%2520other%2520than%2520English.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20quantization%20affect%20models%27%20performance%20on%20long-context%20tasks%3F&entry.906535625=Anmol%20Mekala%20and%20Anirudh%20Atmakuru%20and%20Yixiao%20Song%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20now%20support%20context%20windows%20exceeding%20128K%0Atokens%2C%20but%20this%20comes%20with%20significant%20memory%20requirements%20and%20high%20inference%0Alatency.%20Quantization%20can%20mitigate%20these%20costs%2C%20but%20may%20degrade%20performance.%20In%0Athis%20work%2C%20we%20present%20the%20first%20systematic%20evaluation%20of%20quantized%20LLMs%20on%0Atasks%20with%20long-inputs%20%28%3E64K%20tokens%29%20and%20long-form%20outputs.%20Our%20evaluation%0Aspans%209.7K%20test%20examples%2C%20five%20quantization%20methods%20%28FP8%2C%20GPTQ-int8%2C%20AWQ-int4%2C%0AGPTQ-int4%2C%20BNB-nf4%29%2C%20and%20five%20models%20%28Llama-3.1%208B%20and%2070B%3B%20Qwen-2.5%207B%2C%2032B%2C%0Aand%2072B%29.%20We%20find%20that%2C%20on%20average%2C%208-bit%20quantization%20preserves%20accuracy%0A%28~0.8%25%20drop%29%2C%20whereas%204-bit%20methods%20lead%20to%20substantial%20losses%2C%20especially%20for%0Atasks%20involving%20long%20context%20inputs%20%28drops%20of%20up%20to%2059%25%29.%20This%20degradation%0Atends%20to%20worsen%20when%20the%20input%20is%20in%20a%20language%20other%20than%20English.%20Crucially%2C%0Athe%20effects%20of%20quantization%20depend%20heavily%20on%20the%20quantization%20method%2C%20model%2C%0Aand%20task.%20For%20instance%2C%20while%20Qwen-2.5%2072B%20remains%20robust%20under%20BNB-nf4%2C%0ALlama-3.1%2070B%20experiences%20a%2032%25%20performance%20drop%20on%20the%20same%20task.%20These%0Afindings%20highlight%20the%20importance%20of%20a%20careful%2C%20task-specific%20evaluation%20before%0Adeploying%20quantized%20LLMs%2C%20particularly%20in%20long-context%20scenarios%20and%20with%0Alanguages%20other%20than%20English.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20276v2&entry.124074799=Read"},
{"title": "Efficiently Scaling LLM Reasoning with Certaindex", "author": "Yichao Fu and Junda Chen and Siqi Zhu and Zheyu Fu and Zhongdongming Dai and Yonghao Zhuang and Yian Ma and Aurick Qiao and Tajana Rosing and Ion Stoica and Hao Zhang", "abstract": "  Test-time reasoning algorithms such as chain-of-thought, self-consistency,\nand MCTS enhance LLM problem-solving but can wastefully generate many tokens\nwithout improving accuracy. At the same time, we observe that these algorithms\nexhibit answer stabilization: their intermediate solutions often cease to\nchange after a certain point, and further investment of compute does not change\ntheir final answer. To quantify this phenomenon, we introduce Certaindex, an\nalgorithm-agnostic metric measuring this evolving stability, signaling when\nfurther computation is unlikely to alter the final result. Certaindex is\nlightweight, can accelerate reasoning program inference via early exit, and\nfurther enables dynamic token allocation, gang scheduling, and many\nopportunities when integrated with real-world LLM serving systems. To quantify\nreal-world benefits, we built Certaindex as a scheduler into Dynasor, our\nreasoning-aware LLM serving system, and demonstrate up to 50% compute savings\nand 3.3x higher throughput in real workloads with no accuracy drop. Our code is\navailable at https://github.com/hao-ai-lab/Dynasor.git\n", "link": "http://arxiv.org/abs/2412.20993v2", "date": "2025-05-27", "relevancy": 2.3601, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Scaling%20LLM%20Reasoning%20with%20Certaindex&body=Title%3A%20Efficiently%20Scaling%20LLM%20Reasoning%20with%20Certaindex%0AAuthor%3A%20Yichao%20Fu%20and%20Junda%20Chen%20and%20Siqi%20Zhu%20and%20Zheyu%20Fu%20and%20Zhongdongming%20Dai%20and%20Yonghao%20Zhuang%20and%20Yian%20Ma%20and%20Aurick%20Qiao%20and%20Tajana%20Rosing%20and%20Ion%20Stoica%20and%20Hao%20Zhang%0AAbstract%3A%20%20%20Test-time%20reasoning%20algorithms%20such%20as%20chain-of-thought%2C%20self-consistency%2C%0Aand%20MCTS%20enhance%20LLM%20problem-solving%20but%20can%20wastefully%20generate%20many%20tokens%0Awithout%20improving%20accuracy.%20At%20the%20same%20time%2C%20we%20observe%20that%20these%20algorithms%0Aexhibit%20answer%20stabilization%3A%20their%20intermediate%20solutions%20often%20cease%20to%0Achange%20after%20a%20certain%20point%2C%20and%20further%20investment%20of%20compute%20does%20not%20change%0Atheir%20final%20answer.%20To%20quantify%20this%20phenomenon%2C%20we%20introduce%20Certaindex%2C%20an%0Aalgorithm-agnostic%20metric%20measuring%20this%20evolving%20stability%2C%20signaling%20when%0Afurther%20computation%20is%20unlikely%20to%20alter%20the%20final%20result.%20Certaindex%20is%0Alightweight%2C%20can%20accelerate%20reasoning%20program%20inference%20via%20early%20exit%2C%20and%0Afurther%20enables%20dynamic%20token%20allocation%2C%20gang%20scheduling%2C%20and%20many%0Aopportunities%20when%20integrated%20with%20real-world%20LLM%20serving%20systems.%20To%20quantify%0Areal-world%20benefits%2C%20we%20built%20Certaindex%20as%20a%20scheduler%20into%20Dynasor%2C%20our%0Areasoning-aware%20LLM%20serving%20system%2C%20and%20demonstrate%20up%20to%2050%25%20compute%20savings%0Aand%203.3x%20higher%20throughput%20in%20real%20workloads%20with%20no%20accuracy%20drop.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/hao-ai-lab/Dynasor.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20993v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Scaling%2520LLM%2520Reasoning%2520with%2520Certaindex%26entry.906535625%3DYichao%2520Fu%2520and%2520Junda%2520Chen%2520and%2520Siqi%2520Zhu%2520and%2520Zheyu%2520Fu%2520and%2520Zhongdongming%2520Dai%2520and%2520Yonghao%2520Zhuang%2520and%2520Yian%2520Ma%2520and%2520Aurick%2520Qiao%2520and%2520Tajana%2520Rosing%2520and%2520Ion%2520Stoica%2520and%2520Hao%2520Zhang%26entry.1292438233%3D%2520%2520Test-time%2520reasoning%2520algorithms%2520such%2520as%2520chain-of-thought%252C%2520self-consistency%252C%250Aand%2520MCTS%2520enhance%2520LLM%2520problem-solving%2520but%2520can%2520wastefully%2520generate%2520many%2520tokens%250Awithout%2520improving%2520accuracy.%2520At%2520the%2520same%2520time%252C%2520we%2520observe%2520that%2520these%2520algorithms%250Aexhibit%2520answer%2520stabilization%253A%2520their%2520intermediate%2520solutions%2520often%2520cease%2520to%250Achange%2520after%2520a%2520certain%2520point%252C%2520and%2520further%2520investment%2520of%2520compute%2520does%2520not%2520change%250Atheir%2520final%2520answer.%2520To%2520quantify%2520this%2520phenomenon%252C%2520we%2520introduce%2520Certaindex%252C%2520an%250Aalgorithm-agnostic%2520metric%2520measuring%2520this%2520evolving%2520stability%252C%2520signaling%2520when%250Afurther%2520computation%2520is%2520unlikely%2520to%2520alter%2520the%2520final%2520result.%2520Certaindex%2520is%250Alightweight%252C%2520can%2520accelerate%2520reasoning%2520program%2520inference%2520via%2520early%2520exit%252C%2520and%250Afurther%2520enables%2520dynamic%2520token%2520allocation%252C%2520gang%2520scheduling%252C%2520and%2520many%250Aopportunities%2520when%2520integrated%2520with%2520real-world%2520LLM%2520serving%2520systems.%2520To%2520quantify%250Areal-world%2520benefits%252C%2520we%2520built%2520Certaindex%2520as%2520a%2520scheduler%2520into%2520Dynasor%252C%2520our%250Areasoning-aware%2520LLM%2520serving%2520system%252C%2520and%2520demonstrate%2520up%2520to%252050%2525%2520compute%2520savings%250Aand%25203.3x%2520higher%2520throughput%2520in%2520real%2520workloads%2520with%2520no%2520accuracy%2520drop.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/hao-ai-lab/Dynasor.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20993v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Scaling%20LLM%20Reasoning%20with%20Certaindex&entry.906535625=Yichao%20Fu%20and%20Junda%20Chen%20and%20Siqi%20Zhu%20and%20Zheyu%20Fu%20and%20Zhongdongming%20Dai%20and%20Yonghao%20Zhuang%20and%20Yian%20Ma%20and%20Aurick%20Qiao%20and%20Tajana%20Rosing%20and%20Ion%20Stoica%20and%20Hao%20Zhang&entry.1292438233=%20%20Test-time%20reasoning%20algorithms%20such%20as%20chain-of-thought%2C%20self-consistency%2C%0Aand%20MCTS%20enhance%20LLM%20problem-solving%20but%20can%20wastefully%20generate%20many%20tokens%0Awithout%20improving%20accuracy.%20At%20the%20same%20time%2C%20we%20observe%20that%20these%20algorithms%0Aexhibit%20answer%20stabilization%3A%20their%20intermediate%20solutions%20often%20cease%20to%0Achange%20after%20a%20certain%20point%2C%20and%20further%20investment%20of%20compute%20does%20not%20change%0Atheir%20final%20answer.%20To%20quantify%20this%20phenomenon%2C%20we%20introduce%20Certaindex%2C%20an%0Aalgorithm-agnostic%20metric%20measuring%20this%20evolving%20stability%2C%20signaling%20when%0Afurther%20computation%20is%20unlikely%20to%20alter%20the%20final%20result.%20Certaindex%20is%0Alightweight%2C%20can%20accelerate%20reasoning%20program%20inference%20via%20early%20exit%2C%20and%0Afurther%20enables%20dynamic%20token%20allocation%2C%20gang%20scheduling%2C%20and%20many%0Aopportunities%20when%20integrated%20with%20real-world%20LLM%20serving%20systems.%20To%20quantify%0Areal-world%20benefits%2C%20we%20built%20Certaindex%20as%20a%20scheduler%20into%20Dynasor%2C%20our%0Areasoning-aware%20LLM%20serving%20system%2C%20and%20demonstrate%20up%20to%2050%25%20compute%20savings%0Aand%203.3x%20higher%20throughput%20in%20real%20workloads%20with%20no%20accuracy%20drop.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/hao-ai-lab/Dynasor.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20993v2&entry.124074799=Read"},
{"title": "Securing Federated Learning against Backdoor Threats with Foundation\n  Model Integration", "author": "Xiaohuan Bi and Xi Li", "abstract": "  Federated Learning (FL) enables decentralized model training while preserving\nprivacy. Recently, the integration of Foundation Models (FMs) into FL has\nenhanced performance but introduced a novel backdoor attack mechanism.\nAttackers can exploit FM vulnerabilities to embed backdoors into synthetic data\ngenerated by FMs. During global model fusion, these backdoors are transferred\nto the global model through compromised synthetic data, subsequently infecting\nall client models. Existing FL backdoor defenses are ineffective against this\nnovel attack due to its fundamentally different mechanism compared to classic\nones. In this work, we propose a novel data-free defense strategy that\naddresses both classic and novel backdoor attacks in FL. The shared attack\npattern lies in the abnormal activations within the hidden feature space during\nmodel aggregation. Hence, we propose to constrain internal activations to\nremain within reasonable ranges, effectively mitigating attacks while\npreserving model functionality. The activation constraints are optimized using\nsynthetic data alongside FL training. Extensive experiments demonstrate its\neffectiveness against both novel and classic backdoor attacks, outperforming\nexisting defenses.\n", "link": "http://arxiv.org/abs/2410.17573v3", "date": "2025-05-27", "relevancy": 2.3502, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4762}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Securing%20Federated%20Learning%20against%20Backdoor%20Threats%20with%20Foundation%0A%20%20Model%20Integration&body=Title%3A%20Securing%20Federated%20Learning%20against%20Backdoor%20Threats%20with%20Foundation%0A%20%20Model%20Integration%0AAuthor%3A%20Xiaohuan%20Bi%20and%20Xi%20Li%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20decentralized%20model%20training%20while%20preserving%0Aprivacy.%20Recently%2C%20the%20integration%20of%20Foundation%20Models%20%28FMs%29%20into%20FL%20has%0Aenhanced%20performance%20but%20introduced%20a%20novel%20backdoor%20attack%20mechanism.%0AAttackers%20can%20exploit%20FM%20vulnerabilities%20to%20embed%20backdoors%20into%20synthetic%20data%0Agenerated%20by%20FMs.%20During%20global%20model%20fusion%2C%20these%20backdoors%20are%20transferred%0Ato%20the%20global%20model%20through%20compromised%20synthetic%20data%2C%20subsequently%20infecting%0Aall%20client%20models.%20Existing%20FL%20backdoor%20defenses%20are%20ineffective%20against%20this%0Anovel%20attack%20due%20to%20its%20fundamentally%20different%20mechanism%20compared%20to%20classic%0Aones.%20In%20this%20work%2C%20we%20propose%20a%20novel%20data-free%20defense%20strategy%20that%0Aaddresses%20both%20classic%20and%20novel%20backdoor%20attacks%20in%20FL.%20The%20shared%20attack%0Apattern%20lies%20in%20the%20abnormal%20activations%20within%20the%20hidden%20feature%20space%20during%0Amodel%20aggregation.%20Hence%2C%20we%20propose%20to%20constrain%20internal%20activations%20to%0Aremain%20within%20reasonable%20ranges%2C%20effectively%20mitigating%20attacks%20while%0Apreserving%20model%20functionality.%20The%20activation%20constraints%20are%20optimized%20using%0Asynthetic%20data%20alongside%20FL%20training.%20Extensive%20experiments%20demonstrate%20its%0Aeffectiveness%20against%20both%20novel%20and%20classic%20backdoor%20attacks%2C%20outperforming%0Aexisting%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17573v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecuring%2520Federated%2520Learning%2520against%2520Backdoor%2520Threats%2520with%2520Foundation%250A%2520%2520Model%2520Integration%26entry.906535625%3DXiaohuan%2520Bi%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520decentralized%2520model%2520training%2520while%2520preserving%250Aprivacy.%2520Recently%252C%2520the%2520integration%2520of%2520Foundation%2520Models%2520%2528FMs%2529%2520into%2520FL%2520has%250Aenhanced%2520performance%2520but%2520introduced%2520a%2520novel%2520backdoor%2520attack%2520mechanism.%250AAttackers%2520can%2520exploit%2520FM%2520vulnerabilities%2520to%2520embed%2520backdoors%2520into%2520synthetic%2520data%250Agenerated%2520by%2520FMs.%2520During%2520global%2520model%2520fusion%252C%2520these%2520backdoors%2520are%2520transferred%250Ato%2520the%2520global%2520model%2520through%2520compromised%2520synthetic%2520data%252C%2520subsequently%2520infecting%250Aall%2520client%2520models.%2520Existing%2520FL%2520backdoor%2520defenses%2520are%2520ineffective%2520against%2520this%250Anovel%2520attack%2520due%2520to%2520its%2520fundamentally%2520different%2520mechanism%2520compared%2520to%2520classic%250Aones.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520data-free%2520defense%2520strategy%2520that%250Aaddresses%2520both%2520classic%2520and%2520novel%2520backdoor%2520attacks%2520in%2520FL.%2520The%2520shared%2520attack%250Apattern%2520lies%2520in%2520the%2520abnormal%2520activations%2520within%2520the%2520hidden%2520feature%2520space%2520during%250Amodel%2520aggregation.%2520Hence%252C%2520we%2520propose%2520to%2520constrain%2520internal%2520activations%2520to%250Aremain%2520within%2520reasonable%2520ranges%252C%2520effectively%2520mitigating%2520attacks%2520while%250Apreserving%2520model%2520functionality.%2520The%2520activation%2520constraints%2520are%2520optimized%2520using%250Asynthetic%2520data%2520alongside%2520FL%2520training.%2520Extensive%2520experiments%2520demonstrate%2520its%250Aeffectiveness%2520against%2520both%2520novel%2520and%2520classic%2520backdoor%2520attacks%252C%2520outperforming%250Aexisting%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17573v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Securing%20Federated%20Learning%20against%20Backdoor%20Threats%20with%20Foundation%0A%20%20Model%20Integration&entry.906535625=Xiaohuan%20Bi%20and%20Xi%20Li&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20decentralized%20model%20training%20while%20preserving%0Aprivacy.%20Recently%2C%20the%20integration%20of%20Foundation%20Models%20%28FMs%29%20into%20FL%20has%0Aenhanced%20performance%20but%20introduced%20a%20novel%20backdoor%20attack%20mechanism.%0AAttackers%20can%20exploit%20FM%20vulnerabilities%20to%20embed%20backdoors%20into%20synthetic%20data%0Agenerated%20by%20FMs.%20During%20global%20model%20fusion%2C%20these%20backdoors%20are%20transferred%0Ato%20the%20global%20model%20through%20compromised%20synthetic%20data%2C%20subsequently%20infecting%0Aall%20client%20models.%20Existing%20FL%20backdoor%20defenses%20are%20ineffective%20against%20this%0Anovel%20attack%20due%20to%20its%20fundamentally%20different%20mechanism%20compared%20to%20classic%0Aones.%20In%20this%20work%2C%20we%20propose%20a%20novel%20data-free%20defense%20strategy%20that%0Aaddresses%20both%20classic%20and%20novel%20backdoor%20attacks%20in%20FL.%20The%20shared%20attack%0Apattern%20lies%20in%20the%20abnormal%20activations%20within%20the%20hidden%20feature%20space%20during%0Amodel%20aggregation.%20Hence%2C%20we%20propose%20to%20constrain%20internal%20activations%20to%0Aremain%20within%20reasonable%20ranges%2C%20effectively%20mitigating%20attacks%20while%0Apreserving%20model%20functionality.%20The%20activation%20constraints%20are%20optimized%20using%0Asynthetic%20data%20alongside%20FL%20training.%20Extensive%20experiments%20demonstrate%20its%0Aeffectiveness%20against%20both%20novel%20and%20classic%20backdoor%20attacks%2C%20outperforming%0Aexisting%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17573v3&entry.124074799=Read"},
{"title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs.\n  Dialogue History", "author": "Qishuai Zhong and Zongmin Li and Siqi Fan and Aixin Sun", "abstract": "  Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.\n", "link": "http://arxiv.org/abs/2505.21362v1", "date": "2025-05-27", "relevancy": 2.333, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20LLM%20Adaptation%20to%20Sociodemographic%20Factors%3A%20User%20Profile%20vs.%0A%20%20Dialogue%20History&body=Title%3A%20Evaluating%20LLM%20Adaptation%20to%20Sociodemographic%20Factors%3A%20User%20Profile%20vs.%0A%20%20Dialogue%20History%0AAuthor%3A%20Qishuai%20Zhong%20and%20Zongmin%20Li%20and%20Siqi%20Fan%20and%20Aixin%20Sun%0AAbstract%3A%20%20%20Effective%20engagement%20by%20large%20language%20models%20%28LLMs%29%20requires%20adapting%0Aresponses%20to%20users%27%20sociodemographic%20characteristics%2C%20such%20as%20age%2C%20occupation%2C%0Aand%20education%20level.%20While%20many%20real-world%20applications%20leverage%20dialogue%0Ahistory%20for%20contextualization%2C%20existing%20evaluations%20of%20LLMs%27%20behavioral%0Aadaptation%20often%20focus%20on%20single-turn%20prompts.%20In%20this%20paper%2C%20we%20propose%20a%0Aframework%20to%20evaluate%20LLM%20adaptation%20when%20attributes%20are%20introduced%20either%20%281%29%0Aexplicitly%20via%20user%20profiles%20in%20the%20prompt%20or%20%282%29%20implicitly%20through%20multi-turn%0Adialogue%20history.%20We%20assess%20the%20consistency%20of%20model%20behavior%20across%20these%0Amodalities.%20Using%20a%20multi-agent%20pipeline%2C%20we%20construct%20a%20synthetic%20dataset%0Apairing%20dialogue%20histories%20with%20distinct%20user%20profiles%20and%20employ%20questions%0Afrom%20the%20Value%20Survey%20Module%20%28VSM%202013%29%20%28Hofstede%20and%20Hofstede%2C%202016%29%20to%20probe%0Avalue%20expression.%20Our%20findings%20indicate%20that%20most%20models%20adjust%20their%20expressed%0Avalues%20in%20response%20to%20demographic%20changes%2C%20particularly%20in%20age%20and%20education%0Alevel%2C%20but%20consistency%20varies.%20Models%20with%20stronger%20reasoning%20capabilities%0Ademonstrate%20greater%20alignment%2C%20indicating%20the%20importance%20of%20reasoning%20in%20robust%0Asociodemographic%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520LLM%2520Adaptation%2520to%2520Sociodemographic%2520Factors%253A%2520User%2520Profile%2520vs.%250A%2520%2520Dialogue%2520History%26entry.906535625%3DQishuai%2520Zhong%2520and%2520Zongmin%2520Li%2520and%2520Siqi%2520Fan%2520and%2520Aixin%2520Sun%26entry.1292438233%3D%2520%2520Effective%2520engagement%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520requires%2520adapting%250Aresponses%2520to%2520users%2527%2520sociodemographic%2520characteristics%252C%2520such%2520as%2520age%252C%2520occupation%252C%250Aand%2520education%2520level.%2520While%2520many%2520real-world%2520applications%2520leverage%2520dialogue%250Ahistory%2520for%2520contextualization%252C%2520existing%2520evaluations%2520of%2520LLMs%2527%2520behavioral%250Aadaptation%2520often%2520focus%2520on%2520single-turn%2520prompts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aframework%2520to%2520evaluate%2520LLM%2520adaptation%2520when%2520attributes%2520are%2520introduced%2520either%2520%25281%2529%250Aexplicitly%2520via%2520user%2520profiles%2520in%2520the%2520prompt%2520or%2520%25282%2529%2520implicitly%2520through%2520multi-turn%250Adialogue%2520history.%2520We%2520assess%2520the%2520consistency%2520of%2520model%2520behavior%2520across%2520these%250Amodalities.%2520Using%2520a%2520multi-agent%2520pipeline%252C%2520we%2520construct%2520a%2520synthetic%2520dataset%250Apairing%2520dialogue%2520histories%2520with%2520distinct%2520user%2520profiles%2520and%2520employ%2520questions%250Afrom%2520the%2520Value%2520Survey%2520Module%2520%2528VSM%25202013%2529%2520%2528Hofstede%2520and%2520Hofstede%252C%25202016%2529%2520to%2520probe%250Avalue%2520expression.%2520Our%2520findings%2520indicate%2520that%2520most%2520models%2520adjust%2520their%2520expressed%250Avalues%2520in%2520response%2520to%2520demographic%2520changes%252C%2520particularly%2520in%2520age%2520and%2520education%250Alevel%252C%2520but%2520consistency%2520varies.%2520Models%2520with%2520stronger%2520reasoning%2520capabilities%250Ademonstrate%2520greater%2520alignment%252C%2520indicating%2520the%2520importance%2520of%2520reasoning%2520in%2520robust%250Asociodemographic%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20LLM%20Adaptation%20to%20Sociodemographic%20Factors%3A%20User%20Profile%20vs.%0A%20%20Dialogue%20History&entry.906535625=Qishuai%20Zhong%20and%20Zongmin%20Li%20and%20Siqi%20Fan%20and%20Aixin%20Sun&entry.1292438233=%20%20Effective%20engagement%20by%20large%20language%20models%20%28LLMs%29%20requires%20adapting%0Aresponses%20to%20users%27%20sociodemographic%20characteristics%2C%20such%20as%20age%2C%20occupation%2C%0Aand%20education%20level.%20While%20many%20real-world%20applications%20leverage%20dialogue%0Ahistory%20for%20contextualization%2C%20existing%20evaluations%20of%20LLMs%27%20behavioral%0Aadaptation%20often%20focus%20on%20single-turn%20prompts.%20In%20this%20paper%2C%20we%20propose%20a%0Aframework%20to%20evaluate%20LLM%20adaptation%20when%20attributes%20are%20introduced%20either%20%281%29%0Aexplicitly%20via%20user%20profiles%20in%20the%20prompt%20or%20%282%29%20implicitly%20through%20multi-turn%0Adialogue%20history.%20We%20assess%20the%20consistency%20of%20model%20behavior%20across%20these%0Amodalities.%20Using%20a%20multi-agent%20pipeline%2C%20we%20construct%20a%20synthetic%20dataset%0Apairing%20dialogue%20histories%20with%20distinct%20user%20profiles%20and%20employ%20questions%0Afrom%20the%20Value%20Survey%20Module%20%28VSM%202013%29%20%28Hofstede%20and%20Hofstede%2C%202016%29%20to%20probe%0Avalue%20expression.%20Our%20findings%20indicate%20that%20most%20models%20adjust%20their%20expressed%0Avalues%20in%20response%20to%20demographic%20changes%2C%20particularly%20in%20age%20and%20education%0Alevel%2C%20but%20consistency%20varies.%20Models%20with%20stronger%20reasoning%20capabilities%0Ademonstrate%20greater%20alignment%2C%20indicating%20the%20importance%20of%20reasoning%20in%20robust%0Asociodemographic%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21362v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Bengali Math Word Problem Solving\n  with Chain of Thought Reasoning", "author": "Bidyarthi Paul and Jalisha Jashim Era and Mirazur Rahman Zim and Tahmid Sattar Aothoi and Faisal Muhammad Shah", "abstract": "  Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.\n", "link": "http://arxiv.org/abs/2505.21354v1", "date": "2025-05-27", "relevancy": 2.3299, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Bengali%20Math%20Word%20Problem%20Solving%0A%20%20with%20Chain%20of%20Thought%20Reasoning&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Bengali%20Math%20Word%20Problem%20Solving%0A%20%20with%20Chain%20of%20Thought%20Reasoning%0AAuthor%3A%20Bidyarthi%20Paul%20and%20Jalisha%20Jashim%20Era%20and%20Mirazur%20Rahman%20Zim%20and%20Tahmid%20Sattar%20Aothoi%20and%20Faisal%20Muhammad%20Shah%0AAbstract%3A%20%20%20Solving%20Bengali%20Math%20Word%20Problems%20%28MWPs%29%20remains%20a%20major%20challenge%20in%0Anatural%20language%20processing%20%28NLP%29%20due%20to%20the%20language%27s%20low-resource%20status%20and%0Athe%20multi-step%20reasoning%20required.%20Existing%20models%20struggle%20with%20complex%0ABengali%20MWPs%2C%20largely%20because%20no%20human-annotated%20Bengali%20dataset%20has%20previously%0Aaddressed%20this%20task.%20This%20gap%20has%20limited%20progress%20in%20Bengali%20mathematical%0Areasoning.%20To%20address%20this%2C%20we%20created%20SOMADHAN%2C%20a%20dataset%20of%208792%20complex%0ABengali%20MWPs%20with%20manually%20written%2C%20step-by-step%20solutions.%20We%20designed%20this%0Adataset%20to%20support%20reasoning-focused%20evaluation%20and%20model%20development%20in%20a%0Alinguistically%20underrepresented%20context.%20Using%20SOMADHAN%2C%20we%20evaluated%20a%20range%0Aof%20large%20language%20models%20%28LLMs%29%20-%20including%20GPT-4o%2C%20GPT-3.5%20Turbo%2C%20LLaMA%20series%0Amodels%2C%20Deepseek%2C%20and%20Qwen%20-%20through%20both%20zero-shot%20and%20few-shot%20prompting%20with%0Aand%20without%20Chain%20of%20Thought%20%28CoT%29%20reasoning.%20CoT%20prompting%20consistently%0Aimproved%20performance%20over%20standard%20prompting%2C%20especially%20in%20tasks%20requiring%0Amulti-step%20logic.%20LLaMA-3.3%2070B%20achieved%20the%20highest%20accuracy%20of%2088%25%20with%0Afew-shot%20CoT%20prompting.%20We%20also%20applied%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20fine-tune%0Amodels%20efficiently%2C%20enabling%20them%20to%20adapt%20to%20Bengali%20MWPs%20with%20minimal%0Acomputational%20cost.%20Our%20work%20fills%20a%20critical%20gap%20in%20Bengali%20NLP%20by%20providing%20a%0Ahigh-quality%20reasoning%20dataset%20and%20a%20scalable%20framework%20for%20solving%20complex%0AMWPs.%20We%20aim%20to%20advance%20equitable%20research%20in%20low-resource%20languages%20and%0Aenhance%20reasoning%20capabilities%20in%20educational%20and%20language%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Bengali%2520Math%2520Word%2520Problem%2520Solving%250A%2520%2520with%2520Chain%2520of%2520Thought%2520Reasoning%26entry.906535625%3DBidyarthi%2520Paul%2520and%2520Jalisha%2520Jashim%2520Era%2520and%2520Mirazur%2520Rahman%2520Zim%2520and%2520Tahmid%2520Sattar%2520Aothoi%2520and%2520Faisal%2520Muhammad%2520Shah%26entry.1292438233%3D%2520%2520Solving%2520Bengali%2520Math%2520Word%2520Problems%2520%2528MWPs%2529%2520remains%2520a%2520major%2520challenge%2520in%250Anatural%2520language%2520processing%2520%2528NLP%2529%2520due%2520to%2520the%2520language%2527s%2520low-resource%2520status%2520and%250Athe%2520multi-step%2520reasoning%2520required.%2520Existing%2520models%2520struggle%2520with%2520complex%250ABengali%2520MWPs%252C%2520largely%2520because%2520no%2520human-annotated%2520Bengali%2520dataset%2520has%2520previously%250Aaddressed%2520this%2520task.%2520This%2520gap%2520has%2520limited%2520progress%2520in%2520Bengali%2520mathematical%250Areasoning.%2520To%2520address%2520this%252C%2520we%2520created%2520SOMADHAN%252C%2520a%2520dataset%2520of%25208792%2520complex%250ABengali%2520MWPs%2520with%2520manually%2520written%252C%2520step-by-step%2520solutions.%2520We%2520designed%2520this%250Adataset%2520to%2520support%2520reasoning-focused%2520evaluation%2520and%2520model%2520development%2520in%2520a%250Alinguistically%2520underrepresented%2520context.%2520Using%2520SOMADHAN%252C%2520we%2520evaluated%2520a%2520range%250Aof%2520large%2520language%2520models%2520%2528LLMs%2529%2520-%2520including%2520GPT-4o%252C%2520GPT-3.5%2520Turbo%252C%2520LLaMA%2520series%250Amodels%252C%2520Deepseek%252C%2520and%2520Qwen%2520-%2520through%2520both%2520zero-shot%2520and%2520few-shot%2520prompting%2520with%250Aand%2520without%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520reasoning.%2520CoT%2520prompting%2520consistently%250Aimproved%2520performance%2520over%2520standard%2520prompting%252C%2520especially%2520in%2520tasks%2520requiring%250Amulti-step%2520logic.%2520LLaMA-3.3%252070B%2520achieved%2520the%2520highest%2520accuracy%2520of%252088%2525%2520with%250Afew-shot%2520CoT%2520prompting.%2520We%2520also%2520applied%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520fine-tune%250Amodels%2520efficiently%252C%2520enabling%2520them%2520to%2520adapt%2520to%2520Bengali%2520MWPs%2520with%2520minimal%250Acomputational%2520cost.%2520Our%2520work%2520fills%2520a%2520critical%2520gap%2520in%2520Bengali%2520NLP%2520by%2520providing%2520a%250Ahigh-quality%2520reasoning%2520dataset%2520and%2520a%2520scalable%2520framework%2520for%2520solving%2520complex%250AMWPs.%2520We%2520aim%2520to%2520advance%2520equitable%2520research%2520in%2520low-resource%2520languages%2520and%250Aenhance%2520reasoning%2520capabilities%2520in%2520educational%2520and%2520language%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Bengali%20Math%20Word%20Problem%20Solving%0A%20%20with%20Chain%20of%20Thought%20Reasoning&entry.906535625=Bidyarthi%20Paul%20and%20Jalisha%20Jashim%20Era%20and%20Mirazur%20Rahman%20Zim%20and%20Tahmid%20Sattar%20Aothoi%20and%20Faisal%20Muhammad%20Shah&entry.1292438233=%20%20Solving%20Bengali%20Math%20Word%20Problems%20%28MWPs%29%20remains%20a%20major%20challenge%20in%0Anatural%20language%20processing%20%28NLP%29%20due%20to%20the%20language%27s%20low-resource%20status%20and%0Athe%20multi-step%20reasoning%20required.%20Existing%20models%20struggle%20with%20complex%0ABengali%20MWPs%2C%20largely%20because%20no%20human-annotated%20Bengali%20dataset%20has%20previously%0Aaddressed%20this%20task.%20This%20gap%20has%20limited%20progress%20in%20Bengali%20mathematical%0Areasoning.%20To%20address%20this%2C%20we%20created%20SOMADHAN%2C%20a%20dataset%20of%208792%20complex%0ABengali%20MWPs%20with%20manually%20written%2C%20step-by-step%20solutions.%20We%20designed%20this%0Adataset%20to%20support%20reasoning-focused%20evaluation%20and%20model%20development%20in%20a%0Alinguistically%20underrepresented%20context.%20Using%20SOMADHAN%2C%20we%20evaluated%20a%20range%0Aof%20large%20language%20models%20%28LLMs%29%20-%20including%20GPT-4o%2C%20GPT-3.5%20Turbo%2C%20LLaMA%20series%0Amodels%2C%20Deepseek%2C%20and%20Qwen%20-%20through%20both%20zero-shot%20and%20few-shot%20prompting%20with%0Aand%20without%20Chain%20of%20Thought%20%28CoT%29%20reasoning.%20CoT%20prompting%20consistently%0Aimproved%20performance%20over%20standard%20prompting%2C%20especially%20in%20tasks%20requiring%0Amulti-step%20logic.%20LLaMA-3.3%2070B%20achieved%20the%20highest%20accuracy%20of%2088%25%20with%0Afew-shot%20CoT%20prompting.%20We%20also%20applied%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20fine-tune%0Amodels%20efficiently%2C%20enabling%20them%20to%20adapt%20to%20Bengali%20MWPs%20with%20minimal%0Acomputational%20cost.%20Our%20work%20fills%20a%20critical%20gap%20in%20Bengali%20NLP%20by%20providing%20a%0Ahigh-quality%20reasoning%20dataset%20and%20a%20scalable%20framework%20for%20solving%20complex%0AMWPs.%20We%20aim%20to%20advance%20equitable%20research%20in%20low-resource%20languages%20and%0Aenhance%20reasoning%20capabilities%20in%20educational%20and%20language%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21354v1&entry.124074799=Read"},
{"title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios", "author": "Yang Shi and Huanqian Wang and Wulin Xie and Huanyao Zhang and Lijie Zhao and Yi-Fan Zhang and Xinfeng Li and Chaoyou Fu and Zhuoer Wen and Wenting Liu and Zhuoran Zhang and Xinlong Chen and Bohan Zeng and Sihan Yang and Yuanxing Zhang and Pengfei Wan and Haotian Wang and Wenjing Yang", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.\n", "link": "http://arxiv.org/abs/2505.21333v1", "date": "2025-05-27", "relevancy": 2.3257, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MME-VideoOCR%3A%20Evaluating%20OCR-Based%20Capabilities%20of%20Multimodal%20LLMs%20in%0A%20%20Video%20Scenarios&body=Title%3A%20MME-VideoOCR%3A%20Evaluating%20OCR-Based%20Capabilities%20of%20Multimodal%20LLMs%20in%0A%20%20Video%20Scenarios%0AAuthor%3A%20Yang%20Shi%20and%20Huanqian%20Wang%20and%20Wulin%20Xie%20and%20Huanyao%20Zhang%20and%20Lijie%20Zhao%20and%20Yi-Fan%20Zhang%20and%20Xinfeng%20Li%20and%20Chaoyou%20Fu%20and%20Zhuoer%20Wen%20and%20Wenting%20Liu%20and%20Zhuoran%20Zhang%20and%20Xinlong%20Chen%20and%20Bohan%20Zeng%20and%20Sihan%20Yang%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Haotian%20Wang%20and%20Wenjing%20Yang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20considerable%20accuracy%0Ain%20Optical%20Character%20Recognition%20%28OCR%29%20from%20static%20images.%20However%2C%20their%0Aefficacy%20in%20video%20OCR%20is%20significantly%20diminished%20due%20to%20factors%20such%20as%20motion%0Ablur%2C%20temporal%20variations%2C%20and%20visual%20effects%20inherent%20in%20video%20content.%20To%0Aprovide%20clearer%20guidance%20for%20training%20practical%20MLLMs%2C%20we%20introduce%20the%0AMME-VideoOCR%20benchmark%2C%20which%20encompasses%20a%20comprehensive%20range%20of%20video%20OCR%0Aapplication%20scenarios.%20MME-VideoOCR%20features%2010%20task%20categories%20comprising%2025%0Aindividual%20tasks%20and%20spans%2044%20diverse%20scenarios.%20These%20tasks%20extend%20beyond%20text%0Arecognition%20to%20incorporate%20deeper%20comprehension%20and%20reasoning%20of%20textual%0Acontent%20within%20videos.%20The%20benchmark%20consists%20of%201%2C464%20videos%20with%20varying%0Aresolutions%2C%20aspect%20ratios%2C%20and%20durations%2C%20along%20with%202%2C000%20meticulously%0Acurated%2C%20manually%20annotated%20question-answer%20pairs.%20We%20evaluate%2018%0Astate-of-the-art%20MLLMs%20on%20MME-VideoOCR%2C%20revealing%20that%20even%20the%20best-performing%0Amodel%20%28Gemini-2.5%20Pro%29%20achieves%20an%20accuracy%20of%20only%2073.7%25.%20Fine-grained%0Aanalysis%20indicates%20that%20while%20existing%20MLLMs%20demonstrate%20strong%20performance%20on%0Atasks%20where%20relevant%20texts%20are%20contained%20within%20a%20single%20or%20few%20frames%2C%20they%0Aexhibit%20limited%20capability%20in%20effectively%20handling%20tasks%20that%20demand%20holistic%0Avideo%20comprehension.%20These%20limitations%20are%20especially%20evident%20in%20scenarios%20that%0Arequire%20spatio-temporal%20reasoning%2C%20cross-frame%20information%20integration%2C%20or%0Aresistance%20to%20language%20prior%20bias.%20Our%20findings%20also%20highlight%20the%20importance%0Aof%20high-resolution%20visual%20input%20and%20sufficient%20temporal%20coverage%20for%20reliable%0AOCR%20in%20dynamic%20video%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMME-VideoOCR%253A%2520Evaluating%2520OCR-Based%2520Capabilities%2520of%2520Multimodal%2520LLMs%2520in%250A%2520%2520Video%2520Scenarios%26entry.906535625%3DYang%2520Shi%2520and%2520Huanqian%2520Wang%2520and%2520Wulin%2520Xie%2520and%2520Huanyao%2520Zhang%2520and%2520Lijie%2520Zhao%2520and%2520Yi-Fan%2520Zhang%2520and%2520Xinfeng%2520Li%2520and%2520Chaoyou%2520Fu%2520and%2520Zhuoer%2520Wen%2520and%2520Wenting%2520Liu%2520and%2520Zhuoran%2520Zhang%2520and%2520Xinlong%2520Chen%2520and%2520Bohan%2520Zeng%2520and%2520Sihan%2520Yang%2520and%2520Yuanxing%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Haotian%2520Wang%2520and%2520Wenjing%2520Yang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520considerable%2520accuracy%250Ain%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520from%2520static%2520images.%2520However%252C%2520their%250Aefficacy%2520in%2520video%2520OCR%2520is%2520significantly%2520diminished%2520due%2520to%2520factors%2520such%2520as%2520motion%250Ablur%252C%2520temporal%2520variations%252C%2520and%2520visual%2520effects%2520inherent%2520in%2520video%2520content.%2520To%250Aprovide%2520clearer%2520guidance%2520for%2520training%2520practical%2520MLLMs%252C%2520we%2520introduce%2520the%250AMME-VideoOCR%2520benchmark%252C%2520which%2520encompasses%2520a%2520comprehensive%2520range%2520of%2520video%2520OCR%250Aapplication%2520scenarios.%2520MME-VideoOCR%2520features%252010%2520task%2520categories%2520comprising%252025%250Aindividual%2520tasks%2520and%2520spans%252044%2520diverse%2520scenarios.%2520These%2520tasks%2520extend%2520beyond%2520text%250Arecognition%2520to%2520incorporate%2520deeper%2520comprehension%2520and%2520reasoning%2520of%2520textual%250Acontent%2520within%2520videos.%2520The%2520benchmark%2520consists%2520of%25201%252C464%2520videos%2520with%2520varying%250Aresolutions%252C%2520aspect%2520ratios%252C%2520and%2520durations%252C%2520along%2520with%25202%252C000%2520meticulously%250Acurated%252C%2520manually%2520annotated%2520question-answer%2520pairs.%2520We%2520evaluate%252018%250Astate-of-the-art%2520MLLMs%2520on%2520MME-VideoOCR%252C%2520revealing%2520that%2520even%2520the%2520best-performing%250Amodel%2520%2528Gemini-2.5%2520Pro%2529%2520achieves%2520an%2520accuracy%2520of%2520only%252073.7%2525.%2520Fine-grained%250Aanalysis%2520indicates%2520that%2520while%2520existing%2520MLLMs%2520demonstrate%2520strong%2520performance%2520on%250Atasks%2520where%2520relevant%2520texts%2520are%2520contained%2520within%2520a%2520single%2520or%2520few%2520frames%252C%2520they%250Aexhibit%2520limited%2520capability%2520in%2520effectively%2520handling%2520tasks%2520that%2520demand%2520holistic%250Avideo%2520comprehension.%2520These%2520limitations%2520are%2520especially%2520evident%2520in%2520scenarios%2520that%250Arequire%2520spatio-temporal%2520reasoning%252C%2520cross-frame%2520information%2520integration%252C%2520or%250Aresistance%2520to%2520language%2520prior%2520bias.%2520Our%2520findings%2520also%2520highlight%2520the%2520importance%250Aof%2520high-resolution%2520visual%2520input%2520and%2520sufficient%2520temporal%2520coverage%2520for%2520reliable%250AOCR%2520in%2520dynamic%2520video%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MME-VideoOCR%3A%20Evaluating%20OCR-Based%20Capabilities%20of%20Multimodal%20LLMs%20in%0A%20%20Video%20Scenarios&entry.906535625=Yang%20Shi%20and%20Huanqian%20Wang%20and%20Wulin%20Xie%20and%20Huanyao%20Zhang%20and%20Lijie%20Zhao%20and%20Yi-Fan%20Zhang%20and%20Xinfeng%20Li%20and%20Chaoyou%20Fu%20and%20Zhuoer%20Wen%20and%20Wenting%20Liu%20and%20Zhuoran%20Zhang%20and%20Xinlong%20Chen%20and%20Bohan%20Zeng%20and%20Sihan%20Yang%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Haotian%20Wang%20and%20Wenjing%20Yang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20considerable%20accuracy%0Ain%20Optical%20Character%20Recognition%20%28OCR%29%20from%20static%20images.%20However%2C%20their%0Aefficacy%20in%20video%20OCR%20is%20significantly%20diminished%20due%20to%20factors%20such%20as%20motion%0Ablur%2C%20temporal%20variations%2C%20and%20visual%20effects%20inherent%20in%20video%20content.%20To%0Aprovide%20clearer%20guidance%20for%20training%20practical%20MLLMs%2C%20we%20introduce%20the%0AMME-VideoOCR%20benchmark%2C%20which%20encompasses%20a%20comprehensive%20range%20of%20video%20OCR%0Aapplication%20scenarios.%20MME-VideoOCR%20features%2010%20task%20categories%20comprising%2025%0Aindividual%20tasks%20and%20spans%2044%20diverse%20scenarios.%20These%20tasks%20extend%20beyond%20text%0Arecognition%20to%20incorporate%20deeper%20comprehension%20and%20reasoning%20of%20textual%0Acontent%20within%20videos.%20The%20benchmark%20consists%20of%201%2C464%20videos%20with%20varying%0Aresolutions%2C%20aspect%20ratios%2C%20and%20durations%2C%20along%20with%202%2C000%20meticulously%0Acurated%2C%20manually%20annotated%20question-answer%20pairs.%20We%20evaluate%2018%0Astate-of-the-art%20MLLMs%20on%20MME-VideoOCR%2C%20revealing%20that%20even%20the%20best-performing%0Amodel%20%28Gemini-2.5%20Pro%29%20achieves%20an%20accuracy%20of%20only%2073.7%25.%20Fine-grained%0Aanalysis%20indicates%20that%20while%20existing%20MLLMs%20demonstrate%20strong%20performance%20on%0Atasks%20where%20relevant%20texts%20are%20contained%20within%20a%20single%20or%20few%20frames%2C%20they%0Aexhibit%20limited%20capability%20in%20effectively%20handling%20tasks%20that%20demand%20holistic%0Avideo%20comprehension.%20These%20limitations%20are%20especially%20evident%20in%20scenarios%20that%0Arequire%20spatio-temporal%20reasoning%2C%20cross-frame%20information%20integration%2C%20or%0Aresistance%20to%20language%20prior%20bias.%20Our%20findings%20also%20highlight%20the%20importance%0Aof%20high-resolution%20visual%20input%20and%20sufficient%20temporal%20coverage%20for%20reliable%0AOCR%20in%20dynamic%20video%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21333v1&entry.124074799=Read"},
{"title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?", "author": "Junhao Cheng and Yuying Ge and Teng Wang and Yixiao Ge and Jing Liao and Ying Shan", "abstract": "  Recent advances in CoT reasoning and RL post-training have been reported to\nenhance video reasoning capabilities of MLLMs. This progress naturally raises a\nquestion: can these models perform complex video reasoning in a manner\ncomparable to human experts? However, existing video benchmarks primarily\nevaluate visual perception and grounding abilities, with questions that can be\nanswered based on explicit prompts or isolated visual cues. Such benchmarks do\nnot fully capture the intricacies of real-world reasoning, where humans must\nactively search for, integrate, and analyze multiple clues before reaching a\nconclusion. To address this issue, we present Video-Holmes, a benchmark\ninspired by the reasoning process of Sherlock Holmes, designed to evaluate the\ncomplex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837\nquestions derived from 270 manually annotated suspense short films, which spans\nseven carefully designed tasks. Each task is constructed by first identifying\nkey events and causal relationships within films, and then designing questions\nthat require models to actively locate and connect multiple relevant visual\nclues scattered across different video segments. Our comprehensive evaluation\nof state-of-the-art MLLMs reveals that, while these models generally excel at\nvisual perception, they encounter substantial difficulties with integrating\ninformation and often miss critical clues. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models\nscoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for\nmultimodal reasoning, motivating models to reason more like humans and\nemphasizing the ongoing challenges in this field. The benchmark is released in\nhttps://github.com/TencentARC/Video-Holmes.\n", "link": "http://arxiv.org/abs/2505.21374v1", "date": "2025-05-27", "relevancy": 2.3092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-Holmes%3A%20Can%20MLLM%20Think%20Like%20Holmes%20for%20Complex%20Video%20Reasoning%3F&body=Title%3A%20Video-Holmes%3A%20Can%20MLLM%20Think%20Like%20Holmes%20for%20Complex%20Video%20Reasoning%3F%0AAuthor%3A%20Junhao%20Cheng%20and%20Yuying%20Ge%20and%20Teng%20Wang%20and%20Yixiao%20Ge%20and%20Jing%20Liao%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Recent%20advances%20in%20CoT%20reasoning%20and%20RL%20post-training%20have%20been%20reported%20to%0Aenhance%20video%20reasoning%20capabilities%20of%20MLLMs.%20This%20progress%20naturally%20raises%20a%0Aquestion%3A%20can%20these%20models%20perform%20complex%20video%20reasoning%20in%20a%20manner%0Acomparable%20to%20human%20experts%3F%20However%2C%20existing%20video%20benchmarks%20primarily%0Aevaluate%20visual%20perception%20and%20grounding%20abilities%2C%20with%20questions%20that%20can%20be%0Aanswered%20based%20on%20explicit%20prompts%20or%20isolated%20visual%20cues.%20Such%20benchmarks%20do%0Anot%20fully%20capture%20the%20intricacies%20of%20real-world%20reasoning%2C%20where%20humans%20must%0Aactively%20search%20for%2C%20integrate%2C%20and%20analyze%20multiple%20clues%20before%20reaching%20a%0Aconclusion.%20To%20address%20this%20issue%2C%20we%20present%20Video-Holmes%2C%20a%20benchmark%0Ainspired%20by%20the%20reasoning%20process%20of%20Sherlock%20Holmes%2C%20designed%20to%20evaluate%20the%0Acomplex%20video%20reasoning%20capabilities%20of%20MLLMs.%20Video-Holmes%20consists%20of%201%2C837%0Aquestions%20derived%20from%20270%20manually%20annotated%20suspense%20short%20films%2C%20which%20spans%0Aseven%20carefully%20designed%20tasks.%20Each%20task%20is%20constructed%20by%20first%20identifying%0Akey%20events%20and%20causal%20relationships%20within%20films%2C%20and%20then%20designing%20questions%0Athat%20require%20models%20to%20actively%20locate%20and%20connect%20multiple%20relevant%20visual%0Aclues%20scattered%20across%20different%20video%20segments.%20Our%20comprehensive%20evaluation%0Aof%20state-of-the-art%20MLLMs%20reveals%20that%2C%20while%20these%20models%20generally%20excel%20at%0Avisual%20perception%2C%20they%20encounter%20substantial%20difficulties%20with%20integrating%0Ainformation%20and%20often%20miss%20critical%20clues.%20For%20example%2C%20the%20best-performing%0Amodel%2C%20Gemini-2.5-Pro%2C%20achieves%20an%20accuracy%20of%20only%2045%25%2C%20with%20most%20models%0Ascoring%20below%2040%25.%20We%20aim%20that%20Video-Holmes%20can%20serve%20as%20a%20%22Holmes-test%22%20for%0Amultimodal%20reasoning%2C%20motivating%20models%20to%20reason%20more%20like%20humans%20and%0Aemphasizing%20the%20ongoing%20challenges%20in%20this%20field.%20The%20benchmark%20is%20released%20in%0Ahttps%3A//github.com/TencentARC/Video-Holmes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-Holmes%253A%2520Can%2520MLLM%2520Think%2520Like%2520Holmes%2520for%2520Complex%2520Video%2520Reasoning%253F%26entry.906535625%3DJunhao%2520Cheng%2520and%2520Yuying%2520Ge%2520and%2520Teng%2520Wang%2520and%2520Yixiao%2520Ge%2520and%2520Jing%2520Liao%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520CoT%2520reasoning%2520and%2520RL%2520post-training%2520have%2520been%2520reported%2520to%250Aenhance%2520video%2520reasoning%2520capabilities%2520of%2520MLLMs.%2520This%2520progress%2520naturally%2520raises%2520a%250Aquestion%253A%2520can%2520these%2520models%2520perform%2520complex%2520video%2520reasoning%2520in%2520a%2520manner%250Acomparable%2520to%2520human%2520experts%253F%2520However%252C%2520existing%2520video%2520benchmarks%2520primarily%250Aevaluate%2520visual%2520perception%2520and%2520grounding%2520abilities%252C%2520with%2520questions%2520that%2520can%2520be%250Aanswered%2520based%2520on%2520explicit%2520prompts%2520or%2520isolated%2520visual%2520cues.%2520Such%2520benchmarks%2520do%250Anot%2520fully%2520capture%2520the%2520intricacies%2520of%2520real-world%2520reasoning%252C%2520where%2520humans%2520must%250Aactively%2520search%2520for%252C%2520integrate%252C%2520and%2520analyze%2520multiple%2520clues%2520before%2520reaching%2520a%250Aconclusion.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520Video-Holmes%252C%2520a%2520benchmark%250Ainspired%2520by%2520the%2520reasoning%2520process%2520of%2520Sherlock%2520Holmes%252C%2520designed%2520to%2520evaluate%2520the%250Acomplex%2520video%2520reasoning%2520capabilities%2520of%2520MLLMs.%2520Video-Holmes%2520consists%2520of%25201%252C837%250Aquestions%2520derived%2520from%2520270%2520manually%2520annotated%2520suspense%2520short%2520films%252C%2520which%2520spans%250Aseven%2520carefully%2520designed%2520tasks.%2520Each%2520task%2520is%2520constructed%2520by%2520first%2520identifying%250Akey%2520events%2520and%2520causal%2520relationships%2520within%2520films%252C%2520and%2520then%2520designing%2520questions%250Athat%2520require%2520models%2520to%2520actively%2520locate%2520and%2520connect%2520multiple%2520relevant%2520visual%250Aclues%2520scattered%2520across%2520different%2520video%2520segments.%2520Our%2520comprehensive%2520evaluation%250Aof%2520state-of-the-art%2520MLLMs%2520reveals%2520that%252C%2520while%2520these%2520models%2520generally%2520excel%2520at%250Avisual%2520perception%252C%2520they%2520encounter%2520substantial%2520difficulties%2520with%2520integrating%250Ainformation%2520and%2520often%2520miss%2520critical%2520clues.%2520For%2520example%252C%2520the%2520best-performing%250Amodel%252C%2520Gemini-2.5-Pro%252C%2520achieves%2520an%2520accuracy%2520of%2520only%252045%2525%252C%2520with%2520most%2520models%250Ascoring%2520below%252040%2525.%2520We%2520aim%2520that%2520Video-Holmes%2520can%2520serve%2520as%2520a%2520%2522Holmes-test%2522%2520for%250Amultimodal%2520reasoning%252C%2520motivating%2520models%2520to%2520reason%2520more%2520like%2520humans%2520and%250Aemphasizing%2520the%2520ongoing%2520challenges%2520in%2520this%2520field.%2520The%2520benchmark%2520is%2520released%2520in%250Ahttps%253A//github.com/TencentARC/Video-Holmes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Holmes%3A%20Can%20MLLM%20Think%20Like%20Holmes%20for%20Complex%20Video%20Reasoning%3F&entry.906535625=Junhao%20Cheng%20and%20Yuying%20Ge%20and%20Teng%20Wang%20and%20Yixiao%20Ge%20and%20Jing%20Liao%20and%20Ying%20Shan&entry.1292438233=%20%20Recent%20advances%20in%20CoT%20reasoning%20and%20RL%20post-training%20have%20been%20reported%20to%0Aenhance%20video%20reasoning%20capabilities%20of%20MLLMs.%20This%20progress%20naturally%20raises%20a%0Aquestion%3A%20can%20these%20models%20perform%20complex%20video%20reasoning%20in%20a%20manner%0Acomparable%20to%20human%20experts%3F%20However%2C%20existing%20video%20benchmarks%20primarily%0Aevaluate%20visual%20perception%20and%20grounding%20abilities%2C%20with%20questions%20that%20can%20be%0Aanswered%20based%20on%20explicit%20prompts%20or%20isolated%20visual%20cues.%20Such%20benchmarks%20do%0Anot%20fully%20capture%20the%20intricacies%20of%20real-world%20reasoning%2C%20where%20humans%20must%0Aactively%20search%20for%2C%20integrate%2C%20and%20analyze%20multiple%20clues%20before%20reaching%20a%0Aconclusion.%20To%20address%20this%20issue%2C%20we%20present%20Video-Holmes%2C%20a%20benchmark%0Ainspired%20by%20the%20reasoning%20process%20of%20Sherlock%20Holmes%2C%20designed%20to%20evaluate%20the%0Acomplex%20video%20reasoning%20capabilities%20of%20MLLMs.%20Video-Holmes%20consists%20of%201%2C837%0Aquestions%20derived%20from%20270%20manually%20annotated%20suspense%20short%20films%2C%20which%20spans%0Aseven%20carefully%20designed%20tasks.%20Each%20task%20is%20constructed%20by%20first%20identifying%0Akey%20events%20and%20causal%20relationships%20within%20films%2C%20and%20then%20designing%20questions%0Athat%20require%20models%20to%20actively%20locate%20and%20connect%20multiple%20relevant%20visual%0Aclues%20scattered%20across%20different%20video%20segments.%20Our%20comprehensive%20evaluation%0Aof%20state-of-the-art%20MLLMs%20reveals%20that%2C%20while%20these%20models%20generally%20excel%20at%0Avisual%20perception%2C%20they%20encounter%20substantial%20difficulties%20with%20integrating%0Ainformation%20and%20often%20miss%20critical%20clues.%20For%20example%2C%20the%20best-performing%0Amodel%2C%20Gemini-2.5-Pro%2C%20achieves%20an%20accuracy%20of%20only%2045%25%2C%20with%20most%20models%0Ascoring%20below%2040%25.%20We%20aim%20that%20Video-Holmes%20can%20serve%20as%20a%20%22Holmes-test%22%20for%0Amultimodal%20reasoning%2C%20motivating%20models%20to%20reason%20more%20like%20humans%20and%0Aemphasizing%20the%20ongoing%20challenges%20in%20this%20field.%20The%20benchmark%20is%20released%20in%0Ahttps%3A//github.com/TencentARC/Video-Holmes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21374v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters", "author": "Byungjun Kim and Minju Kim and Dayeon Seo and Bugeun Kim", "abstract": "  We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions with active NPCs have been a focus, price negotiations\nbetween merchant NPCs and players remain underexplored. First, passive pricing\nrefers to the limited ability of merchants to modify predefined item prices.\nSecond, passive communication means that merchants can only interact with\nplayers in a scripted manner. To tackle these issues and create an active\nmerchant NPC, we propose a merchant framework based on large language models\n(LLMs), called MART, which consists of an appraiser module and a negotiator\nmodule. We conducted two experiments to explore various implementation options\nunder different training methods and LLM sizes, considering a range of possible\ngame environments. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs.\n", "link": "http://arxiv.org/abs/2412.11189v3", "date": "2025-05-27", "relevancy": 2.3079, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Active%20Merchant%20Non-player%0A%20%20Characters&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Active%20Merchant%20Non-player%0A%20%20Characters%0AAuthor%3A%20Byungjun%20Kim%20and%20Minju%20Kim%20and%20Dayeon%20Seo%20and%20Bugeun%20Kim%0AAbstract%3A%20%20%20We%20highlight%20two%20significant%20issues%20leading%20to%20the%20passivity%20of%20current%0Amerchant%20non-player%20characters%20%28NPCs%29%3A%20pricing%20and%20communication.%20While%0Aimmersive%20interactions%20with%20active%20NPCs%20have%20been%20a%20focus%2C%20price%20negotiations%0Abetween%20merchant%20NPCs%20and%20players%20remain%20underexplored.%20First%2C%20passive%20pricing%0Arefers%20to%20the%20limited%20ability%20of%20merchants%20to%20modify%20predefined%20item%20prices.%0ASecond%2C%20passive%20communication%20means%20that%20merchants%20can%20only%20interact%20with%0Aplayers%20in%20a%20scripted%20manner.%20To%20tackle%20these%20issues%20and%20create%20an%20active%0Amerchant%20NPC%2C%20we%20propose%20a%20merchant%20framework%20based%20on%20large%20language%20models%0A%28LLMs%29%2C%20called%20MART%2C%20which%20consists%20of%20an%20appraiser%20module%20and%20a%20negotiator%0Amodule.%20We%20conducted%20two%20experiments%20to%20explore%20various%20implementation%20options%0Aunder%20different%20training%20methods%20and%20LLM%20sizes%2C%20considering%20a%20range%20of%20possible%0Agame%20environments.%20Our%20findings%20indicate%20that%20finetuning%20methods%2C%20such%20as%0Asupervised%20finetuning%20%28SFT%29%20and%20knowledge%20distillation%20%28KD%29%2C%20are%20effective%20in%0Ausing%20smaller%20LLMs%20to%20implement%20active%20merchant%20NPCs.%20Additionally%2C%20we%20found%0Athree%20irregular%20cases%20arising%20from%20the%20responses%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11189v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Active%2520Merchant%2520Non-player%250A%2520%2520Characters%26entry.906535625%3DByungjun%2520Kim%2520and%2520Minju%2520Kim%2520and%2520Dayeon%2520Seo%2520and%2520Bugeun%2520Kim%26entry.1292438233%3D%2520%2520We%2520highlight%2520two%2520significant%2520issues%2520leading%2520to%2520the%2520passivity%2520of%2520current%250Amerchant%2520non-player%2520characters%2520%2528NPCs%2529%253A%2520pricing%2520and%2520communication.%2520While%250Aimmersive%2520interactions%2520with%2520active%2520NPCs%2520have%2520been%2520a%2520focus%252C%2520price%2520negotiations%250Abetween%2520merchant%2520NPCs%2520and%2520players%2520remain%2520underexplored.%2520First%252C%2520passive%2520pricing%250Arefers%2520to%2520the%2520limited%2520ability%2520of%2520merchants%2520to%2520modify%2520predefined%2520item%2520prices.%250ASecond%252C%2520passive%2520communication%2520means%2520that%2520merchants%2520can%2520only%2520interact%2520with%250Aplayers%2520in%2520a%2520scripted%2520manner.%2520To%2520tackle%2520these%2520issues%2520and%2520create%2520an%2520active%250Amerchant%2520NPC%252C%2520we%2520propose%2520a%2520merchant%2520framework%2520based%2520on%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520called%2520MART%252C%2520which%2520consists%2520of%2520an%2520appraiser%2520module%2520and%2520a%2520negotiator%250Amodule.%2520We%2520conducted%2520two%2520experiments%2520to%2520explore%2520various%2520implementation%2520options%250Aunder%2520different%2520training%2520methods%2520and%2520LLM%2520sizes%252C%2520considering%2520a%2520range%2520of%2520possible%250Agame%2520environments.%2520Our%2520findings%2520indicate%2520that%2520finetuning%2520methods%252C%2520such%2520as%250Asupervised%2520finetuning%2520%2528SFT%2529%2520and%2520knowledge%2520distillation%2520%2528KD%2529%252C%2520are%2520effective%2520in%250Ausing%2520smaller%2520LLMs%2520to%2520implement%2520active%2520merchant%2520NPCs.%2520Additionally%252C%2520we%2520found%250Athree%2520irregular%2520cases%2520arising%2520from%2520the%2520responses%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11189v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Active%20Merchant%20Non-player%0A%20%20Characters&entry.906535625=Byungjun%20Kim%20and%20Minju%20Kim%20and%20Dayeon%20Seo%20and%20Bugeun%20Kim&entry.1292438233=%20%20We%20highlight%20two%20significant%20issues%20leading%20to%20the%20passivity%20of%20current%0Amerchant%20non-player%20characters%20%28NPCs%29%3A%20pricing%20and%20communication.%20While%0Aimmersive%20interactions%20with%20active%20NPCs%20have%20been%20a%20focus%2C%20price%20negotiations%0Abetween%20merchant%20NPCs%20and%20players%20remain%20underexplored.%20First%2C%20passive%20pricing%0Arefers%20to%20the%20limited%20ability%20of%20merchants%20to%20modify%20predefined%20item%20prices.%0ASecond%2C%20passive%20communication%20means%20that%20merchants%20can%20only%20interact%20with%0Aplayers%20in%20a%20scripted%20manner.%20To%20tackle%20these%20issues%20and%20create%20an%20active%0Amerchant%20NPC%2C%20we%20propose%20a%20merchant%20framework%20based%20on%20large%20language%20models%0A%28LLMs%29%2C%20called%20MART%2C%20which%20consists%20of%20an%20appraiser%20module%20and%20a%20negotiator%0Amodule.%20We%20conducted%20two%20experiments%20to%20explore%20various%20implementation%20options%0Aunder%20different%20training%20methods%20and%20LLM%20sizes%2C%20considering%20a%20range%20of%20possible%0Agame%20environments.%20Our%20findings%20indicate%20that%20finetuning%20methods%2C%20such%20as%0Asupervised%20finetuning%20%28SFT%29%20and%20knowledge%20distillation%20%28KD%29%2C%20are%20effective%20in%0Ausing%20smaller%20LLMs%20to%20implement%20active%20merchant%20NPCs.%20Additionally%2C%20we%20found%0Athree%20irregular%20cases%20arising%20from%20the%20responses%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11189v3&entry.124074799=Read"},
{"title": "Can Large Language Models Understand Symbolic Graphics Programs?", "author": "Zeju Qiu and Weiyang Liu and Haiwen Feng and Zhen Liu and Tim Z. Xiao and Katherine M. Collins and Joshua B. Tenenbaum and Adrian Weller and Michael J. Black and Bernhard Sch\u00f6lkopf", "abstract": "  Against the backdrop of enthusiasm for large language models (LLMs), there is\na growing need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer semantic\nquestions about the images or 3D geometries without a vision encoder. To\nsemantically understand the symbolic programs, LLMs would need to possess the\nability to \"imagine\" and reason how the corresponding graphics content would\nlook with only the symbolic description of the local curvatures and strokes. We\nuse this task to evaluate LLMs by creating a large benchmark for the semantic\nvisual understanding of symbolic graphics programs, built procedurally with\nminimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.\n", "link": "http://arxiv.org/abs/2408.08313v4", "date": "2025-05-27", "relevancy": 2.3074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Understand%20Symbolic%20Graphics%20Programs%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Understand%20Symbolic%20Graphics%20Programs%3F%0AAuthor%3A%20Zeju%20Qiu%20and%20Weiyang%20Liu%20and%20Haiwen%20Feng%20and%20Zhen%20Liu%20and%20Tim%20Z.%20Xiao%20and%20Katherine%20M.%20Collins%20and%20Joshua%20B.%20Tenenbaum%20and%20Adrian%20Weller%20and%20Michael%20J.%20Black%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20Against%20the%20backdrop%20of%20enthusiasm%20for%20large%20language%20models%20%28LLMs%29%2C%20there%20is%0Aa%20growing%20need%20to%20scientifically%20assess%20their%20capabilities%20and%20shortcomings.%0AThis%20is%20nontrivial%20in%20part%20because%20it%20is%20difficult%20to%20find%20tasks%20which%20the%0Amodels%20have%20not%20encountered%20during%20training.%20Utilizing%20symbolic%20graphics%0Aprograms%2C%20we%20propose%20a%20domain%20well-suited%20to%20test%20multiple%20spatial-semantic%0Areasoning%20skills%20of%20LLMs.%20Popular%20in%20computer%20graphics%2C%20these%20programs%0Aprocedurally%20generate%20visual%20data.%20While%20LLMs%20exhibit%20impressive%20skills%20in%0Ageneral%20program%20synthesis%20and%20analysis%2C%20symbolic%20graphics%20programs%20offer%20a%20new%0Alayer%20of%20evaluation%3A%20they%20allow%20us%20to%20test%20an%20LLM%27s%20ability%20to%20answer%20semantic%0Aquestions%20about%20the%20images%20or%203D%20geometries%20without%20a%20vision%20encoder.%20To%0Asemantically%20understand%20the%20symbolic%20programs%2C%20LLMs%20would%20need%20to%20possess%20the%0Aability%20to%20%22imagine%22%20and%20reason%20how%20the%20corresponding%20graphics%20content%20would%0Alook%20with%20only%20the%20symbolic%20description%20of%20the%20local%20curvatures%20and%20strokes.%20We%0Ause%20this%20task%20to%20evaluate%20LLMs%20by%20creating%20a%20large%20benchmark%20for%20the%20semantic%0Avisual%20understanding%20of%20symbolic%20graphics%20programs%2C%20built%20procedurally%20with%0Aminimal%20human%20effort.%20Particular%20emphasis%20is%20placed%20on%20transformations%20of%0Aimages%20that%20leave%20the%20image%20level%20semantics%20invariant%20while%20introducing%0Asignificant%20changes%20to%20the%20underlying%20program.%20We%20evaluate%20commercial%20and%0Aopen-source%20LLMs%20on%20our%20benchmark%20to%20assess%20their%20ability%20to%20reason%20about%0Avisual%20output%20of%20programs%2C%20finding%20that%20LLMs%20considered%20stronger%20at%20reasoning%0Agenerally%20perform%20better.%20Lastly%2C%20we%20introduce%20a%20novel%20method%20to%20improve%20this%0Aability%20--%20Symbolic%20Instruction%20Tuning%20%28SIT%29%2C%20in%20which%20the%20LLM%20is%20finetuned%0Awith%20pre-collected%20instruction%20data%20on%20symbolic%20graphics%20programs.%0AInterestingly%2C%20we%20find%20that%20SIT%20not%20only%20improves%20LLM%27s%20understanding%20on%0Asymbolic%20programs%2C%20but%20it%20also%20improves%20general%20reasoning%20ability%20on%20various%0Aother%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08313v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Understand%2520Symbolic%2520Graphics%2520Programs%253F%26entry.906535625%3DZeju%2520Qiu%2520and%2520Weiyang%2520Liu%2520and%2520Haiwen%2520Feng%2520and%2520Zhen%2520Liu%2520and%2520Tim%2520Z.%2520Xiao%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Adrian%2520Weller%2520and%2520Michael%2520J.%2520Black%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520Against%2520the%2520backdrop%2520of%2520enthusiasm%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%2520is%250Aa%2520growing%2520need%2520to%2520scientifically%2520assess%2520their%2520capabilities%2520and%2520shortcomings.%250AThis%2520is%2520nontrivial%2520in%2520part%2520because%2520it%2520is%2520difficult%2520to%2520find%2520tasks%2520which%2520the%250Amodels%2520have%2520not%2520encountered%2520during%2520training.%2520Utilizing%2520symbolic%2520graphics%250Aprograms%252C%2520we%2520propose%2520a%2520domain%2520well-suited%2520to%2520test%2520multiple%2520spatial-semantic%250Areasoning%2520skills%2520of%2520LLMs.%2520Popular%2520in%2520computer%2520graphics%252C%2520these%2520programs%250Aprocedurally%2520generate%2520visual%2520data.%2520While%2520LLMs%2520exhibit%2520impressive%2520skills%2520in%250Ageneral%2520program%2520synthesis%2520and%2520analysis%252C%2520symbolic%2520graphics%2520programs%2520offer%2520a%2520new%250Alayer%2520of%2520evaluation%253A%2520they%2520allow%2520us%2520to%2520test%2520an%2520LLM%2527s%2520ability%2520to%2520answer%2520semantic%250Aquestions%2520about%2520the%2520images%2520or%25203D%2520geometries%2520without%2520a%2520vision%2520encoder.%2520To%250Asemantically%2520understand%2520the%2520symbolic%2520programs%252C%2520LLMs%2520would%2520need%2520to%2520possess%2520the%250Aability%2520to%2520%2522imagine%2522%2520and%2520reason%2520how%2520the%2520corresponding%2520graphics%2520content%2520would%250Alook%2520with%2520only%2520the%2520symbolic%2520description%2520of%2520the%2520local%2520curvatures%2520and%2520strokes.%2520We%250Ause%2520this%2520task%2520to%2520evaluate%2520LLMs%2520by%2520creating%2520a%2520large%2520benchmark%2520for%2520the%2520semantic%250Avisual%2520understanding%2520of%2520symbolic%2520graphics%2520programs%252C%2520built%2520procedurally%2520with%250Aminimal%2520human%2520effort.%2520Particular%2520emphasis%2520is%2520placed%2520on%2520transformations%2520of%250Aimages%2520that%2520leave%2520the%2520image%2520level%2520semantics%2520invariant%2520while%2520introducing%250Asignificant%2520changes%2520to%2520the%2520underlying%2520program.%2520We%2520evaluate%2520commercial%2520and%250Aopen-source%2520LLMs%2520on%2520our%2520benchmark%2520to%2520assess%2520their%2520ability%2520to%2520reason%2520about%250Avisual%2520output%2520of%2520programs%252C%2520finding%2520that%2520LLMs%2520considered%2520stronger%2520at%2520reasoning%250Agenerally%2520perform%2520better.%2520Lastly%252C%2520we%2520introduce%2520a%2520novel%2520method%2520to%2520improve%2520this%250Aability%2520--%2520Symbolic%2520Instruction%2520Tuning%2520%2528SIT%2529%252C%2520in%2520which%2520the%2520LLM%2520is%2520finetuned%250Awith%2520pre-collected%2520instruction%2520data%2520on%2520symbolic%2520graphics%2520programs.%250AInterestingly%252C%2520we%2520find%2520that%2520SIT%2520not%2520only%2520improves%2520LLM%2527s%2520understanding%2520on%250Asymbolic%2520programs%252C%2520but%2520it%2520also%2520improves%2520general%2520reasoning%2520ability%2520on%2520various%250Aother%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08313v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Understand%20Symbolic%20Graphics%20Programs%3F&entry.906535625=Zeju%20Qiu%20and%20Weiyang%20Liu%20and%20Haiwen%20Feng%20and%20Zhen%20Liu%20and%20Tim%20Z.%20Xiao%20and%20Katherine%20M.%20Collins%20and%20Joshua%20B.%20Tenenbaum%20and%20Adrian%20Weller%20and%20Michael%20J.%20Black%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20Against%20the%20backdrop%20of%20enthusiasm%20for%20large%20language%20models%20%28LLMs%29%2C%20there%20is%0Aa%20growing%20need%20to%20scientifically%20assess%20their%20capabilities%20and%20shortcomings.%0AThis%20is%20nontrivial%20in%20part%20because%20it%20is%20difficult%20to%20find%20tasks%20which%20the%0Amodels%20have%20not%20encountered%20during%20training.%20Utilizing%20symbolic%20graphics%0Aprograms%2C%20we%20propose%20a%20domain%20well-suited%20to%20test%20multiple%20spatial-semantic%0Areasoning%20skills%20of%20LLMs.%20Popular%20in%20computer%20graphics%2C%20these%20programs%0Aprocedurally%20generate%20visual%20data.%20While%20LLMs%20exhibit%20impressive%20skills%20in%0Ageneral%20program%20synthesis%20and%20analysis%2C%20symbolic%20graphics%20programs%20offer%20a%20new%0Alayer%20of%20evaluation%3A%20they%20allow%20us%20to%20test%20an%20LLM%27s%20ability%20to%20answer%20semantic%0Aquestions%20about%20the%20images%20or%203D%20geometries%20without%20a%20vision%20encoder.%20To%0Asemantically%20understand%20the%20symbolic%20programs%2C%20LLMs%20would%20need%20to%20possess%20the%0Aability%20to%20%22imagine%22%20and%20reason%20how%20the%20corresponding%20graphics%20content%20would%0Alook%20with%20only%20the%20symbolic%20description%20of%20the%20local%20curvatures%20and%20strokes.%20We%0Ause%20this%20task%20to%20evaluate%20LLMs%20by%20creating%20a%20large%20benchmark%20for%20the%20semantic%0Avisual%20understanding%20of%20symbolic%20graphics%20programs%2C%20built%20procedurally%20with%0Aminimal%20human%20effort.%20Particular%20emphasis%20is%20placed%20on%20transformations%20of%0Aimages%20that%20leave%20the%20image%20level%20semantics%20invariant%20while%20introducing%0Asignificant%20changes%20to%20the%20underlying%20program.%20We%20evaluate%20commercial%20and%0Aopen-source%20LLMs%20on%20our%20benchmark%20to%20assess%20their%20ability%20to%20reason%20about%0Avisual%20output%20of%20programs%2C%20finding%20that%20LLMs%20considered%20stronger%20at%20reasoning%0Agenerally%20perform%20better.%20Lastly%2C%20we%20introduce%20a%20novel%20method%20to%20improve%20this%0Aability%20--%20Symbolic%20Instruction%20Tuning%20%28SIT%29%2C%20in%20which%20the%20LLM%20is%20finetuned%0Awith%20pre-collected%20instruction%20data%20on%20symbolic%20graphics%20programs.%0AInterestingly%2C%20we%20find%20that%20SIT%20not%20only%20improves%20LLM%27s%20understanding%20on%0Asymbolic%20programs%2C%20but%20it%20also%20improves%20general%20reasoning%20ability%20on%20various%0Aother%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08313v4&entry.124074799=Read"},
{"title": "Quantum AIXI: Universal Intelligence via Quantum Information", "author": "Elija Perrier", "abstract": "  AIXI is a widely studied model of artificial general intelligence (AGI) based\nupon principles of induction and reinforcement learning. However, AIXI is\nfundamentally classical in nature - as are the environments in which it is\nmodelled. Given the universe is quantum mechanical in nature and the\nexponential overhead required to simulate quantum mechanical systems\nclassically, the question arises as to whether there are quantum mechanical\nanalogues of AIXI which are theoretically consistent or practically feasible as\nmodels of universal intelligence. To address this question, we extend the\nframework to quantum information and present Quantum AIXI (QAIXI). We introduce\na model of quantum agent/environment interaction based upon quantum and\nclassical registers and channels, showing how quantum AIXI agents may take both\nclassical and quantum actions. We formulate the key components of AIXI in\nquantum information terms, extending previous research on quantum Kolmogorov\ncomplexity and a QAIXI value function. We discuss conditions and limitations\nupon quantum Solomonoff induction and show how contextuality fundamentally\naffects QAIXI models.\n", "link": "http://arxiv.org/abs/2505.21170v1", "date": "2025-05-27", "relevancy": 2.3057, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20AIXI%3A%20Universal%20Intelligence%20via%20Quantum%20Information&body=Title%3A%20Quantum%20AIXI%3A%20Universal%20Intelligence%20via%20Quantum%20Information%0AAuthor%3A%20Elija%20Perrier%0AAbstract%3A%20%20%20AIXI%20is%20a%20widely%20studied%20model%20of%20artificial%20general%20intelligence%20%28AGI%29%20based%0Aupon%20principles%20of%20induction%20and%20reinforcement%20learning.%20However%2C%20AIXI%20is%0Afundamentally%20classical%20in%20nature%20-%20as%20are%20the%20environments%20in%20which%20it%20is%0Amodelled.%20Given%20the%20universe%20is%20quantum%20mechanical%20in%20nature%20and%20the%0Aexponential%20overhead%20required%20to%20simulate%20quantum%20mechanical%20systems%0Aclassically%2C%20the%20question%20arises%20as%20to%20whether%20there%20are%20quantum%20mechanical%0Aanalogues%20of%20AIXI%20which%20are%20theoretically%20consistent%20or%20practically%20feasible%20as%0Amodels%20of%20universal%20intelligence.%20To%20address%20this%20question%2C%20we%20extend%20the%0Aframework%20to%20quantum%20information%20and%20present%20Quantum%20AIXI%20%28QAIXI%29.%20We%20introduce%0Aa%20model%20of%20quantum%20agent/environment%20interaction%20based%20upon%20quantum%20and%0Aclassical%20registers%20and%20channels%2C%20showing%20how%20quantum%20AIXI%20agents%20may%20take%20both%0Aclassical%20and%20quantum%20actions.%20We%20formulate%20the%20key%20components%20of%20AIXI%20in%0Aquantum%20information%20terms%2C%20extending%20previous%20research%20on%20quantum%20Kolmogorov%0Acomplexity%20and%20a%20QAIXI%20value%20function.%20We%20discuss%20conditions%20and%20limitations%0Aupon%20quantum%20Solomonoff%20induction%20and%20show%20how%20contextuality%20fundamentally%0Aaffects%20QAIXI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520AIXI%253A%2520Universal%2520Intelligence%2520via%2520Quantum%2520Information%26entry.906535625%3DElija%2520Perrier%26entry.1292438233%3D%2520%2520AIXI%2520is%2520a%2520widely%2520studied%2520model%2520of%2520artificial%2520general%2520intelligence%2520%2528AGI%2529%2520based%250Aupon%2520principles%2520of%2520induction%2520and%2520reinforcement%2520learning.%2520However%252C%2520AIXI%2520is%250Afundamentally%2520classical%2520in%2520nature%2520-%2520as%2520are%2520the%2520environments%2520in%2520which%2520it%2520is%250Amodelled.%2520Given%2520the%2520universe%2520is%2520quantum%2520mechanical%2520in%2520nature%2520and%2520the%250Aexponential%2520overhead%2520required%2520to%2520simulate%2520quantum%2520mechanical%2520systems%250Aclassically%252C%2520the%2520question%2520arises%2520as%2520to%2520whether%2520there%2520are%2520quantum%2520mechanical%250Aanalogues%2520of%2520AIXI%2520which%2520are%2520theoretically%2520consistent%2520or%2520practically%2520feasible%2520as%250Amodels%2520of%2520universal%2520intelligence.%2520To%2520address%2520this%2520question%252C%2520we%2520extend%2520the%250Aframework%2520to%2520quantum%2520information%2520and%2520present%2520Quantum%2520AIXI%2520%2528QAIXI%2529.%2520We%2520introduce%250Aa%2520model%2520of%2520quantum%2520agent/environment%2520interaction%2520based%2520upon%2520quantum%2520and%250Aclassical%2520registers%2520and%2520channels%252C%2520showing%2520how%2520quantum%2520AIXI%2520agents%2520may%2520take%2520both%250Aclassical%2520and%2520quantum%2520actions.%2520We%2520formulate%2520the%2520key%2520components%2520of%2520AIXI%2520in%250Aquantum%2520information%2520terms%252C%2520extending%2520previous%2520research%2520on%2520quantum%2520Kolmogorov%250Acomplexity%2520and%2520a%2520QAIXI%2520value%2520function.%2520We%2520discuss%2520conditions%2520and%2520limitations%250Aupon%2520quantum%2520Solomonoff%2520induction%2520and%2520show%2520how%2520contextuality%2520fundamentally%250Aaffects%2520QAIXI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20AIXI%3A%20Universal%20Intelligence%20via%20Quantum%20Information&entry.906535625=Elija%20Perrier&entry.1292438233=%20%20AIXI%20is%20a%20widely%20studied%20model%20of%20artificial%20general%20intelligence%20%28AGI%29%20based%0Aupon%20principles%20of%20induction%20and%20reinforcement%20learning.%20However%2C%20AIXI%20is%0Afundamentally%20classical%20in%20nature%20-%20as%20are%20the%20environments%20in%20which%20it%20is%0Amodelled.%20Given%20the%20universe%20is%20quantum%20mechanical%20in%20nature%20and%20the%0Aexponential%20overhead%20required%20to%20simulate%20quantum%20mechanical%20systems%0Aclassically%2C%20the%20question%20arises%20as%20to%20whether%20there%20are%20quantum%20mechanical%0Aanalogues%20of%20AIXI%20which%20are%20theoretically%20consistent%20or%20practically%20feasible%20as%0Amodels%20of%20universal%20intelligence.%20To%20address%20this%20question%2C%20we%20extend%20the%0Aframework%20to%20quantum%20information%20and%20present%20Quantum%20AIXI%20%28QAIXI%29.%20We%20introduce%0Aa%20model%20of%20quantum%20agent/environment%20interaction%20based%20upon%20quantum%20and%0Aclassical%20registers%20and%20channels%2C%20showing%20how%20quantum%20AIXI%20agents%20may%20take%20both%0Aclassical%20and%20quantum%20actions.%20We%20formulate%20the%20key%20components%20of%20AIXI%20in%0Aquantum%20information%20terms%2C%20extending%20previous%20research%20on%20quantum%20Kolmogorov%0Acomplexity%20and%20a%20QAIXI%20value%20function.%20We%20discuss%20conditions%20and%20limitations%0Aupon%20quantum%20Solomonoff%20induction%20and%20show%20how%20contextuality%20fundamentally%0Aaffects%20QAIXI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21170v1&entry.124074799=Read"},
{"title": "PLANETALIGN: A Comprehensive Python Library for Benchmarking Network\n  Alignment", "author": "Qi Yu and Zhichen Zeng and Yuchen Yan and Zhining Liu and Baoyu Jing and Ruizhong Qiu and Ariful Azad and Hanghang Tong", "abstract": "  Network alignment (NA) aims to identify node correspondence across different\nnetworks and serves as a critical cornerstone behind various downstream\nmulti-network learning tasks. Despite growing research in NA, there lacks a\ncomprehensive library that facilitates the systematic development and\nbenchmarking of NA methods. In this work, we introduce PLANETALIGN, a\ncomprehensive Python library for network alignment that features a rich\ncollection of built-in datasets, methods, and evaluation pipelines with\neasy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA\nmethods with extensible APIs for easy use and development of NA methods. Our\nstandardized evaluation pipeline encompasses a wide range of metrics, enabling\na systematic assessment of the effectiveness, scalability, and robustness of NA\nmethods. Through extensive comparative studies, we reveal practical insights\ninto the strengths and limitations of existing NA methods. We hope that\nPLANETALIGN can foster a deeper understanding of the NA problem and facilitate\nthe development and benchmarking of more effective, scalable, and robust\nmethods in the future. The source code of PLANETALIGN is available at\nhttps://github.com/yq-leo/PlanetAlign.\n", "link": "http://arxiv.org/abs/2505.21366v1", "date": "2025-05-27", "relevancy": 2.3018, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4973}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4494}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLANETALIGN%3A%20A%20Comprehensive%20Python%20Library%20for%20Benchmarking%20Network%0A%20%20Alignment&body=Title%3A%20PLANETALIGN%3A%20A%20Comprehensive%20Python%20Library%20for%20Benchmarking%20Network%0A%20%20Alignment%0AAuthor%3A%20Qi%20Yu%20and%20Zhichen%20Zeng%20and%20Yuchen%20Yan%20and%20Zhining%20Liu%20and%20Baoyu%20Jing%20and%20Ruizhong%20Qiu%20and%20Ariful%20Azad%20and%20Hanghang%20Tong%0AAbstract%3A%20%20%20Network%20alignment%20%28NA%29%20aims%20to%20identify%20node%20correspondence%20across%20different%0Anetworks%20and%20serves%20as%20a%20critical%20cornerstone%20behind%20various%20downstream%0Amulti-network%20learning%20tasks.%20Despite%20growing%20research%20in%20NA%2C%20there%20lacks%20a%0Acomprehensive%20library%20that%20facilitates%20the%20systematic%20development%20and%0Abenchmarking%20of%20NA%20methods.%20In%20this%20work%2C%20we%20introduce%20PLANETALIGN%2C%20a%0Acomprehensive%20Python%20library%20for%20network%20alignment%20that%20features%20a%20rich%0Acollection%20of%20built-in%20datasets%2C%20methods%2C%20and%20evaluation%20pipelines%20with%0Aeasy-to-use%20APIs.%20Specifically%2C%20PLANETALIGN%20integrates%2018%20datasets%20and%2014%20NA%0Amethods%20with%20extensible%20APIs%20for%20easy%20use%20and%20development%20of%20NA%20methods.%20Our%0Astandardized%20evaluation%20pipeline%20encompasses%20a%20wide%20range%20of%20metrics%2C%20enabling%0Aa%20systematic%20assessment%20of%20the%20effectiveness%2C%20scalability%2C%20and%20robustness%20of%20NA%0Amethods.%20Through%20extensive%20comparative%20studies%2C%20we%20reveal%20practical%20insights%0Ainto%20the%20strengths%20and%20limitations%20of%20existing%20NA%20methods.%20We%20hope%20that%0APLANETALIGN%20can%20foster%20a%20deeper%20understanding%20of%20the%20NA%20problem%20and%20facilitate%0Athe%20development%20and%20benchmarking%20of%20more%20effective%2C%20scalable%2C%20and%20robust%0Amethods%20in%20the%20future.%20The%20source%20code%20of%20PLANETALIGN%20is%20available%20at%0Ahttps%3A//github.com/yq-leo/PlanetAlign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLANETALIGN%253A%2520A%2520Comprehensive%2520Python%2520Library%2520for%2520Benchmarking%2520Network%250A%2520%2520Alignment%26entry.906535625%3DQi%2520Yu%2520and%2520Zhichen%2520Zeng%2520and%2520Yuchen%2520Yan%2520and%2520Zhining%2520Liu%2520and%2520Baoyu%2520Jing%2520and%2520Ruizhong%2520Qiu%2520and%2520Ariful%2520Azad%2520and%2520Hanghang%2520Tong%26entry.1292438233%3D%2520%2520Network%2520alignment%2520%2528NA%2529%2520aims%2520to%2520identify%2520node%2520correspondence%2520across%2520different%250Anetworks%2520and%2520serves%2520as%2520a%2520critical%2520cornerstone%2520behind%2520various%2520downstream%250Amulti-network%2520learning%2520tasks.%2520Despite%2520growing%2520research%2520in%2520NA%252C%2520there%2520lacks%2520a%250Acomprehensive%2520library%2520that%2520facilitates%2520the%2520systematic%2520development%2520and%250Abenchmarking%2520of%2520NA%2520methods.%2520In%2520this%2520work%252C%2520we%2520introduce%2520PLANETALIGN%252C%2520a%250Acomprehensive%2520Python%2520library%2520for%2520network%2520alignment%2520that%2520features%2520a%2520rich%250Acollection%2520of%2520built-in%2520datasets%252C%2520methods%252C%2520and%2520evaluation%2520pipelines%2520with%250Aeasy-to-use%2520APIs.%2520Specifically%252C%2520PLANETALIGN%2520integrates%252018%2520datasets%2520and%252014%2520NA%250Amethods%2520with%2520extensible%2520APIs%2520for%2520easy%2520use%2520and%2520development%2520of%2520NA%2520methods.%2520Our%250Astandardized%2520evaluation%2520pipeline%2520encompasses%2520a%2520wide%2520range%2520of%2520metrics%252C%2520enabling%250Aa%2520systematic%2520assessment%2520of%2520the%2520effectiveness%252C%2520scalability%252C%2520and%2520robustness%2520of%2520NA%250Amethods.%2520Through%2520extensive%2520comparative%2520studies%252C%2520we%2520reveal%2520practical%2520insights%250Ainto%2520the%2520strengths%2520and%2520limitations%2520of%2520existing%2520NA%2520methods.%2520We%2520hope%2520that%250APLANETALIGN%2520can%2520foster%2520a%2520deeper%2520understanding%2520of%2520the%2520NA%2520problem%2520and%2520facilitate%250Athe%2520development%2520and%2520benchmarking%2520of%2520more%2520effective%252C%2520scalable%252C%2520and%2520robust%250Amethods%2520in%2520the%2520future.%2520The%2520source%2520code%2520of%2520PLANETALIGN%2520is%2520available%2520at%250Ahttps%253A//github.com/yq-leo/PlanetAlign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLANETALIGN%3A%20A%20Comprehensive%20Python%20Library%20for%20Benchmarking%20Network%0A%20%20Alignment&entry.906535625=Qi%20Yu%20and%20Zhichen%20Zeng%20and%20Yuchen%20Yan%20and%20Zhining%20Liu%20and%20Baoyu%20Jing%20and%20Ruizhong%20Qiu%20and%20Ariful%20Azad%20and%20Hanghang%20Tong&entry.1292438233=%20%20Network%20alignment%20%28NA%29%20aims%20to%20identify%20node%20correspondence%20across%20different%0Anetworks%20and%20serves%20as%20a%20critical%20cornerstone%20behind%20various%20downstream%0Amulti-network%20learning%20tasks.%20Despite%20growing%20research%20in%20NA%2C%20there%20lacks%20a%0Acomprehensive%20library%20that%20facilitates%20the%20systematic%20development%20and%0Abenchmarking%20of%20NA%20methods.%20In%20this%20work%2C%20we%20introduce%20PLANETALIGN%2C%20a%0Acomprehensive%20Python%20library%20for%20network%20alignment%20that%20features%20a%20rich%0Acollection%20of%20built-in%20datasets%2C%20methods%2C%20and%20evaluation%20pipelines%20with%0Aeasy-to-use%20APIs.%20Specifically%2C%20PLANETALIGN%20integrates%2018%20datasets%20and%2014%20NA%0Amethods%20with%20extensible%20APIs%20for%20easy%20use%20and%20development%20of%20NA%20methods.%20Our%0Astandardized%20evaluation%20pipeline%20encompasses%20a%20wide%20range%20of%20metrics%2C%20enabling%0Aa%20systematic%20assessment%20of%20the%20effectiveness%2C%20scalability%2C%20and%20robustness%20of%20NA%0Amethods.%20Through%20extensive%20comparative%20studies%2C%20we%20reveal%20practical%20insights%0Ainto%20the%20strengths%20and%20limitations%20of%20existing%20NA%20methods.%20We%20hope%20that%0APLANETALIGN%20can%20foster%20a%20deeper%20understanding%20of%20the%20NA%20problem%20and%20facilitate%0Athe%20development%20and%20benchmarking%20of%20more%20effective%2C%20scalable%2C%20and%20robust%0Amethods%20in%20the%20future.%20The%20source%20code%20of%20PLANETALIGN%20is%20available%20at%0Ahttps%3A//github.com/yq-leo/PlanetAlign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21366v1&entry.124074799=Read"},
{"title": "CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for\n  Medical Image Segmentation", "author": "Jiong Wu and Yang Xing and Boxiao Yu and Wei Shao and Kuang Gong", "abstract": "  Most publicly available medical segmentation datasets are only partially\nlabeled, with annotations provided for a subset of anatomical structures. When\nmultiple datasets are combined for training, this incomplete annotation poses\nchallenges, as it limits the model's ability to learn shared anatomical\nrepresentations among datasets. Furthermore, vision-only frameworks often fail\nto capture complex anatomical relationships and task-specific distinctions,\nleading to reduced segmentation accuracy and poor generalizability to unseen\ndatasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven\nSegmentation Network (CDPDNet), which combined a self-supervised vision\ntransformer with CLIP-based text embedding and introduced task-specific text\nprompts to tackle these challenges. Specifically, the framework was constructed\nupon a convolutional neural network (CNN) and incorporated DINOv2 to extract\nboth fine-grained and global visual features, which were then fused using a\nmulti-head cross-attention module to overcome the limited long-range modeling\ncapability of CNNs. In addition, CLIP-derived text embeddings were projected\ninto the visual space to help model complex relationships among organs and\ntumors. To further address the partial label challenge and enhance inter-task\ndiscriminative capability, a Text-based Task Prompt Generation (TTPG) module\nthat generated task-specific prompts was designed to guide the segmentation.\nExtensive experiments on multiple medical imaging datasets demonstrated that\nCDPDNet consistently outperformed existing state-of-the-art segmentation\nmethods. Code and pretrained model are available at:\nhttps://github.com/wujiong-hub/CDPDNet.git.\n", "link": "http://arxiv.org/abs/2505.18958v2", "date": "2025-05-27", "relevancy": 2.2952, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.585}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5701}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDPDNet%3A%20Integrating%20Text%20Guidance%20with%20Hybrid%20Vision%20Encoders%20for%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20CDPDNet%3A%20Integrating%20Text%20Guidance%20with%20Hybrid%20Vision%20Encoders%20for%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Jiong%20Wu%20and%20Yang%20Xing%20and%20Boxiao%20Yu%20and%20Wei%20Shao%20and%20Kuang%20Gong%0AAbstract%3A%20%20%20Most%20publicly%20available%20medical%20segmentation%20datasets%20are%20only%20partially%0Alabeled%2C%20with%20annotations%20provided%20for%20a%20subset%20of%20anatomical%20structures.%20When%0Amultiple%20datasets%20are%20combined%20for%20training%2C%20this%20incomplete%20annotation%20poses%0Achallenges%2C%20as%20it%20limits%20the%20model%27s%20ability%20to%20learn%20shared%20anatomical%0Arepresentations%20among%20datasets.%20Furthermore%2C%20vision-only%20frameworks%20often%20fail%0Ato%20capture%20complex%20anatomical%20relationships%20and%20task-specific%20distinctions%2C%0Aleading%20to%20reduced%20segmentation%20accuracy%20and%20poor%20generalizability%20to%20unseen%0Adatasets.%20In%20this%20study%2C%20we%20proposed%20a%20novel%20CLIP-DINO%20Prompt-Driven%0ASegmentation%20Network%20%28CDPDNet%29%2C%20which%20combined%20a%20self-supervised%20vision%0Atransformer%20with%20CLIP-based%20text%20embedding%20and%20introduced%20task-specific%20text%0Aprompts%20to%20tackle%20these%20challenges.%20Specifically%2C%20the%20framework%20was%20constructed%0Aupon%20a%20convolutional%20neural%20network%20%28CNN%29%20and%20incorporated%20DINOv2%20to%20extract%0Aboth%20fine-grained%20and%20global%20visual%20features%2C%20which%20were%20then%20fused%20using%20a%0Amulti-head%20cross-attention%20module%20to%20overcome%20the%20limited%20long-range%20modeling%0Acapability%20of%20CNNs.%20In%20addition%2C%20CLIP-derived%20text%20embeddings%20were%20projected%0Ainto%20the%20visual%20space%20to%20help%20model%20complex%20relationships%20among%20organs%20and%0Atumors.%20To%20further%20address%20the%20partial%20label%20challenge%20and%20enhance%20inter-task%0Adiscriminative%20capability%2C%20a%20Text-based%20Task%20Prompt%20Generation%20%28TTPG%29%20module%0Athat%20generated%20task-specific%20prompts%20was%20designed%20to%20guide%20the%20segmentation.%0AExtensive%20experiments%20on%20multiple%20medical%20imaging%20datasets%20demonstrated%20that%0ACDPDNet%20consistently%20outperformed%20existing%20state-of-the-art%20segmentation%0Amethods.%20Code%20and%20pretrained%20model%20are%20available%20at%3A%0Ahttps%3A//github.com/wujiong-hub/CDPDNet.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDPDNet%253A%2520Integrating%2520Text%2520Guidance%2520with%2520Hybrid%2520Vision%2520Encoders%2520for%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJiong%2520Wu%2520and%2520Yang%2520Xing%2520and%2520Boxiao%2520Yu%2520and%2520Wei%2520Shao%2520and%2520Kuang%2520Gong%26entry.1292438233%3D%2520%2520Most%2520publicly%2520available%2520medical%2520segmentation%2520datasets%2520are%2520only%2520partially%250Alabeled%252C%2520with%2520annotations%2520provided%2520for%2520a%2520subset%2520of%2520anatomical%2520structures.%2520When%250Amultiple%2520datasets%2520are%2520combined%2520for%2520training%252C%2520this%2520incomplete%2520annotation%2520poses%250Achallenges%252C%2520as%2520it%2520limits%2520the%2520model%2527s%2520ability%2520to%2520learn%2520shared%2520anatomical%250Arepresentations%2520among%2520datasets.%2520Furthermore%252C%2520vision-only%2520frameworks%2520often%2520fail%250Ato%2520capture%2520complex%2520anatomical%2520relationships%2520and%2520task-specific%2520distinctions%252C%250Aleading%2520to%2520reduced%2520segmentation%2520accuracy%2520and%2520poor%2520generalizability%2520to%2520unseen%250Adatasets.%2520In%2520this%2520study%252C%2520we%2520proposed%2520a%2520novel%2520CLIP-DINO%2520Prompt-Driven%250ASegmentation%2520Network%2520%2528CDPDNet%2529%252C%2520which%2520combined%2520a%2520self-supervised%2520vision%250Atransformer%2520with%2520CLIP-based%2520text%2520embedding%2520and%2520introduced%2520task-specific%2520text%250Aprompts%2520to%2520tackle%2520these%2520challenges.%2520Specifically%252C%2520the%2520framework%2520was%2520constructed%250Aupon%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520and%2520incorporated%2520DINOv2%2520to%2520extract%250Aboth%2520fine-grained%2520and%2520global%2520visual%2520features%252C%2520which%2520were%2520then%2520fused%2520using%2520a%250Amulti-head%2520cross-attention%2520module%2520to%2520overcome%2520the%2520limited%2520long-range%2520modeling%250Acapability%2520of%2520CNNs.%2520In%2520addition%252C%2520CLIP-derived%2520text%2520embeddings%2520were%2520projected%250Ainto%2520the%2520visual%2520space%2520to%2520help%2520model%2520complex%2520relationships%2520among%2520organs%2520and%250Atumors.%2520To%2520further%2520address%2520the%2520partial%2520label%2520challenge%2520and%2520enhance%2520inter-task%250Adiscriminative%2520capability%252C%2520a%2520Text-based%2520Task%2520Prompt%2520Generation%2520%2528TTPG%2529%2520module%250Athat%2520generated%2520task-specific%2520prompts%2520was%2520designed%2520to%2520guide%2520the%2520segmentation.%250AExtensive%2520experiments%2520on%2520multiple%2520medical%2520imaging%2520datasets%2520demonstrated%2520that%250ACDPDNet%2520consistently%2520outperformed%2520existing%2520state-of-the-art%2520segmentation%250Amethods.%2520Code%2520and%2520pretrained%2520model%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/wujiong-hub/CDPDNet.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDPDNet%3A%20Integrating%20Text%20Guidance%20with%20Hybrid%20Vision%20Encoders%20for%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Jiong%20Wu%20and%20Yang%20Xing%20and%20Boxiao%20Yu%20and%20Wei%20Shao%20and%20Kuang%20Gong&entry.1292438233=%20%20Most%20publicly%20available%20medical%20segmentation%20datasets%20are%20only%20partially%0Alabeled%2C%20with%20annotations%20provided%20for%20a%20subset%20of%20anatomical%20structures.%20When%0Amultiple%20datasets%20are%20combined%20for%20training%2C%20this%20incomplete%20annotation%20poses%0Achallenges%2C%20as%20it%20limits%20the%20model%27s%20ability%20to%20learn%20shared%20anatomical%0Arepresentations%20among%20datasets.%20Furthermore%2C%20vision-only%20frameworks%20often%20fail%0Ato%20capture%20complex%20anatomical%20relationships%20and%20task-specific%20distinctions%2C%0Aleading%20to%20reduced%20segmentation%20accuracy%20and%20poor%20generalizability%20to%20unseen%0Adatasets.%20In%20this%20study%2C%20we%20proposed%20a%20novel%20CLIP-DINO%20Prompt-Driven%0ASegmentation%20Network%20%28CDPDNet%29%2C%20which%20combined%20a%20self-supervised%20vision%0Atransformer%20with%20CLIP-based%20text%20embedding%20and%20introduced%20task-specific%20text%0Aprompts%20to%20tackle%20these%20challenges.%20Specifically%2C%20the%20framework%20was%20constructed%0Aupon%20a%20convolutional%20neural%20network%20%28CNN%29%20and%20incorporated%20DINOv2%20to%20extract%0Aboth%20fine-grained%20and%20global%20visual%20features%2C%20which%20were%20then%20fused%20using%20a%0Amulti-head%20cross-attention%20module%20to%20overcome%20the%20limited%20long-range%20modeling%0Acapability%20of%20CNNs.%20In%20addition%2C%20CLIP-derived%20text%20embeddings%20were%20projected%0Ainto%20the%20visual%20space%20to%20help%20model%20complex%20relationships%20among%20organs%20and%0Atumors.%20To%20further%20address%20the%20partial%20label%20challenge%20and%20enhance%20inter-task%0Adiscriminative%20capability%2C%20a%20Text-based%20Task%20Prompt%20Generation%20%28TTPG%29%20module%0Athat%20generated%20task-specific%20prompts%20was%20designed%20to%20guide%20the%20segmentation.%0AExtensive%20experiments%20on%20multiple%20medical%20imaging%20datasets%20demonstrated%20that%0ACDPDNet%20consistently%20outperformed%20existing%20state-of-the-art%20segmentation%0Amethods.%20Code%20and%20pretrained%20model%20are%20available%20at%3A%0Ahttps%3A//github.com/wujiong-hub/CDPDNet.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18958v2&entry.124074799=Read"},
{"title": "Cognitive Disentanglement for Referring Multi-Object Tracking", "author": "Shaofeng Liang and Runwei Guan and Wangwang Lian and Daizong Liu and Xiaolou Sun and Dongming Wu and Yutao Yue and Weiping Ding and Hui Xiong", "abstract": "  As a significant application of multi-source information fusion in\nintelligent transportation perception systems, Referring Multi-Object Tracking\n(RMOT) involves localizing and tracking specific objects in video sequences\nbased on language references. However, existing RMOT approaches often treat\nlanguage descriptions as holistic embeddings and struggle to effectively\nintegrate the rich semantic information contained in language expressions with\nvisual features. This limitation is especially apparent in complex scenes\nrequiring comprehensive understanding of both static object attributes and\nspatial motion information. In this paper, we propose a Cognitive\nDisentanglement for Referring Multi-Object Tracking (CDRMT) framework that\naddresses these challenges. It adapts the \"what\" and \"where\" pathways from the\nhuman visual processing system to RMOT tasks. Specifically, our framework first\nestablishes cross-modal connections while preserving modality-specific\ncharacteristics. It then disentangles language descriptions and hierarchically\ninjects them into object queries, refining object understanding from coarse to\nfine-grained semantic levels. Finally, we reconstruct language representations\nbased on visual features, ensuring that tracked objects faithfully reflect the\nreferring expression. Extensive experiments on different benchmark datasets\ndemonstrate that CDRMT achieves substantial improvements over state-of-the-art\nmethods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on\nRefer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while\nsimultaneously providing new insights into multi-source information fusion.\n", "link": "http://arxiv.org/abs/2503.11496v4", "date": "2025-05-27", "relevancy": 2.2867, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5833}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Disentanglement%20for%20Referring%20Multi-Object%20Tracking&body=Title%3A%20Cognitive%20Disentanglement%20for%20Referring%20Multi-Object%20Tracking%0AAuthor%3A%20Shaofeng%20Liang%20and%20Runwei%20Guan%20and%20Wangwang%20Lian%20and%20Daizong%20Liu%20and%20Xiaolou%20Sun%20and%20Dongming%20Wu%20and%20Yutao%20Yue%20and%20Weiping%20Ding%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20As%20a%20significant%20application%20of%20multi-source%20information%20fusion%20in%0Aintelligent%20transportation%20perception%20systems%2C%20Referring%20Multi-Object%20Tracking%0A%28RMOT%29%20involves%20localizing%20and%20tracking%20specific%20objects%20in%20video%20sequences%0Abased%20on%20language%20references.%20However%2C%20existing%20RMOT%20approaches%20often%20treat%0Alanguage%20descriptions%20as%20holistic%20embeddings%20and%20struggle%20to%20effectively%0Aintegrate%20the%20rich%20semantic%20information%20contained%20in%20language%20expressions%20with%0Avisual%20features.%20This%20limitation%20is%20especially%20apparent%20in%20complex%20scenes%0Arequiring%20comprehensive%20understanding%20of%20both%20static%20object%20attributes%20and%0Aspatial%20motion%20information.%20In%20this%20paper%2C%20we%20propose%20a%20Cognitive%0ADisentanglement%20for%20Referring%20Multi-Object%20Tracking%20%28CDRMT%29%20framework%20that%0Aaddresses%20these%20challenges.%20It%20adapts%20the%20%22what%22%20and%20%22where%22%20pathways%20from%20the%0Ahuman%20visual%20processing%20system%20to%20RMOT%20tasks.%20Specifically%2C%20our%20framework%20first%0Aestablishes%20cross-modal%20connections%20while%20preserving%20modality-specific%0Acharacteristics.%20It%20then%20disentangles%20language%20descriptions%20and%20hierarchically%0Ainjects%20them%20into%20object%20queries%2C%20refining%20object%20understanding%20from%20coarse%20to%0Afine-grained%20semantic%20levels.%20Finally%2C%20we%20reconstruct%20language%20representations%0Abased%20on%20visual%20features%2C%20ensuring%20that%20tracked%20objects%20faithfully%20reflect%20the%0Areferring%20expression.%20Extensive%20experiments%20on%20different%20benchmark%20datasets%0Ademonstrate%20that%20CDRMT%20achieves%20substantial%20improvements%20over%20state-of-the-art%0Amethods%2C%20with%20average%20gains%20of%206.0%25%20in%20HOTA%20score%20on%20Refer-KITTI%20and%203.2%25%20on%0ARefer-KITTI-V2.%20Our%20approach%20advances%20the%20state-of-the-art%20in%20RMOT%20while%0Asimultaneously%20providing%20new%20insights%20into%20multi-source%20information%20fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11496v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Disentanglement%2520for%2520Referring%2520Multi-Object%2520Tracking%26entry.906535625%3DShaofeng%2520Liang%2520and%2520Runwei%2520Guan%2520and%2520Wangwang%2520Lian%2520and%2520Daizong%2520Liu%2520and%2520Xiaolou%2520Sun%2520and%2520Dongming%2520Wu%2520and%2520Yutao%2520Yue%2520and%2520Weiping%2520Ding%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520As%2520a%2520significant%2520application%2520of%2520multi-source%2520information%2520fusion%2520in%250Aintelligent%2520transportation%2520perception%2520systems%252C%2520Referring%2520Multi-Object%2520Tracking%250A%2528RMOT%2529%2520involves%2520localizing%2520and%2520tracking%2520specific%2520objects%2520in%2520video%2520sequences%250Abased%2520on%2520language%2520references.%2520However%252C%2520existing%2520RMOT%2520approaches%2520often%2520treat%250Alanguage%2520descriptions%2520as%2520holistic%2520embeddings%2520and%2520struggle%2520to%2520effectively%250Aintegrate%2520the%2520rich%2520semantic%2520information%2520contained%2520in%2520language%2520expressions%2520with%250Avisual%2520features.%2520This%2520limitation%2520is%2520especially%2520apparent%2520in%2520complex%2520scenes%250Arequiring%2520comprehensive%2520understanding%2520of%2520both%2520static%2520object%2520attributes%2520and%250Aspatial%2520motion%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Cognitive%250ADisentanglement%2520for%2520Referring%2520Multi-Object%2520Tracking%2520%2528CDRMT%2529%2520framework%2520that%250Aaddresses%2520these%2520challenges.%2520It%2520adapts%2520the%2520%2522what%2522%2520and%2520%2522where%2522%2520pathways%2520from%2520the%250Ahuman%2520visual%2520processing%2520system%2520to%2520RMOT%2520tasks.%2520Specifically%252C%2520our%2520framework%2520first%250Aestablishes%2520cross-modal%2520connections%2520while%2520preserving%2520modality-specific%250Acharacteristics.%2520It%2520then%2520disentangles%2520language%2520descriptions%2520and%2520hierarchically%250Ainjects%2520them%2520into%2520object%2520queries%252C%2520refining%2520object%2520understanding%2520from%2520coarse%2520to%250Afine-grained%2520semantic%2520levels.%2520Finally%252C%2520we%2520reconstruct%2520language%2520representations%250Abased%2520on%2520visual%2520features%252C%2520ensuring%2520that%2520tracked%2520objects%2520faithfully%2520reflect%2520the%250Areferring%2520expression.%2520Extensive%2520experiments%2520on%2520different%2520benchmark%2520datasets%250Ademonstrate%2520that%2520CDRMT%2520achieves%2520substantial%2520improvements%2520over%2520state-of-the-art%250Amethods%252C%2520with%2520average%2520gains%2520of%25206.0%2525%2520in%2520HOTA%2520score%2520on%2520Refer-KITTI%2520and%25203.2%2525%2520on%250ARefer-KITTI-V2.%2520Our%2520approach%2520advances%2520the%2520state-of-the-art%2520in%2520RMOT%2520while%250Asimultaneously%2520providing%2520new%2520insights%2520into%2520multi-source%2520information%2520fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11496v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Disentanglement%20for%20Referring%20Multi-Object%20Tracking&entry.906535625=Shaofeng%20Liang%20and%20Runwei%20Guan%20and%20Wangwang%20Lian%20and%20Daizong%20Liu%20and%20Xiaolou%20Sun%20and%20Dongming%20Wu%20and%20Yutao%20Yue%20and%20Weiping%20Ding%20and%20Hui%20Xiong&entry.1292438233=%20%20As%20a%20significant%20application%20of%20multi-source%20information%20fusion%20in%0Aintelligent%20transportation%20perception%20systems%2C%20Referring%20Multi-Object%20Tracking%0A%28RMOT%29%20involves%20localizing%20and%20tracking%20specific%20objects%20in%20video%20sequences%0Abased%20on%20language%20references.%20However%2C%20existing%20RMOT%20approaches%20often%20treat%0Alanguage%20descriptions%20as%20holistic%20embeddings%20and%20struggle%20to%20effectively%0Aintegrate%20the%20rich%20semantic%20information%20contained%20in%20language%20expressions%20with%0Avisual%20features.%20This%20limitation%20is%20especially%20apparent%20in%20complex%20scenes%0Arequiring%20comprehensive%20understanding%20of%20both%20static%20object%20attributes%20and%0Aspatial%20motion%20information.%20In%20this%20paper%2C%20we%20propose%20a%20Cognitive%0ADisentanglement%20for%20Referring%20Multi-Object%20Tracking%20%28CDRMT%29%20framework%20that%0Aaddresses%20these%20challenges.%20It%20adapts%20the%20%22what%22%20and%20%22where%22%20pathways%20from%20the%0Ahuman%20visual%20processing%20system%20to%20RMOT%20tasks.%20Specifically%2C%20our%20framework%20first%0Aestablishes%20cross-modal%20connections%20while%20preserving%20modality-specific%0Acharacteristics.%20It%20then%20disentangles%20language%20descriptions%20and%20hierarchically%0Ainjects%20them%20into%20object%20queries%2C%20refining%20object%20understanding%20from%20coarse%20to%0Afine-grained%20semantic%20levels.%20Finally%2C%20we%20reconstruct%20language%20representations%0Abased%20on%20visual%20features%2C%20ensuring%20that%20tracked%20objects%20faithfully%20reflect%20the%0Areferring%20expression.%20Extensive%20experiments%20on%20different%20benchmark%20datasets%0Ademonstrate%20that%20CDRMT%20achieves%20substantial%20improvements%20over%20state-of-the-art%0Amethods%2C%20with%20average%20gains%20of%206.0%25%20in%20HOTA%20score%20on%20Refer-KITTI%20and%203.2%25%20on%0ARefer-KITTI-V2.%20Our%20approach%20advances%20the%20state-of-the-art%20in%20RMOT%20while%0Asimultaneously%20providing%20new%20insights%20into%20multi-source%20information%20fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11496v4&entry.124074799=Read"},
{"title": "Policy Optimized Text-to-Image Pipeline Design", "author": "Uri Gadot and Rinon Gal and Yftah Ziser and Gal Chechik and Shie Mannor", "abstract": "  Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines.\n", "link": "http://arxiv.org/abs/2505.21478v1", "date": "2025-05-27", "relevancy": 2.2732, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6374}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5693}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Policy%20Optimized%20Text-to-Image%20Pipeline%20Design&body=Title%3A%20Policy%20Optimized%20Text-to-Image%20Pipeline%20Design%0AAuthor%3A%20Uri%20Gadot%20and%20Rinon%20Gal%20and%20Yftah%20Ziser%20and%20Gal%20Chechik%20and%20Shie%20Mannor%0AAbstract%3A%20%20%20Text-to-image%20generation%20has%20evolved%20beyond%20single%20monolithic%20models%20to%0Acomplex%20multi-component%20pipelines.%20These%20combine%20fine-tuned%20generators%2C%0Aadapters%2C%20upscaling%20blocks%20and%20even%20editing%20steps%2C%20leading%20to%20significant%0Aimprovements%20in%20image%20quality.%20However%2C%20their%20effective%20design%20requires%0Asubstantial%20expertise.%20Recent%20approaches%20have%20shown%20promise%20in%20automating%20this%0Aprocess%20through%20large%20language%20models%20%28LLMs%29%2C%20but%20they%20suffer%20from%20two%20critical%0Alimitations%3A%20extensive%20computational%20requirements%20from%20generating%20images%20with%0Ahundreds%20of%20predefined%20pipelines%2C%20and%20poor%20generalization%20beyond%20memorized%0Atraining%20examples.%20We%20introduce%20a%20novel%20reinforcement%20learning-based%20framework%0Athat%20addresses%20these%20inefficiencies.%20Our%20approach%20first%20trains%20an%20ensemble%20of%0Areward%20models%20capable%20of%20predicting%20image%20quality%20scores%20directly%20from%0Aprompt-workflow%20combinations%2C%20eliminating%20the%20need%20for%20costly%20image%20generation%0Aduring%20training.%20We%20then%20implement%20a%20two-phase%20training%20strategy%3A%20initial%0Aworkflow%20vocabulary%20training%20followed%20by%20GRPO-based%20optimization%20that%20guides%0Athe%20model%20toward%20higher-performing%20regions%20of%20the%20workflow%20space.%20Additionally%2C%0Awe%20incorporate%20a%20classifier-free%20guidance%20based%20enhancement%20technique%20that%0Aextrapolates%20along%20the%20path%20between%20the%20initial%20and%20GRPO-tuned%20models%2C%20further%0Aimproving%20output%20quality.%20We%20validate%20our%20approach%20through%20a%20set%20of%0Acomparisons%2C%20showing%20that%20it%20can%20successfully%20create%20new%20flows%20with%20greater%0Adiversity%20and%20lead%20to%20superior%20image%20quality%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolicy%2520Optimized%2520Text-to-Image%2520Pipeline%2520Design%26entry.906535625%3DUri%2520Gadot%2520and%2520Rinon%2520Gal%2520and%2520Yftah%2520Ziser%2520and%2520Gal%2520Chechik%2520and%2520Shie%2520Mannor%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520has%2520evolved%2520beyond%2520single%2520monolithic%2520models%2520to%250Acomplex%2520multi-component%2520pipelines.%2520These%2520combine%2520fine-tuned%2520generators%252C%250Aadapters%252C%2520upscaling%2520blocks%2520and%2520even%2520editing%2520steps%252C%2520leading%2520to%2520significant%250Aimprovements%2520in%2520image%2520quality.%2520However%252C%2520their%2520effective%2520design%2520requires%250Asubstantial%2520expertise.%2520Recent%2520approaches%2520have%2520shown%2520promise%2520in%2520automating%2520this%250Aprocess%2520through%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520but%2520they%2520suffer%2520from%2520two%2520critical%250Alimitations%253A%2520extensive%2520computational%2520requirements%2520from%2520generating%2520images%2520with%250Ahundreds%2520of%2520predefined%2520pipelines%252C%2520and%2520poor%2520generalization%2520beyond%2520memorized%250Atraining%2520examples.%2520We%2520introduce%2520a%2520novel%2520reinforcement%2520learning-based%2520framework%250Athat%2520addresses%2520these%2520inefficiencies.%2520Our%2520approach%2520first%2520trains%2520an%2520ensemble%2520of%250Areward%2520models%2520capable%2520of%2520predicting%2520image%2520quality%2520scores%2520directly%2520from%250Aprompt-workflow%2520combinations%252C%2520eliminating%2520the%2520need%2520for%2520costly%2520image%2520generation%250Aduring%2520training.%2520We%2520then%2520implement%2520a%2520two-phase%2520training%2520strategy%253A%2520initial%250Aworkflow%2520vocabulary%2520training%2520followed%2520by%2520GRPO-based%2520optimization%2520that%2520guides%250Athe%2520model%2520toward%2520higher-performing%2520regions%2520of%2520the%2520workflow%2520space.%2520Additionally%252C%250Awe%2520incorporate%2520a%2520classifier-free%2520guidance%2520based%2520enhancement%2520technique%2520that%250Aextrapolates%2520along%2520the%2520path%2520between%2520the%2520initial%2520and%2520GRPO-tuned%2520models%252C%2520further%250Aimproving%2520output%2520quality.%2520We%2520validate%2520our%2520approach%2520through%2520a%2520set%2520of%250Acomparisons%252C%2520showing%2520that%2520it%2520can%2520successfully%2520create%2520new%2520flows%2520with%2520greater%250Adiversity%2520and%2520lead%2520to%2520superior%2520image%2520quality%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy%20Optimized%20Text-to-Image%20Pipeline%20Design&entry.906535625=Uri%20Gadot%20and%20Rinon%20Gal%20and%20Yftah%20Ziser%20and%20Gal%20Chechik%20and%20Shie%20Mannor&entry.1292438233=%20%20Text-to-image%20generation%20has%20evolved%20beyond%20single%20monolithic%20models%20to%0Acomplex%20multi-component%20pipelines.%20These%20combine%20fine-tuned%20generators%2C%0Aadapters%2C%20upscaling%20blocks%20and%20even%20editing%20steps%2C%20leading%20to%20significant%0Aimprovements%20in%20image%20quality.%20However%2C%20their%20effective%20design%20requires%0Asubstantial%20expertise.%20Recent%20approaches%20have%20shown%20promise%20in%20automating%20this%0Aprocess%20through%20large%20language%20models%20%28LLMs%29%2C%20but%20they%20suffer%20from%20two%20critical%0Alimitations%3A%20extensive%20computational%20requirements%20from%20generating%20images%20with%0Ahundreds%20of%20predefined%20pipelines%2C%20and%20poor%20generalization%20beyond%20memorized%0Atraining%20examples.%20We%20introduce%20a%20novel%20reinforcement%20learning-based%20framework%0Athat%20addresses%20these%20inefficiencies.%20Our%20approach%20first%20trains%20an%20ensemble%20of%0Areward%20models%20capable%20of%20predicting%20image%20quality%20scores%20directly%20from%0Aprompt-workflow%20combinations%2C%20eliminating%20the%20need%20for%20costly%20image%20generation%0Aduring%20training.%20We%20then%20implement%20a%20two-phase%20training%20strategy%3A%20initial%0Aworkflow%20vocabulary%20training%20followed%20by%20GRPO-based%20optimization%20that%20guides%0Athe%20model%20toward%20higher-performing%20regions%20of%20the%20workflow%20space.%20Additionally%2C%0Awe%20incorporate%20a%20classifier-free%20guidance%20based%20enhancement%20technique%20that%0Aextrapolates%20along%20the%20path%20between%20the%20initial%20and%20GRPO-tuned%20models%2C%20further%0Aimproving%20output%20quality.%20We%20validate%20our%20approach%20through%20a%20set%20of%0Acomparisons%2C%20showing%20that%20it%20can%20successfully%20create%20new%20flows%20with%20greater%0Adiversity%20and%20lead%20to%20superior%20image%20quality%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21478v1&entry.124074799=Read"},
{"title": "OmniSync: Towards Universal Lip Synchronization via Diffusion\n  Transformers", "author": "Ziqiao Peng and Jiwen Liu and Haoxian Zhang and Xiaoqiang Liu and Songlin Tang and Pengfei Wan and Di Zhang and Hongyan Liu and Jun He", "abstract": "  Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.\n", "link": "http://arxiv.org/abs/2505.21448v1", "date": "2025-05-27", "relevancy": 2.2722, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5865}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5674}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniSync%3A%20Towards%20Universal%20Lip%20Synchronization%20via%20Diffusion%0A%20%20Transformers&body=Title%3A%20OmniSync%3A%20Towards%20Universal%20Lip%20Synchronization%20via%20Diffusion%0A%20%20Transformers%0AAuthor%3A%20Ziqiao%20Peng%20and%20Jiwen%20Liu%20and%20Haoxian%20Zhang%20and%20Xiaoqiang%20Liu%20and%20Songlin%20Tang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Hongyan%20Liu%20and%20Jun%20He%0AAbstract%3A%20%20%20Lip%20synchronization%20is%20the%20task%20of%20aligning%20a%20speaker%27s%20lip%20movements%20in%0Avideo%20with%20corresponding%20speech%20audio%2C%20and%20it%20is%20essential%20for%20creating%0Arealistic%2C%20expressive%20video%20content.%20However%2C%20existing%20methods%20often%20rely%20on%0Areference%20frames%20and%20masked-frame%20inpainting%2C%20which%20limit%20their%20robustness%20to%0Aidentity%20consistency%2C%20pose%20variations%2C%20facial%20occlusions%2C%20and%20stylized%20content.%0AIn%20addition%2C%20since%20audio%20signals%20provide%20weaker%20conditioning%20than%20visual%20cues%2C%0Alip%20shape%20leakage%20from%20the%20original%20video%20will%20affect%20lip%20sync%20quality.%20In%20this%0Apaper%2C%20we%20present%20OmniSync%2C%20a%20universal%20lip%20synchronization%20framework%20for%0Adiverse%20visual%20scenarios.%20Our%20approach%20introduces%20a%20mask-free%20training%20paradigm%0Ausing%20Diffusion%20Transformer%20models%20for%20direct%20frame%20editing%20without%20explicit%0Amasks%2C%20enabling%20unlimited-duration%20inference%20while%20maintaining%20natural%20facial%0Adynamics%20and%20preserving%20character%20identity.%20During%20inference%2C%20we%20propose%20a%0Aflow-matching-based%20progressive%20noise%20initialization%20to%20ensure%20pose%20and%0Aidentity%20consistency%2C%20while%20allowing%20precise%20mouth-region%20editing.%20To%20address%0Athe%20weak%20conditioning%20signal%20of%20audio%2C%20we%20develop%20a%20Dynamic%20Spatiotemporal%0AClassifier-Free%20Guidance%20%28DS-CFG%29%20mechanism%20that%20adaptively%20adjusts%20guidance%0Astrength%20over%20time%20and%20space.%20We%20also%20establish%20the%20AIGC-LipSync%20Benchmark%2C%20the%0Afirst%20evaluation%20suite%20for%20lip%20synchronization%20in%20diverse%20AI-generated%20videos.%0AExtensive%20experiments%20demonstrate%20that%20OmniSync%20significantly%20outperforms%20prior%0Amethods%20in%20both%20visual%20quality%20and%20lip%20sync%20accuracy%2C%20achieving%20superior%0Aresults%20in%20both%20real-world%20and%20AI-generated%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniSync%253A%2520Towards%2520Universal%2520Lip%2520Synchronization%2520via%2520Diffusion%250A%2520%2520Transformers%26entry.906535625%3DZiqiao%2520Peng%2520and%2520Jiwen%2520Liu%2520and%2520Haoxian%2520Zhang%2520and%2520Xiaoqiang%2520Liu%2520and%2520Songlin%2520Tang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Hongyan%2520Liu%2520and%2520Jun%2520He%26entry.1292438233%3D%2520%2520Lip%2520synchronization%2520is%2520the%2520task%2520of%2520aligning%2520a%2520speaker%2527s%2520lip%2520movements%2520in%250Avideo%2520with%2520corresponding%2520speech%2520audio%252C%2520and%2520it%2520is%2520essential%2520for%2520creating%250Arealistic%252C%2520expressive%2520video%2520content.%2520However%252C%2520existing%2520methods%2520often%2520rely%2520on%250Areference%2520frames%2520and%2520masked-frame%2520inpainting%252C%2520which%2520limit%2520their%2520robustness%2520to%250Aidentity%2520consistency%252C%2520pose%2520variations%252C%2520facial%2520occlusions%252C%2520and%2520stylized%2520content.%250AIn%2520addition%252C%2520since%2520audio%2520signals%2520provide%2520weaker%2520conditioning%2520than%2520visual%2520cues%252C%250Alip%2520shape%2520leakage%2520from%2520the%2520original%2520video%2520will%2520affect%2520lip%2520sync%2520quality.%2520In%2520this%250Apaper%252C%2520we%2520present%2520OmniSync%252C%2520a%2520universal%2520lip%2520synchronization%2520framework%2520for%250Adiverse%2520visual%2520scenarios.%2520Our%2520approach%2520introduces%2520a%2520mask-free%2520training%2520paradigm%250Ausing%2520Diffusion%2520Transformer%2520models%2520for%2520direct%2520frame%2520editing%2520without%2520explicit%250Amasks%252C%2520enabling%2520unlimited-duration%2520inference%2520while%2520maintaining%2520natural%2520facial%250Adynamics%2520and%2520preserving%2520character%2520identity.%2520During%2520inference%252C%2520we%2520propose%2520a%250Aflow-matching-based%2520progressive%2520noise%2520initialization%2520to%2520ensure%2520pose%2520and%250Aidentity%2520consistency%252C%2520while%2520allowing%2520precise%2520mouth-region%2520editing.%2520To%2520address%250Athe%2520weak%2520conditioning%2520signal%2520of%2520audio%252C%2520we%2520develop%2520a%2520Dynamic%2520Spatiotemporal%250AClassifier-Free%2520Guidance%2520%2528DS-CFG%2529%2520mechanism%2520that%2520adaptively%2520adjusts%2520guidance%250Astrength%2520over%2520time%2520and%2520space.%2520We%2520also%2520establish%2520the%2520AIGC-LipSync%2520Benchmark%252C%2520the%250Afirst%2520evaluation%2520suite%2520for%2520lip%2520synchronization%2520in%2520diverse%2520AI-generated%2520videos.%250AExtensive%2520experiments%2520demonstrate%2520that%2520OmniSync%2520significantly%2520outperforms%2520prior%250Amethods%2520in%2520both%2520visual%2520quality%2520and%2520lip%2520sync%2520accuracy%252C%2520achieving%2520superior%250Aresults%2520in%2520both%2520real-world%2520and%2520AI-generated%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniSync%3A%20Towards%20Universal%20Lip%20Synchronization%20via%20Diffusion%0A%20%20Transformers&entry.906535625=Ziqiao%20Peng%20and%20Jiwen%20Liu%20and%20Haoxian%20Zhang%20and%20Xiaoqiang%20Liu%20and%20Songlin%20Tang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Hongyan%20Liu%20and%20Jun%20He&entry.1292438233=%20%20Lip%20synchronization%20is%20the%20task%20of%20aligning%20a%20speaker%27s%20lip%20movements%20in%0Avideo%20with%20corresponding%20speech%20audio%2C%20and%20it%20is%20essential%20for%20creating%0Arealistic%2C%20expressive%20video%20content.%20However%2C%20existing%20methods%20often%20rely%20on%0Areference%20frames%20and%20masked-frame%20inpainting%2C%20which%20limit%20their%20robustness%20to%0Aidentity%20consistency%2C%20pose%20variations%2C%20facial%20occlusions%2C%20and%20stylized%20content.%0AIn%20addition%2C%20since%20audio%20signals%20provide%20weaker%20conditioning%20than%20visual%20cues%2C%0Alip%20shape%20leakage%20from%20the%20original%20video%20will%20affect%20lip%20sync%20quality.%20In%20this%0Apaper%2C%20we%20present%20OmniSync%2C%20a%20universal%20lip%20synchronization%20framework%20for%0Adiverse%20visual%20scenarios.%20Our%20approach%20introduces%20a%20mask-free%20training%20paradigm%0Ausing%20Diffusion%20Transformer%20models%20for%20direct%20frame%20editing%20without%20explicit%0Amasks%2C%20enabling%20unlimited-duration%20inference%20while%20maintaining%20natural%20facial%0Adynamics%20and%20preserving%20character%20identity.%20During%20inference%2C%20we%20propose%20a%0Aflow-matching-based%20progressive%20noise%20initialization%20to%20ensure%20pose%20and%0Aidentity%20consistency%2C%20while%20allowing%20precise%20mouth-region%20editing.%20To%20address%0Athe%20weak%20conditioning%20signal%20of%20audio%2C%20we%20develop%20a%20Dynamic%20Spatiotemporal%0AClassifier-Free%20Guidance%20%28DS-CFG%29%20mechanism%20that%20adaptively%20adjusts%20guidance%0Astrength%20over%20time%20and%20space.%20We%20also%20establish%20the%20AIGC-LipSync%20Benchmark%2C%20the%0Afirst%20evaluation%20suite%20for%20lip%20synchronization%20in%20diverse%20AI-generated%20videos.%0AExtensive%20experiments%20demonstrate%20that%20OmniSync%20significantly%20outperforms%20prior%0Amethods%20in%20both%20visual%20quality%20and%20lip%20sync%20accuracy%2C%20achieving%20superior%0Aresults%20in%20both%20real-world%20and%20AI-generated%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21448v1&entry.124074799=Read"},
{"title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous\n  Graphs", "author": "Honglin Gao and Xiang Li and Lan Zhao and Gaoxi Xiao", "abstract": "  Heterogeneous graph neural networks (HGNNs) have recently drawn increasing\nattention for modeling complex multi-relational data in domains such as\nrecommendation, finance, and social networks. While existing research has been\nlargely focused on enhancing HGNNs' predictive performance, their robustness\nand security, especially under backdoor attacks, remain underexplored. In this\npaper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework\nfor node classification tasks on heterogeneous graphs. HeteroBA inserts\ncarefully crafted trigger nodes with realistic features and targeted structural\nconnections, leveraging attention-based and clustering-based strategies to\nselect influential auxiliary nodes for effective trigger propagation, thereby\ncausing the model to misclassify specific nodes into a target label while\nmaintaining accuracy on clean data. Experimental results on three datasets and\nvarious HGNN architectures demonstrate that HeteroBA achieves high attack\nsuccess rates with minimal impact on the clean accuracy. Our method sheds light\non potential vulnerabilities in HGNNs and calls for more robust defenses\nagainst backdoor threats in multi-relational graph scenarios.\n", "link": "http://arxiv.org/abs/2505.21140v1", "date": "2025-05-27", "relevancy": 2.2682, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4705}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeteroBA%3A%20A%20Structure-Manipulating%20Backdoor%20Attack%20on%20Heterogeneous%0A%20%20Graphs&body=Title%3A%20HeteroBA%3A%20A%20Structure-Manipulating%20Backdoor%20Attack%20on%20Heterogeneous%0A%20%20Graphs%0AAuthor%3A%20Honglin%20Gao%20and%20Xiang%20Li%20and%20Lan%20Zhao%20and%20Gaoxi%20Xiao%0AAbstract%3A%20%20%20Heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%20have%20recently%20drawn%20increasing%0Aattention%20for%20modeling%20complex%20multi-relational%20data%20in%20domains%20such%20as%0Arecommendation%2C%20finance%2C%20and%20social%20networks.%20While%20existing%20research%20has%20been%0Alargely%20focused%20on%20enhancing%20HGNNs%27%20predictive%20performance%2C%20their%20robustness%0Aand%20security%2C%20especially%20under%20backdoor%20attacks%2C%20remain%20underexplored.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20Heterogeneous%20Backdoor%20Attack%20%28HeteroBA%29%20framework%0Afor%20node%20classification%20tasks%20on%20heterogeneous%20graphs.%20HeteroBA%20inserts%0Acarefully%20crafted%20trigger%20nodes%20with%20realistic%20features%20and%20targeted%20structural%0Aconnections%2C%20leveraging%20attention-based%20and%20clustering-based%20strategies%20to%0Aselect%20influential%20auxiliary%20nodes%20for%20effective%20trigger%20propagation%2C%20thereby%0Acausing%20the%20model%20to%20misclassify%20specific%20nodes%20into%20a%20target%20label%20while%0Amaintaining%20accuracy%20on%20clean%20data.%20Experimental%20results%20on%20three%20datasets%20and%0Avarious%20HGNN%20architectures%20demonstrate%20that%20HeteroBA%20achieves%20high%20attack%0Asuccess%20rates%20with%20minimal%20impact%20on%20the%20clean%20accuracy.%20Our%20method%20sheds%20light%0Aon%20potential%20vulnerabilities%20in%20HGNNs%20and%20calls%20for%20more%20robust%20defenses%0Aagainst%20backdoor%20threats%20in%20multi-relational%20graph%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeteroBA%253A%2520A%2520Structure-Manipulating%2520Backdoor%2520Attack%2520on%2520Heterogeneous%250A%2520%2520Graphs%26entry.906535625%3DHonglin%2520Gao%2520and%2520Xiang%2520Li%2520and%2520Lan%2520Zhao%2520and%2520Gaoxi%2520Xiao%26entry.1292438233%3D%2520%2520Heterogeneous%2520graph%2520neural%2520networks%2520%2528HGNNs%2529%2520have%2520recently%2520drawn%2520increasing%250Aattention%2520for%2520modeling%2520complex%2520multi-relational%2520data%2520in%2520domains%2520such%2520as%250Arecommendation%252C%2520finance%252C%2520and%2520social%2520networks.%2520While%2520existing%2520research%2520has%2520been%250Alargely%2520focused%2520on%2520enhancing%2520HGNNs%2527%2520predictive%2520performance%252C%2520their%2520robustness%250Aand%2520security%252C%2520especially%2520under%2520backdoor%2520attacks%252C%2520remain%2520underexplored.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520Heterogeneous%2520Backdoor%2520Attack%2520%2528HeteroBA%2529%2520framework%250Afor%2520node%2520classification%2520tasks%2520on%2520heterogeneous%2520graphs.%2520HeteroBA%2520inserts%250Acarefully%2520crafted%2520trigger%2520nodes%2520with%2520realistic%2520features%2520and%2520targeted%2520structural%250Aconnections%252C%2520leveraging%2520attention-based%2520and%2520clustering-based%2520strategies%2520to%250Aselect%2520influential%2520auxiliary%2520nodes%2520for%2520effective%2520trigger%2520propagation%252C%2520thereby%250Acausing%2520the%2520model%2520to%2520misclassify%2520specific%2520nodes%2520into%2520a%2520target%2520label%2520while%250Amaintaining%2520accuracy%2520on%2520clean%2520data.%2520Experimental%2520results%2520on%2520three%2520datasets%2520and%250Avarious%2520HGNN%2520architectures%2520demonstrate%2520that%2520HeteroBA%2520achieves%2520high%2520attack%250Asuccess%2520rates%2520with%2520minimal%2520impact%2520on%2520the%2520clean%2520accuracy.%2520Our%2520method%2520sheds%2520light%250Aon%2520potential%2520vulnerabilities%2520in%2520HGNNs%2520and%2520calls%2520for%2520more%2520robust%2520defenses%250Aagainst%2520backdoor%2520threats%2520in%2520multi-relational%2520graph%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeteroBA%3A%20A%20Structure-Manipulating%20Backdoor%20Attack%20on%20Heterogeneous%0A%20%20Graphs&entry.906535625=Honglin%20Gao%20and%20Xiang%20Li%20and%20Lan%20Zhao%20and%20Gaoxi%20Xiao&entry.1292438233=%20%20Heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%20have%20recently%20drawn%20increasing%0Aattention%20for%20modeling%20complex%20multi-relational%20data%20in%20domains%20such%20as%0Arecommendation%2C%20finance%2C%20and%20social%20networks.%20While%20existing%20research%20has%20been%0Alargely%20focused%20on%20enhancing%20HGNNs%27%20predictive%20performance%2C%20their%20robustness%0Aand%20security%2C%20especially%20under%20backdoor%20attacks%2C%20remain%20underexplored.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20Heterogeneous%20Backdoor%20Attack%20%28HeteroBA%29%20framework%0Afor%20node%20classification%20tasks%20on%20heterogeneous%20graphs.%20HeteroBA%20inserts%0Acarefully%20crafted%20trigger%20nodes%20with%20realistic%20features%20and%20targeted%20structural%0Aconnections%2C%20leveraging%20attention-based%20and%20clustering-based%20strategies%20to%0Aselect%20influential%20auxiliary%20nodes%20for%20effective%20trigger%20propagation%2C%20thereby%0Acausing%20the%20model%20to%20misclassify%20specific%20nodes%20into%20a%20target%20label%20while%0Amaintaining%20accuracy%20on%20clean%20data.%20Experimental%20results%20on%20three%20datasets%20and%0Avarious%20HGNN%20architectures%20demonstrate%20that%20HeteroBA%20achieves%20high%20attack%0Asuccess%20rates%20with%20minimal%20impact%20on%20the%20clean%20accuracy.%20Our%20method%20sheds%20light%0Aon%20potential%20vulnerabilities%20in%20HGNNs%20and%20calls%20for%20more%20robust%20defenses%0Aagainst%20backdoor%20threats%20in%20multi-relational%20graph%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21140v1&entry.124074799=Read"},
{"title": "Predicting Implicit Arguments in Procedural Video Instructions", "author": "Anil Batra and Laura Sevilla-Lara and Marcus Rohrbach and Frank Keller", "abstract": "  Procedural texts help AI enhance reasoning about context and action\nsequences. Transforming these into Semantic Role Labeling (SRL) improves\nunderstanding of individual steps by identifying predicate-argument structure\nlike {verb,what,where/with}. Procedural instructions are highly elliptic, for\ninstance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second\nstep's where argument is inferred from the context, referring to where the\ncucumber was placed. Prior SRL benchmarks often miss implicit arguments,\nleading to incomplete understanding. To address this, we introduce\nImplicit-VidSRL, a dataset that necessitates inferring implicit and explicit\narguments from contextual information in multimodal cooking procedures. Our\nproposed dataset benchmarks multimodal models' contextual reasoning, requiring\nentity tracking through visual changes in recipes. We study recent multimodal\nLLMs and reveal that they struggle to predict implicit arguments of what and\nwhere/with from multi-modal procedural data given the verb. Lastly, we propose\niSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for\nwhat-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.\n", "link": "http://arxiv.org/abs/2505.21068v1", "date": "2025-05-27", "relevancy": 2.2568, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Implicit%20Arguments%20in%20Procedural%20Video%20Instructions&body=Title%3A%20Predicting%20Implicit%20Arguments%20in%20Procedural%20Video%20Instructions%0AAuthor%3A%20Anil%20Batra%20and%20Laura%20Sevilla-Lara%20and%20Marcus%20Rohrbach%20and%20Frank%20Keller%0AAbstract%3A%20%20%20Procedural%20texts%20help%20AI%20enhance%20reasoning%20about%20context%20and%20action%0Asequences.%20Transforming%20these%20into%20Semantic%20Role%20Labeling%20%28SRL%29%20improves%0Aunderstanding%20of%20individual%20steps%20by%20identifying%20predicate-argument%20structure%0Alike%20%7Bverb%2Cwhat%2Cwhere/with%7D.%20Procedural%20instructions%20are%20highly%20elliptic%2C%20for%0Ainstance%2C%20%28i%29%20add%20cucumber%20to%20the%20bowl%20and%20%28ii%29%20add%20sliced%20tomatoes%2C%20the%20second%0Astep%27s%20where%20argument%20is%20inferred%20from%20the%20context%2C%20referring%20to%20where%20the%0Acucumber%20was%20placed.%20Prior%20SRL%20benchmarks%20often%20miss%20implicit%20arguments%2C%0Aleading%20to%20incomplete%20understanding.%20To%20address%20this%2C%20we%20introduce%0AImplicit-VidSRL%2C%20a%20dataset%20that%20necessitates%20inferring%20implicit%20and%20explicit%0Aarguments%20from%20contextual%20information%20in%20multimodal%20cooking%20procedures.%20Our%0Aproposed%20dataset%20benchmarks%20multimodal%20models%27%20contextual%20reasoning%2C%20requiring%0Aentity%20tracking%20through%20visual%20changes%20in%20recipes.%20We%20study%20recent%20multimodal%0ALLMs%20and%20reveal%20that%20they%20struggle%20to%20predict%20implicit%20arguments%20of%20what%20and%0Awhere/with%20from%20multi-modal%20procedural%20data%20given%20the%20verb.%20Lastly%2C%20we%20propose%0AiSRL-Qwen2-VL%2C%20which%20achieves%20a%2017%25%20relative%20improvement%20in%20F1-score%20for%0Awhat-implicit%20and%20a%2014.7%25%20for%20where/with-implicit%20semantic%20roles%20over%20GPT-4o.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Implicit%2520Arguments%2520in%2520Procedural%2520Video%2520Instructions%26entry.906535625%3DAnil%2520Batra%2520and%2520Laura%2520Sevilla-Lara%2520and%2520Marcus%2520Rohrbach%2520and%2520Frank%2520Keller%26entry.1292438233%3D%2520%2520Procedural%2520texts%2520help%2520AI%2520enhance%2520reasoning%2520about%2520context%2520and%2520action%250Asequences.%2520Transforming%2520these%2520into%2520Semantic%2520Role%2520Labeling%2520%2528SRL%2529%2520improves%250Aunderstanding%2520of%2520individual%2520steps%2520by%2520identifying%2520predicate-argument%2520structure%250Alike%2520%257Bverb%252Cwhat%252Cwhere/with%257D.%2520Procedural%2520instructions%2520are%2520highly%2520elliptic%252C%2520for%250Ainstance%252C%2520%2528i%2529%2520add%2520cucumber%2520to%2520the%2520bowl%2520and%2520%2528ii%2529%2520add%2520sliced%2520tomatoes%252C%2520the%2520second%250Astep%2527s%2520where%2520argument%2520is%2520inferred%2520from%2520the%2520context%252C%2520referring%2520to%2520where%2520the%250Acucumber%2520was%2520placed.%2520Prior%2520SRL%2520benchmarks%2520often%2520miss%2520implicit%2520arguments%252C%250Aleading%2520to%2520incomplete%2520understanding.%2520To%2520address%2520this%252C%2520we%2520introduce%250AImplicit-VidSRL%252C%2520a%2520dataset%2520that%2520necessitates%2520inferring%2520implicit%2520and%2520explicit%250Aarguments%2520from%2520contextual%2520information%2520in%2520multimodal%2520cooking%2520procedures.%2520Our%250Aproposed%2520dataset%2520benchmarks%2520multimodal%2520models%2527%2520contextual%2520reasoning%252C%2520requiring%250Aentity%2520tracking%2520through%2520visual%2520changes%2520in%2520recipes.%2520We%2520study%2520recent%2520multimodal%250ALLMs%2520and%2520reveal%2520that%2520they%2520struggle%2520to%2520predict%2520implicit%2520arguments%2520of%2520what%2520and%250Awhere/with%2520from%2520multi-modal%2520procedural%2520data%2520given%2520the%2520verb.%2520Lastly%252C%2520we%2520propose%250AiSRL-Qwen2-VL%252C%2520which%2520achieves%2520a%252017%2525%2520relative%2520improvement%2520in%2520F1-score%2520for%250Awhat-implicit%2520and%2520a%252014.7%2525%2520for%2520where/with-implicit%2520semantic%2520roles%2520over%2520GPT-4o.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Implicit%20Arguments%20in%20Procedural%20Video%20Instructions&entry.906535625=Anil%20Batra%20and%20Laura%20Sevilla-Lara%20and%20Marcus%20Rohrbach%20and%20Frank%20Keller&entry.1292438233=%20%20Procedural%20texts%20help%20AI%20enhance%20reasoning%20about%20context%20and%20action%0Asequences.%20Transforming%20these%20into%20Semantic%20Role%20Labeling%20%28SRL%29%20improves%0Aunderstanding%20of%20individual%20steps%20by%20identifying%20predicate-argument%20structure%0Alike%20%7Bverb%2Cwhat%2Cwhere/with%7D.%20Procedural%20instructions%20are%20highly%20elliptic%2C%20for%0Ainstance%2C%20%28i%29%20add%20cucumber%20to%20the%20bowl%20and%20%28ii%29%20add%20sliced%20tomatoes%2C%20the%20second%0Astep%27s%20where%20argument%20is%20inferred%20from%20the%20context%2C%20referring%20to%20where%20the%0Acucumber%20was%20placed.%20Prior%20SRL%20benchmarks%20often%20miss%20implicit%20arguments%2C%0Aleading%20to%20incomplete%20understanding.%20To%20address%20this%2C%20we%20introduce%0AImplicit-VidSRL%2C%20a%20dataset%20that%20necessitates%20inferring%20implicit%20and%20explicit%0Aarguments%20from%20contextual%20information%20in%20multimodal%20cooking%20procedures.%20Our%0Aproposed%20dataset%20benchmarks%20multimodal%20models%27%20contextual%20reasoning%2C%20requiring%0Aentity%20tracking%20through%20visual%20changes%20in%20recipes.%20We%20study%20recent%20multimodal%0ALLMs%20and%20reveal%20that%20they%20struggle%20to%20predict%20implicit%20arguments%20of%20what%20and%0Awhere/with%20from%20multi-modal%20procedural%20data%20given%20the%20verb.%20Lastly%2C%20we%20propose%0AiSRL-Qwen2-VL%2C%20which%20achieves%20a%2017%25%20relative%20improvement%20in%20F1-score%20for%0Awhat-implicit%20and%20a%2014.7%25%20for%20where/with-implicit%20semantic%20roles%20over%20GPT-4o.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21068v1&entry.124074799=Read"},
{"title": "Interpreting Social Bias in LVLMs via Information Flow Analysis and\n  Multi-Round Dialogue Evaluation", "author": "Zhengyang Ji and Yifan Jia and Shang Gao and Yutao Yue", "abstract": "  Large Vision Language Models (LVLMs) have achieved remarkable progress in\nmultimodal tasks, yet they also exhibit notable social biases. These biases\noften manifest as unintended associations between neutral concepts and\nsensitive human attributes, leading to disparate model behaviors across\ndemographic groups. While existing studies primarily focus on detecting and\nquantifying such biases, they offer limited insight into the underlying\nmechanisms within the models. To address this gap, we propose an explanatory\nframework that combines information flow analysis with multi-round dialogue\nevaluation, aiming to understand the origin of social bias from the perspective\nof imbalanced internal information utilization. Specifically, we first identify\nhigh-contribution image tokens involved in the model's reasoning process for\nneutral questions via information flow analysis. Then, we design a multi-turn\ndialogue mechanism to evaluate the extent to which these key tokens encode\nsensitive information. Extensive experiments reveal that LVLMs exhibit\nsystematic disparities in information usage when processing images of different\ndemographic groups, suggesting that social bias is deeply rooted in the model's\ninternal reasoning dynamics. Furthermore, we complement our findings from a\ntextual modality perspective, showing that the model's semantic representations\nalready display biased proximity patterns, thereby offering a cross-modal\nexplanation of bias formation.\n", "link": "http://arxiv.org/abs/2505.21106v1", "date": "2025-05-27", "relevancy": 2.2449, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Social%20Bias%20in%20LVLMs%20via%20Information%20Flow%20Analysis%20and%0A%20%20Multi-Round%20Dialogue%20Evaluation&body=Title%3A%20Interpreting%20Social%20Bias%20in%20LVLMs%20via%20Information%20Flow%20Analysis%20and%0A%20%20Multi-Round%20Dialogue%20Evaluation%0AAuthor%3A%20Zhengyang%20Ji%20and%20Yifan%20Jia%20and%20Shang%20Gao%20and%20Yutao%20Yue%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20achieved%20remarkable%20progress%20in%0Amultimodal%20tasks%2C%20yet%20they%20also%20exhibit%20notable%20social%20biases.%20These%20biases%0Aoften%20manifest%20as%20unintended%20associations%20between%20neutral%20concepts%20and%0Asensitive%20human%20attributes%2C%20leading%20to%20disparate%20model%20behaviors%20across%0Ademographic%20groups.%20While%20existing%20studies%20primarily%20focus%20on%20detecting%20and%0Aquantifying%20such%20biases%2C%20they%20offer%20limited%20insight%20into%20the%20underlying%0Amechanisms%20within%20the%20models.%20To%20address%20this%20gap%2C%20we%20propose%20an%20explanatory%0Aframework%20that%20combines%20information%20flow%20analysis%20with%20multi-round%20dialogue%0Aevaluation%2C%20aiming%20to%20understand%20the%20origin%20of%20social%20bias%20from%20the%20perspective%0Aof%20imbalanced%20internal%20information%20utilization.%20Specifically%2C%20we%20first%20identify%0Ahigh-contribution%20image%20tokens%20involved%20in%20the%20model%27s%20reasoning%20process%20for%0Aneutral%20questions%20via%20information%20flow%20analysis.%20Then%2C%20we%20design%20a%20multi-turn%0Adialogue%20mechanism%20to%20evaluate%20the%20extent%20to%20which%20these%20key%20tokens%20encode%0Asensitive%20information.%20Extensive%20experiments%20reveal%20that%20LVLMs%20exhibit%0Asystematic%20disparities%20in%20information%20usage%20when%20processing%20images%20of%20different%0Ademographic%20groups%2C%20suggesting%20that%20social%20bias%20is%20deeply%20rooted%20in%20the%20model%27s%0Ainternal%20reasoning%20dynamics.%20Furthermore%2C%20we%20complement%20our%20findings%20from%20a%0Atextual%20modality%20perspective%2C%20showing%20that%20the%20model%27s%20semantic%20representations%0Aalready%20display%20biased%20proximity%20patterns%2C%20thereby%20offering%20a%20cross-modal%0Aexplanation%20of%20bias%20formation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Social%2520Bias%2520in%2520LVLMs%2520via%2520Information%2520Flow%2520Analysis%2520and%250A%2520%2520Multi-Round%2520Dialogue%2520Evaluation%26entry.906535625%3DZhengyang%2520Ji%2520and%2520Yifan%2520Jia%2520and%2520Shang%2520Gao%2520and%2520Yutao%2520Yue%26entry.1292438233%3D%2520%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%250Amultimodal%2520tasks%252C%2520yet%2520they%2520also%2520exhibit%2520notable%2520social%2520biases.%2520These%2520biases%250Aoften%2520manifest%2520as%2520unintended%2520associations%2520between%2520neutral%2520concepts%2520and%250Asensitive%2520human%2520attributes%252C%2520leading%2520to%2520disparate%2520model%2520behaviors%2520across%250Ademographic%2520groups.%2520While%2520existing%2520studies%2520primarily%2520focus%2520on%2520detecting%2520and%250Aquantifying%2520such%2520biases%252C%2520they%2520offer%2520limited%2520insight%2520into%2520the%2520underlying%250Amechanisms%2520within%2520the%2520models.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520an%2520explanatory%250Aframework%2520that%2520combines%2520information%2520flow%2520analysis%2520with%2520multi-round%2520dialogue%250Aevaluation%252C%2520aiming%2520to%2520understand%2520the%2520origin%2520of%2520social%2520bias%2520from%2520the%2520perspective%250Aof%2520imbalanced%2520internal%2520information%2520utilization.%2520Specifically%252C%2520we%2520first%2520identify%250Ahigh-contribution%2520image%2520tokens%2520involved%2520in%2520the%2520model%2527s%2520reasoning%2520process%2520for%250Aneutral%2520questions%2520via%2520information%2520flow%2520analysis.%2520Then%252C%2520we%2520design%2520a%2520multi-turn%250Adialogue%2520mechanism%2520to%2520evaluate%2520the%2520extent%2520to%2520which%2520these%2520key%2520tokens%2520encode%250Asensitive%2520information.%2520Extensive%2520experiments%2520reveal%2520that%2520LVLMs%2520exhibit%250Asystematic%2520disparities%2520in%2520information%2520usage%2520when%2520processing%2520images%2520of%2520different%250Ademographic%2520groups%252C%2520suggesting%2520that%2520social%2520bias%2520is%2520deeply%2520rooted%2520in%2520the%2520model%2527s%250Ainternal%2520reasoning%2520dynamics.%2520Furthermore%252C%2520we%2520complement%2520our%2520findings%2520from%2520a%250Atextual%2520modality%2520perspective%252C%2520showing%2520that%2520the%2520model%2527s%2520semantic%2520representations%250Aalready%2520display%2520biased%2520proximity%2520patterns%252C%2520thereby%2520offering%2520a%2520cross-modal%250Aexplanation%2520of%2520bias%2520formation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Social%20Bias%20in%20LVLMs%20via%20Information%20Flow%20Analysis%20and%0A%20%20Multi-Round%20Dialogue%20Evaluation&entry.906535625=Zhengyang%20Ji%20and%20Yifan%20Jia%20and%20Shang%20Gao%20and%20Yutao%20Yue&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20achieved%20remarkable%20progress%20in%0Amultimodal%20tasks%2C%20yet%20they%20also%20exhibit%20notable%20social%20biases.%20These%20biases%0Aoften%20manifest%20as%20unintended%20associations%20between%20neutral%20concepts%20and%0Asensitive%20human%20attributes%2C%20leading%20to%20disparate%20model%20behaviors%20across%0Ademographic%20groups.%20While%20existing%20studies%20primarily%20focus%20on%20detecting%20and%0Aquantifying%20such%20biases%2C%20they%20offer%20limited%20insight%20into%20the%20underlying%0Amechanisms%20within%20the%20models.%20To%20address%20this%20gap%2C%20we%20propose%20an%20explanatory%0Aframework%20that%20combines%20information%20flow%20analysis%20with%20multi-round%20dialogue%0Aevaluation%2C%20aiming%20to%20understand%20the%20origin%20of%20social%20bias%20from%20the%20perspective%0Aof%20imbalanced%20internal%20information%20utilization.%20Specifically%2C%20we%20first%20identify%0Ahigh-contribution%20image%20tokens%20involved%20in%20the%20model%27s%20reasoning%20process%20for%0Aneutral%20questions%20via%20information%20flow%20analysis.%20Then%2C%20we%20design%20a%20multi-turn%0Adialogue%20mechanism%20to%20evaluate%20the%20extent%20to%20which%20these%20key%20tokens%20encode%0Asensitive%20information.%20Extensive%20experiments%20reveal%20that%20LVLMs%20exhibit%0Asystematic%20disparities%20in%20information%20usage%20when%20processing%20images%20of%20different%0Ademographic%20groups%2C%20suggesting%20that%20social%20bias%20is%20deeply%20rooted%20in%20the%20model%27s%0Ainternal%20reasoning%20dynamics.%20Furthermore%2C%20we%20complement%20our%20findings%20from%20a%0Atextual%20modality%20perspective%2C%20showing%20that%20the%20model%27s%20semantic%20representations%0Aalready%20display%20biased%20proximity%20patterns%2C%20thereby%20offering%20a%20cross-modal%0Aexplanation%20of%20bias%20formation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21106v1&entry.124074799=Read"},
{"title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs", "author": "Rochelle Choenni and Ivan Titov", "abstract": "  Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning.\n", "link": "http://arxiv.org/abs/2505.21171v1", "date": "2025-05-27", "relevancy": 2.2396, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4632}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M-Wanda%3A%20Improving%20One-Shot%20Pruning%20for%20Multilingual%20LLMs&body=Title%3A%20M-Wanda%3A%20Improving%20One-Shot%20Pruning%20for%20Multilingual%20LLMs%0AAuthor%3A%20Rochelle%20Choenni%20and%20Ivan%20Titov%0AAbstract%3A%20%20%20Multilingual%20LLM%20performance%20is%20often%20critically%20dependent%20on%20model%20size.%0AWith%20an%20eye%20on%20efficiency%2C%20this%20has%20led%20to%20a%20surge%20in%20interest%20in%20one-shot%0Apruning%20methods%20that%20retain%20the%20benefits%20of%20large-scale%20pretraining%20while%0Ashrinking%20the%20model%20size.%20However%2C%20as%20pruning%20tends%20to%20come%20with%20performance%0Aloss%2C%20it%20is%20important%20to%20understand%20the%20trade-offs%20between%20multilinguality%20and%0Asparsification.%20In%20this%20work%2C%20we%20study%20multilingual%20performance%20under%20different%0Asparsity%20constraints%20and%20show%20that%20moderate%20ratios%20already%20substantially%20harm%0Aperformance.%20To%20help%20bridge%20this%20gap%2C%20we%20propose%20M-Wanda%2C%20a%20pruning%20method%20that%0Amodels%20cross-lingual%20variation%20by%20incorporating%20language-aware%20activation%0Astatistics%20into%20its%20pruning%20criterion%20and%20dynamically%20adjusts%20layerwise%0Asparsity%20based%20on%20cross-lingual%20importance.%20We%20show%20that%20M-Wanda%20consistently%0Aimproves%20performance%20at%20minimal%20additional%20costs.%20We%20are%20the%20first%20to%0Aexplicitly%20optimize%20pruning%20to%20retain%20multilingual%20performance%2C%20and%20hope%20to%0Ainspire%20future%20advances%20in%20multilingual%20pruning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM-Wanda%253A%2520Improving%2520One-Shot%2520Pruning%2520for%2520Multilingual%2520LLMs%26entry.906535625%3DRochelle%2520Choenni%2520and%2520Ivan%2520Titov%26entry.1292438233%3D%2520%2520Multilingual%2520LLM%2520performance%2520is%2520often%2520critically%2520dependent%2520on%2520model%2520size.%250AWith%2520an%2520eye%2520on%2520efficiency%252C%2520this%2520has%2520led%2520to%2520a%2520surge%2520in%2520interest%2520in%2520one-shot%250Apruning%2520methods%2520that%2520retain%2520the%2520benefits%2520of%2520large-scale%2520pretraining%2520while%250Ashrinking%2520the%2520model%2520size.%2520However%252C%2520as%2520pruning%2520tends%2520to%2520come%2520with%2520performance%250Aloss%252C%2520it%2520is%2520important%2520to%2520understand%2520the%2520trade-offs%2520between%2520multilinguality%2520and%250Asparsification.%2520In%2520this%2520work%252C%2520we%2520study%2520multilingual%2520performance%2520under%2520different%250Asparsity%2520constraints%2520and%2520show%2520that%2520moderate%2520ratios%2520already%2520substantially%2520harm%250Aperformance.%2520To%2520help%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520M-Wanda%252C%2520a%2520pruning%2520method%2520that%250Amodels%2520cross-lingual%2520variation%2520by%2520incorporating%2520language-aware%2520activation%250Astatistics%2520into%2520its%2520pruning%2520criterion%2520and%2520dynamically%2520adjusts%2520layerwise%250Asparsity%2520based%2520on%2520cross-lingual%2520importance.%2520We%2520show%2520that%2520M-Wanda%2520consistently%250Aimproves%2520performance%2520at%2520minimal%2520additional%2520costs.%2520We%2520are%2520the%2520first%2520to%250Aexplicitly%2520optimize%2520pruning%2520to%2520retain%2520multilingual%2520performance%252C%2520and%2520hope%2520to%250Ainspire%2520future%2520advances%2520in%2520multilingual%2520pruning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M-Wanda%3A%20Improving%20One-Shot%20Pruning%20for%20Multilingual%20LLMs&entry.906535625=Rochelle%20Choenni%20and%20Ivan%20Titov&entry.1292438233=%20%20Multilingual%20LLM%20performance%20is%20often%20critically%20dependent%20on%20model%20size.%0AWith%20an%20eye%20on%20efficiency%2C%20this%20has%20led%20to%20a%20surge%20in%20interest%20in%20one-shot%0Apruning%20methods%20that%20retain%20the%20benefits%20of%20large-scale%20pretraining%20while%0Ashrinking%20the%20model%20size.%20However%2C%20as%20pruning%20tends%20to%20come%20with%20performance%0Aloss%2C%20it%20is%20important%20to%20understand%20the%20trade-offs%20between%20multilinguality%20and%0Asparsification.%20In%20this%20work%2C%20we%20study%20multilingual%20performance%20under%20different%0Asparsity%20constraints%20and%20show%20that%20moderate%20ratios%20already%20substantially%20harm%0Aperformance.%20To%20help%20bridge%20this%20gap%2C%20we%20propose%20M-Wanda%2C%20a%20pruning%20method%20that%0Amodels%20cross-lingual%20variation%20by%20incorporating%20language-aware%20activation%0Astatistics%20into%20its%20pruning%20criterion%20and%20dynamically%20adjusts%20layerwise%0Asparsity%20based%20on%20cross-lingual%20importance.%20We%20show%20that%20M-Wanda%20consistently%0Aimproves%20performance%20at%20minimal%20additional%20costs.%20We%20are%20the%20first%20to%0Aexplicitly%20optimize%20pruning%20to%20retain%20multilingual%20performance%2C%20and%20hope%20to%0Ainspire%20future%20advances%20in%20multilingual%20pruning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21171v1&entry.124074799=Read"},
{"title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling", "author": "Xiaolong Tang and Meina Kan and Shiguang Shan and Xilin Chen", "abstract": "  Safe and feasible trajectory planning is essential for real-world autonomous\ndriving systems. However, existing learning-based planning methods often rely\non expert demonstrations, which not only lack explicit safety awareness but\nalso risk inheriting unsafe behaviors such as speeding from suboptimal human\ndriving data. Inspired by the success of large language models, we propose\nPlan-R1, a novel two-stage trajectory planning framework that formulates\ntrajectory planning as a sequential prediction task, guided by explicit\nplanning principles such as safety, comfort, and traffic rule compliance. In\nthe first stage, we train an autoregressive trajectory predictor via next\nmotion token prediction on expert data. In the second stage, we design\nrule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the\nmodel using Group Relative Policy Optimization (GRPO), a reinforcement learning\nstrategy, to align its predictions with these planning principles. Experiments\non the nuPlan benchmark demonstrate that our Plan-R1 significantly improves\nplanning safety and feasibility, achieving state-of-the-art performance. Our\ncode will be made public soon.\n", "link": "http://arxiv.org/abs/2505.17659v2", "date": "2025-05-27", "relevancy": 1.5442, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5134}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plan-R1%3A%20Safe%20and%20Feasible%20Trajectory%20Planning%20as%20Language%20Modeling&body=Title%3A%20Plan-R1%3A%20Safe%20and%20Feasible%20Trajectory%20Planning%20as%20Language%20Modeling%0AAuthor%3A%20Xiaolong%20Tang%20and%20Meina%20Kan%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20Safe%20and%20feasible%20trajectory%20planning%20is%20essential%20for%20real-world%20autonomous%0Adriving%20systems.%20However%2C%20existing%20learning-based%20planning%20methods%20often%20rely%0Aon%20expert%20demonstrations%2C%20which%20not%20only%20lack%20explicit%20safety%20awareness%20but%0Aalso%20risk%20inheriting%20unsafe%20behaviors%20such%20as%20speeding%20from%20suboptimal%20human%0Adriving%20data.%20Inspired%20by%20the%20success%20of%20large%20language%20models%2C%20we%20propose%0APlan-R1%2C%20a%20novel%20two-stage%20trajectory%20planning%20framework%20that%20formulates%0Atrajectory%20planning%20as%20a%20sequential%20prediction%20task%2C%20guided%20by%20explicit%0Aplanning%20principles%20such%20as%20safety%2C%20comfort%2C%20and%20traffic%20rule%20compliance.%20In%0Athe%20first%20stage%2C%20we%20train%20an%20autoregressive%20trajectory%20predictor%20via%20next%0Amotion%20token%20prediction%20on%20expert%20data.%20In%20the%20second%20stage%2C%20we%20design%0Arule-based%20rewards%20%28e.g.%2C%20collision%20avoidance%2C%20speed%20limits%29%20and%20fine-tune%20the%0Amodel%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20a%20reinforcement%20learning%0Astrategy%2C%20to%20align%20its%20predictions%20with%20these%20planning%20principles.%20Experiments%0Aon%20the%20nuPlan%20benchmark%20demonstrate%20that%20our%20Plan-R1%20significantly%20improves%0Aplanning%20safety%20and%20feasibility%2C%20achieving%20state-of-the-art%20performance.%20Our%0Acode%20will%20be%20made%20public%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlan-R1%253A%2520Safe%2520and%2520Feasible%2520Trajectory%2520Planning%2520as%2520Language%2520Modeling%26entry.906535625%3DXiaolong%2520Tang%2520and%2520Meina%2520Kan%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3D%2520%2520Safe%2520and%2520feasible%2520trajectory%2520planning%2520is%2520essential%2520for%2520real-world%2520autonomous%250Adriving%2520systems.%2520However%252C%2520existing%2520learning-based%2520planning%2520methods%2520often%2520rely%250Aon%2520expert%2520demonstrations%252C%2520which%2520not%2520only%2520lack%2520explicit%2520safety%2520awareness%2520but%250Aalso%2520risk%2520inheriting%2520unsafe%2520behaviors%2520such%2520as%2520speeding%2520from%2520suboptimal%2520human%250Adriving%2520data.%2520Inspired%2520by%2520the%2520success%2520of%2520large%2520language%2520models%252C%2520we%2520propose%250APlan-R1%252C%2520a%2520novel%2520two-stage%2520trajectory%2520planning%2520framework%2520that%2520formulates%250Atrajectory%2520planning%2520as%2520a%2520sequential%2520prediction%2520task%252C%2520guided%2520by%2520explicit%250Aplanning%2520principles%2520such%2520as%2520safety%252C%2520comfort%252C%2520and%2520traffic%2520rule%2520compliance.%2520In%250Athe%2520first%2520stage%252C%2520we%2520train%2520an%2520autoregressive%2520trajectory%2520predictor%2520via%2520next%250Amotion%2520token%2520prediction%2520on%2520expert%2520data.%2520In%2520the%2520second%2520stage%252C%2520we%2520design%250Arule-based%2520rewards%2520%2528e.g.%252C%2520collision%2520avoidance%252C%2520speed%2520limits%2529%2520and%2520fine-tune%2520the%250Amodel%2520using%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520a%2520reinforcement%2520learning%250Astrategy%252C%2520to%2520align%2520its%2520predictions%2520with%2520these%2520planning%2520principles.%2520Experiments%250Aon%2520the%2520nuPlan%2520benchmark%2520demonstrate%2520that%2520our%2520Plan-R1%2520significantly%2520improves%250Aplanning%2520safety%2520and%2520feasibility%252C%2520achieving%2520state-of-the-art%2520performance.%2520Our%250Acode%2520will%2520be%2520made%2520public%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plan-R1%3A%20Safe%20and%20Feasible%20Trajectory%20Planning%20as%20Language%20Modeling&entry.906535625=Xiaolong%20Tang%20and%20Meina%20Kan%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20Safe%20and%20feasible%20trajectory%20planning%20is%20essential%20for%20real-world%20autonomous%0Adriving%20systems.%20However%2C%20existing%20learning-based%20planning%20methods%20often%20rely%0Aon%20expert%20demonstrations%2C%20which%20not%20only%20lack%20explicit%20safety%20awareness%20but%0Aalso%20risk%20inheriting%20unsafe%20behaviors%20such%20as%20speeding%20from%20suboptimal%20human%0Adriving%20data.%20Inspired%20by%20the%20success%20of%20large%20language%20models%2C%20we%20propose%0APlan-R1%2C%20a%20novel%20two-stage%20trajectory%20planning%20framework%20that%20formulates%0Atrajectory%20planning%20as%20a%20sequential%20prediction%20task%2C%20guided%20by%20explicit%0Aplanning%20principles%20such%20as%20safety%2C%20comfort%2C%20and%20traffic%20rule%20compliance.%20In%0Athe%20first%20stage%2C%20we%20train%20an%20autoregressive%20trajectory%20predictor%20via%20next%0Amotion%20token%20prediction%20on%20expert%20data.%20In%20the%20second%20stage%2C%20we%20design%0Arule-based%20rewards%20%28e.g.%2C%20collision%20avoidance%2C%20speed%20limits%29%20and%20fine-tune%20the%0Amodel%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20a%20reinforcement%20learning%0Astrategy%2C%20to%20align%20its%20predictions%20with%20these%20planning%20principles.%20Experiments%0Aon%20the%20nuPlan%20benchmark%20demonstrate%20that%20our%20Plan-R1%20significantly%20improves%0Aplanning%20safety%20and%20feasibility%2C%20achieving%20state-of-the-art%20performance.%20Our%0Acode%20will%20be%20made%20public%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17659v2&entry.124074799=Read"},
{"title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents", "author": "Shashank Sharma and Janina Hoffmann and Vinay Namboodiri", "abstract": "  Hierarchical reinforcement learning (HRL) relies on abstract skills to solve\nlong-horizon tasks efficiently. While existing skill discovery methods learns\nthese skills automatically, they are limited to a single skill per task. In\ncontrast, humans learn and use both fine-grained and coarse motor skills\nsimultaneously. Inspired by human motor control, we propose Multi-Resolution\nSkill Discovery (MRSD), an HRL framework that learns multiple skill encoders at\ndifferent temporal resolutions in parallel. A high-level manager dynamically\nselects among these skills, enabling adaptive control strategies over time. We\nevaluate MRSD on tasks from the DeepMind Control Suite and show that it\noutperforms prior state-of-the-art skill discovery and HRL methods, achieving\nfaster convergence and higher final performance. Our findings highlight the\nbenefits of integrating multi-resolution skills in HRL, paving the way for more\nversatile and efficient agents.\n", "link": "http://arxiv.org/abs/2505.21410v1", "date": "2025-05-27", "relevancy": 1.5859, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5379}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRSD%3A%20Multi-Resolution%20Skill%20Discovery%20for%20HRL%20Agents&body=Title%3A%20MRSD%3A%20Multi-Resolution%20Skill%20Discovery%20for%20HRL%20Agents%0AAuthor%3A%20Shashank%20Sharma%20and%20Janina%20Hoffmann%20and%20Vinay%20Namboodiri%0AAbstract%3A%20%20%20Hierarchical%20reinforcement%20learning%20%28HRL%29%20relies%20on%20abstract%20skills%20to%20solve%0Along-horizon%20tasks%20efficiently.%20While%20existing%20skill%20discovery%20methods%20learns%0Athese%20skills%20automatically%2C%20they%20are%20limited%20to%20a%20single%20skill%20per%20task.%20In%0Acontrast%2C%20humans%20learn%20and%20use%20both%20fine-grained%20and%20coarse%20motor%20skills%0Asimultaneously.%20Inspired%20by%20human%20motor%20control%2C%20we%20propose%20Multi-Resolution%0ASkill%20Discovery%20%28MRSD%29%2C%20an%20HRL%20framework%20that%20learns%20multiple%20skill%20encoders%20at%0Adifferent%20temporal%20resolutions%20in%20parallel.%20A%20high-level%20manager%20dynamically%0Aselects%20among%20these%20skills%2C%20enabling%20adaptive%20control%20strategies%20over%20time.%20We%0Aevaluate%20MRSD%20on%20tasks%20from%20the%20DeepMind%20Control%20Suite%20and%20show%20that%20it%0Aoutperforms%20prior%20state-of-the-art%20skill%20discovery%20and%20HRL%20methods%2C%20achieving%0Afaster%20convergence%20and%20higher%20final%20performance.%20Our%20findings%20highlight%20the%0Abenefits%20of%20integrating%20multi-resolution%20skills%20in%20HRL%2C%20paving%20the%20way%20for%20more%0Aversatile%20and%20efficient%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRSD%253A%2520Multi-Resolution%2520Skill%2520Discovery%2520for%2520HRL%2520Agents%26entry.906535625%3DShashank%2520Sharma%2520and%2520Janina%2520Hoffmann%2520and%2520Vinay%2520Namboodiri%26entry.1292438233%3D%2520%2520Hierarchical%2520reinforcement%2520learning%2520%2528HRL%2529%2520relies%2520on%2520abstract%2520skills%2520to%2520solve%250Along-horizon%2520tasks%2520efficiently.%2520While%2520existing%2520skill%2520discovery%2520methods%2520learns%250Athese%2520skills%2520automatically%252C%2520they%2520are%2520limited%2520to%2520a%2520single%2520skill%2520per%2520task.%2520In%250Acontrast%252C%2520humans%2520learn%2520and%2520use%2520both%2520fine-grained%2520and%2520coarse%2520motor%2520skills%250Asimultaneously.%2520Inspired%2520by%2520human%2520motor%2520control%252C%2520we%2520propose%2520Multi-Resolution%250ASkill%2520Discovery%2520%2528MRSD%2529%252C%2520an%2520HRL%2520framework%2520that%2520learns%2520multiple%2520skill%2520encoders%2520at%250Adifferent%2520temporal%2520resolutions%2520in%2520parallel.%2520A%2520high-level%2520manager%2520dynamically%250Aselects%2520among%2520these%2520skills%252C%2520enabling%2520adaptive%2520control%2520strategies%2520over%2520time.%2520We%250Aevaluate%2520MRSD%2520on%2520tasks%2520from%2520the%2520DeepMind%2520Control%2520Suite%2520and%2520show%2520that%2520it%250Aoutperforms%2520prior%2520state-of-the-art%2520skill%2520discovery%2520and%2520HRL%2520methods%252C%2520achieving%250Afaster%2520convergence%2520and%2520higher%2520final%2520performance.%2520Our%2520findings%2520highlight%2520the%250Abenefits%2520of%2520integrating%2520multi-resolution%2520skills%2520in%2520HRL%252C%2520paving%2520the%2520way%2520for%2520more%250Aversatile%2520and%2520efficient%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRSD%3A%20Multi-Resolution%20Skill%20Discovery%20for%20HRL%20Agents&entry.906535625=Shashank%20Sharma%20and%20Janina%20Hoffmann%20and%20Vinay%20Namboodiri&entry.1292438233=%20%20Hierarchical%20reinforcement%20learning%20%28HRL%29%20relies%20on%20abstract%20skills%20to%20solve%0Along-horizon%20tasks%20efficiently.%20While%20existing%20skill%20discovery%20methods%20learns%0Athese%20skills%20automatically%2C%20they%20are%20limited%20to%20a%20single%20skill%20per%20task.%20In%0Acontrast%2C%20humans%20learn%20and%20use%20both%20fine-grained%20and%20coarse%20motor%20skills%0Asimultaneously.%20Inspired%20by%20human%20motor%20control%2C%20we%20propose%20Multi-Resolution%0ASkill%20Discovery%20%28MRSD%29%2C%20an%20HRL%20framework%20that%20learns%20multiple%20skill%20encoders%20at%0Adifferent%20temporal%20resolutions%20in%20parallel.%20A%20high-level%20manager%20dynamically%0Aselects%20among%20these%20skills%2C%20enabling%20adaptive%20control%20strategies%20over%20time.%20We%0Aevaluate%20MRSD%20on%20tasks%20from%20the%20DeepMind%20Control%20Suite%20and%20show%20that%20it%0Aoutperforms%20prior%20state-of-the-art%20skill%20discovery%20and%20HRL%20methods%2C%20achieving%0Afaster%20convergence%20and%20higher%20final%20performance.%20Our%20findings%20highlight%20the%0Abenefits%20of%20integrating%20multi-resolution%20skills%20in%20HRL%2C%20paving%20the%20way%20for%20more%0Aversatile%20and%20efficient%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21410v1&entry.124074799=Read"},
{"title": "XBOUND: Exploring the Capability Boundaries of Device-Control Agents\n  through Trajectory Tree Exploration", "author": "Shaoqing Zhang and Kehai Chen and Zhuosheng Zhang and Rumei Li and Rongxiang Weng and Yang Xiang and Liqiang Nie and Min Zhang", "abstract": "  Recent advancements in vision-language models (VLMs) have spurred increased\ninterest in Device-Control Agents (DC agents), such as utilizing in-the-wild\ndevice control to manage graphical user interfaces. Conventional methods for\nassessing the capabilities of DC agents, such as computing step-wise action\naccuracy and overall task success rates, provide a macroscopic view of DC\nagents' performance; however, they fail to offer microscopic insights into\npotential errors that may occur in real-world applications. Conducting a\nfiner-grained performance evaluation of DC agents presents significant\nchallenges. This study introduces a new perspective on evaluation methods for\nDC agents by proposing the XBOUND evaluation method, which employs the\ncalculation of a novel Explore Metric to delineate the capability boundaries of\nDC agents. Compared to previous evaluation methods, XBOUND focuses on\nindividual states to assess the proficiency of DC agents in mastering these\nstates. Furthermore, we have developed a ``pseudo'' episode tree dataset\nderived from Android Control test data. Utilizing this dataset and XBOUND, we\ncomprehensively evaluate the OS-Atlas and UI-TARS series, examining both the\noverall and specific performance across five common tasks. Additionally, we\nselect representative cases to highlight the current deficiencies and\nlimitations inherent in both series. Code is available at\nhttps://github.com/sqzhang-lazy/XBOUND.\n", "link": "http://arxiv.org/abs/2505.21279v1", "date": "2025-05-27", "relevancy": 1.6808, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5919}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5604}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XBOUND%3A%20Exploring%20the%20Capability%20Boundaries%20of%20Device-Control%20Agents%0A%20%20through%20Trajectory%20Tree%20Exploration&body=Title%3A%20XBOUND%3A%20Exploring%20the%20Capability%20Boundaries%20of%20Device-Control%20Agents%0A%20%20through%20Trajectory%20Tree%20Exploration%0AAuthor%3A%20Shaoqing%20Zhang%20and%20Kehai%20Chen%20and%20Zhuosheng%20Zhang%20and%20Rumei%20Li%20and%20Rongxiang%20Weng%20and%20Yang%20Xiang%20and%20Liqiang%20Nie%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20have%20spurred%20increased%0Ainterest%20in%20Device-Control%20Agents%20%28DC%20agents%29%2C%20such%20as%20utilizing%20in-the-wild%0Adevice%20control%20to%20manage%20graphical%20user%20interfaces.%20Conventional%20methods%20for%0Aassessing%20the%20capabilities%20of%20DC%20agents%2C%20such%20as%20computing%20step-wise%20action%0Aaccuracy%20and%20overall%20task%20success%20rates%2C%20provide%20a%20macroscopic%20view%20of%20DC%0Aagents%27%20performance%3B%20however%2C%20they%20fail%20to%20offer%20microscopic%20insights%20into%0Apotential%20errors%20that%20may%20occur%20in%20real-world%20applications.%20Conducting%20a%0Afiner-grained%20performance%20evaluation%20of%20DC%20agents%20presents%20significant%0Achallenges.%20This%20study%20introduces%20a%20new%20perspective%20on%20evaluation%20methods%20for%0ADC%20agents%20by%20proposing%20the%20XBOUND%20evaluation%20method%2C%20which%20employs%20the%0Acalculation%20of%20a%20novel%20Explore%20Metric%20to%20delineate%20the%20capability%20boundaries%20of%0ADC%20agents.%20Compared%20to%20previous%20evaluation%20methods%2C%20XBOUND%20focuses%20on%0Aindividual%20states%20to%20assess%20the%20proficiency%20of%20DC%20agents%20in%20mastering%20these%0Astates.%20Furthermore%2C%20we%20have%20developed%20a%20%60%60pseudo%27%27%20episode%20tree%20dataset%0Aderived%20from%20Android%20Control%20test%20data.%20Utilizing%20this%20dataset%20and%20XBOUND%2C%20we%0Acomprehensively%20evaluate%20the%20OS-Atlas%20and%20UI-TARS%20series%2C%20examining%20both%20the%0Aoverall%20and%20specific%20performance%20across%20five%20common%20tasks.%20Additionally%2C%20we%0Aselect%20representative%20cases%20to%20highlight%20the%20current%20deficiencies%20and%0Alimitations%20inherent%20in%20both%20series.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sqzhang-lazy/XBOUND.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXBOUND%253A%2520Exploring%2520the%2520Capability%2520Boundaries%2520of%2520Device-Control%2520Agents%250A%2520%2520through%2520Trajectory%2520Tree%2520Exploration%26entry.906535625%3DShaoqing%2520Zhang%2520and%2520Kehai%2520Chen%2520and%2520Zhuosheng%2520Zhang%2520and%2520Rumei%2520Li%2520and%2520Rongxiang%2520Weng%2520and%2520Yang%2520Xiang%2520and%2520Liqiang%2520Nie%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520spurred%2520increased%250Ainterest%2520in%2520Device-Control%2520Agents%2520%2528DC%2520agents%2529%252C%2520such%2520as%2520utilizing%2520in-the-wild%250Adevice%2520control%2520to%2520manage%2520graphical%2520user%2520interfaces.%2520Conventional%2520methods%2520for%250Aassessing%2520the%2520capabilities%2520of%2520DC%2520agents%252C%2520such%2520as%2520computing%2520step-wise%2520action%250Aaccuracy%2520and%2520overall%2520task%2520success%2520rates%252C%2520provide%2520a%2520macroscopic%2520view%2520of%2520DC%250Aagents%2527%2520performance%253B%2520however%252C%2520they%2520fail%2520to%2520offer%2520microscopic%2520insights%2520into%250Apotential%2520errors%2520that%2520may%2520occur%2520in%2520real-world%2520applications.%2520Conducting%2520a%250Afiner-grained%2520performance%2520evaluation%2520of%2520DC%2520agents%2520presents%2520significant%250Achallenges.%2520This%2520study%2520introduces%2520a%2520new%2520perspective%2520on%2520evaluation%2520methods%2520for%250ADC%2520agents%2520by%2520proposing%2520the%2520XBOUND%2520evaluation%2520method%252C%2520which%2520employs%2520the%250Acalculation%2520of%2520a%2520novel%2520Explore%2520Metric%2520to%2520delineate%2520the%2520capability%2520boundaries%2520of%250ADC%2520agents.%2520Compared%2520to%2520previous%2520evaluation%2520methods%252C%2520XBOUND%2520focuses%2520on%250Aindividual%2520states%2520to%2520assess%2520the%2520proficiency%2520of%2520DC%2520agents%2520in%2520mastering%2520these%250Astates.%2520Furthermore%252C%2520we%2520have%2520developed%2520a%2520%2560%2560pseudo%2527%2527%2520episode%2520tree%2520dataset%250Aderived%2520from%2520Android%2520Control%2520test%2520data.%2520Utilizing%2520this%2520dataset%2520and%2520XBOUND%252C%2520we%250Acomprehensively%2520evaluate%2520the%2520OS-Atlas%2520and%2520UI-TARS%2520series%252C%2520examining%2520both%2520the%250Aoverall%2520and%2520specific%2520performance%2520across%2520five%2520common%2520tasks.%2520Additionally%252C%2520we%250Aselect%2520representative%2520cases%2520to%2520highlight%2520the%2520current%2520deficiencies%2520and%250Alimitations%2520inherent%2520in%2520both%2520series.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/sqzhang-lazy/XBOUND.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XBOUND%3A%20Exploring%20the%20Capability%20Boundaries%20of%20Device-Control%20Agents%0A%20%20through%20Trajectory%20Tree%20Exploration&entry.906535625=Shaoqing%20Zhang%20and%20Kehai%20Chen%20and%20Zhuosheng%20Zhang%20and%20Rumei%20Li%20and%20Rongxiang%20Weng%20and%20Yang%20Xiang%20and%20Liqiang%20Nie%20and%20Min%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20have%20spurred%20increased%0Ainterest%20in%20Device-Control%20Agents%20%28DC%20agents%29%2C%20such%20as%20utilizing%20in-the-wild%0Adevice%20control%20to%20manage%20graphical%20user%20interfaces.%20Conventional%20methods%20for%0Aassessing%20the%20capabilities%20of%20DC%20agents%2C%20such%20as%20computing%20step-wise%20action%0Aaccuracy%20and%20overall%20task%20success%20rates%2C%20provide%20a%20macroscopic%20view%20of%20DC%0Aagents%27%20performance%3B%20however%2C%20they%20fail%20to%20offer%20microscopic%20insights%20into%0Apotential%20errors%20that%20may%20occur%20in%20real-world%20applications.%20Conducting%20a%0Afiner-grained%20performance%20evaluation%20of%20DC%20agents%20presents%20significant%0Achallenges.%20This%20study%20introduces%20a%20new%20perspective%20on%20evaluation%20methods%20for%0ADC%20agents%20by%20proposing%20the%20XBOUND%20evaluation%20method%2C%20which%20employs%20the%0Acalculation%20of%20a%20novel%20Explore%20Metric%20to%20delineate%20the%20capability%20boundaries%20of%0ADC%20agents.%20Compared%20to%20previous%20evaluation%20methods%2C%20XBOUND%20focuses%20on%0Aindividual%20states%20to%20assess%20the%20proficiency%20of%20DC%20agents%20in%20mastering%20these%0Astates.%20Furthermore%2C%20we%20have%20developed%20a%20%60%60pseudo%27%27%20episode%20tree%20dataset%0Aderived%20from%20Android%20Control%20test%20data.%20Utilizing%20this%20dataset%20and%20XBOUND%2C%20we%0Acomprehensively%20evaluate%20the%20OS-Atlas%20and%20UI-TARS%20series%2C%20examining%20both%20the%0Aoverall%20and%20specific%20performance%20across%20five%20common%20tasks.%20Additionally%2C%20we%0Aselect%20representative%20cases%20to%20highlight%20the%20current%20deficiencies%20and%0Alimitations%20inherent%20in%20both%20series.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sqzhang-lazy/XBOUND.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21279v1&entry.124074799=Read"},
{"title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models:\n  Capabilities and Challenges", "author": "Pengrui Quan and Brian Wang and Kang Yang and Liying Han and Mani Srivastava", "abstract": "  Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs.\n", "link": "http://arxiv.org/abs/2505.11618v2", "date": "2025-05-27", "relevancy": 2.1329, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Spatiotemporal%20Reasoning%20in%20LLMs%20and%20Reasoning%20Models%3A%0A%20%20Capabilities%20and%20Challenges&body=Title%3A%20Benchmarking%20Spatiotemporal%20Reasoning%20in%20LLMs%20and%20Reasoning%20Models%3A%0A%20%20Capabilities%20and%20Challenges%0AAuthor%3A%20Pengrui%20Quan%20and%20Brian%20Wang%20and%20Kang%20Yang%20and%20Liying%20Han%20and%20Mani%20Srivastava%0AAbstract%3A%20%20%20Spatiotemporal%20reasoning%20plays%20a%20key%20role%20in%20Cyber-Physical%20Systems%20%28CPS%29.%0ADespite%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Large%20Reasoning%20Models%0A%28LRMs%29%2C%20their%20capacity%20to%20reason%20about%20complex%20spatiotemporal%20signals%20remains%0Aunderexplored.%20This%20paper%20proposes%20a%20hierarchical%20SpatioTemporal%20reAsoning%0AbenchmaRK%2C%20STARK%2C%20to%20systematically%20evaluate%20LLMs%20across%20three%20levels%20of%0Areasoning%20complexity%3A%20state%20estimation%20%28e.g.%2C%20predicting%20field%20variables%2C%0Alocalizing%20and%20tracking%20events%20in%20space%20and%20time%29%2C%20spatiotemporal%20reasoning%0Aover%20states%20%28e.g.%2C%20inferring%20spatial-temporal%20relationships%29%2C%20and%0Aworld-knowledge-aware%20reasoning%20that%20integrates%20contextual%20and%20domain%20knowledge%0A%28e.g.%2C%20intent%20prediction%2C%20landmark-aware%20navigation%29.%20We%20curate%2026%20distinct%0Aspatiotemporal%20tasks%20with%20diverse%20sensor%20modalities%2C%20comprising%2014%2C552%0Achallenges%20where%20models%20answer%20directly%20or%20by%20Python%20Code%20Interpreter.%0AEvaluating%203%20LRMs%20and%208%20LLMs%2C%20we%20find%20LLMs%20achieve%20limited%20success%20in%20tasks%0Arequiring%20geometric%20reasoning%20%28e.g.%2C%20multilateration%20or%20triangulation%29%2C%0Aparticularly%20as%20complexity%20increases.%20Surprisingly%2C%20LRMs%20show%20robust%0Aperformance%20across%20tasks%20with%20various%20levels%20of%20difficulty%2C%20often%20competing%20or%0Asurpassing%20traditional%20first-principle-based%20methods.%20Our%20results%20show%20that%20in%0Areasoning%20tasks%20requiring%20world%20knowledge%2C%20the%20performance%20gap%20between%20LLMs%20and%0ALRMs%20narrows%2C%20with%20some%20LLMs%20even%20surpassing%20LRMs.%20However%2C%20the%20LRM%20o3%20model%0Acontinues%20to%20achieve%20leading%20performance%20across%20all%20evaluated%20tasks%2C%20a%20result%0Aattributed%20primarily%20to%20the%20larger%20size%20of%20the%20reasoning%20models.%20STARK%0Amotivates%20future%20innovations%20in%20model%20architectures%20and%20reasoning%20paradigms%20for%0Aintelligent%20CPS%20by%20providing%20a%20structured%20framework%20to%20identify%20limitations%20in%0Athe%20spatiotemporal%20reasoning%20of%20LLMs%20and%20LRMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Spatiotemporal%2520Reasoning%2520in%2520LLMs%2520and%2520Reasoning%2520Models%253A%250A%2520%2520Capabilities%2520and%2520Challenges%26entry.906535625%3DPengrui%2520Quan%2520and%2520Brian%2520Wang%2520and%2520Kang%2520Yang%2520and%2520Liying%2520Han%2520and%2520Mani%2520Srivastava%26entry.1292438233%3D%2520%2520Spatiotemporal%2520reasoning%2520plays%2520a%2520key%2520role%2520in%2520Cyber-Physical%2520Systems%2520%2528CPS%2529.%250ADespite%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Large%2520Reasoning%2520Models%250A%2528LRMs%2529%252C%2520their%2520capacity%2520to%2520reason%2520about%2520complex%2520spatiotemporal%2520signals%2520remains%250Aunderexplored.%2520This%2520paper%2520proposes%2520a%2520hierarchical%2520SpatioTemporal%2520reAsoning%250AbenchmaRK%252C%2520STARK%252C%2520to%2520systematically%2520evaluate%2520LLMs%2520across%2520three%2520levels%2520of%250Areasoning%2520complexity%253A%2520state%2520estimation%2520%2528e.g.%252C%2520predicting%2520field%2520variables%252C%250Alocalizing%2520and%2520tracking%2520events%2520in%2520space%2520and%2520time%2529%252C%2520spatiotemporal%2520reasoning%250Aover%2520states%2520%2528e.g.%252C%2520inferring%2520spatial-temporal%2520relationships%2529%252C%2520and%250Aworld-knowledge-aware%2520reasoning%2520that%2520integrates%2520contextual%2520and%2520domain%2520knowledge%250A%2528e.g.%252C%2520intent%2520prediction%252C%2520landmark-aware%2520navigation%2529.%2520We%2520curate%252026%2520distinct%250Aspatiotemporal%2520tasks%2520with%2520diverse%2520sensor%2520modalities%252C%2520comprising%252014%252C552%250Achallenges%2520where%2520models%2520answer%2520directly%2520or%2520by%2520Python%2520Code%2520Interpreter.%250AEvaluating%25203%2520LRMs%2520and%25208%2520LLMs%252C%2520we%2520find%2520LLMs%2520achieve%2520limited%2520success%2520in%2520tasks%250Arequiring%2520geometric%2520reasoning%2520%2528e.g.%252C%2520multilateration%2520or%2520triangulation%2529%252C%250Aparticularly%2520as%2520complexity%2520increases.%2520Surprisingly%252C%2520LRMs%2520show%2520robust%250Aperformance%2520across%2520tasks%2520with%2520various%2520levels%2520of%2520difficulty%252C%2520often%2520competing%2520or%250Asurpassing%2520traditional%2520first-principle-based%2520methods.%2520Our%2520results%2520show%2520that%2520in%250Areasoning%2520tasks%2520requiring%2520world%2520knowledge%252C%2520the%2520performance%2520gap%2520between%2520LLMs%2520and%250ALRMs%2520narrows%252C%2520with%2520some%2520LLMs%2520even%2520surpassing%2520LRMs.%2520However%252C%2520the%2520LRM%2520o3%2520model%250Acontinues%2520to%2520achieve%2520leading%2520performance%2520across%2520all%2520evaluated%2520tasks%252C%2520a%2520result%250Aattributed%2520primarily%2520to%2520the%2520larger%2520size%2520of%2520the%2520reasoning%2520models.%2520STARK%250Amotivates%2520future%2520innovations%2520in%2520model%2520architectures%2520and%2520reasoning%2520paradigms%2520for%250Aintelligent%2520CPS%2520by%2520providing%2520a%2520structured%2520framework%2520to%2520identify%2520limitations%2520in%250Athe%2520spatiotemporal%2520reasoning%2520of%2520LLMs%2520and%2520LRMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Spatiotemporal%20Reasoning%20in%20LLMs%20and%20Reasoning%20Models%3A%0A%20%20Capabilities%20and%20Challenges&entry.906535625=Pengrui%20Quan%20and%20Brian%20Wang%20and%20Kang%20Yang%20and%20Liying%20Han%20and%20Mani%20Srivastava&entry.1292438233=%20%20Spatiotemporal%20reasoning%20plays%20a%20key%20role%20in%20Cyber-Physical%20Systems%20%28CPS%29.%0ADespite%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Large%20Reasoning%20Models%0A%28LRMs%29%2C%20their%20capacity%20to%20reason%20about%20complex%20spatiotemporal%20signals%20remains%0Aunderexplored.%20This%20paper%20proposes%20a%20hierarchical%20SpatioTemporal%20reAsoning%0AbenchmaRK%2C%20STARK%2C%20to%20systematically%20evaluate%20LLMs%20across%20three%20levels%20of%0Areasoning%20complexity%3A%20state%20estimation%20%28e.g.%2C%20predicting%20field%20variables%2C%0Alocalizing%20and%20tracking%20events%20in%20space%20and%20time%29%2C%20spatiotemporal%20reasoning%0Aover%20states%20%28e.g.%2C%20inferring%20spatial-temporal%20relationships%29%2C%20and%0Aworld-knowledge-aware%20reasoning%20that%20integrates%20contextual%20and%20domain%20knowledge%0A%28e.g.%2C%20intent%20prediction%2C%20landmark-aware%20navigation%29.%20We%20curate%2026%20distinct%0Aspatiotemporal%20tasks%20with%20diverse%20sensor%20modalities%2C%20comprising%2014%2C552%0Achallenges%20where%20models%20answer%20directly%20or%20by%20Python%20Code%20Interpreter.%0AEvaluating%203%20LRMs%20and%208%20LLMs%2C%20we%20find%20LLMs%20achieve%20limited%20success%20in%20tasks%0Arequiring%20geometric%20reasoning%20%28e.g.%2C%20multilateration%20or%20triangulation%29%2C%0Aparticularly%20as%20complexity%20increases.%20Surprisingly%2C%20LRMs%20show%20robust%0Aperformance%20across%20tasks%20with%20various%20levels%20of%20difficulty%2C%20often%20competing%20or%0Asurpassing%20traditional%20first-principle-based%20methods.%20Our%20results%20show%20that%20in%0Areasoning%20tasks%20requiring%20world%20knowledge%2C%20the%20performance%20gap%20between%20LLMs%20and%0ALRMs%20narrows%2C%20with%20some%20LLMs%20even%20surpassing%20LRMs.%20However%2C%20the%20LRM%20o3%20model%0Acontinues%20to%20achieve%20leading%20performance%20across%20all%20evaluated%20tasks%2C%20a%20result%0Aattributed%20primarily%20to%20the%20larger%20size%20of%20the%20reasoning%20models.%20STARK%0Amotivates%20future%20innovations%20in%20model%20architectures%20and%20reasoning%20paradigms%20for%0Aintelligent%20CPS%20by%20providing%20a%20structured%20framework%20to%20identify%20limitations%20in%0Athe%20spatiotemporal%20reasoning%20of%20LLMs%20and%20LRMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11618v2&entry.124074799=Read"},
{"title": "Subgroups Matter for Robust Bias Mitigation", "author": "Anissa Alloula and Charles Jones and Ben Glocker and Bart\u0142omiej W. Papie\u017c", "abstract": "  Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and suggest it as a alternative lever for improving the\nrobustness and fairness of machine learning models.\n", "link": "http://arxiv.org/abs/2505.21363v1", "date": "2025-05-27", "relevancy": 1.3209, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4588}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subgroups%20Matter%20for%20Robust%20Bias%20Mitigation&body=Title%3A%20Subgroups%20Matter%20for%20Robust%20Bias%20Mitigation%0AAuthor%3A%20Anissa%20Alloula%20and%20Charles%20Jones%20and%20Ben%20Glocker%20and%20Bart%C5%82omiej%20W.%20Papie%C5%BC%0AAbstract%3A%20%20%20Despite%20the%20constant%20development%20of%20new%20bias%20mitigation%20methods%20for%20machine%0Alearning%2C%20no%20method%20consistently%20succeeds%2C%20and%20a%20fundamental%20question%20remains%0Aunanswered%3A%20when%20and%20why%20do%20bias%20mitigation%20techniques%20fail%3F%20In%20this%20paper%2C%20we%0Ahypothesise%20that%20a%20key%20factor%20may%20be%20the%20often-overlooked%20but%20crucial%20step%0Ashared%20by%20many%20bias%20mitigation%20methods%3A%20the%20definition%20of%20subgroups.%20To%0Ainvestigate%20this%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20state-of-the-art%0Abias%20mitigation%20methods%20across%20multiple%20vision%20and%20language%20classification%0Atasks%2C%20systematically%20varying%20subgroup%20definitions%2C%20including%20coarse%2C%0Afine-grained%2C%20intersectional%2C%20and%20noisy%20subgroups.%20Our%20results%20reveal%20that%0Asubgroup%20choice%20significantly%20impacts%20performance%2C%20with%20certain%20groupings%0Aparadoxically%20leading%20to%20worse%20outcomes%20than%20no%20mitigation%20at%20all.%20Our%20findings%0Asuggest%20that%20observing%20a%20disparity%20between%20a%20set%20of%20subgroups%20is%20not%20a%0Asufficient%20reason%20to%20use%20those%20subgroups%20for%20mitigation.%20Through%20theoretical%0Aanalysis%2C%20we%20explain%20these%20phenomena%20and%20uncover%20a%20counter-intuitive%20insight%0Athat%2C%20in%20some%20cases%2C%20improving%20fairness%20with%20respect%20to%20a%20particular%20set%20of%0Asubgroups%20is%20best%20achieved%20by%20using%20a%20different%20set%20of%20subgroups%20for%0Amitigation.%20Our%20work%20highlights%20the%20importance%20of%20careful%20subgroup%20definition%0Ain%20bias%20mitigation%20and%20suggest%20it%20as%20a%20alternative%20lever%20for%20improving%20the%0Arobustness%20and%20fairness%20of%20machine%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubgroups%2520Matter%2520for%2520Robust%2520Bias%2520Mitigation%26entry.906535625%3DAnissa%2520Alloula%2520and%2520Charles%2520Jones%2520and%2520Ben%2520Glocker%2520and%2520Bart%25C5%2582omiej%2520W.%2520Papie%25C5%25BC%26entry.1292438233%3D%2520%2520Despite%2520the%2520constant%2520development%2520of%2520new%2520bias%2520mitigation%2520methods%2520for%2520machine%250Alearning%252C%2520no%2520method%2520consistently%2520succeeds%252C%2520and%2520a%2520fundamental%2520question%2520remains%250Aunanswered%253A%2520when%2520and%2520why%2520do%2520bias%2520mitigation%2520techniques%2520fail%253F%2520In%2520this%2520paper%252C%2520we%250Ahypothesise%2520that%2520a%2520key%2520factor%2520may%2520be%2520the%2520often-overlooked%2520but%2520crucial%2520step%250Ashared%2520by%2520many%2520bias%2520mitigation%2520methods%253A%2520the%2520definition%2520of%2520subgroups.%2520To%250Ainvestigate%2520this%252C%2520we%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520state-of-the-art%250Abias%2520mitigation%2520methods%2520across%2520multiple%2520vision%2520and%2520language%2520classification%250Atasks%252C%2520systematically%2520varying%2520subgroup%2520definitions%252C%2520including%2520coarse%252C%250Afine-grained%252C%2520intersectional%252C%2520and%2520noisy%2520subgroups.%2520Our%2520results%2520reveal%2520that%250Asubgroup%2520choice%2520significantly%2520impacts%2520performance%252C%2520with%2520certain%2520groupings%250Aparadoxically%2520leading%2520to%2520worse%2520outcomes%2520than%2520no%2520mitigation%2520at%2520all.%2520Our%2520findings%250Asuggest%2520that%2520observing%2520a%2520disparity%2520between%2520a%2520set%2520of%2520subgroups%2520is%2520not%2520a%250Asufficient%2520reason%2520to%2520use%2520those%2520subgroups%2520for%2520mitigation.%2520Through%2520theoretical%250Aanalysis%252C%2520we%2520explain%2520these%2520phenomena%2520and%2520uncover%2520a%2520counter-intuitive%2520insight%250Athat%252C%2520in%2520some%2520cases%252C%2520improving%2520fairness%2520with%2520respect%2520to%2520a%2520particular%2520set%2520of%250Asubgroups%2520is%2520best%2520achieved%2520by%2520using%2520a%2520different%2520set%2520of%2520subgroups%2520for%250Amitigation.%2520Our%2520work%2520highlights%2520the%2520importance%2520of%2520careful%2520subgroup%2520definition%250Ain%2520bias%2520mitigation%2520and%2520suggest%2520it%2520as%2520a%2520alternative%2520lever%2520for%2520improving%2520the%250Arobustness%2520and%2520fairness%2520of%2520machine%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subgroups%20Matter%20for%20Robust%20Bias%20Mitigation&entry.906535625=Anissa%20Alloula%20and%20Charles%20Jones%20and%20Ben%20Glocker%20and%20Bart%C5%82omiej%20W.%20Papie%C5%BC&entry.1292438233=%20%20Despite%20the%20constant%20development%20of%20new%20bias%20mitigation%20methods%20for%20machine%0Alearning%2C%20no%20method%20consistently%20succeeds%2C%20and%20a%20fundamental%20question%20remains%0Aunanswered%3A%20when%20and%20why%20do%20bias%20mitigation%20techniques%20fail%3F%20In%20this%20paper%2C%20we%0Ahypothesise%20that%20a%20key%20factor%20may%20be%20the%20often-overlooked%20but%20crucial%20step%0Ashared%20by%20many%20bias%20mitigation%20methods%3A%20the%20definition%20of%20subgroups.%20To%0Ainvestigate%20this%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20state-of-the-art%0Abias%20mitigation%20methods%20across%20multiple%20vision%20and%20language%20classification%0Atasks%2C%20systematically%20varying%20subgroup%20definitions%2C%20including%20coarse%2C%0Afine-grained%2C%20intersectional%2C%20and%20noisy%20subgroups.%20Our%20results%20reveal%20that%0Asubgroup%20choice%20significantly%20impacts%20performance%2C%20with%20certain%20groupings%0Aparadoxically%20leading%20to%20worse%20outcomes%20than%20no%20mitigation%20at%20all.%20Our%20findings%0Asuggest%20that%20observing%20a%20disparity%20between%20a%20set%20of%20subgroups%20is%20not%20a%0Asufficient%20reason%20to%20use%20those%20subgroups%20for%20mitigation.%20Through%20theoretical%0Aanalysis%2C%20we%20explain%20these%20phenomena%20and%20uncover%20a%20counter-intuitive%20insight%0Athat%2C%20in%20some%20cases%2C%20improving%20fairness%20with%20respect%20to%20a%20particular%20set%20of%0Asubgroups%20is%20best%20achieved%20by%20using%20a%20different%20set%20of%20subgroups%20for%0Amitigation.%20Our%20work%20highlights%20the%20importance%20of%20careful%20subgroup%20definition%0Ain%20bias%20mitigation%20and%20suggest%20it%20as%20a%20alternative%20lever%20for%20improving%20the%0Arobustness%20and%20fairness%20of%20machine%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21363v1&entry.124074799=Read"},
{"title": "Instance Data Condensation for Image Super-Resolution", "author": "Tianhao Peng and Ho Man Kwan and Yuxuan Jiang and Ge Gao and Fan Zhang and Xiaozhong Xu and Shan Liu and David Bull", "abstract": "  Deep learning based image Super-Resolution (ISR) relies on large training\ndatasets to optimize model generalization; this requires substantial\ncomputational and storage resources during training. While dataset condensation\nhas shown potential in improving data efficiency and privacy for high-level\ncomputer vision tasks, it has not yet been fully exploited for ISR. In this\npaper, we propose a novel Instance Data Condensation (IDC) framework\nspecifically for ISR, which achieves instance-level data condensation through\nRandom Local Fourier Feature Extraction and Multi-level Feature Distribution\nMatching. This aims to optimize feature distributions at both global and local\nlevels and obtain high-quality synthesized training content with fine detail.\nThis framework has been utilized to condense the most commonly used training\ndataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic\ndataset offers comparable or (in certain cases) even better performance\ncompared to the original full dataset and excellent training stability when\nused to train various popular ISR models. To the best of our knowledge, this is\nthe first time that a condensed/synthetic dataset (with a 10% data volume) has\ndemonstrated such performance. The source code and the synthetic dataset have\nbeen made available at https://github.com/.\n", "link": "http://arxiv.org/abs/2505.21099v1", "date": "2025-05-27", "relevancy": 1.643, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.541}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance%20Data%20Condensation%20for%20Image%20Super-Resolution&body=Title%3A%20Instance%20Data%20Condensation%20for%20Image%20Super-Resolution%0AAuthor%3A%20Tianhao%20Peng%20and%20Ho%20Man%20Kwan%20and%20Yuxuan%20Jiang%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Xiaozhong%20Xu%20and%20Shan%20Liu%20and%20David%20Bull%0AAbstract%3A%20%20%20Deep%20learning%20based%20image%20Super-Resolution%20%28ISR%29%20relies%20on%20large%20training%0Adatasets%20to%20optimize%20model%20generalization%3B%20this%20requires%20substantial%0Acomputational%20and%20storage%20resources%20during%20training.%20While%20dataset%20condensation%0Ahas%20shown%20potential%20in%20improving%20data%20efficiency%20and%20privacy%20for%20high-level%0Acomputer%20vision%20tasks%2C%20it%20has%20not%20yet%20been%20fully%20exploited%20for%20ISR.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20Instance%20Data%20Condensation%20%28IDC%29%20framework%0Aspecifically%20for%20ISR%2C%20which%20achieves%20instance-level%20data%20condensation%20through%0ARandom%20Local%20Fourier%20Feature%20Extraction%20and%20Multi-level%20Feature%20Distribution%0AMatching.%20This%20aims%20to%20optimize%20feature%20distributions%20at%20both%20global%20and%20local%0Alevels%20and%20obtain%20high-quality%20synthesized%20training%20content%20with%20fine%20detail.%0AThis%20framework%20has%20been%20utilized%20to%20condense%20the%20most%20commonly%20used%20training%0Adataset%20for%20ISR%2C%20DIV2K%2C%20with%20a%2010%25%20condensation%20rate.%20The%20resulting%20synthetic%0Adataset%20offers%20comparable%20or%20%28in%20certain%20cases%29%20even%20better%20performance%0Acompared%20to%20the%20original%20full%20dataset%20and%20excellent%20training%20stability%20when%0Aused%20to%20train%20various%20popular%20ISR%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20time%20that%20a%20condensed/synthetic%20dataset%20%28with%20a%2010%25%20data%20volume%29%20has%0Ademonstrated%20such%20performance.%20The%20source%20code%20and%20the%20synthetic%20dataset%20have%0Abeen%20made%20available%20at%20https%3A//github.com/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance%2520Data%2520Condensation%2520for%2520Image%2520Super-Resolution%26entry.906535625%3DTianhao%2520Peng%2520and%2520Ho%2520Man%2520Kwan%2520and%2520Yuxuan%2520Jiang%2520and%2520Ge%2520Gao%2520and%2520Fan%2520Zhang%2520and%2520Xiaozhong%2520Xu%2520and%2520Shan%2520Liu%2520and%2520David%2520Bull%26entry.1292438233%3D%2520%2520Deep%2520learning%2520based%2520image%2520Super-Resolution%2520%2528ISR%2529%2520relies%2520on%2520large%2520training%250Adatasets%2520to%2520optimize%2520model%2520generalization%253B%2520this%2520requires%2520substantial%250Acomputational%2520and%2520storage%2520resources%2520during%2520training.%2520While%2520dataset%2520condensation%250Ahas%2520shown%2520potential%2520in%2520improving%2520data%2520efficiency%2520and%2520privacy%2520for%2520high-level%250Acomputer%2520vision%2520tasks%252C%2520it%2520has%2520not%2520yet%2520been%2520fully%2520exploited%2520for%2520ISR.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520Instance%2520Data%2520Condensation%2520%2528IDC%2529%2520framework%250Aspecifically%2520for%2520ISR%252C%2520which%2520achieves%2520instance-level%2520data%2520condensation%2520through%250ARandom%2520Local%2520Fourier%2520Feature%2520Extraction%2520and%2520Multi-level%2520Feature%2520Distribution%250AMatching.%2520This%2520aims%2520to%2520optimize%2520feature%2520distributions%2520at%2520both%2520global%2520and%2520local%250Alevels%2520and%2520obtain%2520high-quality%2520synthesized%2520training%2520content%2520with%2520fine%2520detail.%250AThis%2520framework%2520has%2520been%2520utilized%2520to%2520condense%2520the%2520most%2520commonly%2520used%2520training%250Adataset%2520for%2520ISR%252C%2520DIV2K%252C%2520with%2520a%252010%2525%2520condensation%2520rate.%2520The%2520resulting%2520synthetic%250Adataset%2520offers%2520comparable%2520or%2520%2528in%2520certain%2520cases%2529%2520even%2520better%2520performance%250Acompared%2520to%2520the%2520original%2520full%2520dataset%2520and%2520excellent%2520training%2520stability%2520when%250Aused%2520to%2520train%2520various%2520popular%2520ISR%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520time%2520that%2520a%2520condensed/synthetic%2520dataset%2520%2528with%2520a%252010%2525%2520data%2520volume%2529%2520has%250Ademonstrated%2520such%2520performance.%2520The%2520source%2520code%2520and%2520the%2520synthetic%2520dataset%2520have%250Abeen%2520made%2520available%2520at%2520https%253A//github.com/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance%20Data%20Condensation%20for%20Image%20Super-Resolution&entry.906535625=Tianhao%20Peng%20and%20Ho%20Man%20Kwan%20and%20Yuxuan%20Jiang%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Xiaozhong%20Xu%20and%20Shan%20Liu%20and%20David%20Bull&entry.1292438233=%20%20Deep%20learning%20based%20image%20Super-Resolution%20%28ISR%29%20relies%20on%20large%20training%0Adatasets%20to%20optimize%20model%20generalization%3B%20this%20requires%20substantial%0Acomputational%20and%20storage%20resources%20during%20training.%20While%20dataset%20condensation%0Ahas%20shown%20potential%20in%20improving%20data%20efficiency%20and%20privacy%20for%20high-level%0Acomputer%20vision%20tasks%2C%20it%20has%20not%20yet%20been%20fully%20exploited%20for%20ISR.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20Instance%20Data%20Condensation%20%28IDC%29%20framework%0Aspecifically%20for%20ISR%2C%20which%20achieves%20instance-level%20data%20condensation%20through%0ARandom%20Local%20Fourier%20Feature%20Extraction%20and%20Multi-level%20Feature%20Distribution%0AMatching.%20This%20aims%20to%20optimize%20feature%20distributions%20at%20both%20global%20and%20local%0Alevels%20and%20obtain%20high-quality%20synthesized%20training%20content%20with%20fine%20detail.%0AThis%20framework%20has%20been%20utilized%20to%20condense%20the%20most%20commonly%20used%20training%0Adataset%20for%20ISR%2C%20DIV2K%2C%20with%20a%2010%25%20condensation%20rate.%20The%20resulting%20synthetic%0Adataset%20offers%20comparable%20or%20%28in%20certain%20cases%29%20even%20better%20performance%0Acompared%20to%20the%20original%20full%20dataset%20and%20excellent%20training%20stability%20when%0Aused%20to%20train%20various%20popular%20ISR%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20time%20that%20a%20condensed/synthetic%20dataset%20%28with%20a%2010%25%20data%20volume%29%20has%0Ademonstrated%20such%20performance.%20The%20source%20code%20and%20the%20synthetic%20dataset%20have%0Abeen%20made%20available%20at%20https%3A//github.com/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21099v1&entry.124074799=Read"},
{"title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs", "author": "Jiakang Yuan and Tianshuo Peng and Yilei Jiang and Yiting Lu and Renrui Zhang and Kaituo Feng and Chaoyou Fu and Tao Chen and Lei Bai and Bo Zhang and Xiangyu Yue", "abstract": "  Logical reasoning is a fundamental aspect of human intelligence and an\nessential capability for multimodal large language models (MLLMs). Despite the\nsignificant advancement in multimodal reasoning, existing benchmarks fail to\ncomprehensively evaluate their reasoning abilities due to the lack of explicit\ncategorization for logical reasoning types and an unclear understanding of\nreasoning. To address these issues, we introduce MME-Reasoning, a comprehensive\nbenchmark designed to evaluate the reasoning ability of MLLMs, which covers all\nthree types of reasoning (i.e., inductive, deductive, and abductive) in its\nquestions. We carefully curate the data to ensure that each question\neffectively evaluates reasoning ability rather than perceptual skills or\nknowledge breadth, and extend the evaluation protocols to cover the evaluation\nof diverse questions. Our evaluation reveals substantial limitations of\nstate-of-the-art MLLMs when subjected to holistic assessments of logical\nreasoning capabilities. Even the most advanced MLLMs show limited performance\nin comprehensive logical reasoning, with notable performance imbalances across\nreasoning types. In addition, we conducted an in-depth analysis of approaches\nsuch as ``thinking mode'' and Rule-based RL, which are commonly believed to\nenhance reasoning abilities. These findings highlight the critical limitations\nand performance imbalances of current MLLMs in diverse logical reasoning\nscenarios, providing comprehensive and systematic insights into the\nunderstanding and evaluation of reasoning capabilities.\n", "link": "http://arxiv.org/abs/2505.21327v1", "date": "2025-05-27", "relevancy": 1.9707, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MME-Reasoning%3A%20A%20Comprehensive%20Benchmark%20for%20Logical%20Reasoning%20in%20MLLMs&body=Title%3A%20MME-Reasoning%3A%20A%20Comprehensive%20Benchmark%20for%20Logical%20Reasoning%20in%20MLLMs%0AAuthor%3A%20Jiakang%20Yuan%20and%20Tianshuo%20Peng%20and%20Yilei%20Jiang%20and%20Yiting%20Lu%20and%20Renrui%20Zhang%20and%20Kaituo%20Feng%20and%20Chaoyou%20Fu%20and%20Tao%20Chen%20and%20Lei%20Bai%20and%20Bo%20Zhang%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Logical%20reasoning%20is%20a%20fundamental%20aspect%20of%20human%20intelligence%20and%20an%0Aessential%20capability%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Despite%20the%0Asignificant%20advancement%20in%20multimodal%20reasoning%2C%20existing%20benchmarks%20fail%20to%0Acomprehensively%20evaluate%20their%20reasoning%20abilities%20due%20to%20the%20lack%20of%20explicit%0Acategorization%20for%20logical%20reasoning%20types%20and%20an%20unclear%20understanding%20of%0Areasoning.%20To%20address%20these%20issues%2C%20we%20introduce%20MME-Reasoning%2C%20a%20comprehensive%0Abenchmark%20designed%20to%20evaluate%20the%20reasoning%20ability%20of%20MLLMs%2C%20which%20covers%20all%0Athree%20types%20of%20reasoning%20%28i.e.%2C%20inductive%2C%20deductive%2C%20and%20abductive%29%20in%20its%0Aquestions.%20We%20carefully%20curate%20the%20data%20to%20ensure%20that%20each%20question%0Aeffectively%20evaluates%20reasoning%20ability%20rather%20than%20perceptual%20skills%20or%0Aknowledge%20breadth%2C%20and%20extend%20the%20evaluation%20protocols%20to%20cover%20the%20evaluation%0Aof%20diverse%20questions.%20Our%20evaluation%20reveals%20substantial%20limitations%20of%0Astate-of-the-art%20MLLMs%20when%20subjected%20to%20holistic%20assessments%20of%20logical%0Areasoning%20capabilities.%20Even%20the%20most%20advanced%20MLLMs%20show%20limited%20performance%0Ain%20comprehensive%20logical%20reasoning%2C%20with%20notable%20performance%20imbalances%20across%0Areasoning%20types.%20In%20addition%2C%20we%20conducted%20an%20in-depth%20analysis%20of%20approaches%0Asuch%20as%20%60%60thinking%20mode%27%27%20and%20Rule-based%20RL%2C%20which%20are%20commonly%20believed%20to%0Aenhance%20reasoning%20abilities.%20These%20findings%20highlight%20the%20critical%20limitations%0Aand%20performance%20imbalances%20of%20current%20MLLMs%20in%20diverse%20logical%20reasoning%0Ascenarios%2C%20providing%20comprehensive%20and%20systematic%20insights%20into%20the%0Aunderstanding%20and%20evaluation%20of%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMME-Reasoning%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Logical%2520Reasoning%2520in%2520MLLMs%26entry.906535625%3DJiakang%2520Yuan%2520and%2520Tianshuo%2520Peng%2520and%2520Yilei%2520Jiang%2520and%2520Yiting%2520Lu%2520and%2520Renrui%2520Zhang%2520and%2520Kaituo%2520Feng%2520and%2520Chaoyou%2520Fu%2520and%2520Tao%2520Chen%2520and%2520Lei%2520Bai%2520and%2520Bo%2520Zhang%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Logical%2520reasoning%2520is%2520a%2520fundamental%2520aspect%2520of%2520human%2520intelligence%2520and%2520an%250Aessential%2520capability%2520for%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Despite%2520the%250Asignificant%2520advancement%2520in%2520multimodal%2520reasoning%252C%2520existing%2520benchmarks%2520fail%2520to%250Acomprehensively%2520evaluate%2520their%2520reasoning%2520abilities%2520due%2520to%2520the%2520lack%2520of%2520explicit%250Acategorization%2520for%2520logical%2520reasoning%2520types%2520and%2520an%2520unclear%2520understanding%2520of%250Areasoning.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520MME-Reasoning%252C%2520a%2520comprehensive%250Abenchmark%2520designed%2520to%2520evaluate%2520the%2520reasoning%2520ability%2520of%2520MLLMs%252C%2520which%2520covers%2520all%250Athree%2520types%2520of%2520reasoning%2520%2528i.e.%252C%2520inductive%252C%2520deductive%252C%2520and%2520abductive%2529%2520in%2520its%250Aquestions.%2520We%2520carefully%2520curate%2520the%2520data%2520to%2520ensure%2520that%2520each%2520question%250Aeffectively%2520evaluates%2520reasoning%2520ability%2520rather%2520than%2520perceptual%2520skills%2520or%250Aknowledge%2520breadth%252C%2520and%2520extend%2520the%2520evaluation%2520protocols%2520to%2520cover%2520the%2520evaluation%250Aof%2520diverse%2520questions.%2520Our%2520evaluation%2520reveals%2520substantial%2520limitations%2520of%250Astate-of-the-art%2520MLLMs%2520when%2520subjected%2520to%2520holistic%2520assessments%2520of%2520logical%250Areasoning%2520capabilities.%2520Even%2520the%2520most%2520advanced%2520MLLMs%2520show%2520limited%2520performance%250Ain%2520comprehensive%2520logical%2520reasoning%252C%2520with%2520notable%2520performance%2520imbalances%2520across%250Areasoning%2520types.%2520In%2520addition%252C%2520we%2520conducted%2520an%2520in-depth%2520analysis%2520of%2520approaches%250Asuch%2520as%2520%2560%2560thinking%2520mode%2527%2527%2520and%2520Rule-based%2520RL%252C%2520which%2520are%2520commonly%2520believed%2520to%250Aenhance%2520reasoning%2520abilities.%2520These%2520findings%2520highlight%2520the%2520critical%2520limitations%250Aand%2520performance%2520imbalances%2520of%2520current%2520MLLMs%2520in%2520diverse%2520logical%2520reasoning%250Ascenarios%252C%2520providing%2520comprehensive%2520and%2520systematic%2520insights%2520into%2520the%250Aunderstanding%2520and%2520evaluation%2520of%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MME-Reasoning%3A%20A%20Comprehensive%20Benchmark%20for%20Logical%20Reasoning%20in%20MLLMs&entry.906535625=Jiakang%20Yuan%20and%20Tianshuo%20Peng%20and%20Yilei%20Jiang%20and%20Yiting%20Lu%20and%20Renrui%20Zhang%20and%20Kaituo%20Feng%20and%20Chaoyou%20Fu%20and%20Tao%20Chen%20and%20Lei%20Bai%20and%20Bo%20Zhang%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Logical%20reasoning%20is%20a%20fundamental%20aspect%20of%20human%20intelligence%20and%20an%0Aessential%20capability%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Despite%20the%0Asignificant%20advancement%20in%20multimodal%20reasoning%2C%20existing%20benchmarks%20fail%20to%0Acomprehensively%20evaluate%20their%20reasoning%20abilities%20due%20to%20the%20lack%20of%20explicit%0Acategorization%20for%20logical%20reasoning%20types%20and%20an%20unclear%20understanding%20of%0Areasoning.%20To%20address%20these%20issues%2C%20we%20introduce%20MME-Reasoning%2C%20a%20comprehensive%0Abenchmark%20designed%20to%20evaluate%20the%20reasoning%20ability%20of%20MLLMs%2C%20which%20covers%20all%0Athree%20types%20of%20reasoning%20%28i.e.%2C%20inductive%2C%20deductive%2C%20and%20abductive%29%20in%20its%0Aquestions.%20We%20carefully%20curate%20the%20data%20to%20ensure%20that%20each%20question%0Aeffectively%20evaluates%20reasoning%20ability%20rather%20than%20perceptual%20skills%20or%0Aknowledge%20breadth%2C%20and%20extend%20the%20evaluation%20protocols%20to%20cover%20the%20evaluation%0Aof%20diverse%20questions.%20Our%20evaluation%20reveals%20substantial%20limitations%20of%0Astate-of-the-art%20MLLMs%20when%20subjected%20to%20holistic%20assessments%20of%20logical%0Areasoning%20capabilities.%20Even%20the%20most%20advanced%20MLLMs%20show%20limited%20performance%0Ain%20comprehensive%20logical%20reasoning%2C%20with%20notable%20performance%20imbalances%20across%0Areasoning%20types.%20In%20addition%2C%20we%20conducted%20an%20in-depth%20analysis%20of%20approaches%0Asuch%20as%20%60%60thinking%20mode%27%27%20and%20Rule-based%20RL%2C%20which%20are%20commonly%20believed%20to%0Aenhance%20reasoning%20abilities.%20These%20findings%20highlight%20the%20critical%20limitations%0Aand%20performance%20imbalances%20of%20current%20MLLMs%20in%20diverse%20logical%20reasoning%0Ascenarios%2C%20providing%20comprehensive%20and%20systematic%20insights%20into%20the%0Aunderstanding%20and%20evaluation%20of%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21327v1&entry.124074799=Read"},
{"title": "Transparent and Coherent Procedural Mistake Detection", "author": "Shane Storks and Itamar Bar-Yossef and Yayuan Li and Zheyuan Zhang and Jason J. Corso and Joyce Chai", "abstract": "  Procedural mistake detection (PMD) is a challenging problem of classifying\nwhether a human user (observed through egocentric video) has successfully\nexecuted a task (specified by a procedural text). Despite significant recent\nefforts, machine performance in the wild remains nonviable, and the reasoning\nprocesses underlying this performance are opaque. As such, we extend PMD to\nrequire generating visual self-dialog rationales to inform decisions. Given the\nimpressive, mature image understanding capabilities observed in recent\nvision-and-language models (VLMs), we curate a suitable benchmark dataset for\nPMD based on individual frames. As our reformulation enables unprecedented\ntransparency, we leverage a natural language inference (NLI) model to formulate\ntwo automated metrics for the coherence of generated rationales. We establish\nbaselines for this reframed task, showing that while VLMs struggle\noff-the-shelf, their accuracy, coherence, and efficiency can be improved by\nincorporating these metrics into common inference and fine-tuning methods-\nthough not without tradeoff. Lastly, our multi-faceted metrics visualize common\noutcomes, highlighting areas for further improvement.\n", "link": "http://arxiv.org/abs/2412.11927v2", "date": "2025-05-27", "relevancy": 2.1129, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transparent%20and%20Coherent%20Procedural%20Mistake%20Detection&body=Title%3A%20Transparent%20and%20Coherent%20Procedural%20Mistake%20Detection%0AAuthor%3A%20Shane%20Storks%20and%20Itamar%20Bar-Yossef%20and%20Yayuan%20Li%20and%20Zheyuan%20Zhang%20and%20Jason%20J.%20Corso%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Procedural%20mistake%20detection%20%28PMD%29%20is%20a%20challenging%20problem%20of%20classifying%0Awhether%20a%20human%20user%20%28observed%20through%20egocentric%20video%29%20has%20successfully%0Aexecuted%20a%20task%20%28specified%20by%20a%20procedural%20text%29.%20Despite%20significant%20recent%0Aefforts%2C%20machine%20performance%20in%20the%20wild%20remains%20nonviable%2C%20and%20the%20reasoning%0Aprocesses%20underlying%20this%20performance%20are%20opaque.%20As%20such%2C%20we%20extend%20PMD%20to%0Arequire%20generating%20visual%20self-dialog%20rationales%20to%20inform%20decisions.%20Given%20the%0Aimpressive%2C%20mature%20image%20understanding%20capabilities%20observed%20in%20recent%0Avision-and-language%20models%20%28VLMs%29%2C%20we%20curate%20a%20suitable%20benchmark%20dataset%20for%0APMD%20based%20on%20individual%20frames.%20As%20our%20reformulation%20enables%20unprecedented%0Atransparency%2C%20we%20leverage%20a%20natural%20language%20inference%20%28NLI%29%20model%20to%20formulate%0Atwo%20automated%20metrics%20for%20the%20coherence%20of%20generated%20rationales.%20We%20establish%0Abaselines%20for%20this%20reframed%20task%2C%20showing%20that%20while%20VLMs%20struggle%0Aoff-the-shelf%2C%20their%20accuracy%2C%20coherence%2C%20and%20efficiency%20can%20be%20improved%20by%0Aincorporating%20these%20metrics%20into%20common%20inference%20and%20fine-tuning%20methods-%0Athough%20not%20without%20tradeoff.%20Lastly%2C%20our%20multi-faceted%20metrics%20visualize%20common%0Aoutcomes%2C%20highlighting%20areas%20for%20further%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransparent%2520and%2520Coherent%2520Procedural%2520Mistake%2520Detection%26entry.906535625%3DShane%2520Storks%2520and%2520Itamar%2520Bar-Yossef%2520and%2520Yayuan%2520Li%2520and%2520Zheyuan%2520Zhang%2520and%2520Jason%2520J.%2520Corso%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Procedural%2520mistake%2520detection%2520%2528PMD%2529%2520is%2520a%2520challenging%2520problem%2520of%2520classifying%250Awhether%2520a%2520human%2520user%2520%2528observed%2520through%2520egocentric%2520video%2529%2520has%2520successfully%250Aexecuted%2520a%2520task%2520%2528specified%2520by%2520a%2520procedural%2520text%2529.%2520Despite%2520significant%2520recent%250Aefforts%252C%2520machine%2520performance%2520in%2520the%2520wild%2520remains%2520nonviable%252C%2520and%2520the%2520reasoning%250Aprocesses%2520underlying%2520this%2520performance%2520are%2520opaque.%2520As%2520such%252C%2520we%2520extend%2520PMD%2520to%250Arequire%2520generating%2520visual%2520self-dialog%2520rationales%2520to%2520inform%2520decisions.%2520Given%2520the%250Aimpressive%252C%2520mature%2520image%2520understanding%2520capabilities%2520observed%2520in%2520recent%250Avision-and-language%2520models%2520%2528VLMs%2529%252C%2520we%2520curate%2520a%2520suitable%2520benchmark%2520dataset%2520for%250APMD%2520based%2520on%2520individual%2520frames.%2520As%2520our%2520reformulation%2520enables%2520unprecedented%250Atransparency%252C%2520we%2520leverage%2520a%2520natural%2520language%2520inference%2520%2528NLI%2529%2520model%2520to%2520formulate%250Atwo%2520automated%2520metrics%2520for%2520the%2520coherence%2520of%2520generated%2520rationales.%2520We%2520establish%250Abaselines%2520for%2520this%2520reframed%2520task%252C%2520showing%2520that%2520while%2520VLMs%2520struggle%250Aoff-the-shelf%252C%2520their%2520accuracy%252C%2520coherence%252C%2520and%2520efficiency%2520can%2520be%2520improved%2520by%250Aincorporating%2520these%2520metrics%2520into%2520common%2520inference%2520and%2520fine-tuning%2520methods-%250Athough%2520not%2520without%2520tradeoff.%2520Lastly%252C%2520our%2520multi-faceted%2520metrics%2520visualize%2520common%250Aoutcomes%252C%2520highlighting%2520areas%2520for%2520further%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transparent%20and%20Coherent%20Procedural%20Mistake%20Detection&entry.906535625=Shane%20Storks%20and%20Itamar%20Bar-Yossef%20and%20Yayuan%20Li%20and%20Zheyuan%20Zhang%20and%20Jason%20J.%20Corso%20and%20Joyce%20Chai&entry.1292438233=%20%20Procedural%20mistake%20detection%20%28PMD%29%20is%20a%20challenging%20problem%20of%20classifying%0Awhether%20a%20human%20user%20%28observed%20through%20egocentric%20video%29%20has%20successfully%0Aexecuted%20a%20task%20%28specified%20by%20a%20procedural%20text%29.%20Despite%20significant%20recent%0Aefforts%2C%20machine%20performance%20in%20the%20wild%20remains%20nonviable%2C%20and%20the%20reasoning%0Aprocesses%20underlying%20this%20performance%20are%20opaque.%20As%20such%2C%20we%20extend%20PMD%20to%0Arequire%20generating%20visual%20self-dialog%20rationales%20to%20inform%20decisions.%20Given%20the%0Aimpressive%2C%20mature%20image%20understanding%20capabilities%20observed%20in%20recent%0Avision-and-language%20models%20%28VLMs%29%2C%20we%20curate%20a%20suitable%20benchmark%20dataset%20for%0APMD%20based%20on%20individual%20frames.%20As%20our%20reformulation%20enables%20unprecedented%0Atransparency%2C%20we%20leverage%20a%20natural%20language%20inference%20%28NLI%29%20model%20to%20formulate%0Atwo%20automated%20metrics%20for%20the%20coherence%20of%20generated%20rationales.%20We%20establish%0Abaselines%20for%20this%20reframed%20task%2C%20showing%20that%20while%20VLMs%20struggle%0Aoff-the-shelf%2C%20their%20accuracy%2C%20coherence%2C%20and%20efficiency%20can%20be%20improved%20by%0Aincorporating%20these%20metrics%20into%20common%20inference%20and%20fine-tuning%20methods-%0Athough%20not%20without%20tradeoff.%20Lastly%2C%20our%20multi-faceted%20metrics%20visualize%20common%0Aoutcomes%2C%20highlighting%20areas%20for%20further%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11927v2&entry.124074799=Read"},
{"title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach", "author": "Xu Zhang and Kaidi Xu and Ziqing Hu and Ren Wang", "abstract": "  Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods. The code is publicly\navailable at https://github.com/TIML-Group/Robust-MoE-Dual-Model.\n", "link": "http://arxiv.org/abs/2502.06832v3", "date": "2025-05-27", "relevancy": 2.1068, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5309}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5258}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Robustness%20and%20Accuracy%20in%20Mixture%20of%20Experts%3A%20A%20Dual-Model%0A%20%20Approach&body=Title%3A%20Optimizing%20Robustness%20and%20Accuracy%20in%20Mixture%20of%20Experts%3A%20A%20Dual-Model%0A%20%20Approach%0AAuthor%3A%20Xu%20Zhang%20and%20Kaidi%20Xu%20and%20Ziqing%20Hu%20and%20Ren%20Wang%0AAbstract%3A%20%20%20Mixture%20of%20Experts%20%28MoE%29%20have%20shown%20remarkable%20success%20in%20leveraging%0Aspecialized%20expert%20networks%20for%20complex%20machine%20learning%20tasks.%20However%2C%20their%0Asusceptibility%20to%20adversarial%20attacks%20presents%20a%20critical%20challenge%20for%0Adeployment%20in%20robust%20applications.%20This%20paper%20addresses%20the%20critical%20question%0Aof%20how%20to%20incorporate%20robustness%20into%20MoEs%20while%20maintaining%20high%20natural%0Aaccuracy.%20We%20begin%20by%20analyzing%20the%20vulnerability%20of%20MoE%20components%2C%20finding%0Athat%20expert%20networks%20are%20notably%20more%20susceptible%20to%20adversarial%20attacks%20than%0Athe%20router.%20Based%20on%20this%20insight%2C%20we%20propose%20a%20targeted%20robust%20training%0Atechnique%20that%20integrates%20a%20novel%20loss%20function%20to%20enhance%20the%20adversarial%0Arobustness%20of%20MoE%2C%20requiring%20only%20the%20robustification%20of%20one%20additional%20expert%0Awithout%20compromising%20training%20or%20inference%20efficiency.%20Building%20on%20this%2C%20we%0Aintroduce%20a%20dual-model%20strategy%20that%20linearly%20combines%20a%20standard%20MoE%20model%0Awith%20our%20robustified%20MoE%20model%20using%20a%20smoothing%20parameter.%20This%20approach%0Aallows%20for%20flexible%20control%20over%20the%20robustness-accuracy%20trade-off.%20We%20further%0Aprovide%20theoretical%20foundations%20by%20deriving%20certified%20robustness%20bounds%20for%0Aboth%20the%20single%20MoE%20and%20the%20dual-model.%20To%20push%20the%20boundaries%20of%20robustness%0Aand%20accuracy%2C%20we%20propose%20a%20novel%20joint%20training%20strategy%20JTDMoE%20for%20the%0Adual-model.%20This%20joint%20training%20enhances%20both%20robustness%20and%20accuracy%20beyond%0Awhat%20is%20achievable%20with%20separate%20models.%20Experimental%20results%20on%20CIFAR-10%20and%0ATinyImageNet%20datasets%20using%20ResNet18%20and%20Vision%20Transformer%20%28ViT%29%20architectures%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20methods.%20The%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/TIML-Group/Robust-MoE-Dual-Model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06832v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Robustness%2520and%2520Accuracy%2520in%2520Mixture%2520of%2520Experts%253A%2520A%2520Dual-Model%250A%2520%2520Approach%26entry.906535625%3DXu%2520Zhang%2520and%2520Kaidi%2520Xu%2520and%2520Ziqing%2520Hu%2520and%2520Ren%2520Wang%26entry.1292438233%3D%2520%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520have%2520shown%2520remarkable%2520success%2520in%2520leveraging%250Aspecialized%2520expert%2520networks%2520for%2520complex%2520machine%2520learning%2520tasks.%2520However%252C%2520their%250Asusceptibility%2520to%2520adversarial%2520attacks%2520presents%2520a%2520critical%2520challenge%2520for%250Adeployment%2520in%2520robust%2520applications.%2520This%2520paper%2520addresses%2520the%2520critical%2520question%250Aof%2520how%2520to%2520incorporate%2520robustness%2520into%2520MoEs%2520while%2520maintaining%2520high%2520natural%250Aaccuracy.%2520We%2520begin%2520by%2520analyzing%2520the%2520vulnerability%2520of%2520MoE%2520components%252C%2520finding%250Athat%2520expert%2520networks%2520are%2520notably%2520more%2520susceptible%2520to%2520adversarial%2520attacks%2520than%250Athe%2520router.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520targeted%2520robust%2520training%250Atechnique%2520that%2520integrates%2520a%2520novel%2520loss%2520function%2520to%2520enhance%2520the%2520adversarial%250Arobustness%2520of%2520MoE%252C%2520requiring%2520only%2520the%2520robustification%2520of%2520one%2520additional%2520expert%250Awithout%2520compromising%2520training%2520or%2520inference%2520efficiency.%2520Building%2520on%2520this%252C%2520we%250Aintroduce%2520a%2520dual-model%2520strategy%2520that%2520linearly%2520combines%2520a%2520standard%2520MoE%2520model%250Awith%2520our%2520robustified%2520MoE%2520model%2520using%2520a%2520smoothing%2520parameter.%2520This%2520approach%250Aallows%2520for%2520flexible%2520control%2520over%2520the%2520robustness-accuracy%2520trade-off.%2520We%2520further%250Aprovide%2520theoretical%2520foundations%2520by%2520deriving%2520certified%2520robustness%2520bounds%2520for%250Aboth%2520the%2520single%2520MoE%2520and%2520the%2520dual-model.%2520To%2520push%2520the%2520boundaries%2520of%2520robustness%250Aand%2520accuracy%252C%2520we%2520propose%2520a%2520novel%2520joint%2520training%2520strategy%2520JTDMoE%2520for%2520the%250Adual-model.%2520This%2520joint%2520training%2520enhances%2520both%2520robustness%2520and%2520accuracy%2520beyond%250Awhat%2520is%2520achievable%2520with%2520separate%2520models.%2520Experimental%2520results%2520on%2520CIFAR-10%2520and%250ATinyImageNet%2520datasets%2520using%2520ResNet18%2520and%2520Vision%2520Transformer%2520%2528ViT%2529%2520architectures%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520methods.%2520The%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/TIML-Group/Robust-MoE-Dual-Model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06832v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Robustness%20and%20Accuracy%20in%20Mixture%20of%20Experts%3A%20A%20Dual-Model%0A%20%20Approach&entry.906535625=Xu%20Zhang%20and%20Kaidi%20Xu%20and%20Ziqing%20Hu%20and%20Ren%20Wang&entry.1292438233=%20%20Mixture%20of%20Experts%20%28MoE%29%20have%20shown%20remarkable%20success%20in%20leveraging%0Aspecialized%20expert%20networks%20for%20complex%20machine%20learning%20tasks.%20However%2C%20their%0Asusceptibility%20to%20adversarial%20attacks%20presents%20a%20critical%20challenge%20for%0Adeployment%20in%20robust%20applications.%20This%20paper%20addresses%20the%20critical%20question%0Aof%20how%20to%20incorporate%20robustness%20into%20MoEs%20while%20maintaining%20high%20natural%0Aaccuracy.%20We%20begin%20by%20analyzing%20the%20vulnerability%20of%20MoE%20components%2C%20finding%0Athat%20expert%20networks%20are%20notably%20more%20susceptible%20to%20adversarial%20attacks%20than%0Athe%20router.%20Based%20on%20this%20insight%2C%20we%20propose%20a%20targeted%20robust%20training%0Atechnique%20that%20integrates%20a%20novel%20loss%20function%20to%20enhance%20the%20adversarial%0Arobustness%20of%20MoE%2C%20requiring%20only%20the%20robustification%20of%20one%20additional%20expert%0Awithout%20compromising%20training%20or%20inference%20efficiency.%20Building%20on%20this%2C%20we%0Aintroduce%20a%20dual-model%20strategy%20that%20linearly%20combines%20a%20standard%20MoE%20model%0Awith%20our%20robustified%20MoE%20model%20using%20a%20smoothing%20parameter.%20This%20approach%0Aallows%20for%20flexible%20control%20over%20the%20robustness-accuracy%20trade-off.%20We%20further%0Aprovide%20theoretical%20foundations%20by%20deriving%20certified%20robustness%20bounds%20for%0Aboth%20the%20single%20MoE%20and%20the%20dual-model.%20To%20push%20the%20boundaries%20of%20robustness%0Aand%20accuracy%2C%20we%20propose%20a%20novel%20joint%20training%20strategy%20JTDMoE%20for%20the%0Adual-model.%20This%20joint%20training%20enhances%20both%20robustness%20and%20accuracy%20beyond%0Awhat%20is%20achievable%20with%20separate%20models.%20Experimental%20results%20on%20CIFAR-10%20and%0ATinyImageNet%20datasets%20using%20ResNet18%20and%20Vision%20Transformer%20%28ViT%29%20architectures%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20methods.%20The%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/TIML-Group/Robust-MoE-Dual-Model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06832v3&entry.124074799=Read"},
{"title": "Sequential Resource Trading Using Comparison-Based Gradient Estimation", "author": "Surya Murthy and Mustafa O. Karabag and Ufuk Topcu", "abstract": "  Autonomous agents interact with other autonomous agents and humans of unknown\npreferences to share resources in their environment. We explore sequential\ntrading for resource allocation in a setting where two greedily rational agents\nsequentially trade resources from a finite set of categories. Each agent has a\nutility function that depends on the amount of resources it possesses in each\ncategory. The offering agent makes trade offers to improve its utility without\nknowing the responding agent's utility function, and the responding agent only\naccepts offers that improve its utility. To facilitate cooperation between an\nautonomous agent and another autonomous agent or a human, we present an\nalgorithm for the offering agent to estimate the responding agent's gradient\n(preferences) and make offers based on previous acceptance or rejection\nresponses. The algorithm's goal is to reach a Pareto-optimal resource\nallocation state while ensuring that the utilities of both agents improve after\nevery accepted trade. The algorithm estimates the responding agent's gradient\nby leveraging the rejected offers and the greedy rationality assumption, to\nprune the space of potential gradients. We show that, after the algorithm makes\na finite number of rejected offers, the algorithm either finds a mutually\nbeneficial trade or certifies that the current state is epsilon-weakly Pareto\noptimal. We compare the proposed algorithm against various baselines in\ncontinuous and discrete trading scenarios and show that it improves the\nsocietal benefit with fewer offers. Additionally, we validate these findings in\na user study with human participants, where the algorithm achieves high\nperformance in scenarios with high resource conflict due to aligned agent\ngoals.\n", "link": "http://arxiv.org/abs/2408.11186v3", "date": "2025-05-27", "relevancy": 1.9073, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4895}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4699}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequential%20Resource%20Trading%20Using%20Comparison-Based%20Gradient%20Estimation&body=Title%3A%20Sequential%20Resource%20Trading%20Using%20Comparison-Based%20Gradient%20Estimation%0AAuthor%3A%20Surya%20Murthy%20and%20Mustafa%20O.%20Karabag%20and%20Ufuk%20Topcu%0AAbstract%3A%20%20%20Autonomous%20agents%20interact%20with%20other%20autonomous%20agents%20and%20humans%20of%20unknown%0Apreferences%20to%20share%20resources%20in%20their%20environment.%20We%20explore%20sequential%0Atrading%20for%20resource%20allocation%20in%20a%20setting%20where%20two%20greedily%20rational%20agents%0Asequentially%20trade%20resources%20from%20a%20finite%20set%20of%20categories.%20Each%20agent%20has%20a%0Autility%20function%20that%20depends%20on%20the%20amount%20of%20resources%20it%20possesses%20in%20each%0Acategory.%20The%20offering%20agent%20makes%20trade%20offers%20to%20improve%20its%20utility%20without%0Aknowing%20the%20responding%20agent%27s%20utility%20function%2C%20and%20the%20responding%20agent%20only%0Aaccepts%20offers%20that%20improve%20its%20utility.%20To%20facilitate%20cooperation%20between%20an%0Aautonomous%20agent%20and%20another%20autonomous%20agent%20or%20a%20human%2C%20we%20present%20an%0Aalgorithm%20for%20the%20offering%20agent%20to%20estimate%20the%20responding%20agent%27s%20gradient%0A%28preferences%29%20and%20make%20offers%20based%20on%20previous%20acceptance%20or%20rejection%0Aresponses.%20The%20algorithm%27s%20goal%20is%20to%20reach%20a%20Pareto-optimal%20resource%0Aallocation%20state%20while%20ensuring%20that%20the%20utilities%20of%20both%20agents%20improve%20after%0Aevery%20accepted%20trade.%20The%20algorithm%20estimates%20the%20responding%20agent%27s%20gradient%0Aby%20leveraging%20the%20rejected%20offers%20and%20the%20greedy%20rationality%20assumption%2C%20to%0Aprune%20the%20space%20of%20potential%20gradients.%20We%20show%20that%2C%20after%20the%20algorithm%20makes%0Aa%20finite%20number%20of%20rejected%20offers%2C%20the%20algorithm%20either%20finds%20a%20mutually%0Abeneficial%20trade%20or%20certifies%20that%20the%20current%20state%20is%20epsilon-weakly%20Pareto%0Aoptimal.%20We%20compare%20the%20proposed%20algorithm%20against%20various%20baselines%20in%0Acontinuous%20and%20discrete%20trading%20scenarios%20and%20show%20that%20it%20improves%20the%0Asocietal%20benefit%20with%20fewer%20offers.%20Additionally%2C%20we%20validate%20these%20findings%20in%0Aa%20user%20study%20with%20human%20participants%2C%20where%20the%20algorithm%20achieves%20high%0Aperformance%20in%20scenarios%20with%20high%20resource%20conflict%20due%20to%20aligned%20agent%0Agoals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11186v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequential%2520Resource%2520Trading%2520Using%2520Comparison-Based%2520Gradient%2520Estimation%26entry.906535625%3DSurya%2520Murthy%2520and%2520Mustafa%2520O.%2520Karabag%2520and%2520Ufuk%2520Topcu%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520interact%2520with%2520other%2520autonomous%2520agents%2520and%2520humans%2520of%2520unknown%250Apreferences%2520to%2520share%2520resources%2520in%2520their%2520environment.%2520We%2520explore%2520sequential%250Atrading%2520for%2520resource%2520allocation%2520in%2520a%2520setting%2520where%2520two%2520greedily%2520rational%2520agents%250Asequentially%2520trade%2520resources%2520from%2520a%2520finite%2520set%2520of%2520categories.%2520Each%2520agent%2520has%2520a%250Autility%2520function%2520that%2520depends%2520on%2520the%2520amount%2520of%2520resources%2520it%2520possesses%2520in%2520each%250Acategory.%2520The%2520offering%2520agent%2520makes%2520trade%2520offers%2520to%2520improve%2520its%2520utility%2520without%250Aknowing%2520the%2520responding%2520agent%2527s%2520utility%2520function%252C%2520and%2520the%2520responding%2520agent%2520only%250Aaccepts%2520offers%2520that%2520improve%2520its%2520utility.%2520To%2520facilitate%2520cooperation%2520between%2520an%250Aautonomous%2520agent%2520and%2520another%2520autonomous%2520agent%2520or%2520a%2520human%252C%2520we%2520present%2520an%250Aalgorithm%2520for%2520the%2520offering%2520agent%2520to%2520estimate%2520the%2520responding%2520agent%2527s%2520gradient%250A%2528preferences%2529%2520and%2520make%2520offers%2520based%2520on%2520previous%2520acceptance%2520or%2520rejection%250Aresponses.%2520The%2520algorithm%2527s%2520goal%2520is%2520to%2520reach%2520a%2520Pareto-optimal%2520resource%250Aallocation%2520state%2520while%2520ensuring%2520that%2520the%2520utilities%2520of%2520both%2520agents%2520improve%2520after%250Aevery%2520accepted%2520trade.%2520The%2520algorithm%2520estimates%2520the%2520responding%2520agent%2527s%2520gradient%250Aby%2520leveraging%2520the%2520rejected%2520offers%2520and%2520the%2520greedy%2520rationality%2520assumption%252C%2520to%250Aprune%2520the%2520space%2520of%2520potential%2520gradients.%2520We%2520show%2520that%252C%2520after%2520the%2520algorithm%2520makes%250Aa%2520finite%2520number%2520of%2520rejected%2520offers%252C%2520the%2520algorithm%2520either%2520finds%2520a%2520mutually%250Abeneficial%2520trade%2520or%2520certifies%2520that%2520the%2520current%2520state%2520is%2520epsilon-weakly%2520Pareto%250Aoptimal.%2520We%2520compare%2520the%2520proposed%2520algorithm%2520against%2520various%2520baselines%2520in%250Acontinuous%2520and%2520discrete%2520trading%2520scenarios%2520and%2520show%2520that%2520it%2520improves%2520the%250Asocietal%2520benefit%2520with%2520fewer%2520offers.%2520Additionally%252C%2520we%2520validate%2520these%2520findings%2520in%250Aa%2520user%2520study%2520with%2520human%2520participants%252C%2520where%2520the%2520algorithm%2520achieves%2520high%250Aperformance%2520in%2520scenarios%2520with%2520high%2520resource%2520conflict%2520due%2520to%2520aligned%2520agent%250Agoals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11186v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequential%20Resource%20Trading%20Using%20Comparison-Based%20Gradient%20Estimation&entry.906535625=Surya%20Murthy%20and%20Mustafa%20O.%20Karabag%20and%20Ufuk%20Topcu&entry.1292438233=%20%20Autonomous%20agents%20interact%20with%20other%20autonomous%20agents%20and%20humans%20of%20unknown%0Apreferences%20to%20share%20resources%20in%20their%20environment.%20We%20explore%20sequential%0Atrading%20for%20resource%20allocation%20in%20a%20setting%20where%20two%20greedily%20rational%20agents%0Asequentially%20trade%20resources%20from%20a%20finite%20set%20of%20categories.%20Each%20agent%20has%20a%0Autility%20function%20that%20depends%20on%20the%20amount%20of%20resources%20it%20possesses%20in%20each%0Acategory.%20The%20offering%20agent%20makes%20trade%20offers%20to%20improve%20its%20utility%20without%0Aknowing%20the%20responding%20agent%27s%20utility%20function%2C%20and%20the%20responding%20agent%20only%0Aaccepts%20offers%20that%20improve%20its%20utility.%20To%20facilitate%20cooperation%20between%20an%0Aautonomous%20agent%20and%20another%20autonomous%20agent%20or%20a%20human%2C%20we%20present%20an%0Aalgorithm%20for%20the%20offering%20agent%20to%20estimate%20the%20responding%20agent%27s%20gradient%0A%28preferences%29%20and%20make%20offers%20based%20on%20previous%20acceptance%20or%20rejection%0Aresponses.%20The%20algorithm%27s%20goal%20is%20to%20reach%20a%20Pareto-optimal%20resource%0Aallocation%20state%20while%20ensuring%20that%20the%20utilities%20of%20both%20agents%20improve%20after%0Aevery%20accepted%20trade.%20The%20algorithm%20estimates%20the%20responding%20agent%27s%20gradient%0Aby%20leveraging%20the%20rejected%20offers%20and%20the%20greedy%20rationality%20assumption%2C%20to%0Aprune%20the%20space%20of%20potential%20gradients.%20We%20show%20that%2C%20after%20the%20algorithm%20makes%0Aa%20finite%20number%20of%20rejected%20offers%2C%20the%20algorithm%20either%20finds%20a%20mutually%0Abeneficial%20trade%20or%20certifies%20that%20the%20current%20state%20is%20epsilon-weakly%20Pareto%0Aoptimal.%20We%20compare%20the%20proposed%20algorithm%20against%20various%20baselines%20in%0Acontinuous%20and%20discrete%20trading%20scenarios%20and%20show%20that%20it%20improves%20the%0Asocietal%20benefit%20with%20fewer%20offers.%20Additionally%2C%20we%20validate%20these%20findings%20in%0Aa%20user%20study%20with%20human%20participants%2C%20where%20the%20algorithm%20achieves%20high%0Aperformance%20in%20scenarios%20with%20high%20resource%20conflict%20due%20to%20aligned%20agent%0Agoals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11186v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


