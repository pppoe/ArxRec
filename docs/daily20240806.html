<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240805.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Interactive 3D Medical Image Segmentation with SAM 2", "author": "Chuyun Shen and Wenhao Li and Yuhang Shi and Xiangfeng Wang", "abstract": "  Interactive medical image segmentation (IMIS) has shown significant potential\nin enhancing segmentation accuracy by integrating iterative feedback from\nmedical professionals. However, the limited availability of enough 3D medical\ndata restricts the generalization and robustness of most IMIS methods. The\nSegment Anything Model (SAM), though effective for 2D images, requires\nexpensive semi-auto slice-by-slice annotations for 3D medical images. In this\npaper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta\nSAM model trained on videos, for 3D medical image segmentation. By treating\nsequential 2D slices of 3D images as video frames, SAM 2 can fully\nautomatically propagate annotations from a single frame to the entire 3D\nvolume. We propose a practical pipeline for using SAM 2 in 3D medical image\nsegmentation and present key findings highlighting its efficiency and potential\nfor further optimization. Concretely, numerical experiments on the BraTS2020\nand the medical segmentation decathlon datasets demonstrate that SAM 2 still\nhas a gap with supervised methods but can narrow the gap in specific settings\nand organ types, significantly reducing the annotation burden on medical\nprofessionals. Our code will be open-sourced and available at\nhttps://github.com/Chuyun-Shen/SAM_2_Medical_3D.\n", "link": "http://arxiv.org/abs/2408.02635v1", "date": "2024-08-05", "relevancy": 2.869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5943}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5635}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%203D%20Medical%20Image%20Segmentation%20with%20SAM%202&body=Title%3A%20Interactive%203D%20Medical%20Image%20Segmentation%20with%20SAM%202%0AAuthor%3A%20Chuyun%20Shen%20and%20Wenhao%20Li%20and%20Yuhang%20Shi%20and%20Xiangfeng%20Wang%0AAbstract%3A%20%20%20Interactive%20medical%20image%20segmentation%20%28IMIS%29%20has%20shown%20significant%20potential%0Ain%20enhancing%20segmentation%20accuracy%20by%20integrating%20iterative%20feedback%20from%0Amedical%20professionals.%20However%2C%20the%20limited%20availability%20of%20enough%203D%20medical%0Adata%20restricts%20the%20generalization%20and%20robustness%20of%20most%20IMIS%20methods.%20The%0ASegment%20Anything%20Model%20%28SAM%29%2C%20though%20effective%20for%202D%20images%2C%20requires%0Aexpensive%20semi-auto%20slice-by-slice%20annotations%20for%203D%20medical%20images.%20In%20this%0Apaper%2C%20we%20explore%20the%20zero-shot%20capabilities%20of%20SAM%202%2C%20the%20next-generation%20Meta%0ASAM%20model%20trained%20on%20videos%2C%20for%203D%20medical%20image%20segmentation.%20By%20treating%0Asequential%202D%20slices%20of%203D%20images%20as%20video%20frames%2C%20SAM%202%20can%20fully%0Aautomatically%20propagate%20annotations%20from%20a%20single%20frame%20to%20the%20entire%203D%0Avolume.%20We%20propose%20a%20practical%20pipeline%20for%20using%20SAM%202%20in%203D%20medical%20image%0Asegmentation%20and%20present%20key%20findings%20highlighting%20its%20efficiency%20and%20potential%0Afor%20further%20optimization.%20Concretely%2C%20numerical%20experiments%20on%20the%20BraTS2020%0Aand%20the%20medical%20segmentation%20decathlon%20datasets%20demonstrate%20that%20SAM%202%20still%0Ahas%20a%20gap%20with%20supervised%20methods%20but%20can%20narrow%20the%20gap%20in%20specific%20settings%0Aand%20organ%20types%2C%20significantly%20reducing%20the%20annotation%20burden%20on%20medical%0Aprofessionals.%20Our%20code%20will%20be%20open-sourced%20and%20available%20at%0Ahttps%3A//github.com/Chuyun-Shen/SAM_2_Medical_3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%25203D%2520Medical%2520Image%2520Segmentation%2520with%2520SAM%25202%26entry.906535625%3DChuyun%2520Shen%2520and%2520Wenhao%2520Li%2520and%2520Yuhang%2520Shi%2520and%2520Xiangfeng%2520Wang%26entry.1292438233%3D%2520%2520Interactive%2520medical%2520image%2520segmentation%2520%2528IMIS%2529%2520has%2520shown%2520significant%2520potential%250Ain%2520enhancing%2520segmentation%2520accuracy%2520by%2520integrating%2520iterative%2520feedback%2520from%250Amedical%2520professionals.%2520However%252C%2520the%2520limited%2520availability%2520of%2520enough%25203D%2520medical%250Adata%2520restricts%2520the%2520generalization%2520and%2520robustness%2520of%2520most%2520IMIS%2520methods.%2520The%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520though%2520effective%2520for%25202D%2520images%252C%2520requires%250Aexpensive%2520semi-auto%2520slice-by-slice%2520annotations%2520for%25203D%2520medical%2520images.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520the%2520zero-shot%2520capabilities%2520of%2520SAM%25202%252C%2520the%2520next-generation%2520Meta%250ASAM%2520model%2520trained%2520on%2520videos%252C%2520for%25203D%2520medical%2520image%2520segmentation.%2520By%2520treating%250Asequential%25202D%2520slices%2520of%25203D%2520images%2520as%2520video%2520frames%252C%2520SAM%25202%2520can%2520fully%250Aautomatically%2520propagate%2520annotations%2520from%2520a%2520single%2520frame%2520to%2520the%2520entire%25203D%250Avolume.%2520We%2520propose%2520a%2520practical%2520pipeline%2520for%2520using%2520SAM%25202%2520in%25203D%2520medical%2520image%250Asegmentation%2520and%2520present%2520key%2520findings%2520highlighting%2520its%2520efficiency%2520and%2520potential%250Afor%2520further%2520optimization.%2520Concretely%252C%2520numerical%2520experiments%2520on%2520the%2520BraTS2020%250Aand%2520the%2520medical%2520segmentation%2520decathlon%2520datasets%2520demonstrate%2520that%2520SAM%25202%2520still%250Ahas%2520a%2520gap%2520with%2520supervised%2520methods%2520but%2520can%2520narrow%2520the%2520gap%2520in%2520specific%2520settings%250Aand%2520organ%2520types%252C%2520significantly%2520reducing%2520the%2520annotation%2520burden%2520on%2520medical%250Aprofessionals.%2520Our%2520code%2520will%2520be%2520open-sourced%2520and%2520available%2520at%250Ahttps%253A//github.com/Chuyun-Shen/SAM_2_Medical_3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%203D%20Medical%20Image%20Segmentation%20with%20SAM%202&entry.906535625=Chuyun%20Shen%20and%20Wenhao%20Li%20and%20Yuhang%20Shi%20and%20Xiangfeng%20Wang&entry.1292438233=%20%20Interactive%20medical%20image%20segmentation%20%28IMIS%29%20has%20shown%20significant%20potential%0Ain%20enhancing%20segmentation%20accuracy%20by%20integrating%20iterative%20feedback%20from%0Amedical%20professionals.%20However%2C%20the%20limited%20availability%20of%20enough%203D%20medical%0Adata%20restricts%20the%20generalization%20and%20robustness%20of%20most%20IMIS%20methods.%20The%0ASegment%20Anything%20Model%20%28SAM%29%2C%20though%20effective%20for%202D%20images%2C%20requires%0Aexpensive%20semi-auto%20slice-by-slice%20annotations%20for%203D%20medical%20images.%20In%20this%0Apaper%2C%20we%20explore%20the%20zero-shot%20capabilities%20of%20SAM%202%2C%20the%20next-generation%20Meta%0ASAM%20model%20trained%20on%20videos%2C%20for%203D%20medical%20image%20segmentation.%20By%20treating%0Asequential%202D%20slices%20of%203D%20images%20as%20video%20frames%2C%20SAM%202%20can%20fully%0Aautomatically%20propagate%20annotations%20from%20a%20single%20frame%20to%20the%20entire%203D%0Avolume.%20We%20propose%20a%20practical%20pipeline%20for%20using%20SAM%202%20in%203D%20medical%20image%0Asegmentation%20and%20present%20key%20findings%20highlighting%20its%20efficiency%20and%20potential%0Afor%20further%20optimization.%20Concretely%2C%20numerical%20experiments%20on%20the%20BraTS2020%0Aand%20the%20medical%20segmentation%20decathlon%20datasets%20demonstrate%20that%20SAM%202%20still%0Ahas%20a%20gap%20with%20supervised%20methods%20but%20can%20narrow%20the%20gap%20in%20specific%20settings%0Aand%20organ%20types%2C%20significantly%20reducing%20the%20annotation%20burden%20on%20medical%0Aprofessionals.%20Our%20code%20will%20be%20open-sourced%20and%20available%20at%0Ahttps%3A//github.com/Chuyun-Shen/SAM_2_Medical_3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02635v1&entry.124074799=Read"},
{"title": "Latent-INR: A Flexible Framework for Implicit Representations of Videos\n  with Discriminative Semantics", "author": "Shishira R Maiya and Anubhav Gupta and Matthew Gwilliam and Max Ehrlich and Abhinav Shrivastava", "abstract": "  Implicit Neural Networks (INRs) have emerged as powerful representations to\nencode all forms of data, including images, videos, audios, and scenes. With\nvideo, many INRs for video have been proposed for the compression task, and\nrecent methods feature significant improvements with respect to encoding time,\nstorage, and reconstruction quality. However, these encoded representations\nlack semantic meaning, so they cannot be used for any downstream tasks that\nrequire such properties, such as retrieval. This can act as a barrier for\nadoption of video INRs over traditional codecs as they do not offer any\nsignificant edge apart from compression. To alleviate this, we propose a\nflexible framework that decouples the spatial and temporal aspects of the video\nINR. We accomplish this with a dictionary of per-frame latents that are learned\njointly with a set of video specific hypernetworks, such that given a latent,\nthese hypernetworks can predict the INR weights to reconstruct the given frame.\nThis framework not only retains the compression efficiency, but the learned\nlatents can be aligned with features from large vision models, which grants\nthem discriminative properties. We align these latents with CLIP and show good\nperformance for both compression and video retrieval tasks. By aligning with\nVideoLlama, we are able to perform open-ended chat with our learned latents as\nthe visual inputs. Additionally, the learned latents serve as a proxy for the\nunderlying weights, allowing us perform tasks like video interpolation. These\nsemantic properties and applications, existing simultaneously with ability to\nperform compression, interpolation, and superresolution properties, are a first\nin this field of work.\n", "link": "http://arxiv.org/abs/2408.02672v1", "date": "2024-08-05", "relevancy": 2.8084, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5684}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5609}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent-INR%3A%20A%20Flexible%20Framework%20for%20Implicit%20Representations%20of%20Videos%0A%20%20with%20Discriminative%20Semantics&body=Title%3A%20Latent-INR%3A%20A%20Flexible%20Framework%20for%20Implicit%20Representations%20of%20Videos%0A%20%20with%20Discriminative%20Semantics%0AAuthor%3A%20Shishira%20R%20Maiya%20and%20Anubhav%20Gupta%20and%20Matthew%20Gwilliam%20and%20Max%20Ehrlich%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Implicit%20Neural%20Networks%20%28INRs%29%20have%20emerged%20as%20powerful%20representations%20to%0Aencode%20all%20forms%20of%20data%2C%20including%20images%2C%20videos%2C%20audios%2C%20and%20scenes.%20With%0Avideo%2C%20many%20INRs%20for%20video%20have%20been%20proposed%20for%20the%20compression%20task%2C%20and%0Arecent%20methods%20feature%20significant%20improvements%20with%20respect%20to%20encoding%20time%2C%0Astorage%2C%20and%20reconstruction%20quality.%20However%2C%20these%20encoded%20representations%0Alack%20semantic%20meaning%2C%20so%20they%20cannot%20be%20used%20for%20any%20downstream%20tasks%20that%0Arequire%20such%20properties%2C%20such%20as%20retrieval.%20This%20can%20act%20as%20a%20barrier%20for%0Aadoption%20of%20video%20INRs%20over%20traditional%20codecs%20as%20they%20do%20not%20offer%20any%0Asignificant%20edge%20apart%20from%20compression.%20To%20alleviate%20this%2C%20we%20propose%20a%0Aflexible%20framework%20that%20decouples%20the%20spatial%20and%20temporal%20aspects%20of%20the%20video%0AINR.%20We%20accomplish%20this%20with%20a%20dictionary%20of%20per-frame%20latents%20that%20are%20learned%0Ajointly%20with%20a%20set%20of%20video%20specific%20hypernetworks%2C%20such%20that%20given%20a%20latent%2C%0Athese%20hypernetworks%20can%20predict%20the%20INR%20weights%20to%20reconstruct%20the%20given%20frame.%0AThis%20framework%20not%20only%20retains%20the%20compression%20efficiency%2C%20but%20the%20learned%0Alatents%20can%20be%20aligned%20with%20features%20from%20large%20vision%20models%2C%20which%20grants%0Athem%20discriminative%20properties.%20We%20align%20these%20latents%20with%20CLIP%20and%20show%20good%0Aperformance%20for%20both%20compression%20and%20video%20retrieval%20tasks.%20By%20aligning%20with%0AVideoLlama%2C%20we%20are%20able%20to%20perform%20open-ended%20chat%20with%20our%20learned%20latents%20as%0Athe%20visual%20inputs.%20Additionally%2C%20the%20learned%20latents%20serve%20as%20a%20proxy%20for%20the%0Aunderlying%20weights%2C%20allowing%20us%20perform%20tasks%20like%20video%20interpolation.%20These%0Asemantic%20properties%20and%20applications%2C%20existing%20simultaneously%20with%20ability%20to%0Aperform%20compression%2C%20interpolation%2C%20and%20superresolution%20properties%2C%20are%20a%20first%0Ain%20this%20field%20of%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent-INR%253A%2520A%2520Flexible%2520Framework%2520for%2520Implicit%2520Representations%2520of%2520Videos%250A%2520%2520with%2520Discriminative%2520Semantics%26entry.906535625%3DShishira%2520R%2520Maiya%2520and%2520Anubhav%2520Gupta%2520and%2520Matthew%2520Gwilliam%2520and%2520Max%2520Ehrlich%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Networks%2520%2528INRs%2529%2520have%2520emerged%2520as%2520powerful%2520representations%2520to%250Aencode%2520all%2520forms%2520of%2520data%252C%2520including%2520images%252C%2520videos%252C%2520audios%252C%2520and%2520scenes.%2520With%250Avideo%252C%2520many%2520INRs%2520for%2520video%2520have%2520been%2520proposed%2520for%2520the%2520compression%2520task%252C%2520and%250Arecent%2520methods%2520feature%2520significant%2520improvements%2520with%2520respect%2520to%2520encoding%2520time%252C%250Astorage%252C%2520and%2520reconstruction%2520quality.%2520However%252C%2520these%2520encoded%2520representations%250Alack%2520semantic%2520meaning%252C%2520so%2520they%2520cannot%2520be%2520used%2520for%2520any%2520downstream%2520tasks%2520that%250Arequire%2520such%2520properties%252C%2520such%2520as%2520retrieval.%2520This%2520can%2520act%2520as%2520a%2520barrier%2520for%250Aadoption%2520of%2520video%2520INRs%2520over%2520traditional%2520codecs%2520as%2520they%2520do%2520not%2520offer%2520any%250Asignificant%2520edge%2520apart%2520from%2520compression.%2520To%2520alleviate%2520this%252C%2520we%2520propose%2520a%250Aflexible%2520framework%2520that%2520decouples%2520the%2520spatial%2520and%2520temporal%2520aspects%2520of%2520the%2520video%250AINR.%2520We%2520accomplish%2520this%2520with%2520a%2520dictionary%2520of%2520per-frame%2520latents%2520that%2520are%2520learned%250Ajointly%2520with%2520a%2520set%2520of%2520video%2520specific%2520hypernetworks%252C%2520such%2520that%2520given%2520a%2520latent%252C%250Athese%2520hypernetworks%2520can%2520predict%2520the%2520INR%2520weights%2520to%2520reconstruct%2520the%2520given%2520frame.%250AThis%2520framework%2520not%2520only%2520retains%2520the%2520compression%2520efficiency%252C%2520but%2520the%2520learned%250Alatents%2520can%2520be%2520aligned%2520with%2520features%2520from%2520large%2520vision%2520models%252C%2520which%2520grants%250Athem%2520discriminative%2520properties.%2520We%2520align%2520these%2520latents%2520with%2520CLIP%2520and%2520show%2520good%250Aperformance%2520for%2520both%2520compression%2520and%2520video%2520retrieval%2520tasks.%2520By%2520aligning%2520with%250AVideoLlama%252C%2520we%2520are%2520able%2520to%2520perform%2520open-ended%2520chat%2520with%2520our%2520learned%2520latents%2520as%250Athe%2520visual%2520inputs.%2520Additionally%252C%2520the%2520learned%2520latents%2520serve%2520as%2520a%2520proxy%2520for%2520the%250Aunderlying%2520weights%252C%2520allowing%2520us%2520perform%2520tasks%2520like%2520video%2520interpolation.%2520These%250Asemantic%2520properties%2520and%2520applications%252C%2520existing%2520simultaneously%2520with%2520ability%2520to%250Aperform%2520compression%252C%2520interpolation%252C%2520and%2520superresolution%2520properties%252C%2520are%2520a%2520first%250Ain%2520this%2520field%2520of%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent-INR%3A%20A%20Flexible%20Framework%20for%20Implicit%20Representations%20of%20Videos%0A%20%20with%20Discriminative%20Semantics&entry.906535625=Shishira%20R%20Maiya%20and%20Anubhav%20Gupta%20and%20Matthew%20Gwilliam%20and%20Max%20Ehrlich%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Implicit%20Neural%20Networks%20%28INRs%29%20have%20emerged%20as%20powerful%20representations%20to%0Aencode%20all%20forms%20of%20data%2C%20including%20images%2C%20videos%2C%20audios%2C%20and%20scenes.%20With%0Avideo%2C%20many%20INRs%20for%20video%20have%20been%20proposed%20for%20the%20compression%20task%2C%20and%0Arecent%20methods%20feature%20significant%20improvements%20with%20respect%20to%20encoding%20time%2C%0Astorage%2C%20and%20reconstruction%20quality.%20However%2C%20these%20encoded%20representations%0Alack%20semantic%20meaning%2C%20so%20they%20cannot%20be%20used%20for%20any%20downstream%20tasks%20that%0Arequire%20such%20properties%2C%20such%20as%20retrieval.%20This%20can%20act%20as%20a%20barrier%20for%0Aadoption%20of%20video%20INRs%20over%20traditional%20codecs%20as%20they%20do%20not%20offer%20any%0Asignificant%20edge%20apart%20from%20compression.%20To%20alleviate%20this%2C%20we%20propose%20a%0Aflexible%20framework%20that%20decouples%20the%20spatial%20and%20temporal%20aspects%20of%20the%20video%0AINR.%20We%20accomplish%20this%20with%20a%20dictionary%20of%20per-frame%20latents%20that%20are%20learned%0Ajointly%20with%20a%20set%20of%20video%20specific%20hypernetworks%2C%20such%20that%20given%20a%20latent%2C%0Athese%20hypernetworks%20can%20predict%20the%20INR%20weights%20to%20reconstruct%20the%20given%20frame.%0AThis%20framework%20not%20only%20retains%20the%20compression%20efficiency%2C%20but%20the%20learned%0Alatents%20can%20be%20aligned%20with%20features%20from%20large%20vision%20models%2C%20which%20grants%0Athem%20discriminative%20properties.%20We%20align%20these%20latents%20with%20CLIP%20and%20show%20good%0Aperformance%20for%20both%20compression%20and%20video%20retrieval%20tasks.%20By%20aligning%20with%0AVideoLlama%2C%20we%20are%20able%20to%20perform%20open-ended%20chat%20with%20our%20learned%20latents%20as%0Athe%20visual%20inputs.%20Additionally%2C%20the%20learned%20latents%20serve%20as%20a%20proxy%20for%20the%0Aunderlying%20weights%2C%20allowing%20us%20perform%20tasks%20like%20video%20interpolation.%20These%0Asemantic%20properties%20and%20applications%2C%20existing%20simultaneously%20with%20ability%20to%0Aperform%20compression%2C%20interpolation%2C%20and%20superresolution%20properties%2C%20are%20a%20first%0Ain%20this%20field%20of%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02672v1&entry.124074799=Read"},
{"title": "MaFreeI2P: A Matching-Free Image-to-Point Cloud Registration Paradigm\n  with Active Camera Pose Retrieval", "author": "Gongxin Yao and Xinyang Li and Yixin Xuan and Yu Pan", "abstract": "  Image-to-point cloud registration seeks to estimate their relative camera\npose, which remains an open question due to the data modality gaps. The recent\nmatching-based methods tend to tackle this by building 2D-3D correspondences.\nIn this paper, we reveal the information loss inherent in these methods and\npropose a matching-free paradigm, named MaFreeI2P. Our key insight is to\nactively retrieve the camera pose in SE(3) space by contrasting the geometric\nfeatures between the point cloud and the query image. To achieve this, we first\nsample a set of candidate camera poses and construct their cost volume using\nthe cross-modal features. Superior to matching, cost volume can preserve more\ninformation and its feature similarity implicitly reflects the confidence level\nof the sampled poses. Afterwards, we employ a convolutional network to\nadaptively formulate a similarity assessment function, where the input cost\nvolume is further improved by filtering and pose-based weighting. Finally, we\nupdate the camera pose based on the similarity scores, and adopt a heuristic\nstrategy to iteratively shrink the pose sampling space for convergence. Our\nMaFreeI2P achieves a very competitive registration accuracy and recall on the\nKITTI-Odometry and Apollo-DaoxiangLake datasets.\n", "link": "http://arxiv.org/abs/2408.02392v1", "date": "2024-08-05", "relevancy": 2.8072, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5908}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaFreeI2P%3A%20A%20Matching-Free%20Image-to-Point%20Cloud%20Registration%20Paradigm%0A%20%20with%20Active%20Camera%20Pose%20Retrieval&body=Title%3A%20MaFreeI2P%3A%20A%20Matching-Free%20Image-to-Point%20Cloud%20Registration%20Paradigm%0A%20%20with%20Active%20Camera%20Pose%20Retrieval%0AAuthor%3A%20Gongxin%20Yao%20and%20Xinyang%20Li%20and%20Yixin%20Xuan%20and%20Yu%20Pan%0AAbstract%3A%20%20%20Image-to-point%20cloud%20registration%20seeks%20to%20estimate%20their%20relative%20camera%0Apose%2C%20which%20remains%20an%20open%20question%20due%20to%20the%20data%20modality%20gaps.%20The%20recent%0Amatching-based%20methods%20tend%20to%20tackle%20this%20by%20building%202D-3D%20correspondences.%0AIn%20this%20paper%2C%20we%20reveal%20the%20information%20loss%20inherent%20in%20these%20methods%20and%0Apropose%20a%20matching-free%20paradigm%2C%20named%20MaFreeI2P.%20Our%20key%20insight%20is%20to%0Aactively%20retrieve%20the%20camera%20pose%20in%20SE%283%29%20space%20by%20contrasting%20the%20geometric%0Afeatures%20between%20the%20point%20cloud%20and%20the%20query%20image.%20To%20achieve%20this%2C%20we%20first%0Asample%20a%20set%20of%20candidate%20camera%20poses%20and%20construct%20their%20cost%20volume%20using%0Athe%20cross-modal%20features.%20Superior%20to%20matching%2C%20cost%20volume%20can%20preserve%20more%0Ainformation%20and%20its%20feature%20similarity%20implicitly%20reflects%20the%20confidence%20level%0Aof%20the%20sampled%20poses.%20Afterwards%2C%20we%20employ%20a%20convolutional%20network%20to%0Aadaptively%20formulate%20a%20similarity%20assessment%20function%2C%20where%20the%20input%20cost%0Avolume%20is%20further%20improved%20by%20filtering%20and%20pose-based%20weighting.%20Finally%2C%20we%0Aupdate%20the%20camera%20pose%20based%20on%20the%20similarity%20scores%2C%20and%20adopt%20a%20heuristic%0Astrategy%20to%20iteratively%20shrink%20the%20pose%20sampling%20space%20for%20convergence.%20Our%0AMaFreeI2P%20achieves%20a%20very%20competitive%20registration%20accuracy%20and%20recall%20on%20the%0AKITTI-Odometry%20and%20Apollo-DaoxiangLake%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaFreeI2P%253A%2520A%2520Matching-Free%2520Image-to-Point%2520Cloud%2520Registration%2520Paradigm%250A%2520%2520with%2520Active%2520Camera%2520Pose%2520Retrieval%26entry.906535625%3DGongxin%2520Yao%2520and%2520Xinyang%2520Li%2520and%2520Yixin%2520Xuan%2520and%2520Yu%2520Pan%26entry.1292438233%3D%2520%2520Image-to-point%2520cloud%2520registration%2520seeks%2520to%2520estimate%2520their%2520relative%2520camera%250Apose%252C%2520which%2520remains%2520an%2520open%2520question%2520due%2520to%2520the%2520data%2520modality%2520gaps.%2520The%2520recent%250Amatching-based%2520methods%2520tend%2520to%2520tackle%2520this%2520by%2520building%25202D-3D%2520correspondences.%250AIn%2520this%2520paper%252C%2520we%2520reveal%2520the%2520information%2520loss%2520inherent%2520in%2520these%2520methods%2520and%250Apropose%2520a%2520matching-free%2520paradigm%252C%2520named%2520MaFreeI2P.%2520Our%2520key%2520insight%2520is%2520to%250Aactively%2520retrieve%2520the%2520camera%2520pose%2520in%2520SE%25283%2529%2520space%2520by%2520contrasting%2520the%2520geometric%250Afeatures%2520between%2520the%2520point%2520cloud%2520and%2520the%2520query%2520image.%2520To%2520achieve%2520this%252C%2520we%2520first%250Asample%2520a%2520set%2520of%2520candidate%2520camera%2520poses%2520and%2520construct%2520their%2520cost%2520volume%2520using%250Athe%2520cross-modal%2520features.%2520Superior%2520to%2520matching%252C%2520cost%2520volume%2520can%2520preserve%2520more%250Ainformation%2520and%2520its%2520feature%2520similarity%2520implicitly%2520reflects%2520the%2520confidence%2520level%250Aof%2520the%2520sampled%2520poses.%2520Afterwards%252C%2520we%2520employ%2520a%2520convolutional%2520network%2520to%250Aadaptively%2520formulate%2520a%2520similarity%2520assessment%2520function%252C%2520where%2520the%2520input%2520cost%250Avolume%2520is%2520further%2520improved%2520by%2520filtering%2520and%2520pose-based%2520weighting.%2520Finally%252C%2520we%250Aupdate%2520the%2520camera%2520pose%2520based%2520on%2520the%2520similarity%2520scores%252C%2520and%2520adopt%2520a%2520heuristic%250Astrategy%2520to%2520iteratively%2520shrink%2520the%2520pose%2520sampling%2520space%2520for%2520convergence.%2520Our%250AMaFreeI2P%2520achieves%2520a%2520very%2520competitive%2520registration%2520accuracy%2520and%2520recall%2520on%2520the%250AKITTI-Odometry%2520and%2520Apollo-DaoxiangLake%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaFreeI2P%3A%20A%20Matching-Free%20Image-to-Point%20Cloud%20Registration%20Paradigm%0A%20%20with%20Active%20Camera%20Pose%20Retrieval&entry.906535625=Gongxin%20Yao%20and%20Xinyang%20Li%20and%20Yixin%20Xuan%20and%20Yu%20Pan&entry.1292438233=%20%20Image-to-point%20cloud%20registration%20seeks%20to%20estimate%20their%20relative%20camera%0Apose%2C%20which%20remains%20an%20open%20question%20due%20to%20the%20data%20modality%20gaps.%20The%20recent%0Amatching-based%20methods%20tend%20to%20tackle%20this%20by%20building%202D-3D%20correspondences.%0AIn%20this%20paper%2C%20we%20reveal%20the%20information%20loss%20inherent%20in%20these%20methods%20and%0Apropose%20a%20matching-free%20paradigm%2C%20named%20MaFreeI2P.%20Our%20key%20insight%20is%20to%0Aactively%20retrieve%20the%20camera%20pose%20in%20SE%283%29%20space%20by%20contrasting%20the%20geometric%0Afeatures%20between%20the%20point%20cloud%20and%20the%20query%20image.%20To%20achieve%20this%2C%20we%20first%0Asample%20a%20set%20of%20candidate%20camera%20poses%20and%20construct%20their%20cost%20volume%20using%0Athe%20cross-modal%20features.%20Superior%20to%20matching%2C%20cost%20volume%20can%20preserve%20more%0Ainformation%20and%20its%20feature%20similarity%20implicitly%20reflects%20the%20confidence%20level%0Aof%20the%20sampled%20poses.%20Afterwards%2C%20we%20employ%20a%20convolutional%20network%20to%0Aadaptively%20formulate%20a%20similarity%20assessment%20function%2C%20where%20the%20input%20cost%0Avolume%20is%20further%20improved%20by%20filtering%20and%20pose-based%20weighting.%20Finally%2C%20we%0Aupdate%20the%20camera%20pose%20based%20on%20the%20similarity%20scores%2C%20and%20adopt%20a%20heuristic%0Astrategy%20to%20iteratively%20shrink%20the%20pose%20sampling%20space%20for%20convergence.%20Our%0AMaFreeI2P%20achieves%20a%20very%20competitive%20registration%20accuracy%20and%20recall%20on%20the%0AKITTI-Odometry%20and%20Apollo-DaoxiangLake%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02392v1&entry.124074799=Read"},
{"title": "Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial\n  Images", "author": "Yash Dixit and Naman Srivastava and Joel D Joy and Rohan Olikara and Swarup E and Rakshit Ramesh", "abstract": "  Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning and is one of the key elements in developing smart and sustainable\ncities. This study introduces a semi-supervised segmentation model for LULC\nprediction using high-resolution satellite images with a huge diversity in data\ndistributions in different areas from the country of India. Our approach\nensures a robust generalization across different types of buildings, roads,\ntrees, and water bodies within these distinct areas. We propose a modified\nCross Pseudo Supervision framework to train image segmentation models on\nsparsely labelled data. The proposed framework addresses the limitations of the\npopular \"Cross Pseudo Supervision\" technique for semi-supervised learning.\nSpecifically, it tackles the challenges of training segmentation models on\nnoisy satellite image data with sparse and inaccurate labels. This\ncomprehensive approach enhances the accuracy and utility of LULC mapping for\nvarious urban planning applications.\n", "link": "http://arxiv.org/abs/2408.02382v1", "date": "2024-08-05", "relevancy": 2.7862, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5943}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.573}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross%20Psuedo%20Supervision%20Framework%20for%20Sparsely%20Labelled%20Geo-spatial%0A%20%20Images&body=Title%3A%20Cross%20Psuedo%20Supervision%20Framework%20for%20Sparsely%20Labelled%20Geo-spatial%0A%20%20Images%0AAuthor%3A%20Yash%20Dixit%20and%20Naman%20Srivastava%20and%20Joel%20D%20Joy%20and%20Rohan%20Olikara%20and%20Swarup%20E%20and%20Rakshit%20Ramesh%0AAbstract%3A%20%20%20Land%20Use%20Land%20Cover%20%28LULC%29%20mapping%20is%20essential%20for%20urban%20and%20resource%0Aplanning%20and%20is%20one%20of%20the%20key%20elements%20in%20developing%20smart%20and%20sustainable%0Acities.%20This%20study%20introduces%20a%20semi-supervised%20segmentation%20model%20for%20LULC%0Aprediction%20using%20high-resolution%20satellite%20images%20with%20a%20huge%20diversity%20in%20data%0Adistributions%20in%20different%20areas%20from%20the%20country%20of%20India.%20Our%20approach%0Aensures%20a%20robust%20generalization%20across%20different%20types%20of%20buildings%2C%20roads%2C%0Atrees%2C%20and%20water%20bodies%20within%20these%20distinct%20areas.%20We%20propose%20a%20modified%0ACross%20Pseudo%20Supervision%20framework%20to%20train%20image%20segmentation%20models%20on%0Asparsely%20labelled%20data.%20The%20proposed%20framework%20addresses%20the%20limitations%20of%20the%0Apopular%20%22Cross%20Pseudo%20Supervision%22%20technique%20for%20semi-supervised%20learning.%0ASpecifically%2C%20it%20tackles%20the%20challenges%20of%20training%20segmentation%20models%20on%0Anoisy%20satellite%20image%20data%20with%20sparse%20and%20inaccurate%20labels.%20This%0Acomprehensive%20approach%20enhances%20the%20accuracy%20and%20utility%20of%20LULC%20mapping%20for%0Avarious%20urban%20planning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross%2520Psuedo%2520Supervision%2520Framework%2520for%2520Sparsely%2520Labelled%2520Geo-spatial%250A%2520%2520Images%26entry.906535625%3DYash%2520Dixit%2520and%2520Naman%2520Srivastava%2520and%2520Joel%2520D%2520Joy%2520and%2520Rohan%2520Olikara%2520and%2520Swarup%2520E%2520and%2520Rakshit%2520Ramesh%26entry.1292438233%3D%2520%2520Land%2520Use%2520Land%2520Cover%2520%2528LULC%2529%2520mapping%2520is%2520essential%2520for%2520urban%2520and%2520resource%250Aplanning%2520and%2520is%2520one%2520of%2520the%2520key%2520elements%2520in%2520developing%2520smart%2520and%2520sustainable%250Acities.%2520This%2520study%2520introduces%2520a%2520semi-supervised%2520segmentation%2520model%2520for%2520LULC%250Aprediction%2520using%2520high-resolution%2520satellite%2520images%2520with%2520a%2520huge%2520diversity%2520in%2520data%250Adistributions%2520in%2520different%2520areas%2520from%2520the%2520country%2520of%2520India.%2520Our%2520approach%250Aensures%2520a%2520robust%2520generalization%2520across%2520different%2520types%2520of%2520buildings%252C%2520roads%252C%250Atrees%252C%2520and%2520water%2520bodies%2520within%2520these%2520distinct%2520areas.%2520We%2520propose%2520a%2520modified%250ACross%2520Pseudo%2520Supervision%2520framework%2520to%2520train%2520image%2520segmentation%2520models%2520on%250Asparsely%2520labelled%2520data.%2520The%2520proposed%2520framework%2520addresses%2520the%2520limitations%2520of%2520the%250Apopular%2520%2522Cross%2520Pseudo%2520Supervision%2522%2520technique%2520for%2520semi-supervised%2520learning.%250ASpecifically%252C%2520it%2520tackles%2520the%2520challenges%2520of%2520training%2520segmentation%2520models%2520on%250Anoisy%2520satellite%2520image%2520data%2520with%2520sparse%2520and%2520inaccurate%2520labels.%2520This%250Acomprehensive%2520approach%2520enhances%2520the%2520accuracy%2520and%2520utility%2520of%2520LULC%2520mapping%2520for%250Avarious%2520urban%2520planning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross%20Psuedo%20Supervision%20Framework%20for%20Sparsely%20Labelled%20Geo-spatial%0A%20%20Images&entry.906535625=Yash%20Dixit%20and%20Naman%20Srivastava%20and%20Joel%20D%20Joy%20and%20Rohan%20Olikara%20and%20Swarup%20E%20and%20Rakshit%20Ramesh&entry.1292438233=%20%20Land%20Use%20Land%20Cover%20%28LULC%29%20mapping%20is%20essential%20for%20urban%20and%20resource%0Aplanning%20and%20is%20one%20of%20the%20key%20elements%20in%20developing%20smart%20and%20sustainable%0Acities.%20This%20study%20introduces%20a%20semi-supervised%20segmentation%20model%20for%20LULC%0Aprediction%20using%20high-resolution%20satellite%20images%20with%20a%20huge%20diversity%20in%20data%0Adistributions%20in%20different%20areas%20from%20the%20country%20of%20India.%20Our%20approach%0Aensures%20a%20robust%20generalization%20across%20different%20types%20of%20buildings%2C%20roads%2C%0Atrees%2C%20and%20water%20bodies%20within%20these%20distinct%20areas.%20We%20propose%20a%20modified%0ACross%20Pseudo%20Supervision%20framework%20to%20train%20image%20segmentation%20models%20on%0Asparsely%20labelled%20data.%20The%20proposed%20framework%20addresses%20the%20limitations%20of%20the%0Apopular%20%22Cross%20Pseudo%20Supervision%22%20technique%20for%20semi-supervised%20learning.%0ASpecifically%2C%20it%20tackles%20the%20challenges%20of%20training%20segmentation%20models%20on%0Anoisy%20satellite%20image%20data%20with%20sparse%20and%20inaccurate%20labels.%20This%0Acomprehensive%20approach%20enhances%20the%20accuracy%20and%20utility%20of%20LULC%20mapping%20for%0Avarious%20urban%20planning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02382v1&entry.124074799=Read"},
{"title": "StoDIP: Efficient 3D MRF image reconstruction with deep image priors and\n  stochastic iterations", "author": "Perla Mayo and Matteo Cencini and Carolin M. Pirkl and Marion I. Menzel and Michela Tosetti and Bjoern H. Menze and Mohammad Golbabaee", "abstract": "  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI for multiparametric tissue mapping. The reconstruction of\nquantitative maps requires tailored algorithms for removing aliasing artefacts\nfrom the compressed sampled MRF acquisitions. Within approaches found in the\nliterature, many focus solely on two-dimensional (2D) image reconstruction,\nneglecting the extension to volumetric (3D) scans despite their higher\nrelevance and clinical value. A reason for this is that transitioning to 3D\nimaging without appropriate mitigations presents significant challenges,\nincluding increased computational cost and storage requirements, and the need\nfor large amount of ground-truth (artefact-free) data for training. To address\nthese issues, we introduce StoDIP, a new algorithm that extends the\nground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.\nStoDIP employs memory-efficient stochastic updates across the multicoil MRF\ndata, a carefully selected neural network architecture, as well as faster\nnonuniform FFT (NUFFT) transformations. This enables a faster convergence\ncompared against a conventional DIP implementation without these features.\nTested on a dataset of whole-brain scans from healthy volunteers, StoDIP\ndemonstrated superior performance over the ground-truth-free reconstruction\nbaselines, both quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2408.02367v1", "date": "2024-08-05", "relevancy": 2.7643, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5614}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoDIP%3A%20Efficient%203D%20MRF%20image%20reconstruction%20with%20deep%20image%20priors%20and%0A%20%20stochastic%20iterations&body=Title%3A%20StoDIP%3A%20Efficient%203D%20MRF%20image%20reconstruction%20with%20deep%20image%20priors%20and%0A%20%20stochastic%20iterations%0AAuthor%3A%20Perla%20Mayo%20and%20Matteo%20Cencini%20and%20Carolin%20M.%20Pirkl%20and%20Marion%20I.%20Menzel%20and%20Michela%20Tosetti%20and%20Bjoern%20H.%20Menze%20and%20Mohammad%20Golbabaee%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Fingerprinting%20%28MRF%29%20is%20a%20time-efficient%20approach%20to%0Aquantitative%20MRI%20for%20multiparametric%20tissue%20mapping.%20The%20reconstruction%20of%0Aquantitative%20maps%20requires%20tailored%20algorithms%20for%20removing%20aliasing%20artefacts%0Afrom%20the%20compressed%20sampled%20MRF%20acquisitions.%20Within%20approaches%20found%20in%20the%0Aliterature%2C%20many%20focus%20solely%20on%20two-dimensional%20%282D%29%20image%20reconstruction%2C%0Aneglecting%20the%20extension%20to%20volumetric%20%283D%29%20scans%20despite%20their%20higher%0Arelevance%20and%20clinical%20value.%20A%20reason%20for%20this%20is%20that%20transitioning%20to%203D%0Aimaging%20without%20appropriate%20mitigations%20presents%20significant%20challenges%2C%0Aincluding%20increased%20computational%20cost%20and%20storage%20requirements%2C%20and%20the%20need%0Afor%20large%20amount%20of%20ground-truth%20%28artefact-free%29%20data%20for%20training.%20To%20address%0Athese%20issues%2C%20we%20introduce%20StoDIP%2C%20a%20new%20algorithm%20that%20extends%20the%0Aground-truth-free%20Deep%20Image%20Prior%20%28DIP%29%20reconstruction%20to%203D%20MRF%20imaging.%0AStoDIP%20employs%20memory-efficient%20stochastic%20updates%20across%20the%20multicoil%20MRF%0Adata%2C%20a%20carefully%20selected%20neural%20network%20architecture%2C%20as%20well%20as%20faster%0Anonuniform%20FFT%20%28NUFFT%29%20transformations.%20This%20enables%20a%20faster%20convergence%0Acompared%20against%20a%20conventional%20DIP%20implementation%20without%20these%20features.%0ATested%20on%20a%20dataset%20of%20whole-brain%20scans%20from%20healthy%20volunteers%2C%20StoDIP%0Ademonstrated%20superior%20performance%20over%20the%20ground-truth-free%20reconstruction%0Abaselines%2C%20both%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoDIP%253A%2520Efficient%25203D%2520MRF%2520image%2520reconstruction%2520with%2520deep%2520image%2520priors%2520and%250A%2520%2520stochastic%2520iterations%26entry.906535625%3DPerla%2520Mayo%2520and%2520Matteo%2520Cencini%2520and%2520Carolin%2520M.%2520Pirkl%2520and%2520Marion%2520I.%2520Menzel%2520and%2520Michela%2520Tosetti%2520and%2520Bjoern%2520H.%2520Menze%2520and%2520Mohammad%2520Golbabaee%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Fingerprinting%2520%2528MRF%2529%2520is%2520a%2520time-efficient%2520approach%2520to%250Aquantitative%2520MRI%2520for%2520multiparametric%2520tissue%2520mapping.%2520The%2520reconstruction%2520of%250Aquantitative%2520maps%2520requires%2520tailored%2520algorithms%2520for%2520removing%2520aliasing%2520artefacts%250Afrom%2520the%2520compressed%2520sampled%2520MRF%2520acquisitions.%2520Within%2520approaches%2520found%2520in%2520the%250Aliterature%252C%2520many%2520focus%2520solely%2520on%2520two-dimensional%2520%25282D%2529%2520image%2520reconstruction%252C%250Aneglecting%2520the%2520extension%2520to%2520volumetric%2520%25283D%2529%2520scans%2520despite%2520their%2520higher%250Arelevance%2520and%2520clinical%2520value.%2520A%2520reason%2520for%2520this%2520is%2520that%2520transitioning%2520to%25203D%250Aimaging%2520without%2520appropriate%2520mitigations%2520presents%2520significant%2520challenges%252C%250Aincluding%2520increased%2520computational%2520cost%2520and%2520storage%2520requirements%252C%2520and%2520the%2520need%250Afor%2520large%2520amount%2520of%2520ground-truth%2520%2528artefact-free%2529%2520data%2520for%2520training.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520StoDIP%252C%2520a%2520new%2520algorithm%2520that%2520extends%2520the%250Aground-truth-free%2520Deep%2520Image%2520Prior%2520%2528DIP%2529%2520reconstruction%2520to%25203D%2520MRF%2520imaging.%250AStoDIP%2520employs%2520memory-efficient%2520stochastic%2520updates%2520across%2520the%2520multicoil%2520MRF%250Adata%252C%2520a%2520carefully%2520selected%2520neural%2520network%2520architecture%252C%2520as%2520well%2520as%2520faster%250Anonuniform%2520FFT%2520%2528NUFFT%2529%2520transformations.%2520This%2520enables%2520a%2520faster%2520convergence%250Acompared%2520against%2520a%2520conventional%2520DIP%2520implementation%2520without%2520these%2520features.%250ATested%2520on%2520a%2520dataset%2520of%2520whole-brain%2520scans%2520from%2520healthy%2520volunteers%252C%2520StoDIP%250Ademonstrated%2520superior%2520performance%2520over%2520the%2520ground-truth-free%2520reconstruction%250Abaselines%252C%2520both%2520quantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoDIP%3A%20Efficient%203D%20MRF%20image%20reconstruction%20with%20deep%20image%20priors%20and%0A%20%20stochastic%20iterations&entry.906535625=Perla%20Mayo%20and%20Matteo%20Cencini%20and%20Carolin%20M.%20Pirkl%20and%20Marion%20I.%20Menzel%20and%20Michela%20Tosetti%20and%20Bjoern%20H.%20Menze%20and%20Mohammad%20Golbabaee&entry.1292438233=%20%20Magnetic%20Resonance%20Fingerprinting%20%28MRF%29%20is%20a%20time-efficient%20approach%20to%0Aquantitative%20MRI%20for%20multiparametric%20tissue%20mapping.%20The%20reconstruction%20of%0Aquantitative%20maps%20requires%20tailored%20algorithms%20for%20removing%20aliasing%20artefacts%0Afrom%20the%20compressed%20sampled%20MRF%20acquisitions.%20Within%20approaches%20found%20in%20the%0Aliterature%2C%20many%20focus%20solely%20on%20two-dimensional%20%282D%29%20image%20reconstruction%2C%0Aneglecting%20the%20extension%20to%20volumetric%20%283D%29%20scans%20despite%20their%20higher%0Arelevance%20and%20clinical%20value.%20A%20reason%20for%20this%20is%20that%20transitioning%20to%203D%0Aimaging%20without%20appropriate%20mitigations%20presents%20significant%20challenges%2C%0Aincluding%20increased%20computational%20cost%20and%20storage%20requirements%2C%20and%20the%20need%0Afor%20large%20amount%20of%20ground-truth%20%28artefact-free%29%20data%20for%20training.%20To%20address%0Athese%20issues%2C%20we%20introduce%20StoDIP%2C%20a%20new%20algorithm%20that%20extends%20the%0Aground-truth-free%20Deep%20Image%20Prior%20%28DIP%29%20reconstruction%20to%203D%20MRF%20imaging.%0AStoDIP%20employs%20memory-efficient%20stochastic%20updates%20across%20the%20multicoil%20MRF%0Adata%2C%20a%20carefully%20selected%20neural%20network%20architecture%2C%20as%20well%20as%20faster%0Anonuniform%20FFT%20%28NUFFT%29%20transformations.%20This%20enables%20a%20faster%20convergence%0Acompared%20against%20a%20conventional%20DIP%20implementation%20without%20these%20features.%0ATested%20on%20a%20dataset%20of%20whole-brain%20scans%20from%20healthy%20volunteers%2C%20StoDIP%0Ademonstrated%20superior%20performance%20over%20the%20ground-truth-free%20reconstruction%0Abaselines%2C%20both%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02367v1&entry.124074799=Read"},
{"title": "Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data", "author": "Tim Elsner and Paula Usinger and Victor Czech and Gregor Kobsik and Yanjiang He and Isaak Lim and Leif Kobbelt", "abstract": "  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n", "link": "http://arxiv.org/abs/2407.11913v2", "date": "2024-08-05", "relevancy": 2.756, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5809}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.548}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantised%20Global%20Autoencoder%3A%20A%20Holistic%20Approach%20to%20Representing%20Visual%0A%20%20Data&body=Title%3A%20Quantised%20Global%20Autoencoder%3A%20A%20Holistic%20Approach%20to%20Representing%20Visual%0A%20%20Data%0AAuthor%3A%20Tim%20Elsner%20and%20Paula%20Usinger%20and%20Victor%20Czech%20and%20Gregor%20Kobsik%20and%20Yanjiang%20He%20and%20Isaak%20Lim%20and%20Leif%20Kobbelt%0AAbstract%3A%20%20%20In%20quantised%20autoencoders%2C%20images%20are%20usually%20split%20into%20local%20patches%2C%20each%0Aencoded%20by%20one%20token.%20This%20representation%20is%20redundant%20in%20the%20sense%20that%20the%0Asame%20number%20of%20tokens%20is%20spend%20per%20region%2C%20regardless%20of%20the%20visual%20information%0Acontent%20in%20that%20region.%20Adaptive%20discretisation%20schemes%20like%20quadtrees%20are%0Aapplied%20to%20allocate%20tokens%20for%20patches%20with%20varying%20sizes%2C%20but%20this%20just%20varies%0Athe%20region%20of%20influence%20for%20a%20token%20which%20nevertheless%20remains%20a%20local%0Adescriptor.%20Modern%20architectures%20add%20an%20attention%20mechanism%20to%20the%20autoencoder%0Awhich%20infuses%20some%20degree%20of%20global%20information%20into%20the%20local%20tokens.%20Despite%0Athe%20global%20context%2C%20tokens%20are%20still%20associated%20with%20a%20local%20image%20region.%20In%0Acontrast%2C%20our%20method%20is%20inspired%20by%20spectral%20decompositions%20which%20transform%20an%0Ainput%20signal%20into%20a%20superposition%20of%20global%20frequencies.%20Taking%20the%20data-driven%0Aperspective%2C%20we%20learn%20custom%20basis%20functions%20corresponding%20to%20the%20codebook%0Aentries%20in%20our%20VQ-VAE%20setup.%20Furthermore%2C%20a%20decoder%20combines%20these%20basis%0Afunctions%20in%20a%20non-linear%20fashion%2C%20going%20beyond%20the%20simple%20linear%20superposition%0Aof%20spectral%20decompositions.%20We%20can%20achieve%20this%20global%20description%20with%20an%0Aefficient%20transpose%20operation%20between%20features%20and%20channels%20and%20demonstrate%20our%0Aperformance%20on%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantised%2520Global%2520Autoencoder%253A%2520A%2520Holistic%2520Approach%2520to%2520Representing%2520Visual%250A%2520%2520Data%26entry.906535625%3DTim%2520Elsner%2520and%2520Paula%2520Usinger%2520and%2520Victor%2520Czech%2520and%2520Gregor%2520Kobsik%2520and%2520Yanjiang%2520He%2520and%2520Isaak%2520Lim%2520and%2520Leif%2520Kobbelt%26entry.1292438233%3D%2520%2520In%2520quantised%2520autoencoders%252C%2520images%2520are%2520usually%2520split%2520into%2520local%2520patches%252C%2520each%250Aencoded%2520by%2520one%2520token.%2520This%2520representation%2520is%2520redundant%2520in%2520the%2520sense%2520that%2520the%250Asame%2520number%2520of%2520tokens%2520is%2520spend%2520per%2520region%252C%2520regardless%2520of%2520the%2520visual%2520information%250Acontent%2520in%2520that%2520region.%2520Adaptive%2520discretisation%2520schemes%2520like%2520quadtrees%2520are%250Aapplied%2520to%2520allocate%2520tokens%2520for%2520patches%2520with%2520varying%2520sizes%252C%2520but%2520this%2520just%2520varies%250Athe%2520region%2520of%2520influence%2520for%2520a%2520token%2520which%2520nevertheless%2520remains%2520a%2520local%250Adescriptor.%2520Modern%2520architectures%2520add%2520an%2520attention%2520mechanism%2520to%2520the%2520autoencoder%250Awhich%2520infuses%2520some%2520degree%2520of%2520global%2520information%2520into%2520the%2520local%2520tokens.%2520Despite%250Athe%2520global%2520context%252C%2520tokens%2520are%2520still%2520associated%2520with%2520a%2520local%2520image%2520region.%2520In%250Acontrast%252C%2520our%2520method%2520is%2520inspired%2520by%2520spectral%2520decompositions%2520which%2520transform%2520an%250Ainput%2520signal%2520into%2520a%2520superposition%2520of%2520global%2520frequencies.%2520Taking%2520the%2520data-driven%250Aperspective%252C%2520we%2520learn%2520custom%2520basis%2520functions%2520corresponding%2520to%2520the%2520codebook%250Aentries%2520in%2520our%2520VQ-VAE%2520setup.%2520Furthermore%252C%2520a%2520decoder%2520combines%2520these%2520basis%250Afunctions%2520in%2520a%2520non-linear%2520fashion%252C%2520going%2520beyond%2520the%2520simple%2520linear%2520superposition%250Aof%2520spectral%2520decompositions.%2520We%2520can%2520achieve%2520this%2520global%2520description%2520with%2520an%250Aefficient%2520transpose%2520operation%2520between%2520features%2520and%2520channels%2520and%2520demonstrate%2520our%250Aperformance%2520on%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantised%20Global%20Autoencoder%3A%20A%20Holistic%20Approach%20to%20Representing%20Visual%0A%20%20Data&entry.906535625=Tim%20Elsner%20and%20Paula%20Usinger%20and%20Victor%20Czech%20and%20Gregor%20Kobsik%20and%20Yanjiang%20He%20and%20Isaak%20Lim%20and%20Leif%20Kobbelt&entry.1292438233=%20%20In%20quantised%20autoencoders%2C%20images%20are%20usually%20split%20into%20local%20patches%2C%20each%0Aencoded%20by%20one%20token.%20This%20representation%20is%20redundant%20in%20the%20sense%20that%20the%0Asame%20number%20of%20tokens%20is%20spend%20per%20region%2C%20regardless%20of%20the%20visual%20information%0Acontent%20in%20that%20region.%20Adaptive%20discretisation%20schemes%20like%20quadtrees%20are%0Aapplied%20to%20allocate%20tokens%20for%20patches%20with%20varying%20sizes%2C%20but%20this%20just%20varies%0Athe%20region%20of%20influence%20for%20a%20token%20which%20nevertheless%20remains%20a%20local%0Adescriptor.%20Modern%20architectures%20add%20an%20attention%20mechanism%20to%20the%20autoencoder%0Awhich%20infuses%20some%20degree%20of%20global%20information%20into%20the%20local%20tokens.%20Despite%0Athe%20global%20context%2C%20tokens%20are%20still%20associated%20with%20a%20local%20image%20region.%20In%0Acontrast%2C%20our%20method%20is%20inspired%20by%20spectral%20decompositions%20which%20transform%20an%0Ainput%20signal%20into%20a%20superposition%20of%20global%20frequencies.%20Taking%20the%20data-driven%0Aperspective%2C%20we%20learn%20custom%20basis%20functions%20corresponding%20to%20the%20codebook%0Aentries%20in%20our%20VQ-VAE%20setup.%20Furthermore%2C%20a%20decoder%20combines%20these%20basis%0Afunctions%20in%20a%20non-linear%20fashion%2C%20going%20beyond%20the%20simple%20linear%20superposition%0Aof%20spectral%20decompositions.%20We%20can%20achieve%20this%20global%20description%20with%20an%0Aefficient%20transpose%20operation%20between%20features%20and%20channels%20and%20demonstrate%20our%0Aperformance%20on%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11913v2&entry.124074799=Read"},
{"title": "UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with\n  Ultrasound Reflection Direction Parameterization", "author": "Ziwen Guo and Zi Fang and Zhuang Fu", "abstract": "  Three-dimensional ultrasound imaging is a critical technology widely used in\nmedical diagnostics. However, traditional 3D ultrasound imaging methods have\nlimitations such as fixed resolution, low storage efficiency, and insufficient\ncontextual connectivity, leading to poor performance in handling complex\nartifacts and reflection characteristics. Recently, techniques based on NeRF\n(Neural Radiance Fields) have made significant progress in view synthesis and\n3D reconstruction, but there remains a research gap in high-quality ultrasound\nimaging. To address these issues, we propose a new model, UlRe-NeRF, which\ncombines implicit neural networks and explicit ultrasound volume rendering into\nan ultrasound neural rendering architecture. This model incorporates reflection\ndirection parameterization and harmonic encoding, using a directional MLP\nmodule to generate view-dependent high-frequency reflection intensity\nestimates, and a spatial MLP module to produce the medium's physical property\nparameters. These parameters are used in the volume rendering process to\naccurately reproduce the propagation and reflection behavior of ultrasound\nwaves in the medium. Experimental results demonstrate that the UlRe-NeRF model\nsignificantly enhances the realism and accuracy of high-fidelity ultrasound\nimage reconstruction, especially in handling complex medium structures.\n", "link": "http://arxiv.org/abs/2408.00860v2", "date": "2024-08-05", "relevancy": 2.7029, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5416}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UlRe-NeRF%3A%203D%20Ultrasound%20Imaging%20through%20Neural%20Rendering%20with%0A%20%20Ultrasound%20Reflection%20Direction%20Parameterization&body=Title%3A%20UlRe-NeRF%3A%203D%20Ultrasound%20Imaging%20through%20Neural%20Rendering%20with%0A%20%20Ultrasound%20Reflection%20Direction%20Parameterization%0AAuthor%3A%20Ziwen%20Guo%20and%20Zi%20Fang%20and%20Zhuang%20Fu%0AAbstract%3A%20%20%20Three-dimensional%20ultrasound%20imaging%20is%20a%20critical%20technology%20widely%20used%20in%0Amedical%20diagnostics.%20However%2C%20traditional%203D%20ultrasound%20imaging%20methods%20have%0Alimitations%20such%20as%20fixed%20resolution%2C%20low%20storage%20efficiency%2C%20and%20insufficient%0Acontextual%20connectivity%2C%20leading%20to%20poor%20performance%20in%20handling%20complex%0Aartifacts%20and%20reflection%20characteristics.%20Recently%2C%20techniques%20based%20on%20NeRF%0A%28Neural%20Radiance%20Fields%29%20have%20made%20significant%20progress%20in%20view%20synthesis%20and%0A3D%20reconstruction%2C%20but%20there%20remains%20a%20research%20gap%20in%20high-quality%20ultrasound%0Aimaging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20model%2C%20UlRe-NeRF%2C%20which%0Acombines%20implicit%20neural%20networks%20and%20explicit%20ultrasound%20volume%20rendering%20into%0Aan%20ultrasound%20neural%20rendering%20architecture.%20This%20model%20incorporates%20reflection%0Adirection%20parameterization%20and%20harmonic%20encoding%2C%20using%20a%20directional%20MLP%0Amodule%20to%20generate%20view-dependent%20high-frequency%20reflection%20intensity%0Aestimates%2C%20and%20a%20spatial%20MLP%20module%20to%20produce%20the%20medium%27s%20physical%20property%0Aparameters.%20These%20parameters%20are%20used%20in%20the%20volume%20rendering%20process%20to%0Aaccurately%20reproduce%20the%20propagation%20and%20reflection%20behavior%20of%20ultrasound%0Awaves%20in%20the%20medium.%20Experimental%20results%20demonstrate%20that%20the%20UlRe-NeRF%20model%0Asignificantly%20enhances%20the%20realism%20and%20accuracy%20of%20high-fidelity%20ultrasound%0Aimage%20reconstruction%2C%20especially%20in%20handling%20complex%20medium%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUlRe-NeRF%253A%25203D%2520Ultrasound%2520Imaging%2520through%2520Neural%2520Rendering%2520with%250A%2520%2520Ultrasound%2520Reflection%2520Direction%2520Parameterization%26entry.906535625%3DZiwen%2520Guo%2520and%2520Zi%2520Fang%2520and%2520Zhuang%2520Fu%26entry.1292438233%3D%2520%2520Three-dimensional%2520ultrasound%2520imaging%2520is%2520a%2520critical%2520technology%2520widely%2520used%2520in%250Amedical%2520diagnostics.%2520However%252C%2520traditional%25203D%2520ultrasound%2520imaging%2520methods%2520have%250Alimitations%2520such%2520as%2520fixed%2520resolution%252C%2520low%2520storage%2520efficiency%252C%2520and%2520insufficient%250Acontextual%2520connectivity%252C%2520leading%2520to%2520poor%2520performance%2520in%2520handling%2520complex%250Aartifacts%2520and%2520reflection%2520characteristics.%2520Recently%252C%2520techniques%2520based%2520on%2520NeRF%250A%2528Neural%2520Radiance%2520Fields%2529%2520have%2520made%2520significant%2520progress%2520in%2520view%2520synthesis%2520and%250A3D%2520reconstruction%252C%2520but%2520there%2520remains%2520a%2520research%2520gap%2520in%2520high-quality%2520ultrasound%250Aimaging.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520new%2520model%252C%2520UlRe-NeRF%252C%2520which%250Acombines%2520implicit%2520neural%2520networks%2520and%2520explicit%2520ultrasound%2520volume%2520rendering%2520into%250Aan%2520ultrasound%2520neural%2520rendering%2520architecture.%2520This%2520model%2520incorporates%2520reflection%250Adirection%2520parameterization%2520and%2520harmonic%2520encoding%252C%2520using%2520a%2520directional%2520MLP%250Amodule%2520to%2520generate%2520view-dependent%2520high-frequency%2520reflection%2520intensity%250Aestimates%252C%2520and%2520a%2520spatial%2520MLP%2520module%2520to%2520produce%2520the%2520medium%2527s%2520physical%2520property%250Aparameters.%2520These%2520parameters%2520are%2520used%2520in%2520the%2520volume%2520rendering%2520process%2520to%250Aaccurately%2520reproduce%2520the%2520propagation%2520and%2520reflection%2520behavior%2520of%2520ultrasound%250Awaves%2520in%2520the%2520medium.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520UlRe-NeRF%2520model%250Asignificantly%2520enhances%2520the%2520realism%2520and%2520accuracy%2520of%2520high-fidelity%2520ultrasound%250Aimage%2520reconstruction%252C%2520especially%2520in%2520handling%2520complex%2520medium%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UlRe-NeRF%3A%203D%20Ultrasound%20Imaging%20through%20Neural%20Rendering%20with%0A%20%20Ultrasound%20Reflection%20Direction%20Parameterization&entry.906535625=Ziwen%20Guo%20and%20Zi%20Fang%20and%20Zhuang%20Fu&entry.1292438233=%20%20Three-dimensional%20ultrasound%20imaging%20is%20a%20critical%20technology%20widely%20used%20in%0Amedical%20diagnostics.%20However%2C%20traditional%203D%20ultrasound%20imaging%20methods%20have%0Alimitations%20such%20as%20fixed%20resolution%2C%20low%20storage%20efficiency%2C%20and%20insufficient%0Acontextual%20connectivity%2C%20leading%20to%20poor%20performance%20in%20handling%20complex%0Aartifacts%20and%20reflection%20characteristics.%20Recently%2C%20techniques%20based%20on%20NeRF%0A%28Neural%20Radiance%20Fields%29%20have%20made%20significant%20progress%20in%20view%20synthesis%20and%0A3D%20reconstruction%2C%20but%20there%20remains%20a%20research%20gap%20in%20high-quality%20ultrasound%0Aimaging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20model%2C%20UlRe-NeRF%2C%20which%0Acombines%20implicit%20neural%20networks%20and%20explicit%20ultrasound%20volume%20rendering%20into%0Aan%20ultrasound%20neural%20rendering%20architecture.%20This%20model%20incorporates%20reflection%0Adirection%20parameterization%20and%20harmonic%20encoding%2C%20using%20a%20directional%20MLP%0Amodule%20to%20generate%20view-dependent%20high-frequency%20reflection%20intensity%0Aestimates%2C%20and%20a%20spatial%20MLP%20module%20to%20produce%20the%20medium%27s%20physical%20property%0Aparameters.%20These%20parameters%20are%20used%20in%20the%20volume%20rendering%20process%20to%0Aaccurately%20reproduce%20the%20propagation%20and%20reflection%20behavior%20of%20ultrasound%0Awaves%20in%20the%20medium.%20Experimental%20results%20demonstrate%20that%20the%20UlRe-NeRF%20model%0Asignificantly%20enhances%20the%20realism%20and%20accuracy%20of%20high-fidelity%20ultrasound%0Aimage%20reconstruction%2C%20especially%20in%20handling%20complex%20medium%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00860v2&entry.124074799=Read"},
{"title": "HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions", "author": "Chiranjeev Chiranjeev and Muskan Dosi and Kartik Thakral and Mayank Vatsa and Richa Singh", "abstract": "  Traditional deep learning models rely on methods such as softmax\ncross-entropy and ArcFace loss for tasks like classification and face\nrecognition. These methods mainly explore angular features in a hyperspherical\nspace, often resulting in entangled inter-class features due to dense angular\ndata across many classes. In this paper, a new field of feature exploration is\nproposed known as HyperSpaceX which enhances class discrimination by exploring\nboth angular and radial dimensions in multi-hyperspherical spaces, facilitated\nby a novel DistArc loss. The proposed DistArc loss encompasses three feature\narrangement components: two angular and one radial, enforcing intra-class\nbinding and inter-class separation in multi-radial arrangement, improving\nfeature discriminability. Evaluation of HyperSpaceX framework for the novel\nrepresentation utilizes a proposed predictive measure that accounts for both\nangular and radial elements, providing a more comprehensive assessment of model\naccuracy beyond standard metrics. Experiments across seven object\nclassification and six face recognition datasets demonstrate state-of-the-art\n(SoTA) results obtained from HyperSpaceX, achieving up to a 20% performance\nimprovement on large-scale object datasets in lower dimensions and up to 6%\ngain in higher dimensions.\n", "link": "http://arxiv.org/abs/2408.02494v1", "date": "2024-08-05", "relevancy": 2.6774, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5539}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5389}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperSpaceX%3A%20Radial%20and%20Angular%20Exploration%20of%20HyperSpherical%20Dimensions&body=Title%3A%20HyperSpaceX%3A%20Radial%20and%20Angular%20Exploration%20of%20HyperSpherical%20Dimensions%0AAuthor%3A%20Chiranjeev%20Chiranjeev%20and%20Muskan%20Dosi%20and%20Kartik%20Thakral%20and%20Mayank%20Vatsa%20and%20Richa%20Singh%0AAbstract%3A%20%20%20Traditional%20deep%20learning%20models%20rely%20on%20methods%20such%20as%20softmax%0Across-entropy%20and%20ArcFace%20loss%20for%20tasks%20like%20classification%20and%20face%0Arecognition.%20These%20methods%20mainly%20explore%20angular%20features%20in%20a%20hyperspherical%0Aspace%2C%20often%20resulting%20in%20entangled%20inter-class%20features%20due%20to%20dense%20angular%0Adata%20across%20many%20classes.%20In%20this%20paper%2C%20a%20new%20field%20of%20feature%20exploration%20is%0Aproposed%20known%20as%20HyperSpaceX%20which%20enhances%20class%20discrimination%20by%20exploring%0Aboth%20angular%20and%20radial%20dimensions%20in%20multi-hyperspherical%20spaces%2C%20facilitated%0Aby%20a%20novel%20DistArc%20loss.%20The%20proposed%20DistArc%20loss%20encompasses%20three%20feature%0Aarrangement%20components%3A%20two%20angular%20and%20one%20radial%2C%20enforcing%20intra-class%0Abinding%20and%20inter-class%20separation%20in%20multi-radial%20arrangement%2C%20improving%0Afeature%20discriminability.%20Evaluation%20of%20HyperSpaceX%20framework%20for%20the%20novel%0Arepresentation%20utilizes%20a%20proposed%20predictive%20measure%20that%20accounts%20for%20both%0Aangular%20and%20radial%20elements%2C%20providing%20a%20more%20comprehensive%20assessment%20of%20model%0Aaccuracy%20beyond%20standard%20metrics.%20Experiments%20across%20seven%20object%0Aclassification%20and%20six%20face%20recognition%20datasets%20demonstrate%20state-of-the-art%0A%28SoTA%29%20results%20obtained%20from%20HyperSpaceX%2C%20achieving%20up%20to%20a%2020%25%20performance%0Aimprovement%20on%20large-scale%20object%20datasets%20in%20lower%20dimensions%20and%20up%20to%206%25%0Again%20in%20higher%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperSpaceX%253A%2520Radial%2520and%2520Angular%2520Exploration%2520of%2520HyperSpherical%2520Dimensions%26entry.906535625%3DChiranjeev%2520Chiranjeev%2520and%2520Muskan%2520Dosi%2520and%2520Kartik%2520Thakral%2520and%2520Mayank%2520Vatsa%2520and%2520Richa%2520Singh%26entry.1292438233%3D%2520%2520Traditional%2520deep%2520learning%2520models%2520rely%2520on%2520methods%2520such%2520as%2520softmax%250Across-entropy%2520and%2520ArcFace%2520loss%2520for%2520tasks%2520like%2520classification%2520and%2520face%250Arecognition.%2520These%2520methods%2520mainly%2520explore%2520angular%2520features%2520in%2520a%2520hyperspherical%250Aspace%252C%2520often%2520resulting%2520in%2520entangled%2520inter-class%2520features%2520due%2520to%2520dense%2520angular%250Adata%2520across%2520many%2520classes.%2520In%2520this%2520paper%252C%2520a%2520new%2520field%2520of%2520feature%2520exploration%2520is%250Aproposed%2520known%2520as%2520HyperSpaceX%2520which%2520enhances%2520class%2520discrimination%2520by%2520exploring%250Aboth%2520angular%2520and%2520radial%2520dimensions%2520in%2520multi-hyperspherical%2520spaces%252C%2520facilitated%250Aby%2520a%2520novel%2520DistArc%2520loss.%2520The%2520proposed%2520DistArc%2520loss%2520encompasses%2520three%2520feature%250Aarrangement%2520components%253A%2520two%2520angular%2520and%2520one%2520radial%252C%2520enforcing%2520intra-class%250Abinding%2520and%2520inter-class%2520separation%2520in%2520multi-radial%2520arrangement%252C%2520improving%250Afeature%2520discriminability.%2520Evaluation%2520of%2520HyperSpaceX%2520framework%2520for%2520the%2520novel%250Arepresentation%2520utilizes%2520a%2520proposed%2520predictive%2520measure%2520that%2520accounts%2520for%2520both%250Aangular%2520and%2520radial%2520elements%252C%2520providing%2520a%2520more%2520comprehensive%2520assessment%2520of%2520model%250Aaccuracy%2520beyond%2520standard%2520metrics.%2520Experiments%2520across%2520seven%2520object%250Aclassification%2520and%2520six%2520face%2520recognition%2520datasets%2520demonstrate%2520state-of-the-art%250A%2528SoTA%2529%2520results%2520obtained%2520from%2520HyperSpaceX%252C%2520achieving%2520up%2520to%2520a%252020%2525%2520performance%250Aimprovement%2520on%2520large-scale%2520object%2520datasets%2520in%2520lower%2520dimensions%2520and%2520up%2520to%25206%2525%250Again%2520in%2520higher%2520dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperSpaceX%3A%20Radial%20and%20Angular%20Exploration%20of%20HyperSpherical%20Dimensions&entry.906535625=Chiranjeev%20Chiranjeev%20and%20Muskan%20Dosi%20and%20Kartik%20Thakral%20and%20Mayank%20Vatsa%20and%20Richa%20Singh&entry.1292438233=%20%20Traditional%20deep%20learning%20models%20rely%20on%20methods%20such%20as%20softmax%0Across-entropy%20and%20ArcFace%20loss%20for%20tasks%20like%20classification%20and%20face%0Arecognition.%20These%20methods%20mainly%20explore%20angular%20features%20in%20a%20hyperspherical%0Aspace%2C%20often%20resulting%20in%20entangled%20inter-class%20features%20due%20to%20dense%20angular%0Adata%20across%20many%20classes.%20In%20this%20paper%2C%20a%20new%20field%20of%20feature%20exploration%20is%0Aproposed%20known%20as%20HyperSpaceX%20which%20enhances%20class%20discrimination%20by%20exploring%0Aboth%20angular%20and%20radial%20dimensions%20in%20multi-hyperspherical%20spaces%2C%20facilitated%0Aby%20a%20novel%20DistArc%20loss.%20The%20proposed%20DistArc%20loss%20encompasses%20three%20feature%0Aarrangement%20components%3A%20two%20angular%20and%20one%20radial%2C%20enforcing%20intra-class%0Abinding%20and%20inter-class%20separation%20in%20multi-radial%20arrangement%2C%20improving%0Afeature%20discriminability.%20Evaluation%20of%20HyperSpaceX%20framework%20for%20the%20novel%0Arepresentation%20utilizes%20a%20proposed%20predictive%20measure%20that%20accounts%20for%20both%0Aangular%20and%20radial%20elements%2C%20providing%20a%20more%20comprehensive%20assessment%20of%20model%0Aaccuracy%20beyond%20standard%20metrics.%20Experiments%20across%20seven%20object%0Aclassification%20and%20six%20face%20recognition%20datasets%20demonstrate%20state-of-the-art%0A%28SoTA%29%20results%20obtained%20from%20HyperSpaceX%2C%20achieving%20up%20to%20a%2020%25%20performance%0Aimprovement%20on%20large-scale%20object%20datasets%20in%20lower%20dimensions%20and%20up%20to%206%25%0Again%20in%20higher%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02494v1&entry.124074799=Read"},
{"title": "MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh\n  Tokenization", "author": "Yiwen Chen and Yikai Wang and Yihao Luo and Zhengyi Wang and Zilong Chen and Jun Zhu and Chi Zhang and Guosheng Lin", "abstract": "  We introduce MeshAnything V2, an autoregressive transformer that generates\nArtist-Created Meshes (AM) aligned to given shapes. It can be integrated with\nvarious 3D asset production pipelines to achieve high-quality, highly\ncontrollable AM generation. MeshAnything V2 surpasses previous methods in both\nefficiency and performance using models of the same size. These improvements\nare due to our newly proposed mesh tokenization method: Adjacent Mesh\nTokenization (AMT). Different from previous methods that represent each face\nwith three vertices, AMT uses a single vertex whenever possible. Compared to\nprevious methods, AMT requires about half the token sequence length to\nrepresent the same mesh in average. Furthermore, the token sequences from AMT\nare more compact and well-structured, fundamentally benefiting AM generation.\nOur extensive experiments show that AMT significantly improves the efficiency\nand performance of AM generation. Project Page:\nhttps://buaacyw.github.io/meshanything-v2/\n", "link": "http://arxiv.org/abs/2408.02555v1", "date": "2024-08-05", "relevancy": 2.6646, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshAnything%20V2%3A%20Artist-Created%20Mesh%20Generation%20With%20Adjacent%20Mesh%0A%20%20Tokenization&body=Title%3A%20MeshAnything%20V2%3A%20Artist-Created%20Mesh%20Generation%20With%20Adjacent%20Mesh%0A%20%20Tokenization%0AAuthor%3A%20Yiwen%20Chen%20and%20Yikai%20Wang%20and%20Yihao%20Luo%20and%20Zhengyi%20Wang%20and%20Zilong%20Chen%20and%20Jun%20Zhu%20and%20Chi%20Zhang%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20We%20introduce%20MeshAnything%20V2%2C%20an%20autoregressive%20transformer%20that%20generates%0AArtist-Created%20Meshes%20%28AM%29%20aligned%20to%20given%20shapes.%20It%20can%20be%20integrated%20with%0Avarious%203D%20asset%20production%20pipelines%20to%20achieve%20high-quality%2C%20highly%0Acontrollable%20AM%20generation.%20MeshAnything%20V2%20surpasses%20previous%20methods%20in%20both%0Aefficiency%20and%20performance%20using%20models%20of%20the%20same%20size.%20These%20improvements%0Aare%20due%20to%20our%20newly%20proposed%20mesh%20tokenization%20method%3A%20Adjacent%20Mesh%0ATokenization%20%28AMT%29.%20Different%20from%20previous%20methods%20that%20represent%20each%20face%0Awith%20three%20vertices%2C%20AMT%20uses%20a%20single%20vertex%20whenever%20possible.%20Compared%20to%0Aprevious%20methods%2C%20AMT%20requires%20about%20half%20the%20token%20sequence%20length%20to%0Arepresent%20the%20same%20mesh%20in%20average.%20Furthermore%2C%20the%20token%20sequences%20from%20AMT%0Aare%20more%20compact%20and%20well-structured%2C%20fundamentally%20benefiting%20AM%20generation.%0AOur%20extensive%20experiments%20show%20that%20AMT%20significantly%20improves%20the%20efficiency%0Aand%20performance%20of%20AM%20generation.%20Project%20Page%3A%0Ahttps%3A//buaacyw.github.io/meshanything-v2/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshAnything%2520V2%253A%2520Artist-Created%2520Mesh%2520Generation%2520With%2520Adjacent%2520Mesh%250A%2520%2520Tokenization%26entry.906535625%3DYiwen%2520Chen%2520and%2520Yikai%2520Wang%2520and%2520Yihao%2520Luo%2520and%2520Zhengyi%2520Wang%2520and%2520Zilong%2520Chen%2520and%2520Jun%2520Zhu%2520and%2520Chi%2520Zhang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520We%2520introduce%2520MeshAnything%2520V2%252C%2520an%2520autoregressive%2520transformer%2520that%2520generates%250AArtist-Created%2520Meshes%2520%2528AM%2529%2520aligned%2520to%2520given%2520shapes.%2520It%2520can%2520be%2520integrated%2520with%250Avarious%25203D%2520asset%2520production%2520pipelines%2520to%2520achieve%2520high-quality%252C%2520highly%250Acontrollable%2520AM%2520generation.%2520MeshAnything%2520V2%2520surpasses%2520previous%2520methods%2520in%2520both%250Aefficiency%2520and%2520performance%2520using%2520models%2520of%2520the%2520same%2520size.%2520These%2520improvements%250Aare%2520due%2520to%2520our%2520newly%2520proposed%2520mesh%2520tokenization%2520method%253A%2520Adjacent%2520Mesh%250ATokenization%2520%2528AMT%2529.%2520Different%2520from%2520previous%2520methods%2520that%2520represent%2520each%2520face%250Awith%2520three%2520vertices%252C%2520AMT%2520uses%2520a%2520single%2520vertex%2520whenever%2520possible.%2520Compared%2520to%250Aprevious%2520methods%252C%2520AMT%2520requires%2520about%2520half%2520the%2520token%2520sequence%2520length%2520to%250Arepresent%2520the%2520same%2520mesh%2520in%2520average.%2520Furthermore%252C%2520the%2520token%2520sequences%2520from%2520AMT%250Aare%2520more%2520compact%2520and%2520well-structured%252C%2520fundamentally%2520benefiting%2520AM%2520generation.%250AOur%2520extensive%2520experiments%2520show%2520that%2520AMT%2520significantly%2520improves%2520the%2520efficiency%250Aand%2520performance%2520of%2520AM%2520generation.%2520Project%2520Page%253A%250Ahttps%253A//buaacyw.github.io/meshanything-v2/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshAnything%20V2%3A%20Artist-Created%20Mesh%20Generation%20With%20Adjacent%20Mesh%0A%20%20Tokenization&entry.906535625=Yiwen%20Chen%20and%20Yikai%20Wang%20and%20Yihao%20Luo%20and%20Zhengyi%20Wang%20and%20Zilong%20Chen%20and%20Jun%20Zhu%20and%20Chi%20Zhang%20and%20Guosheng%20Lin&entry.1292438233=%20%20We%20introduce%20MeshAnything%20V2%2C%20an%20autoregressive%20transformer%20that%20generates%0AArtist-Created%20Meshes%20%28AM%29%20aligned%20to%20given%20shapes.%20It%20can%20be%20integrated%20with%0Avarious%203D%20asset%20production%20pipelines%20to%20achieve%20high-quality%2C%20highly%0Acontrollable%20AM%20generation.%20MeshAnything%20V2%20surpasses%20previous%20methods%20in%20both%0Aefficiency%20and%20performance%20using%20models%20of%20the%20same%20size.%20These%20improvements%0Aare%20due%20to%20our%20newly%20proposed%20mesh%20tokenization%20method%3A%20Adjacent%20Mesh%0ATokenization%20%28AMT%29.%20Different%20from%20previous%20methods%20that%20represent%20each%20face%0Awith%20three%20vertices%2C%20AMT%20uses%20a%20single%20vertex%20whenever%20possible.%20Compared%20to%0Aprevious%20methods%2C%20AMT%20requires%20about%20half%20the%20token%20sequence%20length%20to%0Arepresent%20the%20same%20mesh%20in%20average.%20Furthermore%2C%20the%20token%20sequences%20from%20AMT%0Aare%20more%20compact%20and%20well-structured%2C%20fundamentally%20benefiting%20AM%20generation.%0AOur%20extensive%20experiments%20show%20that%20AMT%20significantly%20improves%20the%20efficiency%0Aand%20performance%20of%20AM%20generation.%20Project%20Page%3A%0Ahttps%3A//buaacyw.github.io/meshanything-v2/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02555v1&entry.124074799=Read"},
{"title": "FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes", "author": "Charles Gaydon and Michel Daab and Floryne Roche", "abstract": "  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n", "link": "http://arxiv.org/abs/2405.04634v3", "date": "2024-08-05", "relevancy": 2.6162, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5335}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRACTAL%3A%20An%20Ultra-Large-Scale%20Aerial%20Lidar%20Dataset%20for%203D%20Semantic%0A%20%20Segmentation%20of%20Diverse%20Landscapes&body=Title%3A%20FRACTAL%3A%20An%20Ultra-Large-Scale%20Aerial%20Lidar%20Dataset%20for%203D%20Semantic%0A%20%20Segmentation%20of%20Diverse%20Landscapes%0AAuthor%3A%20Charles%20Gaydon%20and%20Michel%20Daab%20and%20Floryne%20Roche%0AAbstract%3A%20%20%20Mapping%20agencies%20are%20increasingly%20adopting%20Aerial%20Lidar%20Scanning%20%28ALS%29%20as%20a%0Anew%20tool%20to%20monitor%20territory%20and%20support%20public%20policies.%20Processing%20ALS%20data%0Aat%20scale%20requires%20efficient%20point%20classification%20methods%20that%20perform%20well%20over%0Ahighly%20diverse%20territories.%20To%20evaluate%20them%2C%20researchers%20need%20large%20annotated%0ALidar%20datasets%2C%20however%2C%20current%20Lidar%20benchmark%20datasets%20have%20restricted%20scope%0Aand%20often%20cover%20a%20single%20urban%20area.%20To%20bridge%20this%20data%20gap%2C%20we%20present%20the%0AFRench%20ALS%20Clouds%20from%20TArgeted%20Landscapes%20%28FRACTAL%29%20dataset%3A%20an%0Aultra-large-scale%20aerial%20Lidar%20dataset%20made%20of%20100%2C000%20dense%20point%20clouds%20with%0Ahigh-quality%20labels%20for%207%20semantic%20classes%20and%20spanning%20250%20km%24%5E2%24.%20FRACTAL%20is%0Abuilt%20upon%20France%27s%20nationwide%20open%20Lidar%20data.%20It%20achieves%20spatial%20and%0Asemantic%20diversity%20via%20a%20sampling%20scheme%20that%20explicitly%20concentrates%20rare%0Aclasses%20and%20challenging%20landscapes%20from%20five%20French%20regions.%20It%20should%20support%0Athe%20development%20of%203D%20deep%20learning%20approaches%20for%20large-scale%20land%20monitoring.%0AWe%20describe%20the%20nature%20of%20the%20source%20data%2C%20the%20sampling%20workflow%2C%20the%20content%0Aof%20the%20resulting%20dataset%2C%20and%20provide%20an%20initial%20evaluation%20of%20segmentation%0Aperformance%20using%20a%20performant%203D%20neural%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04634v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRACTAL%253A%2520An%2520Ultra-Large-Scale%2520Aerial%2520Lidar%2520Dataset%2520for%25203D%2520Semantic%250A%2520%2520Segmentation%2520of%2520Diverse%2520Landscapes%26entry.906535625%3DCharles%2520Gaydon%2520and%2520Michel%2520Daab%2520and%2520Floryne%2520Roche%26entry.1292438233%3D%2520%2520Mapping%2520agencies%2520are%2520increasingly%2520adopting%2520Aerial%2520Lidar%2520Scanning%2520%2528ALS%2529%2520as%2520a%250Anew%2520tool%2520to%2520monitor%2520territory%2520and%2520support%2520public%2520policies.%2520Processing%2520ALS%2520data%250Aat%2520scale%2520requires%2520efficient%2520point%2520classification%2520methods%2520that%2520perform%2520well%2520over%250Ahighly%2520diverse%2520territories.%2520To%2520evaluate%2520them%252C%2520researchers%2520need%2520large%2520annotated%250ALidar%2520datasets%252C%2520however%252C%2520current%2520Lidar%2520benchmark%2520datasets%2520have%2520restricted%2520scope%250Aand%2520often%2520cover%2520a%2520single%2520urban%2520area.%2520To%2520bridge%2520this%2520data%2520gap%252C%2520we%2520present%2520the%250AFRench%2520ALS%2520Clouds%2520from%2520TArgeted%2520Landscapes%2520%2528FRACTAL%2529%2520dataset%253A%2520an%250Aultra-large-scale%2520aerial%2520Lidar%2520dataset%2520made%2520of%2520100%252C000%2520dense%2520point%2520clouds%2520with%250Ahigh-quality%2520labels%2520for%25207%2520semantic%2520classes%2520and%2520spanning%2520250%2520km%2524%255E2%2524.%2520FRACTAL%2520is%250Abuilt%2520upon%2520France%2527s%2520nationwide%2520open%2520Lidar%2520data.%2520It%2520achieves%2520spatial%2520and%250Asemantic%2520diversity%2520via%2520a%2520sampling%2520scheme%2520that%2520explicitly%2520concentrates%2520rare%250Aclasses%2520and%2520challenging%2520landscapes%2520from%2520five%2520French%2520regions.%2520It%2520should%2520support%250Athe%2520development%2520of%25203D%2520deep%2520learning%2520approaches%2520for%2520large-scale%2520land%2520monitoring.%250AWe%2520describe%2520the%2520nature%2520of%2520the%2520source%2520data%252C%2520the%2520sampling%2520workflow%252C%2520the%2520content%250Aof%2520the%2520resulting%2520dataset%252C%2520and%2520provide%2520an%2520initial%2520evaluation%2520of%2520segmentation%250Aperformance%2520using%2520a%2520performant%25203D%2520neural%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04634v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRACTAL%3A%20An%20Ultra-Large-Scale%20Aerial%20Lidar%20Dataset%20for%203D%20Semantic%0A%20%20Segmentation%20of%20Diverse%20Landscapes&entry.906535625=Charles%20Gaydon%20and%20Michel%20Daab%20and%20Floryne%20Roche&entry.1292438233=%20%20Mapping%20agencies%20are%20increasingly%20adopting%20Aerial%20Lidar%20Scanning%20%28ALS%29%20as%20a%0Anew%20tool%20to%20monitor%20territory%20and%20support%20public%20policies.%20Processing%20ALS%20data%0Aat%20scale%20requires%20efficient%20point%20classification%20methods%20that%20perform%20well%20over%0Ahighly%20diverse%20territories.%20To%20evaluate%20them%2C%20researchers%20need%20large%20annotated%0ALidar%20datasets%2C%20however%2C%20current%20Lidar%20benchmark%20datasets%20have%20restricted%20scope%0Aand%20often%20cover%20a%20single%20urban%20area.%20To%20bridge%20this%20data%20gap%2C%20we%20present%20the%0AFRench%20ALS%20Clouds%20from%20TArgeted%20Landscapes%20%28FRACTAL%29%20dataset%3A%20an%0Aultra-large-scale%20aerial%20Lidar%20dataset%20made%20of%20100%2C000%20dense%20point%20clouds%20with%0Ahigh-quality%20labels%20for%207%20semantic%20classes%20and%20spanning%20250%20km%24%5E2%24.%20FRACTAL%20is%0Abuilt%20upon%20France%27s%20nationwide%20open%20Lidar%20data.%20It%20achieves%20spatial%20and%0Asemantic%20diversity%20via%20a%20sampling%20scheme%20that%20explicitly%20concentrates%20rare%0Aclasses%20and%20challenging%20landscapes%20from%20five%20French%20regions.%20It%20should%20support%0Athe%20development%20of%203D%20deep%20learning%20approaches%20for%20large-scale%20land%20monitoring.%0AWe%20describe%20the%20nature%20of%20the%20source%20data%2C%20the%20sampling%20workflow%2C%20the%20content%0Aof%20the%20resulting%20dataset%2C%20and%20provide%20an%20initial%20evaluation%20of%20segmentation%0Aperformance%20using%20a%20performant%203D%20neural%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04634v3&entry.124074799=Read"},
{"title": "Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model", "author": "Longrong Yang and Dong Shen and Chaoxiang Cai and Fan Yang and Size Li and Di Zhang and Xi Li", "abstract": "  The Mixture-of-Experts (MoE) has gained increasing attention in studying\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthey usually employ a router to predict the routing of each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization directions of tokens. This may lead to severe optimization\ninterference between different tokens assigned to an expert. To address this\nproblem, this paper proposes a novel method based on token-level gradient\nanalysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first\nuse token-level gradients to identify conflicting tokens in experts. After\nthat, we add a specialized loss tailored to eliminate conflicts among tokens\nwithin each expert. Our method can serve as a plug-in for diverse Large\nVision-Language Models, and extensive experimental results demonstrate its\neffectiveness. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n", "link": "http://arxiv.org/abs/2406.19905v2", "date": "2024-08-05", "relevancy": 2.5594, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5321}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5117}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Token%20Gradient%20Conflict%20in%20Mixture-of-Experts%20for%20Large%0A%20%20Vision-Language%20Model&body=Title%3A%20Solving%20Token%20Gradient%20Conflict%20in%20Mixture-of-Experts%20for%20Large%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Longrong%20Yang%20and%20Dong%20Shen%20and%20Chaoxiang%20Cai%20and%20Fan%20Yang%20and%20Size%20Li%20and%20Di%20Zhang%20and%20Xi%20Li%0AAbstract%3A%20%20%20The%20Mixture-of-Experts%20%28MoE%29%20has%20gained%20increasing%20attention%20in%20studying%0ALarge%20Vision-Language%20Models%20%28LVLMs%29.%20It%20uses%20a%20sparse%20model%20to%20replace%20the%0Adense%20model%2C%20achieving%20comparable%20performance%20while%20activating%20fewer%20parameters%0Aduring%20inference%2C%20thus%20significantly%20reducing%20the%20inference%20cost.%20Existing%20MoE%0Amethods%20in%20LVLMs%20encourage%20different%20experts%20to%20handle%20different%20tokens%2C%20and%0Athey%20usually%20employ%20a%20router%20to%20predict%20the%20routing%20of%20each%20token.%20However%2C%20the%0Apredictions%20are%20based%20solely%20on%20sample%20features%20and%20do%20not%20truly%20reveal%20the%0Aoptimization%20directions%20of%20tokens.%20This%20may%20lead%20to%20severe%20optimization%0Ainterference%20between%20different%20tokens%20assigned%20to%20an%20expert.%20To%20address%20this%0Aproblem%2C%20this%20paper%20proposes%20a%20novel%20method%20based%20on%20token-level%20gradient%0Aanalysis%2C%20i.e.%2C%20Solving%20Token%20Gradient%20Conflict%20%28STGC%29.%20Specifically%2C%20we%20first%0Ause%20token-level%20gradients%20to%20identify%20conflicting%20tokens%20in%20experts.%20After%0Athat%2C%20we%20add%20a%20specialized%20loss%20tailored%20to%20eliminate%20conflicts%20among%20tokens%0Awithin%20each%20expert.%20Our%20method%20can%20serve%20as%20a%20plug-in%20for%20diverse%20Large%0AVision-Language%20Models%2C%20and%20extensive%20experimental%20results%20demonstrate%20its%0Aeffectiveness.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/longrongyang/STGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Token%2520Gradient%2520Conflict%2520in%2520Mixture-of-Experts%2520for%2520Large%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DLongrong%2520Yang%2520and%2520Dong%2520Shen%2520and%2520Chaoxiang%2520Cai%2520and%2520Fan%2520Yang%2520and%2520Size%2520Li%2520and%2520Di%2520Zhang%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520The%2520Mixture-of-Experts%2520%2528MoE%2529%2520has%2520gained%2520increasing%2520attention%2520in%2520studying%250ALarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529.%2520It%2520uses%2520a%2520sparse%2520model%2520to%2520replace%2520the%250Adense%2520model%252C%2520achieving%2520comparable%2520performance%2520while%2520activating%2520fewer%2520parameters%250Aduring%2520inference%252C%2520thus%2520significantly%2520reducing%2520the%2520inference%2520cost.%2520Existing%2520MoE%250Amethods%2520in%2520LVLMs%2520encourage%2520different%2520experts%2520to%2520handle%2520different%2520tokens%252C%2520and%250Athey%2520usually%2520employ%2520a%2520router%2520to%2520predict%2520the%2520routing%2520of%2520each%2520token.%2520However%252C%2520the%250Apredictions%2520are%2520based%2520solely%2520on%2520sample%2520features%2520and%2520do%2520not%2520truly%2520reveal%2520the%250Aoptimization%2520directions%2520of%2520tokens.%2520This%2520may%2520lead%2520to%2520severe%2520optimization%250Ainterference%2520between%2520different%2520tokens%2520assigned%2520to%2520an%2520expert.%2520To%2520address%2520this%250Aproblem%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520method%2520based%2520on%2520token-level%2520gradient%250Aanalysis%252C%2520i.e.%252C%2520Solving%2520Token%2520Gradient%2520Conflict%2520%2528STGC%2529.%2520Specifically%252C%2520we%2520first%250Ause%2520token-level%2520gradients%2520to%2520identify%2520conflicting%2520tokens%2520in%2520experts.%2520After%250Athat%252C%2520we%2520add%2520a%2520specialized%2520loss%2520tailored%2520to%2520eliminate%2520conflicts%2520among%2520tokens%250Awithin%2520each%2520expert.%2520Our%2520method%2520can%2520serve%2520as%2520a%2520plug-in%2520for%2520diverse%2520Large%250AVision-Language%2520Models%252C%2520and%2520extensive%2520experimental%2520results%2520demonstrate%2520its%250Aeffectiveness.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/longrongyang/STGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Token%20Gradient%20Conflict%20in%20Mixture-of-Experts%20for%20Large%0A%20%20Vision-Language%20Model&entry.906535625=Longrong%20Yang%20and%20Dong%20Shen%20and%20Chaoxiang%20Cai%20and%20Fan%20Yang%20and%20Size%20Li%20and%20Di%20Zhang%20and%20Xi%20Li&entry.1292438233=%20%20The%20Mixture-of-Experts%20%28MoE%29%20has%20gained%20increasing%20attention%20in%20studying%0ALarge%20Vision-Language%20Models%20%28LVLMs%29.%20It%20uses%20a%20sparse%20model%20to%20replace%20the%0Adense%20model%2C%20achieving%20comparable%20performance%20while%20activating%20fewer%20parameters%0Aduring%20inference%2C%20thus%20significantly%20reducing%20the%20inference%20cost.%20Existing%20MoE%0Amethods%20in%20LVLMs%20encourage%20different%20experts%20to%20handle%20different%20tokens%2C%20and%0Athey%20usually%20employ%20a%20router%20to%20predict%20the%20routing%20of%20each%20token.%20However%2C%20the%0Apredictions%20are%20based%20solely%20on%20sample%20features%20and%20do%20not%20truly%20reveal%20the%0Aoptimization%20directions%20of%20tokens.%20This%20may%20lead%20to%20severe%20optimization%0Ainterference%20between%20different%20tokens%20assigned%20to%20an%20expert.%20To%20address%20this%0Aproblem%2C%20this%20paper%20proposes%20a%20novel%20method%20based%20on%20token-level%20gradient%0Aanalysis%2C%20i.e.%2C%20Solving%20Token%20Gradient%20Conflict%20%28STGC%29.%20Specifically%2C%20we%20first%0Ause%20token-level%20gradients%20to%20identify%20conflicting%20tokens%20in%20experts.%20After%0Athat%2C%20we%20add%20a%20specialized%20loss%20tailored%20to%20eliminate%20conflicts%20among%20tokens%0Awithin%20each%20expert.%20Our%20method%20can%20serve%20as%20a%20plug-in%20for%20diverse%20Large%0AVision-Language%20Models%2C%20and%20extensive%20experimental%20results%20demonstrate%20its%0Aeffectiveness.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/longrongyang/STGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19905v2&entry.124074799=Read"},
{"title": "Tensorial template matching for fast cross-correlation with rotations\n  and its application for tomography", "author": "Antonio Martinez-Sanchez and Ulrike Homberg and Jos\u00e9 Mar\u00eda Almira and Harold Phelippeau", "abstract": "  Object detection is a main task in computer vision. Template matching is the\nreference method for detecting objects with arbitrary templates. However,\ntemplate matching computational complexity depends on the rotation accuracy,\nbeing a limiting factor for large 3D images (tomograms). Here, we implement a\nnew algorithm called tensorial template matching, based on a mathematical\nframework that represents all rotations of a template with a tensor field.\nContrary to standard template matching, the computational complexity of the\npresented algorithm is independent of the rotation accuracy. Using both,\nsynthetic and real data from tomography, we demonstrate that tensorial template\nmatching is much faster than template matching and has the potential to improve\nits accuracy\n", "link": "http://arxiv.org/abs/2408.02398v1", "date": "2024-08-05", "relevancy": 2.5332, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5247}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensorial%20template%20matching%20for%20fast%20cross-correlation%20with%20rotations%0A%20%20and%20its%20application%20for%20tomography&body=Title%3A%20Tensorial%20template%20matching%20for%20fast%20cross-correlation%20with%20rotations%0A%20%20and%20its%20application%20for%20tomography%0AAuthor%3A%20Antonio%20Martinez-Sanchez%20and%20Ulrike%20Homberg%20and%20Jos%C3%A9%20Mar%C3%ADa%20Almira%20and%20Harold%20Phelippeau%0AAbstract%3A%20%20%20Object%20detection%20is%20a%20main%20task%20in%20computer%20vision.%20Template%20matching%20is%20the%0Areference%20method%20for%20detecting%20objects%20with%20arbitrary%20templates.%20However%2C%0Atemplate%20matching%20computational%20complexity%20depends%20on%20the%20rotation%20accuracy%2C%0Abeing%20a%20limiting%20factor%20for%20large%203D%20images%20%28tomograms%29.%20Here%2C%20we%20implement%20a%0Anew%20algorithm%20called%20tensorial%20template%20matching%2C%20based%20on%20a%20mathematical%0Aframework%20that%20represents%20all%20rotations%20of%20a%20template%20with%20a%20tensor%20field.%0AContrary%20to%20standard%20template%20matching%2C%20the%20computational%20complexity%20of%20the%0Apresented%20algorithm%20is%20independent%20of%20the%20rotation%20accuracy.%20Using%20both%2C%0Asynthetic%20and%20real%20data%20from%20tomography%2C%20we%20demonstrate%20that%20tensorial%20template%0Amatching%20is%20much%20faster%20than%20template%20matching%20and%20has%20the%20potential%20to%20improve%0Aits%20accuracy%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensorial%2520template%2520matching%2520for%2520fast%2520cross-correlation%2520with%2520rotations%250A%2520%2520and%2520its%2520application%2520for%2520tomography%26entry.906535625%3DAntonio%2520Martinez-Sanchez%2520and%2520Ulrike%2520Homberg%2520and%2520Jos%25C3%25A9%2520Mar%25C3%25ADa%2520Almira%2520and%2520Harold%2520Phelippeau%26entry.1292438233%3D%2520%2520Object%2520detection%2520is%2520a%2520main%2520task%2520in%2520computer%2520vision.%2520Template%2520matching%2520is%2520the%250Areference%2520method%2520for%2520detecting%2520objects%2520with%2520arbitrary%2520templates.%2520However%252C%250Atemplate%2520matching%2520computational%2520complexity%2520depends%2520on%2520the%2520rotation%2520accuracy%252C%250Abeing%2520a%2520limiting%2520factor%2520for%2520large%25203D%2520images%2520%2528tomograms%2529.%2520Here%252C%2520we%2520implement%2520a%250Anew%2520algorithm%2520called%2520tensorial%2520template%2520matching%252C%2520based%2520on%2520a%2520mathematical%250Aframework%2520that%2520represents%2520all%2520rotations%2520of%2520a%2520template%2520with%2520a%2520tensor%2520field.%250AContrary%2520to%2520standard%2520template%2520matching%252C%2520the%2520computational%2520complexity%2520of%2520the%250Apresented%2520algorithm%2520is%2520independent%2520of%2520the%2520rotation%2520accuracy.%2520Using%2520both%252C%250Asynthetic%2520and%2520real%2520data%2520from%2520tomography%252C%2520we%2520demonstrate%2520that%2520tensorial%2520template%250Amatching%2520is%2520much%2520faster%2520than%2520template%2520matching%2520and%2520has%2520the%2520potential%2520to%2520improve%250Aits%2520accuracy%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensorial%20template%20matching%20for%20fast%20cross-correlation%20with%20rotations%0A%20%20and%20its%20application%20for%20tomography&entry.906535625=Antonio%20Martinez-Sanchez%20and%20Ulrike%20Homberg%20and%20Jos%C3%A9%20Mar%C3%ADa%20Almira%20and%20Harold%20Phelippeau&entry.1292438233=%20%20Object%20detection%20is%20a%20main%20task%20in%20computer%20vision.%20Template%20matching%20is%20the%0Areference%20method%20for%20detecting%20objects%20with%20arbitrary%20templates.%20However%2C%0Atemplate%20matching%20computational%20complexity%20depends%20on%20the%20rotation%20accuracy%2C%0Abeing%20a%20limiting%20factor%20for%20large%203D%20images%20%28tomograms%29.%20Here%2C%20we%20implement%20a%0Anew%20algorithm%20called%20tensorial%20template%20matching%2C%20based%20on%20a%20mathematical%0Aframework%20that%20represents%20all%20rotations%20of%20a%20template%20with%20a%20tensor%20field.%0AContrary%20to%20standard%20template%20matching%2C%20the%20computational%20complexity%20of%20the%0Apresented%20algorithm%20is%20independent%20of%20the%20rotation%20accuracy.%20Using%20both%2C%0Asynthetic%20and%20real%20data%20from%20tomography%2C%20we%20demonstrate%20that%20tensorial%20template%0Amatching%20is%20much%20faster%20than%20template%20matching%20and%20has%20the%20potential%20to%20improve%0Aits%20accuracy%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02398v1&entry.124074799=Read"},
{"title": "SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous\n  Graph Neural Networks", "author": "Haoyu Liu and Ningyi Liao and Siqiang Luo", "abstract": "  Graph neural networks (GNNs) realize great success in graph learning but\nsuffer from performance loss when meeting heterophily, i.e. neighboring nodes\nare dissimilar, due to their local and uniform aggregation. Existing attempts\nof heterophilous GNNs incorporate long-range or global aggregations to\ndistinguish nodes in the graph. However, these aggregations usually require\niteratively maintaining and updating full-graph information, which limits their\nefficiency when applying to large-scale graphs. In this paper, we propose\n\\aggname{}, an efficient global heterophilous GNN aggregation integrating the\nstructural similarity measurement SimRank. Our theoretical analysis illustrates\nthat \\aggname{} inherently captures distant global similarity even under\nheterophily, that conventional approaches can only achieve after iterative\naggregations. Furthermore, it enjoys efficient one-time computation with a\ncomplexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive\nevaluation demonstrates that \\aggname{} achieves state-of-the-art performance\nwith superior aggregation and overall efficiency. Notably, it obtains 5$\\times$\nacceleration on the large-scale heterophily dataset \\emph{pokec} with over 30\nmillion edges compared to the best baseline aggregation.\n", "link": "http://arxiv.org/abs/2305.09958v2", "date": "2024-08-05", "relevancy": 2.465, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5198}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4888}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIGMA%3A%20Similarity-based%20Efficient%20Global%20Aggregation%20for%20Heterophilous%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20SIGMA%3A%20Similarity-based%20Efficient%20Global%20Aggregation%20for%20Heterophilous%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Haoyu%20Liu%20and%20Ningyi%20Liao%20and%20Siqiang%20Luo%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20realize%20great%20success%20in%20graph%20learning%20but%0Asuffer%20from%20performance%20loss%20when%20meeting%20heterophily%2C%20i.e.%20neighboring%20nodes%0Aare%20dissimilar%2C%20due%20to%20their%20local%20and%20uniform%20aggregation.%20Existing%20attempts%0Aof%20heterophilous%20GNNs%20incorporate%20long-range%20or%20global%20aggregations%20to%0Adistinguish%20nodes%20in%20the%20graph.%20However%2C%20these%20aggregations%20usually%20require%0Aiteratively%20maintaining%20and%20updating%20full-graph%20information%2C%20which%20limits%20their%0Aefficiency%20when%20applying%20to%20large-scale%20graphs.%20In%20this%20paper%2C%20we%20propose%0A%5Caggname%7B%7D%2C%20an%20efficient%20global%20heterophilous%20GNN%20aggregation%20integrating%20the%0Astructural%20similarity%20measurement%20SimRank.%20Our%20theoretical%20analysis%20illustrates%0Athat%20%5Caggname%7B%7D%20inherently%20captures%20distant%20global%20similarity%20even%20under%0Aheterophily%2C%20that%20conventional%20approaches%20can%20only%20achieve%20after%20iterative%0Aaggregations.%20Furthermore%2C%20it%20enjoys%20efficient%20one-time%20computation%20with%20a%0Acomplexity%20only%20linear%20to%20the%20node%20set%20size%20%24%5Cmathcal%7BO%7D%28n%29%24.%20Comprehensive%0Aevaluation%20demonstrates%20that%20%5Caggname%7B%7D%20achieves%20state-of-the-art%20performance%0Awith%20superior%20aggregation%20and%20overall%20efficiency.%20Notably%2C%20it%20obtains%205%24%5Ctimes%24%0Aacceleration%20on%20the%20large-scale%20heterophily%20dataset%20%5Cemph%7Bpokec%7D%20with%20over%2030%0Amillion%20edges%20compared%20to%20the%20best%20baseline%20aggregation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.09958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIGMA%253A%2520Similarity-based%2520Efficient%2520Global%2520Aggregation%2520for%2520Heterophilous%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DHaoyu%2520Liu%2520and%2520Ningyi%2520Liao%2520and%2520Siqiang%2520Luo%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520realize%2520great%2520success%2520in%2520graph%2520learning%2520but%250Asuffer%2520from%2520performance%2520loss%2520when%2520meeting%2520heterophily%252C%2520i.e.%2520neighboring%2520nodes%250Aare%2520dissimilar%252C%2520due%2520to%2520their%2520local%2520and%2520uniform%2520aggregation.%2520Existing%2520attempts%250Aof%2520heterophilous%2520GNNs%2520incorporate%2520long-range%2520or%2520global%2520aggregations%2520to%250Adistinguish%2520nodes%2520in%2520the%2520graph.%2520However%252C%2520these%2520aggregations%2520usually%2520require%250Aiteratively%2520maintaining%2520and%2520updating%2520full-graph%2520information%252C%2520which%2520limits%2520their%250Aefficiency%2520when%2520applying%2520to%2520large-scale%2520graphs.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%255Caggname%257B%257D%252C%2520an%2520efficient%2520global%2520heterophilous%2520GNN%2520aggregation%2520integrating%2520the%250Astructural%2520similarity%2520measurement%2520SimRank.%2520Our%2520theoretical%2520analysis%2520illustrates%250Athat%2520%255Caggname%257B%257D%2520inherently%2520captures%2520distant%2520global%2520similarity%2520even%2520under%250Aheterophily%252C%2520that%2520conventional%2520approaches%2520can%2520only%2520achieve%2520after%2520iterative%250Aaggregations.%2520Furthermore%252C%2520it%2520enjoys%2520efficient%2520one-time%2520computation%2520with%2520a%250Acomplexity%2520only%2520linear%2520to%2520the%2520node%2520set%2520size%2520%2524%255Cmathcal%257BO%257D%2528n%2529%2524.%2520Comprehensive%250Aevaluation%2520demonstrates%2520that%2520%255Caggname%257B%257D%2520achieves%2520state-of-the-art%2520performance%250Awith%2520superior%2520aggregation%2520and%2520overall%2520efficiency.%2520Notably%252C%2520it%2520obtains%25205%2524%255Ctimes%2524%250Aacceleration%2520on%2520the%2520large-scale%2520heterophily%2520dataset%2520%255Cemph%257Bpokec%257D%2520with%2520over%252030%250Amillion%2520edges%2520compared%2520to%2520the%2520best%2520baseline%2520aggregation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.09958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIGMA%3A%20Similarity-based%20Efficient%20Global%20Aggregation%20for%20Heterophilous%0A%20%20Graph%20Neural%20Networks&entry.906535625=Haoyu%20Liu%20and%20Ningyi%20Liao%20and%20Siqiang%20Luo&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20realize%20great%20success%20in%20graph%20learning%20but%0Asuffer%20from%20performance%20loss%20when%20meeting%20heterophily%2C%20i.e.%20neighboring%20nodes%0Aare%20dissimilar%2C%20due%20to%20their%20local%20and%20uniform%20aggregation.%20Existing%20attempts%0Aof%20heterophilous%20GNNs%20incorporate%20long-range%20or%20global%20aggregations%20to%0Adistinguish%20nodes%20in%20the%20graph.%20However%2C%20these%20aggregations%20usually%20require%0Aiteratively%20maintaining%20and%20updating%20full-graph%20information%2C%20which%20limits%20their%0Aefficiency%20when%20applying%20to%20large-scale%20graphs.%20In%20this%20paper%2C%20we%20propose%0A%5Caggname%7B%7D%2C%20an%20efficient%20global%20heterophilous%20GNN%20aggregation%20integrating%20the%0Astructural%20similarity%20measurement%20SimRank.%20Our%20theoretical%20analysis%20illustrates%0Athat%20%5Caggname%7B%7D%20inherently%20captures%20distant%20global%20similarity%20even%20under%0Aheterophily%2C%20that%20conventional%20approaches%20can%20only%20achieve%20after%20iterative%0Aaggregations.%20Furthermore%2C%20it%20enjoys%20efficient%20one-time%20computation%20with%20a%0Acomplexity%20only%20linear%20to%20the%20node%20set%20size%20%24%5Cmathcal%7BO%7D%28n%29%24.%20Comprehensive%0Aevaluation%20demonstrates%20that%20%5Caggname%7B%7D%20achieves%20state-of-the-art%20performance%0Awith%20superior%20aggregation%20and%20overall%20efficiency.%20Notably%2C%20it%20obtains%205%24%5Ctimes%24%0Aacceleration%20on%20the%20large-scale%20heterophily%20dataset%20%5Cemph%7Bpokec%7D%20with%20over%2030%0Amillion%20edges%20compared%20to%20the%20best%20baseline%20aggregation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.09958v2&entry.124074799=Read"},
{"title": "Enhancing Heterogeneous Knowledge Graph Completion with a Novel\n  GAT-based Approach", "author": "Wanxu Wei and Yitong Song and Bin Yao", "abstract": "  Knowledge graphs (KGs) play a vital role in enhancing search results and\nrecommendation systems. With the rapid increase in the size of the KGs, they\nare becoming inaccuracy and incomplete. This problem can be solved by the\nknowledge graph completion methods, of which graph attention network\n(GAT)-based methods stand out since their superior performance. However,\nexisting GAT-based knowledge graph completion methods often suffer from\noverfitting issues when dealing with heterogeneous knowledge graphs, primarily\ndue to the unbalanced number of samples. Additionally, these methods\ndemonstrate poor performance in predicting the tail (head) entity that shares\nthe same relation and head (tail) entity with others. To solve these problems,\nwe propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH\nincorporates two separate attention network modules that work synergistically\nto predict the missing entities. We also introduce novel encoding and feature\ntransformation approaches, enabling the robust performance of GATH in scenarios\nwith imbalanced samples. Comprehensive experiments are conducted to evaluate\nthe GATH's performance. Compared with the existing SOTA GAT-based model on\nHits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the\nFB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.\n", "link": "http://arxiv.org/abs/2408.02456v1", "date": "2024-08-05", "relevancy": 2.4223, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5137}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4743}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Heterogeneous%20Knowledge%20Graph%20Completion%20with%20a%20Novel%0A%20%20GAT-based%20Approach&body=Title%3A%20Enhancing%20Heterogeneous%20Knowledge%20Graph%20Completion%20with%20a%20Novel%0A%20%20GAT-based%20Approach%0AAuthor%3A%20Wanxu%20Wei%20and%20Yitong%20Song%20and%20Bin%20Yao%0AAbstract%3A%20%20%20Knowledge%20graphs%20%28KGs%29%20play%20a%20vital%20role%20in%20enhancing%20search%20results%20and%0Arecommendation%20systems.%20With%20the%20rapid%20increase%20in%20the%20size%20of%20the%20KGs%2C%20they%0Aare%20becoming%20inaccuracy%20and%20incomplete.%20This%20problem%20can%20be%20solved%20by%20the%0Aknowledge%20graph%20completion%20methods%2C%20of%20which%20graph%20attention%20network%0A%28GAT%29-based%20methods%20stand%20out%20since%20their%20superior%20performance.%20However%2C%0Aexisting%20GAT-based%20knowledge%20graph%20completion%20methods%20often%20suffer%20from%0Aoverfitting%20issues%20when%20dealing%20with%20heterogeneous%20knowledge%20graphs%2C%20primarily%0Adue%20to%20the%20unbalanced%20number%20of%20samples.%20Additionally%2C%20these%20methods%0Ademonstrate%20poor%20performance%20in%20predicting%20the%20tail%20%28head%29%20entity%20that%20shares%0Athe%20same%20relation%20and%20head%20%28tail%29%20entity%20with%20others.%20To%20solve%20these%20problems%2C%0Awe%20propose%20GATH%2C%20a%20novel%20GAT-based%20method%20designed%20for%20Heterogeneous%20KGs.%20GATH%0Aincorporates%20two%20separate%20attention%20network%20modules%20that%20work%20synergistically%0Ato%20predict%20the%20missing%20entities.%20We%20also%20introduce%20novel%20encoding%20and%20feature%0Atransformation%20approaches%2C%20enabling%20the%20robust%20performance%20of%20GATH%20in%20scenarios%0Awith%20imbalanced%20samples.%20Comprehensive%20experiments%20are%20conducted%20to%20evaluate%0Athe%20GATH%27s%20performance.%20Compared%20with%20the%20existing%20SOTA%20GAT-based%20model%20on%0AHits%4010%20and%20MRR%20metrics%2C%20our%20model%20improves%20performance%20by%205.2%25%20and%205.2%25%20on%20the%0AFB15K-237%20dataset%2C%20and%20by%204.5%25%20and%2014.6%25%20on%20the%20WN18RR%20dataset%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Heterogeneous%2520Knowledge%2520Graph%2520Completion%2520with%2520a%2520Novel%250A%2520%2520GAT-based%2520Approach%26entry.906535625%3DWanxu%2520Wei%2520and%2520Yitong%2520Song%2520and%2520Bin%2520Yao%26entry.1292438233%3D%2520%2520Knowledge%2520graphs%2520%2528KGs%2529%2520play%2520a%2520vital%2520role%2520in%2520enhancing%2520search%2520results%2520and%250Arecommendation%2520systems.%2520With%2520the%2520rapid%2520increase%2520in%2520the%2520size%2520of%2520the%2520KGs%252C%2520they%250Aare%2520becoming%2520inaccuracy%2520and%2520incomplete.%2520This%2520problem%2520can%2520be%2520solved%2520by%2520the%250Aknowledge%2520graph%2520completion%2520methods%252C%2520of%2520which%2520graph%2520attention%2520network%250A%2528GAT%2529-based%2520methods%2520stand%2520out%2520since%2520their%2520superior%2520performance.%2520However%252C%250Aexisting%2520GAT-based%2520knowledge%2520graph%2520completion%2520methods%2520often%2520suffer%2520from%250Aoverfitting%2520issues%2520when%2520dealing%2520with%2520heterogeneous%2520knowledge%2520graphs%252C%2520primarily%250Adue%2520to%2520the%2520unbalanced%2520number%2520of%2520samples.%2520Additionally%252C%2520these%2520methods%250Ademonstrate%2520poor%2520performance%2520in%2520predicting%2520the%2520tail%2520%2528head%2529%2520entity%2520that%2520shares%250Athe%2520same%2520relation%2520and%2520head%2520%2528tail%2529%2520entity%2520with%2520others.%2520To%2520solve%2520these%2520problems%252C%250Awe%2520propose%2520GATH%252C%2520a%2520novel%2520GAT-based%2520method%2520designed%2520for%2520Heterogeneous%2520KGs.%2520GATH%250Aincorporates%2520two%2520separate%2520attention%2520network%2520modules%2520that%2520work%2520synergistically%250Ato%2520predict%2520the%2520missing%2520entities.%2520We%2520also%2520introduce%2520novel%2520encoding%2520and%2520feature%250Atransformation%2520approaches%252C%2520enabling%2520the%2520robust%2520performance%2520of%2520GATH%2520in%2520scenarios%250Awith%2520imbalanced%2520samples.%2520Comprehensive%2520experiments%2520are%2520conducted%2520to%2520evaluate%250Athe%2520GATH%2527s%2520performance.%2520Compared%2520with%2520the%2520existing%2520SOTA%2520GAT-based%2520model%2520on%250AHits%254010%2520and%2520MRR%2520metrics%252C%2520our%2520model%2520improves%2520performance%2520by%25205.2%2525%2520and%25205.2%2525%2520on%2520the%250AFB15K-237%2520dataset%252C%2520and%2520by%25204.5%2525%2520and%252014.6%2525%2520on%2520the%2520WN18RR%2520dataset%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Heterogeneous%20Knowledge%20Graph%20Completion%20with%20a%20Novel%0A%20%20GAT-based%20Approach&entry.906535625=Wanxu%20Wei%20and%20Yitong%20Song%20and%20Bin%20Yao&entry.1292438233=%20%20Knowledge%20graphs%20%28KGs%29%20play%20a%20vital%20role%20in%20enhancing%20search%20results%20and%0Arecommendation%20systems.%20With%20the%20rapid%20increase%20in%20the%20size%20of%20the%20KGs%2C%20they%0Aare%20becoming%20inaccuracy%20and%20incomplete.%20This%20problem%20can%20be%20solved%20by%20the%0Aknowledge%20graph%20completion%20methods%2C%20of%20which%20graph%20attention%20network%0A%28GAT%29-based%20methods%20stand%20out%20since%20their%20superior%20performance.%20However%2C%0Aexisting%20GAT-based%20knowledge%20graph%20completion%20methods%20often%20suffer%20from%0Aoverfitting%20issues%20when%20dealing%20with%20heterogeneous%20knowledge%20graphs%2C%20primarily%0Adue%20to%20the%20unbalanced%20number%20of%20samples.%20Additionally%2C%20these%20methods%0Ademonstrate%20poor%20performance%20in%20predicting%20the%20tail%20%28head%29%20entity%20that%20shares%0Athe%20same%20relation%20and%20head%20%28tail%29%20entity%20with%20others.%20To%20solve%20these%20problems%2C%0Awe%20propose%20GATH%2C%20a%20novel%20GAT-based%20method%20designed%20for%20Heterogeneous%20KGs.%20GATH%0Aincorporates%20two%20separate%20attention%20network%20modules%20that%20work%20synergistically%0Ato%20predict%20the%20missing%20entities.%20We%20also%20introduce%20novel%20encoding%20and%20feature%0Atransformation%20approaches%2C%20enabling%20the%20robust%20performance%20of%20GATH%20in%20scenarios%0Awith%20imbalanced%20samples.%20Comprehensive%20experiments%20are%20conducted%20to%20evaluate%0Athe%20GATH%27s%20performance.%20Compared%20with%20the%20existing%20SOTA%20GAT-based%20model%20on%0AHits%4010%20and%20MRR%20metrics%2C%20our%20model%20improves%20performance%20by%205.2%25%20and%205.2%25%20on%20the%0AFB15K-237%20dataset%2C%20and%20by%204.5%25%20and%2014.6%25%20on%20the%20WN18RR%20dataset%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02456v1&entry.124074799=Read"},
{"title": "CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point\n  Cloud Registration", "author": "Gongxin Yao and Yixin Xuan and Xinyang Li and Yu Pan", "abstract": "  Image-to-point cloud registration aims to determine the relative camera pose\nof an RGB image with respect to a point cloud. It plays an important role in\ncamera localization within pre-built LiDAR maps. Despite the modality gaps,\nmost learning-based methods establish 2D-3D point correspondences in feature\nspace without any feedback mechanism for iterative optimization, resulting in\npoor accuracy and interpretability. In this paper, we propose to reformulate\nthe registration procedure as an iterative Markov decision process, allowing\nfor incremental adjustments to the camera pose based on each intermediate\nstate. To achieve this, we employ reinforcement learning to develop a\ncross-modal registration agent (CMR-Agent), and use imitation learning to\ninitialize its registration policy for stability and quick-start of the\ntraining. According to the cross-modal observations, we propose a 2D-3D hybrid\nstate representation that fully exploits the fine-grained features of RGB\nimages while reducing the useless neutral states caused by the spatial\ntruncation of camera frustum. Additionally, the overall framework is\nwell-designed to efficiently reuse one-shot cross-modal embeddings, avoiding\nrepetitive and time-consuming feature extraction. Extensive experiments on the\nKITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves\ncompetitive accuracy and efficiency in registration. Once the one-shot\nembeddings are completed, each iteration only takes a few milliseconds.\n", "link": "http://arxiv.org/abs/2408.02394v1", "date": "2024-08-05", "relevancy": 2.3989, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6099}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6009}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMR-Agent%3A%20Learning%20a%20Cross-Modal%20Agent%20for%20Iterative%20Image-to-Point%0A%20%20Cloud%20Registration&body=Title%3A%20CMR-Agent%3A%20Learning%20a%20Cross-Modal%20Agent%20for%20Iterative%20Image-to-Point%0A%20%20Cloud%20Registration%0AAuthor%3A%20Gongxin%20Yao%20and%20Yixin%20Xuan%20and%20Xinyang%20Li%20and%20Yu%20Pan%0AAbstract%3A%20%20%20Image-to-point%20cloud%20registration%20aims%20to%20determine%20the%20relative%20camera%20pose%0Aof%20an%20RGB%20image%20with%20respect%20to%20a%20point%20cloud.%20It%20plays%20an%20important%20role%20in%0Acamera%20localization%20within%20pre-built%20LiDAR%20maps.%20Despite%20the%20modality%20gaps%2C%0Amost%20learning-based%20methods%20establish%202D-3D%20point%20correspondences%20in%20feature%0Aspace%20without%20any%20feedback%20mechanism%20for%20iterative%20optimization%2C%20resulting%20in%0Apoor%20accuracy%20and%20interpretability.%20In%20this%20paper%2C%20we%20propose%20to%20reformulate%0Athe%20registration%20procedure%20as%20an%20iterative%20Markov%20decision%20process%2C%20allowing%0Afor%20incremental%20adjustments%20to%20the%20camera%20pose%20based%20on%20each%20intermediate%0Astate.%20To%20achieve%20this%2C%20we%20employ%20reinforcement%20learning%20to%20develop%20a%0Across-modal%20registration%20agent%20%28CMR-Agent%29%2C%20and%20use%20imitation%20learning%20to%0Ainitialize%20its%20registration%20policy%20for%20stability%20and%20quick-start%20of%20the%0Atraining.%20According%20to%20the%20cross-modal%20observations%2C%20we%20propose%20a%202D-3D%20hybrid%0Astate%20representation%20that%20fully%20exploits%20the%20fine-grained%20features%20of%20RGB%0Aimages%20while%20reducing%20the%20useless%20neutral%20states%20caused%20by%20the%20spatial%0Atruncation%20of%20camera%20frustum.%20Additionally%2C%20the%20overall%20framework%20is%0Awell-designed%20to%20efficiently%20reuse%20one-shot%20cross-modal%20embeddings%2C%20avoiding%0Arepetitive%20and%20time-consuming%20feature%20extraction.%20Extensive%20experiments%20on%20the%0AKITTI-Odometry%20and%20NuScenes%20datasets%20demonstrate%20that%20CMR-Agent%20achieves%0Acompetitive%20accuracy%20and%20efficiency%20in%20registration.%20Once%20the%20one-shot%0Aembeddings%20are%20completed%2C%20each%20iteration%20only%20takes%20a%20few%20milliseconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMR-Agent%253A%2520Learning%2520a%2520Cross-Modal%2520Agent%2520for%2520Iterative%2520Image-to-Point%250A%2520%2520Cloud%2520Registration%26entry.906535625%3DGongxin%2520Yao%2520and%2520Yixin%2520Xuan%2520and%2520Xinyang%2520Li%2520and%2520Yu%2520Pan%26entry.1292438233%3D%2520%2520Image-to-point%2520cloud%2520registration%2520aims%2520to%2520determine%2520the%2520relative%2520camera%2520pose%250Aof%2520an%2520RGB%2520image%2520with%2520respect%2520to%2520a%2520point%2520cloud.%2520It%2520plays%2520an%2520important%2520role%2520in%250Acamera%2520localization%2520within%2520pre-built%2520LiDAR%2520maps.%2520Despite%2520the%2520modality%2520gaps%252C%250Amost%2520learning-based%2520methods%2520establish%25202D-3D%2520point%2520correspondences%2520in%2520feature%250Aspace%2520without%2520any%2520feedback%2520mechanism%2520for%2520iterative%2520optimization%252C%2520resulting%2520in%250Apoor%2520accuracy%2520and%2520interpretability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520reformulate%250Athe%2520registration%2520procedure%2520as%2520an%2520iterative%2520Markov%2520decision%2520process%252C%2520allowing%250Afor%2520incremental%2520adjustments%2520to%2520the%2520camera%2520pose%2520based%2520on%2520each%2520intermediate%250Astate.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520reinforcement%2520learning%2520to%2520develop%2520a%250Across-modal%2520registration%2520agent%2520%2528CMR-Agent%2529%252C%2520and%2520use%2520imitation%2520learning%2520to%250Ainitialize%2520its%2520registration%2520policy%2520for%2520stability%2520and%2520quick-start%2520of%2520the%250Atraining.%2520According%2520to%2520the%2520cross-modal%2520observations%252C%2520we%2520propose%2520a%25202D-3D%2520hybrid%250Astate%2520representation%2520that%2520fully%2520exploits%2520the%2520fine-grained%2520features%2520of%2520RGB%250Aimages%2520while%2520reducing%2520the%2520useless%2520neutral%2520states%2520caused%2520by%2520the%2520spatial%250Atruncation%2520of%2520camera%2520frustum.%2520Additionally%252C%2520the%2520overall%2520framework%2520is%250Awell-designed%2520to%2520efficiently%2520reuse%2520one-shot%2520cross-modal%2520embeddings%252C%2520avoiding%250Arepetitive%2520and%2520time-consuming%2520feature%2520extraction.%2520Extensive%2520experiments%2520on%2520the%250AKITTI-Odometry%2520and%2520NuScenes%2520datasets%2520demonstrate%2520that%2520CMR-Agent%2520achieves%250Acompetitive%2520accuracy%2520and%2520efficiency%2520in%2520registration.%2520Once%2520the%2520one-shot%250Aembeddings%2520are%2520completed%252C%2520each%2520iteration%2520only%2520takes%2520a%2520few%2520milliseconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMR-Agent%3A%20Learning%20a%20Cross-Modal%20Agent%20for%20Iterative%20Image-to-Point%0A%20%20Cloud%20Registration&entry.906535625=Gongxin%20Yao%20and%20Yixin%20Xuan%20and%20Xinyang%20Li%20and%20Yu%20Pan&entry.1292438233=%20%20Image-to-point%20cloud%20registration%20aims%20to%20determine%20the%20relative%20camera%20pose%0Aof%20an%20RGB%20image%20with%20respect%20to%20a%20point%20cloud.%20It%20plays%20an%20important%20role%20in%0Acamera%20localization%20within%20pre-built%20LiDAR%20maps.%20Despite%20the%20modality%20gaps%2C%0Amost%20learning-based%20methods%20establish%202D-3D%20point%20correspondences%20in%20feature%0Aspace%20without%20any%20feedback%20mechanism%20for%20iterative%20optimization%2C%20resulting%20in%0Apoor%20accuracy%20and%20interpretability.%20In%20this%20paper%2C%20we%20propose%20to%20reformulate%0Athe%20registration%20procedure%20as%20an%20iterative%20Markov%20decision%20process%2C%20allowing%0Afor%20incremental%20adjustments%20to%20the%20camera%20pose%20based%20on%20each%20intermediate%0Astate.%20To%20achieve%20this%2C%20we%20employ%20reinforcement%20learning%20to%20develop%20a%0Across-modal%20registration%20agent%20%28CMR-Agent%29%2C%20and%20use%20imitation%20learning%20to%0Ainitialize%20its%20registration%20policy%20for%20stability%20and%20quick-start%20of%20the%0Atraining.%20According%20to%20the%20cross-modal%20observations%2C%20we%20propose%20a%202D-3D%20hybrid%0Astate%20representation%20that%20fully%20exploits%20the%20fine-grained%20features%20of%20RGB%0Aimages%20while%20reducing%20the%20useless%20neutral%20states%20caused%20by%20the%20spatial%0Atruncation%20of%20camera%20frustum.%20Additionally%2C%20the%20overall%20framework%20is%0Awell-designed%20to%20efficiently%20reuse%20one-shot%20cross-modal%20embeddings%2C%20avoiding%0Arepetitive%20and%20time-consuming%20feature%20extraction.%20Extensive%20experiments%20on%20the%0AKITTI-Odometry%20and%20NuScenes%20datasets%20demonstrate%20that%20CMR-Agent%20achieves%0Acompetitive%20accuracy%20and%20efficiency%20in%20registration.%20Once%20the%20one-shot%0Aembeddings%20are%20completed%2C%20each%20iteration%20only%20takes%20a%20few%20milliseconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02394v1&entry.124074799=Read"},
{"title": "Cluster Exploration using Informative Manifold Projections", "author": "Stavros Gerolymatos and Xenophon Evangelopoulos and Vladimir Gusev and John Y. Goulermas", "abstract": "  Dimensionality reduction (DR) is one of the key tools for the visual\nexploration of high-dimensional data and uncovering its cluster structure in\ntwo- or three-dimensional spaces. The vast majority of DR methods in the\nliterature do not take into account any prior knowledge a practitioner may have\nregarding the dataset under consideration. We propose a novel method to\ngenerate informative embeddings which not only factor out the structure\nassociated with different kinds of prior knowledge but also aim to reveal any\nremaining underlying structure. To achieve this, we employ a linear combination\nof two objectives: firstly, contrastive PCA that discounts the structure\nassociated with the prior information, and secondly, kurtosis projection\npursuit which ensures meaningful data separation in the obtained embeddings. We\nformulate this task as a manifold optimization problem and validate it\nempirically across a variety of datasets considering three distinct types of\nprior knowledge. Lastly, we provide an automated framework to perform iterative\nvisual exploration of high-dimensional data.\n", "link": "http://arxiv.org/abs/2309.14857v2", "date": "2024-08-05", "relevancy": 2.3644, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4716}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster%20Exploration%20using%20Informative%20Manifold%20Projections&body=Title%3A%20Cluster%20Exploration%20using%20Informative%20Manifold%20Projections%0AAuthor%3A%20Stavros%20Gerolymatos%20and%20Xenophon%20Evangelopoulos%20and%20Vladimir%20Gusev%20and%20John%20Y.%20Goulermas%0AAbstract%3A%20%20%20Dimensionality%20reduction%20%28DR%29%20is%20one%20of%20the%20key%20tools%20for%20the%20visual%0Aexploration%20of%20high-dimensional%20data%20and%20uncovering%20its%20cluster%20structure%20in%0Atwo-%20or%20three-dimensional%20spaces.%20The%20vast%20majority%20of%20DR%20methods%20in%20the%0Aliterature%20do%20not%20take%20into%20account%20any%20prior%20knowledge%20a%20practitioner%20may%20have%0Aregarding%20the%20dataset%20under%20consideration.%20We%20propose%20a%20novel%20method%20to%0Agenerate%20informative%20embeddings%20which%20not%20only%20factor%20out%20the%20structure%0Aassociated%20with%20different%20kinds%20of%20prior%20knowledge%20but%20also%20aim%20to%20reveal%20any%0Aremaining%20underlying%20structure.%20To%20achieve%20this%2C%20we%20employ%20a%20linear%20combination%0Aof%20two%20objectives%3A%20firstly%2C%20contrastive%20PCA%20that%20discounts%20the%20structure%0Aassociated%20with%20the%20prior%20information%2C%20and%20secondly%2C%20kurtosis%20projection%0Apursuit%20which%20ensures%20meaningful%20data%20separation%20in%20the%20obtained%20embeddings.%20We%0Aformulate%20this%20task%20as%20a%20manifold%20optimization%20problem%20and%20validate%20it%0Aempirically%20across%20a%20variety%20of%20datasets%20considering%20three%20distinct%20types%20of%0Aprior%20knowledge.%20Lastly%2C%20we%20provide%20an%20automated%20framework%20to%20perform%20iterative%0Avisual%20exploration%20of%20high-dimensional%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster%2520Exploration%2520using%2520Informative%2520Manifold%2520Projections%26entry.906535625%3DStavros%2520Gerolymatos%2520and%2520Xenophon%2520Evangelopoulos%2520and%2520Vladimir%2520Gusev%2520and%2520John%2520Y.%2520Goulermas%26entry.1292438233%3D%2520%2520Dimensionality%2520reduction%2520%2528DR%2529%2520is%2520one%2520of%2520the%2520key%2520tools%2520for%2520the%2520visual%250Aexploration%2520of%2520high-dimensional%2520data%2520and%2520uncovering%2520its%2520cluster%2520structure%2520in%250Atwo-%2520or%2520three-dimensional%2520spaces.%2520The%2520vast%2520majority%2520of%2520DR%2520methods%2520in%2520the%250Aliterature%2520do%2520not%2520take%2520into%2520account%2520any%2520prior%2520knowledge%2520a%2520practitioner%2520may%2520have%250Aregarding%2520the%2520dataset%2520under%2520consideration.%2520We%2520propose%2520a%2520novel%2520method%2520to%250Agenerate%2520informative%2520embeddings%2520which%2520not%2520only%2520factor%2520out%2520the%2520structure%250Aassociated%2520with%2520different%2520kinds%2520of%2520prior%2520knowledge%2520but%2520also%2520aim%2520to%2520reveal%2520any%250Aremaining%2520underlying%2520structure.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520linear%2520combination%250Aof%2520two%2520objectives%253A%2520firstly%252C%2520contrastive%2520PCA%2520that%2520discounts%2520the%2520structure%250Aassociated%2520with%2520the%2520prior%2520information%252C%2520and%2520secondly%252C%2520kurtosis%2520projection%250Apursuit%2520which%2520ensures%2520meaningful%2520data%2520separation%2520in%2520the%2520obtained%2520embeddings.%2520We%250Aformulate%2520this%2520task%2520as%2520a%2520manifold%2520optimization%2520problem%2520and%2520validate%2520it%250Aempirically%2520across%2520a%2520variety%2520of%2520datasets%2520considering%2520three%2520distinct%2520types%2520of%250Aprior%2520knowledge.%2520Lastly%252C%2520we%2520provide%2520an%2520automated%2520framework%2520to%2520perform%2520iterative%250Avisual%2520exploration%2520of%2520high-dimensional%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster%20Exploration%20using%20Informative%20Manifold%20Projections&entry.906535625=Stavros%20Gerolymatos%20and%20Xenophon%20Evangelopoulos%20and%20Vladimir%20Gusev%20and%20John%20Y.%20Goulermas&entry.1292438233=%20%20Dimensionality%20reduction%20%28DR%29%20is%20one%20of%20the%20key%20tools%20for%20the%20visual%0Aexploration%20of%20high-dimensional%20data%20and%20uncovering%20its%20cluster%20structure%20in%0Atwo-%20or%20three-dimensional%20spaces.%20The%20vast%20majority%20of%20DR%20methods%20in%20the%0Aliterature%20do%20not%20take%20into%20account%20any%20prior%20knowledge%20a%20practitioner%20may%20have%0Aregarding%20the%20dataset%20under%20consideration.%20We%20propose%20a%20novel%20method%20to%0Agenerate%20informative%20embeddings%20which%20not%20only%20factor%20out%20the%20structure%0Aassociated%20with%20different%20kinds%20of%20prior%20knowledge%20but%20also%20aim%20to%20reveal%20any%0Aremaining%20underlying%20structure.%20To%20achieve%20this%2C%20we%20employ%20a%20linear%20combination%0Aof%20two%20objectives%3A%20firstly%2C%20contrastive%20PCA%20that%20discounts%20the%20structure%0Aassociated%20with%20the%20prior%20information%2C%20and%20secondly%2C%20kurtosis%20projection%0Apursuit%20which%20ensures%20meaningful%20data%20separation%20in%20the%20obtained%20embeddings.%20We%0Aformulate%20this%20task%20as%20a%20manifold%20optimization%20problem%20and%20validate%20it%0Aempirically%20across%20a%20variety%20of%20datasets%20considering%20three%20distinct%20types%20of%0Aprior%20knowledge.%20Lastly%2C%20we%20provide%20an%20automated%20framework%20to%20perform%20iterative%0Avisual%20exploration%20of%20high-dimensional%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14857v2&entry.124074799=Read"},
{"title": "RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders", "author": "Danil Gusak and Gleb Mezentsev and Ivan Oseledets and Evgeny Frolov", "abstract": "  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n", "link": "http://arxiv.org/abs/2408.02354v1", "date": "2024-08-05", "relevancy": 2.3552, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4795}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4728}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders&body=Title%3A%20RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders%0AAuthor%3A%20Danil%20Gusak%20and%20Gleb%20Mezentsev%20and%20Ivan%20Oseledets%20and%20Evgeny%20Frolov%0AAbstract%3A%20%20%20Scalability%20is%20a%20major%20challenge%20in%20modern%20recommender%20systems.%20In%20sequential%0Arecommendations%2C%20full%20Cross-Entropy%20%28CE%29%20loss%20achieves%20state-of-the-art%0Arecommendation%20quality%20but%20consumes%20excessive%20GPU%20memory%20with%20large%20item%0Acatalogs%2C%20limiting%20its%20practicality.%20Using%20a%20GPU-efficient%20locality-sensitive%0Ahashing-like%20algorithm%20for%20approximating%20large%20tensor%20of%20logits%2C%20this%20paper%0Aintroduces%20a%20novel%20RECE%20%28REduced%20Cross-Entropy%29%20loss.%20RECE%20significantly%0Areduces%20memory%20consumption%20while%20allowing%20one%20to%20enjoy%20the%20state-of-the-art%0Aperformance%20of%20full%20CE%20loss.%20Experimental%20results%20on%20various%20datasets%20show%20that%0ARECE%20cuts%20training%20peak%20memory%20usage%20by%20up%20to%2012%20times%20compared%20to%20existing%0Amethods%20while%20retaining%20or%20exceeding%20performance%20metrics%20of%20CE%20loss.%20The%0Aapproach%20also%20opens%20up%20new%20possibilities%20for%20large-scale%20applications%20in%20other%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRECE%253A%2520Reduced%2520Cross-Entropy%2520Loss%2520for%2520Large-Catalogue%2520Sequential%250A%2520%2520Recommenders%26entry.906535625%3DDanil%2520Gusak%2520and%2520Gleb%2520Mezentsev%2520and%2520Ivan%2520Oseledets%2520and%2520Evgeny%2520Frolov%26entry.1292438233%3D%2520%2520Scalability%2520is%2520a%2520major%2520challenge%2520in%2520modern%2520recommender%2520systems.%2520In%2520sequential%250Arecommendations%252C%2520full%2520Cross-Entropy%2520%2528CE%2529%2520loss%2520achieves%2520state-of-the-art%250Arecommendation%2520quality%2520but%2520consumes%2520excessive%2520GPU%2520memory%2520with%2520large%2520item%250Acatalogs%252C%2520limiting%2520its%2520practicality.%2520Using%2520a%2520GPU-efficient%2520locality-sensitive%250Ahashing-like%2520algorithm%2520for%2520approximating%2520large%2520tensor%2520of%2520logits%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520RECE%2520%2528REduced%2520Cross-Entropy%2529%2520loss.%2520RECE%2520significantly%250Areduces%2520memory%2520consumption%2520while%2520allowing%2520one%2520to%2520enjoy%2520the%2520state-of-the-art%250Aperformance%2520of%2520full%2520CE%2520loss.%2520Experimental%2520results%2520on%2520various%2520datasets%2520show%2520that%250ARECE%2520cuts%2520training%2520peak%2520memory%2520usage%2520by%2520up%2520to%252012%2520times%2520compared%2520to%2520existing%250Amethods%2520while%2520retaining%2520or%2520exceeding%2520performance%2520metrics%2520of%2520CE%2520loss.%2520The%250Aapproach%2520also%2520opens%2520up%2520new%2520possibilities%2520for%2520large-scale%2520applications%2520in%2520other%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders&entry.906535625=Danil%20Gusak%20and%20Gleb%20Mezentsev%20and%20Ivan%20Oseledets%20and%20Evgeny%20Frolov&entry.1292438233=%20%20Scalability%20is%20a%20major%20challenge%20in%20modern%20recommender%20systems.%20In%20sequential%0Arecommendations%2C%20full%20Cross-Entropy%20%28CE%29%20loss%20achieves%20state-of-the-art%0Arecommendation%20quality%20but%20consumes%20excessive%20GPU%20memory%20with%20large%20item%0Acatalogs%2C%20limiting%20its%20practicality.%20Using%20a%20GPU-efficient%20locality-sensitive%0Ahashing-like%20algorithm%20for%20approximating%20large%20tensor%20of%20logits%2C%20this%20paper%0Aintroduces%20a%20novel%20RECE%20%28REduced%20Cross-Entropy%29%20loss.%20RECE%20significantly%0Areduces%20memory%20consumption%20while%20allowing%20one%20to%20enjoy%20the%20state-of-the-art%0Aperformance%20of%20full%20CE%20loss.%20Experimental%20results%20on%20various%20datasets%20show%20that%0ARECE%20cuts%20training%20peak%20memory%20usage%20by%20up%20to%2012%20times%20compared%20to%20existing%0Amethods%20while%20retaining%20or%20exceeding%20performance%20metrics%20of%20CE%20loss.%20The%0Aapproach%20also%20opens%20up%20new%20possibilities%20for%20large-scale%20applications%20in%20other%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02354v1&entry.124074799=Read"},
{"title": "Automatic rating of incomplete hippocampal inversions evaluated across\n  multiple cohorts", "author": "Lisa Hemforth and Baptiste Couvy-Duchesne and Kevin De Matos and Camille Brianceau and Matthieu Joulot and Tobias Banaschewski and Arun L. W. Bokde and Sylvane Desrivi\u00e8res and Herta Flor and Antoine Grigis and Hugh Garavan and Penny Gowland and Andreas Heinz and R\u00fcdiger Br\u00fchl and Jean-Luc Martinot and Marie-Laure Paill\u00e8re Martinot and Eric Artiges and Dimitri Papadopoulos and Herve Lemaitre and Tomas Paus and Luise Poustka and Sarah Hohmann and Nathalie Holz and Juliane H. Fr\u00f6hner and Michael N. Smolka and Nilakshi Vaidya and Henrik Walter and Robert Whelan and Gunter Schumann and Christian B\u00fcchel and JB Poline and Bernd Itterman and Vincent Frouin and Alexandre Martin and IMAGEN study group and Claire Cury and Olivier Colliot", "abstract": "  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal\nmalrotation, is an atypical anatomical pattern of the hippocampus found in\nabout 20% of the general population. IHI can be visually assessed on coronal\nslices of T1 weighted MR images, using a composite score that combines four\nanatomical criteria. IHI has been associated with several brain disorders\n(epilepsy, schizophrenia). However, these studies were based on small samples.\nFurthermore, the factors (genetic or environmental) that contribute to the\ngenesis of IHI are largely unknown. Large-scale studies are thus needed to\nfurther understand IHI and their potential relationships to neurological and\npsychiatric disorders. However, visual evaluation is long and tedious,\njustifying the need for an automatic method. In this paper, we propose, for the\nfirst time, to automatically rate IHI. We proceed by predicting four anatomical\ncriteria, which are then summed up to form the IHI score, providing the\nadvantage of an interpretable score. We provided an extensive experimental\ninvestigation of different machine learning methods and training strategies. We\nperformed automatic rating using a variety of deep learning models (conv5-FC3,\nResNet and SECNN) as well as a ridge regression. We studied the generalization\nof our models using different cohorts and performed multi-cohort learning. We\nrelied on a large population of 2,008 participants from the IMAGEN study, 993\nand 403 participants from the QTIM/QTAB studies as well as 985 subjects from\nthe UKBiobank. We showed that deep learning models outperformed a ridge\nregression. We demonstrated that the performances of the conv5-FC3 network were\nat least as good as more complex networks while maintaining a low complexity\nand computation time. We showed that training on a single cohort may lack in\nvariability while training on several cohorts improves generalization.\n", "link": "http://arxiv.org/abs/2408.02496v1", "date": "2024-08-05", "relevancy": 2.3291, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4654}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20rating%20of%20incomplete%20hippocampal%20inversions%20evaluated%20across%0A%20%20multiple%20cohorts&body=Title%3A%20Automatic%20rating%20of%20incomplete%20hippocampal%20inversions%20evaluated%20across%0A%20%20multiple%20cohorts%0AAuthor%3A%20Lisa%20Hemforth%20and%20Baptiste%20Couvy-Duchesne%20and%20Kevin%20De%20Matos%20and%20Camille%20Brianceau%20and%20Matthieu%20Joulot%20and%20Tobias%20Banaschewski%20and%20Arun%20L.%20W.%20Bokde%20and%20Sylvane%20Desrivi%C3%A8res%20and%20Herta%20Flor%20and%20Antoine%20Grigis%20and%20Hugh%20Garavan%20and%20Penny%20Gowland%20and%20Andreas%20Heinz%20and%20R%C3%BCdiger%20Br%C3%BChl%20and%20Jean-Luc%20Martinot%20and%20Marie-Laure%20Paill%C3%A8re%20Martinot%20and%20Eric%20Artiges%20and%20Dimitri%20Papadopoulos%20and%20Herve%20Lemaitre%20and%20Tomas%20Paus%20and%20Luise%20Poustka%20and%20Sarah%20Hohmann%20and%20Nathalie%20Holz%20and%20Juliane%20H.%20Fr%C3%B6hner%20and%20Michael%20N.%20Smolka%20and%20Nilakshi%20Vaidya%20and%20Henrik%20Walter%20and%20Robert%20Whelan%20and%20Gunter%20Schumann%20and%20Christian%20B%C3%BCchel%20and%20JB%20Poline%20and%20Bernd%20Itterman%20and%20Vincent%20Frouin%20and%20Alexandre%20Martin%20and%20IMAGEN%20study%20group%20and%20Claire%20Cury%20and%20Olivier%20Colliot%0AAbstract%3A%20%20%20Incomplete%20Hippocampal%20Inversion%20%28IHI%29%2C%20sometimes%20called%20hippocampal%0Amalrotation%2C%20is%20an%20atypical%20anatomical%20pattern%20of%20the%20hippocampus%20found%20in%0Aabout%2020%25%20of%20the%20general%20population.%20IHI%20can%20be%20visually%20assessed%20on%20coronal%0Aslices%20of%20T1%20weighted%20MR%20images%2C%20using%20a%20composite%20score%20that%20combines%20four%0Aanatomical%20criteria.%20IHI%20has%20been%20associated%20with%20several%20brain%20disorders%0A%28epilepsy%2C%20schizophrenia%29.%20However%2C%20these%20studies%20were%20based%20on%20small%20samples.%0AFurthermore%2C%20the%20factors%20%28genetic%20or%20environmental%29%20that%20contribute%20to%20the%0Agenesis%20of%20IHI%20are%20largely%20unknown.%20Large-scale%20studies%20are%20thus%20needed%20to%0Afurther%20understand%20IHI%20and%20their%20potential%20relationships%20to%20neurological%20and%0Apsychiatric%20disorders.%20However%2C%20visual%20evaluation%20is%20long%20and%20tedious%2C%0Ajustifying%20the%20need%20for%20an%20automatic%20method.%20In%20this%20paper%2C%20we%20propose%2C%20for%20the%0Afirst%20time%2C%20to%20automatically%20rate%20IHI.%20We%20proceed%20by%20predicting%20four%20anatomical%0Acriteria%2C%20which%20are%20then%20summed%20up%20to%20form%20the%20IHI%20score%2C%20providing%20the%0Aadvantage%20of%20an%20interpretable%20score.%20We%20provided%20an%20extensive%20experimental%0Ainvestigation%20of%20different%20machine%20learning%20methods%20and%20training%20strategies.%20We%0Aperformed%20automatic%20rating%20using%20a%20variety%20of%20deep%20learning%20models%20%28conv5-FC3%2C%0AResNet%20and%20SECNN%29%20as%20well%20as%20a%20ridge%20regression.%20We%20studied%20the%20generalization%0Aof%20our%20models%20using%20different%20cohorts%20and%20performed%20multi-cohort%20learning.%20We%0Arelied%20on%20a%20large%20population%20of%202%2C008%20participants%20from%20the%20IMAGEN%20study%2C%20993%0Aand%20403%20participants%20from%20the%20QTIM/QTAB%20studies%20as%20well%20as%20985%20subjects%20from%0Athe%20UKBiobank.%20We%20showed%20that%20deep%20learning%20models%20outperformed%20a%20ridge%0Aregression.%20We%20demonstrated%20that%20the%20performances%20of%20the%20conv5-FC3%20network%20were%0Aat%20least%20as%20good%20as%20more%20complex%20networks%20while%20maintaining%20a%20low%20complexity%0Aand%20computation%20time.%20We%20showed%20that%20training%20on%20a%20single%20cohort%20may%20lack%20in%0Avariability%20while%20training%20on%20several%20cohorts%20improves%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520rating%2520of%2520incomplete%2520hippocampal%2520inversions%2520evaluated%2520across%250A%2520%2520multiple%2520cohorts%26entry.906535625%3DLisa%2520Hemforth%2520and%2520Baptiste%2520Couvy-Duchesne%2520and%2520Kevin%2520De%2520Matos%2520and%2520Camille%2520Brianceau%2520and%2520Matthieu%2520Joulot%2520and%2520Tobias%2520Banaschewski%2520and%2520Arun%2520L.%2520W.%2520Bokde%2520and%2520Sylvane%2520Desrivi%25C3%25A8res%2520and%2520Herta%2520Flor%2520and%2520Antoine%2520Grigis%2520and%2520Hugh%2520Garavan%2520and%2520Penny%2520Gowland%2520and%2520Andreas%2520Heinz%2520and%2520R%25C3%25BCdiger%2520Br%25C3%25BChl%2520and%2520Jean-Luc%2520Martinot%2520and%2520Marie-Laure%2520Paill%25C3%25A8re%2520Martinot%2520and%2520Eric%2520Artiges%2520and%2520Dimitri%2520Papadopoulos%2520and%2520Herve%2520Lemaitre%2520and%2520Tomas%2520Paus%2520and%2520Luise%2520Poustka%2520and%2520Sarah%2520Hohmann%2520and%2520Nathalie%2520Holz%2520and%2520Juliane%2520H.%2520Fr%25C3%25B6hner%2520and%2520Michael%2520N.%2520Smolka%2520and%2520Nilakshi%2520Vaidya%2520and%2520Henrik%2520Walter%2520and%2520Robert%2520Whelan%2520and%2520Gunter%2520Schumann%2520and%2520Christian%2520B%25C3%25BCchel%2520and%2520JB%2520Poline%2520and%2520Bernd%2520Itterman%2520and%2520Vincent%2520Frouin%2520and%2520Alexandre%2520Martin%2520and%2520IMAGEN%2520study%2520group%2520and%2520Claire%2520Cury%2520and%2520Olivier%2520Colliot%26entry.1292438233%3D%2520%2520Incomplete%2520Hippocampal%2520Inversion%2520%2528IHI%2529%252C%2520sometimes%2520called%2520hippocampal%250Amalrotation%252C%2520is%2520an%2520atypical%2520anatomical%2520pattern%2520of%2520the%2520hippocampus%2520found%2520in%250Aabout%252020%2525%2520of%2520the%2520general%2520population.%2520IHI%2520can%2520be%2520visually%2520assessed%2520on%2520coronal%250Aslices%2520of%2520T1%2520weighted%2520MR%2520images%252C%2520using%2520a%2520composite%2520score%2520that%2520combines%2520four%250Aanatomical%2520criteria.%2520IHI%2520has%2520been%2520associated%2520with%2520several%2520brain%2520disorders%250A%2528epilepsy%252C%2520schizophrenia%2529.%2520However%252C%2520these%2520studies%2520were%2520based%2520on%2520small%2520samples.%250AFurthermore%252C%2520the%2520factors%2520%2528genetic%2520or%2520environmental%2529%2520that%2520contribute%2520to%2520the%250Agenesis%2520of%2520IHI%2520are%2520largely%2520unknown.%2520Large-scale%2520studies%2520are%2520thus%2520needed%2520to%250Afurther%2520understand%2520IHI%2520and%2520their%2520potential%2520relationships%2520to%2520neurological%2520and%250Apsychiatric%2520disorders.%2520However%252C%2520visual%2520evaluation%2520is%2520long%2520and%2520tedious%252C%250Ajustifying%2520the%2520need%2520for%2520an%2520automatic%2520method.%2520In%2520this%2520paper%252C%2520we%2520propose%252C%2520for%2520the%250Afirst%2520time%252C%2520to%2520automatically%2520rate%2520IHI.%2520We%2520proceed%2520by%2520predicting%2520four%2520anatomical%250Acriteria%252C%2520which%2520are%2520then%2520summed%2520up%2520to%2520form%2520the%2520IHI%2520score%252C%2520providing%2520the%250Aadvantage%2520of%2520an%2520interpretable%2520score.%2520We%2520provided%2520an%2520extensive%2520experimental%250Ainvestigation%2520of%2520different%2520machine%2520learning%2520methods%2520and%2520training%2520strategies.%2520We%250Aperformed%2520automatic%2520rating%2520using%2520a%2520variety%2520of%2520deep%2520learning%2520models%2520%2528conv5-FC3%252C%250AResNet%2520and%2520SECNN%2529%2520as%2520well%2520as%2520a%2520ridge%2520regression.%2520We%2520studied%2520the%2520generalization%250Aof%2520our%2520models%2520using%2520different%2520cohorts%2520and%2520performed%2520multi-cohort%2520learning.%2520We%250Arelied%2520on%2520a%2520large%2520population%2520of%25202%252C008%2520participants%2520from%2520the%2520IMAGEN%2520study%252C%2520993%250Aand%2520403%2520participants%2520from%2520the%2520QTIM/QTAB%2520studies%2520as%2520well%2520as%2520985%2520subjects%2520from%250Athe%2520UKBiobank.%2520We%2520showed%2520that%2520deep%2520learning%2520models%2520outperformed%2520a%2520ridge%250Aregression.%2520We%2520demonstrated%2520that%2520the%2520performances%2520of%2520the%2520conv5-FC3%2520network%2520were%250Aat%2520least%2520as%2520good%2520as%2520more%2520complex%2520networks%2520while%2520maintaining%2520a%2520low%2520complexity%250Aand%2520computation%2520time.%2520We%2520showed%2520that%2520training%2520on%2520a%2520single%2520cohort%2520may%2520lack%2520in%250Avariability%2520while%2520training%2520on%2520several%2520cohorts%2520improves%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20rating%20of%20incomplete%20hippocampal%20inversions%20evaluated%20across%0A%20%20multiple%20cohorts&entry.906535625=Lisa%20Hemforth%20and%20Baptiste%20Couvy-Duchesne%20and%20Kevin%20De%20Matos%20and%20Camille%20Brianceau%20and%20Matthieu%20Joulot%20and%20Tobias%20Banaschewski%20and%20Arun%20L.%20W.%20Bokde%20and%20Sylvane%20Desrivi%C3%A8res%20and%20Herta%20Flor%20and%20Antoine%20Grigis%20and%20Hugh%20Garavan%20and%20Penny%20Gowland%20and%20Andreas%20Heinz%20and%20R%C3%BCdiger%20Br%C3%BChl%20and%20Jean-Luc%20Martinot%20and%20Marie-Laure%20Paill%C3%A8re%20Martinot%20and%20Eric%20Artiges%20and%20Dimitri%20Papadopoulos%20and%20Herve%20Lemaitre%20and%20Tomas%20Paus%20and%20Luise%20Poustka%20and%20Sarah%20Hohmann%20and%20Nathalie%20Holz%20and%20Juliane%20H.%20Fr%C3%B6hner%20and%20Michael%20N.%20Smolka%20and%20Nilakshi%20Vaidya%20and%20Henrik%20Walter%20and%20Robert%20Whelan%20and%20Gunter%20Schumann%20and%20Christian%20B%C3%BCchel%20and%20JB%20Poline%20and%20Bernd%20Itterman%20and%20Vincent%20Frouin%20and%20Alexandre%20Martin%20and%20IMAGEN%20study%20group%20and%20Claire%20Cury%20and%20Olivier%20Colliot&entry.1292438233=%20%20Incomplete%20Hippocampal%20Inversion%20%28IHI%29%2C%20sometimes%20called%20hippocampal%0Amalrotation%2C%20is%20an%20atypical%20anatomical%20pattern%20of%20the%20hippocampus%20found%20in%0Aabout%2020%25%20of%20the%20general%20population.%20IHI%20can%20be%20visually%20assessed%20on%20coronal%0Aslices%20of%20T1%20weighted%20MR%20images%2C%20using%20a%20composite%20score%20that%20combines%20four%0Aanatomical%20criteria.%20IHI%20has%20been%20associated%20with%20several%20brain%20disorders%0A%28epilepsy%2C%20schizophrenia%29.%20However%2C%20these%20studies%20were%20based%20on%20small%20samples.%0AFurthermore%2C%20the%20factors%20%28genetic%20or%20environmental%29%20that%20contribute%20to%20the%0Agenesis%20of%20IHI%20are%20largely%20unknown.%20Large-scale%20studies%20are%20thus%20needed%20to%0Afurther%20understand%20IHI%20and%20their%20potential%20relationships%20to%20neurological%20and%0Apsychiatric%20disorders.%20However%2C%20visual%20evaluation%20is%20long%20and%20tedious%2C%0Ajustifying%20the%20need%20for%20an%20automatic%20method.%20In%20this%20paper%2C%20we%20propose%2C%20for%20the%0Afirst%20time%2C%20to%20automatically%20rate%20IHI.%20We%20proceed%20by%20predicting%20four%20anatomical%0Acriteria%2C%20which%20are%20then%20summed%20up%20to%20form%20the%20IHI%20score%2C%20providing%20the%0Aadvantage%20of%20an%20interpretable%20score.%20We%20provided%20an%20extensive%20experimental%0Ainvestigation%20of%20different%20machine%20learning%20methods%20and%20training%20strategies.%20We%0Aperformed%20automatic%20rating%20using%20a%20variety%20of%20deep%20learning%20models%20%28conv5-FC3%2C%0AResNet%20and%20SECNN%29%20as%20well%20as%20a%20ridge%20regression.%20We%20studied%20the%20generalization%0Aof%20our%20models%20using%20different%20cohorts%20and%20performed%20multi-cohort%20learning.%20We%0Arelied%20on%20a%20large%20population%20of%202%2C008%20participants%20from%20the%20IMAGEN%20study%2C%20993%0Aand%20403%20participants%20from%20the%20QTIM/QTAB%20studies%20as%20well%20as%20985%20subjects%20from%0Athe%20UKBiobank.%20We%20showed%20that%20deep%20learning%20models%20outperformed%20a%20ridge%0Aregression.%20We%20demonstrated%20that%20the%20performances%20of%20the%20conv5-FC3%20network%20were%0Aat%20least%20as%20good%20as%20more%20complex%20networks%20while%20maintaining%20a%20low%20complexity%0Aand%20computation%20time.%20We%20showed%20that%20training%20on%20a%20single%20cohort%20may%20lack%20in%0Avariability%20while%20training%20on%20several%20cohorts%20improves%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02496v1&entry.124074799=Read"},
{"title": "VidGen-1M: A Large-Scale Dataset for Text-to-video Generation", "author": "Zhiyu Tan and Xiaomeng Yang and Luozheng Qin and Hao Li", "abstract": "  The quality of video-text pairs fundamentally determines the upper bound of\ntext-to-video models. Currently, the datasets used for training these models\nsuffer from significant shortcomings, including low temporal consistency,\npoor-quality captions, substandard video quality, and imbalanced data\ndistribution. The prevailing video curation process, which depends on image\nmodels for tagging and manual rule-based curation, leads to a high\ncomputational load and leaves behind unclean data. As a result, there is a lack\nof appropriate training datasets for text-to-video models. To address this\nproblem, we present VidGen-1M, a superior training dataset for text-to-video\nmodels. Produced through a coarse-to-fine curation strategy, this dataset\nguarantees high-quality videos and detailed captions with excellent temporal\nconsistency. When used to train the video generation model, this dataset has\nled to experimental results that surpass those obtained with other models.\n", "link": "http://arxiv.org/abs/2408.02629v1", "date": "2024-08-05", "relevancy": 2.2825, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5929}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5549}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidGen-1M%3A%20A%20Large-Scale%20Dataset%20for%20Text-to-video%20Generation&body=Title%3A%20VidGen-1M%3A%20A%20Large-Scale%20Dataset%20for%20Text-to-video%20Generation%0AAuthor%3A%20Zhiyu%20Tan%20and%20Xiaomeng%20Yang%20and%20Luozheng%20Qin%20and%20Hao%20Li%0AAbstract%3A%20%20%20The%20quality%20of%20video-text%20pairs%20fundamentally%20determines%20the%20upper%20bound%20of%0Atext-to-video%20models.%20Currently%2C%20the%20datasets%20used%20for%20training%20these%20models%0Asuffer%20from%20significant%20shortcomings%2C%20including%20low%20temporal%20consistency%2C%0Apoor-quality%20captions%2C%20substandard%20video%20quality%2C%20and%20imbalanced%20data%0Adistribution.%20The%20prevailing%20video%20curation%20process%2C%20which%20depends%20on%20image%0Amodels%20for%20tagging%20and%20manual%20rule-based%20curation%2C%20leads%20to%20a%20high%0Acomputational%20load%20and%20leaves%20behind%20unclean%20data.%20As%20a%20result%2C%20there%20is%20a%20lack%0Aof%20appropriate%20training%20datasets%20for%20text-to-video%20models.%20To%20address%20this%0Aproblem%2C%20we%20present%20VidGen-1M%2C%20a%20superior%20training%20dataset%20for%20text-to-video%0Amodels.%20Produced%20through%20a%20coarse-to-fine%20curation%20strategy%2C%20this%20dataset%0Aguarantees%20high-quality%20videos%20and%20detailed%20captions%20with%20excellent%20temporal%0Aconsistency.%20When%20used%20to%20train%20the%20video%20generation%20model%2C%20this%20dataset%20has%0Aled%20to%20experimental%20results%20that%20surpass%20those%20obtained%20with%20other%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidGen-1M%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Text-to-video%2520Generation%26entry.906535625%3DZhiyu%2520Tan%2520and%2520Xiaomeng%2520Yang%2520and%2520Luozheng%2520Qin%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520The%2520quality%2520of%2520video-text%2520pairs%2520fundamentally%2520determines%2520the%2520upper%2520bound%2520of%250Atext-to-video%2520models.%2520Currently%252C%2520the%2520datasets%2520used%2520for%2520training%2520these%2520models%250Asuffer%2520from%2520significant%2520shortcomings%252C%2520including%2520low%2520temporal%2520consistency%252C%250Apoor-quality%2520captions%252C%2520substandard%2520video%2520quality%252C%2520and%2520imbalanced%2520data%250Adistribution.%2520The%2520prevailing%2520video%2520curation%2520process%252C%2520which%2520depends%2520on%2520image%250Amodels%2520for%2520tagging%2520and%2520manual%2520rule-based%2520curation%252C%2520leads%2520to%2520a%2520high%250Acomputational%2520load%2520and%2520leaves%2520behind%2520unclean%2520data.%2520As%2520a%2520result%252C%2520there%2520is%2520a%2520lack%250Aof%2520appropriate%2520training%2520datasets%2520for%2520text-to-video%2520models.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520present%2520VidGen-1M%252C%2520a%2520superior%2520training%2520dataset%2520for%2520text-to-video%250Amodels.%2520Produced%2520through%2520a%2520coarse-to-fine%2520curation%2520strategy%252C%2520this%2520dataset%250Aguarantees%2520high-quality%2520videos%2520and%2520detailed%2520captions%2520with%2520excellent%2520temporal%250Aconsistency.%2520When%2520used%2520to%2520train%2520the%2520video%2520generation%2520model%252C%2520this%2520dataset%2520has%250Aled%2520to%2520experimental%2520results%2520that%2520surpass%2520those%2520obtained%2520with%2520other%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidGen-1M%3A%20A%20Large-Scale%20Dataset%20for%20Text-to-video%20Generation&entry.906535625=Zhiyu%20Tan%20and%20Xiaomeng%20Yang%20and%20Luozheng%20Qin%20and%20Hao%20Li&entry.1292438233=%20%20The%20quality%20of%20video-text%20pairs%20fundamentally%20determines%20the%20upper%20bound%20of%0Atext-to-video%20models.%20Currently%2C%20the%20datasets%20used%20for%20training%20these%20models%0Asuffer%20from%20significant%20shortcomings%2C%20including%20low%20temporal%20consistency%2C%0Apoor-quality%20captions%2C%20substandard%20video%20quality%2C%20and%20imbalanced%20data%0Adistribution.%20The%20prevailing%20video%20curation%20process%2C%20which%20depends%20on%20image%0Amodels%20for%20tagging%20and%20manual%20rule-based%20curation%2C%20leads%20to%20a%20high%0Acomputational%20load%20and%20leaves%20behind%20unclean%20data.%20As%20a%20result%2C%20there%20is%20a%20lack%0Aof%20appropriate%20training%20datasets%20for%20text-to-video%20models.%20To%20address%20this%0Aproblem%2C%20we%20present%20VidGen-1M%2C%20a%20superior%20training%20dataset%20for%20text-to-video%0Amodels.%20Produced%20through%20a%20coarse-to-fine%20curation%20strategy%2C%20this%20dataset%0Aguarantees%20high-quality%20videos%20and%20detailed%20captions%20with%20excellent%20temporal%0Aconsistency.%20When%20used%20to%20train%20the%20video%20generation%20model%2C%20this%20dataset%20has%0Aled%20to%20experimental%20results%20that%20surpass%20those%20obtained%20with%20other%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02629v1&entry.124074799=Read"},
{"title": "You Only Acquire Sparse-channel (YOAS): A Unified Framework for\n  Dense-channel EEG Generation", "author": "Hongyu Chen and Weiming Zeng and Luhui Cai and Lei Wang and Jia Lu and Yueyang Li and Hongjie Yan and Wai Ting Siok and Nizhuan Wang", "abstract": "  High-precision acquisition of dense-channel electroencephalogram (EEG)\nsignals is often impeded by the costliness and lack of portability of\nequipment. In contrast, generating dense-channel EEG signals effectively from\nsparse channels shows promise and economic viability. However, sparse-channel\nEEG poses challenges such as reduced spatial resolution, information loss,\nsignal mixing, and heightened susceptibility to noise and interference. To\naddress these challenges, we first theoretically formulate the dense-channel\nEEG generation problem as by optimizing a set of cross-channel EEG signal\ngeneration problems. Then, we propose the YOAS framework for generating\ndense-channel data from sparse-channel EEG signals. The YOAS totally consists\nof four sequential stages: Data Preparation, Data Preprocessing, Biased-EEG\nGeneration, and Synthetic EEG Generation. Data Preparation and Preprocessing\ncarefully consider the distribution of EEG electrodes and low signal-to-noise\nratio problem of EEG signals. Biased-EEG Generation includes sub-modules of\nBiasEEGGanFormer and BiasEEGDiffFormer, which facilitate long-term feature\nextraction with attention and generate signals by combining electrode position\nalignment with diffusion model, respectively. Synthetic EEG Generation\nsynthesizes the final signals, employing a deduction paradigm for multi-channel\nEEG generation. Extensive experiments confirmed YOAS's feasibility, efficiency,\nand theoretical validity, even remarkably enhancing data discernibility. This\nbreakthrough in dense-channel EEG signal generation from sparse-channel data\nopens new avenues for exploration in EEG signal processing and application.\n", "link": "http://arxiv.org/abs/2406.15269v2", "date": "2024-08-05", "relevancy": 2.2771, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4676}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4525}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Only%20Acquire%20Sparse-channel%20%28YOAS%29%3A%20A%20Unified%20Framework%20for%0A%20%20Dense-channel%20EEG%20Generation&body=Title%3A%20You%20Only%20Acquire%20Sparse-channel%20%28YOAS%29%3A%20A%20Unified%20Framework%20for%0A%20%20Dense-channel%20EEG%20Generation%0AAuthor%3A%20Hongyu%20Chen%20and%20Weiming%20Zeng%20and%20Luhui%20Cai%20and%20Lei%20Wang%20and%20Jia%20Lu%20and%20Yueyang%20Li%20and%20Hongjie%20Yan%20and%20Wai%20Ting%20Siok%20and%20Nizhuan%20Wang%0AAbstract%3A%20%20%20High-precision%20acquisition%20of%20dense-channel%20electroencephalogram%20%28EEG%29%0Asignals%20is%20often%20impeded%20by%20the%20costliness%20and%20lack%20of%20portability%20of%0Aequipment.%20In%20contrast%2C%20generating%20dense-channel%20EEG%20signals%20effectively%20from%0Asparse%20channels%20shows%20promise%20and%20economic%20viability.%20However%2C%20sparse-channel%0AEEG%20poses%20challenges%20such%20as%20reduced%20spatial%20resolution%2C%20information%20loss%2C%0Asignal%20mixing%2C%20and%20heightened%20susceptibility%20to%20noise%20and%20interference.%20To%0Aaddress%20these%20challenges%2C%20we%20first%20theoretically%20formulate%20the%20dense-channel%0AEEG%20generation%20problem%20as%20by%20optimizing%20a%20set%20of%20cross-channel%20EEG%20signal%0Ageneration%20problems.%20Then%2C%20we%20propose%20the%20YOAS%20framework%20for%20generating%0Adense-channel%20data%20from%20sparse-channel%20EEG%20signals.%20The%20YOAS%20totally%20consists%0Aof%20four%20sequential%20stages%3A%20Data%20Preparation%2C%20Data%20Preprocessing%2C%20Biased-EEG%0AGeneration%2C%20and%20Synthetic%20EEG%20Generation.%20Data%20Preparation%20and%20Preprocessing%0Acarefully%20consider%20the%20distribution%20of%20EEG%20electrodes%20and%20low%20signal-to-noise%0Aratio%20problem%20of%20EEG%20signals.%20Biased-EEG%20Generation%20includes%20sub-modules%20of%0ABiasEEGGanFormer%20and%20BiasEEGDiffFormer%2C%20which%20facilitate%20long-term%20feature%0Aextraction%20with%20attention%20and%20generate%20signals%20by%20combining%20electrode%20position%0Aalignment%20with%20diffusion%20model%2C%20respectively.%20Synthetic%20EEG%20Generation%0Asynthesizes%20the%20final%20signals%2C%20employing%20a%20deduction%20paradigm%20for%20multi-channel%0AEEG%20generation.%20Extensive%20experiments%20confirmed%20YOAS%27s%20feasibility%2C%20efficiency%2C%0Aand%20theoretical%20validity%2C%20even%20remarkably%20enhancing%20data%20discernibility.%20This%0Abreakthrough%20in%20dense-channel%20EEG%20signal%20generation%20from%20sparse-channel%20data%0Aopens%20new%20avenues%20for%20exploration%20in%20EEG%20signal%20processing%20and%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Only%2520Acquire%2520Sparse-channel%2520%2528YOAS%2529%253A%2520A%2520Unified%2520Framework%2520for%250A%2520%2520Dense-channel%2520EEG%2520Generation%26entry.906535625%3DHongyu%2520Chen%2520and%2520Weiming%2520Zeng%2520and%2520Luhui%2520Cai%2520and%2520Lei%2520Wang%2520and%2520Jia%2520Lu%2520and%2520Yueyang%2520Li%2520and%2520Hongjie%2520Yan%2520and%2520Wai%2520Ting%2520Siok%2520and%2520Nizhuan%2520Wang%26entry.1292438233%3D%2520%2520High-precision%2520acquisition%2520of%2520dense-channel%2520electroencephalogram%2520%2528EEG%2529%250Asignals%2520is%2520often%2520impeded%2520by%2520the%2520costliness%2520and%2520lack%2520of%2520portability%2520of%250Aequipment.%2520In%2520contrast%252C%2520generating%2520dense-channel%2520EEG%2520signals%2520effectively%2520from%250Asparse%2520channels%2520shows%2520promise%2520and%2520economic%2520viability.%2520However%252C%2520sparse-channel%250AEEG%2520poses%2520challenges%2520such%2520as%2520reduced%2520spatial%2520resolution%252C%2520information%2520loss%252C%250Asignal%2520mixing%252C%2520and%2520heightened%2520susceptibility%2520to%2520noise%2520and%2520interference.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520first%2520theoretically%2520formulate%2520the%2520dense-channel%250AEEG%2520generation%2520problem%2520as%2520by%2520optimizing%2520a%2520set%2520of%2520cross-channel%2520EEG%2520signal%250Ageneration%2520problems.%2520Then%252C%2520we%2520propose%2520the%2520YOAS%2520framework%2520for%2520generating%250Adense-channel%2520data%2520from%2520sparse-channel%2520EEG%2520signals.%2520The%2520YOAS%2520totally%2520consists%250Aof%2520four%2520sequential%2520stages%253A%2520Data%2520Preparation%252C%2520Data%2520Preprocessing%252C%2520Biased-EEG%250AGeneration%252C%2520and%2520Synthetic%2520EEG%2520Generation.%2520Data%2520Preparation%2520and%2520Preprocessing%250Acarefully%2520consider%2520the%2520distribution%2520of%2520EEG%2520electrodes%2520and%2520low%2520signal-to-noise%250Aratio%2520problem%2520of%2520EEG%2520signals.%2520Biased-EEG%2520Generation%2520includes%2520sub-modules%2520of%250ABiasEEGGanFormer%2520and%2520BiasEEGDiffFormer%252C%2520which%2520facilitate%2520long-term%2520feature%250Aextraction%2520with%2520attention%2520and%2520generate%2520signals%2520by%2520combining%2520electrode%2520position%250Aalignment%2520with%2520diffusion%2520model%252C%2520respectively.%2520Synthetic%2520EEG%2520Generation%250Asynthesizes%2520the%2520final%2520signals%252C%2520employing%2520a%2520deduction%2520paradigm%2520for%2520multi-channel%250AEEG%2520generation.%2520Extensive%2520experiments%2520confirmed%2520YOAS%2527s%2520feasibility%252C%2520efficiency%252C%250Aand%2520theoretical%2520validity%252C%2520even%2520remarkably%2520enhancing%2520data%2520discernibility.%2520This%250Abreakthrough%2520in%2520dense-channel%2520EEG%2520signal%2520generation%2520from%2520sparse-channel%2520data%250Aopens%2520new%2520avenues%2520for%2520exploration%2520in%2520EEG%2520signal%2520processing%2520and%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Only%20Acquire%20Sparse-channel%20%28YOAS%29%3A%20A%20Unified%20Framework%20for%0A%20%20Dense-channel%20EEG%20Generation&entry.906535625=Hongyu%20Chen%20and%20Weiming%20Zeng%20and%20Luhui%20Cai%20and%20Lei%20Wang%20and%20Jia%20Lu%20and%20Yueyang%20Li%20and%20Hongjie%20Yan%20and%20Wai%20Ting%20Siok%20and%20Nizhuan%20Wang&entry.1292438233=%20%20High-precision%20acquisition%20of%20dense-channel%20electroencephalogram%20%28EEG%29%0Asignals%20is%20often%20impeded%20by%20the%20costliness%20and%20lack%20of%20portability%20of%0Aequipment.%20In%20contrast%2C%20generating%20dense-channel%20EEG%20signals%20effectively%20from%0Asparse%20channels%20shows%20promise%20and%20economic%20viability.%20However%2C%20sparse-channel%0AEEG%20poses%20challenges%20such%20as%20reduced%20spatial%20resolution%2C%20information%20loss%2C%0Asignal%20mixing%2C%20and%20heightened%20susceptibility%20to%20noise%20and%20interference.%20To%0Aaddress%20these%20challenges%2C%20we%20first%20theoretically%20formulate%20the%20dense-channel%0AEEG%20generation%20problem%20as%20by%20optimizing%20a%20set%20of%20cross-channel%20EEG%20signal%0Ageneration%20problems.%20Then%2C%20we%20propose%20the%20YOAS%20framework%20for%20generating%0Adense-channel%20data%20from%20sparse-channel%20EEG%20signals.%20The%20YOAS%20totally%20consists%0Aof%20four%20sequential%20stages%3A%20Data%20Preparation%2C%20Data%20Preprocessing%2C%20Biased-EEG%0AGeneration%2C%20and%20Synthetic%20EEG%20Generation.%20Data%20Preparation%20and%20Preprocessing%0Acarefully%20consider%20the%20distribution%20of%20EEG%20electrodes%20and%20low%20signal-to-noise%0Aratio%20problem%20of%20EEG%20signals.%20Biased-EEG%20Generation%20includes%20sub-modules%20of%0ABiasEEGGanFormer%20and%20BiasEEGDiffFormer%2C%20which%20facilitate%20long-term%20feature%0Aextraction%20with%20attention%20and%20generate%20signals%20by%20combining%20electrode%20position%0Aalignment%20with%20diffusion%20model%2C%20respectively.%20Synthetic%20EEG%20Generation%0Asynthesizes%20the%20final%20signals%2C%20employing%20a%20deduction%20paradigm%20for%20multi-channel%0AEEG%20generation.%20Extensive%20experiments%20confirmed%20YOAS%27s%20feasibility%2C%20efficiency%2C%0Aand%20theoretical%20validity%2C%20even%20remarkably%20enhancing%20data%20discernibility.%20This%0Abreakthrough%20in%20dense-channel%20EEG%20signal%20generation%20from%20sparse-channel%20data%0Aopens%20new%20avenues%20for%20exploration%20in%20EEG%20signal%20processing%20and%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15269v2&entry.124074799=Read"},
{"title": "FE-Adapter: Adapting Image-based Emotion Classifiers to Videos", "author": "Shreyank N Gowda and Boyan Gao and David A. Clifton", "abstract": "  Utilizing large pre-trained models for specific tasks has yielded impressive\nresults. However, fully fine-tuning these increasingly large models is becoming\nprohibitively resource-intensive. This has led to a focus on more\nparameter-efficient transfer learning, primarily within the same modality. But\nthis approach has limitations, particularly in video understanding where\nsuitable pre-trained models are less common. Addressing this, our study\nintroduces a novel cross-modality transfer learning approach from images to\nvideos, which we call parameter-efficient image-to-video transfer learning. We\npresent the Facial-Emotion Adapter (FE-Adapter), designed for efficient\nfine-tuning in video tasks. This adapter allows pre-trained image models, which\ntraditionally lack temporal processing capabilities, to analyze dynamic video\ncontent efficiently. Notably, it uses about 15 times fewer parameters than\nprevious methods, while improving accuracy. Our experiments in video emotion\nrecognition demonstrate that the FE-Adapter can match or even surpass existing\nfine-tuning and video emotion models in both performance and efficiency. This\nbreakthrough highlights the potential for cross-modality approaches in\nenhancing the capabilities of AI models, particularly in fields like video\nemotion analysis where the demand for efficiency and accuracy is constantly\nrising.\n", "link": "http://arxiv.org/abs/2408.02421v1", "date": "2024-08-05", "relevancy": 2.2445, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5966}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5662}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FE-Adapter%3A%20Adapting%20Image-based%20Emotion%20Classifiers%20to%20Videos&body=Title%3A%20FE-Adapter%3A%20Adapting%20Image-based%20Emotion%20Classifiers%20to%20Videos%0AAuthor%3A%20Shreyank%20N%20Gowda%20and%20Boyan%20Gao%20and%20David%20A.%20Clifton%0AAbstract%3A%20%20%20Utilizing%20large%20pre-trained%20models%20for%20specific%20tasks%20has%20yielded%20impressive%0Aresults.%20However%2C%20fully%20fine-tuning%20these%20increasingly%20large%20models%20is%20becoming%0Aprohibitively%20resource-intensive.%20This%20has%20led%20to%20a%20focus%20on%20more%0Aparameter-efficient%20transfer%20learning%2C%20primarily%20within%20the%20same%20modality.%20But%0Athis%20approach%20has%20limitations%2C%20particularly%20in%20video%20understanding%20where%0Asuitable%20pre-trained%20models%20are%20less%20common.%20Addressing%20this%2C%20our%20study%0Aintroduces%20a%20novel%20cross-modality%20transfer%20learning%20approach%20from%20images%20to%0Avideos%2C%20which%20we%20call%20parameter-efficient%20image-to-video%20transfer%20learning.%20We%0Apresent%20the%20Facial-Emotion%20Adapter%20%28FE-Adapter%29%2C%20designed%20for%20efficient%0Afine-tuning%20in%20video%20tasks.%20This%20adapter%20allows%20pre-trained%20image%20models%2C%20which%0Atraditionally%20lack%20temporal%20processing%20capabilities%2C%20to%20analyze%20dynamic%20video%0Acontent%20efficiently.%20Notably%2C%20it%20uses%20about%2015%20times%20fewer%20parameters%20than%0Aprevious%20methods%2C%20while%20improving%20accuracy.%20Our%20experiments%20in%20video%20emotion%0Arecognition%20demonstrate%20that%20the%20FE-Adapter%20can%20match%20or%20even%20surpass%20existing%0Afine-tuning%20and%20video%20emotion%20models%20in%20both%20performance%20and%20efficiency.%20This%0Abreakthrough%20highlights%20the%20potential%20for%20cross-modality%20approaches%20in%0Aenhancing%20the%20capabilities%20of%20AI%20models%2C%20particularly%20in%20fields%20like%20video%0Aemotion%20analysis%20where%20the%20demand%20for%20efficiency%20and%20accuracy%20is%20constantly%0Arising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFE-Adapter%253A%2520Adapting%2520Image-based%2520Emotion%2520Classifiers%2520to%2520Videos%26entry.906535625%3DShreyank%2520N%2520Gowda%2520and%2520Boyan%2520Gao%2520and%2520David%2520A.%2520Clifton%26entry.1292438233%3D%2520%2520Utilizing%2520large%2520pre-trained%2520models%2520for%2520specific%2520tasks%2520has%2520yielded%2520impressive%250Aresults.%2520However%252C%2520fully%2520fine-tuning%2520these%2520increasingly%2520large%2520models%2520is%2520becoming%250Aprohibitively%2520resource-intensive.%2520This%2520has%2520led%2520to%2520a%2520focus%2520on%2520more%250Aparameter-efficient%2520transfer%2520learning%252C%2520primarily%2520within%2520the%2520same%2520modality.%2520But%250Athis%2520approach%2520has%2520limitations%252C%2520particularly%2520in%2520video%2520understanding%2520where%250Asuitable%2520pre-trained%2520models%2520are%2520less%2520common.%2520Addressing%2520this%252C%2520our%2520study%250Aintroduces%2520a%2520novel%2520cross-modality%2520transfer%2520learning%2520approach%2520from%2520images%2520to%250Avideos%252C%2520which%2520we%2520call%2520parameter-efficient%2520image-to-video%2520transfer%2520learning.%2520We%250Apresent%2520the%2520Facial-Emotion%2520Adapter%2520%2528FE-Adapter%2529%252C%2520designed%2520for%2520efficient%250Afine-tuning%2520in%2520video%2520tasks.%2520This%2520adapter%2520allows%2520pre-trained%2520image%2520models%252C%2520which%250Atraditionally%2520lack%2520temporal%2520processing%2520capabilities%252C%2520to%2520analyze%2520dynamic%2520video%250Acontent%2520efficiently.%2520Notably%252C%2520it%2520uses%2520about%252015%2520times%2520fewer%2520parameters%2520than%250Aprevious%2520methods%252C%2520while%2520improving%2520accuracy.%2520Our%2520experiments%2520in%2520video%2520emotion%250Arecognition%2520demonstrate%2520that%2520the%2520FE-Adapter%2520can%2520match%2520or%2520even%2520surpass%2520existing%250Afine-tuning%2520and%2520video%2520emotion%2520models%2520in%2520both%2520performance%2520and%2520efficiency.%2520This%250Abreakthrough%2520highlights%2520the%2520potential%2520for%2520cross-modality%2520approaches%2520in%250Aenhancing%2520the%2520capabilities%2520of%2520AI%2520models%252C%2520particularly%2520in%2520fields%2520like%2520video%250Aemotion%2520analysis%2520where%2520the%2520demand%2520for%2520efficiency%2520and%2520accuracy%2520is%2520constantly%250Arising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FE-Adapter%3A%20Adapting%20Image-based%20Emotion%20Classifiers%20to%20Videos&entry.906535625=Shreyank%20N%20Gowda%20and%20Boyan%20Gao%20and%20David%20A.%20Clifton&entry.1292438233=%20%20Utilizing%20large%20pre-trained%20models%20for%20specific%20tasks%20has%20yielded%20impressive%0Aresults.%20However%2C%20fully%20fine-tuning%20these%20increasingly%20large%20models%20is%20becoming%0Aprohibitively%20resource-intensive.%20This%20has%20led%20to%20a%20focus%20on%20more%0Aparameter-efficient%20transfer%20learning%2C%20primarily%20within%20the%20same%20modality.%20But%0Athis%20approach%20has%20limitations%2C%20particularly%20in%20video%20understanding%20where%0Asuitable%20pre-trained%20models%20are%20less%20common.%20Addressing%20this%2C%20our%20study%0Aintroduces%20a%20novel%20cross-modality%20transfer%20learning%20approach%20from%20images%20to%0Avideos%2C%20which%20we%20call%20parameter-efficient%20image-to-video%20transfer%20learning.%20We%0Apresent%20the%20Facial-Emotion%20Adapter%20%28FE-Adapter%29%2C%20designed%20for%20efficient%0Afine-tuning%20in%20video%20tasks.%20This%20adapter%20allows%20pre-trained%20image%20models%2C%20which%0Atraditionally%20lack%20temporal%20processing%20capabilities%2C%20to%20analyze%20dynamic%20video%0Acontent%20efficiently.%20Notably%2C%20it%20uses%20about%2015%20times%20fewer%20parameters%20than%0Aprevious%20methods%2C%20while%20improving%20accuracy.%20Our%20experiments%20in%20video%20emotion%0Arecognition%20demonstrate%20that%20the%20FE-Adapter%20can%20match%20or%20even%20surpass%20existing%0Afine-tuning%20and%20video%20emotion%20models%20in%20both%20performance%20and%20efficiency.%20This%0Abreakthrough%20highlights%20the%20potential%20for%20cross-modality%20approaches%20in%0Aenhancing%20the%20capabilities%20of%20AI%20models%2C%20particularly%20in%20fields%20like%20video%0Aemotion%20analysis%20where%20the%20demand%20for%20efficiency%20and%20accuracy%20is%20constantly%0Arising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02421v1&entry.124074799=Read"},
{"title": "SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications", "author": "Amira Guesmi and Muhammad Abdullah Hanif and Ihsen Alouani and Bassem Ouni and Muhammad Shafique", "abstract": "  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n", "link": "http://arxiv.org/abs/2403.11515v2", "date": "2024-08-05", "relevancy": 2.2401, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSAP%3A%20A%20Shape-Sensitive%20Adversarial%20Patch%20for%20Comprehensive%20Disruption%0A%20%20of%20Monocular%20Depth%20Estimation%20in%20Autonomous%20Navigation%20Applications&body=Title%3A%20SSAP%3A%20A%20Shape-Sensitive%20Adversarial%20Patch%20for%20Comprehensive%20Disruption%0A%20%20of%20Monocular%20Depth%20Estimation%20in%20Autonomous%20Navigation%20Applications%0AAuthor%3A%20Amira%20Guesmi%20and%20Muhammad%20Abdullah%20Hanif%20and%20Ihsen%20Alouani%20and%20Bassem%20Ouni%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20%28MDE%29%20has%20advanced%20significantly%2C%20primarily%0Athrough%20the%20integration%20of%20convolutional%20neural%20networks%20%28CNNs%29%20and%20more%0Arecently%2C%20Transformers.%20However%2C%20concerns%20about%20their%20susceptibility%20to%0Aadversarial%20attacks%20have%20emerged%2C%20especially%20in%20safety-critical%20domains%20like%0Aautonomous%20driving%20and%20robotic%20navigation.%20Existing%20approaches%20for%20assessing%0ACNN-based%20depth%20prediction%20methods%20have%20fallen%20short%20in%20inducing%20comprehensive%0Adisruptions%20to%20the%20vision%20system%2C%20often%20limited%20to%20specific%20local%20areas.%20In%0Athis%20paper%2C%20we%20introduce%20SSAP%20%28Shape-Sensitive%20Adversarial%20Patch%29%2C%20a%20novel%0Aapproach%20designed%20to%20comprehensively%20disrupt%20monocular%20depth%20estimation%20%28MDE%29%0Ain%20autonomous%20navigation%20applications.%20Our%20patch%20is%20crafted%20to%20selectively%0Aundermine%20MDE%20in%20two%20distinct%20ways%3A%20by%20distorting%20estimated%20distances%20or%20by%0Acreating%20the%20illusion%20of%20an%20object%20disappearing%20from%20the%20system%27s%20perspective.%0ANotably%2C%20our%20patch%20is%20shape-sensitive%2C%20meaning%20it%20considers%20the%20specific%20shape%0Aand%20scale%20of%20the%20target%20object%2C%20thereby%20extending%20its%20influence%20beyond%0Aimmediate%20proximity.%20Furthermore%2C%20our%20patch%20is%20trained%20to%20effectively%20address%0Adifferent%20scales%20and%20distances%20from%20the%20camera.%20Experimental%20results%0Ademonstrate%20that%20our%20approach%20induces%20a%20mean%20depth%20estimation%20error%20surpassing%0A0.5%2C%20impacting%20up%20to%2099%25%20of%20the%20targeted%20region%20for%20CNN-based%20MDE%20models.%0AAdditionally%2C%20we%20investigate%20the%20vulnerability%20of%20Transformer-based%20MDE%20models%0Ato%20patch-based%20attacks%2C%20revealing%20that%20SSAP%20yields%20a%20significant%20error%20of%200.59%0Aand%20exerts%20substantial%20influence%20over%2099%25%20of%20the%20target%20region%20on%20these%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSAP%253A%2520A%2520Shape-Sensitive%2520Adversarial%2520Patch%2520for%2520Comprehensive%2520Disruption%250A%2520%2520of%2520Monocular%2520Depth%2520Estimation%2520in%2520Autonomous%2520Navigation%2520Applications%26entry.906535625%3DAmira%2520Guesmi%2520and%2520Muhammad%2520Abdullah%2520Hanif%2520and%2520Ihsen%2520Alouani%2520and%2520Bassem%2520Ouni%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520%2528MDE%2529%2520has%2520advanced%2520significantly%252C%2520primarily%250Athrough%2520the%2520integration%2520of%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520more%250Arecently%252C%2520Transformers.%2520However%252C%2520concerns%2520about%2520their%2520susceptibility%2520to%250Aadversarial%2520attacks%2520have%2520emerged%252C%2520especially%2520in%2520safety-critical%2520domains%2520like%250Aautonomous%2520driving%2520and%2520robotic%2520navigation.%2520Existing%2520approaches%2520for%2520assessing%250ACNN-based%2520depth%2520prediction%2520methods%2520have%2520fallen%2520short%2520in%2520inducing%2520comprehensive%250Adisruptions%2520to%2520the%2520vision%2520system%252C%2520often%2520limited%2520to%2520specific%2520local%2520areas.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520SSAP%2520%2528Shape-Sensitive%2520Adversarial%2520Patch%2529%252C%2520a%2520novel%250Aapproach%2520designed%2520to%2520comprehensively%2520disrupt%2520monocular%2520depth%2520estimation%2520%2528MDE%2529%250Ain%2520autonomous%2520navigation%2520applications.%2520Our%2520patch%2520is%2520crafted%2520to%2520selectively%250Aundermine%2520MDE%2520in%2520two%2520distinct%2520ways%253A%2520by%2520distorting%2520estimated%2520distances%2520or%2520by%250Acreating%2520the%2520illusion%2520of%2520an%2520object%2520disappearing%2520from%2520the%2520system%2527s%2520perspective.%250ANotably%252C%2520our%2520patch%2520is%2520shape-sensitive%252C%2520meaning%2520it%2520considers%2520the%2520specific%2520shape%250Aand%2520scale%2520of%2520the%2520target%2520object%252C%2520thereby%2520extending%2520its%2520influence%2520beyond%250Aimmediate%2520proximity.%2520Furthermore%252C%2520our%2520patch%2520is%2520trained%2520to%2520effectively%2520address%250Adifferent%2520scales%2520and%2520distances%2520from%2520the%2520camera.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520approach%2520induces%2520a%2520mean%2520depth%2520estimation%2520error%2520surpassing%250A0.5%252C%2520impacting%2520up%2520to%252099%2525%2520of%2520the%2520targeted%2520region%2520for%2520CNN-based%2520MDE%2520models.%250AAdditionally%252C%2520we%2520investigate%2520the%2520vulnerability%2520of%2520Transformer-based%2520MDE%2520models%250Ato%2520patch-based%2520attacks%252C%2520revealing%2520that%2520SSAP%2520yields%2520a%2520significant%2520error%2520of%25200.59%250Aand%2520exerts%2520substantial%2520influence%2520over%252099%2525%2520of%2520the%2520target%2520region%2520on%2520these%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSAP%3A%20A%20Shape-Sensitive%20Adversarial%20Patch%20for%20Comprehensive%20Disruption%0A%20%20of%20Monocular%20Depth%20Estimation%20in%20Autonomous%20Navigation%20Applications&entry.906535625=Amira%20Guesmi%20and%20Muhammad%20Abdullah%20Hanif%20and%20Ihsen%20Alouani%20and%20Bassem%20Ouni%20and%20Muhammad%20Shafique&entry.1292438233=%20%20Monocular%20depth%20estimation%20%28MDE%29%20has%20advanced%20significantly%2C%20primarily%0Athrough%20the%20integration%20of%20convolutional%20neural%20networks%20%28CNNs%29%20and%20more%0Arecently%2C%20Transformers.%20However%2C%20concerns%20about%20their%20susceptibility%20to%0Aadversarial%20attacks%20have%20emerged%2C%20especially%20in%20safety-critical%20domains%20like%0Aautonomous%20driving%20and%20robotic%20navigation.%20Existing%20approaches%20for%20assessing%0ACNN-based%20depth%20prediction%20methods%20have%20fallen%20short%20in%20inducing%20comprehensive%0Adisruptions%20to%20the%20vision%20system%2C%20often%20limited%20to%20specific%20local%20areas.%20In%0Athis%20paper%2C%20we%20introduce%20SSAP%20%28Shape-Sensitive%20Adversarial%20Patch%29%2C%20a%20novel%0Aapproach%20designed%20to%20comprehensively%20disrupt%20monocular%20depth%20estimation%20%28MDE%29%0Ain%20autonomous%20navigation%20applications.%20Our%20patch%20is%20crafted%20to%20selectively%0Aundermine%20MDE%20in%20two%20distinct%20ways%3A%20by%20distorting%20estimated%20distances%20or%20by%0Acreating%20the%20illusion%20of%20an%20object%20disappearing%20from%20the%20system%27s%20perspective.%0ANotably%2C%20our%20patch%20is%20shape-sensitive%2C%20meaning%20it%20considers%20the%20specific%20shape%0Aand%20scale%20of%20the%20target%20object%2C%20thereby%20extending%20its%20influence%20beyond%0Aimmediate%20proximity.%20Furthermore%2C%20our%20patch%20is%20trained%20to%20effectively%20address%0Adifferent%20scales%20and%20distances%20from%20the%20camera.%20Experimental%20results%0Ademonstrate%20that%20our%20approach%20induces%20a%20mean%20depth%20estimation%20error%20surpassing%0A0.5%2C%20impacting%20up%20to%2099%25%20of%20the%20targeted%20region%20for%20CNN-based%20MDE%20models.%0AAdditionally%2C%20we%20investigate%20the%20vulnerability%20of%20Transformer-based%20MDE%20models%0Ato%20patch-based%20attacks%2C%20revealing%20that%20SSAP%20yields%20a%20significant%20error%20of%200.59%0Aand%20exerts%20substantial%20influence%20over%2099%25%20of%20the%20target%20region%20on%20these%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11515v2&entry.124074799=Read"},
{"title": "Vision Learners Meet Web Image-Text Pairs", "author": "Bingchen Zhao and Quan Cui and Hao Wu and Osamu Yoshie and Cheng Yang and Oisin Mac Aodha", "abstract": "  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n", "link": "http://arxiv.org/abs/2301.07088v3", "date": "2024-08-05", "relevancy": 2.2377, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5791}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5543}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Learners%20Meet%20Web%20Image-Text%20Pairs&body=Title%3A%20Vision%20Learners%20Meet%20Web%20Image-Text%20Pairs%0AAuthor%3A%20Bingchen%20Zhao%20and%20Quan%20Cui%20and%20Hao%20Wu%20and%20Osamu%20Yoshie%20and%20Cheng%20Yang%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20Many%20self-supervised%20learning%20methods%20are%20pre-trained%20on%20the%20well-curated%0AImageNet-1K%20dataset.%20In%20this%20work%2C%20given%20the%20excellent%20scalability%20of%20web%20data%2C%0Awe%20consider%20self-supervised%20pre-training%20on%20noisy%20web%20sourced%20image-text%20paired%0Adata.%20First%2C%20we%20conduct%20a%20benchmark%20study%20of%20representative%20self-supervised%0Apre-training%20methods%20on%20large-scale%20web%20data%20in%20a%20like-for-like%20setting.%20We%0Acompare%20a%20range%20of%20methods%2C%20including%20single-modal%20ones%20that%20use%20masked%0Atraining%20objectives%20and%20multi-modal%20ones%20that%20use%20image-text%20constrastive%0Atraining.%20We%20observe%20that%20existing%20multi-modal%20methods%20do%20not%20outperform%20their%0Asingle-modal%20counterparts%20on%20vision%20transfer%20learning%20tasks.%20We%20derive%20an%0Ainformation-theoretical%20view%20to%20explain%20these%20benchmark%20results%2C%20which%20provides%0Ainsight%20into%20how%20to%20design%20a%20novel%20vision%20learner.%20Inspired%20by%20this%20insight%2C%20we%0Apresent%20a%20new%20visual%20representation%20pre-training%20method%2C%20MUlti-modal%0AGenerator~%28MUG%29%2C%20that%20learns%20from%20scalable%20web%20sourced%20image-text%20data.%20MUG%0Aachieves%20state-of-the-art%20transfer%20performance%20on%20a%20variety%20of%20tasks%20and%0Ademonstrates%20promising%20scaling%20properties.%20Pre-trained%20models%20and%20code%20will%20be%0Amade%20public%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.07088v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Learners%2520Meet%2520Web%2520Image-Text%2520Pairs%26entry.906535625%3DBingchen%2520Zhao%2520and%2520Quan%2520Cui%2520and%2520Hao%2520Wu%2520and%2520Osamu%2520Yoshie%2520and%2520Cheng%2520Yang%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520Many%2520self-supervised%2520learning%2520methods%2520are%2520pre-trained%2520on%2520the%2520well-curated%250AImageNet-1K%2520dataset.%2520In%2520this%2520work%252C%2520given%2520the%2520excellent%2520scalability%2520of%2520web%2520data%252C%250Awe%2520consider%2520self-supervised%2520pre-training%2520on%2520noisy%2520web%2520sourced%2520image-text%2520paired%250Adata.%2520First%252C%2520we%2520conduct%2520a%2520benchmark%2520study%2520of%2520representative%2520self-supervised%250Apre-training%2520methods%2520on%2520large-scale%2520web%2520data%2520in%2520a%2520like-for-like%2520setting.%2520We%250Acompare%2520a%2520range%2520of%2520methods%252C%2520including%2520single-modal%2520ones%2520that%2520use%2520masked%250Atraining%2520objectives%2520and%2520multi-modal%2520ones%2520that%2520use%2520image-text%2520constrastive%250Atraining.%2520We%2520observe%2520that%2520existing%2520multi-modal%2520methods%2520do%2520not%2520outperform%2520their%250Asingle-modal%2520counterparts%2520on%2520vision%2520transfer%2520learning%2520tasks.%2520We%2520derive%2520an%250Ainformation-theoretical%2520view%2520to%2520explain%2520these%2520benchmark%2520results%252C%2520which%2520provides%250Ainsight%2520into%2520how%2520to%2520design%2520a%2520novel%2520vision%2520learner.%2520Inspired%2520by%2520this%2520insight%252C%2520we%250Apresent%2520a%2520new%2520visual%2520representation%2520pre-training%2520method%252C%2520MUlti-modal%250AGenerator~%2528MUG%2529%252C%2520that%2520learns%2520from%2520scalable%2520web%2520sourced%2520image-text%2520data.%2520MUG%250Aachieves%2520state-of-the-art%2520transfer%2520performance%2520on%2520a%2520variety%2520of%2520tasks%2520and%250Ademonstrates%2520promising%2520scaling%2520properties.%2520Pre-trained%2520models%2520and%2520code%2520will%2520be%250Amade%2520public%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.07088v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Learners%20Meet%20Web%20Image-Text%20Pairs&entry.906535625=Bingchen%20Zhao%20and%20Quan%20Cui%20and%20Hao%20Wu%20and%20Osamu%20Yoshie%20and%20Cheng%20Yang%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20Many%20self-supervised%20learning%20methods%20are%20pre-trained%20on%20the%20well-curated%0AImageNet-1K%20dataset.%20In%20this%20work%2C%20given%20the%20excellent%20scalability%20of%20web%20data%2C%0Awe%20consider%20self-supervised%20pre-training%20on%20noisy%20web%20sourced%20image-text%20paired%0Adata.%20First%2C%20we%20conduct%20a%20benchmark%20study%20of%20representative%20self-supervised%0Apre-training%20methods%20on%20large-scale%20web%20data%20in%20a%20like-for-like%20setting.%20We%0Acompare%20a%20range%20of%20methods%2C%20including%20single-modal%20ones%20that%20use%20masked%0Atraining%20objectives%20and%20multi-modal%20ones%20that%20use%20image-text%20constrastive%0Atraining.%20We%20observe%20that%20existing%20multi-modal%20methods%20do%20not%20outperform%20their%0Asingle-modal%20counterparts%20on%20vision%20transfer%20learning%20tasks.%20We%20derive%20an%0Ainformation-theoretical%20view%20to%20explain%20these%20benchmark%20results%2C%20which%20provides%0Ainsight%20into%20how%20to%20design%20a%20novel%20vision%20learner.%20Inspired%20by%20this%20insight%2C%20we%0Apresent%20a%20new%20visual%20representation%20pre-training%20method%2C%20MUlti-modal%0AGenerator~%28MUG%29%2C%20that%20learns%20from%20scalable%20web%20sourced%20image-text%20data.%20MUG%0Aachieves%20state-of-the-art%20transfer%20performance%20on%20a%20variety%20of%20tasks%20and%0Ademonstrates%20promising%20scaling%20properties.%20Pre-trained%20models%20and%20code%20will%20be%0Amade%20public%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.07088v3&entry.124074799=Read"},
{"title": "Clustering and Mining Accented Speech for Inclusive and Fair Speech\n  Recognition", "author": "Jaeyoung Kim and Han Lu and Soheil Khorram and Anshuman Tripathi and Qian Zhang and Hasim Sak", "abstract": "  Modern automatic speech recognition (ASR) systems are typically trained on\nmore than tens of thousands hours of speech data, which is one of the main\nfactors for their great success. However, the distribution of such data is\ntypically biased towards common accents or typical speech patterns. As a\nresult, those systems often poorly perform on atypical accented speech. In this\npaper, we present accent clustering and mining schemes for fair speech\nrecognition systems which can perform equally well on under-represented\naccented speech. For accent recognition, we applied three schemes to overcome\nlimited size of supervised accent data: supervised or unsupervised\npre-training, distributionally robust optimization (DRO) and unsupervised\nclustering. Three schemes can significantly improve the accent recognition\nmodel especially for unbalanced and small accented speech. Fine-tuning ASR on\nthe mined Indian accent speech using the proposed supervised or unsupervised\nclustering schemes showed 10.0% and 5.3% relative improvements compared to\nfine-tuning on the randomly sampled speech, respectively.\n", "link": "http://arxiv.org/abs/2408.02582v1", "date": "2024-08-05", "relevancy": 2.2229, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4887}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4299}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20and%20Mining%20Accented%20Speech%20for%20Inclusive%20and%20Fair%20Speech%0A%20%20Recognition&body=Title%3A%20Clustering%20and%20Mining%20Accented%20Speech%20for%20Inclusive%20and%20Fair%20Speech%0A%20%20Recognition%0AAuthor%3A%20Jaeyoung%20Kim%20and%20Han%20Lu%20and%20Soheil%20Khorram%20and%20Anshuman%20Tripathi%20and%20Qian%20Zhang%20and%20Hasim%20Sak%0AAbstract%3A%20%20%20Modern%20automatic%20speech%20recognition%20%28ASR%29%20systems%20are%20typically%20trained%20on%0Amore%20than%20tens%20of%20thousands%20hours%20of%20speech%20data%2C%20which%20is%20one%20of%20the%20main%0Afactors%20for%20their%20great%20success.%20However%2C%20the%20distribution%20of%20such%20data%20is%0Atypically%20biased%20towards%20common%20accents%20or%20typical%20speech%20patterns.%20As%20a%0Aresult%2C%20those%20systems%20often%20poorly%20perform%20on%20atypical%20accented%20speech.%20In%20this%0Apaper%2C%20we%20present%20accent%20clustering%20and%20mining%20schemes%20for%20fair%20speech%0Arecognition%20systems%20which%20can%20perform%20equally%20well%20on%20under-represented%0Aaccented%20speech.%20For%20accent%20recognition%2C%20we%20applied%20three%20schemes%20to%20overcome%0Alimited%20size%20of%20supervised%20accent%20data%3A%20supervised%20or%20unsupervised%0Apre-training%2C%20distributionally%20robust%20optimization%20%28DRO%29%20and%20unsupervised%0Aclustering.%20Three%20schemes%20can%20significantly%20improve%20the%20accent%20recognition%0Amodel%20especially%20for%20unbalanced%20and%20small%20accented%20speech.%20Fine-tuning%20ASR%20on%0Athe%20mined%20Indian%20accent%20speech%20using%20the%20proposed%20supervised%20or%20unsupervised%0Aclustering%20schemes%20showed%2010.0%25%20and%205.3%25%20relative%20improvements%20compared%20to%0Afine-tuning%20on%20the%20randomly%20sampled%20speech%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520and%2520Mining%2520Accented%2520Speech%2520for%2520Inclusive%2520and%2520Fair%2520Speech%250A%2520%2520Recognition%26entry.906535625%3DJaeyoung%2520Kim%2520and%2520Han%2520Lu%2520and%2520Soheil%2520Khorram%2520and%2520Anshuman%2520Tripathi%2520and%2520Qian%2520Zhang%2520and%2520Hasim%2520Sak%26entry.1292438233%3D%2520%2520Modern%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems%2520are%2520typically%2520trained%2520on%250Amore%2520than%2520tens%2520of%2520thousands%2520hours%2520of%2520speech%2520data%252C%2520which%2520is%2520one%2520of%2520the%2520main%250Afactors%2520for%2520their%2520great%2520success.%2520However%252C%2520the%2520distribution%2520of%2520such%2520data%2520is%250Atypically%2520biased%2520towards%2520common%2520accents%2520or%2520typical%2520speech%2520patterns.%2520As%2520a%250Aresult%252C%2520those%2520systems%2520often%2520poorly%2520perform%2520on%2520atypical%2520accented%2520speech.%2520In%2520this%250Apaper%252C%2520we%2520present%2520accent%2520clustering%2520and%2520mining%2520schemes%2520for%2520fair%2520speech%250Arecognition%2520systems%2520which%2520can%2520perform%2520equally%2520well%2520on%2520under-represented%250Aaccented%2520speech.%2520For%2520accent%2520recognition%252C%2520we%2520applied%2520three%2520schemes%2520to%2520overcome%250Alimited%2520size%2520of%2520supervised%2520accent%2520data%253A%2520supervised%2520or%2520unsupervised%250Apre-training%252C%2520distributionally%2520robust%2520optimization%2520%2528DRO%2529%2520and%2520unsupervised%250Aclustering.%2520Three%2520schemes%2520can%2520significantly%2520improve%2520the%2520accent%2520recognition%250Amodel%2520especially%2520for%2520unbalanced%2520and%2520small%2520accented%2520speech.%2520Fine-tuning%2520ASR%2520on%250Athe%2520mined%2520Indian%2520accent%2520speech%2520using%2520the%2520proposed%2520supervised%2520or%2520unsupervised%250Aclustering%2520schemes%2520showed%252010.0%2525%2520and%25205.3%2525%2520relative%2520improvements%2520compared%2520to%250Afine-tuning%2520on%2520the%2520randomly%2520sampled%2520speech%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20and%20Mining%20Accented%20Speech%20for%20Inclusive%20and%20Fair%20Speech%0A%20%20Recognition&entry.906535625=Jaeyoung%20Kim%20and%20Han%20Lu%20and%20Soheil%20Khorram%20and%20Anshuman%20Tripathi%20and%20Qian%20Zhang%20and%20Hasim%20Sak&entry.1292438233=%20%20Modern%20automatic%20speech%20recognition%20%28ASR%29%20systems%20are%20typically%20trained%20on%0Amore%20than%20tens%20of%20thousands%20hours%20of%20speech%20data%2C%20which%20is%20one%20of%20the%20main%0Afactors%20for%20their%20great%20success.%20However%2C%20the%20distribution%20of%20such%20data%20is%0Atypically%20biased%20towards%20common%20accents%20or%20typical%20speech%20patterns.%20As%20a%0Aresult%2C%20those%20systems%20often%20poorly%20perform%20on%20atypical%20accented%20speech.%20In%20this%0Apaper%2C%20we%20present%20accent%20clustering%20and%20mining%20schemes%20for%20fair%20speech%0Arecognition%20systems%20which%20can%20perform%20equally%20well%20on%20under-represented%0Aaccented%20speech.%20For%20accent%20recognition%2C%20we%20applied%20three%20schemes%20to%20overcome%0Alimited%20size%20of%20supervised%20accent%20data%3A%20supervised%20or%20unsupervised%0Apre-training%2C%20distributionally%20robust%20optimization%20%28DRO%29%20and%20unsupervised%0Aclustering.%20Three%20schemes%20can%20significantly%20improve%20the%20accent%20recognition%0Amodel%20especially%20for%20unbalanced%20and%20small%20accented%20speech.%20Fine-tuning%20ASR%20on%0Athe%20mined%20Indian%20accent%20speech%20using%20the%20proposed%20supervised%20or%20unsupervised%0Aclustering%20schemes%20showed%2010.0%25%20and%205.3%25%20relative%20improvements%20compared%20to%0Afine-tuning%20on%20the%20randomly%20sampled%20speech%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02582v1&entry.124074799=Read"},
{"title": "APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth\n  Estimation for Autonomous Navigation", "author": "Amira Guesmi and Muhammad Abdullah Hanif and Ihsen Alouani and Muhammad Shafique", "abstract": "  In recent times, monocular depth estimation (MDE) has experienced significant\nadvancements in performance, largely attributed to the integration of\ninnovative architectures, i.e., convolutional neural networks (CNNs) and\nTransformers. Nevertheless, the susceptibility of these models to adversarial\nattacks has emerged as a noteworthy concern, especially in domains where safety\nand security are paramount. This concern holds particular weight for MDE due to\nits critical role in applications like autonomous driving and robotic\nnavigation, where accurate scene understanding is pivotal. To assess the\nvulnerability of CNN-based depth prediction methods, recent work tries to\ndesign adversarial patches against MDE. However, the existing approaches fall\nshort of inducing a comprehensive and substantially disruptive impact on the\nvision system. Instead, their influence is partial and confined to specific\nlocal areas. These methods lead to erroneous depth predictions only within the\noverlapping region with the input image, without considering the\ncharacteristics of the target object, such as its size, shape, and position. In\nthis paper, we introduce a novel adversarial patch named APARATE. This patch\npossesses the ability to selectively undermine MDE in two distinct ways: by\ndistorting the estimated distances or by creating the illusion of an object\ndisappearing from the perspective of the autonomous system. Notably, APARATE is\ndesigned to be sensitive to the shape and scale of the target object, and its\ninfluence extends beyond immediate proximity. APARATE, results in a mean depth\nestimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of\nthe targeted region when applied to CNN-based MDE models. Furthermore, it\nyields a significant error of $0.34$ and exerts substantial influence over\n$94\\%$ of the target region in the context of Transformer-based MDE.\n", "link": "http://arxiv.org/abs/2303.01351v3", "date": "2024-08-05", "relevancy": 2.1977, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.57}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5452}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APARATE%3A%20Adaptive%20Adversarial%20Patch%20for%20CNN-based%20Monocular%20Depth%0A%20%20Estimation%20for%20Autonomous%20Navigation&body=Title%3A%20APARATE%3A%20Adaptive%20Adversarial%20Patch%20for%20CNN-based%20Monocular%20Depth%0A%20%20Estimation%20for%20Autonomous%20Navigation%0AAuthor%3A%20Amira%20Guesmi%20and%20Muhammad%20Abdullah%20Hanif%20and%20Ihsen%20Alouani%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20In%20recent%20times%2C%20monocular%20depth%20estimation%20%28MDE%29%20has%20experienced%20significant%0Aadvancements%20in%20performance%2C%20largely%20attributed%20to%20the%20integration%20of%0Ainnovative%20architectures%2C%20i.e.%2C%20convolutional%20neural%20networks%20%28CNNs%29%20and%0ATransformers.%20Nevertheless%2C%20the%20susceptibility%20of%20these%20models%20to%20adversarial%0Aattacks%20has%20emerged%20as%20a%20noteworthy%20concern%2C%20especially%20in%20domains%20where%20safety%0Aand%20security%20are%20paramount.%20This%20concern%20holds%20particular%20weight%20for%20MDE%20due%20to%0Aits%20critical%20role%20in%20applications%20like%20autonomous%20driving%20and%20robotic%0Anavigation%2C%20where%20accurate%20scene%20understanding%20is%20pivotal.%20To%20assess%20the%0Avulnerability%20of%20CNN-based%20depth%20prediction%20methods%2C%20recent%20work%20tries%20to%0Adesign%20adversarial%20patches%20against%20MDE.%20However%2C%20the%20existing%20approaches%20fall%0Ashort%20of%20inducing%20a%20comprehensive%20and%20substantially%20disruptive%20impact%20on%20the%0Avision%20system.%20Instead%2C%20their%20influence%20is%20partial%20and%20confined%20to%20specific%0Alocal%20areas.%20These%20methods%20lead%20to%20erroneous%20depth%20predictions%20only%20within%20the%0Aoverlapping%20region%20with%20the%20input%20image%2C%20without%20considering%20the%0Acharacteristics%20of%20the%20target%20object%2C%20such%20as%20its%20size%2C%20shape%2C%20and%20position.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20adversarial%20patch%20named%20APARATE.%20This%20patch%0Apossesses%20the%20ability%20to%20selectively%20undermine%20MDE%20in%20two%20distinct%20ways%3A%20by%0Adistorting%20the%20estimated%20distances%20or%20by%20creating%20the%20illusion%20of%20an%20object%0Adisappearing%20from%20the%20perspective%20of%20the%20autonomous%20system.%20Notably%2C%20APARATE%20is%0Adesigned%20to%20be%20sensitive%20to%20the%20shape%20and%20scale%20of%20the%20target%20object%2C%20and%20its%0Ainfluence%20extends%20beyond%20immediate%20proximity.%20APARATE%2C%20results%20in%20a%20mean%20depth%0Aestimation%20error%20surpassing%20%240.5%24%2C%20significantly%20impacting%20as%20much%20as%20%2499%5C%25%24%20of%0Athe%20targeted%20region%20when%20applied%20to%20CNN-based%20MDE%20models.%20Furthermore%2C%20it%0Ayields%20a%20significant%20error%20of%20%240.34%24%20and%20exerts%20substantial%20influence%20over%0A%2494%5C%25%24%20of%20the%20target%20region%20in%20the%20context%20of%20Transformer-based%20MDE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.01351v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPARATE%253A%2520Adaptive%2520Adversarial%2520Patch%2520for%2520CNN-based%2520Monocular%2520Depth%250A%2520%2520Estimation%2520for%2520Autonomous%2520Navigation%26entry.906535625%3DAmira%2520Guesmi%2520and%2520Muhammad%2520Abdullah%2520Hanif%2520and%2520Ihsen%2520Alouani%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520monocular%2520depth%2520estimation%2520%2528MDE%2529%2520has%2520experienced%2520significant%250Aadvancements%2520in%2520performance%252C%2520largely%2520attributed%2520to%2520the%2520integration%2520of%250Ainnovative%2520architectures%252C%2520i.e.%252C%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%250ATransformers.%2520Nevertheless%252C%2520the%2520susceptibility%2520of%2520these%2520models%2520to%2520adversarial%250Aattacks%2520has%2520emerged%2520as%2520a%2520noteworthy%2520concern%252C%2520especially%2520in%2520domains%2520where%2520safety%250Aand%2520security%2520are%2520paramount.%2520This%2520concern%2520holds%2520particular%2520weight%2520for%2520MDE%2520due%2520to%250Aits%2520critical%2520role%2520in%2520applications%2520like%2520autonomous%2520driving%2520and%2520robotic%250Anavigation%252C%2520where%2520accurate%2520scene%2520understanding%2520is%2520pivotal.%2520To%2520assess%2520the%250Avulnerability%2520of%2520CNN-based%2520depth%2520prediction%2520methods%252C%2520recent%2520work%2520tries%2520to%250Adesign%2520adversarial%2520patches%2520against%2520MDE.%2520However%252C%2520the%2520existing%2520approaches%2520fall%250Ashort%2520of%2520inducing%2520a%2520comprehensive%2520and%2520substantially%2520disruptive%2520impact%2520on%2520the%250Avision%2520system.%2520Instead%252C%2520their%2520influence%2520is%2520partial%2520and%2520confined%2520to%2520specific%250Alocal%2520areas.%2520These%2520methods%2520lead%2520to%2520erroneous%2520depth%2520predictions%2520only%2520within%2520the%250Aoverlapping%2520region%2520with%2520the%2520input%2520image%252C%2520without%2520considering%2520the%250Acharacteristics%2520of%2520the%2520target%2520object%252C%2520such%2520as%2520its%2520size%252C%2520shape%252C%2520and%2520position.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520adversarial%2520patch%2520named%2520APARATE.%2520This%2520patch%250Apossesses%2520the%2520ability%2520to%2520selectively%2520undermine%2520MDE%2520in%2520two%2520distinct%2520ways%253A%2520by%250Adistorting%2520the%2520estimated%2520distances%2520or%2520by%2520creating%2520the%2520illusion%2520of%2520an%2520object%250Adisappearing%2520from%2520the%2520perspective%2520of%2520the%2520autonomous%2520system.%2520Notably%252C%2520APARATE%2520is%250Adesigned%2520to%2520be%2520sensitive%2520to%2520the%2520shape%2520and%2520scale%2520of%2520the%2520target%2520object%252C%2520and%2520its%250Ainfluence%2520extends%2520beyond%2520immediate%2520proximity.%2520APARATE%252C%2520results%2520in%2520a%2520mean%2520depth%250Aestimation%2520error%2520surpassing%2520%25240.5%2524%252C%2520significantly%2520impacting%2520as%2520much%2520as%2520%252499%255C%2525%2524%2520of%250Athe%2520targeted%2520region%2520when%2520applied%2520to%2520CNN-based%2520MDE%2520models.%2520Furthermore%252C%2520it%250Ayields%2520a%2520significant%2520error%2520of%2520%25240.34%2524%2520and%2520exerts%2520substantial%2520influence%2520over%250A%252494%255C%2525%2524%2520of%2520the%2520target%2520region%2520in%2520the%2520context%2520of%2520Transformer-based%2520MDE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.01351v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APARATE%3A%20Adaptive%20Adversarial%20Patch%20for%20CNN-based%20Monocular%20Depth%0A%20%20Estimation%20for%20Autonomous%20Navigation&entry.906535625=Amira%20Guesmi%20and%20Muhammad%20Abdullah%20Hanif%20and%20Ihsen%20Alouani%20and%20Muhammad%20Shafique&entry.1292438233=%20%20In%20recent%20times%2C%20monocular%20depth%20estimation%20%28MDE%29%20has%20experienced%20significant%0Aadvancements%20in%20performance%2C%20largely%20attributed%20to%20the%20integration%20of%0Ainnovative%20architectures%2C%20i.e.%2C%20convolutional%20neural%20networks%20%28CNNs%29%20and%0ATransformers.%20Nevertheless%2C%20the%20susceptibility%20of%20these%20models%20to%20adversarial%0Aattacks%20has%20emerged%20as%20a%20noteworthy%20concern%2C%20especially%20in%20domains%20where%20safety%0Aand%20security%20are%20paramount.%20This%20concern%20holds%20particular%20weight%20for%20MDE%20due%20to%0Aits%20critical%20role%20in%20applications%20like%20autonomous%20driving%20and%20robotic%0Anavigation%2C%20where%20accurate%20scene%20understanding%20is%20pivotal.%20To%20assess%20the%0Avulnerability%20of%20CNN-based%20depth%20prediction%20methods%2C%20recent%20work%20tries%20to%0Adesign%20adversarial%20patches%20against%20MDE.%20However%2C%20the%20existing%20approaches%20fall%0Ashort%20of%20inducing%20a%20comprehensive%20and%20substantially%20disruptive%20impact%20on%20the%0Avision%20system.%20Instead%2C%20their%20influence%20is%20partial%20and%20confined%20to%20specific%0Alocal%20areas.%20These%20methods%20lead%20to%20erroneous%20depth%20predictions%20only%20within%20the%0Aoverlapping%20region%20with%20the%20input%20image%2C%20without%20considering%20the%0Acharacteristics%20of%20the%20target%20object%2C%20such%20as%20its%20size%2C%20shape%2C%20and%20position.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20adversarial%20patch%20named%20APARATE.%20This%20patch%0Apossesses%20the%20ability%20to%20selectively%20undermine%20MDE%20in%20two%20distinct%20ways%3A%20by%0Adistorting%20the%20estimated%20distances%20or%20by%20creating%20the%20illusion%20of%20an%20object%0Adisappearing%20from%20the%20perspective%20of%20the%20autonomous%20system.%20Notably%2C%20APARATE%20is%0Adesigned%20to%20be%20sensitive%20to%20the%20shape%20and%20scale%20of%20the%20target%20object%2C%20and%20its%0Ainfluence%20extends%20beyond%20immediate%20proximity.%20APARATE%2C%20results%20in%20a%20mean%20depth%0Aestimation%20error%20surpassing%20%240.5%24%2C%20significantly%20impacting%20as%20much%20as%20%2499%5C%25%24%20of%0Athe%20targeted%20region%20when%20applied%20to%20CNN-based%20MDE%20models.%20Furthermore%2C%20it%0Ayields%20a%20significant%20error%20of%20%240.34%24%20and%20exerts%20substantial%20influence%20over%0A%2494%5C%25%24%20of%20the%20target%20region%20in%20the%20context%20of%20Transformer-based%20MDE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.01351v3&entry.124074799=Read"},
{"title": "Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales", "author": "Minghe Gao and Shuang Chen and Liang Pang and Yuan Yao and Jisheng Dang and Wenqiao Zhang and Juncheng Li and Siliang Tang and Yueting Zhuang and Tat-Seng Chua", "abstract": "  The remarkable performance of Multimodal Large Language Models (MLLMs) has\nunequivocally demonstrated their proficient understanding capabilities in\nhandling a wide array of visual tasks. Nevertheless, the opaque nature of their\nblack-box reasoning processes persists as an enigma, rendering them\nuninterpretable and struggling with hallucination. Their ability to execute\nintricate compositional reasoning tasks is also constrained, culminating in a\nstagnation of learning progression for these models. In this work, we introduce\nFact, a novel paradigm designed to generate multimodal rationales that are\nfaithful, concise, and transferable for teaching MLLMs. This paradigm utilizes\nverifiable visual programming to generate executable code guaranteeing\nfaithfulness and precision. Subsequently, through a series of operations\nincluding pruning, merging, and bridging, the rationale enhances its\nconciseness. Furthermore, we filter rationales that can be transferred to\nend-to-end paradigms from programming paradigms to guarantee transferability.\nEmpirical evidence from experiments demonstrates the superiority of our method\nacross models of varying parameter sizes, significantly enhancing their\ncompositional reasoning and generalization ability. Our approach also reduces\nhallucinations owing to its high correlation between images and text.\n", "link": "http://arxiv.org/abs/2404.11129v2", "date": "2024-08-05", "relevancy": 2.1492, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5417}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fact%20%3ATeaching%20MLLMs%20with%20Faithful%2C%20Concise%20and%20Transferable%20Rationales&body=Title%3A%20Fact%20%3ATeaching%20MLLMs%20with%20Faithful%2C%20Concise%20and%20Transferable%20Rationales%0AAuthor%3A%20Minghe%20Gao%20and%20Shuang%20Chen%20and%20Liang%20Pang%20and%20Yuan%20Yao%20and%20Jisheng%20Dang%20and%20Wenqiao%20Zhang%20and%20Juncheng%20Li%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20The%20remarkable%20performance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%0Aunequivocally%20demonstrated%20their%20proficient%20understanding%20capabilities%20in%0Ahandling%20a%20wide%20array%20of%20visual%20tasks.%20Nevertheless%2C%20the%20opaque%20nature%20of%20their%0Ablack-box%20reasoning%20processes%20persists%20as%20an%20enigma%2C%20rendering%20them%0Auninterpretable%20and%20struggling%20with%20hallucination.%20Their%20ability%20to%20execute%0Aintricate%20compositional%20reasoning%20tasks%20is%20also%20constrained%2C%20culminating%20in%20a%0Astagnation%20of%20learning%20progression%20for%20these%20models.%20In%20this%20work%2C%20we%20introduce%0AFact%2C%20a%20novel%20paradigm%20designed%20to%20generate%20multimodal%20rationales%20that%20are%0Afaithful%2C%20concise%2C%20and%20transferable%20for%20teaching%20MLLMs.%20This%20paradigm%20utilizes%0Averifiable%20visual%20programming%20to%20generate%20executable%20code%20guaranteeing%0Afaithfulness%20and%20precision.%20Subsequently%2C%20through%20a%20series%20of%20operations%0Aincluding%20pruning%2C%20merging%2C%20and%20bridging%2C%20the%20rationale%20enhances%20its%0Aconciseness.%20Furthermore%2C%20we%20filter%20rationales%20that%20can%20be%20transferred%20to%0Aend-to-end%20paradigms%20from%20programming%20paradigms%20to%20guarantee%20transferability.%0AEmpirical%20evidence%20from%20experiments%20demonstrates%20the%20superiority%20of%20our%20method%0Aacross%20models%20of%20varying%20parameter%20sizes%2C%20significantly%20enhancing%20their%0Acompositional%20reasoning%20and%20generalization%20ability.%20Our%20approach%20also%20reduces%0Ahallucinations%20owing%20to%20its%20high%20correlation%20between%20images%20and%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFact%2520%253ATeaching%2520MLLMs%2520with%2520Faithful%252C%2520Concise%2520and%2520Transferable%2520Rationales%26entry.906535625%3DMinghe%2520Gao%2520and%2520Shuang%2520Chen%2520and%2520Liang%2520Pang%2520and%2520Yuan%2520Yao%2520and%2520Jisheng%2520Dang%2520and%2520Wenqiao%2520Zhang%2520and%2520Juncheng%2520Li%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520The%2520remarkable%2520performance%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%250Aunequivocally%2520demonstrated%2520their%2520proficient%2520understanding%2520capabilities%2520in%250Ahandling%2520a%2520wide%2520array%2520of%2520visual%2520tasks.%2520Nevertheless%252C%2520the%2520opaque%2520nature%2520of%2520their%250Ablack-box%2520reasoning%2520processes%2520persists%2520as%2520an%2520enigma%252C%2520rendering%2520them%250Auninterpretable%2520and%2520struggling%2520with%2520hallucination.%2520Their%2520ability%2520to%2520execute%250Aintricate%2520compositional%2520reasoning%2520tasks%2520is%2520also%2520constrained%252C%2520culminating%2520in%2520a%250Astagnation%2520of%2520learning%2520progression%2520for%2520these%2520models.%2520In%2520this%2520work%252C%2520we%2520introduce%250AFact%252C%2520a%2520novel%2520paradigm%2520designed%2520to%2520generate%2520multimodal%2520rationales%2520that%2520are%250Afaithful%252C%2520concise%252C%2520and%2520transferable%2520for%2520teaching%2520MLLMs.%2520This%2520paradigm%2520utilizes%250Averifiable%2520visual%2520programming%2520to%2520generate%2520executable%2520code%2520guaranteeing%250Afaithfulness%2520and%2520precision.%2520Subsequently%252C%2520through%2520a%2520series%2520of%2520operations%250Aincluding%2520pruning%252C%2520merging%252C%2520and%2520bridging%252C%2520the%2520rationale%2520enhances%2520its%250Aconciseness.%2520Furthermore%252C%2520we%2520filter%2520rationales%2520that%2520can%2520be%2520transferred%2520to%250Aend-to-end%2520paradigms%2520from%2520programming%2520paradigms%2520to%2520guarantee%2520transferability.%250AEmpirical%2520evidence%2520from%2520experiments%2520demonstrates%2520the%2520superiority%2520of%2520our%2520method%250Aacross%2520models%2520of%2520varying%2520parameter%2520sizes%252C%2520significantly%2520enhancing%2520their%250Acompositional%2520reasoning%2520and%2520generalization%2520ability.%2520Our%2520approach%2520also%2520reduces%250Ahallucinations%2520owing%2520to%2520its%2520high%2520correlation%2520between%2520images%2520and%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fact%20%3ATeaching%20MLLMs%20with%20Faithful%2C%20Concise%20and%20Transferable%20Rationales&entry.906535625=Minghe%20Gao%20and%20Shuang%20Chen%20and%20Liang%20Pang%20and%20Yuan%20Yao%20and%20Jisheng%20Dang%20and%20Wenqiao%20Zhang%20and%20Juncheng%20Li%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20The%20remarkable%20performance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%0Aunequivocally%20demonstrated%20their%20proficient%20understanding%20capabilities%20in%0Ahandling%20a%20wide%20array%20of%20visual%20tasks.%20Nevertheless%2C%20the%20opaque%20nature%20of%20their%0Ablack-box%20reasoning%20processes%20persists%20as%20an%20enigma%2C%20rendering%20them%0Auninterpretable%20and%20struggling%20with%20hallucination.%20Their%20ability%20to%20execute%0Aintricate%20compositional%20reasoning%20tasks%20is%20also%20constrained%2C%20culminating%20in%20a%0Astagnation%20of%20learning%20progression%20for%20these%20models.%20In%20this%20work%2C%20we%20introduce%0AFact%2C%20a%20novel%20paradigm%20designed%20to%20generate%20multimodal%20rationales%20that%20are%0Afaithful%2C%20concise%2C%20and%20transferable%20for%20teaching%20MLLMs.%20This%20paradigm%20utilizes%0Averifiable%20visual%20programming%20to%20generate%20executable%20code%20guaranteeing%0Afaithfulness%20and%20precision.%20Subsequently%2C%20through%20a%20series%20of%20operations%0Aincluding%20pruning%2C%20merging%2C%20and%20bridging%2C%20the%20rationale%20enhances%20its%0Aconciseness.%20Furthermore%2C%20we%20filter%20rationales%20that%20can%20be%20transferred%20to%0Aend-to-end%20paradigms%20from%20programming%20paradigms%20to%20guarantee%20transferability.%0AEmpirical%20evidence%20from%20experiments%20demonstrates%20the%20superiority%20of%20our%20method%0Aacross%20models%20of%20varying%20parameter%20sizes%2C%20significantly%20enhancing%20their%0Acompositional%20reasoning%20and%20generalization%20ability.%20Our%20approach%20also%20reduces%0Ahallucinations%20owing%20to%20its%20high%20correlation%20between%20images%20and%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11129v2&entry.124074799=Read"},
{"title": "Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples", "author": "Philipp J. R\u00f6sch and Norbert Oswald and Michaela Geierhos and Jind\u0159ich Libovick\u00fd", "abstract": "  Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.\n", "link": "http://arxiv.org/abs/2403.02875v2", "date": "2024-08-05", "relevancy": 2.149, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5704}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5515}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Conceptual%20Understanding%20in%20Multimodal%20Contrastive%20Learning%0A%20%20through%20Hard%20Negative%20Samples&body=Title%3A%20Enhancing%20Conceptual%20Understanding%20in%20Multimodal%20Contrastive%20Learning%0A%20%20through%20Hard%20Negative%20Samples%0AAuthor%3A%20Philipp%20J.%20R%C3%B6sch%20and%20Norbert%20Oswald%20and%20Michaela%20Geierhos%20and%20Jind%C5%99ich%20Libovick%C3%BD%0AAbstract%3A%20%20%20Current%20multimodal%20models%20leveraging%20contrastive%20learning%20often%20face%0Alimitations%20in%20developing%20fine-grained%20conceptual%20understanding.%20This%20is%20due%20to%0Arandom%20negative%20samples%20during%20pretraining%2C%20causing%20almost%20exclusively%20very%0Adissimilar%20concepts%20to%20be%20compared%20in%20the%20loss%20function.%20Consequently%2C%20the%0Amodels%20struggle%20with%20fine-grained%20semantic%20differences.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20novel%20pretraining%20method%20incorporating%20synthetic%20hard%0Anegative%20text%20examples.%20The%20hard%20negatives%20permute%20terms%20corresponding%20to%0Avisual%20concepts%2C%20leading%20to%20a%20more%20fine-grained%20visual%20and%20textual%20concept%0Aalignment.%20Further%2C%20we%20introduce%20InpaintCOCO%2C%20a%20new%20challenging%20dataset%20for%0Aassessing%20the%20fine-grained%20alignment%20of%20colors%2C%20objects%2C%20and%20sizes%20in%0Avision-language%20models.%20We%20created%20the%20dataset%20using%20generative%20inpainting%20from%0ACOCO%20images%20by%20changing%20the%20visual%20concepts%20so%20that%20the%20images%20no%20longer%20match%0Atheir%20original%20captions.%20Our%20results%20show%20significant%20improvements%20in%0Afine-grained%20concept%20understanding%20across%20a%20wide%20range%20of%20vision-language%0Adatasets%2C%20including%20our%20InpaintCOCO%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Conceptual%2520Understanding%2520in%2520Multimodal%2520Contrastive%2520Learning%250A%2520%2520through%2520Hard%2520Negative%2520Samples%26entry.906535625%3DPhilipp%2520J.%2520R%25C3%25B6sch%2520and%2520Norbert%2520Oswald%2520and%2520Michaela%2520Geierhos%2520and%2520Jind%25C5%2599ich%2520Libovick%25C3%25BD%26entry.1292438233%3D%2520%2520Current%2520multimodal%2520models%2520leveraging%2520contrastive%2520learning%2520often%2520face%250Alimitations%2520in%2520developing%2520fine-grained%2520conceptual%2520understanding.%2520This%2520is%2520due%2520to%250Arandom%2520negative%2520samples%2520during%2520pretraining%252C%2520causing%2520almost%2520exclusively%2520very%250Adissimilar%2520concepts%2520to%2520be%2520compared%2520in%2520the%2520loss%2520function.%2520Consequently%252C%2520the%250Amodels%2520struggle%2520with%2520fine-grained%2520semantic%2520differences.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520introduce%2520a%2520novel%2520pretraining%2520method%2520incorporating%2520synthetic%2520hard%250Anegative%2520text%2520examples.%2520The%2520hard%2520negatives%2520permute%2520terms%2520corresponding%2520to%250Avisual%2520concepts%252C%2520leading%2520to%2520a%2520more%2520fine-grained%2520visual%2520and%2520textual%2520concept%250Aalignment.%2520Further%252C%2520we%2520introduce%2520InpaintCOCO%252C%2520a%2520new%2520challenging%2520dataset%2520for%250Aassessing%2520the%2520fine-grained%2520alignment%2520of%2520colors%252C%2520objects%252C%2520and%2520sizes%2520in%250Avision-language%2520models.%2520We%2520created%2520the%2520dataset%2520using%2520generative%2520inpainting%2520from%250ACOCO%2520images%2520by%2520changing%2520the%2520visual%2520concepts%2520so%2520that%2520the%2520images%2520no%2520longer%2520match%250Atheir%2520original%2520captions.%2520Our%2520results%2520show%2520significant%2520improvements%2520in%250Afine-grained%2520concept%2520understanding%2520across%2520a%2520wide%2520range%2520of%2520vision-language%250Adatasets%252C%2520including%2520our%2520InpaintCOCO%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Conceptual%20Understanding%20in%20Multimodal%20Contrastive%20Learning%0A%20%20through%20Hard%20Negative%20Samples&entry.906535625=Philipp%20J.%20R%C3%B6sch%20and%20Norbert%20Oswald%20and%20Michaela%20Geierhos%20and%20Jind%C5%99ich%20Libovick%C3%BD&entry.1292438233=%20%20Current%20multimodal%20models%20leveraging%20contrastive%20learning%20often%20face%0Alimitations%20in%20developing%20fine-grained%20conceptual%20understanding.%20This%20is%20due%20to%0Arandom%20negative%20samples%20during%20pretraining%2C%20causing%20almost%20exclusively%20very%0Adissimilar%20concepts%20to%20be%20compared%20in%20the%20loss%20function.%20Consequently%2C%20the%0Amodels%20struggle%20with%20fine-grained%20semantic%20differences.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20novel%20pretraining%20method%20incorporating%20synthetic%20hard%0Anegative%20text%20examples.%20The%20hard%20negatives%20permute%20terms%20corresponding%20to%0Avisual%20concepts%2C%20leading%20to%20a%20more%20fine-grained%20visual%20and%20textual%20concept%0Aalignment.%20Further%2C%20we%20introduce%20InpaintCOCO%2C%20a%20new%20challenging%20dataset%20for%0Aassessing%20the%20fine-grained%20alignment%20of%20colors%2C%20objects%2C%20and%20sizes%20in%0Avision-language%20models.%20We%20created%20the%20dataset%20using%20generative%20inpainting%20from%0ACOCO%20images%20by%20changing%20the%20visual%20concepts%20so%20that%20the%20images%20no%20longer%20match%0Atheir%20original%20captions.%20Our%20results%20show%20significant%20improvements%20in%0Afine-grained%20concept%20understanding%20across%20a%20wide%20range%20of%20vision-language%0Adatasets%2C%20including%20our%20InpaintCOCO%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02875v2&entry.124074799=Read"},
{"title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft", "author": "Haobin Jiang and Junpeng Yue and Hao Luo and Ziluo Ding and Zongqing Lu", "abstract": "  One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https://github.com/PKU-RL/CLIP4MC.\n", "link": "http://arxiv.org/abs/2303.10571v2", "date": "2024-08-05", "relevancy": 2.1282, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5309}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Friendly%20Vision-Language%20Model%20for%20Minecraft&body=Title%3A%20Reinforcement%20Learning%20Friendly%20Vision-Language%20Model%20for%20Minecraft%0AAuthor%3A%20Haobin%20Jiang%20and%20Junpeng%20Yue%20and%20Hao%20Luo%20and%20Ziluo%20Ding%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20One%20of%20the%20essential%20missions%20in%20the%20AI%20research%20community%20is%20to%20build%20an%0Aautonomous%20embodied%20agent%20that%20can%20achieve%20high-level%20performance%20across%20a%20wide%0Aspectrum%20of%20tasks.%20However%2C%20acquiring%20or%20manually%20designing%20rewards%20for%20all%0Aopen-ended%20tasks%20is%20unrealistic.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20cross-modal%0Acontrastive%20learning%20framework%20architecture%2C%20CLIP4MC%2C%20aiming%20to%20learn%20a%0Areinforcement%20learning%20%28RL%29%20friendly%20vision-language%20model%20%28VLM%29%20that%20serves%20as%0Aan%20intrinsic%20reward%20function%20for%20open-ended%20tasks.%20Simply%20utilizing%20the%0Asimilarity%20between%20the%20video%20snippet%20and%20the%20language%20prompt%20is%20not%20RL-friendly%0Asince%20standard%20VLMs%20may%20only%20capture%20the%20similarity%20at%20a%20coarse%20level.%20To%0Aachieve%20RL-friendliness%2C%20we%20incorporate%20the%20task%20completion%20degree%20into%20the%20VLM%0Atraining%20objective%2C%20as%20this%20information%20can%20assist%20agents%20in%20distinguishing%20the%0Aimportance%20between%20different%20states.%20Moreover%2C%20we%20provide%20neat%20YouTube%20datasets%0Abased%20on%20the%20large-scale%20YouTube%20database%20provided%20by%20MineDojo.%20Specifically%2C%0Atwo%20rounds%20of%20filtering%20operations%20guarantee%20that%20the%20dataset%20covers%20enough%0Aessential%20information%20and%20that%20the%20video-text%20pair%20is%20highly%20correlated.%0AEmpirically%2C%20we%20demonstrate%20that%20the%20proposed%20method%20achieves%20better%0Aperformance%20on%20RL%20tasks%20compared%20with%20baselines.%20The%20code%20and%20datasets%20are%0Aavailable%20at%20https%3A//github.com/PKU-RL/CLIP4MC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10571v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Friendly%2520Vision-Language%2520Model%2520for%2520Minecraft%26entry.906535625%3DHaobin%2520Jiang%2520and%2520Junpeng%2520Yue%2520and%2520Hao%2520Luo%2520and%2520Ziluo%2520Ding%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520essential%2520missions%2520in%2520the%2520AI%2520research%2520community%2520is%2520to%2520build%2520an%250Aautonomous%2520embodied%2520agent%2520that%2520can%2520achieve%2520high-level%2520performance%2520across%2520a%2520wide%250Aspectrum%2520of%2520tasks.%2520However%252C%2520acquiring%2520or%2520manually%2520designing%2520rewards%2520for%2520all%250Aopen-ended%2520tasks%2520is%2520unrealistic.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520cross-modal%250Acontrastive%2520learning%2520framework%2520architecture%252C%2520CLIP4MC%252C%2520aiming%2520to%2520learn%2520a%250Areinforcement%2520learning%2520%2528RL%2529%2520friendly%2520vision-language%2520model%2520%2528VLM%2529%2520that%2520serves%2520as%250Aan%2520intrinsic%2520reward%2520function%2520for%2520open-ended%2520tasks.%2520Simply%2520utilizing%2520the%250Asimilarity%2520between%2520the%2520video%2520snippet%2520and%2520the%2520language%2520prompt%2520is%2520not%2520RL-friendly%250Asince%2520standard%2520VLMs%2520may%2520only%2520capture%2520the%2520similarity%2520at%2520a%2520coarse%2520level.%2520To%250Aachieve%2520RL-friendliness%252C%2520we%2520incorporate%2520the%2520task%2520completion%2520degree%2520into%2520the%2520VLM%250Atraining%2520objective%252C%2520as%2520this%2520information%2520can%2520assist%2520agents%2520in%2520distinguishing%2520the%250Aimportance%2520between%2520different%2520states.%2520Moreover%252C%2520we%2520provide%2520neat%2520YouTube%2520datasets%250Abased%2520on%2520the%2520large-scale%2520YouTube%2520database%2520provided%2520by%2520MineDojo.%2520Specifically%252C%250Atwo%2520rounds%2520of%2520filtering%2520operations%2520guarantee%2520that%2520the%2520dataset%2520covers%2520enough%250Aessential%2520information%2520and%2520that%2520the%2520video-text%2520pair%2520is%2520highly%2520correlated.%250AEmpirically%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520better%250Aperformance%2520on%2520RL%2520tasks%2520compared%2520with%2520baselines.%2520The%2520code%2520and%2520datasets%2520are%250Aavailable%2520at%2520https%253A//github.com/PKU-RL/CLIP4MC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.10571v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Friendly%20Vision-Language%20Model%20for%20Minecraft&entry.906535625=Haobin%20Jiang%20and%20Junpeng%20Yue%20and%20Hao%20Luo%20and%20Ziluo%20Ding%20and%20Zongqing%20Lu&entry.1292438233=%20%20One%20of%20the%20essential%20missions%20in%20the%20AI%20research%20community%20is%20to%20build%20an%0Aautonomous%20embodied%20agent%20that%20can%20achieve%20high-level%20performance%20across%20a%20wide%0Aspectrum%20of%20tasks.%20However%2C%20acquiring%20or%20manually%20designing%20rewards%20for%20all%0Aopen-ended%20tasks%20is%20unrealistic.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20cross-modal%0Acontrastive%20learning%20framework%20architecture%2C%20CLIP4MC%2C%20aiming%20to%20learn%20a%0Areinforcement%20learning%20%28RL%29%20friendly%20vision-language%20model%20%28VLM%29%20that%20serves%20as%0Aan%20intrinsic%20reward%20function%20for%20open-ended%20tasks.%20Simply%20utilizing%20the%0Asimilarity%20between%20the%20video%20snippet%20and%20the%20language%20prompt%20is%20not%20RL-friendly%0Asince%20standard%20VLMs%20may%20only%20capture%20the%20similarity%20at%20a%20coarse%20level.%20To%0Aachieve%20RL-friendliness%2C%20we%20incorporate%20the%20task%20completion%20degree%20into%20the%20VLM%0Atraining%20objective%2C%20as%20this%20information%20can%20assist%20agents%20in%20distinguishing%20the%0Aimportance%20between%20different%20states.%20Moreover%2C%20we%20provide%20neat%20YouTube%20datasets%0Abased%20on%20the%20large-scale%20YouTube%20database%20provided%20by%20MineDojo.%20Specifically%2C%0Atwo%20rounds%20of%20filtering%20operations%20guarantee%20that%20the%20dataset%20covers%20enough%0Aessential%20information%20and%20that%20the%20video-text%20pair%20is%20highly%20correlated.%0AEmpirically%2C%20we%20demonstrate%20that%20the%20proposed%20method%20achieves%20better%0Aperformance%20on%20RL%20tasks%20compared%20with%20baselines.%20The%20code%20and%20datasets%20are%0Aavailable%20at%20https%3A//github.com/PKU-RL/CLIP4MC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10571v2&entry.124074799=Read"},
{"title": "Partial End-to-end Reinforcement Learning for Robustness Against\n  Modelling Error in Autonomous Racing", "author": "Andrew Murdoch and Johannes Cornelius Schoeman and Hendrik Willem Jordaan", "abstract": "  In this paper, we address the issue of increasing the performance of\nreinforcement learning (RL) solutions for autonomous racing cars when\nnavigating under conditions where practical vehicle modelling errors (commonly\nknown as \\emph{model mismatches}) are present. To address this challenge, we\npropose a partial end-to-end algorithm that decouples the planning and control\ntasks. Within this framework, an RL agent generates a trajectory comprising a\npath and velocity, which is subsequently tracked using a pure pursuit steering\ncontroller and a proportional velocity controller, respectively. In contrast,\nmany current learning-based (i.e., reinforcement and imitation learning)\nalgorithms utilise an end-to-end approach whereby a deep neural network\ndirectly maps from sensor data to control commands. By leveraging the\nrobustness of a classical controller, our partial end-to-end driving algorithm\nexhibits better robustness towards model mismatches than standard end-to-end\nalgorithms.\n", "link": "http://arxiv.org/abs/2312.06406v2", "date": "2024-08-05", "relevancy": 2.1047, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial%20End-to-end%20Reinforcement%20Learning%20for%20Robustness%20Against%0A%20%20Modelling%20Error%20in%20Autonomous%20Racing&body=Title%3A%20Partial%20End-to-end%20Reinforcement%20Learning%20for%20Robustness%20Against%0A%20%20Modelling%20Error%20in%20Autonomous%20Racing%0AAuthor%3A%20Andrew%20Murdoch%20and%20Johannes%20Cornelius%20Schoeman%20and%20Hendrik%20Willem%20Jordaan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20issue%20of%20increasing%20the%20performance%20of%0Areinforcement%20learning%20%28RL%29%20solutions%20for%20autonomous%20racing%20cars%20when%0Anavigating%20under%20conditions%20where%20practical%20vehicle%20modelling%20errors%20%28commonly%0Aknown%20as%20%5Cemph%7Bmodel%20mismatches%7D%29%20are%20present.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20partial%20end-to-end%20algorithm%20that%20decouples%20the%20planning%20and%20control%0Atasks.%20Within%20this%20framework%2C%20an%20RL%20agent%20generates%20a%20trajectory%20comprising%20a%0Apath%20and%20velocity%2C%20which%20is%20subsequently%20tracked%20using%20a%20pure%20pursuit%20steering%0Acontroller%20and%20a%20proportional%20velocity%20controller%2C%20respectively.%20In%20contrast%2C%0Amany%20current%20learning-based%20%28i.e.%2C%20reinforcement%20and%20imitation%20learning%29%0Aalgorithms%20utilise%20an%20end-to-end%20approach%20whereby%20a%20deep%20neural%20network%0Adirectly%20maps%20from%20sensor%20data%20to%20control%20commands.%20By%20leveraging%20the%0Arobustness%20of%20a%20classical%20controller%2C%20our%20partial%20end-to-end%20driving%20algorithm%0Aexhibits%20better%20robustness%20towards%20model%20mismatches%20than%20standard%20end-to-end%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial%2520End-to-end%2520Reinforcement%2520Learning%2520for%2520Robustness%2520Against%250A%2520%2520Modelling%2520Error%2520in%2520Autonomous%2520Racing%26entry.906535625%3DAndrew%2520Murdoch%2520and%2520Johannes%2520Cornelius%2520Schoeman%2520and%2520Hendrik%2520Willem%2520Jordaan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520issue%2520of%2520increasing%2520the%2520performance%2520of%250Areinforcement%2520learning%2520%2528RL%2529%2520solutions%2520for%2520autonomous%2520racing%2520cars%2520when%250Anavigating%2520under%2520conditions%2520where%2520practical%2520vehicle%2520modelling%2520errors%2520%2528commonly%250Aknown%2520as%2520%255Cemph%257Bmodel%2520mismatches%257D%2529%2520are%2520present.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520partial%2520end-to-end%2520algorithm%2520that%2520decouples%2520the%2520planning%2520and%2520control%250Atasks.%2520Within%2520this%2520framework%252C%2520an%2520RL%2520agent%2520generates%2520a%2520trajectory%2520comprising%2520a%250Apath%2520and%2520velocity%252C%2520which%2520is%2520subsequently%2520tracked%2520using%2520a%2520pure%2520pursuit%2520steering%250Acontroller%2520and%2520a%2520proportional%2520velocity%2520controller%252C%2520respectively.%2520In%2520contrast%252C%250Amany%2520current%2520learning-based%2520%2528i.e.%252C%2520reinforcement%2520and%2520imitation%2520learning%2529%250Aalgorithms%2520utilise%2520an%2520end-to-end%2520approach%2520whereby%2520a%2520deep%2520neural%2520network%250Adirectly%2520maps%2520from%2520sensor%2520data%2520to%2520control%2520commands.%2520By%2520leveraging%2520the%250Arobustness%2520of%2520a%2520classical%2520controller%252C%2520our%2520partial%2520end-to-end%2520driving%2520algorithm%250Aexhibits%2520better%2520robustness%2520towards%2520model%2520mismatches%2520than%2520standard%2520end-to-end%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20End-to-end%20Reinforcement%20Learning%20for%20Robustness%20Against%0A%20%20Modelling%20Error%20in%20Autonomous%20Racing&entry.906535625=Andrew%20Murdoch%20and%20Johannes%20Cornelius%20Schoeman%20and%20Hendrik%20Willem%20Jordaan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20issue%20of%20increasing%20the%20performance%20of%0Areinforcement%20learning%20%28RL%29%20solutions%20for%20autonomous%20racing%20cars%20when%0Anavigating%20under%20conditions%20where%20practical%20vehicle%20modelling%20errors%20%28commonly%0Aknown%20as%20%5Cemph%7Bmodel%20mismatches%7D%29%20are%20present.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20partial%20end-to-end%20algorithm%20that%20decouples%20the%20planning%20and%20control%0Atasks.%20Within%20this%20framework%2C%20an%20RL%20agent%20generates%20a%20trajectory%20comprising%20a%0Apath%20and%20velocity%2C%20which%20is%20subsequently%20tracked%20using%20a%20pure%20pursuit%20steering%0Acontroller%20and%20a%20proportional%20velocity%20controller%2C%20respectively.%20In%20contrast%2C%0Amany%20current%20learning-based%20%28i.e.%2C%20reinforcement%20and%20imitation%20learning%29%0Aalgorithms%20utilise%20an%20end-to-end%20approach%20whereby%20a%20deep%20neural%20network%0Adirectly%20maps%20from%20sensor%20data%20to%20control%20commands.%20By%20leveraging%20the%0Arobustness%20of%20a%20classical%20controller%2C%20our%20partial%20end-to-end%20driving%20algorithm%0Aexhibits%20better%20robustness%20towards%20model%20mismatches%20than%20standard%20end-to-end%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06406v2&entry.124074799=Read"},
{"title": "RIs-Calib: An Open-Source Spatiotemporal Calibrator for Multiple 3D\n  Radars and IMUs Based on Continuous-Time Estimation", "author": "Shuolong Chen and Xingxing Li and Shengyu Li and Yuxuan Zhou and Shiwen Wang", "abstract": "  Aided inertial navigation system (INS), typically consisting of an inertial\nmeasurement unit (IMU) and an exteroceptive sensor, has been widely accepted as\na feasible solution for navigation. Compared with vision-aided and LiDAR-aided\nINS, radar-aided INS could achieve better performance in adverse weather\nconditions since the radar utilizes low-frequency measuring signals with less\nattenuation effect in atmospheric gases and rain. For such a radar-aided INS,\naccurate spatiotemporal transformation is a fundamental prerequisite to\nachieving optimal information fusion. In this work, we present RIs-Calib: a\nspatiotemporal calibrator for multiple 3D radars and IMUs based on\ncontinuous-time estimation, which enables accurate spatiotemporal calibration\nand does not require any additional artificial infrastructure or prior\nknowledge. Our approach starts with a rigorous and robust procedure for state\ninitialization, followed by batch optimizations, where all parameters can be\nrefined to global optimal states steadily. We validate and evaluate RIs-Calib\non both simulated and real-world experiments, and the results demonstrate that\nRIs-Calib is capable of accurate and consistent calibration. We open-source our\nimplementations at (https://github.com/Unsigned-Long/RIs-Calib) to benefit the\nresearch community.\n", "link": "http://arxiv.org/abs/2408.02444v1", "date": "2024-08-05", "relevancy": 2.087, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5267}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5247}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIs-Calib%3A%20An%20Open-Source%20Spatiotemporal%20Calibrator%20for%20Multiple%203D%0A%20%20Radars%20and%20IMUs%20Based%20on%20Continuous-Time%20Estimation&body=Title%3A%20RIs-Calib%3A%20An%20Open-Source%20Spatiotemporal%20Calibrator%20for%20Multiple%203D%0A%20%20Radars%20and%20IMUs%20Based%20on%20Continuous-Time%20Estimation%0AAuthor%3A%20Shuolong%20Chen%20and%20Xingxing%20Li%20and%20Shengyu%20Li%20and%20Yuxuan%20Zhou%20and%20Shiwen%20Wang%0AAbstract%3A%20%20%20Aided%20inertial%20navigation%20system%20%28INS%29%2C%20typically%20consisting%20of%20an%20inertial%0Ameasurement%20unit%20%28IMU%29%20and%20an%20exteroceptive%20sensor%2C%20has%20been%20widely%20accepted%20as%0Aa%20feasible%20solution%20for%20navigation.%20Compared%20with%20vision-aided%20and%20LiDAR-aided%0AINS%2C%20radar-aided%20INS%20could%20achieve%20better%20performance%20in%20adverse%20weather%0Aconditions%20since%20the%20radar%20utilizes%20low-frequency%20measuring%20signals%20with%20less%0Aattenuation%20effect%20in%20atmospheric%20gases%20and%20rain.%20For%20such%20a%20radar-aided%20INS%2C%0Aaccurate%20spatiotemporal%20transformation%20is%20a%20fundamental%20prerequisite%20to%0Aachieving%20optimal%20information%20fusion.%20In%20this%20work%2C%20we%20present%20RIs-Calib%3A%20a%0Aspatiotemporal%20calibrator%20for%20multiple%203D%20radars%20and%20IMUs%20based%20on%0Acontinuous-time%20estimation%2C%20which%20enables%20accurate%20spatiotemporal%20calibration%0Aand%20does%20not%20require%20any%20additional%20artificial%20infrastructure%20or%20prior%0Aknowledge.%20Our%20approach%20starts%20with%20a%20rigorous%20and%20robust%20procedure%20for%20state%0Ainitialization%2C%20followed%20by%20batch%20optimizations%2C%20where%20all%20parameters%20can%20be%0Arefined%20to%20global%20optimal%20states%20steadily.%20We%20validate%20and%20evaluate%20RIs-Calib%0Aon%20both%20simulated%20and%20real-world%20experiments%2C%20and%20the%20results%20demonstrate%20that%0ARIs-Calib%20is%20capable%20of%20accurate%20and%20consistent%20calibration.%20We%20open-source%20our%0Aimplementations%20at%20%28https%3A//github.com/Unsigned-Long/RIs-Calib%29%20to%20benefit%20the%0Aresearch%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIs-Calib%253A%2520An%2520Open-Source%2520Spatiotemporal%2520Calibrator%2520for%2520Multiple%25203D%250A%2520%2520Radars%2520and%2520IMUs%2520Based%2520on%2520Continuous-Time%2520Estimation%26entry.906535625%3DShuolong%2520Chen%2520and%2520Xingxing%2520Li%2520and%2520Shengyu%2520Li%2520and%2520Yuxuan%2520Zhou%2520and%2520Shiwen%2520Wang%26entry.1292438233%3D%2520%2520Aided%2520inertial%2520navigation%2520system%2520%2528INS%2529%252C%2520typically%2520consisting%2520of%2520an%2520inertial%250Ameasurement%2520unit%2520%2528IMU%2529%2520and%2520an%2520exteroceptive%2520sensor%252C%2520has%2520been%2520widely%2520accepted%2520as%250Aa%2520feasible%2520solution%2520for%2520navigation.%2520Compared%2520with%2520vision-aided%2520and%2520LiDAR-aided%250AINS%252C%2520radar-aided%2520INS%2520could%2520achieve%2520better%2520performance%2520in%2520adverse%2520weather%250Aconditions%2520since%2520the%2520radar%2520utilizes%2520low-frequency%2520measuring%2520signals%2520with%2520less%250Aattenuation%2520effect%2520in%2520atmospheric%2520gases%2520and%2520rain.%2520For%2520such%2520a%2520radar-aided%2520INS%252C%250Aaccurate%2520spatiotemporal%2520transformation%2520is%2520a%2520fundamental%2520prerequisite%2520to%250Aachieving%2520optimal%2520information%2520fusion.%2520In%2520this%2520work%252C%2520we%2520present%2520RIs-Calib%253A%2520a%250Aspatiotemporal%2520calibrator%2520for%2520multiple%25203D%2520radars%2520and%2520IMUs%2520based%2520on%250Acontinuous-time%2520estimation%252C%2520which%2520enables%2520accurate%2520spatiotemporal%2520calibration%250Aand%2520does%2520not%2520require%2520any%2520additional%2520artificial%2520infrastructure%2520or%2520prior%250Aknowledge.%2520Our%2520approach%2520starts%2520with%2520a%2520rigorous%2520and%2520robust%2520procedure%2520for%2520state%250Ainitialization%252C%2520followed%2520by%2520batch%2520optimizations%252C%2520where%2520all%2520parameters%2520can%2520be%250Arefined%2520to%2520global%2520optimal%2520states%2520steadily.%2520We%2520validate%2520and%2520evaluate%2520RIs-Calib%250Aon%2520both%2520simulated%2520and%2520real-world%2520experiments%252C%2520and%2520the%2520results%2520demonstrate%2520that%250ARIs-Calib%2520is%2520capable%2520of%2520accurate%2520and%2520consistent%2520calibration.%2520We%2520open-source%2520our%250Aimplementations%2520at%2520%2528https%253A//github.com/Unsigned-Long/RIs-Calib%2529%2520to%2520benefit%2520the%250Aresearch%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIs-Calib%3A%20An%20Open-Source%20Spatiotemporal%20Calibrator%20for%20Multiple%203D%0A%20%20Radars%20and%20IMUs%20Based%20on%20Continuous-Time%20Estimation&entry.906535625=Shuolong%20Chen%20and%20Xingxing%20Li%20and%20Shengyu%20Li%20and%20Yuxuan%20Zhou%20and%20Shiwen%20Wang&entry.1292438233=%20%20Aided%20inertial%20navigation%20system%20%28INS%29%2C%20typically%20consisting%20of%20an%20inertial%0Ameasurement%20unit%20%28IMU%29%20and%20an%20exteroceptive%20sensor%2C%20has%20been%20widely%20accepted%20as%0Aa%20feasible%20solution%20for%20navigation.%20Compared%20with%20vision-aided%20and%20LiDAR-aided%0AINS%2C%20radar-aided%20INS%20could%20achieve%20better%20performance%20in%20adverse%20weather%0Aconditions%20since%20the%20radar%20utilizes%20low-frequency%20measuring%20signals%20with%20less%0Aattenuation%20effect%20in%20atmospheric%20gases%20and%20rain.%20For%20such%20a%20radar-aided%20INS%2C%0Aaccurate%20spatiotemporal%20transformation%20is%20a%20fundamental%20prerequisite%20to%0Aachieving%20optimal%20information%20fusion.%20In%20this%20work%2C%20we%20present%20RIs-Calib%3A%20a%0Aspatiotemporal%20calibrator%20for%20multiple%203D%20radars%20and%20IMUs%20based%20on%0Acontinuous-time%20estimation%2C%20which%20enables%20accurate%20spatiotemporal%20calibration%0Aand%20does%20not%20require%20any%20additional%20artificial%20infrastructure%20or%20prior%0Aknowledge.%20Our%20approach%20starts%20with%20a%20rigorous%20and%20robust%20procedure%20for%20state%0Ainitialization%2C%20followed%20by%20batch%20optimizations%2C%20where%20all%20parameters%20can%20be%0Arefined%20to%20global%20optimal%20states%20steadily.%20We%20validate%20and%20evaluate%20RIs-Calib%0Aon%20both%20simulated%20and%20real-world%20experiments%2C%20and%20the%20results%20demonstrate%20that%0ARIs-Calib%20is%20capable%20of%20accurate%20and%20consistent%20calibration.%20We%20open-source%20our%0Aimplementations%20at%20%28https%3A//github.com/Unsigned-Long/RIs-Calib%29%20to%20benefit%20the%0Aresearch%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02444v1&entry.124074799=Read"},
{"title": "FPT+: A Parameter and Memory Efficient Transfer Learning Method for\n  High-resolution Medical Image Classification", "author": "Yijin Huang and Pujin Cheng and Roger Tam and Xiaoying Tang", "abstract": "  The success of large-scale pre-trained models has established fine-tuning as\na standard method for achieving significant improvements in downstream tasks.\nHowever, fine-tuning the entire parameter set of a pre-trained model is costly.\nParameter-efficient transfer learning (PETL) has recently emerged as a\ncost-effective alternative for adapting pre-trained models to downstream tasks.\nDespite its advantages, the increasing model size and input resolution present\nchallenges for PETL, as the training memory consumption is not reduced as\neffectively as the parameter usage. In this paper, we introduce Fine-grained\nPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medical\nimage classification, which significantly reduces memory consumption compared\nto other PETL methods. FPT+ performs transfer learning by training a\nlightweight side network and accessing pre-trained knowledge from a large\npre-trained model (LPM) through fine-grained prompts and fusion modules.\nSpecifically, we freeze the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM processes high-resolution images to extract\nfine-grained features, while the side network employs the corresponding\ndown-sampled low-resolution images to minimize the memory usage. To enable the\nside network to leverage pre-trained knowledge, we propose fine-grained prompts\nand fusion modules, which collaborate to summarize information through the\nLPM's intermediate activations. We evaluate FPT+ on eight medical image\ndatasets of varying sizes, modalities, and complexities. Experimental results\ndemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the\nlearnable parameters and 3.18% of the memory required for fine-tuning an entire\nViT-B model. Our code is available at https://github.com/YijinHuang/FPT.\n", "link": "http://arxiv.org/abs/2408.02426v1", "date": "2024-08-05", "relevancy": 2.0848, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5299}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FPT%2B%3A%20A%20Parameter%20and%20Memory%20Efficient%20Transfer%20Learning%20Method%20for%0A%20%20High-resolution%20Medical%20Image%20Classification&body=Title%3A%20FPT%2B%3A%20A%20Parameter%20and%20Memory%20Efficient%20Transfer%20Learning%20Method%20for%0A%20%20High-resolution%20Medical%20Image%20Classification%0AAuthor%3A%20Yijin%20Huang%20and%20Pujin%20Cheng%20and%20Roger%20Tam%20and%20Xiaoying%20Tang%0AAbstract%3A%20%20%20The%20success%20of%20large-scale%20pre-trained%20models%20has%20established%20fine-tuning%20as%0Aa%20standard%20method%20for%20achieving%20significant%20improvements%20in%20downstream%20tasks.%0AHowever%2C%20fine-tuning%20the%20entire%20parameter%20set%20of%20a%20pre-trained%20model%20is%20costly.%0AParameter-efficient%20transfer%20learning%20%28PETL%29%20has%20recently%20emerged%20as%20a%0Acost-effective%20alternative%20for%20adapting%20pre-trained%20models%20to%20downstream%20tasks.%0ADespite%20its%20advantages%2C%20the%20increasing%20model%20size%20and%20input%20resolution%20present%0Achallenges%20for%20PETL%2C%20as%20the%20training%20memory%20consumption%20is%20not%20reduced%20as%0Aeffectively%20as%20the%20parameter%20usage.%20In%20this%20paper%2C%20we%20introduce%20Fine-grained%0APrompt%20Tuning%20plus%20%28FPT%2B%29%2C%20a%20PETL%20method%20designed%20for%20high-resolution%20medical%0Aimage%20classification%2C%20which%20significantly%20reduces%20memory%20consumption%20compared%0Ato%20other%20PETL%20methods.%20FPT%2B%20performs%20transfer%20learning%20by%20training%20a%0Alightweight%20side%20network%20and%20accessing%20pre-trained%20knowledge%20from%20a%20large%0Apre-trained%20model%20%28LPM%29%20through%20fine-grained%20prompts%20and%20fusion%20modules.%0ASpecifically%2C%20we%20freeze%20the%20LPM%20and%20construct%20a%20learnable%20lightweight%20side%0Anetwork.%20The%20frozen%20LPM%20processes%20high-resolution%20images%20to%20extract%0Afine-grained%20features%2C%20while%20the%20side%20network%20employs%20the%20corresponding%0Adown-sampled%20low-resolution%20images%20to%20minimize%20the%20memory%20usage.%20To%20enable%20the%0Aside%20network%20to%20leverage%20pre-trained%20knowledge%2C%20we%20propose%20fine-grained%20prompts%0Aand%20fusion%20modules%2C%20which%20collaborate%20to%20summarize%20information%20through%20the%0ALPM%27s%20intermediate%20activations.%20We%20evaluate%20FPT%2B%20on%20eight%20medical%20image%0Adatasets%20of%20varying%20sizes%2C%20modalities%2C%20and%20complexities.%20Experimental%20results%0Ademonstrate%20that%20FPT%2B%20outperforms%20other%20PETL%20methods%2C%20using%20only%201.03%25%20of%20the%0Alearnable%20parameters%20and%203.18%25%20of%20the%20memory%20required%20for%20fine-tuning%20an%20entire%0AViT-B%20model.%20Our%20code%20is%20available%20at%20https%3A//github.com/YijinHuang/FPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFPT%252B%253A%2520A%2520Parameter%2520and%2520Memory%2520Efficient%2520Transfer%2520Learning%2520Method%2520for%250A%2520%2520High-resolution%2520Medical%2520Image%2520Classification%26entry.906535625%3DYijin%2520Huang%2520and%2520Pujin%2520Cheng%2520and%2520Roger%2520Tam%2520and%2520Xiaoying%2520Tang%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520large-scale%2520pre-trained%2520models%2520has%2520established%2520fine-tuning%2520as%250Aa%2520standard%2520method%2520for%2520achieving%2520significant%2520improvements%2520in%2520downstream%2520tasks.%250AHowever%252C%2520fine-tuning%2520the%2520entire%2520parameter%2520set%2520of%2520a%2520pre-trained%2520model%2520is%2520costly.%250AParameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520has%2520recently%2520emerged%2520as%2520a%250Acost-effective%2520alternative%2520for%2520adapting%2520pre-trained%2520models%2520to%2520downstream%2520tasks.%250ADespite%2520its%2520advantages%252C%2520the%2520increasing%2520model%2520size%2520and%2520input%2520resolution%2520present%250Achallenges%2520for%2520PETL%252C%2520as%2520the%2520training%2520memory%2520consumption%2520is%2520not%2520reduced%2520as%250Aeffectively%2520as%2520the%2520parameter%2520usage.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Fine-grained%250APrompt%2520Tuning%2520plus%2520%2528FPT%252B%2529%252C%2520a%2520PETL%2520method%2520designed%2520for%2520high-resolution%2520medical%250Aimage%2520classification%252C%2520which%2520significantly%2520reduces%2520memory%2520consumption%2520compared%250Ato%2520other%2520PETL%2520methods.%2520FPT%252B%2520performs%2520transfer%2520learning%2520by%2520training%2520a%250Alightweight%2520side%2520network%2520and%2520accessing%2520pre-trained%2520knowledge%2520from%2520a%2520large%250Apre-trained%2520model%2520%2528LPM%2529%2520through%2520fine-grained%2520prompts%2520and%2520fusion%2520modules.%250ASpecifically%252C%2520we%2520freeze%2520the%2520LPM%2520and%2520construct%2520a%2520learnable%2520lightweight%2520side%250Anetwork.%2520The%2520frozen%2520LPM%2520processes%2520high-resolution%2520images%2520to%2520extract%250Afine-grained%2520features%252C%2520while%2520the%2520side%2520network%2520employs%2520the%2520corresponding%250Adown-sampled%2520low-resolution%2520images%2520to%2520minimize%2520the%2520memory%2520usage.%2520To%2520enable%2520the%250Aside%2520network%2520to%2520leverage%2520pre-trained%2520knowledge%252C%2520we%2520propose%2520fine-grained%2520prompts%250Aand%2520fusion%2520modules%252C%2520which%2520collaborate%2520to%2520summarize%2520information%2520through%2520the%250ALPM%2527s%2520intermediate%2520activations.%2520We%2520evaluate%2520FPT%252B%2520on%2520eight%2520medical%2520image%250Adatasets%2520of%2520varying%2520sizes%252C%2520modalities%252C%2520and%2520complexities.%2520Experimental%2520results%250Ademonstrate%2520that%2520FPT%252B%2520outperforms%2520other%2520PETL%2520methods%252C%2520using%2520only%25201.03%2525%2520of%2520the%250Alearnable%2520parameters%2520and%25203.18%2525%2520of%2520the%2520memory%2520required%2520for%2520fine-tuning%2520an%2520entire%250AViT-B%2520model.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/YijinHuang/FPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FPT%2B%3A%20A%20Parameter%20and%20Memory%20Efficient%20Transfer%20Learning%20Method%20for%0A%20%20High-resolution%20Medical%20Image%20Classification&entry.906535625=Yijin%20Huang%20and%20Pujin%20Cheng%20and%20Roger%20Tam%20and%20Xiaoying%20Tang&entry.1292438233=%20%20The%20success%20of%20large-scale%20pre-trained%20models%20has%20established%20fine-tuning%20as%0Aa%20standard%20method%20for%20achieving%20significant%20improvements%20in%20downstream%20tasks.%0AHowever%2C%20fine-tuning%20the%20entire%20parameter%20set%20of%20a%20pre-trained%20model%20is%20costly.%0AParameter-efficient%20transfer%20learning%20%28PETL%29%20has%20recently%20emerged%20as%20a%0Acost-effective%20alternative%20for%20adapting%20pre-trained%20models%20to%20downstream%20tasks.%0ADespite%20its%20advantages%2C%20the%20increasing%20model%20size%20and%20input%20resolution%20present%0Achallenges%20for%20PETL%2C%20as%20the%20training%20memory%20consumption%20is%20not%20reduced%20as%0Aeffectively%20as%20the%20parameter%20usage.%20In%20this%20paper%2C%20we%20introduce%20Fine-grained%0APrompt%20Tuning%20plus%20%28FPT%2B%29%2C%20a%20PETL%20method%20designed%20for%20high-resolution%20medical%0Aimage%20classification%2C%20which%20significantly%20reduces%20memory%20consumption%20compared%0Ato%20other%20PETL%20methods.%20FPT%2B%20performs%20transfer%20learning%20by%20training%20a%0Alightweight%20side%20network%20and%20accessing%20pre-trained%20knowledge%20from%20a%20large%0Apre-trained%20model%20%28LPM%29%20through%20fine-grained%20prompts%20and%20fusion%20modules.%0ASpecifically%2C%20we%20freeze%20the%20LPM%20and%20construct%20a%20learnable%20lightweight%20side%0Anetwork.%20The%20frozen%20LPM%20processes%20high-resolution%20images%20to%20extract%0Afine-grained%20features%2C%20while%20the%20side%20network%20employs%20the%20corresponding%0Adown-sampled%20low-resolution%20images%20to%20minimize%20the%20memory%20usage.%20To%20enable%20the%0Aside%20network%20to%20leverage%20pre-trained%20knowledge%2C%20we%20propose%20fine-grained%20prompts%0Aand%20fusion%20modules%2C%20which%20collaborate%20to%20summarize%20information%20through%20the%0ALPM%27s%20intermediate%20activations.%20We%20evaluate%20FPT%2B%20on%20eight%20medical%20image%0Adatasets%20of%20varying%20sizes%2C%20modalities%2C%20and%20complexities.%20Experimental%20results%0Ademonstrate%20that%20FPT%2B%20outperforms%20other%20PETL%20methods%2C%20using%20only%201.03%25%20of%20the%0Alearnable%20parameters%20and%203.18%25%20of%20the%20memory%20required%20for%20fine-tuning%20an%20entire%0AViT-B%20model.%20Our%20code%20is%20available%20at%20https%3A//github.com/YijinHuang/FPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02426v1&entry.124074799=Read"},
{"title": "The Role of Functional Muscle Networks in Improving Hand Gesture\n  Perception for Human-Machine Interfaces", "author": "Costanza Armanini and Tuka Alhanai and Farah E. Shamout and S. Farokh Atashzar", "abstract": "  Developing accurate hand gesture perception models is critical for various\nrobotic applications, enabling effective communication between humans and\nmachines and directly impacting neurorobotics and interactive robots. Recently,\nsurface electromyography (sEMG) has been explored for its rich informational\ncontext and accessibility when combined with advanced machine learning\napproaches and wearable systems. The literature presents numerous approaches to\nboost performance while ensuring robustness for neurorobots using sEMG, often\nresulting in models requiring high processing power, large datasets, and less\nscalable solutions. This paper addresses this challenge by proposing the\ndecoding of muscle synchronization rather than individual muscle activation. We\nstudy coherence-based functional muscle networks as the core of our perception\nmodel, proposing that functional synchronization between muscles and the\ngraph-based network of muscle connectivity encode contextual information about\nintended hand gestures. This can be decoded using shallow machine learning\napproaches without the need for deep temporal networks. Our technique could\nimpact myoelectric control of neurorobots by reducing computational burdens and\nenhancing efficiency. The approach is benchmarked on the Ninapro database,\nwhich contains 12 EMG signals from 40 subjects performing 17 hand gestures. It\nachieves an accuracy of 85.1%, demonstrating improved performance compared to\nexisting methods while requiring much less computational power. The results\nsupport the hypothesis that a coherence-based functional muscle network encodes\ncritical information related to gesture execution, significantly enhancing hand\ngesture perception with potential applications for neurorobotic systems and\ninteractive machines.\n", "link": "http://arxiv.org/abs/2408.02547v1", "date": "2024-08-05", "relevancy": 2.0759, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5402}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5217}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Functional%20Muscle%20Networks%20in%20Improving%20Hand%20Gesture%0A%20%20Perception%20for%20Human-Machine%20Interfaces&body=Title%3A%20The%20Role%20of%20Functional%20Muscle%20Networks%20in%20Improving%20Hand%20Gesture%0A%20%20Perception%20for%20Human-Machine%20Interfaces%0AAuthor%3A%20Costanza%20Armanini%20and%20Tuka%20Alhanai%20and%20Farah%20E.%20Shamout%20and%20S.%20Farokh%20Atashzar%0AAbstract%3A%20%20%20Developing%20accurate%20hand%20gesture%20perception%20models%20is%20critical%20for%20various%0Arobotic%20applications%2C%20enabling%20effective%20communication%20between%20humans%20and%0Amachines%20and%20directly%20impacting%20neurorobotics%20and%20interactive%20robots.%20Recently%2C%0Asurface%20electromyography%20%28sEMG%29%20has%20been%20explored%20for%20its%20rich%20informational%0Acontext%20and%20accessibility%20when%20combined%20with%20advanced%20machine%20learning%0Aapproaches%20and%20wearable%20systems.%20The%20literature%20presents%20numerous%20approaches%20to%0Aboost%20performance%20while%20ensuring%20robustness%20for%20neurorobots%20using%20sEMG%2C%20often%0Aresulting%20in%20models%20requiring%20high%20processing%20power%2C%20large%20datasets%2C%20and%20less%0Ascalable%20solutions.%20This%20paper%20addresses%20this%20challenge%20by%20proposing%20the%0Adecoding%20of%20muscle%20synchronization%20rather%20than%20individual%20muscle%20activation.%20We%0Astudy%20coherence-based%20functional%20muscle%20networks%20as%20the%20core%20of%20our%20perception%0Amodel%2C%20proposing%20that%20functional%20synchronization%20between%20muscles%20and%20the%0Agraph-based%20network%20of%20muscle%20connectivity%20encode%20contextual%20information%20about%0Aintended%20hand%20gestures.%20This%20can%20be%20decoded%20using%20shallow%20machine%20learning%0Aapproaches%20without%20the%20need%20for%20deep%20temporal%20networks.%20Our%20technique%20could%0Aimpact%20myoelectric%20control%20of%20neurorobots%20by%20reducing%20computational%20burdens%20and%0Aenhancing%20efficiency.%20The%20approach%20is%20benchmarked%20on%20the%20Ninapro%20database%2C%0Awhich%20contains%2012%20EMG%20signals%20from%2040%20subjects%20performing%2017%20hand%20gestures.%20It%0Aachieves%20an%20accuracy%20of%2085.1%25%2C%20demonstrating%20improved%20performance%20compared%20to%0Aexisting%20methods%20while%20requiring%20much%20less%20computational%20power.%20The%20results%0Asupport%20the%20hypothesis%20that%20a%20coherence-based%20functional%20muscle%20network%20encodes%0Acritical%20information%20related%20to%20gesture%20execution%2C%20significantly%20enhancing%20hand%0Agesture%20perception%20with%20potential%20applications%20for%20neurorobotic%20systems%20and%0Ainteractive%20machines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Functional%2520Muscle%2520Networks%2520in%2520Improving%2520Hand%2520Gesture%250A%2520%2520Perception%2520for%2520Human-Machine%2520Interfaces%26entry.906535625%3DCostanza%2520Armanini%2520and%2520Tuka%2520Alhanai%2520and%2520Farah%2520E.%2520Shamout%2520and%2520S.%2520Farokh%2520Atashzar%26entry.1292438233%3D%2520%2520Developing%2520accurate%2520hand%2520gesture%2520perception%2520models%2520is%2520critical%2520for%2520various%250Arobotic%2520applications%252C%2520enabling%2520effective%2520communication%2520between%2520humans%2520and%250Amachines%2520and%2520directly%2520impacting%2520neurorobotics%2520and%2520interactive%2520robots.%2520Recently%252C%250Asurface%2520electromyography%2520%2528sEMG%2529%2520has%2520been%2520explored%2520for%2520its%2520rich%2520informational%250Acontext%2520and%2520accessibility%2520when%2520combined%2520with%2520advanced%2520machine%2520learning%250Aapproaches%2520and%2520wearable%2520systems.%2520The%2520literature%2520presents%2520numerous%2520approaches%2520to%250Aboost%2520performance%2520while%2520ensuring%2520robustness%2520for%2520neurorobots%2520using%2520sEMG%252C%2520often%250Aresulting%2520in%2520models%2520requiring%2520high%2520processing%2520power%252C%2520large%2520datasets%252C%2520and%2520less%250Ascalable%2520solutions.%2520This%2520paper%2520addresses%2520this%2520challenge%2520by%2520proposing%2520the%250Adecoding%2520of%2520muscle%2520synchronization%2520rather%2520than%2520individual%2520muscle%2520activation.%2520We%250Astudy%2520coherence-based%2520functional%2520muscle%2520networks%2520as%2520the%2520core%2520of%2520our%2520perception%250Amodel%252C%2520proposing%2520that%2520functional%2520synchronization%2520between%2520muscles%2520and%2520the%250Agraph-based%2520network%2520of%2520muscle%2520connectivity%2520encode%2520contextual%2520information%2520about%250Aintended%2520hand%2520gestures.%2520This%2520can%2520be%2520decoded%2520using%2520shallow%2520machine%2520learning%250Aapproaches%2520without%2520the%2520need%2520for%2520deep%2520temporal%2520networks.%2520Our%2520technique%2520could%250Aimpact%2520myoelectric%2520control%2520of%2520neurorobots%2520by%2520reducing%2520computational%2520burdens%2520and%250Aenhancing%2520efficiency.%2520The%2520approach%2520is%2520benchmarked%2520on%2520the%2520Ninapro%2520database%252C%250Awhich%2520contains%252012%2520EMG%2520signals%2520from%252040%2520subjects%2520performing%252017%2520hand%2520gestures.%2520It%250Aachieves%2520an%2520accuracy%2520of%252085.1%2525%252C%2520demonstrating%2520improved%2520performance%2520compared%2520to%250Aexisting%2520methods%2520while%2520requiring%2520much%2520less%2520computational%2520power.%2520The%2520results%250Asupport%2520the%2520hypothesis%2520that%2520a%2520coherence-based%2520functional%2520muscle%2520network%2520encodes%250Acritical%2520information%2520related%2520to%2520gesture%2520execution%252C%2520significantly%2520enhancing%2520hand%250Agesture%2520perception%2520with%2520potential%2520applications%2520for%2520neurorobotic%2520systems%2520and%250Ainteractive%2520machines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Functional%20Muscle%20Networks%20in%20Improving%20Hand%20Gesture%0A%20%20Perception%20for%20Human-Machine%20Interfaces&entry.906535625=Costanza%20Armanini%20and%20Tuka%20Alhanai%20and%20Farah%20E.%20Shamout%20and%20S.%20Farokh%20Atashzar&entry.1292438233=%20%20Developing%20accurate%20hand%20gesture%20perception%20models%20is%20critical%20for%20various%0Arobotic%20applications%2C%20enabling%20effective%20communication%20between%20humans%20and%0Amachines%20and%20directly%20impacting%20neurorobotics%20and%20interactive%20robots.%20Recently%2C%0Asurface%20electromyography%20%28sEMG%29%20has%20been%20explored%20for%20its%20rich%20informational%0Acontext%20and%20accessibility%20when%20combined%20with%20advanced%20machine%20learning%0Aapproaches%20and%20wearable%20systems.%20The%20literature%20presents%20numerous%20approaches%20to%0Aboost%20performance%20while%20ensuring%20robustness%20for%20neurorobots%20using%20sEMG%2C%20often%0Aresulting%20in%20models%20requiring%20high%20processing%20power%2C%20large%20datasets%2C%20and%20less%0Ascalable%20solutions.%20This%20paper%20addresses%20this%20challenge%20by%20proposing%20the%0Adecoding%20of%20muscle%20synchronization%20rather%20than%20individual%20muscle%20activation.%20We%0Astudy%20coherence-based%20functional%20muscle%20networks%20as%20the%20core%20of%20our%20perception%0Amodel%2C%20proposing%20that%20functional%20synchronization%20between%20muscles%20and%20the%0Agraph-based%20network%20of%20muscle%20connectivity%20encode%20contextual%20information%20about%0Aintended%20hand%20gestures.%20This%20can%20be%20decoded%20using%20shallow%20machine%20learning%0Aapproaches%20without%20the%20need%20for%20deep%20temporal%20networks.%20Our%20technique%20could%0Aimpact%20myoelectric%20control%20of%20neurorobots%20by%20reducing%20computational%20burdens%20and%0Aenhancing%20efficiency.%20The%20approach%20is%20benchmarked%20on%20the%20Ninapro%20database%2C%0Awhich%20contains%2012%20EMG%20signals%20from%2040%20subjects%20performing%2017%20hand%20gestures.%20It%0Aachieves%20an%20accuracy%20of%2085.1%25%2C%20demonstrating%20improved%20performance%20compared%20to%0Aexisting%20methods%20while%20requiring%20much%20less%20computational%20power.%20The%20results%0Asupport%20the%20hypothesis%20that%20a%20coherence-based%20functional%20muscle%20network%20encodes%0Acritical%20information%20related%20to%20gesture%20execution%2C%20significantly%20enhancing%20hand%0Agesture%20perception%20with%20potential%20applications%20for%20neurorobotic%20systems%20and%0Ainteractive%20machines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02547v1&entry.124074799=Read"},
{"title": "HQOD: Harmonious Quantization for Object Detection", "author": "Long Huang and Zhiwei Dong and Song-Lu Chen and Ruiyao Zhang and Shutong Ti and Feng Chen and Xu-Cheng Yin", "abstract": "  Task inharmony problem commonly occurs in modern object detectors, leading to\ninconsistent qualities between classification and regression tasks. The\npredicted boxes with high classification scores but poor localization positions\nor low classification scores but accurate localization positions will worsen\nthe performance of detectors after Non-Maximum Suppression. Furthermore, when\nobject detectors collaborate with Quantization-Aware Training (QAT), we observe\nthat the task inharmony problem will be further exacerbated, which is\nconsidered one of the main causes of the performance degradation of quantized\ndetectors. To tackle this issue, we propose the Harmonious Quantization for\nObject Detection (HQOD) framework, which consists of two components. Firstly,\nwe propose a task-correlated loss to encourage detectors to focus on improving\nsamples with lower task harmony quality during QAT. Secondly, a harmonious\nIntersection over Union (IoU) loss is incorporated to balance the optimization\nof the regression branch across different IoU levels. The proposed HQOD can be\neasily integrated into different QAT algorithms and detectors. Remarkably, on\nthe MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves a\nstate-of-the-art mAP of 39.6%, even surpassing the full-precision one.\n", "link": "http://arxiv.org/abs/2408.02561v1", "date": "2024-08-05", "relevancy": 2.0727, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5283}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HQOD%3A%20Harmonious%20Quantization%20for%20Object%20Detection&body=Title%3A%20HQOD%3A%20Harmonious%20Quantization%20for%20Object%20Detection%0AAuthor%3A%20Long%20Huang%20and%20Zhiwei%20Dong%20and%20Song-Lu%20Chen%20and%20Ruiyao%20Zhang%20and%20Shutong%20Ti%20and%20Feng%20Chen%20and%20Xu-Cheng%20Yin%0AAbstract%3A%20%20%20Task%20inharmony%20problem%20commonly%20occurs%20in%20modern%20object%20detectors%2C%20leading%20to%0Ainconsistent%20qualities%20between%20classification%20and%20regression%20tasks.%20The%0Apredicted%20boxes%20with%20high%20classification%20scores%20but%20poor%20localization%20positions%0Aor%20low%20classification%20scores%20but%20accurate%20localization%20positions%20will%20worsen%0Athe%20performance%20of%20detectors%20after%20Non-Maximum%20Suppression.%20Furthermore%2C%20when%0Aobject%20detectors%20collaborate%20with%20Quantization-Aware%20Training%20%28QAT%29%2C%20we%20observe%0Athat%20the%20task%20inharmony%20problem%20will%20be%20further%20exacerbated%2C%20which%20is%0Aconsidered%20one%20of%20the%20main%20causes%20of%20the%20performance%20degradation%20of%20quantized%0Adetectors.%20To%20tackle%20this%20issue%2C%20we%20propose%20the%20Harmonious%20Quantization%20for%0AObject%20Detection%20%28HQOD%29%20framework%2C%20which%20consists%20of%20two%20components.%20Firstly%2C%0Awe%20propose%20a%20task-correlated%20loss%20to%20encourage%20detectors%20to%20focus%20on%20improving%0Asamples%20with%20lower%20task%20harmony%20quality%20during%20QAT.%20Secondly%2C%20a%20harmonious%0AIntersection%20over%20Union%20%28IoU%29%20loss%20is%20incorporated%20to%20balance%20the%20optimization%0Aof%20the%20regression%20branch%20across%20different%20IoU%20levels.%20The%20proposed%20HQOD%20can%20be%0Aeasily%20integrated%20into%20different%20QAT%20algorithms%20and%20detectors.%20Remarkably%2C%20on%0Athe%20MS%20COCO%20dataset%2C%20our%204-bit%20ATSS%20with%20ResNet-50%20backbone%20achieves%20a%0Astate-of-the-art%20mAP%20of%2039.6%25%2C%20even%20surpassing%20the%20full-precision%20one.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHQOD%253A%2520Harmonious%2520Quantization%2520for%2520Object%2520Detection%26entry.906535625%3DLong%2520Huang%2520and%2520Zhiwei%2520Dong%2520and%2520Song-Lu%2520Chen%2520and%2520Ruiyao%2520Zhang%2520and%2520Shutong%2520Ti%2520and%2520Feng%2520Chen%2520and%2520Xu-Cheng%2520Yin%26entry.1292438233%3D%2520%2520Task%2520inharmony%2520problem%2520commonly%2520occurs%2520in%2520modern%2520object%2520detectors%252C%2520leading%2520to%250Ainconsistent%2520qualities%2520between%2520classification%2520and%2520regression%2520tasks.%2520The%250Apredicted%2520boxes%2520with%2520high%2520classification%2520scores%2520but%2520poor%2520localization%2520positions%250Aor%2520low%2520classification%2520scores%2520but%2520accurate%2520localization%2520positions%2520will%2520worsen%250Athe%2520performance%2520of%2520detectors%2520after%2520Non-Maximum%2520Suppression.%2520Furthermore%252C%2520when%250Aobject%2520detectors%2520collaborate%2520with%2520Quantization-Aware%2520Training%2520%2528QAT%2529%252C%2520we%2520observe%250Athat%2520the%2520task%2520inharmony%2520problem%2520will%2520be%2520further%2520exacerbated%252C%2520which%2520is%250Aconsidered%2520one%2520of%2520the%2520main%2520causes%2520of%2520the%2520performance%2520degradation%2520of%2520quantized%250Adetectors.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520the%2520Harmonious%2520Quantization%2520for%250AObject%2520Detection%2520%2528HQOD%2529%2520framework%252C%2520which%2520consists%2520of%2520two%2520components.%2520Firstly%252C%250Awe%2520propose%2520a%2520task-correlated%2520loss%2520to%2520encourage%2520detectors%2520to%2520focus%2520on%2520improving%250Asamples%2520with%2520lower%2520task%2520harmony%2520quality%2520during%2520QAT.%2520Secondly%252C%2520a%2520harmonious%250AIntersection%2520over%2520Union%2520%2528IoU%2529%2520loss%2520is%2520incorporated%2520to%2520balance%2520the%2520optimization%250Aof%2520the%2520regression%2520branch%2520across%2520different%2520IoU%2520levels.%2520The%2520proposed%2520HQOD%2520can%2520be%250Aeasily%2520integrated%2520into%2520different%2520QAT%2520algorithms%2520and%2520detectors.%2520Remarkably%252C%2520on%250Athe%2520MS%2520COCO%2520dataset%252C%2520our%25204-bit%2520ATSS%2520with%2520ResNet-50%2520backbone%2520achieves%2520a%250Astate-of-the-art%2520mAP%2520of%252039.6%2525%252C%2520even%2520surpassing%2520the%2520full-precision%2520one.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HQOD%3A%20Harmonious%20Quantization%20for%20Object%20Detection&entry.906535625=Long%20Huang%20and%20Zhiwei%20Dong%20and%20Song-Lu%20Chen%20and%20Ruiyao%20Zhang%20and%20Shutong%20Ti%20and%20Feng%20Chen%20and%20Xu-Cheng%20Yin&entry.1292438233=%20%20Task%20inharmony%20problem%20commonly%20occurs%20in%20modern%20object%20detectors%2C%20leading%20to%0Ainconsistent%20qualities%20between%20classification%20and%20regression%20tasks.%20The%0Apredicted%20boxes%20with%20high%20classification%20scores%20but%20poor%20localization%20positions%0Aor%20low%20classification%20scores%20but%20accurate%20localization%20positions%20will%20worsen%0Athe%20performance%20of%20detectors%20after%20Non-Maximum%20Suppression.%20Furthermore%2C%20when%0Aobject%20detectors%20collaborate%20with%20Quantization-Aware%20Training%20%28QAT%29%2C%20we%20observe%0Athat%20the%20task%20inharmony%20problem%20will%20be%20further%20exacerbated%2C%20which%20is%0Aconsidered%20one%20of%20the%20main%20causes%20of%20the%20performance%20degradation%20of%20quantized%0Adetectors.%20To%20tackle%20this%20issue%2C%20we%20propose%20the%20Harmonious%20Quantization%20for%0AObject%20Detection%20%28HQOD%29%20framework%2C%20which%20consists%20of%20two%20components.%20Firstly%2C%0Awe%20propose%20a%20task-correlated%20loss%20to%20encourage%20detectors%20to%20focus%20on%20improving%0Asamples%20with%20lower%20task%20harmony%20quality%20during%20QAT.%20Secondly%2C%20a%20harmonious%0AIntersection%20over%20Union%20%28IoU%29%20loss%20is%20incorporated%20to%20balance%20the%20optimization%0Aof%20the%20regression%20branch%20across%20different%20IoU%20levels.%20The%20proposed%20HQOD%20can%20be%0Aeasily%20integrated%20into%20different%20QAT%20algorithms%20and%20detectors.%20Remarkably%2C%20on%0Athe%20MS%20COCO%20dataset%2C%20our%204-bit%20ATSS%20with%20ResNet-50%20backbone%20achieves%20a%0Astate-of-the-art%20mAP%20of%2039.6%25%2C%20even%20surpassing%20the%20full-precision%20one.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02561v1&entry.124074799=Read"},
{"title": "YOWOv3: An Efficient and Generalized Framework for Human Action\n  Detection and Recognition", "author": "Duc Manh Nguyen Dang and Viet Hang Duong and Jia Ching Wang and Nhan Bui Duc", "abstract": "  In this paper, we propose a new framework called YOWOv3, which is an improved\nversion of YOWOv2, designed specifically for the task of Human Action Detection\nand Recognition. This framework is designed to facilitate extensive\nexperimentation with different configurations and supports easy customization\nof various components within the model, reducing efforts required for\nunderstanding and modifying the code. YOWOv3 demonstrates its superior\nperformance compared to YOWOv2 on two widely used datasets for Human Action\nDetection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor\nmodel YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,\nrespectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -\nYOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%\nand 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that\nYOWOv3 significantly reduces the number of parameters and GFLOPs while still\nachieving comparable performance.\n", "link": "http://arxiv.org/abs/2408.02623v1", "date": "2024-08-05", "relevancy": 2.0582, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOWOv3%3A%20An%20Efficient%20and%20Generalized%20Framework%20for%20Human%20Action%0A%20%20Detection%20and%20Recognition&body=Title%3A%20YOWOv3%3A%20An%20Efficient%20and%20Generalized%20Framework%20for%20Human%20Action%0A%20%20Detection%20and%20Recognition%0AAuthor%3A%20Duc%20Manh%20Nguyen%20Dang%20and%20Viet%20Hang%20Duong%20and%20Jia%20Ching%20Wang%20and%20Nhan%20Bui%20Duc%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20called%20YOWOv3%2C%20which%20is%20an%20improved%0Aversion%20of%20YOWOv2%2C%20designed%20specifically%20for%20the%20task%20of%20Human%20Action%20Detection%0Aand%20Recognition.%20This%20framework%20is%20designed%20to%20facilitate%20extensive%0Aexperimentation%20with%20different%20configurations%20and%20supports%20easy%20customization%0Aof%20various%20components%20within%20the%20model%2C%20reducing%20efforts%20required%20for%0Aunderstanding%20and%20modifying%20the%20code.%20YOWOv3%20demonstrates%20its%20superior%0Aperformance%20compared%20to%20YOWOv2%20on%20two%20widely%20used%20datasets%20for%20Human%20Action%0ADetection%20and%20Recognition%3A%20UCF101-24%20and%20AVAv2.2.%20Specifically%2C%20the%20predecessor%0Amodel%20YOWOv2%20achieves%20an%20mAP%20of%2085.2%25%20and%2020.3%25%20on%20UCF101-24%20and%20AVAv2.2%2C%0Arespectively%2C%20with%20109.7M%20parameters%20and%2053.6%20GFLOPs.%20In%20contrast%2C%20our%20model%20-%0AYOWOv3%2C%20with%20only%2059.8M%20parameters%20and%2039.8%20GFLOPs%2C%20achieves%20an%20mAP%20of%2088.33%25%0Aand%2020.31%25%20on%20UCF101-24%20and%20AVAv2.2%2C%20respectively.%20The%20results%20demonstrate%20that%0AYOWOv3%20significantly%20reduces%20the%20number%20of%20parameters%20and%20GFLOPs%20while%20still%0Aachieving%20comparable%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOWOv3%253A%2520An%2520Efficient%2520and%2520Generalized%2520Framework%2520for%2520Human%2520Action%250A%2520%2520Detection%2520and%2520Recognition%26entry.906535625%3DDuc%2520Manh%2520Nguyen%2520Dang%2520and%2520Viet%2520Hang%2520Duong%2520and%2520Jia%2520Ching%2520Wang%2520and%2520Nhan%2520Bui%2520Duc%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520framework%2520called%2520YOWOv3%252C%2520which%2520is%2520an%2520improved%250Aversion%2520of%2520YOWOv2%252C%2520designed%2520specifically%2520for%2520the%2520task%2520of%2520Human%2520Action%2520Detection%250Aand%2520Recognition.%2520This%2520framework%2520is%2520designed%2520to%2520facilitate%2520extensive%250Aexperimentation%2520with%2520different%2520configurations%2520and%2520supports%2520easy%2520customization%250Aof%2520various%2520components%2520within%2520the%2520model%252C%2520reducing%2520efforts%2520required%2520for%250Aunderstanding%2520and%2520modifying%2520the%2520code.%2520YOWOv3%2520demonstrates%2520its%2520superior%250Aperformance%2520compared%2520to%2520YOWOv2%2520on%2520two%2520widely%2520used%2520datasets%2520for%2520Human%2520Action%250ADetection%2520and%2520Recognition%253A%2520UCF101-24%2520and%2520AVAv2.2.%2520Specifically%252C%2520the%2520predecessor%250Amodel%2520YOWOv2%2520achieves%2520an%2520mAP%2520of%252085.2%2525%2520and%252020.3%2525%2520on%2520UCF101-24%2520and%2520AVAv2.2%252C%250Arespectively%252C%2520with%2520109.7M%2520parameters%2520and%252053.6%2520GFLOPs.%2520In%2520contrast%252C%2520our%2520model%2520-%250AYOWOv3%252C%2520with%2520only%252059.8M%2520parameters%2520and%252039.8%2520GFLOPs%252C%2520achieves%2520an%2520mAP%2520of%252088.33%2525%250Aand%252020.31%2525%2520on%2520UCF101-24%2520and%2520AVAv2.2%252C%2520respectively.%2520The%2520results%2520demonstrate%2520that%250AYOWOv3%2520significantly%2520reduces%2520the%2520number%2520of%2520parameters%2520and%2520GFLOPs%2520while%2520still%250Aachieving%2520comparable%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOWOv3%3A%20An%20Efficient%20and%20Generalized%20Framework%20for%20Human%20Action%0A%20%20Detection%20and%20Recognition&entry.906535625=Duc%20Manh%20Nguyen%20Dang%20and%20Viet%20Hang%20Duong%20and%20Jia%20Ching%20Wang%20and%20Nhan%20Bui%20Duc&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20called%20YOWOv3%2C%20which%20is%20an%20improved%0Aversion%20of%20YOWOv2%2C%20designed%20specifically%20for%20the%20task%20of%20Human%20Action%20Detection%0Aand%20Recognition.%20This%20framework%20is%20designed%20to%20facilitate%20extensive%0Aexperimentation%20with%20different%20configurations%20and%20supports%20easy%20customization%0Aof%20various%20components%20within%20the%20model%2C%20reducing%20efforts%20required%20for%0Aunderstanding%20and%20modifying%20the%20code.%20YOWOv3%20demonstrates%20its%20superior%0Aperformance%20compared%20to%20YOWOv2%20on%20two%20widely%20used%20datasets%20for%20Human%20Action%0ADetection%20and%20Recognition%3A%20UCF101-24%20and%20AVAv2.2.%20Specifically%2C%20the%20predecessor%0Amodel%20YOWOv2%20achieves%20an%20mAP%20of%2085.2%25%20and%2020.3%25%20on%20UCF101-24%20and%20AVAv2.2%2C%0Arespectively%2C%20with%20109.7M%20parameters%20and%2053.6%20GFLOPs.%20In%20contrast%2C%20our%20model%20-%0AYOWOv3%2C%20with%20only%2059.8M%20parameters%20and%2039.8%20GFLOPs%2C%20achieves%20an%20mAP%20of%2088.33%25%0Aand%2020.31%25%20on%20UCF101-24%20and%20AVAv2.2%2C%20respectively.%20The%20results%20demonstrate%20that%0AYOWOv3%20significantly%20reduces%20the%20number%20of%20parameters%20and%20GFLOPs%20while%20still%0Aachieving%20comparable%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02623v1&entry.124074799=Read"},
{"title": "Progressively Selective Label Enhancement for Language Model Alignment", "author": "Biao Liu and Ning Xu and Xin Geng", "abstract": "  Large Language Models have demonstrated impressive capabilities in various\nlanguage tasks but may produce content that misaligns with human expectations,\nraising ethical and legal concerns. Therefore, it is important to explore the\nlimitations and implement restrictions on the models to ensure safety and\ncompliance, with Reinforcement Learning from Human Feedback (RLHF) being the\nprimary method. Due to challenges in stability and scalability with the RLHF\nstages, researchers are exploring alternative methods to achieve effects\ncomparable to those of RLHF. However, these methods often depend on large\nhigh-quality datasets and inefficiently utilize generated data. To deal with\nthis problem, we propose PSLE, i.e., Progressively Selective Label Enhancement\nfor Language Model Alignment, a framework that fully utilizes all generated\ndata by guiding the model with principles to align outputs with human\nexpectations. Using a dynamically updated threshold, our approach ensures\nefficient data utilization by incorporating all generated responses and\nweighting them based on their corresponding reward scores. Experimental results\non multiple datasets demonstrate the effectiveness of PSLE compared to existing\nlanguage model alignment methods.\n", "link": "http://arxiv.org/abs/2408.02599v1", "date": "2024-08-05", "relevancy": 2.0461, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.561}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressively%20Selective%20Label%20Enhancement%20for%20Language%20Model%20Alignment&body=Title%3A%20Progressively%20Selective%20Label%20Enhancement%20for%20Language%20Model%20Alignment%0AAuthor%3A%20Biao%20Liu%20and%20Ning%20Xu%20and%20Xin%20Geng%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20demonstrated%20impressive%20capabilities%20in%20various%0Alanguage%20tasks%20but%20may%20produce%20content%20that%20misaligns%20with%20human%20expectations%2C%0Araising%20ethical%20and%20legal%20concerns.%20Therefore%2C%20it%20is%20important%20to%20explore%20the%0Alimitations%20and%20implement%20restrictions%20on%20the%20models%20to%20ensure%20safety%20and%0Acompliance%2C%20with%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20being%20the%0Aprimary%20method.%20Due%20to%20challenges%20in%20stability%20and%20scalability%20with%20the%20RLHF%0Astages%2C%20researchers%20are%20exploring%20alternative%20methods%20to%20achieve%20effects%0Acomparable%20to%20those%20of%20RLHF.%20However%2C%20these%20methods%20often%20depend%20on%20large%0Ahigh-quality%20datasets%20and%20inefficiently%20utilize%20generated%20data.%20To%20deal%20with%0Athis%20problem%2C%20we%20propose%20PSLE%2C%20i.e.%2C%20Progressively%20Selective%20Label%20Enhancement%0Afor%20Language%20Model%20Alignment%2C%20a%20framework%20that%20fully%20utilizes%20all%20generated%0Adata%20by%20guiding%20the%20model%20with%20principles%20to%20align%20outputs%20with%20human%0Aexpectations.%20Using%20a%20dynamically%20updated%20threshold%2C%20our%20approach%20ensures%0Aefficient%20data%20utilization%20by%20incorporating%20all%20generated%20responses%20and%0Aweighting%20them%20based%20on%20their%20corresponding%20reward%20scores.%20Experimental%20results%0Aon%20multiple%20datasets%20demonstrate%20the%20effectiveness%20of%20PSLE%20compared%20to%20existing%0Alanguage%20model%20alignment%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressively%2520Selective%2520Label%2520Enhancement%2520for%2520Language%2520Model%2520Alignment%26entry.906535625%3DBiao%2520Liu%2520and%2520Ning%2520Xu%2520and%2520Xin%2520Geng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520various%250Alanguage%2520tasks%2520but%2520may%2520produce%2520content%2520that%2520misaligns%2520with%2520human%2520expectations%252C%250Araising%2520ethical%2520and%2520legal%2520concerns.%2520Therefore%252C%2520it%2520is%2520important%2520to%2520explore%2520the%250Alimitations%2520and%2520implement%2520restrictions%2520on%2520the%2520models%2520to%2520ensure%2520safety%2520and%250Acompliance%252C%2520with%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520being%2520the%250Aprimary%2520method.%2520Due%2520to%2520challenges%2520in%2520stability%2520and%2520scalability%2520with%2520the%2520RLHF%250Astages%252C%2520researchers%2520are%2520exploring%2520alternative%2520methods%2520to%2520achieve%2520effects%250Acomparable%2520to%2520those%2520of%2520RLHF.%2520However%252C%2520these%2520methods%2520often%2520depend%2520on%2520large%250Ahigh-quality%2520datasets%2520and%2520inefficiently%2520utilize%2520generated%2520data.%2520To%2520deal%2520with%250Athis%2520problem%252C%2520we%2520propose%2520PSLE%252C%2520i.e.%252C%2520Progressively%2520Selective%2520Label%2520Enhancement%250Afor%2520Language%2520Model%2520Alignment%252C%2520a%2520framework%2520that%2520fully%2520utilizes%2520all%2520generated%250Adata%2520by%2520guiding%2520the%2520model%2520with%2520principles%2520to%2520align%2520outputs%2520with%2520human%250Aexpectations.%2520Using%2520a%2520dynamically%2520updated%2520threshold%252C%2520our%2520approach%2520ensures%250Aefficient%2520data%2520utilization%2520by%2520incorporating%2520all%2520generated%2520responses%2520and%250Aweighting%2520them%2520based%2520on%2520their%2520corresponding%2520reward%2520scores.%2520Experimental%2520results%250Aon%2520multiple%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520PSLE%2520compared%2520to%2520existing%250Alanguage%2520model%2520alignment%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressively%20Selective%20Label%20Enhancement%20for%20Language%20Model%20Alignment&entry.906535625=Biao%20Liu%20and%20Ning%20Xu%20and%20Xin%20Geng&entry.1292438233=%20%20Large%20Language%20Models%20have%20demonstrated%20impressive%20capabilities%20in%20various%0Alanguage%20tasks%20but%20may%20produce%20content%20that%20misaligns%20with%20human%20expectations%2C%0Araising%20ethical%20and%20legal%20concerns.%20Therefore%2C%20it%20is%20important%20to%20explore%20the%0Alimitations%20and%20implement%20restrictions%20on%20the%20models%20to%20ensure%20safety%20and%0Acompliance%2C%20with%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20being%20the%0Aprimary%20method.%20Due%20to%20challenges%20in%20stability%20and%20scalability%20with%20the%20RLHF%0Astages%2C%20researchers%20are%20exploring%20alternative%20methods%20to%20achieve%20effects%0Acomparable%20to%20those%20of%20RLHF.%20However%2C%20these%20methods%20often%20depend%20on%20large%0Ahigh-quality%20datasets%20and%20inefficiently%20utilize%20generated%20data.%20To%20deal%20with%0Athis%20problem%2C%20we%20propose%20PSLE%2C%20i.e.%2C%20Progressively%20Selective%20Label%20Enhancement%0Afor%20Language%20Model%20Alignment%2C%20a%20framework%20that%20fully%20utilizes%20all%20generated%0Adata%20by%20guiding%20the%20model%20with%20principles%20to%20align%20outputs%20with%20human%0Aexpectations.%20Using%20a%20dynamically%20updated%20threshold%2C%20our%20approach%20ensures%0Aefficient%20data%20utilization%20by%20incorporating%20all%20generated%20responses%20and%0Aweighting%20them%20based%20on%20their%20corresponding%20reward%20scores.%20Experimental%20results%0Aon%20multiple%20datasets%20demonstrate%20the%20effectiveness%20of%20PSLE%20compared%20to%20existing%0Alanguage%20model%20alignment%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02599v1&entry.124074799=Read"},
{"title": "Zero shot VLMs for hate meme detection: Are we there yet?", "author": "Naquee Rizwan and Paramananda Bhaskar and Mithun Das and Swadhin Satyaprakash Majhi and Punyajoy Saha and Animesh Mukherjee", "abstract": "  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n", "link": "http://arxiv.org/abs/2402.12198v2", "date": "2024-08-05", "relevancy": 2.0222, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4965}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero%20shot%20VLMs%20for%20hate%20meme%20detection%3A%20Are%20we%20there%20yet%3F&body=Title%3A%20Zero%20shot%20VLMs%20for%20hate%20meme%20detection%3A%20Are%20we%20there%20yet%3F%0AAuthor%3A%20Naquee%20Rizwan%20and%20Paramananda%20Bhaskar%20and%20Mithun%20Das%20and%20Swadhin%20Satyaprakash%20Majhi%20and%20Punyajoy%20Saha%20and%20Animesh%20Mukherjee%0AAbstract%3A%20%20%20Multimedia%20content%20on%20social%20media%20is%20rapidly%20evolving%2C%20with%20memes%20gaining%0Aprominence%20as%20a%20distinctive%20form.%20Unfortunately%2C%20some%20malicious%20users%20exploit%0Amemes%20to%20target%20individuals%20or%20vulnerable%20communities%2C%20making%20it%20imperative%20to%0Aidentify%20and%20address%20such%20instances%20of%20hateful%20memes.%20Extensive%20research%20has%0Abeen%20conducted%20to%20address%20this%20issue%20by%20developing%20hate%20meme%20detection%20models.%0AHowever%2C%20a%20notable%20limitation%20of%20traditional%20machine/deep%20learning%20models%20is%0Athe%20requirement%20for%20labeled%20datasets%20for%20accurate%20classification.%20Recently%2C%20the%0Aresearch%20community%20has%20witnessed%20the%20emergence%20of%20several%20visual%20language%0Amodels%20that%20have%20exhibited%20outstanding%20performance%20across%20various%20tasks.%20In%0Athis%20study%2C%20we%20aim%20to%20investigate%20the%20efficacy%20of%20these%20visual%20language%20models%0Ain%20handling%20intricate%20tasks%20such%20as%20hate%20meme%20detection.%20We%20use%20various%20prompt%0Asettings%20to%20focus%20on%20zero-shot%20classification%20of%20hateful/harmful%20memes.%20Through%0Aour%20analysis%2C%20we%20observe%20that%20large%20VLMs%20are%20still%20vulnerable%20for%20zero-shot%0Ahate%20meme%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12198v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero%2520shot%2520VLMs%2520for%2520hate%2520meme%2520detection%253A%2520Are%2520we%2520there%2520yet%253F%26entry.906535625%3DNaquee%2520Rizwan%2520and%2520Paramananda%2520Bhaskar%2520and%2520Mithun%2520Das%2520and%2520Swadhin%2520Satyaprakash%2520Majhi%2520and%2520Punyajoy%2520Saha%2520and%2520Animesh%2520Mukherjee%26entry.1292438233%3D%2520%2520Multimedia%2520content%2520on%2520social%2520media%2520is%2520rapidly%2520evolving%252C%2520with%2520memes%2520gaining%250Aprominence%2520as%2520a%2520distinctive%2520form.%2520Unfortunately%252C%2520some%2520malicious%2520users%2520exploit%250Amemes%2520to%2520target%2520individuals%2520or%2520vulnerable%2520communities%252C%2520making%2520it%2520imperative%2520to%250Aidentify%2520and%2520address%2520such%2520instances%2520of%2520hateful%2520memes.%2520Extensive%2520research%2520has%250Abeen%2520conducted%2520to%2520address%2520this%2520issue%2520by%2520developing%2520hate%2520meme%2520detection%2520models.%250AHowever%252C%2520a%2520notable%2520limitation%2520of%2520traditional%2520machine/deep%2520learning%2520models%2520is%250Athe%2520requirement%2520for%2520labeled%2520datasets%2520for%2520accurate%2520classification.%2520Recently%252C%2520the%250Aresearch%2520community%2520has%2520witnessed%2520the%2520emergence%2520of%2520several%2520visual%2520language%250Amodels%2520that%2520have%2520exhibited%2520outstanding%2520performance%2520across%2520various%2520tasks.%2520In%250Athis%2520study%252C%2520we%2520aim%2520to%2520investigate%2520the%2520efficacy%2520of%2520these%2520visual%2520language%2520models%250Ain%2520handling%2520intricate%2520tasks%2520such%2520as%2520hate%2520meme%2520detection.%2520We%2520use%2520various%2520prompt%250Asettings%2520to%2520focus%2520on%2520zero-shot%2520classification%2520of%2520hateful/harmful%2520memes.%2520Through%250Aour%2520analysis%252C%2520we%2520observe%2520that%2520large%2520VLMs%2520are%2520still%2520vulnerable%2520for%2520zero-shot%250Ahate%2520meme%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12198v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero%20shot%20VLMs%20for%20hate%20meme%20detection%3A%20Are%20we%20there%20yet%3F&entry.906535625=Naquee%20Rizwan%20and%20Paramananda%20Bhaskar%20and%20Mithun%20Das%20and%20Swadhin%20Satyaprakash%20Majhi%20and%20Punyajoy%20Saha%20and%20Animesh%20Mukherjee&entry.1292438233=%20%20Multimedia%20content%20on%20social%20media%20is%20rapidly%20evolving%2C%20with%20memes%20gaining%0Aprominence%20as%20a%20distinctive%20form.%20Unfortunately%2C%20some%20malicious%20users%20exploit%0Amemes%20to%20target%20individuals%20or%20vulnerable%20communities%2C%20making%20it%20imperative%20to%0Aidentify%20and%20address%20such%20instances%20of%20hateful%20memes.%20Extensive%20research%20has%0Abeen%20conducted%20to%20address%20this%20issue%20by%20developing%20hate%20meme%20detection%20models.%0AHowever%2C%20a%20notable%20limitation%20of%20traditional%20machine/deep%20learning%20models%20is%0Athe%20requirement%20for%20labeled%20datasets%20for%20accurate%20classification.%20Recently%2C%20the%0Aresearch%20community%20has%20witnessed%20the%20emergence%20of%20several%20visual%20language%0Amodels%20that%20have%20exhibited%20outstanding%20performance%20across%20various%20tasks.%20In%0Athis%20study%2C%20we%20aim%20to%20investigate%20the%20efficacy%20of%20these%20visual%20language%20models%0Ain%20handling%20intricate%20tasks%20such%20as%20hate%20meme%20detection.%20We%20use%20various%20prompt%0Asettings%20to%20focus%20on%20zero-shot%20classification%20of%20hateful/harmful%20memes.%20Through%0Aour%20analysis%2C%20we%20observe%20that%20large%20VLMs%20are%20still%20vulnerable%20for%20zero-shot%0Ahate%20meme%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12198v2&entry.124074799=Read"},
{"title": "Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval", "author": "Shengjie Ma and Chengjin Xu and Xuhui Jiang and Muzhi Li and Huaren Qu and Jian Guo", "abstract": "  Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.\n", "link": "http://arxiv.org/abs/2407.10805v2", "date": "2024-08-05", "relevancy": 2.0135, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5289}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5147}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think-on-Graph%202.0%3A%20Deep%20and%20Interpretable%20Large%20Language%20Model%0A%20%20Reasoning%20with%20Knowledge%20Graph-guided%20Retrieval&body=Title%3A%20Think-on-Graph%202.0%3A%20Deep%20and%20Interpretable%20Large%20Language%20Model%0A%20%20Reasoning%20with%20Knowledge%20Graph-guided%20Retrieval%0AAuthor%3A%20Shengjie%20Ma%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Muzhi%20Li%20and%20Huaren%20Qu%20and%20Jian%20Guo%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20significantly%20advanced%20large%0Alanguage%20models%20%28LLMs%29%20by%20enabling%20dynamic%20information%20retrieval%20to%20mitigate%0Aknowledge%20gaps%20and%20hallucinations%20in%20generated%20content.%20However%2C%20these%20systems%0Aoften%20falter%20with%20complex%20reasoning%20and%20consistency%20across%20diverse%20queries.%20In%0Athis%20work%2C%20we%20present%20Think-on-Graph%202.0%2C%20an%20enhanced%20RAG%20framework%20that%20aligns%0Aquestions%20with%20the%20knowledge%20graph%20and%20uses%20it%20as%20a%20navigational%20tool%2C%20which%0Adeepens%20and%20refines%20the%20RAG%20paradigm%20for%20information%20collection%20and%0Aintegration.%20The%20KG-guided%20navigation%20fosters%20deep%20and%20long-range%20associations%0Ato%20uphold%20logical%20consistency%20and%20optimize%20the%20scope%20of%20retrieval%20for%20precision%0Aand%20interoperability.%20In%20conjunction%2C%20factual%20consistency%20can%20be%20better%20ensured%0Athrough%20semantic%20similarity%20guided%20by%20precise%20directives.%20ToG%24%7B2.0%7D%24%20not%20only%0Aimproves%20the%20accuracy%20and%20reliability%20of%20LLMs%27%20responses%20but%20also%20demonstrates%0Athe%20potential%20of%20hybrid%20structured%20knowledge%20systems%20to%20significantly%20advance%0ALLM%20reasoning%2C%20aligning%20it%20closer%20to%20human-like%20performance.%20We%20conducted%0Aextensive%20experiments%20on%20four%20public%20datasets%20to%20demonstrate%20the%20advantages%20of%0Aour%20method%20compared%20to%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10805v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink-on-Graph%25202.0%253A%2520Deep%2520and%2520Interpretable%2520Large%2520Language%2520Model%250A%2520%2520Reasoning%2520with%2520Knowledge%2520Graph-guided%2520Retrieval%26entry.906535625%3DShengjie%2520Ma%2520and%2520Chengjin%2520Xu%2520and%2520Xuhui%2520Jiang%2520and%2520Muzhi%2520Li%2520and%2520Huaren%2520Qu%2520and%2520Jian%2520Guo%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520significantly%2520advanced%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520by%2520enabling%2520dynamic%2520information%2520retrieval%2520to%2520mitigate%250Aknowledge%2520gaps%2520and%2520hallucinations%2520in%2520generated%2520content.%2520However%252C%2520these%2520systems%250Aoften%2520falter%2520with%2520complex%2520reasoning%2520and%2520consistency%2520across%2520diverse%2520queries.%2520In%250Athis%2520work%252C%2520we%2520present%2520Think-on-Graph%25202.0%252C%2520an%2520enhanced%2520RAG%2520framework%2520that%2520aligns%250Aquestions%2520with%2520the%2520knowledge%2520graph%2520and%2520uses%2520it%2520as%2520a%2520navigational%2520tool%252C%2520which%250Adeepens%2520and%2520refines%2520the%2520RAG%2520paradigm%2520for%2520information%2520collection%2520and%250Aintegration.%2520The%2520KG-guided%2520navigation%2520fosters%2520deep%2520and%2520long-range%2520associations%250Ato%2520uphold%2520logical%2520consistency%2520and%2520optimize%2520the%2520scope%2520of%2520retrieval%2520for%2520precision%250Aand%2520interoperability.%2520In%2520conjunction%252C%2520factual%2520consistency%2520can%2520be%2520better%2520ensured%250Athrough%2520semantic%2520similarity%2520guided%2520by%2520precise%2520directives.%2520ToG%2524%257B2.0%257D%2524%2520not%2520only%250Aimproves%2520the%2520accuracy%2520and%2520reliability%2520of%2520LLMs%2527%2520responses%2520but%2520also%2520demonstrates%250Athe%2520potential%2520of%2520hybrid%2520structured%2520knowledge%2520systems%2520to%2520significantly%2520advance%250ALLM%2520reasoning%252C%2520aligning%2520it%2520closer%2520to%2520human-like%2520performance.%2520We%2520conducted%250Aextensive%2520experiments%2520on%2520four%2520public%2520datasets%2520to%2520demonstrate%2520the%2520advantages%2520of%250Aour%2520method%2520compared%2520to%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10805v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think-on-Graph%202.0%3A%20Deep%20and%20Interpretable%20Large%20Language%20Model%0A%20%20Reasoning%20with%20Knowledge%20Graph-guided%20Retrieval&entry.906535625=Shengjie%20Ma%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Muzhi%20Li%20and%20Huaren%20Qu%20and%20Jian%20Guo&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20significantly%20advanced%20large%0Alanguage%20models%20%28LLMs%29%20by%20enabling%20dynamic%20information%20retrieval%20to%20mitigate%0Aknowledge%20gaps%20and%20hallucinations%20in%20generated%20content.%20However%2C%20these%20systems%0Aoften%20falter%20with%20complex%20reasoning%20and%20consistency%20across%20diverse%20queries.%20In%0Athis%20work%2C%20we%20present%20Think-on-Graph%202.0%2C%20an%20enhanced%20RAG%20framework%20that%20aligns%0Aquestions%20with%20the%20knowledge%20graph%20and%20uses%20it%20as%20a%20navigational%20tool%2C%20which%0Adeepens%20and%20refines%20the%20RAG%20paradigm%20for%20information%20collection%20and%0Aintegration.%20The%20KG-guided%20navigation%20fosters%20deep%20and%20long-range%20associations%0Ato%20uphold%20logical%20consistency%20and%20optimize%20the%20scope%20of%20retrieval%20for%20precision%0Aand%20interoperability.%20In%20conjunction%2C%20factual%20consistency%20can%20be%20better%20ensured%0Athrough%20semantic%20similarity%20guided%20by%20precise%20directives.%20ToG%24%7B2.0%7D%24%20not%20only%0Aimproves%20the%20accuracy%20and%20reliability%20of%20LLMs%27%20responses%20but%20also%20demonstrates%0Athe%20potential%20of%20hybrid%20structured%20knowledge%20systems%20to%20significantly%20advance%0ALLM%20reasoning%2C%20aligning%20it%20closer%20to%20human-like%20performance.%20We%20conducted%0Aextensive%20experiments%20on%20four%20public%20datasets%20to%20demonstrate%20the%20advantages%20of%0Aour%20method%20compared%20to%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10805v2&entry.124074799=Read"},
{"title": "Estimating Pore Location of PBF-LB/M Processes with Segmentation Models", "author": "Hans Aoyang Zhou and Jan Theunissen and Marco Kemmerling and Anas Abdelrazeq and Johannes Henrich Schleifenbaum and Robert H. Schmitt", "abstract": "  Reliably manufacturing defect free products is still an open challenge for\nLaser Powder Bed Fusion processes. Particularly, pores that occur frequently\nhave a negative impact on mechanical properties like fatigue performance.\nTherefore, an accurate localisation of pores is mandatory for quality\nassurance, but requires time-consuming post-processing steps like computer\ntomography scans. Although existing solutions using in-situ monitoring data can\ndetect pore occurrence within a layer, they are limited in their localisation\nprecision. Therefore, we propose a pore localisation approach that estimates\ntheir position within a single layer using a Gaussian kernel density\nestimation. This allows segmentation models to learn the correlation between\nin-situ monitoring data and the derived probability distribution of pore\noccurrence. Within our experiments, we compare the prediction performance of\ndifferent segmentation models depending on machine parameter configuration and\ngeometry features. From our results, we conclude that our approach allows a\nprecise localisation of pores that requires minimal data preprocessing. Our\nresearch extends the literature by providing a foundation for more precise pore\ndetection systems.\n", "link": "http://arxiv.org/abs/2408.02507v1", "date": "2024-08-05", "relevancy": 2.0062, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Pore%20Location%20of%20PBF-LB/M%20Processes%20with%20Segmentation%20Models&body=Title%3A%20Estimating%20Pore%20Location%20of%20PBF-LB/M%20Processes%20with%20Segmentation%20Models%0AAuthor%3A%20Hans%20Aoyang%20Zhou%20and%20Jan%20Theunissen%20and%20Marco%20Kemmerling%20and%20Anas%20Abdelrazeq%20and%20Johannes%20Henrich%20Schleifenbaum%20and%20Robert%20H.%20Schmitt%0AAbstract%3A%20%20%20Reliably%20manufacturing%20defect%20free%20products%20is%20still%20an%20open%20challenge%20for%0ALaser%20Powder%20Bed%20Fusion%20processes.%20Particularly%2C%20pores%20that%20occur%20frequently%0Ahave%20a%20negative%20impact%20on%20mechanical%20properties%20like%20fatigue%20performance.%0ATherefore%2C%20an%20accurate%20localisation%20of%20pores%20is%20mandatory%20for%20quality%0Aassurance%2C%20but%20requires%20time-consuming%20post-processing%20steps%20like%20computer%0Atomography%20scans.%20Although%20existing%20solutions%20using%20in-situ%20monitoring%20data%20can%0Adetect%20pore%20occurrence%20within%20a%20layer%2C%20they%20are%20limited%20in%20their%20localisation%0Aprecision.%20Therefore%2C%20we%20propose%20a%20pore%20localisation%20approach%20that%20estimates%0Atheir%20position%20within%20a%20single%20layer%20using%20a%20Gaussian%20kernel%20density%0Aestimation.%20This%20allows%20segmentation%20models%20to%20learn%20the%20correlation%20between%0Ain-situ%20monitoring%20data%20and%20the%20derived%20probability%20distribution%20of%20pore%0Aoccurrence.%20Within%20our%20experiments%2C%20we%20compare%20the%20prediction%20performance%20of%0Adifferent%20segmentation%20models%20depending%20on%20machine%20parameter%20configuration%20and%0Ageometry%20features.%20From%20our%20results%2C%20we%20conclude%20that%20our%20approach%20allows%20a%0Aprecise%20localisation%20of%20pores%20that%20requires%20minimal%20data%20preprocessing.%20Our%0Aresearch%20extends%20the%20literature%20by%20providing%20a%20foundation%20for%20more%20precise%20pore%0Adetection%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Pore%2520Location%2520of%2520PBF-LB/M%2520Processes%2520with%2520Segmentation%2520Models%26entry.906535625%3DHans%2520Aoyang%2520Zhou%2520and%2520Jan%2520Theunissen%2520and%2520Marco%2520Kemmerling%2520and%2520Anas%2520Abdelrazeq%2520and%2520Johannes%2520Henrich%2520Schleifenbaum%2520and%2520Robert%2520H.%2520Schmitt%26entry.1292438233%3D%2520%2520Reliably%2520manufacturing%2520defect%2520free%2520products%2520is%2520still%2520an%2520open%2520challenge%2520for%250ALaser%2520Powder%2520Bed%2520Fusion%2520processes.%2520Particularly%252C%2520pores%2520that%2520occur%2520frequently%250Ahave%2520a%2520negative%2520impact%2520on%2520mechanical%2520properties%2520like%2520fatigue%2520performance.%250ATherefore%252C%2520an%2520accurate%2520localisation%2520of%2520pores%2520is%2520mandatory%2520for%2520quality%250Aassurance%252C%2520but%2520requires%2520time-consuming%2520post-processing%2520steps%2520like%2520computer%250Atomography%2520scans.%2520Although%2520existing%2520solutions%2520using%2520in-situ%2520monitoring%2520data%2520can%250Adetect%2520pore%2520occurrence%2520within%2520a%2520layer%252C%2520they%2520are%2520limited%2520in%2520their%2520localisation%250Aprecision.%2520Therefore%252C%2520we%2520propose%2520a%2520pore%2520localisation%2520approach%2520that%2520estimates%250Atheir%2520position%2520within%2520a%2520single%2520layer%2520using%2520a%2520Gaussian%2520kernel%2520density%250Aestimation.%2520This%2520allows%2520segmentation%2520models%2520to%2520learn%2520the%2520correlation%2520between%250Ain-situ%2520monitoring%2520data%2520and%2520the%2520derived%2520probability%2520distribution%2520of%2520pore%250Aoccurrence.%2520Within%2520our%2520experiments%252C%2520we%2520compare%2520the%2520prediction%2520performance%2520of%250Adifferent%2520segmentation%2520models%2520depending%2520on%2520machine%2520parameter%2520configuration%2520and%250Ageometry%2520features.%2520From%2520our%2520results%252C%2520we%2520conclude%2520that%2520our%2520approach%2520allows%2520a%250Aprecise%2520localisation%2520of%2520pores%2520that%2520requires%2520minimal%2520data%2520preprocessing.%2520Our%250Aresearch%2520extends%2520the%2520literature%2520by%2520providing%2520a%2520foundation%2520for%2520more%2520precise%2520pore%250Adetection%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Pore%20Location%20of%20PBF-LB/M%20Processes%20with%20Segmentation%20Models&entry.906535625=Hans%20Aoyang%20Zhou%20and%20Jan%20Theunissen%20and%20Marco%20Kemmerling%20and%20Anas%20Abdelrazeq%20and%20Johannes%20Henrich%20Schleifenbaum%20and%20Robert%20H.%20Schmitt&entry.1292438233=%20%20Reliably%20manufacturing%20defect%20free%20products%20is%20still%20an%20open%20challenge%20for%0ALaser%20Powder%20Bed%20Fusion%20processes.%20Particularly%2C%20pores%20that%20occur%20frequently%0Ahave%20a%20negative%20impact%20on%20mechanical%20properties%20like%20fatigue%20performance.%0ATherefore%2C%20an%20accurate%20localisation%20of%20pores%20is%20mandatory%20for%20quality%0Aassurance%2C%20but%20requires%20time-consuming%20post-processing%20steps%20like%20computer%0Atomography%20scans.%20Although%20existing%20solutions%20using%20in-situ%20monitoring%20data%20can%0Adetect%20pore%20occurrence%20within%20a%20layer%2C%20they%20are%20limited%20in%20their%20localisation%0Aprecision.%20Therefore%2C%20we%20propose%20a%20pore%20localisation%20approach%20that%20estimates%0Atheir%20position%20within%20a%20single%20layer%20using%20a%20Gaussian%20kernel%20density%0Aestimation.%20This%20allows%20segmentation%20models%20to%20learn%20the%20correlation%20between%0Ain-situ%20monitoring%20data%20and%20the%20derived%20probability%20distribution%20of%20pore%0Aoccurrence.%20Within%20our%20experiments%2C%20we%20compare%20the%20prediction%20performance%20of%0Adifferent%20segmentation%20models%20depending%20on%20machine%20parameter%20configuration%20and%0Ageometry%20features.%20From%20our%20results%2C%20we%20conclude%20that%20our%20approach%20allows%20a%0Aprecise%20localisation%20of%20pores%20that%20requires%20minimal%20data%20preprocessing.%20Our%0Aresearch%20extends%20the%20literature%20by%20providing%20a%20foundation%20for%20more%20precise%20pore%0Adetection%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02507v1&entry.124074799=Read"},
{"title": "A Few-Shot Approach for Relation Extraction Domain Adaptation using\n  Large Language Models", "author": "Vanni Zavarella and Juan Carlos Gamero-Salinas and Sergio Consoli", "abstract": "  Knowledge graphs (KGs) have been successfully applied to the analysis of\ncomplex scientific and technological domains, with automatic KG generation\nmethods typically building upon relation extraction models capturing\nfine-grained relations between domain entities in text. While these relations\nare fully applicable across scientific areas, existing models are trained on\nfew domain-specific datasets such as SciERC and do not perform well on new\ntarget domains. In this paper, we experiment with leveraging in-context\nlearning capabilities of Large Language Models to perform schema-constrained\ndata annotation, collecting in-domain training instances for a\nTransformer-based relation extraction model deployed on titles and abstracts of\nresearch papers in the Architecture, Construction, Engineering and Operations\n(AECO) domain. By assessing the performance gain with respect to a baseline\nDeep Learning architecture trained on off-domain data, we show that by using a\nfew-shot learning strategy with structured prompts and only minimal expert\nannotation the presented approach can potentially support domain adaptation of\na science KG generation model.\n", "link": "http://arxiv.org/abs/2408.02377v1", "date": "2024-08-05", "relevancy": 2.0023, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Few-Shot%20Approach%20for%20Relation%20Extraction%20Domain%20Adaptation%20using%0A%20%20Large%20Language%20Models&body=Title%3A%20A%20Few-Shot%20Approach%20for%20Relation%20Extraction%20Domain%20Adaptation%20using%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Vanni%20Zavarella%20and%20Juan%20Carlos%20Gamero-Salinas%20and%20Sergio%20Consoli%0AAbstract%3A%20%20%20Knowledge%20graphs%20%28KGs%29%20have%20been%20successfully%20applied%20to%20the%20analysis%20of%0Acomplex%20scientific%20and%20technological%20domains%2C%20with%20automatic%20KG%20generation%0Amethods%20typically%20building%20upon%20relation%20extraction%20models%20capturing%0Afine-grained%20relations%20between%20domain%20entities%20in%20text.%20While%20these%20relations%0Aare%20fully%20applicable%20across%20scientific%20areas%2C%20existing%20models%20are%20trained%20on%0Afew%20domain-specific%20datasets%20such%20as%20SciERC%20and%20do%20not%20perform%20well%20on%20new%0Atarget%20domains.%20In%20this%20paper%2C%20we%20experiment%20with%20leveraging%20in-context%0Alearning%20capabilities%20of%20Large%20Language%20Models%20to%20perform%20schema-constrained%0Adata%20annotation%2C%20collecting%20in-domain%20training%20instances%20for%20a%0ATransformer-based%20relation%20extraction%20model%20deployed%20on%20titles%20and%20abstracts%20of%0Aresearch%20papers%20in%20the%20Architecture%2C%20Construction%2C%20Engineering%20and%20Operations%0A%28AECO%29%20domain.%20By%20assessing%20the%20performance%20gain%20with%20respect%20to%20a%20baseline%0ADeep%20Learning%20architecture%20trained%20on%20off-domain%20data%2C%20we%20show%20that%20by%20using%20a%0Afew-shot%20learning%20strategy%20with%20structured%20prompts%20and%20only%20minimal%20expert%0Aannotation%20the%20presented%20approach%20can%20potentially%20support%20domain%20adaptation%20of%0Aa%20science%20KG%20generation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Few-Shot%2520Approach%2520for%2520Relation%2520Extraction%2520Domain%2520Adaptation%2520using%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DVanni%2520Zavarella%2520and%2520Juan%2520Carlos%2520Gamero-Salinas%2520and%2520Sergio%2520Consoli%26entry.1292438233%3D%2520%2520Knowledge%2520graphs%2520%2528KGs%2529%2520have%2520been%2520successfully%2520applied%2520to%2520the%2520analysis%2520of%250Acomplex%2520scientific%2520and%2520technological%2520domains%252C%2520with%2520automatic%2520KG%2520generation%250Amethods%2520typically%2520building%2520upon%2520relation%2520extraction%2520models%2520capturing%250Afine-grained%2520relations%2520between%2520domain%2520entities%2520in%2520text.%2520While%2520these%2520relations%250Aare%2520fully%2520applicable%2520across%2520scientific%2520areas%252C%2520existing%2520models%2520are%2520trained%2520on%250Afew%2520domain-specific%2520datasets%2520such%2520as%2520SciERC%2520and%2520do%2520not%2520perform%2520well%2520on%2520new%250Atarget%2520domains.%2520In%2520this%2520paper%252C%2520we%2520experiment%2520with%2520leveraging%2520in-context%250Alearning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520to%2520perform%2520schema-constrained%250Adata%2520annotation%252C%2520collecting%2520in-domain%2520training%2520instances%2520for%2520a%250ATransformer-based%2520relation%2520extraction%2520model%2520deployed%2520on%2520titles%2520and%2520abstracts%2520of%250Aresearch%2520papers%2520in%2520the%2520Architecture%252C%2520Construction%252C%2520Engineering%2520and%2520Operations%250A%2528AECO%2529%2520domain.%2520By%2520assessing%2520the%2520performance%2520gain%2520with%2520respect%2520to%2520a%2520baseline%250ADeep%2520Learning%2520architecture%2520trained%2520on%2520off-domain%2520data%252C%2520we%2520show%2520that%2520by%2520using%2520a%250Afew-shot%2520learning%2520strategy%2520with%2520structured%2520prompts%2520and%2520only%2520minimal%2520expert%250Aannotation%2520the%2520presented%2520approach%2520can%2520potentially%2520support%2520domain%2520adaptation%2520of%250Aa%2520science%2520KG%2520generation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Few-Shot%20Approach%20for%20Relation%20Extraction%20Domain%20Adaptation%20using%0A%20%20Large%20Language%20Models&entry.906535625=Vanni%20Zavarella%20and%20Juan%20Carlos%20Gamero-Salinas%20and%20Sergio%20Consoli&entry.1292438233=%20%20Knowledge%20graphs%20%28KGs%29%20have%20been%20successfully%20applied%20to%20the%20analysis%20of%0Acomplex%20scientific%20and%20technological%20domains%2C%20with%20automatic%20KG%20generation%0Amethods%20typically%20building%20upon%20relation%20extraction%20models%20capturing%0Afine-grained%20relations%20between%20domain%20entities%20in%20text.%20While%20these%20relations%0Aare%20fully%20applicable%20across%20scientific%20areas%2C%20existing%20models%20are%20trained%20on%0Afew%20domain-specific%20datasets%20such%20as%20SciERC%20and%20do%20not%20perform%20well%20on%20new%0Atarget%20domains.%20In%20this%20paper%2C%20we%20experiment%20with%20leveraging%20in-context%0Alearning%20capabilities%20of%20Large%20Language%20Models%20to%20perform%20schema-constrained%0Adata%20annotation%2C%20collecting%20in-domain%20training%20instances%20for%20a%0ATransformer-based%20relation%20extraction%20model%20deployed%20on%20titles%20and%20abstracts%20of%0Aresearch%20papers%20in%20the%20Architecture%2C%20Construction%2C%20Engineering%20and%20Operations%0A%28AECO%29%20domain.%20By%20assessing%20the%20performance%20gain%20with%20respect%20to%20a%20baseline%0ADeep%20Learning%20architecture%20trained%20on%20off-domain%20data%2C%20we%20show%20that%20by%20using%20a%0Afew-shot%20learning%20strategy%20with%20structured%20prompts%20and%20only%20minimal%20expert%0Aannotation%20the%20presented%20approach%20can%20potentially%20support%20domain%20adaptation%20of%0Aa%20science%20KG%20generation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02377v1&entry.124074799=Read"},
{"title": "Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need", "author": "Da-Wei Zhou and Zi-Wen Cai and Han-Jia Ye and De-Chuan Zhan and Ziwei Liu", "abstract": "  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred for CIL. In this work, we revisit\nCIL with PTMs and argue that the core factors in CIL are adaptivity for model\nupdating and generalizability for knowledge transferring. 1) We first reveal\nthat frozen PTM can already provide generalizable embeddings for CIL.\nSurprisingly, a simple baseline (SimpleCIL) which continually sets the\nclassifiers of PTM to prototype features can beat state-of-the-art even without\ntraining on the downstream task. 2) Due to the distribution gap between\npre-trained and downstream datasets, PTM can be further cultivated with\nadaptivity via model adaptation. We propose AdaPt and mERge (APER), which\naggregates the embeddings of PTM and adapted models for classifier\nconstruction. APER is a general framework that can be orthogonally combined\nwith any parameter-efficient tuning method, which holds the advantages of PTM's\ngeneralizability and adapted model's adaptivity. 3) Additionally, considering\nprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to data\noverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,\nObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the\neffectiveness of APER with a unified and concise framework. Code is available\nat https://github.com/zhoudw-zdw/RevisitingCIL\n", "link": "http://arxiv.org/abs/2303.07338v2", "date": "2024-08-05", "relevancy": 1.9969, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5467}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4668}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Class-Incremental%20Learning%20with%20Pre-Trained%20Models%3A%0A%20%20Generalizability%20and%20Adaptivity%20are%20All%20You%20Need&body=Title%3A%20Revisiting%20Class-Incremental%20Learning%20with%20Pre-Trained%20Models%3A%0A%20%20Generalizability%20and%20Adaptivity%20are%20All%20You%20Need%0AAuthor%3A%20Da-Wei%20Zhou%20and%20Zi-Wen%20Cai%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Class-incremental%20learning%20%28CIL%29%20aims%20to%20adapt%20to%20emerging%20new%20classes%0Awithout%20forgetting%20old%20ones.%20Traditional%20CIL%20models%20are%20trained%20from%20scratch%20to%0Acontinually%20acquire%20knowledge%20as%20data%20evolves.%20Recently%2C%20pre-training%20has%0Aachieved%20substantial%20progress%2C%20making%20vast%20pre-trained%20models%20%28PTMs%29%20accessible%0Afor%20CIL.%20Contrary%20to%20traditional%20methods%2C%20PTMs%20possess%20generalizable%0Aembeddings%2C%20which%20can%20be%20easily%20transferred%20for%20CIL.%20In%20this%20work%2C%20we%20revisit%0ACIL%20with%20PTMs%20and%20argue%20that%20the%20core%20factors%20in%20CIL%20are%20adaptivity%20for%20model%0Aupdating%20and%20generalizability%20for%20knowledge%20transferring.%201%29%20We%20first%20reveal%0Athat%20frozen%20PTM%20can%20already%20provide%20generalizable%20embeddings%20for%20CIL.%0ASurprisingly%2C%20a%20simple%20baseline%20%28SimpleCIL%29%20which%20continually%20sets%20the%0Aclassifiers%20of%20PTM%20to%20prototype%20features%20can%20beat%20state-of-the-art%20even%20without%0Atraining%20on%20the%20downstream%20task.%202%29%20Due%20to%20the%20distribution%20gap%20between%0Apre-trained%20and%20downstream%20datasets%2C%20PTM%20can%20be%20further%20cultivated%20with%0Aadaptivity%20via%20model%20adaptation.%20We%20propose%20AdaPt%20and%20mERge%20%28APER%29%2C%20which%0Aaggregates%20the%20embeddings%20of%20PTM%20and%20adapted%20models%20for%20classifier%0Aconstruction.%20APER%20is%20a%20general%20framework%20that%20can%20be%20orthogonally%20combined%0Awith%20any%20parameter-efficient%20tuning%20method%2C%20which%20holds%20the%20advantages%20of%20PTM%27s%0Ageneralizability%20and%20adapted%20model%27s%20adaptivity.%203%29%20Additionally%2C%20considering%0Aprevious%20ImageNet-based%20benchmarks%20are%20unsuitable%20in%20the%20era%20of%20PTM%20due%20to%20data%0Aoverlapping%2C%20we%20propose%20four%20new%20benchmarks%20for%20assessment%2C%20namely%20ImageNet-A%2C%0AObjectNet%2C%20OmniBenchmark%2C%20and%20VTAB.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20APER%20with%20a%20unified%20and%20concise%20framework.%20Code%20is%20available%0Aat%20https%3A//github.com/zhoudw-zdw/RevisitingCIL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07338v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Class-Incremental%2520Learning%2520with%2520Pre-Trained%2520Models%253A%250A%2520%2520Generalizability%2520and%2520Adaptivity%2520are%2520All%2520You%2520Need%26entry.906535625%3DDa-Wei%2520Zhou%2520and%2520Zi-Wen%2520Cai%2520and%2520Han-Jia%2520Ye%2520and%2520De-Chuan%2520Zhan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Class-incremental%2520learning%2520%2528CIL%2529%2520aims%2520to%2520adapt%2520to%2520emerging%2520new%2520classes%250Awithout%2520forgetting%2520old%2520ones.%2520Traditional%2520CIL%2520models%2520are%2520trained%2520from%2520scratch%2520to%250Acontinually%2520acquire%2520knowledge%2520as%2520data%2520evolves.%2520Recently%252C%2520pre-training%2520has%250Aachieved%2520substantial%2520progress%252C%2520making%2520vast%2520pre-trained%2520models%2520%2528PTMs%2529%2520accessible%250Afor%2520CIL.%2520Contrary%2520to%2520traditional%2520methods%252C%2520PTMs%2520possess%2520generalizable%250Aembeddings%252C%2520which%2520can%2520be%2520easily%2520transferred%2520for%2520CIL.%2520In%2520this%2520work%252C%2520we%2520revisit%250ACIL%2520with%2520PTMs%2520and%2520argue%2520that%2520the%2520core%2520factors%2520in%2520CIL%2520are%2520adaptivity%2520for%2520model%250Aupdating%2520and%2520generalizability%2520for%2520knowledge%2520transferring.%25201%2529%2520We%2520first%2520reveal%250Athat%2520frozen%2520PTM%2520can%2520already%2520provide%2520generalizable%2520embeddings%2520for%2520CIL.%250ASurprisingly%252C%2520a%2520simple%2520baseline%2520%2528SimpleCIL%2529%2520which%2520continually%2520sets%2520the%250Aclassifiers%2520of%2520PTM%2520to%2520prototype%2520features%2520can%2520beat%2520state-of-the-art%2520even%2520without%250Atraining%2520on%2520the%2520downstream%2520task.%25202%2529%2520Due%2520to%2520the%2520distribution%2520gap%2520between%250Apre-trained%2520and%2520downstream%2520datasets%252C%2520PTM%2520can%2520be%2520further%2520cultivated%2520with%250Aadaptivity%2520via%2520model%2520adaptation.%2520We%2520propose%2520AdaPt%2520and%2520mERge%2520%2528APER%2529%252C%2520which%250Aaggregates%2520the%2520embeddings%2520of%2520PTM%2520and%2520adapted%2520models%2520for%2520classifier%250Aconstruction.%2520APER%2520is%2520a%2520general%2520framework%2520that%2520can%2520be%2520orthogonally%2520combined%250Awith%2520any%2520parameter-efficient%2520tuning%2520method%252C%2520which%2520holds%2520the%2520advantages%2520of%2520PTM%2527s%250Ageneralizability%2520and%2520adapted%2520model%2527s%2520adaptivity.%25203%2529%2520Additionally%252C%2520considering%250Aprevious%2520ImageNet-based%2520benchmarks%2520are%2520unsuitable%2520in%2520the%2520era%2520of%2520PTM%2520due%2520to%2520data%250Aoverlapping%252C%2520we%2520propose%2520four%2520new%2520benchmarks%2520for%2520assessment%252C%2520namely%2520ImageNet-A%252C%250AObjectNet%252C%2520OmniBenchmark%252C%2520and%2520VTAB.%2520Extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520APER%2520with%2520a%2520unified%2520and%2520concise%2520framework.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/zhoudw-zdw/RevisitingCIL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.07338v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Class-Incremental%20Learning%20with%20Pre-Trained%20Models%3A%0A%20%20Generalizability%20and%20Adaptivity%20are%20All%20You%20Need&entry.906535625=Da-Wei%20Zhou%20and%20Zi-Wen%20Cai%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%20and%20Ziwei%20Liu&entry.1292438233=%20%20Class-incremental%20learning%20%28CIL%29%20aims%20to%20adapt%20to%20emerging%20new%20classes%0Awithout%20forgetting%20old%20ones.%20Traditional%20CIL%20models%20are%20trained%20from%20scratch%20to%0Acontinually%20acquire%20knowledge%20as%20data%20evolves.%20Recently%2C%20pre-training%20has%0Aachieved%20substantial%20progress%2C%20making%20vast%20pre-trained%20models%20%28PTMs%29%20accessible%0Afor%20CIL.%20Contrary%20to%20traditional%20methods%2C%20PTMs%20possess%20generalizable%0Aembeddings%2C%20which%20can%20be%20easily%20transferred%20for%20CIL.%20In%20this%20work%2C%20we%20revisit%0ACIL%20with%20PTMs%20and%20argue%20that%20the%20core%20factors%20in%20CIL%20are%20adaptivity%20for%20model%0Aupdating%20and%20generalizability%20for%20knowledge%20transferring.%201%29%20We%20first%20reveal%0Athat%20frozen%20PTM%20can%20already%20provide%20generalizable%20embeddings%20for%20CIL.%0ASurprisingly%2C%20a%20simple%20baseline%20%28SimpleCIL%29%20which%20continually%20sets%20the%0Aclassifiers%20of%20PTM%20to%20prototype%20features%20can%20beat%20state-of-the-art%20even%20without%0Atraining%20on%20the%20downstream%20task.%202%29%20Due%20to%20the%20distribution%20gap%20between%0Apre-trained%20and%20downstream%20datasets%2C%20PTM%20can%20be%20further%20cultivated%20with%0Aadaptivity%20via%20model%20adaptation.%20We%20propose%20AdaPt%20and%20mERge%20%28APER%29%2C%20which%0Aaggregates%20the%20embeddings%20of%20PTM%20and%20adapted%20models%20for%20classifier%0Aconstruction.%20APER%20is%20a%20general%20framework%20that%20can%20be%20orthogonally%20combined%0Awith%20any%20parameter-efficient%20tuning%20method%2C%20which%20holds%20the%20advantages%20of%20PTM%27s%0Ageneralizability%20and%20adapted%20model%27s%20adaptivity.%203%29%20Additionally%2C%20considering%0Aprevious%20ImageNet-based%20benchmarks%20are%20unsuitable%20in%20the%20era%20of%20PTM%20due%20to%20data%0Aoverlapping%2C%20we%20propose%20four%20new%20benchmarks%20for%20assessment%2C%20namely%20ImageNet-A%2C%0AObjectNet%2C%20OmniBenchmark%2C%20and%20VTAB.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20APER%20with%20a%20unified%20and%20concise%20framework.%20Code%20is%20available%0Aat%20https%3A//github.com/zhoudw-zdw/RevisitingCIL%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07338v2&entry.124074799=Read"},
{"title": "Dissecting Deep RL with High Update Ratios: Combatting Value Divergence", "author": "Marcel Hussing and Claas Voelcker and Igor Gilitschenski and Amir-massoud Farahmand and Eric Eaton", "abstract": "  We show that deep reinforcement learning algorithms can retain their ability\nto learn without resetting network parameters in settings where the number of\ngradient updates greatly exceeds the number of environment samples by\ncombatting value function divergence. Under large update-to-data ratios, a\nrecent study by Nikishin et al. (2022) suggested the emergence of a primacy\nbias, in which agents overfit early interactions and downplay later experience,\nimpairing their ability to learn. In this work, we investigate the phenomena\nleading to the primacy bias. We inspect the early stages of training that were\nconjectured to cause the failure to learn and find that one fundamental\nchallenge is a long-standing acquaintance: value function divergence.\nOverinflated Q-values are found not only on out-of-distribution but also\nin-distribution data and can be linked to overestimation on unseen action\nprediction propelled by optimizer momentum. We employ a simple unit-ball\nnormalization that enables learning under large update ratios, show its\nefficacy on the widely used dm_control suite, and obtain strong performance on\nthe challenging dog tasks, competitive with model-based approaches. Our results\nquestion, in parts, the prior explanation for sub-optimal learning due to\noverfitting early data.\n", "link": "http://arxiv.org/abs/2403.05996v3", "date": "2024-08-05", "relevancy": 1.9965, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5022}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4987}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Deep%20RL%20with%20High%20Update%20Ratios%3A%20Combatting%20Value%20Divergence&body=Title%3A%20Dissecting%20Deep%20RL%20with%20High%20Update%20Ratios%3A%20Combatting%20Value%20Divergence%0AAuthor%3A%20Marcel%20Hussing%20and%20Claas%20Voelcker%20and%20Igor%20Gilitschenski%20and%20Amir-massoud%20Farahmand%20and%20Eric%20Eaton%0AAbstract%3A%20%20%20We%20show%20that%20deep%20reinforcement%20learning%20algorithms%20can%20retain%20their%20ability%0Ato%20learn%20without%20resetting%20network%20parameters%20in%20settings%20where%20the%20number%20of%0Agradient%20updates%20greatly%20exceeds%20the%20number%20of%20environment%20samples%20by%0Acombatting%20value%20function%20divergence.%20Under%20large%20update-to-data%20ratios%2C%20a%0Arecent%20study%20by%20Nikishin%20et%20al.%20%282022%29%20suggested%20the%20emergence%20of%20a%20primacy%0Abias%2C%20in%20which%20agents%20overfit%20early%20interactions%20and%20downplay%20later%20experience%2C%0Aimpairing%20their%20ability%20to%20learn.%20In%20this%20work%2C%20we%20investigate%20the%20phenomena%0Aleading%20to%20the%20primacy%20bias.%20We%20inspect%20the%20early%20stages%20of%20training%20that%20were%0Aconjectured%20to%20cause%20the%20failure%20to%20learn%20and%20find%20that%20one%20fundamental%0Achallenge%20is%20a%20long-standing%20acquaintance%3A%20value%20function%20divergence.%0AOverinflated%20Q-values%20are%20found%20not%20only%20on%20out-of-distribution%20but%20also%0Ain-distribution%20data%20and%20can%20be%20linked%20to%20overestimation%20on%20unseen%20action%0Aprediction%20propelled%20by%20optimizer%20momentum.%20We%20employ%20a%20simple%20unit-ball%0Anormalization%20that%20enables%20learning%20under%20large%20update%20ratios%2C%20show%20its%0Aefficacy%20on%20the%20widely%20used%20dm_control%20suite%2C%20and%20obtain%20strong%20performance%20on%0Athe%20challenging%20dog%20tasks%2C%20competitive%20with%20model-based%20approaches.%20Our%20results%0Aquestion%2C%20in%20parts%2C%20the%20prior%20explanation%20for%20sub-optimal%20learning%20due%20to%0Aoverfitting%20early%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05996v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Deep%2520RL%2520with%2520High%2520Update%2520Ratios%253A%2520Combatting%2520Value%2520Divergence%26entry.906535625%3DMarcel%2520Hussing%2520and%2520Claas%2520Voelcker%2520and%2520Igor%2520Gilitschenski%2520and%2520Amir-massoud%2520Farahmand%2520and%2520Eric%2520Eaton%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520deep%2520reinforcement%2520learning%2520algorithms%2520can%2520retain%2520their%2520ability%250Ato%2520learn%2520without%2520resetting%2520network%2520parameters%2520in%2520settings%2520where%2520the%2520number%2520of%250Agradient%2520updates%2520greatly%2520exceeds%2520the%2520number%2520of%2520environment%2520samples%2520by%250Acombatting%2520value%2520function%2520divergence.%2520Under%2520large%2520update-to-data%2520ratios%252C%2520a%250Arecent%2520study%2520by%2520Nikishin%2520et%2520al.%2520%25282022%2529%2520suggested%2520the%2520emergence%2520of%2520a%2520primacy%250Abias%252C%2520in%2520which%2520agents%2520overfit%2520early%2520interactions%2520and%2520downplay%2520later%2520experience%252C%250Aimpairing%2520their%2520ability%2520to%2520learn.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520phenomena%250Aleading%2520to%2520the%2520primacy%2520bias.%2520We%2520inspect%2520the%2520early%2520stages%2520of%2520training%2520that%2520were%250Aconjectured%2520to%2520cause%2520the%2520failure%2520to%2520learn%2520and%2520find%2520that%2520one%2520fundamental%250Achallenge%2520is%2520a%2520long-standing%2520acquaintance%253A%2520value%2520function%2520divergence.%250AOverinflated%2520Q-values%2520are%2520found%2520not%2520only%2520on%2520out-of-distribution%2520but%2520also%250Ain-distribution%2520data%2520and%2520can%2520be%2520linked%2520to%2520overestimation%2520on%2520unseen%2520action%250Aprediction%2520propelled%2520by%2520optimizer%2520momentum.%2520We%2520employ%2520a%2520simple%2520unit-ball%250Anormalization%2520that%2520enables%2520learning%2520under%2520large%2520update%2520ratios%252C%2520show%2520its%250Aefficacy%2520on%2520the%2520widely%2520used%2520dm_control%2520suite%252C%2520and%2520obtain%2520strong%2520performance%2520on%250Athe%2520challenging%2520dog%2520tasks%252C%2520competitive%2520with%2520model-based%2520approaches.%2520Our%2520results%250Aquestion%252C%2520in%2520parts%252C%2520the%2520prior%2520explanation%2520for%2520sub-optimal%2520learning%2520due%2520to%250Aoverfitting%2520early%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05996v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Deep%20RL%20with%20High%20Update%20Ratios%3A%20Combatting%20Value%20Divergence&entry.906535625=Marcel%20Hussing%20and%20Claas%20Voelcker%20and%20Igor%20Gilitschenski%20and%20Amir-massoud%20Farahmand%20and%20Eric%20Eaton&entry.1292438233=%20%20We%20show%20that%20deep%20reinforcement%20learning%20algorithms%20can%20retain%20their%20ability%0Ato%20learn%20without%20resetting%20network%20parameters%20in%20settings%20where%20the%20number%20of%0Agradient%20updates%20greatly%20exceeds%20the%20number%20of%20environment%20samples%20by%0Acombatting%20value%20function%20divergence.%20Under%20large%20update-to-data%20ratios%2C%20a%0Arecent%20study%20by%20Nikishin%20et%20al.%20%282022%29%20suggested%20the%20emergence%20of%20a%20primacy%0Abias%2C%20in%20which%20agents%20overfit%20early%20interactions%20and%20downplay%20later%20experience%2C%0Aimpairing%20their%20ability%20to%20learn.%20In%20this%20work%2C%20we%20investigate%20the%20phenomena%0Aleading%20to%20the%20primacy%20bias.%20We%20inspect%20the%20early%20stages%20of%20training%20that%20were%0Aconjectured%20to%20cause%20the%20failure%20to%20learn%20and%20find%20that%20one%20fundamental%0Achallenge%20is%20a%20long-standing%20acquaintance%3A%20value%20function%20divergence.%0AOverinflated%20Q-values%20are%20found%20not%20only%20on%20out-of-distribution%20but%20also%0Ain-distribution%20data%20and%20can%20be%20linked%20to%20overestimation%20on%20unseen%20action%0Aprediction%20propelled%20by%20optimizer%20momentum.%20We%20employ%20a%20simple%20unit-ball%0Anormalization%20that%20enables%20learning%20under%20large%20update%20ratios%2C%20show%20its%0Aefficacy%20on%20the%20widely%20used%20dm_control%20suite%2C%20and%20obtain%20strong%20performance%20on%0Athe%20challenging%20dog%20tasks%2C%20competitive%20with%20model-based%20approaches.%20Our%20results%0Aquestion%2C%20in%20parts%2C%20the%20prior%20explanation%20for%20sub-optimal%20learning%20due%20to%0Aoverfitting%20early%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05996v3&entry.124074799=Read"},
{"title": "Fairness and Bias Mitigation in Computer Vision: A Survey", "author": "Sepehr Dehdashtian and Ruozhen He and Yi Li and Guha Balakrishnan and Nuno Vasconcelos and Vicente Ordonez and Vishnu Naresh Boddeti", "abstract": "  Computer vision systems have witnessed rapid progress over the past two\ndecades due to multiple advances in the field. As these systems are\nincreasingly being deployed in high-stakes real-world applications, there is a\ndire need to ensure that they do not propagate or amplify any discriminatory\ntendencies in historical or human-curated data or inadvertently learn biases\nfrom spurious correlations. This paper presents a comprehensive survey on\nfairness that summarizes and sheds light on ongoing trends and successes in the\ncontext of computer vision. The topics we discuss include 1) The origin and\ntechnical definitions of fairness drawn from the wider fair machine learning\nliterature and adjacent disciplines. 2) Work that sought to discover and\nanalyze biases in computer vision systems. 3) A summary of methods proposed to\nmitigate bias in computer vision systems in recent years. 4) A comprehensive\nsummary of resources and datasets produced by researchers to measure, analyze,\nand mitigate bias and enhance fairness. 5) Discussion of the field's success,\ncontinuing trends in the context of multimodal foundation and generative\nmodels, and gaps that still need to be addressed. The presented\ncharacterization should help researchers understand the importance of\nidentifying and mitigating bias in computer vision and the state of the field\nand identify potential directions for future research.\n", "link": "http://arxiv.org/abs/2408.02464v1", "date": "2024-08-05", "relevancy": 1.9859, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5121}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4964}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20and%20Bias%20Mitigation%20in%20Computer%20Vision%3A%20A%20Survey&body=Title%3A%20Fairness%20and%20Bias%20Mitigation%20in%20Computer%20Vision%3A%20A%20Survey%0AAuthor%3A%20Sepehr%20Dehdashtian%20and%20Ruozhen%20He%20and%20Yi%20Li%20and%20Guha%20Balakrishnan%20and%20Nuno%20Vasconcelos%20and%20Vicente%20Ordonez%20and%20Vishnu%20Naresh%20Boddeti%0AAbstract%3A%20%20%20Computer%20vision%20systems%20have%20witnessed%20rapid%20progress%20over%20the%20past%20two%0Adecades%20due%20to%20multiple%20advances%20in%20the%20field.%20As%20these%20systems%20are%0Aincreasingly%20being%20deployed%20in%20high-stakes%20real-world%20applications%2C%20there%20is%20a%0Adire%20need%20to%20ensure%20that%20they%20do%20not%20propagate%20or%20amplify%20any%20discriminatory%0Atendencies%20in%20historical%20or%20human-curated%20data%20or%20inadvertently%20learn%20biases%0Afrom%20spurious%20correlations.%20This%20paper%20presents%20a%20comprehensive%20survey%20on%0Afairness%20that%20summarizes%20and%20sheds%20light%20on%20ongoing%20trends%20and%20successes%20in%20the%0Acontext%20of%20computer%20vision.%20The%20topics%20we%20discuss%20include%201%29%20The%20origin%20and%0Atechnical%20definitions%20of%20fairness%20drawn%20from%20the%20wider%20fair%20machine%20learning%0Aliterature%20and%20adjacent%20disciplines.%202%29%20Work%20that%20sought%20to%20discover%20and%0Aanalyze%20biases%20in%20computer%20vision%20systems.%203%29%20A%20summary%20of%20methods%20proposed%20to%0Amitigate%20bias%20in%20computer%20vision%20systems%20in%20recent%20years.%204%29%20A%20comprehensive%0Asummary%20of%20resources%20and%20datasets%20produced%20by%20researchers%20to%20measure%2C%20analyze%2C%0Aand%20mitigate%20bias%20and%20enhance%20fairness.%205%29%20Discussion%20of%20the%20field%27s%20success%2C%0Acontinuing%20trends%20in%20the%20context%20of%20multimodal%20foundation%20and%20generative%0Amodels%2C%20and%20gaps%20that%20still%20need%20to%20be%20addressed.%20The%20presented%0Acharacterization%20should%20help%20researchers%20understand%20the%20importance%20of%0Aidentifying%20and%20mitigating%20bias%20in%20computer%20vision%20and%20the%20state%20of%20the%20field%0Aand%20identify%20potential%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520and%2520Bias%2520Mitigation%2520in%2520Computer%2520Vision%253A%2520A%2520Survey%26entry.906535625%3DSepehr%2520Dehdashtian%2520and%2520Ruozhen%2520He%2520and%2520Yi%2520Li%2520and%2520Guha%2520Balakrishnan%2520and%2520Nuno%2520Vasconcelos%2520and%2520Vicente%2520Ordonez%2520and%2520Vishnu%2520Naresh%2520Boddeti%26entry.1292438233%3D%2520%2520Computer%2520vision%2520systems%2520have%2520witnessed%2520rapid%2520progress%2520over%2520the%2520past%2520two%250Adecades%2520due%2520to%2520multiple%2520advances%2520in%2520the%2520field.%2520As%2520these%2520systems%2520are%250Aincreasingly%2520being%2520deployed%2520in%2520high-stakes%2520real-world%2520applications%252C%2520there%2520is%2520a%250Adire%2520need%2520to%2520ensure%2520that%2520they%2520do%2520not%2520propagate%2520or%2520amplify%2520any%2520discriminatory%250Atendencies%2520in%2520historical%2520or%2520human-curated%2520data%2520or%2520inadvertently%2520learn%2520biases%250Afrom%2520spurious%2520correlations.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520survey%2520on%250Afairness%2520that%2520summarizes%2520and%2520sheds%2520light%2520on%2520ongoing%2520trends%2520and%2520successes%2520in%2520the%250Acontext%2520of%2520computer%2520vision.%2520The%2520topics%2520we%2520discuss%2520include%25201%2529%2520The%2520origin%2520and%250Atechnical%2520definitions%2520of%2520fairness%2520drawn%2520from%2520the%2520wider%2520fair%2520machine%2520learning%250Aliterature%2520and%2520adjacent%2520disciplines.%25202%2529%2520Work%2520that%2520sought%2520to%2520discover%2520and%250Aanalyze%2520biases%2520in%2520computer%2520vision%2520systems.%25203%2529%2520A%2520summary%2520of%2520methods%2520proposed%2520to%250Amitigate%2520bias%2520in%2520computer%2520vision%2520systems%2520in%2520recent%2520years.%25204%2529%2520A%2520comprehensive%250Asummary%2520of%2520resources%2520and%2520datasets%2520produced%2520by%2520researchers%2520to%2520measure%252C%2520analyze%252C%250Aand%2520mitigate%2520bias%2520and%2520enhance%2520fairness.%25205%2529%2520Discussion%2520of%2520the%2520field%2527s%2520success%252C%250Acontinuing%2520trends%2520in%2520the%2520context%2520of%2520multimodal%2520foundation%2520and%2520generative%250Amodels%252C%2520and%2520gaps%2520that%2520still%2520need%2520to%2520be%2520addressed.%2520The%2520presented%250Acharacterization%2520should%2520help%2520researchers%2520understand%2520the%2520importance%2520of%250Aidentifying%2520and%2520mitigating%2520bias%2520in%2520computer%2520vision%2520and%2520the%2520state%2520of%2520the%2520field%250Aand%2520identify%2520potential%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20and%20Bias%20Mitigation%20in%20Computer%20Vision%3A%20A%20Survey&entry.906535625=Sepehr%20Dehdashtian%20and%20Ruozhen%20He%20and%20Yi%20Li%20and%20Guha%20Balakrishnan%20and%20Nuno%20Vasconcelos%20and%20Vicente%20Ordonez%20and%20Vishnu%20Naresh%20Boddeti&entry.1292438233=%20%20Computer%20vision%20systems%20have%20witnessed%20rapid%20progress%20over%20the%20past%20two%0Adecades%20due%20to%20multiple%20advances%20in%20the%20field.%20As%20these%20systems%20are%0Aincreasingly%20being%20deployed%20in%20high-stakes%20real-world%20applications%2C%20there%20is%20a%0Adire%20need%20to%20ensure%20that%20they%20do%20not%20propagate%20or%20amplify%20any%20discriminatory%0Atendencies%20in%20historical%20or%20human-curated%20data%20or%20inadvertently%20learn%20biases%0Afrom%20spurious%20correlations.%20This%20paper%20presents%20a%20comprehensive%20survey%20on%0Afairness%20that%20summarizes%20and%20sheds%20light%20on%20ongoing%20trends%20and%20successes%20in%20the%0Acontext%20of%20computer%20vision.%20The%20topics%20we%20discuss%20include%201%29%20The%20origin%20and%0Atechnical%20definitions%20of%20fairness%20drawn%20from%20the%20wider%20fair%20machine%20learning%0Aliterature%20and%20adjacent%20disciplines.%202%29%20Work%20that%20sought%20to%20discover%20and%0Aanalyze%20biases%20in%20computer%20vision%20systems.%203%29%20A%20summary%20of%20methods%20proposed%20to%0Amitigate%20bias%20in%20computer%20vision%20systems%20in%20recent%20years.%204%29%20A%20comprehensive%0Asummary%20of%20resources%20and%20datasets%20produced%20by%20researchers%20to%20measure%2C%20analyze%2C%0Aand%20mitigate%20bias%20and%20enhance%20fairness.%205%29%20Discussion%20of%20the%20field%27s%20success%2C%0Acontinuing%20trends%20in%20the%20context%20of%20multimodal%20foundation%20and%20generative%0Amodels%2C%20and%20gaps%20that%20still%20need%20to%20be%20addressed.%20The%20presented%0Acharacterization%20should%20help%20researchers%20understand%20the%20importance%20of%0Aidentifying%20and%20mitigating%20bias%20in%20computer%20vision%20and%20the%20state%20of%20the%20field%0Aand%20identify%20potential%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02464v1&entry.124074799=Read"},
{"title": "Self-Taught Evaluators", "author": "Tianlu Wang and Ilia Kulikov and Olga Golovneva and Ping Yu and Weizhe Yuan and Jane Dwivedi-Yu and Richard Yuanzhe Pang and Maryam Fazel-Zarandi and Jason Weston and Xian Li", "abstract": "  Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.\n", "link": "http://arxiv.org/abs/2408.02666v1", "date": "2024-08-05", "relevancy": 1.979, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5353}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4871}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Taught%20Evaluators&body=Title%3A%20Self-Taught%20Evaluators%0AAuthor%3A%20Tianlu%20Wang%20and%20Ilia%20Kulikov%20and%20Olga%20Golovneva%20and%20Ping%20Yu%20and%20Weizhe%20Yuan%20and%20Jane%20Dwivedi-Yu%20and%20Richard%20Yuanzhe%20Pang%20and%20Maryam%20Fazel-Zarandi%20and%20Jason%20Weston%20and%20Xian%20Li%0AAbstract%3A%20%20%20Model-based%20evaluation%20is%20at%20the%20heart%20of%20successful%20model%20development%20--%20as%0Aa%20reward%20model%20for%20training%2C%20and%20as%20a%20replacement%20for%20human%20evaluation.%20To%0Atrain%20such%20evaluators%2C%20the%20standard%20approach%20is%20to%20collect%20a%20large%20amount%20of%0Ahuman%20preference%20judgments%20over%20model%20responses%2C%20which%20is%20costly%20and%20the%20data%0Abecomes%20stale%20as%20models%20improve.%20In%20this%20work%2C%20we%20present%20an%20approach%20that%20aims%0Ato%20im-prove%20evaluators%20without%20human%20annotations%2C%20using%20synthetic%20training%20data%0Aonly.%20Starting%20from%20unlabeled%20instructions%2C%20our%20iterative%20self-improvement%0Ascheme%20generates%20contrasting%20model%20outputs%20and%20trains%20an%20LLM-as-a-Judge%20to%0Aproduce%20reasoning%20traces%20and%20final%20judgments%2C%20repeating%20this%20training%20at%20each%0Anew%20iteration%20using%20the%20improved%20predictions.%20Without%20any%20labeled%20preference%0Adata%2C%20our%20Self-Taught%20Evaluator%20can%20improve%20a%20strong%20LLM%20%28Llama3-70B-Instruct%29%0Afrom%2075.4%20to%2088.3%20%2888.7%20with%20majority%20vote%29%20on%20RewardBench.%20This%20outperforms%0Acommonly%20used%20LLM%20judges%20such%20as%20GPT-4%20and%20matches%20the%20performance%20of%20the%0Atop-performing%20reward%20models%20trained%20with%20labeled%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Taught%2520Evaluators%26entry.906535625%3DTianlu%2520Wang%2520and%2520Ilia%2520Kulikov%2520and%2520Olga%2520Golovneva%2520and%2520Ping%2520Yu%2520and%2520Weizhe%2520Yuan%2520and%2520Jane%2520Dwivedi-Yu%2520and%2520Richard%2520Yuanzhe%2520Pang%2520and%2520Maryam%2520Fazel-Zarandi%2520and%2520Jason%2520Weston%2520and%2520Xian%2520Li%26entry.1292438233%3D%2520%2520Model-based%2520evaluation%2520is%2520at%2520the%2520heart%2520of%2520successful%2520model%2520development%2520--%2520as%250Aa%2520reward%2520model%2520for%2520training%252C%2520and%2520as%2520a%2520replacement%2520for%2520human%2520evaluation.%2520To%250Atrain%2520such%2520evaluators%252C%2520the%2520standard%2520approach%2520is%2520to%2520collect%2520a%2520large%2520amount%2520of%250Ahuman%2520preference%2520judgments%2520over%2520model%2520responses%252C%2520which%2520is%2520costly%2520and%2520the%2520data%250Abecomes%2520stale%2520as%2520models%2520improve.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520approach%2520that%2520aims%250Ato%2520im-prove%2520evaluators%2520without%2520human%2520annotations%252C%2520using%2520synthetic%2520training%2520data%250Aonly.%2520Starting%2520from%2520unlabeled%2520instructions%252C%2520our%2520iterative%2520self-improvement%250Ascheme%2520generates%2520contrasting%2520model%2520outputs%2520and%2520trains%2520an%2520LLM-as-a-Judge%2520to%250Aproduce%2520reasoning%2520traces%2520and%2520final%2520judgments%252C%2520repeating%2520this%2520training%2520at%2520each%250Anew%2520iteration%2520using%2520the%2520improved%2520predictions.%2520Without%2520any%2520labeled%2520preference%250Adata%252C%2520our%2520Self-Taught%2520Evaluator%2520can%2520improve%2520a%2520strong%2520LLM%2520%2528Llama3-70B-Instruct%2529%250Afrom%252075.4%2520to%252088.3%2520%252888.7%2520with%2520majority%2520vote%2529%2520on%2520RewardBench.%2520This%2520outperforms%250Acommonly%2520used%2520LLM%2520judges%2520such%2520as%2520GPT-4%2520and%2520matches%2520the%2520performance%2520of%2520the%250Atop-performing%2520reward%2520models%2520trained%2520with%2520labeled%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Taught%20Evaluators&entry.906535625=Tianlu%20Wang%20and%20Ilia%20Kulikov%20and%20Olga%20Golovneva%20and%20Ping%20Yu%20and%20Weizhe%20Yuan%20and%20Jane%20Dwivedi-Yu%20and%20Richard%20Yuanzhe%20Pang%20and%20Maryam%20Fazel-Zarandi%20and%20Jason%20Weston%20and%20Xian%20Li&entry.1292438233=%20%20Model-based%20evaluation%20is%20at%20the%20heart%20of%20successful%20model%20development%20--%20as%0Aa%20reward%20model%20for%20training%2C%20and%20as%20a%20replacement%20for%20human%20evaluation.%20To%0Atrain%20such%20evaluators%2C%20the%20standard%20approach%20is%20to%20collect%20a%20large%20amount%20of%0Ahuman%20preference%20judgments%20over%20model%20responses%2C%20which%20is%20costly%20and%20the%20data%0Abecomes%20stale%20as%20models%20improve.%20In%20this%20work%2C%20we%20present%20an%20approach%20that%20aims%0Ato%20im-prove%20evaluators%20without%20human%20annotations%2C%20using%20synthetic%20training%20data%0Aonly.%20Starting%20from%20unlabeled%20instructions%2C%20our%20iterative%20self-improvement%0Ascheme%20generates%20contrasting%20model%20outputs%20and%20trains%20an%20LLM-as-a-Judge%20to%0Aproduce%20reasoning%20traces%20and%20final%20judgments%2C%20repeating%20this%20training%20at%20each%0Anew%20iteration%20using%20the%20improved%20predictions.%20Without%20any%20labeled%20preference%0Adata%2C%20our%20Self-Taught%20Evaluator%20can%20improve%20a%20strong%20LLM%20%28Llama3-70B-Instruct%29%0Afrom%2075.4%20to%2088.3%20%2888.7%20with%20majority%20vote%29%20on%20RewardBench.%20This%20outperforms%0Acommonly%20used%20LLM%20judges%20such%20as%20GPT-4%20and%20matches%20the%20performance%20of%20the%0Atop-performing%20reward%20models%20trained%20with%20labeled%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02666v1&entry.124074799=Read"},
{"title": "PENDRAM: Enabling High-Performance and Energy-Efficient Processing of\n  Deep Neural Networks through a Generalized DRAM Data Mapping Policy", "author": "Rachmad Vidya Wicaksana Putra and Muhammad Abdullah Hanif and Muhammad Shafique", "abstract": "  Convolutional Neural Networks (CNNs), a prominent type of Deep Neural\nNetworks (DNNs), have emerged as a state-of-the-art solution for solving\nmachine learning tasks. To improve the performance and energy efficiency of CNN\ninference, the employment of specialized hardware accelerators is prevalent.\nHowever, CNN accelerators still face performance- and energy-efficiency\nchallenges due to high off-chip memory (DRAM) access latency and energy, which\nare especially crucial for latency- and energy-constrained embedded\napplications. Moreover, different DRAM architectures have different profiles of\naccess latency and energy, thus making it challenging to optimize them for high\nperformance and energy-efficient CNN accelerators. To address this, we present\nPENDRAM, a novel design space exploration methodology that enables\nhigh-performance and energy-efficient CNN acceleration through a generalized\nDRAM data mapping policy. Specifically, it explores the impact of different\nDRAM data mapping policies and DRAM architectures across different CNN\npartitioning and scheduling schemes on the DRAM access latency and energy, then\nidentifies the pareto-optimal design choices. The experimental results show\nthat our DRAM data mapping policy improves the energy-delay-product of DRAM\naccesses in the CNN accelerator over other mapping policies by up to 96%. In\nthis manner, our PENDRAM methodology offers high-performance and\nenergy-efficient CNN acceleration under any given DRAM architectures for\ndiverse embedded AI applications.\n", "link": "http://arxiv.org/abs/2408.02412v1", "date": "2024-08-05", "relevancy": 1.9784, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5234}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.493}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PENDRAM%3A%20Enabling%20High-Performance%20and%20Energy-Efficient%20Processing%20of%0A%20%20Deep%20Neural%20Networks%20through%20a%20Generalized%20DRAM%20Data%20Mapping%20Policy&body=Title%3A%20PENDRAM%3A%20Enabling%20High-Performance%20and%20Energy-Efficient%20Processing%20of%0A%20%20Deep%20Neural%20Networks%20through%20a%20Generalized%20DRAM%20Data%20Mapping%20Policy%0AAuthor%3A%20Rachmad%20Vidya%20Wicaksana%20Putra%20and%20Muhammad%20Abdullah%20Hanif%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20a%20prominent%20type%20of%20Deep%20Neural%0ANetworks%20%28DNNs%29%2C%20have%20emerged%20as%20a%20state-of-the-art%20solution%20for%20solving%0Amachine%20learning%20tasks.%20To%20improve%20the%20performance%20and%20energy%20efficiency%20of%20CNN%0Ainference%2C%20the%20employment%20of%20specialized%20hardware%20accelerators%20is%20prevalent.%0AHowever%2C%20CNN%20accelerators%20still%20face%20performance-%20and%20energy-efficiency%0Achallenges%20due%20to%20high%20off-chip%20memory%20%28DRAM%29%20access%20latency%20and%20energy%2C%20which%0Aare%20especially%20crucial%20for%20latency-%20and%20energy-constrained%20embedded%0Aapplications.%20Moreover%2C%20different%20DRAM%20architectures%20have%20different%20profiles%20of%0Aaccess%20latency%20and%20energy%2C%20thus%20making%20it%20challenging%20to%20optimize%20them%20for%20high%0Aperformance%20and%20energy-efficient%20CNN%20accelerators.%20To%20address%20this%2C%20we%20present%0APENDRAM%2C%20a%20novel%20design%20space%20exploration%20methodology%20that%20enables%0Ahigh-performance%20and%20energy-efficient%20CNN%20acceleration%20through%20a%20generalized%0ADRAM%20data%20mapping%20policy.%20Specifically%2C%20it%20explores%20the%20impact%20of%20different%0ADRAM%20data%20mapping%20policies%20and%20DRAM%20architectures%20across%20different%20CNN%0Apartitioning%20and%20scheduling%20schemes%20on%20the%20DRAM%20access%20latency%20and%20energy%2C%20then%0Aidentifies%20the%20pareto-optimal%20design%20choices.%20The%20experimental%20results%20show%0Athat%20our%20DRAM%20data%20mapping%20policy%20improves%20the%20energy-delay-product%20of%20DRAM%0Aaccesses%20in%20the%20CNN%20accelerator%20over%20other%20mapping%20policies%20by%20up%20to%2096%25.%20In%0Athis%20manner%2C%20our%20PENDRAM%20methodology%20offers%20high-performance%20and%0Aenergy-efficient%20CNN%20acceleration%20under%20any%20given%20DRAM%20architectures%20for%0Adiverse%20embedded%20AI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPENDRAM%253A%2520Enabling%2520High-Performance%2520and%2520Energy-Efficient%2520Processing%2520of%250A%2520%2520Deep%2520Neural%2520Networks%2520through%2520a%2520Generalized%2520DRAM%2520Data%2520Mapping%2520Policy%26entry.906535625%3DRachmad%2520Vidya%2520Wicaksana%2520Putra%2520and%2520Muhammad%2520Abdullah%2520Hanif%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520a%2520prominent%2520type%2520of%2520Deep%2520Neural%250ANetworks%2520%2528DNNs%2529%252C%2520have%2520emerged%2520as%2520a%2520state-of-the-art%2520solution%2520for%2520solving%250Amachine%2520learning%2520tasks.%2520To%2520improve%2520the%2520performance%2520and%2520energy%2520efficiency%2520of%2520CNN%250Ainference%252C%2520the%2520employment%2520of%2520specialized%2520hardware%2520accelerators%2520is%2520prevalent.%250AHowever%252C%2520CNN%2520accelerators%2520still%2520face%2520performance-%2520and%2520energy-efficiency%250Achallenges%2520due%2520to%2520high%2520off-chip%2520memory%2520%2528DRAM%2529%2520access%2520latency%2520and%2520energy%252C%2520which%250Aare%2520especially%2520crucial%2520for%2520latency-%2520and%2520energy-constrained%2520embedded%250Aapplications.%2520Moreover%252C%2520different%2520DRAM%2520architectures%2520have%2520different%2520profiles%2520of%250Aaccess%2520latency%2520and%2520energy%252C%2520thus%2520making%2520it%2520challenging%2520to%2520optimize%2520them%2520for%2520high%250Aperformance%2520and%2520energy-efficient%2520CNN%2520accelerators.%2520To%2520address%2520this%252C%2520we%2520present%250APENDRAM%252C%2520a%2520novel%2520design%2520space%2520exploration%2520methodology%2520that%2520enables%250Ahigh-performance%2520and%2520energy-efficient%2520CNN%2520acceleration%2520through%2520a%2520generalized%250ADRAM%2520data%2520mapping%2520policy.%2520Specifically%252C%2520it%2520explores%2520the%2520impact%2520of%2520different%250ADRAM%2520data%2520mapping%2520policies%2520and%2520DRAM%2520architectures%2520across%2520different%2520CNN%250Apartitioning%2520and%2520scheduling%2520schemes%2520on%2520the%2520DRAM%2520access%2520latency%2520and%2520energy%252C%2520then%250Aidentifies%2520the%2520pareto-optimal%2520design%2520choices.%2520The%2520experimental%2520results%2520show%250Athat%2520our%2520DRAM%2520data%2520mapping%2520policy%2520improves%2520the%2520energy-delay-product%2520of%2520DRAM%250Aaccesses%2520in%2520the%2520CNN%2520accelerator%2520over%2520other%2520mapping%2520policies%2520by%2520up%2520to%252096%2525.%2520In%250Athis%2520manner%252C%2520our%2520PENDRAM%2520methodology%2520offers%2520high-performance%2520and%250Aenergy-efficient%2520CNN%2520acceleration%2520under%2520any%2520given%2520DRAM%2520architectures%2520for%250Adiverse%2520embedded%2520AI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PENDRAM%3A%20Enabling%20High-Performance%20and%20Energy-Efficient%20Processing%20of%0A%20%20Deep%20Neural%20Networks%20through%20a%20Generalized%20DRAM%20Data%20Mapping%20Policy&entry.906535625=Rachmad%20Vidya%20Wicaksana%20Putra%20and%20Muhammad%20Abdullah%20Hanif%20and%20Muhammad%20Shafique&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20a%20prominent%20type%20of%20Deep%20Neural%0ANetworks%20%28DNNs%29%2C%20have%20emerged%20as%20a%20state-of-the-art%20solution%20for%20solving%0Amachine%20learning%20tasks.%20To%20improve%20the%20performance%20and%20energy%20efficiency%20of%20CNN%0Ainference%2C%20the%20employment%20of%20specialized%20hardware%20accelerators%20is%20prevalent.%0AHowever%2C%20CNN%20accelerators%20still%20face%20performance-%20and%20energy-efficiency%0Achallenges%20due%20to%20high%20off-chip%20memory%20%28DRAM%29%20access%20latency%20and%20energy%2C%20which%0Aare%20especially%20crucial%20for%20latency-%20and%20energy-constrained%20embedded%0Aapplications.%20Moreover%2C%20different%20DRAM%20architectures%20have%20different%20profiles%20of%0Aaccess%20latency%20and%20energy%2C%20thus%20making%20it%20challenging%20to%20optimize%20them%20for%20high%0Aperformance%20and%20energy-efficient%20CNN%20accelerators.%20To%20address%20this%2C%20we%20present%0APENDRAM%2C%20a%20novel%20design%20space%20exploration%20methodology%20that%20enables%0Ahigh-performance%20and%20energy-efficient%20CNN%20acceleration%20through%20a%20generalized%0ADRAM%20data%20mapping%20policy.%20Specifically%2C%20it%20explores%20the%20impact%20of%20different%0ADRAM%20data%20mapping%20policies%20and%20DRAM%20architectures%20across%20different%20CNN%0Apartitioning%20and%20scheduling%20schemes%20on%20the%20DRAM%20access%20latency%20and%20energy%2C%20then%0Aidentifies%20the%20pareto-optimal%20design%20choices.%20The%20experimental%20results%20show%0Athat%20our%20DRAM%20data%20mapping%20policy%20improves%20the%20energy-delay-product%20of%20DRAM%0Aaccesses%20in%20the%20CNN%20accelerator%20over%20other%20mapping%20policies%20by%20up%20to%2096%25.%20In%0Athis%20manner%2C%20our%20PENDRAM%20methodology%20offers%20high-performance%20and%0Aenergy-efficient%20CNN%20acceleration%20under%20any%20given%20DRAM%20architectures%20for%0Adiverse%20embedded%20AI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02412v1&entry.124074799=Read"},
{"title": "Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human\n  Annotation: A Case Study Using Schedule-of-Event Table Detection", "author": "Bhawesh Kumar and Jonathan Amar and Eric Yang and Nan Li and Yugang Jia", "abstract": "  Large Language Models (LLMs) have demonstrated their efficacy across a broad\nspectrum of tasks in healthcare applications. However, often LLMs need to be\nfine-tuned on task-specific expert annotated data to achieve optimal\nperformance, which can be expensive and time consuming. In this study, we\nfine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels\nobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)\ntables, which specify care plan in clinical trial protocols. We introduce a\nfiltering mechanism to select high-confidence labels for this table\nclassification task, thereby reducing the noise in the auto-generated labels.\nWe show that fine-tuned PaLM-2 with those labels achieves performance that\nexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is\nclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our\nresults show that leveraging LLM-generated labels through powerful models like\ngemini-pro can potentially serve as a viable strategy for improving LLM\nperformance through fine-tuning in specialized tasks, particularly in domains\nwhere expert annotations are scarce, expensive, or time-consuming to obtain.\n", "link": "http://arxiv.org/abs/2405.06093v2", "date": "2024-08-05", "relevancy": 1.9598, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5299}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5039}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Fine-tuning%20on%20LLM-labeled%20Data%20May%20Reduce%20Reliance%20on%20Human%0A%20%20Annotation%3A%20A%20Case%20Study%20Using%20Schedule-of-Event%20Table%20Detection&body=Title%3A%20Selective%20Fine-tuning%20on%20LLM-labeled%20Data%20May%20Reduce%20Reliance%20on%20Human%0A%20%20Annotation%3A%20A%20Case%20Study%20Using%20Schedule-of-Event%20Table%20Detection%0AAuthor%3A%20Bhawesh%20Kumar%20and%20Jonathan%20Amar%20and%20Eric%20Yang%20and%20Nan%20Li%20and%20Yugang%20Jia%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%20efficacy%20across%20a%20broad%0Aspectrum%20of%20tasks%20in%20healthcare%20applications.%20However%2C%20often%20LLMs%20need%20to%20be%0Afine-tuned%20on%20task-specific%20expert%20annotated%20data%20to%20achieve%20optimal%0Aperformance%2C%20which%20can%20be%20expensive%20and%20time%20consuming.%20In%20this%20study%2C%20we%0Afine-tune%20PaLM-2%20with%20parameter%20efficient%20fine-tuning%20%28PEFT%29%20using%20noisy%20labels%0Aobtained%20from%20gemini-pro%201.0%20for%20the%20detection%20of%20Schedule-of-Event%20%28SoE%29%0Atables%2C%20which%20specify%20care%20plan%20in%20clinical%20trial%20protocols.%20We%20introduce%20a%0Afiltering%20mechanism%20to%20select%20high-confidence%20labels%20for%20this%20table%0Aclassification%20task%2C%20thereby%20reducing%20the%20noise%20in%20the%20auto-generated%20labels.%0AWe%20show%20that%20fine-tuned%20PaLM-2%20with%20those%20labels%20achieves%20performance%20that%0Aexceeds%20the%20gemini-pro%201.0%20and%20other%20LLMs.%20Furthermore%2C%20its%20performance%20is%0Aclose%20to%20a%20PaLM-2%20fine-tuned%20on%20labels%20obtained%20from%20non-expert%20annotators.%20Our%0Aresults%20show%20that%20leveraging%20LLM-generated%20labels%20through%20powerful%20models%20like%0Agemini-pro%20can%20potentially%20serve%20as%20a%20viable%20strategy%20for%20improving%20LLM%0Aperformance%20through%20fine-tuning%20in%20specialized%20tasks%2C%20particularly%20in%20domains%0Awhere%20expert%20annotations%20are%20scarce%2C%20expensive%2C%20or%20time-consuming%20to%20obtain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06093v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Fine-tuning%2520on%2520LLM-labeled%2520Data%2520May%2520Reduce%2520Reliance%2520on%2520Human%250A%2520%2520Annotation%253A%2520A%2520Case%2520Study%2520Using%2520Schedule-of-Event%2520Table%2520Detection%26entry.906535625%3DBhawesh%2520Kumar%2520and%2520Jonathan%2520Amar%2520and%2520Eric%2520Yang%2520and%2520Nan%2520Li%2520and%2520Yugang%2520Jia%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520their%2520efficacy%2520across%2520a%2520broad%250Aspectrum%2520of%2520tasks%2520in%2520healthcare%2520applications.%2520However%252C%2520often%2520LLMs%2520need%2520to%2520be%250Afine-tuned%2520on%2520task-specific%2520expert%2520annotated%2520data%2520to%2520achieve%2520optimal%250Aperformance%252C%2520which%2520can%2520be%2520expensive%2520and%2520time%2520consuming.%2520In%2520this%2520study%252C%2520we%250Afine-tune%2520PaLM-2%2520with%2520parameter%2520efficient%2520fine-tuning%2520%2528PEFT%2529%2520using%2520noisy%2520labels%250Aobtained%2520from%2520gemini-pro%25201.0%2520for%2520the%2520detection%2520of%2520Schedule-of-Event%2520%2528SoE%2529%250Atables%252C%2520which%2520specify%2520care%2520plan%2520in%2520clinical%2520trial%2520protocols.%2520We%2520introduce%2520a%250Afiltering%2520mechanism%2520to%2520select%2520high-confidence%2520labels%2520for%2520this%2520table%250Aclassification%2520task%252C%2520thereby%2520reducing%2520the%2520noise%2520in%2520the%2520auto-generated%2520labels.%250AWe%2520show%2520that%2520fine-tuned%2520PaLM-2%2520with%2520those%2520labels%2520achieves%2520performance%2520that%250Aexceeds%2520the%2520gemini-pro%25201.0%2520and%2520other%2520LLMs.%2520Furthermore%252C%2520its%2520performance%2520is%250Aclose%2520to%2520a%2520PaLM-2%2520fine-tuned%2520on%2520labels%2520obtained%2520from%2520non-expert%2520annotators.%2520Our%250Aresults%2520show%2520that%2520leveraging%2520LLM-generated%2520labels%2520through%2520powerful%2520models%2520like%250Agemini-pro%2520can%2520potentially%2520serve%2520as%2520a%2520viable%2520strategy%2520for%2520improving%2520LLM%250Aperformance%2520through%2520fine-tuning%2520in%2520specialized%2520tasks%252C%2520particularly%2520in%2520domains%250Awhere%2520expert%2520annotations%2520are%2520scarce%252C%2520expensive%252C%2520or%2520time-consuming%2520to%2520obtain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06093v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Fine-tuning%20on%20LLM-labeled%20Data%20May%20Reduce%20Reliance%20on%20Human%0A%20%20Annotation%3A%20A%20Case%20Study%20Using%20Schedule-of-Event%20Table%20Detection&entry.906535625=Bhawesh%20Kumar%20and%20Jonathan%20Amar%20and%20Eric%20Yang%20and%20Nan%20Li%20and%20Yugang%20Jia&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%20efficacy%20across%20a%20broad%0Aspectrum%20of%20tasks%20in%20healthcare%20applications.%20However%2C%20often%20LLMs%20need%20to%20be%0Afine-tuned%20on%20task-specific%20expert%20annotated%20data%20to%20achieve%20optimal%0Aperformance%2C%20which%20can%20be%20expensive%20and%20time%20consuming.%20In%20this%20study%2C%20we%0Afine-tune%20PaLM-2%20with%20parameter%20efficient%20fine-tuning%20%28PEFT%29%20using%20noisy%20labels%0Aobtained%20from%20gemini-pro%201.0%20for%20the%20detection%20of%20Schedule-of-Event%20%28SoE%29%0Atables%2C%20which%20specify%20care%20plan%20in%20clinical%20trial%20protocols.%20We%20introduce%20a%0Afiltering%20mechanism%20to%20select%20high-confidence%20labels%20for%20this%20table%0Aclassification%20task%2C%20thereby%20reducing%20the%20noise%20in%20the%20auto-generated%20labels.%0AWe%20show%20that%20fine-tuned%20PaLM-2%20with%20those%20labels%20achieves%20performance%20that%0Aexceeds%20the%20gemini-pro%201.0%20and%20other%20LLMs.%20Furthermore%2C%20its%20performance%20is%0Aclose%20to%20a%20PaLM-2%20fine-tuned%20on%20labels%20obtained%20from%20non-expert%20annotators.%20Our%0Aresults%20show%20that%20leveraging%20LLM-generated%20labels%20through%20powerful%20models%20like%0Agemini-pro%20can%20potentially%20serve%20as%20a%20viable%20strategy%20for%20improving%20LLM%0Aperformance%20through%20fine-tuning%20in%20specialized%20tasks%2C%20particularly%20in%20domains%0Awhere%20expert%20annotations%20are%20scarce%2C%20expensive%2C%20or%20time-consuming%20to%20obtain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06093v2&entry.124074799=Read"},
{"title": "What Do Language Models Learn in Context? The Structured Task Hypothesis", "author": "Jiaoda Li and Yifan Hou and Mrinmaya Sachan and Ryan Cotterell", "abstract": "  Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.\n", "link": "http://arxiv.org/abs/2406.04216v3", "date": "2024-08-05", "relevancy": 1.9538, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5199}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.47}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Do%20Language%20Models%20Learn%20in%20Context%3F%20The%20Structured%20Task%20Hypothesis&body=Title%3A%20What%20Do%20Language%20Models%20Learn%20in%20Context%3F%20The%20Structured%20Task%20Hypothesis%0AAuthor%3A%20Jiaoda%20Li%20and%20Yifan%20Hou%20and%20Mrinmaya%20Sachan%20and%20Ryan%20Cotterell%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20an%20intriguing%20ability%20to%20learn%20a%20novel%0Atask%20from%20in-context%20examples%20presented%20in%20a%20demonstration%2C%20termed%20in-context%0Alearning%20%28ICL%29.%20Understandably%2C%20a%20swath%20of%20research%20has%20been%20dedicated%20to%0Auncovering%20the%20theories%20underpinning%20ICL.%20One%20popular%20hypothesis%20explains%20ICL%0Aby%20task%20selection.%20LLMs%20identify%20the%20task%20based%20on%20the%20demonstration%20and%0Ageneralize%20it%20to%20the%20prompt.%20Another%20popular%20hypothesis%20is%20that%20ICL%20is%20a%20form%0Aof%20meta-learning%2C%20i.e.%2C%20the%20models%20learn%20a%20learning%20algorithm%20at%20pre-training%0Atime%20and%20apply%20it%20to%20the%20demonstration.%20Finally%2C%20a%20third%20hypothesis%20argues%20that%0ALLMs%20use%20the%20demonstration%20to%20select%20a%20composition%20of%20tasks%20learned%20during%0Apre-training%20to%20perform%20ICL.%20In%20this%20paper%2C%20we%20empirically%20explore%20these%20three%0Ahypotheses%20that%20explain%20LLMs%27%20ability%20to%20learn%20in%20context%20with%20a%20suite%20of%0Aexperiments%20derived%20from%20common%20text%20classification%20tasks.%20We%20invalidate%20the%0Afirst%20two%20hypotheses%20with%20counterexamples%20and%20provide%20evidence%20in%20support%20of%0Athe%20last%20hypothesis.%20Our%20results%20suggest%20an%20LLM%20could%20learn%20a%20novel%20task%20in%0Acontext%20via%20composing%20tasks%20learned%20during%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04216v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Do%2520Language%2520Models%2520Learn%2520in%2520Context%253F%2520The%2520Structured%2520Task%2520Hypothesis%26entry.906535625%3DJiaoda%2520Li%2520and%2520Yifan%2520Hou%2520and%2520Mrinmaya%2520Sachan%2520and%2520Ryan%2520Cotterell%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520an%2520intriguing%2520ability%2520to%2520learn%2520a%2520novel%250Atask%2520from%2520in-context%2520examples%2520presented%2520in%2520a%2520demonstration%252C%2520termed%2520in-context%250Alearning%2520%2528ICL%2529.%2520Understandably%252C%2520a%2520swath%2520of%2520research%2520has%2520been%2520dedicated%2520to%250Auncovering%2520the%2520theories%2520underpinning%2520ICL.%2520One%2520popular%2520hypothesis%2520explains%2520ICL%250Aby%2520task%2520selection.%2520LLMs%2520identify%2520the%2520task%2520based%2520on%2520the%2520demonstration%2520and%250Ageneralize%2520it%2520to%2520the%2520prompt.%2520Another%2520popular%2520hypothesis%2520is%2520that%2520ICL%2520is%2520a%2520form%250Aof%2520meta-learning%252C%2520i.e.%252C%2520the%2520models%2520learn%2520a%2520learning%2520algorithm%2520at%2520pre-training%250Atime%2520and%2520apply%2520it%2520to%2520the%2520demonstration.%2520Finally%252C%2520a%2520third%2520hypothesis%2520argues%2520that%250ALLMs%2520use%2520the%2520demonstration%2520to%2520select%2520a%2520composition%2520of%2520tasks%2520learned%2520during%250Apre-training%2520to%2520perform%2520ICL.%2520In%2520this%2520paper%252C%2520we%2520empirically%2520explore%2520these%2520three%250Ahypotheses%2520that%2520explain%2520LLMs%2527%2520ability%2520to%2520learn%2520in%2520context%2520with%2520a%2520suite%2520of%250Aexperiments%2520derived%2520from%2520common%2520text%2520classification%2520tasks.%2520We%2520invalidate%2520the%250Afirst%2520two%2520hypotheses%2520with%2520counterexamples%2520and%2520provide%2520evidence%2520in%2520support%2520of%250Athe%2520last%2520hypothesis.%2520Our%2520results%2520suggest%2520an%2520LLM%2520could%2520learn%2520a%2520novel%2520task%2520in%250Acontext%2520via%2520composing%2520tasks%2520learned%2520during%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04216v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Do%20Language%20Models%20Learn%20in%20Context%3F%20The%20Structured%20Task%20Hypothesis&entry.906535625=Jiaoda%20Li%20and%20Yifan%20Hou%20and%20Mrinmaya%20Sachan%20and%20Ryan%20Cotterell&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20an%20intriguing%20ability%20to%20learn%20a%20novel%0Atask%20from%20in-context%20examples%20presented%20in%20a%20demonstration%2C%20termed%20in-context%0Alearning%20%28ICL%29.%20Understandably%2C%20a%20swath%20of%20research%20has%20been%20dedicated%20to%0Auncovering%20the%20theories%20underpinning%20ICL.%20One%20popular%20hypothesis%20explains%20ICL%0Aby%20task%20selection.%20LLMs%20identify%20the%20task%20based%20on%20the%20demonstration%20and%0Ageneralize%20it%20to%20the%20prompt.%20Another%20popular%20hypothesis%20is%20that%20ICL%20is%20a%20form%0Aof%20meta-learning%2C%20i.e.%2C%20the%20models%20learn%20a%20learning%20algorithm%20at%20pre-training%0Atime%20and%20apply%20it%20to%20the%20demonstration.%20Finally%2C%20a%20third%20hypothesis%20argues%20that%0ALLMs%20use%20the%20demonstration%20to%20select%20a%20composition%20of%20tasks%20learned%20during%0Apre-training%20to%20perform%20ICL.%20In%20this%20paper%2C%20we%20empirically%20explore%20these%20three%0Ahypotheses%20that%20explain%20LLMs%27%20ability%20to%20learn%20in%20context%20with%20a%20suite%20of%0Aexperiments%20derived%20from%20common%20text%20classification%20tasks.%20We%20invalidate%20the%0Afirst%20two%20hypotheses%20with%20counterexamples%20and%20provide%20evidence%20in%20support%20of%0Athe%20last%20hypothesis.%20Our%20results%20suggest%20an%20LLM%20could%20learn%20a%20novel%20task%20in%0Acontext%20via%20composing%20tasks%20learned%20during%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04216v3&entry.124074799=Read"},
{"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "author": "Daniel Fleischer and Moshe Berchansky and Moshe Wasserblat and Peter Izsak", "abstract": "  Implementing Retrieval-Augmented Generation (RAG) systems is inherently\ncomplex, requiring deep understanding of data, use cases, and intricate design\ndecisions. Additionally, evaluating these systems presents significant\nchallenges, necessitating assessment of both retrieval accuracy and generative\nquality through a multi-faceted approach. We introduce RAG Foundry, an\nopen-source framework for augmenting large language models for RAG use cases.\nRAG Foundry integrates data creation, training, inference and evaluation into a\nsingle workflow, facilitating the creation of data-augmented datasets for\ntraining and evaluating large language models in RAG settings. This integration\nenables rapid prototyping and experimentation with various RAG techniques,\nallowing users to easily generate datasets and train RAG models using internal\nor specialized knowledge sources. We demonstrate the framework effectiveness by\naugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG\nconfigurations, showcasing consistent improvements across three\nknowledge-intensive datasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n", "link": "http://arxiv.org/abs/2408.02545v1", "date": "2024-08-05", "relevancy": 1.9372, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4962}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4779}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAG%20Foundry%3A%20A%20Framework%20for%20Enhancing%20LLMs%20for%20Retrieval%20Augmented%0A%20%20Generation&body=Title%3A%20RAG%20Foundry%3A%20A%20Framework%20for%20Enhancing%20LLMs%20for%20Retrieval%20Augmented%0A%20%20Generation%0AAuthor%3A%20Daniel%20Fleischer%20and%20Moshe%20Berchansky%20and%20Moshe%20Wasserblat%20and%20Peter%20Izsak%0AAbstract%3A%20%20%20Implementing%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20is%20inherently%0Acomplex%2C%20requiring%20deep%20understanding%20of%20data%2C%20use%20cases%2C%20and%20intricate%20design%0Adecisions.%20Additionally%2C%20evaluating%20these%20systems%20presents%20significant%0Achallenges%2C%20necessitating%20assessment%20of%20both%20retrieval%20accuracy%20and%20generative%0Aquality%20through%20a%20multi-faceted%20approach.%20We%20introduce%20RAG%20Foundry%2C%20an%0Aopen-source%20framework%20for%20augmenting%20large%20language%20models%20for%20RAG%20use%20cases.%0ARAG%20Foundry%20integrates%20data%20creation%2C%20training%2C%20inference%20and%20evaluation%20into%20a%0Asingle%20workflow%2C%20facilitating%20the%20creation%20of%20data-augmented%20datasets%20for%0Atraining%20and%20evaluating%20large%20language%20models%20in%20RAG%20settings.%20This%20integration%0Aenables%20rapid%20prototyping%20and%20experimentation%20with%20various%20RAG%20techniques%2C%0Aallowing%20users%20to%20easily%20generate%20datasets%20and%20train%20RAG%20models%20using%20internal%0Aor%20specialized%20knowledge%20sources.%20We%20demonstrate%20the%20framework%20effectiveness%20by%0Aaugmenting%20and%20fine-tuning%20Llama-3%20and%20Phi-3%20models%20with%20diverse%20RAG%0Aconfigurations%2C%20showcasing%20consistent%20improvements%20across%20three%0Aknowledge-intensive%20datasets.%20Code%20is%20released%20as%20open-source%20in%0Ahttps%3A//github.com/IntelLabs/RAGFoundry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAG%2520Foundry%253A%2520A%2520Framework%2520for%2520Enhancing%2520LLMs%2520for%2520Retrieval%2520Augmented%250A%2520%2520Generation%26entry.906535625%3DDaniel%2520Fleischer%2520and%2520Moshe%2520Berchansky%2520and%2520Moshe%2520Wasserblat%2520and%2520Peter%2520Izsak%26entry.1292438233%3D%2520%2520Implementing%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520is%2520inherently%250Acomplex%252C%2520requiring%2520deep%2520understanding%2520of%2520data%252C%2520use%2520cases%252C%2520and%2520intricate%2520design%250Adecisions.%2520Additionally%252C%2520evaluating%2520these%2520systems%2520presents%2520significant%250Achallenges%252C%2520necessitating%2520assessment%2520of%2520both%2520retrieval%2520accuracy%2520and%2520generative%250Aquality%2520through%2520a%2520multi-faceted%2520approach.%2520We%2520introduce%2520RAG%2520Foundry%252C%2520an%250Aopen-source%2520framework%2520for%2520augmenting%2520large%2520language%2520models%2520for%2520RAG%2520use%2520cases.%250ARAG%2520Foundry%2520integrates%2520data%2520creation%252C%2520training%252C%2520inference%2520and%2520evaluation%2520into%2520a%250Asingle%2520workflow%252C%2520facilitating%2520the%2520creation%2520of%2520data-augmented%2520datasets%2520for%250Atraining%2520and%2520evaluating%2520large%2520language%2520models%2520in%2520RAG%2520settings.%2520This%2520integration%250Aenables%2520rapid%2520prototyping%2520and%2520experimentation%2520with%2520various%2520RAG%2520techniques%252C%250Aallowing%2520users%2520to%2520easily%2520generate%2520datasets%2520and%2520train%2520RAG%2520models%2520using%2520internal%250Aor%2520specialized%2520knowledge%2520sources.%2520We%2520demonstrate%2520the%2520framework%2520effectiveness%2520by%250Aaugmenting%2520and%2520fine-tuning%2520Llama-3%2520and%2520Phi-3%2520models%2520with%2520diverse%2520RAG%250Aconfigurations%252C%2520showcasing%2520consistent%2520improvements%2520across%2520three%250Aknowledge-intensive%2520datasets.%2520Code%2520is%2520released%2520as%2520open-source%2520in%250Ahttps%253A//github.com/IntelLabs/RAGFoundry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAG%20Foundry%3A%20A%20Framework%20for%20Enhancing%20LLMs%20for%20Retrieval%20Augmented%0A%20%20Generation&entry.906535625=Daniel%20Fleischer%20and%20Moshe%20Berchansky%20and%20Moshe%20Wasserblat%20and%20Peter%20Izsak&entry.1292438233=%20%20Implementing%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20is%20inherently%0Acomplex%2C%20requiring%20deep%20understanding%20of%20data%2C%20use%20cases%2C%20and%20intricate%20design%0Adecisions.%20Additionally%2C%20evaluating%20these%20systems%20presents%20significant%0Achallenges%2C%20necessitating%20assessment%20of%20both%20retrieval%20accuracy%20and%20generative%0Aquality%20through%20a%20multi-faceted%20approach.%20We%20introduce%20RAG%20Foundry%2C%20an%0Aopen-source%20framework%20for%20augmenting%20large%20language%20models%20for%20RAG%20use%20cases.%0ARAG%20Foundry%20integrates%20data%20creation%2C%20training%2C%20inference%20and%20evaluation%20into%20a%0Asingle%20workflow%2C%20facilitating%20the%20creation%20of%20data-augmented%20datasets%20for%0Atraining%20and%20evaluating%20large%20language%20models%20in%20RAG%20settings.%20This%20integration%0Aenables%20rapid%20prototyping%20and%20experimentation%20with%20various%20RAG%20techniques%2C%0Aallowing%20users%20to%20easily%20generate%20datasets%20and%20train%20RAG%20models%20using%20internal%0Aor%20specialized%20knowledge%20sources.%20We%20demonstrate%20the%20framework%20effectiveness%20by%0Aaugmenting%20and%20fine-tuning%20Llama-3%20and%20Phi-3%20models%20with%20diverse%20RAG%0Aconfigurations%2C%20showcasing%20consistent%20improvements%20across%20three%0Aknowledge-intensive%20datasets.%20Code%20is%20released%20as%20open-source%20in%0Ahttps%3A//github.com/IntelLabs/RAGFoundry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02545v1&entry.124074799=Read"},
{"title": "Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality\n  Aspect-Based Summarization", "author": "Ankan Mullick and Sombit Bose and Rounak Saha and Ayan Kumar Bhowmick and Aditya Vempaty and Pawan Goyal and Niloy Ganguly and Prasenjit Dey and Ravi Kokku", "abstract": "  The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.\n", "link": "http://arxiv.org/abs/2408.02584v1", "date": "2024-08-05", "relevancy": 1.9193, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4793}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20the%20Power%20of%20LLMs%3A%20A%20Fine-Tuning%20Approach%20for%20High-Quality%0A%20%20Aspect-Based%20Summarization&body=Title%3A%20Leveraging%20the%20Power%20of%20LLMs%3A%20A%20Fine-Tuning%20Approach%20for%20High-Quality%0A%20%20Aspect-Based%20Summarization%0AAuthor%3A%20Ankan%20Mullick%20and%20Sombit%20Bose%20and%20Rounak%20Saha%20and%20Ayan%20Kumar%20Bhowmick%20and%20Aditya%20Vempaty%20and%20Pawan%20Goyal%20and%20Niloy%20Ganguly%20and%20Prasenjit%20Dey%20and%20Ravi%20Kokku%0AAbstract%3A%20%20%20The%20ever-increasing%20volume%20of%20digital%20information%20necessitates%20efficient%0Amethods%20for%20users%20to%20extract%20key%20insights%20from%20lengthy%20documents.%20Aspect-based%0Asummarization%20offers%20a%20targeted%20approach%2C%20generating%20summaries%20focused%20on%0Aspecific%20aspects%20within%20a%20document.%20Despite%20advancements%20in%20aspect-based%0Asummarization%20research%2C%20there%20is%20a%20continuous%20quest%20for%20improved%20model%0Aperformance.%20Given%20that%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20the%0Apotential%20to%20revolutionize%20diverse%20tasks%20within%20natural%20language%20processing%2C%0Aparticularly%20in%20the%20problem%20of%20summarization%2C%20this%20paper%20explores%20the%20potential%0Aof%20fine-tuning%20LLMs%20for%20the%20aspect-based%20summarization%20task.%20We%20evaluate%20the%0Aimpact%20of%20fine-tuning%20open-source%20foundation%20LLMs%2C%20including%20Llama2%2C%20Mistral%2C%0AGemma%20and%20Aya%2C%20on%20a%20publicly%20available%20domain-specific%20aspect%20based%20summary%0Adataset.%20We%20hypothesize%20that%20this%20approach%20will%20enable%20these%20models%20to%0Aeffectively%20identify%20and%20extract%20aspect-related%20information%2C%20leading%20to%0Asuperior%20quality%20aspect-based%20summaries%20compared%20to%20the%20state-of-the-art.%20We%0Aestablish%20a%20comprehensive%20evaluation%20framework%20to%20compare%20the%20performance%20of%0Afine-tuned%20LLMs%20against%20competing%20aspect-based%20summarization%20methods%20and%0Avanilla%20counterparts%20of%20the%20fine-tuned%20LLMs.%20Our%20work%20contributes%20to%20the%20field%0Aof%20aspect-based%20summarization%20by%20demonstrating%20the%20efficacy%20of%20fine-tuning%20LLMs%0Afor%20generating%20high-quality%20aspect-based%20summaries.%20Furthermore%2C%20it%20opens%20doors%0Afor%20further%20exploration%20of%20using%20LLMs%20for%20targeted%20information%20extraction%20tasks%0Aacross%20various%20NLP%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520the%2520Power%2520of%2520LLMs%253A%2520A%2520Fine-Tuning%2520Approach%2520for%2520High-Quality%250A%2520%2520Aspect-Based%2520Summarization%26entry.906535625%3DAnkan%2520Mullick%2520and%2520Sombit%2520Bose%2520and%2520Rounak%2520Saha%2520and%2520Ayan%2520Kumar%2520Bhowmick%2520and%2520Aditya%2520Vempaty%2520and%2520Pawan%2520Goyal%2520and%2520Niloy%2520Ganguly%2520and%2520Prasenjit%2520Dey%2520and%2520Ravi%2520Kokku%26entry.1292438233%3D%2520%2520The%2520ever-increasing%2520volume%2520of%2520digital%2520information%2520necessitates%2520efficient%250Amethods%2520for%2520users%2520to%2520extract%2520key%2520insights%2520from%2520lengthy%2520documents.%2520Aspect-based%250Asummarization%2520offers%2520a%2520targeted%2520approach%252C%2520generating%2520summaries%2520focused%2520on%250Aspecific%2520aspects%2520within%2520a%2520document.%2520Despite%2520advancements%2520in%2520aspect-based%250Asummarization%2520research%252C%2520there%2520is%2520a%2520continuous%2520quest%2520for%2520improved%2520model%250Aperformance.%2520Given%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520the%250Apotential%2520to%2520revolutionize%2520diverse%2520tasks%2520within%2520natural%2520language%2520processing%252C%250Aparticularly%2520in%2520the%2520problem%2520of%2520summarization%252C%2520this%2520paper%2520explores%2520the%2520potential%250Aof%2520fine-tuning%2520LLMs%2520for%2520the%2520aspect-based%2520summarization%2520task.%2520We%2520evaluate%2520the%250Aimpact%2520of%2520fine-tuning%2520open-source%2520foundation%2520LLMs%252C%2520including%2520Llama2%252C%2520Mistral%252C%250AGemma%2520and%2520Aya%252C%2520on%2520a%2520publicly%2520available%2520domain-specific%2520aspect%2520based%2520summary%250Adataset.%2520We%2520hypothesize%2520that%2520this%2520approach%2520will%2520enable%2520these%2520models%2520to%250Aeffectively%2520identify%2520and%2520extract%2520aspect-related%2520information%252C%2520leading%2520to%250Asuperior%2520quality%2520aspect-based%2520summaries%2520compared%2520to%2520the%2520state-of-the-art.%2520We%250Aestablish%2520a%2520comprehensive%2520evaluation%2520framework%2520to%2520compare%2520the%2520performance%2520of%250Afine-tuned%2520LLMs%2520against%2520competing%2520aspect-based%2520summarization%2520methods%2520and%250Avanilla%2520counterparts%2520of%2520the%2520fine-tuned%2520LLMs.%2520Our%2520work%2520contributes%2520to%2520the%2520field%250Aof%2520aspect-based%2520summarization%2520by%2520demonstrating%2520the%2520efficacy%2520of%2520fine-tuning%2520LLMs%250Afor%2520generating%2520high-quality%2520aspect-based%2520summaries.%2520Furthermore%252C%2520it%2520opens%2520doors%250Afor%2520further%2520exploration%2520of%2520using%2520LLMs%2520for%2520targeted%2520information%2520extraction%2520tasks%250Aacross%2520various%2520NLP%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20the%20Power%20of%20LLMs%3A%20A%20Fine-Tuning%20Approach%20for%20High-Quality%0A%20%20Aspect-Based%20Summarization&entry.906535625=Ankan%20Mullick%20and%20Sombit%20Bose%20and%20Rounak%20Saha%20and%20Ayan%20Kumar%20Bhowmick%20and%20Aditya%20Vempaty%20and%20Pawan%20Goyal%20and%20Niloy%20Ganguly%20and%20Prasenjit%20Dey%20and%20Ravi%20Kokku&entry.1292438233=%20%20The%20ever-increasing%20volume%20of%20digital%20information%20necessitates%20efficient%0Amethods%20for%20users%20to%20extract%20key%20insights%20from%20lengthy%20documents.%20Aspect-based%0Asummarization%20offers%20a%20targeted%20approach%2C%20generating%20summaries%20focused%20on%0Aspecific%20aspects%20within%20a%20document.%20Despite%20advancements%20in%20aspect-based%0Asummarization%20research%2C%20there%20is%20a%20continuous%20quest%20for%20improved%20model%0Aperformance.%20Given%20that%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20the%0Apotential%20to%20revolutionize%20diverse%20tasks%20within%20natural%20language%20processing%2C%0Aparticularly%20in%20the%20problem%20of%20summarization%2C%20this%20paper%20explores%20the%20potential%0Aof%20fine-tuning%20LLMs%20for%20the%20aspect-based%20summarization%20task.%20We%20evaluate%20the%0Aimpact%20of%20fine-tuning%20open-source%20foundation%20LLMs%2C%20including%20Llama2%2C%20Mistral%2C%0AGemma%20and%20Aya%2C%20on%20a%20publicly%20available%20domain-specific%20aspect%20based%20summary%0Adataset.%20We%20hypothesize%20that%20this%20approach%20will%20enable%20these%20models%20to%0Aeffectively%20identify%20and%20extract%20aspect-related%20information%2C%20leading%20to%0Asuperior%20quality%20aspect-based%20summaries%20compared%20to%20the%20state-of-the-art.%20We%0Aestablish%20a%20comprehensive%20evaluation%20framework%20to%20compare%20the%20performance%20of%0Afine-tuned%20LLMs%20against%20competing%20aspect-based%20summarization%20methods%20and%0Avanilla%20counterparts%20of%20the%20fine-tuned%20LLMs.%20Our%20work%20contributes%20to%20the%20field%0Aof%20aspect-based%20summarization%20by%20demonstrating%20the%20efficacy%20of%20fine-tuning%20LLMs%0Afor%20generating%20high-quality%20aspect-based%20summaries.%20Furthermore%2C%20it%20opens%20doors%0Afor%20further%20exploration%20of%20using%20LLMs%20for%20targeted%20information%20extraction%20tasks%0Aacross%20various%20NLP%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02584v1&entry.124074799=Read"},
{"title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models", "author": "Raz Lapid and Ron Langberg and Moshe Sipper", "abstract": "  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n", "link": "http://arxiv.org/abs/2309.01446v4", "date": "2024-08-05", "relevancy": 1.9169, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4909}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4757}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Sesame%21%20Universal%20Black%20Box%20Jailbreaking%20of%20Large%20Language%20Models&body=Title%3A%20Open%20Sesame%21%20Universal%20Black%20Box%20Jailbreaking%20of%20Large%20Language%20Models%0AAuthor%3A%20Raz%20Lapid%20and%20Ron%20Langberg%20and%20Moshe%20Sipper%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20designed%20to%20provide%20helpful%20and%20safe%20responses%2C%0Aoften%20rely%20on%20alignment%20techniques%20to%20align%20with%20user%20intent%20and%20social%0Aguidelines.%20Unfortunately%2C%20this%20alignment%20can%20be%20exploited%20by%20malicious%20actors%0Aseeking%20to%20manipulate%20an%20LLM%27s%20outputs%20for%20unintended%20purposes.%20In%20this%20paper%0Awe%20introduce%20a%20novel%20approach%20that%20employs%20a%20genetic%20algorithm%20%28GA%29%20to%0Amanipulate%20LLMs%20when%20model%20architecture%20and%20parameters%20are%20inaccessible.%20The%20GA%0Aattack%20works%20by%20optimizing%20a%20universal%20adversarial%20prompt%20that%20--%20when%20combined%0Awith%20a%20user%27s%20query%20--%20disrupts%20the%20attacked%20model%27s%20alignment%2C%20resulting%20in%0Aunintended%20and%20potentially%20harmful%20outputs.%20Our%20novel%20approach%20systematically%0Areveals%20a%20model%27s%20limitations%20and%20vulnerabilities%20by%20uncovering%20instances%20where%0Aits%20responses%20deviate%20from%20expected%20behavior.%20Through%20extensive%20experiments%20we%0Ademonstrate%20the%20efficacy%20of%20our%20technique%2C%20thus%20contributing%20to%20the%20ongoing%0Adiscussion%20on%20responsible%20AI%20development%20by%20providing%20a%20diagnostic%20tool%20for%0Aevaluating%20and%20enhancing%20alignment%20of%20LLMs%20with%20human%20intent.%20To%20our%20knowledge%0Athis%20is%20the%20first%20automated%20universal%20black%20box%20jailbreak%20attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.01446v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Sesame%2521%2520Universal%2520Black%2520Box%2520Jailbreaking%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DRaz%2520Lapid%2520and%2520Ron%2520Langberg%2520and%2520Moshe%2520Sipper%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520designed%2520to%2520provide%2520helpful%2520and%2520safe%2520responses%252C%250Aoften%2520rely%2520on%2520alignment%2520techniques%2520to%2520align%2520with%2520user%2520intent%2520and%2520social%250Aguidelines.%2520Unfortunately%252C%2520this%2520alignment%2520can%2520be%2520exploited%2520by%2520malicious%2520actors%250Aseeking%2520to%2520manipulate%2520an%2520LLM%2527s%2520outputs%2520for%2520unintended%2520purposes.%2520In%2520this%2520paper%250Awe%2520introduce%2520a%2520novel%2520approach%2520that%2520employs%2520a%2520genetic%2520algorithm%2520%2528GA%2529%2520to%250Amanipulate%2520LLMs%2520when%2520model%2520architecture%2520and%2520parameters%2520are%2520inaccessible.%2520The%2520GA%250Aattack%2520works%2520by%2520optimizing%2520a%2520universal%2520adversarial%2520prompt%2520that%2520--%2520when%2520combined%250Awith%2520a%2520user%2527s%2520query%2520--%2520disrupts%2520the%2520attacked%2520model%2527s%2520alignment%252C%2520resulting%2520in%250Aunintended%2520and%2520potentially%2520harmful%2520outputs.%2520Our%2520novel%2520approach%2520systematically%250Areveals%2520a%2520model%2527s%2520limitations%2520and%2520vulnerabilities%2520by%2520uncovering%2520instances%2520where%250Aits%2520responses%2520deviate%2520from%2520expected%2520behavior.%2520Through%2520extensive%2520experiments%2520we%250Ademonstrate%2520the%2520efficacy%2520of%2520our%2520technique%252C%2520thus%2520contributing%2520to%2520the%2520ongoing%250Adiscussion%2520on%2520responsible%2520AI%2520development%2520by%2520providing%2520a%2520diagnostic%2520tool%2520for%250Aevaluating%2520and%2520enhancing%2520alignment%2520of%2520LLMs%2520with%2520human%2520intent.%2520To%2520our%2520knowledge%250Athis%2520is%2520the%2520first%2520automated%2520universal%2520black%2520box%2520jailbreak%2520attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.01446v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Sesame%21%20Universal%20Black%20Box%20Jailbreaking%20of%20Large%20Language%20Models&entry.906535625=Raz%20Lapid%20and%20Ron%20Langberg%20and%20Moshe%20Sipper&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20designed%20to%20provide%20helpful%20and%20safe%20responses%2C%0Aoften%20rely%20on%20alignment%20techniques%20to%20align%20with%20user%20intent%20and%20social%0Aguidelines.%20Unfortunately%2C%20this%20alignment%20can%20be%20exploited%20by%20malicious%20actors%0Aseeking%20to%20manipulate%20an%20LLM%27s%20outputs%20for%20unintended%20purposes.%20In%20this%20paper%0Awe%20introduce%20a%20novel%20approach%20that%20employs%20a%20genetic%20algorithm%20%28GA%29%20to%0Amanipulate%20LLMs%20when%20model%20architecture%20and%20parameters%20are%20inaccessible.%20The%20GA%0Aattack%20works%20by%20optimizing%20a%20universal%20adversarial%20prompt%20that%20--%20when%20combined%0Awith%20a%20user%27s%20query%20--%20disrupts%20the%20attacked%20model%27s%20alignment%2C%20resulting%20in%0Aunintended%20and%20potentially%20harmful%20outputs.%20Our%20novel%20approach%20systematically%0Areveals%20a%20model%27s%20limitations%20and%20vulnerabilities%20by%20uncovering%20instances%20where%0Aits%20responses%20deviate%20from%20expected%20behavior.%20Through%20extensive%20experiments%20we%0Ademonstrate%20the%20efficacy%20of%20our%20technique%2C%20thus%20contributing%20to%20the%20ongoing%0Adiscussion%20on%20responsible%20AI%20development%20by%20providing%20a%20diagnostic%20tool%20for%0Aevaluating%20and%20enhancing%20alignment%20of%20LLMs%20with%20human%20intent.%20To%20our%20knowledge%0Athis%20is%20the%20first%20automated%20universal%20black%20box%20jailbreak%20attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.01446v4&entry.124074799=Read"},
{"title": "LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local\n  Attention and Mamba", "author": "Yunxiang Fu and Chaoqi Chen and Yizhou Yu", "abstract": "  Recent Transformer-based diffusion models have shown remarkable performance,\nlargely attributed to the ability of the self-attention mechanism to accurately\ncapture both global and local contexts by computing all-pair interactions among\ninput tokens. However, their quadratic complexity poses significant\ncomputational challenges for long-sequence inputs. Conversely, a recent state\nspace model called Mamba offers linear complexity by compressing a filtered\nglobal context into a hidden state. Despite its efficiency, compression\ninevitably leads to information loss of fine-grained local dependencies among\ntokens, which are crucial for effective visual generative modeling. Motivated\nby these observations, we introduce Local Attentional Mamba (LaMamba) blocks\nthat combine the strengths of self-attention and Mamba, capturing both global\ncontexts and local details with linear complexity. Leveraging the efficient\nU-Net architecture, our model exhibits exceptional scalability and surpasses\nthe performance of DiT across various model scales on ImageNet at 256x256\nresolution, all while utilizing substantially fewer GFLOPs and a comparable\nnumber of parameters. Compared to state-of-the-art diffusion models on ImageNet\n256x256 and 512x512, our largest model presents notable advantages, such as a\nreduction of up to 62\\% GFLOPs compared to DiT-XL/2, while achieving superior\nperformance with comparable or fewer parameters.\n", "link": "http://arxiv.org/abs/2408.02615v1", "date": "2024-08-05", "relevancy": 1.9055, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6748}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6258}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaMamba-Diff%3A%20Linear-Time%20High-Fidelity%20Diffusion%20Models%20Based%20on%20Local%0A%20%20Attention%20and%20Mamba&body=Title%3A%20LaMamba-Diff%3A%20Linear-Time%20High-Fidelity%20Diffusion%20Models%20Based%20on%20Local%0A%20%20Attention%20and%20Mamba%0AAuthor%3A%20Yunxiang%20Fu%20and%20Chaoqi%20Chen%20and%20Yizhou%20Yu%0AAbstract%3A%20%20%20Recent%20Transformer-based%20diffusion%20models%20have%20shown%20remarkable%20performance%2C%0Alargely%20attributed%20to%20the%20ability%20of%20the%20self-attention%20mechanism%20to%20accurately%0Acapture%20both%20global%20and%20local%20contexts%20by%20computing%20all-pair%20interactions%20among%0Ainput%20tokens.%20However%2C%20their%20quadratic%20complexity%20poses%20significant%0Acomputational%20challenges%20for%20long-sequence%20inputs.%20Conversely%2C%20a%20recent%20state%0Aspace%20model%20called%20Mamba%20offers%20linear%20complexity%20by%20compressing%20a%20filtered%0Aglobal%20context%20into%20a%20hidden%20state.%20Despite%20its%20efficiency%2C%20compression%0Ainevitably%20leads%20to%20information%20loss%20of%20fine-grained%20local%20dependencies%20among%0Atokens%2C%20which%20are%20crucial%20for%20effective%20visual%20generative%20modeling.%20Motivated%0Aby%20these%20observations%2C%20we%20introduce%20Local%20Attentional%20Mamba%20%28LaMamba%29%20blocks%0Athat%20combine%20the%20strengths%20of%20self-attention%20and%20Mamba%2C%20capturing%20both%20global%0Acontexts%20and%20local%20details%20with%20linear%20complexity.%20Leveraging%20the%20efficient%0AU-Net%20architecture%2C%20our%20model%20exhibits%20exceptional%20scalability%20and%20surpasses%0Athe%20performance%20of%20DiT%20across%20various%20model%20scales%20on%20ImageNet%20at%20256x256%0Aresolution%2C%20all%20while%20utilizing%20substantially%20fewer%20GFLOPs%20and%20a%20comparable%0Anumber%20of%20parameters.%20Compared%20to%20state-of-the-art%20diffusion%20models%20on%20ImageNet%0A256x256%20and%20512x512%2C%20our%20largest%20model%20presents%20notable%20advantages%2C%20such%20as%20a%0Areduction%20of%20up%20to%2062%5C%25%20GFLOPs%20compared%20to%20DiT-XL/2%2C%20while%20achieving%20superior%0Aperformance%20with%20comparable%20or%20fewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaMamba-Diff%253A%2520Linear-Time%2520High-Fidelity%2520Diffusion%2520Models%2520Based%2520on%2520Local%250A%2520%2520Attention%2520and%2520Mamba%26entry.906535625%3DYunxiang%2520Fu%2520and%2520Chaoqi%2520Chen%2520and%2520Yizhou%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520Transformer-based%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520performance%252C%250Alargely%2520attributed%2520to%2520the%2520ability%2520of%2520the%2520self-attention%2520mechanism%2520to%2520accurately%250Acapture%2520both%2520global%2520and%2520local%2520contexts%2520by%2520computing%2520all-pair%2520interactions%2520among%250Ainput%2520tokens.%2520However%252C%2520their%2520quadratic%2520complexity%2520poses%2520significant%250Acomputational%2520challenges%2520for%2520long-sequence%2520inputs.%2520Conversely%252C%2520a%2520recent%2520state%250Aspace%2520model%2520called%2520Mamba%2520offers%2520linear%2520complexity%2520by%2520compressing%2520a%2520filtered%250Aglobal%2520context%2520into%2520a%2520hidden%2520state.%2520Despite%2520its%2520efficiency%252C%2520compression%250Ainevitably%2520leads%2520to%2520information%2520loss%2520of%2520fine-grained%2520local%2520dependencies%2520among%250Atokens%252C%2520which%2520are%2520crucial%2520for%2520effective%2520visual%2520generative%2520modeling.%2520Motivated%250Aby%2520these%2520observations%252C%2520we%2520introduce%2520Local%2520Attentional%2520Mamba%2520%2528LaMamba%2529%2520blocks%250Athat%2520combine%2520the%2520strengths%2520of%2520self-attention%2520and%2520Mamba%252C%2520capturing%2520both%2520global%250Acontexts%2520and%2520local%2520details%2520with%2520linear%2520complexity.%2520Leveraging%2520the%2520efficient%250AU-Net%2520architecture%252C%2520our%2520model%2520exhibits%2520exceptional%2520scalability%2520and%2520surpasses%250Athe%2520performance%2520of%2520DiT%2520across%2520various%2520model%2520scales%2520on%2520ImageNet%2520at%2520256x256%250Aresolution%252C%2520all%2520while%2520utilizing%2520substantially%2520fewer%2520GFLOPs%2520and%2520a%2520comparable%250Anumber%2520of%2520parameters.%2520Compared%2520to%2520state-of-the-art%2520diffusion%2520models%2520on%2520ImageNet%250A256x256%2520and%2520512x512%252C%2520our%2520largest%2520model%2520presents%2520notable%2520advantages%252C%2520such%2520as%2520a%250Areduction%2520of%2520up%2520to%252062%255C%2525%2520GFLOPs%2520compared%2520to%2520DiT-XL/2%252C%2520while%2520achieving%2520superior%250Aperformance%2520with%2520comparable%2520or%2520fewer%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaMamba-Diff%3A%20Linear-Time%20High-Fidelity%20Diffusion%20Models%20Based%20on%20Local%0A%20%20Attention%20and%20Mamba&entry.906535625=Yunxiang%20Fu%20and%20Chaoqi%20Chen%20and%20Yizhou%20Yu&entry.1292438233=%20%20Recent%20Transformer-based%20diffusion%20models%20have%20shown%20remarkable%20performance%2C%0Alargely%20attributed%20to%20the%20ability%20of%20the%20self-attention%20mechanism%20to%20accurately%0Acapture%20both%20global%20and%20local%20contexts%20by%20computing%20all-pair%20interactions%20among%0Ainput%20tokens.%20However%2C%20their%20quadratic%20complexity%20poses%20significant%0Acomputational%20challenges%20for%20long-sequence%20inputs.%20Conversely%2C%20a%20recent%20state%0Aspace%20model%20called%20Mamba%20offers%20linear%20complexity%20by%20compressing%20a%20filtered%0Aglobal%20context%20into%20a%20hidden%20state.%20Despite%20its%20efficiency%2C%20compression%0Ainevitably%20leads%20to%20information%20loss%20of%20fine-grained%20local%20dependencies%20among%0Atokens%2C%20which%20are%20crucial%20for%20effective%20visual%20generative%20modeling.%20Motivated%0Aby%20these%20observations%2C%20we%20introduce%20Local%20Attentional%20Mamba%20%28LaMamba%29%20blocks%0Athat%20combine%20the%20strengths%20of%20self-attention%20and%20Mamba%2C%20capturing%20both%20global%0Acontexts%20and%20local%20details%20with%20linear%20complexity.%20Leveraging%20the%20efficient%0AU-Net%20architecture%2C%20our%20model%20exhibits%20exceptional%20scalability%20and%20surpasses%0Athe%20performance%20of%20DiT%20across%20various%20model%20scales%20on%20ImageNet%20at%20256x256%0Aresolution%2C%20all%20while%20utilizing%20substantially%20fewer%20GFLOPs%20and%20a%20comparable%0Anumber%20of%20parameters.%20Compared%20to%20state-of-the-art%20diffusion%20models%20on%20ImageNet%0A256x256%20and%20512x512%2C%20our%20largest%20model%20presents%20notable%20advantages%2C%20such%20as%20a%0Areduction%20of%20up%20to%2062%5C%25%20GFLOPs%20compared%20to%20DiT-XL/2%2C%20while%20achieving%20superior%0Aperformance%20with%20comparable%20or%20fewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02615v1&entry.124074799=Read"},
{"title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large\n  Language Models?", "author": "Mohammad Bahrami Karkevandi and Nishant Vishwamitra and Peyman Najafirad", "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language tasks, but their safety and morality remain contentious due to\ntheir training on internet text corpora. To address these concerns, alignment\ntechniques have been developed to improve the public usability and safety of\nLLMs. Yet, the potential for generating harmful content through these models\nseems to persist. This paper explores the concept of jailbreaking\nLLMs-reversing their alignment through adversarial triggers. Previous methods,\nsuch as soft embedding prompts, manually crafted prompts, and gradient-based\nautomatic prompts, have had limited success on black-box models due to their\nrequirements for model access and for producing a low variety of manually\ncrafted prompts, making them susceptible to being blocked. This paper\nintroduces a novel approach using reinforcement learning to optimize\nadversarial triggers, requiring only inference API access to the target model\nand a small surrogate model. Our method, which leverages a BERTScore-based\nreward function, enhances the transferability and effectiveness of adversarial\ntriggers on new black-box models. We demonstrate that this approach improves\nthe performance of adversarial triggers on a previously untested language\nmodel.\n", "link": "http://arxiv.org/abs/2408.02651v1", "date": "2024-08-05", "relevancy": 1.8855, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4777}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Reinforcement%20Learning%20Unlock%20the%20Hidden%20Dangers%20in%20Aligned%20Large%0A%20%20Language%20Models%3F&body=Title%3A%20Can%20Reinforcement%20Learning%20Unlock%20the%20Hidden%20Dangers%20in%20Aligned%20Large%0A%20%20Language%20Models%3F%0AAuthor%3A%20Mohammad%20Bahrami%20Karkevandi%20and%20Nishant%20Vishwamitra%20and%20Peyman%20Najafirad%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Anatural%20language%20tasks%2C%20but%20their%20safety%20and%20morality%20remain%20contentious%20due%20to%0Atheir%20training%20on%20internet%20text%20corpora.%20To%20address%20these%20concerns%2C%20alignment%0Atechniques%20have%20been%20developed%20to%20improve%20the%20public%20usability%20and%20safety%20of%0ALLMs.%20Yet%2C%20the%20potential%20for%20generating%20harmful%20content%20through%20these%20models%0Aseems%20to%20persist.%20This%20paper%20explores%20the%20concept%20of%20jailbreaking%0ALLMs-reversing%20their%20alignment%20through%20adversarial%20triggers.%20Previous%20methods%2C%0Asuch%20as%20soft%20embedding%20prompts%2C%20manually%20crafted%20prompts%2C%20and%20gradient-based%0Aautomatic%20prompts%2C%20have%20had%20limited%20success%20on%20black-box%20models%20due%20to%20their%0Arequirements%20for%20model%20access%20and%20for%20producing%20a%20low%20variety%20of%20manually%0Acrafted%20prompts%2C%20making%20them%20susceptible%20to%20being%20blocked.%20This%20paper%0Aintroduces%20a%20novel%20approach%20using%20reinforcement%20learning%20to%20optimize%0Aadversarial%20triggers%2C%20requiring%20only%20inference%20API%20access%20to%20the%20target%20model%0Aand%20a%20small%20surrogate%20model.%20Our%20method%2C%20which%20leverages%20a%20BERTScore-based%0Areward%20function%2C%20enhances%20the%20transferability%20and%20effectiveness%20of%20adversarial%0Atriggers%20on%20new%20black-box%20models.%20We%20demonstrate%20that%20this%20approach%20improves%0Athe%20performance%20of%20adversarial%20triggers%20on%20a%20previously%20untested%20language%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Reinforcement%2520Learning%2520Unlock%2520the%2520Hidden%2520Dangers%2520in%2520Aligned%2520Large%250A%2520%2520Language%2520Models%253F%26entry.906535625%3DMohammad%2520Bahrami%2520Karkevandi%2520and%2520Nishant%2520Vishwamitra%2520and%2520Peyman%2520Najafirad%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Anatural%2520language%2520tasks%252C%2520but%2520their%2520safety%2520and%2520morality%2520remain%2520contentious%2520due%2520to%250Atheir%2520training%2520on%2520internet%2520text%2520corpora.%2520To%2520address%2520these%2520concerns%252C%2520alignment%250Atechniques%2520have%2520been%2520developed%2520to%2520improve%2520the%2520public%2520usability%2520and%2520safety%2520of%250ALLMs.%2520Yet%252C%2520the%2520potential%2520for%2520generating%2520harmful%2520content%2520through%2520these%2520models%250Aseems%2520to%2520persist.%2520This%2520paper%2520explores%2520the%2520concept%2520of%2520jailbreaking%250ALLMs-reversing%2520their%2520alignment%2520through%2520adversarial%2520triggers.%2520Previous%2520methods%252C%250Asuch%2520as%2520soft%2520embedding%2520prompts%252C%2520manually%2520crafted%2520prompts%252C%2520and%2520gradient-based%250Aautomatic%2520prompts%252C%2520have%2520had%2520limited%2520success%2520on%2520black-box%2520models%2520due%2520to%2520their%250Arequirements%2520for%2520model%2520access%2520and%2520for%2520producing%2520a%2520low%2520variety%2520of%2520manually%250Acrafted%2520prompts%252C%2520making%2520them%2520susceptible%2520to%2520being%2520blocked.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520approach%2520using%2520reinforcement%2520learning%2520to%2520optimize%250Aadversarial%2520triggers%252C%2520requiring%2520only%2520inference%2520API%2520access%2520to%2520the%2520target%2520model%250Aand%2520a%2520small%2520surrogate%2520model.%2520Our%2520method%252C%2520which%2520leverages%2520a%2520BERTScore-based%250Areward%2520function%252C%2520enhances%2520the%2520transferability%2520and%2520effectiveness%2520of%2520adversarial%250Atriggers%2520on%2520new%2520black-box%2520models.%2520We%2520demonstrate%2520that%2520this%2520approach%2520improves%250Athe%2520performance%2520of%2520adversarial%2520triggers%2520on%2520a%2520previously%2520untested%2520language%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Reinforcement%20Learning%20Unlock%20the%20Hidden%20Dangers%20in%20Aligned%20Large%0A%20%20Language%20Models%3F&entry.906535625=Mohammad%20Bahrami%20Karkevandi%20and%20Nishant%20Vishwamitra%20and%20Peyman%20Najafirad&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Anatural%20language%20tasks%2C%20but%20their%20safety%20and%20morality%20remain%20contentious%20due%20to%0Atheir%20training%20on%20internet%20text%20corpora.%20To%20address%20these%20concerns%2C%20alignment%0Atechniques%20have%20been%20developed%20to%20improve%20the%20public%20usability%20and%20safety%20of%0ALLMs.%20Yet%2C%20the%20potential%20for%20generating%20harmful%20content%20through%20these%20models%0Aseems%20to%20persist.%20This%20paper%20explores%20the%20concept%20of%20jailbreaking%0ALLMs-reversing%20their%20alignment%20through%20adversarial%20triggers.%20Previous%20methods%2C%0Asuch%20as%20soft%20embedding%20prompts%2C%20manually%20crafted%20prompts%2C%20and%20gradient-based%0Aautomatic%20prompts%2C%20have%20had%20limited%20success%20on%20black-box%20models%20due%20to%20their%0Arequirements%20for%20model%20access%20and%20for%20producing%20a%20low%20variety%20of%20manually%0Acrafted%20prompts%2C%20making%20them%20susceptible%20to%20being%20blocked.%20This%20paper%0Aintroduces%20a%20novel%20approach%20using%20reinforcement%20learning%20to%20optimize%0Aadversarial%20triggers%2C%20requiring%20only%20inference%20API%20access%20to%20the%20target%20model%0Aand%20a%20small%20surrogate%20model.%20Our%20method%2C%20which%20leverages%20a%20BERTScore-based%0Areward%20function%2C%20enhances%20the%20transferability%20and%20effectiveness%20of%20adversarial%0Atriggers%20on%20new%20black-box%20models.%20We%20demonstrate%20that%20this%20approach%20improves%0Athe%20performance%20of%20adversarial%20triggers%20on%20a%20previously%20untested%20language%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02651v1&entry.124074799=Read"},
{"title": "Vertical Federated Learning: Challenges, Methodologies and Experiments", "author": "Kang Wei and Jun Li and Chuan Ma and Ming Ding and Sha Wei and Fan Wu and Guihai Chen and Thilina Ranbaduge", "abstract": "  Recently, federated learning (FL) has emerged as a promising distributed\nmachine learning (ML) technology, owing to the advancing computational and\nsensing capacities of end-user devices, however with the increasing concerns on\nusers' privacy. As a special architecture in FL, vertical FL (VFL) is capable\nof constructing a hyper ML model by embracing sub-models from different\nclients. These sub-models are trained locally by vertically partitioned data\nwith distinct attributes. Therefore, the design of VFL is fundamentally\ndifferent from that of conventional FL, raising new and unique research issues.\nIn this paper, we aim to discuss key challenges in VFL with effective\nsolutions, and conduct experiments on real-life datasets to shed light on these\nissues. Specifically, we first propose a general framework on VFL, and\nhighlight the key differences between VFL and conventional FL. Then, we discuss\nresearch challenges rooted in VFL systems under four aspects, i.e., security\nand privacy risks, expensive computation and communication costs, possible\nstructural damage caused by model splitting, and system heterogeneity.\nAfterwards, we develop solutions to addressing the aforementioned challenges,\nand conduct extensive experiments to showcase the effectiveness of our proposed\nsolutions.\n", "link": "http://arxiv.org/abs/2202.04309v2", "date": "2024-08-05", "relevancy": 1.8752, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4771}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4655}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vertical%20Federated%20Learning%3A%20Challenges%2C%20Methodologies%20and%20Experiments&body=Title%3A%20Vertical%20Federated%20Learning%3A%20Challenges%2C%20Methodologies%20and%20Experiments%0AAuthor%3A%20Kang%20Wei%20and%20Jun%20Li%20and%20Chuan%20Ma%20and%20Ming%20Ding%20and%20Sha%20Wei%20and%20Fan%20Wu%20and%20Guihai%20Chen%20and%20Thilina%20Ranbaduge%0AAbstract%3A%20%20%20Recently%2C%20federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20distributed%0Amachine%20learning%20%28ML%29%20technology%2C%20owing%20to%20the%20advancing%20computational%20and%0Asensing%20capacities%20of%20end-user%20devices%2C%20however%20with%20the%20increasing%20concerns%20on%0Ausers%27%20privacy.%20As%20a%20special%20architecture%20in%20FL%2C%20vertical%20FL%20%28VFL%29%20is%20capable%0Aof%20constructing%20a%20hyper%20ML%20model%20by%20embracing%20sub-models%20from%20different%0Aclients.%20These%20sub-models%20are%20trained%20locally%20by%20vertically%20partitioned%20data%0Awith%20distinct%20attributes.%20Therefore%2C%20the%20design%20of%20VFL%20is%20fundamentally%0Adifferent%20from%20that%20of%20conventional%20FL%2C%20raising%20new%20and%20unique%20research%20issues.%0AIn%20this%20paper%2C%20we%20aim%20to%20discuss%20key%20challenges%20in%20VFL%20with%20effective%0Asolutions%2C%20and%20conduct%20experiments%20on%20real-life%20datasets%20to%20shed%20light%20on%20these%0Aissues.%20Specifically%2C%20we%20first%20propose%20a%20general%20framework%20on%20VFL%2C%20and%0Ahighlight%20the%20key%20differences%20between%20VFL%20and%20conventional%20FL.%20Then%2C%20we%20discuss%0Aresearch%20challenges%20rooted%20in%20VFL%20systems%20under%20four%20aspects%2C%20i.e.%2C%20security%0Aand%20privacy%20risks%2C%20expensive%20computation%20and%20communication%20costs%2C%20possible%0Astructural%20damage%20caused%20by%20model%20splitting%2C%20and%20system%20heterogeneity.%0AAfterwards%2C%20we%20develop%20solutions%20to%20addressing%20the%20aforementioned%20challenges%2C%0Aand%20conduct%20extensive%20experiments%20to%20showcase%20the%20effectiveness%20of%20our%20proposed%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.04309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVertical%2520Federated%2520Learning%253A%2520Challenges%252C%2520Methodologies%2520and%2520Experiments%26entry.906535625%3DKang%2520Wei%2520and%2520Jun%2520Li%2520and%2520Chuan%2520Ma%2520and%2520Ming%2520Ding%2520and%2520Sha%2520Wei%2520and%2520Fan%2520Wu%2520and%2520Guihai%2520Chen%2520and%2520Thilina%2520Ranbaduge%26entry.1292438233%3D%2520%2520Recently%252C%2520federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520distributed%250Amachine%2520learning%2520%2528ML%2529%2520technology%252C%2520owing%2520to%2520the%2520advancing%2520computational%2520and%250Asensing%2520capacities%2520of%2520end-user%2520devices%252C%2520however%2520with%2520the%2520increasing%2520concerns%2520on%250Ausers%2527%2520privacy.%2520As%2520a%2520special%2520architecture%2520in%2520FL%252C%2520vertical%2520FL%2520%2528VFL%2529%2520is%2520capable%250Aof%2520constructing%2520a%2520hyper%2520ML%2520model%2520by%2520embracing%2520sub-models%2520from%2520different%250Aclients.%2520These%2520sub-models%2520are%2520trained%2520locally%2520by%2520vertically%2520partitioned%2520data%250Awith%2520distinct%2520attributes.%2520Therefore%252C%2520the%2520design%2520of%2520VFL%2520is%2520fundamentally%250Adifferent%2520from%2520that%2520of%2520conventional%2520FL%252C%2520raising%2520new%2520and%2520unique%2520research%2520issues.%250AIn%2520this%2520paper%252C%2520we%2520aim%2520to%2520discuss%2520key%2520challenges%2520in%2520VFL%2520with%2520effective%250Asolutions%252C%2520and%2520conduct%2520experiments%2520on%2520real-life%2520datasets%2520to%2520shed%2520light%2520on%2520these%250Aissues.%2520Specifically%252C%2520we%2520first%2520propose%2520a%2520general%2520framework%2520on%2520VFL%252C%2520and%250Ahighlight%2520the%2520key%2520differences%2520between%2520VFL%2520and%2520conventional%2520FL.%2520Then%252C%2520we%2520discuss%250Aresearch%2520challenges%2520rooted%2520in%2520VFL%2520systems%2520under%2520four%2520aspects%252C%2520i.e.%252C%2520security%250Aand%2520privacy%2520risks%252C%2520expensive%2520computation%2520and%2520communication%2520costs%252C%2520possible%250Astructural%2520damage%2520caused%2520by%2520model%2520splitting%252C%2520and%2520system%2520heterogeneity.%250AAfterwards%252C%2520we%2520develop%2520solutions%2520to%2520addressing%2520the%2520aforementioned%2520challenges%252C%250Aand%2520conduct%2520extensive%2520experiments%2520to%2520showcase%2520the%2520effectiveness%2520of%2520our%2520proposed%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.04309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vertical%20Federated%20Learning%3A%20Challenges%2C%20Methodologies%20and%20Experiments&entry.906535625=Kang%20Wei%20and%20Jun%20Li%20and%20Chuan%20Ma%20and%20Ming%20Ding%20and%20Sha%20Wei%20and%20Fan%20Wu%20and%20Guihai%20Chen%20and%20Thilina%20Ranbaduge&entry.1292438233=%20%20Recently%2C%20federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20distributed%0Amachine%20learning%20%28ML%29%20technology%2C%20owing%20to%20the%20advancing%20computational%20and%0Asensing%20capacities%20of%20end-user%20devices%2C%20however%20with%20the%20increasing%20concerns%20on%0Ausers%27%20privacy.%20As%20a%20special%20architecture%20in%20FL%2C%20vertical%20FL%20%28VFL%29%20is%20capable%0Aof%20constructing%20a%20hyper%20ML%20model%20by%20embracing%20sub-models%20from%20different%0Aclients.%20These%20sub-models%20are%20trained%20locally%20by%20vertically%20partitioned%20data%0Awith%20distinct%20attributes.%20Therefore%2C%20the%20design%20of%20VFL%20is%20fundamentally%0Adifferent%20from%20that%20of%20conventional%20FL%2C%20raising%20new%20and%20unique%20research%20issues.%0AIn%20this%20paper%2C%20we%20aim%20to%20discuss%20key%20challenges%20in%20VFL%20with%20effective%0Asolutions%2C%20and%20conduct%20experiments%20on%20real-life%20datasets%20to%20shed%20light%20on%20these%0Aissues.%20Specifically%2C%20we%20first%20propose%20a%20general%20framework%20on%20VFL%2C%20and%0Ahighlight%20the%20key%20differences%20between%20VFL%20and%20conventional%20FL.%20Then%2C%20we%20discuss%0Aresearch%20challenges%20rooted%20in%20VFL%20systems%20under%20four%20aspects%2C%20i.e.%2C%20security%0Aand%20privacy%20risks%2C%20expensive%20computation%20and%20communication%20costs%2C%20possible%0Astructural%20damage%20caused%20by%20model%20splitting%2C%20and%20system%20heterogeneity.%0AAfterwards%2C%20we%20develop%20solutions%20to%20addressing%20the%20aforementioned%20challenges%2C%0Aand%20conduct%20extensive%20experiments%20to%20showcase%20the%20effectiveness%20of%20our%20proposed%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.04309v2&entry.124074799=Read"},
{"title": "AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models\n  with Neural Networks", "author": "Tiago Monteiro", "abstract": "  In quantitative finance, machine learning methods are essential for alpha\ngeneration. This study introduces a new approach that combines Hidden Markov\nModels (HMM) and neural networks, integrated with Black-Litterman portfolio\noptimization. During the COVID period (2019-2022), this dual-model approach\nachieved a 97% return with a Sharpe ratio of 0.992. It incorporates two risk\nmodels to enhance risk management, showing efficiency during volatile periods.\nThe methodology was implemented on the QuantConnect platform, which was chosen\nfor its robust framework and experimental reproducibility. The system, which\npredicts future price movements, includes a three-year warm-up to ensure proper\nalgorithm function. It targets highly liquid, large-cap energy stocks to ensure\nstable and predictable performance while also considering broker payments. The\ndual-model alpha system utilizes log returns to select the optimal state based\non the historical performance. It combines state predictions with neural\nnetwork outputs, which are based on historical data, to generate trading\nsignals. This study examined the architecture of the trading system, data\npre-processing, training, and performance. The full code and backtesting data\nare available under the MIT license.\n", "link": "http://arxiv.org/abs/2407.19858v2", "date": "2024-08-05", "relevancy": 1.8599, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4505}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Powered%20Energy%20Algorithmic%20Trading%3A%20Integrating%20Hidden%20Markov%20Models%0A%20%20with%20Neural%20Networks&body=Title%3A%20AI-Powered%20Energy%20Algorithmic%20Trading%3A%20Integrating%20Hidden%20Markov%20Models%0A%20%20with%20Neural%20Networks%0AAuthor%3A%20Tiago%20Monteiro%0AAbstract%3A%20%20%20In%20quantitative%20finance%2C%20machine%20learning%20methods%20are%20essential%20for%20alpha%0Ageneration.%20This%20study%20introduces%20a%20new%20approach%20that%20combines%20Hidden%20Markov%0AModels%20%28HMM%29%20and%20neural%20networks%2C%20integrated%20with%20Black-Litterman%20portfolio%0Aoptimization.%20During%20the%20COVID%20period%20%282019-2022%29%2C%20this%20dual-model%20approach%0Aachieved%20a%2097%25%20return%20with%20a%20Sharpe%20ratio%20of%200.992.%20It%20incorporates%20two%20risk%0Amodels%20to%20enhance%20risk%20management%2C%20showing%20efficiency%20during%20volatile%20periods.%0AThe%20methodology%20was%20implemented%20on%20the%20QuantConnect%20platform%2C%20which%20was%20chosen%0Afor%20its%20robust%20framework%20and%20experimental%20reproducibility.%20The%20system%2C%20which%0Apredicts%20future%20price%20movements%2C%20includes%20a%20three-year%20warm-up%20to%20ensure%20proper%0Aalgorithm%20function.%20It%20targets%20highly%20liquid%2C%20large-cap%20energy%20stocks%20to%20ensure%0Astable%20and%20predictable%20performance%20while%20also%20considering%20broker%20payments.%20The%0Adual-model%20alpha%20system%20utilizes%20log%20returns%20to%20select%20the%20optimal%20state%20based%0Aon%20the%20historical%20performance.%20It%20combines%20state%20predictions%20with%20neural%0Anetwork%20outputs%2C%20which%20are%20based%20on%20historical%20data%2C%20to%20generate%20trading%0Asignals.%20This%20study%20examined%20the%20architecture%20of%20the%20trading%20system%2C%20data%0Apre-processing%2C%20training%2C%20and%20performance.%20The%20full%20code%20and%20backtesting%20data%0Aare%20available%20under%20the%20MIT%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Powered%2520Energy%2520Algorithmic%2520Trading%253A%2520Integrating%2520Hidden%2520Markov%2520Models%250A%2520%2520with%2520Neural%2520Networks%26entry.906535625%3DTiago%2520Monteiro%26entry.1292438233%3D%2520%2520In%2520quantitative%2520finance%252C%2520machine%2520learning%2520methods%2520are%2520essential%2520for%2520alpha%250Ageneration.%2520This%2520study%2520introduces%2520a%2520new%2520approach%2520that%2520combines%2520Hidden%2520Markov%250AModels%2520%2528HMM%2529%2520and%2520neural%2520networks%252C%2520integrated%2520with%2520Black-Litterman%2520portfolio%250Aoptimization.%2520During%2520the%2520COVID%2520period%2520%25282019-2022%2529%252C%2520this%2520dual-model%2520approach%250Aachieved%2520a%252097%2525%2520return%2520with%2520a%2520Sharpe%2520ratio%2520of%25200.992.%2520It%2520incorporates%2520two%2520risk%250Amodels%2520to%2520enhance%2520risk%2520management%252C%2520showing%2520efficiency%2520during%2520volatile%2520periods.%250AThe%2520methodology%2520was%2520implemented%2520on%2520the%2520QuantConnect%2520platform%252C%2520which%2520was%2520chosen%250Afor%2520its%2520robust%2520framework%2520and%2520experimental%2520reproducibility.%2520The%2520system%252C%2520which%250Apredicts%2520future%2520price%2520movements%252C%2520includes%2520a%2520three-year%2520warm-up%2520to%2520ensure%2520proper%250Aalgorithm%2520function.%2520It%2520targets%2520highly%2520liquid%252C%2520large-cap%2520energy%2520stocks%2520to%2520ensure%250Astable%2520and%2520predictable%2520performance%2520while%2520also%2520considering%2520broker%2520payments.%2520The%250Adual-model%2520alpha%2520system%2520utilizes%2520log%2520returns%2520to%2520select%2520the%2520optimal%2520state%2520based%250Aon%2520the%2520historical%2520performance.%2520It%2520combines%2520state%2520predictions%2520with%2520neural%250Anetwork%2520outputs%252C%2520which%2520are%2520based%2520on%2520historical%2520data%252C%2520to%2520generate%2520trading%250Asignals.%2520This%2520study%2520examined%2520the%2520architecture%2520of%2520the%2520trading%2520system%252C%2520data%250Apre-processing%252C%2520training%252C%2520and%2520performance.%2520The%2520full%2520code%2520and%2520backtesting%2520data%250Aare%2520available%2520under%2520the%2520MIT%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Powered%20Energy%20Algorithmic%20Trading%3A%20Integrating%20Hidden%20Markov%20Models%0A%20%20with%20Neural%20Networks&entry.906535625=Tiago%20Monteiro&entry.1292438233=%20%20In%20quantitative%20finance%2C%20machine%20learning%20methods%20are%20essential%20for%20alpha%0Ageneration.%20This%20study%20introduces%20a%20new%20approach%20that%20combines%20Hidden%20Markov%0AModels%20%28HMM%29%20and%20neural%20networks%2C%20integrated%20with%20Black-Litterman%20portfolio%0Aoptimization.%20During%20the%20COVID%20period%20%282019-2022%29%2C%20this%20dual-model%20approach%0Aachieved%20a%2097%25%20return%20with%20a%20Sharpe%20ratio%20of%200.992.%20It%20incorporates%20two%20risk%0Amodels%20to%20enhance%20risk%20management%2C%20showing%20efficiency%20during%20volatile%20periods.%0AThe%20methodology%20was%20implemented%20on%20the%20QuantConnect%20platform%2C%20which%20was%20chosen%0Afor%20its%20robust%20framework%20and%20experimental%20reproducibility.%20The%20system%2C%20which%0Apredicts%20future%20price%20movements%2C%20includes%20a%20three-year%20warm-up%20to%20ensure%20proper%0Aalgorithm%20function.%20It%20targets%20highly%20liquid%2C%20large-cap%20energy%20stocks%20to%20ensure%0Astable%20and%20predictable%20performance%20while%20also%20considering%20broker%20payments.%20The%0Adual-model%20alpha%20system%20utilizes%20log%20returns%20to%20select%20the%20optimal%20state%20based%0Aon%20the%20historical%20performance.%20It%20combines%20state%20predictions%20with%20neural%0Anetwork%20outputs%2C%20which%20are%20based%20on%20historical%20data%2C%20to%20generate%20trading%0Asignals.%20This%20study%20examined%20the%20architecture%20of%20the%20trading%20system%2C%20data%0Apre-processing%2C%20training%2C%20and%20performance.%20The%20full%20code%20and%20backtesting%20data%0Aare%20available%20under%20the%20MIT%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19858v2&entry.124074799=Read"},
{"title": "Bridging Smoothness and Approximation: Theoretical Insights into\n  Over-Smoothing in Graph Neural Networks", "author": "Guangrui Yang and Jianfei Li and Ming Li and Han Feng and Ding-Xuan Zhou", "abstract": "  In this paper, we explore the approximation theory of functions defined on\ngraphs. Our study builds upon the approximation results derived from the\n$K$-functional. We establish a theoretical framework to assess the lower bounds\nof approximation for target functions using Graph Convolutional Networks (GCNs)\nand examine the over-smoothing phenomenon commonly observed in these networks.\nInitially, we introduce the concept of a $K$-functional on graphs, establishing\nits equivalence to the modulus of smoothness. We then analyze a typical type of\nGCN to demonstrate how the high-frequency energy of the output decays, an\nindicator of over-smoothing. This analysis provides theoretical insights into\nthe nature of over-smoothing within GCNs. Furthermore, we establish a lower\nbound for the approximation of target functions by GCNs, which is governed by\nthe modulus of smoothness of these functions. This finding offers a new\nperspective on the approximation capabilities of GCNs. In our numerical\nexperiments, we analyze several widely applied GCNs and observe the phenomenon\nof energy decay. These observations corroborate our theoretical results on\nexponential decay order.\n", "link": "http://arxiv.org/abs/2407.01281v2", "date": "2024-08-05", "relevancy": 1.8586, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4645}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Smoothness%20and%20Approximation%3A%20Theoretical%20Insights%20into%0A%20%20Over-Smoothing%20in%20Graph%20Neural%20Networks&body=Title%3A%20Bridging%20Smoothness%20and%20Approximation%3A%20Theoretical%20Insights%20into%0A%20%20Over-Smoothing%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Guangrui%20Yang%20and%20Jianfei%20Li%20and%20Ming%20Li%20and%20Han%20Feng%20and%20Ding-Xuan%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20the%20approximation%20theory%20of%20functions%20defined%20on%0Agraphs.%20Our%20study%20builds%20upon%20the%20approximation%20results%20derived%20from%20the%0A%24K%24-functional.%20We%20establish%20a%20theoretical%20framework%20to%20assess%20the%20lower%20bounds%0Aof%20approximation%20for%20target%20functions%20using%20Graph%20Convolutional%20Networks%20%28GCNs%29%0Aand%20examine%20the%20over-smoothing%20phenomenon%20commonly%20observed%20in%20these%20networks.%0AInitially%2C%20we%20introduce%20the%20concept%20of%20a%20%24K%24-functional%20on%20graphs%2C%20establishing%0Aits%20equivalence%20to%20the%20modulus%20of%20smoothness.%20We%20then%20analyze%20a%20typical%20type%20of%0AGCN%20to%20demonstrate%20how%20the%20high-frequency%20energy%20of%20the%20output%20decays%2C%20an%0Aindicator%20of%20over-smoothing.%20This%20analysis%20provides%20theoretical%20insights%20into%0Athe%20nature%20of%20over-smoothing%20within%20GCNs.%20Furthermore%2C%20we%20establish%20a%20lower%0Abound%20for%20the%20approximation%20of%20target%20functions%20by%20GCNs%2C%20which%20is%20governed%20by%0Athe%20modulus%20of%20smoothness%20of%20these%20functions.%20This%20finding%20offers%20a%20new%0Aperspective%20on%20the%20approximation%20capabilities%20of%20GCNs.%20In%20our%20numerical%0Aexperiments%2C%20we%20analyze%20several%20widely%20applied%20GCNs%20and%20observe%20the%20phenomenon%0Aof%20energy%20decay.%20These%20observations%20corroborate%20our%20theoretical%20results%20on%0Aexponential%20decay%20order.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01281v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Smoothness%2520and%2520Approximation%253A%2520Theoretical%2520Insights%2520into%250A%2520%2520Over-Smoothing%2520in%2520Graph%2520Neural%2520Networks%26entry.906535625%3DGuangrui%2520Yang%2520and%2520Jianfei%2520Li%2520and%2520Ming%2520Li%2520and%2520Han%2520Feng%2520and%2520Ding-Xuan%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520approximation%2520theory%2520of%2520functions%2520defined%2520on%250Agraphs.%2520Our%2520study%2520builds%2520upon%2520the%2520approximation%2520results%2520derived%2520from%2520the%250A%2524K%2524-functional.%2520We%2520establish%2520a%2520theoretical%2520framework%2520to%2520assess%2520the%2520lower%2520bounds%250Aof%2520approximation%2520for%2520target%2520functions%2520using%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%250Aand%2520examine%2520the%2520over-smoothing%2520phenomenon%2520commonly%2520observed%2520in%2520these%2520networks.%250AInitially%252C%2520we%2520introduce%2520the%2520concept%2520of%2520a%2520%2524K%2524-functional%2520on%2520graphs%252C%2520establishing%250Aits%2520equivalence%2520to%2520the%2520modulus%2520of%2520smoothness.%2520We%2520then%2520analyze%2520a%2520typical%2520type%2520of%250AGCN%2520to%2520demonstrate%2520how%2520the%2520high-frequency%2520energy%2520of%2520the%2520output%2520decays%252C%2520an%250Aindicator%2520of%2520over-smoothing.%2520This%2520analysis%2520provides%2520theoretical%2520insights%2520into%250Athe%2520nature%2520of%2520over-smoothing%2520within%2520GCNs.%2520Furthermore%252C%2520we%2520establish%2520a%2520lower%250Abound%2520for%2520the%2520approximation%2520of%2520target%2520functions%2520by%2520GCNs%252C%2520which%2520is%2520governed%2520by%250Athe%2520modulus%2520of%2520smoothness%2520of%2520these%2520functions.%2520This%2520finding%2520offers%2520a%2520new%250Aperspective%2520on%2520the%2520approximation%2520capabilities%2520of%2520GCNs.%2520In%2520our%2520numerical%250Aexperiments%252C%2520we%2520analyze%2520several%2520widely%2520applied%2520GCNs%2520and%2520observe%2520the%2520phenomenon%250Aof%2520energy%2520decay.%2520These%2520observations%2520corroborate%2520our%2520theoretical%2520results%2520on%250Aexponential%2520decay%2520order.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01281v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Smoothness%20and%20Approximation%3A%20Theoretical%20Insights%20into%0A%20%20Over-Smoothing%20in%20Graph%20Neural%20Networks&entry.906535625=Guangrui%20Yang%20and%20Jianfei%20Li%20and%20Ming%20Li%20and%20Han%20Feng%20and%20Ding-Xuan%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20the%20approximation%20theory%20of%20functions%20defined%20on%0Agraphs.%20Our%20study%20builds%20upon%20the%20approximation%20results%20derived%20from%20the%0A%24K%24-functional.%20We%20establish%20a%20theoretical%20framework%20to%20assess%20the%20lower%20bounds%0Aof%20approximation%20for%20target%20functions%20using%20Graph%20Convolutional%20Networks%20%28GCNs%29%0Aand%20examine%20the%20over-smoothing%20phenomenon%20commonly%20observed%20in%20these%20networks.%0AInitially%2C%20we%20introduce%20the%20concept%20of%20a%20%24K%24-functional%20on%20graphs%2C%20establishing%0Aits%20equivalence%20to%20the%20modulus%20of%20smoothness.%20We%20then%20analyze%20a%20typical%20type%20of%0AGCN%20to%20demonstrate%20how%20the%20high-frequency%20energy%20of%20the%20output%20decays%2C%20an%0Aindicator%20of%20over-smoothing.%20This%20analysis%20provides%20theoretical%20insights%20into%0Athe%20nature%20of%20over-smoothing%20within%20GCNs.%20Furthermore%2C%20we%20establish%20a%20lower%0Abound%20for%20the%20approximation%20of%20target%20functions%20by%20GCNs%2C%20which%20is%20governed%20by%0Athe%20modulus%20of%20smoothness%20of%20these%20functions.%20This%20finding%20offers%20a%20new%0Aperspective%20on%20the%20approximation%20capabilities%20of%20GCNs.%20In%20our%20numerical%0Aexperiments%2C%20we%20analyze%20several%20widely%20applied%20GCNs%20and%20observe%20the%20phenomenon%0Aof%20energy%20decay.%20These%20observations%20corroborate%20our%20theoretical%20results%20on%0Aexponential%20decay%20order.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01281v2&entry.124074799=Read"},
{"title": "Enhancing AI-based Generation of Software Exploits with Contextual\n  Information", "author": "Pietro Liguori and Cristina Improta and Roberto Natella and Bojan Cukic and Domenico Cotroneo", "abstract": "  This practical experience report explores Neural Machine Translation (NMT)\nmodels' capability to generate offensive security code from natural language\n(NL) descriptions, highlighting the significance of contextual understanding\nand its impact on model performance. Our study employs a dataset comprising\nreal shellcodes to evaluate the models across various scenarios, including\nmissing information, necessary context, and unnecessary context. The\nexperiments are designed to assess the models' resilience against incomplete\ndescriptions, their proficiency in leveraging context for enhanced accuracy,\nand their ability to discern irrelevant information. The findings reveal that\nthe introduction of contextual data significantly improves performance.\nHowever, the benefits of additional context diminish beyond a certain point,\nindicating an optimal level of contextual information for model training.\nMoreover, the models demonstrate an ability to filter out unnecessary context,\nmaintaining high levels of accuracy in the generation of offensive security\ncode. This study paves the way for future research on optimizing context use in\nAI-driven code generation, particularly for applications requiring a high\ndegree of technical precision such as the generation of offensive code.\n", "link": "http://arxiv.org/abs/2408.02402v1", "date": "2024-08-05", "relevancy": 1.8444, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4729}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4623}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information&body=Title%3A%20Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information%0AAuthor%3A%20Pietro%20Liguori%20and%20Cristina%20Improta%20and%20Roberto%20Natella%20and%20Bojan%20Cukic%20and%20Domenico%20Cotroneo%0AAbstract%3A%20%20%20This%20practical%20experience%20report%20explores%20Neural%20Machine%20Translation%20%28NMT%29%0Amodels%27%20capability%20to%20generate%20offensive%20security%20code%20from%20natural%20language%0A%28NL%29%20descriptions%2C%20highlighting%20the%20significance%20of%20contextual%20understanding%0Aand%20its%20impact%20on%20model%20performance.%20Our%20study%20employs%20a%20dataset%20comprising%0Areal%20shellcodes%20to%20evaluate%20the%20models%20across%20various%20scenarios%2C%20including%0Amissing%20information%2C%20necessary%20context%2C%20and%20unnecessary%20context.%20The%0Aexperiments%20are%20designed%20to%20assess%20the%20models%27%20resilience%20against%20incomplete%0Adescriptions%2C%20their%20proficiency%20in%20leveraging%20context%20for%20enhanced%20accuracy%2C%0Aand%20their%20ability%20to%20discern%20irrelevant%20information.%20The%20findings%20reveal%20that%0Athe%20introduction%20of%20contextual%20data%20significantly%20improves%20performance.%0AHowever%2C%20the%20benefits%20of%20additional%20context%20diminish%20beyond%20a%20certain%20point%2C%0Aindicating%20an%20optimal%20level%20of%20contextual%20information%20for%20model%20training.%0AMoreover%2C%20the%20models%20demonstrate%20an%20ability%20to%20filter%20out%20unnecessary%20context%2C%0Amaintaining%20high%20levels%20of%20accuracy%20in%20the%20generation%20of%20offensive%20security%0Acode.%20This%20study%20paves%20the%20way%20for%20future%20research%20on%20optimizing%20context%20use%20in%0AAI-driven%20code%20generation%2C%20particularly%20for%20applications%20requiring%20a%20high%0Adegree%20of%20technical%20precision%20such%20as%20the%20generation%20of%20offensive%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520AI-based%2520Generation%2520of%2520Software%2520Exploits%2520with%2520Contextual%250A%2520%2520Information%26entry.906535625%3DPietro%2520Liguori%2520and%2520Cristina%2520Improta%2520and%2520Roberto%2520Natella%2520and%2520Bojan%2520Cukic%2520and%2520Domenico%2520Cotroneo%26entry.1292438233%3D%2520%2520This%2520practical%2520experience%2520report%2520explores%2520Neural%2520Machine%2520Translation%2520%2528NMT%2529%250Amodels%2527%2520capability%2520to%2520generate%2520offensive%2520security%2520code%2520from%2520natural%2520language%250A%2528NL%2529%2520descriptions%252C%2520highlighting%2520the%2520significance%2520of%2520contextual%2520understanding%250Aand%2520its%2520impact%2520on%2520model%2520performance.%2520Our%2520study%2520employs%2520a%2520dataset%2520comprising%250Areal%2520shellcodes%2520to%2520evaluate%2520the%2520models%2520across%2520various%2520scenarios%252C%2520including%250Amissing%2520information%252C%2520necessary%2520context%252C%2520and%2520unnecessary%2520context.%2520The%250Aexperiments%2520are%2520designed%2520to%2520assess%2520the%2520models%2527%2520resilience%2520against%2520incomplete%250Adescriptions%252C%2520their%2520proficiency%2520in%2520leveraging%2520context%2520for%2520enhanced%2520accuracy%252C%250Aand%2520their%2520ability%2520to%2520discern%2520irrelevant%2520information.%2520The%2520findings%2520reveal%2520that%250Athe%2520introduction%2520of%2520contextual%2520data%2520significantly%2520improves%2520performance.%250AHowever%252C%2520the%2520benefits%2520of%2520additional%2520context%2520diminish%2520beyond%2520a%2520certain%2520point%252C%250Aindicating%2520an%2520optimal%2520level%2520of%2520contextual%2520information%2520for%2520model%2520training.%250AMoreover%252C%2520the%2520models%2520demonstrate%2520an%2520ability%2520to%2520filter%2520out%2520unnecessary%2520context%252C%250Amaintaining%2520high%2520levels%2520of%2520accuracy%2520in%2520the%2520generation%2520of%2520offensive%2520security%250Acode.%2520This%2520study%2520paves%2520the%2520way%2520for%2520future%2520research%2520on%2520optimizing%2520context%2520use%2520in%250AAI-driven%2520code%2520generation%252C%2520particularly%2520for%2520applications%2520requiring%2520a%2520high%250Adegree%2520of%2520technical%2520precision%2520such%2520as%2520the%2520generation%2520of%2520offensive%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information&entry.906535625=Pietro%20Liguori%20and%20Cristina%20Improta%20and%20Roberto%20Natella%20and%20Bojan%20Cukic%20and%20Domenico%20Cotroneo&entry.1292438233=%20%20This%20practical%20experience%20report%20explores%20Neural%20Machine%20Translation%20%28NMT%29%0Amodels%27%20capability%20to%20generate%20offensive%20security%20code%20from%20natural%20language%0A%28NL%29%20descriptions%2C%20highlighting%20the%20significance%20of%20contextual%20understanding%0Aand%20its%20impact%20on%20model%20performance.%20Our%20study%20employs%20a%20dataset%20comprising%0Areal%20shellcodes%20to%20evaluate%20the%20models%20across%20various%20scenarios%2C%20including%0Amissing%20information%2C%20necessary%20context%2C%20and%20unnecessary%20context.%20The%0Aexperiments%20are%20designed%20to%20assess%20the%20models%27%20resilience%20against%20incomplete%0Adescriptions%2C%20their%20proficiency%20in%20leveraging%20context%20for%20enhanced%20accuracy%2C%0Aand%20their%20ability%20to%20discern%20irrelevant%20information.%20The%20findings%20reveal%20that%0Athe%20introduction%20of%20contextual%20data%20significantly%20improves%20performance.%0AHowever%2C%20the%20benefits%20of%20additional%20context%20diminish%20beyond%20a%20certain%20point%2C%0Aindicating%20an%20optimal%20level%20of%20contextual%20information%20for%20model%20training.%0AMoreover%2C%20the%20models%20demonstrate%20an%20ability%20to%20filter%20out%20unnecessary%20context%2C%0Amaintaining%20high%20levels%20of%20accuracy%20in%20the%20generation%20of%20offensive%20security%0Acode.%20This%20study%20paves%20the%20way%20for%20future%20research%20on%20optimizing%20context%20use%20in%0AAI-driven%20code%20generation%2C%20particularly%20for%20applications%20requiring%20a%20high%0Adegree%20of%20technical%20precision%20such%20as%20the%20generation%20of%20offensive%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02402v1&entry.124074799=Read"},
{"title": "Long Input Benchmark for Russian Analysis", "author": "Igor Churin and Murat Apishev and Maria Tikhonova and Denis Shevelev and Aydar Bulatov and Yuri Kuratov and Sergej Averkiev and Alena Fenogenova", "abstract": "  Recent advancements in Natural Language Processing (NLP) have fostered the\ndevelopment of Large Language Models (LLMs) that can solve an immense variety\nof tasks. One of the key aspects of their application is their ability to work\nwith long text documents and to process long sequences of tokens. This has\ncreated a demand for proper evaluation of long-context understanding. To\naddress this need for the Russian language, we propose LIBRA (Long Input\nBenchmark for Russian Analysis), which comprises 21 adapted datasets to study\nthe LLM's abilities to understand long texts thoroughly. The tests are divided\ninto four complexity groups and allow the evaluation of models across various\ncontext lengths ranging from 4k up to 128k tokens. We provide the open-source\ndatasets, codebase, and public leaderboard for LIBRA to guide forthcoming\nresearch.\n", "link": "http://arxiv.org/abs/2408.02439v1", "date": "2024-08-05", "relevancy": 1.8412, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4872}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4415}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Input%20Benchmark%20for%20Russian%20Analysis&body=Title%3A%20Long%20Input%20Benchmark%20for%20Russian%20Analysis%0AAuthor%3A%20Igor%20Churin%20and%20Murat%20Apishev%20and%20Maria%20Tikhonova%20and%20Denis%20Shevelev%20and%20Aydar%20Bulatov%20and%20Yuri%20Kuratov%20and%20Sergej%20Averkiev%20and%20Alena%20Fenogenova%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Natural%20Language%20Processing%20%28NLP%29%20have%20fostered%20the%0Adevelopment%20of%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20solve%20an%20immense%20variety%0Aof%20tasks.%20One%20of%20the%20key%20aspects%20of%20their%20application%20is%20their%20ability%20to%20work%0Awith%20long%20text%20documents%20and%20to%20process%20long%20sequences%20of%20tokens.%20This%20has%0Acreated%20a%20demand%20for%20proper%20evaluation%20of%20long-context%20understanding.%20To%0Aaddress%20this%20need%20for%20the%20Russian%20language%2C%20we%20propose%20LIBRA%20%28Long%20Input%0ABenchmark%20for%20Russian%20Analysis%29%2C%20which%20comprises%2021%20adapted%20datasets%20to%20study%0Athe%20LLM%27s%20abilities%20to%20understand%20long%20texts%20thoroughly.%20The%20tests%20are%20divided%0Ainto%20four%20complexity%20groups%20and%20allow%20the%20evaluation%20of%20models%20across%20various%0Acontext%20lengths%20ranging%20from%204k%20up%20to%20128k%20tokens.%20We%20provide%20the%20open-source%0Adatasets%2C%20codebase%2C%20and%20public%20leaderboard%20for%20LIBRA%20to%20guide%20forthcoming%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Input%2520Benchmark%2520for%2520Russian%2520Analysis%26entry.906535625%3DIgor%2520Churin%2520and%2520Murat%2520Apishev%2520and%2520Maria%2520Tikhonova%2520and%2520Denis%2520Shevelev%2520and%2520Aydar%2520Bulatov%2520and%2520Yuri%2520Kuratov%2520and%2520Sergej%2520Averkiev%2520and%2520Alena%2520Fenogenova%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520have%2520fostered%2520the%250Adevelopment%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520that%2520can%2520solve%2520an%2520immense%2520variety%250Aof%2520tasks.%2520One%2520of%2520the%2520key%2520aspects%2520of%2520their%2520application%2520is%2520their%2520ability%2520to%2520work%250Awith%2520long%2520text%2520documents%2520and%2520to%2520process%2520long%2520sequences%2520of%2520tokens.%2520This%2520has%250Acreated%2520a%2520demand%2520for%2520proper%2520evaluation%2520of%2520long-context%2520understanding.%2520To%250Aaddress%2520this%2520need%2520for%2520the%2520Russian%2520language%252C%2520we%2520propose%2520LIBRA%2520%2528Long%2520Input%250ABenchmark%2520for%2520Russian%2520Analysis%2529%252C%2520which%2520comprises%252021%2520adapted%2520datasets%2520to%2520study%250Athe%2520LLM%2527s%2520abilities%2520to%2520understand%2520long%2520texts%2520thoroughly.%2520The%2520tests%2520are%2520divided%250Ainto%2520four%2520complexity%2520groups%2520and%2520allow%2520the%2520evaluation%2520of%2520models%2520across%2520various%250Acontext%2520lengths%2520ranging%2520from%25204k%2520up%2520to%2520128k%2520tokens.%2520We%2520provide%2520the%2520open-source%250Adatasets%252C%2520codebase%252C%2520and%2520public%2520leaderboard%2520for%2520LIBRA%2520to%2520guide%2520forthcoming%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Input%20Benchmark%20for%20Russian%20Analysis&entry.906535625=Igor%20Churin%20and%20Murat%20Apishev%20and%20Maria%20Tikhonova%20and%20Denis%20Shevelev%20and%20Aydar%20Bulatov%20and%20Yuri%20Kuratov%20and%20Sergej%20Averkiev%20and%20Alena%20Fenogenova&entry.1292438233=%20%20Recent%20advancements%20in%20Natural%20Language%20Processing%20%28NLP%29%20have%20fostered%20the%0Adevelopment%20of%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20solve%20an%20immense%20variety%0Aof%20tasks.%20One%20of%20the%20key%20aspects%20of%20their%20application%20is%20their%20ability%20to%20work%0Awith%20long%20text%20documents%20and%20to%20process%20long%20sequences%20of%20tokens.%20This%20has%0Acreated%20a%20demand%20for%20proper%20evaluation%20of%20long-context%20understanding.%20To%0Aaddress%20this%20need%20for%20the%20Russian%20language%2C%20we%20propose%20LIBRA%20%28Long%20Input%0ABenchmark%20for%20Russian%20Analysis%29%2C%20which%20comprises%2021%20adapted%20datasets%20to%20study%0Athe%20LLM%27s%20abilities%20to%20understand%20long%20texts%20thoroughly.%20The%20tests%20are%20divided%0Ainto%20four%20complexity%20groups%20and%20allow%20the%20evaluation%20of%20models%20across%20various%0Acontext%20lengths%20ranging%20from%204k%20up%20to%20128k%20tokens.%20We%20provide%20the%20open-source%0Adatasets%2C%20codebase%2C%20and%20public%20leaderboard%20for%20LIBRA%20to%20guide%20forthcoming%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02439v1&entry.124074799=Read"},
{"title": "An investigation into the causes of race bias in AI-based cine CMR\n  segmentation", "author": "Tiarna Lee and Esther Puyol-Anton and Bram Ruijsink and Sebastien Roujol and Theodore Barfoot and Shaheim Ogbomo-Harmitt and Miaojing Shi and Andrew P. King", "abstract": "  Artificial intelligence (AI) methods are being used increasingly for the\nautomated segmentation of cine cardiac magnetic resonance (CMR) imaging.\nHowever, these methods have been shown to be subject to race bias, i.e. they\nexhibit different levels of performance for different races depending on the\n(im)balance of the data used to train the AI model. In this paper we\ninvestigate the source of this bias, seeking to understand its root cause(s) so\nthat it can be effectively mitigated. We perform a series of classification and\nsegmentation experiments on short-axis cine CMR images acquired from Black and\nWhite subjects from the UK Biobank and apply AI interpretability methods to\nunderstand the results. In the classification experiments, we found that race\ncan be predicted with high accuracy from the images alone, but less accurately\nfrom ground truth segmentations, suggesting that the distributional shift\nbetween races, which is often the cause of AI bias, is mostly image-based\nrather than segmentation-based. The interpretability methods showed that most\nattention in the classification models was focused on non-heart regions, such\nas subcutaneous fat. Cropping the images tightly around the heart reduced\nclassification accuracy to around chance level. Similarly, race can be\npredicted from the latent representations of a biased segmentation model,\nsuggesting that race information is encoded in the model. Cropping images\ntightly around the heart reduced but did not eliminate segmentation bias. We\nalso investigate the influence of possible confounders on the bias observed.\n", "link": "http://arxiv.org/abs/2408.02462v1", "date": "2024-08-05", "relevancy": 1.8341, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4836}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4561}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20investigation%20into%20the%20causes%20of%20race%20bias%20in%20AI-based%20cine%20CMR%0A%20%20segmentation&body=Title%3A%20An%20investigation%20into%20the%20causes%20of%20race%20bias%20in%20AI-based%20cine%20CMR%0A%20%20segmentation%0AAuthor%3A%20Tiarna%20Lee%20and%20Esther%20Puyol-Anton%20and%20Bram%20Ruijsink%20and%20Sebastien%20Roujol%20and%20Theodore%20Barfoot%20and%20Shaheim%20Ogbomo-Harmitt%20and%20Miaojing%20Shi%20and%20Andrew%20P.%20King%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20methods%20are%20being%20used%20increasingly%20for%20the%0Aautomated%20segmentation%20of%20cine%20cardiac%20magnetic%20resonance%20%28CMR%29%20imaging.%0AHowever%2C%20these%20methods%20have%20been%20shown%20to%20be%20subject%20to%20race%20bias%2C%20i.e.%20they%0Aexhibit%20different%20levels%20of%20performance%20for%20different%20races%20depending%20on%20the%0A%28im%29balance%20of%20the%20data%20used%20to%20train%20the%20AI%20model.%20In%20this%20paper%20we%0Ainvestigate%20the%20source%20of%20this%20bias%2C%20seeking%20to%20understand%20its%20root%20cause%28s%29%20so%0Athat%20it%20can%20be%20effectively%20mitigated.%20We%20perform%20a%20series%20of%20classification%20and%0Asegmentation%20experiments%20on%20short-axis%20cine%20CMR%20images%20acquired%20from%20Black%20and%0AWhite%20subjects%20from%20the%20UK%20Biobank%20and%20apply%20AI%20interpretability%20methods%20to%0Aunderstand%20the%20results.%20In%20the%20classification%20experiments%2C%20we%20found%20that%20race%0Acan%20be%20predicted%20with%20high%20accuracy%20from%20the%20images%20alone%2C%20but%20less%20accurately%0Afrom%20ground%20truth%20segmentations%2C%20suggesting%20that%20the%20distributional%20shift%0Abetween%20races%2C%20which%20is%20often%20the%20cause%20of%20AI%20bias%2C%20is%20mostly%20image-based%0Arather%20than%20segmentation-based.%20The%20interpretability%20methods%20showed%20that%20most%0Aattention%20in%20the%20classification%20models%20was%20focused%20on%20non-heart%20regions%2C%20such%0Aas%20subcutaneous%20fat.%20Cropping%20the%20images%20tightly%20around%20the%20heart%20reduced%0Aclassification%20accuracy%20to%20around%20chance%20level.%20Similarly%2C%20race%20can%20be%0Apredicted%20from%20the%20latent%20representations%20of%20a%20biased%20segmentation%20model%2C%0Asuggesting%20that%20race%20information%20is%20encoded%20in%20the%20model.%20Cropping%20images%0Atightly%20around%20the%20heart%20reduced%20but%20did%20not%20eliminate%20segmentation%20bias.%20We%0Aalso%20investigate%20the%20influence%20of%20possible%20confounders%20on%20the%20bias%20observed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520investigation%2520into%2520the%2520causes%2520of%2520race%2520bias%2520in%2520AI-based%2520cine%2520CMR%250A%2520%2520segmentation%26entry.906535625%3DTiarna%2520Lee%2520and%2520Esther%2520Puyol-Anton%2520and%2520Bram%2520Ruijsink%2520and%2520Sebastien%2520Roujol%2520and%2520Theodore%2520Barfoot%2520and%2520Shaheim%2520Ogbomo-Harmitt%2520and%2520Miaojing%2520Shi%2520and%2520Andrew%2520P.%2520King%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520methods%2520are%2520being%2520used%2520increasingly%2520for%2520the%250Aautomated%2520segmentation%2520of%2520cine%2520cardiac%2520magnetic%2520resonance%2520%2528CMR%2529%2520imaging.%250AHowever%252C%2520these%2520methods%2520have%2520been%2520shown%2520to%2520be%2520subject%2520to%2520race%2520bias%252C%2520i.e.%2520they%250Aexhibit%2520different%2520levels%2520of%2520performance%2520for%2520different%2520races%2520depending%2520on%2520the%250A%2528im%2529balance%2520of%2520the%2520data%2520used%2520to%2520train%2520the%2520AI%2520model.%2520In%2520this%2520paper%2520we%250Ainvestigate%2520the%2520source%2520of%2520this%2520bias%252C%2520seeking%2520to%2520understand%2520its%2520root%2520cause%2528s%2529%2520so%250Athat%2520it%2520can%2520be%2520effectively%2520mitigated.%2520We%2520perform%2520a%2520series%2520of%2520classification%2520and%250Asegmentation%2520experiments%2520on%2520short-axis%2520cine%2520CMR%2520images%2520acquired%2520from%2520Black%2520and%250AWhite%2520subjects%2520from%2520the%2520UK%2520Biobank%2520and%2520apply%2520AI%2520interpretability%2520methods%2520to%250Aunderstand%2520the%2520results.%2520In%2520the%2520classification%2520experiments%252C%2520we%2520found%2520that%2520race%250Acan%2520be%2520predicted%2520with%2520high%2520accuracy%2520from%2520the%2520images%2520alone%252C%2520but%2520less%2520accurately%250Afrom%2520ground%2520truth%2520segmentations%252C%2520suggesting%2520that%2520the%2520distributional%2520shift%250Abetween%2520races%252C%2520which%2520is%2520often%2520the%2520cause%2520of%2520AI%2520bias%252C%2520is%2520mostly%2520image-based%250Arather%2520than%2520segmentation-based.%2520The%2520interpretability%2520methods%2520showed%2520that%2520most%250Aattention%2520in%2520the%2520classification%2520models%2520was%2520focused%2520on%2520non-heart%2520regions%252C%2520such%250Aas%2520subcutaneous%2520fat.%2520Cropping%2520the%2520images%2520tightly%2520around%2520the%2520heart%2520reduced%250Aclassification%2520accuracy%2520to%2520around%2520chance%2520level.%2520Similarly%252C%2520race%2520can%2520be%250Apredicted%2520from%2520the%2520latent%2520representations%2520of%2520a%2520biased%2520segmentation%2520model%252C%250Asuggesting%2520that%2520race%2520information%2520is%2520encoded%2520in%2520the%2520model.%2520Cropping%2520images%250Atightly%2520around%2520the%2520heart%2520reduced%2520but%2520did%2520not%2520eliminate%2520segmentation%2520bias.%2520We%250Aalso%2520investigate%2520the%2520influence%2520of%2520possible%2520confounders%2520on%2520the%2520bias%2520observed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20investigation%20into%20the%20causes%20of%20race%20bias%20in%20AI-based%20cine%20CMR%0A%20%20segmentation&entry.906535625=Tiarna%20Lee%20and%20Esther%20Puyol-Anton%20and%20Bram%20Ruijsink%20and%20Sebastien%20Roujol%20and%20Theodore%20Barfoot%20and%20Shaheim%20Ogbomo-Harmitt%20and%20Miaojing%20Shi%20and%20Andrew%20P.%20King&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20methods%20are%20being%20used%20increasingly%20for%20the%0Aautomated%20segmentation%20of%20cine%20cardiac%20magnetic%20resonance%20%28CMR%29%20imaging.%0AHowever%2C%20these%20methods%20have%20been%20shown%20to%20be%20subject%20to%20race%20bias%2C%20i.e.%20they%0Aexhibit%20different%20levels%20of%20performance%20for%20different%20races%20depending%20on%20the%0A%28im%29balance%20of%20the%20data%20used%20to%20train%20the%20AI%20model.%20In%20this%20paper%20we%0Ainvestigate%20the%20source%20of%20this%20bias%2C%20seeking%20to%20understand%20its%20root%20cause%28s%29%20so%0Athat%20it%20can%20be%20effectively%20mitigated.%20We%20perform%20a%20series%20of%20classification%20and%0Asegmentation%20experiments%20on%20short-axis%20cine%20CMR%20images%20acquired%20from%20Black%20and%0AWhite%20subjects%20from%20the%20UK%20Biobank%20and%20apply%20AI%20interpretability%20methods%20to%0Aunderstand%20the%20results.%20In%20the%20classification%20experiments%2C%20we%20found%20that%20race%0Acan%20be%20predicted%20with%20high%20accuracy%20from%20the%20images%20alone%2C%20but%20less%20accurately%0Afrom%20ground%20truth%20segmentations%2C%20suggesting%20that%20the%20distributional%20shift%0Abetween%20races%2C%20which%20is%20often%20the%20cause%20of%20AI%20bias%2C%20is%20mostly%20image-based%0Arather%20than%20segmentation-based.%20The%20interpretability%20methods%20showed%20that%20most%0Aattention%20in%20the%20classification%20models%20was%20focused%20on%20non-heart%20regions%2C%20such%0Aas%20subcutaneous%20fat.%20Cropping%20the%20images%20tightly%20around%20the%20heart%20reduced%0Aclassification%20accuracy%20to%20around%20chance%20level.%20Similarly%2C%20race%20can%20be%0Apredicted%20from%20the%20latent%20representations%20of%20a%20biased%20segmentation%20model%2C%0Asuggesting%20that%20race%20information%20is%20encoded%20in%20the%20model.%20Cropping%20images%0Atightly%20around%20the%20heart%20reduced%20but%20did%20not%20eliminate%20segmentation%20bias.%20We%0Aalso%20investigate%20the%20influence%20of%20possible%20confounders%20on%20the%20bias%20observed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02462v1&entry.124074799=Read"},
{"title": "A Surprisingly Efficient Representation for Multi-Finger Grasping", "author": "Hengxu Yan and Hao-Shu Fang and Cewu Lu", "abstract": "  The problem of grasping objects using a multi-finger hand has received\nsignificant attention in recent years. However, it remains challenging to\nhandle a large number of unfamiliar objects in real and cluttered environments.\nIn this work, we propose a representation that can be effectively mapped to the\nmulti-finger grasp space. Based on this representation, we develop a simple\ndecision model that generates accurate grasp quality scores for different\nmulti-finger grasp poses using only hundreds to thousands of training samples.\nWe demonstrate that our representation performs well on a real robot and\nachieves a success rate of 78.64% after training with only 500 real-world grasp\nattempts and 87% with 4500 grasp attempts. Additionally, we achieve a success\nrate of 84.51% in a dynamic human-robot handover scenario using a multi-finger\nhand.\n", "link": "http://arxiv.org/abs/2408.02455v1", "date": "2024-08-05", "relevancy": 1.8328, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5712}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Surprisingly%20Efficient%20Representation%20for%20Multi-Finger%20Grasping&body=Title%3A%20A%20Surprisingly%20Efficient%20Representation%20for%20Multi-Finger%20Grasping%0AAuthor%3A%20Hengxu%20Yan%20and%20Hao-Shu%20Fang%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20The%20problem%20of%20grasping%20objects%20using%20a%20multi-finger%20hand%20has%20received%0Asignificant%20attention%20in%20recent%20years.%20However%2C%20it%20remains%20challenging%20to%0Ahandle%20a%20large%20number%20of%20unfamiliar%20objects%20in%20real%20and%20cluttered%20environments.%0AIn%20this%20work%2C%20we%20propose%20a%20representation%20that%20can%20be%20effectively%20mapped%20to%20the%0Amulti-finger%20grasp%20space.%20Based%20on%20this%20representation%2C%20we%20develop%20a%20simple%0Adecision%20model%20that%20generates%20accurate%20grasp%20quality%20scores%20for%20different%0Amulti-finger%20grasp%20poses%20using%20only%20hundreds%20to%20thousands%20of%20training%20samples.%0AWe%20demonstrate%20that%20our%20representation%20performs%20well%20on%20a%20real%20robot%20and%0Aachieves%20a%20success%20rate%20of%2078.64%25%20after%20training%20with%20only%20500%20real-world%20grasp%0Aattempts%20and%2087%25%20with%204500%20grasp%20attempts.%20Additionally%2C%20we%20achieve%20a%20success%0Arate%20of%2084.51%25%20in%20a%20dynamic%20human-robot%20handover%20scenario%20using%20a%20multi-finger%0Ahand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Surprisingly%2520Efficient%2520Representation%2520for%2520Multi-Finger%2520Grasping%26entry.906535625%3DHengxu%2520Yan%2520and%2520Hao-Shu%2520Fang%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520grasping%2520objects%2520using%2520a%2520multi-finger%2520hand%2520has%2520received%250Asignificant%2520attention%2520in%2520recent%2520years.%2520However%252C%2520it%2520remains%2520challenging%2520to%250Ahandle%2520a%2520large%2520number%2520of%2520unfamiliar%2520objects%2520in%2520real%2520and%2520cluttered%2520environments.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520representation%2520that%2520can%2520be%2520effectively%2520mapped%2520to%2520the%250Amulti-finger%2520grasp%2520space.%2520Based%2520on%2520this%2520representation%252C%2520we%2520develop%2520a%2520simple%250Adecision%2520model%2520that%2520generates%2520accurate%2520grasp%2520quality%2520scores%2520for%2520different%250Amulti-finger%2520grasp%2520poses%2520using%2520only%2520hundreds%2520to%2520thousands%2520of%2520training%2520samples.%250AWe%2520demonstrate%2520that%2520our%2520representation%2520performs%2520well%2520on%2520a%2520real%2520robot%2520and%250Aachieves%2520a%2520success%2520rate%2520of%252078.64%2525%2520after%2520training%2520with%2520only%2520500%2520real-world%2520grasp%250Aattempts%2520and%252087%2525%2520with%25204500%2520grasp%2520attempts.%2520Additionally%252C%2520we%2520achieve%2520a%2520success%250Arate%2520of%252084.51%2525%2520in%2520a%2520dynamic%2520human-robot%2520handover%2520scenario%2520using%2520a%2520multi-finger%250Ahand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Surprisingly%20Efficient%20Representation%20for%20Multi-Finger%20Grasping&entry.906535625=Hengxu%20Yan%20and%20Hao-Shu%20Fang%20and%20Cewu%20Lu&entry.1292438233=%20%20The%20problem%20of%20grasping%20objects%20using%20a%20multi-finger%20hand%20has%20received%0Asignificant%20attention%20in%20recent%20years.%20However%2C%20it%20remains%20challenging%20to%0Ahandle%20a%20large%20number%20of%20unfamiliar%20objects%20in%20real%20and%20cluttered%20environments.%0AIn%20this%20work%2C%20we%20propose%20a%20representation%20that%20can%20be%20effectively%20mapped%20to%20the%0Amulti-finger%20grasp%20space.%20Based%20on%20this%20representation%2C%20we%20develop%20a%20simple%0Adecision%20model%20that%20generates%20accurate%20grasp%20quality%20scores%20for%20different%0Amulti-finger%20grasp%20poses%20using%20only%20hundreds%20to%20thousands%20of%20training%20samples.%0AWe%20demonstrate%20that%20our%20representation%20performs%20well%20on%20a%20real%20robot%20and%0Aachieves%20a%20success%20rate%20of%2078.64%25%20after%20training%20with%20only%20500%20real-world%20grasp%0Aattempts%20and%2087%25%20with%204500%20grasp%20attempts.%20Additionally%2C%20we%20achieve%20a%20success%0Arate%20of%2084.51%25%20in%20a%20dynamic%20human-robot%20handover%20scenario%20using%20a%20multi-finger%0Ahand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02455v1&entry.124074799=Read"},
{"title": "TGS: Trajectory Generation and Selection using Vision Language Models in\n  Mapless Outdoor Environments", "author": "Daeun Song and Jing Liang and Xuesu Xiao and Dinesh Manocha", "abstract": "  We present a multi-modal trajectory generation and selection algorithm for\nreal-world mapless outdoor navigation in challenging scenarios with\nunstructured off-road features like buildings, grass, and curbs. Our goal is to\ncompute suitable trajectories that (1) satisfy the environment-specific\ntraversability constraints and (2) match human-like paths while navigating in\ncrosswalks, sidewalks, etc. Our formulation uses a Conditional Variational\nAutoencoder (CVAE) generative model enhanced with traversability constraints to\ngenerate multiple candidate trajectories for global navigation. We use VLMs and\na visual prompting approach with their zero-shot ability of semantic\nunderstanding and logical reasoning to choose the best trajectory given the\ncontextual information about the task. We evaluate our methods in various\noutdoor scenes with wheeled robots and compare the performance with other\nglobal navigation algorithms. In practice, we observe at least 3.35%\nimprovement in the traversability and 20.61% improvement in terms of human-like\nnavigation in generated trajectories in challenging outdoor navigation\nscenarios, such as sidewalks, crosswalks, etc.\n", "link": "http://arxiv.org/abs/2408.02454v1", "date": "2024-08-05", "relevancy": 1.7962, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6175}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6025}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TGS%3A%20Trajectory%20Generation%20and%20Selection%20using%20Vision%20Language%20Models%20in%0A%20%20Mapless%20Outdoor%20Environments&body=Title%3A%20TGS%3A%20Trajectory%20Generation%20and%20Selection%20using%20Vision%20Language%20Models%20in%0A%20%20Mapless%20Outdoor%20Environments%0AAuthor%3A%20Daeun%20Song%20and%20Jing%20Liang%20and%20Xuesu%20Xiao%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20a%20multi-modal%20trajectory%20generation%20and%20selection%20algorithm%20for%0Areal-world%20mapless%20outdoor%20navigation%20in%20challenging%20scenarios%20with%0Aunstructured%20off-road%20features%20like%20buildings%2C%20grass%2C%20and%20curbs.%20Our%20goal%20is%20to%0Acompute%20suitable%20trajectories%20that%20%281%29%20satisfy%20the%20environment-specific%0Atraversability%20constraints%20and%20%282%29%20match%20human-like%20paths%20while%20navigating%20in%0Acrosswalks%2C%20sidewalks%2C%20etc.%20Our%20formulation%20uses%20a%20Conditional%20Variational%0AAutoencoder%20%28CVAE%29%20generative%20model%20enhanced%20with%20traversability%20constraints%20to%0Agenerate%20multiple%20candidate%20trajectories%20for%20global%20navigation.%20We%20use%20VLMs%20and%0Aa%20visual%20prompting%20approach%20with%20their%20zero-shot%20ability%20of%20semantic%0Aunderstanding%20and%20logical%20reasoning%20to%20choose%20the%20best%20trajectory%20given%20the%0Acontextual%20information%20about%20the%20task.%20We%20evaluate%20our%20methods%20in%20various%0Aoutdoor%20scenes%20with%20wheeled%20robots%20and%20compare%20the%20performance%20with%20other%0Aglobal%20navigation%20algorithms.%20In%20practice%2C%20we%20observe%20at%20least%203.35%25%0Aimprovement%20in%20the%20traversability%20and%2020.61%25%20improvement%20in%20terms%20of%20human-like%0Anavigation%20in%20generated%20trajectories%20in%20challenging%20outdoor%20navigation%0Ascenarios%2C%20such%20as%20sidewalks%2C%20crosswalks%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTGS%253A%2520Trajectory%2520Generation%2520and%2520Selection%2520using%2520Vision%2520Language%2520Models%2520in%250A%2520%2520Mapless%2520Outdoor%2520Environments%26entry.906535625%3DDaeun%2520Song%2520and%2520Jing%2520Liang%2520and%2520Xuesu%2520Xiao%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520multi-modal%2520trajectory%2520generation%2520and%2520selection%2520algorithm%2520for%250Areal-world%2520mapless%2520outdoor%2520navigation%2520in%2520challenging%2520scenarios%2520with%250Aunstructured%2520off-road%2520features%2520like%2520buildings%252C%2520grass%252C%2520and%2520curbs.%2520Our%2520goal%2520is%2520to%250Acompute%2520suitable%2520trajectories%2520that%2520%25281%2529%2520satisfy%2520the%2520environment-specific%250Atraversability%2520constraints%2520and%2520%25282%2529%2520match%2520human-like%2520paths%2520while%2520navigating%2520in%250Acrosswalks%252C%2520sidewalks%252C%2520etc.%2520Our%2520formulation%2520uses%2520a%2520Conditional%2520Variational%250AAutoencoder%2520%2528CVAE%2529%2520generative%2520model%2520enhanced%2520with%2520traversability%2520constraints%2520to%250Agenerate%2520multiple%2520candidate%2520trajectories%2520for%2520global%2520navigation.%2520We%2520use%2520VLMs%2520and%250Aa%2520visual%2520prompting%2520approach%2520with%2520their%2520zero-shot%2520ability%2520of%2520semantic%250Aunderstanding%2520and%2520logical%2520reasoning%2520to%2520choose%2520the%2520best%2520trajectory%2520given%2520the%250Acontextual%2520information%2520about%2520the%2520task.%2520We%2520evaluate%2520our%2520methods%2520in%2520various%250Aoutdoor%2520scenes%2520with%2520wheeled%2520robots%2520and%2520compare%2520the%2520performance%2520with%2520other%250Aglobal%2520navigation%2520algorithms.%2520In%2520practice%252C%2520we%2520observe%2520at%2520least%25203.35%2525%250Aimprovement%2520in%2520the%2520traversability%2520and%252020.61%2525%2520improvement%2520in%2520terms%2520of%2520human-like%250Anavigation%2520in%2520generated%2520trajectories%2520in%2520challenging%2520outdoor%2520navigation%250Ascenarios%252C%2520such%2520as%2520sidewalks%252C%2520crosswalks%252C%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TGS%3A%20Trajectory%20Generation%20and%20Selection%20using%20Vision%20Language%20Models%20in%0A%20%20Mapless%20Outdoor%20Environments&entry.906535625=Daeun%20Song%20and%20Jing%20Liang%20and%20Xuesu%20Xiao%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20a%20multi-modal%20trajectory%20generation%20and%20selection%20algorithm%20for%0Areal-world%20mapless%20outdoor%20navigation%20in%20challenging%20scenarios%20with%0Aunstructured%20off-road%20features%20like%20buildings%2C%20grass%2C%20and%20curbs.%20Our%20goal%20is%20to%0Acompute%20suitable%20trajectories%20that%20%281%29%20satisfy%20the%20environment-specific%0Atraversability%20constraints%20and%20%282%29%20match%20human-like%20paths%20while%20navigating%20in%0Acrosswalks%2C%20sidewalks%2C%20etc.%20Our%20formulation%20uses%20a%20Conditional%20Variational%0AAutoencoder%20%28CVAE%29%20generative%20model%20enhanced%20with%20traversability%20constraints%20to%0Agenerate%20multiple%20candidate%20trajectories%20for%20global%20navigation.%20We%20use%20VLMs%20and%0Aa%20visual%20prompting%20approach%20with%20their%20zero-shot%20ability%20of%20semantic%0Aunderstanding%20and%20logical%20reasoning%20to%20choose%20the%20best%20trajectory%20given%20the%0Acontextual%20information%20about%20the%20task.%20We%20evaluate%20our%20methods%20in%20various%0Aoutdoor%20scenes%20with%20wheeled%20robots%20and%20compare%20the%20performance%20with%20other%0Aglobal%20navigation%20algorithms.%20In%20practice%2C%20we%20observe%20at%20least%203.35%25%0Aimprovement%20in%20the%20traversability%20and%2020.61%25%20improvement%20in%20terms%20of%20human-like%0Anavigation%20in%20generated%20trajectories%20in%20challenging%20outdoor%20navigation%0Ascenarios%2C%20such%20as%20sidewalks%2C%20crosswalks%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02454v1&entry.124074799=Read"},
{"title": "LOG-LIO2: A LiDAR-Inertial Odometry with Efficient Uncertainty Analysis", "author": "Kai Huang and Junqiao Zhao and Jiaye Lin and Zhongyang Zhu and Shuangfu Song and Chen Ye and Tiantian Feng", "abstract": "  Uncertainty in LiDAR measurements, stemming from factors such as range\nsensing, is crucial for LIO (LiDAR-Inertial Odometry) systems as it affects the\naccurate weighting in the loss function. While recent LIO systems address\nuncertainty related to range sensing, the impact of incident angle on\nuncertainty is often overlooked by the community. Moreover, the existing\nuncertainty propagation methods suffer from computational inefficiency. This\npaper proposes a comprehensive point uncertainty model that accounts for both\nthe uncertainties from LiDAR measurements and surface characteristics, along\nwith an efficient local uncertainty analytical method for LiDAR-based state\nestimation problem. We employ a projection operator that separates the\nuncertainty into the ray direction and its orthogonal plane. Then, we derive\nincremental Jacobian matrices of eigenvalues and eigenvectors w.r.t. points,\nwhich enables a fast approximation of uncertainty propagation. This approach\neliminates the requirement for redundant traversal of points, significantly\nreducing the time complexity of uncertainty propagation from $\\mathcal{O} (n)$\nto $\\mathcal{O} (1)$ when a new point is added. Simulations and experiments on\npublic datasets are conducted to validate the accuracy and efficiency of our\nformulations. The proposed methods have been integrated into a LIO system,\nwhich is available at https://github.com/tiev-tongji/LOG-LIO2.\n", "link": "http://arxiv.org/abs/2405.01316v2", "date": "2024-08-05", "relevancy": 1.792, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6237}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOG-LIO2%3A%20A%20LiDAR-Inertial%20Odometry%20with%20Efficient%20Uncertainty%20Analysis&body=Title%3A%20LOG-LIO2%3A%20A%20LiDAR-Inertial%20Odometry%20with%20Efficient%20Uncertainty%20Analysis%0AAuthor%3A%20Kai%20Huang%20and%20Junqiao%20Zhao%20and%20Jiaye%20Lin%20and%20Zhongyang%20Zhu%20and%20Shuangfu%20Song%20and%20Chen%20Ye%20and%20Tiantian%20Feng%0AAbstract%3A%20%20%20Uncertainty%20in%20LiDAR%20measurements%2C%20stemming%20from%20factors%20such%20as%20range%0Asensing%2C%20is%20crucial%20for%20LIO%20%28LiDAR-Inertial%20Odometry%29%20systems%20as%20it%20affects%20the%0Aaccurate%20weighting%20in%20the%20loss%20function.%20While%20recent%20LIO%20systems%20address%0Auncertainty%20related%20to%20range%20sensing%2C%20the%20impact%20of%20incident%20angle%20on%0Auncertainty%20is%20often%20overlooked%20by%20the%20community.%20Moreover%2C%20the%20existing%0Auncertainty%20propagation%20methods%20suffer%20from%20computational%20inefficiency.%20This%0Apaper%20proposes%20a%20comprehensive%20point%20uncertainty%20model%20that%20accounts%20for%20both%0Athe%20uncertainties%20from%20LiDAR%20measurements%20and%20surface%20characteristics%2C%20along%0Awith%20an%20efficient%20local%20uncertainty%20analytical%20method%20for%20LiDAR-based%20state%0Aestimation%20problem.%20We%20employ%20a%20projection%20operator%20that%20separates%20the%0Auncertainty%20into%20the%20ray%20direction%20and%20its%20orthogonal%20plane.%20Then%2C%20we%20derive%0Aincremental%20Jacobian%20matrices%20of%20eigenvalues%20and%20eigenvectors%20w.r.t.%20points%2C%0Awhich%20enables%20a%20fast%20approximation%20of%20uncertainty%20propagation.%20This%20approach%0Aeliminates%20the%20requirement%20for%20redundant%20traversal%20of%20points%2C%20significantly%0Areducing%20the%20time%20complexity%20of%20uncertainty%20propagation%20from%20%24%5Cmathcal%7BO%7D%20%28n%29%24%0Ato%20%24%5Cmathcal%7BO%7D%20%281%29%24%20when%20a%20new%20point%20is%20added.%20Simulations%20and%20experiments%20on%0Apublic%20datasets%20are%20conducted%20to%20validate%20the%20accuracy%20and%20efficiency%20of%20our%0Aformulations.%20The%20proposed%20methods%20have%20been%20integrated%20into%20a%20LIO%20system%2C%0Awhich%20is%20available%20at%20https%3A//github.com/tiev-tongji/LOG-LIO2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOG-LIO2%253A%2520A%2520LiDAR-Inertial%2520Odometry%2520with%2520Efficient%2520Uncertainty%2520Analysis%26entry.906535625%3DKai%2520Huang%2520and%2520Junqiao%2520Zhao%2520and%2520Jiaye%2520Lin%2520and%2520Zhongyang%2520Zhu%2520and%2520Shuangfu%2520Song%2520and%2520Chen%2520Ye%2520and%2520Tiantian%2520Feng%26entry.1292438233%3D%2520%2520Uncertainty%2520in%2520LiDAR%2520measurements%252C%2520stemming%2520from%2520factors%2520such%2520as%2520range%250Asensing%252C%2520is%2520crucial%2520for%2520LIO%2520%2528LiDAR-Inertial%2520Odometry%2529%2520systems%2520as%2520it%2520affects%2520the%250Aaccurate%2520weighting%2520in%2520the%2520loss%2520function.%2520While%2520recent%2520LIO%2520systems%2520address%250Auncertainty%2520related%2520to%2520range%2520sensing%252C%2520the%2520impact%2520of%2520incident%2520angle%2520on%250Auncertainty%2520is%2520often%2520overlooked%2520by%2520the%2520community.%2520Moreover%252C%2520the%2520existing%250Auncertainty%2520propagation%2520methods%2520suffer%2520from%2520computational%2520inefficiency.%2520This%250Apaper%2520proposes%2520a%2520comprehensive%2520point%2520uncertainty%2520model%2520that%2520accounts%2520for%2520both%250Athe%2520uncertainties%2520from%2520LiDAR%2520measurements%2520and%2520surface%2520characteristics%252C%2520along%250Awith%2520an%2520efficient%2520local%2520uncertainty%2520analytical%2520method%2520for%2520LiDAR-based%2520state%250Aestimation%2520problem.%2520We%2520employ%2520a%2520projection%2520operator%2520that%2520separates%2520the%250Auncertainty%2520into%2520the%2520ray%2520direction%2520and%2520its%2520orthogonal%2520plane.%2520Then%252C%2520we%2520derive%250Aincremental%2520Jacobian%2520matrices%2520of%2520eigenvalues%2520and%2520eigenvectors%2520w.r.t.%2520points%252C%250Awhich%2520enables%2520a%2520fast%2520approximation%2520of%2520uncertainty%2520propagation.%2520This%2520approach%250Aeliminates%2520the%2520requirement%2520for%2520redundant%2520traversal%2520of%2520points%252C%2520significantly%250Areducing%2520the%2520time%2520complexity%2520of%2520uncertainty%2520propagation%2520from%2520%2524%255Cmathcal%257BO%257D%2520%2528n%2529%2524%250Ato%2520%2524%255Cmathcal%257BO%257D%2520%25281%2529%2524%2520when%2520a%2520new%2520point%2520is%2520added.%2520Simulations%2520and%2520experiments%2520on%250Apublic%2520datasets%2520are%2520conducted%2520to%2520validate%2520the%2520accuracy%2520and%2520efficiency%2520of%2520our%250Aformulations.%2520The%2520proposed%2520methods%2520have%2520been%2520integrated%2520into%2520a%2520LIO%2520system%252C%250Awhich%2520is%2520available%2520at%2520https%253A//github.com/tiev-tongji/LOG-LIO2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOG-LIO2%3A%20A%20LiDAR-Inertial%20Odometry%20with%20Efficient%20Uncertainty%20Analysis&entry.906535625=Kai%20Huang%20and%20Junqiao%20Zhao%20and%20Jiaye%20Lin%20and%20Zhongyang%20Zhu%20and%20Shuangfu%20Song%20and%20Chen%20Ye%20and%20Tiantian%20Feng&entry.1292438233=%20%20Uncertainty%20in%20LiDAR%20measurements%2C%20stemming%20from%20factors%20such%20as%20range%0Asensing%2C%20is%20crucial%20for%20LIO%20%28LiDAR-Inertial%20Odometry%29%20systems%20as%20it%20affects%20the%0Aaccurate%20weighting%20in%20the%20loss%20function.%20While%20recent%20LIO%20systems%20address%0Auncertainty%20related%20to%20range%20sensing%2C%20the%20impact%20of%20incident%20angle%20on%0Auncertainty%20is%20often%20overlooked%20by%20the%20community.%20Moreover%2C%20the%20existing%0Auncertainty%20propagation%20methods%20suffer%20from%20computational%20inefficiency.%20This%0Apaper%20proposes%20a%20comprehensive%20point%20uncertainty%20model%20that%20accounts%20for%20both%0Athe%20uncertainties%20from%20LiDAR%20measurements%20and%20surface%20characteristics%2C%20along%0Awith%20an%20efficient%20local%20uncertainty%20analytical%20method%20for%20LiDAR-based%20state%0Aestimation%20problem.%20We%20employ%20a%20projection%20operator%20that%20separates%20the%0Auncertainty%20into%20the%20ray%20direction%20and%20its%20orthogonal%20plane.%20Then%2C%20we%20derive%0Aincremental%20Jacobian%20matrices%20of%20eigenvalues%20and%20eigenvectors%20w.r.t.%20points%2C%0Awhich%20enables%20a%20fast%20approximation%20of%20uncertainty%20propagation.%20This%20approach%0Aeliminates%20the%20requirement%20for%20redundant%20traversal%20of%20points%2C%20significantly%0Areducing%20the%20time%20complexity%20of%20uncertainty%20propagation%20from%20%24%5Cmathcal%7BO%7D%20%28n%29%24%0Ato%20%24%5Cmathcal%7BO%7D%20%281%29%24%20when%20a%20new%20point%20is%20added.%20Simulations%20and%20experiments%20on%0Apublic%20datasets%20are%20conducted%20to%20validate%20the%20accuracy%20and%20efficiency%20of%20our%0Aformulations.%20The%20proposed%20methods%20have%20been%20integrated%20into%20a%20LIO%20system%2C%0Awhich%20is%20available%20at%20https%3A//github.com/tiev-tongji/LOG-LIO2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01316v2&entry.124074799=Read"},
{"title": "Revolutionizing Urban Safety Perception Assessments: Integrating\n  Multimodal Large Language Models with Street View Images", "author": "Jiaxin Zhang and Yunqin Li and Tomohiro Fukuda and Bowen Wang", "abstract": "  Measuring urban safety perception is an important and complex task that\ntraditionally relies heavily on human resources. This process often involves\nextensive field surveys, manual data collection, and subjective assessments,\nwhich can be time-consuming, costly, and sometimes inconsistent. Street View\nImages (SVIs), along with deep learning methods, provide a way to realize\nlarge-scale urban safety detection. However, achieving this goal often requires\nextensive human annotation to train safety ranking models, and the\narchitectural differences between cities hinder the transferability of these\nmodels. Thus, a fully automated method for conducting safety evaluations is\nessential. Recent advances in multimodal large language models (MLLMs) have\ndemonstrated powerful reasoning and analytical capabilities. Cutting-edge\nmodels, e.g., GPT-4 have shown surprising performance in many tasks. We\nemployed these models for urban safety ranking on a human-annotated anchor set\nand validated that the results from MLLMs align closely with human perceptions.\nAdditionally, we proposed a method based on the pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) feature and K-Nearest Neighbors (K-NN)\nretrieval to quickly assess the safety index of the entire city. Experimental\nresults show that our method outperforms existing training needed deep learning\napproaches, achieving efficient and accurate urban safety evaluations. The\nproposed automation for urban safety perception assessment is a valuable tool\nfor city planners, policymakers, and researchers aiming to improve urban\nenvironments.\n", "link": "http://arxiv.org/abs/2407.19719v2", "date": "2024-08-05", "relevancy": 1.7675, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5932}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5923}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revolutionizing%20Urban%20Safety%20Perception%20Assessments%3A%20Integrating%0A%20%20Multimodal%20Large%20Language%20Models%20with%20Street%20View%20Images&body=Title%3A%20Revolutionizing%20Urban%20Safety%20Perception%20Assessments%3A%20Integrating%0A%20%20Multimodal%20Large%20Language%20Models%20with%20Street%20View%20Images%0AAuthor%3A%20Jiaxin%20Zhang%20and%20Yunqin%20Li%20and%20Tomohiro%20Fukuda%20and%20Bowen%20Wang%0AAbstract%3A%20%20%20Measuring%20urban%20safety%20perception%20is%20an%20important%20and%20complex%20task%20that%0Atraditionally%20relies%20heavily%20on%20human%20resources.%20This%20process%20often%20involves%0Aextensive%20field%20surveys%2C%20manual%20data%20collection%2C%20and%20subjective%20assessments%2C%0Awhich%20can%20be%20time-consuming%2C%20costly%2C%20and%20sometimes%20inconsistent.%20Street%20View%0AImages%20%28SVIs%29%2C%20along%20with%20deep%20learning%20methods%2C%20provide%20a%20way%20to%20realize%0Alarge-scale%20urban%20safety%20detection.%20However%2C%20achieving%20this%20goal%20often%20requires%0Aextensive%20human%20annotation%20to%20train%20safety%20ranking%20models%2C%20and%20the%0Aarchitectural%20differences%20between%20cities%20hinder%20the%20transferability%20of%20these%0Amodels.%20Thus%2C%20a%20fully%20automated%20method%20for%20conducting%20safety%20evaluations%20is%0Aessential.%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20powerful%20reasoning%20and%20analytical%20capabilities.%20Cutting-edge%0Amodels%2C%20e.g.%2C%20GPT-4%20have%20shown%20surprising%20performance%20in%20many%20tasks.%20We%0Aemployed%20these%20models%20for%20urban%20safety%20ranking%20on%20a%20human-annotated%20anchor%20set%0Aand%20validated%20that%20the%20results%20from%20MLLMs%20align%20closely%20with%20human%20perceptions.%0AAdditionally%2C%20we%20proposed%20a%20method%20based%20on%20the%20pre-trained%20Contrastive%0ALanguage-Image%20Pre-training%20%28CLIP%29%20feature%20and%20K-Nearest%20Neighbors%20%28K-NN%29%0Aretrieval%20to%20quickly%20assess%20the%20safety%20index%20of%20the%20entire%20city.%20Experimental%0Aresults%20show%20that%20our%20method%20outperforms%20existing%20training%20needed%20deep%20learning%0Aapproaches%2C%20achieving%20efficient%20and%20accurate%20urban%20safety%20evaluations.%20The%0Aproposed%20automation%20for%20urban%20safety%20perception%20assessment%20is%20a%20valuable%20tool%0Afor%20city%20planners%2C%20policymakers%2C%20and%20researchers%20aiming%20to%20improve%20urban%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19719v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevolutionizing%2520Urban%2520Safety%2520Perception%2520Assessments%253A%2520Integrating%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%2520with%2520Street%2520View%2520Images%26entry.906535625%3DJiaxin%2520Zhang%2520and%2520Yunqin%2520Li%2520and%2520Tomohiro%2520Fukuda%2520and%2520Bowen%2520Wang%26entry.1292438233%3D%2520%2520Measuring%2520urban%2520safety%2520perception%2520is%2520an%2520important%2520and%2520complex%2520task%2520that%250Atraditionally%2520relies%2520heavily%2520on%2520human%2520resources.%2520This%2520process%2520often%2520involves%250Aextensive%2520field%2520surveys%252C%2520manual%2520data%2520collection%252C%2520and%2520subjective%2520assessments%252C%250Awhich%2520can%2520be%2520time-consuming%252C%2520costly%252C%2520and%2520sometimes%2520inconsistent.%2520Street%2520View%250AImages%2520%2528SVIs%2529%252C%2520along%2520with%2520deep%2520learning%2520methods%252C%2520provide%2520a%2520way%2520to%2520realize%250Alarge-scale%2520urban%2520safety%2520detection.%2520However%252C%2520achieving%2520this%2520goal%2520often%2520requires%250Aextensive%2520human%2520annotation%2520to%2520train%2520safety%2520ranking%2520models%252C%2520and%2520the%250Aarchitectural%2520differences%2520between%2520cities%2520hinder%2520the%2520transferability%2520of%2520these%250Amodels.%2520Thus%252C%2520a%2520fully%2520automated%2520method%2520for%2520conducting%2520safety%2520evaluations%2520is%250Aessential.%2520Recent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%250Ademonstrated%2520powerful%2520reasoning%2520and%2520analytical%2520capabilities.%2520Cutting-edge%250Amodels%252C%2520e.g.%252C%2520GPT-4%2520have%2520shown%2520surprising%2520performance%2520in%2520many%2520tasks.%2520We%250Aemployed%2520these%2520models%2520for%2520urban%2520safety%2520ranking%2520on%2520a%2520human-annotated%2520anchor%2520set%250Aand%2520validated%2520that%2520the%2520results%2520from%2520MLLMs%2520align%2520closely%2520with%2520human%2520perceptions.%250AAdditionally%252C%2520we%2520proposed%2520a%2520method%2520based%2520on%2520the%2520pre-trained%2520Contrastive%250ALanguage-Image%2520Pre-training%2520%2528CLIP%2529%2520feature%2520and%2520K-Nearest%2520Neighbors%2520%2528K-NN%2529%250Aretrieval%2520to%2520quickly%2520assess%2520the%2520safety%2520index%2520of%2520the%2520entire%2520city.%2520Experimental%250Aresults%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520training%2520needed%2520deep%2520learning%250Aapproaches%252C%2520achieving%2520efficient%2520and%2520accurate%2520urban%2520safety%2520evaluations.%2520The%250Aproposed%2520automation%2520for%2520urban%2520safety%2520perception%2520assessment%2520is%2520a%2520valuable%2520tool%250Afor%2520city%2520planners%252C%2520policymakers%252C%2520and%2520researchers%2520aiming%2520to%2520improve%2520urban%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19719v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revolutionizing%20Urban%20Safety%20Perception%20Assessments%3A%20Integrating%0A%20%20Multimodal%20Large%20Language%20Models%20with%20Street%20View%20Images&entry.906535625=Jiaxin%20Zhang%20and%20Yunqin%20Li%20and%20Tomohiro%20Fukuda%20and%20Bowen%20Wang&entry.1292438233=%20%20Measuring%20urban%20safety%20perception%20is%20an%20important%20and%20complex%20task%20that%0Atraditionally%20relies%20heavily%20on%20human%20resources.%20This%20process%20often%20involves%0Aextensive%20field%20surveys%2C%20manual%20data%20collection%2C%20and%20subjective%20assessments%2C%0Awhich%20can%20be%20time-consuming%2C%20costly%2C%20and%20sometimes%20inconsistent.%20Street%20View%0AImages%20%28SVIs%29%2C%20along%20with%20deep%20learning%20methods%2C%20provide%20a%20way%20to%20realize%0Alarge-scale%20urban%20safety%20detection.%20However%2C%20achieving%20this%20goal%20often%20requires%0Aextensive%20human%20annotation%20to%20train%20safety%20ranking%20models%2C%20and%20the%0Aarchitectural%20differences%20between%20cities%20hinder%20the%20transferability%20of%20these%0Amodels.%20Thus%2C%20a%20fully%20automated%20method%20for%20conducting%20safety%20evaluations%20is%0Aessential.%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20powerful%20reasoning%20and%20analytical%20capabilities.%20Cutting-edge%0Amodels%2C%20e.g.%2C%20GPT-4%20have%20shown%20surprising%20performance%20in%20many%20tasks.%20We%0Aemployed%20these%20models%20for%20urban%20safety%20ranking%20on%20a%20human-annotated%20anchor%20set%0Aand%20validated%20that%20the%20results%20from%20MLLMs%20align%20closely%20with%20human%20perceptions.%0AAdditionally%2C%20we%20proposed%20a%20method%20based%20on%20the%20pre-trained%20Contrastive%0ALanguage-Image%20Pre-training%20%28CLIP%29%20feature%20and%20K-Nearest%20Neighbors%20%28K-NN%29%0Aretrieval%20to%20quickly%20assess%20the%20safety%20index%20of%20the%20entire%20city.%20Experimental%0Aresults%20show%20that%20our%20method%20outperforms%20existing%20training%20needed%20deep%20learning%0Aapproaches%2C%20achieving%20efficient%20and%20accurate%20urban%20safety%20evaluations.%20The%0Aproposed%20automation%20for%20urban%20safety%20perception%20assessment%20is%20a%20valuable%20tool%0Afor%20city%20planners%2C%20policymakers%2C%20and%20researchers%20aiming%20to%20improve%20urban%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19719v2&entry.124074799=Read"},
{"title": "Language Model Can Listen While Speaking", "author": "Ziyang Ma and Yakun Song and Chenpeng Du and Jian Cong and Zhuo Chen and Yuping Wang and Yuxuan Wang and Xie Chen", "abstract": "  Dialogue serves as the most natural manner of human-computer interaction\n(HCI). Recent advancements in speech language models (SLM) have significantly\nenhanced speech-based conversational AI. However, these models are limited to\nturn-based conversation, lacking the ability to interact with humans in\nreal-time spoken scenarios, for example, being interrupted when the generated\ncontent is not satisfactory. To address these limitations, we explore full\nduplex modeling (FDM) in interactive speech language models (iSLM), focusing on\nenhancing real-time interaction and, more explicitly, exploring the\nquintessential ability of interruption. We introduce a novel model design,\nnamely listening-while-speaking language model (LSLM), an end-to-end system\nequipped with both listening and speaking channels. Our LSLM employs a\ntoken-based decoder-only TTS for speech generation and a streaming\nself-supervised learning (SSL) encoder for real-time audio input. LSLM fuses\nboth channels for autoregressive generation and detects turn-taking in real\ntime. Three fusion strategies -- early fusion, middle fusion, and late fusion\n-- are explored, with middle fusion achieving an optimal balance between speech\ngeneration and real-time interaction. Two experimental settings, command-based\nFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity\nto diverse instructions. Our results highlight LSLM's capability to achieve\nduplex communication with minimal impact on existing systems. This study aims\nto advance the development of interactive speech dialogue systems, enhancing\ntheir applicability in real-world contexts.\n", "link": "http://arxiv.org/abs/2408.02622v1", "date": "2024-08-05", "relevancy": 1.7657, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4604}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4312}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Model%20Can%20Listen%20While%20Speaking&body=Title%3A%20Language%20Model%20Can%20Listen%20While%20Speaking%0AAuthor%3A%20Ziyang%20Ma%20and%20Yakun%20Song%20and%20Chenpeng%20Du%20and%20Jian%20Cong%20and%20Zhuo%20Chen%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Xie%20Chen%0AAbstract%3A%20%20%20Dialogue%20serves%20as%20the%20most%20natural%20manner%20of%20human-computer%20interaction%0A%28HCI%29.%20Recent%20advancements%20in%20speech%20language%20models%20%28SLM%29%20have%20significantly%0Aenhanced%20speech-based%20conversational%20AI.%20However%2C%20these%20models%20are%20limited%20to%0Aturn-based%20conversation%2C%20lacking%20the%20ability%20to%20interact%20with%20humans%20in%0Areal-time%20spoken%20scenarios%2C%20for%20example%2C%20being%20interrupted%20when%20the%20generated%0Acontent%20is%20not%20satisfactory.%20To%20address%20these%20limitations%2C%20we%20explore%20full%0Aduplex%20modeling%20%28FDM%29%20in%20interactive%20speech%20language%20models%20%28iSLM%29%2C%20focusing%20on%0Aenhancing%20real-time%20interaction%20and%2C%20more%20explicitly%2C%20exploring%20the%0Aquintessential%20ability%20of%20interruption.%20We%20introduce%20a%20novel%20model%20design%2C%0Anamely%20listening-while-speaking%20language%20model%20%28LSLM%29%2C%20an%20end-to-end%20system%0Aequipped%20with%20both%20listening%20and%20speaking%20channels.%20Our%20LSLM%20employs%20a%0Atoken-based%20decoder-only%20TTS%20for%20speech%20generation%20and%20a%20streaming%0Aself-supervised%20learning%20%28SSL%29%20encoder%20for%20real-time%20audio%20input.%20LSLM%20fuses%0Aboth%20channels%20for%20autoregressive%20generation%20and%20detects%20turn-taking%20in%20real%0Atime.%20Three%20fusion%20strategies%20--%20early%20fusion%2C%20middle%20fusion%2C%20and%20late%20fusion%0A--%20are%20explored%2C%20with%20middle%20fusion%20achieving%20an%20optimal%20balance%20between%20speech%0Ageneration%20and%20real-time%20interaction.%20Two%20experimental%20settings%2C%20command-based%0AFDM%20and%20voice-based%20FDM%2C%20demonstrate%20LSLM%27s%20robustness%20to%20noise%20and%20sensitivity%0Ato%20diverse%20instructions.%20Our%20results%20highlight%20LSLM%27s%20capability%20to%20achieve%0Aduplex%20communication%20with%20minimal%20impact%20on%20existing%20systems.%20This%20study%20aims%0Ato%20advance%20the%20development%20of%20interactive%20speech%20dialogue%20systems%2C%20enhancing%0Atheir%20applicability%20in%20real-world%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Model%2520Can%2520Listen%2520While%2520Speaking%26entry.906535625%3DZiyang%2520Ma%2520and%2520Yakun%2520Song%2520and%2520Chenpeng%2520Du%2520and%2520Jian%2520Cong%2520and%2520Zhuo%2520Chen%2520and%2520Yuping%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Xie%2520Chen%26entry.1292438233%3D%2520%2520Dialogue%2520serves%2520as%2520the%2520most%2520natural%2520manner%2520of%2520human-computer%2520interaction%250A%2528HCI%2529.%2520Recent%2520advancements%2520in%2520speech%2520language%2520models%2520%2528SLM%2529%2520have%2520significantly%250Aenhanced%2520speech-based%2520conversational%2520AI.%2520However%252C%2520these%2520models%2520are%2520limited%2520to%250Aturn-based%2520conversation%252C%2520lacking%2520the%2520ability%2520to%2520interact%2520with%2520humans%2520in%250Areal-time%2520spoken%2520scenarios%252C%2520for%2520example%252C%2520being%2520interrupted%2520when%2520the%2520generated%250Acontent%2520is%2520not%2520satisfactory.%2520To%2520address%2520these%2520limitations%252C%2520we%2520explore%2520full%250Aduplex%2520modeling%2520%2528FDM%2529%2520in%2520interactive%2520speech%2520language%2520models%2520%2528iSLM%2529%252C%2520focusing%2520on%250Aenhancing%2520real-time%2520interaction%2520and%252C%2520more%2520explicitly%252C%2520exploring%2520the%250Aquintessential%2520ability%2520of%2520interruption.%2520We%2520introduce%2520a%2520novel%2520model%2520design%252C%250Anamely%2520listening-while-speaking%2520language%2520model%2520%2528LSLM%2529%252C%2520an%2520end-to-end%2520system%250Aequipped%2520with%2520both%2520listening%2520and%2520speaking%2520channels.%2520Our%2520LSLM%2520employs%2520a%250Atoken-based%2520decoder-only%2520TTS%2520for%2520speech%2520generation%2520and%2520a%2520streaming%250Aself-supervised%2520learning%2520%2528SSL%2529%2520encoder%2520for%2520real-time%2520audio%2520input.%2520LSLM%2520fuses%250Aboth%2520channels%2520for%2520autoregressive%2520generation%2520and%2520detects%2520turn-taking%2520in%2520real%250Atime.%2520Three%2520fusion%2520strategies%2520--%2520early%2520fusion%252C%2520middle%2520fusion%252C%2520and%2520late%2520fusion%250A--%2520are%2520explored%252C%2520with%2520middle%2520fusion%2520achieving%2520an%2520optimal%2520balance%2520between%2520speech%250Ageneration%2520and%2520real-time%2520interaction.%2520Two%2520experimental%2520settings%252C%2520command-based%250AFDM%2520and%2520voice-based%2520FDM%252C%2520demonstrate%2520LSLM%2527s%2520robustness%2520to%2520noise%2520and%2520sensitivity%250Ato%2520diverse%2520instructions.%2520Our%2520results%2520highlight%2520LSLM%2527s%2520capability%2520to%2520achieve%250Aduplex%2520communication%2520with%2520minimal%2520impact%2520on%2520existing%2520systems.%2520This%2520study%2520aims%250Ato%2520advance%2520the%2520development%2520of%2520interactive%2520speech%2520dialogue%2520systems%252C%2520enhancing%250Atheir%2520applicability%2520in%2520real-world%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Model%20Can%20Listen%20While%20Speaking&entry.906535625=Ziyang%20Ma%20and%20Yakun%20Song%20and%20Chenpeng%20Du%20and%20Jian%20Cong%20and%20Zhuo%20Chen%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Xie%20Chen&entry.1292438233=%20%20Dialogue%20serves%20as%20the%20most%20natural%20manner%20of%20human-computer%20interaction%0A%28HCI%29.%20Recent%20advancements%20in%20speech%20language%20models%20%28SLM%29%20have%20significantly%0Aenhanced%20speech-based%20conversational%20AI.%20However%2C%20these%20models%20are%20limited%20to%0Aturn-based%20conversation%2C%20lacking%20the%20ability%20to%20interact%20with%20humans%20in%0Areal-time%20spoken%20scenarios%2C%20for%20example%2C%20being%20interrupted%20when%20the%20generated%0Acontent%20is%20not%20satisfactory.%20To%20address%20these%20limitations%2C%20we%20explore%20full%0Aduplex%20modeling%20%28FDM%29%20in%20interactive%20speech%20language%20models%20%28iSLM%29%2C%20focusing%20on%0Aenhancing%20real-time%20interaction%20and%2C%20more%20explicitly%2C%20exploring%20the%0Aquintessential%20ability%20of%20interruption.%20We%20introduce%20a%20novel%20model%20design%2C%0Anamely%20listening-while-speaking%20language%20model%20%28LSLM%29%2C%20an%20end-to-end%20system%0Aequipped%20with%20both%20listening%20and%20speaking%20channels.%20Our%20LSLM%20employs%20a%0Atoken-based%20decoder-only%20TTS%20for%20speech%20generation%20and%20a%20streaming%0Aself-supervised%20learning%20%28SSL%29%20encoder%20for%20real-time%20audio%20input.%20LSLM%20fuses%0Aboth%20channels%20for%20autoregressive%20generation%20and%20detects%20turn-taking%20in%20real%0Atime.%20Three%20fusion%20strategies%20--%20early%20fusion%2C%20middle%20fusion%2C%20and%20late%20fusion%0A--%20are%20explored%2C%20with%20middle%20fusion%20achieving%20an%20optimal%20balance%20between%20speech%0Ageneration%20and%20real-time%20interaction.%20Two%20experimental%20settings%2C%20command-based%0AFDM%20and%20voice-based%20FDM%2C%20demonstrate%20LSLM%27s%20robustness%20to%20noise%20and%20sensitivity%0Ato%20diverse%20instructions.%20Our%20results%20highlight%20LSLM%27s%20capability%20to%20achieve%0Aduplex%20communication%20with%20minimal%20impact%20on%20existing%20systems.%20This%20study%20aims%0Ato%20advance%20the%20development%20of%20interactive%20speech%20dialogue%20systems%2C%20enhancing%0Atheir%20applicability%20in%20real-world%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02622v1&entry.124074799=Read"},
{"title": "Perfect Information Monte Carlo with Postponing Reasoning", "author": "J\u00e9r\u00f4me Arjonilla and Abdallah Saffidine and Tristan Cazenave", "abstract": "  Imperfect information games, such as Bridge and Skat, present challenges due\nto state-space explosion and hidden information, posing formidable obstacles\nfor search algorithms. Determinization-based algorithms offer a resolution by\nsampling hidden information and solving the game in a perfect information\nsetting, facilitating rapid and effective action estimation. However,\ntransitioning to perfect information introduces challenges, notably one called\nstrategy fusion.This research introduces `Extended Perfect Information Monte\nCarlo' (EPIMC), an online algorithm inspired by the state-of-the-art\ndeterminization-based approach Perfect Information Monte Carlo (PIMC). EPIMC\nenhances the capabilities of PIMC by postponing the perfect information\nresolution, reducing alleviating issues related to strategy fusion. However,\nthe decision to postpone the leaf evaluator introduces novel considerations,\nsuch as the interplay between prior levels of reasoning and the newly deferred\nresolution. In our empirical analysis, we investigate the performance of EPIMC\nacross a range of games, with a particular focus on those characterized by\nvarying degrees of strategy fusion. Our results demonstrate notable performance\nenhancements, particularly in games where strategy fusion significantly impacts\ngameplay. Furthermore, our research contributes to the theoretical foundation\nof determinization-based algorithms addressing challenges associated with\nstrategy fusion.%, thereby enhancing our understanding of these algorithms\nwithin the context of imperfect information game scenarios.\n", "link": "http://arxiv.org/abs/2408.02380v1", "date": "2024-08-05", "relevancy": 1.757, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.451}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4405}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perfect%20Information%20Monte%20Carlo%20with%20Postponing%20Reasoning&body=Title%3A%20Perfect%20Information%20Monte%20Carlo%20with%20Postponing%20Reasoning%0AAuthor%3A%20J%C3%A9r%C3%B4me%20Arjonilla%20and%20Abdallah%20Saffidine%20and%20Tristan%20Cazenave%0AAbstract%3A%20%20%20Imperfect%20information%20games%2C%20such%20as%20Bridge%20and%20Skat%2C%20present%20challenges%20due%0Ato%20state-space%20explosion%20and%20hidden%20information%2C%20posing%20formidable%20obstacles%0Afor%20search%20algorithms.%20Determinization-based%20algorithms%20offer%20a%20resolution%20by%0Asampling%20hidden%20information%20and%20solving%20the%20game%20in%20a%20perfect%20information%0Asetting%2C%20facilitating%20rapid%20and%20effective%20action%20estimation.%20However%2C%0Atransitioning%20to%20perfect%20information%20introduces%20challenges%2C%20notably%20one%20called%0Astrategy%20fusion.This%20research%20introduces%20%60Extended%20Perfect%20Information%20Monte%0ACarlo%27%20%28EPIMC%29%2C%20an%20online%20algorithm%20inspired%20by%20the%20state-of-the-art%0Adeterminization-based%20approach%20Perfect%20Information%20Monte%20Carlo%20%28PIMC%29.%20EPIMC%0Aenhances%20the%20capabilities%20of%20PIMC%20by%20postponing%20the%20perfect%20information%0Aresolution%2C%20reducing%20alleviating%20issues%20related%20to%20strategy%20fusion.%20However%2C%0Athe%20decision%20to%20postpone%20the%20leaf%20evaluator%20introduces%20novel%20considerations%2C%0Asuch%20as%20the%20interplay%20between%20prior%20levels%20of%20reasoning%20and%20the%20newly%20deferred%0Aresolution.%20In%20our%20empirical%20analysis%2C%20we%20investigate%20the%20performance%20of%20EPIMC%0Aacross%20a%20range%20of%20games%2C%20with%20a%20particular%20focus%20on%20those%20characterized%20by%0Avarying%20degrees%20of%20strategy%20fusion.%20Our%20results%20demonstrate%20notable%20performance%0Aenhancements%2C%20particularly%20in%20games%20where%20strategy%20fusion%20significantly%20impacts%0Agameplay.%20Furthermore%2C%20our%20research%20contributes%20to%20the%20theoretical%20foundation%0Aof%20determinization-based%20algorithms%20addressing%20challenges%20associated%20with%0Astrategy%20fusion.%25%2C%20thereby%20enhancing%20our%20understanding%20of%20these%20algorithms%0Awithin%20the%20context%20of%20imperfect%20information%20game%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerfect%2520Information%2520Monte%2520Carlo%2520with%2520Postponing%2520Reasoning%26entry.906535625%3DJ%25C3%25A9r%25C3%25B4me%2520Arjonilla%2520and%2520Abdallah%2520Saffidine%2520and%2520Tristan%2520Cazenave%26entry.1292438233%3D%2520%2520Imperfect%2520information%2520games%252C%2520such%2520as%2520Bridge%2520and%2520Skat%252C%2520present%2520challenges%2520due%250Ato%2520state-space%2520explosion%2520and%2520hidden%2520information%252C%2520posing%2520formidable%2520obstacles%250Afor%2520search%2520algorithms.%2520Determinization-based%2520algorithms%2520offer%2520a%2520resolution%2520by%250Asampling%2520hidden%2520information%2520and%2520solving%2520the%2520game%2520in%2520a%2520perfect%2520information%250Asetting%252C%2520facilitating%2520rapid%2520and%2520effective%2520action%2520estimation.%2520However%252C%250Atransitioning%2520to%2520perfect%2520information%2520introduces%2520challenges%252C%2520notably%2520one%2520called%250Astrategy%2520fusion.This%2520research%2520introduces%2520%2560Extended%2520Perfect%2520Information%2520Monte%250ACarlo%2527%2520%2528EPIMC%2529%252C%2520an%2520online%2520algorithm%2520inspired%2520by%2520the%2520state-of-the-art%250Adeterminization-based%2520approach%2520Perfect%2520Information%2520Monte%2520Carlo%2520%2528PIMC%2529.%2520EPIMC%250Aenhances%2520the%2520capabilities%2520of%2520PIMC%2520by%2520postponing%2520the%2520perfect%2520information%250Aresolution%252C%2520reducing%2520alleviating%2520issues%2520related%2520to%2520strategy%2520fusion.%2520However%252C%250Athe%2520decision%2520to%2520postpone%2520the%2520leaf%2520evaluator%2520introduces%2520novel%2520considerations%252C%250Asuch%2520as%2520the%2520interplay%2520between%2520prior%2520levels%2520of%2520reasoning%2520and%2520the%2520newly%2520deferred%250Aresolution.%2520In%2520our%2520empirical%2520analysis%252C%2520we%2520investigate%2520the%2520performance%2520of%2520EPIMC%250Aacross%2520a%2520range%2520of%2520games%252C%2520with%2520a%2520particular%2520focus%2520on%2520those%2520characterized%2520by%250Avarying%2520degrees%2520of%2520strategy%2520fusion.%2520Our%2520results%2520demonstrate%2520notable%2520performance%250Aenhancements%252C%2520particularly%2520in%2520games%2520where%2520strategy%2520fusion%2520significantly%2520impacts%250Agameplay.%2520Furthermore%252C%2520our%2520research%2520contributes%2520to%2520the%2520theoretical%2520foundation%250Aof%2520determinization-based%2520algorithms%2520addressing%2520challenges%2520associated%2520with%250Astrategy%2520fusion.%2525%252C%2520thereby%2520enhancing%2520our%2520understanding%2520of%2520these%2520algorithms%250Awithin%2520the%2520context%2520of%2520imperfect%2520information%2520game%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perfect%20Information%20Monte%20Carlo%20with%20Postponing%20Reasoning&entry.906535625=J%C3%A9r%C3%B4me%20Arjonilla%20and%20Abdallah%20Saffidine%20and%20Tristan%20Cazenave&entry.1292438233=%20%20Imperfect%20information%20games%2C%20such%20as%20Bridge%20and%20Skat%2C%20present%20challenges%20due%0Ato%20state-space%20explosion%20and%20hidden%20information%2C%20posing%20formidable%20obstacles%0Afor%20search%20algorithms.%20Determinization-based%20algorithms%20offer%20a%20resolution%20by%0Asampling%20hidden%20information%20and%20solving%20the%20game%20in%20a%20perfect%20information%0Asetting%2C%20facilitating%20rapid%20and%20effective%20action%20estimation.%20However%2C%0Atransitioning%20to%20perfect%20information%20introduces%20challenges%2C%20notably%20one%20called%0Astrategy%20fusion.This%20research%20introduces%20%60Extended%20Perfect%20Information%20Monte%0ACarlo%27%20%28EPIMC%29%2C%20an%20online%20algorithm%20inspired%20by%20the%20state-of-the-art%0Adeterminization-based%20approach%20Perfect%20Information%20Monte%20Carlo%20%28PIMC%29.%20EPIMC%0Aenhances%20the%20capabilities%20of%20PIMC%20by%20postponing%20the%20perfect%20information%0Aresolution%2C%20reducing%20alleviating%20issues%20related%20to%20strategy%20fusion.%20However%2C%0Athe%20decision%20to%20postpone%20the%20leaf%20evaluator%20introduces%20novel%20considerations%2C%0Asuch%20as%20the%20interplay%20between%20prior%20levels%20of%20reasoning%20and%20the%20newly%20deferred%0Aresolution.%20In%20our%20empirical%20analysis%2C%20we%20investigate%20the%20performance%20of%20EPIMC%0Aacross%20a%20range%20of%20games%2C%20with%20a%20particular%20focus%20on%20those%20characterized%20by%0Avarying%20degrees%20of%20strategy%20fusion.%20Our%20results%20demonstrate%20notable%20performance%0Aenhancements%2C%20particularly%20in%20games%20where%20strategy%20fusion%20significantly%20impacts%0Agameplay.%20Furthermore%2C%20our%20research%20contributes%20to%20the%20theoretical%20foundation%0Aof%20determinization-based%20algorithms%20addressing%20challenges%20associated%20with%0Astrategy%20fusion.%25%2C%20thereby%20enhancing%20our%20understanding%20of%20these%20algorithms%0Awithin%20the%20context%20of%20imperfect%20information%20game%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02380v1&entry.124074799=Read"},
{"title": "Contrastive Learning-based Multi Modal Architecture for Emoticon\n  Prediction by Employing Image-Text Pairs", "author": "Ananya Pandey and Dinesh Kumar Vishwakarma", "abstract": "  The emoticons are symbolic representations that generally accompany the\ntextual content to visually enhance or summarize the true intention of a\nwritten message. Although widely utilized in the realm of social media, the\ncore semantics of these emoticons have not been extensively explored based on\nmultiple modalities. Incorporating textual and visual information within a\nsingle message develops an advanced way of conveying information. Hence, this\nresearch aims to analyze the relationship among sentences, visuals, and\nemoticons. For an orderly exposition, this paper initially provides a detailed\nexamination of the various techniques for extracting multimodal features,\nemphasizing the pros and cons of each method. Through conducting a\ncomprehensive examination of several multimodal algorithms, with specific\nemphasis on the fusion approaches, we have proposed a novel contrastive\nlearning based multimodal architecture. The proposed model employs the joint\ntraining of dual-branch encoder along with the contrastive learning to\naccurately map text and images into a common latent space. Our key finding is\nthat by integrating the principle of contrastive learning with that of the\nother two branches yields superior results. The experimental results\ndemonstrate that our suggested methodology surpasses existing multimodal\napproaches in terms of accuracy and robustness. The proposed model attained an\naccuracy of 91% and an MCC-score of 90% while assessing emoticons using the\nMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence\nthat deep features acquired by contrastive learning are more efficient,\nsuggesting that the proposed fusion technique also possesses strong\ngeneralisation capabilities for recognising emoticons across several modes.\n", "link": "http://arxiv.org/abs/2408.02571v1", "date": "2024-08-05", "relevancy": 1.742, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5919}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5817}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning-based%20Multi%20Modal%20Architecture%20for%20Emoticon%0A%20%20Prediction%20by%20Employing%20Image-Text%20Pairs&body=Title%3A%20Contrastive%20Learning-based%20Multi%20Modal%20Architecture%20for%20Emoticon%0A%20%20Prediction%20by%20Employing%20Image-Text%20Pairs%0AAuthor%3A%20Ananya%20Pandey%20and%20Dinesh%20Kumar%20Vishwakarma%0AAbstract%3A%20%20%20The%20emoticons%20are%20symbolic%20representations%20that%20generally%20accompany%20the%0Atextual%20content%20to%20visually%20enhance%20or%20summarize%20the%20true%20intention%20of%20a%0Awritten%20message.%20Although%20widely%20utilized%20in%20the%20realm%20of%20social%20media%2C%20the%0Acore%20semantics%20of%20these%20emoticons%20have%20not%20been%20extensively%20explored%20based%20on%0Amultiple%20modalities.%20Incorporating%20textual%20and%20visual%20information%20within%20a%0Asingle%20message%20develops%20an%20advanced%20way%20of%20conveying%20information.%20Hence%2C%20this%0Aresearch%20aims%20to%20analyze%20the%20relationship%20among%20sentences%2C%20visuals%2C%20and%0Aemoticons.%20For%20an%20orderly%20exposition%2C%20this%20paper%20initially%20provides%20a%20detailed%0Aexamination%20of%20the%20various%20techniques%20for%20extracting%20multimodal%20features%2C%0Aemphasizing%20the%20pros%20and%20cons%20of%20each%20method.%20Through%20conducting%20a%0Acomprehensive%20examination%20of%20several%20multimodal%20algorithms%2C%20with%20specific%0Aemphasis%20on%20the%20fusion%20approaches%2C%20we%20have%20proposed%20a%20novel%20contrastive%0Alearning%20based%20multimodal%20architecture.%20The%20proposed%20model%20employs%20the%20joint%0Atraining%20of%20dual-branch%20encoder%20along%20with%20the%20contrastive%20learning%20to%0Aaccurately%20map%20text%20and%20images%20into%20a%20common%20latent%20space.%20Our%20key%20finding%20is%0Athat%20by%20integrating%20the%20principle%20of%20contrastive%20learning%20with%20that%20of%20the%0Aother%20two%20branches%20yields%20superior%20results.%20The%20experimental%20results%0Ademonstrate%20that%20our%20suggested%20methodology%20surpasses%20existing%20multimodal%0Aapproaches%20in%20terms%20of%20accuracy%20and%20robustness.%20The%20proposed%20model%20attained%20an%0Aaccuracy%20of%2091%25%20and%20an%20MCC-score%20of%2090%25%20while%20assessing%20emoticons%20using%20the%0AMultimodal-Twitter%20Emoticon%20dataset%20acquired%20from%20Twitter.%20We%20provide%20evidence%0Athat%20deep%20features%20acquired%20by%20contrastive%20learning%20are%20more%20efficient%2C%0Asuggesting%20that%20the%20proposed%20fusion%20technique%20also%20possesses%20strong%0Ageneralisation%20capabilities%20for%20recognising%20emoticons%20across%20several%20modes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning-based%2520Multi%2520Modal%2520Architecture%2520for%2520Emoticon%250A%2520%2520Prediction%2520by%2520Employing%2520Image-Text%2520Pairs%26entry.906535625%3DAnanya%2520Pandey%2520and%2520Dinesh%2520Kumar%2520Vishwakarma%26entry.1292438233%3D%2520%2520The%2520emoticons%2520are%2520symbolic%2520representations%2520that%2520generally%2520accompany%2520the%250Atextual%2520content%2520to%2520visually%2520enhance%2520or%2520summarize%2520the%2520true%2520intention%2520of%2520a%250Awritten%2520message.%2520Although%2520widely%2520utilized%2520in%2520the%2520realm%2520of%2520social%2520media%252C%2520the%250Acore%2520semantics%2520of%2520these%2520emoticons%2520have%2520not%2520been%2520extensively%2520explored%2520based%2520on%250Amultiple%2520modalities.%2520Incorporating%2520textual%2520and%2520visual%2520information%2520within%2520a%250Asingle%2520message%2520develops%2520an%2520advanced%2520way%2520of%2520conveying%2520information.%2520Hence%252C%2520this%250Aresearch%2520aims%2520to%2520analyze%2520the%2520relationship%2520among%2520sentences%252C%2520visuals%252C%2520and%250Aemoticons.%2520For%2520an%2520orderly%2520exposition%252C%2520this%2520paper%2520initially%2520provides%2520a%2520detailed%250Aexamination%2520of%2520the%2520various%2520techniques%2520for%2520extracting%2520multimodal%2520features%252C%250Aemphasizing%2520the%2520pros%2520and%2520cons%2520of%2520each%2520method.%2520Through%2520conducting%2520a%250Acomprehensive%2520examination%2520of%2520several%2520multimodal%2520algorithms%252C%2520with%2520specific%250Aemphasis%2520on%2520the%2520fusion%2520approaches%252C%2520we%2520have%2520proposed%2520a%2520novel%2520contrastive%250Alearning%2520based%2520multimodal%2520architecture.%2520The%2520proposed%2520model%2520employs%2520the%2520joint%250Atraining%2520of%2520dual-branch%2520encoder%2520along%2520with%2520the%2520contrastive%2520learning%2520to%250Aaccurately%2520map%2520text%2520and%2520images%2520into%2520a%2520common%2520latent%2520space.%2520Our%2520key%2520finding%2520is%250Athat%2520by%2520integrating%2520the%2520principle%2520of%2520contrastive%2520learning%2520with%2520that%2520of%2520the%250Aother%2520two%2520branches%2520yields%2520superior%2520results.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520suggested%2520methodology%2520surpasses%2520existing%2520multimodal%250Aapproaches%2520in%2520terms%2520of%2520accuracy%2520and%2520robustness.%2520The%2520proposed%2520model%2520attained%2520an%250Aaccuracy%2520of%252091%2525%2520and%2520an%2520MCC-score%2520of%252090%2525%2520while%2520assessing%2520emoticons%2520using%2520the%250AMultimodal-Twitter%2520Emoticon%2520dataset%2520acquired%2520from%2520Twitter.%2520We%2520provide%2520evidence%250Athat%2520deep%2520features%2520acquired%2520by%2520contrastive%2520learning%2520are%2520more%2520efficient%252C%250Asuggesting%2520that%2520the%2520proposed%2520fusion%2520technique%2520also%2520possesses%2520strong%250Ageneralisation%2520capabilities%2520for%2520recognising%2520emoticons%2520across%2520several%2520modes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning-based%20Multi%20Modal%20Architecture%20for%20Emoticon%0A%20%20Prediction%20by%20Employing%20Image-Text%20Pairs&entry.906535625=Ananya%20Pandey%20and%20Dinesh%20Kumar%20Vishwakarma&entry.1292438233=%20%20The%20emoticons%20are%20symbolic%20representations%20that%20generally%20accompany%20the%0Atextual%20content%20to%20visually%20enhance%20or%20summarize%20the%20true%20intention%20of%20a%0Awritten%20message.%20Although%20widely%20utilized%20in%20the%20realm%20of%20social%20media%2C%20the%0Acore%20semantics%20of%20these%20emoticons%20have%20not%20been%20extensively%20explored%20based%20on%0Amultiple%20modalities.%20Incorporating%20textual%20and%20visual%20information%20within%20a%0Asingle%20message%20develops%20an%20advanced%20way%20of%20conveying%20information.%20Hence%2C%20this%0Aresearch%20aims%20to%20analyze%20the%20relationship%20among%20sentences%2C%20visuals%2C%20and%0Aemoticons.%20For%20an%20orderly%20exposition%2C%20this%20paper%20initially%20provides%20a%20detailed%0Aexamination%20of%20the%20various%20techniques%20for%20extracting%20multimodal%20features%2C%0Aemphasizing%20the%20pros%20and%20cons%20of%20each%20method.%20Through%20conducting%20a%0Acomprehensive%20examination%20of%20several%20multimodal%20algorithms%2C%20with%20specific%0Aemphasis%20on%20the%20fusion%20approaches%2C%20we%20have%20proposed%20a%20novel%20contrastive%0Alearning%20based%20multimodal%20architecture.%20The%20proposed%20model%20employs%20the%20joint%0Atraining%20of%20dual-branch%20encoder%20along%20with%20the%20contrastive%20learning%20to%0Aaccurately%20map%20text%20and%20images%20into%20a%20common%20latent%20space.%20Our%20key%20finding%20is%0Athat%20by%20integrating%20the%20principle%20of%20contrastive%20learning%20with%20that%20of%20the%0Aother%20two%20branches%20yields%20superior%20results.%20The%20experimental%20results%0Ademonstrate%20that%20our%20suggested%20methodology%20surpasses%20existing%20multimodal%0Aapproaches%20in%20terms%20of%20accuracy%20and%20robustness.%20The%20proposed%20model%20attained%20an%0Aaccuracy%20of%2091%25%20and%20an%20MCC-score%20of%2090%25%20while%20assessing%20emoticons%20using%20the%0AMultimodal-Twitter%20Emoticon%20dataset%20acquired%20from%20Twitter.%20We%20provide%20evidence%0Athat%20deep%20features%20acquired%20by%20contrastive%20learning%20are%20more%20efficient%2C%0Asuggesting%20that%20the%20proposed%20fusion%20technique%20also%20possesses%20strong%0Ageneralisation%20capabilities%20for%20recognising%20emoticons%20across%20several%20modes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02571v1&entry.124074799=Read"},
{"title": "Earth System Data Cubes: Avenues for advancing Earth system research", "author": "David Montero and Guido Kraemer and Anca Anghelea and C\u00e9sar Aybar and Gunnar Brandt and Gustau Camps-Valls and Felix Cremer and Ida Flik and Fabian Gans and Sarah Habershon and Chaonan Ji and Teja Kattenborn and Laura Mart\u00ednez-Ferrer and Francesco Martinuzzi and Martin Reinhardt and Maximilian S\u00f6chting and Khalil Teber and Miguel D. Mahecha", "abstract": "  Recent advancements in Earth system science have been marked by the\nexponential increase in the availability of diverse, multivariate datasets\ncharacterised by moderate to high spatio-temporal resolutions. Earth System\nData Cubes (ESDCs) have emerged as one suitable solution for transforming this\nflood of data into a simple yet robust data structure. ESDCs achieve this by\norganising data into an analysis-ready format aligned with a spatio-temporal\ngrid, facilitating user-friendly analysis and diminishing the need for\nextensive technical data processing knowledge. Despite these significant\nbenefits, the completion of the entire ESDC life cycle remains a challenging\ntask. Obstacles are not only of a technical nature but also relate to\ndomain-specific problems in Earth system research. There exist barriers to\nrealising the full potential of data collections in light of novel cloud-based\ntechnologies, particularly in curating data tailored for specific application\ndomains. These include transforming data to conform to a spatio-temporal grid\nwith minimum distortions and managing complexities such as spatio-temporal\nautocorrelation issues. Addressing these challenges is pivotal for the\neffective application of Artificial Intelligence (AI) approaches. Furthermore,\nadhering to open science principles for data dissemination, reproducibility,\nvisualisation, and reuse is crucial for fostering sustainable research.\nOvercoming these challenges offers a substantial opportunity to advance\ndata-driven Earth system research, unlocking the full potential of an\nintegrated, multidimensional view of Earth system processes. This is\nparticularly true when such research is coupled with innovative research\nparadigms and technological progress.\n", "link": "http://arxiv.org/abs/2408.02348v1", "date": "2024-08-05", "relevancy": 1.742, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4461}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4334}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Earth%20System%20Data%20Cubes%3A%20Avenues%20for%20advancing%20Earth%20system%20research&body=Title%3A%20Earth%20System%20Data%20Cubes%3A%20Avenues%20for%20advancing%20Earth%20system%20research%0AAuthor%3A%20David%20Montero%20and%20Guido%20Kraemer%20and%20Anca%20Anghelea%20and%20C%C3%A9sar%20Aybar%20and%20Gunnar%20Brandt%20and%20Gustau%20Camps-Valls%20and%20Felix%20Cremer%20and%20Ida%20Flik%20and%20Fabian%20Gans%20and%20Sarah%20Habershon%20and%20Chaonan%20Ji%20and%20Teja%20Kattenborn%20and%20Laura%20Mart%C3%ADnez-Ferrer%20and%20Francesco%20Martinuzzi%20and%20Martin%20Reinhardt%20and%20Maximilian%20S%C3%B6chting%20and%20Khalil%20Teber%20and%20Miguel%20D.%20Mahecha%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Earth%20system%20science%20have%20been%20marked%20by%20the%0Aexponential%20increase%20in%20the%20availability%20of%20diverse%2C%20multivariate%20datasets%0Acharacterised%20by%20moderate%20to%20high%20spatio-temporal%20resolutions.%20Earth%20System%0AData%20Cubes%20%28ESDCs%29%20have%20emerged%20as%20one%20suitable%20solution%20for%20transforming%20this%0Aflood%20of%20data%20into%20a%20simple%20yet%20robust%20data%20structure.%20ESDCs%20achieve%20this%20by%0Aorganising%20data%20into%20an%20analysis-ready%20format%20aligned%20with%20a%20spatio-temporal%0Agrid%2C%20facilitating%20user-friendly%20analysis%20and%20diminishing%20the%20need%20for%0Aextensive%20technical%20data%20processing%20knowledge.%20Despite%20these%20significant%0Abenefits%2C%20the%20completion%20of%20the%20entire%20ESDC%20life%20cycle%20remains%20a%20challenging%0Atask.%20Obstacles%20are%20not%20only%20of%20a%20technical%20nature%20but%20also%20relate%20to%0Adomain-specific%20problems%20in%20Earth%20system%20research.%20There%20exist%20barriers%20to%0Arealising%20the%20full%20potential%20of%20data%20collections%20in%20light%20of%20novel%20cloud-based%0Atechnologies%2C%20particularly%20in%20curating%20data%20tailored%20for%20specific%20application%0Adomains.%20These%20include%20transforming%20data%20to%20conform%20to%20a%20spatio-temporal%20grid%0Awith%20minimum%20distortions%20and%20managing%20complexities%20such%20as%20spatio-temporal%0Aautocorrelation%20issues.%20Addressing%20these%20challenges%20is%20pivotal%20for%20the%0Aeffective%20application%20of%20Artificial%20Intelligence%20%28AI%29%20approaches.%20Furthermore%2C%0Aadhering%20to%20open%20science%20principles%20for%20data%20dissemination%2C%20reproducibility%2C%0Avisualisation%2C%20and%20reuse%20is%20crucial%20for%20fostering%20sustainable%20research.%0AOvercoming%20these%20challenges%20offers%20a%20substantial%20opportunity%20to%20advance%0Adata-driven%20Earth%20system%20research%2C%20unlocking%20the%20full%20potential%20of%20an%0Aintegrated%2C%20multidimensional%20view%20of%20Earth%20system%20processes.%20This%20is%0Aparticularly%20true%20when%20such%20research%20is%20coupled%20with%20innovative%20research%0Aparadigms%20and%20technological%20progress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarth%2520System%2520Data%2520Cubes%253A%2520Avenues%2520for%2520advancing%2520Earth%2520system%2520research%26entry.906535625%3DDavid%2520Montero%2520and%2520Guido%2520Kraemer%2520and%2520Anca%2520Anghelea%2520and%2520C%25C3%25A9sar%2520Aybar%2520and%2520Gunnar%2520Brandt%2520and%2520Gustau%2520Camps-Valls%2520and%2520Felix%2520Cremer%2520and%2520Ida%2520Flik%2520and%2520Fabian%2520Gans%2520and%2520Sarah%2520Habershon%2520and%2520Chaonan%2520Ji%2520and%2520Teja%2520Kattenborn%2520and%2520Laura%2520Mart%25C3%25ADnez-Ferrer%2520and%2520Francesco%2520Martinuzzi%2520and%2520Martin%2520Reinhardt%2520and%2520Maximilian%2520S%25C3%25B6chting%2520and%2520Khalil%2520Teber%2520and%2520Miguel%2520D.%2520Mahecha%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Earth%2520system%2520science%2520have%2520been%2520marked%2520by%2520the%250Aexponential%2520increase%2520in%2520the%2520availability%2520of%2520diverse%252C%2520multivariate%2520datasets%250Acharacterised%2520by%2520moderate%2520to%2520high%2520spatio-temporal%2520resolutions.%2520Earth%2520System%250AData%2520Cubes%2520%2528ESDCs%2529%2520have%2520emerged%2520as%2520one%2520suitable%2520solution%2520for%2520transforming%2520this%250Aflood%2520of%2520data%2520into%2520a%2520simple%2520yet%2520robust%2520data%2520structure.%2520ESDCs%2520achieve%2520this%2520by%250Aorganising%2520data%2520into%2520an%2520analysis-ready%2520format%2520aligned%2520with%2520a%2520spatio-temporal%250Agrid%252C%2520facilitating%2520user-friendly%2520analysis%2520and%2520diminishing%2520the%2520need%2520for%250Aextensive%2520technical%2520data%2520processing%2520knowledge.%2520Despite%2520these%2520significant%250Abenefits%252C%2520the%2520completion%2520of%2520the%2520entire%2520ESDC%2520life%2520cycle%2520remains%2520a%2520challenging%250Atask.%2520Obstacles%2520are%2520not%2520only%2520of%2520a%2520technical%2520nature%2520but%2520also%2520relate%2520to%250Adomain-specific%2520problems%2520in%2520Earth%2520system%2520research.%2520There%2520exist%2520barriers%2520to%250Arealising%2520the%2520full%2520potential%2520of%2520data%2520collections%2520in%2520light%2520of%2520novel%2520cloud-based%250Atechnologies%252C%2520particularly%2520in%2520curating%2520data%2520tailored%2520for%2520specific%2520application%250Adomains.%2520These%2520include%2520transforming%2520data%2520to%2520conform%2520to%2520a%2520spatio-temporal%2520grid%250Awith%2520minimum%2520distortions%2520and%2520managing%2520complexities%2520such%2520as%2520spatio-temporal%250Aautocorrelation%2520issues.%2520Addressing%2520these%2520challenges%2520is%2520pivotal%2520for%2520the%250Aeffective%2520application%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520approaches.%2520Furthermore%252C%250Aadhering%2520to%2520open%2520science%2520principles%2520for%2520data%2520dissemination%252C%2520reproducibility%252C%250Avisualisation%252C%2520and%2520reuse%2520is%2520crucial%2520for%2520fostering%2520sustainable%2520research.%250AOvercoming%2520these%2520challenges%2520offers%2520a%2520substantial%2520opportunity%2520to%2520advance%250Adata-driven%2520Earth%2520system%2520research%252C%2520unlocking%2520the%2520full%2520potential%2520of%2520an%250Aintegrated%252C%2520multidimensional%2520view%2520of%2520Earth%2520system%2520processes.%2520This%2520is%250Aparticularly%2520true%2520when%2520such%2520research%2520is%2520coupled%2520with%2520innovative%2520research%250Aparadigms%2520and%2520technological%2520progress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Earth%20System%20Data%20Cubes%3A%20Avenues%20for%20advancing%20Earth%20system%20research&entry.906535625=David%20Montero%20and%20Guido%20Kraemer%20and%20Anca%20Anghelea%20and%20C%C3%A9sar%20Aybar%20and%20Gunnar%20Brandt%20and%20Gustau%20Camps-Valls%20and%20Felix%20Cremer%20and%20Ida%20Flik%20and%20Fabian%20Gans%20and%20Sarah%20Habershon%20and%20Chaonan%20Ji%20and%20Teja%20Kattenborn%20and%20Laura%20Mart%C3%ADnez-Ferrer%20and%20Francesco%20Martinuzzi%20and%20Martin%20Reinhardt%20and%20Maximilian%20S%C3%B6chting%20and%20Khalil%20Teber%20and%20Miguel%20D.%20Mahecha&entry.1292438233=%20%20Recent%20advancements%20in%20Earth%20system%20science%20have%20been%20marked%20by%20the%0Aexponential%20increase%20in%20the%20availability%20of%20diverse%2C%20multivariate%20datasets%0Acharacterised%20by%20moderate%20to%20high%20spatio-temporal%20resolutions.%20Earth%20System%0AData%20Cubes%20%28ESDCs%29%20have%20emerged%20as%20one%20suitable%20solution%20for%20transforming%20this%0Aflood%20of%20data%20into%20a%20simple%20yet%20robust%20data%20structure.%20ESDCs%20achieve%20this%20by%0Aorganising%20data%20into%20an%20analysis-ready%20format%20aligned%20with%20a%20spatio-temporal%0Agrid%2C%20facilitating%20user-friendly%20analysis%20and%20diminishing%20the%20need%20for%0Aextensive%20technical%20data%20processing%20knowledge.%20Despite%20these%20significant%0Abenefits%2C%20the%20completion%20of%20the%20entire%20ESDC%20life%20cycle%20remains%20a%20challenging%0Atask.%20Obstacles%20are%20not%20only%20of%20a%20technical%20nature%20but%20also%20relate%20to%0Adomain-specific%20problems%20in%20Earth%20system%20research.%20There%20exist%20barriers%20to%0Arealising%20the%20full%20potential%20of%20data%20collections%20in%20light%20of%20novel%20cloud-based%0Atechnologies%2C%20particularly%20in%20curating%20data%20tailored%20for%20specific%20application%0Adomains.%20These%20include%20transforming%20data%20to%20conform%20to%20a%20spatio-temporal%20grid%0Awith%20minimum%20distortions%20and%20managing%20complexities%20such%20as%20spatio-temporal%0Aautocorrelation%20issues.%20Addressing%20these%20challenges%20is%20pivotal%20for%20the%0Aeffective%20application%20of%20Artificial%20Intelligence%20%28AI%29%20approaches.%20Furthermore%2C%0Aadhering%20to%20open%20science%20principles%20for%20data%20dissemination%2C%20reproducibility%2C%0Avisualisation%2C%20and%20reuse%20is%20crucial%20for%20fostering%20sustainable%20research.%0AOvercoming%20these%20challenges%20offers%20a%20substantial%20opportunity%20to%20advance%0Adata-driven%20Earth%20system%20research%2C%20unlocking%20the%20full%20potential%20of%20an%0Aintegrated%2C%20multidimensional%20view%20of%20Earth%20system%20processes.%20This%20is%0Aparticularly%20true%20when%20such%20research%20is%20coupled%20with%20innovative%20research%0Aparadigms%20and%20technological%20progress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02348v1&entry.124074799=Read"},
{"title": "Neural networks for bifurcation and linear stability analysis of steady\n  states in partial differential equations", "author": "Muhammad Luthfi Shahab and Hadi Susanto", "abstract": "  This research introduces an extended application of neural networks for\nsolving nonlinear partial differential equations (PDEs). A neural network,\ncombined with a pseudo-arclength continuation, is proposed to construct\nbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural\nnetwork approach is also presented for solving eigenvalue problems to analyze\nsolution linear stability, focusing on identifying the largest eigenvalue. The\neffectiveness of the proposed neural network is examined through experiments on\nthe Bratu equation and the Burgers equation. Results from a finite difference\nmethod are also presented as comparison. Varying numbers of grid points are\nemployed in each case to assess the behavior and accuracy of both the neural\nnetwork and the finite difference method. The experimental results demonstrate\nthat the proposed neural network produces better solutions, generates more\naccurate bifurcation diagrams, has reasonable computational times, and proves\neffective for linear stability analysis.\n", "link": "http://arxiv.org/abs/2407.19707v3", "date": "2024-08-05", "relevancy": 1.7366, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4259}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20networks%20for%20bifurcation%20and%20linear%20stability%20analysis%20of%20steady%0A%20%20states%20in%20partial%20differential%20equations&body=Title%3A%20Neural%20networks%20for%20bifurcation%20and%20linear%20stability%20analysis%20of%20steady%0A%20%20states%20in%20partial%20differential%20equations%0AAuthor%3A%20Muhammad%20Luthfi%20Shahab%20and%20Hadi%20Susanto%0AAbstract%3A%20%20%20This%20research%20introduces%20an%20extended%20application%20of%20neural%20networks%20for%0Asolving%20nonlinear%20partial%20differential%20equations%20%28PDEs%29.%20A%20neural%20network%2C%0Acombined%20with%20a%20pseudo-arclength%20continuation%2C%20is%20proposed%20to%20construct%0Abifurcation%20diagrams%20from%20parameterized%20nonlinear%20PDEs.%20Additionally%2C%20a%20neural%0Anetwork%20approach%20is%20also%20presented%20for%20solving%20eigenvalue%20problems%20to%20analyze%0Asolution%20linear%20stability%2C%20focusing%20on%20identifying%20the%20largest%20eigenvalue.%20The%0Aeffectiveness%20of%20the%20proposed%20neural%20network%20is%20examined%20through%20experiments%20on%0Athe%20Bratu%20equation%20and%20the%20Burgers%20equation.%20Results%20from%20a%20finite%20difference%0Amethod%20are%20also%20presented%20as%20comparison.%20Varying%20numbers%20of%20grid%20points%20are%0Aemployed%20in%20each%20case%20to%20assess%20the%20behavior%20and%20accuracy%20of%20both%20the%20neural%0Anetwork%20and%20the%20finite%20difference%20method.%20The%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20neural%20network%20produces%20better%20solutions%2C%20generates%20more%0Aaccurate%20bifurcation%20diagrams%2C%20has%20reasonable%20computational%20times%2C%20and%20proves%0Aeffective%20for%20linear%20stability%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19707v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520networks%2520for%2520bifurcation%2520and%2520linear%2520stability%2520analysis%2520of%2520steady%250A%2520%2520states%2520in%2520partial%2520differential%2520equations%26entry.906535625%3DMuhammad%2520Luthfi%2520Shahab%2520and%2520Hadi%2520Susanto%26entry.1292438233%3D%2520%2520This%2520research%2520introduces%2520an%2520extended%2520application%2520of%2520neural%2520networks%2520for%250Asolving%2520nonlinear%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520A%2520neural%2520network%252C%250Acombined%2520with%2520a%2520pseudo-arclength%2520continuation%252C%2520is%2520proposed%2520to%2520construct%250Abifurcation%2520diagrams%2520from%2520parameterized%2520nonlinear%2520PDEs.%2520Additionally%252C%2520a%2520neural%250Anetwork%2520approach%2520is%2520also%2520presented%2520for%2520solving%2520eigenvalue%2520problems%2520to%2520analyze%250Asolution%2520linear%2520stability%252C%2520focusing%2520on%2520identifying%2520the%2520largest%2520eigenvalue.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520neural%2520network%2520is%2520examined%2520through%2520experiments%2520on%250Athe%2520Bratu%2520equation%2520and%2520the%2520Burgers%2520equation.%2520Results%2520from%2520a%2520finite%2520difference%250Amethod%2520are%2520also%2520presented%2520as%2520comparison.%2520Varying%2520numbers%2520of%2520grid%2520points%2520are%250Aemployed%2520in%2520each%2520case%2520to%2520assess%2520the%2520behavior%2520and%2520accuracy%2520of%2520both%2520the%2520neural%250Anetwork%2520and%2520the%2520finite%2520difference%2520method.%2520The%2520experimental%2520results%2520demonstrate%250Athat%2520the%2520proposed%2520neural%2520network%2520produces%2520better%2520solutions%252C%2520generates%2520more%250Aaccurate%2520bifurcation%2520diagrams%252C%2520has%2520reasonable%2520computational%2520times%252C%2520and%2520proves%250Aeffective%2520for%2520linear%2520stability%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19707v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20networks%20for%20bifurcation%20and%20linear%20stability%20analysis%20of%20steady%0A%20%20states%20in%20partial%20differential%20equations&entry.906535625=Muhammad%20Luthfi%20Shahab%20and%20Hadi%20Susanto&entry.1292438233=%20%20This%20research%20introduces%20an%20extended%20application%20of%20neural%20networks%20for%0Asolving%20nonlinear%20partial%20differential%20equations%20%28PDEs%29.%20A%20neural%20network%2C%0Acombined%20with%20a%20pseudo-arclength%20continuation%2C%20is%20proposed%20to%20construct%0Abifurcation%20diagrams%20from%20parameterized%20nonlinear%20PDEs.%20Additionally%2C%20a%20neural%0Anetwork%20approach%20is%20also%20presented%20for%20solving%20eigenvalue%20problems%20to%20analyze%0Asolution%20linear%20stability%2C%20focusing%20on%20identifying%20the%20largest%20eigenvalue.%20The%0Aeffectiveness%20of%20the%20proposed%20neural%20network%20is%20examined%20through%20experiments%20on%0Athe%20Bratu%20equation%20and%20the%20Burgers%20equation.%20Results%20from%20a%20finite%20difference%0Amethod%20are%20also%20presented%20as%20comparison.%20Varying%20numbers%20of%20grid%20points%20are%0Aemployed%20in%20each%20case%20to%20assess%20the%20behavior%20and%20accuracy%20of%20both%20the%20neural%0Anetwork%20and%20the%20finite%20difference%20method.%20The%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20neural%20network%20produces%20better%20solutions%2C%20generates%20more%0Aaccurate%20bifurcation%20diagrams%2C%20has%20reasonable%20computational%20times%2C%20and%20proves%0Aeffective%20for%20linear%20stability%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19707v3&entry.124074799=Read"},
{"title": "Time-Series Classification in Smart Manufacturing Systems: An\n  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms", "author": "Mojtaba A. Farahani and M. R. McCormick and Ramy Harik and Thorsten Wuest", "abstract": "  Manufacturing is gathering extensive amounts of diverse data, thanks to the\ngrowing number of sensors and rapid advances in sensing technologies. Among the\nvarious data types available in SMS settings, time-series data plays a pivotal\nrole. Hence, TSC emerges is crucial in this domain. The objective of this study\nis to fill this gap by providing a rigorous experimental evaluation of the SoTA\nML and DL algorithms for TSC tasks in manufacturing and industrial settings. We\nfirst explored and compiled a comprehensive list of more than 92 SoTA\nalgorithms from both TSC and manufacturing literature. Following, we selected\nthe 36 most representative algorithms from this list. To evaluate their\nperformance across various manufacturing classification tasks, we curated a set\nof 22 manufacturing datasets, representative of different characteristics that\ncover diverse manufacturing problems. Subsequently, we implemented and\nevaluated the algorithms on the manufacturing benchmark datasets, and analyzed\nthe results for each dataset. Based on the results, ResNet, DrCIF,\nInceptionTime, and ARSENAL are the top-performing algorithms, boasting an\naverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. These\nfindings underscore the robustness, efficiency, scalability, and effectiveness\nof convolutional kernels in capturing temporal features in time-series data, as\nthree out of the top four performing algorithms leverage these kernels for\nfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve\nrecognition for their effectiveness in capturing features within time-series\ndata using RNN-based structures.\n", "link": "http://arxiv.org/abs/2310.02812v2", "date": "2024-08-05", "relevancy": 1.7323, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4435}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4354}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Series%20Classification%20in%20Smart%20Manufacturing%20Systems%3A%20An%0A%20%20Experimental%20Evaluation%20of%20State-of-the-Art%20Machine%20Learning%20Algorithms&body=Title%3A%20Time-Series%20Classification%20in%20Smart%20Manufacturing%20Systems%3A%20An%0A%20%20Experimental%20Evaluation%20of%20State-of-the-Art%20Machine%20Learning%20Algorithms%0AAuthor%3A%20Mojtaba%20A.%20Farahani%20and%20M.%20R.%20McCormick%20and%20Ramy%20Harik%20and%20Thorsten%20Wuest%0AAbstract%3A%20%20%20Manufacturing%20is%20gathering%20extensive%20amounts%20of%20diverse%20data%2C%20thanks%20to%20the%0Agrowing%20number%20of%20sensors%20and%20rapid%20advances%20in%20sensing%20technologies.%20Among%20the%0Avarious%20data%20types%20available%20in%20SMS%20settings%2C%20time-series%20data%20plays%20a%20pivotal%0Arole.%20Hence%2C%20TSC%20emerges%20is%20crucial%20in%20this%20domain.%20The%20objective%20of%20this%20study%0Ais%20to%20fill%20this%20gap%20by%20providing%20a%20rigorous%20experimental%20evaluation%20of%20the%20SoTA%0AML%20and%20DL%20algorithms%20for%20TSC%20tasks%20in%20manufacturing%20and%20industrial%20settings.%20We%0Afirst%20explored%20and%20compiled%20a%20comprehensive%20list%20of%20more%20than%2092%20SoTA%0Aalgorithms%20from%20both%20TSC%20and%20manufacturing%20literature.%20Following%2C%20we%20selected%0Athe%2036%20most%20representative%20algorithms%20from%20this%20list.%20To%20evaluate%20their%0Aperformance%20across%20various%20manufacturing%20classification%20tasks%2C%20we%20curated%20a%20set%0Aof%2022%20manufacturing%20datasets%2C%20representative%20of%20different%20characteristics%20that%0Acover%20diverse%20manufacturing%20problems.%20Subsequently%2C%20we%20implemented%20and%0Aevaluated%20the%20algorithms%20on%20the%20manufacturing%20benchmark%20datasets%2C%20and%20analyzed%0Athe%20results%20for%20each%20dataset.%20Based%20on%20the%20results%2C%20ResNet%2C%20DrCIF%2C%0AInceptionTime%2C%20and%20ARSENAL%20are%20the%20top-performing%20algorithms%2C%20boasting%20an%0Aaverage%20accuracy%20of%20over%2096.6%25%20across%20all%2022%20manufacturing%20TSC%20datasets.%20These%0Afindings%20underscore%20the%20robustness%2C%20efficiency%2C%20scalability%2C%20and%20effectiveness%0Aof%20convolutional%20kernels%20in%20capturing%20temporal%20features%20in%20time-series%20data%2C%20as%0Athree%20out%20of%20the%20top%20four%20performing%20algorithms%20leverage%20these%20kernels%20for%0Afeature%20extraction.%20Additionally%2C%20LSTM%2C%20BiLSTM%2C%20and%20TS-LSTM%20algorithms%20deserve%0Arecognition%20for%20their%20effectiveness%20in%20capturing%20features%20within%20time-series%0Adata%20using%20RNN-based%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02812v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Series%2520Classification%2520in%2520Smart%2520Manufacturing%2520Systems%253A%2520An%250A%2520%2520Experimental%2520Evaluation%2520of%2520State-of-the-Art%2520Machine%2520Learning%2520Algorithms%26entry.906535625%3DMojtaba%2520A.%2520Farahani%2520and%2520M.%2520R.%2520McCormick%2520and%2520Ramy%2520Harik%2520and%2520Thorsten%2520Wuest%26entry.1292438233%3D%2520%2520Manufacturing%2520is%2520gathering%2520extensive%2520amounts%2520of%2520diverse%2520data%252C%2520thanks%2520to%2520the%250Agrowing%2520number%2520of%2520sensors%2520and%2520rapid%2520advances%2520in%2520sensing%2520technologies.%2520Among%2520the%250Avarious%2520data%2520types%2520available%2520in%2520SMS%2520settings%252C%2520time-series%2520data%2520plays%2520a%2520pivotal%250Arole.%2520Hence%252C%2520TSC%2520emerges%2520is%2520crucial%2520in%2520this%2520domain.%2520The%2520objective%2520of%2520this%2520study%250Ais%2520to%2520fill%2520this%2520gap%2520by%2520providing%2520a%2520rigorous%2520experimental%2520evaluation%2520of%2520the%2520SoTA%250AML%2520and%2520DL%2520algorithms%2520for%2520TSC%2520tasks%2520in%2520manufacturing%2520and%2520industrial%2520settings.%2520We%250Afirst%2520explored%2520and%2520compiled%2520a%2520comprehensive%2520list%2520of%2520more%2520than%252092%2520SoTA%250Aalgorithms%2520from%2520both%2520TSC%2520and%2520manufacturing%2520literature.%2520Following%252C%2520we%2520selected%250Athe%252036%2520most%2520representative%2520algorithms%2520from%2520this%2520list.%2520To%2520evaluate%2520their%250Aperformance%2520across%2520various%2520manufacturing%2520classification%2520tasks%252C%2520we%2520curated%2520a%2520set%250Aof%252022%2520manufacturing%2520datasets%252C%2520representative%2520of%2520different%2520characteristics%2520that%250Acover%2520diverse%2520manufacturing%2520problems.%2520Subsequently%252C%2520we%2520implemented%2520and%250Aevaluated%2520the%2520algorithms%2520on%2520the%2520manufacturing%2520benchmark%2520datasets%252C%2520and%2520analyzed%250Athe%2520results%2520for%2520each%2520dataset.%2520Based%2520on%2520the%2520results%252C%2520ResNet%252C%2520DrCIF%252C%250AInceptionTime%252C%2520and%2520ARSENAL%2520are%2520the%2520top-performing%2520algorithms%252C%2520boasting%2520an%250Aaverage%2520accuracy%2520of%2520over%252096.6%2525%2520across%2520all%252022%2520manufacturing%2520TSC%2520datasets.%2520These%250Afindings%2520underscore%2520the%2520robustness%252C%2520efficiency%252C%2520scalability%252C%2520and%2520effectiveness%250Aof%2520convolutional%2520kernels%2520in%2520capturing%2520temporal%2520features%2520in%2520time-series%2520data%252C%2520as%250Athree%2520out%2520of%2520the%2520top%2520four%2520performing%2520algorithms%2520leverage%2520these%2520kernels%2520for%250Afeature%2520extraction.%2520Additionally%252C%2520LSTM%252C%2520BiLSTM%252C%2520and%2520TS-LSTM%2520algorithms%2520deserve%250Arecognition%2520for%2520their%2520effectiveness%2520in%2520capturing%2520features%2520within%2520time-series%250Adata%2520using%2520RNN-based%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02812v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Series%20Classification%20in%20Smart%20Manufacturing%20Systems%3A%20An%0A%20%20Experimental%20Evaluation%20of%20State-of-the-Art%20Machine%20Learning%20Algorithms&entry.906535625=Mojtaba%20A.%20Farahani%20and%20M.%20R.%20McCormick%20and%20Ramy%20Harik%20and%20Thorsten%20Wuest&entry.1292438233=%20%20Manufacturing%20is%20gathering%20extensive%20amounts%20of%20diverse%20data%2C%20thanks%20to%20the%0Agrowing%20number%20of%20sensors%20and%20rapid%20advances%20in%20sensing%20technologies.%20Among%20the%0Avarious%20data%20types%20available%20in%20SMS%20settings%2C%20time-series%20data%20plays%20a%20pivotal%0Arole.%20Hence%2C%20TSC%20emerges%20is%20crucial%20in%20this%20domain.%20The%20objective%20of%20this%20study%0Ais%20to%20fill%20this%20gap%20by%20providing%20a%20rigorous%20experimental%20evaluation%20of%20the%20SoTA%0AML%20and%20DL%20algorithms%20for%20TSC%20tasks%20in%20manufacturing%20and%20industrial%20settings.%20We%0Afirst%20explored%20and%20compiled%20a%20comprehensive%20list%20of%20more%20than%2092%20SoTA%0Aalgorithms%20from%20both%20TSC%20and%20manufacturing%20literature.%20Following%2C%20we%20selected%0Athe%2036%20most%20representative%20algorithms%20from%20this%20list.%20To%20evaluate%20their%0Aperformance%20across%20various%20manufacturing%20classification%20tasks%2C%20we%20curated%20a%20set%0Aof%2022%20manufacturing%20datasets%2C%20representative%20of%20different%20characteristics%20that%0Acover%20diverse%20manufacturing%20problems.%20Subsequently%2C%20we%20implemented%20and%0Aevaluated%20the%20algorithms%20on%20the%20manufacturing%20benchmark%20datasets%2C%20and%20analyzed%0Athe%20results%20for%20each%20dataset.%20Based%20on%20the%20results%2C%20ResNet%2C%20DrCIF%2C%0AInceptionTime%2C%20and%20ARSENAL%20are%20the%20top-performing%20algorithms%2C%20boasting%20an%0Aaverage%20accuracy%20of%20over%2096.6%25%20across%20all%2022%20manufacturing%20TSC%20datasets.%20These%0Afindings%20underscore%20the%20robustness%2C%20efficiency%2C%20scalability%2C%20and%20effectiveness%0Aof%20convolutional%20kernels%20in%20capturing%20temporal%20features%20in%20time-series%20data%2C%20as%0Athree%20out%20of%20the%20top%20four%20performing%20algorithms%20leverage%20these%20kernels%20for%0Afeature%20extraction.%20Additionally%2C%20LSTM%2C%20BiLSTM%2C%20and%20TS-LSTM%20algorithms%20deserve%0Arecognition%20for%20their%20effectiveness%20in%20capturing%20features%20within%20time-series%0Adata%20using%20RNN-based%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02812v2&entry.124074799=Read"},
{"title": "U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs\n  Autonomous Flight", "author": "Tongtong Feng and Xin Wang and Feilin Han and Leping Zhang and Wenwu Zhu", "abstract": "  Modern perception systems for autonomous flight are sensitive to occlusion\nand have limited long-range capability, which is a key bottleneck in improving\nlow-altitude economic task performance. Recent research has shown that the\nUAV-to-UAV (U2U) cooperative perception system has great potential to\nrevolutionize the autonomous flight industry. However, the lack of a\nlarge-scale dataset is hindering progress in this area. This paper presents\nU2UData, the first large-scale cooperative perception dataset for swarm UAVs\nautonomous flight. The dataset was collected by three UAVs flying autonomously\nin the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames,\n945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes.\nIt also includes brightness, temperature, humidity, smoke, and airflow values\ncovering all flight routes. U2USim is the first real-world mapping swarm UAVs\nsimulation environment. It takes Yunnan Province as the prototype and includes\n4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two\nperception tasks: cooperative 3D object detection and cooperative 3D object\ntracking. This paper provides comprehensive benchmarks of recent cooperative\nperception algorithms on these tasks.\n", "link": "http://arxiv.org/abs/2408.00606v2", "date": "2024-08-05", "relevancy": 1.729, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U2UData%3A%20A%20Large-scale%20Cooperative%20Perception%20Dataset%20for%20Swarm%20UAVs%0A%20%20Autonomous%20Flight&body=Title%3A%20U2UData%3A%20A%20Large-scale%20Cooperative%20Perception%20Dataset%20for%20Swarm%20UAVs%0A%20%20Autonomous%20Flight%0AAuthor%3A%20Tongtong%20Feng%20and%20Xin%20Wang%20and%20Feilin%20Han%20and%20Leping%20Zhang%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Modern%20perception%20systems%20for%20autonomous%20flight%20are%20sensitive%20to%20occlusion%0Aand%20have%20limited%20long-range%20capability%2C%20which%20is%20a%20key%20bottleneck%20in%20improving%0Alow-altitude%20economic%20task%20performance.%20Recent%20research%20has%20shown%20that%20the%0AUAV-to-UAV%20%28U2U%29%20cooperative%20perception%20system%20has%20great%20potential%20to%0Arevolutionize%20the%20autonomous%20flight%20industry.%20However%2C%20the%20lack%20of%20a%0Alarge-scale%20dataset%20is%20hindering%20progress%20in%20this%20area.%20This%20paper%20presents%0AU2UData%2C%20the%20first%20large-scale%20cooperative%20perception%20dataset%20for%20swarm%20UAVs%0Aautonomous%20flight.%20The%20dataset%20was%20collected%20by%20three%20UAVs%20flying%20autonomously%0Ain%20the%20U2USim%2C%20covering%20a%209%20km%24%5E2%24%20flight%20area.%20It%20comprises%20315K%20LiDAR%20frames%2C%0A945K%20RGB%20and%20depth%20frames%2C%20and%202.41M%20annotated%203D%20bounding%20boxes%20for%203%20classes.%0AIt%20also%20includes%20brightness%2C%20temperature%2C%20humidity%2C%20smoke%2C%20and%20airflow%20values%0Acovering%20all%20flight%20routes.%20U2USim%20is%20the%20first%20real-world%20mapping%20swarm%20UAVs%0Asimulation%20environment.%20It%20takes%20Yunnan%20Province%20as%20the%20prototype%20and%20includes%0A4%20terrains%2C%207%20weather%20conditions%2C%20and%208%20sensor%20types.%20U2UData%20introduces%20two%0Aperception%20tasks%3A%20cooperative%203D%20object%20detection%20and%20cooperative%203D%20object%0Atracking.%20This%20paper%20provides%20comprehensive%20benchmarks%20of%20recent%20cooperative%0Aperception%20algorithms%20on%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU2UData%253A%2520A%2520Large-scale%2520Cooperative%2520Perception%2520Dataset%2520for%2520Swarm%2520UAVs%250A%2520%2520Autonomous%2520Flight%26entry.906535625%3DTongtong%2520Feng%2520and%2520Xin%2520Wang%2520and%2520Feilin%2520Han%2520and%2520Leping%2520Zhang%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Modern%2520perception%2520systems%2520for%2520autonomous%2520flight%2520are%2520sensitive%2520to%2520occlusion%250Aand%2520have%2520limited%2520long-range%2520capability%252C%2520which%2520is%2520a%2520key%2520bottleneck%2520in%2520improving%250Alow-altitude%2520economic%2520task%2520performance.%2520Recent%2520research%2520has%2520shown%2520that%2520the%250AUAV-to-UAV%2520%2528U2U%2529%2520cooperative%2520perception%2520system%2520has%2520great%2520potential%2520to%250Arevolutionize%2520the%2520autonomous%2520flight%2520industry.%2520However%252C%2520the%2520lack%2520of%2520a%250Alarge-scale%2520dataset%2520is%2520hindering%2520progress%2520in%2520this%2520area.%2520This%2520paper%2520presents%250AU2UData%252C%2520the%2520first%2520large-scale%2520cooperative%2520perception%2520dataset%2520for%2520swarm%2520UAVs%250Aautonomous%2520flight.%2520The%2520dataset%2520was%2520collected%2520by%2520three%2520UAVs%2520flying%2520autonomously%250Ain%2520the%2520U2USim%252C%2520covering%2520a%25209%2520km%2524%255E2%2524%2520flight%2520area.%2520It%2520comprises%2520315K%2520LiDAR%2520frames%252C%250A945K%2520RGB%2520and%2520depth%2520frames%252C%2520and%25202.41M%2520annotated%25203D%2520bounding%2520boxes%2520for%25203%2520classes.%250AIt%2520also%2520includes%2520brightness%252C%2520temperature%252C%2520humidity%252C%2520smoke%252C%2520and%2520airflow%2520values%250Acovering%2520all%2520flight%2520routes.%2520U2USim%2520is%2520the%2520first%2520real-world%2520mapping%2520swarm%2520UAVs%250Asimulation%2520environment.%2520It%2520takes%2520Yunnan%2520Province%2520as%2520the%2520prototype%2520and%2520includes%250A4%2520terrains%252C%25207%2520weather%2520conditions%252C%2520and%25208%2520sensor%2520types.%2520U2UData%2520introduces%2520two%250Aperception%2520tasks%253A%2520cooperative%25203D%2520object%2520detection%2520and%2520cooperative%25203D%2520object%250Atracking.%2520This%2520paper%2520provides%2520comprehensive%2520benchmarks%2520of%2520recent%2520cooperative%250Aperception%2520algorithms%2520on%2520these%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U2UData%3A%20A%20Large-scale%20Cooperative%20Perception%20Dataset%20for%20Swarm%20UAVs%0A%20%20Autonomous%20Flight&entry.906535625=Tongtong%20Feng%20and%20Xin%20Wang%20and%20Feilin%20Han%20and%20Leping%20Zhang%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Modern%20perception%20systems%20for%20autonomous%20flight%20are%20sensitive%20to%20occlusion%0Aand%20have%20limited%20long-range%20capability%2C%20which%20is%20a%20key%20bottleneck%20in%20improving%0Alow-altitude%20economic%20task%20performance.%20Recent%20research%20has%20shown%20that%20the%0AUAV-to-UAV%20%28U2U%29%20cooperative%20perception%20system%20has%20great%20potential%20to%0Arevolutionize%20the%20autonomous%20flight%20industry.%20However%2C%20the%20lack%20of%20a%0Alarge-scale%20dataset%20is%20hindering%20progress%20in%20this%20area.%20This%20paper%20presents%0AU2UData%2C%20the%20first%20large-scale%20cooperative%20perception%20dataset%20for%20swarm%20UAVs%0Aautonomous%20flight.%20The%20dataset%20was%20collected%20by%20three%20UAVs%20flying%20autonomously%0Ain%20the%20U2USim%2C%20covering%20a%209%20km%24%5E2%24%20flight%20area.%20It%20comprises%20315K%20LiDAR%20frames%2C%0A945K%20RGB%20and%20depth%20frames%2C%20and%202.41M%20annotated%203D%20bounding%20boxes%20for%203%20classes.%0AIt%20also%20includes%20brightness%2C%20temperature%2C%20humidity%2C%20smoke%2C%20and%20airflow%20values%0Acovering%20all%20flight%20routes.%20U2USim%20is%20the%20first%20real-world%20mapping%20swarm%20UAVs%0Asimulation%20environment.%20It%20takes%20Yunnan%20Province%20as%20the%20prototype%20and%20includes%0A4%20terrains%2C%207%20weather%20conditions%2C%20and%208%20sensor%20types.%20U2UData%20introduces%20two%0Aperception%20tasks%3A%20cooperative%203D%20object%20detection%20and%20cooperative%203D%20object%0Atracking.%20This%20paper%20provides%20comprehensive%20benchmarks%20of%20recent%20cooperative%0Aperception%20algorithms%20on%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00606v2&entry.124074799=Read"},
{"title": "Context-aware Mamba-based Reinforcement Learning for social robot\n  navigation", "author": "Syed Muhammad Mustafa and Omema Rizvi and Zain Ahmed Usmani and Abdul Basit Memon", "abstract": "  Social robot navigation (SRN) is a relevant problem that involves navigating\na pedestrian-rich environment in a socially acceptable manner. It is an\nessential part of making social robots effective in pedestrian-rich settings.\nThe use cases of such robots could vary from companion robots to warehouse\nrobots to autonomous wheelchairs. In recent years, deep reinforcement learning\nhas been increasingly used in research on social robot navigation. Our work\nintroduces CAMRL (Context-Aware Mamba-based Reinforcement Learning). Mamba is a\nnew deep learning-based State Space Model (SSM) that has achieved results\ncomparable to transformers in sequencing tasks. CAMRL uses Mamba to determine\nthe robot's next action, which maximizes the value of the next state predicted\nby the neural network, enabling the robot to navigate effectively based on the\nrewards assigned. We evaluate CAMRL alongside existing solutions (CADRL,\nLSTM-RL, SARL) using a rigorous testing dataset which involves a variety of\ndensities and environment behaviors based on ORCA and SFM, thus, demonstrating\nthat CAMRL achieves higher success rates, minimizes collisions, and maintains\nsafer distances from pedestrians. This work introduces a new SRN planner,\nshowcasing the potential for deep-state space models for robot navigation.\n", "link": "http://arxiv.org/abs/2408.02661v1", "date": "2024-08-05", "relevancy": 1.7218, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6032}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5991}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Mamba-based%20Reinforcement%20Learning%20for%20social%20robot%0A%20%20navigation&body=Title%3A%20Context-aware%20Mamba-based%20Reinforcement%20Learning%20for%20social%20robot%0A%20%20navigation%0AAuthor%3A%20Syed%20Muhammad%20Mustafa%20and%20Omema%20Rizvi%20and%20Zain%20Ahmed%20Usmani%20and%20Abdul%20Basit%20Memon%0AAbstract%3A%20%20%20Social%20robot%20navigation%20%28SRN%29%20is%20a%20relevant%20problem%20that%20involves%20navigating%0Aa%20pedestrian-rich%20environment%20in%20a%20socially%20acceptable%20manner.%20It%20is%20an%0Aessential%20part%20of%20making%20social%20robots%20effective%20in%20pedestrian-rich%20settings.%0AThe%20use%20cases%20of%20such%20robots%20could%20vary%20from%20companion%20robots%20to%20warehouse%0Arobots%20to%20autonomous%20wheelchairs.%20In%20recent%20years%2C%20deep%20reinforcement%20learning%0Ahas%20been%20increasingly%20used%20in%20research%20on%20social%20robot%20navigation.%20Our%20work%0Aintroduces%20CAMRL%20%28Context-Aware%20Mamba-based%20Reinforcement%20Learning%29.%20Mamba%20is%20a%0Anew%20deep%20learning-based%20State%20Space%20Model%20%28SSM%29%20that%20has%20achieved%20results%0Acomparable%20to%20transformers%20in%20sequencing%20tasks.%20CAMRL%20uses%20Mamba%20to%20determine%0Athe%20robot%27s%20next%20action%2C%20which%20maximizes%20the%20value%20of%20the%20next%20state%20predicted%0Aby%20the%20neural%20network%2C%20enabling%20the%20robot%20to%20navigate%20effectively%20based%20on%20the%0Arewards%20assigned.%20We%20evaluate%20CAMRL%20alongside%20existing%20solutions%20%28CADRL%2C%0ALSTM-RL%2C%20SARL%29%20using%20a%20rigorous%20testing%20dataset%20which%20involves%20a%20variety%20of%0Adensities%20and%20environment%20behaviors%20based%20on%20ORCA%20and%20SFM%2C%20thus%2C%20demonstrating%0Athat%20CAMRL%20achieves%20higher%20success%20rates%2C%20minimizes%20collisions%2C%20and%20maintains%0Asafer%20distances%20from%20pedestrians.%20This%20work%20introduces%20a%20new%20SRN%20planner%2C%0Ashowcasing%20the%20potential%20for%20deep-state%20space%20models%20for%20robot%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-aware%2520Mamba-based%2520Reinforcement%2520Learning%2520for%2520social%2520robot%250A%2520%2520navigation%26entry.906535625%3DSyed%2520Muhammad%2520Mustafa%2520and%2520Omema%2520Rizvi%2520and%2520Zain%2520Ahmed%2520Usmani%2520and%2520Abdul%2520Basit%2520Memon%26entry.1292438233%3D%2520%2520Social%2520robot%2520navigation%2520%2528SRN%2529%2520is%2520a%2520relevant%2520problem%2520that%2520involves%2520navigating%250Aa%2520pedestrian-rich%2520environment%2520in%2520a%2520socially%2520acceptable%2520manner.%2520It%2520is%2520an%250Aessential%2520part%2520of%2520making%2520social%2520robots%2520effective%2520in%2520pedestrian-rich%2520settings.%250AThe%2520use%2520cases%2520of%2520such%2520robots%2520could%2520vary%2520from%2520companion%2520robots%2520to%2520warehouse%250Arobots%2520to%2520autonomous%2520wheelchairs.%2520In%2520recent%2520years%252C%2520deep%2520reinforcement%2520learning%250Ahas%2520been%2520increasingly%2520used%2520in%2520research%2520on%2520social%2520robot%2520navigation.%2520Our%2520work%250Aintroduces%2520CAMRL%2520%2528Context-Aware%2520Mamba-based%2520Reinforcement%2520Learning%2529.%2520Mamba%2520is%2520a%250Anew%2520deep%2520learning-based%2520State%2520Space%2520Model%2520%2528SSM%2529%2520that%2520has%2520achieved%2520results%250Acomparable%2520to%2520transformers%2520in%2520sequencing%2520tasks.%2520CAMRL%2520uses%2520Mamba%2520to%2520determine%250Athe%2520robot%2527s%2520next%2520action%252C%2520which%2520maximizes%2520the%2520value%2520of%2520the%2520next%2520state%2520predicted%250Aby%2520the%2520neural%2520network%252C%2520enabling%2520the%2520robot%2520to%2520navigate%2520effectively%2520based%2520on%2520the%250Arewards%2520assigned.%2520We%2520evaluate%2520CAMRL%2520alongside%2520existing%2520solutions%2520%2528CADRL%252C%250ALSTM-RL%252C%2520SARL%2529%2520using%2520a%2520rigorous%2520testing%2520dataset%2520which%2520involves%2520a%2520variety%2520of%250Adensities%2520and%2520environment%2520behaviors%2520based%2520on%2520ORCA%2520and%2520SFM%252C%2520thus%252C%2520demonstrating%250Athat%2520CAMRL%2520achieves%2520higher%2520success%2520rates%252C%2520minimizes%2520collisions%252C%2520and%2520maintains%250Asafer%2520distances%2520from%2520pedestrians.%2520This%2520work%2520introduces%2520a%2520new%2520SRN%2520planner%252C%250Ashowcasing%2520the%2520potential%2520for%2520deep-state%2520space%2520models%2520for%2520robot%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Mamba-based%20Reinforcement%20Learning%20for%20social%20robot%0A%20%20navigation&entry.906535625=Syed%20Muhammad%20Mustafa%20and%20Omema%20Rizvi%20and%20Zain%20Ahmed%20Usmani%20and%20Abdul%20Basit%20Memon&entry.1292438233=%20%20Social%20robot%20navigation%20%28SRN%29%20is%20a%20relevant%20problem%20that%20involves%20navigating%0Aa%20pedestrian-rich%20environment%20in%20a%20socially%20acceptable%20manner.%20It%20is%20an%0Aessential%20part%20of%20making%20social%20robots%20effective%20in%20pedestrian-rich%20settings.%0AThe%20use%20cases%20of%20such%20robots%20could%20vary%20from%20companion%20robots%20to%20warehouse%0Arobots%20to%20autonomous%20wheelchairs.%20In%20recent%20years%2C%20deep%20reinforcement%20learning%0Ahas%20been%20increasingly%20used%20in%20research%20on%20social%20robot%20navigation.%20Our%20work%0Aintroduces%20CAMRL%20%28Context-Aware%20Mamba-based%20Reinforcement%20Learning%29.%20Mamba%20is%20a%0Anew%20deep%20learning-based%20State%20Space%20Model%20%28SSM%29%20that%20has%20achieved%20results%0Acomparable%20to%20transformers%20in%20sequencing%20tasks.%20CAMRL%20uses%20Mamba%20to%20determine%0Athe%20robot%27s%20next%20action%2C%20which%20maximizes%20the%20value%20of%20the%20next%20state%20predicted%0Aby%20the%20neural%20network%2C%20enabling%20the%20robot%20to%20navigate%20effectively%20based%20on%20the%0Arewards%20assigned.%20We%20evaluate%20CAMRL%20alongside%20existing%20solutions%20%28CADRL%2C%0ALSTM-RL%2C%20SARL%29%20using%20a%20rigorous%20testing%20dataset%20which%20involves%20a%20variety%20of%0Adensities%20and%20environment%20behaviors%20based%20on%20ORCA%20and%20SFM%2C%20thus%2C%20demonstrating%0Athat%20CAMRL%20achieves%20higher%20success%20rates%2C%20minimizes%20collisions%2C%20and%20maintains%0Asafer%20distances%20from%20pedestrians.%20This%20work%20introduces%20a%20new%20SRN%20planner%2C%0Ashowcasing%20the%20potential%20for%20deep-state%20space%20models%20for%20robot%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02661v1&entry.124074799=Read"},
{"title": "Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification", "author": "Pawe\u0142 Zyblewski and Leandro L. Minku", "abstract": "  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n", "link": "http://arxiv.org/abs/2408.02568v1", "date": "2024-08-05", "relevancy": 1.7079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5894}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.547}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modality%20Clustering-based%20Self-Labeling%20for%20Multimodal%20Data%0A%20%20Classification&body=Title%3A%20Cross-Modality%20Clustering-based%20Self-Labeling%20for%20Multimodal%20Data%0A%20%20Classification%0AAuthor%3A%20Pawe%C5%82%20Zyblewski%20and%20Leandro%20L.%20Minku%0AAbstract%3A%20%20%20Technological%20advances%20facilitate%20the%20ability%20to%20acquire%20multimodal%20data%2C%0Aposing%20a%20challenge%20for%20recognition%20systems%20while%20also%20providing%20an%20opportunity%0Ato%20use%20the%20heterogeneous%20nature%20of%20the%20information%20to%20increase%20the%0Ageneralization%20capability%20of%20models.%20An%20often%20overlooked%20issue%20is%20the%20cost%20of%0Athe%20labeling%20process%2C%20which%20is%20typically%20high%20due%20to%20the%20need%20for%20a%20significant%0Ainvestment%20in%20time%20and%20money%20associated%20with%20human%20experts.%20Existing%0Asemi-supervised%20learning%20methods%20often%20focus%20on%20operating%20in%20the%20feature%20space%0Acreated%20by%20the%20fusion%20of%20available%20modalities%2C%20neglecting%20the%20potential%20for%0Across-utilizing%20complementary%20information%20available%20in%20each%20modality.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Cross-Modality%20Clustering-based%20Self-Labeling%0A%28CMCSL%29.%20Based%20on%20a%20small%20set%20of%20pre-labeled%20data%2C%20CMCSL%20groups%20instances%0Abelonging%20to%20each%20modality%20in%20the%20deep%20feature%20space%20and%20then%20propagates%20known%0Alabels%20within%20the%20resulting%20clusters.%20Next%2C%20information%20about%20the%20instances%27%0Aclass%20membership%20in%20each%20modality%20is%20exchanged%20based%20on%20the%20Euclidean%20distance%0Ato%20ensure%20more%20accurate%20labeling.%20Experimental%20evaluation%20conducted%20on%2020%0Adatasets%20derived%20from%20the%20MM-IMDb%20dataset%20indicates%20that%20cross-propagation%20of%0Alabels%20between%20modalities%20--%20especially%20when%20the%20number%20of%20pre-labeled%0Ainstances%20is%20small%20--%20can%20allow%20for%20more%20reliable%20labeling%20and%20thus%20increase%0Athe%20classification%20performance%20in%20each%20modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modality%2520Clustering-based%2520Self-Labeling%2520for%2520Multimodal%2520Data%250A%2520%2520Classification%26entry.906535625%3DPawe%25C5%2582%2520Zyblewski%2520and%2520Leandro%2520L.%2520Minku%26entry.1292438233%3D%2520%2520Technological%2520advances%2520facilitate%2520the%2520ability%2520to%2520acquire%2520multimodal%2520data%252C%250Aposing%2520a%2520challenge%2520for%2520recognition%2520systems%2520while%2520also%2520providing%2520an%2520opportunity%250Ato%2520use%2520the%2520heterogeneous%2520nature%2520of%2520the%2520information%2520to%2520increase%2520the%250Ageneralization%2520capability%2520of%2520models.%2520An%2520often%2520overlooked%2520issue%2520is%2520the%2520cost%2520of%250Athe%2520labeling%2520process%252C%2520which%2520is%2520typically%2520high%2520due%2520to%2520the%2520need%2520for%2520a%2520significant%250Ainvestment%2520in%2520time%2520and%2520money%2520associated%2520with%2520human%2520experts.%2520Existing%250Asemi-supervised%2520learning%2520methods%2520often%2520focus%2520on%2520operating%2520in%2520the%2520feature%2520space%250Acreated%2520by%2520the%2520fusion%2520of%2520available%2520modalities%252C%2520neglecting%2520the%2520potential%2520for%250Across-utilizing%2520complementary%2520information%2520available%2520in%2520each%2520modality.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520Cross-Modality%2520Clustering-based%2520Self-Labeling%250A%2528CMCSL%2529.%2520Based%2520on%2520a%2520small%2520set%2520of%2520pre-labeled%2520data%252C%2520CMCSL%2520groups%2520instances%250Abelonging%2520to%2520each%2520modality%2520in%2520the%2520deep%2520feature%2520space%2520and%2520then%2520propagates%2520known%250Alabels%2520within%2520the%2520resulting%2520clusters.%2520Next%252C%2520information%2520about%2520the%2520instances%2527%250Aclass%2520membership%2520in%2520each%2520modality%2520is%2520exchanged%2520based%2520on%2520the%2520Euclidean%2520distance%250Ato%2520ensure%2520more%2520accurate%2520labeling.%2520Experimental%2520evaluation%2520conducted%2520on%252020%250Adatasets%2520derived%2520from%2520the%2520MM-IMDb%2520dataset%2520indicates%2520that%2520cross-propagation%2520of%250Alabels%2520between%2520modalities%2520--%2520especially%2520when%2520the%2520number%2520of%2520pre-labeled%250Ainstances%2520is%2520small%2520--%2520can%2520allow%2520for%2520more%2520reliable%2520labeling%2520and%2520thus%2520increase%250Athe%2520classification%2520performance%2520in%2520each%2520modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modality%20Clustering-based%20Self-Labeling%20for%20Multimodal%20Data%0A%20%20Classification&entry.906535625=Pawe%C5%82%20Zyblewski%20and%20Leandro%20L.%20Minku&entry.1292438233=%20%20Technological%20advances%20facilitate%20the%20ability%20to%20acquire%20multimodal%20data%2C%0Aposing%20a%20challenge%20for%20recognition%20systems%20while%20also%20providing%20an%20opportunity%0Ato%20use%20the%20heterogeneous%20nature%20of%20the%20information%20to%20increase%20the%0Ageneralization%20capability%20of%20models.%20An%20often%20overlooked%20issue%20is%20the%20cost%20of%0Athe%20labeling%20process%2C%20which%20is%20typically%20high%20due%20to%20the%20need%20for%20a%20significant%0Ainvestment%20in%20time%20and%20money%20associated%20with%20human%20experts.%20Existing%0Asemi-supervised%20learning%20methods%20often%20focus%20on%20operating%20in%20the%20feature%20space%0Acreated%20by%20the%20fusion%20of%20available%20modalities%2C%20neglecting%20the%20potential%20for%0Across-utilizing%20complementary%20information%20available%20in%20each%20modality.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Cross-Modality%20Clustering-based%20Self-Labeling%0A%28CMCSL%29.%20Based%20on%20a%20small%20set%20of%20pre-labeled%20data%2C%20CMCSL%20groups%20instances%0Abelonging%20to%20each%20modality%20in%20the%20deep%20feature%20space%20and%20then%20propagates%20known%0Alabels%20within%20the%20resulting%20clusters.%20Next%2C%20information%20about%20the%20instances%27%0Aclass%20membership%20in%20each%20modality%20is%20exchanged%20based%20on%20the%20Euclidean%20distance%0Ato%20ensure%20more%20accurate%20labeling.%20Experimental%20evaluation%20conducted%20on%2020%0Adatasets%20derived%20from%20the%20MM-IMDb%20dataset%20indicates%20that%20cross-propagation%20of%0Alabels%20between%20modalities%20--%20especially%20when%20the%20number%20of%20pre-labeled%0Ainstances%20is%20small%20--%20can%20allow%20for%20more%20reliable%20labeling%20and%20thus%20increase%0Athe%20classification%20performance%20in%20each%20modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02568v1&entry.124074799=Read"},
{"title": "Fast Partition-Based Cross-Validation With Centering and Scaling for\n  $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$", "author": "Ole-Christian Galbo Engstr\u00f8m and Martin Holm Jensen", "abstract": "  We present algorithms that substantially accelerate partition-based\ncross-validation for machine learning models that require matrix products\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Our\nalgorithms have applications in model selection for, e.g., principal component\nanalysis (PCA), principal component regression (PCR), ridge regression (RR),\nordinary least squares (OLS), and partial least squares (PLS). Our algorithms\nsupport all combinations of column-wise centering and scaling of $\\mathbf{X}$\nand $\\mathbf{Y}$, and we demonstrate in our accompanying implementation that\nthis adds only a manageable, practical constant over efficient variants without\npreprocessing. We prove the correctness of our algorithms under a fold-based\npartitioning scheme and show that the running time is independent of the number\nof folds; that is, they have the same time complexity as that of computing\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ and\nspace complexity equivalent to storing $\\mathbf{X}$, $\\mathbf{Y}$,\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$, and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$.\nImportantly, unlike alternatives found in the literature, we avoid data leakage\ndue to preprocessing. We achieve these results by eliminating redundant\ncomputations in the overlap between training partitions. Concretely, we show\nhow to manipulate $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and\n$\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ using only samples from the validation\npartition to obtain the preprocessed training partition-wise\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. To our\nknowledge, we are the first to derive correct and efficient cross-validation\nalgorithms for any of the $16$ combinations of column-wise centering and\nscaling, for which we also prove only $12$ give distinct matrix products.\n", "link": "http://arxiv.org/abs/2401.13185v2", "date": "2024-08-05", "relevancy": 1.7039, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4356}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4308}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Partition-Based%20Cross-Validation%20With%20Centering%20and%20Scaling%20for%0A%20%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24&body=Title%3A%20Fast%20Partition-Based%20Cross-Validation%20With%20Centering%20and%20Scaling%20for%0A%20%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24%0AAuthor%3A%20Ole-Christian%20Galbo%20Engstr%C3%B8m%20and%20Martin%20Holm%20Jensen%0AAbstract%3A%20%20%20We%20present%20algorithms%20that%20substantially%20accelerate%20partition-based%0Across-validation%20for%20machine%20learning%20models%20that%20require%20matrix%20products%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24.%20Our%0Aalgorithms%20have%20applications%20in%20model%20selection%20for%2C%20e.g.%2C%20principal%20component%0Aanalysis%20%28PCA%29%2C%20principal%20component%20regression%20%28PCR%29%2C%20ridge%20regression%20%28RR%29%2C%0Aordinary%20least%20squares%20%28OLS%29%2C%20and%20partial%20least%20squares%20%28PLS%29.%20Our%20algorithms%0Asupport%20all%20combinations%20of%20column-wise%20centering%20and%20scaling%20of%20%24%5Cmathbf%7BX%7D%24%0Aand%20%24%5Cmathbf%7BY%7D%24%2C%20and%20we%20demonstrate%20in%20our%20accompanying%20implementation%20that%0Athis%20adds%20only%20a%20manageable%2C%20practical%20constant%20over%20efficient%20variants%20without%0Apreprocessing.%20We%20prove%20the%20correctness%20of%20our%20algorithms%20under%20a%20fold-based%0Apartitioning%20scheme%20and%20show%20that%20the%20running%20time%20is%20independent%20of%20the%20number%0Aof%20folds%3B%20that%20is%2C%20they%20have%20the%20same%20time%20complexity%20as%20that%20of%20computing%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24%20and%0Aspace%20complexity%20equivalent%20to%20storing%20%24%5Cmathbf%7BX%7D%24%2C%20%24%5Cmathbf%7BY%7D%24%2C%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%2C%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24.%0AImportantly%2C%20unlike%20alternatives%20found%20in%20the%20literature%2C%20we%20avoid%20data%20leakage%0Adue%20to%20preprocessing.%20We%20achieve%20these%20results%20by%20eliminating%20redundant%0Acomputations%20in%20the%20overlap%20between%20training%20partitions.%20Concretely%2C%20we%20show%0Ahow%20to%20manipulate%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24%20using%20only%20samples%20from%20the%20validation%0Apartition%20to%20obtain%20the%20preprocessed%20training%20partition-wise%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24.%20To%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20derive%20correct%20and%20efficient%20cross-validation%0Aalgorithms%20for%20any%20of%20the%20%2416%24%20combinations%20of%20column-wise%20centering%20and%0Ascaling%2C%20for%20which%20we%20also%20prove%20only%20%2412%24%20give%20distinct%20matrix%20products.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13185v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Partition-Based%2520Cross-Validation%2520With%2520Centering%2520and%2520Scaling%2520for%250A%2520%2520%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BX%257D%2524%2520and%2520%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BY%257D%2524%26entry.906535625%3DOle-Christian%2520Galbo%2520Engstr%25C3%25B8m%2520and%2520Martin%2520Holm%2520Jensen%26entry.1292438233%3D%2520%2520We%2520present%2520algorithms%2520that%2520substantially%2520accelerate%2520partition-based%250Across-validation%2520for%2520machine%2520learning%2520models%2520that%2520require%2520matrix%2520products%250A%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BX%257D%2524%2520and%2520%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BY%257D%2524.%2520Our%250Aalgorithms%2520have%2520applications%2520in%2520model%2520selection%2520for%252C%2520e.g.%252C%2520principal%2520component%250Aanalysis%2520%2528PCA%2529%252C%2520principal%2520component%2520regression%2520%2528PCR%2529%252C%2520ridge%2520regression%2520%2528RR%2529%252C%250Aordinary%2520least%2520squares%2520%2528OLS%2529%252C%2520and%2520partial%2520least%2520squares%2520%2528PLS%2529.%2520Our%2520algorithms%250Asupport%2520all%2520combinations%2520of%2520column-wise%2520centering%2520and%2520scaling%2520of%2520%2524%255Cmathbf%257BX%257D%2524%250Aand%2520%2524%255Cmathbf%257BY%257D%2524%252C%2520and%2520we%2520demonstrate%2520in%2520our%2520accompanying%2520implementation%2520that%250Athis%2520adds%2520only%2520a%2520manageable%252C%2520practical%2520constant%2520over%2520efficient%2520variants%2520without%250Apreprocessing.%2520We%2520prove%2520the%2520correctness%2520of%2520our%2520algorithms%2520under%2520a%2520fold-based%250Apartitioning%2520scheme%2520and%2520show%2520that%2520the%2520running%2520time%2520is%2520independent%2520of%2520the%2520number%250Aof%2520folds%253B%2520that%2520is%252C%2520they%2520have%2520the%2520same%2520time%2520complexity%2520as%2520that%2520of%2520computing%250A%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BX%257D%2524%2520and%2520%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BY%257D%2524%2520and%250Aspace%2520complexity%2520equivalent%2520to%2520storing%2520%2524%255Cmathbf%257BX%257D%2524%252C%2520%2524%255Cmathbf%257BY%257D%2524%252C%250A%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BX%257D%2524%252C%2520and%2520%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BY%257D%2524.%250AImportantly%252C%2520unlike%2520alternatives%2520found%2520in%2520the%2520literature%252C%2520we%2520avoid%2520data%2520leakage%250Adue%2520to%2520preprocessing.%2520We%2520achieve%2520these%2520results%2520by%2520eliminating%2520redundant%250Acomputations%2520in%2520the%2520overlap%2520between%2520training%2520partitions.%2520Concretely%252C%2520we%2520show%250Ahow%2520to%2520manipulate%2520%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BX%257D%2524%2520and%250A%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BY%257D%2524%2520using%2520only%2520samples%2520from%2520the%2520validation%250Apartition%2520to%2520obtain%2520the%2520preprocessed%2520training%2520partition-wise%250A%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BX%257D%2524%2520and%2520%2524%255Cmathbf%257BX%257D%255E%255Cmathbf%257BT%257D%255Cmathbf%257BY%257D%2524.%2520To%2520our%250Aknowledge%252C%2520we%2520are%2520the%2520first%2520to%2520derive%2520correct%2520and%2520efficient%2520cross-validation%250Aalgorithms%2520for%2520any%2520of%2520the%2520%252416%2524%2520combinations%2520of%2520column-wise%2520centering%2520and%250Ascaling%252C%2520for%2520which%2520we%2520also%2520prove%2520only%2520%252412%2524%2520give%2520distinct%2520matrix%2520products.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13185v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Partition-Based%20Cross-Validation%20With%20Centering%20and%20Scaling%20for%0A%20%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24&entry.906535625=Ole-Christian%20Galbo%20Engstr%C3%B8m%20and%20Martin%20Holm%20Jensen&entry.1292438233=%20%20We%20present%20algorithms%20that%20substantially%20accelerate%20partition-based%0Across-validation%20for%20machine%20learning%20models%20that%20require%20matrix%20products%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24.%20Our%0Aalgorithms%20have%20applications%20in%20model%20selection%20for%2C%20e.g.%2C%20principal%20component%0Aanalysis%20%28PCA%29%2C%20principal%20component%20regression%20%28PCR%29%2C%20ridge%20regression%20%28RR%29%2C%0Aordinary%20least%20squares%20%28OLS%29%2C%20and%20partial%20least%20squares%20%28PLS%29.%20Our%20algorithms%0Asupport%20all%20combinations%20of%20column-wise%20centering%20and%20scaling%20of%20%24%5Cmathbf%7BX%7D%24%0Aand%20%24%5Cmathbf%7BY%7D%24%2C%20and%20we%20demonstrate%20in%20our%20accompanying%20implementation%20that%0Athis%20adds%20only%20a%20manageable%2C%20practical%20constant%20over%20efficient%20variants%20without%0Apreprocessing.%20We%20prove%20the%20correctness%20of%20our%20algorithms%20under%20a%20fold-based%0Apartitioning%20scheme%20and%20show%20that%20the%20running%20time%20is%20independent%20of%20the%20number%0Aof%20folds%3B%20that%20is%2C%20they%20have%20the%20same%20time%20complexity%20as%20that%20of%20computing%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24%20and%0Aspace%20complexity%20equivalent%20to%20storing%20%24%5Cmathbf%7BX%7D%24%2C%20%24%5Cmathbf%7BY%7D%24%2C%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%2C%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24.%0AImportantly%2C%20unlike%20alternatives%20found%20in%20the%20literature%2C%20we%20avoid%20data%20leakage%0Adue%20to%20preprocessing.%20We%20achieve%20these%20results%20by%20eliminating%20redundant%0Acomputations%20in%20the%20overlap%20between%20training%20partitions.%20Concretely%2C%20we%20show%0Ahow%20to%20manipulate%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24%20using%20only%20samples%20from%20the%20validation%0Apartition%20to%20obtain%20the%20preprocessed%20training%20partition-wise%0A%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BX%7D%24%20and%20%24%5Cmathbf%7BX%7D%5E%5Cmathbf%7BT%7D%5Cmathbf%7BY%7D%24.%20To%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20derive%20correct%20and%20efficient%20cross-validation%0Aalgorithms%20for%20any%20of%20the%20%2416%24%20combinations%20of%20column-wise%20centering%20and%0Ascaling%2C%20for%20which%20we%20also%20prove%20only%20%2412%24%20give%20distinct%20matrix%20products.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13185v2&entry.124074799=Read"},
{"title": "Multi-weather Cross-view Geo-localization Using Denoising Diffusion\n  Models", "author": "Tongtong Feng and Qing Li and Xin Wang and Mingzi Wang and Guangyao Li and Wenwu Zhu", "abstract": "  Cross-view geo-localization in GNSS-denied environments aims to determine an\nunknown location by matching drone-view images with the correct geo-tagged\nsatellite-view images from a large gallery. Recent research shows that learning\ndiscriminative image representations under specific weather conditions can\nsignificantly enhance performance. However, the frequent occurrence of unseen\nextreme weather conditions hinders progress. This paper introduces MCGF, a\nMulti-weather Cross-view Geo-localization Framework designed to dynamically\nadapt to unseen weather conditions. MCGF establishes a joint optimization\nbetween image restoration and geo-localization using denoising diffusion\nmodels. For image restoration, MCGF incorporates a shared encoder and a\nlightweight restoration module to help the backbone eliminate weather-specific\ninformation. For geo-localization, MCGF uses EVA-02 as a backbone for feature\nextraction, with cross-entropy loss for training and cosine distance for\ntesting. Extensive experiments on University160k-WX demonstrate that MCGF\nachieves competitive results for geo-localization in varying weather\nconditions.\n", "link": "http://arxiv.org/abs/2408.02408v1", "date": "2024-08-05", "relevancy": 1.6975, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5809}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5625}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-weather%20Cross-view%20Geo-localization%20Using%20Denoising%20Diffusion%0A%20%20Models&body=Title%3A%20Multi-weather%20Cross-view%20Geo-localization%20Using%20Denoising%20Diffusion%0A%20%20Models%0AAuthor%3A%20Tongtong%20Feng%20and%20Qing%20Li%20and%20Xin%20Wang%20and%20Mingzi%20Wang%20and%20Guangyao%20Li%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Cross-view%20geo-localization%20in%20GNSS-denied%20environments%20aims%20to%20determine%20an%0Aunknown%20location%20by%20matching%20drone-view%20images%20with%20the%20correct%20geo-tagged%0Asatellite-view%20images%20from%20a%20large%20gallery.%20Recent%20research%20shows%20that%20learning%0Adiscriminative%20image%20representations%20under%20specific%20weather%20conditions%20can%0Asignificantly%20enhance%20performance.%20However%2C%20the%20frequent%20occurrence%20of%20unseen%0Aextreme%20weather%20conditions%20hinders%20progress.%20This%20paper%20introduces%20MCGF%2C%20a%0AMulti-weather%20Cross-view%20Geo-localization%20Framework%20designed%20to%20dynamically%0Aadapt%20to%20unseen%20weather%20conditions.%20MCGF%20establishes%20a%20joint%20optimization%0Abetween%20image%20restoration%20and%20geo-localization%20using%20denoising%20diffusion%0Amodels.%20For%20image%20restoration%2C%20MCGF%20incorporates%20a%20shared%20encoder%20and%20a%0Alightweight%20restoration%20module%20to%20help%20the%20backbone%20eliminate%20weather-specific%0Ainformation.%20For%20geo-localization%2C%20MCGF%20uses%20EVA-02%20as%20a%20backbone%20for%20feature%0Aextraction%2C%20with%20cross-entropy%20loss%20for%20training%20and%20cosine%20distance%20for%0Atesting.%20Extensive%20experiments%20on%20University160k-WX%20demonstrate%20that%20MCGF%0Aachieves%20competitive%20results%20for%20geo-localization%20in%20varying%20weather%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-weather%2520Cross-view%2520Geo-localization%2520Using%2520Denoising%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DTongtong%2520Feng%2520and%2520Qing%2520Li%2520and%2520Xin%2520Wang%2520and%2520Mingzi%2520Wang%2520and%2520Guangyao%2520Li%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Cross-view%2520geo-localization%2520in%2520GNSS-denied%2520environments%2520aims%2520to%2520determine%2520an%250Aunknown%2520location%2520by%2520matching%2520drone-view%2520images%2520with%2520the%2520correct%2520geo-tagged%250Asatellite-view%2520images%2520from%2520a%2520large%2520gallery.%2520Recent%2520research%2520shows%2520that%2520learning%250Adiscriminative%2520image%2520representations%2520under%2520specific%2520weather%2520conditions%2520can%250Asignificantly%2520enhance%2520performance.%2520However%252C%2520the%2520frequent%2520occurrence%2520of%2520unseen%250Aextreme%2520weather%2520conditions%2520hinders%2520progress.%2520This%2520paper%2520introduces%2520MCGF%252C%2520a%250AMulti-weather%2520Cross-view%2520Geo-localization%2520Framework%2520designed%2520to%2520dynamically%250Aadapt%2520to%2520unseen%2520weather%2520conditions.%2520MCGF%2520establishes%2520a%2520joint%2520optimization%250Abetween%2520image%2520restoration%2520and%2520geo-localization%2520using%2520denoising%2520diffusion%250Amodels.%2520For%2520image%2520restoration%252C%2520MCGF%2520incorporates%2520a%2520shared%2520encoder%2520and%2520a%250Alightweight%2520restoration%2520module%2520to%2520help%2520the%2520backbone%2520eliminate%2520weather-specific%250Ainformation.%2520For%2520geo-localization%252C%2520MCGF%2520uses%2520EVA-02%2520as%2520a%2520backbone%2520for%2520feature%250Aextraction%252C%2520with%2520cross-entropy%2520loss%2520for%2520training%2520and%2520cosine%2520distance%2520for%250Atesting.%2520Extensive%2520experiments%2520on%2520University160k-WX%2520demonstrate%2520that%2520MCGF%250Aachieves%2520competitive%2520results%2520for%2520geo-localization%2520in%2520varying%2520weather%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-weather%20Cross-view%20Geo-localization%20Using%20Denoising%20Diffusion%0A%20%20Models&entry.906535625=Tongtong%20Feng%20and%20Qing%20Li%20and%20Xin%20Wang%20and%20Mingzi%20Wang%20and%20Guangyao%20Li%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Cross-view%20geo-localization%20in%20GNSS-denied%20environments%20aims%20to%20determine%20an%0Aunknown%20location%20by%20matching%20drone-view%20images%20with%20the%20correct%20geo-tagged%0Asatellite-view%20images%20from%20a%20large%20gallery.%20Recent%20research%20shows%20that%20learning%0Adiscriminative%20image%20representations%20under%20specific%20weather%20conditions%20can%0Asignificantly%20enhance%20performance.%20However%2C%20the%20frequent%20occurrence%20of%20unseen%0Aextreme%20weather%20conditions%20hinders%20progress.%20This%20paper%20introduces%20MCGF%2C%20a%0AMulti-weather%20Cross-view%20Geo-localization%20Framework%20designed%20to%20dynamically%0Aadapt%20to%20unseen%20weather%20conditions.%20MCGF%20establishes%20a%20joint%20optimization%0Abetween%20image%20restoration%20and%20geo-localization%20using%20denoising%20diffusion%0Amodels.%20For%20image%20restoration%2C%20MCGF%20incorporates%20a%20shared%20encoder%20and%20a%0Alightweight%20restoration%20module%20to%20help%20the%20backbone%20eliminate%20weather-specific%0Ainformation.%20For%20geo-localization%2C%20MCGF%20uses%20EVA-02%20as%20a%20backbone%20for%20feature%0Aextraction%2C%20with%20cross-entropy%20loss%20for%20training%20and%20cosine%20distance%20for%0Atesting.%20Extensive%20experiments%20on%20University160k-WX%20demonstrate%20that%20MCGF%0Aachieves%20competitive%20results%20for%20geo-localization%20in%20varying%20weather%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02408v1&entry.124074799=Read"},
{"title": "Annotation Cost-Efficient Active Learning for Deep Metric Learning\n  Driven Remote Sensing Image Retrieval", "author": "Genc Hoxha and Gencer Sumbul and Julia Henkel and Lars M\u00f6llenbrok and Beg\u00fcm Demir", "abstract": "  Deep metric learning (DML) has shown to be effective for content-based image\nretrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a\nhigh number of annotated images to accurately learn model parameters of deep\nneural networks (DNNs). However, gathering such data is time-consuming and\ncostly. To address this, we propose an annotation cost-efficient active\nlearning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to\ncreate a small but informative training set made up of similar and dissimilar\nimage pairs to be utilized for accurately learning a metric space. The\ninformativeness of image pairs is evaluated by combining uncertainty and\ndiversity criteria. To assess the uncertainty of image pairs, we introduce two\nalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary\nclassifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically\nestimates a threshold value that acts as a boundary between similar and\ndissimilar image pairs based on the distances in the metric space. The closer\nthe similarity between image pairs is to the estimated threshold value the\nhigher their uncertainty. BCGUE algorithm estimates the uncertainty of the\nimage pairs based on the confidence of the classifier in assigning correct\nsimilarity labels. The diversity criterion is assessed through a\nclustering-based strategy. ANNEAL combines either MGUE or BCGUE algorithm with\nthe clustering-based strategy to select the most informative image pairs, which\nare then labelled by expert annotators as similar or dissimilar. This way of\nannotating images significantly reduces the annotation cost compared to\nannotating images with land-use land-cover class labels. Experimental results\non two RS benchmark datasets demonstrate the effectiveness of our method. The\ncode of this work is publicly available at\n\\url{https://git.tu-berlin.de/rsim/anneal_tgrs}.\n", "link": "http://arxiv.org/abs/2406.10107v2", "date": "2024-08-05", "relevancy": 1.6947, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.607}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annotation%20Cost-Efficient%20Active%20Learning%20for%20Deep%20Metric%20Learning%0A%20%20Driven%20Remote%20Sensing%20Image%20Retrieval&body=Title%3A%20Annotation%20Cost-Efficient%20Active%20Learning%20for%20Deep%20Metric%20Learning%0A%20%20Driven%20Remote%20Sensing%20Image%20Retrieval%0AAuthor%3A%20Genc%20Hoxha%20and%20Gencer%20Sumbul%20and%20Julia%20Henkel%20and%20Lars%20M%C3%B6llenbrok%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Deep%20metric%20learning%20%28DML%29%20has%20shown%20to%20be%20effective%20for%20content-based%20image%0Aretrieval%20%28CBIR%29%20in%20remote%20sensing%20%28RS%29.%20Most%20of%20DML%20methods%20for%20CBIR%20rely%20on%20a%0Ahigh%20number%20of%20annotated%20images%20to%20accurately%20learn%20model%20parameters%20of%20deep%0Aneural%20networks%20%28DNNs%29.%20However%2C%20gathering%20such%20data%20is%20time-consuming%20and%0Acostly.%20To%20address%20this%2C%20we%20propose%20an%20annotation%20cost-efficient%20active%0Alearning%20%28ANNEAL%29%20method%20tailored%20to%20DML-driven%20CBIR%20in%20RS.%20ANNEAL%20aims%20to%0Acreate%20a%20small%20but%20informative%20training%20set%20made%20up%20of%20similar%20and%20dissimilar%0Aimage%20pairs%20to%20be%20utilized%20for%20accurately%20learning%20a%20metric%20space.%20The%0Ainformativeness%20of%20image%20pairs%20is%20evaluated%20by%20combining%20uncertainty%20and%0Adiversity%20criteria.%20To%20assess%20the%20uncertainty%20of%20image%20pairs%2C%20we%20introduce%20two%0Aalgorithms%3A%201%29%20metric-guided%20uncertainty%20estimation%20%28MGUE%29%3B%20and%202%29%20binary%0Aclassifier%20guided%20uncertainty%20estimation%20%28BCGUE%29.%20MGUE%20algorithm%20automatically%0Aestimates%20a%20threshold%20value%20that%20acts%20as%20a%20boundary%20between%20similar%20and%0Adissimilar%20image%20pairs%20based%20on%20the%20distances%20in%20the%20metric%20space.%20The%20closer%0Athe%20similarity%20between%20image%20pairs%20is%20to%20the%20estimated%20threshold%20value%20the%0Ahigher%20their%20uncertainty.%20BCGUE%20algorithm%20estimates%20the%20uncertainty%20of%20the%0Aimage%20pairs%20based%20on%20the%20confidence%20of%20the%20classifier%20in%20assigning%20correct%0Asimilarity%20labels.%20The%20diversity%20criterion%20is%20assessed%20through%20a%0Aclustering-based%20strategy.%20ANNEAL%20combines%20either%20MGUE%20or%20BCGUE%20algorithm%20with%0Athe%20clustering-based%20strategy%20to%20select%20the%20most%20informative%20image%20pairs%2C%20which%0Aare%20then%20labelled%20by%20expert%20annotators%20as%20similar%20or%20dissimilar.%20This%20way%20of%0Aannotating%20images%20significantly%20reduces%20the%20annotation%20cost%20compared%20to%0Aannotating%20images%20with%20land-use%20land-cover%20class%20labels.%20Experimental%20results%0Aon%20two%20RS%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20method.%20The%0Acode%20of%20this%20work%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//git.tu-berlin.de/rsim/anneal_tgrs%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnotation%2520Cost-Efficient%2520Active%2520Learning%2520for%2520Deep%2520Metric%2520Learning%250A%2520%2520Driven%2520Remote%2520Sensing%2520Image%2520Retrieval%26entry.906535625%3DGenc%2520Hoxha%2520and%2520Gencer%2520Sumbul%2520and%2520Julia%2520Henkel%2520and%2520Lars%2520M%25C3%25B6llenbrok%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520Deep%2520metric%2520learning%2520%2528DML%2529%2520has%2520shown%2520to%2520be%2520effective%2520for%2520content-based%2520image%250Aretrieval%2520%2528CBIR%2529%2520in%2520remote%2520sensing%2520%2528RS%2529.%2520Most%2520of%2520DML%2520methods%2520for%2520CBIR%2520rely%2520on%2520a%250Ahigh%2520number%2520of%2520annotated%2520images%2520to%2520accurately%2520learn%2520model%2520parameters%2520of%2520deep%250Aneural%2520networks%2520%2528DNNs%2529.%2520However%252C%2520gathering%2520such%2520data%2520is%2520time-consuming%2520and%250Acostly.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520annotation%2520cost-efficient%2520active%250Alearning%2520%2528ANNEAL%2529%2520method%2520tailored%2520to%2520DML-driven%2520CBIR%2520in%2520RS.%2520ANNEAL%2520aims%2520to%250Acreate%2520a%2520small%2520but%2520informative%2520training%2520set%2520made%2520up%2520of%2520similar%2520and%2520dissimilar%250Aimage%2520pairs%2520to%2520be%2520utilized%2520for%2520accurately%2520learning%2520a%2520metric%2520space.%2520The%250Ainformativeness%2520of%2520image%2520pairs%2520is%2520evaluated%2520by%2520combining%2520uncertainty%2520and%250Adiversity%2520criteria.%2520To%2520assess%2520the%2520uncertainty%2520of%2520image%2520pairs%252C%2520we%2520introduce%2520two%250Aalgorithms%253A%25201%2529%2520metric-guided%2520uncertainty%2520estimation%2520%2528MGUE%2529%253B%2520and%25202%2529%2520binary%250Aclassifier%2520guided%2520uncertainty%2520estimation%2520%2528BCGUE%2529.%2520MGUE%2520algorithm%2520automatically%250Aestimates%2520a%2520threshold%2520value%2520that%2520acts%2520as%2520a%2520boundary%2520between%2520similar%2520and%250Adissimilar%2520image%2520pairs%2520based%2520on%2520the%2520distances%2520in%2520the%2520metric%2520space.%2520The%2520closer%250Athe%2520similarity%2520between%2520image%2520pairs%2520is%2520to%2520the%2520estimated%2520threshold%2520value%2520the%250Ahigher%2520their%2520uncertainty.%2520BCGUE%2520algorithm%2520estimates%2520the%2520uncertainty%2520of%2520the%250Aimage%2520pairs%2520based%2520on%2520the%2520confidence%2520of%2520the%2520classifier%2520in%2520assigning%2520correct%250Asimilarity%2520labels.%2520The%2520diversity%2520criterion%2520is%2520assessed%2520through%2520a%250Aclustering-based%2520strategy.%2520ANNEAL%2520combines%2520either%2520MGUE%2520or%2520BCGUE%2520algorithm%2520with%250Athe%2520clustering-based%2520strategy%2520to%2520select%2520the%2520most%2520informative%2520image%2520pairs%252C%2520which%250Aare%2520then%2520labelled%2520by%2520expert%2520annotators%2520as%2520similar%2520or%2520dissimilar.%2520This%2520way%2520of%250Aannotating%2520images%2520significantly%2520reduces%2520the%2520annotation%2520cost%2520compared%2520to%250Aannotating%2520images%2520with%2520land-use%2520land-cover%2520class%2520labels.%2520Experimental%2520results%250Aon%2520two%2520RS%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%2520The%250Acode%2520of%2520this%2520work%2520is%2520publicly%2520available%2520at%250A%255Curl%257Bhttps%253A//git.tu-berlin.de/rsim/anneal_tgrs%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotation%20Cost-Efficient%20Active%20Learning%20for%20Deep%20Metric%20Learning%0A%20%20Driven%20Remote%20Sensing%20Image%20Retrieval&entry.906535625=Genc%20Hoxha%20and%20Gencer%20Sumbul%20and%20Julia%20Henkel%20and%20Lars%20M%C3%B6llenbrok%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Deep%20metric%20learning%20%28DML%29%20has%20shown%20to%20be%20effective%20for%20content-based%20image%0Aretrieval%20%28CBIR%29%20in%20remote%20sensing%20%28RS%29.%20Most%20of%20DML%20methods%20for%20CBIR%20rely%20on%20a%0Ahigh%20number%20of%20annotated%20images%20to%20accurately%20learn%20model%20parameters%20of%20deep%0Aneural%20networks%20%28DNNs%29.%20However%2C%20gathering%20such%20data%20is%20time-consuming%20and%0Acostly.%20To%20address%20this%2C%20we%20propose%20an%20annotation%20cost-efficient%20active%0Alearning%20%28ANNEAL%29%20method%20tailored%20to%20DML-driven%20CBIR%20in%20RS.%20ANNEAL%20aims%20to%0Acreate%20a%20small%20but%20informative%20training%20set%20made%20up%20of%20similar%20and%20dissimilar%0Aimage%20pairs%20to%20be%20utilized%20for%20accurately%20learning%20a%20metric%20space.%20The%0Ainformativeness%20of%20image%20pairs%20is%20evaluated%20by%20combining%20uncertainty%20and%0Adiversity%20criteria.%20To%20assess%20the%20uncertainty%20of%20image%20pairs%2C%20we%20introduce%20two%0Aalgorithms%3A%201%29%20metric-guided%20uncertainty%20estimation%20%28MGUE%29%3B%20and%202%29%20binary%0Aclassifier%20guided%20uncertainty%20estimation%20%28BCGUE%29.%20MGUE%20algorithm%20automatically%0Aestimates%20a%20threshold%20value%20that%20acts%20as%20a%20boundary%20between%20similar%20and%0Adissimilar%20image%20pairs%20based%20on%20the%20distances%20in%20the%20metric%20space.%20The%20closer%0Athe%20similarity%20between%20image%20pairs%20is%20to%20the%20estimated%20threshold%20value%20the%0Ahigher%20their%20uncertainty.%20BCGUE%20algorithm%20estimates%20the%20uncertainty%20of%20the%0Aimage%20pairs%20based%20on%20the%20confidence%20of%20the%20classifier%20in%20assigning%20correct%0Asimilarity%20labels.%20The%20diversity%20criterion%20is%20assessed%20through%20a%0Aclustering-based%20strategy.%20ANNEAL%20combines%20either%20MGUE%20or%20BCGUE%20algorithm%20with%0Athe%20clustering-based%20strategy%20to%20select%20the%20most%20informative%20image%20pairs%2C%20which%0Aare%20then%20labelled%20by%20expert%20annotators%20as%20similar%20or%20dissimilar.%20This%20way%20of%0Aannotating%20images%20significantly%20reduces%20the%20annotation%20cost%20compared%20to%0Aannotating%20images%20with%20land-use%20land-cover%20class%20labels.%20Experimental%20results%0Aon%20two%20RS%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20method.%20The%0Acode%20of%20this%20work%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//git.tu-berlin.de/rsim/anneal_tgrs%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10107v2&entry.124074799=Read"},
{"title": "A First Look at License Compliance Capability of LLMs in Code Generation", "author": "Weiwei Xu and Kai Gao and Hao He and Minghui Zhou", "abstract": "  Recent advances in Large Language Models (LLMs) have revolutionized code\ngeneration, leading to widespread adoption of AI coding tools by developers.\nHowever, LLMs can generate license-protected code without providing the\nnecessary license information, leading to potential intellectual property\nviolations during software production. This paper addresses the critical, yet\nunderexplored, issue of license compliance in LLM-generated code by\nestablishing a benchmark to evaluate the ability of LLMs to provide accurate\nlicense information for their generated code. To establish this benchmark, we\nconduct an empirical study to identify a reasonable standard for \"striking\nsimilarity\" that excludes the possibility of independent creation, indicating a\ncopy relationship between the LLM output and certain open-source code. Based on\nthis standard, we propose an evaluation benchmark LiCoEval, to evaluate the\nlicense compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular\nLLMs, finding that even top-performing LLMs produce a non-negligible proportion\n(0.88% to 2.01%) of code strikingly similar to existing open-source\nimplementations. Notably, most LLMs fail to provide accurate license\ninformation, particularly for code under copyleft licenses. These findings\nunderscore the urgent need to enhance LLM compliance capabilities in code\ngeneration tasks. Our study provides a foundation for future research and\ndevelopment to improve license compliance in AI-assisted software development,\ncontributing to both the protection of open-source software copyrights and the\nmitigation of legal risks for LLM users.\n", "link": "http://arxiv.org/abs/2408.02487v1", "date": "2024-08-05", "relevancy": 1.6761, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4363}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4156}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20First%20Look%20at%20License%20Compliance%20Capability%20of%20LLMs%20in%20Code%20Generation&body=Title%3A%20A%20First%20Look%20at%20License%20Compliance%20Capability%20of%20LLMs%20in%20Code%20Generation%0AAuthor%3A%20Weiwei%20Xu%20and%20Kai%20Gao%20and%20Hao%20He%20and%20Minghui%20Zhou%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20code%0Ageneration%2C%20leading%20to%20widespread%20adoption%20of%20AI%20coding%20tools%20by%20developers.%0AHowever%2C%20LLMs%20can%20generate%20license-protected%20code%20without%20providing%20the%0Anecessary%20license%20information%2C%20leading%20to%20potential%20intellectual%20property%0Aviolations%20during%20software%20production.%20This%20paper%20addresses%20the%20critical%2C%20yet%0Aunderexplored%2C%20issue%20of%20license%20compliance%20in%20LLM-generated%20code%20by%0Aestablishing%20a%20benchmark%20to%20evaluate%20the%20ability%20of%20LLMs%20to%20provide%20accurate%0Alicense%20information%20for%20their%20generated%20code.%20To%20establish%20this%20benchmark%2C%20we%0Aconduct%20an%20empirical%20study%20to%20identify%20a%20reasonable%20standard%20for%20%22striking%0Asimilarity%22%20that%20excludes%20the%20possibility%20of%20independent%20creation%2C%20indicating%20a%0Acopy%20relationship%20between%20the%20LLM%20output%20and%20certain%20open-source%20code.%20Based%20on%0Athis%20standard%2C%20we%20propose%20an%20evaluation%20benchmark%20LiCoEval%2C%20to%20evaluate%20the%0Alicense%20compliance%20capabilities%20of%20LLMs.%20Using%20LiCoEval%2C%20we%20evaluate%2014%20popular%0ALLMs%2C%20finding%20that%20even%20top-performing%20LLMs%20produce%20a%20non-negligible%20proportion%0A%280.88%25%20to%202.01%25%29%20of%20code%20strikingly%20similar%20to%20existing%20open-source%0Aimplementations.%20Notably%2C%20most%20LLMs%20fail%20to%20provide%20accurate%20license%0Ainformation%2C%20particularly%20for%20code%20under%20copyleft%20licenses.%20These%20findings%0Aunderscore%20the%20urgent%20need%20to%20enhance%20LLM%20compliance%20capabilities%20in%20code%0Ageneration%20tasks.%20Our%20study%20provides%20a%20foundation%20for%20future%20research%20and%0Adevelopment%20to%20improve%20license%20compliance%20in%20AI-assisted%20software%20development%2C%0Acontributing%20to%20both%20the%20protection%20of%20open-source%20software%20copyrights%20and%20the%0Amitigation%20of%20legal%20risks%20for%20LLM%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520First%2520Look%2520at%2520License%2520Compliance%2520Capability%2520of%2520LLMs%2520in%2520Code%2520Generation%26entry.906535625%3DWeiwei%2520Xu%2520and%2520Kai%2520Gao%2520and%2520Hao%2520He%2520and%2520Minghui%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520code%250Ageneration%252C%2520leading%2520to%2520widespread%2520adoption%2520of%2520AI%2520coding%2520tools%2520by%2520developers.%250AHowever%252C%2520LLMs%2520can%2520generate%2520license-protected%2520code%2520without%2520providing%2520the%250Anecessary%2520license%2520information%252C%2520leading%2520to%2520potential%2520intellectual%2520property%250Aviolations%2520during%2520software%2520production.%2520This%2520paper%2520addresses%2520the%2520critical%252C%2520yet%250Aunderexplored%252C%2520issue%2520of%2520license%2520compliance%2520in%2520LLM-generated%2520code%2520by%250Aestablishing%2520a%2520benchmark%2520to%2520evaluate%2520the%2520ability%2520of%2520LLMs%2520to%2520provide%2520accurate%250Alicense%2520information%2520for%2520their%2520generated%2520code.%2520To%2520establish%2520this%2520benchmark%252C%2520we%250Aconduct%2520an%2520empirical%2520study%2520to%2520identify%2520a%2520reasonable%2520standard%2520for%2520%2522striking%250Asimilarity%2522%2520that%2520excludes%2520the%2520possibility%2520of%2520independent%2520creation%252C%2520indicating%2520a%250Acopy%2520relationship%2520between%2520the%2520LLM%2520output%2520and%2520certain%2520open-source%2520code.%2520Based%2520on%250Athis%2520standard%252C%2520we%2520propose%2520an%2520evaluation%2520benchmark%2520LiCoEval%252C%2520to%2520evaluate%2520the%250Alicense%2520compliance%2520capabilities%2520of%2520LLMs.%2520Using%2520LiCoEval%252C%2520we%2520evaluate%252014%2520popular%250ALLMs%252C%2520finding%2520that%2520even%2520top-performing%2520LLMs%2520produce%2520a%2520non-negligible%2520proportion%250A%25280.88%2525%2520to%25202.01%2525%2529%2520of%2520code%2520strikingly%2520similar%2520to%2520existing%2520open-source%250Aimplementations.%2520Notably%252C%2520most%2520LLMs%2520fail%2520to%2520provide%2520accurate%2520license%250Ainformation%252C%2520particularly%2520for%2520code%2520under%2520copyleft%2520licenses.%2520These%2520findings%250Aunderscore%2520the%2520urgent%2520need%2520to%2520enhance%2520LLM%2520compliance%2520capabilities%2520in%2520code%250Ageneration%2520tasks.%2520Our%2520study%2520provides%2520a%2520foundation%2520for%2520future%2520research%2520and%250Adevelopment%2520to%2520improve%2520license%2520compliance%2520in%2520AI-assisted%2520software%2520development%252C%250Acontributing%2520to%2520both%2520the%2520protection%2520of%2520open-source%2520software%2520copyrights%2520and%2520the%250Amitigation%2520of%2520legal%2520risks%2520for%2520LLM%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20First%20Look%20at%20License%20Compliance%20Capability%20of%20LLMs%20in%20Code%20Generation&entry.906535625=Weiwei%20Xu%20and%20Kai%20Gao%20and%20Hao%20He%20and%20Minghui%20Zhou&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20code%0Ageneration%2C%20leading%20to%20widespread%20adoption%20of%20AI%20coding%20tools%20by%20developers.%0AHowever%2C%20LLMs%20can%20generate%20license-protected%20code%20without%20providing%20the%0Anecessary%20license%20information%2C%20leading%20to%20potential%20intellectual%20property%0Aviolations%20during%20software%20production.%20This%20paper%20addresses%20the%20critical%2C%20yet%0Aunderexplored%2C%20issue%20of%20license%20compliance%20in%20LLM-generated%20code%20by%0Aestablishing%20a%20benchmark%20to%20evaluate%20the%20ability%20of%20LLMs%20to%20provide%20accurate%0Alicense%20information%20for%20their%20generated%20code.%20To%20establish%20this%20benchmark%2C%20we%0Aconduct%20an%20empirical%20study%20to%20identify%20a%20reasonable%20standard%20for%20%22striking%0Asimilarity%22%20that%20excludes%20the%20possibility%20of%20independent%20creation%2C%20indicating%20a%0Acopy%20relationship%20between%20the%20LLM%20output%20and%20certain%20open-source%20code.%20Based%20on%0Athis%20standard%2C%20we%20propose%20an%20evaluation%20benchmark%20LiCoEval%2C%20to%20evaluate%20the%0Alicense%20compliance%20capabilities%20of%20LLMs.%20Using%20LiCoEval%2C%20we%20evaluate%2014%20popular%0ALLMs%2C%20finding%20that%20even%20top-performing%20LLMs%20produce%20a%20non-negligible%20proportion%0A%280.88%25%20to%202.01%25%29%20of%20code%20strikingly%20similar%20to%20existing%20open-source%0Aimplementations.%20Notably%2C%20most%20LLMs%20fail%20to%20provide%20accurate%20license%0Ainformation%2C%20particularly%20for%20code%20under%20copyleft%20licenses.%20These%20findings%0Aunderscore%20the%20urgent%20need%20to%20enhance%20LLM%20compliance%20capabilities%20in%20code%0Ageneration%20tasks.%20Our%20study%20provides%20a%20foundation%20for%20future%20research%20and%0Adevelopment%20to%20improve%20license%20compliance%20in%20AI-assisted%20software%20development%2C%0Acontributing%20to%20both%20the%20protection%20of%20open-source%20software%20copyrights%20and%20the%0Amitigation%20of%20legal%20risks%20for%20LLM%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02487v1&entry.124074799=Read"},
{"title": "Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection", "author": "Ting Lei and Shaofeng Yin and Yuxin Peng and Yang Liu", "abstract": "  Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier\ntopic due to its capability to detect HOIs beyond a predefined set of\ncategories. This task entails not only identifying the interactiveness of\nhuman-object pairs and localizing them but also recognizing both seen and\nunseen interaction categories. In this paper, we introduce a novel framework\nfor zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.\nThis approach enhances the generalization of large foundation models, such as\nCLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning\nmethods, we propose learning decoupled vision and language prompts for\ninteractiveness-aware visual feature extraction and generalizable interaction\nclassification, respectively. Specifically, we integrate prior knowledge of\ndifferent granularity into conditional vision prompts, including an\ninput-conditioned instance prior and a global spatial pattern prior. The former\nencourages the image encoder to treat instances belonging to seen or\npotentially unseen HOI concepts equally while the latter provides\nrepresentative plausible spatial configuration of the human and object under\ninteraction. Besides, we employ language-aware prompt learning with a\nconsistency constraint to preserve the knowledge of the large foundation model\nto enable better generalization in the text branch. Extensive experiments\ndemonstrate the efficacy of our detector with conditional multi-modal prompts,\noutperforming previous state-of-the-art on unseen classes of various zero-shot\nsettings. The code and models are available at\n\\url{https://github.com/ltttpku/CMMP}.\n", "link": "http://arxiv.org/abs/2408.02484v1", "date": "2024-08-05", "relevancy": 1.6642, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Conditional%20Multi-Modal%20Prompts%20for%20Zero-shot%20HOI%20Detection&body=Title%3A%20Exploring%20Conditional%20Multi-Modal%20Prompts%20for%20Zero-shot%20HOI%20Detection%0AAuthor%3A%20Ting%20Lei%20and%20Shaofeng%20Yin%20and%20Yuxin%20Peng%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Zero-shot%20Human-Object%20Interaction%20%28HOI%29%20detection%20has%20emerged%20as%20a%20frontier%0Atopic%20due%20to%20its%20capability%20to%20detect%20HOIs%20beyond%20a%20predefined%20set%20of%0Acategories.%20This%20task%20entails%20not%20only%20identifying%20the%20interactiveness%20of%0Ahuman-object%20pairs%20and%20localizing%20them%20but%20also%20recognizing%20both%20seen%20and%0Aunseen%20interaction%20categories.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%0Afor%20zero-shot%20HOI%20detection%20using%20Conditional%20Multi-Modal%20Prompts%2C%20namely%20CMMP.%0AThis%20approach%20enhances%20the%20generalization%20of%20large%20foundation%20models%2C%20such%20as%0ACLIP%2C%20when%20fine-tuned%20for%20HOI%20detection.%20Unlike%20traditional%20prompt-learning%0Amethods%2C%20we%20propose%20learning%20decoupled%20vision%20and%20language%20prompts%20for%0Ainteractiveness-aware%20visual%20feature%20extraction%20and%20generalizable%20interaction%0Aclassification%2C%20respectively.%20Specifically%2C%20we%20integrate%20prior%20knowledge%20of%0Adifferent%20granularity%20into%20conditional%20vision%20prompts%2C%20including%20an%0Ainput-conditioned%20instance%20prior%20and%20a%20global%20spatial%20pattern%20prior.%20The%20former%0Aencourages%20the%20image%20encoder%20to%20treat%20instances%20belonging%20to%20seen%20or%0Apotentially%20unseen%20HOI%20concepts%20equally%20while%20the%20latter%20provides%0Arepresentative%20plausible%20spatial%20configuration%20of%20the%20human%20and%20object%20under%0Ainteraction.%20Besides%2C%20we%20employ%20language-aware%20prompt%20learning%20with%20a%0Aconsistency%20constraint%20to%20preserve%20the%20knowledge%20of%20the%20large%20foundation%20model%0Ato%20enable%20better%20generalization%20in%20the%20text%20branch.%20Extensive%20experiments%0Ademonstrate%20the%20efficacy%20of%20our%20detector%20with%20conditional%20multi-modal%20prompts%2C%0Aoutperforming%20previous%20state-of-the-art%20on%20unseen%20classes%20of%20various%20zero-shot%0Asettings.%20The%20code%20and%20models%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ltttpku/CMMP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Conditional%2520Multi-Modal%2520Prompts%2520for%2520Zero-shot%2520HOI%2520Detection%26entry.906535625%3DTing%2520Lei%2520and%2520Shaofeng%2520Yin%2520and%2520Yuxin%2520Peng%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Zero-shot%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%2520has%2520emerged%2520as%2520a%2520frontier%250Atopic%2520due%2520to%2520its%2520capability%2520to%2520detect%2520HOIs%2520beyond%2520a%2520predefined%2520set%2520of%250Acategories.%2520This%2520task%2520entails%2520not%2520only%2520identifying%2520the%2520interactiveness%2520of%250Ahuman-object%2520pairs%2520and%2520localizing%2520them%2520but%2520also%2520recognizing%2520both%2520seen%2520and%250Aunseen%2520interaction%2520categories.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520framework%250Afor%2520zero-shot%2520HOI%2520detection%2520using%2520Conditional%2520Multi-Modal%2520Prompts%252C%2520namely%2520CMMP.%250AThis%2520approach%2520enhances%2520the%2520generalization%2520of%2520large%2520foundation%2520models%252C%2520such%2520as%250ACLIP%252C%2520when%2520fine-tuned%2520for%2520HOI%2520detection.%2520Unlike%2520traditional%2520prompt-learning%250Amethods%252C%2520we%2520propose%2520learning%2520decoupled%2520vision%2520and%2520language%2520prompts%2520for%250Ainteractiveness-aware%2520visual%2520feature%2520extraction%2520and%2520generalizable%2520interaction%250Aclassification%252C%2520respectively.%2520Specifically%252C%2520we%2520integrate%2520prior%2520knowledge%2520of%250Adifferent%2520granularity%2520into%2520conditional%2520vision%2520prompts%252C%2520including%2520an%250Ainput-conditioned%2520instance%2520prior%2520and%2520a%2520global%2520spatial%2520pattern%2520prior.%2520The%2520former%250Aencourages%2520the%2520image%2520encoder%2520to%2520treat%2520instances%2520belonging%2520to%2520seen%2520or%250Apotentially%2520unseen%2520HOI%2520concepts%2520equally%2520while%2520the%2520latter%2520provides%250Arepresentative%2520plausible%2520spatial%2520configuration%2520of%2520the%2520human%2520and%2520object%2520under%250Ainteraction.%2520Besides%252C%2520we%2520employ%2520language-aware%2520prompt%2520learning%2520with%2520a%250Aconsistency%2520constraint%2520to%2520preserve%2520the%2520knowledge%2520of%2520the%2520large%2520foundation%2520model%250Ato%2520enable%2520better%2520generalization%2520in%2520the%2520text%2520branch.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520efficacy%2520of%2520our%2520detector%2520with%2520conditional%2520multi-modal%2520prompts%252C%250Aoutperforming%2520previous%2520state-of-the-art%2520on%2520unseen%2520classes%2520of%2520various%2520zero-shot%250Asettings.%2520The%2520code%2520and%2520models%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/ltttpku/CMMP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Conditional%20Multi-Modal%20Prompts%20for%20Zero-shot%20HOI%20Detection&entry.906535625=Ting%20Lei%20and%20Shaofeng%20Yin%20and%20Yuxin%20Peng%20and%20Yang%20Liu&entry.1292438233=%20%20Zero-shot%20Human-Object%20Interaction%20%28HOI%29%20detection%20has%20emerged%20as%20a%20frontier%0Atopic%20due%20to%20its%20capability%20to%20detect%20HOIs%20beyond%20a%20predefined%20set%20of%0Acategories.%20This%20task%20entails%20not%20only%20identifying%20the%20interactiveness%20of%0Ahuman-object%20pairs%20and%20localizing%20them%20but%20also%20recognizing%20both%20seen%20and%0Aunseen%20interaction%20categories.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%0Afor%20zero-shot%20HOI%20detection%20using%20Conditional%20Multi-Modal%20Prompts%2C%20namely%20CMMP.%0AThis%20approach%20enhances%20the%20generalization%20of%20large%20foundation%20models%2C%20such%20as%0ACLIP%2C%20when%20fine-tuned%20for%20HOI%20detection.%20Unlike%20traditional%20prompt-learning%0Amethods%2C%20we%20propose%20learning%20decoupled%20vision%20and%20language%20prompts%20for%0Ainteractiveness-aware%20visual%20feature%20extraction%20and%20generalizable%20interaction%0Aclassification%2C%20respectively.%20Specifically%2C%20we%20integrate%20prior%20knowledge%20of%0Adifferent%20granularity%20into%20conditional%20vision%20prompts%2C%20including%20an%0Ainput-conditioned%20instance%20prior%20and%20a%20global%20spatial%20pattern%20prior.%20The%20former%0Aencourages%20the%20image%20encoder%20to%20treat%20instances%20belonging%20to%20seen%20or%0Apotentially%20unseen%20HOI%20concepts%20equally%20while%20the%20latter%20provides%0Arepresentative%20plausible%20spatial%20configuration%20of%20the%20human%20and%20object%20under%0Ainteraction.%20Besides%2C%20we%20employ%20language-aware%20prompt%20learning%20with%20a%0Aconsistency%20constraint%20to%20preserve%20the%20knowledge%20of%20the%20large%20foundation%20model%0Ato%20enable%20better%20generalization%20in%20the%20text%20branch.%20Extensive%20experiments%0Ademonstrate%20the%20efficacy%20of%20our%20detector%20with%20conditional%20multi-modal%20prompts%2C%0Aoutperforming%20previous%20state-of-the-art%20on%20unseen%20classes%20of%20various%20zero-shot%0Asettings.%20The%20code%20and%20models%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ltttpku/CMMP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02484v1&entry.124074799=Read"},
{"title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation\n  with Multimodal Generative Pretraining", "author": "Dongyang Liu and Shitian Zhao and Le Zhuo and Weifeng Lin and Yu Qiao and Hongsheng Li and Peng Gao", "abstract": "  We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. Unlike existing\nautoregressive image generation approaches, Lumina-mGPT employs a pretrained\ndecoder-only transformer as a unified framework for modeling multimodal token\nsequences. Our key insight is that a simple decoder-only transformer with\nmultimodal Generative PreTraining (mGPT), utilizing the next-token prediction\nobjective on massive interleaved text-image sequences, can learn broad and\ngeneral multimodal capabilities, thereby illuminating photorealistic\ntext-to-image generation. Building on these pretrained models, we propose\nFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text\npairs to fully unlock their potential for high-aesthetic image synthesis at any\nresolution while maintaining their general multimodal capabilities.\nFurthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),\ntransforming Lumina-mGPT into a foundation model that seamlessly achieves\nomnipotent task unification. The resulting model demonstrates versatile\nmultimodal capabilities, including visual generation tasks like flexible\ntext-to-image generation and controllable generation, visual recognition tasks\nlike segmentation and depth estimation, and vision-language tasks like\nmultiturn visual question answering. Additionally, we analyze the differences\nand similarities between diffusion-based and autoregressive methods in a direct\ncomparison.\n", "link": "http://arxiv.org/abs/2408.02657v1", "date": "2024-08-05", "relevancy": 1.6531, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5878}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5432}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining&body=Title%3A%20Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining%0AAuthor%3A%20Dongyang%20Liu%20and%20Shitian%20Zhao%20and%20Le%20Zhuo%20and%20Weifeng%20Lin%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%20and%20Peng%20Gao%0AAbstract%3A%20%20%20We%20present%20Lumina-mGPT%2C%20a%20family%20of%20multimodal%20autoregressive%20models%20capable%0Aof%20various%20vision%20and%20language%20tasks%2C%20particularly%20excelling%20in%20generating%0Aflexible%20photorealistic%20images%20from%20text%20descriptions.%20Unlike%20existing%0Aautoregressive%20image%20generation%20approaches%2C%20Lumina-mGPT%20employs%20a%20pretrained%0Adecoder-only%20transformer%20as%20a%20unified%20framework%20for%20modeling%20multimodal%20token%0Asequences.%20Our%20key%20insight%20is%20that%20a%20simple%20decoder-only%20transformer%20with%0Amultimodal%20Generative%20PreTraining%20%28mGPT%29%2C%20utilizing%20the%20next-token%20prediction%0Aobjective%20on%20massive%20interleaved%20text-image%20sequences%2C%20can%20learn%20broad%20and%0Ageneral%20multimodal%20capabilities%2C%20thereby%20illuminating%20photorealistic%0Atext-to-image%20generation.%20Building%20on%20these%20pretrained%20models%2C%20we%20propose%0AFlexible%20Progressive%20Supervised%20Finetuning%20%28FP-SFT%29%20on%20high-quality%20image-text%0Apairs%20to%20fully%20unlock%20their%20potential%20for%20high-aesthetic%20image%20synthesis%20at%20any%0Aresolution%20while%20maintaining%20their%20general%20multimodal%20capabilities.%0AFurthermore%2C%20we%20introduce%20Ominiponent%20Supervised%20Finetuning%20%28Omni-SFT%29%2C%0Atransforming%20Lumina-mGPT%20into%20a%20foundation%20model%20that%20seamlessly%20achieves%0Aomnipotent%20task%20unification.%20The%20resulting%20model%20demonstrates%20versatile%0Amultimodal%20capabilities%2C%20including%20visual%20generation%20tasks%20like%20flexible%0Atext-to-image%20generation%20and%20controllable%20generation%2C%20visual%20recognition%20tasks%0Alike%20segmentation%20and%20depth%20estimation%2C%20and%20vision-language%20tasks%20like%0Amultiturn%20visual%20question%20answering.%20Additionally%2C%20we%20analyze%20the%20differences%0Aand%20similarities%20between%20diffusion-based%20and%20autoregressive%20methods%20in%20a%20direct%0Acomparison.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumina-mGPT%253A%2520Illuminate%2520Flexible%2520Photorealistic%2520Text-to-Image%2520Generation%250A%2520%2520with%2520Multimodal%2520Generative%2520Pretraining%26entry.906535625%3DDongyang%2520Liu%2520and%2520Shitian%2520Zhao%2520and%2520Le%2520Zhuo%2520and%2520Weifeng%2520Lin%2520and%2520Yu%2520Qiao%2520and%2520Hongsheng%2520Li%2520and%2520Peng%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520Lumina-mGPT%252C%2520a%2520family%2520of%2520multimodal%2520autoregressive%2520models%2520capable%250Aof%2520various%2520vision%2520and%2520language%2520tasks%252C%2520particularly%2520excelling%2520in%2520generating%250Aflexible%2520photorealistic%2520images%2520from%2520text%2520descriptions.%2520Unlike%2520existing%250Aautoregressive%2520image%2520generation%2520approaches%252C%2520Lumina-mGPT%2520employs%2520a%2520pretrained%250Adecoder-only%2520transformer%2520as%2520a%2520unified%2520framework%2520for%2520modeling%2520multimodal%2520token%250Asequences.%2520Our%2520key%2520insight%2520is%2520that%2520a%2520simple%2520decoder-only%2520transformer%2520with%250Amultimodal%2520Generative%2520PreTraining%2520%2528mGPT%2529%252C%2520utilizing%2520the%2520next-token%2520prediction%250Aobjective%2520on%2520massive%2520interleaved%2520text-image%2520sequences%252C%2520can%2520learn%2520broad%2520and%250Ageneral%2520multimodal%2520capabilities%252C%2520thereby%2520illuminating%2520photorealistic%250Atext-to-image%2520generation.%2520Building%2520on%2520these%2520pretrained%2520models%252C%2520we%2520propose%250AFlexible%2520Progressive%2520Supervised%2520Finetuning%2520%2528FP-SFT%2529%2520on%2520high-quality%2520image-text%250Apairs%2520to%2520fully%2520unlock%2520their%2520potential%2520for%2520high-aesthetic%2520image%2520synthesis%2520at%2520any%250Aresolution%2520while%2520maintaining%2520their%2520general%2520multimodal%2520capabilities.%250AFurthermore%252C%2520we%2520introduce%2520Ominiponent%2520Supervised%2520Finetuning%2520%2528Omni-SFT%2529%252C%250Atransforming%2520Lumina-mGPT%2520into%2520a%2520foundation%2520model%2520that%2520seamlessly%2520achieves%250Aomnipotent%2520task%2520unification.%2520The%2520resulting%2520model%2520demonstrates%2520versatile%250Amultimodal%2520capabilities%252C%2520including%2520visual%2520generation%2520tasks%2520like%2520flexible%250Atext-to-image%2520generation%2520and%2520controllable%2520generation%252C%2520visual%2520recognition%2520tasks%250Alike%2520segmentation%2520and%2520depth%2520estimation%252C%2520and%2520vision-language%2520tasks%2520like%250Amultiturn%2520visual%2520question%2520answering.%2520Additionally%252C%2520we%2520analyze%2520the%2520differences%250Aand%2520similarities%2520between%2520diffusion-based%2520and%2520autoregressive%2520methods%2520in%2520a%2520direct%250Acomparison.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining&entry.906535625=Dongyang%20Liu%20and%20Shitian%20Zhao%20and%20Le%20Zhuo%20and%20Weifeng%20Lin%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%20and%20Peng%20Gao&entry.1292438233=%20%20We%20present%20Lumina-mGPT%2C%20a%20family%20of%20multimodal%20autoregressive%20models%20capable%0Aof%20various%20vision%20and%20language%20tasks%2C%20particularly%20excelling%20in%20generating%0Aflexible%20photorealistic%20images%20from%20text%20descriptions.%20Unlike%20existing%0Aautoregressive%20image%20generation%20approaches%2C%20Lumina-mGPT%20employs%20a%20pretrained%0Adecoder-only%20transformer%20as%20a%20unified%20framework%20for%20modeling%20multimodal%20token%0Asequences.%20Our%20key%20insight%20is%20that%20a%20simple%20decoder-only%20transformer%20with%0Amultimodal%20Generative%20PreTraining%20%28mGPT%29%2C%20utilizing%20the%20next-token%20prediction%0Aobjective%20on%20massive%20interleaved%20text-image%20sequences%2C%20can%20learn%20broad%20and%0Ageneral%20multimodal%20capabilities%2C%20thereby%20illuminating%20photorealistic%0Atext-to-image%20generation.%20Building%20on%20these%20pretrained%20models%2C%20we%20propose%0AFlexible%20Progressive%20Supervised%20Finetuning%20%28FP-SFT%29%20on%20high-quality%20image-text%0Apairs%20to%20fully%20unlock%20their%20potential%20for%20high-aesthetic%20image%20synthesis%20at%20any%0Aresolution%20while%20maintaining%20their%20general%20multimodal%20capabilities.%0AFurthermore%2C%20we%20introduce%20Ominiponent%20Supervised%20Finetuning%20%28Omni-SFT%29%2C%0Atransforming%20Lumina-mGPT%20into%20a%20foundation%20model%20that%20seamlessly%20achieves%0Aomnipotent%20task%20unification.%20The%20resulting%20model%20demonstrates%20versatile%0Amultimodal%20capabilities%2C%20including%20visual%20generation%20tasks%20like%20flexible%0Atext-to-image%20generation%20and%20controllable%20generation%2C%20visual%20recognition%20tasks%0Alike%20segmentation%20and%20depth%20estimation%2C%20and%20vision-language%20tasks%20like%0Amultiturn%20visual%20question%20answering.%20Additionally%2C%20we%20analyze%20the%20differences%0Aand%20similarities%20between%20diffusion-based%20and%20autoregressive%20methods%20in%20a%20direct%0Acomparison.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02657v1&entry.124074799=Read"},
{"title": "Integrating Model-Based Footstep Planning with Model-Free Reinforcement\n  Learning for Dynamic Legged Locomotion", "author": "Ho Jae Lee and Seungwoo Hong and Sangbae Kim", "abstract": "  In this work, we introduce a control framework that combines model-based\nfootstep planning with Reinforcement Learning (RL), leveraging desired footstep\npatterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing\nthe LIP model, our method forward predicts robot states and determines the\ndesired foot placement given the velocity commands. We then train an RL policy\nto track the foot placements without following the full reference motions\nderived from the LIP model. This partial guidance from the physics model allows\nthe RL policy to integrate the predictive capabilities of the physics-informed\ndynamics and the adaptability characteristics of the RL controller without\noverfitting the policy to the template model. Our approach is validated on the\nMIT Humanoid, demonstrating that our policy can achieve stable yet dynamic\nlocomotion for walking and turning. We further validate the adaptability and\ngeneralizability of our policy by extending the locomotion task to unseen,\nuneven terrain. During the hardware deployment, we have achieved forward\nwalking speeds of up to 1.5 m/s on a treadmill and have successfully performed\ndynamic locomotion maneuvers such as 90-degree and 180-degree turns.\n", "link": "http://arxiv.org/abs/2408.02662v1", "date": "2024-08-05", "relevancy": 1.6425, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6228}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.53}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Model-Based%20Footstep%20Planning%20with%20Model-Free%20Reinforcement%0A%20%20Learning%20for%20Dynamic%20Legged%20Locomotion&body=Title%3A%20Integrating%20Model-Based%20Footstep%20Planning%20with%20Model-Free%20Reinforcement%0A%20%20Learning%20for%20Dynamic%20Legged%20Locomotion%0AAuthor%3A%20Ho%20Jae%20Lee%20and%20Seungwoo%20Hong%20and%20Sangbae%20Kim%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20a%20control%20framework%20that%20combines%20model-based%0Afootstep%20planning%20with%20Reinforcement%20Learning%20%28RL%29%2C%20leveraging%20desired%20footstep%0Apatterns%20derived%20from%20the%20Linear%20Inverted%20Pendulum%20%28LIP%29%20dynamics.%20Utilizing%0Athe%20LIP%20model%2C%20our%20method%20forward%20predicts%20robot%20states%20and%20determines%20the%0Adesired%20foot%20placement%20given%20the%20velocity%20commands.%20We%20then%20train%20an%20RL%20policy%0Ato%20track%20the%20foot%20placements%20without%20following%20the%20full%20reference%20motions%0Aderived%20from%20the%20LIP%20model.%20This%20partial%20guidance%20from%20the%20physics%20model%20allows%0Athe%20RL%20policy%20to%20integrate%20the%20predictive%20capabilities%20of%20the%20physics-informed%0Adynamics%20and%20the%20adaptability%20characteristics%20of%20the%20RL%20controller%20without%0Aoverfitting%20the%20policy%20to%20the%20template%20model.%20Our%20approach%20is%20validated%20on%20the%0AMIT%20Humanoid%2C%20demonstrating%20that%20our%20policy%20can%20achieve%20stable%20yet%20dynamic%0Alocomotion%20for%20walking%20and%20turning.%20We%20further%20validate%20the%20adaptability%20and%0Ageneralizability%20of%20our%20policy%20by%20extending%20the%20locomotion%20task%20to%20unseen%2C%0Auneven%20terrain.%20During%20the%20hardware%20deployment%2C%20we%20have%20achieved%20forward%0Awalking%20speeds%20of%20up%20to%201.5%20m/s%20on%20a%20treadmill%20and%20have%20successfully%20performed%0Adynamic%20locomotion%20maneuvers%20such%20as%2090-degree%20and%20180-degree%20turns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Model-Based%2520Footstep%2520Planning%2520with%2520Model-Free%2520Reinforcement%250A%2520%2520Learning%2520for%2520Dynamic%2520Legged%2520Locomotion%26entry.906535625%3DHo%2520Jae%2520Lee%2520and%2520Seungwoo%2520Hong%2520and%2520Sangbae%2520Kim%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520control%2520framework%2520that%2520combines%2520model-based%250Afootstep%2520planning%2520with%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520leveraging%2520desired%2520footstep%250Apatterns%2520derived%2520from%2520the%2520Linear%2520Inverted%2520Pendulum%2520%2528LIP%2529%2520dynamics.%2520Utilizing%250Athe%2520LIP%2520model%252C%2520our%2520method%2520forward%2520predicts%2520robot%2520states%2520and%2520determines%2520the%250Adesired%2520foot%2520placement%2520given%2520the%2520velocity%2520commands.%2520We%2520then%2520train%2520an%2520RL%2520policy%250Ato%2520track%2520the%2520foot%2520placements%2520without%2520following%2520the%2520full%2520reference%2520motions%250Aderived%2520from%2520the%2520LIP%2520model.%2520This%2520partial%2520guidance%2520from%2520the%2520physics%2520model%2520allows%250Athe%2520RL%2520policy%2520to%2520integrate%2520the%2520predictive%2520capabilities%2520of%2520the%2520physics-informed%250Adynamics%2520and%2520the%2520adaptability%2520characteristics%2520of%2520the%2520RL%2520controller%2520without%250Aoverfitting%2520the%2520policy%2520to%2520the%2520template%2520model.%2520Our%2520approach%2520is%2520validated%2520on%2520the%250AMIT%2520Humanoid%252C%2520demonstrating%2520that%2520our%2520policy%2520can%2520achieve%2520stable%2520yet%2520dynamic%250Alocomotion%2520for%2520walking%2520and%2520turning.%2520We%2520further%2520validate%2520the%2520adaptability%2520and%250Ageneralizability%2520of%2520our%2520policy%2520by%2520extending%2520the%2520locomotion%2520task%2520to%2520unseen%252C%250Auneven%2520terrain.%2520During%2520the%2520hardware%2520deployment%252C%2520we%2520have%2520achieved%2520forward%250Awalking%2520speeds%2520of%2520up%2520to%25201.5%2520m/s%2520on%2520a%2520treadmill%2520and%2520have%2520successfully%2520performed%250Adynamic%2520locomotion%2520maneuvers%2520such%2520as%252090-degree%2520and%2520180-degree%2520turns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Model-Based%20Footstep%20Planning%20with%20Model-Free%20Reinforcement%0A%20%20Learning%20for%20Dynamic%20Legged%20Locomotion&entry.906535625=Ho%20Jae%20Lee%20and%20Seungwoo%20Hong%20and%20Sangbae%20Kim&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20a%20control%20framework%20that%20combines%20model-based%0Afootstep%20planning%20with%20Reinforcement%20Learning%20%28RL%29%2C%20leveraging%20desired%20footstep%0Apatterns%20derived%20from%20the%20Linear%20Inverted%20Pendulum%20%28LIP%29%20dynamics.%20Utilizing%0Athe%20LIP%20model%2C%20our%20method%20forward%20predicts%20robot%20states%20and%20determines%20the%0Adesired%20foot%20placement%20given%20the%20velocity%20commands.%20We%20then%20train%20an%20RL%20policy%0Ato%20track%20the%20foot%20placements%20without%20following%20the%20full%20reference%20motions%0Aderived%20from%20the%20LIP%20model.%20This%20partial%20guidance%20from%20the%20physics%20model%20allows%0Athe%20RL%20policy%20to%20integrate%20the%20predictive%20capabilities%20of%20the%20physics-informed%0Adynamics%20and%20the%20adaptability%20characteristics%20of%20the%20RL%20controller%20without%0Aoverfitting%20the%20policy%20to%20the%20template%20model.%20Our%20approach%20is%20validated%20on%20the%0AMIT%20Humanoid%2C%20demonstrating%20that%20our%20policy%20can%20achieve%20stable%20yet%20dynamic%0Alocomotion%20for%20walking%20and%20turning.%20We%20further%20validate%20the%20adaptability%20and%0Ageneralizability%20of%20our%20policy%20by%20extending%20the%20locomotion%20task%20to%20unseen%2C%0Auneven%20terrain.%20During%20the%20hardware%20deployment%2C%20we%20have%20achieved%20forward%0Awalking%20speeds%20of%20up%20to%201.5%20m/s%20on%20a%20treadmill%20and%20have%20successfully%20performed%0Adynamic%20locomotion%20maneuvers%20such%20as%2090-degree%20and%20180-degree%20turns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02662v1&entry.124074799=Read"},
{"title": "Tell me why: Training preferences-based RL with human preferences and\n  step-level explanations", "author": "Jakob Karalus", "abstract": "  Human-in-the-loop reinforcement learning allows the training of agents\nthrough various interfaces, even for non-expert humans. Recently,\npreference-based methods (PbRL), where the human has to give his preference\nover two trajectories, increased in popularity since they allow training in\ndomains where more direct feedback is hard to formulate. However, the current\nPBRL methods have limitations and do not provide humans with an expressive\ninterface for giving feedback. With this work, we propose a new\npreference-based learning method that provides humans with a more expressive\ninterface to provide their preference over trajectories and a factual\nexplanation (or annotation of why they have this preference). These\nexplanations allow the human to explain what parts of the trajectory are most\nrelevant for the preference. We allow the expression of the explanations over\nindividual trajectory steps. We evaluate our method in various simulations\nusing a simulated human oracle (with realistic restrictions), and our results\nshow that our extended feedback can improve the speed of learning.\n", "link": "http://arxiv.org/abs/2405.14244v2", "date": "2024-08-05", "relevancy": 1.6227, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.551}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5469}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tell%20me%20why%3A%20Training%20preferences-based%20RL%20with%20human%20preferences%20and%0A%20%20step-level%20explanations&body=Title%3A%20Tell%20me%20why%3A%20Training%20preferences-based%20RL%20with%20human%20preferences%20and%0A%20%20step-level%20explanations%0AAuthor%3A%20Jakob%20Karalus%0AAbstract%3A%20%20%20Human-in-the-loop%20reinforcement%20learning%20allows%20the%20training%20of%20agents%0Athrough%20various%20interfaces%2C%20even%20for%20non-expert%20humans.%20Recently%2C%0Apreference-based%20methods%20%28PbRL%29%2C%20where%20the%20human%20has%20to%20give%20his%20preference%0Aover%20two%20trajectories%2C%20increased%20in%20popularity%20since%20they%20allow%20training%20in%0Adomains%20where%20more%20direct%20feedback%20is%20hard%20to%20formulate.%20However%2C%20the%20current%0APBRL%20methods%20have%20limitations%20and%20do%20not%20provide%20humans%20with%20an%20expressive%0Ainterface%20for%20giving%20feedback.%20With%20this%20work%2C%20we%20propose%20a%20new%0Apreference-based%20learning%20method%20that%20provides%20humans%20with%20a%20more%20expressive%0Ainterface%20to%20provide%20their%20preference%20over%20trajectories%20and%20a%20factual%0Aexplanation%20%28or%20annotation%20of%20why%20they%20have%20this%20preference%29.%20These%0Aexplanations%20allow%20the%20human%20to%20explain%20what%20parts%20of%20the%20trajectory%20are%20most%0Arelevant%20for%20the%20preference.%20We%20allow%20the%20expression%20of%20the%20explanations%20over%0Aindividual%20trajectory%20steps.%20We%20evaluate%20our%20method%20in%20various%20simulations%0Ausing%20a%20simulated%20human%20oracle%20%28with%20realistic%20restrictions%29%2C%20and%20our%20results%0Ashow%20that%20our%20extended%20feedback%20can%20improve%20the%20speed%20of%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14244v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTell%2520me%2520why%253A%2520Training%2520preferences-based%2520RL%2520with%2520human%2520preferences%2520and%250A%2520%2520step-level%2520explanations%26entry.906535625%3DJakob%2520Karalus%26entry.1292438233%3D%2520%2520Human-in-the-loop%2520reinforcement%2520learning%2520allows%2520the%2520training%2520of%2520agents%250Athrough%2520various%2520interfaces%252C%2520even%2520for%2520non-expert%2520humans.%2520Recently%252C%250Apreference-based%2520methods%2520%2528PbRL%2529%252C%2520where%2520the%2520human%2520has%2520to%2520give%2520his%2520preference%250Aover%2520two%2520trajectories%252C%2520increased%2520in%2520popularity%2520since%2520they%2520allow%2520training%2520in%250Adomains%2520where%2520more%2520direct%2520feedback%2520is%2520hard%2520to%2520formulate.%2520However%252C%2520the%2520current%250APBRL%2520methods%2520have%2520limitations%2520and%2520do%2520not%2520provide%2520humans%2520with%2520an%2520expressive%250Ainterface%2520for%2520giving%2520feedback.%2520With%2520this%2520work%252C%2520we%2520propose%2520a%2520new%250Apreference-based%2520learning%2520method%2520that%2520provides%2520humans%2520with%2520a%2520more%2520expressive%250Ainterface%2520to%2520provide%2520their%2520preference%2520over%2520trajectories%2520and%2520a%2520factual%250Aexplanation%2520%2528or%2520annotation%2520of%2520why%2520they%2520have%2520this%2520preference%2529.%2520These%250Aexplanations%2520allow%2520the%2520human%2520to%2520explain%2520what%2520parts%2520of%2520the%2520trajectory%2520are%2520most%250Arelevant%2520for%2520the%2520preference.%2520We%2520allow%2520the%2520expression%2520of%2520the%2520explanations%2520over%250Aindividual%2520trajectory%2520steps.%2520We%2520evaluate%2520our%2520method%2520in%2520various%2520simulations%250Ausing%2520a%2520simulated%2520human%2520oracle%2520%2528with%2520realistic%2520restrictions%2529%252C%2520and%2520our%2520results%250Ashow%2520that%2520our%2520extended%2520feedback%2520can%2520improve%2520the%2520speed%2520of%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14244v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell%20me%20why%3A%20Training%20preferences-based%20RL%20with%20human%20preferences%20and%0A%20%20step-level%20explanations&entry.906535625=Jakob%20Karalus&entry.1292438233=%20%20Human-in-the-loop%20reinforcement%20learning%20allows%20the%20training%20of%20agents%0Athrough%20various%20interfaces%2C%20even%20for%20non-expert%20humans.%20Recently%2C%0Apreference-based%20methods%20%28PbRL%29%2C%20where%20the%20human%20has%20to%20give%20his%20preference%0Aover%20two%20trajectories%2C%20increased%20in%20popularity%20since%20they%20allow%20training%20in%0Adomains%20where%20more%20direct%20feedback%20is%20hard%20to%20formulate.%20However%2C%20the%20current%0APBRL%20methods%20have%20limitations%20and%20do%20not%20provide%20humans%20with%20an%20expressive%0Ainterface%20for%20giving%20feedback.%20With%20this%20work%2C%20we%20propose%20a%20new%0Apreference-based%20learning%20method%20that%20provides%20humans%20with%20a%20more%20expressive%0Ainterface%20to%20provide%20their%20preference%20over%20trajectories%20and%20a%20factual%0Aexplanation%20%28or%20annotation%20of%20why%20they%20have%20this%20preference%29.%20These%0Aexplanations%20allow%20the%20human%20to%20explain%20what%20parts%20of%20the%20trajectory%20are%20most%0Arelevant%20for%20the%20preference.%20We%20allow%20the%20expression%20of%20the%20explanations%20over%0Aindividual%20trajectory%20steps.%20We%20evaluate%20our%20method%20in%20various%20simulations%0Ausing%20a%20simulated%20human%20oracle%20%28with%20realistic%20restrictions%29%2C%20and%20our%20results%0Ashow%20that%20our%20extended%20feedback%20can%20improve%20the%20speed%20of%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14244v2&entry.124074799=Read"},
{"title": "Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture\n  and Automated Deployment Flow", "author": "Philip Wiese and Gamze \u0130slamo\u011flu and Moritz Scherer and Luka Macan and Victor J. B. Jung and Alessio Burrello and Francesco Conti and Luca Benini", "abstract": "  One of the challenges for Tiny Machine Learning (tinyML) is keeping up with\nthe evolution of Machine Learning models from Convolutional Neural Networks to\nTransformers. We address this by leveraging a heterogeneous architectural\ntemplate coupling RISC-V processors with hardwired accelerators supported by an\nautomated deployment flow. We demonstrate an Attention-based model in a tinyML\npower envelope with an octa-core cluster coupled with an accelerator for\nquantized Attention. Our deployment flow enables an end-to-end 8-bit\nMobileBERT, achieving leading-edge energy efficiency and throughput of 2960\nGOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI\ntechnology).\n", "link": "http://arxiv.org/abs/2408.02473v1", "date": "2024-08-05", "relevancy": 1.6205, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5462}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Attention-based%20TinyML%3A%20A%20Heterogeneous%20Accelerated%20Architecture%0A%20%20and%20Automated%20Deployment%20Flow&body=Title%3A%20Toward%20Attention-based%20TinyML%3A%20A%20Heterogeneous%20Accelerated%20Architecture%0A%20%20and%20Automated%20Deployment%20Flow%0AAuthor%3A%20Philip%20Wiese%20and%20Gamze%20%C4%B0slamo%C4%9Flu%20and%20Moritz%20Scherer%20and%20Luka%20Macan%20and%20Victor%20J.%20B.%20Jung%20and%20Alessio%20Burrello%20and%20Francesco%20Conti%20and%20Luca%20Benini%0AAbstract%3A%20%20%20One%20of%20the%20challenges%20for%20Tiny%20Machine%20Learning%20%28tinyML%29%20is%20keeping%20up%20with%0Athe%20evolution%20of%20Machine%20Learning%20models%20from%20Convolutional%20Neural%20Networks%20to%0ATransformers.%20We%20address%20this%20by%20leveraging%20a%20heterogeneous%20architectural%0Atemplate%20coupling%20RISC-V%20processors%20with%20hardwired%20accelerators%20supported%20by%20an%0Aautomated%20deployment%20flow.%20We%20demonstrate%20an%20Attention-based%20model%20in%20a%20tinyML%0Apower%20envelope%20with%20an%20octa-core%20cluster%20coupled%20with%20an%20accelerator%20for%0Aquantized%20Attention.%20Our%20deployment%20flow%20enables%20an%20end-to-end%208-bit%0AMobileBERT%2C%20achieving%20leading-edge%20energy%20efficiency%20and%20throughput%20of%202960%0AGOp/J%20and%20154%20GOp/s%20at%2032.5%20Inf/s%20consuming%2052.0%20mW%20%280.65%20V%2C%2022%20nm%20FD-SOI%0Atechnology%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Attention-based%2520TinyML%253A%2520A%2520Heterogeneous%2520Accelerated%2520Architecture%250A%2520%2520and%2520Automated%2520Deployment%2520Flow%26entry.906535625%3DPhilip%2520Wiese%2520and%2520Gamze%2520%25C4%25B0slamo%25C4%259Flu%2520and%2520Moritz%2520Scherer%2520and%2520Luka%2520Macan%2520and%2520Victor%2520J.%2520B.%2520Jung%2520and%2520Alessio%2520Burrello%2520and%2520Francesco%2520Conti%2520and%2520Luca%2520Benini%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520challenges%2520for%2520Tiny%2520Machine%2520Learning%2520%2528tinyML%2529%2520is%2520keeping%2520up%2520with%250Athe%2520evolution%2520of%2520Machine%2520Learning%2520models%2520from%2520Convolutional%2520Neural%2520Networks%2520to%250ATransformers.%2520We%2520address%2520this%2520by%2520leveraging%2520a%2520heterogeneous%2520architectural%250Atemplate%2520coupling%2520RISC-V%2520processors%2520with%2520hardwired%2520accelerators%2520supported%2520by%2520an%250Aautomated%2520deployment%2520flow.%2520We%2520demonstrate%2520an%2520Attention-based%2520model%2520in%2520a%2520tinyML%250Apower%2520envelope%2520with%2520an%2520octa-core%2520cluster%2520coupled%2520with%2520an%2520accelerator%2520for%250Aquantized%2520Attention.%2520Our%2520deployment%2520flow%2520enables%2520an%2520end-to-end%25208-bit%250AMobileBERT%252C%2520achieving%2520leading-edge%2520energy%2520efficiency%2520and%2520throughput%2520of%25202960%250AGOp/J%2520and%2520154%2520GOp/s%2520at%252032.5%2520Inf/s%2520consuming%252052.0%2520mW%2520%25280.65%2520V%252C%252022%2520nm%2520FD-SOI%250Atechnology%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Attention-based%20TinyML%3A%20A%20Heterogeneous%20Accelerated%20Architecture%0A%20%20and%20Automated%20Deployment%20Flow&entry.906535625=Philip%20Wiese%20and%20Gamze%20%C4%B0slamo%C4%9Flu%20and%20Moritz%20Scherer%20and%20Luka%20Macan%20and%20Victor%20J.%20B.%20Jung%20and%20Alessio%20Burrello%20and%20Francesco%20Conti%20and%20Luca%20Benini&entry.1292438233=%20%20One%20of%20the%20challenges%20for%20Tiny%20Machine%20Learning%20%28tinyML%29%20is%20keeping%20up%20with%0Athe%20evolution%20of%20Machine%20Learning%20models%20from%20Convolutional%20Neural%20Networks%20to%0ATransformers.%20We%20address%20this%20by%20leveraging%20a%20heterogeneous%20architectural%0Atemplate%20coupling%20RISC-V%20processors%20with%20hardwired%20accelerators%20supported%20by%20an%0Aautomated%20deployment%20flow.%20We%20demonstrate%20an%20Attention-based%20model%20in%20a%20tinyML%0Apower%20envelope%20with%20an%20octa-core%20cluster%20coupled%20with%20an%20accelerator%20for%0Aquantized%20Attention.%20Our%20deployment%20flow%20enables%20an%20end-to-end%208-bit%0AMobileBERT%2C%20achieving%20leading-edge%20energy%20efficiency%20and%20throughput%20of%202960%0AGOp/J%20and%20154%20GOp/s%20at%2032.5%20Inf/s%20consuming%2052.0%20mW%20%280.65%20V%2C%2022%20nm%20FD-SOI%0Atechnology%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02473v1&entry.124074799=Read"},
{"title": "Bimanual Manipulation of Steady Hand Eye Robots with Adaptive Sclera\n  Force Control: Cooperative vs. Teleoperation Strategies", "author": "Mojtaba Esfandiari and Peter Gehlbach and Russell H. Taylor and Iulian Iordachita", "abstract": "  Performing retinal vein cannulation (RVC) as a potential treatment for\nretinal vein occlusion (RVO) without the assistance of a surgical robotic\nsystem is very challenging to do safely. The main limitation is the\nphysiological hand tremor of surgeons. Robot-assisted eye surgery technology\nmay resolve the problems of hand tremors and fatigue and improve the safety and\nprecision of RVC. The Steady-Hand Eye Robot (SHER) is an admittance-based\nrobotic system that can filter out hand tremors and enables ophthalmologists to\nmanipulate a surgical instrument inside the eye cooperatively. However, the\nadmittance-based cooperative control mode does not safely minimize the contact\nforce between the surgical instrument and the sclera to prevent tissue damage.\nAdditionally, features like haptic feedback or hand motion scaling, which can\nimprove the safety and precision of surgery, require a teleoperation control\nframework. This work presents a bimanual adaptive teleoperation (BMAT) control\nframework using SHER 2.0 and SHER 2.1 robotic systems. We integrate them with\nan adaptive force control (AFC) algorithm to automatically minimize the\ntool-sclera interaction force. The scleral forces are measured using two fiber\nBragg grating (FBG)-based force-sensing tools. We compare the performance of\nthe BMAT mode with a bimanual adaptive cooperative (BMAC) mode in a\nvessel-following experiment under a surgical microscope. Experimental results\ndemonstrate the effectiveness of the proposed BMAT control framework in\nperforming a safe bimanual telemanipulation of the eye without over-stretching\nit, even in the absence of registration between the two robots.\n", "link": "http://arxiv.org/abs/2402.18088v2", "date": "2024-08-05", "relevancy": 1.6192, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5667}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5151}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bimanual%20Manipulation%20of%20Steady%20Hand%20Eye%20Robots%20with%20Adaptive%20Sclera%0A%20%20Force%20Control%3A%20Cooperative%20vs.%20Teleoperation%20Strategies&body=Title%3A%20Bimanual%20Manipulation%20of%20Steady%20Hand%20Eye%20Robots%20with%20Adaptive%20Sclera%0A%20%20Force%20Control%3A%20Cooperative%20vs.%20Teleoperation%20Strategies%0AAuthor%3A%20Mojtaba%20Esfandiari%20and%20Peter%20Gehlbach%20and%20Russell%20H.%20Taylor%20and%20Iulian%20Iordachita%0AAbstract%3A%20%20%20Performing%20retinal%20vein%20cannulation%20%28RVC%29%20as%20a%20potential%20treatment%20for%0Aretinal%20vein%20occlusion%20%28RVO%29%20without%20the%20assistance%20of%20a%20surgical%20robotic%0Asystem%20is%20very%20challenging%20to%20do%20safely.%20The%20main%20limitation%20is%20the%0Aphysiological%20hand%20tremor%20of%20surgeons.%20Robot-assisted%20eye%20surgery%20technology%0Amay%20resolve%20the%20problems%20of%20hand%20tremors%20and%20fatigue%20and%20improve%20the%20safety%20and%0Aprecision%20of%20RVC.%20The%20Steady-Hand%20Eye%20Robot%20%28SHER%29%20is%20an%20admittance-based%0Arobotic%20system%20that%20can%20filter%20out%20hand%20tremors%20and%20enables%20ophthalmologists%20to%0Amanipulate%20a%20surgical%20instrument%20inside%20the%20eye%20cooperatively.%20However%2C%20the%0Aadmittance-based%20cooperative%20control%20mode%20does%20not%20safely%20minimize%20the%20contact%0Aforce%20between%20the%20surgical%20instrument%20and%20the%20sclera%20to%20prevent%20tissue%20damage.%0AAdditionally%2C%20features%20like%20haptic%20feedback%20or%20hand%20motion%20scaling%2C%20which%20can%0Aimprove%20the%20safety%20and%20precision%20of%20surgery%2C%20require%20a%20teleoperation%20control%0Aframework.%20This%20work%20presents%20a%20bimanual%20adaptive%20teleoperation%20%28BMAT%29%20control%0Aframework%20using%20SHER%202.0%20and%20SHER%202.1%20robotic%20systems.%20We%20integrate%20them%20with%0Aan%20adaptive%20force%20control%20%28AFC%29%20algorithm%20to%20automatically%20minimize%20the%0Atool-sclera%20interaction%20force.%20The%20scleral%20forces%20are%20measured%20using%20two%20fiber%0ABragg%20grating%20%28FBG%29-based%20force-sensing%20tools.%20We%20compare%20the%20performance%20of%0Athe%20BMAT%20mode%20with%20a%20bimanual%20adaptive%20cooperative%20%28BMAC%29%20mode%20in%20a%0Avessel-following%20experiment%20under%20a%20surgical%20microscope.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20BMAT%20control%20framework%20in%0Aperforming%20a%20safe%20bimanual%20telemanipulation%20of%20the%20eye%20without%20over-stretching%0Ait%2C%20even%20in%20the%20absence%20of%20registration%20between%20the%20two%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBimanual%2520Manipulation%2520of%2520Steady%2520Hand%2520Eye%2520Robots%2520with%2520Adaptive%2520Sclera%250A%2520%2520Force%2520Control%253A%2520Cooperative%2520vs.%2520Teleoperation%2520Strategies%26entry.906535625%3DMojtaba%2520Esfandiari%2520and%2520Peter%2520Gehlbach%2520and%2520Russell%2520H.%2520Taylor%2520and%2520Iulian%2520Iordachita%26entry.1292438233%3D%2520%2520Performing%2520retinal%2520vein%2520cannulation%2520%2528RVC%2529%2520as%2520a%2520potential%2520treatment%2520for%250Aretinal%2520vein%2520occlusion%2520%2528RVO%2529%2520without%2520the%2520assistance%2520of%2520a%2520surgical%2520robotic%250Asystem%2520is%2520very%2520challenging%2520to%2520do%2520safely.%2520The%2520main%2520limitation%2520is%2520the%250Aphysiological%2520hand%2520tremor%2520of%2520surgeons.%2520Robot-assisted%2520eye%2520surgery%2520technology%250Amay%2520resolve%2520the%2520problems%2520of%2520hand%2520tremors%2520and%2520fatigue%2520and%2520improve%2520the%2520safety%2520and%250Aprecision%2520of%2520RVC.%2520The%2520Steady-Hand%2520Eye%2520Robot%2520%2528SHER%2529%2520is%2520an%2520admittance-based%250Arobotic%2520system%2520that%2520can%2520filter%2520out%2520hand%2520tremors%2520and%2520enables%2520ophthalmologists%2520to%250Amanipulate%2520a%2520surgical%2520instrument%2520inside%2520the%2520eye%2520cooperatively.%2520However%252C%2520the%250Aadmittance-based%2520cooperative%2520control%2520mode%2520does%2520not%2520safely%2520minimize%2520the%2520contact%250Aforce%2520between%2520the%2520surgical%2520instrument%2520and%2520the%2520sclera%2520to%2520prevent%2520tissue%2520damage.%250AAdditionally%252C%2520features%2520like%2520haptic%2520feedback%2520or%2520hand%2520motion%2520scaling%252C%2520which%2520can%250Aimprove%2520the%2520safety%2520and%2520precision%2520of%2520surgery%252C%2520require%2520a%2520teleoperation%2520control%250Aframework.%2520This%2520work%2520presents%2520a%2520bimanual%2520adaptive%2520teleoperation%2520%2528BMAT%2529%2520control%250Aframework%2520using%2520SHER%25202.0%2520and%2520SHER%25202.1%2520robotic%2520systems.%2520We%2520integrate%2520them%2520with%250Aan%2520adaptive%2520force%2520control%2520%2528AFC%2529%2520algorithm%2520to%2520automatically%2520minimize%2520the%250Atool-sclera%2520interaction%2520force.%2520The%2520scleral%2520forces%2520are%2520measured%2520using%2520two%2520fiber%250ABragg%2520grating%2520%2528FBG%2529-based%2520force-sensing%2520tools.%2520We%2520compare%2520the%2520performance%2520of%250Athe%2520BMAT%2520mode%2520with%2520a%2520bimanual%2520adaptive%2520cooperative%2520%2528BMAC%2529%2520mode%2520in%2520a%250Avessel-following%2520experiment%2520under%2520a%2520surgical%2520microscope.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520BMAT%2520control%2520framework%2520in%250Aperforming%2520a%2520safe%2520bimanual%2520telemanipulation%2520of%2520the%2520eye%2520without%2520over-stretching%250Ait%252C%2520even%2520in%2520the%2520absence%2520of%2520registration%2520between%2520the%2520two%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bimanual%20Manipulation%20of%20Steady%20Hand%20Eye%20Robots%20with%20Adaptive%20Sclera%0A%20%20Force%20Control%3A%20Cooperative%20vs.%20Teleoperation%20Strategies&entry.906535625=Mojtaba%20Esfandiari%20and%20Peter%20Gehlbach%20and%20Russell%20H.%20Taylor%20and%20Iulian%20Iordachita&entry.1292438233=%20%20Performing%20retinal%20vein%20cannulation%20%28RVC%29%20as%20a%20potential%20treatment%20for%0Aretinal%20vein%20occlusion%20%28RVO%29%20without%20the%20assistance%20of%20a%20surgical%20robotic%0Asystem%20is%20very%20challenging%20to%20do%20safely.%20The%20main%20limitation%20is%20the%0Aphysiological%20hand%20tremor%20of%20surgeons.%20Robot-assisted%20eye%20surgery%20technology%0Amay%20resolve%20the%20problems%20of%20hand%20tremors%20and%20fatigue%20and%20improve%20the%20safety%20and%0Aprecision%20of%20RVC.%20The%20Steady-Hand%20Eye%20Robot%20%28SHER%29%20is%20an%20admittance-based%0Arobotic%20system%20that%20can%20filter%20out%20hand%20tremors%20and%20enables%20ophthalmologists%20to%0Amanipulate%20a%20surgical%20instrument%20inside%20the%20eye%20cooperatively.%20However%2C%20the%0Aadmittance-based%20cooperative%20control%20mode%20does%20not%20safely%20minimize%20the%20contact%0Aforce%20between%20the%20surgical%20instrument%20and%20the%20sclera%20to%20prevent%20tissue%20damage.%0AAdditionally%2C%20features%20like%20haptic%20feedback%20or%20hand%20motion%20scaling%2C%20which%20can%0Aimprove%20the%20safety%20and%20precision%20of%20surgery%2C%20require%20a%20teleoperation%20control%0Aframework.%20This%20work%20presents%20a%20bimanual%20adaptive%20teleoperation%20%28BMAT%29%20control%0Aframework%20using%20SHER%202.0%20and%20SHER%202.1%20robotic%20systems.%20We%20integrate%20them%20with%0Aan%20adaptive%20force%20control%20%28AFC%29%20algorithm%20to%20automatically%20minimize%20the%0Atool-sclera%20interaction%20force.%20The%20scleral%20forces%20are%20measured%20using%20two%20fiber%0ABragg%20grating%20%28FBG%29-based%20force-sensing%20tools.%20We%20compare%20the%20performance%20of%0Athe%20BMAT%20mode%20with%20a%20bimanual%20adaptive%20cooperative%20%28BMAC%29%20mode%20in%20a%0Avessel-following%20experiment%20under%20a%20surgical%20microscope.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20BMAT%20control%20framework%20in%0Aperforming%20a%20safe%20bimanual%20telemanipulation%20of%20the%20eye%20without%20over-stretching%0Ait%2C%20even%20in%20the%20absence%20of%20registration%20between%20the%20two%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18088v2&entry.124074799=Read"},
{"title": "Towards Coarse-grained Visual Language Navigation Task Planning Enhanced\n  by Event Knowledge Graph", "author": "Zhao Kaichen and Song Yaoxian and Zhao Haiquan and Liu Haoyu and Li Tiefeng and Li Zhixu", "abstract": "  Visual language navigation (VLN) is one of the important research in embodied\nAI. It aims to enable an agent to understand the surrounding environment and\ncomplete navigation tasks. VLN instructions could be categorized into\ncoarse-grained and fine-grained commands. Fine-grained command describes a\nwhole task with subtasks step-by-step. In contrast, coarse-grained command\ngives an abstract task description, which more suites human habits. Most\nexisting work focuses on the former kind of instruction in VLN tasks, ignoring\nthe latter abstract instructions belonging to daily life scenarios. To overcome\nthe above challenge in abstract instruction, we attempt to consider\ncoarse-grained instruction in VLN by event knowledge enhancement. Specifically,\nwe first propose a prompt-based framework to extract an event knowledge graph\n(named VLN-EventKG) for VLN integrally over multiple mainstream benchmark\ndatasets. Through small and large language model collaboration, we realize\nknowledge-enhanced navigation planning (named EventNav) for VLN tasks with\ncoarse-grained instruction input. Additionally, we design a novel dynamic\nhistory backtracking module to correct potential error action planning in real\ntime. Experimental results in various public benchmarks show our\nknowledge-enhanced method has superiority in coarse-grained-instruction VLN\nusing our proposed VLN-EventKG with over $5\\%$ improvement in success rate. Our\nproject is available at https://sites.google.com/view/vln-eventkg\n", "link": "http://arxiv.org/abs/2408.02535v1", "date": "2024-08-05", "relevancy": 1.6178, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Coarse-grained%20Visual%20Language%20Navigation%20Task%20Planning%20Enhanced%0A%20%20by%20Event%20Knowledge%20Graph&body=Title%3A%20Towards%20Coarse-grained%20Visual%20Language%20Navigation%20Task%20Planning%20Enhanced%0A%20%20by%20Event%20Knowledge%20Graph%0AAuthor%3A%20Zhao%20Kaichen%20and%20Song%20Yaoxian%20and%20Zhao%20Haiquan%20and%20Liu%20Haoyu%20and%20Li%20Tiefeng%20and%20Li%20Zhixu%0AAbstract%3A%20%20%20Visual%20language%20navigation%20%28VLN%29%20is%20one%20of%20the%20important%20research%20in%20embodied%0AAI.%20It%20aims%20to%20enable%20an%20agent%20to%20understand%20the%20surrounding%20environment%20and%0Acomplete%20navigation%20tasks.%20VLN%20instructions%20could%20be%20categorized%20into%0Acoarse-grained%20and%20fine-grained%20commands.%20Fine-grained%20command%20describes%20a%0Awhole%20task%20with%20subtasks%20step-by-step.%20In%20contrast%2C%20coarse-grained%20command%0Agives%20an%20abstract%20task%20description%2C%20which%20more%20suites%20human%20habits.%20Most%0Aexisting%20work%20focuses%20on%20the%20former%20kind%20of%20instruction%20in%20VLN%20tasks%2C%20ignoring%0Athe%20latter%20abstract%20instructions%20belonging%20to%20daily%20life%20scenarios.%20To%20overcome%0Athe%20above%20challenge%20in%20abstract%20instruction%2C%20we%20attempt%20to%20consider%0Acoarse-grained%20instruction%20in%20VLN%20by%20event%20knowledge%20enhancement.%20Specifically%2C%0Awe%20first%20propose%20a%20prompt-based%20framework%20to%20extract%20an%20event%20knowledge%20graph%0A%28named%20VLN-EventKG%29%20for%20VLN%20integrally%20over%20multiple%20mainstream%20benchmark%0Adatasets.%20Through%20small%20and%20large%20language%20model%20collaboration%2C%20we%20realize%0Aknowledge-enhanced%20navigation%20planning%20%28named%20EventNav%29%20for%20VLN%20tasks%20with%0Acoarse-grained%20instruction%20input.%20Additionally%2C%20we%20design%20a%20novel%20dynamic%0Ahistory%20backtracking%20module%20to%20correct%20potential%20error%20action%20planning%20in%20real%0Atime.%20Experimental%20results%20in%20various%20public%20benchmarks%20show%20our%0Aknowledge-enhanced%20method%20has%20superiority%20in%20coarse-grained-instruction%20VLN%0Ausing%20our%20proposed%20VLN-EventKG%20with%20over%20%245%5C%25%24%20improvement%20in%20success%20rate.%20Our%0Aproject%20is%20available%20at%20https%3A//sites.google.com/view/vln-eventkg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Coarse-grained%2520Visual%2520Language%2520Navigation%2520Task%2520Planning%2520Enhanced%250A%2520%2520by%2520Event%2520Knowledge%2520Graph%26entry.906535625%3DZhao%2520Kaichen%2520and%2520Song%2520Yaoxian%2520and%2520Zhao%2520Haiquan%2520and%2520Liu%2520Haoyu%2520and%2520Li%2520Tiefeng%2520and%2520Li%2520Zhixu%26entry.1292438233%3D%2520%2520Visual%2520language%2520navigation%2520%2528VLN%2529%2520is%2520one%2520of%2520the%2520important%2520research%2520in%2520embodied%250AAI.%2520It%2520aims%2520to%2520enable%2520an%2520agent%2520to%2520understand%2520the%2520surrounding%2520environment%2520and%250Acomplete%2520navigation%2520tasks.%2520VLN%2520instructions%2520could%2520be%2520categorized%2520into%250Acoarse-grained%2520and%2520fine-grained%2520commands.%2520Fine-grained%2520command%2520describes%2520a%250Awhole%2520task%2520with%2520subtasks%2520step-by-step.%2520In%2520contrast%252C%2520coarse-grained%2520command%250Agives%2520an%2520abstract%2520task%2520description%252C%2520which%2520more%2520suites%2520human%2520habits.%2520Most%250Aexisting%2520work%2520focuses%2520on%2520the%2520former%2520kind%2520of%2520instruction%2520in%2520VLN%2520tasks%252C%2520ignoring%250Athe%2520latter%2520abstract%2520instructions%2520belonging%2520to%2520daily%2520life%2520scenarios.%2520To%2520overcome%250Athe%2520above%2520challenge%2520in%2520abstract%2520instruction%252C%2520we%2520attempt%2520to%2520consider%250Acoarse-grained%2520instruction%2520in%2520VLN%2520by%2520event%2520knowledge%2520enhancement.%2520Specifically%252C%250Awe%2520first%2520propose%2520a%2520prompt-based%2520framework%2520to%2520extract%2520an%2520event%2520knowledge%2520graph%250A%2528named%2520VLN-EventKG%2529%2520for%2520VLN%2520integrally%2520over%2520multiple%2520mainstream%2520benchmark%250Adatasets.%2520Through%2520small%2520and%2520large%2520language%2520model%2520collaboration%252C%2520we%2520realize%250Aknowledge-enhanced%2520navigation%2520planning%2520%2528named%2520EventNav%2529%2520for%2520VLN%2520tasks%2520with%250Acoarse-grained%2520instruction%2520input.%2520Additionally%252C%2520we%2520design%2520a%2520novel%2520dynamic%250Ahistory%2520backtracking%2520module%2520to%2520correct%2520potential%2520error%2520action%2520planning%2520in%2520real%250Atime.%2520Experimental%2520results%2520in%2520various%2520public%2520benchmarks%2520show%2520our%250Aknowledge-enhanced%2520method%2520has%2520superiority%2520in%2520coarse-grained-instruction%2520VLN%250Ausing%2520our%2520proposed%2520VLN-EventKG%2520with%2520over%2520%25245%255C%2525%2524%2520improvement%2520in%2520success%2520rate.%2520Our%250Aproject%2520is%2520available%2520at%2520https%253A//sites.google.com/view/vln-eventkg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Coarse-grained%20Visual%20Language%20Navigation%20Task%20Planning%20Enhanced%0A%20%20by%20Event%20Knowledge%20Graph&entry.906535625=Zhao%20Kaichen%20and%20Song%20Yaoxian%20and%20Zhao%20Haiquan%20and%20Liu%20Haoyu%20and%20Li%20Tiefeng%20and%20Li%20Zhixu&entry.1292438233=%20%20Visual%20language%20navigation%20%28VLN%29%20is%20one%20of%20the%20important%20research%20in%20embodied%0AAI.%20It%20aims%20to%20enable%20an%20agent%20to%20understand%20the%20surrounding%20environment%20and%0Acomplete%20navigation%20tasks.%20VLN%20instructions%20could%20be%20categorized%20into%0Acoarse-grained%20and%20fine-grained%20commands.%20Fine-grained%20command%20describes%20a%0Awhole%20task%20with%20subtasks%20step-by-step.%20In%20contrast%2C%20coarse-grained%20command%0Agives%20an%20abstract%20task%20description%2C%20which%20more%20suites%20human%20habits.%20Most%0Aexisting%20work%20focuses%20on%20the%20former%20kind%20of%20instruction%20in%20VLN%20tasks%2C%20ignoring%0Athe%20latter%20abstract%20instructions%20belonging%20to%20daily%20life%20scenarios.%20To%20overcome%0Athe%20above%20challenge%20in%20abstract%20instruction%2C%20we%20attempt%20to%20consider%0Acoarse-grained%20instruction%20in%20VLN%20by%20event%20knowledge%20enhancement.%20Specifically%2C%0Awe%20first%20propose%20a%20prompt-based%20framework%20to%20extract%20an%20event%20knowledge%20graph%0A%28named%20VLN-EventKG%29%20for%20VLN%20integrally%20over%20multiple%20mainstream%20benchmark%0Adatasets.%20Through%20small%20and%20large%20language%20model%20collaboration%2C%20we%20realize%0Aknowledge-enhanced%20navigation%20planning%20%28named%20EventNav%29%20for%20VLN%20tasks%20with%0Acoarse-grained%20instruction%20input.%20Additionally%2C%20we%20design%20a%20novel%20dynamic%0Ahistory%20backtracking%20module%20to%20correct%20potential%20error%20action%20planning%20in%20real%0Atime.%20Experimental%20results%20in%20various%20public%20benchmarks%20show%20our%0Aknowledge-enhanced%20method%20has%20superiority%20in%20coarse-grained-instruction%20VLN%0Ausing%20our%20proposed%20VLN-EventKG%20with%20over%20%245%5C%25%24%20improvement%20in%20success%20rate.%20Our%0Aproject%20is%20available%20at%20https%3A//sites.google.com/view/vln-eventkg%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02535v1&entry.124074799=Read"},
{"title": "Unsupervised Change Detection for Space Habitats Using 3D Point Clouds", "author": "Jamie Santos and Holly Dinkel and Julia Di and Paulo V. K. Borges and Marina Moreira and Oleg Alexandrov and Brian Coltin and Trey Smith", "abstract": "  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n", "link": "http://arxiv.org/abs/2312.02396v3", "date": "2024-08-05", "relevancy": 1.5997, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5731}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Change%20Detection%20for%20Space%20Habitats%20Using%203D%20Point%20Clouds&body=Title%3A%20Unsupervised%20Change%20Detection%20for%20Space%20Habitats%20Using%203D%20Point%20Clouds%0AAuthor%3A%20Jamie%20Santos%20and%20Holly%20Dinkel%20and%20Julia%20Di%20and%20Paulo%20V.%20K.%20Borges%20and%20Marina%20Moreira%20and%20Oleg%20Alexandrov%20and%20Brian%20Coltin%20and%20Trey%20Smith%0AAbstract%3A%20%20%20This%20work%20presents%20an%20algorithm%20for%20scene%20change%20detection%20from%20point%20clouds%0Ato%20enable%20autonomous%20robotic%20caretaking%20in%20future%20space%20habitats.%20Autonomous%0Arobotic%20systems%20will%20help%20maintain%20future%20deep-space%20habitats%2C%20such%20as%20the%0AGateway%20space%20station%2C%20which%20will%20be%20uncrewed%20for%20extended%20periods.%20Existing%0Ascene%20analysis%20software%20used%20on%20the%20International%20Space%20Station%20%28ISS%29%20relies%20on%0Amanually-labeled%20images%20for%20detecting%20changes.%20In%20contrast%2C%20the%20algorithm%0Apresented%20in%20this%20work%20uses%20raw%2C%20unlabeled%20point%20clouds%20as%20inputs.%20The%0Aalgorithm%20first%20applies%20modified%20Expectation-Maximization%20Gaussian%20Mixture%0AModel%20%28GMM%29%20clustering%20to%20two%20input%20point%20clouds.%20It%20then%20performs%20change%0Adetection%20by%20comparing%20the%20GMMs%20using%20the%20Earth%20Mover%27s%20Distance.%20The%20algorithm%0Ais%20validated%20quantitatively%20and%20qualitatively%20using%20a%20test%20dataset%20collected%20by%0Aan%20Astrobee%20robot%20in%20the%20NASA%20Ames%20Granite%20Lab%20comprising%20single%20frame%20depth%0Aimages%20taken%20directly%20by%20Astrobee%20and%20full-scene%20reconstructed%20maps%20built%20with%0ARGB-D%20and%20pose%20data%20from%20Astrobee.%20The%20runtimes%20of%20the%20approach%20are%20also%0Aanalyzed%20in%20depth.%20The%20source%20code%20is%20publicly%20released%20to%20promote%20further%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02396v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Change%2520Detection%2520for%2520Space%2520Habitats%2520Using%25203D%2520Point%2520Clouds%26entry.906535625%3DJamie%2520Santos%2520and%2520Holly%2520Dinkel%2520and%2520Julia%2520Di%2520and%2520Paulo%2520V.%2520K.%2520Borges%2520and%2520Marina%2520Moreira%2520and%2520Oleg%2520Alexandrov%2520and%2520Brian%2520Coltin%2520and%2520Trey%2520Smith%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520an%2520algorithm%2520for%2520scene%2520change%2520detection%2520from%2520point%2520clouds%250Ato%2520enable%2520autonomous%2520robotic%2520caretaking%2520in%2520future%2520space%2520habitats.%2520Autonomous%250Arobotic%2520systems%2520will%2520help%2520maintain%2520future%2520deep-space%2520habitats%252C%2520such%2520as%2520the%250AGateway%2520space%2520station%252C%2520which%2520will%2520be%2520uncrewed%2520for%2520extended%2520periods.%2520Existing%250Ascene%2520analysis%2520software%2520used%2520on%2520the%2520International%2520Space%2520Station%2520%2528ISS%2529%2520relies%2520on%250Amanually-labeled%2520images%2520for%2520detecting%2520changes.%2520In%2520contrast%252C%2520the%2520algorithm%250Apresented%2520in%2520this%2520work%2520uses%2520raw%252C%2520unlabeled%2520point%2520clouds%2520as%2520inputs.%2520The%250Aalgorithm%2520first%2520applies%2520modified%2520Expectation-Maximization%2520Gaussian%2520Mixture%250AModel%2520%2528GMM%2529%2520clustering%2520to%2520two%2520input%2520point%2520clouds.%2520It%2520then%2520performs%2520change%250Adetection%2520by%2520comparing%2520the%2520GMMs%2520using%2520the%2520Earth%2520Mover%2527s%2520Distance.%2520The%2520algorithm%250Ais%2520validated%2520quantitatively%2520and%2520qualitatively%2520using%2520a%2520test%2520dataset%2520collected%2520by%250Aan%2520Astrobee%2520robot%2520in%2520the%2520NASA%2520Ames%2520Granite%2520Lab%2520comprising%2520single%2520frame%2520depth%250Aimages%2520taken%2520directly%2520by%2520Astrobee%2520and%2520full-scene%2520reconstructed%2520maps%2520built%2520with%250ARGB-D%2520and%2520pose%2520data%2520from%2520Astrobee.%2520The%2520runtimes%2520of%2520the%2520approach%2520are%2520also%250Aanalyzed%2520in%2520depth.%2520The%2520source%2520code%2520is%2520publicly%2520released%2520to%2520promote%2520further%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02396v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Change%20Detection%20for%20Space%20Habitats%20Using%203D%20Point%20Clouds&entry.906535625=Jamie%20Santos%20and%20Holly%20Dinkel%20and%20Julia%20Di%20and%20Paulo%20V.%20K.%20Borges%20and%20Marina%20Moreira%20and%20Oleg%20Alexandrov%20and%20Brian%20Coltin%20and%20Trey%20Smith&entry.1292438233=%20%20This%20work%20presents%20an%20algorithm%20for%20scene%20change%20detection%20from%20point%20clouds%0Ato%20enable%20autonomous%20robotic%20caretaking%20in%20future%20space%20habitats.%20Autonomous%0Arobotic%20systems%20will%20help%20maintain%20future%20deep-space%20habitats%2C%20such%20as%20the%0AGateway%20space%20station%2C%20which%20will%20be%20uncrewed%20for%20extended%20periods.%20Existing%0Ascene%20analysis%20software%20used%20on%20the%20International%20Space%20Station%20%28ISS%29%20relies%20on%0Amanually-labeled%20images%20for%20detecting%20changes.%20In%20contrast%2C%20the%20algorithm%0Apresented%20in%20this%20work%20uses%20raw%2C%20unlabeled%20point%20clouds%20as%20inputs.%20The%0Aalgorithm%20first%20applies%20modified%20Expectation-Maximization%20Gaussian%20Mixture%0AModel%20%28GMM%29%20clustering%20to%20two%20input%20point%20clouds.%20It%20then%20performs%20change%0Adetection%20by%20comparing%20the%20GMMs%20using%20the%20Earth%20Mover%27s%20Distance.%20The%20algorithm%0Ais%20validated%20quantitatively%20and%20qualitatively%20using%20a%20test%20dataset%20collected%20by%0Aan%20Astrobee%20robot%20in%20the%20NASA%20Ames%20Granite%20Lab%20comprising%20single%20frame%20depth%0Aimages%20taken%20directly%20by%20Astrobee%20and%20full-scene%20reconstructed%20maps%20built%20with%0ARGB-D%20and%20pose%20data%20from%20Astrobee.%20The%20runtimes%20of%20the%20approach%20are%20also%0Aanalyzed%20in%20depth.%20The%20source%20code%20is%20publicly%20released%20to%20promote%20further%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02396v3&entry.124074799=Read"},
{"title": "Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression", "author": "Mateusz Gabor and Rafa\u0142 Zdunek", "abstract": "  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n", "link": "http://arxiv.org/abs/2405.10802v2", "date": "2024-08-05", "relevancy": 1.595, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5427}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5377}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reduced%20storage%20direct%20tensor%20ring%20decomposition%20for%20convolutional%0A%20%20neural%20networks%20compression&body=Title%3A%20Reduced%20storage%20direct%20tensor%20ring%20decomposition%20for%20convolutional%0A%20%20neural%20networks%20compression%0AAuthor%3A%20Mateusz%20Gabor%20and%20Rafa%C5%82%20Zdunek%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20among%20the%20most%20widely%20used%20machine%0Alearning%20models%20for%20computer%20vision%20tasks%2C%20such%20as%20image%20classification.%20To%0Aimprove%20the%20efficiency%20of%20CNNs%2C%20many%20CNNs%20compressing%20approaches%20have%20been%0Adeveloped.%20Low-rank%20methods%20approximate%20the%20original%20convolutional%20kernel%20with%0Aa%20sequence%20of%20smaller%20convolutional%20kernels%2C%20which%20leads%20to%20reduced%20storage%20and%0Atime%20complexities.%20In%20this%20study%2C%20we%20propose%20a%20novel%20low-rank%20CNNs%20compression%0Amethod%20that%20is%20based%20on%20reduced%20storage%20direct%20tensor%20ring%20decomposition%0A%28RSDTR%29.%20The%20proposed%20method%20offers%20a%20higher%20circular%20mode%20permutation%0Aflexibility%2C%20and%20it%20is%20characterized%20by%20large%20parameter%20and%20FLOPS%20compression%0Arates%2C%20while%20preserving%20a%20good%20classification%20accuracy%20of%20the%20compressed%0Anetwork.%20The%20experiments%2C%20performed%20on%20the%20CIFAR-10%20and%20ImageNet%20datasets%2C%0Aclearly%20demonstrate%20the%20efficiency%20of%20RSDTR%20in%20comparison%20to%20other%0Astate-of-the-art%20CNNs%20compression%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReduced%2520storage%2520direct%2520tensor%2520ring%2520decomposition%2520for%2520convolutional%250A%2520%2520neural%2520networks%2520compression%26entry.906535625%3DMateusz%2520Gabor%2520and%2520Rafa%25C5%2582%2520Zdunek%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520are%2520among%2520the%2520most%2520widely%2520used%2520machine%250Alearning%2520models%2520for%2520computer%2520vision%2520tasks%252C%2520such%2520as%2520image%2520classification.%2520To%250Aimprove%2520the%2520efficiency%2520of%2520CNNs%252C%2520many%2520CNNs%2520compressing%2520approaches%2520have%2520been%250Adeveloped.%2520Low-rank%2520methods%2520approximate%2520the%2520original%2520convolutional%2520kernel%2520with%250Aa%2520sequence%2520of%2520smaller%2520convolutional%2520kernels%252C%2520which%2520leads%2520to%2520reduced%2520storage%2520and%250Atime%2520complexities.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520low-rank%2520CNNs%2520compression%250Amethod%2520that%2520is%2520based%2520on%2520reduced%2520storage%2520direct%2520tensor%2520ring%2520decomposition%250A%2528RSDTR%2529.%2520The%2520proposed%2520method%2520offers%2520a%2520higher%2520circular%2520mode%2520permutation%250Aflexibility%252C%2520and%2520it%2520is%2520characterized%2520by%2520large%2520parameter%2520and%2520FLOPS%2520compression%250Arates%252C%2520while%2520preserving%2520a%2520good%2520classification%2520accuracy%2520of%2520the%2520compressed%250Anetwork.%2520The%2520experiments%252C%2520performed%2520on%2520the%2520CIFAR-10%2520and%2520ImageNet%2520datasets%252C%250Aclearly%2520demonstrate%2520the%2520efficiency%2520of%2520RSDTR%2520in%2520comparison%2520to%2520other%250Astate-of-the-art%2520CNNs%2520compression%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reduced%20storage%20direct%20tensor%20ring%20decomposition%20for%20convolutional%0A%20%20neural%20networks%20compression&entry.906535625=Mateusz%20Gabor%20and%20Rafa%C5%82%20Zdunek&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20among%20the%20most%20widely%20used%20machine%0Alearning%20models%20for%20computer%20vision%20tasks%2C%20such%20as%20image%20classification.%20To%0Aimprove%20the%20efficiency%20of%20CNNs%2C%20many%20CNNs%20compressing%20approaches%20have%20been%0Adeveloped.%20Low-rank%20methods%20approximate%20the%20original%20convolutional%20kernel%20with%0Aa%20sequence%20of%20smaller%20convolutional%20kernels%2C%20which%20leads%20to%20reduced%20storage%20and%0Atime%20complexities.%20In%20this%20study%2C%20we%20propose%20a%20novel%20low-rank%20CNNs%20compression%0Amethod%20that%20is%20based%20on%20reduced%20storage%20direct%20tensor%20ring%20decomposition%0A%28RSDTR%29.%20The%20proposed%20method%20offers%20a%20higher%20circular%20mode%20permutation%0Aflexibility%2C%20and%20it%20is%20characterized%20by%20large%20parameter%20and%20FLOPS%20compression%0Arates%2C%20while%20preserving%20a%20good%20classification%20accuracy%20of%20the%20compressed%0Anetwork.%20The%20experiments%2C%20performed%20on%20the%20CIFAR-10%20and%20ImageNet%20datasets%2C%0Aclearly%20demonstrate%20the%20efficiency%20of%20RSDTR%20in%20comparison%20to%20other%0Astate-of-the-art%20CNNs%20compression%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10802v2&entry.124074799=Read"},
{"title": "Learning to Imitate Spatial Organization in Multi-robot Systems", "author": "Ayomide O. Agunloye and Sarvapali D. Ramchurn and Mohammad D. Soorati", "abstract": "  Understanding collective behavior and how it evolves is important to ensure\nthat robot swarms can be trusted in a shared environment. One way to understand\nthe behavior of the swarm is through collective behavior reconstruction using\nprior demonstrations. Existing approaches often require access to the swarm\ncontroller which may not be available. We reconstruct collective behaviors in\ndistinct swarm scenarios involving shared environments without using swarm\ncontroller information. We achieve this by transforming prior demonstrations\ninto features that describe multi-agent interactions before behavior\nreconstruction with multi-agent generative adversarial imitation learning\n(MA-GAIL). We show that our approach outperforms existing algorithms in spatial\norganization, and can be used to observe and reconstruct a swarm's behavior for\nfurther analysis and testing, which might be impractical or undesirable on the\noriginal robot swarm.\n", "link": "http://arxiv.org/abs/2407.11592v2", "date": "2024-08-05", "relevancy": 1.59, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Imitate%20Spatial%20Organization%20in%20Multi-robot%20Systems&body=Title%3A%20Learning%20to%20Imitate%20Spatial%20Organization%20in%20Multi-robot%20Systems%0AAuthor%3A%20Ayomide%20O.%20Agunloye%20and%20Sarvapali%20D.%20Ramchurn%20and%20Mohammad%20D.%20Soorati%0AAbstract%3A%20%20%20Understanding%20collective%20behavior%20and%20how%20it%20evolves%20is%20important%20to%20ensure%0Athat%20robot%20swarms%20can%20be%20trusted%20in%20a%20shared%20environment.%20One%20way%20to%20understand%0Athe%20behavior%20of%20the%20swarm%20is%20through%20collective%20behavior%20reconstruction%20using%0Aprior%20demonstrations.%20Existing%20approaches%20often%20require%20access%20to%20the%20swarm%0Acontroller%20which%20may%20not%20be%20available.%20We%20reconstruct%20collective%20behaviors%20in%0Adistinct%20swarm%20scenarios%20involving%20shared%20environments%20without%20using%20swarm%0Acontroller%20information.%20We%20achieve%20this%20by%20transforming%20prior%20demonstrations%0Ainto%20features%20that%20describe%20multi-agent%20interactions%20before%20behavior%0Areconstruction%20with%20multi-agent%20generative%20adversarial%20imitation%20learning%0A%28MA-GAIL%29.%20We%20show%20that%20our%20approach%20outperforms%20existing%20algorithms%20in%20spatial%0Aorganization%2C%20and%20can%20be%20used%20to%20observe%20and%20reconstruct%20a%20swarm%27s%20behavior%20for%0Afurther%20analysis%20and%20testing%2C%20which%20might%20be%20impractical%20or%20undesirable%20on%20the%0Aoriginal%20robot%20swarm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Imitate%2520Spatial%2520Organization%2520in%2520Multi-robot%2520Systems%26entry.906535625%3DAyomide%2520O.%2520Agunloye%2520and%2520Sarvapali%2520D.%2520Ramchurn%2520and%2520Mohammad%2520D.%2520Soorati%26entry.1292438233%3D%2520%2520Understanding%2520collective%2520behavior%2520and%2520how%2520it%2520evolves%2520is%2520important%2520to%2520ensure%250Athat%2520robot%2520swarms%2520can%2520be%2520trusted%2520in%2520a%2520shared%2520environment.%2520One%2520way%2520to%2520understand%250Athe%2520behavior%2520of%2520the%2520swarm%2520is%2520through%2520collective%2520behavior%2520reconstruction%2520using%250Aprior%2520demonstrations.%2520Existing%2520approaches%2520often%2520require%2520access%2520to%2520the%2520swarm%250Acontroller%2520which%2520may%2520not%2520be%2520available.%2520We%2520reconstruct%2520collective%2520behaviors%2520in%250Adistinct%2520swarm%2520scenarios%2520involving%2520shared%2520environments%2520without%2520using%2520swarm%250Acontroller%2520information.%2520We%2520achieve%2520this%2520by%2520transforming%2520prior%2520demonstrations%250Ainto%2520features%2520that%2520describe%2520multi-agent%2520interactions%2520before%2520behavior%250Areconstruction%2520with%2520multi-agent%2520generative%2520adversarial%2520imitation%2520learning%250A%2528MA-GAIL%2529.%2520We%2520show%2520that%2520our%2520approach%2520outperforms%2520existing%2520algorithms%2520in%2520spatial%250Aorganization%252C%2520and%2520can%2520be%2520used%2520to%2520observe%2520and%2520reconstruct%2520a%2520swarm%2527s%2520behavior%2520for%250Afurther%2520analysis%2520and%2520testing%252C%2520which%2520might%2520be%2520impractical%2520or%2520undesirable%2520on%2520the%250Aoriginal%2520robot%2520swarm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Imitate%20Spatial%20Organization%20in%20Multi-robot%20Systems&entry.906535625=Ayomide%20O.%20Agunloye%20and%20Sarvapali%20D.%20Ramchurn%20and%20Mohammad%20D.%20Soorati&entry.1292438233=%20%20Understanding%20collective%20behavior%20and%20how%20it%20evolves%20is%20important%20to%20ensure%0Athat%20robot%20swarms%20can%20be%20trusted%20in%20a%20shared%20environment.%20One%20way%20to%20understand%0Athe%20behavior%20of%20the%20swarm%20is%20through%20collective%20behavior%20reconstruction%20using%0Aprior%20demonstrations.%20Existing%20approaches%20often%20require%20access%20to%20the%20swarm%0Acontroller%20which%20may%20not%20be%20available.%20We%20reconstruct%20collective%20behaviors%20in%0Adistinct%20swarm%20scenarios%20involving%20shared%20environments%20without%20using%20swarm%0Acontroller%20information.%20We%20achieve%20this%20by%20transforming%20prior%20demonstrations%0Ainto%20features%20that%20describe%20multi-agent%20interactions%20before%20behavior%0Areconstruction%20with%20multi-agent%20generative%20adversarial%20imitation%20learning%0A%28MA-GAIL%29.%20We%20show%20that%20our%20approach%20outperforms%20existing%20algorithms%20in%20spatial%0Aorganization%2C%20and%20can%20be%20used%20to%20observe%20and%20reconstruct%20a%20swarm%27s%20behavior%20for%0Afurther%20analysis%20and%20testing%2C%20which%20might%20be%20impractical%20or%20undesirable%20on%20the%0Aoriginal%20robot%20swarm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11592v2&entry.124074799=Read"},
{"title": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models", "author": "Moreno D'Inc\u00e0 and Elia Peruzzo and Massimiliano Mancini and Dejia Xu and Vidit Goel and Xingqian Xu and Zhangyang Wang and Humphrey Shi and Nicu Sebe", "abstract": "  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n", "link": "http://arxiv.org/abs/2404.07990v2", "date": "2024-08-05", "relevancy": 1.5875, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.544}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5256}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenBias%3A%20Open-set%20Bias%20Detection%20in%20Text-to-Image%20Generative%20Models&body=Title%3A%20OpenBias%3A%20Open-set%20Bias%20Detection%20in%20Text-to-Image%20Generative%20Models%0AAuthor%3A%20Moreno%20D%27Inc%C3%A0%20and%20Elia%20Peruzzo%20and%20Massimiliano%20Mancini%20and%20Dejia%20Xu%20and%20Vidit%20Goel%20and%20Xingqian%20Xu%20and%20Zhangyang%20Wang%20and%20Humphrey%20Shi%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Text-to-image%20generative%20models%20are%20becoming%20increasingly%20popular%20and%0Aaccessible%20to%20the%20general%20public.%20As%20these%20models%20see%20large-scale%20deployments%2C%0Ait%20is%20necessary%20to%20deeply%20investigate%20their%20safety%20and%20fairness%20to%20not%0Adisseminate%20and%20perpetuate%20any%20kind%20of%20biases.%20However%2C%20existing%20works%20focus%20on%0Adetecting%20closed%20sets%20of%20biases%20defined%20a%20priori%2C%20limiting%20the%20studies%20to%0Awell-known%20concepts.%20In%20this%20paper%2C%20we%20tackle%20the%20challenge%20of%20open-set%20bias%0Adetection%20in%20text-to-image%20generative%20models%20presenting%20OpenBias%2C%20a%20new%0Apipeline%20that%20identifies%20and%20quantifies%20the%20severity%20of%20biases%20agnostically%2C%0Awithout%20access%20to%20any%20precompiled%20set.%20OpenBias%20has%20three%20stages.%20In%20the%20first%0Aphase%2C%20we%20leverage%20a%20Large%20Language%20Model%20%28LLM%29%20to%20propose%20biases%20given%20a%20set%0Aof%20captions.%20Secondly%2C%20the%20target%20generative%20model%20produces%20images%20using%20the%0Asame%20set%20of%20captions.%20Lastly%2C%20a%20Vision%20Question%20Answering%20model%20recognizes%20the%0Apresence%20and%20extent%20of%20the%20previously%20proposed%20biases.%20We%20study%20the%20behavior%20of%0AStable%20Diffusion%201.5%2C%202%2C%20and%20XL%20emphasizing%20new%20biases%2C%20never%20investigated%0Abefore.%20Via%20quantitative%20experiments%2C%20we%20demonstrate%20that%20OpenBias%20agrees%20with%0Acurrent%20closed-set%20bias%20detection%20methods%20and%20human%20judgement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenBias%253A%2520Open-set%2520Bias%2520Detection%2520in%2520Text-to-Image%2520Generative%2520Models%26entry.906535625%3DMoreno%2520D%2527Inc%25C3%25A0%2520and%2520Elia%2520Peruzzo%2520and%2520Massimiliano%2520Mancini%2520and%2520Dejia%2520Xu%2520and%2520Vidit%2520Goel%2520and%2520Xingqian%2520Xu%2520and%2520Zhangyang%2520Wang%2520and%2520Humphrey%2520Shi%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Text-to-image%2520generative%2520models%2520are%2520becoming%2520increasingly%2520popular%2520and%250Aaccessible%2520to%2520the%2520general%2520public.%2520As%2520these%2520models%2520see%2520large-scale%2520deployments%252C%250Ait%2520is%2520necessary%2520to%2520deeply%2520investigate%2520their%2520safety%2520and%2520fairness%2520to%2520not%250Adisseminate%2520and%2520perpetuate%2520any%2520kind%2520of%2520biases.%2520However%252C%2520existing%2520works%2520focus%2520on%250Adetecting%2520closed%2520sets%2520of%2520biases%2520defined%2520a%2520priori%252C%2520limiting%2520the%2520studies%2520to%250Awell-known%2520concepts.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520challenge%2520of%2520open-set%2520bias%250Adetection%2520in%2520text-to-image%2520generative%2520models%2520presenting%2520OpenBias%252C%2520a%2520new%250Apipeline%2520that%2520identifies%2520and%2520quantifies%2520the%2520severity%2520of%2520biases%2520agnostically%252C%250Awithout%2520access%2520to%2520any%2520precompiled%2520set.%2520OpenBias%2520has%2520three%2520stages.%2520In%2520the%2520first%250Aphase%252C%2520we%2520leverage%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520propose%2520biases%2520given%2520a%2520set%250Aof%2520captions.%2520Secondly%252C%2520the%2520target%2520generative%2520model%2520produces%2520images%2520using%2520the%250Asame%2520set%2520of%2520captions.%2520Lastly%252C%2520a%2520Vision%2520Question%2520Answering%2520model%2520recognizes%2520the%250Apresence%2520and%2520extent%2520of%2520the%2520previously%2520proposed%2520biases.%2520We%2520study%2520the%2520behavior%2520of%250AStable%2520Diffusion%25201.5%252C%25202%252C%2520and%2520XL%2520emphasizing%2520new%2520biases%252C%2520never%2520investigated%250Abefore.%2520Via%2520quantitative%2520experiments%252C%2520we%2520demonstrate%2520that%2520OpenBias%2520agrees%2520with%250Acurrent%2520closed-set%2520bias%2520detection%2520methods%2520and%2520human%2520judgement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenBias%3A%20Open-set%20Bias%20Detection%20in%20Text-to-Image%20Generative%20Models&entry.906535625=Moreno%20D%27Inc%C3%A0%20and%20Elia%20Peruzzo%20and%20Massimiliano%20Mancini%20and%20Dejia%20Xu%20and%20Vidit%20Goel%20and%20Xingqian%20Xu%20and%20Zhangyang%20Wang%20and%20Humphrey%20Shi%20and%20Nicu%20Sebe&entry.1292438233=%20%20Text-to-image%20generative%20models%20are%20becoming%20increasingly%20popular%20and%0Aaccessible%20to%20the%20general%20public.%20As%20these%20models%20see%20large-scale%20deployments%2C%0Ait%20is%20necessary%20to%20deeply%20investigate%20their%20safety%20and%20fairness%20to%20not%0Adisseminate%20and%20perpetuate%20any%20kind%20of%20biases.%20However%2C%20existing%20works%20focus%20on%0Adetecting%20closed%20sets%20of%20biases%20defined%20a%20priori%2C%20limiting%20the%20studies%20to%0Awell-known%20concepts.%20In%20this%20paper%2C%20we%20tackle%20the%20challenge%20of%20open-set%20bias%0Adetection%20in%20text-to-image%20generative%20models%20presenting%20OpenBias%2C%20a%20new%0Apipeline%20that%20identifies%20and%20quantifies%20the%20severity%20of%20biases%20agnostically%2C%0Awithout%20access%20to%20any%20precompiled%20set.%20OpenBias%20has%20three%20stages.%20In%20the%20first%0Aphase%2C%20we%20leverage%20a%20Large%20Language%20Model%20%28LLM%29%20to%20propose%20biases%20given%20a%20set%0Aof%20captions.%20Secondly%2C%20the%20target%20generative%20model%20produces%20images%20using%20the%0Asame%20set%20of%20captions.%20Lastly%2C%20a%20Vision%20Question%20Answering%20model%20recognizes%20the%0Apresence%20and%20extent%20of%20the%20previously%20proposed%20biases.%20We%20study%20the%20behavior%20of%0AStable%20Diffusion%201.5%2C%202%2C%20and%20XL%20emphasizing%20new%20biases%2C%20never%20investigated%0Abefore.%20Via%20quantitative%20experiments%2C%20we%20demonstrate%20that%20OpenBias%20agrees%20with%0Acurrent%20closed-set%20bias%20detection%20methods%20and%20human%20judgement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07990v2&entry.124074799=Read"},
{"title": "High-arity PAC learning via exchangeability", "author": "Leonardo N. Coregliano and Maryanthe Malliaris", "abstract": "  We develop a theory of high-arity PAC learning, which is statistical learning\nin the presence of \"structured correlation\". In this theory, hypotheses are\neither graphs, hypergraphs or, more generally, structures in finite relational\nlanguages, and i.i.d. sampling is replaced by sampling an induced substructure,\nproducing an exchangeable distribution. Our main theorems establish a\nhigh-arity (agnostic) version of the fundamental theorem of statistical\nlearning.\n", "link": "http://arxiv.org/abs/2402.14294v2", "date": "2024-08-05", "relevancy": 1.5813, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3863}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-arity%20PAC%20learning%20via%20exchangeability&body=Title%3A%20High-arity%20PAC%20learning%20via%20exchangeability%0AAuthor%3A%20Leonardo%20N.%20Coregliano%20and%20Maryanthe%20Malliaris%0AAbstract%3A%20%20%20We%20develop%20a%20theory%20of%20high-arity%20PAC%20learning%2C%20which%20is%20statistical%20learning%0Ain%20the%20presence%20of%20%22structured%20correlation%22.%20In%20this%20theory%2C%20hypotheses%20are%0Aeither%20graphs%2C%20hypergraphs%20or%2C%20more%20generally%2C%20structures%20in%20finite%20relational%0Alanguages%2C%20and%20i.i.d.%20sampling%20is%20replaced%20by%20sampling%20an%20induced%20substructure%2C%0Aproducing%20an%20exchangeable%20distribution.%20Our%20main%20theorems%20establish%20a%0Ahigh-arity%20%28agnostic%29%20version%20of%20the%20fundamental%20theorem%20of%20statistical%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-arity%2520PAC%2520learning%2520via%2520exchangeability%26entry.906535625%3DLeonardo%2520N.%2520Coregliano%2520and%2520Maryanthe%2520Malliaris%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520theory%2520of%2520high-arity%2520PAC%2520learning%252C%2520which%2520is%2520statistical%2520learning%250Ain%2520the%2520presence%2520of%2520%2522structured%2520correlation%2522.%2520In%2520this%2520theory%252C%2520hypotheses%2520are%250Aeither%2520graphs%252C%2520hypergraphs%2520or%252C%2520more%2520generally%252C%2520structures%2520in%2520finite%2520relational%250Alanguages%252C%2520and%2520i.i.d.%2520sampling%2520is%2520replaced%2520by%2520sampling%2520an%2520induced%2520substructure%252C%250Aproducing%2520an%2520exchangeable%2520distribution.%2520Our%2520main%2520theorems%2520establish%2520a%250Ahigh-arity%2520%2528agnostic%2529%2520version%2520of%2520the%2520fundamental%2520theorem%2520of%2520statistical%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-arity%20PAC%20learning%20via%20exchangeability&entry.906535625=Leonardo%20N.%20Coregliano%20and%20Maryanthe%20Malliaris&entry.1292438233=%20%20We%20develop%20a%20theory%20of%20high-arity%20PAC%20learning%2C%20which%20is%20statistical%20learning%0Ain%20the%20presence%20of%20%22structured%20correlation%22.%20In%20this%20theory%2C%20hypotheses%20are%0Aeither%20graphs%2C%20hypergraphs%20or%2C%20more%20generally%2C%20structures%20in%20finite%20relational%0Alanguages%2C%20and%20i.i.d.%20sampling%20is%20replaced%20by%20sampling%20an%20induced%20substructure%2C%0Aproducing%20an%20exchangeable%20distribution.%20Our%20main%20theorems%20establish%20a%0Ahigh-arity%20%28agnostic%29%20version%20of%20the%20fundamental%20theorem%20of%20statistical%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14294v2&entry.124074799=Read"},
{"title": "De-fine: Decomposing and Refining Visual Programs with Auto-Feedback", "author": "Minghe Gao and Juncheng Li and Hao Fei and Liang Pang and Wei Ji and Guoming Wang and Zheqi Lv and Wenqiao Zhang and Siliang Tang and Yueting Zhuang", "abstract": "  Visual programming, a modular and generalizable paradigm, integrates\ndifferent modules and Python operators to solve various vision-language tasks.\nUnlike end-to-end models that need task-specific data, it advances in\nperforming visual processing and reasoning in an unsupervised manner. Current\nvisual programming methods generate programs in a single pass for each task\nwhere the ability to evaluate and optimize based on feedback, unfortunately, is\nlacking, which consequentially limits their effectiveness for complex,\nmulti-step problems. Drawing inspiration from benders decomposition, we\nintroduce De-fine, a training-free framework that automatically decomposes\ncomplex tasks into simpler subtasks and refines programs through auto-feedback.\nThis model-agnostic approach can improve logical reasoning performance by\nintegrating the strengths of multiple models. Our experiments across various\nvisual tasks show that De-fine creates more robust programs. Moreover, viewing\neach feedback module as an independent agent will yield fresh prospects for the\nfield of agent research.\n", "link": "http://arxiv.org/abs/2311.12890v3", "date": "2024-08-05", "relevancy": 1.5469, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5281}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5194}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20De-fine%3A%20Decomposing%20and%20Refining%20Visual%20Programs%20with%20Auto-Feedback&body=Title%3A%20De-fine%3A%20Decomposing%20and%20Refining%20Visual%20Programs%20with%20Auto-Feedback%0AAuthor%3A%20Minghe%20Gao%20and%20Juncheng%20Li%20and%20Hao%20Fei%20and%20Liang%20Pang%20and%20Wei%20Ji%20and%20Guoming%20Wang%20and%20Zheqi%20Lv%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Visual%20programming%2C%20a%20modular%20and%20generalizable%20paradigm%2C%20integrates%0Adifferent%20modules%20and%20Python%20operators%20to%20solve%20various%20vision-language%20tasks.%0AUnlike%20end-to-end%20models%20that%20need%20task-specific%20data%2C%20it%20advances%20in%0Aperforming%20visual%20processing%20and%20reasoning%20in%20an%20unsupervised%20manner.%20Current%0Avisual%20programming%20methods%20generate%20programs%20in%20a%20single%20pass%20for%20each%20task%0Awhere%20the%20ability%20to%20evaluate%20and%20optimize%20based%20on%20feedback%2C%20unfortunately%2C%20is%0Alacking%2C%20which%20consequentially%20limits%20their%20effectiveness%20for%20complex%2C%0Amulti-step%20problems.%20Drawing%20inspiration%20from%20benders%20decomposition%2C%20we%0Aintroduce%20De-fine%2C%20a%20training-free%20framework%20that%20automatically%20decomposes%0Acomplex%20tasks%20into%20simpler%20subtasks%20and%20refines%20programs%20through%20auto-feedback.%0AThis%20model-agnostic%20approach%20can%20improve%20logical%20reasoning%20performance%20by%0Aintegrating%20the%20strengths%20of%20multiple%20models.%20Our%20experiments%20across%20various%0Avisual%20tasks%20show%20that%20De-fine%20creates%20more%20robust%20programs.%20Moreover%2C%20viewing%0Aeach%20feedback%20module%20as%20an%20independent%20agent%20will%20yield%20fresh%20prospects%20for%20the%0Afield%20of%20agent%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12890v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDe-fine%253A%2520Decomposing%2520and%2520Refining%2520Visual%2520Programs%2520with%2520Auto-Feedback%26entry.906535625%3DMinghe%2520Gao%2520and%2520Juncheng%2520Li%2520and%2520Hao%2520Fei%2520and%2520Liang%2520Pang%2520and%2520Wei%2520Ji%2520and%2520Guoming%2520Wang%2520and%2520Zheqi%2520Lv%2520and%2520Wenqiao%2520Zhang%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Visual%2520programming%252C%2520a%2520modular%2520and%2520generalizable%2520paradigm%252C%2520integrates%250Adifferent%2520modules%2520and%2520Python%2520operators%2520to%2520solve%2520various%2520vision-language%2520tasks.%250AUnlike%2520end-to-end%2520models%2520that%2520need%2520task-specific%2520data%252C%2520it%2520advances%2520in%250Aperforming%2520visual%2520processing%2520and%2520reasoning%2520in%2520an%2520unsupervised%2520manner.%2520Current%250Avisual%2520programming%2520methods%2520generate%2520programs%2520in%2520a%2520single%2520pass%2520for%2520each%2520task%250Awhere%2520the%2520ability%2520to%2520evaluate%2520and%2520optimize%2520based%2520on%2520feedback%252C%2520unfortunately%252C%2520is%250Alacking%252C%2520which%2520consequentially%2520limits%2520their%2520effectiveness%2520for%2520complex%252C%250Amulti-step%2520problems.%2520Drawing%2520inspiration%2520from%2520benders%2520decomposition%252C%2520we%250Aintroduce%2520De-fine%252C%2520a%2520training-free%2520framework%2520that%2520automatically%2520decomposes%250Acomplex%2520tasks%2520into%2520simpler%2520subtasks%2520and%2520refines%2520programs%2520through%2520auto-feedback.%250AThis%2520model-agnostic%2520approach%2520can%2520improve%2520logical%2520reasoning%2520performance%2520by%250Aintegrating%2520the%2520strengths%2520of%2520multiple%2520models.%2520Our%2520experiments%2520across%2520various%250Avisual%2520tasks%2520show%2520that%2520De-fine%2520creates%2520more%2520robust%2520programs.%2520Moreover%252C%2520viewing%250Aeach%2520feedback%2520module%2520as%2520an%2520independent%2520agent%2520will%2520yield%2520fresh%2520prospects%2520for%2520the%250Afield%2520of%2520agent%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12890v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=De-fine%3A%20Decomposing%20and%20Refining%20Visual%20Programs%20with%20Auto-Feedback&entry.906535625=Minghe%20Gao%20and%20Juncheng%20Li%20and%20Hao%20Fei%20and%20Liang%20Pang%20and%20Wei%20Ji%20and%20Guoming%20Wang%20and%20Zheqi%20Lv%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Visual%20programming%2C%20a%20modular%20and%20generalizable%20paradigm%2C%20integrates%0Adifferent%20modules%20and%20Python%20operators%20to%20solve%20various%20vision-language%20tasks.%0AUnlike%20end-to-end%20models%20that%20need%20task-specific%20data%2C%20it%20advances%20in%0Aperforming%20visual%20processing%20and%20reasoning%20in%20an%20unsupervised%20manner.%20Current%0Avisual%20programming%20methods%20generate%20programs%20in%20a%20single%20pass%20for%20each%20task%0Awhere%20the%20ability%20to%20evaluate%20and%20optimize%20based%20on%20feedback%2C%20unfortunately%2C%20is%0Alacking%2C%20which%20consequentially%20limits%20their%20effectiveness%20for%20complex%2C%0Amulti-step%20problems.%20Drawing%20inspiration%20from%20benders%20decomposition%2C%20we%0Aintroduce%20De-fine%2C%20a%20training-free%20framework%20that%20automatically%20decomposes%0Acomplex%20tasks%20into%20simpler%20subtasks%20and%20refines%20programs%20through%20auto-feedback.%0AThis%20model-agnostic%20approach%20can%20improve%20logical%20reasoning%20performance%20by%0Aintegrating%20the%20strengths%20of%20multiple%20models.%20Our%20experiments%20across%20various%0Avisual%20tasks%20show%20that%20De-fine%20creates%20more%20robust%20programs.%20Moreover%2C%20viewing%0Aeach%20feedback%20module%20as%20an%20independent%20agent%20will%20yield%20fresh%20prospects%20for%20the%0Afield%20of%20agent%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12890v3&entry.124074799=Read"},
{"title": "Mastering Agile Jumping Skills from Simple Practices with Iterative\n  Learning Control", "author": "Chuong Nguyen and Lingfan Bao and Quan Nguyen", "abstract": "  Achieving precise target jumping with legged robots poses a significant\nchallenge due to the long flight phase and the uncertainties inherent in\ncontact dynamics and hardware. Forcefully attempting these agile motions on\nhardware could result in severe failures and potential damage. Motivated by\nthese challenging problems, we propose an Iterative Learning Control (ILC)\napproach that aims to learn and refine jumping skills from easy to difficult,\ninstead of directly learning these challenging tasks. We verify that learning\nfrom simplicity can enhance safety and target jumping accuracy over trials.\nCompared to other ILC approaches for legged locomotion, our method can tackle\nthe problem of a long flight phase where control input is not available. In\naddition, our approach allows the robot to apply what it learns from a simple\njumping task to accomplish more challenging tasks within a few trials directly\nin hardware, instead of learning from scratch. We validate the method via\nextensive experiments in the A1 model and hardware for various jumping tasks.\nStarting from a small jump (e.g., a forward leap of 40cm), our learning\napproach empowers the robot to accomplish a variety of challenging targets,\nincluding jumping onto a 20cm high box, jumping to a greater distance of up to\n60cm, as well as performing jumps while carrying an unknown payload of 2kg. Our\nframework can allow the robot to reach the desired position and orientation\ntargets with approximate errors of 1cm and 1 degree within a few trials.\n", "link": "http://arxiv.org/abs/2408.02619v1", "date": "2024-08-05", "relevancy": 1.5468, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mastering%20Agile%20Jumping%20Skills%20from%20Simple%20Practices%20with%20Iterative%0A%20%20Learning%20Control&body=Title%3A%20Mastering%20Agile%20Jumping%20Skills%20from%20Simple%20Practices%20with%20Iterative%0A%20%20Learning%20Control%0AAuthor%3A%20Chuong%20Nguyen%20and%20Lingfan%20Bao%20and%20Quan%20Nguyen%0AAbstract%3A%20%20%20Achieving%20precise%20target%20jumping%20with%20legged%20robots%20poses%20a%20significant%0Achallenge%20due%20to%20the%20long%20flight%20phase%20and%20the%20uncertainties%20inherent%20in%0Acontact%20dynamics%20and%20hardware.%20Forcefully%20attempting%20these%20agile%20motions%20on%0Ahardware%20could%20result%20in%20severe%20failures%20and%20potential%20damage.%20Motivated%20by%0Athese%20challenging%20problems%2C%20we%20propose%20an%20Iterative%20Learning%20Control%20%28ILC%29%0Aapproach%20that%20aims%20to%20learn%20and%20refine%20jumping%20skills%20from%20easy%20to%20difficult%2C%0Ainstead%20of%20directly%20learning%20these%20challenging%20tasks.%20We%20verify%20that%20learning%0Afrom%20simplicity%20can%20enhance%20safety%20and%20target%20jumping%20accuracy%20over%20trials.%0ACompared%20to%20other%20ILC%20approaches%20for%20legged%20locomotion%2C%20our%20method%20can%20tackle%0Athe%20problem%20of%20a%20long%20flight%20phase%20where%20control%20input%20is%20not%20available.%20In%0Aaddition%2C%20our%20approach%20allows%20the%20robot%20to%20apply%20what%20it%20learns%20from%20a%20simple%0Ajumping%20task%20to%20accomplish%20more%20challenging%20tasks%20within%20a%20few%20trials%20directly%0Ain%20hardware%2C%20instead%20of%20learning%20from%20scratch.%20We%20validate%20the%20method%20via%0Aextensive%20experiments%20in%20the%20A1%20model%20and%20hardware%20for%20various%20jumping%20tasks.%0AStarting%20from%20a%20small%20jump%20%28e.g.%2C%20a%20forward%20leap%20of%2040cm%29%2C%20our%20learning%0Aapproach%20empowers%20the%20robot%20to%20accomplish%20a%20variety%20of%20challenging%20targets%2C%0Aincluding%20jumping%20onto%20a%2020cm%20high%20box%2C%20jumping%20to%20a%20greater%20distance%20of%20up%20to%0A60cm%2C%20as%20well%20as%20performing%20jumps%20while%20carrying%20an%20unknown%20payload%20of%202kg.%20Our%0Aframework%20can%20allow%20the%20robot%20to%20reach%20the%20desired%20position%20and%20orientation%0Atargets%20with%20approximate%20errors%20of%201cm%20and%201%20degree%20within%20a%20few%20trials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMastering%2520Agile%2520Jumping%2520Skills%2520from%2520Simple%2520Practices%2520with%2520Iterative%250A%2520%2520Learning%2520Control%26entry.906535625%3DChuong%2520Nguyen%2520and%2520Lingfan%2520Bao%2520and%2520Quan%2520Nguyen%26entry.1292438233%3D%2520%2520Achieving%2520precise%2520target%2520jumping%2520with%2520legged%2520robots%2520poses%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520long%2520flight%2520phase%2520and%2520the%2520uncertainties%2520inherent%2520in%250Acontact%2520dynamics%2520and%2520hardware.%2520Forcefully%2520attempting%2520these%2520agile%2520motions%2520on%250Ahardware%2520could%2520result%2520in%2520severe%2520failures%2520and%2520potential%2520damage.%2520Motivated%2520by%250Athese%2520challenging%2520problems%252C%2520we%2520propose%2520an%2520Iterative%2520Learning%2520Control%2520%2528ILC%2529%250Aapproach%2520that%2520aims%2520to%2520learn%2520and%2520refine%2520jumping%2520skills%2520from%2520easy%2520to%2520difficult%252C%250Ainstead%2520of%2520directly%2520learning%2520these%2520challenging%2520tasks.%2520We%2520verify%2520that%2520learning%250Afrom%2520simplicity%2520can%2520enhance%2520safety%2520and%2520target%2520jumping%2520accuracy%2520over%2520trials.%250ACompared%2520to%2520other%2520ILC%2520approaches%2520for%2520legged%2520locomotion%252C%2520our%2520method%2520can%2520tackle%250Athe%2520problem%2520of%2520a%2520long%2520flight%2520phase%2520where%2520control%2520input%2520is%2520not%2520available.%2520In%250Aaddition%252C%2520our%2520approach%2520allows%2520the%2520robot%2520to%2520apply%2520what%2520it%2520learns%2520from%2520a%2520simple%250Ajumping%2520task%2520to%2520accomplish%2520more%2520challenging%2520tasks%2520within%2520a%2520few%2520trials%2520directly%250Ain%2520hardware%252C%2520instead%2520of%2520learning%2520from%2520scratch.%2520We%2520validate%2520the%2520method%2520via%250Aextensive%2520experiments%2520in%2520the%2520A1%2520model%2520and%2520hardware%2520for%2520various%2520jumping%2520tasks.%250AStarting%2520from%2520a%2520small%2520jump%2520%2528e.g.%252C%2520a%2520forward%2520leap%2520of%252040cm%2529%252C%2520our%2520learning%250Aapproach%2520empowers%2520the%2520robot%2520to%2520accomplish%2520a%2520variety%2520of%2520challenging%2520targets%252C%250Aincluding%2520jumping%2520onto%2520a%252020cm%2520high%2520box%252C%2520jumping%2520to%2520a%2520greater%2520distance%2520of%2520up%2520to%250A60cm%252C%2520as%2520well%2520as%2520performing%2520jumps%2520while%2520carrying%2520an%2520unknown%2520payload%2520of%25202kg.%2520Our%250Aframework%2520can%2520allow%2520the%2520robot%2520to%2520reach%2520the%2520desired%2520position%2520and%2520orientation%250Atargets%2520with%2520approximate%2520errors%2520of%25201cm%2520and%25201%2520degree%2520within%2520a%2520few%2520trials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mastering%20Agile%20Jumping%20Skills%20from%20Simple%20Practices%20with%20Iterative%0A%20%20Learning%20Control&entry.906535625=Chuong%20Nguyen%20and%20Lingfan%20Bao%20and%20Quan%20Nguyen&entry.1292438233=%20%20Achieving%20precise%20target%20jumping%20with%20legged%20robots%20poses%20a%20significant%0Achallenge%20due%20to%20the%20long%20flight%20phase%20and%20the%20uncertainties%20inherent%20in%0Acontact%20dynamics%20and%20hardware.%20Forcefully%20attempting%20these%20agile%20motions%20on%0Ahardware%20could%20result%20in%20severe%20failures%20and%20potential%20damage.%20Motivated%20by%0Athese%20challenging%20problems%2C%20we%20propose%20an%20Iterative%20Learning%20Control%20%28ILC%29%0Aapproach%20that%20aims%20to%20learn%20and%20refine%20jumping%20skills%20from%20easy%20to%20difficult%2C%0Ainstead%20of%20directly%20learning%20these%20challenging%20tasks.%20We%20verify%20that%20learning%0Afrom%20simplicity%20can%20enhance%20safety%20and%20target%20jumping%20accuracy%20over%20trials.%0ACompared%20to%20other%20ILC%20approaches%20for%20legged%20locomotion%2C%20our%20method%20can%20tackle%0Athe%20problem%20of%20a%20long%20flight%20phase%20where%20control%20input%20is%20not%20available.%20In%0Aaddition%2C%20our%20approach%20allows%20the%20robot%20to%20apply%20what%20it%20learns%20from%20a%20simple%0Ajumping%20task%20to%20accomplish%20more%20challenging%20tasks%20within%20a%20few%20trials%20directly%0Ain%20hardware%2C%20instead%20of%20learning%20from%20scratch.%20We%20validate%20the%20method%20via%0Aextensive%20experiments%20in%20the%20A1%20model%20and%20hardware%20for%20various%20jumping%20tasks.%0AStarting%20from%20a%20small%20jump%20%28e.g.%2C%20a%20forward%20leap%20of%2040cm%29%2C%20our%20learning%0Aapproach%20empowers%20the%20robot%20to%20accomplish%20a%20variety%20of%20challenging%20targets%2C%0Aincluding%20jumping%20onto%20a%2020cm%20high%20box%2C%20jumping%20to%20a%20greater%20distance%20of%20up%20to%0A60cm%2C%20as%20well%20as%20performing%20jumps%20while%20carrying%20an%20unknown%20payload%20of%202kg.%20Our%0Aframework%20can%20allow%20the%20robot%20to%20reach%20the%20desired%20position%20and%20orientation%0Atargets%20with%20approximate%20errors%20of%201cm%20and%201%20degree%20within%20a%20few%20trials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02619v1&entry.124074799=Read"},
{"title": "Modelling Visual Semantics via Image Captioning to extract Enhanced\n  Multi-Level Cross-Modal Semantic Incongruity Representation with Attention\n  for Multimodal Sarcasm Detection", "author": "Sajal Aggarwal and Ananya Pandey and Dinesh Kumar Vishwakarma", "abstract": "  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n", "link": "http://arxiv.org/abs/2408.02595v1", "date": "2024-08-05", "relevancy": 1.5455, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5177}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modelling%20Visual%20Semantics%20via%20Image%20Captioning%20to%20extract%20Enhanced%0A%20%20Multi-Level%20Cross-Modal%20Semantic%20Incongruity%20Representation%20with%20Attention%0A%20%20for%20Multimodal%20Sarcasm%20Detection&body=Title%3A%20Modelling%20Visual%20Semantics%20via%20Image%20Captioning%20to%20extract%20Enhanced%0A%20%20Multi-Level%20Cross-Modal%20Semantic%20Incongruity%20Representation%20with%20Attention%0A%20%20for%20Multimodal%20Sarcasm%20Detection%0AAuthor%3A%20Sajal%20Aggarwal%20and%20Ananya%20Pandey%20and%20Dinesh%20Kumar%20Vishwakarma%0AAbstract%3A%20%20%20Sarcasm%20is%20a%20type%20of%20irony%2C%20characterized%20by%20an%20inherent%20mismatch%20between%20the%0Aliteral%20interpretation%20and%20the%20intended%20connotation.%20Though%20sarcasm%20detection%0Ain%20text%20has%20been%20extensively%20studied%2C%20there%20are%20situations%20in%20which%20textual%0Ainput%20alone%20might%20be%20insufficient%20to%20perceive%20sarcasm.%20The%20inclusion%20of%0Aadditional%20contextual%20cues%2C%20such%20as%20images%2C%20is%20essential%20to%20recognize%20sarcasm%0Ain%20social%20media%20data%20effectively.%20This%20study%20presents%20a%20novel%20framework%20for%0Amultimodal%20sarcasm%20detection%20that%20can%20process%20input%20triplets.%20Two%20components%20of%0Athese%20triplets%20comprise%20the%20input%20text%20and%20its%20associated%20image%2C%20as%20provided%20in%0Athe%20datasets.%20Additionally%2C%20a%20supplementary%20modality%20is%20introduced%20in%20the%20form%0Aof%20descriptive%20image%20captions.%20The%20motivation%20behind%20incorporating%20this%20visual%0Asemantic%20representation%20is%20to%20more%20accurately%20capture%20the%20discrepancies%20between%0Athe%20textual%20and%20visual%20content%2C%20which%20are%20fundamental%20to%20the%20sarcasm%20detection%0Atask.%20The%20primary%20contributions%20of%20this%20study%20are%3A%20%281%29%20a%20robust%20textual%20feature%0Aextraction%20branch%20that%20utilizes%20a%20cross-lingual%20language%20model%3B%20%282%29%20a%20visual%0Afeature%20extraction%20branch%20that%20incorporates%20a%20self-regulated%20residual%20ConvNet%0Aintegrated%20with%20a%20lightweight%20spatially%20aware%20attention%20module%3B%20%283%29%20an%0Aadditional%20modality%20in%20the%20form%20of%20image%20captions%20generated%20using%20an%0Aencoder-decoder%20architecture%20capable%20of%20reading%20text%20embedded%20in%20images%3B%20%284%29%0Adistinct%20attention%20modules%20to%20effectively%20identify%20the%20incongruities%20between%0Athe%20text%20and%20two%20levels%20of%20image%20representations%3B%20%285%29%20multi-level%20cross-domain%0Asemantic%20incongruity%20representation%20achieved%20through%20feature%20fusion.%20Compared%0Awith%20cutting-edge%20baselines%2C%20the%20proposed%20model%20achieves%20the%20best%20accuracy%20of%0A92.89%25%20and%2064.48%25%2C%20respectively%2C%20on%20the%20Twitter%20multimodal%20sarcasm%20and%0AMultiBully%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModelling%2520Visual%2520Semantics%2520via%2520Image%2520Captioning%2520to%2520extract%2520Enhanced%250A%2520%2520Multi-Level%2520Cross-Modal%2520Semantic%2520Incongruity%2520Representation%2520with%2520Attention%250A%2520%2520for%2520Multimodal%2520Sarcasm%2520Detection%26entry.906535625%3DSajal%2520Aggarwal%2520and%2520Ananya%2520Pandey%2520and%2520Dinesh%2520Kumar%2520Vishwakarma%26entry.1292438233%3D%2520%2520Sarcasm%2520is%2520a%2520type%2520of%2520irony%252C%2520characterized%2520by%2520an%2520inherent%2520mismatch%2520between%2520the%250Aliteral%2520interpretation%2520and%2520the%2520intended%2520connotation.%2520Though%2520sarcasm%2520detection%250Ain%2520text%2520has%2520been%2520extensively%2520studied%252C%2520there%2520are%2520situations%2520in%2520which%2520textual%250Ainput%2520alone%2520might%2520be%2520insufficient%2520to%2520perceive%2520sarcasm.%2520The%2520inclusion%2520of%250Aadditional%2520contextual%2520cues%252C%2520such%2520as%2520images%252C%2520is%2520essential%2520to%2520recognize%2520sarcasm%250Ain%2520social%2520media%2520data%2520effectively.%2520This%2520study%2520presents%2520a%2520novel%2520framework%2520for%250Amultimodal%2520sarcasm%2520detection%2520that%2520can%2520process%2520input%2520triplets.%2520Two%2520components%2520of%250Athese%2520triplets%2520comprise%2520the%2520input%2520text%2520and%2520its%2520associated%2520image%252C%2520as%2520provided%2520in%250Athe%2520datasets.%2520Additionally%252C%2520a%2520supplementary%2520modality%2520is%2520introduced%2520in%2520the%2520form%250Aof%2520descriptive%2520image%2520captions.%2520The%2520motivation%2520behind%2520incorporating%2520this%2520visual%250Asemantic%2520representation%2520is%2520to%2520more%2520accurately%2520capture%2520the%2520discrepancies%2520between%250Athe%2520textual%2520and%2520visual%2520content%252C%2520which%2520are%2520fundamental%2520to%2520the%2520sarcasm%2520detection%250Atask.%2520The%2520primary%2520contributions%2520of%2520this%2520study%2520are%253A%2520%25281%2529%2520a%2520robust%2520textual%2520feature%250Aextraction%2520branch%2520that%2520utilizes%2520a%2520cross-lingual%2520language%2520model%253B%2520%25282%2529%2520a%2520visual%250Afeature%2520extraction%2520branch%2520that%2520incorporates%2520a%2520self-regulated%2520residual%2520ConvNet%250Aintegrated%2520with%2520a%2520lightweight%2520spatially%2520aware%2520attention%2520module%253B%2520%25283%2529%2520an%250Aadditional%2520modality%2520in%2520the%2520form%2520of%2520image%2520captions%2520generated%2520using%2520an%250Aencoder-decoder%2520architecture%2520capable%2520of%2520reading%2520text%2520embedded%2520in%2520images%253B%2520%25284%2529%250Adistinct%2520attention%2520modules%2520to%2520effectively%2520identify%2520the%2520incongruities%2520between%250Athe%2520text%2520and%2520two%2520levels%2520of%2520image%2520representations%253B%2520%25285%2529%2520multi-level%2520cross-domain%250Asemantic%2520incongruity%2520representation%2520achieved%2520through%2520feature%2520fusion.%2520Compared%250Awith%2520cutting-edge%2520baselines%252C%2520the%2520proposed%2520model%2520achieves%2520the%2520best%2520accuracy%2520of%250A92.89%2525%2520and%252064.48%2525%252C%2520respectively%252C%2520on%2520the%2520Twitter%2520multimodal%2520sarcasm%2520and%250AMultiBully%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modelling%20Visual%20Semantics%20via%20Image%20Captioning%20to%20extract%20Enhanced%0A%20%20Multi-Level%20Cross-Modal%20Semantic%20Incongruity%20Representation%20with%20Attention%0A%20%20for%20Multimodal%20Sarcasm%20Detection&entry.906535625=Sajal%20Aggarwal%20and%20Ananya%20Pandey%20and%20Dinesh%20Kumar%20Vishwakarma&entry.1292438233=%20%20Sarcasm%20is%20a%20type%20of%20irony%2C%20characterized%20by%20an%20inherent%20mismatch%20between%20the%0Aliteral%20interpretation%20and%20the%20intended%20connotation.%20Though%20sarcasm%20detection%0Ain%20text%20has%20been%20extensively%20studied%2C%20there%20are%20situations%20in%20which%20textual%0Ainput%20alone%20might%20be%20insufficient%20to%20perceive%20sarcasm.%20The%20inclusion%20of%0Aadditional%20contextual%20cues%2C%20such%20as%20images%2C%20is%20essential%20to%20recognize%20sarcasm%0Ain%20social%20media%20data%20effectively.%20This%20study%20presents%20a%20novel%20framework%20for%0Amultimodal%20sarcasm%20detection%20that%20can%20process%20input%20triplets.%20Two%20components%20of%0Athese%20triplets%20comprise%20the%20input%20text%20and%20its%20associated%20image%2C%20as%20provided%20in%0Athe%20datasets.%20Additionally%2C%20a%20supplementary%20modality%20is%20introduced%20in%20the%20form%0Aof%20descriptive%20image%20captions.%20The%20motivation%20behind%20incorporating%20this%20visual%0Asemantic%20representation%20is%20to%20more%20accurately%20capture%20the%20discrepancies%20between%0Athe%20textual%20and%20visual%20content%2C%20which%20are%20fundamental%20to%20the%20sarcasm%20detection%0Atask.%20The%20primary%20contributions%20of%20this%20study%20are%3A%20%281%29%20a%20robust%20textual%20feature%0Aextraction%20branch%20that%20utilizes%20a%20cross-lingual%20language%20model%3B%20%282%29%20a%20visual%0Afeature%20extraction%20branch%20that%20incorporates%20a%20self-regulated%20residual%20ConvNet%0Aintegrated%20with%20a%20lightweight%20spatially%20aware%20attention%20module%3B%20%283%29%20an%0Aadditional%20modality%20in%20the%20form%20of%20image%20captions%20generated%20using%20an%0Aencoder-decoder%20architecture%20capable%20of%20reading%20text%20embedded%20in%20images%3B%20%284%29%0Adistinct%20attention%20modules%20to%20effectively%20identify%20the%20incongruities%20between%0Athe%20text%20and%20two%20levels%20of%20image%20representations%3B%20%285%29%20multi-level%20cross-domain%0Asemantic%20incongruity%20representation%20achieved%20through%20feature%20fusion.%20Compared%0Awith%20cutting-edge%20baselines%2C%20the%20proposed%20model%20achieves%20the%20best%20accuracy%20of%0A92.89%25%20and%2064.48%25%2C%20respectively%2C%20on%20the%20Twitter%20multimodal%20sarcasm%20and%0AMultiBully%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02595v1&entry.124074799=Read"},
{"title": "Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep\n  Reinforcement Learning (CDRL) Approach", "author": "Zonghui Yang and Shijian Gao and Xiang Cheng", "abstract": "  Integrated sensing and communication (ISAC) technology is essential for\nenabling the vehicular networks. However, the communication channel in this\nscenario exhibits time-varying characteristics, and the potential targets may\nmove rapidly, creating a doubly-dynamic phenomenon. This nature poses a\nchallenge for real-time precoder design. While optimization-based solutions are\nwidely researched, they are complex and heavily rely on perfect prior\ninformation, which is impractical in double dynamics. To address this\nchallenge, we propose using constrained deep reinforcement learning (CDRL) to\nfacilitate dynamic updates to the ISAC precoder design. Additionally, the\nprimal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertinger\narchitecture are tailored to efficiently train the algorithm under complex\nconstraints and variable numbers of users. The proposed scheme not only adapts\nto the dynamics based on observations but also leverages environmental\ninformation to enhance performance and reduce complexity. Its superiority over\nexisting candidates has been validated through experiments.\n", "link": "http://arxiv.org/abs/2405.14347v2", "date": "2024-08-05", "relevancy": 1.5323, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doubly-Dynamic%20ISAC%20Precoding%20for%20Vehicular%20Networks%3A%20A%20Constrained%20Deep%0A%20%20Reinforcement%20Learning%20%28CDRL%29%20Approach&body=Title%3A%20Doubly-Dynamic%20ISAC%20Precoding%20for%20Vehicular%20Networks%3A%20A%20Constrained%20Deep%0A%20%20Reinforcement%20Learning%20%28CDRL%29%20Approach%0AAuthor%3A%20Zonghui%20Yang%20and%20Shijian%20Gao%20and%20Xiang%20Cheng%0AAbstract%3A%20%20%20Integrated%20sensing%20and%20communication%20%28ISAC%29%20technology%20is%20essential%20for%0Aenabling%20the%20vehicular%20networks.%20However%2C%20the%20communication%20channel%20in%20this%0Ascenario%20exhibits%20time-varying%20characteristics%2C%20and%20the%20potential%20targets%20may%0Amove%20rapidly%2C%20creating%20a%20doubly-dynamic%20phenomenon.%20This%20nature%20poses%20a%0Achallenge%20for%20real-time%20precoder%20design.%20While%20optimization-based%20solutions%20are%0Awidely%20researched%2C%20they%20are%20complex%20and%20heavily%20rely%20on%20perfect%20prior%0Ainformation%2C%20which%20is%20impractical%20in%20double%20dynamics.%20To%20address%20this%0Achallenge%2C%20we%20propose%20using%20constrained%20deep%20reinforcement%20learning%20%28CDRL%29%20to%0Afacilitate%20dynamic%20updates%20to%20the%20ISAC%20precoder%20design.%20Additionally%2C%20the%0Aprimal%20dual-deep%20deterministic%20policy%20gradient%20%28PD-DDPG%29%20and%20Wolpertinger%0Aarchitecture%20are%20tailored%20to%20efficiently%20train%20the%20algorithm%20under%20complex%0Aconstraints%20and%20variable%20numbers%20of%20users.%20The%20proposed%20scheme%20not%20only%20adapts%0Ato%20the%20dynamics%20based%20on%20observations%20but%20also%20leverages%20environmental%0Ainformation%20to%20enhance%20performance%20and%20reduce%20complexity.%20Its%20superiority%20over%0Aexisting%20candidates%20has%20been%20validated%20through%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubly-Dynamic%2520ISAC%2520Precoding%2520for%2520Vehicular%2520Networks%253A%2520A%2520Constrained%2520Deep%250A%2520%2520Reinforcement%2520Learning%2520%2528CDRL%2529%2520Approach%26entry.906535625%3DZonghui%2520Yang%2520and%2520Shijian%2520Gao%2520and%2520Xiang%2520Cheng%26entry.1292438233%3D%2520%2520Integrated%2520sensing%2520and%2520communication%2520%2528ISAC%2529%2520technology%2520is%2520essential%2520for%250Aenabling%2520the%2520vehicular%2520networks.%2520However%252C%2520the%2520communication%2520channel%2520in%2520this%250Ascenario%2520exhibits%2520time-varying%2520characteristics%252C%2520and%2520the%2520potential%2520targets%2520may%250Amove%2520rapidly%252C%2520creating%2520a%2520doubly-dynamic%2520phenomenon.%2520This%2520nature%2520poses%2520a%250Achallenge%2520for%2520real-time%2520precoder%2520design.%2520While%2520optimization-based%2520solutions%2520are%250Awidely%2520researched%252C%2520they%2520are%2520complex%2520and%2520heavily%2520rely%2520on%2520perfect%2520prior%250Ainformation%252C%2520which%2520is%2520impractical%2520in%2520double%2520dynamics.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520using%2520constrained%2520deep%2520reinforcement%2520learning%2520%2528CDRL%2529%2520to%250Afacilitate%2520dynamic%2520updates%2520to%2520the%2520ISAC%2520precoder%2520design.%2520Additionally%252C%2520the%250Aprimal%2520dual-deep%2520deterministic%2520policy%2520gradient%2520%2528PD-DDPG%2529%2520and%2520Wolpertinger%250Aarchitecture%2520are%2520tailored%2520to%2520efficiently%2520train%2520the%2520algorithm%2520under%2520complex%250Aconstraints%2520and%2520variable%2520numbers%2520of%2520users.%2520The%2520proposed%2520scheme%2520not%2520only%2520adapts%250Ato%2520the%2520dynamics%2520based%2520on%2520observations%2520but%2520also%2520leverages%2520environmental%250Ainformation%2520to%2520enhance%2520performance%2520and%2520reduce%2520complexity.%2520Its%2520superiority%2520over%250Aexisting%2520candidates%2520has%2520been%2520validated%2520through%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doubly-Dynamic%20ISAC%20Precoding%20for%20Vehicular%20Networks%3A%20A%20Constrained%20Deep%0A%20%20Reinforcement%20Learning%20%28CDRL%29%20Approach&entry.906535625=Zonghui%20Yang%20and%20Shijian%20Gao%20and%20Xiang%20Cheng&entry.1292438233=%20%20Integrated%20sensing%20and%20communication%20%28ISAC%29%20technology%20is%20essential%20for%0Aenabling%20the%20vehicular%20networks.%20However%2C%20the%20communication%20channel%20in%20this%0Ascenario%20exhibits%20time-varying%20characteristics%2C%20and%20the%20potential%20targets%20may%0Amove%20rapidly%2C%20creating%20a%20doubly-dynamic%20phenomenon.%20This%20nature%20poses%20a%0Achallenge%20for%20real-time%20precoder%20design.%20While%20optimization-based%20solutions%20are%0Awidely%20researched%2C%20they%20are%20complex%20and%20heavily%20rely%20on%20perfect%20prior%0Ainformation%2C%20which%20is%20impractical%20in%20double%20dynamics.%20To%20address%20this%0Achallenge%2C%20we%20propose%20using%20constrained%20deep%20reinforcement%20learning%20%28CDRL%29%20to%0Afacilitate%20dynamic%20updates%20to%20the%20ISAC%20precoder%20design.%20Additionally%2C%20the%0Aprimal%20dual-deep%20deterministic%20policy%20gradient%20%28PD-DDPG%29%20and%20Wolpertinger%0Aarchitecture%20are%20tailored%20to%20efficiently%20train%20the%20algorithm%20under%20complex%0Aconstraints%20and%20variable%20numbers%20of%20users.%20The%20proposed%20scheme%20not%20only%20adapts%0Ato%20the%20dynamics%20based%20on%20observations%20but%20also%20leverages%20environmental%0Ainformation%20to%20enhance%20performance%20and%20reduce%20complexity.%20Its%20superiority%20over%0Aexisting%20candidates%20has%20been%20validated%20through%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14347v2&entry.124074799=Read"},
{"title": "Learning rheological parameters of non-Newtonian fluids from velocimetry\n  data", "author": "Alexandros Kontogiannis and Richard Hodgkinson and Emily L. Manchester", "abstract": "  We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates\nvelocimetry data in order to jointly reconstruct the flow field and learn the\nunknown N-S parameters. By incorporating a Carreau shear-thinning viscosity\nmodel into the N-S problem, we devise an algorithm that learns the most likely\nCarreau parameters of a shear-thinning fluid, and estimates their\nuncertainties, from velocimetry data alone. We then conduct a flow-MRI\nexperiment to obtain velocimetry data of an axisymmetric laminar jet through an\nidealised medical device (FDA nozzle) for a blood analogue fluid. We show that\nthe algorithm can successfully reconstruct the flow field by learning the most\nlikely Carreau parameters, and that the learned parameters are in very good\nagreement with rheometry measurements. The algorithm accepts any algebraic\neffective viscosity model, as long as the model is differentiable, and it can\nbe extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) if\na viscoelastic model is incorporated into the N-S problem.\n", "link": "http://arxiv.org/abs/2408.02604v1", "date": "2024-08-05", "relevancy": 1.3927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5008}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4771}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20rheological%20parameters%20of%20non-Newtonian%20fluids%20from%20velocimetry%0A%20%20data&body=Title%3A%20Learning%20rheological%20parameters%20of%20non-Newtonian%20fluids%20from%20velocimetry%0A%20%20data%0AAuthor%3A%20Alexandros%20Kontogiannis%20and%20Richard%20Hodgkinson%20and%20Emily%20L.%20Manchester%0AAbstract%3A%20%20%20We%20solve%20a%20Bayesian%20inverse%20Navier-Stokes%20%28N-S%29%20problem%20that%20assimilates%0Avelocimetry%20data%20in%20order%20to%20jointly%20reconstruct%20the%20flow%20field%20and%20learn%20the%0Aunknown%20N-S%20parameters.%20By%20incorporating%20a%20Carreau%20shear-thinning%20viscosity%0Amodel%20into%20the%20N-S%20problem%2C%20we%20devise%20an%20algorithm%20that%20learns%20the%20most%20likely%0ACarreau%20parameters%20of%20a%20shear-thinning%20fluid%2C%20and%20estimates%20their%0Auncertainties%2C%20from%20velocimetry%20data%20alone.%20We%20then%20conduct%20a%20flow-MRI%0Aexperiment%20to%20obtain%20velocimetry%20data%20of%20an%20axisymmetric%20laminar%20jet%20through%20an%0Aidealised%20medical%20device%20%28FDA%20nozzle%29%20for%20a%20blood%20analogue%20fluid.%20We%20show%20that%0Athe%20algorithm%20can%20successfully%20reconstruct%20the%20flow%20field%20by%20learning%20the%20most%0Alikely%20Carreau%20parameters%2C%20and%20that%20the%20learned%20parameters%20are%20in%20very%20good%0Aagreement%20with%20rheometry%20measurements.%20The%20algorithm%20accepts%20any%20algebraic%0Aeffective%20viscosity%20model%2C%20as%20long%20as%20the%20model%20is%20differentiable%2C%20and%20it%20can%0Abe%20extended%20to%20more%20complicated%20non-Newtonian%20fluids%20%28e.g.%20Oldroyd-B%20fluid%29%20if%0Aa%20viscoelastic%20model%20is%20incorporated%20into%20the%20N-S%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520rheological%2520parameters%2520of%2520non-Newtonian%2520fluids%2520from%2520velocimetry%250A%2520%2520data%26entry.906535625%3DAlexandros%2520Kontogiannis%2520and%2520Richard%2520Hodgkinson%2520and%2520Emily%2520L.%2520Manchester%26entry.1292438233%3D%2520%2520We%2520solve%2520a%2520Bayesian%2520inverse%2520Navier-Stokes%2520%2528N-S%2529%2520problem%2520that%2520assimilates%250Avelocimetry%2520data%2520in%2520order%2520to%2520jointly%2520reconstruct%2520the%2520flow%2520field%2520and%2520learn%2520the%250Aunknown%2520N-S%2520parameters.%2520By%2520incorporating%2520a%2520Carreau%2520shear-thinning%2520viscosity%250Amodel%2520into%2520the%2520N-S%2520problem%252C%2520we%2520devise%2520an%2520algorithm%2520that%2520learns%2520the%2520most%2520likely%250ACarreau%2520parameters%2520of%2520a%2520shear-thinning%2520fluid%252C%2520and%2520estimates%2520their%250Auncertainties%252C%2520from%2520velocimetry%2520data%2520alone.%2520We%2520then%2520conduct%2520a%2520flow-MRI%250Aexperiment%2520to%2520obtain%2520velocimetry%2520data%2520of%2520an%2520axisymmetric%2520laminar%2520jet%2520through%2520an%250Aidealised%2520medical%2520device%2520%2528FDA%2520nozzle%2529%2520for%2520a%2520blood%2520analogue%2520fluid.%2520We%2520show%2520that%250Athe%2520algorithm%2520can%2520successfully%2520reconstruct%2520the%2520flow%2520field%2520by%2520learning%2520the%2520most%250Alikely%2520Carreau%2520parameters%252C%2520and%2520that%2520the%2520learned%2520parameters%2520are%2520in%2520very%2520good%250Aagreement%2520with%2520rheometry%2520measurements.%2520The%2520algorithm%2520accepts%2520any%2520algebraic%250Aeffective%2520viscosity%2520model%252C%2520as%2520long%2520as%2520the%2520model%2520is%2520differentiable%252C%2520and%2520it%2520can%250Abe%2520extended%2520to%2520more%2520complicated%2520non-Newtonian%2520fluids%2520%2528e.g.%2520Oldroyd-B%2520fluid%2529%2520if%250Aa%2520viscoelastic%2520model%2520is%2520incorporated%2520into%2520the%2520N-S%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20rheological%20parameters%20of%20non-Newtonian%20fluids%20from%20velocimetry%0A%20%20data&entry.906535625=Alexandros%20Kontogiannis%20and%20Richard%20Hodgkinson%20and%20Emily%20L.%20Manchester&entry.1292438233=%20%20We%20solve%20a%20Bayesian%20inverse%20Navier-Stokes%20%28N-S%29%20problem%20that%20assimilates%0Avelocimetry%20data%20in%20order%20to%20jointly%20reconstruct%20the%20flow%20field%20and%20learn%20the%0Aunknown%20N-S%20parameters.%20By%20incorporating%20a%20Carreau%20shear-thinning%20viscosity%0Amodel%20into%20the%20N-S%20problem%2C%20we%20devise%20an%20algorithm%20that%20learns%20the%20most%20likely%0ACarreau%20parameters%20of%20a%20shear-thinning%20fluid%2C%20and%20estimates%20their%0Auncertainties%2C%20from%20velocimetry%20data%20alone.%20We%20then%20conduct%20a%20flow-MRI%0Aexperiment%20to%20obtain%20velocimetry%20data%20of%20an%20axisymmetric%20laminar%20jet%20through%20an%0Aidealised%20medical%20device%20%28FDA%20nozzle%29%20for%20a%20blood%20analogue%20fluid.%20We%20show%20that%0Athe%20algorithm%20can%20successfully%20reconstruct%20the%20flow%20field%20by%20learning%20the%20most%0Alikely%20Carreau%20parameters%2C%20and%20that%20the%20learned%20parameters%20are%20in%20very%20good%0Aagreement%20with%20rheometry%20measurements.%20The%20algorithm%20accepts%20any%20algebraic%0Aeffective%20viscosity%20model%2C%20as%20long%20as%20the%20model%20is%20differentiable%2C%20and%20it%20can%0Abe%20extended%20to%20more%20complicated%20non-Newtonian%20fluids%20%28e.g.%20Oldroyd-B%20fluid%29%20if%0Aa%20viscoelastic%20model%20is%20incorporated%20into%20the%20N-S%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02604v1&entry.124074799=Read"},
{"title": "Command-line Obfuscation Detection using Small Language Models", "author": "Vojtech Outrata and Michael Adam Polak and Martin Kopp", "abstract": "  To avoid detection, adversaries often use command-line obfuscation. There are\nnumerous techniques of the command-line obfuscation, all designed to alter the\ncommand-line syntax without affecting its original functionality. This\nvariability forces most security solutions to create an exhaustive enumeration\nof signatures for even a single pattern. In contrast to using signatures, we\nhave implemented a scalable NLP-based detection method that leverages a\ncustom-trained, small transformer language model that can be applied to any\nsource of execution logs. The evaluation on top of real-world telemetry\ndemonstrates that our approach yields high-precision detections even on\nhigh-volume telemetry from a diverse set of environments spanning from\nuniversities and businesses to healthcare or finance. The practical value is\ndemonstrated in a case study of real-world samples detected by our model. We\nshow the model's superiority to signatures on established malware known to\nemploy obfuscation and showcase previously unseen obfuscated samples detected\nby our model.\n", "link": "http://arxiv.org/abs/2408.02637v1", "date": "2024-08-05", "relevancy": 1.3053, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4405}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4288}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Command-line%20Obfuscation%20Detection%20using%20Small%20Language%20Models&body=Title%3A%20Command-line%20Obfuscation%20Detection%20using%20Small%20Language%20Models%0AAuthor%3A%20Vojtech%20Outrata%20and%20Michael%20Adam%20Polak%20and%20Martin%20Kopp%0AAbstract%3A%20%20%20To%20avoid%20detection%2C%20adversaries%20often%20use%20command-line%20obfuscation.%20There%20are%0Anumerous%20techniques%20of%20the%20command-line%20obfuscation%2C%20all%20designed%20to%20alter%20the%0Acommand-line%20syntax%20without%20affecting%20its%20original%20functionality.%20This%0Avariability%20forces%20most%20security%20solutions%20to%20create%20an%20exhaustive%20enumeration%0Aof%20signatures%20for%20even%20a%20single%20pattern.%20In%20contrast%20to%20using%20signatures%2C%20we%0Ahave%20implemented%20a%20scalable%20NLP-based%20detection%20method%20that%20leverages%20a%0Acustom-trained%2C%20small%20transformer%20language%20model%20that%20can%20be%20applied%20to%20any%0Asource%20of%20execution%20logs.%20The%20evaluation%20on%20top%20of%20real-world%20telemetry%0Ademonstrates%20that%20our%20approach%20yields%20high-precision%20detections%20even%20on%0Ahigh-volume%20telemetry%20from%20a%20diverse%20set%20of%20environments%20spanning%20from%0Auniversities%20and%20businesses%20to%20healthcare%20or%20finance.%20The%20practical%20value%20is%0Ademonstrated%20in%20a%20case%20study%20of%20real-world%20samples%20detected%20by%20our%20model.%20We%0Ashow%20the%20model%27s%20superiority%20to%20signatures%20on%20established%20malware%20known%20to%0Aemploy%20obfuscation%20and%20showcase%20previously%20unseen%20obfuscated%20samples%20detected%0Aby%20our%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommand-line%2520Obfuscation%2520Detection%2520using%2520Small%2520Language%2520Models%26entry.906535625%3DVojtech%2520Outrata%2520and%2520Michael%2520Adam%2520Polak%2520and%2520Martin%2520Kopp%26entry.1292438233%3D%2520%2520To%2520avoid%2520detection%252C%2520adversaries%2520often%2520use%2520command-line%2520obfuscation.%2520There%2520are%250Anumerous%2520techniques%2520of%2520the%2520command-line%2520obfuscation%252C%2520all%2520designed%2520to%2520alter%2520the%250Acommand-line%2520syntax%2520without%2520affecting%2520its%2520original%2520functionality.%2520This%250Avariability%2520forces%2520most%2520security%2520solutions%2520to%2520create%2520an%2520exhaustive%2520enumeration%250Aof%2520signatures%2520for%2520even%2520a%2520single%2520pattern.%2520In%2520contrast%2520to%2520using%2520signatures%252C%2520we%250Ahave%2520implemented%2520a%2520scalable%2520NLP-based%2520detection%2520method%2520that%2520leverages%2520a%250Acustom-trained%252C%2520small%2520transformer%2520language%2520model%2520that%2520can%2520be%2520applied%2520to%2520any%250Asource%2520of%2520execution%2520logs.%2520The%2520evaluation%2520on%2520top%2520of%2520real-world%2520telemetry%250Ademonstrates%2520that%2520our%2520approach%2520yields%2520high-precision%2520detections%2520even%2520on%250Ahigh-volume%2520telemetry%2520from%2520a%2520diverse%2520set%2520of%2520environments%2520spanning%2520from%250Auniversities%2520and%2520businesses%2520to%2520healthcare%2520or%2520finance.%2520The%2520practical%2520value%2520is%250Ademonstrated%2520in%2520a%2520case%2520study%2520of%2520real-world%2520samples%2520detected%2520by%2520our%2520model.%2520We%250Ashow%2520the%2520model%2527s%2520superiority%2520to%2520signatures%2520on%2520established%2520malware%2520known%2520to%250Aemploy%2520obfuscation%2520and%2520showcase%2520previously%2520unseen%2520obfuscated%2520samples%2520detected%250Aby%2520our%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Command-line%20Obfuscation%20Detection%20using%20Small%20Language%20Models&entry.906535625=Vojtech%20Outrata%20and%20Michael%20Adam%20Polak%20and%20Martin%20Kopp&entry.1292438233=%20%20To%20avoid%20detection%2C%20adversaries%20often%20use%20command-line%20obfuscation.%20There%20are%0Anumerous%20techniques%20of%20the%20command-line%20obfuscation%2C%20all%20designed%20to%20alter%20the%0Acommand-line%20syntax%20without%20affecting%20its%20original%20functionality.%20This%0Avariability%20forces%20most%20security%20solutions%20to%20create%20an%20exhaustive%20enumeration%0Aof%20signatures%20for%20even%20a%20single%20pattern.%20In%20contrast%20to%20using%20signatures%2C%20we%0Ahave%20implemented%20a%20scalable%20NLP-based%20detection%20method%20that%20leverages%20a%0Acustom-trained%2C%20small%20transformer%20language%20model%20that%20can%20be%20applied%20to%20any%0Asource%20of%20execution%20logs.%20The%20evaluation%20on%20top%20of%20real-world%20telemetry%0Ademonstrates%20that%20our%20approach%20yields%20high-precision%20detections%20even%20on%0Ahigh-volume%20telemetry%20from%20a%20diverse%20set%20of%20environments%20spanning%20from%0Auniversities%20and%20businesses%20to%20healthcare%20or%20finance.%20The%20practical%20value%20is%0Ademonstrated%20in%20a%20case%20study%20of%20real-world%20samples%20detected%20by%20our%20model.%20We%0Ashow%20the%20model%27s%20superiority%20to%20signatures%20on%20established%20malware%20known%20to%0Aemploy%20obfuscation%20and%20showcase%20previously%20unseen%20obfuscated%20samples%20detected%0Aby%20our%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02637v1&entry.124074799=Read"},
{"title": "Backward explanations via redefinition of predicates", "author": "L\u00e9o Sauli\u00e8res and Martin C. Cooper and Florence Dupin de Saint Cyr", "abstract": "  History eXplanation based on Predicates (HXP), studies the behavior of a\nReinforcement Learning (RL) agent in a sequence of agent's interactions with\nthe environment (a history), through the prism of an arbitrary predicate. To\nthis end, an action importance score is computed for each action in the\nhistory. The explanation consists in displaying the most important actions to\nthe user. As the calculation of an action's importance is #W[1]-hard, it is\nnecessary for long histories to approximate the scores, at the expense of their\nquality. We therefore propose a new HXP method, called Backward-HXP, to provide\nexplanations for these histories without having to approximate scores.\nExperiments show the ability of B-HXP to summarise long histories.\n", "link": "http://arxiv.org/abs/2408.02606v1", "date": "2024-08-05", "relevancy": 1.2174, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4064}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backward%20explanations%20via%20redefinition%20of%20predicates&body=Title%3A%20Backward%20explanations%20via%20redefinition%20of%20predicates%0AAuthor%3A%20L%C3%A9o%20Sauli%C3%A8res%20and%20Martin%20C.%20Cooper%20and%20Florence%20Dupin%20de%20Saint%20Cyr%0AAbstract%3A%20%20%20History%20eXplanation%20based%20on%20Predicates%20%28HXP%29%2C%20studies%20the%20behavior%20of%20a%0AReinforcement%20Learning%20%28RL%29%20agent%20in%20a%20sequence%20of%20agent%27s%20interactions%20with%0Athe%20environment%20%28a%20history%29%2C%20through%20the%20prism%20of%20an%20arbitrary%20predicate.%20To%0Athis%20end%2C%20an%20action%20importance%20score%20is%20computed%20for%20each%20action%20in%20the%0Ahistory.%20The%20explanation%20consists%20in%20displaying%20the%20most%20important%20actions%20to%0Athe%20user.%20As%20the%20calculation%20of%20an%20action%27s%20importance%20is%20%23W%5B1%5D-hard%2C%20it%20is%0Anecessary%20for%20long%20histories%20to%20approximate%20the%20scores%2C%20at%20the%20expense%20of%20their%0Aquality.%20We%20therefore%20propose%20a%20new%20HXP%20method%2C%20called%20Backward-HXP%2C%20to%20provide%0Aexplanations%20for%20these%20histories%20without%20having%20to%20approximate%20scores.%0AExperiments%20show%20the%20ability%20of%20B-HXP%20to%20summarise%20long%20histories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackward%2520explanations%2520via%2520redefinition%2520of%2520predicates%26entry.906535625%3DL%25C3%25A9o%2520Sauli%25C3%25A8res%2520and%2520Martin%2520C.%2520Cooper%2520and%2520Florence%2520Dupin%2520de%2520Saint%2520Cyr%26entry.1292438233%3D%2520%2520History%2520eXplanation%2520based%2520on%2520Predicates%2520%2528HXP%2529%252C%2520studies%2520the%2520behavior%2520of%2520a%250AReinforcement%2520Learning%2520%2528RL%2529%2520agent%2520in%2520a%2520sequence%2520of%2520agent%2527s%2520interactions%2520with%250Athe%2520environment%2520%2528a%2520history%2529%252C%2520through%2520the%2520prism%2520of%2520an%2520arbitrary%2520predicate.%2520To%250Athis%2520end%252C%2520an%2520action%2520importance%2520score%2520is%2520computed%2520for%2520each%2520action%2520in%2520the%250Ahistory.%2520The%2520explanation%2520consists%2520in%2520displaying%2520the%2520most%2520important%2520actions%2520to%250Athe%2520user.%2520As%2520the%2520calculation%2520of%2520an%2520action%2527s%2520importance%2520is%2520%2523W%255B1%255D-hard%252C%2520it%2520is%250Anecessary%2520for%2520long%2520histories%2520to%2520approximate%2520the%2520scores%252C%2520at%2520the%2520expense%2520of%2520their%250Aquality.%2520We%2520therefore%2520propose%2520a%2520new%2520HXP%2520method%252C%2520called%2520Backward-HXP%252C%2520to%2520provide%250Aexplanations%2520for%2520these%2520histories%2520without%2520having%2520to%2520approximate%2520scores.%250AExperiments%2520show%2520the%2520ability%2520of%2520B-HXP%2520to%2520summarise%2520long%2520histories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backward%20explanations%20via%20redefinition%20of%20predicates&entry.906535625=L%C3%A9o%20Sauli%C3%A8res%20and%20Martin%20C.%20Cooper%20and%20Florence%20Dupin%20de%20Saint%20Cyr&entry.1292438233=%20%20History%20eXplanation%20based%20on%20Predicates%20%28HXP%29%2C%20studies%20the%20behavior%20of%20a%0AReinforcement%20Learning%20%28RL%29%20agent%20in%20a%20sequence%20of%20agent%27s%20interactions%20with%0Athe%20environment%20%28a%20history%29%2C%20through%20the%20prism%20of%20an%20arbitrary%20predicate.%20To%0Athis%20end%2C%20an%20action%20importance%20score%20is%20computed%20for%20each%20action%20in%20the%0Ahistory.%20The%20explanation%20consists%20in%20displaying%20the%20most%20important%20actions%20to%0Athe%20user.%20As%20the%20calculation%20of%20an%20action%27s%20importance%20is%20%23W%5B1%5D-hard%2C%20it%20is%0Anecessary%20for%20long%20histories%20to%20approximate%20the%20scores%2C%20at%20the%20expense%20of%20their%0Aquality.%20We%20therefore%20propose%20a%20new%20HXP%20method%2C%20called%20Backward-HXP%2C%20to%20provide%0Aexplanations%20for%20these%20histories%20without%20having%20to%20approximate%20scores.%0AExperiments%20show%20the%20ability%20of%20B-HXP%20to%20summarise%20long%20histories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02606v1&entry.124074799=Read"},
{"title": "Operationalizing Contextual Integrity in Privacy-Conscious Assistants", "author": "Sahra Ghalebikesabi and Eugene Bagdasaryan and Ren Yi and Itay Yona and Ilia Shumailov and Aneesh Pappu and Chongyang Shi and Laura Weidinger and Robert Stanforth and Leonard Berrada and Pushmeet Kohli and Po-Sen Huang and Borja Balle", "abstract": "  Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize $\\textit{contextual integrity}$\n(CI), a framework that equates privacy with the appropriate flow of information\nin a given context. In particular, we design and evaluate a number of\nstrategies to steer assistants' information-sharing actions to be CI compliant.\nOur evaluation is based on a novel form filling benchmark composed of synthetic\ndata and human annotations, and it reveals that prompting frontier LLMs to\nperform CI-based reasoning yields strong results.\n", "link": "http://arxiv.org/abs/2408.02373v1", "date": "2024-08-05", "relevancy": 0.8944, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4574}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operationalizing%20Contextual%20Integrity%20in%20Privacy-Conscious%20Assistants&body=Title%3A%20Operationalizing%20Contextual%20Integrity%20in%20Privacy-Conscious%20Assistants%0AAuthor%3A%20Sahra%20Ghalebikesabi%20and%20Eugene%20Bagdasaryan%20and%20Ren%20Yi%20and%20Itay%20Yona%20and%20Ilia%20Shumailov%20and%20Aneesh%20Pappu%20and%20Chongyang%20Shi%20and%20Laura%20Weidinger%20and%20Robert%20Stanforth%20and%20Leonard%20Berrada%20and%20Pushmeet%20Kohli%20and%20Po-Sen%20Huang%20and%20Borja%20Balle%0AAbstract%3A%20%20%20Advanced%20AI%20assistants%20combine%20frontier%20LLMs%20and%20tool%20access%20to%20autonomously%0Aperform%20complex%20tasks%20on%20behalf%20of%20users.%20While%20the%20helpfulness%20of%20such%0Aassistants%20can%20increase%20dramatically%20with%20access%20to%20user%20information%20including%0Aemails%20and%20documents%2C%20this%20raises%20privacy%20concerns%20about%20assistants%20sharing%0Ainappropriate%20information%20with%20third%20parties%20without%20user%20supervision.%20To%20steer%0Ainformation-sharing%20assistants%20to%20behave%20in%20accordance%20with%20privacy%0Aexpectations%2C%20we%20propose%20to%20operationalize%20%24%5Ctextit%7Bcontextual%20integrity%7D%24%0A%28CI%29%2C%20a%20framework%20that%20equates%20privacy%20with%20the%20appropriate%20flow%20of%20information%0Ain%20a%20given%20context.%20In%20particular%2C%20we%20design%20and%20evaluate%20a%20number%20of%0Astrategies%20to%20steer%20assistants%27%20information-sharing%20actions%20to%20be%20CI%20compliant.%0AOur%20evaluation%20is%20based%20on%20a%20novel%20form%20filling%20benchmark%20composed%20of%20synthetic%0Adata%20and%20human%20annotations%2C%20and%20it%20reveals%20that%20prompting%20frontier%20LLMs%20to%0Aperform%20CI-based%20reasoning%20yields%20strong%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperationalizing%2520Contextual%2520Integrity%2520in%2520Privacy-Conscious%2520Assistants%26entry.906535625%3DSahra%2520Ghalebikesabi%2520and%2520Eugene%2520Bagdasaryan%2520and%2520Ren%2520Yi%2520and%2520Itay%2520Yona%2520and%2520Ilia%2520Shumailov%2520and%2520Aneesh%2520Pappu%2520and%2520Chongyang%2520Shi%2520and%2520Laura%2520Weidinger%2520and%2520Robert%2520Stanforth%2520and%2520Leonard%2520Berrada%2520and%2520Pushmeet%2520Kohli%2520and%2520Po-Sen%2520Huang%2520and%2520Borja%2520Balle%26entry.1292438233%3D%2520%2520Advanced%2520AI%2520assistants%2520combine%2520frontier%2520LLMs%2520and%2520tool%2520access%2520to%2520autonomously%250Aperform%2520complex%2520tasks%2520on%2520behalf%2520of%2520users.%2520While%2520the%2520helpfulness%2520of%2520such%250Aassistants%2520can%2520increase%2520dramatically%2520with%2520access%2520to%2520user%2520information%2520including%250Aemails%2520and%2520documents%252C%2520this%2520raises%2520privacy%2520concerns%2520about%2520assistants%2520sharing%250Ainappropriate%2520information%2520with%2520third%2520parties%2520without%2520user%2520supervision.%2520To%2520steer%250Ainformation-sharing%2520assistants%2520to%2520behave%2520in%2520accordance%2520with%2520privacy%250Aexpectations%252C%2520we%2520propose%2520to%2520operationalize%2520%2524%255Ctextit%257Bcontextual%2520integrity%257D%2524%250A%2528CI%2529%252C%2520a%2520framework%2520that%2520equates%2520privacy%2520with%2520the%2520appropriate%2520flow%2520of%2520information%250Ain%2520a%2520given%2520context.%2520In%2520particular%252C%2520we%2520design%2520and%2520evaluate%2520a%2520number%2520of%250Astrategies%2520to%2520steer%2520assistants%2527%2520information-sharing%2520actions%2520to%2520be%2520CI%2520compliant.%250AOur%2520evaluation%2520is%2520based%2520on%2520a%2520novel%2520form%2520filling%2520benchmark%2520composed%2520of%2520synthetic%250Adata%2520and%2520human%2520annotations%252C%2520and%2520it%2520reveals%2520that%2520prompting%2520frontier%2520LLMs%2520to%250Aperform%2520CI-based%2520reasoning%2520yields%2520strong%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operationalizing%20Contextual%20Integrity%20in%20Privacy-Conscious%20Assistants&entry.906535625=Sahra%20Ghalebikesabi%20and%20Eugene%20Bagdasaryan%20and%20Ren%20Yi%20and%20Itay%20Yona%20and%20Ilia%20Shumailov%20and%20Aneesh%20Pappu%20and%20Chongyang%20Shi%20and%20Laura%20Weidinger%20and%20Robert%20Stanforth%20and%20Leonard%20Berrada%20and%20Pushmeet%20Kohli%20and%20Po-Sen%20Huang%20and%20Borja%20Balle&entry.1292438233=%20%20Advanced%20AI%20assistants%20combine%20frontier%20LLMs%20and%20tool%20access%20to%20autonomously%0Aperform%20complex%20tasks%20on%20behalf%20of%20users.%20While%20the%20helpfulness%20of%20such%0Aassistants%20can%20increase%20dramatically%20with%20access%20to%20user%20information%20including%0Aemails%20and%20documents%2C%20this%20raises%20privacy%20concerns%20about%20assistants%20sharing%0Ainappropriate%20information%20with%20third%20parties%20without%20user%20supervision.%20To%20steer%0Ainformation-sharing%20assistants%20to%20behave%20in%20accordance%20with%20privacy%0Aexpectations%2C%20we%20propose%20to%20operationalize%20%24%5Ctextit%7Bcontextual%20integrity%7D%24%0A%28CI%29%2C%20a%20framework%20that%20equates%20privacy%20with%20the%20appropriate%20flow%20of%20information%0Ain%20a%20given%20context.%20In%20particular%2C%20we%20design%20and%20evaluate%20a%20number%20of%0Astrategies%20to%20steer%20assistants%27%20information-sharing%20actions%20to%20be%20CI%20compliant.%0AOur%20evaluation%20is%20based%20on%20a%20novel%20form%20filling%20benchmark%20composed%20of%20synthetic%0Adata%20and%20human%20annotations%2C%20and%20it%20reveals%20that%20prompting%20frontier%20LLMs%20to%0Aperform%20CI-based%20reasoning%20yields%20strong%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02373v1&entry.124074799=Read"},
{"title": "Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning", "author": "Khanh Nguyen and Huy Hoang Nguyen and Egor Panfilov and Aleksei Tiulpin", "abstract": "  Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2408.02349v1", "date": "2024-08-05", "relevancy": 1.4803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4908}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Sensing%20of%20Knee%20Osteoarthritis%20Progression%20with%20Reinforcement%0A%20%20Learning&body=Title%3A%20Active%20Sensing%20of%20Knee%20Osteoarthritis%20Progression%20with%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Khanh%20Nguyen%20and%20Huy%20Hoang%20Nguyen%20and%20Egor%20Panfilov%20and%20Aleksei%20Tiulpin%0AAbstract%3A%20%20%20Osteoarthritis%20%28OA%29%20is%20the%20most%20common%20musculoskeletal%20disease%2C%20which%20has%20no%0Acure.%20Knee%20OA%20%28KOA%29%20is%20one%20of%20the%20highest%20causes%20of%20disability%20worldwide%2C%20and%0Ait%20costs%20billions%20of%20United%20States%20dollars%20to%20the%20global%20community.%20Prediction%0Aof%20KOA%20progression%20has%20been%20of%20high%20interest%20to%20the%20community%20for%20years%2C%20as%20it%0Acan%20advance%20treatment%20development%20through%20more%20efficient%20clinical%20trials%20and%0Aimprove%20patient%20outcomes%20through%20more%20efficient%20healthcare%20utilization.%0AExisting%20approaches%20for%20predicting%20KOA%2C%20however%2C%20are%20predominantly%20static%2C%20i.e.%0Aconsider%20data%20from%20a%20single%20time%20point%20to%20predict%20progression%20many%20years%20into%0Athe%20future%2C%20and%20knee%20level%2C%20i.e.%20consider%20progression%20in%20a%20single%20joint%20only.%0ADue%20to%20these%20and%20related%20reasons%2C%20these%20methods%20fail%20to%20deliver%20the%20level%20of%0Apredictive%20performance%2C%20which%20is%20sufficient%20to%20result%20in%20cost%20savings%20and%0Abetter%20patient%20outcomes.%20Collecting%20extensive%20data%20from%20all%20patients%20on%20a%0Aregular%20basis%20could%20address%20the%20issue%2C%20but%20it%20is%20limited%20by%20the%20high%20cost%20at%20a%0Apopulation%20level.%20In%20this%20work%2C%20we%20propose%20to%20go%20beyond%20static%20prediction%0Amodels%20in%20OA%2C%20and%20bring%20a%20novel%20Active%20Sensing%20%28AS%29%20approach%2C%20designed%20to%0Adynamically%20follow%20up%20patients%20with%20the%20objective%20of%20maximizing%20the%20number%20of%0Ainformative%20data%20acquisitions%2C%20while%20minimizing%20their%20total%20cost%20over%20a%20period%0Aof%20time.%20Our%20approach%20is%20based%20on%20Reinforcement%20Learning%20%28RL%29%2C%20and%20it%20leverages%0Aa%20novel%20reward%20function%20designed%20specifically%20for%20AS%20of%20disease%20progression%20in%0Amore%20than%20one%20part%20of%20a%20human%20body.%20Our%20method%20is%20end-to-end%2C%20relies%20on%0Amulti-modal%20Deep%20Learning%2C%20and%20requires%20no%20human%20input%20at%20inference%20time.%0AThroughout%20an%20exhaustive%20experimental%20evaluation%2C%20we%20show%20that%20using%20RL%20can%0Aprovide%20a%20higher%20monetary%20benefit%20when%20compared%20to%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Sensing%2520of%2520Knee%2520Osteoarthritis%2520Progression%2520with%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DKhanh%2520Nguyen%2520and%2520Huy%2520Hoang%2520Nguyen%2520and%2520Egor%2520Panfilov%2520and%2520Aleksei%2520Tiulpin%26entry.1292438233%3D%2520%2520Osteoarthritis%2520%2528OA%2529%2520is%2520the%2520most%2520common%2520musculoskeletal%2520disease%252C%2520which%2520has%2520no%250Acure.%2520Knee%2520OA%2520%2528KOA%2529%2520is%2520one%2520of%2520the%2520highest%2520causes%2520of%2520disability%2520worldwide%252C%2520and%250Ait%2520costs%2520billions%2520of%2520United%2520States%2520dollars%2520to%2520the%2520global%2520community.%2520Prediction%250Aof%2520KOA%2520progression%2520has%2520been%2520of%2520high%2520interest%2520to%2520the%2520community%2520for%2520years%252C%2520as%2520it%250Acan%2520advance%2520treatment%2520development%2520through%2520more%2520efficient%2520clinical%2520trials%2520and%250Aimprove%2520patient%2520outcomes%2520through%2520more%2520efficient%2520healthcare%2520utilization.%250AExisting%2520approaches%2520for%2520predicting%2520KOA%252C%2520however%252C%2520are%2520predominantly%2520static%252C%2520i.e.%250Aconsider%2520data%2520from%2520a%2520single%2520time%2520point%2520to%2520predict%2520progression%2520many%2520years%2520into%250Athe%2520future%252C%2520and%2520knee%2520level%252C%2520i.e.%2520consider%2520progression%2520in%2520a%2520single%2520joint%2520only.%250ADue%2520to%2520these%2520and%2520related%2520reasons%252C%2520these%2520methods%2520fail%2520to%2520deliver%2520the%2520level%2520of%250Apredictive%2520performance%252C%2520which%2520is%2520sufficient%2520to%2520result%2520in%2520cost%2520savings%2520and%250Abetter%2520patient%2520outcomes.%2520Collecting%2520extensive%2520data%2520from%2520all%2520patients%2520on%2520a%250Aregular%2520basis%2520could%2520address%2520the%2520issue%252C%2520but%2520it%2520is%2520limited%2520by%2520the%2520high%2520cost%2520at%2520a%250Apopulation%2520level.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520go%2520beyond%2520static%2520prediction%250Amodels%2520in%2520OA%252C%2520and%2520bring%2520a%2520novel%2520Active%2520Sensing%2520%2528AS%2529%2520approach%252C%2520designed%2520to%250Adynamically%2520follow%2520up%2520patients%2520with%2520the%2520objective%2520of%2520maximizing%2520the%2520number%2520of%250Ainformative%2520data%2520acquisitions%252C%2520while%2520minimizing%2520their%2520total%2520cost%2520over%2520a%2520period%250Aof%2520time.%2520Our%2520approach%2520is%2520based%2520on%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520and%2520it%2520leverages%250Aa%2520novel%2520reward%2520function%2520designed%2520specifically%2520for%2520AS%2520of%2520disease%2520progression%2520in%250Amore%2520than%2520one%2520part%2520of%2520a%2520human%2520body.%2520Our%2520method%2520is%2520end-to-end%252C%2520relies%2520on%250Amulti-modal%2520Deep%2520Learning%252C%2520and%2520requires%2520no%2520human%2520input%2520at%2520inference%2520time.%250AThroughout%2520an%2520exhaustive%2520experimental%2520evaluation%252C%2520we%2520show%2520that%2520using%2520RL%2520can%250Aprovide%2520a%2520higher%2520monetary%2520benefit%2520when%2520compared%2520to%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Sensing%20of%20Knee%20Osteoarthritis%20Progression%20with%20Reinforcement%0A%20%20Learning&entry.906535625=Khanh%20Nguyen%20and%20Huy%20Hoang%20Nguyen%20and%20Egor%20Panfilov%20and%20Aleksei%20Tiulpin&entry.1292438233=%20%20Osteoarthritis%20%28OA%29%20is%20the%20most%20common%20musculoskeletal%20disease%2C%20which%20has%20no%0Acure.%20Knee%20OA%20%28KOA%29%20is%20one%20of%20the%20highest%20causes%20of%20disability%20worldwide%2C%20and%0Ait%20costs%20billions%20of%20United%20States%20dollars%20to%20the%20global%20community.%20Prediction%0Aof%20KOA%20progression%20has%20been%20of%20high%20interest%20to%20the%20community%20for%20years%2C%20as%20it%0Acan%20advance%20treatment%20development%20through%20more%20efficient%20clinical%20trials%20and%0Aimprove%20patient%20outcomes%20through%20more%20efficient%20healthcare%20utilization.%0AExisting%20approaches%20for%20predicting%20KOA%2C%20however%2C%20are%20predominantly%20static%2C%20i.e.%0Aconsider%20data%20from%20a%20single%20time%20point%20to%20predict%20progression%20many%20years%20into%0Athe%20future%2C%20and%20knee%20level%2C%20i.e.%20consider%20progression%20in%20a%20single%20joint%20only.%0ADue%20to%20these%20and%20related%20reasons%2C%20these%20methods%20fail%20to%20deliver%20the%20level%20of%0Apredictive%20performance%2C%20which%20is%20sufficient%20to%20result%20in%20cost%20savings%20and%0Abetter%20patient%20outcomes.%20Collecting%20extensive%20data%20from%20all%20patients%20on%20a%0Aregular%20basis%20could%20address%20the%20issue%2C%20but%20it%20is%20limited%20by%20the%20high%20cost%20at%20a%0Apopulation%20level.%20In%20this%20work%2C%20we%20propose%20to%20go%20beyond%20static%20prediction%0Amodels%20in%20OA%2C%20and%20bring%20a%20novel%20Active%20Sensing%20%28AS%29%20approach%2C%20designed%20to%0Adynamically%20follow%20up%20patients%20with%20the%20objective%20of%20maximizing%20the%20number%20of%0Ainformative%20data%20acquisitions%2C%20while%20minimizing%20their%20total%20cost%20over%20a%20period%0Aof%20time.%20Our%20approach%20is%20based%20on%20Reinforcement%20Learning%20%28RL%29%2C%20and%20it%20leverages%0Aa%20novel%20reward%20function%20designed%20specifically%20for%20AS%20of%20disease%20progression%20in%0Amore%20than%20one%20part%20of%20a%20human%20body.%20Our%20method%20is%20end-to-end%2C%20relies%20on%0Amulti-modal%20Deep%20Learning%2C%20and%20requires%20no%20human%20input%20at%20inference%20time.%0AThroughout%20an%20exhaustive%20experimental%20evaluation%2C%20we%20show%20that%20using%20RL%20can%0Aprovide%20a%20higher%20monetary%20benefit%20when%20compared%20to%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02349v1&entry.124074799=Read"},
{"title": "Quantile Regression using Random Forest Proximities", "author": "Mingshu Li and Bhaskarjit Sarmah and Dhruv Desai and Joshua Rosaler and Snigdha Bhagat and Philip Sommer and Dhagash Mehta", "abstract": "  Due to the dynamic nature of financial markets, maintaining models that\nproduce precise predictions over time is difficult. Often the goal isn't just\npoint prediction but determining uncertainty. Quantifying uncertainty,\nespecially the aleatoric uncertainty due to the unpredictable nature of market\ndrivers, helps investors understand varying risk levels. Recently, quantile\nregression forests (QRF) have emerged as a promising solution: Unlike most\nbasic quantile regression methods that need separate models for each quantile,\nquantile regression forests estimate the entire conditional distribution of the\ntarget variable with a single model, while retaining all the salient features\nof a typical random forest. We introduce a novel approach to compute quantile\nregressions from random forests that leverages the proximity (i.e., distance\nmetric) learned by the model and infers the conditional distribution of the\ntarget variable. We evaluate the proposed methodology using publicly available\ndatasets and then apply it towards the problem of forecasting the average daily\nvolume of corporate bonds. We show that using quantile regression using Random\nForest proximities demonstrates superior performance in approximating\nconditional target distributions and prediction intervals to the original\nversion of QRF. We also demonstrate that the proposed framework is\nsignificantly more computationally efficient than traditional approaches to\nquantile regressions.\n", "link": "http://arxiv.org/abs/2408.02355v1", "date": "2024-08-05", "relevancy": 1.2372, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4511}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.421}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantile%20Regression%20using%20Random%20Forest%20Proximities&body=Title%3A%20Quantile%20Regression%20using%20Random%20Forest%20Proximities%0AAuthor%3A%20Mingshu%20Li%20and%20Bhaskarjit%20Sarmah%20and%20Dhruv%20Desai%20and%20Joshua%20Rosaler%20and%20Snigdha%20Bhagat%20and%20Philip%20Sommer%20and%20Dhagash%20Mehta%0AAbstract%3A%20%20%20Due%20to%20the%20dynamic%20nature%20of%20financial%20markets%2C%20maintaining%20models%20that%0Aproduce%20precise%20predictions%20over%20time%20is%20difficult.%20Often%20the%20goal%20isn%27t%20just%0Apoint%20prediction%20but%20determining%20uncertainty.%20Quantifying%20uncertainty%2C%0Aespecially%20the%20aleatoric%20uncertainty%20due%20to%20the%20unpredictable%20nature%20of%20market%0Adrivers%2C%20helps%20investors%20understand%20varying%20risk%20levels.%20Recently%2C%20quantile%0Aregression%20forests%20%28QRF%29%20have%20emerged%20as%20a%20promising%20solution%3A%20Unlike%20most%0Abasic%20quantile%20regression%20methods%20that%20need%20separate%20models%20for%20each%20quantile%2C%0Aquantile%20regression%20forests%20estimate%20the%20entire%20conditional%20distribution%20of%20the%0Atarget%20variable%20with%20a%20single%20model%2C%20while%20retaining%20all%20the%20salient%20features%0Aof%20a%20typical%20random%20forest.%20We%20introduce%20a%20novel%20approach%20to%20compute%20quantile%0Aregressions%20from%20random%20forests%20that%20leverages%20the%20proximity%20%28i.e.%2C%20distance%0Ametric%29%20learned%20by%20the%20model%20and%20infers%20the%20conditional%20distribution%20of%20the%0Atarget%20variable.%20We%20evaluate%20the%20proposed%20methodology%20using%20publicly%20available%0Adatasets%20and%20then%20apply%20it%20towards%20the%20problem%20of%20forecasting%20the%20average%20daily%0Avolume%20of%20corporate%20bonds.%20We%20show%20that%20using%20quantile%20regression%20using%20Random%0AForest%20proximities%20demonstrates%20superior%20performance%20in%20approximating%0Aconditional%20target%20distributions%20and%20prediction%20intervals%20to%20the%20original%0Aversion%20of%20QRF.%20We%20also%20demonstrate%20that%20the%20proposed%20framework%20is%0Asignificantly%20more%20computationally%20efficient%20than%20traditional%20approaches%20to%0Aquantile%20regressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantile%2520Regression%2520using%2520Random%2520Forest%2520Proximities%26entry.906535625%3DMingshu%2520Li%2520and%2520Bhaskarjit%2520Sarmah%2520and%2520Dhruv%2520Desai%2520and%2520Joshua%2520Rosaler%2520and%2520Snigdha%2520Bhagat%2520and%2520Philip%2520Sommer%2520and%2520Dhagash%2520Mehta%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520dynamic%2520nature%2520of%2520financial%2520markets%252C%2520maintaining%2520models%2520that%250Aproduce%2520precise%2520predictions%2520over%2520time%2520is%2520difficult.%2520Often%2520the%2520goal%2520isn%2527t%2520just%250Apoint%2520prediction%2520but%2520determining%2520uncertainty.%2520Quantifying%2520uncertainty%252C%250Aespecially%2520the%2520aleatoric%2520uncertainty%2520due%2520to%2520the%2520unpredictable%2520nature%2520of%2520market%250Adrivers%252C%2520helps%2520investors%2520understand%2520varying%2520risk%2520levels.%2520Recently%252C%2520quantile%250Aregression%2520forests%2520%2528QRF%2529%2520have%2520emerged%2520as%2520a%2520promising%2520solution%253A%2520Unlike%2520most%250Abasic%2520quantile%2520regression%2520methods%2520that%2520need%2520separate%2520models%2520for%2520each%2520quantile%252C%250Aquantile%2520regression%2520forests%2520estimate%2520the%2520entire%2520conditional%2520distribution%2520of%2520the%250Atarget%2520variable%2520with%2520a%2520single%2520model%252C%2520while%2520retaining%2520all%2520the%2520salient%2520features%250Aof%2520a%2520typical%2520random%2520forest.%2520We%2520introduce%2520a%2520novel%2520approach%2520to%2520compute%2520quantile%250Aregressions%2520from%2520random%2520forests%2520that%2520leverages%2520the%2520proximity%2520%2528i.e.%252C%2520distance%250Ametric%2529%2520learned%2520by%2520the%2520model%2520and%2520infers%2520the%2520conditional%2520distribution%2520of%2520the%250Atarget%2520variable.%2520We%2520evaluate%2520the%2520proposed%2520methodology%2520using%2520publicly%2520available%250Adatasets%2520and%2520then%2520apply%2520it%2520towards%2520the%2520problem%2520of%2520forecasting%2520the%2520average%2520daily%250Avolume%2520of%2520corporate%2520bonds.%2520We%2520show%2520that%2520using%2520quantile%2520regression%2520using%2520Random%250AForest%2520proximities%2520demonstrates%2520superior%2520performance%2520in%2520approximating%250Aconditional%2520target%2520distributions%2520and%2520prediction%2520intervals%2520to%2520the%2520original%250Aversion%2520of%2520QRF.%2520We%2520also%2520demonstrate%2520that%2520the%2520proposed%2520framework%2520is%250Asignificantly%2520more%2520computationally%2520efficient%2520than%2520traditional%2520approaches%2520to%250Aquantile%2520regressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantile%20Regression%20using%20Random%20Forest%20Proximities&entry.906535625=Mingshu%20Li%20and%20Bhaskarjit%20Sarmah%20and%20Dhruv%20Desai%20and%20Joshua%20Rosaler%20and%20Snigdha%20Bhagat%20and%20Philip%20Sommer%20and%20Dhagash%20Mehta&entry.1292438233=%20%20Due%20to%20the%20dynamic%20nature%20of%20financial%20markets%2C%20maintaining%20models%20that%0Aproduce%20precise%20predictions%20over%20time%20is%20difficult.%20Often%20the%20goal%20isn%27t%20just%0Apoint%20prediction%20but%20determining%20uncertainty.%20Quantifying%20uncertainty%2C%0Aespecially%20the%20aleatoric%20uncertainty%20due%20to%20the%20unpredictable%20nature%20of%20market%0Adrivers%2C%20helps%20investors%20understand%20varying%20risk%20levels.%20Recently%2C%20quantile%0Aregression%20forests%20%28QRF%29%20have%20emerged%20as%20a%20promising%20solution%3A%20Unlike%20most%0Abasic%20quantile%20regression%20methods%20that%20need%20separate%20models%20for%20each%20quantile%2C%0Aquantile%20regression%20forests%20estimate%20the%20entire%20conditional%20distribution%20of%20the%0Atarget%20variable%20with%20a%20single%20model%2C%20while%20retaining%20all%20the%20salient%20features%0Aof%20a%20typical%20random%20forest.%20We%20introduce%20a%20novel%20approach%20to%20compute%20quantile%0Aregressions%20from%20random%20forests%20that%20leverages%20the%20proximity%20%28i.e.%2C%20distance%0Ametric%29%20learned%20by%20the%20model%20and%20infers%20the%20conditional%20distribution%20of%20the%0Atarget%20variable.%20We%20evaluate%20the%20proposed%20methodology%20using%20publicly%20available%0Adatasets%20and%20then%20apply%20it%20towards%20the%20problem%20of%20forecasting%20the%20average%20daily%0Avolume%20of%20corporate%20bonds.%20We%20show%20that%20using%20quantile%20regression%20using%20Random%0AForest%20proximities%20demonstrates%20superior%20performance%20in%20approximating%0Aconditional%20target%20distributions%20and%20prediction%20intervals%20to%20the%20original%0Aversion%20of%20QRF.%20We%20also%20demonstrate%20that%20the%20proposed%20framework%20is%0Asignificantly%20more%20computationally%20efficient%20than%20traditional%20approaches%20to%0Aquantile%20regressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02355v1&entry.124074799=Read"},
{"title": "Operational range bounding of spectroscopy models with anomaly detection", "author": "Lu\u00eds F. Sim\u00f5es and Pierluigi Casale and Mar\u00edlia Felismino and Kai Hou Yip and Ingo P. Waldmann and Giovanna Tinetti and Theresa Lueftinger", "abstract": "  Safe operation of machine learning models requires architectures that\nexplicitly delimit their operational ranges. We evaluate the ability of anomaly\ndetection algorithms to provide indicators correlated with degraded model\nperformance. By placing acceptance thresholds over such indicators, hard\nboundaries are formed that define the model's coverage. As a use case, we\nconsider the extraction of exoplanetary spectra from transit light curves,\nspecifically within the context of ESA's upcoming Ariel mission. Isolation\nForests are shown to effectively identify contexts where prediction models are\nlikely to fail. Coverage/error trade-offs are evaluated under conditions of\ndata and concept drift. The best performance is seen when Isolation Forests\nmodel projections of the prediction model's explainability SHAP values.\n", "link": "http://arxiv.org/abs/2408.02581v1", "date": "2024-08-05", "relevancy": 1.4193, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4806}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4739}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operational%20range%20bounding%20of%20spectroscopy%20models%20with%20anomaly%20detection&body=Title%3A%20Operational%20range%20bounding%20of%20spectroscopy%20models%20with%20anomaly%20detection%0AAuthor%3A%20Lu%C3%ADs%20F.%20Sim%C3%B5es%20and%20Pierluigi%20Casale%20and%20Mar%C3%ADlia%20Felismino%20and%20Kai%20Hou%20Yip%20and%20Ingo%20P.%20Waldmann%20and%20Giovanna%20Tinetti%20and%20Theresa%20Lueftinger%0AAbstract%3A%20%20%20Safe%20operation%20of%20machine%20learning%20models%20requires%20architectures%20that%0Aexplicitly%20delimit%20their%20operational%20ranges.%20We%20evaluate%20the%20ability%20of%20anomaly%0Adetection%20algorithms%20to%20provide%20indicators%20correlated%20with%20degraded%20model%0Aperformance.%20By%20placing%20acceptance%20thresholds%20over%20such%20indicators%2C%20hard%0Aboundaries%20are%20formed%20that%20define%20the%20model%27s%20coverage.%20As%20a%20use%20case%2C%20we%0Aconsider%20the%20extraction%20of%20exoplanetary%20spectra%20from%20transit%20light%20curves%2C%0Aspecifically%20within%20the%20context%20of%20ESA%27s%20upcoming%20Ariel%20mission.%20Isolation%0AForests%20are%20shown%20to%20effectively%20identify%20contexts%20where%20prediction%20models%20are%0Alikely%20to%20fail.%20Coverage/error%20trade-offs%20are%20evaluated%20under%20conditions%20of%0Adata%20and%20concept%20drift.%20The%20best%20performance%20is%20seen%20when%20Isolation%20Forests%0Amodel%20projections%20of%20the%20prediction%20model%27s%20explainability%20SHAP%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperational%2520range%2520bounding%2520of%2520spectroscopy%2520models%2520with%2520anomaly%2520detection%26entry.906535625%3DLu%25C3%25ADs%2520F.%2520Sim%25C3%25B5es%2520and%2520Pierluigi%2520Casale%2520and%2520Mar%25C3%25ADlia%2520Felismino%2520and%2520Kai%2520Hou%2520Yip%2520and%2520Ingo%2520P.%2520Waldmann%2520and%2520Giovanna%2520Tinetti%2520and%2520Theresa%2520Lueftinger%26entry.1292438233%3D%2520%2520Safe%2520operation%2520of%2520machine%2520learning%2520models%2520requires%2520architectures%2520that%250Aexplicitly%2520delimit%2520their%2520operational%2520ranges.%2520We%2520evaluate%2520the%2520ability%2520of%2520anomaly%250Adetection%2520algorithms%2520to%2520provide%2520indicators%2520correlated%2520with%2520degraded%2520model%250Aperformance.%2520By%2520placing%2520acceptance%2520thresholds%2520over%2520such%2520indicators%252C%2520hard%250Aboundaries%2520are%2520formed%2520that%2520define%2520the%2520model%2527s%2520coverage.%2520As%2520a%2520use%2520case%252C%2520we%250Aconsider%2520the%2520extraction%2520of%2520exoplanetary%2520spectra%2520from%2520transit%2520light%2520curves%252C%250Aspecifically%2520within%2520the%2520context%2520of%2520ESA%2527s%2520upcoming%2520Ariel%2520mission.%2520Isolation%250AForests%2520are%2520shown%2520to%2520effectively%2520identify%2520contexts%2520where%2520prediction%2520models%2520are%250Alikely%2520to%2520fail.%2520Coverage/error%2520trade-offs%2520are%2520evaluated%2520under%2520conditions%2520of%250Adata%2520and%2520concept%2520drift.%2520The%2520best%2520performance%2520is%2520seen%2520when%2520Isolation%2520Forests%250Amodel%2520projections%2520of%2520the%2520prediction%2520model%2527s%2520explainability%2520SHAP%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operational%20range%20bounding%20of%20spectroscopy%20models%20with%20anomaly%20detection&entry.906535625=Lu%C3%ADs%20F.%20Sim%C3%B5es%20and%20Pierluigi%20Casale%20and%20Mar%C3%ADlia%20Felismino%20and%20Kai%20Hou%20Yip%20and%20Ingo%20P.%20Waldmann%20and%20Giovanna%20Tinetti%20and%20Theresa%20Lueftinger&entry.1292438233=%20%20Safe%20operation%20of%20machine%20learning%20models%20requires%20architectures%20that%0Aexplicitly%20delimit%20their%20operational%20ranges.%20We%20evaluate%20the%20ability%20of%20anomaly%0Adetection%20algorithms%20to%20provide%20indicators%20correlated%20with%20degraded%20model%0Aperformance.%20By%20placing%20acceptance%20thresholds%20over%20such%20indicators%2C%20hard%0Aboundaries%20are%20formed%20that%20define%20the%20model%27s%20coverage.%20As%20a%20use%20case%2C%20we%0Aconsider%20the%20extraction%20of%20exoplanetary%20spectra%20from%20transit%20light%20curves%2C%0Aspecifically%20within%20the%20context%20of%20ESA%27s%20upcoming%20Ariel%20mission.%20Isolation%0AForests%20are%20shown%20to%20effectively%20identify%20contexts%20where%20prediction%20models%20are%0Alikely%20to%20fail.%20Coverage/error%20trade-offs%20are%20evaluated%20under%20conditions%20of%0Adata%20and%20concept%20drift.%20The%20best%20performance%20is%20seen%20when%20Isolation%20Forests%0Amodel%20projections%20of%20the%20prediction%20model%27s%20explainability%20SHAP%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02581v1&entry.124074799=Read"},
{"title": "Artificial Intelligence for Public Health Surveillance in Africa:\n  Applications and Opportunities", "author": "Jean Marie Tshimula and Mitterrand Kalengayi and Dieumerci Makenga and Dorcas Lilonge and Marius Asumani and D\u00e9borah Madiya and \u00c9lie Nkuba Kalonji and Hugues Kanda and Ren\u00e9 Manass\u00e9 Galekwa and Josias Kumbu and Hardy Mikese and Grace Tshimula and Jean Tshibangu Muabila and Christian N. Mayemba and D'Jeff K. Nkashama and Kalonji Kalala and Steve Ataky and Tighana Wenge Basele and Mbuyi Mukendi Didier and Selain K. Kasereka and Maximilien V. Dialufuma and Godwill Ilunga Wa Kumwita and Lionel Muyuku and Jean-Paul Kimpesa and Dominique Muteba and Aaron Aruna Abedi and Lambert Mukendi Ntobo and Gloria M. Bundutidi and D\u00e9sir\u00e9 Kulimba Mashinda and Emmanuel Kabengele Mpinga and Nathana\u00ebl M. Kasoro", "abstract": "  Artificial Intelligence (AI) is revolutionizing various fields, including\npublic health surveillance. In Africa, where health systems frequently\nencounter challenges such as limited resources, inadequate infrastructure,\nfailed health information systems and a shortage of skilled health\nprofessionals, AI offers a transformative opportunity. This paper investigates\nthe applications of AI in public health surveillance across the continent,\npresenting successful case studies and examining the benefits, opportunities,\nand challenges of implementing AI technologies in African healthcare settings.\nOur paper highlights AI's potential to enhance disease monitoring and health\noutcomes, and support effective public health interventions. The findings\npresented in the paper demonstrate that AI can significantly improve the\naccuracy and timeliness of disease detection and prediction, optimize resource\nallocation, and facilitate targeted public health strategies. Additionally, our\npaper identified key barriers to the widespread adoption of AI in African\npublic health systems and proposed actionable recommendations to overcome these\nchallenges.\n", "link": "http://arxiv.org/abs/2408.02575v1", "date": "2024-08-05", "relevancy": 1.5108, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3711}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20for%20Public%20Health%20Surveillance%20in%20Africa%3A%0A%20%20Applications%20and%20Opportunities&body=Title%3A%20Artificial%20Intelligence%20for%20Public%20Health%20Surveillance%20in%20Africa%3A%0A%20%20Applications%20and%20Opportunities%0AAuthor%3A%20Jean%20Marie%20Tshimula%20and%20Mitterrand%20Kalengayi%20and%20Dieumerci%20Makenga%20and%20Dorcas%20Lilonge%20and%20Marius%20Asumani%20and%20D%C3%A9borah%20Madiya%20and%20%C3%89lie%20Nkuba%20Kalonji%20and%20Hugues%20Kanda%20and%20Ren%C3%A9%20Manass%C3%A9%20Galekwa%20and%20Josias%20Kumbu%20and%20Hardy%20Mikese%20and%20Grace%20Tshimula%20and%20Jean%20Tshibangu%20Muabila%20and%20Christian%20N.%20Mayemba%20and%20D%27Jeff%20K.%20Nkashama%20and%20Kalonji%20Kalala%20and%20Steve%20Ataky%20and%20Tighana%20Wenge%20Basele%20and%20Mbuyi%20Mukendi%20Didier%20and%20Selain%20K.%20Kasereka%20and%20Maximilien%20V.%20Dialufuma%20and%20Godwill%20Ilunga%20Wa%20Kumwita%20and%20Lionel%20Muyuku%20and%20Jean-Paul%20Kimpesa%20and%20Dominique%20Muteba%20and%20Aaron%20Aruna%20Abedi%20and%20Lambert%20Mukendi%20Ntobo%20and%20Gloria%20M.%20Bundutidi%20and%20D%C3%A9sir%C3%A9%20Kulimba%20Mashinda%20and%20Emmanuel%20Kabengele%20Mpinga%20and%20Nathana%C3%ABl%20M.%20Kasoro%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20is%20revolutionizing%20various%20fields%2C%20including%0Apublic%20health%20surveillance.%20In%20Africa%2C%20where%20health%20systems%20frequently%0Aencounter%20challenges%20such%20as%20limited%20resources%2C%20inadequate%20infrastructure%2C%0Afailed%20health%20information%20systems%20and%20a%20shortage%20of%20skilled%20health%0Aprofessionals%2C%20AI%20offers%20a%20transformative%20opportunity.%20This%20paper%20investigates%0Athe%20applications%20of%20AI%20in%20public%20health%20surveillance%20across%20the%20continent%2C%0Apresenting%20successful%20case%20studies%20and%20examining%20the%20benefits%2C%20opportunities%2C%0Aand%20challenges%20of%20implementing%20AI%20technologies%20in%20African%20healthcare%20settings.%0AOur%20paper%20highlights%20AI%27s%20potential%20to%20enhance%20disease%20monitoring%20and%20health%0Aoutcomes%2C%20and%20support%20effective%20public%20health%20interventions.%20The%20findings%0Apresented%20in%20the%20paper%20demonstrate%20that%20AI%20can%20significantly%20improve%20the%0Aaccuracy%20and%20timeliness%20of%20disease%20detection%20and%20prediction%2C%20optimize%20resource%0Aallocation%2C%20and%20facilitate%20targeted%20public%20health%20strategies.%20Additionally%2C%20our%0Apaper%20identified%20key%20barriers%20to%20the%20widespread%20adoption%20of%20AI%20in%20African%0Apublic%20health%20systems%20and%20proposed%20actionable%20recommendations%20to%20overcome%20these%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520for%2520Public%2520Health%2520Surveillance%2520in%2520Africa%253A%250A%2520%2520Applications%2520and%2520Opportunities%26entry.906535625%3DJean%2520Marie%2520Tshimula%2520and%2520Mitterrand%2520Kalengayi%2520and%2520Dieumerci%2520Makenga%2520and%2520Dorcas%2520Lilonge%2520and%2520Marius%2520Asumani%2520and%2520D%25C3%25A9borah%2520Madiya%2520and%2520%25C3%2589lie%2520Nkuba%2520Kalonji%2520and%2520Hugues%2520Kanda%2520and%2520Ren%25C3%25A9%2520Manass%25C3%25A9%2520Galekwa%2520and%2520Josias%2520Kumbu%2520and%2520Hardy%2520Mikese%2520and%2520Grace%2520Tshimula%2520and%2520Jean%2520Tshibangu%2520Muabila%2520and%2520Christian%2520N.%2520Mayemba%2520and%2520D%2527Jeff%2520K.%2520Nkashama%2520and%2520Kalonji%2520Kalala%2520and%2520Steve%2520Ataky%2520and%2520Tighana%2520Wenge%2520Basele%2520and%2520Mbuyi%2520Mukendi%2520Didier%2520and%2520Selain%2520K.%2520Kasereka%2520and%2520Maximilien%2520V.%2520Dialufuma%2520and%2520Godwill%2520Ilunga%2520Wa%2520Kumwita%2520and%2520Lionel%2520Muyuku%2520and%2520Jean-Paul%2520Kimpesa%2520and%2520Dominique%2520Muteba%2520and%2520Aaron%2520Aruna%2520Abedi%2520and%2520Lambert%2520Mukendi%2520Ntobo%2520and%2520Gloria%2520M.%2520Bundutidi%2520and%2520D%25C3%25A9sir%25C3%25A9%2520Kulimba%2520Mashinda%2520and%2520Emmanuel%2520Kabengele%2520Mpinga%2520and%2520Nathana%25C3%25ABl%2520M.%2520Kasoro%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520is%2520revolutionizing%2520various%2520fields%252C%2520including%250Apublic%2520health%2520surveillance.%2520In%2520Africa%252C%2520where%2520health%2520systems%2520frequently%250Aencounter%2520challenges%2520such%2520as%2520limited%2520resources%252C%2520inadequate%2520infrastructure%252C%250Afailed%2520health%2520information%2520systems%2520and%2520a%2520shortage%2520of%2520skilled%2520health%250Aprofessionals%252C%2520AI%2520offers%2520a%2520transformative%2520opportunity.%2520This%2520paper%2520investigates%250Athe%2520applications%2520of%2520AI%2520in%2520public%2520health%2520surveillance%2520across%2520the%2520continent%252C%250Apresenting%2520successful%2520case%2520studies%2520and%2520examining%2520the%2520benefits%252C%2520opportunities%252C%250Aand%2520challenges%2520of%2520implementing%2520AI%2520technologies%2520in%2520African%2520healthcare%2520settings.%250AOur%2520paper%2520highlights%2520AI%2527s%2520potential%2520to%2520enhance%2520disease%2520monitoring%2520and%2520health%250Aoutcomes%252C%2520and%2520support%2520effective%2520public%2520health%2520interventions.%2520The%2520findings%250Apresented%2520in%2520the%2520paper%2520demonstrate%2520that%2520AI%2520can%2520significantly%2520improve%2520the%250Aaccuracy%2520and%2520timeliness%2520of%2520disease%2520detection%2520and%2520prediction%252C%2520optimize%2520resource%250Aallocation%252C%2520and%2520facilitate%2520targeted%2520public%2520health%2520strategies.%2520Additionally%252C%2520our%250Apaper%2520identified%2520key%2520barriers%2520to%2520the%2520widespread%2520adoption%2520of%2520AI%2520in%2520African%250Apublic%2520health%2520systems%2520and%2520proposed%2520actionable%2520recommendations%2520to%2520overcome%2520these%250Achallenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20for%20Public%20Health%20Surveillance%20in%20Africa%3A%0A%20%20Applications%20and%20Opportunities&entry.906535625=Jean%20Marie%20Tshimula%20and%20Mitterrand%20Kalengayi%20and%20Dieumerci%20Makenga%20and%20Dorcas%20Lilonge%20and%20Marius%20Asumani%20and%20D%C3%A9borah%20Madiya%20and%20%C3%89lie%20Nkuba%20Kalonji%20and%20Hugues%20Kanda%20and%20Ren%C3%A9%20Manass%C3%A9%20Galekwa%20and%20Josias%20Kumbu%20and%20Hardy%20Mikese%20and%20Grace%20Tshimula%20and%20Jean%20Tshibangu%20Muabila%20and%20Christian%20N.%20Mayemba%20and%20D%27Jeff%20K.%20Nkashama%20and%20Kalonji%20Kalala%20and%20Steve%20Ataky%20and%20Tighana%20Wenge%20Basele%20and%20Mbuyi%20Mukendi%20Didier%20and%20Selain%20K.%20Kasereka%20and%20Maximilien%20V.%20Dialufuma%20and%20Godwill%20Ilunga%20Wa%20Kumwita%20and%20Lionel%20Muyuku%20and%20Jean-Paul%20Kimpesa%20and%20Dominique%20Muteba%20and%20Aaron%20Aruna%20Abedi%20and%20Lambert%20Mukendi%20Ntobo%20and%20Gloria%20M.%20Bundutidi%20and%20D%C3%A9sir%C3%A9%20Kulimba%20Mashinda%20and%20Emmanuel%20Kabengele%20Mpinga%20and%20Nathana%C3%ABl%20M.%20Kasoro&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20is%20revolutionizing%20various%20fields%2C%20including%0Apublic%20health%20surveillance.%20In%20Africa%2C%20where%20health%20systems%20frequently%0Aencounter%20challenges%20such%20as%20limited%20resources%2C%20inadequate%20infrastructure%2C%0Afailed%20health%20information%20systems%20and%20a%20shortage%20of%20skilled%20health%0Aprofessionals%2C%20AI%20offers%20a%20transformative%20opportunity.%20This%20paper%20investigates%0Athe%20applications%20of%20AI%20in%20public%20health%20surveillance%20across%20the%20continent%2C%0Apresenting%20successful%20case%20studies%20and%20examining%20the%20benefits%2C%20opportunities%2C%0Aand%20challenges%20of%20implementing%20AI%20technologies%20in%20African%20healthcare%20settings.%0AOur%20paper%20highlights%20AI%27s%20potential%20to%20enhance%20disease%20monitoring%20and%20health%0Aoutcomes%2C%20and%20support%20effective%20public%20health%20interventions.%20The%20findings%0Apresented%20in%20the%20paper%20demonstrate%20that%20AI%20can%20significantly%20improve%20the%0Aaccuracy%20and%20timeliness%20of%20disease%20detection%20and%20prediction%2C%20optimize%20resource%0Aallocation%2C%20and%20facilitate%20targeted%20public%20health%20strategies.%20Additionally%2C%20our%0Apaper%20identified%20key%20barriers%20to%20the%20widespread%20adoption%20of%20AI%20in%20African%0Apublic%20health%20systems%20and%20proposed%20actionable%20recommendations%20to%20overcome%20these%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02575v1&entry.124074799=Read"},
{"title": "Attenuation-adjusted deep learning of pore defects in 2D radiographs of\n  additive manufacturing powders", "author": "Andreas Bjerregaard and David Schumacher and Jon Sporring", "abstract": "  The presence of gas pores in metal feedstock powder for additive\nmanufacturing greatly affects the final AM product. Since current porosity\nanalysis often involves lengthy X-ray computed tomography (XCT) scans with a\nfull rotation around the sample, motivation exists to explore methods that\nallow for high throughput -- possibly enabling in-line porosity analysis during\nmanufacturing. Through labelling pore pixels on single 2D radiographs of\npowders, this work seeks to simulate such future efficient setups. High\nsegmentation accuracy is achieved by combining a model of X-ray attenuation\nthrough particles with a variant of the widely applied UNet architecture;\nnotably, F1-score increases by $11.4\\%$ compared to the baseline UNet. The\nproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)\nmaking tight particle cutouts, and 3) subtracting an ideal particle without\npores generated from a distance map inspired by Lambert-Beers law. This paper\nexplores four image processing methods, where the fastest (yet still\nunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,\nand the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable\nnature, these strategies can be involved in making high throughput porosity\nanalysis of metal feedstock powder for additive manufacturing.\n", "link": "http://arxiv.org/abs/2408.02427v1", "date": "2024-08-05", "relevancy": 1.5135, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5112}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attenuation-adjusted%20deep%20learning%20of%20pore%20defects%20in%202D%20radiographs%20of%0A%20%20additive%20manufacturing%20powders&body=Title%3A%20Attenuation-adjusted%20deep%20learning%20of%20pore%20defects%20in%202D%20radiographs%20of%0A%20%20additive%20manufacturing%20powders%0AAuthor%3A%20Andreas%20Bjerregaard%20and%20David%20Schumacher%20and%20Jon%20Sporring%0AAbstract%3A%20%20%20The%20presence%20of%20gas%20pores%20in%20metal%20feedstock%20powder%20for%20additive%0Amanufacturing%20greatly%20affects%20the%20final%20AM%20product.%20Since%20current%20porosity%0Aanalysis%20often%20involves%20lengthy%20X-ray%20computed%20tomography%20%28XCT%29%20scans%20with%20a%0Afull%20rotation%20around%20the%20sample%2C%20motivation%20exists%20to%20explore%20methods%20that%0Aallow%20for%20high%20throughput%20--%20possibly%20enabling%20in-line%20porosity%20analysis%20during%0Amanufacturing.%20Through%20labelling%20pore%20pixels%20on%20single%202D%20radiographs%20of%0Apowders%2C%20this%20work%20seeks%20to%20simulate%20such%20future%20efficient%20setups.%20High%0Asegmentation%20accuracy%20is%20achieved%20by%20combining%20a%20model%20of%20X-ray%20attenuation%0Athrough%20particles%20with%20a%20variant%20of%20the%20widely%20applied%20UNet%20architecture%3B%0Anotably%2C%20F1-score%20increases%20by%20%2411.4%5C%25%24%20compared%20to%20the%20baseline%20UNet.%20The%0Aproposed%20pore%20segmentation%20is%20enabled%20by%3A%201%29%20pretraining%20on%20synthetic%20data%2C%202%29%0Amaking%20tight%20particle%20cutouts%2C%20and%203%29%20subtracting%20an%20ideal%20particle%20without%0Apores%20generated%20from%20a%20distance%20map%20inspired%20by%20Lambert-Beers%20law.%20This%20paper%0Aexplores%20four%20image%20processing%20methods%2C%20where%20the%20fastest%20%28yet%20still%0Aunoptimized%29%20segments%20a%20particle%20in%20mean%20%240.014s%24%20time%20with%20F1-score%20%240.78%24%2C%0Aand%20the%20most%20accurate%20in%20%240.291s%24%20with%20F1-score%20%240.87%24.%20Due%20to%20their%20scalable%0Anature%2C%20these%20strategies%20can%20be%20involved%20in%20making%20high%20throughput%20porosity%0Aanalysis%20of%20metal%20feedstock%20powder%20for%20additive%20manufacturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttenuation-adjusted%2520deep%2520learning%2520of%2520pore%2520defects%2520in%25202D%2520radiographs%2520of%250A%2520%2520additive%2520manufacturing%2520powders%26entry.906535625%3DAndreas%2520Bjerregaard%2520and%2520David%2520Schumacher%2520and%2520Jon%2520Sporring%26entry.1292438233%3D%2520%2520The%2520presence%2520of%2520gas%2520pores%2520in%2520metal%2520feedstock%2520powder%2520for%2520additive%250Amanufacturing%2520greatly%2520affects%2520the%2520final%2520AM%2520product.%2520Since%2520current%2520porosity%250Aanalysis%2520often%2520involves%2520lengthy%2520X-ray%2520computed%2520tomography%2520%2528XCT%2529%2520scans%2520with%2520a%250Afull%2520rotation%2520around%2520the%2520sample%252C%2520motivation%2520exists%2520to%2520explore%2520methods%2520that%250Aallow%2520for%2520high%2520throughput%2520--%2520possibly%2520enabling%2520in-line%2520porosity%2520analysis%2520during%250Amanufacturing.%2520Through%2520labelling%2520pore%2520pixels%2520on%2520single%25202D%2520radiographs%2520of%250Apowders%252C%2520this%2520work%2520seeks%2520to%2520simulate%2520such%2520future%2520efficient%2520setups.%2520High%250Asegmentation%2520accuracy%2520is%2520achieved%2520by%2520combining%2520a%2520model%2520of%2520X-ray%2520attenuation%250Athrough%2520particles%2520with%2520a%2520variant%2520of%2520the%2520widely%2520applied%2520UNet%2520architecture%253B%250Anotably%252C%2520F1-score%2520increases%2520by%2520%252411.4%255C%2525%2524%2520compared%2520to%2520the%2520baseline%2520UNet.%2520The%250Aproposed%2520pore%2520segmentation%2520is%2520enabled%2520by%253A%25201%2529%2520pretraining%2520on%2520synthetic%2520data%252C%25202%2529%250Amaking%2520tight%2520particle%2520cutouts%252C%2520and%25203%2529%2520subtracting%2520an%2520ideal%2520particle%2520without%250Apores%2520generated%2520from%2520a%2520distance%2520map%2520inspired%2520by%2520Lambert-Beers%2520law.%2520This%2520paper%250Aexplores%2520four%2520image%2520processing%2520methods%252C%2520where%2520the%2520fastest%2520%2528yet%2520still%250Aunoptimized%2529%2520segments%2520a%2520particle%2520in%2520mean%2520%25240.014s%2524%2520time%2520with%2520F1-score%2520%25240.78%2524%252C%250Aand%2520the%2520most%2520accurate%2520in%2520%25240.291s%2524%2520with%2520F1-score%2520%25240.87%2524.%2520Due%2520to%2520their%2520scalable%250Anature%252C%2520these%2520strategies%2520can%2520be%2520involved%2520in%2520making%2520high%2520throughput%2520porosity%250Aanalysis%2520of%2520metal%2520feedstock%2520powder%2520for%2520additive%2520manufacturing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attenuation-adjusted%20deep%20learning%20of%20pore%20defects%20in%202D%20radiographs%20of%0A%20%20additive%20manufacturing%20powders&entry.906535625=Andreas%20Bjerregaard%20and%20David%20Schumacher%20and%20Jon%20Sporring&entry.1292438233=%20%20The%20presence%20of%20gas%20pores%20in%20metal%20feedstock%20powder%20for%20additive%0Amanufacturing%20greatly%20affects%20the%20final%20AM%20product.%20Since%20current%20porosity%0Aanalysis%20often%20involves%20lengthy%20X-ray%20computed%20tomography%20%28XCT%29%20scans%20with%20a%0Afull%20rotation%20around%20the%20sample%2C%20motivation%20exists%20to%20explore%20methods%20that%0Aallow%20for%20high%20throughput%20--%20possibly%20enabling%20in-line%20porosity%20analysis%20during%0Amanufacturing.%20Through%20labelling%20pore%20pixels%20on%20single%202D%20radiographs%20of%0Apowders%2C%20this%20work%20seeks%20to%20simulate%20such%20future%20efficient%20setups.%20High%0Asegmentation%20accuracy%20is%20achieved%20by%20combining%20a%20model%20of%20X-ray%20attenuation%0Athrough%20particles%20with%20a%20variant%20of%20the%20widely%20applied%20UNet%20architecture%3B%0Anotably%2C%20F1-score%20increases%20by%20%2411.4%5C%25%24%20compared%20to%20the%20baseline%20UNet.%20The%0Aproposed%20pore%20segmentation%20is%20enabled%20by%3A%201%29%20pretraining%20on%20synthetic%20data%2C%202%29%0Amaking%20tight%20particle%20cutouts%2C%20and%203%29%20subtracting%20an%20ideal%20particle%20without%0Apores%20generated%20from%20a%20distance%20map%20inspired%20by%20Lambert-Beers%20law.%20This%20paper%0Aexplores%20four%20image%20processing%20methods%2C%20where%20the%20fastest%20%28yet%20still%0Aunoptimized%29%20segments%20a%20particle%20in%20mean%20%240.014s%24%20time%20with%20F1-score%20%240.78%24%2C%0Aand%20the%20most%20accurate%20in%20%240.291s%24%20with%20F1-score%20%240.87%24.%20Due%20to%20their%20scalable%0Anature%2C%20these%20strategies%20can%20be%20involved%20in%20making%20high%20throughput%20porosity%0Aanalysis%20of%20metal%20feedstock%20powder%20for%20additive%20manufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02427v1&entry.124074799=Read"},
{"title": "AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU\n  Student Stopout", "author": "Yan Zhao and Amy Otteson", "abstract": "  Not everyone who enrolls in college will leave with a certificate or degree,\nbut the number of people who drop out or take a break is much higher than\nexperts previously believed. In December 2013, there were 29 million people\nwith some college education but no degree. That number jumped to 36 million by\nDecember of 2018, according to a new report from the National Student\nClearinghouse Research Center[1]. It is imperative to understand the underlying\nfactors contributing to student withdrawal and to assist decision-makers to\nidentify effective strategies to prevent it. By analyzing the characteristics\nand educational pathways of the stopout student population, our aim is to\nprovide actionable insights that can benefit institutions facing similar\nchallenges. Eastern Michigan University (EMU) faces significant challenges in\nstudent retention, with approximately 55% of its undergraduate students not\ncompleting their degrees within six years. As an institution committed to\nstudent success, EMU conducted a comprehensive study of student withdrawals to\nunderstand the influencing factors. And the paper revealed a high correlation\nbetween certain factors and withdrawals, even in the early stages of university\nattendance. Based on these findings, we developed a predictive model that\nemploys artificial intelligence techniques to assess the potential risk that\nstudents abandon their studies. These models enable universities to implement\nearly intervention strategies, support at-risk students, and improve overall\nhigher education success.\n", "link": "http://arxiv.org/abs/2408.02598v1", "date": "2024-08-05", "relevancy": 1.4401, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3703}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3615}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Driven%20Strategies%20for%20Reducing%20Student%20Withdrawal%20--%20A%20Study%20of%20EMU%0A%20%20Student%20Stopout&body=Title%3A%20AI-Driven%20Strategies%20for%20Reducing%20Student%20Withdrawal%20--%20A%20Study%20of%20EMU%0A%20%20Student%20Stopout%0AAuthor%3A%20Yan%20Zhao%20and%20Amy%20Otteson%0AAbstract%3A%20%20%20Not%20everyone%20who%20enrolls%20in%20college%20will%20leave%20with%20a%20certificate%20or%20degree%2C%0Abut%20the%20number%20of%20people%20who%20drop%20out%20or%20take%20a%20break%20is%20much%20higher%20than%0Aexperts%20previously%20believed.%20In%20December%202013%2C%20there%20were%2029%20million%20people%0Awith%20some%20college%20education%20but%20no%20degree.%20That%20number%20jumped%20to%2036%20million%20by%0ADecember%20of%202018%2C%20according%20to%20a%20new%20report%20from%20the%20National%20Student%0AClearinghouse%20Research%20Center%5B1%5D.%20It%20is%20imperative%20to%20understand%20the%20underlying%0Afactors%20contributing%20to%20student%20withdrawal%20and%20to%20assist%20decision-makers%20to%0Aidentify%20effective%20strategies%20to%20prevent%20it.%20By%20analyzing%20the%20characteristics%0Aand%20educational%20pathways%20of%20the%20stopout%20student%20population%2C%20our%20aim%20is%20to%0Aprovide%20actionable%20insights%20that%20can%20benefit%20institutions%20facing%20similar%0Achallenges.%20Eastern%20Michigan%20University%20%28EMU%29%20faces%20significant%20challenges%20in%0Astudent%20retention%2C%20with%20approximately%2055%25%20of%20its%20undergraduate%20students%20not%0Acompleting%20their%20degrees%20within%20six%20years.%20As%20an%20institution%20committed%20to%0Astudent%20success%2C%20EMU%20conducted%20a%20comprehensive%20study%20of%20student%20withdrawals%20to%0Aunderstand%20the%20influencing%20factors.%20And%20the%20paper%20revealed%20a%20high%20correlation%0Abetween%20certain%20factors%20and%20withdrawals%2C%20even%20in%20the%20early%20stages%20of%20university%0Aattendance.%20Based%20on%20these%20findings%2C%20we%20developed%20a%20predictive%20model%20that%0Aemploys%20artificial%20intelligence%20techniques%20to%20assess%20the%20potential%20risk%20that%0Astudents%20abandon%20their%20studies.%20These%20models%20enable%20universities%20to%20implement%0Aearly%20intervention%20strategies%2C%20support%20at-risk%20students%2C%20and%20improve%20overall%0Ahigher%20education%20success.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Driven%2520Strategies%2520for%2520Reducing%2520Student%2520Withdrawal%2520--%2520A%2520Study%2520of%2520EMU%250A%2520%2520Student%2520Stopout%26entry.906535625%3DYan%2520Zhao%2520and%2520Amy%2520Otteson%26entry.1292438233%3D%2520%2520Not%2520everyone%2520who%2520enrolls%2520in%2520college%2520will%2520leave%2520with%2520a%2520certificate%2520or%2520degree%252C%250Abut%2520the%2520number%2520of%2520people%2520who%2520drop%2520out%2520or%2520take%2520a%2520break%2520is%2520much%2520higher%2520than%250Aexperts%2520previously%2520believed.%2520In%2520December%25202013%252C%2520there%2520were%252029%2520million%2520people%250Awith%2520some%2520college%2520education%2520but%2520no%2520degree.%2520That%2520number%2520jumped%2520to%252036%2520million%2520by%250ADecember%2520of%25202018%252C%2520according%2520to%2520a%2520new%2520report%2520from%2520the%2520National%2520Student%250AClearinghouse%2520Research%2520Center%255B1%255D.%2520It%2520is%2520imperative%2520to%2520understand%2520the%2520underlying%250Afactors%2520contributing%2520to%2520student%2520withdrawal%2520and%2520to%2520assist%2520decision-makers%2520to%250Aidentify%2520effective%2520strategies%2520to%2520prevent%2520it.%2520By%2520analyzing%2520the%2520characteristics%250Aand%2520educational%2520pathways%2520of%2520the%2520stopout%2520student%2520population%252C%2520our%2520aim%2520is%2520to%250Aprovide%2520actionable%2520insights%2520that%2520can%2520benefit%2520institutions%2520facing%2520similar%250Achallenges.%2520Eastern%2520Michigan%2520University%2520%2528EMU%2529%2520faces%2520significant%2520challenges%2520in%250Astudent%2520retention%252C%2520with%2520approximately%252055%2525%2520of%2520its%2520undergraduate%2520students%2520not%250Acompleting%2520their%2520degrees%2520within%2520six%2520years.%2520As%2520an%2520institution%2520committed%2520to%250Astudent%2520success%252C%2520EMU%2520conducted%2520a%2520comprehensive%2520study%2520of%2520student%2520withdrawals%2520to%250Aunderstand%2520the%2520influencing%2520factors.%2520And%2520the%2520paper%2520revealed%2520a%2520high%2520correlation%250Abetween%2520certain%2520factors%2520and%2520withdrawals%252C%2520even%2520in%2520the%2520early%2520stages%2520of%2520university%250Aattendance.%2520Based%2520on%2520these%2520findings%252C%2520we%2520developed%2520a%2520predictive%2520model%2520that%250Aemploys%2520artificial%2520intelligence%2520techniques%2520to%2520assess%2520the%2520potential%2520risk%2520that%250Astudents%2520abandon%2520their%2520studies.%2520These%2520models%2520enable%2520universities%2520to%2520implement%250Aearly%2520intervention%2520strategies%252C%2520support%2520at-risk%2520students%252C%2520and%2520improve%2520overall%250Ahigher%2520education%2520success.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Driven%20Strategies%20for%20Reducing%20Student%20Withdrawal%20--%20A%20Study%20of%20EMU%0A%20%20Student%20Stopout&entry.906535625=Yan%20Zhao%20and%20Amy%20Otteson&entry.1292438233=%20%20Not%20everyone%20who%20enrolls%20in%20college%20will%20leave%20with%20a%20certificate%20or%20degree%2C%0Abut%20the%20number%20of%20people%20who%20drop%20out%20or%20take%20a%20break%20is%20much%20higher%20than%0Aexperts%20previously%20believed.%20In%20December%202013%2C%20there%20were%2029%20million%20people%0Awith%20some%20college%20education%20but%20no%20degree.%20That%20number%20jumped%20to%2036%20million%20by%0ADecember%20of%202018%2C%20according%20to%20a%20new%20report%20from%20the%20National%20Student%0AClearinghouse%20Research%20Center%5B1%5D.%20It%20is%20imperative%20to%20understand%20the%20underlying%0Afactors%20contributing%20to%20student%20withdrawal%20and%20to%20assist%20decision-makers%20to%0Aidentify%20effective%20strategies%20to%20prevent%20it.%20By%20analyzing%20the%20characteristics%0Aand%20educational%20pathways%20of%20the%20stopout%20student%20population%2C%20our%20aim%20is%20to%0Aprovide%20actionable%20insights%20that%20can%20benefit%20institutions%20facing%20similar%0Achallenges.%20Eastern%20Michigan%20University%20%28EMU%29%20faces%20significant%20challenges%20in%0Astudent%20retention%2C%20with%20approximately%2055%25%20of%20its%20undergraduate%20students%20not%0Acompleting%20their%20degrees%20within%20six%20years.%20As%20an%20institution%20committed%20to%0Astudent%20success%2C%20EMU%20conducted%20a%20comprehensive%20study%20of%20student%20withdrawals%20to%0Aunderstand%20the%20influencing%20factors.%20And%20the%20paper%20revealed%20a%20high%20correlation%0Abetween%20certain%20factors%20and%20withdrawals%2C%20even%20in%20the%20early%20stages%20of%20university%0Aattendance.%20Based%20on%20these%20findings%2C%20we%20developed%20a%20predictive%20model%20that%0Aemploys%20artificial%20intelligence%20techniques%20to%20assess%20the%20potential%20risk%20that%0Astudents%20abandon%20their%20studies.%20These%20models%20enable%20universities%20to%20implement%0Aearly%20intervention%20strategies%2C%20support%20at-risk%20students%2C%20and%20improve%20overall%0Ahigher%20education%20success.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02598v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


