<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250908.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level\n  Guidance in Large Scenes", "author": "Shengkai Zhang and Yuhe Liu and Guanjun Wu and Jianhua He and Xinggang Wang and Mozi Chen and Kezhong Liu", "abstract": "  VIM-GS is a Gaussian Splatting (GS) framework using monocular images for\nnovel-view synthesis (NVS) in large scenes. GS typically requires accurate\ndepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited\ndepth sensing range makes it difficult for GS to work in large scenes.\nMonocular images, however, lack depth to guide the learning and lead to\ninferior NVS results. Although large foundation models (LFMs) for monocular\ndepth estimation are available, they suffer from cross-frame inconsistency,\ninaccuracy for distant scenes, and ambiguity in deceptive texture cues. This\npaper aims to generate dense, accurate depth images from monocular RGB inputs\nfor high-definite GS rendering. The key idea is to leverage the accurate but\nsparse depth from visual-inertial Structure-from-Motion (SfM) to refine the\ndense but coarse depth from LFMs. To bridge the sparse input and dense output,\nwe propose an object-segmented depth propagation algorithm that renders the\ndepth of pixels of structured objects. Then we develop a dynamic depth\nrefinement module to handle the crippled SfM depth of dynamic objects and\nrefine the coarse LFM depth. Experiments using public and customized datasets\ndemonstrate the superior rendering quality of VIM-GS in large scenes.\n", "link": "http://arxiv.org/abs/2509.06685v1", "date": "2025-09-08", "relevancy": 3.4215, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7102}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6728}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIM-GS%3A%20Visual-Inertial%20Monocular%20Gaussian%20Splatting%20via%20Object-level%0A%20%20Guidance%20in%20Large%20Scenes&body=Title%3A%20VIM-GS%3A%20Visual-Inertial%20Monocular%20Gaussian%20Splatting%20via%20Object-level%0A%20%20Guidance%20in%20Large%20Scenes%0AAuthor%3A%20Shengkai%20Zhang%20and%20Yuhe%20Liu%20and%20Guanjun%20Wu%20and%20Jianhua%20He%20and%20Xinggang%20Wang%20and%20Mozi%20Chen%20and%20Kezhong%20Liu%0AAbstract%3A%20%20%20VIM-GS%20is%20a%20Gaussian%20Splatting%20%28GS%29%20framework%20using%20monocular%20images%20for%0Anovel-view%20synthesis%20%28NVS%29%20in%20large%20scenes.%20GS%20typically%20requires%20accurate%0Adepth%20to%20initiate%20Gaussian%20ellipsoids%20using%20RGB-D/stereo%20cameras.%20Their%20limited%0Adepth%20sensing%20range%20makes%20it%20difficult%20for%20GS%20to%20work%20in%20large%20scenes.%0AMonocular%20images%2C%20however%2C%20lack%20depth%20to%20guide%20the%20learning%20and%20lead%20to%0Ainferior%20NVS%20results.%20Although%20large%20foundation%20models%20%28LFMs%29%20for%20monocular%0Adepth%20estimation%20are%20available%2C%20they%20suffer%20from%20cross-frame%20inconsistency%2C%0Ainaccuracy%20for%20distant%20scenes%2C%20and%20ambiguity%20in%20deceptive%20texture%20cues.%20This%0Apaper%20aims%20to%20generate%20dense%2C%20accurate%20depth%20images%20from%20monocular%20RGB%20inputs%0Afor%20high-definite%20GS%20rendering.%20The%20key%20idea%20is%20to%20leverage%20the%20accurate%20but%0Asparse%20depth%20from%20visual-inertial%20Structure-from-Motion%20%28SfM%29%20to%20refine%20the%0Adense%20but%20coarse%20depth%20from%20LFMs.%20To%20bridge%20the%20sparse%20input%20and%20dense%20output%2C%0Awe%20propose%20an%20object-segmented%20depth%20propagation%20algorithm%20that%20renders%20the%0Adepth%20of%20pixels%20of%20structured%20objects.%20Then%20we%20develop%20a%20dynamic%20depth%0Arefinement%20module%20to%20handle%20the%20crippled%20SfM%20depth%20of%20dynamic%20objects%20and%0Arefine%20the%20coarse%20LFM%20depth.%20Experiments%20using%20public%20and%20customized%20datasets%0Ademonstrate%20the%20superior%20rendering%20quality%20of%20VIM-GS%20in%20large%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIM-GS%253A%2520Visual-Inertial%2520Monocular%2520Gaussian%2520Splatting%2520via%2520Object-level%250A%2520%2520Guidance%2520in%2520Large%2520Scenes%26entry.906535625%3DShengkai%2520Zhang%2520and%2520Yuhe%2520Liu%2520and%2520Guanjun%2520Wu%2520and%2520Jianhua%2520He%2520and%2520Xinggang%2520Wang%2520and%2520Mozi%2520Chen%2520and%2520Kezhong%2520Liu%26entry.1292438233%3D%2520%2520VIM-GS%2520is%2520a%2520Gaussian%2520Splatting%2520%2528GS%2529%2520framework%2520using%2520monocular%2520images%2520for%250Anovel-view%2520synthesis%2520%2528NVS%2529%2520in%2520large%2520scenes.%2520GS%2520typically%2520requires%2520accurate%250Adepth%2520to%2520initiate%2520Gaussian%2520ellipsoids%2520using%2520RGB-D/stereo%2520cameras.%2520Their%2520limited%250Adepth%2520sensing%2520range%2520makes%2520it%2520difficult%2520for%2520GS%2520to%2520work%2520in%2520large%2520scenes.%250AMonocular%2520images%252C%2520however%252C%2520lack%2520depth%2520to%2520guide%2520the%2520learning%2520and%2520lead%2520to%250Ainferior%2520NVS%2520results.%2520Although%2520large%2520foundation%2520models%2520%2528LFMs%2529%2520for%2520monocular%250Adepth%2520estimation%2520are%2520available%252C%2520they%2520suffer%2520from%2520cross-frame%2520inconsistency%252C%250Ainaccuracy%2520for%2520distant%2520scenes%252C%2520and%2520ambiguity%2520in%2520deceptive%2520texture%2520cues.%2520This%250Apaper%2520aims%2520to%2520generate%2520dense%252C%2520accurate%2520depth%2520images%2520from%2520monocular%2520RGB%2520inputs%250Afor%2520high-definite%2520GS%2520rendering.%2520The%2520key%2520idea%2520is%2520to%2520leverage%2520the%2520accurate%2520but%250Asparse%2520depth%2520from%2520visual-inertial%2520Structure-from-Motion%2520%2528SfM%2529%2520to%2520refine%2520the%250Adense%2520but%2520coarse%2520depth%2520from%2520LFMs.%2520To%2520bridge%2520the%2520sparse%2520input%2520and%2520dense%2520output%252C%250Awe%2520propose%2520an%2520object-segmented%2520depth%2520propagation%2520algorithm%2520that%2520renders%2520the%250Adepth%2520of%2520pixels%2520of%2520structured%2520objects.%2520Then%2520we%2520develop%2520a%2520dynamic%2520depth%250Arefinement%2520module%2520to%2520handle%2520the%2520crippled%2520SfM%2520depth%2520of%2520dynamic%2520objects%2520and%250Arefine%2520the%2520coarse%2520LFM%2520depth.%2520Experiments%2520using%2520public%2520and%2520customized%2520datasets%250Ademonstrate%2520the%2520superior%2520rendering%2520quality%2520of%2520VIM-GS%2520in%2520large%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIM-GS%3A%20Visual-Inertial%20Monocular%20Gaussian%20Splatting%20via%20Object-level%0A%20%20Guidance%20in%20Large%20Scenes&entry.906535625=Shengkai%20Zhang%20and%20Yuhe%20Liu%20and%20Guanjun%20Wu%20and%20Jianhua%20He%20and%20Xinggang%20Wang%20and%20Mozi%20Chen%20and%20Kezhong%20Liu&entry.1292438233=%20%20VIM-GS%20is%20a%20Gaussian%20Splatting%20%28GS%29%20framework%20using%20monocular%20images%20for%0Anovel-view%20synthesis%20%28NVS%29%20in%20large%20scenes.%20GS%20typically%20requires%20accurate%0Adepth%20to%20initiate%20Gaussian%20ellipsoids%20using%20RGB-D/stereo%20cameras.%20Their%20limited%0Adepth%20sensing%20range%20makes%20it%20difficult%20for%20GS%20to%20work%20in%20large%20scenes.%0AMonocular%20images%2C%20however%2C%20lack%20depth%20to%20guide%20the%20learning%20and%20lead%20to%0Ainferior%20NVS%20results.%20Although%20large%20foundation%20models%20%28LFMs%29%20for%20monocular%0Adepth%20estimation%20are%20available%2C%20they%20suffer%20from%20cross-frame%20inconsistency%2C%0Ainaccuracy%20for%20distant%20scenes%2C%20and%20ambiguity%20in%20deceptive%20texture%20cues.%20This%0Apaper%20aims%20to%20generate%20dense%2C%20accurate%20depth%20images%20from%20monocular%20RGB%20inputs%0Afor%20high-definite%20GS%20rendering.%20The%20key%20idea%20is%20to%20leverage%20the%20accurate%20but%0Asparse%20depth%20from%20visual-inertial%20Structure-from-Motion%20%28SfM%29%20to%20refine%20the%0Adense%20but%20coarse%20depth%20from%20LFMs.%20To%20bridge%20the%20sparse%20input%20and%20dense%20output%2C%0Awe%20propose%20an%20object-segmented%20depth%20propagation%20algorithm%20that%20renders%20the%0Adepth%20of%20pixels%20of%20structured%20objects.%20Then%20we%20develop%20a%20dynamic%20depth%0Arefinement%20module%20to%20handle%20the%20crippled%20SfM%20depth%20of%20dynamic%20objects%20and%0Arefine%20the%20coarse%20LFM%20depth.%20Experiments%20using%20public%20and%20customized%20datasets%0Ademonstrate%20the%20superior%20rendering%20quality%20of%20VIM-GS%20in%20large%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06685v1&entry.124074799=Read"},
{"title": "MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image\n  Analysis", "author": "Daniel Scholz and Ayhan Can Erdur and Viktoria Ehm and Anke Meyer-Baese and Jan C. Peeken and Daniel Rueckert and Benedikt Wiestler", "abstract": "  Vision foundation models like DINOv2 demonstrate remarkable potential in\nmedical imaging despite their origin in natural image domains. However, their\ndesign inherently works best for uni-modal image analysis, limiting their\neffectiveness for multi-modal imaging tasks that are common in many medical\nfields, such as neurology and oncology. While supervised models perform well in\nthis setting, they fail to leverage unlabeled datasets and struggle with\nmissing modalities, a frequent challenge in clinical settings. To bridge these\ngaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the\npre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our\napproach incorporates multi-modal patch embeddings, enabling vision foundation\nmodels to effectively process multi-modal imaging data. To address missing\nmodalities, we employ full-modality masking, which encourages the model to\nlearn robust cross-modality relationships. Furthermore, we leverage\nsemi-supervised learning to harness large unlabeled datasets, enhancing both\nthe accuracy and reliability of medical predictions. Applied to glioma subtype\nclassification from multi-sequence brain MRI, our method achieves a Matthews\nCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassing\nstate-of-the-art supervised approaches by +11.1%. Our work establishes a\nscalable and robust solution for multi-modal medical imaging tasks, leveraging\npowerful vision foundation models pre-trained on natural images while\naddressing real-world clinical challenges such as missing data and limited\nannotations.\n", "link": "http://arxiv.org/abs/2509.06617v1", "date": "2025-09-08", "relevancy": 3.0837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-DINOv2%3A%20Adapting%20Foundation%20Models%20for%20Multi-Modal%20Medical%20Image%0A%20%20Analysis&body=Title%3A%20MM-DINOv2%3A%20Adapting%20Foundation%20Models%20for%20Multi-Modal%20Medical%20Image%0A%20%20Analysis%0AAuthor%3A%20Daniel%20Scholz%20and%20Ayhan%20Can%20Erdur%20and%20Viktoria%20Ehm%20and%20Anke%20Meyer-Baese%20and%20Jan%20C.%20Peeken%20and%20Daniel%20Rueckert%20and%20Benedikt%20Wiestler%0AAbstract%3A%20%20%20Vision%20foundation%20models%20like%20DINOv2%20demonstrate%20remarkable%20potential%20in%0Amedical%20imaging%20despite%20their%20origin%20in%20natural%20image%20domains.%20However%2C%20their%0Adesign%20inherently%20works%20best%20for%20uni-modal%20image%20analysis%2C%20limiting%20their%0Aeffectiveness%20for%20multi-modal%20imaging%20tasks%20that%20are%20common%20in%20many%20medical%0Afields%2C%20such%20as%20neurology%20and%20oncology.%20While%20supervised%20models%20perform%20well%20in%0Athis%20setting%2C%20they%20fail%20to%20leverage%20unlabeled%20datasets%20and%20struggle%20with%0Amissing%20modalities%2C%20a%20frequent%20challenge%20in%20clinical%20settings.%20To%20bridge%20these%0Agaps%2C%20we%20introduce%20MM-DINOv2%2C%20a%20novel%20and%20efficient%20framework%20that%20adapts%20the%0Apre-trained%20vision%20foundation%20model%20DINOv2%20for%20multi-modal%20medical%20imaging.%20Our%0Aapproach%20incorporates%20multi-modal%20patch%20embeddings%2C%20enabling%20vision%20foundation%0Amodels%20to%20effectively%20process%20multi-modal%20imaging%20data.%20To%20address%20missing%0Amodalities%2C%20we%20employ%20full-modality%20masking%2C%20which%20encourages%20the%20model%20to%0Alearn%20robust%20cross-modality%20relationships.%20Furthermore%2C%20we%20leverage%0Asemi-supervised%20learning%20to%20harness%20large%20unlabeled%20datasets%2C%20enhancing%20both%0Athe%20accuracy%20and%20reliability%20of%20medical%20predictions.%20Applied%20to%20glioma%20subtype%0Aclassification%20from%20multi-sequence%20brain%20MRI%2C%20our%20method%20achieves%20a%20Matthews%0ACorrelation%20Coefficient%20%28MCC%29%20of%200.6%20on%20an%20external%20test%20set%2C%20surpassing%0Astate-of-the-art%20supervised%20approaches%20by%20%2B11.1%25.%20Our%20work%20establishes%20a%0Ascalable%20and%20robust%20solution%20for%20multi-modal%20medical%20imaging%20tasks%2C%20leveraging%0Apowerful%20vision%20foundation%20models%20pre-trained%20on%20natural%20images%20while%0Aaddressing%20real-world%20clinical%20challenges%20such%20as%20missing%20data%20and%20limited%0Aannotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-DINOv2%253A%2520Adapting%2520Foundation%2520Models%2520for%2520Multi-Modal%2520Medical%2520Image%250A%2520%2520Analysis%26entry.906535625%3DDaniel%2520Scholz%2520and%2520Ayhan%2520Can%2520Erdur%2520and%2520Viktoria%2520Ehm%2520and%2520Anke%2520Meyer-Baese%2520and%2520Jan%2520C.%2520Peeken%2520and%2520Daniel%2520Rueckert%2520and%2520Benedikt%2520Wiestler%26entry.1292438233%3D%2520%2520Vision%2520foundation%2520models%2520like%2520DINOv2%2520demonstrate%2520remarkable%2520potential%2520in%250Amedical%2520imaging%2520despite%2520their%2520origin%2520in%2520natural%2520image%2520domains.%2520However%252C%2520their%250Adesign%2520inherently%2520works%2520best%2520for%2520uni-modal%2520image%2520analysis%252C%2520limiting%2520their%250Aeffectiveness%2520for%2520multi-modal%2520imaging%2520tasks%2520that%2520are%2520common%2520in%2520many%2520medical%250Afields%252C%2520such%2520as%2520neurology%2520and%2520oncology.%2520While%2520supervised%2520models%2520perform%2520well%2520in%250Athis%2520setting%252C%2520they%2520fail%2520to%2520leverage%2520unlabeled%2520datasets%2520and%2520struggle%2520with%250Amissing%2520modalities%252C%2520a%2520frequent%2520challenge%2520in%2520clinical%2520settings.%2520To%2520bridge%2520these%250Agaps%252C%2520we%2520introduce%2520MM-DINOv2%252C%2520a%2520novel%2520and%2520efficient%2520framework%2520that%2520adapts%2520the%250Apre-trained%2520vision%2520foundation%2520model%2520DINOv2%2520for%2520multi-modal%2520medical%2520imaging.%2520Our%250Aapproach%2520incorporates%2520multi-modal%2520patch%2520embeddings%252C%2520enabling%2520vision%2520foundation%250Amodels%2520to%2520effectively%2520process%2520multi-modal%2520imaging%2520data.%2520To%2520address%2520missing%250Amodalities%252C%2520we%2520employ%2520full-modality%2520masking%252C%2520which%2520encourages%2520the%2520model%2520to%250Alearn%2520robust%2520cross-modality%2520relationships.%2520Furthermore%252C%2520we%2520leverage%250Asemi-supervised%2520learning%2520to%2520harness%2520large%2520unlabeled%2520datasets%252C%2520enhancing%2520both%250Athe%2520accuracy%2520and%2520reliability%2520of%2520medical%2520predictions.%2520Applied%2520to%2520glioma%2520subtype%250Aclassification%2520from%2520multi-sequence%2520brain%2520MRI%252C%2520our%2520method%2520achieves%2520a%2520Matthews%250ACorrelation%2520Coefficient%2520%2528MCC%2529%2520of%25200.6%2520on%2520an%2520external%2520test%2520set%252C%2520surpassing%250Astate-of-the-art%2520supervised%2520approaches%2520by%2520%252B11.1%2525.%2520Our%2520work%2520establishes%2520a%250Ascalable%2520and%2520robust%2520solution%2520for%2520multi-modal%2520medical%2520imaging%2520tasks%252C%2520leveraging%250Apowerful%2520vision%2520foundation%2520models%2520pre-trained%2520on%2520natural%2520images%2520while%250Aaddressing%2520real-world%2520clinical%2520challenges%2520such%2520as%2520missing%2520data%2520and%2520limited%250Aannotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-DINOv2%3A%20Adapting%20Foundation%20Models%20for%20Multi-Modal%20Medical%20Image%0A%20%20Analysis&entry.906535625=Daniel%20Scholz%20and%20Ayhan%20Can%20Erdur%20and%20Viktoria%20Ehm%20and%20Anke%20Meyer-Baese%20and%20Jan%20C.%20Peeken%20and%20Daniel%20Rueckert%20and%20Benedikt%20Wiestler&entry.1292438233=%20%20Vision%20foundation%20models%20like%20DINOv2%20demonstrate%20remarkable%20potential%20in%0Amedical%20imaging%20despite%20their%20origin%20in%20natural%20image%20domains.%20However%2C%20their%0Adesign%20inherently%20works%20best%20for%20uni-modal%20image%20analysis%2C%20limiting%20their%0Aeffectiveness%20for%20multi-modal%20imaging%20tasks%20that%20are%20common%20in%20many%20medical%0Afields%2C%20such%20as%20neurology%20and%20oncology.%20While%20supervised%20models%20perform%20well%20in%0Athis%20setting%2C%20they%20fail%20to%20leverage%20unlabeled%20datasets%20and%20struggle%20with%0Amissing%20modalities%2C%20a%20frequent%20challenge%20in%20clinical%20settings.%20To%20bridge%20these%0Agaps%2C%20we%20introduce%20MM-DINOv2%2C%20a%20novel%20and%20efficient%20framework%20that%20adapts%20the%0Apre-trained%20vision%20foundation%20model%20DINOv2%20for%20multi-modal%20medical%20imaging.%20Our%0Aapproach%20incorporates%20multi-modal%20patch%20embeddings%2C%20enabling%20vision%20foundation%0Amodels%20to%20effectively%20process%20multi-modal%20imaging%20data.%20To%20address%20missing%0Amodalities%2C%20we%20employ%20full-modality%20masking%2C%20which%20encourages%20the%20model%20to%0Alearn%20robust%20cross-modality%20relationships.%20Furthermore%2C%20we%20leverage%0Asemi-supervised%20learning%20to%20harness%20large%20unlabeled%20datasets%2C%20enhancing%20both%0Athe%20accuracy%20and%20reliability%20of%20medical%20predictions.%20Applied%20to%20glioma%20subtype%0Aclassification%20from%20multi-sequence%20brain%20MRI%2C%20our%20method%20achieves%20a%20Matthews%0ACorrelation%20Coefficient%20%28MCC%29%20of%200.6%20on%20an%20external%20test%20set%2C%20surpassing%0Astate-of-the-art%20supervised%20approaches%20by%20%2B11.1%25.%20Our%20work%20establishes%20a%0Ascalable%20and%20robust%20solution%20for%20multi-modal%20medical%20imaging%20tasks%2C%20leveraging%0Apowerful%20vision%20foundation%20models%20pre-trained%20on%20natural%20images%20while%0Aaddressing%20real-world%20clinical%20challenges%20such%20as%20missing%20data%20and%20limited%0Aannotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06617v1&entry.124074799=Read"},
{"title": "Integrating Spatial and Semantic Embeddings for Stereo Sound Event\n  Localization in Videos", "author": "Davide Berghi and Philip J. B. Jackson", "abstract": "  In this study, we address the multimodal task of stereo sound event\nlocalization and detection with source distance estimation (3D SELD) in regular\nvideo content. 3D SELD is a complex task that combines temporal event\nclassification with spatial localization, requiring reasoning across spatial,\ntemporal, and semantic dimensions. The last is arguably the most challenging to\nmodel. Traditional SELD approaches typically rely on multichannel input,\nlimiting their capacity to benefit from large-scale pre-training due to data\nconstraints. To overcome this, we enhance a standard SELD architecture with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. We perform an ablation study on\nthe development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the\nindividual contributions of the language-aligned models and benchmark against\nthe DCASE Task 3 baseline systems. Additionally, we detail the curation process\nof large synthetic audio and audio-visual datasets used for model pre-training.\nThese datasets were further expanded through left-right channel swapping\naugmentation. Our approach, combining extensive pre-training, model ensembling,\nand visual post-processing, achieved second rank in the DCASE 2025 Challenge\nTask 3 (Track B), underscoring the effectiveness of our method. Future work\nwill explore the modality-specific contributions and architectural refinements.\n", "link": "http://arxiv.org/abs/2509.06598v1", "date": "2025-09-08", "relevancy": 3.0075, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Spatial%20and%20Semantic%20Embeddings%20for%20Stereo%20Sound%20Event%0A%20%20Localization%20in%20Videos&body=Title%3A%20Integrating%20Spatial%20and%20Semantic%20Embeddings%20for%20Stereo%20Sound%20Event%0A%20%20Localization%20in%20Videos%0AAuthor%3A%20Davide%20Berghi%20and%20Philip%20J.%20B.%20Jackson%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20address%20the%20multimodal%20task%20of%20stereo%20sound%20event%0Alocalization%20and%20detection%20with%20source%20distance%20estimation%20%283D%20SELD%29%20in%20regular%0Avideo%20content.%203D%20SELD%20is%20a%20complex%20task%20that%20combines%20temporal%20event%0Aclassification%20with%20spatial%20localization%2C%20requiring%20reasoning%20across%20spatial%2C%0Atemporal%2C%20and%20semantic%20dimensions.%20The%20last%20is%20arguably%20the%20most%20challenging%20to%0Amodel.%20Traditional%20SELD%20approaches%20typically%20rely%20on%20multichannel%20input%2C%0Alimiting%20their%20capacity%20to%20benefit%20from%20large-scale%20pre-training%20due%20to%20data%0Aconstraints.%20To%20overcome%20this%2C%20we%20enhance%20a%20standard%20SELD%20architecture%20with%0Asemantic%20information%20by%20integrating%20pre-trained%2C%20contrastive%20language-aligned%0Amodels%3A%20CLAP%20for%20audio%20and%20OWL-ViT%20for%20visual%20inputs.%20These%20embeddings%20are%0Aincorporated%20into%20a%20modified%20Conformer%20module%20tailored%20for%20multimodal%20fusion%2C%0Awhich%20we%20refer%20to%20as%20the%20Cross-Modal%20Conformer.%20We%20perform%20an%20ablation%20study%20on%0Athe%20development%20set%20of%20the%20DCASE2025%20Task3%20Stereo%20SELD%20Dataset%20to%20assess%20the%0Aindividual%20contributions%20of%20the%20language-aligned%20models%20and%20benchmark%20against%0Athe%20DCASE%20Task%203%20baseline%20systems.%20Additionally%2C%20we%20detail%20the%20curation%20process%0Aof%20large%20synthetic%20audio%20and%20audio-visual%20datasets%20used%20for%20model%20pre-training.%0AThese%20datasets%20were%20further%20expanded%20through%20left-right%20channel%20swapping%0Aaugmentation.%20Our%20approach%2C%20combining%20extensive%20pre-training%2C%20model%20ensembling%2C%0Aand%20visual%20post-processing%2C%20achieved%20second%20rank%20in%20the%20DCASE%202025%20Challenge%0ATask%203%20%28Track%20B%29%2C%20underscoring%20the%20effectiveness%20of%20our%20method.%20Future%20work%0Awill%20explore%20the%20modality-specific%20contributions%20and%20architectural%20refinements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Spatial%2520and%2520Semantic%2520Embeddings%2520for%2520Stereo%2520Sound%2520Event%250A%2520%2520Localization%2520in%2520Videos%26entry.906535625%3DDavide%2520Berghi%2520and%2520Philip%2520J.%2520B.%2520Jackson%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520multimodal%2520task%2520of%2520stereo%2520sound%2520event%250Alocalization%2520and%2520detection%2520with%2520source%2520distance%2520estimation%2520%25283D%2520SELD%2529%2520in%2520regular%250Avideo%2520content.%25203D%2520SELD%2520is%2520a%2520complex%2520task%2520that%2520combines%2520temporal%2520event%250Aclassification%2520with%2520spatial%2520localization%252C%2520requiring%2520reasoning%2520across%2520spatial%252C%250Atemporal%252C%2520and%2520semantic%2520dimensions.%2520The%2520last%2520is%2520arguably%2520the%2520most%2520challenging%2520to%250Amodel.%2520Traditional%2520SELD%2520approaches%2520typically%2520rely%2520on%2520multichannel%2520input%252C%250Alimiting%2520their%2520capacity%2520to%2520benefit%2520from%2520large-scale%2520pre-training%2520due%2520to%2520data%250Aconstraints.%2520To%2520overcome%2520this%252C%2520we%2520enhance%2520a%2520standard%2520SELD%2520architecture%2520with%250Asemantic%2520information%2520by%2520integrating%2520pre-trained%252C%2520contrastive%2520language-aligned%250Amodels%253A%2520CLAP%2520for%2520audio%2520and%2520OWL-ViT%2520for%2520visual%2520inputs.%2520These%2520embeddings%2520are%250Aincorporated%2520into%2520a%2520modified%2520Conformer%2520module%2520tailored%2520for%2520multimodal%2520fusion%252C%250Awhich%2520we%2520refer%2520to%2520as%2520the%2520Cross-Modal%2520Conformer.%2520We%2520perform%2520an%2520ablation%2520study%2520on%250Athe%2520development%2520set%2520of%2520the%2520DCASE2025%2520Task3%2520Stereo%2520SELD%2520Dataset%2520to%2520assess%2520the%250Aindividual%2520contributions%2520of%2520the%2520language-aligned%2520models%2520and%2520benchmark%2520against%250Athe%2520DCASE%2520Task%25203%2520baseline%2520systems.%2520Additionally%252C%2520we%2520detail%2520the%2520curation%2520process%250Aof%2520large%2520synthetic%2520audio%2520and%2520audio-visual%2520datasets%2520used%2520for%2520model%2520pre-training.%250AThese%2520datasets%2520were%2520further%2520expanded%2520through%2520left-right%2520channel%2520swapping%250Aaugmentation.%2520Our%2520approach%252C%2520combining%2520extensive%2520pre-training%252C%2520model%2520ensembling%252C%250Aand%2520visual%2520post-processing%252C%2520achieved%2520second%2520rank%2520in%2520the%2520DCASE%25202025%2520Challenge%250ATask%25203%2520%2528Track%2520B%2529%252C%2520underscoring%2520the%2520effectiveness%2520of%2520our%2520method.%2520Future%2520work%250Awill%2520explore%2520the%2520modality-specific%2520contributions%2520and%2520architectural%2520refinements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Spatial%20and%20Semantic%20Embeddings%20for%20Stereo%20Sound%20Event%0A%20%20Localization%20in%20Videos&entry.906535625=Davide%20Berghi%20and%20Philip%20J.%20B.%20Jackson&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20the%20multimodal%20task%20of%20stereo%20sound%20event%0Alocalization%20and%20detection%20with%20source%20distance%20estimation%20%283D%20SELD%29%20in%20regular%0Avideo%20content.%203D%20SELD%20is%20a%20complex%20task%20that%20combines%20temporal%20event%0Aclassification%20with%20spatial%20localization%2C%20requiring%20reasoning%20across%20spatial%2C%0Atemporal%2C%20and%20semantic%20dimensions.%20The%20last%20is%20arguably%20the%20most%20challenging%20to%0Amodel.%20Traditional%20SELD%20approaches%20typically%20rely%20on%20multichannel%20input%2C%0Alimiting%20their%20capacity%20to%20benefit%20from%20large-scale%20pre-training%20due%20to%20data%0Aconstraints.%20To%20overcome%20this%2C%20we%20enhance%20a%20standard%20SELD%20architecture%20with%0Asemantic%20information%20by%20integrating%20pre-trained%2C%20contrastive%20language-aligned%0Amodels%3A%20CLAP%20for%20audio%20and%20OWL-ViT%20for%20visual%20inputs.%20These%20embeddings%20are%0Aincorporated%20into%20a%20modified%20Conformer%20module%20tailored%20for%20multimodal%20fusion%2C%0Awhich%20we%20refer%20to%20as%20the%20Cross-Modal%20Conformer.%20We%20perform%20an%20ablation%20study%20on%0Athe%20development%20set%20of%20the%20DCASE2025%20Task3%20Stereo%20SELD%20Dataset%20to%20assess%20the%0Aindividual%20contributions%20of%20the%20language-aligned%20models%20and%20benchmark%20against%0Athe%20DCASE%20Task%203%20baseline%20systems.%20Additionally%2C%20we%20detail%20the%20curation%20process%0Aof%20large%20synthetic%20audio%20and%20audio-visual%20datasets%20used%20for%20model%20pre-training.%0AThese%20datasets%20were%20further%20expanded%20through%20left-right%20channel%20swapping%0Aaugmentation.%20Our%20approach%2C%20combining%20extensive%20pre-training%2C%20model%20ensembling%2C%0Aand%20visual%20post-processing%2C%20achieved%20second%20rank%20in%20the%20DCASE%202025%20Challenge%0ATask%203%20%28Track%20B%29%2C%20underscoring%20the%20effectiveness%20of%20our%20method.%20Future%20work%0Awill%20explore%20the%20modality-specific%20contributions%20and%20architectural%20refinements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06598v1&entry.124074799=Read"},
{"title": "What Can We Learn from Harry Potter? An Exploratory Study of Visual\n  Representation Learning from Atypical Videos", "author": "Qiyue Sun and Qiming Huang and Yang Yang and Hongjun Wang and Jianbo Jiao", "abstract": "  Humans usually show exceptional generalisation and discovery ability in the\nopen world, when being shown uncommon new concepts. Whereas most existing\nstudies in the literature focus on common typical data from closed sets,\nopen-world novel discovery is under-explored in videos. In this paper, we are\ninterested in asking: What if atypical unusual videos are exposed in the\nlearning process? To this end, we collect a new video dataset consisting of\nvarious types of unusual atypical data (e.g., sci-fi, animation, etc.). To\nstudy how such atypical data may benefit open-world learning, we feed them into\nthe model training process for representation learning. Focusing on three key\ntasks in open-world learning: out-of-distribution (OOD) detection, novel\ncategory discovery (NCD), and zero-shot action recognition (ZSAR), we found\nthat even straightforward learning approaches with atypical data consistently\nimprove performance across various settings. Furthermore, we found that\nincreasing the categorical diversity of the atypical samples further boosts OOD\ndetection performance. Additionally, in the NCD task, using a smaller yet more\nsemantically diverse set of atypical samples leads to better performance\ncompared to using a larger but more typical dataset. In the ZSAR setting, the\nsemantic diversity of atypical videos helps the model generalise better to\nunseen action classes. These observations in our extensive experimental\nevaluations reveal the benefits of atypical videos for visual representation\nlearning in the open world, together with the newly proposed dataset,\nencouraging further studies in this direction. The project page is at:\nhttps://julysun98.github.io/atypical_dataset.\n", "link": "http://arxiv.org/abs/2508.21770v2", "date": "2025-09-08", "relevancy": 2.9394, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Can%20We%20Learn%20from%20Harry%20Potter%3F%20An%20Exploratory%20Study%20of%20Visual%0A%20%20Representation%20Learning%20from%20Atypical%20Videos&body=Title%3A%20What%20Can%20We%20Learn%20from%20Harry%20Potter%3F%20An%20Exploratory%20Study%20of%20Visual%0A%20%20Representation%20Learning%20from%20Atypical%20Videos%0AAuthor%3A%20Qiyue%20Sun%20and%20Qiming%20Huang%20and%20Yang%20Yang%20and%20Hongjun%20Wang%20and%20Jianbo%20Jiao%0AAbstract%3A%20%20%20Humans%20usually%20show%20exceptional%20generalisation%20and%20discovery%20ability%20in%20the%0Aopen%20world%2C%20when%20being%20shown%20uncommon%20new%20concepts.%20Whereas%20most%20existing%0Astudies%20in%20the%20literature%20focus%20on%20common%20typical%20data%20from%20closed%20sets%2C%0Aopen-world%20novel%20discovery%20is%20under-explored%20in%20videos.%20In%20this%20paper%2C%20we%20are%0Ainterested%20in%20asking%3A%20What%20if%20atypical%20unusual%20videos%20are%20exposed%20in%20the%0Alearning%20process%3F%20To%20this%20end%2C%20we%20collect%20a%20new%20video%20dataset%20consisting%20of%0Avarious%20types%20of%20unusual%20atypical%20data%20%28e.g.%2C%20sci-fi%2C%20animation%2C%20etc.%29.%20To%0Astudy%20how%20such%20atypical%20data%20may%20benefit%20open-world%20learning%2C%20we%20feed%20them%20into%0Athe%20model%20training%20process%20for%20representation%20learning.%20Focusing%20on%20three%20key%0Atasks%20in%20open-world%20learning%3A%20out-of-distribution%20%28OOD%29%20detection%2C%20novel%0Acategory%20discovery%20%28NCD%29%2C%20and%20zero-shot%20action%20recognition%20%28ZSAR%29%2C%20we%20found%0Athat%20even%20straightforward%20learning%20approaches%20with%20atypical%20data%20consistently%0Aimprove%20performance%20across%20various%20settings.%20Furthermore%2C%20we%20found%20that%0Aincreasing%20the%20categorical%20diversity%20of%20the%20atypical%20samples%20further%20boosts%20OOD%0Adetection%20performance.%20Additionally%2C%20in%20the%20NCD%20task%2C%20using%20a%20smaller%20yet%20more%0Asemantically%20diverse%20set%20of%20atypical%20samples%20leads%20to%20better%20performance%0Acompared%20to%20using%20a%20larger%20but%20more%20typical%20dataset.%20In%20the%20ZSAR%20setting%2C%20the%0Asemantic%20diversity%20of%20atypical%20videos%20helps%20the%20model%20generalise%20better%20to%0Aunseen%20action%20classes.%20These%20observations%20in%20our%20extensive%20experimental%0Aevaluations%20reveal%20the%20benefits%20of%20atypical%20videos%20for%20visual%20representation%0Alearning%20in%20the%20open%20world%2C%20together%20with%20the%20newly%20proposed%20dataset%2C%0Aencouraging%20further%20studies%20in%20this%20direction.%20The%20project%20page%20is%20at%3A%0Ahttps%3A//julysun98.github.io/atypical_dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Can%2520We%2520Learn%2520from%2520Harry%2520Potter%253F%2520An%2520Exploratory%2520Study%2520of%2520Visual%250A%2520%2520Representation%2520Learning%2520from%2520Atypical%2520Videos%26entry.906535625%3DQiyue%2520Sun%2520and%2520Qiming%2520Huang%2520and%2520Yang%2520Yang%2520and%2520Hongjun%2520Wang%2520and%2520Jianbo%2520Jiao%26entry.1292438233%3D%2520%2520Humans%2520usually%2520show%2520exceptional%2520generalisation%2520and%2520discovery%2520ability%2520in%2520the%250Aopen%2520world%252C%2520when%2520being%2520shown%2520uncommon%2520new%2520concepts.%2520Whereas%2520most%2520existing%250Astudies%2520in%2520the%2520literature%2520focus%2520on%2520common%2520typical%2520data%2520from%2520closed%2520sets%252C%250Aopen-world%2520novel%2520discovery%2520is%2520under-explored%2520in%2520videos.%2520In%2520this%2520paper%252C%2520we%2520are%250Ainterested%2520in%2520asking%253A%2520What%2520if%2520atypical%2520unusual%2520videos%2520are%2520exposed%2520in%2520the%250Alearning%2520process%253F%2520To%2520this%2520end%252C%2520we%2520collect%2520a%2520new%2520video%2520dataset%2520consisting%2520of%250Avarious%2520types%2520of%2520unusual%2520atypical%2520data%2520%2528e.g.%252C%2520sci-fi%252C%2520animation%252C%2520etc.%2529.%2520To%250Astudy%2520how%2520such%2520atypical%2520data%2520may%2520benefit%2520open-world%2520learning%252C%2520we%2520feed%2520them%2520into%250Athe%2520model%2520training%2520process%2520for%2520representation%2520learning.%2520Focusing%2520on%2520three%2520key%250Atasks%2520in%2520open-world%2520learning%253A%2520out-of-distribution%2520%2528OOD%2529%2520detection%252C%2520novel%250Acategory%2520discovery%2520%2528NCD%2529%252C%2520and%2520zero-shot%2520action%2520recognition%2520%2528ZSAR%2529%252C%2520we%2520found%250Athat%2520even%2520straightforward%2520learning%2520approaches%2520with%2520atypical%2520data%2520consistently%250Aimprove%2520performance%2520across%2520various%2520settings.%2520Furthermore%252C%2520we%2520found%2520that%250Aincreasing%2520the%2520categorical%2520diversity%2520of%2520the%2520atypical%2520samples%2520further%2520boosts%2520OOD%250Adetection%2520performance.%2520Additionally%252C%2520in%2520the%2520NCD%2520task%252C%2520using%2520a%2520smaller%2520yet%2520more%250Asemantically%2520diverse%2520set%2520of%2520atypical%2520samples%2520leads%2520to%2520better%2520performance%250Acompared%2520to%2520using%2520a%2520larger%2520but%2520more%2520typical%2520dataset.%2520In%2520the%2520ZSAR%2520setting%252C%2520the%250Asemantic%2520diversity%2520of%2520atypical%2520videos%2520helps%2520the%2520model%2520generalise%2520better%2520to%250Aunseen%2520action%2520classes.%2520These%2520observations%2520in%2520our%2520extensive%2520experimental%250Aevaluations%2520reveal%2520the%2520benefits%2520of%2520atypical%2520videos%2520for%2520visual%2520representation%250Alearning%2520in%2520the%2520open%2520world%252C%2520together%2520with%2520the%2520newly%2520proposed%2520dataset%252C%250Aencouraging%2520further%2520studies%2520in%2520this%2520direction.%2520The%2520project%2520page%2520is%2520at%253A%250Ahttps%253A//julysun98.github.io/atypical_dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Can%20We%20Learn%20from%20Harry%20Potter%3F%20An%20Exploratory%20Study%20of%20Visual%0A%20%20Representation%20Learning%20from%20Atypical%20Videos&entry.906535625=Qiyue%20Sun%20and%20Qiming%20Huang%20and%20Yang%20Yang%20and%20Hongjun%20Wang%20and%20Jianbo%20Jiao&entry.1292438233=%20%20Humans%20usually%20show%20exceptional%20generalisation%20and%20discovery%20ability%20in%20the%0Aopen%20world%2C%20when%20being%20shown%20uncommon%20new%20concepts.%20Whereas%20most%20existing%0Astudies%20in%20the%20literature%20focus%20on%20common%20typical%20data%20from%20closed%20sets%2C%0Aopen-world%20novel%20discovery%20is%20under-explored%20in%20videos.%20In%20this%20paper%2C%20we%20are%0Ainterested%20in%20asking%3A%20What%20if%20atypical%20unusual%20videos%20are%20exposed%20in%20the%0Alearning%20process%3F%20To%20this%20end%2C%20we%20collect%20a%20new%20video%20dataset%20consisting%20of%0Avarious%20types%20of%20unusual%20atypical%20data%20%28e.g.%2C%20sci-fi%2C%20animation%2C%20etc.%29.%20To%0Astudy%20how%20such%20atypical%20data%20may%20benefit%20open-world%20learning%2C%20we%20feed%20them%20into%0Athe%20model%20training%20process%20for%20representation%20learning.%20Focusing%20on%20three%20key%0Atasks%20in%20open-world%20learning%3A%20out-of-distribution%20%28OOD%29%20detection%2C%20novel%0Acategory%20discovery%20%28NCD%29%2C%20and%20zero-shot%20action%20recognition%20%28ZSAR%29%2C%20we%20found%0Athat%20even%20straightforward%20learning%20approaches%20with%20atypical%20data%20consistently%0Aimprove%20performance%20across%20various%20settings.%20Furthermore%2C%20we%20found%20that%0Aincreasing%20the%20categorical%20diversity%20of%20the%20atypical%20samples%20further%20boosts%20OOD%0Adetection%20performance.%20Additionally%2C%20in%20the%20NCD%20task%2C%20using%20a%20smaller%20yet%20more%0Asemantically%20diverse%20set%20of%20atypical%20samples%20leads%20to%20better%20performance%0Acompared%20to%20using%20a%20larger%20but%20more%20typical%20dataset.%20In%20the%20ZSAR%20setting%2C%20the%0Asemantic%20diversity%20of%20atypical%20videos%20helps%20the%20model%20generalise%20better%20to%0Aunseen%20action%20classes.%20These%20observations%20in%20our%20extensive%20experimental%0Aevaluations%20reveal%20the%20benefits%20of%20atypical%20videos%20for%20visual%20representation%0Alearning%20in%20the%20open%20world%2C%20together%20with%20the%20newly%20proposed%20dataset%2C%0Aencouraging%20further%20studies%20in%20this%20direction.%20The%20project%20page%20is%20at%3A%0Ahttps%3A//julysun98.github.io/atypical_dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21770v2&entry.124074799=Read"},
{"title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem\n  in VLMs", "author": "Amirmohammad Izadi and Mohammad Ali Banayeeanzade and Fatemeh Askari and Ali Rahimiakbar and Mohammad Mahdi Vahedi and Hosein Hasani and Mahdieh Soleymani Baghshah", "abstract": "  Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the binding problem: the failure to reliably\nassociate perceptual features with their correct visual referents. This\nlimitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces\nVISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective\nintervention: augmenting visual inputs with low-level spatial structures and\npairing this with a textual prompt that encourages sequential, spatially-aware\nparsing. We empirically demonstrate substantial performance improvements across\ncore visual reasoning tasks. Specifically, VISER improves GPT-4o visual search\naccuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit\ndistance error in scene description by 0.32, and enhances performance on\nspatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we\nfind that the visual modification is essential for these gains; purely textual\nstrategies, including Chain-of-Thought prompting, are insufficient and can even\ndegrade performance. VISER enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.\n", "link": "http://arxiv.org/abs/2506.22146v3", "date": "2025-09-08", "relevancy": 2.9261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Structures%20Helps%20Visual%20Reasoning%3A%20Addressing%20the%20Binding%20Problem%0A%20%20in%20VLMs&body=Title%3A%20Visual%20Structures%20Helps%20Visual%20Reasoning%3A%20Addressing%20the%20Binding%20Problem%0A%20%20in%20VLMs%0AAuthor%3A%20Amirmohammad%20Izadi%20and%20Mohammad%20Ali%20Banayeeanzade%20and%20Fatemeh%20Askari%20and%20Ali%20Rahimiakbar%20and%20Mohammad%20Mahdi%20Vahedi%20and%20Hosein%20Hasani%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Despite%20progress%20in%20Vision-Language%20Models%20%28VLMs%29%2C%20their%20capacity%20for%20visual%0Areasoning%20is%20often%20limited%20by%20the%20binding%20problem%3A%20the%20failure%20to%20reliably%0Aassociate%20perceptual%20features%20with%20their%20correct%20visual%20referents.%20This%0Alimitation%20underlies%20persistent%20errors%20in%20tasks%20such%20as%20counting%2C%20visual%0Asearch%2C%20scene%20description%2C%20and%20spatial%20relationship%20understanding.%20A%20key%20factor%0Ais%20that%20current%20VLMs%20process%20visual%20features%20largely%20in%20parallel%2C%20lacking%0Amechanisms%20for%20spatially%20grounded%2C%20serial%20attention.%20This%20paper%20introduces%0AVISER%20%28Visual%20Input%20Structure%20for%20Enhanced%20Reasoning%29%2C%20a%20simple%20yet%20effective%0Aintervention%3A%20augmenting%20visual%20inputs%20with%20low-level%20spatial%20structures%20and%0Apairing%20this%20with%20a%20textual%20prompt%20that%20encourages%20sequential%2C%20spatially-aware%0Aparsing.%20We%20empirically%20demonstrate%20substantial%20performance%20improvements%20across%0Acore%20visual%20reasoning%20tasks.%20Specifically%2C%20VISER%20improves%20GPT-4o%20visual%20search%0Aaccuracy%20by%2025.00%25%2C%20increases%20counting%20accuracy%20by%2026.83%25%2C%20reduces%20edit%0Adistance%20error%20in%20scene%20description%20by%200.32%2C%20and%20enhances%20performance%20on%0Aspatial%20relationship%20tasks%20by%209.50%25%20on%20a%202D%20synthetic%20dataset.%20Furthermore%2C%20we%0Afind%20that%20the%20visual%20modification%20is%20essential%20for%20these%20gains%3B%20purely%20textual%0Astrategies%2C%20including%20Chain-of-Thought%20prompting%2C%20are%20insufficient%20and%20can%20even%0Adegrade%20performance.%20VISER%20enhances%20binding%20only%20with%20a%20single-query%20inference%2C%0Aunderscoring%20the%20importance%20of%20visual%20input%20design%20over%20purely%0Alinguistically-based%20approaches.%20These%20findings%20suggest%20that%20low-level%20visual%0Astructuring%20is%20a%20powerful%20and%20underexplored%20direction%20for%20improving%0Acompositional%20visual%20reasoning%20and%20could%20serve%20as%20a%20general%20strategy%20for%0Aenhancing%20VLM%20performance%20on%20spatially%20grounded%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22146v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Structures%2520Helps%2520Visual%2520Reasoning%253A%2520Addressing%2520the%2520Binding%2520Problem%250A%2520%2520in%2520VLMs%26entry.906535625%3DAmirmohammad%2520Izadi%2520and%2520Mohammad%2520Ali%2520Banayeeanzade%2520and%2520Fatemeh%2520Askari%2520and%2520Ali%2520Rahimiakbar%2520and%2520Mohammad%2520Mahdi%2520Vahedi%2520and%2520Hosein%2520Hasani%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Despite%2520progress%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520their%2520capacity%2520for%2520visual%250Areasoning%2520is%2520often%2520limited%2520by%2520the%2520binding%2520problem%253A%2520the%2520failure%2520to%2520reliably%250Aassociate%2520perceptual%2520features%2520with%2520their%2520correct%2520visual%2520referents.%2520This%250Alimitation%2520underlies%2520persistent%2520errors%2520in%2520tasks%2520such%2520as%2520counting%252C%2520visual%250Asearch%252C%2520scene%2520description%252C%2520and%2520spatial%2520relationship%2520understanding.%2520A%2520key%2520factor%250Ais%2520that%2520current%2520VLMs%2520process%2520visual%2520features%2520largely%2520in%2520parallel%252C%2520lacking%250Amechanisms%2520for%2520spatially%2520grounded%252C%2520serial%2520attention.%2520This%2520paper%2520introduces%250AVISER%2520%2528Visual%2520Input%2520Structure%2520for%2520Enhanced%2520Reasoning%2529%252C%2520a%2520simple%2520yet%2520effective%250Aintervention%253A%2520augmenting%2520visual%2520inputs%2520with%2520low-level%2520spatial%2520structures%2520and%250Apairing%2520this%2520with%2520a%2520textual%2520prompt%2520that%2520encourages%2520sequential%252C%2520spatially-aware%250Aparsing.%2520We%2520empirically%2520demonstrate%2520substantial%2520performance%2520improvements%2520across%250Acore%2520visual%2520reasoning%2520tasks.%2520Specifically%252C%2520VISER%2520improves%2520GPT-4o%2520visual%2520search%250Aaccuracy%2520by%252025.00%2525%252C%2520increases%2520counting%2520accuracy%2520by%252026.83%2525%252C%2520reduces%2520edit%250Adistance%2520error%2520in%2520scene%2520description%2520by%25200.32%252C%2520and%2520enhances%2520performance%2520on%250Aspatial%2520relationship%2520tasks%2520by%25209.50%2525%2520on%2520a%25202D%2520synthetic%2520dataset.%2520Furthermore%252C%2520we%250Afind%2520that%2520the%2520visual%2520modification%2520is%2520essential%2520for%2520these%2520gains%253B%2520purely%2520textual%250Astrategies%252C%2520including%2520Chain-of-Thought%2520prompting%252C%2520are%2520insufficient%2520and%2520can%2520even%250Adegrade%2520performance.%2520VISER%2520enhances%2520binding%2520only%2520with%2520a%2520single-query%2520inference%252C%250Aunderscoring%2520the%2520importance%2520of%2520visual%2520input%2520design%2520over%2520purely%250Alinguistically-based%2520approaches.%2520These%2520findings%2520suggest%2520that%2520low-level%2520visual%250Astructuring%2520is%2520a%2520powerful%2520and%2520underexplored%2520direction%2520for%2520improving%250Acompositional%2520visual%2520reasoning%2520and%2520could%2520serve%2520as%2520a%2520general%2520strategy%2520for%250Aenhancing%2520VLM%2520performance%2520on%2520spatially%2520grounded%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22146v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Structures%20Helps%20Visual%20Reasoning%3A%20Addressing%20the%20Binding%20Problem%0A%20%20in%20VLMs&entry.906535625=Amirmohammad%20Izadi%20and%20Mohammad%20Ali%20Banayeeanzade%20and%20Fatemeh%20Askari%20and%20Ali%20Rahimiakbar%20and%20Mohammad%20Mahdi%20Vahedi%20and%20Hosein%20Hasani%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Despite%20progress%20in%20Vision-Language%20Models%20%28VLMs%29%2C%20their%20capacity%20for%20visual%0Areasoning%20is%20often%20limited%20by%20the%20binding%20problem%3A%20the%20failure%20to%20reliably%0Aassociate%20perceptual%20features%20with%20their%20correct%20visual%20referents.%20This%0Alimitation%20underlies%20persistent%20errors%20in%20tasks%20such%20as%20counting%2C%20visual%0Asearch%2C%20scene%20description%2C%20and%20spatial%20relationship%20understanding.%20A%20key%20factor%0Ais%20that%20current%20VLMs%20process%20visual%20features%20largely%20in%20parallel%2C%20lacking%0Amechanisms%20for%20spatially%20grounded%2C%20serial%20attention.%20This%20paper%20introduces%0AVISER%20%28Visual%20Input%20Structure%20for%20Enhanced%20Reasoning%29%2C%20a%20simple%20yet%20effective%0Aintervention%3A%20augmenting%20visual%20inputs%20with%20low-level%20spatial%20structures%20and%0Apairing%20this%20with%20a%20textual%20prompt%20that%20encourages%20sequential%2C%20spatially-aware%0Aparsing.%20We%20empirically%20demonstrate%20substantial%20performance%20improvements%20across%0Acore%20visual%20reasoning%20tasks.%20Specifically%2C%20VISER%20improves%20GPT-4o%20visual%20search%0Aaccuracy%20by%2025.00%25%2C%20increases%20counting%20accuracy%20by%2026.83%25%2C%20reduces%20edit%0Adistance%20error%20in%20scene%20description%20by%200.32%2C%20and%20enhances%20performance%20on%0Aspatial%20relationship%20tasks%20by%209.50%25%20on%20a%202D%20synthetic%20dataset.%20Furthermore%2C%20we%0Afind%20that%20the%20visual%20modification%20is%20essential%20for%20these%20gains%3B%20purely%20textual%0Astrategies%2C%20including%20Chain-of-Thought%20prompting%2C%20are%20insufficient%20and%20can%20even%0Adegrade%20performance.%20VISER%20enhances%20binding%20only%20with%20a%20single-query%20inference%2C%0Aunderscoring%20the%20importance%20of%20visual%20input%20design%20over%20purely%0Alinguistically-based%20approaches.%20These%20findings%20suggest%20that%20low-level%20visual%0Astructuring%20is%20a%20powerful%20and%20underexplored%20direction%20for%20improving%0Acompositional%20visual%20reasoning%20and%20could%20serve%20as%20a%20general%20strategy%20for%0Aenhancing%20VLM%20performance%20on%20spatially%20grounded%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22146v3&entry.124074799=Read"},
{"title": "Matching Shapes Under Different Topologies: A Topology-Adaptive\n  Deformation Guided Approach", "author": "Aymen Merrouche and Stefanie Wuhrer and Edmond Boyer", "abstract": "  Non-rigid 3D mesh matching is a critical step in computer vision and computer\ngraphics pipelines. We tackle matching meshes that contain topological\nartefacts which can break the assumption made by current approaches. While\nFunctional Maps assume the deformation induced by the ground truth\ncorrespondences to be near-isometric, ARAP-like deformation-guided approaches\nassume the latter to be ARAP. Neither assumption holds in certain topological\nconfigurations of the input shapes. We are motivated by real-world scenarios\nsuch as per-frame multi-view reconstructions, often suffering from topological\nartefacts. To this end, we propose a topology-adaptive deformation model\nallowing changes in shape topology to align shape pairs under ARAP and\nbijective association constraints. Using this model, we jointly optimise for a\ntemplate mesh with adequate topology and for its alignment with the shapes to\nbe matched to extract correspondences. We show that, while not relying on any\ndata-driven prior, our approach applies to highly non-isometric shapes and\nshapes with topological artefacts, including noisy per-frame multi-view\nreconstructions, even outperforming methods trained on large datasets in 3D\nalignment quality.\n", "link": "http://arxiv.org/abs/2509.06862v1", "date": "2025-09-08", "relevancy": 2.9159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6241}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5733}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matching%20Shapes%20Under%20Different%20Topologies%3A%20A%20Topology-Adaptive%0A%20%20Deformation%20Guided%20Approach&body=Title%3A%20Matching%20Shapes%20Under%20Different%20Topologies%3A%20A%20Topology-Adaptive%0A%20%20Deformation%20Guided%20Approach%0AAuthor%3A%20Aymen%20Merrouche%20and%20Stefanie%20Wuhrer%20and%20Edmond%20Boyer%0AAbstract%3A%20%20%20Non-rigid%203D%20mesh%20matching%20is%20a%20critical%20step%20in%20computer%20vision%20and%20computer%0Agraphics%20pipelines.%20We%20tackle%20matching%20meshes%20that%20contain%20topological%0Aartefacts%20which%20can%20break%20the%20assumption%20made%20by%20current%20approaches.%20While%0AFunctional%20Maps%20assume%20the%20deformation%20induced%20by%20the%20ground%20truth%0Acorrespondences%20to%20be%20near-isometric%2C%20ARAP-like%20deformation-guided%20approaches%0Aassume%20the%20latter%20to%20be%20ARAP.%20Neither%20assumption%20holds%20in%20certain%20topological%0Aconfigurations%20of%20the%20input%20shapes.%20We%20are%20motivated%20by%20real-world%20scenarios%0Asuch%20as%20per-frame%20multi-view%20reconstructions%2C%20often%20suffering%20from%20topological%0Aartefacts.%20To%20this%20end%2C%20we%20propose%20a%20topology-adaptive%20deformation%20model%0Aallowing%20changes%20in%20shape%20topology%20to%20align%20shape%20pairs%20under%20ARAP%20and%0Abijective%20association%20constraints.%20Using%20this%20model%2C%20we%20jointly%20optimise%20for%20a%0Atemplate%20mesh%20with%20adequate%20topology%20and%20for%20its%20alignment%20with%20the%20shapes%20to%0Abe%20matched%20to%20extract%20correspondences.%20We%20show%20that%2C%20while%20not%20relying%20on%20any%0Adata-driven%20prior%2C%20our%20approach%20applies%20to%20highly%20non-isometric%20shapes%20and%0Ashapes%20with%20topological%20artefacts%2C%20including%20noisy%20per-frame%20multi-view%0Areconstructions%2C%20even%20outperforming%20methods%20trained%20on%20large%20datasets%20in%203D%0Aalignment%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatching%2520Shapes%2520Under%2520Different%2520Topologies%253A%2520A%2520Topology-Adaptive%250A%2520%2520Deformation%2520Guided%2520Approach%26entry.906535625%3DAymen%2520Merrouche%2520and%2520Stefanie%2520Wuhrer%2520and%2520Edmond%2520Boyer%26entry.1292438233%3D%2520%2520Non-rigid%25203D%2520mesh%2520matching%2520is%2520a%2520critical%2520step%2520in%2520computer%2520vision%2520and%2520computer%250Agraphics%2520pipelines.%2520We%2520tackle%2520matching%2520meshes%2520that%2520contain%2520topological%250Aartefacts%2520which%2520can%2520break%2520the%2520assumption%2520made%2520by%2520current%2520approaches.%2520While%250AFunctional%2520Maps%2520assume%2520the%2520deformation%2520induced%2520by%2520the%2520ground%2520truth%250Acorrespondences%2520to%2520be%2520near-isometric%252C%2520ARAP-like%2520deformation-guided%2520approaches%250Aassume%2520the%2520latter%2520to%2520be%2520ARAP.%2520Neither%2520assumption%2520holds%2520in%2520certain%2520topological%250Aconfigurations%2520of%2520the%2520input%2520shapes.%2520We%2520are%2520motivated%2520by%2520real-world%2520scenarios%250Asuch%2520as%2520per-frame%2520multi-view%2520reconstructions%252C%2520often%2520suffering%2520from%2520topological%250Aartefacts.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520topology-adaptive%2520deformation%2520model%250Aallowing%2520changes%2520in%2520shape%2520topology%2520to%2520align%2520shape%2520pairs%2520under%2520ARAP%2520and%250Abijective%2520association%2520constraints.%2520Using%2520this%2520model%252C%2520we%2520jointly%2520optimise%2520for%2520a%250Atemplate%2520mesh%2520with%2520adequate%2520topology%2520and%2520for%2520its%2520alignment%2520with%2520the%2520shapes%2520to%250Abe%2520matched%2520to%2520extract%2520correspondences.%2520We%2520show%2520that%252C%2520while%2520not%2520relying%2520on%2520any%250Adata-driven%2520prior%252C%2520our%2520approach%2520applies%2520to%2520highly%2520non-isometric%2520shapes%2520and%250Ashapes%2520with%2520topological%2520artefacts%252C%2520including%2520noisy%2520per-frame%2520multi-view%250Areconstructions%252C%2520even%2520outperforming%2520methods%2520trained%2520on%2520large%2520datasets%2520in%25203D%250Aalignment%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20Shapes%20Under%20Different%20Topologies%3A%20A%20Topology-Adaptive%0A%20%20Deformation%20Guided%20Approach&entry.906535625=Aymen%20Merrouche%20and%20Stefanie%20Wuhrer%20and%20Edmond%20Boyer&entry.1292438233=%20%20Non-rigid%203D%20mesh%20matching%20is%20a%20critical%20step%20in%20computer%20vision%20and%20computer%0Agraphics%20pipelines.%20We%20tackle%20matching%20meshes%20that%20contain%20topological%0Aartefacts%20which%20can%20break%20the%20assumption%20made%20by%20current%20approaches.%20While%0AFunctional%20Maps%20assume%20the%20deformation%20induced%20by%20the%20ground%20truth%0Acorrespondences%20to%20be%20near-isometric%2C%20ARAP-like%20deformation-guided%20approaches%0Aassume%20the%20latter%20to%20be%20ARAP.%20Neither%20assumption%20holds%20in%20certain%20topological%0Aconfigurations%20of%20the%20input%20shapes.%20We%20are%20motivated%20by%20real-world%20scenarios%0Asuch%20as%20per-frame%20multi-view%20reconstructions%2C%20often%20suffering%20from%20topological%0Aartefacts.%20To%20this%20end%2C%20we%20propose%20a%20topology-adaptive%20deformation%20model%0Aallowing%20changes%20in%20shape%20topology%20to%20align%20shape%20pairs%20under%20ARAP%20and%0Abijective%20association%20constraints.%20Using%20this%20model%2C%20we%20jointly%20optimise%20for%20a%0Atemplate%20mesh%20with%20adequate%20topology%20and%20for%20its%20alignment%20with%20the%20shapes%20to%0Abe%20matched%20to%20extract%20correspondences.%20We%20show%20that%2C%20while%20not%20relying%20on%20any%0Adata-driven%20prior%2C%20our%20approach%20applies%20to%20highly%20non-isometric%20shapes%20and%0Ashapes%20with%20topological%20artefacts%2C%20including%20noisy%20per-frame%20multi-view%0Areconstructions%2C%20even%20outperforming%20methods%20trained%20on%20large%20datasets%20in%203D%0Aalignment%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06862v1&entry.124074799=Read"},
{"title": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose\n  Transformers", "author": "Wenhao Li and Mengyuan Liu and Hong Liu and Pichao Wang and Shijian Lu and Nicu Sebe", "abstract": "  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a hierarchical plug-and-play pruning-and-recovering\nframework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient\ntransformer-based 3D human pose estimation from videos. H$_{2}$OT begins with\nprogressively pruning pose tokens of redundant frames and ends with recovering\nfull-length sequences, resulting in a few pose tokens in the intermediate\ntransformer blocks and thus improving the model efficiency. It works with two\nkey modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module\n(TRM). TPM dynamically selects a few representative tokens to eliminate the\nredundancy of video frames, while TRM restores the detailed spatio-temporal\ninformation based on the selected tokens, thereby expanding the network output\nto the original full-length temporal resolution for fast inference. Our method\nis general-purpose: it can be easily incorporated into common VPT models on\nboth seq2seq and seq2frame pipelines while effectively accommodating different\ntoken pruning and recovery strategies. In addition, our H$_{2}$OT reveals that\nmaintaining the full pose sequence is unnecessary, and a few pose tokens of\nrepresentative frames can achieve both high efficiency and estimation accuracy.\nExtensive experiments on multiple benchmark datasets demonstrate both the\neffectiveness and efficiency of the proposed method. Code and models are\navailable at https://github.com/NationalGAILab/HoT.\n", "link": "http://arxiv.org/abs/2509.06956v1", "date": "2025-09-08", "relevancy": 2.8994, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6044}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H%24_%7B2%7D%24OT%3A%20Hierarchical%20Hourglass%20Tokenizer%20for%20Efficient%20Video%20Pose%0A%20%20Transformers&body=Title%3A%20H%24_%7B2%7D%24OT%3A%20Hierarchical%20Hourglass%20Tokenizer%20for%20Efficient%20Video%20Pose%0A%20%20Transformers%0AAuthor%3A%20Wenhao%20Li%20and%20Mengyuan%20Liu%20and%20Hong%20Liu%20and%20Pichao%20Wang%20and%20Shijian%20Lu%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Transformers%20have%20been%20successfully%20applied%20in%20the%20field%20of%20video-based%203D%0Ahuman%20pose%20estimation.%20However%2C%20the%20high%20computational%20costs%20of%20these%20video%0Apose%20transformers%20%28VPTs%29%20make%20them%20impractical%20on%20resource-constrained%20devices.%0AIn%20this%20paper%2C%20we%20present%20a%20hierarchical%20plug-and-play%20pruning-and-recovering%0Aframework%2C%20called%20Hierarchical%20Hourglass%20Tokenizer%20%28H%24_%7B2%7D%24OT%29%2C%20for%20efficient%0Atransformer-based%203D%20human%20pose%20estimation%20from%20videos.%20H%24_%7B2%7D%24OT%20begins%20with%0Aprogressively%20pruning%20pose%20tokens%20of%20redundant%20frames%20and%20ends%20with%20recovering%0Afull-length%20sequences%2C%20resulting%20in%20a%20few%20pose%20tokens%20in%20the%20intermediate%0Atransformer%20blocks%20and%20thus%20improving%20the%20model%20efficiency.%20It%20works%20with%20two%0Akey%20modules%2C%20namely%2C%20a%20Token%20Pruning%20Module%20%28TPM%29%20and%20a%20Token%20Recovering%20Module%0A%28TRM%29.%20TPM%20dynamically%20selects%20a%20few%20representative%20tokens%20to%20eliminate%20the%0Aredundancy%20of%20video%20frames%2C%20while%20TRM%20restores%20the%20detailed%20spatio-temporal%0Ainformation%20based%20on%20the%20selected%20tokens%2C%20thereby%20expanding%20the%20network%20output%0Ato%20the%20original%20full-length%20temporal%20resolution%20for%20fast%20inference.%20Our%20method%0Ais%20general-purpose%3A%20it%20can%20be%20easily%20incorporated%20into%20common%20VPT%20models%20on%0Aboth%20seq2seq%20and%20seq2frame%20pipelines%20while%20effectively%20accommodating%20different%0Atoken%20pruning%20and%20recovery%20strategies.%20In%20addition%2C%20our%20H%24_%7B2%7D%24OT%20reveals%20that%0Amaintaining%20the%20full%20pose%20sequence%20is%20unnecessary%2C%20and%20a%20few%20pose%20tokens%20of%0Arepresentative%20frames%20can%20achieve%20both%20high%20efficiency%20and%20estimation%20accuracy.%0AExtensive%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20both%20the%0Aeffectiveness%20and%20efficiency%20of%20the%20proposed%20method.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/NationalGAILab/HoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH%2524_%257B2%257D%2524OT%253A%2520Hierarchical%2520Hourglass%2520Tokenizer%2520for%2520Efficient%2520Video%2520Pose%250A%2520%2520Transformers%26entry.906535625%3DWenhao%2520Li%2520and%2520Mengyuan%2520Liu%2520and%2520Hong%2520Liu%2520and%2520Pichao%2520Wang%2520and%2520Shijian%2520Lu%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Transformers%2520have%2520been%2520successfully%2520applied%2520in%2520the%2520field%2520of%2520video-based%25203D%250Ahuman%2520pose%2520estimation.%2520However%252C%2520the%2520high%2520computational%2520costs%2520of%2520these%2520video%250Apose%2520transformers%2520%2528VPTs%2529%2520make%2520them%2520impractical%2520on%2520resource-constrained%2520devices.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520hierarchical%2520plug-and-play%2520pruning-and-recovering%250Aframework%252C%2520called%2520Hierarchical%2520Hourglass%2520Tokenizer%2520%2528H%2524_%257B2%257D%2524OT%2529%252C%2520for%2520efficient%250Atransformer-based%25203D%2520human%2520pose%2520estimation%2520from%2520videos.%2520H%2524_%257B2%257D%2524OT%2520begins%2520with%250Aprogressively%2520pruning%2520pose%2520tokens%2520of%2520redundant%2520frames%2520and%2520ends%2520with%2520recovering%250Afull-length%2520sequences%252C%2520resulting%2520in%2520a%2520few%2520pose%2520tokens%2520in%2520the%2520intermediate%250Atransformer%2520blocks%2520and%2520thus%2520improving%2520the%2520model%2520efficiency.%2520It%2520works%2520with%2520two%250Akey%2520modules%252C%2520namely%252C%2520a%2520Token%2520Pruning%2520Module%2520%2528TPM%2529%2520and%2520a%2520Token%2520Recovering%2520Module%250A%2528TRM%2529.%2520TPM%2520dynamically%2520selects%2520a%2520few%2520representative%2520tokens%2520to%2520eliminate%2520the%250Aredundancy%2520of%2520video%2520frames%252C%2520while%2520TRM%2520restores%2520the%2520detailed%2520spatio-temporal%250Ainformation%2520based%2520on%2520the%2520selected%2520tokens%252C%2520thereby%2520expanding%2520the%2520network%2520output%250Ato%2520the%2520original%2520full-length%2520temporal%2520resolution%2520for%2520fast%2520inference.%2520Our%2520method%250Ais%2520general-purpose%253A%2520it%2520can%2520be%2520easily%2520incorporated%2520into%2520common%2520VPT%2520models%2520on%250Aboth%2520seq2seq%2520and%2520seq2frame%2520pipelines%2520while%2520effectively%2520accommodating%2520different%250Atoken%2520pruning%2520and%2520recovery%2520strategies.%2520In%2520addition%252C%2520our%2520H%2524_%257B2%257D%2524OT%2520reveals%2520that%250Amaintaining%2520the%2520full%2520pose%2520sequence%2520is%2520unnecessary%252C%2520and%2520a%2520few%2520pose%2520tokens%2520of%250Arepresentative%2520frames%2520can%2520achieve%2520both%2520high%2520efficiency%2520and%2520estimation%2520accuracy.%250AExtensive%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520both%2520the%250Aeffectiveness%2520and%2520efficiency%2520of%2520the%2520proposed%2520method.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/NationalGAILab/HoT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H%24_%7B2%7D%24OT%3A%20Hierarchical%20Hourglass%20Tokenizer%20for%20Efficient%20Video%20Pose%0A%20%20Transformers&entry.906535625=Wenhao%20Li%20and%20Mengyuan%20Liu%20and%20Hong%20Liu%20and%20Pichao%20Wang%20and%20Shijian%20Lu%20and%20Nicu%20Sebe&entry.1292438233=%20%20Transformers%20have%20been%20successfully%20applied%20in%20the%20field%20of%20video-based%203D%0Ahuman%20pose%20estimation.%20However%2C%20the%20high%20computational%20costs%20of%20these%20video%0Apose%20transformers%20%28VPTs%29%20make%20them%20impractical%20on%20resource-constrained%20devices.%0AIn%20this%20paper%2C%20we%20present%20a%20hierarchical%20plug-and-play%20pruning-and-recovering%0Aframework%2C%20called%20Hierarchical%20Hourglass%20Tokenizer%20%28H%24_%7B2%7D%24OT%29%2C%20for%20efficient%0Atransformer-based%203D%20human%20pose%20estimation%20from%20videos.%20H%24_%7B2%7D%24OT%20begins%20with%0Aprogressively%20pruning%20pose%20tokens%20of%20redundant%20frames%20and%20ends%20with%20recovering%0Afull-length%20sequences%2C%20resulting%20in%20a%20few%20pose%20tokens%20in%20the%20intermediate%0Atransformer%20blocks%20and%20thus%20improving%20the%20model%20efficiency.%20It%20works%20with%20two%0Akey%20modules%2C%20namely%2C%20a%20Token%20Pruning%20Module%20%28TPM%29%20and%20a%20Token%20Recovering%20Module%0A%28TRM%29.%20TPM%20dynamically%20selects%20a%20few%20representative%20tokens%20to%20eliminate%20the%0Aredundancy%20of%20video%20frames%2C%20while%20TRM%20restores%20the%20detailed%20spatio-temporal%0Ainformation%20based%20on%20the%20selected%20tokens%2C%20thereby%20expanding%20the%20network%20output%0Ato%20the%20original%20full-length%20temporal%20resolution%20for%20fast%20inference.%20Our%20method%0Ais%20general-purpose%3A%20it%20can%20be%20easily%20incorporated%20into%20common%20VPT%20models%20on%0Aboth%20seq2seq%20and%20seq2frame%20pipelines%20while%20effectively%20accommodating%20different%0Atoken%20pruning%20and%20recovery%20strategies.%20In%20addition%2C%20our%20H%24_%7B2%7D%24OT%20reveals%20that%0Amaintaining%20the%20full%20pose%20sequence%20is%20unnecessary%2C%20and%20a%20few%20pose%20tokens%20of%0Arepresentative%20frames%20can%20achieve%20both%20high%20efficiency%20and%20estimation%20accuracy.%0AExtensive%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20both%20the%0Aeffectiveness%20and%20efficiency%20of%20the%20proposed%20method.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/NationalGAILab/HoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06956v1&entry.124074799=Read"},
{"title": "Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image\n  Retrieval", "author": "Emil Demi\u0107 and Luka \u010cehovin Zajc", "abstract": "  The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural\nimages matching the overall semantics and spatial layout of a free-hand sketch.\nUnlike prior work focused on architectural augmentations of retrieval models,\nwe emphasize the inherent ambiguity and noise present in real-world sketches.\nThis insight motivates a training objective that is explicitly designed to be\nrobust to sketch variability. We show that with an appropriate combination of\npre-training, encoder architecture, and loss formulation, it is possible to\nachieve state-of-the-art performance without the introduction of additional\ncomplexity. Extensive experiments on a challenging FS-COCO and widely-used\nSketchyCOCO datasets confirm the effectiveness of our approach and underline\nthe critical role of training design in cross-modal retrieval tasks, as well as\nthe need to improve the evaluation scenarios of scene-level SBIR.\n", "link": "http://arxiv.org/abs/2509.06566v1", "date": "2025-09-08", "relevancy": 2.8932, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Back%20To%20The%20Drawing%20Board%3A%20Rethinking%20Scene-Level%20Sketch-Based%20Image%0A%20%20Retrieval&body=Title%3A%20Back%20To%20The%20Drawing%20Board%3A%20Rethinking%20Scene-Level%20Sketch-Based%20Image%0A%20%20Retrieval%0AAuthor%3A%20Emil%20Demi%C4%87%20and%20Luka%20%C4%8Cehovin%20Zajc%0AAbstract%3A%20%20%20The%20goal%20of%20Scene-level%20Sketch-Based%20Image%20Retrieval%20is%20to%20retrieve%20natural%0Aimages%20matching%20the%20overall%20semantics%20and%20spatial%20layout%20of%20a%20free-hand%20sketch.%0AUnlike%20prior%20work%20focused%20on%20architectural%20augmentations%20of%20retrieval%20models%2C%0Awe%20emphasize%20the%20inherent%20ambiguity%20and%20noise%20present%20in%20real-world%20sketches.%0AThis%20insight%20motivates%20a%20training%20objective%20that%20is%20explicitly%20designed%20to%20be%0Arobust%20to%20sketch%20variability.%20We%20show%20that%20with%20an%20appropriate%20combination%20of%0Apre-training%2C%20encoder%20architecture%2C%20and%20loss%20formulation%2C%20it%20is%20possible%20to%0Aachieve%20state-of-the-art%20performance%20without%20the%20introduction%20of%20additional%0Acomplexity.%20Extensive%20experiments%20on%20a%20challenging%20FS-COCO%20and%20widely-used%0ASketchyCOCO%20datasets%20confirm%20the%20effectiveness%20of%20our%20approach%20and%20underline%0Athe%20critical%20role%20of%20training%20design%20in%20cross-modal%20retrieval%20tasks%2C%20as%20well%20as%0Athe%20need%20to%20improve%20the%20evaluation%20scenarios%20of%20scene-level%20SBIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBack%2520To%2520The%2520Drawing%2520Board%253A%2520Rethinking%2520Scene-Level%2520Sketch-Based%2520Image%250A%2520%2520Retrieval%26entry.906535625%3DEmil%2520Demi%25C4%2587%2520and%2520Luka%2520%25C4%258Cehovin%2520Zajc%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520Scene-level%2520Sketch-Based%2520Image%2520Retrieval%2520is%2520to%2520retrieve%2520natural%250Aimages%2520matching%2520the%2520overall%2520semantics%2520and%2520spatial%2520layout%2520of%2520a%2520free-hand%2520sketch.%250AUnlike%2520prior%2520work%2520focused%2520on%2520architectural%2520augmentations%2520of%2520retrieval%2520models%252C%250Awe%2520emphasize%2520the%2520inherent%2520ambiguity%2520and%2520noise%2520present%2520in%2520real-world%2520sketches.%250AThis%2520insight%2520motivates%2520a%2520training%2520objective%2520that%2520is%2520explicitly%2520designed%2520to%2520be%250Arobust%2520to%2520sketch%2520variability.%2520We%2520show%2520that%2520with%2520an%2520appropriate%2520combination%2520of%250Apre-training%252C%2520encoder%2520architecture%252C%2520and%2520loss%2520formulation%252C%2520it%2520is%2520possible%2520to%250Aachieve%2520state-of-the-art%2520performance%2520without%2520the%2520introduction%2520of%2520additional%250Acomplexity.%2520Extensive%2520experiments%2520on%2520a%2520challenging%2520FS-COCO%2520and%2520widely-used%250ASketchyCOCO%2520datasets%2520confirm%2520the%2520effectiveness%2520of%2520our%2520approach%2520and%2520underline%250Athe%2520critical%2520role%2520of%2520training%2520design%2520in%2520cross-modal%2520retrieval%2520tasks%252C%2520as%2520well%2520as%250Athe%2520need%2520to%2520improve%2520the%2520evaluation%2520scenarios%2520of%2520scene-level%2520SBIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back%20To%20The%20Drawing%20Board%3A%20Rethinking%20Scene-Level%20Sketch-Based%20Image%0A%20%20Retrieval&entry.906535625=Emil%20Demi%C4%87%20and%20Luka%20%C4%8Cehovin%20Zajc&entry.1292438233=%20%20The%20goal%20of%20Scene-level%20Sketch-Based%20Image%20Retrieval%20is%20to%20retrieve%20natural%0Aimages%20matching%20the%20overall%20semantics%20and%20spatial%20layout%20of%20a%20free-hand%20sketch.%0AUnlike%20prior%20work%20focused%20on%20architectural%20augmentations%20of%20retrieval%20models%2C%0Awe%20emphasize%20the%20inherent%20ambiguity%20and%20noise%20present%20in%20real-world%20sketches.%0AThis%20insight%20motivates%20a%20training%20objective%20that%20is%20explicitly%20designed%20to%20be%0Arobust%20to%20sketch%20variability.%20We%20show%20that%20with%20an%20appropriate%20combination%20of%0Apre-training%2C%20encoder%20architecture%2C%20and%20loss%20formulation%2C%20it%20is%20possible%20to%0Aachieve%20state-of-the-art%20performance%20without%20the%20introduction%20of%20additional%0Acomplexity.%20Extensive%20experiments%20on%20a%20challenging%20FS-COCO%20and%20widely-used%0ASketchyCOCO%20datasets%20confirm%20the%20effectiveness%20of%20our%20approach%20and%20underline%0Athe%20critical%20role%20of%20training%20design%20in%20cross-modal%20retrieval%20tasks%2C%20as%20well%20as%0Athe%20need%20to%20improve%20the%20evaluation%20scenarios%20of%20scene-level%20SBIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06566v1&entry.124074799=Read"},
{"title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation", "author": "Yinglin Duan and Zhengxia Zou and Tongwei Gu and Wei Jia and Zhan Zhao and Luyi Xu and Xinzhu Liu and Yenan Lin and Hao Jiang and Kang Chen and Shuang Qiu", "abstract": "  Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18\n", "link": "http://arxiv.org/abs/2509.05263v2", "date": "2025-09-08", "relevancy": 2.8928, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LatticeWorld%3A%20A%20Multimodal%20Large%20Language%20Model-Empowered%20Framework%20for%0A%20%20Interactive%20Complex%20World%20Generation&body=Title%3A%20LatticeWorld%3A%20A%20Multimodal%20Large%20Language%20Model-Empowered%20Framework%20for%0A%20%20Interactive%20Complex%20World%20Generation%0AAuthor%3A%20Yinglin%20Duan%20and%20Zhengxia%20Zou%20and%20Tongwei%20Gu%20and%20Wei%20Jia%20and%20Zhan%20Zhao%20and%20Luyi%20Xu%20and%20Xinzhu%20Liu%20and%20Yenan%20Lin%20and%20Hao%20Jiang%20and%20Kang%20Chen%20and%20Shuang%20Qiu%0AAbstract%3A%20%20%20Recent%20research%20has%20been%20increasingly%20focusing%20on%20developing%203D%20world%20models%0Athat%20simulate%20complex%20real-world%20scenarios.%20World%20models%20have%20found%20broad%0Aapplications%20across%20various%20domains%2C%20including%20embodied%20AI%2C%20autonomous%20driving%2C%0Aentertainment%2C%20etc.%20A%20more%20realistic%20simulation%20with%20accurate%20physics%20will%0Aeffectively%20narrow%20the%20sim-to-real%20gap%20and%20allow%20us%20to%20gather%20rich%20information%0Aabout%20the%20real%20world%20conveniently.%20While%20traditional%20manual%20modeling%20has%0Aenabled%20the%20creation%20of%20virtual%203D%20scenes%2C%20modern%20approaches%20have%20leveraged%0Aadvanced%20machine%20learning%20algorithms%20for%203D%20world%20generation%2C%20with%20most%20recent%0Aadvances%20focusing%20on%20generative%20methods%20that%20can%20create%20virtual%20worlds%20based%20on%0Auser%20instructions.%20This%20work%20explores%20such%20a%20research%20direction%20by%20proposing%0ALatticeWorld%2C%20a%20simple%20yet%20effective%203D%20world%20generation%20framework%20that%0Astreamlines%20the%20industrial%20production%20pipeline%20of%203D%20environments.%20LatticeWorld%0Aleverages%20lightweight%20LLMs%20%28LLaMA-2-7B%29%20alongside%20the%20industry-grade%20rendering%0Aengine%20%28e.g.%2C%20Unreal%20Engine%205%29%20to%20generate%20a%20dynamic%20environment.%20Our%20proposed%0Aframework%20accepts%20textual%20descriptions%20and%20visual%20instructions%20as%20multimodal%0Ainputs%20and%20creates%20large-scale%203D%20interactive%20worlds%20with%20dynamic%20agents%2C%0Afeaturing%20competitive%20multi-agent%20interaction%2C%20high-fidelity%20physics%0Asimulation%2C%20and%20real-time%20rendering.%20We%20conduct%20comprehensive%20experiments%20to%0Aevaluate%20LatticeWorld%2C%20showing%20that%20it%20achieves%20superior%20accuracy%20in%20scene%0Alayout%20generation%20and%20visual%20fidelity.%20Moreover%2C%20LatticeWorld%20achieves%20over%20a%0A%2490%5Ctimes%24%20increase%20in%20industrial%20production%20efficiency%20while%20maintaining%20high%0Acreative%20quality%20compared%20with%20traditional%20manual%20production%20methods.%20Our%20demo%0Avideo%20is%20available%20at%20https%3A//youtu.be/8VWZXpERR18%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatticeWorld%253A%2520A%2520Multimodal%2520Large%2520Language%2520Model-Empowered%2520Framework%2520for%250A%2520%2520Interactive%2520Complex%2520World%2520Generation%26entry.906535625%3DYinglin%2520Duan%2520and%2520Zhengxia%2520Zou%2520and%2520Tongwei%2520Gu%2520and%2520Wei%2520Jia%2520and%2520Zhan%2520Zhao%2520and%2520Luyi%2520Xu%2520and%2520Xinzhu%2520Liu%2520and%2520Yenan%2520Lin%2520and%2520Hao%2520Jiang%2520and%2520Kang%2520Chen%2520and%2520Shuang%2520Qiu%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520been%2520increasingly%2520focusing%2520on%2520developing%25203D%2520world%2520models%250Athat%2520simulate%2520complex%2520real-world%2520scenarios.%2520World%2520models%2520have%2520found%2520broad%250Aapplications%2520across%2520various%2520domains%252C%2520including%2520embodied%2520AI%252C%2520autonomous%2520driving%252C%250Aentertainment%252C%2520etc.%2520A%2520more%2520realistic%2520simulation%2520with%2520accurate%2520physics%2520will%250Aeffectively%2520narrow%2520the%2520sim-to-real%2520gap%2520and%2520allow%2520us%2520to%2520gather%2520rich%2520information%250Aabout%2520the%2520real%2520world%2520conveniently.%2520While%2520traditional%2520manual%2520modeling%2520has%250Aenabled%2520the%2520creation%2520of%2520virtual%25203D%2520scenes%252C%2520modern%2520approaches%2520have%2520leveraged%250Aadvanced%2520machine%2520learning%2520algorithms%2520for%25203D%2520world%2520generation%252C%2520with%2520most%2520recent%250Aadvances%2520focusing%2520on%2520generative%2520methods%2520that%2520can%2520create%2520virtual%2520worlds%2520based%2520on%250Auser%2520instructions.%2520This%2520work%2520explores%2520such%2520a%2520research%2520direction%2520by%2520proposing%250ALatticeWorld%252C%2520a%2520simple%2520yet%2520effective%25203D%2520world%2520generation%2520framework%2520that%250Astreamlines%2520the%2520industrial%2520production%2520pipeline%2520of%25203D%2520environments.%2520LatticeWorld%250Aleverages%2520lightweight%2520LLMs%2520%2528LLaMA-2-7B%2529%2520alongside%2520the%2520industry-grade%2520rendering%250Aengine%2520%2528e.g.%252C%2520Unreal%2520Engine%25205%2529%2520to%2520generate%2520a%2520dynamic%2520environment.%2520Our%2520proposed%250Aframework%2520accepts%2520textual%2520descriptions%2520and%2520visual%2520instructions%2520as%2520multimodal%250Ainputs%2520and%2520creates%2520large-scale%25203D%2520interactive%2520worlds%2520with%2520dynamic%2520agents%252C%250Afeaturing%2520competitive%2520multi-agent%2520interaction%252C%2520high-fidelity%2520physics%250Asimulation%252C%2520and%2520real-time%2520rendering.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%250Aevaluate%2520LatticeWorld%252C%2520showing%2520that%2520it%2520achieves%2520superior%2520accuracy%2520in%2520scene%250Alayout%2520generation%2520and%2520visual%2520fidelity.%2520Moreover%252C%2520LatticeWorld%2520achieves%2520over%2520a%250A%252490%255Ctimes%2524%2520increase%2520in%2520industrial%2520production%2520efficiency%2520while%2520maintaining%2520high%250Acreative%2520quality%2520compared%2520with%2520traditional%2520manual%2520production%2520methods.%2520Our%2520demo%250Avideo%2520is%2520available%2520at%2520https%253A//youtu.be/8VWZXpERR18%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatticeWorld%3A%20A%20Multimodal%20Large%20Language%20Model-Empowered%20Framework%20for%0A%20%20Interactive%20Complex%20World%20Generation&entry.906535625=Yinglin%20Duan%20and%20Zhengxia%20Zou%20and%20Tongwei%20Gu%20and%20Wei%20Jia%20and%20Zhan%20Zhao%20and%20Luyi%20Xu%20and%20Xinzhu%20Liu%20and%20Yenan%20Lin%20and%20Hao%20Jiang%20and%20Kang%20Chen%20and%20Shuang%20Qiu&entry.1292438233=%20%20Recent%20research%20has%20been%20increasingly%20focusing%20on%20developing%203D%20world%20models%0Athat%20simulate%20complex%20real-world%20scenarios.%20World%20models%20have%20found%20broad%0Aapplications%20across%20various%20domains%2C%20including%20embodied%20AI%2C%20autonomous%20driving%2C%0Aentertainment%2C%20etc.%20A%20more%20realistic%20simulation%20with%20accurate%20physics%20will%0Aeffectively%20narrow%20the%20sim-to-real%20gap%20and%20allow%20us%20to%20gather%20rich%20information%0Aabout%20the%20real%20world%20conveniently.%20While%20traditional%20manual%20modeling%20has%0Aenabled%20the%20creation%20of%20virtual%203D%20scenes%2C%20modern%20approaches%20have%20leveraged%0Aadvanced%20machine%20learning%20algorithms%20for%203D%20world%20generation%2C%20with%20most%20recent%0Aadvances%20focusing%20on%20generative%20methods%20that%20can%20create%20virtual%20worlds%20based%20on%0Auser%20instructions.%20This%20work%20explores%20such%20a%20research%20direction%20by%20proposing%0ALatticeWorld%2C%20a%20simple%20yet%20effective%203D%20world%20generation%20framework%20that%0Astreamlines%20the%20industrial%20production%20pipeline%20of%203D%20environments.%20LatticeWorld%0Aleverages%20lightweight%20LLMs%20%28LLaMA-2-7B%29%20alongside%20the%20industry-grade%20rendering%0Aengine%20%28e.g.%2C%20Unreal%20Engine%205%29%20to%20generate%20a%20dynamic%20environment.%20Our%20proposed%0Aframework%20accepts%20textual%20descriptions%20and%20visual%20instructions%20as%20multimodal%0Ainputs%20and%20creates%20large-scale%203D%20interactive%20worlds%20with%20dynamic%20agents%2C%0Afeaturing%20competitive%20multi-agent%20interaction%2C%20high-fidelity%20physics%0Asimulation%2C%20and%20real-time%20rendering.%20We%20conduct%20comprehensive%20experiments%20to%0Aevaluate%20LatticeWorld%2C%20showing%20that%20it%20achieves%20superior%20accuracy%20in%20scene%0Alayout%20generation%20and%20visual%20fidelity.%20Moreover%2C%20LatticeWorld%20achieves%20over%20a%0A%2490%5Ctimes%24%20increase%20in%20industrial%20production%20efficiency%20while%20maintaining%20high%0Acreative%20quality%20compared%20with%20traditional%20manual%20production%20methods.%20Our%20demo%0Avideo%20is%20available%20at%20https%3A//youtu.be/8VWZXpERR18%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05263v2&entry.124074799=Read"},
{"title": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and\n  Inference-Time Differentiable Levenberg-Marquardt Optimization", "author": "Minheng Chen and Youyong Kong", "abstract": "  Intraoperative 2D/3D registration aligns preoperative 3D volumes with\nreal-time 2D radiographs, enabling accurate localization of instruments and\nimplants. A recent fully differentiable similarity learning framework\napproximates geodesic distances on SE(3), expanding the capture range of\nregistration and mitigating the effects of substantial disturbances, but\nexisting Euclidean approximations distort manifold structure and slow\nconvergence. To address these limitations, we explore similarity learning in\nnon-Euclidean spherical feature spaces to better capture and fit complex\nmanifold structure. We extract feature embeddings using a CNN-Transformer\nencoder, project them into spherical space, and approximate their geodesic\ndistances with Riemannian distances in the bi-invariant SO(4) space. This\nenables a more expressive and geometrically consistent deep similarity metric,\nenhancing the ability to distinguish subtle pose differences. During inference,\nwe replace gradient descent with fully differentiable Levenberg-Marquardt\noptimization to accelerate convergence. Experiments on real and synthetic\ndatasets show superior accuracy in both patient-specific and patient-agnostic\nscenarios.\n", "link": "http://arxiv.org/abs/2509.06890v1", "date": "2025-09-08", "relevancy": 2.862, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6087}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intraoperative%202D/3D%20Registration%20via%20Spherical%20Similarity%20Learning%20and%0A%20%20Inference-Time%20Differentiable%20Levenberg-Marquardt%20Optimization&body=Title%3A%20Intraoperative%202D/3D%20Registration%20via%20Spherical%20Similarity%20Learning%20and%0A%20%20Inference-Time%20Differentiable%20Levenberg-Marquardt%20Optimization%0AAuthor%3A%20Minheng%20Chen%20and%20Youyong%20Kong%0AAbstract%3A%20%20%20Intraoperative%202D/3D%20registration%20aligns%20preoperative%203D%20volumes%20with%0Areal-time%202D%20radiographs%2C%20enabling%20accurate%20localization%20of%20instruments%20and%0Aimplants.%20A%20recent%20fully%20differentiable%20similarity%20learning%20framework%0Aapproximates%20geodesic%20distances%20on%20SE%283%29%2C%20expanding%20the%20capture%20range%20of%0Aregistration%20and%20mitigating%20the%20effects%20of%20substantial%20disturbances%2C%20but%0Aexisting%20Euclidean%20approximations%20distort%20manifold%20structure%20and%20slow%0Aconvergence.%20To%20address%20these%20limitations%2C%20we%20explore%20similarity%20learning%20in%0Anon-Euclidean%20spherical%20feature%20spaces%20to%20better%20capture%20and%20fit%20complex%0Amanifold%20structure.%20We%20extract%20feature%20embeddings%20using%20a%20CNN-Transformer%0Aencoder%2C%20project%20them%20into%20spherical%20space%2C%20and%20approximate%20their%20geodesic%0Adistances%20with%20Riemannian%20distances%20in%20the%20bi-invariant%20SO%284%29%20space.%20This%0Aenables%20a%20more%20expressive%20and%20geometrically%20consistent%20deep%20similarity%20metric%2C%0Aenhancing%20the%20ability%20to%20distinguish%20subtle%20pose%20differences.%20During%20inference%2C%0Awe%20replace%20gradient%20descent%20with%20fully%20differentiable%20Levenberg-Marquardt%0Aoptimization%20to%20accelerate%20convergence.%20Experiments%20on%20real%20and%20synthetic%0Adatasets%20show%20superior%20accuracy%20in%20both%20patient-specific%20and%20patient-agnostic%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntraoperative%25202D/3D%2520Registration%2520via%2520Spherical%2520Similarity%2520Learning%2520and%250A%2520%2520Inference-Time%2520Differentiable%2520Levenberg-Marquardt%2520Optimization%26entry.906535625%3DMinheng%2520Chen%2520and%2520Youyong%2520Kong%26entry.1292438233%3D%2520%2520Intraoperative%25202D/3D%2520registration%2520aligns%2520preoperative%25203D%2520volumes%2520with%250Areal-time%25202D%2520radiographs%252C%2520enabling%2520accurate%2520localization%2520of%2520instruments%2520and%250Aimplants.%2520A%2520recent%2520fully%2520differentiable%2520similarity%2520learning%2520framework%250Aapproximates%2520geodesic%2520distances%2520on%2520SE%25283%2529%252C%2520expanding%2520the%2520capture%2520range%2520of%250Aregistration%2520and%2520mitigating%2520the%2520effects%2520of%2520substantial%2520disturbances%252C%2520but%250Aexisting%2520Euclidean%2520approximations%2520distort%2520manifold%2520structure%2520and%2520slow%250Aconvergence.%2520To%2520address%2520these%2520limitations%252C%2520we%2520explore%2520similarity%2520learning%2520in%250Anon-Euclidean%2520spherical%2520feature%2520spaces%2520to%2520better%2520capture%2520and%2520fit%2520complex%250Amanifold%2520structure.%2520We%2520extract%2520feature%2520embeddings%2520using%2520a%2520CNN-Transformer%250Aencoder%252C%2520project%2520them%2520into%2520spherical%2520space%252C%2520and%2520approximate%2520their%2520geodesic%250Adistances%2520with%2520Riemannian%2520distances%2520in%2520the%2520bi-invariant%2520SO%25284%2529%2520space.%2520This%250Aenables%2520a%2520more%2520expressive%2520and%2520geometrically%2520consistent%2520deep%2520similarity%2520metric%252C%250Aenhancing%2520the%2520ability%2520to%2520distinguish%2520subtle%2520pose%2520differences.%2520During%2520inference%252C%250Awe%2520replace%2520gradient%2520descent%2520with%2520fully%2520differentiable%2520Levenberg-Marquardt%250Aoptimization%2520to%2520accelerate%2520convergence.%2520Experiments%2520on%2520real%2520and%2520synthetic%250Adatasets%2520show%2520superior%2520accuracy%2520in%2520both%2520patient-specific%2520and%2520patient-agnostic%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intraoperative%202D/3D%20Registration%20via%20Spherical%20Similarity%20Learning%20and%0A%20%20Inference-Time%20Differentiable%20Levenberg-Marquardt%20Optimization&entry.906535625=Minheng%20Chen%20and%20Youyong%20Kong&entry.1292438233=%20%20Intraoperative%202D/3D%20registration%20aligns%20preoperative%203D%20volumes%20with%0Areal-time%202D%20radiographs%2C%20enabling%20accurate%20localization%20of%20instruments%20and%0Aimplants.%20A%20recent%20fully%20differentiable%20similarity%20learning%20framework%0Aapproximates%20geodesic%20distances%20on%20SE%283%29%2C%20expanding%20the%20capture%20range%20of%0Aregistration%20and%20mitigating%20the%20effects%20of%20substantial%20disturbances%2C%20but%0Aexisting%20Euclidean%20approximations%20distort%20manifold%20structure%20and%20slow%0Aconvergence.%20To%20address%20these%20limitations%2C%20we%20explore%20similarity%20learning%20in%0Anon-Euclidean%20spherical%20feature%20spaces%20to%20better%20capture%20and%20fit%20complex%0Amanifold%20structure.%20We%20extract%20feature%20embeddings%20using%20a%20CNN-Transformer%0Aencoder%2C%20project%20them%20into%20spherical%20space%2C%20and%20approximate%20their%20geodesic%0Adistances%20with%20Riemannian%20distances%20in%20the%20bi-invariant%20SO%284%29%20space.%20This%0Aenables%20a%20more%20expressive%20and%20geometrically%20consistent%20deep%20similarity%20metric%2C%0Aenhancing%20the%20ability%20to%20distinguish%20subtle%20pose%20differences.%20During%20inference%2C%0Awe%20replace%20gradient%20descent%20with%20fully%20differentiable%20Levenberg-Marquardt%0Aoptimization%20to%20accelerate%20convergence.%20Experiments%20on%20real%20and%20synthetic%0Adatasets%20show%20superior%20accuracy%20in%20both%20patient-specific%20and%20patient-agnostic%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06890v1&entry.124074799=Read"},
{"title": "Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with\n  Hierarchical Graph Attention", "author": "Mohamed Zayaan S", "abstract": "  We present Cortex Synth, a novel end-to-end differentiable framework for\njoint 3D skeleton geometry and topology synthesis from single 2D images. Our\narchitecture introduces three key innovations: (1) A hierarchical graph\nattention mechanism with multi-scale skeletal refinement, (2) Differentiable\nspectral topology optimization via Laplacian eigen decomposition, and (3)\nAdversarial geometric consistency training for pose structure alignment. The\nframework integrates four synergistic modules: a pseudo 3D point cloud\ngenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a\nnovel Differentiable Graph Construction Network (DGCN). Our experiments\ndemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and\n27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological\nerrors by 42 percent compared to previous approaches. The model's end-to-end\ndifferentiability enables applications in robotic manipulation, medical\nimaging, and automated character rigging.\n", "link": "http://arxiv.org/abs/2509.06705v1", "date": "2025-09-08", "relevancy": 2.8567, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6141}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5562}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cortex-Synth%3A%20Differentiable%20Topology-Aware%203D%20Skeleton%20Synthesis%20with%0A%20%20Hierarchical%20Graph%20Attention&body=Title%3A%20Cortex-Synth%3A%20Differentiable%20Topology-Aware%203D%20Skeleton%20Synthesis%20with%0A%20%20Hierarchical%20Graph%20Attention%0AAuthor%3A%20Mohamed%20Zayaan%20S%0AAbstract%3A%20%20%20We%20present%20Cortex%20Synth%2C%20a%20novel%20end-to-end%20differentiable%20framework%20for%0Ajoint%203D%20skeleton%20geometry%20and%20topology%20synthesis%20from%20single%202D%20images.%20Our%0Aarchitecture%20introduces%20three%20key%20innovations%3A%20%281%29%20A%20hierarchical%20graph%0Aattention%20mechanism%20with%20multi-scale%20skeletal%20refinement%2C%20%282%29%20Differentiable%0Aspectral%20topology%20optimization%20via%20Laplacian%20eigen%20decomposition%2C%20and%20%283%29%0AAdversarial%20geometric%20consistency%20training%20for%20pose%20structure%20alignment.%20The%0Aframework%20integrates%20four%20synergistic%20modules%3A%20a%20pseudo%203D%20point%20cloud%0Agenerator%2C%20an%20enhanced%20PointNet%20encoder%2C%20a%20skeleton%20coordinate%20decoder%2C%20and%20a%0Anovel%20Differentiable%20Graph%20Construction%20Network%20%28DGCN%29.%20Our%20experiments%0Ademonstrate%20state-of-the-art%20results%20with%2018.7%20percent%20improvement%20in%20MPJPE%20and%0A27.3%20percent%20in%20Graph%20Edit%20Distance%20on%20ShapeNet%2C%20while%20reducing%20topological%0Aerrors%20by%2042%20percent%20compared%20to%20previous%20approaches.%20The%20model%27s%20end-to-end%0Adifferentiability%20enables%20applications%20in%20robotic%20manipulation%2C%20medical%0Aimaging%2C%20and%20automated%20character%20rigging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCortex-Synth%253A%2520Differentiable%2520Topology-Aware%25203D%2520Skeleton%2520Synthesis%2520with%250A%2520%2520Hierarchical%2520Graph%2520Attention%26entry.906535625%3DMohamed%2520Zayaan%2520S%26entry.1292438233%3D%2520%2520We%2520present%2520Cortex%2520Synth%252C%2520a%2520novel%2520end-to-end%2520differentiable%2520framework%2520for%250Ajoint%25203D%2520skeleton%2520geometry%2520and%2520topology%2520synthesis%2520from%2520single%25202D%2520images.%2520Our%250Aarchitecture%2520introduces%2520three%2520key%2520innovations%253A%2520%25281%2529%2520A%2520hierarchical%2520graph%250Aattention%2520mechanism%2520with%2520multi-scale%2520skeletal%2520refinement%252C%2520%25282%2529%2520Differentiable%250Aspectral%2520topology%2520optimization%2520via%2520Laplacian%2520eigen%2520decomposition%252C%2520and%2520%25283%2529%250AAdversarial%2520geometric%2520consistency%2520training%2520for%2520pose%2520structure%2520alignment.%2520The%250Aframework%2520integrates%2520four%2520synergistic%2520modules%253A%2520a%2520pseudo%25203D%2520point%2520cloud%250Agenerator%252C%2520an%2520enhanced%2520PointNet%2520encoder%252C%2520a%2520skeleton%2520coordinate%2520decoder%252C%2520and%2520a%250Anovel%2520Differentiable%2520Graph%2520Construction%2520Network%2520%2528DGCN%2529.%2520Our%2520experiments%250Ademonstrate%2520state-of-the-art%2520results%2520with%252018.7%2520percent%2520improvement%2520in%2520MPJPE%2520and%250A27.3%2520percent%2520in%2520Graph%2520Edit%2520Distance%2520on%2520ShapeNet%252C%2520while%2520reducing%2520topological%250Aerrors%2520by%252042%2520percent%2520compared%2520to%2520previous%2520approaches.%2520The%2520model%2527s%2520end-to-end%250Adifferentiability%2520enables%2520applications%2520in%2520robotic%2520manipulation%252C%2520medical%250Aimaging%252C%2520and%2520automated%2520character%2520rigging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cortex-Synth%3A%20Differentiable%20Topology-Aware%203D%20Skeleton%20Synthesis%20with%0A%20%20Hierarchical%20Graph%20Attention&entry.906535625=Mohamed%20Zayaan%20S&entry.1292438233=%20%20We%20present%20Cortex%20Synth%2C%20a%20novel%20end-to-end%20differentiable%20framework%20for%0Ajoint%203D%20skeleton%20geometry%20and%20topology%20synthesis%20from%20single%202D%20images.%20Our%0Aarchitecture%20introduces%20three%20key%20innovations%3A%20%281%29%20A%20hierarchical%20graph%0Aattention%20mechanism%20with%20multi-scale%20skeletal%20refinement%2C%20%282%29%20Differentiable%0Aspectral%20topology%20optimization%20via%20Laplacian%20eigen%20decomposition%2C%20and%20%283%29%0AAdversarial%20geometric%20consistency%20training%20for%20pose%20structure%20alignment.%20The%0Aframework%20integrates%20four%20synergistic%20modules%3A%20a%20pseudo%203D%20point%20cloud%0Agenerator%2C%20an%20enhanced%20PointNet%20encoder%2C%20a%20skeleton%20coordinate%20decoder%2C%20and%20a%0Anovel%20Differentiable%20Graph%20Construction%20Network%20%28DGCN%29.%20Our%20experiments%0Ademonstrate%20state-of-the-art%20results%20with%2018.7%20percent%20improvement%20in%20MPJPE%20and%0A27.3%20percent%20in%20Graph%20Edit%20Distance%20on%20ShapeNet%2C%20while%20reducing%20topological%0Aerrors%20by%2042%20percent%20compared%20to%20previous%20approaches.%20The%20model%27s%20end-to-end%0Adifferentiability%20enables%20applications%20in%20robotic%20manipulation%2C%20medical%0Aimaging%2C%20and%20automated%20character%20rigging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06705v1&entry.124074799=Read"},
{"title": "Aligning Large Vision-Language Models by Deep Reinforcement Learning and\n  Direct Preference Optimization", "author": "Thanh Thi Nguyen and Campbell Wilson and Janis Dalins", "abstract": "  Large Vision-Language Models (LVLMs) or multimodal large language models\nrepresent a significant advancement in artificial intelligence, enabling\nsystems to understand and generate content across both visual and textual\nmodalities. While large-scale pretraining has driven substantial progress,\nfine-tuning these models for aligning with human values or engaging in specific\ntasks or behaviors remains a critical challenge. Deep Reinforcement Learning\n(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for\nthis aligning process. While DRL enables models to optimize actions using\nreward signals instead of relying solely on supervised preference data, DPO\ndirectly aligns the policy with preferences, eliminating the need for an\nexplicit reward model. This overview explores paradigms for fine-tuning LVLMs,\nhighlighting how DRL and DPO techniques can be used to align models with human\npreferences and values, improve task performance, and enable adaptive\nmultimodal interaction. We categorize key approaches, examine sources of\npreference data, reward signals, and discuss open challenges such as\nscalability, sample efficiency, continual learning, generalization, and safety.\nThe goal is to provide a clear understanding of how DRL and DPO contribute to\nthe evolution of robust and human-aligned LVLMs.\n", "link": "http://arxiv.org/abs/2509.06759v1", "date": "2025-09-08", "relevancy": 2.8444, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Large%20Vision-Language%20Models%20by%20Deep%20Reinforcement%20Learning%20and%0A%20%20Direct%20Preference%20Optimization&body=Title%3A%20Aligning%20Large%20Vision-Language%20Models%20by%20Deep%20Reinforcement%20Learning%20and%0A%20%20Direct%20Preference%20Optimization%0AAuthor%3A%20Thanh%20Thi%20Nguyen%20and%20Campbell%20Wilson%20and%20Janis%20Dalins%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20or%20multimodal%20large%20language%20models%0Arepresent%20a%20significant%20advancement%20in%20artificial%20intelligence%2C%20enabling%0Asystems%20to%20understand%20and%20generate%20content%20across%20both%20visual%20and%20textual%0Amodalities.%20While%20large-scale%20pretraining%20has%20driven%20substantial%20progress%2C%0Afine-tuning%20these%20models%20for%20aligning%20with%20human%20values%20or%20engaging%20in%20specific%0Atasks%20or%20behaviors%20remains%20a%20critical%20challenge.%20Deep%20Reinforcement%20Learning%0A%28DRL%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20offer%20promising%20frameworks%20for%0Athis%20aligning%20process.%20While%20DRL%20enables%20models%20to%20optimize%20actions%20using%0Areward%20signals%20instead%20of%20relying%20solely%20on%20supervised%20preference%20data%2C%20DPO%0Adirectly%20aligns%20the%20policy%20with%20preferences%2C%20eliminating%20the%20need%20for%20an%0Aexplicit%20reward%20model.%20This%20overview%20explores%20paradigms%20for%20fine-tuning%20LVLMs%2C%0Ahighlighting%20how%20DRL%20and%20DPO%20techniques%20can%20be%20used%20to%20align%20models%20with%20human%0Apreferences%20and%20values%2C%20improve%20task%20performance%2C%20and%20enable%20adaptive%0Amultimodal%20interaction.%20We%20categorize%20key%20approaches%2C%20examine%20sources%20of%0Apreference%20data%2C%20reward%20signals%2C%20and%20discuss%20open%20challenges%20such%20as%0Ascalability%2C%20sample%20efficiency%2C%20continual%20learning%2C%20generalization%2C%20and%20safety.%0AThe%20goal%20is%20to%20provide%20a%20clear%20understanding%20of%20how%20DRL%20and%20DPO%20contribute%20to%0Athe%20evolution%20of%20robust%20and%20human-aligned%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Large%2520Vision-Language%2520Models%2520by%2520Deep%2520Reinforcement%2520Learning%2520and%250A%2520%2520Direct%2520Preference%2520Optimization%26entry.906535625%3DThanh%2520Thi%2520Nguyen%2520and%2520Campbell%2520Wilson%2520and%2520Janis%2520Dalins%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520or%2520multimodal%2520large%2520language%2520models%250Arepresent%2520a%2520significant%2520advancement%2520in%2520artificial%2520intelligence%252C%2520enabling%250Asystems%2520to%2520understand%2520and%2520generate%2520content%2520across%2520both%2520visual%2520and%2520textual%250Amodalities.%2520While%2520large-scale%2520pretraining%2520has%2520driven%2520substantial%2520progress%252C%250Afine-tuning%2520these%2520models%2520for%2520aligning%2520with%2520human%2520values%2520or%2520engaging%2520in%2520specific%250Atasks%2520or%2520behaviors%2520remains%2520a%2520critical%2520challenge.%2520Deep%2520Reinforcement%2520Learning%250A%2528DRL%2529%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520offer%2520promising%2520frameworks%2520for%250Athis%2520aligning%2520process.%2520While%2520DRL%2520enables%2520models%2520to%2520optimize%2520actions%2520using%250Areward%2520signals%2520instead%2520of%2520relying%2520solely%2520on%2520supervised%2520preference%2520data%252C%2520DPO%250Adirectly%2520aligns%2520the%2520policy%2520with%2520preferences%252C%2520eliminating%2520the%2520need%2520for%2520an%250Aexplicit%2520reward%2520model.%2520This%2520overview%2520explores%2520paradigms%2520for%2520fine-tuning%2520LVLMs%252C%250Ahighlighting%2520how%2520DRL%2520and%2520DPO%2520techniques%2520can%2520be%2520used%2520to%2520align%2520models%2520with%2520human%250Apreferences%2520and%2520values%252C%2520improve%2520task%2520performance%252C%2520and%2520enable%2520adaptive%250Amultimodal%2520interaction.%2520We%2520categorize%2520key%2520approaches%252C%2520examine%2520sources%2520of%250Apreference%2520data%252C%2520reward%2520signals%252C%2520and%2520discuss%2520open%2520challenges%2520such%2520as%250Ascalability%252C%2520sample%2520efficiency%252C%2520continual%2520learning%252C%2520generalization%252C%2520and%2520safety.%250AThe%2520goal%2520is%2520to%2520provide%2520a%2520clear%2520understanding%2520of%2520how%2520DRL%2520and%2520DPO%2520contribute%2520to%250Athe%2520evolution%2520of%2520robust%2520and%2520human-aligned%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Large%20Vision-Language%20Models%20by%20Deep%20Reinforcement%20Learning%20and%0A%20%20Direct%20Preference%20Optimization&entry.906535625=Thanh%20Thi%20Nguyen%20and%20Campbell%20Wilson%20and%20Janis%20Dalins&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20or%20multimodal%20large%20language%20models%0Arepresent%20a%20significant%20advancement%20in%20artificial%20intelligence%2C%20enabling%0Asystems%20to%20understand%20and%20generate%20content%20across%20both%20visual%20and%20textual%0Amodalities.%20While%20large-scale%20pretraining%20has%20driven%20substantial%20progress%2C%0Afine-tuning%20these%20models%20for%20aligning%20with%20human%20values%20or%20engaging%20in%20specific%0Atasks%20or%20behaviors%20remains%20a%20critical%20challenge.%20Deep%20Reinforcement%20Learning%0A%28DRL%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20offer%20promising%20frameworks%20for%0Athis%20aligning%20process.%20While%20DRL%20enables%20models%20to%20optimize%20actions%20using%0Areward%20signals%20instead%20of%20relying%20solely%20on%20supervised%20preference%20data%2C%20DPO%0Adirectly%20aligns%20the%20policy%20with%20preferences%2C%20eliminating%20the%20need%20for%20an%0Aexplicit%20reward%20model.%20This%20overview%20explores%20paradigms%20for%20fine-tuning%20LVLMs%2C%0Ahighlighting%20how%20DRL%20and%20DPO%20techniques%20can%20be%20used%20to%20align%20models%20with%20human%0Apreferences%20and%20values%2C%20improve%20task%20performance%2C%20and%20enable%20adaptive%0Amultimodal%20interaction.%20We%20categorize%20key%20approaches%2C%20examine%20sources%20of%0Apreference%20data%2C%20reward%20signals%2C%20and%20discuss%20open%20challenges%20such%20as%0Ascalability%2C%20sample%20efficiency%2C%20continual%20learning%2C%20generalization%2C%20and%20safety.%0AThe%20goal%20is%20to%20provide%20a%20clear%20understanding%20of%20how%20DRL%20and%20DPO%20contribute%20to%0Athe%20evolution%20of%20robust%20and%20human-aligned%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06759v1&entry.124074799=Read"},
{"title": "IMAGGarment: Fine-Grained Garment Generation for Controllable Fashion\n  Design", "author": "Fei Shen and Jian Yu and Cong Wang and Xin Jiang and Xiaoyu Du and Jinhui Tang", "abstract": "  This paper presents IMAGGarment, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nCode, models, and datasets are publicly available at\nhttps://github.com/muzishen/IMAGGarment.\n", "link": "http://arxiv.org/abs/2504.13176v2", "date": "2025-09-08", "relevancy": 2.8395, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7401}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.697}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMAGGarment%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design&body=Title%3A%20IMAGGarment%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design%0AAuthor%3A%20Fei%20Shen%20and%20Jian%20Yu%20and%20Cong%20Wang%20and%20Xin%20Jiang%20and%20Xiaoyu%20Du%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20This%20paper%20presents%20IMAGGarment%2C%20a%20fine-grained%20garment%20generation%20%28FGG%29%0Aframework%20that%20enables%20high-fidelity%20garment%20synthesis%20with%20precise%20control%0Aover%20silhouette%2C%20color%2C%20and%20logo%20placement.%20Unlike%20existing%20methods%20that%20are%0Alimited%20to%20single-condition%20inputs%2C%20IMAGGarment%20addresses%20the%20challenges%20of%0Amulti-conditional%20controllability%20in%20personalized%20fashion%20design%20and%20digital%0Aapparel%20applications.%20Specifically%2C%20IMAGGarment%20employs%20a%20two-stage%20training%0Astrategy%20to%20separately%20model%20global%20appearance%20and%20local%20details%2C%20while%0Aenabling%20unified%20and%20controllable%20generation%20through%20end-to-end%20inference.%20In%0Athe%20first%20stage%2C%20we%20propose%20a%20global%20appearance%20model%20that%20jointly%20encodes%0Asilhouette%20and%20color%20using%20a%20mixed%20attention%20module%20and%20a%20color%20adapter.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20local%20enhancement%20model%20with%20an%20adaptive%0Aappearance-aware%20module%20to%20inject%20user-defined%20logos%20and%20spatial%20constraints%2C%0Aenabling%20accurate%20placement%20and%20visual%20consistency.%20To%20support%20this%20task%2C%20we%0Arelease%20GarmentBench%2C%20a%20large-scale%20dataset%20comprising%20over%20180K%20garment%0Asamples%20paired%20with%20multi-level%20design%20conditions%2C%20including%20sketches%2C%20color%0Areferences%2C%20logo%20placements%2C%20and%20textual%20prompts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Astructural%20stability%2C%20color%20fidelity%2C%20and%20local%20controllability%20performance.%0ACode%2C%20models%2C%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/muzishen/IMAGGarment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMAGGarment%253A%2520Fine-Grained%2520Garment%2520Generation%2520for%2520Controllable%2520Fashion%250A%2520%2520Design%26entry.906535625%3DFei%2520Shen%2520and%2520Jian%2520Yu%2520and%2520Cong%2520Wang%2520and%2520Xin%2520Jiang%2520and%2520Xiaoyu%2520Du%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520IMAGGarment%252C%2520a%2520fine-grained%2520garment%2520generation%2520%2528FGG%2529%250Aframework%2520that%2520enables%2520high-fidelity%2520garment%2520synthesis%2520with%2520precise%2520control%250Aover%2520silhouette%252C%2520color%252C%2520and%2520logo%2520placement.%2520Unlike%2520existing%2520methods%2520that%2520are%250Alimited%2520to%2520single-condition%2520inputs%252C%2520IMAGGarment%2520addresses%2520the%2520challenges%2520of%250Amulti-conditional%2520controllability%2520in%2520personalized%2520fashion%2520design%2520and%2520digital%250Aapparel%2520applications.%2520Specifically%252C%2520IMAGGarment%2520employs%2520a%2520two-stage%2520training%250Astrategy%2520to%2520separately%2520model%2520global%2520appearance%2520and%2520local%2520details%252C%2520while%250Aenabling%2520unified%2520and%2520controllable%2520generation%2520through%2520end-to-end%2520inference.%2520In%250Athe%2520first%2520stage%252C%2520we%2520propose%2520a%2520global%2520appearance%2520model%2520that%2520jointly%2520encodes%250Asilhouette%2520and%2520color%2520using%2520a%2520mixed%2520attention%2520module%2520and%2520a%2520color%2520adapter.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520present%2520a%2520local%2520enhancement%2520model%2520with%2520an%2520adaptive%250Aappearance-aware%2520module%2520to%2520inject%2520user-defined%2520logos%2520and%2520spatial%2520constraints%252C%250Aenabling%2520accurate%2520placement%2520and%2520visual%2520consistency.%2520To%2520support%2520this%2520task%252C%2520we%250Arelease%2520GarmentBench%252C%2520a%2520large-scale%2520dataset%2520comprising%2520over%2520180K%2520garment%250Asamples%2520paired%2520with%2520multi-level%2520design%2520conditions%252C%2520including%2520sketches%252C%2520color%250Areferences%252C%2520logo%2520placements%252C%2520and%2520textual%2520prompts.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520baselines%252C%2520achieving%2520superior%250Astructural%2520stability%252C%2520color%2520fidelity%252C%2520and%2520local%2520controllability%2520performance.%250ACode%252C%2520models%252C%2520and%2520datasets%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/muzishen/IMAGGarment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMAGGarment%3A%20Fine-Grained%20Garment%20Generation%20for%20Controllable%20Fashion%0A%20%20Design&entry.906535625=Fei%20Shen%20and%20Jian%20Yu%20and%20Cong%20Wang%20and%20Xin%20Jiang%20and%20Xiaoyu%20Du%20and%20Jinhui%20Tang&entry.1292438233=%20%20This%20paper%20presents%20IMAGGarment%2C%20a%20fine-grained%20garment%20generation%20%28FGG%29%0Aframework%20that%20enables%20high-fidelity%20garment%20synthesis%20with%20precise%20control%0Aover%20silhouette%2C%20color%2C%20and%20logo%20placement.%20Unlike%20existing%20methods%20that%20are%0Alimited%20to%20single-condition%20inputs%2C%20IMAGGarment%20addresses%20the%20challenges%20of%0Amulti-conditional%20controllability%20in%20personalized%20fashion%20design%20and%20digital%0Aapparel%20applications.%20Specifically%2C%20IMAGGarment%20employs%20a%20two-stage%20training%0Astrategy%20to%20separately%20model%20global%20appearance%20and%20local%20details%2C%20while%0Aenabling%20unified%20and%20controllable%20generation%20through%20end-to-end%20inference.%20In%0Athe%20first%20stage%2C%20we%20propose%20a%20global%20appearance%20model%20that%20jointly%20encodes%0Asilhouette%20and%20color%20using%20a%20mixed%20attention%20module%20and%20a%20color%20adapter.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20local%20enhancement%20model%20with%20an%20adaptive%0Aappearance-aware%20module%20to%20inject%20user-defined%20logos%20and%20spatial%20constraints%2C%0Aenabling%20accurate%20placement%20and%20visual%20consistency.%20To%20support%20this%20task%2C%20we%0Arelease%20GarmentBench%2C%20a%20large-scale%20dataset%20comprising%20over%20180K%20garment%0Asamples%20paired%20with%20multi-level%20design%20conditions%2C%20including%20sketches%2C%20color%0Areferences%2C%20logo%20placements%2C%20and%20textual%20prompts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Astructural%20stability%2C%20color%20fidelity%2C%20and%20local%20controllability%20performance.%0ACode%2C%20models%2C%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/muzishen/IMAGGarment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13176v2&entry.124074799=Read"},
{"title": "Leveraging Generic Foundation Models for Multimodal Surgical Data\n  Analysis", "author": "Simon Pezold and J\u00e9r\u00f4me A. Kurylec and Jan S. Liechti and Beat P. M\u00fcller and Jo\u00ebl L. Lavanchy", "abstract": "  We investigate how both the adaptation of a generic foundation model via\ntransfer learning and the integration of complementary modalities from the\noperating room (OR) can support surgical data science. To this end, we use\nV-JEPA as the single-modality foundation of a multimodal model for minimally\ninvasive surgery support. We analyze how the model's downstream performance can\nbenefit (a) from finetuning on unlabeled surgical video data and (b) from\nproviding additional time-resolved data streams from the OR in a multimodal\nsetup.\n  In an in-house dataset of liver surgery videos, we analyze the tasks of\npredicting hospital length of stay and postoperative complications. In videos\nof the public HeiCo dataset, we analyze the task of surgical phase recognition.\nAs a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on\nunlabeled, held-out videos to investigate its change in performance after\ndomain adaptation. Following the idea of modular decision support networks, we\nintegrate additional data streams from the OR by training a separate encoder to\nform a shared representation space with V-JEPA's embeddings.\n  Our experiments show that finetuning on domain-specific data increases model\nperformance. On the in-house data, integrating additional time-resolved data\nlikewise benefits the model. On the HeiCo data, accuracy of the pretrained\nvideo-only, single-modality baseline setup is on par with the top-performing\nsubmissions of the EndoVis2017 challenge, while finetuning on domain-specific\ndata increases accuracy further. Our results thus demonstrate how surgical data\nscience can leverage public, generic foundation models. Likewise, they indicate\nthe potential of domain adaptation and of integrating suitable complementary\ndata streams from the OR. To support further research, we release our code and\nmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.\n", "link": "http://arxiv.org/abs/2509.06831v1", "date": "2025-09-08", "relevancy": 2.8246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5702}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Generic%20Foundation%20Models%20for%20Multimodal%20Surgical%20Data%0A%20%20Analysis&body=Title%3A%20Leveraging%20Generic%20Foundation%20Models%20for%20Multimodal%20Surgical%20Data%0A%20%20Analysis%0AAuthor%3A%20Simon%20Pezold%20and%20J%C3%A9r%C3%B4me%20A.%20Kurylec%20and%20Jan%20S.%20Liechti%20and%20Beat%20P.%20M%C3%BCller%20and%20Jo%C3%ABl%20L.%20Lavanchy%0AAbstract%3A%20%20%20We%20investigate%20how%20both%20the%20adaptation%20of%20a%20generic%20foundation%20model%20via%0Atransfer%20learning%20and%20the%20integration%20of%20complementary%20modalities%20from%20the%0Aoperating%20room%20%28OR%29%20can%20support%20surgical%20data%20science.%20To%20this%20end%2C%20we%20use%0AV-JEPA%20as%20the%20single-modality%20foundation%20of%20a%20multimodal%20model%20for%20minimally%0Ainvasive%20surgery%20support.%20We%20analyze%20how%20the%20model%27s%20downstream%20performance%20can%0Abenefit%20%28a%29%20from%20finetuning%20on%20unlabeled%20surgical%20video%20data%20and%20%28b%29%20from%0Aproviding%20additional%20time-resolved%20data%20streams%20from%20the%20OR%20in%20a%20multimodal%0Asetup.%0A%20%20In%20an%20in-house%20dataset%20of%20liver%20surgery%20videos%2C%20we%20analyze%20the%20tasks%20of%0Apredicting%20hospital%20length%20of%20stay%20and%20postoperative%20complications.%20In%20videos%0Aof%20the%20public%20HeiCo%20dataset%2C%20we%20analyze%20the%20task%20of%20surgical%20phase%20recognition.%0AAs%20a%20baseline%2C%20we%20apply%20pretrained%20V-JEPA%20to%20all%20tasks.%20We%20then%20finetune%20it%20on%0Aunlabeled%2C%20held-out%20videos%20to%20investigate%20its%20change%20in%20performance%20after%0Adomain%20adaptation.%20Following%20the%20idea%20of%20modular%20decision%20support%20networks%2C%20we%0Aintegrate%20additional%20data%20streams%20from%20the%20OR%20by%20training%20a%20separate%20encoder%20to%0Aform%20a%20shared%20representation%20space%20with%20V-JEPA%27s%20embeddings.%0A%20%20Our%20experiments%20show%20that%20finetuning%20on%20domain-specific%20data%20increases%20model%0Aperformance.%20On%20the%20in-house%20data%2C%20integrating%20additional%20time-resolved%20data%0Alikewise%20benefits%20the%20model.%20On%20the%20HeiCo%20data%2C%20accuracy%20of%20the%20pretrained%0Avideo-only%2C%20single-modality%20baseline%20setup%20is%20on%20par%20with%20the%20top-performing%0Asubmissions%20of%20the%20EndoVis2017%20challenge%2C%20while%20finetuning%20on%20domain-specific%0Adata%20increases%20accuracy%20further.%20Our%20results%20thus%20demonstrate%20how%20surgical%20data%0Ascience%20can%20leverage%20public%2C%20generic%20foundation%20models.%20Likewise%2C%20they%20indicate%0Athe%20potential%20of%20domain%20adaptation%20and%20of%20integrating%20suitable%20complementary%0Adata%20streams%20from%20the%20OR.%20To%20support%20further%20research%2C%20we%20release%20our%20code%20and%0Amodel%20weights%20at%20https%3A//github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Generic%2520Foundation%2520Models%2520for%2520Multimodal%2520Surgical%2520Data%250A%2520%2520Analysis%26entry.906535625%3DSimon%2520Pezold%2520and%2520J%25C3%25A9r%25C3%25B4me%2520A.%2520Kurylec%2520and%2520Jan%2520S.%2520Liechti%2520and%2520Beat%2520P.%2520M%25C3%25BCller%2520and%2520Jo%25C3%25ABl%2520L.%2520Lavanchy%26entry.1292438233%3D%2520%2520We%2520investigate%2520how%2520both%2520the%2520adaptation%2520of%2520a%2520generic%2520foundation%2520model%2520via%250Atransfer%2520learning%2520and%2520the%2520integration%2520of%2520complementary%2520modalities%2520from%2520the%250Aoperating%2520room%2520%2528OR%2529%2520can%2520support%2520surgical%2520data%2520science.%2520To%2520this%2520end%252C%2520we%2520use%250AV-JEPA%2520as%2520the%2520single-modality%2520foundation%2520of%2520a%2520multimodal%2520model%2520for%2520minimally%250Ainvasive%2520surgery%2520support.%2520We%2520analyze%2520how%2520the%2520model%2527s%2520downstream%2520performance%2520can%250Abenefit%2520%2528a%2529%2520from%2520finetuning%2520on%2520unlabeled%2520surgical%2520video%2520data%2520and%2520%2528b%2529%2520from%250Aproviding%2520additional%2520time-resolved%2520data%2520streams%2520from%2520the%2520OR%2520in%2520a%2520multimodal%250Asetup.%250A%2520%2520In%2520an%2520in-house%2520dataset%2520of%2520liver%2520surgery%2520videos%252C%2520we%2520analyze%2520the%2520tasks%2520of%250Apredicting%2520hospital%2520length%2520of%2520stay%2520and%2520postoperative%2520complications.%2520In%2520videos%250Aof%2520the%2520public%2520HeiCo%2520dataset%252C%2520we%2520analyze%2520the%2520task%2520of%2520surgical%2520phase%2520recognition.%250AAs%2520a%2520baseline%252C%2520we%2520apply%2520pretrained%2520V-JEPA%2520to%2520all%2520tasks.%2520We%2520then%2520finetune%2520it%2520on%250Aunlabeled%252C%2520held-out%2520videos%2520to%2520investigate%2520its%2520change%2520in%2520performance%2520after%250Adomain%2520adaptation.%2520Following%2520the%2520idea%2520of%2520modular%2520decision%2520support%2520networks%252C%2520we%250Aintegrate%2520additional%2520data%2520streams%2520from%2520the%2520OR%2520by%2520training%2520a%2520separate%2520encoder%2520to%250Aform%2520a%2520shared%2520representation%2520space%2520with%2520V-JEPA%2527s%2520embeddings.%250A%2520%2520Our%2520experiments%2520show%2520that%2520finetuning%2520on%2520domain-specific%2520data%2520increases%2520model%250Aperformance.%2520On%2520the%2520in-house%2520data%252C%2520integrating%2520additional%2520time-resolved%2520data%250Alikewise%2520benefits%2520the%2520model.%2520On%2520the%2520HeiCo%2520data%252C%2520accuracy%2520of%2520the%2520pretrained%250Avideo-only%252C%2520single-modality%2520baseline%2520setup%2520is%2520on%2520par%2520with%2520the%2520top-performing%250Asubmissions%2520of%2520the%2520EndoVis2017%2520challenge%252C%2520while%2520finetuning%2520on%2520domain-specific%250Adata%2520increases%2520accuracy%2520further.%2520Our%2520results%2520thus%2520demonstrate%2520how%2520surgical%2520data%250Ascience%2520can%2520leverage%2520public%252C%2520generic%2520foundation%2520models.%2520Likewise%252C%2520they%2520indicate%250Athe%2520potential%2520of%2520domain%2520adaptation%2520and%2520of%2520integrating%2520suitable%2520complementary%250Adata%2520streams%2520from%2520the%2520OR.%2520To%2520support%2520further%2520research%252C%2520we%2520release%2520our%2520code%2520and%250Amodel%2520weights%2520at%2520https%253A//github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Generic%20Foundation%20Models%20for%20Multimodal%20Surgical%20Data%0A%20%20Analysis&entry.906535625=Simon%20Pezold%20and%20J%C3%A9r%C3%B4me%20A.%20Kurylec%20and%20Jan%20S.%20Liechti%20and%20Beat%20P.%20M%C3%BCller%20and%20Jo%C3%ABl%20L.%20Lavanchy&entry.1292438233=%20%20We%20investigate%20how%20both%20the%20adaptation%20of%20a%20generic%20foundation%20model%20via%0Atransfer%20learning%20and%20the%20integration%20of%20complementary%20modalities%20from%20the%0Aoperating%20room%20%28OR%29%20can%20support%20surgical%20data%20science.%20To%20this%20end%2C%20we%20use%0AV-JEPA%20as%20the%20single-modality%20foundation%20of%20a%20multimodal%20model%20for%20minimally%0Ainvasive%20surgery%20support.%20We%20analyze%20how%20the%20model%27s%20downstream%20performance%20can%0Abenefit%20%28a%29%20from%20finetuning%20on%20unlabeled%20surgical%20video%20data%20and%20%28b%29%20from%0Aproviding%20additional%20time-resolved%20data%20streams%20from%20the%20OR%20in%20a%20multimodal%0Asetup.%0A%20%20In%20an%20in-house%20dataset%20of%20liver%20surgery%20videos%2C%20we%20analyze%20the%20tasks%20of%0Apredicting%20hospital%20length%20of%20stay%20and%20postoperative%20complications.%20In%20videos%0Aof%20the%20public%20HeiCo%20dataset%2C%20we%20analyze%20the%20task%20of%20surgical%20phase%20recognition.%0AAs%20a%20baseline%2C%20we%20apply%20pretrained%20V-JEPA%20to%20all%20tasks.%20We%20then%20finetune%20it%20on%0Aunlabeled%2C%20held-out%20videos%20to%20investigate%20its%20change%20in%20performance%20after%0Adomain%20adaptation.%20Following%20the%20idea%20of%20modular%20decision%20support%20networks%2C%20we%0Aintegrate%20additional%20data%20streams%20from%20the%20OR%20by%20training%20a%20separate%20encoder%20to%0Aform%20a%20shared%20representation%20space%20with%20V-JEPA%27s%20embeddings.%0A%20%20Our%20experiments%20show%20that%20finetuning%20on%20domain-specific%20data%20increases%20model%0Aperformance.%20On%20the%20in-house%20data%2C%20integrating%20additional%20time-resolved%20data%0Alikewise%20benefits%20the%20model.%20On%20the%20HeiCo%20data%2C%20accuracy%20of%20the%20pretrained%0Avideo-only%2C%20single-modality%20baseline%20setup%20is%20on%20par%20with%20the%20top-performing%0Asubmissions%20of%20the%20EndoVis2017%20challenge%2C%20while%20finetuning%20on%20domain-specific%0Adata%20increases%20accuracy%20further.%20Our%20results%20thus%20demonstrate%20how%20surgical%20data%0Ascience%20can%20leverage%20public%2C%20generic%20foundation%20models.%20Likewise%2C%20they%20indicate%0Athe%20potential%20of%20domain%20adaptation%20and%20of%20integrating%20suitable%20complementary%0Adata%20streams%20from%20the%20OR.%20To%20support%20further%20research%2C%20we%20release%20our%20code%20and%0Amodel%20weights%20at%20https%3A//github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06831v1&entry.124074799=Read"},
{"title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation\n  to Actions", "author": "Qi Lv and Weijie Kong and Hao Li and Jia Zeng and Zherui Qiu and Delin Qu and Haoming Song and Qizhi Chen and Xiang Deng and Jiangmiao Pang", "abstract": "  Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.\n", "link": "http://arxiv.org/abs/2509.06951v1", "date": "2025-09-08", "relevancy": 2.8242, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5777}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F1%3A%20A%20Vision-Language-Action%20Model%20Bridging%20Understanding%20and%20Generation%0A%20%20to%20Actions&body=Title%3A%20F1%3A%20A%20Vision-Language-Action%20Model%20Bridging%20Understanding%20and%20Generation%0A%20%20to%20Actions%0AAuthor%3A%20Qi%20Lv%20and%20Weijie%20Kong%20and%20Hao%20Li%20and%20Jia%20Zeng%20and%20Zherui%20Qiu%20and%20Delin%20Qu%20and%20Haoming%20Song%20and%20Qizhi%20Chen%20and%20Xiang%20Deng%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Executing%20language-conditioned%20tasks%20in%20dynamic%20visual%20environments%20remains%20a%0Acentral%20challenge%20in%20embodied%20AI.%20Existing%20Vision-Language-Action%20%28VLA%29%20models%0Apredominantly%20adopt%20reactive%20state-to-action%20mappings%2C%20often%20leading%20to%0Ashort-sighted%20behaviors%20and%20poor%20robustness%20in%20dynamic%20scenes.%20In%20this%20paper%2C%0Awe%20introduce%20F1%2C%20a%20pretrained%20VLA%20framework%20which%20integrates%20the%20visual%0Aforesight%20generation%20into%20decision-making%20pipeline.%20F1%20adopts%20a%0AMixture-of-Transformer%20architecture%20with%20dedicated%20modules%20for%20perception%2C%0Aforesight%20generation%2C%20and%20control%2C%20thereby%20bridging%20understanding%2C%20generation%2C%0Aand%20actions.%20At%20its%20core%2C%20F1%20employs%20a%20next-scale%20prediction%20mechanism%20to%0Asynthesize%20goal-conditioned%20visual%20foresight%20as%20explicit%20planning%20targets.%20By%0Aforecasting%20plausible%20future%20visual%20states%2C%20F1%20reformulates%20action%20generation%0Aas%20a%20foresight-guided%20inverse%20dynamics%20problem%2C%20enabling%20actions%20that%0Aimplicitly%20achieve%20visual%20goals.%20To%20endow%20F1%20with%20robust%20and%20generalizable%0Acapabilities%2C%20we%20propose%20a%20three-stage%20training%20recipe%20on%20an%20extensive%20dataset%0Acomprising%20over%20330k%20trajectories%20across%20136%20diverse%20tasks.%20This%20training%0Ascheme%20enhances%20modular%20reasoning%20and%20equips%20the%20model%20with%20transferable%20visual%0Aforesight%2C%20which%20is%20critical%20for%20complex%20and%20dynamic%20environments.%20Extensive%0Aevaluations%20on%20real-world%20tasks%20and%20simulation%20benchmarks%20demonstrate%20F1%0Aconsistently%20outperforms%20existing%20approaches%2C%20achieving%20substantial%20gains%20in%0Aboth%20task%20success%20rate%20and%20generalization%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF1%253A%2520A%2520Vision-Language-Action%2520Model%2520Bridging%2520Understanding%2520and%2520Generation%250A%2520%2520to%2520Actions%26entry.906535625%3DQi%2520Lv%2520and%2520Weijie%2520Kong%2520and%2520Hao%2520Li%2520and%2520Jia%2520Zeng%2520and%2520Zherui%2520Qiu%2520and%2520Delin%2520Qu%2520and%2520Haoming%2520Song%2520and%2520Qizhi%2520Chen%2520and%2520Xiang%2520Deng%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Executing%2520language-conditioned%2520tasks%2520in%2520dynamic%2520visual%2520environments%2520remains%2520a%250Acentral%2520challenge%2520in%2520embodied%2520AI.%2520Existing%2520Vision-Language-Action%2520%2528VLA%2529%2520models%250Apredominantly%2520adopt%2520reactive%2520state-to-action%2520mappings%252C%2520often%2520leading%2520to%250Ashort-sighted%2520behaviors%2520and%2520poor%2520robustness%2520in%2520dynamic%2520scenes.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520F1%252C%2520a%2520pretrained%2520VLA%2520framework%2520which%2520integrates%2520the%2520visual%250Aforesight%2520generation%2520into%2520decision-making%2520pipeline.%2520F1%2520adopts%2520a%250AMixture-of-Transformer%2520architecture%2520with%2520dedicated%2520modules%2520for%2520perception%252C%250Aforesight%2520generation%252C%2520and%2520control%252C%2520thereby%2520bridging%2520understanding%252C%2520generation%252C%250Aand%2520actions.%2520At%2520its%2520core%252C%2520F1%2520employs%2520a%2520next-scale%2520prediction%2520mechanism%2520to%250Asynthesize%2520goal-conditioned%2520visual%2520foresight%2520as%2520explicit%2520planning%2520targets.%2520By%250Aforecasting%2520plausible%2520future%2520visual%2520states%252C%2520F1%2520reformulates%2520action%2520generation%250Aas%2520a%2520foresight-guided%2520inverse%2520dynamics%2520problem%252C%2520enabling%2520actions%2520that%250Aimplicitly%2520achieve%2520visual%2520goals.%2520To%2520endow%2520F1%2520with%2520robust%2520and%2520generalizable%250Acapabilities%252C%2520we%2520propose%2520a%2520three-stage%2520training%2520recipe%2520on%2520an%2520extensive%2520dataset%250Acomprising%2520over%2520330k%2520trajectories%2520across%2520136%2520diverse%2520tasks.%2520This%2520training%250Ascheme%2520enhances%2520modular%2520reasoning%2520and%2520equips%2520the%2520model%2520with%2520transferable%2520visual%250Aforesight%252C%2520which%2520is%2520critical%2520for%2520complex%2520and%2520dynamic%2520environments.%2520Extensive%250Aevaluations%2520on%2520real-world%2520tasks%2520and%2520simulation%2520benchmarks%2520demonstrate%2520F1%250Aconsistently%2520outperforms%2520existing%2520approaches%252C%2520achieving%2520substantial%2520gains%2520in%250Aboth%2520task%2520success%2520rate%2520and%2520generalization%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F1%3A%20A%20Vision-Language-Action%20Model%20Bridging%20Understanding%20and%20Generation%0A%20%20to%20Actions&entry.906535625=Qi%20Lv%20and%20Weijie%20Kong%20and%20Hao%20Li%20and%20Jia%20Zeng%20and%20Zherui%20Qiu%20and%20Delin%20Qu%20and%20Haoming%20Song%20and%20Qizhi%20Chen%20and%20Xiang%20Deng%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Executing%20language-conditioned%20tasks%20in%20dynamic%20visual%20environments%20remains%20a%0Acentral%20challenge%20in%20embodied%20AI.%20Existing%20Vision-Language-Action%20%28VLA%29%20models%0Apredominantly%20adopt%20reactive%20state-to-action%20mappings%2C%20often%20leading%20to%0Ashort-sighted%20behaviors%20and%20poor%20robustness%20in%20dynamic%20scenes.%20In%20this%20paper%2C%0Awe%20introduce%20F1%2C%20a%20pretrained%20VLA%20framework%20which%20integrates%20the%20visual%0Aforesight%20generation%20into%20decision-making%20pipeline.%20F1%20adopts%20a%0AMixture-of-Transformer%20architecture%20with%20dedicated%20modules%20for%20perception%2C%0Aforesight%20generation%2C%20and%20control%2C%20thereby%20bridging%20understanding%2C%20generation%2C%0Aand%20actions.%20At%20its%20core%2C%20F1%20employs%20a%20next-scale%20prediction%20mechanism%20to%0Asynthesize%20goal-conditioned%20visual%20foresight%20as%20explicit%20planning%20targets.%20By%0Aforecasting%20plausible%20future%20visual%20states%2C%20F1%20reformulates%20action%20generation%0Aas%20a%20foresight-guided%20inverse%20dynamics%20problem%2C%20enabling%20actions%20that%0Aimplicitly%20achieve%20visual%20goals.%20To%20endow%20F1%20with%20robust%20and%20generalizable%0Acapabilities%2C%20we%20propose%20a%20three-stage%20training%20recipe%20on%20an%20extensive%20dataset%0Acomprising%20over%20330k%20trajectories%20across%20136%20diverse%20tasks.%20This%20training%0Ascheme%20enhances%20modular%20reasoning%20and%20equips%20the%20model%20with%20transferable%20visual%0Aforesight%2C%20which%20is%20critical%20for%20complex%20and%20dynamic%20environments.%20Extensive%0Aevaluations%20on%20real-world%20tasks%20and%20simulation%20benchmarks%20demonstrate%20F1%0Aconsistently%20outperforms%20existing%20approaches%2C%20achieving%20substantial%20gains%20in%0Aboth%20task%20success%20rate%20and%20generalization%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06951v1&entry.124074799=Read"},
{"title": "P3-SAM: Native 3D Part Segmentation", "author": "Changfeng Ma and Yang Li and Xinhao Yan and Jiachen Xu and Yunhan Yang and Chunshi Wang and Zibo Zhao and Yanwen Guo and Zhuo Chen and Chunchao Guo", "abstract": "  Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.\n", "link": "http://arxiv.org/abs/2509.06784v1", "date": "2025-09-08", "relevancy": 2.8155, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5637}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5637}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P3-SAM%3A%20Native%203D%20Part%20Segmentation&body=Title%3A%20P3-SAM%3A%20Native%203D%20Part%20Segmentation%0AAuthor%3A%20Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P3-SAM%2C%20designed%20to%20fully%20automate%20the%20segmentation%0Aof%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P3-SAM%20consists%20of%20a%0Afeature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%20predictor%2C%20enabling%0Ainteractive%20segmentation%20for%20users.%20We%20also%20propose%20an%20algorithm%20to%0Aautomatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%20part%20instance%0Asegmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%20containing%20nearly%0A3.7%20million%20models%20with%20reasonable%20segmentation%20labels.%20Comparisons%20show%20that%0Aour%20method%20achieves%20precise%20segmentation%20results%20and%20strong%20robustness%20on%20any%0Acomplex%20objects%2C%20attaining%20state-of-the-art%20performance.%20Our%20code%20will%20be%0Areleased%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP3-SAM%253A%2520Native%25203D%2520Part%2520Segmentation%26entry.906535625%3DChangfeng%2520Ma%2520and%2520Yang%2520Li%2520and%2520Xinhao%2520Yan%2520and%2520Jiachen%2520Xu%2520and%2520Yunhan%2520Yang%2520and%2520Chunshi%2520Wang%2520and%2520Zibo%2520Zhao%2520and%2520Yanwen%2520Guo%2520and%2520Zhuo%2520Chen%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Segmenting%25203D%2520assets%2520into%2520their%2520constituent%2520parts%2520is%2520crucial%2520for%2520enhancing%25203D%250Aunderstanding%252C%2520facilitating%2520model%2520reuse%252C%2520and%2520supporting%2520various%2520applications%250Asuch%2520as%2520part%2520generation.%2520However%252C%2520current%2520methods%2520face%2520limitations%2520such%2520as%2520poor%250Arobustness%2520when%2520dealing%2520with%2520complex%2520objects%2520and%2520cannot%2520fully%2520automate%2520the%250Aprocess.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520native%25203D%2520point-promptable%2520part%250Asegmentation%2520model%2520termed%2520P3-SAM%252C%2520designed%2520to%2520fully%2520automate%2520the%2520segmentation%250Aof%2520any%25203D%2520objects%2520into%2520components.%2520Inspired%2520by%2520SAM%252C%2520P3-SAM%2520consists%2520of%2520a%250Afeature%2520extractor%252C%2520multiple%2520segmentation%2520heads%252C%2520and%2520an%2520IoU%2520predictor%252C%2520enabling%250Ainteractive%2520segmentation%2520for%2520users.%2520We%2520also%2520propose%2520an%2520algorithm%2520to%250Aautomatically%2520select%2520and%2520merge%2520masks%2520predicted%2520by%2520our%2520model%2520for%2520part%2520instance%250Asegmentation.%2520Our%2520model%2520is%2520trained%2520on%2520a%2520newly%2520built%2520dataset%2520containing%2520nearly%250A3.7%2520million%2520models%2520with%2520reasonable%2520segmentation%2520labels.%2520Comparisons%2520show%2520that%250Aour%2520method%2520achieves%2520precise%2520segmentation%2520results%2520and%2520strong%2520robustness%2520on%2520any%250Acomplex%2520objects%252C%2520attaining%2520state-of-the-art%2520performance.%2520Our%2520code%2520will%2520be%250Areleased%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P3-SAM%3A%20Native%203D%20Part%20Segmentation&entry.906535625=Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo&entry.1292438233=%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P3-SAM%2C%20designed%20to%20fully%20automate%20the%20segmentation%0Aof%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P3-SAM%20consists%20of%20a%0Afeature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%20predictor%2C%20enabling%0Ainteractive%20segmentation%20for%20users.%20We%20also%20propose%20an%20algorithm%20to%0Aautomatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%20part%20instance%0Asegmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%20containing%20nearly%0A3.7%20million%20models%20with%20reasonable%20segmentation%20labels.%20Comparisons%20show%20that%0Aour%20method%20achieves%20precise%20segmentation%20results%20and%20strong%20robustness%20on%20any%0Acomplex%20objects%2C%20attaining%20state-of-the-art%20performance.%20Our%20code%20will%20be%0Areleased%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06784v1&entry.124074799=Read"},
{"title": "SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for\n  High-Fidelity Asset Generation and Driving Data Synthesis", "author": "Zhengqing Chen and Ruohong Mei and Xiaoyang Guo and Qingjie Wang and Yubin Hu and Wei Yin and Weiqiang Ren and Qian Zhang", "abstract": "  In the field of autonomous driving, sensor simulation is essential for\ngenerating rare and diverse scenarios that are difficult to capture in\nreal-world environments. Current solutions fall into two categories: 1)\nCG-based methods, such as CARLA, which lack diversity and struggle to scale to\nthe vast array of rare cases required for robust perception training; and 2)\nlearning-based approaches, such as NeuSim, which are limited to specific object\ncategories (vehicles) and require extensive multi-sensor data, hindering their\napplicability to generic objects. To address these limitations, we propose a\nscalable real2sim2real system that leverages 3D generation to automate asset\nmining, generation, and rare-case data synthesis.\n", "link": "http://arxiv.org/abs/2509.06798v1", "date": "2025-09-08", "relevancy": 2.799, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.561}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5592}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthDrive%3A%20Scalable%20Real2Sim2Real%20Sensor%20Simulation%20Pipeline%20for%0A%20%20High-Fidelity%20Asset%20Generation%20and%20Driving%20Data%20Synthesis&body=Title%3A%20SynthDrive%3A%20Scalable%20Real2Sim2Real%20Sensor%20Simulation%20Pipeline%20for%0A%20%20High-Fidelity%20Asset%20Generation%20and%20Driving%20Data%20Synthesis%0AAuthor%3A%20Zhengqing%20Chen%20and%20Ruohong%20Mei%20and%20Xiaoyang%20Guo%20and%20Qingjie%20Wang%20and%20Yubin%20Hu%20and%20Wei%20Yin%20and%20Weiqiang%20Ren%20and%20Qian%20Zhang%0AAbstract%3A%20%20%20In%20the%20field%20of%20autonomous%20driving%2C%20sensor%20simulation%20is%20essential%20for%0Agenerating%20rare%20and%20diverse%20scenarios%20that%20are%20difficult%20to%20capture%20in%0Areal-world%20environments.%20Current%20solutions%20fall%20into%20two%20categories%3A%201%29%0ACG-based%20methods%2C%20such%20as%20CARLA%2C%20which%20lack%20diversity%20and%20struggle%20to%20scale%20to%0Athe%20vast%20array%20of%20rare%20cases%20required%20for%20robust%20perception%20training%3B%20and%202%29%0Alearning-based%20approaches%2C%20such%20as%20NeuSim%2C%20which%20are%20limited%20to%20specific%20object%0Acategories%20%28vehicles%29%20and%20require%20extensive%20multi-sensor%20data%2C%20hindering%20their%0Aapplicability%20to%20generic%20objects.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Ascalable%20real2sim2real%20system%20that%20leverages%203D%20generation%20to%20automate%20asset%0Amining%2C%20generation%2C%20and%20rare-case%20data%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthDrive%253A%2520Scalable%2520Real2Sim2Real%2520Sensor%2520Simulation%2520Pipeline%2520for%250A%2520%2520High-Fidelity%2520Asset%2520Generation%2520and%2520Driving%2520Data%2520Synthesis%26entry.906535625%3DZhengqing%2520Chen%2520and%2520Ruohong%2520Mei%2520and%2520Xiaoyang%2520Guo%2520and%2520Qingjie%2520Wang%2520and%2520Yubin%2520Hu%2520and%2520Wei%2520Yin%2520and%2520Weiqiang%2520Ren%2520and%2520Qian%2520Zhang%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520autonomous%2520driving%252C%2520sensor%2520simulation%2520is%2520essential%2520for%250Agenerating%2520rare%2520and%2520diverse%2520scenarios%2520that%2520are%2520difficult%2520to%2520capture%2520in%250Areal-world%2520environments.%2520Current%2520solutions%2520fall%2520into%2520two%2520categories%253A%25201%2529%250ACG-based%2520methods%252C%2520such%2520as%2520CARLA%252C%2520which%2520lack%2520diversity%2520and%2520struggle%2520to%2520scale%2520to%250Athe%2520vast%2520array%2520of%2520rare%2520cases%2520required%2520for%2520robust%2520perception%2520training%253B%2520and%25202%2529%250Alearning-based%2520approaches%252C%2520such%2520as%2520NeuSim%252C%2520which%2520are%2520limited%2520to%2520specific%2520object%250Acategories%2520%2528vehicles%2529%2520and%2520require%2520extensive%2520multi-sensor%2520data%252C%2520hindering%2520their%250Aapplicability%2520to%2520generic%2520objects.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Ascalable%2520real2sim2real%2520system%2520that%2520leverages%25203D%2520generation%2520to%2520automate%2520asset%250Amining%252C%2520generation%252C%2520and%2520rare-case%2520data%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthDrive%3A%20Scalable%20Real2Sim2Real%20Sensor%20Simulation%20Pipeline%20for%0A%20%20High-Fidelity%20Asset%20Generation%20and%20Driving%20Data%20Synthesis&entry.906535625=Zhengqing%20Chen%20and%20Ruohong%20Mei%20and%20Xiaoyang%20Guo%20and%20Qingjie%20Wang%20and%20Yubin%20Hu%20and%20Wei%20Yin%20and%20Weiqiang%20Ren%20and%20Qian%20Zhang&entry.1292438233=%20%20In%20the%20field%20of%20autonomous%20driving%2C%20sensor%20simulation%20is%20essential%20for%0Agenerating%20rare%20and%20diverse%20scenarios%20that%20are%20difficult%20to%20capture%20in%0Areal-world%20environments.%20Current%20solutions%20fall%20into%20two%20categories%3A%201%29%0ACG-based%20methods%2C%20such%20as%20CARLA%2C%20which%20lack%20diversity%20and%20struggle%20to%20scale%20to%0Athe%20vast%20array%20of%20rare%20cases%20required%20for%20robust%20perception%20training%3B%20and%202%29%0Alearning-based%20approaches%2C%20such%20as%20NeuSim%2C%20which%20are%20limited%20to%20specific%20object%0Acategories%20%28vehicles%29%20and%20require%20extensive%20multi-sensor%20data%2C%20hindering%20their%0Aapplicability%20to%20generic%20objects.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Ascalable%20real2sim2real%20system%20that%20leverages%203D%20generation%20to%20automate%20asset%0Amining%2C%20generation%2C%20and%20rare-case%20data%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06798v1&entry.124074799=Read"},
{"title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis", "author": "Xin Kong and Daniel Watson and Yannick Str\u00fcmpler and Michael Niemeyer and Federico Tombari", "abstract": "  Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.\n", "link": "http://arxiv.org/abs/2509.06579v1", "date": "2025-09-08", "relevancy": 2.785, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.706}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.706}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausNVS%3A%20Autoregressive%20Multi-view%20Diffusion%20for%20Flexible%203D%20Novel%20View%0A%20%20Synthesis&body=Title%3A%20CausNVS%3A%20Autoregressive%20Multi-view%20Diffusion%20for%20Flexible%203D%20Novel%20View%0A%20%20Synthesis%0AAuthor%3A%20Xin%20Kong%20and%20Daniel%20Watson%20and%20Yannick%20Str%C3%BCmpler%20and%20Michael%20Niemeyer%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Multi-view%20diffusion%20models%20have%20shown%20promise%20in%203D%20novel%20view%20synthesis%2C%0Abut%20most%20existing%20methods%20adopt%20a%20non-autoregressive%20formulation.%20This%20limits%0Atheir%20applicability%20in%20world%20modeling%2C%20as%20they%20only%20support%20a%20fixed%20number%20of%0Aviews%20and%20suffer%20from%20slow%20inference%20due%20to%20denoising%20all%20frames%0Asimultaneously.%20To%20address%20these%20limitations%2C%20we%20propose%20CausNVS%2C%20a%20multi-view%0Adiffusion%20model%20in%20an%20autoregressive%20setting%2C%20which%20supports%20arbitrary%0Ainput-output%20view%20configurations%20and%20generates%20views%20sequentially.%20We%20train%0ACausNVS%20with%20causal%20masking%20and%20per-frame%20noise%2C%20using%20pairwise-relative%20camera%0Apose%20encodings%20%28CaPE%29%20for%20precise%20camera%20control.%20At%20inference%20time%2C%20we%20combine%0Aa%20spatially-aware%20sliding-window%20with%20key-value%20caching%20and%20noise%20conditioning%0Aaugmentation%20to%20mitigate%20drift.%20Our%20experiments%20demonstrate%20that%20CausNVS%0Asupports%20a%20broad%20range%20of%20camera%20trajectories%2C%20enables%20flexible%20autoregressive%0Anovel%20view%20synthesis%2C%20and%20achieves%20consistently%20strong%20visual%20quality%20across%0Adiverse%20settings.%20Project%20page%3A%20https%3A//kxhit.github.io/CausNVS.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausNVS%253A%2520Autoregressive%2520Multi-view%2520Diffusion%2520for%2520Flexible%25203D%2520Novel%2520View%250A%2520%2520Synthesis%26entry.906535625%3DXin%2520Kong%2520and%2520Daniel%2520Watson%2520and%2520Yannick%2520Str%25C3%25BCmpler%2520and%2520Michael%2520Niemeyer%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520Multi-view%2520diffusion%2520models%2520have%2520shown%2520promise%2520in%25203D%2520novel%2520view%2520synthesis%252C%250Abut%2520most%2520existing%2520methods%2520adopt%2520a%2520non-autoregressive%2520formulation.%2520This%2520limits%250Atheir%2520applicability%2520in%2520world%2520modeling%252C%2520as%2520they%2520only%2520support%2520a%2520fixed%2520number%2520of%250Aviews%2520and%2520suffer%2520from%2520slow%2520inference%2520due%2520to%2520denoising%2520all%2520frames%250Asimultaneously.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520CausNVS%252C%2520a%2520multi-view%250Adiffusion%2520model%2520in%2520an%2520autoregressive%2520setting%252C%2520which%2520supports%2520arbitrary%250Ainput-output%2520view%2520configurations%2520and%2520generates%2520views%2520sequentially.%2520We%2520train%250ACausNVS%2520with%2520causal%2520masking%2520and%2520per-frame%2520noise%252C%2520using%2520pairwise-relative%2520camera%250Apose%2520encodings%2520%2528CaPE%2529%2520for%2520precise%2520camera%2520control.%2520At%2520inference%2520time%252C%2520we%2520combine%250Aa%2520spatially-aware%2520sliding-window%2520with%2520key-value%2520caching%2520and%2520noise%2520conditioning%250Aaugmentation%2520to%2520mitigate%2520drift.%2520Our%2520experiments%2520demonstrate%2520that%2520CausNVS%250Asupports%2520a%2520broad%2520range%2520of%2520camera%2520trajectories%252C%2520enables%2520flexible%2520autoregressive%250Anovel%2520view%2520synthesis%252C%2520and%2520achieves%2520consistently%2520strong%2520visual%2520quality%2520across%250Adiverse%2520settings.%2520Project%2520page%253A%2520https%253A//kxhit.github.io/CausNVS.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausNVS%3A%20Autoregressive%20Multi-view%20Diffusion%20for%20Flexible%203D%20Novel%20View%0A%20%20Synthesis&entry.906535625=Xin%20Kong%20and%20Daniel%20Watson%20and%20Yannick%20Str%C3%BCmpler%20and%20Michael%20Niemeyer%20and%20Federico%20Tombari&entry.1292438233=%20%20Multi-view%20diffusion%20models%20have%20shown%20promise%20in%203D%20novel%20view%20synthesis%2C%0Abut%20most%20existing%20methods%20adopt%20a%20non-autoregressive%20formulation.%20This%20limits%0Atheir%20applicability%20in%20world%20modeling%2C%20as%20they%20only%20support%20a%20fixed%20number%20of%0Aviews%20and%20suffer%20from%20slow%20inference%20due%20to%20denoising%20all%20frames%0Asimultaneously.%20To%20address%20these%20limitations%2C%20we%20propose%20CausNVS%2C%20a%20multi-view%0Adiffusion%20model%20in%20an%20autoregressive%20setting%2C%20which%20supports%20arbitrary%0Ainput-output%20view%20configurations%20and%20generates%20views%20sequentially.%20We%20train%0ACausNVS%20with%20causal%20masking%20and%20per-frame%20noise%2C%20using%20pairwise-relative%20camera%0Apose%20encodings%20%28CaPE%29%20for%20precise%20camera%20control.%20At%20inference%20time%2C%20we%20combine%0Aa%20spatially-aware%20sliding-window%20with%20key-value%20caching%20and%20noise%20conditioning%0Aaugmentation%20to%20mitigate%20drift.%20Our%20experiments%20demonstrate%20that%20CausNVS%0Asupports%20a%20broad%20range%20of%20camera%20trajectories%2C%20enables%20flexible%20autoregressive%0Anovel%20view%20synthesis%2C%20and%20achieves%20consistently%20strong%20visual%20quality%20across%0Adiverse%20settings.%20Project%20page%3A%20https%3A//kxhit.github.io/CausNVS.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06579v1&entry.124074799=Read"},
{"title": "AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive\n  Representations", "author": "Shuhan Ding and Jingjing Fu and Yu Gu and Naiteek Sangani and Mu Wei and Paul Vozila and Nan Liu and Jiang Bian and Hoifung Poon", "abstract": "  Medical image synthesis has become an essential strategy for augmenting\ndatasets and improving model generalization in data-scarce clinical settings.\nHowever, fine-grained and controllable synthesis remains difficult due to\nlimited high-quality annotations and domain shifts across datasets. Existing\nmethods, often designed for natural images or well-defined tumors, struggle to\ngeneralize to chest radiographs, where disease patterns are morphologically\ndiverse and tightly intertwined with anatomical structures. To address these\nchallenges, we propose AURAD, a controllable radiology synthesis framework that\njointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike\nprior approaches that rely on randomly sampled masks-limiting diversity,\ncontrollability, and clinical relevance-our method learns to generate masks\nthat capture multi-pathology coexistence and anatomical-pathological\nconsistency. It follows a progressive pipeline: pseudo masks are first\ngenerated from clinical prompts conditioned on anatomical structures, and then\nused to guide image synthesis. We also leverage pretrained expert medical\nmodels to filter outputs and ensure clinical plausibility. Beyond visual\nrealism, the synthesized masks also serve as labels for downstream tasks such\nas detection and segmentation, bridging the gap between generative modeling and\nreal-world clinical applications. Extensive experiments and blinded radiologist\nevaluations demonstrate the effectiveness and generalizability of our method\nacross tasks and datasets. In particular, 78% of our synthesized images are\nclassified as authentic by board-certified radiologists, and over 40% of\npredicted segmentation overlays are rated as clinically useful. All code,\npre-trained models, and the synthesized dataset will be released upon\npublication.\n", "link": "http://arxiv.org/abs/2509.04819v2", "date": "2025-09-08", "relevancy": 2.7634, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5559}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5559}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AURAD%3A%20Anatomy-Pathology%20Unified%20Radiology%20Synthesis%20with%20Progressive%0A%20%20Representations&body=Title%3A%20AURAD%3A%20Anatomy-Pathology%20Unified%20Radiology%20Synthesis%20with%20Progressive%0A%20%20Representations%0AAuthor%3A%20Shuhan%20Ding%20and%20Jingjing%20Fu%20and%20Yu%20Gu%20and%20Naiteek%20Sangani%20and%20Mu%20Wei%20and%20Paul%20Vozila%20and%20Nan%20Liu%20and%20Jiang%20Bian%20and%20Hoifung%20Poon%0AAbstract%3A%20%20%20Medical%20image%20synthesis%20has%20become%20an%20essential%20strategy%20for%20augmenting%0Adatasets%20and%20improving%20model%20generalization%20in%20data-scarce%20clinical%20settings.%0AHowever%2C%20fine-grained%20and%20controllable%20synthesis%20remains%20difficult%20due%20to%0Alimited%20high-quality%20annotations%20and%20domain%20shifts%20across%20datasets.%20Existing%0Amethods%2C%20often%20designed%20for%20natural%20images%20or%20well-defined%20tumors%2C%20struggle%20to%0Ageneralize%20to%20chest%20radiographs%2C%20where%20disease%20patterns%20are%20morphologically%0Adiverse%20and%20tightly%20intertwined%20with%20anatomical%20structures.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AURAD%2C%20a%20controllable%20radiology%20synthesis%20framework%20that%0Ajointly%20generates%20high-fidelity%20chest%20X-rays%20and%20pseudo%20semantic%20masks.%20Unlike%0Aprior%20approaches%20that%20rely%20on%20randomly%20sampled%20masks-limiting%20diversity%2C%0Acontrollability%2C%20and%20clinical%20relevance-our%20method%20learns%20to%20generate%20masks%0Athat%20capture%20multi-pathology%20coexistence%20and%20anatomical-pathological%0Aconsistency.%20It%20follows%20a%20progressive%20pipeline%3A%20pseudo%20masks%20are%20first%0Agenerated%20from%20clinical%20prompts%20conditioned%20on%20anatomical%20structures%2C%20and%20then%0Aused%20to%20guide%20image%20synthesis.%20We%20also%20leverage%20pretrained%20expert%20medical%0Amodels%20to%20filter%20outputs%20and%20ensure%20clinical%20plausibility.%20Beyond%20visual%0Arealism%2C%20the%20synthesized%20masks%20also%20serve%20as%20labels%20for%20downstream%20tasks%20such%0Aas%20detection%20and%20segmentation%2C%20bridging%20the%20gap%20between%20generative%20modeling%20and%0Areal-world%20clinical%20applications.%20Extensive%20experiments%20and%20blinded%20radiologist%0Aevaluations%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20our%20method%0Aacross%20tasks%20and%20datasets.%20In%20particular%2C%2078%25%20of%20our%20synthesized%20images%20are%0Aclassified%20as%20authentic%20by%20board-certified%20radiologists%2C%20and%20over%2040%25%20of%0Apredicted%20segmentation%20overlays%20are%20rated%20as%20clinically%20useful.%20All%20code%2C%0Apre-trained%20models%2C%20and%20the%20synthesized%20dataset%20will%20be%20released%20upon%0Apublication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04819v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAURAD%253A%2520Anatomy-Pathology%2520Unified%2520Radiology%2520Synthesis%2520with%2520Progressive%250A%2520%2520Representations%26entry.906535625%3DShuhan%2520Ding%2520and%2520Jingjing%2520Fu%2520and%2520Yu%2520Gu%2520and%2520Naiteek%2520Sangani%2520and%2520Mu%2520Wei%2520and%2520Paul%2520Vozila%2520and%2520Nan%2520Liu%2520and%2520Jiang%2520Bian%2520and%2520Hoifung%2520Poon%26entry.1292438233%3D%2520%2520Medical%2520image%2520synthesis%2520has%2520become%2520an%2520essential%2520strategy%2520for%2520augmenting%250Adatasets%2520and%2520improving%2520model%2520generalization%2520in%2520data-scarce%2520clinical%2520settings.%250AHowever%252C%2520fine-grained%2520and%2520controllable%2520synthesis%2520remains%2520difficult%2520due%2520to%250Alimited%2520high-quality%2520annotations%2520and%2520domain%2520shifts%2520across%2520datasets.%2520Existing%250Amethods%252C%2520often%2520designed%2520for%2520natural%2520images%2520or%2520well-defined%2520tumors%252C%2520struggle%2520to%250Ageneralize%2520to%2520chest%2520radiographs%252C%2520where%2520disease%2520patterns%2520are%2520morphologically%250Adiverse%2520and%2520tightly%2520intertwined%2520with%2520anatomical%2520structures.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520AURAD%252C%2520a%2520controllable%2520radiology%2520synthesis%2520framework%2520that%250Ajointly%2520generates%2520high-fidelity%2520chest%2520X-rays%2520and%2520pseudo%2520semantic%2520masks.%2520Unlike%250Aprior%2520approaches%2520that%2520rely%2520on%2520randomly%2520sampled%2520masks-limiting%2520diversity%252C%250Acontrollability%252C%2520and%2520clinical%2520relevance-our%2520method%2520learns%2520to%2520generate%2520masks%250Athat%2520capture%2520multi-pathology%2520coexistence%2520and%2520anatomical-pathological%250Aconsistency.%2520It%2520follows%2520a%2520progressive%2520pipeline%253A%2520pseudo%2520masks%2520are%2520first%250Agenerated%2520from%2520clinical%2520prompts%2520conditioned%2520on%2520anatomical%2520structures%252C%2520and%2520then%250Aused%2520to%2520guide%2520image%2520synthesis.%2520We%2520also%2520leverage%2520pretrained%2520expert%2520medical%250Amodels%2520to%2520filter%2520outputs%2520and%2520ensure%2520clinical%2520plausibility.%2520Beyond%2520visual%250Arealism%252C%2520the%2520synthesized%2520masks%2520also%2520serve%2520as%2520labels%2520for%2520downstream%2520tasks%2520such%250Aas%2520detection%2520and%2520segmentation%252C%2520bridging%2520the%2520gap%2520between%2520generative%2520modeling%2520and%250Areal-world%2520clinical%2520applications.%2520Extensive%2520experiments%2520and%2520blinded%2520radiologist%250Aevaluations%2520demonstrate%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520our%2520method%250Aacross%2520tasks%2520and%2520datasets.%2520In%2520particular%252C%252078%2525%2520of%2520our%2520synthesized%2520images%2520are%250Aclassified%2520as%2520authentic%2520by%2520board-certified%2520radiologists%252C%2520and%2520over%252040%2525%2520of%250Apredicted%2520segmentation%2520overlays%2520are%2520rated%2520as%2520clinically%2520useful.%2520All%2520code%252C%250Apre-trained%2520models%252C%2520and%2520the%2520synthesized%2520dataset%2520will%2520be%2520released%2520upon%250Apublication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04819v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AURAD%3A%20Anatomy-Pathology%20Unified%20Radiology%20Synthesis%20with%20Progressive%0A%20%20Representations&entry.906535625=Shuhan%20Ding%20and%20Jingjing%20Fu%20and%20Yu%20Gu%20and%20Naiteek%20Sangani%20and%20Mu%20Wei%20and%20Paul%20Vozila%20and%20Nan%20Liu%20and%20Jiang%20Bian%20and%20Hoifung%20Poon&entry.1292438233=%20%20Medical%20image%20synthesis%20has%20become%20an%20essential%20strategy%20for%20augmenting%0Adatasets%20and%20improving%20model%20generalization%20in%20data-scarce%20clinical%20settings.%0AHowever%2C%20fine-grained%20and%20controllable%20synthesis%20remains%20difficult%20due%20to%0Alimited%20high-quality%20annotations%20and%20domain%20shifts%20across%20datasets.%20Existing%0Amethods%2C%20often%20designed%20for%20natural%20images%20or%20well-defined%20tumors%2C%20struggle%20to%0Ageneralize%20to%20chest%20radiographs%2C%20where%20disease%20patterns%20are%20morphologically%0Adiverse%20and%20tightly%20intertwined%20with%20anatomical%20structures.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AURAD%2C%20a%20controllable%20radiology%20synthesis%20framework%20that%0Ajointly%20generates%20high-fidelity%20chest%20X-rays%20and%20pseudo%20semantic%20masks.%20Unlike%0Aprior%20approaches%20that%20rely%20on%20randomly%20sampled%20masks-limiting%20diversity%2C%0Acontrollability%2C%20and%20clinical%20relevance-our%20method%20learns%20to%20generate%20masks%0Athat%20capture%20multi-pathology%20coexistence%20and%20anatomical-pathological%0Aconsistency.%20It%20follows%20a%20progressive%20pipeline%3A%20pseudo%20masks%20are%20first%0Agenerated%20from%20clinical%20prompts%20conditioned%20on%20anatomical%20structures%2C%20and%20then%0Aused%20to%20guide%20image%20synthesis.%20We%20also%20leverage%20pretrained%20expert%20medical%0Amodels%20to%20filter%20outputs%20and%20ensure%20clinical%20plausibility.%20Beyond%20visual%0Arealism%2C%20the%20synthesized%20masks%20also%20serve%20as%20labels%20for%20downstream%20tasks%20such%0Aas%20detection%20and%20segmentation%2C%20bridging%20the%20gap%20between%20generative%20modeling%20and%0Areal-world%20clinical%20applications.%20Extensive%20experiments%20and%20blinded%20radiologist%0Aevaluations%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20our%20method%0Aacross%20tasks%20and%20datasets.%20In%20particular%2C%2078%25%20of%20our%20synthesized%20images%20are%0Aclassified%20as%20authentic%20by%20board-certified%20radiologists%2C%20and%20over%2040%25%20of%0Apredicted%20segmentation%20overlays%20are%20rated%20as%20clinically%20useful.%20All%20code%2C%0Apre-trained%20models%2C%20and%20the%20synthesized%20dataset%20will%20be%20released%20upon%0Apublication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04819v2&entry.124074799=Read"},
{"title": "Evolving from Unknown to Known: Retentive Angular Representation\n  Learning for Incremental Open Set Recognition", "author": "Runqing Yang and Yimin Fu and Changyuan Wu and Zhunga Liu", "abstract": "  Existing open set recognition (OSR) methods are typically designed for static\nscenarios, where models aim to classify known classes and identify unknown ones\nwithin fixed scopes. This deviates from the expectation that the model should\nincrementally identify newly emerging unknown classes from continuous data\nstreams and acquire corresponding knowledge. In such evolving scenarios, the\ndiscriminability of OSR decision boundaries is hard to maintain due to\nrestricted access to former training data, causing severe inter-class\nconfusion. To solve this problem, we propose retentive angular representation\nlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknown\nrepresentations are encouraged to align around inactive prototypes within an\nangular space constructed under the equiangular tight frame, thereby mitigating\nexcessive representation drift during knowledge updates. Specifically, we adopt\na virtual-intrinsic interactive (VII) training strategy, which compacts known\nrepresentations by enforcing clear inter-class margins through\nboundary-proximal virtual classes. Furthermore, a stratified rectification\nstrategy is designed to refine decision boundaries, mitigating representation\nbias and feature space distortion caused by imbalances between old/new and\npositive/negative class samples. We conduct thorough evaluations on CIFAR100\nand TinyImageNet datasets and establish a new benchmark for IOSR. Experimental\nresults across various task setups demonstrate that the proposed method\nachieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2509.06570v1", "date": "2025-09-08", "relevancy": 2.7551, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5784}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolving%20from%20Unknown%20to%20Known%3A%20Retentive%20Angular%20Representation%0A%20%20Learning%20for%20Incremental%20Open%20Set%20Recognition&body=Title%3A%20Evolving%20from%20Unknown%20to%20Known%3A%20Retentive%20Angular%20Representation%0A%20%20Learning%20for%20Incremental%20Open%20Set%20Recognition%0AAuthor%3A%20Runqing%20Yang%20and%20Yimin%20Fu%20and%20Changyuan%20Wu%20and%20Zhunga%20Liu%0AAbstract%3A%20%20%20Existing%20open%20set%20recognition%20%28OSR%29%20methods%20are%20typically%20designed%20for%20static%0Ascenarios%2C%20where%20models%20aim%20to%20classify%20known%20classes%20and%20identify%20unknown%20ones%0Awithin%20fixed%20scopes.%20This%20deviates%20from%20the%20expectation%20that%20the%20model%20should%0Aincrementally%20identify%20newly%20emerging%20unknown%20classes%20from%20continuous%20data%0Astreams%20and%20acquire%20corresponding%20knowledge.%20In%20such%20evolving%20scenarios%2C%20the%0Adiscriminability%20of%20OSR%20decision%20boundaries%20is%20hard%20to%20maintain%20due%20to%0Arestricted%20access%20to%20former%20training%20data%2C%20causing%20severe%20inter-class%0Aconfusion.%20To%20solve%20this%20problem%2C%20we%20propose%20retentive%20angular%20representation%0Alearning%20%28RARL%29%20for%20incremental%20open%20set%20recognition%20%28IOSR%29.%20In%20RARL%2C%20unknown%0Arepresentations%20are%20encouraged%20to%20align%20around%20inactive%20prototypes%20within%20an%0Aangular%20space%20constructed%20under%20the%20equiangular%20tight%20frame%2C%20thereby%20mitigating%0Aexcessive%20representation%20drift%20during%20knowledge%20updates.%20Specifically%2C%20we%20adopt%0Aa%20virtual-intrinsic%20interactive%20%28VII%29%20training%20strategy%2C%20which%20compacts%20known%0Arepresentations%20by%20enforcing%20clear%20inter-class%20margins%20through%0Aboundary-proximal%20virtual%20classes.%20Furthermore%2C%20a%20stratified%20rectification%0Astrategy%20is%20designed%20to%20refine%20decision%20boundaries%2C%20mitigating%20representation%0Abias%20and%20feature%20space%20distortion%20caused%20by%20imbalances%20between%20old/new%20and%0Apositive/negative%20class%20samples.%20We%20conduct%20thorough%20evaluations%20on%20CIFAR100%0Aand%20TinyImageNet%20datasets%20and%20establish%20a%20new%20benchmark%20for%20IOSR.%20Experimental%0Aresults%20across%20various%20task%20setups%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolving%2520from%2520Unknown%2520to%2520Known%253A%2520Retentive%2520Angular%2520Representation%250A%2520%2520Learning%2520for%2520Incremental%2520Open%2520Set%2520Recognition%26entry.906535625%3DRunqing%2520Yang%2520and%2520Yimin%2520Fu%2520and%2520Changyuan%2520Wu%2520and%2520Zhunga%2520Liu%26entry.1292438233%3D%2520%2520Existing%2520open%2520set%2520recognition%2520%2528OSR%2529%2520methods%2520are%2520typically%2520designed%2520for%2520static%250Ascenarios%252C%2520where%2520models%2520aim%2520to%2520classify%2520known%2520classes%2520and%2520identify%2520unknown%2520ones%250Awithin%2520fixed%2520scopes.%2520This%2520deviates%2520from%2520the%2520expectation%2520that%2520the%2520model%2520should%250Aincrementally%2520identify%2520newly%2520emerging%2520unknown%2520classes%2520from%2520continuous%2520data%250Astreams%2520and%2520acquire%2520corresponding%2520knowledge.%2520In%2520such%2520evolving%2520scenarios%252C%2520the%250Adiscriminability%2520of%2520OSR%2520decision%2520boundaries%2520is%2520hard%2520to%2520maintain%2520due%2520to%250Arestricted%2520access%2520to%2520former%2520training%2520data%252C%2520causing%2520severe%2520inter-class%250Aconfusion.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520retentive%2520angular%2520representation%250Alearning%2520%2528RARL%2529%2520for%2520incremental%2520open%2520set%2520recognition%2520%2528IOSR%2529.%2520In%2520RARL%252C%2520unknown%250Arepresentations%2520are%2520encouraged%2520to%2520align%2520around%2520inactive%2520prototypes%2520within%2520an%250Aangular%2520space%2520constructed%2520under%2520the%2520equiangular%2520tight%2520frame%252C%2520thereby%2520mitigating%250Aexcessive%2520representation%2520drift%2520during%2520knowledge%2520updates.%2520Specifically%252C%2520we%2520adopt%250Aa%2520virtual-intrinsic%2520interactive%2520%2528VII%2529%2520training%2520strategy%252C%2520which%2520compacts%2520known%250Arepresentations%2520by%2520enforcing%2520clear%2520inter-class%2520margins%2520through%250Aboundary-proximal%2520virtual%2520classes.%2520Furthermore%252C%2520a%2520stratified%2520rectification%250Astrategy%2520is%2520designed%2520to%2520refine%2520decision%2520boundaries%252C%2520mitigating%2520representation%250Abias%2520and%2520feature%2520space%2520distortion%2520caused%2520by%2520imbalances%2520between%2520old/new%2520and%250Apositive/negative%2520class%2520samples.%2520We%2520conduct%2520thorough%2520evaluations%2520on%2520CIFAR100%250Aand%2520TinyImageNet%2520datasets%2520and%2520establish%2520a%2520new%2520benchmark%2520for%2520IOSR.%2520Experimental%250Aresults%2520across%2520various%2520task%2520setups%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aachieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolving%20from%20Unknown%20to%20Known%3A%20Retentive%20Angular%20Representation%0A%20%20Learning%20for%20Incremental%20Open%20Set%20Recognition&entry.906535625=Runqing%20Yang%20and%20Yimin%20Fu%20and%20Changyuan%20Wu%20and%20Zhunga%20Liu&entry.1292438233=%20%20Existing%20open%20set%20recognition%20%28OSR%29%20methods%20are%20typically%20designed%20for%20static%0Ascenarios%2C%20where%20models%20aim%20to%20classify%20known%20classes%20and%20identify%20unknown%20ones%0Awithin%20fixed%20scopes.%20This%20deviates%20from%20the%20expectation%20that%20the%20model%20should%0Aincrementally%20identify%20newly%20emerging%20unknown%20classes%20from%20continuous%20data%0Astreams%20and%20acquire%20corresponding%20knowledge.%20In%20such%20evolving%20scenarios%2C%20the%0Adiscriminability%20of%20OSR%20decision%20boundaries%20is%20hard%20to%20maintain%20due%20to%0Arestricted%20access%20to%20former%20training%20data%2C%20causing%20severe%20inter-class%0Aconfusion.%20To%20solve%20this%20problem%2C%20we%20propose%20retentive%20angular%20representation%0Alearning%20%28RARL%29%20for%20incremental%20open%20set%20recognition%20%28IOSR%29.%20In%20RARL%2C%20unknown%0Arepresentations%20are%20encouraged%20to%20align%20around%20inactive%20prototypes%20within%20an%0Aangular%20space%20constructed%20under%20the%20equiangular%20tight%20frame%2C%20thereby%20mitigating%0Aexcessive%20representation%20drift%20during%20knowledge%20updates.%20Specifically%2C%20we%20adopt%0Aa%20virtual-intrinsic%20interactive%20%28VII%29%20training%20strategy%2C%20which%20compacts%20known%0Arepresentations%20by%20enforcing%20clear%20inter-class%20margins%20through%0Aboundary-proximal%20virtual%20classes.%20Furthermore%2C%20a%20stratified%20rectification%0Astrategy%20is%20designed%20to%20refine%20decision%20boundaries%2C%20mitigating%20representation%0Abias%20and%20feature%20space%20distortion%20caused%20by%20imbalances%20between%20old/new%20and%0Apositive/negative%20class%20samples.%20We%20conduct%20thorough%20evaluations%20on%20CIFAR100%0Aand%20TinyImageNet%20datasets%20and%20establish%20a%20new%20benchmark%20for%20IOSR.%20Experimental%0Aresults%20across%20various%20task%20setups%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06570v1&entry.124074799=Read"},
{"title": "MachineLearningLM: Continued Pretraining Language Models on Millions of\n  Synthetic Tabular Prediction Tasks Scales In-Context ML", "author": "Haoyu Dong and Pengkun Zhang and Mingzhe Lu and Yanzhen Shen and Guolin Ke", "abstract": "  Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.\n", "link": "http://arxiv.org/abs/2509.06806v1", "date": "2025-09-08", "relevancy": 2.7474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MachineLearningLM%3A%20Continued%20Pretraining%20Language%20Models%20on%20Millions%20of%0A%20%20Synthetic%20Tabular%20Prediction%20Tasks%20Scales%20In-Context%20ML&body=Title%3A%20MachineLearningLM%3A%20Continued%20Pretraining%20Language%20Models%20on%20Millions%20of%0A%20%20Synthetic%20Tabular%20Prediction%20Tasks%20Scales%20In-Context%20ML%0AAuthor%3A%20Haoyu%20Dong%20and%20Pengkun%20Zhang%20and%20Mingzhe%20Lu%20and%20Yanzhen%20Shen%20and%20Guolin%20Ke%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20possess%20broad%20world%20knowledge%20and%20strong%0Ageneral-purpose%20reasoning%20ability%2C%20yet%20they%20struggle%20to%20learn%20from%20many%0Ain-context%20examples%20on%20standard%20machine%20learning%20%28ML%29%20tasks%2C%20that%20is%2C%20to%0Aleverage%20many-shot%20demonstrations%20purely%20via%20in-context%20learning%20%28ICL%29%20without%0Agradient%20descent.%20We%20introduce%20MachineLearningLM%2C%20a%20portable%0Acontinued-pretraining%20framework%20that%20equips%20a%20general-purpose%20LLM%20with%20robust%0Ain-context%20ML%20capability%20while%20preserving%20its%20general%20knowledge%20and%20reasoning%0Afor%20broader%20chat%20workflows.%0A%20%20Our%20pretraining%20procedure%20synthesizes%20ML%20tasks%20from%20millions%20of%20structural%0Acausal%20models%20%28SCMs%29%2C%20spanning%20shot%20counts%20up%20to%201%2C024.%20We%20begin%20with%20a%0Arandom-forest%20teacher%2C%20distilling%20tree-based%20decision%20strategies%20into%20the%20LLM%0Ato%20strengthen%20robustness%20in%20numerical%20modeling.%20All%20tasks%20are%20serialized%20with%20a%0Atoken-efficient%20prompt%2C%20enabling%203x%20to%206x%20more%20examples%20per%20context%20window%20and%0Adelivering%20up%20to%2050x%20amortized%20throughput%20via%20batch%20inference.%0A%20%20Despite%20a%20modest%20setup%20%28Qwen-2.5-7B-Instruct%20with%20LoRA%20rank%208%29%2C%0AMachineLearningLM%20outperforms%20strong%20LLM%20baselines%20%28e.g.%2C%20GPT-5-mini%29%20by%20an%0Aaverage%20of%20about%2015%25%20on%20out-of-distribution%20tabular%20classification%20across%0Afinance%2C%20physics%2C%20biology%2C%20and%20healthcare%20domains.%20It%20exhibits%20a%20striking%0Amany-shot%20scaling%20law%3A%20accuracy%20increases%20monotonically%20as%20in-context%0Ademonstrations%20grow%20from%208%20to%201%2C024.%20Without%20any%20task-specific%20training%2C%20it%0Aattains%20random-forest-level%20accuracy%20across%20hundreds%20of%20shots.%20General%20chat%0Acapabilities%2C%20including%20knowledge%20and%20reasoning%2C%20are%20preserved%3A%20it%20achieves%0A75.4%25%20on%20MMLU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachineLearningLM%253A%2520Continued%2520Pretraining%2520Language%2520Models%2520on%2520Millions%2520of%250A%2520%2520Synthetic%2520Tabular%2520Prediction%2520Tasks%2520Scales%2520In-Context%2520ML%26entry.906535625%3DHaoyu%2520Dong%2520and%2520Pengkun%2520Zhang%2520and%2520Mingzhe%2520Lu%2520and%2520Yanzhen%2520Shen%2520and%2520Guolin%2520Ke%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520possess%2520broad%2520world%2520knowledge%2520and%2520strong%250Ageneral-purpose%2520reasoning%2520ability%252C%2520yet%2520they%2520struggle%2520to%2520learn%2520from%2520many%250Ain-context%2520examples%2520on%2520standard%2520machine%2520learning%2520%2528ML%2529%2520tasks%252C%2520that%2520is%252C%2520to%250Aleverage%2520many-shot%2520demonstrations%2520purely%2520via%2520in-context%2520learning%2520%2528ICL%2529%2520without%250Agradient%2520descent.%2520We%2520introduce%2520MachineLearningLM%252C%2520a%2520portable%250Acontinued-pretraining%2520framework%2520that%2520equips%2520a%2520general-purpose%2520LLM%2520with%2520robust%250Ain-context%2520ML%2520capability%2520while%2520preserving%2520its%2520general%2520knowledge%2520and%2520reasoning%250Afor%2520broader%2520chat%2520workflows.%250A%2520%2520Our%2520pretraining%2520procedure%2520synthesizes%2520ML%2520tasks%2520from%2520millions%2520of%2520structural%250Acausal%2520models%2520%2528SCMs%2529%252C%2520spanning%2520shot%2520counts%2520up%2520to%25201%252C024.%2520We%2520begin%2520with%2520a%250Arandom-forest%2520teacher%252C%2520distilling%2520tree-based%2520decision%2520strategies%2520into%2520the%2520LLM%250Ato%2520strengthen%2520robustness%2520in%2520numerical%2520modeling.%2520All%2520tasks%2520are%2520serialized%2520with%2520a%250Atoken-efficient%2520prompt%252C%2520enabling%25203x%2520to%25206x%2520more%2520examples%2520per%2520context%2520window%2520and%250Adelivering%2520up%2520to%252050x%2520amortized%2520throughput%2520via%2520batch%2520inference.%250A%2520%2520Despite%2520a%2520modest%2520setup%2520%2528Qwen-2.5-7B-Instruct%2520with%2520LoRA%2520rank%25208%2529%252C%250AMachineLearningLM%2520outperforms%2520strong%2520LLM%2520baselines%2520%2528e.g.%252C%2520GPT-5-mini%2529%2520by%2520an%250Aaverage%2520of%2520about%252015%2525%2520on%2520out-of-distribution%2520tabular%2520classification%2520across%250Afinance%252C%2520physics%252C%2520biology%252C%2520and%2520healthcare%2520domains.%2520It%2520exhibits%2520a%2520striking%250Amany-shot%2520scaling%2520law%253A%2520accuracy%2520increases%2520monotonically%2520as%2520in-context%250Ademonstrations%2520grow%2520from%25208%2520to%25201%252C024.%2520Without%2520any%2520task-specific%2520training%252C%2520it%250Aattains%2520random-forest-level%2520accuracy%2520across%2520hundreds%2520of%2520shots.%2520General%2520chat%250Acapabilities%252C%2520including%2520knowledge%2520and%2520reasoning%252C%2520are%2520preserved%253A%2520it%2520achieves%250A75.4%2525%2520on%2520MMLU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MachineLearningLM%3A%20Continued%20Pretraining%20Language%20Models%20on%20Millions%20of%0A%20%20Synthetic%20Tabular%20Prediction%20Tasks%20Scales%20In-Context%20ML&entry.906535625=Haoyu%20Dong%20and%20Pengkun%20Zhang%20and%20Mingzhe%20Lu%20and%20Yanzhen%20Shen%20and%20Guolin%20Ke&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20possess%20broad%20world%20knowledge%20and%20strong%0Ageneral-purpose%20reasoning%20ability%2C%20yet%20they%20struggle%20to%20learn%20from%20many%0Ain-context%20examples%20on%20standard%20machine%20learning%20%28ML%29%20tasks%2C%20that%20is%2C%20to%0Aleverage%20many-shot%20demonstrations%20purely%20via%20in-context%20learning%20%28ICL%29%20without%0Agradient%20descent.%20We%20introduce%20MachineLearningLM%2C%20a%20portable%0Acontinued-pretraining%20framework%20that%20equips%20a%20general-purpose%20LLM%20with%20robust%0Ain-context%20ML%20capability%20while%20preserving%20its%20general%20knowledge%20and%20reasoning%0Afor%20broader%20chat%20workflows.%0A%20%20Our%20pretraining%20procedure%20synthesizes%20ML%20tasks%20from%20millions%20of%20structural%0Acausal%20models%20%28SCMs%29%2C%20spanning%20shot%20counts%20up%20to%201%2C024.%20We%20begin%20with%20a%0Arandom-forest%20teacher%2C%20distilling%20tree-based%20decision%20strategies%20into%20the%20LLM%0Ato%20strengthen%20robustness%20in%20numerical%20modeling.%20All%20tasks%20are%20serialized%20with%20a%0Atoken-efficient%20prompt%2C%20enabling%203x%20to%206x%20more%20examples%20per%20context%20window%20and%0Adelivering%20up%20to%2050x%20amortized%20throughput%20via%20batch%20inference.%0A%20%20Despite%20a%20modest%20setup%20%28Qwen-2.5-7B-Instruct%20with%20LoRA%20rank%208%29%2C%0AMachineLearningLM%20outperforms%20strong%20LLM%20baselines%20%28e.g.%2C%20GPT-5-mini%29%20by%20an%0Aaverage%20of%20about%2015%25%20on%20out-of-distribution%20tabular%20classification%20across%0Afinance%2C%20physics%2C%20biology%2C%20and%20healthcare%20domains.%20It%20exhibits%20a%20striking%0Amany-shot%20scaling%20law%3A%20accuracy%20increases%20monotonically%20as%20in-context%0Ademonstrations%20grow%20from%208%20to%201%2C024.%20Without%20any%20task-specific%20training%2C%20it%0Aattains%20random-forest-level%20accuracy%20across%20hundreds%20of%20shots.%20General%20chat%0Acapabilities%2C%20including%20knowledge%20and%20reasoning%2C%20are%20preserved%3A%20it%20achieves%0A75.4%25%20on%20MMLU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06806v1&entry.124074799=Read"},
{"title": "Preacher: Paper-to-Video Agentic System", "author": "Jingwei Liu and Ling Yang and Hao Luo and Fan Wang and Hongyan Li and Mengdi Wang", "abstract": "  The paper-to-video task converts a research paper into a structured video\nabstract, distilling key concepts, methods, and conclusions into an accessible,\nwell-organized format. While state-of-the-art video generation models\ndemonstrate potential, they are constrained by limited context windows, rigid\nvideo duration constraints, limited stylistic diversity, and an inability to\nrepresent domain-specific knowledge. To address these limitations, we introduce\nPreacher, the first paper-to-video agentic system. Preacher employs a topdown\napproach to decompose, summarize, and reformulate the paper, followed by\nbottom-up video generation, synthesizing diverse video segments into a coherent\nabstract. To align cross-modal representations, we define key scenes and\nintroduce a Progressive Chain of Thought (P-CoT) for granular, iterative\nplanning. Preacher successfully generates high-quality video abstracts across\nfive research fields, demonstrating expertise beyond current video generation\nmodels. Code will be released at: https://github.com/Gen-Verse/Paper2Video\n", "link": "http://arxiv.org/abs/2508.09632v6", "date": "2025-09-08", "relevancy": 2.6942, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5534}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5319}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preacher%3A%20Paper-to-Video%20Agentic%20System&body=Title%3A%20Preacher%3A%20Paper-to-Video%20Agentic%20System%0AAuthor%3A%20Jingwei%20Liu%20and%20Ling%20Yang%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Hongyan%20Li%20and%20Mengdi%20Wang%0AAbstract%3A%20%20%20The%20paper-to-video%20task%20converts%20a%20research%20paper%20into%20a%20structured%20video%0Aabstract%2C%20distilling%20key%20concepts%2C%20methods%2C%20and%20conclusions%20into%20an%20accessible%2C%0Awell-organized%20format.%20While%20state-of-the-art%20video%20generation%20models%0Ademonstrate%20potential%2C%20they%20are%20constrained%20by%20limited%20context%20windows%2C%20rigid%0Avideo%20duration%20constraints%2C%20limited%20stylistic%20diversity%2C%20and%20an%20inability%20to%0Arepresent%20domain-specific%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%0APreacher%2C%20the%20first%20paper-to-video%20agentic%20system.%20Preacher%20employs%20a%20topdown%0Aapproach%20to%20decompose%2C%20summarize%2C%20and%20reformulate%20the%20paper%2C%20followed%20by%0Abottom-up%20video%20generation%2C%20synthesizing%20diverse%20video%20segments%20into%20a%20coherent%0Aabstract.%20To%20align%20cross-modal%20representations%2C%20we%20define%20key%20scenes%20and%0Aintroduce%20a%20Progressive%20Chain%20of%20Thought%20%28P-CoT%29%20for%20granular%2C%20iterative%0Aplanning.%20Preacher%20successfully%20generates%20high-quality%20video%20abstracts%20across%0Afive%20research%20fields%2C%20demonstrating%20expertise%20beyond%20current%20video%20generation%0Amodels.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/Gen-Verse/Paper2Video%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09632v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreacher%253A%2520Paper-to-Video%2520Agentic%2520System%26entry.906535625%3DJingwei%2520Liu%2520and%2520Ling%2520Yang%2520and%2520Hao%2520Luo%2520and%2520Fan%2520Wang%2520and%2520Hongyan%2520Li%2520and%2520Mengdi%2520Wang%26entry.1292438233%3D%2520%2520The%2520paper-to-video%2520task%2520converts%2520a%2520research%2520paper%2520into%2520a%2520structured%2520video%250Aabstract%252C%2520distilling%2520key%2520concepts%252C%2520methods%252C%2520and%2520conclusions%2520into%2520an%2520accessible%252C%250Awell-organized%2520format.%2520While%2520state-of-the-art%2520video%2520generation%2520models%250Ademonstrate%2520potential%252C%2520they%2520are%2520constrained%2520by%2520limited%2520context%2520windows%252C%2520rigid%250Avideo%2520duration%2520constraints%252C%2520limited%2520stylistic%2520diversity%252C%2520and%2520an%2520inability%2520to%250Arepresent%2520domain-specific%2520knowledge.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250APreacher%252C%2520the%2520first%2520paper-to-video%2520agentic%2520system.%2520Preacher%2520employs%2520a%2520topdown%250Aapproach%2520to%2520decompose%252C%2520summarize%252C%2520and%2520reformulate%2520the%2520paper%252C%2520followed%2520by%250Abottom-up%2520video%2520generation%252C%2520synthesizing%2520diverse%2520video%2520segments%2520into%2520a%2520coherent%250Aabstract.%2520To%2520align%2520cross-modal%2520representations%252C%2520we%2520define%2520key%2520scenes%2520and%250Aintroduce%2520a%2520Progressive%2520Chain%2520of%2520Thought%2520%2528P-CoT%2529%2520for%2520granular%252C%2520iterative%250Aplanning.%2520Preacher%2520successfully%2520generates%2520high-quality%2520video%2520abstracts%2520across%250Afive%2520research%2520fields%252C%2520demonstrating%2520expertise%2520beyond%2520current%2520video%2520generation%250Amodels.%2520Code%2520will%2520be%2520released%2520at%253A%2520https%253A//github.com/Gen-Verse/Paper2Video%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09632v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preacher%3A%20Paper-to-Video%20Agentic%20System&entry.906535625=Jingwei%20Liu%20and%20Ling%20Yang%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Hongyan%20Li%20and%20Mengdi%20Wang&entry.1292438233=%20%20The%20paper-to-video%20task%20converts%20a%20research%20paper%20into%20a%20structured%20video%0Aabstract%2C%20distilling%20key%20concepts%2C%20methods%2C%20and%20conclusions%20into%20an%20accessible%2C%0Awell-organized%20format.%20While%20state-of-the-art%20video%20generation%20models%0Ademonstrate%20potential%2C%20they%20are%20constrained%20by%20limited%20context%20windows%2C%20rigid%0Avideo%20duration%20constraints%2C%20limited%20stylistic%20diversity%2C%20and%20an%20inability%20to%0Arepresent%20domain-specific%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%0APreacher%2C%20the%20first%20paper-to-video%20agentic%20system.%20Preacher%20employs%20a%20topdown%0Aapproach%20to%20decompose%2C%20summarize%2C%20and%20reformulate%20the%20paper%2C%20followed%20by%0Abottom-up%20video%20generation%2C%20synthesizing%20diverse%20video%20segments%20into%20a%20coherent%0Aabstract.%20To%20align%20cross-modal%20representations%2C%20we%20define%20key%20scenes%20and%0Aintroduce%20a%20Progressive%20Chain%20of%20Thought%20%28P-CoT%29%20for%20granular%2C%20iterative%0Aplanning.%20Preacher%20successfully%20generates%20high-quality%20video%20abstracts%20across%0Afive%20research%20fields%2C%20demonstrating%20expertise%20beyond%20current%20video%20generation%0Amodels.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/Gen-Verse/Paper2Video%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09632v6&entry.124074799=Read"},
{"title": "Investigating Location-Regularised Self-Supervised Feature Learning for\n  Seafloor Visual Imagery", "author": "Cailei Liang and Adrian Bodenmann and Emma J Curtis and Samuel Simmons and Kazunori Nagano and Stan Brown and Adam Riese and Blair Thornton", "abstract": "  High-throughput interpretation of robotically gathered seafloor visual\nimagery can increase the efficiency of marine monitoring and exploration.\nAlthough recent research has suggested that location metadata can enhance\nself-supervised feature learning (SSL), its benefits across different SSL\nstrategies, models and seafloor image datasets are underexplored. This study\nevaluates the impact of location-based regularisation on six state-of-the-art\nSSL frameworks, which include Convolutional Neural Network (CNN) and Vision\nTransformer (ViT) models with varying latent-space dimensionality. Evaluation\nacross three diverse seafloor image datasets finds that location-regularisation\nconsistently improves downstream classification performance over standard SSL,\nwith average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for\nViTs, respectively. While CNNs pretrained on generic datasets benefit from\nhigh-dimensional latent representations, dataset-optimised SSL achieves similar\nperformance across the high (512) and low (128) dimensional latent\nrepresentations. Location-regularised SSL improves CNN performance over\npre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and\nlow-dimensional latent representations, respectively. For ViTs,\nhigh-dimensionality benefits both pre-trained and dataset-optimised SSL.\nAlthough location-regularisation improves SSL performance compared to standard\nSSL methods, pre-trained ViTs show strong generalisation, matching the\nbest-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$\nand $0.795 \\pm 0.077$, respectively. The findings highlight the value of\nlocation metadata for SSL regularisation, particularly when using\nlow-dimensional latent representations, and demonstrate strong generalisation\nof high-dimensional ViTs for seafloor image analysis.\n", "link": "http://arxiv.org/abs/2509.06660v1", "date": "2025-09-08", "relevancy": 2.6872, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5429}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Location-Regularised%20Self-Supervised%20Feature%20Learning%20for%0A%20%20Seafloor%20Visual%20Imagery&body=Title%3A%20Investigating%20Location-Regularised%20Self-Supervised%20Feature%20Learning%20for%0A%20%20Seafloor%20Visual%20Imagery%0AAuthor%3A%20Cailei%20Liang%20and%20Adrian%20Bodenmann%20and%20Emma%20J%20Curtis%20and%20Samuel%20Simmons%20and%20Kazunori%20Nagano%20and%20Stan%20Brown%20and%20Adam%20Riese%20and%20Blair%20Thornton%0AAbstract%3A%20%20%20High-throughput%20interpretation%20of%20robotically%20gathered%20seafloor%20visual%0Aimagery%20can%20increase%20the%20efficiency%20of%20marine%20monitoring%20and%20exploration.%0AAlthough%20recent%20research%20has%20suggested%20that%20location%20metadata%20can%20enhance%0Aself-supervised%20feature%20learning%20%28SSL%29%2C%20its%20benefits%20across%20different%20SSL%0Astrategies%2C%20models%20and%20seafloor%20image%20datasets%20are%20underexplored.%20This%20study%0Aevaluates%20the%20impact%20of%20location-based%20regularisation%20on%20six%20state-of-the-art%0ASSL%20frameworks%2C%20which%20include%20Convolutional%20Neural%20Network%20%28CNN%29%20and%20Vision%0ATransformer%20%28ViT%29%20models%20with%20varying%20latent-space%20dimensionality.%20Evaluation%0Aacross%20three%20diverse%20seafloor%20image%20datasets%20finds%20that%20location-regularisation%0Aconsistently%20improves%20downstream%20classification%20performance%20over%20standard%20SSL%2C%0Awith%20average%20F1-score%20gains%20of%20%244.9%20%5Cpm%204.0%25%24%20for%20CNNs%20and%20%246.3%20%5Cpm%208.9%25%24%20for%0AViTs%2C%20respectively.%20While%20CNNs%20pretrained%20on%20generic%20datasets%20benefit%20from%0Ahigh-dimensional%20latent%20representations%2C%20dataset-optimised%20SSL%20achieves%20similar%0Aperformance%20across%20the%20high%20%28512%29%20and%20low%20%28128%29%20dimensional%20latent%0Arepresentations.%20Location-regularised%20SSL%20improves%20CNN%20performance%20over%0Apre-trained%20models%20by%20%242.7%20%5Cpm%202.7%25%24%20and%20%2410.1%20%5Cpm%209.4%25%24%20for%20high%20and%0Alow-dimensional%20latent%20representations%2C%20respectively.%20For%20ViTs%2C%0Ahigh-dimensionality%20benefits%20both%20pre-trained%20and%20dataset-optimised%20SSL.%0AAlthough%20location-regularisation%20improves%20SSL%20performance%20compared%20to%20standard%0ASSL%20methods%2C%20pre-trained%20ViTs%20show%20strong%20generalisation%2C%20matching%20the%0Abest-performing%20location-regularised%20SSL%20with%20F1-scores%20of%20%240.795%20%5Cpm%200.075%24%0Aand%20%240.795%20%5Cpm%200.077%24%2C%20respectively.%20The%20findings%20highlight%20the%20value%20of%0Alocation%20metadata%20for%20SSL%20regularisation%2C%20particularly%20when%20using%0Alow-dimensional%20latent%20representations%2C%20and%20demonstrate%20strong%20generalisation%0Aof%20high-dimensional%20ViTs%20for%20seafloor%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Location-Regularised%2520Self-Supervised%2520Feature%2520Learning%2520for%250A%2520%2520Seafloor%2520Visual%2520Imagery%26entry.906535625%3DCailei%2520Liang%2520and%2520Adrian%2520Bodenmann%2520and%2520Emma%2520J%2520Curtis%2520and%2520Samuel%2520Simmons%2520and%2520Kazunori%2520Nagano%2520and%2520Stan%2520Brown%2520and%2520Adam%2520Riese%2520and%2520Blair%2520Thornton%26entry.1292438233%3D%2520%2520High-throughput%2520interpretation%2520of%2520robotically%2520gathered%2520seafloor%2520visual%250Aimagery%2520can%2520increase%2520the%2520efficiency%2520of%2520marine%2520monitoring%2520and%2520exploration.%250AAlthough%2520recent%2520research%2520has%2520suggested%2520that%2520location%2520metadata%2520can%2520enhance%250Aself-supervised%2520feature%2520learning%2520%2528SSL%2529%252C%2520its%2520benefits%2520across%2520different%2520SSL%250Astrategies%252C%2520models%2520and%2520seafloor%2520image%2520datasets%2520are%2520underexplored.%2520This%2520study%250Aevaluates%2520the%2520impact%2520of%2520location-based%2520regularisation%2520on%2520six%2520state-of-the-art%250ASSL%2520frameworks%252C%2520which%2520include%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520and%2520Vision%250ATransformer%2520%2528ViT%2529%2520models%2520with%2520varying%2520latent-space%2520dimensionality.%2520Evaluation%250Aacross%2520three%2520diverse%2520seafloor%2520image%2520datasets%2520finds%2520that%2520location-regularisation%250Aconsistently%2520improves%2520downstream%2520classification%2520performance%2520over%2520standard%2520SSL%252C%250Awith%2520average%2520F1-score%2520gains%2520of%2520%25244.9%2520%255Cpm%25204.0%2525%2524%2520for%2520CNNs%2520and%2520%25246.3%2520%255Cpm%25208.9%2525%2524%2520for%250AViTs%252C%2520respectively.%2520While%2520CNNs%2520pretrained%2520on%2520generic%2520datasets%2520benefit%2520from%250Ahigh-dimensional%2520latent%2520representations%252C%2520dataset-optimised%2520SSL%2520achieves%2520similar%250Aperformance%2520across%2520the%2520high%2520%2528512%2529%2520and%2520low%2520%2528128%2529%2520dimensional%2520latent%250Arepresentations.%2520Location-regularised%2520SSL%2520improves%2520CNN%2520performance%2520over%250Apre-trained%2520models%2520by%2520%25242.7%2520%255Cpm%25202.7%2525%2524%2520and%2520%252410.1%2520%255Cpm%25209.4%2525%2524%2520for%2520high%2520and%250Alow-dimensional%2520latent%2520representations%252C%2520respectively.%2520For%2520ViTs%252C%250Ahigh-dimensionality%2520benefits%2520both%2520pre-trained%2520and%2520dataset-optimised%2520SSL.%250AAlthough%2520location-regularisation%2520improves%2520SSL%2520performance%2520compared%2520to%2520standard%250ASSL%2520methods%252C%2520pre-trained%2520ViTs%2520show%2520strong%2520generalisation%252C%2520matching%2520the%250Abest-performing%2520location-regularised%2520SSL%2520with%2520F1-scores%2520of%2520%25240.795%2520%255Cpm%25200.075%2524%250Aand%2520%25240.795%2520%255Cpm%25200.077%2524%252C%2520respectively.%2520The%2520findings%2520highlight%2520the%2520value%2520of%250Alocation%2520metadata%2520for%2520SSL%2520regularisation%252C%2520particularly%2520when%2520using%250Alow-dimensional%2520latent%2520representations%252C%2520and%2520demonstrate%2520strong%2520generalisation%250Aof%2520high-dimensional%2520ViTs%2520for%2520seafloor%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Location-Regularised%20Self-Supervised%20Feature%20Learning%20for%0A%20%20Seafloor%20Visual%20Imagery&entry.906535625=Cailei%20Liang%20and%20Adrian%20Bodenmann%20and%20Emma%20J%20Curtis%20and%20Samuel%20Simmons%20and%20Kazunori%20Nagano%20and%20Stan%20Brown%20and%20Adam%20Riese%20and%20Blair%20Thornton&entry.1292438233=%20%20High-throughput%20interpretation%20of%20robotically%20gathered%20seafloor%20visual%0Aimagery%20can%20increase%20the%20efficiency%20of%20marine%20monitoring%20and%20exploration.%0AAlthough%20recent%20research%20has%20suggested%20that%20location%20metadata%20can%20enhance%0Aself-supervised%20feature%20learning%20%28SSL%29%2C%20its%20benefits%20across%20different%20SSL%0Astrategies%2C%20models%20and%20seafloor%20image%20datasets%20are%20underexplored.%20This%20study%0Aevaluates%20the%20impact%20of%20location-based%20regularisation%20on%20six%20state-of-the-art%0ASSL%20frameworks%2C%20which%20include%20Convolutional%20Neural%20Network%20%28CNN%29%20and%20Vision%0ATransformer%20%28ViT%29%20models%20with%20varying%20latent-space%20dimensionality.%20Evaluation%0Aacross%20three%20diverse%20seafloor%20image%20datasets%20finds%20that%20location-regularisation%0Aconsistently%20improves%20downstream%20classification%20performance%20over%20standard%20SSL%2C%0Awith%20average%20F1-score%20gains%20of%20%244.9%20%5Cpm%204.0%25%24%20for%20CNNs%20and%20%246.3%20%5Cpm%208.9%25%24%20for%0AViTs%2C%20respectively.%20While%20CNNs%20pretrained%20on%20generic%20datasets%20benefit%20from%0Ahigh-dimensional%20latent%20representations%2C%20dataset-optimised%20SSL%20achieves%20similar%0Aperformance%20across%20the%20high%20%28512%29%20and%20low%20%28128%29%20dimensional%20latent%0Arepresentations.%20Location-regularised%20SSL%20improves%20CNN%20performance%20over%0Apre-trained%20models%20by%20%242.7%20%5Cpm%202.7%25%24%20and%20%2410.1%20%5Cpm%209.4%25%24%20for%20high%20and%0Alow-dimensional%20latent%20representations%2C%20respectively.%20For%20ViTs%2C%0Ahigh-dimensionality%20benefits%20both%20pre-trained%20and%20dataset-optimised%20SSL.%0AAlthough%20location-regularisation%20improves%20SSL%20performance%20compared%20to%20standard%0ASSL%20methods%2C%20pre-trained%20ViTs%20show%20strong%20generalisation%2C%20matching%20the%0Abest-performing%20location-regularised%20SSL%20with%20F1-scores%20of%20%240.795%20%5Cpm%200.075%24%0Aand%20%240.795%20%5Cpm%200.077%24%2C%20respectively.%20The%20findings%20highlight%20the%20value%20of%0Alocation%20metadata%20for%20SSL%20regularisation%2C%20particularly%20when%20using%0Alow-dimensional%20latent%20representations%2C%20and%20demonstrate%20strong%20generalisation%0Aof%20high-dimensional%20ViTs%20for%20seafloor%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06660v1&entry.124074799=Read"},
{"title": "Curia: A Multi-Modal Foundation Model for Radiology", "author": "Corentin Dancette and Julien Khlaut and Antoine Saporta and Helene Philippe and Elodie Ferreres and Baptiste Callard and Th\u00e9o Danielou and L\u00e9o Alberge and L\u00e9o Machado and Daniel Tordjman and Julie Dupuis and Korentin Le Floch and Jean Du Terrail and Mariam Moshiri and Laurent Dercle and Tom Boeken and Jules Gregory and Maxime Ronot and Fran\u00e7ois Legou and Pascal Roux and Marc Sapoval and Pierre Manceron and Paul H\u00e9rent", "abstract": "  AI-assisted radiological interpretation is based on predominantly narrow,\nsingle-task models. This approach is impractical for covering the vast spectrum\nof imaging modalities, diseases, and radiological findings. Foundation models\n(FMs) hold the promise of broad generalization across modalities and in\nlow-data settings. However, this potential has remained largely unrealized in\nradiology. We introduce Curia, a foundation model trained on the entire\ncross-sectional imaging output of a major hospital over several years, which to\nour knowledge is the largest such corpus of real-world data-encompassing\n150,000 exams (130 TB). On a newly curated 19-task external validation\nbenchmark, Curia accurately identifies organs, detects conditions like brain\nhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.\nCuria meets or surpasses the performance of radiologists and recent foundation\nmodels, and exhibits clinically significant emergent properties in\ncross-modality, and low-data regimes. To accelerate progress, we release our\nbase model's weights at https://huggingface.co/raidium/curia.\n", "link": "http://arxiv.org/abs/2509.06830v1", "date": "2025-09-08", "relevancy": 2.6869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curia%3A%20A%20Multi-Modal%20Foundation%20Model%20for%20Radiology&body=Title%3A%20Curia%3A%20A%20Multi-Modal%20Foundation%20Model%20for%20Radiology%0AAuthor%3A%20Corentin%20Dancette%20and%20Julien%20Khlaut%20and%20Antoine%20Saporta%20and%20Helene%20Philippe%20and%20Elodie%20Ferreres%20and%20Baptiste%20Callard%20and%20Th%C3%A9o%20Danielou%20and%20L%C3%A9o%20Alberge%20and%20L%C3%A9o%20Machado%20and%20Daniel%20Tordjman%20and%20Julie%20Dupuis%20and%20Korentin%20Le%20Floch%20and%20Jean%20Du%20Terrail%20and%20Mariam%20Moshiri%20and%20Laurent%20Dercle%20and%20Tom%20Boeken%20and%20Jules%20Gregory%20and%20Maxime%20Ronot%20and%20Fran%C3%A7ois%20Legou%20and%20Pascal%20Roux%20and%20Marc%20Sapoval%20and%20Pierre%20Manceron%20and%20Paul%20H%C3%A9rent%0AAbstract%3A%20%20%20AI-assisted%20radiological%20interpretation%20is%20based%20on%20predominantly%20narrow%2C%0Asingle-task%20models.%20This%20approach%20is%20impractical%20for%20covering%20the%20vast%20spectrum%0Aof%20imaging%20modalities%2C%20diseases%2C%20and%20radiological%20findings.%20Foundation%20models%0A%28FMs%29%20hold%20the%20promise%20of%20broad%20generalization%20across%20modalities%20and%20in%0Alow-data%20settings.%20However%2C%20this%20potential%20has%20remained%20largely%20unrealized%20in%0Aradiology.%20We%20introduce%20Curia%2C%20a%20foundation%20model%20trained%20on%20the%20entire%0Across-sectional%20imaging%20output%20of%20a%20major%20hospital%20over%20several%20years%2C%20which%20to%0Aour%20knowledge%20is%20the%20largest%20such%20corpus%20of%20real-world%20data-encompassing%0A150%2C000%20exams%20%28130%20TB%29.%20On%20a%20newly%20curated%2019-task%20external%20validation%0Abenchmark%2C%20Curia%20accurately%20identifies%20organs%2C%20detects%20conditions%20like%20brain%0Ahemorrhages%20and%20myocardial%20infarctions%2C%20and%20predicts%20outcomes%20in%20tumor%20staging.%0ACuria%20meets%20or%20surpasses%20the%20performance%20of%20radiologists%20and%20recent%20foundation%0Amodels%2C%20and%20exhibits%20clinically%20significant%20emergent%20properties%20in%0Across-modality%2C%20and%20low-data%20regimes.%20To%20accelerate%20progress%2C%20we%20release%20our%0Abase%20model%27s%20weights%20at%20https%3A//huggingface.co/raidium/curia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCuria%253A%2520A%2520Multi-Modal%2520Foundation%2520Model%2520for%2520Radiology%26entry.906535625%3DCorentin%2520Dancette%2520and%2520Julien%2520Khlaut%2520and%2520Antoine%2520Saporta%2520and%2520Helene%2520Philippe%2520and%2520Elodie%2520Ferreres%2520and%2520Baptiste%2520Callard%2520and%2520Th%25C3%25A9o%2520Danielou%2520and%2520L%25C3%25A9o%2520Alberge%2520and%2520L%25C3%25A9o%2520Machado%2520and%2520Daniel%2520Tordjman%2520and%2520Julie%2520Dupuis%2520and%2520Korentin%2520Le%2520Floch%2520and%2520Jean%2520Du%2520Terrail%2520and%2520Mariam%2520Moshiri%2520and%2520Laurent%2520Dercle%2520and%2520Tom%2520Boeken%2520and%2520Jules%2520Gregory%2520and%2520Maxime%2520Ronot%2520and%2520Fran%25C3%25A7ois%2520Legou%2520and%2520Pascal%2520Roux%2520and%2520Marc%2520Sapoval%2520and%2520Pierre%2520Manceron%2520and%2520Paul%2520H%25C3%25A9rent%26entry.1292438233%3D%2520%2520AI-assisted%2520radiological%2520interpretation%2520is%2520based%2520on%2520predominantly%2520narrow%252C%250Asingle-task%2520models.%2520This%2520approach%2520is%2520impractical%2520for%2520covering%2520the%2520vast%2520spectrum%250Aof%2520imaging%2520modalities%252C%2520diseases%252C%2520and%2520radiological%2520findings.%2520Foundation%2520models%250A%2528FMs%2529%2520hold%2520the%2520promise%2520of%2520broad%2520generalization%2520across%2520modalities%2520and%2520in%250Alow-data%2520settings.%2520However%252C%2520this%2520potential%2520has%2520remained%2520largely%2520unrealized%2520in%250Aradiology.%2520We%2520introduce%2520Curia%252C%2520a%2520foundation%2520model%2520trained%2520on%2520the%2520entire%250Across-sectional%2520imaging%2520output%2520of%2520a%2520major%2520hospital%2520over%2520several%2520years%252C%2520which%2520to%250Aour%2520knowledge%2520is%2520the%2520largest%2520such%2520corpus%2520of%2520real-world%2520data-encompassing%250A150%252C000%2520exams%2520%2528130%2520TB%2529.%2520On%2520a%2520newly%2520curated%252019-task%2520external%2520validation%250Abenchmark%252C%2520Curia%2520accurately%2520identifies%2520organs%252C%2520detects%2520conditions%2520like%2520brain%250Ahemorrhages%2520and%2520myocardial%2520infarctions%252C%2520and%2520predicts%2520outcomes%2520in%2520tumor%2520staging.%250ACuria%2520meets%2520or%2520surpasses%2520the%2520performance%2520of%2520radiologists%2520and%2520recent%2520foundation%250Amodels%252C%2520and%2520exhibits%2520clinically%2520significant%2520emergent%2520properties%2520in%250Across-modality%252C%2520and%2520low-data%2520regimes.%2520To%2520accelerate%2520progress%252C%2520we%2520release%2520our%250Abase%2520model%2527s%2520weights%2520at%2520https%253A//huggingface.co/raidium/curia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curia%3A%20A%20Multi-Modal%20Foundation%20Model%20for%20Radiology&entry.906535625=Corentin%20Dancette%20and%20Julien%20Khlaut%20and%20Antoine%20Saporta%20and%20Helene%20Philippe%20and%20Elodie%20Ferreres%20and%20Baptiste%20Callard%20and%20Th%C3%A9o%20Danielou%20and%20L%C3%A9o%20Alberge%20and%20L%C3%A9o%20Machado%20and%20Daniel%20Tordjman%20and%20Julie%20Dupuis%20and%20Korentin%20Le%20Floch%20and%20Jean%20Du%20Terrail%20and%20Mariam%20Moshiri%20and%20Laurent%20Dercle%20and%20Tom%20Boeken%20and%20Jules%20Gregory%20and%20Maxime%20Ronot%20and%20Fran%C3%A7ois%20Legou%20and%20Pascal%20Roux%20and%20Marc%20Sapoval%20and%20Pierre%20Manceron%20and%20Paul%20H%C3%A9rent&entry.1292438233=%20%20AI-assisted%20radiological%20interpretation%20is%20based%20on%20predominantly%20narrow%2C%0Asingle-task%20models.%20This%20approach%20is%20impractical%20for%20covering%20the%20vast%20spectrum%0Aof%20imaging%20modalities%2C%20diseases%2C%20and%20radiological%20findings.%20Foundation%20models%0A%28FMs%29%20hold%20the%20promise%20of%20broad%20generalization%20across%20modalities%20and%20in%0Alow-data%20settings.%20However%2C%20this%20potential%20has%20remained%20largely%20unrealized%20in%0Aradiology.%20We%20introduce%20Curia%2C%20a%20foundation%20model%20trained%20on%20the%20entire%0Across-sectional%20imaging%20output%20of%20a%20major%20hospital%20over%20several%20years%2C%20which%20to%0Aour%20knowledge%20is%20the%20largest%20such%20corpus%20of%20real-world%20data-encompassing%0A150%2C000%20exams%20%28130%20TB%29.%20On%20a%20newly%20curated%2019-task%20external%20validation%0Abenchmark%2C%20Curia%20accurately%20identifies%20organs%2C%20detects%20conditions%20like%20brain%0Ahemorrhages%20and%20myocardial%20infarctions%2C%20and%20predicts%20outcomes%20in%20tumor%20staging.%0ACuria%20meets%20or%20surpasses%20the%20performance%20of%20radiologists%20and%20recent%20foundation%0Amodels%2C%20and%20exhibits%20clinically%20significant%20emergent%20properties%20in%0Across-modality%2C%20and%20low-data%20regimes.%20To%20accelerate%20progress%2C%20we%20release%20our%0Abase%20model%27s%20weights%20at%20https%3A//huggingface.co/raidium/curia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06830v1&entry.124074799=Read"},
{"title": "DCPO: Dynamic Clipping Policy Optimization", "author": "Shihui Yang and Chengfeng Dou and Peidong Guo and Kai Lu and Qiang Ju and Fei Deng and Rihui Xin", "abstract": "  Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npromising framework for enhancing the reasoning capabilities of large language\nmodels. However, existing approaches such as GRPO often suffer from zero\ngradients. This problem arises primarily due to fixed clipping bounds for\ntoken-level probability ratios and the standardization of identical rewards,\nwhich can lead to ineffective gradient updates and underutilization of\ngenerated responses. In this work, we propose Dynamic Clipping Policy\nOptimization(DCPO), which introduces a dynamic clipping strategy that\nadaptively adjusts clipping bounds based on token-specific prior probabilities\nto enhance token-level exploration, and a smooth advantage standardization\ntechnique that standardizes rewards across cumulative training steps to improve\nthe response-level effective utilization of generated responses. DCPO achieved\nstate-of-the-art performance on four benchmarks based on four different models.\nIn particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an\nAvg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO\n(36.7/31.6), GRPO (36.7/32.1) and GSPO (40.0/34.9) on the Qwen2.5-Math-7B\nmodel. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a\nperformance of (23.3/19.0), surpassing GRPO (13.3/10.5), DAPO (20.0/15.3) and\nGSPO (16.7/9.9). Furthermore, DCPO achieved an average 28% improvement in the\nnonzero advantage over GRPO in four models, doubled the training efficiency\nover DAPO, and significantly reduced the token clipping ratio by an order of\nmagnitude compared to both GRPO and DAPO, while achieving superior performance.\nThese results highlight DCPO's effectiveness in leveraging generated data more\nefficiently for reinforcement learning in large language models.\n", "link": "http://arxiv.org/abs/2509.02333v2", "date": "2025-09-08", "relevancy": 2.6385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCPO%3A%20Dynamic%20Clipping%20Policy%20Optimization&body=Title%3A%20DCPO%3A%20Dynamic%20Clipping%20Policy%20Optimization%0AAuthor%3A%20Shihui%20Yang%20and%20Chengfeng%20Dou%20and%20Peidong%20Guo%20and%20Kai%20Lu%20and%20Qiang%20Ju%20and%20Fei%20Deng%20and%20Rihui%20Xin%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apromising%20framework%20for%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%0Amodels.%20However%2C%20existing%20approaches%20such%20as%20GRPO%20often%20suffer%20from%20zero%0Agradients.%20This%20problem%20arises%20primarily%20due%20to%20fixed%20clipping%20bounds%20for%0Atoken-level%20probability%20ratios%20and%20the%20standardization%20of%20identical%20rewards%2C%0Awhich%20can%20lead%20to%20ineffective%20gradient%20updates%20and%20underutilization%20of%0Agenerated%20responses.%20In%20this%20work%2C%20we%20propose%20Dynamic%20Clipping%20Policy%0AOptimization%28DCPO%29%2C%20which%20introduces%20a%20dynamic%20clipping%20strategy%20that%0Aadaptively%20adjusts%20clipping%20bounds%20based%20on%20token-specific%20prior%20probabilities%0Ato%20enhance%20token-level%20exploration%2C%20and%20a%20smooth%20advantage%20standardization%0Atechnique%20that%20standardizes%20rewards%20across%20cumulative%20training%20steps%20to%20improve%0Athe%20response-level%20effective%20utilization%20of%20generated%20responses.%20DCPO%20achieved%0Astate-of-the-art%20performance%20on%20four%20benchmarks%20based%20on%20four%20different%20models.%0AIn%20particular%2C%20DCPO%20achieved%20an%20Avg%401%20of%2046.7%20under%20greedy%20decoding%20and%20an%0AAvg%4032%20of%2038.8%20under%2032%20times%20sampling%20on%20the%20AIME24%20benchmark%2C%20surpassing%20DAPO%0A%2836.7/31.6%29%2C%20GRPO%20%2836.7/32.1%29%20and%20GSPO%20%2840.0/34.9%29%20on%20the%20Qwen2.5-Math-7B%0Amodel.%20On%20the%20AIME25%20benchmark%20based%20on%20Qwen2.5-14B%2C%20DCPO%20achieves%20a%0Aperformance%20of%20%2823.3/19.0%29%2C%20surpassing%20GRPO%20%2813.3/10.5%29%2C%20DAPO%20%2820.0/15.3%29%20and%0AGSPO%20%2816.7/9.9%29.%20Furthermore%2C%20DCPO%20achieved%20an%20average%2028%25%20improvement%20in%20the%0Anonzero%20advantage%20over%20GRPO%20in%20four%20models%2C%20doubled%20the%20training%20efficiency%0Aover%20DAPO%2C%20and%20significantly%20reduced%20the%20token%20clipping%20ratio%20by%20an%20order%20of%0Amagnitude%20compared%20to%20both%20GRPO%20and%20DAPO%2C%20while%20achieving%20superior%20performance.%0AThese%20results%20highlight%20DCPO%27s%20effectiveness%20in%20leveraging%20generated%20data%20more%0Aefficiently%20for%20reinforcement%20learning%20in%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCPO%253A%2520Dynamic%2520Clipping%2520Policy%2520Optimization%26entry.906535625%3DShihui%2520Yang%2520and%2520Chengfeng%2520Dou%2520and%2520Peidong%2520Guo%2520and%2520Kai%2520Lu%2520and%2520Qiang%2520Ju%2520and%2520Fei%2520Deng%2520and%2520Rihui%2520Xin%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%250Apromising%2520framework%2520for%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%250Amodels.%2520However%252C%2520existing%2520approaches%2520such%2520as%2520GRPO%2520often%2520suffer%2520from%2520zero%250Agradients.%2520This%2520problem%2520arises%2520primarily%2520due%2520to%2520fixed%2520clipping%2520bounds%2520for%250Atoken-level%2520probability%2520ratios%2520and%2520the%2520standardization%2520of%2520identical%2520rewards%252C%250Awhich%2520can%2520lead%2520to%2520ineffective%2520gradient%2520updates%2520and%2520underutilization%2520of%250Agenerated%2520responses.%2520In%2520this%2520work%252C%2520we%2520propose%2520Dynamic%2520Clipping%2520Policy%250AOptimization%2528DCPO%2529%252C%2520which%2520introduces%2520a%2520dynamic%2520clipping%2520strategy%2520that%250Aadaptively%2520adjusts%2520clipping%2520bounds%2520based%2520on%2520token-specific%2520prior%2520probabilities%250Ato%2520enhance%2520token-level%2520exploration%252C%2520and%2520a%2520smooth%2520advantage%2520standardization%250Atechnique%2520that%2520standardizes%2520rewards%2520across%2520cumulative%2520training%2520steps%2520to%2520improve%250Athe%2520response-level%2520effective%2520utilization%2520of%2520generated%2520responses.%2520DCPO%2520achieved%250Astate-of-the-art%2520performance%2520on%2520four%2520benchmarks%2520based%2520on%2520four%2520different%2520models.%250AIn%2520particular%252C%2520DCPO%2520achieved%2520an%2520Avg%25401%2520of%252046.7%2520under%2520greedy%2520decoding%2520and%2520an%250AAvg%254032%2520of%252038.8%2520under%252032%2520times%2520sampling%2520on%2520the%2520AIME24%2520benchmark%252C%2520surpassing%2520DAPO%250A%252836.7/31.6%2529%252C%2520GRPO%2520%252836.7/32.1%2529%2520and%2520GSPO%2520%252840.0/34.9%2529%2520on%2520the%2520Qwen2.5-Math-7B%250Amodel.%2520On%2520the%2520AIME25%2520benchmark%2520based%2520on%2520Qwen2.5-14B%252C%2520DCPO%2520achieves%2520a%250Aperformance%2520of%2520%252823.3/19.0%2529%252C%2520surpassing%2520GRPO%2520%252813.3/10.5%2529%252C%2520DAPO%2520%252820.0/15.3%2529%2520and%250AGSPO%2520%252816.7/9.9%2529.%2520Furthermore%252C%2520DCPO%2520achieved%2520an%2520average%252028%2525%2520improvement%2520in%2520the%250Anonzero%2520advantage%2520over%2520GRPO%2520in%2520four%2520models%252C%2520doubled%2520the%2520training%2520efficiency%250Aover%2520DAPO%252C%2520and%2520significantly%2520reduced%2520the%2520token%2520clipping%2520ratio%2520by%2520an%2520order%2520of%250Amagnitude%2520compared%2520to%2520both%2520GRPO%2520and%2520DAPO%252C%2520while%2520achieving%2520superior%2520performance.%250AThese%2520results%2520highlight%2520DCPO%2527s%2520effectiveness%2520in%2520leveraging%2520generated%2520data%2520more%250Aefficiently%2520for%2520reinforcement%2520learning%2520in%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCPO%3A%20Dynamic%20Clipping%20Policy%20Optimization&entry.906535625=Shihui%20Yang%20and%20Chengfeng%20Dou%20and%20Peidong%20Guo%20and%20Kai%20Lu%20and%20Qiang%20Ju%20and%20Fei%20Deng%20and%20Rihui%20Xin&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apromising%20framework%20for%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%0Amodels.%20However%2C%20existing%20approaches%20such%20as%20GRPO%20often%20suffer%20from%20zero%0Agradients.%20This%20problem%20arises%20primarily%20due%20to%20fixed%20clipping%20bounds%20for%0Atoken-level%20probability%20ratios%20and%20the%20standardization%20of%20identical%20rewards%2C%0Awhich%20can%20lead%20to%20ineffective%20gradient%20updates%20and%20underutilization%20of%0Agenerated%20responses.%20In%20this%20work%2C%20we%20propose%20Dynamic%20Clipping%20Policy%0AOptimization%28DCPO%29%2C%20which%20introduces%20a%20dynamic%20clipping%20strategy%20that%0Aadaptively%20adjusts%20clipping%20bounds%20based%20on%20token-specific%20prior%20probabilities%0Ato%20enhance%20token-level%20exploration%2C%20and%20a%20smooth%20advantage%20standardization%0Atechnique%20that%20standardizes%20rewards%20across%20cumulative%20training%20steps%20to%20improve%0Athe%20response-level%20effective%20utilization%20of%20generated%20responses.%20DCPO%20achieved%0Astate-of-the-art%20performance%20on%20four%20benchmarks%20based%20on%20four%20different%20models.%0AIn%20particular%2C%20DCPO%20achieved%20an%20Avg%401%20of%2046.7%20under%20greedy%20decoding%20and%20an%0AAvg%4032%20of%2038.8%20under%2032%20times%20sampling%20on%20the%20AIME24%20benchmark%2C%20surpassing%20DAPO%0A%2836.7/31.6%29%2C%20GRPO%20%2836.7/32.1%29%20and%20GSPO%20%2840.0/34.9%29%20on%20the%20Qwen2.5-Math-7B%0Amodel.%20On%20the%20AIME25%20benchmark%20based%20on%20Qwen2.5-14B%2C%20DCPO%20achieves%20a%0Aperformance%20of%20%2823.3/19.0%29%2C%20surpassing%20GRPO%20%2813.3/10.5%29%2C%20DAPO%20%2820.0/15.3%29%20and%0AGSPO%20%2816.7/9.9%29.%20Furthermore%2C%20DCPO%20achieved%20an%20average%2028%25%20improvement%20in%20the%0Anonzero%20advantage%20over%20GRPO%20in%20four%20models%2C%20doubled%20the%20training%20efficiency%0Aover%20DAPO%2C%20and%20significantly%20reduced%20the%20token%20clipping%20ratio%20by%20an%20order%20of%0Amagnitude%20compared%20to%20both%20GRPO%20and%20DAPO%2C%20while%20achieving%20superior%20performance.%0AThese%20results%20highlight%20DCPO%27s%20effectiveness%20in%20leveraging%20generated%20data%20more%0Aefficiently%20for%20reinforcement%20learning%20in%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02333v2&entry.124074799=Read"},
{"title": "SLiNT: Structure-aware Language Model with Injection and Contrastive\n  Training for Knowledge Graph Completion", "author": "Mengxue Yang and Chun Yang and Jiaqi Zhu and Jiafan Li and Jingqi Zhang and Yuyang Li and Ying Li", "abstract": "  Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion.\n", "link": "http://arxiv.org/abs/2509.06531v1", "date": "2025-09-08", "relevancy": 2.6367, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLiNT%3A%20Structure-aware%20Language%20Model%20with%20Injection%20and%20Contrastive%0A%20%20Training%20for%20Knowledge%20Graph%20Completion&body=Title%3A%20SLiNT%3A%20Structure-aware%20Language%20Model%20with%20Injection%20and%20Contrastive%0A%20%20Training%20for%20Knowledge%20Graph%20Completion%0AAuthor%3A%20Mengxue%20Yang%20and%20Chun%20Yang%20and%20Jiaqi%20Zhu%20and%20Jiafan%20Li%20and%20Jingqi%20Zhang%20and%20Yuyang%20Li%20and%20Ying%20Li%0AAbstract%3A%20%20%20Link%20prediction%20in%20knowledge%20graphs%20requires%20integrating%20structural%0Ainformation%20and%20semantic%20context%20to%20infer%20missing%20entities.%20While%20large%0Alanguage%20models%20offer%20strong%20generative%20reasoning%20capabilities%2C%20their%20limited%0Aexploitation%20of%20structural%20signals%20often%20results%20in%20structural%20sparsity%20and%0Asemantic%20ambiguity%2C%20especially%20under%20incomplete%20or%20zero-shot%20settings.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20SLiNT%20%28Structure-aware%20Language%20model%20with%0AInjection%20and%20coNtrastive%20Training%29%2C%20a%20modular%20framework%20that%20injects%0Aknowledge-graph-derived%20structural%20context%20into%20a%20frozen%20LLM%20backbone%20with%0Alightweight%20LoRA-based%20adaptation%20for%20robust%20link%20prediction.%20Specifically%2C%0AStructure-Guided%20Neighborhood%20Enhancement%20%28SGNE%29%20retrieves%20pseudo-neighbors%20to%0Aenrich%20sparse%20entities%20and%20mitigate%20missing%20context%3B%20Dynamic%20Hard%20Contrastive%0ALearning%20%28DHCL%29%20introduces%20fine-grained%20supervision%20by%20interpolating%20hard%0Apositives%20and%20negatives%20to%20resolve%20entity-level%20ambiguity%3B%20and%0AGradient-Decoupled%20Dual%20Injection%20%28GDDI%29%20performs%20token-level%20structure-aware%0Aintervention%20while%20preserving%20the%20core%20LLM%20parameters.%20Experiments%20on%20WN18RR%0Aand%20FB15k-237%20show%20that%20SLiNT%20achieves%20superior%20or%20competitive%20performance%0Acompared%20with%20both%20embedding-based%20and%20generation-based%20baselines%2C%0Ademonstrating%20the%20effectiveness%20of%20structure-aware%20representation%20learning%20for%0Ascalable%20knowledge%20graph%20completion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLiNT%253A%2520Structure-aware%2520Language%2520Model%2520with%2520Injection%2520and%2520Contrastive%250A%2520%2520Training%2520for%2520Knowledge%2520Graph%2520Completion%26entry.906535625%3DMengxue%2520Yang%2520and%2520Chun%2520Yang%2520and%2520Jiaqi%2520Zhu%2520and%2520Jiafan%2520Li%2520and%2520Jingqi%2520Zhang%2520and%2520Yuyang%2520Li%2520and%2520Ying%2520Li%26entry.1292438233%3D%2520%2520Link%2520prediction%2520in%2520knowledge%2520graphs%2520requires%2520integrating%2520structural%250Ainformation%2520and%2520semantic%2520context%2520to%2520infer%2520missing%2520entities.%2520While%2520large%250Alanguage%2520models%2520offer%2520strong%2520generative%2520reasoning%2520capabilities%252C%2520their%2520limited%250Aexploitation%2520of%2520structural%2520signals%2520often%2520results%2520in%2520structural%2520sparsity%2520and%250Asemantic%2520ambiguity%252C%2520especially%2520under%2520incomplete%2520or%2520zero-shot%2520settings.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520SLiNT%2520%2528Structure-aware%2520Language%2520model%2520with%250AInjection%2520and%2520coNtrastive%2520Training%2529%252C%2520a%2520modular%2520framework%2520that%2520injects%250Aknowledge-graph-derived%2520structural%2520context%2520into%2520a%2520frozen%2520LLM%2520backbone%2520with%250Alightweight%2520LoRA-based%2520adaptation%2520for%2520robust%2520link%2520prediction.%2520Specifically%252C%250AStructure-Guided%2520Neighborhood%2520Enhancement%2520%2528SGNE%2529%2520retrieves%2520pseudo-neighbors%2520to%250Aenrich%2520sparse%2520entities%2520and%2520mitigate%2520missing%2520context%253B%2520Dynamic%2520Hard%2520Contrastive%250ALearning%2520%2528DHCL%2529%2520introduces%2520fine-grained%2520supervision%2520by%2520interpolating%2520hard%250Apositives%2520and%2520negatives%2520to%2520resolve%2520entity-level%2520ambiguity%253B%2520and%250AGradient-Decoupled%2520Dual%2520Injection%2520%2528GDDI%2529%2520performs%2520token-level%2520structure-aware%250Aintervention%2520while%2520preserving%2520the%2520core%2520LLM%2520parameters.%2520Experiments%2520on%2520WN18RR%250Aand%2520FB15k-237%2520show%2520that%2520SLiNT%2520achieves%2520superior%2520or%2520competitive%2520performance%250Acompared%2520with%2520both%2520embedding-based%2520and%2520generation-based%2520baselines%252C%250Ademonstrating%2520the%2520effectiveness%2520of%2520structure-aware%2520representation%2520learning%2520for%250Ascalable%2520knowledge%2520graph%2520completion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLiNT%3A%20Structure-aware%20Language%20Model%20with%20Injection%20and%20Contrastive%0A%20%20Training%20for%20Knowledge%20Graph%20Completion&entry.906535625=Mengxue%20Yang%20and%20Chun%20Yang%20and%20Jiaqi%20Zhu%20and%20Jiafan%20Li%20and%20Jingqi%20Zhang%20and%20Yuyang%20Li%20and%20Ying%20Li&entry.1292438233=%20%20Link%20prediction%20in%20knowledge%20graphs%20requires%20integrating%20structural%0Ainformation%20and%20semantic%20context%20to%20infer%20missing%20entities.%20While%20large%0Alanguage%20models%20offer%20strong%20generative%20reasoning%20capabilities%2C%20their%20limited%0Aexploitation%20of%20structural%20signals%20often%20results%20in%20structural%20sparsity%20and%0Asemantic%20ambiguity%2C%20especially%20under%20incomplete%20or%20zero-shot%20settings.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20SLiNT%20%28Structure-aware%20Language%20model%20with%0AInjection%20and%20coNtrastive%20Training%29%2C%20a%20modular%20framework%20that%20injects%0Aknowledge-graph-derived%20structural%20context%20into%20a%20frozen%20LLM%20backbone%20with%0Alightweight%20LoRA-based%20adaptation%20for%20robust%20link%20prediction.%20Specifically%2C%0AStructure-Guided%20Neighborhood%20Enhancement%20%28SGNE%29%20retrieves%20pseudo-neighbors%20to%0Aenrich%20sparse%20entities%20and%20mitigate%20missing%20context%3B%20Dynamic%20Hard%20Contrastive%0ALearning%20%28DHCL%29%20introduces%20fine-grained%20supervision%20by%20interpolating%20hard%0Apositives%20and%20negatives%20to%20resolve%20entity-level%20ambiguity%3B%20and%0AGradient-Decoupled%20Dual%20Injection%20%28GDDI%29%20performs%20token-level%20structure-aware%0Aintervention%20while%20preserving%20the%20core%20LLM%20parameters.%20Experiments%20on%20WN18RR%0Aand%20FB15k-237%20show%20that%20SLiNT%20achieves%20superior%20or%20competitive%20performance%0Acompared%20with%20both%20embedding-based%20and%20generation-based%20baselines%2C%0Ademonstrating%20the%20effectiveness%20of%20structure-aware%20representation%20learning%20for%0Ascalable%20knowledge%20graph%20completion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06531v1&entry.124074799=Read"},
{"title": "Contrastive Self-Supervised Network Intrusion Detection using Augmented\n  Negative Pairs", "author": "Jack Wilkie and Hanan Hindy and Christos Tachtatzis and Robert Atkinson", "abstract": "  Network intrusion detection remains a critical challenge in cybersecurity.\nWhile supervised machine learning models achieve state-of-the-art performance,\ntheir reliance on large labelled datasets makes them impractical for many\nreal-world applications. Anomaly detection methods, which train exclusively on\nbenign traffic to identify malicious activity, suffer from high false positive\nrates, limiting their usability. Recently, self-supervised learning techniques\nhave demonstrated improved performance with lower false positive rates by\nlearning discriminative latent representations of benign traffic. In\nparticular, contrastive self-supervised models achieve this by minimizing the\ndistance between similar (positive) views of benign traffic while maximizing it\nbetween dissimilar (negative) views. Existing approaches generate positive\nviews through data augmentation and treat other samples as negative. In\ncontrast, this work introduces Contrastive Learning using Augmented Negative\npairs (CLAN), a novel paradigm for network intrusion detection where augmented\nsamples are treated as negative views - representing potentially malicious\ndistributions - while other benign samples serve as positive views. This\napproach enhances both classification accuracy and inference efficiency after\npretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset\ndemonstrates that the proposed method surpasses existing self-supervised and\nanomaly detection techniques in a binary classification task. Furthermore, when\nfine-tuned on a limited labelled dataset, the proposed approach achieves\nsuperior multi-class classification performance compared to existing\nself-supervised models.\n", "link": "http://arxiv.org/abs/2509.06550v1", "date": "2025-09-08", "relevancy": 2.6137, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5324}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5257}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Self-Supervised%20Network%20Intrusion%20Detection%20using%20Augmented%0A%20%20Negative%20Pairs&body=Title%3A%20Contrastive%20Self-Supervised%20Network%20Intrusion%20Detection%20using%20Augmented%0A%20%20Negative%20Pairs%0AAuthor%3A%20Jack%20Wilkie%20and%20Hanan%20Hindy%20and%20Christos%20Tachtatzis%20and%20Robert%20Atkinson%0AAbstract%3A%20%20%20Network%20intrusion%20detection%20remains%20a%20critical%20challenge%20in%20cybersecurity.%0AWhile%20supervised%20machine%20learning%20models%20achieve%20state-of-the-art%20performance%2C%0Atheir%20reliance%20on%20large%20labelled%20datasets%20makes%20them%20impractical%20for%20many%0Areal-world%20applications.%20Anomaly%20detection%20methods%2C%20which%20train%20exclusively%20on%0Abenign%20traffic%20to%20identify%20malicious%20activity%2C%20suffer%20from%20high%20false%20positive%0Arates%2C%20limiting%20their%20usability.%20Recently%2C%20self-supervised%20learning%20techniques%0Ahave%20demonstrated%20improved%20performance%20with%20lower%20false%20positive%20rates%20by%0Alearning%20discriminative%20latent%20representations%20of%20benign%20traffic.%20In%0Aparticular%2C%20contrastive%20self-supervised%20models%20achieve%20this%20by%20minimizing%20the%0Adistance%20between%20similar%20%28positive%29%20views%20of%20benign%20traffic%20while%20maximizing%20it%0Abetween%20dissimilar%20%28negative%29%20views.%20Existing%20approaches%20generate%20positive%0Aviews%20through%20data%20augmentation%20and%20treat%20other%20samples%20as%20negative.%20In%0Acontrast%2C%20this%20work%20introduces%20Contrastive%20Learning%20using%20Augmented%20Negative%0Apairs%20%28CLAN%29%2C%20a%20novel%20paradigm%20for%20network%20intrusion%20detection%20where%20augmented%0Asamples%20are%20treated%20as%20negative%20views%20-%20representing%20potentially%20malicious%0Adistributions%20-%20while%20other%20benign%20samples%20serve%20as%20positive%20views.%20This%0Aapproach%20enhances%20both%20classification%20accuracy%20and%20inference%20efficiency%20after%0Apretraining%20on%20benign%20traffic.%20Experimental%20evaluation%20on%20the%20Lycos2017%20dataset%0Ademonstrates%20that%20the%20proposed%20method%20surpasses%20existing%20self-supervised%20and%0Aanomaly%20detection%20techniques%20in%20a%20binary%20classification%20task.%20Furthermore%2C%20when%0Afine-tuned%20on%20a%20limited%20labelled%20dataset%2C%20the%20proposed%20approach%20achieves%0Asuperior%20multi-class%20classification%20performance%20compared%20to%20existing%0Aself-supervised%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Self-Supervised%2520Network%2520Intrusion%2520Detection%2520using%2520Augmented%250A%2520%2520Negative%2520Pairs%26entry.906535625%3DJack%2520Wilkie%2520and%2520Hanan%2520Hindy%2520and%2520Christos%2520Tachtatzis%2520and%2520Robert%2520Atkinson%26entry.1292438233%3D%2520%2520Network%2520intrusion%2520detection%2520remains%2520a%2520critical%2520challenge%2520in%2520cybersecurity.%250AWhile%2520supervised%2520machine%2520learning%2520models%2520achieve%2520state-of-the-art%2520performance%252C%250Atheir%2520reliance%2520on%2520large%2520labelled%2520datasets%2520makes%2520them%2520impractical%2520for%2520many%250Areal-world%2520applications.%2520Anomaly%2520detection%2520methods%252C%2520which%2520train%2520exclusively%2520on%250Abenign%2520traffic%2520to%2520identify%2520malicious%2520activity%252C%2520suffer%2520from%2520high%2520false%2520positive%250Arates%252C%2520limiting%2520their%2520usability.%2520Recently%252C%2520self-supervised%2520learning%2520techniques%250Ahave%2520demonstrated%2520improved%2520performance%2520with%2520lower%2520false%2520positive%2520rates%2520by%250Alearning%2520discriminative%2520latent%2520representations%2520of%2520benign%2520traffic.%2520In%250Aparticular%252C%2520contrastive%2520self-supervised%2520models%2520achieve%2520this%2520by%2520minimizing%2520the%250Adistance%2520between%2520similar%2520%2528positive%2529%2520views%2520of%2520benign%2520traffic%2520while%2520maximizing%2520it%250Abetween%2520dissimilar%2520%2528negative%2529%2520views.%2520Existing%2520approaches%2520generate%2520positive%250Aviews%2520through%2520data%2520augmentation%2520and%2520treat%2520other%2520samples%2520as%2520negative.%2520In%250Acontrast%252C%2520this%2520work%2520introduces%2520Contrastive%2520Learning%2520using%2520Augmented%2520Negative%250Apairs%2520%2528CLAN%2529%252C%2520a%2520novel%2520paradigm%2520for%2520network%2520intrusion%2520detection%2520where%2520augmented%250Asamples%2520are%2520treated%2520as%2520negative%2520views%2520-%2520representing%2520potentially%2520malicious%250Adistributions%2520-%2520while%2520other%2520benign%2520samples%2520serve%2520as%2520positive%2520views.%2520This%250Aapproach%2520enhances%2520both%2520classification%2520accuracy%2520and%2520inference%2520efficiency%2520after%250Apretraining%2520on%2520benign%2520traffic.%2520Experimental%2520evaluation%2520on%2520the%2520Lycos2017%2520dataset%250Ademonstrates%2520that%2520the%2520proposed%2520method%2520surpasses%2520existing%2520self-supervised%2520and%250Aanomaly%2520detection%2520techniques%2520in%2520a%2520binary%2520classification%2520task.%2520Furthermore%252C%2520when%250Afine-tuned%2520on%2520a%2520limited%2520labelled%2520dataset%252C%2520the%2520proposed%2520approach%2520achieves%250Asuperior%2520multi-class%2520classification%2520performance%2520compared%2520to%2520existing%250Aself-supervised%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Self-Supervised%20Network%20Intrusion%20Detection%20using%20Augmented%0A%20%20Negative%20Pairs&entry.906535625=Jack%20Wilkie%20and%20Hanan%20Hindy%20and%20Christos%20Tachtatzis%20and%20Robert%20Atkinson&entry.1292438233=%20%20Network%20intrusion%20detection%20remains%20a%20critical%20challenge%20in%20cybersecurity.%0AWhile%20supervised%20machine%20learning%20models%20achieve%20state-of-the-art%20performance%2C%0Atheir%20reliance%20on%20large%20labelled%20datasets%20makes%20them%20impractical%20for%20many%0Areal-world%20applications.%20Anomaly%20detection%20methods%2C%20which%20train%20exclusively%20on%0Abenign%20traffic%20to%20identify%20malicious%20activity%2C%20suffer%20from%20high%20false%20positive%0Arates%2C%20limiting%20their%20usability.%20Recently%2C%20self-supervised%20learning%20techniques%0Ahave%20demonstrated%20improved%20performance%20with%20lower%20false%20positive%20rates%20by%0Alearning%20discriminative%20latent%20representations%20of%20benign%20traffic.%20In%0Aparticular%2C%20contrastive%20self-supervised%20models%20achieve%20this%20by%20minimizing%20the%0Adistance%20between%20similar%20%28positive%29%20views%20of%20benign%20traffic%20while%20maximizing%20it%0Abetween%20dissimilar%20%28negative%29%20views.%20Existing%20approaches%20generate%20positive%0Aviews%20through%20data%20augmentation%20and%20treat%20other%20samples%20as%20negative.%20In%0Acontrast%2C%20this%20work%20introduces%20Contrastive%20Learning%20using%20Augmented%20Negative%0Apairs%20%28CLAN%29%2C%20a%20novel%20paradigm%20for%20network%20intrusion%20detection%20where%20augmented%0Asamples%20are%20treated%20as%20negative%20views%20-%20representing%20potentially%20malicious%0Adistributions%20-%20while%20other%20benign%20samples%20serve%20as%20positive%20views.%20This%0Aapproach%20enhances%20both%20classification%20accuracy%20and%20inference%20efficiency%20after%0Apretraining%20on%20benign%20traffic.%20Experimental%20evaluation%20on%20the%20Lycos2017%20dataset%0Ademonstrates%20that%20the%20proposed%20method%20surpasses%20existing%20self-supervised%20and%0Aanomaly%20detection%20techniques%20in%20a%20binary%20classification%20task.%20Furthermore%2C%20when%0Afine-tuned%20on%20a%20limited%20labelled%20dataset%2C%20the%20proposed%20approach%20achieves%0Asuperior%20multi-class%20classification%20performance%20compared%20to%20existing%0Aself-supervised%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06550v1&entry.124074799=Read"},
{"title": "Learn2Reg 2024: New Benchmark Datasets Driving Progress on New\n  Challenges", "author": "Lasse Hansen and Wiebke Heyer and Christoph Gro\u00dfbr\u00f6hmer and Frederic Madesta and Thilo Sentker and Wang Jiazheng and Yuxi Zhang and Hang Zhang and Min Liu and Junyi Wang and Xi Zhu and Yuhua Li and Liwen Wang and Daniil Morozov and Nazim Haouchine and Joel Honkamaa and Pekka Marttinen and Yichao Zhou and Zuopeng Tan and Zhuoyuan Wang and Yi Wang and Hongchao Zhou and Shunbo Hu and Yi Zhang and Qian Tao and Lukas F\u00f6rner and Thomas Wendler and Bailiang Jian and Christian Wachinger and Jin Kim and Dan Ruan and Marek Wodzinski and Henning M\u00fcller and Tony C. W. Mok and Xi Jia and Jinming Duan and Mikael Brudfors and Seyed-Ahmad Ahmadi and Yunzheng Zhu and William Hsu and Tina Kapur and William M. Wells and Alexandra Golby and Aaron Carass and Harrison Bai and Yihao Liu and Perrine Paul-Gilloteaux and Joakim Lindblad and Nata\u0161a Sladoje and Andreas Walter and Junyu Chen and Reuben Dorent and Alessa Hering and Mattias P. Heinrich", "abstract": "  Medical image registration is critical for clinical applications, and fair\nbenchmarking of different methods is essential for monitoring ongoing progress.\nTo date, the Learn2Reg 2020-2023 challenges have released several complementary\ndatasets and established metrics for evaluations. However, these editions did\nnot capture all aspects of the registration problem, particularly in terms of\nmodality diversity and task complexity. To address these limitations, the 2024\nedition introduces three new tasks, including large-scale multi-modal\nregistration and unsupervised inter-subject brain registration, as well as the\nfirst microscopy-focused benchmark within Learn2Reg. The new datasets also\ninspired new method developments, including invertibility constraints, pyramid\nfeatures, keypoints alignment and instance optimisation.\n", "link": "http://arxiv.org/abs/2509.01217v2", "date": "2025-09-08", "relevancy": 2.5963, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn2Reg%202024%3A%20New%20Benchmark%20Datasets%20Driving%20Progress%20on%20New%0A%20%20Challenges&body=Title%3A%20Learn2Reg%202024%3A%20New%20Benchmark%20Datasets%20Driving%20Progress%20on%20New%0A%20%20Challenges%0AAuthor%3A%20Lasse%20Hansen%20and%20Wiebke%20Heyer%20and%20Christoph%20Gro%C3%9Fbr%C3%B6hmer%20and%20Frederic%20Madesta%20and%20Thilo%20Sentker%20and%20Wang%20Jiazheng%20and%20Yuxi%20Zhang%20and%20Hang%20Zhang%20and%20Min%20Liu%20and%20Junyi%20Wang%20and%20Xi%20Zhu%20and%20Yuhua%20Li%20and%20Liwen%20Wang%20and%20Daniil%20Morozov%20and%20Nazim%20Haouchine%20and%20Joel%20Honkamaa%20and%20Pekka%20Marttinen%20and%20Yichao%20Zhou%20and%20Zuopeng%20Tan%20and%20Zhuoyuan%20Wang%20and%20Yi%20Wang%20and%20Hongchao%20Zhou%20and%20Shunbo%20Hu%20and%20Yi%20Zhang%20and%20Qian%20Tao%20and%20Lukas%20F%C3%B6rner%20and%20Thomas%20Wendler%20and%20Bailiang%20Jian%20and%20Christian%20Wachinger%20and%20Jin%20Kim%20and%20Dan%20Ruan%20and%20Marek%20Wodzinski%20and%20Henning%20M%C3%BCller%20and%20Tony%20C.%20W.%20Mok%20and%20Xi%20Jia%20and%20Jinming%20Duan%20and%20Mikael%20Brudfors%20and%20Seyed-Ahmad%20Ahmadi%20and%20Yunzheng%20Zhu%20and%20William%20Hsu%20and%20Tina%20Kapur%20and%20William%20M.%20Wells%20and%20Alexandra%20Golby%20and%20Aaron%20Carass%20and%20Harrison%20Bai%20and%20Yihao%20Liu%20and%20Perrine%20Paul-Gilloteaux%20and%20Joakim%20Lindblad%20and%20Nata%C5%A1a%20Sladoje%20and%20Andreas%20Walter%20and%20Junyu%20Chen%20and%20Reuben%20Dorent%20and%20Alessa%20Hering%20and%20Mattias%20P.%20Heinrich%0AAbstract%3A%20%20%20Medical%20image%20registration%20is%20critical%20for%20clinical%20applications%2C%20and%20fair%0Abenchmarking%20of%20different%20methods%20is%20essential%20for%20monitoring%20ongoing%20progress.%0ATo%20date%2C%20the%20Learn2Reg%202020-2023%20challenges%20have%20released%20several%20complementary%0Adatasets%20and%20established%20metrics%20for%20evaluations.%20However%2C%20these%20editions%20did%0Anot%20capture%20all%20aspects%20of%20the%20registration%20problem%2C%20particularly%20in%20terms%20of%0Amodality%20diversity%20and%20task%20complexity.%20To%20address%20these%20limitations%2C%20the%202024%0Aedition%20introduces%20three%20new%20tasks%2C%20including%20large-scale%20multi-modal%0Aregistration%20and%20unsupervised%20inter-subject%20brain%20registration%2C%20as%20well%20as%20the%0Afirst%20microscopy-focused%20benchmark%20within%20Learn2Reg.%20The%20new%20datasets%20also%0Ainspired%20new%20method%20developments%2C%20including%20invertibility%20constraints%2C%20pyramid%0Afeatures%2C%20keypoints%20alignment%20and%20instance%20optimisation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01217v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn2Reg%25202024%253A%2520New%2520Benchmark%2520Datasets%2520Driving%2520Progress%2520on%2520New%250A%2520%2520Challenges%26entry.906535625%3DLasse%2520Hansen%2520and%2520Wiebke%2520Heyer%2520and%2520Christoph%2520Gro%25C3%259Fbr%25C3%25B6hmer%2520and%2520Frederic%2520Madesta%2520and%2520Thilo%2520Sentker%2520and%2520Wang%2520Jiazheng%2520and%2520Yuxi%2520Zhang%2520and%2520Hang%2520Zhang%2520and%2520Min%2520Liu%2520and%2520Junyi%2520Wang%2520and%2520Xi%2520Zhu%2520and%2520Yuhua%2520Li%2520and%2520Liwen%2520Wang%2520and%2520Daniil%2520Morozov%2520and%2520Nazim%2520Haouchine%2520and%2520Joel%2520Honkamaa%2520and%2520Pekka%2520Marttinen%2520and%2520Yichao%2520Zhou%2520and%2520Zuopeng%2520Tan%2520and%2520Zhuoyuan%2520Wang%2520and%2520Yi%2520Wang%2520and%2520Hongchao%2520Zhou%2520and%2520Shunbo%2520Hu%2520and%2520Yi%2520Zhang%2520and%2520Qian%2520Tao%2520and%2520Lukas%2520F%25C3%25B6rner%2520and%2520Thomas%2520Wendler%2520and%2520Bailiang%2520Jian%2520and%2520Christian%2520Wachinger%2520and%2520Jin%2520Kim%2520and%2520Dan%2520Ruan%2520and%2520Marek%2520Wodzinski%2520and%2520Henning%2520M%25C3%25BCller%2520and%2520Tony%2520C.%2520W.%2520Mok%2520and%2520Xi%2520Jia%2520and%2520Jinming%2520Duan%2520and%2520Mikael%2520Brudfors%2520and%2520Seyed-Ahmad%2520Ahmadi%2520and%2520Yunzheng%2520Zhu%2520and%2520William%2520Hsu%2520and%2520Tina%2520Kapur%2520and%2520William%2520M.%2520Wells%2520and%2520Alexandra%2520Golby%2520and%2520Aaron%2520Carass%2520and%2520Harrison%2520Bai%2520and%2520Yihao%2520Liu%2520and%2520Perrine%2520Paul-Gilloteaux%2520and%2520Joakim%2520Lindblad%2520and%2520Nata%25C5%25A1a%2520Sladoje%2520and%2520Andreas%2520Walter%2520and%2520Junyu%2520Chen%2520and%2520Reuben%2520Dorent%2520and%2520Alessa%2520Hering%2520and%2520Mattias%2520P.%2520Heinrich%26entry.1292438233%3D%2520%2520Medical%2520image%2520registration%2520is%2520critical%2520for%2520clinical%2520applications%252C%2520and%2520fair%250Abenchmarking%2520of%2520different%2520methods%2520is%2520essential%2520for%2520monitoring%2520ongoing%2520progress.%250ATo%2520date%252C%2520the%2520Learn2Reg%25202020-2023%2520challenges%2520have%2520released%2520several%2520complementary%250Adatasets%2520and%2520established%2520metrics%2520for%2520evaluations.%2520However%252C%2520these%2520editions%2520did%250Anot%2520capture%2520all%2520aspects%2520of%2520the%2520registration%2520problem%252C%2520particularly%2520in%2520terms%2520of%250Amodality%2520diversity%2520and%2520task%2520complexity.%2520To%2520address%2520these%2520limitations%252C%2520the%25202024%250Aedition%2520introduces%2520three%2520new%2520tasks%252C%2520including%2520large-scale%2520multi-modal%250Aregistration%2520and%2520unsupervised%2520inter-subject%2520brain%2520registration%252C%2520as%2520well%2520as%2520the%250Afirst%2520microscopy-focused%2520benchmark%2520within%2520Learn2Reg.%2520The%2520new%2520datasets%2520also%250Ainspired%2520new%2520method%2520developments%252C%2520including%2520invertibility%2520constraints%252C%2520pyramid%250Afeatures%252C%2520keypoints%2520alignment%2520and%2520instance%2520optimisation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01217v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn2Reg%202024%3A%20New%20Benchmark%20Datasets%20Driving%20Progress%20on%20New%0A%20%20Challenges&entry.906535625=Lasse%20Hansen%20and%20Wiebke%20Heyer%20and%20Christoph%20Gro%C3%9Fbr%C3%B6hmer%20and%20Frederic%20Madesta%20and%20Thilo%20Sentker%20and%20Wang%20Jiazheng%20and%20Yuxi%20Zhang%20and%20Hang%20Zhang%20and%20Min%20Liu%20and%20Junyi%20Wang%20and%20Xi%20Zhu%20and%20Yuhua%20Li%20and%20Liwen%20Wang%20and%20Daniil%20Morozov%20and%20Nazim%20Haouchine%20and%20Joel%20Honkamaa%20and%20Pekka%20Marttinen%20and%20Yichao%20Zhou%20and%20Zuopeng%20Tan%20and%20Zhuoyuan%20Wang%20and%20Yi%20Wang%20and%20Hongchao%20Zhou%20and%20Shunbo%20Hu%20and%20Yi%20Zhang%20and%20Qian%20Tao%20and%20Lukas%20F%C3%B6rner%20and%20Thomas%20Wendler%20and%20Bailiang%20Jian%20and%20Christian%20Wachinger%20and%20Jin%20Kim%20and%20Dan%20Ruan%20and%20Marek%20Wodzinski%20and%20Henning%20M%C3%BCller%20and%20Tony%20C.%20W.%20Mok%20and%20Xi%20Jia%20and%20Jinming%20Duan%20and%20Mikael%20Brudfors%20and%20Seyed-Ahmad%20Ahmadi%20and%20Yunzheng%20Zhu%20and%20William%20Hsu%20and%20Tina%20Kapur%20and%20William%20M.%20Wells%20and%20Alexandra%20Golby%20and%20Aaron%20Carass%20and%20Harrison%20Bai%20and%20Yihao%20Liu%20and%20Perrine%20Paul-Gilloteaux%20and%20Joakim%20Lindblad%20and%20Nata%C5%A1a%20Sladoje%20and%20Andreas%20Walter%20and%20Junyu%20Chen%20and%20Reuben%20Dorent%20and%20Alessa%20Hering%20and%20Mattias%20P.%20Heinrich&entry.1292438233=%20%20Medical%20image%20registration%20is%20critical%20for%20clinical%20applications%2C%20and%20fair%0Abenchmarking%20of%20different%20methods%20is%20essential%20for%20monitoring%20ongoing%20progress.%0ATo%20date%2C%20the%20Learn2Reg%202020-2023%20challenges%20have%20released%20several%20complementary%0Adatasets%20and%20established%20metrics%20for%20evaluations.%20However%2C%20these%20editions%20did%0Anot%20capture%20all%20aspects%20of%20the%20registration%20problem%2C%20particularly%20in%20terms%20of%0Amodality%20diversity%20and%20task%20complexity.%20To%20address%20these%20limitations%2C%20the%202024%0Aedition%20introduces%20three%20new%20tasks%2C%20including%20large-scale%20multi-modal%0Aregistration%20and%20unsupervised%20inter-subject%20brain%20registration%2C%20as%20well%20as%20the%0Afirst%20microscopy-focused%20benchmark%20within%20Learn2Reg.%20The%20new%20datasets%20also%0Ainspired%20new%20method%20developments%2C%20including%20invertibility%20constraints%2C%20pyramid%0Afeatures%2C%20keypoints%20alignment%20and%20instance%20optimisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01217v2&entry.124074799=Read"},
{"title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via\n  Test-Time Training", "author": "Ruicheng Zhang and Jun Zhou and Zunnan Xu and Zihao Liu and Jiehui Huang and Mingyang Zhang and Yu Sun and Xiu Li", "abstract": "  Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches.\n", "link": "http://arxiv.org/abs/2509.06723v1", "date": "2025-09-08", "relevancy": 2.5723, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6569}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.651}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%203D-Aware%20Trajectory-Guided%20image-to-video%20generation%20via%0A%20%20Test-Time%20Training&body=Title%3A%20Zero-shot%203D-Aware%20Trajectory-Guided%20image-to-video%20generation%20via%0A%20%20Test-Time%20Training%0AAuthor%3A%20Ruicheng%20Zhang%20and%20Jun%20Zhou%20and%20Zunnan%20Xu%20and%20Zihao%20Liu%20and%20Jiehui%20Huang%20and%20Mingyang%20Zhang%20and%20Yu%20Sun%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Trajectory-Guided%20image-to-video%20%28I2V%29%20generation%20aims%20to%20synthesize%20videos%0Athat%20adhere%20to%20user-specified%20motion%20instructions.%20Existing%20methods%20typically%0Arely%20on%20computationally%20expensive%20fine-tuning%20on%20scarce%20annotated%20datasets.%0AAlthough%20some%20zero-shot%20methods%20attempt%20to%20trajectory%20control%20in%20the%20latent%0Aspace%2C%20they%20may%20yield%20unrealistic%20motion%20by%20neglecting%203D%20perspective%20and%0Acreating%20a%20misalignment%20between%20the%20manipulated%20latents%20and%20the%20network%27s%20noise%0Apredictions.%20To%20address%20these%20challenges%2C%20we%20introduce%20Zo3T%2C%20a%20novel%20zero-shot%0Atest-time-training%20framework%20for%20trajectory-guided%20generation%20with%20three%20core%0Ainnovations%3A%20First%2C%20we%20incorporate%20a%203D-Aware%20Kinematic%20Projection%2C%20leveraging%0Ainferring%20scene%20depth%20to%20derive%20perspective-correct%20affine%20transformations%20for%0Atarget%20regions.%20Second%2C%20we%20introduce%20Trajectory-Guided%20Test-Time%20LoRA%2C%20a%0Amechanism%20that%20dynamically%20injects%20and%20optimizes%20ephemeral%20LoRA%20adapters%20into%0Athe%20denoising%20network%20alongside%20the%20latent%20state.%20Driven%20by%20a%20regional%20feature%0Aconsistency%20loss%2C%20this%20co-adaptation%20effectively%20enforces%20motion%20constraints%0Awhile%20allowing%20the%20pre-trained%20model%20to%20locally%20adapt%20its%20internal%0Arepresentations%20to%20the%20manipulated%20latent%2C%20thereby%20ensuring%20generative%20fidelity%0Aand%20on-manifold%20adherence.%20Finally%2C%20we%20develop%20Guidance%20Field%20Rectification%2C%0Awhich%20refines%20the%20denoising%20evolutionary%20path%20by%20optimizing%20the%20conditional%0Aguidance%20field%20through%20a%20one-step%20lookahead%20strategy%2C%20ensuring%20efficient%0Agenerative%20progression%20towards%20the%20target%20trajectory.%20Zo3T%20significantly%0Aenhances%203D%20realism%20and%20motion%20accuracy%20in%20trajectory-controlled%20I2V%0Ageneration%2C%20demonstrating%20superior%20performance%20over%20existing%20training-based%20and%0Azero-shot%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%25203D-Aware%2520Trajectory-Guided%2520image-to-video%2520generation%2520via%250A%2520%2520Test-Time%2520Training%26entry.906535625%3DRuicheng%2520Zhang%2520and%2520Jun%2520Zhou%2520and%2520Zunnan%2520Xu%2520and%2520Zihao%2520Liu%2520and%2520Jiehui%2520Huang%2520and%2520Mingyang%2520Zhang%2520and%2520Yu%2520Sun%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520Trajectory-Guided%2520image-to-video%2520%2528I2V%2529%2520generation%2520aims%2520to%2520synthesize%2520videos%250Athat%2520adhere%2520to%2520user-specified%2520motion%2520instructions.%2520Existing%2520methods%2520typically%250Arely%2520on%2520computationally%2520expensive%2520fine-tuning%2520on%2520scarce%2520annotated%2520datasets.%250AAlthough%2520some%2520zero-shot%2520methods%2520attempt%2520to%2520trajectory%2520control%2520in%2520the%2520latent%250Aspace%252C%2520they%2520may%2520yield%2520unrealistic%2520motion%2520by%2520neglecting%25203D%2520perspective%2520and%250Acreating%2520a%2520misalignment%2520between%2520the%2520manipulated%2520latents%2520and%2520the%2520network%2527s%2520noise%250Apredictions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Zo3T%252C%2520a%2520novel%2520zero-shot%250Atest-time-training%2520framework%2520for%2520trajectory-guided%2520generation%2520with%2520three%2520core%250Ainnovations%253A%2520First%252C%2520we%2520incorporate%2520a%25203D-Aware%2520Kinematic%2520Projection%252C%2520leveraging%250Ainferring%2520scene%2520depth%2520to%2520derive%2520perspective-correct%2520affine%2520transformations%2520for%250Atarget%2520regions.%2520Second%252C%2520we%2520introduce%2520Trajectory-Guided%2520Test-Time%2520LoRA%252C%2520a%250Amechanism%2520that%2520dynamically%2520injects%2520and%2520optimizes%2520ephemeral%2520LoRA%2520adapters%2520into%250Athe%2520denoising%2520network%2520alongside%2520the%2520latent%2520state.%2520Driven%2520by%2520a%2520regional%2520feature%250Aconsistency%2520loss%252C%2520this%2520co-adaptation%2520effectively%2520enforces%2520motion%2520constraints%250Awhile%2520allowing%2520the%2520pre-trained%2520model%2520to%2520locally%2520adapt%2520its%2520internal%250Arepresentations%2520to%2520the%2520manipulated%2520latent%252C%2520thereby%2520ensuring%2520generative%2520fidelity%250Aand%2520on-manifold%2520adherence.%2520Finally%252C%2520we%2520develop%2520Guidance%2520Field%2520Rectification%252C%250Awhich%2520refines%2520the%2520denoising%2520evolutionary%2520path%2520by%2520optimizing%2520the%2520conditional%250Aguidance%2520field%2520through%2520a%2520one-step%2520lookahead%2520strategy%252C%2520ensuring%2520efficient%250Agenerative%2520progression%2520towards%2520the%2520target%2520trajectory.%2520Zo3T%2520significantly%250Aenhances%25203D%2520realism%2520and%2520motion%2520accuracy%2520in%2520trajectory-controlled%2520I2V%250Ageneration%252C%2520demonstrating%2520superior%2520performance%2520over%2520existing%2520training-based%2520and%250Azero-shot%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%203D-Aware%20Trajectory-Guided%20image-to-video%20generation%20via%0A%20%20Test-Time%20Training&entry.906535625=Ruicheng%20Zhang%20and%20Jun%20Zhou%20and%20Zunnan%20Xu%20and%20Zihao%20Liu%20and%20Jiehui%20Huang%20and%20Mingyang%20Zhang%20and%20Yu%20Sun%20and%20Xiu%20Li&entry.1292438233=%20%20Trajectory-Guided%20image-to-video%20%28I2V%29%20generation%20aims%20to%20synthesize%20videos%0Athat%20adhere%20to%20user-specified%20motion%20instructions.%20Existing%20methods%20typically%0Arely%20on%20computationally%20expensive%20fine-tuning%20on%20scarce%20annotated%20datasets.%0AAlthough%20some%20zero-shot%20methods%20attempt%20to%20trajectory%20control%20in%20the%20latent%0Aspace%2C%20they%20may%20yield%20unrealistic%20motion%20by%20neglecting%203D%20perspective%20and%0Acreating%20a%20misalignment%20between%20the%20manipulated%20latents%20and%20the%20network%27s%20noise%0Apredictions.%20To%20address%20these%20challenges%2C%20we%20introduce%20Zo3T%2C%20a%20novel%20zero-shot%0Atest-time-training%20framework%20for%20trajectory-guided%20generation%20with%20three%20core%0Ainnovations%3A%20First%2C%20we%20incorporate%20a%203D-Aware%20Kinematic%20Projection%2C%20leveraging%0Ainferring%20scene%20depth%20to%20derive%20perspective-correct%20affine%20transformations%20for%0Atarget%20regions.%20Second%2C%20we%20introduce%20Trajectory-Guided%20Test-Time%20LoRA%2C%20a%0Amechanism%20that%20dynamically%20injects%20and%20optimizes%20ephemeral%20LoRA%20adapters%20into%0Athe%20denoising%20network%20alongside%20the%20latent%20state.%20Driven%20by%20a%20regional%20feature%0Aconsistency%20loss%2C%20this%20co-adaptation%20effectively%20enforces%20motion%20constraints%0Awhile%20allowing%20the%20pre-trained%20model%20to%20locally%20adapt%20its%20internal%0Arepresentations%20to%20the%20manipulated%20latent%2C%20thereby%20ensuring%20generative%20fidelity%0Aand%20on-manifold%20adherence.%20Finally%2C%20we%20develop%20Guidance%20Field%20Rectification%2C%0Awhich%20refines%20the%20denoising%20evolutionary%20path%20by%20optimizing%20the%20conditional%0Aguidance%20field%20through%20a%20one-step%20lookahead%20strategy%2C%20ensuring%20efficient%0Agenerative%20progression%20towards%20the%20target%20trajectory.%20Zo3T%20significantly%0Aenhances%203D%20realism%20and%20motion%20accuracy%20in%20trajectory-controlled%20I2V%0Ageneration%2C%20demonstrating%20superior%20performance%20over%20existing%20training-based%20and%0Azero-shot%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06723v1&entry.124074799=Read"},
{"title": "NF3DM: Combining Neural Fields and Deformation Models for 3D Non-Rigid\n  Motion Reconstruction", "author": "Aymen Merrouche and Stefanie Wuhrer and Edmond Boyer", "abstract": "  We introduce a novel, data-driven approach for reconstructing temporally\ncoherent 3D motion from unstructured and potentially partial observations of\nnon-rigidly deforming shapes. Our goal is to achieve high-fidelity motion\nreconstructions for shapes that undergo near-isometric deformations, such as\nhumans wearing loose clothing. The key novelty of our work lies in its ability\nto combine implicit shape representations with explicit mesh-based deformation\nmodels, enabling detailed and temporally coherent motion reconstructions\nwithout relying on parametric shape models or decoupling shape and motion. Each\nframe is represented as a neural field decoded from a feature space where\nobservations over time are fused, hence preserving geometric details present in\nthe input data. Temporal coherence is enforced with a near-isometric\ndeformation constraint between adjacent frames that applies to the underlying\nsurface in the neural field. Our method outperforms state-of-the-art\napproaches, as demonstrated by its application to human and animal motion\nsequences reconstructed from monocular depth videos.\n", "link": "http://arxiv.org/abs/2412.08511v2", "date": "2025-09-08", "relevancy": 2.5677, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6761}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6292}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NF3DM%3A%20Combining%20Neural%20Fields%20and%20Deformation%20Models%20for%203D%20Non-Rigid%0A%20%20Motion%20Reconstruction&body=Title%3A%20NF3DM%3A%20Combining%20Neural%20Fields%20and%20Deformation%20Models%20for%203D%20Non-Rigid%0A%20%20Motion%20Reconstruction%0AAuthor%3A%20Aymen%20Merrouche%20and%20Stefanie%20Wuhrer%20and%20Edmond%20Boyer%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%2C%20data-driven%20approach%20for%20reconstructing%20temporally%0Acoherent%203D%20motion%20from%20unstructured%20and%20potentially%20partial%20observations%20of%0Anon-rigidly%20deforming%20shapes.%20Our%20goal%20is%20to%20achieve%20high-fidelity%20motion%0Areconstructions%20for%20shapes%20that%20undergo%20near-isometric%20deformations%2C%20such%20as%0Ahumans%20wearing%20loose%20clothing.%20The%20key%20novelty%20of%20our%20work%20lies%20in%20its%20ability%0Ato%20combine%20implicit%20shape%20representations%20with%20explicit%20mesh-based%20deformation%0Amodels%2C%20enabling%20detailed%20and%20temporally%20coherent%20motion%20reconstructions%0Awithout%20relying%20on%20parametric%20shape%20models%20or%20decoupling%20shape%20and%20motion.%20Each%0Aframe%20is%20represented%20as%20a%20neural%20field%20decoded%20from%20a%20feature%20space%20where%0Aobservations%20over%20time%20are%20fused%2C%20hence%20preserving%20geometric%20details%20present%20in%0Athe%20input%20data.%20Temporal%20coherence%20is%20enforced%20with%20a%20near-isometric%0Adeformation%20constraint%20between%20adjacent%20frames%20that%20applies%20to%20the%20underlying%0Asurface%20in%20the%20neural%20field.%20Our%20method%20outperforms%20state-of-the-art%0Aapproaches%2C%20as%20demonstrated%20by%20its%20application%20to%20human%20and%20animal%20motion%0Asequences%20reconstructed%20from%20monocular%20depth%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNF3DM%253A%2520Combining%2520Neural%2520Fields%2520and%2520Deformation%2520Models%2520for%25203D%2520Non-Rigid%250A%2520%2520Motion%2520Reconstruction%26entry.906535625%3DAymen%2520Merrouche%2520and%2520Stefanie%2520Wuhrer%2520and%2520Edmond%2520Boyer%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%252C%2520data-driven%2520approach%2520for%2520reconstructing%2520temporally%250Acoherent%25203D%2520motion%2520from%2520unstructured%2520and%2520potentially%2520partial%2520observations%2520of%250Anon-rigidly%2520deforming%2520shapes.%2520Our%2520goal%2520is%2520to%2520achieve%2520high-fidelity%2520motion%250Areconstructions%2520for%2520shapes%2520that%2520undergo%2520near-isometric%2520deformations%252C%2520such%2520as%250Ahumans%2520wearing%2520loose%2520clothing.%2520The%2520key%2520novelty%2520of%2520our%2520work%2520lies%2520in%2520its%2520ability%250Ato%2520combine%2520implicit%2520shape%2520representations%2520with%2520explicit%2520mesh-based%2520deformation%250Amodels%252C%2520enabling%2520detailed%2520and%2520temporally%2520coherent%2520motion%2520reconstructions%250Awithout%2520relying%2520on%2520parametric%2520shape%2520models%2520or%2520decoupling%2520shape%2520and%2520motion.%2520Each%250Aframe%2520is%2520represented%2520as%2520a%2520neural%2520field%2520decoded%2520from%2520a%2520feature%2520space%2520where%250Aobservations%2520over%2520time%2520are%2520fused%252C%2520hence%2520preserving%2520geometric%2520details%2520present%2520in%250Athe%2520input%2520data.%2520Temporal%2520coherence%2520is%2520enforced%2520with%2520a%2520near-isometric%250Adeformation%2520constraint%2520between%2520adjacent%2520frames%2520that%2520applies%2520to%2520the%2520underlying%250Asurface%2520in%2520the%2520neural%2520field.%2520Our%2520method%2520outperforms%2520state-of-the-art%250Aapproaches%252C%2520as%2520demonstrated%2520by%2520its%2520application%2520to%2520human%2520and%2520animal%2520motion%250Asequences%2520reconstructed%2520from%2520monocular%2520depth%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NF3DM%3A%20Combining%20Neural%20Fields%20and%20Deformation%20Models%20for%203D%20Non-Rigid%0A%20%20Motion%20Reconstruction&entry.906535625=Aymen%20Merrouche%20and%20Stefanie%20Wuhrer%20and%20Edmond%20Boyer&entry.1292438233=%20%20We%20introduce%20a%20novel%2C%20data-driven%20approach%20for%20reconstructing%20temporally%0Acoherent%203D%20motion%20from%20unstructured%20and%20potentially%20partial%20observations%20of%0Anon-rigidly%20deforming%20shapes.%20Our%20goal%20is%20to%20achieve%20high-fidelity%20motion%0Areconstructions%20for%20shapes%20that%20undergo%20near-isometric%20deformations%2C%20such%20as%0Ahumans%20wearing%20loose%20clothing.%20The%20key%20novelty%20of%20our%20work%20lies%20in%20its%20ability%0Ato%20combine%20implicit%20shape%20representations%20with%20explicit%20mesh-based%20deformation%0Amodels%2C%20enabling%20detailed%20and%20temporally%20coherent%20motion%20reconstructions%0Awithout%20relying%20on%20parametric%20shape%20models%20or%20decoupling%20shape%20and%20motion.%20Each%0Aframe%20is%20represented%20as%20a%20neural%20field%20decoded%20from%20a%20feature%20space%20where%0Aobservations%20over%20time%20are%20fused%2C%20hence%20preserving%20geometric%20details%20present%20in%0Athe%20input%20data.%20Temporal%20coherence%20is%20enforced%20with%20a%20near-isometric%0Adeformation%20constraint%20between%20adjacent%20frames%20that%20applies%20to%20the%20underlying%0Asurface%20in%20the%20neural%20field.%20Our%20method%20outperforms%20state-of-the-art%0Aapproaches%2C%20as%20demonstrated%20by%20its%20application%20to%20human%20and%20animal%20motion%0Asequences%20reconstructed%20from%20monocular%20depth%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08511v2&entry.124074799=Read"},
{"title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models", "author": "Zheyu Zhang and Shuo Yang and Bardh Prenkaj and Gjergji Kasneci", "abstract": "  Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.\n", "link": "http://arxiv.org/abs/2507.18504v2", "date": "2025-09-08", "relevancy": 2.5413, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5016}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Features%20Deserve%20Attention%3A%20Graph-Guided%20Dependency%20Learning%20for%0A%20%20Tabular%20Data%20Generation%20with%20Language%20Models&body=Title%3A%20Not%20All%20Features%20Deserve%20Attention%3A%20Graph-Guided%20Dependency%20Learning%20for%0A%20%20Tabular%20Data%20Generation%20with%20Language%20Models%0AAuthor%3A%20Zheyu%20Zhang%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20strong%20potential%20for%20tabular%20data%0Ageneration%20by%20modeling%20textualized%20feature-value%20pairs.%20However%2C%20tabular%20data%0Ainherently%20exhibits%20sparse%20feature-level%20dependencies%2C%20where%20many%20feature%0Ainteractions%20are%20structurally%20insignificant.%20This%20creates%20a%20fundamental%0Amismatch%20as%20LLMs%27%20self-attention%20mechanism%20inevitably%20distributes%20focus%20across%0Aall%20pairs%2C%20diluting%20attention%20on%20critical%20relationships%2C%20particularly%20in%0Adatasets%20with%20complex%20dependencies%20or%20semantically%20ambiguous%20features.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20GraDe%20%28Graph-Guided%20Dependency%20Learning%29%2C%20a%0Anovel%20method%20that%20explicitly%20integrates%20sparse%20dependency%20graphs%20into%20LLMs%27%0Aattention%20mechanism.%20GraDe%20employs%20a%20lightweight%20dynamic%20graph%20learning%20module%0Aguided%20by%20externally%20extracted%20functional%20dependencies%2C%20prioritizing%20key%0Afeature%20interactions%20while%20suppressing%20irrelevant%20ones.%20Our%20experiments%20across%0Adiverse%20real-world%20datasets%20demonstrate%20that%20GraDe%20outperforms%20existing%0ALLM-based%20approaches%20by%20up%20to%2012%25%20on%20complex%20datasets%20while%20achieving%0Acompetitive%20results%20with%20state-of-the-art%20approaches%20in%20synthetic%20data%20quality.%0AOur%20method%20is%20minimally%20intrusive%20yet%20effective%2C%20offering%20a%20practical%20solution%0Afor%20structure-aware%20tabular%20data%20modeling%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Features%2520Deserve%2520Attention%253A%2520Graph-Guided%2520Dependency%2520Learning%2520for%250A%2520%2520Tabular%2520Data%2520Generation%2520with%2520Language%2520Models%26entry.906535625%3DZheyu%2520Zhang%2520and%2520Shuo%2520Yang%2520and%2520Bardh%2520Prenkaj%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520potential%2520for%2520tabular%2520data%250Ageneration%2520by%2520modeling%2520textualized%2520feature-value%2520pairs.%2520However%252C%2520tabular%2520data%250Ainherently%2520exhibits%2520sparse%2520feature-level%2520dependencies%252C%2520where%2520many%2520feature%250Ainteractions%2520are%2520structurally%2520insignificant.%2520This%2520creates%2520a%2520fundamental%250Amismatch%2520as%2520LLMs%2527%2520self-attention%2520mechanism%2520inevitably%2520distributes%2520focus%2520across%250Aall%2520pairs%252C%2520diluting%2520attention%2520on%2520critical%2520relationships%252C%2520particularly%2520in%250Adatasets%2520with%2520complex%2520dependencies%2520or%2520semantically%2520ambiguous%2520features.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520GraDe%2520%2528Graph-Guided%2520Dependency%2520Learning%2529%252C%2520a%250Anovel%2520method%2520that%2520explicitly%2520integrates%2520sparse%2520dependency%2520graphs%2520into%2520LLMs%2527%250Aattention%2520mechanism.%2520GraDe%2520employs%2520a%2520lightweight%2520dynamic%2520graph%2520learning%2520module%250Aguided%2520by%2520externally%2520extracted%2520functional%2520dependencies%252C%2520prioritizing%2520key%250Afeature%2520interactions%2520while%2520suppressing%2520irrelevant%2520ones.%2520Our%2520experiments%2520across%250Adiverse%2520real-world%2520datasets%2520demonstrate%2520that%2520GraDe%2520outperforms%2520existing%250ALLM-based%2520approaches%2520by%2520up%2520to%252012%2525%2520on%2520complex%2520datasets%2520while%2520achieving%250Acompetitive%2520results%2520with%2520state-of-the-art%2520approaches%2520in%2520synthetic%2520data%2520quality.%250AOur%2520method%2520is%2520minimally%2520intrusive%2520yet%2520effective%252C%2520offering%2520a%2520practical%2520solution%250Afor%2520structure-aware%2520tabular%2520data%2520modeling%2520with%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Features%20Deserve%20Attention%3A%20Graph-Guided%20Dependency%20Learning%20for%0A%20%20Tabular%20Data%20Generation%20with%20Language%20Models&entry.906535625=Zheyu%20Zhang%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20strong%20potential%20for%20tabular%20data%0Ageneration%20by%20modeling%20textualized%20feature-value%20pairs.%20However%2C%20tabular%20data%0Ainherently%20exhibits%20sparse%20feature-level%20dependencies%2C%20where%20many%20feature%0Ainteractions%20are%20structurally%20insignificant.%20This%20creates%20a%20fundamental%0Amismatch%20as%20LLMs%27%20self-attention%20mechanism%20inevitably%20distributes%20focus%20across%0Aall%20pairs%2C%20diluting%20attention%20on%20critical%20relationships%2C%20particularly%20in%0Adatasets%20with%20complex%20dependencies%20or%20semantically%20ambiguous%20features.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20GraDe%20%28Graph-Guided%20Dependency%20Learning%29%2C%20a%0Anovel%20method%20that%20explicitly%20integrates%20sparse%20dependency%20graphs%20into%20LLMs%27%0Aattention%20mechanism.%20GraDe%20employs%20a%20lightweight%20dynamic%20graph%20learning%20module%0Aguided%20by%20externally%20extracted%20functional%20dependencies%2C%20prioritizing%20key%0Afeature%20interactions%20while%20suppressing%20irrelevant%20ones.%20Our%20experiments%20across%0Adiverse%20real-world%20datasets%20demonstrate%20that%20GraDe%20outperforms%20existing%0ALLM-based%20approaches%20by%20up%20to%2012%25%20on%20complex%20datasets%20while%20achieving%0Acompetitive%20results%20with%20state-of-the-art%20approaches%20in%20synthetic%20data%20quality.%0AOur%20method%20is%20minimally%20intrusive%20yet%20effective%2C%20offering%20a%20practical%20solution%0Afor%20structure-aware%20tabular%20data%20modeling%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18504v2&entry.124074799=Read"},
{"title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large\n  Language Models", "author": "Xiang Meng and Kayhan Behdin and Haoyue Wang and Rahul Mazumder", "abstract": "  The impressive performance of Large Language Models (LLMs) across various\nnatural language processing tasks comes at the cost of vast computational\nresources and storage requirements. One-shot pruning techniques offer a way to\nalleviate these burdens by removing redundant weights without the need for\nretraining. Yet, the massive scale of LLMs often forces current pruning\napproaches to rely on heuristics instead of optimization-based techniques,\npotentially resulting in suboptimal compression. In this paper, we introduce\nALPS, an optimization-based framework that tackles the pruning problem using\nthe operator splitting technique and a preconditioned conjugate gradient-based\npost-processing step. Our approach incorporates novel techniques to accelerate\nand theoretically guarantee convergence while leveraging vectorization and GPU\nparallelism for efficiency. ALPS substantially outperforms state-of-the-art\nmethods in terms of the pruning objective and perplexity reduction,\nparticularly for highly sparse models. On the OPT-30B model with 70% sparsity,\nALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a\n19% improvement in zero-shot benchmark performance compared to existing\nmethods.\n", "link": "http://arxiv.org/abs/2406.07831v3", "date": "2025-09-08", "relevancy": 2.54, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALPS%3A%20Improved%20Optimization%20for%20Highly%20Sparse%20One-Shot%20Pruning%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20ALPS%3A%20Improved%20Optimization%20for%20Highly%20Sparse%20One-Shot%20Pruning%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Xiang%20Meng%20and%20Kayhan%20Behdin%20and%20Haoyue%20Wang%20and%20Rahul%20Mazumder%0AAbstract%3A%20%20%20The%20impressive%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%0Anatural%20language%20processing%20tasks%20comes%20at%20the%20cost%20of%20vast%20computational%0Aresources%20and%20storage%20requirements.%20One-shot%20pruning%20techniques%20offer%20a%20way%20to%0Aalleviate%20these%20burdens%20by%20removing%20redundant%20weights%20without%20the%20need%20for%0Aretraining.%20Yet%2C%20the%20massive%20scale%20of%20LLMs%20often%20forces%20current%20pruning%0Aapproaches%20to%20rely%20on%20heuristics%20instead%20of%20optimization-based%20techniques%2C%0Apotentially%20resulting%20in%20suboptimal%20compression.%20In%20this%20paper%2C%20we%20introduce%0AALPS%2C%20an%20optimization-based%20framework%20that%20tackles%20the%20pruning%20problem%20using%0Athe%20operator%20splitting%20technique%20and%20a%20preconditioned%20conjugate%20gradient-based%0Apost-processing%20step.%20Our%20approach%20incorporates%20novel%20techniques%20to%20accelerate%0Aand%20theoretically%20guarantee%20convergence%20while%20leveraging%20vectorization%20and%20GPU%0Aparallelism%20for%20efficiency.%20ALPS%20substantially%20outperforms%20state-of-the-art%0Amethods%20in%20terms%20of%20the%20pruning%20objective%20and%20perplexity%20reduction%2C%0Aparticularly%20for%20highly%20sparse%20models.%20On%20the%20OPT-30B%20model%20with%2070%25%20sparsity%2C%0AALPS%20achieves%20a%2013%25%20reduction%20in%20test%20perplexity%20on%20the%20WikiText%20dataset%20and%20a%0A19%25%20improvement%20in%20zero-shot%20benchmark%20performance%20compared%20to%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07831v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALPS%253A%2520Improved%2520Optimization%2520for%2520Highly%2520Sparse%2520One-Shot%2520Pruning%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DXiang%2520Meng%2520and%2520Kayhan%2520Behdin%2520and%2520Haoyue%2520Wang%2520and%2520Rahul%2520Mazumder%26entry.1292438233%3D%2520%2520The%2520impressive%2520performance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520various%250Anatural%2520language%2520processing%2520tasks%2520comes%2520at%2520the%2520cost%2520of%2520vast%2520computational%250Aresources%2520and%2520storage%2520requirements.%2520One-shot%2520pruning%2520techniques%2520offer%2520a%2520way%2520to%250Aalleviate%2520these%2520burdens%2520by%2520removing%2520redundant%2520weights%2520without%2520the%2520need%2520for%250Aretraining.%2520Yet%252C%2520the%2520massive%2520scale%2520of%2520LLMs%2520often%2520forces%2520current%2520pruning%250Aapproaches%2520to%2520rely%2520on%2520heuristics%2520instead%2520of%2520optimization-based%2520techniques%252C%250Apotentially%2520resulting%2520in%2520suboptimal%2520compression.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AALPS%252C%2520an%2520optimization-based%2520framework%2520that%2520tackles%2520the%2520pruning%2520problem%2520using%250Athe%2520operator%2520splitting%2520technique%2520and%2520a%2520preconditioned%2520conjugate%2520gradient-based%250Apost-processing%2520step.%2520Our%2520approach%2520incorporates%2520novel%2520techniques%2520to%2520accelerate%250Aand%2520theoretically%2520guarantee%2520convergence%2520while%2520leveraging%2520vectorization%2520and%2520GPU%250Aparallelism%2520for%2520efficiency.%2520ALPS%2520substantially%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520terms%2520of%2520the%2520pruning%2520objective%2520and%2520perplexity%2520reduction%252C%250Aparticularly%2520for%2520highly%2520sparse%2520models.%2520On%2520the%2520OPT-30B%2520model%2520with%252070%2525%2520sparsity%252C%250AALPS%2520achieves%2520a%252013%2525%2520reduction%2520in%2520test%2520perplexity%2520on%2520the%2520WikiText%2520dataset%2520and%2520a%250A19%2525%2520improvement%2520in%2520zero-shot%2520benchmark%2520performance%2520compared%2520to%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07831v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALPS%3A%20Improved%20Optimization%20for%20Highly%20Sparse%20One-Shot%20Pruning%20for%20Large%0A%20%20Language%20Models&entry.906535625=Xiang%20Meng%20and%20Kayhan%20Behdin%20and%20Haoyue%20Wang%20and%20Rahul%20Mazumder&entry.1292438233=%20%20The%20impressive%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%0Anatural%20language%20processing%20tasks%20comes%20at%20the%20cost%20of%20vast%20computational%0Aresources%20and%20storage%20requirements.%20One-shot%20pruning%20techniques%20offer%20a%20way%20to%0Aalleviate%20these%20burdens%20by%20removing%20redundant%20weights%20without%20the%20need%20for%0Aretraining.%20Yet%2C%20the%20massive%20scale%20of%20LLMs%20often%20forces%20current%20pruning%0Aapproaches%20to%20rely%20on%20heuristics%20instead%20of%20optimization-based%20techniques%2C%0Apotentially%20resulting%20in%20suboptimal%20compression.%20In%20this%20paper%2C%20we%20introduce%0AALPS%2C%20an%20optimization-based%20framework%20that%20tackles%20the%20pruning%20problem%20using%0Athe%20operator%20splitting%20technique%20and%20a%20preconditioned%20conjugate%20gradient-based%0Apost-processing%20step.%20Our%20approach%20incorporates%20novel%20techniques%20to%20accelerate%0Aand%20theoretically%20guarantee%20convergence%20while%20leveraging%20vectorization%20and%20GPU%0Aparallelism%20for%20efficiency.%20ALPS%20substantially%20outperforms%20state-of-the-art%0Amethods%20in%20terms%20of%20the%20pruning%20objective%20and%20perplexity%20reduction%2C%0Aparticularly%20for%20highly%20sparse%20models.%20On%20the%20OPT-30B%20model%20with%2070%25%20sparsity%2C%0AALPS%20achieves%20a%2013%25%20reduction%20in%20test%20perplexity%20on%20the%20WikiText%20dataset%20and%20a%0A19%25%20improvement%20in%20zero-shot%20benchmark%20performance%20compared%20to%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07831v3&entry.124074799=Read"},
{"title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated\n  Chain-of-Thought-based Reinforced Fine-Tuning", "author": "Wenqiao Zhu and Ji Liu and Rongjuncheng Zhang and Haipang Wu and Yulun Zhang", "abstract": "  Reasoning capability plays a significantly critical role in the the broad\napplications of Large Language Models (LLMs). To enhance the reasoning\nperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning\napproaches have been proposed to address the limited generalization capability\nof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their\neffectiveness, two major limitations hinder the advancement of LLMs. First,\nvanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and\nincorporate unstable reasoning path sampling, which typically results in model\ncollapse, unstable training process, and suboptimal performance. Second,\nexisting SFT approaches generally overemphasize the annotated CoT, potentially\nleading to performance degradation due to insufficient exploitation of\npotential CoT. In this paper, we propose a Contrastive learning with annotated\nCoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the\nreasoning performance of LLMs while addressing the aforementioned limitations.\nSpecifically, we propose learning a representation for each CoT. Based on this\nrepresentation, we design novel contrastive signals to guide the fine-tuning\nprocess. Our approach not only fully exploits the available annotated CoT but\nalso stabilizes the fine-tuning procedure by incorporating an additional\nunsupervised learning signal. We conduct comprehensive experiments and in-depth\nanalysis with three baseline approaches, two foundation models, and two\ndatasets to demonstrate significant advantages of \\TheName{} in terms of\nrobustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code\nis available at https://github.com/WNQzhu/CARFT.\n", "link": "http://arxiv.org/abs/2508.15868v2", "date": "2025-09-08", "relevancy": 2.5379, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5265}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARFT%3A%20Boosting%20LLM%20Reasoning%20via%20Contrastive%20Learning%20with%20Annotated%0A%20%20Chain-of-Thought-based%20Reinforced%20Fine-Tuning&body=Title%3A%20CARFT%3A%20Boosting%20LLM%20Reasoning%20via%20Contrastive%20Learning%20with%20Annotated%0A%20%20Chain-of-Thought-based%20Reinforced%20Fine-Tuning%0AAuthor%3A%20Wenqiao%20Zhu%20and%20Ji%20Liu%20and%20Rongjuncheng%20Zhang%20and%20Haipang%20Wu%20and%20Yulun%20Zhang%0AAbstract%3A%20%20%20Reasoning%20capability%20plays%20a%20significantly%20critical%20role%20in%20the%20the%20broad%0Aapplications%20of%20Large%20Language%20Models%20%28LLMs%29.%20To%20enhance%20the%20reasoning%0Aperformance%20of%20LLMs%2C%20diverse%20Reinforcement%20Learning%20%28RL%29-based%20fine-tuning%0Aapproaches%20have%20been%20proposed%20to%20address%20the%20limited%20generalization%20capability%0Aof%20LLMs%20trained%20solely%20via%20Supervised%20Fine-Tuning%20%28SFT%29.%20Despite%20their%0Aeffectiveness%2C%20two%20major%20limitations%20hinder%20the%20advancement%20of%20LLMs.%20First%2C%0Avanilla%20RL-based%20approaches%20ignore%20annotated%20Chain-of-Thought%20%28CoT%29%20and%0Aincorporate%20unstable%20reasoning%20path%20sampling%2C%20which%20typically%20results%20in%20model%0Acollapse%2C%20unstable%20training%20process%2C%20and%20suboptimal%20performance.%20Second%2C%0Aexisting%20SFT%20approaches%20generally%20overemphasize%20the%20annotated%20CoT%2C%20potentially%0Aleading%20to%20performance%20degradation%20due%20to%20insufficient%20exploitation%20of%0Apotential%20CoT.%20In%20this%20paper%2C%20we%20propose%20a%20Contrastive%20learning%20with%20annotated%0ACoT-based%20Reinforced%20Fine-Tuning%20approach%2C%20i.e.%2C%20%5CTheName%7B%7D%2C%20to%20enhance%20the%0Areasoning%20performance%20of%20LLMs%20while%20addressing%20the%20aforementioned%20limitations.%0ASpecifically%2C%20we%20propose%20learning%20a%20representation%20for%20each%20CoT.%20Based%20on%20this%0Arepresentation%2C%20we%20design%20novel%20contrastive%20signals%20to%20guide%20the%20fine-tuning%0Aprocess.%20Our%20approach%20not%20only%20fully%20exploits%20the%20available%20annotated%20CoT%20but%0Aalso%20stabilizes%20the%20fine-tuning%20procedure%20by%20incorporating%20an%20additional%0Aunsupervised%20learning%20signal.%20We%20conduct%20comprehensive%20experiments%20and%20in-depth%0Aanalysis%20with%20three%20baseline%20approaches%2C%20two%20foundation%20models%2C%20and%20two%0Adatasets%20to%20demonstrate%20significant%20advantages%20of%20%5CTheName%7B%7D%20in%20terms%20of%0Arobustness%2C%20performance%20%28up%20to%2010.15%5C%25%29%2C%20and%20efficiency%20%28up%20to%2030.62%5C%25%29.%20Code%0Ais%20available%20at%20https%3A//github.com/WNQzhu/CARFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARFT%253A%2520Boosting%2520LLM%2520Reasoning%2520via%2520Contrastive%2520Learning%2520with%2520Annotated%250A%2520%2520Chain-of-Thought-based%2520Reinforced%2520Fine-Tuning%26entry.906535625%3DWenqiao%2520Zhu%2520and%2520Ji%2520Liu%2520and%2520Rongjuncheng%2520Zhang%2520and%2520Haipang%2520Wu%2520and%2520Yulun%2520Zhang%26entry.1292438233%3D%2520%2520Reasoning%2520capability%2520plays%2520a%2520significantly%2520critical%2520role%2520in%2520the%2520the%2520broad%250Aapplications%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520To%2520enhance%2520the%2520reasoning%250Aperformance%2520of%2520LLMs%252C%2520diverse%2520Reinforcement%2520Learning%2520%2528RL%2529-based%2520fine-tuning%250Aapproaches%2520have%2520been%2520proposed%2520to%2520address%2520the%2520limited%2520generalization%2520capability%250Aof%2520LLMs%2520trained%2520solely%2520via%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529.%2520Despite%2520their%250Aeffectiveness%252C%2520two%2520major%2520limitations%2520hinder%2520the%2520advancement%2520of%2520LLMs.%2520First%252C%250Avanilla%2520RL-based%2520approaches%2520ignore%2520annotated%2520Chain-of-Thought%2520%2528CoT%2529%2520and%250Aincorporate%2520unstable%2520reasoning%2520path%2520sampling%252C%2520which%2520typically%2520results%2520in%2520model%250Acollapse%252C%2520unstable%2520training%2520process%252C%2520and%2520suboptimal%2520performance.%2520Second%252C%250Aexisting%2520SFT%2520approaches%2520generally%2520overemphasize%2520the%2520annotated%2520CoT%252C%2520potentially%250Aleading%2520to%2520performance%2520degradation%2520due%2520to%2520insufficient%2520exploitation%2520of%250Apotential%2520CoT.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Contrastive%2520learning%2520with%2520annotated%250ACoT-based%2520Reinforced%2520Fine-Tuning%2520approach%252C%2520i.e.%252C%2520%255CTheName%257B%257D%252C%2520to%2520enhance%2520the%250Areasoning%2520performance%2520of%2520LLMs%2520while%2520addressing%2520the%2520aforementioned%2520limitations.%250ASpecifically%252C%2520we%2520propose%2520learning%2520a%2520representation%2520for%2520each%2520CoT.%2520Based%2520on%2520this%250Arepresentation%252C%2520we%2520design%2520novel%2520contrastive%2520signals%2520to%2520guide%2520the%2520fine-tuning%250Aprocess.%2520Our%2520approach%2520not%2520only%2520fully%2520exploits%2520the%2520available%2520annotated%2520CoT%2520but%250Aalso%2520stabilizes%2520the%2520fine-tuning%2520procedure%2520by%2520incorporating%2520an%2520additional%250Aunsupervised%2520learning%2520signal.%2520We%2520conduct%2520comprehensive%2520experiments%2520and%2520in-depth%250Aanalysis%2520with%2520three%2520baseline%2520approaches%252C%2520two%2520foundation%2520models%252C%2520and%2520two%250Adatasets%2520to%2520demonstrate%2520significant%2520advantages%2520of%2520%255CTheName%257B%257D%2520in%2520terms%2520of%250Arobustness%252C%2520performance%2520%2528up%2520to%252010.15%255C%2525%2529%252C%2520and%2520efficiency%2520%2528up%2520to%252030.62%255C%2525%2529.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/WNQzhu/CARFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARFT%3A%20Boosting%20LLM%20Reasoning%20via%20Contrastive%20Learning%20with%20Annotated%0A%20%20Chain-of-Thought-based%20Reinforced%20Fine-Tuning&entry.906535625=Wenqiao%20Zhu%20and%20Ji%20Liu%20and%20Rongjuncheng%20Zhang%20and%20Haipang%20Wu%20and%20Yulun%20Zhang&entry.1292438233=%20%20Reasoning%20capability%20plays%20a%20significantly%20critical%20role%20in%20the%20the%20broad%0Aapplications%20of%20Large%20Language%20Models%20%28LLMs%29.%20To%20enhance%20the%20reasoning%0Aperformance%20of%20LLMs%2C%20diverse%20Reinforcement%20Learning%20%28RL%29-based%20fine-tuning%0Aapproaches%20have%20been%20proposed%20to%20address%20the%20limited%20generalization%20capability%0Aof%20LLMs%20trained%20solely%20via%20Supervised%20Fine-Tuning%20%28SFT%29.%20Despite%20their%0Aeffectiveness%2C%20two%20major%20limitations%20hinder%20the%20advancement%20of%20LLMs.%20First%2C%0Avanilla%20RL-based%20approaches%20ignore%20annotated%20Chain-of-Thought%20%28CoT%29%20and%0Aincorporate%20unstable%20reasoning%20path%20sampling%2C%20which%20typically%20results%20in%20model%0Acollapse%2C%20unstable%20training%20process%2C%20and%20suboptimal%20performance.%20Second%2C%0Aexisting%20SFT%20approaches%20generally%20overemphasize%20the%20annotated%20CoT%2C%20potentially%0Aleading%20to%20performance%20degradation%20due%20to%20insufficient%20exploitation%20of%0Apotential%20CoT.%20In%20this%20paper%2C%20we%20propose%20a%20Contrastive%20learning%20with%20annotated%0ACoT-based%20Reinforced%20Fine-Tuning%20approach%2C%20i.e.%2C%20%5CTheName%7B%7D%2C%20to%20enhance%20the%0Areasoning%20performance%20of%20LLMs%20while%20addressing%20the%20aforementioned%20limitations.%0ASpecifically%2C%20we%20propose%20learning%20a%20representation%20for%20each%20CoT.%20Based%20on%20this%0Arepresentation%2C%20we%20design%20novel%20contrastive%20signals%20to%20guide%20the%20fine-tuning%0Aprocess.%20Our%20approach%20not%20only%20fully%20exploits%20the%20available%20annotated%20CoT%20but%0Aalso%20stabilizes%20the%20fine-tuning%20procedure%20by%20incorporating%20an%20additional%0Aunsupervised%20learning%20signal.%20We%20conduct%20comprehensive%20experiments%20and%20in-depth%0Aanalysis%20with%20three%20baseline%20approaches%2C%20two%20foundation%20models%2C%20and%20two%0Adatasets%20to%20demonstrate%20significant%20advantages%20of%20%5CTheName%7B%7D%20in%20terms%20of%0Arobustness%2C%20performance%20%28up%20to%2010.15%5C%25%29%2C%20and%20efficiency%20%28up%20to%2030.62%5C%25%29.%20Code%0Ais%20available%20at%20https%3A//github.com/WNQzhu/CARFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15868v2&entry.124074799=Read"},
{"title": "Online Clustering of Seafloor Imagery for Interpretation during\n  Long-Term AUV Operations", "author": "Cailei Liang and Adrian Bodenmann and Sam Fenton and Blair Thornton", "abstract": "  As long-endurance and seafloor-resident AUVs become more capable, there is an\nincreasing need for extended, real-time interpretation of seafloor imagery to\nenable adaptive missions and optimise communication efficiency. Although\noffline image analysis methods are well established, they rely on access to\ncomplete datasets and human-labelled examples to manage the strong influence of\nenvironmental and operational conditions on seafloor image\nappearance-requirements that cannot be met in real-time settings. To address\nthis, we introduce an online clustering framework (OCF) capable of interpreting\nseafloor imagery without supervision, which is designed to operate in real-time\non continuous data streams in a scalable, adaptive, and self-consistent manner.\nThe method enables the efficient review and consolidation of common patterns\nacross the entire data history in constant time by identifying and maintaining\na set of representative samples that capture the evolving feature distribution,\nsupporting dynamic cluster merging and splitting without reprocessing the full\nimage history. We evaluate the framework on three diverse seafloor image\ndatasets, analysing the impact of different representative sampling strategies\non both clustering accuracy and computational cost. The OCF achieves the\nhighest average F1 score of 0.68 across the three datasets among all\ncomparative online clustering approaches, with a standard deviation of 3%\nacross three distinct survey trajectories, demonstrating its superior\nclustering capability and robustness to trajectory variation. In addition, it\nmaintains consistently lower and bounded computational time as the data volume\nincreases. These properties are beneficial for generating survey data summaries\nand supporting informative path planning in long-term, persistent autonomous\nmarine exploration.\n", "link": "http://arxiv.org/abs/2509.06678v1", "date": "2025-09-08", "relevancy": 2.5281, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5149}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.501}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Clustering%20of%20Seafloor%20Imagery%20for%20Interpretation%20during%0A%20%20Long-Term%20AUV%20Operations&body=Title%3A%20Online%20Clustering%20of%20Seafloor%20Imagery%20for%20Interpretation%20during%0A%20%20Long-Term%20AUV%20Operations%0AAuthor%3A%20Cailei%20Liang%20and%20Adrian%20Bodenmann%20and%20Sam%20Fenton%20and%20Blair%20Thornton%0AAbstract%3A%20%20%20As%20long-endurance%20and%20seafloor-resident%20AUVs%20become%20more%20capable%2C%20there%20is%20an%0Aincreasing%20need%20for%20extended%2C%20real-time%20interpretation%20of%20seafloor%20imagery%20to%0Aenable%20adaptive%20missions%20and%20optimise%20communication%20efficiency.%20Although%0Aoffline%20image%20analysis%20methods%20are%20well%20established%2C%20they%20rely%20on%20access%20to%0Acomplete%20datasets%20and%20human-labelled%20examples%20to%20manage%20the%20strong%20influence%20of%0Aenvironmental%20and%20operational%20conditions%20on%20seafloor%20image%0Aappearance-requirements%20that%20cannot%20be%20met%20in%20real-time%20settings.%20To%20address%0Athis%2C%20we%20introduce%20an%20online%20clustering%20framework%20%28OCF%29%20capable%20of%20interpreting%0Aseafloor%20imagery%20without%20supervision%2C%20which%20is%20designed%20to%20operate%20in%20real-time%0Aon%20continuous%20data%20streams%20in%20a%20scalable%2C%20adaptive%2C%20and%20self-consistent%20manner.%0AThe%20method%20enables%20the%20efficient%20review%20and%20consolidation%20of%20common%20patterns%0Aacross%20the%20entire%20data%20history%20in%20constant%20time%20by%20identifying%20and%20maintaining%0Aa%20set%20of%20representative%20samples%20that%20capture%20the%20evolving%20feature%20distribution%2C%0Asupporting%20dynamic%20cluster%20merging%20and%20splitting%20without%20reprocessing%20the%20full%0Aimage%20history.%20We%20evaluate%20the%20framework%20on%20three%20diverse%20seafloor%20image%0Adatasets%2C%20analysing%20the%20impact%20of%20different%20representative%20sampling%20strategies%0Aon%20both%20clustering%20accuracy%20and%20computational%20cost.%20The%20OCF%20achieves%20the%0Ahighest%20average%20F1%20score%20of%200.68%20across%20the%20three%20datasets%20among%20all%0Acomparative%20online%20clustering%20approaches%2C%20with%20a%20standard%20deviation%20of%203%25%0Aacross%20three%20distinct%20survey%20trajectories%2C%20demonstrating%20its%20superior%0Aclustering%20capability%20and%20robustness%20to%20trajectory%20variation.%20In%20addition%2C%20it%0Amaintains%20consistently%20lower%20and%20bounded%20computational%20time%20as%20the%20data%20volume%0Aincreases.%20These%20properties%20are%20beneficial%20for%20generating%20survey%20data%20summaries%0Aand%20supporting%20informative%20path%20planning%20in%20long-term%2C%20persistent%20autonomous%0Amarine%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Clustering%2520of%2520Seafloor%2520Imagery%2520for%2520Interpretation%2520during%250A%2520%2520Long-Term%2520AUV%2520Operations%26entry.906535625%3DCailei%2520Liang%2520and%2520Adrian%2520Bodenmann%2520and%2520Sam%2520Fenton%2520and%2520Blair%2520Thornton%26entry.1292438233%3D%2520%2520As%2520long-endurance%2520and%2520seafloor-resident%2520AUVs%2520become%2520more%2520capable%252C%2520there%2520is%2520an%250Aincreasing%2520need%2520for%2520extended%252C%2520real-time%2520interpretation%2520of%2520seafloor%2520imagery%2520to%250Aenable%2520adaptive%2520missions%2520and%2520optimise%2520communication%2520efficiency.%2520Although%250Aoffline%2520image%2520analysis%2520methods%2520are%2520well%2520established%252C%2520they%2520rely%2520on%2520access%2520to%250Acomplete%2520datasets%2520and%2520human-labelled%2520examples%2520to%2520manage%2520the%2520strong%2520influence%2520of%250Aenvironmental%2520and%2520operational%2520conditions%2520on%2520seafloor%2520image%250Aappearance-requirements%2520that%2520cannot%2520be%2520met%2520in%2520real-time%2520settings.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520an%2520online%2520clustering%2520framework%2520%2528OCF%2529%2520capable%2520of%2520interpreting%250Aseafloor%2520imagery%2520without%2520supervision%252C%2520which%2520is%2520designed%2520to%2520operate%2520in%2520real-time%250Aon%2520continuous%2520data%2520streams%2520in%2520a%2520scalable%252C%2520adaptive%252C%2520and%2520self-consistent%2520manner.%250AThe%2520method%2520enables%2520the%2520efficient%2520review%2520and%2520consolidation%2520of%2520common%2520patterns%250Aacross%2520the%2520entire%2520data%2520history%2520in%2520constant%2520time%2520by%2520identifying%2520and%2520maintaining%250Aa%2520set%2520of%2520representative%2520samples%2520that%2520capture%2520the%2520evolving%2520feature%2520distribution%252C%250Asupporting%2520dynamic%2520cluster%2520merging%2520and%2520splitting%2520without%2520reprocessing%2520the%2520full%250Aimage%2520history.%2520We%2520evaluate%2520the%2520framework%2520on%2520three%2520diverse%2520seafloor%2520image%250Adatasets%252C%2520analysing%2520the%2520impact%2520of%2520different%2520representative%2520sampling%2520strategies%250Aon%2520both%2520clustering%2520accuracy%2520and%2520computational%2520cost.%2520The%2520OCF%2520achieves%2520the%250Ahighest%2520average%2520F1%2520score%2520of%25200.68%2520across%2520the%2520three%2520datasets%2520among%2520all%250Acomparative%2520online%2520clustering%2520approaches%252C%2520with%2520a%2520standard%2520deviation%2520of%25203%2525%250Aacross%2520three%2520distinct%2520survey%2520trajectories%252C%2520demonstrating%2520its%2520superior%250Aclustering%2520capability%2520and%2520robustness%2520to%2520trajectory%2520variation.%2520In%2520addition%252C%2520it%250Amaintains%2520consistently%2520lower%2520and%2520bounded%2520computational%2520time%2520as%2520the%2520data%2520volume%250Aincreases.%2520These%2520properties%2520are%2520beneficial%2520for%2520generating%2520survey%2520data%2520summaries%250Aand%2520supporting%2520informative%2520path%2520planning%2520in%2520long-term%252C%2520persistent%2520autonomous%250Amarine%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Clustering%20of%20Seafloor%20Imagery%20for%20Interpretation%20during%0A%20%20Long-Term%20AUV%20Operations&entry.906535625=Cailei%20Liang%20and%20Adrian%20Bodenmann%20and%20Sam%20Fenton%20and%20Blair%20Thornton&entry.1292438233=%20%20As%20long-endurance%20and%20seafloor-resident%20AUVs%20become%20more%20capable%2C%20there%20is%20an%0Aincreasing%20need%20for%20extended%2C%20real-time%20interpretation%20of%20seafloor%20imagery%20to%0Aenable%20adaptive%20missions%20and%20optimise%20communication%20efficiency.%20Although%0Aoffline%20image%20analysis%20methods%20are%20well%20established%2C%20they%20rely%20on%20access%20to%0Acomplete%20datasets%20and%20human-labelled%20examples%20to%20manage%20the%20strong%20influence%20of%0Aenvironmental%20and%20operational%20conditions%20on%20seafloor%20image%0Aappearance-requirements%20that%20cannot%20be%20met%20in%20real-time%20settings.%20To%20address%0Athis%2C%20we%20introduce%20an%20online%20clustering%20framework%20%28OCF%29%20capable%20of%20interpreting%0Aseafloor%20imagery%20without%20supervision%2C%20which%20is%20designed%20to%20operate%20in%20real-time%0Aon%20continuous%20data%20streams%20in%20a%20scalable%2C%20adaptive%2C%20and%20self-consistent%20manner.%0AThe%20method%20enables%20the%20efficient%20review%20and%20consolidation%20of%20common%20patterns%0Aacross%20the%20entire%20data%20history%20in%20constant%20time%20by%20identifying%20and%20maintaining%0Aa%20set%20of%20representative%20samples%20that%20capture%20the%20evolving%20feature%20distribution%2C%0Asupporting%20dynamic%20cluster%20merging%20and%20splitting%20without%20reprocessing%20the%20full%0Aimage%20history.%20We%20evaluate%20the%20framework%20on%20three%20diverse%20seafloor%20image%0Adatasets%2C%20analysing%20the%20impact%20of%20different%20representative%20sampling%20strategies%0Aon%20both%20clustering%20accuracy%20and%20computational%20cost.%20The%20OCF%20achieves%20the%0Ahighest%20average%20F1%20score%20of%200.68%20across%20the%20three%20datasets%20among%20all%0Acomparative%20online%20clustering%20approaches%2C%20with%20a%20standard%20deviation%20of%203%25%0Aacross%20three%20distinct%20survey%20trajectories%2C%20demonstrating%20its%20superior%0Aclustering%20capability%20and%20robustness%20to%20trajectory%20variation.%20In%20addition%2C%20it%0Amaintains%20consistently%20lower%20and%20bounded%20computational%20time%20as%20the%20data%20volume%0Aincreases.%20These%20properties%20are%20beneficial%20for%20generating%20survey%20data%20summaries%0Aand%20supporting%20informative%20path%20planning%20in%20long-term%2C%20persistent%20autonomous%0Amarine%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06678v1&entry.124074799=Read"},
{"title": "CHIRLA: Comprehensive High-resolution Identification and\n  Re-identification for Large-scale Analysis", "author": "Bessie Dominguez-Dager and Felix Escalona and Francisco Gomez-Donoso and Miguel Cazorla", "abstract": "  Person re-identification (Re-ID) is a key challenge in computer vision,\nrequiring the matching of individuals across cameras, locations, and time.\nWhile most research focuses on short-term scenarios with minimal appearance\nchanges, real-world applications demand robust systems that handle long-term\nvariations caused by clothing and physical changes. We present CHIRLA,\nComprehensive High-resolution Identification and Re-identification for\nLarge-scale Analysis, a novel dataset designed for video-based long-term person\nRe-ID. CHIRLA was recorded over seven months in four connected indoor\nenvironments using seven strategically placed cameras, capturing realistic\nmovements with substantial clothing and appearance variability. The dataset\nincludes 22 individuals, more than five hours of video, and about 1M bounding\nboxes with identity annotations obtained through semi-automatic labeling. We\nalso define benchmark protocols for person tracking and Re-ID, covering diverse\nand challenging scenarios such as occlusion, reappearance, and multi-camera\nconditions. By introducing this comprehensive benchmark, we aim to facilitate\nthe development and evaluation of Re-ID algorithms that can reliably perform in\nchallenging, long-term real-world scenarios. The benchmark code is publicly\navailable at: https://github.com/bdager/CHIRLA.\n", "link": "http://arxiv.org/abs/2502.06681v2", "date": "2025-09-08", "relevancy": 2.5274, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5052}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHIRLA%3A%20Comprehensive%20High-resolution%20Identification%20and%0A%20%20Re-identification%20for%20Large-scale%20Analysis&body=Title%3A%20CHIRLA%3A%20Comprehensive%20High-resolution%20Identification%20and%0A%20%20Re-identification%20for%20Large-scale%20Analysis%0AAuthor%3A%20Bessie%20Dominguez-Dager%20and%20Felix%20Escalona%20and%20Francisco%20Gomez-Donoso%20and%20Miguel%20Cazorla%0AAbstract%3A%20%20%20Person%20re-identification%20%28Re-ID%29%20is%20a%20key%20challenge%20in%20computer%20vision%2C%0Arequiring%20the%20matching%20of%20individuals%20across%20cameras%2C%20locations%2C%20and%20time.%0AWhile%20most%20research%20focuses%20on%20short-term%20scenarios%20with%20minimal%20appearance%0Achanges%2C%20real-world%20applications%20demand%20robust%20systems%20that%20handle%20long-term%0Avariations%20caused%20by%20clothing%20and%20physical%20changes.%20We%20present%20CHIRLA%2C%0AComprehensive%20High-resolution%20Identification%20and%20Re-identification%20for%0ALarge-scale%20Analysis%2C%20a%20novel%20dataset%20designed%20for%20video-based%20long-term%20person%0ARe-ID.%20CHIRLA%20was%20recorded%20over%20seven%20months%20in%20four%20connected%20indoor%0Aenvironments%20using%20seven%20strategically%20placed%20cameras%2C%20capturing%20realistic%0Amovements%20with%20substantial%20clothing%20and%20appearance%20variability.%20The%20dataset%0Aincludes%2022%20individuals%2C%20more%20than%20five%20hours%20of%20video%2C%20and%20about%201M%20bounding%0Aboxes%20with%20identity%20annotations%20obtained%20through%20semi-automatic%20labeling.%20We%0Aalso%20define%20benchmark%20protocols%20for%20person%20tracking%20and%20Re-ID%2C%20covering%20diverse%0Aand%20challenging%20scenarios%20such%20as%20occlusion%2C%20reappearance%2C%20and%20multi-camera%0Aconditions.%20By%20introducing%20this%20comprehensive%20benchmark%2C%20we%20aim%20to%20facilitate%0Athe%20development%20and%20evaluation%20of%20Re-ID%20algorithms%20that%20can%20reliably%20perform%20in%0Achallenging%2C%20long-term%20real-world%20scenarios.%20The%20benchmark%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/bdager/CHIRLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHIRLA%253A%2520Comprehensive%2520High-resolution%2520Identification%2520and%250A%2520%2520Re-identification%2520for%2520Large-scale%2520Analysis%26entry.906535625%3DBessie%2520Dominguez-Dager%2520and%2520Felix%2520Escalona%2520and%2520Francisco%2520Gomez-Donoso%2520and%2520Miguel%2520Cazorla%26entry.1292438233%3D%2520%2520Person%2520re-identification%2520%2528Re-ID%2529%2520is%2520a%2520key%2520challenge%2520in%2520computer%2520vision%252C%250Arequiring%2520the%2520matching%2520of%2520individuals%2520across%2520cameras%252C%2520locations%252C%2520and%2520time.%250AWhile%2520most%2520research%2520focuses%2520on%2520short-term%2520scenarios%2520with%2520minimal%2520appearance%250Achanges%252C%2520real-world%2520applications%2520demand%2520robust%2520systems%2520that%2520handle%2520long-term%250Avariations%2520caused%2520by%2520clothing%2520and%2520physical%2520changes.%2520We%2520present%2520CHIRLA%252C%250AComprehensive%2520High-resolution%2520Identification%2520and%2520Re-identification%2520for%250ALarge-scale%2520Analysis%252C%2520a%2520novel%2520dataset%2520designed%2520for%2520video-based%2520long-term%2520person%250ARe-ID.%2520CHIRLA%2520was%2520recorded%2520over%2520seven%2520months%2520in%2520four%2520connected%2520indoor%250Aenvironments%2520using%2520seven%2520strategically%2520placed%2520cameras%252C%2520capturing%2520realistic%250Amovements%2520with%2520substantial%2520clothing%2520and%2520appearance%2520variability.%2520The%2520dataset%250Aincludes%252022%2520individuals%252C%2520more%2520than%2520five%2520hours%2520of%2520video%252C%2520and%2520about%25201M%2520bounding%250Aboxes%2520with%2520identity%2520annotations%2520obtained%2520through%2520semi-automatic%2520labeling.%2520We%250Aalso%2520define%2520benchmark%2520protocols%2520for%2520person%2520tracking%2520and%2520Re-ID%252C%2520covering%2520diverse%250Aand%2520challenging%2520scenarios%2520such%2520as%2520occlusion%252C%2520reappearance%252C%2520and%2520multi-camera%250Aconditions.%2520By%2520introducing%2520this%2520comprehensive%2520benchmark%252C%2520we%2520aim%2520to%2520facilitate%250Athe%2520development%2520and%2520evaluation%2520of%2520Re-ID%2520algorithms%2520that%2520can%2520reliably%2520perform%2520in%250Achallenging%252C%2520long-term%2520real-world%2520scenarios.%2520The%2520benchmark%2520code%2520is%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/bdager/CHIRLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHIRLA%3A%20Comprehensive%20High-resolution%20Identification%20and%0A%20%20Re-identification%20for%20Large-scale%20Analysis&entry.906535625=Bessie%20Dominguez-Dager%20and%20Felix%20Escalona%20and%20Francisco%20Gomez-Donoso%20and%20Miguel%20Cazorla&entry.1292438233=%20%20Person%20re-identification%20%28Re-ID%29%20is%20a%20key%20challenge%20in%20computer%20vision%2C%0Arequiring%20the%20matching%20of%20individuals%20across%20cameras%2C%20locations%2C%20and%20time.%0AWhile%20most%20research%20focuses%20on%20short-term%20scenarios%20with%20minimal%20appearance%0Achanges%2C%20real-world%20applications%20demand%20robust%20systems%20that%20handle%20long-term%0Avariations%20caused%20by%20clothing%20and%20physical%20changes.%20We%20present%20CHIRLA%2C%0AComprehensive%20High-resolution%20Identification%20and%20Re-identification%20for%0ALarge-scale%20Analysis%2C%20a%20novel%20dataset%20designed%20for%20video-based%20long-term%20person%0ARe-ID.%20CHIRLA%20was%20recorded%20over%20seven%20months%20in%20four%20connected%20indoor%0Aenvironments%20using%20seven%20strategically%20placed%20cameras%2C%20capturing%20realistic%0Amovements%20with%20substantial%20clothing%20and%20appearance%20variability.%20The%20dataset%0Aincludes%2022%20individuals%2C%20more%20than%20five%20hours%20of%20video%2C%20and%20about%201M%20bounding%0Aboxes%20with%20identity%20annotations%20obtained%20through%20semi-automatic%20labeling.%20We%0Aalso%20define%20benchmark%20protocols%20for%20person%20tracking%20and%20Re-ID%2C%20covering%20diverse%0Aand%20challenging%20scenarios%20such%20as%20occlusion%2C%20reappearance%2C%20and%20multi-camera%0Aconditions.%20By%20introducing%20this%20comprehensive%20benchmark%2C%20we%20aim%20to%20facilitate%0Athe%20development%20and%20evaluation%20of%20Re-ID%20algorithms%20that%20can%20reliably%20perform%20in%0Achallenging%2C%20long-term%20real-world%20scenarios.%20The%20benchmark%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/bdager/CHIRLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06681v2&entry.124074799=Read"},
{"title": "Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and\n  Nuclei Segmentation", "author": "Qing Xu and Wenting Duan and Zhen Chen", "abstract": "  Histopathology image analysis is critical yet challenged by the demand of\nsegmenting tissue regions and nuclei instances for tumor microenvironment and\ncellular morphology analysis. Existing studies focused on tissue semantic\nsegmentation or nuclei instance segmentation separately, but ignored the\ninherent relationship between these two tasks, resulting in insufficient\nhistopathology understanding. To address this issue, we propose a Co-Seg\nframework for collaborative tissue and nuclei segmentation. Specifically, we\nintroduce a novel co-segmentation paradigm, allowing tissue and nuclei\nsegmentation tasks to mutually enhance each other. To this end, we first devise\na region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and\ninstance region prompts as prior constraints. Moreover, we design a mutual\nprompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen\nthe contextual consistency of both tasks, collaboratively computing semantic\nand instance segmentation masks. Extensive experiments on the PUMA dataset\ndemonstrate that the proposed Co-Seg surpasses state-of-the-arts in the\nsemantic, instance and panoptic segmentation of tumor tissues and nuclei\ninstances. The source code is available at https://github.com/xq141839/Co-Seg.\n", "link": "http://arxiv.org/abs/2509.06740v1", "date": "2025-09-08", "relevancy": 2.5178, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Seg%3A%20Mutual%20Prompt-Guided%20Collaborative%20Learning%20for%20Tissue%20and%0A%20%20Nuclei%20Segmentation&body=Title%3A%20Co-Seg%3A%20Mutual%20Prompt-Guided%20Collaborative%20Learning%20for%20Tissue%20and%0A%20%20Nuclei%20Segmentation%0AAuthor%3A%20Qing%20Xu%20and%20Wenting%20Duan%20and%20Zhen%20Chen%0AAbstract%3A%20%20%20Histopathology%20image%20analysis%20is%20critical%20yet%20challenged%20by%20the%20demand%20of%0Asegmenting%20tissue%20regions%20and%20nuclei%20instances%20for%20tumor%20microenvironment%20and%0Acellular%20morphology%20analysis.%20Existing%20studies%20focused%20on%20tissue%20semantic%0Asegmentation%20or%20nuclei%20instance%20segmentation%20separately%2C%20but%20ignored%20the%0Ainherent%20relationship%20between%20these%20two%20tasks%2C%20resulting%20in%20insufficient%0Ahistopathology%20understanding.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Co-Seg%0Aframework%20for%20collaborative%20tissue%20and%20nuclei%20segmentation.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20co-segmentation%20paradigm%2C%20allowing%20tissue%20and%20nuclei%0Asegmentation%20tasks%20to%20mutually%20enhance%20each%20other.%20To%20this%20end%2C%20we%20first%20devise%0Aa%20region-aware%20prompt%20encoder%20%28RP-Encoder%29%20to%20provide%20high-quality%20semantic%20and%0Ainstance%20region%20prompts%20as%20prior%20constraints.%20Moreover%2C%20we%20design%20a%20mutual%0Aprompt%20mask%20decoder%20%28MP-Decoder%29%20that%20leverages%20cross-guidance%20to%20strengthen%0Athe%20contextual%20consistency%20of%20both%20tasks%2C%20collaboratively%20computing%20semantic%0Aand%20instance%20segmentation%20masks.%20Extensive%20experiments%20on%20the%20PUMA%20dataset%0Ademonstrate%20that%20the%20proposed%20Co-Seg%20surpasses%20state-of-the-arts%20in%20the%0Asemantic%2C%20instance%20and%20panoptic%20segmentation%20of%20tumor%20tissues%20and%20nuclei%0Ainstances.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xq141839/Co-Seg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Seg%253A%2520Mutual%2520Prompt-Guided%2520Collaborative%2520Learning%2520for%2520Tissue%2520and%250A%2520%2520Nuclei%2520Segmentation%26entry.906535625%3DQing%2520Xu%2520and%2520Wenting%2520Duan%2520and%2520Zhen%2520Chen%26entry.1292438233%3D%2520%2520Histopathology%2520image%2520analysis%2520is%2520critical%2520yet%2520challenged%2520by%2520the%2520demand%2520of%250Asegmenting%2520tissue%2520regions%2520and%2520nuclei%2520instances%2520for%2520tumor%2520microenvironment%2520and%250Acellular%2520morphology%2520analysis.%2520Existing%2520studies%2520focused%2520on%2520tissue%2520semantic%250Asegmentation%2520or%2520nuclei%2520instance%2520segmentation%2520separately%252C%2520but%2520ignored%2520the%250Ainherent%2520relationship%2520between%2520these%2520two%2520tasks%252C%2520resulting%2520in%2520insufficient%250Ahistopathology%2520understanding.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Co-Seg%250Aframework%2520for%2520collaborative%2520tissue%2520and%2520nuclei%2520segmentation.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520novel%2520co-segmentation%2520paradigm%252C%2520allowing%2520tissue%2520and%2520nuclei%250Asegmentation%2520tasks%2520to%2520mutually%2520enhance%2520each%2520other.%2520To%2520this%2520end%252C%2520we%2520first%2520devise%250Aa%2520region-aware%2520prompt%2520encoder%2520%2528RP-Encoder%2529%2520to%2520provide%2520high-quality%2520semantic%2520and%250Ainstance%2520region%2520prompts%2520as%2520prior%2520constraints.%2520Moreover%252C%2520we%2520design%2520a%2520mutual%250Aprompt%2520mask%2520decoder%2520%2528MP-Decoder%2529%2520that%2520leverages%2520cross-guidance%2520to%2520strengthen%250Athe%2520contextual%2520consistency%2520of%2520both%2520tasks%252C%2520collaboratively%2520computing%2520semantic%250Aand%2520instance%2520segmentation%2520masks.%2520Extensive%2520experiments%2520on%2520the%2520PUMA%2520dataset%250Ademonstrate%2520that%2520the%2520proposed%2520Co-Seg%2520surpasses%2520state-of-the-arts%2520in%2520the%250Asemantic%252C%2520instance%2520and%2520panoptic%2520segmentation%2520of%2520tumor%2520tissues%2520and%2520nuclei%250Ainstances.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/xq141839/Co-Seg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Seg%3A%20Mutual%20Prompt-Guided%20Collaborative%20Learning%20for%20Tissue%20and%0A%20%20Nuclei%20Segmentation&entry.906535625=Qing%20Xu%20and%20Wenting%20Duan%20and%20Zhen%20Chen&entry.1292438233=%20%20Histopathology%20image%20analysis%20is%20critical%20yet%20challenged%20by%20the%20demand%20of%0Asegmenting%20tissue%20regions%20and%20nuclei%20instances%20for%20tumor%20microenvironment%20and%0Acellular%20morphology%20analysis.%20Existing%20studies%20focused%20on%20tissue%20semantic%0Asegmentation%20or%20nuclei%20instance%20segmentation%20separately%2C%20but%20ignored%20the%0Ainherent%20relationship%20between%20these%20two%20tasks%2C%20resulting%20in%20insufficient%0Ahistopathology%20understanding.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Co-Seg%0Aframework%20for%20collaborative%20tissue%20and%20nuclei%20segmentation.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20co-segmentation%20paradigm%2C%20allowing%20tissue%20and%20nuclei%0Asegmentation%20tasks%20to%20mutually%20enhance%20each%20other.%20To%20this%20end%2C%20we%20first%20devise%0Aa%20region-aware%20prompt%20encoder%20%28RP-Encoder%29%20to%20provide%20high-quality%20semantic%20and%0Ainstance%20region%20prompts%20as%20prior%20constraints.%20Moreover%2C%20we%20design%20a%20mutual%0Aprompt%20mask%20decoder%20%28MP-Decoder%29%20that%20leverages%20cross-guidance%20to%20strengthen%0Athe%20contextual%20consistency%20of%20both%20tasks%2C%20collaboratively%20computing%20semantic%0Aand%20instance%20segmentation%20masks.%20Extensive%20experiments%20on%20the%20PUMA%20dataset%0Ademonstrate%20that%20the%20proposed%20Co-Seg%20surpasses%20state-of-the-arts%20in%20the%0Asemantic%2C%20instance%20and%20panoptic%20segmentation%20of%20tumor%20tissues%20and%20nuclei%0Ainstances.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xq141839/Co-Seg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06740v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios", "author": "Luana Bulla and Gabriele Tuccio and Misael Mongiov\u00ec and Aldo Gangemi", "abstract": "  Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities.\n", "link": "http://arxiv.org/abs/2508.18183v2", "date": "2025-09-08", "relevancy": 2.4997, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Accurate%20Sign%20Language%20Translation%0A%20%20in%20Low-Resource%20Scenarios&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Accurate%20Sign%20Language%20Translation%0A%20%20in%20Low-Resource%20Scenarios%0AAuthor%3A%20Luana%20Bulla%20and%20Gabriele%20Tuccio%20and%20Misael%20Mongiov%C3%AC%20and%20Aldo%20Gangemi%0AAbstract%3A%20%20%20Translating%20natural%20languages%20into%20sign%20languages%20is%20a%20highly%20complex%20and%0Aunderexplored%20task.%20Despite%20growing%20interest%20in%20accessibility%20and%20inclusivity%2C%0Athe%20development%20of%20robust%20translation%20systems%20remains%20hindered%20by%20the%20limited%0Aavailability%20of%20parallel%20corpora%20which%20align%20natural%20language%20with%20sign%0Alanguage%20data.%20Existing%20methods%20often%20struggle%20to%20generalize%20in%20these%0Adata-scarce%20environments%2C%20as%20the%20few%20datasets%20available%20are%20typically%0Adomain-specific%2C%20lack%20standardization%2C%20or%20fail%20to%20capture%20the%20full%20linguistic%0Arichness%20of%20sign%20languages.%20To%20address%20this%20limitation%2C%20we%20propose%20Advanced%20Use%0Aof%20LLMs%20for%20Sign%20Language%20Translation%20%28AulSign%29%2C%20a%20novel%20method%20that%20leverages%0ALarge%20Language%20Models%20via%20dynamic%20prompting%20and%20in-context%20learning%20with%20sample%0Aselection%20and%20subsequent%20sign%20association.%20Despite%20their%20impressive%20abilities%0Ain%20processing%20text%2C%20LLMs%20lack%20intrinsic%20knowledge%20of%20sign%20languages%3B%20therefore%2C%0Athey%20are%20unable%20to%20natively%20perform%20this%20kind%20of%20translation.%20To%20overcome%20this%0Alimitation%2C%20we%20associate%20the%20signs%20with%20compact%20descriptions%20in%20natural%0Alanguage%20and%20instruct%20the%20model%20to%20use%20them.%20We%20evaluate%20our%20method%20on%20both%0AEnglish%20and%20Italian%20languages%20using%20SignBank%2B%2C%20a%20recognized%20benchmark%20in%20the%0Afield%2C%20as%20well%20as%20the%20Italian%20LaCAM%20CNR-ISTC%20dataset.%20We%20demonstrate%20superior%0Aperformance%20compared%20to%20state-of-the-art%20models%20in%20low-data%20scenario.%20Our%0Afindings%20demonstrate%20the%20effectiveness%20of%20AulSign%2C%20with%20the%20potential%20to%0Aenhance%20accessibility%20and%20inclusivity%20in%20communication%20technologies%20for%0Aunderrepresented%20linguistic%20communities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Accurate%2520Sign%2520Language%2520Translation%250A%2520%2520in%2520Low-Resource%2520Scenarios%26entry.906535625%3DLuana%2520Bulla%2520and%2520Gabriele%2520Tuccio%2520and%2520Misael%2520Mongiov%25C3%25AC%2520and%2520Aldo%2520Gangemi%26entry.1292438233%3D%2520%2520Translating%2520natural%2520languages%2520into%2520sign%2520languages%2520is%2520a%2520highly%2520complex%2520and%250Aunderexplored%2520task.%2520Despite%2520growing%2520interest%2520in%2520accessibility%2520and%2520inclusivity%252C%250Athe%2520development%2520of%2520robust%2520translation%2520systems%2520remains%2520hindered%2520by%2520the%2520limited%250Aavailability%2520of%2520parallel%2520corpora%2520which%2520align%2520natural%2520language%2520with%2520sign%250Alanguage%2520data.%2520Existing%2520methods%2520often%2520struggle%2520to%2520generalize%2520in%2520these%250Adata-scarce%2520environments%252C%2520as%2520the%2520few%2520datasets%2520available%2520are%2520typically%250Adomain-specific%252C%2520lack%2520standardization%252C%2520or%2520fail%2520to%2520capture%2520the%2520full%2520linguistic%250Arichness%2520of%2520sign%2520languages.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Advanced%2520Use%250Aof%2520LLMs%2520for%2520Sign%2520Language%2520Translation%2520%2528AulSign%2529%252C%2520a%2520novel%2520method%2520that%2520leverages%250ALarge%2520Language%2520Models%2520via%2520dynamic%2520prompting%2520and%2520in-context%2520learning%2520with%2520sample%250Aselection%2520and%2520subsequent%2520sign%2520association.%2520Despite%2520their%2520impressive%2520abilities%250Ain%2520processing%2520text%252C%2520LLMs%2520lack%2520intrinsic%2520knowledge%2520of%2520sign%2520languages%253B%2520therefore%252C%250Athey%2520are%2520unable%2520to%2520natively%2520perform%2520this%2520kind%2520of%2520translation.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520associate%2520the%2520signs%2520with%2520compact%2520descriptions%2520in%2520natural%250Alanguage%2520and%2520instruct%2520the%2520model%2520to%2520use%2520them.%2520We%2520evaluate%2520our%2520method%2520on%2520both%250AEnglish%2520and%2520Italian%2520languages%2520using%2520SignBank%252B%252C%2520a%2520recognized%2520benchmark%2520in%2520the%250Afield%252C%2520as%2520well%2520as%2520the%2520Italian%2520LaCAM%2520CNR-ISTC%2520dataset.%2520We%2520demonstrate%2520superior%250Aperformance%2520compared%2520to%2520state-of-the-art%2520models%2520in%2520low-data%2520scenario.%2520Our%250Afindings%2520demonstrate%2520the%2520effectiveness%2520of%2520AulSign%252C%2520with%2520the%2520potential%2520to%250Aenhance%2520accessibility%2520and%2520inclusivity%2520in%2520communication%2520technologies%2520for%250Aunderrepresented%2520linguistic%2520communities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Accurate%20Sign%20Language%20Translation%0A%20%20in%20Low-Resource%20Scenarios&entry.906535625=Luana%20Bulla%20and%20Gabriele%20Tuccio%20and%20Misael%20Mongiov%C3%AC%20and%20Aldo%20Gangemi&entry.1292438233=%20%20Translating%20natural%20languages%20into%20sign%20languages%20is%20a%20highly%20complex%20and%0Aunderexplored%20task.%20Despite%20growing%20interest%20in%20accessibility%20and%20inclusivity%2C%0Athe%20development%20of%20robust%20translation%20systems%20remains%20hindered%20by%20the%20limited%0Aavailability%20of%20parallel%20corpora%20which%20align%20natural%20language%20with%20sign%0Alanguage%20data.%20Existing%20methods%20often%20struggle%20to%20generalize%20in%20these%0Adata-scarce%20environments%2C%20as%20the%20few%20datasets%20available%20are%20typically%0Adomain-specific%2C%20lack%20standardization%2C%20or%20fail%20to%20capture%20the%20full%20linguistic%0Arichness%20of%20sign%20languages.%20To%20address%20this%20limitation%2C%20we%20propose%20Advanced%20Use%0Aof%20LLMs%20for%20Sign%20Language%20Translation%20%28AulSign%29%2C%20a%20novel%20method%20that%20leverages%0ALarge%20Language%20Models%20via%20dynamic%20prompting%20and%20in-context%20learning%20with%20sample%0Aselection%20and%20subsequent%20sign%20association.%20Despite%20their%20impressive%20abilities%0Ain%20processing%20text%2C%20LLMs%20lack%20intrinsic%20knowledge%20of%20sign%20languages%3B%20therefore%2C%0Athey%20are%20unable%20to%20natively%20perform%20this%20kind%20of%20translation.%20To%20overcome%20this%0Alimitation%2C%20we%20associate%20the%20signs%20with%20compact%20descriptions%20in%20natural%0Alanguage%20and%20instruct%20the%20model%20to%20use%20them.%20We%20evaluate%20our%20method%20on%20both%0AEnglish%20and%20Italian%20languages%20using%20SignBank%2B%2C%20a%20recognized%20benchmark%20in%20the%0Afield%2C%20as%20well%20as%20the%20Italian%20LaCAM%20CNR-ISTC%20dataset.%20We%20demonstrate%20superior%0Aperformance%20compared%20to%20state-of-the-art%20models%20in%20low-data%20scenario.%20Our%0Afindings%20demonstrate%20the%20effectiveness%20of%20AulSign%2C%20with%20the%20potential%20to%0Aenhance%20accessibility%20and%20inclusivity%20in%20communication%20technologies%20for%0Aunderrepresented%20linguistic%20communities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18183v2&entry.124074799=Read"},
{"title": "Pothole Detection and Recognition based on Transfer Learning", "author": "Mang Hu and Qianqian Xia", "abstract": "  With the rapid development of computer vision and machine learning, automated\nmethods for pothole detection and recognition based on image and video data\nhave received significant attention. It is of great significance for social\ndevelopment to conduct an in-depth analysis of road images through feature\nextraction, thereby achieving automatic identification of the pothole condition\nin new images. Consequently, this is the main issue addressed in this study.\nBased on preprocessing techniques such as standardization, normalization, and\ndata augmentation applied to the collected raw dataset, we continuously\nimproved the network model based on experimental results. Ultimately, we\nconstructed a deep learning feature extraction network\nResNet50-EfficientNet-RegNet model based on transfer learning. This model\nexhibits high classification accuracy and computational efficiency. In terms of\nmodel evaluation, this study employed a comparative evaluation approach by\ncomparing the performance of the proposed transfer learning model with other\nmodels, including Random Forest, MLP, SVM, and LightGBM. The comparison\nanalysis was conducted based on metrics such as Accuracy, Recall, Precision,\nF1-score, and FPS, to assess the classification performance of the transfer\nlearning model proposed in this paper. The results demonstrate that our model\nexhibits high performance in terms of recognition speed and accuracy,\nsurpassing the performance of other models. Through careful parameter selection\nand model optimization, our transfer learning model achieved a classification\naccuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%\n(890/900) on the expanded test set.\n", "link": "http://arxiv.org/abs/2509.06750v1", "date": "2025-09-08", "relevancy": 2.4971, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5074}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4999}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pothole%20Detection%20and%20Recognition%20based%20on%20Transfer%20Learning&body=Title%3A%20Pothole%20Detection%20and%20Recognition%20based%20on%20Transfer%20Learning%0AAuthor%3A%20Mang%20Hu%20and%20Qianqian%20Xia%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20computer%20vision%20and%20machine%20learning%2C%20automated%0Amethods%20for%20pothole%20detection%20and%20recognition%20based%20on%20image%20and%20video%20data%0Ahave%20received%20significant%20attention.%20It%20is%20of%20great%20significance%20for%20social%0Adevelopment%20to%20conduct%20an%20in-depth%20analysis%20of%20road%20images%20through%20feature%0Aextraction%2C%20thereby%20achieving%20automatic%20identification%20of%20the%20pothole%20condition%0Ain%20new%20images.%20Consequently%2C%20this%20is%20the%20main%20issue%20addressed%20in%20this%20study.%0ABased%20on%20preprocessing%20techniques%20such%20as%20standardization%2C%20normalization%2C%20and%0Adata%20augmentation%20applied%20to%20the%20collected%20raw%20dataset%2C%20we%20continuously%0Aimproved%20the%20network%20model%20based%20on%20experimental%20results.%20Ultimately%2C%20we%0Aconstructed%20a%20deep%20learning%20feature%20extraction%20network%0AResNet50-EfficientNet-RegNet%20model%20based%20on%20transfer%20learning.%20This%20model%0Aexhibits%20high%20classification%20accuracy%20and%20computational%20efficiency.%20In%20terms%20of%0Amodel%20evaluation%2C%20this%20study%20employed%20a%20comparative%20evaluation%20approach%20by%0Acomparing%20the%20performance%20of%20the%20proposed%20transfer%20learning%20model%20with%20other%0Amodels%2C%20including%20Random%20Forest%2C%20MLP%2C%20SVM%2C%20and%20LightGBM.%20The%20comparison%0Aanalysis%20was%20conducted%20based%20on%20metrics%20such%20as%20Accuracy%2C%20Recall%2C%20Precision%2C%0AF1-score%2C%20and%20FPS%2C%20to%20assess%20the%20classification%20performance%20of%20the%20transfer%0Alearning%20model%20proposed%20in%20this%20paper.%20The%20results%20demonstrate%20that%20our%20model%0Aexhibits%20high%20performance%20in%20terms%20of%20recognition%20speed%20and%20accuracy%2C%0Asurpassing%20the%20performance%20of%20other%20models.%20Through%20careful%20parameter%20selection%0Aand%20model%20optimization%2C%20our%20transfer%20learning%20model%20achieved%20a%20classification%0Aaccuracy%20of%2097.78%25%20%2888/90%29%20on%20the%20initial%20set%20of%2090%20test%20samples%20and%2098.89%25%0A%28890/900%29%20on%20the%20expanded%20test%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPothole%2520Detection%2520and%2520Recognition%2520based%2520on%2520Transfer%2520Learning%26entry.906535625%3DMang%2520Hu%2520and%2520Qianqian%2520Xia%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520computer%2520vision%2520and%2520machine%2520learning%252C%2520automated%250Amethods%2520for%2520pothole%2520detection%2520and%2520recognition%2520based%2520on%2520image%2520and%2520video%2520data%250Ahave%2520received%2520significant%2520attention.%2520It%2520is%2520of%2520great%2520significance%2520for%2520social%250Adevelopment%2520to%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520road%2520images%2520through%2520feature%250Aextraction%252C%2520thereby%2520achieving%2520automatic%2520identification%2520of%2520the%2520pothole%2520condition%250Ain%2520new%2520images.%2520Consequently%252C%2520this%2520is%2520the%2520main%2520issue%2520addressed%2520in%2520this%2520study.%250ABased%2520on%2520preprocessing%2520techniques%2520such%2520as%2520standardization%252C%2520normalization%252C%2520and%250Adata%2520augmentation%2520applied%2520to%2520the%2520collected%2520raw%2520dataset%252C%2520we%2520continuously%250Aimproved%2520the%2520network%2520model%2520based%2520on%2520experimental%2520results.%2520Ultimately%252C%2520we%250Aconstructed%2520a%2520deep%2520learning%2520feature%2520extraction%2520network%250AResNet50-EfficientNet-RegNet%2520model%2520based%2520on%2520transfer%2520learning.%2520This%2520model%250Aexhibits%2520high%2520classification%2520accuracy%2520and%2520computational%2520efficiency.%2520In%2520terms%2520of%250Amodel%2520evaluation%252C%2520this%2520study%2520employed%2520a%2520comparative%2520evaluation%2520approach%2520by%250Acomparing%2520the%2520performance%2520of%2520the%2520proposed%2520transfer%2520learning%2520model%2520with%2520other%250Amodels%252C%2520including%2520Random%2520Forest%252C%2520MLP%252C%2520SVM%252C%2520and%2520LightGBM.%2520The%2520comparison%250Aanalysis%2520was%2520conducted%2520based%2520on%2520metrics%2520such%2520as%2520Accuracy%252C%2520Recall%252C%2520Precision%252C%250AF1-score%252C%2520and%2520FPS%252C%2520to%2520assess%2520the%2520classification%2520performance%2520of%2520the%2520transfer%250Alearning%2520model%2520proposed%2520in%2520this%2520paper.%2520The%2520results%2520demonstrate%2520that%2520our%2520model%250Aexhibits%2520high%2520performance%2520in%2520terms%2520of%2520recognition%2520speed%2520and%2520accuracy%252C%250Asurpassing%2520the%2520performance%2520of%2520other%2520models.%2520Through%2520careful%2520parameter%2520selection%250Aand%2520model%2520optimization%252C%2520our%2520transfer%2520learning%2520model%2520achieved%2520a%2520classification%250Aaccuracy%2520of%252097.78%2525%2520%252888/90%2529%2520on%2520the%2520initial%2520set%2520of%252090%2520test%2520samples%2520and%252098.89%2525%250A%2528890/900%2529%2520on%2520the%2520expanded%2520test%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pothole%20Detection%20and%20Recognition%20based%20on%20Transfer%20Learning&entry.906535625=Mang%20Hu%20and%20Qianqian%20Xia&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20computer%20vision%20and%20machine%20learning%2C%20automated%0Amethods%20for%20pothole%20detection%20and%20recognition%20based%20on%20image%20and%20video%20data%0Ahave%20received%20significant%20attention.%20It%20is%20of%20great%20significance%20for%20social%0Adevelopment%20to%20conduct%20an%20in-depth%20analysis%20of%20road%20images%20through%20feature%0Aextraction%2C%20thereby%20achieving%20automatic%20identification%20of%20the%20pothole%20condition%0Ain%20new%20images.%20Consequently%2C%20this%20is%20the%20main%20issue%20addressed%20in%20this%20study.%0ABased%20on%20preprocessing%20techniques%20such%20as%20standardization%2C%20normalization%2C%20and%0Adata%20augmentation%20applied%20to%20the%20collected%20raw%20dataset%2C%20we%20continuously%0Aimproved%20the%20network%20model%20based%20on%20experimental%20results.%20Ultimately%2C%20we%0Aconstructed%20a%20deep%20learning%20feature%20extraction%20network%0AResNet50-EfficientNet-RegNet%20model%20based%20on%20transfer%20learning.%20This%20model%0Aexhibits%20high%20classification%20accuracy%20and%20computational%20efficiency.%20In%20terms%20of%0Amodel%20evaluation%2C%20this%20study%20employed%20a%20comparative%20evaluation%20approach%20by%0Acomparing%20the%20performance%20of%20the%20proposed%20transfer%20learning%20model%20with%20other%0Amodels%2C%20including%20Random%20Forest%2C%20MLP%2C%20SVM%2C%20and%20LightGBM.%20The%20comparison%0Aanalysis%20was%20conducted%20based%20on%20metrics%20such%20as%20Accuracy%2C%20Recall%2C%20Precision%2C%0AF1-score%2C%20and%20FPS%2C%20to%20assess%20the%20classification%20performance%20of%20the%20transfer%0Alearning%20model%20proposed%20in%20this%20paper.%20The%20results%20demonstrate%20that%20our%20model%0Aexhibits%20high%20performance%20in%20terms%20of%20recognition%20speed%20and%20accuracy%2C%0Asurpassing%20the%20performance%20of%20other%20models.%20Through%20careful%20parameter%20selection%0Aand%20model%20optimization%2C%20our%20transfer%20learning%20model%20achieved%20a%20classification%0Aaccuracy%20of%2097.78%25%20%2888/90%29%20on%20the%20initial%20set%20of%2090%20test%20samples%20and%2098.89%25%0A%28890/900%29%20on%20the%20expanded%20test%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06750v1&entry.124074799=Read"},
{"title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "author": "Marc Marone and Orion Weller and William Fleshman and Eugene Yang and Dawn Lawrie and Benjamin Van Durme", "abstract": "  Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages.\n", "link": "http://arxiv.org/abs/2509.06888v1", "date": "2025-09-08", "relevancy": 2.4926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mmBERT%3A%20A%20Modern%20Multilingual%20Encoder%20with%20Annealed%20Language%20Learning&body=Title%3A%20mmBERT%3A%20A%20Modern%20Multilingual%20Encoder%20with%20Annealed%20Language%20Learning%0AAuthor%3A%20Marc%20Marone%20and%20Orion%20Weller%20and%20William%20Fleshman%20and%20Eugene%20Yang%20and%20Dawn%20Lawrie%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20Encoder-only%20languages%20models%20are%20frequently%20used%20for%20a%20variety%20of%20standard%0Amachine%20learning%20tasks%2C%20including%20classification%20and%20retrieval.%20However%2C%20there%0Ahas%20been%20a%20lack%20of%20recent%20research%20for%20encoder%20models%2C%20especially%20with%20respect%0Ato%20multilingual%20models.%20We%20introduce%20mmBERT%2C%20an%20encoder-only%20language%20model%0Apretrained%20on%203T%20tokens%20of%20multilingual%20text%20in%20over%201800%20languages.%20To%20build%0AmmBERT%20we%20introduce%20several%20novel%20elements%2C%20including%20an%20inverse%20mask%20ratio%0Aschedule%20and%20an%20inverse%20temperature%20sampling%20ratio.%20We%20add%20over%201700%0Alow-resource%20languages%20to%20the%20data%20mix%20only%20during%20the%20decay%20phase%2C%20showing%0Athat%20it%20boosts%20performance%20dramatically%20and%20maximizes%20the%20gains%20from%20the%0Arelatively%20small%20amount%20of%20training%20data.%20Despite%20only%20including%20these%0Alow-resource%20languages%20in%20the%20short%20decay%20phase%20we%20achieve%20similar%0Aclassification%20performance%20to%20models%20like%20OpenAI%27s%20o3%20and%20Google%27s%20Gemini%202.5%0APro.%20Overall%2C%20we%20show%20that%20mmBERT%20significantly%20outperforms%20the%20previous%0Ageneration%20of%20models%20on%20classification%20and%20retrieval%20tasks%20--%20on%20both%20high%20and%0Alow-resource%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmmBERT%253A%2520A%2520Modern%2520Multilingual%2520Encoder%2520with%2520Annealed%2520Language%2520Learning%26entry.906535625%3DMarc%2520Marone%2520and%2520Orion%2520Weller%2520and%2520William%2520Fleshman%2520and%2520Eugene%2520Yang%2520and%2520Dawn%2520Lawrie%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520Encoder-only%2520languages%2520models%2520are%2520frequently%2520used%2520for%2520a%2520variety%2520of%2520standard%250Amachine%2520learning%2520tasks%252C%2520including%2520classification%2520and%2520retrieval.%2520However%252C%2520there%250Ahas%2520been%2520a%2520lack%2520of%2520recent%2520research%2520for%2520encoder%2520models%252C%2520especially%2520with%2520respect%250Ato%2520multilingual%2520models.%2520We%2520introduce%2520mmBERT%252C%2520an%2520encoder-only%2520language%2520model%250Apretrained%2520on%25203T%2520tokens%2520of%2520multilingual%2520text%2520in%2520over%25201800%2520languages.%2520To%2520build%250AmmBERT%2520we%2520introduce%2520several%2520novel%2520elements%252C%2520including%2520an%2520inverse%2520mask%2520ratio%250Aschedule%2520and%2520an%2520inverse%2520temperature%2520sampling%2520ratio.%2520We%2520add%2520over%25201700%250Alow-resource%2520languages%2520to%2520the%2520data%2520mix%2520only%2520during%2520the%2520decay%2520phase%252C%2520showing%250Athat%2520it%2520boosts%2520performance%2520dramatically%2520and%2520maximizes%2520the%2520gains%2520from%2520the%250Arelatively%2520small%2520amount%2520of%2520training%2520data.%2520Despite%2520only%2520including%2520these%250Alow-resource%2520languages%2520in%2520the%2520short%2520decay%2520phase%2520we%2520achieve%2520similar%250Aclassification%2520performance%2520to%2520models%2520like%2520OpenAI%2527s%2520o3%2520and%2520Google%2527s%2520Gemini%25202.5%250APro.%2520Overall%252C%2520we%2520show%2520that%2520mmBERT%2520significantly%2520outperforms%2520the%2520previous%250Ageneration%2520of%2520models%2520on%2520classification%2520and%2520retrieval%2520tasks%2520--%2520on%2520both%2520high%2520and%250Alow-resource%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mmBERT%3A%20A%20Modern%20Multilingual%20Encoder%20with%20Annealed%20Language%20Learning&entry.906535625=Marc%20Marone%20and%20Orion%20Weller%20and%20William%20Fleshman%20and%20Eugene%20Yang%20and%20Dawn%20Lawrie%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20Encoder-only%20languages%20models%20are%20frequently%20used%20for%20a%20variety%20of%20standard%0Amachine%20learning%20tasks%2C%20including%20classification%20and%20retrieval.%20However%2C%20there%0Ahas%20been%20a%20lack%20of%20recent%20research%20for%20encoder%20models%2C%20especially%20with%20respect%0Ato%20multilingual%20models.%20We%20introduce%20mmBERT%2C%20an%20encoder-only%20language%20model%0Apretrained%20on%203T%20tokens%20of%20multilingual%20text%20in%20over%201800%20languages.%20To%20build%0AmmBERT%20we%20introduce%20several%20novel%20elements%2C%20including%20an%20inverse%20mask%20ratio%0Aschedule%20and%20an%20inverse%20temperature%20sampling%20ratio.%20We%20add%20over%201700%0Alow-resource%20languages%20to%20the%20data%20mix%20only%20during%20the%20decay%20phase%2C%20showing%0Athat%20it%20boosts%20performance%20dramatically%20and%20maximizes%20the%20gains%20from%20the%0Arelatively%20small%20amount%20of%20training%20data.%20Despite%20only%20including%20these%0Alow-resource%20languages%20in%20the%20short%20decay%20phase%20we%20achieve%20similar%0Aclassification%20performance%20to%20models%20like%20OpenAI%27s%20o3%20and%20Google%27s%20Gemini%202.5%0APro.%20Overall%2C%20we%20show%20that%20mmBERT%20significantly%20outperforms%20the%20previous%0Ageneration%20of%20models%20on%20classification%20and%20retrieval%20tasks%20--%20on%20both%20high%20and%0Alow-resource%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06888v1&entry.124074799=Read"},
{"title": "PRO: Projection Domain Synthesis for CT Imaging", "author": "Kang Chen and Bin Huang and Xuebin Yang and Junyan Zhang and Yongbo Wang and Qiegen Liu", "abstract": "  Synthetic CT projection data is crucial for advancing imaging research, yet\nits generation remains challenging. Current image domain methods are limited as\nthey cannot simulate the physical acquisition process or utilize the complete\nstatistical information present in projection data, restricting their utility\nand fidelity. In this work, we present PRO, a projection domain synthesis\nfoundation model for CT imaging. To the best of our knowledge, this is the\nfirst study that performs CT synthesis in the projection domain. Unlike\nprevious approaches that operate in the image domain, PRO learns rich\nstructural representations from projection data and leverages anatomical text\nprompts for controllable synthesis. Projection data generation models can\nutilize complete measurement signals and simulate the physical processes of\nscanning, including material attenuation characteristics, beam hardening,\nscattering, and projection geometry, and support research on downstream imaging\ntasks. Moreover, PRO functions as a foundation model, capable of generalizing\nacross diverse downstream tasks by adjusting its generative behavior via prompt\ninputs. Experimental results demonstrated that incorporating our synthesized\ndata significantly improves performance across multiple downstream tasks,\nincluding low-dose and sparse-view reconstruction. These findings underscore\nthe versatility and scalability of PRO in data generation for various CT\napplications. These results highlight the potential of projection domain\nsynthesis as a powerful tool for data augmentation and robust CT imaging. Our\nsource code is publicly available at: https://github.com/yqx7150/PRO.\n", "link": "http://arxiv.org/abs/2506.13443v3", "date": "2025-09-08", "relevancy": 2.4481, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6206}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6206}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRO%3A%20Projection%20Domain%20Synthesis%20for%20CT%20Imaging&body=Title%3A%20PRO%3A%20Projection%20Domain%20Synthesis%20for%20CT%20Imaging%0AAuthor%3A%20Kang%20Chen%20and%20Bin%20Huang%20and%20Xuebin%20Yang%20and%20Junyan%20Zhang%20and%20Yongbo%20Wang%20and%20Qiegen%20Liu%0AAbstract%3A%20%20%20Synthetic%20CT%20projection%20data%20is%20crucial%20for%20advancing%20imaging%20research%2C%20yet%0Aits%20generation%20remains%20challenging.%20Current%20image%20domain%20methods%20are%20limited%20as%0Athey%20cannot%20simulate%20the%20physical%20acquisition%20process%20or%20utilize%20the%20complete%0Astatistical%20information%20present%20in%20projection%20data%2C%20restricting%20their%20utility%0Aand%20fidelity.%20In%20this%20work%2C%20we%20present%20PRO%2C%20a%20projection%20domain%20synthesis%0Afoundation%20model%20for%20CT%20imaging.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20study%20that%20performs%20CT%20synthesis%20in%20the%20projection%20domain.%20Unlike%0Aprevious%20approaches%20that%20operate%20in%20the%20image%20domain%2C%20PRO%20learns%20rich%0Astructural%20representations%20from%20projection%20data%20and%20leverages%20anatomical%20text%0Aprompts%20for%20controllable%20synthesis.%20Projection%20data%20generation%20models%20can%0Autilize%20complete%20measurement%20signals%20and%20simulate%20the%20physical%20processes%20of%0Ascanning%2C%20including%20material%20attenuation%20characteristics%2C%20beam%20hardening%2C%0Ascattering%2C%20and%20projection%20geometry%2C%20and%20support%20research%20on%20downstream%20imaging%0Atasks.%20Moreover%2C%20PRO%20functions%20as%20a%20foundation%20model%2C%20capable%20of%20generalizing%0Aacross%20diverse%20downstream%20tasks%20by%20adjusting%20its%20generative%20behavior%20via%20prompt%0Ainputs.%20Experimental%20results%20demonstrated%20that%20incorporating%20our%20synthesized%0Adata%20significantly%20improves%20performance%20across%20multiple%20downstream%20tasks%2C%0Aincluding%20low-dose%20and%20sparse-view%20reconstruction.%20These%20findings%20underscore%0Athe%20versatility%20and%20scalability%20of%20PRO%20in%20data%20generation%20for%20various%20CT%0Aapplications.%20These%20results%20highlight%20the%20potential%20of%20projection%20domain%0Asynthesis%20as%20a%20powerful%20tool%20for%20data%20augmentation%20and%20robust%20CT%20imaging.%20Our%0Asource%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/yqx7150/PRO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13443v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRO%253A%2520Projection%2520Domain%2520Synthesis%2520for%2520CT%2520Imaging%26entry.906535625%3DKang%2520Chen%2520and%2520Bin%2520Huang%2520and%2520Xuebin%2520Yang%2520and%2520Junyan%2520Zhang%2520and%2520Yongbo%2520Wang%2520and%2520Qiegen%2520Liu%26entry.1292438233%3D%2520%2520Synthetic%2520CT%2520projection%2520data%2520is%2520crucial%2520for%2520advancing%2520imaging%2520research%252C%2520yet%250Aits%2520generation%2520remains%2520challenging.%2520Current%2520image%2520domain%2520methods%2520are%2520limited%2520as%250Athey%2520cannot%2520simulate%2520the%2520physical%2520acquisition%2520process%2520or%2520utilize%2520the%2520complete%250Astatistical%2520information%2520present%2520in%2520projection%2520data%252C%2520restricting%2520their%2520utility%250Aand%2520fidelity.%2520In%2520this%2520work%252C%2520we%2520present%2520PRO%252C%2520a%2520projection%2520domain%2520synthesis%250Afoundation%2520model%2520for%2520CT%2520imaging.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520study%2520that%2520performs%2520CT%2520synthesis%2520in%2520the%2520projection%2520domain.%2520Unlike%250Aprevious%2520approaches%2520that%2520operate%2520in%2520the%2520image%2520domain%252C%2520PRO%2520learns%2520rich%250Astructural%2520representations%2520from%2520projection%2520data%2520and%2520leverages%2520anatomical%2520text%250Aprompts%2520for%2520controllable%2520synthesis.%2520Projection%2520data%2520generation%2520models%2520can%250Autilize%2520complete%2520measurement%2520signals%2520and%2520simulate%2520the%2520physical%2520processes%2520of%250Ascanning%252C%2520including%2520material%2520attenuation%2520characteristics%252C%2520beam%2520hardening%252C%250Ascattering%252C%2520and%2520projection%2520geometry%252C%2520and%2520support%2520research%2520on%2520downstream%2520imaging%250Atasks.%2520Moreover%252C%2520PRO%2520functions%2520as%2520a%2520foundation%2520model%252C%2520capable%2520of%2520generalizing%250Aacross%2520diverse%2520downstream%2520tasks%2520by%2520adjusting%2520its%2520generative%2520behavior%2520via%2520prompt%250Ainputs.%2520Experimental%2520results%2520demonstrated%2520that%2520incorporating%2520our%2520synthesized%250Adata%2520significantly%2520improves%2520performance%2520across%2520multiple%2520downstream%2520tasks%252C%250Aincluding%2520low-dose%2520and%2520sparse-view%2520reconstruction.%2520These%2520findings%2520underscore%250Athe%2520versatility%2520and%2520scalability%2520of%2520PRO%2520in%2520data%2520generation%2520for%2520various%2520CT%250Aapplications.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520projection%2520domain%250Asynthesis%2520as%2520a%2520powerful%2520tool%2520for%2520data%2520augmentation%2520and%2520robust%2520CT%2520imaging.%2520Our%250Asource%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/yqx7150/PRO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13443v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRO%3A%20Projection%20Domain%20Synthesis%20for%20CT%20Imaging&entry.906535625=Kang%20Chen%20and%20Bin%20Huang%20and%20Xuebin%20Yang%20and%20Junyan%20Zhang%20and%20Yongbo%20Wang%20and%20Qiegen%20Liu&entry.1292438233=%20%20Synthetic%20CT%20projection%20data%20is%20crucial%20for%20advancing%20imaging%20research%2C%20yet%0Aits%20generation%20remains%20challenging.%20Current%20image%20domain%20methods%20are%20limited%20as%0Athey%20cannot%20simulate%20the%20physical%20acquisition%20process%20or%20utilize%20the%20complete%0Astatistical%20information%20present%20in%20projection%20data%2C%20restricting%20their%20utility%0Aand%20fidelity.%20In%20this%20work%2C%20we%20present%20PRO%2C%20a%20projection%20domain%20synthesis%0Afoundation%20model%20for%20CT%20imaging.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20study%20that%20performs%20CT%20synthesis%20in%20the%20projection%20domain.%20Unlike%0Aprevious%20approaches%20that%20operate%20in%20the%20image%20domain%2C%20PRO%20learns%20rich%0Astructural%20representations%20from%20projection%20data%20and%20leverages%20anatomical%20text%0Aprompts%20for%20controllable%20synthesis.%20Projection%20data%20generation%20models%20can%0Autilize%20complete%20measurement%20signals%20and%20simulate%20the%20physical%20processes%20of%0Ascanning%2C%20including%20material%20attenuation%20characteristics%2C%20beam%20hardening%2C%0Ascattering%2C%20and%20projection%20geometry%2C%20and%20support%20research%20on%20downstream%20imaging%0Atasks.%20Moreover%2C%20PRO%20functions%20as%20a%20foundation%20model%2C%20capable%20of%20generalizing%0Aacross%20diverse%20downstream%20tasks%20by%20adjusting%20its%20generative%20behavior%20via%20prompt%0Ainputs.%20Experimental%20results%20demonstrated%20that%20incorporating%20our%20synthesized%0Adata%20significantly%20improves%20performance%20across%20multiple%20downstream%20tasks%2C%0Aincluding%20low-dose%20and%20sparse-view%20reconstruction.%20These%20findings%20underscore%0Athe%20versatility%20and%20scalability%20of%20PRO%20in%20data%20generation%20for%20various%20CT%0Aapplications.%20These%20results%20highlight%20the%20potential%20of%20projection%20domain%0Asynthesis%20as%20a%20powerful%20tool%20for%20data%20augmentation%20and%20robust%20CT%20imaging.%20Our%0Asource%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/yqx7150/PRO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13443v3&entry.124074799=Read"},
{"title": "From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital\n  Humans", "author": "Marilyn Keller and Keenon Werling and Soyong Shin and Scott Delp and Sergi Pujades and C. Karen Liu and Michael J. Black", "abstract": "  Great progress has been made in estimating 3D human pose and shape from\nimages and video by training neural networks to directly regress the parameters\nof parametric human models like SMPL. However, existing body models have\nsimplified kinematic structures that do not correspond to the true joint\nlocations and articulations in the human skeletal system, limiting their\npotential use in biomechanics. On the other hand, methods for estimating\nbiomechanically accurate skeletal motion typically rely on complex motion\ncapture systems and expensive optimization methods. What is needed is a\nparametric 3D human model with a biomechanically accurate skeletal structure\nthat can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL\nbody model with a biomechanics skeleton. To enable this, we need training data\nof skeletons inside SMPL meshes in diverse poses.\n  We build such a dataset by optimizing biomechanically accurate skeletons\ninside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL\nmesh vertices to the optimized joint locations and bone rotations. Finally, we\nre-parametrize the SMPL mesh with the new kinematic parameters. The resulting\nSKEL model is animatable like SMPL but with fewer, and\nbiomechanically-realistic, degrees of freedom. We show that SKEL has more\nbiomechanically accurate joint locations than SMPL, and the bones fit inside\nthe body surface better than previous methods. By fitting SKEL to SMPL meshes\nwe are able to \"upgrade\" existing human pose and shape datasets to include\nbiomechanical parameters. SKEL provides a new tool to enable biomechanics in\nthe wild, while also providing vision and graphics researchers with a better\nconstrained and more realistic model of human articulation. The model, code,\nand data are available for research at https://skel.is.tue.mpg.de..\n", "link": "http://arxiv.org/abs/2509.06607v1", "date": "2025-09-08", "relevancy": 2.441, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6332}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6211}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Skin%20to%20Skeleton%3A%20Towards%20Biomechanically%20Accurate%203D%20Digital%0A%20%20Humans&body=Title%3A%20From%20Skin%20to%20Skeleton%3A%20Towards%20Biomechanically%20Accurate%203D%20Digital%0A%20%20Humans%0AAuthor%3A%20Marilyn%20Keller%20and%20Keenon%20Werling%20and%20Soyong%20Shin%20and%20Scott%20Delp%20and%20Sergi%20Pujades%20and%20C.%20Karen%20Liu%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20Great%20progress%20has%20been%20made%20in%20estimating%203D%20human%20pose%20and%20shape%20from%0Aimages%20and%20video%20by%20training%20neural%20networks%20to%20directly%20regress%20the%20parameters%0Aof%20parametric%20human%20models%20like%20SMPL.%20However%2C%20existing%20body%20models%20have%0Asimplified%20kinematic%20structures%20that%20do%20not%20correspond%20to%20the%20true%20joint%0Alocations%20and%20articulations%20in%20the%20human%20skeletal%20system%2C%20limiting%20their%0Apotential%20use%20in%20biomechanics.%20On%20the%20other%20hand%2C%20methods%20for%20estimating%0Abiomechanically%20accurate%20skeletal%20motion%20typically%20rely%20on%20complex%20motion%0Acapture%20systems%20and%20expensive%20optimization%20methods.%20What%20is%20needed%20is%20a%0Aparametric%203D%20human%20model%20with%20a%20biomechanically%20accurate%20skeletal%20structure%0Athat%20can%20be%20easily%20posed.%20To%20that%20end%2C%20we%20develop%20SKEL%2C%20which%20re-rigs%20the%20SMPL%0Abody%20model%20with%20a%20biomechanics%20skeleton.%20To%20enable%20this%2C%20we%20need%20training%20data%0Aof%20skeletons%20inside%20SMPL%20meshes%20in%20diverse%20poses.%0A%20%20We%20build%20such%20a%20dataset%20by%20optimizing%20biomechanically%20accurate%20skeletons%0Ainside%20SMPL%20meshes%20from%20AMASS%20sequences.%20We%20then%20learn%20a%20regressor%20from%20SMPL%0Amesh%20vertices%20to%20the%20optimized%20joint%20locations%20and%20bone%20rotations.%20Finally%2C%20we%0Are-parametrize%20the%20SMPL%20mesh%20with%20the%20new%20kinematic%20parameters.%20The%20resulting%0ASKEL%20model%20is%20animatable%20like%20SMPL%20but%20with%20fewer%2C%20and%0Abiomechanically-realistic%2C%20degrees%20of%20freedom.%20We%20show%20that%20SKEL%20has%20more%0Abiomechanically%20accurate%20joint%20locations%20than%20SMPL%2C%20and%20the%20bones%20fit%20inside%0Athe%20body%20surface%20better%20than%20previous%20methods.%20By%20fitting%20SKEL%20to%20SMPL%20meshes%0Awe%20are%20able%20to%20%22upgrade%22%20existing%20human%20pose%20and%20shape%20datasets%20to%20include%0Abiomechanical%20parameters.%20SKEL%20provides%20a%20new%20tool%20to%20enable%20biomechanics%20in%0Athe%20wild%2C%20while%20also%20providing%20vision%20and%20graphics%20researchers%20with%20a%20better%0Aconstrained%20and%20more%20realistic%20model%20of%20human%20articulation.%20The%20model%2C%20code%2C%0Aand%20data%20are%20available%20for%20research%20at%20https%3A//skel.is.tue.mpg.de..%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Skin%2520to%2520Skeleton%253A%2520Towards%2520Biomechanically%2520Accurate%25203D%2520Digital%250A%2520%2520Humans%26entry.906535625%3DMarilyn%2520Keller%2520and%2520Keenon%2520Werling%2520and%2520Soyong%2520Shin%2520and%2520Scott%2520Delp%2520and%2520Sergi%2520Pujades%2520and%2520C.%2520Karen%2520Liu%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520Great%2520progress%2520has%2520been%2520made%2520in%2520estimating%25203D%2520human%2520pose%2520and%2520shape%2520from%250Aimages%2520and%2520video%2520by%2520training%2520neural%2520networks%2520to%2520directly%2520regress%2520the%2520parameters%250Aof%2520parametric%2520human%2520models%2520like%2520SMPL.%2520However%252C%2520existing%2520body%2520models%2520have%250Asimplified%2520kinematic%2520structures%2520that%2520do%2520not%2520correspond%2520to%2520the%2520true%2520joint%250Alocations%2520and%2520articulations%2520in%2520the%2520human%2520skeletal%2520system%252C%2520limiting%2520their%250Apotential%2520use%2520in%2520biomechanics.%2520On%2520the%2520other%2520hand%252C%2520methods%2520for%2520estimating%250Abiomechanically%2520accurate%2520skeletal%2520motion%2520typically%2520rely%2520on%2520complex%2520motion%250Acapture%2520systems%2520and%2520expensive%2520optimization%2520methods.%2520What%2520is%2520needed%2520is%2520a%250Aparametric%25203D%2520human%2520model%2520with%2520a%2520biomechanically%2520accurate%2520skeletal%2520structure%250Athat%2520can%2520be%2520easily%2520posed.%2520To%2520that%2520end%252C%2520we%2520develop%2520SKEL%252C%2520which%2520re-rigs%2520the%2520SMPL%250Abody%2520model%2520with%2520a%2520biomechanics%2520skeleton.%2520To%2520enable%2520this%252C%2520we%2520need%2520training%2520data%250Aof%2520skeletons%2520inside%2520SMPL%2520meshes%2520in%2520diverse%2520poses.%250A%2520%2520We%2520build%2520such%2520a%2520dataset%2520by%2520optimizing%2520biomechanically%2520accurate%2520skeletons%250Ainside%2520SMPL%2520meshes%2520from%2520AMASS%2520sequences.%2520We%2520then%2520learn%2520a%2520regressor%2520from%2520SMPL%250Amesh%2520vertices%2520to%2520the%2520optimized%2520joint%2520locations%2520and%2520bone%2520rotations.%2520Finally%252C%2520we%250Are-parametrize%2520the%2520SMPL%2520mesh%2520with%2520the%2520new%2520kinematic%2520parameters.%2520The%2520resulting%250ASKEL%2520model%2520is%2520animatable%2520like%2520SMPL%2520but%2520with%2520fewer%252C%2520and%250Abiomechanically-realistic%252C%2520degrees%2520of%2520freedom.%2520We%2520show%2520that%2520SKEL%2520has%2520more%250Abiomechanically%2520accurate%2520joint%2520locations%2520than%2520SMPL%252C%2520and%2520the%2520bones%2520fit%2520inside%250Athe%2520body%2520surface%2520better%2520than%2520previous%2520methods.%2520By%2520fitting%2520SKEL%2520to%2520SMPL%2520meshes%250Awe%2520are%2520able%2520to%2520%2522upgrade%2522%2520existing%2520human%2520pose%2520and%2520shape%2520datasets%2520to%2520include%250Abiomechanical%2520parameters.%2520SKEL%2520provides%2520a%2520new%2520tool%2520to%2520enable%2520biomechanics%2520in%250Athe%2520wild%252C%2520while%2520also%2520providing%2520vision%2520and%2520graphics%2520researchers%2520with%2520a%2520better%250Aconstrained%2520and%2520more%2520realistic%2520model%2520of%2520human%2520articulation.%2520The%2520model%252C%2520code%252C%250Aand%2520data%2520are%2520available%2520for%2520research%2520at%2520https%253A//skel.is.tue.mpg.de..%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Skin%20to%20Skeleton%3A%20Towards%20Biomechanically%20Accurate%203D%20Digital%0A%20%20Humans&entry.906535625=Marilyn%20Keller%20and%20Keenon%20Werling%20and%20Soyong%20Shin%20and%20Scott%20Delp%20and%20Sergi%20Pujades%20and%20C.%20Karen%20Liu%20and%20Michael%20J.%20Black&entry.1292438233=%20%20Great%20progress%20has%20been%20made%20in%20estimating%203D%20human%20pose%20and%20shape%20from%0Aimages%20and%20video%20by%20training%20neural%20networks%20to%20directly%20regress%20the%20parameters%0Aof%20parametric%20human%20models%20like%20SMPL.%20However%2C%20existing%20body%20models%20have%0Asimplified%20kinematic%20structures%20that%20do%20not%20correspond%20to%20the%20true%20joint%0Alocations%20and%20articulations%20in%20the%20human%20skeletal%20system%2C%20limiting%20their%0Apotential%20use%20in%20biomechanics.%20On%20the%20other%20hand%2C%20methods%20for%20estimating%0Abiomechanically%20accurate%20skeletal%20motion%20typically%20rely%20on%20complex%20motion%0Acapture%20systems%20and%20expensive%20optimization%20methods.%20What%20is%20needed%20is%20a%0Aparametric%203D%20human%20model%20with%20a%20biomechanically%20accurate%20skeletal%20structure%0Athat%20can%20be%20easily%20posed.%20To%20that%20end%2C%20we%20develop%20SKEL%2C%20which%20re-rigs%20the%20SMPL%0Abody%20model%20with%20a%20biomechanics%20skeleton.%20To%20enable%20this%2C%20we%20need%20training%20data%0Aof%20skeletons%20inside%20SMPL%20meshes%20in%20diverse%20poses.%0A%20%20We%20build%20such%20a%20dataset%20by%20optimizing%20biomechanically%20accurate%20skeletons%0Ainside%20SMPL%20meshes%20from%20AMASS%20sequences.%20We%20then%20learn%20a%20regressor%20from%20SMPL%0Amesh%20vertices%20to%20the%20optimized%20joint%20locations%20and%20bone%20rotations.%20Finally%2C%20we%0Are-parametrize%20the%20SMPL%20mesh%20with%20the%20new%20kinematic%20parameters.%20The%20resulting%0ASKEL%20model%20is%20animatable%20like%20SMPL%20but%20with%20fewer%2C%20and%0Abiomechanically-realistic%2C%20degrees%20of%20freedom.%20We%20show%20that%20SKEL%20has%20more%0Abiomechanically%20accurate%20joint%20locations%20than%20SMPL%2C%20and%20the%20bones%20fit%20inside%0Athe%20body%20surface%20better%20than%20previous%20methods.%20By%20fitting%20SKEL%20to%20SMPL%20meshes%0Awe%20are%20able%20to%20%22upgrade%22%20existing%20human%20pose%20and%20shape%20datasets%20to%20include%0Abiomechanical%20parameters.%20SKEL%20provides%20a%20new%20tool%20to%20enable%20biomechanics%20in%0Athe%20wild%2C%20while%20also%20providing%20vision%20and%20graphics%20researchers%20with%20a%20better%0Aconstrained%20and%20more%20realistic%20model%20of%20human%20articulation.%20The%20model%2C%20code%2C%0Aand%20data%20are%20available%20for%20research%20at%20https%3A//skel.is.tue.mpg.de..%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06607v1&entry.124074799=Read"},
{"title": "Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly\n  Detection", "author": "Margarida Ferreira and Victor Nicolet and Luan Pham and Joey Dodds and Daniel Kroening and Ines Lynce and Ruben Martins", "abstract": "  We propose HyGLAD, a novel algorithm that automatically builds a set of\ninterpretable patterns that model event data. These patterns can then be used\nto detect event-based anomalies in a stationary system, where any deviation\nfrom past behavior may indicate malicious activity. The algorithm infers\nequivalence classes of entities with similar behavior observed from the events,\nand then builds regular expressions that capture the values of those entities.\nAs opposed to deep-learning approaches, the regular expressions are directly\ninterpretable, which also translates to interpretable anomalies. We evaluate\nHyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five\ndatasets from real-world systems. The experimental results show that on average\nHyGLAD outperforms existing deep-learning methods while being an order of\nmagnitude more efficient in training and inference (single CPU vs GPU).\nPrecision improved by 1.2x and recall by 1.3x compared to the second-best\nbaseline.\n", "link": "http://arxiv.org/abs/2509.06911v1", "date": "2025-09-08", "relevancy": 2.4382, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5004}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4883}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypergraph-Guided%20Regex%20Filter%20Synthesis%20for%20Event-Based%20Anomaly%0A%20%20Detection&body=Title%3A%20Hypergraph-Guided%20Regex%20Filter%20Synthesis%20for%20Event-Based%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Margarida%20Ferreira%20and%20Victor%20Nicolet%20and%20Luan%20Pham%20and%20Joey%20Dodds%20and%20Daniel%20Kroening%20and%20Ines%20Lynce%20and%20Ruben%20Martins%0AAbstract%3A%20%20%20We%20propose%20HyGLAD%2C%20a%20novel%20algorithm%20that%20automatically%20builds%20a%20set%20of%0Ainterpretable%20patterns%20that%20model%20event%20data.%20These%20patterns%20can%20then%20be%20used%0Ato%20detect%20event-based%20anomalies%20in%20a%20stationary%20system%2C%20where%20any%20deviation%0Afrom%20past%20behavior%20may%20indicate%20malicious%20activity.%20The%20algorithm%20infers%0Aequivalence%20classes%20of%20entities%20with%20similar%20behavior%20observed%20from%20the%20events%2C%0Aand%20then%20builds%20regular%20expressions%20that%20capture%20the%20values%20of%20those%20entities.%0AAs%20opposed%20to%20deep-learning%20approaches%2C%20the%20regular%20expressions%20are%20directly%0Ainterpretable%2C%20which%20also%20translates%20to%20interpretable%20anomalies.%20We%20evaluate%0AHyGLAD%20against%20all%207%20unsupervised%20anomaly%20detection%20methods%20from%20DeepOD%20on%20five%0Adatasets%20from%20real-world%20systems.%20The%20experimental%20results%20show%20that%20on%20average%0AHyGLAD%20outperforms%20existing%20deep-learning%20methods%20while%20being%20an%20order%20of%0Amagnitude%20more%20efficient%20in%20training%20and%20inference%20%28single%20CPU%20vs%20GPU%29.%0APrecision%20improved%20by%201.2x%20and%20recall%20by%201.3x%20compared%20to%20the%20second-best%0Abaseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypergraph-Guided%2520Regex%2520Filter%2520Synthesis%2520for%2520Event-Based%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DMargarida%2520Ferreira%2520and%2520Victor%2520Nicolet%2520and%2520Luan%2520Pham%2520and%2520Joey%2520Dodds%2520and%2520Daniel%2520Kroening%2520and%2520Ines%2520Lynce%2520and%2520Ruben%2520Martins%26entry.1292438233%3D%2520%2520We%2520propose%2520HyGLAD%252C%2520a%2520novel%2520algorithm%2520that%2520automatically%2520builds%2520a%2520set%2520of%250Ainterpretable%2520patterns%2520that%2520model%2520event%2520data.%2520These%2520patterns%2520can%2520then%2520be%2520used%250Ato%2520detect%2520event-based%2520anomalies%2520in%2520a%2520stationary%2520system%252C%2520where%2520any%2520deviation%250Afrom%2520past%2520behavior%2520may%2520indicate%2520malicious%2520activity.%2520The%2520algorithm%2520infers%250Aequivalence%2520classes%2520of%2520entities%2520with%2520similar%2520behavior%2520observed%2520from%2520the%2520events%252C%250Aand%2520then%2520builds%2520regular%2520expressions%2520that%2520capture%2520the%2520values%2520of%2520those%2520entities.%250AAs%2520opposed%2520to%2520deep-learning%2520approaches%252C%2520the%2520regular%2520expressions%2520are%2520directly%250Ainterpretable%252C%2520which%2520also%2520translates%2520to%2520interpretable%2520anomalies.%2520We%2520evaluate%250AHyGLAD%2520against%2520all%25207%2520unsupervised%2520anomaly%2520detection%2520methods%2520from%2520DeepOD%2520on%2520five%250Adatasets%2520from%2520real-world%2520systems.%2520The%2520experimental%2520results%2520show%2520that%2520on%2520average%250AHyGLAD%2520outperforms%2520existing%2520deep-learning%2520methods%2520while%2520being%2520an%2520order%2520of%250Amagnitude%2520more%2520efficient%2520in%2520training%2520and%2520inference%2520%2528single%2520CPU%2520vs%2520GPU%2529.%250APrecision%2520improved%2520by%25201.2x%2520and%2520recall%2520by%25201.3x%2520compared%2520to%2520the%2520second-best%250Abaseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph-Guided%20Regex%20Filter%20Synthesis%20for%20Event-Based%20Anomaly%0A%20%20Detection&entry.906535625=Margarida%20Ferreira%20and%20Victor%20Nicolet%20and%20Luan%20Pham%20and%20Joey%20Dodds%20and%20Daniel%20Kroening%20and%20Ines%20Lynce%20and%20Ruben%20Martins&entry.1292438233=%20%20We%20propose%20HyGLAD%2C%20a%20novel%20algorithm%20that%20automatically%20builds%20a%20set%20of%0Ainterpretable%20patterns%20that%20model%20event%20data.%20These%20patterns%20can%20then%20be%20used%0Ato%20detect%20event-based%20anomalies%20in%20a%20stationary%20system%2C%20where%20any%20deviation%0Afrom%20past%20behavior%20may%20indicate%20malicious%20activity.%20The%20algorithm%20infers%0Aequivalence%20classes%20of%20entities%20with%20similar%20behavior%20observed%20from%20the%20events%2C%0Aand%20then%20builds%20regular%20expressions%20that%20capture%20the%20values%20of%20those%20entities.%0AAs%20opposed%20to%20deep-learning%20approaches%2C%20the%20regular%20expressions%20are%20directly%0Ainterpretable%2C%20which%20also%20translates%20to%20interpretable%20anomalies.%20We%20evaluate%0AHyGLAD%20against%20all%207%20unsupervised%20anomaly%20detection%20methods%20from%20DeepOD%20on%20five%0Adatasets%20from%20real-world%20systems.%20The%20experimental%20results%20show%20that%20on%20average%0AHyGLAD%20outperforms%20existing%20deep-learning%20methods%20while%20being%20an%20order%20of%0Amagnitude%20more%20efficient%20in%20training%20and%20inference%20%28single%20CPU%20vs%20GPU%29.%0APrecision%20improved%20by%201.2x%20and%20recall%20by%201.3x%20compared%20to%20the%20second-best%0Abaseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06911v1&entry.124074799=Read"},
{"title": "Integrated Detection and Tracking Based on Radar Range-Doppler Feature", "author": "Chenyu Zhang and Yuanhang Wu and Xiaoxi Ma and Wei Yi", "abstract": "  Detection and tracking are the basic tasks of radar systems. Current joint\ndetection tracking methods, which focus on dynamically adjusting detection\nthresholds from tracking results, still present challenges in fully utilizing\nthe potential of radar signals. These are mainly reflected in the limited\ncapacity of the constant false-alarm rate model to accurately represent\ninformation, the insufficient depiction of complex scenes, and the limited\ninformation acquired by the tracker. We introduce the Integrated Detection and\nTracking based on radar feature (InDT) method, which comprises a network\narchitecture for radar signal detection and a tracker that leverages detection\nassistance. The InDT detector extracts feature information from each\nRange-Doppler (RD) matrix and then returns the target position through the\nfeature enhancement module and the detection head. The InDT tracker adaptively\nupdates the measurement noise covariance of the Kalman filter based on\ndetection confidence. The similarity of target RD features is measured by\ncosine distance, which enhances the data association process by combining\nlocation and feature information. Finally, the efficacy of the proposed method\nwas validated through testing on both simulated data and publicly available\ndatasets.\n", "link": "http://arxiv.org/abs/2509.06569v1", "date": "2025-09-08", "relevancy": 2.437, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5021}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Detection%20and%20Tracking%20Based%20on%20Radar%20Range-Doppler%20Feature&body=Title%3A%20Integrated%20Detection%20and%20Tracking%20Based%20on%20Radar%20Range-Doppler%20Feature%0AAuthor%3A%20Chenyu%20Zhang%20and%20Yuanhang%20Wu%20and%20Xiaoxi%20Ma%20and%20Wei%20Yi%0AAbstract%3A%20%20%20Detection%20and%20tracking%20are%20the%20basic%20tasks%20of%20radar%20systems.%20Current%20joint%0Adetection%20tracking%20methods%2C%20which%20focus%20on%20dynamically%20adjusting%20detection%0Athresholds%20from%20tracking%20results%2C%20still%20present%20challenges%20in%20fully%20utilizing%0Athe%20potential%20of%20radar%20signals.%20These%20are%20mainly%20reflected%20in%20the%20limited%0Acapacity%20of%20the%20constant%20false-alarm%20rate%20model%20to%20accurately%20represent%0Ainformation%2C%20the%20insufficient%20depiction%20of%20complex%20scenes%2C%20and%20the%20limited%0Ainformation%20acquired%20by%20the%20tracker.%20We%20introduce%20the%20Integrated%20Detection%20and%0ATracking%20based%20on%20radar%20feature%20%28InDT%29%20method%2C%20which%20comprises%20a%20network%0Aarchitecture%20for%20radar%20signal%20detection%20and%20a%20tracker%20that%20leverages%20detection%0Aassistance.%20The%20InDT%20detector%20extracts%20feature%20information%20from%20each%0ARange-Doppler%20%28RD%29%20matrix%20and%20then%20returns%20the%20target%20position%20through%20the%0Afeature%20enhancement%20module%20and%20the%20detection%20head.%20The%20InDT%20tracker%20adaptively%0Aupdates%20the%20measurement%20noise%20covariance%20of%20the%20Kalman%20filter%20based%20on%0Adetection%20confidence.%20The%20similarity%20of%20target%20RD%20features%20is%20measured%20by%0Acosine%20distance%2C%20which%20enhances%20the%20data%20association%20process%20by%20combining%0Alocation%20and%20feature%20information.%20Finally%2C%20the%20efficacy%20of%20the%20proposed%20method%0Awas%20validated%20through%20testing%20on%20both%20simulated%20data%20and%20publicly%20available%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Detection%2520and%2520Tracking%2520Based%2520on%2520Radar%2520Range-Doppler%2520Feature%26entry.906535625%3DChenyu%2520Zhang%2520and%2520Yuanhang%2520Wu%2520and%2520Xiaoxi%2520Ma%2520and%2520Wei%2520Yi%26entry.1292438233%3D%2520%2520Detection%2520and%2520tracking%2520are%2520the%2520basic%2520tasks%2520of%2520radar%2520systems.%2520Current%2520joint%250Adetection%2520tracking%2520methods%252C%2520which%2520focus%2520on%2520dynamically%2520adjusting%2520detection%250Athresholds%2520from%2520tracking%2520results%252C%2520still%2520present%2520challenges%2520in%2520fully%2520utilizing%250Athe%2520potential%2520of%2520radar%2520signals.%2520These%2520are%2520mainly%2520reflected%2520in%2520the%2520limited%250Acapacity%2520of%2520the%2520constant%2520false-alarm%2520rate%2520model%2520to%2520accurately%2520represent%250Ainformation%252C%2520the%2520insufficient%2520depiction%2520of%2520complex%2520scenes%252C%2520and%2520the%2520limited%250Ainformation%2520acquired%2520by%2520the%2520tracker.%2520We%2520introduce%2520the%2520Integrated%2520Detection%2520and%250ATracking%2520based%2520on%2520radar%2520feature%2520%2528InDT%2529%2520method%252C%2520which%2520comprises%2520a%2520network%250Aarchitecture%2520for%2520radar%2520signal%2520detection%2520and%2520a%2520tracker%2520that%2520leverages%2520detection%250Aassistance.%2520The%2520InDT%2520detector%2520extracts%2520feature%2520information%2520from%2520each%250ARange-Doppler%2520%2528RD%2529%2520matrix%2520and%2520then%2520returns%2520the%2520target%2520position%2520through%2520the%250Afeature%2520enhancement%2520module%2520and%2520the%2520detection%2520head.%2520The%2520InDT%2520tracker%2520adaptively%250Aupdates%2520the%2520measurement%2520noise%2520covariance%2520of%2520the%2520Kalman%2520filter%2520based%2520on%250Adetection%2520confidence.%2520The%2520similarity%2520of%2520target%2520RD%2520features%2520is%2520measured%2520by%250Acosine%2520distance%252C%2520which%2520enhances%2520the%2520data%2520association%2520process%2520by%2520combining%250Alocation%2520and%2520feature%2520information.%2520Finally%252C%2520the%2520efficacy%2520of%2520the%2520proposed%2520method%250Awas%2520validated%2520through%2520testing%2520on%2520both%2520simulated%2520data%2520and%2520publicly%2520available%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Detection%20and%20Tracking%20Based%20on%20Radar%20Range-Doppler%20Feature&entry.906535625=Chenyu%20Zhang%20and%20Yuanhang%20Wu%20and%20Xiaoxi%20Ma%20and%20Wei%20Yi&entry.1292438233=%20%20Detection%20and%20tracking%20are%20the%20basic%20tasks%20of%20radar%20systems.%20Current%20joint%0Adetection%20tracking%20methods%2C%20which%20focus%20on%20dynamically%20adjusting%20detection%0Athresholds%20from%20tracking%20results%2C%20still%20present%20challenges%20in%20fully%20utilizing%0Athe%20potential%20of%20radar%20signals.%20These%20are%20mainly%20reflected%20in%20the%20limited%0Acapacity%20of%20the%20constant%20false-alarm%20rate%20model%20to%20accurately%20represent%0Ainformation%2C%20the%20insufficient%20depiction%20of%20complex%20scenes%2C%20and%20the%20limited%0Ainformation%20acquired%20by%20the%20tracker.%20We%20introduce%20the%20Integrated%20Detection%20and%0ATracking%20based%20on%20radar%20feature%20%28InDT%29%20method%2C%20which%20comprises%20a%20network%0Aarchitecture%20for%20radar%20signal%20detection%20and%20a%20tracker%20that%20leverages%20detection%0Aassistance.%20The%20InDT%20detector%20extracts%20feature%20information%20from%20each%0ARange-Doppler%20%28RD%29%20matrix%20and%20then%20returns%20the%20target%20position%20through%20the%0Afeature%20enhancement%20module%20and%20the%20detection%20head.%20The%20InDT%20tracker%20adaptively%0Aupdates%20the%20measurement%20noise%20covariance%20of%20the%20Kalman%20filter%20based%20on%0Adetection%20confidence.%20The%20similarity%20of%20target%20RD%20features%20is%20measured%20by%0Acosine%20distance%2C%20which%20enhances%20the%20data%20association%20process%20by%20combining%0Alocation%20and%20feature%20information.%20Finally%2C%20the%20efficacy%20of%20the%20proposed%20method%0Awas%20validated%20through%20testing%20on%20both%20simulated%20data%20and%20publicly%20available%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06569v1&entry.124074799=Read"},
{"title": "On the Reproducibility of \"FairCLIP: Harnessing Fairness in\n  Vision-Language Learning''", "author": "Hua Chang Bakker and Stan Fris and Angela Madelon Bernardy and Stan Deutekom", "abstract": "  We investigated the reproducibility of FairCLIP, proposed by Luo et al.\n(2024), for improving the group fairness of CLIP (Radford et al., 2021) by\nminimizing image-text similarity score disparities across sensitive groups\nusing the Sinkhorn distance. The experimental setup of Luo et al. (2024) was\nreproduced to primarily investigate the research findings for FairCLIP. The\nmodel description by Luo et al. (2024) was found to differ from the original\nimplementation. Therefore, a new implementation, A-FairCLIP, is introduced to\nexamine specific design choices. Furthermore, FairCLIP+ is proposed to extend\nthe FairCLIP objective to include multiple attributes. Additionally, the impact\nof the distance minimization on FairCLIP's fairness and performance was\nexplored. In alignment with the original authors, CLIP was found to be biased\ntowards certain demographics when applied to zero-shot glaucoma classification\nusing medical scans and clinical notes from the Harvard-FairVLMed dataset.\nHowever, the experimental results on two datasets do not support their claim\nthat FairCLIP improves the performance and fairness of CLIP. Although the\nregularization objective reduces Sinkhorn distances, both the official\nimplementation and the aligned implementation, A-FairCLIP, were not found to\nimprove performance nor fairness in zero-shot glaucoma classification.\n", "link": "http://arxiv.org/abs/2509.06535v1", "date": "2025-09-08", "relevancy": 2.4256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4748}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Reproducibility%20of%20%22FairCLIP%3A%20Harnessing%20Fairness%20in%0A%20%20Vision-Language%20Learning%27%27&body=Title%3A%20On%20the%20Reproducibility%20of%20%22FairCLIP%3A%20Harnessing%20Fairness%20in%0A%20%20Vision-Language%20Learning%27%27%0AAuthor%3A%20Hua%20Chang%20Bakker%20and%20Stan%20Fris%20and%20Angela%20Madelon%20Bernardy%20and%20Stan%20Deutekom%0AAbstract%3A%20%20%20We%20investigated%20the%20reproducibility%20of%20FairCLIP%2C%20proposed%20by%20Luo%20et%20al.%0A%282024%29%2C%20for%20improving%20the%20group%20fairness%20of%20CLIP%20%28Radford%20et%20al.%2C%202021%29%20by%0Aminimizing%20image-text%20similarity%20score%20disparities%20across%20sensitive%20groups%0Ausing%20the%20Sinkhorn%20distance.%20The%20experimental%20setup%20of%20Luo%20et%20al.%20%282024%29%20was%0Areproduced%20to%20primarily%20investigate%20the%20research%20findings%20for%20FairCLIP.%20The%0Amodel%20description%20by%20Luo%20et%20al.%20%282024%29%20was%20found%20to%20differ%20from%20the%20original%0Aimplementation.%20Therefore%2C%20a%20new%20implementation%2C%20A-FairCLIP%2C%20is%20introduced%20to%0Aexamine%20specific%20design%20choices.%20Furthermore%2C%20FairCLIP%2B%20is%20proposed%20to%20extend%0Athe%20FairCLIP%20objective%20to%20include%20multiple%20attributes.%20Additionally%2C%20the%20impact%0Aof%20the%20distance%20minimization%20on%20FairCLIP%27s%20fairness%20and%20performance%20was%0Aexplored.%20In%20alignment%20with%20the%20original%20authors%2C%20CLIP%20was%20found%20to%20be%20biased%0Atowards%20certain%20demographics%20when%20applied%20to%20zero-shot%20glaucoma%20classification%0Ausing%20medical%20scans%20and%20clinical%20notes%20from%20the%20Harvard-FairVLMed%20dataset.%0AHowever%2C%20the%20experimental%20results%20on%20two%20datasets%20do%20not%20support%20their%20claim%0Athat%20FairCLIP%20improves%20the%20performance%20and%20fairness%20of%20CLIP.%20Although%20the%0Aregularization%20objective%20reduces%20Sinkhorn%20distances%2C%20both%20the%20official%0Aimplementation%20and%20the%20aligned%20implementation%2C%20A-FairCLIP%2C%20were%20not%20found%20to%0Aimprove%20performance%20nor%20fairness%20in%20zero-shot%20glaucoma%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Reproducibility%2520of%2520%2522FairCLIP%253A%2520Harnessing%2520Fairness%2520in%250A%2520%2520Vision-Language%2520Learning%2527%2527%26entry.906535625%3DHua%2520Chang%2520Bakker%2520and%2520Stan%2520Fris%2520and%2520Angela%2520Madelon%2520Bernardy%2520and%2520Stan%2520Deutekom%26entry.1292438233%3D%2520%2520We%2520investigated%2520the%2520reproducibility%2520of%2520FairCLIP%252C%2520proposed%2520by%2520Luo%2520et%2520al.%250A%25282024%2529%252C%2520for%2520improving%2520the%2520group%2520fairness%2520of%2520CLIP%2520%2528Radford%2520et%2520al.%252C%25202021%2529%2520by%250Aminimizing%2520image-text%2520similarity%2520score%2520disparities%2520across%2520sensitive%2520groups%250Ausing%2520the%2520Sinkhorn%2520distance.%2520The%2520experimental%2520setup%2520of%2520Luo%2520et%2520al.%2520%25282024%2529%2520was%250Areproduced%2520to%2520primarily%2520investigate%2520the%2520research%2520findings%2520for%2520FairCLIP.%2520The%250Amodel%2520description%2520by%2520Luo%2520et%2520al.%2520%25282024%2529%2520was%2520found%2520to%2520differ%2520from%2520the%2520original%250Aimplementation.%2520Therefore%252C%2520a%2520new%2520implementation%252C%2520A-FairCLIP%252C%2520is%2520introduced%2520to%250Aexamine%2520specific%2520design%2520choices.%2520Furthermore%252C%2520FairCLIP%252B%2520is%2520proposed%2520to%2520extend%250Athe%2520FairCLIP%2520objective%2520to%2520include%2520multiple%2520attributes.%2520Additionally%252C%2520the%2520impact%250Aof%2520the%2520distance%2520minimization%2520on%2520FairCLIP%2527s%2520fairness%2520and%2520performance%2520was%250Aexplored.%2520In%2520alignment%2520with%2520the%2520original%2520authors%252C%2520CLIP%2520was%2520found%2520to%2520be%2520biased%250Atowards%2520certain%2520demographics%2520when%2520applied%2520to%2520zero-shot%2520glaucoma%2520classification%250Ausing%2520medical%2520scans%2520and%2520clinical%2520notes%2520from%2520the%2520Harvard-FairVLMed%2520dataset.%250AHowever%252C%2520the%2520experimental%2520results%2520on%2520two%2520datasets%2520do%2520not%2520support%2520their%2520claim%250Athat%2520FairCLIP%2520improves%2520the%2520performance%2520and%2520fairness%2520of%2520CLIP.%2520Although%2520the%250Aregularization%2520objective%2520reduces%2520Sinkhorn%2520distances%252C%2520both%2520the%2520official%250Aimplementation%2520and%2520the%2520aligned%2520implementation%252C%2520A-FairCLIP%252C%2520were%2520not%2520found%2520to%250Aimprove%2520performance%2520nor%2520fairness%2520in%2520zero-shot%2520glaucoma%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Reproducibility%20of%20%22FairCLIP%3A%20Harnessing%20Fairness%20in%0A%20%20Vision-Language%20Learning%27%27&entry.906535625=Hua%20Chang%20Bakker%20and%20Stan%20Fris%20and%20Angela%20Madelon%20Bernardy%20and%20Stan%20Deutekom&entry.1292438233=%20%20We%20investigated%20the%20reproducibility%20of%20FairCLIP%2C%20proposed%20by%20Luo%20et%20al.%0A%282024%29%2C%20for%20improving%20the%20group%20fairness%20of%20CLIP%20%28Radford%20et%20al.%2C%202021%29%20by%0Aminimizing%20image-text%20similarity%20score%20disparities%20across%20sensitive%20groups%0Ausing%20the%20Sinkhorn%20distance.%20The%20experimental%20setup%20of%20Luo%20et%20al.%20%282024%29%20was%0Areproduced%20to%20primarily%20investigate%20the%20research%20findings%20for%20FairCLIP.%20The%0Amodel%20description%20by%20Luo%20et%20al.%20%282024%29%20was%20found%20to%20differ%20from%20the%20original%0Aimplementation.%20Therefore%2C%20a%20new%20implementation%2C%20A-FairCLIP%2C%20is%20introduced%20to%0Aexamine%20specific%20design%20choices.%20Furthermore%2C%20FairCLIP%2B%20is%20proposed%20to%20extend%0Athe%20FairCLIP%20objective%20to%20include%20multiple%20attributes.%20Additionally%2C%20the%20impact%0Aof%20the%20distance%20minimization%20on%20FairCLIP%27s%20fairness%20and%20performance%20was%0Aexplored.%20In%20alignment%20with%20the%20original%20authors%2C%20CLIP%20was%20found%20to%20be%20biased%0Atowards%20certain%20demographics%20when%20applied%20to%20zero-shot%20glaucoma%20classification%0Ausing%20medical%20scans%20and%20clinical%20notes%20from%20the%20Harvard-FairVLMed%20dataset.%0AHowever%2C%20the%20experimental%20results%20on%20two%20datasets%20do%20not%20support%20their%20claim%0Athat%20FairCLIP%20improves%20the%20performance%20and%20fairness%20of%20CLIP.%20Although%20the%0Aregularization%20objective%20reduces%20Sinkhorn%20distances%2C%20both%20the%20official%0Aimplementation%20and%20the%20aligned%20implementation%2C%20A-FairCLIP%2C%20were%20not%20found%20to%0Aimprove%20performance%20nor%20fairness%20in%20zero-shot%20glaucoma%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06535v1&entry.124074799=Read"},
{"title": "Convolutional Neural Networks Can (Meta-)Learn the Same-Different\n  Relation", "author": "Max Gupta and Sunayana Rane and R. Thomas McCoy and Thomas L. Griffiths", "abstract": "  While convolutional neural networks (CNNs) have come to match and exceed\nhuman performance in many settings, the tasks these models optimize for are\nlargely constrained to the level of individual objects, such as classification\nand captioning. Humans remain vastly superior to CNNs in visual tasks involving\nrelations, including the ability to identify two objects as `same' or\n`different'. A number of studies have shown that while CNNs can be coaxed into\nlearning the same-different relation in some settings, they tend to generalize\npoorly to other instances of this relation. In this work we show that the same\nCNN architectures that fail to generalize the same-different relation with\nconventional training are able to succeed when trained via meta-learning, which\nexplicitly encourages abstraction and generalization across tasks.\n", "link": "http://arxiv.org/abs/2503.23212v3", "date": "2025-09-08", "relevancy": 2.4231, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convolutional%20Neural%20Networks%20Can%20%28Meta-%29Learn%20the%20Same-Different%0A%20%20Relation&body=Title%3A%20Convolutional%20Neural%20Networks%20Can%20%28Meta-%29Learn%20the%20Same-Different%0A%20%20Relation%0AAuthor%3A%20Max%20Gupta%20and%20Sunayana%20Rane%20and%20R.%20Thomas%20McCoy%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20While%20convolutional%20neural%20networks%20%28CNNs%29%20have%20come%20to%20match%20and%20exceed%0Ahuman%20performance%20in%20many%20settings%2C%20the%20tasks%20these%20models%20optimize%20for%20are%0Alargely%20constrained%20to%20the%20level%20of%20individual%20objects%2C%20such%20as%20classification%0Aand%20captioning.%20Humans%20remain%20vastly%20superior%20to%20CNNs%20in%20visual%20tasks%20involving%0Arelations%2C%20including%20the%20ability%20to%20identify%20two%20objects%20as%20%60same%27%20or%0A%60different%27.%20A%20number%20of%20studies%20have%20shown%20that%20while%20CNNs%20can%20be%20coaxed%20into%0Alearning%20the%20same-different%20relation%20in%20some%20settings%2C%20they%20tend%20to%20generalize%0Apoorly%20to%20other%20instances%20of%20this%20relation.%20In%20this%20work%20we%20show%20that%20the%20same%0ACNN%20architectures%20that%20fail%20to%20generalize%20the%20same-different%20relation%20with%0Aconventional%20training%20are%20able%20to%20succeed%20when%20trained%20via%20meta-learning%2C%20which%0Aexplicitly%20encourages%20abstraction%20and%20generalization%20across%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23212v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvolutional%2520Neural%2520Networks%2520Can%2520%2528Meta-%2529Learn%2520the%2520Same-Different%250A%2520%2520Relation%26entry.906535625%3DMax%2520Gupta%2520and%2520Sunayana%2520Rane%2520and%2520R.%2520Thomas%2520McCoy%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520While%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520come%2520to%2520match%2520and%2520exceed%250Ahuman%2520performance%2520in%2520many%2520settings%252C%2520the%2520tasks%2520these%2520models%2520optimize%2520for%2520are%250Alargely%2520constrained%2520to%2520the%2520level%2520of%2520individual%2520objects%252C%2520such%2520as%2520classification%250Aand%2520captioning.%2520Humans%2520remain%2520vastly%2520superior%2520to%2520CNNs%2520in%2520visual%2520tasks%2520involving%250Arelations%252C%2520including%2520the%2520ability%2520to%2520identify%2520two%2520objects%2520as%2520%2560same%2527%2520or%250A%2560different%2527.%2520A%2520number%2520of%2520studies%2520have%2520shown%2520that%2520while%2520CNNs%2520can%2520be%2520coaxed%2520into%250Alearning%2520the%2520same-different%2520relation%2520in%2520some%2520settings%252C%2520they%2520tend%2520to%2520generalize%250Apoorly%2520to%2520other%2520instances%2520of%2520this%2520relation.%2520In%2520this%2520work%2520we%2520show%2520that%2520the%2520same%250ACNN%2520architectures%2520that%2520fail%2520to%2520generalize%2520the%2520same-different%2520relation%2520with%250Aconventional%2520training%2520are%2520able%2520to%2520succeed%2520when%2520trained%2520via%2520meta-learning%252C%2520which%250Aexplicitly%2520encourages%2520abstraction%2520and%2520generalization%2520across%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23212v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Neural%20Networks%20Can%20%28Meta-%29Learn%20the%20Same-Different%0A%20%20Relation&entry.906535625=Max%20Gupta%20and%20Sunayana%20Rane%20and%20R.%20Thomas%20McCoy%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20While%20convolutional%20neural%20networks%20%28CNNs%29%20have%20come%20to%20match%20and%20exceed%0Ahuman%20performance%20in%20many%20settings%2C%20the%20tasks%20these%20models%20optimize%20for%20are%0Alargely%20constrained%20to%20the%20level%20of%20individual%20objects%2C%20such%20as%20classification%0Aand%20captioning.%20Humans%20remain%20vastly%20superior%20to%20CNNs%20in%20visual%20tasks%20involving%0Arelations%2C%20including%20the%20ability%20to%20identify%20two%20objects%20as%20%60same%27%20or%0A%60different%27.%20A%20number%20of%20studies%20have%20shown%20that%20while%20CNNs%20can%20be%20coaxed%20into%0Alearning%20the%20same-different%20relation%20in%20some%20settings%2C%20they%20tend%20to%20generalize%0Apoorly%20to%20other%20instances%20of%20this%20relation.%20In%20this%20work%20we%20show%20that%20the%20same%0ACNN%20architectures%20that%20fail%20to%20generalize%20the%20same-different%20relation%20with%0Aconventional%20training%20are%20able%20to%20succeed%20when%20trained%20via%20meta-learning%2C%20which%0Aexplicitly%20encourages%20abstraction%20and%20generalization%20across%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23212v3&entry.124074799=Read"},
{"title": "An All-Atom Generative Model for Designing Protein Complexes", "author": "Ruizhe Chen and Dongyu Xue and Xiangxin Zhou and Zaixiang Zheng and Xiangxiang Zeng and Quanquan Gu", "abstract": "  Proteins typically exist in complexes, interacting with other proteins or\nbiomolecules to perform their specific biological roles. Research on\nsingle-chain protein modeling has been extensively and deeply explored, with\nadvancements seen in models like the series of ESM and AlphaFold2. Despite\nthese developments, the study and modeling of multi-chain proteins remain\nlargely uncharted, though they are vital for understanding biological\nfunctions. Recognizing the importance of these interactions, we introduce APM\n(All-Atom Protein Generative Model), a model specifically designed for modeling\nmulti-chain proteins. By integrating atom-level information and leveraging data\non multi-chain proteins, APM is capable of precisely modeling inter-chain\ninteractions and designing protein complexes with binding capabilities from\nscratch. It also performs folding and inverse-folding tasks for multi-chain\nproteins. Moreover, APM demonstrates versatility in downstream applications: it\nachieves enhanced performance through supervised fine-tuning (SFT) while also\nsupporting zero-shot sampling in certain tasks, achieving state-of-the-art\nresults. We released our code at https://github.com/bytedance/apm.\n", "link": "http://arxiv.org/abs/2504.13075v3", "date": "2025-09-08", "relevancy": 2.4155, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4843}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4843}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes&body=Title%3A%20An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes%0AAuthor%3A%20Ruizhe%20Chen%20and%20Dongyu%20Xue%20and%20Xiangxin%20Zhou%20and%20Zaixiang%20Zheng%20and%20Xiangxiang%20Zeng%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Proteins%20typically%20exist%20in%20complexes%2C%20interacting%20with%20other%20proteins%20or%0Abiomolecules%20to%20perform%20their%20specific%20biological%20roles.%20Research%20on%0Asingle-chain%20protein%20modeling%20has%20been%20extensively%20and%20deeply%20explored%2C%20with%0Aadvancements%20seen%20in%20models%20like%20the%20series%20of%20ESM%20and%20AlphaFold2.%20Despite%0Athese%20developments%2C%20the%20study%20and%20modeling%20of%20multi-chain%20proteins%20remain%0Alargely%20uncharted%2C%20though%20they%20are%20vital%20for%20understanding%20biological%0Afunctions.%20Recognizing%20the%20importance%20of%20these%20interactions%2C%20we%20introduce%20APM%0A%28All-Atom%20Protein%20Generative%20Model%29%2C%20a%20model%20specifically%20designed%20for%20modeling%0Amulti-chain%20proteins.%20By%20integrating%20atom-level%20information%20and%20leveraging%20data%0Aon%20multi-chain%20proteins%2C%20APM%20is%20capable%20of%20precisely%20modeling%20inter-chain%0Ainteractions%20and%20designing%20protein%20complexes%20with%20binding%20capabilities%20from%0Ascratch.%20It%20also%20performs%20folding%20and%20inverse-folding%20tasks%20for%20multi-chain%0Aproteins.%20Moreover%2C%20APM%20demonstrates%20versatility%20in%20downstream%20applications%3A%20it%0Aachieves%20enhanced%20performance%20through%20supervised%20fine-tuning%20%28SFT%29%20while%20also%0Asupporting%20zero-shot%20sampling%20in%20certain%20tasks%2C%20achieving%20state-of-the-art%0Aresults.%20We%20released%20our%20code%20at%20https%3A//github.com/bytedance/apm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13075v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520All-Atom%2520Generative%2520Model%2520for%2520Designing%2520Protein%2520Complexes%26entry.906535625%3DRuizhe%2520Chen%2520and%2520Dongyu%2520Xue%2520and%2520Xiangxin%2520Zhou%2520and%2520Zaixiang%2520Zheng%2520and%2520Xiangxiang%2520Zeng%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520Proteins%2520typically%2520exist%2520in%2520complexes%252C%2520interacting%2520with%2520other%2520proteins%2520or%250Abiomolecules%2520to%2520perform%2520their%2520specific%2520biological%2520roles.%2520Research%2520on%250Asingle-chain%2520protein%2520modeling%2520has%2520been%2520extensively%2520and%2520deeply%2520explored%252C%2520with%250Aadvancements%2520seen%2520in%2520models%2520like%2520the%2520series%2520of%2520ESM%2520and%2520AlphaFold2.%2520Despite%250Athese%2520developments%252C%2520the%2520study%2520and%2520modeling%2520of%2520multi-chain%2520proteins%2520remain%250Alargely%2520uncharted%252C%2520though%2520they%2520are%2520vital%2520for%2520understanding%2520biological%250Afunctions.%2520Recognizing%2520the%2520importance%2520of%2520these%2520interactions%252C%2520we%2520introduce%2520APM%250A%2528All-Atom%2520Protein%2520Generative%2520Model%2529%252C%2520a%2520model%2520specifically%2520designed%2520for%2520modeling%250Amulti-chain%2520proteins.%2520By%2520integrating%2520atom-level%2520information%2520and%2520leveraging%2520data%250Aon%2520multi-chain%2520proteins%252C%2520APM%2520is%2520capable%2520of%2520precisely%2520modeling%2520inter-chain%250Ainteractions%2520and%2520designing%2520protein%2520complexes%2520with%2520binding%2520capabilities%2520from%250Ascratch.%2520It%2520also%2520performs%2520folding%2520and%2520inverse-folding%2520tasks%2520for%2520multi-chain%250Aproteins.%2520Moreover%252C%2520APM%2520demonstrates%2520versatility%2520in%2520downstream%2520applications%253A%2520it%250Aachieves%2520enhanced%2520performance%2520through%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520while%2520also%250Asupporting%2520zero-shot%2520sampling%2520in%2520certain%2520tasks%252C%2520achieving%2520state-of-the-art%250Aresults.%2520We%2520released%2520our%2520code%2520at%2520https%253A//github.com/bytedance/apm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13075v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20All-Atom%20Generative%20Model%20for%20Designing%20Protein%20Complexes&entry.906535625=Ruizhe%20Chen%20and%20Dongyu%20Xue%20and%20Xiangxin%20Zhou%20and%20Zaixiang%20Zheng%20and%20Xiangxiang%20Zeng%20and%20Quanquan%20Gu&entry.1292438233=%20%20Proteins%20typically%20exist%20in%20complexes%2C%20interacting%20with%20other%20proteins%20or%0Abiomolecules%20to%20perform%20their%20specific%20biological%20roles.%20Research%20on%0Asingle-chain%20protein%20modeling%20has%20been%20extensively%20and%20deeply%20explored%2C%20with%0Aadvancements%20seen%20in%20models%20like%20the%20series%20of%20ESM%20and%20AlphaFold2.%20Despite%0Athese%20developments%2C%20the%20study%20and%20modeling%20of%20multi-chain%20proteins%20remain%0Alargely%20uncharted%2C%20though%20they%20are%20vital%20for%20understanding%20biological%0Afunctions.%20Recognizing%20the%20importance%20of%20these%20interactions%2C%20we%20introduce%20APM%0A%28All-Atom%20Protein%20Generative%20Model%29%2C%20a%20model%20specifically%20designed%20for%20modeling%0Amulti-chain%20proteins.%20By%20integrating%20atom-level%20information%20and%20leveraging%20data%0Aon%20multi-chain%20proteins%2C%20APM%20is%20capable%20of%20precisely%20modeling%20inter-chain%0Ainteractions%20and%20designing%20protein%20complexes%20with%20binding%20capabilities%20from%0Ascratch.%20It%20also%20performs%20folding%20and%20inverse-folding%20tasks%20for%20multi-chain%0Aproteins.%20Moreover%2C%20APM%20demonstrates%20versatility%20in%20downstream%20applications%3A%20it%0Aachieves%20enhanced%20performance%20through%20supervised%20fine-tuning%20%28SFT%29%20while%20also%0Asupporting%20zero-shot%20sampling%20in%20certain%20tasks%2C%20achieving%20state-of-the-art%0Aresults.%20We%20released%20our%20code%20at%20https%3A//github.com/bytedance/apm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13075v3&entry.124074799=Read"},
{"title": "A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific\n  Modeling", "author": "Meher V. R. Malladi and Tiziano Guadagnino and Luca Lobefaro and Cyrill Stachniss", "abstract": "  Accurate odometry is a critical component in a robotic navigation stack, and\nsubsequent modules such as planning and control often rely on an estimate of\nthe robot's motion. Sensor-based odometry approaches should be robust across\nsensor types and deployable in different target domains, from solid-state\nLiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on\nhandheld packages used in unstructured natural environments. In this paper, we\npropose a robust LiDAR-inertial odometry system that does not rely on\nsensor-specific modeling. Sensor fusion techniques for LiDAR and inertial\nmeasurement unit (IMU) data typically integrate IMU data iteratively in a\nKalman filter or use pre-integration in a factor graph framework, combined with\nLiDAR scan matching often exploiting some form of feature extraction. We\npropose an alternative strategy that only requires a simplified motion model\nfor IMU integration and directly registers LiDAR scans in a scan-to-map\napproach. Our approach allows us to impose a novel regularization on the LiDAR\nregistration, improving the overall odometry performance. We detail extensive\nexperiments on a number of datasets covering a wide array of commonly used\nrobotic sensors and platforms. We show that our approach works with the exact\nsame configuration in all these scenarios, demonstrating its robustness. We\nhave open-sourced our implementation so that the community can build further on\nour work and use it in their navigation stacks.\n", "link": "http://arxiv.org/abs/2509.06593v1", "date": "2025-09-08", "relevancy": 2.4016, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6154}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6134}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robust%20Approach%20for%20LiDAR-Inertial%20Odometry%20Without%20Sensor-Specific%0A%20%20Modeling&body=Title%3A%20A%20Robust%20Approach%20for%20LiDAR-Inertial%20Odometry%20Without%20Sensor-Specific%0A%20%20Modeling%0AAuthor%3A%20Meher%20V.%20R.%20Malladi%20and%20Tiziano%20Guadagnino%20and%20Luca%20Lobefaro%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Accurate%20odometry%20is%20a%20critical%20component%20in%20a%20robotic%20navigation%20stack%2C%20and%0Asubsequent%20modules%20such%20as%20planning%20and%20control%20often%20rely%20on%20an%20estimate%20of%0Athe%20robot%27s%20motion.%20Sensor-based%20odometry%20approaches%20should%20be%20robust%20across%0Asensor%20types%20and%20deployable%20in%20different%20target%20domains%2C%20from%20solid-state%0ALiDARs%20mounted%20on%20cars%20in%20urban-driving%20scenarios%20to%20spinning%20LiDARs%20on%0Ahandheld%20packages%20used%20in%20unstructured%20natural%20environments.%20In%20this%20paper%2C%20we%0Apropose%20a%20robust%20LiDAR-inertial%20odometry%20system%20that%20does%20not%20rely%20on%0Asensor-specific%20modeling.%20Sensor%20fusion%20techniques%20for%20LiDAR%20and%20inertial%0Ameasurement%20unit%20%28IMU%29%20data%20typically%20integrate%20IMU%20data%20iteratively%20in%20a%0AKalman%20filter%20or%20use%20pre-integration%20in%20a%20factor%20graph%20framework%2C%20combined%20with%0ALiDAR%20scan%20matching%20often%20exploiting%20some%20form%20of%20feature%20extraction.%20We%0Apropose%20an%20alternative%20strategy%20that%20only%20requires%20a%20simplified%20motion%20model%0Afor%20IMU%20integration%20and%20directly%20registers%20LiDAR%20scans%20in%20a%20scan-to-map%0Aapproach.%20Our%20approach%20allows%20us%20to%20impose%20a%20novel%20regularization%20on%20the%20LiDAR%0Aregistration%2C%20improving%20the%20overall%20odometry%20performance.%20We%20detail%20extensive%0Aexperiments%20on%20a%20number%20of%20datasets%20covering%20a%20wide%20array%20of%20commonly%20used%0Arobotic%20sensors%20and%20platforms.%20We%20show%20that%20our%20approach%20works%20with%20the%20exact%0Asame%20configuration%20in%20all%20these%20scenarios%2C%20demonstrating%20its%20robustness.%20We%0Ahave%20open-sourced%20our%20implementation%20so%20that%20the%20community%20can%20build%20further%20on%0Aour%20work%20and%20use%20it%20in%20their%20navigation%20stacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robust%2520Approach%2520for%2520LiDAR-Inertial%2520Odometry%2520Without%2520Sensor-Specific%250A%2520%2520Modeling%26entry.906535625%3DMeher%2520V.%2520R.%2520Malladi%2520and%2520Tiziano%2520Guadagnino%2520and%2520Luca%2520Lobefaro%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520Accurate%2520odometry%2520is%2520a%2520critical%2520component%2520in%2520a%2520robotic%2520navigation%2520stack%252C%2520and%250Asubsequent%2520modules%2520such%2520as%2520planning%2520and%2520control%2520often%2520rely%2520on%2520an%2520estimate%2520of%250Athe%2520robot%2527s%2520motion.%2520Sensor-based%2520odometry%2520approaches%2520should%2520be%2520robust%2520across%250Asensor%2520types%2520and%2520deployable%2520in%2520different%2520target%2520domains%252C%2520from%2520solid-state%250ALiDARs%2520mounted%2520on%2520cars%2520in%2520urban-driving%2520scenarios%2520to%2520spinning%2520LiDARs%2520on%250Ahandheld%2520packages%2520used%2520in%2520unstructured%2520natural%2520environments.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520robust%2520LiDAR-inertial%2520odometry%2520system%2520that%2520does%2520not%2520rely%2520on%250Asensor-specific%2520modeling.%2520Sensor%2520fusion%2520techniques%2520for%2520LiDAR%2520and%2520inertial%250Ameasurement%2520unit%2520%2528IMU%2529%2520data%2520typically%2520integrate%2520IMU%2520data%2520iteratively%2520in%2520a%250AKalman%2520filter%2520or%2520use%2520pre-integration%2520in%2520a%2520factor%2520graph%2520framework%252C%2520combined%2520with%250ALiDAR%2520scan%2520matching%2520often%2520exploiting%2520some%2520form%2520of%2520feature%2520extraction.%2520We%250Apropose%2520an%2520alternative%2520strategy%2520that%2520only%2520requires%2520a%2520simplified%2520motion%2520model%250Afor%2520IMU%2520integration%2520and%2520directly%2520registers%2520LiDAR%2520scans%2520in%2520a%2520scan-to-map%250Aapproach.%2520Our%2520approach%2520allows%2520us%2520to%2520impose%2520a%2520novel%2520regularization%2520on%2520the%2520LiDAR%250Aregistration%252C%2520improving%2520the%2520overall%2520odometry%2520performance.%2520We%2520detail%2520extensive%250Aexperiments%2520on%2520a%2520number%2520of%2520datasets%2520covering%2520a%2520wide%2520array%2520of%2520commonly%2520used%250Arobotic%2520sensors%2520and%2520platforms.%2520We%2520show%2520that%2520our%2520approach%2520works%2520with%2520the%2520exact%250Asame%2520configuration%2520in%2520all%2520these%2520scenarios%252C%2520demonstrating%2520its%2520robustness.%2520We%250Ahave%2520open-sourced%2520our%2520implementation%2520so%2520that%2520the%2520community%2520can%2520build%2520further%2520on%250Aour%2520work%2520and%2520use%2520it%2520in%2520their%2520navigation%2520stacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robust%20Approach%20for%20LiDAR-Inertial%20Odometry%20Without%20Sensor-Specific%0A%20%20Modeling&entry.906535625=Meher%20V.%20R.%20Malladi%20and%20Tiziano%20Guadagnino%20and%20Luca%20Lobefaro%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Accurate%20odometry%20is%20a%20critical%20component%20in%20a%20robotic%20navigation%20stack%2C%20and%0Asubsequent%20modules%20such%20as%20planning%20and%20control%20often%20rely%20on%20an%20estimate%20of%0Athe%20robot%27s%20motion.%20Sensor-based%20odometry%20approaches%20should%20be%20robust%20across%0Asensor%20types%20and%20deployable%20in%20different%20target%20domains%2C%20from%20solid-state%0ALiDARs%20mounted%20on%20cars%20in%20urban-driving%20scenarios%20to%20spinning%20LiDARs%20on%0Ahandheld%20packages%20used%20in%20unstructured%20natural%20environments.%20In%20this%20paper%2C%20we%0Apropose%20a%20robust%20LiDAR-inertial%20odometry%20system%20that%20does%20not%20rely%20on%0Asensor-specific%20modeling.%20Sensor%20fusion%20techniques%20for%20LiDAR%20and%20inertial%0Ameasurement%20unit%20%28IMU%29%20data%20typically%20integrate%20IMU%20data%20iteratively%20in%20a%0AKalman%20filter%20or%20use%20pre-integration%20in%20a%20factor%20graph%20framework%2C%20combined%20with%0ALiDAR%20scan%20matching%20often%20exploiting%20some%20form%20of%20feature%20extraction.%20We%0Apropose%20an%20alternative%20strategy%20that%20only%20requires%20a%20simplified%20motion%20model%0Afor%20IMU%20integration%20and%20directly%20registers%20LiDAR%20scans%20in%20a%20scan-to-map%0Aapproach.%20Our%20approach%20allows%20us%20to%20impose%20a%20novel%20regularization%20on%20the%20LiDAR%0Aregistration%2C%20improving%20the%20overall%20odometry%20performance.%20We%20detail%20extensive%0Aexperiments%20on%20a%20number%20of%20datasets%20covering%20a%20wide%20array%20of%20commonly%20used%0Arobotic%20sensors%20and%20platforms.%20We%20show%20that%20our%20approach%20works%20with%20the%20exact%0Asame%20configuration%20in%20all%20these%20scenarios%2C%20demonstrating%20its%20robustness.%20We%0Ahave%20open-sourced%20our%20implementation%20so%20that%20the%20community%20can%20build%20further%20on%0Aour%20work%20and%20use%20it%20in%20their%20navigation%20stacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06593v1&entry.124074799=Read"},
{"title": "Scaling Transformer-Based Novel View Synthesis Models with Token\n  Disentanglement and Synthetic Data", "author": "Nithin Gopalakrishnan Nair and Srinivas Kaza and Xuan Luo and Vishal M. Patel and Stephen Lombardi and Jungyeon Park", "abstract": "  Large transformer-based models have made significant progress in\ngeneralizable novel view synthesis (NVS) from sparse input views, generating\nnovel viewpoints without the need for test-time optimization. However, these\nmodels are constrained by the limited diversity of publicly available scene\ndatasets, making most real-world (in-the-wild) scenes out-of-distribution. To\novercome this, we incorporate synthetic training data generated from diffusion\nmodels, which improves generalization across unseen domains. While synthetic\ndata offers scalability, we identify artifacts introduced during data\ngeneration as a key bottleneck affecting reconstruction quality. To address\nthis, we propose a token disentanglement process within the transformer\narchitecture, enhancing feature separation and ensuring more effective\nlearning. This refinement not only improves reconstruction quality over\nstandard transformers but also enables scalable training with synthetic data.\nAs a result, our method outperforms existing models on both in-dataset and\ncross-dataset evaluations, achieving state-of-the-art results across multiple\nbenchmarks while significantly reducing computational costs. Project page:\nhttps://scaling3dnvs.github.io/\n", "link": "http://arxiv.org/abs/2509.06950v1", "date": "2025-09-08", "relevancy": 2.3997, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6779}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5924}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Transformer-Based%20Novel%20View%20Synthesis%20Models%20with%20Token%0A%20%20Disentanglement%20and%20Synthetic%20Data&body=Title%3A%20Scaling%20Transformer-Based%20Novel%20View%20Synthesis%20Models%20with%20Token%0A%20%20Disentanglement%20and%20Synthetic%20Data%0AAuthor%3A%20Nithin%20Gopalakrishnan%20Nair%20and%20Srinivas%20Kaza%20and%20Xuan%20Luo%20and%20Vishal%20M.%20Patel%20and%20Stephen%20Lombardi%20and%20Jungyeon%20Park%0AAbstract%3A%20%20%20Large%20transformer-based%20models%20have%20made%20significant%20progress%20in%0Ageneralizable%20novel%20view%20synthesis%20%28NVS%29%20from%20sparse%20input%20views%2C%20generating%0Anovel%20viewpoints%20without%20the%20need%20for%20test-time%20optimization.%20However%2C%20these%0Amodels%20are%20constrained%20by%20the%20limited%20diversity%20of%20publicly%20available%20scene%0Adatasets%2C%20making%20most%20real-world%20%28in-the-wild%29%20scenes%20out-of-distribution.%20To%0Aovercome%20this%2C%20we%20incorporate%20synthetic%20training%20data%20generated%20from%20diffusion%0Amodels%2C%20which%20improves%20generalization%20across%20unseen%20domains.%20While%20synthetic%0Adata%20offers%20scalability%2C%20we%20identify%20artifacts%20introduced%20during%20data%0Ageneration%20as%20a%20key%20bottleneck%20affecting%20reconstruction%20quality.%20To%20address%0Athis%2C%20we%20propose%20a%20token%20disentanglement%20process%20within%20the%20transformer%0Aarchitecture%2C%20enhancing%20feature%20separation%20and%20ensuring%20more%20effective%0Alearning.%20This%20refinement%20not%20only%20improves%20reconstruction%20quality%20over%0Astandard%20transformers%20but%20also%20enables%20scalable%20training%20with%20synthetic%20data.%0AAs%20a%20result%2C%20our%20method%20outperforms%20existing%20models%20on%20both%20in-dataset%20and%0Across-dataset%20evaluations%2C%20achieving%20state-of-the-art%20results%20across%20multiple%0Abenchmarks%20while%20significantly%20reducing%20computational%20costs.%20Project%20page%3A%0Ahttps%3A//scaling3dnvs.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Transformer-Based%2520Novel%2520View%2520Synthesis%2520Models%2520with%2520Token%250A%2520%2520Disentanglement%2520and%2520Synthetic%2520Data%26entry.906535625%3DNithin%2520Gopalakrishnan%2520Nair%2520and%2520Srinivas%2520Kaza%2520and%2520Xuan%2520Luo%2520and%2520Vishal%2520M.%2520Patel%2520and%2520Stephen%2520Lombardi%2520and%2520Jungyeon%2520Park%26entry.1292438233%3D%2520%2520Large%2520transformer-based%2520models%2520have%2520made%2520significant%2520progress%2520in%250Ageneralizable%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520from%2520sparse%2520input%2520views%252C%2520generating%250Anovel%2520viewpoints%2520without%2520the%2520need%2520for%2520test-time%2520optimization.%2520However%252C%2520these%250Amodels%2520are%2520constrained%2520by%2520the%2520limited%2520diversity%2520of%2520publicly%2520available%2520scene%250Adatasets%252C%2520making%2520most%2520real-world%2520%2528in-the-wild%2529%2520scenes%2520out-of-distribution.%2520To%250Aovercome%2520this%252C%2520we%2520incorporate%2520synthetic%2520training%2520data%2520generated%2520from%2520diffusion%250Amodels%252C%2520which%2520improves%2520generalization%2520across%2520unseen%2520domains.%2520While%2520synthetic%250Adata%2520offers%2520scalability%252C%2520we%2520identify%2520artifacts%2520introduced%2520during%2520data%250Ageneration%2520as%2520a%2520key%2520bottleneck%2520affecting%2520reconstruction%2520quality.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520token%2520disentanglement%2520process%2520within%2520the%2520transformer%250Aarchitecture%252C%2520enhancing%2520feature%2520separation%2520and%2520ensuring%2520more%2520effective%250Alearning.%2520This%2520refinement%2520not%2520only%2520improves%2520reconstruction%2520quality%2520over%250Astandard%2520transformers%2520but%2520also%2520enables%2520scalable%2520training%2520with%2520synthetic%2520data.%250AAs%2520a%2520result%252C%2520our%2520method%2520outperforms%2520existing%2520models%2520on%2520both%2520in-dataset%2520and%250Across-dataset%2520evaluations%252C%2520achieving%2520state-of-the-art%2520results%2520across%2520multiple%250Abenchmarks%2520while%2520significantly%2520reducing%2520computational%2520costs.%2520Project%2520page%253A%250Ahttps%253A//scaling3dnvs.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Transformer-Based%20Novel%20View%20Synthesis%20Models%20with%20Token%0A%20%20Disentanglement%20and%20Synthetic%20Data&entry.906535625=Nithin%20Gopalakrishnan%20Nair%20and%20Srinivas%20Kaza%20and%20Xuan%20Luo%20and%20Vishal%20M.%20Patel%20and%20Stephen%20Lombardi%20and%20Jungyeon%20Park&entry.1292438233=%20%20Large%20transformer-based%20models%20have%20made%20significant%20progress%20in%0Ageneralizable%20novel%20view%20synthesis%20%28NVS%29%20from%20sparse%20input%20views%2C%20generating%0Anovel%20viewpoints%20without%20the%20need%20for%20test-time%20optimization.%20However%2C%20these%0Amodels%20are%20constrained%20by%20the%20limited%20diversity%20of%20publicly%20available%20scene%0Adatasets%2C%20making%20most%20real-world%20%28in-the-wild%29%20scenes%20out-of-distribution.%20To%0Aovercome%20this%2C%20we%20incorporate%20synthetic%20training%20data%20generated%20from%20diffusion%0Amodels%2C%20which%20improves%20generalization%20across%20unseen%20domains.%20While%20synthetic%0Adata%20offers%20scalability%2C%20we%20identify%20artifacts%20introduced%20during%20data%0Ageneration%20as%20a%20key%20bottleneck%20affecting%20reconstruction%20quality.%20To%20address%0Athis%2C%20we%20propose%20a%20token%20disentanglement%20process%20within%20the%20transformer%0Aarchitecture%2C%20enhancing%20feature%20separation%20and%20ensuring%20more%20effective%0Alearning.%20This%20refinement%20not%20only%20improves%20reconstruction%20quality%20over%0Astandard%20transformers%20but%20also%20enables%20scalable%20training%20with%20synthetic%20data.%0AAs%20a%20result%2C%20our%20method%20outperforms%20existing%20models%20on%20both%20in-dataset%20and%0Across-dataset%20evaluations%2C%20achieving%20state-of-the-art%20results%20across%20multiple%0Abenchmarks%20while%20significantly%20reducing%20computational%20costs.%20Project%20page%3A%0Ahttps%3A//scaling3dnvs.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06950v1&entry.124074799=Read"},
{"title": "A Survey of Generalization of Graph Anomaly Detection: From Transfer\n  Learning to Foundation Models", "author": "Junjun Pan and Yu Zheng and Yue Tan and Yixin Liu", "abstract": "  Graph anomaly detection (GAD) has attracted increasing attention in recent\nyears for identifying malicious samples in a wide range of graph-based\napplications, such as social media and e-commerce. However, most GAD methods\nassume identical training and testing distributions and are tailored to\nspecific tasks, resulting in limited adaptability to real-world scenarios such\nas shifting data distributions and scarce training samples in new applications.\nTo address the limitations, recent work has focused on improving the\ngeneralization capability of GAD models through transfer learning that\nleverages knowledge from related domains to enhance detection performance, or\ndeveloping \"one-for-all\" GAD foundation models that generalize across multiple\napplications. Since a systematic understanding of generalization in GAD is\nstill lacking, in this paper, we provide a comprehensive review of\ngeneralization in GAD. We first trace the evolution of generalization in GAD\nand formalize the problem settings, which further leads to our systematic\ntaxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive\nreview is conducted for the existing generalized GAD methods. Finally, we\nidentify current open challenges and suggest future directions to inspire\nfuture research in this emerging field.\n", "link": "http://arxiv.org/abs/2509.06609v1", "date": "2025-09-08", "relevancy": 2.3694, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4904}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4663}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Generalization%20of%20Graph%20Anomaly%20Detection%3A%20From%20Transfer%0A%20%20Learning%20to%20Foundation%20Models&body=Title%3A%20A%20Survey%20of%20Generalization%20of%20Graph%20Anomaly%20Detection%3A%20From%20Transfer%0A%20%20Learning%20to%20Foundation%20Models%0AAuthor%3A%20Junjun%20Pan%20and%20Yu%20Zheng%20and%20Yue%20Tan%20and%20Yixin%20Liu%0AAbstract%3A%20%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20attracted%20increasing%20attention%20in%20recent%0Ayears%20for%20identifying%20malicious%20samples%20in%20a%20wide%20range%20of%20graph-based%0Aapplications%2C%20such%20as%20social%20media%20and%20e-commerce.%20However%2C%20most%20GAD%20methods%0Aassume%20identical%20training%20and%20testing%20distributions%20and%20are%20tailored%20to%0Aspecific%20tasks%2C%20resulting%20in%20limited%20adaptability%20to%20real-world%20scenarios%20such%0Aas%20shifting%20data%20distributions%20and%20scarce%20training%20samples%20in%20new%20applications.%0ATo%20address%20the%20limitations%2C%20recent%20work%20has%20focused%20on%20improving%20the%0Ageneralization%20capability%20of%20GAD%20models%20through%20transfer%20learning%20that%0Aleverages%20knowledge%20from%20related%20domains%20to%20enhance%20detection%20performance%2C%20or%0Adeveloping%20%22one-for-all%22%20GAD%20foundation%20models%20that%20generalize%20across%20multiple%0Aapplications.%20Since%20a%20systematic%20understanding%20of%20generalization%20in%20GAD%20is%0Astill%20lacking%2C%20in%20this%20paper%2C%20we%20provide%20a%20comprehensive%20review%20of%0Ageneralization%20in%20GAD.%20We%20first%20trace%20the%20evolution%20of%20generalization%20in%20GAD%0Aand%20formalize%20the%20problem%20settings%2C%20which%20further%20leads%20to%20our%20systematic%0Ataxonomy.%20Rooted%20in%20this%20fine-grained%20taxonomy%2C%20an%20up-to-date%20and%20comprehensive%0Areview%20is%20conducted%20for%20the%20existing%20generalized%20GAD%20methods.%20Finally%2C%20we%0Aidentify%20current%20open%20challenges%20and%20suggest%20future%20directions%20to%20inspire%0Afuture%20research%20in%20this%20emerging%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Generalization%2520of%2520Graph%2520Anomaly%2520Detection%253A%2520From%2520Transfer%250A%2520%2520Learning%2520to%2520Foundation%2520Models%26entry.906535625%3DJunjun%2520Pan%2520and%2520Yu%2520Zheng%2520and%2520Yue%2520Tan%2520and%2520Yixin%2520Liu%26entry.1292438233%3D%2520%2520Graph%2520anomaly%2520detection%2520%2528GAD%2529%2520has%2520attracted%2520increasing%2520attention%2520in%2520recent%250Ayears%2520for%2520identifying%2520malicious%2520samples%2520in%2520a%2520wide%2520range%2520of%2520graph-based%250Aapplications%252C%2520such%2520as%2520social%2520media%2520and%2520e-commerce.%2520However%252C%2520most%2520GAD%2520methods%250Aassume%2520identical%2520training%2520and%2520testing%2520distributions%2520and%2520are%2520tailored%2520to%250Aspecific%2520tasks%252C%2520resulting%2520in%2520limited%2520adaptability%2520to%2520real-world%2520scenarios%2520such%250Aas%2520shifting%2520data%2520distributions%2520and%2520scarce%2520training%2520samples%2520in%2520new%2520applications.%250ATo%2520address%2520the%2520limitations%252C%2520recent%2520work%2520has%2520focused%2520on%2520improving%2520the%250Ageneralization%2520capability%2520of%2520GAD%2520models%2520through%2520transfer%2520learning%2520that%250Aleverages%2520knowledge%2520from%2520related%2520domains%2520to%2520enhance%2520detection%2520performance%252C%2520or%250Adeveloping%2520%2522one-for-all%2522%2520GAD%2520foundation%2520models%2520that%2520generalize%2520across%2520multiple%250Aapplications.%2520Since%2520a%2520systematic%2520understanding%2520of%2520generalization%2520in%2520GAD%2520is%250Astill%2520lacking%252C%2520in%2520this%2520paper%252C%2520we%2520provide%2520a%2520comprehensive%2520review%2520of%250Ageneralization%2520in%2520GAD.%2520We%2520first%2520trace%2520the%2520evolution%2520of%2520generalization%2520in%2520GAD%250Aand%2520formalize%2520the%2520problem%2520settings%252C%2520which%2520further%2520leads%2520to%2520our%2520systematic%250Ataxonomy.%2520Rooted%2520in%2520this%2520fine-grained%2520taxonomy%252C%2520an%2520up-to-date%2520and%2520comprehensive%250Areview%2520is%2520conducted%2520for%2520the%2520existing%2520generalized%2520GAD%2520methods.%2520Finally%252C%2520we%250Aidentify%2520current%2520open%2520challenges%2520and%2520suggest%2520future%2520directions%2520to%2520inspire%250Afuture%2520research%2520in%2520this%2520emerging%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Generalization%20of%20Graph%20Anomaly%20Detection%3A%20From%20Transfer%0A%20%20Learning%20to%20Foundation%20Models&entry.906535625=Junjun%20Pan%20and%20Yu%20Zheng%20and%20Yue%20Tan%20and%20Yixin%20Liu&entry.1292438233=%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20attracted%20increasing%20attention%20in%20recent%0Ayears%20for%20identifying%20malicious%20samples%20in%20a%20wide%20range%20of%20graph-based%0Aapplications%2C%20such%20as%20social%20media%20and%20e-commerce.%20However%2C%20most%20GAD%20methods%0Aassume%20identical%20training%20and%20testing%20distributions%20and%20are%20tailored%20to%0Aspecific%20tasks%2C%20resulting%20in%20limited%20adaptability%20to%20real-world%20scenarios%20such%0Aas%20shifting%20data%20distributions%20and%20scarce%20training%20samples%20in%20new%20applications.%0ATo%20address%20the%20limitations%2C%20recent%20work%20has%20focused%20on%20improving%20the%0Ageneralization%20capability%20of%20GAD%20models%20through%20transfer%20learning%20that%0Aleverages%20knowledge%20from%20related%20domains%20to%20enhance%20detection%20performance%2C%20or%0Adeveloping%20%22one-for-all%22%20GAD%20foundation%20models%20that%20generalize%20across%20multiple%0Aapplications.%20Since%20a%20systematic%20understanding%20of%20generalization%20in%20GAD%20is%0Astill%20lacking%2C%20in%20this%20paper%2C%20we%20provide%20a%20comprehensive%20review%20of%0Ageneralization%20in%20GAD.%20We%20first%20trace%20the%20evolution%20of%20generalization%20in%20GAD%0Aand%20formalize%20the%20problem%20settings%2C%20which%20further%20leads%20to%20our%20systematic%0Ataxonomy.%20Rooted%20in%20this%20fine-grained%20taxonomy%2C%20an%20up-to-date%20and%20comprehensive%0Areview%20is%20conducted%20for%20the%20existing%20generalized%20GAD%20methods.%20Finally%2C%20we%0Aidentify%20current%20open%20challenges%20and%20suggest%20future%20directions%20to%20inspire%0Afuture%20research%20in%20this%20emerging%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06609v1&entry.124074799=Read"},
{"title": "Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid\n  Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing\n  Clinical Practice", "author": "Hajar Moradmand and Lei Ren", "abstract": "  Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van\nDer Heijde Score (TSS) is crucial, but manual scoring is often time-consuming\nand subjective. This study introduces an Automated Radiographic Sharp Scoring\n(ARTSS) framework that leverages deep learning to analyze full-hand X-ray\nimages, aiming to reduce inter- and intra-observer variability. The research\nuniquely accommodates patients with joint disappearance and variable-length\nimage sequences. We developed ARTSS using data from 970 patients, structured\ninto four stages: I) Image pre-processing and re-orientation using ResNet50,\nII) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and\nIV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,\nEfficientNetB0, and Vision Transformer (ViT). We evaluated model performance\nwith Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute\nerror (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS\nfrom two radiologists was used as the ground truth. Model training employed\n3-fold cross-validation, with each fold consisting of 452 training and 227\nvalidation samples, and external testing included 291 unseen subjects. Our\njoint identification model achieved 99% accuracy. The best-performing model,\nViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results\ndemonstrate the potential of deep learning to automate RA scoring, which can\nsignificantly enhance clinical practice. Our approach addresses the challenge\nof joint disappearance and variable joint numbers, offers timesaving benefits,\nreduces inter- and intra-reader variability, improves radiologist accuracy, and\naids rheumatologists in making more informed decisions.\n", "link": "http://arxiv.org/abs/2509.06854v1", "date": "2025-09-08", "relevancy": 2.3587, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4774}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4754}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Radiographic%20Total%20Sharp%20Score%20%28ARTSS%29%20in%20Rheumatoid%0A%20%20Arthritis%3A%20A%20Solution%20to%20Reduce%20Inter-Intra%20Reader%20Variation%20and%20Enhancing%0A%20%20Clinical%20Practice&body=Title%3A%20Automated%20Radiographic%20Total%20Sharp%20Score%20%28ARTSS%29%20in%20Rheumatoid%0A%20%20Arthritis%3A%20A%20Solution%20to%20Reduce%20Inter-Intra%20Reader%20Variation%20and%20Enhancing%0A%20%20Clinical%20Practice%0AAuthor%3A%20Hajar%20Moradmand%20and%20Lei%20Ren%0AAbstract%3A%20%20%20Assessing%20the%20severity%20of%20rheumatoid%20arthritis%20%28RA%29%20using%20the%20Total%20Sharp/Van%0ADer%20Heijde%20Score%20%28TSS%29%20is%20crucial%2C%20but%20manual%20scoring%20is%20often%20time-consuming%0Aand%20subjective.%20This%20study%20introduces%20an%20Automated%20Radiographic%20Sharp%20Scoring%0A%28ARTSS%29%20framework%20that%20leverages%20deep%20learning%20to%20analyze%20full-hand%20X-ray%0Aimages%2C%20aiming%20to%20reduce%20inter-%20and%20intra-observer%20variability.%20The%20research%0Auniquely%20accommodates%20patients%20with%20joint%20disappearance%20and%20variable-length%0Aimage%20sequences.%20We%20developed%20ARTSS%20using%20data%20from%20970%20patients%2C%20structured%0Ainto%20four%20stages%3A%20I%29%20Image%20pre-processing%20and%20re-orientation%20using%20ResNet50%2C%0AII%29%20Hand%20segmentation%20using%20UNet.3%2C%20III%29%20Joint%20identification%20using%20YOLOv7%2C%20and%0AIV%29%20TSS%20prediction%20using%20models%20such%20as%20VGG16%2C%20VGG19%2C%20ResNet50%2C%20DenseNet201%2C%0AEfficientNetB0%2C%20and%20Vision%20Transformer%20%28ViT%29.%20We%20evaluated%20model%20performance%0Awith%20Intersection%20over%20Union%20%28IoU%29%2C%20Mean%20Average%20Precision%20%28MAP%29%2C%20mean%20absolute%0Aerror%20%28MAE%29%2C%20Root%20Mean%20Squared%20Error%20%28RMSE%29%2C%20and%20Huber%20loss.%20The%20average%20TSS%0Afrom%20two%20radiologists%20was%20used%20as%20the%20ground%20truth.%20Model%20training%20employed%0A3-fold%20cross-validation%2C%20with%20each%20fold%20consisting%20of%20452%20training%20and%20227%0Avalidation%20samples%2C%20and%20external%20testing%20included%20291%20unseen%20subjects.%20Our%0Ajoint%20identification%20model%20achieved%2099%25%20accuracy.%20The%20best-performing%20model%2C%0AViT%2C%20achieved%20a%20notably%20low%20Huber%20loss%20of%200.87%20for%20TSS%20prediction.%20Our%20results%0Ademonstrate%20the%20potential%20of%20deep%20learning%20to%20automate%20RA%20scoring%2C%20which%20can%0Asignificantly%20enhance%20clinical%20practice.%20Our%20approach%20addresses%20the%20challenge%0Aof%20joint%20disappearance%20and%20variable%20joint%20numbers%2C%20offers%20timesaving%20benefits%2C%0Areduces%20inter-%20and%20intra-reader%20variability%2C%20improves%20radiologist%20accuracy%2C%20and%0Aaids%20rheumatologists%20in%20making%20more%20informed%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Radiographic%2520Total%2520Sharp%2520Score%2520%2528ARTSS%2529%2520in%2520Rheumatoid%250A%2520%2520Arthritis%253A%2520A%2520Solution%2520to%2520Reduce%2520Inter-Intra%2520Reader%2520Variation%2520and%2520Enhancing%250A%2520%2520Clinical%2520Practice%26entry.906535625%3DHajar%2520Moradmand%2520and%2520Lei%2520Ren%26entry.1292438233%3D%2520%2520Assessing%2520the%2520severity%2520of%2520rheumatoid%2520arthritis%2520%2528RA%2529%2520using%2520the%2520Total%2520Sharp/Van%250ADer%2520Heijde%2520Score%2520%2528TSS%2529%2520is%2520crucial%252C%2520but%2520manual%2520scoring%2520is%2520often%2520time-consuming%250Aand%2520subjective.%2520This%2520study%2520introduces%2520an%2520Automated%2520Radiographic%2520Sharp%2520Scoring%250A%2528ARTSS%2529%2520framework%2520that%2520leverages%2520deep%2520learning%2520to%2520analyze%2520full-hand%2520X-ray%250Aimages%252C%2520aiming%2520to%2520reduce%2520inter-%2520and%2520intra-observer%2520variability.%2520The%2520research%250Auniquely%2520accommodates%2520patients%2520with%2520joint%2520disappearance%2520and%2520variable-length%250Aimage%2520sequences.%2520We%2520developed%2520ARTSS%2520using%2520data%2520from%2520970%2520patients%252C%2520structured%250Ainto%2520four%2520stages%253A%2520I%2529%2520Image%2520pre-processing%2520and%2520re-orientation%2520using%2520ResNet50%252C%250AII%2529%2520Hand%2520segmentation%2520using%2520UNet.3%252C%2520III%2529%2520Joint%2520identification%2520using%2520YOLOv7%252C%2520and%250AIV%2529%2520TSS%2520prediction%2520using%2520models%2520such%2520as%2520VGG16%252C%2520VGG19%252C%2520ResNet50%252C%2520DenseNet201%252C%250AEfficientNetB0%252C%2520and%2520Vision%2520Transformer%2520%2528ViT%2529.%2520We%2520evaluated%2520model%2520performance%250Awith%2520Intersection%2520over%2520Union%2520%2528IoU%2529%252C%2520Mean%2520Average%2520Precision%2520%2528MAP%2529%252C%2520mean%2520absolute%250Aerror%2520%2528MAE%2529%252C%2520Root%2520Mean%2520Squared%2520Error%2520%2528RMSE%2529%252C%2520and%2520Huber%2520loss.%2520The%2520average%2520TSS%250Afrom%2520two%2520radiologists%2520was%2520used%2520as%2520the%2520ground%2520truth.%2520Model%2520training%2520employed%250A3-fold%2520cross-validation%252C%2520with%2520each%2520fold%2520consisting%2520of%2520452%2520training%2520and%2520227%250Avalidation%2520samples%252C%2520and%2520external%2520testing%2520included%2520291%2520unseen%2520subjects.%2520Our%250Ajoint%2520identification%2520model%2520achieved%252099%2525%2520accuracy.%2520The%2520best-performing%2520model%252C%250AViT%252C%2520achieved%2520a%2520notably%2520low%2520Huber%2520loss%2520of%25200.87%2520for%2520TSS%2520prediction.%2520Our%2520results%250Ademonstrate%2520the%2520potential%2520of%2520deep%2520learning%2520to%2520automate%2520RA%2520scoring%252C%2520which%2520can%250Asignificantly%2520enhance%2520clinical%2520practice.%2520Our%2520approach%2520addresses%2520the%2520challenge%250Aof%2520joint%2520disappearance%2520and%2520variable%2520joint%2520numbers%252C%2520offers%2520timesaving%2520benefits%252C%250Areduces%2520inter-%2520and%2520intra-reader%2520variability%252C%2520improves%2520radiologist%2520accuracy%252C%2520and%250Aaids%2520rheumatologists%2520in%2520making%2520more%2520informed%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Radiographic%20Total%20Sharp%20Score%20%28ARTSS%29%20in%20Rheumatoid%0A%20%20Arthritis%3A%20A%20Solution%20to%20Reduce%20Inter-Intra%20Reader%20Variation%20and%20Enhancing%0A%20%20Clinical%20Practice&entry.906535625=Hajar%20Moradmand%20and%20Lei%20Ren&entry.1292438233=%20%20Assessing%20the%20severity%20of%20rheumatoid%20arthritis%20%28RA%29%20using%20the%20Total%20Sharp/Van%0ADer%20Heijde%20Score%20%28TSS%29%20is%20crucial%2C%20but%20manual%20scoring%20is%20often%20time-consuming%0Aand%20subjective.%20This%20study%20introduces%20an%20Automated%20Radiographic%20Sharp%20Scoring%0A%28ARTSS%29%20framework%20that%20leverages%20deep%20learning%20to%20analyze%20full-hand%20X-ray%0Aimages%2C%20aiming%20to%20reduce%20inter-%20and%20intra-observer%20variability.%20The%20research%0Auniquely%20accommodates%20patients%20with%20joint%20disappearance%20and%20variable-length%0Aimage%20sequences.%20We%20developed%20ARTSS%20using%20data%20from%20970%20patients%2C%20structured%0Ainto%20four%20stages%3A%20I%29%20Image%20pre-processing%20and%20re-orientation%20using%20ResNet50%2C%0AII%29%20Hand%20segmentation%20using%20UNet.3%2C%20III%29%20Joint%20identification%20using%20YOLOv7%2C%20and%0AIV%29%20TSS%20prediction%20using%20models%20such%20as%20VGG16%2C%20VGG19%2C%20ResNet50%2C%20DenseNet201%2C%0AEfficientNetB0%2C%20and%20Vision%20Transformer%20%28ViT%29.%20We%20evaluated%20model%20performance%0Awith%20Intersection%20over%20Union%20%28IoU%29%2C%20Mean%20Average%20Precision%20%28MAP%29%2C%20mean%20absolute%0Aerror%20%28MAE%29%2C%20Root%20Mean%20Squared%20Error%20%28RMSE%29%2C%20and%20Huber%20loss.%20The%20average%20TSS%0Afrom%20two%20radiologists%20was%20used%20as%20the%20ground%20truth.%20Model%20training%20employed%0A3-fold%20cross-validation%2C%20with%20each%20fold%20consisting%20of%20452%20training%20and%20227%0Avalidation%20samples%2C%20and%20external%20testing%20included%20291%20unseen%20subjects.%20Our%0Ajoint%20identification%20model%20achieved%2099%25%20accuracy.%20The%20best-performing%20model%2C%0AViT%2C%20achieved%20a%20notably%20low%20Huber%20loss%20of%200.87%20for%20TSS%20prediction.%20Our%20results%0Ademonstrate%20the%20potential%20of%20deep%20learning%20to%20automate%20RA%20scoring%2C%20which%20can%0Asignificantly%20enhance%20clinical%20practice.%20Our%20approach%20addresses%20the%20challenge%0Aof%20joint%20disappearance%20and%20variable%20joint%20numbers%2C%20offers%20timesaving%20benefits%2C%0Areduces%20inter-%20and%20intra-reader%20variability%2C%20improves%20radiologist%20accuracy%2C%20and%0Aaids%20rheumatologists%20in%20making%20more%20informed%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06854v1&entry.124074799=Read"},
{"title": "UMO: Scaling Multi-Identity Consistency for Image Customization via\n  Matching Reward", "author": "Yufeng Cheng and Wenxu Wu and Shaojin Wu and Mengqi Huang and Fei Ding and Qian He", "abstract": "  Recent advancements in image customization exhibit a wide range of\napplication prospects due to stronger customization capabilities. However,\nsince we humans are more sensitive to faces, a significant challenge remains in\npreserving consistent identity while avoiding identity confusion with\nmulti-reference images, limiting the identity scalability of customization\nmodels. To address this, we present UMO, a Unified Multi-identity Optimization\nframework, designed to maintain high-fidelity identity preservation and\nalleviate identity confusion with scalability. With \"multi-to-multi matching\"\nparadigm, UMO reformulates multi-identity generation as a global assignment\noptimization problem and unleashes multi-identity consistency for existing\nimage customization methods generally through reinforcement learning on\ndiffusion models. To facilitate the training of UMO, we develop a scalable\ncustomization dataset with multi-reference images, consisting of both\nsynthesised and real parts. Additionally, we propose a new metric to measure\nidentity confusion. Extensive experiments demonstrate that UMO not only\nimproves identity consistency significantly, but also reduces identity\nconfusion on several image customization methods, setting a new\nstate-of-the-art among open-source methods along the dimension of identity\npreserving. Code and model: https://github.com/bytedance/UMO\n", "link": "http://arxiv.org/abs/2509.06818v1", "date": "2025-09-08", "relevancy": 2.3508, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5917}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5905}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMO%3A%20Scaling%20Multi-Identity%20Consistency%20for%20Image%20Customization%20via%0A%20%20Matching%20Reward&body=Title%3A%20UMO%3A%20Scaling%20Multi-Identity%20Consistency%20for%20Image%20Customization%20via%0A%20%20Matching%20Reward%0AAuthor%3A%20Yufeng%20Cheng%20and%20Wenxu%20Wu%20and%20Shaojin%20Wu%20and%20Mengqi%20Huang%20and%20Fei%20Ding%20and%20Qian%20He%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image%20customization%20exhibit%20a%20wide%20range%20of%0Aapplication%20prospects%20due%20to%20stronger%20customization%20capabilities.%20However%2C%0Asince%20we%20humans%20are%20more%20sensitive%20to%20faces%2C%20a%20significant%20challenge%20remains%20in%0Apreserving%20consistent%20identity%20while%20avoiding%20identity%20confusion%20with%0Amulti-reference%20images%2C%20limiting%20the%20identity%20scalability%20of%20customization%0Amodels.%20To%20address%20this%2C%20we%20present%20UMO%2C%20a%20Unified%20Multi-identity%20Optimization%0Aframework%2C%20designed%20to%20maintain%20high-fidelity%20identity%20preservation%20and%0Aalleviate%20identity%20confusion%20with%20scalability.%20With%20%22multi-to-multi%20matching%22%0Aparadigm%2C%20UMO%20reformulates%20multi-identity%20generation%20as%20a%20global%20assignment%0Aoptimization%20problem%20and%20unleashes%20multi-identity%20consistency%20for%20existing%0Aimage%20customization%20methods%20generally%20through%20reinforcement%20learning%20on%0Adiffusion%20models.%20To%20facilitate%20the%20training%20of%20UMO%2C%20we%20develop%20a%20scalable%0Acustomization%20dataset%20with%20multi-reference%20images%2C%20consisting%20of%20both%0Asynthesised%20and%20real%20parts.%20Additionally%2C%20we%20propose%20a%20new%20metric%20to%20measure%0Aidentity%20confusion.%20Extensive%20experiments%20demonstrate%20that%20UMO%20not%20only%0Aimproves%20identity%20consistency%20significantly%2C%20but%20also%20reduces%20identity%0Aconfusion%20on%20several%20image%20customization%20methods%2C%20setting%20a%20new%0Astate-of-the-art%20among%20open-source%20methods%20along%20the%20dimension%20of%20identity%0Apreserving.%20Code%20and%20model%3A%20https%3A//github.com/bytedance/UMO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMO%253A%2520Scaling%2520Multi-Identity%2520Consistency%2520for%2520Image%2520Customization%2520via%250A%2520%2520Matching%2520Reward%26entry.906535625%3DYufeng%2520Cheng%2520and%2520Wenxu%2520Wu%2520and%2520Shaojin%2520Wu%2520and%2520Mengqi%2520Huang%2520and%2520Fei%2520Ding%2520and%2520Qian%2520He%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image%2520customization%2520exhibit%2520a%2520wide%2520range%2520of%250Aapplication%2520prospects%2520due%2520to%2520stronger%2520customization%2520capabilities.%2520However%252C%250Asince%2520we%2520humans%2520are%2520more%2520sensitive%2520to%2520faces%252C%2520a%2520significant%2520challenge%2520remains%2520in%250Apreserving%2520consistent%2520identity%2520while%2520avoiding%2520identity%2520confusion%2520with%250Amulti-reference%2520images%252C%2520limiting%2520the%2520identity%2520scalability%2520of%2520customization%250Amodels.%2520To%2520address%2520this%252C%2520we%2520present%2520UMO%252C%2520a%2520Unified%2520Multi-identity%2520Optimization%250Aframework%252C%2520designed%2520to%2520maintain%2520high-fidelity%2520identity%2520preservation%2520and%250Aalleviate%2520identity%2520confusion%2520with%2520scalability.%2520With%2520%2522multi-to-multi%2520matching%2522%250Aparadigm%252C%2520UMO%2520reformulates%2520multi-identity%2520generation%2520as%2520a%2520global%2520assignment%250Aoptimization%2520problem%2520and%2520unleashes%2520multi-identity%2520consistency%2520for%2520existing%250Aimage%2520customization%2520methods%2520generally%2520through%2520reinforcement%2520learning%2520on%250Adiffusion%2520models.%2520To%2520facilitate%2520the%2520training%2520of%2520UMO%252C%2520we%2520develop%2520a%2520scalable%250Acustomization%2520dataset%2520with%2520multi-reference%2520images%252C%2520consisting%2520of%2520both%250Asynthesised%2520and%2520real%2520parts.%2520Additionally%252C%2520we%2520propose%2520a%2520new%2520metric%2520to%2520measure%250Aidentity%2520confusion.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UMO%2520not%2520only%250Aimproves%2520identity%2520consistency%2520significantly%252C%2520but%2520also%2520reduces%2520identity%250Aconfusion%2520on%2520several%2520image%2520customization%2520methods%252C%2520setting%2520a%2520new%250Astate-of-the-art%2520among%2520open-source%2520methods%2520along%2520the%2520dimension%2520of%2520identity%250Apreserving.%2520Code%2520and%2520model%253A%2520https%253A//github.com/bytedance/UMO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMO%3A%20Scaling%20Multi-Identity%20Consistency%20for%20Image%20Customization%20via%0A%20%20Matching%20Reward&entry.906535625=Yufeng%20Cheng%20and%20Wenxu%20Wu%20and%20Shaojin%20Wu%20and%20Mengqi%20Huang%20and%20Fei%20Ding%20and%20Qian%20He&entry.1292438233=%20%20Recent%20advancements%20in%20image%20customization%20exhibit%20a%20wide%20range%20of%0Aapplication%20prospects%20due%20to%20stronger%20customization%20capabilities.%20However%2C%0Asince%20we%20humans%20are%20more%20sensitive%20to%20faces%2C%20a%20significant%20challenge%20remains%20in%0Apreserving%20consistent%20identity%20while%20avoiding%20identity%20confusion%20with%0Amulti-reference%20images%2C%20limiting%20the%20identity%20scalability%20of%20customization%0Amodels.%20To%20address%20this%2C%20we%20present%20UMO%2C%20a%20Unified%20Multi-identity%20Optimization%0Aframework%2C%20designed%20to%20maintain%20high-fidelity%20identity%20preservation%20and%0Aalleviate%20identity%20confusion%20with%20scalability.%20With%20%22multi-to-multi%20matching%22%0Aparadigm%2C%20UMO%20reformulates%20multi-identity%20generation%20as%20a%20global%20assignment%0Aoptimization%20problem%20and%20unleashes%20multi-identity%20consistency%20for%20existing%0Aimage%20customization%20methods%20generally%20through%20reinforcement%20learning%20on%0Adiffusion%20models.%20To%20facilitate%20the%20training%20of%20UMO%2C%20we%20develop%20a%20scalable%0Acustomization%20dataset%20with%20multi-reference%20images%2C%20consisting%20of%20both%0Asynthesised%20and%20real%20parts.%20Additionally%2C%20we%20propose%20a%20new%20metric%20to%20measure%0Aidentity%20confusion.%20Extensive%20experiments%20demonstrate%20that%20UMO%20not%20only%0Aimproves%20identity%20consistency%20significantly%2C%20but%20also%20reduces%20identity%0Aconfusion%20on%20several%20image%20customization%20methods%2C%20setting%20a%20new%0Astate-of-the-art%20among%20open-source%20methods%20along%20the%20dimension%20of%20identity%0Apreserving.%20Code%20and%20model%3A%20https%3A//github.com/bytedance/UMO%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06818v1&entry.124074799=Read"},
{"title": "Evaluating the Impact of Adversarial Attacks on Traffic Sign\n  Classification using the LISA Dataset", "author": "Nabeyou Tadessa and Balaji Iyangar and Mashrur Chowdhury", "abstract": "  Adversarial attacks pose significant threats to machine learning models by\nintroducing carefully crafted perturbations that cause misclassification. While\nprior work has primarily focused on MNIST and similar datasets, this paper\ninvestigates the vulnerability of traffic sign classifiers using the LISA\nTraffic Sign dataset. We train a convolutional neural network to classify 47\ndifferent traffic signs and evaluate its robustness against Fast Gradient Sign\nMethod (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a\nsharp decline in classification accuracy as the perturbation magnitude\nincreases, highlighting the models susceptibility to adversarial examples. This\nstudy lays the groundwork for future exploration into defense mechanisms\ntailored for real-world traffic sign recognition systems.\n", "link": "http://arxiv.org/abs/2509.06835v1", "date": "2025-09-08", "relevancy": 2.3114, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4584}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Impact%20of%20Adversarial%20Attacks%20on%20Traffic%20Sign%0A%20%20Classification%20using%20the%20LISA%20Dataset&body=Title%3A%20Evaluating%20the%20Impact%20of%20Adversarial%20Attacks%20on%20Traffic%20Sign%0A%20%20Classification%20using%20the%20LISA%20Dataset%0AAuthor%3A%20Nabeyou%20Tadessa%20and%20Balaji%20Iyangar%20and%20Mashrur%20Chowdhury%0AAbstract%3A%20%20%20Adversarial%20attacks%20pose%20significant%20threats%20to%20machine%20learning%20models%20by%0Aintroducing%20carefully%20crafted%20perturbations%20that%20cause%20misclassification.%20While%0Aprior%20work%20has%20primarily%20focused%20on%20MNIST%20and%20similar%20datasets%2C%20this%20paper%0Ainvestigates%20the%20vulnerability%20of%20traffic%20sign%20classifiers%20using%20the%20LISA%0ATraffic%20Sign%20dataset.%20We%20train%20a%20convolutional%20neural%20network%20to%20classify%2047%0Adifferent%20traffic%20signs%20and%20evaluate%20its%20robustness%20against%20Fast%20Gradient%20Sign%0AMethod%20%28FGSM%29%20and%20Projected%20Gradient%20Descent%20%28PGD%29%20attacks.%20Our%20results%20show%20a%0Asharp%20decline%20in%20classification%20accuracy%20as%20the%20perturbation%20magnitude%0Aincreases%2C%20highlighting%20the%20models%20susceptibility%20to%20adversarial%20examples.%20This%0Astudy%20lays%20the%20groundwork%20for%20future%20exploration%20into%20defense%20mechanisms%0Atailored%20for%20real-world%20traffic%20sign%20recognition%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Impact%2520of%2520Adversarial%2520Attacks%2520on%2520Traffic%2520Sign%250A%2520%2520Classification%2520using%2520the%2520LISA%2520Dataset%26entry.906535625%3DNabeyou%2520Tadessa%2520and%2520Balaji%2520Iyangar%2520and%2520Mashrur%2520Chowdhury%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520pose%2520significant%2520threats%2520to%2520machine%2520learning%2520models%2520by%250Aintroducing%2520carefully%2520crafted%2520perturbations%2520that%2520cause%2520misclassification.%2520While%250Aprior%2520work%2520has%2520primarily%2520focused%2520on%2520MNIST%2520and%2520similar%2520datasets%252C%2520this%2520paper%250Ainvestigates%2520the%2520vulnerability%2520of%2520traffic%2520sign%2520classifiers%2520using%2520the%2520LISA%250ATraffic%2520Sign%2520dataset.%2520We%2520train%2520a%2520convolutional%2520neural%2520network%2520to%2520classify%252047%250Adifferent%2520traffic%2520signs%2520and%2520evaluate%2520its%2520robustness%2520against%2520Fast%2520Gradient%2520Sign%250AMethod%2520%2528FGSM%2529%2520and%2520Projected%2520Gradient%2520Descent%2520%2528PGD%2529%2520attacks.%2520Our%2520results%2520show%2520a%250Asharp%2520decline%2520in%2520classification%2520accuracy%2520as%2520the%2520perturbation%2520magnitude%250Aincreases%252C%2520highlighting%2520the%2520models%2520susceptibility%2520to%2520adversarial%2520examples.%2520This%250Astudy%2520lays%2520the%2520groundwork%2520for%2520future%2520exploration%2520into%2520defense%2520mechanisms%250Atailored%2520for%2520real-world%2520traffic%2520sign%2520recognition%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Impact%20of%20Adversarial%20Attacks%20on%20Traffic%20Sign%0A%20%20Classification%20using%20the%20LISA%20Dataset&entry.906535625=Nabeyou%20Tadessa%20and%20Balaji%20Iyangar%20and%20Mashrur%20Chowdhury&entry.1292438233=%20%20Adversarial%20attacks%20pose%20significant%20threats%20to%20machine%20learning%20models%20by%0Aintroducing%20carefully%20crafted%20perturbations%20that%20cause%20misclassification.%20While%0Aprior%20work%20has%20primarily%20focused%20on%20MNIST%20and%20similar%20datasets%2C%20this%20paper%0Ainvestigates%20the%20vulnerability%20of%20traffic%20sign%20classifiers%20using%20the%20LISA%0ATraffic%20Sign%20dataset.%20We%20train%20a%20convolutional%20neural%20network%20to%20classify%2047%0Adifferent%20traffic%20signs%20and%20evaluate%20its%20robustness%20against%20Fast%20Gradient%20Sign%0AMethod%20%28FGSM%29%20and%20Projected%20Gradient%20Descent%20%28PGD%29%20attacks.%20Our%20results%20show%20a%0Asharp%20decline%20in%20classification%20accuracy%20as%20the%20perturbation%20magnitude%0Aincreases%2C%20highlighting%20the%20models%20susceptibility%20to%20adversarial%20examples.%20This%0Astudy%20lays%20the%20groundwork%20for%20future%20exploration%20into%20defense%20mechanisms%0Atailored%20for%20real-world%20traffic%20sign%20recognition%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06835v1&entry.124074799=Read"},
{"title": "AnalysisGNN: Unified Music Analysis with Graph Neural Networks", "author": "Emmanouil Karystinaios and Johannes Hentschel and Markus Neuwirth and Gerhard Widmer", "abstract": "  Recent years have seen a boom in computational approaches to music analysis,\nyet each one is typically tailored to a specific analytical domain. In this\nwork, we introduce AnalysisGNN, a novel graph neural network framework that\nleverages a data-shuffling strategy with a custom weighted multi-task loss and\nlogit fusion between task-specific classifiers to integrate heterogeneously\nannotated symbolic datasets for comprehensive score analysis. We further\nintegrate a Non-Chord-Tone prediction module, which identifies and excludes\npassing and non-functional notes from all tasks, thereby improving the\nconsistency of label signals. Experimental evaluations demonstrate that\nAnalysisGNN achieves performance comparable to traditional static-dataset\napproaches, while showing increased resilience to domain shifts and annotation\ninconsistencies across multiple heterogeneous corpora.\n", "link": "http://arxiv.org/abs/2509.06654v1", "date": "2025-09-08", "relevancy": 2.2649, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4575}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4543}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnalysisGNN%3A%20Unified%20Music%20Analysis%20with%20Graph%20Neural%20Networks&body=Title%3A%20AnalysisGNN%3A%20Unified%20Music%20Analysis%20with%20Graph%20Neural%20Networks%0AAuthor%3A%20Emmanouil%20Karystinaios%20and%20Johannes%20Hentschel%20and%20Markus%20Neuwirth%20and%20Gerhard%20Widmer%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20a%20boom%20in%20computational%20approaches%20to%20music%20analysis%2C%0Ayet%20each%20one%20is%20typically%20tailored%20to%20a%20specific%20analytical%20domain.%20In%20this%0Awork%2C%20we%20introduce%20AnalysisGNN%2C%20a%20novel%20graph%20neural%20network%20framework%20that%0Aleverages%20a%20data-shuffling%20strategy%20with%20a%20custom%20weighted%20multi-task%20loss%20and%0Alogit%20fusion%20between%20task-specific%20classifiers%20to%20integrate%20heterogeneously%0Aannotated%20symbolic%20datasets%20for%20comprehensive%20score%20analysis.%20We%20further%0Aintegrate%20a%20Non-Chord-Tone%20prediction%20module%2C%20which%20identifies%20and%20excludes%0Apassing%20and%20non-functional%20notes%20from%20all%20tasks%2C%20thereby%20improving%20the%0Aconsistency%20of%20label%20signals.%20Experimental%20evaluations%20demonstrate%20that%0AAnalysisGNN%20achieves%20performance%20comparable%20to%20traditional%20static-dataset%0Aapproaches%2C%20while%20showing%20increased%20resilience%20to%20domain%20shifts%20and%20annotation%0Ainconsistencies%20across%20multiple%20heterogeneous%20corpora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysisGNN%253A%2520Unified%2520Music%2520Analysis%2520with%2520Graph%2520Neural%2520Networks%26entry.906535625%3DEmmanouil%2520Karystinaios%2520and%2520Johannes%2520Hentschel%2520and%2520Markus%2520Neuwirth%2520and%2520Gerhard%2520Widmer%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520a%2520boom%2520in%2520computational%2520approaches%2520to%2520music%2520analysis%252C%250Ayet%2520each%2520one%2520is%2520typically%2520tailored%2520to%2520a%2520specific%2520analytical%2520domain.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520AnalysisGNN%252C%2520a%2520novel%2520graph%2520neural%2520network%2520framework%2520that%250Aleverages%2520a%2520data-shuffling%2520strategy%2520with%2520a%2520custom%2520weighted%2520multi-task%2520loss%2520and%250Alogit%2520fusion%2520between%2520task-specific%2520classifiers%2520to%2520integrate%2520heterogeneously%250Aannotated%2520symbolic%2520datasets%2520for%2520comprehensive%2520score%2520analysis.%2520We%2520further%250Aintegrate%2520a%2520Non-Chord-Tone%2520prediction%2520module%252C%2520which%2520identifies%2520and%2520excludes%250Apassing%2520and%2520non-functional%2520notes%2520from%2520all%2520tasks%252C%2520thereby%2520improving%2520the%250Aconsistency%2520of%2520label%2520signals.%2520Experimental%2520evaluations%2520demonstrate%2520that%250AAnalysisGNN%2520achieves%2520performance%2520comparable%2520to%2520traditional%2520static-dataset%250Aapproaches%252C%2520while%2520showing%2520increased%2520resilience%2520to%2520domain%2520shifts%2520and%2520annotation%250Ainconsistencies%2520across%2520multiple%2520heterogeneous%2520corpora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnalysisGNN%3A%20Unified%20Music%20Analysis%20with%20Graph%20Neural%20Networks&entry.906535625=Emmanouil%20Karystinaios%20and%20Johannes%20Hentschel%20and%20Markus%20Neuwirth%20and%20Gerhard%20Widmer&entry.1292438233=%20%20Recent%20years%20have%20seen%20a%20boom%20in%20computational%20approaches%20to%20music%20analysis%2C%0Ayet%20each%20one%20is%20typically%20tailored%20to%20a%20specific%20analytical%20domain.%20In%20this%0Awork%2C%20we%20introduce%20AnalysisGNN%2C%20a%20novel%20graph%20neural%20network%20framework%20that%0Aleverages%20a%20data-shuffling%20strategy%20with%20a%20custom%20weighted%20multi-task%20loss%20and%0Alogit%20fusion%20between%20task-specific%20classifiers%20to%20integrate%20heterogeneously%0Aannotated%20symbolic%20datasets%20for%20comprehensive%20score%20analysis.%20We%20further%0Aintegrate%20a%20Non-Chord-Tone%20prediction%20module%2C%20which%20identifies%20and%20excludes%0Apassing%20and%20non-functional%20notes%20from%20all%20tasks%2C%20thereby%20improving%20the%0Aconsistency%20of%20label%20signals.%20Experimental%20evaluations%20demonstrate%20that%0AAnalysisGNN%20achieves%20performance%20comparable%20to%20traditional%20static-dataset%0Aapproaches%2C%20while%20showing%20increased%20resilience%20to%20domain%20shifts%20and%20annotation%0Ainconsistencies%20across%20multiple%20heterogeneous%20corpora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06654v1&entry.124074799=Read"},
{"title": "Interleaving Reasoning for Better Text-to-Image Generation", "author": "Wenxuan Huang and Shuang Chen and Zheyong Xie and Shaosheng Cao and Shixiang Tang and Yufan Shen and Qingyu Yin and Wenbo Hu and Xiaoman Wang and Yuntian Tang and Junbo Qiao and Yue Guo and Yao Hu and Zhenfei Yin and Philip Torr and Yu Cheng and Wanli Ouyang and Shaohui Lin", "abstract": "  Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .\n", "link": "http://arxiv.org/abs/2509.06945v1", "date": "2025-09-08", "relevancy": 2.2555, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5737}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5618}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interleaving%20Reasoning%20for%20Better%20Text-to-Image%20Generation&body=Title%3A%20Interleaving%20Reasoning%20for%20Better%20Text-to-Image%20Generation%0AAuthor%3A%20Wenxuan%20Huang%20and%20Shuang%20Chen%20and%20Zheyong%20Xie%20and%20Shaosheng%20Cao%20and%20Shixiang%20Tang%20and%20Yufan%20Shen%20and%20Qingyu%20Yin%20and%20Wenbo%20Hu%20and%20Xiaoman%20Wang%20and%20Yuntian%20Tang%20and%20Junbo%20Qiao%20and%20Yue%20Guo%20and%20Yao%20Hu%20and%20Zhenfei%20Yin%20and%20Philip%20Torr%20and%20Yu%20Cheng%20and%20Wanli%20Ouyang%20and%20Shaohui%20Lin%0AAbstract%3A%20%20%20Unified%20multimodal%20understanding%20and%20generation%20models%20recently%20have%20achieve%0Asignificant%20improvement%20in%20image%20generation%20capability%2C%20yet%20a%20large%20gap%20remains%0Ain%20instruction%20following%20and%20detail%20preservation%20compared%20to%20systems%20that%0Atightly%20couple%20comprehension%20with%20generation%20such%20as%20GPT-4o.%20Motivated%20by%0Arecent%20advances%20in%20interleaving%20reasoning%2C%20we%20explore%20whether%20such%20reasoning%0Acan%20further%20improve%20Text-to-Image%20%28T2I%29%20generation.%20We%20introduce%20Interleaving%0AReasoning%20Generation%20%28IRG%29%2C%20a%20framework%20that%20alternates%20between%20text-based%0Athinking%20and%20image%20synthesis%3A%20the%20model%20first%20produces%20a%20text-based%20thinking%20to%0Aguide%20an%20initial%20image%2C%20then%20reflects%20on%20the%20result%20to%20refine%20fine-grained%0Adetails%2C%20visual%20quality%2C%20and%20aesthetics%20while%20preserving%20semantics.%20To%20train%0AIRG%20effectively%2C%20we%20propose%20Interleaving%20Reasoning%20Generation%20Learning%20%28IRGL%29%2C%0Awhich%20targets%20two%20sub-goals%3A%20%281%29%20strengthening%20the%20initial%20think-and-generate%0Astage%20to%20establish%20core%20content%20and%20base%20quality%2C%20and%20%282%29%20enabling%20high-quality%0Atextual%20reflection%20and%20faithful%20implementation%20of%20those%20refinements%20in%20a%0Asubsequent%20image.%20We%20curate%20IRGL-300K%2C%20a%20dataset%20organized%20into%20six%20decomposed%0Alearning%20modes%20that%20jointly%20cover%20learning%20text-based%20thinking%2C%20and%20full%0Athinking-image%20trajectories.%20Starting%20from%20a%20unified%20foundation%20model%20that%0Anatively%20emits%20interleaved%20text-image%20outputs%2C%20our%20two-stage%20training%20first%0Abuilds%20robust%20thinking%20and%20reflection%2C%20then%20efficiently%20tunes%20the%20IRG%20pipeline%0Ain%20the%20full%20thinking-image%20trajectory%20data.%20Extensive%20experiments%20show%20SoTA%0Aperformance%2C%20yielding%20absolute%20gains%20of%205-10%20points%20on%20GenEval%2C%20WISE%2C%20TIIF%2C%0AGenAI-Bench%2C%20and%20OneIG-EN%2C%20alongside%20substantial%20improvements%20in%20visual%20quality%0Aand%20fine-grained%20fidelity.%20The%20code%2C%20model%20weights%20and%20datasets%20will%20be%0Areleased%20in%3A%20https%3A//github.com/Osilly/Interleaving-Reasoning-Generation%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterleaving%2520Reasoning%2520for%2520Better%2520Text-to-Image%2520Generation%26entry.906535625%3DWenxuan%2520Huang%2520and%2520Shuang%2520Chen%2520and%2520Zheyong%2520Xie%2520and%2520Shaosheng%2520Cao%2520and%2520Shixiang%2520Tang%2520and%2520Yufan%2520Shen%2520and%2520Qingyu%2520Yin%2520and%2520Wenbo%2520Hu%2520and%2520Xiaoman%2520Wang%2520and%2520Yuntian%2520Tang%2520and%2520Junbo%2520Qiao%2520and%2520Yue%2520Guo%2520and%2520Yao%2520Hu%2520and%2520Zhenfei%2520Yin%2520and%2520Philip%2520Torr%2520and%2520Yu%2520Cheng%2520and%2520Wanli%2520Ouyang%2520and%2520Shaohui%2520Lin%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520understanding%2520and%2520generation%2520models%2520recently%2520have%2520achieve%250Asignificant%2520improvement%2520in%2520image%2520generation%2520capability%252C%2520yet%2520a%2520large%2520gap%2520remains%250Ain%2520instruction%2520following%2520and%2520detail%2520preservation%2520compared%2520to%2520systems%2520that%250Atightly%2520couple%2520comprehension%2520with%2520generation%2520such%2520as%2520GPT-4o.%2520Motivated%2520by%250Arecent%2520advances%2520in%2520interleaving%2520reasoning%252C%2520we%2520explore%2520whether%2520such%2520reasoning%250Acan%2520further%2520improve%2520Text-to-Image%2520%2528T2I%2529%2520generation.%2520We%2520introduce%2520Interleaving%250AReasoning%2520Generation%2520%2528IRG%2529%252C%2520a%2520framework%2520that%2520alternates%2520between%2520text-based%250Athinking%2520and%2520image%2520synthesis%253A%2520the%2520model%2520first%2520produces%2520a%2520text-based%2520thinking%2520to%250Aguide%2520an%2520initial%2520image%252C%2520then%2520reflects%2520on%2520the%2520result%2520to%2520refine%2520fine-grained%250Adetails%252C%2520visual%2520quality%252C%2520and%2520aesthetics%2520while%2520preserving%2520semantics.%2520To%2520train%250AIRG%2520effectively%252C%2520we%2520propose%2520Interleaving%2520Reasoning%2520Generation%2520Learning%2520%2528IRGL%2529%252C%250Awhich%2520targets%2520two%2520sub-goals%253A%2520%25281%2529%2520strengthening%2520the%2520initial%2520think-and-generate%250Astage%2520to%2520establish%2520core%2520content%2520and%2520base%2520quality%252C%2520and%2520%25282%2529%2520enabling%2520high-quality%250Atextual%2520reflection%2520and%2520faithful%2520implementation%2520of%2520those%2520refinements%2520in%2520a%250Asubsequent%2520image.%2520We%2520curate%2520IRGL-300K%252C%2520a%2520dataset%2520organized%2520into%2520six%2520decomposed%250Alearning%2520modes%2520that%2520jointly%2520cover%2520learning%2520text-based%2520thinking%252C%2520and%2520full%250Athinking-image%2520trajectories.%2520Starting%2520from%2520a%2520unified%2520foundation%2520model%2520that%250Anatively%2520emits%2520interleaved%2520text-image%2520outputs%252C%2520our%2520two-stage%2520training%2520first%250Abuilds%2520robust%2520thinking%2520and%2520reflection%252C%2520then%2520efficiently%2520tunes%2520the%2520IRG%2520pipeline%250Ain%2520the%2520full%2520thinking-image%2520trajectory%2520data.%2520Extensive%2520experiments%2520show%2520SoTA%250Aperformance%252C%2520yielding%2520absolute%2520gains%2520of%25205-10%2520points%2520on%2520GenEval%252C%2520WISE%252C%2520TIIF%252C%250AGenAI-Bench%252C%2520and%2520OneIG-EN%252C%2520alongside%2520substantial%2520improvements%2520in%2520visual%2520quality%250Aand%2520fine-grained%2520fidelity.%2520The%2520code%252C%2520model%2520weights%2520and%2520datasets%2520will%2520be%250Areleased%2520in%253A%2520https%253A//github.com/Osilly/Interleaving-Reasoning-Generation%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interleaving%20Reasoning%20for%20Better%20Text-to-Image%20Generation&entry.906535625=Wenxuan%20Huang%20and%20Shuang%20Chen%20and%20Zheyong%20Xie%20and%20Shaosheng%20Cao%20and%20Shixiang%20Tang%20and%20Yufan%20Shen%20and%20Qingyu%20Yin%20and%20Wenbo%20Hu%20and%20Xiaoman%20Wang%20and%20Yuntian%20Tang%20and%20Junbo%20Qiao%20and%20Yue%20Guo%20and%20Yao%20Hu%20and%20Zhenfei%20Yin%20and%20Philip%20Torr%20and%20Yu%20Cheng%20and%20Wanli%20Ouyang%20and%20Shaohui%20Lin&entry.1292438233=%20%20Unified%20multimodal%20understanding%20and%20generation%20models%20recently%20have%20achieve%0Asignificant%20improvement%20in%20image%20generation%20capability%2C%20yet%20a%20large%20gap%20remains%0Ain%20instruction%20following%20and%20detail%20preservation%20compared%20to%20systems%20that%0Atightly%20couple%20comprehension%20with%20generation%20such%20as%20GPT-4o.%20Motivated%20by%0Arecent%20advances%20in%20interleaving%20reasoning%2C%20we%20explore%20whether%20such%20reasoning%0Acan%20further%20improve%20Text-to-Image%20%28T2I%29%20generation.%20We%20introduce%20Interleaving%0AReasoning%20Generation%20%28IRG%29%2C%20a%20framework%20that%20alternates%20between%20text-based%0Athinking%20and%20image%20synthesis%3A%20the%20model%20first%20produces%20a%20text-based%20thinking%20to%0Aguide%20an%20initial%20image%2C%20then%20reflects%20on%20the%20result%20to%20refine%20fine-grained%0Adetails%2C%20visual%20quality%2C%20and%20aesthetics%20while%20preserving%20semantics.%20To%20train%0AIRG%20effectively%2C%20we%20propose%20Interleaving%20Reasoning%20Generation%20Learning%20%28IRGL%29%2C%0Awhich%20targets%20two%20sub-goals%3A%20%281%29%20strengthening%20the%20initial%20think-and-generate%0Astage%20to%20establish%20core%20content%20and%20base%20quality%2C%20and%20%282%29%20enabling%20high-quality%0Atextual%20reflection%20and%20faithful%20implementation%20of%20those%20refinements%20in%20a%0Asubsequent%20image.%20We%20curate%20IRGL-300K%2C%20a%20dataset%20organized%20into%20six%20decomposed%0Alearning%20modes%20that%20jointly%20cover%20learning%20text-based%20thinking%2C%20and%20full%0Athinking-image%20trajectories.%20Starting%20from%20a%20unified%20foundation%20model%20that%0Anatively%20emits%20interleaved%20text-image%20outputs%2C%20our%20two-stage%20training%20first%0Abuilds%20robust%20thinking%20and%20reflection%2C%20then%20efficiently%20tunes%20the%20IRG%20pipeline%0Ain%20the%20full%20thinking-image%20trajectory%20data.%20Extensive%20experiments%20show%20SoTA%0Aperformance%2C%20yielding%20absolute%20gains%20of%205-10%20points%20on%20GenEval%2C%20WISE%2C%20TIIF%2C%0AGenAI-Bench%2C%20and%20OneIG-EN%2C%20alongside%20substantial%20improvements%20in%20visual%20quality%0Aand%20fine-grained%20fidelity.%20The%20code%2C%20model%20weights%20and%20datasets%20will%20be%0Areleased%20in%3A%20https%3A//github.com/Osilly/Interleaving-Reasoning-Generation%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06945v1&entry.124074799=Read"},
{"title": "The First Voice Timbre Attribute Detection Challenge", "author": "Liping Chen and Jinghao He and Zhengyan Sheng and Kong Aik Lee and Zhen-Hua Ling", "abstract": "  The first voice timbre attribute detection challenge is featured in a special\nsession at NCMMSC 2025. It focuses on the explainability of voice timbre and\ncompares the intensity of two speech utterances in a specified timbre\ndescriptor dimension. The evaluation was conducted on the VCTK-RVA dataset.\nParticipants developed their systems and submitted their outputs to the\norganizer, who evaluated the performance and sent feedback to them. Six teams\nsubmitted their outputs, with five providing descriptions of their\nmethodologies.\n", "link": "http://arxiv.org/abs/2509.06635v1", "date": "2025-09-08", "relevancy": 2.2518, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20First%20Voice%20Timbre%20Attribute%20Detection%20Challenge&body=Title%3A%20The%20First%20Voice%20Timbre%20Attribute%20Detection%20Challenge%0AAuthor%3A%20Liping%20Chen%20and%20Jinghao%20He%20and%20Zhengyan%20Sheng%20and%20Kong%20Aik%20Lee%20and%20Zhen-Hua%20Ling%0AAbstract%3A%20%20%20The%20first%20voice%20timbre%20attribute%20detection%20challenge%20is%20featured%20in%20a%20special%0Asession%20at%20NCMMSC%202025.%20It%20focuses%20on%20the%20explainability%20of%20voice%20timbre%20and%0Acompares%20the%20intensity%20of%20two%20speech%20utterances%20in%20a%20specified%20timbre%0Adescriptor%20dimension.%20The%20evaluation%20was%20conducted%20on%20the%20VCTK-RVA%20dataset.%0AParticipants%20developed%20their%20systems%20and%20submitted%20their%20outputs%20to%20the%0Aorganizer%2C%20who%20evaluated%20the%20performance%20and%20sent%20feedback%20to%20them.%20Six%20teams%0Asubmitted%20their%20outputs%2C%20with%20five%20providing%20descriptions%20of%20their%0Amethodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520First%2520Voice%2520Timbre%2520Attribute%2520Detection%2520Challenge%26entry.906535625%3DLiping%2520Chen%2520and%2520Jinghao%2520He%2520and%2520Zhengyan%2520Sheng%2520and%2520Kong%2520Aik%2520Lee%2520and%2520Zhen-Hua%2520Ling%26entry.1292438233%3D%2520%2520The%2520first%2520voice%2520timbre%2520attribute%2520detection%2520challenge%2520is%2520featured%2520in%2520a%2520special%250Asession%2520at%2520NCMMSC%25202025.%2520It%2520focuses%2520on%2520the%2520explainability%2520of%2520voice%2520timbre%2520and%250Acompares%2520the%2520intensity%2520of%2520two%2520speech%2520utterances%2520in%2520a%2520specified%2520timbre%250Adescriptor%2520dimension.%2520The%2520evaluation%2520was%2520conducted%2520on%2520the%2520VCTK-RVA%2520dataset.%250AParticipants%2520developed%2520their%2520systems%2520and%2520submitted%2520their%2520outputs%2520to%2520the%250Aorganizer%252C%2520who%2520evaluated%2520the%2520performance%2520and%2520sent%2520feedback%2520to%2520them.%2520Six%2520teams%250Asubmitted%2520their%2520outputs%252C%2520with%2520five%2520providing%2520descriptions%2520of%2520their%250Amethodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20First%20Voice%20Timbre%20Attribute%20Detection%20Challenge&entry.906535625=Liping%20Chen%20and%20Jinghao%20He%20and%20Zhengyan%20Sheng%20and%20Kong%20Aik%20Lee%20and%20Zhen-Hua%20Ling&entry.1292438233=%20%20The%20first%20voice%20timbre%20attribute%20detection%20challenge%20is%20featured%20in%20a%20special%0Asession%20at%20NCMMSC%202025.%20It%20focuses%20on%20the%20explainability%20of%20voice%20timbre%20and%0Acompares%20the%20intensity%20of%20two%20speech%20utterances%20in%20a%20specified%20timbre%0Adescriptor%20dimension.%20The%20evaluation%20was%20conducted%20on%20the%20VCTK-RVA%20dataset.%0AParticipants%20developed%20their%20systems%20and%20submitted%20their%20outputs%20to%20the%0Aorganizer%2C%20who%20evaluated%20the%20performance%20and%20sent%20feedback%20to%20them.%20Six%20teams%0Asubmitted%20their%20outputs%2C%20with%20five%20providing%20descriptions%20of%20their%0Amethodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06635v1&entry.124074799=Read"},
{"title": "Active Illumination for Visual Ego-Motion Estimation in the Dark", "author": "Francesco Crocetti and Alberto Dionigi and Raffaele Brilli and Gabriele Costante and Paolo Valigi", "abstract": "  Visual Odometry (VO) and Visual SLAM (V-SLAM) systems often struggle in\nlow-light and dark environments due to the lack of robust visual features. In\nthis paper, we propose a novel active illumination framework to enhance the\nperformance of VO and V-SLAM algorithms in these challenging conditions. The\ndeveloped approach dynamically controls a moving light source to illuminate\nhighly textured areas, thereby improving feature extraction and tracking.\nSpecifically, a detector block, which incorporates a deep learning-based\nenhancing network, identifies regions with relevant features. Then, a pan-tilt\ncontroller is responsible for guiding the light beam toward these areas, so\nthat to provide information-rich images to the ego-motion estimation algorithm.\nExperimental results on a real robotic platform demonstrate the effectiveness\nof the proposed method, showing a reduction in the pose estimation error up to\n75% with respect to a traditional fixed lighting technique.\n", "link": "http://arxiv.org/abs/2502.13708v2", "date": "2025-09-08", "relevancy": 2.2504, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5746}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Illumination%20for%20Visual%20Ego-Motion%20Estimation%20in%20the%20Dark&body=Title%3A%20Active%20Illumination%20for%20Visual%20Ego-Motion%20Estimation%20in%20the%20Dark%0AAuthor%3A%20Francesco%20Crocetti%20and%20Alberto%20Dionigi%20and%20Raffaele%20Brilli%20and%20Gabriele%20Costante%20and%20Paolo%20Valigi%0AAbstract%3A%20%20%20Visual%20Odometry%20%28VO%29%20and%20Visual%20SLAM%20%28V-SLAM%29%20systems%20often%20struggle%20in%0Alow-light%20and%20dark%20environments%20due%20to%20the%20lack%20of%20robust%20visual%20features.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20active%20illumination%20framework%20to%20enhance%20the%0Aperformance%20of%20VO%20and%20V-SLAM%20algorithms%20in%20these%20challenging%20conditions.%20The%0Adeveloped%20approach%20dynamically%20controls%20a%20moving%20light%20source%20to%20illuminate%0Ahighly%20textured%20areas%2C%20thereby%20improving%20feature%20extraction%20and%20tracking.%0ASpecifically%2C%20a%20detector%20block%2C%20which%20incorporates%20a%20deep%20learning-based%0Aenhancing%20network%2C%20identifies%20regions%20with%20relevant%20features.%20Then%2C%20a%20pan-tilt%0Acontroller%20is%20responsible%20for%20guiding%20the%20light%20beam%20toward%20these%20areas%2C%20so%0Athat%20to%20provide%20information-rich%20images%20to%20the%20ego-motion%20estimation%20algorithm.%0AExperimental%20results%20on%20a%20real%20robotic%20platform%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20method%2C%20showing%20a%20reduction%20in%20the%20pose%20estimation%20error%20up%20to%0A75%25%20with%20respect%20to%20a%20traditional%20fixed%20lighting%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Illumination%2520for%2520Visual%2520Ego-Motion%2520Estimation%2520in%2520the%2520Dark%26entry.906535625%3DFrancesco%2520Crocetti%2520and%2520Alberto%2520Dionigi%2520and%2520Raffaele%2520Brilli%2520and%2520Gabriele%2520Costante%2520and%2520Paolo%2520Valigi%26entry.1292438233%3D%2520%2520Visual%2520Odometry%2520%2528VO%2529%2520and%2520Visual%2520SLAM%2520%2528V-SLAM%2529%2520systems%2520often%2520struggle%2520in%250Alow-light%2520and%2520dark%2520environments%2520due%2520to%2520the%2520lack%2520of%2520robust%2520visual%2520features.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520active%2520illumination%2520framework%2520to%2520enhance%2520the%250Aperformance%2520of%2520VO%2520and%2520V-SLAM%2520algorithms%2520in%2520these%2520challenging%2520conditions.%2520The%250Adeveloped%2520approach%2520dynamically%2520controls%2520a%2520moving%2520light%2520source%2520to%2520illuminate%250Ahighly%2520textured%2520areas%252C%2520thereby%2520improving%2520feature%2520extraction%2520and%2520tracking.%250ASpecifically%252C%2520a%2520detector%2520block%252C%2520which%2520incorporates%2520a%2520deep%2520learning-based%250Aenhancing%2520network%252C%2520identifies%2520regions%2520with%2520relevant%2520features.%2520Then%252C%2520a%2520pan-tilt%250Acontroller%2520is%2520responsible%2520for%2520guiding%2520the%2520light%2520beam%2520toward%2520these%2520areas%252C%2520so%250Athat%2520to%2520provide%2520information-rich%2520images%2520to%2520the%2520ego-motion%2520estimation%2520algorithm.%250AExperimental%2520results%2520on%2520a%2520real%2520robotic%2520platform%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520method%252C%2520showing%2520a%2520reduction%2520in%2520the%2520pose%2520estimation%2520error%2520up%2520to%250A75%2525%2520with%2520respect%2520to%2520a%2520traditional%2520fixed%2520lighting%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Illumination%20for%20Visual%20Ego-Motion%20Estimation%20in%20the%20Dark&entry.906535625=Francesco%20Crocetti%20and%20Alberto%20Dionigi%20and%20Raffaele%20Brilli%20and%20Gabriele%20Costante%20and%20Paolo%20Valigi&entry.1292438233=%20%20Visual%20Odometry%20%28VO%29%20and%20Visual%20SLAM%20%28V-SLAM%29%20systems%20often%20struggle%20in%0Alow-light%20and%20dark%20environments%20due%20to%20the%20lack%20of%20robust%20visual%20features.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20active%20illumination%20framework%20to%20enhance%20the%0Aperformance%20of%20VO%20and%20V-SLAM%20algorithms%20in%20these%20challenging%20conditions.%20The%0Adeveloped%20approach%20dynamically%20controls%20a%20moving%20light%20source%20to%20illuminate%0Ahighly%20textured%20areas%2C%20thereby%20improving%20feature%20extraction%20and%20tracking.%0ASpecifically%2C%20a%20detector%20block%2C%20which%20incorporates%20a%20deep%20learning-based%0Aenhancing%20network%2C%20identifies%20regions%20with%20relevant%20features.%20Then%2C%20a%20pan-tilt%0Acontroller%20is%20responsible%20for%20guiding%20the%20light%20beam%20toward%20these%20areas%2C%20so%0Athat%20to%20provide%20information-rich%20images%20to%20the%20ego-motion%20estimation%20algorithm.%0AExperimental%20results%20on%20a%20real%20robotic%20platform%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20method%2C%20showing%20a%20reduction%20in%20the%20pose%20estimation%20error%20up%20to%0A75%25%20with%20respect%20to%20a%20traditional%20fixed%20lighting%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13708v2&entry.124074799=Read"},
{"title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "author": "Hao-Shu Fang and Branden Romero and Yichen Xie and Arthur Hu and Bo-Ruei Huang and Juan Alvarez and Matthew Kim and Gabriel Margolis and Kavya Anbarasu and Masayoshi Tomizuka and Edward Adelson and Pulkit Agrawal", "abstract": "  We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.\n", "link": "http://arxiv.org/abs/2509.04441v2", "date": "2025-09-08", "relevancy": 2.2107, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5788}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5739}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEXOP%3A%20A%20Device%20for%20Robotic%20Transfer%20of%20Dexterous%20Human%20Manipulation&body=Title%3A%20DEXOP%3A%20A%20Device%20for%20Robotic%20Transfer%20of%20Dexterous%20Human%20Manipulation%0AAuthor%3A%20Hao-Shu%20Fang%20and%20Branden%20Romero%20and%20Yichen%20Xie%20and%20Arthur%20Hu%20and%20Bo-Ruei%20Huang%20and%20Juan%20Alvarez%20and%20Matthew%20Kim%20and%20Gabriel%20Margolis%20and%20Kavya%20Anbarasu%20and%20Masayoshi%20Tomizuka%20and%20Edward%20Adelson%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20We%20introduce%20perioperation%2C%20a%20paradigm%20for%20robotic%20data%20collection%20that%0Asensorizes%20and%20records%20human%20manipulation%20while%20maximizing%20the%20transferability%0Aof%20the%20data%20to%20real%20robots.%20We%20implement%20this%20paradigm%20in%20DEXOP%2C%20a%20passive%20hand%0Aexoskeleton%20designed%20to%20maximize%20human%20ability%20to%20collect%20rich%20sensory%20%28vision%0A%2B%20tactile%29%20data%20for%20diverse%20dexterous%20manipulation%20tasks%20in%20natural%0Aenvironments.%20DEXOP%20mechanically%20connects%20human%20fingers%20to%20robot%20fingers%2C%0Aproviding%20users%20with%20direct%20contact%20feedback%20%28via%20proprioception%29%20and%20mirrors%0Athe%20human%20hand%20pose%20to%20the%20passive%20robot%20hand%20to%20maximize%20the%20transfer%20of%0Ademonstrated%20skills%20to%20the%20robot.%20The%20force%20feedback%20and%20pose%20mirroring%20make%0Atask%20demonstrations%20more%20natural%20for%20humans%20compared%20to%20teleoperation%2C%0Aincreasing%20both%20speed%20and%20accuracy.%20We%20evaluate%20DEXOP%20across%20a%20range%20of%0Adexterous%2C%20contact-rich%20tasks%2C%20demonstrating%20its%20ability%20to%20collect%0Ahigh-quality%20demonstration%20data%20at%20scale.%20Policies%20learned%20with%20DEXOP%20data%0Asignificantly%20improve%20task%20performance%20per%20unit%20time%20of%20data%20collection%0Acompared%20to%20teleoperation%2C%20making%20DEXOP%20a%20powerful%20tool%20for%20advancing%20robot%0Adexterity.%20Our%20project%20page%20is%20at%20https%3A//dex-op.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEXOP%253A%2520A%2520Device%2520for%2520Robotic%2520Transfer%2520of%2520Dexterous%2520Human%2520Manipulation%26entry.906535625%3DHao-Shu%2520Fang%2520and%2520Branden%2520Romero%2520and%2520Yichen%2520Xie%2520and%2520Arthur%2520Hu%2520and%2520Bo-Ruei%2520Huang%2520and%2520Juan%2520Alvarez%2520and%2520Matthew%2520Kim%2520and%2520Gabriel%2520Margolis%2520and%2520Kavya%2520Anbarasu%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Edward%2520Adelson%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520We%2520introduce%2520perioperation%252C%2520a%2520paradigm%2520for%2520robotic%2520data%2520collection%2520that%250Asensorizes%2520and%2520records%2520human%2520manipulation%2520while%2520maximizing%2520the%2520transferability%250Aof%2520the%2520data%2520to%2520real%2520robots.%2520We%2520implement%2520this%2520paradigm%2520in%2520DEXOP%252C%2520a%2520passive%2520hand%250Aexoskeleton%2520designed%2520to%2520maximize%2520human%2520ability%2520to%2520collect%2520rich%2520sensory%2520%2528vision%250A%252B%2520tactile%2529%2520data%2520for%2520diverse%2520dexterous%2520manipulation%2520tasks%2520in%2520natural%250Aenvironments.%2520DEXOP%2520mechanically%2520connects%2520human%2520fingers%2520to%2520robot%2520fingers%252C%250Aproviding%2520users%2520with%2520direct%2520contact%2520feedback%2520%2528via%2520proprioception%2529%2520and%2520mirrors%250Athe%2520human%2520hand%2520pose%2520to%2520the%2520passive%2520robot%2520hand%2520to%2520maximize%2520the%2520transfer%2520of%250Ademonstrated%2520skills%2520to%2520the%2520robot.%2520The%2520force%2520feedback%2520and%2520pose%2520mirroring%2520make%250Atask%2520demonstrations%2520more%2520natural%2520for%2520humans%2520compared%2520to%2520teleoperation%252C%250Aincreasing%2520both%2520speed%2520and%2520accuracy.%2520We%2520evaluate%2520DEXOP%2520across%2520a%2520range%2520of%250Adexterous%252C%2520contact-rich%2520tasks%252C%2520demonstrating%2520its%2520ability%2520to%2520collect%250Ahigh-quality%2520demonstration%2520data%2520at%2520scale.%2520Policies%2520learned%2520with%2520DEXOP%2520data%250Asignificantly%2520improve%2520task%2520performance%2520per%2520unit%2520time%2520of%2520data%2520collection%250Acompared%2520to%2520teleoperation%252C%2520making%2520DEXOP%2520a%2520powerful%2520tool%2520for%2520advancing%2520robot%250Adexterity.%2520Our%2520project%2520page%2520is%2520at%2520https%253A//dex-op.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEXOP%3A%20A%20Device%20for%20Robotic%20Transfer%20of%20Dexterous%20Human%20Manipulation&entry.906535625=Hao-Shu%20Fang%20and%20Branden%20Romero%20and%20Yichen%20Xie%20and%20Arthur%20Hu%20and%20Bo-Ruei%20Huang%20and%20Juan%20Alvarez%20and%20Matthew%20Kim%20and%20Gabriel%20Margolis%20and%20Kavya%20Anbarasu%20and%20Masayoshi%20Tomizuka%20and%20Edward%20Adelson%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20We%20introduce%20perioperation%2C%20a%20paradigm%20for%20robotic%20data%20collection%20that%0Asensorizes%20and%20records%20human%20manipulation%20while%20maximizing%20the%20transferability%0Aof%20the%20data%20to%20real%20robots.%20We%20implement%20this%20paradigm%20in%20DEXOP%2C%20a%20passive%20hand%0Aexoskeleton%20designed%20to%20maximize%20human%20ability%20to%20collect%20rich%20sensory%20%28vision%0A%2B%20tactile%29%20data%20for%20diverse%20dexterous%20manipulation%20tasks%20in%20natural%0Aenvironments.%20DEXOP%20mechanically%20connects%20human%20fingers%20to%20robot%20fingers%2C%0Aproviding%20users%20with%20direct%20contact%20feedback%20%28via%20proprioception%29%20and%20mirrors%0Athe%20human%20hand%20pose%20to%20the%20passive%20robot%20hand%20to%20maximize%20the%20transfer%20of%0Ademonstrated%20skills%20to%20the%20robot.%20The%20force%20feedback%20and%20pose%20mirroring%20make%0Atask%20demonstrations%20more%20natural%20for%20humans%20compared%20to%20teleoperation%2C%0Aincreasing%20both%20speed%20and%20accuracy.%20We%20evaluate%20DEXOP%20across%20a%20range%20of%0Adexterous%2C%20contact-rich%20tasks%2C%20demonstrating%20its%20ability%20to%20collect%0Ahigh-quality%20demonstration%20data%20at%20scale.%20Policies%20learned%20with%20DEXOP%20data%0Asignificantly%20improve%20task%20performance%20per%20unit%20time%20of%20data%20collection%0Acompared%20to%20teleoperation%2C%20making%20DEXOP%20a%20powerful%20tool%20for%20advancing%20robot%0Adexterity.%20Our%20project%20page%20is%20at%20https%3A//dex-op.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04441v2&entry.124074799=Read"},
{"title": "Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture\n  Synchronization", "author": "Carlos A. Pinheiro de Sousa and Niklas Gr\u00f6ne and Mathias G\u00fcnther and Oliver Deussen", "abstract": "  We introduce a multi-user VR co-location framework that synchronizes users\nwithin a shared virtual environment aligned to physical space. Our approach\ncombines a motion capture system with SLAM-based inside-out tracking to deliver\nsmooth, high-framerate, low-latency performance. Previous methods either rely\non continuous external tracking, which introduces latency and jitter, or on\none-time calibration, which cannot correct drift over time. In contrast, our\napproach combines the responsiveness of local HMD SLAM tracking with the\nflexibility to realign to an external source when needed. It also supports\nreal-time pose sharing across devices, ensuring consistent spatial alignment\nand engagement between users. Our evaluation demonstrates that our framework\nachieves the spatial accuracy required for natural multi-user interaction while\noffering improved comfort, scalability, and robustness over existing co-located\nVR solutions.\n", "link": "http://arxiv.org/abs/2509.06582v1", "date": "2025-09-08", "relevancy": 2.2008, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5656}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5437}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Located%20VR%20with%20Hybrid%20SLAM-based%20HMD%20Tracking%20and%20Motion%20Capture%0A%20%20Synchronization&body=Title%3A%20Co-Located%20VR%20with%20Hybrid%20SLAM-based%20HMD%20Tracking%20and%20Motion%20Capture%0A%20%20Synchronization%0AAuthor%3A%20Carlos%20A.%20Pinheiro%20de%20Sousa%20and%20Niklas%20Gr%C3%B6ne%20and%20Mathias%20G%C3%BCnther%20and%20Oliver%20Deussen%0AAbstract%3A%20%20%20We%20introduce%20a%20multi-user%20VR%20co-location%20framework%20that%20synchronizes%20users%0Awithin%20a%20shared%20virtual%20environment%20aligned%20to%20physical%20space.%20Our%20approach%0Acombines%20a%20motion%20capture%20system%20with%20SLAM-based%20inside-out%20tracking%20to%20deliver%0Asmooth%2C%20high-framerate%2C%20low-latency%20performance.%20Previous%20methods%20either%20rely%0Aon%20continuous%20external%20tracking%2C%20which%20introduces%20latency%20and%20jitter%2C%20or%20on%0Aone-time%20calibration%2C%20which%20cannot%20correct%20drift%20over%20time.%20In%20contrast%2C%20our%0Aapproach%20combines%20the%20responsiveness%20of%20local%20HMD%20SLAM%20tracking%20with%20the%0Aflexibility%20to%20realign%20to%20an%20external%20source%20when%20needed.%20It%20also%20supports%0Areal-time%20pose%20sharing%20across%20devices%2C%20ensuring%20consistent%20spatial%20alignment%0Aand%20engagement%20between%20users.%20Our%20evaluation%20demonstrates%20that%20our%20framework%0Aachieves%20the%20spatial%20accuracy%20required%20for%20natural%20multi-user%20interaction%20while%0Aoffering%20improved%20comfort%2C%20scalability%2C%20and%20robustness%20over%20existing%20co-located%0AVR%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Located%2520VR%2520with%2520Hybrid%2520SLAM-based%2520HMD%2520Tracking%2520and%2520Motion%2520Capture%250A%2520%2520Synchronization%26entry.906535625%3DCarlos%2520A.%2520Pinheiro%2520de%2520Sousa%2520and%2520Niklas%2520Gr%25C3%25B6ne%2520and%2520Mathias%2520G%25C3%25BCnther%2520and%2520Oliver%2520Deussen%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520multi-user%2520VR%2520co-location%2520framework%2520that%2520synchronizes%2520users%250Awithin%2520a%2520shared%2520virtual%2520environment%2520aligned%2520to%2520physical%2520space.%2520Our%2520approach%250Acombines%2520a%2520motion%2520capture%2520system%2520with%2520SLAM-based%2520inside-out%2520tracking%2520to%2520deliver%250Asmooth%252C%2520high-framerate%252C%2520low-latency%2520performance.%2520Previous%2520methods%2520either%2520rely%250Aon%2520continuous%2520external%2520tracking%252C%2520which%2520introduces%2520latency%2520and%2520jitter%252C%2520or%2520on%250Aone-time%2520calibration%252C%2520which%2520cannot%2520correct%2520drift%2520over%2520time.%2520In%2520contrast%252C%2520our%250Aapproach%2520combines%2520the%2520responsiveness%2520of%2520local%2520HMD%2520SLAM%2520tracking%2520with%2520the%250Aflexibility%2520to%2520realign%2520to%2520an%2520external%2520source%2520when%2520needed.%2520It%2520also%2520supports%250Areal-time%2520pose%2520sharing%2520across%2520devices%252C%2520ensuring%2520consistent%2520spatial%2520alignment%250Aand%2520engagement%2520between%2520users.%2520Our%2520evaluation%2520demonstrates%2520that%2520our%2520framework%250Aachieves%2520the%2520spatial%2520accuracy%2520required%2520for%2520natural%2520multi-user%2520interaction%2520while%250Aoffering%2520improved%2520comfort%252C%2520scalability%252C%2520and%2520robustness%2520over%2520existing%2520co-located%250AVR%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Located%20VR%20with%20Hybrid%20SLAM-based%20HMD%20Tracking%20and%20Motion%20Capture%0A%20%20Synchronization&entry.906535625=Carlos%20A.%20Pinheiro%20de%20Sousa%20and%20Niklas%20Gr%C3%B6ne%20and%20Mathias%20G%C3%BCnther%20and%20Oliver%20Deussen&entry.1292438233=%20%20We%20introduce%20a%20multi-user%20VR%20co-location%20framework%20that%20synchronizes%20users%0Awithin%20a%20shared%20virtual%20environment%20aligned%20to%20physical%20space.%20Our%20approach%0Acombines%20a%20motion%20capture%20system%20with%20SLAM-based%20inside-out%20tracking%20to%20deliver%0Asmooth%2C%20high-framerate%2C%20low-latency%20performance.%20Previous%20methods%20either%20rely%0Aon%20continuous%20external%20tracking%2C%20which%20introduces%20latency%20and%20jitter%2C%20or%20on%0Aone-time%20calibration%2C%20which%20cannot%20correct%20drift%20over%20time.%20In%20contrast%2C%20our%0Aapproach%20combines%20the%20responsiveness%20of%20local%20HMD%20SLAM%20tracking%20with%20the%0Aflexibility%20to%20realign%20to%20an%20external%20source%20when%20needed.%20It%20also%20supports%0Areal-time%20pose%20sharing%20across%20devices%2C%20ensuring%20consistent%20spatial%20alignment%0Aand%20engagement%20between%20users.%20Our%20evaluation%20demonstrates%20that%20our%20framework%0Aachieves%20the%20spatial%20accuracy%20required%20for%20natural%20multi-user%20interaction%20while%0Aoffering%20improved%20comfort%2C%20scalability%2C%20and%20robustness%20over%20existing%20co-located%0AVR%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06582v1&entry.124074799=Read"},
{"title": "CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning", "author": "Zhou-Peng Shou and Zhi-Qiang You and Fang Wang and Hai-Bo Liu", "abstract": "  Targeting the issues of \"shortcuts\" and insufficient contextual understanding\nin complex cross-modal reasoning of multimodal large models, this paper\nproposes a zero-shot multimodal reasoning component guided by human-like\ncognitive strategies centered on an \"intent sketch\". The component comprises a\nplug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and\nStrategy Selector-that explicitly constructs a \"understand-plan-select\"\ncognitive process. By generating and filtering \"intent sketch\" strategies to\nguide the final reasoning, it requires no parameter fine-tuning and achieves\ncross-model transfer solely through in-context engineering.\nInformation-theoretic analysis shows that this process can reduce conditional\nentropy and improve information utilization efficiency, thereby suppressing\nunintended shortcut reasoning. Experiments on IntentBench, WorldSense, and\nDaily-Omni validate the method's generality and robust gains; compared with\ntheir respective baselines, the complete \"three-module\" scheme yields\nconsistent improvements across different reasoning engines and pipeline\ncombinations, with gains up to approximately 9.51 percentage points,\ndemonstrating the practical value and portability of the \"intent sketch\"\nreasoning component in zero-shot scenarios.\n", "link": "http://arxiv.org/abs/2509.06641v1", "date": "2025-09-08", "relevancy": 2.1968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogGuide%3A%20Human-Like%20Guidance%20for%20Zero-Shot%20Omni-Modal%20Reasoning&body=Title%3A%20CogGuide%3A%20Human-Like%20Guidance%20for%20Zero-Shot%20Omni-Modal%20Reasoning%0AAuthor%3A%20Zhou-Peng%20Shou%20and%20Zhi-Qiang%20You%20and%20Fang%20Wang%20and%20Hai-Bo%20Liu%0AAbstract%3A%20%20%20Targeting%20the%20issues%20of%20%22shortcuts%22%20and%20insufficient%20contextual%20understanding%0Ain%20complex%20cross-modal%20reasoning%20of%20multimodal%20large%20models%2C%20this%20paper%0Aproposes%20a%20zero-shot%20multimodal%20reasoning%20component%20guided%20by%20human-like%0Acognitive%20strategies%20centered%20on%20an%20%22intent%20sketch%22.%20The%20component%20comprises%20a%0Aplug-and-play%20three-module%20pipeline-Intent%20Perceiver%2C%20Strategy%20Generator%2C%20and%0AStrategy%20Selector-that%20explicitly%20constructs%20a%20%22understand-plan-select%22%0Acognitive%20process.%20By%20generating%20and%20filtering%20%22intent%20sketch%22%20strategies%20to%0Aguide%20the%20final%20reasoning%2C%20it%20requires%20no%20parameter%20fine-tuning%20and%20achieves%0Across-model%20transfer%20solely%20through%20in-context%20engineering.%0AInformation-theoretic%20analysis%20shows%20that%20this%20process%20can%20reduce%20conditional%0Aentropy%20and%20improve%20information%20utilization%20efficiency%2C%20thereby%20suppressing%0Aunintended%20shortcut%20reasoning.%20Experiments%20on%20IntentBench%2C%20WorldSense%2C%20and%0ADaily-Omni%20validate%20the%20method%27s%20generality%20and%20robust%20gains%3B%20compared%20with%0Atheir%20respective%20baselines%2C%20the%20complete%20%22three-module%22%20scheme%20yields%0Aconsistent%20improvements%20across%20different%20reasoning%20engines%20and%20pipeline%0Acombinations%2C%20with%20gains%20up%20to%20approximately%209.51%20percentage%20points%2C%0Ademonstrating%20the%20practical%20value%20and%20portability%20of%20the%20%22intent%20sketch%22%0Areasoning%20component%20in%20zero-shot%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogGuide%253A%2520Human-Like%2520Guidance%2520for%2520Zero-Shot%2520Omni-Modal%2520Reasoning%26entry.906535625%3DZhou-Peng%2520Shou%2520and%2520Zhi-Qiang%2520You%2520and%2520Fang%2520Wang%2520and%2520Hai-Bo%2520Liu%26entry.1292438233%3D%2520%2520Targeting%2520the%2520issues%2520of%2520%2522shortcuts%2522%2520and%2520insufficient%2520contextual%2520understanding%250Ain%2520complex%2520cross-modal%2520reasoning%2520of%2520multimodal%2520large%2520models%252C%2520this%2520paper%250Aproposes%2520a%2520zero-shot%2520multimodal%2520reasoning%2520component%2520guided%2520by%2520human-like%250Acognitive%2520strategies%2520centered%2520on%2520an%2520%2522intent%2520sketch%2522.%2520The%2520component%2520comprises%2520a%250Aplug-and-play%2520three-module%2520pipeline-Intent%2520Perceiver%252C%2520Strategy%2520Generator%252C%2520and%250AStrategy%2520Selector-that%2520explicitly%2520constructs%2520a%2520%2522understand-plan-select%2522%250Acognitive%2520process.%2520By%2520generating%2520and%2520filtering%2520%2522intent%2520sketch%2522%2520strategies%2520to%250Aguide%2520the%2520final%2520reasoning%252C%2520it%2520requires%2520no%2520parameter%2520fine-tuning%2520and%2520achieves%250Across-model%2520transfer%2520solely%2520through%2520in-context%2520engineering.%250AInformation-theoretic%2520analysis%2520shows%2520that%2520this%2520process%2520can%2520reduce%2520conditional%250Aentropy%2520and%2520improve%2520information%2520utilization%2520efficiency%252C%2520thereby%2520suppressing%250Aunintended%2520shortcut%2520reasoning.%2520Experiments%2520on%2520IntentBench%252C%2520WorldSense%252C%2520and%250ADaily-Omni%2520validate%2520the%2520method%2527s%2520generality%2520and%2520robust%2520gains%253B%2520compared%2520with%250Atheir%2520respective%2520baselines%252C%2520the%2520complete%2520%2522three-module%2522%2520scheme%2520yields%250Aconsistent%2520improvements%2520across%2520different%2520reasoning%2520engines%2520and%2520pipeline%250Acombinations%252C%2520with%2520gains%2520up%2520to%2520approximately%25209.51%2520percentage%2520points%252C%250Ademonstrating%2520the%2520practical%2520value%2520and%2520portability%2520of%2520the%2520%2522intent%2520sketch%2522%250Areasoning%2520component%2520in%2520zero-shot%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogGuide%3A%20Human-Like%20Guidance%20for%20Zero-Shot%20Omni-Modal%20Reasoning&entry.906535625=Zhou-Peng%20Shou%20and%20Zhi-Qiang%20You%20and%20Fang%20Wang%20and%20Hai-Bo%20Liu&entry.1292438233=%20%20Targeting%20the%20issues%20of%20%22shortcuts%22%20and%20insufficient%20contextual%20understanding%0Ain%20complex%20cross-modal%20reasoning%20of%20multimodal%20large%20models%2C%20this%20paper%0Aproposes%20a%20zero-shot%20multimodal%20reasoning%20component%20guided%20by%20human-like%0Acognitive%20strategies%20centered%20on%20an%20%22intent%20sketch%22.%20The%20component%20comprises%20a%0Aplug-and-play%20three-module%20pipeline-Intent%20Perceiver%2C%20Strategy%20Generator%2C%20and%0AStrategy%20Selector-that%20explicitly%20constructs%20a%20%22understand-plan-select%22%0Acognitive%20process.%20By%20generating%20and%20filtering%20%22intent%20sketch%22%20strategies%20to%0Aguide%20the%20final%20reasoning%2C%20it%20requires%20no%20parameter%20fine-tuning%20and%20achieves%0Across-model%20transfer%20solely%20through%20in-context%20engineering.%0AInformation-theoretic%20analysis%20shows%20that%20this%20process%20can%20reduce%20conditional%0Aentropy%20and%20improve%20information%20utilization%20efficiency%2C%20thereby%20suppressing%0Aunintended%20shortcut%20reasoning.%20Experiments%20on%20IntentBench%2C%20WorldSense%2C%20and%0ADaily-Omni%20validate%20the%20method%27s%20generality%20and%20robust%20gains%3B%20compared%20with%0Atheir%20respective%20baselines%2C%20the%20complete%20%22three-module%22%20scheme%20yields%0Aconsistent%20improvements%20across%20different%20reasoning%20engines%20and%20pipeline%0Acombinations%2C%20with%20gains%20up%20to%20approximately%209.51%20percentage%20points%2C%0Ademonstrating%20the%20practical%20value%20and%20portability%20of%20the%20%22intent%20sketch%22%0Areasoning%20component%20in%20zero-shot%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06641v1&entry.124074799=Read"},
{"title": "SUDER: Self-Improving Unified Large Multimodal Models for Understanding\n  and Generation with Dual Self-Rewards", "author": "Jixiang Hong and Yiran Zhang and Guanzhong Wang and Yi Liu and Ji-Rong Wen and Rui Yan", "abstract": "  Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate vision-language alignment,\nprone to generating text responses contradicting the visual input or failing to\nfollow the text-to-image prompts. Current solutions require external\nsupervision (e.g., human feedback or reward models) and only address\nunidirectional tasks-either understanding or generation. In this work, based on\nthe observation that understanding and generation are naturally inverse dual\ntasks, we propose \\textbf{SUDER} (\\textbf{S}elf-improving \\textbf{U}nified LMMs\nwith \\textbf{D}ual s\\textbf{E}lf-\\textbf{R}ewards), a framework reinforcing the\nunderstanding and generation capabilities of LMMs with a self-supervised dual\nreward mechanism. SUDER leverages the inherent duality between understanding\nand generation tasks to provide self-supervised optimization signals for each\nother. Specifically, we sample multiple outputs for a given input in one task\ndomain, then reverse the input-output pairs to compute the dual likelihood\nwithin the model as self-rewards for optimization. Extensive experimental\nresults on visual understanding and generation benchmarks demonstrate that our\nmethod can effectively enhance the performance of the model without any\nexternal supervision, especially achieving remarkable improvements in\ntext-to-image tasks.\n", "link": "http://arxiv.org/abs/2506.07963v3", "date": "2025-09-08", "relevancy": 2.194, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUDER%3A%20Self-Improving%20Unified%20Large%20Multimodal%20Models%20for%20Understanding%0A%20%20and%20Generation%20with%20Dual%20Self-Rewards&body=Title%3A%20SUDER%3A%20Self-Improving%20Unified%20Large%20Multimodal%20Models%20for%20Understanding%0A%20%20and%20Generation%20with%20Dual%20Self-Rewards%0AAuthor%3A%20Jixiang%20Hong%20and%20Yiran%20Zhang%20and%20Guanzhong%20Wang%20and%20Yi%20Liu%20and%20Ji-Rong%20Wen%20and%20Rui%20Yan%0AAbstract%3A%20%20%20Building%20upon%20large%20language%20models%20%28LLMs%29%2C%20recent%20large%20multimodal%20models%0A%28LMMs%29%20unify%20cross-model%20understanding%20and%20generation%20into%20a%20single%20framework.%0AHowever%2C%20LMMs%20still%20struggle%20to%20achieve%20accurate%20vision-language%20alignment%2C%0Aprone%20to%20generating%20text%20responses%20contradicting%20the%20visual%20input%20or%20failing%20to%0Afollow%20the%20text-to-image%20prompts.%20Current%20solutions%20require%20external%0Asupervision%20%28e.g.%2C%20human%20feedback%20or%20reward%20models%29%20and%20only%20address%0Aunidirectional%20tasks-either%20understanding%20or%20generation.%20In%20this%20work%2C%20based%20on%0Athe%20observation%20that%20understanding%20and%20generation%20are%20naturally%20inverse%20dual%0Atasks%2C%20we%20propose%20%5Ctextbf%7BSUDER%7D%20%28%5Ctextbf%7BS%7Delf-improving%20%5Ctextbf%7BU%7Dnified%20LMMs%0Awith%20%5Ctextbf%7BD%7Dual%20s%5Ctextbf%7BE%7Dlf-%5Ctextbf%7BR%7Dewards%29%2C%20a%20framework%20reinforcing%20the%0Aunderstanding%20and%20generation%20capabilities%20of%20LMMs%20with%20a%20self-supervised%20dual%0Areward%20mechanism.%20SUDER%20leverages%20the%20inherent%20duality%20between%20understanding%0Aand%20generation%20tasks%20to%20provide%20self-supervised%20optimization%20signals%20for%20each%0Aother.%20Specifically%2C%20we%20sample%20multiple%20outputs%20for%20a%20given%20input%20in%20one%20task%0Adomain%2C%20then%20reverse%20the%20input-output%20pairs%20to%20compute%20the%20dual%20likelihood%0Awithin%20the%20model%20as%20self-rewards%20for%20optimization.%20Extensive%20experimental%0Aresults%20on%20visual%20understanding%20and%20generation%20benchmarks%20demonstrate%20that%20our%0Amethod%20can%20effectively%20enhance%20the%20performance%20of%20the%20model%20without%20any%0Aexternal%20supervision%2C%20especially%20achieving%20remarkable%20improvements%20in%0Atext-to-image%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07963v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUDER%253A%2520Self-Improving%2520Unified%2520Large%2520Multimodal%2520Models%2520for%2520Understanding%250A%2520%2520and%2520Generation%2520with%2520Dual%2520Self-Rewards%26entry.906535625%3DJixiang%2520Hong%2520and%2520Yiran%2520Zhang%2520and%2520Guanzhong%2520Wang%2520and%2520Yi%2520Liu%2520and%2520Ji-Rong%2520Wen%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520Building%2520upon%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520recent%2520large%2520multimodal%2520models%250A%2528LMMs%2529%2520unify%2520cross-model%2520understanding%2520and%2520generation%2520into%2520a%2520single%2520framework.%250AHowever%252C%2520LMMs%2520still%2520struggle%2520to%2520achieve%2520accurate%2520vision-language%2520alignment%252C%250Aprone%2520to%2520generating%2520text%2520responses%2520contradicting%2520the%2520visual%2520input%2520or%2520failing%2520to%250Afollow%2520the%2520text-to-image%2520prompts.%2520Current%2520solutions%2520require%2520external%250Asupervision%2520%2528e.g.%252C%2520human%2520feedback%2520or%2520reward%2520models%2529%2520and%2520only%2520address%250Aunidirectional%2520tasks-either%2520understanding%2520or%2520generation.%2520In%2520this%2520work%252C%2520based%2520on%250Athe%2520observation%2520that%2520understanding%2520and%2520generation%2520are%2520naturally%2520inverse%2520dual%250Atasks%252C%2520we%2520propose%2520%255Ctextbf%257BSUDER%257D%2520%2528%255Ctextbf%257BS%257Delf-improving%2520%255Ctextbf%257BU%257Dnified%2520LMMs%250Awith%2520%255Ctextbf%257BD%257Dual%2520s%255Ctextbf%257BE%257Dlf-%255Ctextbf%257BR%257Dewards%2529%252C%2520a%2520framework%2520reinforcing%2520the%250Aunderstanding%2520and%2520generation%2520capabilities%2520of%2520LMMs%2520with%2520a%2520self-supervised%2520dual%250Areward%2520mechanism.%2520SUDER%2520leverages%2520the%2520inherent%2520duality%2520between%2520understanding%250Aand%2520generation%2520tasks%2520to%2520provide%2520self-supervised%2520optimization%2520signals%2520for%2520each%250Aother.%2520Specifically%252C%2520we%2520sample%2520multiple%2520outputs%2520for%2520a%2520given%2520input%2520in%2520one%2520task%250Adomain%252C%2520then%2520reverse%2520the%2520input-output%2520pairs%2520to%2520compute%2520the%2520dual%2520likelihood%250Awithin%2520the%2520model%2520as%2520self-rewards%2520for%2520optimization.%2520Extensive%2520experimental%250Aresults%2520on%2520visual%2520understanding%2520and%2520generation%2520benchmarks%2520demonstrate%2520that%2520our%250Amethod%2520can%2520effectively%2520enhance%2520the%2520performance%2520of%2520the%2520model%2520without%2520any%250Aexternal%2520supervision%252C%2520especially%2520achieving%2520remarkable%2520improvements%2520in%250Atext-to-image%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07963v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUDER%3A%20Self-Improving%20Unified%20Large%20Multimodal%20Models%20for%20Understanding%0A%20%20and%20Generation%20with%20Dual%20Self-Rewards&entry.906535625=Jixiang%20Hong%20and%20Yiran%20Zhang%20and%20Guanzhong%20Wang%20and%20Yi%20Liu%20and%20Ji-Rong%20Wen%20and%20Rui%20Yan&entry.1292438233=%20%20Building%20upon%20large%20language%20models%20%28LLMs%29%2C%20recent%20large%20multimodal%20models%0A%28LMMs%29%20unify%20cross-model%20understanding%20and%20generation%20into%20a%20single%20framework.%0AHowever%2C%20LMMs%20still%20struggle%20to%20achieve%20accurate%20vision-language%20alignment%2C%0Aprone%20to%20generating%20text%20responses%20contradicting%20the%20visual%20input%20or%20failing%20to%0Afollow%20the%20text-to-image%20prompts.%20Current%20solutions%20require%20external%0Asupervision%20%28e.g.%2C%20human%20feedback%20or%20reward%20models%29%20and%20only%20address%0Aunidirectional%20tasks-either%20understanding%20or%20generation.%20In%20this%20work%2C%20based%20on%0Athe%20observation%20that%20understanding%20and%20generation%20are%20naturally%20inverse%20dual%0Atasks%2C%20we%20propose%20%5Ctextbf%7BSUDER%7D%20%28%5Ctextbf%7BS%7Delf-improving%20%5Ctextbf%7BU%7Dnified%20LMMs%0Awith%20%5Ctextbf%7BD%7Dual%20s%5Ctextbf%7BE%7Dlf-%5Ctextbf%7BR%7Dewards%29%2C%20a%20framework%20reinforcing%20the%0Aunderstanding%20and%20generation%20capabilities%20of%20LMMs%20with%20a%20self-supervised%20dual%0Areward%20mechanism.%20SUDER%20leverages%20the%20inherent%20duality%20between%20understanding%0Aand%20generation%20tasks%20to%20provide%20self-supervised%20optimization%20signals%20for%20each%0Aother.%20Specifically%2C%20we%20sample%20multiple%20outputs%20for%20a%20given%20input%20in%20one%20task%0Adomain%2C%20then%20reverse%20the%20input-output%20pairs%20to%20compute%20the%20dual%20likelihood%0Awithin%20the%20model%20as%20self-rewards%20for%20optimization.%20Extensive%20experimental%0Aresults%20on%20visual%20understanding%20and%20generation%20benchmarks%20demonstrate%20that%20our%0Amethod%20can%20effectively%20enhance%20the%20performance%20of%20the%20model%20without%20any%0Aexternal%20supervision%2C%20especially%20achieving%20remarkable%20improvements%20in%0Atext-to-image%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07963v3&entry.124074799=Read"},
{"title": "Enhanced Partially Relevant Video Retrieval through Inter- and\n  Intra-Sample Analysis with Coherence Prediction", "author": "Junlong Ren and Gangjian Zhang and Hao Wang and Yu Hu and Jian Shu and Hui Xiong", "abstract": "  Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video\nthat is partially relevant to the text query. The primary challenge in PRVR\narises from the semantic asymmetry between textual and visual modalities, as\nvideos often contain substantial content irrelevant to the query. Existing\nmethods coarsely align paired videos and text queries to construct the semantic\nspace, neglecting the critical cross-modal dual nature inherent in this task:\ninter-sample correlation and intra-sample redundancy. To this end, we propose a\nnovel PRVR framework to systematically exploit these two characteristics. Our\nframework consists of three core modules. First, the Inter Correlation\nEnhancement (ICE) module captures inter-sample correlation by identifying\nsemantically similar yet unpaired text queries and video moments, combining\nthem to form pseudo-positive pairs for more robust semantic space construction.\nSecond, the Intra Redundancy Mining (IRM) module mitigates intra-sample\nredundancy by mining redundant moment features and distinguishing them from\nquery-relevant moments, encouraging the model to learn more discriminative\nrepresentations. Finally, to reinforce these modules, we introduce the Temporal\nCoherence Prediction (TCP) module, which enhances temporal structure learning\nby training the model to predict the original temporal order of randomly\nshuffled video frames and moments. Extensive experiments demonstrate the\nsuperiority of our approach compared to prior methods, achieving\nstate-of-the-art results.\n", "link": "http://arxiv.org/abs/2504.19637v2", "date": "2025-09-08", "relevancy": 2.1923, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Partially%20Relevant%20Video%20Retrieval%20through%20Inter-%20and%0A%20%20Intra-Sample%20Analysis%20with%20Coherence%20Prediction&body=Title%3A%20Enhanced%20Partially%20Relevant%20Video%20Retrieval%20through%20Inter-%20and%0A%20%20Intra-Sample%20Analysis%20with%20Coherence%20Prediction%0AAuthor%3A%20Junlong%20Ren%20and%20Gangjian%20Zhang%20and%20Hao%20Wang%20and%20Yu%20Hu%20and%20Jian%20Shu%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20aims%20to%20retrieve%20the%20target%20video%0Athat%20is%20partially%20relevant%20to%20the%20text%20query.%20The%20primary%20challenge%20in%20PRVR%0Aarises%20from%20the%20semantic%20asymmetry%20between%20textual%20and%20visual%20modalities%2C%20as%0Avideos%20often%20contain%20substantial%20content%20irrelevant%20to%20the%20query.%20Existing%0Amethods%20coarsely%20align%20paired%20videos%20and%20text%20queries%20to%20construct%20the%20semantic%0Aspace%2C%20neglecting%20the%20critical%20cross-modal%20dual%20nature%20inherent%20in%20this%20task%3A%0Ainter-sample%20correlation%20and%20intra-sample%20redundancy.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20PRVR%20framework%20to%20systematically%20exploit%20these%20two%20characteristics.%20Our%0Aframework%20consists%20of%20three%20core%20modules.%20First%2C%20the%20Inter%20Correlation%0AEnhancement%20%28ICE%29%20module%20captures%20inter-sample%20correlation%20by%20identifying%0Asemantically%20similar%20yet%20unpaired%20text%20queries%20and%20video%20moments%2C%20combining%0Athem%20to%20form%20pseudo-positive%20pairs%20for%20more%20robust%20semantic%20space%20construction.%0ASecond%2C%20the%20Intra%20Redundancy%20Mining%20%28IRM%29%20module%20mitigates%20intra-sample%0Aredundancy%20by%20mining%20redundant%20moment%20features%20and%20distinguishing%20them%20from%0Aquery-relevant%20moments%2C%20encouraging%20the%20model%20to%20learn%20more%20discriminative%0Arepresentations.%20Finally%2C%20to%20reinforce%20these%20modules%2C%20we%20introduce%20the%20Temporal%0ACoherence%20Prediction%20%28TCP%29%20module%2C%20which%20enhances%20temporal%20structure%20learning%0Aby%20training%20the%20model%20to%20predict%20the%20original%20temporal%20order%20of%20randomly%0Ashuffled%20video%20frames%20and%20moments.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20compared%20to%20prior%20methods%2C%20achieving%0Astate-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19637v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Partially%2520Relevant%2520Video%2520Retrieval%2520through%2520Inter-%2520and%250A%2520%2520Intra-Sample%2520Analysis%2520with%2520Coherence%2520Prediction%26entry.906535625%3DJunlong%2520Ren%2520and%2520Gangjian%2520Zhang%2520and%2520Hao%2520Wang%2520and%2520Yu%2520Hu%2520and%2520Jian%2520Shu%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Partially%2520Relevant%2520Video%2520Retrieval%2520%2528PRVR%2529%2520aims%2520to%2520retrieve%2520the%2520target%2520video%250Athat%2520is%2520partially%2520relevant%2520to%2520the%2520text%2520query.%2520The%2520primary%2520challenge%2520in%2520PRVR%250Aarises%2520from%2520the%2520semantic%2520asymmetry%2520between%2520textual%2520and%2520visual%2520modalities%252C%2520as%250Avideos%2520often%2520contain%2520substantial%2520content%2520irrelevant%2520to%2520the%2520query.%2520Existing%250Amethods%2520coarsely%2520align%2520paired%2520videos%2520and%2520text%2520queries%2520to%2520construct%2520the%2520semantic%250Aspace%252C%2520neglecting%2520the%2520critical%2520cross-modal%2520dual%2520nature%2520inherent%2520in%2520this%2520task%253A%250Ainter-sample%2520correlation%2520and%2520intra-sample%2520redundancy.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Anovel%2520PRVR%2520framework%2520to%2520systematically%2520exploit%2520these%2520two%2520characteristics.%2520Our%250Aframework%2520consists%2520of%2520three%2520core%2520modules.%2520First%252C%2520the%2520Inter%2520Correlation%250AEnhancement%2520%2528ICE%2529%2520module%2520captures%2520inter-sample%2520correlation%2520by%2520identifying%250Asemantically%2520similar%2520yet%2520unpaired%2520text%2520queries%2520and%2520video%2520moments%252C%2520combining%250Athem%2520to%2520form%2520pseudo-positive%2520pairs%2520for%2520more%2520robust%2520semantic%2520space%2520construction.%250ASecond%252C%2520the%2520Intra%2520Redundancy%2520Mining%2520%2528IRM%2529%2520module%2520mitigates%2520intra-sample%250Aredundancy%2520by%2520mining%2520redundant%2520moment%2520features%2520and%2520distinguishing%2520them%2520from%250Aquery-relevant%2520moments%252C%2520encouraging%2520the%2520model%2520to%2520learn%2520more%2520discriminative%250Arepresentations.%2520Finally%252C%2520to%2520reinforce%2520these%2520modules%252C%2520we%2520introduce%2520the%2520Temporal%250ACoherence%2520Prediction%2520%2528TCP%2529%2520module%252C%2520which%2520enhances%2520temporal%2520structure%2520learning%250Aby%2520training%2520the%2520model%2520to%2520predict%2520the%2520original%2520temporal%2520order%2520of%2520randomly%250Ashuffled%2520video%2520frames%2520and%2520moments.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520approach%2520compared%2520to%2520prior%2520methods%252C%2520achieving%250Astate-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19637v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Partially%20Relevant%20Video%20Retrieval%20through%20Inter-%20and%0A%20%20Intra-Sample%20Analysis%20with%20Coherence%20Prediction&entry.906535625=Junlong%20Ren%20and%20Gangjian%20Zhang%20and%20Hao%20Wang%20and%20Yu%20Hu%20and%20Jian%20Shu%20and%20Hui%20Xiong&entry.1292438233=%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20aims%20to%20retrieve%20the%20target%20video%0Athat%20is%20partially%20relevant%20to%20the%20text%20query.%20The%20primary%20challenge%20in%20PRVR%0Aarises%20from%20the%20semantic%20asymmetry%20between%20textual%20and%20visual%20modalities%2C%20as%0Avideos%20often%20contain%20substantial%20content%20irrelevant%20to%20the%20query.%20Existing%0Amethods%20coarsely%20align%20paired%20videos%20and%20text%20queries%20to%20construct%20the%20semantic%0Aspace%2C%20neglecting%20the%20critical%20cross-modal%20dual%20nature%20inherent%20in%20this%20task%3A%0Ainter-sample%20correlation%20and%20intra-sample%20redundancy.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20PRVR%20framework%20to%20systematically%20exploit%20these%20two%20characteristics.%20Our%0Aframework%20consists%20of%20three%20core%20modules.%20First%2C%20the%20Inter%20Correlation%0AEnhancement%20%28ICE%29%20module%20captures%20inter-sample%20correlation%20by%20identifying%0Asemantically%20similar%20yet%20unpaired%20text%20queries%20and%20video%20moments%2C%20combining%0Athem%20to%20form%20pseudo-positive%20pairs%20for%20more%20robust%20semantic%20space%20construction.%0ASecond%2C%20the%20Intra%20Redundancy%20Mining%20%28IRM%29%20module%20mitigates%20intra-sample%0Aredundancy%20by%20mining%20redundant%20moment%20features%20and%20distinguishing%20them%20from%0Aquery-relevant%20moments%2C%20encouraging%20the%20model%20to%20learn%20more%20discriminative%0Arepresentations.%20Finally%2C%20to%20reinforce%20these%20modules%2C%20we%20introduce%20the%20Temporal%0ACoherence%20Prediction%20%28TCP%29%20module%2C%20which%20enhances%20temporal%20structure%20learning%0Aby%20training%20the%20model%20to%20predict%20the%20original%20temporal%20order%20of%20randomly%0Ashuffled%20video%20frames%20and%20moments.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20compared%20to%20prior%20methods%2C%20achieving%0Astate-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19637v2&entry.124074799=Read"},
{"title": "Barlow-Swin: Toward a novel siamese-based segmentation architecture\n  using Swin-Transformers", "author": "Morteza Kiani Haftlang and Mohammadhossein Malmir and Foroutan Parand and Umberto Michelucci and Safouane El Ghazouali", "abstract": "  Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin.\n", "link": "http://arxiv.org/abs/2509.06885v1", "date": "2025-09-08", "relevancy": 2.1812, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Barlow-Swin%3A%20Toward%20a%20novel%20siamese-based%20segmentation%20architecture%0A%20%20using%20Swin-Transformers&body=Title%3A%20Barlow-Swin%3A%20Toward%20a%20novel%20siamese-based%20segmentation%20architecture%0A%20%20using%20Swin-Transformers%0AAuthor%3A%20Morteza%20Kiani%20Haftlang%20and%20Mohammadhossein%20Malmir%20and%20Foroutan%20Parand%20and%20Umberto%20Michelucci%20and%20Safouane%20El%20Ghazouali%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20a%20critical%20task%20in%20clinical%20workflows%2C%0Aparticularly%20for%20the%20detection%20and%20delineation%20of%20pathological%20regions.%20While%0Aconvolutional%20architectures%20like%20U-Net%20have%20become%20standard%20for%20such%20tasks%2C%0Atheir%20limited%20receptive%20field%20restricts%20global%20context%20modeling.%20Recent%20efforts%0Aintegrating%20transformers%20have%20addressed%20this%2C%20but%20often%20result%20in%20deep%2C%0Acomputationally%20expensive%20models%20unsuitable%20for%20real-time%20use.%20In%20this%20work%2C%20we%0Apresent%20a%20novel%20end-to-end%20lightweight%20architecture%20designed%20specifically%20for%0Areal-time%20binary%20medical%20image%20segmentation.%20Our%20model%20combines%20a%20Swin%0ATransformer-like%20encoder%20with%20a%20U-Net-like%20decoder%2C%20connected%20via%20skip%20pathways%0Ato%20preserve%20spatial%20detail%20while%20capturing%20contextual%20information.%20Unlike%0Aexisting%20designs%20such%20as%20Swin%20Transformer%20or%20U-Net%2C%20our%20architecture%20is%0Asignificantly%20shallower%20and%20competitively%20efficient.%20To%20improve%20the%20encoder%27s%0Aability%20to%20learn%20meaningful%20features%20without%20relying%20on%20large%20amounts%20of%0Alabeled%20data%2C%20we%20first%20train%20it%20using%20Barlow%20Twins%2C%20a%20self-supervised%20learning%0Amethod%20that%20helps%20the%20model%20focus%20on%20important%20patterns%20by%20reducing%20unnecessary%0Arepetition%20in%20the%20learned%20features.%20After%20this%20pretraining%2C%20we%20fine-tune%20the%0Aentire%20model%20for%20our%20specific%20task.%20Experiments%20on%20benchmark%20binary%0Asegmentation%20tasks%20demonstrate%20that%20our%20model%20achieves%20competitive%20accuracy%0Awith%20substantially%20reduced%20parameter%20count%20and%20faster%20inference%2C%20positioning%20it%0Aas%20a%20practical%20alternative%20for%20deployment%20in%20real-time%20and%20resource-limited%0Aclinical%20environments.%20The%20code%20for%20our%20method%20is%20available%20at%20Github%0Arepository%3A%20https%3A//github.com/mkianih/Barlow-Swin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBarlow-Swin%253A%2520Toward%2520a%2520novel%2520siamese-based%2520segmentation%2520architecture%250A%2520%2520using%2520Swin-Transformers%26entry.906535625%3DMorteza%2520Kiani%2520Haftlang%2520and%2520Mohammadhossein%2520Malmir%2520and%2520Foroutan%2520Parand%2520and%2520Umberto%2520Michelucci%2520and%2520Safouane%2520El%2520Ghazouali%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520a%2520critical%2520task%2520in%2520clinical%2520workflows%252C%250Aparticularly%2520for%2520the%2520detection%2520and%2520delineation%2520of%2520pathological%2520regions.%2520While%250Aconvolutional%2520architectures%2520like%2520U-Net%2520have%2520become%2520standard%2520for%2520such%2520tasks%252C%250Atheir%2520limited%2520receptive%2520field%2520restricts%2520global%2520context%2520modeling.%2520Recent%2520efforts%250Aintegrating%2520transformers%2520have%2520addressed%2520this%252C%2520but%2520often%2520result%2520in%2520deep%252C%250Acomputationally%2520expensive%2520models%2520unsuitable%2520for%2520real-time%2520use.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520novel%2520end-to-end%2520lightweight%2520architecture%2520designed%2520specifically%2520for%250Areal-time%2520binary%2520medical%2520image%2520segmentation.%2520Our%2520model%2520combines%2520a%2520Swin%250ATransformer-like%2520encoder%2520with%2520a%2520U-Net-like%2520decoder%252C%2520connected%2520via%2520skip%2520pathways%250Ato%2520preserve%2520spatial%2520detail%2520while%2520capturing%2520contextual%2520information.%2520Unlike%250Aexisting%2520designs%2520such%2520as%2520Swin%2520Transformer%2520or%2520U-Net%252C%2520our%2520architecture%2520is%250Asignificantly%2520shallower%2520and%2520competitively%2520efficient.%2520To%2520improve%2520the%2520encoder%2527s%250Aability%2520to%2520learn%2520meaningful%2520features%2520without%2520relying%2520on%2520large%2520amounts%2520of%250Alabeled%2520data%252C%2520we%2520first%2520train%2520it%2520using%2520Barlow%2520Twins%252C%2520a%2520self-supervised%2520learning%250Amethod%2520that%2520helps%2520the%2520model%2520focus%2520on%2520important%2520patterns%2520by%2520reducing%2520unnecessary%250Arepetition%2520in%2520the%2520learned%2520features.%2520After%2520this%2520pretraining%252C%2520we%2520fine-tune%2520the%250Aentire%2520model%2520for%2520our%2520specific%2520task.%2520Experiments%2520on%2520benchmark%2520binary%250Asegmentation%2520tasks%2520demonstrate%2520that%2520our%2520model%2520achieves%2520competitive%2520accuracy%250Awith%2520substantially%2520reduced%2520parameter%2520count%2520and%2520faster%2520inference%252C%2520positioning%2520it%250Aas%2520a%2520practical%2520alternative%2520for%2520deployment%2520in%2520real-time%2520and%2520resource-limited%250Aclinical%2520environments.%2520The%2520code%2520for%2520our%2520method%2520is%2520available%2520at%2520Github%250Arepository%253A%2520https%253A//github.com/mkianih/Barlow-Swin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Barlow-Swin%3A%20Toward%20a%20novel%20siamese-based%20segmentation%20architecture%0A%20%20using%20Swin-Transformers&entry.906535625=Morteza%20Kiani%20Haftlang%20and%20Mohammadhossein%20Malmir%20and%20Foroutan%20Parand%20and%20Umberto%20Michelucci%20and%20Safouane%20El%20Ghazouali&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20a%20critical%20task%20in%20clinical%20workflows%2C%0Aparticularly%20for%20the%20detection%20and%20delineation%20of%20pathological%20regions.%20While%0Aconvolutional%20architectures%20like%20U-Net%20have%20become%20standard%20for%20such%20tasks%2C%0Atheir%20limited%20receptive%20field%20restricts%20global%20context%20modeling.%20Recent%20efforts%0Aintegrating%20transformers%20have%20addressed%20this%2C%20but%20often%20result%20in%20deep%2C%0Acomputationally%20expensive%20models%20unsuitable%20for%20real-time%20use.%20In%20this%20work%2C%20we%0Apresent%20a%20novel%20end-to-end%20lightweight%20architecture%20designed%20specifically%20for%0Areal-time%20binary%20medical%20image%20segmentation.%20Our%20model%20combines%20a%20Swin%0ATransformer-like%20encoder%20with%20a%20U-Net-like%20decoder%2C%20connected%20via%20skip%20pathways%0Ato%20preserve%20spatial%20detail%20while%20capturing%20contextual%20information.%20Unlike%0Aexisting%20designs%20such%20as%20Swin%20Transformer%20or%20U-Net%2C%20our%20architecture%20is%0Asignificantly%20shallower%20and%20competitively%20efficient.%20To%20improve%20the%20encoder%27s%0Aability%20to%20learn%20meaningful%20features%20without%20relying%20on%20large%20amounts%20of%0Alabeled%20data%2C%20we%20first%20train%20it%20using%20Barlow%20Twins%2C%20a%20self-supervised%20learning%0Amethod%20that%20helps%20the%20model%20focus%20on%20important%20patterns%20by%20reducing%20unnecessary%0Arepetition%20in%20the%20learned%20features.%20After%20this%20pretraining%2C%20we%20fine-tune%20the%0Aentire%20model%20for%20our%20specific%20task.%20Experiments%20on%20benchmark%20binary%0Asegmentation%20tasks%20demonstrate%20that%20our%20model%20achieves%20competitive%20accuracy%0Awith%20substantially%20reduced%20parameter%20count%20and%20faster%20inference%2C%20positioning%20it%0Aas%20a%20practical%20alternative%20for%20deployment%20in%20real-time%20and%20resource-limited%0Aclinical%20environments.%20The%20code%20for%20our%20method%20is%20available%20at%20Github%0Arepository%3A%20https%3A//github.com/mkianih/Barlow-Swin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06885v1&entry.124074799=Read"},
{"title": "Raw2Event: Converting Raw Frame Camera into Event Camera", "author": "Zijie Ning and Enmin Lin and Sudarshan R. Iyengar and Patrick Vandewalle", "abstract": "  Event cameras offer unique advantages such as high temporal resolution, low\nlatency, and high dynamic range, making them more and more popular for vision\ntasks under challenging light conditions. However, their high cost, limited\nresolution, and lack of features such as autofocus hinder their broad adoption,\nparticularly for early-stage development and prototyping. In this work, we\npresent Raw2Event, a complete hardware-software system that enables real-time\nevent generation from low-cost raw frame-based cameras. By leveraging direct\naccess to raw Bayer data and bypassing traditional image signal processors\n(ISP), our system is able to utilize the full potential of camera hardware,\ndelivering higher dynamic range, higher resolution, and more faithful output\nthan RGB-based frame-to-event converters.\n  Built upon the DVS-Voltmeter model, Raw2Event features a configurable\nsimulation framework optimized for deployment on embedded platforms. We further\ndesign a data acquisition pipeline that supports synchronized recording of raw,\nRGB, and event streams, facilitating downstream evaluation and dataset\ncreation. Experimental results show that Raw2Event can generate event streams\nclosely resembling those from real event cameras, while benefiting from higher\nresolution and autofocus capabilities. The system also supports user-intuitive\nparameter tuning, enabling flexible adaptation to various application\nrequirements. Finally, we deploy the system on a Raspberry Pi for real-time\noperation, providing a scalable and cost-effective solution for event-based\nvision research and early-stage system development.\n  The codes are available online:\nhttps://anonymous.4open.science/r/raw2event-BFF2/README.md.\n", "link": "http://arxiv.org/abs/2509.06767v1", "date": "2025-09-08", "relevancy": 2.1685, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5434}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5434}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Raw2Event%3A%20Converting%20Raw%20Frame%20Camera%20into%20Event%20Camera&body=Title%3A%20Raw2Event%3A%20Converting%20Raw%20Frame%20Camera%20into%20Event%20Camera%0AAuthor%3A%20Zijie%20Ning%20and%20Enmin%20Lin%20and%20Sudarshan%20R.%20Iyengar%20and%20Patrick%20Vandewalle%0AAbstract%3A%20%20%20Event%20cameras%20offer%20unique%20advantages%20such%20as%20high%20temporal%20resolution%2C%20low%0Alatency%2C%20and%20high%20dynamic%20range%2C%20making%20them%20more%20and%20more%20popular%20for%20vision%0Atasks%20under%20challenging%20light%20conditions.%20However%2C%20their%20high%20cost%2C%20limited%0Aresolution%2C%20and%20lack%20of%20features%20such%20as%20autofocus%20hinder%20their%20broad%20adoption%2C%0Aparticularly%20for%20early-stage%20development%20and%20prototyping.%20In%20this%20work%2C%20we%0Apresent%20Raw2Event%2C%20a%20complete%20hardware-software%20system%20that%20enables%20real-time%0Aevent%20generation%20from%20low-cost%20raw%20frame-based%20cameras.%20By%20leveraging%20direct%0Aaccess%20to%20raw%20Bayer%20data%20and%20bypassing%20traditional%20image%20signal%20processors%0A%28ISP%29%2C%20our%20system%20is%20able%20to%20utilize%20the%20full%20potential%20of%20camera%20hardware%2C%0Adelivering%20higher%20dynamic%20range%2C%20higher%20resolution%2C%20and%20more%20faithful%20output%0Athan%20RGB-based%20frame-to-event%20converters.%0A%20%20Built%20upon%20the%20DVS-Voltmeter%20model%2C%20Raw2Event%20features%20a%20configurable%0Asimulation%20framework%20optimized%20for%20deployment%20on%20embedded%20platforms.%20We%20further%0Adesign%20a%20data%20acquisition%20pipeline%20that%20supports%20synchronized%20recording%20of%20raw%2C%0ARGB%2C%20and%20event%20streams%2C%20facilitating%20downstream%20evaluation%20and%20dataset%0Acreation.%20Experimental%20results%20show%20that%20Raw2Event%20can%20generate%20event%20streams%0Aclosely%20resembling%20those%20from%20real%20event%20cameras%2C%20while%20benefiting%20from%20higher%0Aresolution%20and%20autofocus%20capabilities.%20The%20system%20also%20supports%20user-intuitive%0Aparameter%20tuning%2C%20enabling%20flexible%20adaptation%20to%20various%20application%0Arequirements.%20Finally%2C%20we%20deploy%20the%20system%20on%20a%20Raspberry%20Pi%20for%20real-time%0Aoperation%2C%20providing%20a%20scalable%20and%20cost-effective%20solution%20for%20event-based%0Avision%20research%20and%20early-stage%20system%20development.%0A%20%20The%20codes%20are%20available%20online%3A%0Ahttps%3A//anonymous.4open.science/r/raw2event-BFF2/README.md.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaw2Event%253A%2520Converting%2520Raw%2520Frame%2520Camera%2520into%2520Event%2520Camera%26entry.906535625%3DZijie%2520Ning%2520and%2520Enmin%2520Lin%2520and%2520Sudarshan%2520R.%2520Iyengar%2520and%2520Patrick%2520Vandewalle%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520unique%2520advantages%2520such%2520as%2520high%2520temporal%2520resolution%252C%2520low%250Alatency%252C%2520and%2520high%2520dynamic%2520range%252C%2520making%2520them%2520more%2520and%2520more%2520popular%2520for%2520vision%250Atasks%2520under%2520challenging%2520light%2520conditions.%2520However%252C%2520their%2520high%2520cost%252C%2520limited%250Aresolution%252C%2520and%2520lack%2520of%2520features%2520such%2520as%2520autofocus%2520hinder%2520their%2520broad%2520adoption%252C%250Aparticularly%2520for%2520early-stage%2520development%2520and%2520prototyping.%2520In%2520this%2520work%252C%2520we%250Apresent%2520Raw2Event%252C%2520a%2520complete%2520hardware-software%2520system%2520that%2520enables%2520real-time%250Aevent%2520generation%2520from%2520low-cost%2520raw%2520frame-based%2520cameras.%2520By%2520leveraging%2520direct%250Aaccess%2520to%2520raw%2520Bayer%2520data%2520and%2520bypassing%2520traditional%2520image%2520signal%2520processors%250A%2528ISP%2529%252C%2520our%2520system%2520is%2520able%2520to%2520utilize%2520the%2520full%2520potential%2520of%2520camera%2520hardware%252C%250Adelivering%2520higher%2520dynamic%2520range%252C%2520higher%2520resolution%252C%2520and%2520more%2520faithful%2520output%250Athan%2520RGB-based%2520frame-to-event%2520converters.%250A%2520%2520Built%2520upon%2520the%2520DVS-Voltmeter%2520model%252C%2520Raw2Event%2520features%2520a%2520configurable%250Asimulation%2520framework%2520optimized%2520for%2520deployment%2520on%2520embedded%2520platforms.%2520We%2520further%250Adesign%2520a%2520data%2520acquisition%2520pipeline%2520that%2520supports%2520synchronized%2520recording%2520of%2520raw%252C%250ARGB%252C%2520and%2520event%2520streams%252C%2520facilitating%2520downstream%2520evaluation%2520and%2520dataset%250Acreation.%2520Experimental%2520results%2520show%2520that%2520Raw2Event%2520can%2520generate%2520event%2520streams%250Aclosely%2520resembling%2520those%2520from%2520real%2520event%2520cameras%252C%2520while%2520benefiting%2520from%2520higher%250Aresolution%2520and%2520autofocus%2520capabilities.%2520The%2520system%2520also%2520supports%2520user-intuitive%250Aparameter%2520tuning%252C%2520enabling%2520flexible%2520adaptation%2520to%2520various%2520application%250Arequirements.%2520Finally%252C%2520we%2520deploy%2520the%2520system%2520on%2520a%2520Raspberry%2520Pi%2520for%2520real-time%250Aoperation%252C%2520providing%2520a%2520scalable%2520and%2520cost-effective%2520solution%2520for%2520event-based%250Avision%2520research%2520and%2520early-stage%2520system%2520development.%250A%2520%2520The%2520codes%2520are%2520available%2520online%253A%250Ahttps%253A//anonymous.4open.science/r/raw2event-BFF2/README.md.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raw2Event%3A%20Converting%20Raw%20Frame%20Camera%20into%20Event%20Camera&entry.906535625=Zijie%20Ning%20and%20Enmin%20Lin%20and%20Sudarshan%20R.%20Iyengar%20and%20Patrick%20Vandewalle&entry.1292438233=%20%20Event%20cameras%20offer%20unique%20advantages%20such%20as%20high%20temporal%20resolution%2C%20low%0Alatency%2C%20and%20high%20dynamic%20range%2C%20making%20them%20more%20and%20more%20popular%20for%20vision%0Atasks%20under%20challenging%20light%20conditions.%20However%2C%20their%20high%20cost%2C%20limited%0Aresolution%2C%20and%20lack%20of%20features%20such%20as%20autofocus%20hinder%20their%20broad%20adoption%2C%0Aparticularly%20for%20early-stage%20development%20and%20prototyping.%20In%20this%20work%2C%20we%0Apresent%20Raw2Event%2C%20a%20complete%20hardware-software%20system%20that%20enables%20real-time%0Aevent%20generation%20from%20low-cost%20raw%20frame-based%20cameras.%20By%20leveraging%20direct%0Aaccess%20to%20raw%20Bayer%20data%20and%20bypassing%20traditional%20image%20signal%20processors%0A%28ISP%29%2C%20our%20system%20is%20able%20to%20utilize%20the%20full%20potential%20of%20camera%20hardware%2C%0Adelivering%20higher%20dynamic%20range%2C%20higher%20resolution%2C%20and%20more%20faithful%20output%0Athan%20RGB-based%20frame-to-event%20converters.%0A%20%20Built%20upon%20the%20DVS-Voltmeter%20model%2C%20Raw2Event%20features%20a%20configurable%0Asimulation%20framework%20optimized%20for%20deployment%20on%20embedded%20platforms.%20We%20further%0Adesign%20a%20data%20acquisition%20pipeline%20that%20supports%20synchronized%20recording%20of%20raw%2C%0ARGB%2C%20and%20event%20streams%2C%20facilitating%20downstream%20evaluation%20and%20dataset%0Acreation.%20Experimental%20results%20show%20that%20Raw2Event%20can%20generate%20event%20streams%0Aclosely%20resembling%20those%20from%20real%20event%20cameras%2C%20while%20benefiting%20from%20higher%0Aresolution%20and%20autofocus%20capabilities.%20The%20system%20also%20supports%20user-intuitive%0Aparameter%20tuning%2C%20enabling%20flexible%20adaptation%20to%20various%20application%0Arequirements.%20Finally%2C%20we%20deploy%20the%20system%20on%20a%20Raspberry%20Pi%20for%20real-time%0Aoperation%2C%20providing%20a%20scalable%20and%20cost-effective%20solution%20for%20event-based%0Avision%20research%20and%20early-stage%20system%20development.%0A%20%20The%20codes%20are%20available%20online%3A%0Ahttps%3A//anonymous.4open.science/r/raw2event-BFF2/README.md.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06767v1&entry.124074799=Read"},
{"title": "UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets", "author": "Muhammad Shahbaz and Shaurya Agarwal", "abstract": "  This article presents UrbanTwin datasets - high-fidelity, realistic replicas\nof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.\nEach UrbanTwin dataset contains 10K annotated frames corresponding to one of\nthe public datasets. Annotations include 3D bounding boxes, instance\nsegmentation labels, and tracking IDs for six object classes, along with\nsemantic segmentation labels for nine classes. These datasets are synthesized\nusing emulated lidar sensors within realistic digital twins, modeled based on\nsurrounding geometry, road alignment at lane level, and the lane topology and\nvehicle movement patterns at intersections of the actual locations\ncorresponding to each real dataset. Due to the precise digital twin modeling,\nthe synthetic datasets are well aligned with their real counterparts, offering\nstrong standalone and augmentative value for training deep learning models on\ntasks such as 3D object detection, tracking, and semantic and instance\nsegmentation. We evaluate the alignment of the synthetic replicas through\nstatistical and structural similarity analysis with real data, and further\ndemonstrate their utility by training 3D object detection models solely on\nsynthetic data and testing them on real, unseen data. The high similarity\nscores and improved detection performance, compared to the models trained on\nreal data, indicate that the UrbanTwin datasets effectively enhance existing\nbenchmark datasets by increasing sample size and scene diversity. In addition,\nthe digital twins can be adapted to test custom scenarios by modifying the\ndesign and dynamics of the simulations. To our knowledge, these are the first\ndigitally synthesized datasets that can replace in-domain real-world datasets\nfor lidar perception tasks. UrbanTwin datasets are publicly available at\nhttps://dataverse.harvard.edu/dataverse/ucf-ut.\n", "link": "http://arxiv.org/abs/2509.06781v1", "date": "2025-09-08", "relevancy": 2.165, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5419}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5419}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanTwin%3A%20High-Fidelity%20Synthetic%20Replicas%20of%20Roadside%20Lidar%20Datasets&body=Title%3A%20UrbanTwin%3A%20High-Fidelity%20Synthetic%20Replicas%20of%20Roadside%20Lidar%20Datasets%0AAuthor%3A%20Muhammad%20Shahbaz%20and%20Shaurya%20Agarwal%0AAbstract%3A%20%20%20This%20article%20presents%20UrbanTwin%20datasets%20-%20high-fidelity%2C%20realistic%20replicas%0Aof%20three%20public%20roadside%20lidar%20datasets%3A%20LUMPI%2C%20V2X-Real-IC%2C%20and%20TUMTraf-I.%0AEach%20UrbanTwin%20dataset%20contains%2010K%20annotated%20frames%20corresponding%20to%20one%20of%0Athe%20public%20datasets.%20Annotations%20include%203D%20bounding%20boxes%2C%20instance%0Asegmentation%20labels%2C%20and%20tracking%20IDs%20for%20six%20object%20classes%2C%20along%20with%0Asemantic%20segmentation%20labels%20for%20nine%20classes.%20These%20datasets%20are%20synthesized%0Ausing%20emulated%20lidar%20sensors%20within%20realistic%20digital%20twins%2C%20modeled%20based%20on%0Asurrounding%20geometry%2C%20road%20alignment%20at%20lane%20level%2C%20and%20the%20lane%20topology%20and%0Avehicle%20movement%20patterns%20at%20intersections%20of%20the%20actual%20locations%0Acorresponding%20to%20each%20real%20dataset.%20Due%20to%20the%20precise%20digital%20twin%20modeling%2C%0Athe%20synthetic%20datasets%20are%20well%20aligned%20with%20their%20real%20counterparts%2C%20offering%0Astrong%20standalone%20and%20augmentative%20value%20for%20training%20deep%20learning%20models%20on%0Atasks%20such%20as%203D%20object%20detection%2C%20tracking%2C%20and%20semantic%20and%20instance%0Asegmentation.%20We%20evaluate%20the%20alignment%20of%20the%20synthetic%20replicas%20through%0Astatistical%20and%20structural%20similarity%20analysis%20with%20real%20data%2C%20and%20further%0Ademonstrate%20their%20utility%20by%20training%203D%20object%20detection%20models%20solely%20on%0Asynthetic%20data%20and%20testing%20them%20on%20real%2C%20unseen%20data.%20The%20high%20similarity%0Ascores%20and%20improved%20detection%20performance%2C%20compared%20to%20the%20models%20trained%20on%0Areal%20data%2C%20indicate%20that%20the%20UrbanTwin%20datasets%20effectively%20enhance%20existing%0Abenchmark%20datasets%20by%20increasing%20sample%20size%20and%20scene%20diversity.%20In%20addition%2C%0Athe%20digital%20twins%20can%20be%20adapted%20to%20test%20custom%20scenarios%20by%20modifying%20the%0Adesign%20and%20dynamics%20of%20the%20simulations.%20To%20our%20knowledge%2C%20these%20are%20the%20first%0Adigitally%20synthesized%20datasets%20that%20can%20replace%20in-domain%20real-world%20datasets%0Afor%20lidar%20perception%20tasks.%20UrbanTwin%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//dataverse.harvard.edu/dataverse/ucf-ut.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanTwin%253A%2520High-Fidelity%2520Synthetic%2520Replicas%2520of%2520Roadside%2520Lidar%2520Datasets%26entry.906535625%3DMuhammad%2520Shahbaz%2520and%2520Shaurya%2520Agarwal%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520UrbanTwin%2520datasets%2520-%2520high-fidelity%252C%2520realistic%2520replicas%250Aof%2520three%2520public%2520roadside%2520lidar%2520datasets%253A%2520LUMPI%252C%2520V2X-Real-IC%252C%2520and%2520TUMTraf-I.%250AEach%2520UrbanTwin%2520dataset%2520contains%252010K%2520annotated%2520frames%2520corresponding%2520to%2520one%2520of%250Athe%2520public%2520datasets.%2520Annotations%2520include%25203D%2520bounding%2520boxes%252C%2520instance%250Asegmentation%2520labels%252C%2520and%2520tracking%2520IDs%2520for%2520six%2520object%2520classes%252C%2520along%2520with%250Asemantic%2520segmentation%2520labels%2520for%2520nine%2520classes.%2520These%2520datasets%2520are%2520synthesized%250Ausing%2520emulated%2520lidar%2520sensors%2520within%2520realistic%2520digital%2520twins%252C%2520modeled%2520based%2520on%250Asurrounding%2520geometry%252C%2520road%2520alignment%2520at%2520lane%2520level%252C%2520and%2520the%2520lane%2520topology%2520and%250Avehicle%2520movement%2520patterns%2520at%2520intersections%2520of%2520the%2520actual%2520locations%250Acorresponding%2520to%2520each%2520real%2520dataset.%2520Due%2520to%2520the%2520precise%2520digital%2520twin%2520modeling%252C%250Athe%2520synthetic%2520datasets%2520are%2520well%2520aligned%2520with%2520their%2520real%2520counterparts%252C%2520offering%250Astrong%2520standalone%2520and%2520augmentative%2520value%2520for%2520training%2520deep%2520learning%2520models%2520on%250Atasks%2520such%2520as%25203D%2520object%2520detection%252C%2520tracking%252C%2520and%2520semantic%2520and%2520instance%250Asegmentation.%2520We%2520evaluate%2520the%2520alignment%2520of%2520the%2520synthetic%2520replicas%2520through%250Astatistical%2520and%2520structural%2520similarity%2520analysis%2520with%2520real%2520data%252C%2520and%2520further%250Ademonstrate%2520their%2520utility%2520by%2520training%25203D%2520object%2520detection%2520models%2520solely%2520on%250Asynthetic%2520data%2520and%2520testing%2520them%2520on%2520real%252C%2520unseen%2520data.%2520The%2520high%2520similarity%250Ascores%2520and%2520improved%2520detection%2520performance%252C%2520compared%2520to%2520the%2520models%2520trained%2520on%250Areal%2520data%252C%2520indicate%2520that%2520the%2520UrbanTwin%2520datasets%2520effectively%2520enhance%2520existing%250Abenchmark%2520datasets%2520by%2520increasing%2520sample%2520size%2520and%2520scene%2520diversity.%2520In%2520addition%252C%250Athe%2520digital%2520twins%2520can%2520be%2520adapted%2520to%2520test%2520custom%2520scenarios%2520by%2520modifying%2520the%250Adesign%2520and%2520dynamics%2520of%2520the%2520simulations.%2520To%2520our%2520knowledge%252C%2520these%2520are%2520the%2520first%250Adigitally%2520synthesized%2520datasets%2520that%2520can%2520replace%2520in-domain%2520real-world%2520datasets%250Afor%2520lidar%2520perception%2520tasks.%2520UrbanTwin%2520datasets%2520are%2520publicly%2520available%2520at%250Ahttps%253A//dataverse.harvard.edu/dataverse/ucf-ut.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanTwin%3A%20High-Fidelity%20Synthetic%20Replicas%20of%20Roadside%20Lidar%20Datasets&entry.906535625=Muhammad%20Shahbaz%20and%20Shaurya%20Agarwal&entry.1292438233=%20%20This%20article%20presents%20UrbanTwin%20datasets%20-%20high-fidelity%2C%20realistic%20replicas%0Aof%20three%20public%20roadside%20lidar%20datasets%3A%20LUMPI%2C%20V2X-Real-IC%2C%20and%20TUMTraf-I.%0AEach%20UrbanTwin%20dataset%20contains%2010K%20annotated%20frames%20corresponding%20to%20one%20of%0Athe%20public%20datasets.%20Annotations%20include%203D%20bounding%20boxes%2C%20instance%0Asegmentation%20labels%2C%20and%20tracking%20IDs%20for%20six%20object%20classes%2C%20along%20with%0Asemantic%20segmentation%20labels%20for%20nine%20classes.%20These%20datasets%20are%20synthesized%0Ausing%20emulated%20lidar%20sensors%20within%20realistic%20digital%20twins%2C%20modeled%20based%20on%0Asurrounding%20geometry%2C%20road%20alignment%20at%20lane%20level%2C%20and%20the%20lane%20topology%20and%0Avehicle%20movement%20patterns%20at%20intersections%20of%20the%20actual%20locations%0Acorresponding%20to%20each%20real%20dataset.%20Due%20to%20the%20precise%20digital%20twin%20modeling%2C%0Athe%20synthetic%20datasets%20are%20well%20aligned%20with%20their%20real%20counterparts%2C%20offering%0Astrong%20standalone%20and%20augmentative%20value%20for%20training%20deep%20learning%20models%20on%0Atasks%20such%20as%203D%20object%20detection%2C%20tracking%2C%20and%20semantic%20and%20instance%0Asegmentation.%20We%20evaluate%20the%20alignment%20of%20the%20synthetic%20replicas%20through%0Astatistical%20and%20structural%20similarity%20analysis%20with%20real%20data%2C%20and%20further%0Ademonstrate%20their%20utility%20by%20training%203D%20object%20detection%20models%20solely%20on%0Asynthetic%20data%20and%20testing%20them%20on%20real%2C%20unseen%20data.%20The%20high%20similarity%0Ascores%20and%20improved%20detection%20performance%2C%20compared%20to%20the%20models%20trained%20on%0Areal%20data%2C%20indicate%20that%20the%20UrbanTwin%20datasets%20effectively%20enhance%20existing%0Abenchmark%20datasets%20by%20increasing%20sample%20size%20and%20scene%20diversity.%20In%20addition%2C%0Athe%20digital%20twins%20can%20be%20adapted%20to%20test%20custom%20scenarios%20by%20modifying%20the%0Adesign%20and%20dynamics%20of%20the%20simulations.%20To%20our%20knowledge%2C%20these%20are%20the%20first%0Adigitally%20synthesized%20datasets%20that%20can%20replace%20in-domain%20real-world%20datasets%0Afor%20lidar%20perception%20tasks.%20UrbanTwin%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//dataverse.harvard.edu/dataverse/ucf-ut.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06781v1&entry.124074799=Read"},
{"title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and\n  Falling Simulations", "author": "Chengtian Ma and Yunyue Wei and Chenhui Zuo and Chen Zhang and Yanan Sui", "abstract": "  Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.\n", "link": "http://arxiv.org/abs/2506.09383v2", "date": "2025-09-08", "relevancy": 2.1584, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5688}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5389}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bipedal%20Balance%20Control%20with%20Whole-body%20Musculoskeletal%20Standing%20and%0A%20%20Falling%20Simulations&body=Title%3A%20Bipedal%20Balance%20Control%20with%20Whole-body%20Musculoskeletal%20Standing%20and%0A%20%20Falling%20Simulations%0AAuthor%3A%20Chengtian%20Ma%20and%20Yunyue%20Wei%20and%20Chenhui%20Zuo%20and%20Chen%20Zhang%20and%20Yanan%20Sui%0AAbstract%3A%20%20%20Balance%20control%20is%20important%20for%20human%20and%20bipedal%20robotic%20systems.%20While%0Adynamic%20balance%20during%20locomotion%20has%20received%20considerable%20attention%2C%0Aquantitative%20understanding%20of%20static%20balance%20and%20falling%20remains%20limited.%20This%0Awork%20presents%20a%20hierarchical%20control%20pipeline%20for%20simulating%20human%20balance%20via%0Aa%20comprehensive%20whole-body%20musculoskeletal%20system.%20We%20identified%20spatiotemporal%0Adynamics%20of%20balancing%20during%20stable%20standing%2C%20revealed%20the%20impact%20of%20muscle%0Ainjury%20on%20balancing%20behavior%2C%20and%20generated%20fall%20contact%20patterns%20that%20aligned%0Awith%20clinical%20data.%20Furthermore%2C%20our%20simulated%20hip%20exoskeleton%20assistance%0Ademonstrated%20improvement%20in%20balance%20maintenance%20and%20reduced%20muscle%20effort%20under%0Aperturbation.%20This%20work%20offers%20unique%20muscle-level%20insights%20into%20human%20balance%0Adynamics%20that%20are%20challenging%20to%20capture%20experimentally.%20It%20could%20provide%20a%0Afoundation%20for%20developing%20targeted%20interventions%20for%20individuals%20with%20balance%0Aimpairments%20and%20support%20the%20advancement%20of%20humanoid%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBipedal%2520Balance%2520Control%2520with%2520Whole-body%2520Musculoskeletal%2520Standing%2520and%250A%2520%2520Falling%2520Simulations%26entry.906535625%3DChengtian%2520Ma%2520and%2520Yunyue%2520Wei%2520and%2520Chenhui%2520Zuo%2520and%2520Chen%2520Zhang%2520and%2520Yanan%2520Sui%26entry.1292438233%3D%2520%2520Balance%2520control%2520is%2520important%2520for%2520human%2520and%2520bipedal%2520robotic%2520systems.%2520While%250Adynamic%2520balance%2520during%2520locomotion%2520has%2520received%2520considerable%2520attention%252C%250Aquantitative%2520understanding%2520of%2520static%2520balance%2520and%2520falling%2520remains%2520limited.%2520This%250Awork%2520presents%2520a%2520hierarchical%2520control%2520pipeline%2520for%2520simulating%2520human%2520balance%2520via%250Aa%2520comprehensive%2520whole-body%2520musculoskeletal%2520system.%2520We%2520identified%2520spatiotemporal%250Adynamics%2520of%2520balancing%2520during%2520stable%2520standing%252C%2520revealed%2520the%2520impact%2520of%2520muscle%250Ainjury%2520on%2520balancing%2520behavior%252C%2520and%2520generated%2520fall%2520contact%2520patterns%2520that%2520aligned%250Awith%2520clinical%2520data.%2520Furthermore%252C%2520our%2520simulated%2520hip%2520exoskeleton%2520assistance%250Ademonstrated%2520improvement%2520in%2520balance%2520maintenance%2520and%2520reduced%2520muscle%2520effort%2520under%250Aperturbation.%2520This%2520work%2520offers%2520unique%2520muscle-level%2520insights%2520into%2520human%2520balance%250Adynamics%2520that%2520are%2520challenging%2520to%2520capture%2520experimentally.%2520It%2520could%2520provide%2520a%250Afoundation%2520for%2520developing%2520targeted%2520interventions%2520for%2520individuals%2520with%2520balance%250Aimpairments%2520and%2520support%2520the%2520advancement%2520of%2520humanoid%2520robotic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bipedal%20Balance%20Control%20with%20Whole-body%20Musculoskeletal%20Standing%20and%0A%20%20Falling%20Simulations&entry.906535625=Chengtian%20Ma%20and%20Yunyue%20Wei%20and%20Chenhui%20Zuo%20and%20Chen%20Zhang%20and%20Yanan%20Sui&entry.1292438233=%20%20Balance%20control%20is%20important%20for%20human%20and%20bipedal%20robotic%20systems.%20While%0Adynamic%20balance%20during%20locomotion%20has%20received%20considerable%20attention%2C%0Aquantitative%20understanding%20of%20static%20balance%20and%20falling%20remains%20limited.%20This%0Awork%20presents%20a%20hierarchical%20control%20pipeline%20for%20simulating%20human%20balance%20via%0Aa%20comprehensive%20whole-body%20musculoskeletal%20system.%20We%20identified%20spatiotemporal%0Adynamics%20of%20balancing%20during%20stable%20standing%2C%20revealed%20the%20impact%20of%20muscle%0Ainjury%20on%20balancing%20behavior%2C%20and%20generated%20fall%20contact%20patterns%20that%20aligned%0Awith%20clinical%20data.%20Furthermore%2C%20our%20simulated%20hip%20exoskeleton%20assistance%0Ademonstrated%20improvement%20in%20balance%20maintenance%20and%20reduced%20muscle%20effort%20under%0Aperturbation.%20This%20work%20offers%20unique%20muscle-level%20insights%20into%20human%20balance%0Adynamics%20that%20are%20challenging%20to%20capture%20experimentally.%20It%20could%20provide%20a%0Afoundation%20for%20developing%20targeted%20interventions%20for%20individuals%20with%20balance%0Aimpairments%20and%20support%20the%20advancement%20of%20humanoid%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09383v2&entry.124074799=Read"},
{"title": "Corner Cases: How Size and Position of Objects Challenge\n  ImageNet-Trained Models", "author": "Mishal Fatima and Steffen Jung and Margret Keuper", "abstract": "  Backgrounds in images play a major role in contributing to spurious\ncorrelations among different data points. Owing to aesthetic preferences of\nhumans capturing the images, datasets can exhibit positional (location of the\nobject within a given frame) and size (region-of-interest to image ratio)\nbiases for different classes. In this paper, we show that these biases can\nimpact how much a model relies on spurious features in the background to make\nits predictions. To better illustrate our findings, we propose a synthetic\ndataset derived from ImageNet-1k, Hard-Spurious-ImageNet, which contains images\nwith various backgrounds, object positions, and object sizes. By evaluating the\ndataset on different pretrained models, we find that most models rely heavily\non spurious features in the background when the region-of-interest (ROI) to\nimage ratio is small and the object is far from the center of the image.\nMoreover, we also show that current methods that aim to mitigate harmful\nspurious features, do not take into account these factors, hence fail to\nachieve considerable performance gains for worst-group accuracies when the size\nand location of core features in an image change. The dataset and\nimplementation code are available at\nhttps://github.com/Mishalfatima/Corner_Cases.\n", "link": "http://arxiv.org/abs/2505.03569v2", "date": "2025-09-08", "relevancy": 2.1499, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5629}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5375}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Corner%20Cases%3A%20How%20Size%20and%20Position%20of%20Objects%20Challenge%0A%20%20ImageNet-Trained%20Models&body=Title%3A%20Corner%20Cases%3A%20How%20Size%20and%20Position%20of%20Objects%20Challenge%0A%20%20ImageNet-Trained%20Models%0AAuthor%3A%20Mishal%20Fatima%20and%20Steffen%20Jung%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Backgrounds%20in%20images%20play%20a%20major%20role%20in%20contributing%20to%20spurious%0Acorrelations%20among%20different%20data%20points.%20Owing%20to%20aesthetic%20preferences%20of%0Ahumans%20capturing%20the%20images%2C%20datasets%20can%20exhibit%20positional%20%28location%20of%20the%0Aobject%20within%20a%20given%20frame%29%20and%20size%20%28region-of-interest%20to%20image%20ratio%29%0Abiases%20for%20different%20classes.%20In%20this%20paper%2C%20we%20show%20that%20these%20biases%20can%0Aimpact%20how%20much%20a%20model%20relies%20on%20spurious%20features%20in%20the%20background%20to%20make%0Aits%20predictions.%20To%20better%20illustrate%20our%20findings%2C%20we%20propose%20a%20synthetic%0Adataset%20derived%20from%20ImageNet-1k%2C%20Hard-Spurious-ImageNet%2C%20which%20contains%20images%0Awith%20various%20backgrounds%2C%20object%20positions%2C%20and%20object%20sizes.%20By%20evaluating%20the%0Adataset%20on%20different%20pretrained%20models%2C%20we%20find%20that%20most%20models%20rely%20heavily%0Aon%20spurious%20features%20in%20the%20background%20when%20the%20region-of-interest%20%28ROI%29%20to%0Aimage%20ratio%20is%20small%20and%20the%20object%20is%20far%20from%20the%20center%20of%20the%20image.%0AMoreover%2C%20we%20also%20show%20that%20current%20methods%20that%20aim%20to%20mitigate%20harmful%0Aspurious%20features%2C%20do%20not%20take%20into%20account%20these%20factors%2C%20hence%20fail%20to%0Aachieve%20considerable%20performance%20gains%20for%20worst-group%20accuracies%20when%20the%20size%0Aand%20location%20of%20core%20features%20in%20an%20image%20change.%20The%20dataset%20and%0Aimplementation%20code%20are%20available%20at%0Ahttps%3A//github.com/Mishalfatima/Corner_Cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorner%2520Cases%253A%2520How%2520Size%2520and%2520Position%2520of%2520Objects%2520Challenge%250A%2520%2520ImageNet-Trained%2520Models%26entry.906535625%3DMishal%2520Fatima%2520and%2520Steffen%2520Jung%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Backgrounds%2520in%2520images%2520play%2520a%2520major%2520role%2520in%2520contributing%2520to%2520spurious%250Acorrelations%2520among%2520different%2520data%2520points.%2520Owing%2520to%2520aesthetic%2520preferences%2520of%250Ahumans%2520capturing%2520the%2520images%252C%2520datasets%2520can%2520exhibit%2520positional%2520%2528location%2520of%2520the%250Aobject%2520within%2520a%2520given%2520frame%2529%2520and%2520size%2520%2528region-of-interest%2520to%2520image%2520ratio%2529%250Abiases%2520for%2520different%2520classes.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520these%2520biases%2520can%250Aimpact%2520how%2520much%2520a%2520model%2520relies%2520on%2520spurious%2520features%2520in%2520the%2520background%2520to%2520make%250Aits%2520predictions.%2520To%2520better%2520illustrate%2520our%2520findings%252C%2520we%2520propose%2520a%2520synthetic%250Adataset%2520derived%2520from%2520ImageNet-1k%252C%2520Hard-Spurious-ImageNet%252C%2520which%2520contains%2520images%250Awith%2520various%2520backgrounds%252C%2520object%2520positions%252C%2520and%2520object%2520sizes.%2520By%2520evaluating%2520the%250Adataset%2520on%2520different%2520pretrained%2520models%252C%2520we%2520find%2520that%2520most%2520models%2520rely%2520heavily%250Aon%2520spurious%2520features%2520in%2520the%2520background%2520when%2520the%2520region-of-interest%2520%2528ROI%2529%2520to%250Aimage%2520ratio%2520is%2520small%2520and%2520the%2520object%2520is%2520far%2520from%2520the%2520center%2520of%2520the%2520image.%250AMoreover%252C%2520we%2520also%2520show%2520that%2520current%2520methods%2520that%2520aim%2520to%2520mitigate%2520harmful%250Aspurious%2520features%252C%2520do%2520not%2520take%2520into%2520account%2520these%2520factors%252C%2520hence%2520fail%2520to%250Aachieve%2520considerable%2520performance%2520gains%2520for%2520worst-group%2520accuracies%2520when%2520the%2520size%250Aand%2520location%2520of%2520core%2520features%2520in%2520an%2520image%2520change.%2520The%2520dataset%2520and%250Aimplementation%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Mishalfatima/Corner_Cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Corner%20Cases%3A%20How%20Size%20and%20Position%20of%20Objects%20Challenge%0A%20%20ImageNet-Trained%20Models&entry.906535625=Mishal%20Fatima%20and%20Steffen%20Jung%20and%20Margret%20Keuper&entry.1292438233=%20%20Backgrounds%20in%20images%20play%20a%20major%20role%20in%20contributing%20to%20spurious%0Acorrelations%20among%20different%20data%20points.%20Owing%20to%20aesthetic%20preferences%20of%0Ahumans%20capturing%20the%20images%2C%20datasets%20can%20exhibit%20positional%20%28location%20of%20the%0Aobject%20within%20a%20given%20frame%29%20and%20size%20%28region-of-interest%20to%20image%20ratio%29%0Abiases%20for%20different%20classes.%20In%20this%20paper%2C%20we%20show%20that%20these%20biases%20can%0Aimpact%20how%20much%20a%20model%20relies%20on%20spurious%20features%20in%20the%20background%20to%20make%0Aits%20predictions.%20To%20better%20illustrate%20our%20findings%2C%20we%20propose%20a%20synthetic%0Adataset%20derived%20from%20ImageNet-1k%2C%20Hard-Spurious-ImageNet%2C%20which%20contains%20images%0Awith%20various%20backgrounds%2C%20object%20positions%2C%20and%20object%20sizes.%20By%20evaluating%20the%0Adataset%20on%20different%20pretrained%20models%2C%20we%20find%20that%20most%20models%20rely%20heavily%0Aon%20spurious%20features%20in%20the%20background%20when%20the%20region-of-interest%20%28ROI%29%20to%0Aimage%20ratio%20is%20small%20and%20the%20object%20is%20far%20from%20the%20center%20of%20the%20image.%0AMoreover%2C%20we%20also%20show%20that%20current%20methods%20that%20aim%20to%20mitigate%20harmful%0Aspurious%20features%2C%20do%20not%20take%20into%20account%20these%20factors%2C%20hence%20fail%20to%0Aachieve%20considerable%20performance%20gains%20for%20worst-group%20accuracies%20when%20the%20size%0Aand%20location%20of%20core%20features%20in%20an%20image%20change.%20The%20dataset%20and%0Aimplementation%20code%20are%20available%20at%0Ahttps%3A//github.com/Mishalfatima/Corner_Cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03569v2&entry.124074799=Read"},
{"title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising", "author": "Yichao Liu and YueYang Teng", "abstract": "  Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.\n", "link": "http://arxiv.org/abs/2509.06591v1", "date": "2025-09-08", "relevancy": 2.1483, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5602}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5265}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising&body=Title%3A%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising%0AAuthor%3A%20Yichao%20Liu%20and%20YueYang%20Teng%0AAbstract%3A%20%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%0Ahave%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%0Asignificantly%20reducing%20radiation%20exposure.%20However%2C%20this%20reduction%20often%0Aresults%20in%20increased%20noise%20and%20artifacts%2C%20which%20can%20compromise%20diagnostic%0Aaccuracy.%20Consequently%2C%20denoising%20for%20LDCT/PET%20has%20become%20a%20vital%20area%20of%0Aresearch%20aimed%20at%20enhancing%20image%20quality%20while%20maintaining%20radiation%20safety.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%0Awhich%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%0Aupsampling%20module.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%0Ainteraction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%0Awhile%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%0AWe%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%0AExperimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%0Aperformance%20compared%20to%20existing%20methods%2C%20while%20maintaining%20a%20lightweight%20model%0Asize%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20This%0Amakes%20our%20approach%20highly%20practical%20for%20real-world%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Swin%2520Attention%2520Networks%2520for%2520Simultaneously%2520Low-Dose%2520PET%2520and%2520CT%250A%2520%2520Denoising%26entry.906535625%3DYichao%2520Liu%2520and%2520YueYang%2520Teng%26entry.1292438233%3D%2520%2520Low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529%250Ahave%2520emerged%2520as%2520safer%2520alternatives%2520to%2520conventional%2520imaging%2520modalities%2520by%250Asignificantly%2520reducing%2520radiation%2520exposure.%2520However%252C%2520this%2520reduction%2520often%250Aresults%2520in%2520increased%2520noise%2520and%2520artifacts%252C%2520which%2520can%2520compromise%2520diagnostic%250Aaccuracy.%2520Consequently%252C%2520denoising%2520for%2520LDCT/PET%2520has%2520become%2520a%2520vital%2520area%2520of%250Aresearch%2520aimed%2520at%2520enhancing%2520image%2520quality%2520while%2520maintaining%2520radiation%2520safety.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520Hybrid%2520Swin%2520Attention%2520Network%2520%2528HSANet%2529%252C%250Awhich%2520incorporates%2520Efficient%2520Global%2520Attention%2520%2528EGA%2529%2520modules%2520and%2520a%2520hybrid%250Aupsampling%2520module.%2520The%2520EGA%2520modules%2520enhance%2520both%2520spatial%2520and%2520channel-wise%250Ainteraction%252C%2520improving%2520the%2520network%2527s%2520capacity%2520to%2520capture%2520relevant%2520features%252C%250Awhile%2520the%2520hybrid%2520upsampling%2520module%2520mitigates%2520the%2520risk%2520of%2520overfitting%2520to%2520noise.%250AWe%2520validate%2520the%2520proposed%2520approach%2520using%2520a%2520publicly%2520available%2520LDCT/PET%2520dataset.%250AExperimental%2520results%2520demonstrate%2520that%2520HSANet%2520achieves%2520superior%2520denoising%250Aperformance%2520compared%2520to%2520existing%2520methods%252C%2520while%2520maintaining%2520a%2520lightweight%2520model%250Asize%2520suitable%2520for%2520deployment%2520on%2520GPUs%2520with%2520standard%2520memory%2520configurations.%2520This%250Amakes%2520our%2520approach%2520highly%2520practical%2520for%2520real-world%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising&entry.906535625=Yichao%20Liu%20and%20YueYang%20Teng&entry.1292438233=%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%0Ahave%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%0Asignificantly%20reducing%20radiation%20exposure.%20However%2C%20this%20reduction%20often%0Aresults%20in%20increased%20noise%20and%20artifacts%2C%20which%20can%20compromise%20diagnostic%0Aaccuracy.%20Consequently%2C%20denoising%20for%20LDCT/PET%20has%20become%20a%20vital%20area%20of%0Aresearch%20aimed%20at%20enhancing%20image%20quality%20while%20maintaining%20radiation%20safety.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%0Awhich%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%0Aupsampling%20module.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%0Ainteraction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%0Awhile%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%0AWe%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%0AExperimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%0Aperformance%20compared%20to%20existing%20methods%2C%20while%20maintaining%20a%20lightweight%20model%0Asize%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20This%0Amakes%20our%20approach%20highly%20practical%20for%20real-world%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06591v1&entry.124074799=Read"},
{"title": "FoMo4Wheat: Toward reliable crop vision foundation models with globally\n  curated data", "author": "Bing Han and Chen Zhu and Dong Han and Rui Yu and Songliang Cao and Jianhui Wu and Scott Chapman and Zijian Wang and Bangyou Zheng and Wei Guo and Marie Weiss and Benoit de Solan and Andreas Hund and Lukas Roth and Kirchgessner Norbert and Andrea Visioni and Yufeng Ge and Wenjuan Li and Alexis Comar and Dong Jiang and Dejun Han and Fred Baret and Yanfeng Ding and Hao Lu and Shouyang Liu", "abstract": "  Vision-driven field monitoring is central to digital agriculture, yet models\nbuilt on general-domain pretrained backbones often fail to generalize across\ntasks, owing to the interaction of fine, variable canopy structures with\nfluctuating field conditions. We present FoMo4Wheat, one of the first\ncrop-domain vision foundation model pretrained with self-supervision on\nImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5\nmillion high-resolution images collected over a decade at 30 global sites,\nspanning >2,000 genotypes and >500 environmental conditions). This\nwheat-specific pretraining yields representations that are robust for wheat and\ntransferable to other crops and weeds. Across ten in-field vision tasks at\ncanopy and organ levels, FoMo4Wheat models consistently outperform\nstate-of-the-art models pretrained on general-domain dataset. These results\ndemonstrate the value of crop-specific foundation models for reliable in-field\nperception and chart a path toward a universal crop foundation model with\ncross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat\ndataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat\nand https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:\nhttps://fomo4wheat.phenix-lab.com/.\n", "link": "http://arxiv.org/abs/2509.06907v1", "date": "2025-09-08", "relevancy": 2.1473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoMo4Wheat%3A%20Toward%20reliable%20crop%20vision%20foundation%20models%20with%20globally%0A%20%20curated%20data&body=Title%3A%20FoMo4Wheat%3A%20Toward%20reliable%20crop%20vision%20foundation%20models%20with%20globally%0A%20%20curated%20data%0AAuthor%3A%20Bing%20Han%20and%20Chen%20Zhu%20and%20Dong%20Han%20and%20Rui%20Yu%20and%20Songliang%20Cao%20and%20Jianhui%20Wu%20and%20Scott%20Chapman%20and%20Zijian%20Wang%20and%20Bangyou%20Zheng%20and%20Wei%20Guo%20and%20Marie%20Weiss%20and%20Benoit%20de%20Solan%20and%20Andreas%20Hund%20and%20Lukas%20Roth%20and%20Kirchgessner%20Norbert%20and%20Andrea%20Visioni%20and%20Yufeng%20Ge%20and%20Wenjuan%20Li%20and%20Alexis%20Comar%20and%20Dong%20Jiang%20and%20Dejun%20Han%20and%20Fred%20Baret%20and%20Yanfeng%20Ding%20and%20Hao%20Lu%20and%20Shouyang%20Liu%0AAbstract%3A%20%20%20Vision-driven%20field%20monitoring%20is%20central%20to%20digital%20agriculture%2C%20yet%20models%0Abuilt%20on%20general-domain%20pretrained%20backbones%20often%20fail%20to%20generalize%20across%0Atasks%2C%20owing%20to%20the%20interaction%20of%20fine%2C%20variable%20canopy%20structures%20with%0Afluctuating%20field%20conditions.%20We%20present%20FoMo4Wheat%2C%20one%20of%20the%20first%0Acrop-domain%20vision%20foundation%20model%20pretrained%20with%20self-supervision%20on%0AImAg4Wheat%2C%20the%20largest%20and%20most%20diverse%20wheat%20image%20dataset%20to%20date%20%282.5%0Amillion%20high-resolution%20images%20collected%20over%20a%20decade%20at%2030%20global%20sites%2C%0Aspanning%20%3E2%2C000%20genotypes%20and%20%3E500%20environmental%20conditions%29.%20This%0Awheat-specific%20pretraining%20yields%20representations%20that%20are%20robust%20for%20wheat%20and%0Atransferable%20to%20other%20crops%20and%20weeds.%20Across%20ten%20in-field%20vision%20tasks%20at%0Acanopy%20and%20organ%20levels%2C%20FoMo4Wheat%20models%20consistently%20outperform%0Astate-of-the-art%20models%20pretrained%20on%20general-domain%20dataset.%20These%20results%0Ademonstrate%20the%20value%20of%20crop-specific%20foundation%20models%20for%20reliable%20in-field%0Aperception%20and%20chart%20a%20path%20toward%20a%20universal%20crop%20foundation%20model%20with%0Across-species%20and%20cross-task%20capabilities.%20FoMo4Wheat%20models%20and%20the%20ImAg4Wheat%0Adataset%20are%20publicly%20available%20online%3A%20https%3A//github.com/PheniX-Lab/FoMo4Wheat%0Aand%20https%3A//huggingface.co/PheniX-Lab/FoMo4Wheat.%20The%20demonstration%20website%20is%3A%0Ahttps%3A//fomo4wheat.phenix-lab.com/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoMo4Wheat%253A%2520Toward%2520reliable%2520crop%2520vision%2520foundation%2520models%2520with%2520globally%250A%2520%2520curated%2520data%26entry.906535625%3DBing%2520Han%2520and%2520Chen%2520Zhu%2520and%2520Dong%2520Han%2520and%2520Rui%2520Yu%2520and%2520Songliang%2520Cao%2520and%2520Jianhui%2520Wu%2520and%2520Scott%2520Chapman%2520and%2520Zijian%2520Wang%2520and%2520Bangyou%2520Zheng%2520and%2520Wei%2520Guo%2520and%2520Marie%2520Weiss%2520and%2520Benoit%2520de%2520Solan%2520and%2520Andreas%2520Hund%2520and%2520Lukas%2520Roth%2520and%2520Kirchgessner%2520Norbert%2520and%2520Andrea%2520Visioni%2520and%2520Yufeng%2520Ge%2520and%2520Wenjuan%2520Li%2520and%2520Alexis%2520Comar%2520and%2520Dong%2520Jiang%2520and%2520Dejun%2520Han%2520and%2520Fred%2520Baret%2520and%2520Yanfeng%2520Ding%2520and%2520Hao%2520Lu%2520and%2520Shouyang%2520Liu%26entry.1292438233%3D%2520%2520Vision-driven%2520field%2520monitoring%2520is%2520central%2520to%2520digital%2520agriculture%252C%2520yet%2520models%250Abuilt%2520on%2520general-domain%2520pretrained%2520backbones%2520often%2520fail%2520to%2520generalize%2520across%250Atasks%252C%2520owing%2520to%2520the%2520interaction%2520of%2520fine%252C%2520variable%2520canopy%2520structures%2520with%250Afluctuating%2520field%2520conditions.%2520We%2520present%2520FoMo4Wheat%252C%2520one%2520of%2520the%2520first%250Acrop-domain%2520vision%2520foundation%2520model%2520pretrained%2520with%2520self-supervision%2520on%250AImAg4Wheat%252C%2520the%2520largest%2520and%2520most%2520diverse%2520wheat%2520image%2520dataset%2520to%2520date%2520%25282.5%250Amillion%2520high-resolution%2520images%2520collected%2520over%2520a%2520decade%2520at%252030%2520global%2520sites%252C%250Aspanning%2520%253E2%252C000%2520genotypes%2520and%2520%253E500%2520environmental%2520conditions%2529.%2520This%250Awheat-specific%2520pretraining%2520yields%2520representations%2520that%2520are%2520robust%2520for%2520wheat%2520and%250Atransferable%2520to%2520other%2520crops%2520and%2520weeds.%2520Across%2520ten%2520in-field%2520vision%2520tasks%2520at%250Acanopy%2520and%2520organ%2520levels%252C%2520FoMo4Wheat%2520models%2520consistently%2520outperform%250Astate-of-the-art%2520models%2520pretrained%2520on%2520general-domain%2520dataset.%2520These%2520results%250Ademonstrate%2520the%2520value%2520of%2520crop-specific%2520foundation%2520models%2520for%2520reliable%2520in-field%250Aperception%2520and%2520chart%2520a%2520path%2520toward%2520a%2520universal%2520crop%2520foundation%2520model%2520with%250Across-species%2520and%2520cross-task%2520capabilities.%2520FoMo4Wheat%2520models%2520and%2520the%2520ImAg4Wheat%250Adataset%2520are%2520publicly%2520available%2520online%253A%2520https%253A//github.com/PheniX-Lab/FoMo4Wheat%250Aand%2520https%253A//huggingface.co/PheniX-Lab/FoMo4Wheat.%2520The%2520demonstration%2520website%2520is%253A%250Ahttps%253A//fomo4wheat.phenix-lab.com/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoMo4Wheat%3A%20Toward%20reliable%20crop%20vision%20foundation%20models%20with%20globally%0A%20%20curated%20data&entry.906535625=Bing%20Han%20and%20Chen%20Zhu%20and%20Dong%20Han%20and%20Rui%20Yu%20and%20Songliang%20Cao%20and%20Jianhui%20Wu%20and%20Scott%20Chapman%20and%20Zijian%20Wang%20and%20Bangyou%20Zheng%20and%20Wei%20Guo%20and%20Marie%20Weiss%20and%20Benoit%20de%20Solan%20and%20Andreas%20Hund%20and%20Lukas%20Roth%20and%20Kirchgessner%20Norbert%20and%20Andrea%20Visioni%20and%20Yufeng%20Ge%20and%20Wenjuan%20Li%20and%20Alexis%20Comar%20and%20Dong%20Jiang%20and%20Dejun%20Han%20and%20Fred%20Baret%20and%20Yanfeng%20Ding%20and%20Hao%20Lu%20and%20Shouyang%20Liu&entry.1292438233=%20%20Vision-driven%20field%20monitoring%20is%20central%20to%20digital%20agriculture%2C%20yet%20models%0Abuilt%20on%20general-domain%20pretrained%20backbones%20often%20fail%20to%20generalize%20across%0Atasks%2C%20owing%20to%20the%20interaction%20of%20fine%2C%20variable%20canopy%20structures%20with%0Afluctuating%20field%20conditions.%20We%20present%20FoMo4Wheat%2C%20one%20of%20the%20first%0Acrop-domain%20vision%20foundation%20model%20pretrained%20with%20self-supervision%20on%0AImAg4Wheat%2C%20the%20largest%20and%20most%20diverse%20wheat%20image%20dataset%20to%20date%20%282.5%0Amillion%20high-resolution%20images%20collected%20over%20a%20decade%20at%2030%20global%20sites%2C%0Aspanning%20%3E2%2C000%20genotypes%20and%20%3E500%20environmental%20conditions%29.%20This%0Awheat-specific%20pretraining%20yields%20representations%20that%20are%20robust%20for%20wheat%20and%0Atransferable%20to%20other%20crops%20and%20weeds.%20Across%20ten%20in-field%20vision%20tasks%20at%0Acanopy%20and%20organ%20levels%2C%20FoMo4Wheat%20models%20consistently%20outperform%0Astate-of-the-art%20models%20pretrained%20on%20general-domain%20dataset.%20These%20results%0Ademonstrate%20the%20value%20of%20crop-specific%20foundation%20models%20for%20reliable%20in-field%0Aperception%20and%20chart%20a%20path%20toward%20a%20universal%20crop%20foundation%20model%20with%0Across-species%20and%20cross-task%20capabilities.%20FoMo4Wheat%20models%20and%20the%20ImAg4Wheat%0Adataset%20are%20publicly%20available%20online%3A%20https%3A//github.com/PheniX-Lab/FoMo4Wheat%0Aand%20https%3A//huggingface.co/PheniX-Lab/FoMo4Wheat.%20The%20demonstration%20website%20is%3A%0Ahttps%3A//fomo4wheat.phenix-lab.com/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06907v1&entry.124074799=Read"},
{"title": "AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results", "author": "George Ciubotariu and Florin-Alexandru Vasluianu and Zhuyun Zhou and Nancy Mehta and Radu Timofte and Ke Wu and Long Sun and Lingshun Kong and Zhongbao Yang and Jinshan Pan and Jiangxin Dong and Jinhui Tang and Hao Chen and Yinghui Fang and Dafeng Zhang and Yongqi Song and Jiangbo Guo and Shuhua Jin and Zeyu Xiao and Rui Zhao and Zhuoyuan Li and Cong Zhang and Yufeng Peng and Xin Lu and Zhijing Sun and Chengjie Ge and Zihao Li and Zishun Liao and Ziang Zhou and Qiyu Kang and Xueyang Fu and Zheng-Jun Zha and Yuqian Zhang and Shuai Liu and Jie Liu and Zhuhao Zhang and Lishen Qu and Zhihao Liu and Shihao Zhou and Yaqi Luo and Juncheng Zhou and Jufeng Yang and Qianfeng Yang and Qiyuan Guan and Xiang Chen and Guiyue Jin and Jiyu Jin", "abstract": "  This paper presents a comprehensive review of the AIM 2025 High FPS\nNon-Uniform Motion Deblurring Challenge, highlighting the proposed solutions\nand final results. The objective of this challenge is to identify effective\nnetworks capable of producing clearer and visually compelling images in diverse\nand challenging conditions, by learning representative visual cues for complex\naggregations of motion types. A total of 68 participants registered for the\ncompetition, and 9 teams ultimately submitted valid entries. This paper\nthoroughly evaluates the state-of-the-art advances in high-FPS single image\nmotion deblurring, showcasing the significant progress in the field, while\nleveraging samples of the novel dataset, MIORe, that introduces challenging\nexamples of movement patterns.\n", "link": "http://arxiv.org/abs/2509.06793v1", "date": "2025-09-08", "relevancy": 2.1468, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5446}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5324}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIM%202025%20Challenge%20on%20High%20FPS%20Motion%20Deblurring%3A%20Methods%20and%20Results&body=Title%3A%20AIM%202025%20Challenge%20on%20High%20FPS%20Motion%20Deblurring%3A%20Methods%20and%20Results%0AAuthor%3A%20George%20Ciubotariu%20and%20Florin-Alexandru%20Vasluianu%20and%20Zhuyun%20Zhou%20and%20Nancy%20Mehta%20and%20Radu%20Timofte%20and%20Ke%20Wu%20and%20Long%20Sun%20and%20Lingshun%20Kong%20and%20Zhongbao%20Yang%20and%20Jinshan%20Pan%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Hao%20Chen%20and%20Yinghui%20Fang%20and%20Dafeng%20Zhang%20and%20Yongqi%20Song%20and%20Jiangbo%20Guo%20and%20Shuhua%20Jin%20and%20Zeyu%20Xiao%20and%20Rui%20Zhao%20and%20Zhuoyuan%20Li%20and%20Cong%20Zhang%20and%20Yufeng%20Peng%20and%20Xin%20Lu%20and%20Zhijing%20Sun%20and%20Chengjie%20Ge%20and%20Zihao%20Li%20and%20Zishun%20Liao%20and%20Ziang%20Zhou%20and%20Qiyu%20Kang%20and%20Xueyang%20Fu%20and%20Zheng-Jun%20Zha%20and%20Yuqian%20Zhang%20and%20Shuai%20Liu%20and%20Jie%20Liu%20and%20Zhuhao%20Zhang%20and%20Lishen%20Qu%20and%20Zhihao%20Liu%20and%20Shihao%20Zhou%20and%20Yaqi%20Luo%20and%20Juncheng%20Zhou%20and%20Jufeng%20Yang%20and%20Qianfeng%20Yang%20and%20Qiyuan%20Guan%20and%20Xiang%20Chen%20and%20Guiyue%20Jin%20and%20Jiyu%20Jin%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20review%20of%20the%20AIM%202025%20High%20FPS%0ANon-Uniform%20Motion%20Deblurring%20Challenge%2C%20highlighting%20the%20proposed%20solutions%0Aand%20final%20results.%20The%20objective%20of%20this%20challenge%20is%20to%20identify%20effective%0Anetworks%20capable%20of%20producing%20clearer%20and%20visually%20compelling%20images%20in%20diverse%0Aand%20challenging%20conditions%2C%20by%20learning%20representative%20visual%20cues%20for%20complex%0Aaggregations%20of%20motion%20types.%20A%20total%20of%2068%20participants%20registered%20for%20the%0Acompetition%2C%20and%209%20teams%20ultimately%20submitted%20valid%20entries.%20This%20paper%0Athoroughly%20evaluates%20the%20state-of-the-art%20advances%20in%20high-FPS%20single%20image%0Amotion%20deblurring%2C%20showcasing%20the%20significant%20progress%20in%20the%20field%2C%20while%0Aleveraging%20samples%20of%20the%20novel%20dataset%2C%20MIORe%2C%20that%20introduces%20challenging%0Aexamples%20of%20movement%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIM%25202025%2520Challenge%2520on%2520High%2520FPS%2520Motion%2520Deblurring%253A%2520Methods%2520and%2520Results%26entry.906535625%3DGeorge%2520Ciubotariu%2520and%2520Florin-Alexandru%2520Vasluianu%2520and%2520Zhuyun%2520Zhou%2520and%2520Nancy%2520Mehta%2520and%2520Radu%2520Timofte%2520and%2520Ke%2520Wu%2520and%2520Long%2520Sun%2520and%2520Lingshun%2520Kong%2520and%2520Zhongbao%2520Yang%2520and%2520Jinshan%2520Pan%2520and%2520Jiangxin%2520Dong%2520and%2520Jinhui%2520Tang%2520and%2520Hao%2520Chen%2520and%2520Yinghui%2520Fang%2520and%2520Dafeng%2520Zhang%2520and%2520Yongqi%2520Song%2520and%2520Jiangbo%2520Guo%2520and%2520Shuhua%2520Jin%2520and%2520Zeyu%2520Xiao%2520and%2520Rui%2520Zhao%2520and%2520Zhuoyuan%2520Li%2520and%2520Cong%2520Zhang%2520and%2520Yufeng%2520Peng%2520and%2520Xin%2520Lu%2520and%2520Zhijing%2520Sun%2520and%2520Chengjie%2520Ge%2520and%2520Zihao%2520Li%2520and%2520Zishun%2520Liao%2520and%2520Ziang%2520Zhou%2520and%2520Qiyu%2520Kang%2520and%2520Xueyang%2520Fu%2520and%2520Zheng-Jun%2520Zha%2520and%2520Yuqian%2520Zhang%2520and%2520Shuai%2520Liu%2520and%2520Jie%2520Liu%2520and%2520Zhuhao%2520Zhang%2520and%2520Lishen%2520Qu%2520and%2520Zhihao%2520Liu%2520and%2520Shihao%2520Zhou%2520and%2520Yaqi%2520Luo%2520and%2520Juncheng%2520Zhou%2520and%2520Jufeng%2520Yang%2520and%2520Qianfeng%2520Yang%2520and%2520Qiyuan%2520Guan%2520and%2520Xiang%2520Chen%2520and%2520Guiyue%2520Jin%2520and%2520Jiyu%2520Jin%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520review%2520of%2520the%2520AIM%25202025%2520High%2520FPS%250ANon-Uniform%2520Motion%2520Deblurring%2520Challenge%252C%2520highlighting%2520the%2520proposed%2520solutions%250Aand%2520final%2520results.%2520The%2520objective%2520of%2520this%2520challenge%2520is%2520to%2520identify%2520effective%250Anetworks%2520capable%2520of%2520producing%2520clearer%2520and%2520visually%2520compelling%2520images%2520in%2520diverse%250Aand%2520challenging%2520conditions%252C%2520by%2520learning%2520representative%2520visual%2520cues%2520for%2520complex%250Aaggregations%2520of%2520motion%2520types.%2520A%2520total%2520of%252068%2520participants%2520registered%2520for%2520the%250Acompetition%252C%2520and%25209%2520teams%2520ultimately%2520submitted%2520valid%2520entries.%2520This%2520paper%250Athoroughly%2520evaluates%2520the%2520state-of-the-art%2520advances%2520in%2520high-FPS%2520single%2520image%250Amotion%2520deblurring%252C%2520showcasing%2520the%2520significant%2520progress%2520in%2520the%2520field%252C%2520while%250Aleveraging%2520samples%2520of%2520the%2520novel%2520dataset%252C%2520MIORe%252C%2520that%2520introduces%2520challenging%250Aexamples%2520of%2520movement%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIM%202025%20Challenge%20on%20High%20FPS%20Motion%20Deblurring%3A%20Methods%20and%20Results&entry.906535625=George%20Ciubotariu%20and%20Florin-Alexandru%20Vasluianu%20and%20Zhuyun%20Zhou%20and%20Nancy%20Mehta%20and%20Radu%20Timofte%20and%20Ke%20Wu%20and%20Long%20Sun%20and%20Lingshun%20Kong%20and%20Zhongbao%20Yang%20and%20Jinshan%20Pan%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Hao%20Chen%20and%20Yinghui%20Fang%20and%20Dafeng%20Zhang%20and%20Yongqi%20Song%20and%20Jiangbo%20Guo%20and%20Shuhua%20Jin%20and%20Zeyu%20Xiao%20and%20Rui%20Zhao%20and%20Zhuoyuan%20Li%20and%20Cong%20Zhang%20and%20Yufeng%20Peng%20and%20Xin%20Lu%20and%20Zhijing%20Sun%20and%20Chengjie%20Ge%20and%20Zihao%20Li%20and%20Zishun%20Liao%20and%20Ziang%20Zhou%20and%20Qiyu%20Kang%20and%20Xueyang%20Fu%20and%20Zheng-Jun%20Zha%20and%20Yuqian%20Zhang%20and%20Shuai%20Liu%20and%20Jie%20Liu%20and%20Zhuhao%20Zhang%20and%20Lishen%20Qu%20and%20Zhihao%20Liu%20and%20Shihao%20Zhou%20and%20Yaqi%20Luo%20and%20Juncheng%20Zhou%20and%20Jufeng%20Yang%20and%20Qianfeng%20Yang%20and%20Qiyuan%20Guan%20and%20Xiang%20Chen%20and%20Guiyue%20Jin%20and%20Jiyu%20Jin&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20review%20of%20the%20AIM%202025%20High%20FPS%0ANon-Uniform%20Motion%20Deblurring%20Challenge%2C%20highlighting%20the%20proposed%20solutions%0Aand%20final%20results.%20The%20objective%20of%20this%20challenge%20is%20to%20identify%20effective%0Anetworks%20capable%20of%20producing%20clearer%20and%20visually%20compelling%20images%20in%20diverse%0Aand%20challenging%20conditions%2C%20by%20learning%20representative%20visual%20cues%20for%20complex%0Aaggregations%20of%20motion%20types.%20A%20total%20of%2068%20participants%20registered%20for%20the%0Acompetition%2C%20and%209%20teams%20ultimately%20submitted%20valid%20entries.%20This%20paper%0Athoroughly%20evaluates%20the%20state-of-the-art%20advances%20in%20high-FPS%20single%20image%0Amotion%20deblurring%2C%20showcasing%20the%20significant%20progress%20in%20the%20field%2C%20while%0Aleveraging%20samples%20of%20the%20novel%20dataset%2C%20MIORe%2C%20that%20introduces%20challenging%0Aexamples%20of%20movement%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06793v1&entry.124074799=Read"},
{"title": "Navigating the Labyrinth: Evaluating LLMs' Ability to Reason About\n  Search Problems", "author": "Nasim Borazjanizadeh and Roei Herzig and Trevor Darrell and Rogerio Feris and Leonid Karlinsky", "abstract": "  Large Language Models (LLMs) have recently achieved impressive performance in\nmath and reasoning benchmarks. However, they often struggle with logic problems\nand puzzles that are relatively easy for humans. To further investigate this,\nwe introduce a new benchmark, SearchBench, which contains 11 unique search\nproblems, each equipped with automated pipelines to generate an arbitrary\nnumber of instances and analyze the feasibility, correctness, and optimality of\nLLM-generated solutions. We show that using language-only reasoning, even the\nmost advanced LLMs fail to solve SearchBench end-to-end, e.g., OpenAI's\nfrontier models GPT4 and o1-preview solve only 1.4% and 18.6% of SearchBench\nproblems, respectively. The reason is that SearchBench problems require\nconsidering multiple pathways to the solution and performing backtracking,\nposing a significant challenge to auto-regressive models. Instructing LLMs to\ngenerate code that solves the problem helps, but only slightly, e.g., GPT4's\nperformance rises to 11.7%. Interestingly, we show that the current strongest\nbaseline on SearchBench is obtained using in-context learning with A* algorithm\nimplementations. We further show that this baseline can be further enhanced via\na Multi-Stage-Multi-Try inference method, raising GPT4's performance above 57%.\n", "link": "http://arxiv.org/abs/2406.12172v2", "date": "2025-09-08", "relevancy": 2.1457, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20Labyrinth%3A%20Evaluating%20LLMs%27%20Ability%20to%20Reason%20About%0A%20%20Search%20Problems&body=Title%3A%20Navigating%20the%20Labyrinth%3A%20Evaluating%20LLMs%27%20Ability%20to%20Reason%20About%0A%20%20Search%20Problems%0AAuthor%3A%20Nasim%20Borazjanizadeh%20and%20Roei%20Herzig%20and%20Trevor%20Darrell%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20achieved%20impressive%20performance%20in%0Amath%20and%20reasoning%20benchmarks.%20However%2C%20they%20often%20struggle%20with%20logic%20problems%0Aand%20puzzles%20that%20are%20relatively%20easy%20for%20humans.%20To%20further%20investigate%20this%2C%0Awe%20introduce%20a%20new%20benchmark%2C%20SearchBench%2C%20which%20contains%2011%20unique%20search%0Aproblems%2C%20each%20equipped%20with%20automated%20pipelines%20to%20generate%20an%20arbitrary%0Anumber%20of%20instances%20and%20analyze%20the%20feasibility%2C%20correctness%2C%20and%20optimality%20of%0ALLM-generated%20solutions.%20We%20show%20that%20using%20language-only%20reasoning%2C%20even%20the%0Amost%20advanced%20LLMs%20fail%20to%20solve%20SearchBench%20end-to-end%2C%20e.g.%2C%20OpenAI%27s%0Afrontier%20models%20GPT4%20and%20o1-preview%20solve%20only%201.4%25%20and%2018.6%25%20of%20SearchBench%0Aproblems%2C%20respectively.%20The%20reason%20is%20that%20SearchBench%20problems%20require%0Aconsidering%20multiple%20pathways%20to%20the%20solution%20and%20performing%20backtracking%2C%0Aposing%20a%20significant%20challenge%20to%20auto-regressive%20models.%20Instructing%20LLMs%20to%0Agenerate%20code%20that%20solves%20the%20problem%20helps%2C%20but%20only%20slightly%2C%20e.g.%2C%20GPT4%27s%0Aperformance%20rises%20to%2011.7%25.%20Interestingly%2C%20we%20show%20that%20the%20current%20strongest%0Abaseline%20on%20SearchBench%20is%20obtained%20using%20in-context%20learning%20with%20A%2A%20algorithm%0Aimplementations.%20We%20further%20show%20that%20this%20baseline%20can%20be%20further%20enhanced%20via%0Aa%20Multi-Stage-Multi-Try%20inference%20method%2C%20raising%20GPT4%27s%20performance%20above%2057%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520Labyrinth%253A%2520Evaluating%2520LLMs%2527%2520Ability%2520to%2520Reason%2520About%250A%2520%2520Search%2520Problems%26entry.906535625%3DNasim%2520Borazjanizadeh%2520and%2520Roei%2520Herzig%2520and%2520Trevor%2520Darrell%2520and%2520Rogerio%2520Feris%2520and%2520Leonid%2520Karlinsky%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520achieved%2520impressive%2520performance%2520in%250Amath%2520and%2520reasoning%2520benchmarks.%2520However%252C%2520they%2520often%2520struggle%2520with%2520logic%2520problems%250Aand%2520puzzles%2520that%2520are%2520relatively%2520easy%2520for%2520humans.%2520To%2520further%2520investigate%2520this%252C%250Awe%2520introduce%2520a%2520new%2520benchmark%252C%2520SearchBench%252C%2520which%2520contains%252011%2520unique%2520search%250Aproblems%252C%2520each%2520equipped%2520with%2520automated%2520pipelines%2520to%2520generate%2520an%2520arbitrary%250Anumber%2520of%2520instances%2520and%2520analyze%2520the%2520feasibility%252C%2520correctness%252C%2520and%2520optimality%2520of%250ALLM-generated%2520solutions.%2520We%2520show%2520that%2520using%2520language-only%2520reasoning%252C%2520even%2520the%250Amost%2520advanced%2520LLMs%2520fail%2520to%2520solve%2520SearchBench%2520end-to-end%252C%2520e.g.%252C%2520OpenAI%2527s%250Afrontier%2520models%2520GPT4%2520and%2520o1-preview%2520solve%2520only%25201.4%2525%2520and%252018.6%2525%2520of%2520SearchBench%250Aproblems%252C%2520respectively.%2520The%2520reason%2520is%2520that%2520SearchBench%2520problems%2520require%250Aconsidering%2520multiple%2520pathways%2520to%2520the%2520solution%2520and%2520performing%2520backtracking%252C%250Aposing%2520a%2520significant%2520challenge%2520to%2520auto-regressive%2520models.%2520Instructing%2520LLMs%2520to%250Agenerate%2520code%2520that%2520solves%2520the%2520problem%2520helps%252C%2520but%2520only%2520slightly%252C%2520e.g.%252C%2520GPT4%2527s%250Aperformance%2520rises%2520to%252011.7%2525.%2520Interestingly%252C%2520we%2520show%2520that%2520the%2520current%2520strongest%250Abaseline%2520on%2520SearchBench%2520is%2520obtained%2520using%2520in-context%2520learning%2520with%2520A%252A%2520algorithm%250Aimplementations.%2520We%2520further%2520show%2520that%2520this%2520baseline%2520can%2520be%2520further%2520enhanced%2520via%250Aa%2520Multi-Stage-Multi-Try%2520inference%2520method%252C%2520raising%2520GPT4%2527s%2520performance%2520above%252057%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20Labyrinth%3A%20Evaluating%20LLMs%27%20Ability%20to%20Reason%20About%0A%20%20Search%20Problems&entry.906535625=Nasim%20Borazjanizadeh%20and%20Roei%20Herzig%20and%20Trevor%20Darrell%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20achieved%20impressive%20performance%20in%0Amath%20and%20reasoning%20benchmarks.%20However%2C%20they%20often%20struggle%20with%20logic%20problems%0Aand%20puzzles%20that%20are%20relatively%20easy%20for%20humans.%20To%20further%20investigate%20this%2C%0Awe%20introduce%20a%20new%20benchmark%2C%20SearchBench%2C%20which%20contains%2011%20unique%20search%0Aproblems%2C%20each%20equipped%20with%20automated%20pipelines%20to%20generate%20an%20arbitrary%0Anumber%20of%20instances%20and%20analyze%20the%20feasibility%2C%20correctness%2C%20and%20optimality%20of%0ALLM-generated%20solutions.%20We%20show%20that%20using%20language-only%20reasoning%2C%20even%20the%0Amost%20advanced%20LLMs%20fail%20to%20solve%20SearchBench%20end-to-end%2C%20e.g.%2C%20OpenAI%27s%0Afrontier%20models%20GPT4%20and%20o1-preview%20solve%20only%201.4%25%20and%2018.6%25%20of%20SearchBench%0Aproblems%2C%20respectively.%20The%20reason%20is%20that%20SearchBench%20problems%20require%0Aconsidering%20multiple%20pathways%20to%20the%20solution%20and%20performing%20backtracking%2C%0Aposing%20a%20significant%20challenge%20to%20auto-regressive%20models.%20Instructing%20LLMs%20to%0Agenerate%20code%20that%20solves%20the%20problem%20helps%2C%20but%20only%20slightly%2C%20e.g.%2C%20GPT4%27s%0Aperformance%20rises%20to%2011.7%25.%20Interestingly%2C%20we%20show%20that%20the%20current%20strongest%0Abaseline%20on%20SearchBench%20is%20obtained%20using%20in-context%20learning%20with%20A%2A%20algorithm%0Aimplementations.%20We%20further%20show%20that%20this%20baseline%20can%20be%20further%20enhanced%20via%0Aa%20Multi-Stage-Multi-Try%20inference%20method%2C%20raising%20GPT4%27s%20performance%20above%2057%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12172v2&entry.124074799=Read"},
{"title": "Safe Robust Predictive Control-based Motion Planning of Automated\n  Surface Vessels in Inland Waterways", "author": "Sajad Ahmadi and Hossein Nejatbakhsh Esfahani and Javad Mohammadpour Velni", "abstract": "  Deploying self-navigating surface vessels in inland waterways offers a\nsustainable alternative to reduce road traffic congestion and emissions.\nHowever, navigating confined waterways presents unique challenges, including\nnarrow channels, higher traffic density, and hydrodynamic disturbances.\nExisting methods for autonomous vessel navigation often lack the robustness or\nprecision required for such environments. This paper presents a new motion\nplanning approach for Automated Surface Vessels (ASVs) using Robust Model\nPredictive Control (RMPC) combined with Control Barrier Functions (CBFs). By\nincorporating channel borders and obstacles as safety constraints within the\ncontrol design framework, the proposed method ensures both collision avoidance\nand robust navigation on complex waterways. Simulation results demonstrate the\nefficacy of the proposed method in safely guiding ASVs under realistic\nconditions, highlighting its improved safety and adaptability compared to the\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2509.06687v1", "date": "2025-09-08", "relevancy": 2.1371, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5436}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Robust%20Predictive%20Control-based%20Motion%20Planning%20of%20Automated%0A%20%20Surface%20Vessels%20in%20Inland%20Waterways&body=Title%3A%20Safe%20Robust%20Predictive%20Control-based%20Motion%20Planning%20of%20Automated%0A%20%20Surface%20Vessels%20in%20Inland%20Waterways%0AAuthor%3A%20Sajad%20Ahmadi%20and%20Hossein%20Nejatbakhsh%20Esfahani%20and%20Javad%20Mohammadpour%20Velni%0AAbstract%3A%20%20%20Deploying%20self-navigating%20surface%20vessels%20in%20inland%20waterways%20offers%20a%0Asustainable%20alternative%20to%20reduce%20road%20traffic%20congestion%20and%20emissions.%0AHowever%2C%20navigating%20confined%20waterways%20presents%20unique%20challenges%2C%20including%0Anarrow%20channels%2C%20higher%20traffic%20density%2C%20and%20hydrodynamic%20disturbances.%0AExisting%20methods%20for%20autonomous%20vessel%20navigation%20often%20lack%20the%20robustness%20or%0Aprecision%20required%20for%20such%20environments.%20This%20paper%20presents%20a%20new%20motion%0Aplanning%20approach%20for%20Automated%20Surface%20Vessels%20%28ASVs%29%20using%20Robust%20Model%0APredictive%20Control%20%28RMPC%29%20combined%20with%20Control%20Barrier%20Functions%20%28CBFs%29.%20By%0Aincorporating%20channel%20borders%20and%20obstacles%20as%20safety%20constraints%20within%20the%0Acontrol%20design%20framework%2C%20the%20proposed%20method%20ensures%20both%20collision%20avoidance%0Aand%20robust%20navigation%20on%20complex%20waterways.%20Simulation%20results%20demonstrate%20the%0Aefficacy%20of%20the%20proposed%20method%20in%20safely%20guiding%20ASVs%20under%20realistic%0Aconditions%2C%20highlighting%20its%20improved%20safety%20and%20adaptability%20compared%20to%20the%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Robust%2520Predictive%2520Control-based%2520Motion%2520Planning%2520of%2520Automated%250A%2520%2520Surface%2520Vessels%2520in%2520Inland%2520Waterways%26entry.906535625%3DSajad%2520Ahmadi%2520and%2520Hossein%2520Nejatbakhsh%2520Esfahani%2520and%2520Javad%2520Mohammadpour%2520Velni%26entry.1292438233%3D%2520%2520Deploying%2520self-navigating%2520surface%2520vessels%2520in%2520inland%2520waterways%2520offers%2520a%250Asustainable%2520alternative%2520to%2520reduce%2520road%2520traffic%2520congestion%2520and%2520emissions.%250AHowever%252C%2520navigating%2520confined%2520waterways%2520presents%2520unique%2520challenges%252C%2520including%250Anarrow%2520channels%252C%2520higher%2520traffic%2520density%252C%2520and%2520hydrodynamic%2520disturbances.%250AExisting%2520methods%2520for%2520autonomous%2520vessel%2520navigation%2520often%2520lack%2520the%2520robustness%2520or%250Aprecision%2520required%2520for%2520such%2520environments.%2520This%2520paper%2520presents%2520a%2520new%2520motion%250Aplanning%2520approach%2520for%2520Automated%2520Surface%2520Vessels%2520%2528ASVs%2529%2520using%2520Robust%2520Model%250APredictive%2520Control%2520%2528RMPC%2529%2520combined%2520with%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529.%2520By%250Aincorporating%2520channel%2520borders%2520and%2520obstacles%2520as%2520safety%2520constraints%2520within%2520the%250Acontrol%2520design%2520framework%252C%2520the%2520proposed%2520method%2520ensures%2520both%2520collision%2520avoidance%250Aand%2520robust%2520navigation%2520on%2520complex%2520waterways.%2520Simulation%2520results%2520demonstrate%2520the%250Aefficacy%2520of%2520the%2520proposed%2520method%2520in%2520safely%2520guiding%2520ASVs%2520under%2520realistic%250Aconditions%252C%2520highlighting%2520its%2520improved%2520safety%2520and%2520adaptability%2520compared%2520to%2520the%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Robust%20Predictive%20Control-based%20Motion%20Planning%20of%20Automated%0A%20%20Surface%20Vessels%20in%20Inland%20Waterways&entry.906535625=Sajad%20Ahmadi%20and%20Hossein%20Nejatbakhsh%20Esfahani%20and%20Javad%20Mohammadpour%20Velni&entry.1292438233=%20%20Deploying%20self-navigating%20surface%20vessels%20in%20inland%20waterways%20offers%20a%0Asustainable%20alternative%20to%20reduce%20road%20traffic%20congestion%20and%20emissions.%0AHowever%2C%20navigating%20confined%20waterways%20presents%20unique%20challenges%2C%20including%0Anarrow%20channels%2C%20higher%20traffic%20density%2C%20and%20hydrodynamic%20disturbances.%0AExisting%20methods%20for%20autonomous%20vessel%20navigation%20often%20lack%20the%20robustness%20or%0Aprecision%20required%20for%20such%20environments.%20This%20paper%20presents%20a%20new%20motion%0Aplanning%20approach%20for%20Automated%20Surface%20Vessels%20%28ASVs%29%20using%20Robust%20Model%0APredictive%20Control%20%28RMPC%29%20combined%20with%20Control%20Barrier%20Functions%20%28CBFs%29.%20By%0Aincorporating%20channel%20borders%20and%20obstacles%20as%20safety%20constraints%20within%20the%0Acontrol%20design%20framework%2C%20the%20proposed%20method%20ensures%20both%20collision%20avoidance%0Aand%20robust%20navigation%20on%20complex%20waterways.%20Simulation%20results%20demonstrate%20the%0Aefficacy%20of%20the%20proposed%20method%20in%20safely%20guiding%20ASVs%20under%20realistic%0Aconditions%2C%20highlighting%20its%20improved%20safety%20and%20adaptability%20compared%20to%20the%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06687v1&entry.124074799=Read"},
{"title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training", "author": "Andrei Baroian and Kasper Notebomer", "abstract": "  Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.\n", "link": "http://arxiv.org/abs/2509.06518v1", "date": "2025-09-08", "relevancy": 2.1326, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5899}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crown%2C%20Frame%2C%20Reverse%3A%20Layer-Wise%20Scaling%20Variants%20for%20LLM%20Pre-Training&body=Title%3A%20Crown%2C%20Frame%2C%20Reverse%3A%20Layer-Wise%20Scaling%20Variants%20for%20LLM%20Pre-Training%0AAuthor%3A%20Andrei%20Baroian%20and%20Kasper%20Notebomer%0AAbstract%3A%20%20%20Transformer-based%20language%20models%20traditionally%20use%20uniform%20%28isotropic%29%20layer%0Asizes%2C%20yet%20they%20ignore%20the%20diverse%20functional%20roles%20that%20different%20depths%20can%0Aplay%20and%20their%20computational%20capacity%20needs.%20Building%20on%20Layer-Wise%20Scaling%0A%28LWS%29%20and%20pruning%20literature%2C%20we%20introduce%20three%20new%20LWS%20variants%20-%20Framed%2C%0AReverse%2C%20and%20Crown%20-%20that%20redistribute%20FFN%20widths%20and%20attention%20heads%20via%20two%0Aor%20three-point%20linear%20interpolation%20in%20the%20pre-training%20stage.%20We%20present%20the%0Afirst%20systematic%20ablation%20of%20LWS%20and%20its%20variants%2C%20on%20a%20fixed%20budget%20of%20180M%0Aparameters%2C%20trained%20on%205B%20tokens.%20All%20models%20converge%20to%20similar%20losses%20and%0Aachieve%20better%20performance%20compared%20to%20an%20equal-cost%20isotropic%20baseline%2C%0Awithout%20a%20substantial%20decrease%20in%20training%20throughput.%20This%20work%20represents%20an%0Ainitial%20step%20into%20the%20design%20space%20of%20layer-wise%20architectures%20for%0Apre-training%2C%20but%20future%20work%20should%20scale%20experiments%20to%20orders%20of%20magnitude%0Amore%20tokens%20and%20parameters%20to%20fully%20assess%20their%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrown%252C%2520Frame%252C%2520Reverse%253A%2520Layer-Wise%2520Scaling%2520Variants%2520for%2520LLM%2520Pre-Training%26entry.906535625%3DAndrei%2520Baroian%2520and%2520Kasper%2520Notebomer%26entry.1292438233%3D%2520%2520Transformer-based%2520language%2520models%2520traditionally%2520use%2520uniform%2520%2528isotropic%2529%2520layer%250Asizes%252C%2520yet%2520they%2520ignore%2520the%2520diverse%2520functional%2520roles%2520that%2520different%2520depths%2520can%250Aplay%2520and%2520their%2520computational%2520capacity%2520needs.%2520Building%2520on%2520Layer-Wise%2520Scaling%250A%2528LWS%2529%2520and%2520pruning%2520literature%252C%2520we%2520introduce%2520three%2520new%2520LWS%2520variants%2520-%2520Framed%252C%250AReverse%252C%2520and%2520Crown%2520-%2520that%2520redistribute%2520FFN%2520widths%2520and%2520attention%2520heads%2520via%2520two%250Aor%2520three-point%2520linear%2520interpolation%2520in%2520the%2520pre-training%2520stage.%2520We%2520present%2520the%250Afirst%2520systematic%2520ablation%2520of%2520LWS%2520and%2520its%2520variants%252C%2520on%2520a%2520fixed%2520budget%2520of%2520180M%250Aparameters%252C%2520trained%2520on%25205B%2520tokens.%2520All%2520models%2520converge%2520to%2520similar%2520losses%2520and%250Aachieve%2520better%2520performance%2520compared%2520to%2520an%2520equal-cost%2520isotropic%2520baseline%252C%250Awithout%2520a%2520substantial%2520decrease%2520in%2520training%2520throughput.%2520This%2520work%2520represents%2520an%250Ainitial%2520step%2520into%2520the%2520design%2520space%2520of%2520layer-wise%2520architectures%2520for%250Apre-training%252C%2520but%2520future%2520work%2520should%2520scale%2520experiments%2520to%2520orders%2520of%2520magnitude%250Amore%2520tokens%2520and%2520parameters%2520to%2520fully%2520assess%2520their%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crown%2C%20Frame%2C%20Reverse%3A%20Layer-Wise%20Scaling%20Variants%20for%20LLM%20Pre-Training&entry.906535625=Andrei%20Baroian%20and%20Kasper%20Notebomer&entry.1292438233=%20%20Transformer-based%20language%20models%20traditionally%20use%20uniform%20%28isotropic%29%20layer%0Asizes%2C%20yet%20they%20ignore%20the%20diverse%20functional%20roles%20that%20different%20depths%20can%0Aplay%20and%20their%20computational%20capacity%20needs.%20Building%20on%20Layer-Wise%20Scaling%0A%28LWS%29%20and%20pruning%20literature%2C%20we%20introduce%20three%20new%20LWS%20variants%20-%20Framed%2C%0AReverse%2C%20and%20Crown%20-%20that%20redistribute%20FFN%20widths%20and%20attention%20heads%20via%20two%0Aor%20three-point%20linear%20interpolation%20in%20the%20pre-training%20stage.%20We%20present%20the%0Afirst%20systematic%20ablation%20of%20LWS%20and%20its%20variants%2C%20on%20a%20fixed%20budget%20of%20180M%0Aparameters%2C%20trained%20on%205B%20tokens.%20All%20models%20converge%20to%20similar%20losses%20and%0Aachieve%20better%20performance%20compared%20to%20an%20equal-cost%20isotropic%20baseline%2C%0Awithout%20a%20substantial%20decrease%20in%20training%20throughput.%20This%20work%20represents%20an%0Ainitial%20step%20into%20the%20design%20space%20of%20layer-wise%20architectures%20for%0Apre-training%2C%20but%20future%20work%20should%20scale%20experiments%20to%20orders%20of%20magnitude%0Amore%20tokens%20and%20parameters%20to%20fully%20assess%20their%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06518v1&entry.124074799=Read"},
{"title": "Automatic Prompt Optimization with Prompt Distillation", "author": "Ernest A. Dyagin and Nikita I. Kulin and Artur R. Khairullin and Viktor N. Zhuravlev and Alena N. Sitkina", "abstract": "  Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.\n", "link": "http://arxiv.org/abs/2508.18992v2", "date": "2025-09-08", "relevancy": 2.1251, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4289}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.424}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Prompt%20Optimization%20with%20Prompt%20Distillation&body=Title%3A%20Automatic%20Prompt%20Optimization%20with%20Prompt%20Distillation%0AAuthor%3A%20Ernest%20A.%20Dyagin%20and%20Nikita%20I.%20Kulin%20and%20Artur%20R.%20Khairullin%20and%20Viktor%20N.%20Zhuravlev%20and%20Alena%20N.%20Sitkina%0AAbstract%3A%20%20%20Autoprompting%20is%20the%20process%20of%20automatically%20selecting%20optimized%20prompts%20for%0Alanguage%20models%2C%20which%20is%20gaining%20popularity%20due%20to%20the%20rapid%20development%20of%0Aprompt%20engineering%20driven%20by%20extensive%20research%20in%20the%20field%20of%20large%20language%0Amodels%20%28LLMs%29.%20This%20paper%20presents%20DistillPrompt%20--%20a%20novel%20autoprompting%0Amethod%20based%20on%20large%20language%20models%20that%20employs%20a%20multi-stage%20integration%20of%0Atask-specific%20information%20into%20prompts%20using%20training%20data.%20DistillPrompt%0Autilizes%20distillation%2C%20compression%2C%20and%20aggregation%20operations%20to%20explore%20the%0Aprompt%20space%20more%20thoroughly.%20The%20method%20was%20tested%20on%20different%20datasets%20for%0Atext%20classification%20and%20generation%20tasks%20using%20the%20t-lite-instruct-0.1%20language%0Amodel.%20The%20results%20demonstrate%20a%20significant%20average%20improvement%20%28e.g.%2C%2020.12%25%0Aacross%20the%20entire%20dataset%20compared%20to%20Grips%29%20in%20key%20metrics%20over%20existing%0Amethods%20in%20the%20field%2C%20establishing%20DistillPrompt%20as%20one%20of%20the%20most%20effective%0Anon-gradient%20approaches%20in%20autoprompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18992v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Prompt%2520Optimization%2520with%2520Prompt%2520Distillation%26entry.906535625%3DErnest%2520A.%2520Dyagin%2520and%2520Nikita%2520I.%2520Kulin%2520and%2520Artur%2520R.%2520Khairullin%2520and%2520Viktor%2520N.%2520Zhuravlev%2520and%2520Alena%2520N.%2520Sitkina%26entry.1292438233%3D%2520%2520Autoprompting%2520is%2520the%2520process%2520of%2520automatically%2520selecting%2520optimized%2520prompts%2520for%250Alanguage%2520models%252C%2520which%2520is%2520gaining%2520popularity%2520due%2520to%2520the%2520rapid%2520development%2520of%250Aprompt%2520engineering%2520driven%2520by%2520extensive%2520research%2520in%2520the%2520field%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520This%2520paper%2520presents%2520DistillPrompt%2520--%2520a%2520novel%2520autoprompting%250Amethod%2520based%2520on%2520large%2520language%2520models%2520that%2520employs%2520a%2520multi-stage%2520integration%2520of%250Atask-specific%2520information%2520into%2520prompts%2520using%2520training%2520data.%2520DistillPrompt%250Autilizes%2520distillation%252C%2520compression%252C%2520and%2520aggregation%2520operations%2520to%2520explore%2520the%250Aprompt%2520space%2520more%2520thoroughly.%2520The%2520method%2520was%2520tested%2520on%2520different%2520datasets%2520for%250Atext%2520classification%2520and%2520generation%2520tasks%2520using%2520the%2520t-lite-instruct-0.1%2520language%250Amodel.%2520The%2520results%2520demonstrate%2520a%2520significant%2520average%2520improvement%2520%2528e.g.%252C%252020.12%2525%250Aacross%2520the%2520entire%2520dataset%2520compared%2520to%2520Grips%2529%2520in%2520key%2520metrics%2520over%2520existing%250Amethods%2520in%2520the%2520field%252C%2520establishing%2520DistillPrompt%2520as%2520one%2520of%2520the%2520most%2520effective%250Anon-gradient%2520approaches%2520in%2520autoprompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18992v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Prompt%20Optimization%20with%20Prompt%20Distillation&entry.906535625=Ernest%20A.%20Dyagin%20and%20Nikita%20I.%20Kulin%20and%20Artur%20R.%20Khairullin%20and%20Viktor%20N.%20Zhuravlev%20and%20Alena%20N.%20Sitkina&entry.1292438233=%20%20Autoprompting%20is%20the%20process%20of%20automatically%20selecting%20optimized%20prompts%20for%0Alanguage%20models%2C%20which%20is%20gaining%20popularity%20due%20to%20the%20rapid%20development%20of%0Aprompt%20engineering%20driven%20by%20extensive%20research%20in%20the%20field%20of%20large%20language%0Amodels%20%28LLMs%29.%20This%20paper%20presents%20DistillPrompt%20--%20a%20novel%20autoprompting%0Amethod%20based%20on%20large%20language%20models%20that%20employs%20a%20multi-stage%20integration%20of%0Atask-specific%20information%20into%20prompts%20using%20training%20data.%20DistillPrompt%0Autilizes%20distillation%2C%20compression%2C%20and%20aggregation%20operations%20to%20explore%20the%0Aprompt%20space%20more%20thoroughly.%20The%20method%20was%20tested%20on%20different%20datasets%20for%0Atext%20classification%20and%20generation%20tasks%20using%20the%20t-lite-instruct-0.1%20language%0Amodel.%20The%20results%20demonstrate%20a%20significant%20average%20improvement%20%28e.g.%2C%2020.12%25%0Aacross%20the%20entire%20dataset%20compared%20to%20Grips%29%20in%20key%20metrics%20over%20existing%0Amethods%20in%20the%20field%2C%20establishing%20DistillPrompt%20as%20one%20of%20the%20most%20effective%0Anon-gradient%20approaches%20in%20autoprompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18992v2&entry.124074799=Read"},
{"title": "AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced\n  Classification", "author": "Sukumar Kishanthan and Asela Hevapathige", "abstract": "  Class imbalance in machine learning poses a significant challenge, as skewed\ndatasets often hinder performance on minority classes. Traditional oversampling\ntechniques, which are commonly used to alleviate class imbalance, have several\ndrawbacks: they treat features independently, lack similarity-based controls,\nlimit sample diversity, and fail to manage synthetic variety effectively. To\novercome these issues, we introduce AxelSMOTE, an innovative agent-based\napproach that views data instances as autonomous agents engaging in complex\ninteractions. Based on Axelrod's cultural dissemination model, AxelSMOTE\nimplements four key innovations: (1) trait-based feature grouping to preserve\ncorrelations; (2) a similarity-based probabilistic exchange mechanism for\nmeaningful interactions; (3) Beta distribution blending for realistic\ninterpolation; and (4) controlled diversity injection to avoid overfitting.\nExperiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms\nstate-of-the-art sampling methods while maintaining computational efficiency.\n", "link": "http://arxiv.org/abs/2509.06875v1", "date": "2025-09-08", "relevancy": 2.1229, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6022}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4802}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AxelSMOTE%3A%20An%20Agent-Based%20Oversampling%20Algorithm%20for%20Imbalanced%0A%20%20Classification&body=Title%3A%20AxelSMOTE%3A%20An%20Agent-Based%20Oversampling%20Algorithm%20for%20Imbalanced%0A%20%20Classification%0AAuthor%3A%20Sukumar%20Kishanthan%20and%20Asela%20Hevapathige%0AAbstract%3A%20%20%20Class%20imbalance%20in%20machine%20learning%20poses%20a%20significant%20challenge%2C%20as%20skewed%0Adatasets%20often%20hinder%20performance%20on%20minority%20classes.%20Traditional%20oversampling%0Atechniques%2C%20which%20are%20commonly%20used%20to%20alleviate%20class%20imbalance%2C%20have%20several%0Adrawbacks%3A%20they%20treat%20features%20independently%2C%20lack%20similarity-based%20controls%2C%0Alimit%20sample%20diversity%2C%20and%20fail%20to%20manage%20synthetic%20variety%20effectively.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20AxelSMOTE%2C%20an%20innovative%20agent-based%0Aapproach%20that%20views%20data%20instances%20as%20autonomous%20agents%20engaging%20in%20complex%0Ainteractions.%20Based%20on%20Axelrod%27s%20cultural%20dissemination%20model%2C%20AxelSMOTE%0Aimplements%20four%20key%20innovations%3A%20%281%29%20trait-based%20feature%20grouping%20to%20preserve%0Acorrelations%3B%20%282%29%20a%20similarity-based%20probabilistic%20exchange%20mechanism%20for%0Ameaningful%20interactions%3B%20%283%29%20Beta%20distribution%20blending%20for%20realistic%0Ainterpolation%3B%20and%20%284%29%20controlled%20diversity%20injection%20to%20avoid%20overfitting.%0AExperiments%20on%20eight%20imbalanced%20datasets%20demonstrate%20that%20AxelSMOTE%20outperforms%0Astate-of-the-art%20sampling%20methods%20while%20maintaining%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAxelSMOTE%253A%2520An%2520Agent-Based%2520Oversampling%2520Algorithm%2520for%2520Imbalanced%250A%2520%2520Classification%26entry.906535625%3DSukumar%2520Kishanthan%2520and%2520Asela%2520Hevapathige%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520in%2520machine%2520learning%2520poses%2520a%2520significant%2520challenge%252C%2520as%2520skewed%250Adatasets%2520often%2520hinder%2520performance%2520on%2520minority%2520classes.%2520Traditional%2520oversampling%250Atechniques%252C%2520which%2520are%2520commonly%2520used%2520to%2520alleviate%2520class%2520imbalance%252C%2520have%2520several%250Adrawbacks%253A%2520they%2520treat%2520features%2520independently%252C%2520lack%2520similarity-based%2520controls%252C%250Alimit%2520sample%2520diversity%252C%2520and%2520fail%2520to%2520manage%2520synthetic%2520variety%2520effectively.%2520To%250Aovercome%2520these%2520issues%252C%2520we%2520introduce%2520AxelSMOTE%252C%2520an%2520innovative%2520agent-based%250Aapproach%2520that%2520views%2520data%2520instances%2520as%2520autonomous%2520agents%2520engaging%2520in%2520complex%250Ainteractions.%2520Based%2520on%2520Axelrod%2527s%2520cultural%2520dissemination%2520model%252C%2520AxelSMOTE%250Aimplements%2520four%2520key%2520innovations%253A%2520%25281%2529%2520trait-based%2520feature%2520grouping%2520to%2520preserve%250Acorrelations%253B%2520%25282%2529%2520a%2520similarity-based%2520probabilistic%2520exchange%2520mechanism%2520for%250Ameaningful%2520interactions%253B%2520%25283%2529%2520Beta%2520distribution%2520blending%2520for%2520realistic%250Ainterpolation%253B%2520and%2520%25284%2529%2520controlled%2520diversity%2520injection%2520to%2520avoid%2520overfitting.%250AExperiments%2520on%2520eight%2520imbalanced%2520datasets%2520demonstrate%2520that%2520AxelSMOTE%2520outperforms%250Astate-of-the-art%2520sampling%2520methods%2520while%2520maintaining%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AxelSMOTE%3A%20An%20Agent-Based%20Oversampling%20Algorithm%20for%20Imbalanced%0A%20%20Classification&entry.906535625=Sukumar%20Kishanthan%20and%20Asela%20Hevapathige&entry.1292438233=%20%20Class%20imbalance%20in%20machine%20learning%20poses%20a%20significant%20challenge%2C%20as%20skewed%0Adatasets%20often%20hinder%20performance%20on%20minority%20classes.%20Traditional%20oversampling%0Atechniques%2C%20which%20are%20commonly%20used%20to%20alleviate%20class%20imbalance%2C%20have%20several%0Adrawbacks%3A%20they%20treat%20features%20independently%2C%20lack%20similarity-based%20controls%2C%0Alimit%20sample%20diversity%2C%20and%20fail%20to%20manage%20synthetic%20variety%20effectively.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20AxelSMOTE%2C%20an%20innovative%20agent-based%0Aapproach%20that%20views%20data%20instances%20as%20autonomous%20agents%20engaging%20in%20complex%0Ainteractions.%20Based%20on%20Axelrod%27s%20cultural%20dissemination%20model%2C%20AxelSMOTE%0Aimplements%20four%20key%20innovations%3A%20%281%29%20trait-based%20feature%20grouping%20to%20preserve%0Acorrelations%3B%20%282%29%20a%20similarity-based%20probabilistic%20exchange%20mechanism%20for%0Ameaningful%20interactions%3B%20%283%29%20Beta%20distribution%20blending%20for%20realistic%0Ainterpolation%3B%20and%20%284%29%20controlled%20diversity%20injection%20to%20avoid%20overfitting.%0AExperiments%20on%20eight%20imbalanced%20datasets%20demonstrate%20that%20AxelSMOTE%20outperforms%0Astate-of-the-art%20sampling%20methods%20while%20maintaining%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06875v1&entry.124074799=Read"},
{"title": "Not All Samples Are Equal: Quantifying Instance-level Difficulty in\n  Targeted Data Poisoning", "author": "William Xu and Yiwei Lu and Yihan Wang and Matthew Y. R. Yang and Zuoqiu Liu and Gautam Kamath and Yaoliang Yu", "abstract": "  Targeted data poisoning attacks pose an increasingly serious threat due to\ntheir ease of deployment and high success rates. These attacks aim to\nmanipulate the prediction for a single test sample in classification models.\nUnlike indiscriminate attacks that aim to decrease overall test performance,\ntargeted attacks present a unique threat to individual test instances. This\nthreat model raises a fundamental question: what factors make certain test\nsamples more susceptible to successful poisoning than others? We investigate\nhow attack difficulty varies across different test instances and identify key\ncharacteristics that influence vulnerability. This paper introduces three\npredictive criteria for targeted data poisoning difficulty: ergodic prediction\naccuracy (analyzed through clean training dynamics), poison distance, and\npoison budget. Our experimental results demonstrate that these metrics\neffectively predict the varying difficulty of real-world targeted poisoning\nattacks across diverse scenarios, offering practitioners valuable insights for\nvulnerability assessment and understanding data poisoning attacks.\n", "link": "http://arxiv.org/abs/2509.06896v1", "date": "2025-09-08", "relevancy": 2.1144, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4555}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4165}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Samples%20Are%20Equal%3A%20Quantifying%20Instance-level%20Difficulty%20in%0A%20%20Targeted%20Data%20Poisoning&body=Title%3A%20Not%20All%20Samples%20Are%20Equal%3A%20Quantifying%20Instance-level%20Difficulty%20in%0A%20%20Targeted%20Data%20Poisoning%0AAuthor%3A%20William%20Xu%20and%20Yiwei%20Lu%20and%20Yihan%20Wang%20and%20Matthew%20Y.%20R.%20Yang%20and%20Zuoqiu%20Liu%20and%20Gautam%20Kamath%20and%20Yaoliang%20Yu%0AAbstract%3A%20%20%20Targeted%20data%20poisoning%20attacks%20pose%20an%20increasingly%20serious%20threat%20due%20to%0Atheir%20ease%20of%20deployment%20and%20high%20success%20rates.%20These%20attacks%20aim%20to%0Amanipulate%20the%20prediction%20for%20a%20single%20test%20sample%20in%20classification%20models.%0AUnlike%20indiscriminate%20attacks%20that%20aim%20to%20decrease%20overall%20test%20performance%2C%0Atargeted%20attacks%20present%20a%20unique%20threat%20to%20individual%20test%20instances.%20This%0Athreat%20model%20raises%20a%20fundamental%20question%3A%20what%20factors%20make%20certain%20test%0Asamples%20more%20susceptible%20to%20successful%20poisoning%20than%20others%3F%20We%20investigate%0Ahow%20attack%20difficulty%20varies%20across%20different%20test%20instances%20and%20identify%20key%0Acharacteristics%20that%20influence%20vulnerability.%20This%20paper%20introduces%20three%0Apredictive%20criteria%20for%20targeted%20data%20poisoning%20difficulty%3A%20ergodic%20prediction%0Aaccuracy%20%28analyzed%20through%20clean%20training%20dynamics%29%2C%20poison%20distance%2C%20and%0Apoison%20budget.%20Our%20experimental%20results%20demonstrate%20that%20these%20metrics%0Aeffectively%20predict%20the%20varying%20difficulty%20of%20real-world%20targeted%20poisoning%0Aattacks%20across%20diverse%20scenarios%2C%20offering%20practitioners%20valuable%20insights%20for%0Avulnerability%20assessment%20and%20understanding%20data%20poisoning%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Samples%2520Are%2520Equal%253A%2520Quantifying%2520Instance-level%2520Difficulty%2520in%250A%2520%2520Targeted%2520Data%2520Poisoning%26entry.906535625%3DWilliam%2520Xu%2520and%2520Yiwei%2520Lu%2520and%2520Yihan%2520Wang%2520and%2520Matthew%2520Y.%2520R.%2520Yang%2520and%2520Zuoqiu%2520Liu%2520and%2520Gautam%2520Kamath%2520and%2520Yaoliang%2520Yu%26entry.1292438233%3D%2520%2520Targeted%2520data%2520poisoning%2520attacks%2520pose%2520an%2520increasingly%2520serious%2520threat%2520due%2520to%250Atheir%2520ease%2520of%2520deployment%2520and%2520high%2520success%2520rates.%2520These%2520attacks%2520aim%2520to%250Amanipulate%2520the%2520prediction%2520for%2520a%2520single%2520test%2520sample%2520in%2520classification%2520models.%250AUnlike%2520indiscriminate%2520attacks%2520that%2520aim%2520to%2520decrease%2520overall%2520test%2520performance%252C%250Atargeted%2520attacks%2520present%2520a%2520unique%2520threat%2520to%2520individual%2520test%2520instances.%2520This%250Athreat%2520model%2520raises%2520a%2520fundamental%2520question%253A%2520what%2520factors%2520make%2520certain%2520test%250Asamples%2520more%2520susceptible%2520to%2520successful%2520poisoning%2520than%2520others%253F%2520We%2520investigate%250Ahow%2520attack%2520difficulty%2520varies%2520across%2520different%2520test%2520instances%2520and%2520identify%2520key%250Acharacteristics%2520that%2520influence%2520vulnerability.%2520This%2520paper%2520introduces%2520three%250Apredictive%2520criteria%2520for%2520targeted%2520data%2520poisoning%2520difficulty%253A%2520ergodic%2520prediction%250Aaccuracy%2520%2528analyzed%2520through%2520clean%2520training%2520dynamics%2529%252C%2520poison%2520distance%252C%2520and%250Apoison%2520budget.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520these%2520metrics%250Aeffectively%2520predict%2520the%2520varying%2520difficulty%2520of%2520real-world%2520targeted%2520poisoning%250Aattacks%2520across%2520diverse%2520scenarios%252C%2520offering%2520practitioners%2520valuable%2520insights%2520for%250Avulnerability%2520assessment%2520and%2520understanding%2520data%2520poisoning%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Samples%20Are%20Equal%3A%20Quantifying%20Instance-level%20Difficulty%20in%0A%20%20Targeted%20Data%20Poisoning&entry.906535625=William%20Xu%20and%20Yiwei%20Lu%20and%20Yihan%20Wang%20and%20Matthew%20Y.%20R.%20Yang%20and%20Zuoqiu%20Liu%20and%20Gautam%20Kamath%20and%20Yaoliang%20Yu&entry.1292438233=%20%20Targeted%20data%20poisoning%20attacks%20pose%20an%20increasingly%20serious%20threat%20due%20to%0Atheir%20ease%20of%20deployment%20and%20high%20success%20rates.%20These%20attacks%20aim%20to%0Amanipulate%20the%20prediction%20for%20a%20single%20test%20sample%20in%20classification%20models.%0AUnlike%20indiscriminate%20attacks%20that%20aim%20to%20decrease%20overall%20test%20performance%2C%0Atargeted%20attacks%20present%20a%20unique%20threat%20to%20individual%20test%20instances.%20This%0Athreat%20model%20raises%20a%20fundamental%20question%3A%20what%20factors%20make%20certain%20test%0Asamples%20more%20susceptible%20to%20successful%20poisoning%20than%20others%3F%20We%20investigate%0Ahow%20attack%20difficulty%20varies%20across%20different%20test%20instances%20and%20identify%20key%0Acharacteristics%20that%20influence%20vulnerability.%20This%20paper%20introduces%20three%0Apredictive%20criteria%20for%20targeted%20data%20poisoning%20difficulty%3A%20ergodic%20prediction%0Aaccuracy%20%28analyzed%20through%20clean%20training%20dynamics%29%2C%20poison%20distance%2C%20and%0Apoison%20budget.%20Our%20experimental%20results%20demonstrate%20that%20these%20metrics%0Aeffectively%20predict%20the%20varying%20difficulty%20of%20real-world%20targeted%20poisoning%0Aattacks%20across%20diverse%20scenarios%2C%20offering%20practitioners%20valuable%20insights%20for%0Avulnerability%20assessment%20and%20understanding%20data%20poisoning%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06896v1&entry.124074799=Read"},
{"title": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural\n  Networks", "author": "Su Hyeong Lee and Risi Kondor and Richard Ngo", "abstract": "  We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems.\n", "link": "http://arxiv.org/abs/2509.06701v1", "date": "2025-09-08", "relevancy": 2.1142, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5263}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Modeling%20of%20Latent%20Agentic%20Substructures%20in%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20Probabilistic%20Modeling%20of%20Latent%20Agentic%20Substructures%20in%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Su%20Hyeong%20Lee%20and%20Risi%20Kondor%20and%20Richard%20Ngo%0AAbstract%3A%20%20%20We%20develop%20a%20theory%20of%20intelligent%20agency%20grounded%20in%20probabilistic%20modeling%0Afor%20neural%20models.%20Agents%20are%20represented%20as%20outcome%20distributions%20with%0Aepistemic%20utility%20given%20by%20log%20score%2C%20and%20compositions%20are%20defined%20through%0Aweighted%20logarithmic%20pooling%20that%20strictly%20improves%20every%20member%27s%20welfare.%20We%0Aprove%20that%20strict%20unanimity%20is%20impossible%20under%20linear%20pooling%20or%20in%20binary%0Aoutcome%20spaces%2C%20but%20possible%20with%20three%20or%20more%20outcomes.%20Our%20framework%20admits%0Arecursive%20structure%20via%20cloning%20invariance%2C%20continuity%2C%20and%20openness%2C%20while%0Atilt-based%20analysis%20rules%20out%20trivial%20duplication.%20Finally%2C%20we%20formalize%20an%0Aagentic%20alignment%20phenomenon%20in%20LLMs%20using%20our%20theory%3A%20eliciting%20a%20benevolent%0Apersona%20%28%22Luigi%27%22%29%20induces%20an%20antagonistic%20counterpart%20%28%22Waluigi%22%29%2C%20while%20a%0Amanifest-then-suppress%20Waluigi%20strategy%20yields%20strictly%20larger%20first-order%0Amisalignment%20reduction%20than%20pure%20Luigi%20reinforcement%20alone.%20These%20results%0Aclarify%20how%20developing%20a%20principled%20mathematical%20framework%20for%20how%20subagents%0Acan%20coalesce%20into%20coherent%20higher-level%20entities%20provides%20novel%20implications%0Afor%20alignment%20in%20agentic%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Modeling%2520of%2520Latent%2520Agentic%2520Substructures%2520in%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DSu%2520Hyeong%2520Lee%2520and%2520Risi%2520Kondor%2520and%2520Richard%2520Ngo%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520theory%2520of%2520intelligent%2520agency%2520grounded%2520in%2520probabilistic%2520modeling%250Afor%2520neural%2520models.%2520Agents%2520are%2520represented%2520as%2520outcome%2520distributions%2520with%250Aepistemic%2520utility%2520given%2520by%2520log%2520score%252C%2520and%2520compositions%2520are%2520defined%2520through%250Aweighted%2520logarithmic%2520pooling%2520that%2520strictly%2520improves%2520every%2520member%2527s%2520welfare.%2520We%250Aprove%2520that%2520strict%2520unanimity%2520is%2520impossible%2520under%2520linear%2520pooling%2520or%2520in%2520binary%250Aoutcome%2520spaces%252C%2520but%2520possible%2520with%2520three%2520or%2520more%2520outcomes.%2520Our%2520framework%2520admits%250Arecursive%2520structure%2520via%2520cloning%2520invariance%252C%2520continuity%252C%2520and%2520openness%252C%2520while%250Atilt-based%2520analysis%2520rules%2520out%2520trivial%2520duplication.%2520Finally%252C%2520we%2520formalize%2520an%250Aagentic%2520alignment%2520phenomenon%2520in%2520LLMs%2520using%2520our%2520theory%253A%2520eliciting%2520a%2520benevolent%250Apersona%2520%2528%2522Luigi%2527%2522%2529%2520induces%2520an%2520antagonistic%2520counterpart%2520%2528%2522Waluigi%2522%2529%252C%2520while%2520a%250Amanifest-then-suppress%2520Waluigi%2520strategy%2520yields%2520strictly%2520larger%2520first-order%250Amisalignment%2520reduction%2520than%2520pure%2520Luigi%2520reinforcement%2520alone.%2520These%2520results%250Aclarify%2520how%2520developing%2520a%2520principled%2520mathematical%2520framework%2520for%2520how%2520subagents%250Acan%2520coalesce%2520into%2520coherent%2520higher-level%2520entities%2520provides%2520novel%2520implications%250Afor%2520alignment%2520in%2520agentic%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Modeling%20of%20Latent%20Agentic%20Substructures%20in%20Deep%20Neural%0A%20%20Networks&entry.906535625=Su%20Hyeong%20Lee%20and%20Risi%20Kondor%20and%20Richard%20Ngo&entry.1292438233=%20%20We%20develop%20a%20theory%20of%20intelligent%20agency%20grounded%20in%20probabilistic%20modeling%0Afor%20neural%20models.%20Agents%20are%20represented%20as%20outcome%20distributions%20with%0Aepistemic%20utility%20given%20by%20log%20score%2C%20and%20compositions%20are%20defined%20through%0Aweighted%20logarithmic%20pooling%20that%20strictly%20improves%20every%20member%27s%20welfare.%20We%0Aprove%20that%20strict%20unanimity%20is%20impossible%20under%20linear%20pooling%20or%20in%20binary%0Aoutcome%20spaces%2C%20but%20possible%20with%20three%20or%20more%20outcomes.%20Our%20framework%20admits%0Arecursive%20structure%20via%20cloning%20invariance%2C%20continuity%2C%20and%20openness%2C%20while%0Atilt-based%20analysis%20rules%20out%20trivial%20duplication.%20Finally%2C%20we%20formalize%20an%0Aagentic%20alignment%20phenomenon%20in%20LLMs%20using%20our%20theory%3A%20eliciting%20a%20benevolent%0Apersona%20%28%22Luigi%27%22%29%20induces%20an%20antagonistic%20counterpart%20%28%22Waluigi%22%29%2C%20while%20a%0Amanifest-then-suppress%20Waluigi%20strategy%20yields%20strictly%20larger%20first-order%0Amisalignment%20reduction%20than%20pure%20Luigi%20reinforcement%20alone.%20These%20results%0Aclarify%20how%20developing%20a%20principled%20mathematical%20framework%20for%20how%20subagents%0Acan%20coalesce%20into%20coherent%20higher-level%20entities%20provides%20novel%20implications%0Afor%20alignment%20in%20agentic%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06701v1&entry.124074799=Read"},
{"title": "Robust and Label-Efficient Deep Waste Detection", "author": "Hassan Abid and Khan Muhammad and Muhammad Haris Khan", "abstract": "  Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection.\n", "link": "http://arxiv.org/abs/2508.18799v2", "date": "2025-09-08", "relevancy": 2.1097, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6021}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Label-Efficient%20Deep%20Waste%20Detection&body=Title%3A%20Robust%20and%20Label-Efficient%20Deep%20Waste%20Detection%0AAuthor%3A%20Hassan%20Abid%20and%20Khan%20Muhammad%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20Effective%20waste%20sorting%20is%20critical%20for%20sustainable%20recycling%2C%20yet%20AI%0Aresearch%20in%20this%20domain%20continues%20to%20lag%20behind%20commercial%20systems%20due%20to%0Alimited%20datasets%20and%20reliance%20on%20legacy%20object%20detectors.%20In%20this%20work%2C%20we%0Aadvance%20AI-driven%20waste%20detection%20by%20establishing%20strong%20baselines%20and%0Aintroducing%20an%20ensemble-based%20semi-supervised%20learning%20framework.%20We%20first%0Abenchmark%20state-of-the-art%20Open-Vocabulary%20Object%20Detection%20%28OVOD%29%20models%20on%0Athe%20real-world%20ZeroWaste%20dataset%2C%20demonstrating%20that%20while%20class-only%20prompts%0Aperform%20poorly%2C%20LLM-optimized%20prompts%20significantly%20enhance%20zero-shot%20accuracy.%0ANext%2C%20to%20address%20domain-specific%20limitations%2C%20we%20fine-tune%20modern%0Atransformer-based%20detectors%2C%20achieving%20a%20new%20baseline%20of%2051.6%20mAP.%20We%20then%0Apropose%20a%20soft%20pseudo-labeling%20strategy%20that%20fuses%20ensemble%20predictions%20using%0Aspatial%20and%20consensus-aware%20weighting%2C%20enabling%20robust%20semi-supervised%0Atraining.%20Applied%20to%20the%20unlabeled%20ZeroWaste-s%20subset%2C%20our%20pseudo-annotations%0Aachieve%20performance%20gains%20that%20surpass%20fully%20supervised%20training%2C%20underscoring%0Athe%20effectiveness%20of%20scalable%20annotation%20pipelines.%20Our%20work%20contributes%20to%20the%0Aresearch%20community%20by%20establishing%20rigorous%20baselines%2C%20introducing%20a%20robust%0Aensemble-based%20pseudo-labeling%20pipeline%2C%20generating%20high-quality%20annotations%0Afor%20the%20unlabeled%20ZeroWaste-s%20subset%2C%20and%20systematically%20evaluating%20OVOD%20models%0Aunder%20real-world%20waste%20sorting%20conditions.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/h-abid97/robust-waste-detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Label-Efficient%2520Deep%2520Waste%2520Detection%26entry.906535625%3DHassan%2520Abid%2520and%2520Khan%2520Muhammad%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520Effective%2520waste%2520sorting%2520is%2520critical%2520for%2520sustainable%2520recycling%252C%2520yet%2520AI%250Aresearch%2520in%2520this%2520domain%2520continues%2520to%2520lag%2520behind%2520commercial%2520systems%2520due%2520to%250Alimited%2520datasets%2520and%2520reliance%2520on%2520legacy%2520object%2520detectors.%2520In%2520this%2520work%252C%2520we%250Aadvance%2520AI-driven%2520waste%2520detection%2520by%2520establishing%2520strong%2520baselines%2520and%250Aintroducing%2520an%2520ensemble-based%2520semi-supervised%2520learning%2520framework.%2520We%2520first%250Abenchmark%2520state-of-the-art%2520Open-Vocabulary%2520Object%2520Detection%2520%2528OVOD%2529%2520models%2520on%250Athe%2520real-world%2520ZeroWaste%2520dataset%252C%2520demonstrating%2520that%2520while%2520class-only%2520prompts%250Aperform%2520poorly%252C%2520LLM-optimized%2520prompts%2520significantly%2520enhance%2520zero-shot%2520accuracy.%250ANext%252C%2520to%2520address%2520domain-specific%2520limitations%252C%2520we%2520fine-tune%2520modern%250Atransformer-based%2520detectors%252C%2520achieving%2520a%2520new%2520baseline%2520of%252051.6%2520mAP.%2520We%2520then%250Apropose%2520a%2520soft%2520pseudo-labeling%2520strategy%2520that%2520fuses%2520ensemble%2520predictions%2520using%250Aspatial%2520and%2520consensus-aware%2520weighting%252C%2520enabling%2520robust%2520semi-supervised%250Atraining.%2520Applied%2520to%2520the%2520unlabeled%2520ZeroWaste-s%2520subset%252C%2520our%2520pseudo-annotations%250Aachieve%2520performance%2520gains%2520that%2520surpass%2520fully%2520supervised%2520training%252C%2520underscoring%250Athe%2520effectiveness%2520of%2520scalable%2520annotation%2520pipelines.%2520Our%2520work%2520contributes%2520to%2520the%250Aresearch%2520community%2520by%2520establishing%2520rigorous%2520baselines%252C%2520introducing%2520a%2520robust%250Aensemble-based%2520pseudo-labeling%2520pipeline%252C%2520generating%2520high-quality%2520annotations%250Afor%2520the%2520unlabeled%2520ZeroWaste-s%2520subset%252C%2520and%2520systematically%2520evaluating%2520OVOD%2520models%250Aunder%2520real-world%2520waste%2520sorting%2520conditions.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/h-abid97/robust-waste-detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Label-Efficient%20Deep%20Waste%20Detection&entry.906535625=Hassan%20Abid%20and%20Khan%20Muhammad%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20Effective%20waste%20sorting%20is%20critical%20for%20sustainable%20recycling%2C%20yet%20AI%0Aresearch%20in%20this%20domain%20continues%20to%20lag%20behind%20commercial%20systems%20due%20to%0Alimited%20datasets%20and%20reliance%20on%20legacy%20object%20detectors.%20In%20this%20work%2C%20we%0Aadvance%20AI-driven%20waste%20detection%20by%20establishing%20strong%20baselines%20and%0Aintroducing%20an%20ensemble-based%20semi-supervised%20learning%20framework.%20We%20first%0Abenchmark%20state-of-the-art%20Open-Vocabulary%20Object%20Detection%20%28OVOD%29%20models%20on%0Athe%20real-world%20ZeroWaste%20dataset%2C%20demonstrating%20that%20while%20class-only%20prompts%0Aperform%20poorly%2C%20LLM-optimized%20prompts%20significantly%20enhance%20zero-shot%20accuracy.%0ANext%2C%20to%20address%20domain-specific%20limitations%2C%20we%20fine-tune%20modern%0Atransformer-based%20detectors%2C%20achieving%20a%20new%20baseline%20of%2051.6%20mAP.%20We%20then%0Apropose%20a%20soft%20pseudo-labeling%20strategy%20that%20fuses%20ensemble%20predictions%20using%0Aspatial%20and%20consensus-aware%20weighting%2C%20enabling%20robust%20semi-supervised%0Atraining.%20Applied%20to%20the%20unlabeled%20ZeroWaste-s%20subset%2C%20our%20pseudo-annotations%0Aachieve%20performance%20gains%20that%20surpass%20fully%20supervised%20training%2C%20underscoring%0Athe%20effectiveness%20of%20scalable%20annotation%20pipelines.%20Our%20work%20contributes%20to%20the%0Aresearch%20community%20by%20establishing%20rigorous%20baselines%2C%20introducing%20a%20robust%0Aensemble-based%20pseudo-labeling%20pipeline%2C%20generating%20high-quality%20annotations%0Afor%20the%20unlabeled%20ZeroWaste-s%20subset%2C%20and%20systematically%20evaluating%20OVOD%20models%0Aunder%20real-world%20waste%20sorting%20conditions.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/h-abid97/robust-waste-detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18799v2&entry.124074799=Read"},
{"title": "Imitative Membership Inference Attack", "author": "Yuntao Du and Yuetian Chen and Hanshen Xiao and Bruno Ribeiro and Ninghui Li", "abstract": "  A Membership Inference Attack (MIA) assesses how much a target machine\nlearning model reveals about its training data by determining whether specific\nquery instances were part of the training set. State-of-the-art MIAs rely on\ntraining hundreds of shadow models that are independent of the target model,\nleading to significant computational overhead. In this paper, we introduce\nImitative Membership Inference Attack (IMIA), which employs a novel imitative\ntraining technique to strategically construct a small number of target-informed\nimitative models that closely replicate the target model's behavior for\ninference. Extensive experimental results demonstrate that IMIA substantially\noutperforms existing MIAs in various attack settings while only requiring less\nthan 5% of the computational cost of state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2509.06796v1", "date": "2025-09-08", "relevancy": 2.1052, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4416}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.424}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitative%20Membership%20Inference%20Attack&body=Title%3A%20Imitative%20Membership%20Inference%20Attack%0AAuthor%3A%20Yuntao%20Du%20and%20Yuetian%20Chen%20and%20Hanshen%20Xiao%20and%20Bruno%20Ribeiro%20and%20Ninghui%20Li%0AAbstract%3A%20%20%20A%20Membership%20Inference%20Attack%20%28MIA%29%20assesses%20how%20much%20a%20target%20machine%0Alearning%20model%20reveals%20about%20its%20training%20data%20by%20determining%20whether%20specific%0Aquery%20instances%20were%20part%20of%20the%20training%20set.%20State-of-the-art%20MIAs%20rely%20on%0Atraining%20hundreds%20of%20shadow%20models%20that%20are%20independent%20of%20the%20target%20model%2C%0Aleading%20to%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%0AImitative%20Membership%20Inference%20Attack%20%28IMIA%29%2C%20which%20employs%20a%20novel%20imitative%0Atraining%20technique%20to%20strategically%20construct%20a%20small%20number%20of%20target-informed%0Aimitative%20models%20that%20closely%20replicate%20the%20target%20model%27s%20behavior%20for%0Ainference.%20Extensive%20experimental%20results%20demonstrate%20that%20IMIA%20substantially%0Aoutperforms%20existing%20MIAs%20in%20various%20attack%20settings%20while%20only%20requiring%20less%0Athan%205%25%20of%20the%20computational%20cost%20of%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitative%2520Membership%2520Inference%2520Attack%26entry.906535625%3DYuntao%2520Du%2520and%2520Yuetian%2520Chen%2520and%2520Hanshen%2520Xiao%2520and%2520Bruno%2520Ribeiro%2520and%2520Ninghui%2520Li%26entry.1292438233%3D%2520%2520A%2520Membership%2520Inference%2520Attack%2520%2528MIA%2529%2520assesses%2520how%2520much%2520a%2520target%2520machine%250Alearning%2520model%2520reveals%2520about%2520its%2520training%2520data%2520by%2520determining%2520whether%2520specific%250Aquery%2520instances%2520were%2520part%2520of%2520the%2520training%2520set.%2520State-of-the-art%2520MIAs%2520rely%2520on%250Atraining%2520hundreds%2520of%2520shadow%2520models%2520that%2520are%2520independent%2520of%2520the%2520target%2520model%252C%250Aleading%2520to%2520significant%2520computational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AImitative%2520Membership%2520Inference%2520Attack%2520%2528IMIA%2529%252C%2520which%2520employs%2520a%2520novel%2520imitative%250Atraining%2520technique%2520to%2520strategically%2520construct%2520a%2520small%2520number%2520of%2520target-informed%250Aimitative%2520models%2520that%2520closely%2520replicate%2520the%2520target%2520model%2527s%2520behavior%2520for%250Ainference.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520IMIA%2520substantially%250Aoutperforms%2520existing%2520MIAs%2520in%2520various%2520attack%2520settings%2520while%2520only%2520requiring%2520less%250Athan%25205%2525%2520of%2520the%2520computational%2520cost%2520of%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitative%20Membership%20Inference%20Attack&entry.906535625=Yuntao%20Du%20and%20Yuetian%20Chen%20and%20Hanshen%20Xiao%20and%20Bruno%20Ribeiro%20and%20Ninghui%20Li&entry.1292438233=%20%20A%20Membership%20Inference%20Attack%20%28MIA%29%20assesses%20how%20much%20a%20target%20machine%0Alearning%20model%20reveals%20about%20its%20training%20data%20by%20determining%20whether%20specific%0Aquery%20instances%20were%20part%20of%20the%20training%20set.%20State-of-the-art%20MIAs%20rely%20on%0Atraining%20hundreds%20of%20shadow%20models%20that%20are%20independent%20of%20the%20target%20model%2C%0Aleading%20to%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%0AImitative%20Membership%20Inference%20Attack%20%28IMIA%29%2C%20which%20employs%20a%20novel%20imitative%0Atraining%20technique%20to%20strategically%20construct%20a%20small%20number%20of%20target-informed%0Aimitative%20models%20that%20closely%20replicate%20the%20target%20model%27s%20behavior%20for%0Ainference.%20Extensive%20experimental%20results%20demonstrate%20that%20IMIA%20substantially%0Aoutperforms%20existing%20MIAs%20in%20various%20attack%20settings%20while%20only%20requiring%20less%0Athan%205%25%20of%20the%20computational%20cost%20of%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06796v1&entry.124074799=Read"},
{"title": "Tackling the Noisy Elephant in the Room: Label Noise-robust\n  Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition", "author": "Tarhib Al Azad and Shahana Ibrahim", "abstract": "  Robust out-of-distribution (OOD) detection is an indispensable component of\nmodern artificial intelligence (AI) systems, especially in safety-critical\napplications where models must identify inputs from unfamiliar classes not seen\nduring training. While OOD detection has been extensively studied in the\nmachine learning literature--with both post hoc and training-based\napproaches--its effectiveness under noisy training labels remains\nunderexplored. Recent studies suggest that label noise can significantly\ndegrade OOD performance, yet principled solutions to this issue are lacking. In\nthis work, we demonstrate that directly combining existing label noise-robust\nmethods with OOD detection strategies is insufficient to address this critical\nchallenge. To overcome this, we propose a robust OOD detection framework that\nintegrates loss correction techniques from the noisy label learning literature\nwith low-rank and sparse decomposition methods from signal processing.\nExtensive experiments on both synthetic and real-world datasets demonstrate\nthat our method significantly outperforms the state-of-the-art OOD detection\ntechniques, particularly under severe noisy label settings.\n", "link": "http://arxiv.org/abs/2509.06918v1", "date": "2025-09-08", "relevancy": 2.1011, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5677}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5226}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20the%20Noisy%20Elephant%20in%20the%20Room%3A%20Label%20Noise-robust%0A%20%20Out-of-Distribution%20Detection%20via%20Loss%20Correction%20and%20Low-rank%20Decomposition&body=Title%3A%20Tackling%20the%20Noisy%20Elephant%20in%20the%20Room%3A%20Label%20Noise-robust%0A%20%20Out-of-Distribution%20Detection%20via%20Loss%20Correction%20and%20Low-rank%20Decomposition%0AAuthor%3A%20Tarhib%20Al%20Azad%20and%20Shahana%20Ibrahim%0AAbstract%3A%20%20%20Robust%20out-of-distribution%20%28OOD%29%20detection%20is%20an%20indispensable%20component%20of%0Amodern%20artificial%20intelligence%20%28AI%29%20systems%2C%20especially%20in%20safety-critical%0Aapplications%20where%20models%20must%20identify%20inputs%20from%20unfamiliar%20classes%20not%20seen%0Aduring%20training.%20While%20OOD%20detection%20has%20been%20extensively%20studied%20in%20the%0Amachine%20learning%20literature--with%20both%20post%20hoc%20and%20training-based%0Aapproaches--its%20effectiveness%20under%20noisy%20training%20labels%20remains%0Aunderexplored.%20Recent%20studies%20suggest%20that%20label%20noise%20can%20significantly%0Adegrade%20OOD%20performance%2C%20yet%20principled%20solutions%20to%20this%20issue%20are%20lacking.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20directly%20combining%20existing%20label%20noise-robust%0Amethods%20with%20OOD%20detection%20strategies%20is%20insufficient%20to%20address%20this%20critical%0Achallenge.%20To%20overcome%20this%2C%20we%20propose%20a%20robust%20OOD%20detection%20framework%20that%0Aintegrates%20loss%20correction%20techniques%20from%20the%20noisy%20label%20learning%20literature%0Awith%20low-rank%20and%20sparse%20decomposition%20methods%20from%20signal%20processing.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%0Athat%20our%20method%20significantly%20outperforms%20the%20state-of-the-art%20OOD%20detection%0Atechniques%2C%20particularly%20under%20severe%20noisy%20label%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520the%2520Noisy%2520Elephant%2520in%2520the%2520Room%253A%2520Label%2520Noise-robust%250A%2520%2520Out-of-Distribution%2520Detection%2520via%2520Loss%2520Correction%2520and%2520Low-rank%2520Decomposition%26entry.906535625%3DTarhib%2520Al%2520Azad%2520and%2520Shahana%2520Ibrahim%26entry.1292438233%3D%2520%2520Robust%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520an%2520indispensable%2520component%2520of%250Amodern%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%252C%2520especially%2520in%2520safety-critical%250Aapplications%2520where%2520models%2520must%2520identify%2520inputs%2520from%2520unfamiliar%2520classes%2520not%2520seen%250Aduring%2520training.%2520While%2520OOD%2520detection%2520has%2520been%2520extensively%2520studied%2520in%2520the%250Amachine%2520learning%2520literature--with%2520both%2520post%2520hoc%2520and%2520training-based%250Aapproaches--its%2520effectiveness%2520under%2520noisy%2520training%2520labels%2520remains%250Aunderexplored.%2520Recent%2520studies%2520suggest%2520that%2520label%2520noise%2520can%2520significantly%250Adegrade%2520OOD%2520performance%252C%2520yet%2520principled%2520solutions%2520to%2520this%2520issue%2520are%2520lacking.%2520In%250Athis%2520work%252C%2520we%2520demonstrate%2520that%2520directly%2520combining%2520existing%2520label%2520noise-robust%250Amethods%2520with%2520OOD%2520detection%2520strategies%2520is%2520insufficient%2520to%2520address%2520this%2520critical%250Achallenge.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520robust%2520OOD%2520detection%2520framework%2520that%250Aintegrates%2520loss%2520correction%2520techniques%2520from%2520the%2520noisy%2520label%2520learning%2520literature%250Awith%2520low-rank%2520and%2520sparse%2520decomposition%2520methods%2520from%2520signal%2520processing.%250AExtensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%250Athat%2520our%2520method%2520significantly%2520outperforms%2520the%2520state-of-the-art%2520OOD%2520detection%250Atechniques%252C%2520particularly%2520under%2520severe%2520noisy%2520label%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20the%20Noisy%20Elephant%20in%20the%20Room%3A%20Label%20Noise-robust%0A%20%20Out-of-Distribution%20Detection%20via%20Loss%20Correction%20and%20Low-rank%20Decomposition&entry.906535625=Tarhib%20Al%20Azad%20and%20Shahana%20Ibrahim&entry.1292438233=%20%20Robust%20out-of-distribution%20%28OOD%29%20detection%20is%20an%20indispensable%20component%20of%0Amodern%20artificial%20intelligence%20%28AI%29%20systems%2C%20especially%20in%20safety-critical%0Aapplications%20where%20models%20must%20identify%20inputs%20from%20unfamiliar%20classes%20not%20seen%0Aduring%20training.%20While%20OOD%20detection%20has%20been%20extensively%20studied%20in%20the%0Amachine%20learning%20literature--with%20both%20post%20hoc%20and%20training-based%0Aapproaches--its%20effectiveness%20under%20noisy%20training%20labels%20remains%0Aunderexplored.%20Recent%20studies%20suggest%20that%20label%20noise%20can%20significantly%0Adegrade%20OOD%20performance%2C%20yet%20principled%20solutions%20to%20this%20issue%20are%20lacking.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20directly%20combining%20existing%20label%20noise-robust%0Amethods%20with%20OOD%20detection%20strategies%20is%20insufficient%20to%20address%20this%20critical%0Achallenge.%20To%20overcome%20this%2C%20we%20propose%20a%20robust%20OOD%20detection%20framework%20that%0Aintegrates%20loss%20correction%20techniques%20from%20the%20noisy%20label%20learning%20literature%0Awith%20low-rank%20and%20sparse%20decomposition%20methods%20from%20signal%20processing.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%0Athat%20our%20method%20significantly%20outperforms%20the%20state-of-the-art%20OOD%20detection%0Atechniques%2C%20particularly%20under%20severe%20noisy%20label%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06918v1&entry.124074799=Read"},
{"title": "A New Hybrid Model of Generative Adversarial Network and You Only Look\n  Once Algorithm for Automatic License-Plate Recognition", "author": "Behnoud Shafiezadeh and Amir Mashmool and Farshad Eshghi and Manoochehr Kelarestaghi", "abstract": "  Automatic License-Plate Recognition (ALPR) plays a pivotal role in\nIntelligent Transportation Systems (ITS) as a fundamental element of Smart\nCities. However, due to its high variability, ALPR faces challenging issues\nmore efficiently addressed by deep learning techniques. In this paper, a\nselective Generative Adversarial Network (GAN) is proposed for deblurring in\nthe preprocessing step, coupled with the state-of-the-art You-Only-Look-Once\n(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and\nthe integrated Character Segmentation (CS) and Character Recognition (CR)\nsteps. The selective preprocessing bypasses unnecessary and sometimes\ncounter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high\naccuracy and low computing cost. As a result, YOLOv5 achieves a detection time\nof 0.026 seconds for both LP and CR detection stages, facilitating real-time\napplications with exceptionally rapid responsiveness. Moreover, the proposed\nmodel achieves accuracy rates of 95\\% and 97\\% in the LPD and CR detection\nphases, respectively. Furthermore, the inclusion of the Deblur-GAN\npre-processor significantly improves detection accuracy by nearly 40\\%,\nespecially when encountering blurred License Plates (LPs).To train and test the\nlearning components, we generated and publicly released our blur and ALPR\ndatasets (using Iranian license plates as a use-case), which are more\nrepresentative of close-to-real-life ad-hoc situations. The findings\ndemonstrate that employing the state-of-the-art YOLO model results in excellent\noverall precision and detection time, making it well-suited for portable\napplications. Additionally, integrating the Deblur-GAN model as a preliminary\nprocessing step enhances the overall effectiveness of our comprehensive model,\nparticularly when confronted with blurred scenes captured by the camera as\ninput.\n", "link": "http://arxiv.org/abs/2509.06868v1", "date": "2025-09-08", "relevancy": 2.097, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.53}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Hybrid%20Model%20of%20Generative%20Adversarial%20Network%20and%20You%20Only%20Look%0A%20%20Once%20Algorithm%20for%20Automatic%20License-Plate%20Recognition&body=Title%3A%20A%20New%20Hybrid%20Model%20of%20Generative%20Adversarial%20Network%20and%20You%20Only%20Look%0A%20%20Once%20Algorithm%20for%20Automatic%20License-Plate%20Recognition%0AAuthor%3A%20Behnoud%20Shafiezadeh%20and%20Amir%20Mashmool%20and%20Farshad%20Eshghi%20and%20Manoochehr%20Kelarestaghi%0AAbstract%3A%20%20%20Automatic%20License-Plate%20Recognition%20%28ALPR%29%20plays%20a%20pivotal%20role%20in%0AIntelligent%20Transportation%20Systems%20%28ITS%29%20as%20a%20fundamental%20element%20of%20Smart%0ACities.%20However%2C%20due%20to%20its%20high%20variability%2C%20ALPR%20faces%20challenging%20issues%0Amore%20efficiently%20addressed%20by%20deep%20learning%20techniques.%20In%20this%20paper%2C%20a%0Aselective%20Generative%20Adversarial%20Network%20%28GAN%29%20is%20proposed%20for%20deblurring%20in%0Athe%20preprocessing%20step%2C%20coupled%20with%20the%20state-of-the-art%20You-Only-Look-Once%0A%28YOLO%29v5%20object%20detection%20architectures%20for%20License-Plate%20Detection%20%28LPD%29%2C%20and%0Athe%20integrated%20Character%20Segmentation%20%28CS%29%20and%20Character%20Recognition%20%28CR%29%0Asteps.%20The%20selective%20preprocessing%20bypasses%20unnecessary%20and%20sometimes%0Acounter-productive%20input%20manipulations%2C%20while%20YOLOv5%20LPD/CS%2BCR%20delivers%20high%0Aaccuracy%20and%20low%20computing%20cost.%20As%20a%20result%2C%20YOLOv5%20achieves%20a%20detection%20time%0Aof%200.026%20seconds%20for%20both%20LP%20and%20CR%20detection%20stages%2C%20facilitating%20real-time%0Aapplications%20with%20exceptionally%20rapid%20responsiveness.%20Moreover%2C%20the%20proposed%0Amodel%20achieves%20accuracy%20rates%20of%2095%5C%25%20and%2097%5C%25%20in%20the%20LPD%20and%20CR%20detection%0Aphases%2C%20respectively.%20Furthermore%2C%20the%20inclusion%20of%20the%20Deblur-GAN%0Apre-processor%20significantly%20improves%20detection%20accuracy%20by%20nearly%2040%5C%25%2C%0Aespecially%20when%20encountering%20blurred%20License%20Plates%20%28LPs%29.To%20train%20and%20test%20the%0Alearning%20components%2C%20we%20generated%20and%20publicly%20released%20our%20blur%20and%20ALPR%0Adatasets%20%28using%20Iranian%20license%20plates%20as%20a%20use-case%29%2C%20which%20are%20more%0Arepresentative%20of%20close-to-real-life%20ad-hoc%20situations.%20The%20findings%0Ademonstrate%20that%20employing%20the%20state-of-the-art%20YOLO%20model%20results%20in%20excellent%0Aoverall%20precision%20and%20detection%20time%2C%20making%20it%20well-suited%20for%20portable%0Aapplications.%20Additionally%2C%20integrating%20the%20Deblur-GAN%20model%20as%20a%20preliminary%0Aprocessing%20step%20enhances%20the%20overall%20effectiveness%20of%20our%20comprehensive%20model%2C%0Aparticularly%20when%20confronted%20with%20blurred%20scenes%20captured%20by%20the%20camera%20as%0Ainput.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Hybrid%2520Model%2520of%2520Generative%2520Adversarial%2520Network%2520and%2520You%2520Only%2520Look%250A%2520%2520Once%2520Algorithm%2520for%2520Automatic%2520License-Plate%2520Recognition%26entry.906535625%3DBehnoud%2520Shafiezadeh%2520and%2520Amir%2520Mashmool%2520and%2520Farshad%2520Eshghi%2520and%2520Manoochehr%2520Kelarestaghi%26entry.1292438233%3D%2520%2520Automatic%2520License-Plate%2520Recognition%2520%2528ALPR%2529%2520plays%2520a%2520pivotal%2520role%2520in%250AIntelligent%2520Transportation%2520Systems%2520%2528ITS%2529%2520as%2520a%2520fundamental%2520element%2520of%2520Smart%250ACities.%2520However%252C%2520due%2520to%2520its%2520high%2520variability%252C%2520ALPR%2520faces%2520challenging%2520issues%250Amore%2520efficiently%2520addressed%2520by%2520deep%2520learning%2520techniques.%2520In%2520this%2520paper%252C%2520a%250Aselective%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520is%2520proposed%2520for%2520deblurring%2520in%250Athe%2520preprocessing%2520step%252C%2520coupled%2520with%2520the%2520state-of-the-art%2520You-Only-Look-Once%250A%2528YOLO%2529v5%2520object%2520detection%2520architectures%2520for%2520License-Plate%2520Detection%2520%2528LPD%2529%252C%2520and%250Athe%2520integrated%2520Character%2520Segmentation%2520%2528CS%2529%2520and%2520Character%2520Recognition%2520%2528CR%2529%250Asteps.%2520The%2520selective%2520preprocessing%2520bypasses%2520unnecessary%2520and%2520sometimes%250Acounter-productive%2520input%2520manipulations%252C%2520while%2520YOLOv5%2520LPD/CS%252BCR%2520delivers%2520high%250Aaccuracy%2520and%2520low%2520computing%2520cost.%2520As%2520a%2520result%252C%2520YOLOv5%2520achieves%2520a%2520detection%2520time%250Aof%25200.026%2520seconds%2520for%2520both%2520LP%2520and%2520CR%2520detection%2520stages%252C%2520facilitating%2520real-time%250Aapplications%2520with%2520exceptionally%2520rapid%2520responsiveness.%2520Moreover%252C%2520the%2520proposed%250Amodel%2520achieves%2520accuracy%2520rates%2520of%252095%255C%2525%2520and%252097%255C%2525%2520in%2520the%2520LPD%2520and%2520CR%2520detection%250Aphases%252C%2520respectively.%2520Furthermore%252C%2520the%2520inclusion%2520of%2520the%2520Deblur-GAN%250Apre-processor%2520significantly%2520improves%2520detection%2520accuracy%2520by%2520nearly%252040%255C%2525%252C%250Aespecially%2520when%2520encountering%2520blurred%2520License%2520Plates%2520%2528LPs%2529.To%2520train%2520and%2520test%2520the%250Alearning%2520components%252C%2520we%2520generated%2520and%2520publicly%2520released%2520our%2520blur%2520and%2520ALPR%250Adatasets%2520%2528using%2520Iranian%2520license%2520plates%2520as%2520a%2520use-case%2529%252C%2520which%2520are%2520more%250Arepresentative%2520of%2520close-to-real-life%2520ad-hoc%2520situations.%2520The%2520findings%250Ademonstrate%2520that%2520employing%2520the%2520state-of-the-art%2520YOLO%2520model%2520results%2520in%2520excellent%250Aoverall%2520precision%2520and%2520detection%2520time%252C%2520making%2520it%2520well-suited%2520for%2520portable%250Aapplications.%2520Additionally%252C%2520integrating%2520the%2520Deblur-GAN%2520model%2520as%2520a%2520preliminary%250Aprocessing%2520step%2520enhances%2520the%2520overall%2520effectiveness%2520of%2520our%2520comprehensive%2520model%252C%250Aparticularly%2520when%2520confronted%2520with%2520blurred%2520scenes%2520captured%2520by%2520the%2520camera%2520as%250Ainput.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Hybrid%20Model%20of%20Generative%20Adversarial%20Network%20and%20You%20Only%20Look%0A%20%20Once%20Algorithm%20for%20Automatic%20License-Plate%20Recognition&entry.906535625=Behnoud%20Shafiezadeh%20and%20Amir%20Mashmool%20and%20Farshad%20Eshghi%20and%20Manoochehr%20Kelarestaghi&entry.1292438233=%20%20Automatic%20License-Plate%20Recognition%20%28ALPR%29%20plays%20a%20pivotal%20role%20in%0AIntelligent%20Transportation%20Systems%20%28ITS%29%20as%20a%20fundamental%20element%20of%20Smart%0ACities.%20However%2C%20due%20to%20its%20high%20variability%2C%20ALPR%20faces%20challenging%20issues%0Amore%20efficiently%20addressed%20by%20deep%20learning%20techniques.%20In%20this%20paper%2C%20a%0Aselective%20Generative%20Adversarial%20Network%20%28GAN%29%20is%20proposed%20for%20deblurring%20in%0Athe%20preprocessing%20step%2C%20coupled%20with%20the%20state-of-the-art%20You-Only-Look-Once%0A%28YOLO%29v5%20object%20detection%20architectures%20for%20License-Plate%20Detection%20%28LPD%29%2C%20and%0Athe%20integrated%20Character%20Segmentation%20%28CS%29%20and%20Character%20Recognition%20%28CR%29%0Asteps.%20The%20selective%20preprocessing%20bypasses%20unnecessary%20and%20sometimes%0Acounter-productive%20input%20manipulations%2C%20while%20YOLOv5%20LPD/CS%2BCR%20delivers%20high%0Aaccuracy%20and%20low%20computing%20cost.%20As%20a%20result%2C%20YOLOv5%20achieves%20a%20detection%20time%0Aof%200.026%20seconds%20for%20both%20LP%20and%20CR%20detection%20stages%2C%20facilitating%20real-time%0Aapplications%20with%20exceptionally%20rapid%20responsiveness.%20Moreover%2C%20the%20proposed%0Amodel%20achieves%20accuracy%20rates%20of%2095%5C%25%20and%2097%5C%25%20in%20the%20LPD%20and%20CR%20detection%0Aphases%2C%20respectively.%20Furthermore%2C%20the%20inclusion%20of%20the%20Deblur-GAN%0Apre-processor%20significantly%20improves%20detection%20accuracy%20by%20nearly%2040%5C%25%2C%0Aespecially%20when%20encountering%20blurred%20License%20Plates%20%28LPs%29.To%20train%20and%20test%20the%0Alearning%20components%2C%20we%20generated%20and%20publicly%20released%20our%20blur%20and%20ALPR%0Adatasets%20%28using%20Iranian%20license%20plates%20as%20a%20use-case%29%2C%20which%20are%20more%0Arepresentative%20of%20close-to-real-life%20ad-hoc%20situations.%20The%20findings%0Ademonstrate%20that%20employing%20the%20state-of-the-art%20YOLO%20model%20results%20in%20excellent%0Aoverall%20precision%20and%20detection%20time%2C%20making%20it%20well-suited%20for%20portable%0Aapplications.%20Additionally%2C%20integrating%20the%20Deblur-GAN%20model%20as%20a%20preliminary%0Aprocessing%20step%20enhances%20the%20overall%20effectiveness%20of%20our%20comprehensive%20model%2C%0Aparticularly%20when%20confronted%20with%20blurred%20scenes%20captured%20by%20the%20camera%20as%0Ainput.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06868v1&entry.124074799=Read"},
{"title": "Topological Regularization for Force Prediction in Active Particle\n  Suspension with EGNN and Persistent Homology", "author": "Sadra Saremi and Amirhossein Ahmadkhan Kordbacheh", "abstract": "  Capturing the dynamics of active particles, i.e., small self-propelled agents\nthat both deform and are deformed by a fluid in which they move is a formidable\nproblem as it requires coupling fine scale hydrodynamics with large scale\ncollective effects. So we present a multi-scale framework that combines the\nthree learning-driven tools to learn in concert within one pipeline. We use\nhigh-resolution Lattice Boltzmann snapshots of fluid velocity and particle\nstresses in a periodic box as input to the learning pipeline. the second step\ntakes the morphology and positions orientations of particles to predict\npairwise interaction forces between them with a E(2)-equivariant graph neural\nnetwork that necessarily respect flat symmetries. Then, a physics-informed\nneural network further updates these local estimates by summing over them with\na stress data using Fourier feature mappings and residual blocks that is\nadditionally regularized with a topological term (introduced by persistent\nhomology) to penalize unrealistically tangled or spurious connections. In\nconcert, these stages deliver an holistic highly-data driven full force network\nprediction empathizing on the physical underpinnings together with emerging\nmulti-scale structure typical for active matter.\n", "link": "http://arxiv.org/abs/2509.06574v1", "date": "2025-09-08", "relevancy": 2.0941, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5296}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5214}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Regularization%20for%20Force%20Prediction%20in%20Active%20Particle%0A%20%20Suspension%20with%20EGNN%20and%20Persistent%20Homology&body=Title%3A%20Topological%20Regularization%20for%20Force%20Prediction%20in%20Active%20Particle%0A%20%20Suspension%20with%20EGNN%20and%20Persistent%20Homology%0AAuthor%3A%20Sadra%20Saremi%20and%20Amirhossein%20Ahmadkhan%20Kordbacheh%0AAbstract%3A%20%20%20Capturing%20the%20dynamics%20of%20active%20particles%2C%20i.e.%2C%20small%20self-propelled%20agents%0Athat%20both%20deform%20and%20are%20deformed%20by%20a%20fluid%20in%20which%20they%20move%20is%20a%20formidable%0Aproblem%20as%20it%20requires%20coupling%20fine%20scale%20hydrodynamics%20with%20large%20scale%0Acollective%20effects.%20So%20we%20present%20a%20multi-scale%20framework%20that%20combines%20the%0Athree%20learning-driven%20tools%20to%20learn%20in%20concert%20within%20one%20pipeline.%20We%20use%0Ahigh-resolution%20Lattice%20Boltzmann%20snapshots%20of%20fluid%20velocity%20and%20particle%0Astresses%20in%20a%20periodic%20box%20as%20input%20to%20the%20learning%20pipeline.%20the%20second%20step%0Atakes%20the%20morphology%20and%20positions%20orientations%20of%20particles%20to%20predict%0Apairwise%20interaction%20forces%20between%20them%20with%20a%20E%282%29-equivariant%20graph%20neural%0Anetwork%20that%20necessarily%20respect%20flat%20symmetries.%20Then%2C%20a%20physics-informed%0Aneural%20network%20further%20updates%20these%20local%20estimates%20by%20summing%20over%20them%20with%0Aa%20stress%20data%20using%20Fourier%20feature%20mappings%20and%20residual%20blocks%20that%20is%0Aadditionally%20regularized%20with%20a%20topological%20term%20%28introduced%20by%20persistent%0Ahomology%29%20to%20penalize%20unrealistically%20tangled%20or%20spurious%20connections.%20In%0Aconcert%2C%20these%20stages%20deliver%20an%20holistic%20highly-data%20driven%20full%20force%20network%0Aprediction%20empathizing%20on%20the%20physical%20underpinnings%20together%20with%20emerging%0Amulti-scale%20structure%20typical%20for%20active%20matter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Regularization%2520for%2520Force%2520Prediction%2520in%2520Active%2520Particle%250A%2520%2520Suspension%2520with%2520EGNN%2520and%2520Persistent%2520Homology%26entry.906535625%3DSadra%2520Saremi%2520and%2520Amirhossein%2520Ahmadkhan%2520Kordbacheh%26entry.1292438233%3D%2520%2520Capturing%2520the%2520dynamics%2520of%2520active%2520particles%252C%2520i.e.%252C%2520small%2520self-propelled%2520agents%250Athat%2520both%2520deform%2520and%2520are%2520deformed%2520by%2520a%2520fluid%2520in%2520which%2520they%2520move%2520is%2520a%2520formidable%250Aproblem%2520as%2520it%2520requires%2520coupling%2520fine%2520scale%2520hydrodynamics%2520with%2520large%2520scale%250Acollective%2520effects.%2520So%2520we%2520present%2520a%2520multi-scale%2520framework%2520that%2520combines%2520the%250Athree%2520learning-driven%2520tools%2520to%2520learn%2520in%2520concert%2520within%2520one%2520pipeline.%2520We%2520use%250Ahigh-resolution%2520Lattice%2520Boltzmann%2520snapshots%2520of%2520fluid%2520velocity%2520and%2520particle%250Astresses%2520in%2520a%2520periodic%2520box%2520as%2520input%2520to%2520the%2520learning%2520pipeline.%2520the%2520second%2520step%250Atakes%2520the%2520morphology%2520and%2520positions%2520orientations%2520of%2520particles%2520to%2520predict%250Apairwise%2520interaction%2520forces%2520between%2520them%2520with%2520a%2520E%25282%2529-equivariant%2520graph%2520neural%250Anetwork%2520that%2520necessarily%2520respect%2520flat%2520symmetries.%2520Then%252C%2520a%2520physics-informed%250Aneural%2520network%2520further%2520updates%2520these%2520local%2520estimates%2520by%2520summing%2520over%2520them%2520with%250Aa%2520stress%2520data%2520using%2520Fourier%2520feature%2520mappings%2520and%2520residual%2520blocks%2520that%2520is%250Aadditionally%2520regularized%2520with%2520a%2520topological%2520term%2520%2528introduced%2520by%2520persistent%250Ahomology%2529%2520to%2520penalize%2520unrealistically%2520tangled%2520or%2520spurious%2520connections.%2520In%250Aconcert%252C%2520these%2520stages%2520deliver%2520an%2520holistic%2520highly-data%2520driven%2520full%2520force%2520network%250Aprediction%2520empathizing%2520on%2520the%2520physical%2520underpinnings%2520together%2520with%2520emerging%250Amulti-scale%2520structure%2520typical%2520for%2520active%2520matter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Regularization%20for%20Force%20Prediction%20in%20Active%20Particle%0A%20%20Suspension%20with%20EGNN%20and%20Persistent%20Homology&entry.906535625=Sadra%20Saremi%20and%20Amirhossein%20Ahmadkhan%20Kordbacheh&entry.1292438233=%20%20Capturing%20the%20dynamics%20of%20active%20particles%2C%20i.e.%2C%20small%20self-propelled%20agents%0Athat%20both%20deform%20and%20are%20deformed%20by%20a%20fluid%20in%20which%20they%20move%20is%20a%20formidable%0Aproblem%20as%20it%20requires%20coupling%20fine%20scale%20hydrodynamics%20with%20large%20scale%0Acollective%20effects.%20So%20we%20present%20a%20multi-scale%20framework%20that%20combines%20the%0Athree%20learning-driven%20tools%20to%20learn%20in%20concert%20within%20one%20pipeline.%20We%20use%0Ahigh-resolution%20Lattice%20Boltzmann%20snapshots%20of%20fluid%20velocity%20and%20particle%0Astresses%20in%20a%20periodic%20box%20as%20input%20to%20the%20learning%20pipeline.%20the%20second%20step%0Atakes%20the%20morphology%20and%20positions%20orientations%20of%20particles%20to%20predict%0Apairwise%20interaction%20forces%20between%20them%20with%20a%20E%282%29-equivariant%20graph%20neural%0Anetwork%20that%20necessarily%20respect%20flat%20symmetries.%20Then%2C%20a%20physics-informed%0Aneural%20network%20further%20updates%20these%20local%20estimates%20by%20summing%20over%20them%20with%0Aa%20stress%20data%20using%20Fourier%20feature%20mappings%20and%20residual%20blocks%20that%20is%0Aadditionally%20regularized%20with%20a%20topological%20term%20%28introduced%20by%20persistent%0Ahomology%29%20to%20penalize%20unrealistically%20tangled%20or%20spurious%20connections.%20In%0Aconcert%2C%20these%20stages%20deliver%20an%20holistic%20highly-data%20driven%20full%20force%20network%0Aprediction%20empathizing%20on%20the%20physical%20underpinnings%20together%20with%20emerging%0Amulti-scale%20structure%20typical%20for%20active%20matter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06574v1&entry.124074799=Read"},
{"title": "An Architecture Built for Federated Learning: Addressing Data\n  Heterogeneity through Adaptive Normalization-Free Feature Recalibration", "author": "Vasilis Siomos and Jonathan Passerat-Palmbach and Giacomo Tarroni", "abstract": "  Federated learning is a decentralized collaborative training paradigm\npreserving stakeholders' data ownership while improving performance and\ngeneralization. However, statistical heterogeneity among client datasets\ndegrades system performance. To address this issue, we propose Adaptive\nNormalization-free Feature Recalibration (ANFR), a model architecture-level\napproach that combines weight standardization and channel attention to combat\nheterogeneous data in FL. ANFR leverages weight standardization to avoid\nmismatched client statistics and inconsistent averaging, ensuring robustness\nunder heterogeneity, and channel attention to produce learnable scaling factors\nfor feature maps, suppressing inconsistencies across clients due to\nheterogeneity. We demonstrate that combining these techniques boosts model\nperformance beyond their individual contributions, by improving class\nselectivity and channel attention weight distribution. ANFR works with any\naggregation method, supports both global and personalized FL, and adds minimal\noverhead. Furthermore, when training with differential privacy, ANFR achieves\nan appealing balance between privacy and utility, enabling strong privacy\nguarantees without sacrificing performance. By integrating weight\nstandardization and channel attention in the backbone model, ANFR offers a\nnovel and versatile approach to the challenge of statistical heterogeneity.\nExtensive experiments show ANFR consistently outperforms established baselines\nacross various aggregation methods, datasets, and heterogeneity conditions.\nCode is provided at https://github.com/siomvas/ANFR.\n", "link": "http://arxiv.org/abs/2410.02006v2", "date": "2025-09-08", "relevancy": 2.0933, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5378}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5157}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Architecture%20Built%20for%20Federated%20Learning%3A%20Addressing%20Data%0A%20%20Heterogeneity%20through%20Adaptive%20Normalization-Free%20Feature%20Recalibration&body=Title%3A%20An%20Architecture%20Built%20for%20Federated%20Learning%3A%20Addressing%20Data%0A%20%20Heterogeneity%20through%20Adaptive%20Normalization-Free%20Feature%20Recalibration%0AAuthor%3A%20Vasilis%20Siomos%20and%20Jonathan%20Passerat-Palmbach%20and%20Giacomo%20Tarroni%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20decentralized%20collaborative%20training%20paradigm%0Apreserving%20stakeholders%27%20data%20ownership%20while%20improving%20performance%20and%0Ageneralization.%20However%2C%20statistical%20heterogeneity%20among%20client%20datasets%0Adegrades%20system%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20Adaptive%0ANormalization-free%20Feature%20Recalibration%20%28ANFR%29%2C%20a%20model%20architecture-level%0Aapproach%20that%20combines%20weight%20standardization%20and%20channel%20attention%20to%20combat%0Aheterogeneous%20data%20in%20FL.%20ANFR%20leverages%20weight%20standardization%20to%20avoid%0Amismatched%20client%20statistics%20and%20inconsistent%20averaging%2C%20ensuring%20robustness%0Aunder%20heterogeneity%2C%20and%20channel%20attention%20to%20produce%20learnable%20scaling%20factors%0Afor%20feature%20maps%2C%20suppressing%20inconsistencies%20across%20clients%20due%20to%0Aheterogeneity.%20We%20demonstrate%20that%20combining%20these%20techniques%20boosts%20model%0Aperformance%20beyond%20their%20individual%20contributions%2C%20by%20improving%20class%0Aselectivity%20and%20channel%20attention%20weight%20distribution.%20ANFR%20works%20with%20any%0Aaggregation%20method%2C%20supports%20both%20global%20and%20personalized%20FL%2C%20and%20adds%20minimal%0Aoverhead.%20Furthermore%2C%20when%20training%20with%20differential%20privacy%2C%20ANFR%20achieves%0Aan%20appealing%20balance%20between%20privacy%20and%20utility%2C%20enabling%20strong%20privacy%0Aguarantees%20without%20sacrificing%20performance.%20By%20integrating%20weight%0Astandardization%20and%20channel%20attention%20in%20the%20backbone%20model%2C%20ANFR%20offers%20a%0Anovel%20and%20versatile%20approach%20to%20the%20challenge%20of%20statistical%20heterogeneity.%0AExtensive%20experiments%20show%20ANFR%20consistently%20outperforms%20established%20baselines%0Aacross%20various%20aggregation%20methods%2C%20datasets%2C%20and%20heterogeneity%20conditions.%0ACode%20is%20provided%20at%20https%3A//github.com/siomvas/ANFR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Architecture%2520Built%2520for%2520Federated%2520Learning%253A%2520Addressing%2520Data%250A%2520%2520Heterogeneity%2520through%2520Adaptive%2520Normalization-Free%2520Feature%2520Recalibration%26entry.906535625%3DVasilis%2520Siomos%2520and%2520Jonathan%2520Passerat-Palmbach%2520and%2520Giacomo%2520Tarroni%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520decentralized%2520collaborative%2520training%2520paradigm%250Apreserving%2520stakeholders%2527%2520data%2520ownership%2520while%2520improving%2520performance%2520and%250Ageneralization.%2520However%252C%2520statistical%2520heterogeneity%2520among%2520client%2520datasets%250Adegrades%2520system%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Adaptive%250ANormalization-free%2520Feature%2520Recalibration%2520%2528ANFR%2529%252C%2520a%2520model%2520architecture-level%250Aapproach%2520that%2520combines%2520weight%2520standardization%2520and%2520channel%2520attention%2520to%2520combat%250Aheterogeneous%2520data%2520in%2520FL.%2520ANFR%2520leverages%2520weight%2520standardization%2520to%2520avoid%250Amismatched%2520client%2520statistics%2520and%2520inconsistent%2520averaging%252C%2520ensuring%2520robustness%250Aunder%2520heterogeneity%252C%2520and%2520channel%2520attention%2520to%2520produce%2520learnable%2520scaling%2520factors%250Afor%2520feature%2520maps%252C%2520suppressing%2520inconsistencies%2520across%2520clients%2520due%2520to%250Aheterogeneity.%2520We%2520demonstrate%2520that%2520combining%2520these%2520techniques%2520boosts%2520model%250Aperformance%2520beyond%2520their%2520individual%2520contributions%252C%2520by%2520improving%2520class%250Aselectivity%2520and%2520channel%2520attention%2520weight%2520distribution.%2520ANFR%2520works%2520with%2520any%250Aaggregation%2520method%252C%2520supports%2520both%2520global%2520and%2520personalized%2520FL%252C%2520and%2520adds%2520minimal%250Aoverhead.%2520Furthermore%252C%2520when%2520training%2520with%2520differential%2520privacy%252C%2520ANFR%2520achieves%250Aan%2520appealing%2520balance%2520between%2520privacy%2520and%2520utility%252C%2520enabling%2520strong%2520privacy%250Aguarantees%2520without%2520sacrificing%2520performance.%2520By%2520integrating%2520weight%250Astandardization%2520and%2520channel%2520attention%2520in%2520the%2520backbone%2520model%252C%2520ANFR%2520offers%2520a%250Anovel%2520and%2520versatile%2520approach%2520to%2520the%2520challenge%2520of%2520statistical%2520heterogeneity.%250AExtensive%2520experiments%2520show%2520ANFR%2520consistently%2520outperforms%2520established%2520baselines%250Aacross%2520various%2520aggregation%2520methods%252C%2520datasets%252C%2520and%2520heterogeneity%2520conditions.%250ACode%2520is%2520provided%2520at%2520https%253A//github.com/siomvas/ANFR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Architecture%20Built%20for%20Federated%20Learning%3A%20Addressing%20Data%0A%20%20Heterogeneity%20through%20Adaptive%20Normalization-Free%20Feature%20Recalibration&entry.906535625=Vasilis%20Siomos%20and%20Jonathan%20Passerat-Palmbach%20and%20Giacomo%20Tarroni&entry.1292438233=%20%20Federated%20learning%20is%20a%20decentralized%20collaborative%20training%20paradigm%0Apreserving%20stakeholders%27%20data%20ownership%20while%20improving%20performance%20and%0Ageneralization.%20However%2C%20statistical%20heterogeneity%20among%20client%20datasets%0Adegrades%20system%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20Adaptive%0ANormalization-free%20Feature%20Recalibration%20%28ANFR%29%2C%20a%20model%20architecture-level%0Aapproach%20that%20combines%20weight%20standardization%20and%20channel%20attention%20to%20combat%0Aheterogeneous%20data%20in%20FL.%20ANFR%20leverages%20weight%20standardization%20to%20avoid%0Amismatched%20client%20statistics%20and%20inconsistent%20averaging%2C%20ensuring%20robustness%0Aunder%20heterogeneity%2C%20and%20channel%20attention%20to%20produce%20learnable%20scaling%20factors%0Afor%20feature%20maps%2C%20suppressing%20inconsistencies%20across%20clients%20due%20to%0Aheterogeneity.%20We%20demonstrate%20that%20combining%20these%20techniques%20boosts%20model%0Aperformance%20beyond%20their%20individual%20contributions%2C%20by%20improving%20class%0Aselectivity%20and%20channel%20attention%20weight%20distribution.%20ANFR%20works%20with%20any%0Aaggregation%20method%2C%20supports%20both%20global%20and%20personalized%20FL%2C%20and%20adds%20minimal%0Aoverhead.%20Furthermore%2C%20when%20training%20with%20differential%20privacy%2C%20ANFR%20achieves%0Aan%20appealing%20balance%20between%20privacy%20and%20utility%2C%20enabling%20strong%20privacy%0Aguarantees%20without%20sacrificing%20performance.%20By%20integrating%20weight%0Astandardization%20and%20channel%20attention%20in%20the%20backbone%20model%2C%20ANFR%20offers%20a%0Anovel%20and%20versatile%20approach%20to%20the%20challenge%20of%20statistical%20heterogeneity.%0AExtensive%20experiments%20show%20ANFR%20consistently%20outperforms%20established%20baselines%0Aacross%20various%20aggregation%20methods%2C%20datasets%2C%20and%20heterogeneity%20conditions.%0ACode%20is%20provided%20at%20https%3A//github.com/siomvas/ANFR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02006v2&entry.124074799=Read"},
{"title": "Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in\n  Panoramic Radiographs using Federated, Centralized and Local Learning", "author": "Johan Andreas Balle Rubak and Khuram Naveed and Sanyam Jain and Lukas Esterle and Alexandros Iosifidis and Ruben Pauwels", "abstract": "  Objectives: Federated learning (FL) may mitigate privacy constraints,\nheterogeneous data quality, and inconsistent labeling in dental diagnostic AI.\nWe compared FL with centralized (CL) and local learning (LL) for tooth\nsegmentation in panoramic radiographs across multiple data corruption\nscenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six\ninstitutions across four settings: baseline (unaltered data); label\nmanipulation (dilated/missing annotations); image-quality manipulation\n(additive Gaussian noise); and exclusion of a faulty client with corrupted\ndata. FL was implemented via the Flower AI framework. Per-client training- and\nvalidation-loss trajectories were monitored for anomaly detection and a set of\nmetrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.\nFrom these metrics significance results were reported through Wilcoxon\nsigned-rank test. CL and LL served as comparators. Results: Baseline: FL\nachieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at\n0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).\nLabel manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:\n1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:\n1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL\nscored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:\n1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:\n1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring\nreliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and\noutperforms LL across corruption scenarios while preserving privacy. Per-client\nloss trajectories provide an effective anomaly-detection mechanism and support\nFL as a practical, privacy-preserving approach for scalable clinical AI\ndeployment.\n", "link": "http://arxiv.org/abs/2509.06553v1", "date": "2025-09-08", "relevancy": 2.0905, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Labeling%20Inaccuracy%20and%20Image%20Noise%20on%20Tooth%20Segmentation%20in%0A%20%20Panoramic%20Radiographs%20using%20Federated%2C%20Centralized%20and%20Local%20Learning&body=Title%3A%20Impact%20of%20Labeling%20Inaccuracy%20and%20Image%20Noise%20on%20Tooth%20Segmentation%20in%0A%20%20Panoramic%20Radiographs%20using%20Federated%2C%20Centralized%20and%20Local%20Learning%0AAuthor%3A%20Johan%20Andreas%20Balle%20Rubak%20and%20Khuram%20Naveed%20and%20Sanyam%20Jain%20and%20Lukas%20Esterle%20and%20Alexandros%20Iosifidis%20and%20Ruben%20Pauwels%0AAbstract%3A%20%20%20Objectives%3A%20Federated%20learning%20%28FL%29%20may%20mitigate%20privacy%20constraints%2C%0Aheterogeneous%20data%20quality%2C%20and%20inconsistent%20labeling%20in%20dental%20diagnostic%20AI.%0AWe%20compared%20FL%20with%20centralized%20%28CL%29%20and%20local%20learning%20%28LL%29%20for%20tooth%0Asegmentation%20in%20panoramic%20radiographs%20across%20multiple%20data%20corruption%0Ascenarios.%20Methods%3A%20An%20Attention%20U-Net%20was%20trained%20on%202066%20radiographs%20from%20six%0Ainstitutions%20across%20four%20settings%3A%20baseline%20%28unaltered%20data%29%3B%20label%0Amanipulation%20%28dilated/missing%20annotations%29%3B%20image-quality%20manipulation%0A%28additive%20Gaussian%20noise%29%3B%20and%20exclusion%20of%20a%20faulty%20client%20with%20corrupted%0Adata.%20FL%20was%20implemented%20via%20the%20Flower%20AI%20framework.%20Per-client%20training-%20and%0Avalidation-loss%20trajectories%20were%20monitored%20for%20anomaly%20detection%20and%20a%20set%20of%0Ametrics%20%28Dice%2C%20IoU%2C%20HD%2C%20HD95%20and%20ASSD%29%20was%20evaluated%20on%20a%20hold-out%20test%20set.%0AFrom%20these%20metrics%20significance%20results%20were%20reported%20through%20Wilcoxon%0Asigned-rank%20test.%20CL%20and%20LL%20served%20as%20comparators.%20Results%3A%20Baseline%3A%20FL%0Aachieved%20a%20median%20Dice%20of%200.94889%20%28ASSD%3A%201.33229%29%2C%20slightly%20better%20than%20CL%20at%0A0.94706%20%28ASSD%3A%201.37074%29%20and%20LL%20at%200.93557-0.94026%20%28ASSD%3A%201.51910-1.69777%29.%0ALabel%20manipulation%3A%20FL%20maintained%20the%20best%20median%20Dice%20score%20at%200.94884%20%28ASSD%3A%0A1.46487%29%20versus%20CL%27s%200.94183%20%28ASSD%3A%201.75738%29%20and%20LL%27s%200.93003-0.94026%20%28ASSD%3A%0A1.51910-2.11462%29.%20Image%20noise%3A%20FL%20led%20with%20Dice%20at%200.94853%20%28ASSD%3A%201.31088%29%3B%20CL%0Ascored%200.94787%20%28ASSD%3A%201.36131%29%3B%20LL%20ranged%20from%200.93179-0.94026%20%28ASSD%3A%0A1.51910-1.77350%29.%20Faulty-client%20exclusion%3A%20FL%20reached%20Dice%20at%200.94790%20%28ASSD%3A%0A1.33113%29%20better%20than%20CL%27s%200.94550%20%28ASSD%3A%201.39318%29.%20Loss-curve%20monitoring%0Areliably%20flagged%20the%20corrupted%20site.%20Conclusions%3A%20FL%20matches%20or%20exceeds%20CL%20and%0Aoutperforms%20LL%20across%20corruption%20scenarios%20while%20preserving%20privacy.%20Per-client%0Aloss%20trajectories%20provide%20an%20effective%20anomaly-detection%20mechanism%20and%20support%0AFL%20as%20a%20practical%2C%20privacy-preserving%20approach%20for%20scalable%20clinical%20AI%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520Labeling%2520Inaccuracy%2520and%2520Image%2520Noise%2520on%2520Tooth%2520Segmentation%2520in%250A%2520%2520Panoramic%2520Radiographs%2520using%2520Federated%252C%2520Centralized%2520and%2520Local%2520Learning%26entry.906535625%3DJohan%2520Andreas%2520Balle%2520Rubak%2520and%2520Khuram%2520Naveed%2520and%2520Sanyam%2520Jain%2520and%2520Lukas%2520Esterle%2520and%2520Alexandros%2520Iosifidis%2520and%2520Ruben%2520Pauwels%26entry.1292438233%3D%2520%2520Objectives%253A%2520Federated%2520learning%2520%2528FL%2529%2520may%2520mitigate%2520privacy%2520constraints%252C%250Aheterogeneous%2520data%2520quality%252C%2520and%2520inconsistent%2520labeling%2520in%2520dental%2520diagnostic%2520AI.%250AWe%2520compared%2520FL%2520with%2520centralized%2520%2528CL%2529%2520and%2520local%2520learning%2520%2528LL%2529%2520for%2520tooth%250Asegmentation%2520in%2520panoramic%2520radiographs%2520across%2520multiple%2520data%2520corruption%250Ascenarios.%2520Methods%253A%2520An%2520Attention%2520U-Net%2520was%2520trained%2520on%25202066%2520radiographs%2520from%2520six%250Ainstitutions%2520across%2520four%2520settings%253A%2520baseline%2520%2528unaltered%2520data%2529%253B%2520label%250Amanipulation%2520%2528dilated/missing%2520annotations%2529%253B%2520image-quality%2520manipulation%250A%2528additive%2520Gaussian%2520noise%2529%253B%2520and%2520exclusion%2520of%2520a%2520faulty%2520client%2520with%2520corrupted%250Adata.%2520FL%2520was%2520implemented%2520via%2520the%2520Flower%2520AI%2520framework.%2520Per-client%2520training-%2520and%250Avalidation-loss%2520trajectories%2520were%2520monitored%2520for%2520anomaly%2520detection%2520and%2520a%2520set%2520of%250Ametrics%2520%2528Dice%252C%2520IoU%252C%2520HD%252C%2520HD95%2520and%2520ASSD%2529%2520was%2520evaluated%2520on%2520a%2520hold-out%2520test%2520set.%250AFrom%2520these%2520metrics%2520significance%2520results%2520were%2520reported%2520through%2520Wilcoxon%250Asigned-rank%2520test.%2520CL%2520and%2520LL%2520served%2520as%2520comparators.%2520Results%253A%2520Baseline%253A%2520FL%250Aachieved%2520a%2520median%2520Dice%2520of%25200.94889%2520%2528ASSD%253A%25201.33229%2529%252C%2520slightly%2520better%2520than%2520CL%2520at%250A0.94706%2520%2528ASSD%253A%25201.37074%2529%2520and%2520LL%2520at%25200.93557-0.94026%2520%2528ASSD%253A%25201.51910-1.69777%2529.%250ALabel%2520manipulation%253A%2520FL%2520maintained%2520the%2520best%2520median%2520Dice%2520score%2520at%25200.94884%2520%2528ASSD%253A%250A1.46487%2529%2520versus%2520CL%2527s%25200.94183%2520%2528ASSD%253A%25201.75738%2529%2520and%2520LL%2527s%25200.93003-0.94026%2520%2528ASSD%253A%250A1.51910-2.11462%2529.%2520Image%2520noise%253A%2520FL%2520led%2520with%2520Dice%2520at%25200.94853%2520%2528ASSD%253A%25201.31088%2529%253B%2520CL%250Ascored%25200.94787%2520%2528ASSD%253A%25201.36131%2529%253B%2520LL%2520ranged%2520from%25200.93179-0.94026%2520%2528ASSD%253A%250A1.51910-1.77350%2529.%2520Faulty-client%2520exclusion%253A%2520FL%2520reached%2520Dice%2520at%25200.94790%2520%2528ASSD%253A%250A1.33113%2529%2520better%2520than%2520CL%2527s%25200.94550%2520%2528ASSD%253A%25201.39318%2529.%2520Loss-curve%2520monitoring%250Areliably%2520flagged%2520the%2520corrupted%2520site.%2520Conclusions%253A%2520FL%2520matches%2520or%2520exceeds%2520CL%2520and%250Aoutperforms%2520LL%2520across%2520corruption%2520scenarios%2520while%2520preserving%2520privacy.%2520Per-client%250Aloss%2520trajectories%2520provide%2520an%2520effective%2520anomaly-detection%2520mechanism%2520and%2520support%250AFL%2520as%2520a%2520practical%252C%2520privacy-preserving%2520approach%2520for%2520scalable%2520clinical%2520AI%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Labeling%20Inaccuracy%20and%20Image%20Noise%20on%20Tooth%20Segmentation%20in%0A%20%20Panoramic%20Radiographs%20using%20Federated%2C%20Centralized%20and%20Local%20Learning&entry.906535625=Johan%20Andreas%20Balle%20Rubak%20and%20Khuram%20Naveed%20and%20Sanyam%20Jain%20and%20Lukas%20Esterle%20and%20Alexandros%20Iosifidis%20and%20Ruben%20Pauwels&entry.1292438233=%20%20Objectives%3A%20Federated%20learning%20%28FL%29%20may%20mitigate%20privacy%20constraints%2C%0Aheterogeneous%20data%20quality%2C%20and%20inconsistent%20labeling%20in%20dental%20diagnostic%20AI.%0AWe%20compared%20FL%20with%20centralized%20%28CL%29%20and%20local%20learning%20%28LL%29%20for%20tooth%0Asegmentation%20in%20panoramic%20radiographs%20across%20multiple%20data%20corruption%0Ascenarios.%20Methods%3A%20An%20Attention%20U-Net%20was%20trained%20on%202066%20radiographs%20from%20six%0Ainstitutions%20across%20four%20settings%3A%20baseline%20%28unaltered%20data%29%3B%20label%0Amanipulation%20%28dilated/missing%20annotations%29%3B%20image-quality%20manipulation%0A%28additive%20Gaussian%20noise%29%3B%20and%20exclusion%20of%20a%20faulty%20client%20with%20corrupted%0Adata.%20FL%20was%20implemented%20via%20the%20Flower%20AI%20framework.%20Per-client%20training-%20and%0Avalidation-loss%20trajectories%20were%20monitored%20for%20anomaly%20detection%20and%20a%20set%20of%0Ametrics%20%28Dice%2C%20IoU%2C%20HD%2C%20HD95%20and%20ASSD%29%20was%20evaluated%20on%20a%20hold-out%20test%20set.%0AFrom%20these%20metrics%20significance%20results%20were%20reported%20through%20Wilcoxon%0Asigned-rank%20test.%20CL%20and%20LL%20served%20as%20comparators.%20Results%3A%20Baseline%3A%20FL%0Aachieved%20a%20median%20Dice%20of%200.94889%20%28ASSD%3A%201.33229%29%2C%20slightly%20better%20than%20CL%20at%0A0.94706%20%28ASSD%3A%201.37074%29%20and%20LL%20at%200.93557-0.94026%20%28ASSD%3A%201.51910-1.69777%29.%0ALabel%20manipulation%3A%20FL%20maintained%20the%20best%20median%20Dice%20score%20at%200.94884%20%28ASSD%3A%0A1.46487%29%20versus%20CL%27s%200.94183%20%28ASSD%3A%201.75738%29%20and%20LL%27s%200.93003-0.94026%20%28ASSD%3A%0A1.51910-2.11462%29.%20Image%20noise%3A%20FL%20led%20with%20Dice%20at%200.94853%20%28ASSD%3A%201.31088%29%3B%20CL%0Ascored%200.94787%20%28ASSD%3A%201.36131%29%3B%20LL%20ranged%20from%200.93179-0.94026%20%28ASSD%3A%0A1.51910-1.77350%29.%20Faulty-client%20exclusion%3A%20FL%20reached%20Dice%20at%200.94790%20%28ASSD%3A%0A1.33113%29%20better%20than%20CL%27s%200.94550%20%28ASSD%3A%201.39318%29.%20Loss-curve%20monitoring%0Areliably%20flagged%20the%20corrupted%20site.%20Conclusions%3A%20FL%20matches%20or%20exceeds%20CL%20and%0Aoutperforms%20LL%20across%20corruption%20scenarios%20while%20preserving%20privacy.%20Per-client%0Aloss%20trajectories%20provide%20an%20effective%20anomaly-detection%20mechanism%20and%20support%0AFL%20as%20a%20practical%2C%20privacy-preserving%20approach%20for%20scalable%20clinical%20AI%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06553v1&entry.124074799=Read"},
{"title": "Sequential Controlled Langevin Diffusions", "author": "Junhua Chen and Lorenz Richter and Julius Berner and Denis Blessing and Gerhard Neumann and Anima Anandkumar", "abstract": "  An effective approach for sampling from unnormalized densities is based on\nthe idea of gradually transporting samples from an easy prior to the\ncomplicated target distribution. Two popular methods are (1) Sequential Monte\nCarlo (SMC), where the transport is performed through successive annealed\ndensities via prescribed Markov chains and resampling steps, and (2) recently\ndeveloped diffusion-based sampling methods, where a learned dynamical transport\nis used. Despite the common goal, both approaches have different, often\ncomplementary, advantages and drawbacks. The resampling steps in SMC allow\nfocusing on promising regions of the space, often leading to robust\nperformance. While the algorithm enjoys asymptotic guarantees, the lack of\nflexible, learnable transitions can lead to slow convergence. On the other\nhand, diffusion-based samplers are learned and can potentially better adapt\nthemselves to the target at hand, yet often suffer from training instabilities.\nIn this work, we present a principled framework for combining SMC with\ndiffusion-based samplers by viewing both methods in continuous time and\nconsidering measures on path space. This culminates in the new Sequential\nControlled Langevin Diffusion (SCLD) sampling method, which is able to utilize\nthe benefits of both methods and reaches improved performance on multiple\nbenchmark problems, in many cases using only 10% of the training budget of\nprevious diffusion-based samplers.\n", "link": "http://arxiv.org/abs/2412.07081v2", "date": "2025-09-08", "relevancy": 2.0855, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5841}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5166}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequential%20Controlled%20Langevin%20Diffusions&body=Title%3A%20Sequential%20Controlled%20Langevin%20Diffusions%0AAuthor%3A%20Junhua%20Chen%20and%20Lorenz%20Richter%20and%20Julius%20Berner%20and%20Denis%20Blessing%20and%20Gerhard%20Neumann%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20An%20effective%20approach%20for%20sampling%20from%20unnormalized%20densities%20is%20based%20on%0Athe%20idea%20of%20gradually%20transporting%20samples%20from%20an%20easy%20prior%20to%20the%0Acomplicated%20target%20distribution.%20Two%20popular%20methods%20are%20%281%29%20Sequential%20Monte%0ACarlo%20%28SMC%29%2C%20where%20the%20transport%20is%20performed%20through%20successive%20annealed%0Adensities%20via%20prescribed%20Markov%20chains%20and%20resampling%20steps%2C%20and%20%282%29%20recently%0Adeveloped%20diffusion-based%20sampling%20methods%2C%20where%20a%20learned%20dynamical%20transport%0Ais%20used.%20Despite%20the%20common%20goal%2C%20both%20approaches%20have%20different%2C%20often%0Acomplementary%2C%20advantages%20and%20drawbacks.%20The%20resampling%20steps%20in%20SMC%20allow%0Afocusing%20on%20promising%20regions%20of%20the%20space%2C%20often%20leading%20to%20robust%0Aperformance.%20While%20the%20algorithm%20enjoys%20asymptotic%20guarantees%2C%20the%20lack%20of%0Aflexible%2C%20learnable%20transitions%20can%20lead%20to%20slow%20convergence.%20On%20the%20other%0Ahand%2C%20diffusion-based%20samplers%20are%20learned%20and%20can%20potentially%20better%20adapt%0Athemselves%20to%20the%20target%20at%20hand%2C%20yet%20often%20suffer%20from%20training%20instabilities.%0AIn%20this%20work%2C%20we%20present%20a%20principled%20framework%20for%20combining%20SMC%20with%0Adiffusion-based%20samplers%20by%20viewing%20both%20methods%20in%20continuous%20time%20and%0Aconsidering%20measures%20on%20path%20space.%20This%20culminates%20in%20the%20new%20Sequential%0AControlled%20Langevin%20Diffusion%20%28SCLD%29%20sampling%20method%2C%20which%20is%20able%20to%20utilize%0Athe%20benefits%20of%20both%20methods%20and%20reaches%20improved%20performance%20on%20multiple%0Abenchmark%20problems%2C%20in%20many%20cases%20using%20only%2010%25%20of%20the%20training%20budget%20of%0Aprevious%20diffusion-based%20samplers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequential%2520Controlled%2520Langevin%2520Diffusions%26entry.906535625%3DJunhua%2520Chen%2520and%2520Lorenz%2520Richter%2520and%2520Julius%2520Berner%2520and%2520Denis%2520Blessing%2520and%2520Gerhard%2520Neumann%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520An%2520effective%2520approach%2520for%2520sampling%2520from%2520unnormalized%2520densities%2520is%2520based%2520on%250Athe%2520idea%2520of%2520gradually%2520transporting%2520samples%2520from%2520an%2520easy%2520prior%2520to%2520the%250Acomplicated%2520target%2520distribution.%2520Two%2520popular%2520methods%2520are%2520%25281%2529%2520Sequential%2520Monte%250ACarlo%2520%2528SMC%2529%252C%2520where%2520the%2520transport%2520is%2520performed%2520through%2520successive%2520annealed%250Adensities%2520via%2520prescribed%2520Markov%2520chains%2520and%2520resampling%2520steps%252C%2520and%2520%25282%2529%2520recently%250Adeveloped%2520diffusion-based%2520sampling%2520methods%252C%2520where%2520a%2520learned%2520dynamical%2520transport%250Ais%2520used.%2520Despite%2520the%2520common%2520goal%252C%2520both%2520approaches%2520have%2520different%252C%2520often%250Acomplementary%252C%2520advantages%2520and%2520drawbacks.%2520The%2520resampling%2520steps%2520in%2520SMC%2520allow%250Afocusing%2520on%2520promising%2520regions%2520of%2520the%2520space%252C%2520often%2520leading%2520to%2520robust%250Aperformance.%2520While%2520the%2520algorithm%2520enjoys%2520asymptotic%2520guarantees%252C%2520the%2520lack%2520of%250Aflexible%252C%2520learnable%2520transitions%2520can%2520lead%2520to%2520slow%2520convergence.%2520On%2520the%2520other%250Ahand%252C%2520diffusion-based%2520samplers%2520are%2520learned%2520and%2520can%2520potentially%2520better%2520adapt%250Athemselves%2520to%2520the%2520target%2520at%2520hand%252C%2520yet%2520often%2520suffer%2520from%2520training%2520instabilities.%250AIn%2520this%2520work%252C%2520we%2520present%2520a%2520principled%2520framework%2520for%2520combining%2520SMC%2520with%250Adiffusion-based%2520samplers%2520by%2520viewing%2520both%2520methods%2520in%2520continuous%2520time%2520and%250Aconsidering%2520measures%2520on%2520path%2520space.%2520This%2520culminates%2520in%2520the%2520new%2520Sequential%250AControlled%2520Langevin%2520Diffusion%2520%2528SCLD%2529%2520sampling%2520method%252C%2520which%2520is%2520able%2520to%2520utilize%250Athe%2520benefits%2520of%2520both%2520methods%2520and%2520reaches%2520improved%2520performance%2520on%2520multiple%250Abenchmark%2520problems%252C%2520in%2520many%2520cases%2520using%2520only%252010%2525%2520of%2520the%2520training%2520budget%2520of%250Aprevious%2520diffusion-based%2520samplers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequential%20Controlled%20Langevin%20Diffusions&entry.906535625=Junhua%20Chen%20and%20Lorenz%20Richter%20and%20Julius%20Berner%20and%20Denis%20Blessing%20and%20Gerhard%20Neumann%20and%20Anima%20Anandkumar&entry.1292438233=%20%20An%20effective%20approach%20for%20sampling%20from%20unnormalized%20densities%20is%20based%20on%0Athe%20idea%20of%20gradually%20transporting%20samples%20from%20an%20easy%20prior%20to%20the%0Acomplicated%20target%20distribution.%20Two%20popular%20methods%20are%20%281%29%20Sequential%20Monte%0ACarlo%20%28SMC%29%2C%20where%20the%20transport%20is%20performed%20through%20successive%20annealed%0Adensities%20via%20prescribed%20Markov%20chains%20and%20resampling%20steps%2C%20and%20%282%29%20recently%0Adeveloped%20diffusion-based%20sampling%20methods%2C%20where%20a%20learned%20dynamical%20transport%0Ais%20used.%20Despite%20the%20common%20goal%2C%20both%20approaches%20have%20different%2C%20often%0Acomplementary%2C%20advantages%20and%20drawbacks.%20The%20resampling%20steps%20in%20SMC%20allow%0Afocusing%20on%20promising%20regions%20of%20the%20space%2C%20often%20leading%20to%20robust%0Aperformance.%20While%20the%20algorithm%20enjoys%20asymptotic%20guarantees%2C%20the%20lack%20of%0Aflexible%2C%20learnable%20transitions%20can%20lead%20to%20slow%20convergence.%20On%20the%20other%0Ahand%2C%20diffusion-based%20samplers%20are%20learned%20and%20can%20potentially%20better%20adapt%0Athemselves%20to%20the%20target%20at%20hand%2C%20yet%20often%20suffer%20from%20training%20instabilities.%0AIn%20this%20work%2C%20we%20present%20a%20principled%20framework%20for%20combining%20SMC%20with%0Adiffusion-based%20samplers%20by%20viewing%20both%20methods%20in%20continuous%20time%20and%0Aconsidering%20measures%20on%20path%20space.%20This%20culminates%20in%20the%20new%20Sequential%0AControlled%20Langevin%20Diffusion%20%28SCLD%29%20sampling%20method%2C%20which%20is%20able%20to%20utilize%0Athe%20benefits%20of%20both%20methods%20and%20reaches%20improved%20performance%20on%20multiple%0Abenchmark%20problems%2C%20in%20many%20cases%20using%20only%2010%25%20of%20the%20training%20budget%20of%0Aprevious%20diffusion-based%20samplers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07081v2&entry.124074799=Read"},
{"title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey", "author": "Wenjun Li and Zhi Chen and Jingru Lin and Hannan Cao and Wei Han and Sheng Liang and Zhi Zhang and Kuicai Dong and Dexun Li and Chen Zhang and Yong Liu", "abstract": "  Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.\n", "link": "http://arxiv.org/abs/2509.06733v1", "date": "2025-09-08", "relevancy": 2.0836, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5388}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Foundations%20for%20Deep%20Research%20Systems%3A%20A%20Survey&body=Title%3A%20Reinforcement%20Learning%20Foundations%20for%20Deep%20Research%20Systems%3A%20A%20Survey%0AAuthor%3A%20Wenjun%20Li%20and%20Zhi%20Chen%20and%20Jingru%20Lin%20and%20Hannan%20Cao%20and%20Wei%20Han%20and%20Sheng%20Liang%20and%20Zhi%20Zhang%20and%20Kuicai%20Dong%20and%20Dexun%20Li%20and%20Chen%20Zhang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Deep%20research%20systems%2C%20agentic%20AI%20that%20solve%20complex%2C%20multi-step%20tasks%20by%0Acoordinating%20reasoning%2C%20search%20across%20the%20open%20web%20and%20user%20files%2C%20and%20tool%0Ause%2C%20are%20moving%20toward%20hierarchical%20deployments%20with%20a%20Planner%2C%20Coordinator%2C%0Aand%20Executors.%20In%20practice%2C%20training%20entire%20stacks%20end-to-end%20remains%0Aimpractical%2C%20so%20most%20work%20trains%20a%20single%20planner%20connected%20to%20core%20tools%20such%0Aas%20search%2C%20browsing%2C%20and%20code.%20While%20SFT%20imparts%20protocol%20fidelity%2C%20it%20suffers%0Afrom%20imitation%20and%20exposure%20biases%20and%20underuses%20environment%20feedback.%0APreference%20alignment%20methods%20such%20as%20DPO%20are%20schema%20and%20proxy-dependent%2C%0Aoff-policy%2C%20and%20weak%20for%20long-horizon%20credit%20assignment%20and%20multi-objective%0Atrade-offs.%20A%20further%20limitation%20of%20SFT%20and%20DPO%20is%20their%20reliance%20on%20human%0Adefined%20decision%20points%20and%20subskills%20through%20schema%20design%20and%20labeled%0Acomparisons.%20Reinforcement%20learning%20aligns%20with%20closed-loop%2C%20tool-interaction%0Aresearch%20by%20optimizing%20trajectory-level%20policies%2C%20enabling%20exploration%2C%0Arecovery%20behaviors%2C%20and%20principled%20credit%20assignment%2C%20and%20it%20reduces%20dependence%0Aon%20such%20human%20priors%20and%20rater%20biases.%0A%20%20This%20survey%20is%2C%20to%20our%20knowledge%2C%20the%20first%20dedicated%20to%20the%20RL%20foundations%0Aof%20deep%20research%20systems.%20It%20systematizes%20work%20after%20DeepSeek-R1%20along%20three%0Aaxes%3A%20%28i%29%20data%20synthesis%20and%20curation%3B%20%28ii%29%20RL%20methods%20for%20agentic%20research%0Acovering%20stability%2C%20sample%20efficiency%2C%20long%20context%20handling%2C%20reward%20and%20credit%0Adesign%2C%20multi-objective%20optimization%2C%20and%20multimodal%20integration%3B%20and%20%28iii%29%0Aagentic%20RL%20training%20systems%20and%20frameworks.%20We%20also%20cover%20agent%20architecture%0Aand%20coordination%2C%20as%20well%20as%20evaluation%20and%20benchmarks%2C%20including%20recent%20QA%2C%0AVQA%2C%20long-form%20synthesis%2C%20and%20domain-grounded%2C%20tool-interaction%20tasks.%20We%0Adistill%20recurring%20patterns%2C%20surface%20infrastructure%20bottlenecks%2C%20and%20offer%0Apractical%20guidance%20for%20training%20robust%2C%20transparent%20deep%20research%20agents%20with%0ARL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Foundations%2520for%2520Deep%2520Research%2520Systems%253A%2520A%2520Survey%26entry.906535625%3DWenjun%2520Li%2520and%2520Zhi%2520Chen%2520and%2520Jingru%2520Lin%2520and%2520Hannan%2520Cao%2520and%2520Wei%2520Han%2520and%2520Sheng%2520Liang%2520and%2520Zhi%2520Zhang%2520and%2520Kuicai%2520Dong%2520and%2520Dexun%2520Li%2520and%2520Chen%2520Zhang%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Deep%2520research%2520systems%252C%2520agentic%2520AI%2520that%2520solve%2520complex%252C%2520multi-step%2520tasks%2520by%250Acoordinating%2520reasoning%252C%2520search%2520across%2520the%2520open%2520web%2520and%2520user%2520files%252C%2520and%2520tool%250Ause%252C%2520are%2520moving%2520toward%2520hierarchical%2520deployments%2520with%2520a%2520Planner%252C%2520Coordinator%252C%250Aand%2520Executors.%2520In%2520practice%252C%2520training%2520entire%2520stacks%2520end-to-end%2520remains%250Aimpractical%252C%2520so%2520most%2520work%2520trains%2520a%2520single%2520planner%2520connected%2520to%2520core%2520tools%2520such%250Aas%2520search%252C%2520browsing%252C%2520and%2520code.%2520While%2520SFT%2520imparts%2520protocol%2520fidelity%252C%2520it%2520suffers%250Afrom%2520imitation%2520and%2520exposure%2520biases%2520and%2520underuses%2520environment%2520feedback.%250APreference%2520alignment%2520methods%2520such%2520as%2520DPO%2520are%2520schema%2520and%2520proxy-dependent%252C%250Aoff-policy%252C%2520and%2520weak%2520for%2520long-horizon%2520credit%2520assignment%2520and%2520multi-objective%250Atrade-offs.%2520A%2520further%2520limitation%2520of%2520SFT%2520and%2520DPO%2520is%2520their%2520reliance%2520on%2520human%250Adefined%2520decision%2520points%2520and%2520subskills%2520through%2520schema%2520design%2520and%2520labeled%250Acomparisons.%2520Reinforcement%2520learning%2520aligns%2520with%2520closed-loop%252C%2520tool-interaction%250Aresearch%2520by%2520optimizing%2520trajectory-level%2520policies%252C%2520enabling%2520exploration%252C%250Arecovery%2520behaviors%252C%2520and%2520principled%2520credit%2520assignment%252C%2520and%2520it%2520reduces%2520dependence%250Aon%2520such%2520human%2520priors%2520and%2520rater%2520biases.%250A%2520%2520This%2520survey%2520is%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520dedicated%2520to%2520the%2520RL%2520foundations%250Aof%2520deep%2520research%2520systems.%2520It%2520systematizes%2520work%2520after%2520DeepSeek-R1%2520along%2520three%250Aaxes%253A%2520%2528i%2529%2520data%2520synthesis%2520and%2520curation%253B%2520%2528ii%2529%2520RL%2520methods%2520for%2520agentic%2520research%250Acovering%2520stability%252C%2520sample%2520efficiency%252C%2520long%2520context%2520handling%252C%2520reward%2520and%2520credit%250Adesign%252C%2520multi-objective%2520optimization%252C%2520and%2520multimodal%2520integration%253B%2520and%2520%2528iii%2529%250Aagentic%2520RL%2520training%2520systems%2520and%2520frameworks.%2520We%2520also%2520cover%2520agent%2520architecture%250Aand%2520coordination%252C%2520as%2520well%2520as%2520evaluation%2520and%2520benchmarks%252C%2520including%2520recent%2520QA%252C%250AVQA%252C%2520long-form%2520synthesis%252C%2520and%2520domain-grounded%252C%2520tool-interaction%2520tasks.%2520We%250Adistill%2520recurring%2520patterns%252C%2520surface%2520infrastructure%2520bottlenecks%252C%2520and%2520offer%250Apractical%2520guidance%2520for%2520training%2520robust%252C%2520transparent%2520deep%2520research%2520agents%2520with%250ARL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Foundations%20for%20Deep%20Research%20Systems%3A%20A%20Survey&entry.906535625=Wenjun%20Li%20and%20Zhi%20Chen%20and%20Jingru%20Lin%20and%20Hannan%20Cao%20and%20Wei%20Han%20and%20Sheng%20Liang%20and%20Zhi%20Zhang%20and%20Kuicai%20Dong%20and%20Dexun%20Li%20and%20Chen%20Zhang%20and%20Yong%20Liu&entry.1292438233=%20%20Deep%20research%20systems%2C%20agentic%20AI%20that%20solve%20complex%2C%20multi-step%20tasks%20by%0Acoordinating%20reasoning%2C%20search%20across%20the%20open%20web%20and%20user%20files%2C%20and%20tool%0Ause%2C%20are%20moving%20toward%20hierarchical%20deployments%20with%20a%20Planner%2C%20Coordinator%2C%0Aand%20Executors.%20In%20practice%2C%20training%20entire%20stacks%20end-to-end%20remains%0Aimpractical%2C%20so%20most%20work%20trains%20a%20single%20planner%20connected%20to%20core%20tools%20such%0Aas%20search%2C%20browsing%2C%20and%20code.%20While%20SFT%20imparts%20protocol%20fidelity%2C%20it%20suffers%0Afrom%20imitation%20and%20exposure%20biases%20and%20underuses%20environment%20feedback.%0APreference%20alignment%20methods%20such%20as%20DPO%20are%20schema%20and%20proxy-dependent%2C%0Aoff-policy%2C%20and%20weak%20for%20long-horizon%20credit%20assignment%20and%20multi-objective%0Atrade-offs.%20A%20further%20limitation%20of%20SFT%20and%20DPO%20is%20their%20reliance%20on%20human%0Adefined%20decision%20points%20and%20subskills%20through%20schema%20design%20and%20labeled%0Acomparisons.%20Reinforcement%20learning%20aligns%20with%20closed-loop%2C%20tool-interaction%0Aresearch%20by%20optimizing%20trajectory-level%20policies%2C%20enabling%20exploration%2C%0Arecovery%20behaviors%2C%20and%20principled%20credit%20assignment%2C%20and%20it%20reduces%20dependence%0Aon%20such%20human%20priors%20and%20rater%20biases.%0A%20%20This%20survey%20is%2C%20to%20our%20knowledge%2C%20the%20first%20dedicated%20to%20the%20RL%20foundations%0Aof%20deep%20research%20systems.%20It%20systematizes%20work%20after%20DeepSeek-R1%20along%20three%0Aaxes%3A%20%28i%29%20data%20synthesis%20and%20curation%3B%20%28ii%29%20RL%20methods%20for%20agentic%20research%0Acovering%20stability%2C%20sample%20efficiency%2C%20long%20context%20handling%2C%20reward%20and%20credit%0Adesign%2C%20multi-objective%20optimization%2C%20and%20multimodal%20integration%3B%20and%20%28iii%29%0Aagentic%20RL%20training%20systems%20and%20frameworks.%20We%20also%20cover%20agent%20architecture%0Aand%20coordination%2C%20as%20well%20as%20evaluation%20and%20benchmarks%2C%20including%20recent%20QA%2C%0AVQA%2C%20long-form%20synthesis%2C%20and%20domain-grounded%2C%20tool-interaction%20tasks.%20We%0Adistill%20recurring%20patterns%2C%20surface%20infrastructure%20bottlenecks%2C%20and%20offer%0Apractical%20guidance%20for%20training%20robust%2C%20transparent%20deep%20research%20agents%20with%0ARL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06733v1&entry.124074799=Read"},
{"title": "Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro\n  Autonomous Surface Vehicles", "author": "Zhiheng Chen and Wei Wang", "abstract": "  Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for\noperations in confined or shallow waters and swarm robotics applications.\nHowever, achieving precise and robust control at such small scales remains\nhighly challenging, mainly due to the complexity of modeling nonlinear\nhydrodynamic forces and the increased sensitivity to self-motion effects and\nenvironmental disturbances, including waves and boundary effects in confined\nspaces. This paper presents a physics-driven dynamics model for an\nover-actuated MicroASV and introduces a data-driven optimal control framework\nthat leverages a weak formulation-based online model learning method. Our\napproach continuously refines the physics-driven model in real time, enabling\nadaptive control that adjusts to changing system parameters. Simulation results\ndemonstrate that the proposed method substantially enhances trajectory tracking\naccuracy and robustness, even under unknown payloads and external disturbances.\nThese findings highlight the potential of data-driven online learning-based\noptimal control to improve MicroASV performance, paving the way for more\nreliable and precise autonomous surface vehicle operations.\n", "link": "http://arxiv.org/abs/2509.06882v1", "date": "2025-09-08", "relevancy": 2.0787, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5566}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Modeling%20and%20Efficient%20Data-Driven%20Optimal%20Control%20for%20Micro%0A%20%20Autonomous%20Surface%20Vehicles&body=Title%3A%20Dynamic%20Modeling%20and%20Efficient%20Data-Driven%20Optimal%20Control%20for%20Micro%0A%20%20Autonomous%20Surface%20Vehicles%0AAuthor%3A%20Zhiheng%20Chen%20and%20Wei%20Wang%0AAbstract%3A%20%20%20Micro%20Autonomous%20Surface%20Vehicles%20%28MicroASVs%29%20offer%20significant%20potential%20for%0Aoperations%20in%20confined%20or%20shallow%20waters%20and%20swarm%20robotics%20applications.%0AHowever%2C%20achieving%20precise%20and%20robust%20control%20at%20such%20small%20scales%20remains%0Ahighly%20challenging%2C%20mainly%20due%20to%20the%20complexity%20of%20modeling%20nonlinear%0Ahydrodynamic%20forces%20and%20the%20increased%20sensitivity%20to%20self-motion%20effects%20and%0Aenvironmental%20disturbances%2C%20including%20waves%20and%20boundary%20effects%20in%20confined%0Aspaces.%20This%20paper%20presents%20a%20physics-driven%20dynamics%20model%20for%20an%0Aover-actuated%20MicroASV%20and%20introduces%20a%20data-driven%20optimal%20control%20framework%0Athat%20leverages%20a%20weak%20formulation-based%20online%20model%20learning%20method.%20Our%0Aapproach%20continuously%20refines%20the%20physics-driven%20model%20in%20real%20time%2C%20enabling%0Aadaptive%20control%20that%20adjusts%20to%20changing%20system%20parameters.%20Simulation%20results%0Ademonstrate%20that%20the%20proposed%20method%20substantially%20enhances%20trajectory%20tracking%0Aaccuracy%20and%20robustness%2C%20even%20under%20unknown%20payloads%20and%20external%20disturbances.%0AThese%20findings%20highlight%20the%20potential%20of%20data-driven%20online%20learning-based%0Aoptimal%20control%20to%20improve%20MicroASV%20performance%2C%20paving%20the%20way%20for%20more%0Areliable%20and%20precise%20autonomous%20surface%20vehicle%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Modeling%2520and%2520Efficient%2520Data-Driven%2520Optimal%2520Control%2520for%2520Micro%250A%2520%2520Autonomous%2520Surface%2520Vehicles%26entry.906535625%3DZhiheng%2520Chen%2520and%2520Wei%2520Wang%26entry.1292438233%3D%2520%2520Micro%2520Autonomous%2520Surface%2520Vehicles%2520%2528MicroASVs%2529%2520offer%2520significant%2520potential%2520for%250Aoperations%2520in%2520confined%2520or%2520shallow%2520waters%2520and%2520swarm%2520robotics%2520applications.%250AHowever%252C%2520achieving%2520precise%2520and%2520robust%2520control%2520at%2520such%2520small%2520scales%2520remains%250Ahighly%2520challenging%252C%2520mainly%2520due%2520to%2520the%2520complexity%2520of%2520modeling%2520nonlinear%250Ahydrodynamic%2520forces%2520and%2520the%2520increased%2520sensitivity%2520to%2520self-motion%2520effects%2520and%250Aenvironmental%2520disturbances%252C%2520including%2520waves%2520and%2520boundary%2520effects%2520in%2520confined%250Aspaces.%2520This%2520paper%2520presents%2520a%2520physics-driven%2520dynamics%2520model%2520for%2520an%250Aover-actuated%2520MicroASV%2520and%2520introduces%2520a%2520data-driven%2520optimal%2520control%2520framework%250Athat%2520leverages%2520a%2520weak%2520formulation-based%2520online%2520model%2520learning%2520method.%2520Our%250Aapproach%2520continuously%2520refines%2520the%2520physics-driven%2520model%2520in%2520real%2520time%252C%2520enabling%250Aadaptive%2520control%2520that%2520adjusts%2520to%2520changing%2520system%2520parameters.%2520Simulation%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520substantially%2520enhances%2520trajectory%2520tracking%250Aaccuracy%2520and%2520robustness%252C%2520even%2520under%2520unknown%2520payloads%2520and%2520external%2520disturbances.%250AThese%2520findings%2520highlight%2520the%2520potential%2520of%2520data-driven%2520online%2520learning-based%250Aoptimal%2520control%2520to%2520improve%2520MicroASV%2520performance%252C%2520paving%2520the%2520way%2520for%2520more%250Areliable%2520and%2520precise%2520autonomous%2520surface%2520vehicle%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Modeling%20and%20Efficient%20Data-Driven%20Optimal%20Control%20for%20Micro%0A%20%20Autonomous%20Surface%20Vehicles&entry.906535625=Zhiheng%20Chen%20and%20Wei%20Wang&entry.1292438233=%20%20Micro%20Autonomous%20Surface%20Vehicles%20%28MicroASVs%29%20offer%20significant%20potential%20for%0Aoperations%20in%20confined%20or%20shallow%20waters%20and%20swarm%20robotics%20applications.%0AHowever%2C%20achieving%20precise%20and%20robust%20control%20at%20such%20small%20scales%20remains%0Ahighly%20challenging%2C%20mainly%20due%20to%20the%20complexity%20of%20modeling%20nonlinear%0Ahydrodynamic%20forces%20and%20the%20increased%20sensitivity%20to%20self-motion%20effects%20and%0Aenvironmental%20disturbances%2C%20including%20waves%20and%20boundary%20effects%20in%20confined%0Aspaces.%20This%20paper%20presents%20a%20physics-driven%20dynamics%20model%20for%20an%0Aover-actuated%20MicroASV%20and%20introduces%20a%20data-driven%20optimal%20control%20framework%0Athat%20leverages%20a%20weak%20formulation-based%20online%20model%20learning%20method.%20Our%0Aapproach%20continuously%20refines%20the%20physics-driven%20model%20in%20real%20time%2C%20enabling%0Aadaptive%20control%20that%20adjusts%20to%20changing%20system%20parameters.%20Simulation%20results%0Ademonstrate%20that%20the%20proposed%20method%20substantially%20enhances%20trajectory%20tracking%0Aaccuracy%20and%20robustness%2C%20even%20under%20unknown%20payloads%20and%20external%20disturbances.%0AThese%20findings%20highlight%20the%20potential%20of%20data-driven%20online%20learning-based%0Aoptimal%20control%20to%20improve%20MicroASV%20performance%2C%20paving%20the%20way%20for%20more%0Areliable%20and%20precise%20autonomous%20surface%20vehicle%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06882v1&entry.124074799=Read"},
{"title": "Robust and Adaptive Spectral Method for Representation Multi-Task\n  Learning with Contamination", "author": "Yian Huang and Yang Feng and Zhiliang Ying", "abstract": "  Representation-based multi-task learning (MTL) improves efficiency by\nlearning a shared structure across tasks, but its practical application is\noften hindered by contamination, outliers, or adversarial tasks. Most existing\nmethods and theories assume a clean or near-clean setting, failing when\ncontamination is significant. This paper tackles representation MTL with an\nunknown and potentially large contamination proportion, while also allowing for\nheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral\nmethod (RAS) that can distill the shared inlier representation effectively and\nefficiently, while requiring no prior knowledge of the contamination level or\nthe true representation dimension. Theoretically, we provide non-asymptotic\nerror bounds for both the learned representation and the per-task parameters.\nThese bounds adapt to inlier task similarity and outlier structure, and\nguarantee that RAS performs at least as well as single-task learning, thus\npreventing negative transfer. We also extend our framework to transfer learning\nwith corresponding theoretical guarantees for the target task. Extensive\nexperiments confirm our theory, showcasing the robustness and adaptivity of\nRAS, and its superior performance in regimes with up to 80\\% task\ncontamination.\n", "link": "http://arxiv.org/abs/2509.06575v1", "date": "2025-09-08", "relevancy": 2.0724, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5169}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Adaptive%20Spectral%20Method%20for%20Representation%20Multi-Task%0A%20%20Learning%20with%20Contamination&body=Title%3A%20Robust%20and%20Adaptive%20Spectral%20Method%20for%20Representation%20Multi-Task%0A%20%20Learning%20with%20Contamination%0AAuthor%3A%20Yian%20Huang%20and%20Yang%20Feng%20and%20Zhiliang%20Ying%0AAbstract%3A%20%20%20Representation-based%20multi-task%20learning%20%28MTL%29%20improves%20efficiency%20by%0Alearning%20a%20shared%20structure%20across%20tasks%2C%20but%20its%20practical%20application%20is%0Aoften%20hindered%20by%20contamination%2C%20outliers%2C%20or%20adversarial%20tasks.%20Most%20existing%0Amethods%20and%20theories%20assume%20a%20clean%20or%20near-clean%20setting%2C%20failing%20when%0Acontamination%20is%20significant.%20This%20paper%20tackles%20representation%20MTL%20with%20an%0Aunknown%20and%20potentially%20large%20contamination%20proportion%2C%20while%20also%20allowing%20for%0Aheterogeneity%20among%20inlier%20tasks.%20We%20introduce%20a%20Robust%20and%20Adaptive%20Spectral%0Amethod%20%28RAS%29%20that%20can%20distill%20the%20shared%20inlier%20representation%20effectively%20and%0Aefficiently%2C%20while%20requiring%20no%20prior%20knowledge%20of%20the%20contamination%20level%20or%0Athe%20true%20representation%20dimension.%20Theoretically%2C%20we%20provide%20non-asymptotic%0Aerror%20bounds%20for%20both%20the%20learned%20representation%20and%20the%20per-task%20parameters.%0AThese%20bounds%20adapt%20to%20inlier%20task%20similarity%20and%20outlier%20structure%2C%20and%0Aguarantee%20that%20RAS%20performs%20at%20least%20as%20well%20as%20single-task%20learning%2C%20thus%0Apreventing%20negative%20transfer.%20We%20also%20extend%20our%20framework%20to%20transfer%20learning%0Awith%20corresponding%20theoretical%20guarantees%20for%20the%20target%20task.%20Extensive%0Aexperiments%20confirm%20our%20theory%2C%20showcasing%20the%20robustness%20and%20adaptivity%20of%0ARAS%2C%20and%20its%20superior%20performance%20in%20regimes%20with%20up%20to%2080%5C%25%20task%0Acontamination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Adaptive%2520Spectral%2520Method%2520for%2520Representation%2520Multi-Task%250A%2520%2520Learning%2520with%2520Contamination%26entry.906535625%3DYian%2520Huang%2520and%2520Yang%2520Feng%2520and%2520Zhiliang%2520Ying%26entry.1292438233%3D%2520%2520Representation-based%2520multi-task%2520learning%2520%2528MTL%2529%2520improves%2520efficiency%2520by%250Alearning%2520a%2520shared%2520structure%2520across%2520tasks%252C%2520but%2520its%2520practical%2520application%2520is%250Aoften%2520hindered%2520by%2520contamination%252C%2520outliers%252C%2520or%2520adversarial%2520tasks.%2520Most%2520existing%250Amethods%2520and%2520theories%2520assume%2520a%2520clean%2520or%2520near-clean%2520setting%252C%2520failing%2520when%250Acontamination%2520is%2520significant.%2520This%2520paper%2520tackles%2520representation%2520MTL%2520with%2520an%250Aunknown%2520and%2520potentially%2520large%2520contamination%2520proportion%252C%2520while%2520also%2520allowing%2520for%250Aheterogeneity%2520among%2520inlier%2520tasks.%2520We%2520introduce%2520a%2520Robust%2520and%2520Adaptive%2520Spectral%250Amethod%2520%2528RAS%2529%2520that%2520can%2520distill%2520the%2520shared%2520inlier%2520representation%2520effectively%2520and%250Aefficiently%252C%2520while%2520requiring%2520no%2520prior%2520knowledge%2520of%2520the%2520contamination%2520level%2520or%250Athe%2520true%2520representation%2520dimension.%2520Theoretically%252C%2520we%2520provide%2520non-asymptotic%250Aerror%2520bounds%2520for%2520both%2520the%2520learned%2520representation%2520and%2520the%2520per-task%2520parameters.%250AThese%2520bounds%2520adapt%2520to%2520inlier%2520task%2520similarity%2520and%2520outlier%2520structure%252C%2520and%250Aguarantee%2520that%2520RAS%2520performs%2520at%2520least%2520as%2520well%2520as%2520single-task%2520learning%252C%2520thus%250Apreventing%2520negative%2520transfer.%2520We%2520also%2520extend%2520our%2520framework%2520to%2520transfer%2520learning%250Awith%2520corresponding%2520theoretical%2520guarantees%2520for%2520the%2520target%2520task.%2520Extensive%250Aexperiments%2520confirm%2520our%2520theory%252C%2520showcasing%2520the%2520robustness%2520and%2520adaptivity%2520of%250ARAS%252C%2520and%2520its%2520superior%2520performance%2520in%2520regimes%2520with%2520up%2520to%252080%255C%2525%2520task%250Acontamination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Adaptive%20Spectral%20Method%20for%20Representation%20Multi-Task%0A%20%20Learning%20with%20Contamination&entry.906535625=Yian%20Huang%20and%20Yang%20Feng%20and%20Zhiliang%20Ying&entry.1292438233=%20%20Representation-based%20multi-task%20learning%20%28MTL%29%20improves%20efficiency%20by%0Alearning%20a%20shared%20structure%20across%20tasks%2C%20but%20its%20practical%20application%20is%0Aoften%20hindered%20by%20contamination%2C%20outliers%2C%20or%20adversarial%20tasks.%20Most%20existing%0Amethods%20and%20theories%20assume%20a%20clean%20or%20near-clean%20setting%2C%20failing%20when%0Acontamination%20is%20significant.%20This%20paper%20tackles%20representation%20MTL%20with%20an%0Aunknown%20and%20potentially%20large%20contamination%20proportion%2C%20while%20also%20allowing%20for%0Aheterogeneity%20among%20inlier%20tasks.%20We%20introduce%20a%20Robust%20and%20Adaptive%20Spectral%0Amethod%20%28RAS%29%20that%20can%20distill%20the%20shared%20inlier%20representation%20effectively%20and%0Aefficiently%2C%20while%20requiring%20no%20prior%20knowledge%20of%20the%20contamination%20level%20or%0Athe%20true%20representation%20dimension.%20Theoretically%2C%20we%20provide%20non-asymptotic%0Aerror%20bounds%20for%20both%20the%20learned%20representation%20and%20the%20per-task%20parameters.%0AThese%20bounds%20adapt%20to%20inlier%20task%20similarity%20and%20outlier%20structure%2C%20and%0Aguarantee%20that%20RAS%20performs%20at%20least%20as%20well%20as%20single-task%20learning%2C%20thus%0Apreventing%20negative%20transfer.%20We%20also%20extend%20our%20framework%20to%20transfer%20learning%0Awith%20corresponding%20theoretical%20guarantees%20for%20the%20target%20task.%20Extensive%0Aexperiments%20confirm%20our%20theory%2C%20showcasing%20the%20robustness%20and%20adaptivity%20of%0ARAS%2C%20and%20its%20superior%20performance%20in%20regimes%20with%20up%20to%2080%5C%25%20task%0Acontamination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06575v1&entry.124074799=Read"},
{"title": "VIBESegmentator: Full Body MRI Segmentation for the NAKO and UK Biobank", "author": "Robert Graf and Paul-S\u00f6ren Platzek and Evamaria Olga Riedel and Constanze Ramsch\u00fctz and Sophie Starck and Hendrik Kristian M\u00f6ller and Matan Atad and Henry V\u00f6lzke and Robin B\u00fclow and Carsten Oliver Schmidt and Julia R\u00fcdebusch and Matthias Jung and Marco Reisert and Jakob Weiss and Maximilian L\u00f6ffler and Fabian Bamberg and Bene Wiestler and Johannes C. Paetzold and Daniel Rueckert and Jan Stefan Kirschke", "abstract": "  Objectives: To present a publicly available deep learning-based torso\nsegmentation model that provides comprehensive voxel-wise coverage, including\ndelineations that extend to the boundaries of anatomical compartments.\nMaterials and Methods: We extracted preliminary segmentations from\nTotalSegmentator, spine, and body composition models for Magnetic Resonance\nTomography (MR) images, then improved them iteratively and retrained an nnUNet\nmodel. Using a random retrospective subset of German National Cohort (NAKO), UK\nBiobank, internal MR and Computed Tomography (CT) data (Training: 2897 series\nfrom 626 subjects, 290 female; mean age 53+-16; 3-fold-cross validation (20%\nhold-out). Internal testing 36 series from 12 subjects, 6 male; mean age\n60+-11), we segmented 71 structures in torso MR and 72 in CT images: 20 organs,\n10 muscles, 19 vessels, 16 bones, ribs in CT, intervertebral discs, spinal\ncord, spinal canal and body composition (subcutaneous fat, unclassified muscles\nand visceral fat). For external validation, we used existing automatic organ\nsegmentations, independent ground truth segmentations on gradient echo images,\nand the Amos data. We used non-parametric bootstrapping for confidence\nintervals and Wilcoxon rank-sum test for computing statistical significance.\nResults: We achieved an average Dice score of 0.90+-0.06 on our internal\ngradient echo test set, which included 71 semantic segmentation labels. Our\nmodel ties with the best model on Amos with a Dice of 0,81+-0.14, while having\na larger field of view and a considerably higher number structures included.\nConclusion: Our work presents a publicly available full-torso segmentation\nmodel for MRI and CT images that classifies almost all subject voxels to date.\n", "link": "http://arxiv.org/abs/2406.00125v4", "date": "2025-09-08", "relevancy": 2.0683, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5607}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4866}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIBESegmentator%3A%20Full%20Body%20MRI%20Segmentation%20for%20the%20NAKO%20and%20UK%20Biobank&body=Title%3A%20VIBESegmentator%3A%20Full%20Body%20MRI%20Segmentation%20for%20the%20NAKO%20and%20UK%20Biobank%0AAuthor%3A%20Robert%20Graf%20and%20Paul-S%C3%B6ren%20Platzek%20and%20Evamaria%20Olga%20Riedel%20and%20Constanze%20Ramsch%C3%BCtz%20and%20Sophie%20Starck%20and%20Hendrik%20Kristian%20M%C3%B6ller%20and%20Matan%20Atad%20and%20Henry%20V%C3%B6lzke%20and%20Robin%20B%C3%BClow%20and%20Carsten%20Oliver%20Schmidt%20and%20Julia%20R%C3%BCdebusch%20and%20Matthias%20Jung%20and%20Marco%20Reisert%20and%20Jakob%20Weiss%20and%20Maximilian%20L%C3%B6ffler%20and%20Fabian%20Bamberg%20and%20Bene%20Wiestler%20and%20Johannes%20C.%20Paetzold%20and%20Daniel%20Rueckert%20and%20Jan%20Stefan%20Kirschke%0AAbstract%3A%20%20%20Objectives%3A%20To%20present%20a%20publicly%20available%20deep%20learning-based%20torso%0Asegmentation%20model%20that%20provides%20comprehensive%20voxel-wise%20coverage%2C%20including%0Adelineations%20that%20extend%20to%20the%20boundaries%20of%20anatomical%20compartments.%0AMaterials%20and%20Methods%3A%20We%20extracted%20preliminary%20segmentations%20from%0ATotalSegmentator%2C%20spine%2C%20and%20body%20composition%20models%20for%20Magnetic%20Resonance%0ATomography%20%28MR%29%20images%2C%20then%20improved%20them%20iteratively%20and%20retrained%20an%20nnUNet%0Amodel.%20Using%20a%20random%20retrospective%20subset%20of%20German%20National%20Cohort%20%28NAKO%29%2C%20UK%0ABiobank%2C%20internal%20MR%20and%20Computed%20Tomography%20%28CT%29%20data%20%28Training%3A%202897%20series%0Afrom%20626%20subjects%2C%20290%20female%3B%20mean%20age%2053%2B-16%3B%203-fold-cross%20validation%20%2820%25%0Ahold-out%29.%20Internal%20testing%2036%20series%20from%2012%20subjects%2C%206%20male%3B%20mean%20age%0A60%2B-11%29%2C%20we%20segmented%2071%20structures%20in%20torso%20MR%20and%2072%20in%20CT%20images%3A%2020%20organs%2C%0A10%20muscles%2C%2019%20vessels%2C%2016%20bones%2C%20ribs%20in%20CT%2C%20intervertebral%20discs%2C%20spinal%0Acord%2C%20spinal%20canal%20and%20body%20composition%20%28subcutaneous%20fat%2C%20unclassified%20muscles%0Aand%20visceral%20fat%29.%20For%20external%20validation%2C%20we%20used%20existing%20automatic%20organ%0Asegmentations%2C%20independent%20ground%20truth%20segmentations%20on%20gradient%20echo%20images%2C%0Aand%20the%20Amos%20data.%20We%20used%20non-parametric%20bootstrapping%20for%20confidence%0Aintervals%20and%20Wilcoxon%20rank-sum%20test%20for%20computing%20statistical%20significance.%0AResults%3A%20We%20achieved%20an%20average%20Dice%20score%20of%200.90%2B-0.06%20on%20our%20internal%0Agradient%20echo%20test%20set%2C%20which%20included%2071%20semantic%20segmentation%20labels.%20Our%0Amodel%20ties%20with%20the%20best%20model%20on%20Amos%20with%20a%20Dice%20of%200%2C81%2B-0.14%2C%20while%20having%0Aa%20larger%20field%20of%20view%20and%20a%20considerably%20higher%20number%20structures%20included.%0AConclusion%3A%20Our%20work%20presents%20a%20publicly%20available%20full-torso%20segmentation%0Amodel%20for%20MRI%20and%20CT%20images%20that%20classifies%20almost%20all%20subject%20voxels%20to%20date.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00125v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIBESegmentator%253A%2520Full%2520Body%2520MRI%2520Segmentation%2520for%2520the%2520NAKO%2520and%2520UK%2520Biobank%26entry.906535625%3DRobert%2520Graf%2520and%2520Paul-S%25C3%25B6ren%2520Platzek%2520and%2520Evamaria%2520Olga%2520Riedel%2520and%2520Constanze%2520Ramsch%25C3%25BCtz%2520and%2520Sophie%2520Starck%2520and%2520Hendrik%2520Kristian%2520M%25C3%25B6ller%2520and%2520Matan%2520Atad%2520and%2520Henry%2520V%25C3%25B6lzke%2520and%2520Robin%2520B%25C3%25BClow%2520and%2520Carsten%2520Oliver%2520Schmidt%2520and%2520Julia%2520R%25C3%25BCdebusch%2520and%2520Matthias%2520Jung%2520and%2520Marco%2520Reisert%2520and%2520Jakob%2520Weiss%2520and%2520Maximilian%2520L%25C3%25B6ffler%2520and%2520Fabian%2520Bamberg%2520and%2520Bene%2520Wiestler%2520and%2520Johannes%2520C.%2520Paetzold%2520and%2520Daniel%2520Rueckert%2520and%2520Jan%2520Stefan%2520Kirschke%26entry.1292438233%3D%2520%2520Objectives%253A%2520To%2520present%2520a%2520publicly%2520available%2520deep%2520learning-based%2520torso%250Asegmentation%2520model%2520that%2520provides%2520comprehensive%2520voxel-wise%2520coverage%252C%2520including%250Adelineations%2520that%2520extend%2520to%2520the%2520boundaries%2520of%2520anatomical%2520compartments.%250AMaterials%2520and%2520Methods%253A%2520We%2520extracted%2520preliminary%2520segmentations%2520from%250ATotalSegmentator%252C%2520spine%252C%2520and%2520body%2520composition%2520models%2520for%2520Magnetic%2520Resonance%250ATomography%2520%2528MR%2529%2520images%252C%2520then%2520improved%2520them%2520iteratively%2520and%2520retrained%2520an%2520nnUNet%250Amodel.%2520Using%2520a%2520random%2520retrospective%2520subset%2520of%2520German%2520National%2520Cohort%2520%2528NAKO%2529%252C%2520UK%250ABiobank%252C%2520internal%2520MR%2520and%2520Computed%2520Tomography%2520%2528CT%2529%2520data%2520%2528Training%253A%25202897%2520series%250Afrom%2520626%2520subjects%252C%2520290%2520female%253B%2520mean%2520age%252053%252B-16%253B%25203-fold-cross%2520validation%2520%252820%2525%250Ahold-out%2529.%2520Internal%2520testing%252036%2520series%2520from%252012%2520subjects%252C%25206%2520male%253B%2520mean%2520age%250A60%252B-11%2529%252C%2520we%2520segmented%252071%2520structures%2520in%2520torso%2520MR%2520and%252072%2520in%2520CT%2520images%253A%252020%2520organs%252C%250A10%2520muscles%252C%252019%2520vessels%252C%252016%2520bones%252C%2520ribs%2520in%2520CT%252C%2520intervertebral%2520discs%252C%2520spinal%250Acord%252C%2520spinal%2520canal%2520and%2520body%2520composition%2520%2528subcutaneous%2520fat%252C%2520unclassified%2520muscles%250Aand%2520visceral%2520fat%2529.%2520For%2520external%2520validation%252C%2520we%2520used%2520existing%2520automatic%2520organ%250Asegmentations%252C%2520independent%2520ground%2520truth%2520segmentations%2520on%2520gradient%2520echo%2520images%252C%250Aand%2520the%2520Amos%2520data.%2520We%2520used%2520non-parametric%2520bootstrapping%2520for%2520confidence%250Aintervals%2520and%2520Wilcoxon%2520rank-sum%2520test%2520for%2520computing%2520statistical%2520significance.%250AResults%253A%2520We%2520achieved%2520an%2520average%2520Dice%2520score%2520of%25200.90%252B-0.06%2520on%2520our%2520internal%250Agradient%2520echo%2520test%2520set%252C%2520which%2520included%252071%2520semantic%2520segmentation%2520labels.%2520Our%250Amodel%2520ties%2520with%2520the%2520best%2520model%2520on%2520Amos%2520with%2520a%2520Dice%2520of%25200%252C81%252B-0.14%252C%2520while%2520having%250Aa%2520larger%2520field%2520of%2520view%2520and%2520a%2520considerably%2520higher%2520number%2520structures%2520included.%250AConclusion%253A%2520Our%2520work%2520presents%2520a%2520publicly%2520available%2520full-torso%2520segmentation%250Amodel%2520for%2520MRI%2520and%2520CT%2520images%2520that%2520classifies%2520almost%2520all%2520subject%2520voxels%2520to%2520date.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00125v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIBESegmentator%3A%20Full%20Body%20MRI%20Segmentation%20for%20the%20NAKO%20and%20UK%20Biobank&entry.906535625=Robert%20Graf%20and%20Paul-S%C3%B6ren%20Platzek%20and%20Evamaria%20Olga%20Riedel%20and%20Constanze%20Ramsch%C3%BCtz%20and%20Sophie%20Starck%20and%20Hendrik%20Kristian%20M%C3%B6ller%20and%20Matan%20Atad%20and%20Henry%20V%C3%B6lzke%20and%20Robin%20B%C3%BClow%20and%20Carsten%20Oliver%20Schmidt%20and%20Julia%20R%C3%BCdebusch%20and%20Matthias%20Jung%20and%20Marco%20Reisert%20and%20Jakob%20Weiss%20and%20Maximilian%20L%C3%B6ffler%20and%20Fabian%20Bamberg%20and%20Bene%20Wiestler%20and%20Johannes%20C.%20Paetzold%20and%20Daniel%20Rueckert%20and%20Jan%20Stefan%20Kirschke&entry.1292438233=%20%20Objectives%3A%20To%20present%20a%20publicly%20available%20deep%20learning-based%20torso%0Asegmentation%20model%20that%20provides%20comprehensive%20voxel-wise%20coverage%2C%20including%0Adelineations%20that%20extend%20to%20the%20boundaries%20of%20anatomical%20compartments.%0AMaterials%20and%20Methods%3A%20We%20extracted%20preliminary%20segmentations%20from%0ATotalSegmentator%2C%20spine%2C%20and%20body%20composition%20models%20for%20Magnetic%20Resonance%0ATomography%20%28MR%29%20images%2C%20then%20improved%20them%20iteratively%20and%20retrained%20an%20nnUNet%0Amodel.%20Using%20a%20random%20retrospective%20subset%20of%20German%20National%20Cohort%20%28NAKO%29%2C%20UK%0ABiobank%2C%20internal%20MR%20and%20Computed%20Tomography%20%28CT%29%20data%20%28Training%3A%202897%20series%0Afrom%20626%20subjects%2C%20290%20female%3B%20mean%20age%2053%2B-16%3B%203-fold-cross%20validation%20%2820%25%0Ahold-out%29.%20Internal%20testing%2036%20series%20from%2012%20subjects%2C%206%20male%3B%20mean%20age%0A60%2B-11%29%2C%20we%20segmented%2071%20structures%20in%20torso%20MR%20and%2072%20in%20CT%20images%3A%2020%20organs%2C%0A10%20muscles%2C%2019%20vessels%2C%2016%20bones%2C%20ribs%20in%20CT%2C%20intervertebral%20discs%2C%20spinal%0Acord%2C%20spinal%20canal%20and%20body%20composition%20%28subcutaneous%20fat%2C%20unclassified%20muscles%0Aand%20visceral%20fat%29.%20For%20external%20validation%2C%20we%20used%20existing%20automatic%20organ%0Asegmentations%2C%20independent%20ground%20truth%20segmentations%20on%20gradient%20echo%20images%2C%0Aand%20the%20Amos%20data.%20We%20used%20non-parametric%20bootstrapping%20for%20confidence%0Aintervals%20and%20Wilcoxon%20rank-sum%20test%20for%20computing%20statistical%20significance.%0AResults%3A%20We%20achieved%20an%20average%20Dice%20score%20of%200.90%2B-0.06%20on%20our%20internal%0Agradient%20echo%20test%20set%2C%20which%20included%2071%20semantic%20segmentation%20labels.%20Our%0Amodel%20ties%20with%20the%20best%20model%20on%20Amos%20with%20a%20Dice%20of%200%2C81%2B-0.14%2C%20while%20having%0Aa%20larger%20field%20of%20view%20and%20a%20considerably%20higher%20number%20structures%20included.%0AConclusion%3A%20Our%20work%20presents%20a%20publicly%20available%20full-torso%20segmentation%0Amodel%20for%20MRI%20and%20CT%20images%20that%20classifies%20almost%20all%20subject%20voxels%20to%20date.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00125v4&entry.124074799=Read"},
{"title": "Generative World Explorer", "author": "Taiming Lu and Tianmin Shu and Alan Yuille and Daniel Khashabi and Jieneng Chen", "abstract": "  Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.\n", "link": "http://arxiv.org/abs/2411.11844v3", "date": "2025-09-08", "relevancy": 2.0652, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.7411}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.669}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20World%20Explorer&body=Title%3A%20Generative%20World%20Explorer%0AAuthor%3A%20Taiming%20Lu%20and%20Tianmin%20Shu%20and%20Alan%20Yuille%20and%20Daniel%20Khashabi%20and%20Jieneng%20Chen%0AAbstract%3A%20%20%20Planning%20with%20partial%20observation%20is%20a%20central%20challenge%20in%20embodied%20AI.%20A%0Amajority%20of%20prior%20works%20have%20tackled%20this%20challenge%20by%20developing%20agents%20that%0Aphysically%20explore%20their%20environment%20to%20update%20their%20beliefs%20about%20the%20world%0Astate.%20In%20contrast%2C%20humans%20can%20%24%5Ctextit%7Bimagine%7D%24%20unseen%20parts%20of%20the%20world%0Athrough%20a%20mental%20exploration%20and%20%24%5Ctextit%7Brevise%7D%24%20their%20beliefs%20with%20imagined%0Aobservations.%20Such%20updated%20beliefs%20can%20allow%20them%20to%20make%20more%20informed%0Adecisions%2C%20without%20necessitating%20the%20physical%20exploration%20of%20the%20world%20at%20all%0Atimes.%20To%20achieve%20this%20human-like%20ability%2C%20we%20introduce%20the%20%24%5Ctextit%7BGenerative%0AWorld%20Explorer%20%28Genex%29%7D%24%2C%20an%20egocentric%20world%20exploration%20framework%20that%20allows%0Aan%20agent%20to%20mentally%20explore%20a%20large-scale%203D%20world%20%28e.g.%2C%20urban%20scenes%29%20and%0Aacquire%20imagined%20observations%20to%20update%20its%20belief.%20This%20updated%20belief%20will%0Athen%20help%20the%20agent%20to%20make%20a%20more%20informed%20decision%20at%20the%20current%20step.%20To%0Atrain%20%24%5Ctextit%7BGenex%7D%24%2C%20we%20create%20a%20synthetic%20urban%20scene%20dataset%2C%20Genex-DB.%0AOur%20experimental%20results%20demonstrate%20that%20%281%29%20%24%5Ctextit%7BGenex%7D%24%20can%20generate%0Ahigh-quality%20and%20consistent%20observations%20during%20long-horizon%20exploration%20of%20a%0Alarge%20virtual%20physical%20world%20and%20%282%29%20the%20beliefs%20updated%20with%20the%20generated%0Aobservations%20can%20inform%20an%20existing%20decision-making%20model%20%28e.g.%2C%20an%20LLM%20agent%29%0Ato%20make%20better%20plans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11844v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520World%2520Explorer%26entry.906535625%3DTaiming%2520Lu%2520and%2520Tianmin%2520Shu%2520and%2520Alan%2520Yuille%2520and%2520Daniel%2520Khashabi%2520and%2520Jieneng%2520Chen%26entry.1292438233%3D%2520%2520Planning%2520with%2520partial%2520observation%2520is%2520a%2520central%2520challenge%2520in%2520embodied%2520AI.%2520A%250Amajority%2520of%2520prior%2520works%2520have%2520tackled%2520this%2520challenge%2520by%2520developing%2520agents%2520that%250Aphysically%2520explore%2520their%2520environment%2520to%2520update%2520their%2520beliefs%2520about%2520the%2520world%250Astate.%2520In%2520contrast%252C%2520humans%2520can%2520%2524%255Ctextit%257Bimagine%257D%2524%2520unseen%2520parts%2520of%2520the%2520world%250Athrough%2520a%2520mental%2520exploration%2520and%2520%2524%255Ctextit%257Brevise%257D%2524%2520their%2520beliefs%2520with%2520imagined%250Aobservations.%2520Such%2520updated%2520beliefs%2520can%2520allow%2520them%2520to%2520make%2520more%2520informed%250Adecisions%252C%2520without%2520necessitating%2520the%2520physical%2520exploration%2520of%2520the%2520world%2520at%2520all%250Atimes.%2520To%2520achieve%2520this%2520human-like%2520ability%252C%2520we%2520introduce%2520the%2520%2524%255Ctextit%257BGenerative%250AWorld%2520Explorer%2520%2528Genex%2529%257D%2524%252C%2520an%2520egocentric%2520world%2520exploration%2520framework%2520that%2520allows%250Aan%2520agent%2520to%2520mentally%2520explore%2520a%2520large-scale%25203D%2520world%2520%2528e.g.%252C%2520urban%2520scenes%2529%2520and%250Aacquire%2520imagined%2520observations%2520to%2520update%2520its%2520belief.%2520This%2520updated%2520belief%2520will%250Athen%2520help%2520the%2520agent%2520to%2520make%2520a%2520more%2520informed%2520decision%2520at%2520the%2520current%2520step.%2520To%250Atrain%2520%2524%255Ctextit%257BGenex%257D%2524%252C%2520we%2520create%2520a%2520synthetic%2520urban%2520scene%2520dataset%252C%2520Genex-DB.%250AOur%2520experimental%2520results%2520demonstrate%2520that%2520%25281%2529%2520%2524%255Ctextit%257BGenex%257D%2524%2520can%2520generate%250Ahigh-quality%2520and%2520consistent%2520observations%2520during%2520long-horizon%2520exploration%2520of%2520a%250Alarge%2520virtual%2520physical%2520world%2520and%2520%25282%2529%2520the%2520beliefs%2520updated%2520with%2520the%2520generated%250Aobservations%2520can%2520inform%2520an%2520existing%2520decision-making%2520model%2520%2528e.g.%252C%2520an%2520LLM%2520agent%2529%250Ato%2520make%2520better%2520plans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11844v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20World%20Explorer&entry.906535625=Taiming%20Lu%20and%20Tianmin%20Shu%20and%20Alan%20Yuille%20and%20Daniel%20Khashabi%20and%20Jieneng%20Chen&entry.1292438233=%20%20Planning%20with%20partial%20observation%20is%20a%20central%20challenge%20in%20embodied%20AI.%20A%0Amajority%20of%20prior%20works%20have%20tackled%20this%20challenge%20by%20developing%20agents%20that%0Aphysically%20explore%20their%20environment%20to%20update%20their%20beliefs%20about%20the%20world%0Astate.%20In%20contrast%2C%20humans%20can%20%24%5Ctextit%7Bimagine%7D%24%20unseen%20parts%20of%20the%20world%0Athrough%20a%20mental%20exploration%20and%20%24%5Ctextit%7Brevise%7D%24%20their%20beliefs%20with%20imagined%0Aobservations.%20Such%20updated%20beliefs%20can%20allow%20them%20to%20make%20more%20informed%0Adecisions%2C%20without%20necessitating%20the%20physical%20exploration%20of%20the%20world%20at%20all%0Atimes.%20To%20achieve%20this%20human-like%20ability%2C%20we%20introduce%20the%20%24%5Ctextit%7BGenerative%0AWorld%20Explorer%20%28Genex%29%7D%24%2C%20an%20egocentric%20world%20exploration%20framework%20that%20allows%0Aan%20agent%20to%20mentally%20explore%20a%20large-scale%203D%20world%20%28e.g.%2C%20urban%20scenes%29%20and%0Aacquire%20imagined%20observations%20to%20update%20its%20belief.%20This%20updated%20belief%20will%0Athen%20help%20the%20agent%20to%20make%20a%20more%20informed%20decision%20at%20the%20current%20step.%20To%0Atrain%20%24%5Ctextit%7BGenex%7D%24%2C%20we%20create%20a%20synthetic%20urban%20scene%20dataset%2C%20Genex-DB.%0AOur%20experimental%20results%20demonstrate%20that%20%281%29%20%24%5Ctextit%7BGenex%7D%24%20can%20generate%0Ahigh-quality%20and%20consistent%20observations%20during%20long-horizon%20exploration%20of%20a%0Alarge%20virtual%20physical%20world%20and%20%282%29%20the%20beliefs%20updated%20with%20the%20generated%0Aobservations%20can%20inform%20an%20existing%20decision-making%20model%20%28e.g.%2C%20an%20LLM%20agent%29%0Ato%20make%20better%20plans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11844v3&entry.124074799=Read"},
{"title": "An Adaptive Coverage Control Approach for Multiple Autonomous Off-road\n  Vehicles in Dynamic Agricultural Fields", "author": "Sajad Ahmadi and Mohammadreza Davoodi and Javad Mohammadpour Velni", "abstract": "  This paper presents an adaptive coverage control method for a fleet of\noff-road and Unmanned Ground Vehicles (UGVs) operating in dynamic\n(time-varying) agricultural environments. Traditional coverage control\napproaches often assume static conditions, making them unsuitable for\nreal-world farming scenarios where obstacles, such as moving machinery and\nuneven terrains, create continuous challenges. To address this, we propose a\nreal-time path planning framework that integrates Unmanned Aerial Vehicles\n(UAVs) for obstacle detection and terrain assessment, allowing UGVs to\ndynamically adjust their coverage paths. The environment is modeled as a\nweighted directed graph, where the edge weights are continuously updated based\non the UAV observations to reflect obstacle motion and terrain variations. The\nproposed approach incorporates Voronoi-based partitioning, adaptive edge weight\nassignment, and cost-based path optimization to enhance navigation efficiency.\nSimulation results demonstrate the effectiveness of the proposed method in\nimproving path planning, reducing traversal costs, and maintaining robust\ncoverage in the presence of dynamic obstacles and muddy terrains.\n", "link": "http://arxiv.org/abs/2509.06682v1", "date": "2025-09-08", "relevancy": 2.0648, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5151}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Coverage%20Control%20Approach%20for%20Multiple%20Autonomous%20Off-road%0A%20%20Vehicles%20in%20Dynamic%20Agricultural%20Fields&body=Title%3A%20An%20Adaptive%20Coverage%20Control%20Approach%20for%20Multiple%20Autonomous%20Off-road%0A%20%20Vehicles%20in%20Dynamic%20Agricultural%20Fields%0AAuthor%3A%20Sajad%20Ahmadi%20and%20Mohammadreza%20Davoodi%20and%20Javad%20Mohammadpour%20Velni%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20adaptive%20coverage%20control%20method%20for%20a%20fleet%20of%0Aoff-road%20and%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20operating%20in%20dynamic%0A%28time-varying%29%20agricultural%20environments.%20Traditional%20coverage%20control%0Aapproaches%20often%20assume%20static%20conditions%2C%20making%20them%20unsuitable%20for%0Areal-world%20farming%20scenarios%20where%20obstacles%2C%20such%20as%20moving%20machinery%20and%0Auneven%20terrains%2C%20create%20continuous%20challenges.%20To%20address%20this%2C%20we%20propose%20a%0Areal-time%20path%20planning%20framework%20that%20integrates%20Unmanned%20Aerial%20Vehicles%0A%28UAVs%29%20for%20obstacle%20detection%20and%20terrain%20assessment%2C%20allowing%20UGVs%20to%0Adynamically%20adjust%20their%20coverage%20paths.%20The%20environment%20is%20modeled%20as%20a%0Aweighted%20directed%20graph%2C%20where%20the%20edge%20weights%20are%20continuously%20updated%20based%0Aon%20the%20UAV%20observations%20to%20reflect%20obstacle%20motion%20and%20terrain%20variations.%20The%0Aproposed%20approach%20incorporates%20Voronoi-based%20partitioning%2C%20adaptive%20edge%20weight%0Aassignment%2C%20and%20cost-based%20path%20optimization%20to%20enhance%20navigation%20efficiency.%0ASimulation%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20in%0Aimproving%20path%20planning%2C%20reducing%20traversal%20costs%2C%20and%20maintaining%20robust%0Acoverage%20in%20the%20presence%20of%20dynamic%20obstacles%20and%20muddy%20terrains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Coverage%2520Control%2520Approach%2520for%2520Multiple%2520Autonomous%2520Off-road%250A%2520%2520Vehicles%2520in%2520Dynamic%2520Agricultural%2520Fields%26entry.906535625%3DSajad%2520Ahmadi%2520and%2520Mohammadreza%2520Davoodi%2520and%2520Javad%2520Mohammadpour%2520Velni%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520adaptive%2520coverage%2520control%2520method%2520for%2520a%2520fleet%2520of%250Aoff-road%2520and%2520Unmanned%2520Ground%2520Vehicles%2520%2528UGVs%2529%2520operating%2520in%2520dynamic%250A%2528time-varying%2529%2520agricultural%2520environments.%2520Traditional%2520coverage%2520control%250Aapproaches%2520often%2520assume%2520static%2520conditions%252C%2520making%2520them%2520unsuitable%2520for%250Areal-world%2520farming%2520scenarios%2520where%2520obstacles%252C%2520such%2520as%2520moving%2520machinery%2520and%250Auneven%2520terrains%252C%2520create%2520continuous%2520challenges.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Areal-time%2520path%2520planning%2520framework%2520that%2520integrates%2520Unmanned%2520Aerial%2520Vehicles%250A%2528UAVs%2529%2520for%2520obstacle%2520detection%2520and%2520terrain%2520assessment%252C%2520allowing%2520UGVs%2520to%250Adynamically%2520adjust%2520their%2520coverage%2520paths.%2520The%2520environment%2520is%2520modeled%2520as%2520a%250Aweighted%2520directed%2520graph%252C%2520where%2520the%2520edge%2520weights%2520are%2520continuously%2520updated%2520based%250Aon%2520the%2520UAV%2520observations%2520to%2520reflect%2520obstacle%2520motion%2520and%2520terrain%2520variations.%2520The%250Aproposed%2520approach%2520incorporates%2520Voronoi-based%2520partitioning%252C%2520adaptive%2520edge%2520weight%250Aassignment%252C%2520and%2520cost-based%2520path%2520optimization%2520to%2520enhance%2520navigation%2520efficiency.%250ASimulation%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520in%250Aimproving%2520path%2520planning%252C%2520reducing%2520traversal%2520costs%252C%2520and%2520maintaining%2520robust%250Acoverage%2520in%2520the%2520presence%2520of%2520dynamic%2520obstacles%2520and%2520muddy%2520terrains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Coverage%20Control%20Approach%20for%20Multiple%20Autonomous%20Off-road%0A%20%20Vehicles%20in%20Dynamic%20Agricultural%20Fields&entry.906535625=Sajad%20Ahmadi%20and%20Mohammadreza%20Davoodi%20and%20Javad%20Mohammadpour%20Velni&entry.1292438233=%20%20This%20paper%20presents%20an%20adaptive%20coverage%20control%20method%20for%20a%20fleet%20of%0Aoff-road%20and%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20operating%20in%20dynamic%0A%28time-varying%29%20agricultural%20environments.%20Traditional%20coverage%20control%0Aapproaches%20often%20assume%20static%20conditions%2C%20making%20them%20unsuitable%20for%0Areal-world%20farming%20scenarios%20where%20obstacles%2C%20such%20as%20moving%20machinery%20and%0Auneven%20terrains%2C%20create%20continuous%20challenges.%20To%20address%20this%2C%20we%20propose%20a%0Areal-time%20path%20planning%20framework%20that%20integrates%20Unmanned%20Aerial%20Vehicles%0A%28UAVs%29%20for%20obstacle%20detection%20and%20terrain%20assessment%2C%20allowing%20UGVs%20to%0Adynamically%20adjust%20their%20coverage%20paths.%20The%20environment%20is%20modeled%20as%20a%0Aweighted%20directed%20graph%2C%20where%20the%20edge%20weights%20are%20continuously%20updated%20based%0Aon%20the%20UAV%20observations%20to%20reflect%20obstacle%20motion%20and%20terrain%20variations.%20The%0Aproposed%20approach%20incorporates%20Voronoi-based%20partitioning%2C%20adaptive%20edge%20weight%0Aassignment%2C%20and%20cost-based%20path%20optimization%20to%20enhance%20navigation%20efficiency.%0ASimulation%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20in%0Aimproving%20path%20planning%2C%20reducing%20traversal%20costs%2C%20and%20maintaining%20robust%0Acoverage%20in%20the%20presence%20of%20dynamic%20obstacles%20and%20muddy%20terrains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06682v1&entry.124074799=Read"},
{"title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average\n  Reward Reinforcement Learning", "author": "Yang Xu and Swetha Ganesh and Vaneet Aggarwal", "abstract": "  We present a non-asymptotic convergence analysis of $Q$-learning and\nactor-critic algorithms for robust average-reward Markov Decision Processes\n(MDPs) under contamination, total-variation (TV) distance, and Wasserstein\nuncertainty sets. A key ingredient of our analysis is showing that the optimal\nrobust $Q$ operator is a strict contraction with respect to a carefully\ndesigned semi-norm (with constant functions quotiented out). This property\nenables a stochastic approximation update that learns the optimal robust\n$Q$-function using $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ samples. We also\nprovide an efficient routine for robust $Q$-function estimation, which in turn\nfacilitates robust critic estimation. Building on this, we introduce an\nactor-critic algorithm that learns an $\\epsilon$-optimal robust policy within\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ samples. We provide numerical simulations\nto evaluate the performance of our algorithms.\n", "link": "http://arxiv.org/abs/2506.07040v2", "date": "2025-09-08", "relevancy": 1.3567, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.464}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20%24Q%24-Learning%20and%20Actor-Critic%20Methods%20for%20Robust%20Average%0A%20%20Reward%20Reinforcement%20Learning&body=Title%3A%20Efficient%20%24Q%24-Learning%20and%20Actor-Critic%20Methods%20for%20Robust%20Average%0A%20%20Reward%20Reinforcement%20Learning%0AAuthor%3A%20Yang%20Xu%20and%20Swetha%20Ganesh%20and%20Vaneet%20Aggarwal%0AAbstract%3A%20%20%20We%20present%20a%20non-asymptotic%20convergence%20analysis%20of%20%24Q%24-learning%20and%0Aactor-critic%20algorithms%20for%20robust%20average-reward%20Markov%20Decision%20Processes%0A%28MDPs%29%20under%20contamination%2C%20total-variation%20%28TV%29%20distance%2C%20and%20Wasserstein%0Auncertainty%20sets.%20A%20key%20ingredient%20of%20our%20analysis%20is%20showing%20that%20the%20optimal%0Arobust%20%24Q%24%20operator%20is%20a%20strict%20contraction%20with%20respect%20to%20a%20carefully%0Adesigned%20semi-norm%20%28with%20constant%20functions%20quotiented%20out%29.%20This%20property%0Aenables%20a%20stochastic%20approximation%20update%20that%20learns%20the%20optimal%20robust%0A%24Q%24-function%20using%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20samples.%20We%20also%0Aprovide%20an%20efficient%20routine%20for%20robust%20%24Q%24-function%20estimation%2C%20which%20in%20turn%0Afacilitates%20robust%20critic%20estimation.%20Building%20on%20this%2C%20we%20introduce%20an%0Aactor-critic%20algorithm%20that%20learns%20an%20%24%5Cepsilon%24-optimal%20robust%20policy%20within%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20samples.%20We%20provide%20numerical%20simulations%0Ato%20evaluate%20the%20performance%20of%20our%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520%2524Q%2524-Learning%2520and%2520Actor-Critic%2520Methods%2520for%2520Robust%2520Average%250A%2520%2520Reward%2520Reinforcement%2520Learning%26entry.906535625%3DYang%2520Xu%2520and%2520Swetha%2520Ganesh%2520and%2520Vaneet%2520Aggarwal%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520non-asymptotic%2520convergence%2520analysis%2520of%2520%2524Q%2524-learning%2520and%250Aactor-critic%2520algorithms%2520for%2520robust%2520average-reward%2520Markov%2520Decision%2520Processes%250A%2528MDPs%2529%2520under%2520contamination%252C%2520total-variation%2520%2528TV%2529%2520distance%252C%2520and%2520Wasserstein%250Auncertainty%2520sets.%2520A%2520key%2520ingredient%2520of%2520our%2520analysis%2520is%2520showing%2520that%2520the%2520optimal%250Arobust%2520%2524Q%2524%2520operator%2520is%2520a%2520strict%2520contraction%2520with%2520respect%2520to%2520a%2520carefully%250Adesigned%2520semi-norm%2520%2528with%2520constant%2520functions%2520quotiented%2520out%2529.%2520This%2520property%250Aenables%2520a%2520stochastic%2520approximation%2520update%2520that%2520learns%2520the%2520optimal%2520robust%250A%2524Q%2524-function%2520using%2520%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cepsilon%255E%257B-2%257D%2529%2524%2520samples.%2520We%2520also%250Aprovide%2520an%2520efficient%2520routine%2520for%2520robust%2520%2524Q%2524-function%2520estimation%252C%2520which%2520in%2520turn%250Afacilitates%2520robust%2520critic%2520estimation.%2520Building%2520on%2520this%252C%2520we%2520introduce%2520an%250Aactor-critic%2520algorithm%2520that%2520learns%2520an%2520%2524%255Cepsilon%2524-optimal%2520robust%2520policy%2520within%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cepsilon%255E%257B-2%257D%2529%2524%2520samples.%2520We%2520provide%2520numerical%2520simulations%250Ato%2520evaluate%2520the%2520performance%2520of%2520our%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20%24Q%24-Learning%20and%20Actor-Critic%20Methods%20for%20Robust%20Average%0A%20%20Reward%20Reinforcement%20Learning&entry.906535625=Yang%20Xu%20and%20Swetha%20Ganesh%20and%20Vaneet%20Aggarwal&entry.1292438233=%20%20We%20present%20a%20non-asymptotic%20convergence%20analysis%20of%20%24Q%24-learning%20and%0Aactor-critic%20algorithms%20for%20robust%20average-reward%20Markov%20Decision%20Processes%0A%28MDPs%29%20under%20contamination%2C%20total-variation%20%28TV%29%20distance%2C%20and%20Wasserstein%0Auncertainty%20sets.%20A%20key%20ingredient%20of%20our%20analysis%20is%20showing%20that%20the%20optimal%0Arobust%20%24Q%24%20operator%20is%20a%20strict%20contraction%20with%20respect%20to%20a%20carefully%0Adesigned%20semi-norm%20%28with%20constant%20functions%20quotiented%20out%29.%20This%20property%0Aenables%20a%20stochastic%20approximation%20update%20that%20learns%20the%20optimal%20robust%0A%24Q%24-function%20using%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20samples.%20We%20also%0Aprovide%20an%20efficient%20routine%20for%20robust%20%24Q%24-function%20estimation%2C%20which%20in%20turn%0Afacilitates%20robust%20critic%20estimation.%20Building%20on%20this%2C%20we%20introduce%20an%0Aactor-critic%20algorithm%20that%20learns%20an%20%24%5Cepsilon%24-optimal%20robust%20policy%20within%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20samples.%20We%20provide%20numerical%20simulations%0Ato%20evaluate%20the%20performance%20of%20our%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07040v2&entry.124074799=Read"},
{"title": "Tackling Device Data Distribution Real-time Shift via Prototype-based\n  Parameter Editing", "author": "Zheqi Lv and Wenqiao Zhang and Kairui Fu and Qi Tian and Shengyu Zhang and Jiajie Su and Jingyuan Chen and Kun Kuang and Fei Wu", "abstract": "  The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality.\n", "link": "http://arxiv.org/abs/2509.06552v1", "date": "2025-09-08", "relevancy": 1.6003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5413}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5391}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20Device%20Data%20Distribution%20Real-time%20Shift%20via%20Prototype-based%0A%20%20Parameter%20Editing&body=Title%3A%20Tackling%20Device%20Data%20Distribution%20Real-time%20Shift%20via%20Prototype-based%0A%20%20Parameter%20Editing%0AAuthor%3A%20Zheqi%20Lv%20and%20Wenqiao%20Zhang%20and%20Kairui%20Fu%20and%20Qi%20Tian%20and%20Shengyu%20Zhang%20and%20Jiajie%20Su%20and%20Jingyuan%20Chen%20and%20Kun%20Kuang%20and%20Fei%20Wu%0AAbstract%3A%20%20%20The%20on-device%20real-time%20data%20distribution%20shift%20on%20devices%20challenges%20the%0Ageneralization%20of%20lightweight%20on-device%20models.%20This%20critical%20issue%20is%20often%0Aoverlooked%20in%20current%20research%2C%20which%20predominantly%20relies%20on%20data-intensive%0Aand%20computationally%20expensive%20fine-tuning%20approaches.%20To%20tackle%20this%2C%20we%0Aintroduce%20Persona%2C%20a%20novel%20personalized%20method%20using%20a%20prototype-based%2C%0Abackpropagation-free%20parameter%20editing%20framework%20to%20enhance%20model%0Ageneralization%20without%20post-deployment%20retraining.%20Persona%20employs%20a%20neural%0Aadapter%20in%20the%20cloud%20to%20generate%20a%20parameter%20editing%20matrix%20based%20on%20real-time%0Adevice%20data.%20This%20matrix%20adeptly%20adapts%20on-device%20models%20to%20the%20prevailing%20data%0Adistributions%2C%20efficiently%20clustering%20them%20into%20prototype%20models.%20The%0Aprototypes%20are%20dynamically%20refined%20via%20the%20parameter%20editing%20matrix%2C%0Afacilitating%20efficient%20evolution.%20Furthermore%2C%20the%20integration%20of%20cross-layer%0Aknowledge%20transfer%20ensures%20consistent%20and%20context-aware%20multi-layer%20parameter%0Achanges%20and%20prototype%20assignment.%20Extensive%20experiments%20on%20vision%20task%20and%0Arecommendation%20task%20on%20multiple%20datasets%20confirm%20Persona%27s%20effectiveness%20and%0Agenerality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520Device%2520Data%2520Distribution%2520Real-time%2520Shift%2520via%2520Prototype-based%250A%2520%2520Parameter%2520Editing%26entry.906535625%3DZheqi%2520Lv%2520and%2520Wenqiao%2520Zhang%2520and%2520Kairui%2520Fu%2520and%2520Qi%2520Tian%2520and%2520Shengyu%2520Zhang%2520and%2520Jiajie%2520Su%2520and%2520Jingyuan%2520Chen%2520and%2520Kun%2520Kuang%2520and%2520Fei%2520Wu%26entry.1292438233%3D%2520%2520The%2520on-device%2520real-time%2520data%2520distribution%2520shift%2520on%2520devices%2520challenges%2520the%250Ageneralization%2520of%2520lightweight%2520on-device%2520models.%2520This%2520critical%2520issue%2520is%2520often%250Aoverlooked%2520in%2520current%2520research%252C%2520which%2520predominantly%2520relies%2520on%2520data-intensive%250Aand%2520computationally%2520expensive%2520fine-tuning%2520approaches.%2520To%2520tackle%2520this%252C%2520we%250Aintroduce%2520Persona%252C%2520a%2520novel%2520personalized%2520method%2520using%2520a%2520prototype-based%252C%250Abackpropagation-free%2520parameter%2520editing%2520framework%2520to%2520enhance%2520model%250Ageneralization%2520without%2520post-deployment%2520retraining.%2520Persona%2520employs%2520a%2520neural%250Aadapter%2520in%2520the%2520cloud%2520to%2520generate%2520a%2520parameter%2520editing%2520matrix%2520based%2520on%2520real-time%250Adevice%2520data.%2520This%2520matrix%2520adeptly%2520adapts%2520on-device%2520models%2520to%2520the%2520prevailing%2520data%250Adistributions%252C%2520efficiently%2520clustering%2520them%2520into%2520prototype%2520models.%2520The%250Aprototypes%2520are%2520dynamically%2520refined%2520via%2520the%2520parameter%2520editing%2520matrix%252C%250Afacilitating%2520efficient%2520evolution.%2520Furthermore%252C%2520the%2520integration%2520of%2520cross-layer%250Aknowledge%2520transfer%2520ensures%2520consistent%2520and%2520context-aware%2520multi-layer%2520parameter%250Achanges%2520and%2520prototype%2520assignment.%2520Extensive%2520experiments%2520on%2520vision%2520task%2520and%250Arecommendation%2520task%2520on%2520multiple%2520datasets%2520confirm%2520Persona%2527s%2520effectiveness%2520and%250Agenerality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20Device%20Data%20Distribution%20Real-time%20Shift%20via%20Prototype-based%0A%20%20Parameter%20Editing&entry.906535625=Zheqi%20Lv%20and%20Wenqiao%20Zhang%20and%20Kairui%20Fu%20and%20Qi%20Tian%20and%20Shengyu%20Zhang%20and%20Jiajie%20Su%20and%20Jingyuan%20Chen%20and%20Kun%20Kuang%20and%20Fei%20Wu&entry.1292438233=%20%20The%20on-device%20real-time%20data%20distribution%20shift%20on%20devices%20challenges%20the%0Ageneralization%20of%20lightweight%20on-device%20models.%20This%20critical%20issue%20is%20often%0Aoverlooked%20in%20current%20research%2C%20which%20predominantly%20relies%20on%20data-intensive%0Aand%20computationally%20expensive%20fine-tuning%20approaches.%20To%20tackle%20this%2C%20we%0Aintroduce%20Persona%2C%20a%20novel%20personalized%20method%20using%20a%20prototype-based%2C%0Abackpropagation-free%20parameter%20editing%20framework%20to%20enhance%20model%0Ageneralization%20without%20post-deployment%20retraining.%20Persona%20employs%20a%20neural%0Aadapter%20in%20the%20cloud%20to%20generate%20a%20parameter%20editing%20matrix%20based%20on%20real-time%0Adevice%20data.%20This%20matrix%20adeptly%20adapts%20on-device%20models%20to%20the%20prevailing%20data%0Adistributions%2C%20efficiently%20clustering%20them%20into%20prototype%20models.%20The%0Aprototypes%20are%20dynamically%20refined%20via%20the%20parameter%20editing%20matrix%2C%0Afacilitating%20efficient%20evolution.%20Furthermore%2C%20the%20integration%20of%20cross-layer%0Aknowledge%20transfer%20ensures%20consistent%20and%20context-aware%20multi-layer%20parameter%0Achanges%20and%20prototype%20assignment.%20Extensive%20experiments%20on%20vision%20task%20and%0Arecommendation%20task%20on%20multiple%20datasets%20confirm%20Persona%27s%20effectiveness%20and%0Agenerality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06552v1&entry.124074799=Read"},
{"title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning", "author": "Haozhe Wang and Qixin Xu and Che Liu and Junhong Wu and Fangzhen Lin and Wenhu Chen", "abstract": "  Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.\n", "link": "http://arxiv.org/abs/2509.03646v2", "date": "2025-09-08", "relevancy": 1.9932, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Hierarchical%20Reasoning%20in%20LLMs%20through%20Reinforcement%20Learning&body=Title%3A%20Emergent%20Hierarchical%20Reasoning%20in%20LLMs%20through%20Reinforcement%20Learning%0AAuthor%3A%20Haozhe%20Wang%20and%20Qixin%20Xu%20and%20Che%20Liu%20and%20Junhong%20Wu%20and%20Fangzhen%20Lin%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20has%20proven%20highly%20effective%20at%20enhancing%20the%0Acomplex%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20underlying%0Amechanisms%20driving%20this%20success%20remain%20largely%20opaque.%20Our%20analysis%20reveals%0Athat%20puzzling%20phenomena%20like%20%60%60aha%20moments%22%2C%20%60%60length-scaling%27%27%20and%20entropy%0Adynamics%20are%20not%20disparate%20occurrences%20but%20hallmarks%20of%20an%20emergent%20reasoning%0Ahierarchy%2C%20akin%20to%20the%20separation%20of%20high-level%20strategic%20planning%20from%0Alow-level%20procedural%20execution%20in%20human%20cognition.%20We%20uncover%20a%20compelling%0Atwo-phase%20dynamic%3A%20initially%2C%20a%20model%20is%20constrained%20by%20procedural%20correctness%0Aand%20must%20improve%20its%20low-level%20skills.%20The%20learning%20bottleneck%20then%20decisively%0Ashifts%2C%20with%20performance%20gains%20being%20driven%20by%20the%20exploration%20and%20mastery%20of%0Ahigh-level%20strategic%20planning.%20This%20insight%20exposes%20a%20core%20inefficiency%20in%0Aprevailing%20RL%20algorithms%20like%20GRPO%2C%20which%20apply%20optimization%20pressure%0Aagnostically%20and%20dilute%20the%20learning%20signal%20across%20all%20tokens.%20To%20address%20this%2C%0Awe%20propose%20HIerarchy-Aware%20Credit%20Assignment%20%28HICRA%29%2C%20an%20algorithm%20that%0Aconcentrates%20optimization%20efforts%20on%20high-impact%20planning%20tokens.%20HICRA%0Asignificantly%20outperforms%20strong%20baselines%2C%20demonstrating%20that%20focusing%20on%20this%0Astrategic%20bottleneck%20is%20key%20to%20unlocking%20advanced%20reasoning.%20Furthermore%2C%20we%0Avalidate%20semantic%20entropy%20as%20a%20superior%20compass%20for%20measuring%20strategic%0Aexploration%20over%20misleading%20metrics%20such%20as%20token-level%20entropy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Hierarchical%2520Reasoning%2520in%2520LLMs%2520through%2520Reinforcement%2520Learning%26entry.906535625%3DHaozhe%2520Wang%2520and%2520Qixin%2520Xu%2520and%2520Che%2520Liu%2520and%2520Junhong%2520Wu%2520and%2520Fangzhen%2520Lin%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520proven%2520highly%2520effective%2520at%2520enhancing%2520the%250Acomplex%2520reasoning%2520abilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520yet%2520underlying%250Amechanisms%2520driving%2520this%2520success%2520remain%2520largely%2520opaque.%2520Our%2520analysis%2520reveals%250Athat%2520puzzling%2520phenomena%2520like%2520%2560%2560aha%2520moments%2522%252C%2520%2560%2560length-scaling%2527%2527%2520and%2520entropy%250Adynamics%2520are%2520not%2520disparate%2520occurrences%2520but%2520hallmarks%2520of%2520an%2520emergent%2520reasoning%250Ahierarchy%252C%2520akin%2520to%2520the%2520separation%2520of%2520high-level%2520strategic%2520planning%2520from%250Alow-level%2520procedural%2520execution%2520in%2520human%2520cognition.%2520We%2520uncover%2520a%2520compelling%250Atwo-phase%2520dynamic%253A%2520initially%252C%2520a%2520model%2520is%2520constrained%2520by%2520procedural%2520correctness%250Aand%2520must%2520improve%2520its%2520low-level%2520skills.%2520The%2520learning%2520bottleneck%2520then%2520decisively%250Ashifts%252C%2520with%2520performance%2520gains%2520being%2520driven%2520by%2520the%2520exploration%2520and%2520mastery%2520of%250Ahigh-level%2520strategic%2520planning.%2520This%2520insight%2520exposes%2520a%2520core%2520inefficiency%2520in%250Aprevailing%2520RL%2520algorithms%2520like%2520GRPO%252C%2520which%2520apply%2520optimization%2520pressure%250Aagnostically%2520and%2520dilute%2520the%2520learning%2520signal%2520across%2520all%2520tokens.%2520To%2520address%2520this%252C%250Awe%2520propose%2520HIerarchy-Aware%2520Credit%2520Assignment%2520%2528HICRA%2529%252C%2520an%2520algorithm%2520that%250Aconcentrates%2520optimization%2520efforts%2520on%2520high-impact%2520planning%2520tokens.%2520HICRA%250Asignificantly%2520outperforms%2520strong%2520baselines%252C%2520demonstrating%2520that%2520focusing%2520on%2520this%250Astrategic%2520bottleneck%2520is%2520key%2520to%2520unlocking%2520advanced%2520reasoning.%2520Furthermore%252C%2520we%250Avalidate%2520semantic%2520entropy%2520as%2520a%2520superior%2520compass%2520for%2520measuring%2520strategic%250Aexploration%2520over%2520misleading%2520metrics%2520such%2520as%2520token-level%2520entropy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Hierarchical%20Reasoning%20in%20LLMs%20through%20Reinforcement%20Learning&entry.906535625=Haozhe%20Wang%20and%20Qixin%20Xu%20and%20Che%20Liu%20and%20Junhong%20Wu%20and%20Fangzhen%20Lin%20and%20Wenhu%20Chen&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20has%20proven%20highly%20effective%20at%20enhancing%20the%0Acomplex%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20underlying%0Amechanisms%20driving%20this%20success%20remain%20largely%20opaque.%20Our%20analysis%20reveals%0Athat%20puzzling%20phenomena%20like%20%60%60aha%20moments%22%2C%20%60%60length-scaling%27%27%20and%20entropy%0Adynamics%20are%20not%20disparate%20occurrences%20but%20hallmarks%20of%20an%20emergent%20reasoning%0Ahierarchy%2C%20akin%20to%20the%20separation%20of%20high-level%20strategic%20planning%20from%0Alow-level%20procedural%20execution%20in%20human%20cognition.%20We%20uncover%20a%20compelling%0Atwo-phase%20dynamic%3A%20initially%2C%20a%20model%20is%20constrained%20by%20procedural%20correctness%0Aand%20must%20improve%20its%20low-level%20skills.%20The%20learning%20bottleneck%20then%20decisively%0Ashifts%2C%20with%20performance%20gains%20being%20driven%20by%20the%20exploration%20and%20mastery%20of%0Ahigh-level%20strategic%20planning.%20This%20insight%20exposes%20a%20core%20inefficiency%20in%0Aprevailing%20RL%20algorithms%20like%20GRPO%2C%20which%20apply%20optimization%20pressure%0Aagnostically%20and%20dilute%20the%20learning%20signal%20across%20all%20tokens.%20To%20address%20this%2C%0Awe%20propose%20HIerarchy-Aware%20Credit%20Assignment%20%28HICRA%29%2C%20an%20algorithm%20that%0Aconcentrates%20optimization%20efforts%20on%20high-impact%20planning%20tokens.%20HICRA%0Asignificantly%20outperforms%20strong%20baselines%2C%20demonstrating%20that%20focusing%20on%20this%0Astrategic%20bottleneck%20is%20key%20to%20unlocking%20advanced%20reasoning.%20Furthermore%2C%20we%0Avalidate%20semantic%20entropy%20as%20a%20superior%20compass%20for%20measuring%20strategic%0Aexploration%20over%20misleading%20metrics%20such%20as%20token-level%20entropy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03646v2&entry.124074799=Read"},
{"title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection", "author": "Haywood Gelman and John D. Hastings and David Kenley", "abstract": "  Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection.\n", "link": "http://arxiv.org/abs/2509.06920v1", "date": "2025-09-08", "relevancy": 1.827, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Ethically%20Grounded%20LLM-Based%20Approach%20to%20Insider%20Threat%20Synthesis%20and%0A%20%20Detection&body=Title%3A%20An%20Ethically%20Grounded%20LLM-Based%20Approach%20to%20Insider%20Threat%20Synthesis%20and%0A%20%20Detection%0AAuthor%3A%20Haywood%20Gelman%20and%20John%20D.%20Hastings%20and%20David%20Kenley%0AAbstract%3A%20%20%20Insider%20threats%20are%20a%20growing%20organizational%20problem%20due%20to%20the%20complexity%20of%0Aidentifying%20their%20technical%20and%20behavioral%20elements.%20A%20large%20research%20body%20is%0Adedicated%20to%20the%20study%20of%20insider%20threats%20from%20technological%2C%20psychological%2C%0Aand%20educational%20perspectives.%20However%2C%20research%20in%20this%20domain%20has%20been%0Agenerally%20dependent%20on%20datasets%20that%20are%20static%20and%20limited%20access%20which%0Arestricts%20the%20development%20of%20adaptive%20detection%20models.%20This%20study%20introduces%20a%0Anovel%2C%20ethically%20grounded%20approach%20that%20uses%20the%20large%20language%20model%20%28LLM%29%0AClaude%20Sonnet%203.7%20to%20dynamically%20synthesize%20syslog%20messages%2C%20some%20of%20which%0Acontain%20indicators%20of%20insider%20threat%20scenarios.%20The%20messages%20reflect%20real-world%0Adata%20distributions%20by%20being%20highly%20imbalanced%20%281%25%20insider%20threats%29.%20The%20syslogs%0Awere%20analyzed%20for%20insider%20threats%20by%20both%20Claude%20Sonnet%203.7%20and%20GPT-4o%2C%20with%0Atheir%20performance%20evaluated%20through%20statistical%20metrics%20including%20precision%2C%0Arecall%2C%20MCC%2C%20and%20ROC%20AUC.%20Sonnet%203.7%20consistently%20outperformed%20GPT-4o%20across%0Anearly%20all%20metrics%2C%20particularly%20in%20reducing%20false%20alarms%20and%20improving%0Adetection%20accuracy.%20The%20results%20show%20strong%20promise%20for%20the%20use%20of%20LLMs%20in%0Asynthetic%20dataset%20generation%20and%20insider%20threat%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Ethically%2520Grounded%2520LLM-Based%2520Approach%2520to%2520Insider%2520Threat%2520Synthesis%2520and%250A%2520%2520Detection%26entry.906535625%3DHaywood%2520Gelman%2520and%2520John%2520D.%2520Hastings%2520and%2520David%2520Kenley%26entry.1292438233%3D%2520%2520Insider%2520threats%2520are%2520a%2520growing%2520organizational%2520problem%2520due%2520to%2520the%2520complexity%2520of%250Aidentifying%2520their%2520technical%2520and%2520behavioral%2520elements.%2520A%2520large%2520research%2520body%2520is%250Adedicated%2520to%2520the%2520study%2520of%2520insider%2520threats%2520from%2520technological%252C%2520psychological%252C%250Aand%2520educational%2520perspectives.%2520However%252C%2520research%2520in%2520this%2520domain%2520has%2520been%250Agenerally%2520dependent%2520on%2520datasets%2520that%2520are%2520static%2520and%2520limited%2520access%2520which%250Arestricts%2520the%2520development%2520of%2520adaptive%2520detection%2520models.%2520This%2520study%2520introduces%2520a%250Anovel%252C%2520ethically%2520grounded%2520approach%2520that%2520uses%2520the%2520large%2520language%2520model%2520%2528LLM%2529%250AClaude%2520Sonnet%25203.7%2520to%2520dynamically%2520synthesize%2520syslog%2520messages%252C%2520some%2520of%2520which%250Acontain%2520indicators%2520of%2520insider%2520threat%2520scenarios.%2520The%2520messages%2520reflect%2520real-world%250Adata%2520distributions%2520by%2520being%2520highly%2520imbalanced%2520%25281%2525%2520insider%2520threats%2529.%2520The%2520syslogs%250Awere%2520analyzed%2520for%2520insider%2520threats%2520by%2520both%2520Claude%2520Sonnet%25203.7%2520and%2520GPT-4o%252C%2520with%250Atheir%2520performance%2520evaluated%2520through%2520statistical%2520metrics%2520including%2520precision%252C%250Arecall%252C%2520MCC%252C%2520and%2520ROC%2520AUC.%2520Sonnet%25203.7%2520consistently%2520outperformed%2520GPT-4o%2520across%250Anearly%2520all%2520metrics%252C%2520particularly%2520in%2520reducing%2520false%2520alarms%2520and%2520improving%250Adetection%2520accuracy.%2520The%2520results%2520show%2520strong%2520promise%2520for%2520the%2520use%2520of%2520LLMs%2520in%250Asynthetic%2520dataset%2520generation%2520and%2520insider%2520threat%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Ethically%20Grounded%20LLM-Based%20Approach%20to%20Insider%20Threat%20Synthesis%20and%0A%20%20Detection&entry.906535625=Haywood%20Gelman%20and%20John%20D.%20Hastings%20and%20David%20Kenley&entry.1292438233=%20%20Insider%20threats%20are%20a%20growing%20organizational%20problem%20due%20to%20the%20complexity%20of%0Aidentifying%20their%20technical%20and%20behavioral%20elements.%20A%20large%20research%20body%20is%0Adedicated%20to%20the%20study%20of%20insider%20threats%20from%20technological%2C%20psychological%2C%0Aand%20educational%20perspectives.%20However%2C%20research%20in%20this%20domain%20has%20been%0Agenerally%20dependent%20on%20datasets%20that%20are%20static%20and%20limited%20access%20which%0Arestricts%20the%20development%20of%20adaptive%20detection%20models.%20This%20study%20introduces%20a%0Anovel%2C%20ethically%20grounded%20approach%20that%20uses%20the%20large%20language%20model%20%28LLM%29%0AClaude%20Sonnet%203.7%20to%20dynamically%20synthesize%20syslog%20messages%2C%20some%20of%20which%0Acontain%20indicators%20of%20insider%20threat%20scenarios.%20The%20messages%20reflect%20real-world%0Adata%20distributions%20by%20being%20highly%20imbalanced%20%281%25%20insider%20threats%29.%20The%20syslogs%0Awere%20analyzed%20for%20insider%20threats%20by%20both%20Claude%20Sonnet%203.7%20and%20GPT-4o%2C%20with%0Atheir%20performance%20evaluated%20through%20statistical%20metrics%20including%20precision%2C%0Arecall%2C%20MCC%2C%20and%20ROC%20AUC.%20Sonnet%203.7%20consistently%20outperformed%20GPT-4o%20across%0Anearly%20all%20metrics%2C%20particularly%20in%20reducing%20false%20alarms%20and%20improving%0Adetection%20accuracy.%20The%20results%20show%20strong%20promise%20for%20the%20use%20of%20LLMs%20in%0Asynthetic%20dataset%20generation%20and%20insider%20threat%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06920v1&entry.124074799=Read"},
{"title": "Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and\n  Opportunities", "author": "Safayat Bin Hakim and Muhammad Adil and Alvaro Velasquez and Shouhuai Xu and Houbing Herbert Song", "abstract": "  Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit\nfundamental limitations: inadequate conceptual grounding leading to\nnon-robustness against novel attacks; limited instructibility impeding\nanalyst-guided adaptation; and misalignment with cybersecurity objectives.\nNeuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize\ncybersecurity AI. However, there is no systematic understanding of this\nemerging approach. These hybrid systems address critical cybersecurity\nchallenges by combining neural pattern recognition with symbolic reasoning,\nenabling enhanced threat understanding while introducing concerning autonomous\noffensive capabilities that reshape threat landscapes. In this survey, we\nsystematically characterize this field by analyzing 127 publications spanning\n2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)\nframework to evaluate these systems, focusing on both cyber defense and cyber\noffense across network security, malware analysis, and cyber operations. Our\nanalysis shows advantages of multi-agent NeSy architectures and identifies\ncritical implementation challenges including standardization gaps,\ncomputational complexity, and human-AI collaboration requirements that\nconstrain deployment. We show that causal reasoning integration is the most\ntransformative advancement, enabling proactive defense beyond correlation-based\napproaches. Our findings highlight dual-use implications where autonomous\nsystems demonstrate substantial capabilities in zero-day exploitation while\nachieving significant cost reductions, altering threat dynamics. We provide\ninsights and future research directions, emphasizing the urgent need for\ncommunity-driven standardization frameworks and responsible development\npractices that ensure advancement serves defensive cybersecurity objectives\nwhile maintaining societal alignment.\n", "link": "http://arxiv.org/abs/2509.06921v1", "date": "2025-09-08", "relevancy": 1.3332, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4579}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4479}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuro-Symbolic%20AI%20for%20Cybersecurity%3A%20State%20of%20the%20Art%2C%20Challenges%2C%20and%0A%20%20Opportunities&body=Title%3A%20Neuro-Symbolic%20AI%20for%20Cybersecurity%3A%20State%20of%20the%20Art%2C%20Challenges%2C%20and%0A%20%20Opportunities%0AAuthor%3A%20Safayat%20Bin%20Hakim%20and%20Muhammad%20Adil%20and%20Alvaro%20Velasquez%20and%20Shouhuai%20Xu%20and%20Houbing%20Herbert%20Song%0AAbstract%3A%20%20%20Traditional%20Artificial%20Intelligence%20%28AI%29%20approaches%20in%20cybersecurity%20exhibit%0Afundamental%20limitations%3A%20inadequate%20conceptual%20grounding%20leading%20to%0Anon-robustness%20against%20novel%20attacks%3B%20limited%20instructibility%20impeding%0Aanalyst-guided%20adaptation%3B%20and%20misalignment%20with%20cybersecurity%20objectives.%0ANeuro-Symbolic%20%28NeSy%29%20AI%20has%20emerged%20with%20the%20potential%20to%20revolutionize%0Acybersecurity%20AI.%20However%2C%20there%20is%20no%20systematic%20understanding%20of%20this%0Aemerging%20approach.%20These%20hybrid%20systems%20address%20critical%20cybersecurity%0Achallenges%20by%20combining%20neural%20pattern%20recognition%20with%20symbolic%20reasoning%2C%0Aenabling%20enhanced%20threat%20understanding%20while%20introducing%20concerning%20autonomous%0Aoffensive%20capabilities%20that%20reshape%20threat%20landscapes.%20In%20this%20survey%2C%20we%0Asystematically%20characterize%20this%20field%20by%20analyzing%20127%20publications%20spanning%0A2019-July%202025.%20We%20introduce%20a%20Grounding-Instructibility-Alignment%20%28G-I-A%29%0Aframework%20to%20evaluate%20these%20systems%2C%20focusing%20on%20both%20cyber%20defense%20and%20cyber%0Aoffense%20across%20network%20security%2C%20malware%20analysis%2C%20and%20cyber%20operations.%20Our%0Aanalysis%20shows%20advantages%20of%20multi-agent%20NeSy%20architectures%20and%20identifies%0Acritical%20implementation%20challenges%20including%20standardization%20gaps%2C%0Acomputational%20complexity%2C%20and%20human-AI%20collaboration%20requirements%20that%0Aconstrain%20deployment.%20We%20show%20that%20causal%20reasoning%20integration%20is%20the%20most%0Atransformative%20advancement%2C%20enabling%20proactive%20defense%20beyond%20correlation-based%0Aapproaches.%20Our%20findings%20highlight%20dual-use%20implications%20where%20autonomous%0Asystems%20demonstrate%20substantial%20capabilities%20in%20zero-day%20exploitation%20while%0Aachieving%20significant%20cost%20reductions%2C%20altering%20threat%20dynamics.%20We%20provide%0Ainsights%20and%20future%20research%20directions%2C%20emphasizing%20the%20urgent%20need%20for%0Acommunity-driven%20standardization%20frameworks%20and%20responsible%20development%0Apractices%20that%20ensure%20advancement%20serves%20defensive%20cybersecurity%20objectives%0Awhile%20maintaining%20societal%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuro-Symbolic%2520AI%2520for%2520Cybersecurity%253A%2520State%2520of%2520the%2520Art%252C%2520Challenges%252C%2520and%250A%2520%2520Opportunities%26entry.906535625%3DSafayat%2520Bin%2520Hakim%2520and%2520Muhammad%2520Adil%2520and%2520Alvaro%2520Velasquez%2520and%2520Shouhuai%2520Xu%2520and%2520Houbing%2520Herbert%2520Song%26entry.1292438233%3D%2520%2520Traditional%2520Artificial%2520Intelligence%2520%2528AI%2529%2520approaches%2520in%2520cybersecurity%2520exhibit%250Afundamental%2520limitations%253A%2520inadequate%2520conceptual%2520grounding%2520leading%2520to%250Anon-robustness%2520against%2520novel%2520attacks%253B%2520limited%2520instructibility%2520impeding%250Aanalyst-guided%2520adaptation%253B%2520and%2520misalignment%2520with%2520cybersecurity%2520objectives.%250ANeuro-Symbolic%2520%2528NeSy%2529%2520AI%2520has%2520emerged%2520with%2520the%2520potential%2520to%2520revolutionize%250Acybersecurity%2520AI.%2520However%252C%2520there%2520is%2520no%2520systematic%2520understanding%2520of%2520this%250Aemerging%2520approach.%2520These%2520hybrid%2520systems%2520address%2520critical%2520cybersecurity%250Achallenges%2520by%2520combining%2520neural%2520pattern%2520recognition%2520with%2520symbolic%2520reasoning%252C%250Aenabling%2520enhanced%2520threat%2520understanding%2520while%2520introducing%2520concerning%2520autonomous%250Aoffensive%2520capabilities%2520that%2520reshape%2520threat%2520landscapes.%2520In%2520this%2520survey%252C%2520we%250Asystematically%2520characterize%2520this%2520field%2520by%2520analyzing%2520127%2520publications%2520spanning%250A2019-July%25202025.%2520We%2520introduce%2520a%2520Grounding-Instructibility-Alignment%2520%2528G-I-A%2529%250Aframework%2520to%2520evaluate%2520these%2520systems%252C%2520focusing%2520on%2520both%2520cyber%2520defense%2520and%2520cyber%250Aoffense%2520across%2520network%2520security%252C%2520malware%2520analysis%252C%2520and%2520cyber%2520operations.%2520Our%250Aanalysis%2520shows%2520advantages%2520of%2520multi-agent%2520NeSy%2520architectures%2520and%2520identifies%250Acritical%2520implementation%2520challenges%2520including%2520standardization%2520gaps%252C%250Acomputational%2520complexity%252C%2520and%2520human-AI%2520collaboration%2520requirements%2520that%250Aconstrain%2520deployment.%2520We%2520show%2520that%2520causal%2520reasoning%2520integration%2520is%2520the%2520most%250Atransformative%2520advancement%252C%2520enabling%2520proactive%2520defense%2520beyond%2520correlation-based%250Aapproaches.%2520Our%2520findings%2520highlight%2520dual-use%2520implications%2520where%2520autonomous%250Asystems%2520demonstrate%2520substantial%2520capabilities%2520in%2520zero-day%2520exploitation%2520while%250Aachieving%2520significant%2520cost%2520reductions%252C%2520altering%2520threat%2520dynamics.%2520We%2520provide%250Ainsights%2520and%2520future%2520research%2520directions%252C%2520emphasizing%2520the%2520urgent%2520need%2520for%250Acommunity-driven%2520standardization%2520frameworks%2520and%2520responsible%2520development%250Apractices%2520that%2520ensure%2520advancement%2520serves%2520defensive%2520cybersecurity%2520objectives%250Awhile%2520maintaining%2520societal%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuro-Symbolic%20AI%20for%20Cybersecurity%3A%20State%20of%20the%20Art%2C%20Challenges%2C%20and%0A%20%20Opportunities&entry.906535625=Safayat%20Bin%20Hakim%20and%20Muhammad%20Adil%20and%20Alvaro%20Velasquez%20and%20Shouhuai%20Xu%20and%20Houbing%20Herbert%20Song&entry.1292438233=%20%20Traditional%20Artificial%20Intelligence%20%28AI%29%20approaches%20in%20cybersecurity%20exhibit%0Afundamental%20limitations%3A%20inadequate%20conceptual%20grounding%20leading%20to%0Anon-robustness%20against%20novel%20attacks%3B%20limited%20instructibility%20impeding%0Aanalyst-guided%20adaptation%3B%20and%20misalignment%20with%20cybersecurity%20objectives.%0ANeuro-Symbolic%20%28NeSy%29%20AI%20has%20emerged%20with%20the%20potential%20to%20revolutionize%0Acybersecurity%20AI.%20However%2C%20there%20is%20no%20systematic%20understanding%20of%20this%0Aemerging%20approach.%20These%20hybrid%20systems%20address%20critical%20cybersecurity%0Achallenges%20by%20combining%20neural%20pattern%20recognition%20with%20symbolic%20reasoning%2C%0Aenabling%20enhanced%20threat%20understanding%20while%20introducing%20concerning%20autonomous%0Aoffensive%20capabilities%20that%20reshape%20threat%20landscapes.%20In%20this%20survey%2C%20we%0Asystematically%20characterize%20this%20field%20by%20analyzing%20127%20publications%20spanning%0A2019-July%202025.%20We%20introduce%20a%20Grounding-Instructibility-Alignment%20%28G-I-A%29%0Aframework%20to%20evaluate%20these%20systems%2C%20focusing%20on%20both%20cyber%20defense%20and%20cyber%0Aoffense%20across%20network%20security%2C%20malware%20analysis%2C%20and%20cyber%20operations.%20Our%0Aanalysis%20shows%20advantages%20of%20multi-agent%20NeSy%20architectures%20and%20identifies%0Acritical%20implementation%20challenges%20including%20standardization%20gaps%2C%0Acomputational%20complexity%2C%20and%20human-AI%20collaboration%20requirements%20that%0Aconstrain%20deployment.%20We%20show%20that%20causal%20reasoning%20integration%20is%20the%20most%0Atransformative%20advancement%2C%20enabling%20proactive%20defense%20beyond%20correlation-based%0Aapproaches.%20Our%20findings%20highlight%20dual-use%20implications%20where%20autonomous%0Asystems%20demonstrate%20substantial%20capabilities%20in%20zero-day%20exploitation%20while%0Aachieving%20significant%20cost%20reductions%2C%20altering%20threat%20dynamics.%20We%20provide%0Ainsights%20and%20future%20research%20directions%2C%20emphasizing%20the%20urgent%20need%20for%0Acommunity-driven%20standardization%20frameworks%20and%20responsible%20development%0Apractices%20that%20ensure%20advancement%20serves%20defensive%20cybersecurity%20objectives%0Awhile%20maintaining%20societal%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06921v1&entry.124074799=Read"},
{"title": "Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks", "author": "Han Ji and Xiping Wu and Zhihong Zeng and Chen Chen", "abstract": "  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a\npromising paradigm of heterogeneous network (HetNet), attributed to the\ncomplementary physical properties of optical spectra and radio frequency.\nHowever, the current development of such HetNets is mostly bottlenecked by the\nexisting transmission control protocol (TCP), which restricts the user\nequipment (UE) to connecting one access point (AP) at a time. While the ongoing\ninvestigation on multipath TCP (MPTCP) can bring significant benefits, it\ncomplicates the network topology of HetNets, making the existing load balancing\n(LB) learning models less effective. Driven by this, we propose a graph neural\nnetwork (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets,\nwhich results in a partial mesh topology. Such a topology can be modeled as a\ngraph, with the channel state information and data rate requirement embedded as\nnode features, while the LB solutions are deemed as edge labels. Compared to\nthe conventional deep neural network (DNN), the proposed GNN-based model\nexhibits two key strengths: i) it can better interpret a complex network\ntopology; and ii) it can handle various numbers of APs and UEs with a single\ntrained model. Simulation results show that against the traditional\noptimisation method, the proposed learning model can achieve near-optimal\nthroughput within a gap of 11.5%, while reducing the inference time by 4 orders\nof magnitude. In contrast to the DNN model, the new method can improve the\nnetwork throughput by up to 21.7%, at a similar inference time level.\n", "link": "http://arxiv.org/abs/2410.17118v2", "date": "2025-09-08", "relevancy": 1.9075, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4992}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4647}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Load%20Balancing%20with%20GNN%20in%20MPTCP-Enabled%20Heterogeneous%20Networks&body=Title%3A%20Learning%20Load%20Balancing%20with%20GNN%20in%20MPTCP-Enabled%20Heterogeneous%20Networks%0AAuthor%3A%20Han%20Ji%20and%20Xiping%20Wu%20and%20Zhihong%20Zeng%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Hybrid%20light%20fidelity%20%28LiFi%29%20and%20wireless%20fidelity%20%28WiFi%29%20networks%20are%20a%0Apromising%20paradigm%20of%20heterogeneous%20network%20%28HetNet%29%2C%20attributed%20to%20the%0Acomplementary%20physical%20properties%20of%20optical%20spectra%20and%20radio%20frequency.%0AHowever%2C%20the%20current%20development%20of%20such%20HetNets%20is%20mostly%20bottlenecked%20by%20the%0Aexisting%20transmission%20control%20protocol%20%28TCP%29%2C%20which%20restricts%20the%20user%0Aequipment%20%28UE%29%20to%20connecting%20one%20access%20point%20%28AP%29%20at%20a%20time.%20While%20the%20ongoing%0Ainvestigation%20on%20multipath%20TCP%20%28MPTCP%29%20can%20bring%20significant%20benefits%2C%20it%0Acomplicates%20the%20network%20topology%20of%20HetNets%2C%20making%20the%20existing%20load%20balancing%0A%28LB%29%20learning%20models%20less%20effective.%20Driven%20by%20this%2C%20we%20propose%20a%20graph%20neural%0Anetwork%20%28GNN%29-based%20model%20to%20tackle%20the%20LB%20problem%20for%20MPTCP-enabled%20HetNets%2C%0Awhich%20results%20in%20a%20partial%20mesh%20topology.%20Such%20a%20topology%20can%20be%20modeled%20as%20a%0Agraph%2C%20with%20the%20channel%20state%20information%20and%20data%20rate%20requirement%20embedded%20as%0Anode%20features%2C%20while%20the%20LB%20solutions%20are%20deemed%20as%20edge%20labels.%20Compared%20to%0Athe%20conventional%20deep%20neural%20network%20%28DNN%29%2C%20the%20proposed%20GNN-based%20model%0Aexhibits%20two%20key%20strengths%3A%20i%29%20it%20can%20better%20interpret%20a%20complex%20network%0Atopology%3B%20and%20ii%29%20it%20can%20handle%20various%20numbers%20of%20APs%20and%20UEs%20with%20a%20single%0Atrained%20model.%20Simulation%20results%20show%20that%20against%20the%20traditional%0Aoptimisation%20method%2C%20the%20proposed%20learning%20model%20can%20achieve%20near-optimal%0Athroughput%20within%20a%20gap%20of%2011.5%25%2C%20while%20reducing%20the%20inference%20time%20by%204%20orders%0Aof%20magnitude.%20In%20contrast%20to%20the%20DNN%20model%2C%20the%20new%20method%20can%20improve%20the%0Anetwork%20throughput%20by%20up%20to%2021.7%25%2C%20at%20a%20similar%20inference%20time%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Load%2520Balancing%2520with%2520GNN%2520in%2520MPTCP-Enabled%2520Heterogeneous%2520Networks%26entry.906535625%3DHan%2520Ji%2520and%2520Xiping%2520Wu%2520and%2520Zhihong%2520Zeng%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Hybrid%2520light%2520fidelity%2520%2528LiFi%2529%2520and%2520wireless%2520fidelity%2520%2528WiFi%2529%2520networks%2520are%2520a%250Apromising%2520paradigm%2520of%2520heterogeneous%2520network%2520%2528HetNet%2529%252C%2520attributed%2520to%2520the%250Acomplementary%2520physical%2520properties%2520of%2520optical%2520spectra%2520and%2520radio%2520frequency.%250AHowever%252C%2520the%2520current%2520development%2520of%2520such%2520HetNets%2520is%2520mostly%2520bottlenecked%2520by%2520the%250Aexisting%2520transmission%2520control%2520protocol%2520%2528TCP%2529%252C%2520which%2520restricts%2520the%2520user%250Aequipment%2520%2528UE%2529%2520to%2520connecting%2520one%2520access%2520point%2520%2528AP%2529%2520at%2520a%2520time.%2520While%2520the%2520ongoing%250Ainvestigation%2520on%2520multipath%2520TCP%2520%2528MPTCP%2529%2520can%2520bring%2520significant%2520benefits%252C%2520it%250Acomplicates%2520the%2520network%2520topology%2520of%2520HetNets%252C%2520making%2520the%2520existing%2520load%2520balancing%250A%2528LB%2529%2520learning%2520models%2520less%2520effective.%2520Driven%2520by%2520this%252C%2520we%2520propose%2520a%2520graph%2520neural%250Anetwork%2520%2528GNN%2529-based%2520model%2520to%2520tackle%2520the%2520LB%2520problem%2520for%2520MPTCP-enabled%2520HetNets%252C%250Awhich%2520results%2520in%2520a%2520partial%2520mesh%2520topology.%2520Such%2520a%2520topology%2520can%2520be%2520modeled%2520as%2520a%250Agraph%252C%2520with%2520the%2520channel%2520state%2520information%2520and%2520data%2520rate%2520requirement%2520embedded%2520as%250Anode%2520features%252C%2520while%2520the%2520LB%2520solutions%2520are%2520deemed%2520as%2520edge%2520labels.%2520Compared%2520to%250Athe%2520conventional%2520deep%2520neural%2520network%2520%2528DNN%2529%252C%2520the%2520proposed%2520GNN-based%2520model%250Aexhibits%2520two%2520key%2520strengths%253A%2520i%2529%2520it%2520can%2520better%2520interpret%2520a%2520complex%2520network%250Atopology%253B%2520and%2520ii%2529%2520it%2520can%2520handle%2520various%2520numbers%2520of%2520APs%2520and%2520UEs%2520with%2520a%2520single%250Atrained%2520model.%2520Simulation%2520results%2520show%2520that%2520against%2520the%2520traditional%250Aoptimisation%2520method%252C%2520the%2520proposed%2520learning%2520model%2520can%2520achieve%2520near-optimal%250Athroughput%2520within%2520a%2520gap%2520of%252011.5%2525%252C%2520while%2520reducing%2520the%2520inference%2520time%2520by%25204%2520orders%250Aof%2520magnitude.%2520In%2520contrast%2520to%2520the%2520DNN%2520model%252C%2520the%2520new%2520method%2520can%2520improve%2520the%250Anetwork%2520throughput%2520by%2520up%2520to%252021.7%2525%252C%2520at%2520a%2520similar%2520inference%2520time%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Load%20Balancing%20with%20GNN%20in%20MPTCP-Enabled%20Heterogeneous%20Networks&entry.906535625=Han%20Ji%20and%20Xiping%20Wu%20and%20Zhihong%20Zeng%20and%20Chen%20Chen&entry.1292438233=%20%20Hybrid%20light%20fidelity%20%28LiFi%29%20and%20wireless%20fidelity%20%28WiFi%29%20networks%20are%20a%0Apromising%20paradigm%20of%20heterogeneous%20network%20%28HetNet%29%2C%20attributed%20to%20the%0Acomplementary%20physical%20properties%20of%20optical%20spectra%20and%20radio%20frequency.%0AHowever%2C%20the%20current%20development%20of%20such%20HetNets%20is%20mostly%20bottlenecked%20by%20the%0Aexisting%20transmission%20control%20protocol%20%28TCP%29%2C%20which%20restricts%20the%20user%0Aequipment%20%28UE%29%20to%20connecting%20one%20access%20point%20%28AP%29%20at%20a%20time.%20While%20the%20ongoing%0Ainvestigation%20on%20multipath%20TCP%20%28MPTCP%29%20can%20bring%20significant%20benefits%2C%20it%0Acomplicates%20the%20network%20topology%20of%20HetNets%2C%20making%20the%20existing%20load%20balancing%0A%28LB%29%20learning%20models%20less%20effective.%20Driven%20by%20this%2C%20we%20propose%20a%20graph%20neural%0Anetwork%20%28GNN%29-based%20model%20to%20tackle%20the%20LB%20problem%20for%20MPTCP-enabled%20HetNets%2C%0Awhich%20results%20in%20a%20partial%20mesh%20topology.%20Such%20a%20topology%20can%20be%20modeled%20as%20a%0Agraph%2C%20with%20the%20channel%20state%20information%20and%20data%20rate%20requirement%20embedded%20as%0Anode%20features%2C%20while%20the%20LB%20solutions%20are%20deemed%20as%20edge%20labels.%20Compared%20to%0Athe%20conventional%20deep%20neural%20network%20%28DNN%29%2C%20the%20proposed%20GNN-based%20model%0Aexhibits%20two%20key%20strengths%3A%20i%29%20it%20can%20better%20interpret%20a%20complex%20network%0Atopology%3B%20and%20ii%29%20it%20can%20handle%20various%20numbers%20of%20APs%20and%20UEs%20with%20a%20single%0Atrained%20model.%20Simulation%20results%20show%20that%20against%20the%20traditional%0Aoptimisation%20method%2C%20the%20proposed%20learning%20model%20can%20achieve%20near-optimal%0Athroughput%20within%20a%20gap%20of%2011.5%25%2C%20while%20reducing%20the%20inference%20time%20by%204%20orders%0Aof%20magnitude.%20In%20contrast%20to%20the%20DNN%20model%2C%20the%20new%20method%20can%20improve%20the%0Anetwork%20throughput%20by%20up%20to%2021.7%25%2C%20at%20a%20similar%20inference%20time%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17118v2&entry.124074799=Read"},
{"title": "In-Context Reverse Classification Accuracy: Efficient Estimation of\n  Segmentation Quality without Ground-Truth", "author": "Matias Cosarinsky and Ramiro Billot and Lucas Mansilla and Gabriel Jimenez and Nicolas Gaggi\u00f3n and Guanghui Fu and Enzo Ferrante", "abstract": "  Assessing the quality of automatic image segmentation is crucial in clinical\npractice, but often very challenging due to the limited availability of ground\ntruth annotations. In this paper, we introduce In-Context Reverse\nClassification Accuracy (In-Context RCA), a novel framework for automatically\nestimating segmentation quality in the absence of ground-truth annotations. By\nleveraging recent in-context learning segmentation models and incorporating\nretrieval-augmentation techniques to select the most relevant reference images,\nour approach enables efficient quality estimation with minimal reference data.\nValidated across diverse medical imaging modalities, our method demonstrates\nrobust performance and computational efficiency, offering a promising solution\nfor automated quality control in clinical workflows, where fast and reliable\nsegmentation assessment is essential. The code is available at\nhttps://github.com/mcosarinsky/In-Context-RCA.\n", "link": "http://arxiv.org/abs/2503.04522v2", "date": "2025-09-08", "relevancy": 1.944, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4905}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Reverse%20Classification%20Accuracy%3A%20Efficient%20Estimation%20of%0A%20%20Segmentation%20Quality%20without%20Ground-Truth&body=Title%3A%20In-Context%20Reverse%20Classification%20Accuracy%3A%20Efficient%20Estimation%20of%0A%20%20Segmentation%20Quality%20without%20Ground-Truth%0AAuthor%3A%20Matias%20Cosarinsky%20and%20Ramiro%20Billot%20and%20Lucas%20Mansilla%20and%20Gabriel%20Jimenez%20and%20Nicolas%20Gaggi%C3%B3n%20and%20Guanghui%20Fu%20and%20Enzo%20Ferrante%0AAbstract%3A%20%20%20Assessing%20the%20quality%20of%20automatic%20image%20segmentation%20is%20crucial%20in%20clinical%0Apractice%2C%20but%20often%20very%20challenging%20due%20to%20the%20limited%20availability%20of%20ground%0Atruth%20annotations.%20In%20this%20paper%2C%20we%20introduce%20In-Context%20Reverse%0AClassification%20Accuracy%20%28In-Context%20RCA%29%2C%20a%20novel%20framework%20for%20automatically%0Aestimating%20segmentation%20quality%20in%20the%20absence%20of%20ground-truth%20annotations.%20By%0Aleveraging%20recent%20in-context%20learning%20segmentation%20models%20and%20incorporating%0Aretrieval-augmentation%20techniques%20to%20select%20the%20most%20relevant%20reference%20images%2C%0Aour%20approach%20enables%20efficient%20quality%20estimation%20with%20minimal%20reference%20data.%0AValidated%20across%20diverse%20medical%20imaging%20modalities%2C%20our%20method%20demonstrates%0Arobust%20performance%20and%20computational%20efficiency%2C%20offering%20a%20promising%20solution%0Afor%20automated%20quality%20control%20in%20clinical%20workflows%2C%20where%20fast%20and%20reliable%0Asegmentation%20assessment%20is%20essential.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/mcosarinsky/In-Context-RCA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Reverse%2520Classification%2520Accuracy%253A%2520Efficient%2520Estimation%2520of%250A%2520%2520Segmentation%2520Quality%2520without%2520Ground-Truth%26entry.906535625%3DMatias%2520Cosarinsky%2520and%2520Ramiro%2520Billot%2520and%2520Lucas%2520Mansilla%2520and%2520Gabriel%2520Jimenez%2520and%2520Nicolas%2520Gaggi%25C3%25B3n%2520and%2520Guanghui%2520Fu%2520and%2520Enzo%2520Ferrante%26entry.1292438233%3D%2520%2520Assessing%2520the%2520quality%2520of%2520automatic%2520image%2520segmentation%2520is%2520crucial%2520in%2520clinical%250Apractice%252C%2520but%2520often%2520very%2520challenging%2520due%2520to%2520the%2520limited%2520availability%2520of%2520ground%250Atruth%2520annotations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520In-Context%2520Reverse%250AClassification%2520Accuracy%2520%2528In-Context%2520RCA%2529%252C%2520a%2520novel%2520framework%2520for%2520automatically%250Aestimating%2520segmentation%2520quality%2520in%2520the%2520absence%2520of%2520ground-truth%2520annotations.%2520By%250Aleveraging%2520recent%2520in-context%2520learning%2520segmentation%2520models%2520and%2520incorporating%250Aretrieval-augmentation%2520techniques%2520to%2520select%2520the%2520most%2520relevant%2520reference%2520images%252C%250Aour%2520approach%2520enables%2520efficient%2520quality%2520estimation%2520with%2520minimal%2520reference%2520data.%250AValidated%2520across%2520diverse%2520medical%2520imaging%2520modalities%252C%2520our%2520method%2520demonstrates%250Arobust%2520performance%2520and%2520computational%2520efficiency%252C%2520offering%2520a%2520promising%2520solution%250Afor%2520automated%2520quality%2520control%2520in%2520clinical%2520workflows%252C%2520where%2520fast%2520and%2520reliable%250Asegmentation%2520assessment%2520is%2520essential.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mcosarinsky/In-Context-RCA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Reverse%20Classification%20Accuracy%3A%20Efficient%20Estimation%20of%0A%20%20Segmentation%20Quality%20without%20Ground-Truth&entry.906535625=Matias%20Cosarinsky%20and%20Ramiro%20Billot%20and%20Lucas%20Mansilla%20and%20Gabriel%20Jimenez%20and%20Nicolas%20Gaggi%C3%B3n%20and%20Guanghui%20Fu%20and%20Enzo%20Ferrante&entry.1292438233=%20%20Assessing%20the%20quality%20of%20automatic%20image%20segmentation%20is%20crucial%20in%20clinical%0Apractice%2C%20but%20often%20very%20challenging%20due%20to%20the%20limited%20availability%20of%20ground%0Atruth%20annotations.%20In%20this%20paper%2C%20we%20introduce%20In-Context%20Reverse%0AClassification%20Accuracy%20%28In-Context%20RCA%29%2C%20a%20novel%20framework%20for%20automatically%0Aestimating%20segmentation%20quality%20in%20the%20absence%20of%20ground-truth%20annotations.%20By%0Aleveraging%20recent%20in-context%20learning%20segmentation%20models%20and%20incorporating%0Aretrieval-augmentation%20techniques%20to%20select%20the%20most%20relevant%20reference%20images%2C%0Aour%20approach%20enables%20efficient%20quality%20estimation%20with%20minimal%20reference%20data.%0AValidated%20across%20diverse%20medical%20imaging%20modalities%2C%20our%20method%20demonstrates%0Arobust%20performance%20and%20computational%20efficiency%2C%20offering%20a%20promising%20solution%0Afor%20automated%20quality%20control%20in%20clinical%20workflows%2C%20where%20fast%20and%20reliable%0Asegmentation%20assessment%20is%20essential.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/mcosarinsky/In-Context-RCA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04522v2&entry.124074799=Read"},
{"title": "On optimal solutions of classical and sliced Wasserstein GANs with\n  non-Gaussian data", "author": "Yu-Jui Huang and Hsin-Hua Shen and Yu-Chih Huang and Wan-Yi Lin and Shih-Chun Lin", "abstract": "  The generative adversarial network (GAN) aims to approximate an unknown\ndistribution via a parameterized neural network (NN). While GANs have been\nwidely applied in reinforcement and semisupervised learning as well as computer\nvision tasks, selecting their parameters often needs an exhaustive search and\nonly a few selection methods can be proved to be theoretically optimal. One of\nthe most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on\noptimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)\nsetting, where the NN is linear and the data is Gaussian. In this paper, we\nfocus on the characterization of optimal WGAN parameters beyond the LQG\nsetting. We derive closed-form optimal parameters for one-dimensional WGANs\nwhen the NN has non-linear activation functions and the data is non-Gaussian.\nTo extend this to high-dimensional WGANs, we adopt the sliced Wasserstein\nframework and replace the constraint on marginal distributions of the randomly\nprojected data by a constraint on the joint distribution of the original\n(unprojected) data. We show that the linear generator can be asymptotically\noptimal for sliced WGAN with non-Gaussian data. Empirical studies show that our\nclosed-form WGAN parameters have good convergence behavior with data under both\nGaussian and Laplace distributions. Also, compared to the r principal component\nanalysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve\nthe same performance while requiring less computational resources.\n", "link": "http://arxiv.org/abs/2509.06505v1", "date": "2025-09-08", "relevancy": 1.962, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4927}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4897}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20optimal%20solutions%20of%20classical%20and%20sliced%20Wasserstein%20GANs%20with%0A%20%20non-Gaussian%20data&body=Title%3A%20On%20optimal%20solutions%20of%20classical%20and%20sliced%20Wasserstein%20GANs%20with%0A%20%20non-Gaussian%20data%0AAuthor%3A%20Yu-Jui%20Huang%20and%20Hsin-Hua%20Shen%20and%20Yu-Chih%20Huang%20and%20Wan-Yi%20Lin%20and%20Shih-Chun%20Lin%0AAbstract%3A%20%20%20The%20generative%20adversarial%20network%20%28GAN%29%20aims%20to%20approximate%20an%20unknown%0Adistribution%20via%20a%20parameterized%20neural%20network%20%28NN%29.%20While%20GANs%20have%20been%0Awidely%20applied%20in%20reinforcement%20and%20semisupervised%20learning%20as%20well%20as%20computer%0Avision%20tasks%2C%20selecting%20their%20parameters%20often%20needs%20an%20exhaustive%20search%20and%0Aonly%20a%20few%20selection%20methods%20can%20be%20proved%20to%20be%20theoretically%20optimal.%20One%20of%0Athe%20most%20promising%20GAN%20variants%20is%20the%20Wasserstein%20GAN%20%28WGAN%29.%20Prior%20work%20on%0Aoptimal%20parameters%20for%20WGAN%20is%20limited%20to%20the%20linear-quadratic-Gaussian%20%28LQG%29%0Asetting%2C%20where%20the%20NN%20is%20linear%20and%20the%20data%20is%20Gaussian.%20In%20this%20paper%2C%20we%0Afocus%20on%20the%20characterization%20of%20optimal%20WGAN%20parameters%20beyond%20the%20LQG%0Asetting.%20We%20derive%20closed-form%20optimal%20parameters%20for%20one-dimensional%20WGANs%0Awhen%20the%20NN%20has%20non-linear%20activation%20functions%20and%20the%20data%20is%20non-Gaussian.%0ATo%20extend%20this%20to%20high-dimensional%20WGANs%2C%20we%20adopt%20the%20sliced%20Wasserstein%0Aframework%20and%20replace%20the%20constraint%20on%20marginal%20distributions%20of%20the%20randomly%0Aprojected%20data%20by%20a%20constraint%20on%20the%20joint%20distribution%20of%20the%20original%0A%28unprojected%29%20data.%20We%20show%20that%20the%20linear%20generator%20can%20be%20asymptotically%0Aoptimal%20for%20sliced%20WGAN%20with%20non-Gaussian%20data.%20Empirical%20studies%20show%20that%20our%0Aclosed-form%20WGAN%20parameters%20have%20good%20convergence%20behavior%20with%20data%20under%20both%0AGaussian%20and%20Laplace%20distributions.%20Also%2C%20compared%20to%20the%20r%20principal%20component%0Aanalysis%20%28r-PCA%29%20solution%2C%20our%20proposed%20solution%20for%20sliced%20WGAN%20can%20achieve%0Athe%20same%20performance%20while%20requiring%20less%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520optimal%2520solutions%2520of%2520classical%2520and%2520sliced%2520Wasserstein%2520GANs%2520with%250A%2520%2520non-Gaussian%2520data%26entry.906535625%3DYu-Jui%2520Huang%2520and%2520Hsin-Hua%2520Shen%2520and%2520Yu-Chih%2520Huang%2520and%2520Wan-Yi%2520Lin%2520and%2520Shih-Chun%2520Lin%26entry.1292438233%3D%2520%2520The%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520aims%2520to%2520approximate%2520an%2520unknown%250Adistribution%2520via%2520a%2520parameterized%2520neural%2520network%2520%2528NN%2529.%2520While%2520GANs%2520have%2520been%250Awidely%2520applied%2520in%2520reinforcement%2520and%2520semisupervised%2520learning%2520as%2520well%2520as%2520computer%250Avision%2520tasks%252C%2520selecting%2520their%2520parameters%2520often%2520needs%2520an%2520exhaustive%2520search%2520and%250Aonly%2520a%2520few%2520selection%2520methods%2520can%2520be%2520proved%2520to%2520be%2520theoretically%2520optimal.%2520One%2520of%250Athe%2520most%2520promising%2520GAN%2520variants%2520is%2520the%2520Wasserstein%2520GAN%2520%2528WGAN%2529.%2520Prior%2520work%2520on%250Aoptimal%2520parameters%2520for%2520WGAN%2520is%2520limited%2520to%2520the%2520linear-quadratic-Gaussian%2520%2528LQG%2529%250Asetting%252C%2520where%2520the%2520NN%2520is%2520linear%2520and%2520the%2520data%2520is%2520Gaussian.%2520In%2520this%2520paper%252C%2520we%250Afocus%2520on%2520the%2520characterization%2520of%2520optimal%2520WGAN%2520parameters%2520beyond%2520the%2520LQG%250Asetting.%2520We%2520derive%2520closed-form%2520optimal%2520parameters%2520for%2520one-dimensional%2520WGANs%250Awhen%2520the%2520NN%2520has%2520non-linear%2520activation%2520functions%2520and%2520the%2520data%2520is%2520non-Gaussian.%250ATo%2520extend%2520this%2520to%2520high-dimensional%2520WGANs%252C%2520we%2520adopt%2520the%2520sliced%2520Wasserstein%250Aframework%2520and%2520replace%2520the%2520constraint%2520on%2520marginal%2520distributions%2520of%2520the%2520randomly%250Aprojected%2520data%2520by%2520a%2520constraint%2520on%2520the%2520joint%2520distribution%2520of%2520the%2520original%250A%2528unprojected%2529%2520data.%2520We%2520show%2520that%2520the%2520linear%2520generator%2520can%2520be%2520asymptotically%250Aoptimal%2520for%2520sliced%2520WGAN%2520with%2520non-Gaussian%2520data.%2520Empirical%2520studies%2520show%2520that%2520our%250Aclosed-form%2520WGAN%2520parameters%2520have%2520good%2520convergence%2520behavior%2520with%2520data%2520under%2520both%250AGaussian%2520and%2520Laplace%2520distributions.%2520Also%252C%2520compared%2520to%2520the%2520r%2520principal%2520component%250Aanalysis%2520%2528r-PCA%2529%2520solution%252C%2520our%2520proposed%2520solution%2520for%2520sliced%2520WGAN%2520can%2520achieve%250Athe%2520same%2520performance%2520while%2520requiring%2520less%2520computational%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20optimal%20solutions%20of%20classical%20and%20sliced%20Wasserstein%20GANs%20with%0A%20%20non-Gaussian%20data&entry.906535625=Yu-Jui%20Huang%20and%20Hsin-Hua%20Shen%20and%20Yu-Chih%20Huang%20and%20Wan-Yi%20Lin%20and%20Shih-Chun%20Lin&entry.1292438233=%20%20The%20generative%20adversarial%20network%20%28GAN%29%20aims%20to%20approximate%20an%20unknown%0Adistribution%20via%20a%20parameterized%20neural%20network%20%28NN%29.%20While%20GANs%20have%20been%0Awidely%20applied%20in%20reinforcement%20and%20semisupervised%20learning%20as%20well%20as%20computer%0Avision%20tasks%2C%20selecting%20their%20parameters%20often%20needs%20an%20exhaustive%20search%20and%0Aonly%20a%20few%20selection%20methods%20can%20be%20proved%20to%20be%20theoretically%20optimal.%20One%20of%0Athe%20most%20promising%20GAN%20variants%20is%20the%20Wasserstein%20GAN%20%28WGAN%29.%20Prior%20work%20on%0Aoptimal%20parameters%20for%20WGAN%20is%20limited%20to%20the%20linear-quadratic-Gaussian%20%28LQG%29%0Asetting%2C%20where%20the%20NN%20is%20linear%20and%20the%20data%20is%20Gaussian.%20In%20this%20paper%2C%20we%0Afocus%20on%20the%20characterization%20of%20optimal%20WGAN%20parameters%20beyond%20the%20LQG%0Asetting.%20We%20derive%20closed-form%20optimal%20parameters%20for%20one-dimensional%20WGANs%0Awhen%20the%20NN%20has%20non-linear%20activation%20functions%20and%20the%20data%20is%20non-Gaussian.%0ATo%20extend%20this%20to%20high-dimensional%20WGANs%2C%20we%20adopt%20the%20sliced%20Wasserstein%0Aframework%20and%20replace%20the%20constraint%20on%20marginal%20distributions%20of%20the%20randomly%0Aprojected%20data%20by%20a%20constraint%20on%20the%20joint%20distribution%20of%20the%20original%0A%28unprojected%29%20data.%20We%20show%20that%20the%20linear%20generator%20can%20be%20asymptotically%0Aoptimal%20for%20sliced%20WGAN%20with%20non-Gaussian%20data.%20Empirical%20studies%20show%20that%20our%0Aclosed-form%20WGAN%20parameters%20have%20good%20convergence%20behavior%20with%20data%20under%20both%0AGaussian%20and%20Laplace%20distributions.%20Also%2C%20compared%20to%20the%20r%20principal%20component%0Aanalysis%20%28r-PCA%29%20solution%2C%20our%20proposed%20solution%20for%20sliced%20WGAN%20can%20achieve%0Athe%20same%20performance%20while%20requiring%20less%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06505v1&entry.124074799=Read"},
{"title": "Conversational Code Generation: a Case Study of Designing a Dialogue\n  System for Generating Driving Scenarios for Testing Autonomous Vehicles", "author": "Rimvydas Rubavicius and Antonio Valerio Miceli-Barone and Alex Lascarides and Subramanian Ramamoorthy", "abstract": "  Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation.\n", "link": "http://arxiv.org/abs/2410.09829v3", "date": "2025-09-08", "relevancy": 2.0456, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5298}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5097}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conversational%20Code%20Generation%3A%20a%20Case%20Study%20of%20Designing%20a%20Dialogue%0A%20%20System%20for%20Generating%20Driving%20Scenarios%20for%20Testing%20Autonomous%20Vehicles&body=Title%3A%20Conversational%20Code%20Generation%3A%20a%20Case%20Study%20of%20Designing%20a%20Dialogue%0A%20%20System%20for%20Generating%20Driving%20Scenarios%20for%20Testing%20Autonomous%20Vehicles%0AAuthor%3A%20Rimvydas%20Rubavicius%20and%20Antonio%20Valerio%20Miceli-Barone%20and%20Alex%20Lascarides%20and%20Subramanian%20Ramamoorthy%0AAbstract%3A%20%20%20Cyber-physical%20systems%20like%20autonomous%20vehicles%20are%20tested%20in%20simulation%0Abefore%20deployment%2C%20using%20domain-specific%20programs%20for%20scenario%20specification.%0ATo%20aid%20the%20testing%20of%20autonomous%20vehicles%20in%20simulation%2C%20we%20design%20a%20natural%0Alanguage%20interface%2C%20using%20an%20instruction-following%20large%20language%20model%2C%20to%0Aassist%20a%20non-coding%20domain%20expert%20in%20synthesising%20the%20desired%20scenarios%20and%0Avehicle%20behaviours.%20We%20show%20that%20using%20it%20to%20convert%20utterances%20to%20the%20symbolic%0Aprogram%20is%20feasible%2C%20despite%20the%20very%20small%20training%20dataset.%20Human%20experiments%0Ashow%20that%20dialogue%20is%20critical%20to%20successful%20simulation%20generation%2C%20leading%20to%0Aa%204.5%20times%20higher%20success%20rate%20than%20a%20generation%20without%20engaging%20in%20extended%0Aconversation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09829v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConversational%2520Code%2520Generation%253A%2520a%2520Case%2520Study%2520of%2520Designing%2520a%2520Dialogue%250A%2520%2520System%2520for%2520Generating%2520Driving%2520Scenarios%2520for%2520Testing%2520Autonomous%2520Vehicles%26entry.906535625%3DRimvydas%2520Rubavicius%2520and%2520Antonio%2520Valerio%2520Miceli-Barone%2520and%2520Alex%2520Lascarides%2520and%2520Subramanian%2520Ramamoorthy%26entry.1292438233%3D%2520%2520Cyber-physical%2520systems%2520like%2520autonomous%2520vehicles%2520are%2520tested%2520in%2520simulation%250Abefore%2520deployment%252C%2520using%2520domain-specific%2520programs%2520for%2520scenario%2520specification.%250ATo%2520aid%2520the%2520testing%2520of%2520autonomous%2520vehicles%2520in%2520simulation%252C%2520we%2520design%2520a%2520natural%250Alanguage%2520interface%252C%2520using%2520an%2520instruction-following%2520large%2520language%2520model%252C%2520to%250Aassist%2520a%2520non-coding%2520domain%2520expert%2520in%2520synthesising%2520the%2520desired%2520scenarios%2520and%250Avehicle%2520behaviours.%2520We%2520show%2520that%2520using%2520it%2520to%2520convert%2520utterances%2520to%2520the%2520symbolic%250Aprogram%2520is%2520feasible%252C%2520despite%2520the%2520very%2520small%2520training%2520dataset.%2520Human%2520experiments%250Ashow%2520that%2520dialogue%2520is%2520critical%2520to%2520successful%2520simulation%2520generation%252C%2520leading%2520to%250Aa%25204.5%2520times%2520higher%2520success%2520rate%2520than%2520a%2520generation%2520without%2520engaging%2520in%2520extended%250Aconversation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09829v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conversational%20Code%20Generation%3A%20a%20Case%20Study%20of%20Designing%20a%20Dialogue%0A%20%20System%20for%20Generating%20Driving%20Scenarios%20for%20Testing%20Autonomous%20Vehicles&entry.906535625=Rimvydas%20Rubavicius%20and%20Antonio%20Valerio%20Miceli-Barone%20and%20Alex%20Lascarides%20and%20Subramanian%20Ramamoorthy&entry.1292438233=%20%20Cyber-physical%20systems%20like%20autonomous%20vehicles%20are%20tested%20in%20simulation%0Abefore%20deployment%2C%20using%20domain-specific%20programs%20for%20scenario%20specification.%0ATo%20aid%20the%20testing%20of%20autonomous%20vehicles%20in%20simulation%2C%20we%20design%20a%20natural%0Alanguage%20interface%2C%20using%20an%20instruction-following%20large%20language%20model%2C%20to%0Aassist%20a%20non-coding%20domain%20expert%20in%20synthesising%20the%20desired%20scenarios%20and%0Avehicle%20behaviours.%20We%20show%20that%20using%20it%20to%20convert%20utterances%20to%20the%20symbolic%0Aprogram%20is%20feasible%2C%20despite%20the%20very%20small%20training%20dataset.%20Human%20experiments%0Ashow%20that%20dialogue%20is%20critical%20to%20successful%20simulation%20generation%2C%20leading%20to%0Aa%204.5%20times%20higher%20success%20rate%20than%20a%20generation%20without%20engaging%20in%20extended%0Aconversation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09829v3&entry.124074799=Read"},
{"title": "Antidistillation Sampling", "author": "Yash Savani and Asher Trockman and Zhili Feng and Yixuan Even Xu and Avi Schwarzschild and Alexander Robey and Marc Finzi and J. Zico Kolter", "abstract": "  Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By strategically\nmodifying a model's next-token probability distribution, antidistillation\nsampling poisons reasoning traces, rendering them significantly less effective\nfor distillation while preserving the model's practical utility. For further\ndetails, see https://antidistillation.com.\n", "link": "http://arxiv.org/abs/2504.13146v4", "date": "2025-09-08", "relevancy": 1.8939, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.499}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4658}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Antidistillation%20Sampling&body=Title%3A%20Antidistillation%20Sampling%0AAuthor%3A%20Yash%20Savani%20and%20Asher%20Trockman%20and%20Zhili%20Feng%20and%20Yixuan%20Even%20Xu%20and%20Avi%20Schwarzschild%20and%20Alexander%20Robey%20and%20Marc%20Finzi%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Frontier%20models%20that%20generate%20extended%20reasoning%20traces%20inadvertently%20produce%0Arich%20token%20sequences%20that%20can%20facilitate%20model%20distillation.%20Recognizing%20this%0Avulnerability%2C%20model%20owners%20may%20seek%20sampling%20strategies%20that%20limit%20the%0Aeffectiveness%20of%20distillation%20without%20compromising%20model%20performance.%0AAntidistillation%20sampling%20provides%20exactly%20this%20capability.%20By%20strategically%0Amodifying%20a%20model%27s%20next-token%20probability%20distribution%2C%20antidistillation%0Asampling%20poisons%20reasoning%20traces%2C%20rendering%20them%20significantly%20less%20effective%0Afor%20distillation%20while%20preserving%20the%20model%27s%20practical%20utility.%20For%20further%0Adetails%2C%20see%20https%3A//antidistillation.com.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13146v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAntidistillation%2520Sampling%26entry.906535625%3DYash%2520Savani%2520and%2520Asher%2520Trockman%2520and%2520Zhili%2520Feng%2520and%2520Yixuan%2520Even%2520Xu%2520and%2520Avi%2520Schwarzschild%2520and%2520Alexander%2520Robey%2520and%2520Marc%2520Finzi%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Frontier%2520models%2520that%2520generate%2520extended%2520reasoning%2520traces%2520inadvertently%2520produce%250Arich%2520token%2520sequences%2520that%2520can%2520facilitate%2520model%2520distillation.%2520Recognizing%2520this%250Avulnerability%252C%2520model%2520owners%2520may%2520seek%2520sampling%2520strategies%2520that%2520limit%2520the%250Aeffectiveness%2520of%2520distillation%2520without%2520compromising%2520model%2520performance.%250AAntidistillation%2520sampling%2520provides%2520exactly%2520this%2520capability.%2520By%2520strategically%250Amodifying%2520a%2520model%2527s%2520next-token%2520probability%2520distribution%252C%2520antidistillation%250Asampling%2520poisons%2520reasoning%2520traces%252C%2520rendering%2520them%2520significantly%2520less%2520effective%250Afor%2520distillation%2520while%2520preserving%2520the%2520model%2527s%2520practical%2520utility.%2520For%2520further%250Adetails%252C%2520see%2520https%253A//antidistillation.com.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13146v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Antidistillation%20Sampling&entry.906535625=Yash%20Savani%20and%20Asher%20Trockman%20and%20Zhili%20Feng%20and%20Yixuan%20Even%20Xu%20and%20Avi%20Schwarzschild%20and%20Alexander%20Robey%20and%20Marc%20Finzi%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Frontier%20models%20that%20generate%20extended%20reasoning%20traces%20inadvertently%20produce%0Arich%20token%20sequences%20that%20can%20facilitate%20model%20distillation.%20Recognizing%20this%0Avulnerability%2C%20model%20owners%20may%20seek%20sampling%20strategies%20that%20limit%20the%0Aeffectiveness%20of%20distillation%20without%20compromising%20model%20performance.%0AAntidistillation%20sampling%20provides%20exactly%20this%20capability.%20By%20strategically%0Amodifying%20a%20model%27s%20next-token%20probability%20distribution%2C%20antidistillation%0Asampling%20poisons%20reasoning%20traces%2C%20rendering%20them%20significantly%20less%20effective%0Afor%20distillation%20while%20preserving%20the%20model%27s%20practical%20utility.%20For%20further%0Adetails%2C%20see%20https%3A//antidistillation.com.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13146v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


