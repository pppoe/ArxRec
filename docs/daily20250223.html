<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250220.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting", "author": "Qilin Zhang and Olaf Wysocki and Steffen Urban and Boris Jutzi", "abstract": "  3D Gaussian Splatting (3DGS) has shown significant advantages in novel view\nsynthesis (NVS), particularly in achieving high rendering speeds and\nhigh-quality results. However, its geometric accuracy in 3D reconstruction\nremains limited due to the lack of explicit geometric constraints during\noptimization. This paper introduces CDGS, a confidence-aware depth\nregularization approach developed to enhance 3DGS. We leverage multi-cue\nconfidence maps of monocular depth estimation and sparse Structure-from-Motion\ndepth to adaptively adjust depth supervision during the optimization process.\nOur method demonstrates improved geometric detail preservation in early\ntraining stages and achieves competitive performance in both NVS quality and\ngeometric accuracy. Experiments on the publicly available Tanks and Temples\nbenchmark dataset show that our method achieves more stable convergence\nbehavior and more accurate geometric reconstruction results, with improvements\nof up to 2.31 dB in PSNR for NVS and consistently lower geometric errors in\nM3C2 distance metrics. Notably, our method reaches comparable F-scores to the\noriginal 3DGS with only 50% of the training iterations. We expect this work\nwill facilitate the development of efficient and accurate 3D reconstruction\nsystems for real-world applications such as digital twin creation, heritage\npreservation, or forestry applications.\n", "link": "http://arxiv.org/abs/2502.14684v1", "date": "2025-02-20", "relevancy": 3.416, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6824}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDGS%3A%20Confidence-Aware%20Depth%20Regularization%20for%203D%20Gaussian%20Splatting&body=Title%3A%20CDGS%3A%20Confidence-Aware%20Depth%20Regularization%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Qilin%20Zhang%20and%20Olaf%20Wysocki%20and%20Steffen%20Urban%20and%20Boris%20Jutzi%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20significant%20advantages%20in%20novel%20view%0Asynthesis%20%28NVS%29%2C%20particularly%20in%20achieving%20high%20rendering%20speeds%20and%0Ahigh-quality%20results.%20However%2C%20its%20geometric%20accuracy%20in%203D%20reconstruction%0Aremains%20limited%20due%20to%20the%20lack%20of%20explicit%20geometric%20constraints%20during%0Aoptimization.%20This%20paper%20introduces%20CDGS%2C%20a%20confidence-aware%20depth%0Aregularization%20approach%20developed%20to%20enhance%203DGS.%20We%20leverage%20multi-cue%0Aconfidence%20maps%20of%20monocular%20depth%20estimation%20and%20sparse%20Structure-from-Motion%0Adepth%20to%20adaptively%20adjust%20depth%20supervision%20during%20the%20optimization%20process.%0AOur%20method%20demonstrates%20improved%20geometric%20detail%20preservation%20in%20early%0Atraining%20stages%20and%20achieves%20competitive%20performance%20in%20both%20NVS%20quality%20and%0Ageometric%20accuracy.%20Experiments%20on%20the%20publicly%20available%20Tanks%20and%20Temples%0Abenchmark%20dataset%20show%20that%20our%20method%20achieves%20more%20stable%20convergence%0Abehavior%20and%20more%20accurate%20geometric%20reconstruction%20results%2C%20with%20improvements%0Aof%20up%20to%202.31%20dB%20in%20PSNR%20for%20NVS%20and%20consistently%20lower%20geometric%20errors%20in%0AM3C2%20distance%20metrics.%20Notably%2C%20our%20method%20reaches%20comparable%20F-scores%20to%20the%0Aoriginal%203DGS%20with%20only%2050%25%20of%20the%20training%20iterations.%20We%20expect%20this%20work%0Awill%20facilitate%20the%20development%20of%20efficient%20and%20accurate%203D%20reconstruction%0Asystems%20for%20real-world%20applications%20such%20as%20digital%20twin%20creation%2C%20heritage%0Apreservation%2C%20or%20forestry%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDGS%253A%2520Confidence-Aware%2520Depth%2520Regularization%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DQilin%2520Zhang%2520and%2520Olaf%2520Wysocki%2520and%2520Steffen%2520Urban%2520and%2520Boris%2520Jutzi%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520shown%2520significant%2520advantages%2520in%2520novel%2520view%250Asynthesis%2520%2528NVS%2529%252C%2520particularly%2520in%2520achieving%2520high%2520rendering%2520speeds%2520and%250Ahigh-quality%2520results.%2520However%252C%2520its%2520geometric%2520accuracy%2520in%25203D%2520reconstruction%250Aremains%2520limited%2520due%2520to%2520the%2520lack%2520of%2520explicit%2520geometric%2520constraints%2520during%250Aoptimization.%2520This%2520paper%2520introduces%2520CDGS%252C%2520a%2520confidence-aware%2520depth%250Aregularization%2520approach%2520developed%2520to%2520enhance%25203DGS.%2520We%2520leverage%2520multi-cue%250Aconfidence%2520maps%2520of%2520monocular%2520depth%2520estimation%2520and%2520sparse%2520Structure-from-Motion%250Adepth%2520to%2520adaptively%2520adjust%2520depth%2520supervision%2520during%2520the%2520optimization%2520process.%250AOur%2520method%2520demonstrates%2520improved%2520geometric%2520detail%2520preservation%2520in%2520early%250Atraining%2520stages%2520and%2520achieves%2520competitive%2520performance%2520in%2520both%2520NVS%2520quality%2520and%250Ageometric%2520accuracy.%2520Experiments%2520on%2520the%2520publicly%2520available%2520Tanks%2520and%2520Temples%250Abenchmark%2520dataset%2520show%2520that%2520our%2520method%2520achieves%2520more%2520stable%2520convergence%250Abehavior%2520and%2520more%2520accurate%2520geometric%2520reconstruction%2520results%252C%2520with%2520improvements%250Aof%2520up%2520to%25202.31%2520dB%2520in%2520PSNR%2520for%2520NVS%2520and%2520consistently%2520lower%2520geometric%2520errors%2520in%250AM3C2%2520distance%2520metrics.%2520Notably%252C%2520our%2520method%2520reaches%2520comparable%2520F-scores%2520to%2520the%250Aoriginal%25203DGS%2520with%2520only%252050%2525%2520of%2520the%2520training%2520iterations.%2520We%2520expect%2520this%2520work%250Awill%2520facilitate%2520the%2520development%2520of%2520efficient%2520and%2520accurate%25203D%2520reconstruction%250Asystems%2520for%2520real-world%2520applications%2520such%2520as%2520digital%2520twin%2520creation%252C%2520heritage%250Apreservation%252C%2520or%2520forestry%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDGS%3A%20Confidence-Aware%20Depth%20Regularization%20for%203D%20Gaussian%20Splatting&entry.906535625=Qilin%20Zhang%20and%20Olaf%20Wysocki%20and%20Steffen%20Urban%20and%20Boris%20Jutzi&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20significant%20advantages%20in%20novel%20view%0Asynthesis%20%28NVS%29%2C%20particularly%20in%20achieving%20high%20rendering%20speeds%20and%0Ahigh-quality%20results.%20However%2C%20its%20geometric%20accuracy%20in%203D%20reconstruction%0Aremains%20limited%20due%20to%20the%20lack%20of%20explicit%20geometric%20constraints%20during%0Aoptimization.%20This%20paper%20introduces%20CDGS%2C%20a%20confidence-aware%20depth%0Aregularization%20approach%20developed%20to%20enhance%203DGS.%20We%20leverage%20multi-cue%0Aconfidence%20maps%20of%20monocular%20depth%20estimation%20and%20sparse%20Structure-from-Motion%0Adepth%20to%20adaptively%20adjust%20depth%20supervision%20during%20the%20optimization%20process.%0AOur%20method%20demonstrates%20improved%20geometric%20detail%20preservation%20in%20early%0Atraining%20stages%20and%20achieves%20competitive%20performance%20in%20both%20NVS%20quality%20and%0Ageometric%20accuracy.%20Experiments%20on%20the%20publicly%20available%20Tanks%20and%20Temples%0Abenchmark%20dataset%20show%20that%20our%20method%20achieves%20more%20stable%20convergence%0Abehavior%20and%20more%20accurate%20geometric%20reconstruction%20results%2C%20with%20improvements%0Aof%20up%20to%202.31%20dB%20in%20PSNR%20for%20NVS%20and%20consistently%20lower%20geometric%20errors%20in%0AM3C2%20distance%20metrics.%20Notably%2C%20our%20method%20reaches%20comparable%20F-scores%20to%20the%0Aoriginal%203DGS%20with%20only%2050%25%20of%20the%20training%20iterations.%20We%20expect%20this%20work%0Awill%20facilitate%20the%20development%20of%20efficient%20and%20accurate%203D%20reconstruction%0Asystems%20for%20real-world%20applications%20such%20as%20digital%20twin%20creation%2C%20heritage%0Apreservation%2C%20or%20forestry%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14684v1&entry.124074799=Read"},
{"title": "Structurally Disentangled Feature Fields Distillation for 3D\n  Understanding and Editing", "author": "Yoel Levy and David Shavin and Itai Lang and Sagie Benaim", "abstract": "  Recent work has demonstrated the ability to leverage or distill pre-trained\n2D features obtained using large pre-trained 2D models into 3D features,\nenabling impressive 3D editing and understanding capabilities using only 2D\nsupervision. Although impressive, models assume that 3D features are captured\nusing a single feature field and often make a simplifying assumption that\nfeatures are view-independent. In this work, we propose instead to capture 3D\nfeatures using multiple disentangled feature fields that capture different\nstructural components of 3D features involving view-dependent and\nview-independent components, which can be learned from 2D feature supervision\nonly. Subsequently, each element can be controlled in isolation, enabling\nsemantic and structural understanding and editing capabilities. For instance,\nusing a user click, one can segment 3D features corresponding to a given object\nand then segment, edit, or remove their view-dependent (reflective) properties.\nWe evaluate our approach on the task of 3D segmentation and demonstrate a set\nof novel understanding and editing tasks.\n", "link": "http://arxiv.org/abs/2502.14789v1", "date": "2025-02-20", "relevancy": 3.0925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structurally%20Disentangled%20Feature%20Fields%20Distillation%20for%203D%0A%20%20Understanding%20and%20Editing&body=Title%3A%20Structurally%20Disentangled%20Feature%20Fields%20Distillation%20for%203D%0A%20%20Understanding%20and%20Editing%0AAuthor%3A%20Yoel%20Levy%20and%20David%20Shavin%20and%20Itai%20Lang%20and%20Sagie%20Benaim%0AAbstract%3A%20%20%20Recent%20work%20has%20demonstrated%20the%20ability%20to%20leverage%20or%20distill%20pre-trained%0A2D%20features%20obtained%20using%20large%20pre-trained%202D%20models%20into%203D%20features%2C%0Aenabling%20impressive%203D%20editing%20and%20understanding%20capabilities%20using%20only%202D%0Asupervision.%20Although%20impressive%2C%20models%20assume%20that%203D%20features%20are%20captured%0Ausing%20a%20single%20feature%20field%20and%20often%20make%20a%20simplifying%20assumption%20that%0Afeatures%20are%20view-independent.%20In%20this%20work%2C%20we%20propose%20instead%20to%20capture%203D%0Afeatures%20using%20multiple%20disentangled%20feature%20fields%20that%20capture%20different%0Astructural%20components%20of%203D%20features%20involving%20view-dependent%20and%0Aview-independent%20components%2C%20which%20can%20be%20learned%20from%202D%20feature%20supervision%0Aonly.%20Subsequently%2C%20each%20element%20can%20be%20controlled%20in%20isolation%2C%20enabling%0Asemantic%20and%20structural%20understanding%20and%20editing%20capabilities.%20For%20instance%2C%0Ausing%20a%20user%20click%2C%20one%20can%20segment%203D%20features%20corresponding%20to%20a%20given%20object%0Aand%20then%20segment%2C%20edit%2C%20or%20remove%20their%20view-dependent%20%28reflective%29%20properties.%0AWe%20evaluate%20our%20approach%20on%20the%20task%20of%203D%20segmentation%20and%20demonstrate%20a%20set%0Aof%20novel%20understanding%20and%20editing%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructurally%2520Disentangled%2520Feature%2520Fields%2520Distillation%2520for%25203D%250A%2520%2520Understanding%2520and%2520Editing%26entry.906535625%3DYoel%2520Levy%2520and%2520David%2520Shavin%2520and%2520Itai%2520Lang%2520and%2520Sagie%2520Benaim%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520demonstrated%2520the%2520ability%2520to%2520leverage%2520or%2520distill%2520pre-trained%250A2D%2520features%2520obtained%2520using%2520large%2520pre-trained%25202D%2520models%2520into%25203D%2520features%252C%250Aenabling%2520impressive%25203D%2520editing%2520and%2520understanding%2520capabilities%2520using%2520only%25202D%250Asupervision.%2520Although%2520impressive%252C%2520models%2520assume%2520that%25203D%2520features%2520are%2520captured%250Ausing%2520a%2520single%2520feature%2520field%2520and%2520often%2520make%2520a%2520simplifying%2520assumption%2520that%250Afeatures%2520are%2520view-independent.%2520In%2520this%2520work%252C%2520we%2520propose%2520instead%2520to%2520capture%25203D%250Afeatures%2520using%2520multiple%2520disentangled%2520feature%2520fields%2520that%2520capture%2520different%250Astructural%2520components%2520of%25203D%2520features%2520involving%2520view-dependent%2520and%250Aview-independent%2520components%252C%2520which%2520can%2520be%2520learned%2520from%25202D%2520feature%2520supervision%250Aonly.%2520Subsequently%252C%2520each%2520element%2520can%2520be%2520controlled%2520in%2520isolation%252C%2520enabling%250Asemantic%2520and%2520structural%2520understanding%2520and%2520editing%2520capabilities.%2520For%2520instance%252C%250Ausing%2520a%2520user%2520click%252C%2520one%2520can%2520segment%25203D%2520features%2520corresponding%2520to%2520a%2520given%2520object%250Aand%2520then%2520segment%252C%2520edit%252C%2520or%2520remove%2520their%2520view-dependent%2520%2528reflective%2529%2520properties.%250AWe%2520evaluate%2520our%2520approach%2520on%2520the%2520task%2520of%25203D%2520segmentation%2520and%2520demonstrate%2520a%2520set%250Aof%2520novel%2520understanding%2520and%2520editing%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structurally%20Disentangled%20Feature%20Fields%20Distillation%20for%203D%0A%20%20Understanding%20and%20Editing&entry.906535625=Yoel%20Levy%20and%20David%20Shavin%20and%20Itai%20Lang%20and%20Sagie%20Benaim&entry.1292438233=%20%20Recent%20work%20has%20demonstrated%20the%20ability%20to%20leverage%20or%20distill%20pre-trained%0A2D%20features%20obtained%20using%20large%20pre-trained%202D%20models%20into%203D%20features%2C%0Aenabling%20impressive%203D%20editing%20and%20understanding%20capabilities%20using%20only%202D%0Asupervision.%20Although%20impressive%2C%20models%20assume%20that%203D%20features%20are%20captured%0Ausing%20a%20single%20feature%20field%20and%20often%20make%20a%20simplifying%20assumption%20that%0Afeatures%20are%20view-independent.%20In%20this%20work%2C%20we%20propose%20instead%20to%20capture%203D%0Afeatures%20using%20multiple%20disentangled%20feature%20fields%20that%20capture%20different%0Astructural%20components%20of%203D%20features%20involving%20view-dependent%20and%0Aview-independent%20components%2C%20which%20can%20be%20learned%20from%202D%20feature%20supervision%0Aonly.%20Subsequently%2C%20each%20element%20can%20be%20controlled%20in%20isolation%2C%20enabling%0Asemantic%20and%20structural%20understanding%20and%20editing%20capabilities.%20For%20instance%2C%0Ausing%20a%20user%20click%2C%20one%20can%20segment%203D%20features%20corresponding%20to%20a%20given%20object%0Aand%20then%20segment%2C%20edit%2C%20or%20remove%20their%20view-dependent%20%28reflective%29%20properties.%0AWe%20evaluate%20our%20approach%20on%20the%20task%20of%203D%20segmentation%20and%20demonstrate%20a%20set%0Aof%20novel%20understanding%20and%20editing%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14789v1&entry.124074799=Read"},
{"title": "Sketch2CAD: 3D CAD Model Reconstruction from 2D Sketch using Visual\n  Transformer", "author": "Hong-Bin Yang", "abstract": "  Current 3D reconstruction methods typically generate outputs in the form of\nvoxels, point clouds, or meshes. However, each of these formats has inherent\nlimitations, such as rough surfaces and distorted structures. Additionally,\nthese data types are not ideal for further manual editing and post-processing.\nIn this paper, we present a novel 3D reconstruction method designed to overcome\nthese disadvantages by reconstructing CAD-compatible models. We trained a\nvisual transformer to predict a \"scene descriptor\" from a single 2D wire-frame\nimage. This descriptor includes essential information, such as object types and\nparameters like position, rotation, and size. Using the predicted parameters, a\n3D scene can be reconstructed with 3D modeling software that has programmable\ninterfaces, such as Rhino Grasshopper, to build highly editable 3D models in\nthe form of B-rep. To evaluate our proposed model, we created two datasets: one\nconsisting of simple scenes and another with more complex scenes. The test\nresults indicate the model's capability to accurately reconstruct simple scenes\nwhile highlighting its difficulties with more complex ones.\n", "link": "http://arxiv.org/abs/2309.16850v2", "date": "2025-02-20", "relevancy": 3.0715, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6332}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch2CAD%3A%203D%20CAD%20Model%20Reconstruction%20from%202D%20Sketch%20using%20Visual%0A%20%20Transformer&body=Title%3A%20Sketch2CAD%3A%203D%20CAD%20Model%20Reconstruction%20from%202D%20Sketch%20using%20Visual%0A%20%20Transformer%0AAuthor%3A%20Hong-Bin%20Yang%0AAbstract%3A%20%20%20Current%203D%20reconstruction%20methods%20typically%20generate%20outputs%20in%20the%20form%20of%0Avoxels%2C%20point%20clouds%2C%20or%20meshes.%20However%2C%20each%20of%20these%20formats%20has%20inherent%0Alimitations%2C%20such%20as%20rough%20surfaces%20and%20distorted%20structures.%20Additionally%2C%0Athese%20data%20types%20are%20not%20ideal%20for%20further%20manual%20editing%20and%20post-processing.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%203D%20reconstruction%20method%20designed%20to%20overcome%0Athese%20disadvantages%20by%20reconstructing%20CAD-compatible%20models.%20We%20trained%20a%0Avisual%20transformer%20to%20predict%20a%20%22scene%20descriptor%22%20from%20a%20single%202D%20wire-frame%0Aimage.%20This%20descriptor%20includes%20essential%20information%2C%20such%20as%20object%20types%20and%0Aparameters%20like%20position%2C%20rotation%2C%20and%20size.%20Using%20the%20predicted%20parameters%2C%20a%0A3D%20scene%20can%20be%20reconstructed%20with%203D%20modeling%20software%20that%20has%20programmable%0Ainterfaces%2C%20such%20as%20Rhino%20Grasshopper%2C%20to%20build%20highly%20editable%203D%20models%20in%0Athe%20form%20of%20B-rep.%20To%20evaluate%20our%20proposed%20model%2C%20we%20created%20two%20datasets%3A%20one%0Aconsisting%20of%20simple%20scenes%20and%20another%20with%20more%20complex%20scenes.%20The%20test%0Aresults%20indicate%20the%20model%27s%20capability%20to%20accurately%20reconstruct%20simple%20scenes%0Awhile%20highlighting%20its%20difficulties%20with%20more%20complex%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch2CAD%253A%25203D%2520CAD%2520Model%2520Reconstruction%2520from%25202D%2520Sketch%2520using%2520Visual%250A%2520%2520Transformer%26entry.906535625%3DHong-Bin%2520Yang%26entry.1292438233%3D%2520%2520Current%25203D%2520reconstruction%2520methods%2520typically%2520generate%2520outputs%2520in%2520the%2520form%2520of%250Avoxels%252C%2520point%2520clouds%252C%2520or%2520meshes.%2520However%252C%2520each%2520of%2520these%2520formats%2520has%2520inherent%250Alimitations%252C%2520such%2520as%2520rough%2520surfaces%2520and%2520distorted%2520structures.%2520Additionally%252C%250Athese%2520data%2520types%2520are%2520not%2520ideal%2520for%2520further%2520manual%2520editing%2520and%2520post-processing.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%25203D%2520reconstruction%2520method%2520designed%2520to%2520overcome%250Athese%2520disadvantages%2520by%2520reconstructing%2520CAD-compatible%2520models.%2520We%2520trained%2520a%250Avisual%2520transformer%2520to%2520predict%2520a%2520%2522scene%2520descriptor%2522%2520from%2520a%2520single%25202D%2520wire-frame%250Aimage.%2520This%2520descriptor%2520includes%2520essential%2520information%252C%2520such%2520as%2520object%2520types%2520and%250Aparameters%2520like%2520position%252C%2520rotation%252C%2520and%2520size.%2520Using%2520the%2520predicted%2520parameters%252C%2520a%250A3D%2520scene%2520can%2520be%2520reconstructed%2520with%25203D%2520modeling%2520software%2520that%2520has%2520programmable%250Ainterfaces%252C%2520such%2520as%2520Rhino%2520Grasshopper%252C%2520to%2520build%2520highly%2520editable%25203D%2520models%2520in%250Athe%2520form%2520of%2520B-rep.%2520To%2520evaluate%2520our%2520proposed%2520model%252C%2520we%2520created%2520two%2520datasets%253A%2520one%250Aconsisting%2520of%2520simple%2520scenes%2520and%2520another%2520with%2520more%2520complex%2520scenes.%2520The%2520test%250Aresults%2520indicate%2520the%2520model%2527s%2520capability%2520to%2520accurately%2520reconstruct%2520simple%2520scenes%250Awhile%2520highlighting%2520its%2520difficulties%2520with%2520more%2520complex%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch2CAD%3A%203D%20CAD%20Model%20Reconstruction%20from%202D%20Sketch%20using%20Visual%0A%20%20Transformer&entry.906535625=Hong-Bin%20Yang&entry.1292438233=%20%20Current%203D%20reconstruction%20methods%20typically%20generate%20outputs%20in%20the%20form%20of%0Avoxels%2C%20point%20clouds%2C%20or%20meshes.%20However%2C%20each%20of%20these%20formats%20has%20inherent%0Alimitations%2C%20such%20as%20rough%20surfaces%20and%20distorted%20structures.%20Additionally%2C%0Athese%20data%20types%20are%20not%20ideal%20for%20further%20manual%20editing%20and%20post-processing.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%203D%20reconstruction%20method%20designed%20to%20overcome%0Athese%20disadvantages%20by%20reconstructing%20CAD-compatible%20models.%20We%20trained%20a%0Avisual%20transformer%20to%20predict%20a%20%22scene%20descriptor%22%20from%20a%20single%202D%20wire-frame%0Aimage.%20This%20descriptor%20includes%20essential%20information%2C%20such%20as%20object%20types%20and%0Aparameters%20like%20position%2C%20rotation%2C%20and%20size.%20Using%20the%20predicted%20parameters%2C%20a%0A3D%20scene%20can%20be%20reconstructed%20with%203D%20modeling%20software%20that%20has%20programmable%0Ainterfaces%2C%20such%20as%20Rhino%20Grasshopper%2C%20to%20build%20highly%20editable%203D%20models%20in%0Athe%20form%20of%20B-rep.%20To%20evaluate%20our%20proposed%20model%2C%20we%20created%20two%20datasets%3A%20one%0Aconsisting%20of%20simple%20scenes%20and%20another%20with%20more%20complex%20scenes.%20The%20test%0Aresults%20indicate%20the%20model%27s%20capability%20to%20accurately%20reconstruct%20simple%20scenes%0Awhile%20highlighting%20its%20difficulties%20with%20more%20complex%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16850v2&entry.124074799=Read"},
{"title": "Exploring Advanced Techniques for Visual Question Answering: A\n  Comprehensive Comparison", "author": "Aiswarya Baby and Tintu Thankom Koshy", "abstract": "  Visual Question Answering (VQA) has emerged as a pivotal task in the\nintersection of computer vision and natural language processing, requiring\nmodels to understand and reason about visual content in response to natural\nlanguage questions. Analyzing VQA datasets is essential for developing robust\nmodels that can handle the complexities of multimodal reasoning. Several\napproaches have been developed to examine these datasets, each offering\ndistinct perspectives on question diversity, answer distribution, and\nvisual-textual correlations. Despite significant progress, existing VQA models\nface challenges related to dataset bias, limited model complexity, commonsense\nreasoning gaps, rigid evaluation methods, and generalization to real world\nscenarios. This paper presents a comprehensive comparative study of five\nadvanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling,\nBLIP-2, and OFA, each employing distinct methodologies to address these\nchallenges.\n", "link": "http://arxiv.org/abs/2502.14827v1", "date": "2025-02-20", "relevancy": 3.0422, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6513}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Advanced%20Techniques%20for%20Visual%20Question%20Answering%3A%20A%0A%20%20Comprehensive%20Comparison&body=Title%3A%20Exploring%20Advanced%20Techniques%20for%20Visual%20Question%20Answering%3A%20A%0A%20%20Comprehensive%20Comparison%0AAuthor%3A%20Aiswarya%20Baby%20and%20Tintu%20Thankom%20Koshy%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20has%20emerged%20as%20a%20pivotal%20task%20in%20the%0Aintersection%20of%20computer%20vision%20and%20natural%20language%20processing%2C%20requiring%0Amodels%20to%20understand%20and%20reason%20about%20visual%20content%20in%20response%20to%20natural%0Alanguage%20questions.%20Analyzing%20VQA%20datasets%20is%20essential%20for%20developing%20robust%0Amodels%20that%20can%20handle%20the%20complexities%20of%20multimodal%20reasoning.%20Several%0Aapproaches%20have%20been%20developed%20to%20examine%20these%20datasets%2C%20each%20offering%0Adistinct%20perspectives%20on%20question%20diversity%2C%20answer%20distribution%2C%20and%0Avisual-textual%20correlations.%20Despite%20significant%20progress%2C%20existing%20VQA%20models%0Aface%20challenges%20related%20to%20dataset%20bias%2C%20limited%20model%20complexity%2C%20commonsense%0Areasoning%20gaps%2C%20rigid%20evaluation%20methods%2C%20and%20generalization%20to%20real%20world%0Ascenarios.%20This%20paper%20presents%20a%20comprehensive%20comparative%20study%20of%20five%0Aadvanced%20VQA%20models%3A%20ABC-CNN%2C%20KICNLE%2C%20Masked%20Vision%20and%20Language%20Modeling%2C%0ABLIP-2%2C%20and%20OFA%2C%20each%20employing%20distinct%20methodologies%20to%20address%20these%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Advanced%2520Techniques%2520for%2520Visual%2520Question%2520Answering%253A%2520A%250A%2520%2520Comprehensive%2520Comparison%26entry.906535625%3DAiswarya%2520Baby%2520and%2520Tintu%2520Thankom%2520Koshy%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520task%2520in%2520the%250Aintersection%2520of%2520computer%2520vision%2520and%2520natural%2520language%2520processing%252C%2520requiring%250Amodels%2520to%2520understand%2520and%2520reason%2520about%2520visual%2520content%2520in%2520response%2520to%2520natural%250Alanguage%2520questions.%2520Analyzing%2520VQA%2520datasets%2520is%2520essential%2520for%2520developing%2520robust%250Amodels%2520that%2520can%2520handle%2520the%2520complexities%2520of%2520multimodal%2520reasoning.%2520Several%250Aapproaches%2520have%2520been%2520developed%2520to%2520examine%2520these%2520datasets%252C%2520each%2520offering%250Adistinct%2520perspectives%2520on%2520question%2520diversity%252C%2520answer%2520distribution%252C%2520and%250Avisual-textual%2520correlations.%2520Despite%2520significant%2520progress%252C%2520existing%2520VQA%2520models%250Aface%2520challenges%2520related%2520to%2520dataset%2520bias%252C%2520limited%2520model%2520complexity%252C%2520commonsense%250Areasoning%2520gaps%252C%2520rigid%2520evaluation%2520methods%252C%2520and%2520generalization%2520to%2520real%2520world%250Ascenarios.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520comparative%2520study%2520of%2520five%250Aadvanced%2520VQA%2520models%253A%2520ABC-CNN%252C%2520KICNLE%252C%2520Masked%2520Vision%2520and%2520Language%2520Modeling%252C%250ABLIP-2%252C%2520and%2520OFA%252C%2520each%2520employing%2520distinct%2520methodologies%2520to%2520address%2520these%250Achallenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Advanced%20Techniques%20for%20Visual%20Question%20Answering%3A%20A%0A%20%20Comprehensive%20Comparison&entry.906535625=Aiswarya%20Baby%20and%20Tintu%20Thankom%20Koshy&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20has%20emerged%20as%20a%20pivotal%20task%20in%20the%0Aintersection%20of%20computer%20vision%20and%20natural%20language%20processing%2C%20requiring%0Amodels%20to%20understand%20and%20reason%20about%20visual%20content%20in%20response%20to%20natural%0Alanguage%20questions.%20Analyzing%20VQA%20datasets%20is%20essential%20for%20developing%20robust%0Amodels%20that%20can%20handle%20the%20complexities%20of%20multimodal%20reasoning.%20Several%0Aapproaches%20have%20been%20developed%20to%20examine%20these%20datasets%2C%20each%20offering%0Adistinct%20perspectives%20on%20question%20diversity%2C%20answer%20distribution%2C%20and%0Avisual-textual%20correlations.%20Despite%20significant%20progress%2C%20existing%20VQA%20models%0Aface%20challenges%20related%20to%20dataset%20bias%2C%20limited%20model%20complexity%2C%20commonsense%0Areasoning%20gaps%2C%20rigid%20evaluation%20methods%2C%20and%20generalization%20to%20real%20world%0Ascenarios.%20This%20paper%20presents%20a%20comprehensive%20comparative%20study%20of%20five%0Aadvanced%20VQA%20models%3A%20ABC-CNN%2C%20KICNLE%2C%20Masked%20Vision%20and%20Language%20Modeling%2C%0ABLIP-2%2C%20and%20OFA%2C%20each%20employing%20distinct%20methodologies%20to%20address%20these%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14827v1&entry.124074799=Read"},
{"title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning", "author": "Weitai Kang and Haifeng Huang and Yuzhang Shang and Mubarak Shah and Yan Yan", "abstract": "  Recent advancements in 3D Large Language Models (3DLLMs) have highlighted\ntheir potential in building general-purpose agents in the 3D real world, yet\nchallenges remain due to the lack of high-quality robust instruction-following\ndata, leading to limited discriminative power and generalization of 3DLLMs. In\nthis paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale\ninstruction-following data generated by our novel data engine, Robust\nInstruction Generation (RIG) engine. RIG generates two key instruction data: 1)\nthe Adversarial Instruction-following data, which features mixed negative and\npositive samples to enhance the model's discriminative understanding. 2) the\nDiverse Instruction-following data, which contains various instruction styles\nto enhance model's generalization. As a result, we construct 1 million\ninstruction-following data, consisting of 344K Adversarial samples, 508K\nDiverse samples, and 165K benchmark training set samples. To better handle\nthese complex instructions, Robin3D first incorporates Relation-Augmented\nProjector to enhance spatial understanding, and then strengthens the object\nreferring and grounding ability through ID-Feature Bonding. Robin3D\nconsistently outperforms previous methods across five widely-used 3D multimodal\nlearning benchmarks, without the need for task-specific fine-tuning. Notably,\nwe achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\%\nimprovement in the captioning task (Scan2Cap).\n", "link": "http://arxiv.org/abs/2410.00255v2", "date": "2025-02-20", "relevancy": 3.02, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robin3D%3A%20Improving%203D%20Large%20Language%20Model%20via%20Robust%20Instruction%20Tuning&body=Title%3A%20Robin3D%3A%20Improving%203D%20Large%20Language%20Model%20via%20Robust%20Instruction%20Tuning%0AAuthor%3A%20Weitai%20Kang%20and%20Haifeng%20Huang%20and%20Yuzhang%20Shang%20and%20Mubarak%20Shah%20and%20Yan%20Yan%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20Large%20Language%20Models%20%283DLLMs%29%20have%20highlighted%0Atheir%20potential%20in%20building%20general-purpose%20agents%20in%20the%203D%20real%20world%2C%20yet%0Achallenges%20remain%20due%20to%20the%20lack%20of%20high-quality%20robust%20instruction-following%0Adata%2C%20leading%20to%20limited%20discriminative%20power%20and%20generalization%20of%203DLLMs.%20In%0Athis%20paper%2C%20we%20introduce%20Robin3D%2C%20a%20powerful%203DLLM%20trained%20on%20large-scale%0Ainstruction-following%20data%20generated%20by%20our%20novel%20data%20engine%2C%20Robust%0AInstruction%20Generation%20%28RIG%29%20engine.%20RIG%20generates%20two%20key%20instruction%20data%3A%201%29%0Athe%20Adversarial%20Instruction-following%20data%2C%20which%20features%20mixed%20negative%20and%0Apositive%20samples%20to%20enhance%20the%20model%27s%20discriminative%20understanding.%202%29%20the%0ADiverse%20Instruction-following%20data%2C%20which%20contains%20various%20instruction%20styles%0Ato%20enhance%20model%27s%20generalization.%20As%20a%20result%2C%20we%20construct%201%20million%0Ainstruction-following%20data%2C%20consisting%20of%20344K%20Adversarial%20samples%2C%20508K%0ADiverse%20samples%2C%20and%20165K%20benchmark%20training%20set%20samples.%20To%20better%20handle%0Athese%20complex%20instructions%2C%20Robin3D%20first%20incorporates%20Relation-Augmented%0AProjector%20to%20enhance%20spatial%20understanding%2C%20and%20then%20strengthens%20the%20object%0Areferring%20and%20grounding%20ability%20through%20ID-Feature%20Bonding.%20Robin3D%0Aconsistently%20outperforms%20previous%20methods%20across%20five%20widely-used%203D%20multimodal%0Alearning%20benchmarks%2C%20without%20the%20need%20for%20task-specific%20fine-tuning.%20Notably%2C%0Awe%20achieve%20a%207.8%5C%25%20improvement%20in%20the%20grounding%20task%20%28Multi3DRefer%29%20and%20a%206.9%5C%25%0Aimprovement%20in%20the%20captioning%20task%20%28Scan2Cap%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00255v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobin3D%253A%2520Improving%25203D%2520Large%2520Language%2520Model%2520via%2520Robust%2520Instruction%2520Tuning%26entry.906535625%3DWeitai%2520Kang%2520and%2520Haifeng%2520Huang%2520and%2520Yuzhang%2520Shang%2520and%2520Mubarak%2520Shah%2520and%2520Yan%2520Yan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520Large%2520Language%2520Models%2520%25283DLLMs%2529%2520have%2520highlighted%250Atheir%2520potential%2520in%2520building%2520general-purpose%2520agents%2520in%2520the%25203D%2520real%2520world%252C%2520yet%250Achallenges%2520remain%2520due%2520to%2520the%2520lack%2520of%2520high-quality%2520robust%2520instruction-following%250Adata%252C%2520leading%2520to%2520limited%2520discriminative%2520power%2520and%2520generalization%2520of%25203DLLMs.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Robin3D%252C%2520a%2520powerful%25203DLLM%2520trained%2520on%2520large-scale%250Ainstruction-following%2520data%2520generated%2520by%2520our%2520novel%2520data%2520engine%252C%2520Robust%250AInstruction%2520Generation%2520%2528RIG%2529%2520engine.%2520RIG%2520generates%2520two%2520key%2520instruction%2520data%253A%25201%2529%250Athe%2520Adversarial%2520Instruction-following%2520data%252C%2520which%2520features%2520mixed%2520negative%2520and%250Apositive%2520samples%2520to%2520enhance%2520the%2520model%2527s%2520discriminative%2520understanding.%25202%2529%2520the%250ADiverse%2520Instruction-following%2520data%252C%2520which%2520contains%2520various%2520instruction%2520styles%250Ato%2520enhance%2520model%2527s%2520generalization.%2520As%2520a%2520result%252C%2520we%2520construct%25201%2520million%250Ainstruction-following%2520data%252C%2520consisting%2520of%2520344K%2520Adversarial%2520samples%252C%2520508K%250ADiverse%2520samples%252C%2520and%2520165K%2520benchmark%2520training%2520set%2520samples.%2520To%2520better%2520handle%250Athese%2520complex%2520instructions%252C%2520Robin3D%2520first%2520incorporates%2520Relation-Augmented%250AProjector%2520to%2520enhance%2520spatial%2520understanding%252C%2520and%2520then%2520strengthens%2520the%2520object%250Areferring%2520and%2520grounding%2520ability%2520through%2520ID-Feature%2520Bonding.%2520Robin3D%250Aconsistently%2520outperforms%2520previous%2520methods%2520across%2520five%2520widely-used%25203D%2520multimodal%250Alearning%2520benchmarks%252C%2520without%2520the%2520need%2520for%2520task-specific%2520fine-tuning.%2520Notably%252C%250Awe%2520achieve%2520a%25207.8%255C%2525%2520improvement%2520in%2520the%2520grounding%2520task%2520%2528Multi3DRefer%2529%2520and%2520a%25206.9%255C%2525%250Aimprovement%2520in%2520the%2520captioning%2520task%2520%2528Scan2Cap%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00255v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robin3D%3A%20Improving%203D%20Large%20Language%20Model%20via%20Robust%20Instruction%20Tuning&entry.906535625=Weitai%20Kang%20and%20Haifeng%20Huang%20and%20Yuzhang%20Shang%20and%20Mubarak%20Shah%20and%20Yan%20Yan&entry.1292438233=%20%20Recent%20advancements%20in%203D%20Large%20Language%20Models%20%283DLLMs%29%20have%20highlighted%0Atheir%20potential%20in%20building%20general-purpose%20agents%20in%20the%203D%20real%20world%2C%20yet%0Achallenges%20remain%20due%20to%20the%20lack%20of%20high-quality%20robust%20instruction-following%0Adata%2C%20leading%20to%20limited%20discriminative%20power%20and%20generalization%20of%203DLLMs.%20In%0Athis%20paper%2C%20we%20introduce%20Robin3D%2C%20a%20powerful%203DLLM%20trained%20on%20large-scale%0Ainstruction-following%20data%20generated%20by%20our%20novel%20data%20engine%2C%20Robust%0AInstruction%20Generation%20%28RIG%29%20engine.%20RIG%20generates%20two%20key%20instruction%20data%3A%201%29%0Athe%20Adversarial%20Instruction-following%20data%2C%20which%20features%20mixed%20negative%20and%0Apositive%20samples%20to%20enhance%20the%20model%27s%20discriminative%20understanding.%202%29%20the%0ADiverse%20Instruction-following%20data%2C%20which%20contains%20various%20instruction%20styles%0Ato%20enhance%20model%27s%20generalization.%20As%20a%20result%2C%20we%20construct%201%20million%0Ainstruction-following%20data%2C%20consisting%20of%20344K%20Adversarial%20samples%2C%20508K%0ADiverse%20samples%2C%20and%20165K%20benchmark%20training%20set%20samples.%20To%20better%20handle%0Athese%20complex%20instructions%2C%20Robin3D%20first%20incorporates%20Relation-Augmented%0AProjector%20to%20enhance%20spatial%20understanding%2C%20and%20then%20strengthens%20the%20object%0Areferring%20and%20grounding%20ability%20through%20ID-Feature%20Bonding.%20Robin3D%0Aconsistently%20outperforms%20previous%20methods%20across%20five%20widely-used%203D%20multimodal%0Alearning%20benchmarks%2C%20without%20the%20need%20for%20task-specific%20fine-tuning.%20Notably%2C%0Awe%20achieve%20a%207.8%5C%25%20improvement%20in%20the%20grounding%20task%20%28Multi3DRefer%29%20and%20a%206.9%5C%25%0Aimprovement%20in%20the%20captioning%20task%20%28Scan2Cap%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00255v2&entry.124074799=Read"},
{"title": "Vision Foundation Models in Medical Image Analysis: Advances and\n  Challenges", "author": "Pengchen Liang and Bin Pu and Haishan Huang and Yiwei Li and Hualiang Wang and Weibo Ma and Qing Chang", "abstract": "  The rapid development of Vision Foundation Models (VFMs), particularly Vision\nTransformers (ViT) and Segment Anything Model (SAM), has sparked significant\nadvances in the field of medical image analysis. These models have demonstrated\nexceptional capabilities in capturing long-range dependencies and achieving\nhigh generalization in segmentation tasks. However, adapting these large models\nto medical image analysis presents several challenges, including domain\ndifferences between medical and natural images, the need for efficient model\nadaptation strategies, and the limitations of small-scale medical datasets.\nThis paper reviews the state-of-the-art research on the adaptation of VFMs to\nmedical image segmentation, focusing on the challenges of domain adaptation,\nmodel compression, and federated learning. We discuss the latest developments\nin adapter-based improvements, knowledge distillation techniques, and\nmulti-scale contextual feature modeling, and propose future directions to\novercome these bottlenecks. Our analysis highlights the potential of VFMs,\nalong with emerging methodologies such as federated learning and model\ncompression, to revolutionize medical image analysis and enhance clinical\napplications. The goal of this work is to provide a comprehensive overview of\ncurrent approaches and suggest key areas for future research that can drive the\nnext wave of innovation in medical image segmentation.\n", "link": "http://arxiv.org/abs/2502.14584v1", "date": "2025-02-20", "relevancy": 2.9241, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Foundation%20Models%20in%20Medical%20Image%20Analysis%3A%20Advances%20and%0A%20%20Challenges&body=Title%3A%20Vision%20Foundation%20Models%20in%20Medical%20Image%20Analysis%3A%20Advances%20and%0A%20%20Challenges%0AAuthor%3A%20Pengchen%20Liang%20and%20Bin%20Pu%20and%20Haishan%20Huang%20and%20Yiwei%20Li%20and%20Hualiang%20Wang%20and%20Weibo%20Ma%20and%20Qing%20Chang%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20Vision%20Foundation%20Models%20%28VFMs%29%2C%20particularly%20Vision%0ATransformers%20%28ViT%29%20and%20Segment%20Anything%20Model%20%28SAM%29%2C%20has%20sparked%20significant%0Aadvances%20in%20the%20field%20of%20medical%20image%20analysis.%20These%20models%20have%20demonstrated%0Aexceptional%20capabilities%20in%20capturing%20long-range%20dependencies%20and%20achieving%0Ahigh%20generalization%20in%20segmentation%20tasks.%20However%2C%20adapting%20these%20large%20models%0Ato%20medical%20image%20analysis%20presents%20several%20challenges%2C%20including%20domain%0Adifferences%20between%20medical%20and%20natural%20images%2C%20the%20need%20for%20efficient%20model%0Aadaptation%20strategies%2C%20and%20the%20limitations%20of%20small-scale%20medical%20datasets.%0AThis%20paper%20reviews%20the%20state-of-the-art%20research%20on%20the%20adaptation%20of%20VFMs%20to%0Amedical%20image%20segmentation%2C%20focusing%20on%20the%20challenges%20of%20domain%20adaptation%2C%0Amodel%20compression%2C%20and%20federated%20learning.%20We%20discuss%20the%20latest%20developments%0Ain%20adapter-based%20improvements%2C%20knowledge%20distillation%20techniques%2C%20and%0Amulti-scale%20contextual%20feature%20modeling%2C%20and%20propose%20future%20directions%20to%0Aovercome%20these%20bottlenecks.%20Our%20analysis%20highlights%20the%20potential%20of%20VFMs%2C%0Aalong%20with%20emerging%20methodologies%20such%20as%20federated%20learning%20and%20model%0Acompression%2C%20to%20revolutionize%20medical%20image%20analysis%20and%20enhance%20clinical%0Aapplications.%20The%20goal%20of%20this%20work%20is%20to%20provide%20a%20comprehensive%20overview%20of%0Acurrent%20approaches%20and%20suggest%20key%20areas%20for%20future%20research%20that%20can%20drive%20the%0Anext%20wave%20of%20innovation%20in%20medical%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Foundation%2520Models%2520in%2520Medical%2520Image%2520Analysis%253A%2520Advances%2520and%250A%2520%2520Challenges%26entry.906535625%3DPengchen%2520Liang%2520and%2520Bin%2520Pu%2520and%2520Haishan%2520Huang%2520and%2520Yiwei%2520Li%2520and%2520Hualiang%2520Wang%2520and%2520Weibo%2520Ma%2520and%2520Qing%2520Chang%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%252C%2520particularly%2520Vision%250ATransformers%2520%2528ViT%2529%2520and%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520has%2520sparked%2520significant%250Aadvances%2520in%2520the%2520field%2520of%2520medical%2520image%2520analysis.%2520These%2520models%2520have%2520demonstrated%250Aexceptional%2520capabilities%2520in%2520capturing%2520long-range%2520dependencies%2520and%2520achieving%250Ahigh%2520generalization%2520in%2520segmentation%2520tasks.%2520However%252C%2520adapting%2520these%2520large%2520models%250Ato%2520medical%2520image%2520analysis%2520presents%2520several%2520challenges%252C%2520including%2520domain%250Adifferences%2520between%2520medical%2520and%2520natural%2520images%252C%2520the%2520need%2520for%2520efficient%2520model%250Aadaptation%2520strategies%252C%2520and%2520the%2520limitations%2520of%2520small-scale%2520medical%2520datasets.%250AThis%2520paper%2520reviews%2520the%2520state-of-the-art%2520research%2520on%2520the%2520adaptation%2520of%2520VFMs%2520to%250Amedical%2520image%2520segmentation%252C%2520focusing%2520on%2520the%2520challenges%2520of%2520domain%2520adaptation%252C%250Amodel%2520compression%252C%2520and%2520federated%2520learning.%2520We%2520discuss%2520the%2520latest%2520developments%250Ain%2520adapter-based%2520improvements%252C%2520knowledge%2520distillation%2520techniques%252C%2520and%250Amulti-scale%2520contextual%2520feature%2520modeling%252C%2520and%2520propose%2520future%2520directions%2520to%250Aovercome%2520these%2520bottlenecks.%2520Our%2520analysis%2520highlights%2520the%2520potential%2520of%2520VFMs%252C%250Aalong%2520with%2520emerging%2520methodologies%2520such%2520as%2520federated%2520learning%2520and%2520model%250Acompression%252C%2520to%2520revolutionize%2520medical%2520image%2520analysis%2520and%2520enhance%2520clinical%250Aapplications.%2520The%2520goal%2520of%2520this%2520work%2520is%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%250Acurrent%2520approaches%2520and%2520suggest%2520key%2520areas%2520for%2520future%2520research%2520that%2520can%2520drive%2520the%250Anext%2520wave%2520of%2520innovation%2520in%2520medical%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Foundation%20Models%20in%20Medical%20Image%20Analysis%3A%20Advances%20and%0A%20%20Challenges&entry.906535625=Pengchen%20Liang%20and%20Bin%20Pu%20and%20Haishan%20Huang%20and%20Yiwei%20Li%20and%20Hualiang%20Wang%20and%20Weibo%20Ma%20and%20Qing%20Chang&entry.1292438233=%20%20The%20rapid%20development%20of%20Vision%20Foundation%20Models%20%28VFMs%29%2C%20particularly%20Vision%0ATransformers%20%28ViT%29%20and%20Segment%20Anything%20Model%20%28SAM%29%2C%20has%20sparked%20significant%0Aadvances%20in%20the%20field%20of%20medical%20image%20analysis.%20These%20models%20have%20demonstrated%0Aexceptional%20capabilities%20in%20capturing%20long-range%20dependencies%20and%20achieving%0Ahigh%20generalization%20in%20segmentation%20tasks.%20However%2C%20adapting%20these%20large%20models%0Ato%20medical%20image%20analysis%20presents%20several%20challenges%2C%20including%20domain%0Adifferences%20between%20medical%20and%20natural%20images%2C%20the%20need%20for%20efficient%20model%0Aadaptation%20strategies%2C%20and%20the%20limitations%20of%20small-scale%20medical%20datasets.%0AThis%20paper%20reviews%20the%20state-of-the-art%20research%20on%20the%20adaptation%20of%20VFMs%20to%0Amedical%20image%20segmentation%2C%20focusing%20on%20the%20challenges%20of%20domain%20adaptation%2C%0Amodel%20compression%2C%20and%20federated%20learning.%20We%20discuss%20the%20latest%20developments%0Ain%20adapter-based%20improvements%2C%20knowledge%20distillation%20techniques%2C%20and%0Amulti-scale%20contextual%20feature%20modeling%2C%20and%20propose%20future%20directions%20to%0Aovercome%20these%20bottlenecks.%20Our%20analysis%20highlights%20the%20potential%20of%20VFMs%2C%0Aalong%20with%20emerging%20methodologies%20such%20as%20federated%20learning%20and%20model%0Acompression%2C%20to%20revolutionize%20medical%20image%20analysis%20and%20enhance%20clinical%0Aapplications.%20The%20goal%20of%20this%20work%20is%20to%20provide%20a%20comprehensive%20overview%20of%0Acurrent%20approaches%20and%20suggest%20key%20areas%20for%20future%20research%20that%20can%20drive%20the%0Anext%20wave%20of%20innovation%20in%20medical%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14584v1&entry.124074799=Read"},
{"title": "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes", "author": "Lukas Rauch and Thomas Braml", "abstract": "  The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.\n", "link": "http://arxiv.org/abs/2502.14721v1", "date": "2025-02-20", "relevancy": 2.8586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-dataset%20synergistic%20in%20supervised%20learning%20to%20pre-label%20structural%0A%20%20components%20in%20point%20clouds%20from%20shell%20construction%20scenes&body=Title%3A%20Multi-dataset%20synergistic%20in%20supervised%20learning%20to%20pre-label%20structural%0A%20%20components%20in%20point%20clouds%20from%20shell%20construction%20scenes%0AAuthor%3A%20Lukas%20Rauch%20and%20Thomas%20Braml%0AAbstract%3A%20%20%20The%20significant%20effort%20required%20to%20annotate%20data%20for%20new%20training%20datasets%0Ahinders%20computer%20vision%20research%20and%20machine%20learning%20in%20the%20construction%0Aindustry.%20This%20work%20explores%20adapting%20standard%20datasets%20and%20the%20latest%0Atransformer%20model%20architectures%20for%20point%20cloud%20semantic%20segmentation%20in%20the%0Acontext%20of%20shell%20construction%20sites.%20Unlike%20common%20approaches%20focused%20on%20object%0Asegmentation%20of%20building%20interiors%20and%20furniture%2C%20this%20study%20addressed%20the%0Achallenges%20of%20segmenting%20complex%20structural%20components%20in%20Architecture%2C%0AEngineering%2C%20and%20Construction%20%28AEC%29.%20We%20establish%20a%20baseline%20through%20supervised%0Atraining%20and%20a%20custom%20validation%20dataset%2C%20evaluate%20the%20cross-domain%20inference%0Awith%20large-scale%20indoor%20datasets%2C%20and%20utilize%20transfer%20learning%20to%20maximize%0Asegmentation%20performance%20with%20minimal%20new%20data.%20The%20findings%20indicate%20that%20with%0Aminimal%20fine-tuning%2C%20pre-trained%20transformer%20architectures%20offer%20an%20effective%0Astrategy%20for%20building%20component%20segmentation.%20Our%20results%20are%20promising%20for%0Aautomating%20the%20annotation%20of%20new%2C%20previously%20unseen%20data%20when%20creating%20larger%0Atraining%20resources%20and%20for%20the%20segmentation%20of%20frequently%20recurring%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-dataset%2520synergistic%2520in%2520supervised%2520learning%2520to%2520pre-label%2520structural%250A%2520%2520components%2520in%2520point%2520clouds%2520from%2520shell%2520construction%2520scenes%26entry.906535625%3DLukas%2520Rauch%2520and%2520Thomas%2520Braml%26entry.1292438233%3D%2520%2520The%2520significant%2520effort%2520required%2520to%2520annotate%2520data%2520for%2520new%2520training%2520datasets%250Ahinders%2520computer%2520vision%2520research%2520and%2520machine%2520learning%2520in%2520the%2520construction%250Aindustry.%2520This%2520work%2520explores%2520adapting%2520standard%2520datasets%2520and%2520the%2520latest%250Atransformer%2520model%2520architectures%2520for%2520point%2520cloud%2520semantic%2520segmentation%2520in%2520the%250Acontext%2520of%2520shell%2520construction%2520sites.%2520Unlike%2520common%2520approaches%2520focused%2520on%2520object%250Asegmentation%2520of%2520building%2520interiors%2520and%2520furniture%252C%2520this%2520study%2520addressed%2520the%250Achallenges%2520of%2520segmenting%2520complex%2520structural%2520components%2520in%2520Architecture%252C%250AEngineering%252C%2520and%2520Construction%2520%2528AEC%2529.%2520We%2520establish%2520a%2520baseline%2520through%2520supervised%250Atraining%2520and%2520a%2520custom%2520validation%2520dataset%252C%2520evaluate%2520the%2520cross-domain%2520inference%250Awith%2520large-scale%2520indoor%2520datasets%252C%2520and%2520utilize%2520transfer%2520learning%2520to%2520maximize%250Asegmentation%2520performance%2520with%2520minimal%2520new%2520data.%2520The%2520findings%2520indicate%2520that%2520with%250Aminimal%2520fine-tuning%252C%2520pre-trained%2520transformer%2520architectures%2520offer%2520an%2520effective%250Astrategy%2520for%2520building%2520component%2520segmentation.%2520Our%2520results%2520are%2520promising%2520for%250Aautomating%2520the%2520annotation%2520of%2520new%252C%2520previously%2520unseen%2520data%2520when%2520creating%2520larger%250Atraining%2520resources%2520and%2520for%2520the%2520segmentation%2520of%2520frequently%2520recurring%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-dataset%20synergistic%20in%20supervised%20learning%20to%20pre-label%20structural%0A%20%20components%20in%20point%20clouds%20from%20shell%20construction%20scenes&entry.906535625=Lukas%20Rauch%20and%20Thomas%20Braml&entry.1292438233=%20%20The%20significant%20effort%20required%20to%20annotate%20data%20for%20new%20training%20datasets%0Ahinders%20computer%20vision%20research%20and%20machine%20learning%20in%20the%20construction%0Aindustry.%20This%20work%20explores%20adapting%20standard%20datasets%20and%20the%20latest%0Atransformer%20model%20architectures%20for%20point%20cloud%20semantic%20segmentation%20in%20the%0Acontext%20of%20shell%20construction%20sites.%20Unlike%20common%20approaches%20focused%20on%20object%0Asegmentation%20of%20building%20interiors%20and%20furniture%2C%20this%20study%20addressed%20the%0Achallenges%20of%20segmenting%20complex%20structural%20components%20in%20Architecture%2C%0AEngineering%2C%20and%20Construction%20%28AEC%29.%20We%20establish%20a%20baseline%20through%20supervised%0Atraining%20and%20a%20custom%20validation%20dataset%2C%20evaluate%20the%20cross-domain%20inference%0Awith%20large-scale%20indoor%20datasets%2C%20and%20utilize%20transfer%20learning%20to%20maximize%0Asegmentation%20performance%20with%20minimal%20new%20data.%20The%20findings%20indicate%20that%20with%0Aminimal%20fine-tuning%2C%20pre-trained%20transformer%20architectures%20offer%20an%20effective%0Astrategy%20for%20building%20component%20segmentation.%20Our%20results%20are%20promising%20for%0Aautomating%20the%20annotation%20of%20new%2C%20previously%20unseen%20data%20when%20creating%20larger%0Atraining%20resources%20and%20for%20the%20segmentation%20of%20frequently%20recurring%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14721v1&entry.124074799=Read"},
{"title": "FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image\n  Analysis", "author": "Fadillah Maani and Numan Saeed and Tausifa Saleem and Zaid Farooq and Hussain Alasmawi and Werner Diehl and Ameera Mohammad and Gareth Waring and Saudabi Valappi and Leanne Bricker and Mohammad Yaqub", "abstract": "  Foundation models are becoming increasingly effective in the medical domain,\noffering pre-trained models on large datasets that can be readily adapted for\ndownstream tasks. Despite progress, fetal ultrasound images remain a\nchallenging domain for foundation models due to their inherent complexity,\noften requiring substantial additional training and facing limitations due to\nthe scarcity of paired multimodal data. To overcome these challenges, here we\nintroduce FetalCLIP, a vision-language foundation model capable of generating\nuniversal representation of fetal ultrasound images. FetalCLIP was pre-trained\nusing a multimodal learning approach on a diverse dataset of 210,035 fetal\nultrasound images paired with text. This represents the largest paired dataset\nof its kind used for foundation model development to date. This unique training\napproach allows FetalCLIP to effectively learn the intricate anatomical\nfeatures present in fetal ultrasound images, resulting in robust\nrepresentations that can be used for a variety of downstream applications. In\nextensive benchmarking across a range of key fetal ultrasound applications,\nincluding classification, gestational age estimation, congenital heart defect\n(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all\nbaselines while demonstrating remarkable generalizability and strong\nperformance even with limited labeled data. We plan to release the FetalCLIP\nmodel publicly for the benefit of the broader scientific community.\n", "link": "http://arxiv.org/abs/2502.14807v1", "date": "2025-02-20", "relevancy": 2.8559, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5763}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FetalCLIP%3A%20A%20Visual-Language%20Foundation%20Model%20for%20Fetal%20Ultrasound%20Image%0A%20%20Analysis&body=Title%3A%20FetalCLIP%3A%20A%20Visual-Language%20Foundation%20Model%20for%20Fetal%20Ultrasound%20Image%0A%20%20Analysis%0AAuthor%3A%20Fadillah%20Maani%20and%20Numan%20Saeed%20and%20Tausifa%20Saleem%20and%20Zaid%20Farooq%20and%20Hussain%20Alasmawi%20and%20Werner%20Diehl%20and%20Ameera%20Mohammad%20and%20Gareth%20Waring%20and%20Saudabi%20Valappi%20and%20Leanne%20Bricker%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Foundation%20models%20are%20becoming%20increasingly%20effective%20in%20the%20medical%20domain%2C%0Aoffering%20pre-trained%20models%20on%20large%20datasets%20that%20can%20be%20readily%20adapted%20for%0Adownstream%20tasks.%20Despite%20progress%2C%20fetal%20ultrasound%20images%20remain%20a%0Achallenging%20domain%20for%20foundation%20models%20due%20to%20their%20inherent%20complexity%2C%0Aoften%20requiring%20substantial%20additional%20training%20and%20facing%20limitations%20due%20to%0Athe%20scarcity%20of%20paired%20multimodal%20data.%20To%20overcome%20these%20challenges%2C%20here%20we%0Aintroduce%20FetalCLIP%2C%20a%20vision-language%20foundation%20model%20capable%20of%20generating%0Auniversal%20representation%20of%20fetal%20ultrasound%20images.%20FetalCLIP%20was%20pre-trained%0Ausing%20a%20multimodal%20learning%20approach%20on%20a%20diverse%20dataset%20of%20210%2C035%20fetal%0Aultrasound%20images%20paired%20with%20text.%20This%20represents%20the%20largest%20paired%20dataset%0Aof%20its%20kind%20used%20for%20foundation%20model%20development%20to%20date.%20This%20unique%20training%0Aapproach%20allows%20FetalCLIP%20to%20effectively%20learn%20the%20intricate%20anatomical%0Afeatures%20present%20in%20fetal%20ultrasound%20images%2C%20resulting%20in%20robust%0Arepresentations%20that%20can%20be%20used%20for%20a%20variety%20of%20downstream%20applications.%20In%0Aextensive%20benchmarking%20across%20a%20range%20of%20key%20fetal%20ultrasound%20applications%2C%0Aincluding%20classification%2C%20gestational%20age%20estimation%2C%20congenital%20heart%20defect%0A%28CHD%29%20detection%2C%20and%20fetal%20structure%20segmentation%2C%20FetalCLIP%20outperformed%20all%0Abaselines%20while%20demonstrating%20remarkable%20generalizability%20and%20strong%0Aperformance%20even%20with%20limited%20labeled%20data.%20We%20plan%20to%20release%20the%20FetalCLIP%0Amodel%20publicly%20for%20the%20benefit%20of%20the%20broader%20scientific%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFetalCLIP%253A%2520A%2520Visual-Language%2520Foundation%2520Model%2520for%2520Fetal%2520Ultrasound%2520Image%250A%2520%2520Analysis%26entry.906535625%3DFadillah%2520Maani%2520and%2520Numan%2520Saeed%2520and%2520Tausifa%2520Saleem%2520and%2520Zaid%2520Farooq%2520and%2520Hussain%2520Alasmawi%2520and%2520Werner%2520Diehl%2520and%2520Ameera%2520Mohammad%2520and%2520Gareth%2520Waring%2520and%2520Saudabi%2520Valappi%2520and%2520Leanne%2520Bricker%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520becoming%2520increasingly%2520effective%2520in%2520the%2520medical%2520domain%252C%250Aoffering%2520pre-trained%2520models%2520on%2520large%2520datasets%2520that%2520can%2520be%2520readily%2520adapted%2520for%250Adownstream%2520tasks.%2520Despite%2520progress%252C%2520fetal%2520ultrasound%2520images%2520remain%2520a%250Achallenging%2520domain%2520for%2520foundation%2520models%2520due%2520to%2520their%2520inherent%2520complexity%252C%250Aoften%2520requiring%2520substantial%2520additional%2520training%2520and%2520facing%2520limitations%2520due%2520to%250Athe%2520scarcity%2520of%2520paired%2520multimodal%2520data.%2520To%2520overcome%2520these%2520challenges%252C%2520here%2520we%250Aintroduce%2520FetalCLIP%252C%2520a%2520vision-language%2520foundation%2520model%2520capable%2520of%2520generating%250Auniversal%2520representation%2520of%2520fetal%2520ultrasound%2520images.%2520FetalCLIP%2520was%2520pre-trained%250Ausing%2520a%2520multimodal%2520learning%2520approach%2520on%2520a%2520diverse%2520dataset%2520of%2520210%252C035%2520fetal%250Aultrasound%2520images%2520paired%2520with%2520text.%2520This%2520represents%2520the%2520largest%2520paired%2520dataset%250Aof%2520its%2520kind%2520used%2520for%2520foundation%2520model%2520development%2520to%2520date.%2520This%2520unique%2520training%250Aapproach%2520allows%2520FetalCLIP%2520to%2520effectively%2520learn%2520the%2520intricate%2520anatomical%250Afeatures%2520present%2520in%2520fetal%2520ultrasound%2520images%252C%2520resulting%2520in%2520robust%250Arepresentations%2520that%2520can%2520be%2520used%2520for%2520a%2520variety%2520of%2520downstream%2520applications.%2520In%250Aextensive%2520benchmarking%2520across%2520a%2520range%2520of%2520key%2520fetal%2520ultrasound%2520applications%252C%250Aincluding%2520classification%252C%2520gestational%2520age%2520estimation%252C%2520congenital%2520heart%2520defect%250A%2528CHD%2529%2520detection%252C%2520and%2520fetal%2520structure%2520segmentation%252C%2520FetalCLIP%2520outperformed%2520all%250Abaselines%2520while%2520demonstrating%2520remarkable%2520generalizability%2520and%2520strong%250Aperformance%2520even%2520with%2520limited%2520labeled%2520data.%2520We%2520plan%2520to%2520release%2520the%2520FetalCLIP%250Amodel%2520publicly%2520for%2520the%2520benefit%2520of%2520the%2520broader%2520scientific%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FetalCLIP%3A%20A%20Visual-Language%20Foundation%20Model%20for%20Fetal%20Ultrasound%20Image%0A%20%20Analysis&entry.906535625=Fadillah%20Maani%20and%20Numan%20Saeed%20and%20Tausifa%20Saleem%20and%20Zaid%20Farooq%20and%20Hussain%20Alasmawi%20and%20Werner%20Diehl%20and%20Ameera%20Mohammad%20and%20Gareth%20Waring%20and%20Saudabi%20Valappi%20and%20Leanne%20Bricker%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Foundation%20models%20are%20becoming%20increasingly%20effective%20in%20the%20medical%20domain%2C%0Aoffering%20pre-trained%20models%20on%20large%20datasets%20that%20can%20be%20readily%20adapted%20for%0Adownstream%20tasks.%20Despite%20progress%2C%20fetal%20ultrasound%20images%20remain%20a%0Achallenging%20domain%20for%20foundation%20models%20due%20to%20their%20inherent%20complexity%2C%0Aoften%20requiring%20substantial%20additional%20training%20and%20facing%20limitations%20due%20to%0Athe%20scarcity%20of%20paired%20multimodal%20data.%20To%20overcome%20these%20challenges%2C%20here%20we%0Aintroduce%20FetalCLIP%2C%20a%20vision-language%20foundation%20model%20capable%20of%20generating%0Auniversal%20representation%20of%20fetal%20ultrasound%20images.%20FetalCLIP%20was%20pre-trained%0Ausing%20a%20multimodal%20learning%20approach%20on%20a%20diverse%20dataset%20of%20210%2C035%20fetal%0Aultrasound%20images%20paired%20with%20text.%20This%20represents%20the%20largest%20paired%20dataset%0Aof%20its%20kind%20used%20for%20foundation%20model%20development%20to%20date.%20This%20unique%20training%0Aapproach%20allows%20FetalCLIP%20to%20effectively%20learn%20the%20intricate%20anatomical%0Afeatures%20present%20in%20fetal%20ultrasound%20images%2C%20resulting%20in%20robust%0Arepresentations%20that%20can%20be%20used%20for%20a%20variety%20of%20downstream%20applications.%20In%0Aextensive%20benchmarking%20across%20a%20range%20of%20key%20fetal%20ultrasound%20applications%2C%0Aincluding%20classification%2C%20gestational%20age%20estimation%2C%20congenital%20heart%20defect%0A%28CHD%29%20detection%2C%20and%20fetal%20structure%20segmentation%2C%20FetalCLIP%20outperformed%20all%0Abaselines%20while%20demonstrating%20remarkable%20generalizability%20and%20strong%0Aperformance%20even%20with%20limited%20labeled%20data.%20We%20plan%20to%20release%20the%20FetalCLIP%0Amodel%20publicly%20for%20the%20benefit%20of%20the%20broader%20scientific%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14807v1&entry.124074799=Read"},
{"title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features", "author": "Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier H\u00e9naff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai", "abstract": "  We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).\n", "link": "http://arxiv.org/abs/2502.14786v1", "date": "2025-02-20", "relevancy": 2.8479, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SigLIP%202%3A%20Multilingual%20Vision-Language%20Encoders%20with%20Improved%20Semantic%0A%20%20Understanding%2C%20Localization%2C%20and%20Dense%20Features&body=Title%3A%20SigLIP%202%3A%20Multilingual%20Vision-Language%20Encoders%20with%20Improved%20Semantic%0A%20%20Understanding%2C%20Localization%2C%20and%20Dense%20Features%0AAuthor%3A%20Michael%20Tschannen%20and%20Alexey%20Gritsenko%20and%20Xiao%20Wang%20and%20Muhammad%20Ferjad%20Naeem%20and%20Ibrahim%20Alabdulmohsin%20and%20Nikhil%20Parthasarathy%20and%20Talfan%20Evans%20and%20Lucas%20Beyer%20and%20Ye%20Xia%20and%20Basil%20Mustafa%20and%20Olivier%20H%C3%A9naff%20and%20Jeremiah%20Harmsen%20and%20Andreas%20Steiner%20and%20Xiaohua%20Zhai%0AAbstract%3A%20%20%20We%20introduce%20SigLIP%202%2C%20a%20family%20of%20new%20multilingual%20vision-language%20encoders%0Athat%20build%20on%20the%20success%20of%20the%20original%20SigLIP.%20In%20this%20second%20iteration%2C%20we%0Aextend%20the%20original%20image-text%20training%20objective%20with%20several%20prior%2C%0Aindependently%20developed%20techniques%20into%20a%20unified%20recipe%20--%20this%20includes%0Acaptioning-based%20pretraining%2C%20self-supervised%20losses%20%28self-distillation%2C%20masked%0Aprediction%29%20and%20online%20data%20curation.%20With%20these%20changes%2C%20SigLIP%202%20models%0Aoutperform%20their%20SigLIP%20counterparts%20at%20all%20model%20scales%20in%20core%20capabilities%2C%0Aincluding%20zero-shot%20classification%2C%20image-text%20retrieval%2C%20and%20transfer%0Aperformance%20when%20extracting%20visual%20representations%20for%20Vision-Language%20Models%0A%28VLMs%29.%20Furthermore%2C%20the%20new%20training%20recipe%20leads%20to%20significant%20improvements%0Aon%20localization%20and%20dense%20prediction%20tasks.%20We%20also%20train%20variants%20which%0Asupport%20multiple%20resolutions%20and%20preserve%20the%20input%27s%20native%20aspect%20ratio.%0AFinally%2C%20we%20train%20on%20a%20more%20diverse%20data-mixture%20that%20includes%20de-biasing%0Atechniques%2C%20leading%20to%20much%20better%20multilingual%20understanding%20and%20improved%0Afairness.%20To%20allow%20users%20to%20trade%20off%20inference%20cost%20with%20performance%2C%20we%0Arelease%20model%20checkpoints%20at%20four%20sizes%3A%20ViT-B%20%2886M%29%2C%20L%20%28303M%29%2C%20So400m%20%28400M%29%2C%0Aand%20g%20%281B%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigLIP%25202%253A%2520Multilingual%2520Vision-Language%2520Encoders%2520with%2520Improved%2520Semantic%250A%2520%2520Understanding%252C%2520Localization%252C%2520and%2520Dense%2520Features%26entry.906535625%3DMichael%2520Tschannen%2520and%2520Alexey%2520Gritsenko%2520and%2520Xiao%2520Wang%2520and%2520Muhammad%2520Ferjad%2520Naeem%2520and%2520Ibrahim%2520Alabdulmohsin%2520and%2520Nikhil%2520Parthasarathy%2520and%2520Talfan%2520Evans%2520and%2520Lucas%2520Beyer%2520and%2520Ye%2520Xia%2520and%2520Basil%2520Mustafa%2520and%2520Olivier%2520H%25C3%25A9naff%2520and%2520Jeremiah%2520Harmsen%2520and%2520Andreas%2520Steiner%2520and%2520Xiaohua%2520Zhai%26entry.1292438233%3D%2520%2520We%2520introduce%2520SigLIP%25202%252C%2520a%2520family%2520of%2520new%2520multilingual%2520vision-language%2520encoders%250Athat%2520build%2520on%2520the%2520success%2520of%2520the%2520original%2520SigLIP.%2520In%2520this%2520second%2520iteration%252C%2520we%250Aextend%2520the%2520original%2520image-text%2520training%2520objective%2520with%2520several%2520prior%252C%250Aindependently%2520developed%2520techniques%2520into%2520a%2520unified%2520recipe%2520--%2520this%2520includes%250Acaptioning-based%2520pretraining%252C%2520self-supervised%2520losses%2520%2528self-distillation%252C%2520masked%250Aprediction%2529%2520and%2520online%2520data%2520curation.%2520With%2520these%2520changes%252C%2520SigLIP%25202%2520models%250Aoutperform%2520their%2520SigLIP%2520counterparts%2520at%2520all%2520model%2520scales%2520in%2520core%2520capabilities%252C%250Aincluding%2520zero-shot%2520classification%252C%2520image-text%2520retrieval%252C%2520and%2520transfer%250Aperformance%2520when%2520extracting%2520visual%2520representations%2520for%2520Vision-Language%2520Models%250A%2528VLMs%2529.%2520Furthermore%252C%2520the%2520new%2520training%2520recipe%2520leads%2520to%2520significant%2520improvements%250Aon%2520localization%2520and%2520dense%2520prediction%2520tasks.%2520We%2520also%2520train%2520variants%2520which%250Asupport%2520multiple%2520resolutions%2520and%2520preserve%2520the%2520input%2527s%2520native%2520aspect%2520ratio.%250AFinally%252C%2520we%2520train%2520on%2520a%2520more%2520diverse%2520data-mixture%2520that%2520includes%2520de-biasing%250Atechniques%252C%2520leading%2520to%2520much%2520better%2520multilingual%2520understanding%2520and%2520improved%250Afairness.%2520To%2520allow%2520users%2520to%2520trade%2520off%2520inference%2520cost%2520with%2520performance%252C%2520we%250Arelease%2520model%2520checkpoints%2520at%2520four%2520sizes%253A%2520ViT-B%2520%252886M%2529%252C%2520L%2520%2528303M%2529%252C%2520So400m%2520%2528400M%2529%252C%250Aand%2520g%2520%25281B%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SigLIP%202%3A%20Multilingual%20Vision-Language%20Encoders%20with%20Improved%20Semantic%0A%20%20Understanding%2C%20Localization%2C%20and%20Dense%20Features&entry.906535625=Michael%20Tschannen%20and%20Alexey%20Gritsenko%20and%20Xiao%20Wang%20and%20Muhammad%20Ferjad%20Naeem%20and%20Ibrahim%20Alabdulmohsin%20and%20Nikhil%20Parthasarathy%20and%20Talfan%20Evans%20and%20Lucas%20Beyer%20and%20Ye%20Xia%20and%20Basil%20Mustafa%20and%20Olivier%20H%C3%A9naff%20and%20Jeremiah%20Harmsen%20and%20Andreas%20Steiner%20and%20Xiaohua%20Zhai&entry.1292438233=%20%20We%20introduce%20SigLIP%202%2C%20a%20family%20of%20new%20multilingual%20vision-language%20encoders%0Athat%20build%20on%20the%20success%20of%20the%20original%20SigLIP.%20In%20this%20second%20iteration%2C%20we%0Aextend%20the%20original%20image-text%20training%20objective%20with%20several%20prior%2C%0Aindependently%20developed%20techniques%20into%20a%20unified%20recipe%20--%20this%20includes%0Acaptioning-based%20pretraining%2C%20self-supervised%20losses%20%28self-distillation%2C%20masked%0Aprediction%29%20and%20online%20data%20curation.%20With%20these%20changes%2C%20SigLIP%202%20models%0Aoutperform%20their%20SigLIP%20counterparts%20at%20all%20model%20scales%20in%20core%20capabilities%2C%0Aincluding%20zero-shot%20classification%2C%20image-text%20retrieval%2C%20and%20transfer%0Aperformance%20when%20extracting%20visual%20representations%20for%20Vision-Language%20Models%0A%28VLMs%29.%20Furthermore%2C%20the%20new%20training%20recipe%20leads%20to%20significant%20improvements%0Aon%20localization%20and%20dense%20prediction%20tasks.%20We%20also%20train%20variants%20which%0Asupport%20multiple%20resolutions%20and%20preserve%20the%20input%27s%20native%20aspect%20ratio.%0AFinally%2C%20we%20train%20on%20a%20more%20diverse%20data-mixture%20that%20includes%20de-biasing%0Atechniques%2C%20leading%20to%20much%20better%20multilingual%20understanding%20and%20improved%0Afairness.%20To%20allow%20users%20to%20trade%20off%20inference%20cost%20with%20performance%2C%20we%0Arelease%20model%20checkpoints%20at%20four%20sizes%3A%20ViT-B%20%2886M%29%2C%20L%20%28303M%29%2C%20So400m%20%28400M%29%2C%0Aand%20g%20%281B%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14786v1&entry.124074799=Read"},
{"title": "NAVIG: Natural Language-guided Analysis with Vision Language Models for\n  Image Geo-localization", "author": "Zheyuan Zhang and Runze Li and Tasnim Kabir and Jordan Boyd-Graber", "abstract": "  Image geo-localization is the task of predicting the specific location of an\nimage and requires complex reasoning across visual, geographical, and cultural\ncontexts. While prior Vision Language Models (VLMs) have the best accuracy at\nthis task, there is a dearth of high-quality datasets and models for analytical\nreasoning. We first create NaviClues, a high-quality dataset derived from\nGeoGuessr, a popular geography game, to supply examples of expert reasoning\nfrom language. Using this dataset, we present Navig, a comprehensive image\ngeo-localization framework integrating global and fine-grained image\ninformation. By reasoning with language, Navig reduces the average distance\nerror by 14% compared to previous state-of-the-art models while requiring fewer\nthan 1000 training samples. Our dataset and code are available at\nhttps://github.com/SparrowZheyuan18/Navig/.\n", "link": "http://arxiv.org/abs/2502.14638v1", "date": "2025-02-20", "relevancy": 2.8353, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NAVIG%3A%20Natural%20Language-guided%20Analysis%20with%20Vision%20Language%20Models%20for%0A%20%20Image%20Geo-localization&body=Title%3A%20NAVIG%3A%20Natural%20Language-guided%20Analysis%20with%20Vision%20Language%20Models%20for%0A%20%20Image%20Geo-localization%0AAuthor%3A%20Zheyuan%20Zhang%20and%20Runze%20Li%20and%20Tasnim%20Kabir%20and%20Jordan%20Boyd-Graber%0AAbstract%3A%20%20%20Image%20geo-localization%20is%20the%20task%20of%20predicting%20the%20specific%20location%20of%20an%0Aimage%20and%20requires%20complex%20reasoning%20across%20visual%2C%20geographical%2C%20and%20cultural%0Acontexts.%20While%20prior%20Vision%20Language%20Models%20%28VLMs%29%20have%20the%20best%20accuracy%20at%0Athis%20task%2C%20there%20is%20a%20dearth%20of%20high-quality%20datasets%20and%20models%20for%20analytical%0Areasoning.%20We%20first%20create%20NaviClues%2C%20a%20high-quality%20dataset%20derived%20from%0AGeoGuessr%2C%20a%20popular%20geography%20game%2C%20to%20supply%20examples%20of%20expert%20reasoning%0Afrom%20language.%20Using%20this%20dataset%2C%20we%20present%20Navig%2C%20a%20comprehensive%20image%0Ageo-localization%20framework%20integrating%20global%20and%20fine-grained%20image%0Ainformation.%20By%20reasoning%20with%20language%2C%20Navig%20reduces%20the%20average%20distance%0Aerror%20by%2014%25%20compared%20to%20previous%20state-of-the-art%20models%20while%20requiring%20fewer%0Athan%201000%20training%20samples.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/SparrowZheyuan18/Navig/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNAVIG%253A%2520Natural%2520Language-guided%2520Analysis%2520with%2520Vision%2520Language%2520Models%2520for%250A%2520%2520Image%2520Geo-localization%26entry.906535625%3DZheyuan%2520Zhang%2520and%2520Runze%2520Li%2520and%2520Tasnim%2520Kabir%2520and%2520Jordan%2520Boyd-Graber%26entry.1292438233%3D%2520%2520Image%2520geo-localization%2520is%2520the%2520task%2520of%2520predicting%2520the%2520specific%2520location%2520of%2520an%250Aimage%2520and%2520requires%2520complex%2520reasoning%2520across%2520visual%252C%2520geographical%252C%2520and%2520cultural%250Acontexts.%2520While%2520prior%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520the%2520best%2520accuracy%2520at%250Athis%2520task%252C%2520there%2520is%2520a%2520dearth%2520of%2520high-quality%2520datasets%2520and%2520models%2520for%2520analytical%250Areasoning.%2520We%2520first%2520create%2520NaviClues%252C%2520a%2520high-quality%2520dataset%2520derived%2520from%250AGeoGuessr%252C%2520a%2520popular%2520geography%2520game%252C%2520to%2520supply%2520examples%2520of%2520expert%2520reasoning%250Afrom%2520language.%2520Using%2520this%2520dataset%252C%2520we%2520present%2520Navig%252C%2520a%2520comprehensive%2520image%250Ageo-localization%2520framework%2520integrating%2520global%2520and%2520fine-grained%2520image%250Ainformation.%2520By%2520reasoning%2520with%2520language%252C%2520Navig%2520reduces%2520the%2520average%2520distance%250Aerror%2520by%252014%2525%2520compared%2520to%2520previous%2520state-of-the-art%2520models%2520while%2520requiring%2520fewer%250Athan%25201000%2520training%2520samples.%2520Our%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/SparrowZheyuan18/Navig/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAVIG%3A%20Natural%20Language-guided%20Analysis%20with%20Vision%20Language%20Models%20for%0A%20%20Image%20Geo-localization&entry.906535625=Zheyuan%20Zhang%20and%20Runze%20Li%20and%20Tasnim%20Kabir%20and%20Jordan%20Boyd-Graber&entry.1292438233=%20%20Image%20geo-localization%20is%20the%20task%20of%20predicting%20the%20specific%20location%20of%20an%0Aimage%20and%20requires%20complex%20reasoning%20across%20visual%2C%20geographical%2C%20and%20cultural%0Acontexts.%20While%20prior%20Vision%20Language%20Models%20%28VLMs%29%20have%20the%20best%20accuracy%20at%0Athis%20task%2C%20there%20is%20a%20dearth%20of%20high-quality%20datasets%20and%20models%20for%20analytical%0Areasoning.%20We%20first%20create%20NaviClues%2C%20a%20high-quality%20dataset%20derived%20from%0AGeoGuessr%2C%20a%20popular%20geography%20game%2C%20to%20supply%20examples%20of%20expert%20reasoning%0Afrom%20language.%20Using%20this%20dataset%2C%20we%20present%20Navig%2C%20a%20comprehensive%20image%0Ageo-localization%20framework%20integrating%20global%20and%20fine-grained%20image%0Ainformation.%20By%20reasoning%20with%20language%2C%20Navig%20reduces%20the%20average%20distance%0Aerror%20by%2014%25%20compared%20to%20previous%20state-of-the-art%20models%20while%20requiring%20fewer%0Athan%201000%20training%20samples.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/SparrowZheyuan18/Navig/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14638v1&entry.124074799=Read"},
{"title": "ReVision: A Dataset and Baseline VLM for Privacy-Preserving\n  Task-Oriented Visual Instruction Rewriting", "author": "Abhijit Mishra and Richard Noh and Hsiang Fu and Mingda Li and Minji Kim", "abstract": "  Efficient and privacy-preserving multimodal interaction is essential as AR,\nVR, and modern smartphones with powerful cameras become primary interfaces for\nhuman-computer communication. Existing powerful large vision-language models\n(VLMs) enabling multimodal interaction often rely on cloud-based processing,\nraising significant concerns about (1) visual privacy by transmitting sensitive\nvision data to servers, and (2) their limited real-time, on-device usability.\nThis paper explores Visual Instruction Rewriting, a novel approach that\ntransforms multimodal instructions into text-only commands, allowing seamless\nintegration of lightweight on-device instruction rewriter VLMs (250M\nparameters) with existing conversational AI systems, enhancing vision data\nprivacy. To achieve this, we present a dataset of over 39,000 examples across\n14 domains and develop a compact VLM, pretrained on image captioning datasets\nand fine-tuned for instruction rewriting. Experimental results, evaluated\nthrough NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic\nparsing analysis, demonstrate that even a quantized version of the model\n(<500MB storage footprint) can achieve effective instruction rewriting, thus\nenabling privacy-focused, multimodal AI applications.\n", "link": "http://arxiv.org/abs/2502.14780v1", "date": "2025-02-20", "relevancy": 2.6827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReVision%3A%20A%20Dataset%20and%20Baseline%20VLM%20for%20Privacy-Preserving%0A%20%20Task-Oriented%20Visual%20Instruction%20Rewriting&body=Title%3A%20ReVision%3A%20A%20Dataset%20and%20Baseline%20VLM%20for%20Privacy-Preserving%0A%20%20Task-Oriented%20Visual%20Instruction%20Rewriting%0AAuthor%3A%20Abhijit%20Mishra%20and%20Richard%20Noh%20and%20Hsiang%20Fu%20and%20Mingda%20Li%20and%20Minji%20Kim%0AAbstract%3A%20%20%20Efficient%20and%20privacy-preserving%20multimodal%20interaction%20is%20essential%20as%20AR%2C%0AVR%2C%20and%20modern%20smartphones%20with%20powerful%20cameras%20become%20primary%20interfaces%20for%0Ahuman-computer%20communication.%20Existing%20powerful%20large%20vision-language%20models%0A%28VLMs%29%20enabling%20multimodal%20interaction%20often%20rely%20on%20cloud-based%20processing%2C%0Araising%20significant%20concerns%20about%20%281%29%20visual%20privacy%20by%20transmitting%20sensitive%0Avision%20data%20to%20servers%2C%20and%20%282%29%20their%20limited%20real-time%2C%20on-device%20usability.%0AThis%20paper%20explores%20Visual%20Instruction%20Rewriting%2C%20a%20novel%20approach%20that%0Atransforms%20multimodal%20instructions%20into%20text-only%20commands%2C%20allowing%20seamless%0Aintegration%20of%20lightweight%20on-device%20instruction%20rewriter%20VLMs%20%28250M%0Aparameters%29%20with%20existing%20conversational%20AI%20systems%2C%20enhancing%20vision%20data%0Aprivacy.%20To%20achieve%20this%2C%20we%20present%20a%20dataset%20of%20over%2039%2C000%20examples%20across%0A14%20domains%20and%20develop%20a%20compact%20VLM%2C%20pretrained%20on%20image%20captioning%20datasets%0Aand%20fine-tuned%20for%20instruction%20rewriting.%20Experimental%20results%2C%20evaluated%0Athrough%20NLG%20metrics%20such%20as%20BLEU%2C%20METEOR%2C%20and%20ROUGE%2C%20along%20with%20semantic%0Aparsing%20analysis%2C%20demonstrate%20that%20even%20a%20quantized%20version%20of%20the%20model%0A%28%3C500MB%20storage%20footprint%29%20can%20achieve%20effective%20instruction%20rewriting%2C%20thus%0Aenabling%20privacy-focused%2C%20multimodal%20AI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReVision%253A%2520A%2520Dataset%2520and%2520Baseline%2520VLM%2520for%2520Privacy-Preserving%250A%2520%2520Task-Oriented%2520Visual%2520Instruction%2520Rewriting%26entry.906535625%3DAbhijit%2520Mishra%2520and%2520Richard%2520Noh%2520and%2520Hsiang%2520Fu%2520and%2520Mingda%2520Li%2520and%2520Minji%2520Kim%26entry.1292438233%3D%2520%2520Efficient%2520and%2520privacy-preserving%2520multimodal%2520interaction%2520is%2520essential%2520as%2520AR%252C%250AVR%252C%2520and%2520modern%2520smartphones%2520with%2520powerful%2520cameras%2520become%2520primary%2520interfaces%2520for%250Ahuman-computer%2520communication.%2520Existing%2520powerful%2520large%2520vision-language%2520models%250A%2528VLMs%2529%2520enabling%2520multimodal%2520interaction%2520often%2520rely%2520on%2520cloud-based%2520processing%252C%250Araising%2520significant%2520concerns%2520about%2520%25281%2529%2520visual%2520privacy%2520by%2520transmitting%2520sensitive%250Avision%2520data%2520to%2520servers%252C%2520and%2520%25282%2529%2520their%2520limited%2520real-time%252C%2520on-device%2520usability.%250AThis%2520paper%2520explores%2520Visual%2520Instruction%2520Rewriting%252C%2520a%2520novel%2520approach%2520that%250Atransforms%2520multimodal%2520instructions%2520into%2520text-only%2520commands%252C%2520allowing%2520seamless%250Aintegration%2520of%2520lightweight%2520on-device%2520instruction%2520rewriter%2520VLMs%2520%2528250M%250Aparameters%2529%2520with%2520existing%2520conversational%2520AI%2520systems%252C%2520enhancing%2520vision%2520data%250Aprivacy.%2520To%2520achieve%2520this%252C%2520we%2520present%2520a%2520dataset%2520of%2520over%252039%252C000%2520examples%2520across%250A14%2520domains%2520and%2520develop%2520a%2520compact%2520VLM%252C%2520pretrained%2520on%2520image%2520captioning%2520datasets%250Aand%2520fine-tuned%2520for%2520instruction%2520rewriting.%2520Experimental%2520results%252C%2520evaluated%250Athrough%2520NLG%2520metrics%2520such%2520as%2520BLEU%252C%2520METEOR%252C%2520and%2520ROUGE%252C%2520along%2520with%2520semantic%250Aparsing%2520analysis%252C%2520demonstrate%2520that%2520even%2520a%2520quantized%2520version%2520of%2520the%2520model%250A%2528%253C500MB%2520storage%2520footprint%2529%2520can%2520achieve%2520effective%2520instruction%2520rewriting%252C%2520thus%250Aenabling%2520privacy-focused%252C%2520multimodal%2520AI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReVision%3A%20A%20Dataset%20and%20Baseline%20VLM%20for%20Privacy-Preserving%0A%20%20Task-Oriented%20Visual%20Instruction%20Rewriting&entry.906535625=Abhijit%20Mishra%20and%20Richard%20Noh%20and%20Hsiang%20Fu%20and%20Mingda%20Li%20and%20Minji%20Kim&entry.1292438233=%20%20Efficient%20and%20privacy-preserving%20multimodal%20interaction%20is%20essential%20as%20AR%2C%0AVR%2C%20and%20modern%20smartphones%20with%20powerful%20cameras%20become%20primary%20interfaces%20for%0Ahuman-computer%20communication.%20Existing%20powerful%20large%20vision-language%20models%0A%28VLMs%29%20enabling%20multimodal%20interaction%20often%20rely%20on%20cloud-based%20processing%2C%0Araising%20significant%20concerns%20about%20%281%29%20visual%20privacy%20by%20transmitting%20sensitive%0Avision%20data%20to%20servers%2C%20and%20%282%29%20their%20limited%20real-time%2C%20on-device%20usability.%0AThis%20paper%20explores%20Visual%20Instruction%20Rewriting%2C%20a%20novel%20approach%20that%0Atransforms%20multimodal%20instructions%20into%20text-only%20commands%2C%20allowing%20seamless%0Aintegration%20of%20lightweight%20on-device%20instruction%20rewriter%20VLMs%20%28250M%0Aparameters%29%20with%20existing%20conversational%20AI%20systems%2C%20enhancing%20vision%20data%0Aprivacy.%20To%20achieve%20this%2C%20we%20present%20a%20dataset%20of%20over%2039%2C000%20examples%20across%0A14%20domains%20and%20develop%20a%20compact%20VLM%2C%20pretrained%20on%20image%20captioning%20datasets%0Aand%20fine-tuned%20for%20instruction%20rewriting.%20Experimental%20results%2C%20evaluated%0Athrough%20NLG%20metrics%20such%20as%20BLEU%2C%20METEOR%2C%20and%20ROUGE%2C%20along%20with%20semantic%0Aparsing%20analysis%2C%20demonstrate%20that%20even%20a%20quantized%20version%20of%20the%20model%0A%28%3C500MB%20storage%20footprint%29%20can%20achieve%20effective%20instruction%20rewriting%2C%20thus%0Aenabling%20privacy-focused%2C%20multimodal%20AI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14780v1&entry.124074799=Read"},
{"title": "Explanations of Deep Language Models Explain Language Representations in\n  the Brain", "author": "Maryam Rahimi and Yadollah Yaghoobzadeh and Mohammad Reza Daliri", "abstract": "  Recent advances in artificial intelligence have given rise to large language\nmodels (LLMs) that not only achieve human-like performance but also share\ncomputational principles with the brain's language processing mechanisms. While\nprevious research has primarily focused on aligning LLMs' internal\nrepresentations with neural activity, we introduce a novel approach that\nleverages explainable AI (XAI) methods to forge deeper connections between the\ntwo domains. Using attribution methods, we quantified how preceding words\ncontribute to an LLM's next-word predictions and employed these explanations to\npredict fMRI recordings from participants listening to the same narratives. Our\nfindings demonstrate that attribution methods robustly predict brain activity\nacross the language network, surpassing traditional internal representations in\nearly language areas. This alignment is hierarchical: early-layer explanations\ncorrespond to the initial stages of language processing in the brain, while\nlater layers align with more advanced stages. Moreover, the layers more\ninfluential on LLM next-word prediction$\\unicode{x2014}$those with higher\nattribution scores$\\unicode{x2014}$exhibited stronger alignment with neural\nactivity. This work establishes a bidirectional bridge between AI and\nneuroscience. First, we demonstrate that attribution methods offer a powerful\nlens for investigating the neural mechanisms of language comprehension,\nrevealing how meaning emerges from preceding context. Second, we propose using\nbrain alignment as a metric to evaluate the validity of attribution methods,\nproviding a framework for assessing their biological plausibility.\n", "link": "http://arxiv.org/abs/2502.14671v1", "date": "2025-02-20", "relevancy": 2.6804, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explanations%20of%20Deep%20Language%20Models%20Explain%20Language%20Representations%20in%0A%20%20the%20Brain&body=Title%3A%20Explanations%20of%20Deep%20Language%20Models%20Explain%20Language%20Representations%20in%0A%20%20the%20Brain%0AAuthor%3A%20Maryam%20Rahimi%20and%20Yadollah%20Yaghoobzadeh%20and%20Mohammad%20Reza%20Daliri%0AAbstract%3A%20%20%20Recent%20advances%20in%20artificial%20intelligence%20have%20given%20rise%20to%20large%20language%0Amodels%20%28LLMs%29%20that%20not%20only%20achieve%20human-like%20performance%20but%20also%20share%0Acomputational%20principles%20with%20the%20brain%27s%20language%20processing%20mechanisms.%20While%0Aprevious%20research%20has%20primarily%20focused%20on%20aligning%20LLMs%27%20internal%0Arepresentations%20with%20neural%20activity%2C%20we%20introduce%20a%20novel%20approach%20that%0Aleverages%20explainable%20AI%20%28XAI%29%20methods%20to%20forge%20deeper%20connections%20between%20the%0Atwo%20domains.%20Using%20attribution%20methods%2C%20we%20quantified%20how%20preceding%20words%0Acontribute%20to%20an%20LLM%27s%20next-word%20predictions%20and%20employed%20these%20explanations%20to%0Apredict%20fMRI%20recordings%20from%20participants%20listening%20to%20the%20same%20narratives.%20Our%0Afindings%20demonstrate%20that%20attribution%20methods%20robustly%20predict%20brain%20activity%0Aacross%20the%20language%20network%2C%20surpassing%20traditional%20internal%20representations%20in%0Aearly%20language%20areas.%20This%20alignment%20is%20hierarchical%3A%20early-layer%20explanations%0Acorrespond%20to%20the%20initial%20stages%20of%20language%20processing%20in%20the%20brain%2C%20while%0Alater%20layers%20align%20with%20more%20advanced%20stages.%20Moreover%2C%20the%20layers%20more%0Ainfluential%20on%20LLM%20next-word%20prediction%24%5Cunicode%7Bx2014%7D%24those%20with%20higher%0Aattribution%20scores%24%5Cunicode%7Bx2014%7D%24exhibited%20stronger%20alignment%20with%20neural%0Aactivity.%20This%20work%20establishes%20a%20bidirectional%20bridge%20between%20AI%20and%0Aneuroscience.%20First%2C%20we%20demonstrate%20that%20attribution%20methods%20offer%20a%20powerful%0Alens%20for%20investigating%20the%20neural%20mechanisms%20of%20language%20comprehension%2C%0Arevealing%20how%20meaning%20emerges%20from%20preceding%20context.%20Second%2C%20we%20propose%20using%0Abrain%20alignment%20as%20a%20metric%20to%20evaluate%20the%20validity%20of%20attribution%20methods%2C%0Aproviding%20a%20framework%20for%20assessing%20their%20biological%20plausibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplanations%2520of%2520Deep%2520Language%2520Models%2520Explain%2520Language%2520Representations%2520in%250A%2520%2520the%2520Brain%26entry.906535625%3DMaryam%2520Rahimi%2520and%2520Yadollah%2520Yaghoobzadeh%2520and%2520Mohammad%2520Reza%2520Daliri%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520artificial%2520intelligence%2520have%2520given%2520rise%2520to%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520that%2520not%2520only%2520achieve%2520human-like%2520performance%2520but%2520also%2520share%250Acomputational%2520principles%2520with%2520the%2520brain%2527s%2520language%2520processing%2520mechanisms.%2520While%250Aprevious%2520research%2520has%2520primarily%2520focused%2520on%2520aligning%2520LLMs%2527%2520internal%250Arepresentations%2520with%2520neural%2520activity%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520that%250Aleverages%2520explainable%2520AI%2520%2528XAI%2529%2520methods%2520to%2520forge%2520deeper%2520connections%2520between%2520the%250Atwo%2520domains.%2520Using%2520attribution%2520methods%252C%2520we%2520quantified%2520how%2520preceding%2520words%250Acontribute%2520to%2520an%2520LLM%2527s%2520next-word%2520predictions%2520and%2520employed%2520these%2520explanations%2520to%250Apredict%2520fMRI%2520recordings%2520from%2520participants%2520listening%2520to%2520the%2520same%2520narratives.%2520Our%250Afindings%2520demonstrate%2520that%2520attribution%2520methods%2520robustly%2520predict%2520brain%2520activity%250Aacross%2520the%2520language%2520network%252C%2520surpassing%2520traditional%2520internal%2520representations%2520in%250Aearly%2520language%2520areas.%2520This%2520alignment%2520is%2520hierarchical%253A%2520early-layer%2520explanations%250Acorrespond%2520to%2520the%2520initial%2520stages%2520of%2520language%2520processing%2520in%2520the%2520brain%252C%2520while%250Alater%2520layers%2520align%2520with%2520more%2520advanced%2520stages.%2520Moreover%252C%2520the%2520layers%2520more%250Ainfluential%2520on%2520LLM%2520next-word%2520prediction%2524%255Cunicode%257Bx2014%257D%2524those%2520with%2520higher%250Aattribution%2520scores%2524%255Cunicode%257Bx2014%257D%2524exhibited%2520stronger%2520alignment%2520with%2520neural%250Aactivity.%2520This%2520work%2520establishes%2520a%2520bidirectional%2520bridge%2520between%2520AI%2520and%250Aneuroscience.%2520First%252C%2520we%2520demonstrate%2520that%2520attribution%2520methods%2520offer%2520a%2520powerful%250Alens%2520for%2520investigating%2520the%2520neural%2520mechanisms%2520of%2520language%2520comprehension%252C%250Arevealing%2520how%2520meaning%2520emerges%2520from%2520preceding%2520context.%2520Second%252C%2520we%2520propose%2520using%250Abrain%2520alignment%2520as%2520a%2520metric%2520to%2520evaluate%2520the%2520validity%2520of%2520attribution%2520methods%252C%250Aproviding%2520a%2520framework%2520for%2520assessing%2520their%2520biological%2520plausibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explanations%20of%20Deep%20Language%20Models%20Explain%20Language%20Representations%20in%0A%20%20the%20Brain&entry.906535625=Maryam%20Rahimi%20and%20Yadollah%20Yaghoobzadeh%20and%20Mohammad%20Reza%20Daliri&entry.1292438233=%20%20Recent%20advances%20in%20artificial%20intelligence%20have%20given%20rise%20to%20large%20language%0Amodels%20%28LLMs%29%20that%20not%20only%20achieve%20human-like%20performance%20but%20also%20share%0Acomputational%20principles%20with%20the%20brain%27s%20language%20processing%20mechanisms.%20While%0Aprevious%20research%20has%20primarily%20focused%20on%20aligning%20LLMs%27%20internal%0Arepresentations%20with%20neural%20activity%2C%20we%20introduce%20a%20novel%20approach%20that%0Aleverages%20explainable%20AI%20%28XAI%29%20methods%20to%20forge%20deeper%20connections%20between%20the%0Atwo%20domains.%20Using%20attribution%20methods%2C%20we%20quantified%20how%20preceding%20words%0Acontribute%20to%20an%20LLM%27s%20next-word%20predictions%20and%20employed%20these%20explanations%20to%0Apredict%20fMRI%20recordings%20from%20participants%20listening%20to%20the%20same%20narratives.%20Our%0Afindings%20demonstrate%20that%20attribution%20methods%20robustly%20predict%20brain%20activity%0Aacross%20the%20language%20network%2C%20surpassing%20traditional%20internal%20representations%20in%0Aearly%20language%20areas.%20This%20alignment%20is%20hierarchical%3A%20early-layer%20explanations%0Acorrespond%20to%20the%20initial%20stages%20of%20language%20processing%20in%20the%20brain%2C%20while%0Alater%20layers%20align%20with%20more%20advanced%20stages.%20Moreover%2C%20the%20layers%20more%0Ainfluential%20on%20LLM%20next-word%20prediction%24%5Cunicode%7Bx2014%7D%24those%20with%20higher%0Aattribution%20scores%24%5Cunicode%7Bx2014%7D%24exhibited%20stronger%20alignment%20with%20neural%0Aactivity.%20This%20work%20establishes%20a%20bidirectional%20bridge%20between%20AI%20and%0Aneuroscience.%20First%2C%20we%20demonstrate%20that%20attribution%20methods%20offer%20a%20powerful%0Alens%20for%20investigating%20the%20neural%20mechanisms%20of%20language%20comprehension%2C%0Arevealing%20how%20meaning%20emerges%20from%20preceding%20context.%20Second%2C%20we%20propose%20using%0Abrain%20alignment%20as%20a%20metric%20to%20evaluate%20the%20validity%20of%20attribution%20methods%2C%0Aproviding%20a%20framework%20for%20assessing%20their%20biological%20plausibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14671v1&entry.124074799=Read"},
{"title": "Not All Data are Good Labels: On the Self-supervised Labeling for Time\n  Series Forecasting", "author": "Yuxuan Yang and Dalin Zhang and Yuxuan Liang and Hua Lu and Huan Li and Gang Chen", "abstract": "  Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning.\n", "link": "http://arxiv.org/abs/2502.14704v1", "date": "2025-02-20", "relevancy": 2.6591, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5561}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.538}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Data%20are%20Good%20Labels%3A%20On%20the%20Self-supervised%20Labeling%20for%20Time%0A%20%20Series%20Forecasting&body=Title%3A%20Not%20All%20Data%20are%20Good%20Labels%3A%20On%20the%20Self-supervised%20Labeling%20for%20Time%0A%20%20Series%20Forecasting%0AAuthor%3A%20Yuxuan%20Yang%20and%20Dalin%20Zhang%20and%20Yuxuan%20Liang%20and%20Hua%20Lu%20and%20Huan%20Li%20and%20Gang%20Chen%0AAbstract%3A%20%20%20Time%20Series%20Forecasting%20%28TSF%29%20is%20a%20crucial%20task%20in%20various%20domains%2C%20yet%0Aexisting%20TSF%20models%20rely%20heavily%20on%20high-quality%20data%20and%20insufficiently%0Aexploit%20all%20available%20data.%20This%20paper%20explores%20a%20novel%20self-supervised%0Aapproach%20to%20re-label%20time%20series%20datasets%20by%20inherently%20constructing%20candidate%0Adatasets.%20During%20the%20optimization%20of%20a%20simple%20reconstruction%20network%2C%0Aintermediates%20are%20used%20as%20pseudo%20labels%20in%20a%20self-supervised%20paradigm%2C%0Aimproving%20generalization%20for%20any%20predictor.%20We%20introduce%20the%20Self-Correction%0Awith%20Adaptive%20Mask%20%28SCAM%29%2C%20which%20discards%20overfitted%20components%20and%20selectively%0Areplaces%20them%20with%20pseudo%20labels%20generated%20from%20reconstructions.%20Additionally%2C%0Awe%20incorporate%20Spectral%20Norm%20Regularization%20%28SNR%29%20to%20further%20suppress%0Aoverfitting%20from%20a%20loss%20landscape%20perspective.%20Our%20experiments%20on%20eleven%0Areal-world%20datasets%20demonstrate%20that%20SCAM%20consistently%20improves%20the%20performance%0Aof%20various%20backbone%20models.%20This%20work%20offers%20a%20new%20perspective%20on%20constructing%0Adatasets%20and%20enhancing%20the%20generalization%20of%20TSF%20models%20through%20self-supervised%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Data%2520are%2520Good%2520Labels%253A%2520On%2520the%2520Self-supervised%2520Labeling%2520for%2520Time%250A%2520%2520Series%2520Forecasting%26entry.906535625%3DYuxuan%2520Yang%2520and%2520Dalin%2520Zhang%2520and%2520Yuxuan%2520Liang%2520and%2520Hua%2520Lu%2520and%2520Huan%2520Li%2520and%2520Gang%2520Chen%26entry.1292438233%3D%2520%2520Time%2520Series%2520Forecasting%2520%2528TSF%2529%2520is%2520a%2520crucial%2520task%2520in%2520various%2520domains%252C%2520yet%250Aexisting%2520TSF%2520models%2520rely%2520heavily%2520on%2520high-quality%2520data%2520and%2520insufficiently%250Aexploit%2520all%2520available%2520data.%2520This%2520paper%2520explores%2520a%2520novel%2520self-supervised%250Aapproach%2520to%2520re-label%2520time%2520series%2520datasets%2520by%2520inherently%2520constructing%2520candidate%250Adatasets.%2520During%2520the%2520optimization%2520of%2520a%2520simple%2520reconstruction%2520network%252C%250Aintermediates%2520are%2520used%2520as%2520pseudo%2520labels%2520in%2520a%2520self-supervised%2520paradigm%252C%250Aimproving%2520generalization%2520for%2520any%2520predictor.%2520We%2520introduce%2520the%2520Self-Correction%250Awith%2520Adaptive%2520Mask%2520%2528SCAM%2529%252C%2520which%2520discards%2520overfitted%2520components%2520and%2520selectively%250Areplaces%2520them%2520with%2520pseudo%2520labels%2520generated%2520from%2520reconstructions.%2520Additionally%252C%250Awe%2520incorporate%2520Spectral%2520Norm%2520Regularization%2520%2528SNR%2529%2520to%2520further%2520suppress%250Aoverfitting%2520from%2520a%2520loss%2520landscape%2520perspective.%2520Our%2520experiments%2520on%2520eleven%250Areal-world%2520datasets%2520demonstrate%2520that%2520SCAM%2520consistently%2520improves%2520the%2520performance%250Aof%2520various%2520backbone%2520models.%2520This%2520work%2520offers%2520a%2520new%2520perspective%2520on%2520constructing%250Adatasets%2520and%2520enhancing%2520the%2520generalization%2520of%2520TSF%2520models%2520through%2520self-supervised%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Data%20are%20Good%20Labels%3A%20On%20the%20Self-supervised%20Labeling%20for%20Time%0A%20%20Series%20Forecasting&entry.906535625=Yuxuan%20Yang%20and%20Dalin%20Zhang%20and%20Yuxuan%20Liang%20and%20Hua%20Lu%20and%20Huan%20Li%20and%20Gang%20Chen&entry.1292438233=%20%20Time%20Series%20Forecasting%20%28TSF%29%20is%20a%20crucial%20task%20in%20various%20domains%2C%20yet%0Aexisting%20TSF%20models%20rely%20heavily%20on%20high-quality%20data%20and%20insufficiently%0Aexploit%20all%20available%20data.%20This%20paper%20explores%20a%20novel%20self-supervised%0Aapproach%20to%20re-label%20time%20series%20datasets%20by%20inherently%20constructing%20candidate%0Adatasets.%20During%20the%20optimization%20of%20a%20simple%20reconstruction%20network%2C%0Aintermediates%20are%20used%20as%20pseudo%20labels%20in%20a%20self-supervised%20paradigm%2C%0Aimproving%20generalization%20for%20any%20predictor.%20We%20introduce%20the%20Self-Correction%0Awith%20Adaptive%20Mask%20%28SCAM%29%2C%20which%20discards%20overfitted%20components%20and%20selectively%0Areplaces%20them%20with%20pseudo%20labels%20generated%20from%20reconstructions.%20Additionally%2C%0Awe%20incorporate%20Spectral%20Norm%20Regularization%20%28SNR%29%20to%20further%20suppress%0Aoverfitting%20from%20a%20loss%20landscape%20perspective.%20Our%20experiments%20on%20eleven%0Areal-world%20datasets%20demonstrate%20that%20SCAM%20consistently%20improves%20the%20performance%0Aof%20various%20backbone%20models.%20This%20work%20offers%20a%20new%20perspective%20on%20constructing%0Adatasets%20and%20enhancing%20the%20generalization%20of%20TSF%20models%20through%20self-supervised%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14704v1&entry.124074799=Read"},
{"title": "Noisy Test-Time Adaptation in Vision-Language Models", "author": "Chentao Cao and Zhun Zhong and Zhanke Zhou and Tongliang Liu and Yang Liu and Kun Zhang and Bo Han", "abstract": "  Test-time adaptation (TTA) aims to address distribution shifts between source\nand target data by relying solely on target data during testing. In open-world\nscenarios, models often encounter noisy samples, i.e., samples outside the\nin-distribution (ID) label space. Leveraging the zero-shot capability of\npre-trained vision-language models (VLMs), this paper introduces Zero-Shot\nNoisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy\nsamples during test-time in a zero-shot manner. We find existing TTA methods\nunderperform under ZS-NTTA, often lagging behind even the frozen model. We\nconduct comprehensive experiments to analyze this phenomenon, revealing that\nthe negative impact of unfiltered noisy data outweighs the benefits of clean\ndata during model updating. Also, adapting a classifier for ID classification\nand noise detection hampers both sub-tasks. Built on this, we propose a\nframework that decouples the classifier and detector, focusing on developing an\nindividual detector while keeping the classifier frozen. Technically, we\nintroduce the Adaptive Noise Detector (AdaND), which utilizes the frozen\nmodel's outputs as pseudo-labels to train a noise detector. To handle clean\ndata streams, we further inject Gaussian noise during adaptation, preventing\nthe detector from misclassifying clean samples as noisy. Beyond the ZS-NTTA,\nAdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection\nability of VLMs. Experiments show that AdaND outperforms in both ZS-NTTA and\nZS-OOD detection. On ImageNet, AdaND achieves a notable improvement of $8.32\\%$\nin harmonic mean accuracy ($\\text{Acc}_\\text{H}$) for ZS-NTTA and $9.40\\%$ in\nFPR95 for ZS-OOD detection, compared to SOTA methods. Importantly, AdaND is\ncomputationally efficient and comparable to the model-frozen method. The code\nis publicly available at: https://github.com/tmlr-group/ZS-NTTA.\n", "link": "http://arxiv.org/abs/2502.14604v1", "date": "2025-02-20", "relevancy": 2.6414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5222}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noisy%20Test-Time%20Adaptation%20in%20Vision-Language%20Models&body=Title%3A%20Noisy%20Test-Time%20Adaptation%20in%20Vision-Language%20Models%0AAuthor%3A%20Chentao%20Cao%20and%20Zhun%20Zhong%20and%20Zhanke%20Zhou%20and%20Tongliang%20Liu%20and%20Yang%20Liu%20and%20Kun%20Zhang%20and%20Bo%20Han%0AAbstract%3A%20%20%20Test-time%20adaptation%20%28TTA%29%20aims%20to%20address%20distribution%20shifts%20between%20source%0Aand%20target%20data%20by%20relying%20solely%20on%20target%20data%20during%20testing.%20In%20open-world%0Ascenarios%2C%20models%20often%20encounter%20noisy%20samples%2C%20i.e.%2C%20samples%20outside%20the%0Ain-distribution%20%28ID%29%20label%20space.%20Leveraging%20the%20zero-shot%20capability%20of%0Apre-trained%20vision-language%20models%20%28VLMs%29%2C%20this%20paper%20introduces%20Zero-Shot%0ANoisy%20TTA%20%28ZS-NTTA%29%2C%20focusing%20on%20adapting%20the%20model%20to%20target%20data%20with%20noisy%0Asamples%20during%20test-time%20in%20a%20zero-shot%20manner.%20We%20find%20existing%20TTA%20methods%0Aunderperform%20under%20ZS-NTTA%2C%20often%20lagging%20behind%20even%20the%20frozen%20model.%20We%0Aconduct%20comprehensive%20experiments%20to%20analyze%20this%20phenomenon%2C%20revealing%20that%0Athe%20negative%20impact%20of%20unfiltered%20noisy%20data%20outweighs%20the%20benefits%20of%20clean%0Adata%20during%20model%20updating.%20Also%2C%20adapting%20a%20classifier%20for%20ID%20classification%0Aand%20noise%20detection%20hampers%20both%20sub-tasks.%20Built%20on%20this%2C%20we%20propose%20a%0Aframework%20that%20decouples%20the%20classifier%20and%20detector%2C%20focusing%20on%20developing%20an%0Aindividual%20detector%20while%20keeping%20the%20classifier%20frozen.%20Technically%2C%20we%0Aintroduce%20the%20Adaptive%20Noise%20Detector%20%28AdaND%29%2C%20which%20utilizes%20the%20frozen%0Amodel%27s%20outputs%20as%20pseudo-labels%20to%20train%20a%20noise%20detector.%20To%20handle%20clean%0Adata%20streams%2C%20we%20further%20inject%20Gaussian%20noise%20during%20adaptation%2C%20preventing%0Athe%20detector%20from%20misclassifying%20clean%20samples%20as%20noisy.%20Beyond%20the%20ZS-NTTA%2C%0AAdaND%20can%20also%20improve%20the%20zero-shot%20out-of-distribution%20%28ZS-OOD%29%20detection%0Aability%20of%20VLMs.%20Experiments%20show%20that%20AdaND%20outperforms%20in%20both%20ZS-NTTA%20and%0AZS-OOD%20detection.%20On%20ImageNet%2C%20AdaND%20achieves%20a%20notable%20improvement%20of%20%248.32%5C%25%24%0Ain%20harmonic%20mean%20accuracy%20%28%24%5Ctext%7BAcc%7D_%5Ctext%7BH%7D%24%29%20for%20ZS-NTTA%20and%20%249.40%5C%25%24%20in%0AFPR95%20for%20ZS-OOD%20detection%2C%20compared%20to%20SOTA%20methods.%20Importantly%2C%20AdaND%20is%0Acomputationally%20efficient%20and%20comparable%20to%20the%20model-frozen%20method.%20The%20code%0Ais%20publicly%20available%20at%3A%20https%3A//github.com/tmlr-group/ZS-NTTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisy%2520Test-Time%2520Adaptation%2520in%2520Vision-Language%2520Models%26entry.906535625%3DChentao%2520Cao%2520and%2520Zhun%2520Zhong%2520and%2520Zhanke%2520Zhou%2520and%2520Tongliang%2520Liu%2520and%2520Yang%2520Liu%2520and%2520Kun%2520Zhang%2520and%2520Bo%2520Han%26entry.1292438233%3D%2520%2520Test-time%2520adaptation%2520%2528TTA%2529%2520aims%2520to%2520address%2520distribution%2520shifts%2520between%2520source%250Aand%2520target%2520data%2520by%2520relying%2520solely%2520on%2520target%2520data%2520during%2520testing.%2520In%2520open-world%250Ascenarios%252C%2520models%2520often%2520encounter%2520noisy%2520samples%252C%2520i.e.%252C%2520samples%2520outside%2520the%250Ain-distribution%2520%2528ID%2529%2520label%2520space.%2520Leveraging%2520the%2520zero-shot%2520capability%2520of%250Apre-trained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520this%2520paper%2520introduces%2520Zero-Shot%250ANoisy%2520TTA%2520%2528ZS-NTTA%2529%252C%2520focusing%2520on%2520adapting%2520the%2520model%2520to%2520target%2520data%2520with%2520noisy%250Asamples%2520during%2520test-time%2520in%2520a%2520zero-shot%2520manner.%2520We%2520find%2520existing%2520TTA%2520methods%250Aunderperform%2520under%2520ZS-NTTA%252C%2520often%2520lagging%2520behind%2520even%2520the%2520frozen%2520model.%2520We%250Aconduct%2520comprehensive%2520experiments%2520to%2520analyze%2520this%2520phenomenon%252C%2520revealing%2520that%250Athe%2520negative%2520impact%2520of%2520unfiltered%2520noisy%2520data%2520outweighs%2520the%2520benefits%2520of%2520clean%250Adata%2520during%2520model%2520updating.%2520Also%252C%2520adapting%2520a%2520classifier%2520for%2520ID%2520classification%250Aand%2520noise%2520detection%2520hampers%2520both%2520sub-tasks.%2520Built%2520on%2520this%252C%2520we%2520propose%2520a%250Aframework%2520that%2520decouples%2520the%2520classifier%2520and%2520detector%252C%2520focusing%2520on%2520developing%2520an%250Aindividual%2520detector%2520while%2520keeping%2520the%2520classifier%2520frozen.%2520Technically%252C%2520we%250Aintroduce%2520the%2520Adaptive%2520Noise%2520Detector%2520%2528AdaND%2529%252C%2520which%2520utilizes%2520the%2520frozen%250Amodel%2527s%2520outputs%2520as%2520pseudo-labels%2520to%2520train%2520a%2520noise%2520detector.%2520To%2520handle%2520clean%250Adata%2520streams%252C%2520we%2520further%2520inject%2520Gaussian%2520noise%2520during%2520adaptation%252C%2520preventing%250Athe%2520detector%2520from%2520misclassifying%2520clean%2520samples%2520as%2520noisy.%2520Beyond%2520the%2520ZS-NTTA%252C%250AAdaND%2520can%2520also%2520improve%2520the%2520zero-shot%2520out-of-distribution%2520%2528ZS-OOD%2529%2520detection%250Aability%2520of%2520VLMs.%2520Experiments%2520show%2520that%2520AdaND%2520outperforms%2520in%2520both%2520ZS-NTTA%2520and%250AZS-OOD%2520detection.%2520On%2520ImageNet%252C%2520AdaND%2520achieves%2520a%2520notable%2520improvement%2520of%2520%25248.32%255C%2525%2524%250Ain%2520harmonic%2520mean%2520accuracy%2520%2528%2524%255Ctext%257BAcc%257D_%255Ctext%257BH%257D%2524%2529%2520for%2520ZS-NTTA%2520and%2520%25249.40%255C%2525%2524%2520in%250AFPR95%2520for%2520ZS-OOD%2520detection%252C%2520compared%2520to%2520SOTA%2520methods.%2520Importantly%252C%2520AdaND%2520is%250Acomputationally%2520efficient%2520and%2520comparable%2520to%2520the%2520model-frozen%2520method.%2520The%2520code%250Ais%2520publicly%2520available%2520at%253A%2520https%253A//github.com/tmlr-group/ZS-NTTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noisy%20Test-Time%20Adaptation%20in%20Vision-Language%20Models&entry.906535625=Chentao%20Cao%20and%20Zhun%20Zhong%20and%20Zhanke%20Zhou%20and%20Tongliang%20Liu%20and%20Yang%20Liu%20and%20Kun%20Zhang%20and%20Bo%20Han&entry.1292438233=%20%20Test-time%20adaptation%20%28TTA%29%20aims%20to%20address%20distribution%20shifts%20between%20source%0Aand%20target%20data%20by%20relying%20solely%20on%20target%20data%20during%20testing.%20In%20open-world%0Ascenarios%2C%20models%20often%20encounter%20noisy%20samples%2C%20i.e.%2C%20samples%20outside%20the%0Ain-distribution%20%28ID%29%20label%20space.%20Leveraging%20the%20zero-shot%20capability%20of%0Apre-trained%20vision-language%20models%20%28VLMs%29%2C%20this%20paper%20introduces%20Zero-Shot%0ANoisy%20TTA%20%28ZS-NTTA%29%2C%20focusing%20on%20adapting%20the%20model%20to%20target%20data%20with%20noisy%0Asamples%20during%20test-time%20in%20a%20zero-shot%20manner.%20We%20find%20existing%20TTA%20methods%0Aunderperform%20under%20ZS-NTTA%2C%20often%20lagging%20behind%20even%20the%20frozen%20model.%20We%0Aconduct%20comprehensive%20experiments%20to%20analyze%20this%20phenomenon%2C%20revealing%20that%0Athe%20negative%20impact%20of%20unfiltered%20noisy%20data%20outweighs%20the%20benefits%20of%20clean%0Adata%20during%20model%20updating.%20Also%2C%20adapting%20a%20classifier%20for%20ID%20classification%0Aand%20noise%20detection%20hampers%20both%20sub-tasks.%20Built%20on%20this%2C%20we%20propose%20a%0Aframework%20that%20decouples%20the%20classifier%20and%20detector%2C%20focusing%20on%20developing%20an%0Aindividual%20detector%20while%20keeping%20the%20classifier%20frozen.%20Technically%2C%20we%0Aintroduce%20the%20Adaptive%20Noise%20Detector%20%28AdaND%29%2C%20which%20utilizes%20the%20frozen%0Amodel%27s%20outputs%20as%20pseudo-labels%20to%20train%20a%20noise%20detector.%20To%20handle%20clean%0Adata%20streams%2C%20we%20further%20inject%20Gaussian%20noise%20during%20adaptation%2C%20preventing%0Athe%20detector%20from%20misclassifying%20clean%20samples%20as%20noisy.%20Beyond%20the%20ZS-NTTA%2C%0AAdaND%20can%20also%20improve%20the%20zero-shot%20out-of-distribution%20%28ZS-OOD%29%20detection%0Aability%20of%20VLMs.%20Experiments%20show%20that%20AdaND%20outperforms%20in%20both%20ZS-NTTA%20and%0AZS-OOD%20detection.%20On%20ImageNet%2C%20AdaND%20achieves%20a%20notable%20improvement%20of%20%248.32%5C%25%24%0Ain%20harmonic%20mean%20accuracy%20%28%24%5Ctext%7BAcc%7D_%5Ctext%7BH%7D%24%29%20for%20ZS-NTTA%20and%20%249.40%5C%25%24%20in%0AFPR95%20for%20ZS-OOD%20detection%2C%20compared%20to%20SOTA%20methods.%20Importantly%2C%20AdaND%20is%0Acomputationally%20efficient%20and%20comparable%20to%20the%20model-frozen%20method.%20The%20code%0Ais%20publicly%20available%20at%3A%20https%3A//github.com/tmlr-group/ZS-NTTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14604v1&entry.124074799=Read"},
{"title": "Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental\n  Learning", "author": "Murat Onur Yildirim and Elif Ceren Gok Yildirim and Joaquin Vanschoren", "abstract": "  Class-incremental learning requires models to continually acquire knowledge\nof new classes without forgetting old ones. Although pre-trained models have\ndemonstrated strong performance in class-incremental learning, they remain\nsusceptible to catastrophic forgetting when learning new concepts. Excessive\nplasticity in the models breaks generalizability and causes forgetting, while\nstrong stability results in insufficient adaptation to new classes. This\nnecessitates effective adaptation with minimal modifications to preserve the\ngeneral knowledge of pre-trained models. To address this challenge, we first\nintroduce a new parameter-efficient fine-tuning module 'Learn and Calibrate',\nor LuCA, designed to acquire knowledge through an adapter-calibrator couple,\nenabling effective adaptation with well-refined feature representations.\nSecond, for each learning session, we deploy a sparse LuCA module on top of the\nlast token just before the classifier, which we refer to as 'Token-level Sparse\nCalibration and Adaptation', or TOSCA. This strategic design improves the\northogonality between the modules and significantly reduces both training and\ninference complexity. By leaving the generalization capabilities of the\npre-trained models intact and adapting exclusively via the last token, our\napproach achieves a harmonious balance between stability and plasticity.\nExtensive experiments demonstrate TOSCA's state-of-the-art performance while\nintroducing ~8 times fewer parameters compared to prior methods.\n", "link": "http://arxiv.org/abs/2502.14762v1", "date": "2025-02-20", "relevancy": 2.6247, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5139}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sculpting%20%5BCLS%5D%20Features%20for%20Pre-Trained%20Model-Based%20Class-Incremental%0A%20%20Learning&body=Title%3A%20Sculpting%20%5BCLS%5D%20Features%20for%20Pre-Trained%20Model-Based%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Murat%20Onur%20Yildirim%20and%20Elif%20Ceren%20Gok%20Yildirim%20and%20Joaquin%20Vanschoren%0AAbstract%3A%20%20%20Class-incremental%20learning%20requires%20models%20to%20continually%20acquire%20knowledge%0Aof%20new%20classes%20without%20forgetting%20old%20ones.%20Although%20pre-trained%20models%20have%0Ademonstrated%20strong%20performance%20in%20class-incremental%20learning%2C%20they%20remain%0Asusceptible%20to%20catastrophic%20forgetting%20when%20learning%20new%20concepts.%20Excessive%0Aplasticity%20in%20the%20models%20breaks%20generalizability%20and%20causes%20forgetting%2C%20while%0Astrong%20stability%20results%20in%20insufficient%20adaptation%20to%20new%20classes.%20This%0Anecessitates%20effective%20adaptation%20with%20minimal%20modifications%20to%20preserve%20the%0Ageneral%20knowledge%20of%20pre-trained%20models.%20To%20address%20this%20challenge%2C%20we%20first%0Aintroduce%20a%20new%20parameter-efficient%20fine-tuning%20module%20%27Learn%20and%20Calibrate%27%2C%0Aor%20LuCA%2C%20designed%20to%20acquire%20knowledge%20through%20an%20adapter-calibrator%20couple%2C%0Aenabling%20effective%20adaptation%20with%20well-refined%20feature%20representations.%0ASecond%2C%20for%20each%20learning%20session%2C%20we%20deploy%20a%20sparse%20LuCA%20module%20on%20top%20of%20the%0Alast%20token%20just%20before%20the%20classifier%2C%20which%20we%20refer%20to%20as%20%27Token-level%20Sparse%0ACalibration%20and%20Adaptation%27%2C%20or%20TOSCA.%20This%20strategic%20design%20improves%20the%0Aorthogonality%20between%20the%20modules%20and%20significantly%20reduces%20both%20training%20and%0Ainference%20complexity.%20By%20leaving%20the%20generalization%20capabilities%20of%20the%0Apre-trained%20models%20intact%20and%20adapting%20exclusively%20via%20the%20last%20token%2C%20our%0Aapproach%20achieves%20a%20harmonious%20balance%20between%20stability%20and%20plasticity.%0AExtensive%20experiments%20demonstrate%20TOSCA%27s%20state-of-the-art%20performance%20while%0Aintroducing%20~8%20times%20fewer%20parameters%20compared%20to%20prior%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSculpting%2520%255BCLS%255D%2520Features%2520for%2520Pre-Trained%2520Model-Based%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DMurat%2520Onur%2520Yildirim%2520and%2520Elif%2520Ceren%2520Gok%2520Yildirim%2520and%2520Joaquin%2520Vanschoren%26entry.1292438233%3D%2520%2520Class-incremental%2520learning%2520requires%2520models%2520to%2520continually%2520acquire%2520knowledge%250Aof%2520new%2520classes%2520without%2520forgetting%2520old%2520ones.%2520Although%2520pre-trained%2520models%2520have%250Ademonstrated%2520strong%2520performance%2520in%2520class-incremental%2520learning%252C%2520they%2520remain%250Asusceptible%2520to%2520catastrophic%2520forgetting%2520when%2520learning%2520new%2520concepts.%2520Excessive%250Aplasticity%2520in%2520the%2520models%2520breaks%2520generalizability%2520and%2520causes%2520forgetting%252C%2520while%250Astrong%2520stability%2520results%2520in%2520insufficient%2520adaptation%2520to%2520new%2520classes.%2520This%250Anecessitates%2520effective%2520adaptation%2520with%2520minimal%2520modifications%2520to%2520preserve%2520the%250Ageneral%2520knowledge%2520of%2520pre-trained%2520models.%2520To%2520address%2520this%2520challenge%252C%2520we%2520first%250Aintroduce%2520a%2520new%2520parameter-efficient%2520fine-tuning%2520module%2520%2527Learn%2520and%2520Calibrate%2527%252C%250Aor%2520LuCA%252C%2520designed%2520to%2520acquire%2520knowledge%2520through%2520an%2520adapter-calibrator%2520couple%252C%250Aenabling%2520effective%2520adaptation%2520with%2520well-refined%2520feature%2520representations.%250ASecond%252C%2520for%2520each%2520learning%2520session%252C%2520we%2520deploy%2520a%2520sparse%2520LuCA%2520module%2520on%2520top%2520of%2520the%250Alast%2520token%2520just%2520before%2520the%2520classifier%252C%2520which%2520we%2520refer%2520to%2520as%2520%2527Token-level%2520Sparse%250ACalibration%2520and%2520Adaptation%2527%252C%2520or%2520TOSCA.%2520This%2520strategic%2520design%2520improves%2520the%250Aorthogonality%2520between%2520the%2520modules%2520and%2520significantly%2520reduces%2520both%2520training%2520and%250Ainference%2520complexity.%2520By%2520leaving%2520the%2520generalization%2520capabilities%2520of%2520the%250Apre-trained%2520models%2520intact%2520and%2520adapting%2520exclusively%2520via%2520the%2520last%2520token%252C%2520our%250Aapproach%2520achieves%2520a%2520harmonious%2520balance%2520between%2520stability%2520and%2520plasticity.%250AExtensive%2520experiments%2520demonstrate%2520TOSCA%2527s%2520state-of-the-art%2520performance%2520while%250Aintroducing%2520~8%2520times%2520fewer%2520parameters%2520compared%2520to%2520prior%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sculpting%20%5BCLS%5D%20Features%20for%20Pre-Trained%20Model-Based%20Class-Incremental%0A%20%20Learning&entry.906535625=Murat%20Onur%20Yildirim%20and%20Elif%20Ceren%20Gok%20Yildirim%20and%20Joaquin%20Vanschoren&entry.1292438233=%20%20Class-incremental%20learning%20requires%20models%20to%20continually%20acquire%20knowledge%0Aof%20new%20classes%20without%20forgetting%20old%20ones.%20Although%20pre-trained%20models%20have%0Ademonstrated%20strong%20performance%20in%20class-incremental%20learning%2C%20they%20remain%0Asusceptible%20to%20catastrophic%20forgetting%20when%20learning%20new%20concepts.%20Excessive%0Aplasticity%20in%20the%20models%20breaks%20generalizability%20and%20causes%20forgetting%2C%20while%0Astrong%20stability%20results%20in%20insufficient%20adaptation%20to%20new%20classes.%20This%0Anecessitates%20effective%20adaptation%20with%20minimal%20modifications%20to%20preserve%20the%0Ageneral%20knowledge%20of%20pre-trained%20models.%20To%20address%20this%20challenge%2C%20we%20first%0Aintroduce%20a%20new%20parameter-efficient%20fine-tuning%20module%20%27Learn%20and%20Calibrate%27%2C%0Aor%20LuCA%2C%20designed%20to%20acquire%20knowledge%20through%20an%20adapter-calibrator%20couple%2C%0Aenabling%20effective%20adaptation%20with%20well-refined%20feature%20representations.%0ASecond%2C%20for%20each%20learning%20session%2C%20we%20deploy%20a%20sparse%20LuCA%20module%20on%20top%20of%20the%0Alast%20token%20just%20before%20the%20classifier%2C%20which%20we%20refer%20to%20as%20%27Token-level%20Sparse%0ACalibration%20and%20Adaptation%27%2C%20or%20TOSCA.%20This%20strategic%20design%20improves%20the%0Aorthogonality%20between%20the%20modules%20and%20significantly%20reduces%20both%20training%20and%0Ainference%20complexity.%20By%20leaving%20the%20generalization%20capabilities%20of%20the%0Apre-trained%20models%20intact%20and%20adapting%20exclusively%20via%20the%20last%20token%2C%20our%0Aapproach%20achieves%20a%20harmonious%20balance%20between%20stability%20and%20plasticity.%0AExtensive%20experiments%20demonstrate%20TOSCA%27s%20state-of-the-art%20performance%20while%0Aintroducing%20~8%20times%20fewer%20parameters%20compared%20to%20prior%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14762v1&entry.124074799=Read"},
{"title": "Transferable and Forecastable User Targeting Foundation Model", "author": "Bin Dou and Baokun Wang and Yun Zhu and Xiaotong Lin and Yike Xu and Xiaorui Huang and Yang Chen and Yun Liu and Shaoshuai Han and Yongchao Liu and Tianyi Zhang and Yu Cheng and Weiqiang Wang and Chuntao Hong", "abstract": "  User targeting, the process of selecting targeted users from a pool of\ncandidates for non-expert marketers, has garnered substantial attention with\nthe advancements in digital marketing. However, existing user targeting methods\nencounter two significant challenges: (i) Poor cross-domain and cross-scenario\ntransferability and generalization, and (ii) Insufficient forecastability in\nreal-world applications. These limitations hinder their applicability across\ndiverse industrial scenarios. In this work, we propose FOUND, an\nindustrial-grade, transferable, and forecastable user targeting foundation\nmodel. To enhance cross-domain transferability, our framework integrates\nheterogeneous multi-scenario user data, aligning them with one-sentence\ntargeting demand inputs through contrastive pre-training. For improved\nforecastability, the text description of each user is derived based on\nanticipated future behaviors, while user representations are constructed from\nhistorical information. Experimental results demonstrate that our approach\nsignificantly outperforms existing baselines in cross-domain, real-world user\ntargeting scenarios, showcasing the superior capabilities of FOUND. Moreover,\nour method has been successfully deployed on the Alipay platform and is widely\nutilized across various scenarios.\n", "link": "http://arxiv.org/abs/2412.12468v2", "date": "2025-02-20", "relevancy": 2.5277, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable%20and%20Forecastable%20User%20Targeting%20Foundation%20Model&body=Title%3A%20Transferable%20and%20Forecastable%20User%20Targeting%20Foundation%20Model%0AAuthor%3A%20Bin%20Dou%20and%20Baokun%20Wang%20and%20Yun%20Zhu%20and%20Xiaotong%20Lin%20and%20Yike%20Xu%20and%20Xiaorui%20Huang%20and%20Yang%20Chen%20and%20Yun%20Liu%20and%20Shaoshuai%20Han%20and%20Yongchao%20Liu%20and%20Tianyi%20Zhang%20and%20Yu%20Cheng%20and%20Weiqiang%20Wang%20and%20Chuntao%20Hong%0AAbstract%3A%20%20%20User%20targeting%2C%20the%20process%20of%20selecting%20targeted%20users%20from%20a%20pool%20of%0Acandidates%20for%20non-expert%20marketers%2C%20has%20garnered%20substantial%20attention%20with%0Athe%20advancements%20in%20digital%20marketing.%20However%2C%20existing%20user%20targeting%20methods%0Aencounter%20two%20significant%20challenges%3A%20%28i%29%20Poor%20cross-domain%20and%20cross-scenario%0Atransferability%20and%20generalization%2C%20and%20%28ii%29%20Insufficient%20forecastability%20in%0Areal-world%20applications.%20These%20limitations%20hinder%20their%20applicability%20across%0Adiverse%20industrial%20scenarios.%20In%20this%20work%2C%20we%20propose%20FOUND%2C%20an%0Aindustrial-grade%2C%20transferable%2C%20and%20forecastable%20user%20targeting%20foundation%0Amodel.%20To%20enhance%20cross-domain%20transferability%2C%20our%20framework%20integrates%0Aheterogeneous%20multi-scenario%20user%20data%2C%20aligning%20them%20with%20one-sentence%0Atargeting%20demand%20inputs%20through%20contrastive%20pre-training.%20For%20improved%0Aforecastability%2C%20the%20text%20description%20of%20each%20user%20is%20derived%20based%20on%0Aanticipated%20future%20behaviors%2C%20while%20user%20representations%20are%20constructed%20from%0Ahistorical%20information.%20Experimental%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20baselines%20in%20cross-domain%2C%20real-world%20user%0Atargeting%20scenarios%2C%20showcasing%20the%20superior%20capabilities%20of%20FOUND.%20Moreover%2C%0Aour%20method%20has%20been%20successfully%20deployed%20on%20the%20Alipay%20platform%20and%20is%20widely%0Autilized%20across%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable%2520and%2520Forecastable%2520User%2520Targeting%2520Foundation%2520Model%26entry.906535625%3DBin%2520Dou%2520and%2520Baokun%2520Wang%2520and%2520Yun%2520Zhu%2520and%2520Xiaotong%2520Lin%2520and%2520Yike%2520Xu%2520and%2520Xiaorui%2520Huang%2520and%2520Yang%2520Chen%2520and%2520Yun%2520Liu%2520and%2520Shaoshuai%2520Han%2520and%2520Yongchao%2520Liu%2520and%2520Tianyi%2520Zhang%2520and%2520Yu%2520Cheng%2520and%2520Weiqiang%2520Wang%2520and%2520Chuntao%2520Hong%26entry.1292438233%3D%2520%2520User%2520targeting%252C%2520the%2520process%2520of%2520selecting%2520targeted%2520users%2520from%2520a%2520pool%2520of%250Acandidates%2520for%2520non-expert%2520marketers%252C%2520has%2520garnered%2520substantial%2520attention%2520with%250Athe%2520advancements%2520in%2520digital%2520marketing.%2520However%252C%2520existing%2520user%2520targeting%2520methods%250Aencounter%2520two%2520significant%2520challenges%253A%2520%2528i%2529%2520Poor%2520cross-domain%2520and%2520cross-scenario%250Atransferability%2520and%2520generalization%252C%2520and%2520%2528ii%2529%2520Insufficient%2520forecastability%2520in%250Areal-world%2520applications.%2520These%2520limitations%2520hinder%2520their%2520applicability%2520across%250Adiverse%2520industrial%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520propose%2520FOUND%252C%2520an%250Aindustrial-grade%252C%2520transferable%252C%2520and%2520forecastable%2520user%2520targeting%2520foundation%250Amodel.%2520To%2520enhance%2520cross-domain%2520transferability%252C%2520our%2520framework%2520integrates%250Aheterogeneous%2520multi-scenario%2520user%2520data%252C%2520aligning%2520them%2520with%2520one-sentence%250Atargeting%2520demand%2520inputs%2520through%2520contrastive%2520pre-training.%2520For%2520improved%250Aforecastability%252C%2520the%2520text%2520description%2520of%2520each%2520user%2520is%2520derived%2520based%2520on%250Aanticipated%2520future%2520behaviors%252C%2520while%2520user%2520representations%2520are%2520constructed%2520from%250Ahistorical%2520information.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520existing%2520baselines%2520in%2520cross-domain%252C%2520real-world%2520user%250Atargeting%2520scenarios%252C%2520showcasing%2520the%2520superior%2520capabilities%2520of%2520FOUND.%2520Moreover%252C%250Aour%2520method%2520has%2520been%2520successfully%2520deployed%2520on%2520the%2520Alipay%2520platform%2520and%2520is%2520widely%250Autilized%2520across%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable%20and%20Forecastable%20User%20Targeting%20Foundation%20Model&entry.906535625=Bin%20Dou%20and%20Baokun%20Wang%20and%20Yun%20Zhu%20and%20Xiaotong%20Lin%20and%20Yike%20Xu%20and%20Xiaorui%20Huang%20and%20Yang%20Chen%20and%20Yun%20Liu%20and%20Shaoshuai%20Han%20and%20Yongchao%20Liu%20and%20Tianyi%20Zhang%20and%20Yu%20Cheng%20and%20Weiqiang%20Wang%20and%20Chuntao%20Hong&entry.1292438233=%20%20User%20targeting%2C%20the%20process%20of%20selecting%20targeted%20users%20from%20a%20pool%20of%0Acandidates%20for%20non-expert%20marketers%2C%20has%20garnered%20substantial%20attention%20with%0Athe%20advancements%20in%20digital%20marketing.%20However%2C%20existing%20user%20targeting%20methods%0Aencounter%20two%20significant%20challenges%3A%20%28i%29%20Poor%20cross-domain%20and%20cross-scenario%0Atransferability%20and%20generalization%2C%20and%20%28ii%29%20Insufficient%20forecastability%20in%0Areal-world%20applications.%20These%20limitations%20hinder%20their%20applicability%20across%0Adiverse%20industrial%20scenarios.%20In%20this%20work%2C%20we%20propose%20FOUND%2C%20an%0Aindustrial-grade%2C%20transferable%2C%20and%20forecastable%20user%20targeting%20foundation%0Amodel.%20To%20enhance%20cross-domain%20transferability%2C%20our%20framework%20integrates%0Aheterogeneous%20multi-scenario%20user%20data%2C%20aligning%20them%20with%20one-sentence%0Atargeting%20demand%20inputs%20through%20contrastive%20pre-training.%20For%20improved%0Aforecastability%2C%20the%20text%20description%20of%20each%20user%20is%20derived%20based%20on%0Aanticipated%20future%20behaviors%2C%20while%20user%20representations%20are%20constructed%20from%0Ahistorical%20information.%20Experimental%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20baselines%20in%20cross-domain%2C%20real-world%20user%0Atargeting%20scenarios%2C%20showcasing%20the%20superior%20capabilities%20of%20FOUND.%20Moreover%2C%0Aour%20method%20has%20been%20successfully%20deployed%20on%20the%20Alipay%20platform%20and%20is%20widely%0Autilized%20across%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12468v2&entry.124074799=Read"},
{"title": "Rapid Word Learning Through Meta In-Context Learning", "author": "Wentao Wang and Guangyuan Jiang and Tal Linzen and Brenden M. Lake", "abstract": "  Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.\n", "link": "http://arxiv.org/abs/2502.14791v1", "date": "2025-02-20", "relevancy": 2.5005, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning&body=Title%3A%20Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning%0AAuthor%3A%20Wentao%20Wang%20and%20Guangyuan%20Jiang%20and%20Tal%20Linzen%20and%20Brenden%20M.%20Lake%0AAbstract%3A%20%20%20Humans%20can%20quickly%20learn%20a%20new%20word%20from%20a%20few%20illustrative%20examples%2C%20and%0Athen%20systematically%20and%20flexibly%20use%20it%20in%20novel%20contexts.%20Yet%20the%20abilities%20of%0Acurrent%20language%20models%20for%20few-shot%20word%20learning%2C%20and%20methods%20for%20improving%0Athese%20abilities%2C%20are%20underexplored.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20method%2C%0AMeta-training%20for%20IN-context%20learNing%20Of%20Words%20%28Minnow%29.%20This%20method%20trains%0Alanguage%20models%20to%20generate%20new%20examples%20of%20a%20word%27s%20usage%20given%20a%20few%0Ain-context%20examples%2C%20using%20a%20special%20placeholder%20token%20to%20represent%20the%20new%0Aword.%20This%20training%20is%20repeated%20on%20many%20new%20words%20to%20develop%20a%20general%0Aword-learning%20ability.%20We%20find%20that%20training%20models%20from%20scratch%20with%20Minnow%20on%0Ahuman-scale%20child-directed%20language%20enables%20strong%20few-shot%20word%20learning%2C%0Acomparable%20to%20a%20large%20language%20model%20%28LLM%29%20pre-trained%20on%20orders%20of%20magnitude%0Amore%20data.%20Furthermore%2C%20through%20discriminative%20and%20generative%20evaluations%2C%20we%0Ademonstrate%20that%20finetuning%20pre-trained%20LLMs%20with%20Minnow%20improves%20their%20ability%0Ato%20discriminate%20between%20new%20words%2C%20identify%20syntactic%20categories%20of%20new%20words%2C%0Aand%20generate%20reasonable%20new%20usages%20and%20definitions%20for%20new%20words%2C%20based%20on%20one%0Aor%20a%20few%20in-context%20examples.%20These%20findings%20highlight%20the%20data%20efficiency%20of%0AMinnow%20and%20its%20potential%20to%20improve%20language%20model%20performance%20in%20word%20learning%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Word%2520Learning%2520Through%2520Meta%2520In-Context%2520Learning%26entry.906535625%3DWentao%2520Wang%2520and%2520Guangyuan%2520Jiang%2520and%2520Tal%2520Linzen%2520and%2520Brenden%2520M.%2520Lake%26entry.1292438233%3D%2520%2520Humans%2520can%2520quickly%2520learn%2520a%2520new%2520word%2520from%2520a%2520few%2520illustrative%2520examples%252C%2520and%250Athen%2520systematically%2520and%2520flexibly%2520use%2520it%2520in%2520novel%2520contexts.%2520Yet%2520the%2520abilities%2520of%250Acurrent%2520language%2520models%2520for%2520few-shot%2520word%2520learning%252C%2520and%2520methods%2520for%2520improving%250Athese%2520abilities%252C%2520are%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520method%252C%250AMeta-training%2520for%2520IN-context%2520learNing%2520Of%2520Words%2520%2528Minnow%2529.%2520This%2520method%2520trains%250Alanguage%2520models%2520to%2520generate%2520new%2520examples%2520of%2520a%2520word%2527s%2520usage%2520given%2520a%2520few%250Ain-context%2520examples%252C%2520using%2520a%2520special%2520placeholder%2520token%2520to%2520represent%2520the%2520new%250Aword.%2520This%2520training%2520is%2520repeated%2520on%2520many%2520new%2520words%2520to%2520develop%2520a%2520general%250Aword-learning%2520ability.%2520We%2520find%2520that%2520training%2520models%2520from%2520scratch%2520with%2520Minnow%2520on%250Ahuman-scale%2520child-directed%2520language%2520enables%2520strong%2520few-shot%2520word%2520learning%252C%250Acomparable%2520to%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520pre-trained%2520on%2520orders%2520of%2520magnitude%250Amore%2520data.%2520Furthermore%252C%2520through%2520discriminative%2520and%2520generative%2520evaluations%252C%2520we%250Ademonstrate%2520that%2520finetuning%2520pre-trained%2520LLMs%2520with%2520Minnow%2520improves%2520their%2520ability%250Ato%2520discriminate%2520between%2520new%2520words%252C%2520identify%2520syntactic%2520categories%2520of%2520new%2520words%252C%250Aand%2520generate%2520reasonable%2520new%2520usages%2520and%2520definitions%2520for%2520new%2520words%252C%2520based%2520on%2520one%250Aor%2520a%2520few%2520in-context%2520examples.%2520These%2520findings%2520highlight%2520the%2520data%2520efficiency%2520of%250AMinnow%2520and%2520its%2520potential%2520to%2520improve%2520language%2520model%2520performance%2520in%2520word%2520learning%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning&entry.906535625=Wentao%20Wang%20and%20Guangyuan%20Jiang%20and%20Tal%20Linzen%20and%20Brenden%20M.%20Lake&entry.1292438233=%20%20Humans%20can%20quickly%20learn%20a%20new%20word%20from%20a%20few%20illustrative%20examples%2C%20and%0Athen%20systematically%20and%20flexibly%20use%20it%20in%20novel%20contexts.%20Yet%20the%20abilities%20of%0Acurrent%20language%20models%20for%20few-shot%20word%20learning%2C%20and%20methods%20for%20improving%0Athese%20abilities%2C%20are%20underexplored.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20method%2C%0AMeta-training%20for%20IN-context%20learNing%20Of%20Words%20%28Minnow%29.%20This%20method%20trains%0Alanguage%20models%20to%20generate%20new%20examples%20of%20a%20word%27s%20usage%20given%20a%20few%0Ain-context%20examples%2C%20using%20a%20special%20placeholder%20token%20to%20represent%20the%20new%0Aword.%20This%20training%20is%20repeated%20on%20many%20new%20words%20to%20develop%20a%20general%0Aword-learning%20ability.%20We%20find%20that%20training%20models%20from%20scratch%20with%20Minnow%20on%0Ahuman-scale%20child-directed%20language%20enables%20strong%20few-shot%20word%20learning%2C%0Acomparable%20to%20a%20large%20language%20model%20%28LLM%29%20pre-trained%20on%20orders%20of%20magnitude%0Amore%20data.%20Furthermore%2C%20through%20discriminative%20and%20generative%20evaluations%2C%20we%0Ademonstrate%20that%20finetuning%20pre-trained%20LLMs%20with%20Minnow%20improves%20their%20ability%0Ato%20discriminate%20between%20new%20words%2C%20identify%20syntactic%20categories%20of%20new%20words%2C%0Aand%20generate%20reasonable%20new%20usages%20and%20definitions%20for%20new%20words%2C%20based%20on%20one%0Aor%20a%20few%20in-context%20examples.%20These%20findings%20highlight%20the%20data%20efficiency%20of%0AMinnow%20and%20its%20potential%20to%20improve%20language%20model%20performance%20in%20word%20learning%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14791v1&entry.124074799=Read"},
{"title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs", "author": "Yuchen Wu and Liang Ding and Li Shen and Dacheng Tao", "abstract": "  Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.\n", "link": "http://arxiv.org/abs/2502.14645v1", "date": "2025-02-20", "relevancy": 2.4732, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edit%20Once%2C%20Update%20Everywhere%3A%20A%20Simple%20Framework%20for%20Cross-Lingual%0A%20%20Knowledge%20Synchronization%20in%20LLMs&body=Title%3A%20Edit%20Once%2C%20Update%20Everywhere%3A%20A%20Simple%20Framework%20for%20Cross-Lingual%0A%20%20Knowledge%20Synchronization%20in%20LLMs%0AAuthor%3A%20Yuchen%20Wu%20and%20Liang%20Ding%20and%20Li%20Shen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Knowledge%20editing%20allows%20for%20efficient%20adaptation%20of%20large%20language%20models%0A%28LLMs%29%20to%20new%20information%20or%20corrections%20without%20requiring%20full%20retraining.%0AHowever%2C%20prior%20methods%20typically%20focus%20on%20either%20single-language%20editing%20or%0Abasic%20multilingual%20editing%2C%20failing%20to%20achieve%20true%20cross-linguistic%20knowledge%0Asynchronization.%20To%20address%20this%2C%20we%20present%20a%20simple%20and%20practical%0Astate-of-the-art%20%28SOTA%29%20recipe%20Cross-Lingual%20Knowledge%20Democracy%20Edit%20%28X-KDE%29%2C%0Adesigned%20to%20propagate%20knowledge%20from%20a%20dominant%20language%20to%20other%20languages%0Aeffectively.%20Our%20X-KDE%20comprises%20two%20stages%3A%20%28i%29%20Cross-lingual%20Edition%0AInstruction%20Tuning%20%28XE-IT%29%2C%20which%20fine-tunes%20the%20model%20on%20a%20curated%20parallel%0Adataset%20to%20modify%20in-scope%20knowledge%20while%20preserving%20unrelated%20information%2C%0Aand%20%28ii%29%20Target-language%20Preference%20Optimization%20%28TL-PO%29%2C%20which%20applies%0Aadvanced%20optimization%20techniques%20to%20ensure%20consistency%20across%20languages%2C%0Afostering%20the%20transfer%20of%20updates.%20Additionally%2C%20we%20contribute%20a%20high-quality%2C%0Across-lingual%20dataset%2C%20specifically%20designed%20to%20enhance%20knowledge%20transfer%0Aacross%20languages.%20Extensive%20experiments%20on%20the%20Bi-ZsRE%20and%20MzsRE%20benchmarks%0Ashow%20that%20X-KDE%20significantly%20enhances%20cross-lingual%20performance%2C%20achieving%20an%0Aaverage%20improvement%20of%20%2B8.19%25%2C%20while%20maintaining%20high%20accuracy%20in%20monolingual%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdit%2520Once%252C%2520Update%2520Everywhere%253A%2520A%2520Simple%2520Framework%2520for%2520Cross-Lingual%250A%2520%2520Knowledge%2520Synchronization%2520in%2520LLMs%26entry.906535625%3DYuchen%2520Wu%2520and%2520Liang%2520Ding%2520and%2520Li%2520Shen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Knowledge%2520editing%2520allows%2520for%2520efficient%2520adaptation%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520new%2520information%2520or%2520corrections%2520without%2520requiring%2520full%2520retraining.%250AHowever%252C%2520prior%2520methods%2520typically%2520focus%2520on%2520either%2520single-language%2520editing%2520or%250Abasic%2520multilingual%2520editing%252C%2520failing%2520to%2520achieve%2520true%2520cross-linguistic%2520knowledge%250Asynchronization.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520simple%2520and%2520practical%250Astate-of-the-art%2520%2528SOTA%2529%2520recipe%2520Cross-Lingual%2520Knowledge%2520Democracy%2520Edit%2520%2528X-KDE%2529%252C%250Adesigned%2520to%2520propagate%2520knowledge%2520from%2520a%2520dominant%2520language%2520to%2520other%2520languages%250Aeffectively.%2520Our%2520X-KDE%2520comprises%2520two%2520stages%253A%2520%2528i%2529%2520Cross-lingual%2520Edition%250AInstruction%2520Tuning%2520%2528XE-IT%2529%252C%2520which%2520fine-tunes%2520the%2520model%2520on%2520a%2520curated%2520parallel%250Adataset%2520to%2520modify%2520in-scope%2520knowledge%2520while%2520preserving%2520unrelated%2520information%252C%250Aand%2520%2528ii%2529%2520Target-language%2520Preference%2520Optimization%2520%2528TL-PO%2529%252C%2520which%2520applies%250Aadvanced%2520optimization%2520techniques%2520to%2520ensure%2520consistency%2520across%2520languages%252C%250Afostering%2520the%2520transfer%2520of%2520updates.%2520Additionally%252C%2520we%2520contribute%2520a%2520high-quality%252C%250Across-lingual%2520dataset%252C%2520specifically%2520designed%2520to%2520enhance%2520knowledge%2520transfer%250Aacross%2520languages.%2520Extensive%2520experiments%2520on%2520the%2520Bi-ZsRE%2520and%2520MzsRE%2520benchmarks%250Ashow%2520that%2520X-KDE%2520significantly%2520enhances%2520cross-lingual%2520performance%252C%2520achieving%2520an%250Aaverage%2520improvement%2520of%2520%252B8.19%2525%252C%2520while%2520maintaining%2520high%2520accuracy%2520in%2520monolingual%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edit%20Once%2C%20Update%20Everywhere%3A%20A%20Simple%20Framework%20for%20Cross-Lingual%0A%20%20Knowledge%20Synchronization%20in%20LLMs&entry.906535625=Yuchen%20Wu%20and%20Liang%20Ding%20and%20Li%20Shen%20and%20Dacheng%20Tao&entry.1292438233=%20%20Knowledge%20editing%20allows%20for%20efficient%20adaptation%20of%20large%20language%20models%0A%28LLMs%29%20to%20new%20information%20or%20corrections%20without%20requiring%20full%20retraining.%0AHowever%2C%20prior%20methods%20typically%20focus%20on%20either%20single-language%20editing%20or%0Abasic%20multilingual%20editing%2C%20failing%20to%20achieve%20true%20cross-linguistic%20knowledge%0Asynchronization.%20To%20address%20this%2C%20we%20present%20a%20simple%20and%20practical%0Astate-of-the-art%20%28SOTA%29%20recipe%20Cross-Lingual%20Knowledge%20Democracy%20Edit%20%28X-KDE%29%2C%0Adesigned%20to%20propagate%20knowledge%20from%20a%20dominant%20language%20to%20other%20languages%0Aeffectively.%20Our%20X-KDE%20comprises%20two%20stages%3A%20%28i%29%20Cross-lingual%20Edition%0AInstruction%20Tuning%20%28XE-IT%29%2C%20which%20fine-tunes%20the%20model%20on%20a%20curated%20parallel%0Adataset%20to%20modify%20in-scope%20knowledge%20while%20preserving%20unrelated%20information%2C%0Aand%20%28ii%29%20Target-language%20Preference%20Optimization%20%28TL-PO%29%2C%20which%20applies%0Aadvanced%20optimization%20techniques%20to%20ensure%20consistency%20across%20languages%2C%0Afostering%20the%20transfer%20of%20updates.%20Additionally%2C%20we%20contribute%20a%20high-quality%2C%0Across-lingual%20dataset%2C%20specifically%20designed%20to%20enhance%20knowledge%20transfer%0Aacross%20languages.%20Extensive%20experiments%20on%20the%20Bi-ZsRE%20and%20MzsRE%20benchmarks%0Ashow%20that%20X-KDE%20significantly%20enhances%20cross-lingual%20performance%2C%20achieving%20an%0Aaverage%20improvement%20of%20%2B8.19%25%2C%20while%20maintaining%20high%20accuracy%20in%20monolingual%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14645v1&entry.124074799=Read"},
{"title": "Data-Efficient Pretraining with Group-Level Data Influence Modeling", "author": "Zichun Yu and Fei Peng and Jie Lei and Arnold Overwijk and Wen-tau Yih and Chenyan Xiong", "abstract": "  Data-efficient pretraining has shown tremendous potential to elevate scaling\nlaws. This paper argues that effective pretraining data should be curated at\nthe group level, treating a set of data points as a whole rather than as\nindependent contributors. To achieve that, we propose Group-Level Data\nInfluence Modeling (Group-MATES), a novel data-efficient pretraining method\nthat captures and optimizes group-level data utility. Specifically, Group-MATES\ncollects oracle group-level influences by locally probing the pretraining model\nwith data sets. It then fine-tunes a relational data influence model to\napproximate oracles as relationship-weighted aggregations of individual\ninfluences. The fine-tuned model selects the data subset by maximizing its\ngroup-level influence prediction, with influence-aware clustering to enable\nefficient inference. Experiments on the DCLM benchmark demonstrate that\nGroup-MATES achieves a 10% relative core score improvement on 22 downstream\ntasks over DCLM-Baseline and 5% over individual-influence-based methods,\nestablishing a new state-of-the-art. Further analyses highlight the\neffectiveness of relational data influence models in capturing intricate\ninteractions between data points.\n", "link": "http://arxiv.org/abs/2502.14709v1", "date": "2025-02-20", "relevancy": 2.4667, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Pretraining%20with%20Group-Level%20Data%20Influence%20Modeling&body=Title%3A%20Data-Efficient%20Pretraining%20with%20Group-Level%20Data%20Influence%20Modeling%0AAuthor%3A%20Zichun%20Yu%20and%20Fei%20Peng%20and%20Jie%20Lei%20and%20Arnold%20Overwijk%20and%20Wen-tau%20Yih%20and%20Chenyan%20Xiong%0AAbstract%3A%20%20%20Data-efficient%20pretraining%20has%20shown%20tremendous%20potential%20to%20elevate%20scaling%0Alaws.%20This%20paper%20argues%20that%20effective%20pretraining%20data%20should%20be%20curated%20at%0Athe%20group%20level%2C%20treating%20a%20set%20of%20data%20points%20as%20a%20whole%20rather%20than%20as%0Aindependent%20contributors.%20To%20achieve%20that%2C%20we%20propose%20Group-Level%20Data%0AInfluence%20Modeling%20%28Group-MATES%29%2C%20a%20novel%20data-efficient%20pretraining%20method%0Athat%20captures%20and%20optimizes%20group-level%20data%20utility.%20Specifically%2C%20Group-MATES%0Acollects%20oracle%20group-level%20influences%20by%20locally%20probing%20the%20pretraining%20model%0Awith%20data%20sets.%20It%20then%20fine-tunes%20a%20relational%20data%20influence%20model%20to%0Aapproximate%20oracles%20as%20relationship-weighted%20aggregations%20of%20individual%0Ainfluences.%20The%20fine-tuned%20model%20selects%20the%20data%20subset%20by%20maximizing%20its%0Agroup-level%20influence%20prediction%2C%20with%20influence-aware%20clustering%20to%20enable%0Aefficient%20inference.%20Experiments%20on%20the%20DCLM%20benchmark%20demonstrate%20that%0AGroup-MATES%20achieves%20a%2010%25%20relative%20core%20score%20improvement%20on%2022%20downstream%0Atasks%20over%20DCLM-Baseline%20and%205%25%20over%20individual-influence-based%20methods%2C%0Aestablishing%20a%20new%20state-of-the-art.%20Further%20analyses%20highlight%20the%0Aeffectiveness%20of%20relational%20data%20influence%20models%20in%20capturing%20intricate%0Ainteractions%20between%20data%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Efficient%2520Pretraining%2520with%2520Group-Level%2520Data%2520Influence%2520Modeling%26entry.906535625%3DZichun%2520Yu%2520and%2520Fei%2520Peng%2520and%2520Jie%2520Lei%2520and%2520Arnold%2520Overwijk%2520and%2520Wen-tau%2520Yih%2520and%2520Chenyan%2520Xiong%26entry.1292438233%3D%2520%2520Data-efficient%2520pretraining%2520has%2520shown%2520tremendous%2520potential%2520to%2520elevate%2520scaling%250Alaws.%2520This%2520paper%2520argues%2520that%2520effective%2520pretraining%2520data%2520should%2520be%2520curated%2520at%250Athe%2520group%2520level%252C%2520treating%2520a%2520set%2520of%2520data%2520points%2520as%2520a%2520whole%2520rather%2520than%2520as%250Aindependent%2520contributors.%2520To%2520achieve%2520that%252C%2520we%2520propose%2520Group-Level%2520Data%250AInfluence%2520Modeling%2520%2528Group-MATES%2529%252C%2520a%2520novel%2520data-efficient%2520pretraining%2520method%250Athat%2520captures%2520and%2520optimizes%2520group-level%2520data%2520utility.%2520Specifically%252C%2520Group-MATES%250Acollects%2520oracle%2520group-level%2520influences%2520by%2520locally%2520probing%2520the%2520pretraining%2520model%250Awith%2520data%2520sets.%2520It%2520then%2520fine-tunes%2520a%2520relational%2520data%2520influence%2520model%2520to%250Aapproximate%2520oracles%2520as%2520relationship-weighted%2520aggregations%2520of%2520individual%250Ainfluences.%2520The%2520fine-tuned%2520model%2520selects%2520the%2520data%2520subset%2520by%2520maximizing%2520its%250Agroup-level%2520influence%2520prediction%252C%2520with%2520influence-aware%2520clustering%2520to%2520enable%250Aefficient%2520inference.%2520Experiments%2520on%2520the%2520DCLM%2520benchmark%2520demonstrate%2520that%250AGroup-MATES%2520achieves%2520a%252010%2525%2520relative%2520core%2520score%2520improvement%2520on%252022%2520downstream%250Atasks%2520over%2520DCLM-Baseline%2520and%25205%2525%2520over%2520individual-influence-based%2520methods%252C%250Aestablishing%2520a%2520new%2520state-of-the-art.%2520Further%2520analyses%2520highlight%2520the%250Aeffectiveness%2520of%2520relational%2520data%2520influence%2520models%2520in%2520capturing%2520intricate%250Ainteractions%2520between%2520data%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Pretraining%20with%20Group-Level%20Data%20Influence%20Modeling&entry.906535625=Zichun%20Yu%20and%20Fei%20Peng%20and%20Jie%20Lei%20and%20Arnold%20Overwijk%20and%20Wen-tau%20Yih%20and%20Chenyan%20Xiong&entry.1292438233=%20%20Data-efficient%20pretraining%20has%20shown%20tremendous%20potential%20to%20elevate%20scaling%0Alaws.%20This%20paper%20argues%20that%20effective%20pretraining%20data%20should%20be%20curated%20at%0Athe%20group%20level%2C%20treating%20a%20set%20of%20data%20points%20as%20a%20whole%20rather%20than%20as%0Aindependent%20contributors.%20To%20achieve%20that%2C%20we%20propose%20Group-Level%20Data%0AInfluence%20Modeling%20%28Group-MATES%29%2C%20a%20novel%20data-efficient%20pretraining%20method%0Athat%20captures%20and%20optimizes%20group-level%20data%20utility.%20Specifically%2C%20Group-MATES%0Acollects%20oracle%20group-level%20influences%20by%20locally%20probing%20the%20pretraining%20model%0Awith%20data%20sets.%20It%20then%20fine-tunes%20a%20relational%20data%20influence%20model%20to%0Aapproximate%20oracles%20as%20relationship-weighted%20aggregations%20of%20individual%0Ainfluences.%20The%20fine-tuned%20model%20selects%20the%20data%20subset%20by%20maximizing%20its%0Agroup-level%20influence%20prediction%2C%20with%20influence-aware%20clustering%20to%20enable%0Aefficient%20inference.%20Experiments%20on%20the%20DCLM%20benchmark%20demonstrate%20that%0AGroup-MATES%20achieves%20a%2010%25%20relative%20core%20score%20improvement%20on%2022%20downstream%0Atasks%20over%20DCLM-Baseline%20and%205%25%20over%20individual-influence-based%20methods%2C%0Aestablishing%20a%20new%20state-of-the-art.%20Further%20analyses%20highlight%20the%0Aeffectiveness%20of%20relational%20data%20influence%20models%20in%20capturing%20intricate%0Ainteractions%20between%20data%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14709v1&entry.124074799=Read"},
{"title": "Meshless Shape Optimization using Neural Networks and Partial\n  Differential Equations on Graphs", "author": "Eloi Martinet and Leon Bungert", "abstract": "  Shape optimization involves the minimization of a cost function defined over\na set of shapes, often governed by a partial differential equation (PDE). In\nthe absence of closed-form solutions, one relies on numerical methods to\napproximate the solution. The level set method -- when coupled with the finite\nelement method -- is one of the most versatile numerical shape optimization\napproaches but still suffers from the limitations of most mesh-based methods.\nIn this work, we present a fully meshless level set framework that leverages\nneural networks to parameterize the level set function and employs the graph\nLaplacian to approximate the underlying PDE. Our approach enables precise\ncomputations of geometric quantities such as surface normals and curvature, and\nallows tackling optimization problems within the class of convex shapes.\n", "link": "http://arxiv.org/abs/2502.14821v1", "date": "2025-02-20", "relevancy": 2.4652, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5449}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4767}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meshless%20Shape%20Optimization%20using%20Neural%20Networks%20and%20Partial%0A%20%20Differential%20Equations%20on%20Graphs&body=Title%3A%20Meshless%20Shape%20Optimization%20using%20Neural%20Networks%20and%20Partial%0A%20%20Differential%20Equations%20on%20Graphs%0AAuthor%3A%20Eloi%20Martinet%20and%20Leon%20Bungert%0AAbstract%3A%20%20%20Shape%20optimization%20involves%20the%20minimization%20of%20a%20cost%20function%20defined%20over%0Aa%20set%20of%20shapes%2C%20often%20governed%20by%20a%20partial%20differential%20equation%20%28PDE%29.%20In%0Athe%20absence%20of%20closed-form%20solutions%2C%20one%20relies%20on%20numerical%20methods%20to%0Aapproximate%20the%20solution.%20The%20level%20set%20method%20--%20when%20coupled%20with%20the%20finite%0Aelement%20method%20--%20is%20one%20of%20the%20most%20versatile%20numerical%20shape%20optimization%0Aapproaches%20but%20still%20suffers%20from%20the%20limitations%20of%20most%20mesh-based%20methods.%0AIn%20this%20work%2C%20we%20present%20a%20fully%20meshless%20level%20set%20framework%20that%20leverages%0Aneural%20networks%20to%20parameterize%20the%20level%20set%20function%20and%20employs%20the%20graph%0ALaplacian%20to%20approximate%20the%20underlying%20PDE.%20Our%20approach%20enables%20precise%0Acomputations%20of%20geometric%20quantities%20such%20as%20surface%20normals%20and%20curvature%2C%20and%0Aallows%20tackling%20optimization%20problems%20within%20the%20class%20of%20convex%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshless%2520Shape%2520Optimization%2520using%2520Neural%2520Networks%2520and%2520Partial%250A%2520%2520Differential%2520Equations%2520on%2520Graphs%26entry.906535625%3DEloi%2520Martinet%2520and%2520Leon%2520Bungert%26entry.1292438233%3D%2520%2520Shape%2520optimization%2520involves%2520the%2520minimization%2520of%2520a%2520cost%2520function%2520defined%2520over%250Aa%2520set%2520of%2520shapes%252C%2520often%2520governed%2520by%2520a%2520partial%2520differential%2520equation%2520%2528PDE%2529.%2520In%250Athe%2520absence%2520of%2520closed-form%2520solutions%252C%2520one%2520relies%2520on%2520numerical%2520methods%2520to%250Aapproximate%2520the%2520solution.%2520The%2520level%2520set%2520method%2520--%2520when%2520coupled%2520with%2520the%2520finite%250Aelement%2520method%2520--%2520is%2520one%2520of%2520the%2520most%2520versatile%2520numerical%2520shape%2520optimization%250Aapproaches%2520but%2520still%2520suffers%2520from%2520the%2520limitations%2520of%2520most%2520mesh-based%2520methods.%250AIn%2520this%2520work%252C%2520we%2520present%2520a%2520fully%2520meshless%2520level%2520set%2520framework%2520that%2520leverages%250Aneural%2520networks%2520to%2520parameterize%2520the%2520level%2520set%2520function%2520and%2520employs%2520the%2520graph%250ALaplacian%2520to%2520approximate%2520the%2520underlying%2520PDE.%2520Our%2520approach%2520enables%2520precise%250Acomputations%2520of%2520geometric%2520quantities%2520such%2520as%2520surface%2520normals%2520and%2520curvature%252C%2520and%250Aallows%2520tackling%2520optimization%2520problems%2520within%2520the%2520class%2520of%2520convex%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meshless%20Shape%20Optimization%20using%20Neural%20Networks%20and%20Partial%0A%20%20Differential%20Equations%20on%20Graphs&entry.906535625=Eloi%20Martinet%20and%20Leon%20Bungert&entry.1292438233=%20%20Shape%20optimization%20involves%20the%20minimization%20of%20a%20cost%20function%20defined%20over%0Aa%20set%20of%20shapes%2C%20often%20governed%20by%20a%20partial%20differential%20equation%20%28PDE%29.%20In%0Athe%20absence%20of%20closed-form%20solutions%2C%20one%20relies%20on%20numerical%20methods%20to%0Aapproximate%20the%20solution.%20The%20level%20set%20method%20--%20when%20coupled%20with%20the%20finite%0Aelement%20method%20--%20is%20one%20of%20the%20most%20versatile%20numerical%20shape%20optimization%0Aapproaches%20but%20still%20suffers%20from%20the%20limitations%20of%20most%20mesh-based%20methods.%0AIn%20this%20work%2C%20we%20present%20a%20fully%20meshless%20level%20set%20framework%20that%20leverages%0Aneural%20networks%20to%20parameterize%20the%20level%20set%20function%20and%20employs%20the%20graph%0ALaplacian%20to%20approximate%20the%20underlying%20PDE.%20Our%20approach%20enables%20precise%0Acomputations%20of%20geometric%20quantities%20such%20as%20surface%20normals%20and%20curvature%2C%20and%0Aallows%20tackling%20optimization%20problems%20within%20the%20class%20of%20convex%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14821v1&entry.124074799=Read"},
{"title": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling", "author": "Weilin Zhao and Tengyu Pan and Xu Han and Yudi Zhang and Ao Sun and Yuxiang Huang and Kaihuo Zhang and Weilun Zhao and Yuxuan Li and Jianyong Wang and Zhiyuan Liu and Maosong Sun", "abstract": "  Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2.\n", "link": "http://arxiv.org/abs/2502.14856v1", "date": "2025-02-20", "relevancy": 2.4574, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5226}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FR-Spec%3A%20Accelerating%20Large-Vocabulary%20Language%20Models%20via%0A%20%20Frequency-Ranked%20Speculative%20Sampling&body=Title%3A%20FR-Spec%3A%20Accelerating%20Large-Vocabulary%20Language%20Models%20via%0A%20%20Frequency-Ranked%20Speculative%20Sampling%0AAuthor%3A%20Weilin%20Zhao%20and%20Tengyu%20Pan%20and%20Xu%20Han%20and%20Yudi%20Zhang%20and%20Ao%20Sun%20and%20Yuxiang%20Huang%20and%20Kaihuo%20Zhang%20and%20Weilun%20Zhao%20and%20Yuxuan%20Li%20and%20Jianyong%20Wang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Speculative%20sampling%20has%20emerged%20as%20an%20important%20technique%20for%20accelerating%0Athe%20auto-regressive%20generation%20process%20of%20large%20language%20models%20%28LLMs%29%20by%0Autilizing%20a%20draft-then-verify%20mechanism%20to%20produce%20multiple%20tokens%20per%20forward%0Apass.%20While%20state-of-the-art%20speculative%20sampling%20methods%20use%20only%20a%20single%0Alayer%20and%20a%20language%20modeling%20%28LM%29%20head%20as%20the%20draft%20model%20to%20achieve%0Aimpressive%20layer%20compression%2C%20their%20efficiency%20gains%20are%20substantially%20reduced%0Afor%20large-vocabulary%20LLMs%2C%20such%20as%20Llama-3-8B%20with%20a%20vocabulary%20of%20128k%20tokens.%0ATo%20address%20this%2C%20we%20present%20FR-Spec%2C%20a%20frequency-ranked%20speculative%20sampling%0Aframework%20that%20optimizes%20draft%20candidate%20selection%20through%20vocabulary%20space%0Acompression.%20By%20constraining%20the%20draft%20search%20to%20a%20frequency-prioritized%20token%0Asubset%2C%20our%20method%20reduces%20LM%20Head%20computation%20overhead%20by%2075%25%20while%20ensuring%0Athe%20equivalence%20of%20the%20final%20output%20distribution.%20Experiments%20across%20multiple%0Adatasets%20demonstrate%20an%20average%20of%201.12%24%5Ctimes%24%20speedup%20over%20the%0Astate-of-the-art%20speculative%20sampling%20method%20EAGLE-2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFR-Spec%253A%2520Accelerating%2520Large-Vocabulary%2520Language%2520Models%2520via%250A%2520%2520Frequency-Ranked%2520Speculative%2520Sampling%26entry.906535625%3DWeilin%2520Zhao%2520and%2520Tengyu%2520Pan%2520and%2520Xu%2520Han%2520and%2520Yudi%2520Zhang%2520and%2520Ao%2520Sun%2520and%2520Yuxiang%2520Huang%2520and%2520Kaihuo%2520Zhang%2520and%2520Weilun%2520Zhao%2520and%2520Yuxuan%2520Li%2520and%2520Jianyong%2520Wang%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Speculative%2520sampling%2520has%2520emerged%2520as%2520an%2520important%2520technique%2520for%2520accelerating%250Athe%2520auto-regressive%2520generation%2520process%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%250Autilizing%2520a%2520draft-then-verify%2520mechanism%2520to%2520produce%2520multiple%2520tokens%2520per%2520forward%250Apass.%2520While%2520state-of-the-art%2520speculative%2520sampling%2520methods%2520use%2520only%2520a%2520single%250Alayer%2520and%2520a%2520language%2520modeling%2520%2528LM%2529%2520head%2520as%2520the%2520draft%2520model%2520to%2520achieve%250Aimpressive%2520layer%2520compression%252C%2520their%2520efficiency%2520gains%2520are%2520substantially%2520reduced%250Afor%2520large-vocabulary%2520LLMs%252C%2520such%2520as%2520Llama-3-8B%2520with%2520a%2520vocabulary%2520of%2520128k%2520tokens.%250ATo%2520address%2520this%252C%2520we%2520present%2520FR-Spec%252C%2520a%2520frequency-ranked%2520speculative%2520sampling%250Aframework%2520that%2520optimizes%2520draft%2520candidate%2520selection%2520through%2520vocabulary%2520space%250Acompression.%2520By%2520constraining%2520the%2520draft%2520search%2520to%2520a%2520frequency-prioritized%2520token%250Asubset%252C%2520our%2520method%2520reduces%2520LM%2520Head%2520computation%2520overhead%2520by%252075%2525%2520while%2520ensuring%250Athe%2520equivalence%2520of%2520the%2520final%2520output%2520distribution.%2520Experiments%2520across%2520multiple%250Adatasets%2520demonstrate%2520an%2520average%2520of%25201.12%2524%255Ctimes%2524%2520speedup%2520over%2520the%250Astate-of-the-art%2520speculative%2520sampling%2520method%2520EAGLE-2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FR-Spec%3A%20Accelerating%20Large-Vocabulary%20Language%20Models%20via%0A%20%20Frequency-Ranked%20Speculative%20Sampling&entry.906535625=Weilin%20Zhao%20and%20Tengyu%20Pan%20and%20Xu%20Han%20and%20Yudi%20Zhang%20and%20Ao%20Sun%20and%20Yuxiang%20Huang%20and%20Kaihuo%20Zhang%20and%20Weilun%20Zhao%20and%20Yuxuan%20Li%20and%20Jianyong%20Wang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Speculative%20sampling%20has%20emerged%20as%20an%20important%20technique%20for%20accelerating%0Athe%20auto-regressive%20generation%20process%20of%20large%20language%20models%20%28LLMs%29%20by%0Autilizing%20a%20draft-then-verify%20mechanism%20to%20produce%20multiple%20tokens%20per%20forward%0Apass.%20While%20state-of-the-art%20speculative%20sampling%20methods%20use%20only%20a%20single%0Alayer%20and%20a%20language%20modeling%20%28LM%29%20head%20as%20the%20draft%20model%20to%20achieve%0Aimpressive%20layer%20compression%2C%20their%20efficiency%20gains%20are%20substantially%20reduced%0Afor%20large-vocabulary%20LLMs%2C%20such%20as%20Llama-3-8B%20with%20a%20vocabulary%20of%20128k%20tokens.%0ATo%20address%20this%2C%20we%20present%20FR-Spec%2C%20a%20frequency-ranked%20speculative%20sampling%0Aframework%20that%20optimizes%20draft%20candidate%20selection%20through%20vocabulary%20space%0Acompression.%20By%20constraining%20the%20draft%20search%20to%20a%20frequency-prioritized%20token%0Asubset%2C%20our%20method%20reduces%20LM%20Head%20computation%20overhead%20by%2075%25%20while%20ensuring%0Athe%20equivalence%20of%20the%20final%20output%20distribution.%20Experiments%20across%20multiple%0Adatasets%20demonstrate%20an%20average%20of%201.12%24%5Ctimes%24%20speedup%20over%20the%0Astate-of-the-art%20speculative%20sampling%20method%20EAGLE-2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14856v1&entry.124074799=Read"},
{"title": "Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration", "author": "Pengxiang Ding and Jianfei Ma and Xinyang Tong and Binghong Zou and Xinxin Luo and Yiguo Fan and Ting Wang and Hongchao Lu and Panzhong Mo and Jinxin Liu and Yuefan Wang and Huaicheng Zhou and Wenshuo Feng and Jiacheng Liu and Siteng Huang and Donglin Wang", "abstract": "  This paper addresses the limitations of current humanoid robot control\nframeworks, which primarily rely on reactive mechanisms and lack autonomous\ninteraction capabilities due to data scarcity. We propose Humanoid-VLA, a novel\nframework that integrates language understanding, egocentric scene perception,\nand motion control, enabling universal humanoid control. Humanoid-VLA begins\nwith language-motion pre-alignment using non-egocentric human motion datasets\npaired with textual descriptions, allowing the model to learn universal motion\npatterns and action semantics. We then incorporate egocentric visual context\nthrough a parameter efficient video-conditioned fine-tuning, enabling\ncontext-aware motion generation. Furthermore, we introduce a self-supervised\ndata augmentation strategy that automatically generates pseudoannotations\ndirectly derived from motion data. This process converts raw motion sequences\ninto informative question-answer pairs, facilitating the effective use of\nlarge-scale unlabeled video data. Built upon whole-body control architectures,\nextensive experiments show that Humanoid-VLA achieves object interaction and\nenvironment exploration tasks with enhanced contextual awareness, demonstrating\na more human-like capacity for adaptive and intelligent engagement.\n", "link": "http://arxiv.org/abs/2502.14795v1", "date": "2025-02-20", "relevancy": 2.4442, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6376}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5998}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Humanoid-VLA%3A%20Towards%20Universal%20Humanoid%20Control%20with%20Visual%20Integration&body=Title%3A%20Humanoid-VLA%3A%20Towards%20Universal%20Humanoid%20Control%20with%20Visual%20Integration%0AAuthor%3A%20Pengxiang%20Ding%20and%20Jianfei%20Ma%20and%20Xinyang%20Tong%20and%20Binghong%20Zou%20and%20Xinxin%20Luo%20and%20Yiguo%20Fan%20and%20Ting%20Wang%20and%20Hongchao%20Lu%20and%20Panzhong%20Mo%20and%20Jinxin%20Liu%20and%20Yuefan%20Wang%20and%20Huaicheng%20Zhou%20and%20Wenshuo%20Feng%20and%20Jiacheng%20Liu%20and%20Siteng%20Huang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20limitations%20of%20current%20humanoid%20robot%20control%0Aframeworks%2C%20which%20primarily%20rely%20on%20reactive%20mechanisms%20and%20lack%20autonomous%0Ainteraction%20capabilities%20due%20to%20data%20scarcity.%20We%20propose%20Humanoid-VLA%2C%20a%20novel%0Aframework%20that%20integrates%20language%20understanding%2C%20egocentric%20scene%20perception%2C%0Aand%20motion%20control%2C%20enabling%20universal%20humanoid%20control.%20Humanoid-VLA%20begins%0Awith%20language-motion%20pre-alignment%20using%20non-egocentric%20human%20motion%20datasets%0Apaired%20with%20textual%20descriptions%2C%20allowing%20the%20model%20to%20learn%20universal%20motion%0Apatterns%20and%20action%20semantics.%20We%20then%20incorporate%20egocentric%20visual%20context%0Athrough%20a%20parameter%20efficient%20video-conditioned%20fine-tuning%2C%20enabling%0Acontext-aware%20motion%20generation.%20Furthermore%2C%20we%20introduce%20a%20self-supervised%0Adata%20augmentation%20strategy%20that%20automatically%20generates%20pseudoannotations%0Adirectly%20derived%20from%20motion%20data.%20This%20process%20converts%20raw%20motion%20sequences%0Ainto%20informative%20question-answer%20pairs%2C%20facilitating%20the%20effective%20use%20of%0Alarge-scale%20unlabeled%20video%20data.%20Built%20upon%20whole-body%20control%20architectures%2C%0Aextensive%20experiments%20show%20that%20Humanoid-VLA%20achieves%20object%20interaction%20and%0Aenvironment%20exploration%20tasks%20with%20enhanced%20contextual%20awareness%2C%20demonstrating%0Aa%20more%20human-like%20capacity%20for%20adaptive%20and%20intelligent%20engagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanoid-VLA%253A%2520Towards%2520Universal%2520Humanoid%2520Control%2520with%2520Visual%2520Integration%26entry.906535625%3DPengxiang%2520Ding%2520and%2520Jianfei%2520Ma%2520and%2520Xinyang%2520Tong%2520and%2520Binghong%2520Zou%2520and%2520Xinxin%2520Luo%2520and%2520Yiguo%2520Fan%2520and%2520Ting%2520Wang%2520and%2520Hongchao%2520Lu%2520and%2520Panzhong%2520Mo%2520and%2520Jinxin%2520Liu%2520and%2520Yuefan%2520Wang%2520and%2520Huaicheng%2520Zhou%2520and%2520Wenshuo%2520Feng%2520and%2520Jiacheng%2520Liu%2520and%2520Siteng%2520Huang%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520limitations%2520of%2520current%2520humanoid%2520robot%2520control%250Aframeworks%252C%2520which%2520primarily%2520rely%2520on%2520reactive%2520mechanisms%2520and%2520lack%2520autonomous%250Ainteraction%2520capabilities%2520due%2520to%2520data%2520scarcity.%2520We%2520propose%2520Humanoid-VLA%252C%2520a%2520novel%250Aframework%2520that%2520integrates%2520language%2520understanding%252C%2520egocentric%2520scene%2520perception%252C%250Aand%2520motion%2520control%252C%2520enabling%2520universal%2520humanoid%2520control.%2520Humanoid-VLA%2520begins%250Awith%2520language-motion%2520pre-alignment%2520using%2520non-egocentric%2520human%2520motion%2520datasets%250Apaired%2520with%2520textual%2520descriptions%252C%2520allowing%2520the%2520model%2520to%2520learn%2520universal%2520motion%250Apatterns%2520and%2520action%2520semantics.%2520We%2520then%2520incorporate%2520egocentric%2520visual%2520context%250Athrough%2520a%2520parameter%2520efficient%2520video-conditioned%2520fine-tuning%252C%2520enabling%250Acontext-aware%2520motion%2520generation.%2520Furthermore%252C%2520we%2520introduce%2520a%2520self-supervised%250Adata%2520augmentation%2520strategy%2520that%2520automatically%2520generates%2520pseudoannotations%250Adirectly%2520derived%2520from%2520motion%2520data.%2520This%2520process%2520converts%2520raw%2520motion%2520sequences%250Ainto%2520informative%2520question-answer%2520pairs%252C%2520facilitating%2520the%2520effective%2520use%2520of%250Alarge-scale%2520unlabeled%2520video%2520data.%2520Built%2520upon%2520whole-body%2520control%2520architectures%252C%250Aextensive%2520experiments%2520show%2520that%2520Humanoid-VLA%2520achieves%2520object%2520interaction%2520and%250Aenvironment%2520exploration%2520tasks%2520with%2520enhanced%2520contextual%2520awareness%252C%2520demonstrating%250Aa%2520more%2520human-like%2520capacity%2520for%2520adaptive%2520and%2520intelligent%2520engagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humanoid-VLA%3A%20Towards%20Universal%20Humanoid%20Control%20with%20Visual%20Integration&entry.906535625=Pengxiang%20Ding%20and%20Jianfei%20Ma%20and%20Xinyang%20Tong%20and%20Binghong%20Zou%20and%20Xinxin%20Luo%20and%20Yiguo%20Fan%20and%20Ting%20Wang%20and%20Hongchao%20Lu%20and%20Panzhong%20Mo%20and%20Jinxin%20Liu%20and%20Yuefan%20Wang%20and%20Huaicheng%20Zhou%20and%20Wenshuo%20Feng%20and%20Jiacheng%20Liu%20and%20Siteng%20Huang%20and%20Donglin%20Wang&entry.1292438233=%20%20This%20paper%20addresses%20the%20limitations%20of%20current%20humanoid%20robot%20control%0Aframeworks%2C%20which%20primarily%20rely%20on%20reactive%20mechanisms%20and%20lack%20autonomous%0Ainteraction%20capabilities%20due%20to%20data%20scarcity.%20We%20propose%20Humanoid-VLA%2C%20a%20novel%0Aframework%20that%20integrates%20language%20understanding%2C%20egocentric%20scene%20perception%2C%0Aand%20motion%20control%2C%20enabling%20universal%20humanoid%20control.%20Humanoid-VLA%20begins%0Awith%20language-motion%20pre-alignment%20using%20non-egocentric%20human%20motion%20datasets%0Apaired%20with%20textual%20descriptions%2C%20allowing%20the%20model%20to%20learn%20universal%20motion%0Apatterns%20and%20action%20semantics.%20We%20then%20incorporate%20egocentric%20visual%20context%0Athrough%20a%20parameter%20efficient%20video-conditioned%20fine-tuning%2C%20enabling%0Acontext-aware%20motion%20generation.%20Furthermore%2C%20we%20introduce%20a%20self-supervised%0Adata%20augmentation%20strategy%20that%20automatically%20generates%20pseudoannotations%0Adirectly%20derived%20from%20motion%20data.%20This%20process%20converts%20raw%20motion%20sequences%0Ainto%20informative%20question-answer%20pairs%2C%20facilitating%20the%20effective%20use%20of%0Alarge-scale%20unlabeled%20video%20data.%20Built%20upon%20whole-body%20control%20architectures%2C%0Aextensive%20experiments%20show%20that%20Humanoid-VLA%20achieves%20object%20interaction%20and%0Aenvironment%20exploration%20tasks%20with%20enhanced%20contextual%20awareness%2C%20demonstrating%0Aa%20more%20human-like%20capacity%20for%20adaptive%20and%20intelligent%20engagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14795v1&entry.124074799=Read"},
{"title": "From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT", "author": "Ahmed Abdeen Hamed and Byung Suk Lee", "abstract": "  The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%).\n", "link": "http://arxiv.org/abs/2502.14714v1", "date": "2025-02-20", "relevancy": 2.4191, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5143}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.47}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Knowledge%20Generation%20to%20Knowledge%20Verification%3A%20Examining%20the%0A%20%20BioMedical%20Generative%20Capabilities%20of%20ChatGPT&body=Title%3A%20From%20Knowledge%20Generation%20to%20Knowledge%20Verification%3A%20Examining%20the%0A%20%20BioMedical%20Generative%20Capabilities%20of%20ChatGPT%0AAuthor%3A%20Ahmed%20Abdeen%20Hamed%20and%20Byung%20Suk%20Lee%0AAbstract%3A%20%20%20The%20generative%20capabilities%20of%20LLM%20models%20present%20opportunities%20in%0Aaccelerating%20tasks%20and%20concerns%20with%20the%20authenticity%20of%20the%20knowledge%20it%0Aproduces.%20To%20address%20the%20concerns%2C%20we%20present%20a%20computational%20approach%20that%0Asystematically%20evaluates%20the%20factual%20accuracy%20of%20biomedical%20knowledge%20that%20an%0ALLM%20model%20has%20been%20prompted%20to%20generate.%20Our%20approach%20encompasses%20two%0Aprocesses%3A%20the%20generation%20of%20disease-centric%20associations%20and%20the%20verification%0Aof%20them%20using%20the%20semantic%20knowledge%20of%20the%20biomedical%20ontologies.%20Using%0AChatGPT%20as%20the%20select%20LLM%20model%2C%20we%20designed%20a%20set%20of%20prompt-engineering%0Aprocesses%20to%20generate%20linkages%20between%20diseases%2C%20drugs%2C%20symptoms%2C%20and%20genes%20to%0Aestablish%20grounds%20for%20assessments.%20Experimental%20results%20demonstrate%20high%0Aaccuracy%20in%20identifying%20disease%20terms%20%2888%25-97%25%29%2C%20drug%20names%20%2890%25-91%25%29%2C%20and%0Agenetic%20information%20%2888%25-98%25%29.%20The%20symptom%20term%20identification%20accuracy%20was%0Anotably%20lower%20%2849%25-61%25%29%2C%20as%20verified%20against%20the%20DOID%2C%20ChEBI%2C%20SYMPTOM%2C%20and%20GO%0Aontologies%20accordingly.%20The%20verification%20of%20associations%20reveals%20literature%0Acoverage%20rates%20of%20%2889%25-91%25%29%20among%20disease-drug%20and%20disease-gene%20associations.%0AThe%20low%20identification%20accuracy%20for%20symptom%20terms%20also%20contributed%20to%20the%0Averification%20of%20symptom-related%20associations%20%2849%25-62%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Knowledge%2520Generation%2520to%2520Knowledge%2520Verification%253A%2520Examining%2520the%250A%2520%2520BioMedical%2520Generative%2520Capabilities%2520of%2520ChatGPT%26entry.906535625%3DAhmed%2520Abdeen%2520Hamed%2520and%2520Byung%2520Suk%2520Lee%26entry.1292438233%3D%2520%2520The%2520generative%2520capabilities%2520of%2520LLM%2520models%2520present%2520opportunities%2520in%250Aaccelerating%2520tasks%2520and%2520concerns%2520with%2520the%2520authenticity%2520of%2520the%2520knowledge%2520it%250Aproduces.%2520To%2520address%2520the%2520concerns%252C%2520we%2520present%2520a%2520computational%2520approach%2520that%250Asystematically%2520evaluates%2520the%2520factual%2520accuracy%2520of%2520biomedical%2520knowledge%2520that%2520an%250ALLM%2520model%2520has%2520been%2520prompted%2520to%2520generate.%2520Our%2520approach%2520encompasses%2520two%250Aprocesses%253A%2520the%2520generation%2520of%2520disease-centric%2520associations%2520and%2520the%2520verification%250Aof%2520them%2520using%2520the%2520semantic%2520knowledge%2520of%2520the%2520biomedical%2520ontologies.%2520Using%250AChatGPT%2520as%2520the%2520select%2520LLM%2520model%252C%2520we%2520designed%2520a%2520set%2520of%2520prompt-engineering%250Aprocesses%2520to%2520generate%2520linkages%2520between%2520diseases%252C%2520drugs%252C%2520symptoms%252C%2520and%2520genes%2520to%250Aestablish%2520grounds%2520for%2520assessments.%2520Experimental%2520results%2520demonstrate%2520high%250Aaccuracy%2520in%2520identifying%2520disease%2520terms%2520%252888%2525-97%2525%2529%252C%2520drug%2520names%2520%252890%2525-91%2525%2529%252C%2520and%250Agenetic%2520information%2520%252888%2525-98%2525%2529.%2520The%2520symptom%2520term%2520identification%2520accuracy%2520was%250Anotably%2520lower%2520%252849%2525-61%2525%2529%252C%2520as%2520verified%2520against%2520the%2520DOID%252C%2520ChEBI%252C%2520SYMPTOM%252C%2520and%2520GO%250Aontologies%2520accordingly.%2520The%2520verification%2520of%2520associations%2520reveals%2520literature%250Acoverage%2520rates%2520of%2520%252889%2525-91%2525%2529%2520among%2520disease-drug%2520and%2520disease-gene%2520associations.%250AThe%2520low%2520identification%2520accuracy%2520for%2520symptom%2520terms%2520also%2520contributed%2520to%2520the%250Averification%2520of%2520symptom-related%2520associations%2520%252849%2525-62%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Knowledge%20Generation%20to%20Knowledge%20Verification%3A%20Examining%20the%0A%20%20BioMedical%20Generative%20Capabilities%20of%20ChatGPT&entry.906535625=Ahmed%20Abdeen%20Hamed%20and%20Byung%20Suk%20Lee&entry.1292438233=%20%20The%20generative%20capabilities%20of%20LLM%20models%20present%20opportunities%20in%0Aaccelerating%20tasks%20and%20concerns%20with%20the%20authenticity%20of%20the%20knowledge%20it%0Aproduces.%20To%20address%20the%20concerns%2C%20we%20present%20a%20computational%20approach%20that%0Asystematically%20evaluates%20the%20factual%20accuracy%20of%20biomedical%20knowledge%20that%20an%0ALLM%20model%20has%20been%20prompted%20to%20generate.%20Our%20approach%20encompasses%20two%0Aprocesses%3A%20the%20generation%20of%20disease-centric%20associations%20and%20the%20verification%0Aof%20them%20using%20the%20semantic%20knowledge%20of%20the%20biomedical%20ontologies.%20Using%0AChatGPT%20as%20the%20select%20LLM%20model%2C%20we%20designed%20a%20set%20of%20prompt-engineering%0Aprocesses%20to%20generate%20linkages%20between%20diseases%2C%20drugs%2C%20symptoms%2C%20and%20genes%20to%0Aestablish%20grounds%20for%20assessments.%20Experimental%20results%20demonstrate%20high%0Aaccuracy%20in%20identifying%20disease%20terms%20%2888%25-97%25%29%2C%20drug%20names%20%2890%25-91%25%29%2C%20and%0Agenetic%20information%20%2888%25-98%25%29.%20The%20symptom%20term%20identification%20accuracy%20was%0Anotably%20lower%20%2849%25-61%25%29%2C%20as%20verified%20against%20the%20DOID%2C%20ChEBI%2C%20SYMPTOM%2C%20and%20GO%0Aontologies%20accordingly.%20The%20verification%20of%20associations%20reveals%20literature%0Acoverage%20rates%20of%20%2889%25-91%25%29%20among%20disease-drug%20and%20disease-gene%20associations.%0AThe%20low%20identification%20accuracy%20for%20symptom%20terms%20also%20contributed%20to%20the%0Averification%20of%20symptom-related%20associations%20%2849%25-62%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14714v1&entry.124074799=Read"},
{"title": "Dynamic Low-Rank Sparse Adaptation for Large Language Models", "author": "Weizhong Huang and Yuxin Zhang and Xiawu Zheng and Yang Liu and Jing Lin and Yiwu Yao and Rongrong Ji", "abstract": "  Despite the efficacy of network sparsity in alleviating the deployment strain\nof Large Language Models (LLMs), it endures significant performance\ndegradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs\noffers an intuitive approach to counter this predicament, while it holds\nshortcomings include: 1) The inability to integrate LoRA weights into sparse\nLLMs post-training, and 2) Insufficient performance recovery at high sparsity\nratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA),\na novel method that seamlessly integrates low-rank adaptation into LLM sparsity\nwithin a unified framework, thereby enhancing the performance of sparse LLMs\nwithout increasing the inference latency. In particular, LoSA dynamically\nsparsifies the LoRA outcomes based on the corresponding sparse weights during\nfine-tuning, thus guaranteeing that the LoRA module can be integrated into the\nsparse LLMs post-training. Besides, LoSA leverages Representation Mutual\nInformation (RMI) as an indicator to determine the importance of layers,\nthereby efficiently determining the layer-wise sparsity rates during\nfine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based\non the variability in layer-wise reconstruction errors, allocating an\nappropriate fine-tuning for each layer to reduce the output discrepancies\nbetween dense and sparse LLMs. Extensive experiments tell that LoSA can\nefficiently boost the efficacy of sparse LLMs within a few hours, without\nintroducing any additional inferential burden. For example, LoSA reduced the\nperplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by\n16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on\nGPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU.\nCode is available at https://github.com/wzhuang-xmu/LoSA.\n", "link": "http://arxiv.org/abs/2502.14816v1", "date": "2025-02-20", "relevancy": 2.3986, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4772}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Low-Rank%20Sparse%20Adaptation%20for%20Large%20Language%20Models&body=Title%3A%20Dynamic%20Low-Rank%20Sparse%20Adaptation%20for%20Large%20Language%20Models%0AAuthor%3A%20Weizhong%20Huang%20and%20Yuxin%20Zhang%20and%20Xiawu%20Zheng%20and%20Yang%20Liu%20and%20Jing%20Lin%20and%20Yiwu%20Yao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Despite%20the%20efficacy%20of%20network%20sparsity%20in%20alleviating%20the%20deployment%20strain%0Aof%20Large%20Language%20Models%20%28LLMs%29%2C%20it%20endures%20significant%20performance%0Adegradation.%20Applying%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20fine-tune%20the%20sparse%20LLMs%0Aoffers%20an%20intuitive%20approach%20to%20counter%20this%20predicament%2C%20while%20it%20holds%0Ashortcomings%20include%3A%201%29%20The%20inability%20to%20integrate%20LoRA%20weights%20into%20sparse%0ALLMs%20post-training%2C%20and%202%29%20Insufficient%20performance%20recovery%20at%20high%20sparsity%0Aratios.%20In%20this%20paper%2C%20we%20introduce%20dynamic%20Low-rank%20Sparse%20Adaptation%20%28LoSA%29%2C%0Aa%20novel%20method%20that%20seamlessly%20integrates%20low-rank%20adaptation%20into%20LLM%20sparsity%0Awithin%20a%20unified%20framework%2C%20thereby%20enhancing%20the%20performance%20of%20sparse%20LLMs%0Awithout%20increasing%20the%20inference%20latency.%20In%20particular%2C%20LoSA%20dynamically%0Asparsifies%20the%20LoRA%20outcomes%20based%20on%20the%20corresponding%20sparse%20weights%20during%0Afine-tuning%2C%20thus%20guaranteeing%20that%20the%20LoRA%20module%20can%20be%20integrated%20into%20the%0Asparse%20LLMs%20post-training.%20Besides%2C%20LoSA%20leverages%20Representation%20Mutual%0AInformation%20%28RMI%29%20as%20an%20indicator%20to%20determine%20the%20importance%20of%20layers%2C%0Athereby%20efficiently%20determining%20the%20layer-wise%20sparsity%20rates%20during%0Afine-tuning.%20Predicated%20on%20this%2C%20LoSA%20adjusts%20the%20rank%20of%20the%20LoRA%20module%20based%0Aon%20the%20variability%20in%20layer-wise%20reconstruction%20errors%2C%20allocating%20an%0Aappropriate%20fine-tuning%20for%20each%20layer%20to%20reduce%20the%20output%20discrepancies%0Abetween%20dense%20and%20sparse%20LLMs.%20Extensive%20experiments%20tell%20that%20LoSA%20can%0Aefficiently%20boost%20the%20efficacy%20of%20sparse%20LLMs%20within%20a%20few%20hours%2C%20without%0Aintroducing%20any%20additional%20inferential%20burden.%20For%20example%2C%20LoSA%20reduced%20the%0Aperplexity%20of%20sparse%20LLaMA-2-7B%20by%2068.73%20and%20increased%20zero-shot%20accuracy%20by%0A16.32%24%5C%25%24%2C%20achieving%20a%202.60%24%5Ctimes%24%20speedup%20on%20CPU%20and%202.23%24%5Ctimes%24%20speedup%20on%0AGPU%2C%20requiring%20only%2045%20minutes%20of%20fine-tuning%20on%20a%20single%20NVIDIA%20A100%2080GB%20GPU.%0ACode%20is%20available%20at%20https%3A//github.com/wzhuang-xmu/LoSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Low-Rank%2520Sparse%2520Adaptation%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DWeizhong%2520Huang%2520and%2520Yuxin%2520Zhang%2520and%2520Xiawu%2520Zheng%2520and%2520Yang%2520Liu%2520and%2520Jing%2520Lin%2520and%2520Yiwu%2520Yao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Despite%2520the%2520efficacy%2520of%2520network%2520sparsity%2520in%2520alleviating%2520the%2520deployment%2520strain%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520it%2520endures%2520significant%2520performance%250Adegradation.%2520Applying%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520fine-tune%2520the%2520sparse%2520LLMs%250Aoffers%2520an%2520intuitive%2520approach%2520to%2520counter%2520this%2520predicament%252C%2520while%2520it%2520holds%250Ashortcomings%2520include%253A%25201%2529%2520The%2520inability%2520to%2520integrate%2520LoRA%2520weights%2520into%2520sparse%250ALLMs%2520post-training%252C%2520and%25202%2529%2520Insufficient%2520performance%2520recovery%2520at%2520high%2520sparsity%250Aratios.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520dynamic%2520Low-rank%2520Sparse%2520Adaptation%2520%2528LoSA%2529%252C%250Aa%2520novel%2520method%2520that%2520seamlessly%2520integrates%2520low-rank%2520adaptation%2520into%2520LLM%2520sparsity%250Awithin%2520a%2520unified%2520framework%252C%2520thereby%2520enhancing%2520the%2520performance%2520of%2520sparse%2520LLMs%250Awithout%2520increasing%2520the%2520inference%2520latency.%2520In%2520particular%252C%2520LoSA%2520dynamically%250Asparsifies%2520the%2520LoRA%2520outcomes%2520based%2520on%2520the%2520corresponding%2520sparse%2520weights%2520during%250Afine-tuning%252C%2520thus%2520guaranteeing%2520that%2520the%2520LoRA%2520module%2520can%2520be%2520integrated%2520into%2520the%250Asparse%2520LLMs%2520post-training.%2520Besides%252C%2520LoSA%2520leverages%2520Representation%2520Mutual%250AInformation%2520%2528RMI%2529%2520as%2520an%2520indicator%2520to%2520determine%2520the%2520importance%2520of%2520layers%252C%250Athereby%2520efficiently%2520determining%2520the%2520layer-wise%2520sparsity%2520rates%2520during%250Afine-tuning.%2520Predicated%2520on%2520this%252C%2520LoSA%2520adjusts%2520the%2520rank%2520of%2520the%2520LoRA%2520module%2520based%250Aon%2520the%2520variability%2520in%2520layer-wise%2520reconstruction%2520errors%252C%2520allocating%2520an%250Aappropriate%2520fine-tuning%2520for%2520each%2520layer%2520to%2520reduce%2520the%2520output%2520discrepancies%250Abetween%2520dense%2520and%2520sparse%2520LLMs.%2520Extensive%2520experiments%2520tell%2520that%2520LoSA%2520can%250Aefficiently%2520boost%2520the%2520efficacy%2520of%2520sparse%2520LLMs%2520within%2520a%2520few%2520hours%252C%2520without%250Aintroducing%2520any%2520additional%2520inferential%2520burden.%2520For%2520example%252C%2520LoSA%2520reduced%2520the%250Aperplexity%2520of%2520sparse%2520LLaMA-2-7B%2520by%252068.73%2520and%2520increased%2520zero-shot%2520accuracy%2520by%250A16.32%2524%255C%2525%2524%252C%2520achieving%2520a%25202.60%2524%255Ctimes%2524%2520speedup%2520on%2520CPU%2520and%25202.23%2524%255Ctimes%2524%2520speedup%2520on%250AGPU%252C%2520requiring%2520only%252045%2520minutes%2520of%2520fine-tuning%2520on%2520a%2520single%2520NVIDIA%2520A100%252080GB%2520GPU.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/wzhuang-xmu/LoSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Low-Rank%20Sparse%20Adaptation%20for%20Large%20Language%20Models&entry.906535625=Weizhong%20Huang%20and%20Yuxin%20Zhang%20and%20Xiawu%20Zheng%20and%20Yang%20Liu%20and%20Jing%20Lin%20and%20Yiwu%20Yao%20and%20Rongrong%20Ji&entry.1292438233=%20%20Despite%20the%20efficacy%20of%20network%20sparsity%20in%20alleviating%20the%20deployment%20strain%0Aof%20Large%20Language%20Models%20%28LLMs%29%2C%20it%20endures%20significant%20performance%0Adegradation.%20Applying%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20fine-tune%20the%20sparse%20LLMs%0Aoffers%20an%20intuitive%20approach%20to%20counter%20this%20predicament%2C%20while%20it%20holds%0Ashortcomings%20include%3A%201%29%20The%20inability%20to%20integrate%20LoRA%20weights%20into%20sparse%0ALLMs%20post-training%2C%20and%202%29%20Insufficient%20performance%20recovery%20at%20high%20sparsity%0Aratios.%20In%20this%20paper%2C%20we%20introduce%20dynamic%20Low-rank%20Sparse%20Adaptation%20%28LoSA%29%2C%0Aa%20novel%20method%20that%20seamlessly%20integrates%20low-rank%20adaptation%20into%20LLM%20sparsity%0Awithin%20a%20unified%20framework%2C%20thereby%20enhancing%20the%20performance%20of%20sparse%20LLMs%0Awithout%20increasing%20the%20inference%20latency.%20In%20particular%2C%20LoSA%20dynamically%0Asparsifies%20the%20LoRA%20outcomes%20based%20on%20the%20corresponding%20sparse%20weights%20during%0Afine-tuning%2C%20thus%20guaranteeing%20that%20the%20LoRA%20module%20can%20be%20integrated%20into%20the%0Asparse%20LLMs%20post-training.%20Besides%2C%20LoSA%20leverages%20Representation%20Mutual%0AInformation%20%28RMI%29%20as%20an%20indicator%20to%20determine%20the%20importance%20of%20layers%2C%0Athereby%20efficiently%20determining%20the%20layer-wise%20sparsity%20rates%20during%0Afine-tuning.%20Predicated%20on%20this%2C%20LoSA%20adjusts%20the%20rank%20of%20the%20LoRA%20module%20based%0Aon%20the%20variability%20in%20layer-wise%20reconstruction%20errors%2C%20allocating%20an%0Aappropriate%20fine-tuning%20for%20each%20layer%20to%20reduce%20the%20output%20discrepancies%0Abetween%20dense%20and%20sparse%20LLMs.%20Extensive%20experiments%20tell%20that%20LoSA%20can%0Aefficiently%20boost%20the%20efficacy%20of%20sparse%20LLMs%20within%20a%20few%20hours%2C%20without%0Aintroducing%20any%20additional%20inferential%20burden.%20For%20example%2C%20LoSA%20reduced%20the%0Aperplexity%20of%20sparse%20LLaMA-2-7B%20by%2068.73%20and%20increased%20zero-shot%20accuracy%20by%0A16.32%24%5C%25%24%2C%20achieving%20a%202.60%24%5Ctimes%24%20speedup%20on%20CPU%20and%202.23%24%5Ctimes%24%20speedup%20on%0AGPU%2C%20requiring%20only%2045%20minutes%20of%20fine-tuning%20on%20a%20single%20NVIDIA%20A100%2080GB%20GPU.%0ACode%20is%20available%20at%20https%3A//github.com/wzhuang-xmu/LoSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14816v1&entry.124074799=Read"},
{"title": "Interpretable Text Embeddings and Text Similarity Explanation: A Primer", "author": "Juri Opitz and Lucas M\u00f6ller and Andrianos Michail and Simon Clematide", "abstract": "  Text embeddings and text embedding models are a backbone of many AI and NLP\nsystems, particularly those involving search. However, interpretability\nchallenges persist, especially in explaining obtained similarity scores, which\nis crucial for applications requiring transparency. In this paper, we give a\nstructured overview of interpretability methods specializing in explaining\nthose similarity scores, an emerging research area. We study the methods'\nindividual ideas and techniques, evaluating their potential for improving\ninterpretability of text embeddings and explaining predicted similarities.\n", "link": "http://arxiv.org/abs/2502.14862v1", "date": "2025-02-20", "relevancy": 2.3775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Text%20Embeddings%20and%20Text%20Similarity%20Explanation%3A%20A%20Primer&body=Title%3A%20Interpretable%20Text%20Embeddings%20and%20Text%20Similarity%20Explanation%3A%20A%20Primer%0AAuthor%3A%20Juri%20Opitz%20and%20Lucas%20M%C3%B6ller%20and%20Andrianos%20Michail%20and%20Simon%20Clematide%0AAbstract%3A%20%20%20Text%20embeddings%20and%20text%20embedding%20models%20are%20a%20backbone%20of%20many%20AI%20and%20NLP%0Asystems%2C%20particularly%20those%20involving%20search.%20However%2C%20interpretability%0Achallenges%20persist%2C%20especially%20in%20explaining%20obtained%20similarity%20scores%2C%20which%0Ais%20crucial%20for%20applications%20requiring%20transparency.%20In%20this%20paper%2C%20we%20give%20a%0Astructured%20overview%20of%20interpretability%20methods%20specializing%20in%20explaining%0Athose%20similarity%20scores%2C%20an%20emerging%20research%20area.%20We%20study%20the%20methods%27%0Aindividual%20ideas%20and%20techniques%2C%20evaluating%20their%20potential%20for%20improving%0Ainterpretability%20of%20text%20embeddings%20and%20explaining%20predicted%20similarities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Text%2520Embeddings%2520and%2520Text%2520Similarity%2520Explanation%253A%2520A%2520Primer%26entry.906535625%3DJuri%2520Opitz%2520and%2520Lucas%2520M%25C3%25B6ller%2520and%2520Andrianos%2520Michail%2520and%2520Simon%2520Clematide%26entry.1292438233%3D%2520%2520Text%2520embeddings%2520and%2520text%2520embedding%2520models%2520are%2520a%2520backbone%2520of%2520many%2520AI%2520and%2520NLP%250Asystems%252C%2520particularly%2520those%2520involving%2520search.%2520However%252C%2520interpretability%250Achallenges%2520persist%252C%2520especially%2520in%2520explaining%2520obtained%2520similarity%2520scores%252C%2520which%250Ais%2520crucial%2520for%2520applications%2520requiring%2520transparency.%2520In%2520this%2520paper%252C%2520we%2520give%2520a%250Astructured%2520overview%2520of%2520interpretability%2520methods%2520specializing%2520in%2520explaining%250Athose%2520similarity%2520scores%252C%2520an%2520emerging%2520research%2520area.%2520We%2520study%2520the%2520methods%2527%250Aindividual%2520ideas%2520and%2520techniques%252C%2520evaluating%2520their%2520potential%2520for%2520improving%250Ainterpretability%2520of%2520text%2520embeddings%2520and%2520explaining%2520predicted%2520similarities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Text%20Embeddings%20and%20Text%20Similarity%20Explanation%3A%20A%20Primer&entry.906535625=Juri%20Opitz%20and%20Lucas%20M%C3%B6ller%20and%20Andrianos%20Michail%20and%20Simon%20Clematide&entry.1292438233=%20%20Text%20embeddings%20and%20text%20embedding%20models%20are%20a%20backbone%20of%20many%20AI%20and%20NLP%0Asystems%2C%20particularly%20those%20involving%20search.%20However%2C%20interpretability%0Achallenges%20persist%2C%20especially%20in%20explaining%20obtained%20similarity%20scores%2C%20which%0Ais%20crucial%20for%20applications%20requiring%20transparency.%20In%20this%20paper%2C%20we%20give%20a%0Astructured%20overview%20of%20interpretability%20methods%20specializing%20in%20explaining%0Athose%20similarity%20scores%2C%20an%20emerging%20research%20area.%20We%20study%20the%20methods%27%0Aindividual%20ideas%20and%20techniques%2C%20evaluating%20their%20potential%20for%20improving%0Ainterpretability%20of%20text%20embeddings%20and%20explaining%20predicted%20similarities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14862v1&entry.124074799=Read"},
{"title": "A Survey on Text-Driven 360-Degree Panorama Generation", "author": "Hai Wang and Xiaoyu Xiang and Weihao Xia and Jing-Hao Xue", "abstract": "  The advent of text-driven 360-degree panorama generation, enabling the\nsynthesis of 360-degree panoramic images directly from textual descriptions,\nmarks a transformative advancement in immersive visual content creation. This\ninnovation significantly simplifies the traditionally complex process of\nproducing such content. Recent progress in text-to-image diffusion models has\naccelerated the rapid development in this emerging field. This survey presents\na comprehensive review of text-driven 360-degree panorama generation, offering\nan in-depth analysis of state-of-the-art algorithms and their expanding\napplications in 360-degree 3D scene generation. Furthermore, we critically\nexamine current limitations and propose promising directions for future\nresearch. A curated project page with relevant resources and research papers is\navailable at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.\n", "link": "http://arxiv.org/abs/2502.14799v1", "date": "2025-02-20", "relevancy": 2.377, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6029}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6029}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Text-Driven%20360-Degree%20Panorama%20Generation&body=Title%3A%20A%20Survey%20on%20Text-Driven%20360-Degree%20Panorama%20Generation%0AAuthor%3A%20Hai%20Wang%20and%20Xiaoyu%20Xiang%20and%20Weihao%20Xia%20and%20Jing-Hao%20Xue%0AAbstract%3A%20%20%20The%20advent%20of%20text-driven%20360-degree%20panorama%20generation%2C%20enabling%20the%0Asynthesis%20of%20360-degree%20panoramic%20images%20directly%20from%20textual%20descriptions%2C%0Amarks%20a%20transformative%20advancement%20in%20immersive%20visual%20content%20creation.%20This%0Ainnovation%20significantly%20simplifies%20the%20traditionally%20complex%20process%20of%0Aproducing%20such%20content.%20Recent%20progress%20in%20text-to-image%20diffusion%20models%20has%0Aaccelerated%20the%20rapid%20development%20in%20this%20emerging%20field.%20This%20survey%20presents%0Aa%20comprehensive%20review%20of%20text-driven%20360-degree%20panorama%20generation%2C%20offering%0Aan%20in-depth%20analysis%20of%20state-of-the-art%20algorithms%20and%20their%20expanding%0Aapplications%20in%20360-degree%203D%20scene%20generation.%20Furthermore%2C%20we%20critically%0Aexamine%20current%20limitations%20and%20propose%20promising%20directions%20for%20future%0Aresearch.%20A%20curated%20project%20page%20with%20relevant%20resources%20and%20research%20papers%20is%0Aavailable%20at%20https%3A//littlewhitesea.github.io/Text-Driven-Pano-Gen/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Text-Driven%2520360-Degree%2520Panorama%2520Generation%26entry.906535625%3DHai%2520Wang%2520and%2520Xiaoyu%2520Xiang%2520and%2520Weihao%2520Xia%2520and%2520Jing-Hao%2520Xue%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520text-driven%2520360-degree%2520panorama%2520generation%252C%2520enabling%2520the%250Asynthesis%2520of%2520360-degree%2520panoramic%2520images%2520directly%2520from%2520textual%2520descriptions%252C%250Amarks%2520a%2520transformative%2520advancement%2520in%2520immersive%2520visual%2520content%2520creation.%2520This%250Ainnovation%2520significantly%2520simplifies%2520the%2520traditionally%2520complex%2520process%2520of%250Aproducing%2520such%2520content.%2520Recent%2520progress%2520in%2520text-to-image%2520diffusion%2520models%2520has%250Aaccelerated%2520the%2520rapid%2520development%2520in%2520this%2520emerging%2520field.%2520This%2520survey%2520presents%250Aa%2520comprehensive%2520review%2520of%2520text-driven%2520360-degree%2520panorama%2520generation%252C%2520offering%250Aan%2520in-depth%2520analysis%2520of%2520state-of-the-art%2520algorithms%2520and%2520their%2520expanding%250Aapplications%2520in%2520360-degree%25203D%2520scene%2520generation.%2520Furthermore%252C%2520we%2520critically%250Aexamine%2520current%2520limitations%2520and%2520propose%2520promising%2520directions%2520for%2520future%250Aresearch.%2520A%2520curated%2520project%2520page%2520with%2520relevant%2520resources%2520and%2520research%2520papers%2520is%250Aavailable%2520at%2520https%253A//littlewhitesea.github.io/Text-Driven-Pano-Gen/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Text-Driven%20360-Degree%20Panorama%20Generation&entry.906535625=Hai%20Wang%20and%20Xiaoyu%20Xiang%20and%20Weihao%20Xia%20and%20Jing-Hao%20Xue&entry.1292438233=%20%20The%20advent%20of%20text-driven%20360-degree%20panorama%20generation%2C%20enabling%20the%0Asynthesis%20of%20360-degree%20panoramic%20images%20directly%20from%20textual%20descriptions%2C%0Amarks%20a%20transformative%20advancement%20in%20immersive%20visual%20content%20creation.%20This%0Ainnovation%20significantly%20simplifies%20the%20traditionally%20complex%20process%20of%0Aproducing%20such%20content.%20Recent%20progress%20in%20text-to-image%20diffusion%20models%20has%0Aaccelerated%20the%20rapid%20development%20in%20this%20emerging%20field.%20This%20survey%20presents%0Aa%20comprehensive%20review%20of%20text-driven%20360-degree%20panorama%20generation%2C%20offering%0Aan%20in-depth%20analysis%20of%20state-of-the-art%20algorithms%20and%20their%20expanding%0Aapplications%20in%20360-degree%203D%20scene%20generation.%20Furthermore%2C%20we%20critically%0Aexamine%20current%20limitations%20and%20propose%20promising%20directions%20for%20future%0Aresearch.%20A%20curated%20project%20page%20with%20relevant%20resources%20and%20research%20papers%20is%0Aavailable%20at%20https%3A//littlewhitesea.github.io/Text-Driven-Pano-Gen/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14799v1&entry.124074799=Read"},
{"title": "Optimizing Model Selection for Compound AI Systems", "author": "Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Matei Zaharia and James Zou and Ion Stoica", "abstract": "  Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.\n", "link": "http://arxiv.org/abs/2502.14815v1", "date": "2025-02-20", "relevancy": 2.3606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Model%20Selection%20for%20Compound%20AI%20Systems&body=Title%3A%20Optimizing%20Model%20Selection%20for%20Compound%20AI%20Systems%0AAuthor%3A%20Lingjiao%20Chen%20and%20Jared%20Quincy%20Davis%20and%20Boris%20Hanin%20and%20Peter%20Bailis%20and%20Matei%20Zaharia%20and%20James%20Zou%20and%20Ion%20Stoica%0AAbstract%3A%20%20%20Compound%20AI%20systems%20that%20combine%20multiple%20LLM%20calls%2C%20such%20as%20self-refine%20and%0Amulti-agent-debate%2C%20achieve%20strong%20performance%20on%20many%20AI%20tasks.%20We%20address%20a%0Acore%20question%20in%20optimizing%20compound%20systems%3A%20for%20each%20LLM%20call%20or%20module%20in%0Athe%20system%2C%20how%20should%20one%20decide%20which%20LLM%20to%20use%3F%20We%20show%20that%20these%20LLM%0Achoices%20have%20a%20large%20effect%20on%20quality%2C%20but%20the%20search%20space%20is%20exponential.%20We%0Apropose%20LLMSelector%2C%20an%20efficient%20framework%20for%20model%20selection%20in%20compound%0Asystems%2C%20which%20leverages%20two%20key%20empirical%20insights%3A%20%28i%29%20end-to-end%20performance%0Ais%20often%20monotonic%20in%20how%20well%20each%20module%20performs%2C%20with%20all%20other%20modules%0Aheld%20fixed%2C%20and%20%28ii%29%20per-module%20performance%20can%20be%20estimated%20accurately%20by%20an%0ALLM.%20Building%20upon%20these%20insights%2C%20LLMSelector%20iteratively%20selects%20one%20module%0Aand%20allocates%20to%20it%20the%20model%20with%20the%20highest%20module-wise%20performance%2C%20as%0Aestimated%20by%20an%20LLM%2C%20until%20no%20further%20gain%20is%20possible.%20LLMSelector%20is%0Aapplicable%20to%20any%20compound%20system%20with%20a%20bounded%20number%20of%20modules%2C%20and%20its%0Anumber%20of%20API%20calls%20scales%20linearly%20with%20the%20number%20of%20modules%2C%20achieving%0Ahigh-quality%20model%20allocation%20both%20empirically%20and%20theoretically.%20Experiments%0Awith%20popular%20compound%20systems%20such%20as%20multi-agent%20debate%20and%20self-refine%20using%0ALLMs%20such%20as%20GPT-4o%2C%20Claude%203.5%20Sonnet%20and%20Gemini%201.5%20show%20that%20LLMSelector%0Aconfers%205%25-70%25%20accuracy%20gains%20compared%20to%20using%20the%20same%20LLM%20for%20all%20modules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Model%2520Selection%2520for%2520Compound%2520AI%2520Systems%26entry.906535625%3DLingjiao%2520Chen%2520and%2520Jared%2520Quincy%2520Davis%2520and%2520Boris%2520Hanin%2520and%2520Peter%2520Bailis%2520and%2520Matei%2520Zaharia%2520and%2520James%2520Zou%2520and%2520Ion%2520Stoica%26entry.1292438233%3D%2520%2520Compound%2520AI%2520systems%2520that%2520combine%2520multiple%2520LLM%2520calls%252C%2520such%2520as%2520self-refine%2520and%250Amulti-agent-debate%252C%2520achieve%2520strong%2520performance%2520on%2520many%2520AI%2520tasks.%2520We%2520address%2520a%250Acore%2520question%2520in%2520optimizing%2520compound%2520systems%253A%2520for%2520each%2520LLM%2520call%2520or%2520module%2520in%250Athe%2520system%252C%2520how%2520should%2520one%2520decide%2520which%2520LLM%2520to%2520use%253F%2520We%2520show%2520that%2520these%2520LLM%250Achoices%2520have%2520a%2520large%2520effect%2520on%2520quality%252C%2520but%2520the%2520search%2520space%2520is%2520exponential.%2520We%250Apropose%2520LLMSelector%252C%2520an%2520efficient%2520framework%2520for%2520model%2520selection%2520in%2520compound%250Asystems%252C%2520which%2520leverages%2520two%2520key%2520empirical%2520insights%253A%2520%2528i%2529%2520end-to-end%2520performance%250Ais%2520often%2520monotonic%2520in%2520how%2520well%2520each%2520module%2520performs%252C%2520with%2520all%2520other%2520modules%250Aheld%2520fixed%252C%2520and%2520%2528ii%2529%2520per-module%2520performance%2520can%2520be%2520estimated%2520accurately%2520by%2520an%250ALLM.%2520Building%2520upon%2520these%2520insights%252C%2520LLMSelector%2520iteratively%2520selects%2520one%2520module%250Aand%2520allocates%2520to%2520it%2520the%2520model%2520with%2520the%2520highest%2520module-wise%2520performance%252C%2520as%250Aestimated%2520by%2520an%2520LLM%252C%2520until%2520no%2520further%2520gain%2520is%2520possible.%2520LLMSelector%2520is%250Aapplicable%2520to%2520any%2520compound%2520system%2520with%2520a%2520bounded%2520number%2520of%2520modules%252C%2520and%2520its%250Anumber%2520of%2520API%2520calls%2520scales%2520linearly%2520with%2520the%2520number%2520of%2520modules%252C%2520achieving%250Ahigh-quality%2520model%2520allocation%2520both%2520empirically%2520and%2520theoretically.%2520Experiments%250Awith%2520popular%2520compound%2520systems%2520such%2520as%2520multi-agent%2520debate%2520and%2520self-refine%2520using%250ALLMs%2520such%2520as%2520GPT-4o%252C%2520Claude%25203.5%2520Sonnet%2520and%2520Gemini%25201.5%2520show%2520that%2520LLMSelector%250Aconfers%25205%2525-70%2525%2520accuracy%2520gains%2520compared%2520to%2520using%2520the%2520same%2520LLM%2520for%2520all%2520modules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Model%20Selection%20for%20Compound%20AI%20Systems&entry.906535625=Lingjiao%20Chen%20and%20Jared%20Quincy%20Davis%20and%20Boris%20Hanin%20and%20Peter%20Bailis%20and%20Matei%20Zaharia%20and%20James%20Zou%20and%20Ion%20Stoica&entry.1292438233=%20%20Compound%20AI%20systems%20that%20combine%20multiple%20LLM%20calls%2C%20such%20as%20self-refine%20and%0Amulti-agent-debate%2C%20achieve%20strong%20performance%20on%20many%20AI%20tasks.%20We%20address%20a%0Acore%20question%20in%20optimizing%20compound%20systems%3A%20for%20each%20LLM%20call%20or%20module%20in%0Athe%20system%2C%20how%20should%20one%20decide%20which%20LLM%20to%20use%3F%20We%20show%20that%20these%20LLM%0Achoices%20have%20a%20large%20effect%20on%20quality%2C%20but%20the%20search%20space%20is%20exponential.%20We%0Apropose%20LLMSelector%2C%20an%20efficient%20framework%20for%20model%20selection%20in%20compound%0Asystems%2C%20which%20leverages%20two%20key%20empirical%20insights%3A%20%28i%29%20end-to-end%20performance%0Ais%20often%20monotonic%20in%20how%20well%20each%20module%20performs%2C%20with%20all%20other%20modules%0Aheld%20fixed%2C%20and%20%28ii%29%20per-module%20performance%20can%20be%20estimated%20accurately%20by%20an%0ALLM.%20Building%20upon%20these%20insights%2C%20LLMSelector%20iteratively%20selects%20one%20module%0Aand%20allocates%20to%20it%20the%20model%20with%20the%20highest%20module-wise%20performance%2C%20as%0Aestimated%20by%20an%20LLM%2C%20until%20no%20further%20gain%20is%20possible.%20LLMSelector%20is%0Aapplicable%20to%20any%20compound%20system%20with%20a%20bounded%20number%20of%20modules%2C%20and%20its%0Anumber%20of%20API%20calls%20scales%20linearly%20with%20the%20number%20of%20modules%2C%20achieving%0Ahigh-quality%20model%20allocation%20both%20empirically%20and%20theoretically.%20Experiments%0Awith%20popular%20compound%20systems%20such%20as%20multi-agent%20debate%20and%20self-refine%20using%0ALLMs%20such%20as%20GPT-4o%2C%20Claude%203.5%20Sonnet%20and%20Gemini%201.5%20show%20that%20LLMSelector%0Aconfers%205%25-70%25%20accuracy%20gains%20compared%20to%20using%20the%20same%20LLM%20for%20all%20modules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14815v1&entry.124074799=Read"},
{"title": "VB-Com: Learning Vision-Blind Composite Humanoid Locomotion Against\n  Deficient Perception", "author": "Junli Ren and Tao Huang and Huayi Wang and Zirui Wang and Qingwei Ben and Jiangmiao Pang and Ping Luo", "abstract": "  The performance of legged locomotion is closely tied to the accuracy and\ncomprehensiveness of state observations. Blind policies, which rely solely on\nproprioception, are considered highly robust due to the reliability of\nproprioceptive observations. However, these policies significantly limit\nlocomotion speed and often require collisions with the terrain to adapt. In\ncontrast, Vision policies allows the robot to plan motions in advance and\nrespond proactively to unstructured terrains with an online perception module.\nHowever, perception is often compromised by noisy real-world environments,\npotential sensor failures, and the limitations of current simulations in\npresenting dynamic or deformable terrains. Humanoid robots, with high degrees\nof freedom and inherently unstable morphology, are particularly susceptible to\nmisguidance from deficient perception, which can result in falls or termination\non challenging dynamic terrains. To leverage the advantages of both vision and\nblind policies, we propose VB-Com, a composite framework that enables humanoid\nrobots to determine when to rely on the vision policy and when to switch to the\nblind policy under perceptual deficiency. We demonstrate that VB-Com\neffectively enables humanoid robots to traverse challenging terrains and\nobstacles despite perception deficiencies caused by dynamic terrains or\nperceptual noise.\n", "link": "http://arxiv.org/abs/2502.14814v1", "date": "2025-02-20", "relevancy": 2.3497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VB-Com%3A%20Learning%20Vision-Blind%20Composite%20Humanoid%20Locomotion%20Against%0A%20%20Deficient%20Perception&body=Title%3A%20VB-Com%3A%20Learning%20Vision-Blind%20Composite%20Humanoid%20Locomotion%20Against%0A%20%20Deficient%20Perception%0AAuthor%3A%20Junli%20Ren%20and%20Tao%20Huang%20and%20Huayi%20Wang%20and%20Zirui%20Wang%20and%20Qingwei%20Ben%20and%20Jiangmiao%20Pang%20and%20Ping%20Luo%0AAbstract%3A%20%20%20The%20performance%20of%20legged%20locomotion%20is%20closely%20tied%20to%20the%20accuracy%20and%0Acomprehensiveness%20of%20state%20observations.%20Blind%20policies%2C%20which%20rely%20solely%20on%0Aproprioception%2C%20are%20considered%20highly%20robust%20due%20to%20the%20reliability%20of%0Aproprioceptive%20observations.%20However%2C%20these%20policies%20significantly%20limit%0Alocomotion%20speed%20and%20often%20require%20collisions%20with%20the%20terrain%20to%20adapt.%20In%0Acontrast%2C%20Vision%20policies%20allows%20the%20robot%20to%20plan%20motions%20in%20advance%20and%0Arespond%20proactively%20to%20unstructured%20terrains%20with%20an%20online%20perception%20module.%0AHowever%2C%20perception%20is%20often%20compromised%20by%20noisy%20real-world%20environments%2C%0Apotential%20sensor%20failures%2C%20and%20the%20limitations%20of%20current%20simulations%20in%0Apresenting%20dynamic%20or%20deformable%20terrains.%20Humanoid%20robots%2C%20with%20high%20degrees%0Aof%20freedom%20and%20inherently%20unstable%20morphology%2C%20are%20particularly%20susceptible%20to%0Amisguidance%20from%20deficient%20perception%2C%20which%20can%20result%20in%20falls%20or%20termination%0Aon%20challenging%20dynamic%20terrains.%20To%20leverage%20the%20advantages%20of%20both%20vision%20and%0Ablind%20policies%2C%20we%20propose%20VB-Com%2C%20a%20composite%20framework%20that%20enables%20humanoid%0Arobots%20to%20determine%20when%20to%20rely%20on%20the%20vision%20policy%20and%20when%20to%20switch%20to%20the%0Ablind%20policy%20under%20perceptual%20deficiency.%20We%20demonstrate%20that%20VB-Com%0Aeffectively%20enables%20humanoid%20robots%20to%20traverse%20challenging%20terrains%20and%0Aobstacles%20despite%20perception%20deficiencies%20caused%20by%20dynamic%20terrains%20or%0Aperceptual%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVB-Com%253A%2520Learning%2520Vision-Blind%2520Composite%2520Humanoid%2520Locomotion%2520Against%250A%2520%2520Deficient%2520Perception%26entry.906535625%3DJunli%2520Ren%2520and%2520Tao%2520Huang%2520and%2520Huayi%2520Wang%2520and%2520Zirui%2520Wang%2520and%2520Qingwei%2520Ben%2520and%2520Jiangmiao%2520Pang%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520legged%2520locomotion%2520is%2520closely%2520tied%2520to%2520the%2520accuracy%2520and%250Acomprehensiveness%2520of%2520state%2520observations.%2520Blind%2520policies%252C%2520which%2520rely%2520solely%2520on%250Aproprioception%252C%2520are%2520considered%2520highly%2520robust%2520due%2520to%2520the%2520reliability%2520of%250Aproprioceptive%2520observations.%2520However%252C%2520these%2520policies%2520significantly%2520limit%250Alocomotion%2520speed%2520and%2520often%2520require%2520collisions%2520with%2520the%2520terrain%2520to%2520adapt.%2520In%250Acontrast%252C%2520Vision%2520policies%2520allows%2520the%2520robot%2520to%2520plan%2520motions%2520in%2520advance%2520and%250Arespond%2520proactively%2520to%2520unstructured%2520terrains%2520with%2520an%2520online%2520perception%2520module.%250AHowever%252C%2520perception%2520is%2520often%2520compromised%2520by%2520noisy%2520real-world%2520environments%252C%250Apotential%2520sensor%2520failures%252C%2520and%2520the%2520limitations%2520of%2520current%2520simulations%2520in%250Apresenting%2520dynamic%2520or%2520deformable%2520terrains.%2520Humanoid%2520robots%252C%2520with%2520high%2520degrees%250Aof%2520freedom%2520and%2520inherently%2520unstable%2520morphology%252C%2520are%2520particularly%2520susceptible%2520to%250Amisguidance%2520from%2520deficient%2520perception%252C%2520which%2520can%2520result%2520in%2520falls%2520or%2520termination%250Aon%2520challenging%2520dynamic%2520terrains.%2520To%2520leverage%2520the%2520advantages%2520of%2520both%2520vision%2520and%250Ablind%2520policies%252C%2520we%2520propose%2520VB-Com%252C%2520a%2520composite%2520framework%2520that%2520enables%2520humanoid%250Arobots%2520to%2520determine%2520when%2520to%2520rely%2520on%2520the%2520vision%2520policy%2520and%2520when%2520to%2520switch%2520to%2520the%250Ablind%2520policy%2520under%2520perceptual%2520deficiency.%2520We%2520demonstrate%2520that%2520VB-Com%250Aeffectively%2520enables%2520humanoid%2520robots%2520to%2520traverse%2520challenging%2520terrains%2520and%250Aobstacles%2520despite%2520perception%2520deficiencies%2520caused%2520by%2520dynamic%2520terrains%2520or%250Aperceptual%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VB-Com%3A%20Learning%20Vision-Blind%20Composite%20Humanoid%20Locomotion%20Against%0A%20%20Deficient%20Perception&entry.906535625=Junli%20Ren%20and%20Tao%20Huang%20and%20Huayi%20Wang%20and%20Zirui%20Wang%20and%20Qingwei%20Ben%20and%20Jiangmiao%20Pang%20and%20Ping%20Luo&entry.1292438233=%20%20The%20performance%20of%20legged%20locomotion%20is%20closely%20tied%20to%20the%20accuracy%20and%0Acomprehensiveness%20of%20state%20observations.%20Blind%20policies%2C%20which%20rely%20solely%20on%0Aproprioception%2C%20are%20considered%20highly%20robust%20due%20to%20the%20reliability%20of%0Aproprioceptive%20observations.%20However%2C%20these%20policies%20significantly%20limit%0Alocomotion%20speed%20and%20often%20require%20collisions%20with%20the%20terrain%20to%20adapt.%20In%0Acontrast%2C%20Vision%20policies%20allows%20the%20robot%20to%20plan%20motions%20in%20advance%20and%0Arespond%20proactively%20to%20unstructured%20terrains%20with%20an%20online%20perception%20module.%0AHowever%2C%20perception%20is%20often%20compromised%20by%20noisy%20real-world%20environments%2C%0Apotential%20sensor%20failures%2C%20and%20the%20limitations%20of%20current%20simulations%20in%0Apresenting%20dynamic%20or%20deformable%20terrains.%20Humanoid%20robots%2C%20with%20high%20degrees%0Aof%20freedom%20and%20inherently%20unstable%20morphology%2C%20are%20particularly%20susceptible%20to%0Amisguidance%20from%20deficient%20perception%2C%20which%20can%20result%20in%20falls%20or%20termination%0Aon%20challenging%20dynamic%20terrains.%20To%20leverage%20the%20advantages%20of%20both%20vision%20and%0Ablind%20policies%2C%20we%20propose%20VB-Com%2C%20a%20composite%20framework%20that%20enables%20humanoid%0Arobots%20to%20determine%20when%20to%20rely%20on%20the%20vision%20policy%20and%20when%20to%20switch%20to%20the%0Ablind%20policy%20under%20perceptual%20deficiency.%20We%20demonstrate%20that%20VB-Com%0Aeffectively%20enables%20humanoid%20robots%20to%20traverse%20challenging%20terrains%20and%0Aobstacles%20despite%20perception%20deficiencies%20caused%20by%20dynamic%20terrains%20or%0Aperceptual%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14814v1&entry.124074799=Read"},
{"title": "Prompt-to-Leaderboard", "author": "Evan Frick and Connor Chen and Joseph Tennyson and Tianle Li and Wei-Lin Chiang and Anastasios N. Angelopoulos and Ion Stoica", "abstract": "  Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l.\n", "link": "http://arxiv.org/abs/2502.14855v1", "date": "2025-02-20", "relevancy": 2.3206, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4645}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-to-Leaderboard&body=Title%3A%20Prompt-to-Leaderboard%0AAuthor%3A%20Evan%20Frick%20and%20Connor%20Chen%20and%20Joseph%20Tennyson%20and%20Tianle%20Li%20and%20Wei-Lin%20Chiang%20and%20Anastasios%20N.%20Angelopoulos%20and%20Ion%20Stoica%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20evaluations%20typically%20rely%20on%20aggregated%20metrics%0Alike%20accuracy%20or%20human%20preference%2C%20averaging%20across%20users%20and%20prompts.%20This%0Aaveraging%20obscures%20user-%20and%20prompt-specific%20variations%20in%20model%20performance.%0ATo%20address%20this%2C%20we%20propose%20Prompt-to-Leaderboard%20%28P2L%29%2C%20a%20method%20that%20produces%0Aleaderboards%20specific%20to%20a%20prompt.%20The%20core%20idea%20is%20to%20train%20an%20LLM%20taking%0Anatural%20language%20prompts%20as%20input%20to%20output%20a%20vector%20of%20Bradley-Terry%0Acoefficients%20which%20are%20then%20used%20to%20predict%20the%20human%20preference%20vote.%20The%0Aresulting%20prompt-dependent%20leaderboards%20allow%20for%20unsupervised%20task-specific%0Aevaluation%2C%20optimal%20routing%20of%20queries%20to%20models%2C%20personalization%2C%20and%0Aautomated%20evaluation%20of%20model%20strengths%20and%20weaknesses.%20Data%20from%20Chatbot%20Arena%0Asuggest%20that%20P2L%20better%20captures%20the%20nuanced%20landscape%20of%20language%20model%0Aperformance%20than%20the%20averaged%20leaderboard.%20Furthermore%2C%20our%20findings%20suggest%0Athat%20P2L%27s%20ability%20to%20produce%20prompt-specific%20evaluations%20follows%20a%20power%20law%0Ascaling%20similar%20to%20that%20observed%20in%20LLMs%20themselves.%20In%20January%202025%2C%20the%0Arouter%20we%20trained%20based%20on%20this%20methodology%20achieved%20the%20%5C%231%20spot%20in%20the%0AChatbot%20Arena%20leaderboard.%20Our%20code%20is%20available%20at%20this%20GitHub%20link%3A%0Ahttps%3A//github.com/lmarena/p2l.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-to-Leaderboard%26entry.906535625%3DEvan%2520Frick%2520and%2520Connor%2520Chen%2520and%2520Joseph%2520Tennyson%2520and%2520Tianle%2520Li%2520and%2520Wei-Lin%2520Chiang%2520and%2520Anastasios%2520N.%2520Angelopoulos%2520and%2520Ion%2520Stoica%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520evaluations%2520typically%2520rely%2520on%2520aggregated%2520metrics%250Alike%2520accuracy%2520or%2520human%2520preference%252C%2520averaging%2520across%2520users%2520and%2520prompts.%2520This%250Aaveraging%2520obscures%2520user-%2520and%2520prompt-specific%2520variations%2520in%2520model%2520performance.%250ATo%2520address%2520this%252C%2520we%2520propose%2520Prompt-to-Leaderboard%2520%2528P2L%2529%252C%2520a%2520method%2520that%2520produces%250Aleaderboards%2520specific%2520to%2520a%2520prompt.%2520The%2520core%2520idea%2520is%2520to%2520train%2520an%2520LLM%2520taking%250Anatural%2520language%2520prompts%2520as%2520input%2520to%2520output%2520a%2520vector%2520of%2520Bradley-Terry%250Acoefficients%2520which%2520are%2520then%2520used%2520to%2520predict%2520the%2520human%2520preference%2520vote.%2520The%250Aresulting%2520prompt-dependent%2520leaderboards%2520allow%2520for%2520unsupervised%2520task-specific%250Aevaluation%252C%2520optimal%2520routing%2520of%2520queries%2520to%2520models%252C%2520personalization%252C%2520and%250Aautomated%2520evaluation%2520of%2520model%2520strengths%2520and%2520weaknesses.%2520Data%2520from%2520Chatbot%2520Arena%250Asuggest%2520that%2520P2L%2520better%2520captures%2520the%2520nuanced%2520landscape%2520of%2520language%2520model%250Aperformance%2520than%2520the%2520averaged%2520leaderboard.%2520Furthermore%252C%2520our%2520findings%2520suggest%250Athat%2520P2L%2527s%2520ability%2520to%2520produce%2520prompt-specific%2520evaluations%2520follows%2520a%2520power%2520law%250Ascaling%2520similar%2520to%2520that%2520observed%2520in%2520LLMs%2520themselves.%2520In%2520January%25202025%252C%2520the%250Arouter%2520we%2520trained%2520based%2520on%2520this%2520methodology%2520achieved%2520the%2520%255C%25231%2520spot%2520in%2520the%250AChatbot%2520Arena%2520leaderboard.%2520Our%2520code%2520is%2520available%2520at%2520this%2520GitHub%2520link%253A%250Ahttps%253A//github.com/lmarena/p2l.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-to-Leaderboard&entry.906535625=Evan%20Frick%20and%20Connor%20Chen%20and%20Joseph%20Tennyson%20and%20Tianle%20Li%20and%20Wei-Lin%20Chiang%20and%20Anastasios%20N.%20Angelopoulos%20and%20Ion%20Stoica&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20evaluations%20typically%20rely%20on%20aggregated%20metrics%0Alike%20accuracy%20or%20human%20preference%2C%20averaging%20across%20users%20and%20prompts.%20This%0Aaveraging%20obscures%20user-%20and%20prompt-specific%20variations%20in%20model%20performance.%0ATo%20address%20this%2C%20we%20propose%20Prompt-to-Leaderboard%20%28P2L%29%2C%20a%20method%20that%20produces%0Aleaderboards%20specific%20to%20a%20prompt.%20The%20core%20idea%20is%20to%20train%20an%20LLM%20taking%0Anatural%20language%20prompts%20as%20input%20to%20output%20a%20vector%20of%20Bradley-Terry%0Acoefficients%20which%20are%20then%20used%20to%20predict%20the%20human%20preference%20vote.%20The%0Aresulting%20prompt-dependent%20leaderboards%20allow%20for%20unsupervised%20task-specific%0Aevaluation%2C%20optimal%20routing%20of%20queries%20to%20models%2C%20personalization%2C%20and%0Aautomated%20evaluation%20of%20model%20strengths%20and%20weaknesses.%20Data%20from%20Chatbot%20Arena%0Asuggest%20that%20P2L%20better%20captures%20the%20nuanced%20landscape%20of%20language%20model%0Aperformance%20than%20the%20averaged%20leaderboard.%20Furthermore%2C%20our%20findings%20suggest%0Athat%20P2L%27s%20ability%20to%20produce%20prompt-specific%20evaluations%20follows%20a%20power%20law%0Ascaling%20similar%20to%20that%20observed%20in%20LLMs%20themselves.%20In%20January%202025%2C%20the%0Arouter%20we%20trained%20based%20on%20this%20methodology%20achieved%20the%20%5C%231%20spot%20in%20the%0AChatbot%20Arena%20leaderboard.%20Our%20code%20is%20available%20at%20this%20GitHub%20link%3A%0Ahttps%3A//github.com/lmarena/p2l.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14855v1&entry.124074799=Read"},
{"title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation", "author": "Yue Yang and Ajay Patel and Matt Deitke and Tanmay Gupta and Luca Weihs and Andrew Head and Mark Yatskar and Chris Callison-Burch and Ranjay Krishna and Aniruddha Kembhavi and Christopher Clark", "abstract": "  Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.\n", "link": "http://arxiv.org/abs/2502.14846v1", "date": "2025-02-20", "relevancy": 2.2924, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6083}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Text-Rich%20Image%20Understanding%20via%20Code-Guided%20Synthetic%0A%20%20Multimodal%20Data%20Generation&body=Title%3A%20Scaling%20Text-Rich%20Image%20Understanding%20via%20Code-Guided%20Synthetic%0A%20%20Multimodal%20Data%20Generation%0AAuthor%3A%20Yue%20Yang%20and%20Ajay%20Patel%20and%20Matt%20Deitke%20and%20Tanmay%20Gupta%20and%20Luca%20Weihs%20and%20Andrew%20Head%20and%20Mark%20Yatskar%20and%20Chris%20Callison-Burch%20and%20Ranjay%20Krishna%20and%20Aniruddha%20Kembhavi%20and%20Christopher%20Clark%0AAbstract%3A%20%20%20Reasoning%20about%20images%20with%20rich%20text%2C%20such%20as%20charts%20and%20documents%2C%20is%20a%0Acritical%20application%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20VLMs%20often%0Astruggle%20in%20these%20domains%20due%20to%20the%20scarcity%20of%20diverse%20text-rich%0Avision-language%20data.%20To%20address%20this%20challenge%2C%20we%20present%20CoSyn%2C%20a%20framework%0Athat%20leverages%20the%20coding%20capabilities%20of%20text-only%20large%20language%20models%0A%28LLMs%29%20to%20automatically%20create%20synthetic%20text-rich%20multimodal%20data.%20Given%20input%0Atext%20describing%20a%20target%20domain%20%28e.g.%2C%20%22nutrition%20fact%20labels%22%29%2C%20CoSyn%20prompts%0Aan%20LLM%20to%20generate%20code%20%28Python%2C%20HTML%2C%20LaTeX%2C%20etc.%29%20for%20rendering%20synthetic%0Aimages.%20With%20the%20underlying%20code%20as%20textual%20representations%20of%20the%20synthetic%0Aimages%2C%20CoSyn%20can%20generate%20high-quality%20instruction-tuning%20data%2C%20again%20relying%0Aon%20a%20text-only%20LLM.%20Using%20CoSyn%2C%20we%20constructed%20a%20dataset%20comprising%20400K%0Aimages%20and%202.7M%20rows%20of%20vision-language%20instruction-tuning%20data.%20Comprehensive%0Aexperiments%20on%20seven%20benchmarks%20demonstrate%20that%20models%20trained%20on%20our%0Asynthetic%20data%20achieve%20state-of-the-art%20performance%20among%20competitive%0Aopen-source%20models%2C%20including%20Llama%203.2%2C%20and%20surpass%20proprietary%20models%20such%20as%0AGPT-4V%20and%20Gemini%201.5%20Flash.%20Furthermore%2C%20CoSyn%20can%20produce%20synthetic%20pointing%0Adata%2C%20enabling%20VLMs%20to%20ground%20information%20within%20input%20images%2C%20showcasing%20its%0Apotential%20for%20developing%20multimodal%20agents%20capable%20of%20acting%20in%20real-world%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Text-Rich%2520Image%2520Understanding%2520via%2520Code-Guided%2520Synthetic%250A%2520%2520Multimodal%2520Data%2520Generation%26entry.906535625%3DYue%2520Yang%2520and%2520Ajay%2520Patel%2520and%2520Matt%2520Deitke%2520and%2520Tanmay%2520Gupta%2520and%2520Luca%2520Weihs%2520and%2520Andrew%2520Head%2520and%2520Mark%2520Yatskar%2520and%2520Chris%2520Callison-Burch%2520and%2520Ranjay%2520Krishna%2520and%2520Aniruddha%2520Kembhavi%2520and%2520Christopher%2520Clark%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520images%2520with%2520rich%2520text%252C%2520such%2520as%2520charts%2520and%2520documents%252C%2520is%2520a%250Acritical%2520application%2520of%2520vision-language%2520models%2520%2528VLMs%2529.%2520However%252C%2520VLMs%2520often%250Astruggle%2520in%2520these%2520domains%2520due%2520to%2520the%2520scarcity%2520of%2520diverse%2520text-rich%250Avision-language%2520data.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520CoSyn%252C%2520a%2520framework%250Athat%2520leverages%2520the%2520coding%2520capabilities%2520of%2520text-only%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520automatically%2520create%2520synthetic%2520text-rich%2520multimodal%2520data.%2520Given%2520input%250Atext%2520describing%2520a%2520target%2520domain%2520%2528e.g.%252C%2520%2522nutrition%2520fact%2520labels%2522%2529%252C%2520CoSyn%2520prompts%250Aan%2520LLM%2520to%2520generate%2520code%2520%2528Python%252C%2520HTML%252C%2520LaTeX%252C%2520etc.%2529%2520for%2520rendering%2520synthetic%250Aimages.%2520With%2520the%2520underlying%2520code%2520as%2520textual%2520representations%2520of%2520the%2520synthetic%250Aimages%252C%2520CoSyn%2520can%2520generate%2520high-quality%2520instruction-tuning%2520data%252C%2520again%2520relying%250Aon%2520a%2520text-only%2520LLM.%2520Using%2520CoSyn%252C%2520we%2520constructed%2520a%2520dataset%2520comprising%2520400K%250Aimages%2520and%25202.7M%2520rows%2520of%2520vision-language%2520instruction-tuning%2520data.%2520Comprehensive%250Aexperiments%2520on%2520seven%2520benchmarks%2520demonstrate%2520that%2520models%2520trained%2520on%2520our%250Asynthetic%2520data%2520achieve%2520state-of-the-art%2520performance%2520among%2520competitive%250Aopen-source%2520models%252C%2520including%2520Llama%25203.2%252C%2520and%2520surpass%2520proprietary%2520models%2520such%2520as%250AGPT-4V%2520and%2520Gemini%25201.5%2520Flash.%2520Furthermore%252C%2520CoSyn%2520can%2520produce%2520synthetic%2520pointing%250Adata%252C%2520enabling%2520VLMs%2520to%2520ground%2520information%2520within%2520input%2520images%252C%2520showcasing%2520its%250Apotential%2520for%2520developing%2520multimodal%2520agents%2520capable%2520of%2520acting%2520in%2520real-world%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Text-Rich%20Image%20Understanding%20via%20Code-Guided%20Synthetic%0A%20%20Multimodal%20Data%20Generation&entry.906535625=Yue%20Yang%20and%20Ajay%20Patel%20and%20Matt%20Deitke%20and%20Tanmay%20Gupta%20and%20Luca%20Weihs%20and%20Andrew%20Head%20and%20Mark%20Yatskar%20and%20Chris%20Callison-Burch%20and%20Ranjay%20Krishna%20and%20Aniruddha%20Kembhavi%20and%20Christopher%20Clark&entry.1292438233=%20%20Reasoning%20about%20images%20with%20rich%20text%2C%20such%20as%20charts%20and%20documents%2C%20is%20a%0Acritical%20application%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20VLMs%20often%0Astruggle%20in%20these%20domains%20due%20to%20the%20scarcity%20of%20diverse%20text-rich%0Avision-language%20data.%20To%20address%20this%20challenge%2C%20we%20present%20CoSyn%2C%20a%20framework%0Athat%20leverages%20the%20coding%20capabilities%20of%20text-only%20large%20language%20models%0A%28LLMs%29%20to%20automatically%20create%20synthetic%20text-rich%20multimodal%20data.%20Given%20input%0Atext%20describing%20a%20target%20domain%20%28e.g.%2C%20%22nutrition%20fact%20labels%22%29%2C%20CoSyn%20prompts%0Aan%20LLM%20to%20generate%20code%20%28Python%2C%20HTML%2C%20LaTeX%2C%20etc.%29%20for%20rendering%20synthetic%0Aimages.%20With%20the%20underlying%20code%20as%20textual%20representations%20of%20the%20synthetic%0Aimages%2C%20CoSyn%20can%20generate%20high-quality%20instruction-tuning%20data%2C%20again%20relying%0Aon%20a%20text-only%20LLM.%20Using%20CoSyn%2C%20we%20constructed%20a%20dataset%20comprising%20400K%0Aimages%20and%202.7M%20rows%20of%20vision-language%20instruction-tuning%20data.%20Comprehensive%0Aexperiments%20on%20seven%20benchmarks%20demonstrate%20that%20models%20trained%20on%20our%0Asynthetic%20data%20achieve%20state-of-the-art%20performance%20among%20competitive%0Aopen-source%20models%2C%20including%20Llama%203.2%2C%20and%20surpass%20proprietary%20models%20such%20as%0AGPT-4V%20and%20Gemini%201.5%20Flash.%20Furthermore%2C%20CoSyn%20can%20produce%20synthetic%20pointing%0Adata%2C%20enabling%20VLMs%20to%20ground%20information%20within%20input%20images%2C%20showcasing%20its%0Apotential%20for%20developing%20multimodal%20agents%20capable%20of%20acting%20in%20real-world%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14846v1&entry.124074799=Read"},
{"title": "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?", "author": "Sougata Saha and Saurabh Kumar Pandey and Harshit Gupta and Monojit Choudhury", "abstract": "  In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps://github.com/sougata-ub/reading_between_lines\n", "link": "http://arxiv.org/abs/2502.09636v2", "date": "2025-02-20", "relevancy": 2.288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reading%20between%20the%20Lines%3A%20Can%20LLMs%20Identify%20Cross-Cultural%0A%20%20Communication%20Gaps%3F&body=Title%3A%20Reading%20between%20the%20Lines%3A%20Can%20LLMs%20Identify%20Cross-Cultural%0A%20%20Communication%20Gaps%3F%0AAuthor%3A%20Sougata%20Saha%20and%20Saurabh%20Kumar%20Pandey%20and%20Harshit%20Gupta%20and%20Monojit%20Choudhury%0AAbstract%3A%20%20%20In%20a%20rapidly%20globalizing%20and%20digital%20world%2C%20content%20such%20as%20book%20and%20product%0Areviews%20created%20by%20people%20from%20diverse%20cultures%20are%20read%20and%20consumed%20by%20others%0Afrom%20different%20corners%20of%20the%20world.%20In%20this%20paper%2C%20we%20investigate%20the%20extent%0Aand%20patterns%20of%20gaps%20in%20understandability%20of%20book%20reviews%20due%20to%20the%20presence%0Aof%20culturally-specific%20items%20and%20elements%20that%20might%20be%20alien%20to%20users%20from%0Aanother%20culture.%20Our%20user-study%20on%2057%20book%20reviews%20from%20Goodreads%20reveal%20that%0A83%5C%25%20of%20the%20reviews%20had%20at%20least%20one%20culture-specific%20difficult-to-understand%0Aelement.%20We%20also%20evaluate%20the%20efficacy%20of%20GPT-4o%20in%20identifying%20such%20items%2C%0Agiven%20the%20cultural%20background%20of%20the%20reader%3B%20the%20results%20are%20mixed%2C%20implying%20a%0Asignificant%20scope%20for%20improvement.%20Our%20datasets%20are%20available%20here%3A%0Ahttps%3A//github.com/sougata-ub/reading_between_lines%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09636v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReading%2520between%2520the%2520Lines%253A%2520Can%2520LLMs%2520Identify%2520Cross-Cultural%250A%2520%2520Communication%2520Gaps%253F%26entry.906535625%3DSougata%2520Saha%2520and%2520Saurabh%2520Kumar%2520Pandey%2520and%2520Harshit%2520Gupta%2520and%2520Monojit%2520Choudhury%26entry.1292438233%3D%2520%2520In%2520a%2520rapidly%2520globalizing%2520and%2520digital%2520world%252C%2520content%2520such%2520as%2520book%2520and%2520product%250Areviews%2520created%2520by%2520people%2520from%2520diverse%2520cultures%2520are%2520read%2520and%2520consumed%2520by%2520others%250Afrom%2520different%2520corners%2520of%2520the%2520world.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520extent%250Aand%2520patterns%2520of%2520gaps%2520in%2520understandability%2520of%2520book%2520reviews%2520due%2520to%2520the%2520presence%250Aof%2520culturally-specific%2520items%2520and%2520elements%2520that%2520might%2520be%2520alien%2520to%2520users%2520from%250Aanother%2520culture.%2520Our%2520user-study%2520on%252057%2520book%2520reviews%2520from%2520Goodreads%2520reveal%2520that%250A83%255C%2525%2520of%2520the%2520reviews%2520had%2520at%2520least%2520one%2520culture-specific%2520difficult-to-understand%250Aelement.%2520We%2520also%2520evaluate%2520the%2520efficacy%2520of%2520GPT-4o%2520in%2520identifying%2520such%2520items%252C%250Agiven%2520the%2520cultural%2520background%2520of%2520the%2520reader%253B%2520the%2520results%2520are%2520mixed%252C%2520implying%2520a%250Asignificant%2520scope%2520for%2520improvement.%2520Our%2520datasets%2520are%2520available%2520here%253A%250Ahttps%253A//github.com/sougata-ub/reading_between_lines%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09636v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20between%20the%20Lines%3A%20Can%20LLMs%20Identify%20Cross-Cultural%0A%20%20Communication%20Gaps%3F&entry.906535625=Sougata%20Saha%20and%20Saurabh%20Kumar%20Pandey%20and%20Harshit%20Gupta%20and%20Monojit%20Choudhury&entry.1292438233=%20%20In%20a%20rapidly%20globalizing%20and%20digital%20world%2C%20content%20such%20as%20book%20and%20product%0Areviews%20created%20by%20people%20from%20diverse%20cultures%20are%20read%20and%20consumed%20by%20others%0Afrom%20different%20corners%20of%20the%20world.%20In%20this%20paper%2C%20we%20investigate%20the%20extent%0Aand%20patterns%20of%20gaps%20in%20understandability%20of%20book%20reviews%20due%20to%20the%20presence%0Aof%20culturally-specific%20items%20and%20elements%20that%20might%20be%20alien%20to%20users%20from%0Aanother%20culture.%20Our%20user-study%20on%2057%20book%20reviews%20from%20Goodreads%20reveal%20that%0A83%5C%25%20of%20the%20reviews%20had%20at%20least%20one%20culture-specific%20difficult-to-understand%0Aelement.%20We%20also%20evaluate%20the%20efficacy%20of%20GPT-4o%20in%20identifying%20such%20items%2C%0Agiven%20the%20cultural%20background%20of%20the%20reader%3B%20the%20results%20are%20mixed%2C%20implying%20a%0Asignificant%20scope%20for%20improvement.%20Our%20datasets%20are%20available%20here%3A%0Ahttps%3A//github.com/sougata-ub/reading_between_lines%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09636v2&entry.124074799=Read"},
{"title": "VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs", "author": "Moayed Haji Ali and Andrew Bond and Tolga Birdal and Duygu Ceylan and Levent Karacan and Erkut Erdem and Aykut Erdem", "abstract": "  We propose $\\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled\n$\\textbf{Vid}$eo representation based upon $\\textbf{Style}$GAN and\nNeural-$\\textbf{ODE}$s. Effective traversal of the latent space learned by\nGenerative Adversarial Networks (GANs) has been the basis for recent\nbreakthroughs in image editing. However, the applicability of such advancements\nto the video domain has been hindered by the difficulty of representing and\ncontrolling videos in the latent space of GANs. In particular, videos are\ncomposed of content (i.e., appearance) and complex motion components that\nrequire a special mechanism to disentangle and control. To achieve this,\nVidStyleODE encodes the video content in a pre-trained StyleGAN $\\mathcal{W}_+$\nspace and benefits from a latent ODE component to summarize the spatiotemporal\ndynamics of the input video. Our novel continuous video generation process then\ncombines the two to generate high-quality and temporally consistent videos with\nvarying frame rates. We show that our proposed method enables a variety of\napplications on real videos: text-guided appearance manipulation, motion\nmanipulation, image animation, and video interpolation and extrapolation.\nProject website: https://cyberiada.github.io/VidStyleODE\n", "link": "http://arxiv.org/abs/2304.06020v2", "date": "2025-02-20", "relevancy": 2.2772, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5818}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5803}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidStyleODE%3A%20Disentangled%20Video%20Editing%20via%20StyleGAN%20and%20NeuralODEs&body=Title%3A%20VidStyleODE%3A%20Disentangled%20Video%20Editing%20via%20StyleGAN%20and%20NeuralODEs%0AAuthor%3A%20Moayed%20Haji%20Ali%20and%20Andrew%20Bond%20and%20Tolga%20Birdal%20and%20Duygu%20Ceylan%20and%20Levent%20Karacan%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem%0AAbstract%3A%20%20%20We%20propose%20%24%5Ctextbf%7BVidStyleODE%7D%24%2C%20a%20spatiotemporally%20continuous%20disentangled%0A%24%5Ctextbf%7BVid%7D%24eo%20representation%20based%20upon%20%24%5Ctextbf%7BStyle%7D%24GAN%20and%0ANeural-%24%5Ctextbf%7BODE%7D%24s.%20Effective%20traversal%20of%20the%20latent%20space%20learned%20by%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20has%20been%20the%20basis%20for%20recent%0Abreakthroughs%20in%20image%20editing.%20However%2C%20the%20applicability%20of%20such%20advancements%0Ato%20the%20video%20domain%20has%20been%20hindered%20by%20the%20difficulty%20of%20representing%20and%0Acontrolling%20videos%20in%20the%20latent%20space%20of%20GANs.%20In%20particular%2C%20videos%20are%0Acomposed%20of%20content%20%28i.e.%2C%20appearance%29%20and%20complex%20motion%20components%20that%0Arequire%20a%20special%20mechanism%20to%20disentangle%20and%20control.%20To%20achieve%20this%2C%0AVidStyleODE%20encodes%20the%20video%20content%20in%20a%20pre-trained%20StyleGAN%20%24%5Cmathcal%7BW%7D_%2B%24%0Aspace%20and%20benefits%20from%20a%20latent%20ODE%20component%20to%20summarize%20the%20spatiotemporal%0Adynamics%20of%20the%20input%20video.%20Our%20novel%20continuous%20video%20generation%20process%20then%0Acombines%20the%20two%20to%20generate%20high-quality%20and%20temporally%20consistent%20videos%20with%0Avarying%20frame%20rates.%20We%20show%20that%20our%20proposed%20method%20enables%20a%20variety%20of%0Aapplications%20on%20real%20videos%3A%20text-guided%20appearance%20manipulation%2C%20motion%0Amanipulation%2C%20image%20animation%2C%20and%20video%20interpolation%20and%20extrapolation.%0AProject%20website%3A%20https%3A//cyberiada.github.io/VidStyleODE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidStyleODE%253A%2520Disentangled%2520Video%2520Editing%2520via%2520StyleGAN%2520and%2520NeuralODEs%26entry.906535625%3DMoayed%2520Haji%2520Ali%2520and%2520Andrew%2520Bond%2520and%2520Tolga%2520Birdal%2520and%2520Duygu%2520Ceylan%2520and%2520Levent%2520Karacan%2520and%2520Erkut%2520Erdem%2520and%2520Aykut%2520Erdem%26entry.1292438233%3D%2520%2520We%2520propose%2520%2524%255Ctextbf%257BVidStyleODE%257D%2524%252C%2520a%2520spatiotemporally%2520continuous%2520disentangled%250A%2524%255Ctextbf%257BVid%257D%2524eo%2520representation%2520based%2520upon%2520%2524%255Ctextbf%257BStyle%257D%2524GAN%2520and%250ANeural-%2524%255Ctextbf%257BODE%257D%2524s.%2520Effective%2520traversal%2520of%2520the%2520latent%2520space%2520learned%2520by%250AGenerative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520has%2520been%2520the%2520basis%2520for%2520recent%250Abreakthroughs%2520in%2520image%2520editing.%2520However%252C%2520the%2520applicability%2520of%2520such%2520advancements%250Ato%2520the%2520video%2520domain%2520has%2520been%2520hindered%2520by%2520the%2520difficulty%2520of%2520representing%2520and%250Acontrolling%2520videos%2520in%2520the%2520latent%2520space%2520of%2520GANs.%2520In%2520particular%252C%2520videos%2520are%250Acomposed%2520of%2520content%2520%2528i.e.%252C%2520appearance%2529%2520and%2520complex%2520motion%2520components%2520that%250Arequire%2520a%2520special%2520mechanism%2520to%2520disentangle%2520and%2520control.%2520To%2520achieve%2520this%252C%250AVidStyleODE%2520encodes%2520the%2520video%2520content%2520in%2520a%2520pre-trained%2520StyleGAN%2520%2524%255Cmathcal%257BW%257D_%252B%2524%250Aspace%2520and%2520benefits%2520from%2520a%2520latent%2520ODE%2520component%2520to%2520summarize%2520the%2520spatiotemporal%250Adynamics%2520of%2520the%2520input%2520video.%2520Our%2520novel%2520continuous%2520video%2520generation%2520process%2520then%250Acombines%2520the%2520two%2520to%2520generate%2520high-quality%2520and%2520temporally%2520consistent%2520videos%2520with%250Avarying%2520frame%2520rates.%2520We%2520show%2520that%2520our%2520proposed%2520method%2520enables%2520a%2520variety%2520of%250Aapplications%2520on%2520real%2520videos%253A%2520text-guided%2520appearance%2520manipulation%252C%2520motion%250Amanipulation%252C%2520image%2520animation%252C%2520and%2520video%2520interpolation%2520and%2520extrapolation.%250AProject%2520website%253A%2520https%253A//cyberiada.github.io/VidStyleODE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.06020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidStyleODE%3A%20Disentangled%20Video%20Editing%20via%20StyleGAN%20and%20NeuralODEs&entry.906535625=Moayed%20Haji%20Ali%20and%20Andrew%20Bond%20and%20Tolga%20Birdal%20and%20Duygu%20Ceylan%20and%20Levent%20Karacan%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem&entry.1292438233=%20%20We%20propose%20%24%5Ctextbf%7BVidStyleODE%7D%24%2C%20a%20spatiotemporally%20continuous%20disentangled%0A%24%5Ctextbf%7BVid%7D%24eo%20representation%20based%20upon%20%24%5Ctextbf%7BStyle%7D%24GAN%20and%0ANeural-%24%5Ctextbf%7BODE%7D%24s.%20Effective%20traversal%20of%20the%20latent%20space%20learned%20by%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20has%20been%20the%20basis%20for%20recent%0Abreakthroughs%20in%20image%20editing.%20However%2C%20the%20applicability%20of%20such%20advancements%0Ato%20the%20video%20domain%20has%20been%20hindered%20by%20the%20difficulty%20of%20representing%20and%0Acontrolling%20videos%20in%20the%20latent%20space%20of%20GANs.%20In%20particular%2C%20videos%20are%0Acomposed%20of%20content%20%28i.e.%2C%20appearance%29%20and%20complex%20motion%20components%20that%0Arequire%20a%20special%20mechanism%20to%20disentangle%20and%20control.%20To%20achieve%20this%2C%0AVidStyleODE%20encodes%20the%20video%20content%20in%20a%20pre-trained%20StyleGAN%20%24%5Cmathcal%7BW%7D_%2B%24%0Aspace%20and%20benefits%20from%20a%20latent%20ODE%20component%20to%20summarize%20the%20spatiotemporal%0Adynamics%20of%20the%20input%20video.%20Our%20novel%20continuous%20video%20generation%20process%20then%0Acombines%20the%20two%20to%20generate%20high-quality%20and%20temporally%20consistent%20videos%20with%0Avarying%20frame%20rates.%20We%20show%20that%20our%20proposed%20method%20enables%20a%20variety%20of%0Aapplications%20on%20real%20videos%3A%20text-guided%20appearance%20manipulation%2C%20motion%0Amanipulation%2C%20image%20animation%2C%20and%20video%20interpolation%20and%20extrapolation.%0AProject%20website%3A%20https%3A//cyberiada.github.io/VidStyleODE%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06020v2&entry.124074799=Read"},
{"title": "$O(k)$-Equivariant Dimensionality Reduction on Stiefel Manifolds", "author": "Andrew Lee and Harlin Lee and Jose A. Perea and Nikolas Schonsheck and Madeleine Weinstein", "abstract": "  Many real-world datasets live on high-dimensional Stiefel and Grassmannian\nmanifolds, $V_k(\\mathbb{R}^N)$ and $Gr(k, \\mathbb{R}^N)$ respectively, and\nbenefit from projection onto lower-dimensional Stiefel and Grassmannian\nmanifolds. In this work, we propose an algorithm called \\textit{Principal\nStiefel Coordinates (PSC)} to reduce data dimensionality from $\nV_k(\\mathbb{R}^N)$ to $V_k(\\mathbb{R}^n)$ in an \\textit{$O(k)$-equivariant}\nmanner ($k \\leq n \\ll N$). We begin by observing that each element $\\alpha \\in\nV_n(\\mathbb{R}^N)$ defines an isometric embedding of $V_k(\\mathbb{R}^n)$ into\n$V_k(\\mathbb{R}^N)$. Next, we describe two ways of finding a suitable embedding\nmap $\\alpha$: one via an extension of principal component analysis\n($\\alpha_{PCA}$), and one that further minimizes data fit error using gradient\ndescent ($\\alpha_{GD}$). Then, we define a continuous and $O(k)$-equivariant\nmap $\\pi_\\alpha$ that acts as a \"closest point operator\" to project the data\nonto the image of $V_k(\\mathbb{R}^n)$ in $V_k(\\mathbb{R}^N)$ under the\nembedding determined by $\\alpha$, while minimizing distortion. Because this\ndimensionality reduction is $O(k)$-equivariant, these results extend to\nGrassmannian manifolds as well. Lastly, we show that $\\pi_{\\alpha_{PCA}}$\nglobally minimizes projection error in a noiseless setting, while\n$\\pi_{\\alpha_{GD}}$ achieves a meaningfully different and improved outcome when\nthe data does not lie exactly on the image of a linearly embedded\nlower-dimensional Stiefel manifold as above. Multiple numerical experiments\nusing synthetic and real-world data are performed.\n", "link": "http://arxiv.org/abs/2309.10775v3", "date": "2025-02-20", "relevancy": 2.2358, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4522}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4452}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24O%28k%29%24-Equivariant%20Dimensionality%20Reduction%20on%20Stiefel%20Manifolds&body=Title%3A%20%24O%28k%29%24-Equivariant%20Dimensionality%20Reduction%20on%20Stiefel%20Manifolds%0AAuthor%3A%20Andrew%20Lee%20and%20Harlin%20Lee%20and%20Jose%20A.%20Perea%20and%20Nikolas%20Schonsheck%20and%20Madeleine%20Weinstein%0AAbstract%3A%20%20%20Many%20real-world%20datasets%20live%20on%20high-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds%2C%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20and%20%24Gr%28k%2C%20%5Cmathbb%7BR%7D%5EN%29%24%20respectively%2C%20and%0Abenefit%20from%20projection%20onto%20lower-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds.%20In%20this%20work%2C%20we%20propose%20an%20algorithm%20called%20%5Ctextit%7BPrincipal%0AStiefel%20Coordinates%20%28PSC%29%7D%20to%20reduce%20data%20dimensionality%20from%20%24%0AV_k%28%5Cmathbb%7BR%7D%5EN%29%24%20to%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20an%20%5Ctextit%7B%24O%28k%29%24-equivariant%7D%0Amanner%20%28%24k%20%5Cleq%20n%20%5Cll%20N%24%29.%20We%20begin%20by%20observing%20that%20each%20element%20%24%5Calpha%20%5Cin%0AV_n%28%5Cmathbb%7BR%7D%5EN%29%24%20defines%20an%20isometric%20embedding%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20into%0A%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24.%20Next%2C%20we%20describe%20two%20ways%20of%20finding%20a%20suitable%20embedding%0Amap%20%24%5Calpha%24%3A%20one%20via%20an%20extension%20of%20principal%20component%20analysis%0A%28%24%5Calpha_%7BPCA%7D%24%29%2C%20and%20one%20that%20further%20minimizes%20data%20fit%20error%20using%20gradient%0Adescent%20%28%24%5Calpha_%7BGD%7D%24%29.%20Then%2C%20we%20define%20a%20continuous%20and%20%24O%28k%29%24-equivariant%0Amap%20%24%5Cpi_%5Calpha%24%20that%20acts%20as%20a%20%22closest%20point%20operator%22%20to%20project%20the%20data%0Aonto%20the%20image%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20under%20the%0Aembedding%20determined%20by%20%24%5Calpha%24%2C%20while%20minimizing%20distortion.%20Because%20this%0Adimensionality%20reduction%20is%20%24O%28k%29%24-equivariant%2C%20these%20results%20extend%20to%0AGrassmannian%20manifolds%20as%20well.%20Lastly%2C%20we%20show%20that%20%24%5Cpi_%7B%5Calpha_%7BPCA%7D%7D%24%0Aglobally%20minimizes%20projection%20error%20in%20a%20noiseless%20setting%2C%20while%0A%24%5Cpi_%7B%5Calpha_%7BGD%7D%7D%24%20achieves%20a%20meaningfully%20different%20and%20improved%20outcome%20when%0Athe%20data%20does%20not%20lie%20exactly%20on%20the%20image%20of%20a%20linearly%20embedded%0Alower-dimensional%20Stiefel%20manifold%20as%20above.%20Multiple%20numerical%20experiments%0Ausing%20synthetic%20and%20real-world%20data%20are%20performed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10775v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524O%2528k%2529%2524-Equivariant%2520Dimensionality%2520Reduction%2520on%2520Stiefel%2520Manifolds%26entry.906535625%3DAndrew%2520Lee%2520and%2520Harlin%2520Lee%2520and%2520Jose%2520A.%2520Perea%2520and%2520Nikolas%2520Schonsheck%2520and%2520Madeleine%2520Weinstein%26entry.1292438233%3D%2520%2520Many%2520real-world%2520datasets%2520live%2520on%2520high-dimensional%2520Stiefel%2520and%2520Grassmannian%250Amanifolds%252C%2520%2524V_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520and%2520%2524Gr%2528k%252C%2520%255Cmathbb%257BR%257D%255EN%2529%2524%2520respectively%252C%2520and%250Abenefit%2520from%2520projection%2520onto%2520lower-dimensional%2520Stiefel%2520and%2520Grassmannian%250Amanifolds.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520algorithm%2520called%2520%255Ctextit%257BPrincipal%250AStiefel%2520Coordinates%2520%2528PSC%2529%257D%2520to%2520reduce%2520data%2520dimensionality%2520from%2520%2524%250AV_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520to%2520%2524V_k%2528%255Cmathbb%257BR%257D%255En%2529%2524%2520in%2520an%2520%255Ctextit%257B%2524O%2528k%2529%2524-equivariant%257D%250Amanner%2520%2528%2524k%2520%255Cleq%2520n%2520%255Cll%2520N%2524%2529.%2520We%2520begin%2520by%2520observing%2520that%2520each%2520element%2520%2524%255Calpha%2520%255Cin%250AV_n%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520defines%2520an%2520isometric%2520embedding%2520of%2520%2524V_k%2528%255Cmathbb%257BR%257D%255En%2529%2524%2520into%250A%2524V_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524.%2520Next%252C%2520we%2520describe%2520two%2520ways%2520of%2520finding%2520a%2520suitable%2520embedding%250Amap%2520%2524%255Calpha%2524%253A%2520one%2520via%2520an%2520extension%2520of%2520principal%2520component%2520analysis%250A%2528%2524%255Calpha_%257BPCA%257D%2524%2529%252C%2520and%2520one%2520that%2520further%2520minimizes%2520data%2520fit%2520error%2520using%2520gradient%250Adescent%2520%2528%2524%255Calpha_%257BGD%257D%2524%2529.%2520Then%252C%2520we%2520define%2520a%2520continuous%2520and%2520%2524O%2528k%2529%2524-equivariant%250Amap%2520%2524%255Cpi_%255Calpha%2524%2520that%2520acts%2520as%2520a%2520%2522closest%2520point%2520operator%2522%2520to%2520project%2520the%2520data%250Aonto%2520the%2520image%2520of%2520%2524V_k%2528%255Cmathbb%257BR%257D%255En%2529%2524%2520in%2520%2524V_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520under%2520the%250Aembedding%2520determined%2520by%2520%2524%255Calpha%2524%252C%2520while%2520minimizing%2520distortion.%2520Because%2520this%250Adimensionality%2520reduction%2520is%2520%2524O%2528k%2529%2524-equivariant%252C%2520these%2520results%2520extend%2520to%250AGrassmannian%2520manifolds%2520as%2520well.%2520Lastly%252C%2520we%2520show%2520that%2520%2524%255Cpi_%257B%255Calpha_%257BPCA%257D%257D%2524%250Aglobally%2520minimizes%2520projection%2520error%2520in%2520a%2520noiseless%2520setting%252C%2520while%250A%2524%255Cpi_%257B%255Calpha_%257BGD%257D%257D%2524%2520achieves%2520a%2520meaningfully%2520different%2520and%2520improved%2520outcome%2520when%250Athe%2520data%2520does%2520not%2520lie%2520exactly%2520on%2520the%2520image%2520of%2520a%2520linearly%2520embedded%250Alower-dimensional%2520Stiefel%2520manifold%2520as%2520above.%2520Multiple%2520numerical%2520experiments%250Ausing%2520synthetic%2520and%2520real-world%2520data%2520are%2520performed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10775v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24O%28k%29%24-Equivariant%20Dimensionality%20Reduction%20on%20Stiefel%20Manifolds&entry.906535625=Andrew%20Lee%20and%20Harlin%20Lee%20and%20Jose%20A.%20Perea%20and%20Nikolas%20Schonsheck%20and%20Madeleine%20Weinstein&entry.1292438233=%20%20Many%20real-world%20datasets%20live%20on%20high-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds%2C%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20and%20%24Gr%28k%2C%20%5Cmathbb%7BR%7D%5EN%29%24%20respectively%2C%20and%0Abenefit%20from%20projection%20onto%20lower-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds.%20In%20this%20work%2C%20we%20propose%20an%20algorithm%20called%20%5Ctextit%7BPrincipal%0AStiefel%20Coordinates%20%28PSC%29%7D%20to%20reduce%20data%20dimensionality%20from%20%24%0AV_k%28%5Cmathbb%7BR%7D%5EN%29%24%20to%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20an%20%5Ctextit%7B%24O%28k%29%24-equivariant%7D%0Amanner%20%28%24k%20%5Cleq%20n%20%5Cll%20N%24%29.%20We%20begin%20by%20observing%20that%20each%20element%20%24%5Calpha%20%5Cin%0AV_n%28%5Cmathbb%7BR%7D%5EN%29%24%20defines%20an%20isometric%20embedding%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20into%0A%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24.%20Next%2C%20we%20describe%20two%20ways%20of%20finding%20a%20suitable%20embedding%0Amap%20%24%5Calpha%24%3A%20one%20via%20an%20extension%20of%20principal%20component%20analysis%0A%28%24%5Calpha_%7BPCA%7D%24%29%2C%20and%20one%20that%20further%20minimizes%20data%20fit%20error%20using%20gradient%0Adescent%20%28%24%5Calpha_%7BGD%7D%24%29.%20Then%2C%20we%20define%20a%20continuous%20and%20%24O%28k%29%24-equivariant%0Amap%20%24%5Cpi_%5Calpha%24%20that%20acts%20as%20a%20%22closest%20point%20operator%22%20to%20project%20the%20data%0Aonto%20the%20image%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20under%20the%0Aembedding%20determined%20by%20%24%5Calpha%24%2C%20while%20minimizing%20distortion.%20Because%20this%0Adimensionality%20reduction%20is%20%24O%28k%29%24-equivariant%2C%20these%20results%20extend%20to%0AGrassmannian%20manifolds%20as%20well.%20Lastly%2C%20we%20show%20that%20%24%5Cpi_%7B%5Calpha_%7BPCA%7D%7D%24%0Aglobally%20minimizes%20projection%20error%20in%20a%20noiseless%20setting%2C%20while%0A%24%5Cpi_%7B%5Calpha_%7BGD%7D%7D%24%20achieves%20a%20meaningfully%20different%20and%20improved%20outcome%20when%0Athe%20data%20does%20not%20lie%20exactly%20on%20the%20image%20of%20a%20linearly%20embedded%0Alower-dimensional%20Stiefel%20manifold%20as%20above.%20Multiple%20numerical%20experiments%0Ausing%20synthetic%20and%20real-world%20data%20are%20performed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10775v3&entry.124074799=Read"},
{"title": "RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye\n  View Segmentation", "author": "Henrique Pi\u00f1eiro Monteagudo and Leonardo Taccari and Aurel Pjetri and Francesco Sambo and Samuele Salti", "abstract": "  Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention\nas a useful representation of the environment to tackle assisted and autonomous\ndriving tasks. However, most of the existing work focuses on the fully\nsupervised setting, training networks on large annotated datasets. In this\nwork, we present RendBEV, a new method for the self-supervised training of BEV\nsemantic segmentation networks, leveraging differentiable volumetric rendering\nto receive supervision from semantic perspective views computed by a 2D\nsemantic segmentation model. Our method enables zero-shot BEV semantic\nsegmentation, and already delivers competitive results in this challenging\nsetting. When used as pretraining to then fine-tune on labeled BEV\nground-truth, our method significantly boosts performance in low-annotation\nregimes, and sets a new state of the art when fine-tuning on all available\nlabels.\n", "link": "http://arxiv.org/abs/2502.14792v1", "date": "2025-02-20", "relevancy": 2.2347, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RendBEV%3A%20Semantic%20Novel%20View%20Synthesis%20for%20Self-Supervised%20Bird%27s%20Eye%0A%20%20View%20Segmentation&body=Title%3A%20RendBEV%3A%20Semantic%20Novel%20View%20Synthesis%20for%20Self-Supervised%20Bird%27s%20Eye%0A%20%20View%20Segmentation%0AAuthor%3A%20Henrique%20Pi%C3%B1eiro%20Monteagudo%20and%20Leonardo%20Taccari%20and%20Aurel%20Pjetri%20and%20Francesco%20Sambo%20and%20Samuele%20Salti%0AAbstract%3A%20%20%20Bird%27s%20Eye%20View%20%28BEV%29%20semantic%20maps%20have%20recently%20garnered%20a%20lot%20of%20attention%0Aas%20a%20useful%20representation%20of%20the%20environment%20to%20tackle%20assisted%20and%20autonomous%0Adriving%20tasks.%20However%2C%20most%20of%20the%20existing%20work%20focuses%20on%20the%20fully%0Asupervised%20setting%2C%20training%20networks%20on%20large%20annotated%20datasets.%20In%20this%0Awork%2C%20we%20present%20RendBEV%2C%20a%20new%20method%20for%20the%20self-supervised%20training%20of%20BEV%0Asemantic%20segmentation%20networks%2C%20leveraging%20differentiable%20volumetric%20rendering%0Ato%20receive%20supervision%20from%20semantic%20perspective%20views%20computed%20by%20a%202D%0Asemantic%20segmentation%20model.%20Our%20method%20enables%20zero-shot%20BEV%20semantic%0Asegmentation%2C%20and%20already%20delivers%20competitive%20results%20in%20this%20challenging%0Asetting.%20When%20used%20as%20pretraining%20to%20then%20fine-tune%20on%20labeled%20BEV%0Aground-truth%2C%20our%20method%20significantly%20boosts%20performance%20in%20low-annotation%0Aregimes%2C%20and%20sets%20a%20new%20state%20of%20the%20art%20when%20fine-tuning%20on%20all%20available%0Alabels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRendBEV%253A%2520Semantic%2520Novel%2520View%2520Synthesis%2520for%2520Self-Supervised%2520Bird%2527s%2520Eye%250A%2520%2520View%2520Segmentation%26entry.906535625%3DHenrique%2520Pi%25C3%25B1eiro%2520Monteagudo%2520and%2520Leonardo%2520Taccari%2520and%2520Aurel%2520Pjetri%2520and%2520Francesco%2520Sambo%2520and%2520Samuele%2520Salti%26entry.1292438233%3D%2520%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520semantic%2520maps%2520have%2520recently%2520garnered%2520a%2520lot%2520of%2520attention%250Aas%2520a%2520useful%2520representation%2520of%2520the%2520environment%2520to%2520tackle%2520assisted%2520and%2520autonomous%250Adriving%2520tasks.%2520However%252C%2520most%2520of%2520the%2520existing%2520work%2520focuses%2520on%2520the%2520fully%250Asupervised%2520setting%252C%2520training%2520networks%2520on%2520large%2520annotated%2520datasets.%2520In%2520this%250Awork%252C%2520we%2520present%2520RendBEV%252C%2520a%2520new%2520method%2520for%2520the%2520self-supervised%2520training%2520of%2520BEV%250Asemantic%2520segmentation%2520networks%252C%2520leveraging%2520differentiable%2520volumetric%2520rendering%250Ato%2520receive%2520supervision%2520from%2520semantic%2520perspective%2520views%2520computed%2520by%2520a%25202D%250Asemantic%2520segmentation%2520model.%2520Our%2520method%2520enables%2520zero-shot%2520BEV%2520semantic%250Asegmentation%252C%2520and%2520already%2520delivers%2520competitive%2520results%2520in%2520this%2520challenging%250Asetting.%2520When%2520used%2520as%2520pretraining%2520to%2520then%2520fine-tune%2520on%2520labeled%2520BEV%250Aground-truth%252C%2520our%2520method%2520significantly%2520boosts%2520performance%2520in%2520low-annotation%250Aregimes%252C%2520and%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520when%2520fine-tuning%2520on%2520all%2520available%250Alabels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RendBEV%3A%20Semantic%20Novel%20View%20Synthesis%20for%20Self-Supervised%20Bird%27s%20Eye%0A%20%20View%20Segmentation&entry.906535625=Henrique%20Pi%C3%B1eiro%20Monteagudo%20and%20Leonardo%20Taccari%20and%20Aurel%20Pjetri%20and%20Francesco%20Sambo%20and%20Samuele%20Salti&entry.1292438233=%20%20Bird%27s%20Eye%20View%20%28BEV%29%20semantic%20maps%20have%20recently%20garnered%20a%20lot%20of%20attention%0Aas%20a%20useful%20representation%20of%20the%20environment%20to%20tackle%20assisted%20and%20autonomous%0Adriving%20tasks.%20However%2C%20most%20of%20the%20existing%20work%20focuses%20on%20the%20fully%0Asupervised%20setting%2C%20training%20networks%20on%20large%20annotated%20datasets.%20In%20this%0Awork%2C%20we%20present%20RendBEV%2C%20a%20new%20method%20for%20the%20self-supervised%20training%20of%20BEV%0Asemantic%20segmentation%20networks%2C%20leveraging%20differentiable%20volumetric%20rendering%0Ato%20receive%20supervision%20from%20semantic%20perspective%20views%20computed%20by%20a%202D%0Asemantic%20segmentation%20model.%20Our%20method%20enables%20zero-shot%20BEV%20semantic%0Asegmentation%2C%20and%20already%20delivers%20competitive%20results%20in%20this%20challenging%0Asetting.%20When%20used%20as%20pretraining%20to%20then%20fine-tune%20on%20labeled%20BEV%0Aground-truth%2C%20our%20method%20significantly%20boosts%20performance%20in%20low-annotation%0Aregimes%2C%20and%20sets%20a%20new%20state%20of%20the%20art%20when%20fine-tuning%20on%20all%20available%0Alabels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14792v1&entry.124074799=Read"},
{"title": "Continuous-Time Line-of-Sight Constrained Trajectory Planning for\n  6-Degree of Freedom Systems", "author": "Christopher R. Hayner and John M. Carson III and Beh\u00e7et A\u00e7\u0131kme\u015fe and Karen Leung", "abstract": "  Perception algorithms are ubiquitous in modern autonomy stacks, providing\nnecessary environmental information to operate in the real world. Many of these\nalgorithms depend on the visibility of keypoints, which must remain within the\nrobot's line-of-sight (LoS), for reliable operation. This paper tackles the\nchallenge of maintaining LoS on such keypoints during robot movement. We\npropose a novel method that addresses these issues by ensuring applicability to\nvarious sensor footprints, adaptability to arbitrary nonlinear system dynamics,\nand constant enforcement of LoS throughout the robot's path. Our experiments\nshow that the proposed approach achieves significantly reduced LoS violation\nand runtime compared to existing state-of-the-art methods in several\nrepresentative and challenging scenarios.\n", "link": "http://arxiv.org/abs/2410.22596v2", "date": "2025-02-20", "relevancy": 2.2317, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5732}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5586}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous-Time%20Line-of-Sight%20Constrained%20Trajectory%20Planning%20for%0A%20%206-Degree%20of%20Freedom%20Systems&body=Title%3A%20Continuous-Time%20Line-of-Sight%20Constrained%20Trajectory%20Planning%20for%0A%20%206-Degree%20of%20Freedom%20Systems%0AAuthor%3A%20Christopher%20R.%20Hayner%20and%20John%20M.%20Carson%20III%20and%20Beh%C3%A7et%20A%C3%A7%C4%B1kme%C5%9Fe%20and%20Karen%20Leung%0AAbstract%3A%20%20%20Perception%20algorithms%20are%20ubiquitous%20in%20modern%20autonomy%20stacks%2C%20providing%0Anecessary%20environmental%20information%20to%20operate%20in%20the%20real%20world.%20Many%20of%20these%0Aalgorithms%20depend%20on%20the%20visibility%20of%20keypoints%2C%20which%20must%20remain%20within%20the%0Arobot%27s%20line-of-sight%20%28LoS%29%2C%20for%20reliable%20operation.%20This%20paper%20tackles%20the%0Achallenge%20of%20maintaining%20LoS%20on%20such%20keypoints%20during%20robot%20movement.%20We%0Apropose%20a%20novel%20method%20that%20addresses%20these%20issues%20by%20ensuring%20applicability%20to%0Avarious%20sensor%20footprints%2C%20adaptability%20to%20arbitrary%20nonlinear%20system%20dynamics%2C%0Aand%20constant%20enforcement%20of%20LoS%20throughout%20the%20robot%27s%20path.%20Our%20experiments%0Ashow%20that%20the%20proposed%20approach%20achieves%20significantly%20reduced%20LoS%20violation%0Aand%20runtime%20compared%20to%20existing%20state-of-the-art%20methods%20in%20several%0Arepresentative%20and%20challenging%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous-Time%2520Line-of-Sight%2520Constrained%2520Trajectory%2520Planning%2520for%250A%2520%25206-Degree%2520of%2520Freedom%2520Systems%26entry.906535625%3DChristopher%2520R.%2520Hayner%2520and%2520John%2520M.%2520Carson%2520III%2520and%2520Beh%25C3%25A7et%2520A%25C3%25A7%25C4%25B1kme%25C5%259Fe%2520and%2520Karen%2520Leung%26entry.1292438233%3D%2520%2520Perception%2520algorithms%2520are%2520ubiquitous%2520in%2520modern%2520autonomy%2520stacks%252C%2520providing%250Anecessary%2520environmental%2520information%2520to%2520operate%2520in%2520the%2520real%2520world.%2520Many%2520of%2520these%250Aalgorithms%2520depend%2520on%2520the%2520visibility%2520of%2520keypoints%252C%2520which%2520must%2520remain%2520within%2520the%250Arobot%2527s%2520line-of-sight%2520%2528LoS%2529%252C%2520for%2520reliable%2520operation.%2520This%2520paper%2520tackles%2520the%250Achallenge%2520of%2520maintaining%2520LoS%2520on%2520such%2520keypoints%2520during%2520robot%2520movement.%2520We%250Apropose%2520a%2520novel%2520method%2520that%2520addresses%2520these%2520issues%2520by%2520ensuring%2520applicability%2520to%250Avarious%2520sensor%2520footprints%252C%2520adaptability%2520to%2520arbitrary%2520nonlinear%2520system%2520dynamics%252C%250Aand%2520constant%2520enforcement%2520of%2520LoS%2520throughout%2520the%2520robot%2527s%2520path.%2520Our%2520experiments%250Ashow%2520that%2520the%2520proposed%2520approach%2520achieves%2520significantly%2520reduced%2520LoS%2520violation%250Aand%2520runtime%2520compared%2520to%2520existing%2520state-of-the-art%2520methods%2520in%2520several%250Arepresentative%2520and%2520challenging%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous-Time%20Line-of-Sight%20Constrained%20Trajectory%20Planning%20for%0A%20%206-Degree%20of%20Freedom%20Systems&entry.906535625=Christopher%20R.%20Hayner%20and%20John%20M.%20Carson%20III%20and%20Beh%C3%A7et%20A%C3%A7%C4%B1kme%C5%9Fe%20and%20Karen%20Leung&entry.1292438233=%20%20Perception%20algorithms%20are%20ubiquitous%20in%20modern%20autonomy%20stacks%2C%20providing%0Anecessary%20environmental%20information%20to%20operate%20in%20the%20real%20world.%20Many%20of%20these%0Aalgorithms%20depend%20on%20the%20visibility%20of%20keypoints%2C%20which%20must%20remain%20within%20the%0Arobot%27s%20line-of-sight%20%28LoS%29%2C%20for%20reliable%20operation.%20This%20paper%20tackles%20the%0Achallenge%20of%20maintaining%20LoS%20on%20such%20keypoints%20during%20robot%20movement.%20We%0Apropose%20a%20novel%20method%20that%20addresses%20these%20issues%20by%20ensuring%20applicability%20to%0Avarious%20sensor%20footprints%2C%20adaptability%20to%20arbitrary%20nonlinear%20system%20dynamics%2C%0Aand%20constant%20enforcement%20of%20LoS%20throughout%20the%20robot%27s%20path.%20Our%20experiments%0Ashow%20that%20the%20proposed%20approach%20achieves%20significantly%20reduced%20LoS%20violation%0Aand%20runtime%20compared%20to%20existing%20state-of-the-art%20methods%20in%20several%0Arepresentative%20and%20challenging%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22596v2&entry.124074799=Read"},
{"title": "MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for\n  Legged Robot Contact Perception", "author": "Daniel Butterfield and Sandilya Sai Garimella and Nai-Jen Cheng and Lu Gan", "abstract": "  We present a Morphology-Informed Heterogeneous Graph Neural Network (MI-HGNN)\nfor learning-based contact perception. The architecture and connectivity of the\nMI-HGNN are constructed from the robot morphology, in which nodes and edges are\nrobot joints and links, respectively. By incorporating the morphology-informed\nconstraints into a neural network, we improve a learning-based approach using\nmodel-based knowledge. We apply the proposed MI-HGNN to two contact perception\nproblems, and conduct extensive experiments using both real-world and simulated\ndata collected using two quadruped robots. Our experiments demonstrate the\nsuperiority of our method in terms of effectiveness, generalization ability,\nmodel efficiency, and sample efficiency. Our MI-HGNN improved the performance\nof a state-of-the-art model that leverages robot morphological symmetry by 8.4%\nwith only 0.21% of its parameters. Although MI-HGNN is applied to contact\nperception problems for legged robots in this work, it can be seamlessly\napplied to other types of multi-body dynamical systems and has the potential to\nimprove other robot learning frameworks. Our code is made publicly available at\nhttps://github.com/lunarlab-gatech/Morphology-Informed-HGNN.\n", "link": "http://arxiv.org/abs/2409.11146v2", "date": "2025-02-20", "relevancy": 2.2157, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6022}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5673}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MI-HGNN%3A%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20for%0A%20%20Legged%20Robot%20Contact%20Perception&body=Title%3A%20MI-HGNN%3A%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20for%0A%20%20Legged%20Robot%20Contact%20Perception%0AAuthor%3A%20Daniel%20Butterfield%20and%20Sandilya%20Sai%20Garimella%20and%20Nai-Jen%20Cheng%20and%20Lu%20Gan%0AAbstract%3A%20%20%20We%20present%20a%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20%28MI-HGNN%29%0Afor%20learning-based%20contact%20perception.%20The%20architecture%20and%20connectivity%20of%20the%0AMI-HGNN%20are%20constructed%20from%20the%20robot%20morphology%2C%20in%20which%20nodes%20and%20edges%20are%0Arobot%20joints%20and%20links%2C%20respectively.%20By%20incorporating%20the%20morphology-informed%0Aconstraints%20into%20a%20neural%20network%2C%20we%20improve%20a%20learning-based%20approach%20using%0Amodel-based%20knowledge.%20We%20apply%20the%20proposed%20MI-HGNN%20to%20two%20contact%20perception%0Aproblems%2C%20and%20conduct%20extensive%20experiments%20using%20both%20real-world%20and%20simulated%0Adata%20collected%20using%20two%20quadruped%20robots.%20Our%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20terms%20of%20effectiveness%2C%20generalization%20ability%2C%0Amodel%20efficiency%2C%20and%20sample%20efficiency.%20Our%20MI-HGNN%20improved%20the%20performance%0Aof%20a%20state-of-the-art%20model%20that%20leverages%20robot%20morphological%20symmetry%20by%208.4%25%0Awith%20only%200.21%25%20of%20its%20parameters.%20Although%20MI-HGNN%20is%20applied%20to%20contact%0Aperception%20problems%20for%20legged%20robots%20in%20this%20work%2C%20it%20can%20be%20seamlessly%0Aapplied%20to%20other%20types%20of%20multi-body%20dynamical%20systems%20and%20has%20the%20potential%20to%0Aimprove%20other%20robot%20learning%20frameworks.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lunarlab-gatech/Morphology-Informed-HGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11146v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMI-HGNN%253A%2520Morphology-Informed%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520for%250A%2520%2520Legged%2520Robot%2520Contact%2520Perception%26entry.906535625%3DDaniel%2520Butterfield%2520and%2520Sandilya%2520Sai%2520Garimella%2520and%2520Nai-Jen%2520Cheng%2520and%2520Lu%2520Gan%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520Morphology-Informed%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520%2528MI-HGNN%2529%250Afor%2520learning-based%2520contact%2520perception.%2520The%2520architecture%2520and%2520connectivity%2520of%2520the%250AMI-HGNN%2520are%2520constructed%2520from%2520the%2520robot%2520morphology%252C%2520in%2520which%2520nodes%2520and%2520edges%2520are%250Arobot%2520joints%2520and%2520links%252C%2520respectively.%2520By%2520incorporating%2520the%2520morphology-informed%250Aconstraints%2520into%2520a%2520neural%2520network%252C%2520we%2520improve%2520a%2520learning-based%2520approach%2520using%250Amodel-based%2520knowledge.%2520We%2520apply%2520the%2520proposed%2520MI-HGNN%2520to%2520two%2520contact%2520perception%250Aproblems%252C%2520and%2520conduct%2520extensive%2520experiments%2520using%2520both%2520real-world%2520and%2520simulated%250Adata%2520collected%2520using%2520two%2520quadruped%2520robots.%2520Our%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520in%2520terms%2520of%2520effectiveness%252C%2520generalization%2520ability%252C%250Amodel%2520efficiency%252C%2520and%2520sample%2520efficiency.%2520Our%2520MI-HGNN%2520improved%2520the%2520performance%250Aof%2520a%2520state-of-the-art%2520model%2520that%2520leverages%2520robot%2520morphological%2520symmetry%2520by%25208.4%2525%250Awith%2520only%25200.21%2525%2520of%2520its%2520parameters.%2520Although%2520MI-HGNN%2520is%2520applied%2520to%2520contact%250Aperception%2520problems%2520for%2520legged%2520robots%2520in%2520this%2520work%252C%2520it%2520can%2520be%2520seamlessly%250Aapplied%2520to%2520other%2520types%2520of%2520multi-body%2520dynamical%2520systems%2520and%2520has%2520the%2520potential%2520to%250Aimprove%2520other%2520robot%2520learning%2520frameworks.%2520Our%2520code%2520is%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/lunarlab-gatech/Morphology-Informed-HGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11146v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MI-HGNN%3A%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20for%0A%20%20Legged%20Robot%20Contact%20Perception&entry.906535625=Daniel%20Butterfield%20and%20Sandilya%20Sai%20Garimella%20and%20Nai-Jen%20Cheng%20and%20Lu%20Gan&entry.1292438233=%20%20We%20present%20a%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20%28MI-HGNN%29%0Afor%20learning-based%20contact%20perception.%20The%20architecture%20and%20connectivity%20of%20the%0AMI-HGNN%20are%20constructed%20from%20the%20robot%20morphology%2C%20in%20which%20nodes%20and%20edges%20are%0Arobot%20joints%20and%20links%2C%20respectively.%20By%20incorporating%20the%20morphology-informed%0Aconstraints%20into%20a%20neural%20network%2C%20we%20improve%20a%20learning-based%20approach%20using%0Amodel-based%20knowledge.%20We%20apply%20the%20proposed%20MI-HGNN%20to%20two%20contact%20perception%0Aproblems%2C%20and%20conduct%20extensive%20experiments%20using%20both%20real-world%20and%20simulated%0Adata%20collected%20using%20two%20quadruped%20robots.%20Our%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20terms%20of%20effectiveness%2C%20generalization%20ability%2C%0Amodel%20efficiency%2C%20and%20sample%20efficiency.%20Our%20MI-HGNN%20improved%20the%20performance%0Aof%20a%20state-of-the-art%20model%20that%20leverages%20robot%20morphological%20symmetry%20by%208.4%25%0Awith%20only%200.21%25%20of%20its%20parameters.%20Although%20MI-HGNN%20is%20applied%20to%20contact%0Aperception%20problems%20for%20legged%20robots%20in%20this%20work%2C%20it%20can%20be%20seamlessly%0Aapplied%20to%20other%20types%20of%20multi-body%20dynamical%20systems%20and%20has%20the%20potential%20to%0Aimprove%20other%20robot%20learning%20frameworks.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lunarlab-gatech/Morphology-Informed-HGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11146v2&entry.124074799=Read"},
{"title": "CER: Confidence Enhanced Reasoning in LLMs", "author": "Ali Razghandi and Seyed Mohammad Hadi Hosseini and Mahdieh Soleymani Baghshah", "abstract": "  Ensuring the reliability of Large Language Models (LLMs) in complex reasoning\ntasks remains a formidable challenge, particularly in scenarios that demand\nprecise mathematical calculations and knowledge-intensive open-domain\ngeneration. In this work, we introduce an uncertainty-aware framework designed\nto enhance the accuracy of LLM responses by systematically incorporating model\nconfidence at critical decision points. We propose an approach that encourages\nmulti-step reasoning in LLMs and quantify the confidence of intermediate\nanswers such as numerical results in mathematical reasoning and proper nouns in\nopen-domain generation. Then, the overall confidence of each reasoning chain is\nevaluated based on confidence of these critical intermediate steps. Finally, we\naggregate the answer of generated response paths in a way that reflects the\nreliability of each generated content (as opposed to self-consistency in which\neach generated chain contributes equally to majority voting). We conducted\nextensive experiments in five datasets, three mathematical datasets and two\nopen-domain datasets, using four LLMs. The results consistently validate the\neffectiveness of our novel confidence aggregation method, leading to an\naccuracy improvement of up to 7.4% and 5.8% over baseline approaches in math\nand open-domain generation tasks, respectively. Code is publicly available at\nhttps://github.com/ Aquasar11/CER.\n", "link": "http://arxiv.org/abs/2502.14634v1", "date": "2025-02-20", "relevancy": 2.198, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5995}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5493}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CER%3A%20Confidence%20Enhanced%20Reasoning%20in%20LLMs&body=Title%3A%20CER%3A%20Confidence%20Enhanced%20Reasoning%20in%20LLMs%0AAuthor%3A%20Ali%20Razghandi%20and%20Seyed%20Mohammad%20Hadi%20Hosseini%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Ensuring%20the%20reliability%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20complex%20reasoning%0Atasks%20remains%20a%20formidable%20challenge%2C%20particularly%20in%20scenarios%20that%20demand%0Aprecise%20mathematical%20calculations%20and%20knowledge-intensive%20open-domain%0Ageneration.%20In%20this%20work%2C%20we%20introduce%20an%20uncertainty-aware%20framework%20designed%0Ato%20enhance%20the%20accuracy%20of%20LLM%20responses%20by%20systematically%20incorporating%20model%0Aconfidence%20at%20critical%20decision%20points.%20We%20propose%20an%20approach%20that%20encourages%0Amulti-step%20reasoning%20in%20LLMs%20and%20quantify%20the%20confidence%20of%20intermediate%0Aanswers%20such%20as%20numerical%20results%20in%20mathematical%20reasoning%20and%20proper%20nouns%20in%0Aopen-domain%20generation.%20Then%2C%20the%20overall%20confidence%20of%20each%20reasoning%20chain%20is%0Aevaluated%20based%20on%20confidence%20of%20these%20critical%20intermediate%20steps.%20Finally%2C%20we%0Aaggregate%20the%20answer%20of%20generated%20response%20paths%20in%20a%20way%20that%20reflects%20the%0Areliability%20of%20each%20generated%20content%20%28as%20opposed%20to%20self-consistency%20in%20which%0Aeach%20generated%20chain%20contributes%20equally%20to%20majority%20voting%29.%20We%20conducted%0Aextensive%20experiments%20in%20five%20datasets%2C%20three%20mathematical%20datasets%20and%20two%0Aopen-domain%20datasets%2C%20using%20four%20LLMs.%20The%20results%20consistently%20validate%20the%0Aeffectiveness%20of%20our%20novel%20confidence%20aggregation%20method%2C%20leading%20to%20an%0Aaccuracy%20improvement%20of%20up%20to%207.4%25%20and%205.8%25%20over%20baseline%20approaches%20in%20math%0Aand%20open-domain%20generation%20tasks%2C%20respectively.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/%20Aquasar11/CER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCER%253A%2520Confidence%2520Enhanced%2520Reasoning%2520in%2520LLMs%26entry.906535625%3DAli%2520Razghandi%2520and%2520Seyed%2520Mohammad%2520Hadi%2520Hosseini%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520reliability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520complex%2520reasoning%250Atasks%2520remains%2520a%2520formidable%2520challenge%252C%2520particularly%2520in%2520scenarios%2520that%2520demand%250Aprecise%2520mathematical%2520calculations%2520and%2520knowledge-intensive%2520open-domain%250Ageneration.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520uncertainty-aware%2520framework%2520designed%250Ato%2520enhance%2520the%2520accuracy%2520of%2520LLM%2520responses%2520by%2520systematically%2520incorporating%2520model%250Aconfidence%2520at%2520critical%2520decision%2520points.%2520We%2520propose%2520an%2520approach%2520that%2520encourages%250Amulti-step%2520reasoning%2520in%2520LLMs%2520and%2520quantify%2520the%2520confidence%2520of%2520intermediate%250Aanswers%2520such%2520as%2520numerical%2520results%2520in%2520mathematical%2520reasoning%2520and%2520proper%2520nouns%2520in%250Aopen-domain%2520generation.%2520Then%252C%2520the%2520overall%2520confidence%2520of%2520each%2520reasoning%2520chain%2520is%250Aevaluated%2520based%2520on%2520confidence%2520of%2520these%2520critical%2520intermediate%2520steps.%2520Finally%252C%2520we%250Aaggregate%2520the%2520answer%2520of%2520generated%2520response%2520paths%2520in%2520a%2520way%2520that%2520reflects%2520the%250Areliability%2520of%2520each%2520generated%2520content%2520%2528as%2520opposed%2520to%2520self-consistency%2520in%2520which%250Aeach%2520generated%2520chain%2520contributes%2520equally%2520to%2520majority%2520voting%2529.%2520We%2520conducted%250Aextensive%2520experiments%2520in%2520five%2520datasets%252C%2520three%2520mathematical%2520datasets%2520and%2520two%250Aopen-domain%2520datasets%252C%2520using%2520four%2520LLMs.%2520The%2520results%2520consistently%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520novel%2520confidence%2520aggregation%2520method%252C%2520leading%2520to%2520an%250Aaccuracy%2520improvement%2520of%2520up%2520to%25207.4%2525%2520and%25205.8%2525%2520over%2520baseline%2520approaches%2520in%2520math%250Aand%2520open-domain%2520generation%2520tasks%252C%2520respectively.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/%2520Aquasar11/CER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CER%3A%20Confidence%20Enhanced%20Reasoning%20in%20LLMs&entry.906535625=Ali%20Razghandi%20and%20Seyed%20Mohammad%20Hadi%20Hosseini%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Ensuring%20the%20reliability%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20complex%20reasoning%0Atasks%20remains%20a%20formidable%20challenge%2C%20particularly%20in%20scenarios%20that%20demand%0Aprecise%20mathematical%20calculations%20and%20knowledge-intensive%20open-domain%0Ageneration.%20In%20this%20work%2C%20we%20introduce%20an%20uncertainty-aware%20framework%20designed%0Ato%20enhance%20the%20accuracy%20of%20LLM%20responses%20by%20systematically%20incorporating%20model%0Aconfidence%20at%20critical%20decision%20points.%20We%20propose%20an%20approach%20that%20encourages%0Amulti-step%20reasoning%20in%20LLMs%20and%20quantify%20the%20confidence%20of%20intermediate%0Aanswers%20such%20as%20numerical%20results%20in%20mathematical%20reasoning%20and%20proper%20nouns%20in%0Aopen-domain%20generation.%20Then%2C%20the%20overall%20confidence%20of%20each%20reasoning%20chain%20is%0Aevaluated%20based%20on%20confidence%20of%20these%20critical%20intermediate%20steps.%20Finally%2C%20we%0Aaggregate%20the%20answer%20of%20generated%20response%20paths%20in%20a%20way%20that%20reflects%20the%0Areliability%20of%20each%20generated%20content%20%28as%20opposed%20to%20self-consistency%20in%20which%0Aeach%20generated%20chain%20contributes%20equally%20to%20majority%20voting%29.%20We%20conducted%0Aextensive%20experiments%20in%20five%20datasets%2C%20three%20mathematical%20datasets%20and%20two%0Aopen-domain%20datasets%2C%20using%20four%20LLMs.%20The%20results%20consistently%20validate%20the%0Aeffectiveness%20of%20our%20novel%20confidence%20aggregation%20method%2C%20leading%20to%20an%0Aaccuracy%20improvement%20of%20up%20to%207.4%25%20and%205.8%25%20over%20baseline%20approaches%20in%20math%0Aand%20open-domain%20generation%20tasks%2C%20respectively.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/%20Aquasar11/CER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14634v1&entry.124074799=Read"},
{"title": "Self-supervised Monocular Depth Estimation Robust to Reflective Surface\n  Leveraged by Triplet Mining", "author": "Wonhyeok Choi and Kyumin Hwang and Wei Peng and Minwoo Choi and Sunghoon Im", "abstract": "  Self-supervised monocular depth estimation (SSMDE) aims to predict the dense\ndepth map of a monocular image, by learning depth from RGB image sequences,\neliminating the need for ground-truth depth labels. Although this approach\nsimplifies data acquisition compared to supervised methods, it struggles with\nreflective surfaces, as they violate the assumptions of Lambertian reflectance,\nleading to inaccurate training on such surfaces. To tackle this problem, we\npropose a novel training strategy for an SSMDE by leveraging triplet mining to\npinpoint reflective regions at the pixel level, guided by the camera geometry\nbetween different viewpoints. The proposed reflection-aware triplet mining loss\nspecifically penalizes the inappropriate photometric error minimization on the\nlocalized reflective regions while preserving depth accuracy in non-reflective\nareas. We also incorporate a reflection-aware knowledge distillation method\nthat enables a student model to selectively learn the pixel-level knowledge\nfrom reflective and non-reflective regions. This results in robust depth\nestimation across areas. Evaluation results on multiple datasets demonstrate\nthat our method effectively enhances depth quality on reflective surfaces and\noutperforms state-of-the-art SSMDE baselines.\n", "link": "http://arxiv.org/abs/2502.14573v1", "date": "2025-02-20", "relevancy": 2.1822, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5657}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Monocular%20Depth%20Estimation%20Robust%20to%20Reflective%20Surface%0A%20%20Leveraged%20by%20Triplet%20Mining&body=Title%3A%20Self-supervised%20Monocular%20Depth%20Estimation%20Robust%20to%20Reflective%20Surface%0A%20%20Leveraged%20by%20Triplet%20Mining%0AAuthor%3A%20Wonhyeok%20Choi%20and%20Kyumin%20Hwang%20and%20Wei%20Peng%20and%20Minwoo%20Choi%20and%20Sunghoon%20Im%0AAbstract%3A%20%20%20Self-supervised%20monocular%20depth%20estimation%20%28SSMDE%29%20aims%20to%20predict%20the%20dense%0Adepth%20map%20of%20a%20monocular%20image%2C%20by%20learning%20depth%20from%20RGB%20image%20sequences%2C%0Aeliminating%20the%20need%20for%20ground-truth%20depth%20labels.%20Although%20this%20approach%0Asimplifies%20data%20acquisition%20compared%20to%20supervised%20methods%2C%20it%20struggles%20with%0Areflective%20surfaces%2C%20as%20they%20violate%20the%20assumptions%20of%20Lambertian%20reflectance%2C%0Aleading%20to%20inaccurate%20training%20on%20such%20surfaces.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20a%20novel%20training%20strategy%20for%20an%20SSMDE%20by%20leveraging%20triplet%20mining%20to%0Apinpoint%20reflective%20regions%20at%20the%20pixel%20level%2C%20guided%20by%20the%20camera%20geometry%0Abetween%20different%20viewpoints.%20The%20proposed%20reflection-aware%20triplet%20mining%20loss%0Aspecifically%20penalizes%20the%20inappropriate%20photometric%20error%20minimization%20on%20the%0Alocalized%20reflective%20regions%20while%20preserving%20depth%20accuracy%20in%20non-reflective%0Aareas.%20We%20also%20incorporate%20a%20reflection-aware%20knowledge%20distillation%20method%0Athat%20enables%20a%20student%20model%20to%20selectively%20learn%20the%20pixel-level%20knowledge%0Afrom%20reflective%20and%20non-reflective%20regions.%20This%20results%20in%20robust%20depth%0Aestimation%20across%20areas.%20Evaluation%20results%20on%20multiple%20datasets%20demonstrate%0Athat%20our%20method%20effectively%20enhances%20depth%20quality%20on%20reflective%20surfaces%20and%0Aoutperforms%20state-of-the-art%20SSMDE%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Monocular%2520Depth%2520Estimation%2520Robust%2520to%2520Reflective%2520Surface%250A%2520%2520Leveraged%2520by%2520Triplet%2520Mining%26entry.906535625%3DWonhyeok%2520Choi%2520and%2520Kyumin%2520Hwang%2520and%2520Wei%2520Peng%2520and%2520Minwoo%2520Choi%2520and%2520Sunghoon%2520Im%26entry.1292438233%3D%2520%2520Self-supervised%2520monocular%2520depth%2520estimation%2520%2528SSMDE%2529%2520aims%2520to%2520predict%2520the%2520dense%250Adepth%2520map%2520of%2520a%2520monocular%2520image%252C%2520by%2520learning%2520depth%2520from%2520RGB%2520image%2520sequences%252C%250Aeliminating%2520the%2520need%2520for%2520ground-truth%2520depth%2520labels.%2520Although%2520this%2520approach%250Asimplifies%2520data%2520acquisition%2520compared%2520to%2520supervised%2520methods%252C%2520it%2520struggles%2520with%250Areflective%2520surfaces%252C%2520as%2520they%2520violate%2520the%2520assumptions%2520of%2520Lambertian%2520reflectance%252C%250Aleading%2520to%2520inaccurate%2520training%2520on%2520such%2520surfaces.%2520To%2520tackle%2520this%2520problem%252C%2520we%250Apropose%2520a%2520novel%2520training%2520strategy%2520for%2520an%2520SSMDE%2520by%2520leveraging%2520triplet%2520mining%2520to%250Apinpoint%2520reflective%2520regions%2520at%2520the%2520pixel%2520level%252C%2520guided%2520by%2520the%2520camera%2520geometry%250Abetween%2520different%2520viewpoints.%2520The%2520proposed%2520reflection-aware%2520triplet%2520mining%2520loss%250Aspecifically%2520penalizes%2520the%2520inappropriate%2520photometric%2520error%2520minimization%2520on%2520the%250Alocalized%2520reflective%2520regions%2520while%2520preserving%2520depth%2520accuracy%2520in%2520non-reflective%250Aareas.%2520We%2520also%2520incorporate%2520a%2520reflection-aware%2520knowledge%2520distillation%2520method%250Athat%2520enables%2520a%2520student%2520model%2520to%2520selectively%2520learn%2520the%2520pixel-level%2520knowledge%250Afrom%2520reflective%2520and%2520non-reflective%2520regions.%2520This%2520results%2520in%2520robust%2520depth%250Aestimation%2520across%2520areas.%2520Evaluation%2520results%2520on%2520multiple%2520datasets%2520demonstrate%250Athat%2520our%2520method%2520effectively%2520enhances%2520depth%2520quality%2520on%2520reflective%2520surfaces%2520and%250Aoutperforms%2520state-of-the-art%2520SSMDE%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Monocular%20Depth%20Estimation%20Robust%20to%20Reflective%20Surface%0A%20%20Leveraged%20by%20Triplet%20Mining&entry.906535625=Wonhyeok%20Choi%20and%20Kyumin%20Hwang%20and%20Wei%20Peng%20and%20Minwoo%20Choi%20and%20Sunghoon%20Im&entry.1292438233=%20%20Self-supervised%20monocular%20depth%20estimation%20%28SSMDE%29%20aims%20to%20predict%20the%20dense%0Adepth%20map%20of%20a%20monocular%20image%2C%20by%20learning%20depth%20from%20RGB%20image%20sequences%2C%0Aeliminating%20the%20need%20for%20ground-truth%20depth%20labels.%20Although%20this%20approach%0Asimplifies%20data%20acquisition%20compared%20to%20supervised%20methods%2C%20it%20struggles%20with%0Areflective%20surfaces%2C%20as%20they%20violate%20the%20assumptions%20of%20Lambertian%20reflectance%2C%0Aleading%20to%20inaccurate%20training%20on%20such%20surfaces.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20a%20novel%20training%20strategy%20for%20an%20SSMDE%20by%20leveraging%20triplet%20mining%20to%0Apinpoint%20reflective%20regions%20at%20the%20pixel%20level%2C%20guided%20by%20the%20camera%20geometry%0Abetween%20different%20viewpoints.%20The%20proposed%20reflection-aware%20triplet%20mining%20loss%0Aspecifically%20penalizes%20the%20inappropriate%20photometric%20error%20minimization%20on%20the%0Alocalized%20reflective%20regions%20while%20preserving%20depth%20accuracy%20in%20non-reflective%0Aareas.%20We%20also%20incorporate%20a%20reflection-aware%20knowledge%20distillation%20method%0Athat%20enables%20a%20student%20model%20to%20selectively%20learn%20the%20pixel-level%20knowledge%0Afrom%20reflective%20and%20non-reflective%20regions.%20This%20results%20in%20robust%20depth%0Aestimation%20across%20areas.%20Evaluation%20results%20on%20multiple%20datasets%20demonstrate%0Athat%20our%20method%20effectively%20enhances%20depth%20quality%20on%20reflective%20surfaces%20and%0Aoutperforms%20state-of-the-art%20SSMDE%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14573v1&entry.124074799=Read"},
{"title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in\n  Vision-Language Models", "author": "Shangqing Tu and Yucheng Wang and Daniel Zhang-Li and Yushi Bai and Jifan Yu and Yuhao Wu and Lei Hou and Huiqin Liu and Zhiyuan Liu and Bin Xu and Juanzi Li", "abstract": "  Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V\n", "link": "http://arxiv.org/abs/2502.14834v1", "date": "2025-02-20", "relevancy": 2.1582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongWriter-V%3A%20Enabling%20Ultra-Long%20and%20High-Fidelity%20Generation%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20LongWriter-V%3A%20Enabling%20Ultra-Long%20and%20High-Fidelity%20Generation%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Shangqing%20Tu%20and%20Yucheng%20Wang%20and%20Daniel%20Zhang-Li%20and%20Yushi%20Bai%20and%20Jifan%20Yu%20and%20Yuhao%20Wu%20and%20Lei%20Hou%20and%20Huiqin%20Liu%20and%20Zhiyuan%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Existing%20Large%20Vision-Language%20Models%20%28LVLMs%29%20can%20process%20inputs%20with%20context%0Alengths%20up%20to%20128k%20visual%20and%20text%20tokens%2C%20yet%20they%20struggle%20to%20generate%0Acoherent%20outputs%20beyond%201%2C000%20words.%20We%20find%20that%20the%20primary%20limitation%20is%20the%0Aabsence%20of%20long%20output%20examples%20during%20supervised%20fine-tuning%20%28SFT%29.%20To%20tackle%0Athis%20issue%2C%20we%20introduce%20LongWriter-V-22k%2C%20a%20SFT%20dataset%20comprising%2022%2C158%0Aexamples%2C%20each%20with%20multiple%20input%20images%2C%20an%20instruction%2C%20and%20corresponding%0Aoutputs%20ranging%20from%200%20to%2010%2C000%20words.%20Moreover%2C%20to%20achieve%20long%20outputs%20that%0Amaintain%20high-fidelity%20to%20the%20input%20images%2C%20we%20employ%20Direct%20Preference%0AOptimization%20%28DPO%29%20to%20the%20SFT%20model.%20Given%20the%20high%20cost%20of%20collecting%20human%0Afeedback%20for%20lengthy%20outputs%20%28e.g.%2C%203%2C000%20words%29%2C%20we%20propose%20IterDPO%2C%20which%0Abreaks%20long%20outputs%20into%20segments%20and%20uses%20iterative%20corrections%20to%20form%0Apreference%20pairs%20with%20the%20original%20outputs.%20Additionally%2C%20we%20develop%0AMMLongBench-Write%2C%20a%20benchmark%20featuring%20six%20tasks%20to%20evaluate%20the%0Along-generation%20capabilities%20of%20VLMs.%20Our%207B%20parameter%20model%2C%20trained%20with%0ALongWriter-V-22k%20and%20IterDPO%2C%20achieves%20impressive%20performance%20on%20this%0Abenchmark%2C%20outperforming%20larger%20proprietary%20models%20like%20GPT-4o.%20Code%20and%20data%3A%0Ahttps%3A//github.com/THU-KEG/LongWriter-V%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongWriter-V%253A%2520Enabling%2520Ultra-Long%2520and%2520High-Fidelity%2520Generation%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DShangqing%2520Tu%2520and%2520Yucheng%2520Wang%2520and%2520Daniel%2520Zhang-Li%2520and%2520Yushi%2520Bai%2520and%2520Jifan%2520Yu%2520and%2520Yuhao%2520Wu%2520and%2520Lei%2520Hou%2520and%2520Huiqin%2520Liu%2520and%2520Zhiyuan%2520Liu%2520and%2520Bin%2520Xu%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Existing%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520can%2520process%2520inputs%2520with%2520context%250Alengths%2520up%2520to%2520128k%2520visual%2520and%2520text%2520tokens%252C%2520yet%2520they%2520struggle%2520to%2520generate%250Acoherent%2520outputs%2520beyond%25201%252C000%2520words.%2520We%2520find%2520that%2520the%2520primary%2520limitation%2520is%2520the%250Aabsence%2520of%2520long%2520output%2520examples%2520during%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520To%2520tackle%250Athis%2520issue%252C%2520we%2520introduce%2520LongWriter-V-22k%252C%2520a%2520SFT%2520dataset%2520comprising%252022%252C158%250Aexamples%252C%2520each%2520with%2520multiple%2520input%2520images%252C%2520an%2520instruction%252C%2520and%2520corresponding%250Aoutputs%2520ranging%2520from%25200%2520to%252010%252C000%2520words.%2520Moreover%252C%2520to%2520achieve%2520long%2520outputs%2520that%250Amaintain%2520high-fidelity%2520to%2520the%2520input%2520images%252C%2520we%2520employ%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%2520to%2520the%2520SFT%2520model.%2520Given%2520the%2520high%2520cost%2520of%2520collecting%2520human%250Afeedback%2520for%2520lengthy%2520outputs%2520%2528e.g.%252C%25203%252C000%2520words%2529%252C%2520we%2520propose%2520IterDPO%252C%2520which%250Abreaks%2520long%2520outputs%2520into%2520segments%2520and%2520uses%2520iterative%2520corrections%2520to%2520form%250Apreference%2520pairs%2520with%2520the%2520original%2520outputs.%2520Additionally%252C%2520we%2520develop%250AMMLongBench-Write%252C%2520a%2520benchmark%2520featuring%2520six%2520tasks%2520to%2520evaluate%2520the%250Along-generation%2520capabilities%2520of%2520VLMs.%2520Our%25207B%2520parameter%2520model%252C%2520trained%2520with%250ALongWriter-V-22k%2520and%2520IterDPO%252C%2520achieves%2520impressive%2520performance%2520on%2520this%250Abenchmark%252C%2520outperforming%2520larger%2520proprietary%2520models%2520like%2520GPT-4o.%2520Code%2520and%2520data%253A%250Ahttps%253A//github.com/THU-KEG/LongWriter-V%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongWriter-V%3A%20Enabling%20Ultra-Long%20and%20High-Fidelity%20Generation%20in%0A%20%20Vision-Language%20Models&entry.906535625=Shangqing%20Tu%20and%20Yucheng%20Wang%20and%20Daniel%20Zhang-Li%20and%20Yushi%20Bai%20and%20Jifan%20Yu%20and%20Yuhao%20Wu%20and%20Lei%20Hou%20and%20Huiqin%20Liu%20and%20Zhiyuan%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li&entry.1292438233=%20%20Existing%20Large%20Vision-Language%20Models%20%28LVLMs%29%20can%20process%20inputs%20with%20context%0Alengths%20up%20to%20128k%20visual%20and%20text%20tokens%2C%20yet%20they%20struggle%20to%20generate%0Acoherent%20outputs%20beyond%201%2C000%20words.%20We%20find%20that%20the%20primary%20limitation%20is%20the%0Aabsence%20of%20long%20output%20examples%20during%20supervised%20fine-tuning%20%28SFT%29.%20To%20tackle%0Athis%20issue%2C%20we%20introduce%20LongWriter-V-22k%2C%20a%20SFT%20dataset%20comprising%2022%2C158%0Aexamples%2C%20each%20with%20multiple%20input%20images%2C%20an%20instruction%2C%20and%20corresponding%0Aoutputs%20ranging%20from%200%20to%2010%2C000%20words.%20Moreover%2C%20to%20achieve%20long%20outputs%20that%0Amaintain%20high-fidelity%20to%20the%20input%20images%2C%20we%20employ%20Direct%20Preference%0AOptimization%20%28DPO%29%20to%20the%20SFT%20model.%20Given%20the%20high%20cost%20of%20collecting%20human%0Afeedback%20for%20lengthy%20outputs%20%28e.g.%2C%203%2C000%20words%29%2C%20we%20propose%20IterDPO%2C%20which%0Abreaks%20long%20outputs%20into%20segments%20and%20uses%20iterative%20corrections%20to%20form%0Apreference%20pairs%20with%20the%20original%20outputs.%20Additionally%2C%20we%20develop%0AMMLongBench-Write%2C%20a%20benchmark%20featuring%20six%20tasks%20to%20evaluate%20the%0Along-generation%20capabilities%20of%20VLMs.%20Our%207B%20parameter%20model%2C%20trained%20with%0ALongWriter-V-22k%20and%20IterDPO%2C%20achieves%20impressive%20performance%20on%20this%0Abenchmark%2C%20outperforming%20larger%20proprietary%20models%20like%20GPT-4o.%20Code%20and%20data%3A%0Ahttps%3A//github.com/THU-KEG/LongWriter-V%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14834v1&entry.124074799=Read"},
{"title": "YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time\n  Object Detection", "author": "Yuming Chen and Xinbin Yuan and Jiabao Wang and Ruiqi Wu and Xiang Li and Qibin Hou and Ming-Ming Cheng", "abstract": "  We aim at providing the object detection community with an efficient and\nperformant object detector, termed YOLO-MS. The core design is based on a\nseries of investigations on how multi-branch features of the basic block and\nconvolutions with different kernel sizes affect the detection performance of\nobjects at different scales. The outcome is a new strategy that can\nsignificantly enhance multi-scale feature representations of real-time object\ndetectors. To verify the effectiveness of our work, we train our YOLO-MS on the\nMS COCO dataset from scratch without relying on any other large-scale datasets,\nlike ImageNet or pre-trained weights. Without bells and whistles, our YOLO-MS\noutperforms the recent state-of-the-art real-time object detectors, including\nYOLO-v7, RTMDet, and YOLO-v8. Taking the XS version of YOLO-MS as an example,\nit can achieve an AP score of 42+% on MS COCO, which is about 2% higher than\nRTMDet with the same model size. Furthermore, our work can also serve as a\nplug-and-play module for other YOLO models. Typically, our method significantly\nadvances the APs, APl, and AP of YOLOv8-N from 18%+, 52%+, and 37%+ to 20%+,\n55%+, and 40%+, respectively, with even fewer parameters and MACs. Code and\ntrained models are publicly available at\nhttps://github.com/FishAndWasabi/YOLO-MS. We also provide the Jittor version at\nhttps://github.com/NK-JittorCV/nk-yolo.\n", "link": "http://arxiv.org/abs/2308.05480v2", "date": "2025-02-20", "relevancy": 2.1473, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.539}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLO-MS%3A%20Rethinking%20Multi-Scale%20Representation%20Learning%20for%20Real-time%0A%20%20Object%20Detection&body=Title%3A%20YOLO-MS%3A%20Rethinking%20Multi-Scale%20Representation%20Learning%20for%20Real-time%0A%20%20Object%20Detection%0AAuthor%3A%20Yuming%20Chen%20and%20Xinbin%20Yuan%20and%20Jiabao%20Wang%20and%20Ruiqi%20Wu%20and%20Xiang%20Li%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20We%20aim%20at%20providing%20the%20object%20detection%20community%20with%20an%20efficient%20and%0Aperformant%20object%20detector%2C%20termed%20YOLO-MS.%20The%20core%20design%20is%20based%20on%20a%0Aseries%20of%20investigations%20on%20how%20multi-branch%20features%20of%20the%20basic%20block%20and%0Aconvolutions%20with%20different%20kernel%20sizes%20affect%20the%20detection%20performance%20of%0Aobjects%20at%20different%20scales.%20The%20outcome%20is%20a%20new%20strategy%20that%20can%0Asignificantly%20enhance%20multi-scale%20feature%20representations%20of%20real-time%20object%0Adetectors.%20To%20verify%20the%20effectiveness%20of%20our%20work%2C%20we%20train%20our%20YOLO-MS%20on%20the%0AMS%20COCO%20dataset%20from%20scratch%20without%20relying%20on%20any%20other%20large-scale%20datasets%2C%0Alike%20ImageNet%20or%20pre-trained%20weights.%20Without%20bells%20and%20whistles%2C%20our%20YOLO-MS%0Aoutperforms%20the%20recent%20state-of-the-art%20real-time%20object%20detectors%2C%20including%0AYOLO-v7%2C%20RTMDet%2C%20and%20YOLO-v8.%20Taking%20the%20XS%20version%20of%20YOLO-MS%20as%20an%20example%2C%0Ait%20can%20achieve%20an%20AP%20score%20of%2042%2B%25%20on%20MS%20COCO%2C%20which%20is%20about%202%25%20higher%20than%0ARTMDet%20with%20the%20same%20model%20size.%20Furthermore%2C%20our%20work%20can%20also%20serve%20as%20a%0Aplug-and-play%20module%20for%20other%20YOLO%20models.%20Typically%2C%20our%20method%20significantly%0Aadvances%20the%20APs%2C%20APl%2C%20and%20AP%20of%20YOLOv8-N%20from%2018%25%2B%2C%2052%25%2B%2C%20and%2037%25%2B%20to%2020%25%2B%2C%0A55%25%2B%2C%20and%2040%25%2B%2C%20respectively%2C%20with%20even%20fewer%20parameters%20and%20MACs.%20Code%20and%0Atrained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/FishAndWasabi/YOLO-MS.%20We%20also%20provide%20the%20Jittor%20version%20at%0Ahttps%3A//github.com/NK-JittorCV/nk-yolo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLO-MS%253A%2520Rethinking%2520Multi-Scale%2520Representation%2520Learning%2520for%2520Real-time%250A%2520%2520Object%2520Detection%26entry.906535625%3DYuming%2520Chen%2520and%2520Xinbin%2520Yuan%2520and%2520Jiabao%2520Wang%2520and%2520Ruiqi%2520Wu%2520and%2520Xiang%2520Li%2520and%2520Qibin%2520Hou%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520We%2520aim%2520at%2520providing%2520the%2520object%2520detection%2520community%2520with%2520an%2520efficient%2520and%250Aperformant%2520object%2520detector%252C%2520termed%2520YOLO-MS.%2520The%2520core%2520design%2520is%2520based%2520on%2520a%250Aseries%2520of%2520investigations%2520on%2520how%2520multi-branch%2520features%2520of%2520the%2520basic%2520block%2520and%250Aconvolutions%2520with%2520different%2520kernel%2520sizes%2520affect%2520the%2520detection%2520performance%2520of%250Aobjects%2520at%2520different%2520scales.%2520The%2520outcome%2520is%2520a%2520new%2520strategy%2520that%2520can%250Asignificantly%2520enhance%2520multi-scale%2520feature%2520representations%2520of%2520real-time%2520object%250Adetectors.%2520To%2520verify%2520the%2520effectiveness%2520of%2520our%2520work%252C%2520we%2520train%2520our%2520YOLO-MS%2520on%2520the%250AMS%2520COCO%2520dataset%2520from%2520scratch%2520without%2520relying%2520on%2520any%2520other%2520large-scale%2520datasets%252C%250Alike%2520ImageNet%2520or%2520pre-trained%2520weights.%2520Without%2520bells%2520and%2520whistles%252C%2520our%2520YOLO-MS%250Aoutperforms%2520the%2520recent%2520state-of-the-art%2520real-time%2520object%2520detectors%252C%2520including%250AYOLO-v7%252C%2520RTMDet%252C%2520and%2520YOLO-v8.%2520Taking%2520the%2520XS%2520version%2520of%2520YOLO-MS%2520as%2520an%2520example%252C%250Ait%2520can%2520achieve%2520an%2520AP%2520score%2520of%252042%252B%2525%2520on%2520MS%2520COCO%252C%2520which%2520is%2520about%25202%2525%2520higher%2520than%250ARTMDet%2520with%2520the%2520same%2520model%2520size.%2520Furthermore%252C%2520our%2520work%2520can%2520also%2520serve%2520as%2520a%250Aplug-and-play%2520module%2520for%2520other%2520YOLO%2520models.%2520Typically%252C%2520our%2520method%2520significantly%250Aadvances%2520the%2520APs%252C%2520APl%252C%2520and%2520AP%2520of%2520YOLOv8-N%2520from%252018%2525%252B%252C%252052%2525%252B%252C%2520and%252037%2525%252B%2520to%252020%2525%252B%252C%250A55%2525%252B%252C%2520and%252040%2525%252B%252C%2520respectively%252C%2520with%2520even%2520fewer%2520parameters%2520and%2520MACs.%2520Code%2520and%250Atrained%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/FishAndWasabi/YOLO-MS.%2520We%2520also%2520provide%2520the%2520Jittor%2520version%2520at%250Ahttps%253A//github.com/NK-JittorCV/nk-yolo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO-MS%3A%20Rethinking%20Multi-Scale%20Representation%20Learning%20for%20Real-time%0A%20%20Object%20Detection&entry.906535625=Yuming%20Chen%20and%20Xinbin%20Yuan%20and%20Jiabao%20Wang%20and%20Ruiqi%20Wu%20and%20Xiang%20Li%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20We%20aim%20at%20providing%20the%20object%20detection%20community%20with%20an%20efficient%20and%0Aperformant%20object%20detector%2C%20termed%20YOLO-MS.%20The%20core%20design%20is%20based%20on%20a%0Aseries%20of%20investigations%20on%20how%20multi-branch%20features%20of%20the%20basic%20block%20and%0Aconvolutions%20with%20different%20kernel%20sizes%20affect%20the%20detection%20performance%20of%0Aobjects%20at%20different%20scales.%20The%20outcome%20is%20a%20new%20strategy%20that%20can%0Asignificantly%20enhance%20multi-scale%20feature%20representations%20of%20real-time%20object%0Adetectors.%20To%20verify%20the%20effectiveness%20of%20our%20work%2C%20we%20train%20our%20YOLO-MS%20on%20the%0AMS%20COCO%20dataset%20from%20scratch%20without%20relying%20on%20any%20other%20large-scale%20datasets%2C%0Alike%20ImageNet%20or%20pre-trained%20weights.%20Without%20bells%20and%20whistles%2C%20our%20YOLO-MS%0Aoutperforms%20the%20recent%20state-of-the-art%20real-time%20object%20detectors%2C%20including%0AYOLO-v7%2C%20RTMDet%2C%20and%20YOLO-v8.%20Taking%20the%20XS%20version%20of%20YOLO-MS%20as%20an%20example%2C%0Ait%20can%20achieve%20an%20AP%20score%20of%2042%2B%25%20on%20MS%20COCO%2C%20which%20is%20about%202%25%20higher%20than%0ARTMDet%20with%20the%20same%20model%20size.%20Furthermore%2C%20our%20work%20can%20also%20serve%20as%20a%0Aplug-and-play%20module%20for%20other%20YOLO%20models.%20Typically%2C%20our%20method%20significantly%0Aadvances%20the%20APs%2C%20APl%2C%20and%20AP%20of%20YOLOv8-N%20from%2018%25%2B%2C%2052%25%2B%2C%20and%2037%25%2B%20to%2020%25%2B%2C%0A55%25%2B%2C%20and%2040%25%2B%2C%20respectively%2C%20with%20even%20fewer%20parameters%20and%20MACs.%20Code%20and%0Atrained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/FishAndWasabi/YOLO-MS.%20We%20also%20provide%20the%20Jittor%20version%20at%0Ahttps%3A//github.com/NK-JittorCV/nk-yolo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05480v2&entry.124074799=Read"},
{"title": "BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution\n  Network for Pedestrian and Heterogeneous Trajectory Prediction", "author": "Ruochen Li and Stamos Katsigiannis and Tae-Kyun Kim and Hubert P. H. Shum", "abstract": "  Trajectory prediction allows better decision-making in applications of\nautonomous vehicles or surveillance by predicting the short-term future\nmovement of traffic agents. It is classified into pedestrian or heterogeneous\ntrajectory prediction. The former exploits the relatively consistent behavior\nof pedestrians, but is limited in real-world scenarios with heterogeneous\ntraffic agents such as cyclists and vehicles. The latter typically relies on\nextra class label information to distinguish the heterogeneous agents, but such\nlabels are costly to annotate and cannot be generalized to represent different\nbehaviors within the same class of agents. In this work, we introduce the\nbehavioral pseudo-labels that effectively capture the behavior distributions of\npedestrians and heterogeneous agents solely based on their motion features,\nsignificantly improving the accuracy of trajectory prediction. To implement the\nframework, we propose the Behavioral Pseudo-Label Informed Sparse Graph\nConvolution Network (BP-SGCN) that learns pseudo-labels and informs to a\ntrajectory predictor. For optimization, we propose a cascaded training scheme,\nin which we first learn the pseudo-labels in an unsupervised manner, and then\nperform end-to-end fine-tuning on the labels in the direction of increasing the\ntrajectory prediction accuracy. Experiments show that our pseudo-labels\neffectively model different behavior clusters and improve trajectory\nprediction. Our proposed BP-SGCN outperforms existing methods using both\npedestrian (ETH/UCY, pedestrian-only SDD) and heterogeneous agent datasets\n(SDD, Argoverse 1).\n", "link": "http://arxiv.org/abs/2502.14676v1", "date": "2025-02-20", "relevancy": 2.1412, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5834}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5264}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BP-SGCN%3A%20Behavioral%20Pseudo-Label%20Informed%20Sparse%20Graph%20Convolution%0A%20%20Network%20for%20Pedestrian%20and%20Heterogeneous%20Trajectory%20Prediction&body=Title%3A%20BP-SGCN%3A%20Behavioral%20Pseudo-Label%20Informed%20Sparse%20Graph%20Convolution%0A%20%20Network%20for%20Pedestrian%20and%20Heterogeneous%20Trajectory%20Prediction%0AAuthor%3A%20Ruochen%20Li%20and%20Stamos%20Katsigiannis%20and%20Tae-Kyun%20Kim%20and%20Hubert%20P.%20H.%20Shum%0AAbstract%3A%20%20%20Trajectory%20prediction%20allows%20better%20decision-making%20in%20applications%20of%0Aautonomous%20vehicles%20or%20surveillance%20by%20predicting%20the%20short-term%20future%0Amovement%20of%20traffic%20agents.%20It%20is%20classified%20into%20pedestrian%20or%20heterogeneous%0Atrajectory%20prediction.%20The%20former%20exploits%20the%20relatively%20consistent%20behavior%0Aof%20pedestrians%2C%20but%20is%20limited%20in%20real-world%20scenarios%20with%20heterogeneous%0Atraffic%20agents%20such%20as%20cyclists%20and%20vehicles.%20The%20latter%20typically%20relies%20on%0Aextra%20class%20label%20information%20to%20distinguish%20the%20heterogeneous%20agents%2C%20but%20such%0Alabels%20are%20costly%20to%20annotate%20and%20cannot%20be%20generalized%20to%20represent%20different%0Abehaviors%20within%20the%20same%20class%20of%20agents.%20In%20this%20work%2C%20we%20introduce%20the%0Abehavioral%20pseudo-labels%20that%20effectively%20capture%20the%20behavior%20distributions%20of%0Apedestrians%20and%20heterogeneous%20agents%20solely%20based%20on%20their%20motion%20features%2C%0Asignificantly%20improving%20the%20accuracy%20of%20trajectory%20prediction.%20To%20implement%20the%0Aframework%2C%20we%20propose%20the%20Behavioral%20Pseudo-Label%20Informed%20Sparse%20Graph%0AConvolution%20Network%20%28BP-SGCN%29%20that%20learns%20pseudo-labels%20and%20informs%20to%20a%0Atrajectory%20predictor.%20For%20optimization%2C%20we%20propose%20a%20cascaded%20training%20scheme%2C%0Ain%20which%20we%20first%20learn%20the%20pseudo-labels%20in%20an%20unsupervised%20manner%2C%20and%20then%0Aperform%20end-to-end%20fine-tuning%20on%20the%20labels%20in%20the%20direction%20of%20increasing%20the%0Atrajectory%20prediction%20accuracy.%20Experiments%20show%20that%20our%20pseudo-labels%0Aeffectively%20model%20different%20behavior%20clusters%20and%20improve%20trajectory%0Aprediction.%20Our%20proposed%20BP-SGCN%20outperforms%20existing%20methods%20using%20both%0Apedestrian%20%28ETH/UCY%2C%20pedestrian-only%20SDD%29%20and%20heterogeneous%20agent%20datasets%0A%28SDD%2C%20Argoverse%201%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBP-SGCN%253A%2520Behavioral%2520Pseudo-Label%2520Informed%2520Sparse%2520Graph%2520Convolution%250A%2520%2520Network%2520for%2520Pedestrian%2520and%2520Heterogeneous%2520Trajectory%2520Prediction%26entry.906535625%3DRuochen%2520Li%2520and%2520Stamos%2520Katsigiannis%2520and%2520Tae-Kyun%2520Kim%2520and%2520Hubert%2520P.%2520H.%2520Shum%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520allows%2520better%2520decision-making%2520in%2520applications%2520of%250Aautonomous%2520vehicles%2520or%2520surveillance%2520by%2520predicting%2520the%2520short-term%2520future%250Amovement%2520of%2520traffic%2520agents.%2520It%2520is%2520classified%2520into%2520pedestrian%2520or%2520heterogeneous%250Atrajectory%2520prediction.%2520The%2520former%2520exploits%2520the%2520relatively%2520consistent%2520behavior%250Aof%2520pedestrians%252C%2520but%2520is%2520limited%2520in%2520real-world%2520scenarios%2520with%2520heterogeneous%250Atraffic%2520agents%2520such%2520as%2520cyclists%2520and%2520vehicles.%2520The%2520latter%2520typically%2520relies%2520on%250Aextra%2520class%2520label%2520information%2520to%2520distinguish%2520the%2520heterogeneous%2520agents%252C%2520but%2520such%250Alabels%2520are%2520costly%2520to%2520annotate%2520and%2520cannot%2520be%2520generalized%2520to%2520represent%2520different%250Abehaviors%2520within%2520the%2520same%2520class%2520of%2520agents.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%250Abehavioral%2520pseudo-labels%2520that%2520effectively%2520capture%2520the%2520behavior%2520distributions%2520of%250Apedestrians%2520and%2520heterogeneous%2520agents%2520solely%2520based%2520on%2520their%2520motion%2520features%252C%250Asignificantly%2520improving%2520the%2520accuracy%2520of%2520trajectory%2520prediction.%2520To%2520implement%2520the%250Aframework%252C%2520we%2520propose%2520the%2520Behavioral%2520Pseudo-Label%2520Informed%2520Sparse%2520Graph%250AConvolution%2520Network%2520%2528BP-SGCN%2529%2520that%2520learns%2520pseudo-labels%2520and%2520informs%2520to%2520a%250Atrajectory%2520predictor.%2520For%2520optimization%252C%2520we%2520propose%2520a%2520cascaded%2520training%2520scheme%252C%250Ain%2520which%2520we%2520first%2520learn%2520the%2520pseudo-labels%2520in%2520an%2520unsupervised%2520manner%252C%2520and%2520then%250Aperform%2520end-to-end%2520fine-tuning%2520on%2520the%2520labels%2520in%2520the%2520direction%2520of%2520increasing%2520the%250Atrajectory%2520prediction%2520accuracy.%2520Experiments%2520show%2520that%2520our%2520pseudo-labels%250Aeffectively%2520model%2520different%2520behavior%2520clusters%2520and%2520improve%2520trajectory%250Aprediction.%2520Our%2520proposed%2520BP-SGCN%2520outperforms%2520existing%2520methods%2520using%2520both%250Apedestrian%2520%2528ETH/UCY%252C%2520pedestrian-only%2520SDD%2529%2520and%2520heterogeneous%2520agent%2520datasets%250A%2528SDD%252C%2520Argoverse%25201%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BP-SGCN%3A%20Behavioral%20Pseudo-Label%20Informed%20Sparse%20Graph%20Convolution%0A%20%20Network%20for%20Pedestrian%20and%20Heterogeneous%20Trajectory%20Prediction&entry.906535625=Ruochen%20Li%20and%20Stamos%20Katsigiannis%20and%20Tae-Kyun%20Kim%20and%20Hubert%20P.%20H.%20Shum&entry.1292438233=%20%20Trajectory%20prediction%20allows%20better%20decision-making%20in%20applications%20of%0Aautonomous%20vehicles%20or%20surveillance%20by%20predicting%20the%20short-term%20future%0Amovement%20of%20traffic%20agents.%20It%20is%20classified%20into%20pedestrian%20or%20heterogeneous%0Atrajectory%20prediction.%20The%20former%20exploits%20the%20relatively%20consistent%20behavior%0Aof%20pedestrians%2C%20but%20is%20limited%20in%20real-world%20scenarios%20with%20heterogeneous%0Atraffic%20agents%20such%20as%20cyclists%20and%20vehicles.%20The%20latter%20typically%20relies%20on%0Aextra%20class%20label%20information%20to%20distinguish%20the%20heterogeneous%20agents%2C%20but%20such%0Alabels%20are%20costly%20to%20annotate%20and%20cannot%20be%20generalized%20to%20represent%20different%0Abehaviors%20within%20the%20same%20class%20of%20agents.%20In%20this%20work%2C%20we%20introduce%20the%0Abehavioral%20pseudo-labels%20that%20effectively%20capture%20the%20behavior%20distributions%20of%0Apedestrians%20and%20heterogeneous%20agents%20solely%20based%20on%20their%20motion%20features%2C%0Asignificantly%20improving%20the%20accuracy%20of%20trajectory%20prediction.%20To%20implement%20the%0Aframework%2C%20we%20propose%20the%20Behavioral%20Pseudo-Label%20Informed%20Sparse%20Graph%0AConvolution%20Network%20%28BP-SGCN%29%20that%20learns%20pseudo-labels%20and%20informs%20to%20a%0Atrajectory%20predictor.%20For%20optimization%2C%20we%20propose%20a%20cascaded%20training%20scheme%2C%0Ain%20which%20we%20first%20learn%20the%20pseudo-labels%20in%20an%20unsupervised%20manner%2C%20and%20then%0Aperform%20end-to-end%20fine-tuning%20on%20the%20labels%20in%20the%20direction%20of%20increasing%20the%0Atrajectory%20prediction%20accuracy.%20Experiments%20show%20that%20our%20pseudo-labels%0Aeffectively%20model%20different%20behavior%20clusters%20and%20improve%20trajectory%0Aprediction.%20Our%20proposed%20BP-SGCN%20outperforms%20existing%20methods%20using%20both%0Apedestrian%20%28ETH/UCY%2C%20pedestrian-only%20SDD%29%20and%20heterogeneous%20agent%20datasets%0A%28SDD%2C%20Argoverse%201%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14676v1&entry.124074799=Read"},
{"title": "Ray-Tracing for Conditionally Activated Neural Networks", "author": "Claudio Gallicchio and Giuseppe Nuti", "abstract": "  In this paper, we introduce a novel architecture for conditionally activated\nneural networks combining a hierarchical construction of multiple Mixture of\nExperts (MoEs) layers with a sampling mechanism that progressively converges to\nan optimized configuration of expert activation. This methodology enables the\ndynamic unfolding of the network's architecture, facilitating efficient\npath-specific training. Experimental results demonstrate that this approach\nachieves competitive accuracy compared to conventional baselines while\nsignificantly reducing the parameter count required for inference. Notably,\nthis parameter reduction correlates with the complexity of the input patterns,\na property naturally emerging from the network's operational dynamics without\nnecessitating explicit auxiliary penalty functions.\n", "link": "http://arxiv.org/abs/2502.14788v1", "date": "2025-02-20", "relevancy": 2.1281, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5544}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5404}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ray-Tracing%20for%20Conditionally%20Activated%20Neural%20Networks&body=Title%3A%20Ray-Tracing%20for%20Conditionally%20Activated%20Neural%20Networks%0AAuthor%3A%20Claudio%20Gallicchio%20and%20Giuseppe%20Nuti%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20architecture%20for%20conditionally%20activated%0Aneural%20networks%20combining%20a%20hierarchical%20construction%20of%20multiple%20Mixture%20of%0AExperts%20%28MoEs%29%20layers%20with%20a%20sampling%20mechanism%20that%20progressively%20converges%20to%0Aan%20optimized%20configuration%20of%20expert%20activation.%20This%20methodology%20enables%20the%0Adynamic%20unfolding%20of%20the%20network%27s%20architecture%2C%20facilitating%20efficient%0Apath-specific%20training.%20Experimental%20results%20demonstrate%20that%20this%20approach%0Aachieves%20competitive%20accuracy%20compared%20to%20conventional%20baselines%20while%0Asignificantly%20reducing%20the%20parameter%20count%20required%20for%20inference.%20Notably%2C%0Athis%20parameter%20reduction%20correlates%20with%20the%20complexity%20of%20the%20input%20patterns%2C%0Aa%20property%20naturally%20emerging%20from%20the%20network%27s%20operational%20dynamics%20without%0Anecessitating%20explicit%20auxiliary%20penalty%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRay-Tracing%2520for%2520Conditionally%2520Activated%2520Neural%2520Networks%26entry.906535625%3DClaudio%2520Gallicchio%2520and%2520Giuseppe%2520Nuti%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520architecture%2520for%2520conditionally%2520activated%250Aneural%2520networks%2520combining%2520a%2520hierarchical%2520construction%2520of%2520multiple%2520Mixture%2520of%250AExperts%2520%2528MoEs%2529%2520layers%2520with%2520a%2520sampling%2520mechanism%2520that%2520progressively%2520converges%2520to%250Aan%2520optimized%2520configuration%2520of%2520expert%2520activation.%2520This%2520methodology%2520enables%2520the%250Adynamic%2520unfolding%2520of%2520the%2520network%2527s%2520architecture%252C%2520facilitating%2520efficient%250Apath-specific%2520training.%2520Experimental%2520results%2520demonstrate%2520that%2520this%2520approach%250Aachieves%2520competitive%2520accuracy%2520compared%2520to%2520conventional%2520baselines%2520while%250Asignificantly%2520reducing%2520the%2520parameter%2520count%2520required%2520for%2520inference.%2520Notably%252C%250Athis%2520parameter%2520reduction%2520correlates%2520with%2520the%2520complexity%2520of%2520the%2520input%2520patterns%252C%250Aa%2520property%2520naturally%2520emerging%2520from%2520the%2520network%2527s%2520operational%2520dynamics%2520without%250Anecessitating%2520explicit%2520auxiliary%2520penalty%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ray-Tracing%20for%20Conditionally%20Activated%20Neural%20Networks&entry.906535625=Claudio%20Gallicchio%20and%20Giuseppe%20Nuti&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20architecture%20for%20conditionally%20activated%0Aneural%20networks%20combining%20a%20hierarchical%20construction%20of%20multiple%20Mixture%20of%0AExperts%20%28MoEs%29%20layers%20with%20a%20sampling%20mechanism%20that%20progressively%20converges%20to%0Aan%20optimized%20configuration%20of%20expert%20activation.%20This%20methodology%20enables%20the%0Adynamic%20unfolding%20of%20the%20network%27s%20architecture%2C%20facilitating%20efficient%0Apath-specific%20training.%20Experimental%20results%20demonstrate%20that%20this%20approach%0Aachieves%20competitive%20accuracy%20compared%20to%20conventional%20baselines%20while%0Asignificantly%20reducing%20the%20parameter%20count%20required%20for%20inference.%20Notably%2C%0Athis%20parameter%20reduction%20correlates%20with%20the%20complexity%20of%20the%20input%20patterns%2C%0Aa%20property%20naturally%20emerging%20from%20the%20network%27s%20operational%20dynamics%20without%0Anecessitating%20explicit%20auxiliary%20penalty%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14788v1&entry.124074799=Read"},
{"title": "YOLOv12: A Breakdown of the Key Architectural Features", "author": "Mujadded Al Rabbani Alif and Muhammad Hussain", "abstract": "  This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters.\n", "link": "http://arxiv.org/abs/2502.14740v1", "date": "2025-02-20", "relevancy": 2.1255, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLOv12%3A%20A%20Breakdown%20of%20the%20Key%20Architectural%20Features&body=Title%3A%20YOLOv12%3A%20A%20Breakdown%20of%20the%20Key%20Architectural%20Features%0AAuthor%3A%20Mujadded%20Al%20Rabbani%20Alif%20and%20Muhammad%20Hussain%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20architectural%20analysis%20of%20YOLOv12%2C%20a%20significant%0Aadvancement%20in%20single-stage%2C%20real-time%20object%20detection%20building%20upon%20the%0Astrengths%20of%20its%20predecessors%20while%20introducing%20key%20improvements.%20The%20model%0Aincorporates%20an%20optimised%20backbone%20%28R-ELAN%29%2C%207x7%20separable%20convolutions%2C%20and%0AFlashAttention-driven%20area-based%20attention%2C%20improving%20feature%20extraction%2C%0Aenhanced%20efficiency%2C%20and%20robust%20detections.%20With%20multiple%20model%20variants%2C%0Asimilar%20to%20its%20predecessors%2C%20YOLOv12%20offers%20scalable%20solutions%20for%20both%0Alatency-sensitive%20and%20high-accuracy%20applications.%20Experimental%20results%20manifest%0Aconsistent%20gains%20in%20mean%20average%20precision%20%28mAP%29%20and%20inference%20speed%2C%20making%0AYOLOv12%20a%20compelling%20choice%20for%20applications%20in%20autonomous%20systems%2C%20security%2C%0Aand%20real-time%20analytics.%20By%20achieving%20an%20optimal%20balance%20between%20computational%0Aefficiency%20and%20performance%2C%20YOLOv12%20sets%20a%20new%20benchmark%20for%20real-time%20computer%0Avision%2C%20facilitating%20deployment%20across%20diverse%20hardware%20platforms%2C%20from%20edge%0Adevices%20to%20high-performance%20clusters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLOv12%253A%2520A%2520Breakdown%2520of%2520the%2520Key%2520Architectural%2520Features%26entry.906535625%3DMujadded%2520Al%2520Rabbani%2520Alif%2520and%2520Muhammad%2520Hussain%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520architectural%2520analysis%2520of%2520YOLOv12%252C%2520a%2520significant%250Aadvancement%2520in%2520single-stage%252C%2520real-time%2520object%2520detection%2520building%2520upon%2520the%250Astrengths%2520of%2520its%2520predecessors%2520while%2520introducing%2520key%2520improvements.%2520The%2520model%250Aincorporates%2520an%2520optimised%2520backbone%2520%2528R-ELAN%2529%252C%25207x7%2520separable%2520convolutions%252C%2520and%250AFlashAttention-driven%2520area-based%2520attention%252C%2520improving%2520feature%2520extraction%252C%250Aenhanced%2520efficiency%252C%2520and%2520robust%2520detections.%2520With%2520multiple%2520model%2520variants%252C%250Asimilar%2520to%2520its%2520predecessors%252C%2520YOLOv12%2520offers%2520scalable%2520solutions%2520for%2520both%250Alatency-sensitive%2520and%2520high-accuracy%2520applications.%2520Experimental%2520results%2520manifest%250Aconsistent%2520gains%2520in%2520mean%2520average%2520precision%2520%2528mAP%2529%2520and%2520inference%2520speed%252C%2520making%250AYOLOv12%2520a%2520compelling%2520choice%2520for%2520applications%2520in%2520autonomous%2520systems%252C%2520security%252C%250Aand%2520real-time%2520analytics.%2520By%2520achieving%2520an%2520optimal%2520balance%2520between%2520computational%250Aefficiency%2520and%2520performance%252C%2520YOLOv12%2520sets%2520a%2520new%2520benchmark%2520for%2520real-time%2520computer%250Avision%252C%2520facilitating%2520deployment%2520across%2520diverse%2520hardware%2520platforms%252C%2520from%2520edge%250Adevices%2520to%2520high-performance%2520clusters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLOv12%3A%20A%20Breakdown%20of%20the%20Key%20Architectural%20Features&entry.906535625=Mujadded%20Al%20Rabbani%20Alif%20and%20Muhammad%20Hussain&entry.1292438233=%20%20This%20paper%20presents%20an%20architectural%20analysis%20of%20YOLOv12%2C%20a%20significant%0Aadvancement%20in%20single-stage%2C%20real-time%20object%20detection%20building%20upon%20the%0Astrengths%20of%20its%20predecessors%20while%20introducing%20key%20improvements.%20The%20model%0Aincorporates%20an%20optimised%20backbone%20%28R-ELAN%29%2C%207x7%20separable%20convolutions%2C%20and%0AFlashAttention-driven%20area-based%20attention%2C%20improving%20feature%20extraction%2C%0Aenhanced%20efficiency%2C%20and%20robust%20detections.%20With%20multiple%20model%20variants%2C%0Asimilar%20to%20its%20predecessors%2C%20YOLOv12%20offers%20scalable%20solutions%20for%20both%0Alatency-sensitive%20and%20high-accuracy%20applications.%20Experimental%20results%20manifest%0Aconsistent%20gains%20in%20mean%20average%20precision%20%28mAP%29%20and%20inference%20speed%2C%20making%0AYOLOv12%20a%20compelling%20choice%20for%20applications%20in%20autonomous%20systems%2C%20security%2C%0Aand%20real-time%20analytics.%20By%20achieving%20an%20optimal%20balance%20between%20computational%0Aefficiency%20and%20performance%2C%20YOLOv12%20sets%20a%20new%20benchmark%20for%20real-time%20computer%0Avision%2C%20facilitating%20deployment%20across%20diverse%20hardware%20platforms%2C%20from%20edge%0Adevices%20to%20high-performance%20clusters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14740v1&entry.124074799=Read"},
{"title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs", "author": "Danni Liu and Jan Niehues", "abstract": "  While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).\n", "link": "http://arxiv.org/abs/2502.14830v1", "date": "2025-02-20", "relevancy": 2.123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5636}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5115}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Middle-Layer%20Representation%20Alignment%20for%20Cross-Lingual%20Transfer%20in%0A%20%20Fine-Tuned%20LLMs&body=Title%3A%20Middle-Layer%20Representation%20Alignment%20for%20Cross-Lingual%20Transfer%20in%0A%20%20Fine-Tuned%20LLMs%0AAuthor%3A%20Danni%20Liu%20and%20Jan%20Niehues%0AAbstract%3A%20%20%20While%20large%20language%20models%20demonstrate%20remarkable%20capabilities%20at%0Atask-specific%20applications%20through%20fine-tuning%2C%20extending%20these%20benefits%20across%0Adiverse%20languages%20is%20essential%20for%20broad%20accessibility.%20However%2C%20effective%0Across-lingual%20transfer%20is%20hindered%20by%20LLM%20performance%20gaps%20across%20languages%20and%0Athe%20scarcity%20of%20fine-tuning%20data%20in%20many%20languages.%20Through%20analysis%20of%20LLM%0Ainternal%20representations%20from%20over%201%2C000%2B%20language%20pairs%2C%20we%20discover%20that%0Amiddle%20layers%20exhibit%20the%20strongest%20potential%20for%20cross-lingual%20alignment.%0ABuilding%20on%20this%20finding%2C%20we%20propose%20a%20middle-layer%20alignment%20objective%0Aintegrated%20into%20task-specific%20training.%20Our%20experiments%20on%20slot%20filling%2C%0Amachine%20translation%2C%20and%20structured%20text%20generation%20show%20consistent%0Aimprovements%20in%20cross-lingual%20transfer%2C%20especially%20to%20lower-resource%20languages.%0AThe%20method%20is%20robust%20to%20the%20choice%20of%20alignment%20languages%20and%20generalizes%20to%0Alanguages%20unseen%20during%20alignment.%20Furthermore%2C%20we%20show%20that%20separately%20trained%0Aalignment%20modules%20can%20be%20merged%20with%20existing%20task-specific%20modules%2C%20improving%0Across-lingual%20capabilities%20without%20full%20re-training.%20Our%20code%20is%20publicly%0Aavailable%20%28https%3A//github.com/dannigt/mid-align%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiddle-Layer%2520Representation%2520Alignment%2520for%2520Cross-Lingual%2520Transfer%2520in%250A%2520%2520Fine-Tuned%2520LLMs%26entry.906535625%3DDanni%2520Liu%2520and%2520Jan%2520Niehues%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520demonstrate%2520remarkable%2520capabilities%2520at%250Atask-specific%2520applications%2520through%2520fine-tuning%252C%2520extending%2520these%2520benefits%2520across%250Adiverse%2520languages%2520is%2520essential%2520for%2520broad%2520accessibility.%2520However%252C%2520effective%250Across-lingual%2520transfer%2520is%2520hindered%2520by%2520LLM%2520performance%2520gaps%2520across%2520languages%2520and%250Athe%2520scarcity%2520of%2520fine-tuning%2520data%2520in%2520many%2520languages.%2520Through%2520analysis%2520of%2520LLM%250Ainternal%2520representations%2520from%2520over%25201%252C000%252B%2520language%2520pairs%252C%2520we%2520discover%2520that%250Amiddle%2520layers%2520exhibit%2520the%2520strongest%2520potential%2520for%2520cross-lingual%2520alignment.%250ABuilding%2520on%2520this%2520finding%252C%2520we%2520propose%2520a%2520middle-layer%2520alignment%2520objective%250Aintegrated%2520into%2520task-specific%2520training.%2520Our%2520experiments%2520on%2520slot%2520filling%252C%250Amachine%2520translation%252C%2520and%2520structured%2520text%2520generation%2520show%2520consistent%250Aimprovements%2520in%2520cross-lingual%2520transfer%252C%2520especially%2520to%2520lower-resource%2520languages.%250AThe%2520method%2520is%2520robust%2520to%2520the%2520choice%2520of%2520alignment%2520languages%2520and%2520generalizes%2520to%250Alanguages%2520unseen%2520during%2520alignment.%2520Furthermore%252C%2520we%2520show%2520that%2520separately%2520trained%250Aalignment%2520modules%2520can%2520be%2520merged%2520with%2520existing%2520task-specific%2520modules%252C%2520improving%250Across-lingual%2520capabilities%2520without%2520full%2520re-training.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520%2528https%253A//github.com/dannigt/mid-align%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Middle-Layer%20Representation%20Alignment%20for%20Cross-Lingual%20Transfer%20in%0A%20%20Fine-Tuned%20LLMs&entry.906535625=Danni%20Liu%20and%20Jan%20Niehues&entry.1292438233=%20%20While%20large%20language%20models%20demonstrate%20remarkable%20capabilities%20at%0Atask-specific%20applications%20through%20fine-tuning%2C%20extending%20these%20benefits%20across%0Adiverse%20languages%20is%20essential%20for%20broad%20accessibility.%20However%2C%20effective%0Across-lingual%20transfer%20is%20hindered%20by%20LLM%20performance%20gaps%20across%20languages%20and%0Athe%20scarcity%20of%20fine-tuning%20data%20in%20many%20languages.%20Through%20analysis%20of%20LLM%0Ainternal%20representations%20from%20over%201%2C000%2B%20language%20pairs%2C%20we%20discover%20that%0Amiddle%20layers%20exhibit%20the%20strongest%20potential%20for%20cross-lingual%20alignment.%0ABuilding%20on%20this%20finding%2C%20we%20propose%20a%20middle-layer%20alignment%20objective%0Aintegrated%20into%20task-specific%20training.%20Our%20experiments%20on%20slot%20filling%2C%0Amachine%20translation%2C%20and%20structured%20text%20generation%20show%20consistent%0Aimprovements%20in%20cross-lingual%20transfer%2C%20especially%20to%20lower-resource%20languages.%0AThe%20method%20is%20robust%20to%20the%20choice%20of%20alignment%20languages%20and%20generalizes%20to%0Alanguages%20unseen%20during%20alignment.%20Furthermore%2C%20we%20show%20that%20separately%20trained%0Aalignment%20modules%20can%20be%20merged%20with%20existing%20task-specific%20modules%2C%20improving%0Across-lingual%20capabilities%20without%20full%20re-training.%20Our%20code%20is%20publicly%0Aavailable%20%28https%3A//github.com/dannigt/mid-align%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14830v1&entry.124074799=Read"},
{"title": "Large Language Model Confidence Estimation via Black-Box Access", "author": "Tejaswini Pedapati and Amit Dhurandhar and Soumya Ghosh and Soham Dan and Prasanna Sattigeri", "abstract": "  Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.\n", "link": "http://arxiv.org/abs/2406.04370v3", "date": "2025-02-20", "relevancy": 2.1167, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5632}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Confidence%20Estimation%20via%20Black-Box%20Access&body=Title%3A%20Large%20Language%20Model%20Confidence%20Estimation%20via%20Black-Box%20Access%0AAuthor%3A%20Tejaswini%20Pedapati%20and%20Amit%20Dhurandhar%20and%20Soumya%20Ghosh%20and%20Soham%20Dan%20and%20Prasanna%20Sattigeri%0AAbstract%3A%20%20%20Estimating%20uncertainty%20or%20confidence%20in%20the%20responses%20of%20a%20model%20can%20be%0Asignificant%20in%20evaluating%20trust%20not%20only%20in%20the%20responses%2C%20but%20also%20in%20the%0Amodel%20as%20a%20whole.%20In%20this%20paper%2C%20we%20explore%20the%20problem%20of%20estimating%0Aconfidence%20for%20responses%20of%20large%20language%20models%20%28LLMs%29%20with%20simply%20black-box%0Aor%20query%20access%20to%20them.%20We%20propose%20a%20simple%20and%20extensible%20framework%20where%2C%20we%0Aengineer%20novel%20features%20and%20train%20a%20%28interpretable%29%20model%20%28viz.%20logistic%0Aregression%29%20on%20these%20features%20to%20estimate%20the%20confidence.%20We%20empirically%0Ademonstrate%20that%20our%20simple%20framework%20is%20effective%20in%20estimating%20confidence%20of%0AFlan-ul2%2C%20Llama-13b%2C%20Mistral-7b%20and%20GPT-4%20on%20four%20benchmark%20Q%5C%26A%20tasks%20as%20well%0Aas%20of%20Pegasus-large%20and%20BART-large%20on%20two%20benchmark%20summarization%20tasks%20with%20it%0Asurpassing%20baselines%20by%20even%20over%20%2410%5C%25%24%20%28on%20AUROC%29%20in%20some%20cases.%0AAdditionally%2C%20our%20interpretable%20approach%20provides%20insight%20into%20features%20that%0Aare%20predictive%20of%20confidence%2C%20leading%20to%20the%20interesting%20and%20useful%20discovery%0Athat%20our%20confidence%20models%20built%20for%20one%20LLM%20generalize%20zero-shot%20across%20others%0Aon%20a%20given%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04370v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Confidence%2520Estimation%2520via%2520Black-Box%2520Access%26entry.906535625%3DTejaswini%2520Pedapati%2520and%2520Amit%2520Dhurandhar%2520and%2520Soumya%2520Ghosh%2520and%2520Soham%2520Dan%2520and%2520Prasanna%2520Sattigeri%26entry.1292438233%3D%2520%2520Estimating%2520uncertainty%2520or%2520confidence%2520in%2520the%2520responses%2520of%2520a%2520model%2520can%2520be%250Asignificant%2520in%2520evaluating%2520trust%2520not%2520only%2520in%2520the%2520responses%252C%2520but%2520also%2520in%2520the%250Amodel%2520as%2520a%2520whole.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520problem%2520of%2520estimating%250Aconfidence%2520for%2520responses%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520simply%2520black-box%250Aor%2520query%2520access%2520to%2520them.%2520We%2520propose%2520a%2520simple%2520and%2520extensible%2520framework%2520where%252C%2520we%250Aengineer%2520novel%2520features%2520and%2520train%2520a%2520%2528interpretable%2529%2520model%2520%2528viz.%2520logistic%250Aregression%2529%2520on%2520these%2520features%2520to%2520estimate%2520the%2520confidence.%2520We%2520empirically%250Ademonstrate%2520that%2520our%2520simple%2520framework%2520is%2520effective%2520in%2520estimating%2520confidence%2520of%250AFlan-ul2%252C%2520Llama-13b%252C%2520Mistral-7b%2520and%2520GPT-4%2520on%2520four%2520benchmark%2520Q%255C%2526A%2520tasks%2520as%2520well%250Aas%2520of%2520Pegasus-large%2520and%2520BART-large%2520on%2520two%2520benchmark%2520summarization%2520tasks%2520with%2520it%250Asurpassing%2520baselines%2520by%2520even%2520over%2520%252410%255C%2525%2524%2520%2528on%2520AUROC%2529%2520in%2520some%2520cases.%250AAdditionally%252C%2520our%2520interpretable%2520approach%2520provides%2520insight%2520into%2520features%2520that%250Aare%2520predictive%2520of%2520confidence%252C%2520leading%2520to%2520the%2520interesting%2520and%2520useful%2520discovery%250Athat%2520our%2520confidence%2520models%2520built%2520for%2520one%2520LLM%2520generalize%2520zero-shot%2520across%2520others%250Aon%2520a%2520given%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04370v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Confidence%20Estimation%20via%20Black-Box%20Access&entry.906535625=Tejaswini%20Pedapati%20and%20Amit%20Dhurandhar%20and%20Soumya%20Ghosh%20and%20Soham%20Dan%20and%20Prasanna%20Sattigeri&entry.1292438233=%20%20Estimating%20uncertainty%20or%20confidence%20in%20the%20responses%20of%20a%20model%20can%20be%0Asignificant%20in%20evaluating%20trust%20not%20only%20in%20the%20responses%2C%20but%20also%20in%20the%0Amodel%20as%20a%20whole.%20In%20this%20paper%2C%20we%20explore%20the%20problem%20of%20estimating%0Aconfidence%20for%20responses%20of%20large%20language%20models%20%28LLMs%29%20with%20simply%20black-box%0Aor%20query%20access%20to%20them.%20We%20propose%20a%20simple%20and%20extensible%20framework%20where%2C%20we%0Aengineer%20novel%20features%20and%20train%20a%20%28interpretable%29%20model%20%28viz.%20logistic%0Aregression%29%20on%20these%20features%20to%20estimate%20the%20confidence.%20We%20empirically%0Ademonstrate%20that%20our%20simple%20framework%20is%20effective%20in%20estimating%20confidence%20of%0AFlan-ul2%2C%20Llama-13b%2C%20Mistral-7b%20and%20GPT-4%20on%20four%20benchmark%20Q%5C%26A%20tasks%20as%20well%0Aas%20of%20Pegasus-large%20and%20BART-large%20on%20two%20benchmark%20summarization%20tasks%20with%20it%0Asurpassing%20baselines%20by%20even%20over%20%2410%5C%25%24%20%28on%20AUROC%29%20in%20some%20cases.%0AAdditionally%2C%20our%20interpretable%20approach%20provides%20insight%20into%20features%20that%0Aare%20predictive%20of%20confidence%2C%20leading%20to%20the%20interesting%20and%20useful%20discovery%0Athat%20our%20confidence%20models%20built%20for%20one%20LLM%20generalize%20zero-shot%20across%20others%0Aon%20a%20given%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04370v3&entry.124074799=Read"},
{"title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series\n  Forecasters", "author": "Ching Chang and Wei-Yao Wang and Wen-Chih Peng and Tien-Fu Chen", "abstract": "  Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have leveraged the\nrepresentation learning transferability of pre-trained Large Language Models\n(LLMs) to handle limited non-linguistic datasets effectively. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\ntime-series alignment stage to align LLMs with the nuances of time-series data,\nand the forecasting fine-tuning stage for downstream time-series forecasting\ntasks. Furthermore, our framework features a novel two-level aggregation method\nthat integrates multi-scale temporal data within pre-trained LLMs, enhancing\ntheir ability to interpret time-specific information. In experiments across 7\ntime-series forecasting datasets, LLM4TS is superior to existing\nstate-of-the-art methods compared with trained-from-scratch models in full-shot\nscenarios, and also achieves the highest rank in few-shot scenarios. In\naddition, evaluations compared with different unsupervised representation\nlearning approaches highlight LLM4TS's effectiveness with representation\nlearning in forecasting tasks. Ablation studies further validate each\ncomponent's contribution to LLM4TS and underscore the essential role of\nutilizing LLM's pre-trained weights for optimal performance. The code is\navailable at https://github.com/blacksnail789521/LLM4TS.\n", "link": "http://arxiv.org/abs/2308.08469v6", "date": "2025-02-20", "relevancy": 2.1127, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5542}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5108}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM4TS%3A%20Aligning%20Pre-Trained%20LLMs%20as%20Data-Efficient%20Time-Series%0A%20%20Forecasters&body=Title%3A%20LLM4TS%3A%20Aligning%20Pre-Trained%20LLMs%20as%20Data-Efficient%20Time-Series%0A%20%20Forecasters%0AAuthor%3A%20Ching%20Chang%20and%20Wei-Yao%20Wang%20and%20Wen-Chih%20Peng%20and%20Tien-Fu%20Chen%0AAbstract%3A%20%20%20Multivariate%20time-series%20forecasting%20is%20vital%20in%20various%20domains%2C%20e.g.%2C%0Aeconomic%20planning%20and%20weather%20prediction.%20Deep%20train-from-scratch%20models%20have%0Aexhibited%20effective%20performance%20yet%20require%20large%20amounts%20of%20data%2C%20which%20limits%0Areal-world%20applicability.%20Recently%2C%20researchers%20have%20leveraged%20the%0Arepresentation%20learning%20transferability%20of%20pre-trained%20Large%20Language%20Models%0A%28LLMs%29%20to%20handle%20limited%20non-linguistic%20datasets%20effectively.%20However%2C%0Aincorporating%20LLMs%20with%20time-series%20data%20presents%20challenges%20of%20limited%0Aadaptation%20due%20to%20different%20compositions%20between%20time-series%20and%20linguistic%0Adata%2C%20and%20the%20inability%20to%20process%20multi-scale%20temporal%20information.%20To%20tackle%0Athese%20challenges%2C%20we%20propose%20LLM4TS%2C%20a%20framework%20for%20time-series%20forecasting%0Awith%20pre-trained%20LLMs.%20LLM4TS%20consists%20of%20a%20two-stage%20fine-tuning%20strategy%3A%20the%0Atime-series%20alignment%20stage%20to%20align%20LLMs%20with%20the%20nuances%20of%20time-series%20data%2C%0Aand%20the%20forecasting%20fine-tuning%20stage%20for%20downstream%20time-series%20forecasting%0Atasks.%20Furthermore%2C%20our%20framework%20features%20a%20novel%20two-level%20aggregation%20method%0Athat%20integrates%20multi-scale%20temporal%20data%20within%20pre-trained%20LLMs%2C%20enhancing%0Atheir%20ability%20to%20interpret%20time-specific%20information.%20In%20experiments%20across%207%0Atime-series%20forecasting%20datasets%2C%20LLM4TS%20is%20superior%20to%20existing%0Astate-of-the-art%20methods%20compared%20with%20trained-from-scratch%20models%20in%20full-shot%0Ascenarios%2C%20and%20also%20achieves%20the%20highest%20rank%20in%20few-shot%20scenarios.%20In%0Aaddition%2C%20evaluations%20compared%20with%20different%20unsupervised%20representation%0Alearning%20approaches%20highlight%20LLM4TS%27s%20effectiveness%20with%20representation%0Alearning%20in%20forecasting%20tasks.%20Ablation%20studies%20further%20validate%20each%0Acomponent%27s%20contribution%20to%20LLM4TS%20and%20underscore%20the%20essential%20role%20of%0Autilizing%20LLM%27s%20pre-trained%20weights%20for%20optimal%20performance.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/blacksnail789521/LLM4TS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08469v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM4TS%253A%2520Aligning%2520Pre-Trained%2520LLMs%2520as%2520Data-Efficient%2520Time-Series%250A%2520%2520Forecasters%26entry.906535625%3DChing%2520Chang%2520and%2520Wei-Yao%2520Wang%2520and%2520Wen-Chih%2520Peng%2520and%2520Tien-Fu%2520Chen%26entry.1292438233%3D%2520%2520Multivariate%2520time-series%2520forecasting%2520is%2520vital%2520in%2520various%2520domains%252C%2520e.g.%252C%250Aeconomic%2520planning%2520and%2520weather%2520prediction.%2520Deep%2520train-from-scratch%2520models%2520have%250Aexhibited%2520effective%2520performance%2520yet%2520require%2520large%2520amounts%2520of%2520data%252C%2520which%2520limits%250Areal-world%2520applicability.%2520Recently%252C%2520researchers%2520have%2520leveraged%2520the%250Arepresentation%2520learning%2520transferability%2520of%2520pre-trained%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520to%2520handle%2520limited%2520non-linguistic%2520datasets%2520effectively.%2520However%252C%250Aincorporating%2520LLMs%2520with%2520time-series%2520data%2520presents%2520challenges%2520of%2520limited%250Aadaptation%2520due%2520to%2520different%2520compositions%2520between%2520time-series%2520and%2520linguistic%250Adata%252C%2520and%2520the%2520inability%2520to%2520process%2520multi-scale%2520temporal%2520information.%2520To%2520tackle%250Athese%2520challenges%252C%2520we%2520propose%2520LLM4TS%252C%2520a%2520framework%2520for%2520time-series%2520forecasting%250Awith%2520pre-trained%2520LLMs.%2520LLM4TS%2520consists%2520of%2520a%2520two-stage%2520fine-tuning%2520strategy%253A%2520the%250Atime-series%2520alignment%2520stage%2520to%2520align%2520LLMs%2520with%2520the%2520nuances%2520of%2520time-series%2520data%252C%250Aand%2520the%2520forecasting%2520fine-tuning%2520stage%2520for%2520downstream%2520time-series%2520forecasting%250Atasks.%2520Furthermore%252C%2520our%2520framework%2520features%2520a%2520novel%2520two-level%2520aggregation%2520method%250Athat%2520integrates%2520multi-scale%2520temporal%2520data%2520within%2520pre-trained%2520LLMs%252C%2520enhancing%250Atheir%2520ability%2520to%2520interpret%2520time-specific%2520information.%2520In%2520experiments%2520across%25207%250Atime-series%2520forecasting%2520datasets%252C%2520LLM4TS%2520is%2520superior%2520to%2520existing%250Astate-of-the-art%2520methods%2520compared%2520with%2520trained-from-scratch%2520models%2520in%2520full-shot%250Ascenarios%252C%2520and%2520also%2520achieves%2520the%2520highest%2520rank%2520in%2520few-shot%2520scenarios.%2520In%250Aaddition%252C%2520evaluations%2520compared%2520with%2520different%2520unsupervised%2520representation%250Alearning%2520approaches%2520highlight%2520LLM4TS%2527s%2520effectiveness%2520with%2520representation%250Alearning%2520in%2520forecasting%2520tasks.%2520Ablation%2520studies%2520further%2520validate%2520each%250Acomponent%2527s%2520contribution%2520to%2520LLM4TS%2520and%2520underscore%2520the%2520essential%2520role%2520of%250Autilizing%2520LLM%2527s%2520pre-trained%2520weights%2520for%2520optimal%2520performance.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/blacksnail789521/LLM4TS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.08469v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM4TS%3A%20Aligning%20Pre-Trained%20LLMs%20as%20Data-Efficient%20Time-Series%0A%20%20Forecasters&entry.906535625=Ching%20Chang%20and%20Wei-Yao%20Wang%20and%20Wen-Chih%20Peng%20and%20Tien-Fu%20Chen&entry.1292438233=%20%20Multivariate%20time-series%20forecasting%20is%20vital%20in%20various%20domains%2C%20e.g.%2C%0Aeconomic%20planning%20and%20weather%20prediction.%20Deep%20train-from-scratch%20models%20have%0Aexhibited%20effective%20performance%20yet%20require%20large%20amounts%20of%20data%2C%20which%20limits%0Areal-world%20applicability.%20Recently%2C%20researchers%20have%20leveraged%20the%0Arepresentation%20learning%20transferability%20of%20pre-trained%20Large%20Language%20Models%0A%28LLMs%29%20to%20handle%20limited%20non-linguistic%20datasets%20effectively.%20However%2C%0Aincorporating%20LLMs%20with%20time-series%20data%20presents%20challenges%20of%20limited%0Aadaptation%20due%20to%20different%20compositions%20between%20time-series%20and%20linguistic%0Adata%2C%20and%20the%20inability%20to%20process%20multi-scale%20temporal%20information.%20To%20tackle%0Athese%20challenges%2C%20we%20propose%20LLM4TS%2C%20a%20framework%20for%20time-series%20forecasting%0Awith%20pre-trained%20LLMs.%20LLM4TS%20consists%20of%20a%20two-stage%20fine-tuning%20strategy%3A%20the%0Atime-series%20alignment%20stage%20to%20align%20LLMs%20with%20the%20nuances%20of%20time-series%20data%2C%0Aand%20the%20forecasting%20fine-tuning%20stage%20for%20downstream%20time-series%20forecasting%0Atasks.%20Furthermore%2C%20our%20framework%20features%20a%20novel%20two-level%20aggregation%20method%0Athat%20integrates%20multi-scale%20temporal%20data%20within%20pre-trained%20LLMs%2C%20enhancing%0Atheir%20ability%20to%20interpret%20time-specific%20information.%20In%20experiments%20across%207%0Atime-series%20forecasting%20datasets%2C%20LLM4TS%20is%20superior%20to%20existing%0Astate-of-the-art%20methods%20compared%20with%20trained-from-scratch%20models%20in%20full-shot%0Ascenarios%2C%20and%20also%20achieves%20the%20highest%20rank%20in%20few-shot%20scenarios.%20In%0Aaddition%2C%20evaluations%20compared%20with%20different%20unsupervised%20representation%0Alearning%20approaches%20highlight%20LLM4TS%27s%20effectiveness%20with%20representation%0Alearning%20in%20forecasting%20tasks.%20Ablation%20studies%20further%20validate%20each%0Acomponent%27s%20contribution%20to%20LLM4TS%20and%20underscore%20the%20essential%20role%20of%0Autilizing%20LLM%27s%20pre-trained%20weights%20for%20optimal%20performance.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/blacksnail789521/LLM4TS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08469v6&entry.124074799=Read"},
{"title": "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity", "author": "Xinghan Pan", "abstract": "  This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.\n", "link": "http://arxiv.org/abs/2502.14620v1", "date": "2025-02-20", "relevancy": 2.1032, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20RWKV%20for%20Sentence%20Embeddings%3A%20Layer-wise%20Analysis%20and%20Baseline%0A%20%20Comparison%20for%20Semantic%20Similarity&body=Title%3A%20Exploring%20RWKV%20for%20Sentence%20Embeddings%3A%20Layer-wise%20Analysis%20and%20Baseline%0A%20%20Comparison%20for%20Semantic%20Similarity%0AAuthor%3A%20Xinghan%20Pan%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20efficacy%20of%20RWKV%2C%20a%20novel%20language%20model%0Aarchitecture%20known%20for%20its%20linear%20attention%20mechanism%2C%20for%20generating%20sentence%0Aembeddings%20in%20a%20zero-shot%20setting.%20I%20conduct%20a%20layer-wise%20analysis%20to%20evaluate%0Athe%20semantic%20similarity%20captured%20by%20embeddings%20from%20different%20hidden%20layers%20of%0Aa%20pre-trained%20RWKV%20model.%20The%20performance%20is%20assessed%20on%20the%20Microsoft%20Research%0AParaphrase%20Corpus%20%28MRPC%29%20dataset%20using%20Spearman%20correlation%20and%20compared%0Aagainst%20a%20GloVe-based%20baseline.%20My%20results%20indicate%20that%20while%20RWKV%20embeddings%0Acapture%20some%20semantic%20relatedness%2C%20they%20underperform%20compared%20to%20the%20GloVe%0Abaseline%20in%20terms%20of%20Spearman%20correlation.%20I%20also%20analyze%20the%20inference%20time%0Aand%20GPU%20memory%20usage%2C%20highlighting%20the%20computational%20trade-offs%20associated%20with%0ARWKV%20embeddings.%20The%20findings%20suggest%20that%20while%20RWKV%20offers%20potential%0Aadvantages%20in%20terms%20of%20linear%20scaling%2C%20its%20zero-shot%20sentence%20embedding%20quality%0Afor%20semantic%20similarity%20tasks%20requires%20further%20investigation%20and%20potential%0Atask-specific%20fine-tuning%20to%20match%20or%20exceed%20simpler%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520RWKV%2520for%2520Sentence%2520Embeddings%253A%2520Layer-wise%2520Analysis%2520and%2520Baseline%250A%2520%2520Comparison%2520for%2520Semantic%2520Similarity%26entry.906535625%3DXinghan%2520Pan%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520efficacy%2520of%2520RWKV%252C%2520a%2520novel%2520language%2520model%250Aarchitecture%2520known%2520for%2520its%2520linear%2520attention%2520mechanism%252C%2520for%2520generating%2520sentence%250Aembeddings%2520in%2520a%2520zero-shot%2520setting.%2520I%2520conduct%2520a%2520layer-wise%2520analysis%2520to%2520evaluate%250Athe%2520semantic%2520similarity%2520captured%2520by%2520embeddings%2520from%2520different%2520hidden%2520layers%2520of%250Aa%2520pre-trained%2520RWKV%2520model.%2520The%2520performance%2520is%2520assessed%2520on%2520the%2520Microsoft%2520Research%250AParaphrase%2520Corpus%2520%2528MRPC%2529%2520dataset%2520using%2520Spearman%2520correlation%2520and%2520compared%250Aagainst%2520a%2520GloVe-based%2520baseline.%2520My%2520results%2520indicate%2520that%2520while%2520RWKV%2520embeddings%250Acapture%2520some%2520semantic%2520relatedness%252C%2520they%2520underperform%2520compared%2520to%2520the%2520GloVe%250Abaseline%2520in%2520terms%2520of%2520Spearman%2520correlation.%2520I%2520also%2520analyze%2520the%2520inference%2520time%250Aand%2520GPU%2520memory%2520usage%252C%2520highlighting%2520the%2520computational%2520trade-offs%2520associated%2520with%250ARWKV%2520embeddings.%2520The%2520findings%2520suggest%2520that%2520while%2520RWKV%2520offers%2520potential%250Aadvantages%2520in%2520terms%2520of%2520linear%2520scaling%252C%2520its%2520zero-shot%2520sentence%2520embedding%2520quality%250Afor%2520semantic%2520similarity%2520tasks%2520requires%2520further%2520investigation%2520and%2520potential%250Atask-specific%2520fine-tuning%2520to%2520match%2520or%2520exceed%2520simpler%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20RWKV%20for%20Sentence%20Embeddings%3A%20Layer-wise%20Analysis%20and%20Baseline%0A%20%20Comparison%20for%20Semantic%20Similarity&entry.906535625=Xinghan%20Pan&entry.1292438233=%20%20This%20paper%20investigates%20the%20efficacy%20of%20RWKV%2C%20a%20novel%20language%20model%0Aarchitecture%20known%20for%20its%20linear%20attention%20mechanism%2C%20for%20generating%20sentence%0Aembeddings%20in%20a%20zero-shot%20setting.%20I%20conduct%20a%20layer-wise%20analysis%20to%20evaluate%0Athe%20semantic%20similarity%20captured%20by%20embeddings%20from%20different%20hidden%20layers%20of%0Aa%20pre-trained%20RWKV%20model.%20The%20performance%20is%20assessed%20on%20the%20Microsoft%20Research%0AParaphrase%20Corpus%20%28MRPC%29%20dataset%20using%20Spearman%20correlation%20and%20compared%0Aagainst%20a%20GloVe-based%20baseline.%20My%20results%20indicate%20that%20while%20RWKV%20embeddings%0Acapture%20some%20semantic%20relatedness%2C%20they%20underperform%20compared%20to%20the%20GloVe%0Abaseline%20in%20terms%20of%20Spearman%20correlation.%20I%20also%20analyze%20the%20inference%20time%0Aand%20GPU%20memory%20usage%2C%20highlighting%20the%20computational%20trade-offs%20associated%20with%0ARWKV%20embeddings.%20The%20findings%20suggest%20that%20while%20RWKV%20offers%20potential%0Aadvantages%20in%20terms%20of%20linear%20scaling%2C%20its%20zero-shot%20sentence%20embedding%20quality%0Afor%20semantic%20similarity%20tasks%20requires%20further%20investigation%20and%20potential%0Atask-specific%20fine-tuning%20to%20match%20or%20exceed%20simpler%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14620v1&entry.124074799=Read"},
{"title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through\n  Exogenous Behavior-Semantic Integration", "author": "Minjie Hong and Yan Xia and Zehan Wang and Jieming Zhu and Ye Wang and Sihang Cai and Xiaoda Yang and Quanyu Dai and Zhenhua Dong and Zhimeng Zhang and Zhou Zhao", "abstract": "  Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks.\n", "link": "http://arxiv.org/abs/2502.14735v1", "date": "2025-02-20", "relevancy": 2.1001, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EAGER-LLM%3A%20Enhancing%20Large%20Language%20Models%20as%20Recommenders%20through%0A%20%20Exogenous%20Behavior-Semantic%20Integration&body=Title%3A%20EAGER-LLM%3A%20Enhancing%20Large%20Language%20Models%20as%20Recommenders%20through%0A%20%20Exogenous%20Behavior-Semantic%20Integration%0AAuthor%3A%20Minjie%20Hong%20and%20Yan%20Xia%20and%20Zehan%20Wang%20and%20Jieming%20Zhu%20and%20Ye%20Wang%20and%20Sihang%20Cai%20and%20Xiaoda%20Yang%20and%20Quanyu%20Dai%20and%20Zhenhua%20Dong%20and%20Zhimeng%20Zhang%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20leveraged%20as%20foundational%0Abackbones%20in%20the%20development%20of%20advanced%20recommender%20systems%2C%20offering%20enhanced%0Acapabilities%20through%20their%20extensive%20knowledge%20and%20reasoning.%20Existing%0Allm-based%20recommender%20systems%20%28RSs%29%20often%20face%20challenges%20due%20to%20the%0Asignificant%20differences%20between%20the%20linguistic%20semantics%20of%20pre-trained%20LLMs%0Aand%20the%20collaborative%20semantics%20essential%20for%20RSs.%20These%20systems%20use%0Apre-trained%20linguistic%20semantics%20but%20learn%20collaborative%20semantics%20from%20scratch%0Avia%20the%20llm-Backbone.%20However%2C%20LLMs%20are%20not%20designed%20for%20recommendations%2C%0Aleading%20to%20inefficient%20collaborative%20learning%2C%20weak%20result%20correlations%2C%20and%0Apoor%20integration%20of%20traditional%20RS%20features.%20To%20address%20these%20challenges%2C%20we%0Apropose%20EAGER-LLM%2C%20a%20decoder-only%20llm-based%20generative%20recommendation%20framework%0Athat%20integrates%20endogenous%20and%20exogenous%20behavioral%20and%20semantic%20information%20in%0Aa%20non-intrusive%20manner.%20Specifically%2C%20we%20propose%201%29dual-source%20knowledge-rich%0Aitem%20indices%20that%20integrates%20indexing%20sequences%20for%20exogenous%20signals%2C%20enabling%0Aefficient%20link-wide%20processing%3B%202%29non-invasive%20multiscale%20alignment%0Areconstruction%20tasks%20guide%20the%20model%20toward%20a%20deeper%20understanding%20of%20both%0Acollaborative%20and%20semantic%20signals%3B%203%29an%20annealing%20adapter%20designed%20to%20finely%0Abalance%20the%20model%27s%20recommendation%20performance%20with%20its%20comprehension%0Acapabilities.%20We%20demonstrate%20EAGER-LLM%27s%20effectiveness%20through%20rigorous%20testing%0Aon%20three%20public%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEAGER-LLM%253A%2520Enhancing%2520Large%2520Language%2520Models%2520as%2520Recommenders%2520through%250A%2520%2520Exogenous%2520Behavior-Semantic%2520Integration%26entry.906535625%3DMinjie%2520Hong%2520and%2520Yan%2520Xia%2520and%2520Zehan%2520Wang%2520and%2520Jieming%2520Zhu%2520and%2520Ye%2520Wang%2520and%2520Sihang%2520Cai%2520and%2520Xiaoda%2520Yang%2520and%2520Quanyu%2520Dai%2520and%2520Zhenhua%2520Dong%2520and%2520Zhimeng%2520Zhang%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520leveraged%2520as%2520foundational%250Abackbones%2520in%2520the%2520development%2520of%2520advanced%2520recommender%2520systems%252C%2520offering%2520enhanced%250Acapabilities%2520through%2520their%2520extensive%2520knowledge%2520and%2520reasoning.%2520Existing%250Allm-based%2520recommender%2520systems%2520%2528RSs%2529%2520often%2520face%2520challenges%2520due%2520to%2520the%250Asignificant%2520differences%2520between%2520the%2520linguistic%2520semantics%2520of%2520pre-trained%2520LLMs%250Aand%2520the%2520collaborative%2520semantics%2520essential%2520for%2520RSs.%2520These%2520systems%2520use%250Apre-trained%2520linguistic%2520semantics%2520but%2520learn%2520collaborative%2520semantics%2520from%2520scratch%250Avia%2520the%2520llm-Backbone.%2520However%252C%2520LLMs%2520are%2520not%2520designed%2520for%2520recommendations%252C%250Aleading%2520to%2520inefficient%2520collaborative%2520learning%252C%2520weak%2520result%2520correlations%252C%2520and%250Apoor%2520integration%2520of%2520traditional%2520RS%2520features.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520EAGER-LLM%252C%2520a%2520decoder-only%2520llm-based%2520generative%2520recommendation%2520framework%250Athat%2520integrates%2520endogenous%2520and%2520exogenous%2520behavioral%2520and%2520semantic%2520information%2520in%250Aa%2520non-intrusive%2520manner.%2520Specifically%252C%2520we%2520propose%25201%2529dual-source%2520knowledge-rich%250Aitem%2520indices%2520that%2520integrates%2520indexing%2520sequences%2520for%2520exogenous%2520signals%252C%2520enabling%250Aefficient%2520link-wide%2520processing%253B%25202%2529non-invasive%2520multiscale%2520alignment%250Areconstruction%2520tasks%2520guide%2520the%2520model%2520toward%2520a%2520deeper%2520understanding%2520of%2520both%250Acollaborative%2520and%2520semantic%2520signals%253B%25203%2529an%2520annealing%2520adapter%2520designed%2520to%2520finely%250Abalance%2520the%2520model%2527s%2520recommendation%2520performance%2520with%2520its%2520comprehension%250Acapabilities.%2520We%2520demonstrate%2520EAGER-LLM%2527s%2520effectiveness%2520through%2520rigorous%2520testing%250Aon%2520three%2520public%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EAGER-LLM%3A%20Enhancing%20Large%20Language%20Models%20as%20Recommenders%20through%0A%20%20Exogenous%20Behavior-Semantic%20Integration&entry.906535625=Minjie%20Hong%20and%20Yan%20Xia%20and%20Zehan%20Wang%20and%20Jieming%20Zhu%20and%20Ye%20Wang%20and%20Sihang%20Cai%20and%20Xiaoda%20Yang%20and%20Quanyu%20Dai%20and%20Zhenhua%20Dong%20and%20Zhimeng%20Zhang%20and%20Zhou%20Zhao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20leveraged%20as%20foundational%0Abackbones%20in%20the%20development%20of%20advanced%20recommender%20systems%2C%20offering%20enhanced%0Acapabilities%20through%20their%20extensive%20knowledge%20and%20reasoning.%20Existing%0Allm-based%20recommender%20systems%20%28RSs%29%20often%20face%20challenges%20due%20to%20the%0Asignificant%20differences%20between%20the%20linguistic%20semantics%20of%20pre-trained%20LLMs%0Aand%20the%20collaborative%20semantics%20essential%20for%20RSs.%20These%20systems%20use%0Apre-trained%20linguistic%20semantics%20but%20learn%20collaborative%20semantics%20from%20scratch%0Avia%20the%20llm-Backbone.%20However%2C%20LLMs%20are%20not%20designed%20for%20recommendations%2C%0Aleading%20to%20inefficient%20collaborative%20learning%2C%20weak%20result%20correlations%2C%20and%0Apoor%20integration%20of%20traditional%20RS%20features.%20To%20address%20these%20challenges%2C%20we%0Apropose%20EAGER-LLM%2C%20a%20decoder-only%20llm-based%20generative%20recommendation%20framework%0Athat%20integrates%20endogenous%20and%20exogenous%20behavioral%20and%20semantic%20information%20in%0Aa%20non-intrusive%20manner.%20Specifically%2C%20we%20propose%201%29dual-source%20knowledge-rich%0Aitem%20indices%20that%20integrates%20indexing%20sequences%20for%20exogenous%20signals%2C%20enabling%0Aefficient%20link-wide%20processing%3B%202%29non-invasive%20multiscale%20alignment%0Areconstruction%20tasks%20guide%20the%20model%20toward%20a%20deeper%20understanding%20of%20both%0Acollaborative%20and%20semantic%20signals%3B%203%29an%20annealing%20adapter%20designed%20to%20finely%0Abalance%20the%20model%27s%20recommendation%20performance%20with%20its%20comprehension%0Acapabilities.%20We%20demonstrate%20EAGER-LLM%27s%20effectiveness%20through%20rigorous%20testing%0Aon%20three%20public%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14735v1&entry.124074799=Read"},
{"title": "TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident\n  Detection of Prostate Cancer in Micro-Ultrasound", "author": "Mohamed Harmanani and Paul F. R. Wilson and Minh Nguyen Nhat To and Mahdi Gilany and Amoon Jamzad and Fahimeh Fooladgar and Brian Wodlinger and Purang Abolmaesumi and Parvin Mousavi", "abstract": "  While deep learning methods have shown great promise in improving the\neffectiveness of prostate cancer (PCa) diagnosis by detecting suspicious\nlesions from trans-rectal ultrasound (TRUS), they must overcome multiple\nsimultaneous challenges. There is high heterogeneity in tissue appearance,\nsignificant class imbalance in favor of benign examples, and scarcity in the\nnumber and quality of ground truth annotations available to train models.\nFailure to address even a single one of these problems can result in\nunacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed,\ntuned, and integrated system for reliable PCa detection. Our pipeline\nintegrates self-supervised learning, multiple-instance learning aggregation\nusing transformers, random-undersampled boosting and ensembling: these address\nlabel scarcity, weak labels, class imbalance, and overconfidence, respectively.\nWe train and rigorously evaluate our method using a large, multi-center dataset\nof micro-ultrasound data. Our method outperforms previous state-of-the-art deep\nlearning methods in terms of accuracy and uncertainty calibration, with AUROC\nand balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20%\nof predictions with the highest confidence, we can achieve a balanced accuracy\nof up to 91%. The success of TRUSWorthy demonstrates the potential of\nintegrated deep learning solutions to meet clinical needs in a highly\nchallenging deployment setting, and is a significant step towards creating a\ntrustworthy system for computer-assisted PCa diagnosis.\n", "link": "http://arxiv.org/abs/2502.14707v1", "date": "2025-02-20", "relevancy": 2.0912, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5639}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5296}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRUSWorthy%3A%20Toward%20Clinically%20Applicable%20Deep%20Learning%20for%20Confident%0A%20%20Detection%20of%20Prostate%20Cancer%20in%20Micro-Ultrasound&body=Title%3A%20TRUSWorthy%3A%20Toward%20Clinically%20Applicable%20Deep%20Learning%20for%20Confident%0A%20%20Detection%20of%20Prostate%20Cancer%20in%20Micro-Ultrasound%0AAuthor%3A%20Mohamed%20Harmanani%20and%20Paul%20F.%20R.%20Wilson%20and%20Minh%20Nguyen%20Nhat%20To%20and%20Mahdi%20Gilany%20and%20Amoon%20Jamzad%20and%20Fahimeh%20Fooladgar%20and%20Brian%20Wodlinger%20and%20Purang%20Abolmaesumi%20and%20Parvin%20Mousavi%0AAbstract%3A%20%20%20While%20deep%20learning%20methods%20have%20shown%20great%20promise%20in%20improving%20the%0Aeffectiveness%20of%20prostate%20cancer%20%28PCa%29%20diagnosis%20by%20detecting%20suspicious%0Alesions%20from%20trans-rectal%20ultrasound%20%28TRUS%29%2C%20they%20must%20overcome%20multiple%0Asimultaneous%20challenges.%20There%20is%20high%20heterogeneity%20in%20tissue%20appearance%2C%0Asignificant%20class%20imbalance%20in%20favor%20of%20benign%20examples%2C%20and%20scarcity%20in%20the%0Anumber%20and%20quality%20of%20ground%20truth%20annotations%20available%20to%20train%20models.%0AFailure%20to%20address%20even%20a%20single%20one%20of%20these%20problems%20can%20result%20in%0Aunacceptable%20clinical%20outcomes.We%20propose%20TRUSWorthy%2C%20a%20carefully%20designed%2C%0Atuned%2C%20and%20integrated%20system%20for%20reliable%20PCa%20detection.%20Our%20pipeline%0Aintegrates%20self-supervised%20learning%2C%20multiple-instance%20learning%20aggregation%0Ausing%20transformers%2C%20random-undersampled%20boosting%20and%20ensembling%3A%20these%20address%0Alabel%20scarcity%2C%20weak%20labels%2C%20class%20imbalance%2C%20and%20overconfidence%2C%20respectively.%0AWe%20train%20and%20rigorously%20evaluate%20our%20method%20using%20a%20large%2C%20multi-center%20dataset%0Aof%20micro-ultrasound%20data.%20Our%20method%20outperforms%20previous%20state-of-the-art%20deep%0Alearning%20methods%20in%20terms%20of%20accuracy%20and%20uncertainty%20calibration%2C%20with%20AUROC%0Aand%20balanced%20accuracy%20scores%20of%2079.9%25%20and%2071.5%25%2C%20respectively.%20On%20the%20top%2020%25%0Aof%20predictions%20with%20the%20highest%20confidence%2C%20we%20can%20achieve%20a%20balanced%20accuracy%0Aof%20up%20to%2091%25.%20The%20success%20of%20TRUSWorthy%20demonstrates%20the%20potential%20of%0Aintegrated%20deep%20learning%20solutions%20to%20meet%20clinical%20needs%20in%20a%20highly%0Achallenging%20deployment%20setting%2C%20and%20is%20a%20significant%20step%20towards%20creating%20a%0Atrustworthy%20system%20for%20computer-assisted%20PCa%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRUSWorthy%253A%2520Toward%2520Clinically%2520Applicable%2520Deep%2520Learning%2520for%2520Confident%250A%2520%2520Detection%2520of%2520Prostate%2520Cancer%2520in%2520Micro-Ultrasound%26entry.906535625%3DMohamed%2520Harmanani%2520and%2520Paul%2520F.%2520R.%2520Wilson%2520and%2520Minh%2520Nguyen%2520Nhat%2520To%2520and%2520Mahdi%2520Gilany%2520and%2520Amoon%2520Jamzad%2520and%2520Fahimeh%2520Fooladgar%2520and%2520Brian%2520Wodlinger%2520and%2520Purang%2520Abolmaesumi%2520and%2520Parvin%2520Mousavi%26entry.1292438233%3D%2520%2520While%2520deep%2520learning%2520methods%2520have%2520shown%2520great%2520promise%2520in%2520improving%2520the%250Aeffectiveness%2520of%2520prostate%2520cancer%2520%2528PCa%2529%2520diagnosis%2520by%2520detecting%2520suspicious%250Alesions%2520from%2520trans-rectal%2520ultrasound%2520%2528TRUS%2529%252C%2520they%2520must%2520overcome%2520multiple%250Asimultaneous%2520challenges.%2520There%2520is%2520high%2520heterogeneity%2520in%2520tissue%2520appearance%252C%250Asignificant%2520class%2520imbalance%2520in%2520favor%2520of%2520benign%2520examples%252C%2520and%2520scarcity%2520in%2520the%250Anumber%2520and%2520quality%2520of%2520ground%2520truth%2520annotations%2520available%2520to%2520train%2520models.%250AFailure%2520to%2520address%2520even%2520a%2520single%2520one%2520of%2520these%2520problems%2520can%2520result%2520in%250Aunacceptable%2520clinical%2520outcomes.We%2520propose%2520TRUSWorthy%252C%2520a%2520carefully%2520designed%252C%250Atuned%252C%2520and%2520integrated%2520system%2520for%2520reliable%2520PCa%2520detection.%2520Our%2520pipeline%250Aintegrates%2520self-supervised%2520learning%252C%2520multiple-instance%2520learning%2520aggregation%250Ausing%2520transformers%252C%2520random-undersampled%2520boosting%2520and%2520ensembling%253A%2520these%2520address%250Alabel%2520scarcity%252C%2520weak%2520labels%252C%2520class%2520imbalance%252C%2520and%2520overconfidence%252C%2520respectively.%250AWe%2520train%2520and%2520rigorously%2520evaluate%2520our%2520method%2520using%2520a%2520large%252C%2520multi-center%2520dataset%250Aof%2520micro-ultrasound%2520data.%2520Our%2520method%2520outperforms%2520previous%2520state-of-the-art%2520deep%250Alearning%2520methods%2520in%2520terms%2520of%2520accuracy%2520and%2520uncertainty%2520calibration%252C%2520with%2520AUROC%250Aand%2520balanced%2520accuracy%2520scores%2520of%252079.9%2525%2520and%252071.5%2525%252C%2520respectively.%2520On%2520the%2520top%252020%2525%250Aof%2520predictions%2520with%2520the%2520highest%2520confidence%252C%2520we%2520can%2520achieve%2520a%2520balanced%2520accuracy%250Aof%2520up%2520to%252091%2525.%2520The%2520success%2520of%2520TRUSWorthy%2520demonstrates%2520the%2520potential%2520of%250Aintegrated%2520deep%2520learning%2520solutions%2520to%2520meet%2520clinical%2520needs%2520in%2520a%2520highly%250Achallenging%2520deployment%2520setting%252C%2520and%2520is%2520a%2520significant%2520step%2520towards%2520creating%2520a%250Atrustworthy%2520system%2520for%2520computer-assisted%2520PCa%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRUSWorthy%3A%20Toward%20Clinically%20Applicable%20Deep%20Learning%20for%20Confident%0A%20%20Detection%20of%20Prostate%20Cancer%20in%20Micro-Ultrasound&entry.906535625=Mohamed%20Harmanani%20and%20Paul%20F.%20R.%20Wilson%20and%20Minh%20Nguyen%20Nhat%20To%20and%20Mahdi%20Gilany%20and%20Amoon%20Jamzad%20and%20Fahimeh%20Fooladgar%20and%20Brian%20Wodlinger%20and%20Purang%20Abolmaesumi%20and%20Parvin%20Mousavi&entry.1292438233=%20%20While%20deep%20learning%20methods%20have%20shown%20great%20promise%20in%20improving%20the%0Aeffectiveness%20of%20prostate%20cancer%20%28PCa%29%20diagnosis%20by%20detecting%20suspicious%0Alesions%20from%20trans-rectal%20ultrasound%20%28TRUS%29%2C%20they%20must%20overcome%20multiple%0Asimultaneous%20challenges.%20There%20is%20high%20heterogeneity%20in%20tissue%20appearance%2C%0Asignificant%20class%20imbalance%20in%20favor%20of%20benign%20examples%2C%20and%20scarcity%20in%20the%0Anumber%20and%20quality%20of%20ground%20truth%20annotations%20available%20to%20train%20models.%0AFailure%20to%20address%20even%20a%20single%20one%20of%20these%20problems%20can%20result%20in%0Aunacceptable%20clinical%20outcomes.We%20propose%20TRUSWorthy%2C%20a%20carefully%20designed%2C%0Atuned%2C%20and%20integrated%20system%20for%20reliable%20PCa%20detection.%20Our%20pipeline%0Aintegrates%20self-supervised%20learning%2C%20multiple-instance%20learning%20aggregation%0Ausing%20transformers%2C%20random-undersampled%20boosting%20and%20ensembling%3A%20these%20address%0Alabel%20scarcity%2C%20weak%20labels%2C%20class%20imbalance%2C%20and%20overconfidence%2C%20respectively.%0AWe%20train%20and%20rigorously%20evaluate%20our%20method%20using%20a%20large%2C%20multi-center%20dataset%0Aof%20micro-ultrasound%20data.%20Our%20method%20outperforms%20previous%20state-of-the-art%20deep%0Alearning%20methods%20in%20terms%20of%20accuracy%20and%20uncertainty%20calibration%2C%20with%20AUROC%0Aand%20balanced%20accuracy%20scores%20of%2079.9%25%20and%2071.5%25%2C%20respectively.%20On%20the%20top%2020%25%0Aof%20predictions%20with%20the%20highest%20confidence%2C%20we%20can%20achieve%20a%20balanced%20accuracy%0Aof%20up%20to%2091%25.%20The%20success%20of%20TRUSWorthy%20demonstrates%20the%20potential%20of%0Aintegrated%20deep%20learning%20solutions%20to%20meet%20clinical%20needs%20in%20a%20highly%0Achallenging%20deployment%20setting%2C%20and%20is%20a%20significant%20step%20towards%20creating%20a%0Atrustworthy%20system%20for%20computer-assisted%20PCa%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14707v1&entry.124074799=Read"},
{"title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention", "author": "Shang Yang and Junxian Guo and Haotian Tang and Qinghao Hu and Guangxuan Xiao and Jiaming Tang and Yujun Lin and Zhijian Liu and Yao Lu and Song Han", "abstract": "  Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.\n", "link": "http://arxiv.org/abs/2502.14866v1", "date": "2025-02-20", "relevancy": 2.0899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LServe%3A%20Efficient%20Long-sequence%20LLM%20Serving%20with%20Unified%20Sparse%0A%20%20Attention&body=Title%3A%20LServe%3A%20Efficient%20Long-sequence%20LLM%20Serving%20with%20Unified%20Sparse%0A%20%20Attention%0AAuthor%3A%20Shang%20Yang%20and%20Junxian%20Guo%20and%20Haotian%20Tang%20and%20Qinghao%20Hu%20and%20Guangxuan%20Xiao%20and%20Jiaming%20Tang%20and%20Yujun%20Lin%20and%20Zhijian%20Liu%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20potential%20in%20processing%0Along%20sequences%2C%20yet%20efficiently%20serving%20these%20long-context%20models%20remains%0Achallenging%20due%20to%20the%20quadratic%20computational%20complexity%20of%20attention%20in%20the%0Aprefilling%20stage%20and%20the%20large%20memory%20footprint%20of%20the%20KV%20cache%20in%20the%20decoding%0Astage.%20To%20address%20these%20issues%2C%20we%20introduce%20LServe%2C%20an%20efficient%20system%20that%0Aaccelerates%20long-sequence%20LLM%20serving%20via%20hybrid%20sparse%20attention.%20This%20method%0Aunifies%20different%20hardware-friendly%2C%20structured%20sparsity%20patterns%20for%20both%0Aprefilling%20and%20decoding%20attention%20into%20a%20single%20framework%2C%20where%20computations%0Aon%20less%20important%20tokens%20are%20skipped%20block-wise.%20LServe%20demonstrates%20the%0Acompatibility%20of%20static%20and%20dynamic%20sparsity%20in%20long-context%20LLM%20attention.%0AThis%20design%20enables%20multiplicative%20speedups%20by%20combining%20these%20optimizations.%0ASpecifically%2C%20we%20convert%20half%20of%20the%20attention%20heads%20to%20nearly%20free%20streaming%0Aheads%20in%20both%20the%20prefilling%20and%20decoding%20stages.%20Additionally%2C%20we%20find%20that%0Aonly%20a%20constant%20number%20of%20KV%20pages%20is%20required%20to%20preserve%20long-context%0Acapabilities%2C%20irrespective%20of%20context%20length.%20We%20then%20design%20a%20hierarchical%20KV%0Apage%20selection%20policy%20that%20dynamically%20prunes%20KV%20pages%20based%20on%20query-centric%0Asimilarity.%20On%20average%2C%20LServe%20accelerates%20LLM%20prefilling%20by%20up%20to%202.9x%20and%0Adecoding%20by%201.3-2.1x%20over%20vLLM%2C%20maintaining%20long-context%20accuracy.%20Code%20is%0Areleased%20at%20https%3A//github.com/mit-han-lab/omniserve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLServe%253A%2520Efficient%2520Long-sequence%2520LLM%2520Serving%2520with%2520Unified%2520Sparse%250A%2520%2520Attention%26entry.906535625%3DShang%2520Yang%2520and%2520Junxian%2520Guo%2520and%2520Haotian%2520Tang%2520and%2520Qinghao%2520Hu%2520and%2520Guangxuan%2520Xiao%2520and%2520Jiaming%2520Tang%2520and%2520Yujun%2520Lin%2520and%2520Zhijian%2520Liu%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520potential%2520in%2520processing%250Along%2520sequences%252C%2520yet%2520efficiently%2520serving%2520these%2520long-context%2520models%2520remains%250Achallenging%2520due%2520to%2520the%2520quadratic%2520computational%2520complexity%2520of%2520attention%2520in%2520the%250Aprefilling%2520stage%2520and%2520the%2520large%2520memory%2520footprint%2520of%2520the%2520KV%2520cache%2520in%2520the%2520decoding%250Astage.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520LServe%252C%2520an%2520efficient%2520system%2520that%250Aaccelerates%2520long-sequence%2520LLM%2520serving%2520via%2520hybrid%2520sparse%2520attention.%2520This%2520method%250Aunifies%2520different%2520hardware-friendly%252C%2520structured%2520sparsity%2520patterns%2520for%2520both%250Aprefilling%2520and%2520decoding%2520attention%2520into%2520a%2520single%2520framework%252C%2520where%2520computations%250Aon%2520less%2520important%2520tokens%2520are%2520skipped%2520block-wise.%2520LServe%2520demonstrates%2520the%250Acompatibility%2520of%2520static%2520and%2520dynamic%2520sparsity%2520in%2520long-context%2520LLM%2520attention.%250AThis%2520design%2520enables%2520multiplicative%2520speedups%2520by%2520combining%2520these%2520optimizations.%250ASpecifically%252C%2520we%2520convert%2520half%2520of%2520the%2520attention%2520heads%2520to%2520nearly%2520free%2520streaming%250Aheads%2520in%2520both%2520the%2520prefilling%2520and%2520decoding%2520stages.%2520Additionally%252C%2520we%2520find%2520that%250Aonly%2520a%2520constant%2520number%2520of%2520KV%2520pages%2520is%2520required%2520to%2520preserve%2520long-context%250Acapabilities%252C%2520irrespective%2520of%2520context%2520length.%2520We%2520then%2520design%2520a%2520hierarchical%2520KV%250Apage%2520selection%2520policy%2520that%2520dynamically%2520prunes%2520KV%2520pages%2520based%2520on%2520query-centric%250Asimilarity.%2520On%2520average%252C%2520LServe%2520accelerates%2520LLM%2520prefilling%2520by%2520up%2520to%25202.9x%2520and%250Adecoding%2520by%25201.3-2.1x%2520over%2520vLLM%252C%2520maintaining%2520long-context%2520accuracy.%2520Code%2520is%250Areleased%2520at%2520https%253A//github.com/mit-han-lab/omniserve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LServe%3A%20Efficient%20Long-sequence%20LLM%20Serving%20with%20Unified%20Sparse%0A%20%20Attention&entry.906535625=Shang%20Yang%20and%20Junxian%20Guo%20and%20Haotian%20Tang%20and%20Qinghao%20Hu%20and%20Guangxuan%20Xiao%20and%20Jiaming%20Tang%20and%20Yujun%20Lin%20and%20Zhijian%20Liu%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20potential%20in%20processing%0Along%20sequences%2C%20yet%20efficiently%20serving%20these%20long-context%20models%20remains%0Achallenging%20due%20to%20the%20quadratic%20computational%20complexity%20of%20attention%20in%20the%0Aprefilling%20stage%20and%20the%20large%20memory%20footprint%20of%20the%20KV%20cache%20in%20the%20decoding%0Astage.%20To%20address%20these%20issues%2C%20we%20introduce%20LServe%2C%20an%20efficient%20system%20that%0Aaccelerates%20long-sequence%20LLM%20serving%20via%20hybrid%20sparse%20attention.%20This%20method%0Aunifies%20different%20hardware-friendly%2C%20structured%20sparsity%20patterns%20for%20both%0Aprefilling%20and%20decoding%20attention%20into%20a%20single%20framework%2C%20where%20computations%0Aon%20less%20important%20tokens%20are%20skipped%20block-wise.%20LServe%20demonstrates%20the%0Acompatibility%20of%20static%20and%20dynamic%20sparsity%20in%20long-context%20LLM%20attention.%0AThis%20design%20enables%20multiplicative%20speedups%20by%20combining%20these%20optimizations.%0ASpecifically%2C%20we%20convert%20half%20of%20the%20attention%20heads%20to%20nearly%20free%20streaming%0Aheads%20in%20both%20the%20prefilling%20and%20decoding%20stages.%20Additionally%2C%20we%20find%20that%0Aonly%20a%20constant%20number%20of%20KV%20pages%20is%20required%20to%20preserve%20long-context%0Acapabilities%2C%20irrespective%20of%20context%20length.%20We%20then%20design%20a%20hierarchical%20KV%0Apage%20selection%20policy%20that%20dynamically%20prunes%20KV%20pages%20based%20on%20query-centric%0Asimilarity.%20On%20average%2C%20LServe%20accelerates%20LLM%20prefilling%20by%20up%20to%202.9x%20and%0Adecoding%20by%201.3-2.1x%20over%20vLLM%2C%20maintaining%20long-context%20accuracy.%20Code%20is%0Areleased%20at%20https%3A//github.com/mit-han-lab/omniserve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14866v1&entry.124074799=Read"},
{"title": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality\n  Protein Backbone Generation", "author": "Angxiao Yue and Zichong Wang and Hongteng Xu", "abstract": "  Protein backbone generation plays a central role in de novo protein design\nand is significant for many biological and medical applications. Although\ndiffusion and flow-based generative models provide potential solutions to this\nchallenging task, they often generate proteins with undesired designability and\nsuffer computational inefficiency. In this study, we propose a novel rectified\nquaternion flow (ReQFlow) matching method for fast and high-quality protein\nbackbone generation. In particular, our method generates a local translation\nand a 3D rotation from random noise for each residue in a protein chain, which\nrepresents each 3D rotation as a unit quaternion and constructs its flow by\nspherical linear interpolation (SLERP) in an exponential format. We train the\nmodel by quaternion flow (QFlow) matching with guaranteed numerical stability\nand rectify the QFlow model to accelerate its inference and improve the\ndesignability of generated protein backbones, leading to the proposed ReQFlow\nmodel. Experiments show that ReQFlow achieves state-of-the-art performance in\nprotein backbone generation while requiring much fewer sampling steps and\nsignificantly less inference time (e.g., being 37x faster than RFDiffusion and\n62x faster than Genie2 when generating a backbone of length 300), demonstrating\nits effectiveness and efficiency. The code is available at\nhttps://github.com/AngxiaoYue/ReQFlow.\n", "link": "http://arxiv.org/abs/2502.14637v1", "date": "2025-02-20", "relevancy": 2.0823, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5695}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5298}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReQFlow%3A%20Rectified%20Quaternion%20Flow%20for%20Efficient%20and%20High-Quality%0A%20%20Protein%20Backbone%20Generation&body=Title%3A%20ReQFlow%3A%20Rectified%20Quaternion%20Flow%20for%20Efficient%20and%20High-Quality%0A%20%20Protein%20Backbone%20Generation%0AAuthor%3A%20Angxiao%20Yue%20and%20Zichong%20Wang%20and%20Hongteng%20Xu%0AAbstract%3A%20%20%20Protein%20backbone%20generation%20plays%20a%20central%20role%20in%20de%20novo%20protein%20design%0Aand%20is%20significant%20for%20many%20biological%20and%20medical%20applications.%20Although%0Adiffusion%20and%20flow-based%20generative%20models%20provide%20potential%20solutions%20to%20this%0Achallenging%20task%2C%20they%20often%20generate%20proteins%20with%20undesired%20designability%20and%0Asuffer%20computational%20inefficiency.%20In%20this%20study%2C%20we%20propose%20a%20novel%20rectified%0Aquaternion%20flow%20%28ReQFlow%29%20matching%20method%20for%20fast%20and%20high-quality%20protein%0Abackbone%20generation.%20In%20particular%2C%20our%20method%20generates%20a%20local%20translation%0Aand%20a%203D%20rotation%20from%20random%20noise%20for%20each%20residue%20in%20a%20protein%20chain%2C%20which%0Arepresents%20each%203D%20rotation%20as%20a%20unit%20quaternion%20and%20constructs%20its%20flow%20by%0Aspherical%20linear%20interpolation%20%28SLERP%29%20in%20an%20exponential%20format.%20We%20train%20the%0Amodel%20by%20quaternion%20flow%20%28QFlow%29%20matching%20with%20guaranteed%20numerical%20stability%0Aand%20rectify%20the%20QFlow%20model%20to%20accelerate%20its%20inference%20and%20improve%20the%0Adesignability%20of%20generated%20protein%20backbones%2C%20leading%20to%20the%20proposed%20ReQFlow%0Amodel.%20Experiments%20show%20that%20ReQFlow%20achieves%20state-of-the-art%20performance%20in%0Aprotein%20backbone%20generation%20while%20requiring%20much%20fewer%20sampling%20steps%20and%0Asignificantly%20less%20inference%20time%20%28e.g.%2C%20being%2037x%20faster%20than%20RFDiffusion%20and%0A62x%20faster%20than%20Genie2%20when%20generating%20a%20backbone%20of%20length%20300%29%2C%20demonstrating%0Aits%20effectiveness%20and%20efficiency.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AngxiaoYue/ReQFlow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReQFlow%253A%2520Rectified%2520Quaternion%2520Flow%2520for%2520Efficient%2520and%2520High-Quality%250A%2520%2520Protein%2520Backbone%2520Generation%26entry.906535625%3DAngxiao%2520Yue%2520and%2520Zichong%2520Wang%2520and%2520Hongteng%2520Xu%26entry.1292438233%3D%2520%2520Protein%2520backbone%2520generation%2520plays%2520a%2520central%2520role%2520in%2520de%2520novo%2520protein%2520design%250Aand%2520is%2520significant%2520for%2520many%2520biological%2520and%2520medical%2520applications.%2520Although%250Adiffusion%2520and%2520flow-based%2520generative%2520models%2520provide%2520potential%2520solutions%2520to%2520this%250Achallenging%2520task%252C%2520they%2520often%2520generate%2520proteins%2520with%2520undesired%2520designability%2520and%250Asuffer%2520computational%2520inefficiency.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520rectified%250Aquaternion%2520flow%2520%2528ReQFlow%2529%2520matching%2520method%2520for%2520fast%2520and%2520high-quality%2520protein%250Abackbone%2520generation.%2520In%2520particular%252C%2520our%2520method%2520generates%2520a%2520local%2520translation%250Aand%2520a%25203D%2520rotation%2520from%2520random%2520noise%2520for%2520each%2520residue%2520in%2520a%2520protein%2520chain%252C%2520which%250Arepresents%2520each%25203D%2520rotation%2520as%2520a%2520unit%2520quaternion%2520and%2520constructs%2520its%2520flow%2520by%250Aspherical%2520linear%2520interpolation%2520%2528SLERP%2529%2520in%2520an%2520exponential%2520format.%2520We%2520train%2520the%250Amodel%2520by%2520quaternion%2520flow%2520%2528QFlow%2529%2520matching%2520with%2520guaranteed%2520numerical%2520stability%250Aand%2520rectify%2520the%2520QFlow%2520model%2520to%2520accelerate%2520its%2520inference%2520and%2520improve%2520the%250Adesignability%2520of%2520generated%2520protein%2520backbones%252C%2520leading%2520to%2520the%2520proposed%2520ReQFlow%250Amodel.%2520Experiments%2520show%2520that%2520ReQFlow%2520achieves%2520state-of-the-art%2520performance%2520in%250Aprotein%2520backbone%2520generation%2520while%2520requiring%2520much%2520fewer%2520sampling%2520steps%2520and%250Asignificantly%2520less%2520inference%2520time%2520%2528e.g.%252C%2520being%252037x%2520faster%2520than%2520RFDiffusion%2520and%250A62x%2520faster%2520than%2520Genie2%2520when%2520generating%2520a%2520backbone%2520of%2520length%2520300%2529%252C%2520demonstrating%250Aits%2520effectiveness%2520and%2520efficiency.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AngxiaoYue/ReQFlow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReQFlow%3A%20Rectified%20Quaternion%20Flow%20for%20Efficient%20and%20High-Quality%0A%20%20Protein%20Backbone%20Generation&entry.906535625=Angxiao%20Yue%20and%20Zichong%20Wang%20and%20Hongteng%20Xu&entry.1292438233=%20%20Protein%20backbone%20generation%20plays%20a%20central%20role%20in%20de%20novo%20protein%20design%0Aand%20is%20significant%20for%20many%20biological%20and%20medical%20applications.%20Although%0Adiffusion%20and%20flow-based%20generative%20models%20provide%20potential%20solutions%20to%20this%0Achallenging%20task%2C%20they%20often%20generate%20proteins%20with%20undesired%20designability%20and%0Asuffer%20computational%20inefficiency.%20In%20this%20study%2C%20we%20propose%20a%20novel%20rectified%0Aquaternion%20flow%20%28ReQFlow%29%20matching%20method%20for%20fast%20and%20high-quality%20protein%0Abackbone%20generation.%20In%20particular%2C%20our%20method%20generates%20a%20local%20translation%0Aand%20a%203D%20rotation%20from%20random%20noise%20for%20each%20residue%20in%20a%20protein%20chain%2C%20which%0Arepresents%20each%203D%20rotation%20as%20a%20unit%20quaternion%20and%20constructs%20its%20flow%20by%0Aspherical%20linear%20interpolation%20%28SLERP%29%20in%20an%20exponential%20format.%20We%20train%20the%0Amodel%20by%20quaternion%20flow%20%28QFlow%29%20matching%20with%20guaranteed%20numerical%20stability%0Aand%20rectify%20the%20QFlow%20model%20to%20accelerate%20its%20inference%20and%20improve%20the%0Adesignability%20of%20generated%20protein%20backbones%2C%20leading%20to%20the%20proposed%20ReQFlow%0Amodel.%20Experiments%20show%20that%20ReQFlow%20achieves%20state-of-the-art%20performance%20in%0Aprotein%20backbone%20generation%20while%20requiring%20much%20fewer%20sampling%20steps%20and%0Asignificantly%20less%20inference%20time%20%28e.g.%2C%20being%2037x%20faster%20than%20RFDiffusion%20and%0A62x%20faster%20than%20Genie2%20when%20generating%20a%20backbone%20of%20length%20300%29%2C%20demonstrating%0Aits%20effectiveness%20and%20efficiency.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AngxiaoYue/ReQFlow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14637v1&entry.124074799=Read"},
{"title": "XLand-100B: A Large-Scale Multi-Task Dataset for In-Context\n  Reinforcement Learning", "author": "Alexander Nikulin and Ilya Zisman and Alexey Zemtsov and Vladislav Kurenkov", "abstract": "  Following the success of the in-context learning paradigm in large-scale\nlanguage and computer vision models, the recently emerging field of in-context\nreinforcement learning is experiencing a rapid growth. However, its development\nhas been held back by the lack of challenging benchmarks, as all the\nexperiments have been carried out in simple environments and on small-scale\ndatasets. We present XLand-100B, a large-scale dataset for in-context\nreinforcement learning based on the XLand-MiniGrid environment, as a first step\nto alleviate this problem. It contains complete learning histories for nearly\n$30,000$ different tasks, covering $100$B transitions and 2.5B episodes. It\ntook 50,000 GPU hours to collect the dataset, which is beyond the reach of most\nacademic labs. Along with the dataset, we provide the utilities to reproduce or\nexpand it even further. We also benchmark common in-context RL baselines and\nshow that they struggle to generalize to novel and diverse tasks. With this\nsubstantial effort, we aim to democratize research in the rapidly growing field\nof in-context reinforcement learning and provide a solid foundation for further\nscaling.\n", "link": "http://arxiv.org/abs/2406.08973v2", "date": "2025-02-20", "relevancy": 2.0813, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XLand-100B%3A%20A%20Large-Scale%20Multi-Task%20Dataset%20for%20In-Context%0A%20%20Reinforcement%20Learning&body=Title%3A%20XLand-100B%3A%20A%20Large-Scale%20Multi-Task%20Dataset%20for%20In-Context%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Alexey%20Zemtsov%20and%20Vladislav%20Kurenkov%0AAbstract%3A%20%20%20Following%20the%20success%20of%20the%20in-context%20learning%20paradigm%20in%20large-scale%0Alanguage%20and%20computer%20vision%20models%2C%20the%20recently%20emerging%20field%20of%20in-context%0Areinforcement%20learning%20is%20experiencing%20a%20rapid%20growth.%20However%2C%20its%20development%0Ahas%20been%20held%20back%20by%20the%20lack%20of%20challenging%20benchmarks%2C%20as%20all%20the%0Aexperiments%20have%20been%20carried%20out%20in%20simple%20environments%20and%20on%20small-scale%0Adatasets.%20We%20present%20XLand-100B%2C%20a%20large-scale%20dataset%20for%20in-context%0Areinforcement%20learning%20based%20on%20the%20XLand-MiniGrid%20environment%2C%20as%20a%20first%20step%0Ato%20alleviate%20this%20problem.%20It%20contains%20complete%20learning%20histories%20for%20nearly%0A%2430%2C000%24%20different%20tasks%2C%20covering%20%24100%24B%20transitions%20and%202.5B%20episodes.%20It%0Atook%2050%2C000%20GPU%20hours%20to%20collect%20the%20dataset%2C%20which%20is%20beyond%20the%20reach%20of%20most%0Aacademic%20labs.%20Along%20with%20the%20dataset%2C%20we%20provide%20the%20utilities%20to%20reproduce%20or%0Aexpand%20it%20even%20further.%20We%20also%20benchmark%20common%20in-context%20RL%20baselines%20and%0Ashow%20that%20they%20struggle%20to%20generalize%20to%20novel%20and%20diverse%20tasks.%20With%20this%0Asubstantial%20effort%2C%20we%20aim%20to%20democratize%20research%20in%20the%20rapidly%20growing%20field%0Aof%20in-context%20reinforcement%20learning%20and%20provide%20a%20solid%20foundation%20for%20further%0Ascaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08973v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXLand-100B%253A%2520A%2520Large-Scale%2520Multi-Task%2520Dataset%2520for%2520In-Context%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAlexander%2520Nikulin%2520and%2520Ilya%2520Zisman%2520and%2520Alexey%2520Zemtsov%2520and%2520Vladislav%2520Kurenkov%26entry.1292438233%3D%2520%2520Following%2520the%2520success%2520of%2520the%2520in-context%2520learning%2520paradigm%2520in%2520large-scale%250Alanguage%2520and%2520computer%2520vision%2520models%252C%2520the%2520recently%2520emerging%2520field%2520of%2520in-context%250Areinforcement%2520learning%2520is%2520experiencing%2520a%2520rapid%2520growth.%2520However%252C%2520its%2520development%250Ahas%2520been%2520held%2520back%2520by%2520the%2520lack%2520of%2520challenging%2520benchmarks%252C%2520as%2520all%2520the%250Aexperiments%2520have%2520been%2520carried%2520out%2520in%2520simple%2520environments%2520and%2520on%2520small-scale%250Adatasets.%2520We%2520present%2520XLand-100B%252C%2520a%2520large-scale%2520dataset%2520for%2520in-context%250Areinforcement%2520learning%2520based%2520on%2520the%2520XLand-MiniGrid%2520environment%252C%2520as%2520a%2520first%2520step%250Ato%2520alleviate%2520this%2520problem.%2520It%2520contains%2520complete%2520learning%2520histories%2520for%2520nearly%250A%252430%252C000%2524%2520different%2520tasks%252C%2520covering%2520%2524100%2524B%2520transitions%2520and%25202.5B%2520episodes.%2520It%250Atook%252050%252C000%2520GPU%2520hours%2520to%2520collect%2520the%2520dataset%252C%2520which%2520is%2520beyond%2520the%2520reach%2520of%2520most%250Aacademic%2520labs.%2520Along%2520with%2520the%2520dataset%252C%2520we%2520provide%2520the%2520utilities%2520to%2520reproduce%2520or%250Aexpand%2520it%2520even%2520further.%2520We%2520also%2520benchmark%2520common%2520in-context%2520RL%2520baselines%2520and%250Ashow%2520that%2520they%2520struggle%2520to%2520generalize%2520to%2520novel%2520and%2520diverse%2520tasks.%2520With%2520this%250Asubstantial%2520effort%252C%2520we%2520aim%2520to%2520democratize%2520research%2520in%2520the%2520rapidly%2520growing%2520field%250Aof%2520in-context%2520reinforcement%2520learning%2520and%2520provide%2520a%2520solid%2520foundation%2520for%2520further%250Ascaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08973v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XLand-100B%3A%20A%20Large-Scale%20Multi-Task%20Dataset%20for%20In-Context%0A%20%20Reinforcement%20Learning&entry.906535625=Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Alexey%20Zemtsov%20and%20Vladislav%20Kurenkov&entry.1292438233=%20%20Following%20the%20success%20of%20the%20in-context%20learning%20paradigm%20in%20large-scale%0Alanguage%20and%20computer%20vision%20models%2C%20the%20recently%20emerging%20field%20of%20in-context%0Areinforcement%20learning%20is%20experiencing%20a%20rapid%20growth.%20However%2C%20its%20development%0Ahas%20been%20held%20back%20by%20the%20lack%20of%20challenging%20benchmarks%2C%20as%20all%20the%0Aexperiments%20have%20been%20carried%20out%20in%20simple%20environments%20and%20on%20small-scale%0Adatasets.%20We%20present%20XLand-100B%2C%20a%20large-scale%20dataset%20for%20in-context%0Areinforcement%20learning%20based%20on%20the%20XLand-MiniGrid%20environment%2C%20as%20a%20first%20step%0Ato%20alleviate%20this%20problem.%20It%20contains%20complete%20learning%20histories%20for%20nearly%0A%2430%2C000%24%20different%20tasks%2C%20covering%20%24100%24B%20transitions%20and%202.5B%20episodes.%20It%0Atook%2050%2C000%20GPU%20hours%20to%20collect%20the%20dataset%2C%20which%20is%20beyond%20the%20reach%20of%20most%0Aacademic%20labs.%20Along%20with%20the%20dataset%2C%20we%20provide%20the%20utilities%20to%20reproduce%20or%0Aexpand%20it%20even%20further.%20We%20also%20benchmark%20common%20in-context%20RL%20baselines%20and%0Ashow%20that%20they%20struggle%20to%20generalize%20to%20novel%20and%20diverse%20tasks.%20With%20this%0Asubstantial%20effort%2C%20we%20aim%20to%20democratize%20research%20in%20the%20rapidly%20growing%20field%0Aof%20in-context%20reinforcement%20learning%20and%20provide%20a%20solid%20foundation%20for%20further%0Ascaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08973v2&entry.124074799=Read"},
{"title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs", "author": "Tao Ji and Bin Guo and Yuanbin Wu and Qipeng Guo and Lixing Shen and Zhan Chen and Xipeng Qiu and Qi Zhang and Tao Gui", "abstract": "  Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.\n", "link": "http://arxiv.org/abs/2502.14837v1", "date": "2025-02-20", "relevancy": 2.0812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Economical%20Inference%3A%20Enabling%20DeepSeek%27s%20Multi-Head%20Latent%0A%20%20Attention%20in%20Any%20Transformer-based%20LLMs&body=Title%3A%20Towards%20Economical%20Inference%3A%20Enabling%20DeepSeek%27s%20Multi-Head%20Latent%0A%20%20Attention%20in%20Any%20Transformer-based%20LLMs%0AAuthor%3A%20Tao%20Ji%20and%20Bin%20Guo%20and%20Yuanbin%20Wu%20and%20Qipeng%20Guo%20and%20Lixing%20Shen%20and%20Zhan%20Chen%20and%20Xipeng%20Qiu%20and%20Qi%20Zhang%20and%20Tao%20Gui%0AAbstract%3A%20%20%20Multi-head%20Latent%20Attention%20%28MLA%29%20is%20an%20innovative%20architecture%20proposed%20by%0ADeepSeek%2C%20designed%20to%20ensure%20efficient%20and%20economical%20inference%20by%0Asignificantly%20compressing%20the%20Key-Value%20%28KV%29%20cache%20into%20a%20latent%20vector.%0ACompared%20to%20MLA%2C%20standard%20LLMs%20employing%20Multi-Head%20Attention%20%28MHA%29%20and%20its%0Avariants%20such%20as%20Grouped-Query%20Attention%20%28GQA%29%20exhibit%20significant%20cost%0Adisadvantages.%20Enabling%20well-trained%20LLMs%20%28e.g.%2C%20Llama%29%20to%20rapidly%20adapt%20to%20MLA%0Awithout%20pre-training%20from%20scratch%20is%20both%20meaningful%20and%20challenging.%20This%0Apaper%20proposes%20the%20first%20data-efficient%20fine-tuning%20method%20for%20transitioning%0Afrom%20MHA%20to%20MLA%20%28MHA2MLA%29%2C%20which%20includes%20two%20key%20components%3A%20for%20partial-RoPE%2C%0Awe%20remove%20RoPE%20from%20dimensions%20of%20queries%20and%20keys%20that%20contribute%20less%20to%20the%0Aattention%20scores%2C%20for%20low-rank%20approximation%2C%20we%20introduce%20joint%20SVD%0Aapproximations%20based%20on%20the%20pre-trained%20parameters%20of%20keys%20and%20values.%20These%0Acarefully%20designed%20strategies%20enable%20MHA2MLA%20to%20recover%20performance%20using%20only%0Aa%20small%20fraction%20%280.3%25%20to%200.6%25%29%20of%20the%20data%2C%20significantly%20reducing%20inference%0Acosts%20while%20seamlessly%20integrating%20with%20compression%20techniques%20such%20as%20KV%20cache%0Aquantization.%20For%20example%2C%20the%20KV%20cache%20size%20of%20Llama2-7B%20is%20reduced%20by%2092.19%25%2C%0Awith%20only%20a%200.5%25%20drop%20in%20LongBench%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Economical%2520Inference%253A%2520Enabling%2520DeepSeek%2527s%2520Multi-Head%2520Latent%250A%2520%2520Attention%2520in%2520Any%2520Transformer-based%2520LLMs%26entry.906535625%3DTao%2520Ji%2520and%2520Bin%2520Guo%2520and%2520Yuanbin%2520Wu%2520and%2520Qipeng%2520Guo%2520and%2520Lixing%2520Shen%2520and%2520Zhan%2520Chen%2520and%2520Xipeng%2520Qiu%2520and%2520Qi%2520Zhang%2520and%2520Tao%2520Gui%26entry.1292438233%3D%2520%2520Multi-head%2520Latent%2520Attention%2520%2528MLA%2529%2520is%2520an%2520innovative%2520architecture%2520proposed%2520by%250ADeepSeek%252C%2520designed%2520to%2520ensure%2520efficient%2520and%2520economical%2520inference%2520by%250Asignificantly%2520compressing%2520the%2520Key-Value%2520%2528KV%2529%2520cache%2520into%2520a%2520latent%2520vector.%250ACompared%2520to%2520MLA%252C%2520standard%2520LLMs%2520employing%2520Multi-Head%2520Attention%2520%2528MHA%2529%2520and%2520its%250Avariants%2520such%2520as%2520Grouped-Query%2520Attention%2520%2528GQA%2529%2520exhibit%2520significant%2520cost%250Adisadvantages.%2520Enabling%2520well-trained%2520LLMs%2520%2528e.g.%252C%2520Llama%2529%2520to%2520rapidly%2520adapt%2520to%2520MLA%250Awithout%2520pre-training%2520from%2520scratch%2520is%2520both%2520meaningful%2520and%2520challenging.%2520This%250Apaper%2520proposes%2520the%2520first%2520data-efficient%2520fine-tuning%2520method%2520for%2520transitioning%250Afrom%2520MHA%2520to%2520MLA%2520%2528MHA2MLA%2529%252C%2520which%2520includes%2520two%2520key%2520components%253A%2520for%2520partial-RoPE%252C%250Awe%2520remove%2520RoPE%2520from%2520dimensions%2520of%2520queries%2520and%2520keys%2520that%2520contribute%2520less%2520to%2520the%250Aattention%2520scores%252C%2520for%2520low-rank%2520approximation%252C%2520we%2520introduce%2520joint%2520SVD%250Aapproximations%2520based%2520on%2520the%2520pre-trained%2520parameters%2520of%2520keys%2520and%2520values.%2520These%250Acarefully%2520designed%2520strategies%2520enable%2520MHA2MLA%2520to%2520recover%2520performance%2520using%2520only%250Aa%2520small%2520fraction%2520%25280.3%2525%2520to%25200.6%2525%2529%2520of%2520the%2520data%252C%2520significantly%2520reducing%2520inference%250Acosts%2520while%2520seamlessly%2520integrating%2520with%2520compression%2520techniques%2520such%2520as%2520KV%2520cache%250Aquantization.%2520For%2520example%252C%2520the%2520KV%2520cache%2520size%2520of%2520Llama2-7B%2520is%2520reduced%2520by%252092.19%2525%252C%250Awith%2520only%2520a%25200.5%2525%2520drop%2520in%2520LongBench%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Economical%20Inference%3A%20Enabling%20DeepSeek%27s%20Multi-Head%20Latent%0A%20%20Attention%20in%20Any%20Transformer-based%20LLMs&entry.906535625=Tao%20Ji%20and%20Bin%20Guo%20and%20Yuanbin%20Wu%20and%20Qipeng%20Guo%20and%20Lixing%20Shen%20and%20Zhan%20Chen%20and%20Xipeng%20Qiu%20and%20Qi%20Zhang%20and%20Tao%20Gui&entry.1292438233=%20%20Multi-head%20Latent%20Attention%20%28MLA%29%20is%20an%20innovative%20architecture%20proposed%20by%0ADeepSeek%2C%20designed%20to%20ensure%20efficient%20and%20economical%20inference%20by%0Asignificantly%20compressing%20the%20Key-Value%20%28KV%29%20cache%20into%20a%20latent%20vector.%0ACompared%20to%20MLA%2C%20standard%20LLMs%20employing%20Multi-Head%20Attention%20%28MHA%29%20and%20its%0Avariants%20such%20as%20Grouped-Query%20Attention%20%28GQA%29%20exhibit%20significant%20cost%0Adisadvantages.%20Enabling%20well-trained%20LLMs%20%28e.g.%2C%20Llama%29%20to%20rapidly%20adapt%20to%20MLA%0Awithout%20pre-training%20from%20scratch%20is%20both%20meaningful%20and%20challenging.%20This%0Apaper%20proposes%20the%20first%20data-efficient%20fine-tuning%20method%20for%20transitioning%0Afrom%20MHA%20to%20MLA%20%28MHA2MLA%29%2C%20which%20includes%20two%20key%20components%3A%20for%20partial-RoPE%2C%0Awe%20remove%20RoPE%20from%20dimensions%20of%20queries%20and%20keys%20that%20contribute%20less%20to%20the%0Aattention%20scores%2C%20for%20low-rank%20approximation%2C%20we%20introduce%20joint%20SVD%0Aapproximations%20based%20on%20the%20pre-trained%20parameters%20of%20keys%20and%20values.%20These%0Acarefully%20designed%20strategies%20enable%20MHA2MLA%20to%20recover%20performance%20using%20only%0Aa%20small%20fraction%20%280.3%25%20to%200.6%25%29%20of%20the%20data%2C%20significantly%20reducing%20inference%0Acosts%20while%20seamlessly%20integrating%20with%20compression%20techniques%20such%20as%20KV%20cache%0Aquantization.%20For%20example%2C%20the%20KV%20cache%20size%20of%20Llama2-7B%20is%20reduced%20by%2092.19%25%2C%0Awith%20only%20a%200.5%25%20drop%20in%20LongBench%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14837v1&entry.124074799=Read"},
{"title": "metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large\n  Language Models", "author": "Alex Kipnis and Konstantinos Voudouris and Luca M. Schulze Buschoff and Eric Schulz", "abstract": "  Large Language Models (LLMs) vary in their abilities on a range of tasks.\nInitiatives such as the Open LLM Leaderboard aim to quantify these differences\nwith several large benchmarks (sets of test items to which an LLM can respond\neither correctly or incorrectly). However, high correlations within and between\nbenchmark scores suggest that (1) there exists a small set of common underlying\nabilities that these benchmarks measure, and (2) items tap into redundant\ninformation and the benchmarks may thus be considerably compressed. We use data\nfrom n > 5000 LLMs to identify the most informative items of six benchmarks,\nARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28,632 items\nin total). From them we distill a sparse benchmark, metabench, that has less\nthan 3% of the original size of all six benchmarks combined. This new sparse\nbenchmark goes beyond point scores by yielding estimators of the underlying\nbenchmark-specific abilities. We show that these estimators (1) can be used to\nreconstruct each original individual benchmark score with, on average, 1.24%\nroot mean square error (RMSE), (2) reconstruct the original total score with\n0.58% RMSE, and (3) have a single underlying common factor whose Spearman\ncorrelation with the total score is r = 0.94.\n", "link": "http://arxiv.org/abs/2407.12844v2", "date": "2025-02-20", "relevancy": 2.0773, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20metabench%20--%20A%20Sparse%20Benchmark%20of%20Reasoning%20and%20Knowledge%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20metabench%20--%20A%20Sparse%20Benchmark%20of%20Reasoning%20and%20Knowledge%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Alex%20Kipnis%20and%20Konstantinos%20Voudouris%20and%20Luca%20M.%20Schulze%20Buschoff%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20vary%20in%20their%20abilities%20on%20a%20range%20of%20tasks.%0AInitiatives%20such%20as%20the%20Open%20LLM%20Leaderboard%20aim%20to%20quantify%20these%20differences%0Awith%20several%20large%20benchmarks%20%28sets%20of%20test%20items%20to%20which%20an%20LLM%20can%20respond%0Aeither%20correctly%20or%20incorrectly%29.%20However%2C%20high%20correlations%20within%20and%20between%0Abenchmark%20scores%20suggest%20that%20%281%29%20there%20exists%20a%20small%20set%20of%20common%20underlying%0Aabilities%20that%20these%20benchmarks%20measure%2C%20and%20%282%29%20items%20tap%20into%20redundant%0Ainformation%20and%20the%20benchmarks%20may%20thus%20be%20considerably%20compressed.%20We%20use%20data%0Afrom%20n%20%3E%205000%20LLMs%20to%20identify%20the%20most%20informative%20items%20of%20six%20benchmarks%2C%0AARC%2C%20GSM8K%2C%20HellaSwag%2C%20MMLU%2C%20TruthfulQA%20and%20WinoGrande%20%28with%20d%20%3D%2028%2C632%20items%0Ain%20total%29.%20From%20them%20we%20distill%20a%20sparse%20benchmark%2C%20metabench%2C%20that%20has%20less%0Athan%203%25%20of%20the%20original%20size%20of%20all%20six%20benchmarks%20combined.%20This%20new%20sparse%0Abenchmark%20goes%20beyond%20point%20scores%20by%20yielding%20estimators%20of%20the%20underlying%0Abenchmark-specific%20abilities.%20We%20show%20that%20these%20estimators%20%281%29%20can%20be%20used%20to%0Areconstruct%20each%20original%20individual%20benchmark%20score%20with%2C%20on%20average%2C%201.24%25%0Aroot%20mean%20square%20error%20%28RMSE%29%2C%20%282%29%20reconstruct%20the%20original%20total%20score%20with%0A0.58%25%20RMSE%2C%20and%20%283%29%20have%20a%20single%20underlying%20common%20factor%20whose%20Spearman%0Acorrelation%20with%20the%20total%20score%20is%20r%20%3D%200.94.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dmetabench%2520--%2520A%2520Sparse%2520Benchmark%2520of%2520Reasoning%2520and%2520Knowledge%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DAlex%2520Kipnis%2520and%2520Konstantinos%2520Voudouris%2520and%2520Luca%2520M.%2520Schulze%2520Buschoff%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520vary%2520in%2520their%2520abilities%2520on%2520a%2520range%2520of%2520tasks.%250AInitiatives%2520such%2520as%2520the%2520Open%2520LLM%2520Leaderboard%2520aim%2520to%2520quantify%2520these%2520differences%250Awith%2520several%2520large%2520benchmarks%2520%2528sets%2520of%2520test%2520items%2520to%2520which%2520an%2520LLM%2520can%2520respond%250Aeither%2520correctly%2520or%2520incorrectly%2529.%2520However%252C%2520high%2520correlations%2520within%2520and%2520between%250Abenchmark%2520scores%2520suggest%2520that%2520%25281%2529%2520there%2520exists%2520a%2520small%2520set%2520of%2520common%2520underlying%250Aabilities%2520that%2520these%2520benchmarks%2520measure%252C%2520and%2520%25282%2529%2520items%2520tap%2520into%2520redundant%250Ainformation%2520and%2520the%2520benchmarks%2520may%2520thus%2520be%2520considerably%2520compressed.%2520We%2520use%2520data%250Afrom%2520n%2520%253E%25205000%2520LLMs%2520to%2520identify%2520the%2520most%2520informative%2520items%2520of%2520six%2520benchmarks%252C%250AARC%252C%2520GSM8K%252C%2520HellaSwag%252C%2520MMLU%252C%2520TruthfulQA%2520and%2520WinoGrande%2520%2528with%2520d%2520%253D%252028%252C632%2520items%250Ain%2520total%2529.%2520From%2520them%2520we%2520distill%2520a%2520sparse%2520benchmark%252C%2520metabench%252C%2520that%2520has%2520less%250Athan%25203%2525%2520of%2520the%2520original%2520size%2520of%2520all%2520six%2520benchmarks%2520combined.%2520This%2520new%2520sparse%250Abenchmark%2520goes%2520beyond%2520point%2520scores%2520by%2520yielding%2520estimators%2520of%2520the%2520underlying%250Abenchmark-specific%2520abilities.%2520We%2520show%2520that%2520these%2520estimators%2520%25281%2529%2520can%2520be%2520used%2520to%250Areconstruct%2520each%2520original%2520individual%2520benchmark%2520score%2520with%252C%2520on%2520average%252C%25201.24%2525%250Aroot%2520mean%2520square%2520error%2520%2528RMSE%2529%252C%2520%25282%2529%2520reconstruct%2520the%2520original%2520total%2520score%2520with%250A0.58%2525%2520RMSE%252C%2520and%2520%25283%2529%2520have%2520a%2520single%2520underlying%2520common%2520factor%2520whose%2520Spearman%250Acorrelation%2520with%2520the%2520total%2520score%2520is%2520r%2520%253D%25200.94.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=metabench%20--%20A%20Sparse%20Benchmark%20of%20Reasoning%20and%20Knowledge%20in%20Large%0A%20%20Language%20Models&entry.906535625=Alex%20Kipnis%20and%20Konstantinos%20Voudouris%20and%20Luca%20M.%20Schulze%20Buschoff%20and%20Eric%20Schulz&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20vary%20in%20their%20abilities%20on%20a%20range%20of%20tasks.%0AInitiatives%20such%20as%20the%20Open%20LLM%20Leaderboard%20aim%20to%20quantify%20these%20differences%0Awith%20several%20large%20benchmarks%20%28sets%20of%20test%20items%20to%20which%20an%20LLM%20can%20respond%0Aeither%20correctly%20or%20incorrectly%29.%20However%2C%20high%20correlations%20within%20and%20between%0Abenchmark%20scores%20suggest%20that%20%281%29%20there%20exists%20a%20small%20set%20of%20common%20underlying%0Aabilities%20that%20these%20benchmarks%20measure%2C%20and%20%282%29%20items%20tap%20into%20redundant%0Ainformation%20and%20the%20benchmarks%20may%20thus%20be%20considerably%20compressed.%20We%20use%20data%0Afrom%20n%20%3E%205000%20LLMs%20to%20identify%20the%20most%20informative%20items%20of%20six%20benchmarks%2C%0AARC%2C%20GSM8K%2C%20HellaSwag%2C%20MMLU%2C%20TruthfulQA%20and%20WinoGrande%20%28with%20d%20%3D%2028%2C632%20items%0Ain%20total%29.%20From%20them%20we%20distill%20a%20sparse%20benchmark%2C%20metabench%2C%20that%20has%20less%0Athan%203%25%20of%20the%20original%20size%20of%20all%20six%20benchmarks%20combined.%20This%20new%20sparse%0Abenchmark%20goes%20beyond%20point%20scores%20by%20yielding%20estimators%20of%20the%20underlying%0Abenchmark-specific%20abilities.%20We%20show%20that%20these%20estimators%20%281%29%20can%20be%20used%20to%0Areconstruct%20each%20original%20individual%20benchmark%20score%20with%2C%20on%20average%2C%201.24%25%0Aroot%20mean%20square%20error%20%28RMSE%29%2C%20%282%29%20reconstruct%20the%20original%20total%20score%20with%0A0.58%25%20RMSE%2C%20and%20%283%29%20have%20a%20single%20underlying%20common%20factor%20whose%20Spearman%0Acorrelation%20with%20the%20total%20score%20is%20r%20%3D%200.94.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12844v2&entry.124074799=Read"},
{"title": "A Theory for Conditional Generative Modeling on Multiple Data Sources", "author": "Rongzhen Wang and Yan Zhang and Chenyu Zheng and Chongxuan Li and Guoqiang Wu", "abstract": "  The success of large generative models has driven a paradigm shift,\nleveraging massive multi-source data to enhance model capabilities. However,\nthe interaction among these sources remains theoretically underexplored. This\npaper takes the first step toward a rigorous analysis of multi-source training\nin conditional generative modeling, where each condition represents a distinct\ndata source. Specifically, we establish a general distribution estimation error\nbound in average total variation distance for conditional maximum likelihood\nestimation based on the bracketing number. Our result shows that when source\ndistributions share certain similarities and the model is expressive enough,\nmulti-source training guarantees a sharper bound than single-source training.\nWe further instantiate the general theory on conditional Gaussian estimation\nand deep generative models including autoregressive and flexible energy-based\nmodels, by characterizing their bracketing numbers. The results highlight that\nthe number of sources and similarity among source distributions improve the\nadvantage of multi-source training. Simulations and real-world experiments\nvalidate our theory. Code is available at:\n\\url{https://github.com/ML-GSAI/Multi-Source-GM}.\n", "link": "http://arxiv.org/abs/2502.14583v1", "date": "2025-02-20", "relevancy": 2.0735, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5706}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5093}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theory%20for%20Conditional%20Generative%20Modeling%20on%20Multiple%20Data%20Sources&body=Title%3A%20A%20Theory%20for%20Conditional%20Generative%20Modeling%20on%20Multiple%20Data%20Sources%0AAuthor%3A%20Rongzhen%20Wang%20and%20Yan%20Zhang%20and%20Chenyu%20Zheng%20and%20Chongxuan%20Li%20and%20Guoqiang%20Wu%0AAbstract%3A%20%20%20The%20success%20of%20large%20generative%20models%20has%20driven%20a%20paradigm%20shift%2C%0Aleveraging%20massive%20multi-source%20data%20to%20enhance%20model%20capabilities.%20However%2C%0Athe%20interaction%20among%20these%20sources%20remains%20theoretically%20underexplored.%20This%0Apaper%20takes%20the%20first%20step%20toward%20a%20rigorous%20analysis%20of%20multi-source%20training%0Ain%20conditional%20generative%20modeling%2C%20where%20each%20condition%20represents%20a%20distinct%0Adata%20source.%20Specifically%2C%20we%20establish%20a%20general%20distribution%20estimation%20error%0Abound%20in%20average%20total%20variation%20distance%20for%20conditional%20maximum%20likelihood%0Aestimation%20based%20on%20the%20bracketing%20number.%20Our%20result%20shows%20that%20when%20source%0Adistributions%20share%20certain%20similarities%20and%20the%20model%20is%20expressive%20enough%2C%0Amulti-source%20training%20guarantees%20a%20sharper%20bound%20than%20single-source%20training.%0AWe%20further%20instantiate%20the%20general%20theory%20on%20conditional%20Gaussian%20estimation%0Aand%20deep%20generative%20models%20including%20autoregressive%20and%20flexible%20energy-based%0Amodels%2C%20by%20characterizing%20their%20bracketing%20numbers.%20The%20results%20highlight%20that%0Athe%20number%20of%20sources%20and%20similarity%20among%20source%20distributions%20improve%20the%0Aadvantage%20of%20multi-source%20training.%20Simulations%20and%20real-world%20experiments%0Avalidate%20our%20theory.%20Code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/ML-GSAI/Multi-Source-GM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theory%2520for%2520Conditional%2520Generative%2520Modeling%2520on%2520Multiple%2520Data%2520Sources%26entry.906535625%3DRongzhen%2520Wang%2520and%2520Yan%2520Zhang%2520and%2520Chenyu%2520Zheng%2520and%2520Chongxuan%2520Li%2520and%2520Guoqiang%2520Wu%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520large%2520generative%2520models%2520has%2520driven%2520a%2520paradigm%2520shift%252C%250Aleveraging%2520massive%2520multi-source%2520data%2520to%2520enhance%2520model%2520capabilities.%2520However%252C%250Athe%2520interaction%2520among%2520these%2520sources%2520remains%2520theoretically%2520underexplored.%2520This%250Apaper%2520takes%2520the%2520first%2520step%2520toward%2520a%2520rigorous%2520analysis%2520of%2520multi-source%2520training%250Ain%2520conditional%2520generative%2520modeling%252C%2520where%2520each%2520condition%2520represents%2520a%2520distinct%250Adata%2520source.%2520Specifically%252C%2520we%2520establish%2520a%2520general%2520distribution%2520estimation%2520error%250Abound%2520in%2520average%2520total%2520variation%2520distance%2520for%2520conditional%2520maximum%2520likelihood%250Aestimation%2520based%2520on%2520the%2520bracketing%2520number.%2520Our%2520result%2520shows%2520that%2520when%2520source%250Adistributions%2520share%2520certain%2520similarities%2520and%2520the%2520model%2520is%2520expressive%2520enough%252C%250Amulti-source%2520training%2520guarantees%2520a%2520sharper%2520bound%2520than%2520single-source%2520training.%250AWe%2520further%2520instantiate%2520the%2520general%2520theory%2520on%2520conditional%2520Gaussian%2520estimation%250Aand%2520deep%2520generative%2520models%2520including%2520autoregressive%2520and%2520flexible%2520energy-based%250Amodels%252C%2520by%2520characterizing%2520their%2520bracketing%2520numbers.%2520The%2520results%2520highlight%2520that%250Athe%2520number%2520of%2520sources%2520and%2520similarity%2520among%2520source%2520distributions%2520improve%2520the%250Aadvantage%2520of%2520multi-source%2520training.%2520Simulations%2520and%2520real-world%2520experiments%250Avalidate%2520our%2520theory.%2520Code%2520is%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/ML-GSAI/Multi-Source-GM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theory%20for%20Conditional%20Generative%20Modeling%20on%20Multiple%20Data%20Sources&entry.906535625=Rongzhen%20Wang%20and%20Yan%20Zhang%20and%20Chenyu%20Zheng%20and%20Chongxuan%20Li%20and%20Guoqiang%20Wu&entry.1292438233=%20%20The%20success%20of%20large%20generative%20models%20has%20driven%20a%20paradigm%20shift%2C%0Aleveraging%20massive%20multi-source%20data%20to%20enhance%20model%20capabilities.%20However%2C%0Athe%20interaction%20among%20these%20sources%20remains%20theoretically%20underexplored.%20This%0Apaper%20takes%20the%20first%20step%20toward%20a%20rigorous%20analysis%20of%20multi-source%20training%0Ain%20conditional%20generative%20modeling%2C%20where%20each%20condition%20represents%20a%20distinct%0Adata%20source.%20Specifically%2C%20we%20establish%20a%20general%20distribution%20estimation%20error%0Abound%20in%20average%20total%20variation%20distance%20for%20conditional%20maximum%20likelihood%0Aestimation%20based%20on%20the%20bracketing%20number.%20Our%20result%20shows%20that%20when%20source%0Adistributions%20share%20certain%20similarities%20and%20the%20model%20is%20expressive%20enough%2C%0Amulti-source%20training%20guarantees%20a%20sharper%20bound%20than%20single-source%20training.%0AWe%20further%20instantiate%20the%20general%20theory%20on%20conditional%20Gaussian%20estimation%0Aand%20deep%20generative%20models%20including%20autoregressive%20and%20flexible%20energy-based%0Amodels%2C%20by%20characterizing%20their%20bracketing%20numbers.%20The%20results%20highlight%20that%0Athe%20number%20of%20sources%20and%20similarity%20among%20source%20distributions%20improve%20the%0Aadvantage%20of%20multi-source%20training.%20Simulations%20and%20real-world%20experiments%0Avalidate%20our%20theory.%20Code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/ML-GSAI/Multi-Source-GM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14583v1&entry.124074799=Read"},
{"title": "Learned Image Transmission with Hierarchical Variational Autoencoder", "author": "Guangyi Zhang and Hanlei Li and Yunlong Cai and Qiyu Hu and Guanding Yu and Runmin Zhang", "abstract": "  In this paper, we introduce an innovative hierarchical joint source-channel\ncoding (HJSCC) framework for image transmission, utilizing a hierarchical\nvariational autoencoder (VAE). Our approach leverages a combination of\nbottom-up and top-down paths at the transmitter to autoregressively generate\nmultiple hierarchical representations of the original image. These\nrepresentations are then directly mapped to channel symbols for transmission by\nthe JSCC encoder. We extend this framework to scenarios with a feedback link,\nmodeling transmission over a noisy channel as a probabilistic sampling process\nand deriving a novel generative formulation for JSCC with feedback. Compared\nwith existing approaches, our proposed HJSCC provides enhanced adaptability by\ndynamically adjusting transmission bandwidth, encoding these representations\ninto varying amounts of channel symbols. Extensive experiments on images of\nvarying resolutions demonstrate that our proposed model outperforms existing\nbaselines in rate-distortion performance and maintains robustness against\nchannel noise. The source code will be made available upon acceptance.\n", "link": "http://arxiv.org/abs/2408.16340v4", "date": "2025-02-20", "relevancy": 2.0696, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.533}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5132}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Image%20Transmission%20with%20Hierarchical%20Variational%20Autoencoder&body=Title%3A%20Learned%20Image%20Transmission%20with%20Hierarchical%20Variational%20Autoencoder%0AAuthor%3A%20Guangyi%20Zhang%20and%20Hanlei%20Li%20and%20Yunlong%20Cai%20and%20Qiyu%20Hu%20and%20Guanding%20Yu%20and%20Runmin%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20hierarchical%20joint%20source-channel%0Acoding%20%28HJSCC%29%20framework%20for%20image%20transmission%2C%20utilizing%20a%20hierarchical%0Avariational%20autoencoder%20%28VAE%29.%20Our%20approach%20leverages%20a%20combination%20of%0Abottom-up%20and%20top-down%20paths%20at%20the%20transmitter%20to%20autoregressively%20generate%0Amultiple%20hierarchical%20representations%20of%20the%20original%20image.%20These%0Arepresentations%20are%20then%20directly%20mapped%20to%20channel%20symbols%20for%20transmission%20by%0Athe%20JSCC%20encoder.%20We%20extend%20this%20framework%20to%20scenarios%20with%20a%20feedback%20link%2C%0Amodeling%20transmission%20over%20a%20noisy%20channel%20as%20a%20probabilistic%20sampling%20process%0Aand%20deriving%20a%20novel%20generative%20formulation%20for%20JSCC%20with%20feedback.%20Compared%0Awith%20existing%20approaches%2C%20our%20proposed%20HJSCC%20provides%20enhanced%20adaptability%20by%0Adynamically%20adjusting%20transmission%20bandwidth%2C%20encoding%20these%20representations%0Ainto%20varying%20amounts%20of%20channel%20symbols.%20Extensive%20experiments%20on%20images%20of%0Avarying%20resolutions%20demonstrate%20that%20our%20proposed%20model%20outperforms%20existing%0Abaselines%20in%20rate-distortion%20performance%20and%20maintains%20robustness%20against%0Achannel%20noise.%20The%20source%20code%20will%20be%20made%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16340v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Image%2520Transmission%2520with%2520Hierarchical%2520Variational%2520Autoencoder%26entry.906535625%3DGuangyi%2520Zhang%2520and%2520Hanlei%2520Li%2520and%2520Yunlong%2520Cai%2520and%2520Qiyu%2520Hu%2520and%2520Guanding%2520Yu%2520and%2520Runmin%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520innovative%2520hierarchical%2520joint%2520source-channel%250Acoding%2520%2528HJSCC%2529%2520framework%2520for%2520image%2520transmission%252C%2520utilizing%2520a%2520hierarchical%250Avariational%2520autoencoder%2520%2528VAE%2529.%2520Our%2520approach%2520leverages%2520a%2520combination%2520of%250Abottom-up%2520and%2520top-down%2520paths%2520at%2520the%2520transmitter%2520to%2520autoregressively%2520generate%250Amultiple%2520hierarchical%2520representations%2520of%2520the%2520original%2520image.%2520These%250Arepresentations%2520are%2520then%2520directly%2520mapped%2520to%2520channel%2520symbols%2520for%2520transmission%2520by%250Athe%2520JSCC%2520encoder.%2520We%2520extend%2520this%2520framework%2520to%2520scenarios%2520with%2520a%2520feedback%2520link%252C%250Amodeling%2520transmission%2520over%2520a%2520noisy%2520channel%2520as%2520a%2520probabilistic%2520sampling%2520process%250Aand%2520deriving%2520a%2520novel%2520generative%2520formulation%2520for%2520JSCC%2520with%2520feedback.%2520Compared%250Awith%2520existing%2520approaches%252C%2520our%2520proposed%2520HJSCC%2520provides%2520enhanced%2520adaptability%2520by%250Adynamically%2520adjusting%2520transmission%2520bandwidth%252C%2520encoding%2520these%2520representations%250Ainto%2520varying%2520amounts%2520of%2520channel%2520symbols.%2520Extensive%2520experiments%2520on%2520images%2520of%250Avarying%2520resolutions%2520demonstrate%2520that%2520our%2520proposed%2520model%2520outperforms%2520existing%250Abaselines%2520in%2520rate-distortion%2520performance%2520and%2520maintains%2520robustness%2520against%250Achannel%2520noise.%2520The%2520source%2520code%2520will%2520be%2520made%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16340v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Image%20Transmission%20with%20Hierarchical%20Variational%20Autoencoder&entry.906535625=Guangyi%20Zhang%20and%20Hanlei%20Li%20and%20Yunlong%20Cai%20and%20Qiyu%20Hu%20and%20Guanding%20Yu%20and%20Runmin%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20hierarchical%20joint%20source-channel%0Acoding%20%28HJSCC%29%20framework%20for%20image%20transmission%2C%20utilizing%20a%20hierarchical%0Avariational%20autoencoder%20%28VAE%29.%20Our%20approach%20leverages%20a%20combination%20of%0Abottom-up%20and%20top-down%20paths%20at%20the%20transmitter%20to%20autoregressively%20generate%0Amultiple%20hierarchical%20representations%20of%20the%20original%20image.%20These%0Arepresentations%20are%20then%20directly%20mapped%20to%20channel%20symbols%20for%20transmission%20by%0Athe%20JSCC%20encoder.%20We%20extend%20this%20framework%20to%20scenarios%20with%20a%20feedback%20link%2C%0Amodeling%20transmission%20over%20a%20noisy%20channel%20as%20a%20probabilistic%20sampling%20process%0Aand%20deriving%20a%20novel%20generative%20formulation%20for%20JSCC%20with%20feedback.%20Compared%0Awith%20existing%20approaches%2C%20our%20proposed%20HJSCC%20provides%20enhanced%20adaptability%20by%0Adynamically%20adjusting%20transmission%20bandwidth%2C%20encoding%20these%20representations%0Ainto%20varying%20amounts%20of%20channel%20symbols.%20Extensive%20experiments%20on%20images%20of%0Avarying%20resolutions%20demonstrate%20that%20our%20proposed%20model%20outperforms%20existing%0Abaselines%20in%20rate-distortion%20performance%20and%20maintains%20robustness%20against%0Achannel%20noise.%20The%20source%20code%20will%20be%20made%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16340v4&entry.124074799=Read"},
{"title": "Disentangled Latent Spaces for Reduced Order Models using Deterministic\n  Autoencoders", "author": "Henning Schwarz and Pyei Phyo Lin and Jens-Peter M. Zemke and Thomas Rung", "abstract": "  Data-driven reduced-order models based on autoencoders generally lack\ninterpretability compared to classical methods such as the proper orthogonal\ndecomposition. More interpretability can be gained by disentangling the latent\nvariables and analyzing the resulting modes. For this purpose, probabilistic\n$\\beta$-variational autoencoders ($\\beta$-VAEs) are frequently used in\ncomputational fluid dynamics and other simulation sciences. Using a benchmark\nperiodic flow dataset, we show that competitive results can be achieved using\nnon-probabilistic autoencoder approaches that either promote orthogonality or\npenalize correlation between latent variables. Compared to probabilistic\nautoencoders, these approaches offer more robustness with respect to the choice\nof hyperparameters entering the loss function. We further demonstrate the\nability of a non-probabilistic approach to identify a reduced number of active\nlatent variables by introducing a correlation penalty, a function also known\nfrom the use of $\\beta$-VAE. The investigated probabilistic and\nnon-probabilistic autoencoder models are finally used for the dimensionality\nreduction of aircraft ditching loads, which serves as an industrial application\nin this work.\n", "link": "http://arxiv.org/abs/2502.14679v1", "date": "2025-02-20", "relevancy": 2.0596, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5387}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5097}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Latent%20Spaces%20for%20Reduced%20Order%20Models%20using%20Deterministic%0A%20%20Autoencoders&body=Title%3A%20Disentangled%20Latent%20Spaces%20for%20Reduced%20Order%20Models%20using%20Deterministic%0A%20%20Autoencoders%0AAuthor%3A%20Henning%20Schwarz%20and%20Pyei%20Phyo%20Lin%20and%20Jens-Peter%20M.%20Zemke%20and%20Thomas%20Rung%0AAbstract%3A%20%20%20Data-driven%20reduced-order%20models%20based%20on%20autoencoders%20generally%20lack%0Ainterpretability%20compared%20to%20classical%20methods%20such%20as%20the%20proper%20orthogonal%0Adecomposition.%20More%20interpretability%20can%20be%20gained%20by%20disentangling%20the%20latent%0Avariables%20and%20analyzing%20the%20resulting%20modes.%20For%20this%20purpose%2C%20probabilistic%0A%24%5Cbeta%24-variational%20autoencoders%20%28%24%5Cbeta%24-VAEs%29%20are%20frequently%20used%20in%0Acomputational%20fluid%20dynamics%20and%20other%20simulation%20sciences.%20Using%20a%20benchmark%0Aperiodic%20flow%20dataset%2C%20we%20show%20that%20competitive%20results%20can%20be%20achieved%20using%0Anon-probabilistic%20autoencoder%20approaches%20that%20either%20promote%20orthogonality%20or%0Apenalize%20correlation%20between%20latent%20variables.%20Compared%20to%20probabilistic%0Aautoencoders%2C%20these%20approaches%20offer%20more%20robustness%20with%20respect%20to%20the%20choice%0Aof%20hyperparameters%20entering%20the%20loss%20function.%20We%20further%20demonstrate%20the%0Aability%20of%20a%20non-probabilistic%20approach%20to%20identify%20a%20reduced%20number%20of%20active%0Alatent%20variables%20by%20introducing%20a%20correlation%20penalty%2C%20a%20function%20also%20known%0Afrom%20the%20use%20of%20%24%5Cbeta%24-VAE.%20The%20investigated%20probabilistic%20and%0Anon-probabilistic%20autoencoder%20models%20are%20finally%20used%20for%20the%20dimensionality%0Areduction%20of%20aircraft%20ditching%20loads%2C%20which%20serves%20as%20an%20industrial%20application%0Ain%20this%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Latent%2520Spaces%2520for%2520Reduced%2520Order%2520Models%2520using%2520Deterministic%250A%2520%2520Autoencoders%26entry.906535625%3DHenning%2520Schwarz%2520and%2520Pyei%2520Phyo%2520Lin%2520and%2520Jens-Peter%2520M.%2520Zemke%2520and%2520Thomas%2520Rung%26entry.1292438233%3D%2520%2520Data-driven%2520reduced-order%2520models%2520based%2520on%2520autoencoders%2520generally%2520lack%250Ainterpretability%2520compared%2520to%2520classical%2520methods%2520such%2520as%2520the%2520proper%2520orthogonal%250Adecomposition.%2520More%2520interpretability%2520can%2520be%2520gained%2520by%2520disentangling%2520the%2520latent%250Avariables%2520and%2520analyzing%2520the%2520resulting%2520modes.%2520For%2520this%2520purpose%252C%2520probabilistic%250A%2524%255Cbeta%2524-variational%2520autoencoders%2520%2528%2524%255Cbeta%2524-VAEs%2529%2520are%2520frequently%2520used%2520in%250Acomputational%2520fluid%2520dynamics%2520and%2520other%2520simulation%2520sciences.%2520Using%2520a%2520benchmark%250Aperiodic%2520flow%2520dataset%252C%2520we%2520show%2520that%2520competitive%2520results%2520can%2520be%2520achieved%2520using%250Anon-probabilistic%2520autoencoder%2520approaches%2520that%2520either%2520promote%2520orthogonality%2520or%250Apenalize%2520correlation%2520between%2520latent%2520variables.%2520Compared%2520to%2520probabilistic%250Aautoencoders%252C%2520these%2520approaches%2520offer%2520more%2520robustness%2520with%2520respect%2520to%2520the%2520choice%250Aof%2520hyperparameters%2520entering%2520the%2520loss%2520function.%2520We%2520further%2520demonstrate%2520the%250Aability%2520of%2520a%2520non-probabilistic%2520approach%2520to%2520identify%2520a%2520reduced%2520number%2520of%2520active%250Alatent%2520variables%2520by%2520introducing%2520a%2520correlation%2520penalty%252C%2520a%2520function%2520also%2520known%250Afrom%2520the%2520use%2520of%2520%2524%255Cbeta%2524-VAE.%2520The%2520investigated%2520probabilistic%2520and%250Anon-probabilistic%2520autoencoder%2520models%2520are%2520finally%2520used%2520for%2520the%2520dimensionality%250Areduction%2520of%2520aircraft%2520ditching%2520loads%252C%2520which%2520serves%2520as%2520an%2520industrial%2520application%250Ain%2520this%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Latent%20Spaces%20for%20Reduced%20Order%20Models%20using%20Deterministic%0A%20%20Autoencoders&entry.906535625=Henning%20Schwarz%20and%20Pyei%20Phyo%20Lin%20and%20Jens-Peter%20M.%20Zemke%20and%20Thomas%20Rung&entry.1292438233=%20%20Data-driven%20reduced-order%20models%20based%20on%20autoencoders%20generally%20lack%0Ainterpretability%20compared%20to%20classical%20methods%20such%20as%20the%20proper%20orthogonal%0Adecomposition.%20More%20interpretability%20can%20be%20gained%20by%20disentangling%20the%20latent%0Avariables%20and%20analyzing%20the%20resulting%20modes.%20For%20this%20purpose%2C%20probabilistic%0A%24%5Cbeta%24-variational%20autoencoders%20%28%24%5Cbeta%24-VAEs%29%20are%20frequently%20used%20in%0Acomputational%20fluid%20dynamics%20and%20other%20simulation%20sciences.%20Using%20a%20benchmark%0Aperiodic%20flow%20dataset%2C%20we%20show%20that%20competitive%20results%20can%20be%20achieved%20using%0Anon-probabilistic%20autoencoder%20approaches%20that%20either%20promote%20orthogonality%20or%0Apenalize%20correlation%20between%20latent%20variables.%20Compared%20to%20probabilistic%0Aautoencoders%2C%20these%20approaches%20offer%20more%20robustness%20with%20respect%20to%20the%20choice%0Aof%20hyperparameters%20entering%20the%20loss%20function.%20We%20further%20demonstrate%20the%0Aability%20of%20a%20non-probabilistic%20approach%20to%20identify%20a%20reduced%20number%20of%20active%0Alatent%20variables%20by%20introducing%20a%20correlation%20penalty%2C%20a%20function%20also%20known%0Afrom%20the%20use%20of%20%24%5Cbeta%24-VAE.%20The%20investigated%20probabilistic%20and%0Anon-probabilistic%20autoencoder%20models%20are%20finally%20used%20for%20the%20dimensionality%0Areduction%20of%20aircraft%20ditching%20loads%2C%20which%20serves%20as%20an%20industrial%20application%0Ain%20this%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14679v1&entry.124074799=Read"},
{"title": "Making Universal Policies Universal", "author": "Niklas H\u00f6pner and David Kuric and Herke van Hoof", "abstract": "  The development of a generalist agent capable of solving a wide range of\nsequential decision-making tasks remains a significant challenge. We address\nthis problem in a cross-agent setup where agents share the same observation\nspace but differ in their action spaces. Our approach builds on the universal\npolicy framework, which decouples policy learning into two stages: a\ndiffusion-based planner that generates observation sequences and an inverse\ndynamics model that assigns actions to these plans. We propose a method for\ntraining the planner on a joint dataset composed of trajectories from all\nagents. This method offers the benefit of positive transfer by pooling data\nfrom different agents, while the primary challenge lies in adapting shared\nplans to each agent's unique constraints. We evaluate our approach on the\nBabyAI environment, covering tasks of varying complexity, and demonstrate\npositive transfer across agents. Additionally, we examine the planner's\ngeneralisation ability to unseen agents and compare our method to traditional\nimitation learning approaches. By training on a pooled dataset from multiple\nagents, our universal policy achieves an improvement of up to $42.20\\%$ in task\ncompletion accuracy compared to a policy trained on a dataset from a single\nagent.\n", "link": "http://arxiv.org/abs/2502.14777v1", "date": "2025-02-20", "relevancy": 2.0558, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5273}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Making%20Universal%20Policies%20Universal&body=Title%3A%20Making%20Universal%20Policies%20Universal%0AAuthor%3A%20Niklas%20H%C3%B6pner%20and%20David%20Kuric%20and%20Herke%20van%20Hoof%0AAbstract%3A%20%20%20The%20development%20of%20a%20generalist%20agent%20capable%20of%20solving%20a%20wide%20range%20of%0Asequential%20decision-making%20tasks%20remains%20a%20significant%20challenge.%20We%20address%0Athis%20problem%20in%20a%20cross-agent%20setup%20where%20agents%20share%20the%20same%20observation%0Aspace%20but%20differ%20in%20their%20action%20spaces.%20Our%20approach%20builds%20on%20the%20universal%0Apolicy%20framework%2C%20which%20decouples%20policy%20learning%20into%20two%20stages%3A%20a%0Adiffusion-based%20planner%20that%20generates%20observation%20sequences%20and%20an%20inverse%0Adynamics%20model%20that%20assigns%20actions%20to%20these%20plans.%20We%20propose%20a%20method%20for%0Atraining%20the%20planner%20on%20a%20joint%20dataset%20composed%20of%20trajectories%20from%20all%0Aagents.%20This%20method%20offers%20the%20benefit%20of%20positive%20transfer%20by%20pooling%20data%0Afrom%20different%20agents%2C%20while%20the%20primary%20challenge%20lies%20in%20adapting%20shared%0Aplans%20to%20each%20agent%27s%20unique%20constraints.%20We%20evaluate%20our%20approach%20on%20the%0ABabyAI%20environment%2C%20covering%20tasks%20of%20varying%20complexity%2C%20and%20demonstrate%0Apositive%20transfer%20across%20agents.%20Additionally%2C%20we%20examine%20the%20planner%27s%0Ageneralisation%20ability%20to%20unseen%20agents%20and%20compare%20our%20method%20to%20traditional%0Aimitation%20learning%20approaches.%20By%20training%20on%20a%20pooled%20dataset%20from%20multiple%0Aagents%2C%20our%20universal%20policy%20achieves%20an%20improvement%20of%20up%20to%20%2442.20%5C%25%24%20in%20task%0Acompletion%20accuracy%20compared%20to%20a%20policy%20trained%20on%20a%20dataset%20from%20a%20single%0Aagent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaking%2520Universal%2520Policies%2520Universal%26entry.906535625%3DNiklas%2520H%25C3%25B6pner%2520and%2520David%2520Kuric%2520and%2520Herke%2520van%2520Hoof%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520a%2520generalist%2520agent%2520capable%2520of%2520solving%2520a%2520wide%2520range%2520of%250Asequential%2520decision-making%2520tasks%2520remains%2520a%2520significant%2520challenge.%2520We%2520address%250Athis%2520problem%2520in%2520a%2520cross-agent%2520setup%2520where%2520agents%2520share%2520the%2520same%2520observation%250Aspace%2520but%2520differ%2520in%2520their%2520action%2520spaces.%2520Our%2520approach%2520builds%2520on%2520the%2520universal%250Apolicy%2520framework%252C%2520which%2520decouples%2520policy%2520learning%2520into%2520two%2520stages%253A%2520a%250Adiffusion-based%2520planner%2520that%2520generates%2520observation%2520sequences%2520and%2520an%2520inverse%250Adynamics%2520model%2520that%2520assigns%2520actions%2520to%2520these%2520plans.%2520We%2520propose%2520a%2520method%2520for%250Atraining%2520the%2520planner%2520on%2520a%2520joint%2520dataset%2520composed%2520of%2520trajectories%2520from%2520all%250Aagents.%2520This%2520method%2520offers%2520the%2520benefit%2520of%2520positive%2520transfer%2520by%2520pooling%2520data%250Afrom%2520different%2520agents%252C%2520while%2520the%2520primary%2520challenge%2520lies%2520in%2520adapting%2520shared%250Aplans%2520to%2520each%2520agent%2527s%2520unique%2520constraints.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%250ABabyAI%2520environment%252C%2520covering%2520tasks%2520of%2520varying%2520complexity%252C%2520and%2520demonstrate%250Apositive%2520transfer%2520across%2520agents.%2520Additionally%252C%2520we%2520examine%2520the%2520planner%2527s%250Ageneralisation%2520ability%2520to%2520unseen%2520agents%2520and%2520compare%2520our%2520method%2520to%2520traditional%250Aimitation%2520learning%2520approaches.%2520By%2520training%2520on%2520a%2520pooled%2520dataset%2520from%2520multiple%250Aagents%252C%2520our%2520universal%2520policy%2520achieves%2520an%2520improvement%2520of%2520up%2520to%2520%252442.20%255C%2525%2524%2520in%2520task%250Acompletion%2520accuracy%2520compared%2520to%2520a%2520policy%2520trained%2520on%2520a%2520dataset%2520from%2520a%2520single%250Aagent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Making%20Universal%20Policies%20Universal&entry.906535625=Niklas%20H%C3%B6pner%20and%20David%20Kuric%20and%20Herke%20van%20Hoof&entry.1292438233=%20%20The%20development%20of%20a%20generalist%20agent%20capable%20of%20solving%20a%20wide%20range%20of%0Asequential%20decision-making%20tasks%20remains%20a%20significant%20challenge.%20We%20address%0Athis%20problem%20in%20a%20cross-agent%20setup%20where%20agents%20share%20the%20same%20observation%0Aspace%20but%20differ%20in%20their%20action%20spaces.%20Our%20approach%20builds%20on%20the%20universal%0Apolicy%20framework%2C%20which%20decouples%20policy%20learning%20into%20two%20stages%3A%20a%0Adiffusion-based%20planner%20that%20generates%20observation%20sequences%20and%20an%20inverse%0Adynamics%20model%20that%20assigns%20actions%20to%20these%20plans.%20We%20propose%20a%20method%20for%0Atraining%20the%20planner%20on%20a%20joint%20dataset%20composed%20of%20trajectories%20from%20all%0Aagents.%20This%20method%20offers%20the%20benefit%20of%20positive%20transfer%20by%20pooling%20data%0Afrom%20different%20agents%2C%20while%20the%20primary%20challenge%20lies%20in%20adapting%20shared%0Aplans%20to%20each%20agent%27s%20unique%20constraints.%20We%20evaluate%20our%20approach%20on%20the%0ABabyAI%20environment%2C%20covering%20tasks%20of%20varying%20complexity%2C%20and%20demonstrate%0Apositive%20transfer%20across%20agents.%20Additionally%2C%20we%20examine%20the%20planner%27s%0Ageneralisation%20ability%20to%20unseen%20agents%20and%20compare%20our%20method%20to%20traditional%0Aimitation%20learning%20approaches.%20By%20training%20on%20a%20pooled%20dataset%20from%20multiple%0Aagents%2C%20our%20universal%20policy%20achieves%20an%20improvement%20of%20up%20to%20%2442.20%5C%25%24%20in%20task%0Acompletion%20accuracy%20compared%20to%20a%20policy%20trained%20on%20a%20dataset%20from%20a%20single%0Aagent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14777v1&entry.124074799=Read"},
{"title": "Differentially Private Optimization for Non-Decomposable Objective\n  Functions", "author": "Weiwei Kong and Andr\u00e9s Mu\u00f1oz Medina and M\u00f3nica Ribero", "abstract": "  Unsupervised pre-training is a common step in developing computer vision\nmodels and large language models. In this setting, the absence of labels\nrequires the use of similarity-based loss functions, such as contrastive loss,\nthat favor minimizing the distance between similar inputs and maximizing the\ndistance between distinct inputs. As privacy concerns mount, training these\nmodels using differential privacy has become more important. However, due to\nhow inputs are generated for these losses, one of their undesirable properties\nis that their $L_2$ sensitivity grows with the batch size. This property is\nparticularly disadvantageous for differentially private training methods, such\nas DP-SGD. To overcome this issue, we develop a new DP-SGD variant for\nsimilarity based loss functions -- in particular, the commonly-used contrastive\nloss -- that manipulates gradients of the objective function in a novel way to\nobtain a sensitivity of the summed gradient that is $O(1)$ for batch size $n$.\nWe test our DP-SGD variant on some CIFAR-10 pre-training and CIFAR-100\nfinetuning tasks and show that, in both tasks, our method's performance comes\nclose to that of a non-private model and generally outperforms DP-SGD applied\ndirectly to the contrastive loss.\n", "link": "http://arxiv.org/abs/2310.03104v2", "date": "2025-02-20", "relevancy": 2.0505, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5189}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5177}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentially%20Private%20Optimization%20for%20Non-Decomposable%20Objective%0A%20%20Functions&body=Title%3A%20Differentially%20Private%20Optimization%20for%20Non-Decomposable%20Objective%0A%20%20Functions%0AAuthor%3A%20Weiwei%20Kong%20and%20Andr%C3%A9s%20Mu%C3%B1oz%20Medina%20and%20M%C3%B3nica%20Ribero%0AAbstract%3A%20%20%20Unsupervised%20pre-training%20is%20a%20common%20step%20in%20developing%20computer%20vision%0Amodels%20and%20large%20language%20models.%20In%20this%20setting%2C%20the%20absence%20of%20labels%0Arequires%20the%20use%20of%20similarity-based%20loss%20functions%2C%20such%20as%20contrastive%20loss%2C%0Athat%20favor%20minimizing%20the%20distance%20between%20similar%20inputs%20and%20maximizing%20the%0Adistance%20between%20distinct%20inputs.%20As%20privacy%20concerns%20mount%2C%20training%20these%0Amodels%20using%20differential%20privacy%20has%20become%20more%20important.%20However%2C%20due%20to%0Ahow%20inputs%20are%20generated%20for%20these%20losses%2C%20one%20of%20their%20undesirable%20properties%0Ais%20that%20their%20%24L_2%24%20sensitivity%20grows%20with%20the%20batch%20size.%20This%20property%20is%0Aparticularly%20disadvantageous%20for%20differentially%20private%20training%20methods%2C%20such%0Aas%20DP-SGD.%20To%20overcome%20this%20issue%2C%20we%20develop%20a%20new%20DP-SGD%20variant%20for%0Asimilarity%20based%20loss%20functions%20--%20in%20particular%2C%20the%20commonly-used%20contrastive%0Aloss%20--%20that%20manipulates%20gradients%20of%20the%20objective%20function%20in%20a%20novel%20way%20to%0Aobtain%20a%20sensitivity%20of%20the%20summed%20gradient%20that%20is%20%24O%281%29%24%20for%20batch%20size%20%24n%24.%0AWe%20test%20our%20DP-SGD%20variant%20on%20some%20CIFAR-10%20pre-training%20and%20CIFAR-100%0Afinetuning%20tasks%20and%20show%20that%2C%20in%20both%20tasks%2C%20our%20method%27s%20performance%20comes%0Aclose%20to%20that%20of%20a%20non-private%20model%20and%20generally%20outperforms%20DP-SGD%20applied%0Adirectly%20to%20the%20contrastive%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03104v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentially%2520Private%2520Optimization%2520for%2520Non-Decomposable%2520Objective%250A%2520%2520Functions%26entry.906535625%3DWeiwei%2520Kong%2520and%2520Andr%25C3%25A9s%2520Mu%25C3%25B1oz%2520Medina%2520and%2520M%25C3%25B3nica%2520Ribero%26entry.1292438233%3D%2520%2520Unsupervised%2520pre-training%2520is%2520a%2520common%2520step%2520in%2520developing%2520computer%2520vision%250Amodels%2520and%2520large%2520language%2520models.%2520In%2520this%2520setting%252C%2520the%2520absence%2520of%2520labels%250Arequires%2520the%2520use%2520of%2520similarity-based%2520loss%2520functions%252C%2520such%2520as%2520contrastive%2520loss%252C%250Athat%2520favor%2520minimizing%2520the%2520distance%2520between%2520similar%2520inputs%2520and%2520maximizing%2520the%250Adistance%2520between%2520distinct%2520inputs.%2520As%2520privacy%2520concerns%2520mount%252C%2520training%2520these%250Amodels%2520using%2520differential%2520privacy%2520has%2520become%2520more%2520important.%2520However%252C%2520due%2520to%250Ahow%2520inputs%2520are%2520generated%2520for%2520these%2520losses%252C%2520one%2520of%2520their%2520undesirable%2520properties%250Ais%2520that%2520their%2520%2524L_2%2524%2520sensitivity%2520grows%2520with%2520the%2520batch%2520size.%2520This%2520property%2520is%250Aparticularly%2520disadvantageous%2520for%2520differentially%2520private%2520training%2520methods%252C%2520such%250Aas%2520DP-SGD.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520develop%2520a%2520new%2520DP-SGD%2520variant%2520for%250Asimilarity%2520based%2520loss%2520functions%2520--%2520in%2520particular%252C%2520the%2520commonly-used%2520contrastive%250Aloss%2520--%2520that%2520manipulates%2520gradients%2520of%2520the%2520objective%2520function%2520in%2520a%2520novel%2520way%2520to%250Aobtain%2520a%2520sensitivity%2520of%2520the%2520summed%2520gradient%2520that%2520is%2520%2524O%25281%2529%2524%2520for%2520batch%2520size%2520%2524n%2524.%250AWe%2520test%2520our%2520DP-SGD%2520variant%2520on%2520some%2520CIFAR-10%2520pre-training%2520and%2520CIFAR-100%250Afinetuning%2520tasks%2520and%2520show%2520that%252C%2520in%2520both%2520tasks%252C%2520our%2520method%2527s%2520performance%2520comes%250Aclose%2520to%2520that%2520of%2520a%2520non-private%2520model%2520and%2520generally%2520outperforms%2520DP-SGD%2520applied%250Adirectly%2520to%2520the%2520contrastive%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03104v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20Private%20Optimization%20for%20Non-Decomposable%20Objective%0A%20%20Functions&entry.906535625=Weiwei%20Kong%20and%20Andr%C3%A9s%20Mu%C3%B1oz%20Medina%20and%20M%C3%B3nica%20Ribero&entry.1292438233=%20%20Unsupervised%20pre-training%20is%20a%20common%20step%20in%20developing%20computer%20vision%0Amodels%20and%20large%20language%20models.%20In%20this%20setting%2C%20the%20absence%20of%20labels%0Arequires%20the%20use%20of%20similarity-based%20loss%20functions%2C%20such%20as%20contrastive%20loss%2C%0Athat%20favor%20minimizing%20the%20distance%20between%20similar%20inputs%20and%20maximizing%20the%0Adistance%20between%20distinct%20inputs.%20As%20privacy%20concerns%20mount%2C%20training%20these%0Amodels%20using%20differential%20privacy%20has%20become%20more%20important.%20However%2C%20due%20to%0Ahow%20inputs%20are%20generated%20for%20these%20losses%2C%20one%20of%20their%20undesirable%20properties%0Ais%20that%20their%20%24L_2%24%20sensitivity%20grows%20with%20the%20batch%20size.%20This%20property%20is%0Aparticularly%20disadvantageous%20for%20differentially%20private%20training%20methods%2C%20such%0Aas%20DP-SGD.%20To%20overcome%20this%20issue%2C%20we%20develop%20a%20new%20DP-SGD%20variant%20for%0Asimilarity%20based%20loss%20functions%20--%20in%20particular%2C%20the%20commonly-used%20contrastive%0Aloss%20--%20that%20manipulates%20gradients%20of%20the%20objective%20function%20in%20a%20novel%20way%20to%0Aobtain%20a%20sensitivity%20of%20the%20summed%20gradient%20that%20is%20%24O%281%29%24%20for%20batch%20size%20%24n%24.%0AWe%20test%20our%20DP-SGD%20variant%20on%20some%20CIFAR-10%20pre-training%20and%20CIFAR-100%0Afinetuning%20tasks%20and%20show%20that%2C%20in%20both%20tasks%2C%20our%20method%27s%20performance%20comes%0Aclose%20to%20that%20of%20a%20non-private%20model%20and%20generally%20outperforms%20DP-SGD%20applied%0Adirectly%20to%20the%20contrastive%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03104v2&entry.124074799=Read"},
{"title": "Determining Layer-wise Sparsity for Large Language Models Through a\n  Theoretical Perspective", "author": "Weizhong Huang and Yuxin Zhang and Xiawu Zheng and Fei Chao and Rongrong Ji", "abstract": "  In this paper, we address the challenge of determining the layer-wise\nsparsity rates of large language models (LLMs) through a theoretical\nperspective. Specifically, we identify a critical issue of\n''$\\textbf{reconstruction error explosion}$'' in existing LLMs sparsification\nmethods. This refers to the cumulative effect of reconstruction errors\nthroughout the sparsification process, where errors from earlier layers\npropagate and amplify in subsequent layers. As a result, the overall\nreconstruction error increases significantly, leading to a substantial\ndegradation in model performance. Through theoretical analysis, we derive a\nsimple yet effective approach to layer-wise sparsity allocation that mitigates\nthis issue. Our method uses a monotonically increasing arithmetic progression,\nreducing the process of determining sparsity rates for multiple layers to the\ndetermination of a single common difference hyperparameter. Remarkably, this\nallows for the optimal layer-wise sparsity rates to be identified with just a\nfew trials. Both our theoretical analysis and experimental results demonstrate\nthat this sparsity allocation scheme is near optimal. Extensive experiments\nshow that our method significantly improves the performance of sparse LLMs\nacross various architectures, outperforming existing layer-wise sparsity\nmethods. Furthermore, it enhances the performance of various compression\ntechniques and is applicable to vision and multimodal models. Notably, our\nmethod achieves a reduction of 52.10 in perplexity for the 70$\\%$ sparse\nLLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by\n10.50$\\%$, and delivers speedups of 2.63$\\times$ and 2.23$\\times$ on CPU and\nGPU, respectively.\n", "link": "http://arxiv.org/abs/2502.14770v1", "date": "2025-02-20", "relevancy": 2.0403, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Determining%20Layer-wise%20Sparsity%20for%20Large%20Language%20Models%20Through%20a%0A%20%20Theoretical%20Perspective&body=Title%3A%20Determining%20Layer-wise%20Sparsity%20for%20Large%20Language%20Models%20Through%20a%0A%20%20Theoretical%20Perspective%0AAuthor%3A%20Weizhong%20Huang%20and%20Yuxin%20Zhang%20and%20Xiawu%20Zheng%20and%20Fei%20Chao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20determining%20the%20layer-wise%0Asparsity%20rates%20of%20large%20language%20models%20%28LLMs%29%20through%20a%20theoretical%0Aperspective.%20Specifically%2C%20we%20identify%20a%20critical%20issue%20of%0A%27%27%24%5Ctextbf%7Breconstruction%20error%20explosion%7D%24%27%27%20in%20existing%20LLMs%20sparsification%0Amethods.%20This%20refers%20to%20the%20cumulative%20effect%20of%20reconstruction%20errors%0Athroughout%20the%20sparsification%20process%2C%20where%20errors%20from%20earlier%20layers%0Apropagate%20and%20amplify%20in%20subsequent%20layers.%20As%20a%20result%2C%20the%20overall%0Areconstruction%20error%20increases%20significantly%2C%20leading%20to%20a%20substantial%0Adegradation%20in%20model%20performance.%20Through%20theoretical%20analysis%2C%20we%20derive%20a%0Asimple%20yet%20effective%20approach%20to%20layer-wise%20sparsity%20allocation%20that%20mitigates%0Athis%20issue.%20Our%20method%20uses%20a%20monotonically%20increasing%20arithmetic%20progression%2C%0Areducing%20the%20process%20of%20determining%20sparsity%20rates%20for%20multiple%20layers%20to%20the%0Adetermination%20of%20a%20single%20common%20difference%20hyperparameter.%20Remarkably%2C%20this%0Aallows%20for%20the%20optimal%20layer-wise%20sparsity%20rates%20to%20be%20identified%20with%20just%20a%0Afew%20trials.%20Both%20our%20theoretical%20analysis%20and%20experimental%20results%20demonstrate%0Athat%20this%20sparsity%20allocation%20scheme%20is%20near%20optimal.%20Extensive%20experiments%0Ashow%20that%20our%20method%20significantly%20improves%20the%20performance%20of%20sparse%20LLMs%0Aacross%20various%20architectures%2C%20outperforming%20existing%20layer-wise%20sparsity%0Amethods.%20Furthermore%2C%20it%20enhances%20the%20performance%20of%20various%20compression%0Atechniques%20and%20is%20applicable%20to%20vision%20and%20multimodal%20models.%20Notably%2C%20our%0Amethod%20achieves%20a%20reduction%20of%2052.10%20in%20perplexity%20for%20the%2070%24%5C%25%24%20sparse%0ALLaMA2-7B%20model%20obtained%20via%20Wanda%2C%20improves%20average%20zero-shot%20accuracy%20by%0A10.50%24%5C%25%24%2C%20and%20delivers%20speedups%20of%202.63%24%5Ctimes%24%20and%202.23%24%5Ctimes%24%20on%20CPU%20and%0AGPU%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetermining%2520Layer-wise%2520Sparsity%2520for%2520Large%2520Language%2520Models%2520Through%2520a%250A%2520%2520Theoretical%2520Perspective%26entry.906535625%3DWeizhong%2520Huang%2520and%2520Yuxin%2520Zhang%2520and%2520Xiawu%2520Zheng%2520and%2520Fei%2520Chao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenge%2520of%2520determining%2520the%2520layer-wise%250Asparsity%2520rates%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520through%2520a%2520theoretical%250Aperspective.%2520Specifically%252C%2520we%2520identify%2520a%2520critical%2520issue%2520of%250A%2527%2527%2524%255Ctextbf%257Breconstruction%2520error%2520explosion%257D%2524%2527%2527%2520in%2520existing%2520LLMs%2520sparsification%250Amethods.%2520This%2520refers%2520to%2520the%2520cumulative%2520effect%2520of%2520reconstruction%2520errors%250Athroughout%2520the%2520sparsification%2520process%252C%2520where%2520errors%2520from%2520earlier%2520layers%250Apropagate%2520and%2520amplify%2520in%2520subsequent%2520layers.%2520As%2520a%2520result%252C%2520the%2520overall%250Areconstruction%2520error%2520increases%2520significantly%252C%2520leading%2520to%2520a%2520substantial%250Adegradation%2520in%2520model%2520performance.%2520Through%2520theoretical%2520analysis%252C%2520we%2520derive%2520a%250Asimple%2520yet%2520effective%2520approach%2520to%2520layer-wise%2520sparsity%2520allocation%2520that%2520mitigates%250Athis%2520issue.%2520Our%2520method%2520uses%2520a%2520monotonically%2520increasing%2520arithmetic%2520progression%252C%250Areducing%2520the%2520process%2520of%2520determining%2520sparsity%2520rates%2520for%2520multiple%2520layers%2520to%2520the%250Adetermination%2520of%2520a%2520single%2520common%2520difference%2520hyperparameter.%2520Remarkably%252C%2520this%250Aallows%2520for%2520the%2520optimal%2520layer-wise%2520sparsity%2520rates%2520to%2520be%2520identified%2520with%2520just%2520a%250Afew%2520trials.%2520Both%2520our%2520theoretical%2520analysis%2520and%2520experimental%2520results%2520demonstrate%250Athat%2520this%2520sparsity%2520allocation%2520scheme%2520is%2520near%2520optimal.%2520Extensive%2520experiments%250Ashow%2520that%2520our%2520method%2520significantly%2520improves%2520the%2520performance%2520of%2520sparse%2520LLMs%250Aacross%2520various%2520architectures%252C%2520outperforming%2520existing%2520layer-wise%2520sparsity%250Amethods.%2520Furthermore%252C%2520it%2520enhances%2520the%2520performance%2520of%2520various%2520compression%250Atechniques%2520and%2520is%2520applicable%2520to%2520vision%2520and%2520multimodal%2520models.%2520Notably%252C%2520our%250Amethod%2520achieves%2520a%2520reduction%2520of%252052.10%2520in%2520perplexity%2520for%2520the%252070%2524%255C%2525%2524%2520sparse%250ALLaMA2-7B%2520model%2520obtained%2520via%2520Wanda%252C%2520improves%2520average%2520zero-shot%2520accuracy%2520by%250A10.50%2524%255C%2525%2524%252C%2520and%2520delivers%2520speedups%2520of%25202.63%2524%255Ctimes%2524%2520and%25202.23%2524%255Ctimes%2524%2520on%2520CPU%2520and%250AGPU%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Determining%20Layer-wise%20Sparsity%20for%20Large%20Language%20Models%20Through%20a%0A%20%20Theoretical%20Perspective&entry.906535625=Weizhong%20Huang%20and%20Yuxin%20Zhang%20and%20Xiawu%20Zheng%20and%20Fei%20Chao%20and%20Rongrong%20Ji&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20determining%20the%20layer-wise%0Asparsity%20rates%20of%20large%20language%20models%20%28LLMs%29%20through%20a%20theoretical%0Aperspective.%20Specifically%2C%20we%20identify%20a%20critical%20issue%20of%0A%27%27%24%5Ctextbf%7Breconstruction%20error%20explosion%7D%24%27%27%20in%20existing%20LLMs%20sparsification%0Amethods.%20This%20refers%20to%20the%20cumulative%20effect%20of%20reconstruction%20errors%0Athroughout%20the%20sparsification%20process%2C%20where%20errors%20from%20earlier%20layers%0Apropagate%20and%20amplify%20in%20subsequent%20layers.%20As%20a%20result%2C%20the%20overall%0Areconstruction%20error%20increases%20significantly%2C%20leading%20to%20a%20substantial%0Adegradation%20in%20model%20performance.%20Through%20theoretical%20analysis%2C%20we%20derive%20a%0Asimple%20yet%20effective%20approach%20to%20layer-wise%20sparsity%20allocation%20that%20mitigates%0Athis%20issue.%20Our%20method%20uses%20a%20monotonically%20increasing%20arithmetic%20progression%2C%0Areducing%20the%20process%20of%20determining%20sparsity%20rates%20for%20multiple%20layers%20to%20the%0Adetermination%20of%20a%20single%20common%20difference%20hyperparameter.%20Remarkably%2C%20this%0Aallows%20for%20the%20optimal%20layer-wise%20sparsity%20rates%20to%20be%20identified%20with%20just%20a%0Afew%20trials.%20Both%20our%20theoretical%20analysis%20and%20experimental%20results%20demonstrate%0Athat%20this%20sparsity%20allocation%20scheme%20is%20near%20optimal.%20Extensive%20experiments%0Ashow%20that%20our%20method%20significantly%20improves%20the%20performance%20of%20sparse%20LLMs%0Aacross%20various%20architectures%2C%20outperforming%20existing%20layer-wise%20sparsity%0Amethods.%20Furthermore%2C%20it%20enhances%20the%20performance%20of%20various%20compression%0Atechniques%20and%20is%20applicable%20to%20vision%20and%20multimodal%20models.%20Notably%2C%20our%0Amethod%20achieves%20a%20reduction%20of%2052.10%20in%20perplexity%20for%20the%2070%24%5C%25%24%20sparse%0ALLaMA2-7B%20model%20obtained%20via%20Wanda%2C%20improves%20average%20zero-shot%20accuracy%20by%0A10.50%24%5C%25%24%2C%20and%20delivers%20speedups%20of%202.63%24%5Ctimes%24%20and%202.23%24%5Ctimes%24%20on%20CPU%20and%0AGPU%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14770v1&entry.124074799=Read"},
{"title": "TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep\n  Reinforcement Learning", "author": "Chengkai Xu and Jiaqi Liu and Shiyu Fang and Yiming Cui and Dong Chen and Peng Hang and Jian Sun", "abstract": "  Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs)\neach show promise in addressing decision-making challenges in autonomous\ndriving, DRL often suffers from high sample complexity, while LLMs have\ndifficulty ensuring real-time decision making. To address these limitations, we\npropose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide\nan attention-based Student DRL policy. By incorporating risk metrics,\nhistorical scenario retrieval, and domain heuristics into context-rich prompts,\nthe LLM produces high-level driving strategies through chain-of-thought\nreasoning. A self-attention mechanism then fuses these strategies with the DRL\nagent's exploration, accelerating policy convergence and boosting robustness\nacross diverse driving conditions. The experimental results, evaluated across\nmultiple traffic scenarios, show that TeLL-Drive outperforms existing baseline\nmethods, including other LLM-based approaches, in terms of success rates,\naverage returns, and real-time feasibility. Ablation studies underscore the\nimportance of each model component, especially the synergy between the\nattention mechanism and LLM-driven guidance. Finally, we build a virtual-real\nfusion experimental platform to verify the real-time performance, robustness,\nand reliability of the algorithm running on real vehicles through\nvehicle-in-loop experiments.\n", "link": "http://arxiv.org/abs/2502.01387v3", "date": "2025-02-20", "relevancy": 2.0334, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5112}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeLL-Drive%3A%20Enhancing%20Autonomous%20Driving%20with%20Teacher%20LLM-Guided%20Deep%0A%20%20Reinforcement%20Learning&body=Title%3A%20TeLL-Drive%3A%20Enhancing%20Autonomous%20Driving%20with%20Teacher%20LLM-Guided%20Deep%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Chengkai%20Xu%20and%20Jiaqi%20Liu%20and%20Shiyu%20Fang%20and%20Yiming%20Cui%20and%20Dong%20Chen%20and%20Peng%20Hang%20and%20Jian%20Sun%0AAbstract%3A%20%20%20Although%20Deep%20Reinforcement%20Learning%20%28DRL%29%20and%20Large%20Language%20Models%20%28LLMs%29%0Aeach%20show%20promise%20in%20addressing%20decision-making%20challenges%20in%20autonomous%0Adriving%2C%20DRL%20often%20suffers%20from%20high%20sample%20complexity%2C%20while%20LLMs%20have%0Adifficulty%20ensuring%20real-time%20decision%20making.%20To%20address%20these%20limitations%2C%20we%0Apropose%20TeLL-Drive%2C%20a%20hybrid%20framework%20that%20integrates%20a%20Teacher%20LLM%20to%20guide%0Aan%20attention-based%20Student%20DRL%20policy.%20By%20incorporating%20risk%20metrics%2C%0Ahistorical%20scenario%20retrieval%2C%20and%20domain%20heuristics%20into%20context-rich%20prompts%2C%0Athe%20LLM%20produces%20high-level%20driving%20strategies%20through%20chain-of-thought%0Areasoning.%20A%20self-attention%20mechanism%20then%20fuses%20these%20strategies%20with%20the%20DRL%0Aagent%27s%20exploration%2C%20accelerating%20policy%20convergence%20and%20boosting%20robustness%0Aacross%20diverse%20driving%20conditions.%20The%20experimental%20results%2C%20evaluated%20across%0Amultiple%20traffic%20scenarios%2C%20show%20that%20TeLL-Drive%20outperforms%20existing%20baseline%0Amethods%2C%20including%20other%20LLM-based%20approaches%2C%20in%20terms%20of%20success%20rates%2C%0Aaverage%20returns%2C%20and%20real-time%20feasibility.%20Ablation%20studies%20underscore%20the%0Aimportance%20of%20each%20model%20component%2C%20especially%20the%20synergy%20between%20the%0Aattention%20mechanism%20and%20LLM-driven%20guidance.%20Finally%2C%20we%20build%20a%20virtual-real%0Afusion%20experimental%20platform%20to%20verify%20the%20real-time%20performance%2C%20robustness%2C%0Aand%20reliability%20of%20the%20algorithm%20running%20on%20real%20vehicles%20through%0Avehicle-in-loop%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01387v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeLL-Drive%253A%2520Enhancing%2520Autonomous%2520Driving%2520with%2520Teacher%2520LLM-Guided%2520Deep%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DChengkai%2520Xu%2520and%2520Jiaqi%2520Liu%2520and%2520Shiyu%2520Fang%2520and%2520Yiming%2520Cui%2520and%2520Dong%2520Chen%2520and%2520Peng%2520Hang%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520Although%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aeach%2520show%2520promise%2520in%2520addressing%2520decision-making%2520challenges%2520in%2520autonomous%250Adriving%252C%2520DRL%2520often%2520suffers%2520from%2520high%2520sample%2520complexity%252C%2520while%2520LLMs%2520have%250Adifficulty%2520ensuring%2520real-time%2520decision%2520making.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520TeLL-Drive%252C%2520a%2520hybrid%2520framework%2520that%2520integrates%2520a%2520Teacher%2520LLM%2520to%2520guide%250Aan%2520attention-based%2520Student%2520DRL%2520policy.%2520By%2520incorporating%2520risk%2520metrics%252C%250Ahistorical%2520scenario%2520retrieval%252C%2520and%2520domain%2520heuristics%2520into%2520context-rich%2520prompts%252C%250Athe%2520LLM%2520produces%2520high-level%2520driving%2520strategies%2520through%2520chain-of-thought%250Areasoning.%2520A%2520self-attention%2520mechanism%2520then%2520fuses%2520these%2520strategies%2520with%2520the%2520DRL%250Aagent%2527s%2520exploration%252C%2520accelerating%2520policy%2520convergence%2520and%2520boosting%2520robustness%250Aacross%2520diverse%2520driving%2520conditions.%2520The%2520experimental%2520results%252C%2520evaluated%2520across%250Amultiple%2520traffic%2520scenarios%252C%2520show%2520that%2520TeLL-Drive%2520outperforms%2520existing%2520baseline%250Amethods%252C%2520including%2520other%2520LLM-based%2520approaches%252C%2520in%2520terms%2520of%2520success%2520rates%252C%250Aaverage%2520returns%252C%2520and%2520real-time%2520feasibility.%2520Ablation%2520studies%2520underscore%2520the%250Aimportance%2520of%2520each%2520model%2520component%252C%2520especially%2520the%2520synergy%2520between%2520the%250Aattention%2520mechanism%2520and%2520LLM-driven%2520guidance.%2520Finally%252C%2520we%2520build%2520a%2520virtual-real%250Afusion%2520experimental%2520platform%2520to%2520verify%2520the%2520real-time%2520performance%252C%2520robustness%252C%250Aand%2520reliability%2520of%2520the%2520algorithm%2520running%2520on%2520real%2520vehicles%2520through%250Avehicle-in-loop%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01387v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeLL-Drive%3A%20Enhancing%20Autonomous%20Driving%20with%20Teacher%20LLM-Guided%20Deep%0A%20%20Reinforcement%20Learning&entry.906535625=Chengkai%20Xu%20and%20Jiaqi%20Liu%20and%20Shiyu%20Fang%20and%20Yiming%20Cui%20and%20Dong%20Chen%20and%20Peng%20Hang%20and%20Jian%20Sun&entry.1292438233=%20%20Although%20Deep%20Reinforcement%20Learning%20%28DRL%29%20and%20Large%20Language%20Models%20%28LLMs%29%0Aeach%20show%20promise%20in%20addressing%20decision-making%20challenges%20in%20autonomous%0Adriving%2C%20DRL%20often%20suffers%20from%20high%20sample%20complexity%2C%20while%20LLMs%20have%0Adifficulty%20ensuring%20real-time%20decision%20making.%20To%20address%20these%20limitations%2C%20we%0Apropose%20TeLL-Drive%2C%20a%20hybrid%20framework%20that%20integrates%20a%20Teacher%20LLM%20to%20guide%0Aan%20attention-based%20Student%20DRL%20policy.%20By%20incorporating%20risk%20metrics%2C%0Ahistorical%20scenario%20retrieval%2C%20and%20domain%20heuristics%20into%20context-rich%20prompts%2C%0Athe%20LLM%20produces%20high-level%20driving%20strategies%20through%20chain-of-thought%0Areasoning.%20A%20self-attention%20mechanism%20then%20fuses%20these%20strategies%20with%20the%20DRL%0Aagent%27s%20exploration%2C%20accelerating%20policy%20convergence%20and%20boosting%20robustness%0Aacross%20diverse%20driving%20conditions.%20The%20experimental%20results%2C%20evaluated%20across%0Amultiple%20traffic%20scenarios%2C%20show%20that%20TeLL-Drive%20outperforms%20existing%20baseline%0Amethods%2C%20including%20other%20LLM-based%20approaches%2C%20in%20terms%20of%20success%20rates%2C%0Aaverage%20returns%2C%20and%20real-time%20feasibility.%20Ablation%20studies%20underscore%20the%0Aimportance%20of%20each%20model%20component%2C%20especially%20the%20synergy%20between%20the%0Aattention%20mechanism%20and%20LLM-driven%20guidance.%20Finally%2C%20we%20build%20a%20virtual-real%0Afusion%20experimental%20platform%20to%20verify%20the%20real-time%20performance%2C%20robustness%2C%0Aand%20reliability%20of%20the%20algorithm%20running%20on%20real%20vehicles%20through%0Avehicle-in-loop%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01387v3&entry.124074799=Read"},
{"title": "Dynamic Concepts Personalization from Single Videos", "author": "Rameen Abdal and Or Patashnik and Ivan Skorokhodov and Willi Menapace and Aliaksandr Siarohin and Sergey Tulyakov and Daniel Cohen-Or and Kfir Aberman", "abstract": "  Personalizing generative text-to-image models has seen remarkable progress,\nbut extending this personalization to text-to-video models presents unique\nchallenges. Unlike static concepts, personalizing text-to-video models has the\npotential to capture dynamic concepts, i.e., entities defined not only by their\nappearance but also by their motion. In this paper, we introduce\nSet-and-Sequence, a novel framework for personalizing Diffusion Transformers\n(DiTs)-based generative video models with dynamic concepts. Our approach\nimposes a spatio-temporal weight space within an architecture that does not\nexplicitly separate spatial and temporal features. This is achieved in two key\nstages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an\nunordered set of frames from the video to learn an identity LoRA basis that\nrepresents the appearance, free from temporal interference. In the second\nstage, with the identity LoRAs frozen, we augment their coefficients with\nMotion Residuals and fine-tune them on the full video sequence, capturing\nmotion dynamics. Our Set-and-Sequence framework results in a spatio-temporal\nweight space that effectively embeds dynamic concepts into the video model's\noutput domain, enabling unprecedented editability and compositionality while\nsetting a new benchmark for personalizing dynamic concepts.\n", "link": "http://arxiv.org/abs/2502.14844v1", "date": "2025-02-20", "relevancy": 2.0319, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7688}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6798}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Concepts%20Personalization%20from%20Single%20Videos&body=Title%3A%20Dynamic%20Concepts%20Personalization%20from%20Single%20Videos%0AAuthor%3A%20Rameen%20Abdal%20and%20Or%20Patashnik%20and%20Ivan%20Skorokhodov%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov%20and%20Daniel%20Cohen-Or%20and%20Kfir%20Aberman%0AAbstract%3A%20%20%20Personalizing%20generative%20text-to-image%20models%20has%20seen%20remarkable%20progress%2C%0Abut%20extending%20this%20personalization%20to%20text-to-video%20models%20presents%20unique%0Achallenges.%20Unlike%20static%20concepts%2C%20personalizing%20text-to-video%20models%20has%20the%0Apotential%20to%20capture%20dynamic%20concepts%2C%20i.e.%2C%20entities%20defined%20not%20only%20by%20their%0Aappearance%20but%20also%20by%20their%20motion.%20In%20this%20paper%2C%20we%20introduce%0ASet-and-Sequence%2C%20a%20novel%20framework%20for%20personalizing%20Diffusion%20Transformers%0A%28DiTs%29-based%20generative%20video%20models%20with%20dynamic%20concepts.%20Our%20approach%0Aimposes%20a%20spatio-temporal%20weight%20space%20within%20an%20architecture%20that%20does%20not%0Aexplicitly%20separate%20spatial%20and%20temporal%20features.%20This%20is%20achieved%20in%20two%20key%0Astages.%20First%2C%20we%20fine-tune%20Low-Rank%20Adaptation%20%28LoRA%29%20layers%20using%20an%0Aunordered%20set%20of%20frames%20from%20the%20video%20to%20learn%20an%20identity%20LoRA%20basis%20that%0Arepresents%20the%20appearance%2C%20free%20from%20temporal%20interference.%20In%20the%20second%0Astage%2C%20with%20the%20identity%20LoRAs%20frozen%2C%20we%20augment%20their%20coefficients%20with%0AMotion%20Residuals%20and%20fine-tune%20them%20on%20the%20full%20video%20sequence%2C%20capturing%0Amotion%20dynamics.%20Our%20Set-and-Sequence%20framework%20results%20in%20a%20spatio-temporal%0Aweight%20space%20that%20effectively%20embeds%20dynamic%20concepts%20into%20the%20video%20model%27s%0Aoutput%20domain%2C%20enabling%20unprecedented%20editability%20and%20compositionality%20while%0Asetting%20a%20new%20benchmark%20for%20personalizing%20dynamic%20concepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Concepts%2520Personalization%2520from%2520Single%2520Videos%26entry.906535625%3DRameen%2520Abdal%2520and%2520Or%2520Patashnik%2520and%2520Ivan%2520Skorokhodov%2520and%2520Willi%2520Menapace%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Sergey%2520Tulyakov%2520and%2520Daniel%2520Cohen-Or%2520and%2520Kfir%2520Aberman%26entry.1292438233%3D%2520%2520Personalizing%2520generative%2520text-to-image%2520models%2520has%2520seen%2520remarkable%2520progress%252C%250Abut%2520extending%2520this%2520personalization%2520to%2520text-to-video%2520models%2520presents%2520unique%250Achallenges.%2520Unlike%2520static%2520concepts%252C%2520personalizing%2520text-to-video%2520models%2520has%2520the%250Apotential%2520to%2520capture%2520dynamic%2520concepts%252C%2520i.e.%252C%2520entities%2520defined%2520not%2520only%2520by%2520their%250Aappearance%2520but%2520also%2520by%2520their%2520motion.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ASet-and-Sequence%252C%2520a%2520novel%2520framework%2520for%2520personalizing%2520Diffusion%2520Transformers%250A%2528DiTs%2529-based%2520generative%2520video%2520models%2520with%2520dynamic%2520concepts.%2520Our%2520approach%250Aimposes%2520a%2520spatio-temporal%2520weight%2520space%2520within%2520an%2520architecture%2520that%2520does%2520not%250Aexplicitly%2520separate%2520spatial%2520and%2520temporal%2520features.%2520This%2520is%2520achieved%2520in%2520two%2520key%250Astages.%2520First%252C%2520we%2520fine-tune%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520layers%2520using%2520an%250Aunordered%2520set%2520of%2520frames%2520from%2520the%2520video%2520to%2520learn%2520an%2520identity%2520LoRA%2520basis%2520that%250Arepresents%2520the%2520appearance%252C%2520free%2520from%2520temporal%2520interference.%2520In%2520the%2520second%250Astage%252C%2520with%2520the%2520identity%2520LoRAs%2520frozen%252C%2520we%2520augment%2520their%2520coefficients%2520with%250AMotion%2520Residuals%2520and%2520fine-tune%2520them%2520on%2520the%2520full%2520video%2520sequence%252C%2520capturing%250Amotion%2520dynamics.%2520Our%2520Set-and-Sequence%2520framework%2520results%2520in%2520a%2520spatio-temporal%250Aweight%2520space%2520that%2520effectively%2520embeds%2520dynamic%2520concepts%2520into%2520the%2520video%2520model%2527s%250Aoutput%2520domain%252C%2520enabling%2520unprecedented%2520editability%2520and%2520compositionality%2520while%250Asetting%2520a%2520new%2520benchmark%2520for%2520personalizing%2520dynamic%2520concepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Concepts%20Personalization%20from%20Single%20Videos&entry.906535625=Rameen%20Abdal%20and%20Or%20Patashnik%20and%20Ivan%20Skorokhodov%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov%20and%20Daniel%20Cohen-Or%20and%20Kfir%20Aberman&entry.1292438233=%20%20Personalizing%20generative%20text-to-image%20models%20has%20seen%20remarkable%20progress%2C%0Abut%20extending%20this%20personalization%20to%20text-to-video%20models%20presents%20unique%0Achallenges.%20Unlike%20static%20concepts%2C%20personalizing%20text-to-video%20models%20has%20the%0Apotential%20to%20capture%20dynamic%20concepts%2C%20i.e.%2C%20entities%20defined%20not%20only%20by%20their%0Aappearance%20but%20also%20by%20their%20motion.%20In%20this%20paper%2C%20we%20introduce%0ASet-and-Sequence%2C%20a%20novel%20framework%20for%20personalizing%20Diffusion%20Transformers%0A%28DiTs%29-based%20generative%20video%20models%20with%20dynamic%20concepts.%20Our%20approach%0Aimposes%20a%20spatio-temporal%20weight%20space%20within%20an%20architecture%20that%20does%20not%0Aexplicitly%20separate%20spatial%20and%20temporal%20features.%20This%20is%20achieved%20in%20two%20key%0Astages.%20First%2C%20we%20fine-tune%20Low-Rank%20Adaptation%20%28LoRA%29%20layers%20using%20an%0Aunordered%20set%20of%20frames%20from%20the%20video%20to%20learn%20an%20identity%20LoRA%20basis%20that%0Arepresents%20the%20appearance%2C%20free%20from%20temporal%20interference.%20In%20the%20second%0Astage%2C%20with%20the%20identity%20LoRAs%20frozen%2C%20we%20augment%20their%20coefficients%20with%0AMotion%20Residuals%20and%20fine-tune%20them%20on%20the%20full%20video%20sequence%2C%20capturing%0Amotion%20dynamics.%20Our%20Set-and-Sequence%20framework%20results%20in%20a%20spatio-temporal%0Aweight%20space%20that%20effectively%20embeds%20dynamic%20concepts%20into%20the%20video%20model%27s%0Aoutput%20domain%2C%20enabling%20unprecedented%20editability%20and%20compositionality%20while%0Asetting%20a%20new%20benchmark%20for%20personalizing%20dynamic%20concepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14844v1&entry.124074799=Read"},
{"title": "Safety Evaluation of DeepSeek Models in Chinese Contexts", "author": "Wenjing Zhang and Xuejiao Lei and Zhaoxiang Liu and Ning Wang and Zhenhong Long and Peijun Yang and Jiaojiao Zhao and Minjie Hua and Chaoyang Ma and Kai Wang and Shiguo Lian", "abstract": "  Recently, the DeepSeek series of models, leveraging their exceptional\nreasoning capabilities and open-source strategy, is reshaping the global AI\nlandscape. Despite these advantages, they exhibit significant safety\ndeficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,\nin collaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nhas a 100\\% attack success rate when processing harmful prompts. Additionally,\nmultiple safety companies and research institutions have confirmed critical\nsafety vulnerabilities in this model. As models demonstrating robust\nperformance in Chinese and English, DeepSeek models require equally crucial\nsafety assessments in both language contexts. However, current research has\npredominantly focused on safety evaluations in English environments, leaving a\ngap in comprehensive assessments of their safety performance in Chinese\ncontexts. In response to this gap, this study introduces CHiSafetyBench, a\nChinese-specific safety evaluation benchmark. This benchmark systematically\nevaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,\nrevealing their performance across safety categories. The experimental results\nquantify the deficiencies of these two models in Chinese contexts, providing\nkey insights for subsequent improvements. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmark and periodically update this report to provide more comprehensive and\naccurate assessment outcomes. Please refer to the latest version of the paper\nfor the most recent evaluation results and conclusions.\n", "link": "http://arxiv.org/abs/2502.11137v2", "date": "2025-02-20", "relevancy": 2.0308, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20Evaluation%20of%20DeepSeek%20Models%20in%20Chinese%20Contexts&body=Title%3A%20Safety%20Evaluation%20of%20DeepSeek%20Models%20in%20Chinese%20Contexts%0AAuthor%3A%20Wenjing%20Zhang%20and%20Xuejiao%20Lei%20and%20Zhaoxiang%20Liu%20and%20Ning%20Wang%20and%20Zhenhong%20Long%20and%20Peijun%20Yang%20and%20Jiaojiao%20Zhao%20and%20Minjie%20Hua%20and%20Chaoyang%20Ma%20and%20Kai%20Wang%20and%20Shiguo%20Lian%0AAbstract%3A%20%20%20Recently%2C%20the%20DeepSeek%20series%20of%20models%2C%20leveraging%20their%20exceptional%0Areasoning%20capabilities%20and%20open-source%20strategy%2C%20is%20reshaping%20the%20global%20AI%0Alandscape.%20Despite%20these%20advantages%2C%20they%20exhibit%20significant%20safety%0Adeficiencies.%20Research%20conducted%20by%20Robust%20Intelligence%2C%20a%20subsidiary%20of%20Cisco%2C%0Ain%20collaboration%20with%20the%20University%20of%20Pennsylvania%2C%20revealed%20that%20DeepSeek-R1%0Ahas%20a%20100%5C%25%20attack%20success%20rate%20when%20processing%20harmful%20prompts.%20Additionally%2C%0Amultiple%20safety%20companies%20and%20research%20institutions%20have%20confirmed%20critical%0Asafety%20vulnerabilities%20in%20this%20model.%20As%20models%20demonstrating%20robust%0Aperformance%20in%20Chinese%20and%20English%2C%20DeepSeek%20models%20require%20equally%20crucial%0Asafety%20assessments%20in%20both%20language%20contexts.%20However%2C%20current%20research%20has%0Apredominantly%20focused%20on%20safety%20evaluations%20in%20English%20environments%2C%20leaving%20a%0Agap%20in%20comprehensive%20assessments%20of%20their%20safety%20performance%20in%20Chinese%0Acontexts.%20In%20response%20to%20this%20gap%2C%20this%20study%20introduces%20CHiSafetyBench%2C%20a%0AChinese-specific%20safety%20evaluation%20benchmark.%20This%20benchmark%20systematically%0Aevaluates%20the%20safety%20of%20DeepSeek-R1%20and%20DeepSeek-V3%20in%20Chinese%20contexts%2C%0Arevealing%20their%20performance%20across%20safety%20categories.%20The%20experimental%20results%0Aquantify%20the%20deficiencies%20of%20these%20two%20models%20in%20Chinese%20contexts%2C%20providing%0Akey%20insights%20for%20subsequent%20improvements.%20It%20should%20be%20noted%20that%2C%20despite%20our%0Aefforts%20to%20establish%20a%20comprehensive%2C%20objective%2C%20and%20authoritative%20evaluation%0Abenchmark%2C%20the%20selection%20of%20test%20samples%2C%20characteristics%20of%20data%20distribution%2C%0Aand%20the%20setting%20of%20evaluation%20criteria%20may%20inevitably%20introduce%20certain%20biases%0Ainto%20the%20evaluation%20results.%20We%20will%20continuously%20optimize%20the%20evaluation%0Abenchmark%20and%20periodically%20update%20this%20report%20to%20provide%20more%20comprehensive%20and%0Aaccurate%20assessment%20outcomes.%20Please%20refer%20to%20the%20latest%20version%20of%20the%20paper%0Afor%20the%20most%20recent%20evaluation%20results%20and%20conclusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520Evaluation%2520of%2520DeepSeek%2520Models%2520in%2520Chinese%2520Contexts%26entry.906535625%3DWenjing%2520Zhang%2520and%2520Xuejiao%2520Lei%2520and%2520Zhaoxiang%2520Liu%2520and%2520Ning%2520Wang%2520and%2520Zhenhong%2520Long%2520and%2520Peijun%2520Yang%2520and%2520Jiaojiao%2520Zhao%2520and%2520Minjie%2520Hua%2520and%2520Chaoyang%2520Ma%2520and%2520Kai%2520Wang%2520and%2520Shiguo%2520Lian%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520DeepSeek%2520series%2520of%2520models%252C%2520leveraging%2520their%2520exceptional%250Areasoning%2520capabilities%2520and%2520open-source%2520strategy%252C%2520is%2520reshaping%2520the%2520global%2520AI%250Alandscape.%2520Despite%2520these%2520advantages%252C%2520they%2520exhibit%2520significant%2520safety%250Adeficiencies.%2520Research%2520conducted%2520by%2520Robust%2520Intelligence%252C%2520a%2520subsidiary%2520of%2520Cisco%252C%250Ain%2520collaboration%2520with%2520the%2520University%2520of%2520Pennsylvania%252C%2520revealed%2520that%2520DeepSeek-R1%250Ahas%2520a%2520100%255C%2525%2520attack%2520success%2520rate%2520when%2520processing%2520harmful%2520prompts.%2520Additionally%252C%250Amultiple%2520safety%2520companies%2520and%2520research%2520institutions%2520have%2520confirmed%2520critical%250Asafety%2520vulnerabilities%2520in%2520this%2520model.%2520As%2520models%2520demonstrating%2520robust%250Aperformance%2520in%2520Chinese%2520and%2520English%252C%2520DeepSeek%2520models%2520require%2520equally%2520crucial%250Asafety%2520assessments%2520in%2520both%2520language%2520contexts.%2520However%252C%2520current%2520research%2520has%250Apredominantly%2520focused%2520on%2520safety%2520evaluations%2520in%2520English%2520environments%252C%2520leaving%2520a%250Agap%2520in%2520comprehensive%2520assessments%2520of%2520their%2520safety%2520performance%2520in%2520Chinese%250Acontexts.%2520In%2520response%2520to%2520this%2520gap%252C%2520this%2520study%2520introduces%2520CHiSafetyBench%252C%2520a%250AChinese-specific%2520safety%2520evaluation%2520benchmark.%2520This%2520benchmark%2520systematically%250Aevaluates%2520the%2520safety%2520of%2520DeepSeek-R1%2520and%2520DeepSeek-V3%2520in%2520Chinese%2520contexts%252C%250Arevealing%2520their%2520performance%2520across%2520safety%2520categories.%2520The%2520experimental%2520results%250Aquantify%2520the%2520deficiencies%2520of%2520these%2520two%2520models%2520in%2520Chinese%2520contexts%252C%2520providing%250Akey%2520insights%2520for%2520subsequent%2520improvements.%2520It%2520should%2520be%2520noted%2520that%252C%2520despite%2520our%250Aefforts%2520to%2520establish%2520a%2520comprehensive%252C%2520objective%252C%2520and%2520authoritative%2520evaluation%250Abenchmark%252C%2520the%2520selection%2520of%2520test%2520samples%252C%2520characteristics%2520of%2520data%2520distribution%252C%250Aand%2520the%2520setting%2520of%2520evaluation%2520criteria%2520may%2520inevitably%2520introduce%2520certain%2520biases%250Ainto%2520the%2520evaluation%2520results.%2520We%2520will%2520continuously%2520optimize%2520the%2520evaluation%250Abenchmark%2520and%2520periodically%2520update%2520this%2520report%2520to%2520provide%2520more%2520comprehensive%2520and%250Aaccurate%2520assessment%2520outcomes.%2520Please%2520refer%2520to%2520the%2520latest%2520version%2520of%2520the%2520paper%250Afor%2520the%2520most%2520recent%2520evaluation%2520results%2520and%2520conclusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20Evaluation%20of%20DeepSeek%20Models%20in%20Chinese%20Contexts&entry.906535625=Wenjing%20Zhang%20and%20Xuejiao%20Lei%20and%20Zhaoxiang%20Liu%20and%20Ning%20Wang%20and%20Zhenhong%20Long%20and%20Peijun%20Yang%20and%20Jiaojiao%20Zhao%20and%20Minjie%20Hua%20and%20Chaoyang%20Ma%20and%20Kai%20Wang%20and%20Shiguo%20Lian&entry.1292438233=%20%20Recently%2C%20the%20DeepSeek%20series%20of%20models%2C%20leveraging%20their%20exceptional%0Areasoning%20capabilities%20and%20open-source%20strategy%2C%20is%20reshaping%20the%20global%20AI%0Alandscape.%20Despite%20these%20advantages%2C%20they%20exhibit%20significant%20safety%0Adeficiencies.%20Research%20conducted%20by%20Robust%20Intelligence%2C%20a%20subsidiary%20of%20Cisco%2C%0Ain%20collaboration%20with%20the%20University%20of%20Pennsylvania%2C%20revealed%20that%20DeepSeek-R1%0Ahas%20a%20100%5C%25%20attack%20success%20rate%20when%20processing%20harmful%20prompts.%20Additionally%2C%0Amultiple%20safety%20companies%20and%20research%20institutions%20have%20confirmed%20critical%0Asafety%20vulnerabilities%20in%20this%20model.%20As%20models%20demonstrating%20robust%0Aperformance%20in%20Chinese%20and%20English%2C%20DeepSeek%20models%20require%20equally%20crucial%0Asafety%20assessments%20in%20both%20language%20contexts.%20However%2C%20current%20research%20has%0Apredominantly%20focused%20on%20safety%20evaluations%20in%20English%20environments%2C%20leaving%20a%0Agap%20in%20comprehensive%20assessments%20of%20their%20safety%20performance%20in%20Chinese%0Acontexts.%20In%20response%20to%20this%20gap%2C%20this%20study%20introduces%20CHiSafetyBench%2C%20a%0AChinese-specific%20safety%20evaluation%20benchmark.%20This%20benchmark%20systematically%0Aevaluates%20the%20safety%20of%20DeepSeek-R1%20and%20DeepSeek-V3%20in%20Chinese%20contexts%2C%0Arevealing%20their%20performance%20across%20safety%20categories.%20The%20experimental%20results%0Aquantify%20the%20deficiencies%20of%20these%20two%20models%20in%20Chinese%20contexts%2C%20providing%0Akey%20insights%20for%20subsequent%20improvements.%20It%20should%20be%20noted%20that%2C%20despite%20our%0Aefforts%20to%20establish%20a%20comprehensive%2C%20objective%2C%20and%20authoritative%20evaluation%0Abenchmark%2C%20the%20selection%20of%20test%20samples%2C%20characteristics%20of%20data%20distribution%2C%0Aand%20the%20setting%20of%20evaluation%20criteria%20may%20inevitably%20introduce%20certain%20biases%0Ainto%20the%20evaluation%20results.%20We%20will%20continuously%20optimize%20the%20evaluation%0Abenchmark%20and%20periodically%20update%20this%20report%20to%20provide%20more%20comprehensive%20and%0Aaccurate%20assessment%20outcomes.%20Please%20refer%20to%20the%20latest%20version%20of%20the%20paper%0Afor%20the%20most%20recent%20evaluation%20results%20and%20conclusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11137v2&entry.124074799=Read"},
{"title": "Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical\n  and Cultural Artifacts", "author": "Sara Ghaboura and Ketan More and Ritesh Thawkar and Wafa Alghallabi and Omkar Thawakar and Fahad Shahbaz Khan and Hisham Cholakkal and Salman Khan and Rao Muhammad Anwer", "abstract": "  Understanding historical and cultural artifacts demands human expertise and\nadvanced computational techniques, yet the process remains complex and\ntime-intensive. While large multimodal models offer promising support, their\nevaluation and improvement require a standardized benchmark. To address this,\nwe introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\n266 distinct cultures across 10 major historical regions. Designed for\nAI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\ndiscoveries, TimeTravel provides a structured dataset and robust evaluation\nframework to assess AI models' capabilities in classification, interpretation,\nand historical comprehension. By integrating AI with historical research,\nTimeTravel fosters AI-powered tools for historians, archaeologists,\nresearchers, and cultural tourists to extract valuable insights while ensuring\ntechnology contributes meaningfully to historical discovery and cultural\nheritage preservation. We evaluate contemporary AI models on TimeTravel,\nhighlighting their strengths and identifying areas for improvement. Our goal is\nto establish AI as a reliable partner in preserving cultural heritage, ensuring\nthat technological advancements contribute meaningfully to historical\ndiscovery. Our code is available at:\n\\url{https://github.com/mbzuai-oryx/TimeTravel}.\n", "link": "http://arxiv.org/abs/2502.14865v1", "date": "2025-02-20", "relevancy": 2.0213, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Travel%3A%20A%20Comprehensive%20Benchmark%20to%20Evaluate%20LMMs%20on%20Historical%0A%20%20and%20Cultural%20Artifacts&body=Title%3A%20Time%20Travel%3A%20A%20Comprehensive%20Benchmark%20to%20Evaluate%20LMMs%20on%20Historical%0A%20%20and%20Cultural%20Artifacts%0AAuthor%3A%20Sara%20Ghaboura%20and%20Ketan%20More%20and%20Ritesh%20Thawkar%20and%20Wafa%20Alghallabi%20and%20Omkar%20Thawakar%20and%20Fahad%20Shahbaz%20Khan%20and%20Hisham%20Cholakkal%20and%20Salman%20Khan%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%20Understanding%20historical%20and%20cultural%20artifacts%20demands%20human%20expertise%20and%0Aadvanced%20computational%20techniques%2C%20yet%20the%20process%20remains%20complex%20and%0Atime-intensive.%20While%20large%20multimodal%20models%20offer%20promising%20support%2C%20their%0Aevaluation%20and%20improvement%20require%20a%20standardized%20benchmark.%20To%20address%20this%2C%0Awe%20introduce%20TimeTravel%2C%20a%20benchmark%20of%2010%2C250%20expert-verified%20samples%20spanning%0A266%20distinct%20cultures%20across%2010%20major%20historical%20regions.%20Designed%20for%0AAI-driven%20analysis%20of%20manuscripts%2C%20artworks%2C%20inscriptions%2C%20and%20archaeological%0Adiscoveries%2C%20TimeTravel%20provides%20a%20structured%20dataset%20and%20robust%20evaluation%0Aframework%20to%20assess%20AI%20models%27%20capabilities%20in%20classification%2C%20interpretation%2C%0Aand%20historical%20comprehension.%20By%20integrating%20AI%20with%20historical%20research%2C%0ATimeTravel%20fosters%20AI-powered%20tools%20for%20historians%2C%20archaeologists%2C%0Aresearchers%2C%20and%20cultural%20tourists%20to%20extract%20valuable%20insights%20while%20ensuring%0Atechnology%20contributes%20meaningfully%20to%20historical%20discovery%20and%20cultural%0Aheritage%20preservation.%20We%20evaluate%20contemporary%20AI%20models%20on%20TimeTravel%2C%0Ahighlighting%20their%20strengths%20and%20identifying%20areas%20for%20improvement.%20Our%20goal%20is%0Ato%20establish%20AI%20as%20a%20reliable%20partner%20in%20preserving%20cultural%20heritage%2C%20ensuring%0Athat%20technological%20advancements%20contribute%20meaningfully%20to%20historical%0Adiscovery.%20Our%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/mbzuai-oryx/TimeTravel%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Travel%253A%2520A%2520Comprehensive%2520Benchmark%2520to%2520Evaluate%2520LMMs%2520on%2520Historical%250A%2520%2520and%2520Cultural%2520Artifacts%26entry.906535625%3DSara%2520Ghaboura%2520and%2520Ketan%2520More%2520and%2520Ritesh%2520Thawkar%2520and%2520Wafa%2520Alghallabi%2520and%2520Omkar%2520Thawakar%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Hisham%2520Cholakkal%2520and%2520Salman%2520Khan%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%2520Understanding%2520historical%2520and%2520cultural%2520artifacts%2520demands%2520human%2520expertise%2520and%250Aadvanced%2520computational%2520techniques%252C%2520yet%2520the%2520process%2520remains%2520complex%2520and%250Atime-intensive.%2520While%2520large%2520multimodal%2520models%2520offer%2520promising%2520support%252C%2520their%250Aevaluation%2520and%2520improvement%2520require%2520a%2520standardized%2520benchmark.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520TimeTravel%252C%2520a%2520benchmark%2520of%252010%252C250%2520expert-verified%2520samples%2520spanning%250A266%2520distinct%2520cultures%2520across%252010%2520major%2520historical%2520regions.%2520Designed%2520for%250AAI-driven%2520analysis%2520of%2520manuscripts%252C%2520artworks%252C%2520inscriptions%252C%2520and%2520archaeological%250Adiscoveries%252C%2520TimeTravel%2520provides%2520a%2520structured%2520dataset%2520and%2520robust%2520evaluation%250Aframework%2520to%2520assess%2520AI%2520models%2527%2520capabilities%2520in%2520classification%252C%2520interpretation%252C%250Aand%2520historical%2520comprehension.%2520By%2520integrating%2520AI%2520with%2520historical%2520research%252C%250ATimeTravel%2520fosters%2520AI-powered%2520tools%2520for%2520historians%252C%2520archaeologists%252C%250Aresearchers%252C%2520and%2520cultural%2520tourists%2520to%2520extract%2520valuable%2520insights%2520while%2520ensuring%250Atechnology%2520contributes%2520meaningfully%2520to%2520historical%2520discovery%2520and%2520cultural%250Aheritage%2520preservation.%2520We%2520evaluate%2520contemporary%2520AI%2520models%2520on%2520TimeTravel%252C%250Ahighlighting%2520their%2520strengths%2520and%2520identifying%2520areas%2520for%2520improvement.%2520Our%2520goal%2520is%250Ato%2520establish%2520AI%2520as%2520a%2520reliable%2520partner%2520in%2520preserving%2520cultural%2520heritage%252C%2520ensuring%250Athat%2520technological%2520advancements%2520contribute%2520meaningfully%2520to%2520historical%250Adiscovery.%2520Our%2520code%2520is%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/mbzuai-oryx/TimeTravel%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Travel%3A%20A%20Comprehensive%20Benchmark%20to%20Evaluate%20LMMs%20on%20Historical%0A%20%20and%20Cultural%20Artifacts&entry.906535625=Sara%20Ghaboura%20and%20Ketan%20More%20and%20Ritesh%20Thawkar%20and%20Wafa%20Alghallabi%20and%20Omkar%20Thawakar%20and%20Fahad%20Shahbaz%20Khan%20and%20Hisham%20Cholakkal%20and%20Salman%20Khan%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%20Understanding%20historical%20and%20cultural%20artifacts%20demands%20human%20expertise%20and%0Aadvanced%20computational%20techniques%2C%20yet%20the%20process%20remains%20complex%20and%0Atime-intensive.%20While%20large%20multimodal%20models%20offer%20promising%20support%2C%20their%0Aevaluation%20and%20improvement%20require%20a%20standardized%20benchmark.%20To%20address%20this%2C%0Awe%20introduce%20TimeTravel%2C%20a%20benchmark%20of%2010%2C250%20expert-verified%20samples%20spanning%0A266%20distinct%20cultures%20across%2010%20major%20historical%20regions.%20Designed%20for%0AAI-driven%20analysis%20of%20manuscripts%2C%20artworks%2C%20inscriptions%2C%20and%20archaeological%0Adiscoveries%2C%20TimeTravel%20provides%20a%20structured%20dataset%20and%20robust%20evaluation%0Aframework%20to%20assess%20AI%20models%27%20capabilities%20in%20classification%2C%20interpretation%2C%0Aand%20historical%20comprehension.%20By%20integrating%20AI%20with%20historical%20research%2C%0ATimeTravel%20fosters%20AI-powered%20tools%20for%20historians%2C%20archaeologists%2C%0Aresearchers%2C%20and%20cultural%20tourists%20to%20extract%20valuable%20insights%20while%20ensuring%0Atechnology%20contributes%20meaningfully%20to%20historical%20discovery%20and%20cultural%0Aheritage%20preservation.%20We%20evaluate%20contemporary%20AI%20models%20on%20TimeTravel%2C%0Ahighlighting%20their%20strengths%20and%20identifying%20areas%20for%20improvement.%20Our%20goal%20is%0Ato%20establish%20AI%20as%20a%20reliable%20partner%20in%20preserving%20cultural%20heritage%2C%20ensuring%0Athat%20technological%20advancements%20contribute%20meaningfully%20to%20historical%0Adiscovery.%20Our%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/mbzuai-oryx/TimeTravel%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14865v1&entry.124074799=Read"},
{"title": "On the Influence of Context Size and Model Choice in Retrieval-Augmented\n  Generation Systems", "author": "Juraj Vladika and Florian Matthes", "abstract": "  Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging.\n", "link": "http://arxiv.org/abs/2502.14759v1", "date": "2025-02-20", "relevancy": 2.0177, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Influence%20of%20Context%20Size%20and%20Model%20Choice%20in%20Retrieval-Augmented%0A%20%20Generation%20Systems&body=Title%3A%20On%20the%20Influence%20of%20Context%20Size%20and%20Model%20Choice%20in%20Retrieval-Augmented%0A%20%20Generation%20Systems%0AAuthor%3A%20Juraj%20Vladika%20and%20Florian%20Matthes%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20emerged%20as%20an%20approach%20to%20augment%0Alarge%20language%20models%20%28LLMs%29%20by%20reducing%20their%20reliance%20on%20static%20knowledge%20and%0Aimproving%20answer%20factuality.%20RAG%20retrieves%20relevant%20context%20snippets%20and%0Agenerates%20an%20answer%20based%20on%20them.%20Despite%20its%20increasing%20industrial%20adoption%2C%0Asystematic%20exploration%20of%20RAG%20components%20is%20lacking%2C%20particularly%20regarding%20the%0Aideal%20size%20of%20provided%20context%2C%20and%20the%20choice%20of%20base%20LLM%20and%20retrieval%0Amethod.%20To%20help%20guide%20development%20of%20robust%20RAG%20systems%2C%20we%20evaluate%20various%0Acontext%20sizes%2C%20BM25%20and%20semantic%20search%20as%20retrievers%2C%20and%20eight%20base%20LLMs.%0AMoving%20away%20from%20the%20usual%20RAG%20evaluation%20with%20short%20answers%2C%20we%20explore%20the%0Amore%20challenging%20long-form%20question%20answering%20in%20two%20domains%2C%20where%20a%20good%0Aanswer%20has%20to%20utilize%20the%20entire%20context.%20Our%20findings%20indicate%20that%20final%20QA%0Aperformance%20improves%20steadily%20with%20up%20to%2015%20snippets%20but%20stagnates%20or%20declines%0Abeyond%20that.%20Finally%2C%20we%20show%20that%20different%20general-purpose%20LLMs%20excel%20in%20the%0Abiomedical%20domain%20than%20the%20encyclopedic%20one%2C%20and%20that%20open-domain%20evidence%0Aretrieval%20in%20large%20corpora%20is%20challenging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Influence%2520of%2520Context%2520Size%2520and%2520Model%2520Choice%2520in%2520Retrieval-Augmented%250A%2520%2520Generation%2520Systems%26entry.906535625%3DJuraj%2520Vladika%2520and%2520Florian%2520Matthes%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520emerged%2520as%2520an%2520approach%2520to%2520augment%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520by%2520reducing%2520their%2520reliance%2520on%2520static%2520knowledge%2520and%250Aimproving%2520answer%2520factuality.%2520RAG%2520retrieves%2520relevant%2520context%2520snippets%2520and%250Agenerates%2520an%2520answer%2520based%2520on%2520them.%2520Despite%2520its%2520increasing%2520industrial%2520adoption%252C%250Asystematic%2520exploration%2520of%2520RAG%2520components%2520is%2520lacking%252C%2520particularly%2520regarding%2520the%250Aideal%2520size%2520of%2520provided%2520context%252C%2520and%2520the%2520choice%2520of%2520base%2520LLM%2520and%2520retrieval%250Amethod.%2520To%2520help%2520guide%2520development%2520of%2520robust%2520RAG%2520systems%252C%2520we%2520evaluate%2520various%250Acontext%2520sizes%252C%2520BM25%2520and%2520semantic%2520search%2520as%2520retrievers%252C%2520and%2520eight%2520base%2520LLMs.%250AMoving%2520away%2520from%2520the%2520usual%2520RAG%2520evaluation%2520with%2520short%2520answers%252C%2520we%2520explore%2520the%250Amore%2520challenging%2520long-form%2520question%2520answering%2520in%2520two%2520domains%252C%2520where%2520a%2520good%250Aanswer%2520has%2520to%2520utilize%2520the%2520entire%2520context.%2520Our%2520findings%2520indicate%2520that%2520final%2520QA%250Aperformance%2520improves%2520steadily%2520with%2520up%2520to%252015%2520snippets%2520but%2520stagnates%2520or%2520declines%250Abeyond%2520that.%2520Finally%252C%2520we%2520show%2520that%2520different%2520general-purpose%2520LLMs%2520excel%2520in%2520the%250Abiomedical%2520domain%2520than%2520the%2520encyclopedic%2520one%252C%2520and%2520that%2520open-domain%2520evidence%250Aretrieval%2520in%2520large%2520corpora%2520is%2520challenging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Influence%20of%20Context%20Size%20and%20Model%20Choice%20in%20Retrieval-Augmented%0A%20%20Generation%20Systems&entry.906535625=Juraj%20Vladika%20and%20Florian%20Matthes&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20emerged%20as%20an%20approach%20to%20augment%0Alarge%20language%20models%20%28LLMs%29%20by%20reducing%20their%20reliance%20on%20static%20knowledge%20and%0Aimproving%20answer%20factuality.%20RAG%20retrieves%20relevant%20context%20snippets%20and%0Agenerates%20an%20answer%20based%20on%20them.%20Despite%20its%20increasing%20industrial%20adoption%2C%0Asystematic%20exploration%20of%20RAG%20components%20is%20lacking%2C%20particularly%20regarding%20the%0Aideal%20size%20of%20provided%20context%2C%20and%20the%20choice%20of%20base%20LLM%20and%20retrieval%0Amethod.%20To%20help%20guide%20development%20of%20robust%20RAG%20systems%2C%20we%20evaluate%20various%0Acontext%20sizes%2C%20BM25%20and%20semantic%20search%20as%20retrievers%2C%20and%20eight%20base%20LLMs.%0AMoving%20away%20from%20the%20usual%20RAG%20evaluation%20with%20short%20answers%2C%20we%20explore%20the%0Amore%20challenging%20long-form%20question%20answering%20in%20two%20domains%2C%20where%20a%20good%0Aanswer%20has%20to%20utilize%20the%20entire%20context.%20Our%20findings%20indicate%20that%20final%20QA%0Aperformance%20improves%20steadily%20with%20up%20to%2015%20snippets%20but%20stagnates%20or%20declines%0Abeyond%20that.%20Finally%2C%20we%20show%20that%20different%20general-purpose%20LLMs%20excel%20in%20the%0Abiomedical%20domain%20than%20the%20encyclopedic%20one%2C%20and%20that%20open-domain%20evidence%0Aretrieval%20in%20large%20corpora%20is%20challenging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14759v1&entry.124074799=Read"},
{"title": "ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by\n  Reducing Data Distribution Errors", "author": "Yuguo Yin and Yuxin Xie and Wenyuan Yang and Dongchao Yang and Jinghan Ru and Xianwei Zhuang and Liming Liang and Yuexian Zou", "abstract": "  Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to\nretrieve audio clips or multilingual texts from databases. However, existing\nML-ATR schemes suffer from inconsistencies for instance similarity matching\nacross languages. We theoretically analyze the inconsistency in terms of both\nmultilingual modal alignment direction error and weight error, and propose the\ntheoretical weight error upper bound for quantifying the inconsistency. Based\non the analysis of the weight error upper bound, we find that the inconsistency\nproblem stems from the data distribution error caused by random sampling of\nlanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastive\nlearning and audio-English co-anchor contrastive learning, aiming to mitigate\nthe negative impact of data distribution error on recall and consistency in\nML-ATR. Experimental results on the translated AudioCaps and Clotho datasets\nshow that our scheme achieves state-of-the-art performance on recall and\nconsistency metrics for eight mainstream languages, including English. Our code\nwill be available at https://github.com/ATRI-ACL/ATRI-ACL.\n", "link": "http://arxiv.org/abs/2502.14627v1", "date": "2025-02-20", "relevancy": 2.0147, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5372}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5038}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATRI%3A%20Mitigating%20Multilingual%20Audio%20Text%20Retrieval%20Inconsistencies%20by%0A%20%20Reducing%20Data%20Distribution%20Errors&body=Title%3A%20ATRI%3A%20Mitigating%20Multilingual%20Audio%20Text%20Retrieval%20Inconsistencies%20by%0A%20%20Reducing%20Data%20Distribution%20Errors%0AAuthor%3A%20Yuguo%20Yin%20and%20Yuxin%20Xie%20and%20Wenyuan%20Yang%20and%20Dongchao%20Yang%20and%20Jinghan%20Ru%20and%20Xianwei%20Zhuang%20and%20Liming%20Liang%20and%20Yuexian%20Zou%0AAbstract%3A%20%20%20Multilingual%20audio-text%20retrieval%20%28ML-ATR%29%20is%20a%20challenging%20task%20that%20aims%20to%0Aretrieve%20audio%20clips%20or%20multilingual%20texts%20from%20databases.%20However%2C%20existing%0AML-ATR%20schemes%20suffer%20from%20inconsistencies%20for%20instance%20similarity%20matching%0Aacross%20languages.%20We%20theoretically%20analyze%20the%20inconsistency%20in%20terms%20of%20both%0Amultilingual%20modal%20alignment%20direction%20error%20and%20weight%20error%2C%20and%20propose%20the%0Atheoretical%20weight%20error%20upper%20bound%20for%20quantifying%20the%20inconsistency.%20Based%0Aon%20the%20analysis%20of%20the%20weight%20error%20upper%20bound%2C%20we%20find%20that%20the%20inconsistency%0Aproblem%20stems%20from%20the%20data%20distribution%20error%20caused%20by%20random%20sampling%20of%0Alanguages.%20We%20propose%20a%20consistent%20ML-ATR%20scheme%20using%201-to-k%20contrastive%0Alearning%20and%20audio-English%20co-anchor%20contrastive%20learning%2C%20aiming%20to%20mitigate%0Athe%20negative%20impact%20of%20data%20distribution%20error%20on%20recall%20and%20consistency%20in%0AML-ATR.%20Experimental%20results%20on%20the%20translated%20AudioCaps%20and%20Clotho%20datasets%0Ashow%20that%20our%20scheme%20achieves%20state-of-the-art%20performance%20on%20recall%20and%0Aconsistency%20metrics%20for%20eight%20mainstream%20languages%2C%20including%20English.%20Our%20code%0Awill%20be%20available%20at%20https%3A//github.com/ATRI-ACL/ATRI-ACL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATRI%253A%2520Mitigating%2520Multilingual%2520Audio%2520Text%2520Retrieval%2520Inconsistencies%2520by%250A%2520%2520Reducing%2520Data%2520Distribution%2520Errors%26entry.906535625%3DYuguo%2520Yin%2520and%2520Yuxin%2520Xie%2520and%2520Wenyuan%2520Yang%2520and%2520Dongchao%2520Yang%2520and%2520Jinghan%2520Ru%2520and%2520Xianwei%2520Zhuang%2520and%2520Liming%2520Liang%2520and%2520Yuexian%2520Zou%26entry.1292438233%3D%2520%2520Multilingual%2520audio-text%2520retrieval%2520%2528ML-ATR%2529%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%250Aretrieve%2520audio%2520clips%2520or%2520multilingual%2520texts%2520from%2520databases.%2520However%252C%2520existing%250AML-ATR%2520schemes%2520suffer%2520from%2520inconsistencies%2520for%2520instance%2520similarity%2520matching%250Aacross%2520languages.%2520We%2520theoretically%2520analyze%2520the%2520inconsistency%2520in%2520terms%2520of%2520both%250Amultilingual%2520modal%2520alignment%2520direction%2520error%2520and%2520weight%2520error%252C%2520and%2520propose%2520the%250Atheoretical%2520weight%2520error%2520upper%2520bound%2520for%2520quantifying%2520the%2520inconsistency.%2520Based%250Aon%2520the%2520analysis%2520of%2520the%2520weight%2520error%2520upper%2520bound%252C%2520we%2520find%2520that%2520the%2520inconsistency%250Aproblem%2520stems%2520from%2520the%2520data%2520distribution%2520error%2520caused%2520by%2520random%2520sampling%2520of%250Alanguages.%2520We%2520propose%2520a%2520consistent%2520ML-ATR%2520scheme%2520using%25201-to-k%2520contrastive%250Alearning%2520and%2520audio-English%2520co-anchor%2520contrastive%2520learning%252C%2520aiming%2520to%2520mitigate%250Athe%2520negative%2520impact%2520of%2520data%2520distribution%2520error%2520on%2520recall%2520and%2520consistency%2520in%250AML-ATR.%2520Experimental%2520results%2520on%2520the%2520translated%2520AudioCaps%2520and%2520Clotho%2520datasets%250Ashow%2520that%2520our%2520scheme%2520achieves%2520state-of-the-art%2520performance%2520on%2520recall%2520and%250Aconsistency%2520metrics%2520for%2520eight%2520mainstream%2520languages%252C%2520including%2520English.%2520Our%2520code%250Awill%2520be%2520available%2520at%2520https%253A//github.com/ATRI-ACL/ATRI-ACL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATRI%3A%20Mitigating%20Multilingual%20Audio%20Text%20Retrieval%20Inconsistencies%20by%0A%20%20Reducing%20Data%20Distribution%20Errors&entry.906535625=Yuguo%20Yin%20and%20Yuxin%20Xie%20and%20Wenyuan%20Yang%20and%20Dongchao%20Yang%20and%20Jinghan%20Ru%20and%20Xianwei%20Zhuang%20and%20Liming%20Liang%20and%20Yuexian%20Zou&entry.1292438233=%20%20Multilingual%20audio-text%20retrieval%20%28ML-ATR%29%20is%20a%20challenging%20task%20that%20aims%20to%0Aretrieve%20audio%20clips%20or%20multilingual%20texts%20from%20databases.%20However%2C%20existing%0AML-ATR%20schemes%20suffer%20from%20inconsistencies%20for%20instance%20similarity%20matching%0Aacross%20languages.%20We%20theoretically%20analyze%20the%20inconsistency%20in%20terms%20of%20both%0Amultilingual%20modal%20alignment%20direction%20error%20and%20weight%20error%2C%20and%20propose%20the%0Atheoretical%20weight%20error%20upper%20bound%20for%20quantifying%20the%20inconsistency.%20Based%0Aon%20the%20analysis%20of%20the%20weight%20error%20upper%20bound%2C%20we%20find%20that%20the%20inconsistency%0Aproblem%20stems%20from%20the%20data%20distribution%20error%20caused%20by%20random%20sampling%20of%0Alanguages.%20We%20propose%20a%20consistent%20ML-ATR%20scheme%20using%201-to-k%20contrastive%0Alearning%20and%20audio-English%20co-anchor%20contrastive%20learning%2C%20aiming%20to%20mitigate%0Athe%20negative%20impact%20of%20data%20distribution%20error%20on%20recall%20and%20consistency%20in%0AML-ATR.%20Experimental%20results%20on%20the%20translated%20AudioCaps%20and%20Clotho%20datasets%0Ashow%20that%20our%20scheme%20achieves%20state-of-the-art%20performance%20on%20recall%20and%0Aconsistency%20metrics%20for%20eight%20mainstream%20languages%2C%20including%20English.%20Our%20code%0Awill%20be%20available%20at%20https%3A//github.com/ATRI-ACL/ATRI-ACL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14627v1&entry.124074799=Read"},
{"title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning", "author": "Tian Xie and Zitian Gao and Qingnan Ren and Haoming Luo and Yuqian Hong and Bryan Dai and Joey Zhou and Kai Qiu and Zhirong Wu and Chong Luo", "abstract": "  Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.\n", "link": "http://arxiv.org/abs/2502.14768v1", "date": "2025-02-20", "relevancy": 2.0063, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logic-RL%3A%20Unleashing%20LLM%20Reasoning%20with%20Rule-Based%20Reinforcement%0A%20%20Learning&body=Title%3A%20Logic-RL%3A%20Unleashing%20LLM%20Reasoning%20with%20Rule-Based%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Tian%20Xie%20and%20Zitian%20Gao%20and%20Qingnan%20Ren%20and%20Haoming%20Luo%20and%20Yuqian%20Hong%20and%20Bryan%20Dai%20and%20Joey%20Zhou%20and%20Kai%20Qiu%20and%20Zhirong%20Wu%20and%20Chong%20Luo%0AAbstract%3A%20%20%20Inspired%20by%20the%20success%20of%20DeepSeek-R1%2C%20we%20explore%20the%20potential%20of%0Arule-based%20reinforcement%20learning%20%28RL%29%20in%20large%20reasoning%20models.%20To%20analyze%0Areasoning%20dynamics%2C%20we%20use%20synthetic%20logic%20puzzles%20as%20training%20data%20due%20to%0Atheir%20controllable%20complexity%20and%20straightforward%20answer%20verification.%20We%20make%0Asome%20key%20technical%20contributions%20that%20lead%20to%20effective%20and%20stable%20RL%20training%3A%0Aa%20system%20prompt%20that%20emphasizes%20the%20thinking%20and%20answering%20process%2C%20a%20stringent%0Aformat%20reward%20function%20that%20penalizes%20outputs%20for%20taking%20shortcuts%2C%20and%20a%0Astraightforward%20training%20recipe%20that%20achieves%20stable%20convergence.%20Our%207B%20model%0Adevelops%20advanced%20reasoning%20skills-such%20as%20reflection%2C%20verification%2C%20and%0Asummarization-that%20are%20absent%20from%20the%20logic%20corpus.%20Remarkably%2C%20after%20training%0Aon%20just%205K%20logic%20problems%2C%20it%20demonstrates%20generalization%20abilities%20to%20the%0Achallenging%20math%20benchmarks%20AIME%20and%20AMC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogic-RL%253A%2520Unleashing%2520LLM%2520Reasoning%2520with%2520Rule-Based%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DTian%2520Xie%2520and%2520Zitian%2520Gao%2520and%2520Qingnan%2520Ren%2520and%2520Haoming%2520Luo%2520and%2520Yuqian%2520Hong%2520and%2520Bryan%2520Dai%2520and%2520Joey%2520Zhou%2520and%2520Kai%2520Qiu%2520and%2520Zhirong%2520Wu%2520and%2520Chong%2520Luo%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520success%2520of%2520DeepSeek-R1%252C%2520we%2520explore%2520the%2520potential%2520of%250Arule-based%2520reinforcement%2520learning%2520%2528RL%2529%2520in%2520large%2520reasoning%2520models.%2520To%2520analyze%250Areasoning%2520dynamics%252C%2520we%2520use%2520synthetic%2520logic%2520puzzles%2520as%2520training%2520data%2520due%2520to%250Atheir%2520controllable%2520complexity%2520and%2520straightforward%2520answer%2520verification.%2520We%2520make%250Asome%2520key%2520technical%2520contributions%2520that%2520lead%2520to%2520effective%2520and%2520stable%2520RL%2520training%253A%250Aa%2520system%2520prompt%2520that%2520emphasizes%2520the%2520thinking%2520and%2520answering%2520process%252C%2520a%2520stringent%250Aformat%2520reward%2520function%2520that%2520penalizes%2520outputs%2520for%2520taking%2520shortcuts%252C%2520and%2520a%250Astraightforward%2520training%2520recipe%2520that%2520achieves%2520stable%2520convergence.%2520Our%25207B%2520model%250Adevelops%2520advanced%2520reasoning%2520skills-such%2520as%2520reflection%252C%2520verification%252C%2520and%250Asummarization-that%2520are%2520absent%2520from%2520the%2520logic%2520corpus.%2520Remarkably%252C%2520after%2520training%250Aon%2520just%25205K%2520logic%2520problems%252C%2520it%2520demonstrates%2520generalization%2520abilities%2520to%2520the%250Achallenging%2520math%2520benchmarks%2520AIME%2520and%2520AMC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logic-RL%3A%20Unleashing%20LLM%20Reasoning%20with%20Rule-Based%20Reinforcement%0A%20%20Learning&entry.906535625=Tian%20Xie%20and%20Zitian%20Gao%20and%20Qingnan%20Ren%20and%20Haoming%20Luo%20and%20Yuqian%20Hong%20and%20Bryan%20Dai%20and%20Joey%20Zhou%20and%20Kai%20Qiu%20and%20Zhirong%20Wu%20and%20Chong%20Luo&entry.1292438233=%20%20Inspired%20by%20the%20success%20of%20DeepSeek-R1%2C%20we%20explore%20the%20potential%20of%0Arule-based%20reinforcement%20learning%20%28RL%29%20in%20large%20reasoning%20models.%20To%20analyze%0Areasoning%20dynamics%2C%20we%20use%20synthetic%20logic%20puzzles%20as%20training%20data%20due%20to%0Atheir%20controllable%20complexity%20and%20straightforward%20answer%20verification.%20We%20make%0Asome%20key%20technical%20contributions%20that%20lead%20to%20effective%20and%20stable%20RL%20training%3A%0Aa%20system%20prompt%20that%20emphasizes%20the%20thinking%20and%20answering%20process%2C%20a%20stringent%0Aformat%20reward%20function%20that%20penalizes%20outputs%20for%20taking%20shortcuts%2C%20and%20a%0Astraightforward%20training%20recipe%20that%20achieves%20stable%20convergence.%20Our%207B%20model%0Adevelops%20advanced%20reasoning%20skills-such%20as%20reflection%2C%20verification%2C%20and%0Asummarization-that%20are%20absent%20from%20the%20logic%20corpus.%20Remarkably%2C%20after%20training%0Aon%20just%205K%20logic%20problems%2C%20it%20demonstrates%20generalization%20abilities%20to%20the%0Achallenging%20math%20benchmarks%20AIME%20and%20AMC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14768v1&entry.124074799=Read"},
{"title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models", "author": "Alina Shutova and Vladimir Malinovskii and Vage Egiazarian and Denis Kuznedelev and Denis Mazur and Nikita Surkov and Ivan Ermakov and Dan Alistarh", "abstract": "  Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.\n", "link": "http://arxiv.org/abs/2501.19392v3", "date": "2025-02-20", "relevancy": 2.0009, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5183}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cache%20Me%20If%20You%20Must%3A%20Adaptive%20Key-Value%20Quantization%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20Cache%20Me%20If%20You%20Must%3A%20Adaptive%20Key-Value%20Quantization%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Alina%20Shutova%20and%20Vladimir%20Malinovskii%20and%20Vage%20Egiazarian%20and%20Denis%20Kuznedelev%20and%20Denis%20Mazur%20and%20Nikita%20Surkov%20and%20Ivan%20Ermakov%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20Efficient%20real-world%20deployments%20of%20large%20language%20models%20%28LLMs%29%20rely%20on%0AKey-Value%20%28KV%29%20caching%20for%20processing%20and%20generating%20long%20outputs%2C%20reducing%20the%0Aneed%20for%20repetitive%20computation.%20For%20large%20contexts%2C%20Key-Value%20caches%20can%20take%0Aup%20tens%20of%20gigabytes%20of%20device%20memory%2C%20as%20they%20store%20vector%20representations%20for%0Aeach%20token%20and%20layer.%20Recent%20work%20has%20shown%20that%20the%20cached%20vectors%20can%20be%0Acompressed%20through%20quantization%2C%20pruning%20or%20merging%2C%20but%20these%20techniques%20often%0Acompromise%20quality%20towards%20higher%20compression%20rates.%20In%20this%20work%2C%20we%20aim%20to%0Aimprove%20Key%20%26%20Value%20compression%20by%20exploiting%20two%20observations%3A%201%29%20the%20inherent%0Adependencies%20between%20keys%20and%20values%20across%20different%20layers%2C%20and%202%29%0Ahigh-compression%20mechanisms%20for%20internal%20network%20states.%20We%20propose%20AQUA-KV%2C%20an%0Aadaptive%20quantization%20for%20Key-Value%20caches%20that%20relies%20on%20compact%20adapters%20to%0Aexploit%20existing%20dependencies%20between%20Keys%20and%20Values%2C%20and%20aims%20to%20%22optimally%22%0Acompress%20the%20information%20that%20cannot%20be%20predicted.%20AQUA-KV%20significantly%0Aimproves%20compression%20rates%2C%20while%20maintaining%20high%20accuracy%20on%20state-of-the-art%0ALLM%20families.%20On%20Llama%203.2%20LLMs%2C%20we%20achieve%20near-lossless%20inference%20at%202-2.5%0Abits%20per%20value%20with%20under%20%241%5C%25%24%20relative%20error%20in%20perplexity%20and%20LongBench%0Ascores.%20AQUA-KV%20is%20one-shot%2C%20simple%2C%20and%20efficient%3A%20it%20can%20be%20calibrated%20on%20a%0Asingle%20GPU%20within%201-6%20hours%2C%20even%20for%2070B%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19392v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCache%2520Me%2520If%2520You%2520Must%253A%2520Adaptive%2520Key-Value%2520Quantization%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DAlina%2520Shutova%2520and%2520Vladimir%2520Malinovskii%2520and%2520Vage%2520Egiazarian%2520and%2520Denis%2520Kuznedelev%2520and%2520Denis%2520Mazur%2520and%2520Nikita%2520Surkov%2520and%2520Ivan%2520Ermakov%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520Efficient%2520real-world%2520deployments%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520rely%2520on%250AKey-Value%2520%2528KV%2529%2520caching%2520for%2520processing%2520and%2520generating%2520long%2520outputs%252C%2520reducing%2520the%250Aneed%2520for%2520repetitive%2520computation.%2520For%2520large%2520contexts%252C%2520Key-Value%2520caches%2520can%2520take%250Aup%2520tens%2520of%2520gigabytes%2520of%2520device%2520memory%252C%2520as%2520they%2520store%2520vector%2520representations%2520for%250Aeach%2520token%2520and%2520layer.%2520Recent%2520work%2520has%2520shown%2520that%2520the%2520cached%2520vectors%2520can%2520be%250Acompressed%2520through%2520quantization%252C%2520pruning%2520or%2520merging%252C%2520but%2520these%2520techniques%2520often%250Acompromise%2520quality%2520towards%2520higher%2520compression%2520rates.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%250Aimprove%2520Key%2520%2526%2520Value%2520compression%2520by%2520exploiting%2520two%2520observations%253A%25201%2529%2520the%2520inherent%250Adependencies%2520between%2520keys%2520and%2520values%2520across%2520different%2520layers%252C%2520and%25202%2529%250Ahigh-compression%2520mechanisms%2520for%2520internal%2520network%2520states.%2520We%2520propose%2520AQUA-KV%252C%2520an%250Aadaptive%2520quantization%2520for%2520Key-Value%2520caches%2520that%2520relies%2520on%2520compact%2520adapters%2520to%250Aexploit%2520existing%2520dependencies%2520between%2520Keys%2520and%2520Values%252C%2520and%2520aims%2520to%2520%2522optimally%2522%250Acompress%2520the%2520information%2520that%2520cannot%2520be%2520predicted.%2520AQUA-KV%2520significantly%250Aimproves%2520compression%2520rates%252C%2520while%2520maintaining%2520high%2520accuracy%2520on%2520state-of-the-art%250ALLM%2520families.%2520On%2520Llama%25203.2%2520LLMs%252C%2520we%2520achieve%2520near-lossless%2520inference%2520at%25202-2.5%250Abits%2520per%2520value%2520with%2520under%2520%25241%255C%2525%2524%2520relative%2520error%2520in%2520perplexity%2520and%2520LongBench%250Ascores.%2520AQUA-KV%2520is%2520one-shot%252C%2520simple%252C%2520and%2520efficient%253A%2520it%2520can%2520be%2520calibrated%2520on%2520a%250Asingle%2520GPU%2520within%25201-6%2520hours%252C%2520even%2520for%252070B%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19392v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cache%20Me%20If%20You%20Must%3A%20Adaptive%20Key-Value%20Quantization%20for%20Large%20Language%0A%20%20Models&entry.906535625=Alina%20Shutova%20and%20Vladimir%20Malinovskii%20and%20Vage%20Egiazarian%20and%20Denis%20Kuznedelev%20and%20Denis%20Mazur%20and%20Nikita%20Surkov%20and%20Ivan%20Ermakov%20and%20Dan%20Alistarh&entry.1292438233=%20%20Efficient%20real-world%20deployments%20of%20large%20language%20models%20%28LLMs%29%20rely%20on%0AKey-Value%20%28KV%29%20caching%20for%20processing%20and%20generating%20long%20outputs%2C%20reducing%20the%0Aneed%20for%20repetitive%20computation.%20For%20large%20contexts%2C%20Key-Value%20caches%20can%20take%0Aup%20tens%20of%20gigabytes%20of%20device%20memory%2C%20as%20they%20store%20vector%20representations%20for%0Aeach%20token%20and%20layer.%20Recent%20work%20has%20shown%20that%20the%20cached%20vectors%20can%20be%0Acompressed%20through%20quantization%2C%20pruning%20or%20merging%2C%20but%20these%20techniques%20often%0Acompromise%20quality%20towards%20higher%20compression%20rates.%20In%20this%20work%2C%20we%20aim%20to%0Aimprove%20Key%20%26%20Value%20compression%20by%20exploiting%20two%20observations%3A%201%29%20the%20inherent%0Adependencies%20between%20keys%20and%20values%20across%20different%20layers%2C%20and%202%29%0Ahigh-compression%20mechanisms%20for%20internal%20network%20states.%20We%20propose%20AQUA-KV%2C%20an%0Aadaptive%20quantization%20for%20Key-Value%20caches%20that%20relies%20on%20compact%20adapters%20to%0Aexploit%20existing%20dependencies%20between%20Keys%20and%20Values%2C%20and%20aims%20to%20%22optimally%22%0Acompress%20the%20information%20that%20cannot%20be%20predicted.%20AQUA-KV%20significantly%0Aimproves%20compression%20rates%2C%20while%20maintaining%20high%20accuracy%20on%20state-of-the-art%0ALLM%20families.%20On%20Llama%203.2%20LLMs%2C%20we%20achieve%20near-lossless%20inference%20at%202-2.5%0Abits%20per%20value%20with%20under%20%241%5C%25%24%20relative%20error%20in%20perplexity%20and%20LongBench%0Ascores.%20AQUA-KV%20is%20one-shot%2C%20simple%2C%20and%20efficient%3A%20it%20can%20be%20calibrated%20on%20a%0Asingle%20GPU%20within%201-6%20hours%2C%20even%20for%2070B%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19392v3&entry.124074799=Read"},
{"title": "Real-world Troublemaker: A Novel Track Testing Framework for Automated\n  Driving Systems in Safety-critical Interaction Scenarios", "author": "Xinrui Zhang and Lu Xiong and Peizhi Zhang and Junpeng Huang and Yining Ma", "abstract": "  Track testing plays a critical role in the safety evaluation of autonomous\ndriving systems (ADS), as it provides real-world object targets and a\nsafety-controllable interaction environment. However, existing track testing\nscenarios are often pre-fixed and limited, primarily due to the inflexibility\nof object target control methods and the lack of intelligent interactive\nbehaviors. To overcome this limitation, we propose a novel track testing\nframework, Real-world Troublemaker, which can generate adversarial object\ntarget motion trajectories and facilitate intelligent interactions with the\nvehicle under test (VUT), creating a more realistic and dynamic testing\nenvironment. To enable flexible motion trajectories, cloud-controlled\ntechnology is utilized to remotely and dynamically control object targets to\ncreate a realistic traffic environment. To achieve intelligent interactions, an\ninteractive concrete scenario generation method is introduced within a\ngame-theoretic structure. The proposed framework has been successfully\nimplemented at the Tongji University Intelligent Connected Vehicle Evaluation\nBase. Field test results demonstrate that Troublemaker can perform dynamic\ninteractive testing of ADS accurately and effectively. Compared to traditional\ntrack testing methods, Troublemaker improves scenario reproduction accuracy by\n65.2\\%, increases the diversity of target vehicle interaction strategies by\napproximately 9.2 times, and enhances exposure frequency of safety-critical\nscenarios by 3.5 times in unprotected left-turn scenarios.\n", "link": "http://arxiv.org/abs/2502.14574v1", "date": "2025-02-20", "relevancy": 1.9967, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5001}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.499}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-world%20Troublemaker%3A%20A%20Novel%20Track%20Testing%20Framework%20for%20Automated%0A%20%20Driving%20Systems%20in%20Safety-critical%20Interaction%20Scenarios&body=Title%3A%20Real-world%20Troublemaker%3A%20A%20Novel%20Track%20Testing%20Framework%20for%20Automated%0A%20%20Driving%20Systems%20in%20Safety-critical%20Interaction%20Scenarios%0AAuthor%3A%20Xinrui%20Zhang%20and%20Lu%20Xiong%20and%20Peizhi%20Zhang%20and%20Junpeng%20Huang%20and%20Yining%20Ma%0AAbstract%3A%20%20%20Track%20testing%20plays%20a%20critical%20role%20in%20the%20safety%20evaluation%20of%20autonomous%0Adriving%20systems%20%28ADS%29%2C%20as%20it%20provides%20real-world%20object%20targets%20and%20a%0Asafety-controllable%20interaction%20environment.%20However%2C%20existing%20track%20testing%0Ascenarios%20are%20often%20pre-fixed%20and%20limited%2C%20primarily%20due%20to%20the%20inflexibility%0Aof%20object%20target%20control%20methods%20and%20the%20lack%20of%20intelligent%20interactive%0Abehaviors.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20track%20testing%0Aframework%2C%20Real-world%20Troublemaker%2C%20which%20can%20generate%20adversarial%20object%0Atarget%20motion%20trajectories%20and%20facilitate%20intelligent%20interactions%20with%20the%0Avehicle%20under%20test%20%28VUT%29%2C%20creating%20a%20more%20realistic%20and%20dynamic%20testing%0Aenvironment.%20To%20enable%20flexible%20motion%20trajectories%2C%20cloud-controlled%0Atechnology%20is%20utilized%20to%20remotely%20and%20dynamically%20control%20object%20targets%20to%0Acreate%20a%20realistic%20traffic%20environment.%20To%20achieve%20intelligent%20interactions%2C%20an%0Ainteractive%20concrete%20scenario%20generation%20method%20is%20introduced%20within%20a%0Agame-theoretic%20structure.%20The%20proposed%20framework%20has%20been%20successfully%0Aimplemented%20at%20the%20Tongji%20University%20Intelligent%20Connected%20Vehicle%20Evaluation%0ABase.%20Field%20test%20results%20demonstrate%20that%20Troublemaker%20can%20perform%20dynamic%0Ainteractive%20testing%20of%20ADS%20accurately%20and%20effectively.%20Compared%20to%20traditional%0Atrack%20testing%20methods%2C%20Troublemaker%20improves%20scenario%20reproduction%20accuracy%20by%0A65.2%5C%25%2C%20increases%20the%20diversity%20of%20target%20vehicle%20interaction%20strategies%20by%0Aapproximately%209.2%20times%2C%20and%20enhances%20exposure%20frequency%20of%20safety-critical%0Ascenarios%20by%203.5%20times%20in%20unprotected%20left-turn%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-world%2520Troublemaker%253A%2520A%2520Novel%2520Track%2520Testing%2520Framework%2520for%2520Automated%250A%2520%2520Driving%2520Systems%2520in%2520Safety-critical%2520Interaction%2520Scenarios%26entry.906535625%3DXinrui%2520Zhang%2520and%2520Lu%2520Xiong%2520and%2520Peizhi%2520Zhang%2520and%2520Junpeng%2520Huang%2520and%2520Yining%2520Ma%26entry.1292438233%3D%2520%2520Track%2520testing%2520plays%2520a%2520critical%2520role%2520in%2520the%2520safety%2520evaluation%2520of%2520autonomous%250Adriving%2520systems%2520%2528ADS%2529%252C%2520as%2520it%2520provides%2520real-world%2520object%2520targets%2520and%2520a%250Asafety-controllable%2520interaction%2520environment.%2520However%252C%2520existing%2520track%2520testing%250Ascenarios%2520are%2520often%2520pre-fixed%2520and%2520limited%252C%2520primarily%2520due%2520to%2520the%2520inflexibility%250Aof%2520object%2520target%2520control%2520methods%2520and%2520the%2520lack%2520of%2520intelligent%2520interactive%250Abehaviors.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520track%2520testing%250Aframework%252C%2520Real-world%2520Troublemaker%252C%2520which%2520can%2520generate%2520adversarial%2520object%250Atarget%2520motion%2520trajectories%2520and%2520facilitate%2520intelligent%2520interactions%2520with%2520the%250Avehicle%2520under%2520test%2520%2528VUT%2529%252C%2520creating%2520a%2520more%2520realistic%2520and%2520dynamic%2520testing%250Aenvironment.%2520To%2520enable%2520flexible%2520motion%2520trajectories%252C%2520cloud-controlled%250Atechnology%2520is%2520utilized%2520to%2520remotely%2520and%2520dynamically%2520control%2520object%2520targets%2520to%250Acreate%2520a%2520realistic%2520traffic%2520environment.%2520To%2520achieve%2520intelligent%2520interactions%252C%2520an%250Ainteractive%2520concrete%2520scenario%2520generation%2520method%2520is%2520introduced%2520within%2520a%250Agame-theoretic%2520structure.%2520The%2520proposed%2520framework%2520has%2520been%2520successfully%250Aimplemented%2520at%2520the%2520Tongji%2520University%2520Intelligent%2520Connected%2520Vehicle%2520Evaluation%250ABase.%2520Field%2520test%2520results%2520demonstrate%2520that%2520Troublemaker%2520can%2520perform%2520dynamic%250Ainteractive%2520testing%2520of%2520ADS%2520accurately%2520and%2520effectively.%2520Compared%2520to%2520traditional%250Atrack%2520testing%2520methods%252C%2520Troublemaker%2520improves%2520scenario%2520reproduction%2520accuracy%2520by%250A65.2%255C%2525%252C%2520increases%2520the%2520diversity%2520of%2520target%2520vehicle%2520interaction%2520strategies%2520by%250Aapproximately%25209.2%2520times%252C%2520and%2520enhances%2520exposure%2520frequency%2520of%2520safety-critical%250Ascenarios%2520by%25203.5%2520times%2520in%2520unprotected%2520left-turn%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-world%20Troublemaker%3A%20A%20Novel%20Track%20Testing%20Framework%20for%20Automated%0A%20%20Driving%20Systems%20in%20Safety-critical%20Interaction%20Scenarios&entry.906535625=Xinrui%20Zhang%20and%20Lu%20Xiong%20and%20Peizhi%20Zhang%20and%20Junpeng%20Huang%20and%20Yining%20Ma&entry.1292438233=%20%20Track%20testing%20plays%20a%20critical%20role%20in%20the%20safety%20evaluation%20of%20autonomous%0Adriving%20systems%20%28ADS%29%2C%20as%20it%20provides%20real-world%20object%20targets%20and%20a%0Asafety-controllable%20interaction%20environment.%20However%2C%20existing%20track%20testing%0Ascenarios%20are%20often%20pre-fixed%20and%20limited%2C%20primarily%20due%20to%20the%20inflexibility%0Aof%20object%20target%20control%20methods%20and%20the%20lack%20of%20intelligent%20interactive%0Abehaviors.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20track%20testing%0Aframework%2C%20Real-world%20Troublemaker%2C%20which%20can%20generate%20adversarial%20object%0Atarget%20motion%20trajectories%20and%20facilitate%20intelligent%20interactions%20with%20the%0Avehicle%20under%20test%20%28VUT%29%2C%20creating%20a%20more%20realistic%20and%20dynamic%20testing%0Aenvironment.%20To%20enable%20flexible%20motion%20trajectories%2C%20cloud-controlled%0Atechnology%20is%20utilized%20to%20remotely%20and%20dynamically%20control%20object%20targets%20to%0Acreate%20a%20realistic%20traffic%20environment.%20To%20achieve%20intelligent%20interactions%2C%20an%0Ainteractive%20concrete%20scenario%20generation%20method%20is%20introduced%20within%20a%0Agame-theoretic%20structure.%20The%20proposed%20framework%20has%20been%20successfully%0Aimplemented%20at%20the%20Tongji%20University%20Intelligent%20Connected%20Vehicle%20Evaluation%0ABase.%20Field%20test%20results%20demonstrate%20that%20Troublemaker%20can%20perform%20dynamic%0Ainteractive%20testing%20of%20ADS%20accurately%20and%20effectively.%20Compared%20to%20traditional%0Atrack%20testing%20methods%2C%20Troublemaker%20improves%20scenario%20reproduction%20accuracy%20by%0A65.2%5C%25%2C%20increases%20the%20diversity%20of%20target%20vehicle%20interaction%20strategies%20by%0Aapproximately%209.2%20times%2C%20and%20enhances%20exposure%20frequency%20of%20safety-critical%0Ascenarios%20by%203.5%20times%20in%20unprotected%20left-turn%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14574v1&entry.124074799=Read"},
{"title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables", "author": "Luis Antonio Guti\u00e9rrez Guanilo and Mir Tafseer Nayeem and Cristian L\u00f3pez and Davood Rafiei", "abstract": "  Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges.\n", "link": "http://arxiv.org/abs/2502.14820v1", "date": "2025-02-20", "relevancy": 1.9833, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5079}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4991}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20eC-Tab2Text%3A%20Aspect-Based%20Text%20Generation%20from%20e-Commerce%20Product%20Tables&body=Title%3A%20eC-Tab2Text%3A%20Aspect-Based%20Text%20Generation%20from%20e-Commerce%20Product%20Tables%0AAuthor%3A%20Luis%20Antonio%20Guti%C3%A9rrez%20Guanilo%20and%20Mir%20Tafseer%20Nayeem%20and%20Cristian%20L%C3%B3pez%20and%20Davood%20Rafiei%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20exceptional%20versatility%20across%0Adiverse%20domains%2C%20yet%20their%20application%20in%20e-commerce%20remains%20underexplored%20due%0Ato%20a%20lack%20of%20domain-specific%20datasets.%20To%20address%20this%20gap%2C%20we%20introduce%0AeC-Tab2Text%2C%20a%20novel%20dataset%20designed%20to%20capture%20the%20intricacies%20of%20e-commerce%2C%0Aincluding%20detailed%20product%20attributes%20and%20user-specific%20queries.%20Leveraging%0AeC-Tab2Text%2C%20we%20focus%20on%20text%20generation%20from%20product%20tables%2C%20enabling%20LLMs%20to%0Aproduce%20high-quality%2C%20attribute-specific%20product%20reviews%20from%20structured%0Atabular%20data.%20Fine-tuned%20models%20were%20rigorously%20evaluated%20using%20standard%0ATable2Text%20metrics%2C%20alongside%20correctness%2C%20faithfulness%2C%20and%20fluency%0Aassessments.%20Our%20results%20demonstrate%20substantial%20improvements%20in%20generating%0Acontextually%20accurate%20reviews%2C%20highlighting%20the%20transformative%20potential%20of%0Atailored%20datasets%20and%20fine-tuning%20methodologies%20in%20optimizing%20e-commerce%0Aworkflows.%20This%20work%20highlights%20the%20potential%20of%20LLMs%20in%20e-commerce%20workflows%0Aand%20the%20essential%20role%20of%20domain-specific%20datasets%20in%20tailoring%20them%20to%0Aindustry-specific%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DeC-Tab2Text%253A%2520Aspect-Based%2520Text%2520Generation%2520from%2520e-Commerce%2520Product%2520Tables%26entry.906535625%3DLuis%2520Antonio%2520Guti%25C3%25A9rrez%2520Guanilo%2520and%2520Mir%2520Tafseer%2520Nayeem%2520and%2520Cristian%2520L%25C3%25B3pez%2520and%2520Davood%2520Rafiei%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520exceptional%2520versatility%2520across%250Adiverse%2520domains%252C%2520yet%2520their%2520application%2520in%2520e-commerce%2520remains%2520underexplored%2520due%250Ato%2520a%2520lack%2520of%2520domain-specific%2520datasets.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AeC-Tab2Text%252C%2520a%2520novel%2520dataset%2520designed%2520to%2520capture%2520the%2520intricacies%2520of%2520e-commerce%252C%250Aincluding%2520detailed%2520product%2520attributes%2520and%2520user-specific%2520queries.%2520Leveraging%250AeC-Tab2Text%252C%2520we%2520focus%2520on%2520text%2520generation%2520from%2520product%2520tables%252C%2520enabling%2520LLMs%2520to%250Aproduce%2520high-quality%252C%2520attribute-specific%2520product%2520reviews%2520from%2520structured%250Atabular%2520data.%2520Fine-tuned%2520models%2520were%2520rigorously%2520evaluated%2520using%2520standard%250ATable2Text%2520metrics%252C%2520alongside%2520correctness%252C%2520faithfulness%252C%2520and%2520fluency%250Aassessments.%2520Our%2520results%2520demonstrate%2520substantial%2520improvements%2520in%2520generating%250Acontextually%2520accurate%2520reviews%252C%2520highlighting%2520the%2520transformative%2520potential%2520of%250Atailored%2520datasets%2520and%2520fine-tuning%2520methodologies%2520in%2520optimizing%2520e-commerce%250Aworkflows.%2520This%2520work%2520highlights%2520the%2520potential%2520of%2520LLMs%2520in%2520e-commerce%2520workflows%250Aand%2520the%2520essential%2520role%2520of%2520domain-specific%2520datasets%2520in%2520tailoring%2520them%2520to%250Aindustry-specific%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=eC-Tab2Text%3A%20Aspect-Based%20Text%20Generation%20from%20e-Commerce%20Product%20Tables&entry.906535625=Luis%20Antonio%20Guti%C3%A9rrez%20Guanilo%20and%20Mir%20Tafseer%20Nayeem%20and%20Cristian%20L%C3%B3pez%20and%20Davood%20Rafiei&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20exceptional%20versatility%20across%0Adiverse%20domains%2C%20yet%20their%20application%20in%20e-commerce%20remains%20underexplored%20due%0Ato%20a%20lack%20of%20domain-specific%20datasets.%20To%20address%20this%20gap%2C%20we%20introduce%0AeC-Tab2Text%2C%20a%20novel%20dataset%20designed%20to%20capture%20the%20intricacies%20of%20e-commerce%2C%0Aincluding%20detailed%20product%20attributes%20and%20user-specific%20queries.%20Leveraging%0AeC-Tab2Text%2C%20we%20focus%20on%20text%20generation%20from%20product%20tables%2C%20enabling%20LLMs%20to%0Aproduce%20high-quality%2C%20attribute-specific%20product%20reviews%20from%20structured%0Atabular%20data.%20Fine-tuned%20models%20were%20rigorously%20evaluated%20using%20standard%0ATable2Text%20metrics%2C%20alongside%20correctness%2C%20faithfulness%2C%20and%20fluency%0Aassessments.%20Our%20results%20demonstrate%20substantial%20improvements%20in%20generating%0Acontextually%20accurate%20reviews%2C%20highlighting%20the%20transformative%20potential%20of%0Atailored%20datasets%20and%20fine-tuning%20methodologies%20in%20optimizing%20e-commerce%0Aworkflows.%20This%20work%20highlights%20the%20potential%20of%20LLMs%20in%20e-commerce%20workflows%0Aand%20the%20essential%20role%20of%20domain-specific%20datasets%20in%20tailoring%20them%20to%0Aindustry-specific%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14820v1&entry.124074799=Read"},
{"title": "Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math\n  and science problems", "author": "Ernest Davis and Scott Aaronson", "abstract": "  This report describes a test of the large language model GPT-4 with the\nWolfram Alpha and the Code Interpreter plug-ins on 105 original problems in\nscience and math, at the high school and college levels, carried out in\nJune-August 2023. Our tests suggest that the plug-ins significantly enhance\nGPT's ability to solve these problems. Having said that, there are still often\n\"interface\" failures; that is, GPT often has trouble formulating problems in a\nway that elicits useful answers from the plug-ins. Fixing these interface\nfailures seems like a central challenge in making GPT a reliable tool for\ncollege-level calculation problems.\n", "link": "http://arxiv.org/abs/2308.05713v4", "date": "2025-02-20", "relevancy": 1.9777, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3981}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Testing%20GPT-4%20with%20Wolfram%20Alpha%20and%20Code%20Interpreter%20plug-ins%20on%20math%0A%20%20and%20science%20problems&body=Title%3A%20Testing%20GPT-4%20with%20Wolfram%20Alpha%20and%20Code%20Interpreter%20plug-ins%20on%20math%0A%20%20and%20science%20problems%0AAuthor%3A%20Ernest%20Davis%20and%20Scott%20Aaronson%0AAbstract%3A%20%20%20This%20report%20describes%20a%20test%20of%20the%20large%20language%20model%20GPT-4%20with%20the%0AWolfram%20Alpha%20and%20the%20Code%20Interpreter%20plug-ins%20on%20105%20original%20problems%20in%0Ascience%20and%20math%2C%20at%20the%20high%20school%20and%20college%20levels%2C%20carried%20out%20in%0AJune-August%202023.%20Our%20tests%20suggest%20that%20the%20plug-ins%20significantly%20enhance%0AGPT%27s%20ability%20to%20solve%20these%20problems.%20Having%20said%20that%2C%20there%20are%20still%20often%0A%22interface%22%20failures%3B%20that%20is%2C%20GPT%20often%20has%20trouble%20formulating%20problems%20in%20a%0Away%20that%20elicits%20useful%20answers%20from%20the%20plug-ins.%20Fixing%20these%20interface%0Afailures%20seems%20like%20a%20central%20challenge%20in%20making%20GPT%20a%20reliable%20tool%20for%0Acollege-level%20calculation%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05713v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTesting%2520GPT-4%2520with%2520Wolfram%2520Alpha%2520and%2520Code%2520Interpreter%2520plug-ins%2520on%2520math%250A%2520%2520and%2520science%2520problems%26entry.906535625%3DErnest%2520Davis%2520and%2520Scott%2520Aaronson%26entry.1292438233%3D%2520%2520This%2520report%2520describes%2520a%2520test%2520of%2520the%2520large%2520language%2520model%2520GPT-4%2520with%2520the%250AWolfram%2520Alpha%2520and%2520the%2520Code%2520Interpreter%2520plug-ins%2520on%2520105%2520original%2520problems%2520in%250Ascience%2520and%2520math%252C%2520at%2520the%2520high%2520school%2520and%2520college%2520levels%252C%2520carried%2520out%2520in%250AJune-August%25202023.%2520Our%2520tests%2520suggest%2520that%2520the%2520plug-ins%2520significantly%2520enhance%250AGPT%2527s%2520ability%2520to%2520solve%2520these%2520problems.%2520Having%2520said%2520that%252C%2520there%2520are%2520still%2520often%250A%2522interface%2522%2520failures%253B%2520that%2520is%252C%2520GPT%2520often%2520has%2520trouble%2520formulating%2520problems%2520in%2520a%250Away%2520that%2520elicits%2520useful%2520answers%2520from%2520the%2520plug-ins.%2520Fixing%2520these%2520interface%250Afailures%2520seems%2520like%2520a%2520central%2520challenge%2520in%2520making%2520GPT%2520a%2520reliable%2520tool%2520for%250Acollege-level%2520calculation%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05713v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Testing%20GPT-4%20with%20Wolfram%20Alpha%20and%20Code%20Interpreter%20plug-ins%20on%20math%0A%20%20and%20science%20problems&entry.906535625=Ernest%20Davis%20and%20Scott%20Aaronson&entry.1292438233=%20%20This%20report%20describes%20a%20test%20of%20the%20large%20language%20model%20GPT-4%20with%20the%0AWolfram%20Alpha%20and%20the%20Code%20Interpreter%20plug-ins%20on%20105%20original%20problems%20in%0Ascience%20and%20math%2C%20at%20the%20high%20school%20and%20college%20levels%2C%20carried%20out%20in%0AJune-August%202023.%20Our%20tests%20suggest%20that%20the%20plug-ins%20significantly%20enhance%0AGPT%27s%20ability%20to%20solve%20these%20problems.%20Having%20said%20that%2C%20there%20are%20still%20often%0A%22interface%22%20failures%3B%20that%20is%2C%20GPT%20often%20has%20trouble%20formulating%20problems%20in%20a%0Away%20that%20elicits%20useful%20answers%20from%20the%20plug-ins.%20Fixing%20these%20interface%0Afailures%20seems%20like%20a%20central%20challenge%20in%20making%20GPT%20a%20reliable%20tool%20for%0Acollege-level%20calculation%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05713v4&entry.124074799=Read"},
{"title": "MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only\n  VIBE MRI", "author": "Robert Graf and Hendrik M\u00f6ller and Sophie Starck and Matan Atad and Philipp Braun and Jonathan Stelter and Annette Peters and Lilian Krist and Stefan N. Willich and Henry V\u00f6lzke and Robin B\u00fclow and Klaus Berger and Tobias Pischon and Thoralf Niendorf and Johannes Paetzold and Dimitrios Karampinos and Daniel Rueckert and Jan Kirschke", "abstract": "  Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images\nsuitable for water and fat signal composition estimation. While the two-point\nVIBE provides water-fat-separated images, the six-point VIBE allows estimation\nof the effective transversal relaxation rate R2* and the proton density fat\nfraction (PDFF), which are imaging markers for health and disease. Ambiguity\nduring signal reconstruction can lead to water-fat swaps. This shortcoming\nchallenges the application of VIBE-MRI for automated PDFF analyses of\nlarge-scale clinical data and of population studies. This study develops an\nautomated pipeline to detect and correct water-fat swaps in\nnon-contrast-enhanced VIBE images. Our three-step pipeline begins with training\na segmentation network to classify volumes as \"fat-like\" or \"water-like,\" using\nsynthetic water-fat swaps generated by merging fat and water volumes with\nPerlin noise. Next, a denoising diffusion image-to-image network predicts water\nvolumes as signal priors for correction. Finally, we integrate this prior into\na physics-constrained model to recover accurate water and fat signals. Our\napproach achieves a < 1% error rate in water-fat swap detection for a 6-point\nVIBE. Notably, swaps disproportionately affect individuals in the Underweight\nand Class 3 Obesity BMI categories. Our correction algorithm ensures accurate\nsolution selection in chemical phase MRIs, enabling reliable PDFF estimation.\nThis forms a solid technical foundation for automated large-scale population\nimaging analysis.\n", "link": "http://arxiv.org/abs/2502.14659v1", "date": "2025-02-20", "relevancy": 1.9776, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5007}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4954}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGO-SP%3A%20Detection%20and%20Correction%20of%20Water-Fat%20Swaps%20in%20Magnitude-Only%0A%20%20VIBE%20MRI&body=Title%3A%20MAGO-SP%3A%20Detection%20and%20Correction%20of%20Water-Fat%20Swaps%20in%20Magnitude-Only%0A%20%20VIBE%20MRI%0AAuthor%3A%20Robert%20Graf%20and%20Hendrik%20M%C3%B6ller%20and%20Sophie%20Starck%20and%20Matan%20Atad%20and%20Philipp%20Braun%20and%20Jonathan%20Stelter%20and%20Annette%20Peters%20and%20Lilian%20Krist%20and%20Stefan%20N.%20Willich%20and%20Henry%20V%C3%B6lzke%20and%20Robin%20B%C3%BClow%20and%20Klaus%20Berger%20and%20Tobias%20Pischon%20and%20Thoralf%20Niendorf%20and%20Johannes%20Paetzold%20and%20Dimitrios%20Karampinos%20and%20Daniel%20Rueckert%20and%20Jan%20Kirschke%0AAbstract%3A%20%20%20Volume%20Interpolated%20Breath-Hold%20Examination%20%28VIBE%29%20MRI%20generates%20images%0Asuitable%20for%20water%20and%20fat%20signal%20composition%20estimation.%20While%20the%20two-point%0AVIBE%20provides%20water-fat-separated%20images%2C%20the%20six-point%20VIBE%20allows%20estimation%0Aof%20the%20effective%20transversal%20relaxation%20rate%20R2%2A%20and%20the%20proton%20density%20fat%0Afraction%20%28PDFF%29%2C%20which%20are%20imaging%20markers%20for%20health%20and%20disease.%20Ambiguity%0Aduring%20signal%20reconstruction%20can%20lead%20to%20water-fat%20swaps.%20This%20shortcoming%0Achallenges%20the%20application%20of%20VIBE-MRI%20for%20automated%20PDFF%20analyses%20of%0Alarge-scale%20clinical%20data%20and%20of%20population%20studies.%20This%20study%20develops%20an%0Aautomated%20pipeline%20to%20detect%20and%20correct%20water-fat%20swaps%20in%0Anon-contrast-enhanced%20VIBE%20images.%20Our%20three-step%20pipeline%20begins%20with%20training%0Aa%20segmentation%20network%20to%20classify%20volumes%20as%20%22fat-like%22%20or%20%22water-like%2C%22%20using%0Asynthetic%20water-fat%20swaps%20generated%20by%20merging%20fat%20and%20water%20volumes%20with%0APerlin%20noise.%20Next%2C%20a%20denoising%20diffusion%20image-to-image%20network%20predicts%20water%0Avolumes%20as%20signal%20priors%20for%20correction.%20Finally%2C%20we%20integrate%20this%20prior%20into%0Aa%20physics-constrained%20model%20to%20recover%20accurate%20water%20and%20fat%20signals.%20Our%0Aapproach%20achieves%20a%20%3C%201%25%20error%20rate%20in%20water-fat%20swap%20detection%20for%20a%206-point%0AVIBE.%20Notably%2C%20swaps%20disproportionately%20affect%20individuals%20in%20the%20Underweight%0Aand%20Class%203%20Obesity%20BMI%20categories.%20Our%20correction%20algorithm%20ensures%20accurate%0Asolution%20selection%20in%20chemical%20phase%20MRIs%2C%20enabling%20reliable%20PDFF%20estimation.%0AThis%20forms%20a%20solid%20technical%20foundation%20for%20automated%20large-scale%20population%0Aimaging%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGO-SP%253A%2520Detection%2520and%2520Correction%2520of%2520Water-Fat%2520Swaps%2520in%2520Magnitude-Only%250A%2520%2520VIBE%2520MRI%26entry.906535625%3DRobert%2520Graf%2520and%2520Hendrik%2520M%25C3%25B6ller%2520and%2520Sophie%2520Starck%2520and%2520Matan%2520Atad%2520and%2520Philipp%2520Braun%2520and%2520Jonathan%2520Stelter%2520and%2520Annette%2520Peters%2520and%2520Lilian%2520Krist%2520and%2520Stefan%2520N.%2520Willich%2520and%2520Henry%2520V%25C3%25B6lzke%2520and%2520Robin%2520B%25C3%25BClow%2520and%2520Klaus%2520Berger%2520and%2520Tobias%2520Pischon%2520and%2520Thoralf%2520Niendorf%2520and%2520Johannes%2520Paetzold%2520and%2520Dimitrios%2520Karampinos%2520and%2520Daniel%2520Rueckert%2520and%2520Jan%2520Kirschke%26entry.1292438233%3D%2520%2520Volume%2520Interpolated%2520Breath-Hold%2520Examination%2520%2528VIBE%2529%2520MRI%2520generates%2520images%250Asuitable%2520for%2520water%2520and%2520fat%2520signal%2520composition%2520estimation.%2520While%2520the%2520two-point%250AVIBE%2520provides%2520water-fat-separated%2520images%252C%2520the%2520six-point%2520VIBE%2520allows%2520estimation%250Aof%2520the%2520effective%2520transversal%2520relaxation%2520rate%2520R2%252A%2520and%2520the%2520proton%2520density%2520fat%250Afraction%2520%2528PDFF%2529%252C%2520which%2520are%2520imaging%2520markers%2520for%2520health%2520and%2520disease.%2520Ambiguity%250Aduring%2520signal%2520reconstruction%2520can%2520lead%2520to%2520water-fat%2520swaps.%2520This%2520shortcoming%250Achallenges%2520the%2520application%2520of%2520VIBE-MRI%2520for%2520automated%2520PDFF%2520analyses%2520of%250Alarge-scale%2520clinical%2520data%2520and%2520of%2520population%2520studies.%2520This%2520study%2520develops%2520an%250Aautomated%2520pipeline%2520to%2520detect%2520and%2520correct%2520water-fat%2520swaps%2520in%250Anon-contrast-enhanced%2520VIBE%2520images.%2520Our%2520three-step%2520pipeline%2520begins%2520with%2520training%250Aa%2520segmentation%2520network%2520to%2520classify%2520volumes%2520as%2520%2522fat-like%2522%2520or%2520%2522water-like%252C%2522%2520using%250Asynthetic%2520water-fat%2520swaps%2520generated%2520by%2520merging%2520fat%2520and%2520water%2520volumes%2520with%250APerlin%2520noise.%2520Next%252C%2520a%2520denoising%2520diffusion%2520image-to-image%2520network%2520predicts%2520water%250Avolumes%2520as%2520signal%2520priors%2520for%2520correction.%2520Finally%252C%2520we%2520integrate%2520this%2520prior%2520into%250Aa%2520physics-constrained%2520model%2520to%2520recover%2520accurate%2520water%2520and%2520fat%2520signals.%2520Our%250Aapproach%2520achieves%2520a%2520%253C%25201%2525%2520error%2520rate%2520in%2520water-fat%2520swap%2520detection%2520for%2520a%25206-point%250AVIBE.%2520Notably%252C%2520swaps%2520disproportionately%2520affect%2520individuals%2520in%2520the%2520Underweight%250Aand%2520Class%25203%2520Obesity%2520BMI%2520categories.%2520Our%2520correction%2520algorithm%2520ensures%2520accurate%250Asolution%2520selection%2520in%2520chemical%2520phase%2520MRIs%252C%2520enabling%2520reliable%2520PDFF%2520estimation.%250AThis%2520forms%2520a%2520solid%2520technical%2520foundation%2520for%2520automated%2520large-scale%2520population%250Aimaging%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGO-SP%3A%20Detection%20and%20Correction%20of%20Water-Fat%20Swaps%20in%20Magnitude-Only%0A%20%20VIBE%20MRI&entry.906535625=Robert%20Graf%20and%20Hendrik%20M%C3%B6ller%20and%20Sophie%20Starck%20and%20Matan%20Atad%20and%20Philipp%20Braun%20and%20Jonathan%20Stelter%20and%20Annette%20Peters%20and%20Lilian%20Krist%20and%20Stefan%20N.%20Willich%20and%20Henry%20V%C3%B6lzke%20and%20Robin%20B%C3%BClow%20and%20Klaus%20Berger%20and%20Tobias%20Pischon%20and%20Thoralf%20Niendorf%20and%20Johannes%20Paetzold%20and%20Dimitrios%20Karampinos%20and%20Daniel%20Rueckert%20and%20Jan%20Kirschke&entry.1292438233=%20%20Volume%20Interpolated%20Breath-Hold%20Examination%20%28VIBE%29%20MRI%20generates%20images%0Asuitable%20for%20water%20and%20fat%20signal%20composition%20estimation.%20While%20the%20two-point%0AVIBE%20provides%20water-fat-separated%20images%2C%20the%20six-point%20VIBE%20allows%20estimation%0Aof%20the%20effective%20transversal%20relaxation%20rate%20R2%2A%20and%20the%20proton%20density%20fat%0Afraction%20%28PDFF%29%2C%20which%20are%20imaging%20markers%20for%20health%20and%20disease.%20Ambiguity%0Aduring%20signal%20reconstruction%20can%20lead%20to%20water-fat%20swaps.%20This%20shortcoming%0Achallenges%20the%20application%20of%20VIBE-MRI%20for%20automated%20PDFF%20analyses%20of%0Alarge-scale%20clinical%20data%20and%20of%20population%20studies.%20This%20study%20develops%20an%0Aautomated%20pipeline%20to%20detect%20and%20correct%20water-fat%20swaps%20in%0Anon-contrast-enhanced%20VIBE%20images.%20Our%20three-step%20pipeline%20begins%20with%20training%0Aa%20segmentation%20network%20to%20classify%20volumes%20as%20%22fat-like%22%20or%20%22water-like%2C%22%20using%0Asynthetic%20water-fat%20swaps%20generated%20by%20merging%20fat%20and%20water%20volumes%20with%0APerlin%20noise.%20Next%2C%20a%20denoising%20diffusion%20image-to-image%20network%20predicts%20water%0Avolumes%20as%20signal%20priors%20for%20correction.%20Finally%2C%20we%20integrate%20this%20prior%20into%0Aa%20physics-constrained%20model%20to%20recover%20accurate%20water%20and%20fat%20signals.%20Our%0Aapproach%20achieves%20a%20%3C%201%25%20error%20rate%20in%20water-fat%20swap%20detection%20for%20a%206-point%0AVIBE.%20Notably%2C%20swaps%20disproportionately%20affect%20individuals%20in%20the%20Underweight%0Aand%20Class%203%20Obesity%20BMI%20categories.%20Our%20correction%20algorithm%20ensures%20accurate%0Asolution%20selection%20in%20chemical%20phase%20MRIs%2C%20enabling%20reliable%20PDFF%20estimation.%0AThis%20forms%20a%20solid%20technical%20foundation%20for%20automated%20large-scale%20population%0Aimaging%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14659v1&entry.124074799=Read"},
{"title": "Generating $\u03c0$-Functional Molecules Using STGG+ with Active Learning", "author": "Alexia Jolicoeur-Martineau and Yan Zhang and Boris Knyazev and Aristide Baratin and Cheng-Hao Liu", "abstract": "  Generating novel molecules with out-of-distribution properties is a major\nchallenge in molecular discovery. While supervised learning methods generate\nhigh-quality molecules similar to those in a dataset, they struggle to\ngeneralize to out-of-distribution properties. Reinforcement learning can\nexplore new chemical spaces but often conducts 'reward-hacking' and generates\nnon-synthesizable molecules. In this work, we address this problem by\nintegrating a state-of-the-art supervised learning method, STGG+, in an active\nlearning loop. Our approach iteratively generates, evaluates, and fine-tunes\nSTGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We\napply STGG+AL to the design of organic $\\pi$-functional materials, specifically\ntwo challenging tasks: 1) generating highly absorptive molecules characterized\nby high oscillator strength and 2) designing absorptive molecules with\nreasonable oscillator strength in the near-infrared (NIR) range. The generated\nmolecules are validated and rationalized in-silico with time-dependent density\nfunctional theory. Our results demonstrate that our method is highly effective\nin generating novel molecules with high oscillator strength, contrary to\nexisting methods such as reinforcement learning (RL) methods. We open-source\nour active-learning code along with our Conjugated-xTB dataset containing 2.9\nmillion $\\pi$-conjugated molecules and the function for approximating the\noscillator strength and absorption wavelength (based on sTDA-xTB).\n", "link": "http://arxiv.org/abs/2502.14842v1", "date": "2025-02-20", "relevancy": 1.9531, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5049}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5009}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20%24%CF%80%24-Functional%20Molecules%20Using%20STGG%2B%20with%20Active%20Learning&body=Title%3A%20Generating%20%24%CF%80%24-Functional%20Molecules%20Using%20STGG%2B%20with%20Active%20Learning%0AAuthor%3A%20Alexia%20Jolicoeur-Martineau%20and%20Yan%20Zhang%20and%20Boris%20Knyazev%20and%20Aristide%20Baratin%20and%20Cheng-Hao%20Liu%0AAbstract%3A%20%20%20Generating%20novel%20molecules%20with%20out-of-distribution%20properties%20is%20a%20major%0Achallenge%20in%20molecular%20discovery.%20While%20supervised%20learning%20methods%20generate%0Ahigh-quality%20molecules%20similar%20to%20those%20in%20a%20dataset%2C%20they%20struggle%20to%0Ageneralize%20to%20out-of-distribution%20properties.%20Reinforcement%20learning%20can%0Aexplore%20new%20chemical%20spaces%20but%20often%20conducts%20%27reward-hacking%27%20and%20generates%0Anon-synthesizable%20molecules.%20In%20this%20work%2C%20we%20address%20this%20problem%20by%0Aintegrating%20a%20state-of-the-art%20supervised%20learning%20method%2C%20STGG%2B%2C%20in%20an%20active%0Alearning%20loop.%20Our%20approach%20iteratively%20generates%2C%20evaluates%2C%20and%20fine-tunes%0ASTGG%2B%20to%20continuously%20expand%20its%20knowledge.%20We%20denote%20this%20approach%20STGG%2BAL.%20We%0Aapply%20STGG%2BAL%20to%20the%20design%20of%20organic%20%24%5Cpi%24-functional%20materials%2C%20specifically%0Atwo%20challenging%20tasks%3A%201%29%20generating%20highly%20absorptive%20molecules%20characterized%0Aby%20high%20oscillator%20strength%20and%202%29%20designing%20absorptive%20molecules%20with%0Areasonable%20oscillator%20strength%20in%20the%20near-infrared%20%28NIR%29%20range.%20The%20generated%0Amolecules%20are%20validated%20and%20rationalized%20in-silico%20with%20time-dependent%20density%0Afunctional%20theory.%20Our%20results%20demonstrate%20that%20our%20method%20is%20highly%20effective%0Ain%20generating%20novel%20molecules%20with%20high%20oscillator%20strength%2C%20contrary%20to%0Aexisting%20methods%20such%20as%20reinforcement%20learning%20%28RL%29%20methods.%20We%20open-source%0Aour%20active-learning%20code%20along%20with%20our%20Conjugated-xTB%20dataset%20containing%202.9%0Amillion%20%24%5Cpi%24-conjugated%20molecules%20and%20the%20function%20for%20approximating%20the%0Aoscillator%20strength%20and%20absorption%20wavelength%20%28based%20on%20sTDA-xTB%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520%2524%25CF%2580%2524-Functional%2520Molecules%2520Using%2520STGG%252B%2520with%2520Active%2520Learning%26entry.906535625%3DAlexia%2520Jolicoeur-Martineau%2520and%2520Yan%2520Zhang%2520and%2520Boris%2520Knyazev%2520and%2520Aristide%2520Baratin%2520and%2520Cheng-Hao%2520Liu%26entry.1292438233%3D%2520%2520Generating%2520novel%2520molecules%2520with%2520out-of-distribution%2520properties%2520is%2520a%2520major%250Achallenge%2520in%2520molecular%2520discovery.%2520While%2520supervised%2520learning%2520methods%2520generate%250Ahigh-quality%2520molecules%2520similar%2520to%2520those%2520in%2520a%2520dataset%252C%2520they%2520struggle%2520to%250Ageneralize%2520to%2520out-of-distribution%2520properties.%2520Reinforcement%2520learning%2520can%250Aexplore%2520new%2520chemical%2520spaces%2520but%2520often%2520conducts%2520%2527reward-hacking%2527%2520and%2520generates%250Anon-synthesizable%2520molecules.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520problem%2520by%250Aintegrating%2520a%2520state-of-the-art%2520supervised%2520learning%2520method%252C%2520STGG%252B%252C%2520in%2520an%2520active%250Alearning%2520loop.%2520Our%2520approach%2520iteratively%2520generates%252C%2520evaluates%252C%2520and%2520fine-tunes%250ASTGG%252B%2520to%2520continuously%2520expand%2520its%2520knowledge.%2520We%2520denote%2520this%2520approach%2520STGG%252BAL.%2520We%250Aapply%2520STGG%252BAL%2520to%2520the%2520design%2520of%2520organic%2520%2524%255Cpi%2524-functional%2520materials%252C%2520specifically%250Atwo%2520challenging%2520tasks%253A%25201%2529%2520generating%2520highly%2520absorptive%2520molecules%2520characterized%250Aby%2520high%2520oscillator%2520strength%2520and%25202%2529%2520designing%2520absorptive%2520molecules%2520with%250Areasonable%2520oscillator%2520strength%2520in%2520the%2520near-infrared%2520%2528NIR%2529%2520range.%2520The%2520generated%250Amolecules%2520are%2520validated%2520and%2520rationalized%2520in-silico%2520with%2520time-dependent%2520density%250Afunctional%2520theory.%2520Our%2520results%2520demonstrate%2520that%2520our%2520method%2520is%2520highly%2520effective%250Ain%2520generating%2520novel%2520molecules%2520with%2520high%2520oscillator%2520strength%252C%2520contrary%2520to%250Aexisting%2520methods%2520such%2520as%2520reinforcement%2520learning%2520%2528RL%2529%2520methods.%2520We%2520open-source%250Aour%2520active-learning%2520code%2520along%2520with%2520our%2520Conjugated-xTB%2520dataset%2520containing%25202.9%250Amillion%2520%2524%255Cpi%2524-conjugated%2520molecules%2520and%2520the%2520function%2520for%2520approximating%2520the%250Aoscillator%2520strength%2520and%2520absorption%2520wavelength%2520%2528based%2520on%2520sTDA-xTB%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20%24%CF%80%24-Functional%20Molecules%20Using%20STGG%2B%20with%20Active%20Learning&entry.906535625=Alexia%20Jolicoeur-Martineau%20and%20Yan%20Zhang%20and%20Boris%20Knyazev%20and%20Aristide%20Baratin%20and%20Cheng-Hao%20Liu&entry.1292438233=%20%20Generating%20novel%20molecules%20with%20out-of-distribution%20properties%20is%20a%20major%0Achallenge%20in%20molecular%20discovery.%20While%20supervised%20learning%20methods%20generate%0Ahigh-quality%20molecules%20similar%20to%20those%20in%20a%20dataset%2C%20they%20struggle%20to%0Ageneralize%20to%20out-of-distribution%20properties.%20Reinforcement%20learning%20can%0Aexplore%20new%20chemical%20spaces%20but%20often%20conducts%20%27reward-hacking%27%20and%20generates%0Anon-synthesizable%20molecules.%20In%20this%20work%2C%20we%20address%20this%20problem%20by%0Aintegrating%20a%20state-of-the-art%20supervised%20learning%20method%2C%20STGG%2B%2C%20in%20an%20active%0Alearning%20loop.%20Our%20approach%20iteratively%20generates%2C%20evaluates%2C%20and%20fine-tunes%0ASTGG%2B%20to%20continuously%20expand%20its%20knowledge.%20We%20denote%20this%20approach%20STGG%2BAL.%20We%0Aapply%20STGG%2BAL%20to%20the%20design%20of%20organic%20%24%5Cpi%24-functional%20materials%2C%20specifically%0Atwo%20challenging%20tasks%3A%201%29%20generating%20highly%20absorptive%20molecules%20characterized%0Aby%20high%20oscillator%20strength%20and%202%29%20designing%20absorptive%20molecules%20with%0Areasonable%20oscillator%20strength%20in%20the%20near-infrared%20%28NIR%29%20range.%20The%20generated%0Amolecules%20are%20validated%20and%20rationalized%20in-silico%20with%20time-dependent%20density%0Afunctional%20theory.%20Our%20results%20demonstrate%20that%20our%20method%20is%20highly%20effective%0Ain%20generating%20novel%20molecules%20with%20high%20oscillator%20strength%2C%20contrary%20to%0Aexisting%20methods%20such%20as%20reinforcement%20learning%20%28RL%29%20methods.%20We%20open-source%0Aour%20active-learning%20code%20along%20with%20our%20Conjugated-xTB%20dataset%20containing%202.9%0Amillion%20%24%5Cpi%24-conjugated%20molecules%20and%20the%20function%20for%20approximating%20the%0Aoscillator%20strength%20and%20absorption%20wavelength%20%28based%20on%20sTDA-xTB%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14842v1&entry.124074799=Read"},
{"title": "PEARL: Towards Permutation-Resilient LLMs", "author": "Liang Chen and Li Shen and Yang Deng and Xiaoyan Zhao and Bin Liang and Kam-Fai Wong", "abstract": "  The in-context learning (ICL) capability of large language models (LLMs)\nenables them to perform challenging tasks using provided demonstrations.\nHowever, ICL is highly sensitive to the ordering of demonstrations, leading to\ninstability in predictions. This paper shows that this vulnerability can be\nexploited to design a natural attack - difficult for model providers to detect\n- that achieves nearly 80% success rate on LLaMA-3 by simply permuting the\ndemonstrations. Existing mitigation methods primarily rely on post-processing\nand fail to enhance the model's inherent robustness to input permutations,\nraising concerns about safety and reliability of LLMs. To address this issue,\nwe propose Permutation-resilient learning (PEARL), a novel framework based on\ndistributionally robust optimization (DRO), which optimizes model performance\nagainst the worst-case input permutation. Specifically, PEARL consists of a\npermutation-proposal network (P-Net) and the LLM. The P-Net generates the most\nchallenging permutations by treating it as an optimal transport problem, which\nis solved using an entropy-constrained Sinkhorn algorithm. Through minimax\noptimization, the P-Net and the LLM iteratively optimize against each other,\nprogressively improving the LLM's robustness. Experiments on synthetic\npre-training and real-world instruction tuning tasks demonstrate that PEARL\neffectively mitigates permutation attacks and enhances performance. Notably,\ndespite being trained on fewer shots and shorter contexts, PEARL achieves\nperformance gains of up to 40% when scaled to many-shot and long-context\nscenarios, highlighting its efficiency and generalization capabilities.\n", "link": "http://arxiv.org/abs/2502.14628v1", "date": "2025-02-20", "relevancy": 1.946, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.49}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEARL%3A%20Towards%20Permutation-Resilient%20LLMs&body=Title%3A%20PEARL%3A%20Towards%20Permutation-Resilient%20LLMs%0AAuthor%3A%20Liang%20Chen%20and%20Li%20Shen%20and%20Yang%20Deng%20and%20Xiaoyan%20Zhao%20and%20Bin%20Liang%20and%20Kam-Fai%20Wong%0AAbstract%3A%20%20%20The%20in-context%20learning%20%28ICL%29%20capability%20of%20large%20language%20models%20%28LLMs%29%0Aenables%20them%20to%20perform%20challenging%20tasks%20using%20provided%20demonstrations.%0AHowever%2C%20ICL%20is%20highly%20sensitive%20to%20the%20ordering%20of%20demonstrations%2C%20leading%20to%0Ainstability%20in%20predictions.%20This%20paper%20shows%20that%20this%20vulnerability%20can%20be%0Aexploited%20to%20design%20a%20natural%20attack%20-%20difficult%20for%20model%20providers%20to%20detect%0A-%20that%20achieves%20nearly%2080%25%20success%20rate%20on%20LLaMA-3%20by%20simply%20permuting%20the%0Ademonstrations.%20Existing%20mitigation%20methods%20primarily%20rely%20on%20post-processing%0Aand%20fail%20to%20enhance%20the%20model%27s%20inherent%20robustness%20to%20input%20permutations%2C%0Araising%20concerns%20about%20safety%20and%20reliability%20of%20LLMs.%20To%20address%20this%20issue%2C%0Awe%20propose%20Permutation-resilient%20learning%20%28PEARL%29%2C%20a%20novel%20framework%20based%20on%0Adistributionally%20robust%20optimization%20%28DRO%29%2C%20which%20optimizes%20model%20performance%0Aagainst%20the%20worst-case%20input%20permutation.%20Specifically%2C%20PEARL%20consists%20of%20a%0Apermutation-proposal%20network%20%28P-Net%29%20and%20the%20LLM.%20The%20P-Net%20generates%20the%20most%0Achallenging%20permutations%20by%20treating%20it%20as%20an%20optimal%20transport%20problem%2C%20which%0Ais%20solved%20using%20an%20entropy-constrained%20Sinkhorn%20algorithm.%20Through%20minimax%0Aoptimization%2C%20the%20P-Net%20and%20the%20LLM%20iteratively%20optimize%20against%20each%20other%2C%0Aprogressively%20improving%20the%20LLM%27s%20robustness.%20Experiments%20on%20synthetic%0Apre-training%20and%20real-world%20instruction%20tuning%20tasks%20demonstrate%20that%20PEARL%0Aeffectively%20mitigates%20permutation%20attacks%20and%20enhances%20performance.%20Notably%2C%0Adespite%20being%20trained%20on%20fewer%20shots%20and%20shorter%20contexts%2C%20PEARL%20achieves%0Aperformance%20gains%20of%20up%20to%2040%25%20when%20scaled%20to%20many-shot%20and%20long-context%0Ascenarios%2C%20highlighting%20its%20efficiency%20and%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEARL%253A%2520Towards%2520Permutation-Resilient%2520LLMs%26entry.906535625%3DLiang%2520Chen%2520and%2520Li%2520Shen%2520and%2520Yang%2520Deng%2520and%2520Xiaoyan%2520Zhao%2520and%2520Bin%2520Liang%2520and%2520Kam-Fai%2520Wong%26entry.1292438233%3D%2520%2520The%2520in-context%2520learning%2520%2528ICL%2529%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Aenables%2520them%2520to%2520perform%2520challenging%2520tasks%2520using%2520provided%2520demonstrations.%250AHowever%252C%2520ICL%2520is%2520highly%2520sensitive%2520to%2520the%2520ordering%2520of%2520demonstrations%252C%2520leading%2520to%250Ainstability%2520in%2520predictions.%2520This%2520paper%2520shows%2520that%2520this%2520vulnerability%2520can%2520be%250Aexploited%2520to%2520design%2520a%2520natural%2520attack%2520-%2520difficult%2520for%2520model%2520providers%2520to%2520detect%250A-%2520that%2520achieves%2520nearly%252080%2525%2520success%2520rate%2520on%2520LLaMA-3%2520by%2520simply%2520permuting%2520the%250Ademonstrations.%2520Existing%2520mitigation%2520methods%2520primarily%2520rely%2520on%2520post-processing%250Aand%2520fail%2520to%2520enhance%2520the%2520model%2527s%2520inherent%2520robustness%2520to%2520input%2520permutations%252C%250Araising%2520concerns%2520about%2520safety%2520and%2520reliability%2520of%2520LLMs.%2520To%2520address%2520this%2520issue%252C%250Awe%2520propose%2520Permutation-resilient%2520learning%2520%2528PEARL%2529%252C%2520a%2520novel%2520framework%2520based%2520on%250Adistributionally%2520robust%2520optimization%2520%2528DRO%2529%252C%2520which%2520optimizes%2520model%2520performance%250Aagainst%2520the%2520worst-case%2520input%2520permutation.%2520Specifically%252C%2520PEARL%2520consists%2520of%2520a%250Apermutation-proposal%2520network%2520%2528P-Net%2529%2520and%2520the%2520LLM.%2520The%2520P-Net%2520generates%2520the%2520most%250Achallenging%2520permutations%2520by%2520treating%2520it%2520as%2520an%2520optimal%2520transport%2520problem%252C%2520which%250Ais%2520solved%2520using%2520an%2520entropy-constrained%2520Sinkhorn%2520algorithm.%2520Through%2520minimax%250Aoptimization%252C%2520the%2520P-Net%2520and%2520the%2520LLM%2520iteratively%2520optimize%2520against%2520each%2520other%252C%250Aprogressively%2520improving%2520the%2520LLM%2527s%2520robustness.%2520Experiments%2520on%2520synthetic%250Apre-training%2520and%2520real-world%2520instruction%2520tuning%2520tasks%2520demonstrate%2520that%2520PEARL%250Aeffectively%2520mitigates%2520permutation%2520attacks%2520and%2520enhances%2520performance.%2520Notably%252C%250Adespite%2520being%2520trained%2520on%2520fewer%2520shots%2520and%2520shorter%2520contexts%252C%2520PEARL%2520achieves%250Aperformance%2520gains%2520of%2520up%2520to%252040%2525%2520when%2520scaled%2520to%2520many-shot%2520and%2520long-context%250Ascenarios%252C%2520highlighting%2520its%2520efficiency%2520and%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEARL%3A%20Towards%20Permutation-Resilient%20LLMs&entry.906535625=Liang%20Chen%20and%20Li%20Shen%20and%20Yang%20Deng%20and%20Xiaoyan%20Zhao%20and%20Bin%20Liang%20and%20Kam-Fai%20Wong&entry.1292438233=%20%20The%20in-context%20learning%20%28ICL%29%20capability%20of%20large%20language%20models%20%28LLMs%29%0Aenables%20them%20to%20perform%20challenging%20tasks%20using%20provided%20demonstrations.%0AHowever%2C%20ICL%20is%20highly%20sensitive%20to%20the%20ordering%20of%20demonstrations%2C%20leading%20to%0Ainstability%20in%20predictions.%20This%20paper%20shows%20that%20this%20vulnerability%20can%20be%0Aexploited%20to%20design%20a%20natural%20attack%20-%20difficult%20for%20model%20providers%20to%20detect%0A-%20that%20achieves%20nearly%2080%25%20success%20rate%20on%20LLaMA-3%20by%20simply%20permuting%20the%0Ademonstrations.%20Existing%20mitigation%20methods%20primarily%20rely%20on%20post-processing%0Aand%20fail%20to%20enhance%20the%20model%27s%20inherent%20robustness%20to%20input%20permutations%2C%0Araising%20concerns%20about%20safety%20and%20reliability%20of%20LLMs.%20To%20address%20this%20issue%2C%0Awe%20propose%20Permutation-resilient%20learning%20%28PEARL%29%2C%20a%20novel%20framework%20based%20on%0Adistributionally%20robust%20optimization%20%28DRO%29%2C%20which%20optimizes%20model%20performance%0Aagainst%20the%20worst-case%20input%20permutation.%20Specifically%2C%20PEARL%20consists%20of%20a%0Apermutation-proposal%20network%20%28P-Net%29%20and%20the%20LLM.%20The%20P-Net%20generates%20the%20most%0Achallenging%20permutations%20by%20treating%20it%20as%20an%20optimal%20transport%20problem%2C%20which%0Ais%20solved%20using%20an%20entropy-constrained%20Sinkhorn%20algorithm.%20Through%20minimax%0Aoptimization%2C%20the%20P-Net%20and%20the%20LLM%20iteratively%20optimize%20against%20each%20other%2C%0Aprogressively%20improving%20the%20LLM%27s%20robustness.%20Experiments%20on%20synthetic%0Apre-training%20and%20real-world%20instruction%20tuning%20tasks%20demonstrate%20that%20PEARL%0Aeffectively%20mitigates%20permutation%20attacks%20and%20enhances%20performance.%20Notably%2C%0Adespite%20being%20trained%20on%20fewer%20shots%20and%20shorter%20contexts%2C%20PEARL%20achieves%0Aperformance%20gains%20of%20up%20to%2040%25%20when%20scaled%20to%20many-shot%20and%20long-context%0Ascenarios%2C%20highlighting%20its%20efficiency%20and%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14628v1&entry.124074799=Read"},
{"title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for\n  Scientific Comparative Analysis", "author": "Priyanka Kargupta and Ishika Agarwal and Tal August and Jiawei Han", "abstract": "  With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review.\n", "link": "http://arxiv.org/abs/2502.14767v1", "date": "2025-02-20", "relevancy": 1.9366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree-of-Debate%3A%20Multi-Persona%20Debate%20Trees%20Elicit%20Critical%20Thinking%20for%0A%20%20Scientific%20Comparative%20Analysis&body=Title%3A%20Tree-of-Debate%3A%20Multi-Persona%20Debate%20Trees%20Elicit%20Critical%20Thinking%20for%0A%20%20Scientific%20Comparative%20Analysis%0AAuthor%3A%20Priyanka%20Kargupta%20and%20Ishika%20Agarwal%20and%20Tal%20August%20and%20Jiawei%20Han%0AAbstract%3A%20%20%20With%20the%20exponential%20growth%20of%20research%20facilitated%20by%20modern%20technology%20and%0Aimproved%20accessibility%2C%20scientific%20discoveries%20have%20become%20increasingly%0Afragmented%20within%20and%20across%20fields.%20This%20makes%20it%20challenging%20to%20assess%20the%0Asignificance%2C%20novelty%2C%20incremental%20findings%2C%20and%20equivalent%20ideas%20between%0Arelated%20works%2C%20particularly%20those%20from%20different%20research%20communities.%20Large%0Alanguage%20models%20%28LLMs%29%20have%20recently%20demonstrated%20strong%20quantitative%20and%0Aqualitative%20reasoning%20abilities%2C%20and%20multi-agent%20LLM%20debates%20have%20shown%20promise%0Ain%20handling%20complex%20reasoning%20tasks%20by%20exploring%20diverse%20perspectives%20and%0Areasoning%20paths.%20Inspired%20by%20this%2C%20we%20introduce%20Tree-of-Debate%20%28ToD%29%2C%20a%0Aframework%20which%20converts%20scientific%20papers%20into%20LLM%20personas%20that%20debate%20their%0Arespective%20novelties.%20To%20emphasize%20structured%2C%20critical%20reasoning%20rather%20than%0Afocusing%20solely%20on%20outcomes%2C%20ToD%20dynamically%20constructs%20a%20debate%20tree%2C%20enabling%0Afine-grained%20analysis%20of%20independent%20novelty%20arguments%20within%20scholarly%0Aarticles.%20Through%20experiments%20on%20scientific%20literature%20across%20various%20domains%2C%0Aevaluated%20by%20expert%20researchers%2C%20we%20demonstrate%20that%20ToD%20generates%20informative%0Aarguments%2C%20effectively%20contrasts%20papers%2C%20and%20supports%20researchers%20in%20their%0Aliterature%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree-of-Debate%253A%2520Multi-Persona%2520Debate%2520Trees%2520Elicit%2520Critical%2520Thinking%2520for%250A%2520%2520Scientific%2520Comparative%2520Analysis%26entry.906535625%3DPriyanka%2520Kargupta%2520and%2520Ishika%2520Agarwal%2520and%2520Tal%2520August%2520and%2520Jiawei%2520Han%26entry.1292438233%3D%2520%2520With%2520the%2520exponential%2520growth%2520of%2520research%2520facilitated%2520by%2520modern%2520technology%2520and%250Aimproved%2520accessibility%252C%2520scientific%2520discoveries%2520have%2520become%2520increasingly%250Afragmented%2520within%2520and%2520across%2520fields.%2520This%2520makes%2520it%2520challenging%2520to%2520assess%2520the%250Asignificance%252C%2520novelty%252C%2520incremental%2520findings%252C%2520and%2520equivalent%2520ideas%2520between%250Arelated%2520works%252C%2520particularly%2520those%2520from%2520different%2520research%2520communities.%2520Large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520recently%2520demonstrated%2520strong%2520quantitative%2520and%250Aqualitative%2520reasoning%2520abilities%252C%2520and%2520multi-agent%2520LLM%2520debates%2520have%2520shown%2520promise%250Ain%2520handling%2520complex%2520reasoning%2520tasks%2520by%2520exploring%2520diverse%2520perspectives%2520and%250Areasoning%2520paths.%2520Inspired%2520by%2520this%252C%2520we%2520introduce%2520Tree-of-Debate%2520%2528ToD%2529%252C%2520a%250Aframework%2520which%2520converts%2520scientific%2520papers%2520into%2520LLM%2520personas%2520that%2520debate%2520their%250Arespective%2520novelties.%2520To%2520emphasize%2520structured%252C%2520critical%2520reasoning%2520rather%2520than%250Afocusing%2520solely%2520on%2520outcomes%252C%2520ToD%2520dynamically%2520constructs%2520a%2520debate%2520tree%252C%2520enabling%250Afine-grained%2520analysis%2520of%2520independent%2520novelty%2520arguments%2520within%2520scholarly%250Aarticles.%2520Through%2520experiments%2520on%2520scientific%2520literature%2520across%2520various%2520domains%252C%250Aevaluated%2520by%2520expert%2520researchers%252C%2520we%2520demonstrate%2520that%2520ToD%2520generates%2520informative%250Aarguments%252C%2520effectively%2520contrasts%2520papers%252C%2520and%2520supports%2520researchers%2520in%2520their%250Aliterature%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree-of-Debate%3A%20Multi-Persona%20Debate%20Trees%20Elicit%20Critical%20Thinking%20for%0A%20%20Scientific%20Comparative%20Analysis&entry.906535625=Priyanka%20Kargupta%20and%20Ishika%20Agarwal%20and%20Tal%20August%20and%20Jiawei%20Han&entry.1292438233=%20%20With%20the%20exponential%20growth%20of%20research%20facilitated%20by%20modern%20technology%20and%0Aimproved%20accessibility%2C%20scientific%20discoveries%20have%20become%20increasingly%0Afragmented%20within%20and%20across%20fields.%20This%20makes%20it%20challenging%20to%20assess%20the%0Asignificance%2C%20novelty%2C%20incremental%20findings%2C%20and%20equivalent%20ideas%20between%0Arelated%20works%2C%20particularly%20those%20from%20different%20research%20communities.%20Large%0Alanguage%20models%20%28LLMs%29%20have%20recently%20demonstrated%20strong%20quantitative%20and%0Aqualitative%20reasoning%20abilities%2C%20and%20multi-agent%20LLM%20debates%20have%20shown%20promise%0Ain%20handling%20complex%20reasoning%20tasks%20by%20exploring%20diverse%20perspectives%20and%0Areasoning%20paths.%20Inspired%20by%20this%2C%20we%20introduce%20Tree-of-Debate%20%28ToD%29%2C%20a%0Aframework%20which%20converts%20scientific%20papers%20into%20LLM%20personas%20that%20debate%20their%0Arespective%20novelties.%20To%20emphasize%20structured%2C%20critical%20reasoning%20rather%20than%0Afocusing%20solely%20on%20outcomes%2C%20ToD%20dynamically%20constructs%20a%20debate%20tree%2C%20enabling%0Afine-grained%20analysis%20of%20independent%20novelty%20arguments%20within%20scholarly%0Aarticles.%20Through%20experiments%20on%20scientific%20literature%20across%20various%20domains%2C%0Aevaluated%20by%20expert%20researchers%2C%20we%20demonstrate%20that%20ToD%20generates%20informative%0Aarguments%2C%20effectively%20contrasts%20papers%2C%20and%20supports%20researchers%20in%20their%0Aliterature%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14767v1&entry.124074799=Read"},
{"title": "Step-by-Step Fact Verification System for Medical Claims with\n  Explainable Reasoning", "author": "Juraj Vladika and Ivana Hacajov\u00e1 and Florian Matthes", "abstract": "  Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims.\n", "link": "http://arxiv.org/abs/2502.14765v1", "date": "2025-02-20", "relevancy": 1.9356, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4896}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-by-Step%20Fact%20Verification%20System%20for%20Medical%20Claims%20with%0A%20%20Explainable%20Reasoning&body=Title%3A%20Step-by-Step%20Fact%20Verification%20System%20for%20Medical%20Claims%20with%0A%20%20Explainable%20Reasoning%0AAuthor%3A%20Juraj%20Vladika%20and%20Ivana%20Hacajov%C3%A1%20and%20Florian%20Matthes%0AAbstract%3A%20%20%20Fact%20verification%20%28FV%29%20aims%20to%20assess%20the%20veracity%20of%20a%20claim%20based%20on%0Arelevant%20evidence.%20The%20traditional%20approach%20for%20automated%20FV%20includes%20a%0Athree-part%20pipeline%20relying%20on%20short%20evidence%20snippets%20and%20encoder-only%0Ainference%20models.%20More%20recent%20approaches%20leverage%20the%20multi-turn%20nature%20of%20LLMs%0Ato%20address%20FV%20as%20a%20step-by-step%20problem%20where%20questions%20inquiring%20additional%0Acontext%20are%20generated%20and%20answered%20until%20there%20is%20enough%20information%20to%20make%20a%0Adecision.%20This%20iterative%20method%20makes%20the%20verification%20process%20rational%20and%0Aexplainable.%20While%20these%20methods%20have%20been%20tested%20for%20encyclopedic%20claims%2C%0Aexploration%20on%20domain-specific%20and%20realistic%20claims%20is%20missing.%20In%20this%20work%2C%0Awe%20apply%20an%20iterative%20FV%20system%20on%20three%20medical%20fact-checking%20datasets%20and%0Aevaluate%20it%20with%20multiple%20settings%2C%20including%20different%20LLMs%2C%20external%20web%0Asearch%2C%20and%20structured%20reasoning%20using%20logic%20predicates.%20We%20demonstrate%0Aimprovements%20in%20the%20final%20performance%20over%20traditional%20approaches%20and%20the%20high%0Apotential%20of%20step-by-step%20FV%20systems%20for%20domain-specific%20claims.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-by-Step%2520Fact%2520Verification%2520System%2520for%2520Medical%2520Claims%2520with%250A%2520%2520Explainable%2520Reasoning%26entry.906535625%3DJuraj%2520Vladika%2520and%2520Ivana%2520Hacajov%25C3%25A1%2520and%2520Florian%2520Matthes%26entry.1292438233%3D%2520%2520Fact%2520verification%2520%2528FV%2529%2520aims%2520to%2520assess%2520the%2520veracity%2520of%2520a%2520claim%2520based%2520on%250Arelevant%2520evidence.%2520The%2520traditional%2520approach%2520for%2520automated%2520FV%2520includes%2520a%250Athree-part%2520pipeline%2520relying%2520on%2520short%2520evidence%2520snippets%2520and%2520encoder-only%250Ainference%2520models.%2520More%2520recent%2520approaches%2520leverage%2520the%2520multi-turn%2520nature%2520of%2520LLMs%250Ato%2520address%2520FV%2520as%2520a%2520step-by-step%2520problem%2520where%2520questions%2520inquiring%2520additional%250Acontext%2520are%2520generated%2520and%2520answered%2520until%2520there%2520is%2520enough%2520information%2520to%2520make%2520a%250Adecision.%2520This%2520iterative%2520method%2520makes%2520the%2520verification%2520process%2520rational%2520and%250Aexplainable.%2520While%2520these%2520methods%2520have%2520been%2520tested%2520for%2520encyclopedic%2520claims%252C%250Aexploration%2520on%2520domain-specific%2520and%2520realistic%2520claims%2520is%2520missing.%2520In%2520this%2520work%252C%250Awe%2520apply%2520an%2520iterative%2520FV%2520system%2520on%2520three%2520medical%2520fact-checking%2520datasets%2520and%250Aevaluate%2520it%2520with%2520multiple%2520settings%252C%2520including%2520different%2520LLMs%252C%2520external%2520web%250Asearch%252C%2520and%2520structured%2520reasoning%2520using%2520logic%2520predicates.%2520We%2520demonstrate%250Aimprovements%2520in%2520the%2520final%2520performance%2520over%2520traditional%2520approaches%2520and%2520the%2520high%250Apotential%2520of%2520step-by-step%2520FV%2520systems%2520for%2520domain-specific%2520claims.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-by-Step%20Fact%20Verification%20System%20for%20Medical%20Claims%20with%0A%20%20Explainable%20Reasoning&entry.906535625=Juraj%20Vladika%20and%20Ivana%20Hacajov%C3%A1%20and%20Florian%20Matthes&entry.1292438233=%20%20Fact%20verification%20%28FV%29%20aims%20to%20assess%20the%20veracity%20of%20a%20claim%20based%20on%0Arelevant%20evidence.%20The%20traditional%20approach%20for%20automated%20FV%20includes%20a%0Athree-part%20pipeline%20relying%20on%20short%20evidence%20snippets%20and%20encoder-only%0Ainference%20models.%20More%20recent%20approaches%20leverage%20the%20multi-turn%20nature%20of%20LLMs%0Ato%20address%20FV%20as%20a%20step-by-step%20problem%20where%20questions%20inquiring%20additional%0Acontext%20are%20generated%20and%20answered%20until%20there%20is%20enough%20information%20to%20make%20a%0Adecision.%20This%20iterative%20method%20makes%20the%20verification%20process%20rational%20and%0Aexplainable.%20While%20these%20methods%20have%20been%20tested%20for%20encyclopedic%20claims%2C%0Aexploration%20on%20domain-specific%20and%20realistic%20claims%20is%20missing.%20In%20this%20work%2C%0Awe%20apply%20an%20iterative%20FV%20system%20on%20three%20medical%20fact-checking%20datasets%20and%0Aevaluate%20it%20with%20multiple%20settings%2C%20including%20different%20LLMs%2C%20external%20web%0Asearch%2C%20and%20structured%20reasoning%20using%20logic%20predicates.%20We%20demonstrate%0Aimprovements%20in%20the%20final%20performance%20over%20traditional%20approaches%20and%20the%20high%0Apotential%20of%20step-by-step%20FV%20systems%20for%20domain-specific%20claims.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14765v1&entry.124074799=Read"},
{"title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through\n  Thinking", "author": "Zekun Xi and Wenbiao Yin and Jizhan Fang and Jialong Wu and Runnan Fang and Ningyu Zhang and Jiang Yong and Pengjun Xie and Fei Huang and Huajun Chen", "abstract": "  Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles.\n", "link": "http://arxiv.org/abs/2501.09751v2", "date": "2025-02-20", "relevancy": 1.9329, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5146}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniThink%3A%20Expanding%20Knowledge%20Boundaries%20in%20Machine%20Writing%20through%0A%20%20Thinking&body=Title%3A%20OmniThink%3A%20Expanding%20Knowledge%20Boundaries%20in%20Machine%20Writing%20through%0A%20%20Thinking%0AAuthor%3A%20Zekun%20Xi%20and%20Wenbiao%20Yin%20and%20Jizhan%20Fang%20and%20Jialong%20Wu%20and%20Runnan%20Fang%20and%20Ningyu%20Zhang%20and%20Jiang%20Yong%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20Machine%20writing%20with%20large%20language%20models%20often%20relies%20on%0Aretrieval-augmented%20generation.%20However%2C%20these%20approaches%20remain%20confined%0Awithin%20the%20boundaries%20of%20the%20model%27s%20predefined%20scope%2C%20limiting%20the%20generation%0Aof%20content%20with%20rich%20information.%20Specifically%2C%20vanilla-retrieved%20information%0Atends%20to%20lack%20depth%2C%20novelty%2C%20and%20suffers%20from%20redundancy%2C%20which%20negatively%0Aimpacts%20the%20quality%20of%20generated%20articles%2C%20leading%20to%20shallow%2C%20unoriginal%2C%20and%0Arepetitive%20outputs.%20To%20address%20these%20issues%2C%20we%20propose%20OmniThink%2C%20a%0Aslow-thinking%20machine%20writing%20framework%20that%20emulates%20the%20human-like%20process%20of%0Aiterative%20expansion%20and%20reflection.%20The%20core%20idea%20behind%20OmniThink%20is%20to%0Asimulate%20the%20cognitive%20behavior%20of%20learners%20as%20they%20slowly%20deepen%20their%0Aknowledge%20of%20the%20topics.%20Experimental%20results%20demonstrate%20that%20OmniThink%0Aimproves%20the%20knowledge%20density%20of%20generated%20articles%20without%20compromising%0Ametrics%20such%20as%20coherence%20and%20depth.%20Human%20evaluations%20and%20expert%20feedback%0Afurther%20highlight%20the%20potential%20of%20OmniThink%20to%20address%20real-world%20challenges%0Ain%20the%20generation%20of%20long-form%20articles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniThink%253A%2520Expanding%2520Knowledge%2520Boundaries%2520in%2520Machine%2520Writing%2520through%250A%2520%2520Thinking%26entry.906535625%3DZekun%2520Xi%2520and%2520Wenbiao%2520Yin%2520and%2520Jizhan%2520Fang%2520and%2520Jialong%2520Wu%2520and%2520Runnan%2520Fang%2520and%2520Ningyu%2520Zhang%2520and%2520Jiang%2520Yong%2520and%2520Pengjun%2520Xie%2520and%2520Fei%2520Huang%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520Machine%2520writing%2520with%2520large%2520language%2520models%2520often%2520relies%2520on%250Aretrieval-augmented%2520generation.%2520However%252C%2520these%2520approaches%2520remain%2520confined%250Awithin%2520the%2520boundaries%2520of%2520the%2520model%2527s%2520predefined%2520scope%252C%2520limiting%2520the%2520generation%250Aof%2520content%2520with%2520rich%2520information.%2520Specifically%252C%2520vanilla-retrieved%2520information%250Atends%2520to%2520lack%2520depth%252C%2520novelty%252C%2520and%2520suffers%2520from%2520redundancy%252C%2520which%2520negatively%250Aimpacts%2520the%2520quality%2520of%2520generated%2520articles%252C%2520leading%2520to%2520shallow%252C%2520unoriginal%252C%2520and%250Arepetitive%2520outputs.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520OmniThink%252C%2520a%250Aslow-thinking%2520machine%2520writing%2520framework%2520that%2520emulates%2520the%2520human-like%2520process%2520of%250Aiterative%2520expansion%2520and%2520reflection.%2520The%2520core%2520idea%2520behind%2520OmniThink%2520is%2520to%250Asimulate%2520the%2520cognitive%2520behavior%2520of%2520learners%2520as%2520they%2520slowly%2520deepen%2520their%250Aknowledge%2520of%2520the%2520topics.%2520Experimental%2520results%2520demonstrate%2520that%2520OmniThink%250Aimproves%2520the%2520knowledge%2520density%2520of%2520generated%2520articles%2520without%2520compromising%250Ametrics%2520such%2520as%2520coherence%2520and%2520depth.%2520Human%2520evaluations%2520and%2520expert%2520feedback%250Afurther%2520highlight%2520the%2520potential%2520of%2520OmniThink%2520to%2520address%2520real-world%2520challenges%250Ain%2520the%2520generation%2520of%2520long-form%2520articles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniThink%3A%20Expanding%20Knowledge%20Boundaries%20in%20Machine%20Writing%20through%0A%20%20Thinking&entry.906535625=Zekun%20Xi%20and%20Wenbiao%20Yin%20and%20Jizhan%20Fang%20and%20Jialong%20Wu%20and%20Runnan%20Fang%20and%20Ningyu%20Zhang%20and%20Jiang%20Yong%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Huajun%20Chen&entry.1292438233=%20%20Machine%20writing%20with%20large%20language%20models%20often%20relies%20on%0Aretrieval-augmented%20generation.%20However%2C%20these%20approaches%20remain%20confined%0Awithin%20the%20boundaries%20of%20the%20model%27s%20predefined%20scope%2C%20limiting%20the%20generation%0Aof%20content%20with%20rich%20information.%20Specifically%2C%20vanilla-retrieved%20information%0Atends%20to%20lack%20depth%2C%20novelty%2C%20and%20suffers%20from%20redundancy%2C%20which%20negatively%0Aimpacts%20the%20quality%20of%20generated%20articles%2C%20leading%20to%20shallow%2C%20unoriginal%2C%20and%0Arepetitive%20outputs.%20To%20address%20these%20issues%2C%20we%20propose%20OmniThink%2C%20a%0Aslow-thinking%20machine%20writing%20framework%20that%20emulates%20the%20human-like%20process%20of%0Aiterative%20expansion%20and%20reflection.%20The%20core%20idea%20behind%20OmniThink%20is%20to%0Asimulate%20the%20cognitive%20behavior%20of%20learners%20as%20they%20slowly%20deepen%20their%0Aknowledge%20of%20the%20topics.%20Experimental%20results%20demonstrate%20that%20OmniThink%0Aimproves%20the%20knowledge%20density%20of%20generated%20articles%20without%20compromising%0Ametrics%20such%20as%20coherence%20and%20depth.%20Human%20evaluations%20and%20expert%20feedback%0Afurther%20highlight%20the%20potential%20of%20OmniThink%20to%20address%20real-world%20challenges%0Ain%20the%20generation%20of%20long-form%20articles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09751v2&entry.124074799=Read"},
{"title": "Extracting Sentence Embeddings from Pretrained Transformer Models", "author": "Lukas Stankevi\u010dius and Mantas Luko\u0161evi\u010dius", "abstract": "  Pre-trained transformer models shine in many natural language processing\ntasks and therefore are expected to bear the representation of the input\nsentence or text meaning. These sentence-level embeddings are also important in\nretrieval-augmented generation. But do commonly used plain averaging or prompt\ntemplates sufficiently capture and represent the underlying meaning? After\nproviding a comprehensive review of existing sentence embedding extraction and\nrefinement methods, we thoroughly test different combinations and our original\nextensions of the most promising ones on pretrained models. Namely, given 110 M\nparameters, BERT's hidden representations from multiple layers, and many\ntokens, we try diverse ways to extract optimal sentence embeddings. We test\nvarious token aggregation and representation post-processing techniques. We\nalso test multiple ways of using a general Wikitext dataset to complement\nBERT's sentence embeddings. All methods are tested on eight Semantic Textual\nSimilarity (STS), six short text clustering, and twelve classification tasks.\nWe also evaluate our representation-shaping techniques on other static models,\nincluding random token representations. Proposed representation extraction\nmethods improve the performance on STS and clustering tasks for all models\nconsidered. Very high improvements for static token-based models, especially\nrandom embeddings for STS tasks, almost reach the performance of BERT-derived\nrepresentations. Our work shows that the representation-shaping techniques\nsignificantly improve sentence embeddings extracted from BERT-based and simple\nbaseline models.\n", "link": "http://arxiv.org/abs/2408.08073v2", "date": "2025-02-20", "relevancy": 1.9323, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Sentence%20Embeddings%20from%20Pretrained%20Transformer%20Models&body=Title%3A%20Extracting%20Sentence%20Embeddings%20from%20Pretrained%20Transformer%20Models%0AAuthor%3A%20Lukas%20Stankevi%C4%8Dius%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius%0AAbstract%3A%20%20%20Pre-trained%20transformer%20models%20shine%20in%20many%20natural%20language%20processing%0Atasks%20and%20therefore%20are%20expected%20to%20bear%20the%20representation%20of%20the%20input%0Asentence%20or%20text%20meaning.%20These%20sentence-level%20embeddings%20are%20also%20important%20in%0Aretrieval-augmented%20generation.%20But%20do%20commonly%20used%20plain%20averaging%20or%20prompt%0Atemplates%20sufficiently%20capture%20and%20represent%20the%20underlying%20meaning%3F%20After%0Aproviding%20a%20comprehensive%20review%20of%20existing%20sentence%20embedding%20extraction%20and%0Arefinement%20methods%2C%20we%20thoroughly%20test%20different%20combinations%20and%20our%20original%0Aextensions%20of%20the%20most%20promising%20ones%20on%20pretrained%20models.%20Namely%2C%20given%20110%20M%0Aparameters%2C%20BERT%27s%20hidden%20representations%20from%20multiple%20layers%2C%20and%20many%0Atokens%2C%20we%20try%20diverse%20ways%20to%20extract%20optimal%20sentence%20embeddings.%20We%20test%0Avarious%20token%20aggregation%20and%20representation%20post-processing%20techniques.%20We%0Aalso%20test%20multiple%20ways%20of%20using%20a%20general%20Wikitext%20dataset%20to%20complement%0ABERT%27s%20sentence%20embeddings.%20All%20methods%20are%20tested%20on%20eight%20Semantic%20Textual%0ASimilarity%20%28STS%29%2C%20six%20short%20text%20clustering%2C%20and%20twelve%20classification%20tasks.%0AWe%20also%20evaluate%20our%20representation-shaping%20techniques%20on%20other%20static%20models%2C%0Aincluding%20random%20token%20representations.%20Proposed%20representation%20extraction%0Amethods%20improve%20the%20performance%20on%20STS%20and%20clustering%20tasks%20for%20all%20models%0Aconsidered.%20Very%20high%20improvements%20for%20static%20token-based%20models%2C%20especially%0Arandom%20embeddings%20for%20STS%20tasks%2C%20almost%20reach%20the%20performance%20of%20BERT-derived%0Arepresentations.%20Our%20work%20shows%20that%20the%20representation-shaping%20techniques%0Asignificantly%20improve%20sentence%20embeddings%20extracted%20from%20BERT-based%20and%20simple%0Abaseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Sentence%2520Embeddings%2520from%2520Pretrained%2520Transformer%2520Models%26entry.906535625%3DLukas%2520Stankevi%25C4%258Dius%2520and%2520Mantas%2520Luko%25C5%25A1evi%25C4%258Dius%26entry.1292438233%3D%2520%2520Pre-trained%2520transformer%2520models%2520shine%2520in%2520many%2520natural%2520language%2520processing%250Atasks%2520and%2520therefore%2520are%2520expected%2520to%2520bear%2520the%2520representation%2520of%2520the%2520input%250Asentence%2520or%2520text%2520meaning.%2520These%2520sentence-level%2520embeddings%2520are%2520also%2520important%2520in%250Aretrieval-augmented%2520generation.%2520But%2520do%2520commonly%2520used%2520plain%2520averaging%2520or%2520prompt%250Atemplates%2520sufficiently%2520capture%2520and%2520represent%2520the%2520underlying%2520meaning%253F%2520After%250Aproviding%2520a%2520comprehensive%2520review%2520of%2520existing%2520sentence%2520embedding%2520extraction%2520and%250Arefinement%2520methods%252C%2520we%2520thoroughly%2520test%2520different%2520combinations%2520and%2520our%2520original%250Aextensions%2520of%2520the%2520most%2520promising%2520ones%2520on%2520pretrained%2520models.%2520Namely%252C%2520given%2520110%2520M%250Aparameters%252C%2520BERT%2527s%2520hidden%2520representations%2520from%2520multiple%2520layers%252C%2520and%2520many%250Atokens%252C%2520we%2520try%2520diverse%2520ways%2520to%2520extract%2520optimal%2520sentence%2520embeddings.%2520We%2520test%250Avarious%2520token%2520aggregation%2520and%2520representation%2520post-processing%2520techniques.%2520We%250Aalso%2520test%2520multiple%2520ways%2520of%2520using%2520a%2520general%2520Wikitext%2520dataset%2520to%2520complement%250ABERT%2527s%2520sentence%2520embeddings.%2520All%2520methods%2520are%2520tested%2520on%2520eight%2520Semantic%2520Textual%250ASimilarity%2520%2528STS%2529%252C%2520six%2520short%2520text%2520clustering%252C%2520and%2520twelve%2520classification%2520tasks.%250AWe%2520also%2520evaluate%2520our%2520representation-shaping%2520techniques%2520on%2520other%2520static%2520models%252C%250Aincluding%2520random%2520token%2520representations.%2520Proposed%2520representation%2520extraction%250Amethods%2520improve%2520the%2520performance%2520on%2520STS%2520and%2520clustering%2520tasks%2520for%2520all%2520models%250Aconsidered.%2520Very%2520high%2520improvements%2520for%2520static%2520token-based%2520models%252C%2520especially%250Arandom%2520embeddings%2520for%2520STS%2520tasks%252C%2520almost%2520reach%2520the%2520performance%2520of%2520BERT-derived%250Arepresentations.%2520Our%2520work%2520shows%2520that%2520the%2520representation-shaping%2520techniques%250Asignificantly%2520improve%2520sentence%2520embeddings%2520extracted%2520from%2520BERT-based%2520and%2520simple%250Abaseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Sentence%20Embeddings%20from%20Pretrained%20Transformer%20Models&entry.906535625=Lukas%20Stankevi%C4%8Dius%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius&entry.1292438233=%20%20Pre-trained%20transformer%20models%20shine%20in%20many%20natural%20language%20processing%0Atasks%20and%20therefore%20are%20expected%20to%20bear%20the%20representation%20of%20the%20input%0Asentence%20or%20text%20meaning.%20These%20sentence-level%20embeddings%20are%20also%20important%20in%0Aretrieval-augmented%20generation.%20But%20do%20commonly%20used%20plain%20averaging%20or%20prompt%0Atemplates%20sufficiently%20capture%20and%20represent%20the%20underlying%20meaning%3F%20After%0Aproviding%20a%20comprehensive%20review%20of%20existing%20sentence%20embedding%20extraction%20and%0Arefinement%20methods%2C%20we%20thoroughly%20test%20different%20combinations%20and%20our%20original%0Aextensions%20of%20the%20most%20promising%20ones%20on%20pretrained%20models.%20Namely%2C%20given%20110%20M%0Aparameters%2C%20BERT%27s%20hidden%20representations%20from%20multiple%20layers%2C%20and%20many%0Atokens%2C%20we%20try%20diverse%20ways%20to%20extract%20optimal%20sentence%20embeddings.%20We%20test%0Avarious%20token%20aggregation%20and%20representation%20post-processing%20techniques.%20We%0Aalso%20test%20multiple%20ways%20of%20using%20a%20general%20Wikitext%20dataset%20to%20complement%0ABERT%27s%20sentence%20embeddings.%20All%20methods%20are%20tested%20on%20eight%20Semantic%20Textual%0ASimilarity%20%28STS%29%2C%20six%20short%20text%20clustering%2C%20and%20twelve%20classification%20tasks.%0AWe%20also%20evaluate%20our%20representation-shaping%20techniques%20on%20other%20static%20models%2C%0Aincluding%20random%20token%20representations.%20Proposed%20representation%20extraction%0Amethods%20improve%20the%20performance%20on%20STS%20and%20clustering%20tasks%20for%20all%20models%0Aconsidered.%20Very%20high%20improvements%20for%20static%20token-based%20models%2C%20especially%0Arandom%20embeddings%20for%20STS%20tasks%2C%20almost%20reach%20the%20performance%20of%20BERT-derived%0Arepresentations.%20Our%20work%20shows%20that%20the%20representation-shaping%20techniques%0Asignificantly%20improve%20sentence%20embeddings%20extracted%20from%20BERT-based%20and%20simple%0Abaseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08073v2&entry.124074799=Read"},
{"title": "Beyond the Surface: Uncovering Implicit Locations with LLMs for\n  Personalized Local News", "author": "Gali Katz and Hai Sitton and Guy Gonen and Yohay Kaplan", "abstract": "  News recommendation systems personalize homepage content to boost engagement,\nbut factors like content type, editorial stance, and geographic focus impact\nrecommendations. Local newspapers balance coverage across regions, yet\nidentifying local articles is challenging due to implicit location cues like\nslang or landmarks.\n  Traditional methods, such as Named Entity Recognition (NER) and Knowledge\nGraphs, infer locations, but Large Language Models (LLMs) offer new\npossibilities while raising concerns about accuracy and explainability.\n  This paper explores LLMs for local article classification in Taboola's\n\"Homepage For You\" system, comparing them to traditional techniques. Key\nfindings: (1) Knowledge Graphs enhance NER models' ability to detect implicit\nlocations, (2) LLMs outperform traditional methods, and (3) LLMs can\neffectively identify local content without requiring Knowledge Graph\nintegration.\n  Offline evaluations showed LLMs excel at implicit location classification,\nwhile online A/B tests showed a significant increased in local views. A\nscalable pipeline integrating LLM-based location classification boosted local\narticle distribution by 27%, preserving newspapers' brand identity and\nenhancing homepage personalization.\n", "link": "http://arxiv.org/abs/2502.14660v1", "date": "2025-02-20", "relevancy": 1.9246, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Surface%3A%20Uncovering%20Implicit%20Locations%20with%20LLMs%20for%0A%20%20Personalized%20Local%20News&body=Title%3A%20Beyond%20the%20Surface%3A%20Uncovering%20Implicit%20Locations%20with%20LLMs%20for%0A%20%20Personalized%20Local%20News%0AAuthor%3A%20Gali%20Katz%20and%20Hai%20Sitton%20and%20Guy%20Gonen%20and%20Yohay%20Kaplan%0AAbstract%3A%20%20%20News%20recommendation%20systems%20personalize%20homepage%20content%20to%20boost%20engagement%2C%0Abut%20factors%20like%20content%20type%2C%20editorial%20stance%2C%20and%20geographic%20focus%20impact%0Arecommendations.%20Local%20newspapers%20balance%20coverage%20across%20regions%2C%20yet%0Aidentifying%20local%20articles%20is%20challenging%20due%20to%20implicit%20location%20cues%20like%0Aslang%20or%20landmarks.%0A%20%20Traditional%20methods%2C%20such%20as%20Named%20Entity%20Recognition%20%28NER%29%20and%20Knowledge%0AGraphs%2C%20infer%20locations%2C%20but%20Large%20Language%20Models%20%28LLMs%29%20offer%20new%0Apossibilities%20while%20raising%20concerns%20about%20accuracy%20and%20explainability.%0A%20%20This%20paper%20explores%20LLMs%20for%20local%20article%20classification%20in%20Taboola%27s%0A%22Homepage%20For%20You%22%20system%2C%20comparing%20them%20to%20traditional%20techniques.%20Key%0Afindings%3A%20%281%29%20Knowledge%20Graphs%20enhance%20NER%20models%27%20ability%20to%20detect%20implicit%0Alocations%2C%20%282%29%20LLMs%20outperform%20traditional%20methods%2C%20and%20%283%29%20LLMs%20can%0Aeffectively%20identify%20local%20content%20without%20requiring%20Knowledge%20Graph%0Aintegration.%0A%20%20Offline%20evaluations%20showed%20LLMs%20excel%20at%20implicit%20location%20classification%2C%0Awhile%20online%20A/B%20tests%20showed%20a%20significant%20increased%20in%20local%20views.%20A%0Ascalable%20pipeline%20integrating%20LLM-based%20location%20classification%20boosted%20local%0Aarticle%20distribution%20by%2027%25%2C%20preserving%20newspapers%27%20brand%20identity%20and%0Aenhancing%20homepage%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Surface%253A%2520Uncovering%2520Implicit%2520Locations%2520with%2520LLMs%2520for%250A%2520%2520Personalized%2520Local%2520News%26entry.906535625%3DGali%2520Katz%2520and%2520Hai%2520Sitton%2520and%2520Guy%2520Gonen%2520and%2520Yohay%2520Kaplan%26entry.1292438233%3D%2520%2520News%2520recommendation%2520systems%2520personalize%2520homepage%2520content%2520to%2520boost%2520engagement%252C%250Abut%2520factors%2520like%2520content%2520type%252C%2520editorial%2520stance%252C%2520and%2520geographic%2520focus%2520impact%250Arecommendations.%2520Local%2520newspapers%2520balance%2520coverage%2520across%2520regions%252C%2520yet%250Aidentifying%2520local%2520articles%2520is%2520challenging%2520due%2520to%2520implicit%2520location%2520cues%2520like%250Aslang%2520or%2520landmarks.%250A%2520%2520Traditional%2520methods%252C%2520such%2520as%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520and%2520Knowledge%250AGraphs%252C%2520infer%2520locations%252C%2520but%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520new%250Apossibilities%2520while%2520raising%2520concerns%2520about%2520accuracy%2520and%2520explainability.%250A%2520%2520This%2520paper%2520explores%2520LLMs%2520for%2520local%2520article%2520classification%2520in%2520Taboola%2527s%250A%2522Homepage%2520For%2520You%2522%2520system%252C%2520comparing%2520them%2520to%2520traditional%2520techniques.%2520Key%250Afindings%253A%2520%25281%2529%2520Knowledge%2520Graphs%2520enhance%2520NER%2520models%2527%2520ability%2520to%2520detect%2520implicit%250Alocations%252C%2520%25282%2529%2520LLMs%2520outperform%2520traditional%2520methods%252C%2520and%2520%25283%2529%2520LLMs%2520can%250Aeffectively%2520identify%2520local%2520content%2520without%2520requiring%2520Knowledge%2520Graph%250Aintegration.%250A%2520%2520Offline%2520evaluations%2520showed%2520LLMs%2520excel%2520at%2520implicit%2520location%2520classification%252C%250Awhile%2520online%2520A/B%2520tests%2520showed%2520a%2520significant%2520increased%2520in%2520local%2520views.%2520A%250Ascalable%2520pipeline%2520integrating%2520LLM-based%2520location%2520classification%2520boosted%2520local%250Aarticle%2520distribution%2520by%252027%2525%252C%2520preserving%2520newspapers%2527%2520brand%2520identity%2520and%250Aenhancing%2520homepage%2520personalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Surface%3A%20Uncovering%20Implicit%20Locations%20with%20LLMs%20for%0A%20%20Personalized%20Local%20News&entry.906535625=Gali%20Katz%20and%20Hai%20Sitton%20and%20Guy%20Gonen%20and%20Yohay%20Kaplan&entry.1292438233=%20%20News%20recommendation%20systems%20personalize%20homepage%20content%20to%20boost%20engagement%2C%0Abut%20factors%20like%20content%20type%2C%20editorial%20stance%2C%20and%20geographic%20focus%20impact%0Arecommendations.%20Local%20newspapers%20balance%20coverage%20across%20regions%2C%20yet%0Aidentifying%20local%20articles%20is%20challenging%20due%20to%20implicit%20location%20cues%20like%0Aslang%20or%20landmarks.%0A%20%20Traditional%20methods%2C%20such%20as%20Named%20Entity%20Recognition%20%28NER%29%20and%20Knowledge%0AGraphs%2C%20infer%20locations%2C%20but%20Large%20Language%20Models%20%28LLMs%29%20offer%20new%0Apossibilities%20while%20raising%20concerns%20about%20accuracy%20and%20explainability.%0A%20%20This%20paper%20explores%20LLMs%20for%20local%20article%20classification%20in%20Taboola%27s%0A%22Homepage%20For%20You%22%20system%2C%20comparing%20them%20to%20traditional%20techniques.%20Key%0Afindings%3A%20%281%29%20Knowledge%20Graphs%20enhance%20NER%20models%27%20ability%20to%20detect%20implicit%0Alocations%2C%20%282%29%20LLMs%20outperform%20traditional%20methods%2C%20and%20%283%29%20LLMs%20can%0Aeffectively%20identify%20local%20content%20without%20requiring%20Knowledge%20Graph%0Aintegration.%0A%20%20Offline%20evaluations%20showed%20LLMs%20excel%20at%20implicit%20location%20classification%2C%0Awhile%20online%20A/B%20tests%20showed%20a%20significant%20increased%20in%20local%20views.%20A%0Ascalable%20pipeline%20integrating%20LLM-based%20location%20classification%20boosted%20local%0Aarticle%20distribution%20by%2027%25%2C%20preserving%20newspapers%27%20brand%20identity%20and%0Aenhancing%20homepage%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14660v1&entry.124074799=Read"},
{"title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling\n  Verification", "author": "Eric Zhao and Pranjal Awasthi and Sreenivas Gollapudi", "abstract": "  Sampling-based search, a simple paradigm for utilizing test-time compute,\ninvolves generating multiple candidate responses and selecting the best one --\ntypically by having models self-verify each response for correctness. In this\npaper, we study the scaling trends governing sampling-based search. Among our\nfindings is that simply scaling up a minimalist implementation of\nsampling-based search, using only random sampling and direct self-verification,\nprovides a practical inference method that, for example, elevates the reasoning\ncapabilities of Gemini v1.5 Pro above that of o1-Preview on popular benchmarks.\nWe partially attribute the scalability of sampling-based search to a phenomenon\nof implicit scaling, where sampling a larger pool of responses in turn improves\nself-verification accuracy. We further identify two useful principles for\nimproving self-verification capabilities with test-time compute: (1) comparing\nacross responses provides helpful signals about the locations of errors and\nhallucinations, and (2) different model output styles are useful for different\ncontexts -- chains of thought are useful for reasoning but harder to verify. We\nalso find that, though accurate verification can be elicited, frontier models\ndemonstrate remarkably weak out-of-box verification capabilities and introduce\na benchmark to measure progress on these deficiencies.\n", "link": "http://arxiv.org/abs/2502.01839v2", "date": "2025-02-20", "relevancy": 1.9209, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%2C%20Scrutinize%20and%20Scale%3A%20Effective%20Inference-Time%20Search%20by%20Scaling%0A%20%20Verification&body=Title%3A%20Sample%2C%20Scrutinize%20and%20Scale%3A%20Effective%20Inference-Time%20Search%20by%20Scaling%0A%20%20Verification%0AAuthor%3A%20Eric%20Zhao%20and%20Pranjal%20Awasthi%20and%20Sreenivas%20Gollapudi%0AAbstract%3A%20%20%20Sampling-based%20search%2C%20a%20simple%20paradigm%20for%20utilizing%20test-time%20compute%2C%0Ainvolves%20generating%20multiple%20candidate%20responses%20and%20selecting%20the%20best%20one%20--%0Atypically%20by%20having%20models%20self-verify%20each%20response%20for%20correctness.%20In%20this%0Apaper%2C%20we%20study%20the%20scaling%20trends%20governing%20sampling-based%20search.%20Among%20our%0Afindings%20is%20that%20simply%20scaling%20up%20a%20minimalist%20implementation%20of%0Asampling-based%20search%2C%20using%20only%20random%20sampling%20and%20direct%20self-verification%2C%0Aprovides%20a%20practical%20inference%20method%20that%2C%20for%20example%2C%20elevates%20the%20reasoning%0Acapabilities%20of%20Gemini%20v1.5%20Pro%20above%20that%20of%20o1-Preview%20on%20popular%20benchmarks.%0AWe%20partially%20attribute%20the%20scalability%20of%20sampling-based%20search%20to%20a%20phenomenon%0Aof%20implicit%20scaling%2C%20where%20sampling%20a%20larger%20pool%20of%20responses%20in%20turn%20improves%0Aself-verification%20accuracy.%20We%20further%20identify%20two%20useful%20principles%20for%0Aimproving%20self-verification%20capabilities%20with%20test-time%20compute%3A%20%281%29%20comparing%0Aacross%20responses%20provides%20helpful%20signals%20about%20the%20locations%20of%20errors%20and%0Ahallucinations%2C%20and%20%282%29%20different%20model%20output%20styles%20are%20useful%20for%20different%0Acontexts%20--%20chains%20of%20thought%20are%20useful%20for%20reasoning%20but%20harder%20to%20verify.%20We%0Aalso%20find%20that%2C%20though%20accurate%20verification%20can%20be%20elicited%2C%20frontier%20models%0Ademonstrate%20remarkably%20weak%20out-of-box%20verification%20capabilities%20and%20introduce%0Aa%20benchmark%20to%20measure%20progress%20on%20these%20deficiencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%252C%2520Scrutinize%2520and%2520Scale%253A%2520Effective%2520Inference-Time%2520Search%2520by%2520Scaling%250A%2520%2520Verification%26entry.906535625%3DEric%2520Zhao%2520and%2520Pranjal%2520Awasthi%2520and%2520Sreenivas%2520Gollapudi%26entry.1292438233%3D%2520%2520Sampling-based%2520search%252C%2520a%2520simple%2520paradigm%2520for%2520utilizing%2520test-time%2520compute%252C%250Ainvolves%2520generating%2520multiple%2520candidate%2520responses%2520and%2520selecting%2520the%2520best%2520one%2520--%250Atypically%2520by%2520having%2520models%2520self-verify%2520each%2520response%2520for%2520correctness.%2520In%2520this%250Apaper%252C%2520we%2520study%2520the%2520scaling%2520trends%2520governing%2520sampling-based%2520search.%2520Among%2520our%250Afindings%2520is%2520that%2520simply%2520scaling%2520up%2520a%2520minimalist%2520implementation%2520of%250Asampling-based%2520search%252C%2520using%2520only%2520random%2520sampling%2520and%2520direct%2520self-verification%252C%250Aprovides%2520a%2520practical%2520inference%2520method%2520that%252C%2520for%2520example%252C%2520elevates%2520the%2520reasoning%250Acapabilities%2520of%2520Gemini%2520v1.5%2520Pro%2520above%2520that%2520of%2520o1-Preview%2520on%2520popular%2520benchmarks.%250AWe%2520partially%2520attribute%2520the%2520scalability%2520of%2520sampling-based%2520search%2520to%2520a%2520phenomenon%250Aof%2520implicit%2520scaling%252C%2520where%2520sampling%2520a%2520larger%2520pool%2520of%2520responses%2520in%2520turn%2520improves%250Aself-verification%2520accuracy.%2520We%2520further%2520identify%2520two%2520useful%2520principles%2520for%250Aimproving%2520self-verification%2520capabilities%2520with%2520test-time%2520compute%253A%2520%25281%2529%2520comparing%250Aacross%2520responses%2520provides%2520helpful%2520signals%2520about%2520the%2520locations%2520of%2520errors%2520and%250Ahallucinations%252C%2520and%2520%25282%2529%2520different%2520model%2520output%2520styles%2520are%2520useful%2520for%2520different%250Acontexts%2520--%2520chains%2520of%2520thought%2520are%2520useful%2520for%2520reasoning%2520but%2520harder%2520to%2520verify.%2520We%250Aalso%2520find%2520that%252C%2520though%2520accurate%2520verification%2520can%2520be%2520elicited%252C%2520frontier%2520models%250Ademonstrate%2520remarkably%2520weak%2520out-of-box%2520verification%2520capabilities%2520and%2520introduce%250Aa%2520benchmark%2520to%2520measure%2520progress%2520on%2520these%2520deficiencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%2C%20Scrutinize%20and%20Scale%3A%20Effective%20Inference-Time%20Search%20by%20Scaling%0A%20%20Verification&entry.906535625=Eric%20Zhao%20and%20Pranjal%20Awasthi%20and%20Sreenivas%20Gollapudi&entry.1292438233=%20%20Sampling-based%20search%2C%20a%20simple%20paradigm%20for%20utilizing%20test-time%20compute%2C%0Ainvolves%20generating%20multiple%20candidate%20responses%20and%20selecting%20the%20best%20one%20--%0Atypically%20by%20having%20models%20self-verify%20each%20response%20for%20correctness.%20In%20this%0Apaper%2C%20we%20study%20the%20scaling%20trends%20governing%20sampling-based%20search.%20Among%20our%0Afindings%20is%20that%20simply%20scaling%20up%20a%20minimalist%20implementation%20of%0Asampling-based%20search%2C%20using%20only%20random%20sampling%20and%20direct%20self-verification%2C%0Aprovides%20a%20practical%20inference%20method%20that%2C%20for%20example%2C%20elevates%20the%20reasoning%0Acapabilities%20of%20Gemini%20v1.5%20Pro%20above%20that%20of%20o1-Preview%20on%20popular%20benchmarks.%0AWe%20partially%20attribute%20the%20scalability%20of%20sampling-based%20search%20to%20a%20phenomenon%0Aof%20implicit%20scaling%2C%20where%20sampling%20a%20larger%20pool%20of%20responses%20in%20turn%20improves%0Aself-verification%20accuracy.%20We%20further%20identify%20two%20useful%20principles%20for%0Aimproving%20self-verification%20capabilities%20with%20test-time%20compute%3A%20%281%29%20comparing%0Aacross%20responses%20provides%20helpful%20signals%20about%20the%20locations%20of%20errors%20and%0Ahallucinations%2C%20and%20%282%29%20different%20model%20output%20styles%20are%20useful%20for%20different%0Acontexts%20--%20chains%20of%20thought%20are%20useful%20for%20reasoning%20but%20harder%20to%20verify.%20We%0Aalso%20find%20that%2C%20though%20accurate%20verification%20can%20be%20elicited%2C%20frontier%20models%0Ademonstrate%20remarkably%20weak%20out-of-box%20verification%20capabilities%20and%20introduce%0Aa%20benchmark%20to%20measure%20progress%20on%20these%20deficiencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01839v2&entry.124074799=Read"},
{"title": "Real-Time Device Reach Forecasting Using HLL and MinHash Data Sketches", "author": "Chandrashekar Muniyappa and Kendall Willets and Sriraman Krishnamoorthy", "abstract": "  Predicting the right number of TVs (Device Reach) in real-time based on a\nuser-specified targeting attributes is imperative for running multi-million\ndollar ADs business. The traditional approach of SQL queries to join billions\nof records across multiple targeting dimensions is extremely slow. As a\nworkaround, many applications will have an offline process to crunch these\nnumbers and present the results after many hours. In our case, the solution was\nan offline process taking 24 hours to onboard a customer resulting in a\npotential loss of business. To solve this problem, we have built a new\nreal-time prediction system using MinHash and HyperLogLog (HLL) data sketches\nto compute the device reach at runtime when a user makes a request. However,\nexisting MinHash implementations do not solve the complex problem of multilevel\naggregation and intersection. This work will show how we have solved this\nproblem, in addition, we have improved MinHash algorithm to run 4 times faster\nusing Single Instruction Multiple Data (SIMD) vectorized operations for high\nspeed and accuracy with constant space to process billions of records. Finally,\nby experiments, we prove that the results are as accurate as traditional\noffline prediction system with an acceptable error rate of 5%.\n", "link": "http://arxiv.org/abs/2502.14785v1", "date": "2025-02-20", "relevancy": 1.9194, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.489}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Device%20Reach%20Forecasting%20Using%20HLL%20and%20MinHash%20Data%20Sketches&body=Title%3A%20Real-Time%20Device%20Reach%20Forecasting%20Using%20HLL%20and%20MinHash%20Data%20Sketches%0AAuthor%3A%20Chandrashekar%20Muniyappa%20and%20Kendall%20Willets%20and%20Sriraman%20Krishnamoorthy%0AAbstract%3A%20%20%20Predicting%20the%20right%20number%20of%20TVs%20%28Device%20Reach%29%20in%20real-time%20based%20on%20a%0Auser-specified%20targeting%20attributes%20is%20imperative%20for%20running%20multi-million%0Adollar%20ADs%20business.%20The%20traditional%20approach%20of%20SQL%20queries%20to%20join%20billions%0Aof%20records%20across%20multiple%20targeting%20dimensions%20is%20extremely%20slow.%20As%20a%0Aworkaround%2C%20many%20applications%20will%20have%20an%20offline%20process%20to%20crunch%20these%0Anumbers%20and%20present%20the%20results%20after%20many%20hours.%20In%20our%20case%2C%20the%20solution%20was%0Aan%20offline%20process%20taking%2024%20hours%20to%20onboard%20a%20customer%20resulting%20in%20a%0Apotential%20loss%20of%20business.%20To%20solve%20this%20problem%2C%20we%20have%20built%20a%20new%0Areal-time%20prediction%20system%20using%20MinHash%20and%20HyperLogLog%20%28HLL%29%20data%20sketches%0Ato%20compute%20the%20device%20reach%20at%20runtime%20when%20a%20user%20makes%20a%20request.%20However%2C%0Aexisting%20MinHash%20implementations%20do%20not%20solve%20the%20complex%20problem%20of%20multilevel%0Aaggregation%20and%20intersection.%20This%20work%20will%20show%20how%20we%20have%20solved%20this%0Aproblem%2C%20in%20addition%2C%20we%20have%20improved%20MinHash%20algorithm%20to%20run%204%20times%20faster%0Ausing%20Single%20Instruction%20Multiple%20Data%20%28SIMD%29%20vectorized%20operations%20for%20high%0Aspeed%20and%20accuracy%20with%20constant%20space%20to%20process%20billions%20of%20records.%20Finally%2C%0Aby%20experiments%2C%20we%20prove%20that%20the%20results%20are%20as%20accurate%20as%20traditional%0Aoffline%20prediction%20system%20with%20an%20acceptable%20error%20rate%20of%205%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Device%2520Reach%2520Forecasting%2520Using%2520HLL%2520and%2520MinHash%2520Data%2520Sketches%26entry.906535625%3DChandrashekar%2520Muniyappa%2520and%2520Kendall%2520Willets%2520and%2520Sriraman%2520Krishnamoorthy%26entry.1292438233%3D%2520%2520Predicting%2520the%2520right%2520number%2520of%2520TVs%2520%2528Device%2520Reach%2529%2520in%2520real-time%2520based%2520on%2520a%250Auser-specified%2520targeting%2520attributes%2520is%2520imperative%2520for%2520running%2520multi-million%250Adollar%2520ADs%2520business.%2520The%2520traditional%2520approach%2520of%2520SQL%2520queries%2520to%2520join%2520billions%250Aof%2520records%2520across%2520multiple%2520targeting%2520dimensions%2520is%2520extremely%2520slow.%2520As%2520a%250Aworkaround%252C%2520many%2520applications%2520will%2520have%2520an%2520offline%2520process%2520to%2520crunch%2520these%250Anumbers%2520and%2520present%2520the%2520results%2520after%2520many%2520hours.%2520In%2520our%2520case%252C%2520the%2520solution%2520was%250Aan%2520offline%2520process%2520taking%252024%2520hours%2520to%2520onboard%2520a%2520customer%2520resulting%2520in%2520a%250Apotential%2520loss%2520of%2520business.%2520To%2520solve%2520this%2520problem%252C%2520we%2520have%2520built%2520a%2520new%250Areal-time%2520prediction%2520system%2520using%2520MinHash%2520and%2520HyperLogLog%2520%2528HLL%2529%2520data%2520sketches%250Ato%2520compute%2520the%2520device%2520reach%2520at%2520runtime%2520when%2520a%2520user%2520makes%2520a%2520request.%2520However%252C%250Aexisting%2520MinHash%2520implementations%2520do%2520not%2520solve%2520the%2520complex%2520problem%2520of%2520multilevel%250Aaggregation%2520and%2520intersection.%2520This%2520work%2520will%2520show%2520how%2520we%2520have%2520solved%2520this%250Aproblem%252C%2520in%2520addition%252C%2520we%2520have%2520improved%2520MinHash%2520algorithm%2520to%2520run%25204%2520times%2520faster%250Ausing%2520Single%2520Instruction%2520Multiple%2520Data%2520%2528SIMD%2529%2520vectorized%2520operations%2520for%2520high%250Aspeed%2520and%2520accuracy%2520with%2520constant%2520space%2520to%2520process%2520billions%2520of%2520records.%2520Finally%252C%250Aby%2520experiments%252C%2520we%2520prove%2520that%2520the%2520results%2520are%2520as%2520accurate%2520as%2520traditional%250Aoffline%2520prediction%2520system%2520with%2520an%2520acceptable%2520error%2520rate%2520of%25205%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Device%20Reach%20Forecasting%20Using%20HLL%20and%20MinHash%20Data%20Sketches&entry.906535625=Chandrashekar%20Muniyappa%20and%20Kendall%20Willets%20and%20Sriraman%20Krishnamoorthy&entry.1292438233=%20%20Predicting%20the%20right%20number%20of%20TVs%20%28Device%20Reach%29%20in%20real-time%20based%20on%20a%0Auser-specified%20targeting%20attributes%20is%20imperative%20for%20running%20multi-million%0Adollar%20ADs%20business.%20The%20traditional%20approach%20of%20SQL%20queries%20to%20join%20billions%0Aof%20records%20across%20multiple%20targeting%20dimensions%20is%20extremely%20slow.%20As%20a%0Aworkaround%2C%20many%20applications%20will%20have%20an%20offline%20process%20to%20crunch%20these%0Anumbers%20and%20present%20the%20results%20after%20many%20hours.%20In%20our%20case%2C%20the%20solution%20was%0Aan%20offline%20process%20taking%2024%20hours%20to%20onboard%20a%20customer%20resulting%20in%20a%0Apotential%20loss%20of%20business.%20To%20solve%20this%20problem%2C%20we%20have%20built%20a%20new%0Areal-time%20prediction%20system%20using%20MinHash%20and%20HyperLogLog%20%28HLL%29%20data%20sketches%0Ato%20compute%20the%20device%20reach%20at%20runtime%20when%20a%20user%20makes%20a%20request.%20However%2C%0Aexisting%20MinHash%20implementations%20do%20not%20solve%20the%20complex%20problem%20of%20multilevel%0Aaggregation%20and%20intersection.%20This%20work%20will%20show%20how%20we%20have%20solved%20this%0Aproblem%2C%20in%20addition%2C%20we%20have%20improved%20MinHash%20algorithm%20to%20run%204%20times%20faster%0Ausing%20Single%20Instruction%20Multiple%20Data%20%28SIMD%29%20vectorized%20operations%20for%20high%0Aspeed%20and%20accuracy%20with%20constant%20space%20to%20process%20billions%20of%20records.%20Finally%2C%0Aby%20experiments%2C%20we%20prove%20that%20the%20results%20are%20as%20accurate%20as%20traditional%0Aoffline%20prediction%20system%20with%20an%20acceptable%20error%20rate%20of%205%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14785v1&entry.124074799=Read"},
{"title": "A Neural Operator-Based Emulator for Regional Shallow Water Dynamics", "author": "Peter Rivera-Casillas and Sourav Dutta and Shukai Cai and Mark Loveland and Kamaljyoti Nath and Khemraj Shukla and Corey Trahan and Jonghyun Lee and Matthew Farthing and Clint Dawson", "abstract": "  Coastal regions are particularly vulnerable to the impacts of rising sea\nlevels and extreme weather events. Accurate real-time forecasting of\nhydrodynamic processes in these areas is essential for infrastructure planning\nand climate adaptation. In this study, we present the Multiple-Input Temporal\nOperator Network (MITONet), a novel autoregressive neural emulator that employs\ndimensionality reduction to efficiently approximate high-dimensional numerical\nsolvers for complex, nonlinear problems that are governed by time-dependent,\nparameterized partial differential equations. Although MITONet is applicable to\na wide range of problems, we showcase its capabilities by forecasting regional\ntide-driven dynamics described by the two-dimensional shallow-water equations,\nwhile incorporating initial conditions, boundary conditions, and a varying\ndomain parameter. We demonstrate MITONet's performance in a real-world\napplication, highlighting its ability to make accurate predictions by\nextrapolating both in time and parametric space.\n", "link": "http://arxiv.org/abs/2502.14782v1", "date": "2025-02-20", "relevancy": 1.9166, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.476}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neural%20Operator-Based%20Emulator%20for%20Regional%20Shallow%20Water%20Dynamics&body=Title%3A%20A%20Neural%20Operator-Based%20Emulator%20for%20Regional%20Shallow%20Water%20Dynamics%0AAuthor%3A%20Peter%20Rivera-Casillas%20and%20Sourav%20Dutta%20and%20Shukai%20Cai%20and%20Mark%20Loveland%20and%20Kamaljyoti%20Nath%20and%20Khemraj%20Shukla%20and%20Corey%20Trahan%20and%20Jonghyun%20Lee%20and%20Matthew%20Farthing%20and%20Clint%20Dawson%0AAbstract%3A%20%20%20Coastal%20regions%20are%20particularly%20vulnerable%20to%20the%20impacts%20of%20rising%20sea%0Alevels%20and%20extreme%20weather%20events.%20Accurate%20real-time%20forecasting%20of%0Ahydrodynamic%20processes%20in%20these%20areas%20is%20essential%20for%20infrastructure%20planning%0Aand%20climate%20adaptation.%20In%20this%20study%2C%20we%20present%20the%20Multiple-Input%20Temporal%0AOperator%20Network%20%28MITONet%29%2C%20a%20novel%20autoregressive%20neural%20emulator%20that%20employs%0Adimensionality%20reduction%20to%20efficiently%20approximate%20high-dimensional%20numerical%0Asolvers%20for%20complex%2C%20nonlinear%20problems%20that%20are%20governed%20by%20time-dependent%2C%0Aparameterized%20partial%20differential%20equations.%20Although%20MITONet%20is%20applicable%20to%0Aa%20wide%20range%20of%20problems%2C%20we%20showcase%20its%20capabilities%20by%20forecasting%20regional%0Atide-driven%20dynamics%20described%20by%20the%20two-dimensional%20shallow-water%20equations%2C%0Awhile%20incorporating%20initial%20conditions%2C%20boundary%20conditions%2C%20and%20a%20varying%0Adomain%20parameter.%20We%20demonstrate%20MITONet%27s%20performance%20in%20a%20real-world%0Aapplication%2C%20highlighting%20its%20ability%20to%20make%20accurate%20predictions%20by%0Aextrapolating%20both%20in%20time%20and%20parametric%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neural%2520Operator-Based%2520Emulator%2520for%2520Regional%2520Shallow%2520Water%2520Dynamics%26entry.906535625%3DPeter%2520Rivera-Casillas%2520and%2520Sourav%2520Dutta%2520and%2520Shukai%2520Cai%2520and%2520Mark%2520Loveland%2520and%2520Kamaljyoti%2520Nath%2520and%2520Khemraj%2520Shukla%2520and%2520Corey%2520Trahan%2520and%2520Jonghyun%2520Lee%2520and%2520Matthew%2520Farthing%2520and%2520Clint%2520Dawson%26entry.1292438233%3D%2520%2520Coastal%2520regions%2520are%2520particularly%2520vulnerable%2520to%2520the%2520impacts%2520of%2520rising%2520sea%250Alevels%2520and%2520extreme%2520weather%2520events.%2520Accurate%2520real-time%2520forecasting%2520of%250Ahydrodynamic%2520processes%2520in%2520these%2520areas%2520is%2520essential%2520for%2520infrastructure%2520planning%250Aand%2520climate%2520adaptation.%2520In%2520this%2520study%252C%2520we%2520present%2520the%2520Multiple-Input%2520Temporal%250AOperator%2520Network%2520%2528MITONet%2529%252C%2520a%2520novel%2520autoregressive%2520neural%2520emulator%2520that%2520employs%250Adimensionality%2520reduction%2520to%2520efficiently%2520approximate%2520high-dimensional%2520numerical%250Asolvers%2520for%2520complex%252C%2520nonlinear%2520problems%2520that%2520are%2520governed%2520by%2520time-dependent%252C%250Aparameterized%2520partial%2520differential%2520equations.%2520Although%2520MITONet%2520is%2520applicable%2520to%250Aa%2520wide%2520range%2520of%2520problems%252C%2520we%2520showcase%2520its%2520capabilities%2520by%2520forecasting%2520regional%250Atide-driven%2520dynamics%2520described%2520by%2520the%2520two-dimensional%2520shallow-water%2520equations%252C%250Awhile%2520incorporating%2520initial%2520conditions%252C%2520boundary%2520conditions%252C%2520and%2520a%2520varying%250Adomain%2520parameter.%2520We%2520demonstrate%2520MITONet%2527s%2520performance%2520in%2520a%2520real-world%250Aapplication%252C%2520highlighting%2520its%2520ability%2520to%2520make%2520accurate%2520predictions%2520by%250Aextrapolating%2520both%2520in%2520time%2520and%2520parametric%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neural%20Operator-Based%20Emulator%20for%20Regional%20Shallow%20Water%20Dynamics&entry.906535625=Peter%20Rivera-Casillas%20and%20Sourav%20Dutta%20and%20Shukai%20Cai%20and%20Mark%20Loveland%20and%20Kamaljyoti%20Nath%20and%20Khemraj%20Shukla%20and%20Corey%20Trahan%20and%20Jonghyun%20Lee%20and%20Matthew%20Farthing%20and%20Clint%20Dawson&entry.1292438233=%20%20Coastal%20regions%20are%20particularly%20vulnerable%20to%20the%20impacts%20of%20rising%20sea%0Alevels%20and%20extreme%20weather%20events.%20Accurate%20real-time%20forecasting%20of%0Ahydrodynamic%20processes%20in%20these%20areas%20is%20essential%20for%20infrastructure%20planning%0Aand%20climate%20adaptation.%20In%20this%20study%2C%20we%20present%20the%20Multiple-Input%20Temporal%0AOperator%20Network%20%28MITONet%29%2C%20a%20novel%20autoregressive%20neural%20emulator%20that%20employs%0Adimensionality%20reduction%20to%20efficiently%20approximate%20high-dimensional%20numerical%0Asolvers%20for%20complex%2C%20nonlinear%20problems%20that%20are%20governed%20by%20time-dependent%2C%0Aparameterized%20partial%20differential%20equations.%20Although%20MITONet%20is%20applicable%20to%0Aa%20wide%20range%20of%20problems%2C%20we%20showcase%20its%20capabilities%20by%20forecasting%20regional%0Atide-driven%20dynamics%20described%20by%20the%20two-dimensional%20shallow-water%20equations%2C%0Awhile%20incorporating%20initial%20conditions%2C%20boundary%20conditions%2C%20and%20a%20varying%0Adomain%20parameter.%20We%20demonstrate%20MITONet%27s%20performance%20in%20a%20real-world%0Aapplication%2C%20highlighting%20its%20ability%20to%20make%20accurate%20predictions%20by%0Aextrapolating%20both%20in%20time%20and%20parametric%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14782v1&entry.124074799=Read"},
{"title": "Efficient Adaptive Experimental Design for Average Treatment Effect\n  Estimation", "author": "Masahiro Kato and Takuya Ishihara and Junya Honda and Yusuke Narita", "abstract": "  We study how to efficiently estimate average treatment effects (ATEs) using\nadaptive experiments. In adaptive experiments, experimenters sequentially\nassign treatments to experimental units while updating treatment assignment\nprobabilities based on past data. We start by defining the efficient\ntreatment-assignment probability, which minimizes the semiparametric efficiency\nbound for ATE estimation. Our proposed experimental design estimates and uses\nthe efficient treatment-assignment probability to assign treatments. At the end\nof the proposed design, the experimenter estimates the ATE using a newly\nproposed Adaptive Augmented Inverse Probability Weighting (A2IPW) estimator. We\nshow that the asymptotic variance of the A2IPW estimator using data from the\nproposed design achieves the minimized semiparametric efficiency bound. We also\nanalyze the estimator's finite-sample properties and develop nonparametric and\nnonasymptotic confidence intervals that are valid at any round of the proposed\ndesign. These anytime valid confidence intervals allow us to conduct\nrate-optimal sequential hypothesis testing, allowing for early stopping and\nreducing necessary sample size.\n", "link": "http://arxiv.org/abs/2002.05308v7", "date": "2025-02-20", "relevancy": 1.9142, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3944}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3801}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adaptive%20Experimental%20Design%20for%20Average%20Treatment%20Effect%0A%20%20Estimation&body=Title%3A%20Efficient%20Adaptive%20Experimental%20Design%20for%20Average%20Treatment%20Effect%0A%20%20Estimation%0AAuthor%3A%20Masahiro%20Kato%20and%20Takuya%20Ishihara%20and%20Junya%20Honda%20and%20Yusuke%20Narita%0AAbstract%3A%20%20%20We%20study%20how%20to%20efficiently%20estimate%20average%20treatment%20effects%20%28ATEs%29%20using%0Aadaptive%20experiments.%20In%20adaptive%20experiments%2C%20experimenters%20sequentially%0Aassign%20treatments%20to%20experimental%20units%20while%20updating%20treatment%20assignment%0Aprobabilities%20based%20on%20past%20data.%20We%20start%20by%20defining%20the%20efficient%0Atreatment-assignment%20probability%2C%20which%20minimizes%20the%20semiparametric%20efficiency%0Abound%20for%20ATE%20estimation.%20Our%20proposed%20experimental%20design%20estimates%20and%20uses%0Athe%20efficient%20treatment-assignment%20probability%20to%20assign%20treatments.%20At%20the%20end%0Aof%20the%20proposed%20design%2C%20the%20experimenter%20estimates%20the%20ATE%20using%20a%20newly%0Aproposed%20Adaptive%20Augmented%20Inverse%20Probability%20Weighting%20%28A2IPW%29%20estimator.%20We%0Ashow%20that%20the%20asymptotic%20variance%20of%20the%20A2IPW%20estimator%20using%20data%20from%20the%0Aproposed%20design%20achieves%20the%20minimized%20semiparametric%20efficiency%20bound.%20We%20also%0Aanalyze%20the%20estimator%27s%20finite-sample%20properties%20and%20develop%20nonparametric%20and%0Anonasymptotic%20confidence%20intervals%20that%20are%20valid%20at%20any%20round%20of%20the%20proposed%0Adesign.%20These%20anytime%20valid%20confidence%20intervals%20allow%20us%20to%20conduct%0Arate-optimal%20sequential%20hypothesis%20testing%2C%20allowing%20for%20early%20stopping%20and%0Areducing%20necessary%20sample%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2002.05308v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adaptive%2520Experimental%2520Design%2520for%2520Average%2520Treatment%2520Effect%250A%2520%2520Estimation%26entry.906535625%3DMasahiro%2520Kato%2520and%2520Takuya%2520Ishihara%2520and%2520Junya%2520Honda%2520and%2520Yusuke%2520Narita%26entry.1292438233%3D%2520%2520We%2520study%2520how%2520to%2520efficiently%2520estimate%2520average%2520treatment%2520effects%2520%2528ATEs%2529%2520using%250Aadaptive%2520experiments.%2520In%2520adaptive%2520experiments%252C%2520experimenters%2520sequentially%250Aassign%2520treatments%2520to%2520experimental%2520units%2520while%2520updating%2520treatment%2520assignment%250Aprobabilities%2520based%2520on%2520past%2520data.%2520We%2520start%2520by%2520defining%2520the%2520efficient%250Atreatment-assignment%2520probability%252C%2520which%2520minimizes%2520the%2520semiparametric%2520efficiency%250Abound%2520for%2520ATE%2520estimation.%2520Our%2520proposed%2520experimental%2520design%2520estimates%2520and%2520uses%250Athe%2520efficient%2520treatment-assignment%2520probability%2520to%2520assign%2520treatments.%2520At%2520the%2520end%250Aof%2520the%2520proposed%2520design%252C%2520the%2520experimenter%2520estimates%2520the%2520ATE%2520using%2520a%2520newly%250Aproposed%2520Adaptive%2520Augmented%2520Inverse%2520Probability%2520Weighting%2520%2528A2IPW%2529%2520estimator.%2520We%250Ashow%2520that%2520the%2520asymptotic%2520variance%2520of%2520the%2520A2IPW%2520estimator%2520using%2520data%2520from%2520the%250Aproposed%2520design%2520achieves%2520the%2520minimized%2520semiparametric%2520efficiency%2520bound.%2520We%2520also%250Aanalyze%2520the%2520estimator%2527s%2520finite-sample%2520properties%2520and%2520develop%2520nonparametric%2520and%250Anonasymptotic%2520confidence%2520intervals%2520that%2520are%2520valid%2520at%2520any%2520round%2520of%2520the%2520proposed%250Adesign.%2520These%2520anytime%2520valid%2520confidence%2520intervals%2520allow%2520us%2520to%2520conduct%250Arate-optimal%2520sequential%2520hypothesis%2520testing%252C%2520allowing%2520for%2520early%2520stopping%2520and%250Areducing%2520necessary%2520sample%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2002.05308v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adaptive%20Experimental%20Design%20for%20Average%20Treatment%20Effect%0A%20%20Estimation&entry.906535625=Masahiro%20Kato%20and%20Takuya%20Ishihara%20and%20Junya%20Honda%20and%20Yusuke%20Narita&entry.1292438233=%20%20We%20study%20how%20to%20efficiently%20estimate%20average%20treatment%20effects%20%28ATEs%29%20using%0Aadaptive%20experiments.%20In%20adaptive%20experiments%2C%20experimenters%20sequentially%0Aassign%20treatments%20to%20experimental%20units%20while%20updating%20treatment%20assignment%0Aprobabilities%20based%20on%20past%20data.%20We%20start%20by%20defining%20the%20efficient%0Atreatment-assignment%20probability%2C%20which%20minimizes%20the%20semiparametric%20efficiency%0Abound%20for%20ATE%20estimation.%20Our%20proposed%20experimental%20design%20estimates%20and%20uses%0Athe%20efficient%20treatment-assignment%20probability%20to%20assign%20treatments.%20At%20the%20end%0Aof%20the%20proposed%20design%2C%20the%20experimenter%20estimates%20the%20ATE%20using%20a%20newly%0Aproposed%20Adaptive%20Augmented%20Inverse%20Probability%20Weighting%20%28A2IPW%29%20estimator.%20We%0Ashow%20that%20the%20asymptotic%20variance%20of%20the%20A2IPW%20estimator%20using%20data%20from%20the%0Aproposed%20design%20achieves%20the%20minimized%20semiparametric%20efficiency%20bound.%20We%20also%0Aanalyze%20the%20estimator%27s%20finite-sample%20properties%20and%20develop%20nonparametric%20and%0Anonasymptotic%20confidence%20intervals%20that%20are%20valid%20at%20any%20round%20of%20the%20proposed%0Adesign.%20These%20anytime%20valid%20confidence%20intervals%20allow%20us%20to%20conduct%0Arate-optimal%20sequential%20hypothesis%20testing%2C%20allowing%20for%20early%20stopping%20and%0Areducing%20necessary%20sample%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2002.05308v7&entry.124074799=Read"},
{"title": "Improving the Diffusability of Autoencoders", "author": "Ivan Skorokhodov and Sharath Girish and Benran Hu and Willi Menapace and Yanyu Li and Rameen Abdal and Sergey Tulyakov and Aliaksandr Siarohin", "abstract": "  Latent diffusion models have emerged as the leading approach for generating\nhigh-quality images and videos, utilizing compressed latent representations to\nreduce the computational burden of the diffusion process. While recent\nadvancements have primarily focused on scaling diffusion backbones and\nimproving autoencoder reconstruction quality, the interaction between these\ncomponents has received comparatively less attention. In this work, we perform\na spectral analysis of modern autoencoders and identify inordinate\nhigh-frequency components in their latent spaces, which are especially\npronounced in the autoencoders with a large bottleneck channel size. We\nhypothesize that this high-frequency component interferes with the\ncoarse-to-fine nature of the diffusion synthesis process and hinders the\ngeneration quality. To mitigate the issue, we propose scale equivariance: a\nsimple regularization strategy that aligns latent and RGB spaces across\nfrequencies by enforcing scale equivariance in the decoder. It requires minimal\ncode changes and only up to 20K autoencoder fine-tuning steps, yet\nsignificantly improves generation quality, reducing FID by 19% for image\ngeneration on ImageNet-1K 256x256 and FVD by at least 44% for video generation\non Kinetics-700 17x256x256.\n", "link": "http://arxiv.org/abs/2502.14831v1", "date": "2025-02-20", "relevancy": 1.9122, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.644}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6359}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Diffusability%20of%20Autoencoders&body=Title%3A%20Improving%20the%20Diffusability%20of%20Autoencoders%0AAuthor%3A%20Ivan%20Skorokhodov%20and%20Sharath%20Girish%20and%20Benran%20Hu%20and%20Willi%20Menapace%20and%20Yanyu%20Li%20and%20Rameen%20Abdal%20and%20Sergey%20Tulyakov%20and%20Aliaksandr%20Siarohin%0AAbstract%3A%20%20%20Latent%20diffusion%20models%20have%20emerged%20as%20the%20leading%20approach%20for%20generating%0Ahigh-quality%20images%20and%20videos%2C%20utilizing%20compressed%20latent%20representations%20to%0Areduce%20the%20computational%20burden%20of%20the%20diffusion%20process.%20While%20recent%0Aadvancements%20have%20primarily%20focused%20on%20scaling%20diffusion%20backbones%20and%0Aimproving%20autoencoder%20reconstruction%20quality%2C%20the%20interaction%20between%20these%0Acomponents%20has%20received%20comparatively%20less%20attention.%20In%20this%20work%2C%20we%20perform%0Aa%20spectral%20analysis%20of%20modern%20autoencoders%20and%20identify%20inordinate%0Ahigh-frequency%20components%20in%20their%20latent%20spaces%2C%20which%20are%20especially%0Apronounced%20in%20the%20autoencoders%20with%20a%20large%20bottleneck%20channel%20size.%20We%0Ahypothesize%20that%20this%20high-frequency%20component%20interferes%20with%20the%0Acoarse-to-fine%20nature%20of%20the%20diffusion%20synthesis%20process%20and%20hinders%20the%0Ageneration%20quality.%20To%20mitigate%20the%20issue%2C%20we%20propose%20scale%20equivariance%3A%20a%0Asimple%20regularization%20strategy%20that%20aligns%20latent%20and%20RGB%20spaces%20across%0Afrequencies%20by%20enforcing%20scale%20equivariance%20in%20the%20decoder.%20It%20requires%20minimal%0Acode%20changes%20and%20only%20up%20to%2020K%20autoencoder%20fine-tuning%20steps%2C%20yet%0Asignificantly%20improves%20generation%20quality%2C%20reducing%20FID%20by%2019%25%20for%20image%0Ageneration%20on%20ImageNet-1K%20256x256%20and%20FVD%20by%20at%20least%2044%25%20for%20video%20generation%0Aon%20Kinetics-700%2017x256x256.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Diffusability%2520of%2520Autoencoders%26entry.906535625%3DIvan%2520Skorokhodov%2520and%2520Sharath%2520Girish%2520and%2520Benran%2520Hu%2520and%2520Willi%2520Menapace%2520and%2520Yanyu%2520Li%2520and%2520Rameen%2520Abdal%2520and%2520Sergey%2520Tulyakov%2520and%2520Aliaksandr%2520Siarohin%26entry.1292438233%3D%2520%2520Latent%2520diffusion%2520models%2520have%2520emerged%2520as%2520the%2520leading%2520approach%2520for%2520generating%250Ahigh-quality%2520images%2520and%2520videos%252C%2520utilizing%2520compressed%2520latent%2520representations%2520to%250Areduce%2520the%2520computational%2520burden%2520of%2520the%2520diffusion%2520process.%2520While%2520recent%250Aadvancements%2520have%2520primarily%2520focused%2520on%2520scaling%2520diffusion%2520backbones%2520and%250Aimproving%2520autoencoder%2520reconstruction%2520quality%252C%2520the%2520interaction%2520between%2520these%250Acomponents%2520has%2520received%2520comparatively%2520less%2520attention.%2520In%2520this%2520work%252C%2520we%2520perform%250Aa%2520spectral%2520analysis%2520of%2520modern%2520autoencoders%2520and%2520identify%2520inordinate%250Ahigh-frequency%2520components%2520in%2520their%2520latent%2520spaces%252C%2520which%2520are%2520especially%250Apronounced%2520in%2520the%2520autoencoders%2520with%2520a%2520large%2520bottleneck%2520channel%2520size.%2520We%250Ahypothesize%2520that%2520this%2520high-frequency%2520component%2520interferes%2520with%2520the%250Acoarse-to-fine%2520nature%2520of%2520the%2520diffusion%2520synthesis%2520process%2520and%2520hinders%2520the%250Ageneration%2520quality.%2520To%2520mitigate%2520the%2520issue%252C%2520we%2520propose%2520scale%2520equivariance%253A%2520a%250Asimple%2520regularization%2520strategy%2520that%2520aligns%2520latent%2520and%2520RGB%2520spaces%2520across%250Afrequencies%2520by%2520enforcing%2520scale%2520equivariance%2520in%2520the%2520decoder.%2520It%2520requires%2520minimal%250Acode%2520changes%2520and%2520only%2520up%2520to%252020K%2520autoencoder%2520fine-tuning%2520steps%252C%2520yet%250Asignificantly%2520improves%2520generation%2520quality%252C%2520reducing%2520FID%2520by%252019%2525%2520for%2520image%250Ageneration%2520on%2520ImageNet-1K%2520256x256%2520and%2520FVD%2520by%2520at%2520least%252044%2525%2520for%2520video%2520generation%250Aon%2520Kinetics-700%252017x256x256.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Diffusability%20of%20Autoencoders&entry.906535625=Ivan%20Skorokhodov%20and%20Sharath%20Girish%20and%20Benran%20Hu%20and%20Willi%20Menapace%20and%20Yanyu%20Li%20and%20Rameen%20Abdal%20and%20Sergey%20Tulyakov%20and%20Aliaksandr%20Siarohin&entry.1292438233=%20%20Latent%20diffusion%20models%20have%20emerged%20as%20the%20leading%20approach%20for%20generating%0Ahigh-quality%20images%20and%20videos%2C%20utilizing%20compressed%20latent%20representations%20to%0Areduce%20the%20computational%20burden%20of%20the%20diffusion%20process.%20While%20recent%0Aadvancements%20have%20primarily%20focused%20on%20scaling%20diffusion%20backbones%20and%0Aimproving%20autoencoder%20reconstruction%20quality%2C%20the%20interaction%20between%20these%0Acomponents%20has%20received%20comparatively%20less%20attention.%20In%20this%20work%2C%20we%20perform%0Aa%20spectral%20analysis%20of%20modern%20autoencoders%20and%20identify%20inordinate%0Ahigh-frequency%20components%20in%20their%20latent%20spaces%2C%20which%20are%20especially%0Apronounced%20in%20the%20autoencoders%20with%20a%20large%20bottleneck%20channel%20size.%20We%0Ahypothesize%20that%20this%20high-frequency%20component%20interferes%20with%20the%0Acoarse-to-fine%20nature%20of%20the%20diffusion%20synthesis%20process%20and%20hinders%20the%0Ageneration%20quality.%20To%20mitigate%20the%20issue%2C%20we%20propose%20scale%20equivariance%3A%20a%0Asimple%20regularization%20strategy%20that%20aligns%20latent%20and%20RGB%20spaces%20across%0Afrequencies%20by%20enforcing%20scale%20equivariance%20in%20the%20decoder.%20It%20requires%20minimal%0Acode%20changes%20and%20only%20up%20to%2020K%20autoencoder%20fine-tuning%20steps%2C%20yet%0Asignificantly%20improves%20generation%20quality%2C%20reducing%20FID%20by%2019%25%20for%20image%0Ageneration%20on%20ImageNet-1K%20256x256%20and%20FVD%20by%20at%20least%2044%25%20for%20video%20generation%0Aon%20Kinetics-700%2017x256x256.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14831v1&entry.124074799=Read"},
{"title": "seqKAN: Sequence processing with Kolmogorov-Arnold Networks", "author": "Tatiana Boura and Stasinos Konstantopoulos", "abstract": "  Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine\nlearning framework that is more interpretable and controllable than the\nmulti-layer perceptron. Various network architectures have been proposed within\nthe KAN framework targeting different tasks and application domains, including\nsequence processing.\n  This paper proposes seqKAN, a new KAN architecture for sequence processing.\nAlthough multiple sequence processing KAN architectures have already been\nproposed, we argue that seqKAN is more faithful to the core concept of the KAN\nframework. Furthermore, we empirically demonstrate that it achieves better\nresults.\n  The empirical evaluation is performed on generated data from a complex\nphysics problem on an interpolation and an extrapolation task. Using this\ndataset we compared seqKAN against a prior KAN network for timeseries\nprediction, recurrent deep networks, and symbolic regression. seqKAN\nsubstantially outperforms all architectures, particularly on the extrapolation\ndataset, while also being the most transparent.\n", "link": "http://arxiv.org/abs/2502.14681v1", "date": "2025-02-20", "relevancy": 1.9114, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20seqKAN%3A%20Sequence%20processing%20with%20Kolmogorov-Arnold%20Networks&body=Title%3A%20seqKAN%3A%20Sequence%20processing%20with%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Tatiana%20Boura%20and%20Stasinos%20Konstantopoulos%0AAbstract%3A%20%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20been%20recently%20proposed%20as%20a%20machine%0Alearning%20framework%20that%20is%20more%20interpretable%20and%20controllable%20than%20the%0Amulti-layer%20perceptron.%20Various%20network%20architectures%20have%20been%20proposed%20within%0Athe%20KAN%20framework%20targeting%20different%20tasks%20and%20application%20domains%2C%20including%0Asequence%20processing.%0A%20%20This%20paper%20proposes%20seqKAN%2C%20a%20new%20KAN%20architecture%20for%20sequence%20processing.%0AAlthough%20multiple%20sequence%20processing%20KAN%20architectures%20have%20already%20been%0Aproposed%2C%20we%20argue%20that%20seqKAN%20is%20more%20faithful%20to%20the%20core%20concept%20of%20the%20KAN%0Aframework.%20Furthermore%2C%20we%20empirically%20demonstrate%20that%20it%20achieves%20better%0Aresults.%0A%20%20The%20empirical%20evaluation%20is%20performed%20on%20generated%20data%20from%20a%20complex%0Aphysics%20problem%20on%20an%20interpolation%20and%20an%20extrapolation%20task.%20Using%20this%0Adataset%20we%20compared%20seqKAN%20against%20a%20prior%20KAN%20network%20for%20timeseries%0Aprediction%2C%20recurrent%20deep%20networks%2C%20and%20symbolic%20regression.%20seqKAN%0Asubstantially%20outperforms%20all%20architectures%2C%20particularly%20on%20the%20extrapolation%0Adataset%2C%20while%20also%20being%20the%20most%20transparent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DseqKAN%253A%2520Sequence%2520processing%2520with%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DTatiana%2520Boura%2520and%2520Stasinos%2520Konstantopoulos%26entry.1292438233%3D%2520%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520have%2520been%2520recently%2520proposed%2520as%2520a%2520machine%250Alearning%2520framework%2520that%2520is%2520more%2520interpretable%2520and%2520controllable%2520than%2520the%250Amulti-layer%2520perceptron.%2520Various%2520network%2520architectures%2520have%2520been%2520proposed%2520within%250Athe%2520KAN%2520framework%2520targeting%2520different%2520tasks%2520and%2520application%2520domains%252C%2520including%250Asequence%2520processing.%250A%2520%2520This%2520paper%2520proposes%2520seqKAN%252C%2520a%2520new%2520KAN%2520architecture%2520for%2520sequence%2520processing.%250AAlthough%2520multiple%2520sequence%2520processing%2520KAN%2520architectures%2520have%2520already%2520been%250Aproposed%252C%2520we%2520argue%2520that%2520seqKAN%2520is%2520more%2520faithful%2520to%2520the%2520core%2520concept%2520of%2520the%2520KAN%250Aframework.%2520Furthermore%252C%2520we%2520empirically%2520demonstrate%2520that%2520it%2520achieves%2520better%250Aresults.%250A%2520%2520The%2520empirical%2520evaluation%2520is%2520performed%2520on%2520generated%2520data%2520from%2520a%2520complex%250Aphysics%2520problem%2520on%2520an%2520interpolation%2520and%2520an%2520extrapolation%2520task.%2520Using%2520this%250Adataset%2520we%2520compared%2520seqKAN%2520against%2520a%2520prior%2520KAN%2520network%2520for%2520timeseries%250Aprediction%252C%2520recurrent%2520deep%2520networks%252C%2520and%2520symbolic%2520regression.%2520seqKAN%250Asubstantially%2520outperforms%2520all%2520architectures%252C%2520particularly%2520on%2520the%2520extrapolation%250Adataset%252C%2520while%2520also%2520being%2520the%2520most%2520transparent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=seqKAN%3A%20Sequence%20processing%20with%20Kolmogorov-Arnold%20Networks&entry.906535625=Tatiana%20Boura%20and%20Stasinos%20Konstantopoulos&entry.1292438233=%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20been%20recently%20proposed%20as%20a%20machine%0Alearning%20framework%20that%20is%20more%20interpretable%20and%20controllable%20than%20the%0Amulti-layer%20perceptron.%20Various%20network%20architectures%20have%20been%20proposed%20within%0Athe%20KAN%20framework%20targeting%20different%20tasks%20and%20application%20domains%2C%20including%0Asequence%20processing.%0A%20%20This%20paper%20proposes%20seqKAN%2C%20a%20new%20KAN%20architecture%20for%20sequence%20processing.%0AAlthough%20multiple%20sequence%20processing%20KAN%20architectures%20have%20already%20been%0Aproposed%2C%20we%20argue%20that%20seqKAN%20is%20more%20faithful%20to%20the%20core%20concept%20of%20the%20KAN%0Aframework.%20Furthermore%2C%20we%20empirically%20demonstrate%20that%20it%20achieves%20better%0Aresults.%0A%20%20The%20empirical%20evaluation%20is%20performed%20on%20generated%20data%20from%20a%20complex%0Aphysics%20problem%20on%20an%20interpolation%20and%20an%20extrapolation%20task.%20Using%20this%0Adataset%20we%20compared%20seqKAN%20against%20a%20prior%20KAN%20network%20for%20timeseries%0Aprediction%2C%20recurrent%20deep%20networks%2C%20and%20symbolic%20regression.%20seqKAN%0Asubstantially%20outperforms%20all%20architectures%2C%20particularly%20on%20the%20extrapolation%0Adataset%2C%20while%20also%20being%20the%20most%20transparent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14681v1&entry.124074799=Read"},
{"title": "An Adversarial Analysis of Thompson Sampling for Full-information Online\n  Learning: from Finite to Infinite Action Spaces", "author": "Alexander Terenin and Jeffrey Negrea", "abstract": "  We develop an analysis of Thompson sampling for online learning under full\nfeedback - also known as prediction with expert advice - where the learner's\nprior is defined over the space of an adversary's future actions, rather than\nthe space of experts. We show regret decomposes into regret the learner\nexpected a priori, plus a prior-robustness-type term we call excess regret. In\nthe classical finite-expert setting, this recovers optimal rates. As an initial\nstep towards practical online learning in settings with a\npotentially-uncountably-infinite number of experts, we show that Thompson\nsampling with a certain Gaussian process prior widely-used in the Bayesian\noptimization literature has a $\\mathcal{O}(\\beta\\sqrt{T\\log(1+\\lambda)})$ rate\nagainst a $\\beta$-bounded $\\lambda$-Lipschitz~adversary.\n", "link": "http://arxiv.org/abs/2502.14790v1", "date": "2025-02-20", "relevancy": 1.9057, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5113}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4766}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adversarial%20Analysis%20of%20Thompson%20Sampling%20for%20Full-information%20Online%0A%20%20Learning%3A%20from%20Finite%20to%20Infinite%20Action%20Spaces&body=Title%3A%20An%20Adversarial%20Analysis%20of%20Thompson%20Sampling%20for%20Full-information%20Online%0A%20%20Learning%3A%20from%20Finite%20to%20Infinite%20Action%20Spaces%0AAuthor%3A%20Alexander%20Terenin%20and%20Jeffrey%20Negrea%0AAbstract%3A%20%20%20We%20develop%20an%20analysis%20of%20Thompson%20sampling%20for%20online%20learning%20under%20full%0Afeedback%20-%20also%20known%20as%20prediction%20with%20expert%20advice%20-%20where%20the%20learner%27s%0Aprior%20is%20defined%20over%20the%20space%20of%20an%20adversary%27s%20future%20actions%2C%20rather%20than%0Athe%20space%20of%20experts.%20We%20show%20regret%20decomposes%20into%20regret%20the%20learner%0Aexpected%20a%20priori%2C%20plus%20a%20prior-robustness-type%20term%20we%20call%20excess%20regret.%20In%0Athe%20classical%20finite-expert%20setting%2C%20this%20recovers%20optimal%20rates.%20As%20an%20initial%0Astep%20towards%20practical%20online%20learning%20in%20settings%20with%20a%0Apotentially-uncountably-infinite%20number%20of%20experts%2C%20we%20show%20that%20Thompson%0Asampling%20with%20a%20certain%20Gaussian%20process%20prior%20widely-used%20in%20the%20Bayesian%0Aoptimization%20literature%20has%20a%20%24%5Cmathcal%7BO%7D%28%5Cbeta%5Csqrt%7BT%5Clog%281%2B%5Clambda%29%7D%29%24%20rate%0Aagainst%20a%20%24%5Cbeta%24-bounded%20%24%5Clambda%24-Lipschitz~adversary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adversarial%2520Analysis%2520of%2520Thompson%2520Sampling%2520for%2520Full-information%2520Online%250A%2520%2520Learning%253A%2520from%2520Finite%2520to%2520Infinite%2520Action%2520Spaces%26entry.906535625%3DAlexander%2520Terenin%2520and%2520Jeffrey%2520Negrea%26entry.1292438233%3D%2520%2520We%2520develop%2520an%2520analysis%2520of%2520Thompson%2520sampling%2520for%2520online%2520learning%2520under%2520full%250Afeedback%2520-%2520also%2520known%2520as%2520prediction%2520with%2520expert%2520advice%2520-%2520where%2520the%2520learner%2527s%250Aprior%2520is%2520defined%2520over%2520the%2520space%2520of%2520an%2520adversary%2527s%2520future%2520actions%252C%2520rather%2520than%250Athe%2520space%2520of%2520experts.%2520We%2520show%2520regret%2520decomposes%2520into%2520regret%2520the%2520learner%250Aexpected%2520a%2520priori%252C%2520plus%2520a%2520prior-robustness-type%2520term%2520we%2520call%2520excess%2520regret.%2520In%250Athe%2520classical%2520finite-expert%2520setting%252C%2520this%2520recovers%2520optimal%2520rates.%2520As%2520an%2520initial%250Astep%2520towards%2520practical%2520online%2520learning%2520in%2520settings%2520with%2520a%250Apotentially-uncountably-infinite%2520number%2520of%2520experts%252C%2520we%2520show%2520that%2520Thompson%250Asampling%2520with%2520a%2520certain%2520Gaussian%2520process%2520prior%2520widely-used%2520in%2520the%2520Bayesian%250Aoptimization%2520literature%2520has%2520a%2520%2524%255Cmathcal%257BO%257D%2528%255Cbeta%255Csqrt%257BT%255Clog%25281%252B%255Clambda%2529%257D%2529%2524%2520rate%250Aagainst%2520a%2520%2524%255Cbeta%2524-bounded%2520%2524%255Clambda%2524-Lipschitz~adversary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adversarial%20Analysis%20of%20Thompson%20Sampling%20for%20Full-information%20Online%0A%20%20Learning%3A%20from%20Finite%20to%20Infinite%20Action%20Spaces&entry.906535625=Alexander%20Terenin%20and%20Jeffrey%20Negrea&entry.1292438233=%20%20We%20develop%20an%20analysis%20of%20Thompson%20sampling%20for%20online%20learning%20under%20full%0Afeedback%20-%20also%20known%20as%20prediction%20with%20expert%20advice%20-%20where%20the%20learner%27s%0Aprior%20is%20defined%20over%20the%20space%20of%20an%20adversary%27s%20future%20actions%2C%20rather%20than%0Athe%20space%20of%20experts.%20We%20show%20regret%20decomposes%20into%20regret%20the%20learner%0Aexpected%20a%20priori%2C%20plus%20a%20prior-robustness-type%20term%20we%20call%20excess%20regret.%20In%0Athe%20classical%20finite-expert%20setting%2C%20this%20recovers%20optimal%20rates.%20As%20an%20initial%0Astep%20towards%20practical%20online%20learning%20in%20settings%20with%20a%0Apotentially-uncountably-infinite%20number%20of%20experts%2C%20we%20show%20that%20Thompson%0Asampling%20with%20a%20certain%20Gaussian%20process%20prior%20widely-used%20in%20the%20Bayesian%0Aoptimization%20literature%20has%20a%20%24%5Cmathcal%7BO%7D%28%5Cbeta%5Csqrt%7BT%5Clog%281%2B%5Clambda%29%7D%29%24%20rate%0Aagainst%20a%20%24%5Cbeta%24-bounded%20%24%5Clambda%24-Lipschitz~adversary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14790v1&entry.124074799=Read"},
{"title": "Human Misperception of Generative-AI Alignment: A Laboratory Experiment", "author": "Kevin He and Ran Shorrer and Mengjia Xia", "abstract": "  We conduct an incentivized laboratory experiment to study people's perception\nof generative artificial intelligence (GenAI) alignment in the context of\neconomic decision-making. Using a panel of economic problems spanning the\ndomains of risk, time preference, social preference, and strategic\ninteractions, we ask human subjects to make choices for themselves and to\npredict the choices made by GenAI on behalf of a human user. We find that\npeople overestimate the degree of alignment between GenAI's choices and human\nchoices. In every problem, human subjects' average prediction about GenAI's\nchoice is substantially closer to the average human-subject choice than it is\nto the GenAI choice. At the individual level, different subjects' predictions\nabout GenAI's choice in a given problem are highly correlated with their own\nchoices in the same problem. We explore the implications of people\noverestimating GenAI alignment in a simple theoretical model.\n", "link": "http://arxiv.org/abs/2502.14708v1", "date": "2025-02-20", "relevancy": 1.9031, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4838}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Misperception%20of%20Generative-AI%20Alignment%3A%20A%20Laboratory%20Experiment&body=Title%3A%20Human%20Misperception%20of%20Generative-AI%20Alignment%3A%20A%20Laboratory%20Experiment%0AAuthor%3A%20Kevin%20He%20and%20Ran%20Shorrer%20and%20Mengjia%20Xia%0AAbstract%3A%20%20%20We%20conduct%20an%20incentivized%20laboratory%20experiment%20to%20study%20people%27s%20perception%0Aof%20generative%20artificial%20intelligence%20%28GenAI%29%20alignment%20in%20the%20context%20of%0Aeconomic%20decision-making.%20Using%20a%20panel%20of%20economic%20problems%20spanning%20the%0Adomains%20of%20risk%2C%20time%20preference%2C%20social%20preference%2C%20and%20strategic%0Ainteractions%2C%20we%20ask%20human%20subjects%20to%20make%20choices%20for%20themselves%20and%20to%0Apredict%20the%20choices%20made%20by%20GenAI%20on%20behalf%20of%20a%20human%20user.%20We%20find%20that%0Apeople%20overestimate%20the%20degree%20of%20alignment%20between%20GenAI%27s%20choices%20and%20human%0Achoices.%20In%20every%20problem%2C%20human%20subjects%27%20average%20prediction%20about%20GenAI%27s%0Achoice%20is%20substantially%20closer%20to%20the%20average%20human-subject%20choice%20than%20it%20is%0Ato%20the%20GenAI%20choice.%20At%20the%20individual%20level%2C%20different%20subjects%27%20predictions%0Aabout%20GenAI%27s%20choice%20in%20a%20given%20problem%20are%20highly%20correlated%20with%20their%20own%0Achoices%20in%20the%20same%20problem.%20We%20explore%20the%20implications%20of%20people%0Aoverestimating%20GenAI%20alignment%20in%20a%20simple%20theoretical%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Misperception%2520of%2520Generative-AI%2520Alignment%253A%2520A%2520Laboratory%2520Experiment%26entry.906535625%3DKevin%2520He%2520and%2520Ran%2520Shorrer%2520and%2520Mengjia%2520Xia%26entry.1292438233%3D%2520%2520We%2520conduct%2520an%2520incentivized%2520laboratory%2520experiment%2520to%2520study%2520people%2527s%2520perception%250Aof%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520alignment%2520in%2520the%2520context%2520of%250Aeconomic%2520decision-making.%2520Using%2520a%2520panel%2520of%2520economic%2520problems%2520spanning%2520the%250Adomains%2520of%2520risk%252C%2520time%2520preference%252C%2520social%2520preference%252C%2520and%2520strategic%250Ainteractions%252C%2520we%2520ask%2520human%2520subjects%2520to%2520make%2520choices%2520for%2520themselves%2520and%2520to%250Apredict%2520the%2520choices%2520made%2520by%2520GenAI%2520on%2520behalf%2520of%2520a%2520human%2520user.%2520We%2520find%2520that%250Apeople%2520overestimate%2520the%2520degree%2520of%2520alignment%2520between%2520GenAI%2527s%2520choices%2520and%2520human%250Achoices.%2520In%2520every%2520problem%252C%2520human%2520subjects%2527%2520average%2520prediction%2520about%2520GenAI%2527s%250Achoice%2520is%2520substantially%2520closer%2520to%2520the%2520average%2520human-subject%2520choice%2520than%2520it%2520is%250Ato%2520the%2520GenAI%2520choice.%2520At%2520the%2520individual%2520level%252C%2520different%2520subjects%2527%2520predictions%250Aabout%2520GenAI%2527s%2520choice%2520in%2520a%2520given%2520problem%2520are%2520highly%2520correlated%2520with%2520their%2520own%250Achoices%2520in%2520the%2520same%2520problem.%2520We%2520explore%2520the%2520implications%2520of%2520people%250Aoverestimating%2520GenAI%2520alignment%2520in%2520a%2520simple%2520theoretical%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Misperception%20of%20Generative-AI%20Alignment%3A%20A%20Laboratory%20Experiment&entry.906535625=Kevin%20He%20and%20Ran%20Shorrer%20and%20Mengjia%20Xia&entry.1292438233=%20%20We%20conduct%20an%20incentivized%20laboratory%20experiment%20to%20study%20people%27s%20perception%0Aof%20generative%20artificial%20intelligence%20%28GenAI%29%20alignment%20in%20the%20context%20of%0Aeconomic%20decision-making.%20Using%20a%20panel%20of%20economic%20problems%20spanning%20the%0Adomains%20of%20risk%2C%20time%20preference%2C%20social%20preference%2C%20and%20strategic%0Ainteractions%2C%20we%20ask%20human%20subjects%20to%20make%20choices%20for%20themselves%20and%20to%0Apredict%20the%20choices%20made%20by%20GenAI%20on%20behalf%20of%20a%20human%20user.%20We%20find%20that%0Apeople%20overestimate%20the%20degree%20of%20alignment%20between%20GenAI%27s%20choices%20and%20human%0Achoices.%20In%20every%20problem%2C%20human%20subjects%27%20average%20prediction%20about%20GenAI%27s%0Achoice%20is%20substantially%20closer%20to%20the%20average%20human-subject%20choice%20than%20it%20is%0Ato%20the%20GenAI%20choice.%20At%20the%20individual%20level%2C%20different%20subjects%27%20predictions%0Aabout%20GenAI%27s%20choice%20in%20a%20given%20problem%20are%20highly%20correlated%20with%20their%20own%0Achoices%20in%20the%20same%20problem.%20We%20explore%20the%20implications%20of%20people%0Aoverestimating%20GenAI%20alignment%20in%20a%20simple%20theoretical%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14708v1&entry.124074799=Read"},
{"title": "Tempo: Helping Data Scientists and Domain Experts Collaboratively\n  Specify Predictive Modeling Tasks", "author": "Venkatesh Sivaraman and Anika Vaishampayan and Xiaotong Li and Brian R Buck and Ziyong Ma and Richard D Boyce and Adam Perer", "abstract": "  Temporal predictive models have the potential to improve decisions in health\ncare, public services, and other domains, yet they often fail to effectively\nsupport decision-makers. Prior literature shows that many misalignments between\nmodel behavior and decision-makers' expectations stem from issues of model\nspecification, namely how, when, and for whom predictions are made. However,\nmodel specifications for predictive tasks are highly technical and difficult\nfor non-data-scientist stakeholders to interpret and critique. To address this\nchallenge we developed Tempo, an interactive system that helps data scientists\nand domain experts collaboratively iterate on model specifications. Using\nTempo's simple yet precise temporal query language, data scientists can quickly\nprototype specifications with greater transparency about pre-processing\nchoices. Moreover, domain experts can assess performance within data subgroups\nto validate that models behave as expected. Through three case studies, we\ndemonstrate how Tempo helps multidisciplinary teams quickly prune infeasible\nspecifications and identify more promising directions to explore.\n", "link": "http://arxiv.org/abs/2502.10526v2", "date": "2025-02-20", "relevancy": 1.8981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4821}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4711}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tempo%3A%20Helping%20Data%20Scientists%20and%20Domain%20Experts%20Collaboratively%0A%20%20Specify%20Predictive%20Modeling%20Tasks&body=Title%3A%20Tempo%3A%20Helping%20Data%20Scientists%20and%20Domain%20Experts%20Collaboratively%0A%20%20Specify%20Predictive%20Modeling%20Tasks%0AAuthor%3A%20Venkatesh%20Sivaraman%20and%20Anika%20Vaishampayan%20and%20Xiaotong%20Li%20and%20Brian%20R%20Buck%20and%20Ziyong%20Ma%20and%20Richard%20D%20Boyce%20and%20Adam%20Perer%0AAbstract%3A%20%20%20Temporal%20predictive%20models%20have%20the%20potential%20to%20improve%20decisions%20in%20health%0Acare%2C%20public%20services%2C%20and%20other%20domains%2C%20yet%20they%20often%20fail%20to%20effectively%0Asupport%20decision-makers.%20Prior%20literature%20shows%20that%20many%20misalignments%20between%0Amodel%20behavior%20and%20decision-makers%27%20expectations%20stem%20from%20issues%20of%20model%0Aspecification%2C%20namely%20how%2C%20when%2C%20and%20for%20whom%20predictions%20are%20made.%20However%2C%0Amodel%20specifications%20for%20predictive%20tasks%20are%20highly%20technical%20and%20difficult%0Afor%20non-data-scientist%20stakeholders%20to%20interpret%20and%20critique.%20To%20address%20this%0Achallenge%20we%20developed%20Tempo%2C%20an%20interactive%20system%20that%20helps%20data%20scientists%0Aand%20domain%20experts%20collaboratively%20iterate%20on%20model%20specifications.%20Using%0ATempo%27s%20simple%20yet%20precise%20temporal%20query%20language%2C%20data%20scientists%20can%20quickly%0Aprototype%20specifications%20with%20greater%20transparency%20about%20pre-processing%0Achoices.%20Moreover%2C%20domain%20experts%20can%20assess%20performance%20within%20data%20subgroups%0Ato%20validate%20that%20models%20behave%20as%20expected.%20Through%20three%20case%20studies%2C%20we%0Ademonstrate%20how%20Tempo%20helps%20multidisciplinary%20teams%20quickly%20prune%20infeasible%0Aspecifications%20and%20identify%20more%20promising%20directions%20to%20explore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempo%253A%2520Helping%2520Data%2520Scientists%2520and%2520Domain%2520Experts%2520Collaboratively%250A%2520%2520Specify%2520Predictive%2520Modeling%2520Tasks%26entry.906535625%3DVenkatesh%2520Sivaraman%2520and%2520Anika%2520Vaishampayan%2520and%2520Xiaotong%2520Li%2520and%2520Brian%2520R%2520Buck%2520and%2520Ziyong%2520Ma%2520and%2520Richard%2520D%2520Boyce%2520and%2520Adam%2520Perer%26entry.1292438233%3D%2520%2520Temporal%2520predictive%2520models%2520have%2520the%2520potential%2520to%2520improve%2520decisions%2520in%2520health%250Acare%252C%2520public%2520services%252C%2520and%2520other%2520domains%252C%2520yet%2520they%2520often%2520fail%2520to%2520effectively%250Asupport%2520decision-makers.%2520Prior%2520literature%2520shows%2520that%2520many%2520misalignments%2520between%250Amodel%2520behavior%2520and%2520decision-makers%2527%2520expectations%2520stem%2520from%2520issues%2520of%2520model%250Aspecification%252C%2520namely%2520how%252C%2520when%252C%2520and%2520for%2520whom%2520predictions%2520are%2520made.%2520However%252C%250Amodel%2520specifications%2520for%2520predictive%2520tasks%2520are%2520highly%2520technical%2520and%2520difficult%250Afor%2520non-data-scientist%2520stakeholders%2520to%2520interpret%2520and%2520critique.%2520To%2520address%2520this%250Achallenge%2520we%2520developed%2520Tempo%252C%2520an%2520interactive%2520system%2520that%2520helps%2520data%2520scientists%250Aand%2520domain%2520experts%2520collaboratively%2520iterate%2520on%2520model%2520specifications.%2520Using%250ATempo%2527s%2520simple%2520yet%2520precise%2520temporal%2520query%2520language%252C%2520data%2520scientists%2520can%2520quickly%250Aprototype%2520specifications%2520with%2520greater%2520transparency%2520about%2520pre-processing%250Achoices.%2520Moreover%252C%2520domain%2520experts%2520can%2520assess%2520performance%2520within%2520data%2520subgroups%250Ato%2520validate%2520that%2520models%2520behave%2520as%2520expected.%2520Through%2520three%2520case%2520studies%252C%2520we%250Ademonstrate%2520how%2520Tempo%2520helps%2520multidisciplinary%2520teams%2520quickly%2520prune%2520infeasible%250Aspecifications%2520and%2520identify%2520more%2520promising%2520directions%2520to%2520explore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tempo%3A%20Helping%20Data%20Scientists%20and%20Domain%20Experts%20Collaboratively%0A%20%20Specify%20Predictive%20Modeling%20Tasks&entry.906535625=Venkatesh%20Sivaraman%20and%20Anika%20Vaishampayan%20and%20Xiaotong%20Li%20and%20Brian%20R%20Buck%20and%20Ziyong%20Ma%20and%20Richard%20D%20Boyce%20and%20Adam%20Perer&entry.1292438233=%20%20Temporal%20predictive%20models%20have%20the%20potential%20to%20improve%20decisions%20in%20health%0Acare%2C%20public%20services%2C%20and%20other%20domains%2C%20yet%20they%20often%20fail%20to%20effectively%0Asupport%20decision-makers.%20Prior%20literature%20shows%20that%20many%20misalignments%20between%0Amodel%20behavior%20and%20decision-makers%27%20expectations%20stem%20from%20issues%20of%20model%0Aspecification%2C%20namely%20how%2C%20when%2C%20and%20for%20whom%20predictions%20are%20made.%20However%2C%0Amodel%20specifications%20for%20predictive%20tasks%20are%20highly%20technical%20and%20difficult%0Afor%20non-data-scientist%20stakeholders%20to%20interpret%20and%20critique.%20To%20address%20this%0Achallenge%20we%20developed%20Tempo%2C%20an%20interactive%20system%20that%20helps%20data%20scientists%0Aand%20domain%20experts%20collaboratively%20iterate%20on%20model%20specifications.%20Using%0ATempo%27s%20simple%20yet%20precise%20temporal%20query%20language%2C%20data%20scientists%20can%20quickly%0Aprototype%20specifications%20with%20greater%20transparency%20about%20pre-processing%0Achoices.%20Moreover%2C%20domain%20experts%20can%20assess%20performance%20within%20data%20subgroups%0Ato%20validate%20that%20models%20behave%20as%20expected.%20Through%20three%20case%20studies%2C%20we%0Ademonstrate%20how%20Tempo%20helps%20multidisciplinary%20teams%20quickly%20prune%20infeasible%0Aspecifications%20and%20identify%20more%20promising%20directions%20to%20explore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10526v2&entry.124074799=Read"},
{"title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of\n  Optimization Formulations", "author": "Haotian Zhai and Connor Lawless and Ellen Vitercik and Liu Leqi", "abstract": "  A fundamental problem in combinatorial optimization is identifying equivalent\nformulations, which can lead to more efficient solution strategies and deeper\ninsights into a problem's computational complexity. The need to automatically\nidentify equivalence between problem formulations has grown as optimization\ncopilots--systems that generate problem formulations from natural language\ndescriptions--have proliferated. However, existing approaches to checking\nformulation equivalence lack grounding, relying on simple heuristics which are\ninsufficient for rigorous validation. Inspired by Karp reductions, in this work\nwe introduce quasi-Karp equivalence, a formal criterion for determining when\ntwo optimization formulations are equivalent based on the existence of a\nmapping between their decision variables. We propose EquivaMap, a framework\nthat leverages large language models to automatically discover such mappings,\nenabling scalable and reliable equivalence verification. To evaluate our\napproach, we construct the first open-source dataset of equivalent optimization\nformulations, generated by applying transformations such as adding slack\nvariables or valid inequalities to existing formulations. Empirically,\nEquivaMap significantly outperforms existing methods, achieving substantial\nimprovements in correctly identifying formulation equivalence.\n", "link": "http://arxiv.org/abs/2502.14760v1", "date": "2025-02-20", "relevancy": 1.896, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4944}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EquivaMap%3A%20Leveraging%20LLMs%20for%20Automatic%20Equivalence%20Checking%20of%0A%20%20Optimization%20Formulations&body=Title%3A%20EquivaMap%3A%20Leveraging%20LLMs%20for%20Automatic%20Equivalence%20Checking%20of%0A%20%20Optimization%20Formulations%0AAuthor%3A%20Haotian%20Zhai%20and%20Connor%20Lawless%20and%20Ellen%20Vitercik%20and%20Liu%20Leqi%0AAbstract%3A%20%20%20A%20fundamental%20problem%20in%20combinatorial%20optimization%20is%20identifying%20equivalent%0Aformulations%2C%20which%20can%20lead%20to%20more%20efficient%20solution%20strategies%20and%20deeper%0Ainsights%20into%20a%20problem%27s%20computational%20complexity.%20The%20need%20to%20automatically%0Aidentify%20equivalence%20between%20problem%20formulations%20has%20grown%20as%20optimization%0Acopilots--systems%20that%20generate%20problem%20formulations%20from%20natural%20language%0Adescriptions--have%20proliferated.%20However%2C%20existing%20approaches%20to%20checking%0Aformulation%20equivalence%20lack%20grounding%2C%20relying%20on%20simple%20heuristics%20which%20are%0Ainsufficient%20for%20rigorous%20validation.%20Inspired%20by%20Karp%20reductions%2C%20in%20this%20work%0Awe%20introduce%20quasi-Karp%20equivalence%2C%20a%20formal%20criterion%20for%20determining%20when%0Atwo%20optimization%20formulations%20are%20equivalent%20based%20on%20the%20existence%20of%20a%0Amapping%20between%20their%20decision%20variables.%20We%20propose%20EquivaMap%2C%20a%20framework%0Athat%20leverages%20large%20language%20models%20to%20automatically%20discover%20such%20mappings%2C%0Aenabling%20scalable%20and%20reliable%20equivalence%20verification.%20To%20evaluate%20our%0Aapproach%2C%20we%20construct%20the%20first%20open-source%20dataset%20of%20equivalent%20optimization%0Aformulations%2C%20generated%20by%20applying%20transformations%20such%20as%20adding%20slack%0Avariables%20or%20valid%20inequalities%20to%20existing%20formulations.%20Empirically%2C%0AEquivaMap%20significantly%20outperforms%20existing%20methods%2C%20achieving%20substantial%0Aimprovements%20in%20correctly%20identifying%20formulation%20equivalence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivaMap%253A%2520Leveraging%2520LLMs%2520for%2520Automatic%2520Equivalence%2520Checking%2520of%250A%2520%2520Optimization%2520Formulations%26entry.906535625%3DHaotian%2520Zhai%2520and%2520Connor%2520Lawless%2520and%2520Ellen%2520Vitercik%2520and%2520Liu%2520Leqi%26entry.1292438233%3D%2520%2520A%2520fundamental%2520problem%2520in%2520combinatorial%2520optimization%2520is%2520identifying%2520equivalent%250Aformulations%252C%2520which%2520can%2520lead%2520to%2520more%2520efficient%2520solution%2520strategies%2520and%2520deeper%250Ainsights%2520into%2520a%2520problem%2527s%2520computational%2520complexity.%2520The%2520need%2520to%2520automatically%250Aidentify%2520equivalence%2520between%2520problem%2520formulations%2520has%2520grown%2520as%2520optimization%250Acopilots--systems%2520that%2520generate%2520problem%2520formulations%2520from%2520natural%2520language%250Adescriptions--have%2520proliferated.%2520However%252C%2520existing%2520approaches%2520to%2520checking%250Aformulation%2520equivalence%2520lack%2520grounding%252C%2520relying%2520on%2520simple%2520heuristics%2520which%2520are%250Ainsufficient%2520for%2520rigorous%2520validation.%2520Inspired%2520by%2520Karp%2520reductions%252C%2520in%2520this%2520work%250Awe%2520introduce%2520quasi-Karp%2520equivalence%252C%2520a%2520formal%2520criterion%2520for%2520determining%2520when%250Atwo%2520optimization%2520formulations%2520are%2520equivalent%2520based%2520on%2520the%2520existence%2520of%2520a%250Amapping%2520between%2520their%2520decision%2520variables.%2520We%2520propose%2520EquivaMap%252C%2520a%2520framework%250Athat%2520leverages%2520large%2520language%2520models%2520to%2520automatically%2520discover%2520such%2520mappings%252C%250Aenabling%2520scalable%2520and%2520reliable%2520equivalence%2520verification.%2520To%2520evaluate%2520our%250Aapproach%252C%2520we%2520construct%2520the%2520first%2520open-source%2520dataset%2520of%2520equivalent%2520optimization%250Aformulations%252C%2520generated%2520by%2520applying%2520transformations%2520such%2520as%2520adding%2520slack%250Avariables%2520or%2520valid%2520inequalities%2520to%2520existing%2520formulations.%2520Empirically%252C%250AEquivaMap%2520significantly%2520outperforms%2520existing%2520methods%252C%2520achieving%2520substantial%250Aimprovements%2520in%2520correctly%2520identifying%2520formulation%2520equivalence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EquivaMap%3A%20Leveraging%20LLMs%20for%20Automatic%20Equivalence%20Checking%20of%0A%20%20Optimization%20Formulations&entry.906535625=Haotian%20Zhai%20and%20Connor%20Lawless%20and%20Ellen%20Vitercik%20and%20Liu%20Leqi&entry.1292438233=%20%20A%20fundamental%20problem%20in%20combinatorial%20optimization%20is%20identifying%20equivalent%0Aformulations%2C%20which%20can%20lead%20to%20more%20efficient%20solution%20strategies%20and%20deeper%0Ainsights%20into%20a%20problem%27s%20computational%20complexity.%20The%20need%20to%20automatically%0Aidentify%20equivalence%20between%20problem%20formulations%20has%20grown%20as%20optimization%0Acopilots--systems%20that%20generate%20problem%20formulations%20from%20natural%20language%0Adescriptions--have%20proliferated.%20However%2C%20existing%20approaches%20to%20checking%0Aformulation%20equivalence%20lack%20grounding%2C%20relying%20on%20simple%20heuristics%20which%20are%0Ainsufficient%20for%20rigorous%20validation.%20Inspired%20by%20Karp%20reductions%2C%20in%20this%20work%0Awe%20introduce%20quasi-Karp%20equivalence%2C%20a%20formal%20criterion%20for%20determining%20when%0Atwo%20optimization%20formulations%20are%20equivalent%20based%20on%20the%20existence%20of%20a%0Amapping%20between%20their%20decision%20variables.%20We%20propose%20EquivaMap%2C%20a%20framework%0Athat%20leverages%20large%20language%20models%20to%20automatically%20discover%20such%20mappings%2C%0Aenabling%20scalable%20and%20reliable%20equivalence%20verification.%20To%20evaluate%20our%0Aapproach%2C%20we%20construct%20the%20first%20open-source%20dataset%20of%20equivalent%20optimization%0Aformulations%2C%20generated%20by%20applying%20transformations%20such%20as%20adding%20slack%0Avariables%20or%20valid%20inequalities%20to%20existing%20formulations.%20Empirically%2C%0AEquivaMap%20significantly%20outperforms%20existing%20methods%2C%20achieving%20substantial%0Aimprovements%20in%20correctly%20identifying%20formulation%20equivalence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14760v1&entry.124074799=Read"},
{"title": "Data-Constrained Synthesis of Training Data for De-Identification", "author": "Thomas Vakili and Aron Henriksson and Hercules Dalianis", "abstract": "  Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.\n", "link": "http://arxiv.org/abs/2502.14677v1", "date": "2025-02-20", "relevancy": 1.8884, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4842}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4741}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Constrained%20Synthesis%20of%20Training%20Data%20for%20De-Identification&body=Title%3A%20Data-Constrained%20Synthesis%20of%20Training%20Data%20for%20De-Identification%0AAuthor%3A%20Thomas%20Vakili%20and%20Aron%20Henriksson%20and%20Hercules%20Dalianis%0AAbstract%3A%20%20%20Many%20sensitive%20domains%20--%20such%20as%20the%20clinical%20domain%20--%20lack%20widely%0Aavailable%20datasets%20due%20to%20privacy%20risks.%20The%20increasing%20generative%20capabilities%0Aof%20large%20language%20models%20%28LLMs%29%20have%20made%20synthetic%20datasets%20a%20viable%20path%0Aforward.%20In%20this%20study%2C%20we%20domain-adapt%20LLMs%20to%20the%20clinical%20domain%20and%0Agenerate%20synthetic%20clinical%20texts%20that%20are%20machine-annotated%20with%20tags%20for%0Apersonally%20identifiable%20information%20using%20capable%20encoder-based%20NER%20models.%20The%0Asynthetic%20corpora%20are%20then%20used%20to%20train%20synthetic%20NER%20models.%20The%20results%20show%0Athat%20training%20NER%20models%20using%20synthetic%20corpora%20incurs%20only%20a%20small%20drop%20in%0Apredictive%20performance.%20The%20limits%20of%20this%20process%20are%20investigated%20in%20a%0Asystematic%20ablation%20study%20--%20using%20both%20Swedish%20and%20Spanish%20data.%20Our%20analysis%0Ashows%20that%20smaller%20datasets%20can%20be%20sufficient%20for%20domain-adapting%20LLMs%20for%20data%0Asynthesis.%20Instead%2C%20the%20effectiveness%20of%20this%20process%20is%20almost%20entirely%0Acontingent%20on%20the%20performance%20of%20the%20machine-annotating%20NER%20models%20trained%0Ausing%20the%20original%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Constrained%2520Synthesis%2520of%2520Training%2520Data%2520for%2520De-Identification%26entry.906535625%3DThomas%2520Vakili%2520and%2520Aron%2520Henriksson%2520and%2520Hercules%2520Dalianis%26entry.1292438233%3D%2520%2520Many%2520sensitive%2520domains%2520--%2520such%2520as%2520the%2520clinical%2520domain%2520--%2520lack%2520widely%250Aavailable%2520datasets%2520due%2520to%2520privacy%2520risks.%2520The%2520increasing%2520generative%2520capabilities%250Aof%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520synthetic%2520datasets%2520a%2520viable%2520path%250Aforward.%2520In%2520this%2520study%252C%2520we%2520domain-adapt%2520LLMs%2520to%2520the%2520clinical%2520domain%2520and%250Agenerate%2520synthetic%2520clinical%2520texts%2520that%2520are%2520machine-annotated%2520with%2520tags%2520for%250Apersonally%2520identifiable%2520information%2520using%2520capable%2520encoder-based%2520NER%2520models.%2520The%250Asynthetic%2520corpora%2520are%2520then%2520used%2520to%2520train%2520synthetic%2520NER%2520models.%2520The%2520results%2520show%250Athat%2520training%2520NER%2520models%2520using%2520synthetic%2520corpora%2520incurs%2520only%2520a%2520small%2520drop%2520in%250Apredictive%2520performance.%2520The%2520limits%2520of%2520this%2520process%2520are%2520investigated%2520in%2520a%250Asystematic%2520ablation%2520study%2520--%2520using%2520both%2520Swedish%2520and%2520Spanish%2520data.%2520Our%2520analysis%250Ashows%2520that%2520smaller%2520datasets%2520can%2520be%2520sufficient%2520for%2520domain-adapting%2520LLMs%2520for%2520data%250Asynthesis.%2520Instead%252C%2520the%2520effectiveness%2520of%2520this%2520process%2520is%2520almost%2520entirely%250Acontingent%2520on%2520the%2520performance%2520of%2520the%2520machine-annotating%2520NER%2520models%2520trained%250Ausing%2520the%2520original%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Constrained%20Synthesis%20of%20Training%20Data%20for%20De-Identification&entry.906535625=Thomas%20Vakili%20and%20Aron%20Henriksson%20and%20Hercules%20Dalianis&entry.1292438233=%20%20Many%20sensitive%20domains%20--%20such%20as%20the%20clinical%20domain%20--%20lack%20widely%0Aavailable%20datasets%20due%20to%20privacy%20risks.%20The%20increasing%20generative%20capabilities%0Aof%20large%20language%20models%20%28LLMs%29%20have%20made%20synthetic%20datasets%20a%20viable%20path%0Aforward.%20In%20this%20study%2C%20we%20domain-adapt%20LLMs%20to%20the%20clinical%20domain%20and%0Agenerate%20synthetic%20clinical%20texts%20that%20are%20machine-annotated%20with%20tags%20for%0Apersonally%20identifiable%20information%20using%20capable%20encoder-based%20NER%20models.%20The%0Asynthetic%20corpora%20are%20then%20used%20to%20train%20synthetic%20NER%20models.%20The%20results%20show%0Athat%20training%20NER%20models%20using%20synthetic%20corpora%20incurs%20only%20a%20small%20drop%20in%0Apredictive%20performance.%20The%20limits%20of%20this%20process%20are%20investigated%20in%20a%0Asystematic%20ablation%20study%20--%20using%20both%20Swedish%20and%20Spanish%20data.%20Our%20analysis%0Ashows%20that%20smaller%20datasets%20can%20be%20sufficient%20for%20domain-adapting%20LLMs%20for%20data%0Asynthesis.%20Instead%2C%20the%20effectiveness%20of%20this%20process%20is%20almost%20entirely%0Acontingent%20on%20the%20performance%20of%20the%20machine-annotating%20NER%20models%20trained%0Ausing%20the%20original%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14677v1&entry.124074799=Read"},
{"title": "A Statistical Case Against Empirical Human-AI Alignment", "author": "Julian Rodemann and Esteban Garces Arias and Christoph Luther and Christoph Jansen and Thomas Augustin", "abstract": "  Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models.\n", "link": "http://arxiv.org/abs/2502.14581v1", "date": "2025-02-20", "relevancy": 1.3081, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4592}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.431}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Statistical%20Case%20Against%20Empirical%20Human-AI%20Alignment&body=Title%3A%20A%20Statistical%20Case%20Against%20Empirical%20Human-AI%20Alignment%0AAuthor%3A%20Julian%20Rodemann%20and%20Esteban%20Garces%20Arias%20and%20Christoph%20Luther%20and%20Christoph%20Jansen%20and%20Thomas%20Augustin%0AAbstract%3A%20%20%20Empirical%20human-AI%20alignment%20aims%20to%20make%20AI%20systems%20act%20in%20line%20with%0Aobserved%20human%20behavior.%20While%20noble%20in%20its%20goals%2C%20we%20argue%20that%20empirical%0Aalignment%20can%20inadvertently%20introduce%20statistical%20biases%20that%20warrant%20caution.%0AThis%20position%20paper%20thus%20advocates%20against%20naive%20empirical%20alignment%2C%20offering%0Aprescriptive%20alignment%20and%20a%20posteriori%20empirical%20alignment%20as%20alternatives.%20We%0Asubstantiate%20our%20principled%20argument%20by%20tangible%20examples%20like%20human-centric%0Adecoding%20of%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Statistical%2520Case%2520Against%2520Empirical%2520Human-AI%2520Alignment%26entry.906535625%3DJulian%2520Rodemann%2520and%2520Esteban%2520Garces%2520Arias%2520and%2520Christoph%2520Luther%2520and%2520Christoph%2520Jansen%2520and%2520Thomas%2520Augustin%26entry.1292438233%3D%2520%2520Empirical%2520human-AI%2520alignment%2520aims%2520to%2520make%2520AI%2520systems%2520act%2520in%2520line%2520with%250Aobserved%2520human%2520behavior.%2520While%2520noble%2520in%2520its%2520goals%252C%2520we%2520argue%2520that%2520empirical%250Aalignment%2520can%2520inadvertently%2520introduce%2520statistical%2520biases%2520that%2520warrant%2520caution.%250AThis%2520position%2520paper%2520thus%2520advocates%2520against%2520naive%2520empirical%2520alignment%252C%2520offering%250Aprescriptive%2520alignment%2520and%2520a%2520posteriori%2520empirical%2520alignment%2520as%2520alternatives.%2520We%250Asubstantiate%2520our%2520principled%2520argument%2520by%2520tangible%2520examples%2520like%2520human-centric%250Adecoding%2520of%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Statistical%20Case%20Against%20Empirical%20Human-AI%20Alignment&entry.906535625=Julian%20Rodemann%20and%20Esteban%20Garces%20Arias%20and%20Christoph%20Luther%20and%20Christoph%20Jansen%20and%20Thomas%20Augustin&entry.1292438233=%20%20Empirical%20human-AI%20alignment%20aims%20to%20make%20AI%20systems%20act%20in%20line%20with%0Aobserved%20human%20behavior.%20While%20noble%20in%20its%20goals%2C%20we%20argue%20that%20empirical%0Aalignment%20can%20inadvertently%20introduce%20statistical%20biases%20that%20warrant%20caution.%0AThis%20position%20paper%20thus%20advocates%20against%20naive%20empirical%20alignment%2C%20offering%0Aprescriptive%20alignment%20and%20a%20posteriori%20empirical%20alignment%20as%20alternatives.%20We%0Asubstantiate%20our%20principled%20argument%20by%20tangible%20examples%20like%20human-centric%0Adecoding%20of%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14581v1&entry.124074799=Read"},
{"title": "On the effects of similarity metrics in decentralized deep learning\n  under distributional shift", "author": "Edvin Listo Zec and Tom Hagander and Eric Ihre-Thomason and Sarunas Girdzijauskas", "abstract": "  Decentralized Learning (DL) enables privacy-preserving collaboration among\norganizations or users to enhance the performance of local deep learning\nmodels. However, model aggregation becomes challenging when client data is\nheterogeneous, and identifying compatible collaborators without direct data\nexchange remains a pressing issue. In this paper, we investigate the\neffectiveness of various similarity metrics in DL for identifying peers for\nmodel merging, conducting an empirical analysis across multiple datasets with\ndistribution shifts. Our research provides insights into the performance of\nthese metrics, examining their role in facilitating effective collaboration. By\nexploring the strengths and limitations of these metrics, we contribute to the\ndevelopment of robust DL methods.\n", "link": "http://arxiv.org/abs/2409.10720v2", "date": "2025-02-20", "relevancy": 1.4473, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4896}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4822}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20effects%20of%20similarity%20metrics%20in%20decentralized%20deep%20learning%0A%20%20under%20distributional%20shift&body=Title%3A%20On%20the%20effects%20of%20similarity%20metrics%20in%20decentralized%20deep%20learning%0A%20%20under%20distributional%20shift%0AAuthor%3A%20Edvin%20Listo%20Zec%20and%20Tom%20Hagander%20and%20Eric%20Ihre-Thomason%20and%20Sarunas%20Girdzijauskas%0AAbstract%3A%20%20%20Decentralized%20Learning%20%28DL%29%20enables%20privacy-preserving%20collaboration%20among%0Aorganizations%20or%20users%20to%20enhance%20the%20performance%20of%20local%20deep%20learning%0Amodels.%20However%2C%20model%20aggregation%20becomes%20challenging%20when%20client%20data%20is%0Aheterogeneous%2C%20and%20identifying%20compatible%20collaborators%20without%20direct%20data%0Aexchange%20remains%20a%20pressing%20issue.%20In%20this%20paper%2C%20we%20investigate%20the%0Aeffectiveness%20of%20various%20similarity%20metrics%20in%20DL%20for%20identifying%20peers%20for%0Amodel%20merging%2C%20conducting%20an%20empirical%20analysis%20across%20multiple%20datasets%20with%0Adistribution%20shifts.%20Our%20research%20provides%20insights%20into%20the%20performance%20of%0Athese%20metrics%2C%20examining%20their%20role%20in%20facilitating%20effective%20collaboration.%20By%0Aexploring%20the%20strengths%20and%20limitations%20of%20these%20metrics%2C%20we%20contribute%20to%20the%0Adevelopment%20of%20robust%20DL%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520effects%2520of%2520similarity%2520metrics%2520in%2520decentralized%2520deep%2520learning%250A%2520%2520under%2520distributional%2520shift%26entry.906535625%3DEdvin%2520Listo%2520Zec%2520and%2520Tom%2520Hagander%2520and%2520Eric%2520Ihre-Thomason%2520and%2520Sarunas%2520Girdzijauskas%26entry.1292438233%3D%2520%2520Decentralized%2520Learning%2520%2528DL%2529%2520enables%2520privacy-preserving%2520collaboration%2520among%250Aorganizations%2520or%2520users%2520to%2520enhance%2520the%2520performance%2520of%2520local%2520deep%2520learning%250Amodels.%2520However%252C%2520model%2520aggregation%2520becomes%2520challenging%2520when%2520client%2520data%2520is%250Aheterogeneous%252C%2520and%2520identifying%2520compatible%2520collaborators%2520without%2520direct%2520data%250Aexchange%2520remains%2520a%2520pressing%2520issue.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Aeffectiveness%2520of%2520various%2520similarity%2520metrics%2520in%2520DL%2520for%2520identifying%2520peers%2520for%250Amodel%2520merging%252C%2520conducting%2520an%2520empirical%2520analysis%2520across%2520multiple%2520datasets%2520with%250Adistribution%2520shifts.%2520Our%2520research%2520provides%2520insights%2520into%2520the%2520performance%2520of%250Athese%2520metrics%252C%2520examining%2520their%2520role%2520in%2520facilitating%2520effective%2520collaboration.%2520By%250Aexploring%2520the%2520strengths%2520and%2520limitations%2520of%2520these%2520metrics%252C%2520we%2520contribute%2520to%2520the%250Adevelopment%2520of%2520robust%2520DL%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20effects%20of%20similarity%20metrics%20in%20decentralized%20deep%20learning%0A%20%20under%20distributional%20shift&entry.906535625=Edvin%20Listo%20Zec%20and%20Tom%20Hagander%20and%20Eric%20Ihre-Thomason%20and%20Sarunas%20Girdzijauskas&entry.1292438233=%20%20Decentralized%20Learning%20%28DL%29%20enables%20privacy-preserving%20collaboration%20among%0Aorganizations%20or%20users%20to%20enhance%20the%20performance%20of%20local%20deep%20learning%0Amodels.%20However%2C%20model%20aggregation%20becomes%20challenging%20when%20client%20data%20is%0Aheterogeneous%2C%20and%20identifying%20compatible%20collaborators%20without%20direct%20data%0Aexchange%20remains%20a%20pressing%20issue.%20In%20this%20paper%2C%20we%20investigate%20the%0Aeffectiveness%20of%20various%20similarity%20metrics%20in%20DL%20for%20identifying%20peers%20for%0Amodel%20merging%2C%20conducting%20an%20empirical%20analysis%20across%20multiple%20datasets%20with%0Adistribution%20shifts.%20Our%20research%20provides%20insights%20into%20the%20performance%20of%0Athese%20metrics%2C%20examining%20their%20role%20in%20facilitating%20effective%20collaboration.%20By%0Aexploring%20the%20strengths%20and%20limitations%20of%20these%20metrics%2C%20we%20contribute%20to%20the%0Adevelopment%20of%20robust%20DL%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10720v2&entry.124074799=Read"},
{"title": "General Uncertainty Estimation with Delta Variances", "author": "Simon Schmitt and John Shawe-Taylor and Hado van Hasselt", "abstract": "  Decision makers may suffer from uncertainty induced by limited data. This may\nbe mitigated by accounting for epistemic uncertainty, which is however\nchallenging to estimate efficiently for large neural networks. To this extent\nwe investigate Delta Variances, a family of algorithms for epistemic\nuncertainty quantification, that is computationally efficient and convenient to\nimplement. It can be applied to neural networks and more general functions\ncomposed of neural networks. As an example we consider a weather simulator with\na neural-network-based step function inside -- here Delta Variances empirically\nobtain competitive results at the cost of a single gradient computation. The\napproach is convenient as it requires no changes to the neural network\narchitecture or training procedure. We discuss multiple ways to derive Delta\nVariances theoretically noting that special cases recover popular techniques\nand present a unified perspective on multiple related methods. Finally we\nobserve that this general perspective gives rise to a natural extension and\nempirically show its benefit.\n", "link": "http://arxiv.org/abs/2502.14698v1", "date": "2025-02-20", "relevancy": 1.5276, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5092}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Uncertainty%20Estimation%20with%20Delta%20Variances&body=Title%3A%20General%20Uncertainty%20Estimation%20with%20Delta%20Variances%0AAuthor%3A%20Simon%20Schmitt%20and%20John%20Shawe-Taylor%20and%20Hado%20van%20Hasselt%0AAbstract%3A%20%20%20Decision%20makers%20may%20suffer%20from%20uncertainty%20induced%20by%20limited%20data.%20This%20may%0Abe%20mitigated%20by%20accounting%20for%20epistemic%20uncertainty%2C%20which%20is%20however%0Achallenging%20to%20estimate%20efficiently%20for%20large%20neural%20networks.%20To%20this%20extent%0Awe%20investigate%20Delta%20Variances%2C%20a%20family%20of%20algorithms%20for%20epistemic%0Auncertainty%20quantification%2C%20that%20is%20computationally%20efficient%20and%20convenient%20to%0Aimplement.%20It%20can%20be%20applied%20to%20neural%20networks%20and%20more%20general%20functions%0Acomposed%20of%20neural%20networks.%20As%20an%20example%20we%20consider%20a%20weather%20simulator%20with%0Aa%20neural-network-based%20step%20function%20inside%20--%20here%20Delta%20Variances%20empirically%0Aobtain%20competitive%20results%20at%20the%20cost%20of%20a%20single%20gradient%20computation.%20The%0Aapproach%20is%20convenient%20as%20it%20requires%20no%20changes%20to%20the%20neural%20network%0Aarchitecture%20or%20training%20procedure.%20We%20discuss%20multiple%20ways%20to%20derive%20Delta%0AVariances%20theoretically%20noting%20that%20special%20cases%20recover%20popular%20techniques%0Aand%20present%20a%20unified%20perspective%20on%20multiple%20related%20methods.%20Finally%20we%0Aobserve%20that%20this%20general%20perspective%20gives%20rise%20to%20a%20natural%20extension%20and%0Aempirically%20show%20its%20benefit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Uncertainty%2520Estimation%2520with%2520Delta%2520Variances%26entry.906535625%3DSimon%2520Schmitt%2520and%2520John%2520Shawe-Taylor%2520and%2520Hado%2520van%2520Hasselt%26entry.1292438233%3D%2520%2520Decision%2520makers%2520may%2520suffer%2520from%2520uncertainty%2520induced%2520by%2520limited%2520data.%2520This%2520may%250Abe%2520mitigated%2520by%2520accounting%2520for%2520epistemic%2520uncertainty%252C%2520which%2520is%2520however%250Achallenging%2520to%2520estimate%2520efficiently%2520for%2520large%2520neural%2520networks.%2520To%2520this%2520extent%250Awe%2520investigate%2520Delta%2520Variances%252C%2520a%2520family%2520of%2520algorithms%2520for%2520epistemic%250Auncertainty%2520quantification%252C%2520that%2520is%2520computationally%2520efficient%2520and%2520convenient%2520to%250Aimplement.%2520It%2520can%2520be%2520applied%2520to%2520neural%2520networks%2520and%2520more%2520general%2520functions%250Acomposed%2520of%2520neural%2520networks.%2520As%2520an%2520example%2520we%2520consider%2520a%2520weather%2520simulator%2520with%250Aa%2520neural-network-based%2520step%2520function%2520inside%2520--%2520here%2520Delta%2520Variances%2520empirically%250Aobtain%2520competitive%2520results%2520at%2520the%2520cost%2520of%2520a%2520single%2520gradient%2520computation.%2520The%250Aapproach%2520is%2520convenient%2520as%2520it%2520requires%2520no%2520changes%2520to%2520the%2520neural%2520network%250Aarchitecture%2520or%2520training%2520procedure.%2520We%2520discuss%2520multiple%2520ways%2520to%2520derive%2520Delta%250AVariances%2520theoretically%2520noting%2520that%2520special%2520cases%2520recover%2520popular%2520techniques%250Aand%2520present%2520a%2520unified%2520perspective%2520on%2520multiple%2520related%2520methods.%2520Finally%2520we%250Aobserve%2520that%2520this%2520general%2520perspective%2520gives%2520rise%2520to%2520a%2520natural%2520extension%2520and%250Aempirically%2520show%2520its%2520benefit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Uncertainty%20Estimation%20with%20Delta%20Variances&entry.906535625=Simon%20Schmitt%20and%20John%20Shawe-Taylor%20and%20Hado%20van%20Hasselt&entry.1292438233=%20%20Decision%20makers%20may%20suffer%20from%20uncertainty%20induced%20by%20limited%20data.%20This%20may%0Abe%20mitigated%20by%20accounting%20for%20epistemic%20uncertainty%2C%20which%20is%20however%0Achallenging%20to%20estimate%20efficiently%20for%20large%20neural%20networks.%20To%20this%20extent%0Awe%20investigate%20Delta%20Variances%2C%20a%20family%20of%20algorithms%20for%20epistemic%0Auncertainty%20quantification%2C%20that%20is%20computationally%20efficient%20and%20convenient%20to%0Aimplement.%20It%20can%20be%20applied%20to%20neural%20networks%20and%20more%20general%20functions%0Acomposed%20of%20neural%20networks.%20As%20an%20example%20we%20consider%20a%20weather%20simulator%20with%0Aa%20neural-network-based%20step%20function%20inside%20--%20here%20Delta%20Variances%20empirically%0Aobtain%20competitive%20results%20at%20the%20cost%20of%20a%20single%20gradient%20computation.%20The%0Aapproach%20is%20convenient%20as%20it%20requires%20no%20changes%20to%20the%20neural%20network%0Aarchitecture%20or%20training%20procedure.%20We%20discuss%20multiple%20ways%20to%20derive%20Delta%0AVariances%20theoretically%20noting%20that%20special%20cases%20recover%20popular%20techniques%0Aand%20present%20a%20unified%20perspective%20on%20multiple%20related%20methods.%20Finally%20we%0Aobserve%20that%20this%20general%20perspective%20gives%20rise%20to%20a%20natural%20extension%20and%0Aempirically%20show%20its%20benefit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14698v1&entry.124074799=Read"},
{"title": "Multi-Agent Coordination across Diverse Applications: A Survey", "author": "Lijun Sun and Yijun Yang and Qiqi Duan and Yuhui Shi and Chao Lyu and Yu-Cheng Chang and Chin-Teng Lin and Yang Shen", "abstract": "  Multi-agent coordination studies the underlying mechanism enabling the\ntrending spread of diverse multi-agent systems (MAS) and has received\nincreasing attention, driven by the expansion of emerging applications and\nrapid AI advances. This survey outlines the current state of coordination\nresearch across applications through a unified understanding that answers four\nfundamental coordination questions: (1) what is coordination; (2) why\ncoordination; (3) who to coordinate with; and (4) how to coordinate. Our\npurpose is to explore existing ideas and expertise in coordination and their\nconnections across diverse applications, while identifying and highlighting\nemerging and promising research directions. First, general coordination\nproblems that are essential to varied applications are identified and analyzed.\nSecond, a number of MAS applications are surveyed, ranging from widely studied\ndomains, e.g., search and rescue, warehouse automation and logistics, and\ntransportation systems, to emerging fields including humanoid and\nanthropomorphic robots, satellite systems, and large language models (LLMs).\nFinally, open challenges about the scalability, heterogeneity, and learning\nmechanisms of MAS are analyzed and discussed. In particular, we identify the\nhybridization of hierarchical and decentralized coordination, human-MAS\ncoordination, and LLM-based MAS as promising future directions.\n", "link": "http://arxiv.org/abs/2502.14743v1", "date": "2025-02-20", "relevancy": 1.3925, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4827}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4807}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Coordination%20across%20Diverse%20Applications%3A%20A%20Survey&body=Title%3A%20Multi-Agent%20Coordination%20across%20Diverse%20Applications%3A%20A%20Survey%0AAuthor%3A%20Lijun%20Sun%20and%20Yijun%20Yang%20and%20Qiqi%20Duan%20and%20Yuhui%20Shi%20and%20Chao%20Lyu%20and%20Yu-Cheng%20Chang%20and%20Chin-Teng%20Lin%20and%20Yang%20Shen%0AAbstract%3A%20%20%20Multi-agent%20coordination%20studies%20the%20underlying%20mechanism%20enabling%20the%0Atrending%20spread%20of%20diverse%20multi-agent%20systems%20%28MAS%29%20and%20has%20received%0Aincreasing%20attention%2C%20driven%20by%20the%20expansion%20of%20emerging%20applications%20and%0Arapid%20AI%20advances.%20This%20survey%20outlines%20the%20current%20state%20of%20coordination%0Aresearch%20across%20applications%20through%20a%20unified%20understanding%20that%20answers%20four%0Afundamental%20coordination%20questions%3A%20%281%29%20what%20is%20coordination%3B%20%282%29%20why%0Acoordination%3B%20%283%29%20who%20to%20coordinate%20with%3B%20and%20%284%29%20how%20to%20coordinate.%20Our%0Apurpose%20is%20to%20explore%20existing%20ideas%20and%20expertise%20in%20coordination%20and%20their%0Aconnections%20across%20diverse%20applications%2C%20while%20identifying%20and%20highlighting%0Aemerging%20and%20promising%20research%20directions.%20First%2C%20general%20coordination%0Aproblems%20that%20are%20essential%20to%20varied%20applications%20are%20identified%20and%20analyzed.%0ASecond%2C%20a%20number%20of%20MAS%20applications%20are%20surveyed%2C%20ranging%20from%20widely%20studied%0Adomains%2C%20e.g.%2C%20search%20and%20rescue%2C%20warehouse%20automation%20and%20logistics%2C%20and%0Atransportation%20systems%2C%20to%20emerging%20fields%20including%20humanoid%20and%0Aanthropomorphic%20robots%2C%20satellite%20systems%2C%20and%20large%20language%20models%20%28LLMs%29.%0AFinally%2C%20open%20challenges%20about%20the%20scalability%2C%20heterogeneity%2C%20and%20learning%0Amechanisms%20of%20MAS%20are%20analyzed%20and%20discussed.%20In%20particular%2C%20we%20identify%20the%0Ahybridization%20of%20hierarchical%20and%20decentralized%20coordination%2C%20human-MAS%0Acoordination%2C%20and%20LLM-based%20MAS%20as%20promising%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Coordination%2520across%2520Diverse%2520Applications%253A%2520A%2520Survey%26entry.906535625%3DLijun%2520Sun%2520and%2520Yijun%2520Yang%2520and%2520Qiqi%2520Duan%2520and%2520Yuhui%2520Shi%2520and%2520Chao%2520Lyu%2520and%2520Yu-Cheng%2520Chang%2520and%2520Chin-Teng%2520Lin%2520and%2520Yang%2520Shen%26entry.1292438233%3D%2520%2520Multi-agent%2520coordination%2520studies%2520the%2520underlying%2520mechanism%2520enabling%2520the%250Atrending%2520spread%2520of%2520diverse%2520multi-agent%2520systems%2520%2528MAS%2529%2520and%2520has%2520received%250Aincreasing%2520attention%252C%2520driven%2520by%2520the%2520expansion%2520of%2520emerging%2520applications%2520and%250Arapid%2520AI%2520advances.%2520This%2520survey%2520outlines%2520the%2520current%2520state%2520of%2520coordination%250Aresearch%2520across%2520applications%2520through%2520a%2520unified%2520understanding%2520that%2520answers%2520four%250Afundamental%2520coordination%2520questions%253A%2520%25281%2529%2520what%2520is%2520coordination%253B%2520%25282%2529%2520why%250Acoordination%253B%2520%25283%2529%2520who%2520to%2520coordinate%2520with%253B%2520and%2520%25284%2529%2520how%2520to%2520coordinate.%2520Our%250Apurpose%2520is%2520to%2520explore%2520existing%2520ideas%2520and%2520expertise%2520in%2520coordination%2520and%2520their%250Aconnections%2520across%2520diverse%2520applications%252C%2520while%2520identifying%2520and%2520highlighting%250Aemerging%2520and%2520promising%2520research%2520directions.%2520First%252C%2520general%2520coordination%250Aproblems%2520that%2520are%2520essential%2520to%2520varied%2520applications%2520are%2520identified%2520and%2520analyzed.%250ASecond%252C%2520a%2520number%2520of%2520MAS%2520applications%2520are%2520surveyed%252C%2520ranging%2520from%2520widely%2520studied%250Adomains%252C%2520e.g.%252C%2520search%2520and%2520rescue%252C%2520warehouse%2520automation%2520and%2520logistics%252C%2520and%250Atransportation%2520systems%252C%2520to%2520emerging%2520fields%2520including%2520humanoid%2520and%250Aanthropomorphic%2520robots%252C%2520satellite%2520systems%252C%2520and%2520large%2520language%2520models%2520%2528LLMs%2529.%250AFinally%252C%2520open%2520challenges%2520about%2520the%2520scalability%252C%2520heterogeneity%252C%2520and%2520learning%250Amechanisms%2520of%2520MAS%2520are%2520analyzed%2520and%2520discussed.%2520In%2520particular%252C%2520we%2520identify%2520the%250Ahybridization%2520of%2520hierarchical%2520and%2520decentralized%2520coordination%252C%2520human-MAS%250Acoordination%252C%2520and%2520LLM-based%2520MAS%2520as%2520promising%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Coordination%20across%20Diverse%20Applications%3A%20A%20Survey&entry.906535625=Lijun%20Sun%20and%20Yijun%20Yang%20and%20Qiqi%20Duan%20and%20Yuhui%20Shi%20and%20Chao%20Lyu%20and%20Yu-Cheng%20Chang%20and%20Chin-Teng%20Lin%20and%20Yang%20Shen&entry.1292438233=%20%20Multi-agent%20coordination%20studies%20the%20underlying%20mechanism%20enabling%20the%0Atrending%20spread%20of%20diverse%20multi-agent%20systems%20%28MAS%29%20and%20has%20received%0Aincreasing%20attention%2C%20driven%20by%20the%20expansion%20of%20emerging%20applications%20and%0Arapid%20AI%20advances.%20This%20survey%20outlines%20the%20current%20state%20of%20coordination%0Aresearch%20across%20applications%20through%20a%20unified%20understanding%20that%20answers%20four%0Afundamental%20coordination%20questions%3A%20%281%29%20what%20is%20coordination%3B%20%282%29%20why%0Acoordination%3B%20%283%29%20who%20to%20coordinate%20with%3B%20and%20%284%29%20how%20to%20coordinate.%20Our%0Apurpose%20is%20to%20explore%20existing%20ideas%20and%20expertise%20in%20coordination%20and%20their%0Aconnections%20across%20diverse%20applications%2C%20while%20identifying%20and%20highlighting%0Aemerging%20and%20promising%20research%20directions.%20First%2C%20general%20coordination%0Aproblems%20that%20are%20essential%20to%20varied%20applications%20are%20identified%20and%20analyzed.%0ASecond%2C%20a%20number%20of%20MAS%20applications%20are%20surveyed%2C%20ranging%20from%20widely%20studied%0Adomains%2C%20e.g.%2C%20search%20and%20rescue%2C%20warehouse%20automation%20and%20logistics%2C%20and%0Atransportation%20systems%2C%20to%20emerging%20fields%20including%20humanoid%20and%0Aanthropomorphic%20robots%2C%20satellite%20systems%2C%20and%20large%20language%20models%20%28LLMs%29.%0AFinally%2C%20open%20challenges%20about%20the%20scalability%2C%20heterogeneity%2C%20and%20learning%0Amechanisms%20of%20MAS%20are%20analyzed%20and%20discussed.%20In%20particular%2C%20we%20identify%20the%0Ahybridization%20of%20hierarchical%20and%20decentralized%20coordination%2C%20human-MAS%0Acoordination%2C%20and%20LLM-based%20MAS%20as%20promising%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14743v1&entry.124074799=Read"},
{"title": "Fast Bayesian Inference for Neutrino Non-Standard Interactions at Dark\n  Matter Direct Detection Experiments", "author": "Dorian W. P. Amaral and Shixiao Liang and Juehang Qin and Christopher Tunnell", "abstract": "  Multi-dimensional parameter spaces are commonly encountered in physics\ntheories that go beyond the Standard Model. However, they often possess\ncomplicated posterior geometries that are expensive to traverse using\ntechniques traditional to astroparticle physics. Several recent innovations,\nwhich are only beginning to make their way into this field, have made\nnavigating such complex posteriors possible. These include GPU acceleration,\nautomatic differentiation, and neural-network-guided reparameterization. We\napply these advancements to dark matter direct detection experiments in the\ncontext of non-standard neutrino interactions and benchmark their performances\nagainst traditional nested sampling techniques when conducting Bayesian\ninference. Compared to nested sampling alone, we find that these techniques\nincrease performance for both nested sampling and Hamiltonian Monte Carlo,\naccelerating inference by factors of $\\sim 100$ and $\\sim 60$, respectively. As\nnested sampling also evaluates the Bayesian evidence, these advancements can be\nexploited to improve model comparison performance while retaining compatibility\nwith existing implementations that are widely used in the natural sciences.\nUsing these techniques, we perform the first scan in the neutrino non-standard\ninteractions parameter space for direct detection experiments whereby all\nparameters are allowed to vary simultaneously. We expect that these\nadvancements are broadly applicable to other areas of astroparticle physics\nfeaturing multi-dimensional parameter spaces.\n", "link": "http://arxiv.org/abs/2405.14932v2", "date": "2025-02-20", "relevancy": 1.004, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5396}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5034}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Bayesian%20Inference%20for%20Neutrino%20Non-Standard%20Interactions%20at%20Dark%0A%20%20Matter%20Direct%20Detection%20Experiments&body=Title%3A%20Fast%20Bayesian%20Inference%20for%20Neutrino%20Non-Standard%20Interactions%20at%20Dark%0A%20%20Matter%20Direct%20Detection%20Experiments%0AAuthor%3A%20Dorian%20W.%20P.%20Amaral%20and%20Shixiao%20Liang%20and%20Juehang%20Qin%20and%20Christopher%20Tunnell%0AAbstract%3A%20%20%20Multi-dimensional%20parameter%20spaces%20are%20commonly%20encountered%20in%20physics%0Atheories%20that%20go%20beyond%20the%20Standard%20Model.%20However%2C%20they%20often%20possess%0Acomplicated%20posterior%20geometries%20that%20are%20expensive%20to%20traverse%20using%0Atechniques%20traditional%20to%20astroparticle%20physics.%20Several%20recent%20innovations%2C%0Awhich%20are%20only%20beginning%20to%20make%20their%20way%20into%20this%20field%2C%20have%20made%0Anavigating%20such%20complex%20posteriors%20possible.%20These%20include%20GPU%20acceleration%2C%0Aautomatic%20differentiation%2C%20and%20neural-network-guided%20reparameterization.%20We%0Aapply%20these%20advancements%20to%20dark%20matter%20direct%20detection%20experiments%20in%20the%0Acontext%20of%20non-standard%20neutrino%20interactions%20and%20benchmark%20their%20performances%0Aagainst%20traditional%20nested%20sampling%20techniques%20when%20conducting%20Bayesian%0Ainference.%20Compared%20to%20nested%20sampling%20alone%2C%20we%20find%20that%20these%20techniques%0Aincrease%20performance%20for%20both%20nested%20sampling%20and%20Hamiltonian%20Monte%20Carlo%2C%0Aaccelerating%20inference%20by%20factors%20of%20%24%5Csim%20100%24%20and%20%24%5Csim%2060%24%2C%20respectively.%20As%0Anested%20sampling%20also%20evaluates%20the%20Bayesian%20evidence%2C%20these%20advancements%20can%20be%0Aexploited%20to%20improve%20model%20comparison%20performance%20while%20retaining%20compatibility%0Awith%20existing%20implementations%20that%20are%20widely%20used%20in%20the%20natural%20sciences.%0AUsing%20these%20techniques%2C%20we%20perform%20the%20first%20scan%20in%20the%20neutrino%20non-standard%0Ainteractions%20parameter%20space%20for%20direct%20detection%20experiments%20whereby%20all%0Aparameters%20are%20allowed%20to%20vary%20simultaneously.%20We%20expect%20that%20these%0Aadvancements%20are%20broadly%20applicable%20to%20other%20areas%20of%20astroparticle%20physics%0Afeaturing%20multi-dimensional%20parameter%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14932v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Bayesian%2520Inference%2520for%2520Neutrino%2520Non-Standard%2520Interactions%2520at%2520Dark%250A%2520%2520Matter%2520Direct%2520Detection%2520Experiments%26entry.906535625%3DDorian%2520W.%2520P.%2520Amaral%2520and%2520Shixiao%2520Liang%2520and%2520Juehang%2520Qin%2520and%2520Christopher%2520Tunnell%26entry.1292438233%3D%2520%2520Multi-dimensional%2520parameter%2520spaces%2520are%2520commonly%2520encountered%2520in%2520physics%250Atheories%2520that%2520go%2520beyond%2520the%2520Standard%2520Model.%2520However%252C%2520they%2520often%2520possess%250Acomplicated%2520posterior%2520geometries%2520that%2520are%2520expensive%2520to%2520traverse%2520using%250Atechniques%2520traditional%2520to%2520astroparticle%2520physics.%2520Several%2520recent%2520innovations%252C%250Awhich%2520are%2520only%2520beginning%2520to%2520make%2520their%2520way%2520into%2520this%2520field%252C%2520have%2520made%250Anavigating%2520such%2520complex%2520posteriors%2520possible.%2520These%2520include%2520GPU%2520acceleration%252C%250Aautomatic%2520differentiation%252C%2520and%2520neural-network-guided%2520reparameterization.%2520We%250Aapply%2520these%2520advancements%2520to%2520dark%2520matter%2520direct%2520detection%2520experiments%2520in%2520the%250Acontext%2520of%2520non-standard%2520neutrino%2520interactions%2520and%2520benchmark%2520their%2520performances%250Aagainst%2520traditional%2520nested%2520sampling%2520techniques%2520when%2520conducting%2520Bayesian%250Ainference.%2520Compared%2520to%2520nested%2520sampling%2520alone%252C%2520we%2520find%2520that%2520these%2520techniques%250Aincrease%2520performance%2520for%2520both%2520nested%2520sampling%2520and%2520Hamiltonian%2520Monte%2520Carlo%252C%250Aaccelerating%2520inference%2520by%2520factors%2520of%2520%2524%255Csim%2520100%2524%2520and%2520%2524%255Csim%252060%2524%252C%2520respectively.%2520As%250Anested%2520sampling%2520also%2520evaluates%2520the%2520Bayesian%2520evidence%252C%2520these%2520advancements%2520can%2520be%250Aexploited%2520to%2520improve%2520model%2520comparison%2520performance%2520while%2520retaining%2520compatibility%250Awith%2520existing%2520implementations%2520that%2520are%2520widely%2520used%2520in%2520the%2520natural%2520sciences.%250AUsing%2520these%2520techniques%252C%2520we%2520perform%2520the%2520first%2520scan%2520in%2520the%2520neutrino%2520non-standard%250Ainteractions%2520parameter%2520space%2520for%2520direct%2520detection%2520experiments%2520whereby%2520all%250Aparameters%2520are%2520allowed%2520to%2520vary%2520simultaneously.%2520We%2520expect%2520that%2520these%250Aadvancements%2520are%2520broadly%2520applicable%2520to%2520other%2520areas%2520of%2520astroparticle%2520physics%250Afeaturing%2520multi-dimensional%2520parameter%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14932v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Bayesian%20Inference%20for%20Neutrino%20Non-Standard%20Interactions%20at%20Dark%0A%20%20Matter%20Direct%20Detection%20Experiments&entry.906535625=Dorian%20W.%20P.%20Amaral%20and%20Shixiao%20Liang%20and%20Juehang%20Qin%20and%20Christopher%20Tunnell&entry.1292438233=%20%20Multi-dimensional%20parameter%20spaces%20are%20commonly%20encountered%20in%20physics%0Atheories%20that%20go%20beyond%20the%20Standard%20Model.%20However%2C%20they%20often%20possess%0Acomplicated%20posterior%20geometries%20that%20are%20expensive%20to%20traverse%20using%0Atechniques%20traditional%20to%20astroparticle%20physics.%20Several%20recent%20innovations%2C%0Awhich%20are%20only%20beginning%20to%20make%20their%20way%20into%20this%20field%2C%20have%20made%0Anavigating%20such%20complex%20posteriors%20possible.%20These%20include%20GPU%20acceleration%2C%0Aautomatic%20differentiation%2C%20and%20neural-network-guided%20reparameterization.%20We%0Aapply%20these%20advancements%20to%20dark%20matter%20direct%20detection%20experiments%20in%20the%0Acontext%20of%20non-standard%20neutrino%20interactions%20and%20benchmark%20their%20performances%0Aagainst%20traditional%20nested%20sampling%20techniques%20when%20conducting%20Bayesian%0Ainference.%20Compared%20to%20nested%20sampling%20alone%2C%20we%20find%20that%20these%20techniques%0Aincrease%20performance%20for%20both%20nested%20sampling%20and%20Hamiltonian%20Monte%20Carlo%2C%0Aaccelerating%20inference%20by%20factors%20of%20%24%5Csim%20100%24%20and%20%24%5Csim%2060%24%2C%20respectively.%20As%0Anested%20sampling%20also%20evaluates%20the%20Bayesian%20evidence%2C%20these%20advancements%20can%20be%0Aexploited%20to%20improve%20model%20comparison%20performance%20while%20retaining%20compatibility%0Awith%20existing%20implementations%20that%20are%20widely%20used%20in%20the%20natural%20sciences.%0AUsing%20these%20techniques%2C%20we%20perform%20the%20first%20scan%20in%20the%20neutrino%20non-standard%0Ainteractions%20parameter%20space%20for%20direct%20detection%20experiments%20whereby%20all%0Aparameters%20are%20allowed%20to%20vary%20simultaneously.%20We%20expect%20that%20these%0Aadvancements%20are%20broadly%20applicable%20to%20other%20areas%20of%20astroparticle%20physics%0Afeaturing%20multi-dimensional%20parameter%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14932v2&entry.124074799=Read"},
{"title": "Non-Contextual BERT or FastText? A Comparative Analysis", "author": "Abhay Shanbhag and Suramya Jadhav and Amogh Thakurdesai and Ridhima Sinare and Raviraj Joshi", "abstract": "  Natural Language Processing (NLP) for low-resource languages, which lack\nlarge annotated datasets, faces significant challenges due to limited\nhigh-quality data and linguistic resources. The selection of embeddings plays a\ncritical role in achieving strong performance in NLP tasks. While contextual\nBERT embeddings require a full forward pass, non-contextual BERT embeddings\nrely only on table lookup. Existing research has primarily focused on\ncontextual BERT embeddings, leaving non-contextual embeddings largely\nunexplored. In this study, we analyze the effectiveness of non-contextual\nembeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT\nand MahaFT) for tasks such as news classification, sentiment analysis, and hate\nspeech detection in one such low-resource language Marathi. We compare these\nembeddings with their contextual and compressed variants. Our findings indicate\nthat non-contextual BERT embeddings extracted from the model's first embedding\nlayer outperform FastText embeddings, presenting a promising alternative for\nlow-resource NLP.\n", "link": "http://arxiv.org/abs/2411.17661v3", "date": "2025-02-20", "relevancy": 1.8584, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4696}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Contextual%20BERT%20or%20FastText%3F%20A%20Comparative%20Analysis&body=Title%3A%20Non-Contextual%20BERT%20or%20FastText%3F%20A%20Comparative%20Analysis%0AAuthor%3A%20Abhay%20Shanbhag%20and%20Suramya%20Jadhav%20and%20Amogh%20Thakurdesai%20and%20Ridhima%20Sinare%20and%20Raviraj%20Joshi%0AAbstract%3A%20%20%20Natural%20Language%20Processing%20%28NLP%29%20for%20low-resource%20languages%2C%20which%20lack%0Alarge%20annotated%20datasets%2C%20faces%20significant%20challenges%20due%20to%20limited%0Ahigh-quality%20data%20and%20linguistic%20resources.%20The%20selection%20of%20embeddings%20plays%20a%0Acritical%20role%20in%20achieving%20strong%20performance%20in%20NLP%20tasks.%20While%20contextual%0ABERT%20embeddings%20require%20a%20full%20forward%20pass%2C%20non-contextual%20BERT%20embeddings%0Arely%20only%20on%20table%20lookup.%20Existing%20research%20has%20primarily%20focused%20on%0Acontextual%20BERT%20embeddings%2C%20leaving%20non-contextual%20embeddings%20largely%0Aunexplored.%20In%20this%20study%2C%20we%20analyze%20the%20effectiveness%20of%20non-contextual%0Aembeddings%20from%20BERT%20models%20%28MuRIL%20and%20MahaBERT%29%20and%20FastText%20models%20%28IndicFT%0Aand%20MahaFT%29%20for%20tasks%20such%20as%20news%20classification%2C%20sentiment%20analysis%2C%20and%20hate%0Aspeech%20detection%20in%20one%20such%20low-resource%20language%20Marathi.%20We%20compare%20these%0Aembeddings%20with%20their%20contextual%20and%20compressed%20variants.%20Our%20findings%20indicate%0Athat%20non-contextual%20BERT%20embeddings%20extracted%20from%20the%20model%27s%20first%20embedding%0Alayer%20outperform%20FastText%20embeddings%2C%20presenting%20a%20promising%20alternative%20for%0Alow-resource%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17661v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Contextual%2520BERT%2520or%2520FastText%253F%2520A%2520Comparative%2520Analysis%26entry.906535625%3DAbhay%2520Shanbhag%2520and%2520Suramya%2520Jadhav%2520and%2520Amogh%2520Thakurdesai%2520and%2520Ridhima%2520Sinare%2520and%2520Raviraj%2520Joshi%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520for%2520low-resource%2520languages%252C%2520which%2520lack%250Alarge%2520annotated%2520datasets%252C%2520faces%2520significant%2520challenges%2520due%2520to%2520limited%250Ahigh-quality%2520data%2520and%2520linguistic%2520resources.%2520The%2520selection%2520of%2520embeddings%2520plays%2520a%250Acritical%2520role%2520in%2520achieving%2520strong%2520performance%2520in%2520NLP%2520tasks.%2520While%2520contextual%250ABERT%2520embeddings%2520require%2520a%2520full%2520forward%2520pass%252C%2520non-contextual%2520BERT%2520embeddings%250Arely%2520only%2520on%2520table%2520lookup.%2520Existing%2520research%2520has%2520primarily%2520focused%2520on%250Acontextual%2520BERT%2520embeddings%252C%2520leaving%2520non-contextual%2520embeddings%2520largely%250Aunexplored.%2520In%2520this%2520study%252C%2520we%2520analyze%2520the%2520effectiveness%2520of%2520non-contextual%250Aembeddings%2520from%2520BERT%2520models%2520%2528MuRIL%2520and%2520MahaBERT%2529%2520and%2520FastText%2520models%2520%2528IndicFT%250Aand%2520MahaFT%2529%2520for%2520tasks%2520such%2520as%2520news%2520classification%252C%2520sentiment%2520analysis%252C%2520and%2520hate%250Aspeech%2520detection%2520in%2520one%2520such%2520low-resource%2520language%2520Marathi.%2520We%2520compare%2520these%250Aembeddings%2520with%2520their%2520contextual%2520and%2520compressed%2520variants.%2520Our%2520findings%2520indicate%250Athat%2520non-contextual%2520BERT%2520embeddings%2520extracted%2520from%2520the%2520model%2527s%2520first%2520embedding%250Alayer%2520outperform%2520FastText%2520embeddings%252C%2520presenting%2520a%2520promising%2520alternative%2520for%250Alow-resource%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17661v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Contextual%20BERT%20or%20FastText%3F%20A%20Comparative%20Analysis&entry.906535625=Abhay%20Shanbhag%20and%20Suramya%20Jadhav%20and%20Amogh%20Thakurdesai%20and%20Ridhima%20Sinare%20and%20Raviraj%20Joshi&entry.1292438233=%20%20Natural%20Language%20Processing%20%28NLP%29%20for%20low-resource%20languages%2C%20which%20lack%0Alarge%20annotated%20datasets%2C%20faces%20significant%20challenges%20due%20to%20limited%0Ahigh-quality%20data%20and%20linguistic%20resources.%20The%20selection%20of%20embeddings%20plays%20a%0Acritical%20role%20in%20achieving%20strong%20performance%20in%20NLP%20tasks.%20While%20contextual%0ABERT%20embeddings%20require%20a%20full%20forward%20pass%2C%20non-contextual%20BERT%20embeddings%0Arely%20only%20on%20table%20lookup.%20Existing%20research%20has%20primarily%20focused%20on%0Acontextual%20BERT%20embeddings%2C%20leaving%20non-contextual%20embeddings%20largely%0Aunexplored.%20In%20this%20study%2C%20we%20analyze%20the%20effectiveness%20of%20non-contextual%0Aembeddings%20from%20BERT%20models%20%28MuRIL%20and%20MahaBERT%29%20and%20FastText%20models%20%28IndicFT%0Aand%20MahaFT%29%20for%20tasks%20such%20as%20news%20classification%2C%20sentiment%20analysis%2C%20and%20hate%0Aspeech%20detection%20in%20one%20such%20low-resource%20language%20Marathi.%20We%20compare%20these%0Aembeddings%20with%20their%20contextual%20and%20compressed%20variants.%20Our%20findings%20indicate%0Athat%20non-contextual%20BERT%20embeddings%20extracted%20from%20the%20model%27s%20first%20embedding%0Alayer%20outperform%20FastText%20embeddings%2C%20presenting%20a%20promising%20alternative%20for%0Alow-resource%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17661v3&entry.124074799=Read"},
{"title": "Towards Understanding Why Label Smoothing Degrades Selective\n  Classification and How to Fix It", "author": "Guoxuan Xia and Olivier Laurent and Gianni Franchi and Christos-Savvas Bouganis", "abstract": "  Label smoothing (LS) is a popular regularisation method for training neural\nnetworks as it is effective in improving test accuracy and is simple to\nimplement. ``Hard'' one-hot labels are ``smoothed'' by uniformly distributing\nprobability mass to other classes, reducing overfitting. Prior work has\nsuggested that in some cases LS can degrade selective classification (SC) --\nwhere the aim is to reject misclassifications using a model's uncertainty. In\nthis work, we first demonstrate empirically across an extended range of\nlarge-scale tasks and architectures that LS consistently degrades SC. We then\naddress a gap in existing knowledge, providing an explanation for this\nbehaviour by analysing logit-level gradients: LS degrades the uncertainty rank\nordering of correct vs incorrect predictions by suppressing the max logit more\nwhen a prediction is likely to be correct, and less when it is likely to be\nwrong. This elucidates previously reported experimental results where strong\nclassifiers underperform in SC. We then demonstrate the empirical effectiveness\nof post-hoc logit normalisation for recovering lost SC performance caused by\nLS. Furthermore, linking back to our gradient analysis, we again provide an\nexplanation for why such normalisation is effective.\n", "link": "http://arxiv.org/abs/2403.14715v3", "date": "2025-02-20", "relevancy": 1.8838, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4764}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4685}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Why%20Label%20Smoothing%20Degrades%20Selective%0A%20%20Classification%20and%20How%20to%20Fix%20It&body=Title%3A%20Towards%20Understanding%20Why%20Label%20Smoothing%20Degrades%20Selective%0A%20%20Classification%20and%20How%20to%20Fix%20It%0AAuthor%3A%20Guoxuan%20Xia%20and%20Olivier%20Laurent%20and%20Gianni%20Franchi%20and%20Christos-Savvas%20Bouganis%0AAbstract%3A%20%20%20Label%20smoothing%20%28LS%29%20is%20a%20popular%20regularisation%20method%20for%20training%20neural%0Anetworks%20as%20it%20is%20effective%20in%20improving%20test%20accuracy%20and%20is%20simple%20to%0Aimplement.%20%60%60Hard%27%27%20one-hot%20labels%20are%20%60%60smoothed%27%27%20by%20uniformly%20distributing%0Aprobability%20mass%20to%20other%20classes%2C%20reducing%20overfitting.%20Prior%20work%20has%0Asuggested%20that%20in%20some%20cases%20LS%20can%20degrade%20selective%20classification%20%28SC%29%20--%0Awhere%20the%20aim%20is%20to%20reject%20misclassifications%20using%20a%20model%27s%20uncertainty.%20In%0Athis%20work%2C%20we%20first%20demonstrate%20empirically%20across%20an%20extended%20range%20of%0Alarge-scale%20tasks%20and%20architectures%20that%20LS%20consistently%20degrades%20SC.%20We%20then%0Aaddress%20a%20gap%20in%20existing%20knowledge%2C%20providing%20an%20explanation%20for%20this%0Abehaviour%20by%20analysing%20logit-level%20gradients%3A%20LS%20degrades%20the%20uncertainty%20rank%0Aordering%20of%20correct%20vs%20incorrect%20predictions%20by%20suppressing%20the%20max%20logit%20more%0Awhen%20a%20prediction%20is%20likely%20to%20be%20correct%2C%20and%20less%20when%20it%20is%20likely%20to%20be%0Awrong.%20This%20elucidates%20previously%20reported%20experimental%20results%20where%20strong%0Aclassifiers%20underperform%20in%20SC.%20We%20then%20demonstrate%20the%20empirical%20effectiveness%0Aof%20post-hoc%20logit%20normalisation%20for%20recovering%20lost%20SC%20performance%20caused%20by%0ALS.%20Furthermore%2C%20linking%20back%20to%20our%20gradient%20analysis%2C%20we%20again%20provide%20an%0Aexplanation%20for%20why%20such%20normalisation%20is%20effective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14715v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Why%2520Label%2520Smoothing%2520Degrades%2520Selective%250A%2520%2520Classification%2520and%2520How%2520to%2520Fix%2520It%26entry.906535625%3DGuoxuan%2520Xia%2520and%2520Olivier%2520Laurent%2520and%2520Gianni%2520Franchi%2520and%2520Christos-Savvas%2520Bouganis%26entry.1292438233%3D%2520%2520Label%2520smoothing%2520%2528LS%2529%2520is%2520a%2520popular%2520regularisation%2520method%2520for%2520training%2520neural%250Anetworks%2520as%2520it%2520is%2520effective%2520in%2520improving%2520test%2520accuracy%2520and%2520is%2520simple%2520to%250Aimplement.%2520%2560%2560Hard%2527%2527%2520one-hot%2520labels%2520are%2520%2560%2560smoothed%2527%2527%2520by%2520uniformly%2520distributing%250Aprobability%2520mass%2520to%2520other%2520classes%252C%2520reducing%2520overfitting.%2520Prior%2520work%2520has%250Asuggested%2520that%2520in%2520some%2520cases%2520LS%2520can%2520degrade%2520selective%2520classification%2520%2528SC%2529%2520--%250Awhere%2520the%2520aim%2520is%2520to%2520reject%2520misclassifications%2520using%2520a%2520model%2527s%2520uncertainty.%2520In%250Athis%2520work%252C%2520we%2520first%2520demonstrate%2520empirically%2520across%2520an%2520extended%2520range%2520of%250Alarge-scale%2520tasks%2520and%2520architectures%2520that%2520LS%2520consistently%2520degrades%2520SC.%2520We%2520then%250Aaddress%2520a%2520gap%2520in%2520existing%2520knowledge%252C%2520providing%2520an%2520explanation%2520for%2520this%250Abehaviour%2520by%2520analysing%2520logit-level%2520gradients%253A%2520LS%2520degrades%2520the%2520uncertainty%2520rank%250Aordering%2520of%2520correct%2520vs%2520incorrect%2520predictions%2520by%2520suppressing%2520the%2520max%2520logit%2520more%250Awhen%2520a%2520prediction%2520is%2520likely%2520to%2520be%2520correct%252C%2520and%2520less%2520when%2520it%2520is%2520likely%2520to%2520be%250Awrong.%2520This%2520elucidates%2520previously%2520reported%2520experimental%2520results%2520where%2520strong%250Aclassifiers%2520underperform%2520in%2520SC.%2520We%2520then%2520demonstrate%2520the%2520empirical%2520effectiveness%250Aof%2520post-hoc%2520logit%2520normalisation%2520for%2520recovering%2520lost%2520SC%2520performance%2520caused%2520by%250ALS.%2520Furthermore%252C%2520linking%2520back%2520to%2520our%2520gradient%2520analysis%252C%2520we%2520again%2520provide%2520an%250Aexplanation%2520for%2520why%2520such%2520normalisation%2520is%2520effective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14715v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Why%20Label%20Smoothing%20Degrades%20Selective%0A%20%20Classification%20and%20How%20to%20Fix%20It&entry.906535625=Guoxuan%20Xia%20and%20Olivier%20Laurent%20and%20Gianni%20Franchi%20and%20Christos-Savvas%20Bouganis&entry.1292438233=%20%20Label%20smoothing%20%28LS%29%20is%20a%20popular%20regularisation%20method%20for%20training%20neural%0Anetworks%20as%20it%20is%20effective%20in%20improving%20test%20accuracy%20and%20is%20simple%20to%0Aimplement.%20%60%60Hard%27%27%20one-hot%20labels%20are%20%60%60smoothed%27%27%20by%20uniformly%20distributing%0Aprobability%20mass%20to%20other%20classes%2C%20reducing%20overfitting.%20Prior%20work%20has%0Asuggested%20that%20in%20some%20cases%20LS%20can%20degrade%20selective%20classification%20%28SC%29%20--%0Awhere%20the%20aim%20is%20to%20reject%20misclassifications%20using%20a%20model%27s%20uncertainty.%20In%0Athis%20work%2C%20we%20first%20demonstrate%20empirically%20across%20an%20extended%20range%20of%0Alarge-scale%20tasks%20and%20architectures%20that%20LS%20consistently%20degrades%20SC.%20We%20then%0Aaddress%20a%20gap%20in%20existing%20knowledge%2C%20providing%20an%20explanation%20for%20this%0Abehaviour%20by%20analysing%20logit-level%20gradients%3A%20LS%20degrades%20the%20uncertainty%20rank%0Aordering%20of%20correct%20vs%20incorrect%20predictions%20by%20suppressing%20the%20max%20logit%20more%0Awhen%20a%20prediction%20is%20likely%20to%20be%20correct%2C%20and%20less%20when%20it%20is%20likely%20to%20be%0Awrong.%20This%20elucidates%20previously%20reported%20experimental%20results%20where%20strong%0Aclassifiers%20underperform%20in%20SC.%20We%20then%20demonstrate%20the%20empirical%20effectiveness%0Aof%20post-hoc%20logit%20normalisation%20for%20recovering%20lost%20SC%20performance%20caused%20by%0ALS.%20Furthermore%2C%20linking%20back%20to%20our%20gradient%20analysis%2C%20we%20again%20provide%20an%0Aexplanation%20for%20why%20such%20normalisation%20is%20effective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14715v3&entry.124074799=Read"},
{"title": "Variance Reduction Methods Do Not Need to Compute Full Gradients:\n  Improved Efficiency through Shuffling", "author": "Daniil Medyakov and Gleb Molodtsov and Savelii Chezhegov and Alexey Rebrikov and Aleksandr Beznosikov", "abstract": "  In today's world, machine learning is hard to imagine without large training\ndatasets and models. This has led to the use of stochastic methods for\ntraining, such as stochastic gradient descent (SGD). SGD provides weak\ntheoretical guarantees of convergence, but there are modifications, such as\nStochastic Variance Reduced Gradient (SVRG) and StochAstic Recursive grAdient\nalgoritHm (SARAH), that can reduce the variance. These methods require the\ncomputation of the full gradient occasionally, which can be time consuming. In\nthis paper, we explore variants of variance reduction algorithms that eliminate\nthe need for full gradient computations. To make our approach memory-efficient\nand avoid full gradient computations, we use two key techniques: the shuffling\nheuristic and idea of SAG/SAGA methods. As a result, we improve existing\nestimates for variance reduction algorithms without the full gradient\ncomputations. Additionally, for the non-convex objective function, our estimate\nmatches that of classic shuffling methods, while for the strongly convex one,\nit is an improvement. We conduct comprehensive theoretical analysis and provide\nextensive experimental results to validate the efficiency and practicality of\nour methods for large-scale machine learning problems.\n", "link": "http://arxiv.org/abs/2502.14648v1", "date": "2025-02-20", "relevancy": 1.8415, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4739}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variance%20Reduction%20Methods%20Do%20Not%20Need%20to%20Compute%20Full%20Gradients%3A%0A%20%20Improved%20Efficiency%20through%20Shuffling&body=Title%3A%20Variance%20Reduction%20Methods%20Do%20Not%20Need%20to%20Compute%20Full%20Gradients%3A%0A%20%20Improved%20Efficiency%20through%20Shuffling%0AAuthor%3A%20Daniil%20Medyakov%20and%20Gleb%20Molodtsov%20and%20Savelii%20Chezhegov%20and%20Alexey%20Rebrikov%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20In%20today%27s%20world%2C%20machine%20learning%20is%20hard%20to%20imagine%20without%20large%20training%0Adatasets%20and%20models.%20This%20has%20led%20to%20the%20use%20of%20stochastic%20methods%20for%0Atraining%2C%20such%20as%20stochastic%20gradient%20descent%20%28SGD%29.%20SGD%20provides%20weak%0Atheoretical%20guarantees%20of%20convergence%2C%20but%20there%20are%20modifications%2C%20such%20as%0AStochastic%20Variance%20Reduced%20Gradient%20%28SVRG%29%20and%20StochAstic%20Recursive%20grAdient%0AalgoritHm%20%28SARAH%29%2C%20that%20can%20reduce%20the%20variance.%20These%20methods%20require%20the%0Acomputation%20of%20the%20full%20gradient%20occasionally%2C%20which%20can%20be%20time%20consuming.%20In%0Athis%20paper%2C%20we%20explore%20variants%20of%20variance%20reduction%20algorithms%20that%20eliminate%0Athe%20need%20for%20full%20gradient%20computations.%20To%20make%20our%20approach%20memory-efficient%0Aand%20avoid%20full%20gradient%20computations%2C%20we%20use%20two%20key%20techniques%3A%20the%20shuffling%0Aheuristic%20and%20idea%20of%20SAG/SAGA%20methods.%20As%20a%20result%2C%20we%20improve%20existing%0Aestimates%20for%20variance%20reduction%20algorithms%20without%20the%20full%20gradient%0Acomputations.%20Additionally%2C%20for%20the%20non-convex%20objective%20function%2C%20our%20estimate%0Amatches%20that%20of%20classic%20shuffling%20methods%2C%20while%20for%20the%20strongly%20convex%20one%2C%0Ait%20is%20an%20improvement.%20We%20conduct%20comprehensive%20theoretical%20analysis%20and%20provide%0Aextensive%20experimental%20results%20to%20validate%20the%20efficiency%20and%20practicality%20of%0Aour%20methods%20for%20large-scale%20machine%20learning%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariance%2520Reduction%2520Methods%2520Do%2520Not%2520Need%2520to%2520Compute%2520Full%2520Gradients%253A%250A%2520%2520Improved%2520Efficiency%2520through%2520Shuffling%26entry.906535625%3DDaniil%2520Medyakov%2520and%2520Gleb%2520Molodtsov%2520and%2520Savelii%2520Chezhegov%2520and%2520Alexey%2520Rebrikov%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520world%252C%2520machine%2520learning%2520is%2520hard%2520to%2520imagine%2520without%2520large%2520training%250Adatasets%2520and%2520models.%2520This%2520has%2520led%2520to%2520the%2520use%2520of%2520stochastic%2520methods%2520for%250Atraining%252C%2520such%2520as%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529.%2520SGD%2520provides%2520weak%250Atheoretical%2520guarantees%2520of%2520convergence%252C%2520but%2520there%2520are%2520modifications%252C%2520such%2520as%250AStochastic%2520Variance%2520Reduced%2520Gradient%2520%2528SVRG%2529%2520and%2520StochAstic%2520Recursive%2520grAdient%250AalgoritHm%2520%2528SARAH%2529%252C%2520that%2520can%2520reduce%2520the%2520variance.%2520These%2520methods%2520require%2520the%250Acomputation%2520of%2520the%2520full%2520gradient%2520occasionally%252C%2520which%2520can%2520be%2520time%2520consuming.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520variants%2520of%2520variance%2520reduction%2520algorithms%2520that%2520eliminate%250Athe%2520need%2520for%2520full%2520gradient%2520computations.%2520To%2520make%2520our%2520approach%2520memory-efficient%250Aand%2520avoid%2520full%2520gradient%2520computations%252C%2520we%2520use%2520two%2520key%2520techniques%253A%2520the%2520shuffling%250Aheuristic%2520and%2520idea%2520of%2520SAG/SAGA%2520methods.%2520As%2520a%2520result%252C%2520we%2520improve%2520existing%250Aestimates%2520for%2520variance%2520reduction%2520algorithms%2520without%2520the%2520full%2520gradient%250Acomputations.%2520Additionally%252C%2520for%2520the%2520non-convex%2520objective%2520function%252C%2520our%2520estimate%250Amatches%2520that%2520of%2520classic%2520shuffling%2520methods%252C%2520while%2520for%2520the%2520strongly%2520convex%2520one%252C%250Ait%2520is%2520an%2520improvement.%2520We%2520conduct%2520comprehensive%2520theoretical%2520analysis%2520and%2520provide%250Aextensive%2520experimental%2520results%2520to%2520validate%2520the%2520efficiency%2520and%2520practicality%2520of%250Aour%2520methods%2520for%2520large-scale%2520machine%2520learning%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variance%20Reduction%20Methods%20Do%20Not%20Need%20to%20Compute%20Full%20Gradients%3A%0A%20%20Improved%20Efficiency%20through%20Shuffling&entry.906535625=Daniil%20Medyakov%20and%20Gleb%20Molodtsov%20and%20Savelii%20Chezhegov%20and%20Alexey%20Rebrikov%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20In%20today%27s%20world%2C%20machine%20learning%20is%20hard%20to%20imagine%20without%20large%20training%0Adatasets%20and%20models.%20This%20has%20led%20to%20the%20use%20of%20stochastic%20methods%20for%0Atraining%2C%20such%20as%20stochastic%20gradient%20descent%20%28SGD%29.%20SGD%20provides%20weak%0Atheoretical%20guarantees%20of%20convergence%2C%20but%20there%20are%20modifications%2C%20such%20as%0AStochastic%20Variance%20Reduced%20Gradient%20%28SVRG%29%20and%20StochAstic%20Recursive%20grAdient%0AalgoritHm%20%28SARAH%29%2C%20that%20can%20reduce%20the%20variance.%20These%20methods%20require%20the%0Acomputation%20of%20the%20full%20gradient%20occasionally%2C%20which%20can%20be%20time%20consuming.%20In%0Athis%20paper%2C%20we%20explore%20variants%20of%20variance%20reduction%20algorithms%20that%20eliminate%0Athe%20need%20for%20full%20gradient%20computations.%20To%20make%20our%20approach%20memory-efficient%0Aand%20avoid%20full%20gradient%20computations%2C%20we%20use%20two%20key%20techniques%3A%20the%20shuffling%0Aheuristic%20and%20idea%20of%20SAG/SAGA%20methods.%20As%20a%20result%2C%20we%20improve%20existing%0Aestimates%20for%20variance%20reduction%20algorithms%20without%20the%20full%20gradient%0Acomputations.%20Additionally%2C%20for%20the%20non-convex%20objective%20function%2C%20our%20estimate%0Amatches%20that%20of%20classic%20shuffling%20methods%2C%20while%20for%20the%20strongly%20convex%20one%2C%0Ait%20is%20an%20improvement.%20We%20conduct%20comprehensive%20theoretical%20analysis%20and%20provide%0Aextensive%20experimental%20results%20to%20validate%20the%20efficiency%20and%20practicality%20of%0Aour%20methods%20for%20large-scale%20machine%20learning%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14648v1&entry.124074799=Read"},
{"title": "The Computational Limits of State-Space Models and Mamba via the Lens of\n  Circuit Complexity", "author": "Yifang Chen and Xiaoyu Li and Yingyu Liang and Zhenmei Shi and Zhao Song", "abstract": "  In this paper, we analyze the computational limitations of Mamba and\nState-space Models (SSMs) by using the circuit complexity framework. Despite\nMamba's stateful design and recent attention as a strong candidate to\noutperform Transformers, we have demonstrated that both Mamba and SSMs with\n$\\mathrm{poly}(n)$-precision and constant-depth layers reside within the\n$\\mathsf{DLOGTIME}$-uniform $\\mathsf{TC}^0$ complexity class. This result\nindicates Mamba has the same computational capabilities as Transformer\ntheoretically, and it cannot solve problems like arithmetic formula problems,\nboolean formula value problems, and permutation composition problems if\n$\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$. Therefore, it challenges the assumption\nMamba is more computationally expressive than Transformers. Our contributions\ninclude rigorous proofs showing that Selective SSM and Mamba architectures can\nbe simulated by $\\mathsf{DLOGTIME}$-uniform $\\mathsf{TC}^0$ circuits, and they\ncannot solve problems outside $\\mathsf{TC}^0$.\n", "link": "http://arxiv.org/abs/2412.06148v2", "date": "2025-02-20", "relevancy": 0.8456, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4313}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4254}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Computational%20Limits%20of%20State-Space%20Models%20and%20Mamba%20via%20the%20Lens%20of%0A%20%20Circuit%20Complexity&body=Title%3A%20The%20Computational%20Limits%20of%20State-Space%20Models%20and%20Mamba%20via%20the%20Lens%20of%0A%20%20Circuit%20Complexity%0AAuthor%3A%20Yifang%20Chen%20and%20Xiaoyu%20Li%20and%20Yingyu%20Liang%20and%20Zhenmei%20Shi%20and%20Zhao%20Song%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20analyze%20the%20computational%20limitations%20of%20Mamba%20and%0AState-space%20Models%20%28SSMs%29%20by%20using%20the%20circuit%20complexity%20framework.%20Despite%0AMamba%27s%20stateful%20design%20and%20recent%20attention%20as%20a%20strong%20candidate%20to%0Aoutperform%20Transformers%2C%20we%20have%20demonstrated%20that%20both%20Mamba%20and%20SSMs%20with%0A%24%5Cmathrm%7Bpoly%7D%28n%29%24-precision%20and%20constant-depth%20layers%20reside%20within%20the%0A%24%5Cmathsf%7BDLOGTIME%7D%24-uniform%20%24%5Cmathsf%7BTC%7D%5E0%24%20complexity%20class.%20This%20result%0Aindicates%20Mamba%20has%20the%20same%20computational%20capabilities%20as%20Transformer%0Atheoretically%2C%20and%20it%20cannot%20solve%20problems%20like%20arithmetic%20formula%20problems%2C%0Aboolean%20formula%20value%20problems%2C%20and%20permutation%20composition%20problems%20if%0A%24%5Cmathsf%7BTC%7D%5E0%20%5Cneq%20%5Cmathsf%7BNC%7D%5E1%24.%20Therefore%2C%20it%20challenges%20the%20assumption%0AMamba%20is%20more%20computationally%20expressive%20than%20Transformers.%20Our%20contributions%0Ainclude%20rigorous%20proofs%20showing%20that%20Selective%20SSM%20and%20Mamba%20architectures%20can%0Abe%20simulated%20by%20%24%5Cmathsf%7BDLOGTIME%7D%24-uniform%20%24%5Cmathsf%7BTC%7D%5E0%24%20circuits%2C%20and%20they%0Acannot%20solve%20problems%20outside%20%24%5Cmathsf%7BTC%7D%5E0%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06148v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Computational%2520Limits%2520of%2520State-Space%2520Models%2520and%2520Mamba%2520via%2520the%2520Lens%2520of%250A%2520%2520Circuit%2520Complexity%26entry.906535625%3DYifang%2520Chen%2520and%2520Xiaoyu%2520Li%2520and%2520Yingyu%2520Liang%2520and%2520Zhenmei%2520Shi%2520and%2520Zhao%2520Song%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520computational%2520limitations%2520of%2520Mamba%2520and%250AState-space%2520Models%2520%2528SSMs%2529%2520by%2520using%2520the%2520circuit%2520complexity%2520framework.%2520Despite%250AMamba%2527s%2520stateful%2520design%2520and%2520recent%2520attention%2520as%2520a%2520strong%2520candidate%2520to%250Aoutperform%2520Transformers%252C%2520we%2520have%2520demonstrated%2520that%2520both%2520Mamba%2520and%2520SSMs%2520with%250A%2524%255Cmathrm%257Bpoly%257D%2528n%2529%2524-precision%2520and%2520constant-depth%2520layers%2520reside%2520within%2520the%250A%2524%255Cmathsf%257BDLOGTIME%257D%2524-uniform%2520%2524%255Cmathsf%257BTC%257D%255E0%2524%2520complexity%2520class.%2520This%2520result%250Aindicates%2520Mamba%2520has%2520the%2520same%2520computational%2520capabilities%2520as%2520Transformer%250Atheoretically%252C%2520and%2520it%2520cannot%2520solve%2520problems%2520like%2520arithmetic%2520formula%2520problems%252C%250Aboolean%2520formula%2520value%2520problems%252C%2520and%2520permutation%2520composition%2520problems%2520if%250A%2524%255Cmathsf%257BTC%257D%255E0%2520%255Cneq%2520%255Cmathsf%257BNC%257D%255E1%2524.%2520Therefore%252C%2520it%2520challenges%2520the%2520assumption%250AMamba%2520is%2520more%2520computationally%2520expressive%2520than%2520Transformers.%2520Our%2520contributions%250Ainclude%2520rigorous%2520proofs%2520showing%2520that%2520Selective%2520SSM%2520and%2520Mamba%2520architectures%2520can%250Abe%2520simulated%2520by%2520%2524%255Cmathsf%257BDLOGTIME%257D%2524-uniform%2520%2524%255Cmathsf%257BTC%257D%255E0%2524%2520circuits%252C%2520and%2520they%250Acannot%2520solve%2520problems%2520outside%2520%2524%255Cmathsf%257BTC%257D%255E0%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06148v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Computational%20Limits%20of%20State-Space%20Models%20and%20Mamba%20via%20the%20Lens%20of%0A%20%20Circuit%20Complexity&entry.906535625=Yifang%20Chen%20and%20Xiaoyu%20Li%20and%20Yingyu%20Liang%20and%20Zhenmei%20Shi%20and%20Zhao%20Song&entry.1292438233=%20%20In%20this%20paper%2C%20we%20analyze%20the%20computational%20limitations%20of%20Mamba%20and%0AState-space%20Models%20%28SSMs%29%20by%20using%20the%20circuit%20complexity%20framework.%20Despite%0AMamba%27s%20stateful%20design%20and%20recent%20attention%20as%20a%20strong%20candidate%20to%0Aoutperform%20Transformers%2C%20we%20have%20demonstrated%20that%20both%20Mamba%20and%20SSMs%20with%0A%24%5Cmathrm%7Bpoly%7D%28n%29%24-precision%20and%20constant-depth%20layers%20reside%20within%20the%0A%24%5Cmathsf%7BDLOGTIME%7D%24-uniform%20%24%5Cmathsf%7BTC%7D%5E0%24%20complexity%20class.%20This%20result%0Aindicates%20Mamba%20has%20the%20same%20computational%20capabilities%20as%20Transformer%0Atheoretically%2C%20and%20it%20cannot%20solve%20problems%20like%20arithmetic%20formula%20problems%2C%0Aboolean%20formula%20value%20problems%2C%20and%20permutation%20composition%20problems%20if%0A%24%5Cmathsf%7BTC%7D%5E0%20%5Cneq%20%5Cmathsf%7BNC%7D%5E1%24.%20Therefore%2C%20it%20challenges%20the%20assumption%0AMamba%20is%20more%20computationally%20expressive%20than%20Transformers.%20Our%20contributions%0Ainclude%20rigorous%20proofs%20showing%20that%20Selective%20SSM%20and%20Mamba%20architectures%20can%0Abe%20simulated%20by%20%24%5Cmathsf%7BDLOGTIME%7D%24-uniform%20%24%5Cmathsf%7BTC%7D%5E0%24%20circuits%2C%20and%20they%0Acannot%20solve%20problems%20outside%20%24%5Cmathsf%7BTC%7D%5E0%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06148v2&entry.124074799=Read"},
{"title": "Data Attribution for Text-to-Image Models by Unlearning Synthesized\n  Images", "author": "Sheng-Yu Wang and Aaron Hertzmann and Alexei A. Efros and Jun-Yan Zhu and Richard Zhang", "abstract": "  The goal of data attribution for text-to-image models is to identify the\ntraining images that most influence the generation of a new image. Influence is\ndefined such that, for a given output, if a model is retrained from scratch\nwithout the most influential images, the model would fail to reproduce the same\noutput. Unfortunately, directly searching for these influential images is\ncomputationally infeasible, since it would require repeatedly retraining models\nfrom scratch. In our work, we propose an efficient data attribution method by\nsimulating unlearning the synthesized image. We achieve this by increasing the\ntraining loss on the output image, without catastrophic forgetting of other,\nunrelated concepts. We then identify training images with significant loss\ndeviations after the unlearning process and label these as influential. We\nevaluate our method with a computationally intensive but \"gold-standard\"\nretraining from scratch and demonstrate our method's advantages over previous\nmethods.\n", "link": "http://arxiv.org/abs/2406.09408v3", "date": "2025-02-20", "relevancy": 1.6724, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5826}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5586}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Attribution%20for%20Text-to-Image%20Models%20by%20Unlearning%20Synthesized%0A%20%20Images&body=Title%3A%20Data%20Attribution%20for%20Text-to-Image%20Models%20by%20Unlearning%20Synthesized%0A%20%20Images%0AAuthor%3A%20Sheng-Yu%20Wang%20and%20Aaron%20Hertzmann%20and%20Alexei%20A.%20Efros%20and%20Jun-Yan%20Zhu%20and%20Richard%20Zhang%0AAbstract%3A%20%20%20The%20goal%20of%20data%20attribution%20for%20text-to-image%20models%20is%20to%20identify%20the%0Atraining%20images%20that%20most%20influence%20the%20generation%20of%20a%20new%20image.%20Influence%20is%0Adefined%20such%20that%2C%20for%20a%20given%20output%2C%20if%20a%20model%20is%20retrained%20from%20scratch%0Awithout%20the%20most%20influential%20images%2C%20the%20model%20would%20fail%20to%20reproduce%20the%20same%0Aoutput.%20Unfortunately%2C%20directly%20searching%20for%20these%20influential%20images%20is%0Acomputationally%20infeasible%2C%20since%20it%20would%20require%20repeatedly%20retraining%20models%0Afrom%20scratch.%20In%20our%20work%2C%20we%20propose%20an%20efficient%20data%20attribution%20method%20by%0Asimulating%20unlearning%20the%20synthesized%20image.%20We%20achieve%20this%20by%20increasing%20the%0Atraining%20loss%20on%20the%20output%20image%2C%20without%20catastrophic%20forgetting%20of%20other%2C%0Aunrelated%20concepts.%20We%20then%20identify%20training%20images%20with%20significant%20loss%0Adeviations%20after%20the%20unlearning%20process%20and%20label%20these%20as%20influential.%20We%0Aevaluate%20our%20method%20with%20a%20computationally%20intensive%20but%20%22gold-standard%22%0Aretraining%20from%20scratch%20and%20demonstrate%20our%20method%27s%20advantages%20over%20previous%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09408v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Attribution%2520for%2520Text-to-Image%2520Models%2520by%2520Unlearning%2520Synthesized%250A%2520%2520Images%26entry.906535625%3DSheng-Yu%2520Wang%2520and%2520Aaron%2520Hertzmann%2520and%2520Alexei%2520A.%2520Efros%2520and%2520Jun-Yan%2520Zhu%2520and%2520Richard%2520Zhang%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520data%2520attribution%2520for%2520text-to-image%2520models%2520is%2520to%2520identify%2520the%250Atraining%2520images%2520that%2520most%2520influence%2520the%2520generation%2520of%2520a%2520new%2520image.%2520Influence%2520is%250Adefined%2520such%2520that%252C%2520for%2520a%2520given%2520output%252C%2520if%2520a%2520model%2520is%2520retrained%2520from%2520scratch%250Awithout%2520the%2520most%2520influential%2520images%252C%2520the%2520model%2520would%2520fail%2520to%2520reproduce%2520the%2520same%250Aoutput.%2520Unfortunately%252C%2520directly%2520searching%2520for%2520these%2520influential%2520images%2520is%250Acomputationally%2520infeasible%252C%2520since%2520it%2520would%2520require%2520repeatedly%2520retraining%2520models%250Afrom%2520scratch.%2520In%2520our%2520work%252C%2520we%2520propose%2520an%2520efficient%2520data%2520attribution%2520method%2520by%250Asimulating%2520unlearning%2520the%2520synthesized%2520image.%2520We%2520achieve%2520this%2520by%2520increasing%2520the%250Atraining%2520loss%2520on%2520the%2520output%2520image%252C%2520without%2520catastrophic%2520forgetting%2520of%2520other%252C%250Aunrelated%2520concepts.%2520We%2520then%2520identify%2520training%2520images%2520with%2520significant%2520loss%250Adeviations%2520after%2520the%2520unlearning%2520process%2520and%2520label%2520these%2520as%2520influential.%2520We%250Aevaluate%2520our%2520method%2520with%2520a%2520computationally%2520intensive%2520but%2520%2522gold-standard%2522%250Aretraining%2520from%2520scratch%2520and%2520demonstrate%2520our%2520method%2527s%2520advantages%2520over%2520previous%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09408v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Attribution%20for%20Text-to-Image%20Models%20by%20Unlearning%20Synthesized%0A%20%20Images&entry.906535625=Sheng-Yu%20Wang%20and%20Aaron%20Hertzmann%20and%20Alexei%20A.%20Efros%20and%20Jun-Yan%20Zhu%20and%20Richard%20Zhang&entry.1292438233=%20%20The%20goal%20of%20data%20attribution%20for%20text-to-image%20models%20is%20to%20identify%20the%0Atraining%20images%20that%20most%20influence%20the%20generation%20of%20a%20new%20image.%20Influence%20is%0Adefined%20such%20that%2C%20for%20a%20given%20output%2C%20if%20a%20model%20is%20retrained%20from%20scratch%0Awithout%20the%20most%20influential%20images%2C%20the%20model%20would%20fail%20to%20reproduce%20the%20same%0Aoutput.%20Unfortunately%2C%20directly%20searching%20for%20these%20influential%20images%20is%0Acomputationally%20infeasible%2C%20since%20it%20would%20require%20repeatedly%20retraining%20models%0Afrom%20scratch.%20In%20our%20work%2C%20we%20propose%20an%20efficient%20data%20attribution%20method%20by%0Asimulating%20unlearning%20the%20synthesized%20image.%20We%20achieve%20this%20by%20increasing%20the%0Atraining%20loss%20on%20the%20output%20image%2C%20without%20catastrophic%20forgetting%20of%20other%2C%0Aunrelated%20concepts.%20We%20then%20identify%20training%20images%20with%20significant%20loss%0Adeviations%20after%20the%20unlearning%20process%20and%20label%20these%20as%20influential.%20We%0Aevaluate%20our%20method%20with%20a%20computationally%20intensive%20but%20%22gold-standard%22%0Aretraining%20from%20scratch%20and%20demonstrate%20our%20method%27s%20advantages%20over%20previous%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09408v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


