<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260127.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction", "author": "Ziyu Zhang and Tianle Liu and Diantao Tu and Shuhan Shen", "abstract": "We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition.", "link": "http://arxiv.org/abs/2601.19489v1", "date": "2026-01-27", "relevancy": 3.6003, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7709}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6959}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Converging%203D%20Gaussian%20Splatting%20for%201-Minute%20Reconstruction&body=Title%3A%20Fast%20Converging%203D%20Gaussian%20Splatting%20for%201-Minute%20Reconstruction%0AAuthor%3A%20Ziyu%20Zhang%20and%20Tianle%20Liu%20and%20Diantao%20Tu%20and%20Shuhan%20Shen%0AAbstract%3A%20We%20present%20a%20fast%203DGS%20reconstruction%20pipeline%20designed%20to%20converge%20within%20one%20minute%2C%20developed%20for%20the%20SIGGRAPH%20Asia%203DGS%20Fast%20Reconstruction%20Challenge.%20The%20challenge%20consists%20of%20an%20initial%20round%20using%20SLAM-generated%20camera%20poses%20%28with%20noisy%20trajectories%29%20and%20a%20final%20round%20using%20COLMAP%20poses%20%28highly%20accurate%29.%20To%20robustly%20handle%20these%20heterogeneous%20settings%2C%20we%20develop%20a%20two-stage%20solution.%20In%20the%20first%20round%2C%20we%20use%20reverse%20per-Gaussian%20parallel%20optimization%20and%20compact%20forward%20splatting%20based%20on%20Taming-GS%20and%20Speedy-splat%2C%20load-balanced%20tiling%2C%20an%20anchor-based%20Neural-Gaussian%20representation%20enabling%20rapid%20convergence%20with%20fewer%20learnable%20parameters%2C%20initialization%20from%20monocular%20depth%20and%20partially%20from%20feed-forward%203DGS%20models%2C%20and%20a%20global%20pose%20refinement%20module%20for%20noisy%20SLAM%20trajectories.%20In%20the%20final%20round%2C%20the%20accurate%20COLMAP%20poses%20change%20the%20optimization%20landscape%3B%20we%20disable%20pose%20refinement%2C%20revert%20from%20Neural-Gaussians%20back%20to%20standard%203DGS%20to%20eliminate%20MLP%20inference%20overhead%2C%20introduce%20multi-view%20consistency-guided%20Gaussian%20splitting%20inspired%20by%20Fast-GS%2C%20and%20introduce%20a%20depth%20estimator%20to%20supervise%20the%20rendered%20depth.%20Together%2C%20these%20techniques%20enable%20high-fidelity%20reconstruction%20under%20a%20strict%20one-minute%20budget.%20Our%20method%20achieved%20the%20top%20performance%20with%20a%20PSNR%20of%2028.43%20and%20ranked%20first%20in%20the%20competition.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Converging%25203D%2520Gaussian%2520Splatting%2520for%25201-Minute%2520Reconstruction%26entry.906535625%3DZiyu%2520Zhang%2520and%2520Tianle%2520Liu%2520and%2520Diantao%2520Tu%2520and%2520Shuhan%2520Shen%26entry.1292438233%3DWe%2520present%2520a%2520fast%25203DGS%2520reconstruction%2520pipeline%2520designed%2520to%2520converge%2520within%2520one%2520minute%252C%2520developed%2520for%2520the%2520SIGGRAPH%2520Asia%25203DGS%2520Fast%2520Reconstruction%2520Challenge.%2520The%2520challenge%2520consists%2520of%2520an%2520initial%2520round%2520using%2520SLAM-generated%2520camera%2520poses%2520%2528with%2520noisy%2520trajectories%2529%2520and%2520a%2520final%2520round%2520using%2520COLMAP%2520poses%2520%2528highly%2520accurate%2529.%2520To%2520robustly%2520handle%2520these%2520heterogeneous%2520settings%252C%2520we%2520develop%2520a%2520two-stage%2520solution.%2520In%2520the%2520first%2520round%252C%2520we%2520use%2520reverse%2520per-Gaussian%2520parallel%2520optimization%2520and%2520compact%2520forward%2520splatting%2520based%2520on%2520Taming-GS%2520and%2520Speedy-splat%252C%2520load-balanced%2520tiling%252C%2520an%2520anchor-based%2520Neural-Gaussian%2520representation%2520enabling%2520rapid%2520convergence%2520with%2520fewer%2520learnable%2520parameters%252C%2520initialization%2520from%2520monocular%2520depth%2520and%2520partially%2520from%2520feed-forward%25203DGS%2520models%252C%2520and%2520a%2520global%2520pose%2520refinement%2520module%2520for%2520noisy%2520SLAM%2520trajectories.%2520In%2520the%2520final%2520round%252C%2520the%2520accurate%2520COLMAP%2520poses%2520change%2520the%2520optimization%2520landscape%253B%2520we%2520disable%2520pose%2520refinement%252C%2520revert%2520from%2520Neural-Gaussians%2520back%2520to%2520standard%25203DGS%2520to%2520eliminate%2520MLP%2520inference%2520overhead%252C%2520introduce%2520multi-view%2520consistency-guided%2520Gaussian%2520splitting%2520inspired%2520by%2520Fast-GS%252C%2520and%2520introduce%2520a%2520depth%2520estimator%2520to%2520supervise%2520the%2520rendered%2520depth.%2520Together%252C%2520these%2520techniques%2520enable%2520high-fidelity%2520reconstruction%2520under%2520a%2520strict%2520one-minute%2520budget.%2520Our%2520method%2520achieved%2520the%2520top%2520performance%2520with%2520a%2520PSNR%2520of%252028.43%2520and%2520ranked%2520first%2520in%2520the%2520competition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Converging%203D%20Gaussian%20Splatting%20for%201-Minute%20Reconstruction&entry.906535625=Ziyu%20Zhang%20and%20Tianle%20Liu%20and%20Diantao%20Tu%20and%20Shuhan%20Shen&entry.1292438233=We%20present%20a%20fast%203DGS%20reconstruction%20pipeline%20designed%20to%20converge%20within%20one%20minute%2C%20developed%20for%20the%20SIGGRAPH%20Asia%203DGS%20Fast%20Reconstruction%20Challenge.%20The%20challenge%20consists%20of%20an%20initial%20round%20using%20SLAM-generated%20camera%20poses%20%28with%20noisy%20trajectories%29%20and%20a%20final%20round%20using%20COLMAP%20poses%20%28highly%20accurate%29.%20To%20robustly%20handle%20these%20heterogeneous%20settings%2C%20we%20develop%20a%20two-stage%20solution.%20In%20the%20first%20round%2C%20we%20use%20reverse%20per-Gaussian%20parallel%20optimization%20and%20compact%20forward%20splatting%20based%20on%20Taming-GS%20and%20Speedy-splat%2C%20load-balanced%20tiling%2C%20an%20anchor-based%20Neural-Gaussian%20representation%20enabling%20rapid%20convergence%20with%20fewer%20learnable%20parameters%2C%20initialization%20from%20monocular%20depth%20and%20partially%20from%20feed-forward%203DGS%20models%2C%20and%20a%20global%20pose%20refinement%20module%20for%20noisy%20SLAM%20trajectories.%20In%20the%20final%20round%2C%20the%20accurate%20COLMAP%20poses%20change%20the%20optimization%20landscape%3B%20we%20disable%20pose%20refinement%2C%20revert%20from%20Neural-Gaussians%20back%20to%20standard%203DGS%20to%20eliminate%20MLP%20inference%20overhead%2C%20introduce%20multi-view%20consistency-guided%20Gaussian%20splitting%20inspired%20by%20Fast-GS%2C%20and%20introduce%20a%20depth%20estimator%20to%20supervise%20the%20rendered%20depth.%20Together%2C%20these%20techniques%20enable%20high-fidelity%20reconstruction%20under%20a%20strict%20one-minute%20budget.%20Our%20method%20achieved%20the%20top%20performance%20with%20a%20PSNR%20of%2028.43%20and%20ranked%20first%20in%20the%20competition.&entry.1838667208=http%3A//arxiv.org/abs/2601.19489v1&entry.124074799=Read"},
{"title": "Geometry-Grounded Gaussian Splatting", "author": "Baowen Zhang and Chenxing Jiang and Heng Li and Shaojie Shen and Ping Tan", "abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.", "link": "http://arxiv.org/abs/2601.17835v2", "date": "2026-01-27", "relevancy": 3.3462, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.686}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6807}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Grounded%20Gaussian%20Splatting&body=Title%3A%20Geometry-Grounded%20Gaussian%20Splatting%0AAuthor%3A%20Baowen%20Zhang%20and%20Chenxing%20Jiang%20and%20Heng%20Li%20and%20Shaojie%20Shen%20and%20Ping%20Tan%0AAbstract%3A%20Gaussian%20Splatting%20%28GS%29%20has%20demonstrated%20impressive%20quality%20and%20efficiency%20in%20novel%20view%20synthesis.%20However%2C%20shape%20extraction%20from%20Gaussian%20primitives%20remains%20an%20open%20problem.%20Due%20to%20inadequate%20geometry%20parameterization%20and%20approximation%2C%20existing%20shape%20reconstruction%20methods%20suffer%20from%20poor%20multi-view%20consistency%20and%20are%20sensitive%20to%20floaters.%20In%20this%20paper%2C%20we%20present%20a%20rigorous%20theoretical%20derivation%20that%20establishes%20Gaussian%20primitives%20as%20a%20specific%20type%20of%20stochastic%20solids.%20This%20theoretical%20framework%20provides%20a%20principled%20foundation%20for%20Geometry-Grounded%20Gaussian%20Splatting%20by%20enabling%20the%20direct%20treatment%20of%20Gaussian%20primitives%20as%20explicit%20geometric%20representations.%20Using%20the%20volumetric%20nature%20of%20stochastic%20solids%2C%20our%20method%20efficiently%20renders%20high-quality%20depth%20maps%20for%20fine-grained%20geometry%20extraction.%20Experiments%20show%20that%20our%20method%20achieves%20the%20best%20shape%20reconstruction%20results%20among%20all%20Gaussian%20Splatting-based%20methods%20on%20public%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.17835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Grounded%2520Gaussian%2520Splatting%26entry.906535625%3DBaowen%2520Zhang%2520and%2520Chenxing%2520Jiang%2520and%2520Heng%2520Li%2520and%2520Shaojie%2520Shen%2520and%2520Ping%2520Tan%26entry.1292438233%3DGaussian%2520Splatting%2520%2528GS%2529%2520has%2520demonstrated%2520impressive%2520quality%2520and%2520efficiency%2520in%2520novel%2520view%2520synthesis.%2520However%252C%2520shape%2520extraction%2520from%2520Gaussian%2520primitives%2520remains%2520an%2520open%2520problem.%2520Due%2520to%2520inadequate%2520geometry%2520parameterization%2520and%2520approximation%252C%2520existing%2520shape%2520reconstruction%2520methods%2520suffer%2520from%2520poor%2520multi-view%2520consistency%2520and%2520are%2520sensitive%2520to%2520floaters.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520rigorous%2520theoretical%2520derivation%2520that%2520establishes%2520Gaussian%2520primitives%2520as%2520a%2520specific%2520type%2520of%2520stochastic%2520solids.%2520This%2520theoretical%2520framework%2520provides%2520a%2520principled%2520foundation%2520for%2520Geometry-Grounded%2520Gaussian%2520Splatting%2520by%2520enabling%2520the%2520direct%2520treatment%2520of%2520Gaussian%2520primitives%2520as%2520explicit%2520geometric%2520representations.%2520Using%2520the%2520volumetric%2520nature%2520of%2520stochastic%2520solids%252C%2520our%2520method%2520efficiently%2520renders%2520high-quality%2520depth%2520maps%2520for%2520fine-grained%2520geometry%2520extraction.%2520Experiments%2520show%2520that%2520our%2520method%2520achieves%2520the%2520best%2520shape%2520reconstruction%2520results%2520among%2520all%2520Gaussian%2520Splatting-based%2520methods%2520on%2520public%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Grounded%20Gaussian%20Splatting&entry.906535625=Baowen%20Zhang%20and%20Chenxing%20Jiang%20and%20Heng%20Li%20and%20Shaojie%20Shen%20and%20Ping%20Tan&entry.1292438233=Gaussian%20Splatting%20%28GS%29%20has%20demonstrated%20impressive%20quality%20and%20efficiency%20in%20novel%20view%20synthesis.%20However%2C%20shape%20extraction%20from%20Gaussian%20primitives%20remains%20an%20open%20problem.%20Due%20to%20inadequate%20geometry%20parameterization%20and%20approximation%2C%20existing%20shape%20reconstruction%20methods%20suffer%20from%20poor%20multi-view%20consistency%20and%20are%20sensitive%20to%20floaters.%20In%20this%20paper%2C%20we%20present%20a%20rigorous%20theoretical%20derivation%20that%20establishes%20Gaussian%20primitives%20as%20a%20specific%20type%20of%20stochastic%20solids.%20This%20theoretical%20framework%20provides%20a%20principled%20foundation%20for%20Geometry-Grounded%20Gaussian%20Splatting%20by%20enabling%20the%20direct%20treatment%20of%20Gaussian%20primitives%20as%20explicit%20geometric%20representations.%20Using%20the%20volumetric%20nature%20of%20stochastic%20solids%2C%20our%20method%20efficiently%20renders%20high-quality%20depth%20maps%20for%20fine-grained%20geometry%20extraction.%20Experiments%20show%20that%20our%20method%20achieves%20the%20best%20shape%20reconstruction%20results%20among%20all%20Gaussian%20Splatting-based%20methods%20on%20public%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2601.17835v2&entry.124074799=Read"},
{"title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps", "author": "Yuhan Chen and Ying Fang and Guofa Li and Wenxuan Yu and Yicui Shi and Jingrui Zhang and Kefei Qian and Wenbo Chu and Keqiang Li", "abstract": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.", "link": "http://arxiv.org/abs/2601.15766v2", "date": "2026-01-27", "relevancy": 3.2456, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6809}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.636}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LL-GaussianMap%3A%20Zero-shot%20Low-Light%20Image%20Enhancement%20via%202D%20Gaussian%20Splatting%20Guided%20Gain%20Maps&body=Title%3A%20LL-GaussianMap%3A%20Zero-shot%20Low-Light%20Image%20Enhancement%20via%202D%20Gaussian%20Splatting%20Guided%20Gain%20Maps%0AAuthor%3A%20Yuhan%20Chen%20and%20Ying%20Fang%20and%20Guofa%20Li%20and%20Wenxuan%20Yu%20and%20Yicui%20Shi%20and%20Jingrui%20Zhang%20and%20Kefei%20Qian%20and%20Wenbo%20Chu%20and%20Keqiang%20Li%0AAbstract%3A%20Significant%20progress%20has%20been%20made%20in%20low-light%20image%20enhancement%20with%20respect%20to%20visual%20quality.%20However%2C%20most%20existing%20methods%20primarily%20operate%20in%20the%20pixel%20domain%20or%20rely%20on%20implicit%20feature%20representations.%20As%20a%20result%2C%20the%20intrinsic%20geometric%20structural%20priors%20of%20images%20are%20often%20neglected.%202D%20Gaussian%20Splatting%20%282DGS%29%20has%20emerged%20as%20a%20prominent%20explicit%20scene%20representation%20technique%20characterized%20by%20superior%20structural%20fitting%20capabilities%20and%20high%20rendering%20efficiency.%20Despite%20these%20advantages%2C%20the%20utilization%20of%202DGS%20in%20low-level%20vision%20tasks%20remains%20unexplored.%20To%20bridge%20this%20gap%2C%20LL-GaussianMap%20is%20proposed%20as%20the%20first%20unsupervised%20framework%20incorporating%202DGS%20into%20low-light%20image%20enhancement.%20Distinct%20from%20conventional%20methodologies%2C%20the%20enhancement%20task%20is%20formulated%20as%20a%20gain%20map%20generation%20process%20guided%20by%202DGS%20primitives.%20The%20proposed%20method%20comprises%20two%20primary%20stages.%20First%2C%20high-fidelity%20structural%20reconstruction%20is%20executed%20utilizing%202DGS.%20Then%2C%20data-driven%20enhancement%20dictionary%20coefficients%20are%20rendered%20via%20the%20rasterization%20mechanism%20of%20Gaussian%20splatting%20through%20an%20innovative%20unified%20enhancement%20module.%20This%20design%20effectively%20incorporates%20the%20structural%20perception%20capabilities%20of%202DGS%20into%20gain%20map%20generation%2C%20thereby%20preserving%20edges%20and%20suppressing%20artifacts%20during%20enhancement.%20Additionally%2C%20the%20reliance%20on%20paired%20data%20is%20circumvented%20through%20unsupervised%20learning.%20Experimental%20results%20demonstrate%20that%20LL-GaussianMap%20achieves%20superior%20enhancement%20performance%20with%20an%20extremely%20low%20storage%20footprint%2C%20highlighting%20the%20effectiveness%20of%20explicit%20Gaussian%20representations%20for%20image%20enhancement.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLL-GaussianMap%253A%2520Zero-shot%2520Low-Light%2520Image%2520Enhancement%2520via%25202D%2520Gaussian%2520Splatting%2520Guided%2520Gain%2520Maps%26entry.906535625%3DYuhan%2520Chen%2520and%2520Ying%2520Fang%2520and%2520Guofa%2520Li%2520and%2520Wenxuan%2520Yu%2520and%2520Yicui%2520Shi%2520and%2520Jingrui%2520Zhang%2520and%2520Kefei%2520Qian%2520and%2520Wenbo%2520Chu%2520and%2520Keqiang%2520Li%26entry.1292438233%3DSignificant%2520progress%2520has%2520been%2520made%2520in%2520low-light%2520image%2520enhancement%2520with%2520respect%2520to%2520visual%2520quality.%2520However%252C%2520most%2520existing%2520methods%2520primarily%2520operate%2520in%2520the%2520pixel%2520domain%2520or%2520rely%2520on%2520implicit%2520feature%2520representations.%2520As%2520a%2520result%252C%2520the%2520intrinsic%2520geometric%2520structural%2520priors%2520of%2520images%2520are%2520often%2520neglected.%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520has%2520emerged%2520as%2520a%2520prominent%2520explicit%2520scene%2520representation%2520technique%2520characterized%2520by%2520superior%2520structural%2520fitting%2520capabilities%2520and%2520high%2520rendering%2520efficiency.%2520Despite%2520these%2520advantages%252C%2520the%2520utilization%2520of%25202DGS%2520in%2520low-level%2520vision%2520tasks%2520remains%2520unexplored.%2520To%2520bridge%2520this%2520gap%252C%2520LL-GaussianMap%2520is%2520proposed%2520as%2520the%2520first%2520unsupervised%2520framework%2520incorporating%25202DGS%2520into%2520low-light%2520image%2520enhancement.%2520Distinct%2520from%2520conventional%2520methodologies%252C%2520the%2520enhancement%2520task%2520is%2520formulated%2520as%2520a%2520gain%2520map%2520generation%2520process%2520guided%2520by%25202DGS%2520primitives.%2520The%2520proposed%2520method%2520comprises%2520two%2520primary%2520stages.%2520First%252C%2520high-fidelity%2520structural%2520reconstruction%2520is%2520executed%2520utilizing%25202DGS.%2520Then%252C%2520data-driven%2520enhancement%2520dictionary%2520coefficients%2520are%2520rendered%2520via%2520the%2520rasterization%2520mechanism%2520of%2520Gaussian%2520splatting%2520through%2520an%2520innovative%2520unified%2520enhancement%2520module.%2520This%2520design%2520effectively%2520incorporates%2520the%2520structural%2520perception%2520capabilities%2520of%25202DGS%2520into%2520gain%2520map%2520generation%252C%2520thereby%2520preserving%2520edges%2520and%2520suppressing%2520artifacts%2520during%2520enhancement.%2520Additionally%252C%2520the%2520reliance%2520on%2520paired%2520data%2520is%2520circumvented%2520through%2520unsupervised%2520learning.%2520Experimental%2520results%2520demonstrate%2520that%2520LL-GaussianMap%2520achieves%2520superior%2520enhancement%2520performance%2520with%2520an%2520extremely%2520low%2520storage%2520footprint%252C%2520highlighting%2520the%2520effectiveness%2520of%2520explicit%2520Gaussian%2520representations%2520for%2520image%2520enhancement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LL-GaussianMap%3A%20Zero-shot%20Low-Light%20Image%20Enhancement%20via%202D%20Gaussian%20Splatting%20Guided%20Gain%20Maps&entry.906535625=Yuhan%20Chen%20and%20Ying%20Fang%20and%20Guofa%20Li%20and%20Wenxuan%20Yu%20and%20Yicui%20Shi%20and%20Jingrui%20Zhang%20and%20Kefei%20Qian%20and%20Wenbo%20Chu%20and%20Keqiang%20Li&entry.1292438233=Significant%20progress%20has%20been%20made%20in%20low-light%20image%20enhancement%20with%20respect%20to%20visual%20quality.%20However%2C%20most%20existing%20methods%20primarily%20operate%20in%20the%20pixel%20domain%20or%20rely%20on%20implicit%20feature%20representations.%20As%20a%20result%2C%20the%20intrinsic%20geometric%20structural%20priors%20of%20images%20are%20often%20neglected.%202D%20Gaussian%20Splatting%20%282DGS%29%20has%20emerged%20as%20a%20prominent%20explicit%20scene%20representation%20technique%20characterized%20by%20superior%20structural%20fitting%20capabilities%20and%20high%20rendering%20efficiency.%20Despite%20these%20advantages%2C%20the%20utilization%20of%202DGS%20in%20low-level%20vision%20tasks%20remains%20unexplored.%20To%20bridge%20this%20gap%2C%20LL-GaussianMap%20is%20proposed%20as%20the%20first%20unsupervised%20framework%20incorporating%202DGS%20into%20low-light%20image%20enhancement.%20Distinct%20from%20conventional%20methodologies%2C%20the%20enhancement%20task%20is%20formulated%20as%20a%20gain%20map%20generation%20process%20guided%20by%202DGS%20primitives.%20The%20proposed%20method%20comprises%20two%20primary%20stages.%20First%2C%20high-fidelity%20structural%20reconstruction%20is%20executed%20utilizing%202DGS.%20Then%2C%20data-driven%20enhancement%20dictionary%20coefficients%20are%20rendered%20via%20the%20rasterization%20mechanism%20of%20Gaussian%20splatting%20through%20an%20innovative%20unified%20enhancement%20module.%20This%20design%20effectively%20incorporates%20the%20structural%20perception%20capabilities%20of%202DGS%20into%20gain%20map%20generation%2C%20thereby%20preserving%20edges%20and%20suppressing%20artifacts%20during%20enhancement.%20Additionally%2C%20the%20reliance%20on%20paired%20data%20is%20circumvented%20through%20unsupervised%20learning.%20Experimental%20results%20demonstrate%20that%20LL-GaussianMap%20achieves%20superior%20enhancement%20performance%20with%20an%20extremely%20low%20storage%20footprint%2C%20highlighting%20the%20effectiveness%20of%20explicit%20Gaussian%20representations%20for%20image%20enhancement.&entry.1838667208=http%3A//arxiv.org/abs/2601.15766v2&entry.124074799=Read"},
{"title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction", "author": "Dominic Maggio and Luca Carlone", "abstract": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.", "link": "http://arxiv.org/abs/2601.19887v1", "date": "2026-01-27", "relevancy": 3.2236, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7555}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5918}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGGT-SLAM%202.0%3A%20Real%20time%20Dense%20Feed-forward%20Scene%20Reconstruction&body=Title%3A%20VGGT-SLAM%202.0%3A%20Real%20time%20Dense%20Feed-forward%20Scene%20Reconstruction%0AAuthor%3A%20Dominic%20Maggio%20and%20Luca%20Carlone%0AAbstract%3A%20We%20present%20VGGT-SLAM%202.0%2C%20a%20real%20time%20RGB%20feed-forward%20SLAM%20system%20which%20substantially%20improves%20upon%20VGGT-SLAM%20for%20incrementally%20aligning%20submaps%20created%20from%20VGGT.%20Firstly%2C%20we%20remove%20high-dimensional%2015-degree-of-freedom%20drift%20and%20planar%20degeneracy%20from%20VGGT-SLAM%20by%20creating%20a%20new%20factor%20graph%20design%20while%20still%20addressing%20the%20reconstruction%20ambiguity%20of%20VGGT%20given%20unknown%20camera%20intrinsics.%20Secondly%2C%20by%20studying%20the%20attention%20layers%20of%20VGGT%2C%20we%20show%20that%20one%20of%20the%20layers%20is%20well%20suited%20to%20assist%20in%20image%20retrieval%20verification%20for%20free%20without%20additional%20training%2C%20which%20enables%20both%20rejecting%20false%20positive%20matches%20and%20allows%20for%20completing%20more%20loop%20closures.%20Finally%2C%20we%20conduct%20a%20suite%20of%20experiments%20which%20includes%20showing%20VGGT-SLAM%202.0%20can%20easily%20be%20adapted%20for%20open-set%20object%20detection%20and%20demonstrating%20real%20time%20performance%20while%20running%20online%20onboard%20a%20ground%20robot%20using%20a%20Jetson%20Thor.%20We%20also%20test%20in%20environments%20ranging%20from%20cluttered%20indoor%20apartments%20and%20office%20scenes%20to%20a%204%2C200%20square%20foot%20barn%2C%20and%20we%20also%20demonstrate%20VGGT-SLAM%202.0%20achieves%20the%20highest%20accuracy%20on%20the%20TUM%20dataset%20with%20about%2023%20percent%20less%20pose%20error%20than%20VGGT-SLAM.%20Code%20will%20be%20released%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGGT-SLAM%25202.0%253A%2520Real%2520time%2520Dense%2520Feed-forward%2520Scene%2520Reconstruction%26entry.906535625%3DDominic%2520Maggio%2520and%2520Luca%2520Carlone%26entry.1292438233%3DWe%2520present%2520VGGT-SLAM%25202.0%252C%2520a%2520real%2520time%2520RGB%2520feed-forward%2520SLAM%2520system%2520which%2520substantially%2520improves%2520upon%2520VGGT-SLAM%2520for%2520incrementally%2520aligning%2520submaps%2520created%2520from%2520VGGT.%2520Firstly%252C%2520we%2520remove%2520high-dimensional%252015-degree-of-freedom%2520drift%2520and%2520planar%2520degeneracy%2520from%2520VGGT-SLAM%2520by%2520creating%2520a%2520new%2520factor%2520graph%2520design%2520while%2520still%2520addressing%2520the%2520reconstruction%2520ambiguity%2520of%2520VGGT%2520given%2520unknown%2520camera%2520intrinsics.%2520Secondly%252C%2520by%2520studying%2520the%2520attention%2520layers%2520of%2520VGGT%252C%2520we%2520show%2520that%2520one%2520of%2520the%2520layers%2520is%2520well%2520suited%2520to%2520assist%2520in%2520image%2520retrieval%2520verification%2520for%2520free%2520without%2520additional%2520training%252C%2520which%2520enables%2520both%2520rejecting%2520false%2520positive%2520matches%2520and%2520allows%2520for%2520completing%2520more%2520loop%2520closures.%2520Finally%252C%2520we%2520conduct%2520a%2520suite%2520of%2520experiments%2520which%2520includes%2520showing%2520VGGT-SLAM%25202.0%2520can%2520easily%2520be%2520adapted%2520for%2520open-set%2520object%2520detection%2520and%2520demonstrating%2520real%2520time%2520performance%2520while%2520running%2520online%2520onboard%2520a%2520ground%2520robot%2520using%2520a%2520Jetson%2520Thor.%2520We%2520also%2520test%2520in%2520environments%2520ranging%2520from%2520cluttered%2520indoor%2520apartments%2520and%2520office%2520scenes%2520to%2520a%25204%252C200%2520square%2520foot%2520barn%252C%2520and%2520we%2520also%2520demonstrate%2520VGGT-SLAM%25202.0%2520achieves%2520the%2520highest%2520accuracy%2520on%2520the%2520TUM%2520dataset%2520with%2520about%252023%2520percent%2520less%2520pose%2520error%2520than%2520VGGT-SLAM.%2520Code%2520will%2520be%2520released%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGGT-SLAM%202.0%3A%20Real%20time%20Dense%20Feed-forward%20Scene%20Reconstruction&entry.906535625=Dominic%20Maggio%20and%20Luca%20Carlone&entry.1292438233=We%20present%20VGGT-SLAM%202.0%2C%20a%20real%20time%20RGB%20feed-forward%20SLAM%20system%20which%20substantially%20improves%20upon%20VGGT-SLAM%20for%20incrementally%20aligning%20submaps%20created%20from%20VGGT.%20Firstly%2C%20we%20remove%20high-dimensional%2015-degree-of-freedom%20drift%20and%20planar%20degeneracy%20from%20VGGT-SLAM%20by%20creating%20a%20new%20factor%20graph%20design%20while%20still%20addressing%20the%20reconstruction%20ambiguity%20of%20VGGT%20given%20unknown%20camera%20intrinsics.%20Secondly%2C%20by%20studying%20the%20attention%20layers%20of%20VGGT%2C%20we%20show%20that%20one%20of%20the%20layers%20is%20well%20suited%20to%20assist%20in%20image%20retrieval%20verification%20for%20free%20without%20additional%20training%2C%20which%20enables%20both%20rejecting%20false%20positive%20matches%20and%20allows%20for%20completing%20more%20loop%20closures.%20Finally%2C%20we%20conduct%20a%20suite%20of%20experiments%20which%20includes%20showing%20VGGT-SLAM%202.0%20can%20easily%20be%20adapted%20for%20open-set%20object%20detection%20and%20demonstrating%20real%20time%20performance%20while%20running%20online%20onboard%20a%20ground%20robot%20using%20a%20Jetson%20Thor.%20We%20also%20test%20in%20environments%20ranging%20from%20cluttered%20indoor%20apartments%20and%20office%20scenes%20to%20a%204%2C200%20square%20foot%20barn%2C%20and%20we%20also%20demonstrate%20VGGT-SLAM%202.0%20achieves%20the%20highest%20accuracy%20on%20the%20TUM%20dataset%20with%20about%2023%20percent%20less%20pose%20error%20than%20VGGT-SLAM.%20Code%20will%20be%20released%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2601.19887v1&entry.124074799=Read"},
{"title": "WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration", "author": "Xinrui Zhang and Yufeng Wang and Shuangkang Fang and Zesheng Wang and Dacheng Qi and Wenrui Ding", "abstract": "Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.", "link": "http://arxiv.org/abs/2601.19753v1", "date": "2026-01-27", "relevancy": 3.0962, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6465}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6172}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaterClear-GS%3A%20Optical-Aware%20Gaussian%20Splatting%20for%20Underwater%20Reconstruction%20and%20Restoration&body=Title%3A%20WaterClear-GS%3A%20Optical-Aware%20Gaussian%20Splatting%20for%20Underwater%20Reconstruction%20and%20Restoration%0AAuthor%3A%20Xinrui%20Zhang%20and%20Yufeng%20Wang%20and%20Shuangkang%20Fang%20and%20Zesheng%20Wang%20and%20Dacheng%20Qi%20and%20Wenrui%20Ding%0AAbstract%3A%20Underwater%203D%20reconstruction%20and%20appearance%20restoration%20are%20hindered%20by%20the%20complex%20optical%20properties%20of%20water%2C%20such%20as%20wavelength-dependent%20attenuation%20and%20scattering.%20Existing%20Neural%20Radiance%20Fields%20%28NeRF%29-based%20methods%20struggle%20with%20slow%20rendering%20speeds%20and%20suboptimal%20color%20restoration%2C%20while%203D%20Gaussian%20Splatting%20%283DGS%29%20inherently%20lacks%20the%20capability%20to%20model%20complex%20volumetric%20scattering%20effects.%20To%20address%20these%20issues%2C%20we%20introduce%20WaterClear-GS%2C%20the%20first%20pure%203DGS-based%20framework%20that%20explicitly%20integrates%20underwater%20optical%20properties%20of%20local%20attenuation%20and%20scattering%20into%20Gaussian%20primitives%2C%20eliminating%20the%20need%20for%20an%20auxiliary%20medium%20network.%20Our%20method%20employs%20a%20dual-branch%20optimization%20strategy%20to%20ensure%20underwater%20photometric%20consistency%20while%20naturally%20recovering%20water-free%20appearances.%20This%20strategy%20is%20enhanced%20by%20depth-guided%20geometry%20regularization%20and%20perception-driven%20image%20loss%2C%20together%20with%20exposure%20constraints%2C%20spatially-adaptive%20regularization%2C%20and%20physically%20guided%20spectral%20regularization%2C%20which%20collectively%20enforce%20local%203D%20coherence%20and%20maintain%20natural%20visual%20perception.%20Experiments%20on%20standard%20benchmarks%20and%20our%20newly%20collected%20dataset%20demonstrate%20that%20WaterClear-GS%20achieves%20outstanding%20performance%20on%20both%20novel%20view%20synthesis%20%28NVS%29%20and%20underwater%20image%20restoration%20%28UIR%29%20tasks%2C%20while%20maintaining%20real-time%20rendering.%20The%20code%20will%20be%20available%20at%20https%3A//buaaxrzhang.github.io/WaterClear-GS/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaterClear-GS%253A%2520Optical-Aware%2520Gaussian%2520Splatting%2520for%2520Underwater%2520Reconstruction%2520and%2520Restoration%26entry.906535625%3DXinrui%2520Zhang%2520and%2520Yufeng%2520Wang%2520and%2520Shuangkang%2520Fang%2520and%2520Zesheng%2520Wang%2520and%2520Dacheng%2520Qi%2520and%2520Wenrui%2520Ding%26entry.1292438233%3DUnderwater%25203D%2520reconstruction%2520and%2520appearance%2520restoration%2520are%2520hindered%2520by%2520the%2520complex%2520optical%2520properties%2520of%2520water%252C%2520such%2520as%2520wavelength-dependent%2520attenuation%2520and%2520scattering.%2520Existing%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529-based%2520methods%2520struggle%2520with%2520slow%2520rendering%2520speeds%2520and%2520suboptimal%2520color%2520restoration%252C%2520while%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520inherently%2520lacks%2520the%2520capability%2520to%2520model%2520complex%2520volumetric%2520scattering%2520effects.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520WaterClear-GS%252C%2520the%2520first%2520pure%25203DGS-based%2520framework%2520that%2520explicitly%2520integrates%2520underwater%2520optical%2520properties%2520of%2520local%2520attenuation%2520and%2520scattering%2520into%2520Gaussian%2520primitives%252C%2520eliminating%2520the%2520need%2520for%2520an%2520auxiliary%2520medium%2520network.%2520Our%2520method%2520employs%2520a%2520dual-branch%2520optimization%2520strategy%2520to%2520ensure%2520underwater%2520photometric%2520consistency%2520while%2520naturally%2520recovering%2520water-free%2520appearances.%2520This%2520strategy%2520is%2520enhanced%2520by%2520depth-guided%2520geometry%2520regularization%2520and%2520perception-driven%2520image%2520loss%252C%2520together%2520with%2520exposure%2520constraints%252C%2520spatially-adaptive%2520regularization%252C%2520and%2520physically%2520guided%2520spectral%2520regularization%252C%2520which%2520collectively%2520enforce%2520local%25203D%2520coherence%2520and%2520maintain%2520natural%2520visual%2520perception.%2520Experiments%2520on%2520standard%2520benchmarks%2520and%2520our%2520newly%2520collected%2520dataset%2520demonstrate%2520that%2520WaterClear-GS%2520achieves%2520outstanding%2520performance%2520on%2520both%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520and%2520underwater%2520image%2520restoration%2520%2528UIR%2529%2520tasks%252C%2520while%2520maintaining%2520real-time%2520rendering.%2520The%2520code%2520will%2520be%2520available%2520at%2520https%253A//buaaxrzhang.github.io/WaterClear-GS/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaterClear-GS%3A%20Optical-Aware%20Gaussian%20Splatting%20for%20Underwater%20Reconstruction%20and%20Restoration&entry.906535625=Xinrui%20Zhang%20and%20Yufeng%20Wang%20and%20Shuangkang%20Fang%20and%20Zesheng%20Wang%20and%20Dacheng%20Qi%20and%20Wenrui%20Ding&entry.1292438233=Underwater%203D%20reconstruction%20and%20appearance%20restoration%20are%20hindered%20by%20the%20complex%20optical%20properties%20of%20water%2C%20such%20as%20wavelength-dependent%20attenuation%20and%20scattering.%20Existing%20Neural%20Radiance%20Fields%20%28NeRF%29-based%20methods%20struggle%20with%20slow%20rendering%20speeds%20and%20suboptimal%20color%20restoration%2C%20while%203D%20Gaussian%20Splatting%20%283DGS%29%20inherently%20lacks%20the%20capability%20to%20model%20complex%20volumetric%20scattering%20effects.%20To%20address%20these%20issues%2C%20we%20introduce%20WaterClear-GS%2C%20the%20first%20pure%203DGS-based%20framework%20that%20explicitly%20integrates%20underwater%20optical%20properties%20of%20local%20attenuation%20and%20scattering%20into%20Gaussian%20primitives%2C%20eliminating%20the%20need%20for%20an%20auxiliary%20medium%20network.%20Our%20method%20employs%20a%20dual-branch%20optimization%20strategy%20to%20ensure%20underwater%20photometric%20consistency%20while%20naturally%20recovering%20water-free%20appearances.%20This%20strategy%20is%20enhanced%20by%20depth-guided%20geometry%20regularization%20and%20perception-driven%20image%20loss%2C%20together%20with%20exposure%20constraints%2C%20spatially-adaptive%20regularization%2C%20and%20physically%20guided%20spectral%20regularization%2C%20which%20collectively%20enforce%20local%203D%20coherence%20and%20maintain%20natural%20visual%20perception.%20Experiments%20on%20standard%20benchmarks%20and%20our%20newly%20collected%20dataset%20demonstrate%20that%20WaterClear-GS%20achieves%20outstanding%20performance%20on%20both%20novel%20view%20synthesis%20%28NVS%29%20and%20underwater%20image%20restoration%20%28UIR%29%20tasks%2C%20while%20maintaining%20real-time%20rendering.%20The%20code%20will%20be%20available%20at%20https%3A//buaaxrzhang.github.io/WaterClear-GS/.&entry.1838667208=http%3A//arxiv.org/abs/2601.19753v1&entry.124074799=Read"},
{"title": "MV-S2V: Multi-View Subject-Consistent Video Generation", "author": "Ziyang Song and Xinyu Gong and Bangya Liu and Zelin Zhao", "abstract": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at: https://szy-young.github.io/mv-s2v", "link": "http://arxiv.org/abs/2601.17756v2", "date": "2026-01-27", "relevancy": 3.0036, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6394}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.583}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-S2V%3A%20Multi-View%20Subject-Consistent%20Video%20Generation&body=Title%3A%20MV-S2V%3A%20Multi-View%20Subject-Consistent%20Video%20Generation%0AAuthor%3A%20Ziyang%20Song%20and%20Xinyu%20Gong%20and%20Bangya%20Liu%20and%20Zelin%20Zhao%0AAbstract%3A%20Existing%20Subject-to-Video%20Generation%20%28S2V%29%20methods%20have%20achieved%20high-fidelity%20and%20subject-consistent%20video%20generation%2C%20yet%20remain%20constrained%20to%20single-view%20subject%20references.%20This%20limitation%20renders%20the%20S2V%20task%20reducible%20to%20an%20S2I%20%2B%20I2V%20pipeline%2C%20failing%20to%20exploit%20the%20full%20potential%20of%20video%20subject%20control.%20In%20this%20work%2C%20we%20propose%20and%20address%20the%20challenging%20Multi-View%20S2V%20%28MV-S2V%29%20task%2C%20which%20synthesizes%20videos%20from%20multiple%20reference%20views%20to%20enforce%203D-level%20subject%20consistency.%20Regarding%20the%20scarcity%20of%20training%20data%2C%20we%20first%20develop%20a%20synthetic%20data%20curation%20pipeline%20to%20generate%20highly%20customized%20synthetic%20data%2C%20complemented%20by%20a%20small-scale%20real-world%20captured%20dataset%20to%20boost%20the%20training%20of%20MV-S2V.%20Another%20key%20issue%20lies%20in%20the%20potential%20confusion%20between%20cross-subject%20and%20cross-view%20references%20in%20conditional%20generation.%20To%20overcome%20this%2C%20we%20further%20introduce%20Temporally%20Shifted%20RoPE%20%28TS-RoPE%29%20to%20distinguish%20between%20different%20subjects%20and%20distinct%20views%20of%20the%20same%20subject%20in%20reference%20conditioning.%20Our%20framework%20achieves%20superior%203D%20subject%20consistency%20w.r.t.%20multi-view%20reference%20images%20and%20high-quality%20visual%20outputs%2C%20establishing%20a%20new%20meaningful%20direction%20for%20subject-driven%20video%20generation.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//szy-young.github.io/mv-s2v%0ALink%3A%20http%3A//arxiv.org/abs/2601.17756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-S2V%253A%2520Multi-View%2520Subject-Consistent%2520Video%2520Generation%26entry.906535625%3DZiyang%2520Song%2520and%2520Xinyu%2520Gong%2520and%2520Bangya%2520Liu%2520and%2520Zelin%2520Zhao%26entry.1292438233%3DExisting%2520Subject-to-Video%2520Generation%2520%2528S2V%2529%2520methods%2520have%2520achieved%2520high-fidelity%2520and%2520subject-consistent%2520video%2520generation%252C%2520yet%2520remain%2520constrained%2520to%2520single-view%2520subject%2520references.%2520This%2520limitation%2520renders%2520the%2520S2V%2520task%2520reducible%2520to%2520an%2520S2I%2520%252B%2520I2V%2520pipeline%252C%2520failing%2520to%2520exploit%2520the%2520full%2520potential%2520of%2520video%2520subject%2520control.%2520In%2520this%2520work%252C%2520we%2520propose%2520and%2520address%2520the%2520challenging%2520Multi-View%2520S2V%2520%2528MV-S2V%2529%2520task%252C%2520which%2520synthesizes%2520videos%2520from%2520multiple%2520reference%2520views%2520to%2520enforce%25203D-level%2520subject%2520consistency.%2520Regarding%2520the%2520scarcity%2520of%2520training%2520data%252C%2520we%2520first%2520develop%2520a%2520synthetic%2520data%2520curation%2520pipeline%2520to%2520generate%2520highly%2520customized%2520synthetic%2520data%252C%2520complemented%2520by%2520a%2520small-scale%2520real-world%2520captured%2520dataset%2520to%2520boost%2520the%2520training%2520of%2520MV-S2V.%2520Another%2520key%2520issue%2520lies%2520in%2520the%2520potential%2520confusion%2520between%2520cross-subject%2520and%2520cross-view%2520references%2520in%2520conditional%2520generation.%2520To%2520overcome%2520this%252C%2520we%2520further%2520introduce%2520Temporally%2520Shifted%2520RoPE%2520%2528TS-RoPE%2529%2520to%2520distinguish%2520between%2520different%2520subjects%2520and%2520distinct%2520views%2520of%2520the%2520same%2520subject%2520in%2520reference%2520conditioning.%2520Our%2520framework%2520achieves%2520superior%25203D%2520subject%2520consistency%2520w.r.t.%2520multi-view%2520reference%2520images%2520and%2520high-quality%2520visual%2520outputs%252C%2520establishing%2520a%2520new%2520meaningful%2520direction%2520for%2520subject-driven%2520video%2520generation.%2520Our%2520project%2520page%2520is%2520available%2520at%253A%2520https%253A//szy-young.github.io/mv-s2v%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-S2V%3A%20Multi-View%20Subject-Consistent%20Video%20Generation&entry.906535625=Ziyang%20Song%20and%20Xinyu%20Gong%20and%20Bangya%20Liu%20and%20Zelin%20Zhao&entry.1292438233=Existing%20Subject-to-Video%20Generation%20%28S2V%29%20methods%20have%20achieved%20high-fidelity%20and%20subject-consistent%20video%20generation%2C%20yet%20remain%20constrained%20to%20single-view%20subject%20references.%20This%20limitation%20renders%20the%20S2V%20task%20reducible%20to%20an%20S2I%20%2B%20I2V%20pipeline%2C%20failing%20to%20exploit%20the%20full%20potential%20of%20video%20subject%20control.%20In%20this%20work%2C%20we%20propose%20and%20address%20the%20challenging%20Multi-View%20S2V%20%28MV-S2V%29%20task%2C%20which%20synthesizes%20videos%20from%20multiple%20reference%20views%20to%20enforce%203D-level%20subject%20consistency.%20Regarding%20the%20scarcity%20of%20training%20data%2C%20we%20first%20develop%20a%20synthetic%20data%20curation%20pipeline%20to%20generate%20highly%20customized%20synthetic%20data%2C%20complemented%20by%20a%20small-scale%20real-world%20captured%20dataset%20to%20boost%20the%20training%20of%20MV-S2V.%20Another%20key%20issue%20lies%20in%20the%20potential%20confusion%20between%20cross-subject%20and%20cross-view%20references%20in%20conditional%20generation.%20To%20overcome%20this%2C%20we%20further%20introduce%20Temporally%20Shifted%20RoPE%20%28TS-RoPE%29%20to%20distinguish%20between%20different%20subjects%20and%20distinct%20views%20of%20the%20same%20subject%20in%20reference%20conditioning.%20Our%20framework%20achieves%20superior%203D%20subject%20consistency%20w.r.t.%20multi-view%20reference%20images%20and%20high-quality%20visual%20outputs%2C%20establishing%20a%20new%20meaningful%20direction%20for%20subject-driven%20video%20generation.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//szy-young.github.io/mv-s2v&entry.1838667208=http%3A//arxiv.org/abs/2601.17756v2&entry.124074799=Read"},
{"title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs", "author": "Jen-Tse Huang and Dasen Dai and Jen-Yuan Huang and Youliang Yuan and Xiaoyuan Liu and Wenxuan Wang and Wenxiang Jiao and Pinjia He and Zhaopeng Tu and Haodong Duan", "abstract": "Humans develop perception through a bottom-up hierarchy: from basic primitives and Gestalt principles to high-level semantics. In contrast, current Multimodal Large Language Models (MLLMs) are trained directly on complex downstream tasks, often bypassing these foundational visual capabilities. To systematically investigate this gap, we introduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from FRCT, a well-established cognitive psychology assessment spanning four domains of human visual cognition. Furthermore, we design algorithms to automatically construct and validate unlimited test cases with controllable difficulty. Using VisFactor, we evaluate 23 frontier MLLMs, including both proprietary (e.g., GPT, Gemini) and open-source (e.g., LLaMA, Qwen) models. The best model achieves a score of only 30.17%. Models consistently fail on tasks such as mental rotation, spatial relation inference, and figure-ground discrimination, regardless of model size or prompting strategy. These findings suggest that performance improvements on existing general benchmarks might represent castles in the air instead of a genuine mastery of human-like visual cognition.", "link": "http://arxiv.org/abs/2502.16435v3", "date": "2026-01-27", "relevancy": 2.9706, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6269}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Cognitive%20Benchmarks%20Reveal%20Foundational%20Visual%20Gaps%20in%20MLLMs&body=Title%3A%20Human%20Cognitive%20Benchmarks%20Reveal%20Foundational%20Visual%20Gaps%20in%20MLLMs%0AAuthor%3A%20Jen-Tse%20Huang%20and%20Dasen%20Dai%20and%20Jen-Yuan%20Huang%20and%20Youliang%20Yuan%20and%20Xiaoyuan%20Liu%20and%20Wenxuan%20Wang%20and%20Wenxiang%20Jiao%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu%20and%20Haodong%20Duan%0AAbstract%3A%20Humans%20develop%20perception%20through%20a%20bottom-up%20hierarchy%3A%20from%20basic%20primitives%20and%20Gestalt%20principles%20to%20high-level%20semantics.%20In%20contrast%2C%20current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20trained%20directly%20on%20complex%20downstream%20tasks%2C%20often%20bypassing%20these%20foundational%20visual%20capabilities.%20To%20systematically%20investigate%20this%20gap%2C%20we%20introduce%20VisFactor%2C%20a%20benchmark%20that%20digitizes%2020%20vision-centric%20subtests%20from%20FRCT%2C%20a%20well-established%20cognitive%20psychology%20assessment%20spanning%20four%20domains%20of%20human%20visual%20cognition.%20Furthermore%2C%20we%20design%20algorithms%20to%20automatically%20construct%20and%20validate%20unlimited%20test%20cases%20with%20controllable%20difficulty.%20Using%20VisFactor%2C%20we%20evaluate%2023%20frontier%20MLLMs%2C%20including%20both%20proprietary%20%28e.g.%2C%20GPT%2C%20Gemini%29%20and%20open-source%20%28e.g.%2C%20LLaMA%2C%20Qwen%29%20models.%20The%20best%20model%20achieves%20a%20score%20of%20only%2030.17%25.%20Models%20consistently%20fail%20on%20tasks%20such%20as%20mental%20rotation%2C%20spatial%20relation%20inference%2C%20and%20figure-ground%20discrimination%2C%20regardless%20of%20model%20size%20or%20prompting%20strategy.%20These%20findings%20suggest%20that%20performance%20improvements%20on%20existing%20general%20benchmarks%20might%20represent%20castles%20in%20the%20air%20instead%20of%20a%20genuine%20mastery%20of%20human-like%20visual%20cognition.%0ALink%3A%20http%3A//arxiv.org/abs/2502.16435v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Cognitive%2520Benchmarks%2520Reveal%2520Foundational%2520Visual%2520Gaps%2520in%2520MLLMs%26entry.906535625%3DJen-Tse%2520Huang%2520and%2520Dasen%2520Dai%2520and%2520Jen-Yuan%2520Huang%2520and%2520Youliang%2520Yuan%2520and%2520Xiaoyuan%2520Liu%2520and%2520Wenxuan%2520Wang%2520and%2520Wenxiang%2520Jiao%2520and%2520Pinjia%2520He%2520and%2520Zhaopeng%2520Tu%2520and%2520Haodong%2520Duan%26entry.1292438233%3DHumans%2520develop%2520perception%2520through%2520a%2520bottom-up%2520hierarchy%253A%2520from%2520basic%2520primitives%2520and%2520Gestalt%2520principles%2520to%2520high-level%2520semantics.%2520In%2520contrast%252C%2520current%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520trained%2520directly%2520on%2520complex%2520downstream%2520tasks%252C%2520often%2520bypassing%2520these%2520foundational%2520visual%2520capabilities.%2520To%2520systematically%2520investigate%2520this%2520gap%252C%2520we%2520introduce%2520VisFactor%252C%2520a%2520benchmark%2520that%2520digitizes%252020%2520vision-centric%2520subtests%2520from%2520FRCT%252C%2520a%2520well-established%2520cognitive%2520psychology%2520assessment%2520spanning%2520four%2520domains%2520of%2520human%2520visual%2520cognition.%2520Furthermore%252C%2520we%2520design%2520algorithms%2520to%2520automatically%2520construct%2520and%2520validate%2520unlimited%2520test%2520cases%2520with%2520controllable%2520difficulty.%2520Using%2520VisFactor%252C%2520we%2520evaluate%252023%2520frontier%2520MLLMs%252C%2520including%2520both%2520proprietary%2520%2528e.g.%252C%2520GPT%252C%2520Gemini%2529%2520and%2520open-source%2520%2528e.g.%252C%2520LLaMA%252C%2520Qwen%2529%2520models.%2520The%2520best%2520model%2520achieves%2520a%2520score%2520of%2520only%252030.17%2525.%2520Models%2520consistently%2520fail%2520on%2520tasks%2520such%2520as%2520mental%2520rotation%252C%2520spatial%2520relation%2520inference%252C%2520and%2520figure-ground%2520discrimination%252C%2520regardless%2520of%2520model%2520size%2520or%2520prompting%2520strategy.%2520These%2520findings%2520suggest%2520that%2520performance%2520improvements%2520on%2520existing%2520general%2520benchmarks%2520might%2520represent%2520castles%2520in%2520the%2520air%2520instead%2520of%2520a%2520genuine%2520mastery%2520of%2520human-like%2520visual%2520cognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16435v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Cognitive%20Benchmarks%20Reveal%20Foundational%20Visual%20Gaps%20in%20MLLMs&entry.906535625=Jen-Tse%20Huang%20and%20Dasen%20Dai%20and%20Jen-Yuan%20Huang%20and%20Youliang%20Yuan%20and%20Xiaoyuan%20Liu%20and%20Wenxuan%20Wang%20and%20Wenxiang%20Jiao%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu%20and%20Haodong%20Duan&entry.1292438233=Humans%20develop%20perception%20through%20a%20bottom-up%20hierarchy%3A%20from%20basic%20primitives%20and%20Gestalt%20principles%20to%20high-level%20semantics.%20In%20contrast%2C%20current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20trained%20directly%20on%20complex%20downstream%20tasks%2C%20often%20bypassing%20these%20foundational%20visual%20capabilities.%20To%20systematically%20investigate%20this%20gap%2C%20we%20introduce%20VisFactor%2C%20a%20benchmark%20that%20digitizes%2020%20vision-centric%20subtests%20from%20FRCT%2C%20a%20well-established%20cognitive%20psychology%20assessment%20spanning%20four%20domains%20of%20human%20visual%20cognition.%20Furthermore%2C%20we%20design%20algorithms%20to%20automatically%20construct%20and%20validate%20unlimited%20test%20cases%20with%20controllable%20difficulty.%20Using%20VisFactor%2C%20we%20evaluate%2023%20frontier%20MLLMs%2C%20including%20both%20proprietary%20%28e.g.%2C%20GPT%2C%20Gemini%29%20and%20open-source%20%28e.g.%2C%20LLaMA%2C%20Qwen%29%20models.%20The%20best%20model%20achieves%20a%20score%20of%20only%2030.17%25.%20Models%20consistently%20fail%20on%20tasks%20such%20as%20mental%20rotation%2C%20spatial%20relation%20inference%2C%20and%20figure-ground%20discrimination%2C%20regardless%20of%20model%20size%20or%20prompting%20strategy.%20These%20findings%20suggest%20that%20performance%20improvements%20on%20existing%20general%20benchmarks%20might%20represent%20castles%20in%20the%20air%20instead%20of%20a%20genuine%20mastery%20of%20human-like%20visual%20cognition.&entry.1838667208=http%3A//arxiv.org/abs/2502.16435v3&entry.124074799=Read"},
{"title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models", "author": "Jialong Wu and Xiaoying Zhang and Hongyi Yuan and Xiangcheng Zhang and Tianhao Huang and Changjing He and Chaoyi Deng and Renrui Zhang and Youbin Wu and Mingsheng Long", "abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.", "link": "http://arxiv.org/abs/2601.19834v1", "date": "2026-01-27", "relevancy": 2.9179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5886}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Generation%20Unlocks%20Human-Like%20Reasoning%20through%20Multimodal%20World%20Models&body=Title%3A%20Visual%20Generation%20Unlocks%20Human-Like%20Reasoning%20through%20Multimodal%20World%20Models%0AAuthor%3A%20Jialong%20Wu%20and%20Xiaoying%20Zhang%20and%20Hongyi%20Yuan%20and%20Xiangcheng%20Zhang%20and%20Tianhao%20Huang%20and%20Changjing%20He%20and%20Chaoyi%20Deng%20and%20Renrui%20Zhang%20and%20Youbin%20Wu%20and%20Mingsheng%20Long%0AAbstract%3A%20Humans%20construct%20internal%20world%20models%20and%20reason%20by%20manipulating%20the%20concepts%20within%20these%20models.%20Recent%20advances%20in%20AI%2C%20particularly%20chain-of-thought%20%28CoT%29%20reasoning%2C%20approximate%20such%20human%20cognitive%20abilities%2C%20where%20world%20models%20are%20believed%20to%20be%20embedded%20within%20large%20language%20models.%20Expert-level%20performance%20in%20formal%20and%20abstract%20domains%20such%20as%20mathematics%20and%20programming%20has%20been%20achieved%20in%20current%20systems%20by%20relying%20predominantly%20on%20verbal%20reasoning.%20However%2C%20they%20still%20lag%20far%20behind%20humans%20in%20domains%20like%20physical%20and%20spatial%20intelligence%2C%20which%20require%20richer%20representations%20and%20prior%20knowledge.%20The%20emergence%20of%20unified%20multimodal%20models%20%28UMMs%29%20capable%20of%20both%20verbal%20and%20visual%20generation%20has%20therefore%20sparked%20interest%20in%20more%20human-like%20reasoning%20grounded%20in%20complementary%20multimodal%20pathways%2C%20though%20their%20benefits%20remain%20unclear.%20From%20a%20world-model%20perspective%2C%20this%20paper%20presents%20the%20first%20principled%20study%20of%20when%20and%20how%20visual%20generation%20benefits%20reasoning.%20Our%20key%20position%20is%20the%20visual%20superiority%20hypothesis%3A%20for%20certain%20tasks--particularly%20those%20grounded%20in%20the%20physical%20world--visual%20generation%20more%20naturally%20serves%20as%20world%20models%2C%20whereas%20purely%20verbal%20world%20models%20encounter%20bottlenecks%20arising%20from%20representational%20limitations%20or%20insufficient%20prior%20knowledge.%20Theoretically%2C%20we%20formalize%20internal%20world%20modeling%20as%20a%20core%20component%20of%20CoT%20reasoning%20and%20analyze%20distinctions%20among%20different%20forms%20of%20world%20models.%20Empirically%2C%20we%20identify%20tasks%20that%20necessitate%20interleaved%20visual-verbal%20CoT%20reasoning%2C%20constructing%20a%20new%20evaluation%20suite%2C%20VisWorld-Eval.%20Controlled%20experiments%20on%20a%20state-of-the-art%20UMM%20show%20that%20interleaved%20CoT%20significantly%20outperforms%20purely%20verbal%20CoT%20on%20tasks%20that%20favor%20visual%20world%20modeling%2C%20but%20offers%20no%20clear%20advantage%20otherwise.%20Together%2C%20this%20work%20clarifies%20the%20potential%20of%20multimodal%20world%20modeling%20for%20more%20powerful%2C%20human-like%20multimodal%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Generation%2520Unlocks%2520Human-Like%2520Reasoning%2520through%2520Multimodal%2520World%2520Models%26entry.906535625%3DJialong%2520Wu%2520and%2520Xiaoying%2520Zhang%2520and%2520Hongyi%2520Yuan%2520and%2520Xiangcheng%2520Zhang%2520and%2520Tianhao%2520Huang%2520and%2520Changjing%2520He%2520and%2520Chaoyi%2520Deng%2520and%2520Renrui%2520Zhang%2520and%2520Youbin%2520Wu%2520and%2520Mingsheng%2520Long%26entry.1292438233%3DHumans%2520construct%2520internal%2520world%2520models%2520and%2520reason%2520by%2520manipulating%2520the%2520concepts%2520within%2520these%2520models.%2520Recent%2520advances%2520in%2520AI%252C%2520particularly%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%252C%2520approximate%2520such%2520human%2520cognitive%2520abilities%252C%2520where%2520world%2520models%2520are%2520believed%2520to%2520be%2520embedded%2520within%2520large%2520language%2520models.%2520Expert-level%2520performance%2520in%2520formal%2520and%2520abstract%2520domains%2520such%2520as%2520mathematics%2520and%2520programming%2520has%2520been%2520achieved%2520in%2520current%2520systems%2520by%2520relying%2520predominantly%2520on%2520verbal%2520reasoning.%2520However%252C%2520they%2520still%2520lag%2520far%2520behind%2520humans%2520in%2520domains%2520like%2520physical%2520and%2520spatial%2520intelligence%252C%2520which%2520require%2520richer%2520representations%2520and%2520prior%2520knowledge.%2520The%2520emergence%2520of%2520unified%2520multimodal%2520models%2520%2528UMMs%2529%2520capable%2520of%2520both%2520verbal%2520and%2520visual%2520generation%2520has%2520therefore%2520sparked%2520interest%2520in%2520more%2520human-like%2520reasoning%2520grounded%2520in%2520complementary%2520multimodal%2520pathways%252C%2520though%2520their%2520benefits%2520remain%2520unclear.%2520From%2520a%2520world-model%2520perspective%252C%2520this%2520paper%2520presents%2520the%2520first%2520principled%2520study%2520of%2520when%2520and%2520how%2520visual%2520generation%2520benefits%2520reasoning.%2520Our%2520key%2520position%2520is%2520the%2520visual%2520superiority%2520hypothesis%253A%2520for%2520certain%2520tasks--particularly%2520those%2520grounded%2520in%2520the%2520physical%2520world--visual%2520generation%2520more%2520naturally%2520serves%2520as%2520world%2520models%252C%2520whereas%2520purely%2520verbal%2520world%2520models%2520encounter%2520bottlenecks%2520arising%2520from%2520representational%2520limitations%2520or%2520insufficient%2520prior%2520knowledge.%2520Theoretically%252C%2520we%2520formalize%2520internal%2520world%2520modeling%2520as%2520a%2520core%2520component%2520of%2520CoT%2520reasoning%2520and%2520analyze%2520distinctions%2520among%2520different%2520forms%2520of%2520world%2520models.%2520Empirically%252C%2520we%2520identify%2520tasks%2520that%2520necessitate%2520interleaved%2520visual-verbal%2520CoT%2520reasoning%252C%2520constructing%2520a%2520new%2520evaluation%2520suite%252C%2520VisWorld-Eval.%2520Controlled%2520experiments%2520on%2520a%2520state-of-the-art%2520UMM%2520show%2520that%2520interleaved%2520CoT%2520significantly%2520outperforms%2520purely%2520verbal%2520CoT%2520on%2520tasks%2520that%2520favor%2520visual%2520world%2520modeling%252C%2520but%2520offers%2520no%2520clear%2520advantage%2520otherwise.%2520Together%252C%2520this%2520work%2520clarifies%2520the%2520potential%2520of%2520multimodal%2520world%2520modeling%2520for%2520more%2520powerful%252C%2520human-like%2520multimodal%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Generation%20Unlocks%20Human-Like%20Reasoning%20through%20Multimodal%20World%20Models&entry.906535625=Jialong%20Wu%20and%20Xiaoying%20Zhang%20and%20Hongyi%20Yuan%20and%20Xiangcheng%20Zhang%20and%20Tianhao%20Huang%20and%20Changjing%20He%20and%20Chaoyi%20Deng%20and%20Renrui%20Zhang%20and%20Youbin%20Wu%20and%20Mingsheng%20Long&entry.1292438233=Humans%20construct%20internal%20world%20models%20and%20reason%20by%20manipulating%20the%20concepts%20within%20these%20models.%20Recent%20advances%20in%20AI%2C%20particularly%20chain-of-thought%20%28CoT%29%20reasoning%2C%20approximate%20such%20human%20cognitive%20abilities%2C%20where%20world%20models%20are%20believed%20to%20be%20embedded%20within%20large%20language%20models.%20Expert-level%20performance%20in%20formal%20and%20abstract%20domains%20such%20as%20mathematics%20and%20programming%20has%20been%20achieved%20in%20current%20systems%20by%20relying%20predominantly%20on%20verbal%20reasoning.%20However%2C%20they%20still%20lag%20far%20behind%20humans%20in%20domains%20like%20physical%20and%20spatial%20intelligence%2C%20which%20require%20richer%20representations%20and%20prior%20knowledge.%20The%20emergence%20of%20unified%20multimodal%20models%20%28UMMs%29%20capable%20of%20both%20verbal%20and%20visual%20generation%20has%20therefore%20sparked%20interest%20in%20more%20human-like%20reasoning%20grounded%20in%20complementary%20multimodal%20pathways%2C%20though%20their%20benefits%20remain%20unclear.%20From%20a%20world-model%20perspective%2C%20this%20paper%20presents%20the%20first%20principled%20study%20of%20when%20and%20how%20visual%20generation%20benefits%20reasoning.%20Our%20key%20position%20is%20the%20visual%20superiority%20hypothesis%3A%20for%20certain%20tasks--particularly%20those%20grounded%20in%20the%20physical%20world--visual%20generation%20more%20naturally%20serves%20as%20world%20models%2C%20whereas%20purely%20verbal%20world%20models%20encounter%20bottlenecks%20arising%20from%20representational%20limitations%20or%20insufficient%20prior%20knowledge.%20Theoretically%2C%20we%20formalize%20internal%20world%20modeling%20as%20a%20core%20component%20of%20CoT%20reasoning%20and%20analyze%20distinctions%20among%20different%20forms%20of%20world%20models.%20Empirically%2C%20we%20identify%20tasks%20that%20necessitate%20interleaved%20visual-verbal%20CoT%20reasoning%2C%20constructing%20a%20new%20evaluation%20suite%2C%20VisWorld-Eval.%20Controlled%20experiments%20on%20a%20state-of-the-art%20UMM%20show%20that%20interleaved%20CoT%20significantly%20outperforms%20purely%20verbal%20CoT%20on%20tasks%20that%20favor%20visual%20world%20modeling%2C%20but%20offers%20no%20clear%20advantage%20otherwise.%20Together%2C%20this%20work%20clarifies%20the%20potential%20of%20multimodal%20world%20modeling%20for%20more%20powerful%2C%20human-like%20multimodal%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2601.19834v1&entry.124074799=Read"},
{"title": "GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining", "author": "Shentong Mo and Zehua Chen and Jun Zhu", "abstract": "Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.", "link": "http://arxiv.org/abs/2601.19606v1", "date": "2026-01-27", "relevancy": 2.8951, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5929}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMS-CAVP%3A%20Improving%20Audio-Video%20Correspondence%20with%20Multi-Scale%20Contrastive%20and%20Generative%20Pretraining&body=Title%3A%20GMS-CAVP%3A%20Improving%20Audio-Video%20Correspondence%20with%20Multi-Scale%20Contrastive%20and%20Generative%20Pretraining%0AAuthor%3A%20Shentong%20Mo%20and%20Zehua%20Chen%20and%20Jun%20Zhu%0AAbstract%3A%20Recent%20advances%20in%20video-audio%20%28V-A%29%20understanding%20and%20generation%20have%20increasingly%20relied%20on%20joint%20V-A%20embeddings%2C%20which%20serve%20as%20the%20foundation%20for%20tasks%20such%20as%20cross-modal%20retrieval%20and%20generation.%20While%20prior%20methods%20like%20CAVP%20effectively%20model%20semantic%20and%20temporal%20correspondences%20between%20modalities%20using%20contrastive%20objectives%2C%20their%20performance%20remains%20suboptimal.%20A%20key%20limitation%20is%20the%20insufficient%20modeling%20of%20the%20dense%2C%20multi-scale%20nature%20of%20both%20video%20and%20audio%20signals%2C%20correspondences%20often%20span%20fine-%20to%20coarse-grained%20spatial-temporal%20structures%2C%20which%20are%20underutilized%20in%20existing%20frameworks.%20To%20this%20end%2C%20we%20propose%20GMS-CAVP%2C%20a%20novel%20framework%20that%20combines%20Multi-Scale%20Video-Audio%20Alignment%20and%20Multi-Scale%20Spatial-Temporal%20Diffusion-based%20pretraining%20objectives%20to%20enhance%20V-A%20correspondence%20modeling.%20First%2C%20GMS-CAVP%20introduces%20a%20multi-scale%20contrastive%20learning%20strategy%20that%20captures%20semantic%20and%20temporal%20relations%20across%20varying%20granularities.%20Second%2C%20we%20go%20beyond%20traditional%20contrastive%20learning%20by%20incorporating%20a%20diffusion-based%20generative%20objective%2C%20enabling%20modality%20translation%20and%20synthesis%20between%20video%20and%20audio.%20This%20unified%20discriminative-generative%20formulation%20facilitates%20deeper%20cross-modal%20understanding%20and%20paves%20the%20way%20for%20high-fidelity%20generation.%20Extensive%20experiments%20on%20VGGSound%2C%20AudioSet%2C%20and%20Panda70M%20demonstrate%20that%20GMS-CAVP%20outperforms%20previous%20methods%20in%20generation%20and%20retrieval.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMS-CAVP%253A%2520Improving%2520Audio-Video%2520Correspondence%2520with%2520Multi-Scale%2520Contrastive%2520and%2520Generative%2520Pretraining%26entry.906535625%3DShentong%2520Mo%2520and%2520Zehua%2520Chen%2520and%2520Jun%2520Zhu%26entry.1292438233%3DRecent%2520advances%2520in%2520video-audio%2520%2528V-A%2529%2520understanding%2520and%2520generation%2520have%2520increasingly%2520relied%2520on%2520joint%2520V-A%2520embeddings%252C%2520which%2520serve%2520as%2520the%2520foundation%2520for%2520tasks%2520such%2520as%2520cross-modal%2520retrieval%2520and%2520generation.%2520While%2520prior%2520methods%2520like%2520CAVP%2520effectively%2520model%2520semantic%2520and%2520temporal%2520correspondences%2520between%2520modalities%2520using%2520contrastive%2520objectives%252C%2520their%2520performance%2520remains%2520suboptimal.%2520A%2520key%2520limitation%2520is%2520the%2520insufficient%2520modeling%2520of%2520the%2520dense%252C%2520multi-scale%2520nature%2520of%2520both%2520video%2520and%2520audio%2520signals%252C%2520correspondences%2520often%2520span%2520fine-%2520to%2520coarse-grained%2520spatial-temporal%2520structures%252C%2520which%2520are%2520underutilized%2520in%2520existing%2520frameworks.%2520To%2520this%2520end%252C%2520we%2520propose%2520GMS-CAVP%252C%2520a%2520novel%2520framework%2520that%2520combines%2520Multi-Scale%2520Video-Audio%2520Alignment%2520and%2520Multi-Scale%2520Spatial-Temporal%2520Diffusion-based%2520pretraining%2520objectives%2520to%2520enhance%2520V-A%2520correspondence%2520modeling.%2520First%252C%2520GMS-CAVP%2520introduces%2520a%2520multi-scale%2520contrastive%2520learning%2520strategy%2520that%2520captures%2520semantic%2520and%2520temporal%2520relations%2520across%2520varying%2520granularities.%2520Second%252C%2520we%2520go%2520beyond%2520traditional%2520contrastive%2520learning%2520by%2520incorporating%2520a%2520diffusion-based%2520generative%2520objective%252C%2520enabling%2520modality%2520translation%2520and%2520synthesis%2520between%2520video%2520and%2520audio.%2520This%2520unified%2520discriminative-generative%2520formulation%2520facilitates%2520deeper%2520cross-modal%2520understanding%2520and%2520paves%2520the%2520way%2520for%2520high-fidelity%2520generation.%2520Extensive%2520experiments%2520on%2520VGGSound%252C%2520AudioSet%252C%2520and%2520Panda70M%2520demonstrate%2520that%2520GMS-CAVP%2520outperforms%2520previous%2520methods%2520in%2520generation%2520and%2520retrieval.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMS-CAVP%3A%20Improving%20Audio-Video%20Correspondence%20with%20Multi-Scale%20Contrastive%20and%20Generative%20Pretraining&entry.906535625=Shentong%20Mo%20and%20Zehua%20Chen%20and%20Jun%20Zhu&entry.1292438233=Recent%20advances%20in%20video-audio%20%28V-A%29%20understanding%20and%20generation%20have%20increasingly%20relied%20on%20joint%20V-A%20embeddings%2C%20which%20serve%20as%20the%20foundation%20for%20tasks%20such%20as%20cross-modal%20retrieval%20and%20generation.%20While%20prior%20methods%20like%20CAVP%20effectively%20model%20semantic%20and%20temporal%20correspondences%20between%20modalities%20using%20contrastive%20objectives%2C%20their%20performance%20remains%20suboptimal.%20A%20key%20limitation%20is%20the%20insufficient%20modeling%20of%20the%20dense%2C%20multi-scale%20nature%20of%20both%20video%20and%20audio%20signals%2C%20correspondences%20often%20span%20fine-%20to%20coarse-grained%20spatial-temporal%20structures%2C%20which%20are%20underutilized%20in%20existing%20frameworks.%20To%20this%20end%2C%20we%20propose%20GMS-CAVP%2C%20a%20novel%20framework%20that%20combines%20Multi-Scale%20Video-Audio%20Alignment%20and%20Multi-Scale%20Spatial-Temporal%20Diffusion-based%20pretraining%20objectives%20to%20enhance%20V-A%20correspondence%20modeling.%20First%2C%20GMS-CAVP%20introduces%20a%20multi-scale%20contrastive%20learning%20strategy%20that%20captures%20semantic%20and%20temporal%20relations%20across%20varying%20granularities.%20Second%2C%20we%20go%20beyond%20traditional%20contrastive%20learning%20by%20incorporating%20a%20diffusion-based%20generative%20objective%2C%20enabling%20modality%20translation%20and%20synthesis%20between%20video%20and%20audio.%20This%20unified%20discriminative-generative%20formulation%20facilitates%20deeper%20cross-modal%20understanding%20and%20paves%20the%20way%20for%20high-fidelity%20generation.%20Extensive%20experiments%20on%20VGGSound%2C%20AudioSet%2C%20and%20Panda70M%20demonstrate%20that%20GMS-CAVP%20outperforms%20previous%20methods%20in%20generation%20and%20retrieval.&entry.1838667208=http%3A//arxiv.org/abs/2601.19606v1&entry.124074799=Read"},
{"title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision", "author": "Zhixiang Wei and Yi Li and Zhehan Kan and Xinghua Jiang and Zuwei Long and Shifeng Liu and Hongze Shen and Wei Liu and Xiaoyu Tan and Haojia Lin and Yubo Zhu and Qianyu Li and Di Yin and Haoyu Cao and Weibo Gu and Xin Li and Yinsong Liu and Deqiang Jiang and Xing Sun and Yunsheng Wu and Mingkong Tang and Shuangyin Liu and Lexiang Tang and Haodong Lin and Junru Lu and Jiarui Qin and Lingfeng Qiao and Ruizhi Qiao and Bo Ke and Jianfeng He and Ke Li and Yangning Li and Yunhang Shen and Mengdan Zhang and Peixian Chen and Kun Yin and Bing Liu and Yunfei Wu and Huang Chen and Zhongpeng Cai and Xiaotian Li", "abstract": "Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.", "link": "http://arxiv.org/abs/2601.19798v1", "date": "2026-01-27", "relevancy": 2.8745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Youtu-VL%3A%20Unleashing%20Visual%20Potential%20via%20Unified%20Vision-Language%20Supervision&body=Title%3A%20Youtu-VL%3A%20Unleashing%20Visual%20Potential%20via%20Unified%20Vision-Language%20Supervision%0AAuthor%3A%20Zhixiang%20Wei%20and%20Yi%20Li%20and%20Zhehan%20Kan%20and%20Xinghua%20Jiang%20and%20Zuwei%20Long%20and%20Shifeng%20Liu%20and%20Hongze%20Shen%20and%20Wei%20Liu%20and%20Xiaoyu%20Tan%20and%20Haojia%20Lin%20and%20Yubo%20Zhu%20and%20Qianyu%20Li%20and%20Di%20Yin%20and%20Haoyu%20Cao%20and%20Weibo%20Gu%20and%20Xin%20Li%20and%20Yinsong%20Liu%20and%20Deqiang%20Jiang%20and%20Xing%20Sun%20and%20Yunsheng%20Wu%20and%20Mingkong%20Tang%20and%20Shuangyin%20Liu%20and%20Lexiang%20Tang%20and%20Haodong%20Lin%20and%20Junru%20Lu%20and%20Jiarui%20Qin%20and%20Lingfeng%20Qiao%20and%20Ruizhi%20Qiao%20and%20Bo%20Ke%20and%20Jianfeng%20He%20and%20Ke%20Li%20and%20Yangning%20Li%20and%20Yunhang%20Shen%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Kun%20Yin%20and%20Bing%20Liu%20and%20Yunfei%20Wu%20and%20Huang%20Chen%20and%20Zhongpeng%20Cai%20and%20Xiaotian%20Li%0AAbstract%3A%20Despite%20the%20significant%20advancements%20represented%20by%20Vision-Language%20Models%20%28VLMs%29%2C%20current%20architectures%20often%20exhibit%20limitations%20in%20retaining%20fine-grained%20visual%20information%2C%20leading%20to%20coarse-grained%20multimodal%20comprehension.%20We%20attribute%20this%20deficiency%20to%20a%20suboptimal%20training%20paradigm%20inherent%20in%20prevailing%20VLMs%2C%20which%20exhibits%20a%20text-dominant%20optimization%20bias%20by%20conceptualizing%20visual%20signals%20merely%20as%20passive%20conditional%20inputs%20rather%20than%20supervisory%20targets.%20To%20mitigate%20this%2C%20we%20introduce%20Youtu-VL%2C%20a%20framework%20leveraging%20the%20Vision-Language%20Unified%20Autoregressive%20Supervision%20%28VLUAS%29%20paradigm%2C%20which%20fundamentally%20shifts%20the%20optimization%20objective%20from%20%60%60vision-as-input%27%27%20to%20%60%60vision-as-target.%27%27%20By%20integrating%20visual%20tokens%20directly%20into%20the%20prediction%20stream%2C%20Youtu-VL%20applies%20unified%20autoregressive%20supervision%20to%20both%20visual%20details%20and%20linguistic%20content.%20Furthermore%2C%20we%20extend%20this%20paradigm%20to%20encompass%20vision-centric%20tasks%2C%20enabling%20a%20standard%20VLM%20to%20perform%20vision-centric%20tasks%20without%20task-specific%20additions.%20Extensive%20empirical%20evaluations%20demonstrate%20that%20Youtu-VL%20achieves%20competitive%20performance%20on%20both%20general%20multimodal%20tasks%20and%20vision-centric%20tasks%2C%20establishing%20a%20robust%20foundation%20for%20the%20development%20of%20comprehensive%20generalist%20visual%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYoutu-VL%253A%2520Unleashing%2520Visual%2520Potential%2520via%2520Unified%2520Vision-Language%2520Supervision%26entry.906535625%3DZhixiang%2520Wei%2520and%2520Yi%2520Li%2520and%2520Zhehan%2520Kan%2520and%2520Xinghua%2520Jiang%2520and%2520Zuwei%2520Long%2520and%2520Shifeng%2520Liu%2520and%2520Hongze%2520Shen%2520and%2520Wei%2520Liu%2520and%2520Xiaoyu%2520Tan%2520and%2520Haojia%2520Lin%2520and%2520Yubo%2520Zhu%2520and%2520Qianyu%2520Li%2520and%2520Di%2520Yin%2520and%2520Haoyu%2520Cao%2520and%2520Weibo%2520Gu%2520and%2520Xin%2520Li%2520and%2520Yinsong%2520Liu%2520and%2520Deqiang%2520Jiang%2520and%2520Xing%2520Sun%2520and%2520Yunsheng%2520Wu%2520and%2520Mingkong%2520Tang%2520and%2520Shuangyin%2520Liu%2520and%2520Lexiang%2520Tang%2520and%2520Haodong%2520Lin%2520and%2520Junru%2520Lu%2520and%2520Jiarui%2520Qin%2520and%2520Lingfeng%2520Qiao%2520and%2520Ruizhi%2520Qiao%2520and%2520Bo%2520Ke%2520and%2520Jianfeng%2520He%2520and%2520Ke%2520Li%2520and%2520Yangning%2520Li%2520and%2520Yunhang%2520Shen%2520and%2520Mengdan%2520Zhang%2520and%2520Peixian%2520Chen%2520and%2520Kun%2520Yin%2520and%2520Bing%2520Liu%2520and%2520Yunfei%2520Wu%2520and%2520Huang%2520Chen%2520and%2520Zhongpeng%2520Cai%2520and%2520Xiaotian%2520Li%26entry.1292438233%3DDespite%2520the%2520significant%2520advancements%2520represented%2520by%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520current%2520architectures%2520often%2520exhibit%2520limitations%2520in%2520retaining%2520fine-grained%2520visual%2520information%252C%2520leading%2520to%2520coarse-grained%2520multimodal%2520comprehension.%2520We%2520attribute%2520this%2520deficiency%2520to%2520a%2520suboptimal%2520training%2520paradigm%2520inherent%2520in%2520prevailing%2520VLMs%252C%2520which%2520exhibits%2520a%2520text-dominant%2520optimization%2520bias%2520by%2520conceptualizing%2520visual%2520signals%2520merely%2520as%2520passive%2520conditional%2520inputs%2520rather%2520than%2520supervisory%2520targets.%2520To%2520mitigate%2520this%252C%2520we%2520introduce%2520Youtu-VL%252C%2520a%2520framework%2520leveraging%2520the%2520Vision-Language%2520Unified%2520Autoregressive%2520Supervision%2520%2528VLUAS%2529%2520paradigm%252C%2520which%2520fundamentally%2520shifts%2520the%2520optimization%2520objective%2520from%2520%2560%2560vision-as-input%2527%2527%2520to%2520%2560%2560vision-as-target.%2527%2527%2520By%2520integrating%2520visual%2520tokens%2520directly%2520into%2520the%2520prediction%2520stream%252C%2520Youtu-VL%2520applies%2520unified%2520autoregressive%2520supervision%2520to%2520both%2520visual%2520details%2520and%2520linguistic%2520content.%2520Furthermore%252C%2520we%2520extend%2520this%2520paradigm%2520to%2520encompass%2520vision-centric%2520tasks%252C%2520enabling%2520a%2520standard%2520VLM%2520to%2520perform%2520vision-centric%2520tasks%2520without%2520task-specific%2520additions.%2520Extensive%2520empirical%2520evaluations%2520demonstrate%2520that%2520Youtu-VL%2520achieves%2520competitive%2520performance%2520on%2520both%2520general%2520multimodal%2520tasks%2520and%2520vision-centric%2520tasks%252C%2520establishing%2520a%2520robust%2520foundation%2520for%2520the%2520development%2520of%2520comprehensive%2520generalist%2520visual%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Youtu-VL%3A%20Unleashing%20Visual%20Potential%20via%20Unified%20Vision-Language%20Supervision&entry.906535625=Zhixiang%20Wei%20and%20Yi%20Li%20and%20Zhehan%20Kan%20and%20Xinghua%20Jiang%20and%20Zuwei%20Long%20and%20Shifeng%20Liu%20and%20Hongze%20Shen%20and%20Wei%20Liu%20and%20Xiaoyu%20Tan%20and%20Haojia%20Lin%20and%20Yubo%20Zhu%20and%20Qianyu%20Li%20and%20Di%20Yin%20and%20Haoyu%20Cao%20and%20Weibo%20Gu%20and%20Xin%20Li%20and%20Yinsong%20Liu%20and%20Deqiang%20Jiang%20and%20Xing%20Sun%20and%20Yunsheng%20Wu%20and%20Mingkong%20Tang%20and%20Shuangyin%20Liu%20and%20Lexiang%20Tang%20and%20Haodong%20Lin%20and%20Junru%20Lu%20and%20Jiarui%20Qin%20and%20Lingfeng%20Qiao%20and%20Ruizhi%20Qiao%20and%20Bo%20Ke%20and%20Jianfeng%20He%20and%20Ke%20Li%20and%20Yangning%20Li%20and%20Yunhang%20Shen%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Kun%20Yin%20and%20Bing%20Liu%20and%20Yunfei%20Wu%20and%20Huang%20Chen%20and%20Zhongpeng%20Cai%20and%20Xiaotian%20Li&entry.1292438233=Despite%20the%20significant%20advancements%20represented%20by%20Vision-Language%20Models%20%28VLMs%29%2C%20current%20architectures%20often%20exhibit%20limitations%20in%20retaining%20fine-grained%20visual%20information%2C%20leading%20to%20coarse-grained%20multimodal%20comprehension.%20We%20attribute%20this%20deficiency%20to%20a%20suboptimal%20training%20paradigm%20inherent%20in%20prevailing%20VLMs%2C%20which%20exhibits%20a%20text-dominant%20optimization%20bias%20by%20conceptualizing%20visual%20signals%20merely%20as%20passive%20conditional%20inputs%20rather%20than%20supervisory%20targets.%20To%20mitigate%20this%2C%20we%20introduce%20Youtu-VL%2C%20a%20framework%20leveraging%20the%20Vision-Language%20Unified%20Autoregressive%20Supervision%20%28VLUAS%29%20paradigm%2C%20which%20fundamentally%20shifts%20the%20optimization%20objective%20from%20%60%60vision-as-input%27%27%20to%20%60%60vision-as-target.%27%27%20By%20integrating%20visual%20tokens%20directly%20into%20the%20prediction%20stream%2C%20Youtu-VL%20applies%20unified%20autoregressive%20supervision%20to%20both%20visual%20details%20and%20linguistic%20content.%20Furthermore%2C%20we%20extend%20this%20paradigm%20to%20encompass%20vision-centric%20tasks%2C%20enabling%20a%20standard%20VLM%20to%20perform%20vision-centric%20tasks%20without%20task-specific%20additions.%20Extensive%20empirical%20evaluations%20demonstrate%20that%20Youtu-VL%20achieves%20competitive%20performance%20on%20both%20general%20multimodal%20tasks%20and%20vision-centric%20tasks%2C%20establishing%20a%20robust%20foundation%20for%20the%20development%20of%20comprehensive%20generalist%20visual%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.19798v1&entry.124074799=Read"},
{"title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning", "author": "Binzhu Xie and Shi Qiu and Sicheng Zhang and Yinqiao Wang and Hao Xu and Muzammal Naseer and Chi-Wing Fu and Pheng-Ann Heng", "abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL", "link": "http://arxiv.org/abs/2601.19850v1", "date": "2026-01-27", "relevancy": 2.8049, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5892}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.548}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoHandICL%3A%20Egocentric%203D%20Hand%20Reconstruction%20with%20In-Context%20Learning&body=Title%3A%20EgoHandICL%3A%20Egocentric%203D%20Hand%20Reconstruction%20with%20In-Context%20Learning%0AAuthor%3A%20Binzhu%20Xie%20and%20Shi%20Qiu%20and%20Sicheng%20Zhang%20and%20Yinqiao%20Wang%20and%20Hao%20Xu%20and%20Muzammal%20Naseer%20and%20Chi-Wing%20Fu%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20Robust%203D%20hand%20reconstruction%20in%20egocentric%20vision%20is%20challenging%20due%20to%20depth%20ambiguity%2C%20self-occlusion%2C%20and%20complex%20hand-object%20interactions.%20Prior%20methods%20mitigate%20these%20issues%20by%20scaling%20training%20data%20or%20adding%20auxiliary%20cues%2C%20but%20they%20often%20struggle%20in%20unseen%20contexts.%20We%20present%20EgoHandICL%2C%20the%20first%20in-context%20learning%20%28ICL%29%20framework%20for%203D%20hand%20reconstruction%20that%20improves%20semantic%20alignment%2C%20visual%20consistency%2C%20and%20robustness%20under%20challenging%20egocentric%20conditions.%20EgoHandICL%20introduces%20complementary%20exemplar%20retrieval%20guided%20by%20vision-language%20models%20%28VLMs%29%2C%20an%20ICL-tailored%20tokenizer%20for%20multimodal%20context%2C%20and%20a%20masked%20autoencoder%20%28MAE%29-based%20architecture%20trained%20with%20hand-guided%20geometric%20and%20perceptual%20objectives.%20Experiments%20on%20ARCTIC%20and%20EgoExo4D%20show%20consistent%20gains%20over%20state-of-the-art%20methods.%20We%20also%20demonstrate%20real-world%20generalization%20and%20improve%20EgoVLM%20hand-object%20interaction%20reasoning%20by%20using%20reconstructed%20hands%20as%20visual%20prompts.%20Code%20and%20data%3A%20https%3A//github.com/Nicous20/EgoHandICL%0ALink%3A%20http%3A//arxiv.org/abs/2601.19850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoHandICL%253A%2520Egocentric%25203D%2520Hand%2520Reconstruction%2520with%2520In-Context%2520Learning%26entry.906535625%3DBinzhu%2520Xie%2520and%2520Shi%2520Qiu%2520and%2520Sicheng%2520Zhang%2520and%2520Yinqiao%2520Wang%2520and%2520Hao%2520Xu%2520and%2520Muzammal%2520Naseer%2520and%2520Chi-Wing%2520Fu%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3DRobust%25203D%2520hand%2520reconstruction%2520in%2520egocentric%2520vision%2520is%2520challenging%2520due%2520to%2520depth%2520ambiguity%252C%2520self-occlusion%252C%2520and%2520complex%2520hand-object%2520interactions.%2520Prior%2520methods%2520mitigate%2520these%2520issues%2520by%2520scaling%2520training%2520data%2520or%2520adding%2520auxiliary%2520cues%252C%2520but%2520they%2520often%2520struggle%2520in%2520unseen%2520contexts.%2520We%2520present%2520EgoHandICL%252C%2520the%2520first%2520in-context%2520learning%2520%2528ICL%2529%2520framework%2520for%25203D%2520hand%2520reconstruction%2520that%2520improves%2520semantic%2520alignment%252C%2520visual%2520consistency%252C%2520and%2520robustness%2520under%2520challenging%2520egocentric%2520conditions.%2520EgoHandICL%2520introduces%2520complementary%2520exemplar%2520retrieval%2520guided%2520by%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520an%2520ICL-tailored%2520tokenizer%2520for%2520multimodal%2520context%252C%2520and%2520a%2520masked%2520autoencoder%2520%2528MAE%2529-based%2520architecture%2520trained%2520with%2520hand-guided%2520geometric%2520and%2520perceptual%2520objectives.%2520Experiments%2520on%2520ARCTIC%2520and%2520EgoExo4D%2520show%2520consistent%2520gains%2520over%2520state-of-the-art%2520methods.%2520We%2520also%2520demonstrate%2520real-world%2520generalization%2520and%2520improve%2520EgoVLM%2520hand-object%2520interaction%2520reasoning%2520by%2520using%2520reconstructed%2520hands%2520as%2520visual%2520prompts.%2520Code%2520and%2520data%253A%2520https%253A//github.com/Nicous20/EgoHandICL%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoHandICL%3A%20Egocentric%203D%20Hand%20Reconstruction%20with%20In-Context%20Learning&entry.906535625=Binzhu%20Xie%20and%20Shi%20Qiu%20and%20Sicheng%20Zhang%20and%20Yinqiao%20Wang%20and%20Hao%20Xu%20and%20Muzammal%20Naseer%20and%20Chi-Wing%20Fu%20and%20Pheng-Ann%20Heng&entry.1292438233=Robust%203D%20hand%20reconstruction%20in%20egocentric%20vision%20is%20challenging%20due%20to%20depth%20ambiguity%2C%20self-occlusion%2C%20and%20complex%20hand-object%20interactions.%20Prior%20methods%20mitigate%20these%20issues%20by%20scaling%20training%20data%20or%20adding%20auxiliary%20cues%2C%20but%20they%20often%20struggle%20in%20unseen%20contexts.%20We%20present%20EgoHandICL%2C%20the%20first%20in-context%20learning%20%28ICL%29%20framework%20for%203D%20hand%20reconstruction%20that%20improves%20semantic%20alignment%2C%20visual%20consistency%2C%20and%20robustness%20under%20challenging%20egocentric%20conditions.%20EgoHandICL%20introduces%20complementary%20exemplar%20retrieval%20guided%20by%20vision-language%20models%20%28VLMs%29%2C%20an%20ICL-tailored%20tokenizer%20for%20multimodal%20context%2C%20and%20a%20masked%20autoencoder%20%28MAE%29-based%20architecture%20trained%20with%20hand-guided%20geometric%20and%20perceptual%20objectives.%20Experiments%20on%20ARCTIC%20and%20EgoExo4D%20show%20consistent%20gains%20over%20state-of-the-art%20methods.%20We%20also%20demonstrate%20real-world%20generalization%20and%20improve%20EgoVLM%20hand-object%20interaction%20reasoning%20by%20using%20reconstructed%20hands%20as%20visual%20prompts.%20Code%20and%20data%3A%20https%3A//github.com/Nicous20/EgoHandICL&entry.1838667208=http%3A//arxiv.org/abs/2601.19850v1&entry.124074799=Read"},
{"title": "Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries", "author": "Kevin Robbins and Xiaotong Liu and Yu Wu and Le Sun and Grady McPeak and Abby Stylianou and Robert Pless", "abstract": "Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.", "link": "http://arxiv.org/abs/2601.17535v2", "date": "2026-01-27", "relevancy": 2.7765, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Will%20It%20Zero-Shot%3F%3A%20Predicting%20Zero-Shot%20Classification%20Performance%20For%20Arbitrary%20Queries&body=Title%3A%20Will%20It%20Zero-Shot%3F%3A%20Predicting%20Zero-Shot%20Classification%20Performance%20For%20Arbitrary%20Queries%0AAuthor%3A%20Kevin%20Robbins%20and%20Xiaotong%20Liu%20and%20Yu%20Wu%20and%20Le%20Sun%20and%20Grady%20McPeak%20and%20Abby%20Stylianou%20and%20Robert%20Pless%0AAbstract%3A%20Vision-Language%20Models%20like%20CLIP%20create%20aligned%20embedding%20spaces%20for%20text%20and%20images%2C%20making%20it%20possible%20for%20anyone%20to%20build%20a%20visual%20classifier%20by%20simply%20naming%20the%20classes%20they%20want%20to%20distinguish.%20However%2C%20a%20model%20that%20works%20well%20in%20one%20domain%20may%20fail%20in%20another%2C%20and%20non-expert%20users%20have%20no%20straightforward%20way%20to%20assess%20whether%20their%20chosen%20VLM%20will%20work%20on%20their%20problem.%20We%20build%20on%20prior%20work%20using%20text-only%20comparisons%20to%20evaluate%20how%20well%20a%20model%20works%20for%20a%20given%20natural%20language%20task%2C%20and%20explore%20approaches%20that%20also%20generate%20synthetic%20images%20relevant%20to%20that%20task%20to%20evaluate%20and%20refine%20the%20prediction%20of%20zero-shot%20accuracy.%20We%20show%20that%20generated%20imagery%20to%20the%20baseline%20text-only%20scores%20substantially%20improves%20the%20quality%20of%20these%20predictions.%20Additionally%2C%20it%20gives%20a%20user%20feedback%20on%20the%20kinds%20of%20images%20that%20were%20used%20to%20make%20the%20assessment.%20Experiments%20on%20standard%20CLIP%20benchmark%20datasets%20demonstrate%20that%20the%20image-based%20approach%20helps%20users%20predict%2C%20without%20any%20labeled%20examples%2C%20whether%20a%20VLM%20will%20be%20effective%20for%20their%20application.%0ALink%3A%20http%3A//arxiv.org/abs/2601.17535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWill%2520It%2520Zero-Shot%253F%253A%2520Predicting%2520Zero-Shot%2520Classification%2520Performance%2520For%2520Arbitrary%2520Queries%26entry.906535625%3DKevin%2520Robbins%2520and%2520Xiaotong%2520Liu%2520and%2520Yu%2520Wu%2520and%2520Le%2520Sun%2520and%2520Grady%2520McPeak%2520and%2520Abby%2520Stylianou%2520and%2520Robert%2520Pless%26entry.1292438233%3DVision-Language%2520Models%2520like%2520CLIP%2520create%2520aligned%2520embedding%2520spaces%2520for%2520text%2520and%2520images%252C%2520making%2520it%2520possible%2520for%2520anyone%2520to%2520build%2520a%2520visual%2520classifier%2520by%2520simply%2520naming%2520the%2520classes%2520they%2520want%2520to%2520distinguish.%2520However%252C%2520a%2520model%2520that%2520works%2520well%2520in%2520one%2520domain%2520may%2520fail%2520in%2520another%252C%2520and%2520non-expert%2520users%2520have%2520no%2520straightforward%2520way%2520to%2520assess%2520whether%2520their%2520chosen%2520VLM%2520will%2520work%2520on%2520their%2520problem.%2520We%2520build%2520on%2520prior%2520work%2520using%2520text-only%2520comparisons%2520to%2520evaluate%2520how%2520well%2520a%2520model%2520works%2520for%2520a%2520given%2520natural%2520language%2520task%252C%2520and%2520explore%2520approaches%2520that%2520also%2520generate%2520synthetic%2520images%2520relevant%2520to%2520that%2520task%2520to%2520evaluate%2520and%2520refine%2520the%2520prediction%2520of%2520zero-shot%2520accuracy.%2520We%2520show%2520that%2520generated%2520imagery%2520to%2520the%2520baseline%2520text-only%2520scores%2520substantially%2520improves%2520the%2520quality%2520of%2520these%2520predictions.%2520Additionally%252C%2520it%2520gives%2520a%2520user%2520feedback%2520on%2520the%2520kinds%2520of%2520images%2520that%2520were%2520used%2520to%2520make%2520the%2520assessment.%2520Experiments%2520on%2520standard%2520CLIP%2520benchmark%2520datasets%2520demonstrate%2520that%2520the%2520image-based%2520approach%2520helps%2520users%2520predict%252C%2520without%2520any%2520labeled%2520examples%252C%2520whether%2520a%2520VLM%2520will%2520be%2520effective%2520for%2520their%2520application.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Will%20It%20Zero-Shot%3F%3A%20Predicting%20Zero-Shot%20Classification%20Performance%20For%20Arbitrary%20Queries&entry.906535625=Kevin%20Robbins%20and%20Xiaotong%20Liu%20and%20Yu%20Wu%20and%20Le%20Sun%20and%20Grady%20McPeak%20and%20Abby%20Stylianou%20and%20Robert%20Pless&entry.1292438233=Vision-Language%20Models%20like%20CLIP%20create%20aligned%20embedding%20spaces%20for%20text%20and%20images%2C%20making%20it%20possible%20for%20anyone%20to%20build%20a%20visual%20classifier%20by%20simply%20naming%20the%20classes%20they%20want%20to%20distinguish.%20However%2C%20a%20model%20that%20works%20well%20in%20one%20domain%20may%20fail%20in%20another%2C%20and%20non-expert%20users%20have%20no%20straightforward%20way%20to%20assess%20whether%20their%20chosen%20VLM%20will%20work%20on%20their%20problem.%20We%20build%20on%20prior%20work%20using%20text-only%20comparisons%20to%20evaluate%20how%20well%20a%20model%20works%20for%20a%20given%20natural%20language%20task%2C%20and%20explore%20approaches%20that%20also%20generate%20synthetic%20images%20relevant%20to%20that%20task%20to%20evaluate%20and%20refine%20the%20prediction%20of%20zero-shot%20accuracy.%20We%20show%20that%20generated%20imagery%20to%20the%20baseline%20text-only%20scores%20substantially%20improves%20the%20quality%20of%20these%20predictions.%20Additionally%2C%20it%20gives%20a%20user%20feedback%20on%20the%20kinds%20of%20images%20that%20were%20used%20to%20make%20the%20assessment.%20Experiments%20on%20standard%20CLIP%20benchmark%20datasets%20demonstrate%20that%20the%20image-based%20approach%20helps%20users%20predict%2C%20without%20any%20labeled%20examples%2C%20whether%20a%20VLM%20will%20be%20effective%20for%20their%20application.&entry.1838667208=http%3A//arxiv.org/abs/2601.17535v2&entry.124074799=Read"},
{"title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool", "author": "Tulsi Patel and Mark W. Jones and Thomas Redfern", "abstract": "Machine learning for remote sensing imaging relies on up-to-date and accurate labels for model training and testing. Labelling remote sensing imagery is time and cost intensive, requiring expert analysis. Previous labelling tools rely on pre-labelled data for training in order to label new unseen data. In this work, we define an unsupervised pipeline for finding and labelling geographical areas of similar context and content within Sentinel-2 satellite imagery. Our approach removes limitations of previous methods by utilising segmentation with convolutional and graph neural networks to encode a more robust feature space for image comparison. Unlike previous approaches we segment the image into homogeneous regions of pixels that are grouped based on colour and spatial similarity. Graph neural networks are used to aggregate information about the surrounding segments enabling the feature representation to encode the local neighbourhood whilst preserving its own local information. This reduces outliers in the labelling tool, allows users to label at a granular level, and allows a rotationally invariant semantic relationship at the image level to be formed within the encoding space. Our pipeline achieves high contextual consistency, with similarity scores of SSIM = 0.96 and SAM = 0.21 under context-aware evaluation, demonstrating robust organisation of the feature space for interactive labelling.", "link": "http://arxiv.org/abs/2508.00506v2", "date": "2026-01-27", "relevancy": 2.772, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5877}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5531}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Convolutional%20and%20Graph%20Networks%20for%20an%20Unsupervised%20Remote%20Sensing%20Labelling%20Tool&body=Title%3A%20Leveraging%20Convolutional%20and%20Graph%20Networks%20for%20an%20Unsupervised%20Remote%20Sensing%20Labelling%20Tool%0AAuthor%3A%20Tulsi%20Patel%20and%20Mark%20W.%20Jones%20and%20Thomas%20Redfern%0AAbstract%3A%20Machine%20learning%20for%20remote%20sensing%20imaging%20relies%20on%20up-to-date%20and%20accurate%20labels%20for%20model%20training%20and%20testing.%20Labelling%20remote%20sensing%20imagery%20is%20time%20and%20cost%20intensive%2C%20requiring%20expert%20analysis.%20Previous%20labelling%20tools%20rely%20on%20pre-labelled%20data%20for%20training%20in%20order%20to%20label%20new%20unseen%20data.%20In%20this%20work%2C%20we%20define%20an%20unsupervised%20pipeline%20for%20finding%20and%20labelling%20geographical%20areas%20of%20similar%20context%20and%20content%20within%20Sentinel-2%20satellite%20imagery.%20Our%20approach%20removes%20limitations%20of%20previous%20methods%20by%20utilising%20segmentation%20with%20convolutional%20and%20graph%20neural%20networks%20to%20encode%20a%20more%20robust%20feature%20space%20for%20image%20comparison.%20Unlike%20previous%20approaches%20we%20segment%20the%20image%20into%20homogeneous%20regions%20of%20pixels%20that%20are%20grouped%20based%20on%20colour%20and%20spatial%20similarity.%20Graph%20neural%20networks%20are%20used%20to%20aggregate%20information%20about%20the%20surrounding%20segments%20enabling%20the%20feature%20representation%20to%20encode%20the%20local%20neighbourhood%20whilst%20preserving%20its%20own%20local%20information.%20This%20reduces%20outliers%20in%20the%20labelling%20tool%2C%20allows%20users%20to%20label%20at%20a%20granular%20level%2C%20and%20allows%20a%20rotationally%20invariant%20semantic%20relationship%20at%20the%20image%20level%20to%20be%20formed%20within%20the%20encoding%20space.%20Our%20pipeline%20achieves%20high%20contextual%20consistency%2C%20with%20similarity%20scores%20of%20SSIM%20%3D%200.96%20and%20SAM%20%3D%200.21%20under%20context-aware%20evaluation%2C%20demonstrating%20robust%20organisation%20of%20the%20feature%20space%20for%20interactive%20labelling.%0ALink%3A%20http%3A//arxiv.org/abs/2508.00506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Convolutional%2520and%2520Graph%2520Networks%2520for%2520an%2520Unsupervised%2520Remote%2520Sensing%2520Labelling%2520Tool%26entry.906535625%3DTulsi%2520Patel%2520and%2520Mark%2520W.%2520Jones%2520and%2520Thomas%2520Redfern%26entry.1292438233%3DMachine%2520learning%2520for%2520remote%2520sensing%2520imaging%2520relies%2520on%2520up-to-date%2520and%2520accurate%2520labels%2520for%2520model%2520training%2520and%2520testing.%2520Labelling%2520remote%2520sensing%2520imagery%2520is%2520time%2520and%2520cost%2520intensive%252C%2520requiring%2520expert%2520analysis.%2520Previous%2520labelling%2520tools%2520rely%2520on%2520pre-labelled%2520data%2520for%2520training%2520in%2520order%2520to%2520label%2520new%2520unseen%2520data.%2520In%2520this%2520work%252C%2520we%2520define%2520an%2520unsupervised%2520pipeline%2520for%2520finding%2520and%2520labelling%2520geographical%2520areas%2520of%2520similar%2520context%2520and%2520content%2520within%2520Sentinel-2%2520satellite%2520imagery.%2520Our%2520approach%2520removes%2520limitations%2520of%2520previous%2520methods%2520by%2520utilising%2520segmentation%2520with%2520convolutional%2520and%2520graph%2520neural%2520networks%2520to%2520encode%2520a%2520more%2520robust%2520feature%2520space%2520for%2520image%2520comparison.%2520Unlike%2520previous%2520approaches%2520we%2520segment%2520the%2520image%2520into%2520homogeneous%2520regions%2520of%2520pixels%2520that%2520are%2520grouped%2520based%2520on%2520colour%2520and%2520spatial%2520similarity.%2520Graph%2520neural%2520networks%2520are%2520used%2520to%2520aggregate%2520information%2520about%2520the%2520surrounding%2520segments%2520enabling%2520the%2520feature%2520representation%2520to%2520encode%2520the%2520local%2520neighbourhood%2520whilst%2520preserving%2520its%2520own%2520local%2520information.%2520This%2520reduces%2520outliers%2520in%2520the%2520labelling%2520tool%252C%2520allows%2520users%2520to%2520label%2520at%2520a%2520granular%2520level%252C%2520and%2520allows%2520a%2520rotationally%2520invariant%2520semantic%2520relationship%2520at%2520the%2520image%2520level%2520to%2520be%2520formed%2520within%2520the%2520encoding%2520space.%2520Our%2520pipeline%2520achieves%2520high%2520contextual%2520consistency%252C%2520with%2520similarity%2520scores%2520of%2520SSIM%2520%253D%25200.96%2520and%2520SAM%2520%253D%25200.21%2520under%2520context-aware%2520evaluation%252C%2520demonstrating%2520robust%2520organisation%2520of%2520the%2520feature%2520space%2520for%2520interactive%2520labelling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Convolutional%20and%20Graph%20Networks%20for%20an%20Unsupervised%20Remote%20Sensing%20Labelling%20Tool&entry.906535625=Tulsi%20Patel%20and%20Mark%20W.%20Jones%20and%20Thomas%20Redfern&entry.1292438233=Machine%20learning%20for%20remote%20sensing%20imaging%20relies%20on%20up-to-date%20and%20accurate%20labels%20for%20model%20training%20and%20testing.%20Labelling%20remote%20sensing%20imagery%20is%20time%20and%20cost%20intensive%2C%20requiring%20expert%20analysis.%20Previous%20labelling%20tools%20rely%20on%20pre-labelled%20data%20for%20training%20in%20order%20to%20label%20new%20unseen%20data.%20In%20this%20work%2C%20we%20define%20an%20unsupervised%20pipeline%20for%20finding%20and%20labelling%20geographical%20areas%20of%20similar%20context%20and%20content%20within%20Sentinel-2%20satellite%20imagery.%20Our%20approach%20removes%20limitations%20of%20previous%20methods%20by%20utilising%20segmentation%20with%20convolutional%20and%20graph%20neural%20networks%20to%20encode%20a%20more%20robust%20feature%20space%20for%20image%20comparison.%20Unlike%20previous%20approaches%20we%20segment%20the%20image%20into%20homogeneous%20regions%20of%20pixels%20that%20are%20grouped%20based%20on%20colour%20and%20spatial%20similarity.%20Graph%20neural%20networks%20are%20used%20to%20aggregate%20information%20about%20the%20surrounding%20segments%20enabling%20the%20feature%20representation%20to%20encode%20the%20local%20neighbourhood%20whilst%20preserving%20its%20own%20local%20information.%20This%20reduces%20outliers%20in%20the%20labelling%20tool%2C%20allows%20users%20to%20label%20at%20a%20granular%20level%2C%20and%20allows%20a%20rotationally%20invariant%20semantic%20relationship%20at%20the%20image%20level%20to%20be%20formed%20within%20the%20encoding%20space.%20Our%20pipeline%20achieves%20high%20contextual%20consistency%2C%20with%20similarity%20scores%20of%20SSIM%20%3D%200.96%20and%20SAM%20%3D%200.21%20under%20context-aware%20evaluation%2C%20demonstrating%20robust%20organisation%20of%20the%20feature%20space%20for%20interactive%20labelling.&entry.1838667208=http%3A//arxiv.org/abs/2508.00506v2&entry.124074799=Read"},
{"title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals", "author": "Octavio Pappalardo", "abstract": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent's post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent's capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.", "link": "http://arxiv.org/abs/2601.19810v1", "date": "2026-01-27", "relevancy": 2.7664, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.575}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5467}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%20of%20Efficient%20Exploration%3A%20Pre-training%20Adaptive%20Policies%20via%20Self-Imposed%20Goals&body=Title%3A%20Unsupervised%20Learning%20of%20Efficient%20Exploration%3A%20Pre-training%20Adaptive%20Policies%20via%20Self-Imposed%20Goals%0AAuthor%3A%20Octavio%20Pappalardo%0AAbstract%3A%20Unsupervised%20pre-training%20can%20equip%20reinforcement%20learning%20agents%20with%20prior%20knowledge%20and%20accelerate%20learning%20in%20downstream%20tasks.%20A%20promising%20direction%2C%20grounded%20in%20human%20development%2C%20investigates%20agents%20that%20learn%20by%20setting%20and%20pursuing%20their%20own%20goals.%20The%20core%20challenge%20lies%20in%20how%20to%20effectively%20generate%2C%20select%2C%20and%20learn%20from%20such%20goals.%20Our%20focus%20is%20on%20broad%20distributions%20of%20downstream%20tasks%20where%20solving%20every%20task%20zero-shot%20is%20infeasible.%20Such%20settings%20naturally%20arise%20when%20the%20target%20tasks%20lie%20outside%20of%20the%20pre-training%20distribution%20or%20when%20their%20identities%20are%20unknown%20to%20the%20agent.%20In%20this%20work%2C%20we%20%28i%29%20optimize%20for%20efficient%20multi-episode%20exploration%20and%20adaptation%20within%20a%20meta-learning%20framework%2C%20and%20%28ii%29%20guide%20the%20training%20curriculum%20with%20evolving%20estimates%20of%20the%20agent%27s%20post-adaptation%20performance.%20We%20present%20ULEE%2C%20an%20unsupervised%20meta-learning%20method%20that%20combines%20an%20in-context%20learner%20with%20an%20adversarial%20goal-generation%20strategy%20that%20maintains%20training%20at%20the%20frontier%20of%20the%20agent%27s%20capabilities.%20On%20XLand-MiniGrid%20benchmarks%2C%20ULEE%20pre-training%20yields%20improved%20exploration%20and%20adaptation%20abilities%20that%20generalize%20to%20novel%20objectives%2C%20environment%20dynamics%2C%20and%20map%20structures.%20The%20resulting%20policy%20attains%20improved%20zero-shot%20and%20few-shot%20performance%2C%20and%20provides%20a%20strong%20initialization%20for%20longer%20fine-tuning%20processes.%20It%20outperforms%20learning%20from%20scratch%2C%20DIAYN%20pre-training%2C%20and%20alternative%20curricula.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Learning%2520of%2520Efficient%2520Exploration%253A%2520Pre-training%2520Adaptive%2520Policies%2520via%2520Self-Imposed%2520Goals%26entry.906535625%3DOctavio%2520Pappalardo%26entry.1292438233%3DUnsupervised%2520pre-training%2520can%2520equip%2520reinforcement%2520learning%2520agents%2520with%2520prior%2520knowledge%2520and%2520accelerate%2520learning%2520in%2520downstream%2520tasks.%2520A%2520promising%2520direction%252C%2520grounded%2520in%2520human%2520development%252C%2520investigates%2520agents%2520that%2520learn%2520by%2520setting%2520and%2520pursuing%2520their%2520own%2520goals.%2520The%2520core%2520challenge%2520lies%2520in%2520how%2520to%2520effectively%2520generate%252C%2520select%252C%2520and%2520learn%2520from%2520such%2520goals.%2520Our%2520focus%2520is%2520on%2520broad%2520distributions%2520of%2520downstream%2520tasks%2520where%2520solving%2520every%2520task%2520zero-shot%2520is%2520infeasible.%2520Such%2520settings%2520naturally%2520arise%2520when%2520the%2520target%2520tasks%2520lie%2520outside%2520of%2520the%2520pre-training%2520distribution%2520or%2520when%2520their%2520identities%2520are%2520unknown%2520to%2520the%2520agent.%2520In%2520this%2520work%252C%2520we%2520%2528i%2529%2520optimize%2520for%2520efficient%2520multi-episode%2520exploration%2520and%2520adaptation%2520within%2520a%2520meta-learning%2520framework%252C%2520and%2520%2528ii%2529%2520guide%2520the%2520training%2520curriculum%2520with%2520evolving%2520estimates%2520of%2520the%2520agent%2527s%2520post-adaptation%2520performance.%2520We%2520present%2520ULEE%252C%2520an%2520unsupervised%2520meta-learning%2520method%2520that%2520combines%2520an%2520in-context%2520learner%2520with%2520an%2520adversarial%2520goal-generation%2520strategy%2520that%2520maintains%2520training%2520at%2520the%2520frontier%2520of%2520the%2520agent%2527s%2520capabilities.%2520On%2520XLand-MiniGrid%2520benchmarks%252C%2520ULEE%2520pre-training%2520yields%2520improved%2520exploration%2520and%2520adaptation%2520abilities%2520that%2520generalize%2520to%2520novel%2520objectives%252C%2520environment%2520dynamics%252C%2520and%2520map%2520structures.%2520The%2520resulting%2520policy%2520attains%2520improved%2520zero-shot%2520and%2520few-shot%2520performance%252C%2520and%2520provides%2520a%2520strong%2520initialization%2520for%2520longer%2520fine-tuning%2520processes.%2520It%2520outperforms%2520learning%2520from%2520scratch%252C%2520DIAYN%2520pre-training%252C%2520and%2520alternative%2520curricula.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%20of%20Efficient%20Exploration%3A%20Pre-training%20Adaptive%20Policies%20via%20Self-Imposed%20Goals&entry.906535625=Octavio%20Pappalardo&entry.1292438233=Unsupervised%20pre-training%20can%20equip%20reinforcement%20learning%20agents%20with%20prior%20knowledge%20and%20accelerate%20learning%20in%20downstream%20tasks.%20A%20promising%20direction%2C%20grounded%20in%20human%20development%2C%20investigates%20agents%20that%20learn%20by%20setting%20and%20pursuing%20their%20own%20goals.%20The%20core%20challenge%20lies%20in%20how%20to%20effectively%20generate%2C%20select%2C%20and%20learn%20from%20such%20goals.%20Our%20focus%20is%20on%20broad%20distributions%20of%20downstream%20tasks%20where%20solving%20every%20task%20zero-shot%20is%20infeasible.%20Such%20settings%20naturally%20arise%20when%20the%20target%20tasks%20lie%20outside%20of%20the%20pre-training%20distribution%20or%20when%20their%20identities%20are%20unknown%20to%20the%20agent.%20In%20this%20work%2C%20we%20%28i%29%20optimize%20for%20efficient%20multi-episode%20exploration%20and%20adaptation%20within%20a%20meta-learning%20framework%2C%20and%20%28ii%29%20guide%20the%20training%20curriculum%20with%20evolving%20estimates%20of%20the%20agent%27s%20post-adaptation%20performance.%20We%20present%20ULEE%2C%20an%20unsupervised%20meta-learning%20method%20that%20combines%20an%20in-context%20learner%20with%20an%20adversarial%20goal-generation%20strategy%20that%20maintains%20training%20at%20the%20frontier%20of%20the%20agent%27s%20capabilities.%20On%20XLand-MiniGrid%20benchmarks%2C%20ULEE%20pre-training%20yields%20improved%20exploration%20and%20adaptation%20abilities%20that%20generalize%20to%20novel%20objectives%2C%20environment%20dynamics%2C%20and%20map%20structures.%20The%20resulting%20policy%20attains%20improved%20zero-shot%20and%20few-shot%20performance%2C%20and%20provides%20a%20strong%20initialization%20for%20longer%20fine-tuning%20processes.%20It%20outperforms%20learning%20from%20scratch%2C%20DIAYN%20pre-training%2C%20and%20alternative%20curricula.&entry.1838667208=http%3A//arxiv.org/abs/2601.19810v1&entry.124074799=Read"},
{"title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "author": "Ji Zeng and Dayuan Fu and Tiantian Mi and Yumin Zhuang and Yaxing Huang and Xuefeng Li and Lyumanshan Ye and Muhang Xie and Qishuo Hua and Zhen Huang and Mohan Jiang and Hanning Wang and Jifan Lin and Yang Xiao and Jie Sun and Yunze Wu and Pengfei Liu", "abstract": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "link": "http://arxiv.org/abs/2601.18418v2", "date": "2026-01-27", "relevancy": 2.7368, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6156}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5217}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20daVinci-Dev%3A%20Agent-native%20Mid-training%20for%20Software%20Engineering&body=Title%3A%20daVinci-Dev%3A%20Agent-native%20Mid-training%20for%20Software%20Engineering%0AAuthor%3A%20Ji%20Zeng%20and%20Dayuan%20Fu%20and%20Tiantian%20Mi%20and%20Yumin%20Zhuang%20and%20Yaxing%20Huang%20and%20Xuefeng%20Li%20and%20Lyumanshan%20Ye%20and%20Muhang%20Xie%20and%20Qishuo%20Hua%20and%20Zhen%20Huang%20and%20Mohan%20Jiang%20and%20Hanning%20Wang%20and%20Jifan%20Lin%20and%20Yang%20Xiao%20and%20Jie%20Sun%20and%20Yunze%20Wu%20and%20Pengfei%20Liu%0AAbstract%3A%20Recently%2C%20the%20frontier%20of%20Large%20Language%20Model%20%28LLM%29%20capabilities%20has%20shifted%20from%20single-turn%20code%20generation%20to%20agentic%20software%20engineering-a%20paradigm%20where%20models%20autonomously%20navigate%2C%20edit%2C%20and%20test%20complex%20repositories.%20While%20post-training%20methods%20have%20become%20the%20de%20facto%20approach%20for%20code%20agents%2C%20%2A%2Aagentic%20mid-training%2A%2A-mid-training%20%28MT%29%20on%20large-scale%20data%20that%20mirrors%20authentic%20agentic%20workflows-remains%20critically%20underexplored%20due%20to%20substantial%20resource%20requirements%2C%20despite%20offering%20a%20more%20scalable%20path%20to%20instilling%20foundational%20agentic%20behaviors%20than%20relying%20solely%20on%20expensive%20reinforcement%20learning.%20A%20central%20challenge%20in%20realizing%20effective%20agentic%20mid-training%20is%20the%20distribution%20mismatch%20between%20static%20training%20data%20and%20the%20dynamic%2C%20feedback-rich%20environment%20of%20real%20development.%20To%20address%20this%2C%20we%20present%20a%20systematic%20study%20of%20agentic%20mid-training%2C%20establishing%20both%20the%20data%20synthesis%20principles%20and%20training%20methodology%20for%20effective%20agent%20development%20at%20scale.%20Central%20to%20our%20approach%20is%20%2A%2Aagent-native%20data%2A%2A-supervision%20comprising%20two%20complementary%20types%20of%20trajectories%3A%20%2A%2Acontextually-native%20trajectories%2A%2A%20that%20preserve%20the%20complete%20information%20flow%20an%20agent%20experiences%2C%20offering%20broad%20coverage%20and%20diversity%3B%20and%20%2A%2Aenvironmentally-native%20trajectories%2A%2A%20collected%20from%20executable%20repositories%20where%20observations%20stem%20from%20actual%20tool%20invocations%20and%20test%20executions%2C%20providing%20depth%20and%20interaction%20authenticity.%20We%20verify%20the%20model%27s%20agentic%20capabilities%20on%20%60SWE-Bench%20Verified%60.%20We%20demonstrate%20our%20superiority%20over%20the%20previous%20open%20software%20engineering%20mid-training%20recipe%20%60Kimi-Dev%60%20under%20two%20post-training%20settings%20with%20an%20aligned%20base%20model%20and%20agentic%20scaffold%2C%20while%20using%20less%20than%20half%20mid-training%20tokens%20%2873.1B%29.%20Besides%20relative%20advantage%2C%20our%20best%20performing%2032B%20and%2072B%20models%20achieve%20%2A%2A56.1%25%2A%2A%20and%20%2A%2A58.5%25%2A%2A%20resolution%20rates%2C%20respectively%2C%20which%20are%20...%0ALink%3A%20http%3A//arxiv.org/abs/2601.18418v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DdaVinci-Dev%253A%2520Agent-native%2520Mid-training%2520for%2520Software%2520Engineering%26entry.906535625%3DJi%2520Zeng%2520and%2520Dayuan%2520Fu%2520and%2520Tiantian%2520Mi%2520and%2520Yumin%2520Zhuang%2520and%2520Yaxing%2520Huang%2520and%2520Xuefeng%2520Li%2520and%2520Lyumanshan%2520Ye%2520and%2520Muhang%2520Xie%2520and%2520Qishuo%2520Hua%2520and%2520Zhen%2520Huang%2520and%2520Mohan%2520Jiang%2520and%2520Hanning%2520Wang%2520and%2520Jifan%2520Lin%2520and%2520Yang%2520Xiao%2520and%2520Jie%2520Sun%2520and%2520Yunze%2520Wu%2520and%2520Pengfei%2520Liu%26entry.1292438233%3DRecently%252C%2520the%2520frontier%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520capabilities%2520has%2520shifted%2520from%2520single-turn%2520code%2520generation%2520to%2520agentic%2520software%2520engineering-a%2520paradigm%2520where%2520models%2520autonomously%2520navigate%252C%2520edit%252C%2520and%2520test%2520complex%2520repositories.%2520While%2520post-training%2520methods%2520have%2520become%2520the%2520de%2520facto%2520approach%2520for%2520code%2520agents%252C%2520%252A%252Aagentic%2520mid-training%252A%252A-mid-training%2520%2528MT%2529%2520on%2520large-scale%2520data%2520that%2520mirrors%2520authentic%2520agentic%2520workflows-remains%2520critically%2520underexplored%2520due%2520to%2520substantial%2520resource%2520requirements%252C%2520despite%2520offering%2520a%2520more%2520scalable%2520path%2520to%2520instilling%2520foundational%2520agentic%2520behaviors%2520than%2520relying%2520solely%2520on%2520expensive%2520reinforcement%2520learning.%2520A%2520central%2520challenge%2520in%2520realizing%2520effective%2520agentic%2520mid-training%2520is%2520the%2520distribution%2520mismatch%2520between%2520static%2520training%2520data%2520and%2520the%2520dynamic%252C%2520feedback-rich%2520environment%2520of%2520real%2520development.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520systematic%2520study%2520of%2520agentic%2520mid-training%252C%2520establishing%2520both%2520the%2520data%2520synthesis%2520principles%2520and%2520training%2520methodology%2520for%2520effective%2520agent%2520development%2520at%2520scale.%2520Central%2520to%2520our%2520approach%2520is%2520%252A%252Aagent-native%2520data%252A%252A-supervision%2520comprising%2520two%2520complementary%2520types%2520of%2520trajectories%253A%2520%252A%252Acontextually-native%2520trajectories%252A%252A%2520that%2520preserve%2520the%2520complete%2520information%2520flow%2520an%2520agent%2520experiences%252C%2520offering%2520broad%2520coverage%2520and%2520diversity%253B%2520and%2520%252A%252Aenvironmentally-native%2520trajectories%252A%252A%2520collected%2520from%2520executable%2520repositories%2520where%2520observations%2520stem%2520from%2520actual%2520tool%2520invocations%2520and%2520test%2520executions%252C%2520providing%2520depth%2520and%2520interaction%2520authenticity.%2520We%2520verify%2520the%2520model%2527s%2520agentic%2520capabilities%2520on%2520%2560SWE-Bench%2520Verified%2560.%2520We%2520demonstrate%2520our%2520superiority%2520over%2520the%2520previous%2520open%2520software%2520engineering%2520mid-training%2520recipe%2520%2560Kimi-Dev%2560%2520under%2520two%2520post-training%2520settings%2520with%2520an%2520aligned%2520base%2520model%2520and%2520agentic%2520scaffold%252C%2520while%2520using%2520less%2520than%2520half%2520mid-training%2520tokens%2520%252873.1B%2529.%2520Besides%2520relative%2520advantage%252C%2520our%2520best%2520performing%252032B%2520and%252072B%2520models%2520achieve%2520%252A%252A56.1%2525%252A%252A%2520and%2520%252A%252A58.5%2525%252A%252A%2520resolution%2520rates%252C%2520respectively%252C%2520which%2520are%2520...%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18418v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=daVinci-Dev%3A%20Agent-native%20Mid-training%20for%20Software%20Engineering&entry.906535625=Ji%20Zeng%20and%20Dayuan%20Fu%20and%20Tiantian%20Mi%20and%20Yumin%20Zhuang%20and%20Yaxing%20Huang%20and%20Xuefeng%20Li%20and%20Lyumanshan%20Ye%20and%20Muhang%20Xie%20and%20Qishuo%20Hua%20and%20Zhen%20Huang%20and%20Mohan%20Jiang%20and%20Hanning%20Wang%20and%20Jifan%20Lin%20and%20Yang%20Xiao%20and%20Jie%20Sun%20and%20Yunze%20Wu%20and%20Pengfei%20Liu&entry.1292438233=Recently%2C%20the%20frontier%20of%20Large%20Language%20Model%20%28LLM%29%20capabilities%20has%20shifted%20from%20single-turn%20code%20generation%20to%20agentic%20software%20engineering-a%20paradigm%20where%20models%20autonomously%20navigate%2C%20edit%2C%20and%20test%20complex%20repositories.%20While%20post-training%20methods%20have%20become%20the%20de%20facto%20approach%20for%20code%20agents%2C%20%2A%2Aagentic%20mid-training%2A%2A-mid-training%20%28MT%29%20on%20large-scale%20data%20that%20mirrors%20authentic%20agentic%20workflows-remains%20critically%20underexplored%20due%20to%20substantial%20resource%20requirements%2C%20despite%20offering%20a%20more%20scalable%20path%20to%20instilling%20foundational%20agentic%20behaviors%20than%20relying%20solely%20on%20expensive%20reinforcement%20learning.%20A%20central%20challenge%20in%20realizing%20effective%20agentic%20mid-training%20is%20the%20distribution%20mismatch%20between%20static%20training%20data%20and%20the%20dynamic%2C%20feedback-rich%20environment%20of%20real%20development.%20To%20address%20this%2C%20we%20present%20a%20systematic%20study%20of%20agentic%20mid-training%2C%20establishing%20both%20the%20data%20synthesis%20principles%20and%20training%20methodology%20for%20effective%20agent%20development%20at%20scale.%20Central%20to%20our%20approach%20is%20%2A%2Aagent-native%20data%2A%2A-supervision%20comprising%20two%20complementary%20types%20of%20trajectories%3A%20%2A%2Acontextually-native%20trajectories%2A%2A%20that%20preserve%20the%20complete%20information%20flow%20an%20agent%20experiences%2C%20offering%20broad%20coverage%20and%20diversity%3B%20and%20%2A%2Aenvironmentally-native%20trajectories%2A%2A%20collected%20from%20executable%20repositories%20where%20observations%20stem%20from%20actual%20tool%20invocations%20and%20test%20executions%2C%20providing%20depth%20and%20interaction%20authenticity.%20We%20verify%20the%20model%27s%20agentic%20capabilities%20on%20%60SWE-Bench%20Verified%60.%20We%20demonstrate%20our%20superiority%20over%20the%20previous%20open%20software%20engineering%20mid-training%20recipe%20%60Kimi-Dev%60%20under%20two%20post-training%20settings%20with%20an%20aligned%20base%20model%20and%20agentic%20scaffold%2C%20while%20using%20less%20than%20half%20mid-training%20tokens%20%2873.1B%29.%20Besides%20relative%20advantage%2C%20our%20best%20performing%2032B%20and%2072B%20models%20achieve%20%2A%2A56.1%25%2A%2A%20and%20%2A%2A58.5%25%2A%2A%20resolution%20rates%2C%20respectively%2C%20which%20are%20...&entry.1838667208=http%3A//arxiv.org/abs/2601.18418v2&entry.124074799=Read"},
{"title": "Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation", "author": "Yizhao Han and Tianxing Shi and Zhao Wang and Zifan Xu and Zhiyuan Pu and Mingxiao Li and Qian Zhang and Wei Yin and Xiao-Xiao Long", "abstract": "Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.", "link": "http://arxiv.org/abs/2601.19488v1", "date": "2026-01-27", "relevancy": 2.699, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5369}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Guided%20k-Guard%20Sampling%20for%20Long-Horizon%20Autoregressive%20Video%20Generation&body=Title%3A%20Entropy-Guided%20k-Guard%20Sampling%20for%20Long-Horizon%20Autoregressive%20Video%20Generation%0AAuthor%3A%20Yizhao%20Han%20and%20Tianxing%20Shi%20and%20Zhao%20Wang%20and%20Zifan%20Xu%20and%20Zhiyuan%20Pu%20and%20Mingxiao%20Li%20and%20Qian%20Zhang%20and%20Wei%20Yin%20and%20Xiao-Xiao%20Long%0AAbstract%3A%20Autoregressive%20%28AR%29%20architectures%20have%20achieved%20significant%20successes%20in%20LLMs%2C%20inspiring%20explorations%20for%20video%20generation.%20In%20LLMs%2C%20top-p/top-k%20sampling%20strategies%20work%20exceptionally%20well%3A%20language%20tokens%20have%20high%20semantic%20density%20and%20low%20redundancy%2C%20so%20a%20fixed%20size%20of%20token%20candidates%20already%20strikes%20a%20balance%20between%20semantic%20accuracy%20and%20generation%20diversity.%20In%20contrast%2C%20video%20tokens%20have%20low%20semantic%20density%20and%20high%20spatio-temporal%20redundancy.%20This%20mismatch%20makes%20static%20top-k/top-p%20strategies%20ineffective%20for%20video%20decoders%3A%20they%20either%20introduce%20unnecessary%20randomness%20for%20low-uncertainty%20regions%20%28static%20backgrounds%29%20or%20get%20stuck%20in%20early%20errors%20for%20high-uncertainty%20regions%20%28foreground%20objects%29.%20Prediction%20errors%20will%20accumulate%20as%20more%20frames%20are%20generated%20and%20eventually%20severely%20degrade%20long-horizon%20quality.%20To%20address%20this%2C%20we%20propose%20Entropy-Guided%20k-Guard%20%28ENkG%29%20sampling%2C%20a%20simple%20yet%20effective%20strategy%20that%20adapts%20sampling%20to%20token-wise%20dispersion%2C%20quantified%20by%20the%20entropy%20of%20each%20token%27s%20predicted%20distribution.%20ENkG%20uses%20adaptive%20token%20candidate%20sizes%3A%20for%20low-entropy%20regions%2C%20it%20employs%20fewer%20candidates%20to%20suppress%20redundant%20noise%20and%20preserve%20structural%20integrity%3B%20for%20high-entropy%20regions%2C%20it%20uses%20more%20candidates%20to%20mitigate%20error%20compounding.%20ENkG%20is%20model-agnostic%2C%20training-free%2C%20and%20adds%20negligible%20overhead.%20Experiments%20demonstrate%20consistent%20improvements%20in%20perceptual%20quality%20and%20structural%20stability%20compared%20to%20static%20top-k/top-p%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Guided%2520k-Guard%2520Sampling%2520for%2520Long-Horizon%2520Autoregressive%2520Video%2520Generation%26entry.906535625%3DYizhao%2520Han%2520and%2520Tianxing%2520Shi%2520and%2520Zhao%2520Wang%2520and%2520Zifan%2520Xu%2520and%2520Zhiyuan%2520Pu%2520and%2520Mingxiao%2520Li%2520and%2520Qian%2520Zhang%2520and%2520Wei%2520Yin%2520and%2520Xiao-Xiao%2520Long%26entry.1292438233%3DAutoregressive%2520%2528AR%2529%2520architectures%2520have%2520achieved%2520significant%2520successes%2520in%2520LLMs%252C%2520inspiring%2520explorations%2520for%2520video%2520generation.%2520In%2520LLMs%252C%2520top-p/top-k%2520sampling%2520strategies%2520work%2520exceptionally%2520well%253A%2520language%2520tokens%2520have%2520high%2520semantic%2520density%2520and%2520low%2520redundancy%252C%2520so%2520a%2520fixed%2520size%2520of%2520token%2520candidates%2520already%2520strikes%2520a%2520balance%2520between%2520semantic%2520accuracy%2520and%2520generation%2520diversity.%2520In%2520contrast%252C%2520video%2520tokens%2520have%2520low%2520semantic%2520density%2520and%2520high%2520spatio-temporal%2520redundancy.%2520This%2520mismatch%2520makes%2520static%2520top-k/top-p%2520strategies%2520ineffective%2520for%2520video%2520decoders%253A%2520they%2520either%2520introduce%2520unnecessary%2520randomness%2520for%2520low-uncertainty%2520regions%2520%2528static%2520backgrounds%2529%2520or%2520get%2520stuck%2520in%2520early%2520errors%2520for%2520high-uncertainty%2520regions%2520%2528foreground%2520objects%2529.%2520Prediction%2520errors%2520will%2520accumulate%2520as%2520more%2520frames%2520are%2520generated%2520and%2520eventually%2520severely%2520degrade%2520long-horizon%2520quality.%2520To%2520address%2520this%252C%2520we%2520propose%2520Entropy-Guided%2520k-Guard%2520%2528ENkG%2529%2520sampling%252C%2520a%2520simple%2520yet%2520effective%2520strategy%2520that%2520adapts%2520sampling%2520to%2520token-wise%2520dispersion%252C%2520quantified%2520by%2520the%2520entropy%2520of%2520each%2520token%2527s%2520predicted%2520distribution.%2520ENkG%2520uses%2520adaptive%2520token%2520candidate%2520sizes%253A%2520for%2520low-entropy%2520regions%252C%2520it%2520employs%2520fewer%2520candidates%2520to%2520suppress%2520redundant%2520noise%2520and%2520preserve%2520structural%2520integrity%253B%2520for%2520high-entropy%2520regions%252C%2520it%2520uses%2520more%2520candidates%2520to%2520mitigate%2520error%2520compounding.%2520ENkG%2520is%2520model-agnostic%252C%2520training-free%252C%2520and%2520adds%2520negligible%2520overhead.%2520Experiments%2520demonstrate%2520consistent%2520improvements%2520in%2520perceptual%2520quality%2520and%2520structural%2520stability%2520compared%2520to%2520static%2520top-k/top-p%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Guided%20k-Guard%20Sampling%20for%20Long-Horizon%20Autoregressive%20Video%20Generation&entry.906535625=Yizhao%20Han%20and%20Tianxing%20Shi%20and%20Zhao%20Wang%20and%20Zifan%20Xu%20and%20Zhiyuan%20Pu%20and%20Mingxiao%20Li%20and%20Qian%20Zhang%20and%20Wei%20Yin%20and%20Xiao-Xiao%20Long&entry.1292438233=Autoregressive%20%28AR%29%20architectures%20have%20achieved%20significant%20successes%20in%20LLMs%2C%20inspiring%20explorations%20for%20video%20generation.%20In%20LLMs%2C%20top-p/top-k%20sampling%20strategies%20work%20exceptionally%20well%3A%20language%20tokens%20have%20high%20semantic%20density%20and%20low%20redundancy%2C%20so%20a%20fixed%20size%20of%20token%20candidates%20already%20strikes%20a%20balance%20between%20semantic%20accuracy%20and%20generation%20diversity.%20In%20contrast%2C%20video%20tokens%20have%20low%20semantic%20density%20and%20high%20spatio-temporal%20redundancy.%20This%20mismatch%20makes%20static%20top-k/top-p%20strategies%20ineffective%20for%20video%20decoders%3A%20they%20either%20introduce%20unnecessary%20randomness%20for%20low-uncertainty%20regions%20%28static%20backgrounds%29%20or%20get%20stuck%20in%20early%20errors%20for%20high-uncertainty%20regions%20%28foreground%20objects%29.%20Prediction%20errors%20will%20accumulate%20as%20more%20frames%20are%20generated%20and%20eventually%20severely%20degrade%20long-horizon%20quality.%20To%20address%20this%2C%20we%20propose%20Entropy-Guided%20k-Guard%20%28ENkG%29%20sampling%2C%20a%20simple%20yet%20effective%20strategy%20that%20adapts%20sampling%20to%20token-wise%20dispersion%2C%20quantified%20by%20the%20entropy%20of%20each%20token%27s%20predicted%20distribution.%20ENkG%20uses%20adaptive%20token%20candidate%20sizes%3A%20for%20low-entropy%20regions%2C%20it%20employs%20fewer%20candidates%20to%20suppress%20redundant%20noise%20and%20preserve%20structural%20integrity%3B%20for%20high-entropy%20regions%2C%20it%20uses%20more%20candidates%20to%20mitigate%20error%20compounding.%20ENkG%20is%20model-agnostic%2C%20training-free%2C%20and%20adds%20negligible%20overhead.%20Experiments%20demonstrate%20consistent%20improvements%20in%20perceptual%20quality%20and%20structural%20stability%20compared%20to%20static%20top-k/top-p%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2601.19488v1&entry.124074799=Read"},
{"title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training", "author": "Alejandro Rodriguez-Garcia and Anindya Ghosh and Srikanth Ramaswamy", "abstract": "Recent work in continual learning has highlighted the stability gap -- a temporary performance drop on previously learned tasks when new ones are introduced. This phenomenon reflects a mismatch between rapid adaptation and strong retention at task boundaries, underscoring the need for optimization mechanisms that balance plasticity and stability over abrupt distribution changes. While optimizers such as momentum-SGD and Adam introduce implicit multi-timescale behavior, they still exhibit pronounced stability gaps. Importantly, these gaps persist even under ideal joint training, making it crucial to study them in this setting to isolate their causes from other sources of forgetting. Motivated by how noradrenergic (neuromodulatory) bursts transiently increase neuronal gain under uncertainty, we introduce a dynamic gain scaling mechanism as a two-timescale optimization technique that balances adaptation and retention by modulating effective learning rates and flattening the local landscape through an effective reparameterization. Across domain- and class-incremental MNIST, CIFAR, and mini-ImageNet benchmarks under task-agnostic joint training, dynamic gain scaling effectively attenuates stability gaps while maintaining competitive accuracy, improving robustness at task transitions.", "link": "http://arxiv.org/abs/2507.14056v2", "date": "2026-01-27", "relevancy": 2.6901, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5804}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5195}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noradrenergic-inspired%20gain%20modulation%20attenuates%20the%20stability%20gap%20in%20joint%20training&body=Title%3A%20Noradrenergic-inspired%20gain%20modulation%20attenuates%20the%20stability%20gap%20in%20joint%20training%0AAuthor%3A%20Alejandro%20Rodriguez-Garcia%20and%20Anindya%20Ghosh%20and%20Srikanth%20Ramaswamy%0AAbstract%3A%20Recent%20work%20in%20continual%20learning%20has%20highlighted%20the%20stability%20gap%20--%20a%20temporary%20performance%20drop%20on%20previously%20learned%20tasks%20when%20new%20ones%20are%20introduced.%20This%20phenomenon%20reflects%20a%20mismatch%20between%20rapid%20adaptation%20and%20strong%20retention%20at%20task%20boundaries%2C%20underscoring%20the%20need%20for%20optimization%20mechanisms%20that%20balance%20plasticity%20and%20stability%20over%20abrupt%20distribution%20changes.%20While%20optimizers%20such%20as%20momentum-SGD%20and%20Adam%20introduce%20implicit%20multi-timescale%20behavior%2C%20they%20still%20exhibit%20pronounced%20stability%20gaps.%20Importantly%2C%20these%20gaps%20persist%20even%20under%20ideal%20joint%20training%2C%20making%20it%20crucial%20to%20study%20them%20in%20this%20setting%20to%20isolate%20their%20causes%20from%20other%20sources%20of%20forgetting.%20Motivated%20by%20how%20noradrenergic%20%28neuromodulatory%29%20bursts%20transiently%20increase%20neuronal%20gain%20under%20uncertainty%2C%20we%20introduce%20a%20dynamic%20gain%20scaling%20mechanism%20as%20a%20two-timescale%20optimization%20technique%20that%20balances%20adaptation%20and%20retention%20by%20modulating%20effective%20learning%20rates%20and%20flattening%20the%20local%20landscape%20through%20an%20effective%20reparameterization.%20Across%20domain-%20and%20class-incremental%20MNIST%2C%20CIFAR%2C%20and%20mini-ImageNet%20benchmarks%20under%20task-agnostic%20joint%20training%2C%20dynamic%20gain%20scaling%20effectively%20attenuates%20stability%20gaps%20while%20maintaining%20competitive%20accuracy%2C%20improving%20robustness%20at%20task%20transitions.%0ALink%3A%20http%3A//arxiv.org/abs/2507.14056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoradrenergic-inspired%2520gain%2520modulation%2520attenuates%2520the%2520stability%2520gap%2520in%2520joint%2520training%26entry.906535625%3DAlejandro%2520Rodriguez-Garcia%2520and%2520Anindya%2520Ghosh%2520and%2520Srikanth%2520Ramaswamy%26entry.1292438233%3DRecent%2520work%2520in%2520continual%2520learning%2520has%2520highlighted%2520the%2520stability%2520gap%2520--%2520a%2520temporary%2520performance%2520drop%2520on%2520previously%2520learned%2520tasks%2520when%2520new%2520ones%2520are%2520introduced.%2520This%2520phenomenon%2520reflects%2520a%2520mismatch%2520between%2520rapid%2520adaptation%2520and%2520strong%2520retention%2520at%2520task%2520boundaries%252C%2520underscoring%2520the%2520need%2520for%2520optimization%2520mechanisms%2520that%2520balance%2520plasticity%2520and%2520stability%2520over%2520abrupt%2520distribution%2520changes.%2520While%2520optimizers%2520such%2520as%2520momentum-SGD%2520and%2520Adam%2520introduce%2520implicit%2520multi-timescale%2520behavior%252C%2520they%2520still%2520exhibit%2520pronounced%2520stability%2520gaps.%2520Importantly%252C%2520these%2520gaps%2520persist%2520even%2520under%2520ideal%2520joint%2520training%252C%2520making%2520it%2520crucial%2520to%2520study%2520them%2520in%2520this%2520setting%2520to%2520isolate%2520their%2520causes%2520from%2520other%2520sources%2520of%2520forgetting.%2520Motivated%2520by%2520how%2520noradrenergic%2520%2528neuromodulatory%2529%2520bursts%2520transiently%2520increase%2520neuronal%2520gain%2520under%2520uncertainty%252C%2520we%2520introduce%2520a%2520dynamic%2520gain%2520scaling%2520mechanism%2520as%2520a%2520two-timescale%2520optimization%2520technique%2520that%2520balances%2520adaptation%2520and%2520retention%2520by%2520modulating%2520effective%2520learning%2520rates%2520and%2520flattening%2520the%2520local%2520landscape%2520through%2520an%2520effective%2520reparameterization.%2520Across%2520domain-%2520and%2520class-incremental%2520MNIST%252C%2520CIFAR%252C%2520and%2520mini-ImageNet%2520benchmarks%2520under%2520task-agnostic%2520joint%2520training%252C%2520dynamic%2520gain%2520scaling%2520effectively%2520attenuates%2520stability%2520gaps%2520while%2520maintaining%2520competitive%2520accuracy%252C%2520improving%2520robustness%2520at%2520task%2520transitions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noradrenergic-inspired%20gain%20modulation%20attenuates%20the%20stability%20gap%20in%20joint%20training&entry.906535625=Alejandro%20Rodriguez-Garcia%20and%20Anindya%20Ghosh%20and%20Srikanth%20Ramaswamy&entry.1292438233=Recent%20work%20in%20continual%20learning%20has%20highlighted%20the%20stability%20gap%20--%20a%20temporary%20performance%20drop%20on%20previously%20learned%20tasks%20when%20new%20ones%20are%20introduced.%20This%20phenomenon%20reflects%20a%20mismatch%20between%20rapid%20adaptation%20and%20strong%20retention%20at%20task%20boundaries%2C%20underscoring%20the%20need%20for%20optimization%20mechanisms%20that%20balance%20plasticity%20and%20stability%20over%20abrupt%20distribution%20changes.%20While%20optimizers%20such%20as%20momentum-SGD%20and%20Adam%20introduce%20implicit%20multi-timescale%20behavior%2C%20they%20still%20exhibit%20pronounced%20stability%20gaps.%20Importantly%2C%20these%20gaps%20persist%20even%20under%20ideal%20joint%20training%2C%20making%20it%20crucial%20to%20study%20them%20in%20this%20setting%20to%20isolate%20their%20causes%20from%20other%20sources%20of%20forgetting.%20Motivated%20by%20how%20noradrenergic%20%28neuromodulatory%29%20bursts%20transiently%20increase%20neuronal%20gain%20under%20uncertainty%2C%20we%20introduce%20a%20dynamic%20gain%20scaling%20mechanism%20as%20a%20two-timescale%20optimization%20technique%20that%20balances%20adaptation%20and%20retention%20by%20modulating%20effective%20learning%20rates%20and%20flattening%20the%20local%20landscape%20through%20an%20effective%20reparameterization.%20Across%20domain-%20and%20class-incremental%20MNIST%2C%20CIFAR%2C%20and%20mini-ImageNet%20benchmarks%20under%20task-agnostic%20joint%20training%2C%20dynamic%20gain%20scaling%20effectively%20attenuates%20stability%20gaps%20while%20maintaining%20competitive%20accuracy%2C%20improving%20robustness%20at%20task%20transitions.&entry.1838667208=http%3A//arxiv.org/abs/2507.14056v2&entry.124074799=Read"},
{"title": "DSTCS: Dual-Student Teacher Framework with Segment Anything Model for Semi-Supervised Pubic Symphysis Fetal Head Segmentation", "author": "Yalin Luo and Shun Long and Huijin Wang and Jieyun Bai", "abstract": "Segmentation of the pubic symphysis and fetal head (PSFH) is a critical procedure in intrapartum monitoring and is essential for evaluating labor progression and identifying potential delivery complications. However, achieving accurate segmentation remains a significant challenge due to class imbalance, ambiguous boundaries, and noise interference in ultrasound images, compounded by the scarcity of high-quality annotated data. Current research on PSFH segmentation predominantly relies on CNN and Transformer architectures, leaving the potential of more powerful models underexplored. In this work, we propose a Dual-Student and Teacher framework combining CNN and SAM (DSTCS), which integrates the Segment Anything Model (SAM) into a dual student-teacher architecture. A cooperative learning mechanism between the CNN and SAM branches significantly improves segmentation accuracy. The proposed scheme also incorporates a specialized data augmentation strategy optimized for boundary processing and a novel loss function. Extensive experiments on the MICCAI 2023 and 2024 PSFH segmentation benchmarks demonstrate that our method exhibits superior robustness and significantly outperforms existing techniques, providing a reliable segmentation tool for clinical practice.", "link": "http://arxiv.org/abs/2601.19446v1", "date": "2026-01-27", "relevancy": 2.6888, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5402}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSTCS%3A%20Dual-Student%20Teacher%20Framework%20with%20Segment%20Anything%20Model%20for%20Semi-Supervised%20Pubic%20Symphysis%20Fetal%20Head%20Segmentation&body=Title%3A%20DSTCS%3A%20Dual-Student%20Teacher%20Framework%20with%20Segment%20Anything%20Model%20for%20Semi-Supervised%20Pubic%20Symphysis%20Fetal%20Head%20Segmentation%0AAuthor%3A%20Yalin%20Luo%20and%20Shun%20Long%20and%20Huijin%20Wang%20and%20Jieyun%20Bai%0AAbstract%3A%20Segmentation%20of%20the%20pubic%20symphysis%20and%20fetal%20head%20%28PSFH%29%20is%20a%20critical%20procedure%20in%20intrapartum%20monitoring%20and%20is%20essential%20for%20evaluating%20labor%20progression%20and%20identifying%20potential%20delivery%20complications.%20However%2C%20achieving%20accurate%20segmentation%20remains%20a%20significant%20challenge%20due%20to%20class%20imbalance%2C%20ambiguous%20boundaries%2C%20and%20noise%20interference%20in%20ultrasound%20images%2C%20compounded%20by%20the%20scarcity%20of%20high-quality%20annotated%20data.%20Current%20research%20on%20PSFH%20segmentation%20predominantly%20relies%20on%20CNN%20and%20Transformer%20architectures%2C%20leaving%20the%20potential%20of%20more%20powerful%20models%20underexplored.%20In%20this%20work%2C%20we%20propose%20a%20Dual-Student%20and%20Teacher%20framework%20combining%20CNN%20and%20SAM%20%28DSTCS%29%2C%20which%20integrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20into%20a%20dual%20student-teacher%20architecture.%20A%20cooperative%20learning%20mechanism%20between%20the%20CNN%20and%20SAM%20branches%20significantly%20improves%20segmentation%20accuracy.%20The%20proposed%20scheme%20also%20incorporates%20a%20specialized%20data%20augmentation%20strategy%20optimized%20for%20boundary%20processing%20and%20a%20novel%20loss%20function.%20Extensive%20experiments%20on%20the%20MICCAI%202023%20and%202024%20PSFH%20segmentation%20benchmarks%20demonstrate%20that%20our%20method%20exhibits%20superior%20robustness%20and%20significantly%20outperforms%20existing%20techniques%2C%20providing%20a%20reliable%20segmentation%20tool%20for%20clinical%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSTCS%253A%2520Dual-Student%2520Teacher%2520Framework%2520with%2520Segment%2520Anything%2520Model%2520for%2520Semi-Supervised%2520Pubic%2520Symphysis%2520Fetal%2520Head%2520Segmentation%26entry.906535625%3DYalin%2520Luo%2520and%2520Shun%2520Long%2520and%2520Huijin%2520Wang%2520and%2520Jieyun%2520Bai%26entry.1292438233%3DSegmentation%2520of%2520the%2520pubic%2520symphysis%2520and%2520fetal%2520head%2520%2528PSFH%2529%2520is%2520a%2520critical%2520procedure%2520in%2520intrapartum%2520monitoring%2520and%2520is%2520essential%2520for%2520evaluating%2520labor%2520progression%2520and%2520identifying%2520potential%2520delivery%2520complications.%2520However%252C%2520achieving%2520accurate%2520segmentation%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520class%2520imbalance%252C%2520ambiguous%2520boundaries%252C%2520and%2520noise%2520interference%2520in%2520ultrasound%2520images%252C%2520compounded%2520by%2520the%2520scarcity%2520of%2520high-quality%2520annotated%2520data.%2520Current%2520research%2520on%2520PSFH%2520segmentation%2520predominantly%2520relies%2520on%2520CNN%2520and%2520Transformer%2520architectures%252C%2520leaving%2520the%2520potential%2520of%2520more%2520powerful%2520models%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Dual-Student%2520and%2520Teacher%2520framework%2520combining%2520CNN%2520and%2520SAM%2520%2528DSTCS%2529%252C%2520which%2520integrates%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520into%2520a%2520dual%2520student-teacher%2520architecture.%2520A%2520cooperative%2520learning%2520mechanism%2520between%2520the%2520CNN%2520and%2520SAM%2520branches%2520significantly%2520improves%2520segmentation%2520accuracy.%2520The%2520proposed%2520scheme%2520also%2520incorporates%2520a%2520specialized%2520data%2520augmentation%2520strategy%2520optimized%2520for%2520boundary%2520processing%2520and%2520a%2520novel%2520loss%2520function.%2520Extensive%2520experiments%2520on%2520the%2520MICCAI%25202023%2520and%25202024%2520PSFH%2520segmentation%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520exhibits%2520superior%2520robustness%2520and%2520significantly%2520outperforms%2520existing%2520techniques%252C%2520providing%2520a%2520reliable%2520segmentation%2520tool%2520for%2520clinical%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSTCS%3A%20Dual-Student%20Teacher%20Framework%20with%20Segment%20Anything%20Model%20for%20Semi-Supervised%20Pubic%20Symphysis%20Fetal%20Head%20Segmentation&entry.906535625=Yalin%20Luo%20and%20Shun%20Long%20and%20Huijin%20Wang%20and%20Jieyun%20Bai&entry.1292438233=Segmentation%20of%20the%20pubic%20symphysis%20and%20fetal%20head%20%28PSFH%29%20is%20a%20critical%20procedure%20in%20intrapartum%20monitoring%20and%20is%20essential%20for%20evaluating%20labor%20progression%20and%20identifying%20potential%20delivery%20complications.%20However%2C%20achieving%20accurate%20segmentation%20remains%20a%20significant%20challenge%20due%20to%20class%20imbalance%2C%20ambiguous%20boundaries%2C%20and%20noise%20interference%20in%20ultrasound%20images%2C%20compounded%20by%20the%20scarcity%20of%20high-quality%20annotated%20data.%20Current%20research%20on%20PSFH%20segmentation%20predominantly%20relies%20on%20CNN%20and%20Transformer%20architectures%2C%20leaving%20the%20potential%20of%20more%20powerful%20models%20underexplored.%20In%20this%20work%2C%20we%20propose%20a%20Dual-Student%20and%20Teacher%20framework%20combining%20CNN%20and%20SAM%20%28DSTCS%29%2C%20which%20integrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20into%20a%20dual%20student-teacher%20architecture.%20A%20cooperative%20learning%20mechanism%20between%20the%20CNN%20and%20SAM%20branches%20significantly%20improves%20segmentation%20accuracy.%20The%20proposed%20scheme%20also%20incorporates%20a%20specialized%20data%20augmentation%20strategy%20optimized%20for%20boundary%20processing%20and%20a%20novel%20loss%20function.%20Extensive%20experiments%20on%20the%20MICCAI%202023%20and%202024%20PSFH%20segmentation%20benchmarks%20demonstrate%20that%20our%20method%20exhibits%20superior%20robustness%20and%20significantly%20outperforms%20existing%20techniques%2C%20providing%20a%20reliable%20segmentation%20tool%20for%20clinical%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2601.19446v1&entry.124074799=Read"},
{"title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution", "author": "Ziyue Wang and Sheng Jin and Zhongrong Zuo and Jiawei Wu and Han Qiu and Qi She and Hao Zhang and Xudong Jiang", "abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.", "link": "http://arxiv.org/abs/2601.19686v1", "date": "2026-01-27", "relevancy": 2.6884, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-KTR%3A%20Reinforcing%20Video%20Reasoning%20via%20Key%20Token%20Attribution&body=Title%3A%20Video-KTR%3A%20Reinforcing%20Video%20Reasoning%20via%20Key%20Token%20Attribution%0AAuthor%3A%20Ziyue%20Wang%20and%20Sheng%20Jin%20and%20Zhongrong%20Zuo%20and%20Jiawei%20Wu%20and%20Han%20Qiu%20and%20Qi%20She%20and%20Hao%20Zhang%20and%20Xudong%20Jiang%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20has%20shown%20strong%20potential%20for%20enhancing%20reasoning%20in%20multimodal%20large%20language%20models%2C%20yet%20existing%20video%20reasoning%20methods%20often%20rely%20on%20coarse%20sequence-level%20rewards%20or%20single-factor%20token%20selection%2C%20neglecting%20fine-grained%20links%20among%20visual%20inputs%2C%20temporal%20dynamics%2C%20and%20linguistic%20outputs%2C%20limiting%20both%20accuracy%20and%20interpretability.%20We%20propose%20Video-KTR%2C%20a%20modality-aware%20policy%20shaping%20framework%20that%20performs%20selective%2C%20token-level%20RL%20by%20combining%20three%20attribution%20signals%3A%20%281%29%20visual-aware%20tokens%20identified%20via%20counterfactual%20masking%20to%20reveal%20perceptual%20dependence%3B%20%282%29%20temporal-aware%20tokens%20detected%20through%20frame%20shuffling%20to%20expose%20temporal%20sensitivity%3B%20and%20%283%29%20high-entropy%20tokens%20signaling%20predictive%20uncertainty.%20By%20reinforcing%20only%20these%20key%20tokens%2C%20Video-KTR%20focuses%20learning%20on%20semantically%20informative%2C%20modality-sensitive%20content%20while%20filtering%20out%20low-value%20tokens.%20Across%20five%20challenging%20benchmarks%2C%20Video-KTR%20achieves%20state-of-the-art%20or%20highly%20competitive%20results%2C%20achieving%2042.7%5C%25%20on%20Video-Holmes%20%28surpassing%20GPT-4o%29%20with%20consistent%20gains%20on%20both%20reasoning%20and%20general%20video%20understanding%20tasks.%20Ablation%20studies%20verify%20the%20complementary%20roles%20of%20the%20attribution%20signals%20and%20the%20robustness%20of%20targeted%20token-level%20updates.%20Overall%2C%20Video-KTR%20improves%20accuracy%20and%20interpretability%2C%20offering%20a%20simple%2C%20drop-in%20extension%20to%20RL%20for%20complex%20video%20reasoning.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/zywang0104/Video-KTR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-KTR%253A%2520Reinforcing%2520Video%2520Reasoning%2520via%2520Key%2520Token%2520Attribution%26entry.906535625%3DZiyue%2520Wang%2520and%2520Sheng%2520Jin%2520and%2520Zhongrong%2520Zuo%2520and%2520Jiawei%2520Wu%2520and%2520Han%2520Qiu%2520and%2520Qi%2520She%2520and%2520Hao%2520Zhang%2520and%2520Xudong%2520Jiang%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520has%2520shown%2520strong%2520potential%2520for%2520enhancing%2520reasoning%2520in%2520multimodal%2520large%2520language%2520models%252C%2520yet%2520existing%2520video%2520reasoning%2520methods%2520often%2520rely%2520on%2520coarse%2520sequence-level%2520rewards%2520or%2520single-factor%2520token%2520selection%252C%2520neglecting%2520fine-grained%2520links%2520among%2520visual%2520inputs%252C%2520temporal%2520dynamics%252C%2520and%2520linguistic%2520outputs%252C%2520limiting%2520both%2520accuracy%2520and%2520interpretability.%2520We%2520propose%2520Video-KTR%252C%2520a%2520modality-aware%2520policy%2520shaping%2520framework%2520that%2520performs%2520selective%252C%2520token-level%2520RL%2520by%2520combining%2520three%2520attribution%2520signals%253A%2520%25281%2529%2520visual-aware%2520tokens%2520identified%2520via%2520counterfactual%2520masking%2520to%2520reveal%2520perceptual%2520dependence%253B%2520%25282%2529%2520temporal-aware%2520tokens%2520detected%2520through%2520frame%2520shuffling%2520to%2520expose%2520temporal%2520sensitivity%253B%2520and%2520%25283%2529%2520high-entropy%2520tokens%2520signaling%2520predictive%2520uncertainty.%2520By%2520reinforcing%2520only%2520these%2520key%2520tokens%252C%2520Video-KTR%2520focuses%2520learning%2520on%2520semantically%2520informative%252C%2520modality-sensitive%2520content%2520while%2520filtering%2520out%2520low-value%2520tokens.%2520Across%2520five%2520challenging%2520benchmarks%252C%2520Video-KTR%2520achieves%2520state-of-the-art%2520or%2520highly%2520competitive%2520results%252C%2520achieving%252042.7%255C%2525%2520on%2520Video-Holmes%2520%2528surpassing%2520GPT-4o%2529%2520with%2520consistent%2520gains%2520on%2520both%2520reasoning%2520and%2520general%2520video%2520understanding%2520tasks.%2520Ablation%2520studies%2520verify%2520the%2520complementary%2520roles%2520of%2520the%2520attribution%2520signals%2520and%2520the%2520robustness%2520of%2520targeted%2520token-level%2520updates.%2520Overall%252C%2520Video-KTR%2520improves%2520accuracy%2520and%2520interpretability%252C%2520offering%2520a%2520simple%252C%2520drop-in%2520extension%2520to%2520RL%2520for%2520complex%2520video%2520reasoning.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/zywang0104/Video-KTR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-KTR%3A%20Reinforcing%20Video%20Reasoning%20via%20Key%20Token%20Attribution&entry.906535625=Ziyue%20Wang%20and%20Sheng%20Jin%20and%20Zhongrong%20Zuo%20and%20Jiawei%20Wu%20and%20Han%20Qiu%20and%20Qi%20She%20and%20Hao%20Zhang%20and%20Xudong%20Jiang&entry.1292438233=Reinforcement%20learning%20%28RL%29%20has%20shown%20strong%20potential%20for%20enhancing%20reasoning%20in%20multimodal%20large%20language%20models%2C%20yet%20existing%20video%20reasoning%20methods%20often%20rely%20on%20coarse%20sequence-level%20rewards%20or%20single-factor%20token%20selection%2C%20neglecting%20fine-grained%20links%20among%20visual%20inputs%2C%20temporal%20dynamics%2C%20and%20linguistic%20outputs%2C%20limiting%20both%20accuracy%20and%20interpretability.%20We%20propose%20Video-KTR%2C%20a%20modality-aware%20policy%20shaping%20framework%20that%20performs%20selective%2C%20token-level%20RL%20by%20combining%20three%20attribution%20signals%3A%20%281%29%20visual-aware%20tokens%20identified%20via%20counterfactual%20masking%20to%20reveal%20perceptual%20dependence%3B%20%282%29%20temporal-aware%20tokens%20detected%20through%20frame%20shuffling%20to%20expose%20temporal%20sensitivity%3B%20and%20%283%29%20high-entropy%20tokens%20signaling%20predictive%20uncertainty.%20By%20reinforcing%20only%20these%20key%20tokens%2C%20Video-KTR%20focuses%20learning%20on%20semantically%20informative%2C%20modality-sensitive%20content%20while%20filtering%20out%20low-value%20tokens.%20Across%20five%20challenging%20benchmarks%2C%20Video-KTR%20achieves%20state-of-the-art%20or%20highly%20competitive%20results%2C%20achieving%2042.7%5C%25%20on%20Video-Holmes%20%28surpassing%20GPT-4o%29%20with%20consistent%20gains%20on%20both%20reasoning%20and%20general%20video%20understanding%20tasks.%20Ablation%20studies%20verify%20the%20complementary%20roles%20of%20the%20attribution%20signals%20and%20the%20robustness%20of%20targeted%20token-level%20updates.%20Overall%2C%20Video-KTR%20improves%20accuracy%20and%20interpretability%2C%20offering%20a%20simple%2C%20drop-in%20extension%20to%20RL%20for%20complex%20video%20reasoning.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/zywang0104/Video-KTR.&entry.1838667208=http%3A//arxiv.org/abs/2601.19686v1&entry.124074799=Read"},
{"title": "Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework", "author": "Hao Chang and Zhihui Wang and Lingxiang Wu and Peijin Wang and Wenhui Diao and Jinqiao Wang", "abstract": "Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.", "link": "http://arxiv.org/abs/2601.19640v1", "date": "2026-01-27", "relevancy": 2.6819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Governance-Oriented%20Low-Altitude%20Intelligence%3A%20A%20Management-Centric%20Multi-Modal%20Benchmark%20With%20Implicitly%20Coordinated%20Vision-Language%20Reasoning%20Framework&body=Title%3A%20Towards%20Governance-Oriented%20Low-Altitude%20Intelligence%3A%20A%20Management-Centric%20Multi-Modal%20Benchmark%20With%20Implicitly%20Coordinated%20Vision-Language%20Reasoning%20Framework%0AAuthor%3A%20Hao%20Chang%20and%20Zhihui%20Wang%20and%20Lingxiang%20Wu%20and%20Peijin%20Wang%20and%20Wenhui%20Diao%20and%20Jinqiao%20Wang%0AAbstract%3A%20Low-altitude%20vision%20systems%20are%20becoming%20a%20critical%20infrastructure%20for%20smart%20city%20governance.%20However%2C%20existing%20object-centric%20perception%20paradigms%20and%20loosely%20coupled%20vision-language%20pipelines%20are%20still%20difficult%20to%20support%20management-oriented%20anomaly%20understanding%20required%20in%20real-world%20urban%20governance.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GovLA-10K%2C%20the%20first%20management-oriented%20multi-modal%20benchmark%20for%20low-altitude%20intelligence%2C%20along%20with%20GovLA-Reasoner%2C%20a%20unified%20vision-language%20reasoning%20framework%20tailored%20for%20governance-aware%20aerial%20perception.%20Unlike%20existing%20studies%20that%20aim%20to%20exhaustively%20annotate%20all%20visible%20objects%2C%20GovLA-10K%20is%20deliberately%20designed%20around%20functionally%20salient%20targets%20that%20directly%20correspond%20to%20practical%20management%20needs%2C%20and%20further%20provides%20actionable%20management%20suggestions%20grounded%20in%20these%20observations.%20To%20effectively%20coordinate%20the%20fine-grained%20visual%20grounding%20with%20high-level%20contextual%20language%20reasoning%2C%20GovLA-Reasoner%20introduces%20an%20efficient%20feature%20adapter%20that%20implicitly%20coordinates%20discriminative%20representation%20sharing%20between%20the%20visual%20detector%20and%20the%20large%20language%20model%20%28LLM%29.%20Extensive%20experiments%20show%20that%20our%20method%20significantly%20improves%20performance%20while%20avoiding%20the%20need%20of%20fine-tuning%20for%20any%20task-specific%20individual%20components.%20We%20believe%20our%20work%20offers%20a%20new%20perspective%20and%20foundation%20for%20future%20studies%20on%20management-aware%20low-altitude%20vision-language%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Governance-Oriented%2520Low-Altitude%2520Intelligence%253A%2520A%2520Management-Centric%2520Multi-Modal%2520Benchmark%2520With%2520Implicitly%2520Coordinated%2520Vision-Language%2520Reasoning%2520Framework%26entry.906535625%3DHao%2520Chang%2520and%2520Zhihui%2520Wang%2520and%2520Lingxiang%2520Wu%2520and%2520Peijin%2520Wang%2520and%2520Wenhui%2520Diao%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3DLow-altitude%2520vision%2520systems%2520are%2520becoming%2520a%2520critical%2520infrastructure%2520for%2520smart%2520city%2520governance.%2520However%252C%2520existing%2520object-centric%2520perception%2520paradigms%2520and%2520loosely%2520coupled%2520vision-language%2520pipelines%2520are%2520still%2520difficult%2520to%2520support%2520management-oriented%2520anomaly%2520understanding%2520required%2520in%2520real-world%2520urban%2520governance.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520GovLA-10K%252C%2520the%2520first%2520management-oriented%2520multi-modal%2520benchmark%2520for%2520low-altitude%2520intelligence%252C%2520along%2520with%2520GovLA-Reasoner%252C%2520a%2520unified%2520vision-language%2520reasoning%2520framework%2520tailored%2520for%2520governance-aware%2520aerial%2520perception.%2520Unlike%2520existing%2520studies%2520that%2520aim%2520to%2520exhaustively%2520annotate%2520all%2520visible%2520objects%252C%2520GovLA-10K%2520is%2520deliberately%2520designed%2520around%2520functionally%2520salient%2520targets%2520that%2520directly%2520correspond%2520to%2520practical%2520management%2520needs%252C%2520and%2520further%2520provides%2520actionable%2520management%2520suggestions%2520grounded%2520in%2520these%2520observations.%2520To%2520effectively%2520coordinate%2520the%2520fine-grained%2520visual%2520grounding%2520with%2520high-level%2520contextual%2520language%2520reasoning%252C%2520GovLA-Reasoner%2520introduces%2520an%2520efficient%2520feature%2520adapter%2520that%2520implicitly%2520coordinates%2520discriminative%2520representation%2520sharing%2520between%2520the%2520visual%2520detector%2520and%2520the%2520large%2520language%2520model%2520%2528LLM%2529.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520significantly%2520improves%2520performance%2520while%2520avoiding%2520the%2520need%2520of%2520fine-tuning%2520for%2520any%2520task-specific%2520individual%2520components.%2520We%2520believe%2520our%2520work%2520offers%2520a%2520new%2520perspective%2520and%2520foundation%2520for%2520future%2520studies%2520on%2520management-aware%2520low-altitude%2520vision-language%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Governance-Oriented%20Low-Altitude%20Intelligence%3A%20A%20Management-Centric%20Multi-Modal%20Benchmark%20With%20Implicitly%20Coordinated%20Vision-Language%20Reasoning%20Framework&entry.906535625=Hao%20Chang%20and%20Zhihui%20Wang%20and%20Lingxiang%20Wu%20and%20Peijin%20Wang%20and%20Wenhui%20Diao%20and%20Jinqiao%20Wang&entry.1292438233=Low-altitude%20vision%20systems%20are%20becoming%20a%20critical%20infrastructure%20for%20smart%20city%20governance.%20However%2C%20existing%20object-centric%20perception%20paradigms%20and%20loosely%20coupled%20vision-language%20pipelines%20are%20still%20difficult%20to%20support%20management-oriented%20anomaly%20understanding%20required%20in%20real-world%20urban%20governance.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GovLA-10K%2C%20the%20first%20management-oriented%20multi-modal%20benchmark%20for%20low-altitude%20intelligence%2C%20along%20with%20GovLA-Reasoner%2C%20a%20unified%20vision-language%20reasoning%20framework%20tailored%20for%20governance-aware%20aerial%20perception.%20Unlike%20existing%20studies%20that%20aim%20to%20exhaustively%20annotate%20all%20visible%20objects%2C%20GovLA-10K%20is%20deliberately%20designed%20around%20functionally%20salient%20targets%20that%20directly%20correspond%20to%20practical%20management%20needs%2C%20and%20further%20provides%20actionable%20management%20suggestions%20grounded%20in%20these%20observations.%20To%20effectively%20coordinate%20the%20fine-grained%20visual%20grounding%20with%20high-level%20contextual%20language%20reasoning%2C%20GovLA-Reasoner%20introduces%20an%20efficient%20feature%20adapter%20that%20implicitly%20coordinates%20discriminative%20representation%20sharing%20between%20the%20visual%20detector%20and%20the%20large%20language%20model%20%28LLM%29.%20Extensive%20experiments%20show%20that%20our%20method%20significantly%20improves%20performance%20while%20avoiding%20the%20need%20of%20fine-tuning%20for%20any%20task-specific%20individual%20components.%20We%20believe%20our%20work%20offers%20a%20new%20perspective%20and%20foundation%20for%20future%20studies%20on%20management-aware%20low-altitude%20vision-language%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.19640v1&entry.124074799=Read"},
{"title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance", "author": "Haozhi Zhu and Miaomiao Zhao and Dingyao Liu and Runze Tian and Yan Zhang and Jie Guo and Fenggen Yu", "abstract": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.", "link": "http://arxiv.org/abs/2601.19785v1", "date": "2026-01-27", "relevancy": 2.6494, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6684}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6611}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoDiff3D%3A%20Self-Supervised%203D%20Scene%20Generation%20with%20Geometry-Constrained%202D%20Diffusion%20Guidance&body=Title%3A%20GeoDiff3D%3A%20Self-Supervised%203D%20Scene%20Generation%20with%20Geometry-Constrained%202D%20Diffusion%20Guidance%0AAuthor%3A%20Haozhi%20Zhu%20and%20Miaomiao%20Zhao%20and%20Dingyao%20Liu%20and%20Runze%20Tian%20and%20Yan%20Zhang%20and%20Jie%20Guo%20and%20Fenggen%20Yu%0AAbstract%3A%203D%20scene%20generation%20is%20a%20core%20technology%20for%20gaming%2C%20film/VFX%2C%20and%20VR/AR.%20Growing%20demand%20for%20rapid%20iteration%2C%20high-fidelity%20detail%2C%20and%20accessible%20content%20creation%20has%20further%20increased%20interest%20in%20this%20area.%20Existing%20methods%20broadly%20follow%20two%20paradigms%20-%20indirect%202D-to-3D%20reconstruction%20and%20direct%203D%20generation%20-%20but%20both%20are%20limited%20by%20weak%20structural%20modeling%20and%20heavy%20reliance%20on%20large-scale%20ground-truth%20supervision%2C%20often%20producing%20structural%20artifacts%2C%20geometric%20inconsistencies%2C%20and%20degraded%20high-frequency%20details%20in%20complex%20scenes.%20We%20propose%20GeoDiff3D%2C%20an%20efficient%20self-supervised%20framework%20that%20uses%20coarse%20geometry%20as%20a%20structural%20anchor%20and%20a%20geometry-constrained%202D%20diffusion%20model%20to%20provide%20texture-rich%20reference%20images.%20Importantly%2C%20GeoDiff3D%20does%20not%20require%20strict%20multi-view%20consistency%20of%20the%20diffusion-generated%20references%20and%20remains%20robust%20to%20the%20resulting%20noisy%2C%20inconsistent%20guidance.%20We%20further%20introduce%20voxel-aligned%203D%20feature%20aggregation%20and%20dual%20self-supervision%20to%20maintain%20scene%20coherence%20and%20fine%20details%20while%20substantially%20reducing%20dependence%20on%20labeled%20data.%20GeoDiff3D%20also%20trains%20with%20low%20computational%20cost%20and%20enables%20fast%2C%20high-quality%203D%20scene%20generation.%20Extensive%20experiments%20on%20challenging%20scenes%20show%20improved%20generalization%20and%20generation%20quality%20over%20existing%20baselines%2C%20offering%20a%20practical%20solution%20for%20accessible%20and%20efficient%203D%20scene%20construction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoDiff3D%253A%2520Self-Supervised%25203D%2520Scene%2520Generation%2520with%2520Geometry-Constrained%25202D%2520Diffusion%2520Guidance%26entry.906535625%3DHaozhi%2520Zhu%2520and%2520Miaomiao%2520Zhao%2520and%2520Dingyao%2520Liu%2520and%2520Runze%2520Tian%2520and%2520Yan%2520Zhang%2520and%2520Jie%2520Guo%2520and%2520Fenggen%2520Yu%26entry.1292438233%3D3D%2520scene%2520generation%2520is%2520a%2520core%2520technology%2520for%2520gaming%252C%2520film/VFX%252C%2520and%2520VR/AR.%2520Growing%2520demand%2520for%2520rapid%2520iteration%252C%2520high-fidelity%2520detail%252C%2520and%2520accessible%2520content%2520creation%2520has%2520further%2520increased%2520interest%2520in%2520this%2520area.%2520Existing%2520methods%2520broadly%2520follow%2520two%2520paradigms%2520-%2520indirect%25202D-to-3D%2520reconstruction%2520and%2520direct%25203D%2520generation%2520-%2520but%2520both%2520are%2520limited%2520by%2520weak%2520structural%2520modeling%2520and%2520heavy%2520reliance%2520on%2520large-scale%2520ground-truth%2520supervision%252C%2520often%2520producing%2520structural%2520artifacts%252C%2520geometric%2520inconsistencies%252C%2520and%2520degraded%2520high-frequency%2520details%2520in%2520complex%2520scenes.%2520We%2520propose%2520GeoDiff3D%252C%2520an%2520efficient%2520self-supervised%2520framework%2520that%2520uses%2520coarse%2520geometry%2520as%2520a%2520structural%2520anchor%2520and%2520a%2520geometry-constrained%25202D%2520diffusion%2520model%2520to%2520provide%2520texture-rich%2520reference%2520images.%2520Importantly%252C%2520GeoDiff3D%2520does%2520not%2520require%2520strict%2520multi-view%2520consistency%2520of%2520the%2520diffusion-generated%2520references%2520and%2520remains%2520robust%2520to%2520the%2520resulting%2520noisy%252C%2520inconsistent%2520guidance.%2520We%2520further%2520introduce%2520voxel-aligned%25203D%2520feature%2520aggregation%2520and%2520dual%2520self-supervision%2520to%2520maintain%2520scene%2520coherence%2520and%2520fine%2520details%2520while%2520substantially%2520reducing%2520dependence%2520on%2520labeled%2520data.%2520GeoDiff3D%2520also%2520trains%2520with%2520low%2520computational%2520cost%2520and%2520enables%2520fast%252C%2520high-quality%25203D%2520scene%2520generation.%2520Extensive%2520experiments%2520on%2520challenging%2520scenes%2520show%2520improved%2520generalization%2520and%2520generation%2520quality%2520over%2520existing%2520baselines%252C%2520offering%2520a%2520practical%2520solution%2520for%2520accessible%2520and%2520efficient%25203D%2520scene%2520construction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoDiff3D%3A%20Self-Supervised%203D%20Scene%20Generation%20with%20Geometry-Constrained%202D%20Diffusion%20Guidance&entry.906535625=Haozhi%20Zhu%20and%20Miaomiao%20Zhao%20and%20Dingyao%20Liu%20and%20Runze%20Tian%20and%20Yan%20Zhang%20and%20Jie%20Guo%20and%20Fenggen%20Yu&entry.1292438233=3D%20scene%20generation%20is%20a%20core%20technology%20for%20gaming%2C%20film/VFX%2C%20and%20VR/AR.%20Growing%20demand%20for%20rapid%20iteration%2C%20high-fidelity%20detail%2C%20and%20accessible%20content%20creation%20has%20further%20increased%20interest%20in%20this%20area.%20Existing%20methods%20broadly%20follow%20two%20paradigms%20-%20indirect%202D-to-3D%20reconstruction%20and%20direct%203D%20generation%20-%20but%20both%20are%20limited%20by%20weak%20structural%20modeling%20and%20heavy%20reliance%20on%20large-scale%20ground-truth%20supervision%2C%20often%20producing%20structural%20artifacts%2C%20geometric%20inconsistencies%2C%20and%20degraded%20high-frequency%20details%20in%20complex%20scenes.%20We%20propose%20GeoDiff3D%2C%20an%20efficient%20self-supervised%20framework%20that%20uses%20coarse%20geometry%20as%20a%20structural%20anchor%20and%20a%20geometry-constrained%202D%20diffusion%20model%20to%20provide%20texture-rich%20reference%20images.%20Importantly%2C%20GeoDiff3D%20does%20not%20require%20strict%20multi-view%20consistency%20of%20the%20diffusion-generated%20references%20and%20remains%20robust%20to%20the%20resulting%20noisy%2C%20inconsistent%20guidance.%20We%20further%20introduce%20voxel-aligned%203D%20feature%20aggregation%20and%20dual%20self-supervision%20to%20maintain%20scene%20coherence%20and%20fine%20details%20while%20substantially%20reducing%20dependence%20on%20labeled%20data.%20GeoDiff3D%20also%20trains%20with%20low%20computational%20cost%20and%20enables%20fast%2C%20high-quality%203D%20scene%20generation.%20Extensive%20experiments%20on%20challenging%20scenes%20show%20improved%20generalization%20and%20generation%20quality%20over%20existing%20baselines%2C%20offering%20a%20practical%20solution%20for%20accessible%20and%20efficient%203D%20scene%20construction.&entry.1838667208=http%3A//arxiv.org/abs/2601.19785v1&entry.124074799=Read"},
{"title": "The Geometric Mechanics of Contrastive Representation Learning: Alignment Potentials, Entropic Dispersion, and Cross-Modal Divergence", "author": "Yichao Cai and Zhen Zhang and Yuhang Liu and Javen Qinfeng Shi", "abstract": "While InfoNCE powers modern contrastive learning, its geometric mechanisms remain under-characterized beyond the canonical alignment--uniformity decomposition. We present a measure-theoretic framework that models learning as the evolution of representation measures on a fixed embedding manifold. By establishing value and gradient consistency in the large-batch limit, we bridge the stochastic objective to explicit deterministic energy landscapes, uncovering a fundamental geometric bifurcation between the unimodal and multimodal regimes. In the unimodal setting, the intrinsic landscape is strictly convex with a unique Gibbs equilibrium; here, entropy acts merely as a tie-breaker, clarifying \"uniformity\" as a constrained expansion within the alignment basin. In contrast, the symmetric multimodal objective contains a persistent negative symmetric divergence term that remains even after kernel sharpening. We show that this term induces barrier-driven co-adaptation, enforcing a population-level modality gap as a structural geometric necessity rather than an initialization artifact. Our results shift the analytical lens from pointwise discrimination to population geometry, offering a principled basis for diagnosing and controlling distributional misalignment.", "link": "http://arxiv.org/abs/2601.19597v1", "date": "2026-01-27", "relevancy": 2.6452, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Geometric%20Mechanics%20of%20Contrastive%20Representation%20Learning%3A%20Alignment%20Potentials%2C%20Entropic%20Dispersion%2C%20and%20Cross-Modal%20Divergence&body=Title%3A%20The%20Geometric%20Mechanics%20of%20Contrastive%20Representation%20Learning%3A%20Alignment%20Potentials%2C%20Entropic%20Dispersion%2C%20and%20Cross-Modal%20Divergence%0AAuthor%3A%20Yichao%20Cai%20and%20Zhen%20Zhang%20and%20Yuhang%20Liu%20and%20Javen%20Qinfeng%20Shi%0AAbstract%3A%20While%20InfoNCE%20powers%20modern%20contrastive%20learning%2C%20its%20geometric%20mechanisms%20remain%20under-characterized%20beyond%20the%20canonical%20alignment--uniformity%20decomposition.%20We%20present%20a%20measure-theoretic%20framework%20that%20models%20learning%20as%20the%20evolution%20of%20representation%20measures%20on%20a%20fixed%20embedding%20manifold.%20By%20establishing%20value%20and%20gradient%20consistency%20in%20the%20large-batch%20limit%2C%20we%20bridge%20the%20stochastic%20objective%20to%20explicit%20deterministic%20energy%20landscapes%2C%20uncovering%20a%20fundamental%20geometric%20bifurcation%20between%20the%20unimodal%20and%20multimodal%20regimes.%20In%20the%20unimodal%20setting%2C%20the%20intrinsic%20landscape%20is%20strictly%20convex%20with%20a%20unique%20Gibbs%20equilibrium%3B%20here%2C%20entropy%20acts%20merely%20as%20a%20tie-breaker%2C%20clarifying%20%22uniformity%22%20as%20a%20constrained%20expansion%20within%20the%20alignment%20basin.%20In%20contrast%2C%20the%20symmetric%20multimodal%20objective%20contains%20a%20persistent%20negative%20symmetric%20divergence%20term%20that%20remains%20even%20after%20kernel%20sharpening.%20We%20show%20that%20this%20term%20induces%20barrier-driven%20co-adaptation%2C%20enforcing%20a%20population-level%20modality%20gap%20as%20a%20structural%20geometric%20necessity%20rather%20than%20an%20initialization%20artifact.%20Our%20results%20shift%20the%20analytical%20lens%20from%20pointwise%20discrimination%20to%20population%20geometry%2C%20offering%20a%20principled%20basis%20for%20diagnosing%20and%20controlling%20distributional%20misalignment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Geometric%2520Mechanics%2520of%2520Contrastive%2520Representation%2520Learning%253A%2520Alignment%2520Potentials%252C%2520Entropic%2520Dispersion%252C%2520and%2520Cross-Modal%2520Divergence%26entry.906535625%3DYichao%2520Cai%2520and%2520Zhen%2520Zhang%2520and%2520Yuhang%2520Liu%2520and%2520Javen%2520Qinfeng%2520Shi%26entry.1292438233%3DWhile%2520InfoNCE%2520powers%2520modern%2520contrastive%2520learning%252C%2520its%2520geometric%2520mechanisms%2520remain%2520under-characterized%2520beyond%2520the%2520canonical%2520alignment--uniformity%2520decomposition.%2520We%2520present%2520a%2520measure-theoretic%2520framework%2520that%2520models%2520learning%2520as%2520the%2520evolution%2520of%2520representation%2520measures%2520on%2520a%2520fixed%2520embedding%2520manifold.%2520By%2520establishing%2520value%2520and%2520gradient%2520consistency%2520in%2520the%2520large-batch%2520limit%252C%2520we%2520bridge%2520the%2520stochastic%2520objective%2520to%2520explicit%2520deterministic%2520energy%2520landscapes%252C%2520uncovering%2520a%2520fundamental%2520geometric%2520bifurcation%2520between%2520the%2520unimodal%2520and%2520multimodal%2520regimes.%2520In%2520the%2520unimodal%2520setting%252C%2520the%2520intrinsic%2520landscape%2520is%2520strictly%2520convex%2520with%2520a%2520unique%2520Gibbs%2520equilibrium%253B%2520here%252C%2520entropy%2520acts%2520merely%2520as%2520a%2520tie-breaker%252C%2520clarifying%2520%2522uniformity%2522%2520as%2520a%2520constrained%2520expansion%2520within%2520the%2520alignment%2520basin.%2520In%2520contrast%252C%2520the%2520symmetric%2520multimodal%2520objective%2520contains%2520a%2520persistent%2520negative%2520symmetric%2520divergence%2520term%2520that%2520remains%2520even%2520after%2520kernel%2520sharpening.%2520We%2520show%2520that%2520this%2520term%2520induces%2520barrier-driven%2520co-adaptation%252C%2520enforcing%2520a%2520population-level%2520modality%2520gap%2520as%2520a%2520structural%2520geometric%2520necessity%2520rather%2520than%2520an%2520initialization%2520artifact.%2520Our%2520results%2520shift%2520the%2520analytical%2520lens%2520from%2520pointwise%2520discrimination%2520to%2520population%2520geometry%252C%2520offering%2520a%2520principled%2520basis%2520for%2520diagnosing%2520and%2520controlling%2520distributional%2520misalignment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Geometric%20Mechanics%20of%20Contrastive%20Representation%20Learning%3A%20Alignment%20Potentials%2C%20Entropic%20Dispersion%2C%20and%20Cross-Modal%20Divergence&entry.906535625=Yichao%20Cai%20and%20Zhen%20Zhang%20and%20Yuhang%20Liu%20and%20Javen%20Qinfeng%20Shi&entry.1292438233=While%20InfoNCE%20powers%20modern%20contrastive%20learning%2C%20its%20geometric%20mechanisms%20remain%20under-characterized%20beyond%20the%20canonical%20alignment--uniformity%20decomposition.%20We%20present%20a%20measure-theoretic%20framework%20that%20models%20learning%20as%20the%20evolution%20of%20representation%20measures%20on%20a%20fixed%20embedding%20manifold.%20By%20establishing%20value%20and%20gradient%20consistency%20in%20the%20large-batch%20limit%2C%20we%20bridge%20the%20stochastic%20objective%20to%20explicit%20deterministic%20energy%20landscapes%2C%20uncovering%20a%20fundamental%20geometric%20bifurcation%20between%20the%20unimodal%20and%20multimodal%20regimes.%20In%20the%20unimodal%20setting%2C%20the%20intrinsic%20landscape%20is%20strictly%20convex%20with%20a%20unique%20Gibbs%20equilibrium%3B%20here%2C%20entropy%20acts%20merely%20as%20a%20tie-breaker%2C%20clarifying%20%22uniformity%22%20as%20a%20constrained%20expansion%20within%20the%20alignment%20basin.%20In%20contrast%2C%20the%20symmetric%20multimodal%20objective%20contains%20a%20persistent%20negative%20symmetric%20divergence%20term%20that%20remains%20even%20after%20kernel%20sharpening.%20We%20show%20that%20this%20term%20induces%20barrier-driven%20co-adaptation%2C%20enforcing%20a%20population-level%20modality%20gap%20as%20a%20structural%20geometric%20necessity%20rather%20than%20an%20initialization%20artifact.%20Our%20results%20shift%20the%20analytical%20lens%20from%20pointwise%20discrimination%20to%20population%20geometry%2C%20offering%20a%20principled%20basis%20for%20diagnosing%20and%20controlling%20distributional%20misalignment.&entry.1838667208=http%3A//arxiv.org/abs/2601.19597v1&entry.124074799=Read"},
{"title": "Learning Dynamic Representations via An Optimally-Weighted Maximum Mean Discrepancy Optimization Framework for Continual Learning", "author": "KaiHui Huang and RunQing Wu and JinHui Sheng and HanYi Zhang and Ling Ge and JinYu Guo and Fei Ye", "abstract": "Continual learning has emerged as a pivotal area of research, primarily due to its advantageous characteristic that allows models to persistently acquire and retain information. However, catastrophic forgetting can severely impair model performance. In this study, we address network forgetting by introducing a novel framework termed Optimally-Weighted Maximum Mean Discrepancy (OWMMD), which imposes penalties on representation alterations via a Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an Adaptive Regularization Optimization (ARO) strategy to refine the adaptive weight vectors, which autonomously assess the significance of each feature layer throughout the optimization process, The proposed ARO approach can relieve the over-regularization problem and promote the future task learning. We conduct a comprehensive series of experiments, benchmarking our proposed method against several established baselines. The empirical findings indicate that our approach achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2501.12121v5", "date": "2026-01-27", "relevancy": 2.6383, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamic%20Representations%20via%20An%20Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20Optimization%20Framework%20for%20Continual%20Learning&body=Title%3A%20Learning%20Dynamic%20Representations%20via%20An%20Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20Optimization%20Framework%20for%20Continual%20Learning%0AAuthor%3A%20KaiHui%20Huang%20and%20RunQing%20Wu%20and%20JinHui%20Sheng%20and%20HanYi%20Zhang%20and%20Ling%20Ge%20and%20JinYu%20Guo%20and%20Fei%20Ye%0AAbstract%3A%20Continual%20learning%20has%20emerged%20as%20a%20pivotal%20area%20of%20research%2C%20primarily%20due%20to%20its%20advantageous%20characteristic%20that%20allows%20models%20to%20persistently%20acquire%20and%20retain%20information.%20However%2C%20catastrophic%20forgetting%20can%20severely%20impair%20model%20performance.%20In%20this%20study%2C%20we%20address%20network%20forgetting%20by%20introducing%20a%20novel%20framework%20termed%20Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20%28OWMMD%29%2C%20which%20imposes%20penalties%20on%20representation%20alterations%20via%20a%20Multi-Level%20Feature%20Matching%20Mechanism%20%28MLFMM%29.%20Furthermore%2C%20we%20propose%20an%20Adaptive%20Regularization%20Optimization%20%28ARO%29%20strategy%20to%20refine%20the%20adaptive%20weight%20vectors%2C%20which%20autonomously%20assess%20the%20significance%20of%20each%20feature%20layer%20throughout%20the%20optimization%20process%2C%20The%20proposed%20ARO%20approach%20can%20relieve%20the%20over-regularization%20problem%20and%20promote%20the%20future%20task%20learning.%20We%20conduct%20a%20comprehensive%20series%20of%20experiments%2C%20benchmarking%20our%20proposed%20method%20against%20several%20established%20baselines.%20The%20empirical%20findings%20indicate%20that%20our%20approach%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2501.12121v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamic%2520Representations%2520via%2520An%2520Optimally-Weighted%2520Maximum%2520Mean%2520Discrepancy%2520Optimization%2520Framework%2520for%2520Continual%2520Learning%26entry.906535625%3DKaiHui%2520Huang%2520and%2520RunQing%2520Wu%2520and%2520JinHui%2520Sheng%2520and%2520HanYi%2520Zhang%2520and%2520Ling%2520Ge%2520and%2520JinYu%2520Guo%2520and%2520Fei%2520Ye%26entry.1292438233%3DContinual%2520learning%2520has%2520emerged%2520as%2520a%2520pivotal%2520area%2520of%2520research%252C%2520primarily%2520due%2520to%2520its%2520advantageous%2520characteristic%2520that%2520allows%2520models%2520to%2520persistently%2520acquire%2520and%2520retain%2520information.%2520However%252C%2520catastrophic%2520forgetting%2520can%2520severely%2520impair%2520model%2520performance.%2520In%2520this%2520study%252C%2520we%2520address%2520network%2520forgetting%2520by%2520introducing%2520a%2520novel%2520framework%2520termed%2520Optimally-Weighted%2520Maximum%2520Mean%2520Discrepancy%2520%2528OWMMD%2529%252C%2520which%2520imposes%2520penalties%2520on%2520representation%2520alterations%2520via%2520a%2520Multi-Level%2520Feature%2520Matching%2520Mechanism%2520%2528MLFMM%2529.%2520Furthermore%252C%2520we%2520propose%2520an%2520Adaptive%2520Regularization%2520Optimization%2520%2528ARO%2529%2520strategy%2520to%2520refine%2520the%2520adaptive%2520weight%2520vectors%252C%2520which%2520autonomously%2520assess%2520the%2520significance%2520of%2520each%2520feature%2520layer%2520throughout%2520the%2520optimization%2520process%252C%2520The%2520proposed%2520ARO%2520approach%2520can%2520relieve%2520the%2520over-regularization%2520problem%2520and%2520promote%2520the%2520future%2520task%2520learning.%2520We%2520conduct%2520a%2520comprehensive%2520series%2520of%2520experiments%252C%2520benchmarking%2520our%2520proposed%2520method%2520against%2520several%2520established%2520baselines.%2520The%2520empirical%2520findings%2520indicate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12121v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamic%20Representations%20via%20An%20Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20Optimization%20Framework%20for%20Continual%20Learning&entry.906535625=KaiHui%20Huang%20and%20RunQing%20Wu%20and%20JinHui%20Sheng%20and%20HanYi%20Zhang%20and%20Ling%20Ge%20and%20JinYu%20Guo%20and%20Fei%20Ye&entry.1292438233=Continual%20learning%20has%20emerged%20as%20a%20pivotal%20area%20of%20research%2C%20primarily%20due%20to%20its%20advantageous%20characteristic%20that%20allows%20models%20to%20persistently%20acquire%20and%20retain%20information.%20However%2C%20catastrophic%20forgetting%20can%20severely%20impair%20model%20performance.%20In%20this%20study%2C%20we%20address%20network%20forgetting%20by%20introducing%20a%20novel%20framework%20termed%20Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20%28OWMMD%29%2C%20which%20imposes%20penalties%20on%20representation%20alterations%20via%20a%20Multi-Level%20Feature%20Matching%20Mechanism%20%28MLFMM%29.%20Furthermore%2C%20we%20propose%20an%20Adaptive%20Regularization%20Optimization%20%28ARO%29%20strategy%20to%20refine%20the%20adaptive%20weight%20vectors%2C%20which%20autonomously%20assess%20the%20significance%20of%20each%20feature%20layer%20throughout%20the%20optimization%20process%2C%20The%20proposed%20ARO%20approach%20can%20relieve%20the%20over-regularization%20problem%20and%20promote%20the%20future%20task%20learning.%20We%20conduct%20a%20comprehensive%20series%20of%20experiments%2C%20benchmarking%20our%20proposed%20method%20against%20several%20established%20baselines.%20The%20empirical%20findings%20indicate%20that%20our%20approach%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2501.12121v5&entry.124074799=Read"},
{"title": "ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting", "author": "Abhijit Mishra and Mingda Li and Hsiang Fu and Richard Noh and Minji Kim", "abstract": "Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.", "link": "http://arxiv.org/abs/2502.14780v3", "date": "2026-01-27", "relevancy": 2.6019, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReVision%3A%20A%20Dataset%20and%20Baseline%20VLM%20for%20Privacy-Preserving%20Task-Oriented%20Visual%20Instruction%20Rewriting&body=Title%3A%20ReVision%3A%20A%20Dataset%20and%20Baseline%20VLM%20for%20Privacy-Preserving%20Task-Oriented%20Visual%20Instruction%20Rewriting%0AAuthor%3A%20Abhijit%20Mishra%20and%20Mingda%20Li%20and%20Hsiang%20Fu%20and%20Richard%20Noh%20and%20Minji%20Kim%0AAbstract%3A%20Efficient%20and%20privacy-preserving%20multimodal%20interaction%20is%20essential%20as%20AR%2C%20VR%2C%20and%20modern%20smartphones%20with%20powerful%20cameras%20become%20primary%20interfaces%20for%20human-computer%20communication.%20Existing%20powerful%20large%20vision-language%20models%20%28VLMs%29%20enabling%20multimodal%20interaction%20often%20rely%20on%20cloud-based%20processing%2C%20raising%20significant%20concerns%20about%20%281%29%20visual%20privacy%20by%20transmitting%20sensitive%20vision%20data%20to%20servers%2C%20and%20%282%29%20their%20limited%20real-time%2C%20on-device%20usability.%20This%20paper%20explores%20Visual%20Instruction%20Rewriting%2C%20a%20novel%20approach%20that%20transforms%20multimodal%20instructions%20into%20text-only%20commands%2C%20allowing%20seamless%20integration%20of%20lightweight%20on-device%20instruction%20rewriter%20VLMs%20%28250M%20parameters%29%20with%20existing%20conversational%20AI%20systems%2C%20enhancing%20vision%20data%20privacy.%20To%20achieve%20this%2C%20we%20present%20a%20dataset%20of%20over%2039%2C000%20examples%20across%2014%20domains%20and%20develop%20a%20compact%20VLM%2C%20pretrained%20on%20image%20captioning%20datasets%20and%20fine-tuned%20for%20instruction%20rewriting.%20Experimental%20results%2C%20evaluated%20through%20NLG%20metrics%20such%20as%20BLEU%2C%20METEOR%2C%20and%20ROUGE%2C%20along%20with%20semantic%20parsing%20analysis%2C%20demonstrate%20that%20even%20a%20quantized%20version%20of%20the%20model%20%28%3C500MB%20storage%20footprint%29%20can%20achieve%20effective%20instruction%20rewriting%2C%20thus%20enabling%20privacy-focused%2C%20multimodal%20AI%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2502.14780v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReVision%253A%2520A%2520Dataset%2520and%2520Baseline%2520VLM%2520for%2520Privacy-Preserving%2520Task-Oriented%2520Visual%2520Instruction%2520Rewriting%26entry.906535625%3DAbhijit%2520Mishra%2520and%2520Mingda%2520Li%2520and%2520Hsiang%2520Fu%2520and%2520Richard%2520Noh%2520and%2520Minji%2520Kim%26entry.1292438233%3DEfficient%2520and%2520privacy-preserving%2520multimodal%2520interaction%2520is%2520essential%2520as%2520AR%252C%2520VR%252C%2520and%2520modern%2520smartphones%2520with%2520powerful%2520cameras%2520become%2520primary%2520interfaces%2520for%2520human-computer%2520communication.%2520Existing%2520powerful%2520large%2520vision-language%2520models%2520%2528VLMs%2529%2520enabling%2520multimodal%2520interaction%2520often%2520rely%2520on%2520cloud-based%2520processing%252C%2520raising%2520significant%2520concerns%2520about%2520%25281%2529%2520visual%2520privacy%2520by%2520transmitting%2520sensitive%2520vision%2520data%2520to%2520servers%252C%2520and%2520%25282%2529%2520their%2520limited%2520real-time%252C%2520on-device%2520usability.%2520This%2520paper%2520explores%2520Visual%2520Instruction%2520Rewriting%252C%2520a%2520novel%2520approach%2520that%2520transforms%2520multimodal%2520instructions%2520into%2520text-only%2520commands%252C%2520allowing%2520seamless%2520integration%2520of%2520lightweight%2520on-device%2520instruction%2520rewriter%2520VLMs%2520%2528250M%2520parameters%2529%2520with%2520existing%2520conversational%2520AI%2520systems%252C%2520enhancing%2520vision%2520data%2520privacy.%2520To%2520achieve%2520this%252C%2520we%2520present%2520a%2520dataset%2520of%2520over%252039%252C000%2520examples%2520across%252014%2520domains%2520and%2520develop%2520a%2520compact%2520VLM%252C%2520pretrained%2520on%2520image%2520captioning%2520datasets%2520and%2520fine-tuned%2520for%2520instruction%2520rewriting.%2520Experimental%2520results%252C%2520evaluated%2520through%2520NLG%2520metrics%2520such%2520as%2520BLEU%252C%2520METEOR%252C%2520and%2520ROUGE%252C%2520along%2520with%2520semantic%2520parsing%2520analysis%252C%2520demonstrate%2520that%2520even%2520a%2520quantized%2520version%2520of%2520the%2520model%2520%2528%253C500MB%2520storage%2520footprint%2529%2520can%2520achieve%2520effective%2520instruction%2520rewriting%252C%2520thus%2520enabling%2520privacy-focused%252C%2520multimodal%2520AI%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14780v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReVision%3A%20A%20Dataset%20and%20Baseline%20VLM%20for%20Privacy-Preserving%20Task-Oriented%20Visual%20Instruction%20Rewriting&entry.906535625=Abhijit%20Mishra%20and%20Mingda%20Li%20and%20Hsiang%20Fu%20and%20Richard%20Noh%20and%20Minji%20Kim&entry.1292438233=Efficient%20and%20privacy-preserving%20multimodal%20interaction%20is%20essential%20as%20AR%2C%20VR%2C%20and%20modern%20smartphones%20with%20powerful%20cameras%20become%20primary%20interfaces%20for%20human-computer%20communication.%20Existing%20powerful%20large%20vision-language%20models%20%28VLMs%29%20enabling%20multimodal%20interaction%20often%20rely%20on%20cloud-based%20processing%2C%20raising%20significant%20concerns%20about%20%281%29%20visual%20privacy%20by%20transmitting%20sensitive%20vision%20data%20to%20servers%2C%20and%20%282%29%20their%20limited%20real-time%2C%20on-device%20usability.%20This%20paper%20explores%20Visual%20Instruction%20Rewriting%2C%20a%20novel%20approach%20that%20transforms%20multimodal%20instructions%20into%20text-only%20commands%2C%20allowing%20seamless%20integration%20of%20lightweight%20on-device%20instruction%20rewriter%20VLMs%20%28250M%20parameters%29%20with%20existing%20conversational%20AI%20systems%2C%20enhancing%20vision%20data%20privacy.%20To%20achieve%20this%2C%20we%20present%20a%20dataset%20of%20over%2039%2C000%20examples%20across%2014%20domains%20and%20develop%20a%20compact%20VLM%2C%20pretrained%20on%20image%20captioning%20datasets%20and%20fine-tuned%20for%20instruction%20rewriting.%20Experimental%20results%2C%20evaluated%20through%20NLG%20metrics%20such%20as%20BLEU%2C%20METEOR%2C%20and%20ROUGE%2C%20along%20with%20semantic%20parsing%20analysis%2C%20demonstrate%20that%20even%20a%20quantized%20version%20of%20the%20model%20%28%3C500MB%20storage%20footprint%29%20can%20achieve%20effective%20instruction%20rewriting%2C%20thus%20enabling%20privacy-focused%2C%20multimodal%20AI%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2502.14780v3&entry.124074799=Read"},
{"title": "LVLMs and Humans Ground Differently in Referential Communication", "author": "Peter Zeng and Weiling Li and Amie Paige and Zhengxiang Wang and Panagiotis Kaliosis and Dimitris Samaras and Gregory Zelinsky and Susan Brennan and Owen Rambow", "abstract": "For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.", "link": "http://arxiv.org/abs/2601.19792v1", "date": "2026-01-27", "relevancy": 2.6001, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVLMs%20and%20Humans%20Ground%20Differently%20in%20Referential%20Communication&body=Title%3A%20LVLMs%20and%20Humans%20Ground%20Differently%20in%20Referential%20Communication%0AAuthor%3A%20Peter%20Zeng%20and%20Weiling%20Li%20and%20Amie%20Paige%20and%20Zhengxiang%20Wang%20and%20Panagiotis%20Kaliosis%20and%20Dimitris%20Samaras%20and%20Gregory%20Zelinsky%20and%20Susan%20Brennan%20and%20Owen%20Rambow%0AAbstract%3A%20For%20generative%20AI%20agents%20to%20partner%20effectively%20with%20human%20users%2C%20the%20ability%20to%20accurately%20predict%20human%20intent%20is%20critical.%20But%20this%20ability%20to%20collaborate%20remains%20limited%20by%20a%20critical%20deficit%3A%20an%20inability%20to%20model%20common%20ground.%20Here%2C%20we%20present%20a%20referential%20communication%20experiment%20with%20a%20factorial%20design%20involving%20director-matcher%20pairs%20%28human-human%2C%20human-AI%2C%20AI-human%2C%20and%20AI-AI%29%20that%20interact%20with%20multiple%20turns%20in%20repeated%20rounds%20to%20match%20pictures%20of%20objects%20not%20associated%20with%20any%20obvious%20lexicalized%20labels.%20We%20release%20the%20online%20pipeline%20for%20data%20collection%2C%20the%20tools%20and%20analyses%20for%20accuracy%2C%20efficiency%2C%20and%20lexical%20overlap%2C%20and%20a%20corpus%20of%20356%20dialogues%20%2889%20pairs%20over%204%20rounds%20each%29%20that%20unmasks%20LVLMs%27%20limitations%20in%20interactively%20resolving%20referring%20expressions%2C%20a%20crucial%20skill%20that%20underlies%20human%20language%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVLMs%2520and%2520Humans%2520Ground%2520Differently%2520in%2520Referential%2520Communication%26entry.906535625%3DPeter%2520Zeng%2520and%2520Weiling%2520Li%2520and%2520Amie%2520Paige%2520and%2520Zhengxiang%2520Wang%2520and%2520Panagiotis%2520Kaliosis%2520and%2520Dimitris%2520Samaras%2520and%2520Gregory%2520Zelinsky%2520and%2520Susan%2520Brennan%2520and%2520Owen%2520Rambow%26entry.1292438233%3DFor%2520generative%2520AI%2520agents%2520to%2520partner%2520effectively%2520with%2520human%2520users%252C%2520the%2520ability%2520to%2520accurately%2520predict%2520human%2520intent%2520is%2520critical.%2520But%2520this%2520ability%2520to%2520collaborate%2520remains%2520limited%2520by%2520a%2520critical%2520deficit%253A%2520an%2520inability%2520to%2520model%2520common%2520ground.%2520Here%252C%2520we%2520present%2520a%2520referential%2520communication%2520experiment%2520with%2520a%2520factorial%2520design%2520involving%2520director-matcher%2520pairs%2520%2528human-human%252C%2520human-AI%252C%2520AI-human%252C%2520and%2520AI-AI%2529%2520that%2520interact%2520with%2520multiple%2520turns%2520in%2520repeated%2520rounds%2520to%2520match%2520pictures%2520of%2520objects%2520not%2520associated%2520with%2520any%2520obvious%2520lexicalized%2520labels.%2520We%2520release%2520the%2520online%2520pipeline%2520for%2520data%2520collection%252C%2520the%2520tools%2520and%2520analyses%2520for%2520accuracy%252C%2520efficiency%252C%2520and%2520lexical%2520overlap%252C%2520and%2520a%2520corpus%2520of%2520356%2520dialogues%2520%252889%2520pairs%2520over%25204%2520rounds%2520each%2529%2520that%2520unmasks%2520LVLMs%2527%2520limitations%2520in%2520interactively%2520resolving%2520referring%2520expressions%252C%2520a%2520crucial%2520skill%2520that%2520underlies%2520human%2520language%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVLMs%20and%20Humans%20Ground%20Differently%20in%20Referential%20Communication&entry.906535625=Peter%20Zeng%20and%20Weiling%20Li%20and%20Amie%20Paige%20and%20Zhengxiang%20Wang%20and%20Panagiotis%20Kaliosis%20and%20Dimitris%20Samaras%20and%20Gregory%20Zelinsky%20and%20Susan%20Brennan%20and%20Owen%20Rambow&entry.1292438233=For%20generative%20AI%20agents%20to%20partner%20effectively%20with%20human%20users%2C%20the%20ability%20to%20accurately%20predict%20human%20intent%20is%20critical.%20But%20this%20ability%20to%20collaborate%20remains%20limited%20by%20a%20critical%20deficit%3A%20an%20inability%20to%20model%20common%20ground.%20Here%2C%20we%20present%20a%20referential%20communication%20experiment%20with%20a%20factorial%20design%20involving%20director-matcher%20pairs%20%28human-human%2C%20human-AI%2C%20AI-human%2C%20and%20AI-AI%29%20that%20interact%20with%20multiple%20turns%20in%20repeated%20rounds%20to%20match%20pictures%20of%20objects%20not%20associated%20with%20any%20obvious%20lexicalized%20labels.%20We%20release%20the%20online%20pipeline%20for%20data%20collection%2C%20the%20tools%20and%20analyses%20for%20accuracy%2C%20efficiency%2C%20and%20lexical%20overlap%2C%20and%20a%20corpus%20of%20356%20dialogues%20%2889%20pairs%20over%204%20rounds%20each%29%20that%20unmasks%20LVLMs%27%20limitations%20in%20interactively%20resolving%20referring%20expressions%2C%20a%20crucial%20skill%20that%20underlies%20human%20language%20use.&entry.1838667208=http%3A//arxiv.org/abs/2601.19792v1&entry.124074799=Read"},
{"title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming", "author": "Jisheng Chu and Wenrui Li and Rui Zhao and Wangmeng Zuo and Shifeng Chen and Xiaopeng Fan", "abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.", "link": "http://arxiv.org/abs/2601.19433v1", "date": "2026-01-27", "relevancy": 2.595, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.7182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoamScene3D%3A%20Immersive%20Text-to-3D%20Scene%20Generation%20via%20Adaptive%20Object-aware%20Roaming&body=Title%3A%20RoamScene3D%3A%20Immersive%20Text-to-3D%20Scene%20Generation%20via%20Adaptive%20Object-aware%20Roaming%0AAuthor%3A%20Jisheng%20Chu%20and%20Wenrui%20Li%20and%20Rui%20Zhao%20and%20Wangmeng%20Zuo%20and%20Shifeng%20Chen%20and%20Xiaopeng%20Fan%0AAbstract%3A%20Generating%20immersive%203D%20scenes%20from%20texts%20is%20a%20core%20task%20in%20computer%20vision%2C%20crucial%20for%20applications%20in%20virtual%20reality%20and%20game%20development.%20Despite%20the%20promise%20of%20leveraging%202D%20diffusion%20priors%2C%20existing%20methods%20suffer%20from%20spatial%20blindness%20and%20rely%20on%20predefined%20trajectories%20that%20fail%20to%20exploit%20the%20inner%20relationships%20among%20salient%20objects.%20Consequently%2C%20these%20approaches%20are%20unable%20to%20comprehend%20the%20semantic%20layout%2C%20preventing%20them%20from%20exploring%20the%20scene%20adaptively%20to%20infer%20occluded%20content.%20Moreover%2C%20current%20inpainting%20models%20operate%20in%202D%20image%20space%2C%20struggling%20to%20plausibly%20fill%20holes%20caused%20by%20camera%20motion.%20To%20address%20these%20limitations%2C%20we%20propose%20RoamScene3D%2C%20a%20novel%20framework%20that%20bridges%20the%20gap%20between%20semantic%20guidance%20and%20spatial%20generation.%20Our%20method%20reasons%20about%20the%20semantic%20relations%20among%20objects%20and%20produces%20consistent%20and%20photorealistic%20scenes.%20Specifically%2C%20we%20employ%20a%20vision-language%20model%20%28VLM%29%20to%20construct%20a%20scene%20graph%20that%20encodes%20object%20relations%2C%20guiding%20the%20camera%20to%20perceive%20salient%20object%20boundaries%20and%20plan%20an%20adaptive%20roaming%20trajectory.%20Furthermore%2C%20to%20mitigate%20the%20limitations%20of%20static%202D%20priors%2C%20we%20introduce%20a%20Motion-Injected%20Inpainting%20model%20that%20is%20fine-tuned%20on%20a%20synthetic%20panoramic%20dataset%20integrating%20authentic%20camera%20trajectories%2C%20making%20it%20adaptive%20to%20camera%20motion.%20Extensive%20experiments%20demonstrate%20that%20with%20semantic%20reasoning%20and%20geometric%20constraints%2C%20our%20method%20significantly%20outperforms%20state-of-the-art%20approaches%20in%20producing%20consistent%20and%20photorealistic%20scenes.%20Our%20code%20is%20available%20at%20https%3A//github.com/JS-CHU/RoamScene3D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoamScene3D%253A%2520Immersive%2520Text-to-3D%2520Scene%2520Generation%2520via%2520Adaptive%2520Object-aware%2520Roaming%26entry.906535625%3DJisheng%2520Chu%2520and%2520Wenrui%2520Li%2520and%2520Rui%2520Zhao%2520and%2520Wangmeng%2520Zuo%2520and%2520Shifeng%2520Chen%2520and%2520Xiaopeng%2520Fan%26entry.1292438233%3DGenerating%2520immersive%25203D%2520scenes%2520from%2520texts%2520is%2520a%2520core%2520task%2520in%2520computer%2520vision%252C%2520crucial%2520for%2520applications%2520in%2520virtual%2520reality%2520and%2520game%2520development.%2520Despite%2520the%2520promise%2520of%2520leveraging%25202D%2520diffusion%2520priors%252C%2520existing%2520methods%2520suffer%2520from%2520spatial%2520blindness%2520and%2520rely%2520on%2520predefined%2520trajectories%2520that%2520fail%2520to%2520exploit%2520the%2520inner%2520relationships%2520among%2520salient%2520objects.%2520Consequently%252C%2520these%2520approaches%2520are%2520unable%2520to%2520comprehend%2520the%2520semantic%2520layout%252C%2520preventing%2520them%2520from%2520exploring%2520the%2520scene%2520adaptively%2520to%2520infer%2520occluded%2520content.%2520Moreover%252C%2520current%2520inpainting%2520models%2520operate%2520in%25202D%2520image%2520space%252C%2520struggling%2520to%2520plausibly%2520fill%2520holes%2520caused%2520by%2520camera%2520motion.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520RoamScene3D%252C%2520a%2520novel%2520framework%2520that%2520bridges%2520the%2520gap%2520between%2520semantic%2520guidance%2520and%2520spatial%2520generation.%2520Our%2520method%2520reasons%2520about%2520the%2520semantic%2520relations%2520among%2520objects%2520and%2520produces%2520consistent%2520and%2520photorealistic%2520scenes.%2520Specifically%252C%2520we%2520employ%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520construct%2520a%2520scene%2520graph%2520that%2520encodes%2520object%2520relations%252C%2520guiding%2520the%2520camera%2520to%2520perceive%2520salient%2520object%2520boundaries%2520and%2520plan%2520an%2520adaptive%2520roaming%2520trajectory.%2520Furthermore%252C%2520to%2520mitigate%2520the%2520limitations%2520of%2520static%25202D%2520priors%252C%2520we%2520introduce%2520a%2520Motion-Injected%2520Inpainting%2520model%2520that%2520is%2520fine-tuned%2520on%2520a%2520synthetic%2520panoramic%2520dataset%2520integrating%2520authentic%2520camera%2520trajectories%252C%2520making%2520it%2520adaptive%2520to%2520camera%2520motion.%2520Extensive%2520experiments%2520demonstrate%2520that%2520with%2520semantic%2520reasoning%2520and%2520geometric%2520constraints%252C%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%2520approaches%2520in%2520producing%2520consistent%2520and%2520photorealistic%2520scenes.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/JS-CHU/RoamScene3D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoamScene3D%3A%20Immersive%20Text-to-3D%20Scene%20Generation%20via%20Adaptive%20Object-aware%20Roaming&entry.906535625=Jisheng%20Chu%20and%20Wenrui%20Li%20and%20Rui%20Zhao%20and%20Wangmeng%20Zuo%20and%20Shifeng%20Chen%20and%20Xiaopeng%20Fan&entry.1292438233=Generating%20immersive%203D%20scenes%20from%20texts%20is%20a%20core%20task%20in%20computer%20vision%2C%20crucial%20for%20applications%20in%20virtual%20reality%20and%20game%20development.%20Despite%20the%20promise%20of%20leveraging%202D%20diffusion%20priors%2C%20existing%20methods%20suffer%20from%20spatial%20blindness%20and%20rely%20on%20predefined%20trajectories%20that%20fail%20to%20exploit%20the%20inner%20relationships%20among%20salient%20objects.%20Consequently%2C%20these%20approaches%20are%20unable%20to%20comprehend%20the%20semantic%20layout%2C%20preventing%20them%20from%20exploring%20the%20scene%20adaptively%20to%20infer%20occluded%20content.%20Moreover%2C%20current%20inpainting%20models%20operate%20in%202D%20image%20space%2C%20struggling%20to%20plausibly%20fill%20holes%20caused%20by%20camera%20motion.%20To%20address%20these%20limitations%2C%20we%20propose%20RoamScene3D%2C%20a%20novel%20framework%20that%20bridges%20the%20gap%20between%20semantic%20guidance%20and%20spatial%20generation.%20Our%20method%20reasons%20about%20the%20semantic%20relations%20among%20objects%20and%20produces%20consistent%20and%20photorealistic%20scenes.%20Specifically%2C%20we%20employ%20a%20vision-language%20model%20%28VLM%29%20to%20construct%20a%20scene%20graph%20that%20encodes%20object%20relations%2C%20guiding%20the%20camera%20to%20perceive%20salient%20object%20boundaries%20and%20plan%20an%20adaptive%20roaming%20trajectory.%20Furthermore%2C%20to%20mitigate%20the%20limitations%20of%20static%202D%20priors%2C%20we%20introduce%20a%20Motion-Injected%20Inpainting%20model%20that%20is%20fine-tuned%20on%20a%20synthetic%20panoramic%20dataset%20integrating%20authentic%20camera%20trajectories%2C%20making%20it%20adaptive%20to%20camera%20motion.%20Extensive%20experiments%20demonstrate%20that%20with%20semantic%20reasoning%20and%20geometric%20constraints%2C%20our%20method%20significantly%20outperforms%20state-of-the-art%20approaches%20in%20producing%20consistent%20and%20photorealistic%20scenes.%20Our%20code%20is%20available%20at%20https%3A//github.com/JS-CHU/RoamScene3D.&entry.1838667208=http%3A//arxiv.org/abs/2601.19433v1&entry.124074799=Read"},
{"title": "TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching", "author": "Runjia Zeng and Qifan Wang and Qiang Guan and Ruixiang Tang and Lifu Huang and Zhenting Wang and Xueling Zhang and Cheng Han and Dongfang Liu", "abstract": "Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency. Homepage: https://runjia.tech/iclr_tokenseek/", "link": "http://arxiv.org/abs/2601.19739v1", "date": "2026-01-27", "relevancy": 2.5903, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5953}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4873}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenSeek%3A%20Memory%20Efficient%20Fine%20Tuning%20via%20Instance-Aware%20Token%20Ditching&body=Title%3A%20TokenSeek%3A%20Memory%20Efficient%20Fine%20Tuning%20via%20Instance-Aware%20Token%20Ditching%0AAuthor%3A%20Runjia%20Zeng%20and%20Qifan%20Wang%20and%20Qiang%20Guan%20and%20Ruixiang%20Tang%20and%20Lifu%20Huang%20and%20Zhenting%20Wang%20and%20Xueling%20Zhang%20and%20Cheng%20Han%20and%20Dongfang%20Liu%0AAbstract%3A%20Fine%20tuning%20has%20been%20regarded%20as%20a%20de%20facto%20approach%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20downstream%20tasks%2C%20but%20the%20high%20training%20memory%20consumption%20inherited%20from%20LLMs%20makes%20this%20process%20inefficient.%20Among%20existing%20memory%20efficient%20approaches%2C%20activation-related%20optimization%20has%20proven%20particularly%20effective%2C%20as%20activations%20consistently%20dominate%20overall%20memory%20consumption.%20Although%20prior%20arts%20offer%20various%20activation%20optimization%20strategies%2C%20their%20data-agnostic%20nature%20ultimately%20results%20in%20ineffective%20and%20unstable%20fine%20tuning.%20In%20this%20paper%2C%20we%20propose%20TokenSeek%2C%20a%20universal%20plugin%20solution%20for%20various%20transformer-based%20models%20through%20instance-aware%20token%20seeking%20and%20ditching%2C%20achieving%20significant%20fine-tuning%20memory%20savings%20%28e.g.%2C%20requiring%20only%2014.8%25%20of%20the%20memory%20on%20Llama3.2%201B%29%20with%20on-par%20or%20even%20better%20performance.%20Furthermore%2C%20our%20interpretable%20token%20seeking%20process%20reveals%20the%20underlying%20reasons%20for%20its%20effectiveness%2C%20offering%20valuable%20insights%20for%20future%20research%20on%20token%20efficiency.%20Homepage%3A%20https%3A//runjia.tech/iclr_tokenseek/%0ALink%3A%20http%3A//arxiv.org/abs/2601.19739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenSeek%253A%2520Memory%2520Efficient%2520Fine%2520Tuning%2520via%2520Instance-Aware%2520Token%2520Ditching%26entry.906535625%3DRunjia%2520Zeng%2520and%2520Qifan%2520Wang%2520and%2520Qiang%2520Guan%2520and%2520Ruixiang%2520Tang%2520and%2520Lifu%2520Huang%2520and%2520Zhenting%2520Wang%2520and%2520Xueling%2520Zhang%2520and%2520Cheng%2520Han%2520and%2520Dongfang%2520Liu%26entry.1292438233%3DFine%2520tuning%2520has%2520been%2520regarded%2520as%2520a%2520de%2520facto%2520approach%2520for%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520downstream%2520tasks%252C%2520but%2520the%2520high%2520training%2520memory%2520consumption%2520inherited%2520from%2520LLMs%2520makes%2520this%2520process%2520inefficient.%2520Among%2520existing%2520memory%2520efficient%2520approaches%252C%2520activation-related%2520optimization%2520has%2520proven%2520particularly%2520effective%252C%2520as%2520activations%2520consistently%2520dominate%2520overall%2520memory%2520consumption.%2520Although%2520prior%2520arts%2520offer%2520various%2520activation%2520optimization%2520strategies%252C%2520their%2520data-agnostic%2520nature%2520ultimately%2520results%2520in%2520ineffective%2520and%2520unstable%2520fine%2520tuning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TokenSeek%252C%2520a%2520universal%2520plugin%2520solution%2520for%2520various%2520transformer-based%2520models%2520through%2520instance-aware%2520token%2520seeking%2520and%2520ditching%252C%2520achieving%2520significant%2520fine-tuning%2520memory%2520savings%2520%2528e.g.%252C%2520requiring%2520only%252014.8%2525%2520of%2520the%2520memory%2520on%2520Llama3.2%25201B%2529%2520with%2520on-par%2520or%2520even%2520better%2520performance.%2520Furthermore%252C%2520our%2520interpretable%2520token%2520seeking%2520process%2520reveals%2520the%2520underlying%2520reasons%2520for%2520its%2520effectiveness%252C%2520offering%2520valuable%2520insights%2520for%2520future%2520research%2520on%2520token%2520efficiency.%2520Homepage%253A%2520https%253A//runjia.tech/iclr_tokenseek/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenSeek%3A%20Memory%20Efficient%20Fine%20Tuning%20via%20Instance-Aware%20Token%20Ditching&entry.906535625=Runjia%20Zeng%20and%20Qifan%20Wang%20and%20Qiang%20Guan%20and%20Ruixiang%20Tang%20and%20Lifu%20Huang%20and%20Zhenting%20Wang%20and%20Xueling%20Zhang%20and%20Cheng%20Han%20and%20Dongfang%20Liu&entry.1292438233=Fine%20tuning%20has%20been%20regarded%20as%20a%20de%20facto%20approach%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20downstream%20tasks%2C%20but%20the%20high%20training%20memory%20consumption%20inherited%20from%20LLMs%20makes%20this%20process%20inefficient.%20Among%20existing%20memory%20efficient%20approaches%2C%20activation-related%20optimization%20has%20proven%20particularly%20effective%2C%20as%20activations%20consistently%20dominate%20overall%20memory%20consumption.%20Although%20prior%20arts%20offer%20various%20activation%20optimization%20strategies%2C%20their%20data-agnostic%20nature%20ultimately%20results%20in%20ineffective%20and%20unstable%20fine%20tuning.%20In%20this%20paper%2C%20we%20propose%20TokenSeek%2C%20a%20universal%20plugin%20solution%20for%20various%20transformer-based%20models%20through%20instance-aware%20token%20seeking%20and%20ditching%2C%20achieving%20significant%20fine-tuning%20memory%20savings%20%28e.g.%2C%20requiring%20only%2014.8%25%20of%20the%20memory%20on%20Llama3.2%201B%29%20with%20on-par%20or%20even%20better%20performance.%20Furthermore%2C%20our%20interpretable%20token%20seeking%20process%20reveals%20the%20underlying%20reasons%20for%20its%20effectiveness%2C%20offering%20valuable%20insights%20for%20future%20research%20on%20token%20efficiency.%20Homepage%3A%20https%3A//runjia.tech/iclr_tokenseek/&entry.1838667208=http%3A//arxiv.org/abs/2601.19739v1&entry.124074799=Read"},
{"title": "R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning", "author": "Zhizheng Jiang and Kang Zhao and Weikai Xu and Xinkui Lin and Wei Liu and Jian Luan and Shuo Shang and Peng Han", "abstract": "Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.", "link": "http://arxiv.org/abs/2601.19620v1", "date": "2026-01-27", "relevancy": 2.59, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R%5E3%3A%20Replay%2C%20Reflection%2C%20and%20Ranking%20Rewards%20for%20LLM%20Reinforcement%20Learning&body=Title%3A%20R%5E3%3A%20Replay%2C%20Reflection%2C%20and%20Ranking%20Rewards%20for%20LLM%20Reinforcement%20Learning%0AAuthor%3A%20Zhizheng%20Jiang%20and%20Kang%20Zhao%20and%20Weikai%20Xu%20and%20Xinkui%20Lin%20and%20Wei%20Liu%20and%20Jian%20Luan%20and%20Shuo%20Shang%20and%20Peng%20Han%0AAbstract%3A%20Large%20reasoning%20models%20%28LRMs%29%20aim%20to%20solve%20diverse%20and%20complex%20problems%20through%20structured%20reasoning.%20Recent%20advances%20in%20group-based%20policy%20optimization%20methods%20have%20shown%20promise%20in%20enabling%20stable%20advantage%20estimation%20without%20reliance%20on%20process-level%20annotations.%20However%2C%20these%20methods%20rely%20on%20advantage%20gaps%20induced%20by%20high-quality%20samples%20within%20the%20same%20batch%2C%20which%20makes%20the%20training%20process%20fragile%20and%20inefficient%20when%20intra-group%20advantages%20collapse%20under%20challenging%20tasks.%20To%20address%20these%20problems%2C%20we%20propose%20a%20reinforcement%20learning%20mechanism%20named%20%5Cemph%7B%5Ctextbf%7BR%5E3%7D%7D%20that%20along%20three%20directions%3A%20%281%29%20a%20%5Cemph%7Bcross-context%20%5Cunderline%7B%5Ctextbf%7BR%7D%7Deplay%7D%20strategy%20that%20maintains%20the%20intra-group%20advantage%20by%20recalling%20valuable%20examples%20from%20historical%20trajectories%20of%20the%20same%20query%2C%20%282%29%20an%20%5Cemph%7Bin-context%20self-%5Cunderline%7B%5Ctextbf%7BR%7D%7Deflection%7D%20mechanism%20enabling%20models%20to%20refine%20outputs%20by%20leveraging%20past%20failures%2C%20and%20%283%29%20a%20%5Cemph%7Bstructural%20entropy%20%5Cunderline%7B%5Ctextbf%7BR%7D%7Danking%20reward%7D%2C%20which%20assigns%20relative%20rewards%20to%20truncated%20or%20failed%20samples%20by%20ranking%20responses%20based%20on%20token-level%20entropy%20patterns%2C%20capturing%20both%20local%20exploration%20and%20global%20stability.%20We%20implement%20our%20method%20on%20Deepseek-R1-Distill-Qwen-1.5B%20and%20train%20it%20on%20the%20DeepscaleR-40k%20in%20the%20math%20domain.%20Experiments%20demonstrate%20our%20method%20achieves%20SoTA%20performance%20on%20several%20math%20benchmarks%2C%20representing%20significant%20improvements%20and%20fewer%20reasoning%20tokens%20over%20the%20base%20models.%20Code%20and%20model%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR%255E3%253A%2520Replay%252C%2520Reflection%252C%2520and%2520Ranking%2520Rewards%2520for%2520LLM%2520Reinforcement%2520Learning%26entry.906535625%3DZhizheng%2520Jiang%2520and%2520Kang%2520Zhao%2520and%2520Weikai%2520Xu%2520and%2520Xinkui%2520Lin%2520and%2520Wei%2520Liu%2520and%2520Jian%2520Luan%2520and%2520Shuo%2520Shang%2520and%2520Peng%2520Han%26entry.1292438233%3DLarge%2520reasoning%2520models%2520%2528LRMs%2529%2520aim%2520to%2520solve%2520diverse%2520and%2520complex%2520problems%2520through%2520structured%2520reasoning.%2520Recent%2520advances%2520in%2520group-based%2520policy%2520optimization%2520methods%2520have%2520shown%2520promise%2520in%2520enabling%2520stable%2520advantage%2520estimation%2520without%2520reliance%2520on%2520process-level%2520annotations.%2520However%252C%2520these%2520methods%2520rely%2520on%2520advantage%2520gaps%2520induced%2520by%2520high-quality%2520samples%2520within%2520the%2520same%2520batch%252C%2520which%2520makes%2520the%2520training%2520process%2520fragile%2520and%2520inefficient%2520when%2520intra-group%2520advantages%2520collapse%2520under%2520challenging%2520tasks.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520reinforcement%2520learning%2520mechanism%2520named%2520%255Cemph%257B%255Ctextbf%257BR%255E3%257D%257D%2520that%2520along%2520three%2520directions%253A%2520%25281%2529%2520a%2520%255Cemph%257Bcross-context%2520%255Cunderline%257B%255Ctextbf%257BR%257D%257Deplay%257D%2520strategy%2520that%2520maintains%2520the%2520intra-group%2520advantage%2520by%2520recalling%2520valuable%2520examples%2520from%2520historical%2520trajectories%2520of%2520the%2520same%2520query%252C%2520%25282%2529%2520an%2520%255Cemph%257Bin-context%2520self-%255Cunderline%257B%255Ctextbf%257BR%257D%257Deflection%257D%2520mechanism%2520enabling%2520models%2520to%2520refine%2520outputs%2520by%2520leveraging%2520past%2520failures%252C%2520and%2520%25283%2529%2520a%2520%255Cemph%257Bstructural%2520entropy%2520%255Cunderline%257B%255Ctextbf%257BR%257D%257Danking%2520reward%257D%252C%2520which%2520assigns%2520relative%2520rewards%2520to%2520truncated%2520or%2520failed%2520samples%2520by%2520ranking%2520responses%2520based%2520on%2520token-level%2520entropy%2520patterns%252C%2520capturing%2520both%2520local%2520exploration%2520and%2520global%2520stability.%2520We%2520implement%2520our%2520method%2520on%2520Deepseek-R1-Distill-Qwen-1.5B%2520and%2520train%2520it%2520on%2520the%2520DeepscaleR-40k%2520in%2520the%2520math%2520domain.%2520Experiments%2520demonstrate%2520our%2520method%2520achieves%2520SoTA%2520performance%2520on%2520several%2520math%2520benchmarks%252C%2520representing%2520significant%2520improvements%2520and%2520fewer%2520reasoning%2520tokens%2520over%2520the%2520base%2520models.%2520Code%2520and%2520model%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R%5E3%3A%20Replay%2C%20Reflection%2C%20and%20Ranking%20Rewards%20for%20LLM%20Reinforcement%20Learning&entry.906535625=Zhizheng%20Jiang%20and%20Kang%20Zhao%20and%20Weikai%20Xu%20and%20Xinkui%20Lin%20and%20Wei%20Liu%20and%20Jian%20Luan%20and%20Shuo%20Shang%20and%20Peng%20Han&entry.1292438233=Large%20reasoning%20models%20%28LRMs%29%20aim%20to%20solve%20diverse%20and%20complex%20problems%20through%20structured%20reasoning.%20Recent%20advances%20in%20group-based%20policy%20optimization%20methods%20have%20shown%20promise%20in%20enabling%20stable%20advantage%20estimation%20without%20reliance%20on%20process-level%20annotations.%20However%2C%20these%20methods%20rely%20on%20advantage%20gaps%20induced%20by%20high-quality%20samples%20within%20the%20same%20batch%2C%20which%20makes%20the%20training%20process%20fragile%20and%20inefficient%20when%20intra-group%20advantages%20collapse%20under%20challenging%20tasks.%20To%20address%20these%20problems%2C%20we%20propose%20a%20reinforcement%20learning%20mechanism%20named%20%5Cemph%7B%5Ctextbf%7BR%5E3%7D%7D%20that%20along%20three%20directions%3A%20%281%29%20a%20%5Cemph%7Bcross-context%20%5Cunderline%7B%5Ctextbf%7BR%7D%7Deplay%7D%20strategy%20that%20maintains%20the%20intra-group%20advantage%20by%20recalling%20valuable%20examples%20from%20historical%20trajectories%20of%20the%20same%20query%2C%20%282%29%20an%20%5Cemph%7Bin-context%20self-%5Cunderline%7B%5Ctextbf%7BR%7D%7Deflection%7D%20mechanism%20enabling%20models%20to%20refine%20outputs%20by%20leveraging%20past%20failures%2C%20and%20%283%29%20a%20%5Cemph%7Bstructural%20entropy%20%5Cunderline%7B%5Ctextbf%7BR%7D%7Danking%20reward%7D%2C%20which%20assigns%20relative%20rewards%20to%20truncated%20or%20failed%20samples%20by%20ranking%20responses%20based%20on%20token-level%20entropy%20patterns%2C%20capturing%20both%20local%20exploration%20and%20global%20stability.%20We%20implement%20our%20method%20on%20Deepseek-R1-Distill-Qwen-1.5B%20and%20train%20it%20on%20the%20DeepscaleR-40k%20in%20the%20math%20domain.%20Experiments%20demonstrate%20our%20method%20achieves%20SoTA%20performance%20on%20several%20math%20benchmarks%2C%20representing%20significant%20improvements%20and%20fewer%20reasoning%20tokens%20over%20the%20base%20models.%20Code%20and%20model%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.19620v1&entry.124074799=Read"},
{"title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs", "author": " Anand and Umberto Cappellazzo and Stavros Petridis and Maja Pantic", "abstract": "Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates.", "link": "http://arxiv.org/abs/2510.22603v3", "date": "2026-01-27", "relevancy": 2.5898, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Attention%20Sinks%20and%20Massive%20Activations%20in%20Audio-Visual%20Speech%20Recognition%20with%20LLMs&body=Title%3A%20Mitigating%20Attention%20Sinks%20and%20Massive%20Activations%20in%20Audio-Visual%20Speech%20Recognition%20with%20LLMs%0AAuthor%3A%20%20Anand%20and%20Umberto%20Cappellazzo%20and%20Stavros%20Petridis%20and%20Maja%20Pantic%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20recently%20advanced%20auditory%20speech%20recognition%20%28ASR%29%2C%20visual%20speech%20recognition%20%28VSR%29%2C%20and%20audio-visual%20speech%20recognition%20%28AVSR%29.%20However%2C%20understanding%20of%20their%20internal%20dynamics%20under%20fine-tuning%20remains%20limited.%20In%20natural%20language%20processing%2C%20recent%20work%20has%20revealed%20attention%20sinks%2C%20tokens%20that%20attract%20disproportionately%20high%20attention%2C%20and%20associated%20massive%20activations%20in%20which%20some%20features%20of%20sink%20tokens%20exhibit%20huge%20activation%20in%20LLMs.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20study%20these%20phenomena%20in%20multimodal%20speech%20recognition.%20Through%20a%20detailed%20analysis%20of%20audio-visual%20LLMs%2C%20we%20identify%20attention%20sinks%20and%20massive%20activations%20not%20only%20at%20the%20BOS%20token%20but%20also%20at%20intermediate%20low-semantic%20tokens%20across%20ASR%2C%20VSR%2C%20and%20AVSR.%20We%20show%20that%20massive%20activations%20originate%20in%20the%20MLP%20layers%20and%20correspond%20to%20fixed%20feature%20indices%20across%20all%20sink%20tokens.%20We%20further%20show%20that%20intermediate%20sink%20tokens%20exhibit%20high%20cosine%20similarity%20to%20the%20BOS%20token%2C%20thereby%20amplifying%20attention%20and%20activation.%20Building%20on%20these%20insights%2C%20we%20introduce%20a%20simple%20decorrelation%20loss%20that%20reduces%20cosine%20similarity%20between%20BOS%20and%20other%20tokens%2C%20effectively%20mitigating%20intermediate%20sinks%20and%20massive%20activations.%20Furthermore%2C%20our%20method%20improves%20word%20error%20rate%20%28WER%29%20under%20high%20audio-visual%20feature%20downsampling%20while%20remaining%20stable%20at%20lower%20downsampling%20rates.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22603v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Attention%2520Sinks%2520and%2520Massive%2520Activations%2520in%2520Audio-Visual%2520Speech%2520Recognition%2520with%2520LLMs%26entry.906535625%3D%2520Anand%2520and%2520Umberto%2520Cappellazzo%2520and%2520Stavros%2520Petridis%2520and%2520Maja%2520Pantic%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520advanced%2520auditory%2520speech%2520recognition%2520%2528ASR%2529%252C%2520visual%2520speech%2520recognition%2520%2528VSR%2529%252C%2520and%2520audio-visual%2520speech%2520recognition%2520%2528AVSR%2529.%2520However%252C%2520understanding%2520of%2520their%2520internal%2520dynamics%2520under%2520fine-tuning%2520remains%2520limited.%2520In%2520natural%2520language%2520processing%252C%2520recent%2520work%2520has%2520revealed%2520attention%2520sinks%252C%2520tokens%2520that%2520attract%2520disproportionately%2520high%2520attention%252C%2520and%2520associated%2520massive%2520activations%2520in%2520which%2520some%2520features%2520of%2520sink%2520tokens%2520exhibit%2520huge%2520activation%2520in%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520are%2520the%2520first%2520to%2520study%2520these%2520phenomena%2520in%2520multimodal%2520speech%2520recognition.%2520Through%2520a%2520detailed%2520analysis%2520of%2520audio-visual%2520LLMs%252C%2520we%2520identify%2520attention%2520sinks%2520and%2520massive%2520activations%2520not%2520only%2520at%2520the%2520BOS%2520token%2520but%2520also%2520at%2520intermediate%2520low-semantic%2520tokens%2520across%2520ASR%252C%2520VSR%252C%2520and%2520AVSR.%2520We%2520show%2520that%2520massive%2520activations%2520originate%2520in%2520the%2520MLP%2520layers%2520and%2520correspond%2520to%2520fixed%2520feature%2520indices%2520across%2520all%2520sink%2520tokens.%2520We%2520further%2520show%2520that%2520intermediate%2520sink%2520tokens%2520exhibit%2520high%2520cosine%2520similarity%2520to%2520the%2520BOS%2520token%252C%2520thereby%2520amplifying%2520attention%2520and%2520activation.%2520Building%2520on%2520these%2520insights%252C%2520we%2520introduce%2520a%2520simple%2520decorrelation%2520loss%2520that%2520reduces%2520cosine%2520similarity%2520between%2520BOS%2520and%2520other%2520tokens%252C%2520effectively%2520mitigating%2520intermediate%2520sinks%2520and%2520massive%2520activations.%2520Furthermore%252C%2520our%2520method%2520improves%2520word%2520error%2520rate%2520%2528WER%2529%2520under%2520high%2520audio-visual%2520feature%2520downsampling%2520while%2520remaining%2520stable%2520at%2520lower%2520downsampling%2520rates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22603v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Attention%20Sinks%20and%20Massive%20Activations%20in%20Audio-Visual%20Speech%20Recognition%20with%20LLMs&entry.906535625=%20Anand%20and%20Umberto%20Cappellazzo%20and%20Stavros%20Petridis%20and%20Maja%20Pantic&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20recently%20advanced%20auditory%20speech%20recognition%20%28ASR%29%2C%20visual%20speech%20recognition%20%28VSR%29%2C%20and%20audio-visual%20speech%20recognition%20%28AVSR%29.%20However%2C%20understanding%20of%20their%20internal%20dynamics%20under%20fine-tuning%20remains%20limited.%20In%20natural%20language%20processing%2C%20recent%20work%20has%20revealed%20attention%20sinks%2C%20tokens%20that%20attract%20disproportionately%20high%20attention%2C%20and%20associated%20massive%20activations%20in%20which%20some%20features%20of%20sink%20tokens%20exhibit%20huge%20activation%20in%20LLMs.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20study%20these%20phenomena%20in%20multimodal%20speech%20recognition.%20Through%20a%20detailed%20analysis%20of%20audio-visual%20LLMs%2C%20we%20identify%20attention%20sinks%20and%20massive%20activations%20not%20only%20at%20the%20BOS%20token%20but%20also%20at%20intermediate%20low-semantic%20tokens%20across%20ASR%2C%20VSR%2C%20and%20AVSR.%20We%20show%20that%20massive%20activations%20originate%20in%20the%20MLP%20layers%20and%20correspond%20to%20fixed%20feature%20indices%20across%20all%20sink%20tokens.%20We%20further%20show%20that%20intermediate%20sink%20tokens%20exhibit%20high%20cosine%20similarity%20to%20the%20BOS%20token%2C%20thereby%20amplifying%20attention%20and%20activation.%20Building%20on%20these%20insights%2C%20we%20introduce%20a%20simple%20decorrelation%20loss%20that%20reduces%20cosine%20similarity%20between%20BOS%20and%20other%20tokens%2C%20effectively%20mitigating%20intermediate%20sinks%20and%20massive%20activations.%20Furthermore%2C%20our%20method%20improves%20word%20error%20rate%20%28WER%29%20under%20high%20audio-visual%20feature%20downsampling%20while%20remaining%20stable%20at%20lower%20downsampling%20rates.&entry.1838667208=http%3A//arxiv.org/abs/2510.22603v3&entry.124074799=Read"},
{"title": "Learning to Detect Unseen Jailbreak Attacks in Large Vision-Language Models", "author": "Shuang Liang and Zhihao Xu and Jiaqi Weng and Jialing Tao and Hui Xue and Xiting Wang", "abstract": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks. To mitigate these risks, existing detection methods are essential, yet they face two major challenges: generalization and accuracy. While learning-based methods trained on specific attacks fail to generalize to unseen attacks, learning-free methods based on hand-crafted heuristics suffer from limited accuracy and reduced efficiency. To address these limitations, we propose Learning to Detect (LoD), a learnable framework that eliminates the need for any attack data or hand-crafted heuristics. LoD operates by first extracting layer-wise safety representations directly from the model's internal activations using Multi-modal Safety Concept Activation Vectors classifiers, and then converting the high-dimensional representations into a one-dimensional anomaly score for detection via a Safety Pattern Auto-Encoder. Extensive experiments demonstrate that LoD consistently achieves state-of-the-art detection performance (AUROC) across diverse unseen jailbreak attacks on multiple LVLMs, while also significantly improving efficiency. Code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.", "link": "http://arxiv.org/abs/2508.09201v4", "date": "2026-01-27", "relevancy": 2.5833, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Detect%20Unseen%20Jailbreak%20Attacks%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Learning%20to%20Detect%20Unseen%20Jailbreak%20Attacks%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Shuang%20Liang%20and%20Zhihao%20Xu%20and%20Jiaqi%20Weng%20and%20Jialing%20Tao%20and%20Hui%20Xue%20and%20Xiting%20Wang%0AAbstract%3A%20Despite%20extensive%20alignment%20efforts%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20remain%20vulnerable%20to%20jailbreak%20attacks.%20To%20mitigate%20these%20risks%2C%20existing%20detection%20methods%20are%20essential%2C%20yet%20they%20face%20two%20major%20challenges%3A%20generalization%20and%20accuracy.%20While%20learning-based%20methods%20trained%20on%20specific%20attacks%20fail%20to%20generalize%20to%20unseen%20attacks%2C%20learning-free%20methods%20based%20on%20hand-crafted%20heuristics%20suffer%20from%20limited%20accuracy%20and%20reduced%20efficiency.%20To%20address%20these%20limitations%2C%20we%20propose%20Learning%20to%20Detect%20%28LoD%29%2C%20a%20learnable%20framework%20that%20eliminates%20the%20need%20for%20any%20attack%20data%20or%20hand-crafted%20heuristics.%20LoD%20operates%20by%20first%20extracting%20layer-wise%20safety%20representations%20directly%20from%20the%20model%27s%20internal%20activations%20using%20Multi-modal%20Safety%20Concept%20Activation%20Vectors%20classifiers%2C%20and%20then%20converting%20the%20high-dimensional%20representations%20into%20a%20one-dimensional%20anomaly%20score%20for%20detection%20via%20a%20Safety%20Pattern%20Auto-Encoder.%20Extensive%20experiments%20demonstrate%20that%20LoD%20consistently%20achieves%20state-of-the-art%20detection%20performance%20%28AUROC%29%20across%20diverse%20unseen%20jailbreak%20attacks%20on%20multiple%20LVLMs%2C%20while%20also%20significantly%20improving%20efficiency.%20Code%20is%20available%20at%20https%3A//anonymous.4open.science/r/Learning-to-Detect-51CB.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09201v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Detect%2520Unseen%2520Jailbreak%2520Attacks%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DShuang%2520Liang%2520and%2520Zhihao%2520Xu%2520and%2520Jiaqi%2520Weng%2520and%2520Jialing%2520Tao%2520and%2520Hui%2520Xue%2520and%2520Xiting%2520Wang%26entry.1292438233%3DDespite%2520extensive%2520alignment%2520efforts%252C%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520remain%2520vulnerable%2520to%2520jailbreak%2520attacks.%2520To%2520mitigate%2520these%2520risks%252C%2520existing%2520detection%2520methods%2520are%2520essential%252C%2520yet%2520they%2520face%2520two%2520major%2520challenges%253A%2520generalization%2520and%2520accuracy.%2520While%2520learning-based%2520methods%2520trained%2520on%2520specific%2520attacks%2520fail%2520to%2520generalize%2520to%2520unseen%2520attacks%252C%2520learning-free%2520methods%2520based%2520on%2520hand-crafted%2520heuristics%2520suffer%2520from%2520limited%2520accuracy%2520and%2520reduced%2520efficiency.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Learning%2520to%2520Detect%2520%2528LoD%2529%252C%2520a%2520learnable%2520framework%2520that%2520eliminates%2520the%2520need%2520for%2520any%2520attack%2520data%2520or%2520hand-crafted%2520heuristics.%2520LoD%2520operates%2520by%2520first%2520extracting%2520layer-wise%2520safety%2520representations%2520directly%2520from%2520the%2520model%2527s%2520internal%2520activations%2520using%2520Multi-modal%2520Safety%2520Concept%2520Activation%2520Vectors%2520classifiers%252C%2520and%2520then%2520converting%2520the%2520high-dimensional%2520representations%2520into%2520a%2520one-dimensional%2520anomaly%2520score%2520for%2520detection%2520via%2520a%2520Safety%2520Pattern%2520Auto-Encoder.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LoD%2520consistently%2520achieves%2520state-of-the-art%2520detection%2520performance%2520%2528AUROC%2529%2520across%2520diverse%2520unseen%2520jailbreak%2520attacks%2520on%2520multiple%2520LVLMs%252C%2520while%2520also%2520significantly%2520improving%2520efficiency.%2520Code%2520is%2520available%2520at%2520https%253A//anonymous.4open.science/r/Learning-to-Detect-51CB.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09201v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Detect%20Unseen%20Jailbreak%20Attacks%20in%20Large%20Vision-Language%20Models&entry.906535625=Shuang%20Liang%20and%20Zhihao%20Xu%20and%20Jiaqi%20Weng%20and%20Jialing%20Tao%20and%20Hui%20Xue%20and%20Xiting%20Wang&entry.1292438233=Despite%20extensive%20alignment%20efforts%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20remain%20vulnerable%20to%20jailbreak%20attacks.%20To%20mitigate%20these%20risks%2C%20existing%20detection%20methods%20are%20essential%2C%20yet%20they%20face%20two%20major%20challenges%3A%20generalization%20and%20accuracy.%20While%20learning-based%20methods%20trained%20on%20specific%20attacks%20fail%20to%20generalize%20to%20unseen%20attacks%2C%20learning-free%20methods%20based%20on%20hand-crafted%20heuristics%20suffer%20from%20limited%20accuracy%20and%20reduced%20efficiency.%20To%20address%20these%20limitations%2C%20we%20propose%20Learning%20to%20Detect%20%28LoD%29%2C%20a%20learnable%20framework%20that%20eliminates%20the%20need%20for%20any%20attack%20data%20or%20hand-crafted%20heuristics.%20LoD%20operates%20by%20first%20extracting%20layer-wise%20safety%20representations%20directly%20from%20the%20model%27s%20internal%20activations%20using%20Multi-modal%20Safety%20Concept%20Activation%20Vectors%20classifiers%2C%20and%20then%20converting%20the%20high-dimensional%20representations%20into%20a%20one-dimensional%20anomaly%20score%20for%20detection%20via%20a%20Safety%20Pattern%20Auto-Encoder.%20Extensive%20experiments%20demonstrate%20that%20LoD%20consistently%20achieves%20state-of-the-art%20detection%20performance%20%28AUROC%29%20across%20diverse%20unseen%20jailbreak%20attacks%20on%20multiple%20LVLMs%2C%20while%20also%20significantly%20improving%20efficiency.%20Code%20is%20available%20at%20https%3A//anonymous.4open.science/r/Learning-to-Detect-51CB.&entry.1838667208=http%3A//arxiv.org/abs/2508.09201v4&entry.124074799=Read"},
{"title": "KeepLoRA: Continual Learning with Residual Gradient Adaptation", "author": "Mao-Lin Luo and Zi-Hao Zhou and Yi-Lin Zhang and Yuanyu Wan and Tong Wei and Min-Ling Zhang", "abstract": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.", "link": "http://arxiv.org/abs/2601.19659v1", "date": "2026-01-27", "relevancy": 2.53, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KeepLoRA%3A%20Continual%20Learning%20with%20Residual%20Gradient%20Adaptation&body=Title%3A%20KeepLoRA%3A%20Continual%20Learning%20with%20Residual%20Gradient%20Adaptation%0AAuthor%3A%20Mao-Lin%20Luo%20and%20Zi-Hao%20Zhou%20and%20Yi-Lin%20Zhang%20and%20Yuanyu%20Wan%20and%20Tong%20Wei%20and%20Min-Ling%20Zhang%0AAbstract%3A%20Continual%20learning%20for%20pre-trained%20vision-language%20models%20requires%20balancing%20three%20competing%20objectives%3A%20retaining%20pre-trained%20knowledge%2C%20preserving%20knowledge%20from%20a%20sequence%20of%20learned%20tasks%2C%20and%20maintaining%20the%20plasticity%20to%20acquire%20new%20knowledge.%20This%20paper%20presents%20a%20simple%20but%20effective%20approach%20called%20KeepLoRA%20to%20effectively%20balance%20these%20objectives.%20We%20first%20analyze%20the%20knowledge%20retention%20mechanism%20within%20the%20model%20parameter%20space%20and%20find%20that%20general%20knowledge%20is%20mainly%20encoded%20in%20the%20principal%20subspace%2C%20while%20task-specific%20knowledge%20is%20encoded%20in%20the%20residual%20subspace.%20Motivated%20by%20this%20finding%2C%20KeepLoRA%20learns%20new%20tasks%20by%20restricting%20LoRA%20parameter%20updates%20in%20the%20residual%20subspace%20to%20prevent%20interfering%20with%20previously%20learned%20capabilities.%20Specifically%2C%20we%20infuse%20knowledge%20for%20a%20new%20task%20by%20projecting%20its%20gradient%20onto%20a%20subspace%20orthogonal%20to%20both%20the%20principal%20subspace%20of%20pre-trained%20model%20and%20the%20dominant%20directions%20of%20previous%20task%20features.%20Our%20theoretical%20and%20empirical%20analyses%20confirm%20that%20KeepLoRA%20balances%20the%20three%20objectives%20and%20achieves%20state-of-the-art%20performance.%20The%20implementation%20code%20is%20available%20at%20https%3A//github.com/MaolinLuo/KeepLoRA.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeepLoRA%253A%2520Continual%2520Learning%2520with%2520Residual%2520Gradient%2520Adaptation%26entry.906535625%3DMao-Lin%2520Luo%2520and%2520Zi-Hao%2520Zhou%2520and%2520Yi-Lin%2520Zhang%2520and%2520Yuanyu%2520Wan%2520and%2520Tong%2520Wei%2520and%2520Min-Ling%2520Zhang%26entry.1292438233%3DContinual%2520learning%2520for%2520pre-trained%2520vision-language%2520models%2520requires%2520balancing%2520three%2520competing%2520objectives%253A%2520retaining%2520pre-trained%2520knowledge%252C%2520preserving%2520knowledge%2520from%2520a%2520sequence%2520of%2520learned%2520tasks%252C%2520and%2520maintaining%2520the%2520plasticity%2520to%2520acquire%2520new%2520knowledge.%2520This%2520paper%2520presents%2520a%2520simple%2520but%2520effective%2520approach%2520called%2520KeepLoRA%2520to%2520effectively%2520balance%2520these%2520objectives.%2520We%2520first%2520analyze%2520the%2520knowledge%2520retention%2520mechanism%2520within%2520the%2520model%2520parameter%2520space%2520and%2520find%2520that%2520general%2520knowledge%2520is%2520mainly%2520encoded%2520in%2520the%2520principal%2520subspace%252C%2520while%2520task-specific%2520knowledge%2520is%2520encoded%2520in%2520the%2520residual%2520subspace.%2520Motivated%2520by%2520this%2520finding%252C%2520KeepLoRA%2520learns%2520new%2520tasks%2520by%2520restricting%2520LoRA%2520parameter%2520updates%2520in%2520the%2520residual%2520subspace%2520to%2520prevent%2520interfering%2520with%2520previously%2520learned%2520capabilities.%2520Specifically%252C%2520we%2520infuse%2520knowledge%2520for%2520a%2520new%2520task%2520by%2520projecting%2520its%2520gradient%2520onto%2520a%2520subspace%2520orthogonal%2520to%2520both%2520the%2520principal%2520subspace%2520of%2520pre-trained%2520model%2520and%2520the%2520dominant%2520directions%2520of%2520previous%2520task%2520features.%2520Our%2520theoretical%2520and%2520empirical%2520analyses%2520confirm%2520that%2520KeepLoRA%2520balances%2520the%2520three%2520objectives%2520and%2520achieves%2520state-of-the-art%2520performance.%2520The%2520implementation%2520code%2520is%2520available%2520at%2520https%253A//github.com/MaolinLuo/KeepLoRA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KeepLoRA%3A%20Continual%20Learning%20with%20Residual%20Gradient%20Adaptation&entry.906535625=Mao-Lin%20Luo%20and%20Zi-Hao%20Zhou%20and%20Yi-Lin%20Zhang%20and%20Yuanyu%20Wan%20and%20Tong%20Wei%20and%20Min-Ling%20Zhang&entry.1292438233=Continual%20learning%20for%20pre-trained%20vision-language%20models%20requires%20balancing%20three%20competing%20objectives%3A%20retaining%20pre-trained%20knowledge%2C%20preserving%20knowledge%20from%20a%20sequence%20of%20learned%20tasks%2C%20and%20maintaining%20the%20plasticity%20to%20acquire%20new%20knowledge.%20This%20paper%20presents%20a%20simple%20but%20effective%20approach%20called%20KeepLoRA%20to%20effectively%20balance%20these%20objectives.%20We%20first%20analyze%20the%20knowledge%20retention%20mechanism%20within%20the%20model%20parameter%20space%20and%20find%20that%20general%20knowledge%20is%20mainly%20encoded%20in%20the%20principal%20subspace%2C%20while%20task-specific%20knowledge%20is%20encoded%20in%20the%20residual%20subspace.%20Motivated%20by%20this%20finding%2C%20KeepLoRA%20learns%20new%20tasks%20by%20restricting%20LoRA%20parameter%20updates%20in%20the%20residual%20subspace%20to%20prevent%20interfering%20with%20previously%20learned%20capabilities.%20Specifically%2C%20we%20infuse%20knowledge%20for%20a%20new%20task%20by%20projecting%20its%20gradient%20onto%20a%20subspace%20orthogonal%20to%20both%20the%20principal%20subspace%20of%20pre-trained%20model%20and%20the%20dominant%20directions%20of%20previous%20task%20features.%20Our%20theoretical%20and%20empirical%20analyses%20confirm%20that%20KeepLoRA%20balances%20the%20three%20objectives%20and%20achieves%20state-of-the-art%20performance.%20The%20implementation%20code%20is%20available%20at%20https%3A//github.com/MaolinLuo/KeepLoRA.&entry.1838667208=http%3A//arxiv.org/abs/2601.19659v1&entry.124074799=Read"},
{"title": "SharpNet: Enhancing MLPs to Represent Functions with Controlled Non-differentiability", "author": "Hanting Niu and Junkai Deng and Fei Hou and Wencheng Wang and Ying He", "abstract": "Multi-layer perceptrons (MLPs) are a standard tool for learning and function approximation, but they inherently yield outputs that are globally smooth. As a result, they struggle to represent functions that are continuous yet deliberately non-differentiable (i.e., with prescribed $C^0$ sharp features) without relying on ad hoc post-processing. We present SharpNet, a modified MLP architecture capable of encoding functions with user-defined sharp features by enriching the network with an auxiliary feature function, which is defined as the solution to a Poisson equation with jump Neumann boundary conditions. It is evaluated via an efficient local integral that is fully differentiable with respect to the feature locations, enabling our method to jointly optimize both the feature locations and the MLP parameters to recover the target functions/models. The $C^0$-continuity of SharpNet is precisely controllable, ensuring $C^0$-continuity at the feature locations and smoothness elsewhere. We validate SharpNet on 2D problems and 3D CAD model reconstruction, and compare it against several state-of-the-art baselines. In both types of tasks, SharpNet accurately recovers sharp edges and corners while maintaining smooth behavior away from those features, whereas existing methods tend to smooth out gradient discontinuities. Both qualitative and quantitative evaluations highlight the benefits of our approach.", "link": "http://arxiv.org/abs/2601.19683v1", "date": "2026-01-27", "relevancy": 2.4929, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.482}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SharpNet%3A%20Enhancing%20MLPs%20to%20Represent%20Functions%20with%20Controlled%20Non-differentiability&body=Title%3A%20SharpNet%3A%20Enhancing%20MLPs%20to%20Represent%20Functions%20with%20Controlled%20Non-differentiability%0AAuthor%3A%20Hanting%20Niu%20and%20Junkai%20Deng%20and%20Fei%20Hou%20and%20Wencheng%20Wang%20and%20Ying%20He%0AAbstract%3A%20Multi-layer%20perceptrons%20%28MLPs%29%20are%20a%20standard%20tool%20for%20learning%20and%20function%20approximation%2C%20but%20they%20inherently%20yield%20outputs%20that%20are%20globally%20smooth.%20As%20a%20result%2C%20they%20struggle%20to%20represent%20functions%20that%20are%20continuous%20yet%20deliberately%20non-differentiable%20%28i.e.%2C%20with%20prescribed%20%24C%5E0%24%20sharp%20features%29%20without%20relying%20on%20ad%20hoc%20post-processing.%20We%20present%20SharpNet%2C%20a%20modified%20MLP%20architecture%20capable%20of%20encoding%20functions%20with%20user-defined%20sharp%20features%20by%20enriching%20the%20network%20with%20an%20auxiliary%20feature%20function%2C%20which%20is%20defined%20as%20the%20solution%20to%20a%20Poisson%20equation%20with%20jump%20Neumann%20boundary%20conditions.%20It%20is%20evaluated%20via%20an%20efficient%20local%20integral%20that%20is%20fully%20differentiable%20with%20respect%20to%20the%20feature%20locations%2C%20enabling%20our%20method%20to%20jointly%20optimize%20both%20the%20feature%20locations%20and%20the%20MLP%20parameters%20to%20recover%20the%20target%20functions/models.%20The%20%24C%5E0%24-continuity%20of%20SharpNet%20is%20precisely%20controllable%2C%20ensuring%20%24C%5E0%24-continuity%20at%20the%20feature%20locations%20and%20smoothness%20elsewhere.%20We%20validate%20SharpNet%20on%202D%20problems%20and%203D%20CAD%20model%20reconstruction%2C%20and%20compare%20it%20against%20several%20state-of-the-art%20baselines.%20In%20both%20types%20of%20tasks%2C%20SharpNet%20accurately%20recovers%20sharp%20edges%20and%20corners%20while%20maintaining%20smooth%20behavior%20away%20from%20those%20features%2C%20whereas%20existing%20methods%20tend%20to%20smooth%20out%20gradient%20discontinuities.%20Both%20qualitative%20and%20quantitative%20evaluations%20highlight%20the%20benefits%20of%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharpNet%253A%2520Enhancing%2520MLPs%2520to%2520Represent%2520Functions%2520with%2520Controlled%2520Non-differentiability%26entry.906535625%3DHanting%2520Niu%2520and%2520Junkai%2520Deng%2520and%2520Fei%2520Hou%2520and%2520Wencheng%2520Wang%2520and%2520Ying%2520He%26entry.1292438233%3DMulti-layer%2520perceptrons%2520%2528MLPs%2529%2520are%2520a%2520standard%2520tool%2520for%2520learning%2520and%2520function%2520approximation%252C%2520but%2520they%2520inherently%2520yield%2520outputs%2520that%2520are%2520globally%2520smooth.%2520As%2520a%2520result%252C%2520they%2520struggle%2520to%2520represent%2520functions%2520that%2520are%2520continuous%2520yet%2520deliberately%2520non-differentiable%2520%2528i.e.%252C%2520with%2520prescribed%2520%2524C%255E0%2524%2520sharp%2520features%2529%2520without%2520relying%2520on%2520ad%2520hoc%2520post-processing.%2520We%2520present%2520SharpNet%252C%2520a%2520modified%2520MLP%2520architecture%2520capable%2520of%2520encoding%2520functions%2520with%2520user-defined%2520sharp%2520features%2520by%2520enriching%2520the%2520network%2520with%2520an%2520auxiliary%2520feature%2520function%252C%2520which%2520is%2520defined%2520as%2520the%2520solution%2520to%2520a%2520Poisson%2520equation%2520with%2520jump%2520Neumann%2520boundary%2520conditions.%2520It%2520is%2520evaluated%2520via%2520an%2520efficient%2520local%2520integral%2520that%2520is%2520fully%2520differentiable%2520with%2520respect%2520to%2520the%2520feature%2520locations%252C%2520enabling%2520our%2520method%2520to%2520jointly%2520optimize%2520both%2520the%2520feature%2520locations%2520and%2520the%2520MLP%2520parameters%2520to%2520recover%2520the%2520target%2520functions/models.%2520The%2520%2524C%255E0%2524-continuity%2520of%2520SharpNet%2520is%2520precisely%2520controllable%252C%2520ensuring%2520%2524C%255E0%2524-continuity%2520at%2520the%2520feature%2520locations%2520and%2520smoothness%2520elsewhere.%2520We%2520validate%2520SharpNet%2520on%25202D%2520problems%2520and%25203D%2520CAD%2520model%2520reconstruction%252C%2520and%2520compare%2520it%2520against%2520several%2520state-of-the-art%2520baselines.%2520In%2520both%2520types%2520of%2520tasks%252C%2520SharpNet%2520accurately%2520recovers%2520sharp%2520edges%2520and%2520corners%2520while%2520maintaining%2520smooth%2520behavior%2520away%2520from%2520those%2520features%252C%2520whereas%2520existing%2520methods%2520tend%2520to%2520smooth%2520out%2520gradient%2520discontinuities.%2520Both%2520qualitative%2520and%2520quantitative%2520evaluations%2520highlight%2520the%2520benefits%2520of%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SharpNet%3A%20Enhancing%20MLPs%20to%20Represent%20Functions%20with%20Controlled%20Non-differentiability&entry.906535625=Hanting%20Niu%20and%20Junkai%20Deng%20and%20Fei%20Hou%20and%20Wencheng%20Wang%20and%20Ying%20He&entry.1292438233=Multi-layer%20perceptrons%20%28MLPs%29%20are%20a%20standard%20tool%20for%20learning%20and%20function%20approximation%2C%20but%20they%20inherently%20yield%20outputs%20that%20are%20globally%20smooth.%20As%20a%20result%2C%20they%20struggle%20to%20represent%20functions%20that%20are%20continuous%20yet%20deliberately%20non-differentiable%20%28i.e.%2C%20with%20prescribed%20%24C%5E0%24%20sharp%20features%29%20without%20relying%20on%20ad%20hoc%20post-processing.%20We%20present%20SharpNet%2C%20a%20modified%20MLP%20architecture%20capable%20of%20encoding%20functions%20with%20user-defined%20sharp%20features%20by%20enriching%20the%20network%20with%20an%20auxiliary%20feature%20function%2C%20which%20is%20defined%20as%20the%20solution%20to%20a%20Poisson%20equation%20with%20jump%20Neumann%20boundary%20conditions.%20It%20is%20evaluated%20via%20an%20efficient%20local%20integral%20that%20is%20fully%20differentiable%20with%20respect%20to%20the%20feature%20locations%2C%20enabling%20our%20method%20to%20jointly%20optimize%20both%20the%20feature%20locations%20and%20the%20MLP%20parameters%20to%20recover%20the%20target%20functions/models.%20The%20%24C%5E0%24-continuity%20of%20SharpNet%20is%20precisely%20controllable%2C%20ensuring%20%24C%5E0%24-continuity%20at%20the%20feature%20locations%20and%20smoothness%20elsewhere.%20We%20validate%20SharpNet%20on%202D%20problems%20and%203D%20CAD%20model%20reconstruction%2C%20and%20compare%20it%20against%20several%20state-of-the-art%20baselines.%20In%20both%20types%20of%20tasks%2C%20SharpNet%20accurately%20recovers%20sharp%20edges%20and%20corners%20while%20maintaining%20smooth%20behavior%20away%20from%20those%20features%2C%20whereas%20existing%20methods%20tend%20to%20smooth%20out%20gradient%20discontinuities.%20Both%20qualitative%20and%20quantitative%20evaluations%20highlight%20the%20benefits%20of%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2601.19683v1&entry.124074799=Read"},
{"title": "GraphDLG: Exploring Deep Leakage from Gradients in Federated Graph Learning", "author": "Shuyue Wei and Wantong Chen and Tongyu Wei and Chen Gong and Yongxin Tong and Lizhen Cui", "abstract": "Federated graph learning (FGL) has recently emerged as a promising privacy-preserving paradigm that enables distributed graph learning across multiple data owners. A critical privacy concern in federated learning is whether an adversary can recover raw data from shared gradients, a vulnerability known as deep leakage from gradients (DLG). However, most prior studies on the DLG problem focused on image or text data, and it remains an open question whether graphs can be effectively recovered, particularly when the graph structure and node features are uniquely entangled in GNNs. In this work, we first theoretically analyze the components in FGL and derive a crucial insight: once the graph structure is recovered, node features can be obtained through a closed-form recursive rule. Building on this analysis, we propose GraphDLG, a novel approach to recover raw training graphs from shared gradients in FGL, which can utilize randomly generated graphs or client-side training graphs as auxiliaries to enhance recovery. Extensive experiments demonstrate that GraphDLG outperforms existing solutions by successfully decoupling the graph structure and node features, achieving improvements of over 5.46% (by MSE) for node feature reconstruction and over 25.04% (by AUC) for graph structure reconstruction.", "link": "http://arxiv.org/abs/2601.19745v1", "date": "2026-01-27", "relevancy": 2.4836, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5095}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4969}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphDLG%3A%20Exploring%20Deep%20Leakage%20from%20Gradients%20in%20Federated%20Graph%20Learning&body=Title%3A%20GraphDLG%3A%20Exploring%20Deep%20Leakage%20from%20Gradients%20in%20Federated%20Graph%20Learning%0AAuthor%3A%20Shuyue%20Wei%20and%20Wantong%20Chen%20and%20Tongyu%20Wei%20and%20Chen%20Gong%20and%20Yongxin%20Tong%20and%20Lizhen%20Cui%0AAbstract%3A%20Federated%20graph%20learning%20%28FGL%29%20has%20recently%20emerged%20as%20a%20promising%20privacy-preserving%20paradigm%20that%20enables%20distributed%20graph%20learning%20across%20multiple%20data%20owners.%20A%20critical%20privacy%20concern%20in%20federated%20learning%20is%20whether%20an%20adversary%20can%20recover%20raw%20data%20from%20shared%20gradients%2C%20a%20vulnerability%20known%20as%20deep%20leakage%20from%20gradients%20%28DLG%29.%20However%2C%20most%20prior%20studies%20on%20the%20DLG%20problem%20focused%20on%20image%20or%20text%20data%2C%20and%20it%20remains%20an%20open%20question%20whether%20graphs%20can%20be%20effectively%20recovered%2C%20particularly%20when%20the%20graph%20structure%20and%20node%20features%20are%20uniquely%20entangled%20in%20GNNs.%20In%20this%20work%2C%20we%20first%20theoretically%20analyze%20the%20components%20in%20FGL%20and%20derive%20a%20crucial%20insight%3A%20once%20the%20graph%20structure%20is%20recovered%2C%20node%20features%20can%20be%20obtained%20through%20a%20closed-form%20recursive%20rule.%20Building%20on%20this%20analysis%2C%20we%20propose%20GraphDLG%2C%20a%20novel%20approach%20to%20recover%20raw%20training%20graphs%20from%20shared%20gradients%20in%20FGL%2C%20which%20can%20utilize%20randomly%20generated%20graphs%20or%20client-side%20training%20graphs%20as%20auxiliaries%20to%20enhance%20recovery.%20Extensive%20experiments%20demonstrate%20that%20GraphDLG%20outperforms%20existing%20solutions%20by%20successfully%20decoupling%20the%20graph%20structure%20and%20node%20features%2C%20achieving%20improvements%20of%20over%205.46%25%20%28by%20MSE%29%20for%20node%20feature%20reconstruction%20and%20over%2025.04%25%20%28by%20AUC%29%20for%20graph%20structure%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphDLG%253A%2520Exploring%2520Deep%2520Leakage%2520from%2520Gradients%2520in%2520Federated%2520Graph%2520Learning%26entry.906535625%3DShuyue%2520Wei%2520and%2520Wantong%2520Chen%2520and%2520Tongyu%2520Wei%2520and%2520Chen%2520Gong%2520and%2520Yongxin%2520Tong%2520and%2520Lizhen%2520Cui%26entry.1292438233%3DFederated%2520graph%2520learning%2520%2528FGL%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520privacy-preserving%2520paradigm%2520that%2520enables%2520distributed%2520graph%2520learning%2520across%2520multiple%2520data%2520owners.%2520A%2520critical%2520privacy%2520concern%2520in%2520federated%2520learning%2520is%2520whether%2520an%2520adversary%2520can%2520recover%2520raw%2520data%2520from%2520shared%2520gradients%252C%2520a%2520vulnerability%2520known%2520as%2520deep%2520leakage%2520from%2520gradients%2520%2528DLG%2529.%2520However%252C%2520most%2520prior%2520studies%2520on%2520the%2520DLG%2520problem%2520focused%2520on%2520image%2520or%2520text%2520data%252C%2520and%2520it%2520remains%2520an%2520open%2520question%2520whether%2520graphs%2520can%2520be%2520effectively%2520recovered%252C%2520particularly%2520when%2520the%2520graph%2520structure%2520and%2520node%2520features%2520are%2520uniquely%2520entangled%2520in%2520GNNs.%2520In%2520this%2520work%252C%2520we%2520first%2520theoretically%2520analyze%2520the%2520components%2520in%2520FGL%2520and%2520derive%2520a%2520crucial%2520insight%253A%2520once%2520the%2520graph%2520structure%2520is%2520recovered%252C%2520node%2520features%2520can%2520be%2520obtained%2520through%2520a%2520closed-form%2520recursive%2520rule.%2520Building%2520on%2520this%2520analysis%252C%2520we%2520propose%2520GraphDLG%252C%2520a%2520novel%2520approach%2520to%2520recover%2520raw%2520training%2520graphs%2520from%2520shared%2520gradients%2520in%2520FGL%252C%2520which%2520can%2520utilize%2520randomly%2520generated%2520graphs%2520or%2520client-side%2520training%2520graphs%2520as%2520auxiliaries%2520to%2520enhance%2520recovery.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GraphDLG%2520outperforms%2520existing%2520solutions%2520by%2520successfully%2520decoupling%2520the%2520graph%2520structure%2520and%2520node%2520features%252C%2520achieving%2520improvements%2520of%2520over%25205.46%2525%2520%2528by%2520MSE%2529%2520for%2520node%2520feature%2520reconstruction%2520and%2520over%252025.04%2525%2520%2528by%2520AUC%2529%2520for%2520graph%2520structure%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphDLG%3A%20Exploring%20Deep%20Leakage%20from%20Gradients%20in%20Federated%20Graph%20Learning&entry.906535625=Shuyue%20Wei%20and%20Wantong%20Chen%20and%20Tongyu%20Wei%20and%20Chen%20Gong%20and%20Yongxin%20Tong%20and%20Lizhen%20Cui&entry.1292438233=Federated%20graph%20learning%20%28FGL%29%20has%20recently%20emerged%20as%20a%20promising%20privacy-preserving%20paradigm%20that%20enables%20distributed%20graph%20learning%20across%20multiple%20data%20owners.%20A%20critical%20privacy%20concern%20in%20federated%20learning%20is%20whether%20an%20adversary%20can%20recover%20raw%20data%20from%20shared%20gradients%2C%20a%20vulnerability%20known%20as%20deep%20leakage%20from%20gradients%20%28DLG%29.%20However%2C%20most%20prior%20studies%20on%20the%20DLG%20problem%20focused%20on%20image%20or%20text%20data%2C%20and%20it%20remains%20an%20open%20question%20whether%20graphs%20can%20be%20effectively%20recovered%2C%20particularly%20when%20the%20graph%20structure%20and%20node%20features%20are%20uniquely%20entangled%20in%20GNNs.%20In%20this%20work%2C%20we%20first%20theoretically%20analyze%20the%20components%20in%20FGL%20and%20derive%20a%20crucial%20insight%3A%20once%20the%20graph%20structure%20is%20recovered%2C%20node%20features%20can%20be%20obtained%20through%20a%20closed-form%20recursive%20rule.%20Building%20on%20this%20analysis%2C%20we%20propose%20GraphDLG%2C%20a%20novel%20approach%20to%20recover%20raw%20training%20graphs%20from%20shared%20gradients%20in%20FGL%2C%20which%20can%20utilize%20randomly%20generated%20graphs%20or%20client-side%20training%20graphs%20as%20auxiliaries%20to%20enhance%20recovery.%20Extensive%20experiments%20demonstrate%20that%20GraphDLG%20outperforms%20existing%20solutions%20by%20successfully%20decoupling%20the%20graph%20structure%20and%20node%20features%2C%20achieving%20improvements%20of%20over%205.46%25%20%28by%20MSE%29%20for%20node%20feature%20reconstruction%20and%20over%2025.04%25%20%28by%20AUC%29%20for%20graph%20structure%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2601.19745v1&entry.124074799=Read"},
{"title": "Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue", "author": "Zhichao Liang and Satoshi Nakamura", "abstract": "Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.", "link": "http://arxiv.org/abs/2601.13687v2", "date": "2026-01-27", "relevancy": 2.4364, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Mental%20States%20to%20Guide%20Social%20Influence%20in%20Multi-Person%20Group%20Dialogue&body=Title%3A%20Understanding%20Mental%20States%20to%20Guide%20Social%20Influence%20in%20Multi-Person%20Group%20Dialogue%0AAuthor%3A%20Zhichao%20Liang%20and%20Satoshi%20Nakamura%0AAbstract%3A%20Existing%20dynamic%20Theory%20of%20Mind%20%28ToM%29%20benchmarks%20mostly%20place%20language%20models%20in%20a%20passive%20role%3A%20the%20model%20reads%20a%20sequence%20of%20connected%20scenarios%20and%20reports%20what%20people%20believe%2C%20feel%2C%20intend%2C%20and%20do%20as%20these%20states%20change.%20In%20real%20social%20interaction%2C%20ToM%20is%20also%20used%20for%20action%3A%20a%20speaker%20plans%20what%20to%20say%20in%20order%20to%20shift%20another%20person%27s%20mental-state%20trajectory%20toward%20a%20goal.%20We%20introduce%20SocialMindChange%2C%20a%20benchmark%20that%20moves%20from%20tracking%20minds%20to%20changing%20minds%20in%20social%20interaction.%20Each%20instance%20defines%20a%20social%20context%20with%204%20characters%20and%20five%20connected%20scenes.%20The%20model%20plays%20one%20character%20and%20generates%20dialogue%20across%20the%20five%20scenes%20to%20reach%20the%20target%20while%20remaining%20consistent%20with%20the%20evolving%20states%20of%20all%20participants.%20SocialMindChange%20also%20includes%20selected%20higher-order%20states.%20Using%20a%20structured%20four-step%20framework%2C%20we%20construct%201%2C200%20social%20contexts%2C%20covering%206000%20scenarios%20and%20over%2090%2C000%20questions%2C%20each%20validated%20for%20realism%20and%20quality.%20Evaluations%20on%20ten%20state-of-the-art%20LLMs%20show%20that%20their%20average%20performance%20is%2054.2%25%20below%20human%20performance.%20This%20gap%20suggests%20that%20current%20LLMs%20still%20struggle%20to%20maintain%20and%20change%20mental-state%20representations%20across%20long%2C%20linked%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Mental%2520States%2520to%2520Guide%2520Social%2520Influence%2520in%2520Multi-Person%2520Group%2520Dialogue%26entry.906535625%3DZhichao%2520Liang%2520and%2520Satoshi%2520Nakamura%26entry.1292438233%3DExisting%2520dynamic%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520benchmarks%2520mostly%2520place%2520language%2520models%2520in%2520a%2520passive%2520role%253A%2520the%2520model%2520reads%2520a%2520sequence%2520of%2520connected%2520scenarios%2520and%2520reports%2520what%2520people%2520believe%252C%2520feel%252C%2520intend%252C%2520and%2520do%2520as%2520these%2520states%2520change.%2520In%2520real%2520social%2520interaction%252C%2520ToM%2520is%2520also%2520used%2520for%2520action%253A%2520a%2520speaker%2520plans%2520what%2520to%2520say%2520in%2520order%2520to%2520shift%2520another%2520person%2527s%2520mental-state%2520trajectory%2520toward%2520a%2520goal.%2520We%2520introduce%2520SocialMindChange%252C%2520a%2520benchmark%2520that%2520moves%2520from%2520tracking%2520minds%2520to%2520changing%2520minds%2520in%2520social%2520interaction.%2520Each%2520instance%2520defines%2520a%2520social%2520context%2520with%25204%2520characters%2520and%2520five%2520connected%2520scenes.%2520The%2520model%2520plays%2520one%2520character%2520and%2520generates%2520dialogue%2520across%2520the%2520five%2520scenes%2520to%2520reach%2520the%2520target%2520while%2520remaining%2520consistent%2520with%2520the%2520evolving%2520states%2520of%2520all%2520participants.%2520SocialMindChange%2520also%2520includes%2520selected%2520higher-order%2520states.%2520Using%2520a%2520structured%2520four-step%2520framework%252C%2520we%2520construct%25201%252C200%2520social%2520contexts%252C%2520covering%25206000%2520scenarios%2520and%2520over%252090%252C000%2520questions%252C%2520each%2520validated%2520for%2520realism%2520and%2520quality.%2520Evaluations%2520on%2520ten%2520state-of-the-art%2520LLMs%2520show%2520that%2520their%2520average%2520performance%2520is%252054.2%2525%2520below%2520human%2520performance.%2520This%2520gap%2520suggests%2520that%2520current%2520LLMs%2520still%2520struggle%2520to%2520maintain%2520and%2520change%2520mental-state%2520representations%2520across%2520long%252C%2520linked%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Mental%20States%20to%20Guide%20Social%20Influence%20in%20Multi-Person%20Group%20Dialogue&entry.906535625=Zhichao%20Liang%20and%20Satoshi%20Nakamura&entry.1292438233=Existing%20dynamic%20Theory%20of%20Mind%20%28ToM%29%20benchmarks%20mostly%20place%20language%20models%20in%20a%20passive%20role%3A%20the%20model%20reads%20a%20sequence%20of%20connected%20scenarios%20and%20reports%20what%20people%20believe%2C%20feel%2C%20intend%2C%20and%20do%20as%20these%20states%20change.%20In%20real%20social%20interaction%2C%20ToM%20is%20also%20used%20for%20action%3A%20a%20speaker%20plans%20what%20to%20say%20in%20order%20to%20shift%20another%20person%27s%20mental-state%20trajectory%20toward%20a%20goal.%20We%20introduce%20SocialMindChange%2C%20a%20benchmark%20that%20moves%20from%20tracking%20minds%20to%20changing%20minds%20in%20social%20interaction.%20Each%20instance%20defines%20a%20social%20context%20with%204%20characters%20and%20five%20connected%20scenes.%20The%20model%20plays%20one%20character%20and%20generates%20dialogue%20across%20the%20five%20scenes%20to%20reach%20the%20target%20while%20remaining%20consistent%20with%20the%20evolving%20states%20of%20all%20participants.%20SocialMindChange%20also%20includes%20selected%20higher-order%20states.%20Using%20a%20structured%20four-step%20framework%2C%20we%20construct%201%2C200%20social%20contexts%2C%20covering%206000%20scenarios%20and%20over%2090%2C000%20questions%2C%20each%20validated%20for%20realism%20and%20quality.%20Evaluations%20on%20ten%20state-of-the-art%20LLMs%20show%20that%20their%20average%20performance%20is%2054.2%25%20below%20human%20performance.%20This%20gap%20suggests%20that%20current%20LLMs%20still%20struggle%20to%20maintain%20and%20change%20mental-state%20representations%20across%20long%2C%20linked%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2601.13687v2&entry.124074799=Read"},
{"title": "AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion", "author": "Tianyue Jiang and Yanli Wang and Yanlin Wang and Daya Guo and Ensheng Shi and Yuchi Ma and Jiachi Chen and Zibin Zheng", "abstract": "Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.", "link": "http://arxiv.org/abs/2601.19697v1", "date": "2026-01-27", "relevancy": 2.4194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignCoder%3A%20Aligning%20Retrieval%20with%20Target%20Intent%20for%20Repository-Level%20Code%20Completion&body=Title%3A%20AlignCoder%3A%20Aligning%20Retrieval%20with%20Target%20Intent%20for%20Repository-Level%20Code%20Completion%0AAuthor%3A%20Tianyue%20Jiang%20and%20Yanli%20Wang%20and%20Yanlin%20Wang%20and%20Daya%20Guo%20and%20Ensheng%20Shi%20and%20Yuchi%20Ma%20and%20Jiachi%20Chen%20and%20Zibin%20Zheng%0AAbstract%3A%20Repository-level%20code%20completion%20remains%20a%20challenging%20task%20for%20existing%20code%20large%20language%20models%20%28code%20LLMs%29%20due%20to%20their%20limited%20understanding%20of%20repository-specific%20context%20and%20domain%20knowledge.%20While%20retrieval-augmented%20generation%20%28RAG%29%20approaches%20have%20shown%20promise%20by%20retrieving%20relevant%20code%20snippets%20as%20cross-file%20context%2C%20they%20suffer%20from%20two%20fundamental%20problems%3A%20misalignment%20between%20the%20query%20and%20the%20target%20code%20in%20the%20retrieval%20process%2C%20and%20the%20inability%20of%20existing%20retrieval%20methods%20to%20effectively%20utilize%20the%20inference%20information.%20To%20address%20these%20challenges%2C%20we%20propose%20AlignCoder%2C%20a%20repository-level%20code%20completion%20framework%20that%20introduces%20a%20query%20enhancement%20mechanism%20and%20a%20reinforcement%20learning%20based%20retriever%20training%20method.%20Our%20approach%20generates%20multiple%20candidate%20completions%20to%20construct%20an%20enhanced%20query%20that%20bridges%20the%20semantic%20gap%20between%20the%20initial%20query%20and%20the%20target%20code.%20Additionally%2C%20we%20employ%20reinforcement%20learning%20to%20train%20an%20AlignRetriever%20that%20learns%20to%20leverage%20inference%20information%20in%20the%20enhanced%20query%20for%20more%20accurate%20retrieval.%20We%20evaluate%20AlignCoder%20on%20two%20widely-used%20benchmarks%20%28CrossCodeEval%20and%20RepoEval%29%20across%20five%20backbone%20code%20LLMs%2C%20demonstrating%20an%2018.1%25%20improvement%20in%20EM%20score%20compared%20to%20baselines%20on%20the%20CrossCodeEval%20benchmark.%20The%20results%20show%20that%20our%20framework%20achieves%20superior%20performance%20and%20exhibits%20high%20generalizability%20across%20various%20code%20LLMs%20and%20programming%20languages.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignCoder%253A%2520Aligning%2520Retrieval%2520with%2520Target%2520Intent%2520for%2520Repository-Level%2520Code%2520Completion%26entry.906535625%3DTianyue%2520Jiang%2520and%2520Yanli%2520Wang%2520and%2520Yanlin%2520Wang%2520and%2520Daya%2520Guo%2520and%2520Ensheng%2520Shi%2520and%2520Yuchi%2520Ma%2520and%2520Jiachi%2520Chen%2520and%2520Zibin%2520Zheng%26entry.1292438233%3DRepository-level%2520code%2520completion%2520remains%2520a%2520challenging%2520task%2520for%2520existing%2520code%2520large%2520language%2520models%2520%2528code%2520LLMs%2529%2520due%2520to%2520their%2520limited%2520understanding%2520of%2520repository-specific%2520context%2520and%2520domain%2520knowledge.%2520While%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520approaches%2520have%2520shown%2520promise%2520by%2520retrieving%2520relevant%2520code%2520snippets%2520as%2520cross-file%2520context%252C%2520they%2520suffer%2520from%2520two%2520fundamental%2520problems%253A%2520misalignment%2520between%2520the%2520query%2520and%2520the%2520target%2520code%2520in%2520the%2520retrieval%2520process%252C%2520and%2520the%2520inability%2520of%2520existing%2520retrieval%2520methods%2520to%2520effectively%2520utilize%2520the%2520inference%2520information.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520AlignCoder%252C%2520a%2520repository-level%2520code%2520completion%2520framework%2520that%2520introduces%2520a%2520query%2520enhancement%2520mechanism%2520and%2520a%2520reinforcement%2520learning%2520based%2520retriever%2520training%2520method.%2520Our%2520approach%2520generates%2520multiple%2520candidate%2520completions%2520to%2520construct%2520an%2520enhanced%2520query%2520that%2520bridges%2520the%2520semantic%2520gap%2520between%2520the%2520initial%2520query%2520and%2520the%2520target%2520code.%2520Additionally%252C%2520we%2520employ%2520reinforcement%2520learning%2520to%2520train%2520an%2520AlignRetriever%2520that%2520learns%2520to%2520leverage%2520inference%2520information%2520in%2520the%2520enhanced%2520query%2520for%2520more%2520accurate%2520retrieval.%2520We%2520evaluate%2520AlignCoder%2520on%2520two%2520widely-used%2520benchmarks%2520%2528CrossCodeEval%2520and%2520RepoEval%2529%2520across%2520five%2520backbone%2520code%2520LLMs%252C%2520demonstrating%2520an%252018.1%2525%2520improvement%2520in%2520EM%2520score%2520compared%2520to%2520baselines%2520on%2520the%2520CrossCodeEval%2520benchmark.%2520The%2520results%2520show%2520that%2520our%2520framework%2520achieves%2520superior%2520performance%2520and%2520exhibits%2520high%2520generalizability%2520across%2520various%2520code%2520LLMs%2520and%2520programming%2520languages.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignCoder%3A%20Aligning%20Retrieval%20with%20Target%20Intent%20for%20Repository-Level%20Code%20Completion&entry.906535625=Tianyue%20Jiang%20and%20Yanli%20Wang%20and%20Yanlin%20Wang%20and%20Daya%20Guo%20and%20Ensheng%20Shi%20and%20Yuchi%20Ma%20and%20Jiachi%20Chen%20and%20Zibin%20Zheng&entry.1292438233=Repository-level%20code%20completion%20remains%20a%20challenging%20task%20for%20existing%20code%20large%20language%20models%20%28code%20LLMs%29%20due%20to%20their%20limited%20understanding%20of%20repository-specific%20context%20and%20domain%20knowledge.%20While%20retrieval-augmented%20generation%20%28RAG%29%20approaches%20have%20shown%20promise%20by%20retrieving%20relevant%20code%20snippets%20as%20cross-file%20context%2C%20they%20suffer%20from%20two%20fundamental%20problems%3A%20misalignment%20between%20the%20query%20and%20the%20target%20code%20in%20the%20retrieval%20process%2C%20and%20the%20inability%20of%20existing%20retrieval%20methods%20to%20effectively%20utilize%20the%20inference%20information.%20To%20address%20these%20challenges%2C%20we%20propose%20AlignCoder%2C%20a%20repository-level%20code%20completion%20framework%20that%20introduces%20a%20query%20enhancement%20mechanism%20and%20a%20reinforcement%20learning%20based%20retriever%20training%20method.%20Our%20approach%20generates%20multiple%20candidate%20completions%20to%20construct%20an%20enhanced%20query%20that%20bridges%20the%20semantic%20gap%20between%20the%20initial%20query%20and%20the%20target%20code.%20Additionally%2C%20we%20employ%20reinforcement%20learning%20to%20train%20an%20AlignRetriever%20that%20learns%20to%20leverage%20inference%20information%20in%20the%20enhanced%20query%20for%20more%20accurate%20retrieval.%20We%20evaluate%20AlignCoder%20on%20two%20widely-used%20benchmarks%20%28CrossCodeEval%20and%20RepoEval%29%20across%20five%20backbone%20code%20LLMs%2C%20demonstrating%20an%2018.1%25%20improvement%20in%20EM%20score%20compared%20to%20baselines%20on%20the%20CrossCodeEval%20benchmark.%20The%20results%20show%20that%20our%20framework%20achieves%20superior%20performance%20and%20exhibits%20high%20generalizability%20across%20various%20code%20LLMs%20and%20programming%20languages.&entry.1838667208=http%3A//arxiv.org/abs/2601.19697v1&entry.124074799=Read"},
{"title": "The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments", "author": "Riccardo Giubilato and Marcus Gerhard M\u00fcller and Marco Sewtz and Laura Alejandra Encinar Gonzalez and John Folkesson and Rudolph Triebel", "abstract": "We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities. Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy. The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water. The data (rmc.dlr.de/s3li_dataset) is accompanied by an open source toolkit (github.com/DLR-RM/s3li-toolkit) providing tools for generating ground truth poses as well as preparation of labelled samples for place recognition tasks.", "link": "http://arxiv.org/abs/2601.19557v1", "date": "2026-01-27", "relevancy": 2.4169, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4914}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4906}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20S3LI%20Vulcano%20Dataset%3A%20A%20Dataset%20for%20Multi-Modal%20SLAM%20in%20Unstructured%20Planetary%20Environments&body=Title%3A%20The%20S3LI%20Vulcano%20Dataset%3A%20A%20Dataset%20for%20Multi-Modal%20SLAM%20in%20Unstructured%20Planetary%20Environments%0AAuthor%3A%20Riccardo%20Giubilato%20and%20Marcus%20Gerhard%20M%C3%BCller%20and%20Marco%20Sewtz%20and%20Laura%20Alejandra%20Encinar%20Gonzalez%20and%20John%20Folkesson%20and%20Rudolph%20Triebel%0AAbstract%3A%20We%20release%20the%20S3LI%20Vulcano%20dataset%2C%20a%20multi-modal%20dataset%20towards%20development%20and%20benchmarking%20of%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20and%20place%20recognition%20algorithms%20that%20rely%20on%20visual%20and%20LiDAR%20modalities.%20Several%20sequences%20are%20recorded%20on%20the%20volcanic%20island%20of%20Vulcano%2C%20from%20the%20Aeolian%20Islands%20in%20Sicily%2C%20Italy.%20The%20sequences%20provide%20users%20with%20data%20from%20a%20variety%20of%20environments%2C%20textures%20and%20terrains%2C%20including%20basaltic%20or%20iron-rich%20rocks%2C%20geological%20formations%20from%20old%20lava%20channels%2C%20as%20well%20as%20dry%20vegetation%20and%20water.%20The%20data%20%28rmc.dlr.de/s3li_dataset%29%20is%20accompanied%20by%20an%20open%20source%20toolkit%20%28github.com/DLR-RM/s3li-toolkit%29%20providing%20tools%20for%20generating%20ground%20truth%20poses%20as%20well%20as%20preparation%20of%20labelled%20samples%20for%20place%20recognition%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520S3LI%2520Vulcano%2520Dataset%253A%2520A%2520Dataset%2520for%2520Multi-Modal%2520SLAM%2520in%2520Unstructured%2520Planetary%2520Environments%26entry.906535625%3DRiccardo%2520Giubilato%2520and%2520Marcus%2520Gerhard%2520M%25C3%25BCller%2520and%2520Marco%2520Sewtz%2520and%2520Laura%2520Alejandra%2520Encinar%2520Gonzalez%2520and%2520John%2520Folkesson%2520and%2520Rudolph%2520Triebel%26entry.1292438233%3DWe%2520release%2520the%2520S3LI%2520Vulcano%2520dataset%252C%2520a%2520multi-modal%2520dataset%2520towards%2520development%2520and%2520benchmarking%2520of%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520and%2520place%2520recognition%2520algorithms%2520that%2520rely%2520on%2520visual%2520and%2520LiDAR%2520modalities.%2520Several%2520sequences%2520are%2520recorded%2520on%2520the%2520volcanic%2520island%2520of%2520Vulcano%252C%2520from%2520the%2520Aeolian%2520Islands%2520in%2520Sicily%252C%2520Italy.%2520The%2520sequences%2520provide%2520users%2520with%2520data%2520from%2520a%2520variety%2520of%2520environments%252C%2520textures%2520and%2520terrains%252C%2520including%2520basaltic%2520or%2520iron-rich%2520rocks%252C%2520geological%2520formations%2520from%2520old%2520lava%2520channels%252C%2520as%2520well%2520as%2520dry%2520vegetation%2520and%2520water.%2520The%2520data%2520%2528rmc.dlr.de/s3li_dataset%2529%2520is%2520accompanied%2520by%2520an%2520open%2520source%2520toolkit%2520%2528github.com/DLR-RM/s3li-toolkit%2529%2520providing%2520tools%2520for%2520generating%2520ground%2520truth%2520poses%2520as%2520well%2520as%2520preparation%2520of%2520labelled%2520samples%2520for%2520place%2520recognition%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20S3LI%20Vulcano%20Dataset%3A%20A%20Dataset%20for%20Multi-Modal%20SLAM%20in%20Unstructured%20Planetary%20Environments&entry.906535625=Riccardo%20Giubilato%20and%20Marcus%20Gerhard%20M%C3%BCller%20and%20Marco%20Sewtz%20and%20Laura%20Alejandra%20Encinar%20Gonzalez%20and%20John%20Folkesson%20and%20Rudolph%20Triebel&entry.1292438233=We%20release%20the%20S3LI%20Vulcano%20dataset%2C%20a%20multi-modal%20dataset%20towards%20development%20and%20benchmarking%20of%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20and%20place%20recognition%20algorithms%20that%20rely%20on%20visual%20and%20LiDAR%20modalities.%20Several%20sequences%20are%20recorded%20on%20the%20volcanic%20island%20of%20Vulcano%2C%20from%20the%20Aeolian%20Islands%20in%20Sicily%2C%20Italy.%20The%20sequences%20provide%20users%20with%20data%20from%20a%20variety%20of%20environments%2C%20textures%20and%20terrains%2C%20including%20basaltic%20or%20iron-rich%20rocks%2C%20geological%20formations%20from%20old%20lava%20channels%2C%20as%20well%20as%20dry%20vegetation%20and%20water.%20The%20data%20%28rmc.dlr.de/s3li_dataset%29%20is%20accompanied%20by%20an%20open%20source%20toolkit%20%28github.com/DLR-RM/s3li-toolkit%29%20providing%20tools%20for%20generating%20ground%20truth%20poses%20as%20well%20as%20preparation%20of%20labelled%20samples%20for%20place%20recognition%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.19557v1&entry.124074799=Read"},
{"title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale", "author": "Cody Kommers and Drew Hemment and Maria Antoniak and Joel Z. Leibo and Hoyt Long and Emily Robinson and Adam Sobey", "abstract": "This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions (numerical representations that enforce standardization and therefore strip human activity of the cultural context which gives it meaning). By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description (verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning). The verbal capabilities of LLMs now provide a means of at least partially automating the generation and processing of thick descriptions, offering new ways to deploy them at scale. We argue that the problem of rendering human meaning legible is not just about selecting better metrics but about developing new representational formats based on thick description. We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static.", "link": "http://arxiv.org/abs/2505.23785v2", "date": "2026-01-27", "relevancy": 2.4125, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meaning%20Is%20Not%20A%20Metric%3A%20Using%20LLMs%20to%20make%20cultural%20context%20legible%20at%20scale&body=Title%3A%20Meaning%20Is%20Not%20A%20Metric%3A%20Using%20LLMs%20to%20make%20cultural%20context%20legible%20at%20scale%0AAuthor%3A%20Cody%20Kommers%20and%20Drew%20Hemment%20and%20Maria%20Antoniak%20and%20Joel%20Z.%20Leibo%20and%20Hoyt%20Long%20and%20Emily%20Robinson%20and%20Adam%20Sobey%0AAbstract%3A%20This%20position%20paper%20argues%20that%20large%20language%20models%20%28LLMs%29%20can%20make%20cultural%20context%2C%20and%20therefore%20human%20meaning%2C%20legible%20at%20an%20unprecedented%20scale%20in%20AI-based%20sociotechnical%20systems.%20We%20argue%20that%20such%20systems%20have%20previously%20been%20unable%20to%20represent%20human%20meaning%20because%20they%20rely%20on%20thin%20descriptions%20%28numerical%20representations%20that%20enforce%20standardization%20and%20therefore%20strip%20human%20activity%20of%20the%20cultural%20context%20which%20gives%20it%20meaning%29.%20By%20contrast%2C%20scholars%20in%20the%20humanities%20and%20qualitative%20social%20sciences%20have%20developed%20frameworks%20for%20representing%20meaning%20through%20thick%20description%20%28verbal%20representations%20that%20accommodate%20heterogeneity%20and%20retain%20contextual%20information%20needed%20to%20represent%20human%20meaning%29.%20The%20verbal%20capabilities%20of%20LLMs%20now%20provide%20a%20means%20of%20at%20least%20partially%20automating%20the%20generation%20and%20processing%20of%20thick%20descriptions%2C%20offering%20new%20ways%20to%20deploy%20them%20at%20scale.%20We%20argue%20that%20the%20problem%20of%20rendering%20human%20meaning%20legible%20is%20not%20just%20about%20selecting%20better%20metrics%20but%20about%20developing%20new%20representational%20formats%20based%20on%20thick%20description.%20We%20frame%20this%20as%20a%20crucial%20direction%20for%20the%20application%20of%20generative%20AI%20and%20identify%20five%20key%20challenges%3A%20preserving%20context%2C%20maintaining%20interpretive%20pluralism%2C%20integrating%20perspectives%20based%20on%20lived%20experience%20and%20critical%20distance%2C%20distinguishing%20qualitative%20content%20from%20quantitative%20magnitude%2C%20and%20acknowledging%20meaning%20as%20dynamic%20rather%20than%20static.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeaning%2520Is%2520Not%2520A%2520Metric%253A%2520Using%2520LLMs%2520to%2520make%2520cultural%2520context%2520legible%2520at%2520scale%26entry.906535625%3DCody%2520Kommers%2520and%2520Drew%2520Hemment%2520and%2520Maria%2520Antoniak%2520and%2520Joel%2520Z.%2520Leibo%2520and%2520Hoyt%2520Long%2520and%2520Emily%2520Robinson%2520and%2520Adam%2520Sobey%26entry.1292438233%3DThis%2520position%2520paper%2520argues%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520make%2520cultural%2520context%252C%2520and%2520therefore%2520human%2520meaning%252C%2520legible%2520at%2520an%2520unprecedented%2520scale%2520in%2520AI-based%2520sociotechnical%2520systems.%2520We%2520argue%2520that%2520such%2520systems%2520have%2520previously%2520been%2520unable%2520to%2520represent%2520human%2520meaning%2520because%2520they%2520rely%2520on%2520thin%2520descriptions%2520%2528numerical%2520representations%2520that%2520enforce%2520standardization%2520and%2520therefore%2520strip%2520human%2520activity%2520of%2520the%2520cultural%2520context%2520which%2520gives%2520it%2520meaning%2529.%2520By%2520contrast%252C%2520scholars%2520in%2520the%2520humanities%2520and%2520qualitative%2520social%2520sciences%2520have%2520developed%2520frameworks%2520for%2520representing%2520meaning%2520through%2520thick%2520description%2520%2528verbal%2520representations%2520that%2520accommodate%2520heterogeneity%2520and%2520retain%2520contextual%2520information%2520needed%2520to%2520represent%2520human%2520meaning%2529.%2520The%2520verbal%2520capabilities%2520of%2520LLMs%2520now%2520provide%2520a%2520means%2520of%2520at%2520least%2520partially%2520automating%2520the%2520generation%2520and%2520processing%2520of%2520thick%2520descriptions%252C%2520offering%2520new%2520ways%2520to%2520deploy%2520them%2520at%2520scale.%2520We%2520argue%2520that%2520the%2520problem%2520of%2520rendering%2520human%2520meaning%2520legible%2520is%2520not%2520just%2520about%2520selecting%2520better%2520metrics%2520but%2520about%2520developing%2520new%2520representational%2520formats%2520based%2520on%2520thick%2520description.%2520We%2520frame%2520this%2520as%2520a%2520crucial%2520direction%2520for%2520the%2520application%2520of%2520generative%2520AI%2520and%2520identify%2520five%2520key%2520challenges%253A%2520preserving%2520context%252C%2520maintaining%2520interpretive%2520pluralism%252C%2520integrating%2520perspectives%2520based%2520on%2520lived%2520experience%2520and%2520critical%2520distance%252C%2520distinguishing%2520qualitative%2520content%2520from%2520quantitative%2520magnitude%252C%2520and%2520acknowledging%2520meaning%2520as%2520dynamic%2520rather%2520than%2520static.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meaning%20Is%20Not%20A%20Metric%3A%20Using%20LLMs%20to%20make%20cultural%20context%20legible%20at%20scale&entry.906535625=Cody%20Kommers%20and%20Drew%20Hemment%20and%20Maria%20Antoniak%20and%20Joel%20Z.%20Leibo%20and%20Hoyt%20Long%20and%20Emily%20Robinson%20and%20Adam%20Sobey&entry.1292438233=This%20position%20paper%20argues%20that%20large%20language%20models%20%28LLMs%29%20can%20make%20cultural%20context%2C%20and%20therefore%20human%20meaning%2C%20legible%20at%20an%20unprecedented%20scale%20in%20AI-based%20sociotechnical%20systems.%20We%20argue%20that%20such%20systems%20have%20previously%20been%20unable%20to%20represent%20human%20meaning%20because%20they%20rely%20on%20thin%20descriptions%20%28numerical%20representations%20that%20enforce%20standardization%20and%20therefore%20strip%20human%20activity%20of%20the%20cultural%20context%20which%20gives%20it%20meaning%29.%20By%20contrast%2C%20scholars%20in%20the%20humanities%20and%20qualitative%20social%20sciences%20have%20developed%20frameworks%20for%20representing%20meaning%20through%20thick%20description%20%28verbal%20representations%20that%20accommodate%20heterogeneity%20and%20retain%20contextual%20information%20needed%20to%20represent%20human%20meaning%29.%20The%20verbal%20capabilities%20of%20LLMs%20now%20provide%20a%20means%20of%20at%20least%20partially%20automating%20the%20generation%20and%20processing%20of%20thick%20descriptions%2C%20offering%20new%20ways%20to%20deploy%20them%20at%20scale.%20We%20argue%20that%20the%20problem%20of%20rendering%20human%20meaning%20legible%20is%20not%20just%20about%20selecting%20better%20metrics%20but%20about%20developing%20new%20representational%20formats%20based%20on%20thick%20description.%20We%20frame%20this%20as%20a%20crucial%20direction%20for%20the%20application%20of%20generative%20AI%20and%20identify%20five%20key%20challenges%3A%20preserving%20context%2C%20maintaining%20interpretive%20pluralism%2C%20integrating%20perspectives%20based%20on%20lived%20experience%20and%20critical%20distance%2C%20distinguishing%20qualitative%20content%20from%20quantitative%20magnitude%2C%20and%20acknowledging%20meaning%20as%20dynamic%20rather%20than%20static.&entry.1838667208=http%3A//arxiv.org/abs/2505.23785v2&entry.124074799=Read"},
{"title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes", "author": "Yifan Wang and Jichen Zheng and Jingyuan Sun and Yunhao Zhang and Chunyu Ye and Jixing Li and Chengqing Zong and Shaonan Wang", "abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.", "link": "http://arxiv.org/abs/2601.19723v1", "date": "2026-01-27", "relevancy": 2.3949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Component-Level%20Lesioning%20of%20Language%20Models%20Reveals%20Clinically%20Aligned%20Aphasia%20Phenotypes&body=Title%3A%20Component-Level%20Lesioning%20of%20Language%20Models%20Reveals%20Clinically%20Aligned%20Aphasia%20Phenotypes%0AAuthor%3A%20Yifan%20Wang%20and%20Jichen%20Zheng%20and%20Jingyuan%20Sun%20and%20Yunhao%20Zhang%20and%20Chunyu%20Ye%20and%20Jixing%20Li%20and%20Chengqing%20Zong%20and%20Shaonan%20Wang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20increasingly%20exhibit%20human-like%20linguistic%20behaviors%20and%20internal%20representations%20that%20they%20could%20serve%20as%20computational%20simulators%20of%20language%20cognition.%20We%20ask%20whether%20LLMs%20can%20be%20systematically%20manipulated%20to%20reproduce%20language-production%20impairments%20characteristic%20of%20aphasia%20following%20focal%20brain%20lesions.%20Such%20models%20could%20provide%20scalable%20proxies%20for%20testing%20rehabilitation%20hypotheses%2C%20and%20offer%20a%20controlled%20framework%20for%20probing%20the%20functional%20organization%20of%20language.%20We%20introduce%20a%20clinically%20grounded%2C%20component-level%20framework%20that%20simulates%20aphasia%20by%20selectively%20perturbing%20functional%20components%20in%20LLMs%2C%20and%20apply%20it%20to%20both%20modular%20Mixture-of-Experts%20models%20and%20dense%20Transformers%20using%20a%20unified%20intervention%20interface.%20Our%20pipeline%20%28i%29%20identifies%20subtype-linked%20components%20for%20Broca%27s%20and%20Wernicke%27s%20aphasia%2C%20%28ii%29%20interprets%20these%20components%20with%20linguistic%20probing%20tasks%2C%20and%20%28iii%29%20induces%20graded%20impairments%20by%20progressively%20perturbing%20the%20top-k%20subtype-linked%20components%2C%20evaluating%20outcomes%20with%20Western%20Aphasia%20Battery%20%28WAB%29%20subtests%20summarized%20by%20Aphasia%20Quotient%20%28AQ%29.%20Across%20architectures%20and%20lesioning%20strategies%2C%20subtype-targeted%20perturbations%20yield%20more%20systematic%2C%20aphasia-like%20regressions%20than%20size-matched%20random%20perturbations%2C%20and%20MoE%20modularity%20supports%20more%20localized%20and%20interpretable%20phenotype-to-component%20mappings.%20These%20findings%20suggest%20that%20modular%20LLMs%2C%20combined%20with%20clinically%20informed%20component%20perturbations%2C%20provide%20a%20promising%20platform%20for%20simulating%20aphasic%20language%20production%20and%20studying%20how%20distinct%20language%20functions%20degrade%20under%20targeted%20disruptions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComponent-Level%2520Lesioning%2520of%2520Language%2520Models%2520Reveals%2520Clinically%2520Aligned%2520Aphasia%2520Phenotypes%26entry.906535625%3DYifan%2520Wang%2520and%2520Jichen%2520Zheng%2520and%2520Jingyuan%2520Sun%2520and%2520Yunhao%2520Zhang%2520and%2520Chunyu%2520Ye%2520and%2520Jixing%2520Li%2520and%2520Chengqing%2520Zong%2520and%2520Shaonan%2520Wang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520exhibit%2520human-like%2520linguistic%2520behaviors%2520and%2520internal%2520representations%2520that%2520they%2520could%2520serve%2520as%2520computational%2520simulators%2520of%2520language%2520cognition.%2520We%2520ask%2520whether%2520LLMs%2520can%2520be%2520systematically%2520manipulated%2520to%2520reproduce%2520language-production%2520impairments%2520characteristic%2520of%2520aphasia%2520following%2520focal%2520brain%2520lesions.%2520Such%2520models%2520could%2520provide%2520scalable%2520proxies%2520for%2520testing%2520rehabilitation%2520hypotheses%252C%2520and%2520offer%2520a%2520controlled%2520framework%2520for%2520probing%2520the%2520functional%2520organization%2520of%2520language.%2520We%2520introduce%2520a%2520clinically%2520grounded%252C%2520component-level%2520framework%2520that%2520simulates%2520aphasia%2520by%2520selectively%2520perturbing%2520functional%2520components%2520in%2520LLMs%252C%2520and%2520apply%2520it%2520to%2520both%2520modular%2520Mixture-of-Experts%2520models%2520and%2520dense%2520Transformers%2520using%2520a%2520unified%2520intervention%2520interface.%2520Our%2520pipeline%2520%2528i%2529%2520identifies%2520subtype-linked%2520components%2520for%2520Broca%2527s%2520and%2520Wernicke%2527s%2520aphasia%252C%2520%2528ii%2529%2520interprets%2520these%2520components%2520with%2520linguistic%2520probing%2520tasks%252C%2520and%2520%2528iii%2529%2520induces%2520graded%2520impairments%2520by%2520progressively%2520perturbing%2520the%2520top-k%2520subtype-linked%2520components%252C%2520evaluating%2520outcomes%2520with%2520Western%2520Aphasia%2520Battery%2520%2528WAB%2529%2520subtests%2520summarized%2520by%2520Aphasia%2520Quotient%2520%2528AQ%2529.%2520Across%2520architectures%2520and%2520lesioning%2520strategies%252C%2520subtype-targeted%2520perturbations%2520yield%2520more%2520systematic%252C%2520aphasia-like%2520regressions%2520than%2520size-matched%2520random%2520perturbations%252C%2520and%2520MoE%2520modularity%2520supports%2520more%2520localized%2520and%2520interpretable%2520phenotype-to-component%2520mappings.%2520These%2520findings%2520suggest%2520that%2520modular%2520LLMs%252C%2520combined%2520with%2520clinically%2520informed%2520component%2520perturbations%252C%2520provide%2520a%2520promising%2520platform%2520for%2520simulating%2520aphasic%2520language%2520production%2520and%2520studying%2520how%2520distinct%2520language%2520functions%2520degrade%2520under%2520targeted%2520disruptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Component-Level%20Lesioning%20of%20Language%20Models%20Reveals%20Clinically%20Aligned%20Aphasia%20Phenotypes&entry.906535625=Yifan%20Wang%20and%20Jichen%20Zheng%20and%20Jingyuan%20Sun%20and%20Yunhao%20Zhang%20and%20Chunyu%20Ye%20and%20Jixing%20Li%20and%20Chengqing%20Zong%20and%20Shaonan%20Wang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20increasingly%20exhibit%20human-like%20linguistic%20behaviors%20and%20internal%20representations%20that%20they%20could%20serve%20as%20computational%20simulators%20of%20language%20cognition.%20We%20ask%20whether%20LLMs%20can%20be%20systematically%20manipulated%20to%20reproduce%20language-production%20impairments%20characteristic%20of%20aphasia%20following%20focal%20brain%20lesions.%20Such%20models%20could%20provide%20scalable%20proxies%20for%20testing%20rehabilitation%20hypotheses%2C%20and%20offer%20a%20controlled%20framework%20for%20probing%20the%20functional%20organization%20of%20language.%20We%20introduce%20a%20clinically%20grounded%2C%20component-level%20framework%20that%20simulates%20aphasia%20by%20selectively%20perturbing%20functional%20components%20in%20LLMs%2C%20and%20apply%20it%20to%20both%20modular%20Mixture-of-Experts%20models%20and%20dense%20Transformers%20using%20a%20unified%20intervention%20interface.%20Our%20pipeline%20%28i%29%20identifies%20subtype-linked%20components%20for%20Broca%27s%20and%20Wernicke%27s%20aphasia%2C%20%28ii%29%20interprets%20these%20components%20with%20linguistic%20probing%20tasks%2C%20and%20%28iii%29%20induces%20graded%20impairments%20by%20progressively%20perturbing%20the%20top-k%20subtype-linked%20components%2C%20evaluating%20outcomes%20with%20Western%20Aphasia%20Battery%20%28WAB%29%20subtests%20summarized%20by%20Aphasia%20Quotient%20%28AQ%29.%20Across%20architectures%20and%20lesioning%20strategies%2C%20subtype-targeted%20perturbations%20yield%20more%20systematic%2C%20aphasia-like%20regressions%20than%20size-matched%20random%20perturbations%2C%20and%20MoE%20modularity%20supports%20more%20localized%20and%20interpretable%20phenotype-to-component%20mappings.%20These%20findings%20suggest%20that%20modular%20LLMs%2C%20combined%20with%20clinically%20informed%20component%20perturbations%2C%20provide%20a%20promising%20platform%20for%20simulating%20aphasic%20language%20production%20and%20studying%20how%20distinct%20language%20functions%20degrade%20under%20targeted%20disruptions.&entry.1838667208=http%3A//arxiv.org/abs/2601.19723v1&entry.124074799=Read"},
{"title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context", "author": "Lei Zhang and Yongda Yu and Minghui Yu and Xinxin Guo and Zhengqi Zhuang and Guoping Rong and Dong Shao and Haifeng Shen and Hongyu Kuang and Zhengfeng Li and Boge Wang and Guoan Zhang and Bangyu Xiang and Xiaobing Xu", "abstract": "High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an \"AI-assisted, Expert-verified\" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285\\% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .", "link": "http://arxiv.org/abs/2601.19494v1", "date": "2026-01-27", "relevancy": 2.3934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AACR-Bench%3A%20Evaluating%20Automatic%20Code%20Review%20with%20Holistic%20Repository-Level%20Context&body=Title%3A%20AACR-Bench%3A%20Evaluating%20Automatic%20Code%20Review%20with%20Holistic%20Repository-Level%20Context%0AAuthor%3A%20Lei%20Zhang%20and%20Yongda%20Yu%20and%20Minghui%20Yu%20and%20Xinxin%20Guo%20and%20Zhengqi%20Zhuang%20and%20Guoping%20Rong%20and%20Dong%20Shao%20and%20Haifeng%20Shen%20and%20Hongyu%20Kuang%20and%20Zhengfeng%20Li%20and%20Boge%20Wang%20and%20Guoan%20Zhang%20and%20Bangyu%20Xiang%20and%20Xiaobing%20Xu%0AAbstract%3A%20High-quality%20evaluation%20benchmarks%20are%20pivotal%20for%20deploying%20Large%20Language%20Models%20%28LLMs%29%20in%20Automated%20Code%20Review%20%28ACR%29.%20However%2C%20existing%20benchmarks%20suffer%20from%20two%20critical%20limitations%3A%20first%2C%20the%20lack%20of%20multi-language%20support%20in%20repository-level%20contexts%2C%20which%20restricts%20the%20generalizability%20of%20evaluation%20results%3B%20second%2C%20the%20reliance%20on%20noisy%2C%20incomplete%20ground%20truth%20derived%20from%20raw%20Pull%20Request%20%28PR%29%20comments%2C%20which%20constrains%20the%20scope%20of%20issue%20detection.%20To%20address%20these%20challenges%2C%20we%20introduce%20AACR-Bench%20a%20comprehensive%20benchmark%20that%20provides%20full%20cross-file%20context%20across%20multiple%20programming%20languages.%20Unlike%20traditional%20datasets%2C%20AACR-Bench%20employs%20an%20%22AI-assisted%2C%20Expert-verified%22%20annotation%20pipeline%20to%20uncover%20latent%20defects%20often%20overlooked%20in%20original%20PRs%2C%20resulting%20in%20a%20285%5C%25%20increase%20in%20defect%20coverage.%20Extensive%20evaluations%20of%20mainstream%20LLMs%20on%20AACR-Bench%20reveal%20that%20previous%20assessments%20may%20have%20either%20misjudged%20or%20only%20partially%20captured%20model%20capabilities%20due%20to%20data%20limitations.%20Our%20work%20establishes%20a%20more%20rigorous%20standard%20for%20ACR%20evaluation%20and%20offers%20new%20insights%20on%20LLM%20based%20ACR%2C%20i.e.%2C%20the%20granularity/level%20of%20context%20and%20the%20choice%20of%20retrieval%20methods%20significantly%20impact%20ACR%20performance%2C%20and%20this%20influence%20varies%20depending%20on%20the%20LLM%2C%20programming%20language%2C%20and%20the%20LLM%20usage%20paradigm%20e.g.%2C%20whether%20an%20Agent%20architecture%20is%20employed.%20The%20code%2C%20data%2C%20and%20other%20artifacts%20of%20our%20evaluation%20set%20are%20available%20at%20https%3A//github.com/alibaba/aacr-bench%20.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAACR-Bench%253A%2520Evaluating%2520Automatic%2520Code%2520Review%2520with%2520Holistic%2520Repository-Level%2520Context%26entry.906535625%3DLei%2520Zhang%2520and%2520Yongda%2520Yu%2520and%2520Minghui%2520Yu%2520and%2520Xinxin%2520Guo%2520and%2520Zhengqi%2520Zhuang%2520and%2520Guoping%2520Rong%2520and%2520Dong%2520Shao%2520and%2520Haifeng%2520Shen%2520and%2520Hongyu%2520Kuang%2520and%2520Zhengfeng%2520Li%2520and%2520Boge%2520Wang%2520and%2520Guoan%2520Zhang%2520and%2520Bangyu%2520Xiang%2520and%2520Xiaobing%2520Xu%26entry.1292438233%3DHigh-quality%2520evaluation%2520benchmarks%2520are%2520pivotal%2520for%2520deploying%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520Automated%2520Code%2520Review%2520%2528ACR%2529.%2520However%252C%2520existing%2520benchmarks%2520suffer%2520from%2520two%2520critical%2520limitations%253A%2520first%252C%2520the%2520lack%2520of%2520multi-language%2520support%2520in%2520repository-level%2520contexts%252C%2520which%2520restricts%2520the%2520generalizability%2520of%2520evaluation%2520results%253B%2520second%252C%2520the%2520reliance%2520on%2520noisy%252C%2520incomplete%2520ground%2520truth%2520derived%2520from%2520raw%2520Pull%2520Request%2520%2528PR%2529%2520comments%252C%2520which%2520constrains%2520the%2520scope%2520of%2520issue%2520detection.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520AACR-Bench%2520a%2520comprehensive%2520benchmark%2520that%2520provides%2520full%2520cross-file%2520context%2520across%2520multiple%2520programming%2520languages.%2520Unlike%2520traditional%2520datasets%252C%2520AACR-Bench%2520employs%2520an%2520%2522AI-assisted%252C%2520Expert-verified%2522%2520annotation%2520pipeline%2520to%2520uncover%2520latent%2520defects%2520often%2520overlooked%2520in%2520original%2520PRs%252C%2520resulting%2520in%2520a%2520285%255C%2525%2520increase%2520in%2520defect%2520coverage.%2520Extensive%2520evaluations%2520of%2520mainstream%2520LLMs%2520on%2520AACR-Bench%2520reveal%2520that%2520previous%2520assessments%2520may%2520have%2520either%2520misjudged%2520or%2520only%2520partially%2520captured%2520model%2520capabilities%2520due%2520to%2520data%2520limitations.%2520Our%2520work%2520establishes%2520a%2520more%2520rigorous%2520standard%2520for%2520ACR%2520evaluation%2520and%2520offers%2520new%2520insights%2520on%2520LLM%2520based%2520ACR%252C%2520i.e.%252C%2520the%2520granularity/level%2520of%2520context%2520and%2520the%2520choice%2520of%2520retrieval%2520methods%2520significantly%2520impact%2520ACR%2520performance%252C%2520and%2520this%2520influence%2520varies%2520depending%2520on%2520the%2520LLM%252C%2520programming%2520language%252C%2520and%2520the%2520LLM%2520usage%2520paradigm%2520e.g.%252C%2520whether%2520an%2520Agent%2520architecture%2520is%2520employed.%2520The%2520code%252C%2520data%252C%2520and%2520other%2520artifacts%2520of%2520our%2520evaluation%2520set%2520are%2520available%2520at%2520https%253A//github.com/alibaba/aacr-bench%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AACR-Bench%3A%20Evaluating%20Automatic%20Code%20Review%20with%20Holistic%20Repository-Level%20Context&entry.906535625=Lei%20Zhang%20and%20Yongda%20Yu%20and%20Minghui%20Yu%20and%20Xinxin%20Guo%20and%20Zhengqi%20Zhuang%20and%20Guoping%20Rong%20and%20Dong%20Shao%20and%20Haifeng%20Shen%20and%20Hongyu%20Kuang%20and%20Zhengfeng%20Li%20and%20Boge%20Wang%20and%20Guoan%20Zhang%20and%20Bangyu%20Xiang%20and%20Xiaobing%20Xu&entry.1292438233=High-quality%20evaluation%20benchmarks%20are%20pivotal%20for%20deploying%20Large%20Language%20Models%20%28LLMs%29%20in%20Automated%20Code%20Review%20%28ACR%29.%20However%2C%20existing%20benchmarks%20suffer%20from%20two%20critical%20limitations%3A%20first%2C%20the%20lack%20of%20multi-language%20support%20in%20repository-level%20contexts%2C%20which%20restricts%20the%20generalizability%20of%20evaluation%20results%3B%20second%2C%20the%20reliance%20on%20noisy%2C%20incomplete%20ground%20truth%20derived%20from%20raw%20Pull%20Request%20%28PR%29%20comments%2C%20which%20constrains%20the%20scope%20of%20issue%20detection.%20To%20address%20these%20challenges%2C%20we%20introduce%20AACR-Bench%20a%20comprehensive%20benchmark%20that%20provides%20full%20cross-file%20context%20across%20multiple%20programming%20languages.%20Unlike%20traditional%20datasets%2C%20AACR-Bench%20employs%20an%20%22AI-assisted%2C%20Expert-verified%22%20annotation%20pipeline%20to%20uncover%20latent%20defects%20often%20overlooked%20in%20original%20PRs%2C%20resulting%20in%20a%20285%5C%25%20increase%20in%20defect%20coverage.%20Extensive%20evaluations%20of%20mainstream%20LLMs%20on%20AACR-Bench%20reveal%20that%20previous%20assessments%20may%20have%20either%20misjudged%20or%20only%20partially%20captured%20model%20capabilities%20due%20to%20data%20limitations.%20Our%20work%20establishes%20a%20more%20rigorous%20standard%20for%20ACR%20evaluation%20and%20offers%20new%20insights%20on%20LLM%20based%20ACR%2C%20i.e.%2C%20the%20granularity/level%20of%20context%20and%20the%20choice%20of%20retrieval%20methods%20significantly%20impact%20ACR%20performance%2C%20and%20this%20influence%20varies%20depending%20on%20the%20LLM%2C%20programming%20language%2C%20and%20the%20LLM%20usage%20paradigm%20e.g.%2C%20whether%20an%20Agent%20architecture%20is%20employed.%20The%20code%2C%20data%2C%20and%20other%20artifacts%20of%20our%20evaluation%20set%20are%20available%20at%20https%3A//github.com/alibaba/aacr-bench%20.&entry.1838667208=http%3A//arxiv.org/abs/2601.19494v1&entry.124074799=Read"},
{"title": "Provable Learning of Random Hierarchy Models and Hierarchical Shallow-to-Deep Chaining", "author": "Yunwei Ren and Yatin Dandi and Florent Krzakala and Jason D. Lee", "abstract": "The empirical success of deep learning is often attributed to deep networks' ability to exploit hierarchical structure in data, constructing increasingly complex features across layers. Yet despite substantial progress in deep learning theory, most optimization results sill focus on networks with only two or three layers, leaving the theoretical understanding of hierarchical learning in genuinely deep models limited. This leads to a natural question: can we prove that deep networks, trained by gradient-based methods, can efficiently exploit hierarchical structure?\n  In this work, we consider Random Hierarchy Models -- a hierarchical context-free grammar introduced by arXiv:2307.02129 and conjectured to separate deep and shallow networks. We prove that, under mild conditions, a deep convolutional network can be efficiently trained to learn this function class. Our proof builds on a general observation: if intermediate layers can receive clean signal from the labels and the relevant features are weakly identifiable, then layerwise training each individual layer suffices to hierarchically learn the target function.", "link": "http://arxiv.org/abs/2601.19756v1", "date": "2026-01-27", "relevancy": 2.3727, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Learning%20of%20Random%20Hierarchy%20Models%20and%20Hierarchical%20Shallow-to-Deep%20Chaining&body=Title%3A%20Provable%20Learning%20of%20Random%20Hierarchy%20Models%20and%20Hierarchical%20Shallow-to-Deep%20Chaining%0AAuthor%3A%20Yunwei%20Ren%20and%20Yatin%20Dandi%20and%20Florent%20Krzakala%20and%20Jason%20D.%20Lee%0AAbstract%3A%20The%20empirical%20success%20of%20deep%20learning%20is%20often%20attributed%20to%20deep%20networks%27%20ability%20to%20exploit%20hierarchical%20structure%20in%20data%2C%20constructing%20increasingly%20complex%20features%20across%20layers.%20Yet%20despite%20substantial%20progress%20in%20deep%20learning%20theory%2C%20most%20optimization%20results%20sill%20focus%20on%20networks%20with%20only%20two%20or%20three%20layers%2C%20leaving%20the%20theoretical%20understanding%20of%20hierarchical%20learning%20in%20genuinely%20deep%20models%20limited.%20This%20leads%20to%20a%20natural%20question%3A%20can%20we%20prove%20that%20deep%20networks%2C%20trained%20by%20gradient-based%20methods%2C%20can%20efficiently%20exploit%20hierarchical%20structure%3F%0A%20%20In%20this%20work%2C%20we%20consider%20Random%20Hierarchy%20Models%20--%20a%20hierarchical%20context-free%20grammar%20introduced%20by%20arXiv%3A2307.02129%20and%20conjectured%20to%20separate%20deep%20and%20shallow%20networks.%20We%20prove%20that%2C%20under%20mild%20conditions%2C%20a%20deep%20convolutional%20network%20can%20be%20efficiently%20trained%20to%20learn%20this%20function%20class.%20Our%20proof%20builds%20on%20a%20general%20observation%3A%20if%20intermediate%20layers%20can%20receive%20clean%20signal%20from%20the%20labels%20and%20the%20relevant%20features%20are%20weakly%20identifiable%2C%20then%20layerwise%20training%20each%20individual%20layer%20suffices%20to%20hierarchically%20learn%20the%20target%20function.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Learning%2520of%2520Random%2520Hierarchy%2520Models%2520and%2520Hierarchical%2520Shallow-to-Deep%2520Chaining%26entry.906535625%3DYunwei%2520Ren%2520and%2520Yatin%2520Dandi%2520and%2520Florent%2520Krzakala%2520and%2520Jason%2520D.%2520Lee%26entry.1292438233%3DThe%2520empirical%2520success%2520of%2520deep%2520learning%2520is%2520often%2520attributed%2520to%2520deep%2520networks%2527%2520ability%2520to%2520exploit%2520hierarchical%2520structure%2520in%2520data%252C%2520constructing%2520increasingly%2520complex%2520features%2520across%2520layers.%2520Yet%2520despite%2520substantial%2520progress%2520in%2520deep%2520learning%2520theory%252C%2520most%2520optimization%2520results%2520sill%2520focus%2520on%2520networks%2520with%2520only%2520two%2520or%2520three%2520layers%252C%2520leaving%2520the%2520theoretical%2520understanding%2520of%2520hierarchical%2520learning%2520in%2520genuinely%2520deep%2520models%2520limited.%2520This%2520leads%2520to%2520a%2520natural%2520question%253A%2520can%2520we%2520prove%2520that%2520deep%2520networks%252C%2520trained%2520by%2520gradient-based%2520methods%252C%2520can%2520efficiently%2520exploit%2520hierarchical%2520structure%253F%250A%2520%2520In%2520this%2520work%252C%2520we%2520consider%2520Random%2520Hierarchy%2520Models%2520--%2520a%2520hierarchical%2520context-free%2520grammar%2520introduced%2520by%2520arXiv%253A2307.02129%2520and%2520conjectured%2520to%2520separate%2520deep%2520and%2520shallow%2520networks.%2520We%2520prove%2520that%252C%2520under%2520mild%2520conditions%252C%2520a%2520deep%2520convolutional%2520network%2520can%2520be%2520efficiently%2520trained%2520to%2520learn%2520this%2520function%2520class.%2520Our%2520proof%2520builds%2520on%2520a%2520general%2520observation%253A%2520if%2520intermediate%2520layers%2520can%2520receive%2520clean%2520signal%2520from%2520the%2520labels%2520and%2520the%2520relevant%2520features%2520are%2520weakly%2520identifiable%252C%2520then%2520layerwise%2520training%2520each%2520individual%2520layer%2520suffices%2520to%2520hierarchically%2520learn%2520the%2520target%2520function.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Learning%20of%20Random%20Hierarchy%20Models%20and%20Hierarchical%20Shallow-to-Deep%20Chaining&entry.906535625=Yunwei%20Ren%20and%20Yatin%20Dandi%20and%20Florent%20Krzakala%20and%20Jason%20D.%20Lee&entry.1292438233=The%20empirical%20success%20of%20deep%20learning%20is%20often%20attributed%20to%20deep%20networks%27%20ability%20to%20exploit%20hierarchical%20structure%20in%20data%2C%20constructing%20increasingly%20complex%20features%20across%20layers.%20Yet%20despite%20substantial%20progress%20in%20deep%20learning%20theory%2C%20most%20optimization%20results%20sill%20focus%20on%20networks%20with%20only%20two%20or%20three%20layers%2C%20leaving%20the%20theoretical%20understanding%20of%20hierarchical%20learning%20in%20genuinely%20deep%20models%20limited.%20This%20leads%20to%20a%20natural%20question%3A%20can%20we%20prove%20that%20deep%20networks%2C%20trained%20by%20gradient-based%20methods%2C%20can%20efficiently%20exploit%20hierarchical%20structure%3F%0A%20%20In%20this%20work%2C%20we%20consider%20Random%20Hierarchy%20Models%20--%20a%20hierarchical%20context-free%20grammar%20introduced%20by%20arXiv%3A2307.02129%20and%20conjectured%20to%20separate%20deep%20and%20shallow%20networks.%20We%20prove%20that%2C%20under%20mild%20conditions%2C%20a%20deep%20convolutional%20network%20can%20be%20efficiently%20trained%20to%20learn%20this%20function%20class.%20Our%20proof%20builds%20on%20a%20general%20observation%3A%20if%20intermediate%20layers%20can%20receive%20clean%20signal%20from%20the%20labels%20and%20the%20relevant%20features%20are%20weakly%20identifiable%2C%20then%20layerwise%20training%20each%20individual%20layer%20suffices%20to%20hierarchically%20learn%20the%20target%20function.&entry.1838667208=http%3A//arxiv.org/abs/2601.19756v1&entry.124074799=Read"},
{"title": "Knowledge-Aware Evolution for Streaming Federated Continual Learning with Category Overlap and without Task Identifiers", "author": "Sixing Tan and Xianmin Liu", "abstract": "Federated Continual Learning (FCL) leverages inter-client collaboration to balance new knowledge acquisition and prior knowledge retention in non-stationary data. However, existing batch-based FCL methods lack adaptability to streaming scenarios featuring category overlap between old and new data and absent task identifiers, leading to indistinguishability of old and new knowledge, uncertain task assignments for samples, and knowledge confusion.To address this, we propose streaming federated continual learning setting: per federated learning (FL) round, clients process streaming data with disjoint samples and potentially overlapping categories without task identifiers, necessitating sustained inference capability for all prior categories after each FL round.Next, we introduce FedKACE: 1) an adaptive inference model switching mechanism that enables unidirectional switching from local model to global model to achieve a trade-off between personalization and generalization; 2) a adaptive gradient-balanced replay scheme that reconciles new knowledge learning and old knowledge retention under overlapping-class scenarios; 3) a kernel spectral boundary buffer maintenance that preserves high-information and high-boundary-influence samples to optimize cross-round knowledge retention. Experiments across multiple scenarios and regret analysis demonstrate the effectiveness of FedKACE.", "link": "http://arxiv.org/abs/2601.19788v1", "date": "2026-01-27", "relevancy": 2.365, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4935}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4635}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Aware%20Evolution%20for%20Streaming%20Federated%20Continual%20Learning%20with%20Category%20Overlap%20and%20without%20Task%20Identifiers&body=Title%3A%20Knowledge-Aware%20Evolution%20for%20Streaming%20Federated%20Continual%20Learning%20with%20Category%20Overlap%20and%20without%20Task%20Identifiers%0AAuthor%3A%20Sixing%20Tan%20and%20Xianmin%20Liu%0AAbstract%3A%20Federated%20Continual%20Learning%20%28FCL%29%20leverages%20inter-client%20collaboration%20to%20balance%20new%20knowledge%20acquisition%20and%20prior%20knowledge%20retention%20in%20non-stationary%20data.%20However%2C%20existing%20batch-based%20FCL%20methods%20lack%20adaptability%20to%20streaming%20scenarios%20featuring%20category%20overlap%20between%20old%20and%20new%20data%20and%20absent%20task%20identifiers%2C%20leading%20to%20indistinguishability%20of%20old%20and%20new%20knowledge%2C%20uncertain%20task%20assignments%20for%20samples%2C%20and%20knowledge%20confusion.To%20address%20this%2C%20we%20propose%20streaming%20federated%20continual%20learning%20setting%3A%20per%20federated%20learning%20%28FL%29%20round%2C%20clients%20process%20streaming%20data%20with%20disjoint%20samples%20and%20potentially%20overlapping%20categories%20without%20task%20identifiers%2C%20necessitating%20sustained%20inference%20capability%20for%20all%20prior%20categories%20after%20each%20FL%20round.Next%2C%20we%20introduce%20FedKACE%3A%201%29%20an%20adaptive%20inference%20model%20switching%20mechanism%20that%20enables%20unidirectional%20switching%20from%20local%20model%20to%20global%20model%20to%20achieve%20a%20trade-off%20between%20personalization%20and%20generalization%3B%202%29%20a%20adaptive%20gradient-balanced%20replay%20scheme%20that%20reconciles%20new%20knowledge%20learning%20and%20old%20knowledge%20retention%20under%20overlapping-class%20scenarios%3B%203%29%20a%20kernel%20spectral%20boundary%20buffer%20maintenance%20that%20preserves%20high-information%20and%20high-boundary-influence%20samples%20to%20optimize%20cross-round%20knowledge%20retention.%20Experiments%20across%20multiple%20scenarios%20and%20regret%20analysis%20demonstrate%20the%20effectiveness%20of%20FedKACE.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Aware%2520Evolution%2520for%2520Streaming%2520Federated%2520Continual%2520Learning%2520with%2520Category%2520Overlap%2520and%2520without%2520Task%2520Identifiers%26entry.906535625%3DSixing%2520Tan%2520and%2520Xianmin%2520Liu%26entry.1292438233%3DFederated%2520Continual%2520Learning%2520%2528FCL%2529%2520leverages%2520inter-client%2520collaboration%2520to%2520balance%2520new%2520knowledge%2520acquisition%2520and%2520prior%2520knowledge%2520retention%2520in%2520non-stationary%2520data.%2520However%252C%2520existing%2520batch-based%2520FCL%2520methods%2520lack%2520adaptability%2520to%2520streaming%2520scenarios%2520featuring%2520category%2520overlap%2520between%2520old%2520and%2520new%2520data%2520and%2520absent%2520task%2520identifiers%252C%2520leading%2520to%2520indistinguishability%2520of%2520old%2520and%2520new%2520knowledge%252C%2520uncertain%2520task%2520assignments%2520for%2520samples%252C%2520and%2520knowledge%2520confusion.To%2520address%2520this%252C%2520we%2520propose%2520streaming%2520federated%2520continual%2520learning%2520setting%253A%2520per%2520federated%2520learning%2520%2528FL%2529%2520round%252C%2520clients%2520process%2520streaming%2520data%2520with%2520disjoint%2520samples%2520and%2520potentially%2520overlapping%2520categories%2520without%2520task%2520identifiers%252C%2520necessitating%2520sustained%2520inference%2520capability%2520for%2520all%2520prior%2520categories%2520after%2520each%2520FL%2520round.Next%252C%2520we%2520introduce%2520FedKACE%253A%25201%2529%2520an%2520adaptive%2520inference%2520model%2520switching%2520mechanism%2520that%2520enables%2520unidirectional%2520switching%2520from%2520local%2520model%2520to%2520global%2520model%2520to%2520achieve%2520a%2520trade-off%2520between%2520personalization%2520and%2520generalization%253B%25202%2529%2520a%2520adaptive%2520gradient-balanced%2520replay%2520scheme%2520that%2520reconciles%2520new%2520knowledge%2520learning%2520and%2520old%2520knowledge%2520retention%2520under%2520overlapping-class%2520scenarios%253B%25203%2529%2520a%2520kernel%2520spectral%2520boundary%2520buffer%2520maintenance%2520that%2520preserves%2520high-information%2520and%2520high-boundary-influence%2520samples%2520to%2520optimize%2520cross-round%2520knowledge%2520retention.%2520Experiments%2520across%2520multiple%2520scenarios%2520and%2520regret%2520analysis%2520demonstrate%2520the%2520effectiveness%2520of%2520FedKACE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Aware%20Evolution%20for%20Streaming%20Federated%20Continual%20Learning%20with%20Category%20Overlap%20and%20without%20Task%20Identifiers&entry.906535625=Sixing%20Tan%20and%20Xianmin%20Liu&entry.1292438233=Federated%20Continual%20Learning%20%28FCL%29%20leverages%20inter-client%20collaboration%20to%20balance%20new%20knowledge%20acquisition%20and%20prior%20knowledge%20retention%20in%20non-stationary%20data.%20However%2C%20existing%20batch-based%20FCL%20methods%20lack%20adaptability%20to%20streaming%20scenarios%20featuring%20category%20overlap%20between%20old%20and%20new%20data%20and%20absent%20task%20identifiers%2C%20leading%20to%20indistinguishability%20of%20old%20and%20new%20knowledge%2C%20uncertain%20task%20assignments%20for%20samples%2C%20and%20knowledge%20confusion.To%20address%20this%2C%20we%20propose%20streaming%20federated%20continual%20learning%20setting%3A%20per%20federated%20learning%20%28FL%29%20round%2C%20clients%20process%20streaming%20data%20with%20disjoint%20samples%20and%20potentially%20overlapping%20categories%20without%20task%20identifiers%2C%20necessitating%20sustained%20inference%20capability%20for%20all%20prior%20categories%20after%20each%20FL%20round.Next%2C%20we%20introduce%20FedKACE%3A%201%29%20an%20adaptive%20inference%20model%20switching%20mechanism%20that%20enables%20unidirectional%20switching%20from%20local%20model%20to%20global%20model%20to%20achieve%20a%20trade-off%20between%20personalization%20and%20generalization%3B%202%29%20a%20adaptive%20gradient-balanced%20replay%20scheme%20that%20reconciles%20new%20knowledge%20learning%20and%20old%20knowledge%20retention%20under%20overlapping-class%20scenarios%3B%203%29%20a%20kernel%20spectral%20boundary%20buffer%20maintenance%20that%20preserves%20high-information%20and%20high-boundary-influence%20samples%20to%20optimize%20cross-round%20knowledge%20retention.%20Experiments%20across%20multiple%20scenarios%20and%20regret%20analysis%20demonstrate%20the%20effectiveness%20of%20FedKACE.&entry.1838667208=http%3A//arxiv.org/abs/2601.19788v1&entry.124074799=Read"},
{"title": "SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models", "author": "Gyubeum Lim and Yemo Koo and Vijay Krishna Madisetti", "abstract": "Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to bridge the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.", "link": "http://arxiv.org/abs/2510.21850v2", "date": "2026-01-27", "relevancy": 2.3614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCoPE%20VLM%3A%20Selective%20Context%20Processing%20for%20Efficient%20Document%20Navigation%20in%20Vision-Language%20Models&body=Title%3A%20SCoPE%20VLM%3A%20Selective%20Context%20Processing%20for%20Efficient%20Document%20Navigation%20in%20Vision-Language%20Models%0AAuthor%3A%20Gyubeum%20Lim%20and%20Yemo%20Koo%20and%20Vijay%20Krishna%20Madisetti%0AAbstract%3A%20Understanding%20long-context%20visual%20information%20remains%20a%20fundamental%20challenge%20for%20vision-language%20models%2C%20particularly%20in%20agentic%20tasks%20such%20as%20GUI%20control%20and%20web%20navigation.%20While%20web%20pages%20and%20GUI%20environments%20are%20inherently%20structured%20documents%2C%20current%20VLMs%20typically%20neglect%20decision-oriented%20document%20understanding%20in%20their%20training%20objectives.%20Existing%20approaches%20primarily%20extend%20visual%20embeddings%20to%20process%20long%2C%20high-resolution%20inputs%2C%20but%20these%20methods%20are%20memory-intensive%20and%20impractical%20for%20locally%20deployable%20solutions.%20To%20address%20these%20issues%2C%20we%20propose%20SCoPE%20VLM%2C%20a%20document%20navigation%20expert%20that%20leverages%20a%20novel%20Chain%20of%20Scroll%20mechanism%20to%20selectively%20and%20recursively%20navigate%20documents%2C%20focusing%20exclusively%20on%20relevant%20segments.%20We%20introduce%20a%20dedicated%20data%20generation%20pipeline%20to%20construct%20informative%20Chain%20of%20Scroll%20trajectories%20and%20Episodic%20Group%20Relative%20Policy%20Optimization%2C%20a%20tailored%20reinforcement%20learning%20method%20to%20bridge%20the%20gap%20between%20training%20and%20inference.%20Our%20method%20substantially%20reduces%20memory%20usage%20and%20effectively%20models%20human-like%20reading%20behaviors.%20To%20the%20best%20of%20our%20knowledge%2C%20SCoPE%20VLM%20is%20the%20first%20framework%20to%20explicitly%20model%20agentic%20reading%20patterns%20in%20multi-page%20document%20question%20answering%2C%20advancing%20the%20capabilities%20of%20multimodal%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCoPE%2520VLM%253A%2520Selective%2520Context%2520Processing%2520for%2520Efficient%2520Document%2520Navigation%2520in%2520Vision-Language%2520Models%26entry.906535625%3DGyubeum%2520Lim%2520and%2520Yemo%2520Koo%2520and%2520Vijay%2520Krishna%2520Madisetti%26entry.1292438233%3DUnderstanding%2520long-context%2520visual%2520information%2520remains%2520a%2520fundamental%2520challenge%2520for%2520vision-language%2520models%252C%2520particularly%2520in%2520agentic%2520tasks%2520such%2520as%2520GUI%2520control%2520and%2520web%2520navigation.%2520While%2520web%2520pages%2520and%2520GUI%2520environments%2520are%2520inherently%2520structured%2520documents%252C%2520current%2520VLMs%2520typically%2520neglect%2520decision-oriented%2520document%2520understanding%2520in%2520their%2520training%2520objectives.%2520Existing%2520approaches%2520primarily%2520extend%2520visual%2520embeddings%2520to%2520process%2520long%252C%2520high-resolution%2520inputs%252C%2520but%2520these%2520methods%2520are%2520memory-intensive%2520and%2520impractical%2520for%2520locally%2520deployable%2520solutions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520SCoPE%2520VLM%252C%2520a%2520document%2520navigation%2520expert%2520that%2520leverages%2520a%2520novel%2520Chain%2520of%2520Scroll%2520mechanism%2520to%2520selectively%2520and%2520recursively%2520navigate%2520documents%252C%2520focusing%2520exclusively%2520on%2520relevant%2520segments.%2520We%2520introduce%2520a%2520dedicated%2520data%2520generation%2520pipeline%2520to%2520construct%2520informative%2520Chain%2520of%2520Scroll%2520trajectories%2520and%2520Episodic%2520Group%2520Relative%2520Policy%2520Optimization%252C%2520a%2520tailored%2520reinforcement%2520learning%2520method%2520to%2520bridge%2520the%2520gap%2520between%2520training%2520and%2520inference.%2520Our%2520method%2520substantially%2520reduces%2520memory%2520usage%2520and%2520effectively%2520models%2520human-like%2520reading%2520behaviors.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520SCoPE%2520VLM%2520is%2520the%2520first%2520framework%2520to%2520explicitly%2520model%2520agentic%2520reading%2520patterns%2520in%2520multi-page%2520document%2520question%2520answering%252C%2520advancing%2520the%2520capabilities%2520of%2520multimodal%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCoPE%20VLM%3A%20Selective%20Context%20Processing%20for%20Efficient%20Document%20Navigation%20in%20Vision-Language%20Models&entry.906535625=Gyubeum%20Lim%20and%20Yemo%20Koo%20and%20Vijay%20Krishna%20Madisetti&entry.1292438233=Understanding%20long-context%20visual%20information%20remains%20a%20fundamental%20challenge%20for%20vision-language%20models%2C%20particularly%20in%20agentic%20tasks%20such%20as%20GUI%20control%20and%20web%20navigation.%20While%20web%20pages%20and%20GUI%20environments%20are%20inherently%20structured%20documents%2C%20current%20VLMs%20typically%20neglect%20decision-oriented%20document%20understanding%20in%20their%20training%20objectives.%20Existing%20approaches%20primarily%20extend%20visual%20embeddings%20to%20process%20long%2C%20high-resolution%20inputs%2C%20but%20these%20methods%20are%20memory-intensive%20and%20impractical%20for%20locally%20deployable%20solutions.%20To%20address%20these%20issues%2C%20we%20propose%20SCoPE%20VLM%2C%20a%20document%20navigation%20expert%20that%20leverages%20a%20novel%20Chain%20of%20Scroll%20mechanism%20to%20selectively%20and%20recursively%20navigate%20documents%2C%20focusing%20exclusively%20on%20relevant%20segments.%20We%20introduce%20a%20dedicated%20data%20generation%20pipeline%20to%20construct%20informative%20Chain%20of%20Scroll%20trajectories%20and%20Episodic%20Group%20Relative%20Policy%20Optimization%2C%20a%20tailored%20reinforcement%20learning%20method%20to%20bridge%20the%20gap%20between%20training%20and%20inference.%20Our%20method%20substantially%20reduces%20memory%20usage%20and%20effectively%20models%20human-like%20reading%20behaviors.%20To%20the%20best%20of%20our%20knowledge%2C%20SCoPE%20VLM%20is%20the%20first%20framework%20to%20explicitly%20model%20agentic%20reading%20patterns%20in%20multi-page%20document%20question%20answering%2C%20advancing%20the%20capabilities%20of%20multimodal%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2510.21850v2&entry.124074799=Read"},
{"title": "ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving", "author": "Yujin Wang and Yutong Zheng and Wenxian Fan and Tianyi Wang and Hongqing Chu and Daxin Tian and Bingzhao Gao and Jianqiang Wang and Hong Chen", "abstract": "In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.", "link": "http://arxiv.org/abs/2601.19582v1", "date": "2026-01-27", "relevancy": 2.3584, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5981}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScenePilot-Bench%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Evaluation%20of%20Vision-Language%20Models%20in%20Autonomous%20Driving&body=Title%3A%20ScenePilot-Bench%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Evaluation%20of%20Vision-Language%20Models%20in%20Autonomous%20Driving%0AAuthor%3A%20Yujin%20Wang%20and%20Yutong%20Zheng%20and%20Wenxian%20Fan%20and%20Tianyi%20Wang%20and%20Hongqing%20Chu%20and%20Daxin%20Tian%20and%20Bingzhao%20Gao%20and%20Jianqiang%20Wang%20and%20Hong%20Chen%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20ScenePilot-Bench%2C%20a%20large-scale%20first-person%20driving%20benchmark%20designed%20to%20evaluate%20vision-language%20models%20%28VLMs%29%20in%20autonomous%20driving%20scenarios.%20ScenePilot-Bench%20is%20built%20upon%20ScenePilot-4K%2C%20a%20diverse%20dataset%20comprising%203%2C847%20hours%20of%20driving%20videos%2C%20annotated%20with%20multi-granularity%20information%20including%20scene%20descriptions%2C%20risk%20assessments%2C%20key%20participant%20identification%2C%20ego%20trajectories%2C%20and%20camera%20parameters.%20The%20benchmark%20features%20a%20four-axis%20evaluation%20suite%20that%20assesses%20VLM%20capabilities%20in%20scene%20understanding%2C%20spatial%20perception%2C%20motion%20planning%2C%20and%20GPT-Score%2C%20with%20safety-aware%20metrics%20and%20cross-region%20generalization%20settings.%20We%20benchmark%20representative%20VLMs%20on%20ScenePilot-Bench%2C%20providing%20empirical%20analyses%20that%20clarify%20current%20performance%20boundaries%20and%20identify%20gaps%20for%20driving-oriented%20reasoning.%20ScenePilot-Bench%20offers%20a%20comprehensive%20framework%20for%20evaluating%20and%20advancing%20VLMs%20in%20safety-critical%20autonomous%20driving%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenePilot-Bench%253A%2520A%2520Large-Scale%2520Dataset%2520and%2520Benchmark%2520for%2520Evaluation%2520of%2520Vision-Language%2520Models%2520in%2520Autonomous%2520Driving%26entry.906535625%3DYujin%2520Wang%2520and%2520Yutong%2520Zheng%2520and%2520Wenxian%2520Fan%2520and%2520Tianyi%2520Wang%2520and%2520Hongqing%2520Chu%2520and%2520Daxin%2520Tian%2520and%2520Bingzhao%2520Gao%2520and%2520Jianqiang%2520Wang%2520and%2520Hong%2520Chen%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520ScenePilot-Bench%252C%2520a%2520large-scale%2520first-person%2520driving%2520benchmark%2520designed%2520to%2520evaluate%2520vision-language%2520models%2520%2528VLMs%2529%2520in%2520autonomous%2520driving%2520scenarios.%2520ScenePilot-Bench%2520is%2520built%2520upon%2520ScenePilot-4K%252C%2520a%2520diverse%2520dataset%2520comprising%25203%252C847%2520hours%2520of%2520driving%2520videos%252C%2520annotated%2520with%2520multi-granularity%2520information%2520including%2520scene%2520descriptions%252C%2520risk%2520assessments%252C%2520key%2520participant%2520identification%252C%2520ego%2520trajectories%252C%2520and%2520camera%2520parameters.%2520The%2520benchmark%2520features%2520a%2520four-axis%2520evaluation%2520suite%2520that%2520assesses%2520VLM%2520capabilities%2520in%2520scene%2520understanding%252C%2520spatial%2520perception%252C%2520motion%2520planning%252C%2520and%2520GPT-Score%252C%2520with%2520safety-aware%2520metrics%2520and%2520cross-region%2520generalization%2520settings.%2520We%2520benchmark%2520representative%2520VLMs%2520on%2520ScenePilot-Bench%252C%2520providing%2520empirical%2520analyses%2520that%2520clarify%2520current%2520performance%2520boundaries%2520and%2520identify%2520gaps%2520for%2520driving-oriented%2520reasoning.%2520ScenePilot-Bench%2520offers%2520a%2520comprehensive%2520framework%2520for%2520evaluating%2520and%2520advancing%2520VLMs%2520in%2520safety-critical%2520autonomous%2520driving%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScenePilot-Bench%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Evaluation%20of%20Vision-Language%20Models%20in%20Autonomous%20Driving&entry.906535625=Yujin%20Wang%20and%20Yutong%20Zheng%20and%20Wenxian%20Fan%20and%20Tianyi%20Wang%20and%20Hongqing%20Chu%20and%20Daxin%20Tian%20and%20Bingzhao%20Gao%20and%20Jianqiang%20Wang%20and%20Hong%20Chen&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20ScenePilot-Bench%2C%20a%20large-scale%20first-person%20driving%20benchmark%20designed%20to%20evaluate%20vision-language%20models%20%28VLMs%29%20in%20autonomous%20driving%20scenarios.%20ScenePilot-Bench%20is%20built%20upon%20ScenePilot-4K%2C%20a%20diverse%20dataset%20comprising%203%2C847%20hours%20of%20driving%20videos%2C%20annotated%20with%20multi-granularity%20information%20including%20scene%20descriptions%2C%20risk%20assessments%2C%20key%20participant%20identification%2C%20ego%20trajectories%2C%20and%20camera%20parameters.%20The%20benchmark%20features%20a%20four-axis%20evaluation%20suite%20that%20assesses%20VLM%20capabilities%20in%20scene%20understanding%2C%20spatial%20perception%2C%20motion%20planning%2C%20and%20GPT-Score%2C%20with%20safety-aware%20metrics%20and%20cross-region%20generalization%20settings.%20We%20benchmark%20representative%20VLMs%20on%20ScenePilot-Bench%2C%20providing%20empirical%20analyses%20that%20clarify%20current%20performance%20boundaries%20and%20identify%20gaps%20for%20driving-oriented%20reasoning.%20ScenePilot-Bench%20offers%20a%20comprehensive%20framework%20for%20evaluating%20and%20advancing%20VLMs%20in%20safety-critical%20autonomous%20driving%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2601.19582v1&entry.124074799=Read"},
{"title": "LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "author": "Shijie Lian and Bin Yu and Xiaopeng Lin and Laurence T. Yang and Zhaolong Shen and Changti Wu and Yuzhuo Miao and Cong Huang and Kai Chen", "abstract": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose LangForce, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $\u03c0(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, LangForce significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "link": "http://arxiv.org/abs/2601.15197v4", "date": "2026-01-27", "relevancy": 2.3468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangForce%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries&body=Title%3A%20LangForce%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries%0AAuthor%3A%20Shijie%20Lian%20and%20Bin%20Yu%20and%20Xiaopeng%20Lin%20and%20Laurence%20T.%20Yang%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Cong%20Huang%20and%20Kai%20Chen%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%20in%20robot%20manipulation%20but%20often%20struggle%20to%20generalize%20to%20new%20instructions%20or%20complex%20multi-task%20scenarios.%20We%20identify%20a%20critical%20pathology%20in%20current%20training%20paradigms%20where%20goal-driven%20data%20collection%20creates%20a%20dataset%20bias.%20In%20such%20datasets%2C%20language%20instructions%20are%20highly%20predictable%20from%20visual%20observations%20alone%2C%20causing%20the%20conditional%20mutual%20information%20between%20instructions%20and%20actions%20to%20vanish%2C%20a%20phenomenon%20we%20term%20Information%20Collapse.%20Consequently%2C%20models%20degenerate%20into%20vision-only%20policies%20that%20ignore%20language%20constraints%20and%20fail%20in%20out-of-distribution%20%28OOD%29%20settings.%20To%20address%20this%2C%20we%20propose%20LangForce%2C%20a%20novel%20framework%20that%20enforces%20instruction%20following%20via%20Bayesian%20decomposition.%20By%20introducing%20learnable%20Latent%20Action%20Queries%2C%20we%20construct%20a%20dual-branch%20architecture%20to%20estimate%20both%20a%20vision-only%20prior%20%24p%28a%20%5Cmid%20v%29%24%20and%20a%20language-conditioned%20posterior%20%24%CF%80%28a%20%5Cmid%20v%2C%20%5Cell%29%24.%20We%20then%20optimize%20the%20policy%20to%20maximize%20the%20conditional%20Pointwise%20Mutual%20Information%20%28PMI%29%20between%20actions%20and%20instructions.%20This%20objective%20effectively%20penalizes%20the%20vision%20shortcut%20and%20rewards%20actions%20that%20explicitly%20explain%20the%20language%20command.%20Without%20requiring%20new%20data%2C%20LangForce%20significantly%20improves%20generalization.%20Extensive%20experiments%20across%20on%20SimplerEnv%20and%20RoboCasa%20demonstrate%20substantial%20gains%2C%20including%20an%2011.3%25%20improvement%20on%20the%20challenging%20OOD%20SimplerEnv%20benchmark%2C%20validating%20the%20ability%20of%20our%20approach%20to%20robustly%20ground%20language%20in%20action.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15197v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangForce%253A%2520Bayesian%2520Decomposition%2520of%2520Vision%2520Language%2520Action%2520Models%2520via%2520Latent%2520Action%2520Queries%26entry.906535625%3DShijie%2520Lian%2520and%2520Bin%2520Yu%2520and%2520Xiaopeng%2520Lin%2520and%2520Laurence%2520T.%2520Yang%2520and%2520Zhaolong%2520Shen%2520and%2520Changti%2520Wu%2520and%2520Yuzhuo%2520Miao%2520and%2520Cong%2520Huang%2520and%2520Kai%2520Chen%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520promise%2520in%2520robot%2520manipulation%2520but%2520often%2520struggle%2520to%2520generalize%2520to%2520new%2520instructions%2520or%2520complex%2520multi-task%2520scenarios.%2520We%2520identify%2520a%2520critical%2520pathology%2520in%2520current%2520training%2520paradigms%2520where%2520goal-driven%2520data%2520collection%2520creates%2520a%2520dataset%2520bias.%2520In%2520such%2520datasets%252C%2520language%2520instructions%2520are%2520highly%2520predictable%2520from%2520visual%2520observations%2520alone%252C%2520causing%2520the%2520conditional%2520mutual%2520information%2520between%2520instructions%2520and%2520actions%2520to%2520vanish%252C%2520a%2520phenomenon%2520we%2520term%2520Information%2520Collapse.%2520Consequently%252C%2520models%2520degenerate%2520into%2520vision-only%2520policies%2520that%2520ignore%2520language%2520constraints%2520and%2520fail%2520in%2520out-of-distribution%2520%2528OOD%2529%2520settings.%2520To%2520address%2520this%252C%2520we%2520propose%2520LangForce%252C%2520a%2520novel%2520framework%2520that%2520enforces%2520instruction%2520following%2520via%2520Bayesian%2520decomposition.%2520By%2520introducing%2520learnable%2520Latent%2520Action%2520Queries%252C%2520we%2520construct%2520a%2520dual-branch%2520architecture%2520to%2520estimate%2520both%2520a%2520vision-only%2520prior%2520%2524p%2528a%2520%255Cmid%2520v%2529%2524%2520and%2520a%2520language-conditioned%2520posterior%2520%2524%25CF%2580%2528a%2520%255Cmid%2520v%252C%2520%255Cell%2529%2524.%2520We%2520then%2520optimize%2520the%2520policy%2520to%2520maximize%2520the%2520conditional%2520Pointwise%2520Mutual%2520Information%2520%2528PMI%2529%2520between%2520actions%2520and%2520instructions.%2520This%2520objective%2520effectively%2520penalizes%2520the%2520vision%2520shortcut%2520and%2520rewards%2520actions%2520that%2520explicitly%2520explain%2520the%2520language%2520command.%2520Without%2520requiring%2520new%2520data%252C%2520LangForce%2520significantly%2520improves%2520generalization.%2520Extensive%2520experiments%2520across%2520on%2520SimplerEnv%2520and%2520RoboCasa%2520demonstrate%2520substantial%2520gains%252C%2520including%2520an%252011.3%2525%2520improvement%2520on%2520the%2520challenging%2520OOD%2520SimplerEnv%2520benchmark%252C%2520validating%2520the%2520ability%2520of%2520our%2520approach%2520to%2520robustly%2520ground%2520language%2520in%2520action.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15197v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangForce%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries&entry.906535625=Shijie%20Lian%20and%20Bin%20Yu%20and%20Xiaopeng%20Lin%20and%20Laurence%20T.%20Yang%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Cong%20Huang%20and%20Kai%20Chen&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%20in%20robot%20manipulation%20but%20often%20struggle%20to%20generalize%20to%20new%20instructions%20or%20complex%20multi-task%20scenarios.%20We%20identify%20a%20critical%20pathology%20in%20current%20training%20paradigms%20where%20goal-driven%20data%20collection%20creates%20a%20dataset%20bias.%20In%20such%20datasets%2C%20language%20instructions%20are%20highly%20predictable%20from%20visual%20observations%20alone%2C%20causing%20the%20conditional%20mutual%20information%20between%20instructions%20and%20actions%20to%20vanish%2C%20a%20phenomenon%20we%20term%20Information%20Collapse.%20Consequently%2C%20models%20degenerate%20into%20vision-only%20policies%20that%20ignore%20language%20constraints%20and%20fail%20in%20out-of-distribution%20%28OOD%29%20settings.%20To%20address%20this%2C%20we%20propose%20LangForce%2C%20a%20novel%20framework%20that%20enforces%20instruction%20following%20via%20Bayesian%20decomposition.%20By%20introducing%20learnable%20Latent%20Action%20Queries%2C%20we%20construct%20a%20dual-branch%20architecture%20to%20estimate%20both%20a%20vision-only%20prior%20%24p%28a%20%5Cmid%20v%29%24%20and%20a%20language-conditioned%20posterior%20%24%CF%80%28a%20%5Cmid%20v%2C%20%5Cell%29%24.%20We%20then%20optimize%20the%20policy%20to%20maximize%20the%20conditional%20Pointwise%20Mutual%20Information%20%28PMI%29%20between%20actions%20and%20instructions.%20This%20objective%20effectively%20penalizes%20the%20vision%20shortcut%20and%20rewards%20actions%20that%20explicitly%20explain%20the%20language%20command.%20Without%20requiring%20new%20data%2C%20LangForce%20significantly%20improves%20generalization.%20Extensive%20experiments%20across%20on%20SimplerEnv%20and%20RoboCasa%20demonstrate%20substantial%20gains%2C%20including%20an%2011.3%25%20improvement%20on%20the%20challenging%20OOD%20SimplerEnv%20benchmark%2C%20validating%20the%20ability%20of%20our%20approach%20to%20robustly%20ground%20language%20in%20action.&entry.1838667208=http%3A//arxiv.org/abs/2601.15197v4&entry.124074799=Read"},
{"title": "Why is Your Language Model a Poor Implicit Reward Model?", "author": "Noam Razin and Yong Lin and Jiarui Yao and Sanjeev Arora", "abstract": "Reward models are key to language model post-training and inference pipelines. Conveniently, recent work showed that every language model defines an implicit reward model (IM-RM), without requiring any architectural changes. However, such IM-RMs tend to generalize worse, especially out-of-distribution, compared to explicit reward models (EX-RMs) that apply a dedicated linear head over the hidden representations of a language model. The existence of a generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They can be trained using the same data, loss function, and language model, and differ only in how the reward is computed. Toward a fundamental understanding of the implicit biases underlying different reward model types, we investigate the root cause of this gap. Our main finding, backed by theory and experiments, is that IM-RMs rely more heavily on superficial token-level cues. Consequently, they often generalize worse than EX-RMs under token-level distribution shifts, as well as in-distribution. Furthermore, we provide evidence against alternative hypotheses for the generalization gap. Most notably, we challenge the claim that IM-RMs struggle in tasks where generation is harder than verification because they can operate both as a verifier and a generator. Overall, our results highlight that seemingly minor design choices can substantially impact the generalization behavior of reward models.", "link": "http://arxiv.org/abs/2507.07981v3", "date": "2026-01-27", "relevancy": 2.3461, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20is%20Your%20Language%20Model%20a%20Poor%20Implicit%20Reward%20Model%3F&body=Title%3A%20Why%20is%20Your%20Language%20Model%20a%20Poor%20Implicit%20Reward%20Model%3F%0AAuthor%3A%20Noam%20Razin%20and%20Yong%20Lin%20and%20Jiarui%20Yao%20and%20Sanjeev%20Arora%0AAbstract%3A%20Reward%20models%20are%20key%20to%20language%20model%20post-training%20and%20inference%20pipelines.%20Conveniently%2C%20recent%20work%20showed%20that%20every%20language%20model%20defines%20an%20implicit%20reward%20model%20%28IM-RM%29%2C%20without%20requiring%20any%20architectural%20changes.%20However%2C%20such%20IM-RMs%20tend%20to%20generalize%20worse%2C%20especially%20out-of-distribution%2C%20compared%20to%20explicit%20reward%20models%20%28EX-RMs%29%20that%20apply%20a%20dedicated%20linear%20head%20over%20the%20hidden%20representations%20of%20a%20language%20model.%20The%20existence%20of%20a%20generalization%20gap%20is%20puzzling%2C%20as%20EX-RMs%20and%20IM-RMs%20are%20nearly%20identical.%20They%20can%20be%20trained%20using%20the%20same%20data%2C%20loss%20function%2C%20and%20language%20model%2C%20and%20differ%20only%20in%20how%20the%20reward%20is%20computed.%20Toward%20a%20fundamental%20understanding%20of%20the%20implicit%20biases%20underlying%20different%20reward%20model%20types%2C%20we%20investigate%20the%20root%20cause%20of%20this%20gap.%20Our%20main%20finding%2C%20backed%20by%20theory%20and%20experiments%2C%20is%20that%20IM-RMs%20rely%20more%20heavily%20on%20superficial%20token-level%20cues.%20Consequently%2C%20they%20often%20generalize%20worse%20than%20EX-RMs%20under%20token-level%20distribution%20shifts%2C%20as%20well%20as%20in-distribution.%20Furthermore%2C%20we%20provide%20evidence%20against%20alternative%20hypotheses%20for%20the%20generalization%20gap.%20Most%20notably%2C%20we%20challenge%20the%20claim%20that%20IM-RMs%20struggle%20in%20tasks%20where%20generation%20is%20harder%20than%20verification%20because%20they%20can%20operate%20both%20as%20a%20verifier%20and%20a%20generator.%20Overall%2C%20our%20results%20highlight%20that%20seemingly%20minor%20design%20choices%20can%20substantially%20impact%20the%20generalization%20behavior%20of%20reward%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2507.07981v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520is%2520Your%2520Language%2520Model%2520a%2520Poor%2520Implicit%2520Reward%2520Model%253F%26entry.906535625%3DNoam%2520Razin%2520and%2520Yong%2520Lin%2520and%2520Jiarui%2520Yao%2520and%2520Sanjeev%2520Arora%26entry.1292438233%3DReward%2520models%2520are%2520key%2520to%2520language%2520model%2520post-training%2520and%2520inference%2520pipelines.%2520Conveniently%252C%2520recent%2520work%2520showed%2520that%2520every%2520language%2520model%2520defines%2520an%2520implicit%2520reward%2520model%2520%2528IM-RM%2529%252C%2520without%2520requiring%2520any%2520architectural%2520changes.%2520However%252C%2520such%2520IM-RMs%2520tend%2520to%2520generalize%2520worse%252C%2520especially%2520out-of-distribution%252C%2520compared%2520to%2520explicit%2520reward%2520models%2520%2528EX-RMs%2529%2520that%2520apply%2520a%2520dedicated%2520linear%2520head%2520over%2520the%2520hidden%2520representations%2520of%2520a%2520language%2520model.%2520The%2520existence%2520of%2520a%2520generalization%2520gap%2520is%2520puzzling%252C%2520as%2520EX-RMs%2520and%2520IM-RMs%2520are%2520nearly%2520identical.%2520They%2520can%2520be%2520trained%2520using%2520the%2520same%2520data%252C%2520loss%2520function%252C%2520and%2520language%2520model%252C%2520and%2520differ%2520only%2520in%2520how%2520the%2520reward%2520is%2520computed.%2520Toward%2520a%2520fundamental%2520understanding%2520of%2520the%2520implicit%2520biases%2520underlying%2520different%2520reward%2520model%2520types%252C%2520we%2520investigate%2520the%2520root%2520cause%2520of%2520this%2520gap.%2520Our%2520main%2520finding%252C%2520backed%2520by%2520theory%2520and%2520experiments%252C%2520is%2520that%2520IM-RMs%2520rely%2520more%2520heavily%2520on%2520superficial%2520token-level%2520cues.%2520Consequently%252C%2520they%2520often%2520generalize%2520worse%2520than%2520EX-RMs%2520under%2520token-level%2520distribution%2520shifts%252C%2520as%2520well%2520as%2520in-distribution.%2520Furthermore%252C%2520we%2520provide%2520evidence%2520against%2520alternative%2520hypotheses%2520for%2520the%2520generalization%2520gap.%2520Most%2520notably%252C%2520we%2520challenge%2520the%2520claim%2520that%2520IM-RMs%2520struggle%2520in%2520tasks%2520where%2520generation%2520is%2520harder%2520than%2520verification%2520because%2520they%2520can%2520operate%2520both%2520as%2520a%2520verifier%2520and%2520a%2520generator.%2520Overall%252C%2520our%2520results%2520highlight%2520that%2520seemingly%2520minor%2520design%2520choices%2520can%2520substantially%2520impact%2520the%2520generalization%2520behavior%2520of%2520reward%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07981v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20is%20Your%20Language%20Model%20a%20Poor%20Implicit%20Reward%20Model%3F&entry.906535625=Noam%20Razin%20and%20Yong%20Lin%20and%20Jiarui%20Yao%20and%20Sanjeev%20Arora&entry.1292438233=Reward%20models%20are%20key%20to%20language%20model%20post-training%20and%20inference%20pipelines.%20Conveniently%2C%20recent%20work%20showed%20that%20every%20language%20model%20defines%20an%20implicit%20reward%20model%20%28IM-RM%29%2C%20without%20requiring%20any%20architectural%20changes.%20However%2C%20such%20IM-RMs%20tend%20to%20generalize%20worse%2C%20especially%20out-of-distribution%2C%20compared%20to%20explicit%20reward%20models%20%28EX-RMs%29%20that%20apply%20a%20dedicated%20linear%20head%20over%20the%20hidden%20representations%20of%20a%20language%20model.%20The%20existence%20of%20a%20generalization%20gap%20is%20puzzling%2C%20as%20EX-RMs%20and%20IM-RMs%20are%20nearly%20identical.%20They%20can%20be%20trained%20using%20the%20same%20data%2C%20loss%20function%2C%20and%20language%20model%2C%20and%20differ%20only%20in%20how%20the%20reward%20is%20computed.%20Toward%20a%20fundamental%20understanding%20of%20the%20implicit%20biases%20underlying%20different%20reward%20model%20types%2C%20we%20investigate%20the%20root%20cause%20of%20this%20gap.%20Our%20main%20finding%2C%20backed%20by%20theory%20and%20experiments%2C%20is%20that%20IM-RMs%20rely%20more%20heavily%20on%20superficial%20token-level%20cues.%20Consequently%2C%20they%20often%20generalize%20worse%20than%20EX-RMs%20under%20token-level%20distribution%20shifts%2C%20as%20well%20as%20in-distribution.%20Furthermore%2C%20we%20provide%20evidence%20against%20alternative%20hypotheses%20for%20the%20generalization%20gap.%20Most%20notably%2C%20we%20challenge%20the%20claim%20that%20IM-RMs%20struggle%20in%20tasks%20where%20generation%20is%20harder%20than%20verification%20because%20they%20can%20operate%20both%20as%20a%20verifier%20and%20a%20generator.%20Overall%2C%20our%20results%20highlight%20that%20seemingly%20minor%20design%20choices%20can%20substantially%20impact%20the%20generalization%20behavior%20of%20reward%20models.&entry.1838667208=http%3A//arxiv.org/abs/2507.07981v3&entry.124074799=Read"},
{"title": "DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization", "author": "Yitong Yang and Xuexin Liu and Yinglin Wang and Jing Wang and Hao Dou and Changshuo Wang and Shuting He", "abstract": "3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.", "link": "http://arxiv.org/abs/2601.19717v1", "date": "2026-01-27", "relevancy": 2.3406, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5992}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5883}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffStyle3D%3A%20Consistent%203D%20Gaussian%20Stylization%20via%20Attention%20Optimization&body=Title%3A%20DiffStyle3D%3A%20Consistent%203D%20Gaussian%20Stylization%20via%20Attention%20Optimization%0AAuthor%3A%20Yitong%20Yang%20and%20Xuexin%20Liu%20and%20Yinglin%20Wang%20and%20Jing%20Wang%20and%20Hao%20Dou%20and%20Changshuo%20Wang%20and%20Shuting%20He%0AAbstract%3A%203D%20style%20transfer%20enables%20the%20creation%20of%20visually%20expressive%203D%20content%2C%20enriching%20the%20visual%20appearance%20of%203D%20scenes%20and%20objects.%20However%2C%20existing%20VGG-%20and%20CLIP-based%20methods%20struggle%20to%20model%20multi-view%20consistency%20within%20the%20model%20itself%2C%20while%20diffusion-based%20approaches%20can%20capture%20such%20consistency%20but%20rely%20on%20denoising%20directions%2C%20leading%20to%20unstable%20training.%20To%20address%20these%20limitations%2C%20we%20propose%20DiffStyle3D%2C%20a%20novel%20diffusion-based%20paradigm%20for%203DGS%20style%20transfer%20that%20directly%20optimizes%20in%20the%20latent%20space.%20Specifically%2C%20we%20introduce%20an%20Attention-Aware%20Loss%20that%20performs%20style%20transfer%20by%20aligning%20style%20features%20in%20the%20self-attention%20space%2C%20while%20preserving%20original%20content%20through%20content%20feature%20alignment.%20Inspired%20by%20the%20geometric%20invariance%20of%203D%20stylization%2C%20we%20propose%20a%20Geometry-Guided%20Multi-View%20Consistency%20method%20that%20integrates%20geometric%20information%20into%20self-attention%20to%20enable%20cross-view%20correspondence%20modeling.%20Based%20on%20geometric%20information%2C%20we%20additionally%20construct%20a%20geometry-aware%20mask%20to%20prevent%20redundant%20optimization%20in%20overlapping%20regions%20across%20views%2C%20which%20further%20improves%20multi-view%20consistency.%20Extensive%20experiments%20show%20that%20DiffStyle3D%20outperforms%20state-of-the-art%20methods%2C%20achieving%20higher%20stylization%20quality%20and%20visual%20realism.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffStyle3D%253A%2520Consistent%25203D%2520Gaussian%2520Stylization%2520via%2520Attention%2520Optimization%26entry.906535625%3DYitong%2520Yang%2520and%2520Xuexin%2520Liu%2520and%2520Yinglin%2520Wang%2520and%2520Jing%2520Wang%2520and%2520Hao%2520Dou%2520and%2520Changshuo%2520Wang%2520and%2520Shuting%2520He%26entry.1292438233%3D3D%2520style%2520transfer%2520enables%2520the%2520creation%2520of%2520visually%2520expressive%25203D%2520content%252C%2520enriching%2520the%2520visual%2520appearance%2520of%25203D%2520scenes%2520and%2520objects.%2520However%252C%2520existing%2520VGG-%2520and%2520CLIP-based%2520methods%2520struggle%2520to%2520model%2520multi-view%2520consistency%2520within%2520the%2520model%2520itself%252C%2520while%2520diffusion-based%2520approaches%2520can%2520capture%2520such%2520consistency%2520but%2520rely%2520on%2520denoising%2520directions%252C%2520leading%2520to%2520unstable%2520training.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520DiffStyle3D%252C%2520a%2520novel%2520diffusion-based%2520paradigm%2520for%25203DGS%2520style%2520transfer%2520that%2520directly%2520optimizes%2520in%2520the%2520latent%2520space.%2520Specifically%252C%2520we%2520introduce%2520an%2520Attention-Aware%2520Loss%2520that%2520performs%2520style%2520transfer%2520by%2520aligning%2520style%2520features%2520in%2520the%2520self-attention%2520space%252C%2520while%2520preserving%2520original%2520content%2520through%2520content%2520feature%2520alignment.%2520Inspired%2520by%2520the%2520geometric%2520invariance%2520of%25203D%2520stylization%252C%2520we%2520propose%2520a%2520Geometry-Guided%2520Multi-View%2520Consistency%2520method%2520that%2520integrates%2520geometric%2520information%2520into%2520self-attention%2520to%2520enable%2520cross-view%2520correspondence%2520modeling.%2520Based%2520on%2520geometric%2520information%252C%2520we%2520additionally%2520construct%2520a%2520geometry-aware%2520mask%2520to%2520prevent%2520redundant%2520optimization%2520in%2520overlapping%2520regions%2520across%2520views%252C%2520which%2520further%2520improves%2520multi-view%2520consistency.%2520Extensive%2520experiments%2520show%2520that%2520DiffStyle3D%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520higher%2520stylization%2520quality%2520and%2520visual%2520realism.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffStyle3D%3A%20Consistent%203D%20Gaussian%20Stylization%20via%20Attention%20Optimization&entry.906535625=Yitong%20Yang%20and%20Xuexin%20Liu%20and%20Yinglin%20Wang%20and%20Jing%20Wang%20and%20Hao%20Dou%20and%20Changshuo%20Wang%20and%20Shuting%20He&entry.1292438233=3D%20style%20transfer%20enables%20the%20creation%20of%20visually%20expressive%203D%20content%2C%20enriching%20the%20visual%20appearance%20of%203D%20scenes%20and%20objects.%20However%2C%20existing%20VGG-%20and%20CLIP-based%20methods%20struggle%20to%20model%20multi-view%20consistency%20within%20the%20model%20itself%2C%20while%20diffusion-based%20approaches%20can%20capture%20such%20consistency%20but%20rely%20on%20denoising%20directions%2C%20leading%20to%20unstable%20training.%20To%20address%20these%20limitations%2C%20we%20propose%20DiffStyle3D%2C%20a%20novel%20diffusion-based%20paradigm%20for%203DGS%20style%20transfer%20that%20directly%20optimizes%20in%20the%20latent%20space.%20Specifically%2C%20we%20introduce%20an%20Attention-Aware%20Loss%20that%20performs%20style%20transfer%20by%20aligning%20style%20features%20in%20the%20self-attention%20space%2C%20while%20preserving%20original%20content%20through%20content%20feature%20alignment.%20Inspired%20by%20the%20geometric%20invariance%20of%203D%20stylization%2C%20we%20propose%20a%20Geometry-Guided%20Multi-View%20Consistency%20method%20that%20integrates%20geometric%20information%20into%20self-attention%20to%20enable%20cross-view%20correspondence%20modeling.%20Based%20on%20geometric%20information%2C%20we%20additionally%20construct%20a%20geometry-aware%20mask%20to%20prevent%20redundant%20optimization%20in%20overlapping%20regions%20across%20views%2C%20which%20further%20improves%20multi-view%20consistency.%20Extensive%20experiments%20show%20that%20DiffStyle3D%20outperforms%20state-of-the-art%20methods%2C%20achieving%20higher%20stylization%20quality%20and%20visual%20realism.&entry.1838667208=http%3A//arxiv.org/abs/2601.19717v1&entry.124074799=Read"},
{"title": "JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation", "author": "Guillem Capellera and Luis Ferraz and Antonio Rubio and Alexandre Alahi and Antonio Agudo", "abstract": "Generative models often treat continuous data and discrete events as separate processes, creating a gap in modeling complex systems where they interact synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion framework designed to unify these two processes by simultaneously generating continuous spatio-temporal data and synchronous discrete events. We demonstrate its efficacy in the sports domain by simultaneously modeling multi-agent trajectories and key possession events. This joint modeling is validated with non-controllable generation and two novel controllable generation scenarios: weak-possessor-guidance, which offers flexible semantic control over game dynamics through a simple list of intended ball possessors, and text-guidance, which enables fine-grained, language-driven generation. To enable the conditioning with these guidance signals, we introduce CrossGuid, an effective conditioning operation for multi-agent domains. We also share a new unified sports benchmark enhanced with textual descriptions for soccer and football datasets. JointDiff achieves state-of-the-art performance, demonstrating that joint modeling is crucial for building realistic and controllable generative models for interactive systems.", "link": "http://arxiv.org/abs/2509.22522v2", "date": "2026-01-27", "relevancy": 2.3222, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5881}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5788}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%20Generation&body=Title%3A%20JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%20Generation%0AAuthor%3A%20Guillem%20Capellera%20and%20Luis%20Ferraz%20and%20Antonio%20Rubio%20and%20Alexandre%20Alahi%20and%20Antonio%20Agudo%0AAbstract%3A%20Generative%20models%20often%20treat%20continuous%20data%20and%20discrete%20events%20as%20separate%20processes%2C%20creating%20a%20gap%20in%20modeling%20complex%20systems%20where%20they%20interact%20synchronously.%20To%20bridge%20this%20gap%2C%20we%20introduce%20JointDiff%2C%20a%20novel%20diffusion%20framework%20designed%20to%20unify%20these%20two%20processes%20by%20simultaneously%20generating%20continuous%20spatio-temporal%20data%20and%20synchronous%20discrete%20events.%20We%20demonstrate%20its%20efficacy%20in%20the%20sports%20domain%20by%20simultaneously%20modeling%20multi-agent%20trajectories%20and%20key%20possession%20events.%20This%20joint%20modeling%20is%20validated%20with%20non-controllable%20generation%20and%20two%20novel%20controllable%20generation%20scenarios%3A%20weak-possessor-guidance%2C%20which%20offers%20flexible%20semantic%20control%20over%20game%20dynamics%20through%20a%20simple%20list%20of%20intended%20ball%20possessors%2C%20and%20text-guidance%2C%20which%20enables%20fine-grained%2C%20language-driven%20generation.%20To%20enable%20the%20conditioning%20with%20these%20guidance%20signals%2C%20we%20introduce%20CrossGuid%2C%20an%20effective%20conditioning%20operation%20for%20multi-agent%20domains.%20We%20also%20share%20a%20new%20unified%20sports%20benchmark%20enhanced%20with%20textual%20descriptions%20for%20soccer%20and%20football%20datasets.%20JointDiff%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20that%20joint%20modeling%20is%20crucial%20for%20building%20realistic%20and%20controllable%20generative%20models%20for%20interactive%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointDiff%253A%2520Bridging%2520Continuous%2520and%2520Discrete%2520in%2520Multi-Agent%2520Trajectory%2520Generation%26entry.906535625%3DGuillem%2520Capellera%2520and%2520Luis%2520Ferraz%2520and%2520Antonio%2520Rubio%2520and%2520Alexandre%2520Alahi%2520and%2520Antonio%2520Agudo%26entry.1292438233%3DGenerative%2520models%2520often%2520treat%2520continuous%2520data%2520and%2520discrete%2520events%2520as%2520separate%2520processes%252C%2520creating%2520a%2520gap%2520in%2520modeling%2520complex%2520systems%2520where%2520they%2520interact%2520synchronously.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520JointDiff%252C%2520a%2520novel%2520diffusion%2520framework%2520designed%2520to%2520unify%2520these%2520two%2520processes%2520by%2520simultaneously%2520generating%2520continuous%2520spatio-temporal%2520data%2520and%2520synchronous%2520discrete%2520events.%2520We%2520demonstrate%2520its%2520efficacy%2520in%2520the%2520sports%2520domain%2520by%2520simultaneously%2520modeling%2520multi-agent%2520trajectories%2520and%2520key%2520possession%2520events.%2520This%2520joint%2520modeling%2520is%2520validated%2520with%2520non-controllable%2520generation%2520and%2520two%2520novel%2520controllable%2520generation%2520scenarios%253A%2520weak-possessor-guidance%252C%2520which%2520offers%2520flexible%2520semantic%2520control%2520over%2520game%2520dynamics%2520through%2520a%2520simple%2520list%2520of%2520intended%2520ball%2520possessors%252C%2520and%2520text-guidance%252C%2520which%2520enables%2520fine-grained%252C%2520language-driven%2520generation.%2520To%2520enable%2520the%2520conditioning%2520with%2520these%2520guidance%2520signals%252C%2520we%2520introduce%2520CrossGuid%252C%2520an%2520effective%2520conditioning%2520operation%2520for%2520multi-agent%2520domains.%2520We%2520also%2520share%2520a%2520new%2520unified%2520sports%2520benchmark%2520enhanced%2520with%2520textual%2520descriptions%2520for%2520soccer%2520and%2520football%2520datasets.%2520JointDiff%2520achieves%2520state-of-the-art%2520performance%252C%2520demonstrating%2520that%2520joint%2520modeling%2520is%2520crucial%2520for%2520building%2520realistic%2520and%2520controllable%2520generative%2520models%2520for%2520interactive%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%20Generation&entry.906535625=Guillem%20Capellera%20and%20Luis%20Ferraz%20and%20Antonio%20Rubio%20and%20Alexandre%20Alahi%20and%20Antonio%20Agudo&entry.1292438233=Generative%20models%20often%20treat%20continuous%20data%20and%20discrete%20events%20as%20separate%20processes%2C%20creating%20a%20gap%20in%20modeling%20complex%20systems%20where%20they%20interact%20synchronously.%20To%20bridge%20this%20gap%2C%20we%20introduce%20JointDiff%2C%20a%20novel%20diffusion%20framework%20designed%20to%20unify%20these%20two%20processes%20by%20simultaneously%20generating%20continuous%20spatio-temporal%20data%20and%20synchronous%20discrete%20events.%20We%20demonstrate%20its%20efficacy%20in%20the%20sports%20domain%20by%20simultaneously%20modeling%20multi-agent%20trajectories%20and%20key%20possession%20events.%20This%20joint%20modeling%20is%20validated%20with%20non-controllable%20generation%20and%20two%20novel%20controllable%20generation%20scenarios%3A%20weak-possessor-guidance%2C%20which%20offers%20flexible%20semantic%20control%20over%20game%20dynamics%20through%20a%20simple%20list%20of%20intended%20ball%20possessors%2C%20and%20text-guidance%2C%20which%20enables%20fine-grained%2C%20language-driven%20generation.%20To%20enable%20the%20conditioning%20with%20these%20guidance%20signals%2C%20we%20introduce%20CrossGuid%2C%20an%20effective%20conditioning%20operation%20for%20multi-agent%20domains.%20We%20also%20share%20a%20new%20unified%20sports%20benchmark%20enhanced%20with%20textual%20descriptions%20for%20soccer%20and%20football%20datasets.%20JointDiff%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20that%20joint%20modeling%20is%20crucial%20for%20building%20realistic%20and%20controllable%20generative%20models%20for%20interactive%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2509.22522v2&entry.124074799=Read"},
{"title": "LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation", "author": "Zhiwei Zheng and Kevin Bryson", "abstract": "Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.\n  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.\n  Availability and implementation: https://github.com/willyzzz/LaCoGSEA", "link": "http://arxiv.org/abs/2601.18604v2", "date": "2026-01-27", "relevancy": 2.3064, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4654}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4627}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaCoGSEA%3A%20Unsupervised%20deep%20learning%20for%20pathway%20analysis%20via%20latent%20correlation&body=Title%3A%20LaCoGSEA%3A%20Unsupervised%20deep%20learning%20for%20pathway%20analysis%20via%20latent%20correlation%0AAuthor%3A%20Zhiwei%20Zheng%20and%20Kevin%20Bryson%0AAbstract%3A%20Motivation%3A%20Pathway%20enrichment%20analysis%20is%20widely%20used%20to%20interpret%20gene%20expression%20data.%20Standard%20approaches%2C%20such%20as%20GSEA%2C%20rely%20on%20predefined%20phenotypic%20labels%20and%20pairwise%20comparisons%2C%20which%20limits%20their%20applicability%20in%20unsupervised%20settings.%20Existing%20unsupervised%20extensions%2C%20including%20single-sample%20methods%2C%20provide%20pathway-level%20summaries%20but%20primarily%20capture%20linear%20relationships%20and%20do%20not%20explicitly%20model%20gene-pathway%20associations.%20More%20recently%2C%20deep%20learning%20models%20have%20been%20explored%20to%20capture%20non-linear%20transcriptomic%20structure.%20However%2C%20their%20interpretation%20has%20typically%20relied%20on%20generic%20explainable%20AI%20%28XAI%29%20techniques%20designed%20for%20feature-level%20attribution.%20As%20these%20methods%20are%20not%20designed%20for%20pathway-level%20interpretation%20in%20unsupervised%20transcriptomic%20analyses%2C%20their%20effectiveness%20in%20this%20setting%20remains%20limited.%0A%20%20Results%3A%20To%20bridge%20this%20gap%2C%20we%20introduce%20LaCoGSEA%20%28Latent%20Correlation%20GSEA%29%2C%20an%20unsupervised%20framework%20that%20integrates%20deep%20representation%20learning%20with%20robust%20pathway%20statistics.%20LaCoGSEA%20employs%20an%20autoencoder%20to%20capture%20non-linear%20manifolds%20and%20proposes%20a%20global%20gene-latent%20correlation%20metric%20as%20a%20proxy%20for%20differential%20expression%2C%20generating%20dense%20gene%20rankings%20without%20prior%20labels.%20We%20demonstrate%20that%20LaCoGSEA%20offers%20three%20key%20advantages%3A%20%28i%29%20it%20achieves%20improved%20clustering%20performance%20in%20distinguishing%20cancer%20subtypes%20compared%20to%20existing%20unsupervised%20baselines%3B%20%28ii%29%20it%20recovers%20a%20broader%20range%20of%20biologically%20meaningful%20pathways%20at%20higher%20ranks%20compared%20with%20linear%20dimensionality%20reduction%20and%20gradient-based%20XAI%20methods%3B%20and%20%28iii%29%20it%20maintains%20high%20robustness%20and%20consistency%20across%20varying%20experimental%20protocols%20and%20dataset%20sizes.%20Overall%2C%20LaCoGSEA%20provides%20state-of-the-art%20performance%20in%20unsupervised%20pathway%20enrichment%20analysis.%0A%20%20Availability%20and%20implementation%3A%20https%3A//github.com/willyzzz/LaCoGSEA%0ALink%3A%20http%3A//arxiv.org/abs/2601.18604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaCoGSEA%253A%2520Unsupervised%2520deep%2520learning%2520for%2520pathway%2520analysis%2520via%2520latent%2520correlation%26entry.906535625%3DZhiwei%2520Zheng%2520and%2520Kevin%2520Bryson%26entry.1292438233%3DMotivation%253A%2520Pathway%2520enrichment%2520analysis%2520is%2520widely%2520used%2520to%2520interpret%2520gene%2520expression%2520data.%2520Standard%2520approaches%252C%2520such%2520as%2520GSEA%252C%2520rely%2520on%2520predefined%2520phenotypic%2520labels%2520and%2520pairwise%2520comparisons%252C%2520which%2520limits%2520their%2520applicability%2520in%2520unsupervised%2520settings.%2520Existing%2520unsupervised%2520extensions%252C%2520including%2520single-sample%2520methods%252C%2520provide%2520pathway-level%2520summaries%2520but%2520primarily%2520capture%2520linear%2520relationships%2520and%2520do%2520not%2520explicitly%2520model%2520gene-pathway%2520associations.%2520More%2520recently%252C%2520deep%2520learning%2520models%2520have%2520been%2520explored%2520to%2520capture%2520non-linear%2520transcriptomic%2520structure.%2520However%252C%2520their%2520interpretation%2520has%2520typically%2520relied%2520on%2520generic%2520explainable%2520AI%2520%2528XAI%2529%2520techniques%2520designed%2520for%2520feature-level%2520attribution.%2520As%2520these%2520methods%2520are%2520not%2520designed%2520for%2520pathway-level%2520interpretation%2520in%2520unsupervised%2520transcriptomic%2520analyses%252C%2520their%2520effectiveness%2520in%2520this%2520setting%2520remains%2520limited.%250A%2520%2520Results%253A%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520LaCoGSEA%2520%2528Latent%2520Correlation%2520GSEA%2529%252C%2520an%2520unsupervised%2520framework%2520that%2520integrates%2520deep%2520representation%2520learning%2520with%2520robust%2520pathway%2520statistics.%2520LaCoGSEA%2520employs%2520an%2520autoencoder%2520to%2520capture%2520non-linear%2520manifolds%2520and%2520proposes%2520a%2520global%2520gene-latent%2520correlation%2520metric%2520as%2520a%2520proxy%2520for%2520differential%2520expression%252C%2520generating%2520dense%2520gene%2520rankings%2520without%2520prior%2520labels.%2520We%2520demonstrate%2520that%2520LaCoGSEA%2520offers%2520three%2520key%2520advantages%253A%2520%2528i%2529%2520it%2520achieves%2520improved%2520clustering%2520performance%2520in%2520distinguishing%2520cancer%2520subtypes%2520compared%2520to%2520existing%2520unsupervised%2520baselines%253B%2520%2528ii%2529%2520it%2520recovers%2520a%2520broader%2520range%2520of%2520biologically%2520meaningful%2520pathways%2520at%2520higher%2520ranks%2520compared%2520with%2520linear%2520dimensionality%2520reduction%2520and%2520gradient-based%2520XAI%2520methods%253B%2520and%2520%2528iii%2529%2520it%2520maintains%2520high%2520robustness%2520and%2520consistency%2520across%2520varying%2520experimental%2520protocols%2520and%2520dataset%2520sizes.%2520Overall%252C%2520LaCoGSEA%2520provides%2520state-of-the-art%2520performance%2520in%2520unsupervised%2520pathway%2520enrichment%2520analysis.%250A%2520%2520Availability%2520and%2520implementation%253A%2520https%253A//github.com/willyzzz/LaCoGSEA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaCoGSEA%3A%20Unsupervised%20deep%20learning%20for%20pathway%20analysis%20via%20latent%20correlation&entry.906535625=Zhiwei%20Zheng%20and%20Kevin%20Bryson&entry.1292438233=Motivation%3A%20Pathway%20enrichment%20analysis%20is%20widely%20used%20to%20interpret%20gene%20expression%20data.%20Standard%20approaches%2C%20such%20as%20GSEA%2C%20rely%20on%20predefined%20phenotypic%20labels%20and%20pairwise%20comparisons%2C%20which%20limits%20their%20applicability%20in%20unsupervised%20settings.%20Existing%20unsupervised%20extensions%2C%20including%20single-sample%20methods%2C%20provide%20pathway-level%20summaries%20but%20primarily%20capture%20linear%20relationships%20and%20do%20not%20explicitly%20model%20gene-pathway%20associations.%20More%20recently%2C%20deep%20learning%20models%20have%20been%20explored%20to%20capture%20non-linear%20transcriptomic%20structure.%20However%2C%20their%20interpretation%20has%20typically%20relied%20on%20generic%20explainable%20AI%20%28XAI%29%20techniques%20designed%20for%20feature-level%20attribution.%20As%20these%20methods%20are%20not%20designed%20for%20pathway-level%20interpretation%20in%20unsupervised%20transcriptomic%20analyses%2C%20their%20effectiveness%20in%20this%20setting%20remains%20limited.%0A%20%20Results%3A%20To%20bridge%20this%20gap%2C%20we%20introduce%20LaCoGSEA%20%28Latent%20Correlation%20GSEA%29%2C%20an%20unsupervised%20framework%20that%20integrates%20deep%20representation%20learning%20with%20robust%20pathway%20statistics.%20LaCoGSEA%20employs%20an%20autoencoder%20to%20capture%20non-linear%20manifolds%20and%20proposes%20a%20global%20gene-latent%20correlation%20metric%20as%20a%20proxy%20for%20differential%20expression%2C%20generating%20dense%20gene%20rankings%20without%20prior%20labels.%20We%20demonstrate%20that%20LaCoGSEA%20offers%20three%20key%20advantages%3A%20%28i%29%20it%20achieves%20improved%20clustering%20performance%20in%20distinguishing%20cancer%20subtypes%20compared%20to%20existing%20unsupervised%20baselines%3B%20%28ii%29%20it%20recovers%20a%20broader%20range%20of%20biologically%20meaningful%20pathways%20at%20higher%20ranks%20compared%20with%20linear%20dimensionality%20reduction%20and%20gradient-based%20XAI%20methods%3B%20and%20%28iii%29%20it%20maintains%20high%20robustness%20and%20consistency%20across%20varying%20experimental%20protocols%20and%20dataset%20sizes.%20Overall%2C%20LaCoGSEA%20provides%20state-of-the-art%20performance%20in%20unsupervised%20pathway%20enrichment%20analysis.%0A%20%20Availability%20and%20implementation%3A%20https%3A//github.com/willyzzz/LaCoGSEA&entry.1838667208=http%3A//arxiv.org/abs/2601.18604v2&entry.124074799=Read"},
{"title": "Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning", "author": "Tongxi Wang and Zhuoyang Xia and Xinran Chen and Shan Liu", "abstract": "Real-world reinforcement learning often faces environment drift, but most existing methods rely on static entropy coefficients/target entropy, causing over-exploration during stable periods and under-exploration after drift (thus slow recovery), and leaving unanswered the principled question of how exploration intensity should scale with drift magnitude. We prove that entropy scheduling under non-stationarity can be reduced to a one-dimensional, round-by-round trade-off, faster tracking of the optimal solution after drift vs. avoiding gratuitous randomness when the environment is stable, so exploration strength can be driven by measurable online drift signals. Building on this, we propose AES (Adaptive Entropy Scheduling), which adaptively adjusts the entropy coefficient/temperature online using observable drift proxies during training, requiring almost no structural changes and incurring minimal overhead. Across 4 algorithm variants, 12 tasks, and 4 drift modes, AES significantly reduces the fraction of performance degradation caused by drift and accelerates recovery after abrupt changes.", "link": "http://arxiv.org/abs/2601.19624v1", "date": "2026-01-27", "relevancy": 2.3044, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4678}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4659}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracking%20Drift%3A%20Variation-Aware%20Entropy%20Scheduling%20for%20Non-Stationary%20Reinforcement%20Learning&body=Title%3A%20Tracking%20Drift%3A%20Variation-Aware%20Entropy%20Scheduling%20for%20Non-Stationary%20Reinforcement%20Learning%0AAuthor%3A%20Tongxi%20Wang%20and%20Zhuoyang%20Xia%20and%20Xinran%20Chen%20and%20Shan%20Liu%0AAbstract%3A%20Real-world%20reinforcement%20learning%20often%20faces%20environment%20drift%2C%20but%20most%20existing%20methods%20rely%20on%20static%20entropy%20coefficients/target%20entropy%2C%20causing%20over-exploration%20during%20stable%20periods%20and%20under-exploration%20after%20drift%20%28thus%20slow%20recovery%29%2C%20and%20leaving%20unanswered%20the%20principled%20question%20of%20how%20exploration%20intensity%20should%20scale%20with%20drift%20magnitude.%20We%20prove%20that%20entropy%20scheduling%20under%20non-stationarity%20can%20be%20reduced%20to%20a%20one-dimensional%2C%20round-by-round%20trade-off%2C%20faster%20tracking%20of%20the%20optimal%20solution%20after%20drift%20vs.%20avoiding%20gratuitous%20randomness%20when%20the%20environment%20is%20stable%2C%20so%20exploration%20strength%20can%20be%20driven%20by%20measurable%20online%20drift%20signals.%20Building%20on%20this%2C%20we%20propose%20AES%20%28Adaptive%20Entropy%20Scheduling%29%2C%20which%20adaptively%20adjusts%20the%20entropy%20coefficient/temperature%20online%20using%20observable%20drift%20proxies%20during%20training%2C%20requiring%20almost%20no%20structural%20changes%20and%20incurring%20minimal%20overhead.%20Across%204%20algorithm%20variants%2C%2012%20tasks%2C%20and%204%20drift%20modes%2C%20AES%20significantly%20reduces%20the%20fraction%20of%20performance%20degradation%20caused%20by%20drift%20and%20accelerates%20recovery%20after%20abrupt%20changes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracking%2520Drift%253A%2520Variation-Aware%2520Entropy%2520Scheduling%2520for%2520Non-Stationary%2520Reinforcement%2520Learning%26entry.906535625%3DTongxi%2520Wang%2520and%2520Zhuoyang%2520Xia%2520and%2520Xinran%2520Chen%2520and%2520Shan%2520Liu%26entry.1292438233%3DReal-world%2520reinforcement%2520learning%2520often%2520faces%2520environment%2520drift%252C%2520but%2520most%2520existing%2520methods%2520rely%2520on%2520static%2520entropy%2520coefficients/target%2520entropy%252C%2520causing%2520over-exploration%2520during%2520stable%2520periods%2520and%2520under-exploration%2520after%2520drift%2520%2528thus%2520slow%2520recovery%2529%252C%2520and%2520leaving%2520unanswered%2520the%2520principled%2520question%2520of%2520how%2520exploration%2520intensity%2520should%2520scale%2520with%2520drift%2520magnitude.%2520We%2520prove%2520that%2520entropy%2520scheduling%2520under%2520non-stationarity%2520can%2520be%2520reduced%2520to%2520a%2520one-dimensional%252C%2520round-by-round%2520trade-off%252C%2520faster%2520tracking%2520of%2520the%2520optimal%2520solution%2520after%2520drift%2520vs.%2520avoiding%2520gratuitous%2520randomness%2520when%2520the%2520environment%2520is%2520stable%252C%2520so%2520exploration%2520strength%2520can%2520be%2520driven%2520by%2520measurable%2520online%2520drift%2520signals.%2520Building%2520on%2520this%252C%2520we%2520propose%2520AES%2520%2528Adaptive%2520Entropy%2520Scheduling%2529%252C%2520which%2520adaptively%2520adjusts%2520the%2520entropy%2520coefficient/temperature%2520online%2520using%2520observable%2520drift%2520proxies%2520during%2520training%252C%2520requiring%2520almost%2520no%2520structural%2520changes%2520and%2520incurring%2520minimal%2520overhead.%2520Across%25204%2520algorithm%2520variants%252C%252012%2520tasks%252C%2520and%25204%2520drift%2520modes%252C%2520AES%2520significantly%2520reduces%2520the%2520fraction%2520of%2520performance%2520degradation%2520caused%2520by%2520drift%2520and%2520accelerates%2520recovery%2520after%2520abrupt%2520changes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracking%20Drift%3A%20Variation-Aware%20Entropy%20Scheduling%20for%20Non-Stationary%20Reinforcement%20Learning&entry.906535625=Tongxi%20Wang%20and%20Zhuoyang%20Xia%20and%20Xinran%20Chen%20and%20Shan%20Liu&entry.1292438233=Real-world%20reinforcement%20learning%20often%20faces%20environment%20drift%2C%20but%20most%20existing%20methods%20rely%20on%20static%20entropy%20coefficients/target%20entropy%2C%20causing%20over-exploration%20during%20stable%20periods%20and%20under-exploration%20after%20drift%20%28thus%20slow%20recovery%29%2C%20and%20leaving%20unanswered%20the%20principled%20question%20of%20how%20exploration%20intensity%20should%20scale%20with%20drift%20magnitude.%20We%20prove%20that%20entropy%20scheduling%20under%20non-stationarity%20can%20be%20reduced%20to%20a%20one-dimensional%2C%20round-by-round%20trade-off%2C%20faster%20tracking%20of%20the%20optimal%20solution%20after%20drift%20vs.%20avoiding%20gratuitous%20randomness%20when%20the%20environment%20is%20stable%2C%20so%20exploration%20strength%20can%20be%20driven%20by%20measurable%20online%20drift%20signals.%20Building%20on%20this%2C%20we%20propose%20AES%20%28Adaptive%20Entropy%20Scheduling%29%2C%20which%20adaptively%20adjusts%20the%20entropy%20coefficient/temperature%20online%20using%20observable%20drift%20proxies%20during%20training%2C%20requiring%20almost%20no%20structural%20changes%20and%20incurring%20minimal%20overhead.%20Across%204%20algorithm%20variants%2C%2012%20tasks%2C%20and%204%20drift%20modes%2C%20AES%20significantly%20reduces%20the%20fraction%20of%20performance%20degradation%20caused%20by%20drift%20and%20accelerates%20recovery%20after%20abrupt%20changes.&entry.1838667208=http%3A//arxiv.org/abs/2601.19624v1&entry.124074799=Read"},
{"title": "Mocap Anywhere: Towards Pairwise-Distance based Motion Capture in the Wild (for the Wild)", "author": "Ofir Abramovich and Ariel Shamir and Andreas Aristidou", "abstract": "We introduce a novel motion capture system that reconstructs full-body 3D motion using only sparse pairwise distance (PWD) measurements from body-mounted(UWB) sensors. Using time-of-flight ranging between wireless nodes, our method eliminates the need for external cameras, enabling robust operation in uncontrolled and outdoor environments. Unlike traditional optical or inertial systems, our approach is shape-invariant and resilient to environmental constraints such as lighting and magnetic interference. At the core of our system is Wild-Poser (WiP for short), a compact, real-time Transformer-based architecture that directly predicts 3D joint positions from noisy or corrupted PWD measurements, which can later be used for joint rotation reconstruction via learned methods. WiP generalizes across subjects of varying morphologies, including non-human species, without requiring individual body measurements or shape fitting. Operating in real time, WiP achieves low joint position error and demonstrates accurate 3D motion reconstruction for both human and animal subjects in-the-wild. Our empirical analysis highlights its potential for scalable, low-cost, and general purpose motion capture in real-world settings.", "link": "http://arxiv.org/abs/2601.19519v1", "date": "2026-01-27", "relevancy": 2.2988, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6215}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mocap%20Anywhere%3A%20Towards%20Pairwise-Distance%20based%20Motion%20Capture%20in%20the%20Wild%20%28for%20the%20Wild%29&body=Title%3A%20Mocap%20Anywhere%3A%20Towards%20Pairwise-Distance%20based%20Motion%20Capture%20in%20the%20Wild%20%28for%20the%20Wild%29%0AAuthor%3A%20Ofir%20Abramovich%20and%20Ariel%20Shamir%20and%20Andreas%20Aristidou%0AAbstract%3A%20We%20introduce%20a%20novel%20motion%20capture%20system%20that%20reconstructs%20full-body%203D%20motion%20using%20only%20sparse%20pairwise%20distance%20%28PWD%29%20measurements%20from%20body-mounted%28UWB%29%20sensors.%20Using%20time-of-flight%20ranging%20between%20wireless%20nodes%2C%20our%20method%20eliminates%20the%20need%20for%20external%20cameras%2C%20enabling%20robust%20operation%20in%20uncontrolled%20and%20outdoor%20environments.%20Unlike%20traditional%20optical%20or%20inertial%20systems%2C%20our%20approach%20is%20shape-invariant%20and%20resilient%20to%20environmental%20constraints%20such%20as%20lighting%20and%20magnetic%20interference.%20At%20the%20core%20of%20our%20system%20is%20Wild-Poser%20%28WiP%20for%20short%29%2C%20a%20compact%2C%20real-time%20Transformer-based%20architecture%20that%20directly%20predicts%203D%20joint%20positions%20from%20noisy%20or%20corrupted%20PWD%20measurements%2C%20which%20can%20later%20be%20used%20for%20joint%20rotation%20reconstruction%20via%20learned%20methods.%20WiP%20generalizes%20across%20subjects%20of%20varying%20morphologies%2C%20including%20non-human%20species%2C%20without%20requiring%20individual%20body%20measurements%20or%20shape%20fitting.%20Operating%20in%20real%20time%2C%20WiP%20achieves%20low%20joint%20position%20error%20and%20demonstrates%20accurate%203D%20motion%20reconstruction%20for%20both%20human%20and%20animal%20subjects%20in-the-wild.%20Our%20empirical%20analysis%20highlights%20its%20potential%20for%20scalable%2C%20low-cost%2C%20and%20general%20purpose%20motion%20capture%20in%20real-world%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMocap%2520Anywhere%253A%2520Towards%2520Pairwise-Distance%2520based%2520Motion%2520Capture%2520in%2520the%2520Wild%2520%2528for%2520the%2520Wild%2529%26entry.906535625%3DOfir%2520Abramovich%2520and%2520Ariel%2520Shamir%2520and%2520Andreas%2520Aristidou%26entry.1292438233%3DWe%2520introduce%2520a%2520novel%2520motion%2520capture%2520system%2520that%2520reconstructs%2520full-body%25203D%2520motion%2520using%2520only%2520sparse%2520pairwise%2520distance%2520%2528PWD%2529%2520measurements%2520from%2520body-mounted%2528UWB%2529%2520sensors.%2520Using%2520time-of-flight%2520ranging%2520between%2520wireless%2520nodes%252C%2520our%2520method%2520eliminates%2520the%2520need%2520for%2520external%2520cameras%252C%2520enabling%2520robust%2520operation%2520in%2520uncontrolled%2520and%2520outdoor%2520environments.%2520Unlike%2520traditional%2520optical%2520or%2520inertial%2520systems%252C%2520our%2520approach%2520is%2520shape-invariant%2520and%2520resilient%2520to%2520environmental%2520constraints%2520such%2520as%2520lighting%2520and%2520magnetic%2520interference.%2520At%2520the%2520core%2520of%2520our%2520system%2520is%2520Wild-Poser%2520%2528WiP%2520for%2520short%2529%252C%2520a%2520compact%252C%2520real-time%2520Transformer-based%2520architecture%2520that%2520directly%2520predicts%25203D%2520joint%2520positions%2520from%2520noisy%2520or%2520corrupted%2520PWD%2520measurements%252C%2520which%2520can%2520later%2520be%2520used%2520for%2520joint%2520rotation%2520reconstruction%2520via%2520learned%2520methods.%2520WiP%2520generalizes%2520across%2520subjects%2520of%2520varying%2520morphologies%252C%2520including%2520non-human%2520species%252C%2520without%2520requiring%2520individual%2520body%2520measurements%2520or%2520shape%2520fitting.%2520Operating%2520in%2520real%2520time%252C%2520WiP%2520achieves%2520low%2520joint%2520position%2520error%2520and%2520demonstrates%2520accurate%25203D%2520motion%2520reconstruction%2520for%2520both%2520human%2520and%2520animal%2520subjects%2520in-the-wild.%2520Our%2520empirical%2520analysis%2520highlights%2520its%2520potential%2520for%2520scalable%252C%2520low-cost%252C%2520and%2520general%2520purpose%2520motion%2520capture%2520in%2520real-world%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mocap%20Anywhere%3A%20Towards%20Pairwise-Distance%20based%20Motion%20Capture%20in%20the%20Wild%20%28for%20the%20Wild%29&entry.906535625=Ofir%20Abramovich%20and%20Ariel%20Shamir%20and%20Andreas%20Aristidou&entry.1292438233=We%20introduce%20a%20novel%20motion%20capture%20system%20that%20reconstructs%20full-body%203D%20motion%20using%20only%20sparse%20pairwise%20distance%20%28PWD%29%20measurements%20from%20body-mounted%28UWB%29%20sensors.%20Using%20time-of-flight%20ranging%20between%20wireless%20nodes%2C%20our%20method%20eliminates%20the%20need%20for%20external%20cameras%2C%20enabling%20robust%20operation%20in%20uncontrolled%20and%20outdoor%20environments.%20Unlike%20traditional%20optical%20or%20inertial%20systems%2C%20our%20approach%20is%20shape-invariant%20and%20resilient%20to%20environmental%20constraints%20such%20as%20lighting%20and%20magnetic%20interference.%20At%20the%20core%20of%20our%20system%20is%20Wild-Poser%20%28WiP%20for%20short%29%2C%20a%20compact%2C%20real-time%20Transformer-based%20architecture%20that%20directly%20predicts%203D%20joint%20positions%20from%20noisy%20or%20corrupted%20PWD%20measurements%2C%20which%20can%20later%20be%20used%20for%20joint%20rotation%20reconstruction%20via%20learned%20methods.%20WiP%20generalizes%20across%20subjects%20of%20varying%20morphologies%2C%20including%20non-human%20species%2C%20without%20requiring%20individual%20body%20measurements%20or%20shape%20fitting.%20Operating%20in%20real%20time%2C%20WiP%20achieves%20low%20joint%20position%20error%20and%20demonstrates%20accurate%203D%20motion%20reconstruction%20for%20both%20human%20and%20animal%20subjects%20in-the-wild.%20Our%20empirical%20analysis%20highlights%20its%20potential%20for%20scalable%2C%20low-cost%2C%20and%20general%20purpose%20motion%20capture%20in%20real-world%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.19519v1&entry.124074799=Read"},
{"title": "Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach", "author": "Abdurahman Maarouf and Alket Bakiaj and Stefan Feuerriegel", "abstract": "Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.", "link": "http://arxiv.org/abs/2601.16568v2", "date": "2026-01-27", "relevancy": 2.2963, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Startup%20Success%20Using%20Large%20Language%20Models%3A%20A%20Novel%20In-Context%20Learning%20Approach&body=Title%3A%20Predicting%20Startup%20Success%20Using%20Large%20Language%20Models%3A%20A%20Novel%20In-Context%20Learning%20Approach%0AAuthor%3A%20Abdurahman%20Maarouf%20and%20Alket%20Bakiaj%20and%20Stefan%20Feuerriegel%0AAbstract%3A%20Venture%20capital%20%28VC%29%20investments%20in%20early-stage%20startups%20that%20end%20up%20being%20successful%20can%20yield%20high%20returns.%20However%2C%20predicting%20early-stage%20startup%20success%20remains%20challenging%20due%20to%20data%20scarcity%20%28e.g.%2C%20many%20VC%20firms%20have%20information%20about%20only%20a%20few%20dozen%20of%20early-stage%20startups%20and%20whether%20they%20were%20successful%29.%20This%20limits%20the%20effectiveness%20of%20traditional%20machine%20learning%20methods%20that%20rely%20on%20large%20labeled%20datasets%20for%20model%20training.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20in-context%20learning%20framework%20for%20startup%20success%20prediction%20using%20large%20language%20models%20%28LLMs%29%20that%20requires%20no%20model%20training%20and%20leverages%20only%20a%20small%20set%20of%20labeled%20startups%20as%20demonstration%20examples.%20Specifically%2C%20we%20propose%20a%20novel%20k-nearest-neighbor-based%20in-context%20learning%20framework%2C%20called%20kNN-ICL%2C%20which%20selects%20the%20most%20relevant%20past%20startups%20as%20examples%20based%20on%20similarity.%20Using%20real-world%20profiles%20from%20Crunchbase%2C%20we%20find%20that%20the%20kNN-ICL%20approach%20achieves%20higher%20prediction%20accuracy%20than%20supervised%20machine%20learning%20baselines%20and%20vanilla%20in-context%20learning.%20Further%2C%20we%20study%20how%20performance%20varies%20with%20the%20number%20of%20in-context%20examples%20and%20find%20that%20a%20high%20balanced%20accuracy%20can%20be%20achieved%20with%20as%20few%20as%2050%20examples.%20Together%2C%20we%20demonstrate%20that%20in-context%20learning%20can%20serve%20as%20a%20decision-making%20tool%20for%20VC%20firms%20operating%20in%20data-scarce%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Startup%2520Success%2520Using%2520Large%2520Language%2520Models%253A%2520A%2520Novel%2520In-Context%2520Learning%2520Approach%26entry.906535625%3DAbdurahman%2520Maarouf%2520and%2520Alket%2520Bakiaj%2520and%2520Stefan%2520Feuerriegel%26entry.1292438233%3DVenture%2520capital%2520%2528VC%2529%2520investments%2520in%2520early-stage%2520startups%2520that%2520end%2520up%2520being%2520successful%2520can%2520yield%2520high%2520returns.%2520However%252C%2520predicting%2520early-stage%2520startup%2520success%2520remains%2520challenging%2520due%2520to%2520data%2520scarcity%2520%2528e.g.%252C%2520many%2520VC%2520firms%2520have%2520information%2520about%2520only%2520a%2520few%2520dozen%2520of%2520early-stage%2520startups%2520and%2520whether%2520they%2520were%2520successful%2529.%2520This%2520limits%2520the%2520effectiveness%2520of%2520traditional%2520machine%2520learning%2520methods%2520that%2520rely%2520on%2520large%2520labeled%2520datasets%2520for%2520model%2520training.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520in-context%2520learning%2520framework%2520for%2520startup%2520success%2520prediction%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520that%2520requires%2520no%2520model%2520training%2520and%2520leverages%2520only%2520a%2520small%2520set%2520of%2520labeled%2520startups%2520as%2520demonstration%2520examples.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520k-nearest-neighbor-based%2520in-context%2520learning%2520framework%252C%2520called%2520kNN-ICL%252C%2520which%2520selects%2520the%2520most%2520relevant%2520past%2520startups%2520as%2520examples%2520based%2520on%2520similarity.%2520Using%2520real-world%2520profiles%2520from%2520Crunchbase%252C%2520we%2520find%2520that%2520the%2520kNN-ICL%2520approach%2520achieves%2520higher%2520prediction%2520accuracy%2520than%2520supervised%2520machine%2520learning%2520baselines%2520and%2520vanilla%2520in-context%2520learning.%2520Further%252C%2520we%2520study%2520how%2520performance%2520varies%2520with%2520the%2520number%2520of%2520in-context%2520examples%2520and%2520find%2520that%2520a%2520high%2520balanced%2520accuracy%2520can%2520be%2520achieved%2520with%2520as%2520few%2520as%252050%2520examples.%2520Together%252C%2520we%2520demonstrate%2520that%2520in-context%2520learning%2520can%2520serve%2520as%2520a%2520decision-making%2520tool%2520for%2520VC%2520firms%2520operating%2520in%2520data-scarce%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Startup%20Success%20Using%20Large%20Language%20Models%3A%20A%20Novel%20In-Context%20Learning%20Approach&entry.906535625=Abdurahman%20Maarouf%20and%20Alket%20Bakiaj%20and%20Stefan%20Feuerriegel&entry.1292438233=Venture%20capital%20%28VC%29%20investments%20in%20early-stage%20startups%20that%20end%20up%20being%20successful%20can%20yield%20high%20returns.%20However%2C%20predicting%20early-stage%20startup%20success%20remains%20challenging%20due%20to%20data%20scarcity%20%28e.g.%2C%20many%20VC%20firms%20have%20information%20about%20only%20a%20few%20dozen%20of%20early-stage%20startups%20and%20whether%20they%20were%20successful%29.%20This%20limits%20the%20effectiveness%20of%20traditional%20machine%20learning%20methods%20that%20rely%20on%20large%20labeled%20datasets%20for%20model%20training.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20in-context%20learning%20framework%20for%20startup%20success%20prediction%20using%20large%20language%20models%20%28LLMs%29%20that%20requires%20no%20model%20training%20and%20leverages%20only%20a%20small%20set%20of%20labeled%20startups%20as%20demonstration%20examples.%20Specifically%2C%20we%20propose%20a%20novel%20k-nearest-neighbor-based%20in-context%20learning%20framework%2C%20called%20kNN-ICL%2C%20which%20selects%20the%20most%20relevant%20past%20startups%20as%20examples%20based%20on%20similarity.%20Using%20real-world%20profiles%20from%20Crunchbase%2C%20we%20find%20that%20the%20kNN-ICL%20approach%20achieves%20higher%20prediction%20accuracy%20than%20supervised%20machine%20learning%20baselines%20and%20vanilla%20in-context%20learning.%20Further%2C%20we%20study%20how%20performance%20varies%20with%20the%20number%20of%20in-context%20examples%20and%20find%20that%20a%20high%20balanced%20accuracy%20can%20be%20achieved%20with%20as%20few%20as%2050%20examples.%20Together%2C%20we%20demonstrate%20that%20in-context%20learning%20can%20serve%20as%20a%20decision-making%20tool%20for%20VC%20firms%20operating%20in%20data-scarce%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.16568v2&entry.124074799=Read"},
{"title": "Stability and Generalization of Nonconvex Optimization with Heavy-Tailed Noise", "author": "Hongxu Chen and Ke Wei and Xiaoming Yuan and Luo Luo", "abstract": "The empirical evidence indicates that stochastic optimization with heavy-tailed gradient noise is more appropriate to characterize the training of machine learning models than that with standard bounded gradient variance noise. Most existing works on this phenomenon focus on the convergence of optimization errors, while the analysis for generalization bounds under the heavy-tailed gradient noise remains limited. In this paper, we develop a general framework for establishing generalization bounds under heavy-tailed noise. Specifically, we introduce a truncation argument to achieve the generalization error bound based on the algorithmic stability under the assumption of bounded $p$th centered moment with $p\\in(1,2]$. Building on this framework, we further provide the stability and generalization analysis for several popular stochastic algorithms under heavy-tailed noise, including clipped and normalized stochastic gradient descent, as well as their mini-batch and momentum variants.", "link": "http://arxiv.org/abs/2601.19730v1", "date": "2026-01-27", "relevancy": 2.2946, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4768}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4714}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stability%20and%20Generalization%20of%20Nonconvex%20Optimization%20with%20Heavy-Tailed%20Noise&body=Title%3A%20Stability%20and%20Generalization%20of%20Nonconvex%20Optimization%20with%20Heavy-Tailed%20Noise%0AAuthor%3A%20Hongxu%20Chen%20and%20Ke%20Wei%20and%20Xiaoming%20Yuan%20and%20Luo%20Luo%0AAbstract%3A%20The%20empirical%20evidence%20indicates%20that%20stochastic%20optimization%20with%20heavy-tailed%20gradient%20noise%20is%20more%20appropriate%20to%20characterize%20the%20training%20of%20machine%20learning%20models%20than%20that%20with%20standard%20bounded%20gradient%20variance%20noise.%20Most%20existing%20works%20on%20this%20phenomenon%20focus%20on%20the%20convergence%20of%20optimization%20errors%2C%20while%20the%20analysis%20for%20generalization%20bounds%20under%20the%20heavy-tailed%20gradient%20noise%20remains%20limited.%20In%20this%20paper%2C%20we%20develop%20a%20general%20framework%20for%20establishing%20generalization%20bounds%20under%20heavy-tailed%20noise.%20Specifically%2C%20we%20introduce%20a%20truncation%20argument%20to%20achieve%20the%20generalization%20error%20bound%20based%20on%20the%20algorithmic%20stability%20under%20the%20assumption%20of%20bounded%20%24p%24th%20centered%20moment%20with%20%24p%5Cin%281%2C2%5D%24.%20Building%20on%20this%20framework%2C%20we%20further%20provide%20the%20stability%20and%20generalization%20analysis%20for%20several%20popular%20stochastic%20algorithms%20under%20heavy-tailed%20noise%2C%20including%20clipped%20and%20normalized%20stochastic%20gradient%20descent%2C%20as%20well%20as%20their%20mini-batch%20and%20momentum%20variants.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStability%2520and%2520Generalization%2520of%2520Nonconvex%2520Optimization%2520with%2520Heavy-Tailed%2520Noise%26entry.906535625%3DHongxu%2520Chen%2520and%2520Ke%2520Wei%2520and%2520Xiaoming%2520Yuan%2520and%2520Luo%2520Luo%26entry.1292438233%3DThe%2520empirical%2520evidence%2520indicates%2520that%2520stochastic%2520optimization%2520with%2520heavy-tailed%2520gradient%2520noise%2520is%2520more%2520appropriate%2520to%2520characterize%2520the%2520training%2520of%2520machine%2520learning%2520models%2520than%2520that%2520with%2520standard%2520bounded%2520gradient%2520variance%2520noise.%2520Most%2520existing%2520works%2520on%2520this%2520phenomenon%2520focus%2520on%2520the%2520convergence%2520of%2520optimization%2520errors%252C%2520while%2520the%2520analysis%2520for%2520generalization%2520bounds%2520under%2520the%2520heavy-tailed%2520gradient%2520noise%2520remains%2520limited.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520general%2520framework%2520for%2520establishing%2520generalization%2520bounds%2520under%2520heavy-tailed%2520noise.%2520Specifically%252C%2520we%2520introduce%2520a%2520truncation%2520argument%2520to%2520achieve%2520the%2520generalization%2520error%2520bound%2520based%2520on%2520the%2520algorithmic%2520stability%2520under%2520the%2520assumption%2520of%2520bounded%2520%2524p%2524th%2520centered%2520moment%2520with%2520%2524p%255Cin%25281%252C2%255D%2524.%2520Building%2520on%2520this%2520framework%252C%2520we%2520further%2520provide%2520the%2520stability%2520and%2520generalization%2520analysis%2520for%2520several%2520popular%2520stochastic%2520algorithms%2520under%2520heavy-tailed%2520noise%252C%2520including%2520clipped%2520and%2520normalized%2520stochastic%2520gradient%2520descent%252C%2520as%2520well%2520as%2520their%2520mini-batch%2520and%2520momentum%2520variants.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stability%20and%20Generalization%20of%20Nonconvex%20Optimization%20with%20Heavy-Tailed%20Noise&entry.906535625=Hongxu%20Chen%20and%20Ke%20Wei%20and%20Xiaoming%20Yuan%20and%20Luo%20Luo&entry.1292438233=The%20empirical%20evidence%20indicates%20that%20stochastic%20optimization%20with%20heavy-tailed%20gradient%20noise%20is%20more%20appropriate%20to%20characterize%20the%20training%20of%20machine%20learning%20models%20than%20that%20with%20standard%20bounded%20gradient%20variance%20noise.%20Most%20existing%20works%20on%20this%20phenomenon%20focus%20on%20the%20convergence%20of%20optimization%20errors%2C%20while%20the%20analysis%20for%20generalization%20bounds%20under%20the%20heavy-tailed%20gradient%20noise%20remains%20limited.%20In%20this%20paper%2C%20we%20develop%20a%20general%20framework%20for%20establishing%20generalization%20bounds%20under%20heavy-tailed%20noise.%20Specifically%2C%20we%20introduce%20a%20truncation%20argument%20to%20achieve%20the%20generalization%20error%20bound%20based%20on%20the%20algorithmic%20stability%20under%20the%20assumption%20of%20bounded%20%24p%24th%20centered%20moment%20with%20%24p%5Cin%281%2C2%5D%24.%20Building%20on%20this%20framework%2C%20we%20further%20provide%20the%20stability%20and%20generalization%20analysis%20for%20several%20popular%20stochastic%20algorithms%20under%20heavy-tailed%20noise%2C%20including%20clipped%20and%20normalized%20stochastic%20gradient%20descent%2C%20as%20well%20as%20their%20mini-batch%20and%20momentum%20variants.&entry.1838667208=http%3A//arxiv.org/abs/2601.19730v1&entry.124074799=Read"},
{"title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge", "author": "Marthe Ballon and Andres Algaba and Brecht Verbeken and Vincent Ginis", "abstract": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.", "link": "http://arxiv.org/abs/2601.19532v1", "date": "2026-01-27", "relevancy": 2.2874, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarks%20Saturate%20When%20The%20Model%20Gets%20Smarter%20Than%20The%20Judge&body=Title%3A%20Benchmarks%20Saturate%20When%20The%20Model%20Gets%20Smarter%20Than%20The%20Judge%0AAuthor%3A%20Marthe%20Ballon%20and%20Andres%20Algaba%20and%20Brecht%20Verbeken%20and%20Vincent%20Ginis%0AAbstract%3A%20Benchmarks%20are%20important%20tools%20to%20track%20progress%20in%20the%20development%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20inaccuracies%20in%20datasets%20and%20evaluation%20methods%20consistently%20undermine%20their%20effectiveness.%20Here%2C%20we%20present%20Omni-MATH-2%2C%20a%20manually%20revised%20version%20of%20the%20Omni-MATH%20dataset%20comprising%20a%20clean%2C%20exact-answer%20subset%20%28%24n%7B%3D%7D4181%24%29%20and%20a%20tagged%2C%20non-standard%20subset%20%28%24n%7B%3D%7D247%24%29.%20Each%20problem%20was%20audited%20to%20ensure%20LaTeX%20compilability%2C%20solvability%20and%20verifiability%2C%20which%20involved%20adding%20missing%20figures%20or%20information%2C%20labeling%20problems%20requiring%20a%20proof%2C%20estimation%20or%20image%2C%20and%20removing%20clutter.%20This%20process%20significantly%20reduces%20dataset-induced%20noise%2C%20thereby%20providing%20a%20more%20precise%20assessment%20of%20model%20performance.%20The%20annotated%20dataset%20also%20allows%20us%20to%20evaluate%20judge-induced%20noise%20by%20comparing%20GPT-5%20mini%20with%20the%20original%20Omni-Judge%2C%20revealing%20substantial%20discrepancies%20between%20judges%20on%20both%20the%20clean%20and%20tagged%20problem%20subsets.%20Expert%20annotations%20reveal%20that%20Omni-Judge%20is%20wrong%20in%20%2496.4%5C%25%24%20of%20the%20judge%20disagreements%2C%20indicating%20its%20inability%20to%20differentiate%20between%20models%27%20abilities%2C%20even%20well%20before%20saturation%20of%20the%20benchmark%20occurs.%20As%20problems%20become%20more%20challenging%2C%20we%20find%20that%20increasingly%20competent%20judges%20become%20essential%20in%20order%20to%20prevent%20judge%20errors%20from%20masking%20genuine%20differences%20between%20models.%20Finally%2C%20neither%20judge%20identifies%20the%20present%20failure%20modes%20for%20the%20subset%20of%20tagged%20problems%2C%20demonstrating%20that%20dataset%20quality%20and%20judge%20reliability%20are%20both%20critical%20to%20develop%20accurate%20benchmarks%20of%20model%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarks%2520Saturate%2520When%2520The%2520Model%2520Gets%2520Smarter%2520Than%2520The%2520Judge%26entry.906535625%3DMarthe%2520Ballon%2520and%2520Andres%2520Algaba%2520and%2520Brecht%2520Verbeken%2520and%2520Vincent%2520Ginis%26entry.1292438233%3DBenchmarks%2520are%2520important%2520tools%2520to%2520track%2520progress%2520in%2520the%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520yet%2520inaccuracies%2520in%2520datasets%2520and%2520evaluation%2520methods%2520consistently%2520undermine%2520their%2520effectiveness.%2520Here%252C%2520we%2520present%2520Omni-MATH-2%252C%2520a%2520manually%2520revised%2520version%2520of%2520the%2520Omni-MATH%2520dataset%2520comprising%2520a%2520clean%252C%2520exact-answer%2520subset%2520%2528%2524n%257B%253D%257D4181%2524%2529%2520and%2520a%2520tagged%252C%2520non-standard%2520subset%2520%2528%2524n%257B%253D%257D247%2524%2529.%2520Each%2520problem%2520was%2520audited%2520to%2520ensure%2520LaTeX%2520compilability%252C%2520solvability%2520and%2520verifiability%252C%2520which%2520involved%2520adding%2520missing%2520figures%2520or%2520information%252C%2520labeling%2520problems%2520requiring%2520a%2520proof%252C%2520estimation%2520or%2520image%252C%2520and%2520removing%2520clutter.%2520This%2520process%2520significantly%2520reduces%2520dataset-induced%2520noise%252C%2520thereby%2520providing%2520a%2520more%2520precise%2520assessment%2520of%2520model%2520performance.%2520The%2520annotated%2520dataset%2520also%2520allows%2520us%2520to%2520evaluate%2520judge-induced%2520noise%2520by%2520comparing%2520GPT-5%2520mini%2520with%2520the%2520original%2520Omni-Judge%252C%2520revealing%2520substantial%2520discrepancies%2520between%2520judges%2520on%2520both%2520the%2520clean%2520and%2520tagged%2520problem%2520subsets.%2520Expert%2520annotations%2520reveal%2520that%2520Omni-Judge%2520is%2520wrong%2520in%2520%252496.4%255C%2525%2524%2520of%2520the%2520judge%2520disagreements%252C%2520indicating%2520its%2520inability%2520to%2520differentiate%2520between%2520models%2527%2520abilities%252C%2520even%2520well%2520before%2520saturation%2520of%2520the%2520benchmark%2520occurs.%2520As%2520problems%2520become%2520more%2520challenging%252C%2520we%2520find%2520that%2520increasingly%2520competent%2520judges%2520become%2520essential%2520in%2520order%2520to%2520prevent%2520judge%2520errors%2520from%2520masking%2520genuine%2520differences%2520between%2520models.%2520Finally%252C%2520neither%2520judge%2520identifies%2520the%2520present%2520failure%2520modes%2520for%2520the%2520subset%2520of%2520tagged%2520problems%252C%2520demonstrating%2520that%2520dataset%2520quality%2520and%2520judge%2520reliability%2520are%2520both%2520critical%2520to%2520develop%2520accurate%2520benchmarks%2520of%2520model%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarks%20Saturate%20When%20The%20Model%20Gets%20Smarter%20Than%20The%20Judge&entry.906535625=Marthe%20Ballon%20and%20Andres%20Algaba%20and%20Brecht%20Verbeken%20and%20Vincent%20Ginis&entry.1292438233=Benchmarks%20are%20important%20tools%20to%20track%20progress%20in%20the%20development%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20inaccuracies%20in%20datasets%20and%20evaluation%20methods%20consistently%20undermine%20their%20effectiveness.%20Here%2C%20we%20present%20Omni-MATH-2%2C%20a%20manually%20revised%20version%20of%20the%20Omni-MATH%20dataset%20comprising%20a%20clean%2C%20exact-answer%20subset%20%28%24n%7B%3D%7D4181%24%29%20and%20a%20tagged%2C%20non-standard%20subset%20%28%24n%7B%3D%7D247%24%29.%20Each%20problem%20was%20audited%20to%20ensure%20LaTeX%20compilability%2C%20solvability%20and%20verifiability%2C%20which%20involved%20adding%20missing%20figures%20or%20information%2C%20labeling%20problems%20requiring%20a%20proof%2C%20estimation%20or%20image%2C%20and%20removing%20clutter.%20This%20process%20significantly%20reduces%20dataset-induced%20noise%2C%20thereby%20providing%20a%20more%20precise%20assessment%20of%20model%20performance.%20The%20annotated%20dataset%20also%20allows%20us%20to%20evaluate%20judge-induced%20noise%20by%20comparing%20GPT-5%20mini%20with%20the%20original%20Omni-Judge%2C%20revealing%20substantial%20discrepancies%20between%20judges%20on%20both%20the%20clean%20and%20tagged%20problem%20subsets.%20Expert%20annotations%20reveal%20that%20Omni-Judge%20is%20wrong%20in%20%2496.4%5C%25%24%20of%20the%20judge%20disagreements%2C%20indicating%20its%20inability%20to%20differentiate%20between%20models%27%20abilities%2C%20even%20well%20before%20saturation%20of%20the%20benchmark%20occurs.%20As%20problems%20become%20more%20challenging%2C%20we%20find%20that%20increasingly%20competent%20judges%20become%20essential%20in%20order%20to%20prevent%20judge%20errors%20from%20masking%20genuine%20differences%20between%20models.%20Finally%2C%20neither%20judge%20identifies%20the%20present%20failure%20modes%20for%20the%20subset%20of%20tagged%20problems%2C%20demonstrating%20that%20dataset%20quality%20and%20judge%20reliability%20are%20both%20critical%20to%20develop%20accurate%20benchmarks%20of%20model%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.19532v1&entry.124074799=Read"},
{"title": "Assessing the Effectiveness of Deep Embeddings for Tree Species Classification in the Dutch Forest Inventory", "author": "Takayuki Ishikawa and Carmelo Bonannella and Bas J. W. Lerink and Marc Ru\u00dfwurm", "abstract": "National Forest Inventory serves as the primary source of forest information, however, maintaining these inventories requires labor-intensive on-site campaigns by forestry experts to identify and document tree species. Embeddings from deep pre-trained remote sensing models offer new opportunities to update NFIs more frequently and at larger scales. While training new deep learning models on few data points remains challenging, we show that using pre-computed embeddings can proven effective for distinguishing tree species through seasonal canopy reflectance patternsin combination with Random Forest. This work systematically investigates how deep embeddings improve tree species classification accuracy in the Netherlands with few annotated data. We evaluate this question on three embedding models: Presto, Alpha Earth, and Tessera, using three tree species datasets of varying difficulty. Data-wise, we compare the available embeddings from Alpha Earth and Tessera with dynamically calculated embeddings from a pre-trained Presto model. Our results demonstrate that fine-tuning a publicly available remote sensing time series pre-trained model outperforms the current state-of-the-art in NFI classification in the Netherlands, yielding performance gains of approximately 2-9 percentage points across datasets and evaluation metrics. This indicates that classic hand-defined features are too simple for this task and highlights the potential of using deep embeddings for data-limited applications such as NFI classification. By leveraging openly available satellite data and deep embeddings from pre-trained models, this approach significantly improves classification accuracy compared to traditional methods and can effectively complement existing forest inventory processes.", "link": "http://arxiv.org/abs/2508.18829v2", "date": "2026-01-27", "relevancy": 2.2833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4619}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Effectiveness%20of%20Deep%20Embeddings%20for%20Tree%20Species%20Classification%20in%20the%20Dutch%20Forest%20Inventory&body=Title%3A%20Assessing%20the%20Effectiveness%20of%20Deep%20Embeddings%20for%20Tree%20Species%20Classification%20in%20the%20Dutch%20Forest%20Inventory%0AAuthor%3A%20Takayuki%20Ishikawa%20and%20Carmelo%20Bonannella%20and%20Bas%20J.%20W.%20Lerink%20and%20Marc%20Ru%C3%9Fwurm%0AAbstract%3A%20National%20Forest%20Inventory%20serves%20as%20the%20primary%20source%20of%20forest%20information%2C%20however%2C%20maintaining%20these%20inventories%20requires%20labor-intensive%20on-site%20campaigns%20by%20forestry%20experts%20to%20identify%20and%20document%20tree%20species.%20Embeddings%20from%20deep%20pre-trained%20remote%20sensing%20models%20offer%20new%20opportunities%20to%20update%20NFIs%20more%20frequently%20and%20at%20larger%20scales.%20While%20training%20new%20deep%20learning%20models%20on%20few%20data%20points%20remains%20challenging%2C%20we%20show%20that%20using%20pre-computed%20embeddings%20can%20proven%20effective%20for%20distinguishing%20tree%20species%20through%20seasonal%20canopy%20reflectance%20patternsin%20combination%20with%20Random%20Forest.%20This%20work%20systematically%20investigates%20how%20deep%20embeddings%20improve%20tree%20species%20classification%20accuracy%20in%20the%20Netherlands%20with%20few%20annotated%20data.%20We%20evaluate%20this%20question%20on%20three%20embedding%20models%3A%20Presto%2C%20Alpha%20Earth%2C%20and%20Tessera%2C%20using%20three%20tree%20species%20datasets%20of%20varying%20difficulty.%20Data-wise%2C%20we%20compare%20the%20available%20embeddings%20from%20Alpha%20Earth%20and%20Tessera%20with%20dynamically%20calculated%20embeddings%20from%20a%20pre-trained%20Presto%20model.%20Our%20results%20demonstrate%20that%20fine-tuning%20a%20publicly%20available%20remote%20sensing%20time%20series%20pre-trained%20model%20outperforms%20the%20current%20state-of-the-art%20in%20NFI%20classification%20in%20the%20Netherlands%2C%20yielding%20performance%20gains%20of%20approximately%202-9%20percentage%20points%20across%20datasets%20and%20evaluation%20metrics.%20This%20indicates%20that%20classic%20hand-defined%20features%20are%20too%20simple%20for%20this%20task%20and%20highlights%20the%20potential%20of%20using%20deep%20embeddings%20for%20data-limited%20applications%20such%20as%20NFI%20classification.%20By%20leveraging%20openly%20available%20satellite%20data%20and%20deep%20embeddings%20from%20pre-trained%20models%2C%20this%20approach%20significantly%20improves%20classification%20accuracy%20compared%20to%20traditional%20methods%20and%20can%20effectively%20complement%20existing%20forest%20inventory%20processes.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18829v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Effectiveness%2520of%2520Deep%2520Embeddings%2520for%2520Tree%2520Species%2520Classification%2520in%2520the%2520Dutch%2520Forest%2520Inventory%26entry.906535625%3DTakayuki%2520Ishikawa%2520and%2520Carmelo%2520Bonannella%2520and%2520Bas%2520J.%2520W.%2520Lerink%2520and%2520Marc%2520Ru%25C3%259Fwurm%26entry.1292438233%3DNational%2520Forest%2520Inventory%2520serves%2520as%2520the%2520primary%2520source%2520of%2520forest%2520information%252C%2520however%252C%2520maintaining%2520these%2520inventories%2520requires%2520labor-intensive%2520on-site%2520campaigns%2520by%2520forestry%2520experts%2520to%2520identify%2520and%2520document%2520tree%2520species.%2520Embeddings%2520from%2520deep%2520pre-trained%2520remote%2520sensing%2520models%2520offer%2520new%2520opportunities%2520to%2520update%2520NFIs%2520more%2520frequently%2520and%2520at%2520larger%2520scales.%2520While%2520training%2520new%2520deep%2520learning%2520models%2520on%2520few%2520data%2520points%2520remains%2520challenging%252C%2520we%2520show%2520that%2520using%2520pre-computed%2520embeddings%2520can%2520proven%2520effective%2520for%2520distinguishing%2520tree%2520species%2520through%2520seasonal%2520canopy%2520reflectance%2520patternsin%2520combination%2520with%2520Random%2520Forest.%2520This%2520work%2520systematically%2520investigates%2520how%2520deep%2520embeddings%2520improve%2520tree%2520species%2520classification%2520accuracy%2520in%2520the%2520Netherlands%2520with%2520few%2520annotated%2520data.%2520We%2520evaluate%2520this%2520question%2520on%2520three%2520embedding%2520models%253A%2520Presto%252C%2520Alpha%2520Earth%252C%2520and%2520Tessera%252C%2520using%2520three%2520tree%2520species%2520datasets%2520of%2520varying%2520difficulty.%2520Data-wise%252C%2520we%2520compare%2520the%2520available%2520embeddings%2520from%2520Alpha%2520Earth%2520and%2520Tessera%2520with%2520dynamically%2520calculated%2520embeddings%2520from%2520a%2520pre-trained%2520Presto%2520model.%2520Our%2520results%2520demonstrate%2520that%2520fine-tuning%2520a%2520publicly%2520available%2520remote%2520sensing%2520time%2520series%2520pre-trained%2520model%2520outperforms%2520the%2520current%2520state-of-the-art%2520in%2520NFI%2520classification%2520in%2520the%2520Netherlands%252C%2520yielding%2520performance%2520gains%2520of%2520approximately%25202-9%2520percentage%2520points%2520across%2520datasets%2520and%2520evaluation%2520metrics.%2520This%2520indicates%2520that%2520classic%2520hand-defined%2520features%2520are%2520too%2520simple%2520for%2520this%2520task%2520and%2520highlights%2520the%2520potential%2520of%2520using%2520deep%2520embeddings%2520for%2520data-limited%2520applications%2520such%2520as%2520NFI%2520classification.%2520By%2520leveraging%2520openly%2520available%2520satellite%2520data%2520and%2520deep%2520embeddings%2520from%2520pre-trained%2520models%252C%2520this%2520approach%2520significantly%2520improves%2520classification%2520accuracy%2520compared%2520to%2520traditional%2520methods%2520and%2520can%2520effectively%2520complement%2520existing%2520forest%2520inventory%2520processes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18829v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Effectiveness%20of%20Deep%20Embeddings%20for%20Tree%20Species%20Classification%20in%20the%20Dutch%20Forest%20Inventory&entry.906535625=Takayuki%20Ishikawa%20and%20Carmelo%20Bonannella%20and%20Bas%20J.%20W.%20Lerink%20and%20Marc%20Ru%C3%9Fwurm&entry.1292438233=National%20Forest%20Inventory%20serves%20as%20the%20primary%20source%20of%20forest%20information%2C%20however%2C%20maintaining%20these%20inventories%20requires%20labor-intensive%20on-site%20campaigns%20by%20forestry%20experts%20to%20identify%20and%20document%20tree%20species.%20Embeddings%20from%20deep%20pre-trained%20remote%20sensing%20models%20offer%20new%20opportunities%20to%20update%20NFIs%20more%20frequently%20and%20at%20larger%20scales.%20While%20training%20new%20deep%20learning%20models%20on%20few%20data%20points%20remains%20challenging%2C%20we%20show%20that%20using%20pre-computed%20embeddings%20can%20proven%20effective%20for%20distinguishing%20tree%20species%20through%20seasonal%20canopy%20reflectance%20patternsin%20combination%20with%20Random%20Forest.%20This%20work%20systematically%20investigates%20how%20deep%20embeddings%20improve%20tree%20species%20classification%20accuracy%20in%20the%20Netherlands%20with%20few%20annotated%20data.%20We%20evaluate%20this%20question%20on%20three%20embedding%20models%3A%20Presto%2C%20Alpha%20Earth%2C%20and%20Tessera%2C%20using%20three%20tree%20species%20datasets%20of%20varying%20difficulty.%20Data-wise%2C%20we%20compare%20the%20available%20embeddings%20from%20Alpha%20Earth%20and%20Tessera%20with%20dynamically%20calculated%20embeddings%20from%20a%20pre-trained%20Presto%20model.%20Our%20results%20demonstrate%20that%20fine-tuning%20a%20publicly%20available%20remote%20sensing%20time%20series%20pre-trained%20model%20outperforms%20the%20current%20state-of-the-art%20in%20NFI%20classification%20in%20the%20Netherlands%2C%20yielding%20performance%20gains%20of%20approximately%202-9%20percentage%20points%20across%20datasets%20and%20evaluation%20metrics.%20This%20indicates%20that%20classic%20hand-defined%20features%20are%20too%20simple%20for%20this%20task%20and%20highlights%20the%20potential%20of%20using%20deep%20embeddings%20for%20data-limited%20applications%20such%20as%20NFI%20classification.%20By%20leveraging%20openly%20available%20satellite%20data%20and%20deep%20embeddings%20from%20pre-trained%20models%2C%20this%20approach%20significantly%20improves%20classification%20accuracy%20compared%20to%20traditional%20methods%20and%20can%20effectively%20complement%20existing%20forest%20inventory%20processes.&entry.1838667208=http%3A//arxiv.org/abs/2508.18829v2&entry.124074799=Read"},
{"title": "Learning Adaptive Parallel Execution for Efficient Code Localization", "author": "Ke Xu and Siyang Xiao and Ming Liang and Yichen Yu and Zhixiang Wang and Jingxuan Xu and Dajun Chen and Wei Jiang and Yong Li", "abstract": "Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\\% redundant invocation rate, which negates parallelism benefits. We propose \\textbf{FuseSearch}, reformulating parallel code localization as a \\textbf{joint quality-efficiency optimization} task. Through defining \\textbf{tool efficiency} -- the ratio of unique information gain to invocation count -- we utilize a two-phase SFT and RL training approach for learning adaptive parallel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modulates search breadth according to task context, evolving from exploration phases to refinement stages. Evaluated on SWE-bench Verified, FuseSearch-4B achieves SOTA-level performance (84.7\\% file-level and 56.4\\% function-level $F_1$ scores) with 93.6\\% speedup, utilizing 67.7\\% fewer turns and 68.9\\% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminating noisy redundant signals, enabling high-performance cost-effective localization agents.", "link": "http://arxiv.org/abs/2601.19568v1", "date": "2026-01-27", "relevancy": 2.2802, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.461}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4556}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Adaptive%20Parallel%20Execution%20for%20Efficient%20Code%20Localization&body=Title%3A%20Learning%20Adaptive%20Parallel%20Execution%20for%20Efficient%20Code%20Localization%0AAuthor%3A%20Ke%20Xu%20and%20Siyang%20Xiao%20and%20Ming%20Liang%20and%20Yichen%20Yu%20and%20Zhixiang%20Wang%20and%20Jingxuan%20Xu%20and%20Dajun%20Chen%20and%20Wei%20Jiang%20and%20Yong%20Li%0AAbstract%3A%20Code%20localization%20constitutes%20a%20key%20bottleneck%20in%20automated%20software%20development%20pipelines.%20While%20concurrent%20tool%20execution%20can%20enhance%20discovery%20speed%2C%20current%20agents%20demonstrate%20a%2034.9%5C%25%20redundant%20invocation%20rate%2C%20which%20negates%20parallelism%20benefits.%20We%20propose%20%5Ctextbf%7BFuseSearch%7D%2C%20reformulating%20parallel%20code%20localization%20as%20a%20%5Ctextbf%7Bjoint%20quality-efficiency%20optimization%7D%20task.%20Through%20defining%20%5Ctextbf%7Btool%20efficiency%7D%20--%20the%20ratio%20of%20unique%20information%20gain%20to%20invocation%20count%20--%20we%20utilize%20a%20two-phase%20SFT%20and%20RL%20training%20approach%20for%20learning%20adaptive%20parallel%20strategies.%20Different%20from%20fixed-breadth%20approaches%2C%20FuseSearch%20dynamically%20modulates%20search%20breadth%20according%20to%20task%20context%2C%20evolving%20from%20exploration%20phases%20to%20refinement%20stages.%20Evaluated%20on%20SWE-bench%20Verified%2C%20FuseSearch-4B%20achieves%20SOTA-level%20performance%20%2884.7%5C%25%20file-level%20and%2056.4%5C%25%20function-level%20%24F_1%24%20scores%29%20with%2093.6%5C%25%20speedup%2C%20utilizing%2067.7%5C%25%20fewer%20turns%20and%2068.9%5C%25%20fewer%20tokens.%20Results%20indicate%20that%20efficiency-aware%20training%20naturally%20improves%20quality%20through%20eliminating%20noisy%20redundant%20signals%2C%20enabling%20high-performance%20cost-effective%20localization%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Adaptive%2520Parallel%2520Execution%2520for%2520Efficient%2520Code%2520Localization%26entry.906535625%3DKe%2520Xu%2520and%2520Siyang%2520Xiao%2520and%2520Ming%2520Liang%2520and%2520Yichen%2520Yu%2520and%2520Zhixiang%2520Wang%2520and%2520Jingxuan%2520Xu%2520and%2520Dajun%2520Chen%2520and%2520Wei%2520Jiang%2520and%2520Yong%2520Li%26entry.1292438233%3DCode%2520localization%2520constitutes%2520a%2520key%2520bottleneck%2520in%2520automated%2520software%2520development%2520pipelines.%2520While%2520concurrent%2520tool%2520execution%2520can%2520enhance%2520discovery%2520speed%252C%2520current%2520agents%2520demonstrate%2520a%252034.9%255C%2525%2520redundant%2520invocation%2520rate%252C%2520which%2520negates%2520parallelism%2520benefits.%2520We%2520propose%2520%255Ctextbf%257BFuseSearch%257D%252C%2520reformulating%2520parallel%2520code%2520localization%2520as%2520a%2520%255Ctextbf%257Bjoint%2520quality-efficiency%2520optimization%257D%2520task.%2520Through%2520defining%2520%255Ctextbf%257Btool%2520efficiency%257D%2520--%2520the%2520ratio%2520of%2520unique%2520information%2520gain%2520to%2520invocation%2520count%2520--%2520we%2520utilize%2520a%2520two-phase%2520SFT%2520and%2520RL%2520training%2520approach%2520for%2520learning%2520adaptive%2520parallel%2520strategies.%2520Different%2520from%2520fixed-breadth%2520approaches%252C%2520FuseSearch%2520dynamically%2520modulates%2520search%2520breadth%2520according%2520to%2520task%2520context%252C%2520evolving%2520from%2520exploration%2520phases%2520to%2520refinement%2520stages.%2520Evaluated%2520on%2520SWE-bench%2520Verified%252C%2520FuseSearch-4B%2520achieves%2520SOTA-level%2520performance%2520%252884.7%255C%2525%2520file-level%2520and%252056.4%255C%2525%2520function-level%2520%2524F_1%2524%2520scores%2529%2520with%252093.6%255C%2525%2520speedup%252C%2520utilizing%252067.7%255C%2525%2520fewer%2520turns%2520and%252068.9%255C%2525%2520fewer%2520tokens.%2520Results%2520indicate%2520that%2520efficiency-aware%2520training%2520naturally%2520improves%2520quality%2520through%2520eliminating%2520noisy%2520redundant%2520signals%252C%2520enabling%2520high-performance%2520cost-effective%2520localization%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Adaptive%20Parallel%20Execution%20for%20Efficient%20Code%20Localization&entry.906535625=Ke%20Xu%20and%20Siyang%20Xiao%20and%20Ming%20Liang%20and%20Yichen%20Yu%20and%20Zhixiang%20Wang%20and%20Jingxuan%20Xu%20and%20Dajun%20Chen%20and%20Wei%20Jiang%20and%20Yong%20Li&entry.1292438233=Code%20localization%20constitutes%20a%20key%20bottleneck%20in%20automated%20software%20development%20pipelines.%20While%20concurrent%20tool%20execution%20can%20enhance%20discovery%20speed%2C%20current%20agents%20demonstrate%20a%2034.9%5C%25%20redundant%20invocation%20rate%2C%20which%20negates%20parallelism%20benefits.%20We%20propose%20%5Ctextbf%7BFuseSearch%7D%2C%20reformulating%20parallel%20code%20localization%20as%20a%20%5Ctextbf%7Bjoint%20quality-efficiency%20optimization%7D%20task.%20Through%20defining%20%5Ctextbf%7Btool%20efficiency%7D%20--%20the%20ratio%20of%20unique%20information%20gain%20to%20invocation%20count%20--%20we%20utilize%20a%20two-phase%20SFT%20and%20RL%20training%20approach%20for%20learning%20adaptive%20parallel%20strategies.%20Different%20from%20fixed-breadth%20approaches%2C%20FuseSearch%20dynamically%20modulates%20search%20breadth%20according%20to%20task%20context%2C%20evolving%20from%20exploration%20phases%20to%20refinement%20stages.%20Evaluated%20on%20SWE-bench%20Verified%2C%20FuseSearch-4B%20achieves%20SOTA-level%20performance%20%2884.7%5C%25%20file-level%20and%2056.4%5C%25%20function-level%20%24F_1%24%20scores%29%20with%2093.6%5C%25%20speedup%2C%20utilizing%2067.7%5C%25%20fewer%20turns%20and%2068.9%5C%25%20fewer%20tokens.%20Results%20indicate%20that%20efficiency-aware%20training%20naturally%20improves%20quality%20through%20eliminating%20noisy%20redundant%20signals%2C%20enabling%20high-performance%20cost-effective%20localization%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.19568v1&entry.124074799=Read"},
{"title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding", "author": "Zhiyi Zhu and Xiaoyu Wu and Zihao Liu and Linlin Yang", "abstract": "Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding. Existing Transformer-based methods often suffer from redundant attention and suboptimal multi-modal alignment. To address these limitations, we propose MLVTG, a novel framework that integrates two key modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba blocks as a backbone instead of Transformers to model temporal dependencies and extract robust video representations for multi-modal alignment. LLMRefiner leverages the specific frozen layer of a pre-trained Large Language Model (LLM) to implicitly transfer semantic priors, enhancing multi-modal alignment without fine-tuning. This dual alignment strategy, temporal modeling via structured state-space dynamics and semantic purification via textual priors, enables more precise localization. Extensive experiments on QVHighlights, Charades-STA, and TVSum demonstrate that MLVTG achieves state-of-the-art performance and significantly outperforms existing baselines.", "link": "http://arxiv.org/abs/2506.08512v2", "date": "2026-01-27", "relevancy": 2.2712, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5958}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLVTG%3A%20Mamba-Based%20Feature%20Alignment%20and%20LLM-Driven%20Purification%20for%20Multi-Modal%20Video%20Temporal%20Grounding&body=Title%3A%20MLVTG%3A%20Mamba-Based%20Feature%20Alignment%20and%20LLM-Driven%20Purification%20for%20Multi-Modal%20Video%20Temporal%20Grounding%0AAuthor%3A%20Zhiyi%20Zhu%20and%20Xiaoyu%20Wu%20and%20Zihao%20Liu%20and%20Linlin%20Yang%0AAbstract%3A%20Video%20Temporal%20Grounding%20%28VTG%29%2C%20which%20aims%20to%20localize%20video%20clips%20corresponding%20to%20natural%20language%20queries%2C%20is%20a%20fundamental%20yet%20challenging%20task%20in%20video%20understanding.%20Existing%20Transformer-based%20methods%20often%20suffer%20from%20redundant%20attention%20and%20suboptimal%20multi-modal%20alignment.%20To%20address%20these%20limitations%2C%20we%20propose%20MLVTG%2C%20a%20novel%20framework%20that%20integrates%20two%20key%20modules%3A%20MambaAligner%20and%20LLMRefiner.%20MambaAligner%20uses%20stacked%20Vision%20Mamba%20blocks%20as%20a%20backbone%20instead%20of%20Transformers%20to%20model%20temporal%20dependencies%20and%20extract%20robust%20video%20representations%20for%20multi-modal%20alignment.%20LLMRefiner%20leverages%20the%20specific%20frozen%20layer%20of%20a%20pre-trained%20Large%20Language%20Model%20%28LLM%29%20to%20implicitly%20transfer%20semantic%20priors%2C%20enhancing%20multi-modal%20alignment%20without%20fine-tuning.%20This%20dual%20alignment%20strategy%2C%20temporal%20modeling%20via%20structured%20state-space%20dynamics%20and%20semantic%20purification%20via%20textual%20priors%2C%20enables%20more%20precise%20localization.%20Extensive%20experiments%20on%20QVHighlights%2C%20Charades-STA%2C%20and%20TVSum%20demonstrate%20that%20MLVTG%20achieves%20state-of-the-art%20performance%20and%20significantly%20outperforms%20existing%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2506.08512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLVTG%253A%2520Mamba-Based%2520Feature%2520Alignment%2520and%2520LLM-Driven%2520Purification%2520for%2520Multi-Modal%2520Video%2520Temporal%2520Grounding%26entry.906535625%3DZhiyi%2520Zhu%2520and%2520Xiaoyu%2520Wu%2520and%2520Zihao%2520Liu%2520and%2520Linlin%2520Yang%26entry.1292438233%3DVideo%2520Temporal%2520Grounding%2520%2528VTG%2529%252C%2520which%2520aims%2520to%2520localize%2520video%2520clips%2520corresponding%2520to%2520natural%2520language%2520queries%252C%2520is%2520a%2520fundamental%2520yet%2520challenging%2520task%2520in%2520video%2520understanding.%2520Existing%2520Transformer-based%2520methods%2520often%2520suffer%2520from%2520redundant%2520attention%2520and%2520suboptimal%2520multi-modal%2520alignment.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MLVTG%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520two%2520key%2520modules%253A%2520MambaAligner%2520and%2520LLMRefiner.%2520MambaAligner%2520uses%2520stacked%2520Vision%2520Mamba%2520blocks%2520as%2520a%2520backbone%2520instead%2520of%2520Transformers%2520to%2520model%2520temporal%2520dependencies%2520and%2520extract%2520robust%2520video%2520representations%2520for%2520multi-modal%2520alignment.%2520LLMRefiner%2520leverages%2520the%2520specific%2520frozen%2520layer%2520of%2520a%2520pre-trained%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520implicitly%2520transfer%2520semantic%2520priors%252C%2520enhancing%2520multi-modal%2520alignment%2520without%2520fine-tuning.%2520This%2520dual%2520alignment%2520strategy%252C%2520temporal%2520modeling%2520via%2520structured%2520state-space%2520dynamics%2520and%2520semantic%2520purification%2520via%2520textual%2520priors%252C%2520enables%2520more%2520precise%2520localization.%2520Extensive%2520experiments%2520on%2520QVHighlights%252C%2520Charades-STA%252C%2520and%2520TVSum%2520demonstrate%2520that%2520MLVTG%2520achieves%2520state-of-the-art%2520performance%2520and%2520significantly%2520outperforms%2520existing%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLVTG%3A%20Mamba-Based%20Feature%20Alignment%20and%20LLM-Driven%20Purification%20for%20Multi-Modal%20Video%20Temporal%20Grounding&entry.906535625=Zhiyi%20Zhu%20and%20Xiaoyu%20Wu%20and%20Zihao%20Liu%20and%20Linlin%20Yang&entry.1292438233=Video%20Temporal%20Grounding%20%28VTG%29%2C%20which%20aims%20to%20localize%20video%20clips%20corresponding%20to%20natural%20language%20queries%2C%20is%20a%20fundamental%20yet%20challenging%20task%20in%20video%20understanding.%20Existing%20Transformer-based%20methods%20often%20suffer%20from%20redundant%20attention%20and%20suboptimal%20multi-modal%20alignment.%20To%20address%20these%20limitations%2C%20we%20propose%20MLVTG%2C%20a%20novel%20framework%20that%20integrates%20two%20key%20modules%3A%20MambaAligner%20and%20LLMRefiner.%20MambaAligner%20uses%20stacked%20Vision%20Mamba%20blocks%20as%20a%20backbone%20instead%20of%20Transformers%20to%20model%20temporal%20dependencies%20and%20extract%20robust%20video%20representations%20for%20multi-modal%20alignment.%20LLMRefiner%20leverages%20the%20specific%20frozen%20layer%20of%20a%20pre-trained%20Large%20Language%20Model%20%28LLM%29%20to%20implicitly%20transfer%20semantic%20priors%2C%20enhancing%20multi-modal%20alignment%20without%20fine-tuning.%20This%20dual%20alignment%20strategy%2C%20temporal%20modeling%20via%20structured%20state-space%20dynamics%20and%20semantic%20purification%20via%20textual%20priors%2C%20enables%20more%20precise%20localization.%20Extensive%20experiments%20on%20QVHighlights%2C%20Charades-STA%2C%20and%20TVSum%20demonstrate%20that%20MLVTG%20achieves%20state-of-the-art%20performance%20and%20significantly%20outperforms%20existing%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2506.08512v2&entry.124074799=Read"},
{"title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation", "author": "Hongji Liu and Linwei Zheng and Yongjian Li and Mingkai Tang and Xiaoyang Yan and Ming Liu and Jun Ma", "abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved.", "link": "http://arxiv.org/abs/2601.19536v1", "date": "2026-01-27", "relevancy": 2.2635, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6072}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5392}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Inverse%20Perspective%20Mapping%20for%20Automatic%20Vectorized%20Road%20Map%20Generation&body=Title%3A%20Enhancing%20Inverse%20Perspective%20Mapping%20for%20Automatic%20Vectorized%20Road%20Map%20Generation%0AAuthor%3A%20Hongji%20Liu%20and%20Linwei%20Zheng%20and%20Yongjian%20Li%20and%20Mingkai%20Tang%20and%20Xiaoyang%20Yan%20and%20Ming%20Liu%20and%20Jun%20Ma%0AAbstract%3A%20In%20this%20study%2C%20we%20present%20a%20low-cost%20and%20unified%20framework%20for%20vectorized%20road%20mapping%20leveraging%20enhanced%20inverse%20perspective%20mapping%20%28IPM%29.%20In%20this%20framework%2C%20Catmull-Rom%20splines%20are%20utilized%20to%20characterize%20lane%20lines%2C%20and%20all%20the%20other%20ground%20markings%20are%20depicted%20using%20polygons%20uniformly.%20The%20results%20from%20instance%20segmentation%20serve%20as%20references%20to%20refine%20the%20three-dimensional%20position%20of%20spline%20control%20points%20and%20polygon%20corner%20points.%20In%20conjunction%20with%20this%20process%2C%20the%20homography%20matrix%20of%20IPM%20and%20vehicle%20poses%20are%20optimized%20simultaneously.%20Our%20proposed%20framework%20significantly%20reduces%20the%20mapping%20errors%20associated%20with%20IPM.%20It%20also%20improves%20the%20accuracy%20of%20the%20initial%20IPM%20homography%20matrix%20and%20the%20predicted%20vehicle%20poses.%20Furthermore%2C%20it%20addresses%20the%20limitations%20imposed%20by%20the%20coplanarity%20assumption%20in%20IPM.%20These%20enhancements%20enable%20IPM%20to%20be%20effectively%20applied%20to%20vectorized%20road%20mapping%2C%20which%20serves%20a%20cost-effective%20solution%20with%20enhanced%20accuracy.%20In%20addition%2C%20our%20framework%20generalizes%20road%20map%20elements%20to%20include%20all%20common%20ground%20markings%20and%20lane%20lines.%20The%20proposed%20framework%20is%20evaluated%20in%20two%20different%20practical%20scenarios%2C%20and%20the%20test%20results%20show%20that%20our%20method%20can%20automatically%20generate%20high-precision%20maps%20with%20near-centimeter-level%20accuracy.%20Importantly%2C%20the%20optimized%20IPM%20matrix%20achieves%20an%20accuracy%20comparable%20to%20that%20of%20manual%20calibration%2C%20while%20the%20accuracy%20of%20vehicle%20poses%20is%20also%20significantly%20improved.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Inverse%2520Perspective%2520Mapping%2520for%2520Automatic%2520Vectorized%2520Road%2520Map%2520Generation%26entry.906535625%3DHongji%2520Liu%2520and%2520Linwei%2520Zheng%2520and%2520Yongjian%2520Li%2520and%2520Mingkai%2520Tang%2520and%2520Xiaoyang%2520Yan%2520and%2520Ming%2520Liu%2520and%2520Jun%2520Ma%26entry.1292438233%3DIn%2520this%2520study%252C%2520we%2520present%2520a%2520low-cost%2520and%2520unified%2520framework%2520for%2520vectorized%2520road%2520mapping%2520leveraging%2520enhanced%2520inverse%2520perspective%2520mapping%2520%2528IPM%2529.%2520In%2520this%2520framework%252C%2520Catmull-Rom%2520splines%2520are%2520utilized%2520to%2520characterize%2520lane%2520lines%252C%2520and%2520all%2520the%2520other%2520ground%2520markings%2520are%2520depicted%2520using%2520polygons%2520uniformly.%2520The%2520results%2520from%2520instance%2520segmentation%2520serve%2520as%2520references%2520to%2520refine%2520the%2520three-dimensional%2520position%2520of%2520spline%2520control%2520points%2520and%2520polygon%2520corner%2520points.%2520In%2520conjunction%2520with%2520this%2520process%252C%2520the%2520homography%2520matrix%2520of%2520IPM%2520and%2520vehicle%2520poses%2520are%2520optimized%2520simultaneously.%2520Our%2520proposed%2520framework%2520significantly%2520reduces%2520the%2520mapping%2520errors%2520associated%2520with%2520IPM.%2520It%2520also%2520improves%2520the%2520accuracy%2520of%2520the%2520initial%2520IPM%2520homography%2520matrix%2520and%2520the%2520predicted%2520vehicle%2520poses.%2520Furthermore%252C%2520it%2520addresses%2520the%2520limitations%2520imposed%2520by%2520the%2520coplanarity%2520assumption%2520in%2520IPM.%2520These%2520enhancements%2520enable%2520IPM%2520to%2520be%2520effectively%2520applied%2520to%2520vectorized%2520road%2520mapping%252C%2520which%2520serves%2520a%2520cost-effective%2520solution%2520with%2520enhanced%2520accuracy.%2520In%2520addition%252C%2520our%2520framework%2520generalizes%2520road%2520map%2520elements%2520to%2520include%2520all%2520common%2520ground%2520markings%2520and%2520lane%2520lines.%2520The%2520proposed%2520framework%2520is%2520evaluated%2520in%2520two%2520different%2520practical%2520scenarios%252C%2520and%2520the%2520test%2520results%2520show%2520that%2520our%2520method%2520can%2520automatically%2520generate%2520high-precision%2520maps%2520with%2520near-centimeter-level%2520accuracy.%2520Importantly%252C%2520the%2520optimized%2520IPM%2520matrix%2520achieves%2520an%2520accuracy%2520comparable%2520to%2520that%2520of%2520manual%2520calibration%252C%2520while%2520the%2520accuracy%2520of%2520vehicle%2520poses%2520is%2520also%2520significantly%2520improved.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Inverse%20Perspective%20Mapping%20for%20Automatic%20Vectorized%20Road%20Map%20Generation&entry.906535625=Hongji%20Liu%20and%20Linwei%20Zheng%20and%20Yongjian%20Li%20and%20Mingkai%20Tang%20and%20Xiaoyang%20Yan%20and%20Ming%20Liu%20and%20Jun%20Ma&entry.1292438233=In%20this%20study%2C%20we%20present%20a%20low-cost%20and%20unified%20framework%20for%20vectorized%20road%20mapping%20leveraging%20enhanced%20inverse%20perspective%20mapping%20%28IPM%29.%20In%20this%20framework%2C%20Catmull-Rom%20splines%20are%20utilized%20to%20characterize%20lane%20lines%2C%20and%20all%20the%20other%20ground%20markings%20are%20depicted%20using%20polygons%20uniformly.%20The%20results%20from%20instance%20segmentation%20serve%20as%20references%20to%20refine%20the%20three-dimensional%20position%20of%20spline%20control%20points%20and%20polygon%20corner%20points.%20In%20conjunction%20with%20this%20process%2C%20the%20homography%20matrix%20of%20IPM%20and%20vehicle%20poses%20are%20optimized%20simultaneously.%20Our%20proposed%20framework%20significantly%20reduces%20the%20mapping%20errors%20associated%20with%20IPM.%20It%20also%20improves%20the%20accuracy%20of%20the%20initial%20IPM%20homography%20matrix%20and%20the%20predicted%20vehicle%20poses.%20Furthermore%2C%20it%20addresses%20the%20limitations%20imposed%20by%20the%20coplanarity%20assumption%20in%20IPM.%20These%20enhancements%20enable%20IPM%20to%20be%20effectively%20applied%20to%20vectorized%20road%20mapping%2C%20which%20serves%20a%20cost-effective%20solution%20with%20enhanced%20accuracy.%20In%20addition%2C%20our%20framework%20generalizes%20road%20map%20elements%20to%20include%20all%20common%20ground%20markings%20and%20lane%20lines.%20The%20proposed%20framework%20is%20evaluated%20in%20two%20different%20practical%20scenarios%2C%20and%20the%20test%20results%20show%20that%20our%20method%20can%20automatically%20generate%20high-precision%20maps%20with%20near-centimeter-level%20accuracy.%20Importantly%2C%20the%20optimized%20IPM%20matrix%20achieves%20an%20accuracy%20comparable%20to%20that%20of%20manual%20calibration%2C%20while%20the%20accuracy%20of%20vehicle%20poses%20is%20also%20significantly%20improved.&entry.1838667208=http%3A//arxiv.org/abs/2601.19536v1&entry.124074799=Read"},
{"title": "Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods", "author": "Yida Lin and Bing Xue and Mengjie Zhang and Sam Schofield and Richard Green", "abstract": "Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments. We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms. All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \\times 1080$). Results reveal scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury), while iterative methods show variable cross-benchmark performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury; IGEV: 0.33 px on ETH3D but 4.99 px on Middlebury). Qualitative evaluation on the Tree Branches dataset establishes DEFOM as the gold-standard baseline for vegetation depth estimation, with superior cross-domain consistency (consistently ranking 1st-2nd across benchmarks, average rank 1.75). DEFOM predictions will serve as pseudo-ground-truth for future benchmarking.", "link": "http://arxiv.org/abs/2601.19461v1", "date": "2026-01-27", "relevancy": 2.2502, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Gold-Standard%20Depth%20Estimation%20for%20Tree%20Branches%20in%20UAV%20Forestry%3A%20Benchmarking%20Deep%20Stereo%20Matching%20Methods&body=Title%3A%20Towards%20Gold-Standard%20Depth%20Estimation%20for%20Tree%20Branches%20in%20UAV%20Forestry%3A%20Benchmarking%20Deep%20Stereo%20Matching%20Methods%0AAuthor%3A%20Yida%20Lin%20and%20Bing%20Xue%20and%20Mengjie%20Zhang%20and%20Sam%20Schofield%20and%20Richard%20Green%0AAbstract%3A%20Autonomous%20UAV%20forestry%20operations%20require%20robust%20depth%20estimation%20with%20strong%20cross-domain%20generalization%2C%20yet%20existing%20evaluations%20focus%20on%20urban%20and%20indoor%20scenarios%2C%20leaving%20a%20critical%20gap%20for%20vegetation-dense%20environments.%20We%20present%20the%20first%20systematic%20zero-shot%20evaluation%20of%20eight%20stereo%20methods%20spanning%20iterative%20refinement%2C%20foundation%20model%2C%20diffusion-based%2C%20and%203D%20CNN%20paradigms.%20All%20methods%20use%20officially%20released%20pretrained%20weights%20%28trained%20on%20Scene%20Flow%29%20and%20are%20evaluated%20on%20four%20standard%20benchmarks%20%28ETH3D%2C%20KITTI%202012/2015%2C%20Middlebury%29%20plus%20a%20novel%205%2C313-pair%20Canterbury%20Tree%20Branches%20dataset%20%28%241920%20%5Ctimes%201080%24%29.%20Results%20reveal%20scene-dependent%20patterns%3A%20foundation%20models%20excel%20on%20structured%20scenes%20%28BridgeDepth%3A%200.23%20px%20on%20ETH3D%3B%20DEFOM%3A%204.65%20px%20on%20Middlebury%29%2C%20while%20iterative%20methods%20show%20variable%20cross-benchmark%20performance%20%28IGEV%2B%2B%3A%200.36%20px%20on%20ETH3D%20but%206.77%20px%20on%20Middlebury%3B%20IGEV%3A%200.33%20px%20on%20ETH3D%20but%204.99%20px%20on%20Middlebury%29.%20Qualitative%20evaluation%20on%20the%20Tree%20Branches%20dataset%20establishes%20DEFOM%20as%20the%20gold-standard%20baseline%20for%20vegetation%20depth%20estimation%2C%20with%20superior%20cross-domain%20consistency%20%28consistently%20ranking%201st-2nd%20across%20benchmarks%2C%20average%20rank%201.75%29.%20DEFOM%20predictions%20will%20serve%20as%20pseudo-ground-truth%20for%20future%20benchmarking.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Gold-Standard%2520Depth%2520Estimation%2520for%2520Tree%2520Branches%2520in%2520UAV%2520Forestry%253A%2520Benchmarking%2520Deep%2520Stereo%2520Matching%2520Methods%26entry.906535625%3DYida%2520Lin%2520and%2520Bing%2520Xue%2520and%2520Mengjie%2520Zhang%2520and%2520Sam%2520Schofield%2520and%2520Richard%2520Green%26entry.1292438233%3DAutonomous%2520UAV%2520forestry%2520operations%2520require%2520robust%2520depth%2520estimation%2520with%2520strong%2520cross-domain%2520generalization%252C%2520yet%2520existing%2520evaluations%2520focus%2520on%2520urban%2520and%2520indoor%2520scenarios%252C%2520leaving%2520a%2520critical%2520gap%2520for%2520vegetation-dense%2520environments.%2520We%2520present%2520the%2520first%2520systematic%2520zero-shot%2520evaluation%2520of%2520eight%2520stereo%2520methods%2520spanning%2520iterative%2520refinement%252C%2520foundation%2520model%252C%2520diffusion-based%252C%2520and%25203D%2520CNN%2520paradigms.%2520All%2520methods%2520use%2520officially%2520released%2520pretrained%2520weights%2520%2528trained%2520on%2520Scene%2520Flow%2529%2520and%2520are%2520evaluated%2520on%2520four%2520standard%2520benchmarks%2520%2528ETH3D%252C%2520KITTI%25202012/2015%252C%2520Middlebury%2529%2520plus%2520a%2520novel%25205%252C313-pair%2520Canterbury%2520Tree%2520Branches%2520dataset%2520%2528%25241920%2520%255Ctimes%25201080%2524%2529.%2520Results%2520reveal%2520scene-dependent%2520patterns%253A%2520foundation%2520models%2520excel%2520on%2520structured%2520scenes%2520%2528BridgeDepth%253A%25200.23%2520px%2520on%2520ETH3D%253B%2520DEFOM%253A%25204.65%2520px%2520on%2520Middlebury%2529%252C%2520while%2520iterative%2520methods%2520show%2520variable%2520cross-benchmark%2520performance%2520%2528IGEV%252B%252B%253A%25200.36%2520px%2520on%2520ETH3D%2520but%25206.77%2520px%2520on%2520Middlebury%253B%2520IGEV%253A%25200.33%2520px%2520on%2520ETH3D%2520but%25204.99%2520px%2520on%2520Middlebury%2529.%2520Qualitative%2520evaluation%2520on%2520the%2520Tree%2520Branches%2520dataset%2520establishes%2520DEFOM%2520as%2520the%2520gold-standard%2520baseline%2520for%2520vegetation%2520depth%2520estimation%252C%2520with%2520superior%2520cross-domain%2520consistency%2520%2528consistently%2520ranking%25201st-2nd%2520across%2520benchmarks%252C%2520average%2520rank%25201.75%2529.%2520DEFOM%2520predictions%2520will%2520serve%2520as%2520pseudo-ground-truth%2520for%2520future%2520benchmarking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Gold-Standard%20Depth%20Estimation%20for%20Tree%20Branches%20in%20UAV%20Forestry%3A%20Benchmarking%20Deep%20Stereo%20Matching%20Methods&entry.906535625=Yida%20Lin%20and%20Bing%20Xue%20and%20Mengjie%20Zhang%20and%20Sam%20Schofield%20and%20Richard%20Green&entry.1292438233=Autonomous%20UAV%20forestry%20operations%20require%20robust%20depth%20estimation%20with%20strong%20cross-domain%20generalization%2C%20yet%20existing%20evaluations%20focus%20on%20urban%20and%20indoor%20scenarios%2C%20leaving%20a%20critical%20gap%20for%20vegetation-dense%20environments.%20We%20present%20the%20first%20systematic%20zero-shot%20evaluation%20of%20eight%20stereo%20methods%20spanning%20iterative%20refinement%2C%20foundation%20model%2C%20diffusion-based%2C%20and%203D%20CNN%20paradigms.%20All%20methods%20use%20officially%20released%20pretrained%20weights%20%28trained%20on%20Scene%20Flow%29%20and%20are%20evaluated%20on%20four%20standard%20benchmarks%20%28ETH3D%2C%20KITTI%202012/2015%2C%20Middlebury%29%20plus%20a%20novel%205%2C313-pair%20Canterbury%20Tree%20Branches%20dataset%20%28%241920%20%5Ctimes%201080%24%29.%20Results%20reveal%20scene-dependent%20patterns%3A%20foundation%20models%20excel%20on%20structured%20scenes%20%28BridgeDepth%3A%200.23%20px%20on%20ETH3D%3B%20DEFOM%3A%204.65%20px%20on%20Middlebury%29%2C%20while%20iterative%20methods%20show%20variable%20cross-benchmark%20performance%20%28IGEV%2B%2B%3A%200.36%20px%20on%20ETH3D%20but%206.77%20px%20on%20Middlebury%3B%20IGEV%3A%200.33%20px%20on%20ETH3D%20but%204.99%20px%20on%20Middlebury%29.%20Qualitative%20evaluation%20on%20the%20Tree%20Branches%20dataset%20establishes%20DEFOM%20as%20the%20gold-standard%20baseline%20for%20vegetation%20depth%20estimation%2C%20with%20superior%20cross-domain%20consistency%20%28consistently%20ranking%201st-2nd%20across%20benchmarks%2C%20average%20rank%201.75%29.%20DEFOM%20predictions%20will%20serve%20as%20pseudo-ground-truth%20for%20future%20benchmarking.&entry.1838667208=http%3A//arxiv.org/abs/2601.19461v1&entry.124074799=Read"},
{"title": "LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment", "author": "Haonan Zhang and Dongxia Wang and Yi Liu and Kexin Chen and Wenhai Wang", "abstract": "Safety-aligned LLMs suffer from two failure modes: jailbreak (answering harmful inputs) and over-refusal (declining benign queries). Existing vector steering methods adjust the magnitude of answer vectors, but this creates a fundamental trade-off -- reducing jailbreak increases over-refusal and vice versa. We identify the root cause: LLMs encode the decision to answer (answer vector $v_a$) and the judgment of input safety (benign vector $v_b$) as nearly orthogonal directions, treating them as independent processes. We propose LLM-VA, which aligns $v_a$ with $v_b$ through closed-form weight updates, making the model's willingness to answer causally dependent on its safety assessment -- without fine-tuning or architectural changes. Our method identifies vectors at each layer using SVMs, selects safety-relevant layers, and iteratively aligns vectors via minimum-norm weight modifications. Experiments on 12 LLMs demonstrate that LLM-VA achieves 11.45% higher F1 than the best baseline while preserving 95.92% utility, and automatically adapts to each model's safety bias without manual tuning. Code and models are available at https://hotbento.github.io/LLM-VA-Web/.", "link": "http://arxiv.org/abs/2601.19487v1", "date": "2026-01-27", "relevancy": 2.2498, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4524}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-VA%3A%20Resolving%20the%20Jailbreak-Overrefusal%20Trade-off%20via%20Vector%20Alignment&body=Title%3A%20LLM-VA%3A%20Resolving%20the%20Jailbreak-Overrefusal%20Trade-off%20via%20Vector%20Alignment%0AAuthor%3A%20Haonan%20Zhang%20and%20Dongxia%20Wang%20and%20Yi%20Liu%20and%20Kexin%20Chen%20and%20Wenhai%20Wang%0AAbstract%3A%20Safety-aligned%20LLMs%20suffer%20from%20two%20failure%20modes%3A%20jailbreak%20%28answering%20harmful%20inputs%29%20and%20over-refusal%20%28declining%20benign%20queries%29.%20Existing%20vector%20steering%20methods%20adjust%20the%20magnitude%20of%20answer%20vectors%2C%20but%20this%20creates%20a%20fundamental%20trade-off%20--%20reducing%20jailbreak%20increases%20over-refusal%20and%20vice%20versa.%20We%20identify%20the%20root%20cause%3A%20LLMs%20encode%20the%20decision%20to%20answer%20%28answer%20vector%20%24v_a%24%29%20and%20the%20judgment%20of%20input%20safety%20%28benign%20vector%20%24v_b%24%29%20as%20nearly%20orthogonal%20directions%2C%20treating%20them%20as%20independent%20processes.%20We%20propose%20LLM-VA%2C%20which%20aligns%20%24v_a%24%20with%20%24v_b%24%20through%20closed-form%20weight%20updates%2C%20making%20the%20model%27s%20willingness%20to%20answer%20causally%20dependent%20on%20its%20safety%20assessment%20--%20without%20fine-tuning%20or%20architectural%20changes.%20Our%20method%20identifies%20vectors%20at%20each%20layer%20using%20SVMs%2C%20selects%20safety-relevant%20layers%2C%20and%20iteratively%20aligns%20vectors%20via%20minimum-norm%20weight%20modifications.%20Experiments%20on%2012%20LLMs%20demonstrate%20that%20LLM-VA%20achieves%2011.45%25%20higher%20F1%20than%20the%20best%20baseline%20while%20preserving%2095.92%25%20utility%2C%20and%20automatically%20adapts%20to%20each%20model%27s%20safety%20bias%20without%20manual%20tuning.%20Code%20and%20models%20are%20available%20at%20https%3A//hotbento.github.io/LLM-VA-Web/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-VA%253A%2520Resolving%2520the%2520Jailbreak-Overrefusal%2520Trade-off%2520via%2520Vector%2520Alignment%26entry.906535625%3DHaonan%2520Zhang%2520and%2520Dongxia%2520Wang%2520and%2520Yi%2520Liu%2520and%2520Kexin%2520Chen%2520and%2520Wenhai%2520Wang%26entry.1292438233%3DSafety-aligned%2520LLMs%2520suffer%2520from%2520two%2520failure%2520modes%253A%2520jailbreak%2520%2528answering%2520harmful%2520inputs%2529%2520and%2520over-refusal%2520%2528declining%2520benign%2520queries%2529.%2520Existing%2520vector%2520steering%2520methods%2520adjust%2520the%2520magnitude%2520of%2520answer%2520vectors%252C%2520but%2520this%2520creates%2520a%2520fundamental%2520trade-off%2520--%2520reducing%2520jailbreak%2520increases%2520over-refusal%2520and%2520vice%2520versa.%2520We%2520identify%2520the%2520root%2520cause%253A%2520LLMs%2520encode%2520the%2520decision%2520to%2520answer%2520%2528answer%2520vector%2520%2524v_a%2524%2529%2520and%2520the%2520judgment%2520of%2520input%2520safety%2520%2528benign%2520vector%2520%2524v_b%2524%2529%2520as%2520nearly%2520orthogonal%2520directions%252C%2520treating%2520them%2520as%2520independent%2520processes.%2520We%2520propose%2520LLM-VA%252C%2520which%2520aligns%2520%2524v_a%2524%2520with%2520%2524v_b%2524%2520through%2520closed-form%2520weight%2520updates%252C%2520making%2520the%2520model%2527s%2520willingness%2520to%2520answer%2520causally%2520dependent%2520on%2520its%2520safety%2520assessment%2520--%2520without%2520fine-tuning%2520or%2520architectural%2520changes.%2520Our%2520method%2520identifies%2520vectors%2520at%2520each%2520layer%2520using%2520SVMs%252C%2520selects%2520safety-relevant%2520layers%252C%2520and%2520iteratively%2520aligns%2520vectors%2520via%2520minimum-norm%2520weight%2520modifications.%2520Experiments%2520on%252012%2520LLMs%2520demonstrate%2520that%2520LLM-VA%2520achieves%252011.45%2525%2520higher%2520F1%2520than%2520the%2520best%2520baseline%2520while%2520preserving%252095.92%2525%2520utility%252C%2520and%2520automatically%2520adapts%2520to%2520each%2520model%2527s%2520safety%2520bias%2520without%2520manual%2520tuning.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//hotbento.github.io/LLM-VA-Web/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-VA%3A%20Resolving%20the%20Jailbreak-Overrefusal%20Trade-off%20via%20Vector%20Alignment&entry.906535625=Haonan%20Zhang%20and%20Dongxia%20Wang%20and%20Yi%20Liu%20and%20Kexin%20Chen%20and%20Wenhai%20Wang&entry.1292438233=Safety-aligned%20LLMs%20suffer%20from%20two%20failure%20modes%3A%20jailbreak%20%28answering%20harmful%20inputs%29%20and%20over-refusal%20%28declining%20benign%20queries%29.%20Existing%20vector%20steering%20methods%20adjust%20the%20magnitude%20of%20answer%20vectors%2C%20but%20this%20creates%20a%20fundamental%20trade-off%20--%20reducing%20jailbreak%20increases%20over-refusal%20and%20vice%20versa.%20We%20identify%20the%20root%20cause%3A%20LLMs%20encode%20the%20decision%20to%20answer%20%28answer%20vector%20%24v_a%24%29%20and%20the%20judgment%20of%20input%20safety%20%28benign%20vector%20%24v_b%24%29%20as%20nearly%20orthogonal%20directions%2C%20treating%20them%20as%20independent%20processes.%20We%20propose%20LLM-VA%2C%20which%20aligns%20%24v_a%24%20with%20%24v_b%24%20through%20closed-form%20weight%20updates%2C%20making%20the%20model%27s%20willingness%20to%20answer%20causally%20dependent%20on%20its%20safety%20assessment%20--%20without%20fine-tuning%20or%20architectural%20changes.%20Our%20method%20identifies%20vectors%20at%20each%20layer%20using%20SVMs%2C%20selects%20safety-relevant%20layers%2C%20and%20iteratively%20aligns%20vectors%20via%20minimum-norm%20weight%20modifications.%20Experiments%20on%2012%20LLMs%20demonstrate%20that%20LLM-VA%20achieves%2011.45%25%20higher%20F1%20than%20the%20best%20baseline%20while%20preserving%2095.92%25%20utility%2C%20and%20automatically%20adapts%20to%20each%20model%27s%20safety%20bias%20without%20manual%20tuning.%20Code%20and%20models%20are%20available%20at%20https%3A//hotbento.github.io/LLM-VA-Web/.&entry.1838667208=http%3A//arxiv.org/abs/2601.19487v1&entry.124074799=Read"},
{"title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning", "author": "Xinyuan Song and Keyu Wang and PengXiang Li and Lu Yin and Shiwei Liu", "abstract": "Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.", "link": "http://arxiv.org/abs/2510.02091v4", "date": "2026-01-27", "relevancy": 2.248, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20the%20Roles%20of%20LLM%20Layers%20in%20Retrieval%2C%20Knowledge%2C%20and%20Reasoning&body=Title%3A%20Demystifying%20the%20Roles%20of%20LLM%20Layers%20in%20Retrieval%2C%20Knowledge%2C%20and%20Reasoning%0AAuthor%3A%20Xinyuan%20Song%20and%20Keyu%20Wang%20and%20PengXiang%20Li%20and%20Lu%20Yin%20and%20Shiwei%20Liu%0AAbstract%3A%20Recent%20studies%20suggest%20that%20the%20deeper%20layers%20of%20Large%20Language%20Models%20%28LLMs%29%20contribute%20little%20to%20representation%20learning%20and%20can%20often%20be%20removed%20without%20significant%20performance%20loss.%20However%2C%20such%20claims%20are%20typically%20drawn%20from%20narrow%20evaluations%20and%20may%20overlook%20important%20aspects%20of%20model%20behavior.%20In%20this%20work%2C%20we%20present%20a%20systematic%20study%20of%20depth%20utilization%20across%20diverse%20dimensions%2C%20including%20evaluation%20protocols%2C%20task%20categories%2C%20and%20model%20architectures.%20Our%20analysis%20confirms%20that%20very%20deep%20layers%20are%20generally%20less%20effective%20than%20earlier%20ones%2C%20but%20their%20contributions%20vary%20substantially%20with%20the%20evaluation%20setting.%20Under%20likelihood-based%20metrics%20without%20generation%2C%20pruning%20most%20layers%20preserves%20performance%2C%20with%20only%20the%20initial%20few%20being%20critical.%20By%20contrast%2C%20generation-based%20evaluation%20uncovers%20indispensable%20roles%20for%20middle%20and%20deeper%20layers%20in%20enabling%20reasoning%20and%20maintaining%20long-range%20coherence.%20We%20further%20find%20that%20knowledge%20and%20retrieval%20are%20concentrated%20in%20shallow%20components%2C%20whereas%20reasoning%20accuracy%20relies%20heavily%20on%20deeper%20layers%20--%20yet%20can%20be%20reshaped%20through%20distillation.%20These%20results%20highlight%20that%20depth%20usage%20in%20LLMs%20is%20highly%20heterogeneous%20and%20context-dependent%2C%20underscoring%20the%20need%20for%20task-%2C%20metric-%2C%20and%20model-aware%20perspectives%20in%20both%20interpreting%20and%20compressing%20large%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02091v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520the%2520Roles%2520of%2520LLM%2520Layers%2520in%2520Retrieval%252C%2520Knowledge%252C%2520and%2520Reasoning%26entry.906535625%3DXinyuan%2520Song%2520and%2520Keyu%2520Wang%2520and%2520PengXiang%2520Li%2520and%2520Lu%2520Yin%2520and%2520Shiwei%2520Liu%26entry.1292438233%3DRecent%2520studies%2520suggest%2520that%2520the%2520deeper%2520layers%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520contribute%2520little%2520to%2520representation%2520learning%2520and%2520can%2520often%2520be%2520removed%2520without%2520significant%2520performance%2520loss.%2520However%252C%2520such%2520claims%2520are%2520typically%2520drawn%2520from%2520narrow%2520evaluations%2520and%2520may%2520overlook%2520important%2520aspects%2520of%2520model%2520behavior.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520systematic%2520study%2520of%2520depth%2520utilization%2520across%2520diverse%2520dimensions%252C%2520including%2520evaluation%2520protocols%252C%2520task%2520categories%252C%2520and%2520model%2520architectures.%2520Our%2520analysis%2520confirms%2520that%2520very%2520deep%2520layers%2520are%2520generally%2520less%2520effective%2520than%2520earlier%2520ones%252C%2520but%2520their%2520contributions%2520vary%2520substantially%2520with%2520the%2520evaluation%2520setting.%2520Under%2520likelihood-based%2520metrics%2520without%2520generation%252C%2520pruning%2520most%2520layers%2520preserves%2520performance%252C%2520with%2520only%2520the%2520initial%2520few%2520being%2520critical.%2520By%2520contrast%252C%2520generation-based%2520evaluation%2520uncovers%2520indispensable%2520roles%2520for%2520middle%2520and%2520deeper%2520layers%2520in%2520enabling%2520reasoning%2520and%2520maintaining%2520long-range%2520coherence.%2520We%2520further%2520find%2520that%2520knowledge%2520and%2520retrieval%2520are%2520concentrated%2520in%2520shallow%2520components%252C%2520whereas%2520reasoning%2520accuracy%2520relies%2520heavily%2520on%2520deeper%2520layers%2520--%2520yet%2520can%2520be%2520reshaped%2520through%2520distillation.%2520These%2520results%2520highlight%2520that%2520depth%2520usage%2520in%2520LLMs%2520is%2520highly%2520heterogeneous%2520and%2520context-dependent%252C%2520underscoring%2520the%2520need%2520for%2520task-%252C%2520metric-%252C%2520and%2520model-aware%2520perspectives%2520in%2520both%2520interpreting%2520and%2520compressing%2520large%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02091v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20the%20Roles%20of%20LLM%20Layers%20in%20Retrieval%2C%20Knowledge%2C%20and%20Reasoning&entry.906535625=Xinyuan%20Song%20and%20Keyu%20Wang%20and%20PengXiang%20Li%20and%20Lu%20Yin%20and%20Shiwei%20Liu&entry.1292438233=Recent%20studies%20suggest%20that%20the%20deeper%20layers%20of%20Large%20Language%20Models%20%28LLMs%29%20contribute%20little%20to%20representation%20learning%20and%20can%20often%20be%20removed%20without%20significant%20performance%20loss.%20However%2C%20such%20claims%20are%20typically%20drawn%20from%20narrow%20evaluations%20and%20may%20overlook%20important%20aspects%20of%20model%20behavior.%20In%20this%20work%2C%20we%20present%20a%20systematic%20study%20of%20depth%20utilization%20across%20diverse%20dimensions%2C%20including%20evaluation%20protocols%2C%20task%20categories%2C%20and%20model%20architectures.%20Our%20analysis%20confirms%20that%20very%20deep%20layers%20are%20generally%20less%20effective%20than%20earlier%20ones%2C%20but%20their%20contributions%20vary%20substantially%20with%20the%20evaluation%20setting.%20Under%20likelihood-based%20metrics%20without%20generation%2C%20pruning%20most%20layers%20preserves%20performance%2C%20with%20only%20the%20initial%20few%20being%20critical.%20By%20contrast%2C%20generation-based%20evaluation%20uncovers%20indispensable%20roles%20for%20middle%20and%20deeper%20layers%20in%20enabling%20reasoning%20and%20maintaining%20long-range%20coherence.%20We%20further%20find%20that%20knowledge%20and%20retrieval%20are%20concentrated%20in%20shallow%20components%2C%20whereas%20reasoning%20accuracy%20relies%20heavily%20on%20deeper%20layers%20--%20yet%20can%20be%20reshaped%20through%20distillation.%20These%20results%20highlight%20that%20depth%20usage%20in%20LLMs%20is%20highly%20heterogeneous%20and%20context-dependent%2C%20underscoring%20the%20need%20for%20task-%2C%20metric-%2C%20and%20model-aware%20perspectives%20in%20both%20interpreting%20and%20compressing%20large%20models.&entry.1838667208=http%3A//arxiv.org/abs/2510.02091v4&entry.124074799=Read"},
{"title": "DND: Boosting Large Language Models with Dynamic Nested Depth", "author": "Tieyuan Chen and Xiaodong Chen and Haoxing Chen and Zhenzhong Lan and Weiyao Lin and Jianguo Li", "abstract": "We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing\" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.", "link": "http://arxiv.org/abs/2510.11001v3", "date": "2026-01-27", "relevancy": 2.24, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6148}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.523}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DND%3A%20Boosting%20Large%20Language%20Models%20with%20Dynamic%20Nested%20Depth&body=Title%3A%20DND%3A%20Boosting%20Large%20Language%20Models%20with%20Dynamic%20Nested%20Depth%0AAuthor%3A%20Tieyuan%20Chen%20and%20Xiaodong%20Chen%20and%20Haoxing%20Chen%20and%20Zhenzhong%20Lan%20and%20Weiyao%20Lin%20and%20Jianguo%20Li%0AAbstract%3A%20We%20introduce%20Dynamic%20Nested%20Depth%20%28DND%29%2C%20a%20novel%20method%20that%20improves%20performance%20for%20off-the-shelf%20LLMs%20by%20selecting%20critical%20tokens%20to%20reprocess%20in%20a%20nested%20depth%20manner.%20Specifically%2C%20at%20the%20end%20of%20the%20given%20transformer%20layer%2C%20DND%20identifies%20more%20critical%20tokens%20with%20a%20router%20and%20feeds%20them%20back%20for%20an%20extra%20round%20of%20processing%2C%20effectively%20%60%60reviewing%22%20difficult%20tokens%20while%20avoiding%20redundant%20computation%20for%20easier%20ones.%20The%20dynamic%20selection%20mechanism%20is%20tailored%20for%20precise%20control%20via%20two%20novel%20strategies%3A%20a%20router%20controlling%20loss%20to%20enhance%20token%20selection%20distinguishability%2C%20and%20a%20threshold%20control%20scheme%20to%20ensure%20selection%20stability.%20We%20demonstrate%20the%20effectiveness%20of%20DND%20by%20directly%20integrating%20it%20into%20pre-trained%20dense%20and%20MoE%20models%20during%20a%20post-training%20phase.%20On%20diverse%20benchmarks%2C%20this%20approach%20boosts%20the%20performances%20of%20the%20dense%20Qwen3-1.7B%20by%201.88%25%20and%20the%20MoE%20Qwen3-30B-A3B%20by%200.87%25%2C%20all%20with%20a%20minimal%20parameter%20and%20computing%20increase.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11001v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDND%253A%2520Boosting%2520Large%2520Language%2520Models%2520with%2520Dynamic%2520Nested%2520Depth%26entry.906535625%3DTieyuan%2520Chen%2520and%2520Xiaodong%2520Chen%2520and%2520Haoxing%2520Chen%2520and%2520Zhenzhong%2520Lan%2520and%2520Weiyao%2520Lin%2520and%2520Jianguo%2520Li%26entry.1292438233%3DWe%2520introduce%2520Dynamic%2520Nested%2520Depth%2520%2528DND%2529%252C%2520a%2520novel%2520method%2520that%2520improves%2520performance%2520for%2520off-the-shelf%2520LLMs%2520by%2520selecting%2520critical%2520tokens%2520to%2520reprocess%2520in%2520a%2520nested%2520depth%2520manner.%2520Specifically%252C%2520at%2520the%2520end%2520of%2520the%2520given%2520transformer%2520layer%252C%2520DND%2520identifies%2520more%2520critical%2520tokens%2520with%2520a%2520router%2520and%2520feeds%2520them%2520back%2520for%2520an%2520extra%2520round%2520of%2520processing%252C%2520effectively%2520%2560%2560reviewing%2522%2520difficult%2520tokens%2520while%2520avoiding%2520redundant%2520computation%2520for%2520easier%2520ones.%2520The%2520dynamic%2520selection%2520mechanism%2520is%2520tailored%2520for%2520precise%2520control%2520via%2520two%2520novel%2520strategies%253A%2520a%2520router%2520controlling%2520loss%2520to%2520enhance%2520token%2520selection%2520distinguishability%252C%2520and%2520a%2520threshold%2520control%2520scheme%2520to%2520ensure%2520selection%2520stability.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520DND%2520by%2520directly%2520integrating%2520it%2520into%2520pre-trained%2520dense%2520and%2520MoE%2520models%2520during%2520a%2520post-training%2520phase.%2520On%2520diverse%2520benchmarks%252C%2520this%2520approach%2520boosts%2520the%2520performances%2520of%2520the%2520dense%2520Qwen3-1.7B%2520by%25201.88%2525%2520and%2520the%2520MoE%2520Qwen3-30B-A3B%2520by%25200.87%2525%252C%2520all%2520with%2520a%2520minimal%2520parameter%2520and%2520computing%2520increase.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11001v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DND%3A%20Boosting%20Large%20Language%20Models%20with%20Dynamic%20Nested%20Depth&entry.906535625=Tieyuan%20Chen%20and%20Xiaodong%20Chen%20and%20Haoxing%20Chen%20and%20Zhenzhong%20Lan%20and%20Weiyao%20Lin%20and%20Jianguo%20Li&entry.1292438233=We%20introduce%20Dynamic%20Nested%20Depth%20%28DND%29%2C%20a%20novel%20method%20that%20improves%20performance%20for%20off-the-shelf%20LLMs%20by%20selecting%20critical%20tokens%20to%20reprocess%20in%20a%20nested%20depth%20manner.%20Specifically%2C%20at%20the%20end%20of%20the%20given%20transformer%20layer%2C%20DND%20identifies%20more%20critical%20tokens%20with%20a%20router%20and%20feeds%20them%20back%20for%20an%20extra%20round%20of%20processing%2C%20effectively%20%60%60reviewing%22%20difficult%20tokens%20while%20avoiding%20redundant%20computation%20for%20easier%20ones.%20The%20dynamic%20selection%20mechanism%20is%20tailored%20for%20precise%20control%20via%20two%20novel%20strategies%3A%20a%20router%20controlling%20loss%20to%20enhance%20token%20selection%20distinguishability%2C%20and%20a%20threshold%20control%20scheme%20to%20ensure%20selection%20stability.%20We%20demonstrate%20the%20effectiveness%20of%20DND%20by%20directly%20integrating%20it%20into%20pre-trained%20dense%20and%20MoE%20models%20during%20a%20post-training%20phase.%20On%20diverse%20benchmarks%2C%20this%20approach%20boosts%20the%20performances%20of%20the%20dense%20Qwen3-1.7B%20by%201.88%25%20and%20the%20MoE%20Qwen3-30B-A3B%20by%200.87%25%2C%20all%20with%20a%20minimal%20parameter%20and%20computing%20increase.&entry.1838667208=http%3A//arxiv.org/abs/2510.11001v3&entry.124074799=Read"},
{"title": "MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation", "author": "Ronglai Zuo and Rolandos Alexandros Potamias and Qi Sun and Evangelos Ververas and Jiankang Deng and Stefanos Zafeiriou", "abstract": "Sign language generation (SLG) aims to translate written texts into expressive sign motions, bridging communication barriers for the Deaf and Hard-of-Hearing communities. Recent studies formulate SLG within the language modeling framework using autoregressive language models, which suffer from unidirectional context modeling and slow token-by-token inference. To address these limitations, we present MaDiS, a masked-diffusion-based language model for SLG that captures bidirectional dependencies and supports efficient parallel multi-token generation. We further introduce a tri-level cross-modal pretraining scheme that jointly learns from token-, latent-, and 3D physical-space objectives, leading to richer and more grounded sign representations. To accelerate model convergence in the fine-tuning stage, we design a novel unmasking strategy with temporal checkpoints, reducing the combinatorial complexity of unmasking orders by over $10^{41}$ times. In addition, a mixture-of-parts embedding layer is developed to effectively fuse information stored in different part-wise sign tokens through learnable gates and well-optimized codebooks. Extensive experiments on CSL-Daily, Phoenix-2014T, and How2Sign demonstrate that MaDiS achieves superior performance across multiple metrics, including DTW error and two newly introduced metrics, SiBLEU and SiCLIP, while reducing inference latency by nearly 30%. Code and models will be released on our project page.", "link": "http://arxiv.org/abs/2601.19577v1", "date": "2026-01-27", "relevancy": 2.2399, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5738}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5619}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaDiS%3A%20Taming%20Masked%20Diffusion%20Language%20Models%20for%20Sign%20Language%20Generation&body=Title%3A%20MaDiS%3A%20Taming%20Masked%20Diffusion%20Language%20Models%20for%20Sign%20Language%20Generation%0AAuthor%3A%20Ronglai%20Zuo%20and%20Rolandos%20Alexandros%20Potamias%20and%20Qi%20Sun%20and%20Evangelos%20Ververas%20and%20Jiankang%20Deng%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20Sign%20language%20generation%20%28SLG%29%20aims%20to%20translate%20written%20texts%20into%20expressive%20sign%20motions%2C%20bridging%20communication%20barriers%20for%20the%20Deaf%20and%20Hard-of-Hearing%20communities.%20Recent%20studies%20formulate%20SLG%20within%20the%20language%20modeling%20framework%20using%20autoregressive%20language%20models%2C%20which%20suffer%20from%20unidirectional%20context%20modeling%20and%20slow%20token-by-token%20inference.%20To%20address%20these%20limitations%2C%20we%20present%20MaDiS%2C%20a%20masked-diffusion-based%20language%20model%20for%20SLG%20that%20captures%20bidirectional%20dependencies%20and%20supports%20efficient%20parallel%20multi-token%20generation.%20We%20further%20introduce%20a%20tri-level%20cross-modal%20pretraining%20scheme%20that%20jointly%20learns%20from%20token-%2C%20latent-%2C%20and%203D%20physical-space%20objectives%2C%20leading%20to%20richer%20and%20more%20grounded%20sign%20representations.%20To%20accelerate%20model%20convergence%20in%20the%20fine-tuning%20stage%2C%20we%20design%20a%20novel%20unmasking%20strategy%20with%20temporal%20checkpoints%2C%20reducing%20the%20combinatorial%20complexity%20of%20unmasking%20orders%20by%20over%20%2410%5E%7B41%7D%24%20times.%20In%20addition%2C%20a%20mixture-of-parts%20embedding%20layer%20is%20developed%20to%20effectively%20fuse%20information%20stored%20in%20different%20part-wise%20sign%20tokens%20through%20learnable%20gates%20and%20well-optimized%20codebooks.%20Extensive%20experiments%20on%20CSL-Daily%2C%20Phoenix-2014T%2C%20and%20How2Sign%20demonstrate%20that%20MaDiS%20achieves%20superior%20performance%20across%20multiple%20metrics%2C%20including%20DTW%20error%20and%20two%20newly%20introduced%20metrics%2C%20SiBLEU%20and%20SiCLIP%2C%20while%20reducing%20inference%20latency%20by%20nearly%2030%25.%20Code%20and%20models%20will%20be%20released%20on%20our%20project%20page.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaDiS%253A%2520Taming%2520Masked%2520Diffusion%2520Language%2520Models%2520for%2520Sign%2520Language%2520Generation%26entry.906535625%3DRonglai%2520Zuo%2520and%2520Rolandos%2520Alexandros%2520Potamias%2520and%2520Qi%2520Sun%2520and%2520Evangelos%2520Ververas%2520and%2520Jiankang%2520Deng%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3DSign%2520language%2520generation%2520%2528SLG%2529%2520aims%2520to%2520translate%2520written%2520texts%2520into%2520expressive%2520sign%2520motions%252C%2520bridging%2520communication%2520barriers%2520for%2520the%2520Deaf%2520and%2520Hard-of-Hearing%2520communities.%2520Recent%2520studies%2520formulate%2520SLG%2520within%2520the%2520language%2520modeling%2520framework%2520using%2520autoregressive%2520language%2520models%252C%2520which%2520suffer%2520from%2520unidirectional%2520context%2520modeling%2520and%2520slow%2520token-by-token%2520inference.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520MaDiS%252C%2520a%2520masked-diffusion-based%2520language%2520model%2520for%2520SLG%2520that%2520captures%2520bidirectional%2520dependencies%2520and%2520supports%2520efficient%2520parallel%2520multi-token%2520generation.%2520We%2520further%2520introduce%2520a%2520tri-level%2520cross-modal%2520pretraining%2520scheme%2520that%2520jointly%2520learns%2520from%2520token-%252C%2520latent-%252C%2520and%25203D%2520physical-space%2520objectives%252C%2520leading%2520to%2520richer%2520and%2520more%2520grounded%2520sign%2520representations.%2520To%2520accelerate%2520model%2520convergence%2520in%2520the%2520fine-tuning%2520stage%252C%2520we%2520design%2520a%2520novel%2520unmasking%2520strategy%2520with%2520temporal%2520checkpoints%252C%2520reducing%2520the%2520combinatorial%2520complexity%2520of%2520unmasking%2520orders%2520by%2520over%2520%252410%255E%257B41%257D%2524%2520times.%2520In%2520addition%252C%2520a%2520mixture-of-parts%2520embedding%2520layer%2520is%2520developed%2520to%2520effectively%2520fuse%2520information%2520stored%2520in%2520different%2520part-wise%2520sign%2520tokens%2520through%2520learnable%2520gates%2520and%2520well-optimized%2520codebooks.%2520Extensive%2520experiments%2520on%2520CSL-Daily%252C%2520Phoenix-2014T%252C%2520and%2520How2Sign%2520demonstrate%2520that%2520MaDiS%2520achieves%2520superior%2520performance%2520across%2520multiple%2520metrics%252C%2520including%2520DTW%2520error%2520and%2520two%2520newly%2520introduced%2520metrics%252C%2520SiBLEU%2520and%2520SiCLIP%252C%2520while%2520reducing%2520inference%2520latency%2520by%2520nearly%252030%2525.%2520Code%2520and%2520models%2520will%2520be%2520released%2520on%2520our%2520project%2520page.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaDiS%3A%20Taming%20Masked%20Diffusion%20Language%20Models%20for%20Sign%20Language%20Generation&entry.906535625=Ronglai%20Zuo%20and%20Rolandos%20Alexandros%20Potamias%20and%20Qi%20Sun%20and%20Evangelos%20Ververas%20and%20Jiankang%20Deng%20and%20Stefanos%20Zafeiriou&entry.1292438233=Sign%20language%20generation%20%28SLG%29%20aims%20to%20translate%20written%20texts%20into%20expressive%20sign%20motions%2C%20bridging%20communication%20barriers%20for%20the%20Deaf%20and%20Hard-of-Hearing%20communities.%20Recent%20studies%20formulate%20SLG%20within%20the%20language%20modeling%20framework%20using%20autoregressive%20language%20models%2C%20which%20suffer%20from%20unidirectional%20context%20modeling%20and%20slow%20token-by-token%20inference.%20To%20address%20these%20limitations%2C%20we%20present%20MaDiS%2C%20a%20masked-diffusion-based%20language%20model%20for%20SLG%20that%20captures%20bidirectional%20dependencies%20and%20supports%20efficient%20parallel%20multi-token%20generation.%20We%20further%20introduce%20a%20tri-level%20cross-modal%20pretraining%20scheme%20that%20jointly%20learns%20from%20token-%2C%20latent-%2C%20and%203D%20physical-space%20objectives%2C%20leading%20to%20richer%20and%20more%20grounded%20sign%20representations.%20To%20accelerate%20model%20convergence%20in%20the%20fine-tuning%20stage%2C%20we%20design%20a%20novel%20unmasking%20strategy%20with%20temporal%20checkpoints%2C%20reducing%20the%20combinatorial%20complexity%20of%20unmasking%20orders%20by%20over%20%2410%5E%7B41%7D%24%20times.%20In%20addition%2C%20a%20mixture-of-parts%20embedding%20layer%20is%20developed%20to%20effectively%20fuse%20information%20stored%20in%20different%20part-wise%20sign%20tokens%20through%20learnable%20gates%20and%20well-optimized%20codebooks.%20Extensive%20experiments%20on%20CSL-Daily%2C%20Phoenix-2014T%2C%20and%20How2Sign%20demonstrate%20that%20MaDiS%20achieves%20superior%20performance%20across%20multiple%20metrics%2C%20including%20DTW%20error%20and%20two%20newly%20introduced%20metrics%2C%20SiBLEU%20and%20SiCLIP%2C%20while%20reducing%20inference%20latency%20by%20nearly%2030%25.%20Code%20and%20models%20will%20be%20released%20on%20our%20project%20page.&entry.1838667208=http%3A//arxiv.org/abs/2601.19577v1&entry.124074799=Read"},
{"title": "PALM: Enhanced Generalizability for Local Visuomotor Policies via Perception Alignment", "author": "Ruiyu Wang and Zheyu Zhuang and Danica Kragic and Florian T. Pokorny", "abstract": "Generalizing beyond the training domain in image-based behavior cloning remains challenging. Existing methods address individual axes of generalization, workspace shifts, viewpoint changes, and cross-embodiment transfer, yet they are typically developed in isolation and often rely on complex pipelines. We introduce PALM (Perception Alignment for Local Manipulation), which leverages the invariance of local action distributions between out-of-distribution (OOD) and demonstrated domains to address these OOD shifts concurrently, without additional input modalities, model changes, or data collection. PALM modularizes the manipulation policy into coarse global components and a local policy for fine-grained actions. We reduce the discrepancy between in-domain and OOD inputs at the local policy level by enforcing local visual focus and consistent proprioceptive representation, allowing the policy to retrieve invariant local actions under OOD conditions. Experiments show that PALM limits OOD performance drops to 8% in simulation and 24% in the real world, compared to 45% and 77% for baselines.", "link": "http://arxiv.org/abs/2601.19514v1", "date": "2026-01-27", "relevancy": 2.2208, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5748}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5438}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PALM%3A%20Enhanced%20Generalizability%20for%20Local%20Visuomotor%20Policies%20via%20Perception%20Alignment&body=Title%3A%20PALM%3A%20Enhanced%20Generalizability%20for%20Local%20Visuomotor%20Policies%20via%20Perception%20Alignment%0AAuthor%3A%20Ruiyu%20Wang%20and%20Zheyu%20Zhuang%20and%20Danica%20Kragic%20and%20Florian%20T.%20Pokorny%0AAbstract%3A%20Generalizing%20beyond%20the%20training%20domain%20in%20image-based%20behavior%20cloning%20remains%20challenging.%20Existing%20methods%20address%20individual%20axes%20of%20generalization%2C%20workspace%20shifts%2C%20viewpoint%20changes%2C%20and%20cross-embodiment%20transfer%2C%20yet%20they%20are%20typically%20developed%20in%20isolation%20and%20often%20rely%20on%20complex%20pipelines.%20We%20introduce%20PALM%20%28Perception%20Alignment%20for%20Local%20Manipulation%29%2C%20which%20leverages%20the%20invariance%20of%20local%20action%20distributions%20between%20out-of-distribution%20%28OOD%29%20and%20demonstrated%20domains%20to%20address%20these%20OOD%20shifts%20concurrently%2C%20without%20additional%20input%20modalities%2C%20model%20changes%2C%20or%20data%20collection.%20PALM%20modularizes%20the%20manipulation%20policy%20into%20coarse%20global%20components%20and%20a%20local%20policy%20for%20fine-grained%20actions.%20We%20reduce%20the%20discrepancy%20between%20in-domain%20and%20OOD%20inputs%20at%20the%20local%20policy%20level%20by%20enforcing%20local%20visual%20focus%20and%20consistent%20proprioceptive%20representation%2C%20allowing%20the%20policy%20to%20retrieve%20invariant%20local%20actions%20under%20OOD%20conditions.%20Experiments%20show%20that%20PALM%20limits%20OOD%20performance%20drops%20to%208%25%20in%20simulation%20and%2024%25%20in%20the%20real%20world%2C%20compared%20to%2045%25%20and%2077%25%20for%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPALM%253A%2520Enhanced%2520Generalizability%2520for%2520Local%2520Visuomotor%2520Policies%2520via%2520Perception%2520Alignment%26entry.906535625%3DRuiyu%2520Wang%2520and%2520Zheyu%2520Zhuang%2520and%2520Danica%2520Kragic%2520and%2520Florian%2520T.%2520Pokorny%26entry.1292438233%3DGeneralizing%2520beyond%2520the%2520training%2520domain%2520in%2520image-based%2520behavior%2520cloning%2520remains%2520challenging.%2520Existing%2520methods%2520address%2520individual%2520axes%2520of%2520generalization%252C%2520workspace%2520shifts%252C%2520viewpoint%2520changes%252C%2520and%2520cross-embodiment%2520transfer%252C%2520yet%2520they%2520are%2520typically%2520developed%2520in%2520isolation%2520and%2520often%2520rely%2520on%2520complex%2520pipelines.%2520We%2520introduce%2520PALM%2520%2528Perception%2520Alignment%2520for%2520Local%2520Manipulation%2529%252C%2520which%2520leverages%2520the%2520invariance%2520of%2520local%2520action%2520distributions%2520between%2520out-of-distribution%2520%2528OOD%2529%2520and%2520demonstrated%2520domains%2520to%2520address%2520these%2520OOD%2520shifts%2520concurrently%252C%2520without%2520additional%2520input%2520modalities%252C%2520model%2520changes%252C%2520or%2520data%2520collection.%2520PALM%2520modularizes%2520the%2520manipulation%2520policy%2520into%2520coarse%2520global%2520components%2520and%2520a%2520local%2520policy%2520for%2520fine-grained%2520actions.%2520We%2520reduce%2520the%2520discrepancy%2520between%2520in-domain%2520and%2520OOD%2520inputs%2520at%2520the%2520local%2520policy%2520level%2520by%2520enforcing%2520local%2520visual%2520focus%2520and%2520consistent%2520proprioceptive%2520representation%252C%2520allowing%2520the%2520policy%2520to%2520retrieve%2520invariant%2520local%2520actions%2520under%2520OOD%2520conditions.%2520Experiments%2520show%2520that%2520PALM%2520limits%2520OOD%2520performance%2520drops%2520to%25208%2525%2520in%2520simulation%2520and%252024%2525%2520in%2520the%2520real%2520world%252C%2520compared%2520to%252045%2525%2520and%252077%2525%2520for%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PALM%3A%20Enhanced%20Generalizability%20for%20Local%20Visuomotor%20Policies%20via%20Perception%20Alignment&entry.906535625=Ruiyu%20Wang%20and%20Zheyu%20Zhuang%20and%20Danica%20Kragic%20and%20Florian%20T.%20Pokorny&entry.1292438233=Generalizing%20beyond%20the%20training%20domain%20in%20image-based%20behavior%20cloning%20remains%20challenging.%20Existing%20methods%20address%20individual%20axes%20of%20generalization%2C%20workspace%20shifts%2C%20viewpoint%20changes%2C%20and%20cross-embodiment%20transfer%2C%20yet%20they%20are%20typically%20developed%20in%20isolation%20and%20often%20rely%20on%20complex%20pipelines.%20We%20introduce%20PALM%20%28Perception%20Alignment%20for%20Local%20Manipulation%29%2C%20which%20leverages%20the%20invariance%20of%20local%20action%20distributions%20between%20out-of-distribution%20%28OOD%29%20and%20demonstrated%20domains%20to%20address%20these%20OOD%20shifts%20concurrently%2C%20without%20additional%20input%20modalities%2C%20model%20changes%2C%20or%20data%20collection.%20PALM%20modularizes%20the%20manipulation%20policy%20into%20coarse%20global%20components%20and%20a%20local%20policy%20for%20fine-grained%20actions.%20We%20reduce%20the%20discrepancy%20between%20in-domain%20and%20OOD%20inputs%20at%20the%20local%20policy%20level%20by%20enforcing%20local%20visual%20focus%20and%20consistent%20proprioceptive%20representation%2C%20allowing%20the%20policy%20to%20retrieve%20invariant%20local%20actions%20under%20OOD%20conditions.%20Experiments%20show%20that%20PALM%20limits%20OOD%20performance%20drops%20to%208%25%20in%20simulation%20and%2024%25%20in%20the%20real%20world%2C%20compared%20to%2045%25%20and%2077%25%20for%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.19514v1&entry.124074799=Read"},
{"title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning", "author": "Jinyeop Song and Song Wang and Julian Shun and Yada Zhu", "abstract": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.", "link": "http://arxiv.org/abs/2509.26383v4", "date": "2026-01-27", "relevancy": 2.2045, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5587}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.546}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Transferable%20Agentic%20Knowledge%20Graph%20RAG%20via%20Reinforcement%20Learning&body=Title%3A%20Efficient%20and%20Transferable%20Agentic%20Knowledge%20Graph%20RAG%20via%20Reinforcement%20Learning%0AAuthor%3A%20Jinyeop%20Song%20and%20Song%20Wang%20and%20Julian%20Shun%20and%20Yada%20Zhu%0AAbstract%3A%20Knowledge-graph%20retrieval-augmented%20generation%20%28KG-RAG%29%20couples%20large%20language%20models%20%28LLMs%29%20with%20structured%2C%20verifiable%20knowledge%20graphs%20%28KGs%29%20to%20reduce%20hallucinations%20and%20expose%20reasoning%20traces.%20However%2C%20many%20KG-RAG%20systems%20compose%20multiple%20LLM%20modules%20%28e.g%20planning%2C%20reasoning%2C%20and%20responding%29%2C%20inflating%20inference%20cost%20and%20binding%20behavior%20to%20a%20specific%20target%20KG.%20To%20address%20this%2C%20we%20introduce%20KG-R1%2C%20an%20agentic%20KG%20retrieval-augmented%20generation%20%28KG-RAG%29%20framework%20through%20reinforcement%20learning%20%28RL%29.%20KG-R1%20utilizes%20a%20single%20agent%20that%20interacts%20with%20KGs%20as%20its%20environment%2C%20learning%20to%20retrieve%20at%20each%20step%20and%20incorporating%20the%20retrieved%20information%20into%20its%20reasoning%20and%20generation.%20The%20process%20is%20optimized%20through%20end-to-end%20RL.%20In%20controlled%20experiments%20across%20Knowledge-Graph%20Question%20Answering%20%28KGQA%29%20benchmarks%2C%20our%20method%20demonstrates%20both%20efficiency%20and%20transferability%3A%20Using%20Qwen-2.5-3B%2C%20KG-R1%20improves%20answer%20accuracy%20with%20fewer%20generation%20tokens%20than%20prior%20multi-module%20workflow%20methods%20that%20use%20larger%20foundation%20or%20fine-tuned%20models.%20Furthermore%2C%20KG-R1%20enables%20plug%20and%20play%3A%20after%20training%2C%20it%20maintains%20strong%20accuracy%20on%20new%20KGs%20without%20modification.%20These%20properties%20make%20KG-R1%20a%20promising%20KG-RAG%20framework%20for%20real-world%20deployment.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Jinyeop3110/KG-R1.%0ALink%3A%20http%3A//arxiv.org/abs/2509.26383v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Transferable%2520Agentic%2520Knowledge%2520Graph%2520RAG%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DJinyeop%2520Song%2520and%2520Song%2520Wang%2520and%2520Julian%2520Shun%2520and%2520Yada%2520Zhu%26entry.1292438233%3DKnowledge-graph%2520retrieval-augmented%2520generation%2520%2528KG-RAG%2529%2520couples%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520structured%252C%2520verifiable%2520knowledge%2520graphs%2520%2528KGs%2529%2520to%2520reduce%2520hallucinations%2520and%2520expose%2520reasoning%2520traces.%2520However%252C%2520many%2520KG-RAG%2520systems%2520compose%2520multiple%2520LLM%2520modules%2520%2528e.g%2520planning%252C%2520reasoning%252C%2520and%2520responding%2529%252C%2520inflating%2520inference%2520cost%2520and%2520binding%2520behavior%2520to%2520a%2520specific%2520target%2520KG.%2520To%2520address%2520this%252C%2520we%2520introduce%2520KG-R1%252C%2520an%2520agentic%2520KG%2520retrieval-augmented%2520generation%2520%2528KG-RAG%2529%2520framework%2520through%2520reinforcement%2520learning%2520%2528RL%2529.%2520KG-R1%2520utilizes%2520a%2520single%2520agent%2520that%2520interacts%2520with%2520KGs%2520as%2520its%2520environment%252C%2520learning%2520to%2520retrieve%2520at%2520each%2520step%2520and%2520incorporating%2520the%2520retrieved%2520information%2520into%2520its%2520reasoning%2520and%2520generation.%2520The%2520process%2520is%2520optimized%2520through%2520end-to-end%2520RL.%2520In%2520controlled%2520experiments%2520across%2520Knowledge-Graph%2520Question%2520Answering%2520%2528KGQA%2529%2520benchmarks%252C%2520our%2520method%2520demonstrates%2520both%2520efficiency%2520and%2520transferability%253A%2520Using%2520Qwen-2.5-3B%252C%2520KG-R1%2520improves%2520answer%2520accuracy%2520with%2520fewer%2520generation%2520tokens%2520than%2520prior%2520multi-module%2520workflow%2520methods%2520that%2520use%2520larger%2520foundation%2520or%2520fine-tuned%2520models.%2520Furthermore%252C%2520KG-R1%2520enables%2520plug%2520and%2520play%253A%2520after%2520training%252C%2520it%2520maintains%2520strong%2520accuracy%2520on%2520new%2520KGs%2520without%2520modification.%2520These%2520properties%2520make%2520KG-R1%2520a%2520promising%2520KG-RAG%2520framework%2520for%2520real-world%2520deployment.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Jinyeop3110/KG-R1.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26383v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Transferable%20Agentic%20Knowledge%20Graph%20RAG%20via%20Reinforcement%20Learning&entry.906535625=Jinyeop%20Song%20and%20Song%20Wang%20and%20Julian%20Shun%20and%20Yada%20Zhu&entry.1292438233=Knowledge-graph%20retrieval-augmented%20generation%20%28KG-RAG%29%20couples%20large%20language%20models%20%28LLMs%29%20with%20structured%2C%20verifiable%20knowledge%20graphs%20%28KGs%29%20to%20reduce%20hallucinations%20and%20expose%20reasoning%20traces.%20However%2C%20many%20KG-RAG%20systems%20compose%20multiple%20LLM%20modules%20%28e.g%20planning%2C%20reasoning%2C%20and%20responding%29%2C%20inflating%20inference%20cost%20and%20binding%20behavior%20to%20a%20specific%20target%20KG.%20To%20address%20this%2C%20we%20introduce%20KG-R1%2C%20an%20agentic%20KG%20retrieval-augmented%20generation%20%28KG-RAG%29%20framework%20through%20reinforcement%20learning%20%28RL%29.%20KG-R1%20utilizes%20a%20single%20agent%20that%20interacts%20with%20KGs%20as%20its%20environment%2C%20learning%20to%20retrieve%20at%20each%20step%20and%20incorporating%20the%20retrieved%20information%20into%20its%20reasoning%20and%20generation.%20The%20process%20is%20optimized%20through%20end-to-end%20RL.%20In%20controlled%20experiments%20across%20Knowledge-Graph%20Question%20Answering%20%28KGQA%29%20benchmarks%2C%20our%20method%20demonstrates%20both%20efficiency%20and%20transferability%3A%20Using%20Qwen-2.5-3B%2C%20KG-R1%20improves%20answer%20accuracy%20with%20fewer%20generation%20tokens%20than%20prior%20multi-module%20workflow%20methods%20that%20use%20larger%20foundation%20or%20fine-tuned%20models.%20Furthermore%2C%20KG-R1%20enables%20plug%20and%20play%3A%20after%20training%2C%20it%20maintains%20strong%20accuracy%20on%20new%20KGs%20without%20modification.%20These%20properties%20make%20KG-R1%20a%20promising%20KG-RAG%20framework%20for%20real-world%20deployment.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Jinyeop3110/KG-R1.&entry.1838667208=http%3A//arxiv.org/abs/2509.26383v4&entry.124074799=Read"},
{"title": "Unveiling Perceptual Artifacts: A Fine-Grained Benchmark for Interpretable AI-Generated Image Detection", "author": "Yao Xiao and Weiyan Chen and Jiahao Chen and Zijie Cao and Weijian Deng and Binbin Yang and Ziyi Dong and Xiangyang Ji and Wei Ke and Pengxu Wei and Liang Lin", "abstract": "Current AI-Generated Image (AIGI) detection approaches predominantly rely on binary classification to distinguish real from synthetic images, often lacking interpretable or convincing evidence to substantiate their decisions. This limitation stems from existing AIGI detection benchmarks, which, despite featuring a broad collection of synthetic images, remain restricted in their coverage of artifact diversity and lack detailed, localized annotations. To bridge this gap, we introduce a fine-grained benchmark towards eXplainable AI-Generated image Detection, named X-AIGD, which provides pixel-level, categorized annotations of perceptual artifacts, spanning low-level distortions, high-level semantics, and cognitive-level counterfactuals. These comprehensive annotations facilitate fine-grained interpretability evaluation and deeper insight into model decision-making processes. Our extensive investigation using X-AIGD provides several key insights: (1) Existing AIGI detectors demonstrate negligible reliance on perceptual artifacts, even at the most basic distortion level. (2) While AIGI detectors can be trained to identify specific artifacts, they still substantially base their judgment on uninterpretable features. (3) Explicitly aligning model attention with artifact regions can increase the interpretability and generalization of detectors. The data and code are available at: https://github.com/Coxy7/X-AIGD.", "link": "http://arxiv.org/abs/2601.19430v1", "date": "2026-01-27", "relevancy": 2.2035, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5734}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5483}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Perceptual%20Artifacts%3A%20A%20Fine-Grained%20Benchmark%20for%20Interpretable%20AI-Generated%20Image%20Detection&body=Title%3A%20Unveiling%20Perceptual%20Artifacts%3A%20A%20Fine-Grained%20Benchmark%20for%20Interpretable%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Yao%20Xiao%20and%20Weiyan%20Chen%20and%20Jiahao%20Chen%20and%20Zijie%20Cao%20and%20Weijian%20Deng%20and%20Binbin%20Yang%20and%20Ziyi%20Dong%20and%20Xiangyang%20Ji%20and%20Wei%20Ke%20and%20Pengxu%20Wei%20and%20Liang%20Lin%0AAbstract%3A%20Current%20AI-Generated%20Image%20%28AIGI%29%20detection%20approaches%20predominantly%20rely%20on%20binary%20classification%20to%20distinguish%20real%20from%20synthetic%20images%2C%20often%20lacking%20interpretable%20or%20convincing%20evidence%20to%20substantiate%20their%20decisions.%20This%20limitation%20stems%20from%20existing%20AIGI%20detection%20benchmarks%2C%20which%2C%20despite%20featuring%20a%20broad%20collection%20of%20synthetic%20images%2C%20remain%20restricted%20in%20their%20coverage%20of%20artifact%20diversity%20and%20lack%20detailed%2C%20localized%20annotations.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20fine-grained%20benchmark%20towards%20eXplainable%20AI-Generated%20image%20Detection%2C%20named%20X-AIGD%2C%20which%20provides%20pixel-level%2C%20categorized%20annotations%20of%20perceptual%20artifacts%2C%20spanning%20low-level%20distortions%2C%20high-level%20semantics%2C%20and%20cognitive-level%20counterfactuals.%20These%20comprehensive%20annotations%20facilitate%20fine-grained%20interpretability%20evaluation%20and%20deeper%20insight%20into%20model%20decision-making%20processes.%20Our%20extensive%20investigation%20using%20X-AIGD%20provides%20several%20key%20insights%3A%20%281%29%20Existing%20AIGI%20detectors%20demonstrate%20negligible%20reliance%20on%20perceptual%20artifacts%2C%20even%20at%20the%20most%20basic%20distortion%20level.%20%282%29%20While%20AIGI%20detectors%20can%20be%20trained%20to%20identify%20specific%20artifacts%2C%20they%20still%20substantially%20base%20their%20judgment%20on%20uninterpretable%20features.%20%283%29%20Explicitly%20aligning%20model%20attention%20with%20artifact%20regions%20can%20increase%20the%20interpretability%20and%20generalization%20of%20detectors.%20The%20data%20and%20code%20are%20available%20at%3A%20https%3A//github.com/Coxy7/X-AIGD.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Perceptual%2520Artifacts%253A%2520A%2520Fine-Grained%2520Benchmark%2520for%2520Interpretable%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DYao%2520Xiao%2520and%2520Weiyan%2520Chen%2520and%2520Jiahao%2520Chen%2520and%2520Zijie%2520Cao%2520and%2520Weijian%2520Deng%2520and%2520Binbin%2520Yang%2520and%2520Ziyi%2520Dong%2520and%2520Xiangyang%2520Ji%2520and%2520Wei%2520Ke%2520and%2520Pengxu%2520Wei%2520and%2520Liang%2520Lin%26entry.1292438233%3DCurrent%2520AI-Generated%2520Image%2520%2528AIGI%2529%2520detection%2520approaches%2520predominantly%2520rely%2520on%2520binary%2520classification%2520to%2520distinguish%2520real%2520from%2520synthetic%2520images%252C%2520often%2520lacking%2520interpretable%2520or%2520convincing%2520evidence%2520to%2520substantiate%2520their%2520decisions.%2520This%2520limitation%2520stems%2520from%2520existing%2520AIGI%2520detection%2520benchmarks%252C%2520which%252C%2520despite%2520featuring%2520a%2520broad%2520collection%2520of%2520synthetic%2520images%252C%2520remain%2520restricted%2520in%2520their%2520coverage%2520of%2520artifact%2520diversity%2520and%2520lack%2520detailed%252C%2520localized%2520annotations.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520fine-grained%2520benchmark%2520towards%2520eXplainable%2520AI-Generated%2520image%2520Detection%252C%2520named%2520X-AIGD%252C%2520which%2520provides%2520pixel-level%252C%2520categorized%2520annotations%2520of%2520perceptual%2520artifacts%252C%2520spanning%2520low-level%2520distortions%252C%2520high-level%2520semantics%252C%2520and%2520cognitive-level%2520counterfactuals.%2520These%2520comprehensive%2520annotations%2520facilitate%2520fine-grained%2520interpretability%2520evaluation%2520and%2520deeper%2520insight%2520into%2520model%2520decision-making%2520processes.%2520Our%2520extensive%2520investigation%2520using%2520X-AIGD%2520provides%2520several%2520key%2520insights%253A%2520%25281%2529%2520Existing%2520AIGI%2520detectors%2520demonstrate%2520negligible%2520reliance%2520on%2520perceptual%2520artifacts%252C%2520even%2520at%2520the%2520most%2520basic%2520distortion%2520level.%2520%25282%2529%2520While%2520AIGI%2520detectors%2520can%2520be%2520trained%2520to%2520identify%2520specific%2520artifacts%252C%2520they%2520still%2520substantially%2520base%2520their%2520judgment%2520on%2520uninterpretable%2520features.%2520%25283%2529%2520Explicitly%2520aligning%2520model%2520attention%2520with%2520artifact%2520regions%2520can%2520increase%2520the%2520interpretability%2520and%2520generalization%2520of%2520detectors.%2520The%2520data%2520and%2520code%2520are%2520available%2520at%253A%2520https%253A//github.com/Coxy7/X-AIGD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Perceptual%20Artifacts%3A%20A%20Fine-Grained%20Benchmark%20for%20Interpretable%20AI-Generated%20Image%20Detection&entry.906535625=Yao%20Xiao%20and%20Weiyan%20Chen%20and%20Jiahao%20Chen%20and%20Zijie%20Cao%20and%20Weijian%20Deng%20and%20Binbin%20Yang%20and%20Ziyi%20Dong%20and%20Xiangyang%20Ji%20and%20Wei%20Ke%20and%20Pengxu%20Wei%20and%20Liang%20Lin&entry.1292438233=Current%20AI-Generated%20Image%20%28AIGI%29%20detection%20approaches%20predominantly%20rely%20on%20binary%20classification%20to%20distinguish%20real%20from%20synthetic%20images%2C%20often%20lacking%20interpretable%20or%20convincing%20evidence%20to%20substantiate%20their%20decisions.%20This%20limitation%20stems%20from%20existing%20AIGI%20detection%20benchmarks%2C%20which%2C%20despite%20featuring%20a%20broad%20collection%20of%20synthetic%20images%2C%20remain%20restricted%20in%20their%20coverage%20of%20artifact%20diversity%20and%20lack%20detailed%2C%20localized%20annotations.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20fine-grained%20benchmark%20towards%20eXplainable%20AI-Generated%20image%20Detection%2C%20named%20X-AIGD%2C%20which%20provides%20pixel-level%2C%20categorized%20annotations%20of%20perceptual%20artifacts%2C%20spanning%20low-level%20distortions%2C%20high-level%20semantics%2C%20and%20cognitive-level%20counterfactuals.%20These%20comprehensive%20annotations%20facilitate%20fine-grained%20interpretability%20evaluation%20and%20deeper%20insight%20into%20model%20decision-making%20processes.%20Our%20extensive%20investigation%20using%20X-AIGD%20provides%20several%20key%20insights%3A%20%281%29%20Existing%20AIGI%20detectors%20demonstrate%20negligible%20reliance%20on%20perceptual%20artifacts%2C%20even%20at%20the%20most%20basic%20distortion%20level.%20%282%29%20While%20AIGI%20detectors%20can%20be%20trained%20to%20identify%20specific%20artifacts%2C%20they%20still%20substantially%20base%20their%20judgment%20on%20uninterpretable%20features.%20%283%29%20Explicitly%20aligning%20model%20attention%20with%20artifact%20regions%20can%20increase%20the%20interpretability%20and%20generalization%20of%20detectors.%20The%20data%20and%20code%20are%20available%20at%3A%20https%3A//github.com/Coxy7/X-AIGD.&entry.1838667208=http%3A//arxiv.org/abs/2601.19430v1&entry.124074799=Read"},
{"title": "HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation", "author": "Haya Alyoussef and Ahmad Bdeir and Diego Coello de Portugal Mecke and Tom Hanika and Niels Landwehr and Lars Schmidt-Thieme", "abstract": "Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.", "link": "http://arxiv.org/abs/2601.19849v1", "date": "2026-01-27", "relevancy": 2.1926, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5743}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HexFormer%3A%20Hyperbolic%20Vision%20Transformer%20with%20Exponential%20Map%20Aggregation&body=Title%3A%20HexFormer%3A%20Hyperbolic%20Vision%20Transformer%20with%20Exponential%20Map%20Aggregation%0AAuthor%3A%20Haya%20Alyoussef%20and%20Ahmad%20Bdeir%20and%20Diego%20Coello%20de%20Portugal%20Mecke%20and%20Tom%20Hanika%20and%20Niels%20Landwehr%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20Data%20across%20modalities%20such%20as%20images%2C%20text%2C%20and%20graphs%20often%20contains%20hierarchical%20and%20relational%20structures%2C%20which%20are%20challenging%20to%20model%20within%20Euclidean%20geometry.%20Hyperbolic%20geometry%20provides%20a%20natural%20framework%20for%20representing%20such%20structures.%20Building%20on%20this%20property%2C%20this%20work%20introduces%20HexFormer%2C%20a%20hyperbolic%20vision%20transformer%20for%20image%20classification%20that%20incorporates%20exponential%20map%20aggregation%20within%20its%20attention%20mechanism.%20Two%20designs%20are%20explored%3A%20a%20hyperbolic%20ViT%20%28HexFormer%29%20and%20a%20hybrid%20variant%20%28HexFormer-Hybrid%29%20that%20combines%20a%20hyperbolic%20encoder%20with%20an%20Euclidean%20linear%20classification%20head.%20HexFormer%20incorporates%20a%20novel%20attention%20mechanism%20based%20on%20exponential%20map%20aggregation%2C%20which%20yields%20more%20accurate%20and%20stable%20aggregated%20representations%20than%20standard%20centroid%20based%20averaging%2C%20showing%20that%20simpler%20approaches%20retain%20competitive%20merit.%20Experiments%20across%20multiple%20datasets%20demonstrate%20consistent%20performance%20improvements%20over%20Euclidean%20baselines%20and%20prior%20hyperbolic%20ViTs%2C%20with%20the%20hybrid%20variant%20achieving%20the%20strongest%20overall%20results.%20Additionally%2C%20this%20study%20provides%20an%20analysis%20of%20gradient%20stability%20in%20hyperbolic%20transformers.%20The%20results%20reveal%20that%20hyperbolic%20models%20exhibit%20more%20stable%20gradients%20and%20reduced%20sensitivity%20to%20warmup%20strategies%20compared%20to%20Euclidean%20architectures%2C%20highlighting%20their%20robustness%20and%20efficiency%20in%20training.%20Overall%2C%20these%20findings%20indicate%20that%20hyperbolic%20geometry%20can%20enhance%20vision%20transformer%20architectures%20by%20improving%20gradient%20stability%20and%20accuracy.%20In%20addition%2C%20relatively%20simple%20mechanisms%20such%20as%20exponential%20map%20aggregation%20can%20provide%20strong%20practical%20benefits.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHexFormer%253A%2520Hyperbolic%2520Vision%2520Transformer%2520with%2520Exponential%2520Map%2520Aggregation%26entry.906535625%3DHaya%2520Alyoussef%2520and%2520Ahmad%2520Bdeir%2520and%2520Diego%2520Coello%2520de%2520Portugal%2520Mecke%2520and%2520Tom%2520Hanika%2520and%2520Niels%2520Landwehr%2520and%2520Lars%2520Schmidt-Thieme%26entry.1292438233%3DData%2520across%2520modalities%2520such%2520as%2520images%252C%2520text%252C%2520and%2520graphs%2520often%2520contains%2520hierarchical%2520and%2520relational%2520structures%252C%2520which%2520are%2520challenging%2520to%2520model%2520within%2520Euclidean%2520geometry.%2520Hyperbolic%2520geometry%2520provides%2520a%2520natural%2520framework%2520for%2520representing%2520such%2520structures.%2520Building%2520on%2520this%2520property%252C%2520this%2520work%2520introduces%2520HexFormer%252C%2520a%2520hyperbolic%2520vision%2520transformer%2520for%2520image%2520classification%2520that%2520incorporates%2520exponential%2520map%2520aggregation%2520within%2520its%2520attention%2520mechanism.%2520Two%2520designs%2520are%2520explored%253A%2520a%2520hyperbolic%2520ViT%2520%2528HexFormer%2529%2520and%2520a%2520hybrid%2520variant%2520%2528HexFormer-Hybrid%2529%2520that%2520combines%2520a%2520hyperbolic%2520encoder%2520with%2520an%2520Euclidean%2520linear%2520classification%2520head.%2520HexFormer%2520incorporates%2520a%2520novel%2520attention%2520mechanism%2520based%2520on%2520exponential%2520map%2520aggregation%252C%2520which%2520yields%2520more%2520accurate%2520and%2520stable%2520aggregated%2520representations%2520than%2520standard%2520centroid%2520based%2520averaging%252C%2520showing%2520that%2520simpler%2520approaches%2520retain%2520competitive%2520merit.%2520Experiments%2520across%2520multiple%2520datasets%2520demonstrate%2520consistent%2520performance%2520improvements%2520over%2520Euclidean%2520baselines%2520and%2520prior%2520hyperbolic%2520ViTs%252C%2520with%2520the%2520hybrid%2520variant%2520achieving%2520the%2520strongest%2520overall%2520results.%2520Additionally%252C%2520this%2520study%2520provides%2520an%2520analysis%2520of%2520gradient%2520stability%2520in%2520hyperbolic%2520transformers.%2520The%2520results%2520reveal%2520that%2520hyperbolic%2520models%2520exhibit%2520more%2520stable%2520gradients%2520and%2520reduced%2520sensitivity%2520to%2520warmup%2520strategies%2520compared%2520to%2520Euclidean%2520architectures%252C%2520highlighting%2520their%2520robustness%2520and%2520efficiency%2520in%2520training.%2520Overall%252C%2520these%2520findings%2520indicate%2520that%2520hyperbolic%2520geometry%2520can%2520enhance%2520vision%2520transformer%2520architectures%2520by%2520improving%2520gradient%2520stability%2520and%2520accuracy.%2520In%2520addition%252C%2520relatively%2520simple%2520mechanisms%2520such%2520as%2520exponential%2520map%2520aggregation%2520can%2520provide%2520strong%2520practical%2520benefits.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HexFormer%3A%20Hyperbolic%20Vision%20Transformer%20with%20Exponential%20Map%20Aggregation&entry.906535625=Haya%20Alyoussef%20and%20Ahmad%20Bdeir%20and%20Diego%20Coello%20de%20Portugal%20Mecke%20and%20Tom%20Hanika%20and%20Niels%20Landwehr%20and%20Lars%20Schmidt-Thieme&entry.1292438233=Data%20across%20modalities%20such%20as%20images%2C%20text%2C%20and%20graphs%20often%20contains%20hierarchical%20and%20relational%20structures%2C%20which%20are%20challenging%20to%20model%20within%20Euclidean%20geometry.%20Hyperbolic%20geometry%20provides%20a%20natural%20framework%20for%20representing%20such%20structures.%20Building%20on%20this%20property%2C%20this%20work%20introduces%20HexFormer%2C%20a%20hyperbolic%20vision%20transformer%20for%20image%20classification%20that%20incorporates%20exponential%20map%20aggregation%20within%20its%20attention%20mechanism.%20Two%20designs%20are%20explored%3A%20a%20hyperbolic%20ViT%20%28HexFormer%29%20and%20a%20hybrid%20variant%20%28HexFormer-Hybrid%29%20that%20combines%20a%20hyperbolic%20encoder%20with%20an%20Euclidean%20linear%20classification%20head.%20HexFormer%20incorporates%20a%20novel%20attention%20mechanism%20based%20on%20exponential%20map%20aggregation%2C%20which%20yields%20more%20accurate%20and%20stable%20aggregated%20representations%20than%20standard%20centroid%20based%20averaging%2C%20showing%20that%20simpler%20approaches%20retain%20competitive%20merit.%20Experiments%20across%20multiple%20datasets%20demonstrate%20consistent%20performance%20improvements%20over%20Euclidean%20baselines%20and%20prior%20hyperbolic%20ViTs%2C%20with%20the%20hybrid%20variant%20achieving%20the%20strongest%20overall%20results.%20Additionally%2C%20this%20study%20provides%20an%20analysis%20of%20gradient%20stability%20in%20hyperbolic%20transformers.%20The%20results%20reveal%20that%20hyperbolic%20models%20exhibit%20more%20stable%20gradients%20and%20reduced%20sensitivity%20to%20warmup%20strategies%20compared%20to%20Euclidean%20architectures%2C%20highlighting%20their%20robustness%20and%20efficiency%20in%20training.%20Overall%2C%20these%20findings%20indicate%20that%20hyperbolic%20geometry%20can%20enhance%20vision%20transformer%20architectures%20by%20improving%20gradient%20stability%20and%20accuracy.%20In%20addition%2C%20relatively%20simple%20mechanisms%20such%20as%20exponential%20map%20aggregation%20can%20provide%20strong%20practical%20benefits.&entry.1838667208=http%3A//arxiv.org/abs/2601.19849v1&entry.124074799=Read"},
{"title": "A Non-Invasive 3D Gait Analysis Framework for Quantifying Psychomotor Retardation in Major Depressive Disorder", "author": "Fouad Boutaleb and Emery Pierson and Mohamed Daoudi and Cl\u00e9mence Nineuil and Ali Amad and Fabien D'Hondt", "abstract": "Predicting the status of Major Depressive Disorder (MDD) from objective, non-invasive methods is an active research field. Yet, extracting automatically objective, interpretable features for a detailed analysis of the patient state remains largely unexplored.\n  Among MDD's symptoms, Psychomotor retardation (PMR) is a core item, yet its clinical assessment remains largely subjective. While 3D motion capture offers an objective alternative, its reliance on specialized hardware often precludes routine clinical use. In this paper, we propose a non-invasive computational framework that transforms monocular RGB video into clinically relevant 3D gait kinematics. Our pipeline uses Gravity-View Coordinates along with a novel trajectory-correction algorithm that leverages the closed-loop topology of our adapted Timed Up and Go (TUG) protocol to mitigate monocular depth errors. This novel pipeline enables the extraction of 297 explicit gait biomechanical biomarkers from a single camera capture.\n  To address the challenges of small clinical datasets, we introduce a stability-based machine learning framework that identifies robust motor signatures while preventing overfitting. Validated on the CALYPSO dataset, our method achieves an 83.3% accuracy in detecting PMR and explains 64% of the variance in overall depression severity (R^2=0.64). Notably, our study reveals a strong link between reduced ankle propulsion and restricted pelvic mobility to the depressive motor phenotype. These results demonstrate that physical movement serves as a robust proxy for the cognitive state, offering a transparent and scalable tool for the objective monitoring of depression in standard clinical environments.", "link": "http://arxiv.org/abs/2601.19526v1", "date": "2026-01-27", "relevancy": 2.1868, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5625}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5533}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Non-Invasive%203D%20Gait%20Analysis%20Framework%20for%20Quantifying%20Psychomotor%20Retardation%20in%20Major%20Depressive%20Disorder&body=Title%3A%20A%20Non-Invasive%203D%20Gait%20Analysis%20Framework%20for%20Quantifying%20Psychomotor%20Retardation%20in%20Major%20Depressive%20Disorder%0AAuthor%3A%20Fouad%20Boutaleb%20and%20Emery%20Pierson%20and%20Mohamed%20Daoudi%20and%20Cl%C3%A9mence%20Nineuil%20and%20Ali%20Amad%20and%20Fabien%20D%27Hondt%0AAbstract%3A%20Predicting%20the%20status%20of%20Major%20Depressive%20Disorder%20%28MDD%29%20from%20objective%2C%20non-invasive%20methods%20is%20an%20active%20research%20field.%20Yet%2C%20extracting%20automatically%20objective%2C%20interpretable%20features%20for%20a%20detailed%20analysis%20of%20the%20patient%20state%20remains%20largely%20unexplored.%0A%20%20Among%20MDD%27s%20symptoms%2C%20Psychomotor%20retardation%20%28PMR%29%20is%20a%20core%20item%2C%20yet%20its%20clinical%20assessment%20remains%20largely%20subjective.%20While%203D%20motion%20capture%20offers%20an%20objective%20alternative%2C%20its%20reliance%20on%20specialized%20hardware%20often%20precludes%20routine%20clinical%20use.%20In%20this%20paper%2C%20we%20propose%20a%20non-invasive%20computational%20framework%20that%20transforms%20monocular%20RGB%20video%20into%20clinically%20relevant%203D%20gait%20kinematics.%20Our%20pipeline%20uses%20Gravity-View%20Coordinates%20along%20with%20a%20novel%20trajectory-correction%20algorithm%20that%20leverages%20the%20closed-loop%20topology%20of%20our%20adapted%20Timed%20Up%20and%20Go%20%28TUG%29%20protocol%20to%20mitigate%20monocular%20depth%20errors.%20This%20novel%20pipeline%20enables%20the%20extraction%20of%20297%20explicit%20gait%20biomechanical%20biomarkers%20from%20a%20single%20camera%20capture.%0A%20%20To%20address%20the%20challenges%20of%20small%20clinical%20datasets%2C%20we%20introduce%20a%20stability-based%20machine%20learning%20framework%20that%20identifies%20robust%20motor%20signatures%20while%20preventing%20overfitting.%20Validated%20on%20the%20CALYPSO%20dataset%2C%20our%20method%20achieves%20an%2083.3%25%20accuracy%20in%20detecting%20PMR%20and%20explains%2064%25%20of%20the%20variance%20in%20overall%20depression%20severity%20%28R%5E2%3D0.64%29.%20Notably%2C%20our%20study%20reveals%20a%20strong%20link%20between%20reduced%20ankle%20propulsion%20and%20restricted%20pelvic%20mobility%20to%20the%20depressive%20motor%20phenotype.%20These%20results%20demonstrate%20that%20physical%20movement%20serves%20as%20a%20robust%20proxy%20for%20the%20cognitive%20state%2C%20offering%20a%20transparent%20and%20scalable%20tool%20for%20the%20objective%20monitoring%20of%20depression%20in%20standard%20clinical%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Non-Invasive%25203D%2520Gait%2520Analysis%2520Framework%2520for%2520Quantifying%2520Psychomotor%2520Retardation%2520in%2520Major%2520Depressive%2520Disorder%26entry.906535625%3DFouad%2520Boutaleb%2520and%2520Emery%2520Pierson%2520and%2520Mohamed%2520Daoudi%2520and%2520Cl%25C3%25A9mence%2520Nineuil%2520and%2520Ali%2520Amad%2520and%2520Fabien%2520D%2527Hondt%26entry.1292438233%3DPredicting%2520the%2520status%2520of%2520Major%2520Depressive%2520Disorder%2520%2528MDD%2529%2520from%2520objective%252C%2520non-invasive%2520methods%2520is%2520an%2520active%2520research%2520field.%2520Yet%252C%2520extracting%2520automatically%2520objective%252C%2520interpretable%2520features%2520for%2520a%2520detailed%2520analysis%2520of%2520the%2520patient%2520state%2520remains%2520largely%2520unexplored.%250A%2520%2520Among%2520MDD%2527s%2520symptoms%252C%2520Psychomotor%2520retardation%2520%2528PMR%2529%2520is%2520a%2520core%2520item%252C%2520yet%2520its%2520clinical%2520assessment%2520remains%2520largely%2520subjective.%2520While%25203D%2520motion%2520capture%2520offers%2520an%2520objective%2520alternative%252C%2520its%2520reliance%2520on%2520specialized%2520hardware%2520often%2520precludes%2520routine%2520clinical%2520use.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520non-invasive%2520computational%2520framework%2520that%2520transforms%2520monocular%2520RGB%2520video%2520into%2520clinically%2520relevant%25203D%2520gait%2520kinematics.%2520Our%2520pipeline%2520uses%2520Gravity-View%2520Coordinates%2520along%2520with%2520a%2520novel%2520trajectory-correction%2520algorithm%2520that%2520leverages%2520the%2520closed-loop%2520topology%2520of%2520our%2520adapted%2520Timed%2520Up%2520and%2520Go%2520%2528TUG%2529%2520protocol%2520to%2520mitigate%2520monocular%2520depth%2520errors.%2520This%2520novel%2520pipeline%2520enables%2520the%2520extraction%2520of%2520297%2520explicit%2520gait%2520biomechanical%2520biomarkers%2520from%2520a%2520single%2520camera%2520capture.%250A%2520%2520To%2520address%2520the%2520challenges%2520of%2520small%2520clinical%2520datasets%252C%2520we%2520introduce%2520a%2520stability-based%2520machine%2520learning%2520framework%2520that%2520identifies%2520robust%2520motor%2520signatures%2520while%2520preventing%2520overfitting.%2520Validated%2520on%2520the%2520CALYPSO%2520dataset%252C%2520our%2520method%2520achieves%2520an%252083.3%2525%2520accuracy%2520in%2520detecting%2520PMR%2520and%2520explains%252064%2525%2520of%2520the%2520variance%2520in%2520overall%2520depression%2520severity%2520%2528R%255E2%253D0.64%2529.%2520Notably%252C%2520our%2520study%2520reveals%2520a%2520strong%2520link%2520between%2520reduced%2520ankle%2520propulsion%2520and%2520restricted%2520pelvic%2520mobility%2520to%2520the%2520depressive%2520motor%2520phenotype.%2520These%2520results%2520demonstrate%2520that%2520physical%2520movement%2520serves%2520as%2520a%2520robust%2520proxy%2520for%2520the%2520cognitive%2520state%252C%2520offering%2520a%2520transparent%2520and%2520scalable%2520tool%2520for%2520the%2520objective%2520monitoring%2520of%2520depression%2520in%2520standard%2520clinical%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Non-Invasive%203D%20Gait%20Analysis%20Framework%20for%20Quantifying%20Psychomotor%20Retardation%20in%20Major%20Depressive%20Disorder&entry.906535625=Fouad%20Boutaleb%20and%20Emery%20Pierson%20and%20Mohamed%20Daoudi%20and%20Cl%C3%A9mence%20Nineuil%20and%20Ali%20Amad%20and%20Fabien%20D%27Hondt&entry.1292438233=Predicting%20the%20status%20of%20Major%20Depressive%20Disorder%20%28MDD%29%20from%20objective%2C%20non-invasive%20methods%20is%20an%20active%20research%20field.%20Yet%2C%20extracting%20automatically%20objective%2C%20interpretable%20features%20for%20a%20detailed%20analysis%20of%20the%20patient%20state%20remains%20largely%20unexplored.%0A%20%20Among%20MDD%27s%20symptoms%2C%20Psychomotor%20retardation%20%28PMR%29%20is%20a%20core%20item%2C%20yet%20its%20clinical%20assessment%20remains%20largely%20subjective.%20While%203D%20motion%20capture%20offers%20an%20objective%20alternative%2C%20its%20reliance%20on%20specialized%20hardware%20often%20precludes%20routine%20clinical%20use.%20In%20this%20paper%2C%20we%20propose%20a%20non-invasive%20computational%20framework%20that%20transforms%20monocular%20RGB%20video%20into%20clinically%20relevant%203D%20gait%20kinematics.%20Our%20pipeline%20uses%20Gravity-View%20Coordinates%20along%20with%20a%20novel%20trajectory-correction%20algorithm%20that%20leverages%20the%20closed-loop%20topology%20of%20our%20adapted%20Timed%20Up%20and%20Go%20%28TUG%29%20protocol%20to%20mitigate%20monocular%20depth%20errors.%20This%20novel%20pipeline%20enables%20the%20extraction%20of%20297%20explicit%20gait%20biomechanical%20biomarkers%20from%20a%20single%20camera%20capture.%0A%20%20To%20address%20the%20challenges%20of%20small%20clinical%20datasets%2C%20we%20introduce%20a%20stability-based%20machine%20learning%20framework%20that%20identifies%20robust%20motor%20signatures%20while%20preventing%20overfitting.%20Validated%20on%20the%20CALYPSO%20dataset%2C%20our%20method%20achieves%20an%2083.3%25%20accuracy%20in%20detecting%20PMR%20and%20explains%2064%25%20of%20the%20variance%20in%20overall%20depression%20severity%20%28R%5E2%3D0.64%29.%20Notably%2C%20our%20study%20reveals%20a%20strong%20link%20between%20reduced%20ankle%20propulsion%20and%20restricted%20pelvic%20mobility%20to%20the%20depressive%20motor%20phenotype.%20These%20results%20demonstrate%20that%20physical%20movement%20serves%20as%20a%20robust%20proxy%20for%20the%20cognitive%20state%2C%20offering%20a%20transparent%20and%20scalable%20tool%20for%20the%20objective%20monitoring%20of%20depression%20in%20standard%20clinical%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.19526v1&entry.124074799=Read"},
{"title": "Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking", "author": "Wen Wen and Tianwu Zhi and Kanglong Fan and Yang Li and Xinge Peng and Yabin Zhang and Yiting Liao and Junlin Li and Li Zhang", "abstract": "Improving vision-language models (VLMs) in the post-training stage typically relies on supervised fine-tuning or reinforcement learning, methods that necessitate costly, human-annotated data. While self-supervised techniques have proven effective for enhancing reasoning capabilities, their application to perceptual domains such as image quality assessment (IQA) remains largely unexplored. In this work, we introduce EvoQuality, a novel framework that enables a VLM to autonomously refine its quality perception capabilities without any ground-truth labels. EvoQuality adapts the principle of self-consistency to the ranking-based nature of IQA. It generates pseudo-labels by performing pairwise majority voting on the VLM's own outputs to establish a consensus on relative quality. These pseudo-rankings are then formulated into a fidelity reward that guides the model's iterative evolution through group relative policy optimization (GRPO). By iteratively leveraging its own predictions, EvoQuality progressively refines the VLM's perceptual capability. Extensive experiments show that EvoQuality boosts the base VLM's zero-shot performance by 31.8% on PLCC across diverse IQA benchmarks. Remarkably, despite being entirely self-supervised, EvoQuality achieves performance that is competitive with, or even surpasses, state-of-the-art supervised VLM-based IQA models, outperforming these models on 5 out of 7 IQA benchmarks. Furthermore, the framework demonstrates significant flexibility, allowing it to be stacked with pre-trained IQA models to bolster generalization on unseen datasets.", "link": "http://arxiv.org/abs/2509.25787v4", "date": "2026-01-27", "relevancy": 2.1795, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Evolving%20Vision-Language%20Models%20for%20Image%20Quality%20Assessment%20via%20Voting%20and%20Ranking&body=Title%3A%20Self-Evolving%20Vision-Language%20Models%20for%20Image%20Quality%20Assessment%20via%20Voting%20and%20Ranking%0AAuthor%3A%20Wen%20Wen%20and%20Tianwu%20Zhi%20and%20Kanglong%20Fan%20and%20Yang%20Li%20and%20Xinge%20Peng%20and%20Yabin%20Zhang%20and%20Yiting%20Liao%20and%20Junlin%20Li%20and%20Li%20Zhang%0AAbstract%3A%20Improving%20vision-language%20models%20%28VLMs%29%20in%20the%20post-training%20stage%20typically%20relies%20on%20supervised%20fine-tuning%20or%20reinforcement%20learning%2C%20methods%20that%20necessitate%20costly%2C%20human-annotated%20data.%20While%20self-supervised%20techniques%20have%20proven%20effective%20for%20enhancing%20reasoning%20capabilities%2C%20their%20application%20to%20perceptual%20domains%20such%20as%20image%20quality%20assessment%20%28IQA%29%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20introduce%20EvoQuality%2C%20a%20novel%20framework%20that%20enables%20a%20VLM%20to%20autonomously%20refine%20its%20quality%20perception%20capabilities%20without%20any%20ground-truth%20labels.%20EvoQuality%20adapts%20the%20principle%20of%20self-consistency%20to%20the%20ranking-based%20nature%20of%20IQA.%20It%20generates%20pseudo-labels%20by%20performing%20pairwise%20majority%20voting%20on%20the%20VLM%27s%20own%20outputs%20to%20establish%20a%20consensus%20on%20relative%20quality.%20These%20pseudo-rankings%20are%20then%20formulated%20into%20a%20fidelity%20reward%20that%20guides%20the%20model%27s%20iterative%20evolution%20through%20group%20relative%20policy%20optimization%20%28GRPO%29.%20By%20iteratively%20leveraging%20its%20own%20predictions%2C%20EvoQuality%20progressively%20refines%20the%20VLM%27s%20perceptual%20capability.%20Extensive%20experiments%20show%20that%20EvoQuality%20boosts%20the%20base%20VLM%27s%20zero-shot%20performance%20by%2031.8%25%20on%20PLCC%20across%20diverse%20IQA%20benchmarks.%20Remarkably%2C%20despite%20being%20entirely%20self-supervised%2C%20EvoQuality%20achieves%20performance%20that%20is%20competitive%20with%2C%20or%20even%20surpasses%2C%20state-of-the-art%20supervised%20VLM-based%20IQA%20models%2C%20outperforming%20these%20models%20on%205%20out%20of%207%20IQA%20benchmarks.%20Furthermore%2C%20the%20framework%20demonstrates%20significant%20flexibility%2C%20allowing%20it%20to%20be%20stacked%20with%20pre-trained%20IQA%20models%20to%20bolster%20generalization%20on%20unseen%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2509.25787v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Evolving%2520Vision-Language%2520Models%2520for%2520Image%2520Quality%2520Assessment%2520via%2520Voting%2520and%2520Ranking%26entry.906535625%3DWen%2520Wen%2520and%2520Tianwu%2520Zhi%2520and%2520Kanglong%2520Fan%2520and%2520Yang%2520Li%2520and%2520Xinge%2520Peng%2520and%2520Yabin%2520Zhang%2520and%2520Yiting%2520Liao%2520and%2520Junlin%2520Li%2520and%2520Li%2520Zhang%26entry.1292438233%3DImproving%2520vision-language%2520models%2520%2528VLMs%2529%2520in%2520the%2520post-training%2520stage%2520typically%2520relies%2520on%2520supervised%2520fine-tuning%2520or%2520reinforcement%2520learning%252C%2520methods%2520that%2520necessitate%2520costly%252C%2520human-annotated%2520data.%2520While%2520self-supervised%2520techniques%2520have%2520proven%2520effective%2520for%2520enhancing%2520reasoning%2520capabilities%252C%2520their%2520application%2520to%2520perceptual%2520domains%2520such%2520as%2520image%2520quality%2520assessment%2520%2528IQA%2529%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520introduce%2520EvoQuality%252C%2520a%2520novel%2520framework%2520that%2520enables%2520a%2520VLM%2520to%2520autonomously%2520refine%2520its%2520quality%2520perception%2520capabilities%2520without%2520any%2520ground-truth%2520labels.%2520EvoQuality%2520adapts%2520the%2520principle%2520of%2520self-consistency%2520to%2520the%2520ranking-based%2520nature%2520of%2520IQA.%2520It%2520generates%2520pseudo-labels%2520by%2520performing%2520pairwise%2520majority%2520voting%2520on%2520the%2520VLM%2527s%2520own%2520outputs%2520to%2520establish%2520a%2520consensus%2520on%2520relative%2520quality.%2520These%2520pseudo-rankings%2520are%2520then%2520formulated%2520into%2520a%2520fidelity%2520reward%2520that%2520guides%2520the%2520model%2527s%2520iterative%2520evolution%2520through%2520group%2520relative%2520policy%2520optimization%2520%2528GRPO%2529.%2520By%2520iteratively%2520leveraging%2520its%2520own%2520predictions%252C%2520EvoQuality%2520progressively%2520refines%2520the%2520VLM%2527s%2520perceptual%2520capability.%2520Extensive%2520experiments%2520show%2520that%2520EvoQuality%2520boosts%2520the%2520base%2520VLM%2527s%2520zero-shot%2520performance%2520by%252031.8%2525%2520on%2520PLCC%2520across%2520diverse%2520IQA%2520benchmarks.%2520Remarkably%252C%2520despite%2520being%2520entirely%2520self-supervised%252C%2520EvoQuality%2520achieves%2520performance%2520that%2520is%2520competitive%2520with%252C%2520or%2520even%2520surpasses%252C%2520state-of-the-art%2520supervised%2520VLM-based%2520IQA%2520models%252C%2520outperforming%2520these%2520models%2520on%25205%2520out%2520of%25207%2520IQA%2520benchmarks.%2520Furthermore%252C%2520the%2520framework%2520demonstrates%2520significant%2520flexibility%252C%2520allowing%2520it%2520to%2520be%2520stacked%2520with%2520pre-trained%2520IQA%2520models%2520to%2520bolster%2520generalization%2520on%2520unseen%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25787v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Evolving%20Vision-Language%20Models%20for%20Image%20Quality%20Assessment%20via%20Voting%20and%20Ranking&entry.906535625=Wen%20Wen%20and%20Tianwu%20Zhi%20and%20Kanglong%20Fan%20and%20Yang%20Li%20and%20Xinge%20Peng%20and%20Yabin%20Zhang%20and%20Yiting%20Liao%20and%20Junlin%20Li%20and%20Li%20Zhang&entry.1292438233=Improving%20vision-language%20models%20%28VLMs%29%20in%20the%20post-training%20stage%20typically%20relies%20on%20supervised%20fine-tuning%20or%20reinforcement%20learning%2C%20methods%20that%20necessitate%20costly%2C%20human-annotated%20data.%20While%20self-supervised%20techniques%20have%20proven%20effective%20for%20enhancing%20reasoning%20capabilities%2C%20their%20application%20to%20perceptual%20domains%20such%20as%20image%20quality%20assessment%20%28IQA%29%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20introduce%20EvoQuality%2C%20a%20novel%20framework%20that%20enables%20a%20VLM%20to%20autonomously%20refine%20its%20quality%20perception%20capabilities%20without%20any%20ground-truth%20labels.%20EvoQuality%20adapts%20the%20principle%20of%20self-consistency%20to%20the%20ranking-based%20nature%20of%20IQA.%20It%20generates%20pseudo-labels%20by%20performing%20pairwise%20majority%20voting%20on%20the%20VLM%27s%20own%20outputs%20to%20establish%20a%20consensus%20on%20relative%20quality.%20These%20pseudo-rankings%20are%20then%20formulated%20into%20a%20fidelity%20reward%20that%20guides%20the%20model%27s%20iterative%20evolution%20through%20group%20relative%20policy%20optimization%20%28GRPO%29.%20By%20iteratively%20leveraging%20its%20own%20predictions%2C%20EvoQuality%20progressively%20refines%20the%20VLM%27s%20perceptual%20capability.%20Extensive%20experiments%20show%20that%20EvoQuality%20boosts%20the%20base%20VLM%27s%20zero-shot%20performance%20by%2031.8%25%20on%20PLCC%20across%20diverse%20IQA%20benchmarks.%20Remarkably%2C%20despite%20being%20entirely%20self-supervised%2C%20EvoQuality%20achieves%20performance%20that%20is%20competitive%20with%2C%20or%20even%20surpasses%2C%20state-of-the-art%20supervised%20VLM-based%20IQA%20models%2C%20outperforming%20these%20models%20on%205%20out%20of%207%20IQA%20benchmarks.%20Furthermore%2C%20the%20framework%20demonstrates%20significant%20flexibility%2C%20allowing%20it%20to%20be%20stacked%20with%20pre-trained%20IQA%20models%20to%20bolster%20generalization%20on%20unseen%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2509.25787v4&entry.124074799=Read"},
{"title": "MSCloudCAM: Multi-Scale Context Adaptation with Convolutional Cross-Attention for Multispectral Cloud Segmentation", "author": "Md Abdullah Al Mazid and Liangdong Deng and Naphtali Rishe", "abstract": "Clouds remain a major obstacle in optical satellite imaging, limiting accurate environmental and climate analysis. To address the strong spectral variability and the large scale differences among cloud types, we propose MSCloudCAM, a novel multi-scale context adapter network with convolution based cross-attention tailored for multispectral and multi-sensor cloud segmentation. A key contribution of MSCloudCAM is the explicit modeling of multiple complementary multi-scale context extractors. And also, rather than simply stacking or concatenating their outputs, our formulation uses one extractor's fine-resolution features and the other extractor's global contextual representations enabling dynamic, scale-aware feature selection. Building on this idea, we design a new convolution-based cross attention adapter that effectively fuses localized, detailed information with broader multi-scale context. Integrated with a hierarchical vision backbone and refined through channel and spatial attention mechanisms, MSCloudCAM achieves strong spectral-spatial discrimination. Experiments on various multisensor datatsets e.g. CloudSEN12 (Sentinel-2) and L8Biome (Landsat-8), demonstrate that MSCloudCAM achieves superior overall segmentation performance and competitive class-wise accuracy compared to recent state-of-the-art models, while maintaining competitive model complexity, highlighting the novelty and effectiveness of the proposed design for large-scale Earth observation.", "link": "http://arxiv.org/abs/2510.10802v4", "date": "2026-01-27", "relevancy": 2.1773, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5881}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSCloudCAM%3A%20Multi-Scale%20Context%20Adaptation%20with%20Convolutional%20Cross-Attention%20for%20Multispectral%20Cloud%20Segmentation&body=Title%3A%20MSCloudCAM%3A%20Multi-Scale%20Context%20Adaptation%20with%20Convolutional%20Cross-Attention%20for%20Multispectral%20Cloud%20Segmentation%0AAuthor%3A%20Md%20Abdullah%20Al%20Mazid%20and%20Liangdong%20Deng%20and%20Naphtali%20Rishe%0AAbstract%3A%20Clouds%20remain%20a%20major%20obstacle%20in%20optical%20satellite%20imaging%2C%20limiting%20accurate%20environmental%20and%20climate%20analysis.%20To%20address%20the%20strong%20spectral%20variability%20and%20the%20large%20scale%20differences%20among%20cloud%20types%2C%20we%20propose%20MSCloudCAM%2C%20a%20novel%20multi-scale%20context%20adapter%20network%20with%20convolution%20based%20cross-attention%20tailored%20for%20multispectral%20and%20multi-sensor%20cloud%20segmentation.%20A%20key%20contribution%20of%20MSCloudCAM%20is%20the%20explicit%20modeling%20of%20multiple%20complementary%20multi-scale%20context%20extractors.%20And%20also%2C%20rather%20than%20simply%20stacking%20or%20concatenating%20their%20outputs%2C%20our%20formulation%20uses%20one%20extractor%27s%20fine-resolution%20features%20and%20the%20other%20extractor%27s%20global%20contextual%20representations%20enabling%20dynamic%2C%20scale-aware%20feature%20selection.%20Building%20on%20this%20idea%2C%20we%20design%20a%20new%20convolution-based%20cross%20attention%20adapter%20that%20effectively%20fuses%20localized%2C%20detailed%20information%20with%20broader%20multi-scale%20context.%20Integrated%20with%20a%20hierarchical%20vision%20backbone%20and%20refined%20through%20channel%20and%20spatial%20attention%20mechanisms%2C%20MSCloudCAM%20achieves%20strong%20spectral-spatial%20discrimination.%20Experiments%20on%20various%20multisensor%20datatsets%20e.g.%20CloudSEN12%20%28Sentinel-2%29%20and%20L8Biome%20%28Landsat-8%29%2C%20demonstrate%20that%20MSCloudCAM%20achieves%20superior%20overall%20segmentation%20performance%20and%20competitive%20class-wise%20accuracy%20compared%20to%20recent%20state-of-the-art%20models%2C%20while%20maintaining%20competitive%20model%20complexity%2C%20highlighting%20the%20novelty%20and%20effectiveness%20of%20the%20proposed%20design%20for%20large-scale%20Earth%20observation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10802v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSCloudCAM%253A%2520Multi-Scale%2520Context%2520Adaptation%2520with%2520Convolutional%2520Cross-Attention%2520for%2520Multispectral%2520Cloud%2520Segmentation%26entry.906535625%3DMd%2520Abdullah%2520Al%2520Mazid%2520and%2520Liangdong%2520Deng%2520and%2520Naphtali%2520Rishe%26entry.1292438233%3DClouds%2520remain%2520a%2520major%2520obstacle%2520in%2520optical%2520satellite%2520imaging%252C%2520limiting%2520accurate%2520environmental%2520and%2520climate%2520analysis.%2520To%2520address%2520the%2520strong%2520spectral%2520variability%2520and%2520the%2520large%2520scale%2520differences%2520among%2520cloud%2520types%252C%2520we%2520propose%2520MSCloudCAM%252C%2520a%2520novel%2520multi-scale%2520context%2520adapter%2520network%2520with%2520convolution%2520based%2520cross-attention%2520tailored%2520for%2520multispectral%2520and%2520multi-sensor%2520cloud%2520segmentation.%2520A%2520key%2520contribution%2520of%2520MSCloudCAM%2520is%2520the%2520explicit%2520modeling%2520of%2520multiple%2520complementary%2520multi-scale%2520context%2520extractors.%2520And%2520also%252C%2520rather%2520than%2520simply%2520stacking%2520or%2520concatenating%2520their%2520outputs%252C%2520our%2520formulation%2520uses%2520one%2520extractor%2527s%2520fine-resolution%2520features%2520and%2520the%2520other%2520extractor%2527s%2520global%2520contextual%2520representations%2520enabling%2520dynamic%252C%2520scale-aware%2520feature%2520selection.%2520Building%2520on%2520this%2520idea%252C%2520we%2520design%2520a%2520new%2520convolution-based%2520cross%2520attention%2520adapter%2520that%2520effectively%2520fuses%2520localized%252C%2520detailed%2520information%2520with%2520broader%2520multi-scale%2520context.%2520Integrated%2520with%2520a%2520hierarchical%2520vision%2520backbone%2520and%2520refined%2520through%2520channel%2520and%2520spatial%2520attention%2520mechanisms%252C%2520MSCloudCAM%2520achieves%2520strong%2520spectral-spatial%2520discrimination.%2520Experiments%2520on%2520various%2520multisensor%2520datatsets%2520e.g.%2520CloudSEN12%2520%2528Sentinel-2%2529%2520and%2520L8Biome%2520%2528Landsat-8%2529%252C%2520demonstrate%2520that%2520MSCloudCAM%2520achieves%2520superior%2520overall%2520segmentation%2520performance%2520and%2520competitive%2520class-wise%2520accuracy%2520compared%2520to%2520recent%2520state-of-the-art%2520models%252C%2520while%2520maintaining%2520competitive%2520model%2520complexity%252C%2520highlighting%2520the%2520novelty%2520and%2520effectiveness%2520of%2520the%2520proposed%2520design%2520for%2520large-scale%2520Earth%2520observation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10802v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSCloudCAM%3A%20Multi-Scale%20Context%20Adaptation%20with%20Convolutional%20Cross-Attention%20for%20Multispectral%20Cloud%20Segmentation&entry.906535625=Md%20Abdullah%20Al%20Mazid%20and%20Liangdong%20Deng%20and%20Naphtali%20Rishe&entry.1292438233=Clouds%20remain%20a%20major%20obstacle%20in%20optical%20satellite%20imaging%2C%20limiting%20accurate%20environmental%20and%20climate%20analysis.%20To%20address%20the%20strong%20spectral%20variability%20and%20the%20large%20scale%20differences%20among%20cloud%20types%2C%20we%20propose%20MSCloudCAM%2C%20a%20novel%20multi-scale%20context%20adapter%20network%20with%20convolution%20based%20cross-attention%20tailored%20for%20multispectral%20and%20multi-sensor%20cloud%20segmentation.%20A%20key%20contribution%20of%20MSCloudCAM%20is%20the%20explicit%20modeling%20of%20multiple%20complementary%20multi-scale%20context%20extractors.%20And%20also%2C%20rather%20than%20simply%20stacking%20or%20concatenating%20their%20outputs%2C%20our%20formulation%20uses%20one%20extractor%27s%20fine-resolution%20features%20and%20the%20other%20extractor%27s%20global%20contextual%20representations%20enabling%20dynamic%2C%20scale-aware%20feature%20selection.%20Building%20on%20this%20idea%2C%20we%20design%20a%20new%20convolution-based%20cross%20attention%20adapter%20that%20effectively%20fuses%20localized%2C%20detailed%20information%20with%20broader%20multi-scale%20context.%20Integrated%20with%20a%20hierarchical%20vision%20backbone%20and%20refined%20through%20channel%20and%20spatial%20attention%20mechanisms%2C%20MSCloudCAM%20achieves%20strong%20spectral-spatial%20discrimination.%20Experiments%20on%20various%20multisensor%20datatsets%20e.g.%20CloudSEN12%20%28Sentinel-2%29%20and%20L8Biome%20%28Landsat-8%29%2C%20demonstrate%20that%20MSCloudCAM%20achieves%20superior%20overall%20segmentation%20performance%20and%20competitive%20class-wise%20accuracy%20compared%20to%20recent%20state-of-the-art%20models%2C%20while%20maintaining%20competitive%20model%20complexity%2C%20highlighting%20the%20novelty%20and%20effectiveness%20of%20the%20proposed%20design%20for%20large-scale%20Earth%20observation.&entry.1838667208=http%3A//arxiv.org/abs/2510.10802v4&entry.124074799=Read"},
{"title": "Generative Latent Alignment for Interpretable Radar Based Occupancy Detection in Ambient Assisted Living", "author": "Huy Trinh", "abstract": "In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns. We propose a Generative Latent Alignment (GLA) framework that combines a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to learn a low-dimensional latent representation of radar Range-Angle (RA) heatmaps. The latent space is softly aligned with two semantic anchors corresponding to \"empty room\" and \"person present\", and Grad-CAM is applied in this aligned latent space to visualize which spatial regions support each presence decision. On our mmWave radar dataset, we qualitatively observe that the \"person present\" class produces compact Grad-CAM blobs that coincide with strong RA returns, whereas \"empty room\" samples yield diffuse or no evidence. We also conduct an ablation study using unrelated text prompts, which degrades both reconstruction and localization, suggesting that radar-specific anchors are important for meaningful explanations in this setting.", "link": "http://arxiv.org/abs/2601.19853v1", "date": "2026-01-27", "relevancy": 2.1771, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5636}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5403}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Latent%20Alignment%20for%20Interpretable%20Radar%20Based%20Occupancy%20Detection%20in%20Ambient%20Assisted%20Living&body=Title%3A%20Generative%20Latent%20Alignment%20for%20Interpretable%20Radar%20Based%20Occupancy%20Detection%20in%20Ambient%20Assisted%20Living%0AAuthor%3A%20Huy%20Trinh%0AAbstract%3A%20In%20this%20work%2C%20we%20study%20how%20to%20make%20mmWave%20radar%20presence%20detection%20more%20interpretable%20for%20Ambient%20Assisted%20Living%20%28AAL%29%20settings%2C%20where%20camera-based%20sensing%20raises%20privacy%20concerns.%20We%20propose%20a%20Generative%20Latent%20Alignment%20%28GLA%29%20framework%20that%20combines%20a%20lightweight%20convolutional%20variational%20autoencoder%20with%20a%20frozen%20CLIP%20text%20encoder%20to%20learn%20a%20low-dimensional%20latent%20representation%20of%20radar%20Range-Angle%20%28RA%29%20heatmaps.%20The%20latent%20space%20is%20softly%20aligned%20with%20two%20semantic%20anchors%20corresponding%20to%20%22empty%20room%22%20and%20%22person%20present%22%2C%20and%20Grad-CAM%20is%20applied%20in%20this%20aligned%20latent%20space%20to%20visualize%20which%20spatial%20regions%20support%20each%20presence%20decision.%20On%20our%20mmWave%20radar%20dataset%2C%20we%20qualitatively%20observe%20that%20the%20%22person%20present%22%20class%20produces%20compact%20Grad-CAM%20blobs%20that%20coincide%20with%20strong%20RA%20returns%2C%20whereas%20%22empty%20room%22%20samples%20yield%20diffuse%20or%20no%20evidence.%20We%20also%20conduct%20an%20ablation%20study%20using%20unrelated%20text%20prompts%2C%20which%20degrades%20both%20reconstruction%20and%20localization%2C%20suggesting%20that%20radar-specific%20anchors%20are%20important%20for%20meaningful%20explanations%20in%20this%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Latent%2520Alignment%2520for%2520Interpretable%2520Radar%2520Based%2520Occupancy%2520Detection%2520in%2520Ambient%2520Assisted%2520Living%26entry.906535625%3DHuy%2520Trinh%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520study%2520how%2520to%2520make%2520mmWave%2520radar%2520presence%2520detection%2520more%2520interpretable%2520for%2520Ambient%2520Assisted%2520Living%2520%2528AAL%2529%2520settings%252C%2520where%2520camera-based%2520sensing%2520raises%2520privacy%2520concerns.%2520We%2520propose%2520a%2520Generative%2520Latent%2520Alignment%2520%2528GLA%2529%2520framework%2520that%2520combines%2520a%2520lightweight%2520convolutional%2520variational%2520autoencoder%2520with%2520a%2520frozen%2520CLIP%2520text%2520encoder%2520to%2520learn%2520a%2520low-dimensional%2520latent%2520representation%2520of%2520radar%2520Range-Angle%2520%2528RA%2529%2520heatmaps.%2520The%2520latent%2520space%2520is%2520softly%2520aligned%2520with%2520two%2520semantic%2520anchors%2520corresponding%2520to%2520%2522empty%2520room%2522%2520and%2520%2522person%2520present%2522%252C%2520and%2520Grad-CAM%2520is%2520applied%2520in%2520this%2520aligned%2520latent%2520space%2520to%2520visualize%2520which%2520spatial%2520regions%2520support%2520each%2520presence%2520decision.%2520On%2520our%2520mmWave%2520radar%2520dataset%252C%2520we%2520qualitatively%2520observe%2520that%2520the%2520%2522person%2520present%2522%2520class%2520produces%2520compact%2520Grad-CAM%2520blobs%2520that%2520coincide%2520with%2520strong%2520RA%2520returns%252C%2520whereas%2520%2522empty%2520room%2522%2520samples%2520yield%2520diffuse%2520or%2520no%2520evidence.%2520We%2520also%2520conduct%2520an%2520ablation%2520study%2520using%2520unrelated%2520text%2520prompts%252C%2520which%2520degrades%2520both%2520reconstruction%2520and%2520localization%252C%2520suggesting%2520that%2520radar-specific%2520anchors%2520are%2520important%2520for%2520meaningful%2520explanations%2520in%2520this%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Latent%20Alignment%20for%20Interpretable%20Radar%20Based%20Occupancy%20Detection%20in%20Ambient%20Assisted%20Living&entry.906535625=Huy%20Trinh&entry.1292438233=In%20this%20work%2C%20we%20study%20how%20to%20make%20mmWave%20radar%20presence%20detection%20more%20interpretable%20for%20Ambient%20Assisted%20Living%20%28AAL%29%20settings%2C%20where%20camera-based%20sensing%20raises%20privacy%20concerns.%20We%20propose%20a%20Generative%20Latent%20Alignment%20%28GLA%29%20framework%20that%20combines%20a%20lightweight%20convolutional%20variational%20autoencoder%20with%20a%20frozen%20CLIP%20text%20encoder%20to%20learn%20a%20low-dimensional%20latent%20representation%20of%20radar%20Range-Angle%20%28RA%29%20heatmaps.%20The%20latent%20space%20is%20softly%20aligned%20with%20two%20semantic%20anchors%20corresponding%20to%20%22empty%20room%22%20and%20%22person%20present%22%2C%20and%20Grad-CAM%20is%20applied%20in%20this%20aligned%20latent%20space%20to%20visualize%20which%20spatial%20regions%20support%20each%20presence%20decision.%20On%20our%20mmWave%20radar%20dataset%2C%20we%20qualitatively%20observe%20that%20the%20%22person%20present%22%20class%20produces%20compact%20Grad-CAM%20blobs%20that%20coincide%20with%20strong%20RA%20returns%2C%20whereas%20%22empty%20room%22%20samples%20yield%20diffuse%20or%20no%20evidence.%20We%20also%20conduct%20an%20ablation%20study%20using%20unrelated%20text%20prompts%2C%20which%20degrades%20both%20reconstruction%20and%20localization%2C%20suggesting%20that%20radar-specific%20anchors%20are%20important%20for%20meaningful%20explanations%20in%20this%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2601.19853v1&entry.124074799=Read"},
{"title": "A Multi-directional Meta-Learning Framework for Class-Generalizable Anomaly Detection", "author": "Padmaksha Roy and Lamine Mili and Almuatazbellah Boker", "abstract": "In this paper, we address the problem of class-generalizable anomaly detection, where the objective is to develop a unified model by focusing our learning on the available normal data and a small amount of anomaly data in order to detect the completely unseen anomalies, also referred to as the out-of-distribution (OOD) classes. Adding to this challenge is the fact that the anomaly data is rare and costly to label. To achieve this, we propose a multidirectional meta-learning algorithm -- at the inner level, the model aims to learn the manifold of the normal data (representation); at the outer level, the model is meta-tuned with a few anomaly samples to maximize the softmax confidence margin between the normal and anomaly samples (decision surface calibration), treating normals as in-distribution (ID) and anomalies as out-of-distribution (OOD). By iteratively repeating this process over multiple episodes of predominantly normal and a small number of anomaly samples, we realize a multidirectional meta-learning framework. This two-level optimization, enhanced by multidirectional training, enables stronger generalization to unseen anomaly classes.", "link": "http://arxiv.org/abs/2601.19833v1", "date": "2026-01-27", "relevancy": 2.1733, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5467}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5414}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-directional%20Meta-Learning%20Framework%20for%20Class-Generalizable%20Anomaly%20Detection&body=Title%3A%20A%20Multi-directional%20Meta-Learning%20Framework%20for%20Class-Generalizable%20Anomaly%20Detection%0AAuthor%3A%20Padmaksha%20Roy%20and%20Lamine%20Mili%20and%20Almuatazbellah%20Boker%0AAbstract%3A%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20class-generalizable%20anomaly%20detection%2C%20where%20the%20objective%20is%20to%20develop%20a%20unified%20model%20by%20focusing%20our%20learning%20on%20the%20available%20normal%20data%20and%20a%20small%20amount%20of%20anomaly%20data%20in%20order%20to%20detect%20the%20completely%20unseen%20anomalies%2C%20also%20referred%20to%20as%20the%20out-of-distribution%20%28OOD%29%20classes.%20Adding%20to%20this%20challenge%20is%20the%20fact%20that%20the%20anomaly%20data%20is%20rare%20and%20costly%20to%20label.%20To%20achieve%20this%2C%20we%20propose%20a%20multidirectional%20meta-learning%20algorithm%20--%20at%20the%20inner%20level%2C%20the%20model%20aims%20to%20learn%20the%20manifold%20of%20the%20normal%20data%20%28representation%29%3B%20at%20the%20outer%20level%2C%20the%20model%20is%20meta-tuned%20with%20a%20few%20anomaly%20samples%20to%20maximize%20the%20softmax%20confidence%20margin%20between%20the%20normal%20and%20anomaly%20samples%20%28decision%20surface%20calibration%29%2C%20treating%20normals%20as%20in-distribution%20%28ID%29%20and%20anomalies%20as%20out-of-distribution%20%28OOD%29.%20By%20iteratively%20repeating%20this%20process%20over%20multiple%20episodes%20of%20predominantly%20normal%20and%20a%20small%20number%20of%20anomaly%20samples%2C%20we%20realize%20a%20multidirectional%20meta-learning%20framework.%20This%20two-level%20optimization%2C%20enhanced%20by%20multidirectional%20training%2C%20enables%20stronger%20generalization%20to%20unseen%20anomaly%20classes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-directional%2520Meta-Learning%2520Framework%2520for%2520Class-Generalizable%2520Anomaly%2520Detection%26entry.906535625%3DPadmaksha%2520Roy%2520and%2520Lamine%2520Mili%2520and%2520Almuatazbellah%2520Boker%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520address%2520the%2520problem%2520of%2520class-generalizable%2520anomaly%2520detection%252C%2520where%2520the%2520objective%2520is%2520to%2520develop%2520a%2520unified%2520model%2520by%2520focusing%2520our%2520learning%2520on%2520the%2520available%2520normal%2520data%2520and%2520a%2520small%2520amount%2520of%2520anomaly%2520data%2520in%2520order%2520to%2520detect%2520the%2520completely%2520unseen%2520anomalies%252C%2520also%2520referred%2520to%2520as%2520the%2520out-of-distribution%2520%2528OOD%2529%2520classes.%2520Adding%2520to%2520this%2520challenge%2520is%2520the%2520fact%2520that%2520the%2520anomaly%2520data%2520is%2520rare%2520and%2520costly%2520to%2520label.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520multidirectional%2520meta-learning%2520algorithm%2520--%2520at%2520the%2520inner%2520level%252C%2520the%2520model%2520aims%2520to%2520learn%2520the%2520manifold%2520of%2520the%2520normal%2520data%2520%2528representation%2529%253B%2520at%2520the%2520outer%2520level%252C%2520the%2520model%2520is%2520meta-tuned%2520with%2520a%2520few%2520anomaly%2520samples%2520to%2520maximize%2520the%2520softmax%2520confidence%2520margin%2520between%2520the%2520normal%2520and%2520anomaly%2520samples%2520%2528decision%2520surface%2520calibration%2529%252C%2520treating%2520normals%2520as%2520in-distribution%2520%2528ID%2529%2520and%2520anomalies%2520as%2520out-of-distribution%2520%2528OOD%2529.%2520By%2520iteratively%2520repeating%2520this%2520process%2520over%2520multiple%2520episodes%2520of%2520predominantly%2520normal%2520and%2520a%2520small%2520number%2520of%2520anomaly%2520samples%252C%2520we%2520realize%2520a%2520multidirectional%2520meta-learning%2520framework.%2520This%2520two-level%2520optimization%252C%2520enhanced%2520by%2520multidirectional%2520training%252C%2520enables%2520stronger%2520generalization%2520to%2520unseen%2520anomaly%2520classes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-directional%20Meta-Learning%20Framework%20for%20Class-Generalizable%20Anomaly%20Detection&entry.906535625=Padmaksha%20Roy%20and%20Lamine%20Mili%20and%20Almuatazbellah%20Boker&entry.1292438233=In%20this%20paper%2C%20we%20address%20the%20problem%20of%20class-generalizable%20anomaly%20detection%2C%20where%20the%20objective%20is%20to%20develop%20a%20unified%20model%20by%20focusing%20our%20learning%20on%20the%20available%20normal%20data%20and%20a%20small%20amount%20of%20anomaly%20data%20in%20order%20to%20detect%20the%20completely%20unseen%20anomalies%2C%20also%20referred%20to%20as%20the%20out-of-distribution%20%28OOD%29%20classes.%20Adding%20to%20this%20challenge%20is%20the%20fact%20that%20the%20anomaly%20data%20is%20rare%20and%20costly%20to%20label.%20To%20achieve%20this%2C%20we%20propose%20a%20multidirectional%20meta-learning%20algorithm%20--%20at%20the%20inner%20level%2C%20the%20model%20aims%20to%20learn%20the%20manifold%20of%20the%20normal%20data%20%28representation%29%3B%20at%20the%20outer%20level%2C%20the%20model%20is%20meta-tuned%20with%20a%20few%20anomaly%20samples%20to%20maximize%20the%20softmax%20confidence%20margin%20between%20the%20normal%20and%20anomaly%20samples%20%28decision%20surface%20calibration%29%2C%20treating%20normals%20as%20in-distribution%20%28ID%29%20and%20anomalies%20as%20out-of-distribution%20%28OOD%29.%20By%20iteratively%20repeating%20this%20process%20over%20multiple%20episodes%20of%20predominantly%20normal%20and%20a%20small%20number%20of%20anomaly%20samples%2C%20we%20realize%20a%20multidirectional%20meta-learning%20framework.%20This%20two-level%20optimization%2C%20enhanced%20by%20multidirectional%20training%2C%20enables%20stronger%20generalization%20to%20unseen%20anomaly%20classes.&entry.1838667208=http%3A//arxiv.org/abs/2601.19833v1&entry.124074799=Read"},
{"title": "CBMC-V3: A CNS-inspired Control Framework Towards Agile Manipulation with SNN", "author": "Yanbo Pang and Qingkai Li and Mingguo Zhao", "abstract": "As robotic arm applications expand beyond traditional industrial settings into service-oriented domains such as catering, household and retail, existing control algorithms struggle to achieve the level of agile manipulation required in unstructured environments characterized by dynamic trajectories, unpredictable interactions, and diverse objects. This paper presents a biomimetic control framework based on Spiking Neural Network (SNN), inspired by the human Central Nervous System (CNS), to address these challenges. The proposed framework comprises five control modules-cerebral cortex, cerebellum, thalamus, brainstem, and spinal cord-organized into three hierarchical control levels (first-order, second-order, and third-order) and two information pathways (ascending and descending). All modules are fully implemented using SNN. The framework is validated through both simulation and experiments on a commercial robotic arm platform across a range of control tasks. The results demonstrate that the proposed method outperforms the baseline in terms of agile motion control capability, offering a practical and effective solution for achieving agile manipulation.", "link": "http://arxiv.org/abs/2511.04109v3", "date": "2026-01-27", "relevancy": 2.1689, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5601}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5452}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CBMC-V3%3A%20A%20CNS-inspired%20Control%20Framework%20Towards%20Agile%20Manipulation%20with%20SNN&body=Title%3A%20CBMC-V3%3A%20A%20CNS-inspired%20Control%20Framework%20Towards%20Agile%20Manipulation%20with%20SNN%0AAuthor%3A%20Yanbo%20Pang%20and%20Qingkai%20Li%20and%20Mingguo%20Zhao%0AAbstract%3A%20As%20robotic%20arm%20applications%20expand%20beyond%20traditional%20industrial%20settings%20into%20service-oriented%20domains%20such%20as%20catering%2C%20household%20and%20retail%2C%20existing%20control%20algorithms%20struggle%20to%20achieve%20the%20level%20of%20agile%20manipulation%20required%20in%20unstructured%20environments%20characterized%20by%20dynamic%20trajectories%2C%20unpredictable%20interactions%2C%20and%20diverse%20objects.%20This%20paper%20presents%20a%20biomimetic%20control%20framework%20based%20on%20Spiking%20Neural%20Network%20%28SNN%29%2C%20inspired%20by%20the%20human%20Central%20Nervous%20System%20%28CNS%29%2C%20to%20address%20these%20challenges.%20The%20proposed%20framework%20comprises%20five%20control%20modules-cerebral%20cortex%2C%20cerebellum%2C%20thalamus%2C%20brainstem%2C%20and%20spinal%20cord-organized%20into%20three%20hierarchical%20control%20levels%20%28first-order%2C%20second-order%2C%20and%20third-order%29%20and%20two%20information%20pathways%20%28ascending%20and%20descending%29.%20All%20modules%20are%20fully%20implemented%20using%20SNN.%20The%20framework%20is%20validated%20through%20both%20simulation%20and%20experiments%20on%20a%20commercial%20robotic%20arm%20platform%20across%20a%20range%20of%20control%20tasks.%20The%20results%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%20baseline%20in%20terms%20of%20agile%20motion%20control%20capability%2C%20offering%20a%20practical%20and%20effective%20solution%20for%20achieving%20agile%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.04109v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCBMC-V3%253A%2520A%2520CNS-inspired%2520Control%2520Framework%2520Towards%2520Agile%2520Manipulation%2520with%2520SNN%26entry.906535625%3DYanbo%2520Pang%2520and%2520Qingkai%2520Li%2520and%2520Mingguo%2520Zhao%26entry.1292438233%3DAs%2520robotic%2520arm%2520applications%2520expand%2520beyond%2520traditional%2520industrial%2520settings%2520into%2520service-oriented%2520domains%2520such%2520as%2520catering%252C%2520household%2520and%2520retail%252C%2520existing%2520control%2520algorithms%2520struggle%2520to%2520achieve%2520the%2520level%2520of%2520agile%2520manipulation%2520required%2520in%2520unstructured%2520environments%2520characterized%2520by%2520dynamic%2520trajectories%252C%2520unpredictable%2520interactions%252C%2520and%2520diverse%2520objects.%2520This%2520paper%2520presents%2520a%2520biomimetic%2520control%2520framework%2520based%2520on%2520Spiking%2520Neural%2520Network%2520%2528SNN%2529%252C%2520inspired%2520by%2520the%2520human%2520Central%2520Nervous%2520System%2520%2528CNS%2529%252C%2520to%2520address%2520these%2520challenges.%2520The%2520proposed%2520framework%2520comprises%2520five%2520control%2520modules-cerebral%2520cortex%252C%2520cerebellum%252C%2520thalamus%252C%2520brainstem%252C%2520and%2520spinal%2520cord-organized%2520into%2520three%2520hierarchical%2520control%2520levels%2520%2528first-order%252C%2520second-order%252C%2520and%2520third-order%2529%2520and%2520two%2520information%2520pathways%2520%2528ascending%2520and%2520descending%2529.%2520All%2520modules%2520are%2520fully%2520implemented%2520using%2520SNN.%2520The%2520framework%2520is%2520validated%2520through%2520both%2520simulation%2520and%2520experiments%2520on%2520a%2520commercial%2520robotic%2520arm%2520platform%2520across%2520a%2520range%2520of%2520control%2520tasks.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%2520baseline%2520in%2520terms%2520of%2520agile%2520motion%2520control%2520capability%252C%2520offering%2520a%2520practical%2520and%2520effective%2520solution%2520for%2520achieving%2520agile%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.04109v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBMC-V3%3A%20A%20CNS-inspired%20Control%20Framework%20Towards%20Agile%20Manipulation%20with%20SNN&entry.906535625=Yanbo%20Pang%20and%20Qingkai%20Li%20and%20Mingguo%20Zhao&entry.1292438233=As%20robotic%20arm%20applications%20expand%20beyond%20traditional%20industrial%20settings%20into%20service-oriented%20domains%20such%20as%20catering%2C%20household%20and%20retail%2C%20existing%20control%20algorithms%20struggle%20to%20achieve%20the%20level%20of%20agile%20manipulation%20required%20in%20unstructured%20environments%20characterized%20by%20dynamic%20trajectories%2C%20unpredictable%20interactions%2C%20and%20diverse%20objects.%20This%20paper%20presents%20a%20biomimetic%20control%20framework%20based%20on%20Spiking%20Neural%20Network%20%28SNN%29%2C%20inspired%20by%20the%20human%20Central%20Nervous%20System%20%28CNS%29%2C%20to%20address%20these%20challenges.%20The%20proposed%20framework%20comprises%20five%20control%20modules-cerebral%20cortex%2C%20cerebellum%2C%20thalamus%2C%20brainstem%2C%20and%20spinal%20cord-organized%20into%20three%20hierarchical%20control%20levels%20%28first-order%2C%20second-order%2C%20and%20third-order%29%20and%20two%20information%20pathways%20%28ascending%20and%20descending%29.%20All%20modules%20are%20fully%20implemented%20using%20SNN.%20The%20framework%20is%20validated%20through%20both%20simulation%20and%20experiments%20on%20a%20commercial%20robotic%20arm%20platform%20across%20a%20range%20of%20control%20tasks.%20The%20results%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%20baseline%20in%20terms%20of%20agile%20motion%20control%20capability%2C%20offering%20a%20practical%20and%20effective%20solution%20for%20achieving%20agile%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2511.04109v3&entry.124074799=Read"},
{"title": "To Grok Grokking: Provable Grokking in Ridge Regression", "author": "Mingyue Xu and Gal Vardi and Itay Safran", "abstract": "We study grokking, the onset of generalization long after overfitting, in a classical ridge regression setting. We prove end-to-end grokking results for learning over-parameterized linear regression models using gradient descent with weight decay. Specifically, we prove that the following stages occur: (i) the model overfits the training data early during training; (ii) poor generalization persists long after overfitting has manifested; and (iii) the generalization error eventually becomes arbitrarily small. Moreover, we show, both theoretically and empirically, that grokking can be amplified or eliminated in a principled manner through proper hyperparameter tuning. To the best of our knowledge, these are the first rigorous quantitative bounds on the generalization delay (which we refer to as the \"grokking time\") in terms of training hyperparameters. Lastly, going beyond the linear setting, we empirically demonstrate that our quantitative bounds also capture the behavior of grokking on non-linear neural networks. Our results suggest that grokking is not an inherent failure mode of deep learning, but rather a consequence of specific training conditions, and thus does not require fundamental changes to the model architecture or learning algorithm to avoid.", "link": "http://arxiv.org/abs/2601.19791v1", "date": "2026-01-27", "relevancy": 2.1673, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4525}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4283}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Grok%20Grokking%3A%20Provable%20Grokking%20in%20Ridge%20Regression&body=Title%3A%20To%20Grok%20Grokking%3A%20Provable%20Grokking%20in%20Ridge%20Regression%0AAuthor%3A%20Mingyue%20Xu%20and%20Gal%20Vardi%20and%20Itay%20Safran%0AAbstract%3A%20We%20study%20grokking%2C%20the%20onset%20of%20generalization%20long%20after%20overfitting%2C%20in%20a%20classical%20ridge%20regression%20setting.%20We%20prove%20end-to-end%20grokking%20results%20for%20learning%20over-parameterized%20linear%20regression%20models%20using%20gradient%20descent%20with%20weight%20decay.%20Specifically%2C%20we%20prove%20that%20the%20following%20stages%20occur%3A%20%28i%29%20the%20model%20overfits%20the%20training%20data%20early%20during%20training%3B%20%28ii%29%20poor%20generalization%20persists%20long%20after%20overfitting%20has%20manifested%3B%20and%20%28iii%29%20the%20generalization%20error%20eventually%20becomes%20arbitrarily%20small.%20Moreover%2C%20we%20show%2C%20both%20theoretically%20and%20empirically%2C%20that%20grokking%20can%20be%20amplified%20or%20eliminated%20in%20a%20principled%20manner%20through%20proper%20hyperparameter%20tuning.%20To%20the%20best%20of%20our%20knowledge%2C%20these%20are%20the%20first%20rigorous%20quantitative%20bounds%20on%20the%20generalization%20delay%20%28which%20we%20refer%20to%20as%20the%20%22grokking%20time%22%29%20in%20terms%20of%20training%20hyperparameters.%20Lastly%2C%20going%20beyond%20the%20linear%20setting%2C%20we%20empirically%20demonstrate%20that%20our%20quantitative%20bounds%20also%20capture%20the%20behavior%20of%20grokking%20on%20non-linear%20neural%20networks.%20Our%20results%20suggest%20that%20grokking%20is%20not%20an%20inherent%20failure%20mode%20of%20deep%20learning%2C%20but%20rather%20a%20consequence%20of%20specific%20training%20conditions%2C%20and%20thus%20does%20not%20require%20fundamental%20changes%20to%20the%20model%20architecture%20or%20learning%20algorithm%20to%20avoid.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Grok%2520Grokking%253A%2520Provable%2520Grokking%2520in%2520Ridge%2520Regression%26entry.906535625%3DMingyue%2520Xu%2520and%2520Gal%2520Vardi%2520and%2520Itay%2520Safran%26entry.1292438233%3DWe%2520study%2520grokking%252C%2520the%2520onset%2520of%2520generalization%2520long%2520after%2520overfitting%252C%2520in%2520a%2520classical%2520ridge%2520regression%2520setting.%2520We%2520prove%2520end-to-end%2520grokking%2520results%2520for%2520learning%2520over-parameterized%2520linear%2520regression%2520models%2520using%2520gradient%2520descent%2520with%2520weight%2520decay.%2520Specifically%252C%2520we%2520prove%2520that%2520the%2520following%2520stages%2520occur%253A%2520%2528i%2529%2520the%2520model%2520overfits%2520the%2520training%2520data%2520early%2520during%2520training%253B%2520%2528ii%2529%2520poor%2520generalization%2520persists%2520long%2520after%2520overfitting%2520has%2520manifested%253B%2520and%2520%2528iii%2529%2520the%2520generalization%2520error%2520eventually%2520becomes%2520arbitrarily%2520small.%2520Moreover%252C%2520we%2520show%252C%2520both%2520theoretically%2520and%2520empirically%252C%2520that%2520grokking%2520can%2520be%2520amplified%2520or%2520eliminated%2520in%2520a%2520principled%2520manner%2520through%2520proper%2520hyperparameter%2520tuning.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520these%2520are%2520the%2520first%2520rigorous%2520quantitative%2520bounds%2520on%2520the%2520generalization%2520delay%2520%2528which%2520we%2520refer%2520to%2520as%2520the%2520%2522grokking%2520time%2522%2529%2520in%2520terms%2520of%2520training%2520hyperparameters.%2520Lastly%252C%2520going%2520beyond%2520the%2520linear%2520setting%252C%2520we%2520empirically%2520demonstrate%2520that%2520our%2520quantitative%2520bounds%2520also%2520capture%2520the%2520behavior%2520of%2520grokking%2520on%2520non-linear%2520neural%2520networks.%2520Our%2520results%2520suggest%2520that%2520grokking%2520is%2520not%2520an%2520inherent%2520failure%2520mode%2520of%2520deep%2520learning%252C%2520but%2520rather%2520a%2520consequence%2520of%2520specific%2520training%2520conditions%252C%2520and%2520thus%2520does%2520not%2520require%2520fundamental%2520changes%2520to%2520the%2520model%2520architecture%2520or%2520learning%2520algorithm%2520to%2520avoid.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Grok%20Grokking%3A%20Provable%20Grokking%20in%20Ridge%20Regression&entry.906535625=Mingyue%20Xu%20and%20Gal%20Vardi%20and%20Itay%20Safran&entry.1292438233=We%20study%20grokking%2C%20the%20onset%20of%20generalization%20long%20after%20overfitting%2C%20in%20a%20classical%20ridge%20regression%20setting.%20We%20prove%20end-to-end%20grokking%20results%20for%20learning%20over-parameterized%20linear%20regression%20models%20using%20gradient%20descent%20with%20weight%20decay.%20Specifically%2C%20we%20prove%20that%20the%20following%20stages%20occur%3A%20%28i%29%20the%20model%20overfits%20the%20training%20data%20early%20during%20training%3B%20%28ii%29%20poor%20generalization%20persists%20long%20after%20overfitting%20has%20manifested%3B%20and%20%28iii%29%20the%20generalization%20error%20eventually%20becomes%20arbitrarily%20small.%20Moreover%2C%20we%20show%2C%20both%20theoretically%20and%20empirically%2C%20that%20grokking%20can%20be%20amplified%20or%20eliminated%20in%20a%20principled%20manner%20through%20proper%20hyperparameter%20tuning.%20To%20the%20best%20of%20our%20knowledge%2C%20these%20are%20the%20first%20rigorous%20quantitative%20bounds%20on%20the%20generalization%20delay%20%28which%20we%20refer%20to%20as%20the%20%22grokking%20time%22%29%20in%20terms%20of%20training%20hyperparameters.%20Lastly%2C%20going%20beyond%20the%20linear%20setting%2C%20we%20empirically%20demonstrate%20that%20our%20quantitative%20bounds%20also%20capture%20the%20behavior%20of%20grokking%20on%20non-linear%20neural%20networks.%20Our%20results%20suggest%20that%20grokking%20is%20not%20an%20inherent%20failure%20mode%20of%20deep%20learning%2C%20but%20rather%20a%20consequence%20of%20specific%20training%20conditions%2C%20and%20thus%20does%20not%20require%20fundamental%20changes%20to%20the%20model%20architecture%20or%20learning%20algorithm%20to%20avoid.&entry.1838667208=http%3A//arxiv.org/abs/2601.19791v1&entry.124074799=Read"},
{"title": "SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking", "author": "Adam Remaki and Christel G\u00e9rardin and Eul\u00e0lia Farr\u00e9-Maduell and Martin Krallinger and Xavier Tannier", "abstract": "We present SynCABEL (Synthetic Contextualized Augmentation for Biomedical Entity Linking), a framework that addresses a central bottleneck in supervised biomedical entity linking (BEL): the scarcity of expert-annotated training data. SynCABEL leverages large language models to generate context-rich synthetic training examples for all candidate concepts in a target knowledge base, providing broad supervision without manual annotation. We demonstrate that SynCABEL, when combined with decoder-only models and guided inference establish new state-of-the-art results across three widely used multilingual benchmarks: MedMentions for English, QUAERO for French, and SPACCC for Spanish. Evaluating data efficiency, we show that SynCABEL reaches the performance of full human supervision using up to 60% less annotated data, substantially reducing reliance on labor-intensive and costly expert labeling. Finally, acknowledging that standard evaluation based on exact code matching often underestimates clinically valid predictions due to ontology redundancy, we introduce an LLM-as-a-judge protocol. This analysis reveals that SynCABEL significantly improves the rate of clinically valid predictions. Our synthetic datasets, models, and code are released to support reproducibility and future research.", "link": "http://arxiv.org/abs/2601.19667v1", "date": "2026-01-27", "relevancy": 2.1622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynCABEL%3A%20Synthetic%20Contextualized%20Augmentation%20for%20Biomedical%20Entity%20Linking&body=Title%3A%20SynCABEL%3A%20Synthetic%20Contextualized%20Augmentation%20for%20Biomedical%20Entity%20Linking%0AAuthor%3A%20Adam%20Remaki%20and%20Christel%20G%C3%A9rardin%20and%20Eul%C3%A0lia%20Farr%C3%A9-Maduell%20and%20Martin%20Krallinger%20and%20Xavier%20Tannier%0AAbstract%3A%20We%20present%20SynCABEL%20%28Synthetic%20Contextualized%20Augmentation%20for%20Biomedical%20Entity%20Linking%29%2C%20a%20framework%20that%20addresses%20a%20central%20bottleneck%20in%20supervised%20biomedical%20entity%20linking%20%28BEL%29%3A%20the%20scarcity%20of%20expert-annotated%20training%20data.%20SynCABEL%20leverages%20large%20language%20models%20to%20generate%20context-rich%20synthetic%20training%20examples%20for%20all%20candidate%20concepts%20in%20a%20target%20knowledge%20base%2C%20providing%20broad%20supervision%20without%20manual%20annotation.%20We%20demonstrate%20that%20SynCABEL%2C%20when%20combined%20with%20decoder-only%20models%20and%20guided%20inference%20establish%20new%20state-of-the-art%20results%20across%20three%20widely%20used%20multilingual%20benchmarks%3A%20MedMentions%20for%20English%2C%20QUAERO%20for%20French%2C%20and%20SPACCC%20for%20Spanish.%20Evaluating%20data%20efficiency%2C%20we%20show%20that%20SynCABEL%20reaches%20the%20performance%20of%20full%20human%20supervision%20using%20up%20to%2060%25%20less%20annotated%20data%2C%20substantially%20reducing%20reliance%20on%20labor-intensive%20and%20costly%20expert%20labeling.%20Finally%2C%20acknowledging%20that%20standard%20evaluation%20based%20on%20exact%20code%20matching%20often%20underestimates%20clinically%20valid%20predictions%20due%20to%20ontology%20redundancy%2C%20we%20introduce%20an%20LLM-as-a-judge%20protocol.%20This%20analysis%20reveals%20that%20SynCABEL%20significantly%20improves%20the%20rate%20of%20clinically%20valid%20predictions.%20Our%20synthetic%20datasets%2C%20models%2C%20and%20code%20are%20released%20to%20support%20reproducibility%20and%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynCABEL%253A%2520Synthetic%2520Contextualized%2520Augmentation%2520for%2520Biomedical%2520Entity%2520Linking%26entry.906535625%3DAdam%2520Remaki%2520and%2520Christel%2520G%25C3%25A9rardin%2520and%2520Eul%25C3%25A0lia%2520Farr%25C3%25A9-Maduell%2520and%2520Martin%2520Krallinger%2520and%2520Xavier%2520Tannier%26entry.1292438233%3DWe%2520present%2520SynCABEL%2520%2528Synthetic%2520Contextualized%2520Augmentation%2520for%2520Biomedical%2520Entity%2520Linking%2529%252C%2520a%2520framework%2520that%2520addresses%2520a%2520central%2520bottleneck%2520in%2520supervised%2520biomedical%2520entity%2520linking%2520%2528BEL%2529%253A%2520the%2520scarcity%2520of%2520expert-annotated%2520training%2520data.%2520SynCABEL%2520leverages%2520large%2520language%2520models%2520to%2520generate%2520context-rich%2520synthetic%2520training%2520examples%2520for%2520all%2520candidate%2520concepts%2520in%2520a%2520target%2520knowledge%2520base%252C%2520providing%2520broad%2520supervision%2520without%2520manual%2520annotation.%2520We%2520demonstrate%2520that%2520SynCABEL%252C%2520when%2520combined%2520with%2520decoder-only%2520models%2520and%2520guided%2520inference%2520establish%2520new%2520state-of-the-art%2520results%2520across%2520three%2520widely%2520used%2520multilingual%2520benchmarks%253A%2520MedMentions%2520for%2520English%252C%2520QUAERO%2520for%2520French%252C%2520and%2520SPACCC%2520for%2520Spanish.%2520Evaluating%2520data%2520efficiency%252C%2520we%2520show%2520that%2520SynCABEL%2520reaches%2520the%2520performance%2520of%2520full%2520human%2520supervision%2520using%2520up%2520to%252060%2525%2520less%2520annotated%2520data%252C%2520substantially%2520reducing%2520reliance%2520on%2520labor-intensive%2520and%2520costly%2520expert%2520labeling.%2520Finally%252C%2520acknowledging%2520that%2520standard%2520evaluation%2520based%2520on%2520exact%2520code%2520matching%2520often%2520underestimates%2520clinically%2520valid%2520predictions%2520due%2520to%2520ontology%2520redundancy%252C%2520we%2520introduce%2520an%2520LLM-as-a-judge%2520protocol.%2520This%2520analysis%2520reveals%2520that%2520SynCABEL%2520significantly%2520improves%2520the%2520rate%2520of%2520clinically%2520valid%2520predictions.%2520Our%2520synthetic%2520datasets%252C%2520models%252C%2520and%2520code%2520are%2520released%2520to%2520support%2520reproducibility%2520and%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynCABEL%3A%20Synthetic%20Contextualized%20Augmentation%20for%20Biomedical%20Entity%20Linking&entry.906535625=Adam%20Remaki%20and%20Christel%20G%C3%A9rardin%20and%20Eul%C3%A0lia%20Farr%C3%A9-Maduell%20and%20Martin%20Krallinger%20and%20Xavier%20Tannier&entry.1292438233=We%20present%20SynCABEL%20%28Synthetic%20Contextualized%20Augmentation%20for%20Biomedical%20Entity%20Linking%29%2C%20a%20framework%20that%20addresses%20a%20central%20bottleneck%20in%20supervised%20biomedical%20entity%20linking%20%28BEL%29%3A%20the%20scarcity%20of%20expert-annotated%20training%20data.%20SynCABEL%20leverages%20large%20language%20models%20to%20generate%20context-rich%20synthetic%20training%20examples%20for%20all%20candidate%20concepts%20in%20a%20target%20knowledge%20base%2C%20providing%20broad%20supervision%20without%20manual%20annotation.%20We%20demonstrate%20that%20SynCABEL%2C%20when%20combined%20with%20decoder-only%20models%20and%20guided%20inference%20establish%20new%20state-of-the-art%20results%20across%20three%20widely%20used%20multilingual%20benchmarks%3A%20MedMentions%20for%20English%2C%20QUAERO%20for%20French%2C%20and%20SPACCC%20for%20Spanish.%20Evaluating%20data%20efficiency%2C%20we%20show%20that%20SynCABEL%20reaches%20the%20performance%20of%20full%20human%20supervision%20using%20up%20to%2060%25%20less%20annotated%20data%2C%20substantially%20reducing%20reliance%20on%20labor-intensive%20and%20costly%20expert%20labeling.%20Finally%2C%20acknowledging%20that%20standard%20evaluation%20based%20on%20exact%20code%20matching%20often%20underestimates%20clinically%20valid%20predictions%20due%20to%20ontology%20redundancy%2C%20we%20introduce%20an%20LLM-as-a-judge%20protocol.%20This%20analysis%20reveals%20that%20SynCABEL%20significantly%20improves%20the%20rate%20of%20clinically%20valid%20predictions.%20Our%20synthetic%20datasets%2C%20models%2C%20and%20code%20are%20released%20to%20support%20reproducibility%20and%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.19667v1&entry.124074799=Read"},
{"title": "CNN-based IoT Device Identification: A Comparative Study on Payload vs. Fingerprint", "author": "Kahraman Kostas", "abstract": "The proliferation of the Internet of Things (IoT) has introduced a massive influx of devices into the market, bringing with them significant security vulnerabilities. In this diverse ecosystem, robust IoT device identification is a critical preventive measure for network security and vulnerability management. This study proposes a deep learning-based method to identify IoT devices using the Aalto dataset. We employ Convolutional Neural Networks (CNN) to classify devices by converting network packet payloads into pseudo-images. Furthermore, we compare the performance of this payload-based approach against a feature-based fingerprinting method. Our results indicate that while the fingerprint-based method is significantly faster (approximately 10x), the payload-based image classification achieves comparable accuracy, highlighting the trade-offs between computational efficiency and data granularity in IoT security.", "link": "http://arxiv.org/abs/2304.13894v2", "date": "2026-01-27", "relevancy": 2.144, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4335}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4308}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNN-based%20IoT%20Device%20Identification%3A%20A%20Comparative%20Study%20on%20Payload%20vs.%20Fingerprint&body=Title%3A%20CNN-based%20IoT%20Device%20Identification%3A%20A%20Comparative%20Study%20on%20Payload%20vs.%20Fingerprint%0AAuthor%3A%20Kahraman%20Kostas%0AAbstract%3A%20The%20proliferation%20of%20the%20Internet%20of%20Things%20%28IoT%29%20has%20introduced%20a%20massive%20influx%20of%20devices%20into%20the%20market%2C%20bringing%20with%20them%20significant%20security%20vulnerabilities.%20In%20this%20diverse%20ecosystem%2C%20robust%20IoT%20device%20identification%20is%20a%20critical%20preventive%20measure%20for%20network%20security%20and%20vulnerability%20management.%20This%20study%20proposes%20a%20deep%20learning-based%20method%20to%20identify%20IoT%20devices%20using%20the%20Aalto%20dataset.%20We%20employ%20Convolutional%20Neural%20Networks%20%28CNN%29%20to%20classify%20devices%20by%20converting%20network%20packet%20payloads%20into%20pseudo-images.%20Furthermore%2C%20we%20compare%20the%20performance%20of%20this%20payload-based%20approach%20against%20a%20feature-based%20fingerprinting%20method.%20Our%20results%20indicate%20that%20while%20the%20fingerprint-based%20method%20is%20significantly%20faster%20%28approximately%2010x%29%2C%20the%20payload-based%20image%20classification%20achieves%20comparable%20accuracy%2C%20highlighting%20the%20trade-offs%20between%20computational%20efficiency%20and%20data%20granularity%20in%20IoT%20security.%0ALink%3A%20http%3A//arxiv.org/abs/2304.13894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNN-based%2520IoT%2520Device%2520Identification%253A%2520A%2520Comparative%2520Study%2520on%2520Payload%2520vs.%2520Fingerprint%26entry.906535625%3DKahraman%2520Kostas%26entry.1292438233%3DThe%2520proliferation%2520of%2520the%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520has%2520introduced%2520a%2520massive%2520influx%2520of%2520devices%2520into%2520the%2520market%252C%2520bringing%2520with%2520them%2520significant%2520security%2520vulnerabilities.%2520In%2520this%2520diverse%2520ecosystem%252C%2520robust%2520IoT%2520device%2520identification%2520is%2520a%2520critical%2520preventive%2520measure%2520for%2520network%2520security%2520and%2520vulnerability%2520management.%2520This%2520study%2520proposes%2520a%2520deep%2520learning-based%2520method%2520to%2520identify%2520IoT%2520devices%2520using%2520the%2520Aalto%2520dataset.%2520We%2520employ%2520Convolutional%2520Neural%2520Networks%2520%2528CNN%2529%2520to%2520classify%2520devices%2520by%2520converting%2520network%2520packet%2520payloads%2520into%2520pseudo-images.%2520Furthermore%252C%2520we%2520compare%2520the%2520performance%2520of%2520this%2520payload-based%2520approach%2520against%2520a%2520feature-based%2520fingerprinting%2520method.%2520Our%2520results%2520indicate%2520that%2520while%2520the%2520fingerprint-based%2520method%2520is%2520significantly%2520faster%2520%2528approximately%252010x%2529%252C%2520the%2520payload-based%2520image%2520classification%2520achieves%2520comparable%2520accuracy%252C%2520highlighting%2520the%2520trade-offs%2520between%2520computational%2520efficiency%2520and%2520data%2520granularity%2520in%2520IoT%2520security.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.13894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNN-based%20IoT%20Device%20Identification%3A%20A%20Comparative%20Study%20on%20Payload%20vs.%20Fingerprint&entry.906535625=Kahraman%20Kostas&entry.1292438233=The%20proliferation%20of%20the%20Internet%20of%20Things%20%28IoT%29%20has%20introduced%20a%20massive%20influx%20of%20devices%20into%20the%20market%2C%20bringing%20with%20them%20significant%20security%20vulnerabilities.%20In%20this%20diverse%20ecosystem%2C%20robust%20IoT%20device%20identification%20is%20a%20critical%20preventive%20measure%20for%20network%20security%20and%20vulnerability%20management.%20This%20study%20proposes%20a%20deep%20learning-based%20method%20to%20identify%20IoT%20devices%20using%20the%20Aalto%20dataset.%20We%20employ%20Convolutional%20Neural%20Networks%20%28CNN%29%20to%20classify%20devices%20by%20converting%20network%20packet%20payloads%20into%20pseudo-images.%20Furthermore%2C%20we%20compare%20the%20performance%20of%20this%20payload-based%20approach%20against%20a%20feature-based%20fingerprinting%20method.%20Our%20results%20indicate%20that%20while%20the%20fingerprint-based%20method%20is%20significantly%20faster%20%28approximately%2010x%29%2C%20the%20payload-based%20image%20classification%20achieves%20comparable%20accuracy%2C%20highlighting%20the%20trade-offs%20between%20computational%20efficiency%20and%20data%20granularity%20in%20IoT%20security.&entry.1838667208=http%3A//arxiv.org/abs/2304.13894v2&entry.124074799=Read"},
{"title": "Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling", "author": "Jingren Hou and Hong Wang and Pengyu Xu and Chang Gao and Huafeng Liu and Liping Jing", "abstract": "Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator(LANO) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. LANO achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.", "link": "http://arxiv.org/abs/2601.15547v2", "date": "2026-01-27", "relevancy": 2.1417, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5614}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5332}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Neural%20Operators%20from%20Partial%20Observations%20via%20Latent%20Autoregressive%20Modeling&body=Title%3A%20Learning%20Neural%20Operators%20from%20Partial%20Observations%20via%20Latent%20Autoregressive%20Modeling%0AAuthor%3A%20Jingren%20Hou%20and%20Hong%20Wang%20and%20Pengyu%20Xu%20and%20Chang%20Gao%20and%20Huafeng%20Liu%20and%20Liping%20Jing%0AAbstract%3A%20Real-world%20scientific%20applications%20frequently%20encounter%20incomplete%20observational%20data%20due%20to%20sensor%20limitations%2C%20geographic%20constraints%2C%20or%20measurement%20costs.%20Although%20neural%20operators%20significantly%20advanced%20PDE%20solving%20in%20terms%20of%20computational%20efficiency%20and%20accuracy%2C%20their%20underlying%20assumption%20of%20fully-observed%20spatial%20inputs%20severely%20restricts%20applicability%20in%20real-world%20applications.%20We%20introduce%20the%20first%20systematic%20framework%20for%20learning%20neural%20operators%20from%20partial%20observation.%20We%20identify%20and%20formalize%20two%20fundamental%20obstacles%3A%20%28i%29%20the%20supervision%20gap%20in%20unobserved%20regions%20that%20prevents%20effective%20learning%20of%20physical%20correlations%2C%20and%20%28ii%29%20the%20dynamic%20spatial%20mismatch%20between%20incomplete%20inputs%20and%20complete%20solution%20fields.%20Specifically%2C%20our%20proposed%20Latent%20Autoregressive%20Neural%20Operator%28LANO%29%20introduces%20two%20novel%20components%20designed%20explicitly%20to%20address%20the%20core%20difficulties%20of%20partial%20observations%3A%20%28i%29%20a%20mask-to-predict%20training%20strategy%20that%20creates%20artificial%20supervision%20by%20strategically%20masking%20observed%20regions%2C%20and%20%28ii%29%20a%20Physics-Aware%20Latent%20Propagator%20that%20reconstructs%20solutions%20through%20boundary-first%20autoregressive%20generation%20in%20latent%20space.%20Additionally%2C%20we%20develop%20POBench-PDE%2C%20a%20dedicated%20and%20comprehensive%20benchmark%20designed%20specifically%20for%20evaluating%20neural%20operators%20under%20partial%20observation%20conditions%20across%20three%20PDE-governed%20tasks.%20LANO%20achieves%20state-of-the-art%20performance%20with%2018--69%24%5C%25%24%20relative%20L2%20error%20reduction%20across%20all%20benchmarks%20under%20patch-wise%20missingness%20with%20less%20than%2050%24%5C%25%24%20missing%20rate%2C%20including%20real-world%20climate%20prediction.%20Our%20approach%20effectively%20addresses%20practical%20scenarios%20involving%20up%20to%2075%24%5C%25%24%20missing%20rate%2C%20to%20some%20extent%20bridging%20the%20existing%20gap%20between%20idealized%20research%20settings%20and%20the%20complexities%20of%20real-world%20scientific%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Neural%2520Operators%2520from%2520Partial%2520Observations%2520via%2520Latent%2520Autoregressive%2520Modeling%26entry.906535625%3DJingren%2520Hou%2520and%2520Hong%2520Wang%2520and%2520Pengyu%2520Xu%2520and%2520Chang%2520Gao%2520and%2520Huafeng%2520Liu%2520and%2520Liping%2520Jing%26entry.1292438233%3DReal-world%2520scientific%2520applications%2520frequently%2520encounter%2520incomplete%2520observational%2520data%2520due%2520to%2520sensor%2520limitations%252C%2520geographic%2520constraints%252C%2520or%2520measurement%2520costs.%2520Although%2520neural%2520operators%2520significantly%2520advanced%2520PDE%2520solving%2520in%2520terms%2520of%2520computational%2520efficiency%2520and%2520accuracy%252C%2520their%2520underlying%2520assumption%2520of%2520fully-observed%2520spatial%2520inputs%2520severely%2520restricts%2520applicability%2520in%2520real-world%2520applications.%2520We%2520introduce%2520the%2520first%2520systematic%2520framework%2520for%2520learning%2520neural%2520operators%2520from%2520partial%2520observation.%2520We%2520identify%2520and%2520formalize%2520two%2520fundamental%2520obstacles%253A%2520%2528i%2529%2520the%2520supervision%2520gap%2520in%2520unobserved%2520regions%2520that%2520prevents%2520effective%2520learning%2520of%2520physical%2520correlations%252C%2520and%2520%2528ii%2529%2520the%2520dynamic%2520spatial%2520mismatch%2520between%2520incomplete%2520inputs%2520and%2520complete%2520solution%2520fields.%2520Specifically%252C%2520our%2520proposed%2520Latent%2520Autoregressive%2520Neural%2520Operator%2528LANO%2529%2520introduces%2520two%2520novel%2520components%2520designed%2520explicitly%2520to%2520address%2520the%2520core%2520difficulties%2520of%2520partial%2520observations%253A%2520%2528i%2529%2520a%2520mask-to-predict%2520training%2520strategy%2520that%2520creates%2520artificial%2520supervision%2520by%2520strategically%2520masking%2520observed%2520regions%252C%2520and%2520%2528ii%2529%2520a%2520Physics-Aware%2520Latent%2520Propagator%2520that%2520reconstructs%2520solutions%2520through%2520boundary-first%2520autoregressive%2520generation%2520in%2520latent%2520space.%2520Additionally%252C%2520we%2520develop%2520POBench-PDE%252C%2520a%2520dedicated%2520and%2520comprehensive%2520benchmark%2520designed%2520specifically%2520for%2520evaluating%2520neural%2520operators%2520under%2520partial%2520observation%2520conditions%2520across%2520three%2520PDE-governed%2520tasks.%2520LANO%2520achieves%2520state-of-the-art%2520performance%2520with%252018--69%2524%255C%2525%2524%2520relative%2520L2%2520error%2520reduction%2520across%2520all%2520benchmarks%2520under%2520patch-wise%2520missingness%2520with%2520less%2520than%252050%2524%255C%2525%2524%2520missing%2520rate%252C%2520including%2520real-world%2520climate%2520prediction.%2520Our%2520approach%2520effectively%2520addresses%2520practical%2520scenarios%2520involving%2520up%2520to%252075%2524%255C%2525%2524%2520missing%2520rate%252C%2520to%2520some%2520extent%2520bridging%2520the%2520existing%2520gap%2520between%2520idealized%2520research%2520settings%2520and%2520the%2520complexities%2520of%2520real-world%2520scientific%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Neural%20Operators%20from%20Partial%20Observations%20via%20Latent%20Autoregressive%20Modeling&entry.906535625=Jingren%20Hou%20and%20Hong%20Wang%20and%20Pengyu%20Xu%20and%20Chang%20Gao%20and%20Huafeng%20Liu%20and%20Liping%20Jing&entry.1292438233=Real-world%20scientific%20applications%20frequently%20encounter%20incomplete%20observational%20data%20due%20to%20sensor%20limitations%2C%20geographic%20constraints%2C%20or%20measurement%20costs.%20Although%20neural%20operators%20significantly%20advanced%20PDE%20solving%20in%20terms%20of%20computational%20efficiency%20and%20accuracy%2C%20their%20underlying%20assumption%20of%20fully-observed%20spatial%20inputs%20severely%20restricts%20applicability%20in%20real-world%20applications.%20We%20introduce%20the%20first%20systematic%20framework%20for%20learning%20neural%20operators%20from%20partial%20observation.%20We%20identify%20and%20formalize%20two%20fundamental%20obstacles%3A%20%28i%29%20the%20supervision%20gap%20in%20unobserved%20regions%20that%20prevents%20effective%20learning%20of%20physical%20correlations%2C%20and%20%28ii%29%20the%20dynamic%20spatial%20mismatch%20between%20incomplete%20inputs%20and%20complete%20solution%20fields.%20Specifically%2C%20our%20proposed%20Latent%20Autoregressive%20Neural%20Operator%28LANO%29%20introduces%20two%20novel%20components%20designed%20explicitly%20to%20address%20the%20core%20difficulties%20of%20partial%20observations%3A%20%28i%29%20a%20mask-to-predict%20training%20strategy%20that%20creates%20artificial%20supervision%20by%20strategically%20masking%20observed%20regions%2C%20and%20%28ii%29%20a%20Physics-Aware%20Latent%20Propagator%20that%20reconstructs%20solutions%20through%20boundary-first%20autoregressive%20generation%20in%20latent%20space.%20Additionally%2C%20we%20develop%20POBench-PDE%2C%20a%20dedicated%20and%20comprehensive%20benchmark%20designed%20specifically%20for%20evaluating%20neural%20operators%20under%20partial%20observation%20conditions%20across%20three%20PDE-governed%20tasks.%20LANO%20achieves%20state-of-the-art%20performance%20with%2018--69%24%5C%25%24%20relative%20L2%20error%20reduction%20across%20all%20benchmarks%20under%20patch-wise%20missingness%20with%20less%20than%2050%24%5C%25%24%20missing%20rate%2C%20including%20real-world%20climate%20prediction.%20Our%20approach%20effectively%20addresses%20practical%20scenarios%20involving%20up%20to%2075%24%5C%25%24%20missing%20rate%2C%20to%20some%20extent%20bridging%20the%20existing%20gap%20between%20idealized%20research%20settings%20and%20the%20complexities%20of%20real-world%20scientific%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2601.15547v2&entry.124074799=Read"},
{"title": "Interpretable and backpropagation-free Green Learning for efficient multi-task echocardiographic segmentation and classification", "author": "Jyun-Ping Kao and Jiaxing Yang and C. -C. Jay Kuo and Jonghye Woo", "abstract": "Echocardiography is a cornerstone for managing heart failure (HF), with Left Ventricular Ejection Fraction (LVEF) being a critical metric for guiding therapy. However, manual LVEF assessment suffers from high inter-observer variability, while existing Deep Learning (DL) models are often computationally intensive and data-hungry \"black boxes\" that impede clinical trust and adoption. Here, we propose a backpropagation-free multi-task Green Learning (MTGL) framework that performs simultaneous Left Ventricle (LV) segmentation and LVEF classification. Our framework integrates an unsupervised VoxelHop encoder for hierarchical spatio-temporal feature extraction with a multi-level regression decoder and an XG-Boost classifier. On the EchoNet-Dynamic dataset, our MTGL model achieves state-of-the-art classification and segmentation performance, attaining a classification accuracy of 94.3% and a Dice Similarity Coefficient (DSC) of 0.912, significantly outperforming several advanced 3D DL models. Crucially, our model achieves this with over an order of magnitude fewer parameters, demonstrating exceptional computational efficiency. This work demonstrates that the GL paradigm can deliver highly accurate, efficient, and interpretable solutions for complex medical image analysis, paving the way for more sustainable and trustworthy artificial intelligence in clinical practice.", "link": "http://arxiv.org/abs/2601.19743v1", "date": "2026-01-27", "relevancy": 2.1374, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5409}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5376}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20and%20backpropagation-free%20Green%20Learning%20for%20efficient%20multi-task%20echocardiographic%20segmentation%20and%20classification&body=Title%3A%20Interpretable%20and%20backpropagation-free%20Green%20Learning%20for%20efficient%20multi-task%20echocardiographic%20segmentation%20and%20classification%0AAuthor%3A%20Jyun-Ping%20Kao%20and%20Jiaxing%20Yang%20and%20C.%20-C.%20Jay%20Kuo%20and%20Jonghye%20Woo%0AAbstract%3A%20Echocardiography%20is%20a%20cornerstone%20for%20managing%20heart%20failure%20%28HF%29%2C%20with%20Left%20Ventricular%20Ejection%20Fraction%20%28LVEF%29%20being%20a%20critical%20metric%20for%20guiding%20therapy.%20However%2C%20manual%20LVEF%20assessment%20suffers%20from%20high%20inter-observer%20variability%2C%20while%20existing%20Deep%20Learning%20%28DL%29%20models%20are%20often%20computationally%20intensive%20and%20data-hungry%20%22black%20boxes%22%20that%20impede%20clinical%20trust%20and%20adoption.%20Here%2C%20we%20propose%20a%20backpropagation-free%20multi-task%20Green%20Learning%20%28MTGL%29%20framework%20that%20performs%20simultaneous%20Left%20Ventricle%20%28LV%29%20segmentation%20and%20LVEF%20classification.%20Our%20framework%20integrates%20an%20unsupervised%20VoxelHop%20encoder%20for%20hierarchical%20spatio-temporal%20feature%20extraction%20with%20a%20multi-level%20regression%20decoder%20and%20an%20XG-Boost%20classifier.%20On%20the%20EchoNet-Dynamic%20dataset%2C%20our%20MTGL%20model%20achieves%20state-of-the-art%20classification%20and%20segmentation%20performance%2C%20attaining%20a%20classification%20accuracy%20of%2094.3%25%20and%20a%20Dice%20Similarity%20Coefficient%20%28DSC%29%20of%200.912%2C%20significantly%20outperforming%20several%20advanced%203D%20DL%20models.%20Crucially%2C%20our%20model%20achieves%20this%20with%20over%20an%20order%20of%20magnitude%20fewer%20parameters%2C%20demonstrating%20exceptional%20computational%20efficiency.%20This%20work%20demonstrates%20that%20the%20GL%20paradigm%20can%20deliver%20highly%20accurate%2C%20efficient%2C%20and%20interpretable%20solutions%20for%20complex%20medical%20image%20analysis%2C%20paving%20the%20way%20for%20more%20sustainable%20and%20trustworthy%20artificial%20intelligence%20in%20clinical%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520and%2520backpropagation-free%2520Green%2520Learning%2520for%2520efficient%2520multi-task%2520echocardiographic%2520segmentation%2520and%2520classification%26entry.906535625%3DJyun-Ping%2520Kao%2520and%2520Jiaxing%2520Yang%2520and%2520C.%2520-C.%2520Jay%2520Kuo%2520and%2520Jonghye%2520Woo%26entry.1292438233%3DEchocardiography%2520is%2520a%2520cornerstone%2520for%2520managing%2520heart%2520failure%2520%2528HF%2529%252C%2520with%2520Left%2520Ventricular%2520Ejection%2520Fraction%2520%2528LVEF%2529%2520being%2520a%2520critical%2520metric%2520for%2520guiding%2520therapy.%2520However%252C%2520manual%2520LVEF%2520assessment%2520suffers%2520from%2520high%2520inter-observer%2520variability%252C%2520while%2520existing%2520Deep%2520Learning%2520%2528DL%2529%2520models%2520are%2520often%2520computationally%2520intensive%2520and%2520data-hungry%2520%2522black%2520boxes%2522%2520that%2520impede%2520clinical%2520trust%2520and%2520adoption.%2520Here%252C%2520we%2520propose%2520a%2520backpropagation-free%2520multi-task%2520Green%2520Learning%2520%2528MTGL%2529%2520framework%2520that%2520performs%2520simultaneous%2520Left%2520Ventricle%2520%2528LV%2529%2520segmentation%2520and%2520LVEF%2520classification.%2520Our%2520framework%2520integrates%2520an%2520unsupervised%2520VoxelHop%2520encoder%2520for%2520hierarchical%2520spatio-temporal%2520feature%2520extraction%2520with%2520a%2520multi-level%2520regression%2520decoder%2520and%2520an%2520XG-Boost%2520classifier.%2520On%2520the%2520EchoNet-Dynamic%2520dataset%252C%2520our%2520MTGL%2520model%2520achieves%2520state-of-the-art%2520classification%2520and%2520segmentation%2520performance%252C%2520attaining%2520a%2520classification%2520accuracy%2520of%252094.3%2525%2520and%2520a%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520of%25200.912%252C%2520significantly%2520outperforming%2520several%2520advanced%25203D%2520DL%2520models.%2520Crucially%252C%2520our%2520model%2520achieves%2520this%2520with%2520over%2520an%2520order%2520of%2520magnitude%2520fewer%2520parameters%252C%2520demonstrating%2520exceptional%2520computational%2520efficiency.%2520This%2520work%2520demonstrates%2520that%2520the%2520GL%2520paradigm%2520can%2520deliver%2520highly%2520accurate%252C%2520efficient%252C%2520and%2520interpretable%2520solutions%2520for%2520complex%2520medical%2520image%2520analysis%252C%2520paving%2520the%2520way%2520for%2520more%2520sustainable%2520and%2520trustworthy%2520artificial%2520intelligence%2520in%2520clinical%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20and%20backpropagation-free%20Green%20Learning%20for%20efficient%20multi-task%20echocardiographic%20segmentation%20and%20classification&entry.906535625=Jyun-Ping%20Kao%20and%20Jiaxing%20Yang%20and%20C.%20-C.%20Jay%20Kuo%20and%20Jonghye%20Woo&entry.1292438233=Echocardiography%20is%20a%20cornerstone%20for%20managing%20heart%20failure%20%28HF%29%2C%20with%20Left%20Ventricular%20Ejection%20Fraction%20%28LVEF%29%20being%20a%20critical%20metric%20for%20guiding%20therapy.%20However%2C%20manual%20LVEF%20assessment%20suffers%20from%20high%20inter-observer%20variability%2C%20while%20existing%20Deep%20Learning%20%28DL%29%20models%20are%20often%20computationally%20intensive%20and%20data-hungry%20%22black%20boxes%22%20that%20impede%20clinical%20trust%20and%20adoption.%20Here%2C%20we%20propose%20a%20backpropagation-free%20multi-task%20Green%20Learning%20%28MTGL%29%20framework%20that%20performs%20simultaneous%20Left%20Ventricle%20%28LV%29%20segmentation%20and%20LVEF%20classification.%20Our%20framework%20integrates%20an%20unsupervised%20VoxelHop%20encoder%20for%20hierarchical%20spatio-temporal%20feature%20extraction%20with%20a%20multi-level%20regression%20decoder%20and%20an%20XG-Boost%20classifier.%20On%20the%20EchoNet-Dynamic%20dataset%2C%20our%20MTGL%20model%20achieves%20state-of-the-art%20classification%20and%20segmentation%20performance%2C%20attaining%20a%20classification%20accuracy%20of%2094.3%25%20and%20a%20Dice%20Similarity%20Coefficient%20%28DSC%29%20of%200.912%2C%20significantly%20outperforming%20several%20advanced%203D%20DL%20models.%20Crucially%2C%20our%20model%20achieves%20this%20with%20over%20an%20order%20of%20magnitude%20fewer%20parameters%2C%20demonstrating%20exceptional%20computational%20efficiency.%20This%20work%20demonstrates%20that%20the%20GL%20paradigm%20can%20deliver%20highly%20accurate%2C%20efficient%2C%20and%20interpretable%20solutions%20for%20complex%20medical%20image%20analysis%2C%20paving%20the%20way%20for%20more%20sustainable%20and%20trustworthy%20artificial%20intelligence%20in%20clinical%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2601.19743v1&entry.124074799=Read"},
{"title": "Routing End User Queries to Enterprise Databases", "author": "Saikrishna Sudarshan and Tanay Kulkarni and Manasi Patwardhan and Lovekesh Vig and Ashwin Srinivasan and Tanmay Tulsidas Verlekar", "abstract": "We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.", "link": "http://arxiv.org/abs/2601.19825v1", "date": "2026-01-27", "relevancy": 2.1353, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4357}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Routing%20End%20User%20Queries%20to%20Enterprise%20Databases&body=Title%3A%20Routing%20End%20User%20Queries%20to%20Enterprise%20Databases%0AAuthor%3A%20Saikrishna%20Sudarshan%20and%20Tanay%20Kulkarni%20and%20Manasi%20Patwardhan%20and%20Lovekesh%20Vig%20and%20Ashwin%20Srinivasan%20and%20Tanmay%20Tulsidas%20Verlekar%0AAbstract%3A%20We%20address%20the%20task%20of%20routing%20natural%20language%20queries%20in%20multi-database%20enterprise%20environments.%20We%20construct%20realistic%20benchmarks%20by%20extending%20existing%20NL-to-SQL%20datasets.%20Our%20study%20shows%20that%20routing%20becomes%20increasingly%20challenging%20with%20larger%2C%20domain-overlapping%20DB%20repositories%20and%20ambiguous%20queries%2C%20motivating%20the%20need%20for%20more%20structured%20and%20robust%20reasoning-based%20solutions.%20By%20explicitly%20modelling%20schema%20coverage%2C%20structural%20connectivity%2C%20and%20fine-grained%20semantic%20alignment%2C%20the%20proposed%20modular%2C%20reasoning-driven%20reranking%20strategy%20consistently%20outperforms%20embedding-only%20and%20direct%20LLM-prompting%20baselines%20across%20all%20the%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRouting%2520End%2520User%2520Queries%2520to%2520Enterprise%2520Databases%26entry.906535625%3DSaikrishna%2520Sudarshan%2520and%2520Tanay%2520Kulkarni%2520and%2520Manasi%2520Patwardhan%2520and%2520Lovekesh%2520Vig%2520and%2520Ashwin%2520Srinivasan%2520and%2520Tanmay%2520Tulsidas%2520Verlekar%26entry.1292438233%3DWe%2520address%2520the%2520task%2520of%2520routing%2520natural%2520language%2520queries%2520in%2520multi-database%2520enterprise%2520environments.%2520We%2520construct%2520realistic%2520benchmarks%2520by%2520extending%2520existing%2520NL-to-SQL%2520datasets.%2520Our%2520study%2520shows%2520that%2520routing%2520becomes%2520increasingly%2520challenging%2520with%2520larger%252C%2520domain-overlapping%2520DB%2520repositories%2520and%2520ambiguous%2520queries%252C%2520motivating%2520the%2520need%2520for%2520more%2520structured%2520and%2520robust%2520reasoning-based%2520solutions.%2520By%2520explicitly%2520modelling%2520schema%2520coverage%252C%2520structural%2520connectivity%252C%2520and%2520fine-grained%2520semantic%2520alignment%252C%2520the%2520proposed%2520modular%252C%2520reasoning-driven%2520reranking%2520strategy%2520consistently%2520outperforms%2520embedding-only%2520and%2520direct%2520LLM-prompting%2520baselines%2520across%2520all%2520the%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Routing%20End%20User%20Queries%20to%20Enterprise%20Databases&entry.906535625=Saikrishna%20Sudarshan%20and%20Tanay%20Kulkarni%20and%20Manasi%20Patwardhan%20and%20Lovekesh%20Vig%20and%20Ashwin%20Srinivasan%20and%20Tanmay%20Tulsidas%20Verlekar&entry.1292438233=We%20address%20the%20task%20of%20routing%20natural%20language%20queries%20in%20multi-database%20enterprise%20environments.%20We%20construct%20realistic%20benchmarks%20by%20extending%20existing%20NL-to-SQL%20datasets.%20Our%20study%20shows%20that%20routing%20becomes%20increasingly%20challenging%20with%20larger%2C%20domain-overlapping%20DB%20repositories%20and%20ambiguous%20queries%2C%20motivating%20the%20need%20for%20more%20structured%20and%20robust%20reasoning-based%20solutions.%20By%20explicitly%20modelling%20schema%20coverage%2C%20structural%20connectivity%2C%20and%20fine-grained%20semantic%20alignment%2C%20the%20proposed%20modular%2C%20reasoning-driven%20reranking%20strategy%20consistently%20outperforms%20embedding-only%20and%20direct%20LLM-prompting%20baselines%20across%20all%20the%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2601.19825v1&entry.124074799=Read"},
{"title": "Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots", "author": "Mehdi Heydari Shahna and Seyed Adel Alizadeh Kolagar and Jouni Mattila", "abstract": "Reinforcement learning (RL) can be highly effective at learning goal-reaching policies, but it typically does not provide formal guarantees that the goal will always be reached. A common approach to provide formal goal-reaching guarantees is to introduce a shielding mechanism that restricts the agent to actions that satisfy predefined safety constraints. The main challenge here is integrating this mechanism with RL so that learning and exploration remain effective without becoming overly conservative. Hence, this paper proposes an RL-based control framework that provides formal goal-reaching guarantees for wheeled mobile robots operating in unstructured environments. We first design a real-time RL policy with a set of 15 carefully defined reward terms. These rewards encourage the robot to reach both static and dynamic goals while generating sufficiently smooth command signals that comply with predefined safety specifications, which is critical in practice. Second, a Lyapunov-like stabilizer layer is integrated into the benchmark RL framework as a policy supervisor to formally strengthen the goal-reaching control while preserving meaningful exploration of the state action space. The proposed framework is suitable for real-time deployment in challenging environments, as it provides a formal guarantee of convergence to the intended goal states and compensates for uncertainties by generating real-time control signals based on the current state, while respecting real-world motion constraints. The experimental results show that the proposed Lyapunov-like stabilizer consistently improves the benchmark RL policies, boosting the goal-reaching rate from 84.6% to 99.0%, sharply reducing failures, and improving efficiency.", "link": "http://arxiv.org/abs/2601.19499v1", "date": "2026-01-27", "relevancy": 2.1307, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5231}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Goal-Reaching%20Control%20with%20Guaranteed%20Lyapunov-Like%20Stabilizer%20for%20Mobile%20Robots&body=Title%3A%20Reinforcement%20Learning%20Goal-Reaching%20Control%20with%20Guaranteed%20Lyapunov-Like%20Stabilizer%20for%20Mobile%20Robots%0AAuthor%3A%20Mehdi%20Heydari%20Shahna%20and%20Seyed%20Adel%20Alizadeh%20Kolagar%20and%20Jouni%20Mattila%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20can%20be%20highly%20effective%20at%20learning%20goal-reaching%20policies%2C%20but%20it%20typically%20does%20not%20provide%20formal%20guarantees%20that%20the%20goal%20will%20always%20be%20reached.%20A%20common%20approach%20to%20provide%20formal%20goal-reaching%20guarantees%20is%20to%20introduce%20a%20shielding%20mechanism%20that%20restricts%20the%20agent%20to%20actions%20that%20satisfy%20predefined%20safety%20constraints.%20The%20main%20challenge%20here%20is%20integrating%20this%20mechanism%20with%20RL%20so%20that%20learning%20and%20exploration%20remain%20effective%20without%20becoming%20overly%20conservative.%20Hence%2C%20this%20paper%20proposes%20an%20RL-based%20control%20framework%20that%20provides%20formal%20goal-reaching%20guarantees%20for%20wheeled%20mobile%20robots%20operating%20in%20unstructured%20environments.%20We%20first%20design%20a%20real-time%20RL%20policy%20with%20a%20set%20of%2015%20carefully%20defined%20reward%20terms.%20These%20rewards%20encourage%20the%20robot%20to%20reach%20both%20static%20and%20dynamic%20goals%20while%20generating%20sufficiently%20smooth%20command%20signals%20that%20comply%20with%20predefined%20safety%20specifications%2C%20which%20is%20critical%20in%20practice.%20Second%2C%20a%20Lyapunov-like%20stabilizer%20layer%20is%20integrated%20into%20the%20benchmark%20RL%20framework%20as%20a%20policy%20supervisor%20to%20formally%20strengthen%20the%20goal-reaching%20control%20while%20preserving%20meaningful%20exploration%20of%20the%20state%20action%20space.%20The%20proposed%20framework%20is%20suitable%20for%20real-time%20deployment%20in%20challenging%20environments%2C%20as%20it%20provides%20a%20formal%20guarantee%20of%20convergence%20to%20the%20intended%20goal%20states%20and%20compensates%20for%20uncertainties%20by%20generating%20real-time%20control%20signals%20based%20on%20the%20current%20state%2C%20while%20respecting%20real-world%20motion%20constraints.%20The%20experimental%20results%20show%20that%20the%20proposed%20Lyapunov-like%20stabilizer%20consistently%20improves%20the%20benchmark%20RL%20policies%2C%20boosting%20the%20goal-reaching%20rate%20from%2084.6%25%20to%2099.0%25%2C%20sharply%20reducing%20failures%2C%20and%20improving%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Goal-Reaching%2520Control%2520with%2520Guaranteed%2520Lyapunov-Like%2520Stabilizer%2520for%2520Mobile%2520Robots%26entry.906535625%3DMehdi%2520Heydari%2520Shahna%2520and%2520Seyed%2520Adel%2520Alizadeh%2520Kolagar%2520and%2520Jouni%2520Mattila%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520can%2520be%2520highly%2520effective%2520at%2520learning%2520goal-reaching%2520policies%252C%2520but%2520it%2520typically%2520does%2520not%2520provide%2520formal%2520guarantees%2520that%2520the%2520goal%2520will%2520always%2520be%2520reached.%2520A%2520common%2520approach%2520to%2520provide%2520formal%2520goal-reaching%2520guarantees%2520is%2520to%2520introduce%2520a%2520shielding%2520mechanism%2520that%2520restricts%2520the%2520agent%2520to%2520actions%2520that%2520satisfy%2520predefined%2520safety%2520constraints.%2520The%2520main%2520challenge%2520here%2520is%2520integrating%2520this%2520mechanism%2520with%2520RL%2520so%2520that%2520learning%2520and%2520exploration%2520remain%2520effective%2520without%2520becoming%2520overly%2520conservative.%2520Hence%252C%2520this%2520paper%2520proposes%2520an%2520RL-based%2520control%2520framework%2520that%2520provides%2520formal%2520goal-reaching%2520guarantees%2520for%2520wheeled%2520mobile%2520robots%2520operating%2520in%2520unstructured%2520environments.%2520We%2520first%2520design%2520a%2520real-time%2520RL%2520policy%2520with%2520a%2520set%2520of%252015%2520carefully%2520defined%2520reward%2520terms.%2520These%2520rewards%2520encourage%2520the%2520robot%2520to%2520reach%2520both%2520static%2520and%2520dynamic%2520goals%2520while%2520generating%2520sufficiently%2520smooth%2520command%2520signals%2520that%2520comply%2520with%2520predefined%2520safety%2520specifications%252C%2520which%2520is%2520critical%2520in%2520practice.%2520Second%252C%2520a%2520Lyapunov-like%2520stabilizer%2520layer%2520is%2520integrated%2520into%2520the%2520benchmark%2520RL%2520framework%2520as%2520a%2520policy%2520supervisor%2520to%2520formally%2520strengthen%2520the%2520goal-reaching%2520control%2520while%2520preserving%2520meaningful%2520exploration%2520of%2520the%2520state%2520action%2520space.%2520The%2520proposed%2520framework%2520is%2520suitable%2520for%2520real-time%2520deployment%2520in%2520challenging%2520environments%252C%2520as%2520it%2520provides%2520a%2520formal%2520guarantee%2520of%2520convergence%2520to%2520the%2520intended%2520goal%2520states%2520and%2520compensates%2520for%2520uncertainties%2520by%2520generating%2520real-time%2520control%2520signals%2520based%2520on%2520the%2520current%2520state%252C%2520while%2520respecting%2520real-world%2520motion%2520constraints.%2520The%2520experimental%2520results%2520show%2520that%2520the%2520proposed%2520Lyapunov-like%2520stabilizer%2520consistently%2520improves%2520the%2520benchmark%2520RL%2520policies%252C%2520boosting%2520the%2520goal-reaching%2520rate%2520from%252084.6%2525%2520to%252099.0%2525%252C%2520sharply%2520reducing%2520failures%252C%2520and%2520improving%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Goal-Reaching%20Control%20with%20Guaranteed%20Lyapunov-Like%20Stabilizer%20for%20Mobile%20Robots&entry.906535625=Mehdi%20Heydari%20Shahna%20and%20Seyed%20Adel%20Alizadeh%20Kolagar%20and%20Jouni%20Mattila&entry.1292438233=Reinforcement%20learning%20%28RL%29%20can%20be%20highly%20effective%20at%20learning%20goal-reaching%20policies%2C%20but%20it%20typically%20does%20not%20provide%20formal%20guarantees%20that%20the%20goal%20will%20always%20be%20reached.%20A%20common%20approach%20to%20provide%20formal%20goal-reaching%20guarantees%20is%20to%20introduce%20a%20shielding%20mechanism%20that%20restricts%20the%20agent%20to%20actions%20that%20satisfy%20predefined%20safety%20constraints.%20The%20main%20challenge%20here%20is%20integrating%20this%20mechanism%20with%20RL%20so%20that%20learning%20and%20exploration%20remain%20effective%20without%20becoming%20overly%20conservative.%20Hence%2C%20this%20paper%20proposes%20an%20RL-based%20control%20framework%20that%20provides%20formal%20goal-reaching%20guarantees%20for%20wheeled%20mobile%20robots%20operating%20in%20unstructured%20environments.%20We%20first%20design%20a%20real-time%20RL%20policy%20with%20a%20set%20of%2015%20carefully%20defined%20reward%20terms.%20These%20rewards%20encourage%20the%20robot%20to%20reach%20both%20static%20and%20dynamic%20goals%20while%20generating%20sufficiently%20smooth%20command%20signals%20that%20comply%20with%20predefined%20safety%20specifications%2C%20which%20is%20critical%20in%20practice.%20Second%2C%20a%20Lyapunov-like%20stabilizer%20layer%20is%20integrated%20into%20the%20benchmark%20RL%20framework%20as%20a%20policy%20supervisor%20to%20formally%20strengthen%20the%20goal-reaching%20control%20while%20preserving%20meaningful%20exploration%20of%20the%20state%20action%20space.%20The%20proposed%20framework%20is%20suitable%20for%20real-time%20deployment%20in%20challenging%20environments%2C%20as%20it%20provides%20a%20formal%20guarantee%20of%20convergence%20to%20the%20intended%20goal%20states%20and%20compensates%20for%20uncertainties%20by%20generating%20real-time%20control%20signals%20based%20on%20the%20current%20state%2C%20while%20respecting%20real-world%20motion%20constraints.%20The%20experimental%20results%20show%20that%20the%20proposed%20Lyapunov-like%20stabilizer%20consistently%20improves%20the%20benchmark%20RL%20policies%2C%20boosting%20the%20goal-reaching%20rate%20from%2084.6%25%20to%2099.0%25%2C%20sharply%20reducing%20failures%2C%20and%20improving%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.19499v1&entry.124074799=Read"},
{"title": "Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation", "author": "Franz Thaler and Martin Urschler and Mateusz Kozinski and Matthias AF Gsell and Gernot Plank and Darko Stern", "abstract": "We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation, where we train a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. Our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently at training-time, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware. Overall, our evaluation shows that our method achieves new state-of-the-art performance in DG for medical image segmentation, even matching the performance of the in-domain baseline in several settings. We will release our source code upon acceptance of this manuscript.", "link": "http://arxiv.org/abs/2512.01510v2", "date": "2026-01-27", "relevancy": 2.128, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5506}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5231}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-aware%20Random%20Convolution%20and%20Source%20Matching%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation&body=Title%3A%20Semantic-aware%20Random%20Convolution%20and%20Source%20Matching%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Franz%20Thaler%20and%20Martin%20Urschler%20and%20Mateusz%20Kozinski%20and%20Matthias%20AF%20Gsell%20and%20Gernot%20Plank%20and%20Darko%20Stern%0AAbstract%3A%20We%20tackle%20the%20challenging%20problem%20of%20single-source%20domain%20generalization%20%28DG%29%20for%20medical%20image%20segmentation%2C%20where%20we%20train%20a%20network%20on%20one%20domain%20%28e.g.%2C%20CT%29%20and%20directly%20apply%20it%20to%20a%20different%20domain%20%28e.g.%2C%20MR%29%20without%20adapting%20the%20model%20and%20without%20requiring%20images%20or%20annotations%20from%20the%20new%20domain%20during%20training.%20Our%20method%20diversifies%20the%20source%20domain%20through%20semantic-aware%20random%20convolution%2C%20where%20different%20regions%20of%20a%20source%20image%20are%20augmented%20differently%20at%20training-time%2C%20based%20on%20their%20annotation%20labels.%20At%20test-time%2C%20we%20complement%20the%20randomization%20of%20the%20training%20domain%20via%20mapping%20the%20intensity%20of%20target%20domain%20images%2C%20making%20them%20similar%20to%20source%20domain%20data.%20We%20perform%20a%20comprehensive%20evaluation%20on%20a%20variety%20of%20cross-modality%20and%20cross-center%20generalization%20settings%20for%20abdominal%2C%20whole-heart%20and%20prostate%20segmentation%2C%20where%20we%20outperform%20previous%20DG%20techniques%20in%20a%20vast%20majority%20of%20experiments.%20Additionally%2C%20we%20also%20investigate%20our%20method%20when%20training%20on%20whole-heart%20CT%20or%20MR%20data%20and%20testing%20on%20the%20diastolic%20and%20systolic%20phase%20of%20cine%20MR%20data%20captured%20with%20different%20scanner%20hardware.%20Overall%2C%20our%20evaluation%20shows%20that%20our%20method%20achieves%20new%20state-of-the-art%20performance%20in%20DG%20for%20medical%20image%20segmentation%2C%20even%20matching%20the%20performance%20of%20the%20in-domain%20baseline%20in%20several%20settings.%20We%20will%20release%20our%20source%20code%20upon%20acceptance%20of%20this%20manuscript.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-aware%2520Random%2520Convolution%2520and%2520Source%2520Matching%2520for%2520Domain%2520Generalization%2520in%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DFranz%2520Thaler%2520and%2520Martin%2520Urschler%2520and%2520Mateusz%2520Kozinski%2520and%2520Matthias%2520AF%2520Gsell%2520and%2520Gernot%2520Plank%2520and%2520Darko%2520Stern%26entry.1292438233%3DWe%2520tackle%2520the%2520challenging%2520problem%2520of%2520single-source%2520domain%2520generalization%2520%2528DG%2529%2520for%2520medical%2520image%2520segmentation%252C%2520where%2520we%2520train%2520a%2520network%2520on%2520one%2520domain%2520%2528e.g.%252C%2520CT%2529%2520and%2520directly%2520apply%2520it%2520to%2520a%2520different%2520domain%2520%2528e.g.%252C%2520MR%2529%2520without%2520adapting%2520the%2520model%2520and%2520without%2520requiring%2520images%2520or%2520annotations%2520from%2520the%2520new%2520domain%2520during%2520training.%2520Our%2520method%2520diversifies%2520the%2520source%2520domain%2520through%2520semantic-aware%2520random%2520convolution%252C%2520where%2520different%2520regions%2520of%2520a%2520source%2520image%2520are%2520augmented%2520differently%2520at%2520training-time%252C%2520based%2520on%2520their%2520annotation%2520labels.%2520At%2520test-time%252C%2520we%2520complement%2520the%2520randomization%2520of%2520the%2520training%2520domain%2520via%2520mapping%2520the%2520intensity%2520of%2520target%2520domain%2520images%252C%2520making%2520them%2520similar%2520to%2520source%2520domain%2520data.%2520We%2520perform%2520a%2520comprehensive%2520evaluation%2520on%2520a%2520variety%2520of%2520cross-modality%2520and%2520cross-center%2520generalization%2520settings%2520for%2520abdominal%252C%2520whole-heart%2520and%2520prostate%2520segmentation%252C%2520where%2520we%2520outperform%2520previous%2520DG%2520techniques%2520in%2520a%2520vast%2520majority%2520of%2520experiments.%2520Additionally%252C%2520we%2520also%2520investigate%2520our%2520method%2520when%2520training%2520on%2520whole-heart%2520CT%2520or%2520MR%2520data%2520and%2520testing%2520on%2520the%2520diastolic%2520and%2520systolic%2520phase%2520of%2520cine%2520MR%2520data%2520captured%2520with%2520different%2520scanner%2520hardware.%2520Overall%252C%2520our%2520evaluation%2520shows%2520that%2520our%2520method%2520achieves%2520new%2520state-of-the-art%2520performance%2520in%2520DG%2520for%2520medical%2520image%2520segmentation%252C%2520even%2520matching%2520the%2520performance%2520of%2520the%2520in-domain%2520baseline%2520in%2520several%2520settings.%2520We%2520will%2520release%2520our%2520source%2520code%2520upon%2520acceptance%2520of%2520this%2520manuscript.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-aware%20Random%20Convolution%20and%20Source%20Matching%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation&entry.906535625=Franz%20Thaler%20and%20Martin%20Urschler%20and%20Mateusz%20Kozinski%20and%20Matthias%20AF%20Gsell%20and%20Gernot%20Plank%20and%20Darko%20Stern&entry.1292438233=We%20tackle%20the%20challenging%20problem%20of%20single-source%20domain%20generalization%20%28DG%29%20for%20medical%20image%20segmentation%2C%20where%20we%20train%20a%20network%20on%20one%20domain%20%28e.g.%2C%20CT%29%20and%20directly%20apply%20it%20to%20a%20different%20domain%20%28e.g.%2C%20MR%29%20without%20adapting%20the%20model%20and%20without%20requiring%20images%20or%20annotations%20from%20the%20new%20domain%20during%20training.%20Our%20method%20diversifies%20the%20source%20domain%20through%20semantic-aware%20random%20convolution%2C%20where%20different%20regions%20of%20a%20source%20image%20are%20augmented%20differently%20at%20training-time%2C%20based%20on%20their%20annotation%20labels.%20At%20test-time%2C%20we%20complement%20the%20randomization%20of%20the%20training%20domain%20via%20mapping%20the%20intensity%20of%20target%20domain%20images%2C%20making%20them%20similar%20to%20source%20domain%20data.%20We%20perform%20a%20comprehensive%20evaluation%20on%20a%20variety%20of%20cross-modality%20and%20cross-center%20generalization%20settings%20for%20abdominal%2C%20whole-heart%20and%20prostate%20segmentation%2C%20where%20we%20outperform%20previous%20DG%20techniques%20in%20a%20vast%20majority%20of%20experiments.%20Additionally%2C%20we%20also%20investigate%20our%20method%20when%20training%20on%20whole-heart%20CT%20or%20MR%20data%20and%20testing%20on%20the%20diastolic%20and%20systolic%20phase%20of%20cine%20MR%20data%20captured%20with%20different%20scanner%20hardware.%20Overall%2C%20our%20evaluation%20shows%20that%20our%20method%20achieves%20new%20state-of-the-art%20performance%20in%20DG%20for%20medical%20image%20segmentation%2C%20even%20matching%20the%20performance%20of%20the%20in-domain%20baseline%20in%20several%20settings.%20We%20will%20release%20our%20source%20code%20upon%20acceptance%20of%20this%20manuscript.&entry.1838667208=http%3A//arxiv.org/abs/2512.01510v2&entry.124074799=Read"},
{"title": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection", "author": "Francisco Caetano and Christiaan Viviers and Luis A. Zavala-Mondrag\u00f3n and Peter H. N. de With and Fons van der Sommen", "abstract": "Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications. The code is publicly available.", "link": "http://arxiv.org/abs/2501.08005v7", "date": "2026-01-27", "relevancy": 2.1177, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5419}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5217}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCoPatch%3A%20Taming%20Adversarially-driven%20Batch%20Statistics%20for%20Improved%20Out-of-Distribution%20Detection&body=Title%3A%20DisCoPatch%3A%20Taming%20Adversarially-driven%20Batch%20Statistics%20for%20Improved%20Out-of-Distribution%20Detection%0AAuthor%3A%20Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Luis%20A.%20Zavala-Mondrag%C3%B3n%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20Out-of-distribution%20%28OOD%29%20detection%20holds%20significant%20importance%20across%20many%20applications.%20While%20semantic%20and%20domain-shift%20OOD%20problems%20are%20well-studied%2C%20this%20work%20focuses%20on%20covariate%20shifts%20-%20subtle%20variations%20in%20the%20data%20distribution%20that%20can%20degrade%20machine%20learning%20performance.%20We%20hypothesize%20that%20detecting%20these%20subtle%20shifts%20can%20improve%20our%20understanding%20of%20in-distribution%20boundaries%2C%20ultimately%20improving%20OOD%20detection.%20In%20adversarial%20discriminators%20trained%20with%20Batch%20Normalization%20%28BN%29%2C%20real%20and%20adversarial%20samples%20form%20distinct%20domains%20with%20unique%20batch%20statistics%20-%20a%20property%20we%20exploit%20for%20OOD%20detection.%20We%20introduce%20DisCoPatch%2C%20an%20unsupervised%20Adversarial%20Variational%20Autoencoder%20%28VAE%29%20framework%20that%20harnesses%20this%20mechanism.%20During%20inference%2C%20batches%20consist%20of%20patches%20from%20the%20same%20image%2C%20ensuring%20a%20consistent%20data%20distribution%20that%20allows%20the%20model%20to%20rely%20on%20batch%20statistics.%20DisCoPatch%20uses%20the%20VAE%27s%20suboptimal%20outputs%20%28generated%20and%20reconstructed%29%20as%20negative%20samples%20to%20train%20the%20discriminator%2C%20thereby%20improving%20its%20ability%20to%20delineate%20the%20boundary%20between%20in-distribution%20samples%20and%20covariate%20shifts.%20By%20tightening%20this%20boundary%2C%20DisCoPatch%20achieves%20state-of-the-art%20results%20in%20public%20OOD%20detection%20benchmarks.%20The%20proposed%20model%20not%20only%20excels%20in%20detecting%20covariate%20shifts%2C%20achieving%2095.5%25%20AUROC%20on%20ImageNet-1K%28-C%29%20but%20also%20outperforms%20all%20prior%20methods%20on%20public%20Near-OOD%20%2895.0%25%29%20benchmarks.%20With%20a%20compact%20model%20size%20of%2025MB%2C%20it%20achieves%20high%20OOD%20detection%20performance%20at%20notably%20lower%20latency%20than%20existing%20methods%2C%20making%20it%20an%20efficient%20and%20practical%20solution%20for%20real-world%20OOD%20detection%20applications.%20The%20code%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2501.08005v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCoPatch%253A%2520Taming%2520Adversarially-driven%2520Batch%2520Statistics%2520for%2520Improved%2520Out-of-Distribution%2520Detection%26entry.906535625%3DFrancisco%2520Caetano%2520and%2520Christiaan%2520Viviers%2520and%2520Luis%2520A.%2520Zavala-Mondrag%25C3%25B3n%2520and%2520Peter%2520H.%2520N.%2520de%2520With%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3DOut-of-distribution%2520%2528OOD%2529%2520detection%2520holds%2520significant%2520importance%2520across%2520many%2520applications.%2520While%2520semantic%2520and%2520domain-shift%2520OOD%2520problems%2520are%2520well-studied%252C%2520this%2520work%2520focuses%2520on%2520covariate%2520shifts%2520-%2520subtle%2520variations%2520in%2520the%2520data%2520distribution%2520that%2520can%2520degrade%2520machine%2520learning%2520performance.%2520We%2520hypothesize%2520that%2520detecting%2520these%2520subtle%2520shifts%2520can%2520improve%2520our%2520understanding%2520of%2520in-distribution%2520boundaries%252C%2520ultimately%2520improving%2520OOD%2520detection.%2520In%2520adversarial%2520discriminators%2520trained%2520with%2520Batch%2520Normalization%2520%2528BN%2529%252C%2520real%2520and%2520adversarial%2520samples%2520form%2520distinct%2520domains%2520with%2520unique%2520batch%2520statistics%2520-%2520a%2520property%2520we%2520exploit%2520for%2520OOD%2520detection.%2520We%2520introduce%2520DisCoPatch%252C%2520an%2520unsupervised%2520Adversarial%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520framework%2520that%2520harnesses%2520this%2520mechanism.%2520During%2520inference%252C%2520batches%2520consist%2520of%2520patches%2520from%2520the%2520same%2520image%252C%2520ensuring%2520a%2520consistent%2520data%2520distribution%2520that%2520allows%2520the%2520model%2520to%2520rely%2520on%2520batch%2520statistics.%2520DisCoPatch%2520uses%2520the%2520VAE%2527s%2520suboptimal%2520outputs%2520%2528generated%2520and%2520reconstructed%2529%2520as%2520negative%2520samples%2520to%2520train%2520the%2520discriminator%252C%2520thereby%2520improving%2520its%2520ability%2520to%2520delineate%2520the%2520boundary%2520between%2520in-distribution%2520samples%2520and%2520covariate%2520shifts.%2520By%2520tightening%2520this%2520boundary%252C%2520DisCoPatch%2520achieves%2520state-of-the-art%2520results%2520in%2520public%2520OOD%2520detection%2520benchmarks.%2520The%2520proposed%2520model%2520not%2520only%2520excels%2520in%2520detecting%2520covariate%2520shifts%252C%2520achieving%252095.5%2525%2520AUROC%2520on%2520ImageNet-1K%2528-C%2529%2520but%2520also%2520outperforms%2520all%2520prior%2520methods%2520on%2520public%2520Near-OOD%2520%252895.0%2525%2529%2520benchmarks.%2520With%2520a%2520compact%2520model%2520size%2520of%252025MB%252C%2520it%2520achieves%2520high%2520OOD%2520detection%2520performance%2520at%2520notably%2520lower%2520latency%2520than%2520existing%2520methods%252C%2520making%2520it%2520an%2520efficient%2520and%2520practical%2520solution%2520for%2520real-world%2520OOD%2520detection%2520applications.%2520The%2520code%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08005v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCoPatch%3A%20Taming%20Adversarially-driven%20Batch%20Statistics%20for%20Improved%20Out-of-Distribution%20Detection&entry.906535625=Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Luis%20A.%20Zavala-Mondrag%C3%B3n%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen&entry.1292438233=Out-of-distribution%20%28OOD%29%20detection%20holds%20significant%20importance%20across%20many%20applications.%20While%20semantic%20and%20domain-shift%20OOD%20problems%20are%20well-studied%2C%20this%20work%20focuses%20on%20covariate%20shifts%20-%20subtle%20variations%20in%20the%20data%20distribution%20that%20can%20degrade%20machine%20learning%20performance.%20We%20hypothesize%20that%20detecting%20these%20subtle%20shifts%20can%20improve%20our%20understanding%20of%20in-distribution%20boundaries%2C%20ultimately%20improving%20OOD%20detection.%20In%20adversarial%20discriminators%20trained%20with%20Batch%20Normalization%20%28BN%29%2C%20real%20and%20adversarial%20samples%20form%20distinct%20domains%20with%20unique%20batch%20statistics%20-%20a%20property%20we%20exploit%20for%20OOD%20detection.%20We%20introduce%20DisCoPatch%2C%20an%20unsupervised%20Adversarial%20Variational%20Autoencoder%20%28VAE%29%20framework%20that%20harnesses%20this%20mechanism.%20During%20inference%2C%20batches%20consist%20of%20patches%20from%20the%20same%20image%2C%20ensuring%20a%20consistent%20data%20distribution%20that%20allows%20the%20model%20to%20rely%20on%20batch%20statistics.%20DisCoPatch%20uses%20the%20VAE%27s%20suboptimal%20outputs%20%28generated%20and%20reconstructed%29%20as%20negative%20samples%20to%20train%20the%20discriminator%2C%20thereby%20improving%20its%20ability%20to%20delineate%20the%20boundary%20between%20in-distribution%20samples%20and%20covariate%20shifts.%20By%20tightening%20this%20boundary%2C%20DisCoPatch%20achieves%20state-of-the-art%20results%20in%20public%20OOD%20detection%20benchmarks.%20The%20proposed%20model%20not%20only%20excels%20in%20detecting%20covariate%20shifts%2C%20achieving%2095.5%25%20AUROC%20on%20ImageNet-1K%28-C%29%20but%20also%20outperforms%20all%20prior%20methods%20on%20public%20Near-OOD%20%2895.0%25%29%20benchmarks.%20With%20a%20compact%20model%20size%20of%2025MB%2C%20it%20achieves%20high%20OOD%20detection%20performance%20at%20notably%20lower%20latency%20than%20existing%20methods%2C%20making%20it%20an%20efficient%20and%20practical%20solution%20for%20real-world%20OOD%20detection%20applications.%20The%20code%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2501.08005v7&entry.124074799=Read"},
{"title": "AI Cap-and-Trade: Efficiency Incentives for Accessibility and Sustainability", "author": "Marco Bornstein and Amrit Singh Bedi", "abstract": "The race for artificial intelligence (AI) dominance often prioritizes scale over efficiency. Hyper-scaling is the common industry approach: larger models, more data, and as many computational resources as possible. Using more resources is a simpler path to improved AI performance. Thus, efficiency has been de-emphasized. Consequently, the need for costly computational resources has marginalized academics and smaller companies. Simultaneously, increased energy expenditure, due to growing AI use, has led to mounting environmental costs. In response to accessibility and sustainability concerns, we argue for research into, and implementation of, market-based methods that incentivize AI efficiency. We believe that incentivizing efficient operations and approaches will reduce emissions while opening new opportunities for academics and smaller companies. As a call to action, we propose a cap-and-trade system for AI. Our system provably reduces computations for AI deployment, thereby lowering emissions and monetizing efficiency to the benefit of of academics and smaller companies.", "link": "http://arxiv.org/abs/2601.19886v1", "date": "2026-01-27", "relevancy": 2.1156, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4292}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4209}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Cap-and-Trade%3A%20Efficiency%20Incentives%20for%20Accessibility%20and%20Sustainability&body=Title%3A%20AI%20Cap-and-Trade%3A%20Efficiency%20Incentives%20for%20Accessibility%20and%20Sustainability%0AAuthor%3A%20Marco%20Bornstein%20and%20Amrit%20Singh%20Bedi%0AAbstract%3A%20The%20race%20for%20artificial%20intelligence%20%28AI%29%20dominance%20often%20prioritizes%20scale%20over%20efficiency.%20Hyper-scaling%20is%20the%20common%20industry%20approach%3A%20larger%20models%2C%20more%20data%2C%20and%20as%20many%20computational%20resources%20as%20possible.%20Using%20more%20resources%20is%20a%20simpler%20path%20to%20improved%20AI%20performance.%20Thus%2C%20efficiency%20has%20been%20de-emphasized.%20Consequently%2C%20the%20need%20for%20costly%20computational%20resources%20has%20marginalized%20academics%20and%20smaller%20companies.%20Simultaneously%2C%20increased%20energy%20expenditure%2C%20due%20to%20growing%20AI%20use%2C%20has%20led%20to%20mounting%20environmental%20costs.%20In%20response%20to%20accessibility%20and%20sustainability%20concerns%2C%20we%20argue%20for%20research%20into%2C%20and%20implementation%20of%2C%20market-based%20methods%20that%20incentivize%20AI%20efficiency.%20We%20believe%20that%20incentivizing%20efficient%20operations%20and%20approaches%20will%20reduce%20emissions%20while%20opening%20new%20opportunities%20for%20academics%20and%20smaller%20companies.%20As%20a%20call%20to%20action%2C%20we%20propose%20a%20cap-and-trade%20system%20for%20AI.%20Our%20system%20provably%20reduces%20computations%20for%20AI%20deployment%2C%20thereby%20lowering%20emissions%20and%20monetizing%20efficiency%20to%20the%20benefit%20of%20of%20academics%20and%20smaller%20companies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Cap-and-Trade%253A%2520Efficiency%2520Incentives%2520for%2520Accessibility%2520and%2520Sustainability%26entry.906535625%3DMarco%2520Bornstein%2520and%2520Amrit%2520Singh%2520Bedi%26entry.1292438233%3DThe%2520race%2520for%2520artificial%2520intelligence%2520%2528AI%2529%2520dominance%2520often%2520prioritizes%2520scale%2520over%2520efficiency.%2520Hyper-scaling%2520is%2520the%2520common%2520industry%2520approach%253A%2520larger%2520models%252C%2520more%2520data%252C%2520and%2520as%2520many%2520computational%2520resources%2520as%2520possible.%2520Using%2520more%2520resources%2520is%2520a%2520simpler%2520path%2520to%2520improved%2520AI%2520performance.%2520Thus%252C%2520efficiency%2520has%2520been%2520de-emphasized.%2520Consequently%252C%2520the%2520need%2520for%2520costly%2520computational%2520resources%2520has%2520marginalized%2520academics%2520and%2520smaller%2520companies.%2520Simultaneously%252C%2520increased%2520energy%2520expenditure%252C%2520due%2520to%2520growing%2520AI%2520use%252C%2520has%2520led%2520to%2520mounting%2520environmental%2520costs.%2520In%2520response%2520to%2520accessibility%2520and%2520sustainability%2520concerns%252C%2520we%2520argue%2520for%2520research%2520into%252C%2520and%2520implementation%2520of%252C%2520market-based%2520methods%2520that%2520incentivize%2520AI%2520efficiency.%2520We%2520believe%2520that%2520incentivizing%2520efficient%2520operations%2520and%2520approaches%2520will%2520reduce%2520emissions%2520while%2520opening%2520new%2520opportunities%2520for%2520academics%2520and%2520smaller%2520companies.%2520As%2520a%2520call%2520to%2520action%252C%2520we%2520propose%2520a%2520cap-and-trade%2520system%2520for%2520AI.%2520Our%2520system%2520provably%2520reduces%2520computations%2520for%2520AI%2520deployment%252C%2520thereby%2520lowering%2520emissions%2520and%2520monetizing%2520efficiency%2520to%2520the%2520benefit%2520of%2520of%2520academics%2520and%2520smaller%2520companies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Cap-and-Trade%3A%20Efficiency%20Incentives%20for%20Accessibility%20and%20Sustainability&entry.906535625=Marco%20Bornstein%20and%20Amrit%20Singh%20Bedi&entry.1292438233=The%20race%20for%20artificial%20intelligence%20%28AI%29%20dominance%20often%20prioritizes%20scale%20over%20efficiency.%20Hyper-scaling%20is%20the%20common%20industry%20approach%3A%20larger%20models%2C%20more%20data%2C%20and%20as%20many%20computational%20resources%20as%20possible.%20Using%20more%20resources%20is%20a%20simpler%20path%20to%20improved%20AI%20performance.%20Thus%2C%20efficiency%20has%20been%20de-emphasized.%20Consequently%2C%20the%20need%20for%20costly%20computational%20resources%20has%20marginalized%20academics%20and%20smaller%20companies.%20Simultaneously%2C%20increased%20energy%20expenditure%2C%20due%20to%20growing%20AI%20use%2C%20has%20led%20to%20mounting%20environmental%20costs.%20In%20response%20to%20accessibility%20and%20sustainability%20concerns%2C%20we%20argue%20for%20research%20into%2C%20and%20implementation%20of%2C%20market-based%20methods%20that%20incentivize%20AI%20efficiency.%20We%20believe%20that%20incentivizing%20efficient%20operations%20and%20approaches%20will%20reduce%20emissions%20while%20opening%20new%20opportunities%20for%20academics%20and%20smaller%20companies.%20As%20a%20call%20to%20action%2C%20we%20propose%20a%20cap-and-trade%20system%20for%20AI.%20Our%20system%20provably%20reduces%20computations%20for%20AI%20deployment%2C%20thereby%20lowering%20emissions%20and%20monetizing%20efficiency%20to%20the%20benefit%20of%20of%20academics%20and%20smaller%20companies.&entry.1838667208=http%3A//arxiv.org/abs/2601.19886v1&entry.124074799=Read"},
{"title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity", "author": "Shiwei Liu and Tianlong Chen and Zahra Atashgahi and Xiaohan Chen and Ghada Sokar and Elena Mocanu and Mykola Pechenizkiy and Zhangyang Wang and Decebal Constantin Mocanu", "abstract": "The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called FreeTickets. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, FreeTickets, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, FreeTickets has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. FreeTickets surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, FreeTickets outperforms the naive deep ensemble with ResNet50 on ImageNet using around only 1/5 of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets.", "link": "http://arxiv.org/abs/2106.14568v5", "date": "2026-01-27", "relevancy": 2.1094, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5743}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4977}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Ensembling%20with%20No%20Overhead%20for%20either%20Training%20or%20Testing%3A%20The%20All-Round%20Blessings%20of%20Dynamic%20Sparsity&body=Title%3A%20Deep%20Ensembling%20with%20No%20Overhead%20for%20either%20Training%20or%20Testing%3A%20The%20All-Round%20Blessings%20of%20Dynamic%20Sparsity%0AAuthor%3A%20Shiwei%20Liu%20and%20Tianlong%20Chen%20and%20Zahra%20Atashgahi%20and%20Xiaohan%20Chen%20and%20Ghada%20Sokar%20and%20Elena%20Mocanu%20and%20Mykola%20Pechenizkiy%20and%20Zhangyang%20Wang%20and%20Decebal%20Constantin%20Mocanu%0AAbstract%3A%20The%20success%20of%20deep%20ensembles%20on%20improving%20predictive%20performance%2C%20uncertainty%20estimation%2C%20and%20out-of-distribution%20robustness%20has%20been%20extensively%20studied%20in%20the%20machine%20learning%20literature.%20Albeit%20the%20promising%20results%2C%20naively%20training%20multiple%20deep%20neural%20networks%20and%20combining%20their%20predictions%20at%20inference%20leads%20to%20prohibitive%20computational%20costs%20and%20memory%20requirements.%20Recently%20proposed%20efficient%20ensemble%20approaches%20reach%20the%20performance%20of%20the%20traditional%20deep%20ensembles%20with%20significantly%20lower%20costs.%20However%2C%20the%20training%20resources%20required%20by%20these%20approaches%20are%20still%20at%20least%20the%20same%20as%20training%20a%20single%20dense%20model.%20In%20this%20work%2C%20we%20draw%20a%20unique%20connection%20between%20sparse%20neural%20network%20training%20and%20deep%20ensembles%2C%20yielding%20a%20novel%20efficient%20ensemble%20learning%20framework%20called%20FreeTickets.%20Instead%20of%20training%20multiple%20dense%20networks%20and%20averaging%20them%2C%20we%20directly%20train%20sparse%20subnetworks%20from%20scratch%20and%20extract%20diverse%20yet%20accurate%20subnetworks%20during%20this%20efficient%2C%20sparse-to-sparse%20training.%20Our%20framework%2C%20FreeTickets%2C%20is%20defined%20as%20the%20ensemble%20of%20these%20relatively%20cheap%20sparse%20subnetworks.%20Despite%20being%20an%20ensemble%20method%2C%20FreeTickets%20has%20even%20fewer%20parameters%20and%20training%20FLOPs%20than%20a%20single%20dense%20model.%20This%20seemingly%20counter-intuitive%20outcome%20is%20due%20to%20the%20ultra%20training/inference%20efficiency%20of%20dynamic%20sparse%20training.%20FreeTickets%20surpasses%20the%20dense%20baseline%20in%20all%20the%20following%20criteria%3A%20prediction%20accuracy%2C%20uncertainty%20estimation%2C%20out-of-distribution%20%28OoD%29%20robustness%2C%20as%20well%20as%20efficiency%20for%20both%20training%20and%20inference.%20Impressively%2C%20FreeTickets%20outperforms%20the%20naive%20deep%20ensemble%20with%20ResNet50%20on%20ImageNet%20using%20around%20only%201/5%20of%20the%20training%20FLOPs%20required%20by%20the%20latter.%20We%20have%20released%20our%20source%20code%20at%20https%3A//github.com/VITA-Group/FreeTickets.%0ALink%3A%20http%3A//arxiv.org/abs/2106.14568v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Ensembling%2520with%2520No%2520Overhead%2520for%2520either%2520Training%2520or%2520Testing%253A%2520The%2520All-Round%2520Blessings%2520of%2520Dynamic%2520Sparsity%26entry.906535625%3DShiwei%2520Liu%2520and%2520Tianlong%2520Chen%2520and%2520Zahra%2520Atashgahi%2520and%2520Xiaohan%2520Chen%2520and%2520Ghada%2520Sokar%2520and%2520Elena%2520Mocanu%2520and%2520Mykola%2520Pechenizkiy%2520and%2520Zhangyang%2520Wang%2520and%2520Decebal%2520Constantin%2520Mocanu%26entry.1292438233%3DThe%2520success%2520of%2520deep%2520ensembles%2520on%2520improving%2520predictive%2520performance%252C%2520uncertainty%2520estimation%252C%2520and%2520out-of-distribution%2520robustness%2520has%2520been%2520extensively%2520studied%2520in%2520the%2520machine%2520learning%2520literature.%2520Albeit%2520the%2520promising%2520results%252C%2520naively%2520training%2520multiple%2520deep%2520neural%2520networks%2520and%2520combining%2520their%2520predictions%2520at%2520inference%2520leads%2520to%2520prohibitive%2520computational%2520costs%2520and%2520memory%2520requirements.%2520Recently%2520proposed%2520efficient%2520ensemble%2520approaches%2520reach%2520the%2520performance%2520of%2520the%2520traditional%2520deep%2520ensembles%2520with%2520significantly%2520lower%2520costs.%2520However%252C%2520the%2520training%2520resources%2520required%2520by%2520these%2520approaches%2520are%2520still%2520at%2520least%2520the%2520same%2520as%2520training%2520a%2520single%2520dense%2520model.%2520In%2520this%2520work%252C%2520we%2520draw%2520a%2520unique%2520connection%2520between%2520sparse%2520neural%2520network%2520training%2520and%2520deep%2520ensembles%252C%2520yielding%2520a%2520novel%2520efficient%2520ensemble%2520learning%2520framework%2520called%2520FreeTickets.%2520Instead%2520of%2520training%2520multiple%2520dense%2520networks%2520and%2520averaging%2520them%252C%2520we%2520directly%2520train%2520sparse%2520subnetworks%2520from%2520scratch%2520and%2520extract%2520diverse%2520yet%2520accurate%2520subnetworks%2520during%2520this%2520efficient%252C%2520sparse-to-sparse%2520training.%2520Our%2520framework%252C%2520FreeTickets%252C%2520is%2520defined%2520as%2520the%2520ensemble%2520of%2520these%2520relatively%2520cheap%2520sparse%2520subnetworks.%2520Despite%2520being%2520an%2520ensemble%2520method%252C%2520FreeTickets%2520has%2520even%2520fewer%2520parameters%2520and%2520training%2520FLOPs%2520than%2520a%2520single%2520dense%2520model.%2520This%2520seemingly%2520counter-intuitive%2520outcome%2520is%2520due%2520to%2520the%2520ultra%2520training/inference%2520efficiency%2520of%2520dynamic%2520sparse%2520training.%2520FreeTickets%2520surpasses%2520the%2520dense%2520baseline%2520in%2520all%2520the%2520following%2520criteria%253A%2520prediction%2520accuracy%252C%2520uncertainty%2520estimation%252C%2520out-of-distribution%2520%2528OoD%2529%2520robustness%252C%2520as%2520well%2520as%2520efficiency%2520for%2520both%2520training%2520and%2520inference.%2520Impressively%252C%2520FreeTickets%2520outperforms%2520the%2520naive%2520deep%2520ensemble%2520with%2520ResNet50%2520on%2520ImageNet%2520using%2520around%2520only%25201/5%2520of%2520the%2520training%2520FLOPs%2520required%2520by%2520the%2520latter.%2520We%2520have%2520released%2520our%2520source%2520code%2520at%2520https%253A//github.com/VITA-Group/FreeTickets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2106.14568v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Ensembling%20with%20No%20Overhead%20for%20either%20Training%20or%20Testing%3A%20The%20All-Round%20Blessings%20of%20Dynamic%20Sparsity&entry.906535625=Shiwei%20Liu%20and%20Tianlong%20Chen%20and%20Zahra%20Atashgahi%20and%20Xiaohan%20Chen%20and%20Ghada%20Sokar%20and%20Elena%20Mocanu%20and%20Mykola%20Pechenizkiy%20and%20Zhangyang%20Wang%20and%20Decebal%20Constantin%20Mocanu&entry.1292438233=The%20success%20of%20deep%20ensembles%20on%20improving%20predictive%20performance%2C%20uncertainty%20estimation%2C%20and%20out-of-distribution%20robustness%20has%20been%20extensively%20studied%20in%20the%20machine%20learning%20literature.%20Albeit%20the%20promising%20results%2C%20naively%20training%20multiple%20deep%20neural%20networks%20and%20combining%20their%20predictions%20at%20inference%20leads%20to%20prohibitive%20computational%20costs%20and%20memory%20requirements.%20Recently%20proposed%20efficient%20ensemble%20approaches%20reach%20the%20performance%20of%20the%20traditional%20deep%20ensembles%20with%20significantly%20lower%20costs.%20However%2C%20the%20training%20resources%20required%20by%20these%20approaches%20are%20still%20at%20least%20the%20same%20as%20training%20a%20single%20dense%20model.%20In%20this%20work%2C%20we%20draw%20a%20unique%20connection%20between%20sparse%20neural%20network%20training%20and%20deep%20ensembles%2C%20yielding%20a%20novel%20efficient%20ensemble%20learning%20framework%20called%20FreeTickets.%20Instead%20of%20training%20multiple%20dense%20networks%20and%20averaging%20them%2C%20we%20directly%20train%20sparse%20subnetworks%20from%20scratch%20and%20extract%20diverse%20yet%20accurate%20subnetworks%20during%20this%20efficient%2C%20sparse-to-sparse%20training.%20Our%20framework%2C%20FreeTickets%2C%20is%20defined%20as%20the%20ensemble%20of%20these%20relatively%20cheap%20sparse%20subnetworks.%20Despite%20being%20an%20ensemble%20method%2C%20FreeTickets%20has%20even%20fewer%20parameters%20and%20training%20FLOPs%20than%20a%20single%20dense%20model.%20This%20seemingly%20counter-intuitive%20outcome%20is%20due%20to%20the%20ultra%20training/inference%20efficiency%20of%20dynamic%20sparse%20training.%20FreeTickets%20surpasses%20the%20dense%20baseline%20in%20all%20the%20following%20criteria%3A%20prediction%20accuracy%2C%20uncertainty%20estimation%2C%20out-of-distribution%20%28OoD%29%20robustness%2C%20as%20well%20as%20efficiency%20for%20both%20training%20and%20inference.%20Impressively%2C%20FreeTickets%20outperforms%20the%20naive%20deep%20ensemble%20with%20ResNet50%20on%20ImageNet%20using%20around%20only%201/5%20of%20the%20training%20FLOPs%20required%20by%20the%20latter.%20We%20have%20released%20our%20source%20code%20at%20https%3A//github.com/VITA-Group/FreeTickets.&entry.1838667208=http%3A//arxiv.org/abs/2106.14568v5&entry.124074799=Read"},
{"title": "Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering", "author": "Kun Li and Michael Ying Yang and Sami Sebastian Brandt", "abstract": "Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.", "link": "http://arxiv.org/abs/2601.19821v1", "date": "2026-01-27", "relevancy": 2.1087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Query-Guided%20Spatial-Temporal-Frequency%20Interaction%20for%20Music%20Audio-Visual%20Question%20Answering&body=Title%3A%20Query-Guided%20Spatial-Temporal-Frequency%20Interaction%20for%20Music%20Audio-Visual%20Question%20Answering%0AAuthor%3A%20Kun%20Li%20and%20Michael%20Ying%20Yang%20and%20Sami%20Sebastian%20Brandt%0AAbstract%3A%20Audio--Visual%20Question%20Answering%20%28AVQA%29%20is%20a%20challenging%20multimodal%20task%20that%20requires%20jointly%20reasoning%20over%20audio%2C%20visual%2C%20and%20textual%20information%20in%20a%20given%20video%20to%20answer%20natural%20language%20questions.%20Inspired%20by%20recent%20advances%20in%20Video%20QA%2C%20many%20existing%20AVQA%20approaches%20primarily%20focus%20on%20visual%20information%20processing%2C%20leveraging%20pre-trained%20models%20to%20extract%20object-level%20and%20motion-level%20representations.%20However%2C%20in%20those%20methods%2C%20the%20audio%20input%20is%20primarily%20treated%20as%20complementary%20to%20video%20analysis%2C%20and%20the%20textual%20question%20information%20contributes%20minimally%20to%20audio--visual%20understanding%2C%20as%20it%20is%20typically%20integrated%20only%20in%20the%20final%20stages%20of%20reasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20Query-guided%20Spatial--Temporal--Frequency%20%28QSTar%29%20interaction%20method%2C%20which%20effectively%20incorporates%20question-guided%20clues%20and%20exploits%20the%20distinctive%20frequency-domain%20characteristics%20of%20audio%20signals%2C%20alongside%20spatial%20and%20temporal%20perception%2C%20to%20enhance%20audio--visual%20understanding.%20Furthermore%2C%20we%20introduce%20a%20Query%20Context%20Reasoning%20%28QCR%29%20block%20inspired%20by%20prompting%2C%20which%20guides%20the%20model%20to%20focus%20more%20precisely%20on%20semantically%20relevant%20audio%20and%20visual%20features.%20Extensive%20experiments%20conducted%20on%20several%20AVQA%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20achieving%20significant%20performance%20improvements%20over%20existing%20Audio%20QA%2C%20Visual%20QA%2C%20Video%20QA%2C%20and%20AVQA%20approaches.%20The%20code%20and%20pretrained%20models%20will%20be%20released%20after%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuery-Guided%2520Spatial-Temporal-Frequency%2520Interaction%2520for%2520Music%2520Audio-Visual%2520Question%2520Answering%26entry.906535625%3DKun%2520Li%2520and%2520Michael%2520Ying%2520Yang%2520and%2520Sami%2520Sebastian%2520Brandt%26entry.1292438233%3DAudio--Visual%2520Question%2520Answering%2520%2528AVQA%2529%2520is%2520a%2520challenging%2520multimodal%2520task%2520that%2520requires%2520jointly%2520reasoning%2520over%2520audio%252C%2520visual%252C%2520and%2520textual%2520information%2520in%2520a%2520given%2520video%2520to%2520answer%2520natural%2520language%2520questions.%2520Inspired%2520by%2520recent%2520advances%2520in%2520Video%2520QA%252C%2520many%2520existing%2520AVQA%2520approaches%2520primarily%2520focus%2520on%2520visual%2520information%2520processing%252C%2520leveraging%2520pre-trained%2520models%2520to%2520extract%2520object-level%2520and%2520motion-level%2520representations.%2520However%252C%2520in%2520those%2520methods%252C%2520the%2520audio%2520input%2520is%2520primarily%2520treated%2520as%2520complementary%2520to%2520video%2520analysis%252C%2520and%2520the%2520textual%2520question%2520information%2520contributes%2520minimally%2520to%2520audio--visual%2520understanding%252C%2520as%2520it%2520is%2520typically%2520integrated%2520only%2520in%2520the%2520final%2520stages%2520of%2520reasoning.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520Query-guided%2520Spatial--Temporal--Frequency%2520%2528QSTar%2529%2520interaction%2520method%252C%2520which%2520effectively%2520incorporates%2520question-guided%2520clues%2520and%2520exploits%2520the%2520distinctive%2520frequency-domain%2520characteristics%2520of%2520audio%2520signals%252C%2520alongside%2520spatial%2520and%2520temporal%2520perception%252C%2520to%2520enhance%2520audio--visual%2520understanding.%2520Furthermore%252C%2520we%2520introduce%2520a%2520Query%2520Context%2520Reasoning%2520%2528QCR%2529%2520block%2520inspired%2520by%2520prompting%252C%2520which%2520guides%2520the%2520model%2520to%2520focus%2520more%2520precisely%2520on%2520semantically%2520relevant%2520audio%2520and%2520visual%2520features.%2520Extensive%2520experiments%2520conducted%2520on%2520several%2520AVQA%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%252C%2520achieving%2520significant%2520performance%2520improvements%2520over%2520existing%2520Audio%2520QA%252C%2520Visual%2520QA%252C%2520Video%2520QA%252C%2520and%2520AVQA%2520approaches.%2520The%2520code%2520and%2520pretrained%2520models%2520will%2520be%2520released%2520after%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Query-Guided%20Spatial-Temporal-Frequency%20Interaction%20for%20Music%20Audio-Visual%20Question%20Answering&entry.906535625=Kun%20Li%20and%20Michael%20Ying%20Yang%20and%20Sami%20Sebastian%20Brandt&entry.1292438233=Audio--Visual%20Question%20Answering%20%28AVQA%29%20is%20a%20challenging%20multimodal%20task%20that%20requires%20jointly%20reasoning%20over%20audio%2C%20visual%2C%20and%20textual%20information%20in%20a%20given%20video%20to%20answer%20natural%20language%20questions.%20Inspired%20by%20recent%20advances%20in%20Video%20QA%2C%20many%20existing%20AVQA%20approaches%20primarily%20focus%20on%20visual%20information%20processing%2C%20leveraging%20pre-trained%20models%20to%20extract%20object-level%20and%20motion-level%20representations.%20However%2C%20in%20those%20methods%2C%20the%20audio%20input%20is%20primarily%20treated%20as%20complementary%20to%20video%20analysis%2C%20and%20the%20textual%20question%20information%20contributes%20minimally%20to%20audio--visual%20understanding%2C%20as%20it%20is%20typically%20integrated%20only%20in%20the%20final%20stages%20of%20reasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20Query-guided%20Spatial--Temporal--Frequency%20%28QSTar%29%20interaction%20method%2C%20which%20effectively%20incorporates%20question-guided%20clues%20and%20exploits%20the%20distinctive%20frequency-domain%20characteristics%20of%20audio%20signals%2C%20alongside%20spatial%20and%20temporal%20perception%2C%20to%20enhance%20audio--visual%20understanding.%20Furthermore%2C%20we%20introduce%20a%20Query%20Context%20Reasoning%20%28QCR%29%20block%20inspired%20by%20prompting%2C%20which%20guides%20the%20model%20to%20focus%20more%20precisely%20on%20semantically%20relevant%20audio%20and%20visual%20features.%20Extensive%20experiments%20conducted%20on%20several%20AVQA%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20achieving%20significant%20performance%20improvements%20over%20existing%20Audio%20QA%2C%20Visual%20QA%2C%20Video%20QA%2C%20and%20AVQA%20approaches.%20The%20code%20and%20pretrained%20models%20will%20be%20released%20after%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2601.19821v1&entry.124074799=Read"},
{"title": "LLM-Generated Explanations Do Not Suffice for Ultra-Strong Machine Learning", "author": "Lun Ai and Johannes Langer and Ute Schmid and Stephen Muggleton", "abstract": "Ultra Strong Machine Learning (USML) refers to symbolic learning systems that not only improve their own performance but can also teach their acquired knowledge to quantifiably improve human performance. We introduce LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic framework that combines symbolic program synthesis with large language models (LLMs). This framework automatically generates natural language explanations of learned logic programs, replacing hand-crafted templates used in prior USML work. Using LLMs-as-judges evaluation and expert validation, we show that LENS produces higher-quality explanations than both direct LLM prompting and hand-crafted templates. We then examine whether LENS explanations suffice for achieving USML in a human trial teaching active learning strategies across three related domains. Our exploratory analysis suggests that concise, expert-written explanations may benefit learners with higher initial performance, while LLM-generated explanations provide no advantage over human self learning despite being rated as higher quality. This case study reveals that achieving USML requires methods grounded in human learning, where current LLM-generated explanations do not capture human cognitive constraints and LLMs-as-judges evaluations do not reflect what effectively supports human learning.", "link": "http://arxiv.org/abs/2509.00961v2", "date": "2026-01-27", "relevancy": 2.1028, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Generated%20Explanations%20Do%20Not%20Suffice%20for%20Ultra-Strong%20Machine%20Learning&body=Title%3A%20LLM-Generated%20Explanations%20Do%20Not%20Suffice%20for%20Ultra-Strong%20Machine%20Learning%0AAuthor%3A%20Lun%20Ai%20and%20Johannes%20Langer%20and%20Ute%20Schmid%20and%20Stephen%20Muggleton%0AAbstract%3A%20Ultra%20Strong%20Machine%20Learning%20%28USML%29%20refers%20to%20symbolic%20learning%20systems%20that%20not%20only%20improve%20their%20own%20performance%20but%20can%20also%20teach%20their%20acquired%20knowledge%20to%20quantifiably%20improve%20human%20performance.%20We%20introduce%20LENS%20%28Logic%20Programming%20Explanation%20via%20Neural%20Summarisation%29%2C%20a%20neuro-symbolic%20framework%20that%20combines%20symbolic%20program%20synthesis%20with%20large%20language%20models%20%28LLMs%29.%20This%20framework%20automatically%20generates%20natural%20language%20explanations%20of%20learned%20logic%20programs%2C%20replacing%20hand-crafted%20templates%20used%20in%20prior%20USML%20work.%20Using%20LLMs-as-judges%20evaluation%20and%20expert%20validation%2C%20we%20show%20that%20LENS%20produces%20higher-quality%20explanations%20than%20both%20direct%20LLM%20prompting%20and%20hand-crafted%20templates.%20We%20then%20examine%20whether%20LENS%20explanations%20suffice%20for%20achieving%20USML%20in%20a%20human%20trial%20teaching%20active%20learning%20strategies%20across%20three%20related%20domains.%20Our%20exploratory%20analysis%20suggests%20that%20concise%2C%20expert-written%20explanations%20may%20benefit%20learners%20with%20higher%20initial%20performance%2C%20while%20LLM-generated%20explanations%20provide%20no%20advantage%20over%20human%20self%20learning%20despite%20being%20rated%20as%20higher%20quality.%20This%20case%20study%20reveals%20that%20achieving%20USML%20requires%20methods%20grounded%20in%20human%20learning%2C%20where%20current%20LLM-generated%20explanations%20do%20not%20capture%20human%20cognitive%20constraints%20and%20LLMs-as-judges%20evaluations%20do%20not%20reflect%20what%20effectively%20supports%20human%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2509.00961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Generated%2520Explanations%2520Do%2520Not%2520Suffice%2520for%2520Ultra-Strong%2520Machine%2520Learning%26entry.906535625%3DLun%2520Ai%2520and%2520Johannes%2520Langer%2520and%2520Ute%2520Schmid%2520and%2520Stephen%2520Muggleton%26entry.1292438233%3DUltra%2520Strong%2520Machine%2520Learning%2520%2528USML%2529%2520refers%2520to%2520symbolic%2520learning%2520systems%2520that%2520not%2520only%2520improve%2520their%2520own%2520performance%2520but%2520can%2520also%2520teach%2520their%2520acquired%2520knowledge%2520to%2520quantifiably%2520improve%2520human%2520performance.%2520We%2520introduce%2520LENS%2520%2528Logic%2520Programming%2520Explanation%2520via%2520Neural%2520Summarisation%2529%252C%2520a%2520neuro-symbolic%2520framework%2520that%2520combines%2520symbolic%2520program%2520synthesis%2520with%2520large%2520language%2520models%2520%2528LLMs%2529.%2520This%2520framework%2520automatically%2520generates%2520natural%2520language%2520explanations%2520of%2520learned%2520logic%2520programs%252C%2520replacing%2520hand-crafted%2520templates%2520used%2520in%2520prior%2520USML%2520work.%2520Using%2520LLMs-as-judges%2520evaluation%2520and%2520expert%2520validation%252C%2520we%2520show%2520that%2520LENS%2520produces%2520higher-quality%2520explanations%2520than%2520both%2520direct%2520LLM%2520prompting%2520and%2520hand-crafted%2520templates.%2520We%2520then%2520examine%2520whether%2520LENS%2520explanations%2520suffice%2520for%2520achieving%2520USML%2520in%2520a%2520human%2520trial%2520teaching%2520active%2520learning%2520strategies%2520across%2520three%2520related%2520domains.%2520Our%2520exploratory%2520analysis%2520suggests%2520that%2520concise%252C%2520expert-written%2520explanations%2520may%2520benefit%2520learners%2520with%2520higher%2520initial%2520performance%252C%2520while%2520LLM-generated%2520explanations%2520provide%2520no%2520advantage%2520over%2520human%2520self%2520learning%2520despite%2520being%2520rated%2520as%2520higher%2520quality.%2520This%2520case%2520study%2520reveals%2520that%2520achieving%2520USML%2520requires%2520methods%2520grounded%2520in%2520human%2520learning%252C%2520where%2520current%2520LLM-generated%2520explanations%2520do%2520not%2520capture%2520human%2520cognitive%2520constraints%2520and%2520LLMs-as-judges%2520evaluations%2520do%2520not%2520reflect%2520what%2520effectively%2520supports%2520human%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Generated%20Explanations%20Do%20Not%20Suffice%20for%20Ultra-Strong%20Machine%20Learning&entry.906535625=Lun%20Ai%20and%20Johannes%20Langer%20and%20Ute%20Schmid%20and%20Stephen%20Muggleton&entry.1292438233=Ultra%20Strong%20Machine%20Learning%20%28USML%29%20refers%20to%20symbolic%20learning%20systems%20that%20not%20only%20improve%20their%20own%20performance%20but%20can%20also%20teach%20their%20acquired%20knowledge%20to%20quantifiably%20improve%20human%20performance.%20We%20introduce%20LENS%20%28Logic%20Programming%20Explanation%20via%20Neural%20Summarisation%29%2C%20a%20neuro-symbolic%20framework%20that%20combines%20symbolic%20program%20synthesis%20with%20large%20language%20models%20%28LLMs%29.%20This%20framework%20automatically%20generates%20natural%20language%20explanations%20of%20learned%20logic%20programs%2C%20replacing%20hand-crafted%20templates%20used%20in%20prior%20USML%20work.%20Using%20LLMs-as-judges%20evaluation%20and%20expert%20validation%2C%20we%20show%20that%20LENS%20produces%20higher-quality%20explanations%20than%20both%20direct%20LLM%20prompting%20and%20hand-crafted%20templates.%20We%20then%20examine%20whether%20LENS%20explanations%20suffice%20for%20achieving%20USML%20in%20a%20human%20trial%20teaching%20active%20learning%20strategies%20across%20three%20related%20domains.%20Our%20exploratory%20analysis%20suggests%20that%20concise%2C%20expert-written%20explanations%20may%20benefit%20learners%20with%20higher%20initial%20performance%2C%20while%20LLM-generated%20explanations%20provide%20no%20advantage%20over%20human%20self%20learning%20despite%20being%20rated%20as%20higher%20quality.%20This%20case%20study%20reveals%20that%20achieving%20USML%20requires%20methods%20grounded%20in%20human%20learning%2C%20where%20current%20LLM-generated%20explanations%20do%20not%20capture%20human%20cognitive%20constraints%20and%20LLMs-as-judges%20evaluations%20do%20not%20reflect%20what%20effectively%20supports%20human%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2509.00961v2&entry.124074799=Read"},
{"title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "author": "Piotr Nawrot and Robert Li and Renjie Huang and Sebastian Ruder and Kelly Marchisio and Edoardo M. Ponti", "abstract": "Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its efficiency-accuracy trade-offs remain unclear due to the lack of comprehensive evaluation. We address this gap with the largest-scale empirical analysis to date of training-free sparse attention, evaluating six methods across multiple model families and sizes, sequences up to 128K tokens, and sparsity levels up to 0.95 (i.e., $1/20$ attention budget) on nine diverse tasks. We first organise the rapidly evolving landscape of sparse attention methods into a taxonomy along four design axes. Our analysis then yields actionable insights: 1) sparse attention is effective -- larger sparse models outperform smaller dense ones at equivalent cost, improving the Pareto frontier; 2) due to computational constraints, token-to-page importance estimation is unfeasible during prefilling, where the choice of an alternative solution (global-to-token or block-to-block) depends on the task, but is possible during decoding, enabling better generalisation and tolerance to higher sparsity; 3) longer sequences tolerate higher sparsity, suggesting that fixed-budget methods in production are suboptimal. Together, these findings provide practical guidance for deploying sparse attention and methodological recommendations for future evaluations. Our code is available at https://github.com/PiotrNawrot/sparse-frontier.", "link": "http://arxiv.org/abs/2504.17768v2", "date": "2026-01-27", "relevancy": 2.0977, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5788}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Sparse%20Frontier%3A%20Sparse%20Attention%20Trade-offs%20in%20Transformer%20LLMs&body=Title%3A%20The%20Sparse%20Frontier%3A%20Sparse%20Attention%20Trade-offs%20in%20Transformer%20LLMs%0AAuthor%3A%20Piotr%20Nawrot%20and%20Robert%20Li%20and%20Renjie%20Huang%20and%20Sebastian%20Ruder%20and%20Kelly%20Marchisio%20and%20Edoardo%20M.%20Ponti%0AAbstract%3A%20Sparse%20attention%20offers%20a%20promising%20strategy%20to%20extend%20long-context%20capabilities%20in%20Transformer%20LLMs%2C%20yet%20its%20efficiency-accuracy%20trade-offs%20remain%20unclear%20due%20to%20the%20lack%20of%20comprehensive%20evaluation.%20We%20address%20this%20gap%20with%20the%20largest-scale%20empirical%20analysis%20to%20date%20of%20training-free%20sparse%20attention%2C%20evaluating%20six%20methods%20across%20multiple%20model%20families%20and%20sizes%2C%20sequences%20up%20to%20128K%20tokens%2C%20and%20sparsity%20levels%20up%20to%200.95%20%28i.e.%2C%20%241/20%24%20attention%20budget%29%20on%20nine%20diverse%20tasks.%20We%20first%20organise%20the%20rapidly%20evolving%20landscape%20of%20sparse%20attention%20methods%20into%20a%20taxonomy%20along%20four%20design%20axes.%20Our%20analysis%20then%20yields%20actionable%20insights%3A%201%29%20sparse%20attention%20is%20effective%20--%20larger%20sparse%20models%20outperform%20smaller%20dense%20ones%20at%20equivalent%20cost%2C%20improving%20the%20Pareto%20frontier%3B%202%29%20due%20to%20computational%20constraints%2C%20token-to-page%20importance%20estimation%20is%20unfeasible%20during%20prefilling%2C%20where%20the%20choice%20of%20an%20alternative%20solution%20%28global-to-token%20or%20block-to-block%29%20depends%20on%20the%20task%2C%20but%20is%20possible%20during%20decoding%2C%20enabling%20better%20generalisation%20and%20tolerance%20to%20higher%20sparsity%3B%203%29%20longer%20sequences%20tolerate%20higher%20sparsity%2C%20suggesting%20that%20fixed-budget%20methods%20in%20production%20are%20suboptimal.%20Together%2C%20these%20findings%20provide%20practical%20guidance%20for%20deploying%20sparse%20attention%20and%20methodological%20recommendations%20for%20future%20evaluations.%20Our%20code%20is%20available%20at%20https%3A//github.com/PiotrNawrot/sparse-frontier.%0ALink%3A%20http%3A//arxiv.org/abs/2504.17768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Sparse%2520Frontier%253A%2520Sparse%2520Attention%2520Trade-offs%2520in%2520Transformer%2520LLMs%26entry.906535625%3DPiotr%2520Nawrot%2520and%2520Robert%2520Li%2520and%2520Renjie%2520Huang%2520and%2520Sebastian%2520Ruder%2520and%2520Kelly%2520Marchisio%2520and%2520Edoardo%2520M.%2520Ponti%26entry.1292438233%3DSparse%2520attention%2520offers%2520a%2520promising%2520strategy%2520to%2520extend%2520long-context%2520capabilities%2520in%2520Transformer%2520LLMs%252C%2520yet%2520its%2520efficiency-accuracy%2520trade-offs%2520remain%2520unclear%2520due%2520to%2520the%2520lack%2520of%2520comprehensive%2520evaluation.%2520We%2520address%2520this%2520gap%2520with%2520the%2520largest-scale%2520empirical%2520analysis%2520to%2520date%2520of%2520training-free%2520sparse%2520attention%252C%2520evaluating%2520six%2520methods%2520across%2520multiple%2520model%2520families%2520and%2520sizes%252C%2520sequences%2520up%2520to%2520128K%2520tokens%252C%2520and%2520sparsity%2520levels%2520up%2520to%25200.95%2520%2528i.e.%252C%2520%25241/20%2524%2520attention%2520budget%2529%2520on%2520nine%2520diverse%2520tasks.%2520We%2520first%2520organise%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520sparse%2520attention%2520methods%2520into%2520a%2520taxonomy%2520along%2520four%2520design%2520axes.%2520Our%2520analysis%2520then%2520yields%2520actionable%2520insights%253A%25201%2529%2520sparse%2520attention%2520is%2520effective%2520--%2520larger%2520sparse%2520models%2520outperform%2520smaller%2520dense%2520ones%2520at%2520equivalent%2520cost%252C%2520improving%2520the%2520Pareto%2520frontier%253B%25202%2529%2520due%2520to%2520computational%2520constraints%252C%2520token-to-page%2520importance%2520estimation%2520is%2520unfeasible%2520during%2520prefilling%252C%2520where%2520the%2520choice%2520of%2520an%2520alternative%2520solution%2520%2528global-to-token%2520or%2520block-to-block%2529%2520depends%2520on%2520the%2520task%252C%2520but%2520is%2520possible%2520during%2520decoding%252C%2520enabling%2520better%2520generalisation%2520and%2520tolerance%2520to%2520higher%2520sparsity%253B%25203%2529%2520longer%2520sequences%2520tolerate%2520higher%2520sparsity%252C%2520suggesting%2520that%2520fixed-budget%2520methods%2520in%2520production%2520are%2520suboptimal.%2520Together%252C%2520these%2520findings%2520provide%2520practical%2520guidance%2520for%2520deploying%2520sparse%2520attention%2520and%2520methodological%2520recommendations%2520for%2520future%2520evaluations.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/PiotrNawrot/sparse-frontier.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Sparse%20Frontier%3A%20Sparse%20Attention%20Trade-offs%20in%20Transformer%20LLMs&entry.906535625=Piotr%20Nawrot%20and%20Robert%20Li%20and%20Renjie%20Huang%20and%20Sebastian%20Ruder%20and%20Kelly%20Marchisio%20and%20Edoardo%20M.%20Ponti&entry.1292438233=Sparse%20attention%20offers%20a%20promising%20strategy%20to%20extend%20long-context%20capabilities%20in%20Transformer%20LLMs%2C%20yet%20its%20efficiency-accuracy%20trade-offs%20remain%20unclear%20due%20to%20the%20lack%20of%20comprehensive%20evaluation.%20We%20address%20this%20gap%20with%20the%20largest-scale%20empirical%20analysis%20to%20date%20of%20training-free%20sparse%20attention%2C%20evaluating%20six%20methods%20across%20multiple%20model%20families%20and%20sizes%2C%20sequences%20up%20to%20128K%20tokens%2C%20and%20sparsity%20levels%20up%20to%200.95%20%28i.e.%2C%20%241/20%24%20attention%20budget%29%20on%20nine%20diverse%20tasks.%20We%20first%20organise%20the%20rapidly%20evolving%20landscape%20of%20sparse%20attention%20methods%20into%20a%20taxonomy%20along%20four%20design%20axes.%20Our%20analysis%20then%20yields%20actionable%20insights%3A%201%29%20sparse%20attention%20is%20effective%20--%20larger%20sparse%20models%20outperform%20smaller%20dense%20ones%20at%20equivalent%20cost%2C%20improving%20the%20Pareto%20frontier%3B%202%29%20due%20to%20computational%20constraints%2C%20token-to-page%20importance%20estimation%20is%20unfeasible%20during%20prefilling%2C%20where%20the%20choice%20of%20an%20alternative%20solution%20%28global-to-token%20or%20block-to-block%29%20depends%20on%20the%20task%2C%20but%20is%20possible%20during%20decoding%2C%20enabling%20better%20generalisation%20and%20tolerance%20to%20higher%20sparsity%3B%203%29%20longer%20sequences%20tolerate%20higher%20sparsity%2C%20suggesting%20that%20fixed-budget%20methods%20in%20production%20are%20suboptimal.%20Together%2C%20these%20findings%20provide%20practical%20guidance%20for%20deploying%20sparse%20attention%20and%20methodological%20recommendations%20for%20future%20evaluations.%20Our%20code%20is%20available%20at%20https%3A//github.com/PiotrNawrot/sparse-frontier.&entry.1838667208=http%3A//arxiv.org/abs/2504.17768v2&entry.124074799=Read"},
{"title": "A Riemannian Take on Distance Fields and Geodesic Flows in Robotics", "author": "Yiming Li and Jiacheng Qiu and Sylvain Calinon", "abstract": "Distance functions are crucial in robotics for representing spatial relationships between a robot and its environment. They provide an implicit, continuous, and differentiable representation that integrates seamlessly with control, optimization, and learning. While standard distance fields rely on the Euclidean metric, many robotic tasks inherently involve non-Euclidean structures. To this end, we generalize Euclidean distance fields to more general metric spaces by solving the Riemannian eikonal equation, a first-order partial differential equation whose solution defines a distance field and its associated gradient flow on the manifold, enabling the computation of geodesics and globally length-minimizing paths. We demonstrate that geodesic distance fields, the classical Riemannian distance function represented as a global, continuous, and queryable field, are effective for a broad class of robotic problems where Riemannian geometry naturally arises. To realize this, we present a neural Riemannian eikonal solver (NES) that solves the equation as a mesh-free implicit representation without grid discretization, scaling to high-dimensional robot manipulators. Training leverages a physics-informed neural network (PINN) objective that constrains spatial derivatives via the PDE residual and boundary and metric conditions, so the model is supervised by the governing equation and requires no labeled distances or geodesics. We propose two NES variants, conditioned on boundary data and on spatially varying Riemannian metrics, underscoring the flexibility of the neural parameterization. We validate the effectiveness of our approach through extensive examples, yielding minimal-length geodesics across diverse robot tasks involving Riemannian geometry.", "link": "http://arxiv.org/abs/2412.05197v3", "date": "2026-01-27", "relevancy": 2.0936, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5636}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5161}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Riemannian%20Take%20on%20Distance%20Fields%20and%20Geodesic%20Flows%20in%20Robotics&body=Title%3A%20A%20Riemannian%20Take%20on%20Distance%20Fields%20and%20Geodesic%20Flows%20in%20Robotics%0AAuthor%3A%20Yiming%20Li%20and%20Jiacheng%20Qiu%20and%20Sylvain%20Calinon%0AAbstract%3A%20Distance%20functions%20are%20crucial%20in%20robotics%20for%20representing%20spatial%20relationships%20between%20a%20robot%20and%20its%20environment.%20They%20provide%20an%20implicit%2C%20continuous%2C%20and%20differentiable%20representation%20that%20integrates%20seamlessly%20with%20control%2C%20optimization%2C%20and%20learning.%20While%20standard%20distance%20fields%20rely%20on%20the%20Euclidean%20metric%2C%20many%20robotic%20tasks%20inherently%20involve%20non-Euclidean%20structures.%20To%20this%20end%2C%20we%20generalize%20Euclidean%20distance%20fields%20to%20more%20general%20metric%20spaces%20by%20solving%20the%20Riemannian%20eikonal%20equation%2C%20a%20first-order%20partial%20differential%20equation%20whose%20solution%20defines%20a%20distance%20field%20and%20its%20associated%20gradient%20flow%20on%20the%20manifold%2C%20enabling%20the%20computation%20of%20geodesics%20and%20globally%20length-minimizing%20paths.%20We%20demonstrate%20that%20geodesic%20distance%20fields%2C%20the%20classical%20Riemannian%20distance%20function%20represented%20as%20a%20global%2C%20continuous%2C%20and%20queryable%20field%2C%20are%20effective%20for%20a%20broad%20class%20of%20robotic%20problems%20where%20Riemannian%20geometry%20naturally%20arises.%20To%20realize%20this%2C%20we%20present%20a%20neural%20Riemannian%20eikonal%20solver%20%28NES%29%20that%20solves%20the%20equation%20as%20a%20mesh-free%20implicit%20representation%20without%20grid%20discretization%2C%20scaling%20to%20high-dimensional%20robot%20manipulators.%20Training%20leverages%20a%20physics-informed%20neural%20network%20%28PINN%29%20objective%20that%20constrains%20spatial%20derivatives%20via%20the%20PDE%20residual%20and%20boundary%20and%20metric%20conditions%2C%20so%20the%20model%20is%20supervised%20by%20the%20governing%20equation%20and%20requires%20no%20labeled%20distances%20or%20geodesics.%20We%20propose%20two%20NES%20variants%2C%20conditioned%20on%20boundary%20data%20and%20on%20spatially%20varying%20Riemannian%20metrics%2C%20underscoring%20the%20flexibility%20of%20the%20neural%20parameterization.%20We%20validate%20the%20effectiveness%20of%20our%20approach%20through%20extensive%20examples%2C%20yielding%20minimal-length%20geodesics%20across%20diverse%20robot%20tasks%20involving%20Riemannian%20geometry.%0ALink%3A%20http%3A//arxiv.org/abs/2412.05197v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Riemannian%2520Take%2520on%2520Distance%2520Fields%2520and%2520Geodesic%2520Flows%2520in%2520Robotics%26entry.906535625%3DYiming%2520Li%2520and%2520Jiacheng%2520Qiu%2520and%2520Sylvain%2520Calinon%26entry.1292438233%3DDistance%2520functions%2520are%2520crucial%2520in%2520robotics%2520for%2520representing%2520spatial%2520relationships%2520between%2520a%2520robot%2520and%2520its%2520environment.%2520They%2520provide%2520an%2520implicit%252C%2520continuous%252C%2520and%2520differentiable%2520representation%2520that%2520integrates%2520seamlessly%2520with%2520control%252C%2520optimization%252C%2520and%2520learning.%2520While%2520standard%2520distance%2520fields%2520rely%2520on%2520the%2520Euclidean%2520metric%252C%2520many%2520robotic%2520tasks%2520inherently%2520involve%2520non-Euclidean%2520structures.%2520To%2520this%2520end%252C%2520we%2520generalize%2520Euclidean%2520distance%2520fields%2520to%2520more%2520general%2520metric%2520spaces%2520by%2520solving%2520the%2520Riemannian%2520eikonal%2520equation%252C%2520a%2520first-order%2520partial%2520differential%2520equation%2520whose%2520solution%2520defines%2520a%2520distance%2520field%2520and%2520its%2520associated%2520gradient%2520flow%2520on%2520the%2520manifold%252C%2520enabling%2520the%2520computation%2520of%2520geodesics%2520and%2520globally%2520length-minimizing%2520paths.%2520We%2520demonstrate%2520that%2520geodesic%2520distance%2520fields%252C%2520the%2520classical%2520Riemannian%2520distance%2520function%2520represented%2520as%2520a%2520global%252C%2520continuous%252C%2520and%2520queryable%2520field%252C%2520are%2520effective%2520for%2520a%2520broad%2520class%2520of%2520robotic%2520problems%2520where%2520Riemannian%2520geometry%2520naturally%2520arises.%2520To%2520realize%2520this%252C%2520we%2520present%2520a%2520neural%2520Riemannian%2520eikonal%2520solver%2520%2528NES%2529%2520that%2520solves%2520the%2520equation%2520as%2520a%2520mesh-free%2520implicit%2520representation%2520without%2520grid%2520discretization%252C%2520scaling%2520to%2520high-dimensional%2520robot%2520manipulators.%2520Training%2520leverages%2520a%2520physics-informed%2520neural%2520network%2520%2528PINN%2529%2520objective%2520that%2520constrains%2520spatial%2520derivatives%2520via%2520the%2520PDE%2520residual%2520and%2520boundary%2520and%2520metric%2520conditions%252C%2520so%2520the%2520model%2520is%2520supervised%2520by%2520the%2520governing%2520equation%2520and%2520requires%2520no%2520labeled%2520distances%2520or%2520geodesics.%2520We%2520propose%2520two%2520NES%2520variants%252C%2520conditioned%2520on%2520boundary%2520data%2520and%2520on%2520spatially%2520varying%2520Riemannian%2520metrics%252C%2520underscoring%2520the%2520flexibility%2520of%2520the%2520neural%2520parameterization.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%2520extensive%2520examples%252C%2520yielding%2520minimal-length%2520geodesics%2520across%2520diverse%2520robot%2520tasks%2520involving%2520Riemannian%2520geometry.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05197v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Riemannian%20Take%20on%20Distance%20Fields%20and%20Geodesic%20Flows%20in%20Robotics&entry.906535625=Yiming%20Li%20and%20Jiacheng%20Qiu%20and%20Sylvain%20Calinon&entry.1292438233=Distance%20functions%20are%20crucial%20in%20robotics%20for%20representing%20spatial%20relationships%20between%20a%20robot%20and%20its%20environment.%20They%20provide%20an%20implicit%2C%20continuous%2C%20and%20differentiable%20representation%20that%20integrates%20seamlessly%20with%20control%2C%20optimization%2C%20and%20learning.%20While%20standard%20distance%20fields%20rely%20on%20the%20Euclidean%20metric%2C%20many%20robotic%20tasks%20inherently%20involve%20non-Euclidean%20structures.%20To%20this%20end%2C%20we%20generalize%20Euclidean%20distance%20fields%20to%20more%20general%20metric%20spaces%20by%20solving%20the%20Riemannian%20eikonal%20equation%2C%20a%20first-order%20partial%20differential%20equation%20whose%20solution%20defines%20a%20distance%20field%20and%20its%20associated%20gradient%20flow%20on%20the%20manifold%2C%20enabling%20the%20computation%20of%20geodesics%20and%20globally%20length-minimizing%20paths.%20We%20demonstrate%20that%20geodesic%20distance%20fields%2C%20the%20classical%20Riemannian%20distance%20function%20represented%20as%20a%20global%2C%20continuous%2C%20and%20queryable%20field%2C%20are%20effective%20for%20a%20broad%20class%20of%20robotic%20problems%20where%20Riemannian%20geometry%20naturally%20arises.%20To%20realize%20this%2C%20we%20present%20a%20neural%20Riemannian%20eikonal%20solver%20%28NES%29%20that%20solves%20the%20equation%20as%20a%20mesh-free%20implicit%20representation%20without%20grid%20discretization%2C%20scaling%20to%20high-dimensional%20robot%20manipulators.%20Training%20leverages%20a%20physics-informed%20neural%20network%20%28PINN%29%20objective%20that%20constrains%20spatial%20derivatives%20via%20the%20PDE%20residual%20and%20boundary%20and%20metric%20conditions%2C%20so%20the%20model%20is%20supervised%20by%20the%20governing%20equation%20and%20requires%20no%20labeled%20distances%20or%20geodesics.%20We%20propose%20two%20NES%20variants%2C%20conditioned%20on%20boundary%20data%20and%20on%20spatially%20varying%20Riemannian%20metrics%2C%20underscoring%20the%20flexibility%20of%20the%20neural%20parameterization.%20We%20validate%20the%20effectiveness%20of%20our%20approach%20through%20extensive%20examples%2C%20yielding%20minimal-length%20geodesics%20across%20diverse%20robot%20tasks%20involving%20Riemannian%20geometry.&entry.1838667208=http%3A//arxiv.org/abs/2412.05197v3&entry.124074799=Read"},
{"title": "Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues", "author": "Junchen Fu and Wenhao Deng and Kaiwen Zheng and Alexandros Karatzoglou and Ioannis Arapakis and Yu Ye and Yongxin Ni and Joemon M. Jose and Xuri Ge", "abstract": "Missing-modality information on e-commerce platforms, such as absent product images or textual descriptions, often arises from annotation errors or incomplete metadata, impairing both product presentation and downstream applications such as recommendation systems. Motivated by the multimodal generative capabilities of recent Multimodal Large Language Models (MLLMs), this work investigates a fundamental yet underexplored question: can MLLMs generate missing modalities for products in e-commerce scenarios? We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.\n  We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks. Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment. In addition, performance varies substantially across product categories and model scales, and we observe no trivial correlation between model size and performance, in contrast to trends commonly reported in mainstream benchmarks. We also explore Group Relative Policy Optimization (GRPO) to better align MLLMs with this task. GRPO improves image-to-text completion but does not yield gains for text-to-image completion. Overall, these findings expose the limitations of current MLLMs in real-world cross-modal generation and represent an early step toward more effective missing-modality product completion.", "link": "http://arxiv.org/abs/2601.19750v1", "date": "2026-01-27", "relevancy": 2.0859, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5503}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5171}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Missing%20Modality%20Completion%20in%20Product%20Catalogues&body=Title%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Missing%20Modality%20Completion%20in%20Product%20Catalogues%0AAuthor%3A%20Junchen%20Fu%20and%20Wenhao%20Deng%20and%20Kaiwen%20Zheng%20and%20Alexandros%20Karatzoglou%20and%20Ioannis%20Arapakis%20and%20Yu%20Ye%20and%20Yongxin%20Ni%20and%20Joemon%20M.%20Jose%20and%20Xuri%20Ge%0AAbstract%3A%20Missing-modality%20information%20on%20e-commerce%20platforms%2C%20such%20as%20absent%20product%20images%20or%20textual%20descriptions%2C%20often%20arises%20from%20annotation%20errors%20or%20incomplete%20metadata%2C%20impairing%20both%20product%20presentation%20and%20downstream%20applications%20such%20as%20recommendation%20systems.%20Motivated%20by%20the%20multimodal%20generative%20capabilities%20of%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20this%20work%20investigates%20a%20fundamental%20yet%20underexplored%20question%3A%20can%20MLLMs%20generate%20missing%20modalities%20for%20products%20in%20e-commerce%20scenarios%3F%20We%20propose%20the%20Missing%20Modality%20Product%20Completion%20Benchmark%20%28MMPCBench%29%2C%20which%20consists%20of%20two%20sub-benchmarks%3A%20a%20Content%20Quality%20Completion%20Benchmark%20and%20a%20Recommendation%20Benchmark.%0A%20%20We%20further%20evaluate%20six%20state-of-the-art%20MLLMs%20from%20the%20Qwen2.5-VL%20and%20Gemma-3%20model%20families%20across%20nine%20real-world%20e-commerce%20categories%2C%20focusing%20on%20image-to-text%20and%20text-to-image%20completion%20tasks.%20Experimental%20results%20show%20that%20while%20MLLMs%20can%20capture%20high-level%20semantics%2C%20they%20struggle%20with%20fine-grained%20word-level%20and%20pixel-%20or%20patch-level%20alignment.%20In%20addition%2C%20performance%20varies%20substantially%20across%20product%20categories%20and%20model%20scales%2C%20and%20we%20observe%20no%20trivial%20correlation%20between%20model%20size%20and%20performance%2C%20in%20contrast%20to%20trends%20commonly%20reported%20in%20mainstream%20benchmarks.%20We%20also%20explore%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20better%20align%20MLLMs%20with%20this%20task.%20GRPO%20improves%20image-to-text%20completion%20but%20does%20not%20yield%20gains%20for%20text-to-image%20completion.%20Overall%2C%20these%20findings%20expose%20the%20limitations%20of%20current%20MLLMs%20in%20real-world%20cross-modal%20generation%20and%20represent%20an%20early%20step%20toward%20more%20effective%20missing-modality%20product%20completion.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520Missing%2520Modality%2520Completion%2520in%2520Product%2520Catalogues%26entry.906535625%3DJunchen%2520Fu%2520and%2520Wenhao%2520Deng%2520and%2520Kaiwen%2520Zheng%2520and%2520Alexandros%2520Karatzoglou%2520and%2520Ioannis%2520Arapakis%2520and%2520Yu%2520Ye%2520and%2520Yongxin%2520Ni%2520and%2520Joemon%2520M.%2520Jose%2520and%2520Xuri%2520Ge%26entry.1292438233%3DMissing-modality%2520information%2520on%2520e-commerce%2520platforms%252C%2520such%2520as%2520absent%2520product%2520images%2520or%2520textual%2520descriptions%252C%2520often%2520arises%2520from%2520annotation%2520errors%2520or%2520incomplete%2520metadata%252C%2520impairing%2520both%2520product%2520presentation%2520and%2520downstream%2520applications%2520such%2520as%2520recommendation%2520systems.%2520Motivated%2520by%2520the%2520multimodal%2520generative%2520capabilities%2520of%2520recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520this%2520work%2520investigates%2520a%2520fundamental%2520yet%2520underexplored%2520question%253A%2520can%2520MLLMs%2520generate%2520missing%2520modalities%2520for%2520products%2520in%2520e-commerce%2520scenarios%253F%2520We%2520propose%2520the%2520Missing%2520Modality%2520Product%2520Completion%2520Benchmark%2520%2528MMPCBench%2529%252C%2520which%2520consists%2520of%2520two%2520sub-benchmarks%253A%2520a%2520Content%2520Quality%2520Completion%2520Benchmark%2520and%2520a%2520Recommendation%2520Benchmark.%250A%2520%2520We%2520further%2520evaluate%2520six%2520state-of-the-art%2520MLLMs%2520from%2520the%2520Qwen2.5-VL%2520and%2520Gemma-3%2520model%2520families%2520across%2520nine%2520real-world%2520e-commerce%2520categories%252C%2520focusing%2520on%2520image-to-text%2520and%2520text-to-image%2520completion%2520tasks.%2520Experimental%2520results%2520show%2520that%2520while%2520MLLMs%2520can%2520capture%2520high-level%2520semantics%252C%2520they%2520struggle%2520with%2520fine-grained%2520word-level%2520and%2520pixel-%2520or%2520patch-level%2520alignment.%2520In%2520addition%252C%2520performance%2520varies%2520substantially%2520across%2520product%2520categories%2520and%2520model%2520scales%252C%2520and%2520we%2520observe%2520no%2520trivial%2520correlation%2520between%2520model%2520size%2520and%2520performance%252C%2520in%2520contrast%2520to%2520trends%2520commonly%2520reported%2520in%2520mainstream%2520benchmarks.%2520We%2520also%2520explore%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520better%2520align%2520MLLMs%2520with%2520this%2520task.%2520GRPO%2520improves%2520image-to-text%2520completion%2520but%2520does%2520not%2520yield%2520gains%2520for%2520text-to-image%2520completion.%2520Overall%252C%2520these%2520findings%2520expose%2520the%2520limitations%2520of%2520current%2520MLLMs%2520in%2520real-world%2520cross-modal%2520generation%2520and%2520represent%2520an%2520early%2520step%2520toward%2520more%2520effective%2520missing-modality%2520product%2520completion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Missing%20Modality%20Completion%20in%20Product%20Catalogues&entry.906535625=Junchen%20Fu%20and%20Wenhao%20Deng%20and%20Kaiwen%20Zheng%20and%20Alexandros%20Karatzoglou%20and%20Ioannis%20Arapakis%20and%20Yu%20Ye%20and%20Yongxin%20Ni%20and%20Joemon%20M.%20Jose%20and%20Xuri%20Ge&entry.1292438233=Missing-modality%20information%20on%20e-commerce%20platforms%2C%20such%20as%20absent%20product%20images%20or%20textual%20descriptions%2C%20often%20arises%20from%20annotation%20errors%20or%20incomplete%20metadata%2C%20impairing%20both%20product%20presentation%20and%20downstream%20applications%20such%20as%20recommendation%20systems.%20Motivated%20by%20the%20multimodal%20generative%20capabilities%20of%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20this%20work%20investigates%20a%20fundamental%20yet%20underexplored%20question%3A%20can%20MLLMs%20generate%20missing%20modalities%20for%20products%20in%20e-commerce%20scenarios%3F%20We%20propose%20the%20Missing%20Modality%20Product%20Completion%20Benchmark%20%28MMPCBench%29%2C%20which%20consists%20of%20two%20sub-benchmarks%3A%20a%20Content%20Quality%20Completion%20Benchmark%20and%20a%20Recommendation%20Benchmark.%0A%20%20We%20further%20evaluate%20six%20state-of-the-art%20MLLMs%20from%20the%20Qwen2.5-VL%20and%20Gemma-3%20model%20families%20across%20nine%20real-world%20e-commerce%20categories%2C%20focusing%20on%20image-to-text%20and%20text-to-image%20completion%20tasks.%20Experimental%20results%20show%20that%20while%20MLLMs%20can%20capture%20high-level%20semantics%2C%20they%20struggle%20with%20fine-grained%20word-level%20and%20pixel-%20or%20patch-level%20alignment.%20In%20addition%2C%20performance%20varies%20substantially%20across%20product%20categories%20and%20model%20scales%2C%20and%20we%20observe%20no%20trivial%20correlation%20between%20model%20size%20and%20performance%2C%20in%20contrast%20to%20trends%20commonly%20reported%20in%20mainstream%20benchmarks.%20We%20also%20explore%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20better%20align%20MLLMs%20with%20this%20task.%20GRPO%20improves%20image-to-text%20completion%20but%20does%20not%20yield%20gains%20for%20text-to-image%20completion.%20Overall%2C%20these%20findings%20expose%20the%20limitations%20of%20current%20MLLMs%20in%20real-world%20cross-modal%20generation%20and%20represent%20an%20early%20step%20toward%20more%20effective%20missing-modality%20product%20completion.&entry.1838667208=http%3A//arxiv.org/abs/2601.19750v1&entry.124074799=Read"},
{"title": "A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation", "author": "Jin Huang and Zichen Liu and Haoda Li and Zhikun Wang and Ying Chen", "abstract": "In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance.", "link": "http://arxiv.org/abs/2601.19509v1", "date": "2026-01-27", "relevancy": 2.08, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5115}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20DVL%20Aided%20Loosely%20Coupled%20Inertial%20Navigation%20Strategy%20for%20AUVs%20with%20Attitude%20Error%20Modeling%20and%20Variance%20Propagation&body=Title%3A%20A%20DVL%20Aided%20Loosely%20Coupled%20Inertial%20Navigation%20Strategy%20for%20AUVs%20with%20Attitude%20Error%20Modeling%20and%20Variance%20Propagation%0AAuthor%3A%20Jin%20Huang%20and%20Zichen%20Liu%20and%20Haoda%20Li%20and%20Zhikun%20Wang%20and%20Ying%20Chen%0AAbstract%3A%20In%20underwater%20navigation%20systems%2C%20strap-down%20inertial%20navigation%20system/Doppler%20velocity%20log%20%28SINS/DVL%29-based%20loosely%20coupled%20architectures%20are%20widely%20adopted.%20Conventional%20approaches%20project%20DVL%20velocities%20from%20the%20body%20coordinate%20system%20to%20the%20navigation%20coordinate%20system%20using%20SINS-derived%20attitude%3B%20however%2C%20accumulated%20attitude%20estimation%20errors%20introduce%20biases%20into%20velocity%20projection%20and%20degrade%20navigation%20performance%20during%20long-term%20operation.%20To%20address%20this%20issue%2C%20two%20complementary%20improvements%20are%20introduced.%20First%2C%20a%20vehicle%20attitude%20error-aware%20DVL%20velocity%20transformation%20model%20is%20formulated%20by%20incorporating%20attitude%20error%20terms%20into%20the%20observation%20equation%20to%20reduce%20projection-induced%20velocity%20bias.%20Second%2C%20a%20covariance%20matrix-based%20variance%20propagation%20method%20is%20developed%20to%20transform%20DVL%20measurement%20uncertainty%20across%20coordinate%20systems%2C%20introducing%20an%20expectation-based%20attitude%20error%20compensation%20term%20to%20achieve%20statistically%20consistent%20noise%20modeling.%20Simulation%20and%20field%20experiment%20results%20demonstrate%20that%20both%20improvements%20individually%20enhance%20navigation%20accuracy%20and%20confirm%20that%20accumulated%20attitude%20errors%20affect%20both%20projected%20velocity%20measurements%20and%20their%20associated%20uncertainty.%20When%20jointly%20applied%2C%20long-term%20error%20divergence%20is%20effectively%20suppressed.%20Field%20experimental%20results%20show%20that%20the%20proposed%20approach%20achieves%20a%2078.3%25%20improvement%20in%203D%20position%20RMSE%20and%20a%2071.8%25%20reduction%20in%20the%20maximum%20component-wise%20position%20error%20compared%20with%20the%20baseline%20IMU%2BDVL%20method%2C%20providing%20a%20robust%20solution%20for%20improving%20long-term%20SINS/DVL%20navigation%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520DVL%2520Aided%2520Loosely%2520Coupled%2520Inertial%2520Navigation%2520Strategy%2520for%2520AUVs%2520with%2520Attitude%2520Error%2520Modeling%2520and%2520Variance%2520Propagation%26entry.906535625%3DJin%2520Huang%2520and%2520Zichen%2520Liu%2520and%2520Haoda%2520Li%2520and%2520Zhikun%2520Wang%2520and%2520Ying%2520Chen%26entry.1292438233%3DIn%2520underwater%2520navigation%2520systems%252C%2520strap-down%2520inertial%2520navigation%2520system/Doppler%2520velocity%2520log%2520%2528SINS/DVL%2529-based%2520loosely%2520coupled%2520architectures%2520are%2520widely%2520adopted.%2520Conventional%2520approaches%2520project%2520DVL%2520velocities%2520from%2520the%2520body%2520coordinate%2520system%2520to%2520the%2520navigation%2520coordinate%2520system%2520using%2520SINS-derived%2520attitude%253B%2520however%252C%2520accumulated%2520attitude%2520estimation%2520errors%2520introduce%2520biases%2520into%2520velocity%2520projection%2520and%2520degrade%2520navigation%2520performance%2520during%2520long-term%2520operation.%2520To%2520address%2520this%2520issue%252C%2520two%2520complementary%2520improvements%2520are%2520introduced.%2520First%252C%2520a%2520vehicle%2520attitude%2520error-aware%2520DVL%2520velocity%2520transformation%2520model%2520is%2520formulated%2520by%2520incorporating%2520attitude%2520error%2520terms%2520into%2520the%2520observation%2520equation%2520to%2520reduce%2520projection-induced%2520velocity%2520bias.%2520Second%252C%2520a%2520covariance%2520matrix-based%2520variance%2520propagation%2520method%2520is%2520developed%2520to%2520transform%2520DVL%2520measurement%2520uncertainty%2520across%2520coordinate%2520systems%252C%2520introducing%2520an%2520expectation-based%2520attitude%2520error%2520compensation%2520term%2520to%2520achieve%2520statistically%2520consistent%2520noise%2520modeling.%2520Simulation%2520and%2520field%2520experiment%2520results%2520demonstrate%2520that%2520both%2520improvements%2520individually%2520enhance%2520navigation%2520accuracy%2520and%2520confirm%2520that%2520accumulated%2520attitude%2520errors%2520affect%2520both%2520projected%2520velocity%2520measurements%2520and%2520their%2520associated%2520uncertainty.%2520When%2520jointly%2520applied%252C%2520long-term%2520error%2520divergence%2520is%2520effectively%2520suppressed.%2520Field%2520experimental%2520results%2520show%2520that%2520the%2520proposed%2520approach%2520achieves%2520a%252078.3%2525%2520improvement%2520in%25203D%2520position%2520RMSE%2520and%2520a%252071.8%2525%2520reduction%2520in%2520the%2520maximum%2520component-wise%2520position%2520error%2520compared%2520with%2520the%2520baseline%2520IMU%252BDVL%2520method%252C%2520providing%2520a%2520robust%2520solution%2520for%2520improving%2520long-term%2520SINS/DVL%2520navigation%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20DVL%20Aided%20Loosely%20Coupled%20Inertial%20Navigation%20Strategy%20for%20AUVs%20with%20Attitude%20Error%20Modeling%20and%20Variance%20Propagation&entry.906535625=Jin%20Huang%20and%20Zichen%20Liu%20and%20Haoda%20Li%20and%20Zhikun%20Wang%20and%20Ying%20Chen&entry.1292438233=In%20underwater%20navigation%20systems%2C%20strap-down%20inertial%20navigation%20system/Doppler%20velocity%20log%20%28SINS/DVL%29-based%20loosely%20coupled%20architectures%20are%20widely%20adopted.%20Conventional%20approaches%20project%20DVL%20velocities%20from%20the%20body%20coordinate%20system%20to%20the%20navigation%20coordinate%20system%20using%20SINS-derived%20attitude%3B%20however%2C%20accumulated%20attitude%20estimation%20errors%20introduce%20biases%20into%20velocity%20projection%20and%20degrade%20navigation%20performance%20during%20long-term%20operation.%20To%20address%20this%20issue%2C%20two%20complementary%20improvements%20are%20introduced.%20First%2C%20a%20vehicle%20attitude%20error-aware%20DVL%20velocity%20transformation%20model%20is%20formulated%20by%20incorporating%20attitude%20error%20terms%20into%20the%20observation%20equation%20to%20reduce%20projection-induced%20velocity%20bias.%20Second%2C%20a%20covariance%20matrix-based%20variance%20propagation%20method%20is%20developed%20to%20transform%20DVL%20measurement%20uncertainty%20across%20coordinate%20systems%2C%20introducing%20an%20expectation-based%20attitude%20error%20compensation%20term%20to%20achieve%20statistically%20consistent%20noise%20modeling.%20Simulation%20and%20field%20experiment%20results%20demonstrate%20that%20both%20improvements%20individually%20enhance%20navigation%20accuracy%20and%20confirm%20that%20accumulated%20attitude%20errors%20affect%20both%20projected%20velocity%20measurements%20and%20their%20associated%20uncertainty.%20When%20jointly%20applied%2C%20long-term%20error%20divergence%20is%20effectively%20suppressed.%20Field%20experimental%20results%20show%20that%20the%20proposed%20approach%20achieves%20a%2078.3%25%20improvement%20in%203D%20position%20RMSE%20and%20a%2071.8%25%20reduction%20in%20the%20maximum%20component-wise%20position%20error%20compared%20with%20the%20baseline%20IMU%2BDVL%20method%2C%20providing%20a%20robust%20solution%20for%20improving%20long-term%20SINS/DVL%20navigation%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.19509v1&entry.124074799=Read"},
{"title": "Synchronization on circles and spheres with nonlinear interactions", "author": "Christopher Criscitiello and Quentin Rebjock and Andrew D. McRae and Nicolas Boumal", "abstract": "We consider the dynamics of $n$ points on a sphere in $\\mathbb{R}^d$ ($d \\geq 2$) which attract each other according to a function $\\varphi$ of their inner products. When $\\varphi$ is linear ($\\varphi(t) = t$), the points converge to a common value (i.e., synchronize) in various connectivity scenarios: this is part of classical work on Kuramoto oscillator networks. When $\\varphi$ is exponential ($\\varphi(t) = e^{\u03b2t}$), these dynamics correspond to a limit of how idealized transformers process data, as described by Geshkovski et al. (2025). Accordingly, they ask whether synchronization occurs for exponential $\\varphi$.\n  The answer depends on the dimension $d$. In the context of consensus for multi-agent control, Markdahl et al. (2018) show that for $d \\geq 3$ (spheres), if the interaction graph is connected and $\\varphi$ is increasing and convex, then the system synchronizes. We give a separate proof of this result.\n  What is the situation on circles ($d=2$)? First, we show that $\\varphi$ being increasing and convex is no longer sufficient (even for complete graphs). Then we identify a new condition under which we do have synchronization on the circle (namely, if the Taylor coefficients of $\\varphi'$ are decreasing). As a corollary, this provide synchronization for exponential $\\varphi$ with $\u03b2\\in (0, 1]$. The proofs are based on nonconvex landscape analysis.", "link": "http://arxiv.org/abs/2405.18273v2", "date": "2026-01-27", "relevancy": 2.0779, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4448}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4108}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synchronization%20on%20circles%20and%20spheres%20with%20nonlinear%20interactions&body=Title%3A%20Synchronization%20on%20circles%20and%20spheres%20with%20nonlinear%20interactions%0AAuthor%3A%20Christopher%20Criscitiello%20and%20Quentin%20Rebjock%20and%20Andrew%20D.%20McRae%20and%20Nicolas%20Boumal%0AAbstract%3A%20We%20consider%20the%20dynamics%20of%20%24n%24%20points%20on%20a%20sphere%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20%28%24d%20%5Cgeq%202%24%29%20which%20attract%20each%20other%20according%20to%20a%20function%20%24%5Cvarphi%24%20of%20their%20inner%20products.%20When%20%24%5Cvarphi%24%20is%20linear%20%28%24%5Cvarphi%28t%29%20%3D%20t%24%29%2C%20the%20points%20converge%20to%20a%20common%20value%20%28i.e.%2C%20synchronize%29%20in%20various%20connectivity%20scenarios%3A%20this%20is%20part%20of%20classical%20work%20on%20Kuramoto%20oscillator%20networks.%20When%20%24%5Cvarphi%24%20is%20exponential%20%28%24%5Cvarphi%28t%29%20%3D%20e%5E%7B%CE%B2t%7D%24%29%2C%20these%20dynamics%20correspond%20to%20a%20limit%20of%20how%20idealized%20transformers%20process%20data%2C%20as%20described%20by%20Geshkovski%20et%20al.%20%282025%29.%20Accordingly%2C%20they%20ask%20whether%20synchronization%20occurs%20for%20exponential%20%24%5Cvarphi%24.%0A%20%20The%20answer%20depends%20on%20the%20dimension%20%24d%24.%20In%20the%20context%20of%20consensus%20for%20multi-agent%20control%2C%20Markdahl%20et%20al.%20%282018%29%20show%20that%20for%20%24d%20%5Cgeq%203%24%20%28spheres%29%2C%20if%20the%20interaction%20graph%20is%20connected%20and%20%24%5Cvarphi%24%20is%20increasing%20and%20convex%2C%20then%20the%20system%20synchronizes.%20We%20give%20a%20separate%20proof%20of%20this%20result.%0A%20%20What%20is%20the%20situation%20on%20circles%20%28%24d%3D2%24%29%3F%20First%2C%20we%20show%20that%20%24%5Cvarphi%24%20being%20increasing%20and%20convex%20is%20no%20longer%20sufficient%20%28even%20for%20complete%20graphs%29.%20Then%20we%20identify%20a%20new%20condition%20under%20which%20we%20do%20have%20synchronization%20on%20the%20circle%20%28namely%2C%20if%20the%20Taylor%20coefficients%20of%20%24%5Cvarphi%27%24%20are%20decreasing%29.%20As%20a%20corollary%2C%20this%20provide%20synchronization%20for%20exponential%20%24%5Cvarphi%24%20with%20%24%CE%B2%5Cin%20%280%2C%201%5D%24.%20The%20proofs%20are%20based%20on%20nonconvex%20landscape%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2405.18273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynchronization%2520on%2520circles%2520and%2520spheres%2520with%2520nonlinear%2520interactions%26entry.906535625%3DChristopher%2520Criscitiello%2520and%2520Quentin%2520Rebjock%2520and%2520Andrew%2520D.%2520McRae%2520and%2520Nicolas%2520Boumal%26entry.1292438233%3DWe%2520consider%2520the%2520dynamics%2520of%2520%2524n%2524%2520points%2520on%2520a%2520sphere%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%2520%2528%2524d%2520%255Cgeq%25202%2524%2529%2520which%2520attract%2520each%2520other%2520according%2520to%2520a%2520function%2520%2524%255Cvarphi%2524%2520of%2520their%2520inner%2520products.%2520When%2520%2524%255Cvarphi%2524%2520is%2520linear%2520%2528%2524%255Cvarphi%2528t%2529%2520%253D%2520t%2524%2529%252C%2520the%2520points%2520converge%2520to%2520a%2520common%2520value%2520%2528i.e.%252C%2520synchronize%2529%2520in%2520various%2520connectivity%2520scenarios%253A%2520this%2520is%2520part%2520of%2520classical%2520work%2520on%2520Kuramoto%2520oscillator%2520networks.%2520When%2520%2524%255Cvarphi%2524%2520is%2520exponential%2520%2528%2524%255Cvarphi%2528t%2529%2520%253D%2520e%255E%257B%25CE%25B2t%257D%2524%2529%252C%2520these%2520dynamics%2520correspond%2520to%2520a%2520limit%2520of%2520how%2520idealized%2520transformers%2520process%2520data%252C%2520as%2520described%2520by%2520Geshkovski%2520et%2520al.%2520%25282025%2529.%2520Accordingly%252C%2520they%2520ask%2520whether%2520synchronization%2520occurs%2520for%2520exponential%2520%2524%255Cvarphi%2524.%250A%2520%2520The%2520answer%2520depends%2520on%2520the%2520dimension%2520%2524d%2524.%2520In%2520the%2520context%2520of%2520consensus%2520for%2520multi-agent%2520control%252C%2520Markdahl%2520et%2520al.%2520%25282018%2529%2520show%2520that%2520for%2520%2524d%2520%255Cgeq%25203%2524%2520%2528spheres%2529%252C%2520if%2520the%2520interaction%2520graph%2520is%2520connected%2520and%2520%2524%255Cvarphi%2524%2520is%2520increasing%2520and%2520convex%252C%2520then%2520the%2520system%2520synchronizes.%2520We%2520give%2520a%2520separate%2520proof%2520of%2520this%2520result.%250A%2520%2520What%2520is%2520the%2520situation%2520on%2520circles%2520%2528%2524d%253D2%2524%2529%253F%2520First%252C%2520we%2520show%2520that%2520%2524%255Cvarphi%2524%2520being%2520increasing%2520and%2520convex%2520is%2520no%2520longer%2520sufficient%2520%2528even%2520for%2520complete%2520graphs%2529.%2520Then%2520we%2520identify%2520a%2520new%2520condition%2520under%2520which%2520we%2520do%2520have%2520synchronization%2520on%2520the%2520circle%2520%2528namely%252C%2520if%2520the%2520Taylor%2520coefficients%2520of%2520%2524%255Cvarphi%2527%2524%2520are%2520decreasing%2529.%2520As%2520a%2520corollary%252C%2520this%2520provide%2520synchronization%2520for%2520exponential%2520%2524%255Cvarphi%2524%2520with%2520%2524%25CE%25B2%255Cin%2520%25280%252C%25201%255D%2524.%2520The%2520proofs%2520are%2520based%2520on%2520nonconvex%2520landscape%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synchronization%20on%20circles%20and%20spheres%20with%20nonlinear%20interactions&entry.906535625=Christopher%20Criscitiello%20and%20Quentin%20Rebjock%20and%20Andrew%20D.%20McRae%20and%20Nicolas%20Boumal&entry.1292438233=We%20consider%20the%20dynamics%20of%20%24n%24%20points%20on%20a%20sphere%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20%28%24d%20%5Cgeq%202%24%29%20which%20attract%20each%20other%20according%20to%20a%20function%20%24%5Cvarphi%24%20of%20their%20inner%20products.%20When%20%24%5Cvarphi%24%20is%20linear%20%28%24%5Cvarphi%28t%29%20%3D%20t%24%29%2C%20the%20points%20converge%20to%20a%20common%20value%20%28i.e.%2C%20synchronize%29%20in%20various%20connectivity%20scenarios%3A%20this%20is%20part%20of%20classical%20work%20on%20Kuramoto%20oscillator%20networks.%20When%20%24%5Cvarphi%24%20is%20exponential%20%28%24%5Cvarphi%28t%29%20%3D%20e%5E%7B%CE%B2t%7D%24%29%2C%20these%20dynamics%20correspond%20to%20a%20limit%20of%20how%20idealized%20transformers%20process%20data%2C%20as%20described%20by%20Geshkovski%20et%20al.%20%282025%29.%20Accordingly%2C%20they%20ask%20whether%20synchronization%20occurs%20for%20exponential%20%24%5Cvarphi%24.%0A%20%20The%20answer%20depends%20on%20the%20dimension%20%24d%24.%20In%20the%20context%20of%20consensus%20for%20multi-agent%20control%2C%20Markdahl%20et%20al.%20%282018%29%20show%20that%20for%20%24d%20%5Cgeq%203%24%20%28spheres%29%2C%20if%20the%20interaction%20graph%20is%20connected%20and%20%24%5Cvarphi%24%20is%20increasing%20and%20convex%2C%20then%20the%20system%20synchronizes.%20We%20give%20a%20separate%20proof%20of%20this%20result.%0A%20%20What%20is%20the%20situation%20on%20circles%20%28%24d%3D2%24%29%3F%20First%2C%20we%20show%20that%20%24%5Cvarphi%24%20being%20increasing%20and%20convex%20is%20no%20longer%20sufficient%20%28even%20for%20complete%20graphs%29.%20Then%20we%20identify%20a%20new%20condition%20under%20which%20we%20do%20have%20synchronization%20on%20the%20circle%20%28namely%2C%20if%20the%20Taylor%20coefficients%20of%20%24%5Cvarphi%27%24%20are%20decreasing%29.%20As%20a%20corollary%2C%20this%20provide%20synchronization%20for%20exponential%20%24%5Cvarphi%24%20with%20%24%CE%B2%5Cin%20%280%2C%201%5D%24.%20The%20proofs%20are%20based%20on%20nonconvex%20landscape%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2405.18273v2&entry.124074799=Read"},
{"title": "Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach", "author": "Temirbolat Maratuly and Pakizar Shamoi and Timur Samigulin", "abstract": "Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.", "link": "http://arxiv.org/abs/2601.19527v1", "date": "2026-01-27", "relevancy": 1.638, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4307}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4015}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fuzzy%20expert%20system%20for%20the%20process%20of%20collecting%20and%20purifying%20acidic%20water%3A%20a%20digital%20twin%20approach&body=Title%3A%20Fuzzy%20expert%20system%20for%20the%20process%20of%20collecting%20and%20purifying%20acidic%20water%3A%20a%20digital%20twin%20approach%0AAuthor%3A%20Temirbolat%20Maratuly%20and%20Pakizar%20Shamoi%20and%20Timur%20Samigulin%0AAbstract%3A%20Purifying%20sour%20water%20is%20essential%20for%20reducing%20emissions%2C%20minimizing%20corrosion%20risks%2C%20enabling%20the%20reuse%20of%20treated%20water%20in%20industrial%20or%20domestic%20applications%2C%20and%20ultimately%20lowering%20operational%20costs.%20Moreover%2C%20automating%20the%20purification%20process%20helps%20reduce%20the%20risk%20of%20worker%20harm%20by%20limiting%20human%20involvement.%20Crude%20oil%20contains%20acidic%20components%20such%20as%20hydrogen%20sulfide%2C%20carbon%20dioxide%2C%20and%20other%20chemical%20compounds.%20During%20processing%2C%20these%20substances%20are%20partially%20released%20into%20sour%20water.%20If%20not%20properly%20treated%2C%20sour%20water%20poses%20serious%20environmental%20threats%20and%20accelerates%20the%20corrosion%20of%20pipelines%20and%20equipment.%20This%20paper%20presents%20a%20fuzzy%20expert%20system%2C%20combined%20with%20a%20custom-generated%20digital%20twin%2C%20developed%20from%20a%20documented%20industrial%20process%20to%20maintain%20key%20parameters%20at%20desired%20levels%20by%20mimicking%20human%20reasoning.%20The%20control%20strategy%20is%20designed%20to%20be%20simple%20and%20intuitive%2C%20allowing%20junior%20or%20non-expert%20personnel%20to%20interact%20with%20the%20system%20effectively.%20The%20digital%20twin%20was%20developed%20using%20Honeywell%20UniSim%20Design%20R492%20to%20simulate%20real%20industrial%20behavior%20accurately.%20Valve%20dynamics%20were%20modeled%20through%20system%20identification%20in%20MATLAB%2C%20and%20real-time%20data%20exchange%20between%20the%20simulator%20and%20controller%20was%20established%20using%20OPC%20DA.%20The%20fuzzy%20controller%20applies%20split-range%20control%20to%20two%20valves%20and%20was%20tested%20under%2021%20different%20initial%20pressure%20conditions%20using%20five%20distinct%20defuzzification%20strategies%2C%20resulting%20in%20a%20total%20of%20105%20unique%20test%20scenarios.%20System%20performance%20was%20evaluated%20using%20both%20error-based%20metrics%20%28MSE%2C%20RMSE%2C%20MAE%2C%20IAE%2C%20ISE%2C%20ITAE%29%20and%20dynamic%20response%20metrics%2C%20including%20overshoot%2C%20undershoot%2C%20rise%20time%2C%20fall%20time%2C%20settling%20time%2C%20and%20steady-state%20error.%20A%20web-based%20simulation%20interface%20was%20developed%20in%20Python%20using%20the%20Streamlit%20framework.%20Although%20demonstrated%20here%20for%20sour%20water%20treatment%2C%20the%20proposed%20fuzzy%20expert%20system%20is%20general-purpose.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuzzy%2520expert%2520system%2520for%2520the%2520process%2520of%2520collecting%2520and%2520purifying%2520acidic%2520water%253A%2520a%2520digital%2520twin%2520approach%26entry.906535625%3DTemirbolat%2520Maratuly%2520and%2520Pakizar%2520Shamoi%2520and%2520Timur%2520Samigulin%26entry.1292438233%3DPurifying%2520sour%2520water%2520is%2520essential%2520for%2520reducing%2520emissions%252C%2520minimizing%2520corrosion%2520risks%252C%2520enabling%2520the%2520reuse%2520of%2520treated%2520water%2520in%2520industrial%2520or%2520domestic%2520applications%252C%2520and%2520ultimately%2520lowering%2520operational%2520costs.%2520Moreover%252C%2520automating%2520the%2520purification%2520process%2520helps%2520reduce%2520the%2520risk%2520of%2520worker%2520harm%2520by%2520limiting%2520human%2520involvement.%2520Crude%2520oil%2520contains%2520acidic%2520components%2520such%2520as%2520hydrogen%2520sulfide%252C%2520carbon%2520dioxide%252C%2520and%2520other%2520chemical%2520compounds.%2520During%2520processing%252C%2520these%2520substances%2520are%2520partially%2520released%2520into%2520sour%2520water.%2520If%2520not%2520properly%2520treated%252C%2520sour%2520water%2520poses%2520serious%2520environmental%2520threats%2520and%2520accelerates%2520the%2520corrosion%2520of%2520pipelines%2520and%2520equipment.%2520This%2520paper%2520presents%2520a%2520fuzzy%2520expert%2520system%252C%2520combined%2520with%2520a%2520custom-generated%2520digital%2520twin%252C%2520developed%2520from%2520a%2520documented%2520industrial%2520process%2520to%2520maintain%2520key%2520parameters%2520at%2520desired%2520levels%2520by%2520mimicking%2520human%2520reasoning.%2520The%2520control%2520strategy%2520is%2520designed%2520to%2520be%2520simple%2520and%2520intuitive%252C%2520allowing%2520junior%2520or%2520non-expert%2520personnel%2520to%2520interact%2520with%2520the%2520system%2520effectively.%2520The%2520digital%2520twin%2520was%2520developed%2520using%2520Honeywell%2520UniSim%2520Design%2520R492%2520to%2520simulate%2520real%2520industrial%2520behavior%2520accurately.%2520Valve%2520dynamics%2520were%2520modeled%2520through%2520system%2520identification%2520in%2520MATLAB%252C%2520and%2520real-time%2520data%2520exchange%2520between%2520the%2520simulator%2520and%2520controller%2520was%2520established%2520using%2520OPC%2520DA.%2520The%2520fuzzy%2520controller%2520applies%2520split-range%2520control%2520to%2520two%2520valves%2520and%2520was%2520tested%2520under%252021%2520different%2520initial%2520pressure%2520conditions%2520using%2520five%2520distinct%2520defuzzification%2520strategies%252C%2520resulting%2520in%2520a%2520total%2520of%2520105%2520unique%2520test%2520scenarios.%2520System%2520performance%2520was%2520evaluated%2520using%2520both%2520error-based%2520metrics%2520%2528MSE%252C%2520RMSE%252C%2520MAE%252C%2520IAE%252C%2520ISE%252C%2520ITAE%2529%2520and%2520dynamic%2520response%2520metrics%252C%2520including%2520overshoot%252C%2520undershoot%252C%2520rise%2520time%252C%2520fall%2520time%252C%2520settling%2520time%252C%2520and%2520steady-state%2520error.%2520A%2520web-based%2520simulation%2520interface%2520was%2520developed%2520in%2520Python%2520using%2520the%2520Streamlit%2520framework.%2520Although%2520demonstrated%2520here%2520for%2520sour%2520water%2520treatment%252C%2520the%2520proposed%2520fuzzy%2520expert%2520system%2520is%2520general-purpose.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fuzzy%20expert%20system%20for%20the%20process%20of%20collecting%20and%20purifying%20acidic%20water%3A%20a%20digital%20twin%20approach&entry.906535625=Temirbolat%20Maratuly%20and%20Pakizar%20Shamoi%20and%20Timur%20Samigulin&entry.1292438233=Purifying%20sour%20water%20is%20essential%20for%20reducing%20emissions%2C%20minimizing%20corrosion%20risks%2C%20enabling%20the%20reuse%20of%20treated%20water%20in%20industrial%20or%20domestic%20applications%2C%20and%20ultimately%20lowering%20operational%20costs.%20Moreover%2C%20automating%20the%20purification%20process%20helps%20reduce%20the%20risk%20of%20worker%20harm%20by%20limiting%20human%20involvement.%20Crude%20oil%20contains%20acidic%20components%20such%20as%20hydrogen%20sulfide%2C%20carbon%20dioxide%2C%20and%20other%20chemical%20compounds.%20During%20processing%2C%20these%20substances%20are%20partially%20released%20into%20sour%20water.%20If%20not%20properly%20treated%2C%20sour%20water%20poses%20serious%20environmental%20threats%20and%20accelerates%20the%20corrosion%20of%20pipelines%20and%20equipment.%20This%20paper%20presents%20a%20fuzzy%20expert%20system%2C%20combined%20with%20a%20custom-generated%20digital%20twin%2C%20developed%20from%20a%20documented%20industrial%20process%20to%20maintain%20key%20parameters%20at%20desired%20levels%20by%20mimicking%20human%20reasoning.%20The%20control%20strategy%20is%20designed%20to%20be%20simple%20and%20intuitive%2C%20allowing%20junior%20or%20non-expert%20personnel%20to%20interact%20with%20the%20system%20effectively.%20The%20digital%20twin%20was%20developed%20using%20Honeywell%20UniSim%20Design%20R492%20to%20simulate%20real%20industrial%20behavior%20accurately.%20Valve%20dynamics%20were%20modeled%20through%20system%20identification%20in%20MATLAB%2C%20and%20real-time%20data%20exchange%20between%20the%20simulator%20and%20controller%20was%20established%20using%20OPC%20DA.%20The%20fuzzy%20controller%20applies%20split-range%20control%20to%20two%20valves%20and%20was%20tested%20under%2021%20different%20initial%20pressure%20conditions%20using%20five%20distinct%20defuzzification%20strategies%2C%20resulting%20in%20a%20total%20of%20105%20unique%20test%20scenarios.%20System%20performance%20was%20evaluated%20using%20both%20error-based%20metrics%20%28MSE%2C%20RMSE%2C%20MAE%2C%20IAE%2C%20ISE%2C%20ITAE%29%20and%20dynamic%20response%20metrics%2C%20including%20overshoot%2C%20undershoot%2C%20rise%20time%2C%20fall%20time%2C%20settling%20time%2C%20and%20steady-state%20error.%20A%20web-based%20simulation%20interface%20was%20developed%20in%20Python%20using%20the%20Streamlit%20framework.%20Although%20demonstrated%20here%20for%20sour%20water%20treatment%2C%20the%20proposed%20fuzzy%20expert%20system%20is%20general-purpose.&entry.1838667208=http%3A//arxiv.org/abs/2601.19527v1&entry.124074799=Read"},
{"title": "Universal Multi-Domain Translation via Diffusion Routers", "author": "Duc Kieu and Kien Do and Tuan Hoang and Thao Minh Le and Tung Kieu and Dang Nguyen and Thin Nguyen", "abstract": "Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.", "link": "http://arxiv.org/abs/2510.03252v3", "date": "2026-01-27", "relevancy": 1.6363, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5512}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5474}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Multi-Domain%20Translation%20via%20Diffusion%20Routers&body=Title%3A%20Universal%20Multi-Domain%20Translation%20via%20Diffusion%20Routers%0AAuthor%3A%20Duc%20Kieu%20and%20Kien%20Do%20and%20Tuan%20Hoang%20and%20Thao%20Minh%20Le%20and%20Tung%20Kieu%20and%20Dang%20Nguyen%20and%20Thin%20Nguyen%0AAbstract%3A%20Multi-domain%20translation%20%28MDT%29%20aims%20to%20learn%20translations%20between%20multiple%20domains%2C%20yet%20existing%20approaches%20either%20require%20fully%20aligned%20tuples%20or%20can%20only%20handle%20domain%20pairs%20seen%20in%20training%2C%20limiting%20their%20practicality%20and%20excluding%20many%20cross-domain%20mappings.%20We%20introduce%20universal%20MDT%20%28UMDT%29%2C%20a%20generalization%20of%20MDT%20that%20seeks%20to%20translate%20between%20any%20pair%20of%20%24K%24%20domains%20using%20only%20%24K-1%24%20paired%20datasets%20with%20a%20central%20domain.%20To%20tackle%20this%20problem%2C%20we%20propose%20Diffusion%20Router%20%28DR%29%2C%20a%20unified%20diffusion-based%20framework%20that%20models%20all%20central%24%5Cleftrightarrow%24non-central%20translations%20with%20a%20single%20noise%20predictor%20conditioned%20on%20the%20source%20and%20target%20domain%20labels.%20DR%20enables%20indirect%20non-central%20translations%20by%20routing%20through%20the%20central%20domain.%20We%20further%20introduce%20a%20novel%20scalable%20learning%20strategy%20with%20a%20variational-bound%20objective%20and%20an%20efficient%20Tweedie%20refinement%20procedure%20to%20support%20direct%20non-central%20mappings.%20Through%20evaluation%20on%20three%20large-scale%20UMDT%20benchmarks%2C%20DR%20achieves%20state-of-the-art%20results%20for%20both%20indirect%20and%20direct%20translations%2C%20while%20lowering%20sampling%20cost%20and%20unlocking%20novel%20tasks%20such%20as%20sketch%24%5Cleftrightarrow%24segmentation.%20These%20results%20establish%20DR%20as%20a%20scalable%20and%20versatile%20framework%20for%20universal%20translation%20across%20multiple%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2510.03252v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Multi-Domain%2520Translation%2520via%2520Diffusion%2520Routers%26entry.906535625%3DDuc%2520Kieu%2520and%2520Kien%2520Do%2520and%2520Tuan%2520Hoang%2520and%2520Thao%2520Minh%2520Le%2520and%2520Tung%2520Kieu%2520and%2520Dang%2520Nguyen%2520and%2520Thin%2520Nguyen%26entry.1292438233%3DMulti-domain%2520translation%2520%2528MDT%2529%2520aims%2520to%2520learn%2520translations%2520between%2520multiple%2520domains%252C%2520yet%2520existing%2520approaches%2520either%2520require%2520fully%2520aligned%2520tuples%2520or%2520can%2520only%2520handle%2520domain%2520pairs%2520seen%2520in%2520training%252C%2520limiting%2520their%2520practicality%2520and%2520excluding%2520many%2520cross-domain%2520mappings.%2520We%2520introduce%2520universal%2520MDT%2520%2528UMDT%2529%252C%2520a%2520generalization%2520of%2520MDT%2520that%2520seeks%2520to%2520translate%2520between%2520any%2520pair%2520of%2520%2524K%2524%2520domains%2520using%2520only%2520%2524K-1%2524%2520paired%2520datasets%2520with%2520a%2520central%2520domain.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520Diffusion%2520Router%2520%2528DR%2529%252C%2520a%2520unified%2520diffusion-based%2520framework%2520that%2520models%2520all%2520central%2524%255Cleftrightarrow%2524non-central%2520translations%2520with%2520a%2520single%2520noise%2520predictor%2520conditioned%2520on%2520the%2520source%2520and%2520target%2520domain%2520labels.%2520DR%2520enables%2520indirect%2520non-central%2520translations%2520by%2520routing%2520through%2520the%2520central%2520domain.%2520We%2520further%2520introduce%2520a%2520novel%2520scalable%2520learning%2520strategy%2520with%2520a%2520variational-bound%2520objective%2520and%2520an%2520efficient%2520Tweedie%2520refinement%2520procedure%2520to%2520support%2520direct%2520non-central%2520mappings.%2520Through%2520evaluation%2520on%2520three%2520large-scale%2520UMDT%2520benchmarks%252C%2520DR%2520achieves%2520state-of-the-art%2520results%2520for%2520both%2520indirect%2520and%2520direct%2520translations%252C%2520while%2520lowering%2520sampling%2520cost%2520and%2520unlocking%2520novel%2520tasks%2520such%2520as%2520sketch%2524%255Cleftrightarrow%2524segmentation.%2520These%2520results%2520establish%2520DR%2520as%2520a%2520scalable%2520and%2520versatile%2520framework%2520for%2520universal%2520translation%2520across%2520multiple%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03252v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Multi-Domain%20Translation%20via%20Diffusion%20Routers&entry.906535625=Duc%20Kieu%20and%20Kien%20Do%20and%20Tuan%20Hoang%20and%20Thao%20Minh%20Le%20and%20Tung%20Kieu%20and%20Dang%20Nguyen%20and%20Thin%20Nguyen&entry.1292438233=Multi-domain%20translation%20%28MDT%29%20aims%20to%20learn%20translations%20between%20multiple%20domains%2C%20yet%20existing%20approaches%20either%20require%20fully%20aligned%20tuples%20or%20can%20only%20handle%20domain%20pairs%20seen%20in%20training%2C%20limiting%20their%20practicality%20and%20excluding%20many%20cross-domain%20mappings.%20We%20introduce%20universal%20MDT%20%28UMDT%29%2C%20a%20generalization%20of%20MDT%20that%20seeks%20to%20translate%20between%20any%20pair%20of%20%24K%24%20domains%20using%20only%20%24K-1%24%20paired%20datasets%20with%20a%20central%20domain.%20To%20tackle%20this%20problem%2C%20we%20propose%20Diffusion%20Router%20%28DR%29%2C%20a%20unified%20diffusion-based%20framework%20that%20models%20all%20central%24%5Cleftrightarrow%24non-central%20translations%20with%20a%20single%20noise%20predictor%20conditioned%20on%20the%20source%20and%20target%20domain%20labels.%20DR%20enables%20indirect%20non-central%20translations%20by%20routing%20through%20the%20central%20domain.%20We%20further%20introduce%20a%20novel%20scalable%20learning%20strategy%20with%20a%20variational-bound%20objective%20and%20an%20efficient%20Tweedie%20refinement%20procedure%20to%20support%20direct%20non-central%20mappings.%20Through%20evaluation%20on%20three%20large-scale%20UMDT%20benchmarks%2C%20DR%20achieves%20state-of-the-art%20results%20for%20both%20indirect%20and%20direct%20translations%2C%20while%20lowering%20sampling%20cost%20and%20unlocking%20novel%20tasks%20such%20as%20sketch%24%5Cleftrightarrow%24segmentation.%20These%20results%20establish%20DR%20as%20a%20scalable%20and%20versatile%20framework%20for%20universal%20translation%20across%20multiple%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2510.03252v3&entry.124074799=Read"},
{"title": "Who Gets Cited Most? Benchmarking Long-Context Reasoning on Scientific Articles", "author": "Miao Li and Alexander Gurung and Irina Saparina and Mirella Lapata", "abstract": "We introduce SciTrek, a novel question-answering benchmark designed to evaluate long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by creating benchmark questions that require information aggregation and synthesis across multiple full-text scientific articles. The questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (i.e., titles, authors, and references). These SQL queries provide explicit, verifiable reasoning processes that enable fine-grained error analysis on model answers, and the data construction scales to contexts of up to 1M tokens with minimal supervision. Experiments on open-weight and proprietary LLMs show that SciTrek poses significant challenges as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings of frontier LLMs' ability to effectively perform numerical operations and accurately locate information in long contexts.", "link": "http://arxiv.org/abs/2509.21028v2", "date": "2026-01-27", "relevancy": 1.9465, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20Gets%20Cited%20Most%3F%20Benchmarking%20Long-Context%20Reasoning%20on%20Scientific%20Articles&body=Title%3A%20Who%20Gets%20Cited%20Most%3F%20Benchmarking%20Long-Context%20Reasoning%20on%20Scientific%20Articles%0AAuthor%3A%20Miao%20Li%20and%20Alexander%20Gurung%20and%20Irina%20Saparina%20and%20Mirella%20Lapata%0AAbstract%3A%20We%20introduce%20SciTrek%2C%20a%20novel%20question-answering%20benchmark%20designed%20to%20evaluate%20long-context%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20using%20scientific%20articles.%20Current%20long-context%20benchmarks%20often%20focus%20on%20simple%20information%20retrieval%20tasks%2C%20or%20employ%20artificial%20contexts.%20SciTrek%20addresses%20these%20limitations%20by%20creating%20benchmark%20questions%20that%20require%20information%20aggregation%20and%20synthesis%20across%20multiple%20full-text%20scientific%20articles.%20The%20questions%20and%20their%20ground-truth%20answers%20are%20automatically%20generated%20by%20formulating%20them%20as%20SQL%20queries%20over%20a%20database%20constructed%20from%20article%20metadata%20%28i.e.%2C%20titles%2C%20authors%2C%20and%20references%29.%20These%20SQL%20queries%20provide%20explicit%2C%20verifiable%20reasoning%20processes%20that%20enable%20fine-grained%20error%20analysis%20on%20model%20answers%2C%20and%20the%20data%20construction%20scales%20to%20contexts%20of%20up%20to%201M%20tokens%20with%20minimal%20supervision.%20Experiments%20on%20open-weight%20and%20proprietary%20LLMs%20show%20that%20SciTrek%20poses%20significant%20challenges%20as%20the%20context%20length%20increases%2C%20with%20supervised%20fine-tuning%20and%20reinforcement%20learning%20offering%20only%20limited%20gains.%20Our%20analysis%20reveals%20systematic%20shortcomings%20of%20frontier%20LLMs%27%20ability%20to%20effectively%20perform%20numerical%20operations%20and%20accurately%20locate%20information%20in%20long%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520Gets%2520Cited%2520Most%253F%2520Benchmarking%2520Long-Context%2520Reasoning%2520on%2520Scientific%2520Articles%26entry.906535625%3DMiao%2520Li%2520and%2520Alexander%2520Gurung%2520and%2520Irina%2520Saparina%2520and%2520Mirella%2520Lapata%26entry.1292438233%3DWe%2520introduce%2520SciTrek%252C%2520a%2520novel%2520question-answering%2520benchmark%2520designed%2520to%2520evaluate%2520long-context%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520using%2520scientific%2520articles.%2520Current%2520long-context%2520benchmarks%2520often%2520focus%2520on%2520simple%2520information%2520retrieval%2520tasks%252C%2520or%2520employ%2520artificial%2520contexts.%2520SciTrek%2520addresses%2520these%2520limitations%2520by%2520creating%2520benchmark%2520questions%2520that%2520require%2520information%2520aggregation%2520and%2520synthesis%2520across%2520multiple%2520full-text%2520scientific%2520articles.%2520The%2520questions%2520and%2520their%2520ground-truth%2520answers%2520are%2520automatically%2520generated%2520by%2520formulating%2520them%2520as%2520SQL%2520queries%2520over%2520a%2520database%2520constructed%2520from%2520article%2520metadata%2520%2528i.e.%252C%2520titles%252C%2520authors%252C%2520and%2520references%2529.%2520These%2520SQL%2520queries%2520provide%2520explicit%252C%2520verifiable%2520reasoning%2520processes%2520that%2520enable%2520fine-grained%2520error%2520analysis%2520on%2520model%2520answers%252C%2520and%2520the%2520data%2520construction%2520scales%2520to%2520contexts%2520of%2520up%2520to%25201M%2520tokens%2520with%2520minimal%2520supervision.%2520Experiments%2520on%2520open-weight%2520and%2520proprietary%2520LLMs%2520show%2520that%2520SciTrek%2520poses%2520significant%2520challenges%2520as%2520the%2520context%2520length%2520increases%252C%2520with%2520supervised%2520fine-tuning%2520and%2520reinforcement%2520learning%2520offering%2520only%2520limited%2520gains.%2520Our%2520analysis%2520reveals%2520systematic%2520shortcomings%2520of%2520frontier%2520LLMs%2527%2520ability%2520to%2520effectively%2520perform%2520numerical%2520operations%2520and%2520accurately%2520locate%2520information%2520in%2520long%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Gets%20Cited%20Most%3F%20Benchmarking%20Long-Context%20Reasoning%20on%20Scientific%20Articles&entry.906535625=Miao%20Li%20and%20Alexander%20Gurung%20and%20Irina%20Saparina%20and%20Mirella%20Lapata&entry.1292438233=We%20introduce%20SciTrek%2C%20a%20novel%20question-answering%20benchmark%20designed%20to%20evaluate%20long-context%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20using%20scientific%20articles.%20Current%20long-context%20benchmarks%20often%20focus%20on%20simple%20information%20retrieval%20tasks%2C%20or%20employ%20artificial%20contexts.%20SciTrek%20addresses%20these%20limitations%20by%20creating%20benchmark%20questions%20that%20require%20information%20aggregation%20and%20synthesis%20across%20multiple%20full-text%20scientific%20articles.%20The%20questions%20and%20their%20ground-truth%20answers%20are%20automatically%20generated%20by%20formulating%20them%20as%20SQL%20queries%20over%20a%20database%20constructed%20from%20article%20metadata%20%28i.e.%2C%20titles%2C%20authors%2C%20and%20references%29.%20These%20SQL%20queries%20provide%20explicit%2C%20verifiable%20reasoning%20processes%20that%20enable%20fine-grained%20error%20analysis%20on%20model%20answers%2C%20and%20the%20data%20construction%20scales%20to%20contexts%20of%20up%20to%201M%20tokens%20with%20minimal%20supervision.%20Experiments%20on%20open-weight%20and%20proprietary%20LLMs%20show%20that%20SciTrek%20poses%20significant%20challenges%20as%20the%20context%20length%20increases%2C%20with%20supervised%20fine-tuning%20and%20reinforcement%20learning%20offering%20only%20limited%20gains.%20Our%20analysis%20reveals%20systematic%20shortcomings%20of%20frontier%20LLMs%27%20ability%20to%20effectively%20perform%20numerical%20operations%20and%20accurately%20locate%20information%20in%20long%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2509.21028v2&entry.124074799=Read"},
{"title": "GAVEL: Towards rule-based safety through activation monitoring", "author": "Shir Rozenfeld and Rahul Pankajakshan and Itay Zloczower and Eyal Lenga and Gilad Gressel and Yisroel Mirsky", "abstract": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.", "link": "http://arxiv.org/abs/2601.19768v1", "date": "2026-01-27", "relevancy": 0.9452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.467}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAVEL%3A%20Towards%20rule-based%20safety%20through%20activation%20monitoring&body=Title%3A%20GAVEL%3A%20Towards%20rule-based%20safety%20through%20activation%20monitoring%0AAuthor%3A%20Shir%20Rozenfeld%20and%20Rahul%20Pankajakshan%20and%20Itay%20Zloczower%20and%20Eyal%20Lenga%20and%20Gilad%20Gressel%20and%20Yisroel%20Mirsky%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20paired%20with%20activation-based%20monitoring%20to%20detect%20and%20prevent%20harmful%20behaviors%20that%20may%20not%20be%20apparent%20at%20the%20surface-text%20level.%20However%2C%20existing%20activation%20safety%20approaches%2C%20trained%20on%20broad%20misuse%20datasets%2C%20struggle%20with%20poor%20precision%2C%20limited%20flexibility%2C%20and%20lack%20of%20interpretability.%20This%20paper%20introduces%20a%20new%20paradigm%3A%20rule-based%20activation%20safety%2C%20inspired%20by%20rule-sharing%20practices%20in%20cybersecurity.%20We%20propose%20modeling%20activations%20as%20cognitive%20elements%20%28CEs%29%2C%20fine-grained%2C%20interpretable%20factors%20such%20as%20%27%27making%20a%20threat%27%27%20and%20%27%27payment%20processing%27%27%2C%20that%20can%20be%20composed%20to%20capture%20nuanced%2C%20domain-specific%20behaviors%20with%20higher%20precision.%20Building%20on%20this%20representation%2C%20we%20present%20a%20practical%20framework%20that%20defines%20predicate%20rules%20over%20CEs%20and%20detects%20violations%20in%20real%20time.%20This%20enables%20practitioners%20to%20configure%20and%20update%20safeguards%20without%20retraining%20models%20or%20detectors%2C%20while%20supporting%20transparency%20and%20auditability.%20Our%20results%20show%20that%20compositional%20rule-based%20activation%20safety%20improves%20precision%2C%20supports%20domain%20customization%2C%20and%20lays%20the%20groundwork%20for%20scalable%2C%20interpretable%2C%20and%20auditable%20AI%20governance.%20We%20will%20release%20GAVEL%20as%20an%20open-source%20framework%20and%20provide%20an%20accompanying%20automated%20rule%20creation%20tool.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAVEL%253A%2520Towards%2520rule-based%2520safety%2520through%2520activation%2520monitoring%26entry.906535625%3DShir%2520Rozenfeld%2520and%2520Rahul%2520Pankajakshan%2520and%2520Itay%2520Zloczower%2520and%2520Eyal%2520Lenga%2520and%2520Gilad%2520Gressel%2520and%2520Yisroel%2520Mirsky%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520paired%2520with%2520activation-based%2520monitoring%2520to%2520detect%2520and%2520prevent%2520harmful%2520behaviors%2520that%2520may%2520not%2520be%2520apparent%2520at%2520the%2520surface-text%2520level.%2520However%252C%2520existing%2520activation%2520safety%2520approaches%252C%2520trained%2520on%2520broad%2520misuse%2520datasets%252C%2520struggle%2520with%2520poor%2520precision%252C%2520limited%2520flexibility%252C%2520and%2520lack%2520of%2520interpretability.%2520This%2520paper%2520introduces%2520a%2520new%2520paradigm%253A%2520rule-based%2520activation%2520safety%252C%2520inspired%2520by%2520rule-sharing%2520practices%2520in%2520cybersecurity.%2520We%2520propose%2520modeling%2520activations%2520as%2520cognitive%2520elements%2520%2528CEs%2529%252C%2520fine-grained%252C%2520interpretable%2520factors%2520such%2520as%2520%2527%2527making%2520a%2520threat%2527%2527%2520and%2520%2527%2527payment%2520processing%2527%2527%252C%2520that%2520can%2520be%2520composed%2520to%2520capture%2520nuanced%252C%2520domain-specific%2520behaviors%2520with%2520higher%2520precision.%2520Building%2520on%2520this%2520representation%252C%2520we%2520present%2520a%2520practical%2520framework%2520that%2520defines%2520predicate%2520rules%2520over%2520CEs%2520and%2520detects%2520violations%2520in%2520real%2520time.%2520This%2520enables%2520practitioners%2520to%2520configure%2520and%2520update%2520safeguards%2520without%2520retraining%2520models%2520or%2520detectors%252C%2520while%2520supporting%2520transparency%2520and%2520auditability.%2520Our%2520results%2520show%2520that%2520compositional%2520rule-based%2520activation%2520safety%2520improves%2520precision%252C%2520supports%2520domain%2520customization%252C%2520and%2520lays%2520the%2520groundwork%2520for%2520scalable%252C%2520interpretable%252C%2520and%2520auditable%2520AI%2520governance.%2520We%2520will%2520release%2520GAVEL%2520as%2520an%2520open-source%2520framework%2520and%2520provide%2520an%2520accompanying%2520automated%2520rule%2520creation%2520tool.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAVEL%3A%20Towards%20rule-based%20safety%20through%20activation%20monitoring&entry.906535625=Shir%20Rozenfeld%20and%20Rahul%20Pankajakshan%20and%20Itay%20Zloczower%20and%20Eyal%20Lenga%20and%20Gilad%20Gressel%20and%20Yisroel%20Mirsky&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20paired%20with%20activation-based%20monitoring%20to%20detect%20and%20prevent%20harmful%20behaviors%20that%20may%20not%20be%20apparent%20at%20the%20surface-text%20level.%20However%2C%20existing%20activation%20safety%20approaches%2C%20trained%20on%20broad%20misuse%20datasets%2C%20struggle%20with%20poor%20precision%2C%20limited%20flexibility%2C%20and%20lack%20of%20interpretability.%20This%20paper%20introduces%20a%20new%20paradigm%3A%20rule-based%20activation%20safety%2C%20inspired%20by%20rule-sharing%20practices%20in%20cybersecurity.%20We%20propose%20modeling%20activations%20as%20cognitive%20elements%20%28CEs%29%2C%20fine-grained%2C%20interpretable%20factors%20such%20as%20%27%27making%20a%20threat%27%27%20and%20%27%27payment%20processing%27%27%2C%20that%20can%20be%20composed%20to%20capture%20nuanced%2C%20domain-specific%20behaviors%20with%20higher%20precision.%20Building%20on%20this%20representation%2C%20we%20present%20a%20practical%20framework%20that%20defines%20predicate%20rules%20over%20CEs%20and%20detects%20violations%20in%20real%20time.%20This%20enables%20practitioners%20to%20configure%20and%20update%20safeguards%20without%20retraining%20models%20or%20detectors%2C%20while%20supporting%20transparency%20and%20auditability.%20Our%20results%20show%20that%20compositional%20rule-based%20activation%20safety%20improves%20precision%2C%20supports%20domain%20customization%2C%20and%20lays%20the%20groundwork%20for%20scalable%2C%20interpretable%2C%20and%20auditable%20AI%20governance.%20We%20will%20release%20GAVEL%20as%20an%20open-source%20framework%20and%20provide%20an%20accompanying%20automated%20rule%20creation%20tool.&entry.1838667208=http%3A//arxiv.org/abs/2601.19768v1&entry.124074799=Read"},
{"title": "ProToken: Token-Level Attribution for Federated Large Language Models", "author": "Waris Gill and Ahmad Humayun and Ali Anwar and Muhammad Ali Gulzar", "abstract": "Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.", "link": "http://arxiv.org/abs/2601.19672v1", "date": "2026-01-27", "relevancy": 1.9024, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4986}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4602}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProToken%3A%20Token-Level%20Attribution%20for%20Federated%20Large%20Language%20Models&body=Title%3A%20ProToken%3A%20Token-Level%20Attribution%20for%20Federated%20Large%20Language%20Models%0AAuthor%3A%20Waris%20Gill%20and%20Ahmad%20Humayun%20and%20Ali%20Anwar%20and%20Muhammad%20Ali%20Gulzar%0AAbstract%3A%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20distributed%20data%20sources%20while%20preserving%20privacy.%20However%2C%20when%20federated%20LLMs%20are%20deployed%20in%20critical%20applications%2C%20it%20remains%20unclear%20which%20client%28s%29%20contributed%20to%20specific%20generated%20responses%2C%20hindering%20debugging%2C%20malicious%20client%20identification%2C%20fair%20reward%20allocation%2C%20and%20trust%20verification.%20We%20present%20ProToken%2C%20a%20novel%20Provenance%20methodology%20for%20Token-level%20attribution%20in%20federated%20LLMs%20that%20addresses%20client%20attribution%20during%20autoregressive%20text%20generation%20while%20maintaining%20FL%20privacy%20constraints.%20ProToken%20leverages%20two%20key%20insights%20to%20enable%20provenance%20at%20each%20token%3A%20%281%29%20transformer%20architectures%20concentrate%20task-specific%20signals%20in%20later%20blocks%2C%20enabling%20strategic%20layer%20selection%20for%20computational%20tractability%2C%20and%20%282%29%20gradient-based%20relevance%20weighting%20filters%20out%20irrelevant%20neural%20activations%2C%20focusing%20attribution%20on%20neurons%20that%20directly%20influence%20token%20generation.%20We%20evaluate%20ProToken%20across%2016%20configurations%20spanning%20four%20LLM%20architectures%20%28Gemma%2C%20Llama%2C%20Qwen%2C%20SmolLM%29%20and%20four%20domains%20%28medical%2C%20financial%2C%20mathematical%2C%20coding%29.%20ProToken%20achieves%2098%25%20average%20attribution%20accuracy%20in%20correctly%20localizing%20responsible%20client%28s%29%2C%20and%20maintains%20high%20accuracy%20when%20the%20number%20of%20clients%20are%20scaled%2C%20validating%20its%20practical%20viability%20for%20real-world%20deployment%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProToken%253A%2520Token-Level%2520Attribution%2520for%2520Federated%2520Large%2520Language%2520Models%26entry.906535625%3DWaris%2520Gill%2520and%2520Ahmad%2520Humayun%2520and%2520Ali%2520Anwar%2520and%2520Muhammad%2520Ali%2520Gulzar%26entry.1292438233%3DFederated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520training%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520distributed%2520data%2520sources%2520while%2520preserving%2520privacy.%2520However%252C%2520when%2520federated%2520LLMs%2520are%2520deployed%2520in%2520critical%2520applications%252C%2520it%2520remains%2520unclear%2520which%2520client%2528s%2529%2520contributed%2520to%2520specific%2520generated%2520responses%252C%2520hindering%2520debugging%252C%2520malicious%2520client%2520identification%252C%2520fair%2520reward%2520allocation%252C%2520and%2520trust%2520verification.%2520We%2520present%2520ProToken%252C%2520a%2520novel%2520Provenance%2520methodology%2520for%2520Token-level%2520attribution%2520in%2520federated%2520LLMs%2520that%2520addresses%2520client%2520attribution%2520during%2520autoregressive%2520text%2520generation%2520while%2520maintaining%2520FL%2520privacy%2520constraints.%2520ProToken%2520leverages%2520two%2520key%2520insights%2520to%2520enable%2520provenance%2520at%2520each%2520token%253A%2520%25281%2529%2520transformer%2520architectures%2520concentrate%2520task-specific%2520signals%2520in%2520later%2520blocks%252C%2520enabling%2520strategic%2520layer%2520selection%2520for%2520computational%2520tractability%252C%2520and%2520%25282%2529%2520gradient-based%2520relevance%2520weighting%2520filters%2520out%2520irrelevant%2520neural%2520activations%252C%2520focusing%2520attribution%2520on%2520neurons%2520that%2520directly%2520influence%2520token%2520generation.%2520We%2520evaluate%2520ProToken%2520across%252016%2520configurations%2520spanning%2520four%2520LLM%2520architectures%2520%2528Gemma%252C%2520Llama%252C%2520Qwen%252C%2520SmolLM%2529%2520and%2520four%2520domains%2520%2528medical%252C%2520financial%252C%2520mathematical%252C%2520coding%2529.%2520ProToken%2520achieves%252098%2525%2520average%2520attribution%2520accuracy%2520in%2520correctly%2520localizing%2520responsible%2520client%2528s%2529%252C%2520and%2520maintains%2520high%2520accuracy%2520when%2520the%2520number%2520of%2520clients%2520are%2520scaled%252C%2520validating%2520its%2520practical%2520viability%2520for%2520real-world%2520deployment%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProToken%3A%20Token-Level%20Attribution%20for%20Federated%20Large%20Language%20Models&entry.906535625=Waris%20Gill%20and%20Ahmad%20Humayun%20and%20Ali%20Anwar%20and%20Muhammad%20Ali%20Gulzar&entry.1292438233=Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20distributed%20data%20sources%20while%20preserving%20privacy.%20However%2C%20when%20federated%20LLMs%20are%20deployed%20in%20critical%20applications%2C%20it%20remains%20unclear%20which%20client%28s%29%20contributed%20to%20specific%20generated%20responses%2C%20hindering%20debugging%2C%20malicious%20client%20identification%2C%20fair%20reward%20allocation%2C%20and%20trust%20verification.%20We%20present%20ProToken%2C%20a%20novel%20Provenance%20methodology%20for%20Token-level%20attribution%20in%20federated%20LLMs%20that%20addresses%20client%20attribution%20during%20autoregressive%20text%20generation%20while%20maintaining%20FL%20privacy%20constraints.%20ProToken%20leverages%20two%20key%20insights%20to%20enable%20provenance%20at%20each%20token%3A%20%281%29%20transformer%20architectures%20concentrate%20task-specific%20signals%20in%20later%20blocks%2C%20enabling%20strategic%20layer%20selection%20for%20computational%20tractability%2C%20and%20%282%29%20gradient-based%20relevance%20weighting%20filters%20out%20irrelevant%20neural%20activations%2C%20focusing%20attribution%20on%20neurons%20that%20directly%20influence%20token%20generation.%20We%20evaluate%20ProToken%20across%2016%20configurations%20spanning%20four%20LLM%20architectures%20%28Gemma%2C%20Llama%2C%20Qwen%2C%20SmolLM%29%20and%20four%20domains%20%28medical%2C%20financial%2C%20mathematical%2C%20coding%29.%20ProToken%20achieves%2098%25%20average%20attribution%20accuracy%20in%20correctly%20localizing%20responsible%20client%28s%29%2C%20and%20maintains%20high%20accuracy%20when%20the%20number%20of%20clients%20are%20scaled%2C%20validating%20its%20practical%20viability%20for%20real-world%20deployment%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.19672v1&entry.124074799=Read"},
{"title": "Ad Insertion in LLM-Generated Responses", "author": "Shengwei Xu and Zhaohua Chen and Xiaotie Deng and Zhiyi Huang and Grant Schoenebeck", "abstract": "Sustainable monetization of Large Language Models (LLMs) remains a critical open challenge. Traditional search advertising, which relies on static keywords, fails to capture the fleeting, context-dependent user intents--the specific information, goods, or services a user seeks--embedded in conversational flows. Beyond the standard goal of social welfare maximization, effective LLM advertising imposes additional requirements on contextual coherence (ensuring ads align semantically with transient user intents) and computational efficiency (avoiding user interaction latency), as well as adherence to ethical and regulatory standards, including preserving privacy and ensuring explicit ad disclosure. Although various recent solutions have explored bidding on token-level and query-level, both categories of approaches generally fail to holistically satisfy this multifaceted set of constraints.\n  We propose a practical framework that resolves these tensions through two decoupling strategies. First, we decouple ad insertion from response generation to ensure safety and explicit disclosure. Second, we decouple bidding from specific user queries by using ``genres'' (high-level semantic clusters) as a proxy. This allows advertisers to bid on stable categories rather than sensitive real-time response, reducing computational burden and privacy risks. We demonstrate that applying the VCG auction mechanism to this genre-based framework yields approximately dominant strategy incentive compatibility (DSIC) and individual rationality (IR), as well as approximately optimal social welfare, while maintaining high computational efficiency. Finally, we introduce an \"LLM-as-a-Judge\" metric to estimate contextual coherence. Our experiments show that this metric correlates strongly with human ratings (Spearman's $\u03c1\\approx 0.66$), outperforming 80% of individual human evaluators.", "link": "http://arxiv.org/abs/2601.19435v1", "date": "2026-01-27", "relevancy": 1.9198, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ad%20Insertion%20in%20LLM-Generated%20Responses&body=Title%3A%20Ad%20Insertion%20in%20LLM-Generated%20Responses%0AAuthor%3A%20Shengwei%20Xu%20and%20Zhaohua%20Chen%20and%20Xiaotie%20Deng%20and%20Zhiyi%20Huang%20and%20Grant%20Schoenebeck%0AAbstract%3A%20Sustainable%20monetization%20of%20Large%20Language%20Models%20%28LLMs%29%20remains%20a%20critical%20open%20challenge.%20Traditional%20search%20advertising%2C%20which%20relies%20on%20static%20keywords%2C%20fails%20to%20capture%20the%20fleeting%2C%20context-dependent%20user%20intents--the%20specific%20information%2C%20goods%2C%20or%20services%20a%20user%20seeks--embedded%20in%20conversational%20flows.%20Beyond%20the%20standard%20goal%20of%20social%20welfare%20maximization%2C%20effective%20LLM%20advertising%20imposes%20additional%20requirements%20on%20contextual%20coherence%20%28ensuring%20ads%20align%20semantically%20with%20transient%20user%20intents%29%20and%20computational%20efficiency%20%28avoiding%20user%20interaction%20latency%29%2C%20as%20well%20as%20adherence%20to%20ethical%20and%20regulatory%20standards%2C%20including%20preserving%20privacy%20and%20ensuring%20explicit%20ad%20disclosure.%20Although%20various%20recent%20solutions%20have%20explored%20bidding%20on%20token-level%20and%20query-level%2C%20both%20categories%20of%20approaches%20generally%20fail%20to%20holistically%20satisfy%20this%20multifaceted%20set%20of%20constraints.%0A%20%20We%20propose%20a%20practical%20framework%20that%20resolves%20these%20tensions%20through%20two%20decoupling%20strategies.%20First%2C%20we%20decouple%20ad%20insertion%20from%20response%20generation%20to%20ensure%20safety%20and%20explicit%20disclosure.%20Second%2C%20we%20decouple%20bidding%20from%20specific%20user%20queries%20by%20using%20%60%60genres%27%27%20%28high-level%20semantic%20clusters%29%20as%20a%20proxy.%20This%20allows%20advertisers%20to%20bid%20on%20stable%20categories%20rather%20than%20sensitive%20real-time%20response%2C%20reducing%20computational%20burden%20and%20privacy%20risks.%20We%20demonstrate%20that%20applying%20the%20VCG%20auction%20mechanism%20to%20this%20genre-based%20framework%20yields%20approximately%20dominant%20strategy%20incentive%20compatibility%20%28DSIC%29%20and%20individual%20rationality%20%28IR%29%2C%20as%20well%20as%20approximately%20optimal%20social%20welfare%2C%20while%20maintaining%20high%20computational%20efficiency.%20Finally%2C%20we%20introduce%20an%20%22LLM-as-a-Judge%22%20metric%20to%20estimate%20contextual%20coherence.%20Our%20experiments%20show%20that%20this%20metric%20correlates%20strongly%20with%20human%20ratings%20%28Spearman%27s%20%24%CF%81%5Capprox%200.66%24%29%2C%20outperforming%2080%25%20of%20individual%20human%20evaluators.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAd%2520Insertion%2520in%2520LLM-Generated%2520Responses%26entry.906535625%3DShengwei%2520Xu%2520and%2520Zhaohua%2520Chen%2520and%2520Xiaotie%2520Deng%2520and%2520Zhiyi%2520Huang%2520and%2520Grant%2520Schoenebeck%26entry.1292438233%3DSustainable%2520monetization%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520remains%2520a%2520critical%2520open%2520challenge.%2520Traditional%2520search%2520advertising%252C%2520which%2520relies%2520on%2520static%2520keywords%252C%2520fails%2520to%2520capture%2520the%2520fleeting%252C%2520context-dependent%2520user%2520intents--the%2520specific%2520information%252C%2520goods%252C%2520or%2520services%2520a%2520user%2520seeks--embedded%2520in%2520conversational%2520flows.%2520Beyond%2520the%2520standard%2520goal%2520of%2520social%2520welfare%2520maximization%252C%2520effective%2520LLM%2520advertising%2520imposes%2520additional%2520requirements%2520on%2520contextual%2520coherence%2520%2528ensuring%2520ads%2520align%2520semantically%2520with%2520transient%2520user%2520intents%2529%2520and%2520computational%2520efficiency%2520%2528avoiding%2520user%2520interaction%2520latency%2529%252C%2520as%2520well%2520as%2520adherence%2520to%2520ethical%2520and%2520regulatory%2520standards%252C%2520including%2520preserving%2520privacy%2520and%2520ensuring%2520explicit%2520ad%2520disclosure.%2520Although%2520various%2520recent%2520solutions%2520have%2520explored%2520bidding%2520on%2520token-level%2520and%2520query-level%252C%2520both%2520categories%2520of%2520approaches%2520generally%2520fail%2520to%2520holistically%2520satisfy%2520this%2520multifaceted%2520set%2520of%2520constraints.%250A%2520%2520We%2520propose%2520a%2520practical%2520framework%2520that%2520resolves%2520these%2520tensions%2520through%2520two%2520decoupling%2520strategies.%2520First%252C%2520we%2520decouple%2520ad%2520insertion%2520from%2520response%2520generation%2520to%2520ensure%2520safety%2520and%2520explicit%2520disclosure.%2520Second%252C%2520we%2520decouple%2520bidding%2520from%2520specific%2520user%2520queries%2520by%2520using%2520%2560%2560genres%2527%2527%2520%2528high-level%2520semantic%2520clusters%2529%2520as%2520a%2520proxy.%2520This%2520allows%2520advertisers%2520to%2520bid%2520on%2520stable%2520categories%2520rather%2520than%2520sensitive%2520real-time%2520response%252C%2520reducing%2520computational%2520burden%2520and%2520privacy%2520risks.%2520We%2520demonstrate%2520that%2520applying%2520the%2520VCG%2520auction%2520mechanism%2520to%2520this%2520genre-based%2520framework%2520yields%2520approximately%2520dominant%2520strategy%2520incentive%2520compatibility%2520%2528DSIC%2529%2520and%2520individual%2520rationality%2520%2528IR%2529%252C%2520as%2520well%2520as%2520approximately%2520optimal%2520social%2520welfare%252C%2520while%2520maintaining%2520high%2520computational%2520efficiency.%2520Finally%252C%2520we%2520introduce%2520an%2520%2522LLM-as-a-Judge%2522%2520metric%2520to%2520estimate%2520contextual%2520coherence.%2520Our%2520experiments%2520show%2520that%2520this%2520metric%2520correlates%2520strongly%2520with%2520human%2520ratings%2520%2528Spearman%2527s%2520%2524%25CF%2581%255Capprox%25200.66%2524%2529%252C%2520outperforming%252080%2525%2520of%2520individual%2520human%2520evaluators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ad%20Insertion%20in%20LLM-Generated%20Responses&entry.906535625=Shengwei%20Xu%20and%20Zhaohua%20Chen%20and%20Xiaotie%20Deng%20and%20Zhiyi%20Huang%20and%20Grant%20Schoenebeck&entry.1292438233=Sustainable%20monetization%20of%20Large%20Language%20Models%20%28LLMs%29%20remains%20a%20critical%20open%20challenge.%20Traditional%20search%20advertising%2C%20which%20relies%20on%20static%20keywords%2C%20fails%20to%20capture%20the%20fleeting%2C%20context-dependent%20user%20intents--the%20specific%20information%2C%20goods%2C%20or%20services%20a%20user%20seeks--embedded%20in%20conversational%20flows.%20Beyond%20the%20standard%20goal%20of%20social%20welfare%20maximization%2C%20effective%20LLM%20advertising%20imposes%20additional%20requirements%20on%20contextual%20coherence%20%28ensuring%20ads%20align%20semantically%20with%20transient%20user%20intents%29%20and%20computational%20efficiency%20%28avoiding%20user%20interaction%20latency%29%2C%20as%20well%20as%20adherence%20to%20ethical%20and%20regulatory%20standards%2C%20including%20preserving%20privacy%20and%20ensuring%20explicit%20ad%20disclosure.%20Although%20various%20recent%20solutions%20have%20explored%20bidding%20on%20token-level%20and%20query-level%2C%20both%20categories%20of%20approaches%20generally%20fail%20to%20holistically%20satisfy%20this%20multifaceted%20set%20of%20constraints.%0A%20%20We%20propose%20a%20practical%20framework%20that%20resolves%20these%20tensions%20through%20two%20decoupling%20strategies.%20First%2C%20we%20decouple%20ad%20insertion%20from%20response%20generation%20to%20ensure%20safety%20and%20explicit%20disclosure.%20Second%2C%20we%20decouple%20bidding%20from%20specific%20user%20queries%20by%20using%20%60%60genres%27%27%20%28high-level%20semantic%20clusters%29%20as%20a%20proxy.%20This%20allows%20advertisers%20to%20bid%20on%20stable%20categories%20rather%20than%20sensitive%20real-time%20response%2C%20reducing%20computational%20burden%20and%20privacy%20risks.%20We%20demonstrate%20that%20applying%20the%20VCG%20auction%20mechanism%20to%20this%20genre-based%20framework%20yields%20approximately%20dominant%20strategy%20incentive%20compatibility%20%28DSIC%29%20and%20individual%20rationality%20%28IR%29%2C%20as%20well%20as%20approximately%20optimal%20social%20welfare%2C%20while%20maintaining%20high%20computational%20efficiency.%20Finally%2C%20we%20introduce%20an%20%22LLM-as-a-Judge%22%20metric%20to%20estimate%20contextual%20coherence.%20Our%20experiments%20show%20that%20this%20metric%20correlates%20strongly%20with%20human%20ratings%20%28Spearman%27s%20%24%CF%81%5Capprox%200.66%24%29%2C%20outperforming%2080%25%20of%20individual%20human%20evaluators.&entry.1838667208=http%3A//arxiv.org/abs/2601.19435v1&entry.124074799=Read"},
{"title": "Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning", "author": "Matan Leibovich and Mai Tan and Ramon Manzorro-Ureba and Adria Marcos-Morales and Sreyas Mohan and Peter A. Crozier and Carlos Fernandez-Granda", "abstract": "We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.", "link": "http://arxiv.org/abs/2601.17046v2", "date": "2026-01-27", "relevancy": 1.5449, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5298}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5167}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atomic%20Depth%20Estimation%20From%20Noisy%20Electron%20Microscopy%20Data%20Via%20Deep%20Learning&body=Title%3A%20Atomic%20Depth%20Estimation%20From%20Noisy%20Electron%20Microscopy%20Data%20Via%20Deep%20Learning%0AAuthor%3A%20Matan%20Leibovich%20and%20Mai%20Tan%20and%20Ramon%20Manzorro-Ureba%20and%20Adria%20Marcos-Morales%20and%20Sreyas%20Mohan%20and%20Peter%20A.%20Crozier%20and%20Carlos%20Fernandez-Granda%0AAbstract%3A%20We%20present%20a%20novel%20approach%20for%20extracting%203D%20atomic-level%20information%20from%20transmission%20electron%20microscopy%20%28TEM%29%20images%20affected%20by%20significant%20noise.%20The%20approach%20is%20based%20on%20formulating%20depth%20estimation%20as%20a%20semantic%20segmentation%20problem.%20We%20address%20the%20resulting%20segmentation%20problem%20by%20training%20a%20deep%20convolutional%20neural%20network%20to%20generate%20pixel-wise%20depth%20segmentation%20maps%20using%20simulated%20data%20corrupted%20by%20synthetic%20noise.%20The%20proposed%20method%20was%20applied%20to%20estimate%20the%20depth%20of%20atomic%20columns%20in%20CeO2%20nanoparticles%20from%20simulated%20images%20and%20real-world%20TEM%20data.%20Our%20experiments%20show%20that%20the%20resulting%20depth%20estimates%20are%20accurate%2C%20calibrated%20and%20robust%20to%20noise.%0ALink%3A%20http%3A//arxiv.org/abs/2601.17046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtomic%2520Depth%2520Estimation%2520From%2520Noisy%2520Electron%2520Microscopy%2520Data%2520Via%2520Deep%2520Learning%26entry.906535625%3DMatan%2520Leibovich%2520and%2520Mai%2520Tan%2520and%2520Ramon%2520Manzorro-Ureba%2520and%2520Adria%2520Marcos-Morales%2520and%2520Sreyas%2520Mohan%2520and%2520Peter%2520A.%2520Crozier%2520and%2520Carlos%2520Fernandez-Granda%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520approach%2520for%2520extracting%25203D%2520atomic-level%2520information%2520from%2520transmission%2520electron%2520microscopy%2520%2528TEM%2529%2520images%2520affected%2520by%2520significant%2520noise.%2520The%2520approach%2520is%2520based%2520on%2520formulating%2520depth%2520estimation%2520as%2520a%2520semantic%2520segmentation%2520problem.%2520We%2520address%2520the%2520resulting%2520segmentation%2520problem%2520by%2520training%2520a%2520deep%2520convolutional%2520neural%2520network%2520to%2520generate%2520pixel-wise%2520depth%2520segmentation%2520maps%2520using%2520simulated%2520data%2520corrupted%2520by%2520synthetic%2520noise.%2520The%2520proposed%2520method%2520was%2520applied%2520to%2520estimate%2520the%2520depth%2520of%2520atomic%2520columns%2520in%2520CeO2%2520nanoparticles%2520from%2520simulated%2520images%2520and%2520real-world%2520TEM%2520data.%2520Our%2520experiments%2520show%2520that%2520the%2520resulting%2520depth%2520estimates%2520are%2520accurate%252C%2520calibrated%2520and%2520robust%2520to%2520noise.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atomic%20Depth%20Estimation%20From%20Noisy%20Electron%20Microscopy%20Data%20Via%20Deep%20Learning&entry.906535625=Matan%20Leibovich%20and%20Mai%20Tan%20and%20Ramon%20Manzorro-Ureba%20and%20Adria%20Marcos-Morales%20and%20Sreyas%20Mohan%20and%20Peter%20A.%20Crozier%20and%20Carlos%20Fernandez-Granda&entry.1292438233=We%20present%20a%20novel%20approach%20for%20extracting%203D%20atomic-level%20information%20from%20transmission%20electron%20microscopy%20%28TEM%29%20images%20affected%20by%20significant%20noise.%20The%20approach%20is%20based%20on%20formulating%20depth%20estimation%20as%20a%20semantic%20segmentation%20problem.%20We%20address%20the%20resulting%20segmentation%20problem%20by%20training%20a%20deep%20convolutional%20neural%20network%20to%20generate%20pixel-wise%20depth%20segmentation%20maps%20using%20simulated%20data%20corrupted%20by%20synthetic%20noise.%20The%20proposed%20method%20was%20applied%20to%20estimate%20the%20depth%20of%20atomic%20columns%20in%20CeO2%20nanoparticles%20from%20simulated%20images%20and%20real-world%20TEM%20data.%20Our%20experiments%20show%20that%20the%20resulting%20depth%20estimates%20are%20accurate%2C%20calibrated%20and%20robust%20to%20noise.&entry.1838667208=http%3A//arxiv.org/abs/2601.17046v2&entry.124074799=Read"},
{"title": "There and Back Again: On the relation between Noise and Image Inversions in Diffusion Models", "author": "\u0141ukasz Staniszewski and \u0141ukasz Kuci\u0144ski and Kamil Deja", "abstract": "Diffusion Models achieve state-of-the-art performance in generating new samples but lack a low-dimensional latent space that encodes the data into editable features. Inversion-based methods address this by reversing the denoising trajectory, transferring images to their approximated starting noise. In this work, we thoroughly analyze this procedure and focus on the relation between the initial noise, the generated samples, and their corresponding latent encodings obtained through the DDIM inversion. First, we show that latents exhibit structural patterns in the form of less diverse noise predicted for smooth image areas (e.g., plain sky). Through a series of analyses, we trace this issue to the first inversion steps, which fail to provide accurate and diverse noise. Consequently, the DDIM inversion space is notably less manipulative than the original noise. We show that prior inversion methods do not fully resolve this issue, but our simple fix, where we replace the first DDIM Inversion steps with a forward diffusion process, successfully decorrelates latent encodings and enables higher quality editions and interpolations. The code is available at https://github.com/luk-st/taba.", "link": "http://arxiv.org/abs/2410.23530v5", "date": "2026-01-27", "relevancy": 1.8252, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6099}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.608}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20There%20and%20Back%20Again%3A%20On%20the%20relation%20between%20Noise%20and%20Image%20Inversions%20in%20Diffusion%20Models&body=Title%3A%20There%20and%20Back%20Again%3A%20On%20the%20relation%20between%20Noise%20and%20Image%20Inversions%20in%20Diffusion%20Models%0AAuthor%3A%20%C5%81ukasz%20Staniszewski%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Kamil%20Deja%0AAbstract%3A%20Diffusion%20Models%20achieve%20state-of-the-art%20performance%20in%20generating%20new%20samples%20but%20lack%20a%20low-dimensional%20latent%20space%20that%20encodes%20the%20data%20into%20editable%20features.%20Inversion-based%20methods%20address%20this%20by%20reversing%20the%20denoising%20trajectory%2C%20transferring%20images%20to%20their%20approximated%20starting%20noise.%20In%20this%20work%2C%20we%20thoroughly%20analyze%20this%20procedure%20and%20focus%20on%20the%20relation%20between%20the%20initial%20noise%2C%20the%20generated%20samples%2C%20and%20their%20corresponding%20latent%20encodings%20obtained%20through%20the%20DDIM%20inversion.%20First%2C%20we%20show%20that%20latents%20exhibit%20structural%20patterns%20in%20the%20form%20of%20less%20diverse%20noise%20predicted%20for%20smooth%20image%20areas%20%28e.g.%2C%20plain%20sky%29.%20Through%20a%20series%20of%20analyses%2C%20we%20trace%20this%20issue%20to%20the%20first%20inversion%20steps%2C%20which%20fail%20to%20provide%20accurate%20and%20diverse%20noise.%20Consequently%2C%20the%20DDIM%20inversion%20space%20is%20notably%20less%20manipulative%20than%20the%20original%20noise.%20We%20show%20that%20prior%20inversion%20methods%20do%20not%20fully%20resolve%20this%20issue%2C%20but%20our%20simple%20fix%2C%20where%20we%20replace%20the%20first%20DDIM%20Inversion%20steps%20with%20a%20forward%20diffusion%20process%2C%20successfully%20decorrelates%20latent%20encodings%20and%20enables%20higher%20quality%20editions%20and%20interpolations.%20The%20code%20is%20available%20at%20https%3A//github.com/luk-st/taba.%0ALink%3A%20http%3A//arxiv.org/abs/2410.23530v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThere%2520and%2520Back%2520Again%253A%2520On%2520the%2520relation%2520between%2520Noise%2520and%2520Image%2520Inversions%2520in%2520Diffusion%2520Models%26entry.906535625%3D%25C5%2581ukasz%2520Staniszewski%2520and%2520%25C5%2581ukasz%2520Kuci%25C5%2584ski%2520and%2520Kamil%2520Deja%26entry.1292438233%3DDiffusion%2520Models%2520achieve%2520state-of-the-art%2520performance%2520in%2520generating%2520new%2520samples%2520but%2520lack%2520a%2520low-dimensional%2520latent%2520space%2520that%2520encodes%2520the%2520data%2520into%2520editable%2520features.%2520Inversion-based%2520methods%2520address%2520this%2520by%2520reversing%2520the%2520denoising%2520trajectory%252C%2520transferring%2520images%2520to%2520their%2520approximated%2520starting%2520noise.%2520In%2520this%2520work%252C%2520we%2520thoroughly%2520analyze%2520this%2520procedure%2520and%2520focus%2520on%2520the%2520relation%2520between%2520the%2520initial%2520noise%252C%2520the%2520generated%2520samples%252C%2520and%2520their%2520corresponding%2520latent%2520encodings%2520obtained%2520through%2520the%2520DDIM%2520inversion.%2520First%252C%2520we%2520show%2520that%2520latents%2520exhibit%2520structural%2520patterns%2520in%2520the%2520form%2520of%2520less%2520diverse%2520noise%2520predicted%2520for%2520smooth%2520image%2520areas%2520%2528e.g.%252C%2520plain%2520sky%2529.%2520Through%2520a%2520series%2520of%2520analyses%252C%2520we%2520trace%2520this%2520issue%2520to%2520the%2520first%2520inversion%2520steps%252C%2520which%2520fail%2520to%2520provide%2520accurate%2520and%2520diverse%2520noise.%2520Consequently%252C%2520the%2520DDIM%2520inversion%2520space%2520is%2520notably%2520less%2520manipulative%2520than%2520the%2520original%2520noise.%2520We%2520show%2520that%2520prior%2520inversion%2520methods%2520do%2520not%2520fully%2520resolve%2520this%2520issue%252C%2520but%2520our%2520simple%2520fix%252C%2520where%2520we%2520replace%2520the%2520first%2520DDIM%2520Inversion%2520steps%2520with%2520a%2520forward%2520diffusion%2520process%252C%2520successfully%2520decorrelates%2520latent%2520encodings%2520and%2520enables%2520higher%2520quality%2520editions%2520and%2520interpolations.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/luk-st/taba.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23530v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=There%20and%20Back%20Again%3A%20On%20the%20relation%20between%20Noise%20and%20Image%20Inversions%20in%20Diffusion%20Models&entry.906535625=%C5%81ukasz%20Staniszewski%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Kamil%20Deja&entry.1292438233=Diffusion%20Models%20achieve%20state-of-the-art%20performance%20in%20generating%20new%20samples%20but%20lack%20a%20low-dimensional%20latent%20space%20that%20encodes%20the%20data%20into%20editable%20features.%20Inversion-based%20methods%20address%20this%20by%20reversing%20the%20denoising%20trajectory%2C%20transferring%20images%20to%20their%20approximated%20starting%20noise.%20In%20this%20work%2C%20we%20thoroughly%20analyze%20this%20procedure%20and%20focus%20on%20the%20relation%20between%20the%20initial%20noise%2C%20the%20generated%20samples%2C%20and%20their%20corresponding%20latent%20encodings%20obtained%20through%20the%20DDIM%20inversion.%20First%2C%20we%20show%20that%20latents%20exhibit%20structural%20patterns%20in%20the%20form%20of%20less%20diverse%20noise%20predicted%20for%20smooth%20image%20areas%20%28e.g.%2C%20plain%20sky%29.%20Through%20a%20series%20of%20analyses%2C%20we%20trace%20this%20issue%20to%20the%20first%20inversion%20steps%2C%20which%20fail%20to%20provide%20accurate%20and%20diverse%20noise.%20Consequently%2C%20the%20DDIM%20inversion%20space%20is%20notably%20less%20manipulative%20than%20the%20original%20noise.%20We%20show%20that%20prior%20inversion%20methods%20do%20not%20fully%20resolve%20this%20issue%2C%20but%20our%20simple%20fix%2C%20where%20we%20replace%20the%20first%20DDIM%20Inversion%20steps%20with%20a%20forward%20diffusion%20process%2C%20successfully%20decorrelates%20latent%20encodings%20and%20enables%20higher%20quality%20editions%20and%20interpolations.%20The%20code%20is%20available%20at%20https%3A//github.com/luk-st/taba.&entry.1838667208=http%3A//arxiv.org/abs/2410.23530v5&entry.124074799=Read"},
{"title": "QuaMo: Quaternion Motions for Vision-based 3D Human Kinematics Capture", "author": "Cuong Le and Pavlo Melnyk and Urs Waldmann and M\u00e5rten Wadenb\u00e4ck and Bastian Wandt", "abstract": "Vision-based 3D human motion capture from videos remains a challenge in computer vision. Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion. The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead. A major drawback in current kinematics approaches is their reliance on Euler angles. Despite their simplicity, Euler angles suffer from discontinuity that leads to unstable motion reconstructions, especially in online settings where trajectory refinement is unavailable. Contrarily, quaternions have no discontinuity and can produce continuous transitions between poses. In this paper, we propose QuaMo, a novel Quaternion Motions method using quaternion differential equations (QDE) for human kinematics capture. We utilize the state-space model, an effective system for describing real-time kinematics estimations, with quaternion state and the QDE describing quaternion velocity. The corresponding angular acceleration is computed from a meta-PD controller with a novel acceleration enhancement that adaptively regulates the control signals as the human quickly changes to a new pose. Unlike previous work, our QDE is solved under the quaternion unit-sphere constraint that results in more accurate estimations. Experimental results show that our novel formulation of the QDE with acceleration enhancement accurately estimates 3D human kinematics with no discontinuity and minimal implausibilities. QuaMo outperforms comparable state-of-the-art methods on multiple datasets, namely Human3.6M, Fit3D, SportsPose and AIST. The code is available at https://github.com/cuongle1206/QuaMo", "link": "http://arxiv.org/abs/2601.19580v1", "date": "2026-01-27", "relevancy": 1.7796, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6276}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuaMo%3A%20Quaternion%20Motions%20for%20Vision-based%203D%20Human%20Kinematics%20Capture&body=Title%3A%20QuaMo%3A%20Quaternion%20Motions%20for%20Vision-based%203D%20Human%20Kinematics%20Capture%0AAuthor%3A%20Cuong%20Le%20and%20Pavlo%20Melnyk%20and%20Urs%20Waldmann%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Bastian%20Wandt%0AAbstract%3A%20Vision-based%203D%20human%20motion%20capture%20from%20videos%20remains%20a%20challenge%20in%20computer%20vision.%20Traditional%203D%20pose%20estimation%20approaches%20often%20ignore%20the%20temporal%20consistency%20between%20frames%2C%20causing%20implausible%20and%20jittery%20motion.%20The%20emerging%20field%20of%20kinematics-based%203D%20motion%20capture%20addresses%20these%20issues%20by%20estimating%20the%20temporal%20transitioning%20between%20poses%20instead.%20A%20major%20drawback%20in%20current%20kinematics%20approaches%20is%20their%20reliance%20on%20Euler%20angles.%20Despite%20their%20simplicity%2C%20Euler%20angles%20suffer%20from%20discontinuity%20that%20leads%20to%20unstable%20motion%20reconstructions%2C%20especially%20in%20online%20settings%20where%20trajectory%20refinement%20is%20unavailable.%20Contrarily%2C%20quaternions%20have%20no%20discontinuity%20and%20can%20produce%20continuous%20transitions%20between%20poses.%20In%20this%20paper%2C%20we%20propose%20QuaMo%2C%20a%20novel%20Quaternion%20Motions%20method%20using%20quaternion%20differential%20equations%20%28QDE%29%20for%20human%20kinematics%20capture.%20We%20utilize%20the%20state-space%20model%2C%20an%20effective%20system%20for%20describing%20real-time%20kinematics%20estimations%2C%20with%20quaternion%20state%20and%20the%20QDE%20describing%20quaternion%20velocity.%20The%20corresponding%20angular%20acceleration%20is%20computed%20from%20a%20meta-PD%20controller%20with%20a%20novel%20acceleration%20enhancement%20that%20adaptively%20regulates%20the%20control%20signals%20as%20the%20human%20quickly%20changes%20to%20a%20new%20pose.%20Unlike%20previous%20work%2C%20our%20QDE%20is%20solved%20under%20the%20quaternion%20unit-sphere%20constraint%20that%20results%20in%20more%20accurate%20estimations.%20Experimental%20results%20show%20that%20our%20novel%20formulation%20of%20the%20QDE%20with%20acceleration%20enhancement%20accurately%20estimates%203D%20human%20kinematics%20with%20no%20discontinuity%20and%20minimal%20implausibilities.%20QuaMo%20outperforms%20comparable%20state-of-the-art%20methods%20on%20multiple%20datasets%2C%20namely%20Human3.6M%2C%20Fit3D%2C%20SportsPose%20and%20AIST.%20The%20code%20is%20available%20at%20https%3A//github.com/cuongle1206/QuaMo%0ALink%3A%20http%3A//arxiv.org/abs/2601.19580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuaMo%253A%2520Quaternion%2520Motions%2520for%2520Vision-based%25203D%2520Human%2520Kinematics%2520Capture%26entry.906535625%3DCuong%2520Le%2520and%2520Pavlo%2520Melnyk%2520and%2520Urs%2520Waldmann%2520and%2520M%25C3%25A5rten%2520Wadenb%25C3%25A4ck%2520and%2520Bastian%2520Wandt%26entry.1292438233%3DVision-based%25203D%2520human%2520motion%2520capture%2520from%2520videos%2520remains%2520a%2520challenge%2520in%2520computer%2520vision.%2520Traditional%25203D%2520pose%2520estimation%2520approaches%2520often%2520ignore%2520the%2520temporal%2520consistency%2520between%2520frames%252C%2520causing%2520implausible%2520and%2520jittery%2520motion.%2520The%2520emerging%2520field%2520of%2520kinematics-based%25203D%2520motion%2520capture%2520addresses%2520these%2520issues%2520by%2520estimating%2520the%2520temporal%2520transitioning%2520between%2520poses%2520instead.%2520A%2520major%2520drawback%2520in%2520current%2520kinematics%2520approaches%2520is%2520their%2520reliance%2520on%2520Euler%2520angles.%2520Despite%2520their%2520simplicity%252C%2520Euler%2520angles%2520suffer%2520from%2520discontinuity%2520that%2520leads%2520to%2520unstable%2520motion%2520reconstructions%252C%2520especially%2520in%2520online%2520settings%2520where%2520trajectory%2520refinement%2520is%2520unavailable.%2520Contrarily%252C%2520quaternions%2520have%2520no%2520discontinuity%2520and%2520can%2520produce%2520continuous%2520transitions%2520between%2520poses.%2520In%2520this%2520paper%252C%2520we%2520propose%2520QuaMo%252C%2520a%2520novel%2520Quaternion%2520Motions%2520method%2520using%2520quaternion%2520differential%2520equations%2520%2528QDE%2529%2520for%2520human%2520kinematics%2520capture.%2520We%2520utilize%2520the%2520state-space%2520model%252C%2520an%2520effective%2520system%2520for%2520describing%2520real-time%2520kinematics%2520estimations%252C%2520with%2520quaternion%2520state%2520and%2520the%2520QDE%2520describing%2520quaternion%2520velocity.%2520The%2520corresponding%2520angular%2520acceleration%2520is%2520computed%2520from%2520a%2520meta-PD%2520controller%2520with%2520a%2520novel%2520acceleration%2520enhancement%2520that%2520adaptively%2520regulates%2520the%2520control%2520signals%2520as%2520the%2520human%2520quickly%2520changes%2520to%2520a%2520new%2520pose.%2520Unlike%2520previous%2520work%252C%2520our%2520QDE%2520is%2520solved%2520under%2520the%2520quaternion%2520unit-sphere%2520constraint%2520that%2520results%2520in%2520more%2520accurate%2520estimations.%2520Experimental%2520results%2520show%2520that%2520our%2520novel%2520formulation%2520of%2520the%2520QDE%2520with%2520acceleration%2520enhancement%2520accurately%2520estimates%25203D%2520human%2520kinematics%2520with%2520no%2520discontinuity%2520and%2520minimal%2520implausibilities.%2520QuaMo%2520outperforms%2520comparable%2520state-of-the-art%2520methods%2520on%2520multiple%2520datasets%252C%2520namely%2520Human3.6M%252C%2520Fit3D%252C%2520SportsPose%2520and%2520AIST.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/cuongle1206/QuaMo%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuaMo%3A%20Quaternion%20Motions%20for%20Vision-based%203D%20Human%20Kinematics%20Capture&entry.906535625=Cuong%20Le%20and%20Pavlo%20Melnyk%20and%20Urs%20Waldmann%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Bastian%20Wandt&entry.1292438233=Vision-based%203D%20human%20motion%20capture%20from%20videos%20remains%20a%20challenge%20in%20computer%20vision.%20Traditional%203D%20pose%20estimation%20approaches%20often%20ignore%20the%20temporal%20consistency%20between%20frames%2C%20causing%20implausible%20and%20jittery%20motion.%20The%20emerging%20field%20of%20kinematics-based%203D%20motion%20capture%20addresses%20these%20issues%20by%20estimating%20the%20temporal%20transitioning%20between%20poses%20instead.%20A%20major%20drawback%20in%20current%20kinematics%20approaches%20is%20their%20reliance%20on%20Euler%20angles.%20Despite%20their%20simplicity%2C%20Euler%20angles%20suffer%20from%20discontinuity%20that%20leads%20to%20unstable%20motion%20reconstructions%2C%20especially%20in%20online%20settings%20where%20trajectory%20refinement%20is%20unavailable.%20Contrarily%2C%20quaternions%20have%20no%20discontinuity%20and%20can%20produce%20continuous%20transitions%20between%20poses.%20In%20this%20paper%2C%20we%20propose%20QuaMo%2C%20a%20novel%20Quaternion%20Motions%20method%20using%20quaternion%20differential%20equations%20%28QDE%29%20for%20human%20kinematics%20capture.%20We%20utilize%20the%20state-space%20model%2C%20an%20effective%20system%20for%20describing%20real-time%20kinematics%20estimations%2C%20with%20quaternion%20state%20and%20the%20QDE%20describing%20quaternion%20velocity.%20The%20corresponding%20angular%20acceleration%20is%20computed%20from%20a%20meta-PD%20controller%20with%20a%20novel%20acceleration%20enhancement%20that%20adaptively%20regulates%20the%20control%20signals%20as%20the%20human%20quickly%20changes%20to%20a%20new%20pose.%20Unlike%20previous%20work%2C%20our%20QDE%20is%20solved%20under%20the%20quaternion%20unit-sphere%20constraint%20that%20results%20in%20more%20accurate%20estimations.%20Experimental%20results%20show%20that%20our%20novel%20formulation%20of%20the%20QDE%20with%20acceleration%20enhancement%20accurately%20estimates%203D%20human%20kinematics%20with%20no%20discontinuity%20and%20minimal%20implausibilities.%20QuaMo%20outperforms%20comparable%20state-of-the-art%20methods%20on%20multiple%20datasets%2C%20namely%20Human3.6M%2C%20Fit3D%2C%20SportsPose%20and%20AIST.%20The%20code%20is%20available%20at%20https%3A//github.com/cuongle1206/QuaMo&entry.1838667208=http%3A//arxiv.org/abs/2601.19580v1&entry.124074799=Read"},
{"title": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning", "author": "Jiayu Chen and Le Xu and Wentse Chen and Jeff Schneider", "abstract": "Offline reinforcement learning (RL) is a powerful approach for data-driven decision-making and control. Compared to model-free methods, offline model-based reinforcement learning (MBRL) explicitly learns world models from a static dataset and uses them as surrogate simulators, improving the data efficiency and enabling the learned policy to potentially generalize beyond the dataset support. However, there could be various MDPs that behave identically on the offline dataset and dealing with the uncertainty about the true MDP can be challenging. In this paper, we propose modeling offline MBRL as a Bayes Adaptive Markov Decision Process (BAMDP), which is a principled framework for addressing model uncertainty. We further propose a novel Bayes Adaptive Monte-Carlo planning algorithm capable of solving BAMDPs in continuous state and action spaces with stochastic transitions. This planning process is based on Monte Carlo Tree Search and can be integrated into offline MBRL as a policy improvement operator in policy iteration. Our \"RL + Search\" framework follows in the footsteps of superhuman AIs like AlphaZero, improving on current offline MBRL methods by incorporating more computation input. The proposed algorithm significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo tasks and three challenging, stochastic tokamak control tasks. The codebase is available at: https://github.com/LucasCJYSDL/Offline-RL-Kit.", "link": "http://arxiv.org/abs/2410.11234v4", "date": "2026-01-27", "relevancy": 1.4678, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4881}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayes%20Adaptive%20Monte%20Carlo%20Tree%20Search%20for%20Offline%20Model-based%20Reinforcement%20Learning&body=Title%3A%20Bayes%20Adaptive%20Monte%20Carlo%20Tree%20Search%20for%20Offline%20Model-based%20Reinforcement%20Learning%0AAuthor%3A%20Jiayu%20Chen%20and%20Le%20Xu%20and%20Wentse%20Chen%20and%20Jeff%20Schneider%0AAbstract%3A%20Offline%20reinforcement%20learning%20%28RL%29%20is%20a%20powerful%20approach%20for%20data-driven%20decision-making%20and%20control.%20Compared%20to%20model-free%20methods%2C%20offline%20model-based%20reinforcement%20learning%20%28MBRL%29%20explicitly%20learns%20world%20models%20from%20a%20static%20dataset%20and%20uses%20them%20as%20surrogate%20simulators%2C%20improving%20the%20data%20efficiency%20and%20enabling%20the%20learned%20policy%20to%20potentially%20generalize%20beyond%20the%20dataset%20support.%20However%2C%20there%20could%20be%20various%20MDPs%20that%20behave%20identically%20on%20the%20offline%20dataset%20and%20dealing%20with%20the%20uncertainty%20about%20the%20true%20MDP%20can%20be%20challenging.%20In%20this%20paper%2C%20we%20propose%20modeling%20offline%20MBRL%20as%20a%20Bayes%20Adaptive%20Markov%20Decision%20Process%20%28BAMDP%29%2C%20which%20is%20a%20principled%20framework%20for%20addressing%20model%20uncertainty.%20We%20further%20propose%20a%20novel%20Bayes%20Adaptive%20Monte-Carlo%20planning%20algorithm%20capable%20of%20solving%20BAMDPs%20in%20continuous%20state%20and%20action%20spaces%20with%20stochastic%20transitions.%20This%20planning%20process%20is%20based%20on%20Monte%20Carlo%20Tree%20Search%20and%20can%20be%20integrated%20into%20offline%20MBRL%20as%20a%20policy%20improvement%20operator%20in%20policy%20iteration.%20Our%20%22RL%20%2B%20Search%22%20framework%20follows%20in%20the%20footsteps%20of%20superhuman%20AIs%20like%20AlphaZero%2C%20improving%20on%20current%20offline%20MBRL%20methods%20by%20incorporating%20more%20computation%20input.%20The%20proposed%20algorithm%20significantly%20outperforms%20state-of-the-art%20offline%20RL%20methods%20on%20twelve%20D4RL%20MuJoCo%20tasks%20and%20three%20challenging%2C%20stochastic%20tokamak%20control%20tasks.%20The%20codebase%20is%20available%20at%3A%20https%3A//github.com/LucasCJYSDL/Offline-RL-Kit.%0ALink%3A%20http%3A//arxiv.org/abs/2410.11234v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayes%2520Adaptive%2520Monte%2520Carlo%2520Tree%2520Search%2520for%2520Offline%2520Model-based%2520Reinforcement%2520Learning%26entry.906535625%3DJiayu%2520Chen%2520and%2520Le%2520Xu%2520and%2520Wentse%2520Chen%2520and%2520Jeff%2520Schneider%26entry.1292438233%3DOffline%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520a%2520powerful%2520approach%2520for%2520data-driven%2520decision-making%2520and%2520control.%2520Compared%2520to%2520model-free%2520methods%252C%2520offline%2520model-based%2520reinforcement%2520learning%2520%2528MBRL%2529%2520explicitly%2520learns%2520world%2520models%2520from%2520a%2520static%2520dataset%2520and%2520uses%2520them%2520as%2520surrogate%2520simulators%252C%2520improving%2520the%2520data%2520efficiency%2520and%2520enabling%2520the%2520learned%2520policy%2520to%2520potentially%2520generalize%2520beyond%2520the%2520dataset%2520support.%2520However%252C%2520there%2520could%2520be%2520various%2520MDPs%2520that%2520behave%2520identically%2520on%2520the%2520offline%2520dataset%2520and%2520dealing%2520with%2520the%2520uncertainty%2520about%2520the%2520true%2520MDP%2520can%2520be%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520modeling%2520offline%2520MBRL%2520as%2520a%2520Bayes%2520Adaptive%2520Markov%2520Decision%2520Process%2520%2528BAMDP%2529%252C%2520which%2520is%2520a%2520principled%2520framework%2520for%2520addressing%2520model%2520uncertainty.%2520We%2520further%2520propose%2520a%2520novel%2520Bayes%2520Adaptive%2520Monte-Carlo%2520planning%2520algorithm%2520capable%2520of%2520solving%2520BAMDPs%2520in%2520continuous%2520state%2520and%2520action%2520spaces%2520with%2520stochastic%2520transitions.%2520This%2520planning%2520process%2520is%2520based%2520on%2520Monte%2520Carlo%2520Tree%2520Search%2520and%2520can%2520be%2520integrated%2520into%2520offline%2520MBRL%2520as%2520a%2520policy%2520improvement%2520operator%2520in%2520policy%2520iteration.%2520Our%2520%2522RL%2520%252B%2520Search%2522%2520framework%2520follows%2520in%2520the%2520footsteps%2520of%2520superhuman%2520AIs%2520like%2520AlphaZero%252C%2520improving%2520on%2520current%2520offline%2520MBRL%2520methods%2520by%2520incorporating%2520more%2520computation%2520input.%2520The%2520proposed%2520algorithm%2520significantly%2520outperforms%2520state-of-the-art%2520offline%2520RL%2520methods%2520on%2520twelve%2520D4RL%2520MuJoCo%2520tasks%2520and%2520three%2520challenging%252C%2520stochastic%2520tokamak%2520control%2520tasks.%2520The%2520codebase%2520is%2520available%2520at%253A%2520https%253A//github.com/LucasCJYSDL/Offline-RL-Kit.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11234v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayes%20Adaptive%20Monte%20Carlo%20Tree%20Search%20for%20Offline%20Model-based%20Reinforcement%20Learning&entry.906535625=Jiayu%20Chen%20and%20Le%20Xu%20and%20Wentse%20Chen%20and%20Jeff%20Schneider&entry.1292438233=Offline%20reinforcement%20learning%20%28RL%29%20is%20a%20powerful%20approach%20for%20data-driven%20decision-making%20and%20control.%20Compared%20to%20model-free%20methods%2C%20offline%20model-based%20reinforcement%20learning%20%28MBRL%29%20explicitly%20learns%20world%20models%20from%20a%20static%20dataset%20and%20uses%20them%20as%20surrogate%20simulators%2C%20improving%20the%20data%20efficiency%20and%20enabling%20the%20learned%20policy%20to%20potentially%20generalize%20beyond%20the%20dataset%20support.%20However%2C%20there%20could%20be%20various%20MDPs%20that%20behave%20identically%20on%20the%20offline%20dataset%20and%20dealing%20with%20the%20uncertainty%20about%20the%20true%20MDP%20can%20be%20challenging.%20In%20this%20paper%2C%20we%20propose%20modeling%20offline%20MBRL%20as%20a%20Bayes%20Adaptive%20Markov%20Decision%20Process%20%28BAMDP%29%2C%20which%20is%20a%20principled%20framework%20for%20addressing%20model%20uncertainty.%20We%20further%20propose%20a%20novel%20Bayes%20Adaptive%20Monte-Carlo%20planning%20algorithm%20capable%20of%20solving%20BAMDPs%20in%20continuous%20state%20and%20action%20spaces%20with%20stochastic%20transitions.%20This%20planning%20process%20is%20based%20on%20Monte%20Carlo%20Tree%20Search%20and%20can%20be%20integrated%20into%20offline%20MBRL%20as%20a%20policy%20improvement%20operator%20in%20policy%20iteration.%20Our%20%22RL%20%2B%20Search%22%20framework%20follows%20in%20the%20footsteps%20of%20superhuman%20AIs%20like%20AlphaZero%2C%20improving%20on%20current%20offline%20MBRL%20methods%20by%20incorporating%20more%20computation%20input.%20The%20proposed%20algorithm%20significantly%20outperforms%20state-of-the-art%20offline%20RL%20methods%20on%20twelve%20D4RL%20MuJoCo%20tasks%20and%20three%20challenging%2C%20stochastic%20tokamak%20control%20tasks.%20The%20codebase%20is%20available%20at%3A%20https%3A//github.com/LucasCJYSDL/Offline-RL-Kit.&entry.1838667208=http%3A//arxiv.org/abs/2410.11234v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


