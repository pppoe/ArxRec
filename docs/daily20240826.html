<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240825.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D\n  Control Points", "author": "Bing He and Yunuo Chen and Guo Lu and Li Song and Wenjun Zhang", "abstract": "  Recently, the dynamic scene reconstruction using Gaussians has garnered\nincreased interest. Mainstream approaches typically employ a global deformation\nfield to warp a 3D scene in the canonical space. However, the inherently\nlow-frequency nature of implicit neural fields often leads to ineffective\nrepresentations of complex motions. Moreover, their structural rigidity can\nhinder adaptation to scenes with varying resolutions and durations. To overcome\nthese challenges, we introduce a novel approach utilizing discrete 3D control\npoints. This method models local rays physically and establishes a\nmotion-decoupling coordinate system, which effectively merges traditional\ngraphics with learnable pipelines for a robust and efficient local\n6-degrees-of-freedom (6-DoF) motion representation. Additionally, we have\ndeveloped a generalized framework that incorporates our control points with\nGaussians. Starting from an initial 3D reconstruction, our workflow decomposes\nthe streaming 4D real-world reconstruction into four independent submodules: 3D\nsegmentation, 3D control points generation, object-wise motion manipulation,\nand residual compensation. Our experiments demonstrate that this method\noutperforms existing state-of-the-art 4D Gaussian Splatting techniques on both\nthe Neu3DV and CMU-Panoptic datasets. Our approach also significantly\naccelerates training, with the optimization of our 3D control points achievable\nwithin just 2 seconds per frame on a single NVIDIA 4070 GPU.\n", "link": "http://arxiv.org/abs/2408.13036v1", "date": "2024-08-23", "relevancy": 3.4135, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7253}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7188}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S4D%3A%20Streaming%204D%20Real-World%20Reconstruction%20with%20Gaussians%20and%203D%0A%20%20Control%20Points&body=Title%3A%20S4D%3A%20Streaming%204D%20Real-World%20Reconstruction%20with%20Gaussians%20and%203D%0A%20%20Control%20Points%0AAuthor%3A%20Bing%20He%20and%20Yunuo%20Chen%20and%20Guo%20Lu%20and%20Li%20Song%20and%20Wenjun%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20the%20dynamic%20scene%20reconstruction%20using%20Gaussians%20has%20garnered%0Aincreased%20interest.%20Mainstream%20approaches%20typically%20employ%20a%20global%20deformation%0Afield%20to%20warp%20a%203D%20scene%20in%20the%20canonical%20space.%20However%2C%20the%20inherently%0Alow-frequency%20nature%20of%20implicit%20neural%20fields%20often%20leads%20to%20ineffective%0Arepresentations%20of%20complex%20motions.%20Moreover%2C%20their%20structural%20rigidity%20can%0Ahinder%20adaptation%20to%20scenes%20with%20varying%20resolutions%20and%20durations.%20To%20overcome%0Athese%20challenges%2C%20we%20introduce%20a%20novel%20approach%20utilizing%20discrete%203D%20control%0Apoints.%20This%20method%20models%20local%20rays%20physically%20and%20establishes%20a%0Amotion-decoupling%20coordinate%20system%2C%20which%20effectively%20merges%20traditional%0Agraphics%20with%20learnable%20pipelines%20for%20a%20robust%20and%20efficient%20local%0A6-degrees-of-freedom%20%286-DoF%29%20motion%20representation.%20Additionally%2C%20we%20have%0Adeveloped%20a%20generalized%20framework%20that%20incorporates%20our%20control%20points%20with%0AGaussians.%20Starting%20from%20an%20initial%203D%20reconstruction%2C%20our%20workflow%20decomposes%0Athe%20streaming%204D%20real-world%20reconstruction%20into%20four%20independent%20submodules%3A%203D%0Asegmentation%2C%203D%20control%20points%20generation%2C%20object-wise%20motion%20manipulation%2C%0Aand%20residual%20compensation.%20Our%20experiments%20demonstrate%20that%20this%20method%0Aoutperforms%20existing%20state-of-the-art%204D%20Gaussian%20Splatting%20techniques%20on%20both%0Athe%20Neu3DV%20and%20CMU-Panoptic%20datasets.%20Our%20approach%20also%20significantly%0Aaccelerates%20training%2C%20with%20the%20optimization%20of%20our%203D%20control%20points%20achievable%0Awithin%20just%202%20seconds%20per%20frame%20on%20a%20single%20NVIDIA%204070%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS4D%253A%2520Streaming%25204D%2520Real-World%2520Reconstruction%2520with%2520Gaussians%2520and%25203D%250A%2520%2520Control%2520Points%26entry.906535625%3DBing%2520He%2520and%2520Yunuo%2520Chen%2520and%2520Guo%2520Lu%2520and%2520Li%2520Song%2520and%2520Wenjun%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520dynamic%2520scene%2520reconstruction%2520using%2520Gaussians%2520has%2520garnered%250Aincreased%2520interest.%2520Mainstream%2520approaches%2520typically%2520employ%2520a%2520global%2520deformation%250Afield%2520to%2520warp%2520a%25203D%2520scene%2520in%2520the%2520canonical%2520space.%2520However%252C%2520the%2520inherently%250Alow-frequency%2520nature%2520of%2520implicit%2520neural%2520fields%2520often%2520leads%2520to%2520ineffective%250Arepresentations%2520of%2520complex%2520motions.%2520Moreover%252C%2520their%2520structural%2520rigidity%2520can%250Ahinder%2520adaptation%2520to%2520scenes%2520with%2520varying%2520resolutions%2520and%2520durations.%2520To%2520overcome%250Athese%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520utilizing%2520discrete%25203D%2520control%250Apoints.%2520This%2520method%2520models%2520local%2520rays%2520physically%2520and%2520establishes%2520a%250Amotion-decoupling%2520coordinate%2520system%252C%2520which%2520effectively%2520merges%2520traditional%250Agraphics%2520with%2520learnable%2520pipelines%2520for%2520a%2520robust%2520and%2520efficient%2520local%250A6-degrees-of-freedom%2520%25286-DoF%2529%2520motion%2520representation.%2520Additionally%252C%2520we%2520have%250Adeveloped%2520a%2520generalized%2520framework%2520that%2520incorporates%2520our%2520control%2520points%2520with%250AGaussians.%2520Starting%2520from%2520an%2520initial%25203D%2520reconstruction%252C%2520our%2520workflow%2520decomposes%250Athe%2520streaming%25204D%2520real-world%2520reconstruction%2520into%2520four%2520independent%2520submodules%253A%25203D%250Asegmentation%252C%25203D%2520control%2520points%2520generation%252C%2520object-wise%2520motion%2520manipulation%252C%250Aand%2520residual%2520compensation.%2520Our%2520experiments%2520demonstrate%2520that%2520this%2520method%250Aoutperforms%2520existing%2520state-of-the-art%25204D%2520Gaussian%2520Splatting%2520techniques%2520on%2520both%250Athe%2520Neu3DV%2520and%2520CMU-Panoptic%2520datasets.%2520Our%2520approach%2520also%2520significantly%250Aaccelerates%2520training%252C%2520with%2520the%2520optimization%2520of%2520our%25203D%2520control%2520points%2520achievable%250Awithin%2520just%25202%2520seconds%2520per%2520frame%2520on%2520a%2520single%2520NVIDIA%25204070%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S4D%3A%20Streaming%204D%20Real-World%20Reconstruction%20with%20Gaussians%20and%203D%0A%20%20Control%20Points&entry.906535625=Bing%20He%20and%20Yunuo%20Chen%20and%20Guo%20Lu%20and%20Li%20Song%20and%20Wenjun%20Zhang&entry.1292438233=%20%20Recently%2C%20the%20dynamic%20scene%20reconstruction%20using%20Gaussians%20has%20garnered%0Aincreased%20interest.%20Mainstream%20approaches%20typically%20employ%20a%20global%20deformation%0Afield%20to%20warp%20a%203D%20scene%20in%20the%20canonical%20space.%20However%2C%20the%20inherently%0Alow-frequency%20nature%20of%20implicit%20neural%20fields%20often%20leads%20to%20ineffective%0Arepresentations%20of%20complex%20motions.%20Moreover%2C%20their%20structural%20rigidity%20can%0Ahinder%20adaptation%20to%20scenes%20with%20varying%20resolutions%20and%20durations.%20To%20overcome%0Athese%20challenges%2C%20we%20introduce%20a%20novel%20approach%20utilizing%20discrete%203D%20control%0Apoints.%20This%20method%20models%20local%20rays%20physically%20and%20establishes%20a%0Amotion-decoupling%20coordinate%20system%2C%20which%20effectively%20merges%20traditional%0Agraphics%20with%20learnable%20pipelines%20for%20a%20robust%20and%20efficient%20local%0A6-degrees-of-freedom%20%286-DoF%29%20motion%20representation.%20Additionally%2C%20we%20have%0Adeveloped%20a%20generalized%20framework%20that%20incorporates%20our%20control%20points%20with%0AGaussians.%20Starting%20from%20an%20initial%203D%20reconstruction%2C%20our%20workflow%20decomposes%0Athe%20streaming%204D%20real-world%20reconstruction%20into%20four%20independent%20submodules%3A%203D%0Asegmentation%2C%203D%20control%20points%20generation%2C%20object-wise%20motion%20manipulation%2C%0Aand%20residual%20compensation.%20Our%20experiments%20demonstrate%20that%20this%20method%0Aoutperforms%20existing%20state-of-the-art%204D%20Gaussian%20Splatting%20techniques%20on%20both%0Athe%20Neu3DV%20and%20CMU-Panoptic%20datasets.%20Our%20approach%20also%20significantly%0Aaccelerates%20training%2C%20with%20the%20optimization%20of%20our%203D%20control%20points%20achievable%0Awithin%20just%202%20seconds%20per%20frame%20on%20a%20single%20NVIDIA%204070%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13036v1&entry.124074799=Read"},
{"title": "G3FA: Geometry-guided GAN for Face Animation", "author": "Alireza Javanmardi and Alain Pagani and Didier Stricker", "abstract": "  Animating human face images aims to synthesize a desired source identity in a\nnatural-looking way mimicking a driving video's facial movements. In this\ncontext, Generative Adversarial Networks have demonstrated remarkable potential\nin real-time face reenactment using a single source image, yet are constrained\nby limited geometry consistency compared to graphic-based approaches. In this\npaper, we introduce Geometry-guided GAN for Face Animation (G3FA) to tackle\nthis limitation. Our novel approach empowers the face animation model to\nincorporate 3D information using only 2D images, improving the image generation\ncapabilities of the talking head synthesis model. We integrate inverse\nrendering techniques to extract 3D facial geometry properties, improving the\nfeedback loop to the generator through a weighted average ensemble of\ndiscriminators. In our face reenactment model, we leverage 2D motion warping to\ncapture motion dynamics along with orthogonal ray sampling and volume rendering\ntechniques to produce the ultimate visual output. To evaluate the performance\nof our G3FA, we conducted comprehensive experiments using various evaluation\nprotocols on VoxCeleb2 and TalkingHead benchmarks to demonstrate the\neffectiveness of our proposed framework compared to the state-of-the-art\nreal-time face animation methods.\n", "link": "http://arxiv.org/abs/2408.13049v1", "date": "2024-08-23", "relevancy": 3.2231, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6667}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6667}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G3FA%3A%20Geometry-guided%20GAN%20for%20Face%20Animation&body=Title%3A%20G3FA%3A%20Geometry-guided%20GAN%20for%20Face%20Animation%0AAuthor%3A%20Alireza%20Javanmardi%20and%20Alain%20Pagani%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20Animating%20human%20face%20images%20aims%20to%20synthesize%20a%20desired%20source%20identity%20in%20a%0Anatural-looking%20way%20mimicking%20a%20driving%20video%27s%20facial%20movements.%20In%20this%0Acontext%2C%20Generative%20Adversarial%20Networks%20have%20demonstrated%20remarkable%20potential%0Ain%20real-time%20face%20reenactment%20using%20a%20single%20source%20image%2C%20yet%20are%20constrained%0Aby%20limited%20geometry%20consistency%20compared%20to%20graphic-based%20approaches.%20In%20this%0Apaper%2C%20we%20introduce%20Geometry-guided%20GAN%20for%20Face%20Animation%20%28G3FA%29%20to%20tackle%0Athis%20limitation.%20Our%20novel%20approach%20empowers%20the%20face%20animation%20model%20to%0Aincorporate%203D%20information%20using%20only%202D%20images%2C%20improving%20the%20image%20generation%0Acapabilities%20of%20the%20talking%20head%20synthesis%20model.%20We%20integrate%20inverse%0Arendering%20techniques%20to%20extract%203D%20facial%20geometry%20properties%2C%20improving%20the%0Afeedback%20loop%20to%20the%20generator%20through%20a%20weighted%20average%20ensemble%20of%0Adiscriminators.%20In%20our%20face%20reenactment%20model%2C%20we%20leverage%202D%20motion%20warping%20to%0Acapture%20motion%20dynamics%20along%20with%20orthogonal%20ray%20sampling%20and%20volume%20rendering%0Atechniques%20to%20produce%20the%20ultimate%20visual%20output.%20To%20evaluate%20the%20performance%0Aof%20our%20G3FA%2C%20we%20conducted%20comprehensive%20experiments%20using%20various%20evaluation%0Aprotocols%20on%20VoxCeleb2%20and%20TalkingHead%20benchmarks%20to%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20framework%20compared%20to%20the%20state-of-the-art%0Areal-time%20face%20animation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG3FA%253A%2520Geometry-guided%2520GAN%2520for%2520Face%2520Animation%26entry.906535625%3DAlireza%2520Javanmardi%2520and%2520Alain%2520Pagani%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520Animating%2520human%2520face%2520images%2520aims%2520to%2520synthesize%2520a%2520desired%2520source%2520identity%2520in%2520a%250Anatural-looking%2520way%2520mimicking%2520a%2520driving%2520video%2527s%2520facial%2520movements.%2520In%2520this%250Acontext%252C%2520Generative%2520Adversarial%2520Networks%2520have%2520demonstrated%2520remarkable%2520potential%250Ain%2520real-time%2520face%2520reenactment%2520using%2520a%2520single%2520source%2520image%252C%2520yet%2520are%2520constrained%250Aby%2520limited%2520geometry%2520consistency%2520compared%2520to%2520graphic-based%2520approaches.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520Geometry-guided%2520GAN%2520for%2520Face%2520Animation%2520%2528G3FA%2529%2520to%2520tackle%250Athis%2520limitation.%2520Our%2520novel%2520approach%2520empowers%2520the%2520face%2520animation%2520model%2520to%250Aincorporate%25203D%2520information%2520using%2520only%25202D%2520images%252C%2520improving%2520the%2520image%2520generation%250Acapabilities%2520of%2520the%2520talking%2520head%2520synthesis%2520model.%2520We%2520integrate%2520inverse%250Arendering%2520techniques%2520to%2520extract%25203D%2520facial%2520geometry%2520properties%252C%2520improving%2520the%250Afeedback%2520loop%2520to%2520the%2520generator%2520through%2520a%2520weighted%2520average%2520ensemble%2520of%250Adiscriminators.%2520In%2520our%2520face%2520reenactment%2520model%252C%2520we%2520leverage%25202D%2520motion%2520warping%2520to%250Acapture%2520motion%2520dynamics%2520along%2520with%2520orthogonal%2520ray%2520sampling%2520and%2520volume%2520rendering%250Atechniques%2520to%2520produce%2520the%2520ultimate%2520visual%2520output.%2520To%2520evaluate%2520the%2520performance%250Aof%2520our%2520G3FA%252C%2520we%2520conducted%2520comprehensive%2520experiments%2520using%2520various%2520evaluation%250Aprotocols%2520on%2520VoxCeleb2%2520and%2520TalkingHead%2520benchmarks%2520to%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520framework%2520compared%2520to%2520the%2520state-of-the-art%250Areal-time%2520face%2520animation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G3FA%3A%20Geometry-guided%20GAN%20for%20Face%20Animation&entry.906535625=Alireza%20Javanmardi%20and%20Alain%20Pagani%20and%20Didier%20Stricker&entry.1292438233=%20%20Animating%20human%20face%20images%20aims%20to%20synthesize%20a%20desired%20source%20identity%20in%20a%0Anatural-looking%20way%20mimicking%20a%20driving%20video%27s%20facial%20movements.%20In%20this%0Acontext%2C%20Generative%20Adversarial%20Networks%20have%20demonstrated%20remarkable%20potential%0Ain%20real-time%20face%20reenactment%20using%20a%20single%20source%20image%2C%20yet%20are%20constrained%0Aby%20limited%20geometry%20consistency%20compared%20to%20graphic-based%20approaches.%20In%20this%0Apaper%2C%20we%20introduce%20Geometry-guided%20GAN%20for%20Face%20Animation%20%28G3FA%29%20to%20tackle%0Athis%20limitation.%20Our%20novel%20approach%20empowers%20the%20face%20animation%20model%20to%0Aincorporate%203D%20information%20using%20only%202D%20images%2C%20improving%20the%20image%20generation%0Acapabilities%20of%20the%20talking%20head%20synthesis%20model.%20We%20integrate%20inverse%0Arendering%20techniques%20to%20extract%203D%20facial%20geometry%20properties%2C%20improving%20the%0Afeedback%20loop%20to%20the%20generator%20through%20a%20weighted%20average%20ensemble%20of%0Adiscriminators.%20In%20our%20face%20reenactment%20model%2C%20we%20leverage%202D%20motion%20warping%20to%0Acapture%20motion%20dynamics%20along%20with%20orthogonal%20ray%20sampling%20and%20volume%20rendering%0Atechniques%20to%20produce%20the%20ultimate%20visual%20output.%20To%20evaluate%20the%20performance%0Aof%20our%20G3FA%2C%20we%20conducted%20comprehensive%20experiments%20using%20various%20evaluation%0Aprotocols%20on%20VoxCeleb2%20and%20TalkingHead%20benchmarks%20to%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20framework%20compared%20to%20the%20state-of-the-art%0Areal-time%20face%20animation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13049v1&entry.124074799=Read"},
{"title": "Focus on Neighbors and Know the Whole: Towards Consistent Dense\n  Multiview Text-to-Image Generator for 3D Creation", "author": "Bonan Li and Zicheng Zhang and Xingyi Yang and Xinchao Wang", "abstract": "  Generating dense multiview images from text prompts is crucial for creating\nhigh-fidelity 3D assets. Nevertheless, existing methods struggle with\nspace-view correspondences, resulting in sparse and low-quality outputs. In\nthis paper, we introduce CoSER, a novel consistent dense Multiview\nText-to-Image Generator for Text-to-3D, achieving both efficiency and quality\nby meticulously learning neighbor-view coherence and further alleviating\nambiguity through the swift traversal of all views. For achieving neighbor-view\nconsistency, each viewpoint densely interacts with adjacent viewpoints to\nperceive the global spatial structure, and aggregates information along motion\npaths explicitly defined by physical principles to refine details. To further\nenhance cross-view consistency and alleviate content drift, CoSER rapidly scan\nall views in spiral bidirectional manner to aware holistic information and then\nscores each point based on semantic material. Subsequently, we conduct weighted\ndown-sampling along the spatial dimension based on scores, thereby facilitating\nprominent information fusion across all views with lightweight computation.\nTechnically, the core module is built by integrating the attention mechanism\nwith a selective state space model, exploiting the robust learning capabilities\nof the former and the low overhead of the latter. Extensive evaluation shows\nthat CoSER is capable of producing dense, high-fidelity, content-consistent\nmultiview images that can be flexibly integrated into various 3D generation\nmodels.\n", "link": "http://arxiv.org/abs/2408.13149v1", "date": "2024-08-23", "relevancy": 3.2102, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.662}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.662}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focus%20on%20Neighbors%20and%20Know%20the%20Whole%3A%20Towards%20Consistent%20Dense%0A%20%20Multiview%20Text-to-Image%20Generator%20for%203D%20Creation&body=Title%3A%20Focus%20on%20Neighbors%20and%20Know%20the%20Whole%3A%20Towards%20Consistent%20Dense%0A%20%20Multiview%20Text-to-Image%20Generator%20for%203D%20Creation%0AAuthor%3A%20Bonan%20Li%20and%20Zicheng%20Zhang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Generating%20dense%20multiview%20images%20from%20text%20prompts%20is%20crucial%20for%20creating%0Ahigh-fidelity%203D%20assets.%20Nevertheless%2C%20existing%20methods%20struggle%20with%0Aspace-view%20correspondences%2C%20resulting%20in%20sparse%20and%20low-quality%20outputs.%20In%0Athis%20paper%2C%20we%20introduce%20CoSER%2C%20a%20novel%20consistent%20dense%20Multiview%0AText-to-Image%20Generator%20for%20Text-to-3D%2C%20achieving%20both%20efficiency%20and%20quality%0Aby%20meticulously%20learning%20neighbor-view%20coherence%20and%20further%20alleviating%0Aambiguity%20through%20the%20swift%20traversal%20of%20all%20views.%20For%20achieving%20neighbor-view%0Aconsistency%2C%20each%20viewpoint%20densely%20interacts%20with%20adjacent%20viewpoints%20to%0Aperceive%20the%20global%20spatial%20structure%2C%20and%20aggregates%20information%20along%20motion%0Apaths%20explicitly%20defined%20by%20physical%20principles%20to%20refine%20details.%20To%20further%0Aenhance%20cross-view%20consistency%20and%20alleviate%20content%20drift%2C%20CoSER%20rapidly%20scan%0Aall%20views%20in%20spiral%20bidirectional%20manner%20to%20aware%20holistic%20information%20and%20then%0Ascores%20each%20point%20based%20on%20semantic%20material.%20Subsequently%2C%20we%20conduct%20weighted%0Adown-sampling%20along%20the%20spatial%20dimension%20based%20on%20scores%2C%20thereby%20facilitating%0Aprominent%20information%20fusion%20across%20all%20views%20with%20lightweight%20computation.%0ATechnically%2C%20the%20core%20module%20is%20built%20by%20integrating%20the%20attention%20mechanism%0Awith%20a%20selective%20state%20space%20model%2C%20exploiting%20the%20robust%20learning%20capabilities%0Aof%20the%20former%20and%20the%20low%20overhead%20of%20the%20latter.%20Extensive%20evaluation%20shows%0Athat%20CoSER%20is%20capable%20of%20producing%20dense%2C%20high-fidelity%2C%20content-consistent%0Amultiview%20images%20that%20can%20be%20flexibly%20integrated%20into%20various%203D%20generation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocus%2520on%2520Neighbors%2520and%2520Know%2520the%2520Whole%253A%2520Towards%2520Consistent%2520Dense%250A%2520%2520Multiview%2520Text-to-Image%2520Generator%2520for%25203D%2520Creation%26entry.906535625%3DBonan%2520Li%2520and%2520Zicheng%2520Zhang%2520and%2520Xingyi%2520Yang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Generating%2520dense%2520multiview%2520images%2520from%2520text%2520prompts%2520is%2520crucial%2520for%2520creating%250Ahigh-fidelity%25203D%2520assets.%2520Nevertheless%252C%2520existing%2520methods%2520struggle%2520with%250Aspace-view%2520correspondences%252C%2520resulting%2520in%2520sparse%2520and%2520low-quality%2520outputs.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520CoSER%252C%2520a%2520novel%2520consistent%2520dense%2520Multiview%250AText-to-Image%2520Generator%2520for%2520Text-to-3D%252C%2520achieving%2520both%2520efficiency%2520and%2520quality%250Aby%2520meticulously%2520learning%2520neighbor-view%2520coherence%2520and%2520further%2520alleviating%250Aambiguity%2520through%2520the%2520swift%2520traversal%2520of%2520all%2520views.%2520For%2520achieving%2520neighbor-view%250Aconsistency%252C%2520each%2520viewpoint%2520densely%2520interacts%2520with%2520adjacent%2520viewpoints%2520to%250Aperceive%2520the%2520global%2520spatial%2520structure%252C%2520and%2520aggregates%2520information%2520along%2520motion%250Apaths%2520explicitly%2520defined%2520by%2520physical%2520principles%2520to%2520refine%2520details.%2520To%2520further%250Aenhance%2520cross-view%2520consistency%2520and%2520alleviate%2520content%2520drift%252C%2520CoSER%2520rapidly%2520scan%250Aall%2520views%2520in%2520spiral%2520bidirectional%2520manner%2520to%2520aware%2520holistic%2520information%2520and%2520then%250Ascores%2520each%2520point%2520based%2520on%2520semantic%2520material.%2520Subsequently%252C%2520we%2520conduct%2520weighted%250Adown-sampling%2520along%2520the%2520spatial%2520dimension%2520based%2520on%2520scores%252C%2520thereby%2520facilitating%250Aprominent%2520information%2520fusion%2520across%2520all%2520views%2520with%2520lightweight%2520computation.%250ATechnically%252C%2520the%2520core%2520module%2520is%2520built%2520by%2520integrating%2520the%2520attention%2520mechanism%250Awith%2520a%2520selective%2520state%2520space%2520model%252C%2520exploiting%2520the%2520robust%2520learning%2520capabilities%250Aof%2520the%2520former%2520and%2520the%2520low%2520overhead%2520of%2520the%2520latter.%2520Extensive%2520evaluation%2520shows%250Athat%2520CoSER%2520is%2520capable%2520of%2520producing%2520dense%252C%2520high-fidelity%252C%2520content-consistent%250Amultiview%2520images%2520that%2520can%2520be%2520flexibly%2520integrated%2520into%2520various%25203D%2520generation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focus%20on%20Neighbors%20and%20Know%20the%20Whole%3A%20Towards%20Consistent%20Dense%0A%20%20Multiview%20Text-to-Image%20Generator%20for%203D%20Creation&entry.906535625=Bonan%20Li%20and%20Zicheng%20Zhang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang&entry.1292438233=%20%20Generating%20dense%20multiview%20images%20from%20text%20prompts%20is%20crucial%20for%20creating%0Ahigh-fidelity%203D%20assets.%20Nevertheless%2C%20existing%20methods%20struggle%20with%0Aspace-view%20correspondences%2C%20resulting%20in%20sparse%20and%20low-quality%20outputs.%20In%0Athis%20paper%2C%20we%20introduce%20CoSER%2C%20a%20novel%20consistent%20dense%20Multiview%0AText-to-Image%20Generator%20for%20Text-to-3D%2C%20achieving%20both%20efficiency%20and%20quality%0Aby%20meticulously%20learning%20neighbor-view%20coherence%20and%20further%20alleviating%0Aambiguity%20through%20the%20swift%20traversal%20of%20all%20views.%20For%20achieving%20neighbor-view%0Aconsistency%2C%20each%20viewpoint%20densely%20interacts%20with%20adjacent%20viewpoints%20to%0Aperceive%20the%20global%20spatial%20structure%2C%20and%20aggregates%20information%20along%20motion%0Apaths%20explicitly%20defined%20by%20physical%20principles%20to%20refine%20details.%20To%20further%0Aenhance%20cross-view%20consistency%20and%20alleviate%20content%20drift%2C%20CoSER%20rapidly%20scan%0Aall%20views%20in%20spiral%20bidirectional%20manner%20to%20aware%20holistic%20information%20and%20then%0Ascores%20each%20point%20based%20on%20semantic%20material.%20Subsequently%2C%20we%20conduct%20weighted%0Adown-sampling%20along%20the%20spatial%20dimension%20based%20on%20scores%2C%20thereby%20facilitating%0Aprominent%20information%20fusion%20across%20all%20views%20with%20lightweight%20computation.%0ATechnically%2C%20the%20core%20module%20is%20built%20by%20integrating%20the%20attention%20mechanism%0Awith%20a%20selective%20state%20space%20model%2C%20exploiting%20the%20robust%20learning%20capabilities%0Aof%20the%20former%20and%20the%20low%20overhead%20of%20the%20latter.%20Extensive%20evaluation%20shows%0Athat%20CoSER%20is%20capable%20of%20producing%20dense%2C%20high-fidelity%2C%20content-consistent%0Amultiview%20images%20that%20can%20be%20flexibly%20integrated%20into%20various%203D%20generation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13149v1&entry.124074799=Read"},
{"title": "Map-Free Visual Relocalization Enhanced by Instance Knowledge and Depth\n  Knowledge", "author": "Mingyu Xiao and Runze Chen and Haiyong Luo and Fang Zhao and Juan Wang and Xuepeng Ma", "abstract": "  Map-free relocalization technology is crucial for applications in autonomous\nnavigation and augmented reality, but relying on pre-built maps is often\nimpractical. It faces significant challenges due to limitations in matching\nmethods and the inherent lack of scale in monocular images. These issues lead\nto substantial rotational and metric errors and even localization failures in\nreal-world scenarios. Large matching errors significantly impact the overall\nrelocalization process, affecting both rotational and translational accuracy.\nDue to the inherent limitations of the camera itself, recovering the metric\nscale from a single image is crucial, as this significantly impacts the\ntranslation error. To address these challenges, we propose a map-free\nrelocalization method enhanced by instance knowledge and depth knowledge. By\nleveraging instance-based matching information to improve global matching\nresults, our method significantly reduces the possibility of mismatching across\ndifferent objects. The robustness of instance knowledge across the scene helps\nthe feature point matching model focus on relevant regions and enhance matching\naccuracy. Additionally, we use estimated metric depth from a single image to\nreduce metric errors and improve scale recovery accuracy. By integrating\nmethods dedicated to mitigating large translational and rotational errors, our\napproach demonstrates superior performance in map-free relocalization\ntechniques.\n", "link": "http://arxiv.org/abs/2408.13085v1", "date": "2024-08-23", "relevancy": 3.0697, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6544}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.634}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Map-Free%20Visual%20Relocalization%20Enhanced%20by%20Instance%20Knowledge%20and%20Depth%0A%20%20Knowledge&body=Title%3A%20Map-Free%20Visual%20Relocalization%20Enhanced%20by%20Instance%20Knowledge%20and%20Depth%0A%20%20Knowledge%0AAuthor%3A%20Mingyu%20Xiao%20and%20Runze%20Chen%20and%20Haiyong%20Luo%20and%20Fang%20Zhao%20and%20Juan%20Wang%20and%20Xuepeng%20Ma%0AAbstract%3A%20%20%20Map-free%20relocalization%20technology%20is%20crucial%20for%20applications%20in%20autonomous%0Anavigation%20and%20augmented%20reality%2C%20but%20relying%20on%20pre-built%20maps%20is%20often%0Aimpractical.%20It%20faces%20significant%20challenges%20due%20to%20limitations%20in%20matching%0Amethods%20and%20the%20inherent%20lack%20of%20scale%20in%20monocular%20images.%20These%20issues%20lead%0Ato%20substantial%20rotational%20and%20metric%20errors%20and%20even%20localization%20failures%20in%0Areal-world%20scenarios.%20Large%20matching%20errors%20significantly%20impact%20the%20overall%0Arelocalization%20process%2C%20affecting%20both%20rotational%20and%20translational%20accuracy.%0ADue%20to%20the%20inherent%20limitations%20of%20the%20camera%20itself%2C%20recovering%20the%20metric%0Ascale%20from%20a%20single%20image%20is%20crucial%2C%20as%20this%20significantly%20impacts%20the%0Atranslation%20error.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20map-free%0Arelocalization%20method%20enhanced%20by%20instance%20knowledge%20and%20depth%20knowledge.%20By%0Aleveraging%20instance-based%20matching%20information%20to%20improve%20global%20matching%0Aresults%2C%20our%20method%20significantly%20reduces%20the%20possibility%20of%20mismatching%20across%0Adifferent%20objects.%20The%20robustness%20of%20instance%20knowledge%20across%20the%20scene%20helps%0Athe%20feature%20point%20matching%20model%20focus%20on%20relevant%20regions%20and%20enhance%20matching%0Aaccuracy.%20Additionally%2C%20we%20use%20estimated%20metric%20depth%20from%20a%20single%20image%20to%0Areduce%20metric%20errors%20and%20improve%20scale%20recovery%20accuracy.%20By%20integrating%0Amethods%20dedicated%20to%20mitigating%20large%20translational%20and%20rotational%20errors%2C%20our%0Aapproach%20demonstrates%20superior%20performance%20in%20map-free%20relocalization%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMap-Free%2520Visual%2520Relocalization%2520Enhanced%2520by%2520Instance%2520Knowledge%2520and%2520Depth%250A%2520%2520Knowledge%26entry.906535625%3DMingyu%2520Xiao%2520and%2520Runze%2520Chen%2520and%2520Haiyong%2520Luo%2520and%2520Fang%2520Zhao%2520and%2520Juan%2520Wang%2520and%2520Xuepeng%2520Ma%26entry.1292438233%3D%2520%2520Map-free%2520relocalization%2520technology%2520is%2520crucial%2520for%2520applications%2520in%2520autonomous%250Anavigation%2520and%2520augmented%2520reality%252C%2520but%2520relying%2520on%2520pre-built%2520maps%2520is%2520often%250Aimpractical.%2520It%2520faces%2520significant%2520challenges%2520due%2520to%2520limitations%2520in%2520matching%250Amethods%2520and%2520the%2520inherent%2520lack%2520of%2520scale%2520in%2520monocular%2520images.%2520These%2520issues%2520lead%250Ato%2520substantial%2520rotational%2520and%2520metric%2520errors%2520and%2520even%2520localization%2520failures%2520in%250Areal-world%2520scenarios.%2520Large%2520matching%2520errors%2520significantly%2520impact%2520the%2520overall%250Arelocalization%2520process%252C%2520affecting%2520both%2520rotational%2520and%2520translational%2520accuracy.%250ADue%2520to%2520the%2520inherent%2520limitations%2520of%2520the%2520camera%2520itself%252C%2520recovering%2520the%2520metric%250Ascale%2520from%2520a%2520single%2520image%2520is%2520crucial%252C%2520as%2520this%2520significantly%2520impacts%2520the%250Atranslation%2520error.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520map-free%250Arelocalization%2520method%2520enhanced%2520by%2520instance%2520knowledge%2520and%2520depth%2520knowledge.%2520By%250Aleveraging%2520instance-based%2520matching%2520information%2520to%2520improve%2520global%2520matching%250Aresults%252C%2520our%2520method%2520significantly%2520reduces%2520the%2520possibility%2520of%2520mismatching%2520across%250Adifferent%2520objects.%2520The%2520robustness%2520of%2520instance%2520knowledge%2520across%2520the%2520scene%2520helps%250Athe%2520feature%2520point%2520matching%2520model%2520focus%2520on%2520relevant%2520regions%2520and%2520enhance%2520matching%250Aaccuracy.%2520Additionally%252C%2520we%2520use%2520estimated%2520metric%2520depth%2520from%2520a%2520single%2520image%2520to%250Areduce%2520metric%2520errors%2520and%2520improve%2520scale%2520recovery%2520accuracy.%2520By%2520integrating%250Amethods%2520dedicated%2520to%2520mitigating%2520large%2520translational%2520and%2520rotational%2520errors%252C%2520our%250Aapproach%2520demonstrates%2520superior%2520performance%2520in%2520map-free%2520relocalization%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Map-Free%20Visual%20Relocalization%20Enhanced%20by%20Instance%20Knowledge%20and%20Depth%0A%20%20Knowledge&entry.906535625=Mingyu%20Xiao%20and%20Runze%20Chen%20and%20Haiyong%20Luo%20and%20Fang%20Zhao%20and%20Juan%20Wang%20and%20Xuepeng%20Ma&entry.1292438233=%20%20Map-free%20relocalization%20technology%20is%20crucial%20for%20applications%20in%20autonomous%0Anavigation%20and%20augmented%20reality%2C%20but%20relying%20on%20pre-built%20maps%20is%20often%0Aimpractical.%20It%20faces%20significant%20challenges%20due%20to%20limitations%20in%20matching%0Amethods%20and%20the%20inherent%20lack%20of%20scale%20in%20monocular%20images.%20These%20issues%20lead%0Ato%20substantial%20rotational%20and%20metric%20errors%20and%20even%20localization%20failures%20in%0Areal-world%20scenarios.%20Large%20matching%20errors%20significantly%20impact%20the%20overall%0Arelocalization%20process%2C%20affecting%20both%20rotational%20and%20translational%20accuracy.%0ADue%20to%20the%20inherent%20limitations%20of%20the%20camera%20itself%2C%20recovering%20the%20metric%0Ascale%20from%20a%20single%20image%20is%20crucial%2C%20as%20this%20significantly%20impacts%20the%0Atranslation%20error.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20map-free%0Arelocalization%20method%20enhanced%20by%20instance%20knowledge%20and%20depth%20knowledge.%20By%0Aleveraging%20instance-based%20matching%20information%20to%20improve%20global%20matching%0Aresults%2C%20our%20method%20significantly%20reduces%20the%20possibility%20of%20mismatching%20across%0Adifferent%20objects.%20The%20robustness%20of%20instance%20knowledge%20across%20the%20scene%20helps%0Athe%20feature%20point%20matching%20model%20focus%20on%20relevant%20regions%20and%20enhance%20matching%0Aaccuracy.%20Additionally%2C%20we%20use%20estimated%20metric%20depth%20from%20a%20single%20image%20to%0Areduce%20metric%20errors%20and%20improve%20scale%20recovery%20accuracy.%20By%20integrating%0Amethods%20dedicated%20to%20mitigating%20large%20translational%20and%20rotational%20errors%2C%20our%0Aapproach%20demonstrates%20superior%20performance%20in%20map-free%20relocalization%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13085v1&entry.124074799=Read"},
{"title": "Atlas Gaussians Diffusion for 3D Generation with Infinite Number of\n  Points", "author": "Haitao Yang and Yuan Dong and Hanwen Jiang and Dejia Xu and Georgios Pavlakos and Qixing Huang", "abstract": "  Using the latent diffusion model has proven effective in developing novel 3D\ngeneration techniques. To harness the latent diffusion model, a key challenge\nis designing a high-fidelity and efficient representation that links the latent\nspace and the 3D space. In this paper, we introduce Atlas Gaussians, a novel\nrepresentation for feed-forward native 3D generation. Atlas Gaussians represent\na shape as the union of local patches, and each patch can decode 3D Gaussians.\nWe parameterize a patch as a sequence of feature vectors and design a learnable\nfunction to decode 3D Gaussians from the feature vectors. In this process, we\nincorporate UV-based sampling, enabling the generation of a sufficiently large,\nand theoretically infinite, number of 3D Gaussian points. The large amount of\n3D Gaussians enables high-quality details of generation results. Moreover, due\nto local awareness of the representation, the transformer-based decoding\nprocedure operates on a patch level, ensuring efficiency. We train a\nvariational autoencoder to learn the Atlas Gaussians representation, and then\napply a latent diffusion model on its latent space for learning 3D Generation.\nExperiments show that our approach outperforms the prior arts of feed-forward\nnative 3D generation.\n", "link": "http://arxiv.org/abs/2408.13055v1", "date": "2024-08-23", "relevancy": 3.0305, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6076}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6076}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atlas%20Gaussians%20Diffusion%20for%203D%20Generation%20with%20Infinite%20Number%20of%0A%20%20Points&body=Title%3A%20Atlas%20Gaussians%20Diffusion%20for%203D%20Generation%20with%20Infinite%20Number%20of%0A%20%20Points%0AAuthor%3A%20Haitao%20Yang%20and%20Yuan%20Dong%20and%20Hanwen%20Jiang%20and%20Dejia%20Xu%20and%20Georgios%20Pavlakos%20and%20Qixing%20Huang%0AAbstract%3A%20%20%20Using%20the%20latent%20diffusion%20model%20has%20proven%20effective%20in%20developing%20novel%203D%0Ageneration%20techniques.%20To%20harness%20the%20latent%20diffusion%20model%2C%20a%20key%20challenge%0Ais%20designing%20a%20high-fidelity%20and%20efficient%20representation%20that%20links%20the%20latent%0Aspace%20and%20the%203D%20space.%20In%20this%20paper%2C%20we%20introduce%20Atlas%20Gaussians%2C%20a%20novel%0Arepresentation%20for%20feed-forward%20native%203D%20generation.%20Atlas%20Gaussians%20represent%0Aa%20shape%20as%20the%20union%20of%20local%20patches%2C%20and%20each%20patch%20can%20decode%203D%20Gaussians.%0AWe%20parameterize%20a%20patch%20as%20a%20sequence%20of%20feature%20vectors%20and%20design%20a%20learnable%0Afunction%20to%20decode%203D%20Gaussians%20from%20the%20feature%20vectors.%20In%20this%20process%2C%20we%0Aincorporate%20UV-based%20sampling%2C%20enabling%20the%20generation%20of%20a%20sufficiently%20large%2C%0Aand%20theoretically%20infinite%2C%20number%20of%203D%20Gaussian%20points.%20The%20large%20amount%20of%0A3D%20Gaussians%20enables%20high-quality%20details%20of%20generation%20results.%20Moreover%2C%20due%0Ato%20local%20awareness%20of%20the%20representation%2C%20the%20transformer-based%20decoding%0Aprocedure%20operates%20on%20a%20patch%20level%2C%20ensuring%20efficiency.%20We%20train%20a%0Avariational%20autoencoder%20to%20learn%20the%20Atlas%20Gaussians%20representation%2C%20and%20then%0Aapply%20a%20latent%20diffusion%20model%20on%20its%20latent%20space%20for%20learning%203D%20Generation.%0AExperiments%20show%20that%20our%20approach%20outperforms%20the%20prior%20arts%20of%20feed-forward%0Anative%203D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtlas%2520Gaussians%2520Diffusion%2520for%25203D%2520Generation%2520with%2520Infinite%2520Number%2520of%250A%2520%2520Points%26entry.906535625%3DHaitao%2520Yang%2520and%2520Yuan%2520Dong%2520and%2520Hanwen%2520Jiang%2520and%2520Dejia%2520Xu%2520and%2520Georgios%2520Pavlakos%2520and%2520Qixing%2520Huang%26entry.1292438233%3D%2520%2520Using%2520the%2520latent%2520diffusion%2520model%2520has%2520proven%2520effective%2520in%2520developing%2520novel%25203D%250Ageneration%2520techniques.%2520To%2520harness%2520the%2520latent%2520diffusion%2520model%252C%2520a%2520key%2520challenge%250Ais%2520designing%2520a%2520high-fidelity%2520and%2520efficient%2520representation%2520that%2520links%2520the%2520latent%250Aspace%2520and%2520the%25203D%2520space.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Atlas%2520Gaussians%252C%2520a%2520novel%250Arepresentation%2520for%2520feed-forward%2520native%25203D%2520generation.%2520Atlas%2520Gaussians%2520represent%250Aa%2520shape%2520as%2520the%2520union%2520of%2520local%2520patches%252C%2520and%2520each%2520patch%2520can%2520decode%25203D%2520Gaussians.%250AWe%2520parameterize%2520a%2520patch%2520as%2520a%2520sequence%2520of%2520feature%2520vectors%2520and%2520design%2520a%2520learnable%250Afunction%2520to%2520decode%25203D%2520Gaussians%2520from%2520the%2520feature%2520vectors.%2520In%2520this%2520process%252C%2520we%250Aincorporate%2520UV-based%2520sampling%252C%2520enabling%2520the%2520generation%2520of%2520a%2520sufficiently%2520large%252C%250Aand%2520theoretically%2520infinite%252C%2520number%2520of%25203D%2520Gaussian%2520points.%2520The%2520large%2520amount%2520of%250A3D%2520Gaussians%2520enables%2520high-quality%2520details%2520of%2520generation%2520results.%2520Moreover%252C%2520due%250Ato%2520local%2520awareness%2520of%2520the%2520representation%252C%2520the%2520transformer-based%2520decoding%250Aprocedure%2520operates%2520on%2520a%2520patch%2520level%252C%2520ensuring%2520efficiency.%2520We%2520train%2520a%250Avariational%2520autoencoder%2520to%2520learn%2520the%2520Atlas%2520Gaussians%2520representation%252C%2520and%2520then%250Aapply%2520a%2520latent%2520diffusion%2520model%2520on%2520its%2520latent%2520space%2520for%2520learning%25203D%2520Generation.%250AExperiments%2520show%2520that%2520our%2520approach%2520outperforms%2520the%2520prior%2520arts%2520of%2520feed-forward%250Anative%25203D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atlas%20Gaussians%20Diffusion%20for%203D%20Generation%20with%20Infinite%20Number%20of%0A%20%20Points&entry.906535625=Haitao%20Yang%20and%20Yuan%20Dong%20and%20Hanwen%20Jiang%20and%20Dejia%20Xu%20and%20Georgios%20Pavlakos%20and%20Qixing%20Huang&entry.1292438233=%20%20Using%20the%20latent%20diffusion%20model%20has%20proven%20effective%20in%20developing%20novel%203D%0Ageneration%20techniques.%20To%20harness%20the%20latent%20diffusion%20model%2C%20a%20key%20challenge%0Ais%20designing%20a%20high-fidelity%20and%20efficient%20representation%20that%20links%20the%20latent%0Aspace%20and%20the%203D%20space.%20In%20this%20paper%2C%20we%20introduce%20Atlas%20Gaussians%2C%20a%20novel%0Arepresentation%20for%20feed-forward%20native%203D%20generation.%20Atlas%20Gaussians%20represent%0Aa%20shape%20as%20the%20union%20of%20local%20patches%2C%20and%20each%20patch%20can%20decode%203D%20Gaussians.%0AWe%20parameterize%20a%20patch%20as%20a%20sequence%20of%20feature%20vectors%20and%20design%20a%20learnable%0Afunction%20to%20decode%203D%20Gaussians%20from%20the%20feature%20vectors.%20In%20this%20process%2C%20we%0Aincorporate%20UV-based%20sampling%2C%20enabling%20the%20generation%20of%20a%20sufficiently%20large%2C%0Aand%20theoretically%20infinite%2C%20number%20of%203D%20Gaussian%20points.%20The%20large%20amount%20of%0A3D%20Gaussians%20enables%20high-quality%20details%20of%20generation%20results.%20Moreover%2C%20due%0Ato%20local%20awareness%20of%20the%20representation%2C%20the%20transformer-based%20decoding%0Aprocedure%20operates%20on%20a%20patch%20level%2C%20ensuring%20efficiency.%20We%20train%20a%0Avariational%20autoencoder%20to%20learn%20the%20Atlas%20Gaussians%20representation%2C%20and%20then%0Aapply%20a%20latent%20diffusion%20model%20on%20its%20latent%20space%20for%20learning%203D%20Generation.%0AExperiments%20show%20that%20our%20approach%20outperforms%20the%20prior%20arts%20of%20feed-forward%0Anative%203D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13055v1&entry.124074799=Read"},
{"title": "LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation", "author": "Shuai Yang and Jing Tan and Mengchen Zhang and Tong Wu and Yixuan Li and Gordon Wetzstein and Ziwei Liu and Dahua Lin", "abstract": "  3D immersive scene generation is a challenging yet critical task in computer\nvision and graphics. A desired virtual 3D scene should 1) exhibit\nomnidirectional view consistency, and 2) allow for free exploration in complex\nscene hierarchies. Existing methods either rely on successive scene expansion\nvia inpainting or employ panorama representation to represent large FOV scene\nenvironments. However, the generated scene suffers from semantic drift during\nexpansion and is unable to handle occlusion among scene hierarchies. To tackle\nthese challenges, we introduce LayerPano3D, a novel framework for full-view,\nexplorable panoramic 3D scene generation from a single text prompt. Our key\ninsight is to decompose a reference 2D panorama into multiple layers at\ndifferent depth levels, where each layer reveals the unseen space from the\nreference views via diffusion prior. LayerPano3D comprises multiple dedicated\ndesigns: 1) we introduce a novel text-guided anchor view synthesis pipeline for\nhigh-quality, consistent panorama generation. 2) We pioneer the Layered 3D\nPanorama as underlying representation to manage complex scene hierarchies and\nlift it into 3D Gaussians to splat detailed 360-degree omnidirectional scenes\nwith unconstrained viewing paths. Extensive experiments demonstrate that our\nframework generates state-of-the-art 3D panoramic scene in both full view\nconsistency and immersive exploratory experience. We believe that LayerPano3D\nholds promise for advancing 3D panoramic scene creation with numerous\napplications.\n", "link": "http://arxiv.org/abs/2408.13252v1", "date": "2024-08-23", "relevancy": 2.9489, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6154}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6154}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerPano3D%3A%20Layered%203D%20Panorama%20for%20Hyper-Immersive%20Scene%20Generation&body=Title%3A%20LayerPano3D%3A%20Layered%203D%20Panorama%20for%20Hyper-Immersive%20Scene%20Generation%0AAuthor%3A%20Shuai%20Yang%20and%20Jing%20Tan%20and%20Mengchen%20Zhang%20and%20Tong%20Wu%20and%20Yixuan%20Li%20and%20Gordon%20Wetzstein%20and%20Ziwei%20Liu%20and%20Dahua%20Lin%0AAbstract%3A%20%20%203D%20immersive%20scene%20generation%20is%20a%20challenging%20yet%20critical%20task%20in%20computer%0Avision%20and%20graphics.%20A%20desired%20virtual%203D%20scene%20should%201%29%20exhibit%0Aomnidirectional%20view%20consistency%2C%20and%202%29%20allow%20for%20free%20exploration%20in%20complex%0Ascene%20hierarchies.%20Existing%20methods%20either%20rely%20on%20successive%20scene%20expansion%0Avia%20inpainting%20or%20employ%20panorama%20representation%20to%20represent%20large%20FOV%20scene%0Aenvironments.%20However%2C%20the%20generated%20scene%20suffers%20from%20semantic%20drift%20during%0Aexpansion%20and%20is%20unable%20to%20handle%20occlusion%20among%20scene%20hierarchies.%20To%20tackle%0Athese%20challenges%2C%20we%20introduce%20LayerPano3D%2C%20a%20novel%20framework%20for%20full-view%2C%0Aexplorable%20panoramic%203D%20scene%20generation%20from%20a%20single%20text%20prompt.%20Our%20key%0Ainsight%20is%20to%20decompose%20a%20reference%202D%20panorama%20into%20multiple%20layers%20at%0Adifferent%20depth%20levels%2C%20where%20each%20layer%20reveals%20the%20unseen%20space%20from%20the%0Areference%20views%20via%20diffusion%20prior.%20LayerPano3D%20comprises%20multiple%20dedicated%0Adesigns%3A%201%29%20we%20introduce%20a%20novel%20text-guided%20anchor%20view%20synthesis%20pipeline%20for%0Ahigh-quality%2C%20consistent%20panorama%20generation.%202%29%20We%20pioneer%20the%20Layered%203D%0APanorama%20as%20underlying%20representation%20to%20manage%20complex%20scene%20hierarchies%20and%0Alift%20it%20into%203D%20Gaussians%20to%20splat%20detailed%20360-degree%20omnidirectional%20scenes%0Awith%20unconstrained%20viewing%20paths.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20generates%20state-of-the-art%203D%20panoramic%20scene%20in%20both%20full%20view%0Aconsistency%20and%20immersive%20exploratory%20experience.%20We%20believe%20that%20LayerPano3D%0Aholds%20promise%20for%20advancing%203D%20panoramic%20scene%20creation%20with%20numerous%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerPano3D%253A%2520Layered%25203D%2520Panorama%2520for%2520Hyper-Immersive%2520Scene%2520Generation%26entry.906535625%3DShuai%2520Yang%2520and%2520Jing%2520Tan%2520and%2520Mengchen%2520Zhang%2520and%2520Tong%2520Wu%2520and%2520Yixuan%2520Li%2520and%2520Gordon%2520Wetzstein%2520and%2520Ziwei%2520Liu%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%25203D%2520immersive%2520scene%2520generation%2520is%2520a%2520challenging%2520yet%2520critical%2520task%2520in%2520computer%250Avision%2520and%2520graphics.%2520A%2520desired%2520virtual%25203D%2520scene%2520should%25201%2529%2520exhibit%250Aomnidirectional%2520view%2520consistency%252C%2520and%25202%2529%2520allow%2520for%2520free%2520exploration%2520in%2520complex%250Ascene%2520hierarchies.%2520Existing%2520methods%2520either%2520rely%2520on%2520successive%2520scene%2520expansion%250Avia%2520inpainting%2520or%2520employ%2520panorama%2520representation%2520to%2520represent%2520large%2520FOV%2520scene%250Aenvironments.%2520However%252C%2520the%2520generated%2520scene%2520suffers%2520from%2520semantic%2520drift%2520during%250Aexpansion%2520and%2520is%2520unable%2520to%2520handle%2520occlusion%2520among%2520scene%2520hierarchies.%2520To%2520tackle%250Athese%2520challenges%252C%2520we%2520introduce%2520LayerPano3D%252C%2520a%2520novel%2520framework%2520for%2520full-view%252C%250Aexplorable%2520panoramic%25203D%2520scene%2520generation%2520from%2520a%2520single%2520text%2520prompt.%2520Our%2520key%250Ainsight%2520is%2520to%2520decompose%2520a%2520reference%25202D%2520panorama%2520into%2520multiple%2520layers%2520at%250Adifferent%2520depth%2520levels%252C%2520where%2520each%2520layer%2520reveals%2520the%2520unseen%2520space%2520from%2520the%250Areference%2520views%2520via%2520diffusion%2520prior.%2520LayerPano3D%2520comprises%2520multiple%2520dedicated%250Adesigns%253A%25201%2529%2520we%2520introduce%2520a%2520novel%2520text-guided%2520anchor%2520view%2520synthesis%2520pipeline%2520for%250Ahigh-quality%252C%2520consistent%2520panorama%2520generation.%25202%2529%2520We%2520pioneer%2520the%2520Layered%25203D%250APanorama%2520as%2520underlying%2520representation%2520to%2520manage%2520complex%2520scene%2520hierarchies%2520and%250Alift%2520it%2520into%25203D%2520Gaussians%2520to%2520splat%2520detailed%2520360-degree%2520omnidirectional%2520scenes%250Awith%2520unconstrained%2520viewing%2520paths.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aframework%2520generates%2520state-of-the-art%25203D%2520panoramic%2520scene%2520in%2520both%2520full%2520view%250Aconsistency%2520and%2520immersive%2520exploratory%2520experience.%2520We%2520believe%2520that%2520LayerPano3D%250Aholds%2520promise%2520for%2520advancing%25203D%2520panoramic%2520scene%2520creation%2520with%2520numerous%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerPano3D%3A%20Layered%203D%20Panorama%20for%20Hyper-Immersive%20Scene%20Generation&entry.906535625=Shuai%20Yang%20and%20Jing%20Tan%20and%20Mengchen%20Zhang%20and%20Tong%20Wu%20and%20Yixuan%20Li%20and%20Gordon%20Wetzstein%20and%20Ziwei%20Liu%20and%20Dahua%20Lin&entry.1292438233=%20%203D%20immersive%20scene%20generation%20is%20a%20challenging%20yet%20critical%20task%20in%20computer%0Avision%20and%20graphics.%20A%20desired%20virtual%203D%20scene%20should%201%29%20exhibit%0Aomnidirectional%20view%20consistency%2C%20and%202%29%20allow%20for%20free%20exploration%20in%20complex%0Ascene%20hierarchies.%20Existing%20methods%20either%20rely%20on%20successive%20scene%20expansion%0Avia%20inpainting%20or%20employ%20panorama%20representation%20to%20represent%20large%20FOV%20scene%0Aenvironments.%20However%2C%20the%20generated%20scene%20suffers%20from%20semantic%20drift%20during%0Aexpansion%20and%20is%20unable%20to%20handle%20occlusion%20among%20scene%20hierarchies.%20To%20tackle%0Athese%20challenges%2C%20we%20introduce%20LayerPano3D%2C%20a%20novel%20framework%20for%20full-view%2C%0Aexplorable%20panoramic%203D%20scene%20generation%20from%20a%20single%20text%20prompt.%20Our%20key%0Ainsight%20is%20to%20decompose%20a%20reference%202D%20panorama%20into%20multiple%20layers%20at%0Adifferent%20depth%20levels%2C%20where%20each%20layer%20reveals%20the%20unseen%20space%20from%20the%0Areference%20views%20via%20diffusion%20prior.%20LayerPano3D%20comprises%20multiple%20dedicated%0Adesigns%3A%201%29%20we%20introduce%20a%20novel%20text-guided%20anchor%20view%20synthesis%20pipeline%20for%0Ahigh-quality%2C%20consistent%20panorama%20generation.%202%29%20We%20pioneer%20the%20Layered%203D%0APanorama%20as%20underlying%20representation%20to%20manage%20complex%20scene%20hierarchies%20and%0Alift%20it%20into%203D%20Gaussians%20to%20splat%20detailed%20360-degree%20omnidirectional%20scenes%0Awith%20unconstrained%20viewing%20paths.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20generates%20state-of-the-art%203D%20panoramic%20scene%20in%20both%20full%20view%0Aconsistency%20and%20immersive%20exploratory%20experience.%20We%20believe%20that%20LayerPano3D%0Aholds%20promise%20for%20advancing%203D%20panoramic%20scene%20creation%20with%20numerous%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13252v1&entry.124074799=Read"},
{"title": "ShapeICP: Iterative Category-level Object Pose and Shape Estimation from\n  Depth", "author": "Yihao Zhang and John J. Leonard", "abstract": "  Category-level object pose and shape estimation from a single depth image has\nrecently drawn research attention due to its wide applications in robotics and\nself-driving. The task is particularly challenging because the three unknowns,\nobject pose, object shape, and model-to-measurement correspondences, are\ncompounded together but only a single view of depth measurements is provided.\nThe vast majority of the prior work heavily relies on data-driven approaches to\nobtain solutions to at least one of the unknowns and typically two, running\nwith the risk of failing to generalize to unseen domains. The shape\nrepresentations used in the prior work also mainly focus on point cloud and\nsigned distance field (SDF). In stark contrast to the prior work, we approach\nthe problem using an iterative estimation method that does not require learning\nfrom any pose-annotated data. In addition, we adopt a novel mesh-based object\nactive shape model that has not been explored by the previous literature. Our\nalgorithm, named ShapeICP, has its foundation in the iterative closest point\n(ICP) algorithm but is equipped with additional features for the category-level\npose and shape estimation task. The results show that even without using any\npose-annotated data, ShapeICP surpasses many data-driven approaches that rely\non the pose data for training, opening up new solution space for researchers to\nconsider.\n", "link": "http://arxiv.org/abs/2408.13147v1", "date": "2024-08-23", "relevancy": 2.7046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5383}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeICP%3A%20Iterative%20Category-level%20Object%20Pose%20and%20Shape%20Estimation%20from%0A%20%20Depth&body=Title%3A%20ShapeICP%3A%20Iterative%20Category-level%20Object%20Pose%20and%20Shape%20Estimation%20from%0A%20%20Depth%0AAuthor%3A%20Yihao%20Zhang%20and%20John%20J.%20Leonard%0AAbstract%3A%20%20%20Category-level%20object%20pose%20and%20shape%20estimation%20from%20a%20single%20depth%20image%20has%0Arecently%20drawn%20research%20attention%20due%20to%20its%20wide%20applications%20in%20robotics%20and%0Aself-driving.%20The%20task%20is%20particularly%20challenging%20because%20the%20three%20unknowns%2C%0Aobject%20pose%2C%20object%20shape%2C%20and%20model-to-measurement%20correspondences%2C%20are%0Acompounded%20together%20but%20only%20a%20single%20view%20of%20depth%20measurements%20is%20provided.%0AThe%20vast%20majority%20of%20the%20prior%20work%20heavily%20relies%20on%20data-driven%20approaches%20to%0Aobtain%20solutions%20to%20at%20least%20one%20of%20the%20unknowns%20and%20typically%20two%2C%20running%0Awith%20the%20risk%20of%20failing%20to%20generalize%20to%20unseen%20domains.%20The%20shape%0Arepresentations%20used%20in%20the%20prior%20work%20also%20mainly%20focus%20on%20point%20cloud%20and%0Asigned%20distance%20field%20%28SDF%29.%20In%20stark%20contrast%20to%20the%20prior%20work%2C%20we%20approach%0Athe%20problem%20using%20an%20iterative%20estimation%20method%20that%20does%20not%20require%20learning%0Afrom%20any%20pose-annotated%20data.%20In%20addition%2C%20we%20adopt%20a%20novel%20mesh-based%20object%0Aactive%20shape%20model%20that%20has%20not%20been%20explored%20by%20the%20previous%20literature.%20Our%0Aalgorithm%2C%20named%20ShapeICP%2C%20has%20its%20foundation%20in%20the%20iterative%20closest%20point%0A%28ICP%29%20algorithm%20but%20is%20equipped%20with%20additional%20features%20for%20the%20category-level%0Apose%20and%20shape%20estimation%20task.%20The%20results%20show%20that%20even%20without%20using%20any%0Apose-annotated%20data%2C%20ShapeICP%20surpasses%20many%20data-driven%20approaches%20that%20rely%0Aon%20the%20pose%20data%20for%20training%2C%20opening%20up%20new%20solution%20space%20for%20researchers%20to%0Aconsider.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeICP%253A%2520Iterative%2520Category-level%2520Object%2520Pose%2520and%2520Shape%2520Estimation%2520from%250A%2520%2520Depth%26entry.906535625%3DYihao%2520Zhang%2520and%2520John%2520J.%2520Leonard%26entry.1292438233%3D%2520%2520Category-level%2520object%2520pose%2520and%2520shape%2520estimation%2520from%2520a%2520single%2520depth%2520image%2520has%250Arecently%2520drawn%2520research%2520attention%2520due%2520to%2520its%2520wide%2520applications%2520in%2520robotics%2520and%250Aself-driving.%2520The%2520task%2520is%2520particularly%2520challenging%2520because%2520the%2520three%2520unknowns%252C%250Aobject%2520pose%252C%2520object%2520shape%252C%2520and%2520model-to-measurement%2520correspondences%252C%2520are%250Acompounded%2520together%2520but%2520only%2520a%2520single%2520view%2520of%2520depth%2520measurements%2520is%2520provided.%250AThe%2520vast%2520majority%2520of%2520the%2520prior%2520work%2520heavily%2520relies%2520on%2520data-driven%2520approaches%2520to%250Aobtain%2520solutions%2520to%2520at%2520least%2520one%2520of%2520the%2520unknowns%2520and%2520typically%2520two%252C%2520running%250Awith%2520the%2520risk%2520of%2520failing%2520to%2520generalize%2520to%2520unseen%2520domains.%2520The%2520shape%250Arepresentations%2520used%2520in%2520the%2520prior%2520work%2520also%2520mainly%2520focus%2520on%2520point%2520cloud%2520and%250Asigned%2520distance%2520field%2520%2528SDF%2529.%2520In%2520stark%2520contrast%2520to%2520the%2520prior%2520work%252C%2520we%2520approach%250Athe%2520problem%2520using%2520an%2520iterative%2520estimation%2520method%2520that%2520does%2520not%2520require%2520learning%250Afrom%2520any%2520pose-annotated%2520data.%2520In%2520addition%252C%2520we%2520adopt%2520a%2520novel%2520mesh-based%2520object%250Aactive%2520shape%2520model%2520that%2520has%2520not%2520been%2520explored%2520by%2520the%2520previous%2520literature.%2520Our%250Aalgorithm%252C%2520named%2520ShapeICP%252C%2520has%2520its%2520foundation%2520in%2520the%2520iterative%2520closest%2520point%250A%2528ICP%2529%2520algorithm%2520but%2520is%2520equipped%2520with%2520additional%2520features%2520for%2520the%2520category-level%250Apose%2520and%2520shape%2520estimation%2520task.%2520The%2520results%2520show%2520that%2520even%2520without%2520using%2520any%250Apose-annotated%2520data%252C%2520ShapeICP%2520surpasses%2520many%2520data-driven%2520approaches%2520that%2520rely%250Aon%2520the%2520pose%2520data%2520for%2520training%252C%2520opening%2520up%2520new%2520solution%2520space%2520for%2520researchers%2520to%250Aconsider.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeICP%3A%20Iterative%20Category-level%20Object%20Pose%20and%20Shape%20Estimation%20from%0A%20%20Depth&entry.906535625=Yihao%20Zhang%20and%20John%20J.%20Leonard&entry.1292438233=%20%20Category-level%20object%20pose%20and%20shape%20estimation%20from%20a%20single%20depth%20image%20has%0Arecently%20drawn%20research%20attention%20due%20to%20its%20wide%20applications%20in%20robotics%20and%0Aself-driving.%20The%20task%20is%20particularly%20challenging%20because%20the%20three%20unknowns%2C%0Aobject%20pose%2C%20object%20shape%2C%20and%20model-to-measurement%20correspondences%2C%20are%0Acompounded%20together%20but%20only%20a%20single%20view%20of%20depth%20measurements%20is%20provided.%0AThe%20vast%20majority%20of%20the%20prior%20work%20heavily%20relies%20on%20data-driven%20approaches%20to%0Aobtain%20solutions%20to%20at%20least%20one%20of%20the%20unknowns%20and%20typically%20two%2C%20running%0Awith%20the%20risk%20of%20failing%20to%20generalize%20to%20unseen%20domains.%20The%20shape%0Arepresentations%20used%20in%20the%20prior%20work%20also%20mainly%20focus%20on%20point%20cloud%20and%0Asigned%20distance%20field%20%28SDF%29.%20In%20stark%20contrast%20to%20the%20prior%20work%2C%20we%20approach%0Athe%20problem%20using%20an%20iterative%20estimation%20method%20that%20does%20not%20require%20learning%0Afrom%20any%20pose-annotated%20data.%20In%20addition%2C%20we%20adopt%20a%20novel%20mesh-based%20object%0Aactive%20shape%20model%20that%20has%20not%20been%20explored%20by%20the%20previous%20literature.%20Our%0Aalgorithm%2C%20named%20ShapeICP%2C%20has%20its%20foundation%20in%20the%20iterative%20closest%20point%0A%28ICP%29%20algorithm%20but%20is%20equipped%20with%20additional%20features%20for%20the%20category-level%0Apose%20and%20shape%20estimation%20task.%20The%20results%20show%20that%20even%20without%20using%20any%0Apose-annotated%20data%2C%20ShapeICP%20surpasses%20many%20data-driven%20approaches%20that%20rely%0Aon%20the%20pose%20data%20for%20training%2C%20opening%20up%20new%20solution%20space%20for%20researchers%20to%0Aconsider.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13147v1&entry.124074799=Read"},
{"title": "CustomCrafter: Customized Video Generation with Preserving Motion and\n  Concept Composition Abilities", "author": "Tao Wu and Yong Zhang and Xintao Wang and Xianpan Zhou and Guangcong Zheng and Zhongang Qi and Ying Shan and Xi Li", "abstract": "  Customized video generation aims to generate high-quality videos guided by\ntext prompts and subject's reference images. However, since it is only trained\non static images, the fine-tuning process of subject learning disrupts\nabilities of video diffusion models (VDMs) to combine concepts and generate\nmotions. To restore these abilities, some methods use additional video similar\nto the prompt to fine-tune or guide the model. This requires frequent changes\nof guiding videos and even re-tuning of the model when generating different\nmotions, which is very inconvenient for users. In this paper, we propose\nCustomCrafter, a novel framework that preserves the model's motion generation\nand conceptual combination abilities without additional video and fine-tuning\nto recovery. For preserving conceptual combination ability, we design a\nplug-and-play module to update few parameters in VDMs, enhancing the model's\nability to capture the appearance details and the ability of concept\ncombinations for new subjects. For motion generation, we observed that VDMs\ntend to restore the motion of video in the early stage of denoising, while\nfocusing on the recovery of subject details in the later stage. Therefore, we\npropose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our\nsubject learning modules, we reduce the impact of this module on motion\ngeneration in the early stage of denoising, preserving the ability to generate\nmotion of VDMs. In the later stage of denoising, we restore this module to\nrepair the appearance details of the specified subject, thereby ensuring the\nfidelity of the subject's appearance. Experimental results show that our method\nhas a significant improvement compared to previous methods.\n", "link": "http://arxiv.org/abs/2408.13239v1", "date": "2024-08-23", "relevancy": 2.7035, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7257}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7077}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CustomCrafter%3A%20Customized%20Video%20Generation%20with%20Preserving%20Motion%20and%0A%20%20Concept%20Composition%20Abilities&body=Title%3A%20CustomCrafter%3A%20Customized%20Video%20Generation%20with%20Preserving%20Motion%20and%0A%20%20Concept%20Composition%20Abilities%0AAuthor%3A%20Tao%20Wu%20and%20Yong%20Zhang%20and%20Xintao%20Wang%20and%20Xianpan%20Zhou%20and%20Guangcong%20Zheng%20and%20Zhongang%20Qi%20and%20Ying%20Shan%20and%20Xi%20Li%0AAbstract%3A%20%20%20Customized%20video%20generation%20aims%20to%20generate%20high-quality%20videos%20guided%20by%0Atext%20prompts%20and%20subject%27s%20reference%20images.%20However%2C%20since%20it%20is%20only%20trained%0Aon%20static%20images%2C%20the%20fine-tuning%20process%20of%20subject%20learning%20disrupts%0Aabilities%20of%20video%20diffusion%20models%20%28VDMs%29%20to%20combine%20concepts%20and%20generate%0Amotions.%20To%20restore%20these%20abilities%2C%20some%20methods%20use%20additional%20video%20similar%0Ato%20the%20prompt%20to%20fine-tune%20or%20guide%20the%20model.%20This%20requires%20frequent%20changes%0Aof%20guiding%20videos%20and%20even%20re-tuning%20of%20the%20model%20when%20generating%20different%0Amotions%2C%20which%20is%20very%20inconvenient%20for%20users.%20In%20this%20paper%2C%20we%20propose%0ACustomCrafter%2C%20a%20novel%20framework%20that%20preserves%20the%20model%27s%20motion%20generation%0Aand%20conceptual%20combination%20abilities%20without%20additional%20video%20and%20fine-tuning%0Ato%20recovery.%20For%20preserving%20conceptual%20combination%20ability%2C%20we%20design%20a%0Aplug-and-play%20module%20to%20update%20few%20parameters%20in%20VDMs%2C%20enhancing%20the%20model%27s%0Aability%20to%20capture%20the%20appearance%20details%20and%20the%20ability%20of%20concept%0Acombinations%20for%20new%20subjects.%20For%20motion%20generation%2C%20we%20observed%20that%20VDMs%0Atend%20to%20restore%20the%20motion%20of%20video%20in%20the%20early%20stage%20of%20denoising%2C%20while%0Afocusing%20on%20the%20recovery%20of%20subject%20details%20in%20the%20later%20stage.%20Therefore%2C%20we%0Apropose%20Dynamic%20Weighted%20Video%20Sampling%20Strategy.%20Using%20the%20pluggability%20of%20our%0Asubject%20learning%20modules%2C%20we%20reduce%20the%20impact%20of%20this%20module%20on%20motion%0Ageneration%20in%20the%20early%20stage%20of%20denoising%2C%20preserving%20the%20ability%20to%20generate%0Amotion%20of%20VDMs.%20In%20the%20later%20stage%20of%20denoising%2C%20we%20restore%20this%20module%20to%0Arepair%20the%20appearance%20details%20of%20the%20specified%20subject%2C%20thereby%20ensuring%20the%0Afidelity%20of%20the%20subject%27s%20appearance.%20Experimental%20results%20show%20that%20our%20method%0Ahas%20a%20significant%20improvement%20compared%20to%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomCrafter%253A%2520Customized%2520Video%2520Generation%2520with%2520Preserving%2520Motion%2520and%250A%2520%2520Concept%2520Composition%2520Abilities%26entry.906535625%3DTao%2520Wu%2520and%2520Yong%2520Zhang%2520and%2520Xintao%2520Wang%2520and%2520Xianpan%2520Zhou%2520and%2520Guangcong%2520Zheng%2520and%2520Zhongang%2520Qi%2520and%2520Ying%2520Shan%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Customized%2520video%2520generation%2520aims%2520to%2520generate%2520high-quality%2520videos%2520guided%2520by%250Atext%2520prompts%2520and%2520subject%2527s%2520reference%2520images.%2520However%252C%2520since%2520it%2520is%2520only%2520trained%250Aon%2520static%2520images%252C%2520the%2520fine-tuning%2520process%2520of%2520subject%2520learning%2520disrupts%250Aabilities%2520of%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520to%2520combine%2520concepts%2520and%2520generate%250Amotions.%2520To%2520restore%2520these%2520abilities%252C%2520some%2520methods%2520use%2520additional%2520video%2520similar%250Ato%2520the%2520prompt%2520to%2520fine-tune%2520or%2520guide%2520the%2520model.%2520This%2520requires%2520frequent%2520changes%250Aof%2520guiding%2520videos%2520and%2520even%2520re-tuning%2520of%2520the%2520model%2520when%2520generating%2520different%250Amotions%252C%2520which%2520is%2520very%2520inconvenient%2520for%2520users.%2520In%2520this%2520paper%252C%2520we%2520propose%250ACustomCrafter%252C%2520a%2520novel%2520framework%2520that%2520preserves%2520the%2520model%2527s%2520motion%2520generation%250Aand%2520conceptual%2520combination%2520abilities%2520without%2520additional%2520video%2520and%2520fine-tuning%250Ato%2520recovery.%2520For%2520preserving%2520conceptual%2520combination%2520ability%252C%2520we%2520design%2520a%250Aplug-and-play%2520module%2520to%2520update%2520few%2520parameters%2520in%2520VDMs%252C%2520enhancing%2520the%2520model%2527s%250Aability%2520to%2520capture%2520the%2520appearance%2520details%2520and%2520the%2520ability%2520of%2520concept%250Acombinations%2520for%2520new%2520subjects.%2520For%2520motion%2520generation%252C%2520we%2520observed%2520that%2520VDMs%250Atend%2520to%2520restore%2520the%2520motion%2520of%2520video%2520in%2520the%2520early%2520stage%2520of%2520denoising%252C%2520while%250Afocusing%2520on%2520the%2520recovery%2520of%2520subject%2520details%2520in%2520the%2520later%2520stage.%2520Therefore%252C%2520we%250Apropose%2520Dynamic%2520Weighted%2520Video%2520Sampling%2520Strategy.%2520Using%2520the%2520pluggability%2520of%2520our%250Asubject%2520learning%2520modules%252C%2520we%2520reduce%2520the%2520impact%2520of%2520this%2520module%2520on%2520motion%250Ageneration%2520in%2520the%2520early%2520stage%2520of%2520denoising%252C%2520preserving%2520the%2520ability%2520to%2520generate%250Amotion%2520of%2520VDMs.%2520In%2520the%2520later%2520stage%2520of%2520denoising%252C%2520we%2520restore%2520this%2520module%2520to%250Arepair%2520the%2520appearance%2520details%2520of%2520the%2520specified%2520subject%252C%2520thereby%2520ensuring%2520the%250Afidelity%2520of%2520the%2520subject%2527s%2520appearance.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Ahas%2520a%2520significant%2520improvement%2520compared%2520to%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CustomCrafter%3A%20Customized%20Video%20Generation%20with%20Preserving%20Motion%20and%0A%20%20Concept%20Composition%20Abilities&entry.906535625=Tao%20Wu%20and%20Yong%20Zhang%20and%20Xintao%20Wang%20and%20Xianpan%20Zhou%20and%20Guangcong%20Zheng%20and%20Zhongang%20Qi%20and%20Ying%20Shan%20and%20Xi%20Li&entry.1292438233=%20%20Customized%20video%20generation%20aims%20to%20generate%20high-quality%20videos%20guided%20by%0Atext%20prompts%20and%20subject%27s%20reference%20images.%20However%2C%20since%20it%20is%20only%20trained%0Aon%20static%20images%2C%20the%20fine-tuning%20process%20of%20subject%20learning%20disrupts%0Aabilities%20of%20video%20diffusion%20models%20%28VDMs%29%20to%20combine%20concepts%20and%20generate%0Amotions.%20To%20restore%20these%20abilities%2C%20some%20methods%20use%20additional%20video%20similar%0Ato%20the%20prompt%20to%20fine-tune%20or%20guide%20the%20model.%20This%20requires%20frequent%20changes%0Aof%20guiding%20videos%20and%20even%20re-tuning%20of%20the%20model%20when%20generating%20different%0Amotions%2C%20which%20is%20very%20inconvenient%20for%20users.%20In%20this%20paper%2C%20we%20propose%0ACustomCrafter%2C%20a%20novel%20framework%20that%20preserves%20the%20model%27s%20motion%20generation%0Aand%20conceptual%20combination%20abilities%20without%20additional%20video%20and%20fine-tuning%0Ato%20recovery.%20For%20preserving%20conceptual%20combination%20ability%2C%20we%20design%20a%0Aplug-and-play%20module%20to%20update%20few%20parameters%20in%20VDMs%2C%20enhancing%20the%20model%27s%0Aability%20to%20capture%20the%20appearance%20details%20and%20the%20ability%20of%20concept%0Acombinations%20for%20new%20subjects.%20For%20motion%20generation%2C%20we%20observed%20that%20VDMs%0Atend%20to%20restore%20the%20motion%20of%20video%20in%20the%20early%20stage%20of%20denoising%2C%20while%0Afocusing%20on%20the%20recovery%20of%20subject%20details%20in%20the%20later%20stage.%20Therefore%2C%20we%0Apropose%20Dynamic%20Weighted%20Video%20Sampling%20Strategy.%20Using%20the%20pluggability%20of%20our%0Asubject%20learning%20modules%2C%20we%20reduce%20the%20impact%20of%20this%20module%20on%20motion%0Ageneration%20in%20the%20early%20stage%20of%20denoising%2C%20preserving%20the%20ability%20to%20generate%0Amotion%20of%20VDMs.%20In%20the%20later%20stage%20of%20denoising%2C%20we%20restore%20this%20module%20to%0Arepair%20the%20appearance%20details%20of%20the%20specified%20subject%2C%20thereby%20ensuring%20the%0Afidelity%20of%20the%20subject%27s%20appearance.%20Experimental%20results%20show%20that%20our%20method%0Ahas%20a%20significant%20improvement%20compared%20to%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13239v1&entry.124074799=Read"},
{"title": "TokenPacker: Efficient Visual Projector for Multimodal LLM", "author": "Wentong Li and Yuqian Yuan and Jian Liu and Dongqi Tang and Song Wang and Jie Qin and Jianke Zhu and Lei Zhang", "abstract": "  The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.\n", "link": "http://arxiv.org/abs/2407.02392v3", "date": "2024-08-23", "relevancy": 2.69, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5498}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM&body=Title%3A%20TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM%0AAuthor%3A%20Wentong%20Li%20and%20Yuqian%20Yuan%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Song%20Wang%20and%20Jie%20Qin%20and%20Jianke%20Zhu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20The%20visual%20projector%20serves%20as%20an%20essential%20bridge%20between%20the%20visual%20encoder%0Aand%20the%20Large%20Language%20Model%20%28LLM%29%20in%20a%20Multimodal%20LLM%20%28MLLM%29.%20Typically%2C%20MLLMs%0Aadopt%20a%20simple%20MLP%20to%20preserve%20all%20visual%20contexts%20via%20one-to-one%0Atransformation.%20However%2C%20the%20visual%20tokens%20are%20redundant%20and%20can%20be%0Aconsiderably%20increased%20when%20dealing%20with%20high-resolution%20images%2C%20impairing%20the%0Aefficiency%20of%20MLLMs%20significantly.%20Some%20recent%20works%20have%20introduced%20resampler%0Aor%20abstractor%20to%20reduce%20the%20number%20of%20resulting%20visual%20tokens.%20Unfortunately%2C%0Athey%20fail%20to%20capture%20finer%20details%20and%20undermine%20the%20visual%20reasoning%0Acapabilities%20of%20MLLMs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20visual%20projector%2C%20which%0Aadopts%20a%20coarse-to-fine%20scheme%20to%20inject%20the%20enriched%20characteristics%20to%0Agenerate%20the%20condensed%20visual%20tokens.%20In%20specific%2C%20we%20first%20interpolate%20the%0Avisual%20features%20as%20a%20low-resolution%20point%20query%2C%20providing%20the%20overall%20visual%0Arepresentation%20as%20the%20foundation.%20Then%2C%20we%20introduce%20a%20region-to-point%0Ainjection%20module%20that%20utilizes%20high-resolution%2C%20multi-level%20region-based%20cues%0Aas%20fine-grained%20reference%20keys%20and%20values%2C%20allowing%20them%20to%20be%20fully%20absorbed%0Awithin%20the%20corresponding%20local%20context%20region.%20This%20step%20effectively%20updates%0Athe%20coarse%20point%20query%2C%20transforming%20it%20into%20an%20enriched%20one%20for%20the%20subsequent%0ALLM%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20compresses%0Athe%20visual%20tokens%20by%2075%25~89%25%2C%20while%20achieves%20comparable%20or%20even%20better%0Aperformance%20across%20diverse%20benchmarks%20with%20significantly%20higher%20efficiency.%20The%0Asource%20codes%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/TokenPacker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02392v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenPacker%253A%2520Efficient%2520Visual%2520Projector%2520for%2520Multimodal%2520LLM%26entry.906535625%3DWentong%2520Li%2520and%2520Yuqian%2520Yuan%2520and%2520Jian%2520Liu%2520and%2520Dongqi%2520Tang%2520and%2520Song%2520Wang%2520and%2520Jie%2520Qin%2520and%2520Jianke%2520Zhu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520visual%2520projector%2520serves%2520as%2520an%2520essential%2520bridge%2520between%2520the%2520visual%2520encoder%250Aand%2520the%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520in%2520a%2520Multimodal%2520LLM%2520%2528MLLM%2529.%2520Typically%252C%2520MLLMs%250Aadopt%2520a%2520simple%2520MLP%2520to%2520preserve%2520all%2520visual%2520contexts%2520via%2520one-to-one%250Atransformation.%2520However%252C%2520the%2520visual%2520tokens%2520are%2520redundant%2520and%2520can%2520be%250Aconsiderably%2520increased%2520when%2520dealing%2520with%2520high-resolution%2520images%252C%2520impairing%2520the%250Aefficiency%2520of%2520MLLMs%2520significantly.%2520Some%2520recent%2520works%2520have%2520introduced%2520resampler%250Aor%2520abstractor%2520to%2520reduce%2520the%2520number%2520of%2520resulting%2520visual%2520tokens.%2520Unfortunately%252C%250Athey%2520fail%2520to%2520capture%2520finer%2520details%2520and%2520undermine%2520the%2520visual%2520reasoning%250Acapabilities%2520of%2520MLLMs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520visual%2520projector%252C%2520which%250Aadopts%2520a%2520coarse-to-fine%2520scheme%2520to%2520inject%2520the%2520enriched%2520characteristics%2520to%250Agenerate%2520the%2520condensed%2520visual%2520tokens.%2520In%2520specific%252C%2520we%2520first%2520interpolate%2520the%250Avisual%2520features%2520as%2520a%2520low-resolution%2520point%2520query%252C%2520providing%2520the%2520overall%2520visual%250Arepresentation%2520as%2520the%2520foundation.%2520Then%252C%2520we%2520introduce%2520a%2520region-to-point%250Ainjection%2520module%2520that%2520utilizes%2520high-resolution%252C%2520multi-level%2520region-based%2520cues%250Aas%2520fine-grained%2520reference%2520keys%2520and%2520values%252C%2520allowing%2520them%2520to%2520be%2520fully%2520absorbed%250Awithin%2520the%2520corresponding%2520local%2520context%2520region.%2520This%2520step%2520effectively%2520updates%250Athe%2520coarse%2520point%2520query%252C%2520transforming%2520it%2520into%2520an%2520enriched%2520one%2520for%2520the%2520subsequent%250ALLM%2520reasoning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520compresses%250Athe%2520visual%2520tokens%2520by%252075%2525~89%2525%252C%2520while%2520achieves%2520comparable%2520or%2520even%2520better%250Aperformance%2520across%2520diverse%2520benchmarks%2520with%2520significantly%2520higher%2520efficiency.%2520The%250Asource%2520codes%2520can%2520be%2520found%2520at%2520https%253A//github.com/CircleRadon/TokenPacker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02392v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM&entry.906535625=Wentong%20Li%20and%20Yuqian%20Yuan%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Song%20Wang%20and%20Jie%20Qin%20and%20Jianke%20Zhu%20and%20Lei%20Zhang&entry.1292438233=%20%20The%20visual%20projector%20serves%20as%20an%20essential%20bridge%20between%20the%20visual%20encoder%0Aand%20the%20Large%20Language%20Model%20%28LLM%29%20in%20a%20Multimodal%20LLM%20%28MLLM%29.%20Typically%2C%20MLLMs%0Aadopt%20a%20simple%20MLP%20to%20preserve%20all%20visual%20contexts%20via%20one-to-one%0Atransformation.%20However%2C%20the%20visual%20tokens%20are%20redundant%20and%20can%20be%0Aconsiderably%20increased%20when%20dealing%20with%20high-resolution%20images%2C%20impairing%20the%0Aefficiency%20of%20MLLMs%20significantly.%20Some%20recent%20works%20have%20introduced%20resampler%0Aor%20abstractor%20to%20reduce%20the%20number%20of%20resulting%20visual%20tokens.%20Unfortunately%2C%0Athey%20fail%20to%20capture%20finer%20details%20and%20undermine%20the%20visual%20reasoning%0Acapabilities%20of%20MLLMs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20visual%20projector%2C%20which%0Aadopts%20a%20coarse-to-fine%20scheme%20to%20inject%20the%20enriched%20characteristics%20to%0Agenerate%20the%20condensed%20visual%20tokens.%20In%20specific%2C%20we%20first%20interpolate%20the%0Avisual%20features%20as%20a%20low-resolution%20point%20query%2C%20providing%20the%20overall%20visual%0Arepresentation%20as%20the%20foundation.%20Then%2C%20we%20introduce%20a%20region-to-point%0Ainjection%20module%20that%20utilizes%20high-resolution%2C%20multi-level%20region-based%20cues%0Aas%20fine-grained%20reference%20keys%20and%20values%2C%20allowing%20them%20to%20be%20fully%20absorbed%0Awithin%20the%20corresponding%20local%20context%20region.%20This%20step%20effectively%20updates%0Athe%20coarse%20point%20query%2C%20transforming%20it%20into%20an%20enriched%20one%20for%20the%20subsequent%0ALLM%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20compresses%0Athe%20visual%20tokens%20by%2075%25~89%25%2C%20while%20achieves%20comparable%20or%20even%20better%0Aperformance%20across%20diverse%20benchmarks%20with%20significantly%20higher%20efficiency.%20The%0Asource%20codes%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/TokenPacker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02392v3&entry.124074799=Read"},
{"title": "Large-scale Pre-trained Models are Surprisingly Strong in Incremental\n  Novel Class Discovery", "author": "Mingxuan Liu and Subhankar Roy and Zhun Zhong and Nicu Sebe and Elisa Ricci", "abstract": "  Discovering novel concepts in unlabelled datasets and in a continuous manner\nis an important desideratum of lifelong learners. In the literature such\nproblems have been partially addressed under very restricted settings, where\nnovel classes are learned by jointly accessing a related labelled set (e.g.,\nNCD) or by leveraging only a supervisedly pre-trained model (e.g., class-iNCD).\nIn this work we challenge the status quo in class-iNCD and propose a learning\nparadigm where class discovery occurs continuously and truly unsupervisedly,\nwithout needing any related labelled set. In detail, we propose to exploit the\nricher priors from strong self-supervised pre-trained models (PTM). To this\nend, we propose simple baselines, composed of a frozen PTM backbone and a\nlearnable linear classifier, that are not only simple to implement but also\nresilient under longer learning scenarios. We conduct extensive empirical\nevaluation on a multitude of benchmarks and show the effectiveness of our\nproposed baselines when compared with sophisticated state-of-the-art methods.\nThe code is open source.\n", "link": "http://arxiv.org/abs/2303.15975v5", "date": "2024-08-23", "relevancy": 2.6889, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5499}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5379}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20Pre-trained%20Models%20are%20Surprisingly%20Strong%20in%20Incremental%0A%20%20Novel%20Class%20Discovery&body=Title%3A%20Large-scale%20Pre-trained%20Models%20are%20Surprisingly%20Strong%20in%20Incremental%0A%20%20Novel%20Class%20Discovery%0AAuthor%3A%20Mingxuan%20Liu%20and%20Subhankar%20Roy%20and%20Zhun%20Zhong%20and%20Nicu%20Sebe%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Discovering%20novel%20concepts%20in%20unlabelled%20datasets%20and%20in%20a%20continuous%20manner%0Ais%20an%20important%20desideratum%20of%20lifelong%20learners.%20In%20the%20literature%20such%0Aproblems%20have%20been%20partially%20addressed%20under%20very%20restricted%20settings%2C%20where%0Anovel%20classes%20are%20learned%20by%20jointly%20accessing%20a%20related%20labelled%20set%20%28e.g.%2C%0ANCD%29%20or%20by%20leveraging%20only%20a%20supervisedly%20pre-trained%20model%20%28e.g.%2C%20class-iNCD%29.%0AIn%20this%20work%20we%20challenge%20the%20status%20quo%20in%20class-iNCD%20and%20propose%20a%20learning%0Aparadigm%20where%20class%20discovery%20occurs%20continuously%20and%20truly%20unsupervisedly%2C%0Awithout%20needing%20any%20related%20labelled%20set.%20In%20detail%2C%20we%20propose%20to%20exploit%20the%0Aricher%20priors%20from%20strong%20self-supervised%20pre-trained%20models%20%28PTM%29.%20To%20this%0Aend%2C%20we%20propose%20simple%20baselines%2C%20composed%20of%20a%20frozen%20PTM%20backbone%20and%20a%0Alearnable%20linear%20classifier%2C%20that%20are%20not%20only%20simple%20to%20implement%20but%20also%0Aresilient%20under%20longer%20learning%20scenarios.%20We%20conduct%20extensive%20empirical%0Aevaluation%20on%20a%20multitude%20of%20benchmarks%20and%20show%20the%20effectiveness%20of%20our%0Aproposed%20baselines%20when%20compared%20with%20sophisticated%20state-of-the-art%20methods.%0AThe%20code%20is%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15975v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520Pre-trained%2520Models%2520are%2520Surprisingly%2520Strong%2520in%2520Incremental%250A%2520%2520Novel%2520Class%2520Discovery%26entry.906535625%3DMingxuan%2520Liu%2520and%2520Subhankar%2520Roy%2520and%2520Zhun%2520Zhong%2520and%2520Nicu%2520Sebe%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Discovering%2520novel%2520concepts%2520in%2520unlabelled%2520datasets%2520and%2520in%2520a%2520continuous%2520manner%250Ais%2520an%2520important%2520desideratum%2520of%2520lifelong%2520learners.%2520In%2520the%2520literature%2520such%250Aproblems%2520have%2520been%2520partially%2520addressed%2520under%2520very%2520restricted%2520settings%252C%2520where%250Anovel%2520classes%2520are%2520learned%2520by%2520jointly%2520accessing%2520a%2520related%2520labelled%2520set%2520%2528e.g.%252C%250ANCD%2529%2520or%2520by%2520leveraging%2520only%2520a%2520supervisedly%2520pre-trained%2520model%2520%2528e.g.%252C%2520class-iNCD%2529.%250AIn%2520this%2520work%2520we%2520challenge%2520the%2520status%2520quo%2520in%2520class-iNCD%2520and%2520propose%2520a%2520learning%250Aparadigm%2520where%2520class%2520discovery%2520occurs%2520continuously%2520and%2520truly%2520unsupervisedly%252C%250Awithout%2520needing%2520any%2520related%2520labelled%2520set.%2520In%2520detail%252C%2520we%2520propose%2520to%2520exploit%2520the%250Aricher%2520priors%2520from%2520strong%2520self-supervised%2520pre-trained%2520models%2520%2528PTM%2529.%2520To%2520this%250Aend%252C%2520we%2520propose%2520simple%2520baselines%252C%2520composed%2520of%2520a%2520frozen%2520PTM%2520backbone%2520and%2520a%250Alearnable%2520linear%2520classifier%252C%2520that%2520are%2520not%2520only%2520simple%2520to%2520implement%2520but%2520also%250Aresilient%2520under%2520longer%2520learning%2520scenarios.%2520We%2520conduct%2520extensive%2520empirical%250Aevaluation%2520on%2520a%2520multitude%2520of%2520benchmarks%2520and%2520show%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520baselines%2520when%2520compared%2520with%2520sophisticated%2520state-of-the-art%2520methods.%250AThe%2520code%2520is%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.15975v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20Pre-trained%20Models%20are%20Surprisingly%20Strong%20in%20Incremental%0A%20%20Novel%20Class%20Discovery&entry.906535625=Mingxuan%20Liu%20and%20Subhankar%20Roy%20and%20Zhun%20Zhong%20and%20Nicu%20Sebe%20and%20Elisa%20Ricci&entry.1292438233=%20%20Discovering%20novel%20concepts%20in%20unlabelled%20datasets%20and%20in%20a%20continuous%20manner%0Ais%20an%20important%20desideratum%20of%20lifelong%20learners.%20In%20the%20literature%20such%0Aproblems%20have%20been%20partially%20addressed%20under%20very%20restricted%20settings%2C%20where%0Anovel%20classes%20are%20learned%20by%20jointly%20accessing%20a%20related%20labelled%20set%20%28e.g.%2C%0ANCD%29%20or%20by%20leveraging%20only%20a%20supervisedly%20pre-trained%20model%20%28e.g.%2C%20class-iNCD%29.%0AIn%20this%20work%20we%20challenge%20the%20status%20quo%20in%20class-iNCD%20and%20propose%20a%20learning%0Aparadigm%20where%20class%20discovery%20occurs%20continuously%20and%20truly%20unsupervisedly%2C%0Awithout%20needing%20any%20related%20labelled%20set.%20In%20detail%2C%20we%20propose%20to%20exploit%20the%0Aricher%20priors%20from%20strong%20self-supervised%20pre-trained%20models%20%28PTM%29.%20To%20this%0Aend%2C%20we%20propose%20simple%20baselines%2C%20composed%20of%20a%20frozen%20PTM%20backbone%20and%20a%0Alearnable%20linear%20classifier%2C%20that%20are%20not%20only%20simple%20to%20implement%20but%20also%0Aresilient%20under%20longer%20learning%20scenarios.%20We%20conduct%20extensive%20empirical%0Aevaluation%20on%20a%20multitude%20of%20benchmarks%20and%20show%20the%20effectiveness%20of%20our%0Aproposed%20baselines%20when%20compared%20with%20sophisticated%20state-of-the-art%20methods.%0AThe%20code%20is%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15975v5&entry.124074799=Read"},
{"title": "Avatar Visual Similarity for Social HCI: Increasing Self-Awareness", "author": "Bernhard Hilpert and Claudio Alves da Silva and Leon Christidis and Chirag Bhuvaneshwara and Patrick Gebhard and Fabrizio Nunnari and Dimitra Tsovaltzi", "abstract": "  Self-awareness is a critical factor in social human-human interaction and,\nhence, in social HCI interaction. Increasing self-awareness through mirrors or\nvideo recordings is common in face-to-face trainings, since it influences\nantecedents of self-awareness like explicit identification and implicit\naffective identification (affinity). However, increasing self-awareness has\nbeen scarcely examined in virtual trainings with virtual avatars, which allow\nfor adjusting the similarity, e.g. to avoid negative effects of\nself-consciousness. Automatic visual similarity in avatars is an open issue\nrelated to high costs. It is important to understand which features need to be\nmanipulated and which degree of similarity is necessary for self-awareness to\nleverage the added value of using avatars for self-awareness. This article\nexamines the relationship between avatar visual similarity and increasing\nself-awareness in virtual training environments. We define visual similarity\nbased on perceptually important facial features for human-human identification\nand develop a theory-based methodology to systematically manipulate visual\nsimilarity of virtual avatars and support self-awareness. Three personalized\nversions of virtual avatars with varying degrees of visual similarity to\nparticipants were created (weak, medium and strong facial features\nmanipulation). In a within-subject study (N=33), we tested effects of degree of\nsimilarity on perceived similarity, explicit identification and implicit\naffective identification (affinity). Results show significant differences\nbetween the weak similarity manipulation, and both the strong manipulation and\nthe random avatar for all three antecedents of self-awareness. An increasing\ndegree of avatar visual similarity influences antecedents of self-awareness in\nvirtual environments.\n", "link": "http://arxiv.org/abs/2408.13084v1", "date": "2024-08-23", "relevancy": 2.6306, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5446}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5178}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Avatar%20Visual%20Similarity%20for%20Social%20HCI%3A%20Increasing%20Self-Awareness&body=Title%3A%20Avatar%20Visual%20Similarity%20for%20Social%20HCI%3A%20Increasing%20Self-Awareness%0AAuthor%3A%20Bernhard%20Hilpert%20and%20Claudio%20Alves%20da%20Silva%20and%20Leon%20Christidis%20and%20Chirag%20Bhuvaneshwara%20and%20Patrick%20Gebhard%20and%20Fabrizio%20Nunnari%20and%20Dimitra%20Tsovaltzi%0AAbstract%3A%20%20%20Self-awareness%20is%20a%20critical%20factor%20in%20social%20human-human%20interaction%20and%2C%0Ahence%2C%20in%20social%20HCI%20interaction.%20Increasing%20self-awareness%20through%20mirrors%20or%0Avideo%20recordings%20is%20common%20in%20face-to-face%20trainings%2C%20since%20it%20influences%0Aantecedents%20of%20self-awareness%20like%20explicit%20identification%20and%20implicit%0Aaffective%20identification%20%28affinity%29.%20However%2C%20increasing%20self-awareness%20has%0Abeen%20scarcely%20examined%20in%20virtual%20trainings%20with%20virtual%20avatars%2C%20which%20allow%0Afor%20adjusting%20the%20similarity%2C%20e.g.%20to%20avoid%20negative%20effects%20of%0Aself-consciousness.%20Automatic%20visual%20similarity%20in%20avatars%20is%20an%20open%20issue%0Arelated%20to%20high%20costs.%20It%20is%20important%20to%20understand%20which%20features%20need%20to%20be%0Amanipulated%20and%20which%20degree%20of%20similarity%20is%20necessary%20for%20self-awareness%20to%0Aleverage%20the%20added%20value%20of%20using%20avatars%20for%20self-awareness.%20This%20article%0Aexamines%20the%20relationship%20between%20avatar%20visual%20similarity%20and%20increasing%0Aself-awareness%20in%20virtual%20training%20environments.%20We%20define%20visual%20similarity%0Abased%20on%20perceptually%20important%20facial%20features%20for%20human-human%20identification%0Aand%20develop%20a%20theory-based%20methodology%20to%20systematically%20manipulate%20visual%0Asimilarity%20of%20virtual%20avatars%20and%20support%20self-awareness.%20Three%20personalized%0Aversions%20of%20virtual%20avatars%20with%20varying%20degrees%20of%20visual%20similarity%20to%0Aparticipants%20were%20created%20%28weak%2C%20medium%20and%20strong%20facial%20features%0Amanipulation%29.%20In%20a%20within-subject%20study%20%28N%3D33%29%2C%20we%20tested%20effects%20of%20degree%20of%0Asimilarity%20on%20perceived%20similarity%2C%20explicit%20identification%20and%20implicit%0Aaffective%20identification%20%28affinity%29.%20Results%20show%20significant%20differences%0Abetween%20the%20weak%20similarity%20manipulation%2C%20and%20both%20the%20strong%20manipulation%20and%0Athe%20random%20avatar%20for%20all%20three%20antecedents%20of%20self-awareness.%20An%20increasing%0Adegree%20of%20avatar%20visual%20similarity%20influences%20antecedents%20of%20self-awareness%20in%0Avirtual%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatar%2520Visual%2520Similarity%2520for%2520Social%2520HCI%253A%2520Increasing%2520Self-Awareness%26entry.906535625%3DBernhard%2520Hilpert%2520and%2520Claudio%2520Alves%2520da%2520Silva%2520and%2520Leon%2520Christidis%2520and%2520Chirag%2520Bhuvaneshwara%2520and%2520Patrick%2520Gebhard%2520and%2520Fabrizio%2520Nunnari%2520and%2520Dimitra%2520Tsovaltzi%26entry.1292438233%3D%2520%2520Self-awareness%2520is%2520a%2520critical%2520factor%2520in%2520social%2520human-human%2520interaction%2520and%252C%250Ahence%252C%2520in%2520social%2520HCI%2520interaction.%2520Increasing%2520self-awareness%2520through%2520mirrors%2520or%250Avideo%2520recordings%2520is%2520common%2520in%2520face-to-face%2520trainings%252C%2520since%2520it%2520influences%250Aantecedents%2520of%2520self-awareness%2520like%2520explicit%2520identification%2520and%2520implicit%250Aaffective%2520identification%2520%2528affinity%2529.%2520However%252C%2520increasing%2520self-awareness%2520has%250Abeen%2520scarcely%2520examined%2520in%2520virtual%2520trainings%2520with%2520virtual%2520avatars%252C%2520which%2520allow%250Afor%2520adjusting%2520the%2520similarity%252C%2520e.g.%2520to%2520avoid%2520negative%2520effects%2520of%250Aself-consciousness.%2520Automatic%2520visual%2520similarity%2520in%2520avatars%2520is%2520an%2520open%2520issue%250Arelated%2520to%2520high%2520costs.%2520It%2520is%2520important%2520to%2520understand%2520which%2520features%2520need%2520to%2520be%250Amanipulated%2520and%2520which%2520degree%2520of%2520similarity%2520is%2520necessary%2520for%2520self-awareness%2520to%250Aleverage%2520the%2520added%2520value%2520of%2520using%2520avatars%2520for%2520self-awareness.%2520This%2520article%250Aexamines%2520the%2520relationship%2520between%2520avatar%2520visual%2520similarity%2520and%2520increasing%250Aself-awareness%2520in%2520virtual%2520training%2520environments.%2520We%2520define%2520visual%2520similarity%250Abased%2520on%2520perceptually%2520important%2520facial%2520features%2520for%2520human-human%2520identification%250Aand%2520develop%2520a%2520theory-based%2520methodology%2520to%2520systematically%2520manipulate%2520visual%250Asimilarity%2520of%2520virtual%2520avatars%2520and%2520support%2520self-awareness.%2520Three%2520personalized%250Aversions%2520of%2520virtual%2520avatars%2520with%2520varying%2520degrees%2520of%2520visual%2520similarity%2520to%250Aparticipants%2520were%2520created%2520%2528weak%252C%2520medium%2520and%2520strong%2520facial%2520features%250Amanipulation%2529.%2520In%2520a%2520within-subject%2520study%2520%2528N%253D33%2529%252C%2520we%2520tested%2520effects%2520of%2520degree%2520of%250Asimilarity%2520on%2520perceived%2520similarity%252C%2520explicit%2520identification%2520and%2520implicit%250Aaffective%2520identification%2520%2528affinity%2529.%2520Results%2520show%2520significant%2520differences%250Abetween%2520the%2520weak%2520similarity%2520manipulation%252C%2520and%2520both%2520the%2520strong%2520manipulation%2520and%250Athe%2520random%2520avatar%2520for%2520all%2520three%2520antecedents%2520of%2520self-awareness.%2520An%2520increasing%250Adegree%2520of%2520avatar%2520visual%2520similarity%2520influences%2520antecedents%2520of%2520self-awareness%2520in%250Avirtual%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Avatar%20Visual%20Similarity%20for%20Social%20HCI%3A%20Increasing%20Self-Awareness&entry.906535625=Bernhard%20Hilpert%20and%20Claudio%20Alves%20da%20Silva%20and%20Leon%20Christidis%20and%20Chirag%20Bhuvaneshwara%20and%20Patrick%20Gebhard%20and%20Fabrizio%20Nunnari%20and%20Dimitra%20Tsovaltzi&entry.1292438233=%20%20Self-awareness%20is%20a%20critical%20factor%20in%20social%20human-human%20interaction%20and%2C%0Ahence%2C%20in%20social%20HCI%20interaction.%20Increasing%20self-awareness%20through%20mirrors%20or%0Avideo%20recordings%20is%20common%20in%20face-to-face%20trainings%2C%20since%20it%20influences%0Aantecedents%20of%20self-awareness%20like%20explicit%20identification%20and%20implicit%0Aaffective%20identification%20%28affinity%29.%20However%2C%20increasing%20self-awareness%20has%0Abeen%20scarcely%20examined%20in%20virtual%20trainings%20with%20virtual%20avatars%2C%20which%20allow%0Afor%20adjusting%20the%20similarity%2C%20e.g.%20to%20avoid%20negative%20effects%20of%0Aself-consciousness.%20Automatic%20visual%20similarity%20in%20avatars%20is%20an%20open%20issue%0Arelated%20to%20high%20costs.%20It%20is%20important%20to%20understand%20which%20features%20need%20to%20be%0Amanipulated%20and%20which%20degree%20of%20similarity%20is%20necessary%20for%20self-awareness%20to%0Aleverage%20the%20added%20value%20of%20using%20avatars%20for%20self-awareness.%20This%20article%0Aexamines%20the%20relationship%20between%20avatar%20visual%20similarity%20and%20increasing%0Aself-awareness%20in%20virtual%20training%20environments.%20We%20define%20visual%20similarity%0Abased%20on%20perceptually%20important%20facial%20features%20for%20human-human%20identification%0Aand%20develop%20a%20theory-based%20methodology%20to%20systematically%20manipulate%20visual%0Asimilarity%20of%20virtual%20avatars%20and%20support%20self-awareness.%20Three%20personalized%0Aversions%20of%20virtual%20avatars%20with%20varying%20degrees%20of%20visual%20similarity%20to%0Aparticipants%20were%20created%20%28weak%2C%20medium%20and%20strong%20facial%20features%0Amanipulation%29.%20In%20a%20within-subject%20study%20%28N%3D33%29%2C%20we%20tested%20effects%20of%20degree%20of%0Asimilarity%20on%20perceived%20similarity%2C%20explicit%20identification%20and%20implicit%0Aaffective%20identification%20%28affinity%29.%20Results%20show%20significant%20differences%0Abetween%20the%20weak%20similarity%20manipulation%2C%20and%20both%20the%20strong%20manipulation%20and%0Athe%20random%20avatar%20for%20all%20three%20antecedents%20of%20self-awareness.%20An%20increasing%0Adegree%20of%20avatar%20visual%20similarity%20influences%20antecedents%20of%20self-awareness%20in%0Avirtual%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13084v1&entry.124074799=Read"},
{"title": "SIMPLE: Simultaneous Multi-Plane Self-Supervised Learning for Isotropic\n  MRI Restoration from Anisotropic Data", "author": "Rotem Benisty and Yevgenia Shteynman and Moshe Porat and Anat Illivitzki and Moti Freiman", "abstract": "  Magnetic resonance imaging (MRI) is crucial in diagnosing various abdominal\nconditions and anomalies. Traditional MRI scans often yield anisotropic data\ndue to technical constraints, resulting in varying resolutions across spatial\ndimensions, which limits diagnostic accuracy and volumetric analysis.\nSuper-resolution (SR) techniques aim to address these limitations by\nreconstructing isotropic high-resolution images from anisotropic data. However,\ncurrent SR methods often rely on indirect mappings and limited training data,\nfocusing mainly on two-dimensional improvements rather than achieving true\nthree-dimensional isotropy. We introduce SIMPLE, a Simultaneous Multi-Plane\nSelf-Supervised Learning approach for isotropic MRI restoration from\nanisotropic data. Our method leverages existing anisotropic clinical data\nacquired in different planes, bypassing the need for simulated downsampling\nprocesses. By considering the inherent three-dimensional nature of MRI data,\nSIMPLE ensures realistic isotropic data generation rather than solely improving\nthrough-plane slices. This approach flexibility allows it to be extended to\nmultiple contrast types and acquisition methods commonly used in clinical\nsettings. Our experiments show that SIMPLE outperforms state-of-the-art methods\nboth quantitatively using the Kernel Inception Distance (KID) and\nsemi-quantitatively through radiologist evaluations. The generated isotropic\nvolume facilitates more accurate volumetric analysis and 3D reconstructions,\npromising significant improvements in clinical diagnostic capabilities.\n", "link": "http://arxiv.org/abs/2408.13065v1", "date": "2024-08-23", "relevancy": 2.5634, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5112}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIMPLE%3A%20Simultaneous%20Multi-Plane%20Self-Supervised%20Learning%20for%20Isotropic%0A%20%20MRI%20Restoration%20from%20Anisotropic%20Data&body=Title%3A%20SIMPLE%3A%20Simultaneous%20Multi-Plane%20Self-Supervised%20Learning%20for%20Isotropic%0A%20%20MRI%20Restoration%20from%20Anisotropic%20Data%0AAuthor%3A%20Rotem%20Benisty%20and%20Yevgenia%20Shteynman%20and%20Moshe%20Porat%20and%20Anat%20Illivitzki%20and%20Moti%20Freiman%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20crucial%20in%20diagnosing%20various%20abdominal%0Aconditions%20and%20anomalies.%20Traditional%20MRI%20scans%20often%20yield%20anisotropic%20data%0Adue%20to%20technical%20constraints%2C%20resulting%20in%20varying%20resolutions%20across%20spatial%0Adimensions%2C%20which%20limits%20diagnostic%20accuracy%20and%20volumetric%20analysis.%0ASuper-resolution%20%28SR%29%20techniques%20aim%20to%20address%20these%20limitations%20by%0Areconstructing%20isotropic%20high-resolution%20images%20from%20anisotropic%20data.%20However%2C%0Acurrent%20SR%20methods%20often%20rely%20on%20indirect%20mappings%20and%20limited%20training%20data%2C%0Afocusing%20mainly%20on%20two-dimensional%20improvements%20rather%20than%20achieving%20true%0Athree-dimensional%20isotropy.%20We%20introduce%20SIMPLE%2C%20a%20Simultaneous%20Multi-Plane%0ASelf-Supervised%20Learning%20approach%20for%20isotropic%20MRI%20restoration%20from%0Aanisotropic%20data.%20Our%20method%20leverages%20existing%20anisotropic%20clinical%20data%0Aacquired%20in%20different%20planes%2C%20bypassing%20the%20need%20for%20simulated%20downsampling%0Aprocesses.%20By%20considering%20the%20inherent%20three-dimensional%20nature%20of%20MRI%20data%2C%0ASIMPLE%20ensures%20realistic%20isotropic%20data%20generation%20rather%20than%20solely%20improving%0Athrough-plane%20slices.%20This%20approach%20flexibility%20allows%20it%20to%20be%20extended%20to%0Amultiple%20contrast%20types%20and%20acquisition%20methods%20commonly%20used%20in%20clinical%0Asettings.%20Our%20experiments%20show%20that%20SIMPLE%20outperforms%20state-of-the-art%20methods%0Aboth%20quantitatively%20using%20the%20Kernel%20Inception%20Distance%20%28KID%29%20and%0Asemi-quantitatively%20through%20radiologist%20evaluations.%20The%20generated%20isotropic%0Avolume%20facilitates%20more%20accurate%20volumetric%20analysis%20and%203D%20reconstructions%2C%0Apromising%20significant%20improvements%20in%20clinical%20diagnostic%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIMPLE%253A%2520Simultaneous%2520Multi-Plane%2520Self-Supervised%2520Learning%2520for%2520Isotropic%250A%2520%2520MRI%2520Restoration%2520from%2520Anisotropic%2520Data%26entry.906535625%3DRotem%2520Benisty%2520and%2520Yevgenia%2520Shteynman%2520and%2520Moshe%2520Porat%2520and%2520Anat%2520Illivitzki%2520and%2520Moti%2520Freiman%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520crucial%2520in%2520diagnosing%2520various%2520abdominal%250Aconditions%2520and%2520anomalies.%2520Traditional%2520MRI%2520scans%2520often%2520yield%2520anisotropic%2520data%250Adue%2520to%2520technical%2520constraints%252C%2520resulting%2520in%2520varying%2520resolutions%2520across%2520spatial%250Adimensions%252C%2520which%2520limits%2520diagnostic%2520accuracy%2520and%2520volumetric%2520analysis.%250ASuper-resolution%2520%2528SR%2529%2520techniques%2520aim%2520to%2520address%2520these%2520limitations%2520by%250Areconstructing%2520isotropic%2520high-resolution%2520images%2520from%2520anisotropic%2520data.%2520However%252C%250Acurrent%2520SR%2520methods%2520often%2520rely%2520on%2520indirect%2520mappings%2520and%2520limited%2520training%2520data%252C%250Afocusing%2520mainly%2520on%2520two-dimensional%2520improvements%2520rather%2520than%2520achieving%2520true%250Athree-dimensional%2520isotropy.%2520We%2520introduce%2520SIMPLE%252C%2520a%2520Simultaneous%2520Multi-Plane%250ASelf-Supervised%2520Learning%2520approach%2520for%2520isotropic%2520MRI%2520restoration%2520from%250Aanisotropic%2520data.%2520Our%2520method%2520leverages%2520existing%2520anisotropic%2520clinical%2520data%250Aacquired%2520in%2520different%2520planes%252C%2520bypassing%2520the%2520need%2520for%2520simulated%2520downsampling%250Aprocesses.%2520By%2520considering%2520the%2520inherent%2520three-dimensional%2520nature%2520of%2520MRI%2520data%252C%250ASIMPLE%2520ensures%2520realistic%2520isotropic%2520data%2520generation%2520rather%2520than%2520solely%2520improving%250Athrough-plane%2520slices.%2520This%2520approach%2520flexibility%2520allows%2520it%2520to%2520be%2520extended%2520to%250Amultiple%2520contrast%2520types%2520and%2520acquisition%2520methods%2520commonly%2520used%2520in%2520clinical%250Asettings.%2520Our%2520experiments%2520show%2520that%2520SIMPLE%2520outperforms%2520state-of-the-art%2520methods%250Aboth%2520quantitatively%2520using%2520the%2520Kernel%2520Inception%2520Distance%2520%2528KID%2529%2520and%250Asemi-quantitatively%2520through%2520radiologist%2520evaluations.%2520The%2520generated%2520isotropic%250Avolume%2520facilitates%2520more%2520accurate%2520volumetric%2520analysis%2520and%25203D%2520reconstructions%252C%250Apromising%2520significant%2520improvements%2520in%2520clinical%2520diagnostic%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIMPLE%3A%20Simultaneous%20Multi-Plane%20Self-Supervised%20Learning%20for%20Isotropic%0A%20%20MRI%20Restoration%20from%20Anisotropic%20Data&entry.906535625=Rotem%20Benisty%20and%20Yevgenia%20Shteynman%20and%20Moshe%20Porat%20and%20Anat%20Illivitzki%20and%20Moti%20Freiman&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20crucial%20in%20diagnosing%20various%20abdominal%0Aconditions%20and%20anomalies.%20Traditional%20MRI%20scans%20often%20yield%20anisotropic%20data%0Adue%20to%20technical%20constraints%2C%20resulting%20in%20varying%20resolutions%20across%20spatial%0Adimensions%2C%20which%20limits%20diagnostic%20accuracy%20and%20volumetric%20analysis.%0ASuper-resolution%20%28SR%29%20techniques%20aim%20to%20address%20these%20limitations%20by%0Areconstructing%20isotropic%20high-resolution%20images%20from%20anisotropic%20data.%20However%2C%0Acurrent%20SR%20methods%20often%20rely%20on%20indirect%20mappings%20and%20limited%20training%20data%2C%0Afocusing%20mainly%20on%20two-dimensional%20improvements%20rather%20than%20achieving%20true%0Athree-dimensional%20isotropy.%20We%20introduce%20SIMPLE%2C%20a%20Simultaneous%20Multi-Plane%0ASelf-Supervised%20Learning%20approach%20for%20isotropic%20MRI%20restoration%20from%0Aanisotropic%20data.%20Our%20method%20leverages%20existing%20anisotropic%20clinical%20data%0Aacquired%20in%20different%20planes%2C%20bypassing%20the%20need%20for%20simulated%20downsampling%0Aprocesses.%20By%20considering%20the%20inherent%20three-dimensional%20nature%20of%20MRI%20data%2C%0ASIMPLE%20ensures%20realistic%20isotropic%20data%20generation%20rather%20than%20solely%20improving%0Athrough-plane%20slices.%20This%20approach%20flexibility%20allows%20it%20to%20be%20extended%20to%0Amultiple%20contrast%20types%20and%20acquisition%20methods%20commonly%20used%20in%20clinical%0Asettings.%20Our%20experiments%20show%20that%20SIMPLE%20outperforms%20state-of-the-art%20methods%0Aboth%20quantitatively%20using%20the%20Kernel%20Inception%20Distance%20%28KID%29%20and%0Asemi-quantitatively%20through%20radiologist%20evaluations.%20The%20generated%20isotropic%0Avolume%20facilitates%20more%20accurate%20volumetric%20analysis%20and%203D%20reconstructions%2C%0Apromising%20significant%20improvements%20in%20clinical%20diagnostic%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13065v1&entry.124074799=Read"},
{"title": "EasyControl: Transfer ControlNet to Video Diffusion for Controllable\n  Generation and Interpolation", "author": "Cong Wang and Jiaxi Gu and Panwen Hu and Haoyu Zhao and Yuanfan Guo and Jianhua Han and Hang Xu and Xiaodan Liang", "abstract": "  Following the advancements in text-guided image generation technology\nexemplified by Stable Diffusion, video generation is gaining increased\nattention in the academic community. However, relying solely on text guidance\nfor video generation has serious limitations, as videos contain much richer\ncontent than images, especially in terms of motion. This information can hardly\nbe adequately described with plain text. Fortunately, in computer vision,\nvarious visual representations can serve as additional control signals to guide\ngeneration. With the help of these signals, video generation can be controlled\nin finer detail, allowing for greater flexibility for different applications.\nIntegrating various controls, however, is nontrivial. In this paper, we propose\na universal framework called EasyControl. By propagating and injecting\ncondition features through condition adapters, our method enables users to\ncontrol video generation with a single condition map. With our framework,\nvarious conditions including raw pixels, depth, HED, etc., can be integrated\ninto different Unet-based pre-trained video diffusion models at a low practical\ncost. We conduct comprehensive experiments on public datasets, and both\nquantitative and qualitative results indicate that our method outperforms\nstate-of-the-art methods. EasyControl significantly improves various evaluation\nmetrics across multiple validation datasets compared to previous works.\nSpecifically, for the sketch-to-video generation task, EasyControl achieves an\nimprovement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared\nwith VideoComposer. For fidelity, our model demonstrates powerful image\nretention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared\nto other image-to-video models.\n", "link": "http://arxiv.org/abs/2408.13005v1", "date": "2024-08-23", "relevancy": 2.5252, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.654}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6164}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyControl%3A%20Transfer%20ControlNet%20to%20Video%20Diffusion%20for%20Controllable%0A%20%20Generation%20and%20Interpolation&body=Title%3A%20EasyControl%3A%20Transfer%20ControlNet%20to%20Video%20Diffusion%20for%20Controllable%0A%20%20Generation%20and%20Interpolation%0AAuthor%3A%20Cong%20Wang%20and%20Jiaxi%20Gu%20and%20Panwen%20Hu%20and%20Haoyu%20Zhao%20and%20Yuanfan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Following%20the%20advancements%20in%20text-guided%20image%20generation%20technology%0Aexemplified%20by%20Stable%20Diffusion%2C%20video%20generation%20is%20gaining%20increased%0Aattention%20in%20the%20academic%20community.%20However%2C%20relying%20solely%20on%20text%20guidance%0Afor%20video%20generation%20has%20serious%20limitations%2C%20as%20videos%20contain%20much%20richer%0Acontent%20than%20images%2C%20especially%20in%20terms%20of%20motion.%20This%20information%20can%20hardly%0Abe%20adequately%20described%20with%20plain%20text.%20Fortunately%2C%20in%20computer%20vision%2C%0Avarious%20visual%20representations%20can%20serve%20as%20additional%20control%20signals%20to%20guide%0Ageneration.%20With%20the%20help%20of%20these%20signals%2C%20video%20generation%20can%20be%20controlled%0Ain%20finer%20detail%2C%20allowing%20for%20greater%20flexibility%20for%20different%20applications.%0AIntegrating%20various%20controls%2C%20however%2C%20is%20nontrivial.%20In%20this%20paper%2C%20we%20propose%0Aa%20universal%20framework%20called%20EasyControl.%20By%20propagating%20and%20injecting%0Acondition%20features%20through%20condition%20adapters%2C%20our%20method%20enables%20users%20to%0Acontrol%20video%20generation%20with%20a%20single%20condition%20map.%20With%20our%20framework%2C%0Avarious%20conditions%20including%20raw%20pixels%2C%20depth%2C%20HED%2C%20etc.%2C%20can%20be%20integrated%0Ainto%20different%20Unet-based%20pre-trained%20video%20diffusion%20models%20at%20a%20low%20practical%0Acost.%20We%20conduct%20comprehensive%20experiments%20on%20public%20datasets%2C%20and%20both%0Aquantitative%20and%20qualitative%20results%20indicate%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods.%20EasyControl%20significantly%20improves%20various%20evaluation%0Ametrics%20across%20multiple%20validation%20datasets%20compared%20to%20previous%20works.%0ASpecifically%2C%20for%20the%20sketch-to-video%20generation%20task%2C%20EasyControl%20achieves%20an%0Aimprovement%20of%20152.0%20on%20FVD%20and%2019.9%20on%20IS%2C%20respectively%2C%20in%20UCF101%20compared%0Awith%20VideoComposer.%20For%20fidelity%2C%20our%20model%20demonstrates%20powerful%20image%0Aretention%20ability%2C%20resulting%20in%20high%20FVD%20and%20IS%20in%20UCF101%20and%20MSR-VTT%20compared%0Ato%20other%20image-to-video%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyControl%253A%2520Transfer%2520ControlNet%2520to%2520Video%2520Diffusion%2520for%2520Controllable%250A%2520%2520Generation%2520and%2520Interpolation%26entry.906535625%3DCong%2520Wang%2520and%2520Jiaxi%2520Gu%2520and%2520Panwen%2520Hu%2520and%2520Haoyu%2520Zhao%2520and%2520Yuanfan%2520Guo%2520and%2520Jianhua%2520Han%2520and%2520Hang%2520Xu%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Following%2520the%2520advancements%2520in%2520text-guided%2520image%2520generation%2520technology%250Aexemplified%2520by%2520Stable%2520Diffusion%252C%2520video%2520generation%2520is%2520gaining%2520increased%250Aattention%2520in%2520the%2520academic%2520community.%2520However%252C%2520relying%2520solely%2520on%2520text%2520guidance%250Afor%2520video%2520generation%2520has%2520serious%2520limitations%252C%2520as%2520videos%2520contain%2520much%2520richer%250Acontent%2520than%2520images%252C%2520especially%2520in%2520terms%2520of%2520motion.%2520This%2520information%2520can%2520hardly%250Abe%2520adequately%2520described%2520with%2520plain%2520text.%2520Fortunately%252C%2520in%2520computer%2520vision%252C%250Avarious%2520visual%2520representations%2520can%2520serve%2520as%2520additional%2520control%2520signals%2520to%2520guide%250Ageneration.%2520With%2520the%2520help%2520of%2520these%2520signals%252C%2520video%2520generation%2520can%2520be%2520controlled%250Ain%2520finer%2520detail%252C%2520allowing%2520for%2520greater%2520flexibility%2520for%2520different%2520applications.%250AIntegrating%2520various%2520controls%252C%2520however%252C%2520is%2520nontrivial.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520universal%2520framework%2520called%2520EasyControl.%2520By%2520propagating%2520and%2520injecting%250Acondition%2520features%2520through%2520condition%2520adapters%252C%2520our%2520method%2520enables%2520users%2520to%250Acontrol%2520video%2520generation%2520with%2520a%2520single%2520condition%2520map.%2520With%2520our%2520framework%252C%250Avarious%2520conditions%2520including%2520raw%2520pixels%252C%2520depth%252C%2520HED%252C%2520etc.%252C%2520can%2520be%2520integrated%250Ainto%2520different%2520Unet-based%2520pre-trained%2520video%2520diffusion%2520models%2520at%2520a%2520low%2520practical%250Acost.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520public%2520datasets%252C%2520and%2520both%250Aquantitative%2520and%2520qualitative%2520results%2520indicate%2520that%2520our%2520method%2520outperforms%250Astate-of-the-art%2520methods.%2520EasyControl%2520significantly%2520improves%2520various%2520evaluation%250Ametrics%2520across%2520multiple%2520validation%2520datasets%2520compared%2520to%2520previous%2520works.%250ASpecifically%252C%2520for%2520the%2520sketch-to-video%2520generation%2520task%252C%2520EasyControl%2520achieves%2520an%250Aimprovement%2520of%2520152.0%2520on%2520FVD%2520and%252019.9%2520on%2520IS%252C%2520respectively%252C%2520in%2520UCF101%2520compared%250Awith%2520VideoComposer.%2520For%2520fidelity%252C%2520our%2520model%2520demonstrates%2520powerful%2520image%250Aretention%2520ability%252C%2520resulting%2520in%2520high%2520FVD%2520and%2520IS%2520in%2520UCF101%2520and%2520MSR-VTT%2520compared%250Ato%2520other%2520image-to-video%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyControl%3A%20Transfer%20ControlNet%20to%20Video%20Diffusion%20for%20Controllable%0A%20%20Generation%20and%20Interpolation&entry.906535625=Cong%20Wang%20and%20Jiaxi%20Gu%20and%20Panwen%20Hu%20and%20Haoyu%20Zhao%20and%20Yuanfan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Following%20the%20advancements%20in%20text-guided%20image%20generation%20technology%0Aexemplified%20by%20Stable%20Diffusion%2C%20video%20generation%20is%20gaining%20increased%0Aattention%20in%20the%20academic%20community.%20However%2C%20relying%20solely%20on%20text%20guidance%0Afor%20video%20generation%20has%20serious%20limitations%2C%20as%20videos%20contain%20much%20richer%0Acontent%20than%20images%2C%20especially%20in%20terms%20of%20motion.%20This%20information%20can%20hardly%0Abe%20adequately%20described%20with%20plain%20text.%20Fortunately%2C%20in%20computer%20vision%2C%0Avarious%20visual%20representations%20can%20serve%20as%20additional%20control%20signals%20to%20guide%0Ageneration.%20With%20the%20help%20of%20these%20signals%2C%20video%20generation%20can%20be%20controlled%0Ain%20finer%20detail%2C%20allowing%20for%20greater%20flexibility%20for%20different%20applications.%0AIntegrating%20various%20controls%2C%20however%2C%20is%20nontrivial.%20In%20this%20paper%2C%20we%20propose%0Aa%20universal%20framework%20called%20EasyControl.%20By%20propagating%20and%20injecting%0Acondition%20features%20through%20condition%20adapters%2C%20our%20method%20enables%20users%20to%0Acontrol%20video%20generation%20with%20a%20single%20condition%20map.%20With%20our%20framework%2C%0Avarious%20conditions%20including%20raw%20pixels%2C%20depth%2C%20HED%2C%20etc.%2C%20can%20be%20integrated%0Ainto%20different%20Unet-based%20pre-trained%20video%20diffusion%20models%20at%20a%20low%20practical%0Acost.%20We%20conduct%20comprehensive%20experiments%20on%20public%20datasets%2C%20and%20both%0Aquantitative%20and%20qualitative%20results%20indicate%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods.%20EasyControl%20significantly%20improves%20various%20evaluation%0Ametrics%20across%20multiple%20validation%20datasets%20compared%20to%20previous%20works.%0ASpecifically%2C%20for%20the%20sketch-to-video%20generation%20task%2C%20EasyControl%20achieves%20an%0Aimprovement%20of%20152.0%20on%20FVD%20and%2019.9%20on%20IS%2C%20respectively%2C%20in%20UCF101%20compared%0Awith%20VideoComposer.%20For%20fidelity%2C%20our%20model%20demonstrates%20powerful%20image%0Aretention%20ability%2C%20resulting%20in%20high%20FVD%20and%20IS%20in%20UCF101%20and%20MSR-VTT%20compared%0Ato%20other%20image-to-video%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13005v1&entry.124074799=Read"},
{"title": "CathAction: A Benchmark for Endovascular Intervention Understanding", "author": "Baoru Huang and Tuan Vo and Chayun Kongtongvattana and Giulio Dagnino and Dennis Kundrat and Wenqiang Chi and Mohamed Abdelaziz and Trevor Kwok and Tudor Jianu and Tuong Do and Hieu Le and Minh Nguyen and Hoan Nguyen and Erman Tjiputra and Quang Tran and Jianyang Xie and Yanda Meng and Binod Bhattarai and Zhaorui Tan and Hongbin Liu and Hong Seng Gan and Wei Wang and Xi Yang and Qiufeng Wang and Jionglong Su and Kaizhu Huang and Angelos Stefanidis and Min Guo and Bo Du and Rong Tao and Minh Vu and Guoyan Zheng and Yalin Zheng and Francisco Vasconcelos and Danail Stoyanov and Daniel Elson and Ferdinando Rodriguez y Baena and Anh Nguyen", "abstract": "  Real-time visual feedback from catheterization analysis is crucial for\nenhancing surgical safety and efficiency during endovascular interventions.\nHowever, existing datasets are often limited to specific tasks, small scale,\nand lack the comprehensive annotations necessary for broader endovascular\nintervention understanding. To tackle these limitations, we introduce\nCathAction, a large-scale dataset for catheterization understanding. Our\nCathAction dataset encompasses approximately 500,000 annotated frames for\ncatheterization action understanding and collision detection, and 25,000 ground\ntruth masks for catheter and guidewire segmentation. For each task, we\nbenchmark recent related works in the field. We further discuss the challenges\nof endovascular intentions compared to traditional computer vision tasks and\npoint out open research questions. We hope that CathAction will facilitate the\ndevelopment of endovascular intervention understanding methods that can be\napplied to real-world applications. The dataset is available at\nhttps://airvlab.github.io/cathdata/.\n", "link": "http://arxiv.org/abs/2408.13126v1", "date": "2024-08-23", "relevancy": 2.5135, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5165}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CathAction%3A%20A%20Benchmark%20for%20Endovascular%20Intervention%20Understanding&body=Title%3A%20CathAction%3A%20A%20Benchmark%20for%20Endovascular%20Intervention%20Understanding%0AAuthor%3A%20Baoru%20Huang%20and%20Tuan%20Vo%20and%20Chayun%20Kongtongvattana%20and%20Giulio%20Dagnino%20and%20Dennis%20Kundrat%20and%20Wenqiang%20Chi%20and%20Mohamed%20Abdelaziz%20and%20Trevor%20Kwok%20and%20Tudor%20Jianu%20and%20Tuong%20Do%20and%20Hieu%20Le%20and%20Minh%20Nguyen%20and%20Hoan%20Nguyen%20and%20Erman%20Tjiputra%20and%20Quang%20Tran%20and%20Jianyang%20Xie%20and%20Yanda%20Meng%20and%20Binod%20Bhattarai%20and%20Zhaorui%20Tan%20and%20Hongbin%20Liu%20and%20Hong%20Seng%20Gan%20and%20Wei%20Wang%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Jionglong%20Su%20and%20Kaizhu%20Huang%20and%20Angelos%20Stefanidis%20and%20Min%20Guo%20and%20Bo%20Du%20and%20Rong%20Tao%20and%20Minh%20Vu%20and%20Guoyan%20Zheng%20and%20Yalin%20Zheng%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov%20and%20Daniel%20Elson%20and%20Ferdinando%20Rodriguez%20y%20Baena%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%20Real-time%20visual%20feedback%20from%20catheterization%20analysis%20is%20crucial%20for%0Aenhancing%20surgical%20safety%20and%20efficiency%20during%20endovascular%20interventions.%0AHowever%2C%20existing%20datasets%20are%20often%20limited%20to%20specific%20tasks%2C%20small%20scale%2C%0Aand%20lack%20the%20comprehensive%20annotations%20necessary%20for%20broader%20endovascular%0Aintervention%20understanding.%20To%20tackle%20these%20limitations%2C%20we%20introduce%0ACathAction%2C%20a%20large-scale%20dataset%20for%20catheterization%20understanding.%20Our%0ACathAction%20dataset%20encompasses%20approximately%20500%2C000%20annotated%20frames%20for%0Acatheterization%20action%20understanding%20and%20collision%20detection%2C%20and%2025%2C000%20ground%0Atruth%20masks%20for%20catheter%20and%20guidewire%20segmentation.%20For%20each%20task%2C%20we%0Abenchmark%20recent%20related%20works%20in%20the%20field.%20We%20further%20discuss%20the%20challenges%0Aof%20endovascular%20intentions%20compared%20to%20traditional%20computer%20vision%20tasks%20and%0Apoint%20out%20open%20research%20questions.%20We%20hope%20that%20CathAction%20will%20facilitate%20the%0Adevelopment%20of%20endovascular%20intervention%20understanding%20methods%20that%20can%20be%0Aapplied%20to%20real-world%20applications.%20The%20dataset%20is%20available%20at%0Ahttps%3A//airvlab.github.io/cathdata/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCathAction%253A%2520A%2520Benchmark%2520for%2520Endovascular%2520Intervention%2520Understanding%26entry.906535625%3DBaoru%2520Huang%2520and%2520Tuan%2520Vo%2520and%2520Chayun%2520Kongtongvattana%2520and%2520Giulio%2520Dagnino%2520and%2520Dennis%2520Kundrat%2520and%2520Wenqiang%2520Chi%2520and%2520Mohamed%2520Abdelaziz%2520and%2520Trevor%2520Kwok%2520and%2520Tudor%2520Jianu%2520and%2520Tuong%2520Do%2520and%2520Hieu%2520Le%2520and%2520Minh%2520Nguyen%2520and%2520Hoan%2520Nguyen%2520and%2520Erman%2520Tjiputra%2520and%2520Quang%2520Tran%2520and%2520Jianyang%2520Xie%2520and%2520Yanda%2520Meng%2520and%2520Binod%2520Bhattarai%2520and%2520Zhaorui%2520Tan%2520and%2520Hongbin%2520Liu%2520and%2520Hong%2520Seng%2520Gan%2520and%2520Wei%2520Wang%2520and%2520Xi%2520Yang%2520and%2520Qiufeng%2520Wang%2520and%2520Jionglong%2520Su%2520and%2520Kaizhu%2520Huang%2520and%2520Angelos%2520Stefanidis%2520and%2520Min%2520Guo%2520and%2520Bo%2520Du%2520and%2520Rong%2520Tao%2520and%2520Minh%2520Vu%2520and%2520Guoyan%2520Zheng%2520and%2520Yalin%2520Zheng%2520and%2520Francisco%2520Vasconcelos%2520and%2520Danail%2520Stoyanov%2520and%2520Daniel%2520Elson%2520and%2520Ferdinando%2520Rodriguez%2520y%2520Baena%2520and%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520Real-time%2520visual%2520feedback%2520from%2520catheterization%2520analysis%2520is%2520crucial%2520for%250Aenhancing%2520surgical%2520safety%2520and%2520efficiency%2520during%2520endovascular%2520interventions.%250AHowever%252C%2520existing%2520datasets%2520are%2520often%2520limited%2520to%2520specific%2520tasks%252C%2520small%2520scale%252C%250Aand%2520lack%2520the%2520comprehensive%2520annotations%2520necessary%2520for%2520broader%2520endovascular%250Aintervention%2520understanding.%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520introduce%250ACathAction%252C%2520a%2520large-scale%2520dataset%2520for%2520catheterization%2520understanding.%2520Our%250ACathAction%2520dataset%2520encompasses%2520approximately%2520500%252C000%2520annotated%2520frames%2520for%250Acatheterization%2520action%2520understanding%2520and%2520collision%2520detection%252C%2520and%252025%252C000%2520ground%250Atruth%2520masks%2520for%2520catheter%2520and%2520guidewire%2520segmentation.%2520For%2520each%2520task%252C%2520we%250Abenchmark%2520recent%2520related%2520works%2520in%2520the%2520field.%2520We%2520further%2520discuss%2520the%2520challenges%250Aof%2520endovascular%2520intentions%2520compared%2520to%2520traditional%2520computer%2520vision%2520tasks%2520and%250Apoint%2520out%2520open%2520research%2520questions.%2520We%2520hope%2520that%2520CathAction%2520will%2520facilitate%2520the%250Adevelopment%2520of%2520endovascular%2520intervention%2520understanding%2520methods%2520that%2520can%2520be%250Aapplied%2520to%2520real-world%2520applications.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//airvlab.github.io/cathdata/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CathAction%3A%20A%20Benchmark%20for%20Endovascular%20Intervention%20Understanding&entry.906535625=Baoru%20Huang%20and%20Tuan%20Vo%20and%20Chayun%20Kongtongvattana%20and%20Giulio%20Dagnino%20and%20Dennis%20Kundrat%20and%20Wenqiang%20Chi%20and%20Mohamed%20Abdelaziz%20and%20Trevor%20Kwok%20and%20Tudor%20Jianu%20and%20Tuong%20Do%20and%20Hieu%20Le%20and%20Minh%20Nguyen%20and%20Hoan%20Nguyen%20and%20Erman%20Tjiputra%20and%20Quang%20Tran%20and%20Jianyang%20Xie%20and%20Yanda%20Meng%20and%20Binod%20Bhattarai%20and%20Zhaorui%20Tan%20and%20Hongbin%20Liu%20and%20Hong%20Seng%20Gan%20and%20Wei%20Wang%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Jionglong%20Su%20and%20Kaizhu%20Huang%20and%20Angelos%20Stefanidis%20and%20Min%20Guo%20and%20Bo%20Du%20and%20Rong%20Tao%20and%20Minh%20Vu%20and%20Guoyan%20Zheng%20and%20Yalin%20Zheng%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov%20and%20Daniel%20Elson%20and%20Ferdinando%20Rodriguez%20y%20Baena%20and%20Anh%20Nguyen&entry.1292438233=%20%20Real-time%20visual%20feedback%20from%20catheterization%20analysis%20is%20crucial%20for%0Aenhancing%20surgical%20safety%20and%20efficiency%20during%20endovascular%20interventions.%0AHowever%2C%20existing%20datasets%20are%20often%20limited%20to%20specific%20tasks%2C%20small%20scale%2C%0Aand%20lack%20the%20comprehensive%20annotations%20necessary%20for%20broader%20endovascular%0Aintervention%20understanding.%20To%20tackle%20these%20limitations%2C%20we%20introduce%0ACathAction%2C%20a%20large-scale%20dataset%20for%20catheterization%20understanding.%20Our%0ACathAction%20dataset%20encompasses%20approximately%20500%2C000%20annotated%20frames%20for%0Acatheterization%20action%20understanding%20and%20collision%20detection%2C%20and%2025%2C000%20ground%0Atruth%20masks%20for%20catheter%20and%20guidewire%20segmentation.%20For%20each%20task%2C%20we%0Abenchmark%20recent%20related%20works%20in%20the%20field.%20We%20further%20discuss%20the%20challenges%0Aof%20endovascular%20intentions%20compared%20to%20traditional%20computer%20vision%20tasks%20and%0Apoint%20out%20open%20research%20questions.%20We%20hope%20that%20CathAction%20will%20facilitate%20the%0Adevelopment%20of%20endovascular%20intervention%20understanding%20methods%20that%20can%20be%0Aapplied%20to%20real-world%20applications.%20The%20dataset%20is%20available%20at%0Ahttps%3A//airvlab.github.io/cathdata/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13126v1&entry.124074799=Read"},
{"title": "Long-Term Pre-training for Temporal Action Detection with Transformers", "author": "Jihwan Kim and Miso Lee and Jae-Pil Heo", "abstract": "  Temporal action detection (TAD) is challenging, yet fundamental for\nreal-world video applications. Recently, DETR-based models for TAD have been\nprevailing thanks to their unique benefits. However, transformers demand a huge\ndataset, and unfortunately data scarcity in TAD causes a severe degeneration.\nIn this paper, we identify two crucial problems from data scarcity: attention\ncollapse and imbalanced performance. To this end, we propose a new pre-training\nstrategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two\nmain components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly,\nwe synthesize long-form video features by merging video snippets of a target\nclass and non-target classes. They are analogous to untrimmed data used in TAD,\ndespite being created from trimmed data. In addition, we devise two types of\nlong-term pretext tasks to learn long-term dependency. They impose long-term\nconditions such as finding second-to-fourth or short-duration actions. Our\nextensive experiments show state-of-the-art performances in DETR-based methods\non ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate\nthat LTP significantly relieves the data scarcity issues in TAD.\n", "link": "http://arxiv.org/abs/2408.13152v1", "date": "2024-08-23", "relevancy": 2.5132, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.7006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Term%20Pre-training%20for%20Temporal%20Action%20Detection%20with%20Transformers&body=Title%3A%20Long-Term%20Pre-training%20for%20Temporal%20Action%20Detection%20with%20Transformers%0AAuthor%3A%20Jihwan%20Kim%20and%20Miso%20Lee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Temporal%20action%20detection%20%28TAD%29%20is%20challenging%2C%20yet%20fundamental%20for%0Areal-world%20video%20applications.%20Recently%2C%20DETR-based%20models%20for%20TAD%20have%20been%0Aprevailing%20thanks%20to%20their%20unique%20benefits.%20However%2C%20transformers%20demand%20a%20huge%0Adataset%2C%20and%20unfortunately%20data%20scarcity%20in%20TAD%20causes%20a%20severe%20degeneration.%0AIn%20this%20paper%2C%20we%20identify%20two%20crucial%20problems%20from%20data%20scarcity%3A%20attention%0Acollapse%20and%20imbalanced%20performance.%20To%20this%20end%2C%20we%20propose%20a%20new%20pre-training%0Astrategy%2C%20Long-Term%20Pre-training%20%28LTP%29%2C%20tailored%20for%20transformers.%20LTP%20has%20two%0Amain%20components%3A%201%29%20class-wise%20synthesis%2C%202%29%20long-term%20pretext%20tasks.%20Firstly%2C%0Awe%20synthesize%20long-form%20video%20features%20by%20merging%20video%20snippets%20of%20a%20target%0Aclass%20and%20non-target%20classes.%20They%20are%20analogous%20to%20untrimmed%20data%20used%20in%20TAD%2C%0Adespite%20being%20created%20from%20trimmed%20data.%20In%20addition%2C%20we%20devise%20two%20types%20of%0Along-term%20pretext%20tasks%20to%20learn%20long-term%20dependency.%20They%20impose%20long-term%0Aconditions%20such%20as%20finding%20second-to-fourth%20or%20short-duration%20actions.%20Our%0Aextensive%20experiments%20show%20state-of-the-art%20performances%20in%20DETR-based%20methods%0Aon%20ActivityNet-v1.3%20and%20THUMOS14%20by%20a%20large%20margin.%20Moreover%2C%20we%20demonstrate%0Athat%20LTP%20significantly%20relieves%20the%20data%20scarcity%20issues%20in%20TAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Term%2520Pre-training%2520for%2520Temporal%2520Action%2520Detection%2520with%2520Transformers%26entry.906535625%3DJihwan%2520Kim%2520and%2520Miso%2520Lee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Temporal%2520action%2520detection%2520%2528TAD%2529%2520is%2520challenging%252C%2520yet%2520fundamental%2520for%250Areal-world%2520video%2520applications.%2520Recently%252C%2520DETR-based%2520models%2520for%2520TAD%2520have%2520been%250Aprevailing%2520thanks%2520to%2520their%2520unique%2520benefits.%2520However%252C%2520transformers%2520demand%2520a%2520huge%250Adataset%252C%2520and%2520unfortunately%2520data%2520scarcity%2520in%2520TAD%2520causes%2520a%2520severe%2520degeneration.%250AIn%2520this%2520paper%252C%2520we%2520identify%2520two%2520crucial%2520problems%2520from%2520data%2520scarcity%253A%2520attention%250Acollapse%2520and%2520imbalanced%2520performance.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520new%2520pre-training%250Astrategy%252C%2520Long-Term%2520Pre-training%2520%2528LTP%2529%252C%2520tailored%2520for%2520transformers.%2520LTP%2520has%2520two%250Amain%2520components%253A%25201%2529%2520class-wise%2520synthesis%252C%25202%2529%2520long-term%2520pretext%2520tasks.%2520Firstly%252C%250Awe%2520synthesize%2520long-form%2520video%2520features%2520by%2520merging%2520video%2520snippets%2520of%2520a%2520target%250Aclass%2520and%2520non-target%2520classes.%2520They%2520are%2520analogous%2520to%2520untrimmed%2520data%2520used%2520in%2520TAD%252C%250Adespite%2520being%2520created%2520from%2520trimmed%2520data.%2520In%2520addition%252C%2520we%2520devise%2520two%2520types%2520of%250Along-term%2520pretext%2520tasks%2520to%2520learn%2520long-term%2520dependency.%2520They%2520impose%2520long-term%250Aconditions%2520such%2520as%2520finding%2520second-to-fourth%2520or%2520short-duration%2520actions.%2520Our%250Aextensive%2520experiments%2520show%2520state-of-the-art%2520performances%2520in%2520DETR-based%2520methods%250Aon%2520ActivityNet-v1.3%2520and%2520THUMOS14%2520by%2520a%2520large%2520margin.%2520Moreover%252C%2520we%2520demonstrate%250Athat%2520LTP%2520significantly%2520relieves%2520the%2520data%2520scarcity%2520issues%2520in%2520TAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Term%20Pre-training%20for%20Temporal%20Action%20Detection%20with%20Transformers&entry.906535625=Jihwan%20Kim%20and%20Miso%20Lee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Temporal%20action%20detection%20%28TAD%29%20is%20challenging%2C%20yet%20fundamental%20for%0Areal-world%20video%20applications.%20Recently%2C%20DETR-based%20models%20for%20TAD%20have%20been%0Aprevailing%20thanks%20to%20their%20unique%20benefits.%20However%2C%20transformers%20demand%20a%20huge%0Adataset%2C%20and%20unfortunately%20data%20scarcity%20in%20TAD%20causes%20a%20severe%20degeneration.%0AIn%20this%20paper%2C%20we%20identify%20two%20crucial%20problems%20from%20data%20scarcity%3A%20attention%0Acollapse%20and%20imbalanced%20performance.%20To%20this%20end%2C%20we%20propose%20a%20new%20pre-training%0Astrategy%2C%20Long-Term%20Pre-training%20%28LTP%29%2C%20tailored%20for%20transformers.%20LTP%20has%20two%0Amain%20components%3A%201%29%20class-wise%20synthesis%2C%202%29%20long-term%20pretext%20tasks.%20Firstly%2C%0Awe%20synthesize%20long-form%20video%20features%20by%20merging%20video%20snippets%20of%20a%20target%0Aclass%20and%20non-target%20classes.%20They%20are%20analogous%20to%20untrimmed%20data%20used%20in%20TAD%2C%0Adespite%20being%20created%20from%20trimmed%20data.%20In%20addition%2C%20we%20devise%20two%20types%20of%0Along-term%20pretext%20tasks%20to%20learn%20long-term%20dependency.%20They%20impose%20long-term%0Aconditions%20such%20as%20finding%20second-to-fourth%20or%20short-duration%20actions.%20Our%0Aextensive%20experiments%20show%20state-of-the-art%20performances%20in%20DETR-based%20methods%0Aon%20ActivityNet-v1.3%20and%20THUMOS14%20by%20a%20large%20margin.%20Moreover%2C%20we%20demonstrate%0Athat%20LTP%20significantly%20relieves%20the%20data%20scarcity%20issues%20in%20TAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13152v1&entry.124074799=Read"},
{"title": "KonvLiNA: Integrating Kolmogorov-Arnold Network with Linear Nystr\u00f6m\n  Attention for feature fusion in Crop Field Detection", "author": "Haruna Yunusa and Qin Shiyin and Adamu Lawan and Abdulrahman Hamman Adama Chukkol", "abstract": "  Crop field detection is a critical component of precision agriculture,\nessential for optimizing resource allocation and enhancing agricultural\nproductivity. This study introduces KonvLiNA, a novel framework that integrates\nConvolutional Kolmogorov-Arnold Networks (cKAN) with Nystr\\\"om attention\nmechanisms for effective crop field detection. Leveraging KAN adaptive\nactivation functions and the efficiency of Nystr\\\"om attention in handling\nlargescale data, KonvLiNA significantly enhances feature extraction, enabling\nthe model to capture intricate patterns in complex agricultural environments.\nExperimental results on rice crop dataset demonstrate KonvLiNA superiority over\nstate-of-the-art methods, achieving a 0.415 AP and 0.459 AR with the Swin-L\nbackbone, outperforming traditional YOLOv8 by significant margins.\nAdditionally, evaluation on the COCO dataset showcases competitive performance\nacross small, medium, and large objects, highlighting KonvLiNA efficacy in\ndiverse agricultural settings. This work highlights the potential of hybrid KAN\nand attention mechanisms for advancing precision agriculture through improved\ncrop field detection and management.\n", "link": "http://arxiv.org/abs/2408.13160v1", "date": "2024-08-23", "relevancy": 2.4729, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5214}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4837}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KonvLiNA%3A%20Integrating%20Kolmogorov-Arnold%20Network%20with%20Linear%20Nystr%C3%B6m%0A%20%20Attention%20for%20feature%20fusion%20in%20Crop%20Field%20Detection&body=Title%3A%20KonvLiNA%3A%20Integrating%20Kolmogorov-Arnold%20Network%20with%20Linear%20Nystr%C3%B6m%0A%20%20Attention%20for%20feature%20fusion%20in%20Crop%20Field%20Detection%0AAuthor%3A%20Haruna%20Yunusa%20and%20Qin%20Shiyin%20and%20Adamu%20Lawan%20and%20Abdulrahman%20Hamman%20Adama%20Chukkol%0AAbstract%3A%20%20%20Crop%20field%20detection%20is%20a%20critical%20component%20of%20precision%20agriculture%2C%0Aessential%20for%20optimizing%20resource%20allocation%20and%20enhancing%20agricultural%0Aproductivity.%20This%20study%20introduces%20KonvLiNA%2C%20a%20novel%20framework%20that%20integrates%0AConvolutional%20Kolmogorov-Arnold%20Networks%20%28cKAN%29%20with%20Nystr%5C%22om%20attention%0Amechanisms%20for%20effective%20crop%20field%20detection.%20Leveraging%20KAN%20adaptive%0Aactivation%20functions%20and%20the%20efficiency%20of%20Nystr%5C%22om%20attention%20in%20handling%0Alargescale%20data%2C%20KonvLiNA%20significantly%20enhances%20feature%20extraction%2C%20enabling%0Athe%20model%20to%20capture%20intricate%20patterns%20in%20complex%20agricultural%20environments.%0AExperimental%20results%20on%20rice%20crop%20dataset%20demonstrate%20KonvLiNA%20superiority%20over%0Astate-of-the-art%20methods%2C%20achieving%20a%200.415%20AP%20and%200.459%20AR%20with%20the%20Swin-L%0Abackbone%2C%20outperforming%20traditional%20YOLOv8%20by%20significant%20margins.%0AAdditionally%2C%20evaluation%20on%20the%20COCO%20dataset%20showcases%20competitive%20performance%0Aacross%20small%2C%20medium%2C%20and%20large%20objects%2C%20highlighting%20KonvLiNA%20efficacy%20in%0Adiverse%20agricultural%20settings.%20This%20work%20highlights%20the%20potential%20of%20hybrid%20KAN%0Aand%20attention%20mechanisms%20for%20advancing%20precision%20agriculture%20through%20improved%0Acrop%20field%20detection%20and%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKonvLiNA%253A%2520Integrating%2520Kolmogorov-Arnold%2520Network%2520with%2520Linear%2520Nystr%25C3%25B6m%250A%2520%2520Attention%2520for%2520feature%2520fusion%2520in%2520Crop%2520Field%2520Detection%26entry.906535625%3DHaruna%2520Yunusa%2520and%2520Qin%2520Shiyin%2520and%2520Adamu%2520Lawan%2520and%2520Abdulrahman%2520Hamman%2520Adama%2520Chukkol%26entry.1292438233%3D%2520%2520Crop%2520field%2520detection%2520is%2520a%2520critical%2520component%2520of%2520precision%2520agriculture%252C%250Aessential%2520for%2520optimizing%2520resource%2520allocation%2520and%2520enhancing%2520agricultural%250Aproductivity.%2520This%2520study%2520introduces%2520KonvLiNA%252C%2520a%2520novel%2520framework%2520that%2520integrates%250AConvolutional%2520Kolmogorov-Arnold%2520Networks%2520%2528cKAN%2529%2520with%2520Nystr%255C%2522om%2520attention%250Amechanisms%2520for%2520effective%2520crop%2520field%2520detection.%2520Leveraging%2520KAN%2520adaptive%250Aactivation%2520functions%2520and%2520the%2520efficiency%2520of%2520Nystr%255C%2522om%2520attention%2520in%2520handling%250Alargescale%2520data%252C%2520KonvLiNA%2520significantly%2520enhances%2520feature%2520extraction%252C%2520enabling%250Athe%2520model%2520to%2520capture%2520intricate%2520patterns%2520in%2520complex%2520agricultural%2520environments.%250AExperimental%2520results%2520on%2520rice%2520crop%2520dataset%2520demonstrate%2520KonvLiNA%2520superiority%2520over%250Astate-of-the-art%2520methods%252C%2520achieving%2520a%25200.415%2520AP%2520and%25200.459%2520AR%2520with%2520the%2520Swin-L%250Abackbone%252C%2520outperforming%2520traditional%2520YOLOv8%2520by%2520significant%2520margins.%250AAdditionally%252C%2520evaluation%2520on%2520the%2520COCO%2520dataset%2520showcases%2520competitive%2520performance%250Aacross%2520small%252C%2520medium%252C%2520and%2520large%2520objects%252C%2520highlighting%2520KonvLiNA%2520efficacy%2520in%250Adiverse%2520agricultural%2520settings.%2520This%2520work%2520highlights%2520the%2520potential%2520of%2520hybrid%2520KAN%250Aand%2520attention%2520mechanisms%2520for%2520advancing%2520precision%2520agriculture%2520through%2520improved%250Acrop%2520field%2520detection%2520and%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KonvLiNA%3A%20Integrating%20Kolmogorov-Arnold%20Network%20with%20Linear%20Nystr%C3%B6m%0A%20%20Attention%20for%20feature%20fusion%20in%20Crop%20Field%20Detection&entry.906535625=Haruna%20Yunusa%20and%20Qin%20Shiyin%20and%20Adamu%20Lawan%20and%20Abdulrahman%20Hamman%20Adama%20Chukkol&entry.1292438233=%20%20Crop%20field%20detection%20is%20a%20critical%20component%20of%20precision%20agriculture%2C%0Aessential%20for%20optimizing%20resource%20allocation%20and%20enhancing%20agricultural%0Aproductivity.%20This%20study%20introduces%20KonvLiNA%2C%20a%20novel%20framework%20that%20integrates%0AConvolutional%20Kolmogorov-Arnold%20Networks%20%28cKAN%29%20with%20Nystr%5C%22om%20attention%0Amechanisms%20for%20effective%20crop%20field%20detection.%20Leveraging%20KAN%20adaptive%0Aactivation%20functions%20and%20the%20efficiency%20of%20Nystr%5C%22om%20attention%20in%20handling%0Alargescale%20data%2C%20KonvLiNA%20significantly%20enhances%20feature%20extraction%2C%20enabling%0Athe%20model%20to%20capture%20intricate%20patterns%20in%20complex%20agricultural%20environments.%0AExperimental%20results%20on%20rice%20crop%20dataset%20demonstrate%20KonvLiNA%20superiority%20over%0Astate-of-the-art%20methods%2C%20achieving%20a%200.415%20AP%20and%200.459%20AR%20with%20the%20Swin-L%0Abackbone%2C%20outperforming%20traditional%20YOLOv8%20by%20significant%20margins.%0AAdditionally%2C%20evaluation%20on%20the%20COCO%20dataset%20showcases%20competitive%20performance%0Aacross%20small%2C%20medium%2C%20and%20large%20objects%2C%20highlighting%20KonvLiNA%20efficacy%20in%0Adiverse%20agricultural%20settings.%20This%20work%20highlights%20the%20potential%20of%20hybrid%20KAN%0Aand%20attention%20mechanisms%20for%20advancing%20precision%20agriculture%20through%20improved%0Acrop%20field%20detection%20and%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13160v1&entry.124074799=Read"},
{"title": "Enhancing Training Efficiency Using Packing with Flash Attention", "author": "Achintya Kundu and Rhui Dih Lee and Laura Wynter and Raghu Kiran Ganti and Mayank Mishra", "abstract": "  Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has now been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing.\n", "link": "http://arxiv.org/abs/2407.09105v5", "date": "2024-08-23", "relevancy": 2.4566, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Training%20Efficiency%20Using%20Packing%20with%20Flash%20Attention&body=Title%3A%20Enhancing%20Training%20Efficiency%20Using%20Packing%20with%20Flash%20Attention%0AAuthor%3A%20Achintya%20Kundu%20and%20Rhui%20Dih%20Lee%20and%20Laura%20Wynter%20and%20Raghu%20Kiran%20Ganti%20and%20Mayank%20Mishra%0AAbstract%3A%20%20%20Padding%20is%20often%20used%20in%20tuning%20LLM%20models%20by%20adding%20special%20tokens%20to%0Ashorter%20training%20examples%20to%20match%20the%20length%20of%20the%20longest%20sequence%20in%20each%0Abatch.%20While%20this%20ensures%20uniformity%20for%20batch%20processing%2C%20it%20introduces%0Ainefficiencies%20by%20including%20irrelevant%20padding%20tokens%20in%20the%20computation%20and%0Awastes%20GPU%20resources.%20Hugging%20Face%20SFT%20trainer%20has%20always%20offered%20the%20option%20to%0Ause%20packing%20to%20combine%20multiple%20training%20examples%2C%20allowing%20for%20maximal%0Autilization%20of%20GPU%20resources.%20However%2C%20up%20till%20now%2C%20it%20did%20not%20offer%20proper%0Amasking%20of%20each%20packed%20training%20example.%20This%20capability%20has%20now%20been%20added%20to%0AHugging%20Face%20Transformers%204.44.%20We%20analyse%20this%20new%20feature%20and%20show%20the%0Abenefits%20across%20different%20variations%20of%20packing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09105v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Training%2520Efficiency%2520Using%2520Packing%2520with%2520Flash%2520Attention%26entry.906535625%3DAchintya%2520Kundu%2520and%2520Rhui%2520Dih%2520Lee%2520and%2520Laura%2520Wynter%2520and%2520Raghu%2520Kiran%2520Ganti%2520and%2520Mayank%2520Mishra%26entry.1292438233%3D%2520%2520Padding%2520is%2520often%2520used%2520in%2520tuning%2520LLM%2520models%2520by%2520adding%2520special%2520tokens%2520to%250Ashorter%2520training%2520examples%2520to%2520match%2520the%2520length%2520of%2520the%2520longest%2520sequence%2520in%2520each%250Abatch.%2520While%2520this%2520ensures%2520uniformity%2520for%2520batch%2520processing%252C%2520it%2520introduces%250Ainefficiencies%2520by%2520including%2520irrelevant%2520padding%2520tokens%2520in%2520the%2520computation%2520and%250Awastes%2520GPU%2520resources.%2520Hugging%2520Face%2520SFT%2520trainer%2520has%2520always%2520offered%2520the%2520option%2520to%250Ause%2520packing%2520to%2520combine%2520multiple%2520training%2520examples%252C%2520allowing%2520for%2520maximal%250Autilization%2520of%2520GPU%2520resources.%2520However%252C%2520up%2520till%2520now%252C%2520it%2520did%2520not%2520offer%2520proper%250Amasking%2520of%2520each%2520packed%2520training%2520example.%2520This%2520capability%2520has%2520now%2520been%2520added%2520to%250AHugging%2520Face%2520Transformers%25204.44.%2520We%2520analyse%2520this%2520new%2520feature%2520and%2520show%2520the%250Abenefits%2520across%2520different%2520variations%2520of%2520packing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09105v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Training%20Efficiency%20Using%20Packing%20with%20Flash%20Attention&entry.906535625=Achintya%20Kundu%20and%20Rhui%20Dih%20Lee%20and%20Laura%20Wynter%20and%20Raghu%20Kiran%20Ganti%20and%20Mayank%20Mishra&entry.1292438233=%20%20Padding%20is%20often%20used%20in%20tuning%20LLM%20models%20by%20adding%20special%20tokens%20to%0Ashorter%20training%20examples%20to%20match%20the%20length%20of%20the%20longest%20sequence%20in%20each%0Abatch.%20While%20this%20ensures%20uniformity%20for%20batch%20processing%2C%20it%20introduces%0Ainefficiencies%20by%20including%20irrelevant%20padding%20tokens%20in%20the%20computation%20and%0Awastes%20GPU%20resources.%20Hugging%20Face%20SFT%20trainer%20has%20always%20offered%20the%20option%20to%0Ause%20packing%20to%20combine%20multiple%20training%20examples%2C%20allowing%20for%20maximal%0Autilization%20of%20GPU%20resources.%20However%2C%20up%20till%20now%2C%20it%20did%20not%20offer%20proper%0Amasking%20of%20each%20packed%20training%20example.%20This%20capability%20has%20now%20been%20added%20to%0AHugging%20Face%20Transformers%204.44.%20We%20analyse%20this%20new%20feature%20and%20show%20the%0Abenefits%20across%20different%20variations%20of%20packing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09105v5&entry.124074799=Read"},
{"title": "Graph Classification with GNNs: Optimisation, Representation and\n  Inductive Bias", "author": "P. Krishna Kumar a and Harish G. Ramaswamy", "abstract": "  Theoretical studies on the representation power of GNNs have been centered\naround understanding the equivalence of GNNs, using WL-Tests for detecting\ngraph isomorphism. In this paper, we argue that such equivalence ignores the\naccompanying optimization issues and does not provide a holistic view of the\nGNN learning process. We illustrate these gaps between representation and\noptimization with examples and experiments. We also explore the existence of an\nimplicit inductive bias (e.g. fully connected networks prefer to learn low\nfrequency functions in their input space) in GNNs, in the context of graph\nclassification tasks. We further prove theoretically that the message-passing\nlayers in the graph, have a tendency to search for either discriminative\nsubgraphs, or a collection of discriminative nodes dispersed across the graph,\ndepending on the different global pooling layers used. We empirically verify\nthis bias through experiments over real-world and synthetic datasets. Finally,\nwe show how our work can help in incorporating domain knowledge via attention\nbased architectures, and can evince their capability to discriminate coherent\nsubgraphs.\n", "link": "http://arxiv.org/abs/2408.09266v2", "date": "2024-08-23", "relevancy": 2.4517, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5254}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5104}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Classification%20with%20GNNs%3A%20Optimisation%2C%20Representation%20and%0A%20%20Inductive%20Bias&body=Title%3A%20Graph%20Classification%20with%20GNNs%3A%20Optimisation%2C%20Representation%20and%0A%20%20Inductive%20Bias%0AAuthor%3A%20P.%20Krishna%20Kumar%20a%20and%20Harish%20G.%20Ramaswamy%0AAbstract%3A%20%20%20Theoretical%20studies%20on%20the%20representation%20power%20of%20GNNs%20have%20been%20centered%0Aaround%20understanding%20the%20equivalence%20of%20GNNs%2C%20using%20WL-Tests%20for%20detecting%0Agraph%20isomorphism.%20In%20this%20paper%2C%20we%20argue%20that%20such%20equivalence%20ignores%20the%0Aaccompanying%20optimization%20issues%20and%20does%20not%20provide%20a%20holistic%20view%20of%20the%0AGNN%20learning%20process.%20We%20illustrate%20these%20gaps%20between%20representation%20and%0Aoptimization%20with%20examples%20and%20experiments.%20We%20also%20explore%20the%20existence%20of%20an%0Aimplicit%20inductive%20bias%20%28e.g.%20fully%20connected%20networks%20prefer%20to%20learn%20low%0Afrequency%20functions%20in%20their%20input%20space%29%20in%20GNNs%2C%20in%20the%20context%20of%20graph%0Aclassification%20tasks.%20We%20further%20prove%20theoretically%20that%20the%20message-passing%0Alayers%20in%20the%20graph%2C%20have%20a%20tendency%20to%20search%20for%20either%20discriminative%0Asubgraphs%2C%20or%20a%20collection%20of%20discriminative%20nodes%20dispersed%20across%20the%20graph%2C%0Adepending%20on%20the%20different%20global%20pooling%20layers%20used.%20We%20empirically%20verify%0Athis%20bias%20through%20experiments%20over%20real-world%20and%20synthetic%20datasets.%20Finally%2C%0Awe%20show%20how%20our%20work%20can%20help%20in%20incorporating%20domain%20knowledge%20via%20attention%0Abased%20architectures%2C%20and%20can%20evince%20their%20capability%20to%20discriminate%20coherent%0Asubgraphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09266v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Classification%2520with%2520GNNs%253A%2520Optimisation%252C%2520Representation%2520and%250A%2520%2520Inductive%2520Bias%26entry.906535625%3DP.%2520Krishna%2520Kumar%2520a%2520and%2520Harish%2520G.%2520Ramaswamy%26entry.1292438233%3D%2520%2520Theoretical%2520studies%2520on%2520the%2520representation%2520power%2520of%2520GNNs%2520have%2520been%2520centered%250Aaround%2520understanding%2520the%2520equivalence%2520of%2520GNNs%252C%2520using%2520WL-Tests%2520for%2520detecting%250Agraph%2520isomorphism.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520such%2520equivalence%2520ignores%2520the%250Aaccompanying%2520optimization%2520issues%2520and%2520does%2520not%2520provide%2520a%2520holistic%2520view%2520of%2520the%250AGNN%2520learning%2520process.%2520We%2520illustrate%2520these%2520gaps%2520between%2520representation%2520and%250Aoptimization%2520with%2520examples%2520and%2520experiments.%2520We%2520also%2520explore%2520the%2520existence%2520of%2520an%250Aimplicit%2520inductive%2520bias%2520%2528e.g.%2520fully%2520connected%2520networks%2520prefer%2520to%2520learn%2520low%250Afrequency%2520functions%2520in%2520their%2520input%2520space%2529%2520in%2520GNNs%252C%2520in%2520the%2520context%2520of%2520graph%250Aclassification%2520tasks.%2520We%2520further%2520prove%2520theoretically%2520that%2520the%2520message-passing%250Alayers%2520in%2520the%2520graph%252C%2520have%2520a%2520tendency%2520to%2520search%2520for%2520either%2520discriminative%250Asubgraphs%252C%2520or%2520a%2520collection%2520of%2520discriminative%2520nodes%2520dispersed%2520across%2520the%2520graph%252C%250Adepending%2520on%2520the%2520different%2520global%2520pooling%2520layers%2520used.%2520We%2520empirically%2520verify%250Athis%2520bias%2520through%2520experiments%2520over%2520real-world%2520and%2520synthetic%2520datasets.%2520Finally%252C%250Awe%2520show%2520how%2520our%2520work%2520can%2520help%2520in%2520incorporating%2520domain%2520knowledge%2520via%2520attention%250Abased%2520architectures%252C%2520and%2520can%2520evince%2520their%2520capability%2520to%2520discriminate%2520coherent%250Asubgraphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09266v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Classification%20with%20GNNs%3A%20Optimisation%2C%20Representation%20and%0A%20%20Inductive%20Bias&entry.906535625=P.%20Krishna%20Kumar%20a%20and%20Harish%20G.%20Ramaswamy&entry.1292438233=%20%20Theoretical%20studies%20on%20the%20representation%20power%20of%20GNNs%20have%20been%20centered%0Aaround%20understanding%20the%20equivalence%20of%20GNNs%2C%20using%20WL-Tests%20for%20detecting%0Agraph%20isomorphism.%20In%20this%20paper%2C%20we%20argue%20that%20such%20equivalence%20ignores%20the%0Aaccompanying%20optimization%20issues%20and%20does%20not%20provide%20a%20holistic%20view%20of%20the%0AGNN%20learning%20process.%20We%20illustrate%20these%20gaps%20between%20representation%20and%0Aoptimization%20with%20examples%20and%20experiments.%20We%20also%20explore%20the%20existence%20of%20an%0Aimplicit%20inductive%20bias%20%28e.g.%20fully%20connected%20networks%20prefer%20to%20learn%20low%0Afrequency%20functions%20in%20their%20input%20space%29%20in%20GNNs%2C%20in%20the%20context%20of%20graph%0Aclassification%20tasks.%20We%20further%20prove%20theoretically%20that%20the%20message-passing%0Alayers%20in%20the%20graph%2C%20have%20a%20tendency%20to%20search%20for%20either%20discriminative%0Asubgraphs%2C%20or%20a%20collection%20of%20discriminative%20nodes%20dispersed%20across%20the%20graph%2C%0Adepending%20on%20the%20different%20global%20pooling%20layers%20used.%20We%20empirically%20verify%0Athis%20bias%20through%20experiments%20over%20real-world%20and%20synthetic%20datasets.%20Finally%2C%0Awe%20show%20how%20our%20work%20can%20help%20in%20incorporating%20domain%20knowledge%20via%20attention%0Abased%20architectures%2C%20and%20can%20evince%20their%20capability%20to%20discriminate%20coherent%0Asubgraphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09266v2&entry.124074799=Read"},
{"title": "Say No to Freeloader: Protecting Intellectual Property of Your Deep\n  Model", "author": "Lianyu Wang and Meng Wang and Huazhu Fu and Daoqiang Zhang", "abstract": "  Model intellectual property (IP) protection has attracted growing attention\nas science and technology advancements stem from human intellectual labor and\ncomputational expenses. Ensuring IP safety for trainers and owners is of utmost\nimportance, particularly in domains where ownership verification and\napplicability authorization are required. A notable approach to safeguarding\nmodel IP involves proactively preventing the use of well-trained models of\nauthorized domains from unauthorized domains. In this paper, we introduce a\nnovel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which\nserves as a barrier against illegal transfers from authorized to unauthorized\ndomains. Drawing inspiration from human transitive inference and learning\nabilities, the CUPI-Domain is designed to obstruct cross-domain transfers by\nemphasizing the distinctive style features of the authorized domain. This\nemphasis leads to failure in recognizing irrelevant private style features on\nunauthorized domains. To this end, we propose novel CUPI-Domain generators,\nwhich select features from both authorized and CUPI-Domain as anchors. Then, we\nfuse the style features and semantic features of these anchors to generate\nlabeled and style-rich CUPI-Domain. Additionally, we design external\nDomain-Information Memory Banks (DIMB) for storing and updating labeled pyramid\nfeatures to obtain stable domain class features and domain class-wise style\nfeatures. Based on the proposed whole method, the novel style and\ndiscriminative loss functions are designed to effectively enhance the\ndistinction in style and discriminative features between authorized and\nunauthorized domains, respectively. Moreover, we provide two solutions for\nutilizing CUPI-Domain based on whether the unauthorized domain is known:\ntarget-specified CUPI-Domain and target-free CUPI-Domain.\n", "link": "http://arxiv.org/abs/2408.13161v1", "date": "2024-08-23", "relevancy": 2.3967, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5009}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4731}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Say%20No%20to%20Freeloader%3A%20Protecting%20Intellectual%20Property%20of%20Your%20Deep%0A%20%20Model&body=Title%3A%20Say%20No%20to%20Freeloader%3A%20Protecting%20Intellectual%20Property%20of%20Your%20Deep%0A%20%20Model%0AAuthor%3A%20Lianyu%20Wang%20and%20Meng%20Wang%20and%20Huazhu%20Fu%20and%20Daoqiang%20Zhang%0AAbstract%3A%20%20%20Model%20intellectual%20property%20%28IP%29%20protection%20has%20attracted%20growing%20attention%0Aas%20science%20and%20technology%20advancements%20stem%20from%20human%20intellectual%20labor%20and%0Acomputational%20expenses.%20Ensuring%20IP%20safety%20for%20trainers%20and%20owners%20is%20of%20utmost%0Aimportance%2C%20particularly%20in%20domains%20where%20ownership%20verification%20and%0Aapplicability%20authorization%20are%20required.%20A%20notable%20approach%20to%20safeguarding%0Amodel%20IP%20involves%20proactively%20preventing%20the%20use%20of%20well-trained%20models%20of%0Aauthorized%20domains%20from%20unauthorized%20domains.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20Compact%20Un-transferable%20Pyramid%20Isolation%20Domain%20%28CUPI-Domain%29%20which%0Aserves%20as%20a%20barrier%20against%20illegal%20transfers%20from%20authorized%20to%20unauthorized%0Adomains.%20Drawing%20inspiration%20from%20human%20transitive%20inference%20and%20learning%0Aabilities%2C%20the%20CUPI-Domain%20is%20designed%20to%20obstruct%20cross-domain%20transfers%20by%0Aemphasizing%20the%20distinctive%20style%20features%20of%20the%20authorized%20domain.%20This%0Aemphasis%20leads%20to%20failure%20in%20recognizing%20irrelevant%20private%20style%20features%20on%0Aunauthorized%20domains.%20To%20this%20end%2C%20we%20propose%20novel%20CUPI-Domain%20generators%2C%0Awhich%20select%20features%20from%20both%20authorized%20and%20CUPI-Domain%20as%20anchors.%20Then%2C%20we%0Afuse%20the%20style%20features%20and%20semantic%20features%20of%20these%20anchors%20to%20generate%0Alabeled%20and%20style-rich%20CUPI-Domain.%20Additionally%2C%20we%20design%20external%0ADomain-Information%20Memory%20Banks%20%28DIMB%29%20for%20storing%20and%20updating%20labeled%20pyramid%0Afeatures%20to%20obtain%20stable%20domain%20class%20features%20and%20domain%20class-wise%20style%0Afeatures.%20Based%20on%20the%20proposed%20whole%20method%2C%20the%20novel%20style%20and%0Adiscriminative%20loss%20functions%20are%20designed%20to%20effectively%20enhance%20the%0Adistinction%20in%20style%20and%20discriminative%20features%20between%20authorized%20and%0Aunauthorized%20domains%2C%20respectively.%20Moreover%2C%20we%20provide%20two%20solutions%20for%0Autilizing%20CUPI-Domain%20based%20on%20whether%20the%20unauthorized%20domain%20is%20known%3A%0Atarget-specified%20CUPI-Domain%20and%20target-free%20CUPI-Domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSay%2520No%2520to%2520Freeloader%253A%2520Protecting%2520Intellectual%2520Property%2520of%2520Your%2520Deep%250A%2520%2520Model%26entry.906535625%3DLianyu%2520Wang%2520and%2520Meng%2520Wang%2520and%2520Huazhu%2520Fu%2520and%2520Daoqiang%2520Zhang%26entry.1292438233%3D%2520%2520Model%2520intellectual%2520property%2520%2528IP%2529%2520protection%2520has%2520attracted%2520growing%2520attention%250Aas%2520science%2520and%2520technology%2520advancements%2520stem%2520from%2520human%2520intellectual%2520labor%2520and%250Acomputational%2520expenses.%2520Ensuring%2520IP%2520safety%2520for%2520trainers%2520and%2520owners%2520is%2520of%2520utmost%250Aimportance%252C%2520particularly%2520in%2520domains%2520where%2520ownership%2520verification%2520and%250Aapplicability%2520authorization%2520are%2520required.%2520A%2520notable%2520approach%2520to%2520safeguarding%250Amodel%2520IP%2520involves%2520proactively%2520preventing%2520the%2520use%2520of%2520well-trained%2520models%2520of%250Aauthorized%2520domains%2520from%2520unauthorized%2520domains.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anovel%2520Compact%2520Un-transferable%2520Pyramid%2520Isolation%2520Domain%2520%2528CUPI-Domain%2529%2520which%250Aserves%2520as%2520a%2520barrier%2520against%2520illegal%2520transfers%2520from%2520authorized%2520to%2520unauthorized%250Adomains.%2520Drawing%2520inspiration%2520from%2520human%2520transitive%2520inference%2520and%2520learning%250Aabilities%252C%2520the%2520CUPI-Domain%2520is%2520designed%2520to%2520obstruct%2520cross-domain%2520transfers%2520by%250Aemphasizing%2520the%2520distinctive%2520style%2520features%2520of%2520the%2520authorized%2520domain.%2520This%250Aemphasis%2520leads%2520to%2520failure%2520in%2520recognizing%2520irrelevant%2520private%2520style%2520features%2520on%250Aunauthorized%2520domains.%2520To%2520this%2520end%252C%2520we%2520propose%2520novel%2520CUPI-Domain%2520generators%252C%250Awhich%2520select%2520features%2520from%2520both%2520authorized%2520and%2520CUPI-Domain%2520as%2520anchors.%2520Then%252C%2520we%250Afuse%2520the%2520style%2520features%2520and%2520semantic%2520features%2520of%2520these%2520anchors%2520to%2520generate%250Alabeled%2520and%2520style-rich%2520CUPI-Domain.%2520Additionally%252C%2520we%2520design%2520external%250ADomain-Information%2520Memory%2520Banks%2520%2528DIMB%2529%2520for%2520storing%2520and%2520updating%2520labeled%2520pyramid%250Afeatures%2520to%2520obtain%2520stable%2520domain%2520class%2520features%2520and%2520domain%2520class-wise%2520style%250Afeatures.%2520Based%2520on%2520the%2520proposed%2520whole%2520method%252C%2520the%2520novel%2520style%2520and%250Adiscriminative%2520loss%2520functions%2520are%2520designed%2520to%2520effectively%2520enhance%2520the%250Adistinction%2520in%2520style%2520and%2520discriminative%2520features%2520between%2520authorized%2520and%250Aunauthorized%2520domains%252C%2520respectively.%2520Moreover%252C%2520we%2520provide%2520two%2520solutions%2520for%250Autilizing%2520CUPI-Domain%2520based%2520on%2520whether%2520the%2520unauthorized%2520domain%2520is%2520known%253A%250Atarget-specified%2520CUPI-Domain%2520and%2520target-free%2520CUPI-Domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Say%20No%20to%20Freeloader%3A%20Protecting%20Intellectual%20Property%20of%20Your%20Deep%0A%20%20Model&entry.906535625=Lianyu%20Wang%20and%20Meng%20Wang%20and%20Huazhu%20Fu%20and%20Daoqiang%20Zhang&entry.1292438233=%20%20Model%20intellectual%20property%20%28IP%29%20protection%20has%20attracted%20growing%20attention%0Aas%20science%20and%20technology%20advancements%20stem%20from%20human%20intellectual%20labor%20and%0Acomputational%20expenses.%20Ensuring%20IP%20safety%20for%20trainers%20and%20owners%20is%20of%20utmost%0Aimportance%2C%20particularly%20in%20domains%20where%20ownership%20verification%20and%0Aapplicability%20authorization%20are%20required.%20A%20notable%20approach%20to%20safeguarding%0Amodel%20IP%20involves%20proactively%20preventing%20the%20use%20of%20well-trained%20models%20of%0Aauthorized%20domains%20from%20unauthorized%20domains.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20Compact%20Un-transferable%20Pyramid%20Isolation%20Domain%20%28CUPI-Domain%29%20which%0Aserves%20as%20a%20barrier%20against%20illegal%20transfers%20from%20authorized%20to%20unauthorized%0Adomains.%20Drawing%20inspiration%20from%20human%20transitive%20inference%20and%20learning%0Aabilities%2C%20the%20CUPI-Domain%20is%20designed%20to%20obstruct%20cross-domain%20transfers%20by%0Aemphasizing%20the%20distinctive%20style%20features%20of%20the%20authorized%20domain.%20This%0Aemphasis%20leads%20to%20failure%20in%20recognizing%20irrelevant%20private%20style%20features%20on%0Aunauthorized%20domains.%20To%20this%20end%2C%20we%20propose%20novel%20CUPI-Domain%20generators%2C%0Awhich%20select%20features%20from%20both%20authorized%20and%20CUPI-Domain%20as%20anchors.%20Then%2C%20we%0Afuse%20the%20style%20features%20and%20semantic%20features%20of%20these%20anchors%20to%20generate%0Alabeled%20and%20style-rich%20CUPI-Domain.%20Additionally%2C%20we%20design%20external%0ADomain-Information%20Memory%20Banks%20%28DIMB%29%20for%20storing%20and%20updating%20labeled%20pyramid%0Afeatures%20to%20obtain%20stable%20domain%20class%20features%20and%20domain%20class-wise%20style%0Afeatures.%20Based%20on%20the%20proposed%20whole%20method%2C%20the%20novel%20style%20and%0Adiscriminative%20loss%20functions%20are%20designed%20to%20effectively%20enhance%20the%0Adistinction%20in%20style%20and%20discriminative%20features%20between%20authorized%20and%0Aunauthorized%20domains%2C%20respectively.%20Moreover%2C%20we%20provide%20two%20solutions%20for%0Autilizing%20CUPI-Domain%20based%20on%20whether%20the%20unauthorized%20domain%20is%20known%3A%0Atarget-specified%20CUPI-Domain%20and%20target-free%20CUPI-Domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13161v1&entry.124074799=Read"},
{"title": "Multivariate Time-Series Anomaly Detection based on Enhancing Graph\n  Attention Networks with Topological Analysis", "author": "Zhe Liu and Xiang Huang and Jingyun Zhang and Zhifeng Hao and Li Sun and Hao Peng", "abstract": "  Unsupervised anomaly detection in time series is essential in industrial\napplications, as it significantly reduces the need for manual intervention.\nMultivariate time series pose a complex challenge due to their feature and\ntemporal dimensions. Traditional methods use Graph Neural Networks (GNNs) or\nTransformers to analyze spatial while RNNs to model temporal dependencies.\nThese methods focus narrowly on one dimension or engage in coarse-grained\nfeature extraction, which can be inadequate for large datasets characterized by\nintricate relationships and dynamic changes. This paper introduces a novel\ntemporal model built on an enhanced Graph Attention Network (GAT) for\nmultivariate time series anomaly detection called TopoGDN. Our model analyzes\nboth time and feature dimensions from a fine-grained perspective. First, we\nintroduce a multi-scale temporal convolution module to extract detailed\ntemporal features. Additionally, we present an augmented GAT to manage complex\ninter-feature dependencies, which incorporates graph topology into node\nfeatures across multiple scales, a versatile, plug-and-play enhancement that\nsignificantly boosts the performance of GAT. Our experimental results confirm\nthat our approach surpasses the baseline models on four datasets, demonstrating\nits potential for widespread application in fields requiring robust anomaly\ndetection. The code is available at https://github.com/ljj-cyber/TopoGDN.\n", "link": "http://arxiv.org/abs/2408.13082v1", "date": "2024-08-23", "relevancy": 2.3813, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4854}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4748}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multivariate%20Time-Series%20Anomaly%20Detection%20based%20on%20Enhancing%20Graph%0A%20%20Attention%20Networks%20with%20Topological%20Analysis&body=Title%3A%20Multivariate%20Time-Series%20Anomaly%20Detection%20based%20on%20Enhancing%20Graph%0A%20%20Attention%20Networks%20with%20Topological%20Analysis%0AAuthor%3A%20Zhe%20Liu%20and%20Xiang%20Huang%20and%20Jingyun%20Zhang%20and%20Zhifeng%20Hao%20and%20Li%20Sun%20and%20Hao%20Peng%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20in%20time%20series%20is%20essential%20in%20industrial%0Aapplications%2C%20as%20it%20significantly%20reduces%20the%20need%20for%20manual%20intervention.%0AMultivariate%20time%20series%20pose%20a%20complex%20challenge%20due%20to%20their%20feature%20and%0Atemporal%20dimensions.%20Traditional%20methods%20use%20Graph%20Neural%20Networks%20%28GNNs%29%20or%0ATransformers%20to%20analyze%20spatial%20while%20RNNs%20to%20model%20temporal%20dependencies.%0AThese%20methods%20focus%20narrowly%20on%20one%20dimension%20or%20engage%20in%20coarse-grained%0Afeature%20extraction%2C%20which%20can%20be%20inadequate%20for%20large%20datasets%20characterized%20by%0Aintricate%20relationships%20and%20dynamic%20changes.%20This%20paper%20introduces%20a%20novel%0Atemporal%20model%20built%20on%20an%20enhanced%20Graph%20Attention%20Network%20%28GAT%29%20for%0Amultivariate%20time%20series%20anomaly%20detection%20called%20TopoGDN.%20Our%20model%20analyzes%0Aboth%20time%20and%20feature%20dimensions%20from%20a%20fine-grained%20perspective.%20First%2C%20we%0Aintroduce%20a%20multi-scale%20temporal%20convolution%20module%20to%20extract%20detailed%0Atemporal%20features.%20Additionally%2C%20we%20present%20an%20augmented%20GAT%20to%20manage%20complex%0Ainter-feature%20dependencies%2C%20which%20incorporates%20graph%20topology%20into%20node%0Afeatures%20across%20multiple%20scales%2C%20a%20versatile%2C%20plug-and-play%20enhancement%20that%0Asignificantly%20boosts%20the%20performance%20of%20GAT.%20Our%20experimental%20results%20confirm%0Athat%20our%20approach%20surpasses%20the%20baseline%20models%20on%20four%20datasets%2C%20demonstrating%0Aits%20potential%20for%20widespread%20application%20in%20fields%20requiring%20robust%20anomaly%0Adetection.%20The%20code%20is%20available%20at%20https%3A//github.com/ljj-cyber/TopoGDN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultivariate%2520Time-Series%2520Anomaly%2520Detection%2520based%2520on%2520Enhancing%2520Graph%250A%2520%2520Attention%2520Networks%2520with%2520Topological%2520Analysis%26entry.906535625%3DZhe%2520Liu%2520and%2520Xiang%2520Huang%2520and%2520Jingyun%2520Zhang%2520and%2520Zhifeng%2520Hao%2520and%2520Li%2520Sun%2520and%2520Hao%2520Peng%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520in%2520time%2520series%2520is%2520essential%2520in%2520industrial%250Aapplications%252C%2520as%2520it%2520significantly%2520reduces%2520the%2520need%2520for%2520manual%2520intervention.%250AMultivariate%2520time%2520series%2520pose%2520a%2520complex%2520challenge%2520due%2520to%2520their%2520feature%2520and%250Atemporal%2520dimensions.%2520Traditional%2520methods%2520use%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520or%250ATransformers%2520to%2520analyze%2520spatial%2520while%2520RNNs%2520to%2520model%2520temporal%2520dependencies.%250AThese%2520methods%2520focus%2520narrowly%2520on%2520one%2520dimension%2520or%2520engage%2520in%2520coarse-grained%250Afeature%2520extraction%252C%2520which%2520can%2520be%2520inadequate%2520for%2520large%2520datasets%2520characterized%2520by%250Aintricate%2520relationships%2520and%2520dynamic%2520changes.%2520This%2520paper%2520introduces%2520a%2520novel%250Atemporal%2520model%2520built%2520on%2520an%2520enhanced%2520Graph%2520Attention%2520Network%2520%2528GAT%2529%2520for%250Amultivariate%2520time%2520series%2520anomaly%2520detection%2520called%2520TopoGDN.%2520Our%2520model%2520analyzes%250Aboth%2520time%2520and%2520feature%2520dimensions%2520from%2520a%2520fine-grained%2520perspective.%2520First%252C%2520we%250Aintroduce%2520a%2520multi-scale%2520temporal%2520convolution%2520module%2520to%2520extract%2520detailed%250Atemporal%2520features.%2520Additionally%252C%2520we%2520present%2520an%2520augmented%2520GAT%2520to%2520manage%2520complex%250Ainter-feature%2520dependencies%252C%2520which%2520incorporates%2520graph%2520topology%2520into%2520node%250Afeatures%2520across%2520multiple%2520scales%252C%2520a%2520versatile%252C%2520plug-and-play%2520enhancement%2520that%250Asignificantly%2520boosts%2520the%2520performance%2520of%2520GAT.%2520Our%2520experimental%2520results%2520confirm%250Athat%2520our%2520approach%2520surpasses%2520the%2520baseline%2520models%2520on%2520four%2520datasets%252C%2520demonstrating%250Aits%2520potential%2520for%2520widespread%2520application%2520in%2520fields%2520requiring%2520robust%2520anomaly%250Adetection.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/ljj-cyber/TopoGDN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multivariate%20Time-Series%20Anomaly%20Detection%20based%20on%20Enhancing%20Graph%0A%20%20Attention%20Networks%20with%20Topological%20Analysis&entry.906535625=Zhe%20Liu%20and%20Xiang%20Huang%20and%20Jingyun%20Zhang%20and%20Zhifeng%20Hao%20and%20Li%20Sun%20and%20Hao%20Peng&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20in%20time%20series%20is%20essential%20in%20industrial%0Aapplications%2C%20as%20it%20significantly%20reduces%20the%20need%20for%20manual%20intervention.%0AMultivariate%20time%20series%20pose%20a%20complex%20challenge%20due%20to%20their%20feature%20and%0Atemporal%20dimensions.%20Traditional%20methods%20use%20Graph%20Neural%20Networks%20%28GNNs%29%20or%0ATransformers%20to%20analyze%20spatial%20while%20RNNs%20to%20model%20temporal%20dependencies.%0AThese%20methods%20focus%20narrowly%20on%20one%20dimension%20or%20engage%20in%20coarse-grained%0Afeature%20extraction%2C%20which%20can%20be%20inadequate%20for%20large%20datasets%20characterized%20by%0Aintricate%20relationships%20and%20dynamic%20changes.%20This%20paper%20introduces%20a%20novel%0Atemporal%20model%20built%20on%20an%20enhanced%20Graph%20Attention%20Network%20%28GAT%29%20for%0Amultivariate%20time%20series%20anomaly%20detection%20called%20TopoGDN.%20Our%20model%20analyzes%0Aboth%20time%20and%20feature%20dimensions%20from%20a%20fine-grained%20perspective.%20First%2C%20we%0Aintroduce%20a%20multi-scale%20temporal%20convolution%20module%20to%20extract%20detailed%0Atemporal%20features.%20Additionally%2C%20we%20present%20an%20augmented%20GAT%20to%20manage%20complex%0Ainter-feature%20dependencies%2C%20which%20incorporates%20graph%20topology%20into%20node%0Afeatures%20across%20multiple%20scales%2C%20a%20versatile%2C%20plug-and-play%20enhancement%20that%0Asignificantly%20boosts%20the%20performance%20of%20GAT.%20Our%20experimental%20results%20confirm%0Athat%20our%20approach%20surpasses%20the%20baseline%20models%20on%20four%20datasets%2C%20demonstrating%0Aits%20potential%20for%20widespread%20application%20in%20fields%20requiring%20robust%20anomaly%0Adetection.%20The%20code%20is%20available%20at%20https%3A//github.com/ljj-cyber/TopoGDN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13082v1&entry.124074799=Read"},
{"title": "Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding", "author": "Xianqiang Gao and Pingrui Zhang and Delin Qu and Dong Wang and Zhigang Wang and Yan Ding and Bin Zhao and Xuelong Li", "abstract": "  3D Object Affordance Grounding aims to predict the functional regions on a 3D\nobject and has laid the foundation for a wide range of applications in\nrobotics. Recent advances tackle this problem via learning a mapping between 3D\nregions and a single human-object interaction image. However, the geometric\nstructure of the 3D object and the object in the human-object interaction image\nare not always consistent, leading to poor generalization. To address this\nissue, we propose to learn generalizable invariant affordance knowledge from\nmultiple human-object interaction images within the same affordance category.\nSpecifically, we introduce the \\textbf{M}ulti-\\textbf{I}mage Guided\nInvariant-\\textbf{F}eature-Aware 3D \\textbf{A}ffordance \\textbf{G}rounding\n(\\textbf{MIFAG}) framework. It grounds 3D object affordance regions by\nidentifying common interaction patterns across multiple human-object\ninteraction images. First, the Invariant Affordance Knowledge Extraction Module\n(\\textbf{IAM}) utilizes an iterative updating strategy to gradually extract\naligned affordance knowledge from multiple images and integrate it into an\naffordance dictionary. Then, the Affordance Dictionary Adaptive Fusion Module\n(\\textbf{ADM}) learns comprehensive point cloud representations that consider\nall affordance candidates in multiple images. Besides, the Multi-Image and\nPoint Affordance (\\textbf{MIPA}) benchmark is constructed and our method\noutperforms existing state-of-the-art methods on various experimental\ncomparisons. Project page: \\url{https://goxq.github.io/mifag}\n", "link": "http://arxiv.org/abs/2408.13024v1", "date": "2024-08-23", "relevancy": 2.3363, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5945}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5794}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%202D%20Invariant%20Affordance%20Knowledge%20for%203D%20Affordance%20Grounding&body=Title%3A%20Learning%202D%20Invariant%20Affordance%20Knowledge%20for%203D%20Affordance%20Grounding%0AAuthor%3A%20Xianqiang%20Gao%20and%20Pingrui%20Zhang%20and%20Delin%20Qu%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Yan%20Ding%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%203D%20Object%20Affordance%20Grounding%20aims%20to%20predict%20the%20functional%20regions%20on%20a%203D%0Aobject%20and%20has%20laid%20the%20foundation%20for%20a%20wide%20range%20of%20applications%20in%0Arobotics.%20Recent%20advances%20tackle%20this%20problem%20via%20learning%20a%20mapping%20between%203D%0Aregions%20and%20a%20single%20human-object%20interaction%20image.%20However%2C%20the%20geometric%0Astructure%20of%20the%203D%20object%20and%20the%20object%20in%20the%20human-object%20interaction%20image%0Aare%20not%20always%20consistent%2C%20leading%20to%20poor%20generalization.%20To%20address%20this%0Aissue%2C%20we%20propose%20to%20learn%20generalizable%20invariant%20affordance%20knowledge%20from%0Amultiple%20human-object%20interaction%20images%20within%20the%20same%20affordance%20category.%0ASpecifically%2C%20we%20introduce%20the%20%5Ctextbf%7BM%7Dulti-%5Ctextbf%7BI%7Dmage%20Guided%0AInvariant-%5Ctextbf%7BF%7Deature-Aware%203D%20%5Ctextbf%7BA%7Dffordance%20%5Ctextbf%7BG%7Drounding%0A%28%5Ctextbf%7BMIFAG%7D%29%20framework.%20It%20grounds%203D%20object%20affordance%20regions%20by%0Aidentifying%20common%20interaction%20patterns%20across%20multiple%20human-object%0Ainteraction%20images.%20First%2C%20the%20Invariant%20Affordance%20Knowledge%20Extraction%20Module%0A%28%5Ctextbf%7BIAM%7D%29%20utilizes%20an%20iterative%20updating%20strategy%20to%20gradually%20extract%0Aaligned%20affordance%20knowledge%20from%20multiple%20images%20and%20integrate%20it%20into%20an%0Aaffordance%20dictionary.%20Then%2C%20the%20Affordance%20Dictionary%20Adaptive%20Fusion%20Module%0A%28%5Ctextbf%7BADM%7D%29%20learns%20comprehensive%20point%20cloud%20representations%20that%20consider%0Aall%20affordance%20candidates%20in%20multiple%20images.%20Besides%2C%20the%20Multi-Image%20and%0APoint%20Affordance%20%28%5Ctextbf%7BMIPA%7D%29%20benchmark%20is%20constructed%20and%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20methods%20on%20various%20experimental%0Acomparisons.%20Project%20page%3A%20%5Curl%7Bhttps%3A//goxq.github.io/mifag%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%25202D%2520Invariant%2520Affordance%2520Knowledge%2520for%25203D%2520Affordance%2520Grounding%26entry.906535625%3DXianqiang%2520Gao%2520and%2520Pingrui%2520Zhang%2520and%2520Delin%2520Qu%2520and%2520Dong%2520Wang%2520and%2520Zhigang%2520Wang%2520and%2520Yan%2520Ding%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%25203D%2520Object%2520Affordance%2520Grounding%2520aims%2520to%2520predict%2520the%2520functional%2520regions%2520on%2520a%25203D%250Aobject%2520and%2520has%2520laid%2520the%2520foundation%2520for%2520a%2520wide%2520range%2520of%2520applications%2520in%250Arobotics.%2520Recent%2520advances%2520tackle%2520this%2520problem%2520via%2520learning%2520a%2520mapping%2520between%25203D%250Aregions%2520and%2520a%2520single%2520human-object%2520interaction%2520image.%2520However%252C%2520the%2520geometric%250Astructure%2520of%2520the%25203D%2520object%2520and%2520the%2520object%2520in%2520the%2520human-object%2520interaction%2520image%250Aare%2520not%2520always%2520consistent%252C%2520leading%2520to%2520poor%2520generalization.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520to%2520learn%2520generalizable%2520invariant%2520affordance%2520knowledge%2520from%250Amultiple%2520human-object%2520interaction%2520images%2520within%2520the%2520same%2520affordance%2520category.%250ASpecifically%252C%2520we%2520introduce%2520the%2520%255Ctextbf%257BM%257Dulti-%255Ctextbf%257BI%257Dmage%2520Guided%250AInvariant-%255Ctextbf%257BF%257Deature-Aware%25203D%2520%255Ctextbf%257BA%257Dffordance%2520%255Ctextbf%257BG%257Drounding%250A%2528%255Ctextbf%257BMIFAG%257D%2529%2520framework.%2520It%2520grounds%25203D%2520object%2520affordance%2520regions%2520by%250Aidentifying%2520common%2520interaction%2520patterns%2520across%2520multiple%2520human-object%250Ainteraction%2520images.%2520First%252C%2520the%2520Invariant%2520Affordance%2520Knowledge%2520Extraction%2520Module%250A%2528%255Ctextbf%257BIAM%257D%2529%2520utilizes%2520an%2520iterative%2520updating%2520strategy%2520to%2520gradually%2520extract%250Aaligned%2520affordance%2520knowledge%2520from%2520multiple%2520images%2520and%2520integrate%2520it%2520into%2520an%250Aaffordance%2520dictionary.%2520Then%252C%2520the%2520Affordance%2520Dictionary%2520Adaptive%2520Fusion%2520Module%250A%2528%255Ctextbf%257BADM%257D%2529%2520learns%2520comprehensive%2520point%2520cloud%2520representations%2520that%2520consider%250Aall%2520affordance%2520candidates%2520in%2520multiple%2520images.%2520Besides%252C%2520the%2520Multi-Image%2520and%250APoint%2520Affordance%2520%2528%255Ctextbf%257BMIPA%257D%2529%2520benchmark%2520is%2520constructed%2520and%2520our%2520method%250Aoutperforms%2520existing%2520state-of-the-art%2520methods%2520on%2520various%2520experimental%250Acomparisons.%2520Project%2520page%253A%2520%255Curl%257Bhttps%253A//goxq.github.io/mifag%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%202D%20Invariant%20Affordance%20Knowledge%20for%203D%20Affordance%20Grounding&entry.906535625=Xianqiang%20Gao%20and%20Pingrui%20Zhang%20and%20Delin%20Qu%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Yan%20Ding%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%203D%20Object%20Affordance%20Grounding%20aims%20to%20predict%20the%20functional%20regions%20on%20a%203D%0Aobject%20and%20has%20laid%20the%20foundation%20for%20a%20wide%20range%20of%20applications%20in%0Arobotics.%20Recent%20advances%20tackle%20this%20problem%20via%20learning%20a%20mapping%20between%203D%0Aregions%20and%20a%20single%20human-object%20interaction%20image.%20However%2C%20the%20geometric%0Astructure%20of%20the%203D%20object%20and%20the%20object%20in%20the%20human-object%20interaction%20image%0Aare%20not%20always%20consistent%2C%20leading%20to%20poor%20generalization.%20To%20address%20this%0Aissue%2C%20we%20propose%20to%20learn%20generalizable%20invariant%20affordance%20knowledge%20from%0Amultiple%20human-object%20interaction%20images%20within%20the%20same%20affordance%20category.%0ASpecifically%2C%20we%20introduce%20the%20%5Ctextbf%7BM%7Dulti-%5Ctextbf%7BI%7Dmage%20Guided%0AInvariant-%5Ctextbf%7BF%7Deature-Aware%203D%20%5Ctextbf%7BA%7Dffordance%20%5Ctextbf%7BG%7Drounding%0A%28%5Ctextbf%7BMIFAG%7D%29%20framework.%20It%20grounds%203D%20object%20affordance%20regions%20by%0Aidentifying%20common%20interaction%20patterns%20across%20multiple%20human-object%0Ainteraction%20images.%20First%2C%20the%20Invariant%20Affordance%20Knowledge%20Extraction%20Module%0A%28%5Ctextbf%7BIAM%7D%29%20utilizes%20an%20iterative%20updating%20strategy%20to%20gradually%20extract%0Aaligned%20affordance%20knowledge%20from%20multiple%20images%20and%20integrate%20it%20into%20an%0Aaffordance%20dictionary.%20Then%2C%20the%20Affordance%20Dictionary%20Adaptive%20Fusion%20Module%0A%28%5Ctextbf%7BADM%7D%29%20learns%20comprehensive%20point%20cloud%20representations%20that%20consider%0Aall%20affordance%20candidates%20in%20multiple%20images.%20Besides%2C%20the%20Multi-Image%20and%0APoint%20Affordance%20%28%5Ctextbf%7BMIPA%7D%29%20benchmark%20is%20constructed%20and%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20methods%20on%20various%20experimental%0Acomparisons.%20Project%20page%3A%20%5Curl%7Bhttps%3A//goxq.github.io/mifag%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13024v1&entry.124074799=Read"},
{"title": "SurgicaL-CD: Generating Surgical Images via Unpaired Image Translation\n  with Latent Consistency Diffusion Models", "author": "Danush Kumar Venkatesh and Dominik Rivoir and Micha Pfeiffer and Stefanie Speidel", "abstract": "  Computer-assisted surgery (CAS) systems are designed to assist surgeons\nduring procedures, thereby reducing complications and enhancing patient care.\nTraining machine learning models for these systems requires a large corpus of\nannotated datasets, which is challenging to obtain in the surgical domain due\nto patient privacy concerns and the significant labeling effort required from\ndoctors. Previous methods have explored unpaired image translation using\ngenerative models to create realistic surgical images from simulations.\nHowever, these approaches have struggled to produce high-quality, diverse\nsurgical images. In this work, we introduce \\emph{SurgicaL-CD}, a\nconsistency-distilled diffusion method to generate realistic surgical images\nwith only a few sampling steps without paired data. We evaluate our approach on\nthree datasets, assessing the generated images in terms of quality and utility\nas downstream training datasets. Our results demonstrate that our method\noutperforms GANs and diffusion-based approaches. Our code is available at\nhttps://gitlab.com/nct_tso_public/gan2diffusion.\n", "link": "http://arxiv.org/abs/2408.09822v2", "date": "2024-08-23", "relevancy": 2.3218, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5871}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgicaL-CD%3A%20Generating%20Surgical%20Images%20via%20Unpaired%20Image%20Translation%0A%20%20with%20Latent%20Consistency%20Diffusion%20Models&body=Title%3A%20SurgicaL-CD%3A%20Generating%20Surgical%20Images%20via%20Unpaired%20Image%20Translation%0A%20%20with%20Latent%20Consistency%20Diffusion%20Models%0AAuthor%3A%20Danush%20Kumar%20Venkatesh%20and%20Dominik%20Rivoir%20and%20Micha%20Pfeiffer%20and%20Stefanie%20Speidel%0AAbstract%3A%20%20%20Computer-assisted%20surgery%20%28CAS%29%20systems%20are%20designed%20to%20assist%20surgeons%0Aduring%20procedures%2C%20thereby%20reducing%20complications%20and%20enhancing%20patient%20care.%0ATraining%20machine%20learning%20models%20for%20these%20systems%20requires%20a%20large%20corpus%20of%0Aannotated%20datasets%2C%20which%20is%20challenging%20to%20obtain%20in%20the%20surgical%20domain%20due%0Ato%20patient%20privacy%20concerns%20and%20the%20significant%20labeling%20effort%20required%20from%0Adoctors.%20Previous%20methods%20have%20explored%20unpaired%20image%20translation%20using%0Agenerative%20models%20to%20create%20realistic%20surgical%20images%20from%20simulations.%0AHowever%2C%20these%20approaches%20have%20struggled%20to%20produce%20high-quality%2C%20diverse%0Asurgical%20images.%20In%20this%20work%2C%20we%20introduce%20%5Cemph%7BSurgicaL-CD%7D%2C%20a%0Aconsistency-distilled%20diffusion%20method%20to%20generate%20realistic%20surgical%20images%0Awith%20only%20a%20few%20sampling%20steps%20without%20paired%20data.%20We%20evaluate%20our%20approach%20on%0Athree%20datasets%2C%20assessing%20the%20generated%20images%20in%20terms%20of%20quality%20and%20utility%0Aas%20downstream%20training%20datasets.%20Our%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20GANs%20and%20diffusion-based%20approaches.%20Our%20code%20is%20available%20at%0Ahttps%3A//gitlab.com/nct_tso_public/gan2diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgicaL-CD%253A%2520Generating%2520Surgical%2520Images%2520via%2520Unpaired%2520Image%2520Translation%250A%2520%2520with%2520Latent%2520Consistency%2520Diffusion%2520Models%26entry.906535625%3DDanush%2520Kumar%2520Venkatesh%2520and%2520Dominik%2520Rivoir%2520and%2520Micha%2520Pfeiffer%2520and%2520Stefanie%2520Speidel%26entry.1292438233%3D%2520%2520Computer-assisted%2520surgery%2520%2528CAS%2529%2520systems%2520are%2520designed%2520to%2520assist%2520surgeons%250Aduring%2520procedures%252C%2520thereby%2520reducing%2520complications%2520and%2520enhancing%2520patient%2520care.%250ATraining%2520machine%2520learning%2520models%2520for%2520these%2520systems%2520requires%2520a%2520large%2520corpus%2520of%250Aannotated%2520datasets%252C%2520which%2520is%2520challenging%2520to%2520obtain%2520in%2520the%2520surgical%2520domain%2520due%250Ato%2520patient%2520privacy%2520concerns%2520and%2520the%2520significant%2520labeling%2520effort%2520required%2520from%250Adoctors.%2520Previous%2520methods%2520have%2520explored%2520unpaired%2520image%2520translation%2520using%250Agenerative%2520models%2520to%2520create%2520realistic%2520surgical%2520images%2520from%2520simulations.%250AHowever%252C%2520these%2520approaches%2520have%2520struggled%2520to%2520produce%2520high-quality%252C%2520diverse%250Asurgical%2520images.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%255Cemph%257BSurgicaL-CD%257D%252C%2520a%250Aconsistency-distilled%2520diffusion%2520method%2520to%2520generate%2520realistic%2520surgical%2520images%250Awith%2520only%2520a%2520few%2520sampling%2520steps%2520without%2520paired%2520data.%2520We%2520evaluate%2520our%2520approach%2520on%250Athree%2520datasets%252C%2520assessing%2520the%2520generated%2520images%2520in%2520terms%2520of%2520quality%2520and%2520utility%250Aas%2520downstream%2520training%2520datasets.%2520Our%2520results%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520GANs%2520and%2520diffusion-based%2520approaches.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//gitlab.com/nct_tso_public/gan2diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgicaL-CD%3A%20Generating%20Surgical%20Images%20via%20Unpaired%20Image%20Translation%0A%20%20with%20Latent%20Consistency%20Diffusion%20Models&entry.906535625=Danush%20Kumar%20Venkatesh%20and%20Dominik%20Rivoir%20and%20Micha%20Pfeiffer%20and%20Stefanie%20Speidel&entry.1292438233=%20%20Computer-assisted%20surgery%20%28CAS%29%20systems%20are%20designed%20to%20assist%20surgeons%0Aduring%20procedures%2C%20thereby%20reducing%20complications%20and%20enhancing%20patient%20care.%0ATraining%20machine%20learning%20models%20for%20these%20systems%20requires%20a%20large%20corpus%20of%0Aannotated%20datasets%2C%20which%20is%20challenging%20to%20obtain%20in%20the%20surgical%20domain%20due%0Ato%20patient%20privacy%20concerns%20and%20the%20significant%20labeling%20effort%20required%20from%0Adoctors.%20Previous%20methods%20have%20explored%20unpaired%20image%20translation%20using%0Agenerative%20models%20to%20create%20realistic%20surgical%20images%20from%20simulations.%0AHowever%2C%20these%20approaches%20have%20struggled%20to%20produce%20high-quality%2C%20diverse%0Asurgical%20images.%20In%20this%20work%2C%20we%20introduce%20%5Cemph%7BSurgicaL-CD%7D%2C%20a%0Aconsistency-distilled%20diffusion%20method%20to%20generate%20realistic%20surgical%20images%0Awith%20only%20a%20few%20sampling%20steps%20without%20paired%20data.%20We%20evaluate%20our%20approach%20on%0Athree%20datasets%2C%20assessing%20the%20generated%20images%20in%20terms%20of%20quality%20and%20utility%0Aas%20downstream%20training%20datasets.%20Our%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20GANs%20and%20diffusion-based%20approaches.%20Our%20code%20is%20available%20at%0Ahttps%3A//gitlab.com/nct_tso_public/gan2diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09822v2&entry.124074799=Read"},
{"title": "Adaptive Backtracking For Faster Optimization", "author": "Joao V. Cavalcanti and Laurent Lessard and Ashia C. Wilson", "abstract": "  Backtracking line search is foundational in numerical optimization. The basic\nidea is to adjust the step size of an algorithm by a constant factor until some\nchosen criterion (e.g. Armijo, Goldstein, Descent Lemma) is satisfied. We\npropose a new way for adjusting step sizes, replacing the constant factor used\nin regular backtracking with one that takes into account the degree to which\nthe chosen criterion is violated, without additional computational burden. For\nconvex problems, we prove adaptive backtracking requires fewer adjustments to\nproduce a feasible step size than regular backtracking does for two popular\nline search criteria: the Armijo condition and the descent lemma. For nonconvex\nsmooth problems, we additionally prove adaptive backtracking enjoys the same\nguarantees of regular backtracking. Finally, we perform a variety of\nexperiments on over fifteen real world datasets, all of which confirm that\nadaptive backtracking often leads to significantly faster optimization.\n", "link": "http://arxiv.org/abs/2408.13150v1", "date": "2024-08-23", "relevancy": 2.2484, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4499}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Backtracking%20For%20Faster%20Optimization&body=Title%3A%20Adaptive%20Backtracking%20For%20Faster%20Optimization%0AAuthor%3A%20Joao%20V.%20Cavalcanti%20and%20Laurent%20Lessard%20and%20Ashia%20C.%20Wilson%0AAbstract%3A%20%20%20Backtracking%20line%20search%20is%20foundational%20in%20numerical%20optimization.%20The%20basic%0Aidea%20is%20to%20adjust%20the%20step%20size%20of%20an%20algorithm%20by%20a%20constant%20factor%20until%20some%0Achosen%20criterion%20%28e.g.%20Armijo%2C%20Goldstein%2C%20Descent%20Lemma%29%20is%20satisfied.%20We%0Apropose%20a%20new%20way%20for%20adjusting%20step%20sizes%2C%20replacing%20the%20constant%20factor%20used%0Ain%20regular%20backtracking%20with%20one%20that%20takes%20into%20account%20the%20degree%20to%20which%0Athe%20chosen%20criterion%20is%20violated%2C%20without%20additional%20computational%20burden.%20For%0Aconvex%20problems%2C%20we%20prove%20adaptive%20backtracking%20requires%20fewer%20adjustments%20to%0Aproduce%20a%20feasible%20step%20size%20than%20regular%20backtracking%20does%20for%20two%20popular%0Aline%20search%20criteria%3A%20the%20Armijo%20condition%20and%20the%20descent%20lemma.%20For%20nonconvex%0Asmooth%20problems%2C%20we%20additionally%20prove%20adaptive%20backtracking%20enjoys%20the%20same%0Aguarantees%20of%20regular%20backtracking.%20Finally%2C%20we%20perform%20a%20variety%20of%0Aexperiments%20on%20over%20fifteen%20real%20world%20datasets%2C%20all%20of%20which%20confirm%20that%0Aadaptive%20backtracking%20often%20leads%20to%20significantly%20faster%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Backtracking%2520For%2520Faster%2520Optimization%26entry.906535625%3DJoao%2520V.%2520Cavalcanti%2520and%2520Laurent%2520Lessard%2520and%2520Ashia%2520C.%2520Wilson%26entry.1292438233%3D%2520%2520Backtracking%2520line%2520search%2520is%2520foundational%2520in%2520numerical%2520optimization.%2520The%2520basic%250Aidea%2520is%2520to%2520adjust%2520the%2520step%2520size%2520of%2520an%2520algorithm%2520by%2520a%2520constant%2520factor%2520until%2520some%250Achosen%2520criterion%2520%2528e.g.%2520Armijo%252C%2520Goldstein%252C%2520Descent%2520Lemma%2529%2520is%2520satisfied.%2520We%250Apropose%2520a%2520new%2520way%2520for%2520adjusting%2520step%2520sizes%252C%2520replacing%2520the%2520constant%2520factor%2520used%250Ain%2520regular%2520backtracking%2520with%2520one%2520that%2520takes%2520into%2520account%2520the%2520degree%2520to%2520which%250Athe%2520chosen%2520criterion%2520is%2520violated%252C%2520without%2520additional%2520computational%2520burden.%2520For%250Aconvex%2520problems%252C%2520we%2520prove%2520adaptive%2520backtracking%2520requires%2520fewer%2520adjustments%2520to%250Aproduce%2520a%2520feasible%2520step%2520size%2520than%2520regular%2520backtracking%2520does%2520for%2520two%2520popular%250Aline%2520search%2520criteria%253A%2520the%2520Armijo%2520condition%2520and%2520the%2520descent%2520lemma.%2520For%2520nonconvex%250Asmooth%2520problems%252C%2520we%2520additionally%2520prove%2520adaptive%2520backtracking%2520enjoys%2520the%2520same%250Aguarantees%2520of%2520regular%2520backtracking.%2520Finally%252C%2520we%2520perform%2520a%2520variety%2520of%250Aexperiments%2520on%2520over%2520fifteen%2520real%2520world%2520datasets%252C%2520all%2520of%2520which%2520confirm%2520that%250Aadaptive%2520backtracking%2520often%2520leads%2520to%2520significantly%2520faster%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Backtracking%20For%20Faster%20Optimization&entry.906535625=Joao%20V.%20Cavalcanti%20and%20Laurent%20Lessard%20and%20Ashia%20C.%20Wilson&entry.1292438233=%20%20Backtracking%20line%20search%20is%20foundational%20in%20numerical%20optimization.%20The%20basic%0Aidea%20is%20to%20adjust%20the%20step%20size%20of%20an%20algorithm%20by%20a%20constant%20factor%20until%20some%0Achosen%20criterion%20%28e.g.%20Armijo%2C%20Goldstein%2C%20Descent%20Lemma%29%20is%20satisfied.%20We%0Apropose%20a%20new%20way%20for%20adjusting%20step%20sizes%2C%20replacing%20the%20constant%20factor%20used%0Ain%20regular%20backtracking%20with%20one%20that%20takes%20into%20account%20the%20degree%20to%20which%0Athe%20chosen%20criterion%20is%20violated%2C%20without%20additional%20computational%20burden.%20For%0Aconvex%20problems%2C%20we%20prove%20adaptive%20backtracking%20requires%20fewer%20adjustments%20to%0Aproduce%20a%20feasible%20step%20size%20than%20regular%20backtracking%20does%20for%20two%20popular%0Aline%20search%20criteria%3A%20the%20Armijo%20condition%20and%20the%20descent%20lemma.%20For%20nonconvex%0Asmooth%20problems%2C%20we%20additionally%20prove%20adaptive%20backtracking%20enjoys%20the%20same%0Aguarantees%20of%20regular%20backtracking.%20Finally%2C%20we%20perform%20a%20variety%20of%0Aexperiments%20on%20over%20fifteen%20real%20world%20datasets%2C%20all%20of%20which%20confirm%20that%0Aadaptive%20backtracking%20often%20leads%20to%20significantly%20faster%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13150v1&entry.124074799=Read"},
{"title": "AEMLO: AutoEncoder-Guided Multi-Label Oversampling", "author": "Ao Zhou and Bin Liu and Jin Wang and Kaiwei Sun and Kelin Liu", "abstract": "  Class imbalance significantly impacts the performance of multi-label\nclassifiers. Oversampling is one of the most popular approaches, as it augments\ninstances associated with less frequent labels to balance the class\ndistribution. Existing oversampling methods generate feature vectors of\nsynthetic samples through replication or linear interpolation and assign labels\nthrough neighborhood information. Linear interpolation typically generates new\nsamples between existing data points, which may result in insufficient\ndiversity of synthesized samples and further lead to the overfitting issue.\nDeep learning-based methods, such as AutoEncoders, have been proposed to\ngenerate more diverse and complex synthetic samples, achieving excellent\nperformance on imbalanced binary or multi-class datasets. In this study, we\nintroduce AEMLO, an AutoEncoder-guided Oversampling technique specifically\ndesigned for tackling imbalanced multi-label data. AEMLO is built upon two\nfundamental components. The first is an encoder-decoder architecture that\nenables the model to encode input data into a low-dimensional feature space,\nlearn its latent representations, and then reconstruct it back to its original\ndimension, thus applying to the generation of new data. The second is an\nobjective function tailored to optimize the sampling task for multi-label\nscenarios. We show that AEMLO outperforms the existing state-of-the-art methods\nwith extensive empirical studies.\n", "link": "http://arxiv.org/abs/2408.13078v1", "date": "2024-08-23", "relevancy": 2.2483, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6016}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AEMLO%3A%20AutoEncoder-Guided%20Multi-Label%20Oversampling&body=Title%3A%20AEMLO%3A%20AutoEncoder-Guided%20Multi-Label%20Oversampling%0AAuthor%3A%20Ao%20Zhou%20and%20Bin%20Liu%20and%20Jin%20Wang%20and%20Kaiwei%20Sun%20and%20Kelin%20Liu%0AAbstract%3A%20%20%20Class%20imbalance%20significantly%20impacts%20the%20performance%20of%20multi-label%0Aclassifiers.%20Oversampling%20is%20one%20of%20the%20most%20popular%20approaches%2C%20as%20it%20augments%0Ainstances%20associated%20with%20less%20frequent%20labels%20to%20balance%20the%20class%0Adistribution.%20Existing%20oversampling%20methods%20generate%20feature%20vectors%20of%0Asynthetic%20samples%20through%20replication%20or%20linear%20interpolation%20and%20assign%20labels%0Athrough%20neighborhood%20information.%20Linear%20interpolation%20typically%20generates%20new%0Asamples%20between%20existing%20data%20points%2C%20which%20may%20result%20in%20insufficient%0Adiversity%20of%20synthesized%20samples%20and%20further%20lead%20to%20the%20overfitting%20issue.%0ADeep%20learning-based%20methods%2C%20such%20as%20AutoEncoders%2C%20have%20been%20proposed%20to%0Agenerate%20more%20diverse%20and%20complex%20synthetic%20samples%2C%20achieving%20excellent%0Aperformance%20on%20imbalanced%20binary%20or%20multi-class%20datasets.%20In%20this%20study%2C%20we%0Aintroduce%20AEMLO%2C%20an%20AutoEncoder-guided%20Oversampling%20technique%20specifically%0Adesigned%20for%20tackling%20imbalanced%20multi-label%20data.%20AEMLO%20is%20built%20upon%20two%0Afundamental%20components.%20The%20first%20is%20an%20encoder-decoder%20architecture%20that%0Aenables%20the%20model%20to%20encode%20input%20data%20into%20a%20low-dimensional%20feature%20space%2C%0Alearn%20its%20latent%20representations%2C%20and%20then%20reconstruct%20it%20back%20to%20its%20original%0Adimension%2C%20thus%20applying%20to%20the%20generation%20of%20new%20data.%20The%20second%20is%20an%0Aobjective%20function%20tailored%20to%20optimize%20the%20sampling%20task%20for%20multi-label%0Ascenarios.%20We%20show%20that%20AEMLO%20outperforms%20the%20existing%20state-of-the-art%20methods%0Awith%20extensive%20empirical%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAEMLO%253A%2520AutoEncoder-Guided%2520Multi-Label%2520Oversampling%26entry.906535625%3DAo%2520Zhou%2520and%2520Bin%2520Liu%2520and%2520Jin%2520Wang%2520and%2520Kaiwei%2520Sun%2520and%2520Kelin%2520Liu%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520significantly%2520impacts%2520the%2520performance%2520of%2520multi-label%250Aclassifiers.%2520Oversampling%2520is%2520one%2520of%2520the%2520most%2520popular%2520approaches%252C%2520as%2520it%2520augments%250Ainstances%2520associated%2520with%2520less%2520frequent%2520labels%2520to%2520balance%2520the%2520class%250Adistribution.%2520Existing%2520oversampling%2520methods%2520generate%2520feature%2520vectors%2520of%250Asynthetic%2520samples%2520through%2520replication%2520or%2520linear%2520interpolation%2520and%2520assign%2520labels%250Athrough%2520neighborhood%2520information.%2520Linear%2520interpolation%2520typically%2520generates%2520new%250Asamples%2520between%2520existing%2520data%2520points%252C%2520which%2520may%2520result%2520in%2520insufficient%250Adiversity%2520of%2520synthesized%2520samples%2520and%2520further%2520lead%2520to%2520the%2520overfitting%2520issue.%250ADeep%2520learning-based%2520methods%252C%2520such%2520as%2520AutoEncoders%252C%2520have%2520been%2520proposed%2520to%250Agenerate%2520more%2520diverse%2520and%2520complex%2520synthetic%2520samples%252C%2520achieving%2520excellent%250Aperformance%2520on%2520imbalanced%2520binary%2520or%2520multi-class%2520datasets.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520AEMLO%252C%2520an%2520AutoEncoder-guided%2520Oversampling%2520technique%2520specifically%250Adesigned%2520for%2520tackling%2520imbalanced%2520multi-label%2520data.%2520AEMLO%2520is%2520built%2520upon%2520two%250Afundamental%2520components.%2520The%2520first%2520is%2520an%2520encoder-decoder%2520architecture%2520that%250Aenables%2520the%2520model%2520to%2520encode%2520input%2520data%2520into%2520a%2520low-dimensional%2520feature%2520space%252C%250Alearn%2520its%2520latent%2520representations%252C%2520and%2520then%2520reconstruct%2520it%2520back%2520to%2520its%2520original%250Adimension%252C%2520thus%2520applying%2520to%2520the%2520generation%2520of%2520new%2520data.%2520The%2520second%2520is%2520an%250Aobjective%2520function%2520tailored%2520to%2520optimize%2520the%2520sampling%2520task%2520for%2520multi-label%250Ascenarios.%2520We%2520show%2520that%2520AEMLO%2520outperforms%2520the%2520existing%2520state-of-the-art%2520methods%250Awith%2520extensive%2520empirical%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AEMLO%3A%20AutoEncoder-Guided%20Multi-Label%20Oversampling&entry.906535625=Ao%20Zhou%20and%20Bin%20Liu%20and%20Jin%20Wang%20and%20Kaiwei%20Sun%20and%20Kelin%20Liu&entry.1292438233=%20%20Class%20imbalance%20significantly%20impacts%20the%20performance%20of%20multi-label%0Aclassifiers.%20Oversampling%20is%20one%20of%20the%20most%20popular%20approaches%2C%20as%20it%20augments%0Ainstances%20associated%20with%20less%20frequent%20labels%20to%20balance%20the%20class%0Adistribution.%20Existing%20oversampling%20methods%20generate%20feature%20vectors%20of%0Asynthetic%20samples%20through%20replication%20or%20linear%20interpolation%20and%20assign%20labels%0Athrough%20neighborhood%20information.%20Linear%20interpolation%20typically%20generates%20new%0Asamples%20between%20existing%20data%20points%2C%20which%20may%20result%20in%20insufficient%0Adiversity%20of%20synthesized%20samples%20and%20further%20lead%20to%20the%20overfitting%20issue.%0ADeep%20learning-based%20methods%2C%20such%20as%20AutoEncoders%2C%20have%20been%20proposed%20to%0Agenerate%20more%20diverse%20and%20complex%20synthetic%20samples%2C%20achieving%20excellent%0Aperformance%20on%20imbalanced%20binary%20or%20multi-class%20datasets.%20In%20this%20study%2C%20we%0Aintroduce%20AEMLO%2C%20an%20AutoEncoder-guided%20Oversampling%20technique%20specifically%0Adesigned%20for%20tackling%20imbalanced%20multi-label%20data.%20AEMLO%20is%20built%20upon%20two%0Afundamental%20components.%20The%20first%20is%20an%20encoder-decoder%20architecture%20that%0Aenables%20the%20model%20to%20encode%20input%20data%20into%20a%20low-dimensional%20feature%20space%2C%0Alearn%20its%20latent%20representations%2C%20and%20then%20reconstruct%20it%20back%20to%20its%20original%0Adimension%2C%20thus%20applying%20to%20the%20generation%20of%20new%20data.%20The%20second%20is%20an%0Aobjective%20function%20tailored%20to%20optimize%20the%20sampling%20task%20for%20multi-label%0Ascenarios.%20We%20show%20that%20AEMLO%20outperforms%20the%20existing%20state-of-the-art%20methods%0Awith%20extensive%20empirical%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13078v1&entry.124074799=Read"},
{"title": "A Heterogeneous Dynamic Convolutional Neural Network for Image\n  Super-resolution", "author": "Chunwei Tian and Xuanyu Zhang and Tao Wang and Wangmeng Zuo and Yanning Zhang and Chia-Wen Lin", "abstract": "  Convolutional neural networks can automatically learn features via deep\nnetwork architectures and given input samples. However, robustness of obtained\nmodels may have challenges in varying scenes. Bigger differences of a network\narchitecture are beneficial to extract more complementary structural\ninformation to enhance robustness of an obtained super-resolution model. In\nthis paper, we present a heterogeneous dynamic convolutional network in image\nsuper-resolution (HDSRNet). To capture more information, HDSRNet is implemented\nby a heterogeneous parallel network. The upper network can facilitate more\ncontexture information via stacked heterogeneous blocks to improve effects of\nimage super-resolution. Each heterogeneous block is composed of a combination\nof a dilated, dynamic, common convolutional layers, ReLU and residual learning\noperation. It can not only adaptively adjust parameters, according to different\ninputs, but also prevent long-term dependency problem. The lower network\nutilizes a symmetric architecture to enhance relations of different layers to\nmine more structural information, which is complementary with a upper network\nfor image super-resolution. The relevant experimental results show that the\nproposed HDSRNet is effective to deal with image resolving. The code of HDSRNet\ncan be obtained at https://github.com/hellloxiaotian/HDSRNet.\n", "link": "http://arxiv.org/abs/2402.15704v2", "date": "2024-08-23", "relevancy": 2.2391, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5814}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5708}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Heterogeneous%20Dynamic%20Convolutional%20Neural%20Network%20for%20Image%0A%20%20Super-resolution&body=Title%3A%20A%20Heterogeneous%20Dynamic%20Convolutional%20Neural%20Network%20for%20Image%0A%20%20Super-resolution%0AAuthor%3A%20Chunwei%20Tian%20and%20Xuanyu%20Zhang%20and%20Tao%20Wang%20and%20Wangmeng%20Zuo%20and%20Yanning%20Zhang%20and%20Chia-Wen%20Lin%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20can%20automatically%20learn%20features%20via%20deep%0Anetwork%20architectures%20and%20given%20input%20samples.%20However%2C%20robustness%20of%20obtained%0Amodels%20may%20have%20challenges%20in%20varying%20scenes.%20Bigger%20differences%20of%20a%20network%0Aarchitecture%20are%20beneficial%20to%20extract%20more%20complementary%20structural%0Ainformation%20to%20enhance%20robustness%20of%20an%20obtained%20super-resolution%20model.%20In%0Athis%20paper%2C%20we%20present%20a%20heterogeneous%20dynamic%20convolutional%20network%20in%20image%0Asuper-resolution%20%28HDSRNet%29.%20To%20capture%20more%20information%2C%20HDSRNet%20is%20implemented%0Aby%20a%20heterogeneous%20parallel%20network.%20The%20upper%20network%20can%20facilitate%20more%0Acontexture%20information%20via%20stacked%20heterogeneous%20blocks%20to%20improve%20effects%20of%0Aimage%20super-resolution.%20Each%20heterogeneous%20block%20is%20composed%20of%20a%20combination%0Aof%20a%20dilated%2C%20dynamic%2C%20common%20convolutional%20layers%2C%20ReLU%20and%20residual%20learning%0Aoperation.%20It%20can%20not%20only%20adaptively%20adjust%20parameters%2C%20according%20to%20different%0Ainputs%2C%20but%20also%20prevent%20long-term%20dependency%20problem.%20The%20lower%20network%0Autilizes%20a%20symmetric%20architecture%20to%20enhance%20relations%20of%20different%20layers%20to%0Amine%20more%20structural%20information%2C%20which%20is%20complementary%20with%20a%20upper%20network%0Afor%20image%20super-resolution.%20The%20relevant%20experimental%20results%20show%20that%20the%0Aproposed%20HDSRNet%20is%20effective%20to%20deal%20with%20image%20resolving.%20The%20code%20of%20HDSRNet%0Acan%20be%20obtained%20at%20https%3A//github.com/hellloxiaotian/HDSRNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Heterogeneous%2520Dynamic%2520Convolutional%2520Neural%2520Network%2520for%2520Image%250A%2520%2520Super-resolution%26entry.906535625%3DChunwei%2520Tian%2520and%2520Xuanyu%2520Zhang%2520and%2520Tao%2520Wang%2520and%2520Wangmeng%2520Zuo%2520and%2520Yanning%2520Zhang%2520and%2520Chia-Wen%2520Lin%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520can%2520automatically%2520learn%2520features%2520via%2520deep%250Anetwork%2520architectures%2520and%2520given%2520input%2520samples.%2520However%252C%2520robustness%2520of%2520obtained%250Amodels%2520may%2520have%2520challenges%2520in%2520varying%2520scenes.%2520Bigger%2520differences%2520of%2520a%2520network%250Aarchitecture%2520are%2520beneficial%2520to%2520extract%2520more%2520complementary%2520structural%250Ainformation%2520to%2520enhance%2520robustness%2520of%2520an%2520obtained%2520super-resolution%2520model.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520heterogeneous%2520dynamic%2520convolutional%2520network%2520in%2520image%250Asuper-resolution%2520%2528HDSRNet%2529.%2520To%2520capture%2520more%2520information%252C%2520HDSRNet%2520is%2520implemented%250Aby%2520a%2520heterogeneous%2520parallel%2520network.%2520The%2520upper%2520network%2520can%2520facilitate%2520more%250Acontexture%2520information%2520via%2520stacked%2520heterogeneous%2520blocks%2520to%2520improve%2520effects%2520of%250Aimage%2520super-resolution.%2520Each%2520heterogeneous%2520block%2520is%2520composed%2520of%2520a%2520combination%250Aof%2520a%2520dilated%252C%2520dynamic%252C%2520common%2520convolutional%2520layers%252C%2520ReLU%2520and%2520residual%2520learning%250Aoperation.%2520It%2520can%2520not%2520only%2520adaptively%2520adjust%2520parameters%252C%2520according%2520to%2520different%250Ainputs%252C%2520but%2520also%2520prevent%2520long-term%2520dependency%2520problem.%2520The%2520lower%2520network%250Autilizes%2520a%2520symmetric%2520architecture%2520to%2520enhance%2520relations%2520of%2520different%2520layers%2520to%250Amine%2520more%2520structural%2520information%252C%2520which%2520is%2520complementary%2520with%2520a%2520upper%2520network%250Afor%2520image%2520super-resolution.%2520The%2520relevant%2520experimental%2520results%2520show%2520that%2520the%250Aproposed%2520HDSRNet%2520is%2520effective%2520to%2520deal%2520with%2520image%2520resolving.%2520The%2520code%2520of%2520HDSRNet%250Acan%2520be%2520obtained%2520at%2520https%253A//github.com/hellloxiaotian/HDSRNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Heterogeneous%20Dynamic%20Convolutional%20Neural%20Network%20for%20Image%0A%20%20Super-resolution&entry.906535625=Chunwei%20Tian%20and%20Xuanyu%20Zhang%20and%20Tao%20Wang%20and%20Wangmeng%20Zuo%20and%20Yanning%20Zhang%20and%20Chia-Wen%20Lin&entry.1292438233=%20%20Convolutional%20neural%20networks%20can%20automatically%20learn%20features%20via%20deep%0Anetwork%20architectures%20and%20given%20input%20samples.%20However%2C%20robustness%20of%20obtained%0Amodels%20may%20have%20challenges%20in%20varying%20scenes.%20Bigger%20differences%20of%20a%20network%0Aarchitecture%20are%20beneficial%20to%20extract%20more%20complementary%20structural%0Ainformation%20to%20enhance%20robustness%20of%20an%20obtained%20super-resolution%20model.%20In%0Athis%20paper%2C%20we%20present%20a%20heterogeneous%20dynamic%20convolutional%20network%20in%20image%0Asuper-resolution%20%28HDSRNet%29.%20To%20capture%20more%20information%2C%20HDSRNet%20is%20implemented%0Aby%20a%20heterogeneous%20parallel%20network.%20The%20upper%20network%20can%20facilitate%20more%0Acontexture%20information%20via%20stacked%20heterogeneous%20blocks%20to%20improve%20effects%20of%0Aimage%20super-resolution.%20Each%20heterogeneous%20block%20is%20composed%20of%20a%20combination%0Aof%20a%20dilated%2C%20dynamic%2C%20common%20convolutional%20layers%2C%20ReLU%20and%20residual%20learning%0Aoperation.%20It%20can%20not%20only%20adaptively%20adjust%20parameters%2C%20according%20to%20different%0Ainputs%2C%20but%20also%20prevent%20long-term%20dependency%20problem.%20The%20lower%20network%0Autilizes%20a%20symmetric%20architecture%20to%20enhance%20relations%20of%20different%20layers%20to%0Amine%20more%20structural%20information%2C%20which%20is%20complementary%20with%20a%20upper%20network%0Afor%20image%20super-resolution.%20The%20relevant%20experimental%20results%20show%20that%20the%0Aproposed%20HDSRNet%20is%20effective%20to%20deal%20with%20image%20resolving.%20The%20code%20of%20HDSRNet%0Acan%20be%20obtained%20at%20https%3A//github.com/hellloxiaotian/HDSRNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15704v2&entry.124074799=Read"},
{"title": "MCTR: Multi Camera Tracking Transformer", "author": "Alexandru Niculescu-Mizil and Deep Patel and Iain Melvin", "abstract": "  Multi-camera tracking plays a pivotal role in various real-world\napplications. While end-to-end methods have gained significant interest in\nsingle-camera tracking, multi-camera tracking remains predominantly reliant on\nheuristic techniques. In response to this gap, this paper introduces\nMulti-Camera Tracking tRansformer (MCTR), a novel end-to-end approach tailored\nfor multi-object detection and tracking across multiple cameras with\noverlapping fields of view. MCTR leverages end-to-end detectors like DEtector\nTRansformer (DETR) to produce detections and detection embeddings independently\nfor each camera view. The framework maintains set of track embeddings that\nencaplusate global information about the tracked objects, and updates them at\nevery frame by integrating the local information from the view-specific\ndetection embeddings. The track embeddings are probabilistically associated\nwith detections in every camera view and frame to generate consistent object\ntracks. The soft probabilistic association facilitates the design of\ndifferentiable losses that enable end-to-end training of the entire system. To\nvalidate our approach, we conduct experiments on MMPTrack and AI City\nChallenge, two recently introduced large-scale multi-camera multi-object\ntracking datasets.\n", "link": "http://arxiv.org/abs/2408.13243v1", "date": "2024-08-23", "relevancy": 2.2285, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.565}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCTR%3A%20Multi%20Camera%20Tracking%20Transformer&body=Title%3A%20MCTR%3A%20Multi%20Camera%20Tracking%20Transformer%0AAuthor%3A%20Alexandru%20Niculescu-Mizil%20and%20Deep%20Patel%20and%20Iain%20Melvin%0AAbstract%3A%20%20%20Multi-camera%20tracking%20plays%20a%20pivotal%20role%20in%20various%20real-world%0Aapplications.%20While%20end-to-end%20methods%20have%20gained%20significant%20interest%20in%0Asingle-camera%20tracking%2C%20multi-camera%20tracking%20remains%20predominantly%20reliant%20on%0Aheuristic%20techniques.%20In%20response%20to%20this%20gap%2C%20this%20paper%20introduces%0AMulti-Camera%20Tracking%20tRansformer%20%28MCTR%29%2C%20a%20novel%20end-to-end%20approach%20tailored%0Afor%20multi-object%20detection%20and%20tracking%20across%20multiple%20cameras%20with%0Aoverlapping%20fields%20of%20view.%20MCTR%20leverages%20end-to-end%20detectors%20like%20DEtector%0ATRansformer%20%28DETR%29%20to%20produce%20detections%20and%20detection%20embeddings%20independently%0Afor%20each%20camera%20view.%20The%20framework%20maintains%20set%20of%20track%20embeddings%20that%0Aencaplusate%20global%20information%20about%20the%20tracked%20objects%2C%20and%20updates%20them%20at%0Aevery%20frame%20by%20integrating%20the%20local%20information%20from%20the%20view-specific%0Adetection%20embeddings.%20The%20track%20embeddings%20are%20probabilistically%20associated%0Awith%20detections%20in%20every%20camera%20view%20and%20frame%20to%20generate%20consistent%20object%0Atracks.%20The%20soft%20probabilistic%20association%20facilitates%20the%20design%20of%0Adifferentiable%20losses%20that%20enable%20end-to-end%20training%20of%20the%20entire%20system.%20To%0Avalidate%20our%20approach%2C%20we%20conduct%20experiments%20on%20MMPTrack%20and%20AI%20City%0AChallenge%2C%20two%20recently%20introduced%20large-scale%20multi-camera%20multi-object%0Atracking%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCTR%253A%2520Multi%2520Camera%2520Tracking%2520Transformer%26entry.906535625%3DAlexandru%2520Niculescu-Mizil%2520and%2520Deep%2520Patel%2520and%2520Iain%2520Melvin%26entry.1292438233%3D%2520%2520Multi-camera%2520tracking%2520plays%2520a%2520pivotal%2520role%2520in%2520various%2520real-world%250Aapplications.%2520While%2520end-to-end%2520methods%2520have%2520gained%2520significant%2520interest%2520in%250Asingle-camera%2520tracking%252C%2520multi-camera%2520tracking%2520remains%2520predominantly%2520reliant%2520on%250Aheuristic%2520techniques.%2520In%2520response%2520to%2520this%2520gap%252C%2520this%2520paper%2520introduces%250AMulti-Camera%2520Tracking%2520tRansformer%2520%2528MCTR%2529%252C%2520a%2520novel%2520end-to-end%2520approach%2520tailored%250Afor%2520multi-object%2520detection%2520and%2520tracking%2520across%2520multiple%2520cameras%2520with%250Aoverlapping%2520fields%2520of%2520view.%2520MCTR%2520leverages%2520end-to-end%2520detectors%2520like%2520DEtector%250ATRansformer%2520%2528DETR%2529%2520to%2520produce%2520detections%2520and%2520detection%2520embeddings%2520independently%250Afor%2520each%2520camera%2520view.%2520The%2520framework%2520maintains%2520set%2520of%2520track%2520embeddings%2520that%250Aencaplusate%2520global%2520information%2520about%2520the%2520tracked%2520objects%252C%2520and%2520updates%2520them%2520at%250Aevery%2520frame%2520by%2520integrating%2520the%2520local%2520information%2520from%2520the%2520view-specific%250Adetection%2520embeddings.%2520The%2520track%2520embeddings%2520are%2520probabilistically%2520associated%250Awith%2520detections%2520in%2520every%2520camera%2520view%2520and%2520frame%2520to%2520generate%2520consistent%2520object%250Atracks.%2520The%2520soft%2520probabilistic%2520association%2520facilitates%2520the%2520design%2520of%250Adifferentiable%2520losses%2520that%2520enable%2520end-to-end%2520training%2520of%2520the%2520entire%2520system.%2520To%250Avalidate%2520our%2520approach%252C%2520we%2520conduct%2520experiments%2520on%2520MMPTrack%2520and%2520AI%2520City%250AChallenge%252C%2520two%2520recently%2520introduced%2520large-scale%2520multi-camera%2520multi-object%250Atracking%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCTR%3A%20Multi%20Camera%20Tracking%20Transformer&entry.906535625=Alexandru%20Niculescu-Mizil%20and%20Deep%20Patel%20and%20Iain%20Melvin&entry.1292438233=%20%20Multi-camera%20tracking%20plays%20a%20pivotal%20role%20in%20various%20real-world%0Aapplications.%20While%20end-to-end%20methods%20have%20gained%20significant%20interest%20in%0Asingle-camera%20tracking%2C%20multi-camera%20tracking%20remains%20predominantly%20reliant%20on%0Aheuristic%20techniques.%20In%20response%20to%20this%20gap%2C%20this%20paper%20introduces%0AMulti-Camera%20Tracking%20tRansformer%20%28MCTR%29%2C%20a%20novel%20end-to-end%20approach%20tailored%0Afor%20multi-object%20detection%20and%20tracking%20across%20multiple%20cameras%20with%0Aoverlapping%20fields%20of%20view.%20MCTR%20leverages%20end-to-end%20detectors%20like%20DEtector%0ATRansformer%20%28DETR%29%20to%20produce%20detections%20and%20detection%20embeddings%20independently%0Afor%20each%20camera%20view.%20The%20framework%20maintains%20set%20of%20track%20embeddings%20that%0Aencaplusate%20global%20information%20about%20the%20tracked%20objects%2C%20and%20updates%20them%20at%0Aevery%20frame%20by%20integrating%20the%20local%20information%20from%20the%20view-specific%0Adetection%20embeddings.%20The%20track%20embeddings%20are%20probabilistically%20associated%0Awith%20detections%20in%20every%20camera%20view%20and%20frame%20to%20generate%20consistent%20object%0Atracks.%20The%20soft%20probabilistic%20association%20facilitates%20the%20design%20of%0Adifferentiable%20losses%20that%20enable%20end-to-end%20training%20of%20the%20entire%20system.%20To%0Avalidate%20our%20approach%2C%20we%20conduct%20experiments%20on%20MMPTrack%20and%20AI%20City%0AChallenge%2C%20two%20recently%20introduced%20large-scale%20multi-camera%20multi-object%0Atracking%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13243v1&entry.124074799=Read"},
{"title": "Generative Topological Networks", "author": "Alona Levy-Jurgenson and Zohar Yakhini", "abstract": "  Generative models have seen significant advancements in recent years, yet\noften remain challenging and costly to train and use. We introduce Generative\nTopological Networks (GTNs) -- a new class of generative models that addresses\nthese shortcomings. GTNs are trained deterministically using a simple\nsupervised learning approach grounded in topology theory. GTNs are fast to\ntrain, and require only a single forward pass in a standard feedforward neural\nnetwork to generate samples. We demonstrate the strengths of GTNs on several\ndatasets, including MNIST, CelebA and the Hands and Palm Images dataset.\nFinally, the theory behind GTNs offers insights into how to train generative\nmodels for improved performance. Code and weights are available at:\nhttps://github.com/alonalj/GTN\n", "link": "http://arxiv.org/abs/2406.15152v2", "date": "2024-08-23", "relevancy": 2.205, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5666}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5484}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Topological%20Networks&body=Title%3A%20Generative%20Topological%20Networks%0AAuthor%3A%20Alona%20Levy-Jurgenson%20and%20Zohar%20Yakhini%0AAbstract%3A%20%20%20Generative%20models%20have%20seen%20significant%20advancements%20in%20recent%20years%2C%20yet%0Aoften%20remain%20challenging%20and%20costly%20to%20train%20and%20use.%20We%20introduce%20Generative%0ATopological%20Networks%20%28GTNs%29%20--%20a%20new%20class%20of%20generative%20models%20that%20addresses%0Athese%20shortcomings.%20GTNs%20are%20trained%20deterministically%20using%20a%20simple%0Asupervised%20learning%20approach%20grounded%20in%20topology%20theory.%20GTNs%20are%20fast%20to%0Atrain%2C%20and%20require%20only%20a%20single%20forward%20pass%20in%20a%20standard%20feedforward%20neural%0Anetwork%20to%20generate%20samples.%20We%20demonstrate%20the%20strengths%20of%20GTNs%20on%20several%0Adatasets%2C%20including%20MNIST%2C%20CelebA%20and%20the%20Hands%20and%20Palm%20Images%20dataset.%0AFinally%2C%20the%20theory%20behind%20GTNs%20offers%20insights%20into%20how%20to%20train%20generative%0Amodels%20for%20improved%20performance.%20Code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/alonalj/GTN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15152v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Topological%2520Networks%26entry.906535625%3DAlona%2520Levy-Jurgenson%2520and%2520Zohar%2520Yakhini%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520seen%2520significant%2520advancements%2520in%2520recent%2520years%252C%2520yet%250Aoften%2520remain%2520challenging%2520and%2520costly%2520to%2520train%2520and%2520use.%2520We%2520introduce%2520Generative%250ATopological%2520Networks%2520%2528GTNs%2529%2520--%2520a%2520new%2520class%2520of%2520generative%2520models%2520that%2520addresses%250Athese%2520shortcomings.%2520GTNs%2520are%2520trained%2520deterministically%2520using%2520a%2520simple%250Asupervised%2520learning%2520approach%2520grounded%2520in%2520topology%2520theory.%2520GTNs%2520are%2520fast%2520to%250Atrain%252C%2520and%2520require%2520only%2520a%2520single%2520forward%2520pass%2520in%2520a%2520standard%2520feedforward%2520neural%250Anetwork%2520to%2520generate%2520samples.%2520We%2520demonstrate%2520the%2520strengths%2520of%2520GTNs%2520on%2520several%250Adatasets%252C%2520including%2520MNIST%252C%2520CelebA%2520and%2520the%2520Hands%2520and%2520Palm%2520Images%2520dataset.%250AFinally%252C%2520the%2520theory%2520behind%2520GTNs%2520offers%2520insights%2520into%2520how%2520to%2520train%2520generative%250Amodels%2520for%2520improved%2520performance.%2520Code%2520and%2520weights%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/alonalj/GTN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15152v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Topological%20Networks&entry.906535625=Alona%20Levy-Jurgenson%20and%20Zohar%20Yakhini&entry.1292438233=%20%20Generative%20models%20have%20seen%20significant%20advancements%20in%20recent%20years%2C%20yet%0Aoften%20remain%20challenging%20and%20costly%20to%20train%20and%20use.%20We%20introduce%20Generative%0ATopological%20Networks%20%28GTNs%29%20--%20a%20new%20class%20of%20generative%20models%20that%20addresses%0Athese%20shortcomings.%20GTNs%20are%20trained%20deterministically%20using%20a%20simple%0Asupervised%20learning%20approach%20grounded%20in%20topology%20theory.%20GTNs%20are%20fast%20to%0Atrain%2C%20and%20require%20only%20a%20single%20forward%20pass%20in%20a%20standard%20feedforward%20neural%0Anetwork%20to%20generate%20samples.%20We%20demonstrate%20the%20strengths%20of%20GTNs%20on%20several%0Adatasets%2C%20including%20MNIST%2C%20CelebA%20and%20the%20Hands%20and%20Palm%20Images%20dataset.%0AFinally%2C%20the%20theory%20behind%20GTNs%20offers%20insights%20into%20how%20to%20train%20generative%0Amodels%20for%20improved%20performance.%20Code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/alonalj/GTN%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15152v2&entry.124074799=Read"},
{"title": "EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from\n  Egocentric Videos", "author": "Masashi Hatano and Ryo Hachiuma and Hideo Saito", "abstract": "  Predicting future human behavior from egocentric videos is a challenging but\ncritical task for human intention understanding. Existing methods for\nforecasting 2D hand positions rely on visual representations and mainly focus\non hand-object interactions. In this paper, we investigate the hand forecasting\ntask and tackle two significant issues that persist in the existing methods:\n(1) 2D hand positions in future frames are severely affected by ego-motions in\negocentric videos; (2) prediction based on visual information tends to overfit\nto background or scene textures, posing a challenge for generalization on novel\nscenes or human behaviors. To solve the aforementioned problems, we propose\nEMAG, an ego-motion-aware and generalizable 2D hand forecasting method. In\nresponse to the first problem, we propose a method that considers ego-motion,\nrepresented by a sequence of homography matrices of two consecutive frames. We\nfurther leverage modalities such as optical flow, trajectories of hands and\ninteracting objects, and ego-motions, thereby alleviating the second issue.\nExtensive experiments on two large-scale egocentric video datasets, Ego4D and\nEPIC-Kitchens 55, verify the effectiveness of the proposed method. In\nparticular, our model outperforms prior methods by 1.7% and 7.0% on intra and\ncross-dataset evaluations, respectively. Project page:\nhttps://masashi-hatano.github.io/EMAG/\n", "link": "http://arxiv.org/abs/2405.20030v2", "date": "2024-08-23", "relevancy": 2.2048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5658}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5561}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMAG%3A%20Ego-motion%20Aware%20and%20Generalizable%202D%20Hand%20Forecasting%20from%0A%20%20Egocentric%20Videos&body=Title%3A%20EMAG%3A%20Ego-motion%20Aware%20and%20Generalizable%202D%20Hand%20Forecasting%20from%0A%20%20Egocentric%20Videos%0AAuthor%3A%20Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Hideo%20Saito%0AAbstract%3A%20%20%20Predicting%20future%20human%20behavior%20from%20egocentric%20videos%20is%20a%20challenging%20but%0Acritical%20task%20for%20human%20intention%20understanding.%20Existing%20methods%20for%0Aforecasting%202D%20hand%20positions%20rely%20on%20visual%20representations%20and%20mainly%20focus%0Aon%20hand-object%20interactions.%20In%20this%20paper%2C%20we%20investigate%20the%20hand%20forecasting%0Atask%20and%20tackle%20two%20significant%20issues%20that%20persist%20in%20the%20existing%20methods%3A%0A%281%29%202D%20hand%20positions%20in%20future%20frames%20are%20severely%20affected%20by%20ego-motions%20in%0Aegocentric%20videos%3B%20%282%29%20prediction%20based%20on%20visual%20information%20tends%20to%20overfit%0Ato%20background%20or%20scene%20textures%2C%20posing%20a%20challenge%20for%20generalization%20on%20novel%0Ascenes%20or%20human%20behaviors.%20To%20solve%20the%20aforementioned%20problems%2C%20we%20propose%0AEMAG%2C%20an%20ego-motion-aware%20and%20generalizable%202D%20hand%20forecasting%20method.%20In%0Aresponse%20to%20the%20first%20problem%2C%20we%20propose%20a%20method%20that%20considers%20ego-motion%2C%0Arepresented%20by%20a%20sequence%20of%20homography%20matrices%20of%20two%20consecutive%20frames.%20We%0Afurther%20leverage%20modalities%20such%20as%20optical%20flow%2C%20trajectories%20of%20hands%20and%0Ainteracting%20objects%2C%20and%20ego-motions%2C%20thereby%20alleviating%20the%20second%20issue.%0AExtensive%20experiments%20on%20two%20large-scale%20egocentric%20video%20datasets%2C%20Ego4D%20and%0AEPIC-Kitchens%2055%2C%20verify%20the%20effectiveness%20of%20the%20proposed%20method.%20In%0Aparticular%2C%20our%20model%20outperforms%20prior%20methods%20by%201.7%25%20and%207.0%25%20on%20intra%20and%0Across-dataset%20evaluations%2C%20respectively.%20Project%20page%3A%0Ahttps%3A//masashi-hatano.github.io/EMAG/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20030v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMAG%253A%2520Ego-motion%2520Aware%2520and%2520Generalizable%25202D%2520Hand%2520Forecasting%2520from%250A%2520%2520Egocentric%2520Videos%26entry.906535625%3DMasashi%2520Hatano%2520and%2520Ryo%2520Hachiuma%2520and%2520Hideo%2520Saito%26entry.1292438233%3D%2520%2520Predicting%2520future%2520human%2520behavior%2520from%2520egocentric%2520videos%2520is%2520a%2520challenging%2520but%250Acritical%2520task%2520for%2520human%2520intention%2520understanding.%2520Existing%2520methods%2520for%250Aforecasting%25202D%2520hand%2520positions%2520rely%2520on%2520visual%2520representations%2520and%2520mainly%2520focus%250Aon%2520hand-object%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520hand%2520forecasting%250Atask%2520and%2520tackle%2520two%2520significant%2520issues%2520that%2520persist%2520in%2520the%2520existing%2520methods%253A%250A%25281%2529%25202D%2520hand%2520positions%2520in%2520future%2520frames%2520are%2520severely%2520affected%2520by%2520ego-motions%2520in%250Aegocentric%2520videos%253B%2520%25282%2529%2520prediction%2520based%2520on%2520visual%2520information%2520tends%2520to%2520overfit%250Ato%2520background%2520or%2520scene%2520textures%252C%2520posing%2520a%2520challenge%2520for%2520generalization%2520on%2520novel%250Ascenes%2520or%2520human%2520behaviors.%2520To%2520solve%2520the%2520aforementioned%2520problems%252C%2520we%2520propose%250AEMAG%252C%2520an%2520ego-motion-aware%2520and%2520generalizable%25202D%2520hand%2520forecasting%2520method.%2520In%250Aresponse%2520to%2520the%2520first%2520problem%252C%2520we%2520propose%2520a%2520method%2520that%2520considers%2520ego-motion%252C%250Arepresented%2520by%2520a%2520sequence%2520of%2520homography%2520matrices%2520of%2520two%2520consecutive%2520frames.%2520We%250Afurther%2520leverage%2520modalities%2520such%2520as%2520optical%2520flow%252C%2520trajectories%2520of%2520hands%2520and%250Ainteracting%2520objects%252C%2520and%2520ego-motions%252C%2520thereby%2520alleviating%2520the%2520second%2520issue.%250AExtensive%2520experiments%2520on%2520two%2520large-scale%2520egocentric%2520video%2520datasets%252C%2520Ego4D%2520and%250AEPIC-Kitchens%252055%252C%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520In%250Aparticular%252C%2520our%2520model%2520outperforms%2520prior%2520methods%2520by%25201.7%2525%2520and%25207.0%2525%2520on%2520intra%2520and%250Across-dataset%2520evaluations%252C%2520respectively.%2520Project%2520page%253A%250Ahttps%253A//masashi-hatano.github.io/EMAG/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20030v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMAG%3A%20Ego-motion%20Aware%20and%20Generalizable%202D%20Hand%20Forecasting%20from%0A%20%20Egocentric%20Videos&entry.906535625=Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Hideo%20Saito&entry.1292438233=%20%20Predicting%20future%20human%20behavior%20from%20egocentric%20videos%20is%20a%20challenging%20but%0Acritical%20task%20for%20human%20intention%20understanding.%20Existing%20methods%20for%0Aforecasting%202D%20hand%20positions%20rely%20on%20visual%20representations%20and%20mainly%20focus%0Aon%20hand-object%20interactions.%20In%20this%20paper%2C%20we%20investigate%20the%20hand%20forecasting%0Atask%20and%20tackle%20two%20significant%20issues%20that%20persist%20in%20the%20existing%20methods%3A%0A%281%29%202D%20hand%20positions%20in%20future%20frames%20are%20severely%20affected%20by%20ego-motions%20in%0Aegocentric%20videos%3B%20%282%29%20prediction%20based%20on%20visual%20information%20tends%20to%20overfit%0Ato%20background%20or%20scene%20textures%2C%20posing%20a%20challenge%20for%20generalization%20on%20novel%0Ascenes%20or%20human%20behaviors.%20To%20solve%20the%20aforementioned%20problems%2C%20we%20propose%0AEMAG%2C%20an%20ego-motion-aware%20and%20generalizable%202D%20hand%20forecasting%20method.%20In%0Aresponse%20to%20the%20first%20problem%2C%20we%20propose%20a%20method%20that%20considers%20ego-motion%2C%0Arepresented%20by%20a%20sequence%20of%20homography%20matrices%20of%20two%20consecutive%20frames.%20We%0Afurther%20leverage%20modalities%20such%20as%20optical%20flow%2C%20trajectories%20of%20hands%20and%0Ainteracting%20objects%2C%20and%20ego-motions%2C%20thereby%20alleviating%20the%20second%20issue.%0AExtensive%20experiments%20on%20two%20large-scale%20egocentric%20video%20datasets%2C%20Ego4D%20and%0AEPIC-Kitchens%2055%2C%20verify%20the%20effectiveness%20of%20the%20proposed%20method.%20In%0Aparticular%2C%20our%20model%20outperforms%20prior%20methods%20by%201.7%25%20and%207.0%25%20on%20intra%20and%0Across-dataset%20evaluations%2C%20respectively.%20Project%20page%3A%0Ahttps%3A//masashi-hatano.github.io/EMAG/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20030v2&entry.124074799=Read"},
{"title": "Multimodal Contrastive In-Context Learning", "author": "Yosuke Miyanishi and Minh Le Nguyen", "abstract": "  The rapid growth of Large Language Models (LLMs) usage has highlighted the\nimportance of gradient-free in-context learning (ICL). However, interpreting\ntheir inner workings remains challenging. This paper introduces a novel\nmultimodal contrastive in-context learning framework to enhance our\nunderstanding of ICL in LLMs. First, we present a contrastive learning-based\ninterpretation of ICL in real-world settings, marking the distance of the\nkey-value representation as the differentiator in ICL. Second, we develop an\nanalytical framework to address biases in multimodal input formatting for\nreal-world datasets. We demonstrate the effectiveness of ICL examples where\nbaseline performance is poor, even when they are represented in unseen formats.\nLastly, we propose an on-the-fly approach for ICL (Anchored-by-Text ICL) that\ndemonstrates effectiveness in detecting hateful memes, a task where typical ICL\nstruggles due to resource limitations. Extensive experiments on multimodal\ndatasets reveal that our approach significantly improves ICL performance across\nvarious scenarios, such as challenging tasks and resource-constrained\nenvironments. Moreover, it provides valuable insights into the mechanisms of\nin-context learning in LLMs. Our findings have important implications for\ndeveloping more interpretable, efficient, and robust multimodal AI systems,\nespecially in challenging tasks and resource-constrained environments.\n", "link": "http://arxiv.org/abs/2408.12959v1", "date": "2024-08-23", "relevancy": 2.1821, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5934}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5499}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Contrastive%20In-Context%20Learning&body=Title%3A%20Multimodal%20Contrastive%20In-Context%20Learning%0AAuthor%3A%20Yosuke%20Miyanishi%20and%20Minh%20Le%20Nguyen%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20Large%20Language%20Models%20%28LLMs%29%20usage%20has%20highlighted%20the%0Aimportance%20of%20gradient-free%20in-context%20learning%20%28ICL%29.%20However%2C%20interpreting%0Atheir%20inner%20workings%20remains%20challenging.%20This%20paper%20introduces%20a%20novel%0Amultimodal%20contrastive%20in-context%20learning%20framework%20to%20enhance%20our%0Aunderstanding%20of%20ICL%20in%20LLMs.%20First%2C%20we%20present%20a%20contrastive%20learning-based%0Ainterpretation%20of%20ICL%20in%20real-world%20settings%2C%20marking%20the%20distance%20of%20the%0Akey-value%20representation%20as%20the%20differentiator%20in%20ICL.%20Second%2C%20we%20develop%20an%0Aanalytical%20framework%20to%20address%20biases%20in%20multimodal%20input%20formatting%20for%0Areal-world%20datasets.%20We%20demonstrate%20the%20effectiveness%20of%20ICL%20examples%20where%0Abaseline%20performance%20is%20poor%2C%20even%20when%20they%20are%20represented%20in%20unseen%20formats.%0ALastly%2C%20we%20propose%20an%20on-the-fly%20approach%20for%20ICL%20%28Anchored-by-Text%20ICL%29%20that%0Ademonstrates%20effectiveness%20in%20detecting%20hateful%20memes%2C%20a%20task%20where%20typical%20ICL%0Astruggles%20due%20to%20resource%20limitations.%20Extensive%20experiments%20on%20multimodal%0Adatasets%20reveal%20that%20our%20approach%20significantly%20improves%20ICL%20performance%20across%0Avarious%20scenarios%2C%20such%20as%20challenging%20tasks%20and%20resource-constrained%0Aenvironments.%20Moreover%2C%20it%20provides%20valuable%20insights%20into%20the%20mechanisms%20of%0Ain-context%20learning%20in%20LLMs.%20Our%20findings%20have%20important%20implications%20for%0Adeveloping%20more%20interpretable%2C%20efficient%2C%20and%20robust%20multimodal%20AI%20systems%2C%0Aespecially%20in%20challenging%20tasks%20and%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Contrastive%2520In-Context%2520Learning%26entry.906535625%3DYosuke%2520Miyanishi%2520and%2520Minh%2520Le%2520Nguyen%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520usage%2520has%2520highlighted%2520the%250Aimportance%2520of%2520gradient-free%2520in-context%2520learning%2520%2528ICL%2529.%2520However%252C%2520interpreting%250Atheir%2520inner%2520workings%2520remains%2520challenging.%2520This%2520paper%2520introduces%2520a%2520novel%250Amultimodal%2520contrastive%2520in-context%2520learning%2520framework%2520to%2520enhance%2520our%250Aunderstanding%2520of%2520ICL%2520in%2520LLMs.%2520First%252C%2520we%2520present%2520a%2520contrastive%2520learning-based%250Ainterpretation%2520of%2520ICL%2520in%2520real-world%2520settings%252C%2520marking%2520the%2520distance%2520of%2520the%250Akey-value%2520representation%2520as%2520the%2520differentiator%2520in%2520ICL.%2520Second%252C%2520we%2520develop%2520an%250Aanalytical%2520framework%2520to%2520address%2520biases%2520in%2520multimodal%2520input%2520formatting%2520for%250Areal-world%2520datasets.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520ICL%2520examples%2520where%250Abaseline%2520performance%2520is%2520poor%252C%2520even%2520when%2520they%2520are%2520represented%2520in%2520unseen%2520formats.%250ALastly%252C%2520we%2520propose%2520an%2520on-the-fly%2520approach%2520for%2520ICL%2520%2528Anchored-by-Text%2520ICL%2529%2520that%250Ademonstrates%2520effectiveness%2520in%2520detecting%2520hateful%2520memes%252C%2520a%2520task%2520where%2520typical%2520ICL%250Astruggles%2520due%2520to%2520resource%2520limitations.%2520Extensive%2520experiments%2520on%2520multimodal%250Adatasets%2520reveal%2520that%2520our%2520approach%2520significantly%2520improves%2520ICL%2520performance%2520across%250Avarious%2520scenarios%252C%2520such%2520as%2520challenging%2520tasks%2520and%2520resource-constrained%250Aenvironments.%2520Moreover%252C%2520it%2520provides%2520valuable%2520insights%2520into%2520the%2520mechanisms%2520of%250Ain-context%2520learning%2520in%2520LLMs.%2520Our%2520findings%2520have%2520important%2520implications%2520for%250Adeveloping%2520more%2520interpretable%252C%2520efficient%252C%2520and%2520robust%2520multimodal%2520AI%2520systems%252C%250Aespecially%2520in%2520challenging%2520tasks%2520and%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Contrastive%20In-Context%20Learning&entry.906535625=Yosuke%20Miyanishi%20and%20Minh%20Le%20Nguyen&entry.1292438233=%20%20The%20rapid%20growth%20of%20Large%20Language%20Models%20%28LLMs%29%20usage%20has%20highlighted%20the%0Aimportance%20of%20gradient-free%20in-context%20learning%20%28ICL%29.%20However%2C%20interpreting%0Atheir%20inner%20workings%20remains%20challenging.%20This%20paper%20introduces%20a%20novel%0Amultimodal%20contrastive%20in-context%20learning%20framework%20to%20enhance%20our%0Aunderstanding%20of%20ICL%20in%20LLMs.%20First%2C%20we%20present%20a%20contrastive%20learning-based%0Ainterpretation%20of%20ICL%20in%20real-world%20settings%2C%20marking%20the%20distance%20of%20the%0Akey-value%20representation%20as%20the%20differentiator%20in%20ICL.%20Second%2C%20we%20develop%20an%0Aanalytical%20framework%20to%20address%20biases%20in%20multimodal%20input%20formatting%20for%0Areal-world%20datasets.%20We%20demonstrate%20the%20effectiveness%20of%20ICL%20examples%20where%0Abaseline%20performance%20is%20poor%2C%20even%20when%20they%20are%20represented%20in%20unseen%20formats.%0ALastly%2C%20we%20propose%20an%20on-the-fly%20approach%20for%20ICL%20%28Anchored-by-Text%20ICL%29%20that%0Ademonstrates%20effectiveness%20in%20detecting%20hateful%20memes%2C%20a%20task%20where%20typical%20ICL%0Astruggles%20due%20to%20resource%20limitations.%20Extensive%20experiments%20on%20multimodal%0Adatasets%20reveal%20that%20our%20approach%20significantly%20improves%20ICL%20performance%20across%0Avarious%20scenarios%2C%20such%20as%20challenging%20tasks%20and%20resource-constrained%0Aenvironments.%20Moreover%2C%20it%20provides%20valuable%20insights%20into%20the%20mechanisms%20of%0Ain-context%20learning%20in%20LLMs.%20Our%20findings%20have%20important%20implications%20for%0Adeveloping%20more%20interpretable%2C%20efficient%2C%20and%20robust%20multimodal%20AI%20systems%2C%0Aespecially%20in%20challenging%20tasks%20and%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12959v1&entry.124074799=Read"},
{"title": "Semi-Supervised Unconstrained Head Pose Estimation in the Wild", "author": "Huayi Zhou and Fei Jiang and Jin Yuan and Yong Rui and Hongtao Lu and Kui Jia", "abstract": "  Existing research on unconstrained in-the-wild head pose estimation suffers\nfrom the flaws of its datasets, which consist of either numerous samples by\nnon-realistic synthesis or constrained collection, or small-scale natural\nimages yet with plausible manual annotations. To alleviate it, we propose the\nfirst semi-supervised unconstrained head pose estimation method SemiUHPE, which\ncan leverage abundant easily available unlabeled head images. Technically, we\nchoose semi-supervised rotation regression and adapt it to the error-sensitive\nand label-scarce problem of unconstrained head pose. Our method is based on the\nobservation that the aspect-ratio invariant cropping of wild heads is superior\nto the previous landmark-based affine alignment given that landmarks of\nunconstrained human heads are usually unavailable, especially for less-explored\nnon-frontal heads. Instead of using an empirically fixed threshold to filter\nout pseudo labeled heads, we propose dynamic entropy based filtering to\nadaptively remove unlabeled outliers as training progresses by updating the\nthreshold in multiple stages. We then revisit the design of weak-strong\naugmentations and improve it by devising two novel head-oriented strong\naugmentations, termed pose-irrelevant cut-occlusion and pose-altering rotation\nconsistency respectively. Extensive experiments and ablation studies show that\nSemiUHPE outperforms existing methods greatly on public benchmarks under both\nthe front-range and full-range settings. Code is released in\n\\url{https://github.com/hnuzhy/SemiUHPE}.\n", "link": "http://arxiv.org/abs/2404.02544v2", "date": "2024-08-23", "relevancy": 2.1732, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5736}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5384}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Unconstrained%20Head%20Pose%20Estimation%20in%20the%20Wild&body=Title%3A%20Semi-Supervised%20Unconstrained%20Head%20Pose%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Huayi%20Zhou%20and%20Fei%20Jiang%20and%20Jin%20Yuan%20and%20Yong%20Rui%20and%20Hongtao%20Lu%20and%20Kui%20Jia%0AAbstract%3A%20%20%20Existing%20research%20on%20unconstrained%20in-the-wild%20head%20pose%20estimation%20suffers%0Afrom%20the%20flaws%20of%20its%20datasets%2C%20which%20consist%20of%20either%20numerous%20samples%20by%0Anon-realistic%20synthesis%20or%20constrained%20collection%2C%20or%20small-scale%20natural%0Aimages%20yet%20with%20plausible%20manual%20annotations.%20To%20alleviate%20it%2C%20we%20propose%20the%0Afirst%20semi-supervised%20unconstrained%20head%20pose%20estimation%20method%20SemiUHPE%2C%20which%0Acan%20leverage%20abundant%20easily%20available%20unlabeled%20head%20images.%20Technically%2C%20we%0Achoose%20semi-supervised%20rotation%20regression%20and%20adapt%20it%20to%20the%20error-sensitive%0Aand%20label-scarce%20problem%20of%20unconstrained%20head%20pose.%20Our%20method%20is%20based%20on%20the%0Aobservation%20that%20the%20aspect-ratio%20invariant%20cropping%20of%20wild%20heads%20is%20superior%0Ato%20the%20previous%20landmark-based%20affine%20alignment%20given%20that%20landmarks%20of%0Aunconstrained%20human%20heads%20are%20usually%20unavailable%2C%20especially%20for%20less-explored%0Anon-frontal%20heads.%20Instead%20of%20using%20an%20empirically%20fixed%20threshold%20to%20filter%0Aout%20pseudo%20labeled%20heads%2C%20we%20propose%20dynamic%20entropy%20based%20filtering%20to%0Aadaptively%20remove%20unlabeled%20outliers%20as%20training%20progresses%20by%20updating%20the%0Athreshold%20in%20multiple%20stages.%20We%20then%20revisit%20the%20design%20of%20weak-strong%0Aaugmentations%20and%20improve%20it%20by%20devising%20two%20novel%20head-oriented%20strong%0Aaugmentations%2C%20termed%20pose-irrelevant%20cut-occlusion%20and%20pose-altering%20rotation%0Aconsistency%20respectively.%20Extensive%20experiments%20and%20ablation%20studies%20show%20that%0ASemiUHPE%20outperforms%20existing%20methods%20greatly%20on%20public%20benchmarks%20under%20both%0Athe%20front-range%20and%20full-range%20settings.%20Code%20is%20released%20in%0A%5Curl%7Bhttps%3A//github.com/hnuzhy/SemiUHPE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Unconstrained%2520Head%2520Pose%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DHuayi%2520Zhou%2520and%2520Fei%2520Jiang%2520and%2520Jin%2520Yuan%2520and%2520Yong%2520Rui%2520and%2520Hongtao%2520Lu%2520and%2520Kui%2520Jia%26entry.1292438233%3D%2520%2520Existing%2520research%2520on%2520unconstrained%2520in-the-wild%2520head%2520pose%2520estimation%2520suffers%250Afrom%2520the%2520flaws%2520of%2520its%2520datasets%252C%2520which%2520consist%2520of%2520either%2520numerous%2520samples%2520by%250Anon-realistic%2520synthesis%2520or%2520constrained%2520collection%252C%2520or%2520small-scale%2520natural%250Aimages%2520yet%2520with%2520plausible%2520manual%2520annotations.%2520To%2520alleviate%2520it%252C%2520we%2520propose%2520the%250Afirst%2520semi-supervised%2520unconstrained%2520head%2520pose%2520estimation%2520method%2520SemiUHPE%252C%2520which%250Acan%2520leverage%2520abundant%2520easily%2520available%2520unlabeled%2520head%2520images.%2520Technically%252C%2520we%250Achoose%2520semi-supervised%2520rotation%2520regression%2520and%2520adapt%2520it%2520to%2520the%2520error-sensitive%250Aand%2520label-scarce%2520problem%2520of%2520unconstrained%2520head%2520pose.%2520Our%2520method%2520is%2520based%2520on%2520the%250Aobservation%2520that%2520the%2520aspect-ratio%2520invariant%2520cropping%2520of%2520wild%2520heads%2520is%2520superior%250Ato%2520the%2520previous%2520landmark-based%2520affine%2520alignment%2520given%2520that%2520landmarks%2520of%250Aunconstrained%2520human%2520heads%2520are%2520usually%2520unavailable%252C%2520especially%2520for%2520less-explored%250Anon-frontal%2520heads.%2520Instead%2520of%2520using%2520an%2520empirically%2520fixed%2520threshold%2520to%2520filter%250Aout%2520pseudo%2520labeled%2520heads%252C%2520we%2520propose%2520dynamic%2520entropy%2520based%2520filtering%2520to%250Aadaptively%2520remove%2520unlabeled%2520outliers%2520as%2520training%2520progresses%2520by%2520updating%2520the%250Athreshold%2520in%2520multiple%2520stages.%2520We%2520then%2520revisit%2520the%2520design%2520of%2520weak-strong%250Aaugmentations%2520and%2520improve%2520it%2520by%2520devising%2520two%2520novel%2520head-oriented%2520strong%250Aaugmentations%252C%2520termed%2520pose-irrelevant%2520cut-occlusion%2520and%2520pose-altering%2520rotation%250Aconsistency%2520respectively.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520show%2520that%250ASemiUHPE%2520outperforms%2520existing%2520methods%2520greatly%2520on%2520public%2520benchmarks%2520under%2520both%250Athe%2520front-range%2520and%2520full-range%2520settings.%2520Code%2520is%2520released%2520in%250A%255Curl%257Bhttps%253A//github.com/hnuzhy/SemiUHPE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Unconstrained%20Head%20Pose%20Estimation%20in%20the%20Wild&entry.906535625=Huayi%20Zhou%20and%20Fei%20Jiang%20and%20Jin%20Yuan%20and%20Yong%20Rui%20and%20Hongtao%20Lu%20and%20Kui%20Jia&entry.1292438233=%20%20Existing%20research%20on%20unconstrained%20in-the-wild%20head%20pose%20estimation%20suffers%0Afrom%20the%20flaws%20of%20its%20datasets%2C%20which%20consist%20of%20either%20numerous%20samples%20by%0Anon-realistic%20synthesis%20or%20constrained%20collection%2C%20or%20small-scale%20natural%0Aimages%20yet%20with%20plausible%20manual%20annotations.%20To%20alleviate%20it%2C%20we%20propose%20the%0Afirst%20semi-supervised%20unconstrained%20head%20pose%20estimation%20method%20SemiUHPE%2C%20which%0Acan%20leverage%20abundant%20easily%20available%20unlabeled%20head%20images.%20Technically%2C%20we%0Achoose%20semi-supervised%20rotation%20regression%20and%20adapt%20it%20to%20the%20error-sensitive%0Aand%20label-scarce%20problem%20of%20unconstrained%20head%20pose.%20Our%20method%20is%20based%20on%20the%0Aobservation%20that%20the%20aspect-ratio%20invariant%20cropping%20of%20wild%20heads%20is%20superior%0Ato%20the%20previous%20landmark-based%20affine%20alignment%20given%20that%20landmarks%20of%0Aunconstrained%20human%20heads%20are%20usually%20unavailable%2C%20especially%20for%20less-explored%0Anon-frontal%20heads.%20Instead%20of%20using%20an%20empirically%20fixed%20threshold%20to%20filter%0Aout%20pseudo%20labeled%20heads%2C%20we%20propose%20dynamic%20entropy%20based%20filtering%20to%0Aadaptively%20remove%20unlabeled%20outliers%20as%20training%20progresses%20by%20updating%20the%0Athreshold%20in%20multiple%20stages.%20We%20then%20revisit%20the%20design%20of%20weak-strong%0Aaugmentations%20and%20improve%20it%20by%20devising%20two%20novel%20head-oriented%20strong%0Aaugmentations%2C%20termed%20pose-irrelevant%20cut-occlusion%20and%20pose-altering%20rotation%0Aconsistency%20respectively.%20Extensive%20experiments%20and%20ablation%20studies%20show%20that%0ASemiUHPE%20outperforms%20existing%20methods%20greatly%20on%20public%20benchmarks%20under%20both%0Athe%20front-range%20and%20full-range%20settings.%20Code%20is%20released%20in%0A%5Curl%7Bhttps%3A//github.com/hnuzhy/SemiUHPE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02544v2&entry.124074799=Read"},
{"title": "Optimal OnTheFly Feedback Control of Event Sensors", "author": "Valery Vishnevskiy and Greg Burman and Sebastian Kozerke and Diederik Paul Moeys", "abstract": "  Event-based vision sensors produce an asynchronous stream of events which are\ntriggered when the pixel intensity variation exceeds a predefined threshold.\nSuch sensors offer significant advantages, including reduced data redundancy,\nmicro-second temporal resolution, and low power consumption, making them\nvaluable for applications in robotics and computer vision. In this work, we\nconsider the problem of video reconstruction from events, and propose an\napproach for dynamic feedback control of activation thresholds, in which a\ncontroller network analyzes the past emitted events and predicts the optimal\ndistribution of activation thresholds for the following time segment.\nAdditionally, we allow a user-defined target peak-event-rate for which the\ncontrol network is conditioned and optimized to predict per-column activation\nthresholds that would eventually produce the best possible video\nreconstruction. The proposed OnTheFly control scheme is data-driven and trained\nin an end-to-end fashion using probabilistic relaxation of the discrete event\nrepresentation. We demonstrate that our approach outperforms both fixed and\nrandomly-varying threshold schemes by 6-12% in terms of LPIPS perceptual image\ndissimilarity metric, and by 49% in terms of event rate, achieving superior\nreconstruction quality while enabling a fine-tuned balance between performance\naccuracy and the event rate. Additionally, we show that sampling strategies\nprovided by our OnTheFly control are interpretable and reflect the\ncharacteristics of the scene. Our results, derived from a physically-accurate\nsimulator, underline the promise of the proposed methodology in enhancing the\nutility of event cameras for image reconstruction and other downstream tasks,\npaving the way for hardware implementation of dynamic feedback EVS control in\nsilicon.\n", "link": "http://arxiv.org/abs/2408.12976v1", "date": "2024-08-23", "relevancy": 2.1676, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5581}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5446}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20OnTheFly%20Feedback%20Control%20of%20Event%20Sensors&body=Title%3A%20Optimal%20OnTheFly%20Feedback%20Control%20of%20Event%20Sensors%0AAuthor%3A%20Valery%20Vishnevskiy%20and%20Greg%20Burman%20and%20Sebastian%20Kozerke%20and%20Diederik%20Paul%20Moeys%0AAbstract%3A%20%20%20Event-based%20vision%20sensors%20produce%20an%20asynchronous%20stream%20of%20events%20which%20are%0Atriggered%20when%20the%20pixel%20intensity%20variation%20exceeds%20a%20predefined%20threshold.%0ASuch%20sensors%20offer%20significant%20advantages%2C%20including%20reduced%20data%20redundancy%2C%0Amicro-second%20temporal%20resolution%2C%20and%20low%20power%20consumption%2C%20making%20them%0Avaluable%20for%20applications%20in%20robotics%20and%20computer%20vision.%20In%20this%20work%2C%20we%0Aconsider%20the%20problem%20of%20video%20reconstruction%20from%20events%2C%20and%20propose%20an%0Aapproach%20for%20dynamic%20feedback%20control%20of%20activation%20thresholds%2C%20in%20which%20a%0Acontroller%20network%20analyzes%20the%20past%20emitted%20events%20and%20predicts%20the%20optimal%0Adistribution%20of%20activation%20thresholds%20for%20the%20following%20time%20segment.%0AAdditionally%2C%20we%20allow%20a%20user-defined%20target%20peak-event-rate%20for%20which%20the%0Acontrol%20network%20is%20conditioned%20and%20optimized%20to%20predict%20per-column%20activation%0Athresholds%20that%20would%20eventually%20produce%20the%20best%20possible%20video%0Areconstruction.%20The%20proposed%20OnTheFly%20control%20scheme%20is%20data-driven%20and%20trained%0Ain%20an%20end-to-end%20fashion%20using%20probabilistic%20relaxation%20of%20the%20discrete%20event%0Arepresentation.%20We%20demonstrate%20that%20our%20approach%20outperforms%20both%20fixed%20and%0Arandomly-varying%20threshold%20schemes%20by%206-12%25%20in%20terms%20of%20LPIPS%20perceptual%20image%0Adissimilarity%20metric%2C%20and%20by%2049%25%20in%20terms%20of%20event%20rate%2C%20achieving%20superior%0Areconstruction%20quality%20while%20enabling%20a%20fine-tuned%20balance%20between%20performance%0Aaccuracy%20and%20the%20event%20rate.%20Additionally%2C%20we%20show%20that%20sampling%20strategies%0Aprovided%20by%20our%20OnTheFly%20control%20are%20interpretable%20and%20reflect%20the%0Acharacteristics%20of%20the%20scene.%20Our%20results%2C%20derived%20from%20a%20physically-accurate%0Asimulator%2C%20underline%20the%20promise%20of%20the%20proposed%20methodology%20in%20enhancing%20the%0Autility%20of%20event%20cameras%20for%20image%20reconstruction%20and%20other%20downstream%20tasks%2C%0Apaving%20the%20way%20for%20hardware%20implementation%20of%20dynamic%20feedback%20EVS%20control%20in%0Asilicon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520OnTheFly%2520Feedback%2520Control%2520of%2520Event%2520Sensors%26entry.906535625%3DValery%2520Vishnevskiy%2520and%2520Greg%2520Burman%2520and%2520Sebastian%2520Kozerke%2520and%2520Diederik%2520Paul%2520Moeys%26entry.1292438233%3D%2520%2520Event-based%2520vision%2520sensors%2520produce%2520an%2520asynchronous%2520stream%2520of%2520events%2520which%2520are%250Atriggered%2520when%2520the%2520pixel%2520intensity%2520variation%2520exceeds%2520a%2520predefined%2520threshold.%250ASuch%2520sensors%2520offer%2520significant%2520advantages%252C%2520including%2520reduced%2520data%2520redundancy%252C%250Amicro-second%2520temporal%2520resolution%252C%2520and%2520low%2520power%2520consumption%252C%2520making%2520them%250Avaluable%2520for%2520applications%2520in%2520robotics%2520and%2520computer%2520vision.%2520In%2520this%2520work%252C%2520we%250Aconsider%2520the%2520problem%2520of%2520video%2520reconstruction%2520from%2520events%252C%2520and%2520propose%2520an%250Aapproach%2520for%2520dynamic%2520feedback%2520control%2520of%2520activation%2520thresholds%252C%2520in%2520which%2520a%250Acontroller%2520network%2520analyzes%2520the%2520past%2520emitted%2520events%2520and%2520predicts%2520the%2520optimal%250Adistribution%2520of%2520activation%2520thresholds%2520for%2520the%2520following%2520time%2520segment.%250AAdditionally%252C%2520we%2520allow%2520a%2520user-defined%2520target%2520peak-event-rate%2520for%2520which%2520the%250Acontrol%2520network%2520is%2520conditioned%2520and%2520optimized%2520to%2520predict%2520per-column%2520activation%250Athresholds%2520that%2520would%2520eventually%2520produce%2520the%2520best%2520possible%2520video%250Areconstruction.%2520The%2520proposed%2520OnTheFly%2520control%2520scheme%2520is%2520data-driven%2520and%2520trained%250Ain%2520an%2520end-to-end%2520fashion%2520using%2520probabilistic%2520relaxation%2520of%2520the%2520discrete%2520event%250Arepresentation.%2520We%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520both%2520fixed%2520and%250Arandomly-varying%2520threshold%2520schemes%2520by%25206-12%2525%2520in%2520terms%2520of%2520LPIPS%2520perceptual%2520image%250Adissimilarity%2520metric%252C%2520and%2520by%252049%2525%2520in%2520terms%2520of%2520event%2520rate%252C%2520achieving%2520superior%250Areconstruction%2520quality%2520while%2520enabling%2520a%2520fine-tuned%2520balance%2520between%2520performance%250Aaccuracy%2520and%2520the%2520event%2520rate.%2520Additionally%252C%2520we%2520show%2520that%2520sampling%2520strategies%250Aprovided%2520by%2520our%2520OnTheFly%2520control%2520are%2520interpretable%2520and%2520reflect%2520the%250Acharacteristics%2520of%2520the%2520scene.%2520Our%2520results%252C%2520derived%2520from%2520a%2520physically-accurate%250Asimulator%252C%2520underline%2520the%2520promise%2520of%2520the%2520proposed%2520methodology%2520in%2520enhancing%2520the%250Autility%2520of%2520event%2520cameras%2520for%2520image%2520reconstruction%2520and%2520other%2520downstream%2520tasks%252C%250Apaving%2520the%2520way%2520for%2520hardware%2520implementation%2520of%2520dynamic%2520feedback%2520EVS%2520control%2520in%250Asilicon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20OnTheFly%20Feedback%20Control%20of%20Event%20Sensors&entry.906535625=Valery%20Vishnevskiy%20and%20Greg%20Burman%20and%20Sebastian%20Kozerke%20and%20Diederik%20Paul%20Moeys&entry.1292438233=%20%20Event-based%20vision%20sensors%20produce%20an%20asynchronous%20stream%20of%20events%20which%20are%0Atriggered%20when%20the%20pixel%20intensity%20variation%20exceeds%20a%20predefined%20threshold.%0ASuch%20sensors%20offer%20significant%20advantages%2C%20including%20reduced%20data%20redundancy%2C%0Amicro-second%20temporal%20resolution%2C%20and%20low%20power%20consumption%2C%20making%20them%0Avaluable%20for%20applications%20in%20robotics%20and%20computer%20vision.%20In%20this%20work%2C%20we%0Aconsider%20the%20problem%20of%20video%20reconstruction%20from%20events%2C%20and%20propose%20an%0Aapproach%20for%20dynamic%20feedback%20control%20of%20activation%20thresholds%2C%20in%20which%20a%0Acontroller%20network%20analyzes%20the%20past%20emitted%20events%20and%20predicts%20the%20optimal%0Adistribution%20of%20activation%20thresholds%20for%20the%20following%20time%20segment.%0AAdditionally%2C%20we%20allow%20a%20user-defined%20target%20peak-event-rate%20for%20which%20the%0Acontrol%20network%20is%20conditioned%20and%20optimized%20to%20predict%20per-column%20activation%0Athresholds%20that%20would%20eventually%20produce%20the%20best%20possible%20video%0Areconstruction.%20The%20proposed%20OnTheFly%20control%20scheme%20is%20data-driven%20and%20trained%0Ain%20an%20end-to-end%20fashion%20using%20probabilistic%20relaxation%20of%20the%20discrete%20event%0Arepresentation.%20We%20demonstrate%20that%20our%20approach%20outperforms%20both%20fixed%20and%0Arandomly-varying%20threshold%20schemes%20by%206-12%25%20in%20terms%20of%20LPIPS%20perceptual%20image%0Adissimilarity%20metric%2C%20and%20by%2049%25%20in%20terms%20of%20event%20rate%2C%20achieving%20superior%0Areconstruction%20quality%20while%20enabling%20a%20fine-tuned%20balance%20between%20performance%0Aaccuracy%20and%20the%20event%20rate.%20Additionally%2C%20we%20show%20that%20sampling%20strategies%0Aprovided%20by%20our%20OnTheFly%20control%20are%20interpretable%20and%20reflect%20the%0Acharacteristics%20of%20the%20scene.%20Our%20results%2C%20derived%20from%20a%20physically-accurate%0Asimulator%2C%20underline%20the%20promise%20of%20the%20proposed%20methodology%20in%20enhancing%20the%0Autility%20of%20event%20cameras%20for%20image%20reconstruction%20and%20other%20downstream%20tasks%2C%0Apaving%20the%20way%20for%20hardware%20implementation%20of%20dynamic%20feedback%20EVS%20control%20in%0Asilicon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12976v1&entry.124074799=Read"},
{"title": "VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation\n  Models", "author": "Wentao Wu and Fanghua Hong and Xiao Wang and Chenglong Li and Jin Tang", "abstract": "  Existing vehicle detectors are usually obtained by training a typical\ndetector (e.g., YOLO, RCNN, DETR series) on vehicle images based on a\npre-trained backbone (e.g., ResNet, ViT). Some researchers also exploit and\nenhance the detection performance using pre-trained large foundation models.\nHowever, we think these detectors may only get sub-optimal results because the\nlarge models they use are not specifically designed for vehicles. In addition,\ntheir results heavily rely on visual features, and seldom of they consider the\nalignment between the vehicle's semantic information and visual\nrepresentations. In this work, we propose a new vehicle detection paradigm\nbased on a pre-trained foundation vehicle model (VehicleMAE) and a large\nlanguage model (T5), termed VFM-Det. It follows the region proposal-based\ndetection framework and the features of each proposal can be enhanced using\nVehicleMAE. More importantly, we propose a new VAtt2Vec module that predicts\nthe vehicle semantic attributes of these proposals and transforms them into\nfeature vectors to enhance the vision features via contrastive learning.\nExtensive experiments on three vehicle detection benchmark datasets thoroughly\nproved the effectiveness of our vehicle detector. Specifically, our model\nimproves the baseline approach by $+5.1\\%$, $+6.2\\%$ on the $AP_{0.5}$,\n$AP_{0.75}$ metrics, respectively, on the Cityscapes dataset.The source code of\nthis work will be released at https://github.com/Event-AHU/VFM-Det.\n", "link": "http://arxiv.org/abs/2408.13031v1", "date": "2024-08-23", "relevancy": 2.1329, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5429}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VFM-Det%3A%20Towards%20High-Performance%20Vehicle%20Detection%20via%20Large%20Foundation%0A%20%20Models&body=Title%3A%20VFM-Det%3A%20Towards%20High-Performance%20Vehicle%20Detection%20via%20Large%20Foundation%0A%20%20Models%0AAuthor%3A%20Wentao%20Wu%20and%20Fanghua%20Hong%20and%20Xiao%20Wang%20and%20Chenglong%20Li%20and%20Jin%20Tang%0AAbstract%3A%20%20%20Existing%20vehicle%20detectors%20are%20usually%20obtained%20by%20training%20a%20typical%0Adetector%20%28e.g.%2C%20YOLO%2C%20RCNN%2C%20DETR%20series%29%20on%20vehicle%20images%20based%20on%20a%0Apre-trained%20backbone%20%28e.g.%2C%20ResNet%2C%20ViT%29.%20Some%20researchers%20also%20exploit%20and%0Aenhance%20the%20detection%20performance%20using%20pre-trained%20large%20foundation%20models.%0AHowever%2C%20we%20think%20these%20detectors%20may%20only%20get%20sub-optimal%20results%20because%20the%0Alarge%20models%20they%20use%20are%20not%20specifically%20designed%20for%20vehicles.%20In%20addition%2C%0Atheir%20results%20heavily%20rely%20on%20visual%20features%2C%20and%20seldom%20of%20they%20consider%20the%0Aalignment%20between%20the%20vehicle%27s%20semantic%20information%20and%20visual%0Arepresentations.%20In%20this%20work%2C%20we%20propose%20a%20new%20vehicle%20detection%20paradigm%0Abased%20on%20a%20pre-trained%20foundation%20vehicle%20model%20%28VehicleMAE%29%20and%20a%20large%0Alanguage%20model%20%28T5%29%2C%20termed%20VFM-Det.%20It%20follows%20the%20region%20proposal-based%0Adetection%20framework%20and%20the%20features%20of%20each%20proposal%20can%20be%20enhanced%20using%0AVehicleMAE.%20More%20importantly%2C%20we%20propose%20a%20new%20VAtt2Vec%20module%20that%20predicts%0Athe%20vehicle%20semantic%20attributes%20of%20these%20proposals%20and%20transforms%20them%20into%0Afeature%20vectors%20to%20enhance%20the%20vision%20features%20via%20contrastive%20learning.%0AExtensive%20experiments%20on%20three%20vehicle%20detection%20benchmark%20datasets%20thoroughly%0Aproved%20the%20effectiveness%20of%20our%20vehicle%20detector.%20Specifically%2C%20our%20model%0Aimproves%20the%20baseline%20approach%20by%20%24%2B5.1%5C%25%24%2C%20%24%2B6.2%5C%25%24%20on%20the%20%24AP_%7B0.5%7D%24%2C%0A%24AP_%7B0.75%7D%24%20metrics%2C%20respectively%2C%20on%20the%20Cityscapes%20dataset.The%20source%20code%20of%0Athis%20work%20will%20be%20released%20at%20https%3A//github.com/Event-AHU/VFM-Det.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVFM-Det%253A%2520Towards%2520High-Performance%2520Vehicle%2520Detection%2520via%2520Large%2520Foundation%250A%2520%2520Models%26entry.906535625%3DWentao%2520Wu%2520and%2520Fanghua%2520Hong%2520and%2520Xiao%2520Wang%2520and%2520Chenglong%2520Li%2520and%2520Jin%2520Tang%26entry.1292438233%3D%2520%2520Existing%2520vehicle%2520detectors%2520are%2520usually%2520obtained%2520by%2520training%2520a%2520typical%250Adetector%2520%2528e.g.%252C%2520YOLO%252C%2520RCNN%252C%2520DETR%2520series%2529%2520on%2520vehicle%2520images%2520based%2520on%2520a%250Apre-trained%2520backbone%2520%2528e.g.%252C%2520ResNet%252C%2520ViT%2529.%2520Some%2520researchers%2520also%2520exploit%2520and%250Aenhance%2520the%2520detection%2520performance%2520using%2520pre-trained%2520large%2520foundation%2520models.%250AHowever%252C%2520we%2520think%2520these%2520detectors%2520may%2520only%2520get%2520sub-optimal%2520results%2520because%2520the%250Alarge%2520models%2520they%2520use%2520are%2520not%2520specifically%2520designed%2520for%2520vehicles.%2520In%2520addition%252C%250Atheir%2520results%2520heavily%2520rely%2520on%2520visual%2520features%252C%2520and%2520seldom%2520of%2520they%2520consider%2520the%250Aalignment%2520between%2520the%2520vehicle%2527s%2520semantic%2520information%2520and%2520visual%250Arepresentations.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520vehicle%2520detection%2520paradigm%250Abased%2520on%2520a%2520pre-trained%2520foundation%2520vehicle%2520model%2520%2528VehicleMAE%2529%2520and%2520a%2520large%250Alanguage%2520model%2520%2528T5%2529%252C%2520termed%2520VFM-Det.%2520It%2520follows%2520the%2520region%2520proposal-based%250Adetection%2520framework%2520and%2520the%2520features%2520of%2520each%2520proposal%2520can%2520be%2520enhanced%2520using%250AVehicleMAE.%2520More%2520importantly%252C%2520we%2520propose%2520a%2520new%2520VAtt2Vec%2520module%2520that%2520predicts%250Athe%2520vehicle%2520semantic%2520attributes%2520of%2520these%2520proposals%2520and%2520transforms%2520them%2520into%250Afeature%2520vectors%2520to%2520enhance%2520the%2520vision%2520features%2520via%2520contrastive%2520learning.%250AExtensive%2520experiments%2520on%2520three%2520vehicle%2520detection%2520benchmark%2520datasets%2520thoroughly%250Aproved%2520the%2520effectiveness%2520of%2520our%2520vehicle%2520detector.%2520Specifically%252C%2520our%2520model%250Aimproves%2520the%2520baseline%2520approach%2520by%2520%2524%252B5.1%255C%2525%2524%252C%2520%2524%252B6.2%255C%2525%2524%2520on%2520the%2520%2524AP_%257B0.5%257D%2524%252C%250A%2524AP_%257B0.75%257D%2524%2520metrics%252C%2520respectively%252C%2520on%2520the%2520Cityscapes%2520dataset.The%2520source%2520code%2520of%250Athis%2520work%2520will%2520be%2520released%2520at%2520https%253A//github.com/Event-AHU/VFM-Det.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFM-Det%3A%20Towards%20High-Performance%20Vehicle%20Detection%20via%20Large%20Foundation%0A%20%20Models&entry.906535625=Wentao%20Wu%20and%20Fanghua%20Hong%20and%20Xiao%20Wang%20and%20Chenglong%20Li%20and%20Jin%20Tang&entry.1292438233=%20%20Existing%20vehicle%20detectors%20are%20usually%20obtained%20by%20training%20a%20typical%0Adetector%20%28e.g.%2C%20YOLO%2C%20RCNN%2C%20DETR%20series%29%20on%20vehicle%20images%20based%20on%20a%0Apre-trained%20backbone%20%28e.g.%2C%20ResNet%2C%20ViT%29.%20Some%20researchers%20also%20exploit%20and%0Aenhance%20the%20detection%20performance%20using%20pre-trained%20large%20foundation%20models.%0AHowever%2C%20we%20think%20these%20detectors%20may%20only%20get%20sub-optimal%20results%20because%20the%0Alarge%20models%20they%20use%20are%20not%20specifically%20designed%20for%20vehicles.%20In%20addition%2C%0Atheir%20results%20heavily%20rely%20on%20visual%20features%2C%20and%20seldom%20of%20they%20consider%20the%0Aalignment%20between%20the%20vehicle%27s%20semantic%20information%20and%20visual%0Arepresentations.%20In%20this%20work%2C%20we%20propose%20a%20new%20vehicle%20detection%20paradigm%0Abased%20on%20a%20pre-trained%20foundation%20vehicle%20model%20%28VehicleMAE%29%20and%20a%20large%0Alanguage%20model%20%28T5%29%2C%20termed%20VFM-Det.%20It%20follows%20the%20region%20proposal-based%0Adetection%20framework%20and%20the%20features%20of%20each%20proposal%20can%20be%20enhanced%20using%0AVehicleMAE.%20More%20importantly%2C%20we%20propose%20a%20new%20VAtt2Vec%20module%20that%20predicts%0Athe%20vehicle%20semantic%20attributes%20of%20these%20proposals%20and%20transforms%20them%20into%0Afeature%20vectors%20to%20enhance%20the%20vision%20features%20via%20contrastive%20learning.%0AExtensive%20experiments%20on%20three%20vehicle%20detection%20benchmark%20datasets%20thoroughly%0Aproved%20the%20effectiveness%20of%20our%20vehicle%20detector.%20Specifically%2C%20our%20model%0Aimproves%20the%20baseline%20approach%20by%20%24%2B5.1%5C%25%24%2C%20%24%2B6.2%5C%25%24%20on%20the%20%24AP_%7B0.5%7D%24%2C%0A%24AP_%7B0.75%7D%24%20metrics%2C%20respectively%2C%20on%20the%20Cityscapes%20dataset.The%20source%20code%20of%0Athis%20work%20will%20be%20released%20at%20https%3A//github.com/Event-AHU/VFM-Det.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13031v1&entry.124074799=Read"},
{"title": "Accelerating the k-means++ Algorithm by Using Geometric Information", "author": "Guillem Rodr\u00edguez Corominas and Maria J. Blesa and Christian Blum", "abstract": "  In this paper, we propose an acceleration of the exact k-means++ algorithm\nusing geometric information, specifically the Triangle Inequality and\nadditional norm filters, along with a two-step sampling procedure. Our\nexperiments demonstrate that the accelerated version outperforms the standard\nk-means++ version in terms of the number of visited points and distance\ncalculations, achieving greater speedup as the number of clusters increases.\nThe version utilizing the Triangle Inequality is particularly effective for\nlow-dimensional data, while the additional norm-based filter enhances\nperformance in high-dimensional instances with greater norm variance among\npoints. Additional experiments show the behavior of our algorithms when\nexecuted concurrently across multiple jobs and examine how memory performance\nimpacts practical speedup.\n", "link": "http://arxiv.org/abs/2408.13189v1", "date": "2024-08-23", "relevancy": 2.1135, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4262}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4212}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20the%20k-means%2B%2B%20Algorithm%20by%20Using%20Geometric%20Information&body=Title%3A%20Accelerating%20the%20k-means%2B%2B%20Algorithm%20by%20Using%20Geometric%20Information%0AAuthor%3A%20Guillem%20Rodr%C3%ADguez%20Corominas%20and%20Maria%20J.%20Blesa%20and%20Christian%20Blum%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20acceleration%20of%20the%20exact%20k-means%2B%2B%20algorithm%0Ausing%20geometric%20information%2C%20specifically%20the%20Triangle%20Inequality%20and%0Aadditional%20norm%20filters%2C%20along%20with%20a%20two-step%20sampling%20procedure.%20Our%0Aexperiments%20demonstrate%20that%20the%20accelerated%20version%20outperforms%20the%20standard%0Ak-means%2B%2B%20version%20in%20terms%20of%20the%20number%20of%20visited%20points%20and%20distance%0Acalculations%2C%20achieving%20greater%20speedup%20as%20the%20number%20of%20clusters%20increases.%0AThe%20version%20utilizing%20the%20Triangle%20Inequality%20is%20particularly%20effective%20for%0Alow-dimensional%20data%2C%20while%20the%20additional%20norm-based%20filter%20enhances%0Aperformance%20in%20high-dimensional%20instances%20with%20greater%20norm%20variance%20among%0Apoints.%20Additional%20experiments%20show%20the%20behavior%20of%20our%20algorithms%20when%0Aexecuted%20concurrently%20across%20multiple%20jobs%20and%20examine%20how%20memory%20performance%0Aimpacts%20practical%20speedup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520the%2520k-means%252B%252B%2520Algorithm%2520by%2520Using%2520Geometric%2520Information%26entry.906535625%3DGuillem%2520Rodr%25C3%25ADguez%2520Corominas%2520and%2520Maria%2520J.%2520Blesa%2520and%2520Christian%2520Blum%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520acceleration%2520of%2520the%2520exact%2520k-means%252B%252B%2520algorithm%250Ausing%2520geometric%2520information%252C%2520specifically%2520the%2520Triangle%2520Inequality%2520and%250Aadditional%2520norm%2520filters%252C%2520along%2520with%2520a%2520two-step%2520sampling%2520procedure.%2520Our%250Aexperiments%2520demonstrate%2520that%2520the%2520accelerated%2520version%2520outperforms%2520the%2520standard%250Ak-means%252B%252B%2520version%2520in%2520terms%2520of%2520the%2520number%2520of%2520visited%2520points%2520and%2520distance%250Acalculations%252C%2520achieving%2520greater%2520speedup%2520as%2520the%2520number%2520of%2520clusters%2520increases.%250AThe%2520version%2520utilizing%2520the%2520Triangle%2520Inequality%2520is%2520particularly%2520effective%2520for%250Alow-dimensional%2520data%252C%2520while%2520the%2520additional%2520norm-based%2520filter%2520enhances%250Aperformance%2520in%2520high-dimensional%2520instances%2520with%2520greater%2520norm%2520variance%2520among%250Apoints.%2520Additional%2520experiments%2520show%2520the%2520behavior%2520of%2520our%2520algorithms%2520when%250Aexecuted%2520concurrently%2520across%2520multiple%2520jobs%2520and%2520examine%2520how%2520memory%2520performance%250Aimpacts%2520practical%2520speedup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20the%20k-means%2B%2B%20Algorithm%20by%20Using%20Geometric%20Information&entry.906535625=Guillem%20Rodr%C3%ADguez%20Corominas%20and%20Maria%20J.%20Blesa%20and%20Christian%20Blum&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20acceleration%20of%20the%20exact%20k-means%2B%2B%20algorithm%0Ausing%20geometric%20information%2C%20specifically%20the%20Triangle%20Inequality%20and%0Aadditional%20norm%20filters%2C%20along%20with%20a%20two-step%20sampling%20procedure.%20Our%0Aexperiments%20demonstrate%20that%20the%20accelerated%20version%20outperforms%20the%20standard%0Ak-means%2B%2B%20version%20in%20terms%20of%20the%20number%20of%20visited%20points%20and%20distance%0Acalculations%2C%20achieving%20greater%20speedup%20as%20the%20number%20of%20clusters%20increases.%0AThe%20version%20utilizing%20the%20Triangle%20Inequality%20is%20particularly%20effective%20for%0Alow-dimensional%20data%2C%20while%20the%20additional%20norm-based%20filter%20enhances%0Aperformance%20in%20high-dimensional%20instances%20with%20greater%20norm%20variance%20among%0Apoints.%20Additional%20experiments%20show%20the%20behavior%20of%20our%20algorithms%20when%0Aexecuted%20concurrently%20across%20multiple%20jobs%20and%20examine%20how%20memory%20performance%0Aimpacts%20practical%20speedup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13189v1&entry.124074799=Read"},
{"title": "DeTPP: Leveraging Object Detection for Robust Long-Horizon Event\n  Prediction", "author": "Ivan Karpukhin and Andrey Savchenko", "abstract": "  Forecasting future events over extended periods, known as long-horizon\nprediction, is a fundamental task in various domains, including retail,\nfinance, healthcare, and social networks. Traditional methods, such as Marked\nTemporal Point Processes (MTPP), typically use autoregressive models to predict\nmultiple future events. However, these models frequently encounter issues such\nas converging to constant or repetitive outputs, which significantly limits\ntheir effectiveness and applicability. To overcome these limitations, we\npropose DeTPP (Detection-based Temporal Point Processes), a novel approach\ninspired by object detection methods from computer vision. DeTPP utilizes a\nnovel matching-based loss function that selectively focuses on reliably\npredictable events, enhancing both training robustness and inference diversity.\nOur method sets a new state-of-the-art in long-horizon event prediction,\nsignificantly outperforming existing MTPP and next-K approaches. The\nimplementation of DeTPP is publicly available on GitHub.\n", "link": "http://arxiv.org/abs/2408.13131v1", "date": "2024-08-23", "relevancy": 2.0917, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5483}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeTPP%3A%20Leveraging%20Object%20Detection%20for%20Robust%20Long-Horizon%20Event%0A%20%20Prediction&body=Title%3A%20DeTPP%3A%20Leveraging%20Object%20Detection%20for%20Robust%20Long-Horizon%20Event%0A%20%20Prediction%0AAuthor%3A%20Ivan%20Karpukhin%20and%20Andrey%20Savchenko%0AAbstract%3A%20%20%20Forecasting%20future%20events%20over%20extended%20periods%2C%20known%20as%20long-horizon%0Aprediction%2C%20is%20a%20fundamental%20task%20in%20various%20domains%2C%20including%20retail%2C%0Afinance%2C%20healthcare%2C%20and%20social%20networks.%20Traditional%20methods%2C%20such%20as%20Marked%0ATemporal%20Point%20Processes%20%28MTPP%29%2C%20typically%20use%20autoregressive%20models%20to%20predict%0Amultiple%20future%20events.%20However%2C%20these%20models%20frequently%20encounter%20issues%20such%0Aas%20converging%20to%20constant%20or%20repetitive%20outputs%2C%20which%20significantly%20limits%0Atheir%20effectiveness%20and%20applicability.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20DeTPP%20%28Detection-based%20Temporal%20Point%20Processes%29%2C%20a%20novel%20approach%0Ainspired%20by%20object%20detection%20methods%20from%20computer%20vision.%20DeTPP%20utilizes%20a%0Anovel%20matching-based%20loss%20function%20that%20selectively%20focuses%20on%20reliably%0Apredictable%20events%2C%20enhancing%20both%20training%20robustness%20and%20inference%20diversity.%0AOur%20method%20sets%20a%20new%20state-of-the-art%20in%20long-horizon%20event%20prediction%2C%0Asignificantly%20outperforming%20existing%20MTPP%20and%20next-K%20approaches.%20The%0Aimplementation%20of%20DeTPP%20is%20publicly%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeTPP%253A%2520Leveraging%2520Object%2520Detection%2520for%2520Robust%2520Long-Horizon%2520Event%250A%2520%2520Prediction%26entry.906535625%3DIvan%2520Karpukhin%2520and%2520Andrey%2520Savchenko%26entry.1292438233%3D%2520%2520Forecasting%2520future%2520events%2520over%2520extended%2520periods%252C%2520known%2520as%2520long-horizon%250Aprediction%252C%2520is%2520a%2520fundamental%2520task%2520in%2520various%2520domains%252C%2520including%2520retail%252C%250Afinance%252C%2520healthcare%252C%2520and%2520social%2520networks.%2520Traditional%2520methods%252C%2520such%2520as%2520Marked%250ATemporal%2520Point%2520Processes%2520%2528MTPP%2529%252C%2520typically%2520use%2520autoregressive%2520models%2520to%2520predict%250Amultiple%2520future%2520events.%2520However%252C%2520these%2520models%2520frequently%2520encounter%2520issues%2520such%250Aas%2520converging%2520to%2520constant%2520or%2520repetitive%2520outputs%252C%2520which%2520significantly%2520limits%250Atheir%2520effectiveness%2520and%2520applicability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520DeTPP%2520%2528Detection-based%2520Temporal%2520Point%2520Processes%2529%252C%2520a%2520novel%2520approach%250Ainspired%2520by%2520object%2520detection%2520methods%2520from%2520computer%2520vision.%2520DeTPP%2520utilizes%2520a%250Anovel%2520matching-based%2520loss%2520function%2520that%2520selectively%2520focuses%2520on%2520reliably%250Apredictable%2520events%252C%2520enhancing%2520both%2520training%2520robustness%2520and%2520inference%2520diversity.%250AOur%2520method%2520sets%2520a%2520new%2520state-of-the-art%2520in%2520long-horizon%2520event%2520prediction%252C%250Asignificantly%2520outperforming%2520existing%2520MTPP%2520and%2520next-K%2520approaches.%2520The%250Aimplementation%2520of%2520DeTPP%2520is%2520publicly%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeTPP%3A%20Leveraging%20Object%20Detection%20for%20Robust%20Long-Horizon%20Event%0A%20%20Prediction&entry.906535625=Ivan%20Karpukhin%20and%20Andrey%20Savchenko&entry.1292438233=%20%20Forecasting%20future%20events%20over%20extended%20periods%2C%20known%20as%20long-horizon%0Aprediction%2C%20is%20a%20fundamental%20task%20in%20various%20domains%2C%20including%20retail%2C%0Afinance%2C%20healthcare%2C%20and%20social%20networks.%20Traditional%20methods%2C%20such%20as%20Marked%0ATemporal%20Point%20Processes%20%28MTPP%29%2C%20typically%20use%20autoregressive%20models%20to%20predict%0Amultiple%20future%20events.%20However%2C%20these%20models%20frequently%20encounter%20issues%20such%0Aas%20converging%20to%20constant%20or%20repetitive%20outputs%2C%20which%20significantly%20limits%0Atheir%20effectiveness%20and%20applicability.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20DeTPP%20%28Detection-based%20Temporal%20Point%20Processes%29%2C%20a%20novel%20approach%0Ainspired%20by%20object%20detection%20methods%20from%20computer%20vision.%20DeTPP%20utilizes%20a%0Anovel%20matching-based%20loss%20function%20that%20selectively%20focuses%20on%20reliably%0Apredictable%20events%2C%20enhancing%20both%20training%20robustness%20and%20inference%20diversity.%0AOur%20method%20sets%20a%20new%20state-of-the-art%20in%20long-horizon%20event%20prediction%2C%0Asignificantly%20outperforming%20existing%20MTPP%20and%20next-K%20approaches.%20The%0Aimplementation%20of%20DeTPP%20is%20publicly%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13131v1&entry.124074799=Read"},
{"title": "Dynamic Label Adversarial Training for Deep Learning Robustness Against\n  Adversarial Attacks", "author": "Zhenyu Liu and Haoran Duan and Huizhi Liang and Yang Long and Vaclav Snasel and Guiseppe Nicosia and Rajiv Ranjan and Varun Ojha", "abstract": "  Adversarial training is one of the most effective methods for enhancing model\nrobustness. Recent approaches incorporate adversarial distillation in\nadversarial training architectures. However, we notice two scenarios of defense\nmethods that limit their performance: (1) Previous methods primarily use static\nground truth for adversarial training, but this often causes robust\noverfitting; (2) The loss functions are either Mean Squared Error or\nKL-divergence leading to a sub-optimal performance on clean accuracy. To solve\nthose problems, we propose a dynamic label adversarial training (DYNAT)\nalgorithm that enables the target model to gradually and dynamically gain\nrobustness from the guide model's decisions. Additionally, we found that a\nbudgeted dimension of inner optimization for the target model may contribute to\nthe trade-off between clean accuracy and robust accuracy. Therefore, we propose\na novel inner optimization method to be incorporated into the adversarial\ntraining. This will enable the target model to adaptively search for\nadversarial examples based on dynamic labels from the guiding model,\ncontributing to the robustness of the target model. Extensive experiments\nvalidate the superior performance of our approach.\n", "link": "http://arxiv.org/abs/2408.13102v1", "date": "2024-08-23", "relevancy": 2.0908, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5553}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Label%20Adversarial%20Training%20for%20Deep%20Learning%20Robustness%20Against%0A%20%20Adversarial%20Attacks&body=Title%3A%20Dynamic%20Label%20Adversarial%20Training%20for%20Deep%20Learning%20Robustness%20Against%0A%20%20Adversarial%20Attacks%0AAuthor%3A%20Zhenyu%20Liu%20and%20Haoran%20Duan%20and%20Huizhi%20Liang%20and%20Yang%20Long%20and%20Vaclav%20Snasel%20and%20Guiseppe%20Nicosia%20and%20Rajiv%20Ranjan%20and%20Varun%20Ojha%0AAbstract%3A%20%20%20Adversarial%20training%20is%20one%20of%20the%20most%20effective%20methods%20for%20enhancing%20model%0Arobustness.%20Recent%20approaches%20incorporate%20adversarial%20distillation%20in%0Aadversarial%20training%20architectures.%20However%2C%20we%20notice%20two%20scenarios%20of%20defense%0Amethods%20that%20limit%20their%20performance%3A%20%281%29%20Previous%20methods%20primarily%20use%20static%0Aground%20truth%20for%20adversarial%20training%2C%20but%20this%20often%20causes%20robust%0Aoverfitting%3B%20%282%29%20The%20loss%20functions%20are%20either%20Mean%20Squared%20Error%20or%0AKL-divergence%20leading%20to%20a%20sub-optimal%20performance%20on%20clean%20accuracy.%20To%20solve%0Athose%20problems%2C%20we%20propose%20a%20dynamic%20label%20adversarial%20training%20%28DYNAT%29%0Aalgorithm%20that%20enables%20the%20target%20model%20to%20gradually%20and%20dynamically%20gain%0Arobustness%20from%20the%20guide%20model%27s%20decisions.%20Additionally%2C%20we%20found%20that%20a%0Abudgeted%20dimension%20of%20inner%20optimization%20for%20the%20target%20model%20may%20contribute%20to%0Athe%20trade-off%20between%20clean%20accuracy%20and%20robust%20accuracy.%20Therefore%2C%20we%20propose%0Aa%20novel%20inner%20optimization%20method%20to%20be%20incorporated%20into%20the%20adversarial%0Atraining.%20This%20will%20enable%20the%20target%20model%20to%20adaptively%20search%20for%0Aadversarial%20examples%20based%20on%20dynamic%20labels%20from%20the%20guiding%20model%2C%0Acontributing%20to%20the%20robustness%20of%20the%20target%20model.%20Extensive%20experiments%0Avalidate%20the%20superior%20performance%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Label%2520Adversarial%2520Training%2520for%2520Deep%2520Learning%2520Robustness%2520Against%250A%2520%2520Adversarial%2520Attacks%26entry.906535625%3DZhenyu%2520Liu%2520and%2520Haoran%2520Duan%2520and%2520Huizhi%2520Liang%2520and%2520Yang%2520Long%2520and%2520Vaclav%2520Snasel%2520and%2520Guiseppe%2520Nicosia%2520and%2520Rajiv%2520Ranjan%2520and%2520Varun%2520Ojha%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520is%2520one%2520of%2520the%2520most%2520effective%2520methods%2520for%2520enhancing%2520model%250Arobustness.%2520Recent%2520approaches%2520incorporate%2520adversarial%2520distillation%2520in%250Aadversarial%2520training%2520architectures.%2520However%252C%2520we%2520notice%2520two%2520scenarios%2520of%2520defense%250Amethods%2520that%2520limit%2520their%2520performance%253A%2520%25281%2529%2520Previous%2520methods%2520primarily%2520use%2520static%250Aground%2520truth%2520for%2520adversarial%2520training%252C%2520but%2520this%2520often%2520causes%2520robust%250Aoverfitting%253B%2520%25282%2529%2520The%2520loss%2520functions%2520are%2520either%2520Mean%2520Squared%2520Error%2520or%250AKL-divergence%2520leading%2520to%2520a%2520sub-optimal%2520performance%2520on%2520clean%2520accuracy.%2520To%2520solve%250Athose%2520problems%252C%2520we%2520propose%2520a%2520dynamic%2520label%2520adversarial%2520training%2520%2528DYNAT%2529%250Aalgorithm%2520that%2520enables%2520the%2520target%2520model%2520to%2520gradually%2520and%2520dynamically%2520gain%250Arobustness%2520from%2520the%2520guide%2520model%2527s%2520decisions.%2520Additionally%252C%2520we%2520found%2520that%2520a%250Abudgeted%2520dimension%2520of%2520inner%2520optimization%2520for%2520the%2520target%2520model%2520may%2520contribute%2520to%250Athe%2520trade-off%2520between%2520clean%2520accuracy%2520and%2520robust%2520accuracy.%2520Therefore%252C%2520we%2520propose%250Aa%2520novel%2520inner%2520optimization%2520method%2520to%2520be%2520incorporated%2520into%2520the%2520adversarial%250Atraining.%2520This%2520will%2520enable%2520the%2520target%2520model%2520to%2520adaptively%2520search%2520for%250Aadversarial%2520examples%2520based%2520on%2520dynamic%2520labels%2520from%2520the%2520guiding%2520model%252C%250Acontributing%2520to%2520the%2520robustness%2520of%2520the%2520target%2520model.%2520Extensive%2520experiments%250Avalidate%2520the%2520superior%2520performance%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Label%20Adversarial%20Training%20for%20Deep%20Learning%20Robustness%20Against%0A%20%20Adversarial%20Attacks&entry.906535625=Zhenyu%20Liu%20and%20Haoran%20Duan%20and%20Huizhi%20Liang%20and%20Yang%20Long%20and%20Vaclav%20Snasel%20and%20Guiseppe%20Nicosia%20and%20Rajiv%20Ranjan%20and%20Varun%20Ojha&entry.1292438233=%20%20Adversarial%20training%20is%20one%20of%20the%20most%20effective%20methods%20for%20enhancing%20model%0Arobustness.%20Recent%20approaches%20incorporate%20adversarial%20distillation%20in%0Aadversarial%20training%20architectures.%20However%2C%20we%20notice%20two%20scenarios%20of%20defense%0Amethods%20that%20limit%20their%20performance%3A%20%281%29%20Previous%20methods%20primarily%20use%20static%0Aground%20truth%20for%20adversarial%20training%2C%20but%20this%20often%20causes%20robust%0Aoverfitting%3B%20%282%29%20The%20loss%20functions%20are%20either%20Mean%20Squared%20Error%20or%0AKL-divergence%20leading%20to%20a%20sub-optimal%20performance%20on%20clean%20accuracy.%20To%20solve%0Athose%20problems%2C%20we%20propose%20a%20dynamic%20label%20adversarial%20training%20%28DYNAT%29%0Aalgorithm%20that%20enables%20the%20target%20model%20to%20gradually%20and%20dynamically%20gain%0Arobustness%20from%20the%20guide%20model%27s%20decisions.%20Additionally%2C%20we%20found%20that%20a%0Abudgeted%20dimension%20of%20inner%20optimization%20for%20the%20target%20model%20may%20contribute%20to%0Athe%20trade-off%20between%20clean%20accuracy%20and%20robust%20accuracy.%20Therefore%2C%20we%20propose%0Aa%20novel%20inner%20optimization%20method%20to%20be%20incorporated%20into%20the%20adversarial%0Atraining.%20This%20will%20enable%20the%20target%20model%20to%20adaptively%20search%20for%0Aadversarial%20examples%20based%20on%20dynamic%20labels%20from%20the%20guiding%20model%2C%0Acontributing%20to%20the%20robustness%20of%20the%20target%20model.%20Extensive%20experiments%0Avalidate%20the%20superior%20performance%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13102v1&entry.124074799=Read"},
{"title": "Indoor scene recognition from images under visual corruptions", "author": "Willams de Lima Costa and Raul Ismayilov and Nicola Strisciuglio and Estefania Talavera Martinez", "abstract": "  The classification of indoor scenes is a critical component in various\napplications, such as intelligent robotics for assistive living. While deep\nlearning has significantly advanced this field, models often suffer from\nreduced performance due to image corruption. This paper presents an innovative\napproach to indoor scene recognition that leverages multimodal data fusion,\nintegrating caption-based semantic features with visual data to enhance both\naccuracy and robustness against corruption. We examine two multimodal networks\nthat synergize visual features from CNN models with semantic captions via a\nGraph Convolutional Network (GCN). Our study shows that this fusion markedly\nimproves model performance, with notable gains in Top-1 accuracy when evaluated\nagainst a corrupted subset of the Places365 dataset. Moreover, while standalone\nvisual models displayed high accuracy on uncorrupted images, their performance\ndeteriorated significantly with increased corruption severity. Conversely, the\nmultimodal models demonstrated improved accuracy in clean conditions and\nsubstantial robustness to a range of image corruptions. These results highlight\nthe efficacy of incorporating high-level contextual information through\ncaptions, suggesting a promising direction for enhancing the resilience of\nclassification systems.\n", "link": "http://arxiv.org/abs/2408.13029v1", "date": "2024-08-23", "relevancy": 2.0845, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5389}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5212}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Indoor%20scene%20recognition%20from%20images%20under%20visual%20corruptions&body=Title%3A%20Indoor%20scene%20recognition%20from%20images%20under%20visual%20corruptions%0AAuthor%3A%20Willams%20de%20Lima%20Costa%20and%20Raul%20Ismayilov%20and%20Nicola%20Strisciuglio%20and%20Estefania%20Talavera%20Martinez%0AAbstract%3A%20%20%20The%20classification%20of%20indoor%20scenes%20is%20a%20critical%20component%20in%20various%0Aapplications%2C%20such%20as%20intelligent%20robotics%20for%20assistive%20living.%20While%20deep%0Alearning%20has%20significantly%20advanced%20this%20field%2C%20models%20often%20suffer%20from%0Areduced%20performance%20due%20to%20image%20corruption.%20This%20paper%20presents%20an%20innovative%0Aapproach%20to%20indoor%20scene%20recognition%20that%20leverages%20multimodal%20data%20fusion%2C%0Aintegrating%20caption-based%20semantic%20features%20with%20visual%20data%20to%20enhance%20both%0Aaccuracy%20and%20robustness%20against%20corruption.%20We%20examine%20two%20multimodal%20networks%0Athat%20synergize%20visual%20features%20from%20CNN%20models%20with%20semantic%20captions%20via%20a%0AGraph%20Convolutional%20Network%20%28GCN%29.%20Our%20study%20shows%20that%20this%20fusion%20markedly%0Aimproves%20model%20performance%2C%20with%20notable%20gains%20in%20Top-1%20accuracy%20when%20evaluated%0Aagainst%20a%20corrupted%20subset%20of%20the%20Places365%20dataset.%20Moreover%2C%20while%20standalone%0Avisual%20models%20displayed%20high%20accuracy%20on%20uncorrupted%20images%2C%20their%20performance%0Adeteriorated%20significantly%20with%20increased%20corruption%20severity.%20Conversely%2C%20the%0Amultimodal%20models%20demonstrated%20improved%20accuracy%20in%20clean%20conditions%20and%0Asubstantial%20robustness%20to%20a%20range%20of%20image%20corruptions.%20These%20results%20highlight%0Athe%20efficacy%20of%20incorporating%20high-level%20contextual%20information%20through%0Acaptions%2C%20suggesting%20a%20promising%20direction%20for%20enhancing%20the%20resilience%20of%0Aclassification%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndoor%2520scene%2520recognition%2520from%2520images%2520under%2520visual%2520corruptions%26entry.906535625%3DWillams%2520de%2520Lima%2520Costa%2520and%2520Raul%2520Ismayilov%2520and%2520Nicola%2520Strisciuglio%2520and%2520Estefania%2520Talavera%2520Martinez%26entry.1292438233%3D%2520%2520The%2520classification%2520of%2520indoor%2520scenes%2520is%2520a%2520critical%2520component%2520in%2520various%250Aapplications%252C%2520such%2520as%2520intelligent%2520robotics%2520for%2520assistive%2520living.%2520While%2520deep%250Alearning%2520has%2520significantly%2520advanced%2520this%2520field%252C%2520models%2520often%2520suffer%2520from%250Areduced%2520performance%2520due%2520to%2520image%2520corruption.%2520This%2520paper%2520presents%2520an%2520innovative%250Aapproach%2520to%2520indoor%2520scene%2520recognition%2520that%2520leverages%2520multimodal%2520data%2520fusion%252C%250Aintegrating%2520caption-based%2520semantic%2520features%2520with%2520visual%2520data%2520to%2520enhance%2520both%250Aaccuracy%2520and%2520robustness%2520against%2520corruption.%2520We%2520examine%2520two%2520multimodal%2520networks%250Athat%2520synergize%2520visual%2520features%2520from%2520CNN%2520models%2520with%2520semantic%2520captions%2520via%2520a%250AGraph%2520Convolutional%2520Network%2520%2528GCN%2529.%2520Our%2520study%2520shows%2520that%2520this%2520fusion%2520markedly%250Aimproves%2520model%2520performance%252C%2520with%2520notable%2520gains%2520in%2520Top-1%2520accuracy%2520when%2520evaluated%250Aagainst%2520a%2520corrupted%2520subset%2520of%2520the%2520Places365%2520dataset.%2520Moreover%252C%2520while%2520standalone%250Avisual%2520models%2520displayed%2520high%2520accuracy%2520on%2520uncorrupted%2520images%252C%2520their%2520performance%250Adeteriorated%2520significantly%2520with%2520increased%2520corruption%2520severity.%2520Conversely%252C%2520the%250Amultimodal%2520models%2520demonstrated%2520improved%2520accuracy%2520in%2520clean%2520conditions%2520and%250Asubstantial%2520robustness%2520to%2520a%2520range%2520of%2520image%2520corruptions.%2520These%2520results%2520highlight%250Athe%2520efficacy%2520of%2520incorporating%2520high-level%2520contextual%2520information%2520through%250Acaptions%252C%2520suggesting%2520a%2520promising%2520direction%2520for%2520enhancing%2520the%2520resilience%2520of%250Aclassification%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Indoor%20scene%20recognition%20from%20images%20under%20visual%20corruptions&entry.906535625=Willams%20de%20Lima%20Costa%20and%20Raul%20Ismayilov%20and%20Nicola%20Strisciuglio%20and%20Estefania%20Talavera%20Martinez&entry.1292438233=%20%20The%20classification%20of%20indoor%20scenes%20is%20a%20critical%20component%20in%20various%0Aapplications%2C%20such%20as%20intelligent%20robotics%20for%20assistive%20living.%20While%20deep%0Alearning%20has%20significantly%20advanced%20this%20field%2C%20models%20often%20suffer%20from%0Areduced%20performance%20due%20to%20image%20corruption.%20This%20paper%20presents%20an%20innovative%0Aapproach%20to%20indoor%20scene%20recognition%20that%20leverages%20multimodal%20data%20fusion%2C%0Aintegrating%20caption-based%20semantic%20features%20with%20visual%20data%20to%20enhance%20both%0Aaccuracy%20and%20robustness%20against%20corruption.%20We%20examine%20two%20multimodal%20networks%0Athat%20synergize%20visual%20features%20from%20CNN%20models%20with%20semantic%20captions%20via%20a%0AGraph%20Convolutional%20Network%20%28GCN%29.%20Our%20study%20shows%20that%20this%20fusion%20markedly%0Aimproves%20model%20performance%2C%20with%20notable%20gains%20in%20Top-1%20accuracy%20when%20evaluated%0Aagainst%20a%20corrupted%20subset%20of%20the%20Places365%20dataset.%20Moreover%2C%20while%20standalone%0Avisual%20models%20displayed%20high%20accuracy%20on%20uncorrupted%20images%2C%20their%20performance%0Adeteriorated%20significantly%20with%20increased%20corruption%20severity.%20Conversely%2C%20the%0Amultimodal%20models%20demonstrated%20improved%20accuracy%20in%20clean%20conditions%20and%0Asubstantial%20robustness%20to%20a%20range%20of%20image%20corruptions.%20These%20results%20highlight%0Athe%20efficacy%20of%20incorporating%20high-level%20contextual%20information%20through%0Acaptions%2C%20suggesting%20a%20promising%20direction%20for%20enhancing%20the%20resilience%20of%0Aclassification%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13029v1&entry.124074799=Read"},
{"title": "Identifying Crucial Objects in Blind and Low-Vision Individuals'\n  Navigation", "author": "Md Touhidul Islam and Imran Kabir and Elena Ariel Pearce and Md Alimoor Reza and Syed Masum Billah", "abstract": "  This paper presents a curated list of 90 objects essential for the navigation\nof blind and low-vision (BLV) individuals, encompassing road, sidewalk, and\nindoor environments. We develop the initial list by analyzing 21 publicly\navailable videos featuring BLV individuals navigating various settings. Then,\nwe refine the list through feedback from a focus group study involving blind,\nlow-vision, and sighted companions of BLV individuals. A subsequent analysis\nreveals that most contemporary datasets used to train recent computer vision\nmodels contain only a small subset of the objects in our proposed list.\nFurthermore, we provide detailed object labeling for these 90 objects across 31\nvideo segments derived from the original 21 videos. Finally, we make the object\nlist, the 21 videos, and object labeling in the 31 video segments publicly\navailable. This paper aims to fill the existing gap and foster the development\nof more inclusive and effective navigation aids for the BLV community.\n", "link": "http://arxiv.org/abs/2408.13175v1", "date": "2024-08-23", "relevancy": 2.0827, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5782}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4847}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Crucial%20Objects%20in%20Blind%20and%20Low-Vision%20Individuals%27%0A%20%20Navigation&body=Title%3A%20Identifying%20Crucial%20Objects%20in%20Blind%20and%20Low-Vision%20Individuals%27%0A%20%20Navigation%0AAuthor%3A%20Md%20Touhidul%20Islam%20and%20Imran%20Kabir%20and%20Elena%20Ariel%20Pearce%20and%20Md%20Alimoor%20Reza%20and%20Syed%20Masum%20Billah%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20curated%20list%20of%2090%20objects%20essential%20for%20the%20navigation%0Aof%20blind%20and%20low-vision%20%28BLV%29%20individuals%2C%20encompassing%20road%2C%20sidewalk%2C%20and%0Aindoor%20environments.%20We%20develop%20the%20initial%20list%20by%20analyzing%2021%20publicly%0Aavailable%20videos%20featuring%20BLV%20individuals%20navigating%20various%20settings.%20Then%2C%0Awe%20refine%20the%20list%20through%20feedback%20from%20a%20focus%20group%20study%20involving%20blind%2C%0Alow-vision%2C%20and%20sighted%20companions%20of%20BLV%20individuals.%20A%20subsequent%20analysis%0Areveals%20that%20most%20contemporary%20datasets%20used%20to%20train%20recent%20computer%20vision%0Amodels%20contain%20only%20a%20small%20subset%20of%20the%20objects%20in%20our%20proposed%20list.%0AFurthermore%2C%20we%20provide%20detailed%20object%20labeling%20for%20these%2090%20objects%20across%2031%0Avideo%20segments%20derived%20from%20the%20original%2021%20videos.%20Finally%2C%20we%20make%20the%20object%0Alist%2C%20the%2021%20videos%2C%20and%20object%20labeling%20in%20the%2031%20video%20segments%20publicly%0Aavailable.%20This%20paper%20aims%20to%20fill%20the%20existing%20gap%20and%20foster%20the%20development%0Aof%20more%20inclusive%20and%20effective%20navigation%20aids%20for%20the%20BLV%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Crucial%2520Objects%2520in%2520Blind%2520and%2520Low-Vision%2520Individuals%2527%250A%2520%2520Navigation%26entry.906535625%3DMd%2520Touhidul%2520Islam%2520and%2520Imran%2520Kabir%2520and%2520Elena%2520Ariel%2520Pearce%2520and%2520Md%2520Alimoor%2520Reza%2520and%2520Syed%2520Masum%2520Billah%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520curated%2520list%2520of%252090%2520objects%2520essential%2520for%2520the%2520navigation%250Aof%2520blind%2520and%2520low-vision%2520%2528BLV%2529%2520individuals%252C%2520encompassing%2520road%252C%2520sidewalk%252C%2520and%250Aindoor%2520environments.%2520We%2520develop%2520the%2520initial%2520list%2520by%2520analyzing%252021%2520publicly%250Aavailable%2520videos%2520featuring%2520BLV%2520individuals%2520navigating%2520various%2520settings.%2520Then%252C%250Awe%2520refine%2520the%2520list%2520through%2520feedback%2520from%2520a%2520focus%2520group%2520study%2520involving%2520blind%252C%250Alow-vision%252C%2520and%2520sighted%2520companions%2520of%2520BLV%2520individuals.%2520A%2520subsequent%2520analysis%250Areveals%2520that%2520most%2520contemporary%2520datasets%2520used%2520to%2520train%2520recent%2520computer%2520vision%250Amodels%2520contain%2520only%2520a%2520small%2520subset%2520of%2520the%2520objects%2520in%2520our%2520proposed%2520list.%250AFurthermore%252C%2520we%2520provide%2520detailed%2520object%2520labeling%2520for%2520these%252090%2520objects%2520across%252031%250Avideo%2520segments%2520derived%2520from%2520the%2520original%252021%2520videos.%2520Finally%252C%2520we%2520make%2520the%2520object%250Alist%252C%2520the%252021%2520videos%252C%2520and%2520object%2520labeling%2520in%2520the%252031%2520video%2520segments%2520publicly%250Aavailable.%2520This%2520paper%2520aims%2520to%2520fill%2520the%2520existing%2520gap%2520and%2520foster%2520the%2520development%250Aof%2520more%2520inclusive%2520and%2520effective%2520navigation%2520aids%2520for%2520the%2520BLV%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Crucial%20Objects%20in%20Blind%20and%20Low-Vision%20Individuals%27%0A%20%20Navigation&entry.906535625=Md%20Touhidul%20Islam%20and%20Imran%20Kabir%20and%20Elena%20Ariel%20Pearce%20and%20Md%20Alimoor%20Reza%20and%20Syed%20Masum%20Billah&entry.1292438233=%20%20This%20paper%20presents%20a%20curated%20list%20of%2090%20objects%20essential%20for%20the%20navigation%0Aof%20blind%20and%20low-vision%20%28BLV%29%20individuals%2C%20encompassing%20road%2C%20sidewalk%2C%20and%0Aindoor%20environments.%20We%20develop%20the%20initial%20list%20by%20analyzing%2021%20publicly%0Aavailable%20videos%20featuring%20BLV%20individuals%20navigating%20various%20settings.%20Then%2C%0Awe%20refine%20the%20list%20through%20feedback%20from%20a%20focus%20group%20study%20involving%20blind%2C%0Alow-vision%2C%20and%20sighted%20companions%20of%20BLV%20individuals.%20A%20subsequent%20analysis%0Areveals%20that%20most%20contemporary%20datasets%20used%20to%20train%20recent%20computer%20vision%0Amodels%20contain%20only%20a%20small%20subset%20of%20the%20objects%20in%20our%20proposed%20list.%0AFurthermore%2C%20we%20provide%20detailed%20object%20labeling%20for%20these%2090%20objects%20across%2031%0Avideo%20segments%20derived%20from%20the%20original%2021%20videos.%20Finally%2C%20we%20make%20the%20object%0Alist%2C%20the%2021%20videos%2C%20and%20object%20labeling%20in%20the%2031%20video%20segments%20publicly%0Aavailable.%20This%20paper%20aims%20to%20fill%20the%20existing%20gap%20and%20foster%20the%20development%0Aof%20more%20inclusive%20and%20effective%20navigation%20aids%20for%20the%20BLV%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13175v1&entry.124074799=Read"},
{"title": "Spiking NeRF: Representing the Real-World Geometry by a Discontinuous\n  Representation", "author": "Zhanfeng Liao and Qian Zheng and Yan Liu and Gang Pan", "abstract": "  A crucial reason for the success of existing NeRF-based methods is to build a\nneural density field for the geometry representation via multiple perceptron\nlayers (MLPs). MLPs are continuous functions, however, real geometry or density\nfield is frequently discontinuous at the interface between the air and the\nsurface. Such a contrary brings the problem of unfaithful geometry\nrepresentation. To this end, this paper proposes spiking NeRF, which leverages\nspiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural\nNetwork (SNN) framework to build a discontinuous density field for faithful\ngeometry representation. Specifically, we first demonstrate the reason why\ncontinuous density fields will bring inaccuracy. Then, we propose to use the\nspiking neurons to build a discontinuous density field. We conduct a\ncomprehensive analysis for the problem of existing spiking neuron models and\nthen provide the numerical relationship between the parameter of the spiking\nneuron and the theoretical accuracy of geometry. Based on this, we propose a\nbounded spiking neuron to build the discontinuous density field. Our method\nachieves SOTA performance. The source code and the supplementary material are\navailable at https://github.com/liaozhanfeng/Spiking-NeRF.\n", "link": "http://arxiv.org/abs/2311.09077v3", "date": "2024-08-23", "relevancy": 2.0808, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5311}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20NeRF%3A%20Representing%20the%20Real-World%20Geometry%20by%20a%20Discontinuous%0A%20%20Representation&body=Title%3A%20Spiking%20NeRF%3A%20Representing%20the%20Real-World%20Geometry%20by%20a%20Discontinuous%0A%20%20Representation%0AAuthor%3A%20Zhanfeng%20Liao%20and%20Qian%20Zheng%20and%20Yan%20Liu%20and%20Gang%20Pan%0AAbstract%3A%20%20%20A%20crucial%20reason%20for%20the%20success%20of%20existing%20NeRF-based%20methods%20is%20to%20build%20a%0Aneural%20density%20field%20for%20the%20geometry%20representation%20via%20multiple%20perceptron%0Alayers%20%28MLPs%29.%20MLPs%20are%20continuous%20functions%2C%20however%2C%20real%20geometry%20or%20density%0Afield%20is%20frequently%20discontinuous%20at%20the%20interface%20between%20the%20air%20and%20the%0Asurface.%20Such%20a%20contrary%20brings%20the%20problem%20of%20unfaithful%20geometry%0Arepresentation.%20To%20this%20end%2C%20this%20paper%20proposes%20spiking%20NeRF%2C%20which%20leverages%0Aspiking%20neurons%20and%20a%20hybrid%20Artificial%20Neural%20Network%20%28ANN%29-Spiking%20Neural%0ANetwork%20%28SNN%29%20framework%20to%20build%20a%20discontinuous%20density%20field%20for%20faithful%0Ageometry%20representation.%20Specifically%2C%20we%20first%20demonstrate%20the%20reason%20why%0Acontinuous%20density%20fields%20will%20bring%20inaccuracy.%20Then%2C%20we%20propose%20to%20use%20the%0Aspiking%20neurons%20to%20build%20a%20discontinuous%20density%20field.%20We%20conduct%20a%0Acomprehensive%20analysis%20for%20the%20problem%20of%20existing%20spiking%20neuron%20models%20and%0Athen%20provide%20the%20numerical%20relationship%20between%20the%20parameter%20of%20the%20spiking%0Aneuron%20and%20the%20theoretical%20accuracy%20of%20geometry.%20Based%20on%20this%2C%20we%20propose%20a%0Abounded%20spiking%20neuron%20to%20build%20the%20discontinuous%20density%20field.%20Our%20method%0Aachieves%20SOTA%20performance.%20The%20source%20code%20and%20the%20supplementary%20material%20are%0Aavailable%20at%20https%3A//github.com/liaozhanfeng/Spiking-NeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09077v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520NeRF%253A%2520Representing%2520the%2520Real-World%2520Geometry%2520by%2520a%2520Discontinuous%250A%2520%2520Representation%26entry.906535625%3DZhanfeng%2520Liao%2520and%2520Qian%2520Zheng%2520and%2520Yan%2520Liu%2520and%2520Gang%2520Pan%26entry.1292438233%3D%2520%2520A%2520crucial%2520reason%2520for%2520the%2520success%2520of%2520existing%2520NeRF-based%2520methods%2520is%2520to%2520build%2520a%250Aneural%2520density%2520field%2520for%2520the%2520geometry%2520representation%2520via%2520multiple%2520perceptron%250Alayers%2520%2528MLPs%2529.%2520MLPs%2520are%2520continuous%2520functions%252C%2520however%252C%2520real%2520geometry%2520or%2520density%250Afield%2520is%2520frequently%2520discontinuous%2520at%2520the%2520interface%2520between%2520the%2520air%2520and%2520the%250Asurface.%2520Such%2520a%2520contrary%2520brings%2520the%2520problem%2520of%2520unfaithful%2520geometry%250Arepresentation.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520spiking%2520NeRF%252C%2520which%2520leverages%250Aspiking%2520neurons%2520and%2520a%2520hybrid%2520Artificial%2520Neural%2520Network%2520%2528ANN%2529-Spiking%2520Neural%250ANetwork%2520%2528SNN%2529%2520framework%2520to%2520build%2520a%2520discontinuous%2520density%2520field%2520for%2520faithful%250Ageometry%2520representation.%2520Specifically%252C%2520we%2520first%2520demonstrate%2520the%2520reason%2520why%250Acontinuous%2520density%2520fields%2520will%2520bring%2520inaccuracy.%2520Then%252C%2520we%2520propose%2520to%2520use%2520the%250Aspiking%2520neurons%2520to%2520build%2520a%2520discontinuous%2520density%2520field.%2520We%2520conduct%2520a%250Acomprehensive%2520analysis%2520for%2520the%2520problem%2520of%2520existing%2520spiking%2520neuron%2520models%2520and%250Athen%2520provide%2520the%2520numerical%2520relationship%2520between%2520the%2520parameter%2520of%2520the%2520spiking%250Aneuron%2520and%2520the%2520theoretical%2520accuracy%2520of%2520geometry.%2520Based%2520on%2520this%252C%2520we%2520propose%2520a%250Abounded%2520spiking%2520neuron%2520to%2520build%2520the%2520discontinuous%2520density%2520field.%2520Our%2520method%250Aachieves%2520SOTA%2520performance.%2520The%2520source%2520code%2520and%2520the%2520supplementary%2520material%2520are%250Aavailable%2520at%2520https%253A//github.com/liaozhanfeng/Spiking-NeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09077v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20NeRF%3A%20Representing%20the%20Real-World%20Geometry%20by%20a%20Discontinuous%0A%20%20Representation&entry.906535625=Zhanfeng%20Liao%20and%20Qian%20Zheng%20and%20Yan%20Liu%20and%20Gang%20Pan&entry.1292438233=%20%20A%20crucial%20reason%20for%20the%20success%20of%20existing%20NeRF-based%20methods%20is%20to%20build%20a%0Aneural%20density%20field%20for%20the%20geometry%20representation%20via%20multiple%20perceptron%0Alayers%20%28MLPs%29.%20MLPs%20are%20continuous%20functions%2C%20however%2C%20real%20geometry%20or%20density%0Afield%20is%20frequently%20discontinuous%20at%20the%20interface%20between%20the%20air%20and%20the%0Asurface.%20Such%20a%20contrary%20brings%20the%20problem%20of%20unfaithful%20geometry%0Arepresentation.%20To%20this%20end%2C%20this%20paper%20proposes%20spiking%20NeRF%2C%20which%20leverages%0Aspiking%20neurons%20and%20a%20hybrid%20Artificial%20Neural%20Network%20%28ANN%29-Spiking%20Neural%0ANetwork%20%28SNN%29%20framework%20to%20build%20a%20discontinuous%20density%20field%20for%20faithful%0Ageometry%20representation.%20Specifically%2C%20we%20first%20demonstrate%20the%20reason%20why%0Acontinuous%20density%20fields%20will%20bring%20inaccuracy.%20Then%2C%20we%20propose%20to%20use%20the%0Aspiking%20neurons%20to%20build%20a%20discontinuous%20density%20field.%20We%20conduct%20a%0Acomprehensive%20analysis%20for%20the%20problem%20of%20existing%20spiking%20neuron%20models%20and%0Athen%20provide%20the%20numerical%20relationship%20between%20the%20parameter%20of%20the%20spiking%0Aneuron%20and%20the%20theoretical%20accuracy%20of%20geometry.%20Based%20on%20this%2C%20we%20propose%20a%0Abounded%20spiking%20neuron%20to%20build%20the%20discontinuous%20density%20field.%20Our%20method%0Aachieves%20SOTA%20performance.%20The%20source%20code%20and%20the%20supplementary%20material%20are%0Aavailable%20at%20https%3A//github.com/liaozhanfeng/Spiking-NeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09077v3&entry.124074799=Read"},
{"title": "On Class Separability Pitfalls In Audio-Text Contrastive Zero-Shot\n  Learning", "author": "Tiago Tavares and Fabio Ayres and Zhepei Wang and Paris Smaragdis", "abstract": "  Recent advances in audio-text cross-modal contrastive learning have shown its\npotential towards zero-shot learning. One possibility for this is by projecting\nitem embeddings from pre-trained backbone neural networks into a cross-modal\nspace in which item similarity can be calculated in either domain. This process\nrelies on a strong unimodal pre-training of the backbone networks, and on a\ndata-intensive training task for the projectors. These two processes can be\nbiased by unintentional data leakage, which can arise from using supervised\nlearning in pre-training or from inadvertently training the cross-modal\nprojection using labels from the zero-shot learning evaluation. In this study,\nwe show that a significant part of the measured zero-shot learning accuracy is\ndue to strengths inherited from the audio and text backbones, that is, they are\nnot learned in the cross-modal domain and are not transferred from one modality\nto another.\n", "link": "http://arxiv.org/abs/2408.13068v1", "date": "2024-08-23", "relevancy": 2.0609, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5467}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4979}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Class%20Separability%20Pitfalls%20In%20Audio-Text%20Contrastive%20Zero-Shot%0A%20%20Learning&body=Title%3A%20On%20Class%20Separability%20Pitfalls%20In%20Audio-Text%20Contrastive%20Zero-Shot%0A%20%20Learning%0AAuthor%3A%20Tiago%20Tavares%20and%20Fabio%20Ayres%20and%20Zhepei%20Wang%20and%20Paris%20Smaragdis%0AAbstract%3A%20%20%20Recent%20advances%20in%20audio-text%20cross-modal%20contrastive%20learning%20have%20shown%20its%0Apotential%20towards%20zero-shot%20learning.%20One%20possibility%20for%20this%20is%20by%20projecting%0Aitem%20embeddings%20from%20pre-trained%20backbone%20neural%20networks%20into%20a%20cross-modal%0Aspace%20in%20which%20item%20similarity%20can%20be%20calculated%20in%20either%20domain.%20This%20process%0Arelies%20on%20a%20strong%20unimodal%20pre-training%20of%20the%20backbone%20networks%2C%20and%20on%20a%0Adata-intensive%20training%20task%20for%20the%20projectors.%20These%20two%20processes%20can%20be%0Abiased%20by%20unintentional%20data%20leakage%2C%20which%20can%20arise%20from%20using%20supervised%0Alearning%20in%20pre-training%20or%20from%20inadvertently%20training%20the%20cross-modal%0Aprojection%20using%20labels%20from%20the%20zero-shot%20learning%20evaluation.%20In%20this%20study%2C%0Awe%20show%20that%20a%20significant%20part%20of%20the%20measured%20zero-shot%20learning%20accuracy%20is%0Adue%20to%20strengths%20inherited%20from%20the%20audio%20and%20text%20backbones%2C%20that%20is%2C%20they%20are%0Anot%20learned%20in%20the%20cross-modal%20domain%20and%20are%20not%20transferred%20from%20one%20modality%0Ato%20another.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Class%2520Separability%2520Pitfalls%2520In%2520Audio-Text%2520Contrastive%2520Zero-Shot%250A%2520%2520Learning%26entry.906535625%3DTiago%2520Tavares%2520and%2520Fabio%2520Ayres%2520and%2520Zhepei%2520Wang%2520and%2520Paris%2520Smaragdis%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520audio-text%2520cross-modal%2520contrastive%2520learning%2520have%2520shown%2520its%250Apotential%2520towards%2520zero-shot%2520learning.%2520One%2520possibility%2520for%2520this%2520is%2520by%2520projecting%250Aitem%2520embeddings%2520from%2520pre-trained%2520backbone%2520neural%2520networks%2520into%2520a%2520cross-modal%250Aspace%2520in%2520which%2520item%2520similarity%2520can%2520be%2520calculated%2520in%2520either%2520domain.%2520This%2520process%250Arelies%2520on%2520a%2520strong%2520unimodal%2520pre-training%2520of%2520the%2520backbone%2520networks%252C%2520and%2520on%2520a%250Adata-intensive%2520training%2520task%2520for%2520the%2520projectors.%2520These%2520two%2520processes%2520can%2520be%250Abiased%2520by%2520unintentional%2520data%2520leakage%252C%2520which%2520can%2520arise%2520from%2520using%2520supervised%250Alearning%2520in%2520pre-training%2520or%2520from%2520inadvertently%2520training%2520the%2520cross-modal%250Aprojection%2520using%2520labels%2520from%2520the%2520zero-shot%2520learning%2520evaluation.%2520In%2520this%2520study%252C%250Awe%2520show%2520that%2520a%2520significant%2520part%2520of%2520the%2520measured%2520zero-shot%2520learning%2520accuracy%2520is%250Adue%2520to%2520strengths%2520inherited%2520from%2520the%2520audio%2520and%2520text%2520backbones%252C%2520that%2520is%252C%2520they%2520are%250Anot%2520learned%2520in%2520the%2520cross-modal%2520domain%2520and%2520are%2520not%2520transferred%2520from%2520one%2520modality%250Ato%2520another.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Class%20Separability%20Pitfalls%20In%20Audio-Text%20Contrastive%20Zero-Shot%0A%20%20Learning&entry.906535625=Tiago%20Tavares%20and%20Fabio%20Ayres%20and%20Zhepei%20Wang%20and%20Paris%20Smaragdis&entry.1292438233=%20%20Recent%20advances%20in%20audio-text%20cross-modal%20contrastive%20learning%20have%20shown%20its%0Apotential%20towards%20zero-shot%20learning.%20One%20possibility%20for%20this%20is%20by%20projecting%0Aitem%20embeddings%20from%20pre-trained%20backbone%20neural%20networks%20into%20a%20cross-modal%0Aspace%20in%20which%20item%20similarity%20can%20be%20calculated%20in%20either%20domain.%20This%20process%0Arelies%20on%20a%20strong%20unimodal%20pre-training%20of%20the%20backbone%20networks%2C%20and%20on%20a%0Adata-intensive%20training%20task%20for%20the%20projectors.%20These%20two%20processes%20can%20be%0Abiased%20by%20unintentional%20data%20leakage%2C%20which%20can%20arise%20from%20using%20supervised%0Alearning%20in%20pre-training%20or%20from%20inadvertently%20training%20the%20cross-modal%0Aprojection%20using%20labels%20from%20the%20zero-shot%20learning%20evaluation.%20In%20this%20study%2C%0Awe%20show%20that%20a%20significant%20part%20of%20the%20measured%20zero-shot%20learning%20accuracy%20is%0Adue%20to%20strengths%20inherited%20from%20the%20audio%20and%20text%20backbones%2C%20that%20is%2C%20they%20are%0Anot%20learned%20in%20the%20cross-modal%20domain%20and%20are%20not%20transferred%20from%20one%20modality%0Ato%20another.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13068v1&entry.124074799=Read"},
{"title": "D&M: Enriching E-commerce Videos with Sound Effects by Key Moment\n  Detection and SFX Matching", "author": "Jingyu Liu and Minquan Wang and Ye Ma and Bo Wang and Aozhu Chen and Quan Chen and Peng Jiang and Xirong Li", "abstract": "  Videos showcasing specific products are increasingly important for\nE-commerce. Key moments naturally exist as the first appearance of a specific\nproduct, presentation of its distinctive features, the presence of a buying\nlink, etc. Adding proper sound effects (SFX) to these key moments, or video\ndecoration with SFX (VDSFX), is crucial for enhancing the user engaging\nexperience. Previous studies about adding SFX to videos perform video to SFX\nmatching at a holistic level, lacking the ability of adding SFX to a specific\nmoment. Meanwhile, previous studies on video highlight detection or video\nmoment retrieval consider only moment localization, leaving moment to SFX\nmatching untouched. By contrast, we propose in this paper D&M, a unified method\nthat accomplishes key moment detection and moment to SFX matching\nsimultaneously. Moreover, for the new VDSFX task we build a large-scale dataset\nSFX-Moment from an E-commerce platform. For a fair comparison, we build\ncompetitive baselines by extending a number of current video moment detection\nmethods to the new task. Extensive experiments on SFX-Moment show the superior\nperformance of the proposed method over the baselines. Code and data will be\nreleased.\n", "link": "http://arxiv.org/abs/2408.13226v1", "date": "2024-08-23", "relevancy": 2.0562, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5181}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5149}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D%26M%3A%20Enriching%20E-commerce%20Videos%20with%20Sound%20Effects%20by%20Key%20Moment%0A%20%20Detection%20and%20SFX%20Matching&body=Title%3A%20D%26M%3A%20Enriching%20E-commerce%20Videos%20with%20Sound%20Effects%20by%20Key%20Moment%0A%20%20Detection%20and%20SFX%20Matching%0AAuthor%3A%20Jingyu%20Liu%20and%20Minquan%20Wang%20and%20Ye%20Ma%20and%20Bo%20Wang%20and%20Aozhu%20Chen%20and%20Quan%20Chen%20and%20Peng%20Jiang%20and%20Xirong%20Li%0AAbstract%3A%20%20%20Videos%20showcasing%20specific%20products%20are%20increasingly%20important%20for%0AE-commerce.%20Key%20moments%20naturally%20exist%20as%20the%20first%20appearance%20of%20a%20specific%0Aproduct%2C%20presentation%20of%20its%20distinctive%20features%2C%20the%20presence%20of%20a%20buying%0Alink%2C%20etc.%20Adding%20proper%20sound%20effects%20%28SFX%29%20to%20these%20key%20moments%2C%20or%20video%0Adecoration%20with%20SFX%20%28VDSFX%29%2C%20is%20crucial%20for%20enhancing%20the%20user%20engaging%0Aexperience.%20Previous%20studies%20about%20adding%20SFX%20to%20videos%20perform%20video%20to%20SFX%0Amatching%20at%20a%20holistic%20level%2C%20lacking%20the%20ability%20of%20adding%20SFX%20to%20a%20specific%0Amoment.%20Meanwhile%2C%20previous%20studies%20on%20video%20highlight%20detection%20or%20video%0Amoment%20retrieval%20consider%20only%20moment%20localization%2C%20leaving%20moment%20to%20SFX%0Amatching%20untouched.%20By%20contrast%2C%20we%20propose%20in%20this%20paper%20D%26M%2C%20a%20unified%20method%0Athat%20accomplishes%20key%20moment%20detection%20and%20moment%20to%20SFX%20matching%0Asimultaneously.%20Moreover%2C%20for%20the%20new%20VDSFX%20task%20we%20build%20a%20large-scale%20dataset%0ASFX-Moment%20from%20an%20E-commerce%20platform.%20For%20a%20fair%20comparison%2C%20we%20build%0Acompetitive%20baselines%20by%20extending%20a%20number%20of%20current%20video%20moment%20detection%0Amethods%20to%20the%20new%20task.%20Extensive%20experiments%20on%20SFX-Moment%20show%20the%20superior%0Aperformance%20of%20the%20proposed%20method%20over%20the%20baselines.%20Code%20and%20data%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD%2526M%253A%2520Enriching%2520E-commerce%2520Videos%2520with%2520Sound%2520Effects%2520by%2520Key%2520Moment%250A%2520%2520Detection%2520and%2520SFX%2520Matching%26entry.906535625%3DJingyu%2520Liu%2520and%2520Minquan%2520Wang%2520and%2520Ye%2520Ma%2520and%2520Bo%2520Wang%2520and%2520Aozhu%2520Chen%2520and%2520Quan%2520Chen%2520and%2520Peng%2520Jiang%2520and%2520Xirong%2520Li%26entry.1292438233%3D%2520%2520Videos%2520showcasing%2520specific%2520products%2520are%2520increasingly%2520important%2520for%250AE-commerce.%2520Key%2520moments%2520naturally%2520exist%2520as%2520the%2520first%2520appearance%2520of%2520a%2520specific%250Aproduct%252C%2520presentation%2520of%2520its%2520distinctive%2520features%252C%2520the%2520presence%2520of%2520a%2520buying%250Alink%252C%2520etc.%2520Adding%2520proper%2520sound%2520effects%2520%2528SFX%2529%2520to%2520these%2520key%2520moments%252C%2520or%2520video%250Adecoration%2520with%2520SFX%2520%2528VDSFX%2529%252C%2520is%2520crucial%2520for%2520enhancing%2520the%2520user%2520engaging%250Aexperience.%2520Previous%2520studies%2520about%2520adding%2520SFX%2520to%2520videos%2520perform%2520video%2520to%2520SFX%250Amatching%2520at%2520a%2520holistic%2520level%252C%2520lacking%2520the%2520ability%2520of%2520adding%2520SFX%2520to%2520a%2520specific%250Amoment.%2520Meanwhile%252C%2520previous%2520studies%2520on%2520video%2520highlight%2520detection%2520or%2520video%250Amoment%2520retrieval%2520consider%2520only%2520moment%2520localization%252C%2520leaving%2520moment%2520to%2520SFX%250Amatching%2520untouched.%2520By%2520contrast%252C%2520we%2520propose%2520in%2520this%2520paper%2520D%2526M%252C%2520a%2520unified%2520method%250Athat%2520accomplishes%2520key%2520moment%2520detection%2520and%2520moment%2520to%2520SFX%2520matching%250Asimultaneously.%2520Moreover%252C%2520for%2520the%2520new%2520VDSFX%2520task%2520we%2520build%2520a%2520large-scale%2520dataset%250ASFX-Moment%2520from%2520an%2520E-commerce%2520platform.%2520For%2520a%2520fair%2520comparison%252C%2520we%2520build%250Acompetitive%2520baselines%2520by%2520extending%2520a%2520number%2520of%2520current%2520video%2520moment%2520detection%250Amethods%2520to%2520the%2520new%2520task.%2520Extensive%2520experiments%2520on%2520SFX-Moment%2520show%2520the%2520superior%250Aperformance%2520of%2520the%2520proposed%2520method%2520over%2520the%2520baselines.%2520Code%2520and%2520data%2520will%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D%26M%3A%20Enriching%20E-commerce%20Videos%20with%20Sound%20Effects%20by%20Key%20Moment%0A%20%20Detection%20and%20SFX%20Matching&entry.906535625=Jingyu%20Liu%20and%20Minquan%20Wang%20and%20Ye%20Ma%20and%20Bo%20Wang%20and%20Aozhu%20Chen%20and%20Quan%20Chen%20and%20Peng%20Jiang%20and%20Xirong%20Li&entry.1292438233=%20%20Videos%20showcasing%20specific%20products%20are%20increasingly%20important%20for%0AE-commerce.%20Key%20moments%20naturally%20exist%20as%20the%20first%20appearance%20of%20a%20specific%0Aproduct%2C%20presentation%20of%20its%20distinctive%20features%2C%20the%20presence%20of%20a%20buying%0Alink%2C%20etc.%20Adding%20proper%20sound%20effects%20%28SFX%29%20to%20these%20key%20moments%2C%20or%20video%0Adecoration%20with%20SFX%20%28VDSFX%29%2C%20is%20crucial%20for%20enhancing%20the%20user%20engaging%0Aexperience.%20Previous%20studies%20about%20adding%20SFX%20to%20videos%20perform%20video%20to%20SFX%0Amatching%20at%20a%20holistic%20level%2C%20lacking%20the%20ability%20of%20adding%20SFX%20to%20a%20specific%0Amoment.%20Meanwhile%2C%20previous%20studies%20on%20video%20highlight%20detection%20or%20video%0Amoment%20retrieval%20consider%20only%20moment%20localization%2C%20leaving%20moment%20to%20SFX%0Amatching%20untouched.%20By%20contrast%2C%20we%20propose%20in%20this%20paper%20D%26M%2C%20a%20unified%20method%0Athat%20accomplishes%20key%20moment%20detection%20and%20moment%20to%20SFX%20matching%0Asimultaneously.%20Moreover%2C%20for%20the%20new%20VDSFX%20task%20we%20build%20a%20large-scale%20dataset%0ASFX-Moment%20from%20an%20E-commerce%20platform.%20For%20a%20fair%20comparison%2C%20we%20build%0Acompetitive%20baselines%20by%20extending%20a%20number%20of%20current%20video%20moment%20detection%0Amethods%20to%20the%20new%20task.%20Extensive%20experiments%20on%20SFX-Moment%20show%20the%20superior%0Aperformance%20of%20the%20proposed%20method%20over%20the%20baselines.%20Code%20and%20data%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13226v1&entry.124074799=Read"},
{"title": "Verification of Geometric Robustness of Neural Networks via Piecewise\n  Linear Approximation and Lipschitz Optimisation", "author": "Ben Batten and Yang Zheng and Alessandro De Palma and Panagiotis Kouvaros and Alessio Lomuscio", "abstract": "  We address the problem of verifying neural networks against geometric\ntransformations of the input image, including rotation, scaling, shearing, and\ntranslation. The proposed method computes provably sound piecewise linear\nconstraints for the pixel values by using sampling and linear approximations in\ncombination with branch-and-bound Lipschitz optimisation. A feature of the\nmethod is that it obtains tighter over-approximations of the perturbation\nregion than the present state-of-the-art. We report results from experiments on\na comprehensive set of benchmarks. We show that our proposed implementation\nresolves more verification cases than present approaches while being more\ncomputationally efficient.\n", "link": "http://arxiv.org/abs/2408.13140v1", "date": "2024-08-23", "relevancy": 2.0495, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5343}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verification%20of%20Geometric%20Robustness%20of%20Neural%20Networks%20via%20Piecewise%0A%20%20Linear%20Approximation%20and%20Lipschitz%20Optimisation&body=Title%3A%20Verification%20of%20Geometric%20Robustness%20of%20Neural%20Networks%20via%20Piecewise%0A%20%20Linear%20Approximation%20and%20Lipschitz%20Optimisation%0AAuthor%3A%20Ben%20Batten%20and%20Yang%20Zheng%20and%20Alessandro%20De%20Palma%20and%20Panagiotis%20Kouvaros%20and%20Alessio%20Lomuscio%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20verifying%20neural%20networks%20against%20geometric%0Atransformations%20of%20the%20input%20image%2C%20including%20rotation%2C%20scaling%2C%20shearing%2C%20and%0Atranslation.%20The%20proposed%20method%20computes%20provably%20sound%20piecewise%20linear%0Aconstraints%20for%20the%20pixel%20values%20by%20using%20sampling%20and%20linear%20approximations%20in%0Acombination%20with%20branch-and-bound%20Lipschitz%20optimisation.%20A%20feature%20of%20the%0Amethod%20is%20that%20it%20obtains%20tighter%20over-approximations%20of%20the%20perturbation%0Aregion%20than%20the%20present%20state-of-the-art.%20We%20report%20results%20from%20experiments%20on%0Aa%20comprehensive%20set%20of%20benchmarks.%20We%20show%20that%20our%20proposed%20implementation%0Aresolves%20more%20verification%20cases%20than%20present%20approaches%20while%20being%20more%0Acomputationally%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerification%2520of%2520Geometric%2520Robustness%2520of%2520Neural%2520Networks%2520via%2520Piecewise%250A%2520%2520Linear%2520Approximation%2520and%2520Lipschitz%2520Optimisation%26entry.906535625%3DBen%2520Batten%2520and%2520Yang%2520Zheng%2520and%2520Alessandro%2520De%2520Palma%2520and%2520Panagiotis%2520Kouvaros%2520and%2520Alessio%2520Lomuscio%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520verifying%2520neural%2520networks%2520against%2520geometric%250Atransformations%2520of%2520the%2520input%2520image%252C%2520including%2520rotation%252C%2520scaling%252C%2520shearing%252C%2520and%250Atranslation.%2520The%2520proposed%2520method%2520computes%2520provably%2520sound%2520piecewise%2520linear%250Aconstraints%2520for%2520the%2520pixel%2520values%2520by%2520using%2520sampling%2520and%2520linear%2520approximations%2520in%250Acombination%2520with%2520branch-and-bound%2520Lipschitz%2520optimisation.%2520A%2520feature%2520of%2520the%250Amethod%2520is%2520that%2520it%2520obtains%2520tighter%2520over-approximations%2520of%2520the%2520perturbation%250Aregion%2520than%2520the%2520present%2520state-of-the-art.%2520We%2520report%2520results%2520from%2520experiments%2520on%250Aa%2520comprehensive%2520set%2520of%2520benchmarks.%2520We%2520show%2520that%2520our%2520proposed%2520implementation%250Aresolves%2520more%2520verification%2520cases%2520than%2520present%2520approaches%2520while%2520being%2520more%250Acomputationally%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verification%20of%20Geometric%20Robustness%20of%20Neural%20Networks%20via%20Piecewise%0A%20%20Linear%20Approximation%20and%20Lipschitz%20Optimisation&entry.906535625=Ben%20Batten%20and%20Yang%20Zheng%20and%20Alessandro%20De%20Palma%20and%20Panagiotis%20Kouvaros%20and%20Alessio%20Lomuscio&entry.1292438233=%20%20We%20address%20the%20problem%20of%20verifying%20neural%20networks%20against%20geometric%0Atransformations%20of%20the%20input%20image%2C%20including%20rotation%2C%20scaling%2C%20shearing%2C%20and%0Atranslation.%20The%20proposed%20method%20computes%20provably%20sound%20piecewise%20linear%0Aconstraints%20for%20the%20pixel%20values%20by%20using%20sampling%20and%20linear%20approximations%20in%0Acombination%20with%20branch-and-bound%20Lipschitz%20optimisation.%20A%20feature%20of%20the%0Amethod%20is%20that%20it%20obtains%20tighter%20over-approximations%20of%20the%20perturbation%0Aregion%20than%20the%20present%20state-of-the-art.%20We%20report%20results%20from%20experiments%20on%0Aa%20comprehensive%20set%20of%20benchmarks.%20We%20show%20that%20our%20proposed%20implementation%0Aresolves%20more%20verification%20cases%20than%20present%20approaches%20while%20being%20more%0Acomputationally%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13140v1&entry.124074799=Read"},
{"title": "Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification", "author": "Muhammad Ahmad and Muhammad Hassaan Farooq Butt and Muhammad Usama and Hamad Ahmed Altuwaijri and Manual Mazzara and Salvatore Distenano", "abstract": "  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures\nlong-range dependencies, addressing Transformer limitations. However,\ntraditional Mamba models overlook rich spectral information in HSIs and\nstruggle with high dimensionality and sequential data. To address these issues,\nwe propose the SSM with multi-head self-attention and token enhancement\n(MHSSMamba). This model integrates spectral and spatial information by\nenhancing spectral tokens and using multi-head attention to capture complex\nrelationships between spectral bands and spatial locations. It also manages\nlong-range dependencies and the sequential nature of HSI data, preserving\ncontextual information across spectral bands. MHSSMamba achieved remarkable\nclassification accuracies of 97.62\\% on Pavia University, 96.92\\% on the\nUniversity of Houston, 96.85\\% on Salinas, and 99.49\\% on Wuhan-longKou\ndatasets. The source code is available at\n\\href{https://github.com/MHassaanButt/MHA\\_SS\\_Mamba}{GitHub}.\n", "link": "http://arxiv.org/abs/2408.01224v2", "date": "2024-08-23", "relevancy": 2.04, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Manual%20Mazzara%20and%20Salvatore%20Distenano%0AAbstract%3A%20%20%20Spatial-Spectral%20Mamba%20%28SSM%29%20improves%20computational%20efficiency%20and%20captures%0Along-range%20dependencies%2C%20addressing%20Transformer%20limitations.%20However%2C%0Atraditional%20Mamba%20models%20overlook%20rich%20spectral%20information%20in%20HSIs%20and%0Astruggle%20with%20high%20dimensionality%20and%20sequential%20data.%20To%20address%20these%20issues%2C%0Awe%20propose%20the%20SSM%20with%20multi-head%20self-attention%20and%20token%20enhancement%0A%28MHSSMamba%29.%20This%20model%20integrates%20spectral%20and%20spatial%20information%20by%0Aenhancing%20spectral%20tokens%20and%20using%20multi-head%20attention%20to%20capture%20complex%0Arelationships%20between%20spectral%20bands%20and%20spatial%20locations.%20It%20also%20manages%0Along-range%20dependencies%20and%20the%20sequential%20nature%20of%20HSI%20data%2C%20preserving%0Acontextual%20information%20across%20spectral%20bands.%20MHSSMamba%20achieved%20remarkable%0Aclassification%20accuracies%20of%2097.62%5C%25%20on%20Pavia%20University%2C%2096.92%5C%25%20on%20the%0AUniversity%20of%20Houston%2C%2096.85%5C%25%20on%20Salinas%2C%20and%2099.49%5C%25%20on%20Wuhan-longKou%0Adatasets.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/MHassaanButt/MHA%5C_SS%5C_Mamba%7D%7BGitHub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-head%2520Spatial-Spectral%2520Mamba%2520for%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DMuhammad%2520Ahmad%2520and%2520Muhammad%2520Hassaan%2520Farooq%2520Butt%2520and%2520Muhammad%2520Usama%2520and%2520Hamad%2520Ahmed%2520Altuwaijri%2520and%2520Manual%2520Mazzara%2520and%2520Salvatore%2520Distenano%26entry.1292438233%3D%2520%2520Spatial-Spectral%2520Mamba%2520%2528SSM%2529%2520improves%2520computational%2520efficiency%2520and%2520captures%250Along-range%2520dependencies%252C%2520addressing%2520Transformer%2520limitations.%2520However%252C%250Atraditional%2520Mamba%2520models%2520overlook%2520rich%2520spectral%2520information%2520in%2520HSIs%2520and%250Astruggle%2520with%2520high%2520dimensionality%2520and%2520sequential%2520data.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520the%2520SSM%2520with%2520multi-head%2520self-attention%2520and%2520token%2520enhancement%250A%2528MHSSMamba%2529.%2520This%2520model%2520integrates%2520spectral%2520and%2520spatial%2520information%2520by%250Aenhancing%2520spectral%2520tokens%2520and%2520using%2520multi-head%2520attention%2520to%2520capture%2520complex%250Arelationships%2520between%2520spectral%2520bands%2520and%2520spatial%2520locations.%2520It%2520also%2520manages%250Along-range%2520dependencies%2520and%2520the%2520sequential%2520nature%2520of%2520HSI%2520data%252C%2520preserving%250Acontextual%2520information%2520across%2520spectral%2520bands.%2520MHSSMamba%2520achieved%2520remarkable%250Aclassification%2520accuracies%2520of%252097.62%255C%2525%2520on%2520Pavia%2520University%252C%252096.92%255C%2525%2520on%2520the%250AUniversity%2520of%2520Houston%252C%252096.85%255C%2525%2520on%2520Salinas%252C%2520and%252099.49%255C%2525%2520on%2520Wuhan-longKou%250Adatasets.%2520The%2520source%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/MHassaanButt/MHA%255C_SS%255C_Mamba%257D%257BGitHub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Manual%20Mazzara%20and%20Salvatore%20Distenano&entry.1292438233=%20%20Spatial-Spectral%20Mamba%20%28SSM%29%20improves%20computational%20efficiency%20and%20captures%0Along-range%20dependencies%2C%20addressing%20Transformer%20limitations.%20However%2C%0Atraditional%20Mamba%20models%20overlook%20rich%20spectral%20information%20in%20HSIs%20and%0Astruggle%20with%20high%20dimensionality%20and%20sequential%20data.%20To%20address%20these%20issues%2C%0Awe%20propose%20the%20SSM%20with%20multi-head%20self-attention%20and%20token%20enhancement%0A%28MHSSMamba%29.%20This%20model%20integrates%20spectral%20and%20spatial%20information%20by%0Aenhancing%20spectral%20tokens%20and%20using%20multi-head%20attention%20to%20capture%20complex%0Arelationships%20between%20spectral%20bands%20and%20spatial%20locations.%20It%20also%20manages%0Along-range%20dependencies%20and%20the%20sequential%20nature%20of%20HSI%20data%2C%20preserving%0Acontextual%20information%20across%20spectral%20bands.%20MHSSMamba%20achieved%20remarkable%0Aclassification%20accuracies%20of%2097.62%5C%25%20on%20Pavia%20University%2C%2096.92%5C%25%20on%20the%0AUniversity%20of%20Houston%2C%2096.85%5C%25%20on%20Salinas%2C%20and%2099.49%5C%25%20on%20Wuhan-longKou%0Adatasets.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/MHassaanButt/MHA%5C_SS%5C_Mamba%7D%7BGitHub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01224v2&entry.124074799=Read"},
{"title": "Spatial-Spectral Morphological Mamba for Hyperspectral Image\n  Classification", "author": "Muhammad Ahmad and Muhammad Hassaan Farooq Butt and Muhammad Usama and Adil Mehmood Khan and Manuel Mazzara and Salvatore Distefano and Hamad Ahmed Altuwaijri and Swalpa Kumar Roy and Jocelyn Chanussot and Danfeng Hong", "abstract": "  In recent years, the emergence of Transformers with self-attention mechanism\nhas revolutionized the hyperspectral image (HSI) classification. However, these\nmodels face major challenges in computational efficiency, as their complexity\nincreases quadratically with the sequence length. The Mamba architecture,\nleveraging a state space model (SSM), offers a more efficient alternative to\nTransformers. This paper introduces the Spatial-Spectral Morphological Mamba\n(MorpMamba) model in which, a token generation module first converts the HSI\npatch into spatial-spectral tokens. These tokens are then processed by\nmorphological operations, which compute structural and shape information using\ndepthwise separable convolutional operations. The extracted information is\nenhanced in a feature enhancement module that adjusts the spatial and spectral\ntokens based on the center region of the HSI sample, allowing for effective\ninformation fusion within each block. Subsequently, the tokens are refined\nthrough a multi-head self-attention which further improves the feature space.\nFinally, the combined information is fed into the state space block for\nclassification and the creation of the ground truth map. Experiments on widely\nused HSI datasets demonstrate that the MorpMamba model outperforms (parametric\nefficiency) both CNN and Transformer models. The source code will be made\npublicly available at \\url{https://github.com/MHassaanButt/MorpMamba}.\n", "link": "http://arxiv.org/abs/2408.01372v2", "date": "2024-08-23", "relevancy": 2.0392, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5073}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Spectral%20Morphological%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification&body=Title%3A%20Spatial-Spectral%20Morphological%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Adil%20Mehmood%20Khan%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distefano%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Swalpa%20Kumar%20Roy%20and%20Jocelyn%20Chanussot%20and%20Danfeng%20Hong%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20emergence%20of%20Transformers%20with%20self-attention%20mechanism%0Ahas%20revolutionized%20the%20hyperspectral%20image%20%28HSI%29%20classification.%20However%2C%20these%0Amodels%20face%20major%20challenges%20in%20computational%20efficiency%2C%20as%20their%20complexity%0Aincreases%20quadratically%20with%20the%20sequence%20length.%20The%20Mamba%20architecture%2C%0Aleveraging%20a%20state%20space%20model%20%28SSM%29%2C%20offers%20a%20more%20efficient%20alternative%20to%0ATransformers.%20This%20paper%20introduces%20the%20Spatial-Spectral%20Morphological%20Mamba%0A%28MorpMamba%29%20model%20in%20which%2C%20a%20token%20generation%20module%20first%20converts%20the%20HSI%0Apatch%20into%20spatial-spectral%20tokens.%20These%20tokens%20are%20then%20processed%20by%0Amorphological%20operations%2C%20which%20compute%20structural%20and%20shape%20information%20using%0Adepthwise%20separable%20convolutional%20operations.%20The%20extracted%20information%20is%0Aenhanced%20in%20a%20feature%20enhancement%20module%20that%20adjusts%20the%20spatial%20and%20spectral%0Atokens%20based%20on%20the%20center%20region%20of%20the%20HSI%20sample%2C%20allowing%20for%20effective%0Ainformation%20fusion%20within%20each%20block.%20Subsequently%2C%20the%20tokens%20are%20refined%0Athrough%20a%20multi-head%20self-attention%20which%20further%20improves%20the%20feature%20space.%0AFinally%2C%20the%20combined%20information%20is%20fed%20into%20the%20state%20space%20block%20for%0Aclassification%20and%20the%20creation%20of%20the%20ground%20truth%20map.%20Experiments%20on%20widely%0Aused%20HSI%20datasets%20demonstrate%20that%20the%20MorpMamba%20model%20outperforms%20%28parametric%0Aefficiency%29%20both%20CNN%20and%20Transformer%20models.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/MHassaanButt/MorpMamba%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Spectral%2520Morphological%2520Mamba%2520for%2520Hyperspectral%2520Image%250A%2520%2520Classification%26entry.906535625%3DMuhammad%2520Ahmad%2520and%2520Muhammad%2520Hassaan%2520Farooq%2520Butt%2520and%2520Muhammad%2520Usama%2520and%2520Adil%2520Mehmood%2520Khan%2520and%2520Manuel%2520Mazzara%2520and%2520Salvatore%2520Distefano%2520and%2520Hamad%2520Ahmed%2520Altuwaijri%2520and%2520Swalpa%2520Kumar%2520Roy%2520and%2520Jocelyn%2520Chanussot%2520and%2520Danfeng%2520Hong%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520emergence%2520of%2520Transformers%2520with%2520self-attention%2520mechanism%250Ahas%2520revolutionized%2520the%2520hyperspectral%2520image%2520%2528HSI%2529%2520classification.%2520However%252C%2520these%250Amodels%2520face%2520major%2520challenges%2520in%2520computational%2520efficiency%252C%2520as%2520their%2520complexity%250Aincreases%2520quadratically%2520with%2520the%2520sequence%2520length.%2520The%2520Mamba%2520architecture%252C%250Aleveraging%2520a%2520state%2520space%2520model%2520%2528SSM%2529%252C%2520offers%2520a%2520more%2520efficient%2520alternative%2520to%250ATransformers.%2520This%2520paper%2520introduces%2520the%2520Spatial-Spectral%2520Morphological%2520Mamba%250A%2528MorpMamba%2529%2520model%2520in%2520which%252C%2520a%2520token%2520generation%2520module%2520first%2520converts%2520the%2520HSI%250Apatch%2520into%2520spatial-spectral%2520tokens.%2520These%2520tokens%2520are%2520then%2520processed%2520by%250Amorphological%2520operations%252C%2520which%2520compute%2520structural%2520and%2520shape%2520information%2520using%250Adepthwise%2520separable%2520convolutional%2520operations.%2520The%2520extracted%2520information%2520is%250Aenhanced%2520in%2520a%2520feature%2520enhancement%2520module%2520that%2520adjusts%2520the%2520spatial%2520and%2520spectral%250Atokens%2520based%2520on%2520the%2520center%2520region%2520of%2520the%2520HSI%2520sample%252C%2520allowing%2520for%2520effective%250Ainformation%2520fusion%2520within%2520each%2520block.%2520Subsequently%252C%2520the%2520tokens%2520are%2520refined%250Athrough%2520a%2520multi-head%2520self-attention%2520which%2520further%2520improves%2520the%2520feature%2520space.%250AFinally%252C%2520the%2520combined%2520information%2520is%2520fed%2520into%2520the%2520state%2520space%2520block%2520for%250Aclassification%2520and%2520the%2520creation%2520of%2520the%2520ground%2520truth%2520map.%2520Experiments%2520on%2520widely%250Aused%2520HSI%2520datasets%2520demonstrate%2520that%2520the%2520MorpMamba%2520model%2520outperforms%2520%2528parametric%250Aefficiency%2529%2520both%2520CNN%2520and%2520Transformer%2520models.%2520The%2520source%2520code%2520will%2520be%2520made%250Apublicly%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/MHassaanButt/MorpMamba%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Spectral%20Morphological%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Adil%20Mehmood%20Khan%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distefano%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Swalpa%20Kumar%20Roy%20and%20Jocelyn%20Chanussot%20and%20Danfeng%20Hong&entry.1292438233=%20%20In%20recent%20years%2C%20the%20emergence%20of%20Transformers%20with%20self-attention%20mechanism%0Ahas%20revolutionized%20the%20hyperspectral%20image%20%28HSI%29%20classification.%20However%2C%20these%0Amodels%20face%20major%20challenges%20in%20computational%20efficiency%2C%20as%20their%20complexity%0Aincreases%20quadratically%20with%20the%20sequence%20length.%20The%20Mamba%20architecture%2C%0Aleveraging%20a%20state%20space%20model%20%28SSM%29%2C%20offers%20a%20more%20efficient%20alternative%20to%0ATransformers.%20This%20paper%20introduces%20the%20Spatial-Spectral%20Morphological%20Mamba%0A%28MorpMamba%29%20model%20in%20which%2C%20a%20token%20generation%20module%20first%20converts%20the%20HSI%0Apatch%20into%20spatial-spectral%20tokens.%20These%20tokens%20are%20then%20processed%20by%0Amorphological%20operations%2C%20which%20compute%20structural%20and%20shape%20information%20using%0Adepthwise%20separable%20convolutional%20operations.%20The%20extracted%20information%20is%0Aenhanced%20in%20a%20feature%20enhancement%20module%20that%20adjusts%20the%20spatial%20and%20spectral%0Atokens%20based%20on%20the%20center%20region%20of%20the%20HSI%20sample%2C%20allowing%20for%20effective%0Ainformation%20fusion%20within%20each%20block.%20Subsequently%2C%20the%20tokens%20are%20refined%0Athrough%20a%20multi-head%20self-attention%20which%20further%20improves%20the%20feature%20space.%0AFinally%2C%20the%20combined%20information%20is%20fed%20into%20the%20state%20space%20block%20for%0Aclassification%20and%20the%20creation%20of%20the%20ground%20truth%20map.%20Experiments%20on%20widely%0Aused%20HSI%20datasets%20demonstrate%20that%20the%20MorpMamba%20model%20outperforms%20%28parametric%0Aefficiency%29%20both%20CNN%20and%20Transformer%20models.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/MHassaanButt/MorpMamba%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01372v2&entry.124074799=Read"},
{"title": "Identification and validation of the dynamic model of a tendon-driven\n  anthropomorphic finger", "author": "Junnan Li and Lingyun Chen and Johannes Ringwald and Edmundo Pozo Fortunic and Amartya Ganguly and Sami Haddadin", "abstract": "  This study addresses the absence of an identification framework to quantify a\ncomprehensive dynamic model of human and anthropomorphic tendon-driven fingers,\nwhich is necessary to investigate the physiological properties of human fingers\nand improve the control of robotic hands. First, a generalized dynamic model\nwas formulated, which takes into account the inherent properties of such a\nmechanical system. This includes rigid-body dynamics, coupling matrix, joint\nviscoelasticity, and tendon friction. Then, we propose a methodology comprising\na series of experiments, for step-wise identification and validation of this\ndynamic model. Moreover, an experimental setup was designed and constructed\nthat features actuation modules and peripheral sensors to facilitate the\nidentification process. To verify the proposed methodology, a 3D-printed\nrobotic finger based on the index finger design of the Dexmart hand was\ndeveloped, and the proposed experiments were executed to identify and validate\nits dynamic model. This study could be extended to explore the identification\nof cadaver hands, aiming for a consistent dataset from a single cadaver\nspecimen to improve the development of musculoskeletal hand models.\n", "link": "http://arxiv.org/abs/2408.13044v1", "date": "2024-08-23", "relevancy": 2.0351, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5136}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5097}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identification%20and%20validation%20of%20the%20dynamic%20model%20of%20a%20tendon-driven%0A%20%20anthropomorphic%20finger&body=Title%3A%20Identification%20and%20validation%20of%20the%20dynamic%20model%20of%20a%20tendon-driven%0A%20%20anthropomorphic%20finger%0AAuthor%3A%20Junnan%20Li%20and%20Lingyun%20Chen%20and%20Johannes%20Ringwald%20and%20Edmundo%20Pozo%20Fortunic%20and%20Amartya%20Ganguly%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20absence%20of%20an%20identification%20framework%20to%20quantify%20a%0Acomprehensive%20dynamic%20model%20of%20human%20and%20anthropomorphic%20tendon-driven%20fingers%2C%0Awhich%20is%20necessary%20to%20investigate%20the%20physiological%20properties%20of%20human%20fingers%0Aand%20improve%20the%20control%20of%20robotic%20hands.%20First%2C%20a%20generalized%20dynamic%20model%0Awas%20formulated%2C%20which%20takes%20into%20account%20the%20inherent%20properties%20of%20such%20a%0Amechanical%20system.%20This%20includes%20rigid-body%20dynamics%2C%20coupling%20matrix%2C%20joint%0Aviscoelasticity%2C%20and%20tendon%20friction.%20Then%2C%20we%20propose%20a%20methodology%20comprising%0Aa%20series%20of%20experiments%2C%20for%20step-wise%20identification%20and%20validation%20of%20this%0Adynamic%20model.%20Moreover%2C%20an%20experimental%20setup%20was%20designed%20and%20constructed%0Athat%20features%20actuation%20modules%20and%20peripheral%20sensors%20to%20facilitate%20the%0Aidentification%20process.%20To%20verify%20the%20proposed%20methodology%2C%20a%203D-printed%0Arobotic%20finger%20based%20on%20the%20index%20finger%20design%20of%20the%20Dexmart%20hand%20was%0Adeveloped%2C%20and%20the%20proposed%20experiments%20were%20executed%20to%20identify%20and%20validate%0Aits%20dynamic%20model.%20This%20study%20could%20be%20extended%20to%20explore%20the%20identification%0Aof%20cadaver%20hands%2C%20aiming%20for%20a%20consistent%20dataset%20from%20a%20single%20cadaver%0Aspecimen%20to%20improve%20the%20development%20of%20musculoskeletal%20hand%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentification%2520and%2520validation%2520of%2520the%2520dynamic%2520model%2520of%2520a%2520tendon-driven%250A%2520%2520anthropomorphic%2520finger%26entry.906535625%3DJunnan%2520Li%2520and%2520Lingyun%2520Chen%2520and%2520Johannes%2520Ringwald%2520and%2520Edmundo%2520Pozo%2520Fortunic%2520and%2520Amartya%2520Ganguly%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520absence%2520of%2520an%2520identification%2520framework%2520to%2520quantify%2520a%250Acomprehensive%2520dynamic%2520model%2520of%2520human%2520and%2520anthropomorphic%2520tendon-driven%2520fingers%252C%250Awhich%2520is%2520necessary%2520to%2520investigate%2520the%2520physiological%2520properties%2520of%2520human%2520fingers%250Aand%2520improve%2520the%2520control%2520of%2520robotic%2520hands.%2520First%252C%2520a%2520generalized%2520dynamic%2520model%250Awas%2520formulated%252C%2520which%2520takes%2520into%2520account%2520the%2520inherent%2520properties%2520of%2520such%2520a%250Amechanical%2520system.%2520This%2520includes%2520rigid-body%2520dynamics%252C%2520coupling%2520matrix%252C%2520joint%250Aviscoelasticity%252C%2520and%2520tendon%2520friction.%2520Then%252C%2520we%2520propose%2520a%2520methodology%2520comprising%250Aa%2520series%2520of%2520experiments%252C%2520for%2520step-wise%2520identification%2520and%2520validation%2520of%2520this%250Adynamic%2520model.%2520Moreover%252C%2520an%2520experimental%2520setup%2520was%2520designed%2520and%2520constructed%250Athat%2520features%2520actuation%2520modules%2520and%2520peripheral%2520sensors%2520to%2520facilitate%2520the%250Aidentification%2520process.%2520To%2520verify%2520the%2520proposed%2520methodology%252C%2520a%25203D-printed%250Arobotic%2520finger%2520based%2520on%2520the%2520index%2520finger%2520design%2520of%2520the%2520Dexmart%2520hand%2520was%250Adeveloped%252C%2520and%2520the%2520proposed%2520experiments%2520were%2520executed%2520to%2520identify%2520and%2520validate%250Aits%2520dynamic%2520model.%2520This%2520study%2520could%2520be%2520extended%2520to%2520explore%2520the%2520identification%250Aof%2520cadaver%2520hands%252C%2520aiming%2520for%2520a%2520consistent%2520dataset%2520from%2520a%2520single%2520cadaver%250Aspecimen%2520to%2520improve%2520the%2520development%2520of%2520musculoskeletal%2520hand%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identification%20and%20validation%20of%20the%20dynamic%20model%20of%20a%20tendon-driven%0A%20%20anthropomorphic%20finger&entry.906535625=Junnan%20Li%20and%20Lingyun%20Chen%20and%20Johannes%20Ringwald%20and%20Edmundo%20Pozo%20Fortunic%20and%20Amartya%20Ganguly%20and%20Sami%20Haddadin&entry.1292438233=%20%20This%20study%20addresses%20the%20absence%20of%20an%20identification%20framework%20to%20quantify%20a%0Acomprehensive%20dynamic%20model%20of%20human%20and%20anthropomorphic%20tendon-driven%20fingers%2C%0Awhich%20is%20necessary%20to%20investigate%20the%20physiological%20properties%20of%20human%20fingers%0Aand%20improve%20the%20control%20of%20robotic%20hands.%20First%2C%20a%20generalized%20dynamic%20model%0Awas%20formulated%2C%20which%20takes%20into%20account%20the%20inherent%20properties%20of%20such%20a%0Amechanical%20system.%20This%20includes%20rigid-body%20dynamics%2C%20coupling%20matrix%2C%20joint%0Aviscoelasticity%2C%20and%20tendon%20friction.%20Then%2C%20we%20propose%20a%20methodology%20comprising%0Aa%20series%20of%20experiments%2C%20for%20step-wise%20identification%20and%20validation%20of%20this%0Adynamic%20model.%20Moreover%2C%20an%20experimental%20setup%20was%20designed%20and%20constructed%0Athat%20features%20actuation%20modules%20and%20peripheral%20sensors%20to%20facilitate%20the%0Aidentification%20process.%20To%20verify%20the%20proposed%20methodology%2C%20a%203D-printed%0Arobotic%20finger%20based%20on%20the%20index%20finger%20design%20of%20the%20Dexmart%20hand%20was%0Adeveloped%2C%20and%20the%20proposed%20experiments%20were%20executed%20to%20identify%20and%20validate%0Aits%20dynamic%20model.%20This%20study%20could%20be%20extended%20to%20explore%20the%20identification%0Aof%20cadaver%20hands%2C%20aiming%20for%20a%20consistent%20dataset%20from%20a%20single%20cadaver%0Aspecimen%20to%20improve%20the%20development%20of%20musculoskeletal%20hand%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13044v1&entry.124074799=Read"},
{"title": "Low-light phase retrieval with implicit generative priors", "author": "Raunak Manekar and Elisa Negrini and Minh Pham and Daniel Jacobs and Jaideep Srivastava and Stanley J. Osher and Jianwei Miao", "abstract": "  Phase retrieval (PR) is fundamentally important in scientific imaging and is\ncrucial for nanoscale techniques like coherent diffractive imaging (CDI). Low\nradiation dose imaging is essential for applications involving\nradiation-sensitive samples. However, most PR methods struggle in low-dose\nscenarios due to high shot noise. Recent advancements in optical data\nacquisition setups, such as in-situ CDI, have shown promise for low-dose\nimaging, but they rely on a time series of measurements, making them unsuitable\nfor single-image applications. Similarly, data-driven phase retrieval\ntechniques are not easily adaptable to data-scarce situations. Zero-shot deep\nlearning methods based on pre-trained and implicit generative priors have been\neffective in various imaging tasks but have shown limited success in PR. In\nthis work, we propose low-dose deep image prior (LoDIP), which combines in-situ\nCDI with the power of implicit generative priors to address single-image\nlow-dose phase retrieval. Quantitative evaluations demonstrate LoDIP's superior\nperformance in this task and its applicability to real experimental scenarios.\n", "link": "http://arxiv.org/abs/2402.17745v2", "date": "2024-08-23", "relevancy": 2.0325, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5303}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-light%20phase%20retrieval%20with%20implicit%20generative%20priors&body=Title%3A%20Low-light%20phase%20retrieval%20with%20implicit%20generative%20priors%0AAuthor%3A%20Raunak%20Manekar%20and%20Elisa%20Negrini%20and%20Minh%20Pham%20and%20Daniel%20Jacobs%20and%20Jaideep%20Srivastava%20and%20Stanley%20J.%20Osher%20and%20Jianwei%20Miao%0AAbstract%3A%20%20%20Phase%20retrieval%20%28PR%29%20is%20fundamentally%20important%20in%20scientific%20imaging%20and%20is%0Acrucial%20for%20nanoscale%20techniques%20like%20coherent%20diffractive%20imaging%20%28CDI%29.%20Low%0Aradiation%20dose%20imaging%20is%20essential%20for%20applications%20involving%0Aradiation-sensitive%20samples.%20However%2C%20most%20PR%20methods%20struggle%20in%20low-dose%0Ascenarios%20due%20to%20high%20shot%20noise.%20Recent%20advancements%20in%20optical%20data%0Aacquisition%20setups%2C%20such%20as%20in-situ%20CDI%2C%20have%20shown%20promise%20for%20low-dose%0Aimaging%2C%20but%20they%20rely%20on%20a%20time%20series%20of%20measurements%2C%20making%20them%20unsuitable%0Afor%20single-image%20applications.%20Similarly%2C%20data-driven%20phase%20retrieval%0Atechniques%20are%20not%20easily%20adaptable%20to%20data-scarce%20situations.%20Zero-shot%20deep%0Alearning%20methods%20based%20on%20pre-trained%20and%20implicit%20generative%20priors%20have%20been%0Aeffective%20in%20various%20imaging%20tasks%20but%20have%20shown%20limited%20success%20in%20PR.%20In%0Athis%20work%2C%20we%20propose%20low-dose%20deep%20image%20prior%20%28LoDIP%29%2C%20which%20combines%20in-situ%0ACDI%20with%20the%20power%20of%20implicit%20generative%20priors%20to%20address%20single-image%0Alow-dose%20phase%20retrieval.%20Quantitative%20evaluations%20demonstrate%20LoDIP%27s%20superior%0Aperformance%20in%20this%20task%20and%20its%20applicability%20to%20real%20experimental%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-light%2520phase%2520retrieval%2520with%2520implicit%2520generative%2520priors%26entry.906535625%3DRaunak%2520Manekar%2520and%2520Elisa%2520Negrini%2520and%2520Minh%2520Pham%2520and%2520Daniel%2520Jacobs%2520and%2520Jaideep%2520Srivastava%2520and%2520Stanley%2520J.%2520Osher%2520and%2520Jianwei%2520Miao%26entry.1292438233%3D%2520%2520Phase%2520retrieval%2520%2528PR%2529%2520is%2520fundamentally%2520important%2520in%2520scientific%2520imaging%2520and%2520is%250Acrucial%2520for%2520nanoscale%2520techniques%2520like%2520coherent%2520diffractive%2520imaging%2520%2528CDI%2529.%2520Low%250Aradiation%2520dose%2520imaging%2520is%2520essential%2520for%2520applications%2520involving%250Aradiation-sensitive%2520samples.%2520However%252C%2520most%2520PR%2520methods%2520struggle%2520in%2520low-dose%250Ascenarios%2520due%2520to%2520high%2520shot%2520noise.%2520Recent%2520advancements%2520in%2520optical%2520data%250Aacquisition%2520setups%252C%2520such%2520as%2520in-situ%2520CDI%252C%2520have%2520shown%2520promise%2520for%2520low-dose%250Aimaging%252C%2520but%2520they%2520rely%2520on%2520a%2520time%2520series%2520of%2520measurements%252C%2520making%2520them%2520unsuitable%250Afor%2520single-image%2520applications.%2520Similarly%252C%2520data-driven%2520phase%2520retrieval%250Atechniques%2520are%2520not%2520easily%2520adaptable%2520to%2520data-scarce%2520situations.%2520Zero-shot%2520deep%250Alearning%2520methods%2520based%2520on%2520pre-trained%2520and%2520implicit%2520generative%2520priors%2520have%2520been%250Aeffective%2520in%2520various%2520imaging%2520tasks%2520but%2520have%2520shown%2520limited%2520success%2520in%2520PR.%2520In%250Athis%2520work%252C%2520we%2520propose%2520low-dose%2520deep%2520image%2520prior%2520%2528LoDIP%2529%252C%2520which%2520combines%2520in-situ%250ACDI%2520with%2520the%2520power%2520of%2520implicit%2520generative%2520priors%2520to%2520address%2520single-image%250Alow-dose%2520phase%2520retrieval.%2520Quantitative%2520evaluations%2520demonstrate%2520LoDIP%2527s%2520superior%250Aperformance%2520in%2520this%2520task%2520and%2520its%2520applicability%2520to%2520real%2520experimental%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-light%20phase%20retrieval%20with%20implicit%20generative%20priors&entry.906535625=Raunak%20Manekar%20and%20Elisa%20Negrini%20and%20Minh%20Pham%20and%20Daniel%20Jacobs%20and%20Jaideep%20Srivastava%20and%20Stanley%20J.%20Osher%20and%20Jianwei%20Miao&entry.1292438233=%20%20Phase%20retrieval%20%28PR%29%20is%20fundamentally%20important%20in%20scientific%20imaging%20and%20is%0Acrucial%20for%20nanoscale%20techniques%20like%20coherent%20diffractive%20imaging%20%28CDI%29.%20Low%0Aradiation%20dose%20imaging%20is%20essential%20for%20applications%20involving%0Aradiation-sensitive%20samples.%20However%2C%20most%20PR%20methods%20struggle%20in%20low-dose%0Ascenarios%20due%20to%20high%20shot%20noise.%20Recent%20advancements%20in%20optical%20data%0Aacquisition%20setups%2C%20such%20as%20in-situ%20CDI%2C%20have%20shown%20promise%20for%20low-dose%0Aimaging%2C%20but%20they%20rely%20on%20a%20time%20series%20of%20measurements%2C%20making%20them%20unsuitable%0Afor%20single-image%20applications.%20Similarly%2C%20data-driven%20phase%20retrieval%0Atechniques%20are%20not%20easily%20adaptable%20to%20data-scarce%20situations.%20Zero-shot%20deep%0Alearning%20methods%20based%20on%20pre-trained%20and%20implicit%20generative%20priors%20have%20been%0Aeffective%20in%20various%20imaging%20tasks%20but%20have%20shown%20limited%20success%20in%20PR.%20In%0Athis%20work%2C%20we%20propose%20low-dose%20deep%20image%20prior%20%28LoDIP%29%2C%20which%20combines%20in-situ%0ACDI%20with%20the%20power%20of%20implicit%20generative%20priors%20to%20address%20single-image%0Alow-dose%20phase%20retrieval.%20Quantitative%20evaluations%20demonstrate%20LoDIP%27s%20superior%0Aperformance%20in%20this%20task%20and%20its%20applicability%20to%20real%20experimental%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17745v2&entry.124074799=Read"},
{"title": "Focused Discriminative Training For Streaming CTC-Trained Automatic\n  Speech Recognition Models", "author": "Adnan Haider and Xingyu Na and Erik McDermott and Tim Ng and Zhen Huang and Xiaodan Zhuang", "abstract": "  This paper introduces a novel training framework called Focused\nDiscriminative Training (FDT) to further improve streaming word-piece\nend-to-end (E2E) automatic speech recognition (ASR) models trained using either\nCTC or an interpolation of CTC and attention-based encoder-decoder (AED) loss.\nThe proposed approach presents a novel framework to identify and improve a\nmodel's recognition on challenging segments of an audio. Notably, this training\nframework is independent of hidden Markov models (HMMs) and lattices,\neliminating the need for substantial decision-making regarding HMM topology,\nlexicon, and graph generation, as typically required in standard discriminative\ntraining approaches. Compared to additional fine-tuning with MMI or MWER loss\non the encoder, FDT is shown to be more effective in achieving greater\nreductions in Word Error Rate (WER) on streaming models trained on LibriSpeech.\nAdditionally, this method is shown to be effective in further improving a\nconverged word-piece streaming E2E model trained on 600k hours of assistant and\ndictation dataset.\n", "link": "http://arxiv.org/abs/2408.13008v1", "date": "2024-08-23", "relevancy": 2.0223, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5283}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5031}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focused%20Discriminative%20Training%20For%20Streaming%20CTC-Trained%20Automatic%0A%20%20Speech%20Recognition%20Models&body=Title%3A%20Focused%20Discriminative%20Training%20For%20Streaming%20CTC-Trained%20Automatic%0A%20%20Speech%20Recognition%20Models%0AAuthor%3A%20Adnan%20Haider%20and%20Xingyu%20Na%20and%20Erik%20McDermott%20and%20Tim%20Ng%20and%20Zhen%20Huang%20and%20Xiaodan%20Zhuang%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20training%20framework%20called%20Focused%0ADiscriminative%20Training%20%28FDT%29%20to%20further%20improve%20streaming%20word-piece%0Aend-to-end%20%28E2E%29%20automatic%20speech%20recognition%20%28ASR%29%20models%20trained%20using%20either%0ACTC%20or%20an%20interpolation%20of%20CTC%20and%20attention-based%20encoder-decoder%20%28AED%29%20loss.%0AThe%20proposed%20approach%20presents%20a%20novel%20framework%20to%20identify%20and%20improve%20a%0Amodel%27s%20recognition%20on%20challenging%20segments%20of%20an%20audio.%20Notably%2C%20this%20training%0Aframework%20is%20independent%20of%20hidden%20Markov%20models%20%28HMMs%29%20and%20lattices%2C%0Aeliminating%20the%20need%20for%20substantial%20decision-making%20regarding%20HMM%20topology%2C%0Alexicon%2C%20and%20graph%20generation%2C%20as%20typically%20required%20in%20standard%20discriminative%0Atraining%20approaches.%20Compared%20to%20additional%20fine-tuning%20with%20MMI%20or%20MWER%20loss%0Aon%20the%20encoder%2C%20FDT%20is%20shown%20to%20be%20more%20effective%20in%20achieving%20greater%0Areductions%20in%20Word%20Error%20Rate%20%28WER%29%20on%20streaming%20models%20trained%20on%20LibriSpeech.%0AAdditionally%2C%20this%20method%20is%20shown%20to%20be%20effective%20in%20further%20improving%20a%0Aconverged%20word-piece%20streaming%20E2E%20model%20trained%20on%20600k%20hours%20of%20assistant%20and%0Adictation%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocused%2520Discriminative%2520Training%2520For%2520Streaming%2520CTC-Trained%2520Automatic%250A%2520%2520Speech%2520Recognition%2520Models%26entry.906535625%3DAdnan%2520Haider%2520and%2520Xingyu%2520Na%2520and%2520Erik%2520McDermott%2520and%2520Tim%2520Ng%2520and%2520Zhen%2520Huang%2520and%2520Xiaodan%2520Zhuang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520training%2520framework%2520called%2520Focused%250ADiscriminative%2520Training%2520%2528FDT%2529%2520to%2520further%2520improve%2520streaming%2520word-piece%250Aend-to-end%2520%2528E2E%2529%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520models%2520trained%2520using%2520either%250ACTC%2520or%2520an%2520interpolation%2520of%2520CTC%2520and%2520attention-based%2520encoder-decoder%2520%2528AED%2529%2520loss.%250AThe%2520proposed%2520approach%2520presents%2520a%2520novel%2520framework%2520to%2520identify%2520and%2520improve%2520a%250Amodel%2527s%2520recognition%2520on%2520challenging%2520segments%2520of%2520an%2520audio.%2520Notably%252C%2520this%2520training%250Aframework%2520is%2520independent%2520of%2520hidden%2520Markov%2520models%2520%2528HMMs%2529%2520and%2520lattices%252C%250Aeliminating%2520the%2520need%2520for%2520substantial%2520decision-making%2520regarding%2520HMM%2520topology%252C%250Alexicon%252C%2520and%2520graph%2520generation%252C%2520as%2520typically%2520required%2520in%2520standard%2520discriminative%250Atraining%2520approaches.%2520Compared%2520to%2520additional%2520fine-tuning%2520with%2520MMI%2520or%2520MWER%2520loss%250Aon%2520the%2520encoder%252C%2520FDT%2520is%2520shown%2520to%2520be%2520more%2520effective%2520in%2520achieving%2520greater%250Areductions%2520in%2520Word%2520Error%2520Rate%2520%2528WER%2529%2520on%2520streaming%2520models%2520trained%2520on%2520LibriSpeech.%250AAdditionally%252C%2520this%2520method%2520is%2520shown%2520to%2520be%2520effective%2520in%2520further%2520improving%2520a%250Aconverged%2520word-piece%2520streaming%2520E2E%2520model%2520trained%2520on%2520600k%2520hours%2520of%2520assistant%2520and%250Adictation%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focused%20Discriminative%20Training%20For%20Streaming%20CTC-Trained%20Automatic%0A%20%20Speech%20Recognition%20Models&entry.906535625=Adnan%20Haider%20and%20Xingyu%20Na%20and%20Erik%20McDermott%20and%20Tim%20Ng%20and%20Zhen%20Huang%20and%20Xiaodan%20Zhuang&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20training%20framework%20called%20Focused%0ADiscriminative%20Training%20%28FDT%29%20to%20further%20improve%20streaming%20word-piece%0Aend-to-end%20%28E2E%29%20automatic%20speech%20recognition%20%28ASR%29%20models%20trained%20using%20either%0ACTC%20or%20an%20interpolation%20of%20CTC%20and%20attention-based%20encoder-decoder%20%28AED%29%20loss.%0AThe%20proposed%20approach%20presents%20a%20novel%20framework%20to%20identify%20and%20improve%20a%0Amodel%27s%20recognition%20on%20challenging%20segments%20of%20an%20audio.%20Notably%2C%20this%20training%0Aframework%20is%20independent%20of%20hidden%20Markov%20models%20%28HMMs%29%20and%20lattices%2C%0Aeliminating%20the%20need%20for%20substantial%20decision-making%20regarding%20HMM%20topology%2C%0Alexicon%2C%20and%20graph%20generation%2C%20as%20typically%20required%20in%20standard%20discriminative%0Atraining%20approaches.%20Compared%20to%20additional%20fine-tuning%20with%20MMI%20or%20MWER%20loss%0Aon%20the%20encoder%2C%20FDT%20is%20shown%20to%20be%20more%20effective%20in%20achieving%20greater%0Areductions%20in%20Word%20Error%20Rate%20%28WER%29%20on%20streaming%20models%20trained%20on%20LibriSpeech.%0AAdditionally%2C%20this%20method%20is%20shown%20to%20be%20effective%20in%20further%20improving%20a%0Aconverged%20word-piece%20streaming%20E2E%20model%20trained%20on%20600k%20hours%20of%20assistant%20and%0Adictation%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13008v1&entry.124074799=Read"},
{"title": "Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central\n  Planning Approach", "author": "Johan Peralez and Aur\u00e9lien Delage and Jacopo Castellini and Rafael F. Cunha and Jilles S. Dibangoye", "abstract": "  Centralized training for decentralized execution paradigm emerged as the\nstate-of-the-art approach to epsilon-optimally solving decentralized partially\nobservable Markov decision processes. However, scalability remains a\nsignificant issue. This paper presents a novel and more scalable alternative,\nnamely sequential-move centralized training for decentralized execution. This\nparadigm further pushes the applicability of Bellman's principle of optimality,\nraising three new properties. First, it allows a central planner to reason upon\nsufficient sequential-move statistics instead of prior simultaneous-move ones.\nNext, it proves that epsilon-optimal value functions are piecewise linear and\nconvex in sufficient sequential-move statistics. Finally, it drops the\ncomplexity of the backup operators from double exponential to polynomial at the\nexpense of longer planning horizons. Besides, it makes it easy to use\nsingle-agent methods, e.g., SARSA algorithm enhanced with these findings\napplies while still preserving convergence guarantees. Experiments on two- as\nwell as many-agent domains from the literature against epsilon-optimal\nsimultaneous-move solvers confirm the superiority of the novel approach. This\nparadigm opens the door for efficient planning and reinforcement learning\nmethods for multi-agent systems.\n", "link": "http://arxiv.org/abs/2408.13139v1", "date": "2024-08-23", "relevancy": 2.0191, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5256}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimally%20Solving%20Simultaneous-Move%20Dec-POMDPs%3A%20The%20Sequential%20Central%0A%20%20Planning%20Approach&body=Title%3A%20Optimally%20Solving%20Simultaneous-Move%20Dec-POMDPs%3A%20The%20Sequential%20Central%0A%20%20Planning%20Approach%0AAuthor%3A%20Johan%20Peralez%20and%20Aur%C3%A9lien%20Delage%20and%20Jacopo%20Castellini%20and%20Rafael%20F.%20Cunha%20and%20Jilles%20S.%20Dibangoye%0AAbstract%3A%20%20%20Centralized%20training%20for%20decentralized%20execution%20paradigm%20emerged%20as%20the%0Astate-of-the-art%20approach%20to%20epsilon-optimally%20solving%20decentralized%20partially%0Aobservable%20Markov%20decision%20processes.%20However%2C%20scalability%20remains%20a%0Asignificant%20issue.%20This%20paper%20presents%20a%20novel%20and%20more%20scalable%20alternative%2C%0Anamely%20sequential-move%20centralized%20training%20for%20decentralized%20execution.%20This%0Aparadigm%20further%20pushes%20the%20applicability%20of%20Bellman%27s%20principle%20of%20optimality%2C%0Araising%20three%20new%20properties.%20First%2C%20it%20allows%20a%20central%20planner%20to%20reason%20upon%0Asufficient%20sequential-move%20statistics%20instead%20of%20prior%20simultaneous-move%20ones.%0ANext%2C%20it%20proves%20that%20epsilon-optimal%20value%20functions%20are%20piecewise%20linear%20and%0Aconvex%20in%20sufficient%20sequential-move%20statistics.%20Finally%2C%20it%20drops%20the%0Acomplexity%20of%20the%20backup%20operators%20from%20double%20exponential%20to%20polynomial%20at%20the%0Aexpense%20of%20longer%20planning%20horizons.%20Besides%2C%20it%20makes%20it%20easy%20to%20use%0Asingle-agent%20methods%2C%20e.g.%2C%20SARSA%20algorithm%20enhanced%20with%20these%20findings%0Aapplies%20while%20still%20preserving%20convergence%20guarantees.%20Experiments%20on%20two-%20as%0Awell%20as%20many-agent%20domains%20from%20the%20literature%20against%20epsilon-optimal%0Asimultaneous-move%20solvers%20confirm%20the%20superiority%20of%20the%20novel%20approach.%20This%0Aparadigm%20opens%20the%20door%20for%20efficient%20planning%20and%20reinforcement%20learning%0Amethods%20for%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimally%2520Solving%2520Simultaneous-Move%2520Dec-POMDPs%253A%2520The%2520Sequential%2520Central%250A%2520%2520Planning%2520Approach%26entry.906535625%3DJohan%2520Peralez%2520and%2520Aur%25C3%25A9lien%2520Delage%2520and%2520Jacopo%2520Castellini%2520and%2520Rafael%2520F.%2520Cunha%2520and%2520Jilles%2520S.%2520Dibangoye%26entry.1292438233%3D%2520%2520Centralized%2520training%2520for%2520decentralized%2520execution%2520paradigm%2520emerged%2520as%2520the%250Astate-of-the-art%2520approach%2520to%2520epsilon-optimally%2520solving%2520decentralized%2520partially%250Aobservable%2520Markov%2520decision%2520processes.%2520However%252C%2520scalability%2520remains%2520a%250Asignificant%2520issue.%2520This%2520paper%2520presents%2520a%2520novel%2520and%2520more%2520scalable%2520alternative%252C%250Anamely%2520sequential-move%2520centralized%2520training%2520for%2520decentralized%2520execution.%2520This%250Aparadigm%2520further%2520pushes%2520the%2520applicability%2520of%2520Bellman%2527s%2520principle%2520of%2520optimality%252C%250Araising%2520three%2520new%2520properties.%2520First%252C%2520it%2520allows%2520a%2520central%2520planner%2520to%2520reason%2520upon%250Asufficient%2520sequential-move%2520statistics%2520instead%2520of%2520prior%2520simultaneous-move%2520ones.%250ANext%252C%2520it%2520proves%2520that%2520epsilon-optimal%2520value%2520functions%2520are%2520piecewise%2520linear%2520and%250Aconvex%2520in%2520sufficient%2520sequential-move%2520statistics.%2520Finally%252C%2520it%2520drops%2520the%250Acomplexity%2520of%2520the%2520backup%2520operators%2520from%2520double%2520exponential%2520to%2520polynomial%2520at%2520the%250Aexpense%2520of%2520longer%2520planning%2520horizons.%2520Besides%252C%2520it%2520makes%2520it%2520easy%2520to%2520use%250Asingle-agent%2520methods%252C%2520e.g.%252C%2520SARSA%2520algorithm%2520enhanced%2520with%2520these%2520findings%250Aapplies%2520while%2520still%2520preserving%2520convergence%2520guarantees.%2520Experiments%2520on%2520two-%2520as%250Awell%2520as%2520many-agent%2520domains%2520from%2520the%2520literature%2520against%2520epsilon-optimal%250Asimultaneous-move%2520solvers%2520confirm%2520the%2520superiority%2520of%2520the%2520novel%2520approach.%2520This%250Aparadigm%2520opens%2520the%2520door%2520for%2520efficient%2520planning%2520and%2520reinforcement%2520learning%250Amethods%2520for%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimally%20Solving%20Simultaneous-Move%20Dec-POMDPs%3A%20The%20Sequential%20Central%0A%20%20Planning%20Approach&entry.906535625=Johan%20Peralez%20and%20Aur%C3%A9lien%20Delage%20and%20Jacopo%20Castellini%20and%20Rafael%20F.%20Cunha%20and%20Jilles%20S.%20Dibangoye&entry.1292438233=%20%20Centralized%20training%20for%20decentralized%20execution%20paradigm%20emerged%20as%20the%0Astate-of-the-art%20approach%20to%20epsilon-optimally%20solving%20decentralized%20partially%0Aobservable%20Markov%20decision%20processes.%20However%2C%20scalability%20remains%20a%0Asignificant%20issue.%20This%20paper%20presents%20a%20novel%20and%20more%20scalable%20alternative%2C%0Anamely%20sequential-move%20centralized%20training%20for%20decentralized%20execution.%20This%0Aparadigm%20further%20pushes%20the%20applicability%20of%20Bellman%27s%20principle%20of%20optimality%2C%0Araising%20three%20new%20properties.%20First%2C%20it%20allows%20a%20central%20planner%20to%20reason%20upon%0Asufficient%20sequential-move%20statistics%20instead%20of%20prior%20simultaneous-move%20ones.%0ANext%2C%20it%20proves%20that%20epsilon-optimal%20value%20functions%20are%20piecewise%20linear%20and%0Aconvex%20in%20sufficient%20sequential-move%20statistics.%20Finally%2C%20it%20drops%20the%0Acomplexity%20of%20the%20backup%20operators%20from%20double%20exponential%20to%20polynomial%20at%20the%0Aexpense%20of%20longer%20planning%20horizons.%20Besides%2C%20it%20makes%20it%20easy%20to%20use%0Asingle-agent%20methods%2C%20e.g.%2C%20SARSA%20algorithm%20enhanced%20with%20these%20findings%0Aapplies%20while%20still%20preserving%20convergence%20guarantees.%20Experiments%20on%20two-%20as%0Awell%20as%20many-agent%20domains%20from%20the%20literature%20against%20epsilon-optimal%0Asimultaneous-move%20solvers%20confirm%20the%20superiority%20of%20the%20novel%20approach.%20This%0Aparadigm%20opens%20the%20door%20for%20efficient%20planning%20and%20reinforcement%20learning%0Amethods%20for%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13139v1&entry.124074799=Read"},
{"title": "Servo Integrated Nonlinear Model Predictive Control for Overactuated\n  Tiltable-Quadrotors", "author": "Jinjie Li and Junichiro Sugihara and Moju Zhao", "abstract": "  Utilizing a servo to tilt each rotor transforms quadrotors from underactuated\nto overactuated systems, allowing for independent control of both attitude and\nposition, which provides advantages for aerial manipulation. However, this\nenhancement also introduces model nonlinearity, sluggish servo response, and\nlimited operational range into the system, posing challenges to dynamic\ncontrol. In this study, we propose a control approach for tiltable-quadrotors\nbased on nonlinear model predictive control (NMPC). Unlike conventional cascade\nmethods, our approach preserves the full dynamics without simplification. It\ndirectly uses rotor thrust and servo angle as control inputs, where their\nlimited working ranges are considered input constraints. Notably, we\nincorporate a first-order servo model within the NMPC framework. Simulation\nreveals that integrating the servo dynamics is not only an enhancement to\ncontrol performance but also a critical factor for optimization convergence. To\nevaluate the effectiveness of our approach, we fabricate a tiltable-quadrotor\nand deploy the algorithm onboard at 100 Hz. Extensive real-world experiments\ndemonstrate rapid, robust, and smooth pose-tracking performance.\n", "link": "http://arxiv.org/abs/2405.09871v2", "date": "2024-08-23", "relevancy": 2.0048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5262}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.506}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Servo%20Integrated%20Nonlinear%20Model%20Predictive%20Control%20for%20Overactuated%0A%20%20Tiltable-Quadrotors&body=Title%3A%20Servo%20Integrated%20Nonlinear%20Model%20Predictive%20Control%20for%20Overactuated%0A%20%20Tiltable-Quadrotors%0AAuthor%3A%20Jinjie%20Li%20and%20Junichiro%20Sugihara%20and%20Moju%20Zhao%0AAbstract%3A%20%20%20Utilizing%20a%20servo%20to%20tilt%20each%20rotor%20transforms%20quadrotors%20from%20underactuated%0Ato%20overactuated%20systems%2C%20allowing%20for%20independent%20control%20of%20both%20attitude%20and%0Aposition%2C%20which%20provides%20advantages%20for%20aerial%20manipulation.%20However%2C%20this%0Aenhancement%20also%20introduces%20model%20nonlinearity%2C%20sluggish%20servo%20response%2C%20and%0Alimited%20operational%20range%20into%20the%20system%2C%20posing%20challenges%20to%20dynamic%0Acontrol.%20In%20this%20study%2C%20we%20propose%20a%20control%20approach%20for%20tiltable-quadrotors%0Abased%20on%20nonlinear%20model%20predictive%20control%20%28NMPC%29.%20Unlike%20conventional%20cascade%0Amethods%2C%20our%20approach%20preserves%20the%20full%20dynamics%20without%20simplification.%20It%0Adirectly%20uses%20rotor%20thrust%20and%20servo%20angle%20as%20control%20inputs%2C%20where%20their%0Alimited%20working%20ranges%20are%20considered%20input%20constraints.%20Notably%2C%20we%0Aincorporate%20a%20first-order%20servo%20model%20within%20the%20NMPC%20framework.%20Simulation%0Areveals%20that%20integrating%20the%20servo%20dynamics%20is%20not%20only%20an%20enhancement%20to%0Acontrol%20performance%20but%20also%20a%20critical%20factor%20for%20optimization%20convergence.%20To%0Aevaluate%20the%20effectiveness%20of%20our%20approach%2C%20we%20fabricate%20a%20tiltable-quadrotor%0Aand%20deploy%20the%20algorithm%20onboard%20at%20100%20Hz.%20Extensive%20real-world%20experiments%0Ademonstrate%20rapid%2C%20robust%2C%20and%20smooth%20pose-tracking%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09871v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DServo%2520Integrated%2520Nonlinear%2520Model%2520Predictive%2520Control%2520for%2520Overactuated%250A%2520%2520Tiltable-Quadrotors%26entry.906535625%3DJinjie%2520Li%2520and%2520Junichiro%2520Sugihara%2520and%2520Moju%2520Zhao%26entry.1292438233%3D%2520%2520Utilizing%2520a%2520servo%2520to%2520tilt%2520each%2520rotor%2520transforms%2520quadrotors%2520from%2520underactuated%250Ato%2520overactuated%2520systems%252C%2520allowing%2520for%2520independent%2520control%2520of%2520both%2520attitude%2520and%250Aposition%252C%2520which%2520provides%2520advantages%2520for%2520aerial%2520manipulation.%2520However%252C%2520this%250Aenhancement%2520also%2520introduces%2520model%2520nonlinearity%252C%2520sluggish%2520servo%2520response%252C%2520and%250Alimited%2520operational%2520range%2520into%2520the%2520system%252C%2520posing%2520challenges%2520to%2520dynamic%250Acontrol.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520control%2520approach%2520for%2520tiltable-quadrotors%250Abased%2520on%2520nonlinear%2520model%2520predictive%2520control%2520%2528NMPC%2529.%2520Unlike%2520conventional%2520cascade%250Amethods%252C%2520our%2520approach%2520preserves%2520the%2520full%2520dynamics%2520without%2520simplification.%2520It%250Adirectly%2520uses%2520rotor%2520thrust%2520and%2520servo%2520angle%2520as%2520control%2520inputs%252C%2520where%2520their%250Alimited%2520working%2520ranges%2520are%2520considered%2520input%2520constraints.%2520Notably%252C%2520we%250Aincorporate%2520a%2520first-order%2520servo%2520model%2520within%2520the%2520NMPC%2520framework.%2520Simulation%250Areveals%2520that%2520integrating%2520the%2520servo%2520dynamics%2520is%2520not%2520only%2520an%2520enhancement%2520to%250Acontrol%2520performance%2520but%2520also%2520a%2520critical%2520factor%2520for%2520optimization%2520convergence.%2520To%250Aevaluate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520we%2520fabricate%2520a%2520tiltable-quadrotor%250Aand%2520deploy%2520the%2520algorithm%2520onboard%2520at%2520100%2520Hz.%2520Extensive%2520real-world%2520experiments%250Ademonstrate%2520rapid%252C%2520robust%252C%2520and%2520smooth%2520pose-tracking%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09871v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Servo%20Integrated%20Nonlinear%20Model%20Predictive%20Control%20for%20Overactuated%0A%20%20Tiltable-Quadrotors&entry.906535625=Jinjie%20Li%20and%20Junichiro%20Sugihara%20and%20Moju%20Zhao&entry.1292438233=%20%20Utilizing%20a%20servo%20to%20tilt%20each%20rotor%20transforms%20quadrotors%20from%20underactuated%0Ato%20overactuated%20systems%2C%20allowing%20for%20independent%20control%20of%20both%20attitude%20and%0Aposition%2C%20which%20provides%20advantages%20for%20aerial%20manipulation.%20However%2C%20this%0Aenhancement%20also%20introduces%20model%20nonlinearity%2C%20sluggish%20servo%20response%2C%20and%0Alimited%20operational%20range%20into%20the%20system%2C%20posing%20challenges%20to%20dynamic%0Acontrol.%20In%20this%20study%2C%20we%20propose%20a%20control%20approach%20for%20tiltable-quadrotors%0Abased%20on%20nonlinear%20model%20predictive%20control%20%28NMPC%29.%20Unlike%20conventional%20cascade%0Amethods%2C%20our%20approach%20preserves%20the%20full%20dynamics%20without%20simplification.%20It%0Adirectly%20uses%20rotor%20thrust%20and%20servo%20angle%20as%20control%20inputs%2C%20where%20their%0Alimited%20working%20ranges%20are%20considered%20input%20constraints.%20Notably%2C%20we%0Aincorporate%20a%20first-order%20servo%20model%20within%20the%20NMPC%20framework.%20Simulation%0Areveals%20that%20integrating%20the%20servo%20dynamics%20is%20not%20only%20an%20enhancement%20to%0Acontrol%20performance%20but%20also%20a%20critical%20factor%20for%20optimization%20convergence.%20To%0Aevaluate%20the%20effectiveness%20of%20our%20approach%2C%20we%20fabricate%20a%20tiltable-quadrotor%0Aand%20deploy%20the%20algorithm%20onboard%20at%20100%20Hz.%20Extensive%20real-world%20experiments%0Ademonstrate%20rapid%2C%20robust%2C%20and%20smooth%20pose-tracking%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09871v2&entry.124074799=Read"},
{"title": "Near-field Beam training for Extremely Large-scale MIMO Based on Deep\n  Learning", "author": "Jiali Nie and Yuanhao Cui and Zhaohui Yang and Weijie Yuan and Xiaojun Jing", "abstract": "  Extremely Large-scale Array (ELAA) is considered a frontier technology for\nfuture communication systems, pivotal in improving wireless systems' rate and\nspectral efficiency. As ELAA employs a multitude of antennas operating at\nhigher frequencies, users are typically situated in the near-field region where\nthe spherical wavefront propagates. The near-field beam training in ELAA\nrequires both angle and distance information, which inevitably leads to a\nsignificant increase in the beam training overhead. To address this problem, we\npropose a near-field beam training method based on deep learning. We use a\nconvolutional neural network (CNN) to efficiently learn channel characteristics\nfrom historical data by strategically selecting padding and kernel sizes. The\nnegative value of the user average achievable rate is utilized as the loss\nfunction to optimize the beamformer. This method maximizes multi-user networks'\nachievable rate without predefined beam codebooks. Upon deployment, the model\nrequires solely the pre-estimated channel state information (CSI) to derive the\noptimal beamforming vector. The simulation results demonstrate that the\nproposed scheme achieves a more stable beamforming gain and significantly\nimproves performance compared to the traditional beam training method.\nFurthermore, owing to the inherent traits of deep learning methodologies, this\napproach substantially diminishes the near-field beam training overhead.\n", "link": "http://arxiv.org/abs/2406.03249v2", "date": "2024-08-23", "relevancy": 1.9903, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5056}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4948}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-field%20Beam%20training%20for%20Extremely%20Large-scale%20MIMO%20Based%20on%20Deep%0A%20%20Learning&body=Title%3A%20Near-field%20Beam%20training%20for%20Extremely%20Large-scale%20MIMO%20Based%20on%20Deep%0A%20%20Learning%0AAuthor%3A%20Jiali%20Nie%20and%20Yuanhao%20Cui%20and%20Zhaohui%20Yang%20and%20Weijie%20Yuan%20and%20Xiaojun%20Jing%0AAbstract%3A%20%20%20Extremely%20Large-scale%20Array%20%28ELAA%29%20is%20considered%20a%20frontier%20technology%20for%0Afuture%20communication%20systems%2C%20pivotal%20in%20improving%20wireless%20systems%27%20rate%20and%0Aspectral%20efficiency.%20As%20ELAA%20employs%20a%20multitude%20of%20antennas%20operating%20at%0Ahigher%20frequencies%2C%20users%20are%20typically%20situated%20in%20the%20near-field%20region%20where%0Athe%20spherical%20wavefront%20propagates.%20The%20near-field%20beam%20training%20in%20ELAA%0Arequires%20both%20angle%20and%20distance%20information%2C%20which%20inevitably%20leads%20to%20a%0Asignificant%20increase%20in%20the%20beam%20training%20overhead.%20To%20address%20this%20problem%2C%20we%0Apropose%20a%20near-field%20beam%20training%20method%20based%20on%20deep%20learning.%20We%20use%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20to%20efficiently%20learn%20channel%20characteristics%0Afrom%20historical%20data%20by%20strategically%20selecting%20padding%20and%20kernel%20sizes.%20The%0Anegative%20value%20of%20the%20user%20average%20achievable%20rate%20is%20utilized%20as%20the%20loss%0Afunction%20to%20optimize%20the%20beamformer.%20This%20method%20maximizes%20multi-user%20networks%27%0Aachievable%20rate%20without%20predefined%20beam%20codebooks.%20Upon%20deployment%2C%20the%20model%0Arequires%20solely%20the%20pre-estimated%20channel%20state%20information%20%28CSI%29%20to%20derive%20the%0Aoptimal%20beamforming%20vector.%20The%20simulation%20results%20demonstrate%20that%20the%0Aproposed%20scheme%20achieves%20a%20more%20stable%20beamforming%20gain%20and%20significantly%0Aimproves%20performance%20compared%20to%20the%20traditional%20beam%20training%20method.%0AFurthermore%2C%20owing%20to%20the%20inherent%20traits%20of%20deep%20learning%20methodologies%2C%20this%0Aapproach%20substantially%20diminishes%20the%20near-field%20beam%20training%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03249v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-field%2520Beam%2520training%2520for%2520Extremely%2520Large-scale%2520MIMO%2520Based%2520on%2520Deep%250A%2520%2520Learning%26entry.906535625%3DJiali%2520Nie%2520and%2520Yuanhao%2520Cui%2520and%2520Zhaohui%2520Yang%2520and%2520Weijie%2520Yuan%2520and%2520Xiaojun%2520Jing%26entry.1292438233%3D%2520%2520Extremely%2520Large-scale%2520Array%2520%2528ELAA%2529%2520is%2520considered%2520a%2520frontier%2520technology%2520for%250Afuture%2520communication%2520systems%252C%2520pivotal%2520in%2520improving%2520wireless%2520systems%2527%2520rate%2520and%250Aspectral%2520efficiency.%2520As%2520ELAA%2520employs%2520a%2520multitude%2520of%2520antennas%2520operating%2520at%250Ahigher%2520frequencies%252C%2520users%2520are%2520typically%2520situated%2520in%2520the%2520near-field%2520region%2520where%250Athe%2520spherical%2520wavefront%2520propagates.%2520The%2520near-field%2520beam%2520training%2520in%2520ELAA%250Arequires%2520both%2520angle%2520and%2520distance%2520information%252C%2520which%2520inevitably%2520leads%2520to%2520a%250Asignificant%2520increase%2520in%2520the%2520beam%2520training%2520overhead.%2520To%2520address%2520this%2520problem%252C%2520we%250Apropose%2520a%2520near-field%2520beam%2520training%2520method%2520based%2520on%2520deep%2520learning.%2520We%2520use%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520to%2520efficiently%2520learn%2520channel%2520characteristics%250Afrom%2520historical%2520data%2520by%2520strategically%2520selecting%2520padding%2520and%2520kernel%2520sizes.%2520The%250Anegative%2520value%2520of%2520the%2520user%2520average%2520achievable%2520rate%2520is%2520utilized%2520as%2520the%2520loss%250Afunction%2520to%2520optimize%2520the%2520beamformer.%2520This%2520method%2520maximizes%2520multi-user%2520networks%2527%250Aachievable%2520rate%2520without%2520predefined%2520beam%2520codebooks.%2520Upon%2520deployment%252C%2520the%2520model%250Arequires%2520solely%2520the%2520pre-estimated%2520channel%2520state%2520information%2520%2528CSI%2529%2520to%2520derive%2520the%250Aoptimal%2520beamforming%2520vector.%2520The%2520simulation%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520scheme%2520achieves%2520a%2520more%2520stable%2520beamforming%2520gain%2520and%2520significantly%250Aimproves%2520performance%2520compared%2520to%2520the%2520traditional%2520beam%2520training%2520method.%250AFurthermore%252C%2520owing%2520to%2520the%2520inherent%2520traits%2520of%2520deep%2520learning%2520methodologies%252C%2520this%250Aapproach%2520substantially%2520diminishes%2520the%2520near-field%2520beam%2520training%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03249v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-field%20Beam%20training%20for%20Extremely%20Large-scale%20MIMO%20Based%20on%20Deep%0A%20%20Learning&entry.906535625=Jiali%20Nie%20and%20Yuanhao%20Cui%20and%20Zhaohui%20Yang%20and%20Weijie%20Yuan%20and%20Xiaojun%20Jing&entry.1292438233=%20%20Extremely%20Large-scale%20Array%20%28ELAA%29%20is%20considered%20a%20frontier%20technology%20for%0Afuture%20communication%20systems%2C%20pivotal%20in%20improving%20wireless%20systems%27%20rate%20and%0Aspectral%20efficiency.%20As%20ELAA%20employs%20a%20multitude%20of%20antennas%20operating%20at%0Ahigher%20frequencies%2C%20users%20are%20typically%20situated%20in%20the%20near-field%20region%20where%0Athe%20spherical%20wavefront%20propagates.%20The%20near-field%20beam%20training%20in%20ELAA%0Arequires%20both%20angle%20and%20distance%20information%2C%20which%20inevitably%20leads%20to%20a%0Asignificant%20increase%20in%20the%20beam%20training%20overhead.%20To%20address%20this%20problem%2C%20we%0Apropose%20a%20near-field%20beam%20training%20method%20based%20on%20deep%20learning.%20We%20use%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20to%20efficiently%20learn%20channel%20characteristics%0Afrom%20historical%20data%20by%20strategically%20selecting%20padding%20and%20kernel%20sizes.%20The%0Anegative%20value%20of%20the%20user%20average%20achievable%20rate%20is%20utilized%20as%20the%20loss%0Afunction%20to%20optimize%20the%20beamformer.%20This%20method%20maximizes%20multi-user%20networks%27%0Aachievable%20rate%20without%20predefined%20beam%20codebooks.%20Upon%20deployment%2C%20the%20model%0Arequires%20solely%20the%20pre-estimated%20channel%20state%20information%20%28CSI%29%20to%20derive%20the%0Aoptimal%20beamforming%20vector.%20The%20simulation%20results%20demonstrate%20that%20the%0Aproposed%20scheme%20achieves%20a%20more%20stable%20beamforming%20gain%20and%20significantly%0Aimproves%20performance%20compared%20to%20the%20traditional%20beam%20training%20method.%0AFurthermore%2C%20owing%20to%20the%20inherent%20traits%20of%20deep%20learning%20methodologies%2C%20this%0Aapproach%20substantially%20diminishes%20the%20near-field%20beam%20training%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03249v2&entry.124074799=Read"},
{"title": "Accuracy Improvement of Cell Image Segmentation Using Feedback Former", "author": "Hinako Mitsuoka and Kazuhiro Hotta", "abstract": "  Semantic segmentation of microscopy cell images by deep learning is a\nsignificant technique. We considered that the Transformers, which have recently\noutperformed CNNs in image recognition, could also be improved and developed\nfor cell image segmentation. Transformers tend to focus more on contextual\ninformation than on detailed information. This tendency leads to a lack of\ndetailed information for segmentation. Therefore, to supplement or reinforce\nthe missing detailed information, we hypothesized that feedback processing in\nthe human visual cortex should be effective. Our proposed Feedback Former is a\nnovel architecture for semantic segmentation, in which Transformers is used as\nan encoder and has a feedback processing mechanism. Feature maps with detailed\ninformation are fed back to the lower layers from near the output of the model\nto compensate for the lack of detailed information which is the weakness of\nTransformers and improve the segmentation accuracy. By experiments on three\ncell image datasets, we confirmed that our method surpasses methods without\nfeedback, demonstrating its superior accuracy in cell image segmentation. Our\nmethod achieved higher segmentation accuracy while consuming less computational\ncost than conventional feedback approaches. Moreover, our method offered\nsuperior precision without simply increasing the model size of Transformer\nencoder, demonstrating higher accuracy with lower computational cost.\n", "link": "http://arxiv.org/abs/2408.12974v1", "date": "2024-08-23", "relevancy": 1.9768, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5575}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy%20Improvement%20of%20Cell%20Image%20Segmentation%20Using%20Feedback%20Former&body=Title%3A%20Accuracy%20Improvement%20of%20Cell%20Image%20Segmentation%20Using%20Feedback%20Former%0AAuthor%3A%20Hinako%20Mitsuoka%20and%20Kazuhiro%20Hotta%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20microscopy%20cell%20images%20by%20deep%20learning%20is%20a%0Asignificant%20technique.%20We%20considered%20that%20the%20Transformers%2C%20which%20have%20recently%0Aoutperformed%20CNNs%20in%20image%20recognition%2C%20could%20also%20be%20improved%20and%20developed%0Afor%20cell%20image%20segmentation.%20Transformers%20tend%20to%20focus%20more%20on%20contextual%0Ainformation%20than%20on%20detailed%20information.%20This%20tendency%20leads%20to%20a%20lack%20of%0Adetailed%20information%20for%20segmentation.%20Therefore%2C%20to%20supplement%20or%20reinforce%0Athe%20missing%20detailed%20information%2C%20we%20hypothesized%20that%20feedback%20processing%20in%0Athe%20human%20visual%20cortex%20should%20be%20effective.%20Our%20proposed%20Feedback%20Former%20is%20a%0Anovel%20architecture%20for%20semantic%20segmentation%2C%20in%20which%20Transformers%20is%20used%20as%0Aan%20encoder%20and%20has%20a%20feedback%20processing%20mechanism.%20Feature%20maps%20with%20detailed%0Ainformation%20are%20fed%20back%20to%20the%20lower%20layers%20from%20near%20the%20output%20of%20the%20model%0Ato%20compensate%20for%20the%20lack%20of%20detailed%20information%20which%20is%20the%20weakness%20of%0ATransformers%20and%20improve%20the%20segmentation%20accuracy.%20By%20experiments%20on%20three%0Acell%20image%20datasets%2C%20we%20confirmed%20that%20our%20method%20surpasses%20methods%20without%0Afeedback%2C%20demonstrating%20its%20superior%20accuracy%20in%20cell%20image%20segmentation.%20Our%0Amethod%20achieved%20higher%20segmentation%20accuracy%20while%20consuming%20less%20computational%0Acost%20than%20conventional%20feedback%20approaches.%20Moreover%2C%20our%20method%20offered%0Asuperior%20precision%20without%20simply%20increasing%20the%20model%20size%20of%20Transformer%0Aencoder%2C%20demonstrating%20higher%20accuracy%20with%20lower%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy%2520Improvement%2520of%2520Cell%2520Image%2520Segmentation%2520Using%2520Feedback%2520Former%26entry.906535625%3DHinako%2520Mitsuoka%2520and%2520Kazuhiro%2520Hotta%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520microscopy%2520cell%2520images%2520by%2520deep%2520learning%2520is%2520a%250Asignificant%2520technique.%2520We%2520considered%2520that%2520the%2520Transformers%252C%2520which%2520have%2520recently%250Aoutperformed%2520CNNs%2520in%2520image%2520recognition%252C%2520could%2520also%2520be%2520improved%2520and%2520developed%250Afor%2520cell%2520image%2520segmentation.%2520Transformers%2520tend%2520to%2520focus%2520more%2520on%2520contextual%250Ainformation%2520than%2520on%2520detailed%2520information.%2520This%2520tendency%2520leads%2520to%2520a%2520lack%2520of%250Adetailed%2520information%2520for%2520segmentation.%2520Therefore%252C%2520to%2520supplement%2520or%2520reinforce%250Athe%2520missing%2520detailed%2520information%252C%2520we%2520hypothesized%2520that%2520feedback%2520processing%2520in%250Athe%2520human%2520visual%2520cortex%2520should%2520be%2520effective.%2520Our%2520proposed%2520Feedback%2520Former%2520is%2520a%250Anovel%2520architecture%2520for%2520semantic%2520segmentation%252C%2520in%2520which%2520Transformers%2520is%2520used%2520as%250Aan%2520encoder%2520and%2520has%2520a%2520feedback%2520processing%2520mechanism.%2520Feature%2520maps%2520with%2520detailed%250Ainformation%2520are%2520fed%2520back%2520to%2520the%2520lower%2520layers%2520from%2520near%2520the%2520output%2520of%2520the%2520model%250Ato%2520compensate%2520for%2520the%2520lack%2520of%2520detailed%2520information%2520which%2520is%2520the%2520weakness%2520of%250ATransformers%2520and%2520improve%2520the%2520segmentation%2520accuracy.%2520By%2520experiments%2520on%2520three%250Acell%2520image%2520datasets%252C%2520we%2520confirmed%2520that%2520our%2520method%2520surpasses%2520methods%2520without%250Afeedback%252C%2520demonstrating%2520its%2520superior%2520accuracy%2520in%2520cell%2520image%2520segmentation.%2520Our%250Amethod%2520achieved%2520higher%2520segmentation%2520accuracy%2520while%2520consuming%2520less%2520computational%250Acost%2520than%2520conventional%2520feedback%2520approaches.%2520Moreover%252C%2520our%2520method%2520offered%250Asuperior%2520precision%2520without%2520simply%2520increasing%2520the%2520model%2520size%2520of%2520Transformer%250Aencoder%252C%2520demonstrating%2520higher%2520accuracy%2520with%2520lower%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy%20Improvement%20of%20Cell%20Image%20Segmentation%20Using%20Feedback%20Former&entry.906535625=Hinako%20Mitsuoka%20and%20Kazuhiro%20Hotta&entry.1292438233=%20%20Semantic%20segmentation%20of%20microscopy%20cell%20images%20by%20deep%20learning%20is%20a%0Asignificant%20technique.%20We%20considered%20that%20the%20Transformers%2C%20which%20have%20recently%0Aoutperformed%20CNNs%20in%20image%20recognition%2C%20could%20also%20be%20improved%20and%20developed%0Afor%20cell%20image%20segmentation.%20Transformers%20tend%20to%20focus%20more%20on%20contextual%0Ainformation%20than%20on%20detailed%20information.%20This%20tendency%20leads%20to%20a%20lack%20of%0Adetailed%20information%20for%20segmentation.%20Therefore%2C%20to%20supplement%20or%20reinforce%0Athe%20missing%20detailed%20information%2C%20we%20hypothesized%20that%20feedback%20processing%20in%0Athe%20human%20visual%20cortex%20should%20be%20effective.%20Our%20proposed%20Feedback%20Former%20is%20a%0Anovel%20architecture%20for%20semantic%20segmentation%2C%20in%20which%20Transformers%20is%20used%20as%0Aan%20encoder%20and%20has%20a%20feedback%20processing%20mechanism.%20Feature%20maps%20with%20detailed%0Ainformation%20are%20fed%20back%20to%20the%20lower%20layers%20from%20near%20the%20output%20of%20the%20model%0Ato%20compensate%20for%20the%20lack%20of%20detailed%20information%20which%20is%20the%20weakness%20of%0ATransformers%20and%20improve%20the%20segmentation%20accuracy.%20By%20experiments%20on%20three%0Acell%20image%20datasets%2C%20we%20confirmed%20that%20our%20method%20surpasses%20methods%20without%0Afeedback%2C%20demonstrating%20its%20superior%20accuracy%20in%20cell%20image%20segmentation.%20Our%0Amethod%20achieved%20higher%20segmentation%20accuracy%20while%20consuming%20less%20computational%0Acost%20than%20conventional%20feedback%20approaches.%20Moreover%2C%20our%20method%20offered%0Asuperior%20precision%20without%20simply%20increasing%20the%20model%20size%20of%20Transformer%0Aencoder%2C%20demonstrating%20higher%20accuracy%20with%20lower%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12974v1&entry.124074799=Read"},
{"title": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives", "author": "Desta Haileselassie Hagos and Rick Battle and Danda B. Rawat", "abstract": "  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n", "link": "http://arxiv.org/abs/2407.14962v5", "date": "2024-08-23", "relevancy": 1.971, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5227}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4763}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Generative%20AI%20and%20Large%20Language%20Models%3A%20Current%0A%20%20Status%2C%20Challenges%2C%20and%20Perspectives&body=Title%3A%20Recent%20Advances%20in%20Generative%20AI%20and%20Large%20Language%20Models%3A%20Current%0A%20%20Status%2C%20Challenges%2C%20and%20Perspectives%0AAuthor%3A%20Desta%20Haileselassie%20Hagos%20and%20Rick%20Battle%20and%20Danda%20B.%20Rawat%0AAbstract%3A%20%20%20The%20emergence%20of%20Generative%20Artificial%20Intelligence%20%28AI%29%20and%20Large%20Language%0AModels%20%28LLMs%29%20has%20marked%20a%20new%20era%20of%20Natural%20Language%20Processing%20%28NLP%29%2C%0Aintroducing%20unprecedented%20capabilities%20that%20are%20revolutionizing%20various%0Adomains.%20This%20paper%20explores%20the%20current%20state%20of%20these%20cutting-edge%0Atechnologies%2C%20demonstrating%20their%20remarkable%20advancements%20and%20wide-ranging%0Aapplications.%20Our%20paper%20contributes%20to%20providing%20a%20holistic%20perspective%20on%20the%0Atechnical%20foundations%2C%20practical%20applications%2C%20and%20emerging%20challenges%20within%0Athe%20evolving%20landscape%20of%20Generative%20AI%20and%20LLMs.%20We%20believe%20that%20understanding%0Athe%20generative%20capabilities%20of%20AI%20systems%20and%20the%20specific%20context%20of%20LLMs%20is%0Acrucial%20for%20researchers%2C%20practitioners%2C%20and%20policymakers%20to%20collaboratively%0Ashape%20the%20responsible%20and%20ethical%20integration%20of%20these%20technologies%20into%0Avarious%20domains.%20Furthermore%2C%20we%20identify%20and%20address%20main%20research%20gaps%2C%0Aproviding%20valuable%20insights%20to%20guide%20future%20research%20endeavors%20within%20the%20AI%0Aresearch%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14962v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520Generative%2520AI%2520and%2520Large%2520Language%2520Models%253A%2520Current%250A%2520%2520Status%252C%2520Challenges%252C%2520and%2520Perspectives%26entry.906535625%3DDesta%2520Haileselassie%2520Hagos%2520and%2520Rick%2520Battle%2520and%2520Danda%2520B.%2520Rawat%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520has%2520marked%2520a%2520new%2520era%2520of%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%252C%250Aintroducing%2520unprecedented%2520capabilities%2520that%2520are%2520revolutionizing%2520various%250Adomains.%2520This%2520paper%2520explores%2520the%2520current%2520state%2520of%2520these%2520cutting-edge%250Atechnologies%252C%2520demonstrating%2520their%2520remarkable%2520advancements%2520and%2520wide-ranging%250Aapplications.%2520Our%2520paper%2520contributes%2520to%2520providing%2520a%2520holistic%2520perspective%2520on%2520the%250Atechnical%2520foundations%252C%2520practical%2520applications%252C%2520and%2520emerging%2520challenges%2520within%250Athe%2520evolving%2520landscape%2520of%2520Generative%2520AI%2520and%2520LLMs.%2520We%2520believe%2520that%2520understanding%250Athe%2520generative%2520capabilities%2520of%2520AI%2520systems%2520and%2520the%2520specific%2520context%2520of%2520LLMs%2520is%250Acrucial%2520for%2520researchers%252C%2520practitioners%252C%2520and%2520policymakers%2520to%2520collaboratively%250Ashape%2520the%2520responsible%2520and%2520ethical%2520integration%2520of%2520these%2520technologies%2520into%250Avarious%2520domains.%2520Furthermore%252C%2520we%2520identify%2520and%2520address%2520main%2520research%2520gaps%252C%250Aproviding%2520valuable%2520insights%2520to%2520guide%2520future%2520research%2520endeavors%2520within%2520the%2520AI%250Aresearch%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14962v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Generative%20AI%20and%20Large%20Language%20Models%3A%20Current%0A%20%20Status%2C%20Challenges%2C%20and%20Perspectives&entry.906535625=Desta%20Haileselassie%20Hagos%20and%20Rick%20Battle%20and%20Danda%20B.%20Rawat&entry.1292438233=%20%20The%20emergence%20of%20Generative%20Artificial%20Intelligence%20%28AI%29%20and%20Large%20Language%0AModels%20%28LLMs%29%20has%20marked%20a%20new%20era%20of%20Natural%20Language%20Processing%20%28NLP%29%2C%0Aintroducing%20unprecedented%20capabilities%20that%20are%20revolutionizing%20various%0Adomains.%20This%20paper%20explores%20the%20current%20state%20of%20these%20cutting-edge%0Atechnologies%2C%20demonstrating%20their%20remarkable%20advancements%20and%20wide-ranging%0Aapplications.%20Our%20paper%20contributes%20to%20providing%20a%20holistic%20perspective%20on%20the%0Atechnical%20foundations%2C%20practical%20applications%2C%20and%20emerging%20challenges%20within%0Athe%20evolving%20landscape%20of%20Generative%20AI%20and%20LLMs.%20We%20believe%20that%20understanding%0Athe%20generative%20capabilities%20of%20AI%20systems%20and%20the%20specific%20context%20of%20LLMs%20is%0Acrucial%20for%20researchers%2C%20practitioners%2C%20and%20policymakers%20to%20collaboratively%0Ashape%20the%20responsible%20and%20ethical%20integration%20of%20these%20technologies%20into%0Avarious%20domains.%20Furthermore%2C%20we%20identify%20and%20address%20main%20research%20gaps%2C%0Aproviding%20valuable%20insights%20to%20guide%20future%20research%20endeavors%20within%20the%20AI%0Aresearch%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14962v5&entry.124074799=Read"},
{"title": "Improving Equivariant Model Training via Constraint Relaxation", "author": "Stefanos Pertigkiozoglou and Evangelos Chatzipantazis and Shubhendu Trivedi and Kostas Daniilidis", "abstract": "  Equivariant neural networks have been widely used in a variety of\napplications due to their ability to generalize well in tasks where the\nunderlying data symmetries are known. Despite their successes, such networks\ncan be difficult to optimize and require careful hyperparameter tuning to train\nsuccessfully. In this work, we propose a novel framework for improving the\noptimization of such models by relaxing the hard equivariance constraint during\ntraining: We relax the equivariance constraint of the network's intermediate\nlayers by introducing an additional non-equivariance term that we progressively\nconstrain until we arrive at an equivariant solution. By controlling the\nmagnitude of the activation of the additional relaxation term, we allow the\nmodel to optimize over a larger hypothesis space containing approximate\nequivariant networks and converge back to an equivariant solution at the end of\ntraining. We provide experimental results on different state-of-the-art network\narchitectures, demonstrating how this training framework can result in\nequivariant models with improved generalization performance.\n", "link": "http://arxiv.org/abs/2408.13242v1", "date": "2024-08-23", "relevancy": 1.9684, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5036}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5001}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Equivariant%20Model%20Training%20via%20Constraint%20Relaxation&body=Title%3A%20Improving%20Equivariant%20Model%20Training%20via%20Constraint%20Relaxation%0AAuthor%3A%20Stefanos%20Pertigkiozoglou%20and%20Evangelos%20Chatzipantazis%20and%20Shubhendu%20Trivedi%20and%20Kostas%20Daniilidis%0AAbstract%3A%20%20%20Equivariant%20neural%20networks%20have%20been%20widely%20used%20in%20a%20variety%20of%0Aapplications%20due%20to%20their%20ability%20to%20generalize%20well%20in%20tasks%20where%20the%0Aunderlying%20data%20symmetries%20are%20known.%20Despite%20their%20successes%2C%20such%20networks%0Acan%20be%20difficult%20to%20optimize%20and%20require%20careful%20hyperparameter%20tuning%20to%20train%0Asuccessfully.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20for%20improving%20the%0Aoptimization%20of%20such%20models%20by%20relaxing%20the%20hard%20equivariance%20constraint%20during%0Atraining%3A%20We%20relax%20the%20equivariance%20constraint%20of%20the%20network%27s%20intermediate%0Alayers%20by%20introducing%20an%20additional%20non-equivariance%20term%20that%20we%20progressively%0Aconstrain%20until%20we%20arrive%20at%20an%20equivariant%20solution.%20By%20controlling%20the%0Amagnitude%20of%20the%20activation%20of%20the%20additional%20relaxation%20term%2C%20we%20allow%20the%0Amodel%20to%20optimize%20over%20a%20larger%20hypothesis%20space%20containing%20approximate%0Aequivariant%20networks%20and%20converge%20back%20to%20an%20equivariant%20solution%20at%20the%20end%20of%0Atraining.%20We%20provide%20experimental%20results%20on%20different%20state-of-the-art%20network%0Aarchitectures%2C%20demonstrating%20how%20this%20training%20framework%20can%20result%20in%0Aequivariant%20models%20with%20improved%20generalization%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Equivariant%2520Model%2520Training%2520via%2520Constraint%2520Relaxation%26entry.906535625%3DStefanos%2520Pertigkiozoglou%2520and%2520Evangelos%2520Chatzipantazis%2520and%2520Shubhendu%2520Trivedi%2520and%2520Kostas%2520Daniilidis%26entry.1292438233%3D%2520%2520Equivariant%2520neural%2520networks%2520have%2520been%2520widely%2520used%2520in%2520a%2520variety%2520of%250Aapplications%2520due%2520to%2520their%2520ability%2520to%2520generalize%2520well%2520in%2520tasks%2520where%2520the%250Aunderlying%2520data%2520symmetries%2520are%2520known.%2520Despite%2520their%2520successes%252C%2520such%2520networks%250Acan%2520be%2520difficult%2520to%2520optimize%2520and%2520require%2520careful%2520hyperparameter%2520tuning%2520to%2520train%250Asuccessfully.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520improving%2520the%250Aoptimization%2520of%2520such%2520models%2520by%2520relaxing%2520the%2520hard%2520equivariance%2520constraint%2520during%250Atraining%253A%2520We%2520relax%2520the%2520equivariance%2520constraint%2520of%2520the%2520network%2527s%2520intermediate%250Alayers%2520by%2520introducing%2520an%2520additional%2520non-equivariance%2520term%2520that%2520we%2520progressively%250Aconstrain%2520until%2520we%2520arrive%2520at%2520an%2520equivariant%2520solution.%2520By%2520controlling%2520the%250Amagnitude%2520of%2520the%2520activation%2520of%2520the%2520additional%2520relaxation%2520term%252C%2520we%2520allow%2520the%250Amodel%2520to%2520optimize%2520over%2520a%2520larger%2520hypothesis%2520space%2520containing%2520approximate%250Aequivariant%2520networks%2520and%2520converge%2520back%2520to%2520an%2520equivariant%2520solution%2520at%2520the%2520end%2520of%250Atraining.%2520We%2520provide%2520experimental%2520results%2520on%2520different%2520state-of-the-art%2520network%250Aarchitectures%252C%2520demonstrating%2520how%2520this%2520training%2520framework%2520can%2520result%2520in%250Aequivariant%2520models%2520with%2520improved%2520generalization%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Equivariant%20Model%20Training%20via%20Constraint%20Relaxation&entry.906535625=Stefanos%20Pertigkiozoglou%20and%20Evangelos%20Chatzipantazis%20and%20Shubhendu%20Trivedi%20and%20Kostas%20Daniilidis&entry.1292438233=%20%20Equivariant%20neural%20networks%20have%20been%20widely%20used%20in%20a%20variety%20of%0Aapplications%20due%20to%20their%20ability%20to%20generalize%20well%20in%20tasks%20where%20the%0Aunderlying%20data%20symmetries%20are%20known.%20Despite%20their%20successes%2C%20such%20networks%0Acan%20be%20difficult%20to%20optimize%20and%20require%20careful%20hyperparameter%20tuning%20to%20train%0Asuccessfully.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20for%20improving%20the%0Aoptimization%20of%20such%20models%20by%20relaxing%20the%20hard%20equivariance%20constraint%20during%0Atraining%3A%20We%20relax%20the%20equivariance%20constraint%20of%20the%20network%27s%20intermediate%0Alayers%20by%20introducing%20an%20additional%20non-equivariance%20term%20that%20we%20progressively%0Aconstrain%20until%20we%20arrive%20at%20an%20equivariant%20solution.%20By%20controlling%20the%0Amagnitude%20of%20the%20activation%20of%20the%20additional%20relaxation%20term%2C%20we%20allow%20the%0Amodel%20to%20optimize%20over%20a%20larger%20hypothesis%20space%20containing%20approximate%0Aequivariant%20networks%20and%20converge%20back%20to%20an%20equivariant%20solution%20at%20the%20end%20of%0Atraining.%20We%20provide%20experimental%20results%20on%20different%20state-of-the-art%20network%0Aarchitectures%2C%20demonstrating%20how%20this%20training%20framework%20can%20result%20in%0Aequivariant%20models%20with%20improved%20generalization%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13242v1&entry.124074799=Read"},
{"title": "Energy-Efficient Spiking Recurrent Neural Network for Gesture\n  Recognition on Embedded GPUs", "author": "Marzieh Hassanshahi Varposhti and Mahyar Shahsavari and Marcel van Gerven", "abstract": "  Implementing AI algorithms on event-based embedded devices enables real-time\nprocessing of data, minimizes latency, and enhances power efficiency in edge\ncomputing. This research explores the deployment of a spiking recurrent neural\nnetwork (SRNN) with liquid time constant neurons for gesture recognition. We\nfocus on the energy efficiency and computational efficacy of NVIDIA Jetson Nano\nembedded GPU platforms. The embedded GPU showcases a 14-fold increase in power\nefficiency relative to a conventional GPU, making a compelling argument for its\nuse in energy-constrained applications. The study's empirical findings also\nhighlight that batch processing significantly boosts frame rates across various\nbatch sizes while maintaining accuracy levels well above the baseline. These\ninsights validate the SRNN with liquid time constant neurons as a robust model\nfor interpreting temporal-spatial data in gesture recognition, striking a\ncritical balance between processing speed and power frugality.\n", "link": "http://arxiv.org/abs/2408.12978v1", "date": "2024-08-23", "relevancy": 1.9659, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5131}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4814}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Efficient%20Spiking%20Recurrent%20Neural%20Network%20for%20Gesture%0A%20%20Recognition%20on%20Embedded%20GPUs&body=Title%3A%20Energy-Efficient%20Spiking%20Recurrent%20Neural%20Network%20for%20Gesture%0A%20%20Recognition%20on%20Embedded%20GPUs%0AAuthor%3A%20Marzieh%20Hassanshahi%20Varposhti%20and%20Mahyar%20Shahsavari%20and%20Marcel%20van%20Gerven%0AAbstract%3A%20%20%20Implementing%20AI%20algorithms%20on%20event-based%20embedded%20devices%20enables%20real-time%0Aprocessing%20of%20data%2C%20minimizes%20latency%2C%20and%20enhances%20power%20efficiency%20in%20edge%0Acomputing.%20This%20research%20explores%20the%20deployment%20of%20a%20spiking%20recurrent%20neural%0Anetwork%20%28SRNN%29%20with%20liquid%20time%20constant%20neurons%20for%20gesture%20recognition.%20We%0Afocus%20on%20the%20energy%20efficiency%20and%20computational%20efficacy%20of%20NVIDIA%20Jetson%20Nano%0Aembedded%20GPU%20platforms.%20The%20embedded%20GPU%20showcases%20a%2014-fold%20increase%20in%20power%0Aefficiency%20relative%20to%20a%20conventional%20GPU%2C%20making%20a%20compelling%20argument%20for%20its%0Ause%20in%20energy-constrained%20applications.%20The%20study%27s%20empirical%20findings%20also%0Ahighlight%20that%20batch%20processing%20significantly%20boosts%20frame%20rates%20across%20various%0Abatch%20sizes%20while%20maintaining%20accuracy%20levels%20well%20above%20the%20baseline.%20These%0Ainsights%20validate%20the%20SRNN%20with%20liquid%20time%20constant%20neurons%20as%20a%20robust%20model%0Afor%20interpreting%20temporal-spatial%20data%20in%20gesture%20recognition%2C%20striking%20a%0Acritical%20balance%20between%20processing%20speed%20and%20power%20frugality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Efficient%2520Spiking%2520Recurrent%2520Neural%2520Network%2520for%2520Gesture%250A%2520%2520Recognition%2520on%2520Embedded%2520GPUs%26entry.906535625%3DMarzieh%2520Hassanshahi%2520Varposhti%2520and%2520Mahyar%2520Shahsavari%2520and%2520Marcel%2520van%2520Gerven%26entry.1292438233%3D%2520%2520Implementing%2520AI%2520algorithms%2520on%2520event-based%2520embedded%2520devices%2520enables%2520real-time%250Aprocessing%2520of%2520data%252C%2520minimizes%2520latency%252C%2520and%2520enhances%2520power%2520efficiency%2520in%2520edge%250Acomputing.%2520This%2520research%2520explores%2520the%2520deployment%2520of%2520a%2520spiking%2520recurrent%2520neural%250Anetwork%2520%2528SRNN%2529%2520with%2520liquid%2520time%2520constant%2520neurons%2520for%2520gesture%2520recognition.%2520We%250Afocus%2520on%2520the%2520energy%2520efficiency%2520and%2520computational%2520efficacy%2520of%2520NVIDIA%2520Jetson%2520Nano%250Aembedded%2520GPU%2520platforms.%2520The%2520embedded%2520GPU%2520showcases%2520a%252014-fold%2520increase%2520in%2520power%250Aefficiency%2520relative%2520to%2520a%2520conventional%2520GPU%252C%2520making%2520a%2520compelling%2520argument%2520for%2520its%250Ause%2520in%2520energy-constrained%2520applications.%2520The%2520study%2527s%2520empirical%2520findings%2520also%250Ahighlight%2520that%2520batch%2520processing%2520significantly%2520boosts%2520frame%2520rates%2520across%2520various%250Abatch%2520sizes%2520while%2520maintaining%2520accuracy%2520levels%2520well%2520above%2520the%2520baseline.%2520These%250Ainsights%2520validate%2520the%2520SRNN%2520with%2520liquid%2520time%2520constant%2520neurons%2520as%2520a%2520robust%2520model%250Afor%2520interpreting%2520temporal-spatial%2520data%2520in%2520gesture%2520recognition%252C%2520striking%2520a%250Acritical%2520balance%2520between%2520processing%2520speed%2520and%2520power%2520frugality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Efficient%20Spiking%20Recurrent%20Neural%20Network%20for%20Gesture%0A%20%20Recognition%20on%20Embedded%20GPUs&entry.906535625=Marzieh%20Hassanshahi%20Varposhti%20and%20Mahyar%20Shahsavari%20and%20Marcel%20van%20Gerven&entry.1292438233=%20%20Implementing%20AI%20algorithms%20on%20event-based%20embedded%20devices%20enables%20real-time%0Aprocessing%20of%20data%2C%20minimizes%20latency%2C%20and%20enhances%20power%20efficiency%20in%20edge%0Acomputing.%20This%20research%20explores%20the%20deployment%20of%20a%20spiking%20recurrent%20neural%0Anetwork%20%28SRNN%29%20with%20liquid%20time%20constant%20neurons%20for%20gesture%20recognition.%20We%0Afocus%20on%20the%20energy%20efficiency%20and%20computational%20efficacy%20of%20NVIDIA%20Jetson%20Nano%0Aembedded%20GPU%20platforms.%20The%20embedded%20GPU%20showcases%20a%2014-fold%20increase%20in%20power%0Aefficiency%20relative%20to%20a%20conventional%20GPU%2C%20making%20a%20compelling%20argument%20for%20its%0Ause%20in%20energy-constrained%20applications.%20The%20study%27s%20empirical%20findings%20also%0Ahighlight%20that%20batch%20processing%20significantly%20boosts%20frame%20rates%20across%20various%0Abatch%20sizes%20while%20maintaining%20accuracy%20levels%20well%20above%20the%20baseline.%20These%0Ainsights%20validate%20the%20SRNN%20with%20liquid%20time%20constant%20neurons%20as%20a%20robust%20model%0Afor%20interpreting%20temporal-spatial%20data%20in%20gesture%20recognition%2C%20striking%20a%0Acritical%20balance%20between%20processing%20speed%20and%20power%20frugality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12978v1&entry.124074799=Read"},
{"title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt\n  Tuning through Modular Prompt Composition", "author": "Ahmad Pouramini and Hesham Faili", "abstract": "  In recent years, multi-task prompt tuning has garnered considerable attention\nfor its inherent modularity and potential to enhance parameter-efficient\ntransfer learning across diverse tasks. This paper aims to analyze and improve\nthe performance of multiple tasks by facilitating the transfer of knowledge\nbetween their corresponding prompts in a multi-task setting. Our proposed\napproach decomposes the prompt for each target task into a combination of\nshared prompts (source prompts) and a task-specific prompt (private prompt).\nDuring training, the source prompts undergo fine-tuning and are integrated with\nthe private prompt to drive the target prompt for each task. We present and\ncompare multiple methods for combining source prompts to construct the target\nprompt, analyzing the roles of both source and private prompts within each\nmethod. We investigate their contributions to task performance and offer\nflexible, adjustable configurations based on these insights to optimize\nperformance. Our empirical findings clearly showcase improvements in accuracy\nand robustness compared to the conventional practice of prompt tuning and\nrelated works. Notably, our results substantially outperform other methods in\nthe field in few-shot settings, demonstrating superior performance in various\ntasks across GLUE benchmark, among other tasks. This achievement is attained\nwith a significantly reduced amount of training data, making our method a\npromising one for few-shot settings.\n", "link": "http://arxiv.org/abs/2408.13227v1", "date": "2024-08-23", "relevancy": 1.9612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4747}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Few-Shot%20Transfer%20Learning%20with%20Optimized%20Multi-Task%20Prompt%0A%20%20Tuning%20through%20Modular%20Prompt%20Composition&body=Title%3A%20Enhancing%20Few-Shot%20Transfer%20Learning%20with%20Optimized%20Multi-Task%20Prompt%0A%20%20Tuning%20through%20Modular%20Prompt%20Composition%0AAuthor%3A%20Ahmad%20Pouramini%20and%20Hesham%20Faili%0AAbstract%3A%20%20%20In%20recent%20years%2C%20multi-task%20prompt%20tuning%20has%20garnered%20considerable%20attention%0Afor%20its%20inherent%20modularity%20and%20potential%20to%20enhance%20parameter-efficient%0Atransfer%20learning%20across%20diverse%20tasks.%20This%20paper%20aims%20to%20analyze%20and%20improve%0Athe%20performance%20of%20multiple%20tasks%20by%20facilitating%20the%20transfer%20of%20knowledge%0Abetween%20their%20corresponding%20prompts%20in%20a%20multi-task%20setting.%20Our%20proposed%0Aapproach%20decomposes%20the%20prompt%20for%20each%20target%20task%20into%20a%20combination%20of%0Ashared%20prompts%20%28source%20prompts%29%20and%20a%20task-specific%20prompt%20%28private%20prompt%29.%0ADuring%20training%2C%20the%20source%20prompts%20undergo%20fine-tuning%20and%20are%20integrated%20with%0Athe%20private%20prompt%20to%20drive%20the%20target%20prompt%20for%20each%20task.%20We%20present%20and%0Acompare%20multiple%20methods%20for%20combining%20source%20prompts%20to%20construct%20the%20target%0Aprompt%2C%20analyzing%20the%20roles%20of%20both%20source%20and%20private%20prompts%20within%20each%0Amethod.%20We%20investigate%20their%20contributions%20to%20task%20performance%20and%20offer%0Aflexible%2C%20adjustable%20configurations%20based%20on%20these%20insights%20to%20optimize%0Aperformance.%20Our%20empirical%20findings%20clearly%20showcase%20improvements%20in%20accuracy%0Aand%20robustness%20compared%20to%20the%20conventional%20practice%20of%20prompt%20tuning%20and%0Arelated%20works.%20Notably%2C%20our%20results%20substantially%20outperform%20other%20methods%20in%0Athe%20field%20in%20few-shot%20settings%2C%20demonstrating%20superior%20performance%20in%20various%0Atasks%20across%20GLUE%20benchmark%2C%20among%20other%20tasks.%20This%20achievement%20is%20attained%0Awith%20a%20significantly%20reduced%20amount%20of%20training%20data%2C%20making%20our%20method%20a%0Apromising%20one%20for%20few-shot%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Few-Shot%2520Transfer%2520Learning%2520with%2520Optimized%2520Multi-Task%2520Prompt%250A%2520%2520Tuning%2520through%2520Modular%2520Prompt%2520Composition%26entry.906535625%3DAhmad%2520Pouramini%2520and%2520Hesham%2520Faili%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520multi-task%2520prompt%2520tuning%2520has%2520garnered%2520considerable%2520attention%250Afor%2520its%2520inherent%2520modularity%2520and%2520potential%2520to%2520enhance%2520parameter-efficient%250Atransfer%2520learning%2520across%2520diverse%2520tasks.%2520This%2520paper%2520aims%2520to%2520analyze%2520and%2520improve%250Athe%2520performance%2520of%2520multiple%2520tasks%2520by%2520facilitating%2520the%2520transfer%2520of%2520knowledge%250Abetween%2520their%2520corresponding%2520prompts%2520in%2520a%2520multi-task%2520setting.%2520Our%2520proposed%250Aapproach%2520decomposes%2520the%2520prompt%2520for%2520each%2520target%2520task%2520into%2520a%2520combination%2520of%250Ashared%2520prompts%2520%2528source%2520prompts%2529%2520and%2520a%2520task-specific%2520prompt%2520%2528private%2520prompt%2529.%250ADuring%2520training%252C%2520the%2520source%2520prompts%2520undergo%2520fine-tuning%2520and%2520are%2520integrated%2520with%250Athe%2520private%2520prompt%2520to%2520drive%2520the%2520target%2520prompt%2520for%2520each%2520task.%2520We%2520present%2520and%250Acompare%2520multiple%2520methods%2520for%2520combining%2520source%2520prompts%2520to%2520construct%2520the%2520target%250Aprompt%252C%2520analyzing%2520the%2520roles%2520of%2520both%2520source%2520and%2520private%2520prompts%2520within%2520each%250Amethod.%2520We%2520investigate%2520their%2520contributions%2520to%2520task%2520performance%2520and%2520offer%250Aflexible%252C%2520adjustable%2520configurations%2520based%2520on%2520these%2520insights%2520to%2520optimize%250Aperformance.%2520Our%2520empirical%2520findings%2520clearly%2520showcase%2520improvements%2520in%2520accuracy%250Aand%2520robustness%2520compared%2520to%2520the%2520conventional%2520practice%2520of%2520prompt%2520tuning%2520and%250Arelated%2520works.%2520Notably%252C%2520our%2520results%2520substantially%2520outperform%2520other%2520methods%2520in%250Athe%2520field%2520in%2520few-shot%2520settings%252C%2520demonstrating%2520superior%2520performance%2520in%2520various%250Atasks%2520across%2520GLUE%2520benchmark%252C%2520among%2520other%2520tasks.%2520This%2520achievement%2520is%2520attained%250Awith%2520a%2520significantly%2520reduced%2520amount%2520of%2520training%2520data%252C%2520making%2520our%2520method%2520a%250Apromising%2520one%2520for%2520few-shot%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Few-Shot%20Transfer%20Learning%20with%20Optimized%20Multi-Task%20Prompt%0A%20%20Tuning%20through%20Modular%20Prompt%20Composition&entry.906535625=Ahmad%20Pouramini%20and%20Hesham%20Faili&entry.1292438233=%20%20In%20recent%20years%2C%20multi-task%20prompt%20tuning%20has%20garnered%20considerable%20attention%0Afor%20its%20inherent%20modularity%20and%20potential%20to%20enhance%20parameter-efficient%0Atransfer%20learning%20across%20diverse%20tasks.%20This%20paper%20aims%20to%20analyze%20and%20improve%0Athe%20performance%20of%20multiple%20tasks%20by%20facilitating%20the%20transfer%20of%20knowledge%0Abetween%20their%20corresponding%20prompts%20in%20a%20multi-task%20setting.%20Our%20proposed%0Aapproach%20decomposes%20the%20prompt%20for%20each%20target%20task%20into%20a%20combination%20of%0Ashared%20prompts%20%28source%20prompts%29%20and%20a%20task-specific%20prompt%20%28private%20prompt%29.%0ADuring%20training%2C%20the%20source%20prompts%20undergo%20fine-tuning%20and%20are%20integrated%20with%0Athe%20private%20prompt%20to%20drive%20the%20target%20prompt%20for%20each%20task.%20We%20present%20and%0Acompare%20multiple%20methods%20for%20combining%20source%20prompts%20to%20construct%20the%20target%0Aprompt%2C%20analyzing%20the%20roles%20of%20both%20source%20and%20private%20prompts%20within%20each%0Amethod.%20We%20investigate%20their%20contributions%20to%20task%20performance%20and%20offer%0Aflexible%2C%20adjustable%20configurations%20based%20on%20these%20insights%20to%20optimize%0Aperformance.%20Our%20empirical%20findings%20clearly%20showcase%20improvements%20in%20accuracy%0Aand%20robustness%20compared%20to%20the%20conventional%20practice%20of%20prompt%20tuning%20and%0Arelated%20works.%20Notably%2C%20our%20results%20substantially%20outperform%20other%20methods%20in%0Athe%20field%20in%20few-shot%20settings%2C%20demonstrating%20superior%20performance%20in%20various%0Atasks%20across%20GLUE%20benchmark%2C%20among%20other%20tasks.%20This%20achievement%20is%20attained%0Awith%20a%20significantly%20reduced%20amount%20of%20training%20data%2C%20making%20our%20method%20a%0Apromising%20one%20for%20few-shot%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13227v1&entry.124074799=Read"},
{"title": "Object Recognition from Scientific Document based on Compartment\n  Refinement Framework", "author": "Jinghong Li and Wen Gu and Koichi Ota and Shinobu Hasegawa", "abstract": "  With the rapid development of the internet in the past decade, it has become\nincreasingly important to extract valuable information from vast resources\nefficiently, which is crucial for establishing a comprehensive digital\necosystem, particularly in the context of research surveys and comprehension.\nThe foundation of these tasks focuses on accurate extraction and deep mining of\ndata from scientific documents, which are essential for building a robust data\ninfrastructure. However, parsing raw data or extracting data from complex\nscientific documents have been ongoing challenges. Current data extraction\nmethods for scientific documents typically use rule-based (RB) or machine\nlearning (ML) approaches. However, using rule-based methods can incur high\ncoding costs for articles with intricate typesetting. Conversely, relying\nsolely on machine learning methods necessitates annotation work for complex\ncontent types within the scientific document, which can be costly.\nAdditionally, few studies have thoroughly defined and explored the hierarchical\nlayout within scientific documents. The lack of a comprehensive definition of\nthe internal structure and elements of the documents indirectly impacts the\naccuracy of text classification and object recognition tasks. From the\nperspective of analyzing the standard layout and typesetting used in the\nspecified publication, we propose a new document layout analysis framework\ncalled CTBR(Compartment & Text Blocks Refinement). Firstly, we define\nscientific documents into hierarchical divisions: base domain, compartment, and\ntext blocks. Next, we conduct an in-depth exploration and classification of the\nmeanings of text blocks. Finally, we utilize the results of text block\nclassification to implement object recognition within scientific documents\nbased on rule-based compartment segmentation.\n", "link": "http://arxiv.org/abs/2312.09038v4", "date": "2024-08-23", "relevancy": 1.9611, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5094}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Recognition%20from%20Scientific%20Document%20based%20on%20Compartment%0A%20%20Refinement%20Framework&body=Title%3A%20Object%20Recognition%20from%20Scientific%20Document%20based%20on%20Compartment%0A%20%20Refinement%20Framework%0AAuthor%3A%20Jinghong%20Li%20and%20Wen%20Gu%20and%20Koichi%20Ota%20and%20Shinobu%20Hasegawa%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20the%20internet%20in%20the%20past%20decade%2C%20it%20has%20become%0Aincreasingly%20important%20to%20extract%20valuable%20information%20from%20vast%20resources%0Aefficiently%2C%20which%20is%20crucial%20for%20establishing%20a%20comprehensive%20digital%0Aecosystem%2C%20particularly%20in%20the%20context%20of%20research%20surveys%20and%20comprehension.%0AThe%20foundation%20of%20these%20tasks%20focuses%20on%20accurate%20extraction%20and%20deep%20mining%20of%0Adata%20from%20scientific%20documents%2C%20which%20are%20essential%20for%20building%20a%20robust%20data%0Ainfrastructure.%20However%2C%20parsing%20raw%20data%20or%20extracting%20data%20from%20complex%0Ascientific%20documents%20have%20been%20ongoing%20challenges.%20Current%20data%20extraction%0Amethods%20for%20scientific%20documents%20typically%20use%20rule-based%20%28RB%29%20or%20machine%0Alearning%20%28ML%29%20approaches.%20However%2C%20using%20rule-based%20methods%20can%20incur%20high%0Acoding%20costs%20for%20articles%20with%20intricate%20typesetting.%20Conversely%2C%20relying%0Asolely%20on%20machine%20learning%20methods%20necessitates%20annotation%20work%20for%20complex%0Acontent%20types%20within%20the%20scientific%20document%2C%20which%20can%20be%20costly.%0AAdditionally%2C%20few%20studies%20have%20thoroughly%20defined%20and%20explored%20the%20hierarchical%0Alayout%20within%20scientific%20documents.%20The%20lack%20of%20a%20comprehensive%20definition%20of%0Athe%20internal%20structure%20and%20elements%20of%20the%20documents%20indirectly%20impacts%20the%0Aaccuracy%20of%20text%20classification%20and%20object%20recognition%20tasks.%20From%20the%0Aperspective%20of%20analyzing%20the%20standard%20layout%20and%20typesetting%20used%20in%20the%0Aspecified%20publication%2C%20we%20propose%20a%20new%20document%20layout%20analysis%20framework%0Acalled%20CTBR%28Compartment%20%26%20Text%20Blocks%20Refinement%29.%20Firstly%2C%20we%20define%0Ascientific%20documents%20into%20hierarchical%20divisions%3A%20base%20domain%2C%20compartment%2C%20and%0Atext%20blocks.%20Next%2C%20we%20conduct%20an%20in-depth%20exploration%20and%20classification%20of%20the%0Ameanings%20of%20text%20blocks.%20Finally%2C%20we%20utilize%20the%20results%20of%20text%20block%0Aclassification%20to%20implement%20object%20recognition%20within%20scientific%20documents%0Abased%20on%20rule-based%20compartment%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09038v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Recognition%2520from%2520Scientific%2520Document%2520based%2520on%2520Compartment%250A%2520%2520Refinement%2520Framework%26entry.906535625%3DJinghong%2520Li%2520and%2520Wen%2520Gu%2520and%2520Koichi%2520Ota%2520and%2520Shinobu%2520Hasegawa%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520the%2520internet%2520in%2520the%2520past%2520decade%252C%2520it%2520has%2520become%250Aincreasingly%2520important%2520to%2520extract%2520valuable%2520information%2520from%2520vast%2520resources%250Aefficiently%252C%2520which%2520is%2520crucial%2520for%2520establishing%2520a%2520comprehensive%2520digital%250Aecosystem%252C%2520particularly%2520in%2520the%2520context%2520of%2520research%2520surveys%2520and%2520comprehension.%250AThe%2520foundation%2520of%2520these%2520tasks%2520focuses%2520on%2520accurate%2520extraction%2520and%2520deep%2520mining%2520of%250Adata%2520from%2520scientific%2520documents%252C%2520which%2520are%2520essential%2520for%2520building%2520a%2520robust%2520data%250Ainfrastructure.%2520However%252C%2520parsing%2520raw%2520data%2520or%2520extracting%2520data%2520from%2520complex%250Ascientific%2520documents%2520have%2520been%2520ongoing%2520challenges.%2520Current%2520data%2520extraction%250Amethods%2520for%2520scientific%2520documents%2520typically%2520use%2520rule-based%2520%2528RB%2529%2520or%2520machine%250Alearning%2520%2528ML%2529%2520approaches.%2520However%252C%2520using%2520rule-based%2520methods%2520can%2520incur%2520high%250Acoding%2520costs%2520for%2520articles%2520with%2520intricate%2520typesetting.%2520Conversely%252C%2520relying%250Asolely%2520on%2520machine%2520learning%2520methods%2520necessitates%2520annotation%2520work%2520for%2520complex%250Acontent%2520types%2520within%2520the%2520scientific%2520document%252C%2520which%2520can%2520be%2520costly.%250AAdditionally%252C%2520few%2520studies%2520have%2520thoroughly%2520defined%2520and%2520explored%2520the%2520hierarchical%250Alayout%2520within%2520scientific%2520documents.%2520The%2520lack%2520of%2520a%2520comprehensive%2520definition%2520of%250Athe%2520internal%2520structure%2520and%2520elements%2520of%2520the%2520documents%2520indirectly%2520impacts%2520the%250Aaccuracy%2520of%2520text%2520classification%2520and%2520object%2520recognition%2520tasks.%2520From%2520the%250Aperspective%2520of%2520analyzing%2520the%2520standard%2520layout%2520and%2520typesetting%2520used%2520in%2520the%250Aspecified%2520publication%252C%2520we%2520propose%2520a%2520new%2520document%2520layout%2520analysis%2520framework%250Acalled%2520CTBR%2528Compartment%2520%2526%2520Text%2520Blocks%2520Refinement%2529.%2520Firstly%252C%2520we%2520define%250Ascientific%2520documents%2520into%2520hierarchical%2520divisions%253A%2520base%2520domain%252C%2520compartment%252C%2520and%250Atext%2520blocks.%2520Next%252C%2520we%2520conduct%2520an%2520in-depth%2520exploration%2520and%2520classification%2520of%2520the%250Ameanings%2520of%2520text%2520blocks.%2520Finally%252C%2520we%2520utilize%2520the%2520results%2520of%2520text%2520block%250Aclassification%2520to%2520implement%2520object%2520recognition%2520within%2520scientific%2520documents%250Abased%2520on%2520rule-based%2520compartment%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09038v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Recognition%20from%20Scientific%20Document%20based%20on%20Compartment%0A%20%20Refinement%20Framework&entry.906535625=Jinghong%20Li%20and%20Wen%20Gu%20and%20Koichi%20Ota%20and%20Shinobu%20Hasegawa&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20the%20internet%20in%20the%20past%20decade%2C%20it%20has%20become%0Aincreasingly%20important%20to%20extract%20valuable%20information%20from%20vast%20resources%0Aefficiently%2C%20which%20is%20crucial%20for%20establishing%20a%20comprehensive%20digital%0Aecosystem%2C%20particularly%20in%20the%20context%20of%20research%20surveys%20and%20comprehension.%0AThe%20foundation%20of%20these%20tasks%20focuses%20on%20accurate%20extraction%20and%20deep%20mining%20of%0Adata%20from%20scientific%20documents%2C%20which%20are%20essential%20for%20building%20a%20robust%20data%0Ainfrastructure.%20However%2C%20parsing%20raw%20data%20or%20extracting%20data%20from%20complex%0Ascientific%20documents%20have%20been%20ongoing%20challenges.%20Current%20data%20extraction%0Amethods%20for%20scientific%20documents%20typically%20use%20rule-based%20%28RB%29%20or%20machine%0Alearning%20%28ML%29%20approaches.%20However%2C%20using%20rule-based%20methods%20can%20incur%20high%0Acoding%20costs%20for%20articles%20with%20intricate%20typesetting.%20Conversely%2C%20relying%0Asolely%20on%20machine%20learning%20methods%20necessitates%20annotation%20work%20for%20complex%0Acontent%20types%20within%20the%20scientific%20document%2C%20which%20can%20be%20costly.%0AAdditionally%2C%20few%20studies%20have%20thoroughly%20defined%20and%20explored%20the%20hierarchical%0Alayout%20within%20scientific%20documents.%20The%20lack%20of%20a%20comprehensive%20definition%20of%0Athe%20internal%20structure%20and%20elements%20of%20the%20documents%20indirectly%20impacts%20the%0Aaccuracy%20of%20text%20classification%20and%20object%20recognition%20tasks.%20From%20the%0Aperspective%20of%20analyzing%20the%20standard%20layout%20and%20typesetting%20used%20in%20the%0Aspecified%20publication%2C%20we%20propose%20a%20new%20document%20layout%20analysis%20framework%0Acalled%20CTBR%28Compartment%20%26%20Text%20Blocks%20Refinement%29.%20Firstly%2C%20we%20define%0Ascientific%20documents%20into%20hierarchical%20divisions%3A%20base%20domain%2C%20compartment%2C%20and%0Atext%20blocks.%20Next%2C%20we%20conduct%20an%20in-depth%20exploration%20and%20classification%20of%20the%0Ameanings%20of%20text%20blocks.%20Finally%2C%20we%20utilize%20the%20results%20of%20text%20block%0Aclassification%20to%20implement%20object%20recognition%20within%20scientific%20documents%0Abased%20on%20rule-based%20compartment%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09038v4&entry.124074799=Read"},
{"title": "Interpretable breast cancer classification using CNNs on mammographic\n  images", "author": "Ann-Kristin Balve and Peter Hendrix", "abstract": "  Deep learning models have achieved promising results in breast cancer\nclassification, yet their 'black-box' nature raises interpretability concerns.\nThis research addresses the crucial need to gain insights into the\ndecision-making process of convolutional neural networks (CNNs) for mammogram\nclassification, specifically focusing on the underlying reasons for the CNN's\npredictions of breast cancer. For CNNs trained on the Mammographic Image\nAnalysis Society (MIAS) dataset, we compared the post-hoc interpretability\ntechniques LIME, Grad-CAM, and Kernel SHAP in terms of explanatory depth and\ncomputational efficiency. The results of this analysis indicate that Grad-CAM,\nin particular, provides comprehensive insights into the behavior of the CNN,\nrevealing distinctive patterns in normal, benign, and malignant breast tissue.\nWe discuss the implications of the current findings for the use of machine\nlearning models and interpretation techniques in clinical practice.\n", "link": "http://arxiv.org/abs/2408.13154v1", "date": "2024-08-23", "relevancy": 1.9606, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5147}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20breast%20cancer%20classification%20using%20CNNs%20on%20mammographic%0A%20%20images&body=Title%3A%20Interpretable%20breast%20cancer%20classification%20using%20CNNs%20on%20mammographic%0A%20%20images%0AAuthor%3A%20Ann-Kristin%20Balve%20and%20Peter%20Hendrix%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20achieved%20promising%20results%20in%20breast%20cancer%0Aclassification%2C%20yet%20their%20%27black-box%27%20nature%20raises%20interpretability%20concerns.%0AThis%20research%20addresses%20the%20crucial%20need%20to%20gain%20insights%20into%20the%0Adecision-making%20process%20of%20convolutional%20neural%20networks%20%28CNNs%29%20for%20mammogram%0Aclassification%2C%20specifically%20focusing%20on%20the%20underlying%20reasons%20for%20the%20CNN%27s%0Apredictions%20of%20breast%20cancer.%20For%20CNNs%20trained%20on%20the%20Mammographic%20Image%0AAnalysis%20Society%20%28MIAS%29%20dataset%2C%20we%20compared%20the%20post-hoc%20interpretability%0Atechniques%20LIME%2C%20Grad-CAM%2C%20and%20Kernel%20SHAP%20in%20terms%20of%20explanatory%20depth%20and%0Acomputational%20efficiency.%20The%20results%20of%20this%20analysis%20indicate%20that%20Grad-CAM%2C%0Ain%20particular%2C%20provides%20comprehensive%20insights%20into%20the%20behavior%20of%20the%20CNN%2C%0Arevealing%20distinctive%20patterns%20in%20normal%2C%20benign%2C%20and%20malignant%20breast%20tissue.%0AWe%20discuss%20the%20implications%20of%20the%20current%20findings%20for%20the%20use%20of%20machine%0Alearning%20models%20and%20interpretation%20techniques%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520breast%2520cancer%2520classification%2520using%2520CNNs%2520on%2520mammographic%250A%2520%2520images%26entry.906535625%3DAnn-Kristin%2520Balve%2520and%2520Peter%2520Hendrix%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520achieved%2520promising%2520results%2520in%2520breast%2520cancer%250Aclassification%252C%2520yet%2520their%2520%2527black-box%2527%2520nature%2520raises%2520interpretability%2520concerns.%250AThis%2520research%2520addresses%2520the%2520crucial%2520need%2520to%2520gain%2520insights%2520into%2520the%250Adecision-making%2520process%2520of%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520for%2520mammogram%250Aclassification%252C%2520specifically%2520focusing%2520on%2520the%2520underlying%2520reasons%2520for%2520the%2520CNN%2527s%250Apredictions%2520of%2520breast%2520cancer.%2520For%2520CNNs%2520trained%2520on%2520the%2520Mammographic%2520Image%250AAnalysis%2520Society%2520%2528MIAS%2529%2520dataset%252C%2520we%2520compared%2520the%2520post-hoc%2520interpretability%250Atechniques%2520LIME%252C%2520Grad-CAM%252C%2520and%2520Kernel%2520SHAP%2520in%2520terms%2520of%2520explanatory%2520depth%2520and%250Acomputational%2520efficiency.%2520The%2520results%2520of%2520this%2520analysis%2520indicate%2520that%2520Grad-CAM%252C%250Ain%2520particular%252C%2520provides%2520comprehensive%2520insights%2520into%2520the%2520behavior%2520of%2520the%2520CNN%252C%250Arevealing%2520distinctive%2520patterns%2520in%2520normal%252C%2520benign%252C%2520and%2520malignant%2520breast%2520tissue.%250AWe%2520discuss%2520the%2520implications%2520of%2520the%2520current%2520findings%2520for%2520the%2520use%2520of%2520machine%250Alearning%2520models%2520and%2520interpretation%2520techniques%2520in%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20breast%20cancer%20classification%20using%20CNNs%20on%20mammographic%0A%20%20images&entry.906535625=Ann-Kristin%20Balve%20and%20Peter%20Hendrix&entry.1292438233=%20%20Deep%20learning%20models%20have%20achieved%20promising%20results%20in%20breast%20cancer%0Aclassification%2C%20yet%20their%20%27black-box%27%20nature%20raises%20interpretability%20concerns.%0AThis%20research%20addresses%20the%20crucial%20need%20to%20gain%20insights%20into%20the%0Adecision-making%20process%20of%20convolutional%20neural%20networks%20%28CNNs%29%20for%20mammogram%0Aclassification%2C%20specifically%20focusing%20on%20the%20underlying%20reasons%20for%20the%20CNN%27s%0Apredictions%20of%20breast%20cancer.%20For%20CNNs%20trained%20on%20the%20Mammographic%20Image%0AAnalysis%20Society%20%28MIAS%29%20dataset%2C%20we%20compared%20the%20post-hoc%20interpretability%0Atechniques%20LIME%2C%20Grad-CAM%2C%20and%20Kernel%20SHAP%20in%20terms%20of%20explanatory%20depth%20and%0Acomputational%20efficiency.%20The%20results%20of%20this%20analysis%20indicate%20that%20Grad-CAM%2C%0Ain%20particular%2C%20provides%20comprehensive%20insights%20into%20the%20behavior%20of%20the%20CNN%2C%0Arevealing%20distinctive%20patterns%20in%20normal%2C%20benign%2C%20and%20malignant%20breast%20tissue.%0AWe%20discuss%20the%20implications%20of%20the%20current%20findings%20for%20the%20use%20of%20machine%0Alearning%20models%20and%20interpretation%20techniques%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13154v1&entry.124074799=Read"},
{"title": "Find the Assembly Mistakes: Error Segmentation for Industrial\n  Applications", "author": "Dan Lehman and Tim J. Schoonbeek and Shao-Hsuan Hung and Jacek Kustra and Peter H. N. de With and Fons van der Sommen", "abstract": "  Recognizing errors in assembly and maintenance procedures is valuable for\nindustrial applications, since it can increase worker efficiency and prevent\nunplanned down-time. Although assembly state recognition is gaining attention,\nnone of the current works investigate assembly error localization. Therefore,\nwe propose StateDiffNet, which localizes assembly errors based on detecting the\ndifferences between a (correct) intended assembly state and a test image from a\nsimilar viewpoint. StateDiffNet is trained on synthetically generated image\npairs, providing full control over the type of meaningful change that should be\ndetected. The proposed approach is the first to correctly localize assembly\nerrors taken from real ego-centric video data for both states and error types\nthat are never presented during training. Furthermore, the deployment of change\ndetection to this industrial application provides valuable insights and\nconsiderations into the mechanisms of state-of-the-art change detection\nalgorithms. The code and data generation pipeline are publicly available at:\nhttps://timschoonbeek.github.io/error_seg.\n", "link": "http://arxiv.org/abs/2408.12945v1", "date": "2024-08-23", "relevancy": 1.9604, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4997}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4844}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Find%20the%20Assembly%20Mistakes%3A%20Error%20Segmentation%20for%20Industrial%0A%20%20Applications&body=Title%3A%20Find%20the%20Assembly%20Mistakes%3A%20Error%20Segmentation%20for%20Industrial%0A%20%20Applications%0AAuthor%3A%20Dan%20Lehman%20and%20Tim%20J.%20Schoonbeek%20and%20Shao-Hsuan%20Hung%20and%20Jacek%20Kustra%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20%20%20Recognizing%20errors%20in%20assembly%20and%20maintenance%20procedures%20is%20valuable%20for%0Aindustrial%20applications%2C%20since%20it%20can%20increase%20worker%20efficiency%20and%20prevent%0Aunplanned%20down-time.%20Although%20assembly%20state%20recognition%20is%20gaining%20attention%2C%0Anone%20of%20the%20current%20works%20investigate%20assembly%20error%20localization.%20Therefore%2C%0Awe%20propose%20StateDiffNet%2C%20which%20localizes%20assembly%20errors%20based%20on%20detecting%20the%0Adifferences%20between%20a%20%28correct%29%20intended%20assembly%20state%20and%20a%20test%20image%20from%20a%0Asimilar%20viewpoint.%20StateDiffNet%20is%20trained%20on%20synthetically%20generated%20image%0Apairs%2C%20providing%20full%20control%20over%20the%20type%20of%20meaningful%20change%20that%20should%20be%0Adetected.%20The%20proposed%20approach%20is%20the%20first%20to%20correctly%20localize%20assembly%0Aerrors%20taken%20from%20real%20ego-centric%20video%20data%20for%20both%20states%20and%20error%20types%0Athat%20are%20never%20presented%20during%20training.%20Furthermore%2C%20the%20deployment%20of%20change%0Adetection%20to%20this%20industrial%20application%20provides%20valuable%20insights%20and%0Aconsiderations%20into%20the%20mechanisms%20of%20state-of-the-art%20change%20detection%0Aalgorithms.%20The%20code%20and%20data%20generation%20pipeline%20are%20publicly%20available%20at%3A%0Ahttps%3A//timschoonbeek.github.io/error_seg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFind%2520the%2520Assembly%2520Mistakes%253A%2520Error%2520Segmentation%2520for%2520Industrial%250A%2520%2520Applications%26entry.906535625%3DDan%2520Lehman%2520and%2520Tim%2520J.%2520Schoonbeek%2520and%2520Shao-Hsuan%2520Hung%2520and%2520Jacek%2520Kustra%2520and%2520Peter%2520H.%2520N.%2520de%2520With%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3D%2520%2520Recognizing%2520errors%2520in%2520assembly%2520and%2520maintenance%2520procedures%2520is%2520valuable%2520for%250Aindustrial%2520applications%252C%2520since%2520it%2520can%2520increase%2520worker%2520efficiency%2520and%2520prevent%250Aunplanned%2520down-time.%2520Although%2520assembly%2520state%2520recognition%2520is%2520gaining%2520attention%252C%250Anone%2520of%2520the%2520current%2520works%2520investigate%2520assembly%2520error%2520localization.%2520Therefore%252C%250Awe%2520propose%2520StateDiffNet%252C%2520which%2520localizes%2520assembly%2520errors%2520based%2520on%2520detecting%2520the%250Adifferences%2520between%2520a%2520%2528correct%2529%2520intended%2520assembly%2520state%2520and%2520a%2520test%2520image%2520from%2520a%250Asimilar%2520viewpoint.%2520StateDiffNet%2520is%2520trained%2520on%2520synthetically%2520generated%2520image%250Apairs%252C%2520providing%2520full%2520control%2520over%2520the%2520type%2520of%2520meaningful%2520change%2520that%2520should%2520be%250Adetected.%2520The%2520proposed%2520approach%2520is%2520the%2520first%2520to%2520correctly%2520localize%2520assembly%250Aerrors%2520taken%2520from%2520real%2520ego-centric%2520video%2520data%2520for%2520both%2520states%2520and%2520error%2520types%250Athat%2520are%2520never%2520presented%2520during%2520training.%2520Furthermore%252C%2520the%2520deployment%2520of%2520change%250Adetection%2520to%2520this%2520industrial%2520application%2520provides%2520valuable%2520insights%2520and%250Aconsiderations%2520into%2520the%2520mechanisms%2520of%2520state-of-the-art%2520change%2520detection%250Aalgorithms.%2520The%2520code%2520and%2520data%2520generation%2520pipeline%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//timschoonbeek.github.io/error_seg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Find%20the%20Assembly%20Mistakes%3A%20Error%20Segmentation%20for%20Industrial%0A%20%20Applications&entry.906535625=Dan%20Lehman%20and%20Tim%20J.%20Schoonbeek%20and%20Shao-Hsuan%20Hung%20and%20Jacek%20Kustra%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen&entry.1292438233=%20%20Recognizing%20errors%20in%20assembly%20and%20maintenance%20procedures%20is%20valuable%20for%0Aindustrial%20applications%2C%20since%20it%20can%20increase%20worker%20efficiency%20and%20prevent%0Aunplanned%20down-time.%20Although%20assembly%20state%20recognition%20is%20gaining%20attention%2C%0Anone%20of%20the%20current%20works%20investigate%20assembly%20error%20localization.%20Therefore%2C%0Awe%20propose%20StateDiffNet%2C%20which%20localizes%20assembly%20errors%20based%20on%20detecting%20the%0Adifferences%20between%20a%20%28correct%29%20intended%20assembly%20state%20and%20a%20test%20image%20from%20a%0Asimilar%20viewpoint.%20StateDiffNet%20is%20trained%20on%20synthetically%20generated%20image%0Apairs%2C%20providing%20full%20control%20over%20the%20type%20of%20meaningful%20change%20that%20should%20be%0Adetected.%20The%20proposed%20approach%20is%20the%20first%20to%20correctly%20localize%20assembly%0Aerrors%20taken%20from%20real%20ego-centric%20video%20data%20for%20both%20states%20and%20error%20types%0Athat%20are%20never%20presented%20during%20training.%20Furthermore%2C%20the%20deployment%20of%20change%0Adetection%20to%20this%20industrial%20application%20provides%20valuable%20insights%20and%0Aconsiderations%20into%20the%20mechanisms%20of%20state-of-the-art%20change%20detection%0Aalgorithms.%20The%20code%20and%20data%20generation%20pipeline%20are%20publicly%20available%20at%3A%0Ahttps%3A//timschoonbeek.github.io/error_seg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12945v1&entry.124074799=Read"},
{"title": "cc-DRL: a Convex Combined Deep Reinforcement Learning Flight Control\n  Design for a Morphing Quadrotor", "author": "Tao Yang and Huai-Ning Wu and Jun-Wei Wang", "abstract": "  In comparison to common quadrotors, the shape change of morphing quadrotors\nendows it with a more better flight performance but also results in more\ncomplex flight dynamics. Generally, it is extremely difficult or even\nimpossible for morphing quadrotors to establish an accurate mathematical model\ndescribing their complex flight dynamics. To figure out the issue of flight\ncontrol design for morphing quadrotors, this paper resorts to a combination of\nmodel-free control techniques (e.g., deep reinforcement learning, DRL) and\nconvex combination (CC) technique, and proposes a convex-combined-DRL (cc-DRL)\nflight control algorithm for position and attitude of a class of morphing\nquadrotors, where the shape change is realized by the length variation of four\narm rods. In the proposed cc-DRL flight control algorithm, proximal policy\noptimization algorithm that is a model-free DRL algorithm is utilized to\noff-line train the corresponding optimal flight control laws for some selected\nrepresentative arm length modes and hereby a cc-DRL flight control scheme is\nconstructed by the convex combination technique. Finally, simulation results\nare presented to show the effectiveness and merit of the proposed flight\ncontrol algorithm.\n", "link": "http://arxiv.org/abs/2408.13054v1", "date": "2024-08-23", "relevancy": 1.9579, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4871}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20cc-DRL%3A%20a%20Convex%20Combined%20Deep%20Reinforcement%20Learning%20Flight%20Control%0A%20%20Design%20for%20a%20Morphing%20Quadrotor&body=Title%3A%20cc-DRL%3A%20a%20Convex%20Combined%20Deep%20Reinforcement%20Learning%20Flight%20Control%0A%20%20Design%20for%20a%20Morphing%20Quadrotor%0AAuthor%3A%20Tao%20Yang%20and%20Huai-Ning%20Wu%20and%20Jun-Wei%20Wang%0AAbstract%3A%20%20%20In%20comparison%20to%20common%20quadrotors%2C%20the%20shape%20change%20of%20morphing%20quadrotors%0Aendows%20it%20with%20a%20more%20better%20flight%20performance%20but%20also%20results%20in%20more%0Acomplex%20flight%20dynamics.%20Generally%2C%20it%20is%20extremely%20difficult%20or%20even%0Aimpossible%20for%20morphing%20quadrotors%20to%20establish%20an%20accurate%20mathematical%20model%0Adescribing%20their%20complex%20flight%20dynamics.%20To%20figure%20out%20the%20issue%20of%20flight%0Acontrol%20design%20for%20morphing%20quadrotors%2C%20this%20paper%20resorts%20to%20a%20combination%20of%0Amodel-free%20control%20techniques%20%28e.g.%2C%20deep%20reinforcement%20learning%2C%20DRL%29%20and%0Aconvex%20combination%20%28CC%29%20technique%2C%20and%20proposes%20a%20convex-combined-DRL%20%28cc-DRL%29%0Aflight%20control%20algorithm%20for%20position%20and%20attitude%20of%20a%20class%20of%20morphing%0Aquadrotors%2C%20where%20the%20shape%20change%20is%20realized%20by%20the%20length%20variation%20of%20four%0Aarm%20rods.%20In%20the%20proposed%20cc-DRL%20flight%20control%20algorithm%2C%20proximal%20policy%0Aoptimization%20algorithm%20that%20is%20a%20model-free%20DRL%20algorithm%20is%20utilized%20to%0Aoff-line%20train%20the%20corresponding%20optimal%20flight%20control%20laws%20for%20some%20selected%0Arepresentative%20arm%20length%20modes%20and%20hereby%20a%20cc-DRL%20flight%20control%20scheme%20is%0Aconstructed%20by%20the%20convex%20combination%20technique.%20Finally%2C%20simulation%20results%0Aare%20presented%20to%20show%20the%20effectiveness%20and%20merit%20of%20the%20proposed%20flight%0Acontrol%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dcc-DRL%253A%2520a%2520Convex%2520Combined%2520Deep%2520Reinforcement%2520Learning%2520Flight%2520Control%250A%2520%2520Design%2520for%2520a%2520Morphing%2520Quadrotor%26entry.906535625%3DTao%2520Yang%2520and%2520Huai-Ning%2520Wu%2520and%2520Jun-Wei%2520Wang%26entry.1292438233%3D%2520%2520In%2520comparison%2520to%2520common%2520quadrotors%252C%2520the%2520shape%2520change%2520of%2520morphing%2520quadrotors%250Aendows%2520it%2520with%2520a%2520more%2520better%2520flight%2520performance%2520but%2520also%2520results%2520in%2520more%250Acomplex%2520flight%2520dynamics.%2520Generally%252C%2520it%2520is%2520extremely%2520difficult%2520or%2520even%250Aimpossible%2520for%2520morphing%2520quadrotors%2520to%2520establish%2520an%2520accurate%2520mathematical%2520model%250Adescribing%2520their%2520complex%2520flight%2520dynamics.%2520To%2520figure%2520out%2520the%2520issue%2520of%2520flight%250Acontrol%2520design%2520for%2520morphing%2520quadrotors%252C%2520this%2520paper%2520resorts%2520to%2520a%2520combination%2520of%250Amodel-free%2520control%2520techniques%2520%2528e.g.%252C%2520deep%2520reinforcement%2520learning%252C%2520DRL%2529%2520and%250Aconvex%2520combination%2520%2528CC%2529%2520technique%252C%2520and%2520proposes%2520a%2520convex-combined-DRL%2520%2528cc-DRL%2529%250Aflight%2520control%2520algorithm%2520for%2520position%2520and%2520attitude%2520of%2520a%2520class%2520of%2520morphing%250Aquadrotors%252C%2520where%2520the%2520shape%2520change%2520is%2520realized%2520by%2520the%2520length%2520variation%2520of%2520four%250Aarm%2520rods.%2520In%2520the%2520proposed%2520cc-DRL%2520flight%2520control%2520algorithm%252C%2520proximal%2520policy%250Aoptimization%2520algorithm%2520that%2520is%2520a%2520model-free%2520DRL%2520algorithm%2520is%2520utilized%2520to%250Aoff-line%2520train%2520the%2520corresponding%2520optimal%2520flight%2520control%2520laws%2520for%2520some%2520selected%250Arepresentative%2520arm%2520length%2520modes%2520and%2520hereby%2520a%2520cc-DRL%2520flight%2520control%2520scheme%2520is%250Aconstructed%2520by%2520the%2520convex%2520combination%2520technique.%2520Finally%252C%2520simulation%2520results%250Aare%2520presented%2520to%2520show%2520the%2520effectiveness%2520and%2520merit%2520of%2520the%2520proposed%2520flight%250Acontrol%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=cc-DRL%3A%20a%20Convex%20Combined%20Deep%20Reinforcement%20Learning%20Flight%20Control%0A%20%20Design%20for%20a%20Morphing%20Quadrotor&entry.906535625=Tao%20Yang%20and%20Huai-Ning%20Wu%20and%20Jun-Wei%20Wang&entry.1292438233=%20%20In%20comparison%20to%20common%20quadrotors%2C%20the%20shape%20change%20of%20morphing%20quadrotors%0Aendows%20it%20with%20a%20more%20better%20flight%20performance%20but%20also%20results%20in%20more%0Acomplex%20flight%20dynamics.%20Generally%2C%20it%20is%20extremely%20difficult%20or%20even%0Aimpossible%20for%20morphing%20quadrotors%20to%20establish%20an%20accurate%20mathematical%20model%0Adescribing%20their%20complex%20flight%20dynamics.%20To%20figure%20out%20the%20issue%20of%20flight%0Acontrol%20design%20for%20morphing%20quadrotors%2C%20this%20paper%20resorts%20to%20a%20combination%20of%0Amodel-free%20control%20techniques%20%28e.g.%2C%20deep%20reinforcement%20learning%2C%20DRL%29%20and%0Aconvex%20combination%20%28CC%29%20technique%2C%20and%20proposes%20a%20convex-combined-DRL%20%28cc-DRL%29%0Aflight%20control%20algorithm%20for%20position%20and%20attitude%20of%20a%20class%20of%20morphing%0Aquadrotors%2C%20where%20the%20shape%20change%20is%20realized%20by%20the%20length%20variation%20of%20four%0Aarm%20rods.%20In%20the%20proposed%20cc-DRL%20flight%20control%20algorithm%2C%20proximal%20policy%0Aoptimization%20algorithm%20that%20is%20a%20model-free%20DRL%20algorithm%20is%20utilized%20to%0Aoff-line%20train%20the%20corresponding%20optimal%20flight%20control%20laws%20for%20some%20selected%0Arepresentative%20arm%20length%20modes%20and%20hereby%20a%20cc-DRL%20flight%20control%20scheme%20is%0Aconstructed%20by%20the%20convex%20combination%20technique.%20Finally%2C%20simulation%20results%0Aare%20presented%20to%20show%20the%20effectiveness%20and%20merit%20of%20the%20proposed%20flight%0Acontrol%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13054v1&entry.124074799=Read"},
{"title": "Classifier-Free Guidance is a Predictor-Corrector", "author": "Arwen Bradley and Preetum Nakkiran", "abstract": "  We investigate the theoretical foundations of classifier-free guidance (CFG).\nCFG is the dominant method of conditional sampling for text-to-image diffusion\nmodels, yet unlike other aspects of diffusion, it remains on shaky theoretical\nfooting. In this paper, we disprove common misconceptions, by showing that CFG\ninteracts differently with DDPM (Ho et al., 2020) and DDIM (Song et al., 2021),\nand neither sampler with CFG generates the gamma-powered distribution\n$p(x|c)^\\gamma p(x)^{1-\\gamma}$. Then, we clarify the behavior of CFG by\nshowing that it is a kind of predictor-corrector method (Song et al., 2020)\nthat alternates between denoising and sharpening, which we call\npredictor-corrector guidance (PCG). We prove that in the SDE limit, CFG is\nactually equivalent to combining a DDIM predictor for the conditional\ndistribution together with a Langevin dynamics corrector for a gamma-powered\ndistribution (with a carefully chosen gamma). Our work thus provides a lens to\ntheoretically understand CFG by embedding it in a broader design space of\nprincipled sampling methods.\n", "link": "http://arxiv.org/abs/2408.09000v2", "date": "2024-08-23", "relevancy": 1.9496, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.491}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4905}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classifier-Free%20Guidance%20is%20a%20Predictor-Corrector&body=Title%3A%20Classifier-Free%20Guidance%20is%20a%20Predictor-Corrector%0AAuthor%3A%20Arwen%20Bradley%20and%20Preetum%20Nakkiran%0AAbstract%3A%20%20%20We%20investigate%20the%20theoretical%20foundations%20of%20classifier-free%20guidance%20%28CFG%29.%0ACFG%20is%20the%20dominant%20method%20of%20conditional%20sampling%20for%20text-to-image%20diffusion%0Amodels%2C%20yet%20unlike%20other%20aspects%20of%20diffusion%2C%20it%20remains%20on%20shaky%20theoretical%0Afooting.%20In%20this%20paper%2C%20we%20disprove%20common%20misconceptions%2C%20by%20showing%20that%20CFG%0Ainteracts%20differently%20with%20DDPM%20%28Ho%20et%20al.%2C%202020%29%20and%20DDIM%20%28Song%20et%20al.%2C%202021%29%2C%0Aand%20neither%20sampler%20with%20CFG%20generates%20the%20gamma-powered%20distribution%0A%24p%28x%7Cc%29%5E%5Cgamma%20p%28x%29%5E%7B1-%5Cgamma%7D%24.%20Then%2C%20we%20clarify%20the%20behavior%20of%20CFG%20by%0Ashowing%20that%20it%20is%20a%20kind%20of%20predictor-corrector%20method%20%28Song%20et%20al.%2C%202020%29%0Athat%20alternates%20between%20denoising%20and%20sharpening%2C%20which%20we%20call%0Apredictor-corrector%20guidance%20%28PCG%29.%20We%20prove%20that%20in%20the%20SDE%20limit%2C%20CFG%20is%0Aactually%20equivalent%20to%20combining%20a%20DDIM%20predictor%20for%20the%20conditional%0Adistribution%20together%20with%20a%20Langevin%20dynamics%20corrector%20for%20a%20gamma-powered%0Adistribution%20%28with%20a%20carefully%20chosen%20gamma%29.%20Our%20work%20thus%20provides%20a%20lens%20to%0Atheoretically%20understand%20CFG%20by%20embedding%20it%20in%20a%20broader%20design%20space%20of%0Aprincipled%20sampling%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09000v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassifier-Free%2520Guidance%2520is%2520a%2520Predictor-Corrector%26entry.906535625%3DArwen%2520Bradley%2520and%2520Preetum%2520Nakkiran%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520theoretical%2520foundations%2520of%2520classifier-free%2520guidance%2520%2528CFG%2529.%250ACFG%2520is%2520the%2520dominant%2520method%2520of%2520conditional%2520sampling%2520for%2520text-to-image%2520diffusion%250Amodels%252C%2520yet%2520unlike%2520other%2520aspects%2520of%2520diffusion%252C%2520it%2520remains%2520on%2520shaky%2520theoretical%250Afooting.%2520In%2520this%2520paper%252C%2520we%2520disprove%2520common%2520misconceptions%252C%2520by%2520showing%2520that%2520CFG%250Ainteracts%2520differently%2520with%2520DDPM%2520%2528Ho%2520et%2520al.%252C%25202020%2529%2520and%2520DDIM%2520%2528Song%2520et%2520al.%252C%25202021%2529%252C%250Aand%2520neither%2520sampler%2520with%2520CFG%2520generates%2520the%2520gamma-powered%2520distribution%250A%2524p%2528x%257Cc%2529%255E%255Cgamma%2520p%2528x%2529%255E%257B1-%255Cgamma%257D%2524.%2520Then%252C%2520we%2520clarify%2520the%2520behavior%2520of%2520CFG%2520by%250Ashowing%2520that%2520it%2520is%2520a%2520kind%2520of%2520predictor-corrector%2520method%2520%2528Song%2520et%2520al.%252C%25202020%2529%250Athat%2520alternates%2520between%2520denoising%2520and%2520sharpening%252C%2520which%2520we%2520call%250Apredictor-corrector%2520guidance%2520%2528PCG%2529.%2520We%2520prove%2520that%2520in%2520the%2520SDE%2520limit%252C%2520CFG%2520is%250Aactually%2520equivalent%2520to%2520combining%2520a%2520DDIM%2520predictor%2520for%2520the%2520conditional%250Adistribution%2520together%2520with%2520a%2520Langevin%2520dynamics%2520corrector%2520for%2520a%2520gamma-powered%250Adistribution%2520%2528with%2520a%2520carefully%2520chosen%2520gamma%2529.%2520Our%2520work%2520thus%2520provides%2520a%2520lens%2520to%250Atheoretically%2520understand%2520CFG%2520by%2520embedding%2520it%2520in%2520a%2520broader%2520design%2520space%2520of%250Aprincipled%2520sampling%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09000v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classifier-Free%20Guidance%20is%20a%20Predictor-Corrector&entry.906535625=Arwen%20Bradley%20and%20Preetum%20Nakkiran&entry.1292438233=%20%20We%20investigate%20the%20theoretical%20foundations%20of%20classifier-free%20guidance%20%28CFG%29.%0ACFG%20is%20the%20dominant%20method%20of%20conditional%20sampling%20for%20text-to-image%20diffusion%0Amodels%2C%20yet%20unlike%20other%20aspects%20of%20diffusion%2C%20it%20remains%20on%20shaky%20theoretical%0Afooting.%20In%20this%20paper%2C%20we%20disprove%20common%20misconceptions%2C%20by%20showing%20that%20CFG%0Ainteracts%20differently%20with%20DDPM%20%28Ho%20et%20al.%2C%202020%29%20and%20DDIM%20%28Song%20et%20al.%2C%202021%29%2C%0Aand%20neither%20sampler%20with%20CFG%20generates%20the%20gamma-powered%20distribution%0A%24p%28x%7Cc%29%5E%5Cgamma%20p%28x%29%5E%7B1-%5Cgamma%7D%24.%20Then%2C%20we%20clarify%20the%20behavior%20of%20CFG%20by%0Ashowing%20that%20it%20is%20a%20kind%20of%20predictor-corrector%20method%20%28Song%20et%20al.%2C%202020%29%0Athat%20alternates%20between%20denoising%20and%20sharpening%2C%20which%20we%20call%0Apredictor-corrector%20guidance%20%28PCG%29.%20We%20prove%20that%20in%20the%20SDE%20limit%2C%20CFG%20is%0Aactually%20equivalent%20to%20combining%20a%20DDIM%20predictor%20for%20the%20conditional%0Adistribution%20together%20with%20a%20Langevin%20dynamics%20corrector%20for%20a%20gamma-powered%0Adistribution%20%28with%20a%20carefully%20chosen%20gamma%29.%20Our%20work%20thus%20provides%20a%20lens%20to%0Atheoretically%20understand%20CFG%20by%20embedding%20it%20in%20a%20broader%20design%20space%20of%0Aprincipled%20sampling%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09000v2&entry.124074799=Read"},
{"title": "Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese", "author": "Khang T. Doan and Bao G. Huynh and Dung T. Hoang and Thuc D. Pham and Nhat H. Pham and Quan T. M. Nguyen and Bang Q. Vo and Suong N. Hoang", "abstract": "  In this report, we introduce Vintern-1B, a reliable 1-billion-parameters\nmultimodal large language model (MLLM) for Vietnamese language tasks. By\nintegrating the Qwen2-0.5B-Instruct language model with the\nInternViT-300M-448px visual model, Vintern-1B is optimized for a range of\napplications, including optical character recognition (OCR), document\nextraction, and general question-answering in Vietnamese context. The model is\nfine-tuned on an extensive dataset of over 3 million image-question-answer\npairs, achieving robust performance and reliable results across multiple\nVietnamese language benchmarks like OpenViVQA and ViTextVQA. Vintern-1B is\nsmall enough to fit into various on-device applications easily. Additionally,\nwe have open-sourced several Vietnamese vision question answering (VQA)\ndatasets for text and diagrams, created with Gemini 1.5 Flash. Our models are\navailable at: https://huggingface.co/5CD-AI/Vintern-1B-v2.\n", "link": "http://arxiv.org/abs/2408.12480v2", "date": "2024-08-23", "relevancy": 1.9436, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4932}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4863}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vintern-1B%3A%20An%20Efficient%20Multimodal%20Large%20Language%20Model%20for%20Vietnamese&body=Title%3A%20Vintern-1B%3A%20An%20Efficient%20Multimodal%20Large%20Language%20Model%20for%20Vietnamese%0AAuthor%3A%20Khang%20T.%20Doan%20and%20Bao%20G.%20Huynh%20and%20Dung%20T.%20Hoang%20and%20Thuc%20D.%20Pham%20and%20Nhat%20H.%20Pham%20and%20Quan%20T.%20M.%20Nguyen%20and%20Bang%20Q.%20Vo%20and%20Suong%20N.%20Hoang%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20introduce%20Vintern-1B%2C%20a%20reliable%201-billion-parameters%0Amultimodal%20large%20language%20model%20%28MLLM%29%20for%20Vietnamese%20language%20tasks.%20By%0Aintegrating%20the%20Qwen2-0.5B-Instruct%20language%20model%20with%20the%0AInternViT-300M-448px%20visual%20model%2C%20Vintern-1B%20is%20optimized%20for%20a%20range%20of%0Aapplications%2C%20including%20optical%20character%20recognition%20%28OCR%29%2C%20document%0Aextraction%2C%20and%20general%20question-answering%20in%20Vietnamese%20context.%20The%20model%20is%0Afine-tuned%20on%20an%20extensive%20dataset%20of%20over%203%20million%20image-question-answer%0Apairs%2C%20achieving%20robust%20performance%20and%20reliable%20results%20across%20multiple%0AVietnamese%20language%20benchmarks%20like%20OpenViVQA%20and%20ViTextVQA.%20Vintern-1B%20is%0Asmall%20enough%20to%20fit%20into%20various%20on-device%20applications%20easily.%20Additionally%2C%0Awe%20have%20open-sourced%20several%20Vietnamese%20vision%20question%20answering%20%28VQA%29%0Adatasets%20for%20text%20and%20diagrams%2C%20created%20with%20Gemini%201.5%20Flash.%20Our%20models%20are%0Aavailable%20at%3A%20https%3A//huggingface.co/5CD-AI/Vintern-1B-v2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVintern-1B%253A%2520An%2520Efficient%2520Multimodal%2520Large%2520Language%2520Model%2520for%2520Vietnamese%26entry.906535625%3DKhang%2520T.%2520Doan%2520and%2520Bao%2520G.%2520Huynh%2520and%2520Dung%2520T.%2520Hoang%2520and%2520Thuc%2520D.%2520Pham%2520and%2520Nhat%2520H.%2520Pham%2520and%2520Quan%2520T.%2520M.%2520Nguyen%2520and%2520Bang%2520Q.%2520Vo%2520and%2520Suong%2520N.%2520Hoang%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520introduce%2520Vintern-1B%252C%2520a%2520reliable%25201-billion-parameters%250Amultimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520for%2520Vietnamese%2520language%2520tasks.%2520By%250Aintegrating%2520the%2520Qwen2-0.5B-Instruct%2520language%2520model%2520with%2520the%250AInternViT-300M-448px%2520visual%2520model%252C%2520Vintern-1B%2520is%2520optimized%2520for%2520a%2520range%2520of%250Aapplications%252C%2520including%2520optical%2520character%2520recognition%2520%2528OCR%2529%252C%2520document%250Aextraction%252C%2520and%2520general%2520question-answering%2520in%2520Vietnamese%2520context.%2520The%2520model%2520is%250Afine-tuned%2520on%2520an%2520extensive%2520dataset%2520of%2520over%25203%2520million%2520image-question-answer%250Apairs%252C%2520achieving%2520robust%2520performance%2520and%2520reliable%2520results%2520across%2520multiple%250AVietnamese%2520language%2520benchmarks%2520like%2520OpenViVQA%2520and%2520ViTextVQA.%2520Vintern-1B%2520is%250Asmall%2520enough%2520to%2520fit%2520into%2520various%2520on-device%2520applications%2520easily.%2520Additionally%252C%250Awe%2520have%2520open-sourced%2520several%2520Vietnamese%2520vision%2520question%2520answering%2520%2528VQA%2529%250Adatasets%2520for%2520text%2520and%2520diagrams%252C%2520created%2520with%2520Gemini%25201.5%2520Flash.%2520Our%2520models%2520are%250Aavailable%2520at%253A%2520https%253A//huggingface.co/5CD-AI/Vintern-1B-v2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vintern-1B%3A%20An%20Efficient%20Multimodal%20Large%20Language%20Model%20for%20Vietnamese&entry.906535625=Khang%20T.%20Doan%20and%20Bao%20G.%20Huynh%20and%20Dung%20T.%20Hoang%20and%20Thuc%20D.%20Pham%20and%20Nhat%20H.%20Pham%20and%20Quan%20T.%20M.%20Nguyen%20and%20Bang%20Q.%20Vo%20and%20Suong%20N.%20Hoang&entry.1292438233=%20%20In%20this%20report%2C%20we%20introduce%20Vintern-1B%2C%20a%20reliable%201-billion-parameters%0Amultimodal%20large%20language%20model%20%28MLLM%29%20for%20Vietnamese%20language%20tasks.%20By%0Aintegrating%20the%20Qwen2-0.5B-Instruct%20language%20model%20with%20the%0AInternViT-300M-448px%20visual%20model%2C%20Vintern-1B%20is%20optimized%20for%20a%20range%20of%0Aapplications%2C%20including%20optical%20character%20recognition%20%28OCR%29%2C%20document%0Aextraction%2C%20and%20general%20question-answering%20in%20Vietnamese%20context.%20The%20model%20is%0Afine-tuned%20on%20an%20extensive%20dataset%20of%20over%203%20million%20image-question-answer%0Apairs%2C%20achieving%20robust%20performance%20and%20reliable%20results%20across%20multiple%0AVietnamese%20language%20benchmarks%20like%20OpenViVQA%20and%20ViTextVQA.%20Vintern-1B%20is%0Asmall%20enough%20to%20fit%20into%20various%20on-device%20applications%20easily.%20Additionally%2C%0Awe%20have%20open-sourced%20several%20Vietnamese%20vision%20question%20answering%20%28VQA%29%0Adatasets%20for%20text%20and%20diagrams%2C%20created%20with%20Gemini%201.5%20Flash.%20Our%20models%20are%0Aavailable%20at%3A%20https%3A//huggingface.co/5CD-AI/Vintern-1B-v2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12480v2&entry.124074799=Read"},
{"title": "BoostTrack++: using tracklet information to detect more objects in\n  multiple object tracking", "author": "Vuka\u0161in Stanojevi\u0107 and Branimir Todorovi\u0107", "abstract": "  Multiple object tracking (MOT) depends heavily on selection of true positive\ndetected bounding boxes. However, this aspect of the problem is mostly\noverlooked or mitigated by employing two-stage association and utilizing low\nconfidence detections in the second stage. Recently proposed BoostTrack\nattempts to avoid the drawbacks of multiple stage association approach and use\nlow-confidence detections by applying detection confidence boosting. In this\npaper, we identify the limitations of the confidence boost used in BoostTrack\nand propose a method to improve its performance. To construct a richer\nsimilarity measure and enable a better selection of true positive detections,\nwe propose to use a combination of shape, Mahalanobis distance and novel soft\nBIoU similarity. We propose a soft detection confidence boost technique which\ncalculates new confidence scores based on the similarity measure and the\nprevious confidence scores, and we introduce varying similarity threshold to\naccount for lower similarity measure between detections and tracklets which are\nnot regularly updated. The proposed additions are mutually independent and can\nbe used in any MOT algorithm.\n  Combined with the BoostTrack+ baseline, our method achieves near state of the\nart results on the MOT17 dataset and new state of the art HOTA and IDF1 scores\non the MOT20 dataset.\n  The source code is available at:\nhttps://github.com/vukasin-stanojevic/BoostTrack .\n", "link": "http://arxiv.org/abs/2408.13003v1", "date": "2024-08-23", "relevancy": 1.9342, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5108}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4784}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoostTrack%2B%2B%3A%20using%20tracklet%20information%20to%20detect%20more%20objects%20in%0A%20%20multiple%20object%20tracking&body=Title%3A%20BoostTrack%2B%2B%3A%20using%20tracklet%20information%20to%20detect%20more%20objects%20in%0A%20%20multiple%20object%20tracking%0AAuthor%3A%20Vuka%C5%A1in%20Stanojevi%C4%87%20and%20Branimir%20Todorovi%C4%87%0AAbstract%3A%20%20%20Multiple%20object%20tracking%20%28MOT%29%20depends%20heavily%20on%20selection%20of%20true%20positive%0Adetected%20bounding%20boxes.%20However%2C%20this%20aspect%20of%20the%20problem%20is%20mostly%0Aoverlooked%20or%20mitigated%20by%20employing%20two-stage%20association%20and%20utilizing%20low%0Aconfidence%20detections%20in%20the%20second%20stage.%20Recently%20proposed%20BoostTrack%0Aattempts%20to%20avoid%20the%20drawbacks%20of%20multiple%20stage%20association%20approach%20and%20use%0Alow-confidence%20detections%20by%20applying%20detection%20confidence%20boosting.%20In%20this%0Apaper%2C%20we%20identify%20the%20limitations%20of%20the%20confidence%20boost%20used%20in%20BoostTrack%0Aand%20propose%20a%20method%20to%20improve%20its%20performance.%20To%20construct%20a%20richer%0Asimilarity%20measure%20and%20enable%20a%20better%20selection%20of%20true%20positive%20detections%2C%0Awe%20propose%20to%20use%20a%20combination%20of%20shape%2C%20Mahalanobis%20distance%20and%20novel%20soft%0ABIoU%20similarity.%20We%20propose%20a%20soft%20detection%20confidence%20boost%20technique%20which%0Acalculates%20new%20confidence%20scores%20based%20on%20the%20similarity%20measure%20and%20the%0Aprevious%20confidence%20scores%2C%20and%20we%20introduce%20varying%20similarity%20threshold%20to%0Aaccount%20for%20lower%20similarity%20measure%20between%20detections%20and%20tracklets%20which%20are%0Anot%20regularly%20updated.%20The%20proposed%20additions%20are%20mutually%20independent%20and%20can%0Abe%20used%20in%20any%20MOT%20algorithm.%0A%20%20Combined%20with%20the%20BoostTrack%2B%20baseline%2C%20our%20method%20achieves%20near%20state%20of%20the%0Aart%20results%20on%20the%20MOT17%20dataset%20and%20new%20state%20of%20the%20art%20HOTA%20and%20IDF1%20scores%0Aon%20the%20MOT20%20dataset.%0A%20%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/vukasin-stanojevic/BoostTrack%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoostTrack%252B%252B%253A%2520using%2520tracklet%2520information%2520to%2520detect%2520more%2520objects%2520in%250A%2520%2520multiple%2520object%2520tracking%26entry.906535625%3DVuka%25C5%25A1in%2520Stanojevi%25C4%2587%2520and%2520Branimir%2520Todorovi%25C4%2587%26entry.1292438233%3D%2520%2520Multiple%2520object%2520tracking%2520%2528MOT%2529%2520depends%2520heavily%2520on%2520selection%2520of%2520true%2520positive%250Adetected%2520bounding%2520boxes.%2520However%252C%2520this%2520aspect%2520of%2520the%2520problem%2520is%2520mostly%250Aoverlooked%2520or%2520mitigated%2520by%2520employing%2520two-stage%2520association%2520and%2520utilizing%2520low%250Aconfidence%2520detections%2520in%2520the%2520second%2520stage.%2520Recently%2520proposed%2520BoostTrack%250Aattempts%2520to%2520avoid%2520the%2520drawbacks%2520of%2520multiple%2520stage%2520association%2520approach%2520and%2520use%250Alow-confidence%2520detections%2520by%2520applying%2520detection%2520confidence%2520boosting.%2520In%2520this%250Apaper%252C%2520we%2520identify%2520the%2520limitations%2520of%2520the%2520confidence%2520boost%2520used%2520in%2520BoostTrack%250Aand%2520propose%2520a%2520method%2520to%2520improve%2520its%2520performance.%2520To%2520construct%2520a%2520richer%250Asimilarity%2520measure%2520and%2520enable%2520a%2520better%2520selection%2520of%2520true%2520positive%2520detections%252C%250Awe%2520propose%2520to%2520use%2520a%2520combination%2520of%2520shape%252C%2520Mahalanobis%2520distance%2520and%2520novel%2520soft%250ABIoU%2520similarity.%2520We%2520propose%2520a%2520soft%2520detection%2520confidence%2520boost%2520technique%2520which%250Acalculates%2520new%2520confidence%2520scores%2520based%2520on%2520the%2520similarity%2520measure%2520and%2520the%250Aprevious%2520confidence%2520scores%252C%2520and%2520we%2520introduce%2520varying%2520similarity%2520threshold%2520to%250Aaccount%2520for%2520lower%2520similarity%2520measure%2520between%2520detections%2520and%2520tracklets%2520which%2520are%250Anot%2520regularly%2520updated.%2520The%2520proposed%2520additions%2520are%2520mutually%2520independent%2520and%2520can%250Abe%2520used%2520in%2520any%2520MOT%2520algorithm.%250A%2520%2520Combined%2520with%2520the%2520BoostTrack%252B%2520baseline%252C%2520our%2520method%2520achieves%2520near%2520state%2520of%2520the%250Aart%2520results%2520on%2520the%2520MOT17%2520dataset%2520and%2520new%2520state%2520of%2520the%2520art%2520HOTA%2520and%2520IDF1%2520scores%250Aon%2520the%2520MOT20%2520dataset.%250A%2520%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/vukasin-stanojevic/BoostTrack%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoostTrack%2B%2B%3A%20using%20tracklet%20information%20to%20detect%20more%20objects%20in%0A%20%20multiple%20object%20tracking&entry.906535625=Vuka%C5%A1in%20Stanojevi%C4%87%20and%20Branimir%20Todorovi%C4%87&entry.1292438233=%20%20Multiple%20object%20tracking%20%28MOT%29%20depends%20heavily%20on%20selection%20of%20true%20positive%0Adetected%20bounding%20boxes.%20However%2C%20this%20aspect%20of%20the%20problem%20is%20mostly%0Aoverlooked%20or%20mitigated%20by%20employing%20two-stage%20association%20and%20utilizing%20low%0Aconfidence%20detections%20in%20the%20second%20stage.%20Recently%20proposed%20BoostTrack%0Aattempts%20to%20avoid%20the%20drawbacks%20of%20multiple%20stage%20association%20approach%20and%20use%0Alow-confidence%20detections%20by%20applying%20detection%20confidence%20boosting.%20In%20this%0Apaper%2C%20we%20identify%20the%20limitations%20of%20the%20confidence%20boost%20used%20in%20BoostTrack%0Aand%20propose%20a%20method%20to%20improve%20its%20performance.%20To%20construct%20a%20richer%0Asimilarity%20measure%20and%20enable%20a%20better%20selection%20of%20true%20positive%20detections%2C%0Awe%20propose%20to%20use%20a%20combination%20of%20shape%2C%20Mahalanobis%20distance%20and%20novel%20soft%0ABIoU%20similarity.%20We%20propose%20a%20soft%20detection%20confidence%20boost%20technique%20which%0Acalculates%20new%20confidence%20scores%20based%20on%20the%20similarity%20measure%20and%20the%0Aprevious%20confidence%20scores%2C%20and%20we%20introduce%20varying%20similarity%20threshold%20to%0Aaccount%20for%20lower%20similarity%20measure%20between%20detections%20and%20tracklets%20which%20are%0Anot%20regularly%20updated.%20The%20proposed%20additions%20are%20mutually%20independent%20and%20can%0Abe%20used%20in%20any%20MOT%20algorithm.%0A%20%20Combined%20with%20the%20BoostTrack%2B%20baseline%2C%20our%20method%20achieves%20near%20state%20of%20the%0Aart%20results%20on%20the%20MOT17%20dataset%20and%20new%20state%20of%20the%20art%20HOTA%20and%20IDF1%20scores%0Aon%20the%20MOT20%20dataset.%0A%20%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/vukasin-stanojevic/BoostTrack%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13003v1&entry.124074799=Read"},
{"title": "End-to-end Surface Optimization for Light Control", "author": "Yuou Sun and Bailin Deng and Juyong Zhang", "abstract": "  Designing a freeform surface to reflect or refract light to achieve a target\ndistribution is a challenging inverse problem. In this paper, we propose an\nend-to-end optimization strategy for an optical surface mesh. Our formulation\nleverages a novel differentiable rendering model, and is directly driven by the\ndifference between the resulting light distribution and the target\ndistribution. We also enforce geometric constraints related to fabrication\nrequirements, to facilitate CNC milling and polishing of the designed surface.\nTo address the issue of local minima, we formulate a face-based optimal\ntransport problem between the current mesh and the target distribution, which\nmakes effective large changes to the surface shape. The combination of our\noptimal transport update and rendering-guided optimization produces an optical\nsurface design with a resulting image closely resembling the target, while the\nfabrication constraints in our optimization help to ensure consistency between\nthe rendering model and the final physical results. The effectiveness of our\nalgorithm is demonstrated on a variety of target images using both simulated\nrendering and physical prototypes.\n", "link": "http://arxiv.org/abs/2408.13117v1", "date": "2024-08-23", "relevancy": 1.9306, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5003}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-end%20Surface%20Optimization%20for%20Light%20Control&body=Title%3A%20End-to-end%20Surface%20Optimization%20for%20Light%20Control%0AAuthor%3A%20Yuou%20Sun%20and%20Bailin%20Deng%20and%20Juyong%20Zhang%0AAbstract%3A%20%20%20Designing%20a%20freeform%20surface%20to%20reflect%20or%20refract%20light%20to%20achieve%20a%20target%0Adistribution%20is%20a%20challenging%20inverse%20problem.%20In%20this%20paper%2C%20we%20propose%20an%0Aend-to-end%20optimization%20strategy%20for%20an%20optical%20surface%20mesh.%20Our%20formulation%0Aleverages%20a%20novel%20differentiable%20rendering%20model%2C%20and%20is%20directly%20driven%20by%20the%0Adifference%20between%20the%20resulting%20light%20distribution%20and%20the%20target%0Adistribution.%20We%20also%20enforce%20geometric%20constraints%20related%20to%20fabrication%0Arequirements%2C%20to%20facilitate%20CNC%20milling%20and%20polishing%20of%20the%20designed%20surface.%0ATo%20address%20the%20issue%20of%20local%20minima%2C%20we%20formulate%20a%20face-based%20optimal%0Atransport%20problem%20between%20the%20current%20mesh%20and%20the%20target%20distribution%2C%20which%0Amakes%20effective%20large%20changes%20to%20the%20surface%20shape.%20The%20combination%20of%20our%0Aoptimal%20transport%20update%20and%20rendering-guided%20optimization%20produces%20an%20optical%0Asurface%20design%20with%20a%20resulting%20image%20closely%20resembling%20the%20target%2C%20while%20the%0Afabrication%20constraints%20in%20our%20optimization%20help%20to%20ensure%20consistency%20between%0Athe%20rendering%20model%20and%20the%20final%20physical%20results.%20The%20effectiveness%20of%20our%0Aalgorithm%20is%20demonstrated%20on%20a%20variety%20of%20target%20images%20using%20both%20simulated%0Arendering%20and%20physical%20prototypes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-end%2520Surface%2520Optimization%2520for%2520Light%2520Control%26entry.906535625%3DYuou%2520Sun%2520and%2520Bailin%2520Deng%2520and%2520Juyong%2520Zhang%26entry.1292438233%3D%2520%2520Designing%2520a%2520freeform%2520surface%2520to%2520reflect%2520or%2520refract%2520light%2520to%2520achieve%2520a%2520target%250Adistribution%2520is%2520a%2520challenging%2520inverse%2520problem.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aend-to-end%2520optimization%2520strategy%2520for%2520an%2520optical%2520surface%2520mesh.%2520Our%2520formulation%250Aleverages%2520a%2520novel%2520differentiable%2520rendering%2520model%252C%2520and%2520is%2520directly%2520driven%2520by%2520the%250Adifference%2520between%2520the%2520resulting%2520light%2520distribution%2520and%2520the%2520target%250Adistribution.%2520We%2520also%2520enforce%2520geometric%2520constraints%2520related%2520to%2520fabrication%250Arequirements%252C%2520to%2520facilitate%2520CNC%2520milling%2520and%2520polishing%2520of%2520the%2520designed%2520surface.%250ATo%2520address%2520the%2520issue%2520of%2520local%2520minima%252C%2520we%2520formulate%2520a%2520face-based%2520optimal%250Atransport%2520problem%2520between%2520the%2520current%2520mesh%2520and%2520the%2520target%2520distribution%252C%2520which%250Amakes%2520effective%2520large%2520changes%2520to%2520the%2520surface%2520shape.%2520The%2520combination%2520of%2520our%250Aoptimal%2520transport%2520update%2520and%2520rendering-guided%2520optimization%2520produces%2520an%2520optical%250Asurface%2520design%2520with%2520a%2520resulting%2520image%2520closely%2520resembling%2520the%2520target%252C%2520while%2520the%250Afabrication%2520constraints%2520in%2520our%2520optimization%2520help%2520to%2520ensure%2520consistency%2520between%250Athe%2520rendering%2520model%2520and%2520the%2520final%2520physical%2520results.%2520The%2520effectiveness%2520of%2520our%250Aalgorithm%2520is%2520demonstrated%2520on%2520a%2520variety%2520of%2520target%2520images%2520using%2520both%2520simulated%250Arendering%2520and%2520physical%2520prototypes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Surface%20Optimization%20for%20Light%20Control&entry.906535625=Yuou%20Sun%20and%20Bailin%20Deng%20and%20Juyong%20Zhang&entry.1292438233=%20%20Designing%20a%20freeform%20surface%20to%20reflect%20or%20refract%20light%20to%20achieve%20a%20target%0Adistribution%20is%20a%20challenging%20inverse%20problem.%20In%20this%20paper%2C%20we%20propose%20an%0Aend-to-end%20optimization%20strategy%20for%20an%20optical%20surface%20mesh.%20Our%20formulation%0Aleverages%20a%20novel%20differentiable%20rendering%20model%2C%20and%20is%20directly%20driven%20by%20the%0Adifference%20between%20the%20resulting%20light%20distribution%20and%20the%20target%0Adistribution.%20We%20also%20enforce%20geometric%20constraints%20related%20to%20fabrication%0Arequirements%2C%20to%20facilitate%20CNC%20milling%20and%20polishing%20of%20the%20designed%20surface.%0ATo%20address%20the%20issue%20of%20local%20minima%2C%20we%20formulate%20a%20face-based%20optimal%0Atransport%20problem%20between%20the%20current%20mesh%20and%20the%20target%20distribution%2C%20which%0Amakes%20effective%20large%20changes%20to%20the%20surface%20shape.%20The%20combination%20of%20our%0Aoptimal%20transport%20update%20and%20rendering-guided%20optimization%20produces%20an%20optical%0Asurface%20design%20with%20a%20resulting%20image%20closely%20resembling%20the%20target%2C%20while%20the%0Afabrication%20constraints%20in%20our%20optimization%20help%20to%20ensure%20consistency%20between%0Athe%20rendering%20model%20and%20the%20final%20physical%20results.%20The%20effectiveness%20of%20our%0Aalgorithm%20is%20demonstrated%20on%20a%20variety%20of%20target%20images%20using%20both%20simulated%0Arendering%20and%20physical%20prototypes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13117v1&entry.124074799=Read"},
{"title": "Open Llama2 Model for the Lithuanian Language", "author": "Art\u016bras Nakvosas and Povilas Daniu\u0161is and Vytas Mulevi\u010dius", "abstract": "  In this paper, we propose and describe the first open Llama2 large language\nmodels (LLMs) for the Lithuanian language, including an accompanying\nquestion/answer (Q/A) dataset and translations of popular LLM benchmarks. We\nprovide a brief review of open regional LLMs and detailed information on the\nproposed LLMs and their training process. We also conduct an empirical\nevaluation, comparing the perplexities of the proposed LLMs with those of other\nmodern open LLMs. In addition, benchmarking the proposed LLMs against language\nunderstanding tasks reveals that high-quality pretraining datasets may be\nessential for achieving models that perform efficiently on these benchmarks.\nThe full realisations of the described LLMs are available in the accompanying\nopen repository~\\url{https://huggingface.co/neurotechnology}.\n", "link": "http://arxiv.org/abs/2408.12963v1", "date": "2024-08-23", "relevancy": 1.9211, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4844}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Llama2%20Model%20for%20the%20Lithuanian%20Language&body=Title%3A%20Open%20Llama2%20Model%20for%20the%20Lithuanian%20Language%0AAuthor%3A%20Art%C5%ABras%20Nakvosas%20and%20Povilas%20Daniu%C5%A1is%20and%20Vytas%20Mulevi%C4%8Dius%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20and%20describe%20the%20first%20open%20Llama2%20large%20language%0Amodels%20%28LLMs%29%20for%20the%20Lithuanian%20language%2C%20including%20an%20accompanying%0Aquestion/answer%20%28Q/A%29%20dataset%20and%20translations%20of%20popular%20LLM%20benchmarks.%20We%0Aprovide%20a%20brief%20review%20of%20open%20regional%20LLMs%20and%20detailed%20information%20on%20the%0Aproposed%20LLMs%20and%20their%20training%20process.%20We%20also%20conduct%20an%20empirical%0Aevaluation%2C%20comparing%20the%20perplexities%20of%20the%20proposed%20LLMs%20with%20those%20of%20other%0Amodern%20open%20LLMs.%20In%20addition%2C%20benchmarking%20the%20proposed%20LLMs%20against%20language%0Aunderstanding%20tasks%20reveals%20that%20high-quality%20pretraining%20datasets%20may%20be%0Aessential%20for%20achieving%20models%20that%20perform%20efficiently%20on%20these%20benchmarks.%0AThe%20full%20realisations%20of%20the%20described%20LLMs%20are%20available%20in%20the%20accompanying%0Aopen%20repository~%5Curl%7Bhttps%3A//huggingface.co/neurotechnology%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Llama2%2520Model%2520for%2520the%2520Lithuanian%2520Language%26entry.906535625%3DArt%25C5%25ABras%2520Nakvosas%2520and%2520Povilas%2520Daniu%25C5%25A1is%2520and%2520Vytas%2520Mulevi%25C4%258Dius%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520and%2520describe%2520the%2520first%2520open%2520Llama2%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520for%2520the%2520Lithuanian%2520language%252C%2520including%2520an%2520accompanying%250Aquestion/answer%2520%2528Q/A%2529%2520dataset%2520and%2520translations%2520of%2520popular%2520LLM%2520benchmarks.%2520We%250Aprovide%2520a%2520brief%2520review%2520of%2520open%2520regional%2520LLMs%2520and%2520detailed%2520information%2520on%2520the%250Aproposed%2520LLMs%2520and%2520their%2520training%2520process.%2520We%2520also%2520conduct%2520an%2520empirical%250Aevaluation%252C%2520comparing%2520the%2520perplexities%2520of%2520the%2520proposed%2520LLMs%2520with%2520those%2520of%2520other%250Amodern%2520open%2520LLMs.%2520In%2520addition%252C%2520benchmarking%2520the%2520proposed%2520LLMs%2520against%2520language%250Aunderstanding%2520tasks%2520reveals%2520that%2520high-quality%2520pretraining%2520datasets%2520may%2520be%250Aessential%2520for%2520achieving%2520models%2520that%2520perform%2520efficiently%2520on%2520these%2520benchmarks.%250AThe%2520full%2520realisations%2520of%2520the%2520described%2520LLMs%2520are%2520available%2520in%2520the%2520accompanying%250Aopen%2520repository~%255Curl%257Bhttps%253A//huggingface.co/neurotechnology%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Llama2%20Model%20for%20the%20Lithuanian%20Language&entry.906535625=Art%C5%ABras%20Nakvosas%20and%20Povilas%20Daniu%C5%A1is%20and%20Vytas%20Mulevi%C4%8Dius&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20and%20describe%20the%20first%20open%20Llama2%20large%20language%0Amodels%20%28LLMs%29%20for%20the%20Lithuanian%20language%2C%20including%20an%20accompanying%0Aquestion/answer%20%28Q/A%29%20dataset%20and%20translations%20of%20popular%20LLM%20benchmarks.%20We%0Aprovide%20a%20brief%20review%20of%20open%20regional%20LLMs%20and%20detailed%20information%20on%20the%0Aproposed%20LLMs%20and%20their%20training%20process.%20We%20also%20conduct%20an%20empirical%0Aevaluation%2C%20comparing%20the%20perplexities%20of%20the%20proposed%20LLMs%20with%20those%20of%20other%0Amodern%20open%20LLMs.%20In%20addition%2C%20benchmarking%20the%20proposed%20LLMs%20against%20language%0Aunderstanding%20tasks%20reveals%20that%20high-quality%20pretraining%20datasets%20may%20be%0Aessential%20for%20achieving%20models%20that%20perform%20efficiently%20on%20these%20benchmarks.%0AThe%20full%20realisations%20of%20the%20described%20LLMs%20are%20available%20in%20the%20accompanying%0Aopen%20repository~%5Curl%7Bhttps%3A//huggingface.co/neurotechnology%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12963v1&entry.124074799=Read"},
{"title": "Re-evaluation of Face Anti-spoofing Algorithm in Post COVID-19 Era Using\n  Mask Based Occlusion Attack", "author": "Vaibhav Sundharam and Abhijit Sarkar and A. Lynn Abbott", "abstract": "  Face anti-spoofing algorithms play a pivotal role in the robust deployment of\nface recognition systems against presentation attacks. Conventionally, full\nfacial images are required by such systems to correctly authenticate\nindividuals, but the widespread requirement of masks due to the current\nCOVID-19 pandemic has introduced new challenges for these biometric\nauthentication systems. Hence, in this work, we investigate the performance of\npresentation attack detection (PAD) algorithms under synthetic facial\nocclusions using masks and glasses. We have used five variants of masks to\ncover the lower part of the face with varying coverage areas (low-coverage,\nmedium-coverage, high-coverage, round coverage), and 3D cues. We have also used\ndifferent variants of glasses that cover the upper part of the face. We\nsystematically tested the performance of four PAD algorithms under these\nocclusion attacks using a benchmark dataset. We have specifically looked at\nfour different baseline PAD algorithms that focus on, texture, image quality,\nframe difference/motion, and abstract features through a convolutional neural\nnetwork (CNN). Additionally we have introduced a new hybrid model that uses CNN\nand local binary pattern textures. Our experiment shows that adding the\nocclusions significantly degrades the performance of all of the PAD algorithms.\nOur results show the vulnerability of face anti-spoofing algorithms with\nocclusions, which could be in the usage of such algorithms in the post-pandemic\nera.\n", "link": "http://arxiv.org/abs/2408.13251v1", "date": "2024-08-23", "relevancy": 1.9191, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4883}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4862}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-evaluation%20of%20Face%20Anti-spoofing%20Algorithm%20in%20Post%20COVID-19%20Era%20Using%0A%20%20Mask%20Based%20Occlusion%20Attack&body=Title%3A%20Re-evaluation%20of%20Face%20Anti-spoofing%20Algorithm%20in%20Post%20COVID-19%20Era%20Using%0A%20%20Mask%20Based%20Occlusion%20Attack%0AAuthor%3A%20Vaibhav%20Sundharam%20and%20Abhijit%20Sarkar%20and%20A.%20Lynn%20Abbott%0AAbstract%3A%20%20%20Face%20anti-spoofing%20algorithms%20play%20a%20pivotal%20role%20in%20the%20robust%20deployment%20of%0Aface%20recognition%20systems%20against%20presentation%20attacks.%20Conventionally%2C%20full%0Afacial%20images%20are%20required%20by%20such%20systems%20to%20correctly%20authenticate%0Aindividuals%2C%20but%20the%20widespread%20requirement%20of%20masks%20due%20to%20the%20current%0ACOVID-19%20pandemic%20has%20introduced%20new%20challenges%20for%20these%20biometric%0Aauthentication%20systems.%20Hence%2C%20in%20this%20work%2C%20we%20investigate%20the%20performance%20of%0Apresentation%20attack%20detection%20%28PAD%29%20algorithms%20under%20synthetic%20facial%0Aocclusions%20using%20masks%20and%20glasses.%20We%20have%20used%20five%20variants%20of%20masks%20to%0Acover%20the%20lower%20part%20of%20the%20face%20with%20varying%20coverage%20areas%20%28low-coverage%2C%0Amedium-coverage%2C%20high-coverage%2C%20round%20coverage%29%2C%20and%203D%20cues.%20We%20have%20also%20used%0Adifferent%20variants%20of%20glasses%20that%20cover%20the%20upper%20part%20of%20the%20face.%20We%0Asystematically%20tested%20the%20performance%20of%20four%20PAD%20algorithms%20under%20these%0Aocclusion%20attacks%20using%20a%20benchmark%20dataset.%20We%20have%20specifically%20looked%20at%0Afour%20different%20baseline%20PAD%20algorithms%20that%20focus%20on%2C%20texture%2C%20image%20quality%2C%0Aframe%20difference/motion%2C%20and%20abstract%20features%20through%20a%20convolutional%20neural%0Anetwork%20%28CNN%29.%20Additionally%20we%20have%20introduced%20a%20new%20hybrid%20model%20that%20uses%20CNN%0Aand%20local%20binary%20pattern%20textures.%20Our%20experiment%20shows%20that%20adding%20the%0Aocclusions%20significantly%20degrades%20the%20performance%20of%20all%20of%20the%20PAD%20algorithms.%0AOur%20results%20show%20the%20vulnerability%20of%20face%20anti-spoofing%20algorithms%20with%0Aocclusions%2C%20which%20could%20be%20in%20the%20usage%20of%20such%20algorithms%20in%20the%20post-pandemic%0Aera.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-evaluation%2520of%2520Face%2520Anti-spoofing%2520Algorithm%2520in%2520Post%2520COVID-19%2520Era%2520Using%250A%2520%2520Mask%2520Based%2520Occlusion%2520Attack%26entry.906535625%3DVaibhav%2520Sundharam%2520and%2520Abhijit%2520Sarkar%2520and%2520A.%2520Lynn%2520Abbott%26entry.1292438233%3D%2520%2520Face%2520anti-spoofing%2520algorithms%2520play%2520a%2520pivotal%2520role%2520in%2520the%2520robust%2520deployment%2520of%250Aface%2520recognition%2520systems%2520against%2520presentation%2520attacks.%2520Conventionally%252C%2520full%250Afacial%2520images%2520are%2520required%2520by%2520such%2520systems%2520to%2520correctly%2520authenticate%250Aindividuals%252C%2520but%2520the%2520widespread%2520requirement%2520of%2520masks%2520due%2520to%2520the%2520current%250ACOVID-19%2520pandemic%2520has%2520introduced%2520new%2520challenges%2520for%2520these%2520biometric%250Aauthentication%2520systems.%2520Hence%252C%2520in%2520this%2520work%252C%2520we%2520investigate%2520the%2520performance%2520of%250Apresentation%2520attack%2520detection%2520%2528PAD%2529%2520algorithms%2520under%2520synthetic%2520facial%250Aocclusions%2520using%2520masks%2520and%2520glasses.%2520We%2520have%2520used%2520five%2520variants%2520of%2520masks%2520to%250Acover%2520the%2520lower%2520part%2520of%2520the%2520face%2520with%2520varying%2520coverage%2520areas%2520%2528low-coverage%252C%250Amedium-coverage%252C%2520high-coverage%252C%2520round%2520coverage%2529%252C%2520and%25203D%2520cues.%2520We%2520have%2520also%2520used%250Adifferent%2520variants%2520of%2520glasses%2520that%2520cover%2520the%2520upper%2520part%2520of%2520the%2520face.%2520We%250Asystematically%2520tested%2520the%2520performance%2520of%2520four%2520PAD%2520algorithms%2520under%2520these%250Aocclusion%2520attacks%2520using%2520a%2520benchmark%2520dataset.%2520We%2520have%2520specifically%2520looked%2520at%250Afour%2520different%2520baseline%2520PAD%2520algorithms%2520that%2520focus%2520on%252C%2520texture%252C%2520image%2520quality%252C%250Aframe%2520difference/motion%252C%2520and%2520abstract%2520features%2520through%2520a%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529.%2520Additionally%2520we%2520have%2520introduced%2520a%2520new%2520hybrid%2520model%2520that%2520uses%2520CNN%250Aand%2520local%2520binary%2520pattern%2520textures.%2520Our%2520experiment%2520shows%2520that%2520adding%2520the%250Aocclusions%2520significantly%2520degrades%2520the%2520performance%2520of%2520all%2520of%2520the%2520PAD%2520algorithms.%250AOur%2520results%2520show%2520the%2520vulnerability%2520of%2520face%2520anti-spoofing%2520algorithms%2520with%250Aocclusions%252C%2520which%2520could%2520be%2520in%2520the%2520usage%2520of%2520such%2520algorithms%2520in%2520the%2520post-pandemic%250Aera.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-evaluation%20of%20Face%20Anti-spoofing%20Algorithm%20in%20Post%20COVID-19%20Era%20Using%0A%20%20Mask%20Based%20Occlusion%20Attack&entry.906535625=Vaibhav%20Sundharam%20and%20Abhijit%20Sarkar%20and%20A.%20Lynn%20Abbott&entry.1292438233=%20%20Face%20anti-spoofing%20algorithms%20play%20a%20pivotal%20role%20in%20the%20robust%20deployment%20of%0Aface%20recognition%20systems%20against%20presentation%20attacks.%20Conventionally%2C%20full%0Afacial%20images%20are%20required%20by%20such%20systems%20to%20correctly%20authenticate%0Aindividuals%2C%20but%20the%20widespread%20requirement%20of%20masks%20due%20to%20the%20current%0ACOVID-19%20pandemic%20has%20introduced%20new%20challenges%20for%20these%20biometric%0Aauthentication%20systems.%20Hence%2C%20in%20this%20work%2C%20we%20investigate%20the%20performance%20of%0Apresentation%20attack%20detection%20%28PAD%29%20algorithms%20under%20synthetic%20facial%0Aocclusions%20using%20masks%20and%20glasses.%20We%20have%20used%20five%20variants%20of%20masks%20to%0Acover%20the%20lower%20part%20of%20the%20face%20with%20varying%20coverage%20areas%20%28low-coverage%2C%0Amedium-coverage%2C%20high-coverage%2C%20round%20coverage%29%2C%20and%203D%20cues.%20We%20have%20also%20used%0Adifferent%20variants%20of%20glasses%20that%20cover%20the%20upper%20part%20of%20the%20face.%20We%0Asystematically%20tested%20the%20performance%20of%20four%20PAD%20algorithms%20under%20these%0Aocclusion%20attacks%20using%20a%20benchmark%20dataset.%20We%20have%20specifically%20looked%20at%0Afour%20different%20baseline%20PAD%20algorithms%20that%20focus%20on%2C%20texture%2C%20image%20quality%2C%0Aframe%20difference/motion%2C%20and%20abstract%20features%20through%20a%20convolutional%20neural%0Anetwork%20%28CNN%29.%20Additionally%20we%20have%20introduced%20a%20new%20hybrid%20model%20that%20uses%20CNN%0Aand%20local%20binary%20pattern%20textures.%20Our%20experiment%20shows%20that%20adding%20the%0Aocclusions%20significantly%20degrades%20the%20performance%20of%20all%20of%20the%20PAD%20algorithms.%0AOur%20results%20show%20the%20vulnerability%20of%20face%20anti-spoofing%20algorithms%20with%0Aocclusions%2C%20which%20could%20be%20in%20the%20usage%20of%20such%20algorithms%20in%20the%20post-pandemic%0Aera.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13251v1&entry.124074799=Read"},
{"title": "Search-Adaptor: Embedding Customization for Information Retrieval", "author": "Jinsung Yoon and Sercan O Arik and Yanfei Chen and Tomas Pfister", "abstract": "  Embeddings extracted by pre-trained Large Language Models (LLMs) have\nsignificant potential to improve information retrieval and search. Beyond the\nzero-shot setup in which they are being conventionally used, being able to take\nadvantage of the information from the relevant query-corpus paired data can\nfurther boost the LLM capabilities. In this paper, we propose a novel method,\nSearch-Adaptor, for customizing LLMs for information retrieval in an efficient\nand robust way. Search-Adaptor modifies the embeddings generated by pre-trained\nLLMs, and can be integrated with any LLM, including those only available via\nprediction APIs. On multiple English, multilingual, and multimodal retrieval\ndatasets, we show consistent and significant performance benefits for\nSearch-Adaptor -- e.g., more than 5% improvements for Google Embedding APIs in\nnDCG@10 averaged over 14 BEIR datasets.\n", "link": "http://arxiv.org/abs/2310.08750v3", "date": "2024-08-23", "relevancy": 1.9179, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4639}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search-Adaptor%3A%20Embedding%20Customization%20for%20Information%20Retrieval&body=Title%3A%20Search-Adaptor%3A%20Embedding%20Customization%20for%20Information%20Retrieval%0AAuthor%3A%20Jinsung%20Yoon%20and%20Sercan%20O%20Arik%20and%20Yanfei%20Chen%20and%20Tomas%20Pfister%0AAbstract%3A%20%20%20Embeddings%20extracted%20by%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20have%0Asignificant%20potential%20to%20improve%20information%20retrieval%20and%20search.%20Beyond%20the%0Azero-shot%20setup%20in%20which%20they%20are%20being%20conventionally%20used%2C%20being%20able%20to%20take%0Aadvantage%20of%20the%20information%20from%20the%20relevant%20query-corpus%20paired%20data%20can%0Afurther%20boost%20the%20LLM%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0ASearch-Adaptor%2C%20for%20customizing%20LLMs%20for%20information%20retrieval%20in%20an%20efficient%0Aand%20robust%20way.%20Search-Adaptor%20modifies%20the%20embeddings%20generated%20by%20pre-trained%0ALLMs%2C%20and%20can%20be%20integrated%20with%20any%20LLM%2C%20including%20those%20only%20available%20via%0Aprediction%20APIs.%20On%20multiple%20English%2C%20multilingual%2C%20and%20multimodal%20retrieval%0Adatasets%2C%20we%20show%20consistent%20and%20significant%20performance%20benefits%20for%0ASearch-Adaptor%20--%20e.g.%2C%20more%20than%205%25%20improvements%20for%20Google%20Embedding%20APIs%20in%0AnDCG%4010%20averaged%20over%2014%20BEIR%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08750v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch-Adaptor%253A%2520Embedding%2520Customization%2520for%2520Information%2520Retrieval%26entry.906535625%3DJinsung%2520Yoon%2520and%2520Sercan%2520O%2520Arik%2520and%2520Yanfei%2520Chen%2520and%2520Tomas%2520Pfister%26entry.1292438233%3D%2520%2520Embeddings%2520extracted%2520by%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Asignificant%2520potential%2520to%2520improve%2520information%2520retrieval%2520and%2520search.%2520Beyond%2520the%250Azero-shot%2520setup%2520in%2520which%2520they%2520are%2520being%2520conventionally%2520used%252C%2520being%2520able%2520to%2520take%250Aadvantage%2520of%2520the%2520information%2520from%2520the%2520relevant%2520query-corpus%2520paired%2520data%2520can%250Afurther%2520boost%2520the%2520LLM%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250ASearch-Adaptor%252C%2520for%2520customizing%2520LLMs%2520for%2520information%2520retrieval%2520in%2520an%2520efficient%250Aand%2520robust%2520way.%2520Search-Adaptor%2520modifies%2520the%2520embeddings%2520generated%2520by%2520pre-trained%250ALLMs%252C%2520and%2520can%2520be%2520integrated%2520with%2520any%2520LLM%252C%2520including%2520those%2520only%2520available%2520via%250Aprediction%2520APIs.%2520On%2520multiple%2520English%252C%2520multilingual%252C%2520and%2520multimodal%2520retrieval%250Adatasets%252C%2520we%2520show%2520consistent%2520and%2520significant%2520performance%2520benefits%2520for%250ASearch-Adaptor%2520--%2520e.g.%252C%2520more%2520than%25205%2525%2520improvements%2520for%2520Google%2520Embedding%2520APIs%2520in%250AnDCG%254010%2520averaged%2520over%252014%2520BEIR%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08750v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search-Adaptor%3A%20Embedding%20Customization%20for%20Information%20Retrieval&entry.906535625=Jinsung%20Yoon%20and%20Sercan%20O%20Arik%20and%20Yanfei%20Chen%20and%20Tomas%20Pfister&entry.1292438233=%20%20Embeddings%20extracted%20by%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20have%0Asignificant%20potential%20to%20improve%20information%20retrieval%20and%20search.%20Beyond%20the%0Azero-shot%20setup%20in%20which%20they%20are%20being%20conventionally%20used%2C%20being%20able%20to%20take%0Aadvantage%20of%20the%20information%20from%20the%20relevant%20query-corpus%20paired%20data%20can%0Afurther%20boost%20the%20LLM%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0ASearch-Adaptor%2C%20for%20customizing%20LLMs%20for%20information%20retrieval%20in%20an%20efficient%0Aand%20robust%20way.%20Search-Adaptor%20modifies%20the%20embeddings%20generated%20by%20pre-trained%0ALLMs%2C%20and%20can%20be%20integrated%20with%20any%20LLM%2C%20including%20those%20only%20available%20via%0Aprediction%20APIs.%20On%20multiple%20English%2C%20multilingual%2C%20and%20multimodal%20retrieval%0Adatasets%2C%20we%20show%20consistent%20and%20significant%20performance%20benefits%20for%0ASearch-Adaptor%20--%20e.g.%2C%20more%20than%205%25%20improvements%20for%20Google%20Embedding%20APIs%20in%0AnDCG%4010%20averaged%20over%2014%20BEIR%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08750v3&entry.124074799=Read"},
{"title": "JacNet: Learning Functions with Structured Jacobians", "author": "Jonathan Lorraine and Safwan Hossain", "abstract": "  Neural networks are trained to learn an approximate mapping from an input\ndomain to a target domain. Incorporating prior knowledge about true mappings is\ncritical to learning a useful approximation. With current architectures, it is\nchallenging to enforce structure on the derivatives of the input-output\nmapping. We propose to use a neural network to directly learn the Jacobian of\nthe input-output function, which allows easy control of the derivative. We\nfocus on structuring the derivative to allow invertibility and also demonstrate\nthat other useful priors, such as $k$-Lipschitz, can be enforced. Using this\napproach, we can learn approximations to simple functions that are guaranteed\nto be invertible and easily compute the inverse. We also show similar results\nfor 1-Lipschitz functions.\n", "link": "http://arxiv.org/abs/2408.13237v1", "date": "2024-08-23", "relevancy": 1.8716, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4885}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4637}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JacNet%3A%20Learning%20Functions%20with%20Structured%20Jacobians&body=Title%3A%20JacNet%3A%20Learning%20Functions%20with%20Structured%20Jacobians%0AAuthor%3A%20Jonathan%20Lorraine%20and%20Safwan%20Hossain%0AAbstract%3A%20%20%20Neural%20networks%20are%20trained%20to%20learn%20an%20approximate%20mapping%20from%20an%20input%0Adomain%20to%20a%20target%20domain.%20Incorporating%20prior%20knowledge%20about%20true%20mappings%20is%0Acritical%20to%20learning%20a%20useful%20approximation.%20With%20current%20architectures%2C%20it%20is%0Achallenging%20to%20enforce%20structure%20on%20the%20derivatives%20of%20the%20input-output%0Amapping.%20We%20propose%20to%20use%20a%20neural%20network%20to%20directly%20learn%20the%20Jacobian%20of%0Athe%20input-output%20function%2C%20which%20allows%20easy%20control%20of%20the%20derivative.%20We%0Afocus%20on%20structuring%20the%20derivative%20to%20allow%20invertibility%20and%20also%20demonstrate%0Athat%20other%20useful%20priors%2C%20such%20as%20%24k%24-Lipschitz%2C%20can%20be%20enforced.%20Using%20this%0Aapproach%2C%20we%20can%20learn%20approximations%20to%20simple%20functions%20that%20are%20guaranteed%0Ato%20be%20invertible%20and%20easily%20compute%20the%20inverse.%20We%20also%20show%20similar%20results%0Afor%201-Lipschitz%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJacNet%253A%2520Learning%2520Functions%2520with%2520Structured%2520Jacobians%26entry.906535625%3DJonathan%2520Lorraine%2520and%2520Safwan%2520Hossain%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520trained%2520to%2520learn%2520an%2520approximate%2520mapping%2520from%2520an%2520input%250Adomain%2520to%2520a%2520target%2520domain.%2520Incorporating%2520prior%2520knowledge%2520about%2520true%2520mappings%2520is%250Acritical%2520to%2520learning%2520a%2520useful%2520approximation.%2520With%2520current%2520architectures%252C%2520it%2520is%250Achallenging%2520to%2520enforce%2520structure%2520on%2520the%2520derivatives%2520of%2520the%2520input-output%250Amapping.%2520We%2520propose%2520to%2520use%2520a%2520neural%2520network%2520to%2520directly%2520learn%2520the%2520Jacobian%2520of%250Athe%2520input-output%2520function%252C%2520which%2520allows%2520easy%2520control%2520of%2520the%2520derivative.%2520We%250Afocus%2520on%2520structuring%2520the%2520derivative%2520to%2520allow%2520invertibility%2520and%2520also%2520demonstrate%250Athat%2520other%2520useful%2520priors%252C%2520such%2520as%2520%2524k%2524-Lipschitz%252C%2520can%2520be%2520enforced.%2520Using%2520this%250Aapproach%252C%2520we%2520can%2520learn%2520approximations%2520to%2520simple%2520functions%2520that%2520are%2520guaranteed%250Ato%2520be%2520invertible%2520and%2520easily%2520compute%2520the%2520inverse.%2520We%2520also%2520show%2520similar%2520results%250Afor%25201-Lipschitz%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JacNet%3A%20Learning%20Functions%20with%20Structured%20Jacobians&entry.906535625=Jonathan%20Lorraine%20and%20Safwan%20Hossain&entry.1292438233=%20%20Neural%20networks%20are%20trained%20to%20learn%20an%20approximate%20mapping%20from%20an%20input%0Adomain%20to%20a%20target%20domain.%20Incorporating%20prior%20knowledge%20about%20true%20mappings%20is%0Acritical%20to%20learning%20a%20useful%20approximation.%20With%20current%20architectures%2C%20it%20is%0Achallenging%20to%20enforce%20structure%20on%20the%20derivatives%20of%20the%20input-output%0Amapping.%20We%20propose%20to%20use%20a%20neural%20network%20to%20directly%20learn%20the%20Jacobian%20of%0Athe%20input-output%20function%2C%20which%20allows%20easy%20control%20of%20the%20derivative.%20We%0Afocus%20on%20structuring%20the%20derivative%20to%20allow%20invertibility%20and%20also%20demonstrate%0Athat%20other%20useful%20priors%2C%20such%20as%20%24k%24-Lipschitz%2C%20can%20be%20enforced.%20Using%20this%0Aapproach%2C%20we%20can%20learn%20approximations%20to%20simple%20functions%20that%20are%20guaranteed%0Ato%20be%20invertible%20and%20easily%20compute%20the%20inverse.%20We%20also%20show%20similar%20results%0Afor%201-Lipschitz%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13237v1&entry.124074799=Read"},
{"title": "Protecting against simultaneous data poisoning attacks", "author": "Neel Alex and Shoaib Ahmed Siddiqui and Amartya Sanyal and David Krueger", "abstract": "  Current backdoor defense methods are evaluated against a single attack at a\ntime. This is unrealistic, as powerful machine learning systems are trained on\nlarge datasets scraped from the internet, which may be attacked multiple times\nby one or more attackers. We demonstrate that simultaneously executed data\npoisoning attacks can effectively install multiple backdoors in a single model\nwithout substantially degrading clean accuracy. Furthermore, we show that\nexisting backdoor defense methods do not effectively prevent attacks in this\nsetting. Finally, we leverage insights into the nature of backdoor attacks to\ndevelop a new defense, BaDLoss, that is effective in the multi-attack setting.\nWith minimal clean accuracy degradation, BaDLoss attains an average attack\nsuccess rate in the multi-attack setting of 7.98% in CIFAR-10 and 10.29% in\nGTSRB, compared to the average of other defenses at 64.48% and 84.28%\nrespectively.\n", "link": "http://arxiv.org/abs/2408.13221v1", "date": "2024-08-23", "relevancy": 1.8689, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4701}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4701}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protecting%20against%20simultaneous%20data%20poisoning%20attacks&body=Title%3A%20Protecting%20against%20simultaneous%20data%20poisoning%20attacks%0AAuthor%3A%20Neel%20Alex%20and%20Shoaib%20Ahmed%20Siddiqui%20and%20Amartya%20Sanyal%20and%20David%20Krueger%0AAbstract%3A%20%20%20Current%20backdoor%20defense%20methods%20are%20evaluated%20against%20a%20single%20attack%20at%20a%0Atime.%20This%20is%20unrealistic%2C%20as%20powerful%20machine%20learning%20systems%20are%20trained%20on%0Alarge%20datasets%20scraped%20from%20the%20internet%2C%20which%20may%20be%20attacked%20multiple%20times%0Aby%20one%20or%20more%20attackers.%20We%20demonstrate%20that%20simultaneously%20executed%20data%0Apoisoning%20attacks%20can%20effectively%20install%20multiple%20backdoors%20in%20a%20single%20model%0Awithout%20substantially%20degrading%20clean%20accuracy.%20Furthermore%2C%20we%20show%20that%0Aexisting%20backdoor%20defense%20methods%20do%20not%20effectively%20prevent%20attacks%20in%20this%0Asetting.%20Finally%2C%20we%20leverage%20insights%20into%20the%20nature%20of%20backdoor%20attacks%20to%0Adevelop%20a%20new%20defense%2C%20BaDLoss%2C%20that%20is%20effective%20in%20the%20multi-attack%20setting.%0AWith%20minimal%20clean%20accuracy%20degradation%2C%20BaDLoss%20attains%20an%20average%20attack%0Asuccess%20rate%20in%20the%20multi-attack%20setting%20of%207.98%25%20in%20CIFAR-10%20and%2010.29%25%20in%0AGTSRB%2C%20compared%20to%20the%20average%20of%20other%20defenses%20at%2064.48%25%20and%2084.28%25%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtecting%2520against%2520simultaneous%2520data%2520poisoning%2520attacks%26entry.906535625%3DNeel%2520Alex%2520and%2520Shoaib%2520Ahmed%2520Siddiqui%2520and%2520Amartya%2520Sanyal%2520and%2520David%2520Krueger%26entry.1292438233%3D%2520%2520Current%2520backdoor%2520defense%2520methods%2520are%2520evaluated%2520against%2520a%2520single%2520attack%2520at%2520a%250Atime.%2520This%2520is%2520unrealistic%252C%2520as%2520powerful%2520machine%2520learning%2520systems%2520are%2520trained%2520on%250Alarge%2520datasets%2520scraped%2520from%2520the%2520internet%252C%2520which%2520may%2520be%2520attacked%2520multiple%2520times%250Aby%2520one%2520or%2520more%2520attackers.%2520We%2520demonstrate%2520that%2520simultaneously%2520executed%2520data%250Apoisoning%2520attacks%2520can%2520effectively%2520install%2520multiple%2520backdoors%2520in%2520a%2520single%2520model%250Awithout%2520substantially%2520degrading%2520clean%2520accuracy.%2520Furthermore%252C%2520we%2520show%2520that%250Aexisting%2520backdoor%2520defense%2520methods%2520do%2520not%2520effectively%2520prevent%2520attacks%2520in%2520this%250Asetting.%2520Finally%252C%2520we%2520leverage%2520insights%2520into%2520the%2520nature%2520of%2520backdoor%2520attacks%2520to%250Adevelop%2520a%2520new%2520defense%252C%2520BaDLoss%252C%2520that%2520is%2520effective%2520in%2520the%2520multi-attack%2520setting.%250AWith%2520minimal%2520clean%2520accuracy%2520degradation%252C%2520BaDLoss%2520attains%2520an%2520average%2520attack%250Asuccess%2520rate%2520in%2520the%2520multi-attack%2520setting%2520of%25207.98%2525%2520in%2520CIFAR-10%2520and%252010.29%2525%2520in%250AGTSRB%252C%2520compared%2520to%2520the%2520average%2520of%2520other%2520defenses%2520at%252064.48%2525%2520and%252084.28%2525%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protecting%20against%20simultaneous%20data%20poisoning%20attacks&entry.906535625=Neel%20Alex%20and%20Shoaib%20Ahmed%20Siddiqui%20and%20Amartya%20Sanyal%20and%20David%20Krueger&entry.1292438233=%20%20Current%20backdoor%20defense%20methods%20are%20evaluated%20against%20a%20single%20attack%20at%20a%0Atime.%20This%20is%20unrealistic%2C%20as%20powerful%20machine%20learning%20systems%20are%20trained%20on%0Alarge%20datasets%20scraped%20from%20the%20internet%2C%20which%20may%20be%20attacked%20multiple%20times%0Aby%20one%20or%20more%20attackers.%20We%20demonstrate%20that%20simultaneously%20executed%20data%0Apoisoning%20attacks%20can%20effectively%20install%20multiple%20backdoors%20in%20a%20single%20model%0Awithout%20substantially%20degrading%20clean%20accuracy.%20Furthermore%2C%20we%20show%20that%0Aexisting%20backdoor%20defense%20methods%20do%20not%20effectively%20prevent%20attacks%20in%20this%0Asetting.%20Finally%2C%20we%20leverage%20insights%20into%20the%20nature%20of%20backdoor%20attacks%20to%0Adevelop%20a%20new%20defense%2C%20BaDLoss%2C%20that%20is%20effective%20in%20the%20multi-attack%20setting.%0AWith%20minimal%20clean%20accuracy%20degradation%2C%20BaDLoss%20attains%20an%20average%20attack%0Asuccess%20rate%20in%20the%20multi-attack%20setting%20of%207.98%25%20in%20CIFAR-10%20and%2010.29%25%20in%0AGTSRB%2C%20compared%20to%20the%20average%20of%20other%20defenses%20at%2064.48%25%20and%2084.28%25%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13221v1&entry.124074799=Read"},
{"title": "Leveraging Task Structures for Improved Identifiability in Neural\n  Network Representations", "author": "Wenlin Chen and Julien Horwood and Juyeon Heo and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  This work extends the theory of identifiability in supervised learning by\nconsidering the consequences of having access to a distribution of tasks. In\nsuch cases, we show that linear identifiability is achievable in the general\nmulti-task regression setting. Furthermore, we show that the existence of a\ntask distribution which defines a conditional prior over latent factors reduces\nthe equivalence class for identifiability to permutations and scaling of the\ntrue latent factors, a stronger and more useful result than linear\nidentifiability. Crucially, when we further assume a causal structure over\nthese tasks, our approach enables simple maximum marginal likelihood\noptimization, and suggests potential downstream applications to causal\nrepresentation learning. Empirically, we find that this straightforward\noptimization procedure enables our model to outperform more general\nunsupervised models in recovering canonical representations for both synthetic\ndata and real-world molecular data.\n", "link": "http://arxiv.org/abs/2306.14861v3", "date": "2024-08-23", "relevancy": 1.8648, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4815}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4641}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Task%20Structures%20for%20Improved%20Identifiability%20in%20Neural%0A%20%20Network%20Representations&body=Title%3A%20Leveraging%20Task%20Structures%20for%20Improved%20Identifiability%20in%20Neural%0A%20%20Network%20Representations%0AAuthor%3A%20Wenlin%20Chen%20and%20Julien%20Horwood%20and%20Juyeon%20Heo%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20This%20work%20extends%20the%20theory%20of%20identifiability%20in%20supervised%20learning%20by%0Aconsidering%20the%20consequences%20of%20having%20access%20to%20a%20distribution%20of%20tasks.%20In%0Asuch%20cases%2C%20we%20show%20that%20linear%20identifiability%20is%20achievable%20in%20the%20general%0Amulti-task%20regression%20setting.%20Furthermore%2C%20we%20show%20that%20the%20existence%20of%20a%0Atask%20distribution%20which%20defines%20a%20conditional%20prior%20over%20latent%20factors%20reduces%0Athe%20equivalence%20class%20for%20identifiability%20to%20permutations%20and%20scaling%20of%20the%0Atrue%20latent%20factors%2C%20a%20stronger%20and%20more%20useful%20result%20than%20linear%0Aidentifiability.%20Crucially%2C%20when%20we%20further%20assume%20a%20causal%20structure%20over%0Athese%20tasks%2C%20our%20approach%20enables%20simple%20maximum%20marginal%20likelihood%0Aoptimization%2C%20and%20suggests%20potential%20downstream%20applications%20to%20causal%0Arepresentation%20learning.%20Empirically%2C%20we%20find%20that%20this%20straightforward%0Aoptimization%20procedure%20enables%20our%20model%20to%20outperform%20more%20general%0Aunsupervised%20models%20in%20recovering%20canonical%20representations%20for%20both%20synthetic%0Adata%20and%20real-world%20molecular%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.14861v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Task%2520Structures%2520for%2520Improved%2520Identifiability%2520in%2520Neural%250A%2520%2520Network%2520Representations%26entry.906535625%3DWenlin%2520Chen%2520and%2520Julien%2520Horwood%2520and%2520Juyeon%2520Heo%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520This%2520work%2520extends%2520the%2520theory%2520of%2520identifiability%2520in%2520supervised%2520learning%2520by%250Aconsidering%2520the%2520consequences%2520of%2520having%2520access%2520to%2520a%2520distribution%2520of%2520tasks.%2520In%250Asuch%2520cases%252C%2520we%2520show%2520that%2520linear%2520identifiability%2520is%2520achievable%2520in%2520the%2520general%250Amulti-task%2520regression%2520setting.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520existence%2520of%2520a%250Atask%2520distribution%2520which%2520defines%2520a%2520conditional%2520prior%2520over%2520latent%2520factors%2520reduces%250Athe%2520equivalence%2520class%2520for%2520identifiability%2520to%2520permutations%2520and%2520scaling%2520of%2520the%250Atrue%2520latent%2520factors%252C%2520a%2520stronger%2520and%2520more%2520useful%2520result%2520than%2520linear%250Aidentifiability.%2520Crucially%252C%2520when%2520we%2520further%2520assume%2520a%2520causal%2520structure%2520over%250Athese%2520tasks%252C%2520our%2520approach%2520enables%2520simple%2520maximum%2520marginal%2520likelihood%250Aoptimization%252C%2520and%2520suggests%2520potential%2520downstream%2520applications%2520to%2520causal%250Arepresentation%2520learning.%2520Empirically%252C%2520we%2520find%2520that%2520this%2520straightforward%250Aoptimization%2520procedure%2520enables%2520our%2520model%2520to%2520outperform%2520more%2520general%250Aunsupervised%2520models%2520in%2520recovering%2520canonical%2520representations%2520for%2520both%2520synthetic%250Adata%2520and%2520real-world%2520molecular%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.14861v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Task%20Structures%20for%20Improved%20Identifiability%20in%20Neural%0A%20%20Network%20Representations&entry.906535625=Wenlin%20Chen%20and%20Julien%20Horwood%20and%20Juyeon%20Heo%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20This%20work%20extends%20the%20theory%20of%20identifiability%20in%20supervised%20learning%20by%0Aconsidering%20the%20consequences%20of%20having%20access%20to%20a%20distribution%20of%20tasks.%20In%0Asuch%20cases%2C%20we%20show%20that%20linear%20identifiability%20is%20achievable%20in%20the%20general%0Amulti-task%20regression%20setting.%20Furthermore%2C%20we%20show%20that%20the%20existence%20of%20a%0Atask%20distribution%20which%20defines%20a%20conditional%20prior%20over%20latent%20factors%20reduces%0Athe%20equivalence%20class%20for%20identifiability%20to%20permutations%20and%20scaling%20of%20the%0Atrue%20latent%20factors%2C%20a%20stronger%20and%20more%20useful%20result%20than%20linear%0Aidentifiability.%20Crucially%2C%20when%20we%20further%20assume%20a%20causal%20structure%20over%0Athese%20tasks%2C%20our%20approach%20enables%20simple%20maximum%20marginal%20likelihood%0Aoptimization%2C%20and%20suggests%20potential%20downstream%20applications%20to%20causal%0Arepresentation%20learning.%20Empirically%2C%20we%20find%20that%20this%20straightforward%0Aoptimization%20procedure%20enables%20our%20model%20to%20outperform%20more%20general%0Aunsupervised%20models%20in%20recovering%20canonical%20representations%20for%20both%20synthetic%0Adata%20and%20real-world%20molecular%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14861v3&entry.124074799=Read"},
{"title": "Deep Learning for Lung Disease Classification Using Transfer Learning\n  and a Customized CNN Architecture with Attention", "author": "Xiaoyi Liu and Zhou Yu and Lianghao Tan", "abstract": "  Many people die from lung-related diseases every year. X-ray is an effective\nway to test if one is diagnosed with a lung-related disease or not. This study\nconcentrates on categorizing three distinct types of lung X-rays: those\ndepicting healthy lungs, those showing lung opacities, and those indicative of\nviral pneumonia. Accurately diagnosing the disease at an early phase is\ncritical. In this paper, five different pre-trained models will be tested on\nthe Lung X-ray Image Dataset. SqueezeNet, VGG11, ResNet18, DenseNet, and\nMobileNetV2 achieved accuracies of 0.64, 0.85, 0.87, 0.88, and 0.885,\nrespectively. MobileNetV2, as the best-performing pre-trained model, will then\nbe further analyzed as the base model. Eventually, our own model,\nMobileNet-Lung based on MobileNetV2, with fine-tuning and an additional layer\nof attention within feature layers, was invented to tackle the lung disease\nclassification task and achieved an accuracy of 0.933. This result is\nsignificantly improved compared with all five pre-trained models.\n", "link": "http://arxiv.org/abs/2408.13180v1", "date": "2024-08-23", "relevancy": 1.8574, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4513}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Lung%20Disease%20Classification%20Using%20Transfer%20Learning%0A%20%20and%20a%20Customized%20CNN%20Architecture%20with%20Attention&body=Title%3A%20Deep%20Learning%20for%20Lung%20Disease%20Classification%20Using%20Transfer%20Learning%0A%20%20and%20a%20Customized%20CNN%20Architecture%20with%20Attention%0AAuthor%3A%20Xiaoyi%20Liu%20and%20Zhou%20Yu%20and%20Lianghao%20Tan%0AAbstract%3A%20%20%20Many%20people%20die%20from%20lung-related%20diseases%20every%20year.%20X-ray%20is%20an%20effective%0Away%20to%20test%20if%20one%20is%20diagnosed%20with%20a%20lung-related%20disease%20or%20not.%20This%20study%0Aconcentrates%20on%20categorizing%20three%20distinct%20types%20of%20lung%20X-rays%3A%20those%0Adepicting%20healthy%20lungs%2C%20those%20showing%20lung%20opacities%2C%20and%20those%20indicative%20of%0Aviral%20pneumonia.%20Accurately%20diagnosing%20the%20disease%20at%20an%20early%20phase%20is%0Acritical.%20In%20this%20paper%2C%20five%20different%20pre-trained%20models%20will%20be%20tested%20on%0Athe%20Lung%20X-ray%20Image%20Dataset.%20SqueezeNet%2C%20VGG11%2C%20ResNet18%2C%20DenseNet%2C%20and%0AMobileNetV2%20achieved%20accuracies%20of%200.64%2C%200.85%2C%200.87%2C%200.88%2C%20and%200.885%2C%0Arespectively.%20MobileNetV2%2C%20as%20the%20best-performing%20pre-trained%20model%2C%20will%20then%0Abe%20further%20analyzed%20as%20the%20base%20model.%20Eventually%2C%20our%20own%20model%2C%0AMobileNet-Lung%20based%20on%20MobileNetV2%2C%20with%20fine-tuning%20and%20an%20additional%20layer%0Aof%20attention%20within%20feature%20layers%2C%20was%20invented%20to%20tackle%20the%20lung%20disease%0Aclassification%20task%20and%20achieved%20an%20accuracy%20of%200.933.%20This%20result%20is%0Asignificantly%20improved%20compared%20with%20all%20five%20pre-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Lung%2520Disease%2520Classification%2520Using%2520Transfer%2520Learning%250A%2520%2520and%2520a%2520Customized%2520CNN%2520Architecture%2520with%2520Attention%26entry.906535625%3DXiaoyi%2520Liu%2520and%2520Zhou%2520Yu%2520and%2520Lianghao%2520Tan%26entry.1292438233%3D%2520%2520Many%2520people%2520die%2520from%2520lung-related%2520diseases%2520every%2520year.%2520X-ray%2520is%2520an%2520effective%250Away%2520to%2520test%2520if%2520one%2520is%2520diagnosed%2520with%2520a%2520lung-related%2520disease%2520or%2520not.%2520This%2520study%250Aconcentrates%2520on%2520categorizing%2520three%2520distinct%2520types%2520of%2520lung%2520X-rays%253A%2520those%250Adepicting%2520healthy%2520lungs%252C%2520those%2520showing%2520lung%2520opacities%252C%2520and%2520those%2520indicative%2520of%250Aviral%2520pneumonia.%2520Accurately%2520diagnosing%2520the%2520disease%2520at%2520an%2520early%2520phase%2520is%250Acritical.%2520In%2520this%2520paper%252C%2520five%2520different%2520pre-trained%2520models%2520will%2520be%2520tested%2520on%250Athe%2520Lung%2520X-ray%2520Image%2520Dataset.%2520SqueezeNet%252C%2520VGG11%252C%2520ResNet18%252C%2520DenseNet%252C%2520and%250AMobileNetV2%2520achieved%2520accuracies%2520of%25200.64%252C%25200.85%252C%25200.87%252C%25200.88%252C%2520and%25200.885%252C%250Arespectively.%2520MobileNetV2%252C%2520as%2520the%2520best-performing%2520pre-trained%2520model%252C%2520will%2520then%250Abe%2520further%2520analyzed%2520as%2520the%2520base%2520model.%2520Eventually%252C%2520our%2520own%2520model%252C%250AMobileNet-Lung%2520based%2520on%2520MobileNetV2%252C%2520with%2520fine-tuning%2520and%2520an%2520additional%2520layer%250Aof%2520attention%2520within%2520feature%2520layers%252C%2520was%2520invented%2520to%2520tackle%2520the%2520lung%2520disease%250Aclassification%2520task%2520and%2520achieved%2520an%2520accuracy%2520of%25200.933.%2520This%2520result%2520is%250Asignificantly%2520improved%2520compared%2520with%2520all%2520five%2520pre-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Lung%20Disease%20Classification%20Using%20Transfer%20Learning%0A%20%20and%20a%20Customized%20CNN%20Architecture%20with%20Attention&entry.906535625=Xiaoyi%20Liu%20and%20Zhou%20Yu%20and%20Lianghao%20Tan&entry.1292438233=%20%20Many%20people%20die%20from%20lung-related%20diseases%20every%20year.%20X-ray%20is%20an%20effective%0Away%20to%20test%20if%20one%20is%20diagnosed%20with%20a%20lung-related%20disease%20or%20not.%20This%20study%0Aconcentrates%20on%20categorizing%20three%20distinct%20types%20of%20lung%20X-rays%3A%20those%0Adepicting%20healthy%20lungs%2C%20those%20showing%20lung%20opacities%2C%20and%20those%20indicative%20of%0Aviral%20pneumonia.%20Accurately%20diagnosing%20the%20disease%20at%20an%20early%20phase%20is%0Acritical.%20In%20this%20paper%2C%20five%20different%20pre-trained%20models%20will%20be%20tested%20on%0Athe%20Lung%20X-ray%20Image%20Dataset.%20SqueezeNet%2C%20VGG11%2C%20ResNet18%2C%20DenseNet%2C%20and%0AMobileNetV2%20achieved%20accuracies%20of%200.64%2C%200.85%2C%200.87%2C%200.88%2C%20and%200.885%2C%0Arespectively.%20MobileNetV2%2C%20as%20the%20best-performing%20pre-trained%20model%2C%20will%20then%0Abe%20further%20analyzed%20as%20the%20base%20model.%20Eventually%2C%20our%20own%20model%2C%0AMobileNet-Lung%20based%20on%20MobileNetV2%2C%20with%20fine-tuning%20and%20an%20additional%20layer%0Aof%20attention%20within%20feature%20layers%2C%20was%20invented%20to%20tackle%20the%20lung%20disease%0Aclassification%20task%20and%20achieved%20an%20accuracy%20of%200.933.%20This%20result%20is%0Asignificantly%20improved%20compared%20with%20all%20five%20pre-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13180v1&entry.124074799=Read"},
{"title": "On the design of scalable, high-precision spherical-radial Fourier\n  features", "author": "Ayoub Belhadji and Qianyu Julie Zhu and Youssef Marzouk", "abstract": "  Approximation using Fourier features is a popular technique for scaling\nkernel methods to large-scale problems, with myriad applications in machine\nlearning and statistics. This method replaces the integral representation of a\nshift-invariant kernel with a sum using a quadrature rule. The design of the\nlatter is meant to reduce the number of features required for high-precision\napproximation. Specifically, for the squared exponential kernel, one must\ndesign a quadrature rule that approximates the Gaussian measure on\n$\\mathbb{R}^d$. Previous efforts in this line of research have faced\ndifficulties in higher dimensions. We introduce a new family of quadrature\nrules that accurately approximate the Gaussian measure in higher dimensions by\nexploiting its isotropy. These rules are constructed as a tensor product of a\nradial quadrature rule and a spherical quadrature rule. Compared to previous\nwork, our approach leverages a thorough analysis of the approximation error,\nwhich suggests natural choices for both the radial and spherical components. We\ndemonstrate that this family of Fourier features yields improved approximation\nbounds.\n", "link": "http://arxiv.org/abs/2408.13231v1", "date": "2024-08-23", "relevancy": 1.8213, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4886}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4449}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20design%20of%20scalable%2C%20high-precision%20spherical-radial%20Fourier%0A%20%20features&body=Title%3A%20On%20the%20design%20of%20scalable%2C%20high-precision%20spherical-radial%20Fourier%0A%20%20features%0AAuthor%3A%20Ayoub%20Belhadji%20and%20Qianyu%20Julie%20Zhu%20and%20Youssef%20Marzouk%0AAbstract%3A%20%20%20Approximation%20using%20Fourier%20features%20is%20a%20popular%20technique%20for%20scaling%0Akernel%20methods%20to%20large-scale%20problems%2C%20with%20myriad%20applications%20in%20machine%0Alearning%20and%20statistics.%20This%20method%20replaces%20the%20integral%20representation%20of%20a%0Ashift-invariant%20kernel%20with%20a%20sum%20using%20a%20quadrature%20rule.%20The%20design%20of%20the%0Alatter%20is%20meant%20to%20reduce%20the%20number%20of%20features%20required%20for%20high-precision%0Aapproximation.%20Specifically%2C%20for%20the%20squared%20exponential%20kernel%2C%20one%20must%0Adesign%20a%20quadrature%20rule%20that%20approximates%20the%20Gaussian%20measure%20on%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20Previous%20efforts%20in%20this%20line%20of%20research%20have%20faced%0Adifficulties%20in%20higher%20dimensions.%20We%20introduce%20a%20new%20family%20of%20quadrature%0Arules%20that%20accurately%20approximate%20the%20Gaussian%20measure%20in%20higher%20dimensions%20by%0Aexploiting%20its%20isotropy.%20These%20rules%20are%20constructed%20as%20a%20tensor%20product%20of%20a%0Aradial%20quadrature%20rule%20and%20a%20spherical%20quadrature%20rule.%20Compared%20to%20previous%0Awork%2C%20our%20approach%20leverages%20a%20thorough%20analysis%20of%20the%20approximation%20error%2C%0Awhich%20suggests%20natural%20choices%20for%20both%20the%20radial%20and%20spherical%20components.%20We%0Ademonstrate%20that%20this%20family%20of%20Fourier%20features%20yields%20improved%20approximation%0Abounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520design%2520of%2520scalable%252C%2520high-precision%2520spherical-radial%2520Fourier%250A%2520%2520features%26entry.906535625%3DAyoub%2520Belhadji%2520and%2520Qianyu%2520Julie%2520Zhu%2520and%2520Youssef%2520Marzouk%26entry.1292438233%3D%2520%2520Approximation%2520using%2520Fourier%2520features%2520is%2520a%2520popular%2520technique%2520for%2520scaling%250Akernel%2520methods%2520to%2520large-scale%2520problems%252C%2520with%2520myriad%2520applications%2520in%2520machine%250Alearning%2520and%2520statistics.%2520This%2520method%2520replaces%2520the%2520integral%2520representation%2520of%2520a%250Ashift-invariant%2520kernel%2520with%2520a%2520sum%2520using%2520a%2520quadrature%2520rule.%2520The%2520design%2520of%2520the%250Alatter%2520is%2520meant%2520to%2520reduce%2520the%2520number%2520of%2520features%2520required%2520for%2520high-precision%250Aapproximation.%2520Specifically%252C%2520for%2520the%2520squared%2520exponential%2520kernel%252C%2520one%2520must%250Adesign%2520a%2520quadrature%2520rule%2520that%2520approximates%2520the%2520Gaussian%2520measure%2520on%250A%2524%255Cmathbb%257BR%257D%255Ed%2524.%2520Previous%2520efforts%2520in%2520this%2520line%2520of%2520research%2520have%2520faced%250Adifficulties%2520in%2520higher%2520dimensions.%2520We%2520introduce%2520a%2520new%2520family%2520of%2520quadrature%250Arules%2520that%2520accurately%2520approximate%2520the%2520Gaussian%2520measure%2520in%2520higher%2520dimensions%2520by%250Aexploiting%2520its%2520isotropy.%2520These%2520rules%2520are%2520constructed%2520as%2520a%2520tensor%2520product%2520of%2520a%250Aradial%2520quadrature%2520rule%2520and%2520a%2520spherical%2520quadrature%2520rule.%2520Compared%2520to%2520previous%250Awork%252C%2520our%2520approach%2520leverages%2520a%2520thorough%2520analysis%2520of%2520the%2520approximation%2520error%252C%250Awhich%2520suggests%2520natural%2520choices%2520for%2520both%2520the%2520radial%2520and%2520spherical%2520components.%2520We%250Ademonstrate%2520that%2520this%2520family%2520of%2520Fourier%2520features%2520yields%2520improved%2520approximation%250Abounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20design%20of%20scalable%2C%20high-precision%20spherical-radial%20Fourier%0A%20%20features&entry.906535625=Ayoub%20Belhadji%20and%20Qianyu%20Julie%20Zhu%20and%20Youssef%20Marzouk&entry.1292438233=%20%20Approximation%20using%20Fourier%20features%20is%20a%20popular%20technique%20for%20scaling%0Akernel%20methods%20to%20large-scale%20problems%2C%20with%20myriad%20applications%20in%20machine%0Alearning%20and%20statistics.%20This%20method%20replaces%20the%20integral%20representation%20of%20a%0Ashift-invariant%20kernel%20with%20a%20sum%20using%20a%20quadrature%20rule.%20The%20design%20of%20the%0Alatter%20is%20meant%20to%20reduce%20the%20number%20of%20features%20required%20for%20high-precision%0Aapproximation.%20Specifically%2C%20for%20the%20squared%20exponential%20kernel%2C%20one%20must%0Adesign%20a%20quadrature%20rule%20that%20approximates%20the%20Gaussian%20measure%20on%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20Previous%20efforts%20in%20this%20line%20of%20research%20have%20faced%0Adifficulties%20in%20higher%20dimensions.%20We%20introduce%20a%20new%20family%20of%20quadrature%0Arules%20that%20accurately%20approximate%20the%20Gaussian%20measure%20in%20higher%20dimensions%20by%0Aexploiting%20its%20isotropy.%20These%20rules%20are%20constructed%20as%20a%20tensor%20product%20of%20a%0Aradial%20quadrature%20rule%20and%20a%20spherical%20quadrature%20rule.%20Compared%20to%20previous%0Awork%2C%20our%20approach%20leverages%20a%20thorough%20analysis%20of%20the%20approximation%20error%2C%0Awhich%20suggests%20natural%20choices%20for%20both%20the%20radial%20and%20spherical%20components.%20We%0Ademonstrate%20that%20this%20family%20of%20Fourier%20features%20yields%20improved%20approximation%0Abounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13231v1&entry.124074799=Read"},
{"title": "A Survey on Drowsiness Detection -- Modern Applications and Methods", "author": "Biying Fu and Fadi Boutros and Chin-Teng Lin and Naser Damer", "abstract": "  Drowsiness detection holds paramount importance in ensuring safety in\nworkplaces or behind the wheel, enhancing productivity, and healthcare across\ndiverse domains. Therefore accurate and real-time drowsiness detection plays a\ncritical role in preventing accidents, enhancing safety, and ultimately saving\nlives across various sectors and scenarios. This comprehensive review explores\nthe significance of drowsiness detection in various areas of application,\ntranscending the conventional focus solely on driver drowsiness detection. We\ndelve into the current methodologies, challenges, and technological\nadvancements in drowsiness detection schemes, considering diverse contexts such\nas public transportation, healthcare, workplace safety, and beyond. By\nexamining the multifaceted implications of drowsiness, this work contributes to\na holistic understanding of its impact and the crucial role of accurate and\nreal-time detection techniques in enhancing safety and performance. We\nidentified weaknesses in current algorithms and limitations in existing\nresearch such as accurate and real-time detection, stable data transmission,\nand building bias-free systems. Our survey frames existing works and leads to\npractical recommendations like mitigating the bias issue by using synthetic\ndata, overcoming the hardware limitations with model compression, and\nleveraging fusion to boost model performance. This is a pioneering work to\nsurvey the topic of drowsiness detection in such an entirely and not only\nfocusing on one single aspect. We consider the topic of drowsiness detection as\na dynamic and evolving field, presenting numerous opportunities for further\nexploration.\n", "link": "http://arxiv.org/abs/2408.12990v1", "date": "2024-08-23", "relevancy": 1.8121, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4631}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4494}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Drowsiness%20Detection%20--%20Modern%20Applications%20and%20Methods&body=Title%3A%20A%20Survey%20on%20Drowsiness%20Detection%20--%20Modern%20Applications%20and%20Methods%0AAuthor%3A%20Biying%20Fu%20and%20Fadi%20Boutros%20and%20Chin-Teng%20Lin%20and%20Naser%20Damer%0AAbstract%3A%20%20%20Drowsiness%20detection%20holds%20paramount%20importance%20in%20ensuring%20safety%20in%0Aworkplaces%20or%20behind%20the%20wheel%2C%20enhancing%20productivity%2C%20and%20healthcare%20across%0Adiverse%20domains.%20Therefore%20accurate%20and%20real-time%20drowsiness%20detection%20plays%20a%0Acritical%20role%20in%20preventing%20accidents%2C%20enhancing%20safety%2C%20and%20ultimately%20saving%0Alives%20across%20various%20sectors%20and%20scenarios.%20This%20comprehensive%20review%20explores%0Athe%20significance%20of%20drowsiness%20detection%20in%20various%20areas%20of%20application%2C%0Atranscending%20the%20conventional%20focus%20solely%20on%20driver%20drowsiness%20detection.%20We%0Adelve%20into%20the%20current%20methodologies%2C%20challenges%2C%20and%20technological%0Aadvancements%20in%20drowsiness%20detection%20schemes%2C%20considering%20diverse%20contexts%20such%0Aas%20public%20transportation%2C%20healthcare%2C%20workplace%20safety%2C%20and%20beyond.%20By%0Aexamining%20the%20multifaceted%20implications%20of%20drowsiness%2C%20this%20work%20contributes%20to%0Aa%20holistic%20understanding%20of%20its%20impact%20and%20the%20crucial%20role%20of%20accurate%20and%0Areal-time%20detection%20techniques%20in%20enhancing%20safety%20and%20performance.%20We%0Aidentified%20weaknesses%20in%20current%20algorithms%20and%20limitations%20in%20existing%0Aresearch%20such%20as%20accurate%20and%20real-time%20detection%2C%20stable%20data%20transmission%2C%0Aand%20building%20bias-free%20systems.%20Our%20survey%20frames%20existing%20works%20and%20leads%20to%0Apractical%20recommendations%20like%20mitigating%20the%20bias%20issue%20by%20using%20synthetic%0Adata%2C%20overcoming%20the%20hardware%20limitations%20with%20model%20compression%2C%20and%0Aleveraging%20fusion%20to%20boost%20model%20performance.%20This%20is%20a%20pioneering%20work%20to%0Asurvey%20the%20topic%20of%20drowsiness%20detection%20in%20such%20an%20entirely%20and%20not%20only%0Afocusing%20on%20one%20single%20aspect.%20We%20consider%20the%20topic%20of%20drowsiness%20detection%20as%0Aa%20dynamic%20and%20evolving%20field%2C%20presenting%20numerous%20opportunities%20for%20further%0Aexploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Drowsiness%2520Detection%2520--%2520Modern%2520Applications%2520and%2520Methods%26entry.906535625%3DBiying%2520Fu%2520and%2520Fadi%2520Boutros%2520and%2520Chin-Teng%2520Lin%2520and%2520Naser%2520Damer%26entry.1292438233%3D%2520%2520Drowsiness%2520detection%2520holds%2520paramount%2520importance%2520in%2520ensuring%2520safety%2520in%250Aworkplaces%2520or%2520behind%2520the%2520wheel%252C%2520enhancing%2520productivity%252C%2520and%2520healthcare%2520across%250Adiverse%2520domains.%2520Therefore%2520accurate%2520and%2520real-time%2520drowsiness%2520detection%2520plays%2520a%250Acritical%2520role%2520in%2520preventing%2520accidents%252C%2520enhancing%2520safety%252C%2520and%2520ultimately%2520saving%250Alives%2520across%2520various%2520sectors%2520and%2520scenarios.%2520This%2520comprehensive%2520review%2520explores%250Athe%2520significance%2520of%2520drowsiness%2520detection%2520in%2520various%2520areas%2520of%2520application%252C%250Atranscending%2520the%2520conventional%2520focus%2520solely%2520on%2520driver%2520drowsiness%2520detection.%2520We%250Adelve%2520into%2520the%2520current%2520methodologies%252C%2520challenges%252C%2520and%2520technological%250Aadvancements%2520in%2520drowsiness%2520detection%2520schemes%252C%2520considering%2520diverse%2520contexts%2520such%250Aas%2520public%2520transportation%252C%2520healthcare%252C%2520workplace%2520safety%252C%2520and%2520beyond.%2520By%250Aexamining%2520the%2520multifaceted%2520implications%2520of%2520drowsiness%252C%2520this%2520work%2520contributes%2520to%250Aa%2520holistic%2520understanding%2520of%2520its%2520impact%2520and%2520the%2520crucial%2520role%2520of%2520accurate%2520and%250Areal-time%2520detection%2520techniques%2520in%2520enhancing%2520safety%2520and%2520performance.%2520We%250Aidentified%2520weaknesses%2520in%2520current%2520algorithms%2520and%2520limitations%2520in%2520existing%250Aresearch%2520such%2520as%2520accurate%2520and%2520real-time%2520detection%252C%2520stable%2520data%2520transmission%252C%250Aand%2520building%2520bias-free%2520systems.%2520Our%2520survey%2520frames%2520existing%2520works%2520and%2520leads%2520to%250Apractical%2520recommendations%2520like%2520mitigating%2520the%2520bias%2520issue%2520by%2520using%2520synthetic%250Adata%252C%2520overcoming%2520the%2520hardware%2520limitations%2520with%2520model%2520compression%252C%2520and%250Aleveraging%2520fusion%2520to%2520boost%2520model%2520performance.%2520This%2520is%2520a%2520pioneering%2520work%2520to%250Asurvey%2520the%2520topic%2520of%2520drowsiness%2520detection%2520in%2520such%2520an%2520entirely%2520and%2520not%2520only%250Afocusing%2520on%2520one%2520single%2520aspect.%2520We%2520consider%2520the%2520topic%2520of%2520drowsiness%2520detection%2520as%250Aa%2520dynamic%2520and%2520evolving%2520field%252C%2520presenting%2520numerous%2520opportunities%2520for%2520further%250Aexploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Drowsiness%20Detection%20--%20Modern%20Applications%20and%20Methods&entry.906535625=Biying%20Fu%20and%20Fadi%20Boutros%20and%20Chin-Teng%20Lin%20and%20Naser%20Damer&entry.1292438233=%20%20Drowsiness%20detection%20holds%20paramount%20importance%20in%20ensuring%20safety%20in%0Aworkplaces%20or%20behind%20the%20wheel%2C%20enhancing%20productivity%2C%20and%20healthcare%20across%0Adiverse%20domains.%20Therefore%20accurate%20and%20real-time%20drowsiness%20detection%20plays%20a%0Acritical%20role%20in%20preventing%20accidents%2C%20enhancing%20safety%2C%20and%20ultimately%20saving%0Alives%20across%20various%20sectors%20and%20scenarios.%20This%20comprehensive%20review%20explores%0Athe%20significance%20of%20drowsiness%20detection%20in%20various%20areas%20of%20application%2C%0Atranscending%20the%20conventional%20focus%20solely%20on%20driver%20drowsiness%20detection.%20We%0Adelve%20into%20the%20current%20methodologies%2C%20challenges%2C%20and%20technological%0Aadvancements%20in%20drowsiness%20detection%20schemes%2C%20considering%20diverse%20contexts%20such%0Aas%20public%20transportation%2C%20healthcare%2C%20workplace%20safety%2C%20and%20beyond.%20By%0Aexamining%20the%20multifaceted%20implications%20of%20drowsiness%2C%20this%20work%20contributes%20to%0Aa%20holistic%20understanding%20of%20its%20impact%20and%20the%20crucial%20role%20of%20accurate%20and%0Areal-time%20detection%20techniques%20in%20enhancing%20safety%20and%20performance.%20We%0Aidentified%20weaknesses%20in%20current%20algorithms%20and%20limitations%20in%20existing%0Aresearch%20such%20as%20accurate%20and%20real-time%20detection%2C%20stable%20data%20transmission%2C%0Aand%20building%20bias-free%20systems.%20Our%20survey%20frames%20existing%20works%20and%20leads%20to%0Apractical%20recommendations%20like%20mitigating%20the%20bias%20issue%20by%20using%20synthetic%0Adata%2C%20overcoming%20the%20hardware%20limitations%20with%20model%20compression%2C%20and%0Aleveraging%20fusion%20to%20boost%20model%20performance.%20This%20is%20a%20pioneering%20work%20to%0Asurvey%20the%20topic%20of%20drowsiness%20detection%20in%20such%20an%20entirely%20and%20not%20only%0Afocusing%20on%20one%20single%20aspect.%20We%20consider%20the%20topic%20of%20drowsiness%20detection%20as%0Aa%20dynamic%20and%20evolving%20field%2C%20presenting%20numerous%20opportunities%20for%20further%0Aexploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12990v1&entry.124074799=Read"},
{"title": "Policy Zooming: Adaptive Discretization-based Infinite-Horizon\n  Average-Reward Reinforcement Learning", "author": "Avik Kar and Rahul Singh", "abstract": "  We study infinite-horizon average-reward reinforcement learning (RL) for\nLipschitz MDPs and develop an algorithm PZRL that discretizes the state-action\nspace adaptively and zooms in to promising regions of the \"policy space\" which\nseems to yield high average rewards. We show that the regret of PZRL can be\nbounded as $\\tilde{\\mathcal{O}}\\big(T^{1 - d_{\\text{eff.}}^{-1}}\\big)$, where\n$d_{\\text{eff.}}= 2d_\\mathcal{S} + d^\\Phi_z+2$, $d_\\mathcal{S}$ is the\ndimension of the state space, and $d^\\Phi_z$ is the zooming dimension.\n$d^\\Phi_z$ is a problem-dependent quantity that depends not only on the\nunderlying MDP but also the class of policies $\\Phi$ used by the agent, which\nallows us to conclude that if the agent apriori knows that optimal policy\nbelongs to a low-complexity class (that has small $d^\\Phi_z$), then its regret\nwill be small. The current work shows how to capture adaptivity gains for\ninfinite-horizon average-reward RL in terms of $d^\\Phi_z$. We note that the\npreexisting notions of zooming dimension are adept at handling only the\nepisodic RL case since zooming dimension approaches covering dimension of\nstate-action space as $T\\to\\infty$ and hence do not yield any possible\nadaptivity gains. Several experiments are conducted to evaluate the performance\nof PZRL. PZRL outperforms other state-of-the-art algorithms; this clearly\ndemonstrates the gains arising due to adaptivity.\n", "link": "http://arxiv.org/abs/2405.18793v2", "date": "2024-08-23", "relevancy": 1.8107, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Policy%20Zooming%3A%20Adaptive%20Discretization-based%20Infinite-Horizon%0A%20%20Average-Reward%20Reinforcement%20Learning&body=Title%3A%20Policy%20Zooming%3A%20Adaptive%20Discretization-based%20Infinite-Horizon%0A%20%20Average-Reward%20Reinforcement%20Learning%0AAuthor%3A%20Avik%20Kar%20and%20Rahul%20Singh%0AAbstract%3A%20%20%20We%20study%20infinite-horizon%20average-reward%20reinforcement%20learning%20%28RL%29%20for%0ALipschitz%20MDPs%20and%20develop%20an%20algorithm%20PZRL%20that%20discretizes%20the%20state-action%0Aspace%20adaptively%20and%20zooms%20in%20to%20promising%20regions%20of%20the%20%22policy%20space%22%20which%0Aseems%20to%20yield%20high%20average%20rewards.%20We%20show%20that%20the%20regret%20of%20PZRL%20can%20be%0Abounded%20as%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%5Cbig%28T%5E%7B1%20-%20d_%7B%5Ctext%7Beff.%7D%7D%5E%7B-1%7D%7D%5Cbig%29%24%2C%20where%0A%24d_%7B%5Ctext%7Beff.%7D%7D%3D%202d_%5Cmathcal%7BS%7D%20%2B%20d%5E%5CPhi_z%2B2%24%2C%20%24d_%5Cmathcal%7BS%7D%24%20is%20the%0Adimension%20of%20the%20state%20space%2C%20and%20%24d%5E%5CPhi_z%24%20is%20the%20zooming%20dimension.%0A%24d%5E%5CPhi_z%24%20is%20a%20problem-dependent%20quantity%20that%20depends%20not%20only%20on%20the%0Aunderlying%20MDP%20but%20also%20the%20class%20of%20policies%20%24%5CPhi%24%20used%20by%20the%20agent%2C%20which%0Aallows%20us%20to%20conclude%20that%20if%20the%20agent%20apriori%20knows%20that%20optimal%20policy%0Abelongs%20to%20a%20low-complexity%20class%20%28that%20has%20small%20%24d%5E%5CPhi_z%24%29%2C%20then%20its%20regret%0Awill%20be%20small.%20The%20current%20work%20shows%20how%20to%20capture%20adaptivity%20gains%20for%0Ainfinite-horizon%20average-reward%20RL%20in%20terms%20of%20%24d%5E%5CPhi_z%24.%20We%20note%20that%20the%0Apreexisting%20notions%20of%20zooming%20dimension%20are%20adept%20at%20handling%20only%20the%0Aepisodic%20RL%20case%20since%20zooming%20dimension%20approaches%20covering%20dimension%20of%0Astate-action%20space%20as%20%24T%5Cto%5Cinfty%24%20and%20hence%20do%20not%20yield%20any%20possible%0Aadaptivity%20gains.%20Several%20experiments%20are%20conducted%20to%20evaluate%20the%20performance%0Aof%20PZRL.%20PZRL%20outperforms%20other%20state-of-the-art%20algorithms%3B%20this%20clearly%0Ademonstrates%20the%20gains%20arising%20due%20to%20adaptivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18793v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolicy%2520Zooming%253A%2520Adaptive%2520Discretization-based%2520Infinite-Horizon%250A%2520%2520Average-Reward%2520Reinforcement%2520Learning%26entry.906535625%3DAvik%2520Kar%2520and%2520Rahul%2520Singh%26entry.1292438233%3D%2520%2520We%2520study%2520infinite-horizon%2520average-reward%2520reinforcement%2520learning%2520%2528RL%2529%2520for%250ALipschitz%2520MDPs%2520and%2520develop%2520an%2520algorithm%2520PZRL%2520that%2520discretizes%2520the%2520state-action%250Aspace%2520adaptively%2520and%2520zooms%2520in%2520to%2520promising%2520regions%2520of%2520the%2520%2522policy%2520space%2522%2520which%250Aseems%2520to%2520yield%2520high%2520average%2520rewards.%2520We%2520show%2520that%2520the%2520regret%2520of%2520PZRL%2520can%2520be%250Abounded%2520as%2520%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%255Cbig%2528T%255E%257B1%2520-%2520d_%257B%255Ctext%257Beff.%257D%257D%255E%257B-1%257D%257D%255Cbig%2529%2524%252C%2520where%250A%2524d_%257B%255Ctext%257Beff.%257D%257D%253D%25202d_%255Cmathcal%257BS%257D%2520%252B%2520d%255E%255CPhi_z%252B2%2524%252C%2520%2524d_%255Cmathcal%257BS%257D%2524%2520is%2520the%250Adimension%2520of%2520the%2520state%2520space%252C%2520and%2520%2524d%255E%255CPhi_z%2524%2520is%2520the%2520zooming%2520dimension.%250A%2524d%255E%255CPhi_z%2524%2520is%2520a%2520problem-dependent%2520quantity%2520that%2520depends%2520not%2520only%2520on%2520the%250Aunderlying%2520MDP%2520but%2520also%2520the%2520class%2520of%2520policies%2520%2524%255CPhi%2524%2520used%2520by%2520the%2520agent%252C%2520which%250Aallows%2520us%2520to%2520conclude%2520that%2520if%2520the%2520agent%2520apriori%2520knows%2520that%2520optimal%2520policy%250Abelongs%2520to%2520a%2520low-complexity%2520class%2520%2528that%2520has%2520small%2520%2524d%255E%255CPhi_z%2524%2529%252C%2520then%2520its%2520regret%250Awill%2520be%2520small.%2520The%2520current%2520work%2520shows%2520how%2520to%2520capture%2520adaptivity%2520gains%2520for%250Ainfinite-horizon%2520average-reward%2520RL%2520in%2520terms%2520of%2520%2524d%255E%255CPhi_z%2524.%2520We%2520note%2520that%2520the%250Apreexisting%2520notions%2520of%2520zooming%2520dimension%2520are%2520adept%2520at%2520handling%2520only%2520the%250Aepisodic%2520RL%2520case%2520since%2520zooming%2520dimension%2520approaches%2520covering%2520dimension%2520of%250Astate-action%2520space%2520as%2520%2524T%255Cto%255Cinfty%2524%2520and%2520hence%2520do%2520not%2520yield%2520any%2520possible%250Aadaptivity%2520gains.%2520Several%2520experiments%2520are%2520conducted%2520to%2520evaluate%2520the%2520performance%250Aof%2520PZRL.%2520PZRL%2520outperforms%2520other%2520state-of-the-art%2520algorithms%253B%2520this%2520clearly%250Ademonstrates%2520the%2520gains%2520arising%2520due%2520to%2520adaptivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18793v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy%20Zooming%3A%20Adaptive%20Discretization-based%20Infinite-Horizon%0A%20%20Average-Reward%20Reinforcement%20Learning&entry.906535625=Avik%20Kar%20and%20Rahul%20Singh&entry.1292438233=%20%20We%20study%20infinite-horizon%20average-reward%20reinforcement%20learning%20%28RL%29%20for%0ALipschitz%20MDPs%20and%20develop%20an%20algorithm%20PZRL%20that%20discretizes%20the%20state-action%0Aspace%20adaptively%20and%20zooms%20in%20to%20promising%20regions%20of%20the%20%22policy%20space%22%20which%0Aseems%20to%20yield%20high%20average%20rewards.%20We%20show%20that%20the%20regret%20of%20PZRL%20can%20be%0Abounded%20as%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%5Cbig%28T%5E%7B1%20-%20d_%7B%5Ctext%7Beff.%7D%7D%5E%7B-1%7D%7D%5Cbig%29%24%2C%20where%0A%24d_%7B%5Ctext%7Beff.%7D%7D%3D%202d_%5Cmathcal%7BS%7D%20%2B%20d%5E%5CPhi_z%2B2%24%2C%20%24d_%5Cmathcal%7BS%7D%24%20is%20the%0Adimension%20of%20the%20state%20space%2C%20and%20%24d%5E%5CPhi_z%24%20is%20the%20zooming%20dimension.%0A%24d%5E%5CPhi_z%24%20is%20a%20problem-dependent%20quantity%20that%20depends%20not%20only%20on%20the%0Aunderlying%20MDP%20but%20also%20the%20class%20of%20policies%20%24%5CPhi%24%20used%20by%20the%20agent%2C%20which%0Aallows%20us%20to%20conclude%20that%20if%20the%20agent%20apriori%20knows%20that%20optimal%20policy%0Abelongs%20to%20a%20low-complexity%20class%20%28that%20has%20small%20%24d%5E%5CPhi_z%24%29%2C%20then%20its%20regret%0Awill%20be%20small.%20The%20current%20work%20shows%20how%20to%20capture%20adaptivity%20gains%20for%0Ainfinite-horizon%20average-reward%20RL%20in%20terms%20of%20%24d%5E%5CPhi_z%24.%20We%20note%20that%20the%0Apreexisting%20notions%20of%20zooming%20dimension%20are%20adept%20at%20handling%20only%20the%0Aepisodic%20RL%20case%20since%20zooming%20dimension%20approaches%20covering%20dimension%20of%0Astate-action%20space%20as%20%24T%5Cto%5Cinfty%24%20and%20hence%20do%20not%20yield%20any%20possible%0Aadaptivity%20gains.%20Several%20experiments%20are%20conducted%20to%20evaluate%20the%20performance%0Aof%20PZRL.%20PZRL%20outperforms%20other%20state-of-the-art%20algorithms%3B%20this%20clearly%0Ademonstrates%20the%20gains%20arising%20due%20to%20adaptivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18793v2&entry.124074799=Read"},
{"title": "Augmented Functional Random Forests: Classifier Construction and\n  Unbiased Functional Principal Components Importance through Ad-Hoc\n  Conditional Permutations", "author": "Fabrizio Maturo and Annamaria Porreca", "abstract": "  This paper introduces a novel supervised classification strategy that\nintegrates functional data analysis (FDA) with tree-based methods, addressing\nthe challenges of high-dimensional data and enhancing the classification\nperformance of existing functional classifiers. Specifically, we propose\naugmented versions of functional classification trees and functional random\nforests, incorporating a new tool for assessing the importance of functional\nprincipal components. This tool provides an ad-hoc method for determining\nunbiased permutation feature importance in functional data, particularly when\ndealing with correlated features derived from successive derivatives. Our study\ndemonstrates that these additional features can significantly enhance the\npredictive power of functional classifiers. Experimental evaluations on both\nreal-world and simulated datasets showcase the effectiveness of the proposed\nmethodology, yielding promising results compared to existing methods.\n", "link": "http://arxiv.org/abs/2408.13179v1", "date": "2024-08-23", "relevancy": 1.81, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.48}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4691}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmented%20Functional%20Random%20Forests%3A%20Classifier%20Construction%20and%0A%20%20Unbiased%20Functional%20Principal%20Components%20Importance%20through%20Ad-Hoc%0A%20%20Conditional%20Permutations&body=Title%3A%20Augmented%20Functional%20Random%20Forests%3A%20Classifier%20Construction%20and%0A%20%20Unbiased%20Functional%20Principal%20Components%20Importance%20through%20Ad-Hoc%0A%20%20Conditional%20Permutations%0AAuthor%3A%20Fabrizio%20Maturo%20and%20Annamaria%20Porreca%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20supervised%20classification%20strategy%20that%0Aintegrates%20functional%20data%20analysis%20%28FDA%29%20with%20tree-based%20methods%2C%20addressing%0Athe%20challenges%20of%20high-dimensional%20data%20and%20enhancing%20the%20classification%0Aperformance%20of%20existing%20functional%20classifiers.%20Specifically%2C%20we%20propose%0Aaugmented%20versions%20of%20functional%20classification%20trees%20and%20functional%20random%0Aforests%2C%20incorporating%20a%20new%20tool%20for%20assessing%20the%20importance%20of%20functional%0Aprincipal%20components.%20This%20tool%20provides%20an%20ad-hoc%20method%20for%20determining%0Aunbiased%20permutation%20feature%20importance%20in%20functional%20data%2C%20particularly%20when%0Adealing%20with%20correlated%20features%20derived%20from%20successive%20derivatives.%20Our%20study%0Ademonstrates%20that%20these%20additional%20features%20can%20significantly%20enhance%20the%0Apredictive%20power%20of%20functional%20classifiers.%20Experimental%20evaluations%20on%20both%0Areal-world%20and%20simulated%20datasets%20showcase%20the%20effectiveness%20of%20the%20proposed%0Amethodology%2C%20yielding%20promising%20results%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmented%2520Functional%2520Random%2520Forests%253A%2520Classifier%2520Construction%2520and%250A%2520%2520Unbiased%2520Functional%2520Principal%2520Components%2520Importance%2520through%2520Ad-Hoc%250A%2520%2520Conditional%2520Permutations%26entry.906535625%3DFabrizio%2520Maturo%2520and%2520Annamaria%2520Porreca%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520supervised%2520classification%2520strategy%2520that%250Aintegrates%2520functional%2520data%2520analysis%2520%2528FDA%2529%2520with%2520tree-based%2520methods%252C%2520addressing%250Athe%2520challenges%2520of%2520high-dimensional%2520data%2520and%2520enhancing%2520the%2520classification%250Aperformance%2520of%2520existing%2520functional%2520classifiers.%2520Specifically%252C%2520we%2520propose%250Aaugmented%2520versions%2520of%2520functional%2520classification%2520trees%2520and%2520functional%2520random%250Aforests%252C%2520incorporating%2520a%2520new%2520tool%2520for%2520assessing%2520the%2520importance%2520of%2520functional%250Aprincipal%2520components.%2520This%2520tool%2520provides%2520an%2520ad-hoc%2520method%2520for%2520determining%250Aunbiased%2520permutation%2520feature%2520importance%2520in%2520functional%2520data%252C%2520particularly%2520when%250Adealing%2520with%2520correlated%2520features%2520derived%2520from%2520successive%2520derivatives.%2520Our%2520study%250Ademonstrates%2520that%2520these%2520additional%2520features%2520can%2520significantly%2520enhance%2520the%250Apredictive%2520power%2520of%2520functional%2520classifiers.%2520Experimental%2520evaluations%2520on%2520both%250Areal-world%2520and%2520simulated%2520datasets%2520showcase%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethodology%252C%2520yielding%2520promising%2520results%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmented%20Functional%20Random%20Forests%3A%20Classifier%20Construction%20and%0A%20%20Unbiased%20Functional%20Principal%20Components%20Importance%20through%20Ad-Hoc%0A%20%20Conditional%20Permutations&entry.906535625=Fabrizio%20Maturo%20and%20Annamaria%20Porreca&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20supervised%20classification%20strategy%20that%0Aintegrates%20functional%20data%20analysis%20%28FDA%29%20with%20tree-based%20methods%2C%20addressing%0Athe%20challenges%20of%20high-dimensional%20data%20and%20enhancing%20the%20classification%0Aperformance%20of%20existing%20functional%20classifiers.%20Specifically%2C%20we%20propose%0Aaugmented%20versions%20of%20functional%20classification%20trees%20and%20functional%20random%0Aforests%2C%20incorporating%20a%20new%20tool%20for%20assessing%20the%20importance%20of%20functional%0Aprincipal%20components.%20This%20tool%20provides%20an%20ad-hoc%20method%20for%20determining%0Aunbiased%20permutation%20feature%20importance%20in%20functional%20data%2C%20particularly%20when%0Adealing%20with%20correlated%20features%20derived%20from%20successive%20derivatives.%20Our%20study%0Ademonstrates%20that%20these%20additional%20features%20can%20significantly%20enhance%20the%0Apredictive%20power%20of%20functional%20classifiers.%20Experimental%20evaluations%20on%20both%0Areal-world%20and%20simulated%20datasets%20showcase%20the%20effectiveness%20of%20the%20proposed%0Amethodology%2C%20yielding%20promising%20results%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13179v1&entry.124074799=Read"},
{"title": "Functional Tensor Decompositions for Physics-Informed Neural Networks", "author": "Sai Karthikeya Vemuri and Tim B\u00fcchner and Julia Niebling and Joachim Denzler", "abstract": "  Physics-Informed Neural Networks (PINNs) have shown continuous and increasing\npromise in approximating partial differential equations (PDEs), although they\nremain constrained by the curse of dimensionality. In this paper, we propose a\ngeneralized PINN version of the classical variable separable method. To do\nthis, we first show that, using the universal approximation theorem, a\nmultivariate function can be approximated by the outer product of neural\nnetworks, whose inputs are separated variables. We leverage tensor\ndecomposition forms to separate the variables in a PINN setting. By employing\nCanonic Polyadic (CP), Tensor-Train (TT), and Tucker decomposition forms within\nthe PINN framework, we create robust architectures for learning multivariate\nfunctions from separate neural networks connected by outer products. Our\nmethodology significantly enhances the performance of PINNs, as evidenced by\nimproved results on complex high-dimensional PDEs, including the 3d Helmholtz\nand 5d Poisson equations, among others. This research underscores the potential\nof tensor decomposition-based variably separated PINNs to surpass the\nstate-of-the-art, offering a compelling solution to the dimensionality\nchallenge in PDE approximation.\n", "link": "http://arxiv.org/abs/2408.13101v1", "date": "2024-08-23", "relevancy": 1.7967, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4732}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.445}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functional%20Tensor%20Decompositions%20for%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Functional%20Tensor%20Decompositions%20for%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Sai%20Karthikeya%20Vemuri%20and%20Tim%20B%C3%BCchner%20and%20Julia%20Niebling%20and%20Joachim%20Denzler%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20have%20shown%20continuous%20and%20increasing%0Apromise%20in%20approximating%20partial%20differential%20equations%20%28PDEs%29%2C%20although%20they%0Aremain%20constrained%20by%20the%20curse%20of%20dimensionality.%20In%20this%20paper%2C%20we%20propose%20a%0Ageneralized%20PINN%20version%20of%20the%20classical%20variable%20separable%20method.%20To%20do%0Athis%2C%20we%20first%20show%20that%2C%20using%20the%20universal%20approximation%20theorem%2C%20a%0Amultivariate%20function%20can%20be%20approximated%20by%20the%20outer%20product%20of%20neural%0Anetworks%2C%20whose%20inputs%20are%20separated%20variables.%20We%20leverage%20tensor%0Adecomposition%20forms%20to%20separate%20the%20variables%20in%20a%20PINN%20setting.%20By%20employing%0ACanonic%20Polyadic%20%28CP%29%2C%20Tensor-Train%20%28TT%29%2C%20and%20Tucker%20decomposition%20forms%20within%0Athe%20PINN%20framework%2C%20we%20create%20robust%20architectures%20for%20learning%20multivariate%0Afunctions%20from%20separate%20neural%20networks%20connected%20by%20outer%20products.%20Our%0Amethodology%20significantly%20enhances%20the%20performance%20of%20PINNs%2C%20as%20evidenced%20by%0Aimproved%20results%20on%20complex%20high-dimensional%20PDEs%2C%20including%20the%203d%20Helmholtz%0Aand%205d%20Poisson%20equations%2C%20among%20others.%20This%20research%20underscores%20the%20potential%0Aof%20tensor%20decomposition-based%20variably%20separated%20PINNs%20to%20surpass%20the%0Astate-of-the-art%2C%20offering%20a%20compelling%20solution%20to%20the%20dimensionality%0Achallenge%20in%20PDE%20approximation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctional%2520Tensor%2520Decompositions%2520for%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DSai%2520Karthikeya%2520Vemuri%2520and%2520Tim%2520B%25C3%25BCchner%2520and%2520Julia%2520Niebling%2520and%2520Joachim%2520Denzler%26entry.1292438233%3D%2520%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520have%2520shown%2520continuous%2520and%2520increasing%250Apromise%2520in%2520approximating%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520although%2520they%250Aremain%2520constrained%2520by%2520the%2520curse%2520of%2520dimensionality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ageneralized%2520PINN%2520version%2520of%2520the%2520classical%2520variable%2520separable%2520method.%2520To%2520do%250Athis%252C%2520we%2520first%2520show%2520that%252C%2520using%2520the%2520universal%2520approximation%2520theorem%252C%2520a%250Amultivariate%2520function%2520can%2520be%2520approximated%2520by%2520the%2520outer%2520product%2520of%2520neural%250Anetworks%252C%2520whose%2520inputs%2520are%2520separated%2520variables.%2520We%2520leverage%2520tensor%250Adecomposition%2520forms%2520to%2520separate%2520the%2520variables%2520in%2520a%2520PINN%2520setting.%2520By%2520employing%250ACanonic%2520Polyadic%2520%2528CP%2529%252C%2520Tensor-Train%2520%2528TT%2529%252C%2520and%2520Tucker%2520decomposition%2520forms%2520within%250Athe%2520PINN%2520framework%252C%2520we%2520create%2520robust%2520architectures%2520for%2520learning%2520multivariate%250Afunctions%2520from%2520separate%2520neural%2520networks%2520connected%2520by%2520outer%2520products.%2520Our%250Amethodology%2520significantly%2520enhances%2520the%2520performance%2520of%2520PINNs%252C%2520as%2520evidenced%2520by%250Aimproved%2520results%2520on%2520complex%2520high-dimensional%2520PDEs%252C%2520including%2520the%25203d%2520Helmholtz%250Aand%25205d%2520Poisson%2520equations%252C%2520among%2520others.%2520This%2520research%2520underscores%2520the%2520potential%250Aof%2520tensor%2520decomposition-based%2520variably%2520separated%2520PINNs%2520to%2520surpass%2520the%250Astate-of-the-art%252C%2520offering%2520a%2520compelling%2520solution%2520to%2520the%2520dimensionality%250Achallenge%2520in%2520PDE%2520approximation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functional%20Tensor%20Decompositions%20for%20Physics-Informed%20Neural%20Networks&entry.906535625=Sai%20Karthikeya%20Vemuri%20and%20Tim%20B%C3%BCchner%20and%20Julia%20Niebling%20and%20Joachim%20Denzler&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20have%20shown%20continuous%20and%20increasing%0Apromise%20in%20approximating%20partial%20differential%20equations%20%28PDEs%29%2C%20although%20they%0Aremain%20constrained%20by%20the%20curse%20of%20dimensionality.%20In%20this%20paper%2C%20we%20propose%20a%0Ageneralized%20PINN%20version%20of%20the%20classical%20variable%20separable%20method.%20To%20do%0Athis%2C%20we%20first%20show%20that%2C%20using%20the%20universal%20approximation%20theorem%2C%20a%0Amultivariate%20function%20can%20be%20approximated%20by%20the%20outer%20product%20of%20neural%0Anetworks%2C%20whose%20inputs%20are%20separated%20variables.%20We%20leverage%20tensor%0Adecomposition%20forms%20to%20separate%20the%20variables%20in%20a%20PINN%20setting.%20By%20employing%0ACanonic%20Polyadic%20%28CP%29%2C%20Tensor-Train%20%28TT%29%2C%20and%20Tucker%20decomposition%20forms%20within%0Athe%20PINN%20framework%2C%20we%20create%20robust%20architectures%20for%20learning%20multivariate%0Afunctions%20from%20separate%20neural%20networks%20connected%20by%20outer%20products.%20Our%0Amethodology%20significantly%20enhances%20the%20performance%20of%20PINNs%2C%20as%20evidenced%20by%0Aimproved%20results%20on%20complex%20high-dimensional%20PDEs%2C%20including%20the%203d%20Helmholtz%0Aand%205d%20Poisson%20equations%2C%20among%20others.%20This%20research%20underscores%20the%20potential%0Aof%20tensor%20decomposition-based%20variably%20separated%20PINNs%20to%20surpass%20the%0Astate-of-the-art%2C%20offering%20a%20compelling%20solution%20to%20the%20dimensionality%0Achallenge%20in%20PDE%20approximation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13101v1&entry.124074799=Read"},
{"title": "PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects\n  and Environments", "author": "Kairui Ding and Boyuan Chen and Ruihai Wu and Yuyang Li and Zongzheng Zhang and Huan-ang Gao and Siqi Li and Guyue Zhou and Yixin Zhu and Hao Dong and Hao Zhao", "abstract": "  Robotic manipulation with two-finger grippers is challenged by objects\nlacking distinct graspable features. Traditional pre-grasping methods, which\ntypically involve repositioning objects or utilizing external aids like table\nedges, are limited in their adaptability across different object categories and\nenvironments. To overcome these limitations, we introduce PreAfford, a novel\npre-grasping planning framework incorporating a point-level affordance\nrepresentation and a relay training approach. Our method significantly improves\nadaptability, allowing effective manipulation across a wide range of\nenvironments and object types. When evaluated on the ShapeNet-v2 dataset,\nPreAfford not only enhances grasping success rates by 69% but also demonstrates\nits practicality through successful real-world experiments. These improvements\nhighlight PreAfford's potential to redefine standards for robotic handling of\ncomplex manipulation tasks in diverse settings.\n", "link": "http://arxiv.org/abs/2404.03634v3", "date": "2024-08-23", "relevancy": 1.7831, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6118}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6029}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PreAfford%3A%20Universal%20Affordance-Based%20Pre-Grasping%20for%20Diverse%20Objects%0A%20%20and%20Environments&body=Title%3A%20PreAfford%3A%20Universal%20Affordance-Based%20Pre-Grasping%20for%20Diverse%20Objects%0A%20%20and%20Environments%0AAuthor%3A%20Kairui%20Ding%20and%20Boyuan%20Chen%20and%20Ruihai%20Wu%20and%20Yuyang%20Li%20and%20Zongzheng%20Zhang%20and%20Huan-ang%20Gao%20and%20Siqi%20Li%20and%20Guyue%20Zhou%20and%20Yixin%20Zhu%20and%20Hao%20Dong%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Robotic%20manipulation%20with%20two-finger%20grippers%20is%20challenged%20by%20objects%0Alacking%20distinct%20graspable%20features.%20Traditional%20pre-grasping%20methods%2C%20which%0Atypically%20involve%20repositioning%20objects%20or%20utilizing%20external%20aids%20like%20table%0Aedges%2C%20are%20limited%20in%20their%20adaptability%20across%20different%20object%20categories%20and%0Aenvironments.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20PreAfford%2C%20a%20novel%0Apre-grasping%20planning%20framework%20incorporating%20a%20point-level%20affordance%0Arepresentation%20and%20a%20relay%20training%20approach.%20Our%20method%20significantly%20improves%0Aadaptability%2C%20allowing%20effective%20manipulation%20across%20a%20wide%20range%20of%0Aenvironments%20and%20object%20types.%20When%20evaluated%20on%20the%20ShapeNet-v2%20dataset%2C%0APreAfford%20not%20only%20enhances%20grasping%20success%20rates%20by%2069%25%20but%20also%20demonstrates%0Aits%20practicality%20through%20successful%20real-world%20experiments.%20These%20improvements%0Ahighlight%20PreAfford%27s%20potential%20to%20redefine%20standards%20for%20robotic%20handling%20of%0Acomplex%20manipulation%20tasks%20in%20diverse%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03634v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreAfford%253A%2520Universal%2520Affordance-Based%2520Pre-Grasping%2520for%2520Diverse%2520Objects%250A%2520%2520and%2520Environments%26entry.906535625%3DKairui%2520Ding%2520and%2520Boyuan%2520Chen%2520and%2520Ruihai%2520Wu%2520and%2520Yuyang%2520Li%2520and%2520Zongzheng%2520Zhang%2520and%2520Huan-ang%2520Gao%2520and%2520Siqi%2520Li%2520and%2520Guyue%2520Zhou%2520and%2520Yixin%2520Zhu%2520and%2520Hao%2520Dong%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%2520with%2520two-finger%2520grippers%2520is%2520challenged%2520by%2520objects%250Alacking%2520distinct%2520graspable%2520features.%2520Traditional%2520pre-grasping%2520methods%252C%2520which%250Atypically%2520involve%2520repositioning%2520objects%2520or%2520utilizing%2520external%2520aids%2520like%2520table%250Aedges%252C%2520are%2520limited%2520in%2520their%2520adaptability%2520across%2520different%2520object%2520categories%2520and%250Aenvironments.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520PreAfford%252C%2520a%2520novel%250Apre-grasping%2520planning%2520framework%2520incorporating%2520a%2520point-level%2520affordance%250Arepresentation%2520and%2520a%2520relay%2520training%2520approach.%2520Our%2520method%2520significantly%2520improves%250Aadaptability%252C%2520allowing%2520effective%2520manipulation%2520across%2520a%2520wide%2520range%2520of%250Aenvironments%2520and%2520object%2520types.%2520When%2520evaluated%2520on%2520the%2520ShapeNet-v2%2520dataset%252C%250APreAfford%2520not%2520only%2520enhances%2520grasping%2520success%2520rates%2520by%252069%2525%2520but%2520also%2520demonstrates%250Aits%2520practicality%2520through%2520successful%2520real-world%2520experiments.%2520These%2520improvements%250Ahighlight%2520PreAfford%2527s%2520potential%2520to%2520redefine%2520standards%2520for%2520robotic%2520handling%2520of%250Acomplex%2520manipulation%2520tasks%2520in%2520diverse%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03634v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PreAfford%3A%20Universal%20Affordance-Based%20Pre-Grasping%20for%20Diverse%20Objects%0A%20%20and%20Environments&entry.906535625=Kairui%20Ding%20and%20Boyuan%20Chen%20and%20Ruihai%20Wu%20and%20Yuyang%20Li%20and%20Zongzheng%20Zhang%20and%20Huan-ang%20Gao%20and%20Siqi%20Li%20and%20Guyue%20Zhou%20and%20Yixin%20Zhu%20and%20Hao%20Dong%20and%20Hao%20Zhao&entry.1292438233=%20%20Robotic%20manipulation%20with%20two-finger%20grippers%20is%20challenged%20by%20objects%0Alacking%20distinct%20graspable%20features.%20Traditional%20pre-grasping%20methods%2C%20which%0Atypically%20involve%20repositioning%20objects%20or%20utilizing%20external%20aids%20like%20table%0Aedges%2C%20are%20limited%20in%20their%20adaptability%20across%20different%20object%20categories%20and%0Aenvironments.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20PreAfford%2C%20a%20novel%0Apre-grasping%20planning%20framework%20incorporating%20a%20point-level%20affordance%0Arepresentation%20and%20a%20relay%20training%20approach.%20Our%20method%20significantly%20improves%0Aadaptability%2C%20allowing%20effective%20manipulation%20across%20a%20wide%20range%20of%0Aenvironments%20and%20object%20types.%20When%20evaluated%20on%20the%20ShapeNet-v2%20dataset%2C%0APreAfford%20not%20only%20enhances%20grasping%20success%20rates%20by%2069%25%20but%20also%20demonstrates%0Aits%20practicality%20through%20successful%20real-world%20experiments.%20These%20improvements%0Ahighlight%20PreAfford%27s%20potential%20to%20redefine%20standards%20for%20robotic%20handling%20of%0Acomplex%20manipulation%20tasks%20in%20diverse%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03634v3&entry.124074799=Read"},
{"title": "Imitation Learning in Discounted Linear MDPs without exploration\n  assumptions", "author": "Luca Viano and Stratis Skoulakis and Volkan Cevher", "abstract": "  We present a new algorithm for imitation learning in infinite horizon linear\nMDPs dubbed ILARL which greatly improves the bound on the number of\ntrajectories that the learner needs to sample from the environment. In\nparticular, we remove exploration assumptions required in previous works and we\nimprove the dependence on the desired accuracy $\\epsilon$ from\n$\\mathcal{O}(\\epsilon^{-5})$ to $\\mathcal{O}(\\epsilon^{-4})$. Our result relies\non a connection between imitation learning and online learning in MDPs with\nadversarial losses. For the latter setting, we present the first result for\ninfinite horizon linear MDP which may be of independent interest. Moreover, we\nare able to provide a strengthen result for the finite horizon case where we\nachieve $\\mathcal{O}(\\epsilon^{-2})$. Numerical experiments with linear\nfunction approximation shows that ILARL outperforms other commonly used\nalgorithms.\n", "link": "http://arxiv.org/abs/2405.02181v2", "date": "2024-08-23", "relevancy": 1.7752, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4458}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20Learning%20in%20Discounted%20Linear%20MDPs%20without%20exploration%0A%20%20assumptions&body=Title%3A%20Imitation%20Learning%20in%20Discounted%20Linear%20MDPs%20without%20exploration%0A%20%20assumptions%0AAuthor%3A%20Luca%20Viano%20and%20Stratis%20Skoulakis%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20We%20present%20a%20new%20algorithm%20for%20imitation%20learning%20in%20infinite%20horizon%20linear%0AMDPs%20dubbed%20ILARL%20which%20greatly%20improves%20the%20bound%20on%20the%20number%20of%0Atrajectories%20that%20the%20learner%20needs%20to%20sample%20from%20the%20environment.%20In%0Aparticular%2C%20we%20remove%20exploration%20assumptions%20required%20in%20previous%20works%20and%20we%0Aimprove%20the%20dependence%20on%20the%20desired%20accuracy%20%24%5Cepsilon%24%20from%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-5%7D%29%24%20to%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-4%7D%29%24.%20Our%20result%20relies%0Aon%20a%20connection%20between%20imitation%20learning%20and%20online%20learning%20in%20MDPs%20with%0Aadversarial%20losses.%20For%20the%20latter%20setting%2C%20we%20present%20the%20first%20result%20for%0Ainfinite%20horizon%20linear%20MDP%20which%20may%20be%20of%20independent%20interest.%20Moreover%2C%20we%0Aare%20able%20to%20provide%20a%20strengthen%20result%20for%20the%20finite%20horizon%20case%20where%20we%0Aachieve%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24.%20Numerical%20experiments%20with%20linear%0Afunction%20approximation%20shows%20that%20ILARL%20outperforms%20other%20commonly%20used%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02181v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520Learning%2520in%2520Discounted%2520Linear%2520MDPs%2520without%2520exploration%250A%2520%2520assumptions%26entry.906535625%3DLuca%2520Viano%2520and%2520Stratis%2520Skoulakis%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520algorithm%2520for%2520imitation%2520learning%2520in%2520infinite%2520horizon%2520linear%250AMDPs%2520dubbed%2520ILARL%2520which%2520greatly%2520improves%2520the%2520bound%2520on%2520the%2520number%2520of%250Atrajectories%2520that%2520the%2520learner%2520needs%2520to%2520sample%2520from%2520the%2520environment.%2520In%250Aparticular%252C%2520we%2520remove%2520exploration%2520assumptions%2520required%2520in%2520previous%2520works%2520and%2520we%250Aimprove%2520the%2520dependence%2520on%2520the%2520desired%2520accuracy%2520%2524%255Cepsilon%2524%2520from%250A%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-5%257D%2529%2524%2520to%2520%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-4%257D%2529%2524.%2520Our%2520result%2520relies%250Aon%2520a%2520connection%2520between%2520imitation%2520learning%2520and%2520online%2520learning%2520in%2520MDPs%2520with%250Aadversarial%2520losses.%2520For%2520the%2520latter%2520setting%252C%2520we%2520present%2520the%2520first%2520result%2520for%250Ainfinite%2520horizon%2520linear%2520MDP%2520which%2520may%2520be%2520of%2520independent%2520interest.%2520Moreover%252C%2520we%250Aare%2520able%2520to%2520provide%2520a%2520strengthen%2520result%2520for%2520the%2520finite%2520horizon%2520case%2520where%2520we%250Aachieve%2520%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-2%257D%2529%2524.%2520Numerical%2520experiments%2520with%2520linear%250Afunction%2520approximation%2520shows%2520that%2520ILARL%2520outperforms%2520other%2520commonly%2520used%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02181v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%20in%20Discounted%20Linear%20MDPs%20without%20exploration%0A%20%20assumptions&entry.906535625=Luca%20Viano%20and%20Stratis%20Skoulakis%20and%20Volkan%20Cevher&entry.1292438233=%20%20We%20present%20a%20new%20algorithm%20for%20imitation%20learning%20in%20infinite%20horizon%20linear%0AMDPs%20dubbed%20ILARL%20which%20greatly%20improves%20the%20bound%20on%20the%20number%20of%0Atrajectories%20that%20the%20learner%20needs%20to%20sample%20from%20the%20environment.%20In%0Aparticular%2C%20we%20remove%20exploration%20assumptions%20required%20in%20previous%20works%20and%20we%0Aimprove%20the%20dependence%20on%20the%20desired%20accuracy%20%24%5Cepsilon%24%20from%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-5%7D%29%24%20to%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-4%7D%29%24.%20Our%20result%20relies%0Aon%20a%20connection%20between%20imitation%20learning%20and%20online%20learning%20in%20MDPs%20with%0Aadversarial%20losses.%20For%20the%20latter%20setting%2C%20we%20present%20the%20first%20result%20for%0Ainfinite%20horizon%20linear%20MDP%20which%20may%20be%20of%20independent%20interest.%20Moreover%2C%20we%0Aare%20able%20to%20provide%20a%20strengthen%20result%20for%20the%20finite%20horizon%20case%20where%20we%0Aachieve%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24.%20Numerical%20experiments%20with%20linear%0Afunction%20approximation%20shows%20that%20ILARL%20outperforms%20other%20commonly%20used%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02181v2&entry.124074799=Read"},
{"title": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code\n  Generation", "author": "Qiming Zhu and Jialun Cao and Yaojie Lu and Hongyu Lin and Xianpei Han and Le Sun and Shing-Chi Cheung", "abstract": "  Code benchmarks such as HumanEval are widely adopted to evaluate the\ncapabilities of Large Language Models (LLMs), providing insights into their\nstrengths and weaknesses. However, current benchmarks primarily exercise LLMs'\ncapability on common coding tasks (e.g., bubble sort, greatest common divisor),\nleaving domain-specific coding tasks (e.g., computation, system, cryptography)\nunexplored. To fill this gap, we propose a multi-domain code benchmark,\nDOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our\npipeline works in a fully automated manner, enabling a push-bottom construction\nfrom code repositories into formatted subjects under study. Interesting\nfindings are observed by evaluating 12 representative LLMs against DOMAINEVAL.\nWe notice that LLMs are generally good at computation tasks while falling short\non cryptography and system coding tasks. The performance gap can be as much as\n68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more\nsamples can increase the overall performance of LLMs, while the domain bias may\neven increase. The contributions of this study include a code generation\nbenchmark dataset DOMAINEVAL, encompassing six popular domains, a fully\nautomated pipeline for constructing code benchmarks, and an identification of\nthe limitations of LLMs in code generation tasks based on their performance on\nDOMAINEVAL, providing directions for future research improvements. The\nleaderboard is available at https://domaineval.github.io/.\n", "link": "http://arxiv.org/abs/2408.13204v1", "date": "2024-08-23", "relevancy": 1.773, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4452}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4443}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOMAINEVAL%3A%20An%20Auto-Constructed%20Benchmark%20for%20Multi-Domain%20Code%0A%20%20Generation&body=Title%3A%20DOMAINEVAL%3A%20An%20Auto-Constructed%20Benchmark%20for%20Multi-Domain%20Code%0A%20%20Generation%0AAuthor%3A%20Qiming%20Zhu%20and%20Jialun%20Cao%20and%20Yaojie%20Lu%20and%20Hongyu%20Lin%20and%20Xianpei%20Han%20and%20Le%20Sun%20and%20Shing-Chi%20Cheung%0AAbstract%3A%20%20%20Code%20benchmarks%20such%20as%20HumanEval%20are%20widely%20adopted%20to%20evaluate%20the%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20providing%20insights%20into%20their%0Astrengths%20and%20weaknesses.%20However%2C%20current%20benchmarks%20primarily%20exercise%20LLMs%27%0Acapability%20on%20common%20coding%20tasks%20%28e.g.%2C%20bubble%20sort%2C%20greatest%20common%20divisor%29%2C%0Aleaving%20domain-specific%20coding%20tasks%20%28e.g.%2C%20computation%2C%20system%2C%20cryptography%29%0Aunexplored.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20multi-domain%20code%20benchmark%2C%0ADOMAINEVAL%2C%20designed%20to%20evaluate%20LLMs%27%20coding%20capabilities%20thoroughly.%20Our%0Apipeline%20works%20in%20a%20fully%20automated%20manner%2C%20enabling%20a%20push-bottom%20construction%0Afrom%20code%20repositories%20into%20formatted%20subjects%20under%20study.%20Interesting%0Afindings%20are%20observed%20by%20evaluating%2012%20representative%20LLMs%20against%20DOMAINEVAL.%0AWe%20notice%20that%20LLMs%20are%20generally%20good%20at%20computation%20tasks%20while%20falling%20short%0Aon%20cryptography%20and%20system%20coding%20tasks.%20The%20performance%20gap%20can%20be%20as%20much%20as%0A68.94%25%20%2880.94%25%20-%2012.0%25%29%20in%20some%20LLMs.%20We%20also%20observe%20that%20generating%20more%0Asamples%20can%20increase%20the%20overall%20performance%20of%20LLMs%2C%20while%20the%20domain%20bias%20may%0Aeven%20increase.%20The%20contributions%20of%20this%20study%20include%20a%20code%20generation%0Abenchmark%20dataset%20DOMAINEVAL%2C%20encompassing%20six%20popular%20domains%2C%20a%20fully%0Aautomated%20pipeline%20for%20constructing%20code%20benchmarks%2C%20and%20an%20identification%20of%0Athe%20limitations%20of%20LLMs%20in%20code%20generation%20tasks%20based%20on%20their%20performance%20on%0ADOMAINEVAL%2C%20providing%20directions%20for%20future%20research%20improvements.%20The%0Aleaderboard%20is%20available%20at%20https%3A//domaineval.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOMAINEVAL%253A%2520An%2520Auto-Constructed%2520Benchmark%2520for%2520Multi-Domain%2520Code%250A%2520%2520Generation%26entry.906535625%3DQiming%2520Zhu%2520and%2520Jialun%2520Cao%2520and%2520Yaojie%2520Lu%2520and%2520Hongyu%2520Lin%2520and%2520Xianpei%2520Han%2520and%2520Le%2520Sun%2520and%2520Shing-Chi%2520Cheung%26entry.1292438233%3D%2520%2520Code%2520benchmarks%2520such%2520as%2520HumanEval%2520are%2520widely%2520adopted%2520to%2520evaluate%2520the%250Acapabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520providing%2520insights%2520into%2520their%250Astrengths%2520and%2520weaknesses.%2520However%252C%2520current%2520benchmarks%2520primarily%2520exercise%2520LLMs%2527%250Acapability%2520on%2520common%2520coding%2520tasks%2520%2528e.g.%252C%2520bubble%2520sort%252C%2520greatest%2520common%2520divisor%2529%252C%250Aleaving%2520domain-specific%2520coding%2520tasks%2520%2528e.g.%252C%2520computation%252C%2520system%252C%2520cryptography%2529%250Aunexplored.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520a%2520multi-domain%2520code%2520benchmark%252C%250ADOMAINEVAL%252C%2520designed%2520to%2520evaluate%2520LLMs%2527%2520coding%2520capabilities%2520thoroughly.%2520Our%250Apipeline%2520works%2520in%2520a%2520fully%2520automated%2520manner%252C%2520enabling%2520a%2520push-bottom%2520construction%250Afrom%2520code%2520repositories%2520into%2520formatted%2520subjects%2520under%2520study.%2520Interesting%250Afindings%2520are%2520observed%2520by%2520evaluating%252012%2520representative%2520LLMs%2520against%2520DOMAINEVAL.%250AWe%2520notice%2520that%2520LLMs%2520are%2520generally%2520good%2520at%2520computation%2520tasks%2520while%2520falling%2520short%250Aon%2520cryptography%2520and%2520system%2520coding%2520tasks.%2520The%2520performance%2520gap%2520can%2520be%2520as%2520much%2520as%250A68.94%2525%2520%252880.94%2525%2520-%252012.0%2525%2529%2520in%2520some%2520LLMs.%2520We%2520also%2520observe%2520that%2520generating%2520more%250Asamples%2520can%2520increase%2520the%2520overall%2520performance%2520of%2520LLMs%252C%2520while%2520the%2520domain%2520bias%2520may%250Aeven%2520increase.%2520The%2520contributions%2520of%2520this%2520study%2520include%2520a%2520code%2520generation%250Abenchmark%2520dataset%2520DOMAINEVAL%252C%2520encompassing%2520six%2520popular%2520domains%252C%2520a%2520fully%250Aautomated%2520pipeline%2520for%2520constructing%2520code%2520benchmarks%252C%2520and%2520an%2520identification%2520of%250Athe%2520limitations%2520of%2520LLMs%2520in%2520code%2520generation%2520tasks%2520based%2520on%2520their%2520performance%2520on%250ADOMAINEVAL%252C%2520providing%2520directions%2520for%2520future%2520research%2520improvements.%2520The%250Aleaderboard%2520is%2520available%2520at%2520https%253A//domaineval.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOMAINEVAL%3A%20An%20Auto-Constructed%20Benchmark%20for%20Multi-Domain%20Code%0A%20%20Generation&entry.906535625=Qiming%20Zhu%20and%20Jialun%20Cao%20and%20Yaojie%20Lu%20and%20Hongyu%20Lin%20and%20Xianpei%20Han%20and%20Le%20Sun%20and%20Shing-Chi%20Cheung&entry.1292438233=%20%20Code%20benchmarks%20such%20as%20HumanEval%20are%20widely%20adopted%20to%20evaluate%20the%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20providing%20insights%20into%20their%0Astrengths%20and%20weaknesses.%20However%2C%20current%20benchmarks%20primarily%20exercise%20LLMs%27%0Acapability%20on%20common%20coding%20tasks%20%28e.g.%2C%20bubble%20sort%2C%20greatest%20common%20divisor%29%2C%0Aleaving%20domain-specific%20coding%20tasks%20%28e.g.%2C%20computation%2C%20system%2C%20cryptography%29%0Aunexplored.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20multi-domain%20code%20benchmark%2C%0ADOMAINEVAL%2C%20designed%20to%20evaluate%20LLMs%27%20coding%20capabilities%20thoroughly.%20Our%0Apipeline%20works%20in%20a%20fully%20automated%20manner%2C%20enabling%20a%20push-bottom%20construction%0Afrom%20code%20repositories%20into%20formatted%20subjects%20under%20study.%20Interesting%0Afindings%20are%20observed%20by%20evaluating%2012%20representative%20LLMs%20against%20DOMAINEVAL.%0AWe%20notice%20that%20LLMs%20are%20generally%20good%20at%20computation%20tasks%20while%20falling%20short%0Aon%20cryptography%20and%20system%20coding%20tasks.%20The%20performance%20gap%20can%20be%20as%20much%20as%0A68.94%25%20%2880.94%25%20-%2012.0%25%29%20in%20some%20LLMs.%20We%20also%20observe%20that%20generating%20more%0Asamples%20can%20increase%20the%20overall%20performance%20of%20LLMs%2C%20while%20the%20domain%20bias%20may%0Aeven%20increase.%20The%20contributions%20of%20this%20study%20include%20a%20code%20generation%0Abenchmark%20dataset%20DOMAINEVAL%2C%20encompassing%20six%20popular%20domains%2C%20a%20fully%0Aautomated%20pipeline%20for%20constructing%20code%20benchmarks%2C%20and%20an%20identification%20of%0Athe%20limitations%20of%20LLMs%20in%20code%20generation%20tasks%20based%20on%20their%20performance%20on%0ADOMAINEVAL%2C%20providing%20directions%20for%20future%20research%20improvements.%20The%0Aleaderboard%20is%20available%20at%20https%3A//domaineval.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13204v1&entry.124074799=Read"},
{"title": "Global Attractor for a Reaction-Diffusion Model Arising in Biological\n  Dynamic in 3D Soil Structure", "author": "Mohamed Elghandouri and Khalil Ezzinbi and Mouad Klai and Olivier Monga", "abstract": "  Partial Differential Equations (PDEs) play a crucial role as tools for\nmodeling and comprehending intricate natural processes, notably within the\ndomain of biology. This research explores the domain of microbial activity\nwithin the complex matrix of 3D soil structures, providing valuable\nunderstanding into both the existence and uniqueness of solutions and the\nasymptotic behavior of the corresponding PDE model. Our investigation results\nin the discovery of a global attractor, a fundamental feature with significant\nimplications for long-term system behavior. To enhance the clarity of our\nfindings, numerical simulations are employed to visually illustrate the\nattributes of this global attractor.\n", "link": "http://arxiv.org/abs/2310.02060v2", "date": "2024-08-23", "relevancy": 1.7604, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4453}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4391}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Attractor%20for%20a%20Reaction-Diffusion%20Model%20Arising%20in%20Biological%0A%20%20Dynamic%20in%203D%20Soil%20Structure&body=Title%3A%20Global%20Attractor%20for%20a%20Reaction-Diffusion%20Model%20Arising%20in%20Biological%0A%20%20Dynamic%20in%203D%20Soil%20Structure%0AAuthor%3A%20Mohamed%20Elghandouri%20and%20Khalil%20Ezzinbi%20and%20Mouad%20Klai%20and%20Olivier%20Monga%0AAbstract%3A%20%20%20Partial%20Differential%20Equations%20%28PDEs%29%20play%20a%20crucial%20role%20as%20tools%20for%0Amodeling%20and%20comprehending%20intricate%20natural%20processes%2C%20notably%20within%20the%0Adomain%20of%20biology.%20This%20research%20explores%20the%20domain%20of%20microbial%20activity%0Awithin%20the%20complex%20matrix%20of%203D%20soil%20structures%2C%20providing%20valuable%0Aunderstanding%20into%20both%20the%20existence%20and%20uniqueness%20of%20solutions%20and%20the%0Aasymptotic%20behavior%20of%20the%20corresponding%20PDE%20model.%20Our%20investigation%20results%0Ain%20the%20discovery%20of%20a%20global%20attractor%2C%20a%20fundamental%20feature%20with%20significant%0Aimplications%20for%20long-term%20system%20behavior.%20To%20enhance%20the%20clarity%20of%20our%0Afindings%2C%20numerical%20simulations%20are%20employed%20to%20visually%20illustrate%20the%0Aattributes%20of%20this%20global%20attractor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Attractor%2520for%2520a%2520Reaction-Diffusion%2520Model%2520Arising%2520in%2520Biological%250A%2520%2520Dynamic%2520in%25203D%2520Soil%2520Structure%26entry.906535625%3DMohamed%2520Elghandouri%2520and%2520Khalil%2520Ezzinbi%2520and%2520Mouad%2520Klai%2520and%2520Olivier%2520Monga%26entry.1292438233%3D%2520%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529%2520play%2520a%2520crucial%2520role%2520as%2520tools%2520for%250Amodeling%2520and%2520comprehending%2520intricate%2520natural%2520processes%252C%2520notably%2520within%2520the%250Adomain%2520of%2520biology.%2520This%2520research%2520explores%2520the%2520domain%2520of%2520microbial%2520activity%250Awithin%2520the%2520complex%2520matrix%2520of%25203D%2520soil%2520structures%252C%2520providing%2520valuable%250Aunderstanding%2520into%2520both%2520the%2520existence%2520and%2520uniqueness%2520of%2520solutions%2520and%2520the%250Aasymptotic%2520behavior%2520of%2520the%2520corresponding%2520PDE%2520model.%2520Our%2520investigation%2520results%250Ain%2520the%2520discovery%2520of%2520a%2520global%2520attractor%252C%2520a%2520fundamental%2520feature%2520with%2520significant%250Aimplications%2520for%2520long-term%2520system%2520behavior.%2520To%2520enhance%2520the%2520clarity%2520of%2520our%250Afindings%252C%2520numerical%2520simulations%2520are%2520employed%2520to%2520visually%2520illustrate%2520the%250Aattributes%2520of%2520this%2520global%2520attractor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Attractor%20for%20a%20Reaction-Diffusion%20Model%20Arising%20in%20Biological%0A%20%20Dynamic%20in%203D%20Soil%20Structure&entry.906535625=Mohamed%20Elghandouri%20and%20Khalil%20Ezzinbi%20and%20Mouad%20Klai%20and%20Olivier%20Monga&entry.1292438233=%20%20Partial%20Differential%20Equations%20%28PDEs%29%20play%20a%20crucial%20role%20as%20tools%20for%0Amodeling%20and%20comprehending%20intricate%20natural%20processes%2C%20notably%20within%20the%0Adomain%20of%20biology.%20This%20research%20explores%20the%20domain%20of%20microbial%20activity%0Awithin%20the%20complex%20matrix%20of%203D%20soil%20structures%2C%20providing%20valuable%0Aunderstanding%20into%20both%20the%20existence%20and%20uniqueness%20of%20solutions%20and%20the%0Aasymptotic%20behavior%20of%20the%20corresponding%20PDE%20model.%20Our%20investigation%20results%0Ain%20the%20discovery%20of%20a%20global%20attractor%2C%20a%20fundamental%20feature%20with%20significant%0Aimplications%20for%20long-term%20system%20behavior.%20To%20enhance%20the%20clarity%20of%20our%0Afindings%2C%20numerical%20simulations%20are%20employed%20to%20visually%20illustrate%20the%0Aattributes%20of%20this%20global%20attractor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02060v2&entry.124074799=Read"},
{"title": "Deep Learning at the Intersection: Certified Robustness as a Tool for 3D\n  Vision", "author": "Gabriel P\u00e9rez S and Juan C. P\u00e9rez and Motasem Alfarra and Jes\u00fas Zarzar and Sara Rojas and Bernard Ghanem and Pablo Arbel\u00e1ez", "abstract": "  This paper presents preliminary work on a novel connection between certified\nrobustness in machine learning and the modeling of 3D objects. We highlight an\nintriguing link between the Maximal Certified Radius (MCR) of a classifier\nrepresenting a space's occupancy and the space's Signed Distance Function\n(SDF). Leveraging this relationship, we propose to use the certification method\nof randomized smoothing (RS) to compute SDFs. Since RS' high computational cost\nprevents its practical usage as a way to compute SDFs, we propose an algorithm\nto efficiently run RS in low-dimensional applications, such as 3D space, by\nexpressing RS' fundamental operations as Gaussian smoothing on pre-computed\nvoxel grids. Our approach offers an innovative and practical tool to compute\nSDFs, validated through proof-of-concept experiments in novel view synthesis.\nThis paper bridges two previously disparate areas of machine learning, opening\nnew avenues for further exploration and potential cross-domain advancements.\n", "link": "http://arxiv.org/abs/2408.13135v1", "date": "2024-08-23", "relevancy": 1.7431, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5916}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20at%20the%20Intersection%3A%20Certified%20Robustness%20as%20a%20Tool%20for%203D%0A%20%20Vision&body=Title%3A%20Deep%20Learning%20at%20the%20Intersection%3A%20Certified%20Robustness%20as%20a%20Tool%20for%203D%0A%20%20Vision%0AAuthor%3A%20Gabriel%20P%C3%A9rez%20S%20and%20Juan%20C.%20P%C3%A9rez%20and%20Motasem%20Alfarra%20and%20Jes%C3%BAs%20Zarzar%20and%20Sara%20Rojas%20and%20Bernard%20Ghanem%20and%20Pablo%20Arbel%C3%A1ez%0AAbstract%3A%20%20%20This%20paper%20presents%20preliminary%20work%20on%20a%20novel%20connection%20between%20certified%0Arobustness%20in%20machine%20learning%20and%20the%20modeling%20of%203D%20objects.%20We%20highlight%20an%0Aintriguing%20link%20between%20the%20Maximal%20Certified%20Radius%20%28MCR%29%20of%20a%20classifier%0Arepresenting%20a%20space%27s%20occupancy%20and%20the%20space%27s%20Signed%20Distance%20Function%0A%28SDF%29.%20Leveraging%20this%20relationship%2C%20we%20propose%20to%20use%20the%20certification%20method%0Aof%20randomized%20smoothing%20%28RS%29%20to%20compute%20SDFs.%20Since%20RS%27%20high%20computational%20cost%0Aprevents%20its%20practical%20usage%20as%20a%20way%20to%20compute%20SDFs%2C%20we%20propose%20an%20algorithm%0Ato%20efficiently%20run%20RS%20in%20low-dimensional%20applications%2C%20such%20as%203D%20space%2C%20by%0Aexpressing%20RS%27%20fundamental%20operations%20as%20Gaussian%20smoothing%20on%20pre-computed%0Avoxel%20grids.%20Our%20approach%20offers%20an%20innovative%20and%20practical%20tool%20to%20compute%0ASDFs%2C%20validated%20through%20proof-of-concept%20experiments%20in%20novel%20view%20synthesis.%0AThis%20paper%20bridges%20two%20previously%20disparate%20areas%20of%20machine%20learning%2C%20opening%0Anew%20avenues%20for%20further%20exploration%20and%20potential%20cross-domain%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520at%2520the%2520Intersection%253A%2520Certified%2520Robustness%2520as%2520a%2520Tool%2520for%25203D%250A%2520%2520Vision%26entry.906535625%3DGabriel%2520P%25C3%25A9rez%2520S%2520and%2520Juan%2520C.%2520P%25C3%25A9rez%2520and%2520Motasem%2520Alfarra%2520and%2520Jes%25C3%25BAs%2520Zarzar%2520and%2520Sara%2520Rojas%2520and%2520Bernard%2520Ghanem%2520and%2520Pablo%2520Arbel%25C3%25A1ez%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520preliminary%2520work%2520on%2520a%2520novel%2520connection%2520between%2520certified%250Arobustness%2520in%2520machine%2520learning%2520and%2520the%2520modeling%2520of%25203D%2520objects.%2520We%2520highlight%2520an%250Aintriguing%2520link%2520between%2520the%2520Maximal%2520Certified%2520Radius%2520%2528MCR%2529%2520of%2520a%2520classifier%250Arepresenting%2520a%2520space%2527s%2520occupancy%2520and%2520the%2520space%2527s%2520Signed%2520Distance%2520Function%250A%2528SDF%2529.%2520Leveraging%2520this%2520relationship%252C%2520we%2520propose%2520to%2520use%2520the%2520certification%2520method%250Aof%2520randomized%2520smoothing%2520%2528RS%2529%2520to%2520compute%2520SDFs.%2520Since%2520RS%2527%2520high%2520computational%2520cost%250Aprevents%2520its%2520practical%2520usage%2520as%2520a%2520way%2520to%2520compute%2520SDFs%252C%2520we%2520propose%2520an%2520algorithm%250Ato%2520efficiently%2520run%2520RS%2520in%2520low-dimensional%2520applications%252C%2520such%2520as%25203D%2520space%252C%2520by%250Aexpressing%2520RS%2527%2520fundamental%2520operations%2520as%2520Gaussian%2520smoothing%2520on%2520pre-computed%250Avoxel%2520grids.%2520Our%2520approach%2520offers%2520an%2520innovative%2520and%2520practical%2520tool%2520to%2520compute%250ASDFs%252C%2520validated%2520through%2520proof-of-concept%2520experiments%2520in%2520novel%2520view%2520synthesis.%250AThis%2520paper%2520bridges%2520two%2520previously%2520disparate%2520areas%2520of%2520machine%2520learning%252C%2520opening%250Anew%2520avenues%2520for%2520further%2520exploration%2520and%2520potential%2520cross-domain%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20at%20the%20Intersection%3A%20Certified%20Robustness%20as%20a%20Tool%20for%203D%0A%20%20Vision&entry.906535625=Gabriel%20P%C3%A9rez%20S%20and%20Juan%20C.%20P%C3%A9rez%20and%20Motasem%20Alfarra%20and%20Jes%C3%BAs%20Zarzar%20and%20Sara%20Rojas%20and%20Bernard%20Ghanem%20and%20Pablo%20Arbel%C3%A1ez&entry.1292438233=%20%20This%20paper%20presents%20preliminary%20work%20on%20a%20novel%20connection%20between%20certified%0Arobustness%20in%20machine%20learning%20and%20the%20modeling%20of%203D%20objects.%20We%20highlight%20an%0Aintriguing%20link%20between%20the%20Maximal%20Certified%20Radius%20%28MCR%29%20of%20a%20classifier%0Arepresenting%20a%20space%27s%20occupancy%20and%20the%20space%27s%20Signed%20Distance%20Function%0A%28SDF%29.%20Leveraging%20this%20relationship%2C%20we%20propose%20to%20use%20the%20certification%20method%0Aof%20randomized%20smoothing%20%28RS%29%20to%20compute%20SDFs.%20Since%20RS%27%20high%20computational%20cost%0Aprevents%20its%20practical%20usage%20as%20a%20way%20to%20compute%20SDFs%2C%20we%20propose%20an%20algorithm%0Ato%20efficiently%20run%20RS%20in%20low-dimensional%20applications%2C%20such%20as%203D%20space%2C%20by%0Aexpressing%20RS%27%20fundamental%20operations%20as%20Gaussian%20smoothing%20on%20pre-computed%0Avoxel%20grids.%20Our%20approach%20offers%20an%20innovative%20and%20practical%20tool%20to%20compute%0ASDFs%2C%20validated%20through%20proof-of-concept%20experiments%20in%20novel%20view%20synthesis.%0AThis%20paper%20bridges%20two%20previously%20disparate%20areas%20of%20machine%20learning%2C%20opening%0Anew%20avenues%20for%20further%20exploration%20and%20potential%20cross-domain%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13135v1&entry.124074799=Read"},
{"title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language", "author": "Manjil Karki and Pratik Shakya and Sandesh Acharya and Ravi Pandit and Dinesh Gothe", "abstract": "  Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.\n", "link": "http://arxiv.org/abs/2408.10128v2", "date": "2024-08-23", "relevancy": 1.7351, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4411}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Voice%20Cloning%20for%20Nepali%3A%20Leveraging%20Transfer%20Learning%20in%20a%0A%20%20Low-Resource%20Language&body=Title%3A%20Advancing%20Voice%20Cloning%20for%20Nepali%3A%20Leveraging%20Transfer%20Learning%20in%20a%0A%20%20Low-Resource%20Language%0AAuthor%3A%20Manjil%20Karki%20and%20Pratik%20Shakya%20and%20Sandesh%20Acharya%20and%20Ravi%20Pandit%20and%20Dinesh%20Gothe%0AAbstract%3A%20%20%20Voice%20cloning%20is%20a%20prominent%20feature%20in%20personalized%20speech%20interfaces.%20A%0Aneural%20vocal%20cloning%20system%20can%20mimic%20someone%27s%20voice%20using%20just%20a%20few%20audio%0Asamples.%20Both%20speaker%20encoding%20and%20speaker%20adaptation%20are%20topics%20of%20research%20in%0Athe%20field%20of%20voice%20cloning.%20Speaker%20adaptation%20relies%20on%20fine-tuning%20a%0Amulti-speaker%20generative%20model%2C%20which%20involves%20training%20a%20separate%20model%20to%0Ainfer%20a%20new%20speaker%20embedding%20used%20for%20speaker%20encoding.%20Both%20methods%20can%0Aachieve%20excellent%20performance%2C%20even%20with%20a%20small%20number%20of%20cloning%20audios%2C%20in%0Aterms%20of%20the%20speech%27s%20naturalness%20and%20similarity%20to%20the%20original%20speaker.%0ASpeaker%20encoding%20approaches%20are%20more%20appropriate%20for%20low-resource%20deployment%0Asince%20they%20require%20significantly%20less%20memory%20and%20have%20a%20faster%20cloning%20time%0Athan%20speaker%20adaption%2C%20which%20can%20offer%20slightly%20greater%20naturalness%20and%0Asimilarity.%20The%20main%20goal%20is%20to%20create%20a%20vocal%20cloning%20system%20that%20produces%0Aaudio%20output%20with%20a%20Nepali%20accent%20or%20that%20sounds%20like%20Nepali.%20For%20the%20further%0Aadvancement%20of%20TTS%2C%20the%20idea%20of%20transfer%20learning%20was%20effectively%20used%20to%0Aaddress%20several%20issues%20that%20were%20encountered%20in%20the%20development%20of%20this%20system%2C%0Aincluding%20the%20poor%20audio%20quality%20and%20the%20lack%20of%20available%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Voice%2520Cloning%2520for%2520Nepali%253A%2520Leveraging%2520Transfer%2520Learning%2520in%2520a%250A%2520%2520Low-Resource%2520Language%26entry.906535625%3DManjil%2520Karki%2520and%2520Pratik%2520Shakya%2520and%2520Sandesh%2520Acharya%2520and%2520Ravi%2520Pandit%2520and%2520Dinesh%2520Gothe%26entry.1292438233%3D%2520%2520Voice%2520cloning%2520is%2520a%2520prominent%2520feature%2520in%2520personalized%2520speech%2520interfaces.%2520A%250Aneural%2520vocal%2520cloning%2520system%2520can%2520mimic%2520someone%2527s%2520voice%2520using%2520just%2520a%2520few%2520audio%250Asamples.%2520Both%2520speaker%2520encoding%2520and%2520speaker%2520adaptation%2520are%2520topics%2520of%2520research%2520in%250Athe%2520field%2520of%2520voice%2520cloning.%2520Speaker%2520adaptation%2520relies%2520on%2520fine-tuning%2520a%250Amulti-speaker%2520generative%2520model%252C%2520which%2520involves%2520training%2520a%2520separate%2520model%2520to%250Ainfer%2520a%2520new%2520speaker%2520embedding%2520used%2520for%2520speaker%2520encoding.%2520Both%2520methods%2520can%250Aachieve%2520excellent%2520performance%252C%2520even%2520with%2520a%2520small%2520number%2520of%2520cloning%2520audios%252C%2520in%250Aterms%2520of%2520the%2520speech%2527s%2520naturalness%2520and%2520similarity%2520to%2520the%2520original%2520speaker.%250ASpeaker%2520encoding%2520approaches%2520are%2520more%2520appropriate%2520for%2520low-resource%2520deployment%250Asince%2520they%2520require%2520significantly%2520less%2520memory%2520and%2520have%2520a%2520faster%2520cloning%2520time%250Athan%2520speaker%2520adaption%252C%2520which%2520can%2520offer%2520slightly%2520greater%2520naturalness%2520and%250Asimilarity.%2520The%2520main%2520goal%2520is%2520to%2520create%2520a%2520vocal%2520cloning%2520system%2520that%2520produces%250Aaudio%2520output%2520with%2520a%2520Nepali%2520accent%2520or%2520that%2520sounds%2520like%2520Nepali.%2520For%2520the%2520further%250Aadvancement%2520of%2520TTS%252C%2520the%2520idea%2520of%2520transfer%2520learning%2520was%2520effectively%2520used%2520to%250Aaddress%2520several%2520issues%2520that%2520were%2520encountered%2520in%2520the%2520development%2520of%2520this%2520system%252C%250Aincluding%2520the%2520poor%2520audio%2520quality%2520and%2520the%2520lack%2520of%2520available%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Voice%20Cloning%20for%20Nepali%3A%20Leveraging%20Transfer%20Learning%20in%20a%0A%20%20Low-Resource%20Language&entry.906535625=Manjil%20Karki%20and%20Pratik%20Shakya%20and%20Sandesh%20Acharya%20and%20Ravi%20Pandit%20and%20Dinesh%20Gothe&entry.1292438233=%20%20Voice%20cloning%20is%20a%20prominent%20feature%20in%20personalized%20speech%20interfaces.%20A%0Aneural%20vocal%20cloning%20system%20can%20mimic%20someone%27s%20voice%20using%20just%20a%20few%20audio%0Asamples.%20Both%20speaker%20encoding%20and%20speaker%20adaptation%20are%20topics%20of%20research%20in%0Athe%20field%20of%20voice%20cloning.%20Speaker%20adaptation%20relies%20on%20fine-tuning%20a%0Amulti-speaker%20generative%20model%2C%20which%20involves%20training%20a%20separate%20model%20to%0Ainfer%20a%20new%20speaker%20embedding%20used%20for%20speaker%20encoding.%20Both%20methods%20can%0Aachieve%20excellent%20performance%2C%20even%20with%20a%20small%20number%20of%20cloning%20audios%2C%20in%0Aterms%20of%20the%20speech%27s%20naturalness%20and%20similarity%20to%20the%20original%20speaker.%0ASpeaker%20encoding%20approaches%20are%20more%20appropriate%20for%20low-resource%20deployment%0Asince%20they%20require%20significantly%20less%20memory%20and%20have%20a%20faster%20cloning%20time%0Athan%20speaker%20adaption%2C%20which%20can%20offer%20slightly%20greater%20naturalness%20and%0Asimilarity.%20The%20main%20goal%20is%20to%20create%20a%20vocal%20cloning%20system%20that%20produces%0Aaudio%20output%20with%20a%20Nepali%20accent%20or%20that%20sounds%20like%20Nepali.%20For%20the%20further%0Aadvancement%20of%20TTS%2C%20the%20idea%20of%20transfer%20learning%20was%20effectively%20used%20to%0Aaddress%20several%20issues%20that%20were%20encountered%20in%20the%20development%20of%20this%20system%2C%0Aincluding%20the%20poor%20audio%20quality%20and%20the%20lack%20of%20available%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10128v2&entry.124074799=Read"},
{"title": "Robust Diffusion Models for Adversarial Purification", "author": "Guang Lin and Zerui Tao and Jianhai Zhang and Toshihisa Tanaka and Qibin Zhao", "abstract": "  Diffusion models (DMs) based adversarial purification (AP) has shown to be\nthe most powerful alternative to adversarial training (AT). However, these\nmethods neglect the fact that pre-trained diffusion models themselves are not\nrobust to adversarial attacks as well. Additionally, the diffusion process can\neasily destroy semantic information and generate a high quality image but\ntotally different from the original input image after the reverse process,\nleading to degraded standard accuracy. To overcome these issues, a natural idea\nis to harness adversarial training strategy to retrain or fine-tune the\npre-trained diffusion model, which is computationally prohibitive. We propose a\nnovel robust reverse process with adversarial guidance, which is independent of\ngiven pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust\nguidance can not only ensure to generate purified examples retaining more\nsemantic content but also mitigate the accuracy-robustness trade-off of DMs for\nthe first time, which also provides DM-based AP an efficient adaptive ability\nto new attacks. Extensive experiments are conducted on CIFAR-10, CIFAR-100 and\nImageNet to demonstrate that our method achieves the state-of-the-art results\nand exhibits generalization against different attacks.\n", "link": "http://arxiv.org/abs/2403.16067v3", "date": "2024-08-23", "relevancy": 1.7246, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.629}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5764}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Diffusion%20Models%20for%20Adversarial%20Purification&body=Title%3A%20Robust%20Diffusion%20Models%20for%20Adversarial%20Purification%0AAuthor%3A%20Guang%20Lin%20and%20Zerui%20Tao%20and%20Jianhai%20Zhang%20and%20Toshihisa%20Tanaka%20and%20Qibin%20Zhao%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20based%20adversarial%20purification%20%28AP%29%20has%20shown%20to%20be%0Athe%20most%20powerful%20alternative%20to%20adversarial%20training%20%28AT%29.%20However%2C%20these%0Amethods%20neglect%20the%20fact%20that%20pre-trained%20diffusion%20models%20themselves%20are%20not%0Arobust%20to%20adversarial%20attacks%20as%20well.%20Additionally%2C%20the%20diffusion%20process%20can%0Aeasily%20destroy%20semantic%20information%20and%20generate%20a%20high%20quality%20image%20but%0Atotally%20different%20from%20the%20original%20input%20image%20after%20the%20reverse%20process%2C%0Aleading%20to%20degraded%20standard%20accuracy.%20To%20overcome%20these%20issues%2C%20a%20natural%20idea%0Ais%20to%20harness%20adversarial%20training%20strategy%20to%20retrain%20or%20fine-tune%20the%0Apre-trained%20diffusion%20model%2C%20which%20is%20computationally%20prohibitive.%20We%20propose%20a%0Anovel%20robust%20reverse%20process%20with%20adversarial%20guidance%2C%20which%20is%20independent%20of%0Agiven%20pre-trained%20DMs%20and%20avoids%20retraining%20or%20fine-tuning%20the%20DMs.%20This%20robust%0Aguidance%20can%20not%20only%20ensure%20to%20generate%20purified%20examples%20retaining%20more%0Asemantic%20content%20but%20also%20mitigate%20the%20accuracy-robustness%20trade-off%20of%20DMs%20for%0Athe%20first%20time%2C%20which%20also%20provides%20DM-based%20AP%20an%20efficient%20adaptive%20ability%0Ato%20new%20attacks.%20Extensive%20experiments%20are%20conducted%20on%20CIFAR-10%2C%20CIFAR-100%20and%0AImageNet%20to%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%20results%0Aand%20exhibits%20generalization%20against%20different%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16067v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Diffusion%2520Models%2520for%2520Adversarial%2520Purification%26entry.906535625%3DGuang%2520Lin%2520and%2520Zerui%2520Tao%2520and%2520Jianhai%2520Zhang%2520and%2520Toshihisa%2520Tanaka%2520and%2520Qibin%2520Zhao%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520based%2520adversarial%2520purification%2520%2528AP%2529%2520has%2520shown%2520to%2520be%250Athe%2520most%2520powerful%2520alternative%2520to%2520adversarial%2520training%2520%2528AT%2529.%2520However%252C%2520these%250Amethods%2520neglect%2520the%2520fact%2520that%2520pre-trained%2520diffusion%2520models%2520themselves%2520are%2520not%250Arobust%2520to%2520adversarial%2520attacks%2520as%2520well.%2520Additionally%252C%2520the%2520diffusion%2520process%2520can%250Aeasily%2520destroy%2520semantic%2520information%2520and%2520generate%2520a%2520high%2520quality%2520image%2520but%250Atotally%2520different%2520from%2520the%2520original%2520input%2520image%2520after%2520the%2520reverse%2520process%252C%250Aleading%2520to%2520degraded%2520standard%2520accuracy.%2520To%2520overcome%2520these%2520issues%252C%2520a%2520natural%2520idea%250Ais%2520to%2520harness%2520adversarial%2520training%2520strategy%2520to%2520retrain%2520or%2520fine-tune%2520the%250Apre-trained%2520diffusion%2520model%252C%2520which%2520is%2520computationally%2520prohibitive.%2520We%2520propose%2520a%250Anovel%2520robust%2520reverse%2520process%2520with%2520adversarial%2520guidance%252C%2520which%2520is%2520independent%2520of%250Agiven%2520pre-trained%2520DMs%2520and%2520avoids%2520retraining%2520or%2520fine-tuning%2520the%2520DMs.%2520This%2520robust%250Aguidance%2520can%2520not%2520only%2520ensure%2520to%2520generate%2520purified%2520examples%2520retaining%2520more%250Asemantic%2520content%2520but%2520also%2520mitigate%2520the%2520accuracy-robustness%2520trade-off%2520of%2520DMs%2520for%250Athe%2520first%2520time%252C%2520which%2520also%2520provides%2520DM-based%2520AP%2520an%2520efficient%2520adaptive%2520ability%250Ato%2520new%2520attacks.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520CIFAR-10%252C%2520CIFAR-100%2520and%250AImageNet%2520to%2520demonstrate%2520that%2520our%2520method%2520achieves%2520the%2520state-of-the-art%2520results%250Aand%2520exhibits%2520generalization%2520against%2520different%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16067v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Diffusion%20Models%20for%20Adversarial%20Purification&entry.906535625=Guang%20Lin%20and%20Zerui%20Tao%20and%20Jianhai%20Zhang%20and%20Toshihisa%20Tanaka%20and%20Qibin%20Zhao&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20based%20adversarial%20purification%20%28AP%29%20has%20shown%20to%20be%0Athe%20most%20powerful%20alternative%20to%20adversarial%20training%20%28AT%29.%20However%2C%20these%0Amethods%20neglect%20the%20fact%20that%20pre-trained%20diffusion%20models%20themselves%20are%20not%0Arobust%20to%20adversarial%20attacks%20as%20well.%20Additionally%2C%20the%20diffusion%20process%20can%0Aeasily%20destroy%20semantic%20information%20and%20generate%20a%20high%20quality%20image%20but%0Atotally%20different%20from%20the%20original%20input%20image%20after%20the%20reverse%20process%2C%0Aleading%20to%20degraded%20standard%20accuracy.%20To%20overcome%20these%20issues%2C%20a%20natural%20idea%0Ais%20to%20harness%20adversarial%20training%20strategy%20to%20retrain%20or%20fine-tune%20the%0Apre-trained%20diffusion%20model%2C%20which%20is%20computationally%20prohibitive.%20We%20propose%20a%0Anovel%20robust%20reverse%20process%20with%20adversarial%20guidance%2C%20which%20is%20independent%20of%0Agiven%20pre-trained%20DMs%20and%20avoids%20retraining%20or%20fine-tuning%20the%20DMs.%20This%20robust%0Aguidance%20can%20not%20only%20ensure%20to%20generate%20purified%20examples%20retaining%20more%0Asemantic%20content%20but%20also%20mitigate%20the%20accuracy-robustness%20trade-off%20of%20DMs%20for%0Athe%20first%20time%2C%20which%20also%20provides%20DM-based%20AP%20an%20efficient%20adaptive%20ability%0Ato%20new%20attacks.%20Extensive%20experiments%20are%20conducted%20on%20CIFAR-10%2C%20CIFAR-100%20and%0AImageNet%20to%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%20results%0Aand%20exhibits%20generalization%20against%20different%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16067v3&entry.124074799=Read"},
{"title": "Multimodal Reinforcement Learning for Robots Collaborating with Humans", "author": "Afagh Mehri Shervedani and Siyu Li and Natawut Monaikul and Bahareh Abbasi and Barbara Di Eugenio and Milos Zefran", "abstract": "  Robot assistants for older adults and people with disabilities need to\ninteract with their users in collaborative tasks. The core component of these\nsystems is an interaction manager whose job is to observe and assess the task,\nand infer the state of the human and their intent to choose the best course of\naction for the robot. Due to the sparseness of the data in this domain, the\npolicy for such multi-modal systems is often crafted by hand; as the complexity\nof interactions grows this process is not scalable. In this paper, we propose a\nreinforcement learning (RL) approach to learn the robot policy. In contrast to\nthe dialog systems, our agent is trained with a simulator developed by using\nhuman data and can deal with multiple modalities such as language and physical\nactions. We conducted a human study to evaluate the performance of the system\nin the interaction with a user. Our designed system shows promising preliminary\nresults when it is used by a real user.\n", "link": "http://arxiv.org/abs/2303.07265v2", "date": "2024-08-23", "relevancy": 1.7198, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6151}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5818}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Reinforcement%20Learning%20for%20Robots%20Collaborating%20with%20Humans&body=Title%3A%20Multimodal%20Reinforcement%20Learning%20for%20Robots%20Collaborating%20with%20Humans%0AAuthor%3A%20Afagh%20Mehri%20Shervedani%20and%20Siyu%20Li%20and%20Natawut%20Monaikul%20and%20Bahareh%20Abbasi%20and%20Barbara%20Di%20Eugenio%20and%20Milos%20Zefran%0AAbstract%3A%20%20%20Robot%20assistants%20for%20older%20adults%20and%20people%20with%20disabilities%20need%20to%0Ainteract%20with%20their%20users%20in%20collaborative%20tasks.%20The%20core%20component%20of%20these%0Asystems%20is%20an%20interaction%20manager%20whose%20job%20is%20to%20observe%20and%20assess%20the%20task%2C%0Aand%20infer%20the%20state%20of%20the%20human%20and%20their%20intent%20to%20choose%20the%20best%20course%20of%0Aaction%20for%20the%20robot.%20Due%20to%20the%20sparseness%20of%20the%20data%20in%20this%20domain%2C%20the%0Apolicy%20for%20such%20multi-modal%20systems%20is%20often%20crafted%20by%20hand%3B%20as%20the%20complexity%0Aof%20interactions%20grows%20this%20process%20is%20not%20scalable.%20In%20this%20paper%2C%20we%20propose%20a%0Areinforcement%20learning%20%28RL%29%20approach%20to%20learn%20the%20robot%20policy.%20In%20contrast%20to%0Athe%20dialog%20systems%2C%20our%20agent%20is%20trained%20with%20a%20simulator%20developed%20by%20using%0Ahuman%20data%20and%20can%20deal%20with%20multiple%20modalities%20such%20as%20language%20and%20physical%0Aactions.%20We%20conducted%20a%20human%20study%20to%20evaluate%20the%20performance%20of%20the%20system%0Ain%20the%20interaction%20with%20a%20user.%20Our%20designed%20system%20shows%20promising%20preliminary%0Aresults%20when%20it%20is%20used%20by%20a%20real%20user.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Reinforcement%2520Learning%2520for%2520Robots%2520Collaborating%2520with%2520Humans%26entry.906535625%3DAfagh%2520Mehri%2520Shervedani%2520and%2520Siyu%2520Li%2520and%2520Natawut%2520Monaikul%2520and%2520Bahareh%2520Abbasi%2520and%2520Barbara%2520Di%2520Eugenio%2520and%2520Milos%2520Zefran%26entry.1292438233%3D%2520%2520Robot%2520assistants%2520for%2520older%2520adults%2520and%2520people%2520with%2520disabilities%2520need%2520to%250Ainteract%2520with%2520their%2520users%2520in%2520collaborative%2520tasks.%2520The%2520core%2520component%2520of%2520these%250Asystems%2520is%2520an%2520interaction%2520manager%2520whose%2520job%2520is%2520to%2520observe%2520and%2520assess%2520the%2520task%252C%250Aand%2520infer%2520the%2520state%2520of%2520the%2520human%2520and%2520their%2520intent%2520to%2520choose%2520the%2520best%2520course%2520of%250Aaction%2520for%2520the%2520robot.%2520Due%2520to%2520the%2520sparseness%2520of%2520the%2520data%2520in%2520this%2520domain%252C%2520the%250Apolicy%2520for%2520such%2520multi-modal%2520systems%2520is%2520often%2520crafted%2520by%2520hand%253B%2520as%2520the%2520complexity%250Aof%2520interactions%2520grows%2520this%2520process%2520is%2520not%2520scalable.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Areinforcement%2520learning%2520%2528RL%2529%2520approach%2520to%2520learn%2520the%2520robot%2520policy.%2520In%2520contrast%2520to%250Athe%2520dialog%2520systems%252C%2520our%2520agent%2520is%2520trained%2520with%2520a%2520simulator%2520developed%2520by%2520using%250Ahuman%2520data%2520and%2520can%2520deal%2520with%2520multiple%2520modalities%2520such%2520as%2520language%2520and%2520physical%250Aactions.%2520We%2520conducted%2520a%2520human%2520study%2520to%2520evaluate%2520the%2520performance%2520of%2520the%2520system%250Ain%2520the%2520interaction%2520with%2520a%2520user.%2520Our%2520designed%2520system%2520shows%2520promising%2520preliminary%250Aresults%2520when%2520it%2520is%2520used%2520by%2520a%2520real%2520user.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.07265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Reinforcement%20Learning%20for%20Robots%20Collaborating%20with%20Humans&entry.906535625=Afagh%20Mehri%20Shervedani%20and%20Siyu%20Li%20and%20Natawut%20Monaikul%20and%20Bahareh%20Abbasi%20and%20Barbara%20Di%20Eugenio%20and%20Milos%20Zefran&entry.1292438233=%20%20Robot%20assistants%20for%20older%20adults%20and%20people%20with%20disabilities%20need%20to%0Ainteract%20with%20their%20users%20in%20collaborative%20tasks.%20The%20core%20component%20of%20these%0Asystems%20is%20an%20interaction%20manager%20whose%20job%20is%20to%20observe%20and%20assess%20the%20task%2C%0Aand%20infer%20the%20state%20of%20the%20human%20and%20their%20intent%20to%20choose%20the%20best%20course%20of%0Aaction%20for%20the%20robot.%20Due%20to%20the%20sparseness%20of%20the%20data%20in%20this%20domain%2C%20the%0Apolicy%20for%20such%20multi-modal%20systems%20is%20often%20crafted%20by%20hand%3B%20as%20the%20complexity%0Aof%20interactions%20grows%20this%20process%20is%20not%20scalable.%20In%20this%20paper%2C%20we%20propose%20a%0Areinforcement%20learning%20%28RL%29%20approach%20to%20learn%20the%20robot%20policy.%20In%20contrast%20to%0Athe%20dialog%20systems%2C%20our%20agent%20is%20trained%20with%20a%20simulator%20developed%20by%20using%0Ahuman%20data%20and%20can%20deal%20with%20multiple%20modalities%20such%20as%20language%20and%20physical%0Aactions.%20We%20conducted%20a%20human%20study%20to%20evaluate%20the%20performance%20of%20the%20system%0Ain%20the%20interaction%20with%20a%20user.%20Our%20designed%20system%20shows%20promising%20preliminary%0Aresults%20when%20it%20is%20used%20by%20a%20real%20user.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07265v2&entry.124074799=Read"},
{"title": "MAML MOT: Multiple Object Tracking based on Meta-Learning", "author": "Jiayi Chen and Chunhua Deng", "abstract": "  With the advancement of video analysis technology, the multi-object tracking\n(MOT) problem in complex scenes involving pedestrians is gaining increasing\nimportance. This challenge primarily involves two key tasks: pedestrian\ndetection and re-identification. While significant progress has been achieved\nin pedestrian detection tasks in recent years, enhancing the effectiveness of\nre-identification tasks remains a persistent challenge. This difficulty arises\nfrom the large total number of pedestrian samples in multi-object tracking\ndatasets and the scarcity of individual instance samples. Motivated by recent\nrapid advancements in meta-learning techniques, we introduce MAML MOT, a\nmeta-learning-based training approach for multi-object tracking. This approach\nleverages the rapid learning capability of meta-learning to tackle the issue of\nsample scarcity in pedestrian re-identification tasks, aiming to improve the\nmodel's generalization performance and robustness. Experimental results\ndemonstrate that the proposed method achieves high accuracy on mainstream\ndatasets in the MOT Challenge. This offers new perspectives and solutions for\nresearch in the field of pedestrian multi-object tracking.\n", "link": "http://arxiv.org/abs/2405.07272v3", "date": "2024-08-23", "relevancy": 1.7085, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.565}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAML%20MOT%3A%20Multiple%20Object%20Tracking%20based%20on%20Meta-Learning&body=Title%3A%20MAML%20MOT%3A%20Multiple%20Object%20Tracking%20based%20on%20Meta-Learning%0AAuthor%3A%20Jiayi%20Chen%20and%20Chunhua%20Deng%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20video%20analysis%20technology%2C%20the%20multi-object%20tracking%0A%28MOT%29%20problem%20in%20complex%20scenes%20involving%20pedestrians%20is%20gaining%20increasing%0Aimportance.%20This%20challenge%20primarily%20involves%20two%20key%20tasks%3A%20pedestrian%0Adetection%20and%20re-identification.%20While%20significant%20progress%20has%20been%20achieved%0Ain%20pedestrian%20detection%20tasks%20in%20recent%20years%2C%20enhancing%20the%20effectiveness%20of%0Are-identification%20tasks%20remains%20a%20persistent%20challenge.%20This%20difficulty%20arises%0Afrom%20the%20large%20total%20number%20of%20pedestrian%20samples%20in%20multi-object%20tracking%0Adatasets%20and%20the%20scarcity%20of%20individual%20instance%20samples.%20Motivated%20by%20recent%0Arapid%20advancements%20in%20meta-learning%20techniques%2C%20we%20introduce%20MAML%20MOT%2C%20a%0Ameta-learning-based%20training%20approach%20for%20multi-object%20tracking.%20This%20approach%0Aleverages%20the%20rapid%20learning%20capability%20of%20meta-learning%20to%20tackle%20the%20issue%20of%0Asample%20scarcity%20in%20pedestrian%20re-identification%20tasks%2C%20aiming%20to%20improve%20the%0Amodel%27s%20generalization%20performance%20and%20robustness.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20high%20accuracy%20on%20mainstream%0Adatasets%20in%20the%20MOT%20Challenge.%20This%20offers%20new%20perspectives%20and%20solutions%20for%0Aresearch%20in%20the%20field%20of%20pedestrian%20multi-object%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07272v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAML%2520MOT%253A%2520Multiple%2520Object%2520Tracking%2520based%2520on%2520Meta-Learning%26entry.906535625%3DJiayi%2520Chen%2520and%2520Chunhua%2520Deng%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520video%2520analysis%2520technology%252C%2520the%2520multi-object%2520tracking%250A%2528MOT%2529%2520problem%2520in%2520complex%2520scenes%2520involving%2520pedestrians%2520is%2520gaining%2520increasing%250Aimportance.%2520This%2520challenge%2520primarily%2520involves%2520two%2520key%2520tasks%253A%2520pedestrian%250Adetection%2520and%2520re-identification.%2520While%2520significant%2520progress%2520has%2520been%2520achieved%250Ain%2520pedestrian%2520detection%2520tasks%2520in%2520recent%2520years%252C%2520enhancing%2520the%2520effectiveness%2520of%250Are-identification%2520tasks%2520remains%2520a%2520persistent%2520challenge.%2520This%2520difficulty%2520arises%250Afrom%2520the%2520large%2520total%2520number%2520of%2520pedestrian%2520samples%2520in%2520multi-object%2520tracking%250Adatasets%2520and%2520the%2520scarcity%2520of%2520individual%2520instance%2520samples.%2520Motivated%2520by%2520recent%250Arapid%2520advancements%2520in%2520meta-learning%2520techniques%252C%2520we%2520introduce%2520MAML%2520MOT%252C%2520a%250Ameta-learning-based%2520training%2520approach%2520for%2520multi-object%2520tracking.%2520This%2520approach%250Aleverages%2520the%2520rapid%2520learning%2520capability%2520of%2520meta-learning%2520to%2520tackle%2520the%2520issue%2520of%250Asample%2520scarcity%2520in%2520pedestrian%2520re-identification%2520tasks%252C%2520aiming%2520to%2520improve%2520the%250Amodel%2527s%2520generalization%2520performance%2520and%2520robustness.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520high%2520accuracy%2520on%2520mainstream%250Adatasets%2520in%2520the%2520MOT%2520Challenge.%2520This%2520offers%2520new%2520perspectives%2520and%2520solutions%2520for%250Aresearch%2520in%2520the%2520field%2520of%2520pedestrian%2520multi-object%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07272v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAML%20MOT%3A%20Multiple%20Object%20Tracking%20based%20on%20Meta-Learning&entry.906535625=Jiayi%20Chen%20and%20Chunhua%20Deng&entry.1292438233=%20%20With%20the%20advancement%20of%20video%20analysis%20technology%2C%20the%20multi-object%20tracking%0A%28MOT%29%20problem%20in%20complex%20scenes%20involving%20pedestrians%20is%20gaining%20increasing%0Aimportance.%20This%20challenge%20primarily%20involves%20two%20key%20tasks%3A%20pedestrian%0Adetection%20and%20re-identification.%20While%20significant%20progress%20has%20been%20achieved%0Ain%20pedestrian%20detection%20tasks%20in%20recent%20years%2C%20enhancing%20the%20effectiveness%20of%0Are-identification%20tasks%20remains%20a%20persistent%20challenge.%20This%20difficulty%20arises%0Afrom%20the%20large%20total%20number%20of%20pedestrian%20samples%20in%20multi-object%20tracking%0Adatasets%20and%20the%20scarcity%20of%20individual%20instance%20samples.%20Motivated%20by%20recent%0Arapid%20advancements%20in%20meta-learning%20techniques%2C%20we%20introduce%20MAML%20MOT%2C%20a%0Ameta-learning-based%20training%20approach%20for%20multi-object%20tracking.%20This%20approach%0Aleverages%20the%20rapid%20learning%20capability%20of%20meta-learning%20to%20tackle%20the%20issue%20of%0Asample%20scarcity%20in%20pedestrian%20re-identification%20tasks%2C%20aiming%20to%20improve%20the%0Amodel%27s%20generalization%20performance%20and%20robustness.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20high%20accuracy%20on%20mainstream%0Adatasets%20in%20the%20MOT%20Challenge.%20This%20offers%20new%20perspectives%20and%20solutions%20for%0Aresearch%20in%20the%20field%20of%20pedestrian%20multi-object%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07272v3&entry.124074799=Read"},
{"title": "Tactile-Morph Skills: Energy-Based Control Meets Data-Driven Learning", "author": "Anran Zhang and K\u00fcbra Karacan and Hamid Sadeghian and Yansong Wu and Fan Wu and Sami Haddadin", "abstract": "  Robotic manipulation is essential for modernizing factories and automating\nindustrial tasks like polishing, which require advanced tactile abilities.\nThese robots must be easily set up, safely work with humans, learn tasks\nautonomously, and transfer skills to similar tasks. Addressing these needs, we\nintroduce the tactile-morph skill framework, which integrates unified\nforce-impedance control with data-driven learning. Our system adjusts robot\nmovements and force application based on estimated energy levels for the\ndesired trajectory and force profile, ensuring safety by stopping if energy\nallocated for the control runs out. Using a Temporal Convolutional Network, we\nestimate the energy distribution for a given motion and force profile, enabling\nskill transfer across different tasks and surfaces. Our approach maintains\nstability and performance even on unfamiliar geometries with similar friction\ncharacteristics, demonstrating improved accuracy, zero-shot transferable\nperformance, and enhanced safety in real-world scenarios. This framework\npromises to enhance robotic capabilities in industrial settings, making\nintelligent robots more accessible and valuable.\n", "link": "http://arxiv.org/abs/2408.12285v2", "date": "2024-08-23", "relevancy": 1.693, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6043}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5879}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tactile-Morph%20Skills%3A%20Energy-Based%20Control%20Meets%20Data-Driven%20Learning&body=Title%3A%20Tactile-Morph%20Skills%3A%20Energy-Based%20Control%20Meets%20Data-Driven%20Learning%0AAuthor%3A%20Anran%20Zhang%20and%20K%C3%BCbra%20Karacan%20and%20Hamid%20Sadeghian%20and%20Yansong%20Wu%20and%20Fan%20Wu%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20Robotic%20manipulation%20is%20essential%20for%20modernizing%20factories%20and%20automating%0Aindustrial%20tasks%20like%20polishing%2C%20which%20require%20advanced%20tactile%20abilities.%0AThese%20robots%20must%20be%20easily%20set%20up%2C%20safely%20work%20with%20humans%2C%20learn%20tasks%0Aautonomously%2C%20and%20transfer%20skills%20to%20similar%20tasks.%20Addressing%20these%20needs%2C%20we%0Aintroduce%20the%20tactile-morph%20skill%20framework%2C%20which%20integrates%20unified%0Aforce-impedance%20control%20with%20data-driven%20learning.%20Our%20system%20adjusts%20robot%0Amovements%20and%20force%20application%20based%20on%20estimated%20energy%20levels%20for%20the%0Adesired%20trajectory%20and%20force%20profile%2C%20ensuring%20safety%20by%20stopping%20if%20energy%0Aallocated%20for%20the%20control%20runs%20out.%20Using%20a%20Temporal%20Convolutional%20Network%2C%20we%0Aestimate%20the%20energy%20distribution%20for%20a%20given%20motion%20and%20force%20profile%2C%20enabling%0Askill%20transfer%20across%20different%20tasks%20and%20surfaces.%20Our%20approach%20maintains%0Astability%20and%20performance%20even%20on%20unfamiliar%20geometries%20with%20similar%20friction%0Acharacteristics%2C%20demonstrating%20improved%20accuracy%2C%20zero-shot%20transferable%0Aperformance%2C%20and%20enhanced%20safety%20in%20real-world%20scenarios.%20This%20framework%0Apromises%20to%20enhance%20robotic%20capabilities%20in%20industrial%20settings%2C%20making%0Aintelligent%20robots%20more%20accessible%20and%20valuable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactile-Morph%2520Skills%253A%2520Energy-Based%2520Control%2520Meets%2520Data-Driven%2520Learning%26entry.906535625%3DAnran%2520Zhang%2520and%2520K%25C3%25BCbra%2520Karacan%2520and%2520Hamid%2520Sadeghian%2520and%2520Yansong%2520Wu%2520and%2520Fan%2520Wu%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%2520is%2520essential%2520for%2520modernizing%2520factories%2520and%2520automating%250Aindustrial%2520tasks%2520like%2520polishing%252C%2520which%2520require%2520advanced%2520tactile%2520abilities.%250AThese%2520robots%2520must%2520be%2520easily%2520set%2520up%252C%2520safely%2520work%2520with%2520humans%252C%2520learn%2520tasks%250Aautonomously%252C%2520and%2520transfer%2520skills%2520to%2520similar%2520tasks.%2520Addressing%2520these%2520needs%252C%2520we%250Aintroduce%2520the%2520tactile-morph%2520skill%2520framework%252C%2520which%2520integrates%2520unified%250Aforce-impedance%2520control%2520with%2520data-driven%2520learning.%2520Our%2520system%2520adjusts%2520robot%250Amovements%2520and%2520force%2520application%2520based%2520on%2520estimated%2520energy%2520levels%2520for%2520the%250Adesired%2520trajectory%2520and%2520force%2520profile%252C%2520ensuring%2520safety%2520by%2520stopping%2520if%2520energy%250Aallocated%2520for%2520the%2520control%2520runs%2520out.%2520Using%2520a%2520Temporal%2520Convolutional%2520Network%252C%2520we%250Aestimate%2520the%2520energy%2520distribution%2520for%2520a%2520given%2520motion%2520and%2520force%2520profile%252C%2520enabling%250Askill%2520transfer%2520across%2520different%2520tasks%2520and%2520surfaces.%2520Our%2520approach%2520maintains%250Astability%2520and%2520performance%2520even%2520on%2520unfamiliar%2520geometries%2520with%2520similar%2520friction%250Acharacteristics%252C%2520demonstrating%2520improved%2520accuracy%252C%2520zero-shot%2520transferable%250Aperformance%252C%2520and%2520enhanced%2520safety%2520in%2520real-world%2520scenarios.%2520This%2520framework%250Apromises%2520to%2520enhance%2520robotic%2520capabilities%2520in%2520industrial%2520settings%252C%2520making%250Aintelligent%2520robots%2520more%2520accessible%2520and%2520valuable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tactile-Morph%20Skills%3A%20Energy-Based%20Control%20Meets%20Data-Driven%20Learning&entry.906535625=Anran%20Zhang%20and%20K%C3%BCbra%20Karacan%20and%20Hamid%20Sadeghian%20and%20Yansong%20Wu%20and%20Fan%20Wu%20and%20Sami%20Haddadin&entry.1292438233=%20%20Robotic%20manipulation%20is%20essential%20for%20modernizing%20factories%20and%20automating%0Aindustrial%20tasks%20like%20polishing%2C%20which%20require%20advanced%20tactile%20abilities.%0AThese%20robots%20must%20be%20easily%20set%20up%2C%20safely%20work%20with%20humans%2C%20learn%20tasks%0Aautonomously%2C%20and%20transfer%20skills%20to%20similar%20tasks.%20Addressing%20these%20needs%2C%20we%0Aintroduce%20the%20tactile-morph%20skill%20framework%2C%20which%20integrates%20unified%0Aforce-impedance%20control%20with%20data-driven%20learning.%20Our%20system%20adjusts%20robot%0Amovements%20and%20force%20application%20based%20on%20estimated%20energy%20levels%20for%20the%0Adesired%20trajectory%20and%20force%20profile%2C%20ensuring%20safety%20by%20stopping%20if%20energy%0Aallocated%20for%20the%20control%20runs%20out.%20Using%20a%20Temporal%20Convolutional%20Network%2C%20we%0Aestimate%20the%20energy%20distribution%20for%20a%20given%20motion%20and%20force%20profile%2C%20enabling%0Askill%20transfer%20across%20different%20tasks%20and%20surfaces.%20Our%20approach%20maintains%0Astability%20and%20performance%20even%20on%20unfamiliar%20geometries%20with%20similar%20friction%0Acharacteristics%2C%20demonstrating%20improved%20accuracy%2C%20zero-shot%20transferable%0Aperformance%2C%20and%20enhanced%20safety%20in%20real-world%20scenarios.%20This%20framework%0Apromises%20to%20enhance%20robotic%20capabilities%20in%20industrial%20settings%2C%20making%0Aintelligent%20robots%20more%20accessible%20and%20valuable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12285v2&entry.124074799=Read"},
{"title": "A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear\n  Systems", "author": "El Houcine Bergou and Soumia Boucherouite and Aritra Dutta and Xin Li and Anna Ma", "abstract": "  Large-scale linear systems, $Ax=b$, frequently arise in practice and demand\neffective iterative solvers. Often, these systems are noisy due to operational\nerrors or faulty data-collection processes. In the past decade, the randomized\nKaczmarz (RK) algorithm has been studied extensively as an efficient iterative\nsolver for such systems. However, the convergence study of RK in the noisy\nregime is limited and considers measurement noise in the right-hand side\nvector, $b$. Unfortunately, in practice, that is not always the case; the\ncoefficient matrix $A$ can also be noisy. In this paper, we analyze the\nconvergence of RK for {\\textit{doubly-noisy} linear systems, i.e., when the\ncoefficient matrix, $A$, has additive or multiplicative noise, and $b$ is also\nnoisy}. In our analyses, the quantity $\\tilde R=\\| \\tilde A^{\\dagger} \\|^2\n\\|\\tilde A \\|_F^2$ influences the convergence of RK, where $\\tilde A$\nrepresents a noisy version of $A$. We claim that our analysis is robust and\nrealistically applicable, as we do not require information about the noiseless\ncoefficient matrix, $A$, and considering different conditions on noise, we can\ncontrol the convergence of RK. {We perform numerical experiments to\nsubstantiate our theoretical findings.}\n", "link": "http://arxiv.org/abs/2308.16904v2", "date": "2024-08-23", "relevancy": 1.6908, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4234}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Note%20on%20Randomized%20Kaczmarz%20Algorithm%20for%20Solving%20Doubly-Noisy%20Linear%0A%20%20Systems&body=Title%3A%20A%20Note%20on%20Randomized%20Kaczmarz%20Algorithm%20for%20Solving%20Doubly-Noisy%20Linear%0A%20%20Systems%0AAuthor%3A%20El%20Houcine%20Bergou%20and%20Soumia%20Boucherouite%20and%20Aritra%20Dutta%20and%20Xin%20Li%20and%20Anna%20Ma%0AAbstract%3A%20%20%20Large-scale%20linear%20systems%2C%20%24Ax%3Db%24%2C%20frequently%20arise%20in%20practice%20and%20demand%0Aeffective%20iterative%20solvers.%20Often%2C%20these%20systems%20are%20noisy%20due%20to%20operational%0Aerrors%20or%20faulty%20data-collection%20processes.%20In%20the%20past%20decade%2C%20the%20randomized%0AKaczmarz%20%28RK%29%20algorithm%20has%20been%20studied%20extensively%20as%20an%20efficient%20iterative%0Asolver%20for%20such%20systems.%20However%2C%20the%20convergence%20study%20of%20RK%20in%20the%20noisy%0Aregime%20is%20limited%20and%20considers%20measurement%20noise%20in%20the%20right-hand%20side%0Avector%2C%20%24b%24.%20Unfortunately%2C%20in%20practice%2C%20that%20is%20not%20always%20the%20case%3B%20the%0Acoefficient%20matrix%20%24A%24%20can%20also%20be%20noisy.%20In%20this%20paper%2C%20we%20analyze%20the%0Aconvergence%20of%20RK%20for%20%7B%5Ctextit%7Bdoubly-noisy%7D%20linear%20systems%2C%20i.e.%2C%20when%20the%0Acoefficient%20matrix%2C%20%24A%24%2C%20has%20additive%20or%20multiplicative%20noise%2C%20and%20%24b%24%20is%20also%0Anoisy%7D.%20In%20our%20analyses%2C%20the%20quantity%20%24%5Ctilde%20R%3D%5C%7C%20%5Ctilde%20A%5E%7B%5Cdagger%7D%20%5C%7C%5E2%0A%5C%7C%5Ctilde%20A%20%5C%7C_F%5E2%24%20influences%20the%20convergence%20of%20RK%2C%20where%20%24%5Ctilde%20A%24%0Arepresents%20a%20noisy%20version%20of%20%24A%24.%20We%20claim%20that%20our%20analysis%20is%20robust%20and%0Arealistically%20applicable%2C%20as%20we%20do%20not%20require%20information%20about%20the%20noiseless%0Acoefficient%20matrix%2C%20%24A%24%2C%20and%20considering%20different%20conditions%20on%20noise%2C%20we%20can%0Acontrol%20the%20convergence%20of%20RK.%20%7BWe%20perform%20numerical%20experiments%20to%0Asubstantiate%20our%20theoretical%20findings.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.16904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Note%2520on%2520Randomized%2520Kaczmarz%2520Algorithm%2520for%2520Solving%2520Doubly-Noisy%2520Linear%250A%2520%2520Systems%26entry.906535625%3DEl%2520Houcine%2520Bergou%2520and%2520Soumia%2520Boucherouite%2520and%2520Aritra%2520Dutta%2520and%2520Xin%2520Li%2520and%2520Anna%2520Ma%26entry.1292438233%3D%2520%2520Large-scale%2520linear%2520systems%252C%2520%2524Ax%253Db%2524%252C%2520frequently%2520arise%2520in%2520practice%2520and%2520demand%250Aeffective%2520iterative%2520solvers.%2520Often%252C%2520these%2520systems%2520are%2520noisy%2520due%2520to%2520operational%250Aerrors%2520or%2520faulty%2520data-collection%2520processes.%2520In%2520the%2520past%2520decade%252C%2520the%2520randomized%250AKaczmarz%2520%2528RK%2529%2520algorithm%2520has%2520been%2520studied%2520extensively%2520as%2520an%2520efficient%2520iterative%250Asolver%2520for%2520such%2520systems.%2520However%252C%2520the%2520convergence%2520study%2520of%2520RK%2520in%2520the%2520noisy%250Aregime%2520is%2520limited%2520and%2520considers%2520measurement%2520noise%2520in%2520the%2520right-hand%2520side%250Avector%252C%2520%2524b%2524.%2520Unfortunately%252C%2520in%2520practice%252C%2520that%2520is%2520not%2520always%2520the%2520case%253B%2520the%250Acoefficient%2520matrix%2520%2524A%2524%2520can%2520also%2520be%2520noisy.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%250Aconvergence%2520of%2520RK%2520for%2520%257B%255Ctextit%257Bdoubly-noisy%257D%2520linear%2520systems%252C%2520i.e.%252C%2520when%2520the%250Acoefficient%2520matrix%252C%2520%2524A%2524%252C%2520has%2520additive%2520or%2520multiplicative%2520noise%252C%2520and%2520%2524b%2524%2520is%2520also%250Anoisy%257D.%2520In%2520our%2520analyses%252C%2520the%2520quantity%2520%2524%255Ctilde%2520R%253D%255C%257C%2520%255Ctilde%2520A%255E%257B%255Cdagger%257D%2520%255C%257C%255E2%250A%255C%257C%255Ctilde%2520A%2520%255C%257C_F%255E2%2524%2520influences%2520the%2520convergence%2520of%2520RK%252C%2520where%2520%2524%255Ctilde%2520A%2524%250Arepresents%2520a%2520noisy%2520version%2520of%2520%2524A%2524.%2520We%2520claim%2520that%2520our%2520analysis%2520is%2520robust%2520and%250Arealistically%2520applicable%252C%2520as%2520we%2520do%2520not%2520require%2520information%2520about%2520the%2520noiseless%250Acoefficient%2520matrix%252C%2520%2524A%2524%252C%2520and%2520considering%2520different%2520conditions%2520on%2520noise%252C%2520we%2520can%250Acontrol%2520the%2520convergence%2520of%2520RK.%2520%257BWe%2520perform%2520numerical%2520experiments%2520to%250Asubstantiate%2520our%2520theoretical%2520findings.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.16904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Note%20on%20Randomized%20Kaczmarz%20Algorithm%20for%20Solving%20Doubly-Noisy%20Linear%0A%20%20Systems&entry.906535625=El%20Houcine%20Bergou%20and%20Soumia%20Boucherouite%20and%20Aritra%20Dutta%20and%20Xin%20Li%20and%20Anna%20Ma&entry.1292438233=%20%20Large-scale%20linear%20systems%2C%20%24Ax%3Db%24%2C%20frequently%20arise%20in%20practice%20and%20demand%0Aeffective%20iterative%20solvers.%20Often%2C%20these%20systems%20are%20noisy%20due%20to%20operational%0Aerrors%20or%20faulty%20data-collection%20processes.%20In%20the%20past%20decade%2C%20the%20randomized%0AKaczmarz%20%28RK%29%20algorithm%20has%20been%20studied%20extensively%20as%20an%20efficient%20iterative%0Asolver%20for%20such%20systems.%20However%2C%20the%20convergence%20study%20of%20RK%20in%20the%20noisy%0Aregime%20is%20limited%20and%20considers%20measurement%20noise%20in%20the%20right-hand%20side%0Avector%2C%20%24b%24.%20Unfortunately%2C%20in%20practice%2C%20that%20is%20not%20always%20the%20case%3B%20the%0Acoefficient%20matrix%20%24A%24%20can%20also%20be%20noisy.%20In%20this%20paper%2C%20we%20analyze%20the%0Aconvergence%20of%20RK%20for%20%7B%5Ctextit%7Bdoubly-noisy%7D%20linear%20systems%2C%20i.e.%2C%20when%20the%0Acoefficient%20matrix%2C%20%24A%24%2C%20has%20additive%20or%20multiplicative%20noise%2C%20and%20%24b%24%20is%20also%0Anoisy%7D.%20In%20our%20analyses%2C%20the%20quantity%20%24%5Ctilde%20R%3D%5C%7C%20%5Ctilde%20A%5E%7B%5Cdagger%7D%20%5C%7C%5E2%0A%5C%7C%5Ctilde%20A%20%5C%7C_F%5E2%24%20influences%20the%20convergence%20of%20RK%2C%20where%20%24%5Ctilde%20A%24%0Arepresents%20a%20noisy%20version%20of%20%24A%24.%20We%20claim%20that%20our%20analysis%20is%20robust%20and%0Arealistically%20applicable%2C%20as%20we%20do%20not%20require%20information%20about%20the%20noiseless%0Acoefficient%20matrix%2C%20%24A%24%2C%20and%20considering%20different%20conditions%20on%20noise%2C%20we%20can%0Acontrol%20the%20convergence%20of%20RK.%20%7BWe%20perform%20numerical%20experiments%20to%0Asubstantiate%20our%20theoretical%20findings.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.16904v2&entry.124074799=Read"},
{"title": "HBIC: A Biclustering Algorithm for Heterogeneous Datasets", "author": "Ad\u00e1n Jos\u00e9-Garc\u00eda and Julie Jacques and Cl\u00e9ment Chauvet and Vincent Sobanski and Clarisse Dhaenens", "abstract": "  Biclustering is an unsupervised machine-learning approach aiming to cluster\nrows and columns simultaneously in a data matrix. Several biclustering\nalgorithms have been proposed for handling numeric datasets. However,\nreal-world data mining problems often involve heterogeneous datasets with mixed\nattributes. To address this challenge, we introduce a biclustering approach\ncalled HBIC, capable of discovering meaningful biclusters in complex\nheterogeneous data, including numeric, binary, and categorical data. The\napproach comprises two stages: bicluster generation and bicluster model\nselection. In the initial stage, several candidate biclusters are generated\niteratively by adding and removing rows and columns based on the frequency of\nvalues in the original matrix. In the second stage, we introduce two approaches\nfor selecting the most suitable biclusters by considering their size and\nhomogeneity. Through a series of experiments, we investigated the suitability\nof our approach on a synthetic benchmark and in a biomedical application\ninvolving clinical data of systemic sclerosis patients. The evaluation\ncomparing our method to existing approaches demonstrates its ability to\ndiscover high-quality biclusters from heterogeneous data. Our biclustering\napproach is a starting point for heterogeneous bicluster discovery, leading to\na better understanding of complex underlying data structures.\n", "link": "http://arxiv.org/abs/2408.13217v1", "date": "2024-08-23", "relevancy": 1.676, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4202}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HBIC%3A%20A%20Biclustering%20Algorithm%20for%20Heterogeneous%20Datasets&body=Title%3A%20HBIC%3A%20A%20Biclustering%20Algorithm%20for%20Heterogeneous%20Datasets%0AAuthor%3A%20Ad%C3%A1n%20Jos%C3%A9-Garc%C3%ADa%20and%20Julie%20Jacques%20and%20Cl%C3%A9ment%20Chauvet%20and%20Vincent%20Sobanski%20and%20Clarisse%20Dhaenens%0AAbstract%3A%20%20%20Biclustering%20is%20an%20unsupervised%20machine-learning%20approach%20aiming%20to%20cluster%0Arows%20and%20columns%20simultaneously%20in%20a%20data%20matrix.%20Several%20biclustering%0Aalgorithms%20have%20been%20proposed%20for%20handling%20numeric%20datasets.%20However%2C%0Areal-world%20data%20mining%20problems%20often%20involve%20heterogeneous%20datasets%20with%20mixed%0Aattributes.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20biclustering%20approach%0Acalled%20HBIC%2C%20capable%20of%20discovering%20meaningful%20biclusters%20in%20complex%0Aheterogeneous%20data%2C%20including%20numeric%2C%20binary%2C%20and%20categorical%20data.%20The%0Aapproach%20comprises%20two%20stages%3A%20bicluster%20generation%20and%20bicluster%20model%0Aselection.%20In%20the%20initial%20stage%2C%20several%20candidate%20biclusters%20are%20generated%0Aiteratively%20by%20adding%20and%20removing%20rows%20and%20columns%20based%20on%20the%20frequency%20of%0Avalues%20in%20the%20original%20matrix.%20In%20the%20second%20stage%2C%20we%20introduce%20two%20approaches%0Afor%20selecting%20the%20most%20suitable%20biclusters%20by%20considering%20their%20size%20and%0Ahomogeneity.%20Through%20a%20series%20of%20experiments%2C%20we%20investigated%20the%20suitability%0Aof%20our%20approach%20on%20a%20synthetic%20benchmark%20and%20in%20a%20biomedical%20application%0Ainvolving%20clinical%20data%20of%20systemic%20sclerosis%20patients.%20The%20evaluation%0Acomparing%20our%20method%20to%20existing%20approaches%20demonstrates%20its%20ability%20to%0Adiscover%20high-quality%20biclusters%20from%20heterogeneous%20data.%20Our%20biclustering%0Aapproach%20is%20a%20starting%20point%20for%20heterogeneous%20bicluster%20discovery%2C%20leading%20to%0Aa%20better%20understanding%20of%20complex%20underlying%20data%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHBIC%253A%2520A%2520Biclustering%2520Algorithm%2520for%2520Heterogeneous%2520Datasets%26entry.906535625%3DAd%25C3%25A1n%2520Jos%25C3%25A9-Garc%25C3%25ADa%2520and%2520Julie%2520Jacques%2520and%2520Cl%25C3%25A9ment%2520Chauvet%2520and%2520Vincent%2520Sobanski%2520and%2520Clarisse%2520Dhaenens%26entry.1292438233%3D%2520%2520Biclustering%2520is%2520an%2520unsupervised%2520machine-learning%2520approach%2520aiming%2520to%2520cluster%250Arows%2520and%2520columns%2520simultaneously%2520in%2520a%2520data%2520matrix.%2520Several%2520biclustering%250Aalgorithms%2520have%2520been%2520proposed%2520for%2520handling%2520numeric%2520datasets.%2520However%252C%250Areal-world%2520data%2520mining%2520problems%2520often%2520involve%2520heterogeneous%2520datasets%2520with%2520mixed%250Aattributes.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520biclustering%2520approach%250Acalled%2520HBIC%252C%2520capable%2520of%2520discovering%2520meaningful%2520biclusters%2520in%2520complex%250Aheterogeneous%2520data%252C%2520including%2520numeric%252C%2520binary%252C%2520and%2520categorical%2520data.%2520The%250Aapproach%2520comprises%2520two%2520stages%253A%2520bicluster%2520generation%2520and%2520bicluster%2520model%250Aselection.%2520In%2520the%2520initial%2520stage%252C%2520several%2520candidate%2520biclusters%2520are%2520generated%250Aiteratively%2520by%2520adding%2520and%2520removing%2520rows%2520and%2520columns%2520based%2520on%2520the%2520frequency%2520of%250Avalues%2520in%2520the%2520original%2520matrix.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520two%2520approaches%250Afor%2520selecting%2520the%2520most%2520suitable%2520biclusters%2520by%2520considering%2520their%2520size%2520and%250Ahomogeneity.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520we%2520investigated%2520the%2520suitability%250Aof%2520our%2520approach%2520on%2520a%2520synthetic%2520benchmark%2520and%2520in%2520a%2520biomedical%2520application%250Ainvolving%2520clinical%2520data%2520of%2520systemic%2520sclerosis%2520patients.%2520The%2520evaluation%250Acomparing%2520our%2520method%2520to%2520existing%2520approaches%2520demonstrates%2520its%2520ability%2520to%250Adiscover%2520high-quality%2520biclusters%2520from%2520heterogeneous%2520data.%2520Our%2520biclustering%250Aapproach%2520is%2520a%2520starting%2520point%2520for%2520heterogeneous%2520bicluster%2520discovery%252C%2520leading%2520to%250Aa%2520better%2520understanding%2520of%2520complex%2520underlying%2520data%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HBIC%3A%20A%20Biclustering%20Algorithm%20for%20Heterogeneous%20Datasets&entry.906535625=Ad%C3%A1n%20Jos%C3%A9-Garc%C3%ADa%20and%20Julie%20Jacques%20and%20Cl%C3%A9ment%20Chauvet%20and%20Vincent%20Sobanski%20and%20Clarisse%20Dhaenens&entry.1292438233=%20%20Biclustering%20is%20an%20unsupervised%20machine-learning%20approach%20aiming%20to%20cluster%0Arows%20and%20columns%20simultaneously%20in%20a%20data%20matrix.%20Several%20biclustering%0Aalgorithms%20have%20been%20proposed%20for%20handling%20numeric%20datasets.%20However%2C%0Areal-world%20data%20mining%20problems%20often%20involve%20heterogeneous%20datasets%20with%20mixed%0Aattributes.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20biclustering%20approach%0Acalled%20HBIC%2C%20capable%20of%20discovering%20meaningful%20biclusters%20in%20complex%0Aheterogeneous%20data%2C%20including%20numeric%2C%20binary%2C%20and%20categorical%20data.%20The%0Aapproach%20comprises%20two%20stages%3A%20bicluster%20generation%20and%20bicluster%20model%0Aselection.%20In%20the%20initial%20stage%2C%20several%20candidate%20biclusters%20are%20generated%0Aiteratively%20by%20adding%20and%20removing%20rows%20and%20columns%20based%20on%20the%20frequency%20of%0Avalues%20in%20the%20original%20matrix.%20In%20the%20second%20stage%2C%20we%20introduce%20two%20approaches%0Afor%20selecting%20the%20most%20suitable%20biclusters%20by%20considering%20their%20size%20and%0Ahomogeneity.%20Through%20a%20series%20of%20experiments%2C%20we%20investigated%20the%20suitability%0Aof%20our%20approach%20on%20a%20synthetic%20benchmark%20and%20in%20a%20biomedical%20application%0Ainvolving%20clinical%20data%20of%20systemic%20sclerosis%20patients.%20The%20evaluation%0Acomparing%20our%20method%20to%20existing%20approaches%20demonstrates%20its%20ability%20to%0Adiscover%20high-quality%20biclusters%20from%20heterogeneous%20data.%20Our%20biclustering%0Aapproach%20is%20a%20starting%20point%20for%20heterogeneous%20bicluster%20discovery%2C%20leading%20to%0Aa%20better%20understanding%20of%20complex%20underlying%20data%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13217v1&entry.124074799=Read"},
{"title": "Evidential Deep Partial Multi-View Classification With Discount Fusion", "author": "Haojian Huang and Zhe Liu and Sukumar Letchmunan and Mingwei Lin and Muhammet Deveci and Witold Pedrycz and Patrick Siarry", "abstract": "  Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.13123v1", "date": "2024-08-23", "relevancy": 1.6695, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6009}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5514}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidential%20Deep%20Partial%20Multi-View%20Classification%20With%20Discount%20Fusion&body=Title%3A%20Evidential%20Deep%20Partial%20Multi-View%20Classification%20With%20Discount%20Fusion%0AAuthor%3A%20Haojian%20Huang%20and%20Zhe%20Liu%20and%20Sukumar%20Letchmunan%20and%20Mingwei%20Lin%20and%20Muhammet%20Deveci%20and%20Witold%20Pedrycz%20and%20Patrick%20Siarry%0AAbstract%3A%20%20%20Incomplete%20multi-view%20data%20classification%20poses%20significant%20challenges%20due%20to%0Athe%20common%20issue%20of%20missing%20views%20in%20real-world%20scenarios.%20Despite%0Aadvancements%2C%20existing%20methods%20often%20fail%20to%20provide%20reliable%20predictions%2C%0Alargely%20due%20to%20the%20uncertainty%20of%20missing%20views%20and%20the%20inconsistent%20quality%20of%0Aimputed%20data.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20novel%20framework%20called%0AEvidential%20Deep%20Partial%20Multi-View%20Classification%20%28EDP-MVC%29.%20Initially%2C%20we%20use%0AK-means%20imputation%20to%20address%20missing%20views%2C%20creating%20a%20complete%20set%20of%0Amulti-view%20data.%20However%2C%20the%20potential%20conflicts%20and%20uncertainties%20within%20this%0Aimputed%20data%20can%20affect%20the%20reliability%20of%20downstream%20inferences.%20To%20manage%0Athis%2C%20we%20introduce%20a%20Conflict-Aware%20Evidential%20Fusion%20Network%20%28CAEFN%29%2C%20which%0Adynamically%20adjusts%20based%20on%20the%20reliability%20of%20the%20evidence%2C%20ensuring%0Atrustworthy%20discount%20fusion%20and%20producing%20reliable%20inference%20outcomes.%0AComprehensive%20experiments%20on%20various%20benchmark%20datasets%20reveal%20EDP-MVC%20not%20only%0Amatches%20but%20often%20surpasses%20the%20performance%20of%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidential%2520Deep%2520Partial%2520Multi-View%2520Classification%2520With%2520Discount%2520Fusion%26entry.906535625%3DHaojian%2520Huang%2520and%2520Zhe%2520Liu%2520and%2520Sukumar%2520Letchmunan%2520and%2520Mingwei%2520Lin%2520and%2520Muhammet%2520Deveci%2520and%2520Witold%2520Pedrycz%2520and%2520Patrick%2520Siarry%26entry.1292438233%3D%2520%2520Incomplete%2520multi-view%2520data%2520classification%2520poses%2520significant%2520challenges%2520due%2520to%250Athe%2520common%2520issue%2520of%2520missing%2520views%2520in%2520real-world%2520scenarios.%2520Despite%250Aadvancements%252C%2520existing%2520methods%2520often%2520fail%2520to%2520provide%2520reliable%2520predictions%252C%250Alargely%2520due%2520to%2520the%2520uncertainty%2520of%2520missing%2520views%2520and%2520the%2520inconsistent%2520quality%2520of%250Aimputed%2520data.%2520To%2520tackle%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%250AEvidential%2520Deep%2520Partial%2520Multi-View%2520Classification%2520%2528EDP-MVC%2529.%2520Initially%252C%2520we%2520use%250AK-means%2520imputation%2520to%2520address%2520missing%2520views%252C%2520creating%2520a%2520complete%2520set%2520of%250Amulti-view%2520data.%2520However%252C%2520the%2520potential%2520conflicts%2520and%2520uncertainties%2520within%2520this%250Aimputed%2520data%2520can%2520affect%2520the%2520reliability%2520of%2520downstream%2520inferences.%2520To%2520manage%250Athis%252C%2520we%2520introduce%2520a%2520Conflict-Aware%2520Evidential%2520Fusion%2520Network%2520%2528CAEFN%2529%252C%2520which%250Adynamically%2520adjusts%2520based%2520on%2520the%2520reliability%2520of%2520the%2520evidence%252C%2520ensuring%250Atrustworthy%2520discount%2520fusion%2520and%2520producing%2520reliable%2520inference%2520outcomes.%250AComprehensive%2520experiments%2520on%2520various%2520benchmark%2520datasets%2520reveal%2520EDP-MVC%2520not%2520only%250Amatches%2520but%2520often%2520surpasses%2520the%2520performance%2520of%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidential%20Deep%20Partial%20Multi-View%20Classification%20With%20Discount%20Fusion&entry.906535625=Haojian%20Huang%20and%20Zhe%20Liu%20and%20Sukumar%20Letchmunan%20and%20Mingwei%20Lin%20and%20Muhammet%20Deveci%20and%20Witold%20Pedrycz%20and%20Patrick%20Siarry&entry.1292438233=%20%20Incomplete%20multi-view%20data%20classification%20poses%20significant%20challenges%20due%20to%0Athe%20common%20issue%20of%20missing%20views%20in%20real-world%20scenarios.%20Despite%0Aadvancements%2C%20existing%20methods%20often%20fail%20to%20provide%20reliable%20predictions%2C%0Alargely%20due%20to%20the%20uncertainty%20of%20missing%20views%20and%20the%20inconsistent%20quality%20of%0Aimputed%20data.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20novel%20framework%20called%0AEvidential%20Deep%20Partial%20Multi-View%20Classification%20%28EDP-MVC%29.%20Initially%2C%20we%20use%0AK-means%20imputation%20to%20address%20missing%20views%2C%20creating%20a%20complete%20set%20of%0Amulti-view%20data.%20However%2C%20the%20potential%20conflicts%20and%20uncertainties%20within%20this%0Aimputed%20data%20can%20affect%20the%20reliability%20of%20downstream%20inferences.%20To%20manage%0Athis%2C%20we%20introduce%20a%20Conflict-Aware%20Evidential%20Fusion%20Network%20%28CAEFN%29%2C%20which%0Adynamically%20adjusts%20based%20on%20the%20reliability%20of%20the%20evidence%2C%20ensuring%0Atrustworthy%20discount%20fusion%20and%20producing%20reliable%20inference%20outcomes.%0AComprehensive%20experiments%20on%20various%20benchmark%20datasets%20reveal%20EDP-MVC%20not%20only%0Amatches%20but%20often%20surpasses%20the%20performance%20of%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13123v1&entry.124074799=Read"},
{"title": "Multi-finger Manipulation via Trajectory Optimization with\n  Differentiable Rolling and Geometric Constraints", "author": "Fan Yang and Thomas Power and Sergio Aguilera Marinovic and Soshi Iba and Rana Soltani Zarrin and Dmitry Berenson", "abstract": "  Parameterizing finger rolling and finger-object contacts in a differentiable\nmanner is important for formulating dexterous manipulation as a trajectory\noptimization problem. In contrast to previous methods which often assume\nsimplified geometries of the robot and object or do not explicitly model finger\nrolling, we propose a method to further extend the capabilities of dexterous\nmanipulation by accounting for non-trivial geometries of both the robot and the\nobject. By integrating the object's Signed Distance Field (SDF) with a sampling\nmethod, our method estimates contact and rolling-related variables and includes\nthose in a trajectory optimization framework. This formulation naturally allows\nfor the emergence of finger-rolling behaviors, enabling the robot to locally\nadjust the contact points. Our method is tested in a peg alignment task and a\nscrewdriver turning task, where it outperforms the baselines in terms of\nachieving desired object configurations and avoiding dropping the object. We\nalso successfully apply our method to a real-world screwdriver turning task,\ndemonstrating its robustness to the sim2real gap.\n", "link": "http://arxiv.org/abs/2408.13229v1", "date": "2024-08-23", "relevancy": 1.6634, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5678}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5603}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-finger%20Manipulation%20via%20Trajectory%20Optimization%20with%0A%20%20Differentiable%20Rolling%20and%20Geometric%20Constraints&body=Title%3A%20Multi-finger%20Manipulation%20via%20Trajectory%20Optimization%20with%0A%20%20Differentiable%20Rolling%20and%20Geometric%20Constraints%0AAuthor%3A%20Fan%20Yang%20and%20Thomas%20Power%20and%20Sergio%20Aguilera%20Marinovic%20and%20Soshi%20Iba%20and%20Rana%20Soltani%20Zarrin%20and%20Dmitry%20Berenson%0AAbstract%3A%20%20%20Parameterizing%20finger%20rolling%20and%20finger-object%20contacts%20in%20a%20differentiable%0Amanner%20is%20important%20for%20formulating%20dexterous%20manipulation%20as%20a%20trajectory%0Aoptimization%20problem.%20In%20contrast%20to%20previous%20methods%20which%20often%20assume%0Asimplified%20geometries%20of%20the%20robot%20and%20object%20or%20do%20not%20explicitly%20model%20finger%0Arolling%2C%20we%20propose%20a%20method%20to%20further%20extend%20the%20capabilities%20of%20dexterous%0Amanipulation%20by%20accounting%20for%20non-trivial%20geometries%20of%20both%20the%20robot%20and%20the%0Aobject.%20By%20integrating%20the%20object%27s%20Signed%20Distance%20Field%20%28SDF%29%20with%20a%20sampling%0Amethod%2C%20our%20method%20estimates%20contact%20and%20rolling-related%20variables%20and%20includes%0Athose%20in%20a%20trajectory%20optimization%20framework.%20This%20formulation%20naturally%20allows%0Afor%20the%20emergence%20of%20finger-rolling%20behaviors%2C%20enabling%20the%20robot%20to%20locally%0Aadjust%20the%20contact%20points.%20Our%20method%20is%20tested%20in%20a%20peg%20alignment%20task%20and%20a%0Ascrewdriver%20turning%20task%2C%20where%20it%20outperforms%20the%20baselines%20in%20terms%20of%0Aachieving%20desired%20object%20configurations%20and%20avoiding%20dropping%20the%20object.%20We%0Aalso%20successfully%20apply%20our%20method%20to%20a%20real-world%20screwdriver%20turning%20task%2C%0Ademonstrating%20its%20robustness%20to%20the%20sim2real%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-finger%2520Manipulation%2520via%2520Trajectory%2520Optimization%2520with%250A%2520%2520Differentiable%2520Rolling%2520and%2520Geometric%2520Constraints%26entry.906535625%3DFan%2520Yang%2520and%2520Thomas%2520Power%2520and%2520Sergio%2520Aguilera%2520Marinovic%2520and%2520Soshi%2520Iba%2520and%2520Rana%2520Soltani%2520Zarrin%2520and%2520Dmitry%2520Berenson%26entry.1292438233%3D%2520%2520Parameterizing%2520finger%2520rolling%2520and%2520finger-object%2520contacts%2520in%2520a%2520differentiable%250Amanner%2520is%2520important%2520for%2520formulating%2520dexterous%2520manipulation%2520as%2520a%2520trajectory%250Aoptimization%2520problem.%2520In%2520contrast%2520to%2520previous%2520methods%2520which%2520often%2520assume%250Asimplified%2520geometries%2520of%2520the%2520robot%2520and%2520object%2520or%2520do%2520not%2520explicitly%2520model%2520finger%250Arolling%252C%2520we%2520propose%2520a%2520method%2520to%2520further%2520extend%2520the%2520capabilities%2520of%2520dexterous%250Amanipulation%2520by%2520accounting%2520for%2520non-trivial%2520geometries%2520of%2520both%2520the%2520robot%2520and%2520the%250Aobject.%2520By%2520integrating%2520the%2520object%2527s%2520Signed%2520Distance%2520Field%2520%2528SDF%2529%2520with%2520a%2520sampling%250Amethod%252C%2520our%2520method%2520estimates%2520contact%2520and%2520rolling-related%2520variables%2520and%2520includes%250Athose%2520in%2520a%2520trajectory%2520optimization%2520framework.%2520This%2520formulation%2520naturally%2520allows%250Afor%2520the%2520emergence%2520of%2520finger-rolling%2520behaviors%252C%2520enabling%2520the%2520robot%2520to%2520locally%250Aadjust%2520the%2520contact%2520points.%2520Our%2520method%2520is%2520tested%2520in%2520a%2520peg%2520alignment%2520task%2520and%2520a%250Ascrewdriver%2520turning%2520task%252C%2520where%2520it%2520outperforms%2520the%2520baselines%2520in%2520terms%2520of%250Aachieving%2520desired%2520object%2520configurations%2520and%2520avoiding%2520dropping%2520the%2520object.%2520We%250Aalso%2520successfully%2520apply%2520our%2520method%2520to%2520a%2520real-world%2520screwdriver%2520turning%2520task%252C%250Ademonstrating%2520its%2520robustness%2520to%2520the%2520sim2real%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-finger%20Manipulation%20via%20Trajectory%20Optimization%20with%0A%20%20Differentiable%20Rolling%20and%20Geometric%20Constraints&entry.906535625=Fan%20Yang%20and%20Thomas%20Power%20and%20Sergio%20Aguilera%20Marinovic%20and%20Soshi%20Iba%20and%20Rana%20Soltani%20Zarrin%20and%20Dmitry%20Berenson&entry.1292438233=%20%20Parameterizing%20finger%20rolling%20and%20finger-object%20contacts%20in%20a%20differentiable%0Amanner%20is%20important%20for%20formulating%20dexterous%20manipulation%20as%20a%20trajectory%0Aoptimization%20problem.%20In%20contrast%20to%20previous%20methods%20which%20often%20assume%0Asimplified%20geometries%20of%20the%20robot%20and%20object%20or%20do%20not%20explicitly%20model%20finger%0Arolling%2C%20we%20propose%20a%20method%20to%20further%20extend%20the%20capabilities%20of%20dexterous%0Amanipulation%20by%20accounting%20for%20non-trivial%20geometries%20of%20both%20the%20robot%20and%20the%0Aobject.%20By%20integrating%20the%20object%27s%20Signed%20Distance%20Field%20%28SDF%29%20with%20a%20sampling%0Amethod%2C%20our%20method%20estimates%20contact%20and%20rolling-related%20variables%20and%20includes%0Athose%20in%20a%20trajectory%20optimization%20framework.%20This%20formulation%20naturally%20allows%0Afor%20the%20emergence%20of%20finger-rolling%20behaviors%2C%20enabling%20the%20robot%20to%20locally%0Aadjust%20the%20contact%20points.%20Our%20method%20is%20tested%20in%20a%20peg%20alignment%20task%20and%20a%0Ascrewdriver%20turning%20task%2C%20where%20it%20outperforms%20the%20baselines%20in%20terms%20of%0Aachieving%20desired%20object%20configurations%20and%20avoiding%20dropping%20the%20object.%20We%0Aalso%20successfully%20apply%20our%20method%20to%20a%20real-world%20screwdriver%20turning%20task%2C%0Ademonstrating%20its%20robustness%20to%20the%20sim2real%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13229v1&entry.124074799=Read"},
{"title": "QD-VMR: Query Debiasing with Contextual Understanding Enhancement for\n  Video Moment Retrieval", "author": "Chenghua Gao and Min Li and Jianshuo Liu and Junxing Ren and Lin Chen and Haoyu Liu and Bo Meng and Jitao Fu and Wenwen Su", "abstract": "  Video Moment Retrieval (VMR) aims to retrieve relevant moments of an\nuntrimmed video corresponding to the query. While cross-modal interaction\napproaches have shown progress in filtering out query-irrelevant information in\nvideos, they assume the precise alignment between the query semantics and the\ncorresponding video moments, potentially overlooking the misunderstanding of\nthe natural language semantics. To address this challenge, we propose a novel\nmodel called \\textit{QD-VMR}, a query debiasing model with enhanced contextual\nunderstanding. Firstly, we leverage a Global Partial Aligner module via video\nclip and query features alignment and video-query contrastive learning to\nenhance the cross-modal understanding capabilities of the model. Subsequently,\nwe employ a Query Debiasing Module to obtain debiased query features\nefficiently, and a Visual Enhancement module to refine the video features\nrelated to the query. Finally, we adopt the DETR structure to predict the\npossible target video moments. Through extensive evaluations of three benchmark\ndatasets, QD-VMR achieves state-of-the-art performance, proving its potential\nto improve the accuracy of VMR. Further analytical experiments demonstrate the\neffectiveness of our proposed module. Our code will be released to facilitate\nfuture research.\n", "link": "http://arxiv.org/abs/2408.12981v1", "date": "2024-08-23", "relevancy": 1.6596, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5648}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5427}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QD-VMR%3A%20Query%20Debiasing%20with%20Contextual%20Understanding%20Enhancement%20for%0A%20%20Video%20Moment%20Retrieval&body=Title%3A%20QD-VMR%3A%20Query%20Debiasing%20with%20Contextual%20Understanding%20Enhancement%20for%0A%20%20Video%20Moment%20Retrieval%0AAuthor%3A%20Chenghua%20Gao%20and%20Min%20Li%20and%20Jianshuo%20Liu%20and%20Junxing%20Ren%20and%20Lin%20Chen%20and%20Haoyu%20Liu%20and%20Bo%20Meng%20and%20Jitao%20Fu%20and%20Wenwen%20Su%0AAbstract%3A%20%20%20Video%20Moment%20Retrieval%20%28VMR%29%20aims%20to%20retrieve%20relevant%20moments%20of%20an%0Auntrimmed%20video%20corresponding%20to%20the%20query.%20While%20cross-modal%20interaction%0Aapproaches%20have%20shown%20progress%20in%20filtering%20out%20query-irrelevant%20information%20in%0Avideos%2C%20they%20assume%20the%20precise%20alignment%20between%20the%20query%20semantics%20and%20the%0Acorresponding%20video%20moments%2C%20potentially%20overlooking%20the%20misunderstanding%20of%0Athe%20natural%20language%20semantics.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Amodel%20called%20%5Ctextit%7BQD-VMR%7D%2C%20a%20query%20debiasing%20model%20with%20enhanced%20contextual%0Aunderstanding.%20Firstly%2C%20we%20leverage%20a%20Global%20Partial%20Aligner%20module%20via%20video%0Aclip%20and%20query%20features%20alignment%20and%20video-query%20contrastive%20learning%20to%0Aenhance%20the%20cross-modal%20understanding%20capabilities%20of%20the%20model.%20Subsequently%2C%0Awe%20employ%20a%20Query%20Debiasing%20Module%20to%20obtain%20debiased%20query%20features%0Aefficiently%2C%20and%20a%20Visual%20Enhancement%20module%20to%20refine%20the%20video%20features%0Arelated%20to%20the%20query.%20Finally%2C%20we%20adopt%20the%20DETR%20structure%20to%20predict%20the%0Apossible%20target%20video%20moments.%20Through%20extensive%20evaluations%20of%20three%20benchmark%0Adatasets%2C%20QD-VMR%20achieves%20state-of-the-art%20performance%2C%20proving%20its%20potential%0Ato%20improve%20the%20accuracy%20of%20VMR.%20Further%20analytical%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20module.%20Our%20code%20will%20be%20released%20to%20facilitate%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQD-VMR%253A%2520Query%2520Debiasing%2520with%2520Contextual%2520Understanding%2520Enhancement%2520for%250A%2520%2520Video%2520Moment%2520Retrieval%26entry.906535625%3DChenghua%2520Gao%2520and%2520Min%2520Li%2520and%2520Jianshuo%2520Liu%2520and%2520Junxing%2520Ren%2520and%2520Lin%2520Chen%2520and%2520Haoyu%2520Liu%2520and%2520Bo%2520Meng%2520and%2520Jitao%2520Fu%2520and%2520Wenwen%2520Su%26entry.1292438233%3D%2520%2520Video%2520Moment%2520Retrieval%2520%2528VMR%2529%2520aims%2520to%2520retrieve%2520relevant%2520moments%2520of%2520an%250Auntrimmed%2520video%2520corresponding%2520to%2520the%2520query.%2520While%2520cross-modal%2520interaction%250Aapproaches%2520have%2520shown%2520progress%2520in%2520filtering%2520out%2520query-irrelevant%2520information%2520in%250Avideos%252C%2520they%2520assume%2520the%2520precise%2520alignment%2520between%2520the%2520query%2520semantics%2520and%2520the%250Acorresponding%2520video%2520moments%252C%2520potentially%2520overlooking%2520the%2520misunderstanding%2520of%250Athe%2520natural%2520language%2520semantics.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%250Amodel%2520called%2520%255Ctextit%257BQD-VMR%257D%252C%2520a%2520query%2520debiasing%2520model%2520with%2520enhanced%2520contextual%250Aunderstanding.%2520Firstly%252C%2520we%2520leverage%2520a%2520Global%2520Partial%2520Aligner%2520module%2520via%2520video%250Aclip%2520and%2520query%2520features%2520alignment%2520and%2520video-query%2520contrastive%2520learning%2520to%250Aenhance%2520the%2520cross-modal%2520understanding%2520capabilities%2520of%2520the%2520model.%2520Subsequently%252C%250Awe%2520employ%2520a%2520Query%2520Debiasing%2520Module%2520to%2520obtain%2520debiased%2520query%2520features%250Aefficiently%252C%2520and%2520a%2520Visual%2520Enhancement%2520module%2520to%2520refine%2520the%2520video%2520features%250Arelated%2520to%2520the%2520query.%2520Finally%252C%2520we%2520adopt%2520the%2520DETR%2520structure%2520to%2520predict%2520the%250Apossible%2520target%2520video%2520moments.%2520Through%2520extensive%2520evaluations%2520of%2520three%2520benchmark%250Adatasets%252C%2520QD-VMR%2520achieves%2520state-of-the-art%2520performance%252C%2520proving%2520its%2520potential%250Ato%2520improve%2520the%2520accuracy%2520of%2520VMR.%2520Further%2520analytical%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520module.%2520Our%2520code%2520will%2520be%2520released%2520to%2520facilitate%250Afuture%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QD-VMR%3A%20Query%20Debiasing%20with%20Contextual%20Understanding%20Enhancement%20for%0A%20%20Video%20Moment%20Retrieval&entry.906535625=Chenghua%20Gao%20and%20Min%20Li%20and%20Jianshuo%20Liu%20and%20Junxing%20Ren%20and%20Lin%20Chen%20and%20Haoyu%20Liu%20and%20Bo%20Meng%20and%20Jitao%20Fu%20and%20Wenwen%20Su&entry.1292438233=%20%20Video%20Moment%20Retrieval%20%28VMR%29%20aims%20to%20retrieve%20relevant%20moments%20of%20an%0Auntrimmed%20video%20corresponding%20to%20the%20query.%20While%20cross-modal%20interaction%0Aapproaches%20have%20shown%20progress%20in%20filtering%20out%20query-irrelevant%20information%20in%0Avideos%2C%20they%20assume%20the%20precise%20alignment%20between%20the%20query%20semantics%20and%20the%0Acorresponding%20video%20moments%2C%20potentially%20overlooking%20the%20misunderstanding%20of%0Athe%20natural%20language%20semantics.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Amodel%20called%20%5Ctextit%7BQD-VMR%7D%2C%20a%20query%20debiasing%20model%20with%20enhanced%20contextual%0Aunderstanding.%20Firstly%2C%20we%20leverage%20a%20Global%20Partial%20Aligner%20module%20via%20video%0Aclip%20and%20query%20features%20alignment%20and%20video-query%20contrastive%20learning%20to%0Aenhance%20the%20cross-modal%20understanding%20capabilities%20of%20the%20model.%20Subsequently%2C%0Awe%20employ%20a%20Query%20Debiasing%20Module%20to%20obtain%20debiased%20query%20features%0Aefficiently%2C%20and%20a%20Visual%20Enhancement%20module%20to%20refine%20the%20video%20features%0Arelated%20to%20the%20query.%20Finally%2C%20we%20adopt%20the%20DETR%20structure%20to%20predict%20the%0Apossible%20target%20video%20moments.%20Through%20extensive%20evaluations%20of%20three%20benchmark%0Adatasets%2C%20QD-VMR%20achieves%20state-of-the-art%20performance%2C%20proving%20its%20potential%0Ato%20improve%20the%20accuracy%20of%20VMR.%20Further%20analytical%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20module.%20Our%20code%20will%20be%20released%20to%20facilitate%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12981v1&entry.124074799=Read"},
{"title": "Complete Autonomous Robotic Nasopharyngeal Swab System with Evaluation\n  on a Stochastically Moving Phantom Head", "author": "Peter Q. Lee and John S. Zelek and Katja Mombaur", "abstract": "  The application of autonomous robotics to close-contact healthcare tasks has\na clear role for the future due to its potential to reduce infection risks to\nstaff and improve clinical efficiency. Nasopharyngeal (NP) swab sample\ncollection for diagnosing upper-respiratory illnesses is one type of close\ncontact task that is interesting for robotics due to the dexterity requirements\nand the unobservability of the nasal cavity. We propose a control system that\nperforms the test using a collaborative manipulator arm with an instrumented\nend-effector to take visual and force measurements, under the scenario that the\npatient is unrestrained and the tools are general enough to be applied to other\nclose contact tasks. The system employs a visual servo controller to align the\nswab with the nostrils. A compliant joint velocity controller inserts the swab\nalong a trajectory optimized through a simulation environment, that also reacts\nto measured forces applied to the swab. Additional subsystems include a fuzzy\nlogic system for detecting when the swab reaches the nasopharynx and a method\nfor detaching the swab and aborting the procedure if safety criteria is\nviolated. The system is evaluated using a second robotic arm that holds a nasal\ncavity phantom and simulates the natural head motions that could occur during\nthe procedure. Through extensive experiments, we identify controller\nconfigurations capable of effectively performing the NP swab test even with\nsignificant head motion, which demonstrates the safety and reliability of the\nsystem.\n", "link": "http://arxiv.org/abs/2408.13100v1", "date": "2024-08-23", "relevancy": 1.6442, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5767}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5252}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complete%20Autonomous%20Robotic%20Nasopharyngeal%20Swab%20System%20with%20Evaluation%0A%20%20on%20a%20Stochastically%20Moving%20Phantom%20Head&body=Title%3A%20Complete%20Autonomous%20Robotic%20Nasopharyngeal%20Swab%20System%20with%20Evaluation%0A%20%20on%20a%20Stochastically%20Moving%20Phantom%20Head%0AAuthor%3A%20Peter%20Q.%20Lee%20and%20John%20S.%20Zelek%20and%20Katja%20Mombaur%0AAbstract%3A%20%20%20The%20application%20of%20autonomous%20robotics%20to%20close-contact%20healthcare%20tasks%20has%0Aa%20clear%20role%20for%20the%20future%20due%20to%20its%20potential%20to%20reduce%20infection%20risks%20to%0Astaff%20and%20improve%20clinical%20efficiency.%20Nasopharyngeal%20%28NP%29%20swab%20sample%0Acollection%20for%20diagnosing%20upper-respiratory%20illnesses%20is%20one%20type%20of%20close%0Acontact%20task%20that%20is%20interesting%20for%20robotics%20due%20to%20the%20dexterity%20requirements%0Aand%20the%20unobservability%20of%20the%20nasal%20cavity.%20We%20propose%20a%20control%20system%20that%0Aperforms%20the%20test%20using%20a%20collaborative%20manipulator%20arm%20with%20an%20instrumented%0Aend-effector%20to%20take%20visual%20and%20force%20measurements%2C%20under%20the%20scenario%20that%20the%0Apatient%20is%20unrestrained%20and%20the%20tools%20are%20general%20enough%20to%20be%20applied%20to%20other%0Aclose%20contact%20tasks.%20The%20system%20employs%20a%20visual%20servo%20controller%20to%20align%20the%0Aswab%20with%20the%20nostrils.%20A%20compliant%20joint%20velocity%20controller%20inserts%20the%20swab%0Aalong%20a%20trajectory%20optimized%20through%20a%20simulation%20environment%2C%20that%20also%20reacts%0Ato%20measured%20forces%20applied%20to%20the%20swab.%20Additional%20subsystems%20include%20a%20fuzzy%0Alogic%20system%20for%20detecting%20when%20the%20swab%20reaches%20the%20nasopharynx%20and%20a%20method%0Afor%20detaching%20the%20swab%20and%20aborting%20the%20procedure%20if%20safety%20criteria%20is%0Aviolated.%20The%20system%20is%20evaluated%20using%20a%20second%20robotic%20arm%20that%20holds%20a%20nasal%0Acavity%20phantom%20and%20simulates%20the%20natural%20head%20motions%20that%20could%20occur%20during%0Athe%20procedure.%20Through%20extensive%20experiments%2C%20we%20identify%20controller%0Aconfigurations%20capable%20of%20effectively%20performing%20the%20NP%20swab%20test%20even%20with%0Asignificant%20head%20motion%2C%20which%20demonstrates%20the%20safety%20and%20reliability%20of%20the%0Asystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplete%2520Autonomous%2520Robotic%2520Nasopharyngeal%2520Swab%2520System%2520with%2520Evaluation%250A%2520%2520on%2520a%2520Stochastically%2520Moving%2520Phantom%2520Head%26entry.906535625%3DPeter%2520Q.%2520Lee%2520and%2520John%2520S.%2520Zelek%2520and%2520Katja%2520Mombaur%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520autonomous%2520robotics%2520to%2520close-contact%2520healthcare%2520tasks%2520has%250Aa%2520clear%2520role%2520for%2520the%2520future%2520due%2520to%2520its%2520potential%2520to%2520reduce%2520infection%2520risks%2520to%250Astaff%2520and%2520improve%2520clinical%2520efficiency.%2520Nasopharyngeal%2520%2528NP%2529%2520swab%2520sample%250Acollection%2520for%2520diagnosing%2520upper-respiratory%2520illnesses%2520is%2520one%2520type%2520of%2520close%250Acontact%2520task%2520that%2520is%2520interesting%2520for%2520robotics%2520due%2520to%2520the%2520dexterity%2520requirements%250Aand%2520the%2520unobservability%2520of%2520the%2520nasal%2520cavity.%2520We%2520propose%2520a%2520control%2520system%2520that%250Aperforms%2520the%2520test%2520using%2520a%2520collaborative%2520manipulator%2520arm%2520with%2520an%2520instrumented%250Aend-effector%2520to%2520take%2520visual%2520and%2520force%2520measurements%252C%2520under%2520the%2520scenario%2520that%2520the%250Apatient%2520is%2520unrestrained%2520and%2520the%2520tools%2520are%2520general%2520enough%2520to%2520be%2520applied%2520to%2520other%250Aclose%2520contact%2520tasks.%2520The%2520system%2520employs%2520a%2520visual%2520servo%2520controller%2520to%2520align%2520the%250Aswab%2520with%2520the%2520nostrils.%2520A%2520compliant%2520joint%2520velocity%2520controller%2520inserts%2520the%2520swab%250Aalong%2520a%2520trajectory%2520optimized%2520through%2520a%2520simulation%2520environment%252C%2520that%2520also%2520reacts%250Ato%2520measured%2520forces%2520applied%2520to%2520the%2520swab.%2520Additional%2520subsystems%2520include%2520a%2520fuzzy%250Alogic%2520system%2520for%2520detecting%2520when%2520the%2520swab%2520reaches%2520the%2520nasopharynx%2520and%2520a%2520method%250Afor%2520detaching%2520the%2520swab%2520and%2520aborting%2520the%2520procedure%2520if%2520safety%2520criteria%2520is%250Aviolated.%2520The%2520system%2520is%2520evaluated%2520using%2520a%2520second%2520robotic%2520arm%2520that%2520holds%2520a%2520nasal%250Acavity%2520phantom%2520and%2520simulates%2520the%2520natural%2520head%2520motions%2520that%2520could%2520occur%2520during%250Athe%2520procedure.%2520Through%2520extensive%2520experiments%252C%2520we%2520identify%2520controller%250Aconfigurations%2520capable%2520of%2520effectively%2520performing%2520the%2520NP%2520swab%2520test%2520even%2520with%250Asignificant%2520head%2520motion%252C%2520which%2520demonstrates%2520the%2520safety%2520and%2520reliability%2520of%2520the%250Asystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complete%20Autonomous%20Robotic%20Nasopharyngeal%20Swab%20System%20with%20Evaluation%0A%20%20on%20a%20Stochastically%20Moving%20Phantom%20Head&entry.906535625=Peter%20Q.%20Lee%20and%20John%20S.%20Zelek%20and%20Katja%20Mombaur&entry.1292438233=%20%20The%20application%20of%20autonomous%20robotics%20to%20close-contact%20healthcare%20tasks%20has%0Aa%20clear%20role%20for%20the%20future%20due%20to%20its%20potential%20to%20reduce%20infection%20risks%20to%0Astaff%20and%20improve%20clinical%20efficiency.%20Nasopharyngeal%20%28NP%29%20swab%20sample%0Acollection%20for%20diagnosing%20upper-respiratory%20illnesses%20is%20one%20type%20of%20close%0Acontact%20task%20that%20is%20interesting%20for%20robotics%20due%20to%20the%20dexterity%20requirements%0Aand%20the%20unobservability%20of%20the%20nasal%20cavity.%20We%20propose%20a%20control%20system%20that%0Aperforms%20the%20test%20using%20a%20collaborative%20manipulator%20arm%20with%20an%20instrumented%0Aend-effector%20to%20take%20visual%20and%20force%20measurements%2C%20under%20the%20scenario%20that%20the%0Apatient%20is%20unrestrained%20and%20the%20tools%20are%20general%20enough%20to%20be%20applied%20to%20other%0Aclose%20contact%20tasks.%20The%20system%20employs%20a%20visual%20servo%20controller%20to%20align%20the%0Aswab%20with%20the%20nostrils.%20A%20compliant%20joint%20velocity%20controller%20inserts%20the%20swab%0Aalong%20a%20trajectory%20optimized%20through%20a%20simulation%20environment%2C%20that%20also%20reacts%0Ato%20measured%20forces%20applied%20to%20the%20swab.%20Additional%20subsystems%20include%20a%20fuzzy%0Alogic%20system%20for%20detecting%20when%20the%20swab%20reaches%20the%20nasopharynx%20and%20a%20method%0Afor%20detaching%20the%20swab%20and%20aborting%20the%20procedure%20if%20safety%20criteria%20is%0Aviolated.%20The%20system%20is%20evaluated%20using%20a%20second%20robotic%20arm%20that%20holds%20a%20nasal%0Acavity%20phantom%20and%20simulates%20the%20natural%20head%20motions%20that%20could%20occur%20during%0Athe%20procedure.%20Through%20extensive%20experiments%2C%20we%20identify%20controller%0Aconfigurations%20capable%20of%20effectively%20performing%20the%20NP%20swab%20test%20even%20with%0Asignificant%20head%20motion%2C%20which%20demonstrates%20the%20safety%20and%20reliability%20of%20the%0Asystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13100v1&entry.124074799=Read"},
{"title": "SUMO: Search-Based Uncertainty Estimation for Model-Based Offline\n  Reinforcement Learning", "author": "Zhongjian Qiao and Jiafei Lyu and Kechen Jiao and Qi Liu and Xiu Li", "abstract": "  The performance of offline reinforcement learning (RL) suffers from the\nlimited size and quality of static datasets. Model-based offline RL addresses\nthis issue by generating synthetic samples through a dynamics model to enhance\noverall performance. To evaluate the reliability of the generated samples,\nuncertainty estimation methods are often employed. However, model ensemble, the\nmost commonly used uncertainty estimation method, is not always the best\nchoice. In this paper, we propose a \\textbf{S}earch-based \\textbf{U}ncertainty\nestimation method for \\textbf{M}odel-based \\textbf{O}ffline RL (SUMO) as an\nalternative. SUMO characterizes the uncertainty of synthetic samples by\nmeasuring their cross entropy against the in-distribution dataset samples, and\nuses an efficient search-based method for implementation. In this way, SUMO can\nachieve trustworthy uncertainty estimation. We integrate SUMO into several\nmodel-based offline RL algorithms including MOPO and Adapted MOReL (AMOReL),\nand provide theoretical analysis for them. Extensive experimental results on\nD4RL datasets demonstrate that SUMO can provide more accurate uncertainty\nestimation and boost the performance of base algorithms. These indicate that\nSUMO could be a better uncertainty estimator for model-based offline RL when\nused in either reward penalty or trajectory truncation. Our code is available\nand will be open-source for further research and development.\n", "link": "http://arxiv.org/abs/2408.12970v1", "date": "2024-08-23", "relevancy": 1.6153, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5743}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5319}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUMO%3A%20Search-Based%20Uncertainty%20Estimation%20for%20Model-Based%20Offline%0A%20%20Reinforcement%20Learning&body=Title%3A%20SUMO%3A%20Search-Based%20Uncertainty%20Estimation%20for%20Model-Based%20Offline%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zhongjian%20Qiao%20and%20Jiafei%20Lyu%20and%20Kechen%20Jiao%20and%20Qi%20Liu%20and%20Xiu%20Li%0AAbstract%3A%20%20%20The%20performance%20of%20offline%20reinforcement%20learning%20%28RL%29%20suffers%20from%20the%0Alimited%20size%20and%20quality%20of%20static%20datasets.%20Model-based%20offline%20RL%20addresses%0Athis%20issue%20by%20generating%20synthetic%20samples%20through%20a%20dynamics%20model%20to%20enhance%0Aoverall%20performance.%20To%20evaluate%20the%20reliability%20of%20the%20generated%20samples%2C%0Auncertainty%20estimation%20methods%20are%20often%20employed.%20However%2C%20model%20ensemble%2C%20the%0Amost%20commonly%20used%20uncertainty%20estimation%20method%2C%20is%20not%20always%20the%20best%0Achoice.%20In%20this%20paper%2C%20we%20propose%20a%20%5Ctextbf%7BS%7Dearch-based%20%5Ctextbf%7BU%7Dncertainty%0Aestimation%20method%20for%20%5Ctextbf%7BM%7Dodel-based%20%5Ctextbf%7BO%7Dffline%20RL%20%28SUMO%29%20as%20an%0Aalternative.%20SUMO%20characterizes%20the%20uncertainty%20of%20synthetic%20samples%20by%0Ameasuring%20their%20cross%20entropy%20against%20the%20in-distribution%20dataset%20samples%2C%20and%0Auses%20an%20efficient%20search-based%20method%20for%20implementation.%20In%20this%20way%2C%20SUMO%20can%0Aachieve%20trustworthy%20uncertainty%20estimation.%20We%20integrate%20SUMO%20into%20several%0Amodel-based%20offline%20RL%20algorithms%20including%20MOPO%20and%20Adapted%20MOReL%20%28AMOReL%29%2C%0Aand%20provide%20theoretical%20analysis%20for%20them.%20Extensive%20experimental%20results%20on%0AD4RL%20datasets%20demonstrate%20that%20SUMO%20can%20provide%20more%20accurate%20uncertainty%0Aestimation%20and%20boost%20the%20performance%20of%20base%20algorithms.%20These%20indicate%20that%0ASUMO%20could%20be%20a%20better%20uncertainty%20estimator%20for%20model-based%20offline%20RL%20when%0Aused%20in%20either%20reward%20penalty%20or%20trajectory%20truncation.%20Our%20code%20is%20available%0Aand%20will%20be%20open-source%20for%20further%20research%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUMO%253A%2520Search-Based%2520Uncertainty%2520Estimation%2520for%2520Model-Based%2520Offline%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZhongjian%2520Qiao%2520and%2520Jiafei%2520Lyu%2520and%2520Kechen%2520Jiao%2520and%2520Qi%2520Liu%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520offline%2520reinforcement%2520learning%2520%2528RL%2529%2520suffers%2520from%2520the%250Alimited%2520size%2520and%2520quality%2520of%2520static%2520datasets.%2520Model-based%2520offline%2520RL%2520addresses%250Athis%2520issue%2520by%2520generating%2520synthetic%2520samples%2520through%2520a%2520dynamics%2520model%2520to%2520enhance%250Aoverall%2520performance.%2520To%2520evaluate%2520the%2520reliability%2520of%2520the%2520generated%2520samples%252C%250Auncertainty%2520estimation%2520methods%2520are%2520often%2520employed.%2520However%252C%2520model%2520ensemble%252C%2520the%250Amost%2520commonly%2520used%2520uncertainty%2520estimation%2520method%252C%2520is%2520not%2520always%2520the%2520best%250Achoice.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520%255Ctextbf%257BS%257Dearch-based%2520%255Ctextbf%257BU%257Dncertainty%250Aestimation%2520method%2520for%2520%255Ctextbf%257BM%257Dodel-based%2520%255Ctextbf%257BO%257Dffline%2520RL%2520%2528SUMO%2529%2520as%2520an%250Aalternative.%2520SUMO%2520characterizes%2520the%2520uncertainty%2520of%2520synthetic%2520samples%2520by%250Ameasuring%2520their%2520cross%2520entropy%2520against%2520the%2520in-distribution%2520dataset%2520samples%252C%2520and%250Auses%2520an%2520efficient%2520search-based%2520method%2520for%2520implementation.%2520In%2520this%2520way%252C%2520SUMO%2520can%250Aachieve%2520trustworthy%2520uncertainty%2520estimation.%2520We%2520integrate%2520SUMO%2520into%2520several%250Amodel-based%2520offline%2520RL%2520algorithms%2520including%2520MOPO%2520and%2520Adapted%2520MOReL%2520%2528AMOReL%2529%252C%250Aand%2520provide%2520theoretical%2520analysis%2520for%2520them.%2520Extensive%2520experimental%2520results%2520on%250AD4RL%2520datasets%2520demonstrate%2520that%2520SUMO%2520can%2520provide%2520more%2520accurate%2520uncertainty%250Aestimation%2520and%2520boost%2520the%2520performance%2520of%2520base%2520algorithms.%2520These%2520indicate%2520that%250ASUMO%2520could%2520be%2520a%2520better%2520uncertainty%2520estimator%2520for%2520model-based%2520offline%2520RL%2520when%250Aused%2520in%2520either%2520reward%2520penalty%2520or%2520trajectory%2520truncation.%2520Our%2520code%2520is%2520available%250Aand%2520will%2520be%2520open-source%2520for%2520further%2520research%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUMO%3A%20Search-Based%20Uncertainty%20Estimation%20for%20Model-Based%20Offline%0A%20%20Reinforcement%20Learning&entry.906535625=Zhongjian%20Qiao%20and%20Jiafei%20Lyu%20and%20Kechen%20Jiao%20and%20Qi%20Liu%20and%20Xiu%20Li&entry.1292438233=%20%20The%20performance%20of%20offline%20reinforcement%20learning%20%28RL%29%20suffers%20from%20the%0Alimited%20size%20and%20quality%20of%20static%20datasets.%20Model-based%20offline%20RL%20addresses%0Athis%20issue%20by%20generating%20synthetic%20samples%20through%20a%20dynamics%20model%20to%20enhance%0Aoverall%20performance.%20To%20evaluate%20the%20reliability%20of%20the%20generated%20samples%2C%0Auncertainty%20estimation%20methods%20are%20often%20employed.%20However%2C%20model%20ensemble%2C%20the%0Amost%20commonly%20used%20uncertainty%20estimation%20method%2C%20is%20not%20always%20the%20best%0Achoice.%20In%20this%20paper%2C%20we%20propose%20a%20%5Ctextbf%7BS%7Dearch-based%20%5Ctextbf%7BU%7Dncertainty%0Aestimation%20method%20for%20%5Ctextbf%7BM%7Dodel-based%20%5Ctextbf%7BO%7Dffline%20RL%20%28SUMO%29%20as%20an%0Aalternative.%20SUMO%20characterizes%20the%20uncertainty%20of%20synthetic%20samples%20by%0Ameasuring%20their%20cross%20entropy%20against%20the%20in-distribution%20dataset%20samples%2C%20and%0Auses%20an%20efficient%20search-based%20method%20for%20implementation.%20In%20this%20way%2C%20SUMO%20can%0Aachieve%20trustworthy%20uncertainty%20estimation.%20We%20integrate%20SUMO%20into%20several%0Amodel-based%20offline%20RL%20algorithms%20including%20MOPO%20and%20Adapted%20MOReL%20%28AMOReL%29%2C%0Aand%20provide%20theoretical%20analysis%20for%20them.%20Extensive%20experimental%20results%20on%0AD4RL%20datasets%20demonstrate%20that%20SUMO%20can%20provide%20more%20accurate%20uncertainty%0Aestimation%20and%20boost%20the%20performance%20of%20base%20algorithms.%20These%20indicate%20that%0ASUMO%20could%20be%20a%20better%20uncertainty%20estimator%20for%20model-based%20offline%20RL%20when%0Aused%20in%20either%20reward%20penalty%20or%20trajectory%20truncation.%20Our%20code%20is%20available%0Aand%20will%20be%20open-source%20for%20further%20research%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12970v1&entry.124074799=Read"},
{"title": "Solving Robotics Problems in Zero-Shot with Vision-Language Models", "author": "Zidan Wang and Rui Shen and Bradly Stadie", "abstract": "  We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for\nsolving robotics problems in the zero-shot regime. By zero-shot we mean that,\nfor a novel environment, we feed a VLLM an image of the robot's environment and\na description of the task, and have the VLLM output the sequence of actions\nnecessary for the robot to complete the task. Prior work on VLLMs in robotics\nhas largely focused on settings where some part of the pipeline is fine-tuned,\nsuch as tuning an LLM on robot data or training a separate vision encoder for\nperception and action generation. Surprisingly, due to recent advances in the\ncapabilities of VLLMs, this type of fine-tuning may no longer be necessary for\nmany tasks. In this work, we show that with careful engineering, we can prompt\na single off-the-shelf VLLM to handle all aspects of a robotics task, from\nhigh-level planning to low-level location-extraction and action-execution.\nWonderful Team builds on recent advances in multi-agent LLMs to partition tasks\nacross an agent hierarchy, making it self-corrective and able to effectively\npartition and solve even long-horizon tasks. Extensive experiments on VIMABench\nand real-world robotic environments demonstrate the system's capability to\nhandle a variety of robotic tasks, including manipulation, visual\ngoal-reaching, and visual reasoning, all in a zero-shot manner. These results\nunderscore a key point: vision-language models have progressed rapidly in the\npast year, and should strongly be considered as a backbone for robotics\nproblems going forward.\n", "link": "http://arxiv.org/abs/2407.19094v2", "date": "2024-08-23", "relevancy": 1.6156, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5439}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Robotics%20Problems%20in%20Zero-Shot%20with%20Vision-Language%20Models&body=Title%3A%20Solving%20Robotics%20Problems%20in%20Zero-Shot%20with%20Vision-Language%20Models%0AAuthor%3A%20Zidan%20Wang%20and%20Rui%20Shen%20and%20Bradly%20Stadie%0AAbstract%3A%20%20%20We%20introduce%20Wonderful%20Team%2C%20a%20multi-agent%20visual%20LLM%20%28VLLM%29%20framework%20for%0Asolving%20robotics%20problems%20in%20the%20zero-shot%20regime.%20By%20zero-shot%20we%20mean%20that%2C%0Afor%20a%20novel%20environment%2C%20we%20feed%20a%20VLLM%20an%20image%20of%20the%20robot%27s%20environment%20and%0Aa%20description%20of%20the%20task%2C%20and%20have%20the%20VLLM%20output%20the%20sequence%20of%20actions%0Anecessary%20for%20the%20robot%20to%20complete%20the%20task.%20Prior%20work%20on%20VLLMs%20in%20robotics%0Ahas%20largely%20focused%20on%20settings%20where%20some%20part%20of%20the%20pipeline%20is%20fine-tuned%2C%0Asuch%20as%20tuning%20an%20LLM%20on%20robot%20data%20or%20training%20a%20separate%20vision%20encoder%20for%0Aperception%20and%20action%20generation.%20Surprisingly%2C%20due%20to%20recent%20advances%20in%20the%0Acapabilities%20of%20VLLMs%2C%20this%20type%20of%20fine-tuning%20may%20no%20longer%20be%20necessary%20for%0Amany%20tasks.%20In%20this%20work%2C%20we%20show%20that%20with%20careful%20engineering%2C%20we%20can%20prompt%0Aa%20single%20off-the-shelf%20VLLM%20to%20handle%20all%20aspects%20of%20a%20robotics%20task%2C%20from%0Ahigh-level%20planning%20to%20low-level%20location-extraction%20and%20action-execution.%0AWonderful%20Team%20builds%20on%20recent%20advances%20in%20multi-agent%20LLMs%20to%20partition%20tasks%0Aacross%20an%20agent%20hierarchy%2C%20making%20it%20self-corrective%20and%20able%20to%20effectively%0Apartition%20and%20solve%20even%20long-horizon%20tasks.%20Extensive%20experiments%20on%20VIMABench%0Aand%20real-world%20robotic%20environments%20demonstrate%20the%20system%27s%20capability%20to%0Ahandle%20a%20variety%20of%20robotic%20tasks%2C%20including%20manipulation%2C%20visual%0Agoal-reaching%2C%20and%20visual%20reasoning%2C%20all%20in%20a%20zero-shot%20manner.%20These%20results%0Aunderscore%20a%20key%20point%3A%20vision-language%20models%20have%20progressed%20rapidly%20in%20the%0Apast%20year%2C%20and%20should%20strongly%20be%20considered%20as%20a%20backbone%20for%20robotics%0Aproblems%20going%20forward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Robotics%2520Problems%2520in%2520Zero-Shot%2520with%2520Vision-Language%2520Models%26entry.906535625%3DZidan%2520Wang%2520and%2520Rui%2520Shen%2520and%2520Bradly%2520Stadie%26entry.1292438233%3D%2520%2520We%2520introduce%2520Wonderful%2520Team%252C%2520a%2520multi-agent%2520visual%2520LLM%2520%2528VLLM%2529%2520framework%2520for%250Asolving%2520robotics%2520problems%2520in%2520the%2520zero-shot%2520regime.%2520By%2520zero-shot%2520we%2520mean%2520that%252C%250Afor%2520a%2520novel%2520environment%252C%2520we%2520feed%2520a%2520VLLM%2520an%2520image%2520of%2520the%2520robot%2527s%2520environment%2520and%250Aa%2520description%2520of%2520the%2520task%252C%2520and%2520have%2520the%2520VLLM%2520output%2520the%2520sequence%2520of%2520actions%250Anecessary%2520for%2520the%2520robot%2520to%2520complete%2520the%2520task.%2520Prior%2520work%2520on%2520VLLMs%2520in%2520robotics%250Ahas%2520largely%2520focused%2520on%2520settings%2520where%2520some%2520part%2520of%2520the%2520pipeline%2520is%2520fine-tuned%252C%250Asuch%2520as%2520tuning%2520an%2520LLM%2520on%2520robot%2520data%2520or%2520training%2520a%2520separate%2520vision%2520encoder%2520for%250Aperception%2520and%2520action%2520generation.%2520Surprisingly%252C%2520due%2520to%2520recent%2520advances%2520in%2520the%250Acapabilities%2520of%2520VLLMs%252C%2520this%2520type%2520of%2520fine-tuning%2520may%2520no%2520longer%2520be%2520necessary%2520for%250Amany%2520tasks.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520with%2520careful%2520engineering%252C%2520we%2520can%2520prompt%250Aa%2520single%2520off-the-shelf%2520VLLM%2520to%2520handle%2520all%2520aspects%2520of%2520a%2520robotics%2520task%252C%2520from%250Ahigh-level%2520planning%2520to%2520low-level%2520location-extraction%2520and%2520action-execution.%250AWonderful%2520Team%2520builds%2520on%2520recent%2520advances%2520in%2520multi-agent%2520LLMs%2520to%2520partition%2520tasks%250Aacross%2520an%2520agent%2520hierarchy%252C%2520making%2520it%2520self-corrective%2520and%2520able%2520to%2520effectively%250Apartition%2520and%2520solve%2520even%2520long-horizon%2520tasks.%2520Extensive%2520experiments%2520on%2520VIMABench%250Aand%2520real-world%2520robotic%2520environments%2520demonstrate%2520the%2520system%2527s%2520capability%2520to%250Ahandle%2520a%2520variety%2520of%2520robotic%2520tasks%252C%2520including%2520manipulation%252C%2520visual%250Agoal-reaching%252C%2520and%2520visual%2520reasoning%252C%2520all%2520in%2520a%2520zero-shot%2520manner.%2520These%2520results%250Aunderscore%2520a%2520key%2520point%253A%2520vision-language%2520models%2520have%2520progressed%2520rapidly%2520in%2520the%250Apast%2520year%252C%2520and%2520should%2520strongly%2520be%2520considered%2520as%2520a%2520backbone%2520for%2520robotics%250Aproblems%2520going%2520forward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Robotics%20Problems%20in%20Zero-Shot%20with%20Vision-Language%20Models&entry.906535625=Zidan%20Wang%20and%20Rui%20Shen%20and%20Bradly%20Stadie&entry.1292438233=%20%20We%20introduce%20Wonderful%20Team%2C%20a%20multi-agent%20visual%20LLM%20%28VLLM%29%20framework%20for%0Asolving%20robotics%20problems%20in%20the%20zero-shot%20regime.%20By%20zero-shot%20we%20mean%20that%2C%0Afor%20a%20novel%20environment%2C%20we%20feed%20a%20VLLM%20an%20image%20of%20the%20robot%27s%20environment%20and%0Aa%20description%20of%20the%20task%2C%20and%20have%20the%20VLLM%20output%20the%20sequence%20of%20actions%0Anecessary%20for%20the%20robot%20to%20complete%20the%20task.%20Prior%20work%20on%20VLLMs%20in%20robotics%0Ahas%20largely%20focused%20on%20settings%20where%20some%20part%20of%20the%20pipeline%20is%20fine-tuned%2C%0Asuch%20as%20tuning%20an%20LLM%20on%20robot%20data%20or%20training%20a%20separate%20vision%20encoder%20for%0Aperception%20and%20action%20generation.%20Surprisingly%2C%20due%20to%20recent%20advances%20in%20the%0Acapabilities%20of%20VLLMs%2C%20this%20type%20of%20fine-tuning%20may%20no%20longer%20be%20necessary%20for%0Amany%20tasks.%20In%20this%20work%2C%20we%20show%20that%20with%20careful%20engineering%2C%20we%20can%20prompt%0Aa%20single%20off-the-shelf%20VLLM%20to%20handle%20all%20aspects%20of%20a%20robotics%20task%2C%20from%0Ahigh-level%20planning%20to%20low-level%20location-extraction%20and%20action-execution.%0AWonderful%20Team%20builds%20on%20recent%20advances%20in%20multi-agent%20LLMs%20to%20partition%20tasks%0Aacross%20an%20agent%20hierarchy%2C%20making%20it%20self-corrective%20and%20able%20to%20effectively%0Apartition%20and%20solve%20even%20long-horizon%20tasks.%20Extensive%20experiments%20on%20VIMABench%0Aand%20real-world%20robotic%20environments%20demonstrate%20the%20system%27s%20capability%20to%0Ahandle%20a%20variety%20of%20robotic%20tasks%2C%20including%20manipulation%2C%20visual%0Agoal-reaching%2C%20and%20visual%20reasoning%2C%20all%20in%20a%20zero-shot%20manner.%20These%20results%0Aunderscore%20a%20key%20point%3A%20vision-language%20models%20have%20progressed%20rapidly%20in%20the%0Apast%20year%2C%20and%20should%20strongly%20be%20considered%20as%20a%20backbone%20for%20robotics%0Aproblems%20going%20forward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19094v2&entry.124074799=Read"},
{"title": "Foundational Model for Electron Micrograph Analysis: Instruction-Tuning\n  Small-Scale Language-and-Vision Assistant for Enterprise Adoption", "author": "Sakhinana Sagar Srinivas and Chidaksh Ravuru and Geethan Sannidhi and Venkataramana Runkana", "abstract": "  Semiconductor imaging and analysis are critical yet understudied in deep\nlearning, limiting our ability for precise control and optimization in\nsemiconductor manufacturing. We introduce a small-scale multimodal framework\nfor analyzing semiconductor electron microscopy images (MAEMI) through\nvision-language instruction tuning. We generate a customized\ninstruction-following dataset using large multimodal models on microscopic\nimage analysis. We perform knowledge transfer from larger to smaller models\nthrough knowledge distillation, resulting in improved accuracy of smaller\nmodels on visual question answering (VQA) tasks. This approach eliminates the\nneed for expensive, human expert-annotated datasets for microscopic image\nanalysis tasks. Enterprises can further finetune MAEMI on their intellectual\ndata, enhancing privacy and performance on low-cost consumer hardware. Our\nexperiments show that MAEMI outperforms traditional methods, adapts to data\ndistribution shifts, and supports high-throughput screening.\n", "link": "http://arxiv.org/abs/2408.13248v1", "date": "2024-08-23", "relevancy": 1.6429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5433}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundational%20Model%20for%20Electron%20Micrograph%20Analysis%3A%20Instruction-Tuning%0A%20%20Small-Scale%20Language-and-Vision%20Assistant%20for%20Enterprise%20Adoption&body=Title%3A%20Foundational%20Model%20for%20Electron%20Micrograph%20Analysis%3A%20Instruction-Tuning%0A%20%20Small-Scale%20Language-and-Vision%20Assistant%20for%20Enterprise%20Adoption%0AAuthor%3A%20Sakhinana%20Sagar%20Srinivas%20and%20Chidaksh%20Ravuru%20and%20Geethan%20Sannidhi%20and%20Venkataramana%20Runkana%0AAbstract%3A%20%20%20Semiconductor%20imaging%20and%20analysis%20are%20critical%20yet%20understudied%20in%20deep%0Alearning%2C%20limiting%20our%20ability%20for%20precise%20control%20and%20optimization%20in%0Asemiconductor%20manufacturing.%20We%20introduce%20a%20small-scale%20multimodal%20framework%0Afor%20analyzing%20semiconductor%20electron%20microscopy%20images%20%28MAEMI%29%20through%0Avision-language%20instruction%20tuning.%20We%20generate%20a%20customized%0Ainstruction-following%20dataset%20using%20large%20multimodal%20models%20on%20microscopic%0Aimage%20analysis.%20We%20perform%20knowledge%20transfer%20from%20larger%20to%20smaller%20models%0Athrough%20knowledge%20distillation%2C%20resulting%20in%20improved%20accuracy%20of%20smaller%0Amodels%20on%20visual%20question%20answering%20%28VQA%29%20tasks.%20This%20approach%20eliminates%20the%0Aneed%20for%20expensive%2C%20human%20expert-annotated%20datasets%20for%20microscopic%20image%0Aanalysis%20tasks.%20Enterprises%20can%20further%20finetune%20MAEMI%20on%20their%20intellectual%0Adata%2C%20enhancing%20privacy%20and%20performance%20on%20low-cost%20consumer%20hardware.%20Our%0Aexperiments%20show%20that%20MAEMI%20outperforms%20traditional%20methods%2C%20adapts%20to%20data%0Adistribution%20shifts%2C%20and%20supports%20high-throughput%20screening.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundational%2520Model%2520for%2520Electron%2520Micrograph%2520Analysis%253A%2520Instruction-Tuning%250A%2520%2520Small-Scale%2520Language-and-Vision%2520Assistant%2520for%2520Enterprise%2520Adoption%26entry.906535625%3DSakhinana%2520Sagar%2520Srinivas%2520and%2520Chidaksh%2520Ravuru%2520and%2520Geethan%2520Sannidhi%2520and%2520Venkataramana%2520Runkana%26entry.1292438233%3D%2520%2520Semiconductor%2520imaging%2520and%2520analysis%2520are%2520critical%2520yet%2520understudied%2520in%2520deep%250Alearning%252C%2520limiting%2520our%2520ability%2520for%2520precise%2520control%2520and%2520optimization%2520in%250Asemiconductor%2520manufacturing.%2520We%2520introduce%2520a%2520small-scale%2520multimodal%2520framework%250Afor%2520analyzing%2520semiconductor%2520electron%2520microscopy%2520images%2520%2528MAEMI%2529%2520through%250Avision-language%2520instruction%2520tuning.%2520We%2520generate%2520a%2520customized%250Ainstruction-following%2520dataset%2520using%2520large%2520multimodal%2520models%2520on%2520microscopic%250Aimage%2520analysis.%2520We%2520perform%2520knowledge%2520transfer%2520from%2520larger%2520to%2520smaller%2520models%250Athrough%2520knowledge%2520distillation%252C%2520resulting%2520in%2520improved%2520accuracy%2520of%2520smaller%250Amodels%2520on%2520visual%2520question%2520answering%2520%2528VQA%2529%2520tasks.%2520This%2520approach%2520eliminates%2520the%250Aneed%2520for%2520expensive%252C%2520human%2520expert-annotated%2520datasets%2520for%2520microscopic%2520image%250Aanalysis%2520tasks.%2520Enterprises%2520can%2520further%2520finetune%2520MAEMI%2520on%2520their%2520intellectual%250Adata%252C%2520enhancing%2520privacy%2520and%2520performance%2520on%2520low-cost%2520consumer%2520hardware.%2520Our%250Aexperiments%2520show%2520that%2520MAEMI%2520outperforms%2520traditional%2520methods%252C%2520adapts%2520to%2520data%250Adistribution%2520shifts%252C%2520and%2520supports%2520high-throughput%2520screening.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundational%20Model%20for%20Electron%20Micrograph%20Analysis%3A%20Instruction-Tuning%0A%20%20Small-Scale%20Language-and-Vision%20Assistant%20for%20Enterprise%20Adoption&entry.906535625=Sakhinana%20Sagar%20Srinivas%20and%20Chidaksh%20Ravuru%20and%20Geethan%20Sannidhi%20and%20Venkataramana%20Runkana&entry.1292438233=%20%20Semiconductor%20imaging%20and%20analysis%20are%20critical%20yet%20understudied%20in%20deep%0Alearning%2C%20limiting%20our%20ability%20for%20precise%20control%20and%20optimization%20in%0Asemiconductor%20manufacturing.%20We%20introduce%20a%20small-scale%20multimodal%20framework%0Afor%20analyzing%20semiconductor%20electron%20microscopy%20images%20%28MAEMI%29%20through%0Avision-language%20instruction%20tuning.%20We%20generate%20a%20customized%0Ainstruction-following%20dataset%20using%20large%20multimodal%20models%20on%20microscopic%0Aimage%20analysis.%20We%20perform%20knowledge%20transfer%20from%20larger%20to%20smaller%20models%0Athrough%20knowledge%20distillation%2C%20resulting%20in%20improved%20accuracy%20of%20smaller%0Amodels%20on%20visual%20question%20answering%20%28VQA%29%20tasks.%20This%20approach%20eliminates%20the%0Aneed%20for%20expensive%2C%20human%20expert-annotated%20datasets%20for%20microscopic%20image%0Aanalysis%20tasks.%20Enterprises%20can%20further%20finetune%20MAEMI%20on%20their%20intellectual%0Adata%2C%20enhancing%20privacy%20and%20performance%20on%20low-cost%20consumer%20hardware.%20Our%0Aexperiments%20show%20that%20MAEMI%20outperforms%20traditional%20methods%2C%20adapts%20to%20data%0Adistribution%20shifts%2C%20and%20supports%20high-throughput%20screening.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13248v1&entry.124074799=Read"},
{"title": "XEQ Scale for Evaluating XAI Experience Quality Grounded in Psychometric\n  Theory", "author": "Anjana Wijekoon and Nirmalie Wiratunga and David Corsar and Kyle Martin and Ikechukwu Nkisi-Orji and Belen D\u00edaz-Agudo and Derek Bridge", "abstract": "  Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users' need for holistic \"multi-shot\" explanations and the ability\nto personalise their engagement with XAI systems. We refer to this user-centred\ninteraction as an XAI Experience. Despite advances in creating XAI experiences,\nevaluating them in a user-centred manner has remained challenging. To address\nthis, we introduce the XAI Experience Quality (XEQ) Scale (pronounced \"Seek\"\nScale), for evaluating the user-centred quality of XAI experiences.\nFurthermore, XEQ quantifies the quality of experiences across four evaluation\ndimensions: learning, utility, fulfilment and engagement. These contributions\nextend the state-of-the-art of XAI evaluation, moving beyond the\none-dimensional metrics frequently developed to assess single-shot\nexplanations. In this paper, we present the XEQ scale development and\nvalidation process, including content validation with XAI experts as well as\ndiscriminant and construct validation through a large-scale pilot study. Out\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.\n", "link": "http://arxiv.org/abs/2407.10662v3", "date": "2024-08-23", "relevancy": 1.2728, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4638}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4364}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XEQ%20Scale%20for%20Evaluating%20XAI%20Experience%20Quality%20Grounded%20in%20Psychometric%0A%20%20Theory&body=Title%3A%20XEQ%20Scale%20for%20Evaluating%20XAI%20Experience%20Quality%20Grounded%20in%20Psychometric%0A%20%20Theory%0AAuthor%3A%20Anjana%20Wijekoon%20and%20Nirmalie%20Wiratunga%20and%20David%20Corsar%20and%20Kyle%20Martin%20and%20Ikechukwu%20Nkisi-Orji%20and%20Belen%20D%C3%ADaz-Agudo%20and%20Derek%20Bridge%0AAbstract%3A%20%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20aims%20to%20improve%20the%20transparency%20of%0Aautonomous%20decision-making%20through%20explanations.%20Recent%20literature%20has%0Aemphasised%20users%27%20need%20for%20holistic%20%22multi-shot%22%20explanations%20and%20the%20ability%0Ato%20personalise%20their%20engagement%20with%20XAI%20systems.%20We%20refer%20to%20this%20user-centred%0Ainteraction%20as%20an%20XAI%20Experience.%20Despite%20advances%20in%20creating%20XAI%20experiences%2C%0Aevaluating%20them%20in%20a%20user-centred%20manner%20has%20remained%20challenging.%20To%20address%0Athis%2C%20we%20introduce%20the%20XAI%20Experience%20Quality%20%28XEQ%29%20Scale%20%28pronounced%20%22Seek%22%0AScale%29%2C%20for%20evaluating%20the%20user-centred%20quality%20of%20XAI%20experiences.%0AFurthermore%2C%20XEQ%20quantifies%20the%20quality%20of%20experiences%20across%20four%20evaluation%0Adimensions%3A%20learning%2C%20utility%2C%20fulfilment%20and%20engagement.%20These%20contributions%0Aextend%20the%20state-of-the-art%20of%20XAI%20evaluation%2C%20moving%20beyond%20the%0Aone-dimensional%20metrics%20frequently%20developed%20to%20assess%20single-shot%0Aexplanations.%20In%20this%20paper%2C%20we%20present%20the%20XEQ%20scale%20development%20and%0Avalidation%20process%2C%20including%20content%20validation%20with%20XAI%20experts%20as%20well%20as%0Adiscriminant%20and%20construct%20validation%20through%20a%20large-scale%20pilot%20study.%20Out%0Apilot%20study%20results%20offer%20strong%20evidence%20that%20establishes%20the%20XEQ%20Scale%20as%20a%0Acomprehensive%20framework%20for%20evaluating%20user-centred%20XAI%20experiences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10662v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXEQ%2520Scale%2520for%2520Evaluating%2520XAI%2520Experience%2520Quality%2520Grounded%2520in%2520Psychometric%250A%2520%2520Theory%26entry.906535625%3DAnjana%2520Wijekoon%2520and%2520Nirmalie%2520Wiratunga%2520and%2520David%2520Corsar%2520and%2520Kyle%2520Martin%2520and%2520Ikechukwu%2520Nkisi-Orji%2520and%2520Belen%2520D%25C3%25ADaz-Agudo%2520and%2520Derek%2520Bridge%26entry.1292438233%3D%2520%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520aims%2520to%2520improve%2520the%2520transparency%2520of%250Aautonomous%2520decision-making%2520through%2520explanations.%2520Recent%2520literature%2520has%250Aemphasised%2520users%2527%2520need%2520for%2520holistic%2520%2522multi-shot%2522%2520explanations%2520and%2520the%2520ability%250Ato%2520personalise%2520their%2520engagement%2520with%2520XAI%2520systems.%2520We%2520refer%2520to%2520this%2520user-centred%250Ainteraction%2520as%2520an%2520XAI%2520Experience.%2520Despite%2520advances%2520in%2520creating%2520XAI%2520experiences%252C%250Aevaluating%2520them%2520in%2520a%2520user-centred%2520manner%2520has%2520remained%2520challenging.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520the%2520XAI%2520Experience%2520Quality%2520%2528XEQ%2529%2520Scale%2520%2528pronounced%2520%2522Seek%2522%250AScale%2529%252C%2520for%2520evaluating%2520the%2520user-centred%2520quality%2520of%2520XAI%2520experiences.%250AFurthermore%252C%2520XEQ%2520quantifies%2520the%2520quality%2520of%2520experiences%2520across%2520four%2520evaluation%250Adimensions%253A%2520learning%252C%2520utility%252C%2520fulfilment%2520and%2520engagement.%2520These%2520contributions%250Aextend%2520the%2520state-of-the-art%2520of%2520XAI%2520evaluation%252C%2520moving%2520beyond%2520the%250Aone-dimensional%2520metrics%2520frequently%2520developed%2520to%2520assess%2520single-shot%250Aexplanations.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520XEQ%2520scale%2520development%2520and%250Avalidation%2520process%252C%2520including%2520content%2520validation%2520with%2520XAI%2520experts%2520as%2520well%2520as%250Adiscriminant%2520and%2520construct%2520validation%2520through%2520a%2520large-scale%2520pilot%2520study.%2520Out%250Apilot%2520study%2520results%2520offer%2520strong%2520evidence%2520that%2520establishes%2520the%2520XEQ%2520Scale%2520as%2520a%250Acomprehensive%2520framework%2520for%2520evaluating%2520user-centred%2520XAI%2520experiences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10662v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XEQ%20Scale%20for%20Evaluating%20XAI%20Experience%20Quality%20Grounded%20in%20Psychometric%0A%20%20Theory&entry.906535625=Anjana%20Wijekoon%20and%20Nirmalie%20Wiratunga%20and%20David%20Corsar%20and%20Kyle%20Martin%20and%20Ikechukwu%20Nkisi-Orji%20and%20Belen%20D%C3%ADaz-Agudo%20and%20Derek%20Bridge&entry.1292438233=%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20aims%20to%20improve%20the%20transparency%20of%0Aautonomous%20decision-making%20through%20explanations.%20Recent%20literature%20has%0Aemphasised%20users%27%20need%20for%20holistic%20%22multi-shot%22%20explanations%20and%20the%20ability%0Ato%20personalise%20their%20engagement%20with%20XAI%20systems.%20We%20refer%20to%20this%20user-centred%0Ainteraction%20as%20an%20XAI%20Experience.%20Despite%20advances%20in%20creating%20XAI%20experiences%2C%0Aevaluating%20them%20in%20a%20user-centred%20manner%20has%20remained%20challenging.%20To%20address%0Athis%2C%20we%20introduce%20the%20XAI%20Experience%20Quality%20%28XEQ%29%20Scale%20%28pronounced%20%22Seek%22%0AScale%29%2C%20for%20evaluating%20the%20user-centred%20quality%20of%20XAI%20experiences.%0AFurthermore%2C%20XEQ%20quantifies%20the%20quality%20of%20experiences%20across%20four%20evaluation%0Adimensions%3A%20learning%2C%20utility%2C%20fulfilment%20and%20engagement.%20These%20contributions%0Aextend%20the%20state-of-the-art%20of%20XAI%20evaluation%2C%20moving%20beyond%20the%0Aone-dimensional%20metrics%20frequently%20developed%20to%20assess%20single-shot%0Aexplanations.%20In%20this%20paper%2C%20we%20present%20the%20XEQ%20scale%20development%20and%0Avalidation%20process%2C%20including%20content%20validation%20with%20XAI%20experts%20as%20well%20as%0Adiscriminant%20and%20construct%20validation%20through%20a%20large-scale%20pilot%20study.%20Out%0Apilot%20study%20results%20offer%20strong%20evidence%20that%20establishes%20the%20XEQ%20Scale%20as%20a%0Acomprehensive%20framework%20for%20evaluating%20user-centred%20XAI%20experiences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10662v3&entry.124074799=Read"},
{"title": "A density ratio framework for evaluating the utility of synthetic data", "author": "Thom Benjamin Volker and Peter-Paul de Wolf and Erik-Jan van Kesteren", "abstract": "  Synthetic data generation is a promising technique to facilitate the use of\nsensitive data while mitigating the risk of privacy breaches. However, for\nsynthetic data to be useful in downstream analysis tasks, it needs to be of\nsufficient quality. Various methods have been proposed to measure the utility\nof synthetic data, but their results are often incomplete or even misleading.\nIn this paper, we propose using density ratio estimation to improve quality\nevaluation for synthetic data, and thereby the quality of synthesized datasets.\nWe show how this framework relates to and builds on existing measures, yielding\nglobal and local utility measures that are informative and easy to interpret.\nWe develop an estimator which requires little to no manual tuning due to\nautomatic selection of a nonparametric density ratio model. Through\nsimulations, we find that density ratio estimation yields more accurate\nestimates of global utility than established procedures. A real-world data\napplication demonstrates how the density ratio can guide refinements of\nsynthesis models and can be used to improve downstream analyses. We conclude\nthat density ratio estimation is a valuable tool in synthetic data generation\nworkflows and provide these methods in the accessible open source R-package\ndensityratio.\n", "link": "http://arxiv.org/abs/2408.13167v1", "date": "2024-08-23", "relevancy": 1.3434, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4792}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4391}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20density%20ratio%20framework%20for%20evaluating%20the%20utility%20of%20synthetic%20data&body=Title%3A%20A%20density%20ratio%20framework%20for%20evaluating%20the%20utility%20of%20synthetic%20data%0AAuthor%3A%20Thom%20Benjamin%20Volker%20and%20Peter-Paul%20de%20Wolf%20and%20Erik-Jan%20van%20Kesteren%0AAbstract%3A%20%20%20Synthetic%20data%20generation%20is%20a%20promising%20technique%20to%20facilitate%20the%20use%20of%0Asensitive%20data%20while%20mitigating%20the%20risk%20of%20privacy%20breaches.%20However%2C%20for%0Asynthetic%20data%20to%20be%20useful%20in%20downstream%20analysis%20tasks%2C%20it%20needs%20to%20be%20of%0Asufficient%20quality.%20Various%20methods%20have%20been%20proposed%20to%20measure%20the%20utility%0Aof%20synthetic%20data%2C%20but%20their%20results%20are%20often%20incomplete%20or%20even%20misleading.%0AIn%20this%20paper%2C%20we%20propose%20using%20density%20ratio%20estimation%20to%20improve%20quality%0Aevaluation%20for%20synthetic%20data%2C%20and%20thereby%20the%20quality%20of%20synthesized%20datasets.%0AWe%20show%20how%20this%20framework%20relates%20to%20and%20builds%20on%20existing%20measures%2C%20yielding%0Aglobal%20and%20local%20utility%20measures%20that%20are%20informative%20and%20easy%20to%20interpret.%0AWe%20develop%20an%20estimator%20which%20requires%20little%20to%20no%20manual%20tuning%20due%20to%0Aautomatic%20selection%20of%20a%20nonparametric%20density%20ratio%20model.%20Through%0Asimulations%2C%20we%20find%20that%20density%20ratio%20estimation%20yields%20more%20accurate%0Aestimates%20of%20global%20utility%20than%20established%20procedures.%20A%20real-world%20data%0Aapplication%20demonstrates%20how%20the%20density%20ratio%20can%20guide%20refinements%20of%0Asynthesis%20models%20and%20can%20be%20used%20to%20improve%20downstream%20analyses.%20We%20conclude%0Athat%20density%20ratio%20estimation%20is%20a%20valuable%20tool%20in%20synthetic%20data%20generation%0Aworkflows%20and%20provide%20these%20methods%20in%20the%20accessible%20open%20source%20R-package%0Adensityratio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520density%2520ratio%2520framework%2520for%2520evaluating%2520the%2520utility%2520of%2520synthetic%2520data%26entry.906535625%3DThom%2520Benjamin%2520Volker%2520and%2520Peter-Paul%2520de%2520Wolf%2520and%2520Erik-Jan%2520van%2520Kesteren%26entry.1292438233%3D%2520%2520Synthetic%2520data%2520generation%2520is%2520a%2520promising%2520technique%2520to%2520facilitate%2520the%2520use%2520of%250Asensitive%2520data%2520while%2520mitigating%2520the%2520risk%2520of%2520privacy%2520breaches.%2520However%252C%2520for%250Asynthetic%2520data%2520to%2520be%2520useful%2520in%2520downstream%2520analysis%2520tasks%252C%2520it%2520needs%2520to%2520be%2520of%250Asufficient%2520quality.%2520Various%2520methods%2520have%2520been%2520proposed%2520to%2520measure%2520the%2520utility%250Aof%2520synthetic%2520data%252C%2520but%2520their%2520results%2520are%2520often%2520incomplete%2520or%2520even%2520misleading.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520using%2520density%2520ratio%2520estimation%2520to%2520improve%2520quality%250Aevaluation%2520for%2520synthetic%2520data%252C%2520and%2520thereby%2520the%2520quality%2520of%2520synthesized%2520datasets.%250AWe%2520show%2520how%2520this%2520framework%2520relates%2520to%2520and%2520builds%2520on%2520existing%2520measures%252C%2520yielding%250Aglobal%2520and%2520local%2520utility%2520measures%2520that%2520are%2520informative%2520and%2520easy%2520to%2520interpret.%250AWe%2520develop%2520an%2520estimator%2520which%2520requires%2520little%2520to%2520no%2520manual%2520tuning%2520due%2520to%250Aautomatic%2520selection%2520of%2520a%2520nonparametric%2520density%2520ratio%2520model.%2520Through%250Asimulations%252C%2520we%2520find%2520that%2520density%2520ratio%2520estimation%2520yields%2520more%2520accurate%250Aestimates%2520of%2520global%2520utility%2520than%2520established%2520procedures.%2520A%2520real-world%2520data%250Aapplication%2520demonstrates%2520how%2520the%2520density%2520ratio%2520can%2520guide%2520refinements%2520of%250Asynthesis%2520models%2520and%2520can%2520be%2520used%2520to%2520improve%2520downstream%2520analyses.%2520We%2520conclude%250Athat%2520density%2520ratio%2520estimation%2520is%2520a%2520valuable%2520tool%2520in%2520synthetic%2520data%2520generation%250Aworkflows%2520and%2520provide%2520these%2520methods%2520in%2520the%2520accessible%2520open%2520source%2520R-package%250Adensityratio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20density%20ratio%20framework%20for%20evaluating%20the%20utility%20of%20synthetic%20data&entry.906535625=Thom%20Benjamin%20Volker%20and%20Peter-Paul%20de%20Wolf%20and%20Erik-Jan%20van%20Kesteren&entry.1292438233=%20%20Synthetic%20data%20generation%20is%20a%20promising%20technique%20to%20facilitate%20the%20use%20of%0Asensitive%20data%20while%20mitigating%20the%20risk%20of%20privacy%20breaches.%20However%2C%20for%0Asynthetic%20data%20to%20be%20useful%20in%20downstream%20analysis%20tasks%2C%20it%20needs%20to%20be%20of%0Asufficient%20quality.%20Various%20methods%20have%20been%20proposed%20to%20measure%20the%20utility%0Aof%20synthetic%20data%2C%20but%20their%20results%20are%20often%20incomplete%20or%20even%20misleading.%0AIn%20this%20paper%2C%20we%20propose%20using%20density%20ratio%20estimation%20to%20improve%20quality%0Aevaluation%20for%20synthetic%20data%2C%20and%20thereby%20the%20quality%20of%20synthesized%20datasets.%0AWe%20show%20how%20this%20framework%20relates%20to%20and%20builds%20on%20existing%20measures%2C%20yielding%0Aglobal%20and%20local%20utility%20measures%20that%20are%20informative%20and%20easy%20to%20interpret.%0AWe%20develop%20an%20estimator%20which%20requires%20little%20to%20no%20manual%20tuning%20due%20to%0Aautomatic%20selection%20of%20a%20nonparametric%20density%20ratio%20model.%20Through%0Asimulations%2C%20we%20find%20that%20density%20ratio%20estimation%20yields%20more%20accurate%0Aestimates%20of%20global%20utility%20than%20established%20procedures.%20A%20real-world%20data%0Aapplication%20demonstrates%20how%20the%20density%20ratio%20can%20guide%20refinements%20of%0Asynthesis%20models%20and%20can%20be%20used%20to%20improve%20downstream%20analyses.%20We%20conclude%0Athat%20density%20ratio%20estimation%20is%20a%20valuable%20tool%20in%20synthetic%20data%20generation%0Aworkflows%20and%20provide%20these%20methods%20in%20the%20accessible%20open%20source%20R-package%0Adensityratio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13167v1&entry.124074799=Read"},
{"title": "Hierarchical Spatio-Temporal State-Space Modeling for fMRI Analysis", "author": "Yuxiang Wei and Anees Abrol and Reihaneh Hassanzadeh and Vince Calhoun", "abstract": "  Recent advances in deep learning structured state space models, especially\nthe Mamba architecture, have demonstrated remarkable performance improvements\nwhile maintaining linear complexity. In this study, we introduce functional\nspatiotemporal Mamba (FST-Mamba), a Mamba-based model designed for discovering\nneurological biomarkers using functional magnetic resonance imaging (fMRI). We\nfocus on dynamic functional network connectivity (dFNC) derived from fMRI and\npropose a hierarchical spatiotemporal Mamba-based network that processes\nspatial and temporal information separately using Mamba-based encoders.\nLeveraging the topological uniqueness of the FNC matrix, we introduce a\ncomponent-wise varied-scale aggregation (CVA) mechanism to aggregate\nconnectivity across individual components within brain networks, enabling the\nmodel to capture both inter-component and inter-network information. To better\nhandle the FNC data, we develop a new component-specific scanning order.\nAdditionally, we propose symmetric rotary position encoding (SymRope) to encode\nthe relative positions of each functional connection while considering the\nsymmetric nature of the FNC matrix. Experimental results demonstrate\nsignificant improvements in the proposed FST-Mamba model on various brain-based\nclassification and regression tasks. Our work reveals the substantial potential\nof attention-free sequence modeling in brain discovery.\n", "link": "http://arxiv.org/abs/2408.13074v1", "date": "2024-08-23", "relevancy": 1.4499, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5032}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4779}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Spatio-Temporal%20State-Space%20Modeling%20for%20fMRI%20Analysis&body=Title%3A%20Hierarchical%20Spatio-Temporal%20State-Space%20Modeling%20for%20fMRI%20Analysis%0AAuthor%3A%20Yuxiang%20Wei%20and%20Anees%20Abrol%20and%20Reihaneh%20Hassanzadeh%20and%20Vince%20Calhoun%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20learning%20structured%20state%20space%20models%2C%20especially%0Athe%20Mamba%20architecture%2C%20have%20demonstrated%20remarkable%20performance%20improvements%0Awhile%20maintaining%20linear%20complexity.%20In%20this%20study%2C%20we%20introduce%20functional%0Aspatiotemporal%20Mamba%20%28FST-Mamba%29%2C%20a%20Mamba-based%20model%20designed%20for%20discovering%0Aneurological%20biomarkers%20using%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20We%0Afocus%20on%20dynamic%20functional%20network%20connectivity%20%28dFNC%29%20derived%20from%20fMRI%20and%0Apropose%20a%20hierarchical%20spatiotemporal%20Mamba-based%20network%20that%20processes%0Aspatial%20and%20temporal%20information%20separately%20using%20Mamba-based%20encoders.%0ALeveraging%20the%20topological%20uniqueness%20of%20the%20FNC%20matrix%2C%20we%20introduce%20a%0Acomponent-wise%20varied-scale%20aggregation%20%28CVA%29%20mechanism%20to%20aggregate%0Aconnectivity%20across%20individual%20components%20within%20brain%20networks%2C%20enabling%20the%0Amodel%20to%20capture%20both%20inter-component%20and%20inter-network%20information.%20To%20better%0Ahandle%20the%20FNC%20data%2C%20we%20develop%20a%20new%20component-specific%20scanning%20order.%0AAdditionally%2C%20we%20propose%20symmetric%20rotary%20position%20encoding%20%28SymRope%29%20to%20encode%0Athe%20relative%20positions%20of%20each%20functional%20connection%20while%20considering%20the%0Asymmetric%20nature%20of%20the%20FNC%20matrix.%20Experimental%20results%20demonstrate%0Asignificant%20improvements%20in%20the%20proposed%20FST-Mamba%20model%20on%20various%20brain-based%0Aclassification%20and%20regression%20tasks.%20Our%20work%20reveals%20the%20substantial%20potential%0Aof%20attention-free%20sequence%20modeling%20in%20brain%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Spatio-Temporal%2520State-Space%2520Modeling%2520for%2520fMRI%2520Analysis%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Anees%2520Abrol%2520and%2520Reihaneh%2520Hassanzadeh%2520and%2520Vince%2520Calhoun%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520deep%2520learning%2520structured%2520state%2520space%2520models%252C%2520especially%250Athe%2520Mamba%2520architecture%252C%2520have%2520demonstrated%2520remarkable%2520performance%2520improvements%250Awhile%2520maintaining%2520linear%2520complexity.%2520In%2520this%2520study%252C%2520we%2520introduce%2520functional%250Aspatiotemporal%2520Mamba%2520%2528FST-Mamba%2529%252C%2520a%2520Mamba-based%2520model%2520designed%2520for%2520discovering%250Aneurological%2520biomarkers%2520using%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529.%2520We%250Afocus%2520on%2520dynamic%2520functional%2520network%2520connectivity%2520%2528dFNC%2529%2520derived%2520from%2520fMRI%2520and%250Apropose%2520a%2520hierarchical%2520spatiotemporal%2520Mamba-based%2520network%2520that%2520processes%250Aspatial%2520and%2520temporal%2520information%2520separately%2520using%2520Mamba-based%2520encoders.%250ALeveraging%2520the%2520topological%2520uniqueness%2520of%2520the%2520FNC%2520matrix%252C%2520we%2520introduce%2520a%250Acomponent-wise%2520varied-scale%2520aggregation%2520%2528CVA%2529%2520mechanism%2520to%2520aggregate%250Aconnectivity%2520across%2520individual%2520components%2520within%2520brain%2520networks%252C%2520enabling%2520the%250Amodel%2520to%2520capture%2520both%2520inter-component%2520and%2520inter-network%2520information.%2520To%2520better%250Ahandle%2520the%2520FNC%2520data%252C%2520we%2520develop%2520a%2520new%2520component-specific%2520scanning%2520order.%250AAdditionally%252C%2520we%2520propose%2520symmetric%2520rotary%2520position%2520encoding%2520%2528SymRope%2529%2520to%2520encode%250Athe%2520relative%2520positions%2520of%2520each%2520functional%2520connection%2520while%2520considering%2520the%250Asymmetric%2520nature%2520of%2520the%2520FNC%2520matrix.%2520Experimental%2520results%2520demonstrate%250Asignificant%2520improvements%2520in%2520the%2520proposed%2520FST-Mamba%2520model%2520on%2520various%2520brain-based%250Aclassification%2520and%2520regression%2520tasks.%2520Our%2520work%2520reveals%2520the%2520substantial%2520potential%250Aof%2520attention-free%2520sequence%2520modeling%2520in%2520brain%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Spatio-Temporal%20State-Space%20Modeling%20for%20fMRI%20Analysis&entry.906535625=Yuxiang%20Wei%20and%20Anees%20Abrol%20and%20Reihaneh%20Hassanzadeh%20and%20Vince%20Calhoun&entry.1292438233=%20%20Recent%20advances%20in%20deep%20learning%20structured%20state%20space%20models%2C%20especially%0Athe%20Mamba%20architecture%2C%20have%20demonstrated%20remarkable%20performance%20improvements%0Awhile%20maintaining%20linear%20complexity.%20In%20this%20study%2C%20we%20introduce%20functional%0Aspatiotemporal%20Mamba%20%28FST-Mamba%29%2C%20a%20Mamba-based%20model%20designed%20for%20discovering%0Aneurological%20biomarkers%20using%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20We%0Afocus%20on%20dynamic%20functional%20network%20connectivity%20%28dFNC%29%20derived%20from%20fMRI%20and%0Apropose%20a%20hierarchical%20spatiotemporal%20Mamba-based%20network%20that%20processes%0Aspatial%20and%20temporal%20information%20separately%20using%20Mamba-based%20encoders.%0ALeveraging%20the%20topological%20uniqueness%20of%20the%20FNC%20matrix%2C%20we%20introduce%20a%0Acomponent-wise%20varied-scale%20aggregation%20%28CVA%29%20mechanism%20to%20aggregate%0Aconnectivity%20across%20individual%20components%20within%20brain%20networks%2C%20enabling%20the%0Amodel%20to%20capture%20both%20inter-component%20and%20inter-network%20information.%20To%20better%0Ahandle%20the%20FNC%20data%2C%20we%20develop%20a%20new%20component-specific%20scanning%20order.%0AAdditionally%2C%20we%20propose%20symmetric%20rotary%20position%20encoding%20%28SymRope%29%20to%20encode%0Athe%20relative%20positions%20of%20each%20functional%20connection%20while%20considering%20the%0Asymmetric%20nature%20of%20the%20FNC%20matrix.%20Experimental%20results%20demonstrate%0Asignificant%20improvements%20in%20the%20proposed%20FST-Mamba%20model%20on%20various%20brain-based%0Aclassification%20and%20regression%20tasks.%20Our%20work%20reveals%20the%20substantial%20potential%0Aof%20attention-free%20sequence%20modeling%20in%20brain%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13074v1&entry.124074799=Read"},
{"title": "Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep\n  Reinforcement Learning (CDRL) Approach", "author": "Zonghui Yang and Shijian Gao and Xiang Cheng", "abstract": "  Integrated sensing and communication (ISAC) technology is essential for\nsupporting vehicular networks. However, the communication channel in this\nscenario exhibits time variations, and the potential targets may move rapidly,\nresulting in double dynamics. This nature poses a challenge for real-time\nprecoder design. While optimization-based solutions are widely researched, they\nare complex and heavily rely on perfect channel-related information, which is\nimpractical in double dynamics. To address this challenge, we propose using\nconstrained deep reinforcement learning to facilitate dynamic updates to the\nISAC precoder. Additionally, the primal dual-deep deterministic policy gradient\nand Wolpertinger architecture are tailored to efficiently train the algorithm\nunder complex constraints and varying numbers of users. The proposed scheme not\nonly adapts to the dynamics based on observations but also leverages\nenvironmental information to enhance performance and reduce complexity. Its\nsuperiority over existing candidates has been validated through experiments.\n", "link": "http://arxiv.org/abs/2405.14347v3", "date": "2024-08-23", "relevancy": 1.5483, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5057}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doubly-Dynamic%20ISAC%20Precoding%20for%20Vehicular%20Networks%3A%20A%20Constrained%20Deep%0A%20%20Reinforcement%20Learning%20%28CDRL%29%20Approach&body=Title%3A%20Doubly-Dynamic%20ISAC%20Precoding%20for%20Vehicular%20Networks%3A%20A%20Constrained%20Deep%0A%20%20Reinforcement%20Learning%20%28CDRL%29%20Approach%0AAuthor%3A%20Zonghui%20Yang%20and%20Shijian%20Gao%20and%20Xiang%20Cheng%0AAbstract%3A%20%20%20Integrated%20sensing%20and%20communication%20%28ISAC%29%20technology%20is%20essential%20for%0Asupporting%20vehicular%20networks.%20However%2C%20the%20communication%20channel%20in%20this%0Ascenario%20exhibits%20time%20variations%2C%20and%20the%20potential%20targets%20may%20move%20rapidly%2C%0Aresulting%20in%20double%20dynamics.%20This%20nature%20poses%20a%20challenge%20for%20real-time%0Aprecoder%20design.%20While%20optimization-based%20solutions%20are%20widely%20researched%2C%20they%0Aare%20complex%20and%20heavily%20rely%20on%20perfect%20channel-related%20information%2C%20which%20is%0Aimpractical%20in%20double%20dynamics.%20To%20address%20this%20challenge%2C%20we%20propose%20using%0Aconstrained%20deep%20reinforcement%20learning%20to%20facilitate%20dynamic%20updates%20to%20the%0AISAC%20precoder.%20Additionally%2C%20the%20primal%20dual-deep%20deterministic%20policy%20gradient%0Aand%20Wolpertinger%20architecture%20are%20tailored%20to%20efficiently%20train%20the%20algorithm%0Aunder%20complex%20constraints%20and%20varying%20numbers%20of%20users.%20The%20proposed%20scheme%20not%0Aonly%20adapts%20to%20the%20dynamics%20based%20on%20observations%20but%20also%20leverages%0Aenvironmental%20information%20to%20enhance%20performance%20and%20reduce%20complexity.%20Its%0Asuperiority%20over%20existing%20candidates%20has%20been%20validated%20through%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14347v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubly-Dynamic%2520ISAC%2520Precoding%2520for%2520Vehicular%2520Networks%253A%2520A%2520Constrained%2520Deep%250A%2520%2520Reinforcement%2520Learning%2520%2528CDRL%2529%2520Approach%26entry.906535625%3DZonghui%2520Yang%2520and%2520Shijian%2520Gao%2520and%2520Xiang%2520Cheng%26entry.1292438233%3D%2520%2520Integrated%2520sensing%2520and%2520communication%2520%2528ISAC%2529%2520technology%2520is%2520essential%2520for%250Asupporting%2520vehicular%2520networks.%2520However%252C%2520the%2520communication%2520channel%2520in%2520this%250Ascenario%2520exhibits%2520time%2520variations%252C%2520and%2520the%2520potential%2520targets%2520may%2520move%2520rapidly%252C%250Aresulting%2520in%2520double%2520dynamics.%2520This%2520nature%2520poses%2520a%2520challenge%2520for%2520real-time%250Aprecoder%2520design.%2520While%2520optimization-based%2520solutions%2520are%2520widely%2520researched%252C%2520they%250Aare%2520complex%2520and%2520heavily%2520rely%2520on%2520perfect%2520channel-related%2520information%252C%2520which%2520is%250Aimpractical%2520in%2520double%2520dynamics.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520using%250Aconstrained%2520deep%2520reinforcement%2520learning%2520to%2520facilitate%2520dynamic%2520updates%2520to%2520the%250AISAC%2520precoder.%2520Additionally%252C%2520the%2520primal%2520dual-deep%2520deterministic%2520policy%2520gradient%250Aand%2520Wolpertinger%2520architecture%2520are%2520tailored%2520to%2520efficiently%2520train%2520the%2520algorithm%250Aunder%2520complex%2520constraints%2520and%2520varying%2520numbers%2520of%2520users.%2520The%2520proposed%2520scheme%2520not%250Aonly%2520adapts%2520to%2520the%2520dynamics%2520based%2520on%2520observations%2520but%2520also%2520leverages%250Aenvironmental%2520information%2520to%2520enhance%2520performance%2520and%2520reduce%2520complexity.%2520Its%250Asuperiority%2520over%2520existing%2520candidates%2520has%2520been%2520validated%2520through%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14347v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doubly-Dynamic%20ISAC%20Precoding%20for%20Vehicular%20Networks%3A%20A%20Constrained%20Deep%0A%20%20Reinforcement%20Learning%20%28CDRL%29%20Approach&entry.906535625=Zonghui%20Yang%20and%20Shijian%20Gao%20and%20Xiang%20Cheng&entry.1292438233=%20%20Integrated%20sensing%20and%20communication%20%28ISAC%29%20technology%20is%20essential%20for%0Asupporting%20vehicular%20networks.%20However%2C%20the%20communication%20channel%20in%20this%0Ascenario%20exhibits%20time%20variations%2C%20and%20the%20potential%20targets%20may%20move%20rapidly%2C%0Aresulting%20in%20double%20dynamics.%20This%20nature%20poses%20a%20challenge%20for%20real-time%0Aprecoder%20design.%20While%20optimization-based%20solutions%20are%20widely%20researched%2C%20they%0Aare%20complex%20and%20heavily%20rely%20on%20perfect%20channel-related%20information%2C%20which%20is%0Aimpractical%20in%20double%20dynamics.%20To%20address%20this%20challenge%2C%20we%20propose%20using%0Aconstrained%20deep%20reinforcement%20learning%20to%20facilitate%20dynamic%20updates%20to%20the%0AISAC%20precoder.%20Additionally%2C%20the%20primal%20dual-deep%20deterministic%20policy%20gradient%0Aand%20Wolpertinger%20architecture%20are%20tailored%20to%20efficiently%20train%20the%20algorithm%0Aunder%20complex%20constraints%20and%20varying%20numbers%20of%20users.%20The%20proposed%20scheme%20not%0Aonly%20adapts%20to%20the%20dynamics%20based%20on%20observations%20but%20also%20leverages%0Aenvironmental%20information%20to%20enhance%20performance%20and%20reduce%20complexity.%20Its%0Asuperiority%20over%20existing%20candidates%20has%20been%20validated%20through%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14347v3&entry.124074799=Read"},
{"title": "MedDec: A Dataset for Extracting Medical Decisions from Discharge\n  Summaries", "author": "Mohamed Elgaar and Jiali Cheng and Nidhi Vakil and Hadi Amiri and Leo Anthony Celi", "abstract": "  Medical decisions directly impact individuals' health and well-being.\nExtracting decision spans from clinical notes plays a crucial role in\nunderstanding medical decision-making processes. In this paper, we develop a\nnew dataset called \"MedDec\", which contains clinical notes of eleven different\nphenotypes (diseases) annotated by ten types of medical decisions. We introduce\nthe task of medical decision extraction, aiming to jointly extract and classify\ndifferent types of medical decisions within clinical notes. We provide a\ncomprehensive analysis of the dataset, develop a span detection model as a\nbaseline for this task, evaluate recent span detection approaches, and employ a\nfew metrics to measure the complexity of data samples. Our findings shed light\non the complexities inherent in clinical decision extraction and enable future\nwork in this area of research. The dataset and code are available through\nhttps://github.com/CLU-UML/MedDec.\n", "link": "http://arxiv.org/abs/2408.12980v1", "date": "2024-08-23", "relevancy": 1.259, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.479}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4112}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedDec%3A%20A%20Dataset%20for%20Extracting%20Medical%20Decisions%20from%20Discharge%0A%20%20Summaries&body=Title%3A%20MedDec%3A%20A%20Dataset%20for%20Extracting%20Medical%20Decisions%20from%20Discharge%0A%20%20Summaries%0AAuthor%3A%20Mohamed%20Elgaar%20and%20Jiali%20Cheng%20and%20Nidhi%20Vakil%20and%20Hadi%20Amiri%20and%20Leo%20Anthony%20Celi%0AAbstract%3A%20%20%20Medical%20decisions%20directly%20impact%20individuals%27%20health%20and%20well-being.%0AExtracting%20decision%20spans%20from%20clinical%20notes%20plays%20a%20crucial%20role%20in%0Aunderstanding%20medical%20decision-making%20processes.%20In%20this%20paper%2C%20we%20develop%20a%0Anew%20dataset%20called%20%22MedDec%22%2C%20which%20contains%20clinical%20notes%20of%20eleven%20different%0Aphenotypes%20%28diseases%29%20annotated%20by%20ten%20types%20of%20medical%20decisions.%20We%20introduce%0Athe%20task%20of%20medical%20decision%20extraction%2C%20aiming%20to%20jointly%20extract%20and%20classify%0Adifferent%20types%20of%20medical%20decisions%20within%20clinical%20notes.%20We%20provide%20a%0Acomprehensive%20analysis%20of%20the%20dataset%2C%20develop%20a%20span%20detection%20model%20as%20a%0Abaseline%20for%20this%20task%2C%20evaluate%20recent%20span%20detection%20approaches%2C%20and%20employ%20a%0Afew%20metrics%20to%20measure%20the%20complexity%20of%20data%20samples.%20Our%20findings%20shed%20light%0Aon%20the%20complexities%20inherent%20in%20clinical%20decision%20extraction%20and%20enable%20future%0Awork%20in%20this%20area%20of%20research.%20The%20dataset%20and%20code%20are%20available%20through%0Ahttps%3A//github.com/CLU-UML/MedDec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedDec%253A%2520A%2520Dataset%2520for%2520Extracting%2520Medical%2520Decisions%2520from%2520Discharge%250A%2520%2520Summaries%26entry.906535625%3DMohamed%2520Elgaar%2520and%2520Jiali%2520Cheng%2520and%2520Nidhi%2520Vakil%2520and%2520Hadi%2520Amiri%2520and%2520Leo%2520Anthony%2520Celi%26entry.1292438233%3D%2520%2520Medical%2520decisions%2520directly%2520impact%2520individuals%2527%2520health%2520and%2520well-being.%250AExtracting%2520decision%2520spans%2520from%2520clinical%2520notes%2520plays%2520a%2520crucial%2520role%2520in%250Aunderstanding%2520medical%2520decision-making%2520processes.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%250Anew%2520dataset%2520called%2520%2522MedDec%2522%252C%2520which%2520contains%2520clinical%2520notes%2520of%2520eleven%2520different%250Aphenotypes%2520%2528diseases%2529%2520annotated%2520by%2520ten%2520types%2520of%2520medical%2520decisions.%2520We%2520introduce%250Athe%2520task%2520of%2520medical%2520decision%2520extraction%252C%2520aiming%2520to%2520jointly%2520extract%2520and%2520classify%250Adifferent%2520types%2520of%2520medical%2520decisions%2520within%2520clinical%2520notes.%2520We%2520provide%2520a%250Acomprehensive%2520analysis%2520of%2520the%2520dataset%252C%2520develop%2520a%2520span%2520detection%2520model%2520as%2520a%250Abaseline%2520for%2520this%2520task%252C%2520evaluate%2520recent%2520span%2520detection%2520approaches%252C%2520and%2520employ%2520a%250Afew%2520metrics%2520to%2520measure%2520the%2520complexity%2520of%2520data%2520samples.%2520Our%2520findings%2520shed%2520light%250Aon%2520the%2520complexities%2520inherent%2520in%2520clinical%2520decision%2520extraction%2520and%2520enable%2520future%250Awork%2520in%2520this%2520area%2520of%2520research.%2520The%2520dataset%2520and%2520code%2520are%2520available%2520through%250Ahttps%253A//github.com/CLU-UML/MedDec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedDec%3A%20A%20Dataset%20for%20Extracting%20Medical%20Decisions%20from%20Discharge%0A%20%20Summaries&entry.906535625=Mohamed%20Elgaar%20and%20Jiali%20Cheng%20and%20Nidhi%20Vakil%20and%20Hadi%20Amiri%20and%20Leo%20Anthony%20Celi&entry.1292438233=%20%20Medical%20decisions%20directly%20impact%20individuals%27%20health%20and%20well-being.%0AExtracting%20decision%20spans%20from%20clinical%20notes%20plays%20a%20crucial%20role%20in%0Aunderstanding%20medical%20decision-making%20processes.%20In%20this%20paper%2C%20we%20develop%20a%0Anew%20dataset%20called%20%22MedDec%22%2C%20which%20contains%20clinical%20notes%20of%20eleven%20different%0Aphenotypes%20%28diseases%29%20annotated%20by%20ten%20types%20of%20medical%20decisions.%20We%20introduce%0Athe%20task%20of%20medical%20decision%20extraction%2C%20aiming%20to%20jointly%20extract%20and%20classify%0Adifferent%20types%20of%20medical%20decisions%20within%20clinical%20notes.%20We%20provide%20a%0Acomprehensive%20analysis%20of%20the%20dataset%2C%20develop%20a%20span%20detection%20model%20as%20a%0Abaseline%20for%20this%20task%2C%20evaluate%20recent%20span%20detection%20approaches%2C%20and%20employ%20a%0Afew%20metrics%20to%20measure%20the%20complexity%20of%20data%20samples.%20Our%20findings%20shed%20light%0Aon%20the%20complexities%20inherent%20in%20clinical%20decision%20extraction%20and%20enable%20future%0Awork%20in%20this%20area%20of%20research.%20The%20dataset%20and%20code%20are%20available%20through%0Ahttps%3A//github.com/CLU-UML/MedDec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12980v1&entry.124074799=Read"},
{"title": "On the good reliability of an interval-based metric to validate\n  prediction uncertainty for machine learning regression tasks", "author": "Pascal Pernot", "abstract": "  This short study presents an opportunistic approach to a (more) reliable\nvalidation method for prediction uncertainty average calibration. Considering\nthat variance-based calibration metrics (ZMS, NLL, RCE...) are quite sensitive\nto the presence of heavy tails in the uncertainty and error distributions, a\nshift is proposed to an interval-based metric, the Prediction Interval Coverage\nProbability (PICP). It is shown on a large ensemble of molecular properties\ndatasets that (1) sets of z-scores are well represented by Student's-$t(\\nu)$\ndistributions, $\\nu$ being the number of degrees of freedom; (2) accurate\nestimation of 95 $\\%$ prediction intervals can be obtained by the simple\n$2\\sigma$ rule for $\\nu>3$; and (3) the resulting PICPs are more quickly and\nreliably tested than variance-based calibration metrics. Overall, this method\nenables to test 20 $\\%$ more datasets than ZMS testing. Conditional calibration\nis also assessed using the PICP approach.\n", "link": "http://arxiv.org/abs/2408.13089v1", "date": "2024-08-23", "relevancy": 1.4479, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4825}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20good%20reliability%20of%20an%20interval-based%20metric%20to%20validate%0A%20%20prediction%20uncertainty%20for%20machine%20learning%20regression%20tasks&body=Title%3A%20On%20the%20good%20reliability%20of%20an%20interval-based%20metric%20to%20validate%0A%20%20prediction%20uncertainty%20for%20machine%20learning%20regression%20tasks%0AAuthor%3A%20Pascal%20Pernot%0AAbstract%3A%20%20%20This%20short%20study%20presents%20an%20opportunistic%20approach%20to%20a%20%28more%29%20reliable%0Avalidation%20method%20for%20prediction%20uncertainty%20average%20calibration.%20Considering%0Athat%20variance-based%20calibration%20metrics%20%28ZMS%2C%20NLL%2C%20RCE...%29%20are%20quite%20sensitive%0Ato%20the%20presence%20of%20heavy%20tails%20in%20the%20uncertainty%20and%20error%20distributions%2C%20a%0Ashift%20is%20proposed%20to%20an%20interval-based%20metric%2C%20the%20Prediction%20Interval%20Coverage%0AProbability%20%28PICP%29.%20It%20is%20shown%20on%20a%20large%20ensemble%20of%20molecular%20properties%0Adatasets%20that%20%281%29%20sets%20of%20z-scores%20are%20well%20represented%20by%20Student%27s-%24t%28%5Cnu%29%24%0Adistributions%2C%20%24%5Cnu%24%20being%20the%20number%20of%20degrees%20of%20freedom%3B%20%282%29%20accurate%0Aestimation%20of%2095%20%24%5C%25%24%20prediction%20intervals%20can%20be%20obtained%20by%20the%20simple%0A%242%5Csigma%24%20rule%20for%20%24%5Cnu%3E3%24%3B%20and%20%283%29%20the%20resulting%20PICPs%20are%20more%20quickly%20and%0Areliably%20tested%20than%20variance-based%20calibration%20metrics.%20Overall%2C%20this%20method%0Aenables%20to%20test%2020%20%24%5C%25%24%20more%20datasets%20than%20ZMS%20testing.%20Conditional%20calibration%0Ais%20also%20assessed%20using%20the%20PICP%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520good%2520reliability%2520of%2520an%2520interval-based%2520metric%2520to%2520validate%250A%2520%2520prediction%2520uncertainty%2520for%2520machine%2520learning%2520regression%2520tasks%26entry.906535625%3DPascal%2520Pernot%26entry.1292438233%3D%2520%2520This%2520short%2520study%2520presents%2520an%2520opportunistic%2520approach%2520to%2520a%2520%2528more%2529%2520reliable%250Avalidation%2520method%2520for%2520prediction%2520uncertainty%2520average%2520calibration.%2520Considering%250Athat%2520variance-based%2520calibration%2520metrics%2520%2528ZMS%252C%2520NLL%252C%2520RCE...%2529%2520are%2520quite%2520sensitive%250Ato%2520the%2520presence%2520of%2520heavy%2520tails%2520in%2520the%2520uncertainty%2520and%2520error%2520distributions%252C%2520a%250Ashift%2520is%2520proposed%2520to%2520an%2520interval-based%2520metric%252C%2520the%2520Prediction%2520Interval%2520Coverage%250AProbability%2520%2528PICP%2529.%2520It%2520is%2520shown%2520on%2520a%2520large%2520ensemble%2520of%2520molecular%2520properties%250Adatasets%2520that%2520%25281%2529%2520sets%2520of%2520z-scores%2520are%2520well%2520represented%2520by%2520Student%2527s-%2524t%2528%255Cnu%2529%2524%250Adistributions%252C%2520%2524%255Cnu%2524%2520being%2520the%2520number%2520of%2520degrees%2520of%2520freedom%253B%2520%25282%2529%2520accurate%250Aestimation%2520of%252095%2520%2524%255C%2525%2524%2520prediction%2520intervals%2520can%2520be%2520obtained%2520by%2520the%2520simple%250A%25242%255Csigma%2524%2520rule%2520for%2520%2524%255Cnu%253E3%2524%253B%2520and%2520%25283%2529%2520the%2520resulting%2520PICPs%2520are%2520more%2520quickly%2520and%250Areliably%2520tested%2520than%2520variance-based%2520calibration%2520metrics.%2520Overall%252C%2520this%2520method%250Aenables%2520to%2520test%252020%2520%2524%255C%2525%2524%2520more%2520datasets%2520than%2520ZMS%2520testing.%2520Conditional%2520calibration%250Ais%2520also%2520assessed%2520using%2520the%2520PICP%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20good%20reliability%20of%20an%20interval-based%20metric%20to%20validate%0A%20%20prediction%20uncertainty%20for%20machine%20learning%20regression%20tasks&entry.906535625=Pascal%20Pernot&entry.1292438233=%20%20This%20short%20study%20presents%20an%20opportunistic%20approach%20to%20a%20%28more%29%20reliable%0Avalidation%20method%20for%20prediction%20uncertainty%20average%20calibration.%20Considering%0Athat%20variance-based%20calibration%20metrics%20%28ZMS%2C%20NLL%2C%20RCE...%29%20are%20quite%20sensitive%0Ato%20the%20presence%20of%20heavy%20tails%20in%20the%20uncertainty%20and%20error%20distributions%2C%20a%0Ashift%20is%20proposed%20to%20an%20interval-based%20metric%2C%20the%20Prediction%20Interval%20Coverage%0AProbability%20%28PICP%29.%20It%20is%20shown%20on%20a%20large%20ensemble%20of%20molecular%20properties%0Adatasets%20that%20%281%29%20sets%20of%20z-scores%20are%20well%20represented%20by%20Student%27s-%24t%28%5Cnu%29%24%0Adistributions%2C%20%24%5Cnu%24%20being%20the%20number%20of%20degrees%20of%20freedom%3B%20%282%29%20accurate%0Aestimation%20of%2095%20%24%5C%25%24%20prediction%20intervals%20can%20be%20obtained%20by%20the%20simple%0A%242%5Csigma%24%20rule%20for%20%24%5Cnu%3E3%24%3B%20and%20%283%29%20the%20resulting%20PICPs%20are%20more%20quickly%20and%0Areliably%20tested%20than%20variance-based%20calibration%20metrics.%20Overall%2C%20this%20method%0Aenables%20to%20test%2020%20%24%5C%25%24%20more%20datasets%20than%20ZMS%20testing.%20Conditional%20calibration%0Ais%20also%20assessed%20using%20the%20PICP%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13089v1&entry.124074799=Read"},
{"title": "iMTSP: Solving Min-Max Multiple Traveling Salesman Problem with\n  Imperative Learning", "author": "Yifan Guo and Zhongqiang Ren and Chen Wang", "abstract": "  This paper considers a Min-Max Multiple Traveling Salesman Problem (MTSP),\nwhere the goal is to find a set of tours, one for each agent, to collectively\nvisit all the cities while minimizing the length of the longest tour. Though\nMTSP has been widely studied, obtaining near-optimal solutions for large-scale\nproblems is still challenging due to its NP-hardness. Recent efforts in\ndata-driven methods face challenges of the need for hard-to-obtain supervision\nand issues with high variance in gradient estimations, leading to slow\nconvergence and highly suboptimal solutions. We address these issues by\nreformulating MTSP as a bilevel optimization problem, using the concept of\nimperative learning (IL). This involves introducing an allocation network that\ndecomposes the MTSP into multiple single-agent traveling salesman problems\n(TSPs). The longest tour from these TSP solutions is then used to\nself-supervise the allocation network, resulting in a new self-supervised,\nbilevel, end-to-end learning framework, which we refer to as imperative MTSP\n(iMTSP). Additionally, to tackle the high-variance gradient issues during the\noptimization, we introduce a control variate-based gradient estimation\nalgorithm. Our experiments showed that these innovative designs enable our\ngradient estimator to converge 20% faster than the advanced reinforcement\nlearning baseline and find up to 80% shorter tour length compared with Google\nOR-Tools MTSP solver, especially in large-scale problems (e.g. 1000 cities and\n15 agents).\n", "link": "http://arxiv.org/abs/2405.00285v4", "date": "2024-08-23", "relevancy": 1.3552, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4503}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iMTSP%3A%20Solving%20Min-Max%20Multiple%20Traveling%20Salesman%20Problem%20with%0A%20%20Imperative%20Learning&body=Title%3A%20iMTSP%3A%20Solving%20Min-Max%20Multiple%20Traveling%20Salesman%20Problem%20with%0A%20%20Imperative%20Learning%0AAuthor%3A%20Yifan%20Guo%20and%20Zhongqiang%20Ren%20and%20Chen%20Wang%0AAbstract%3A%20%20%20This%20paper%20considers%20a%20Min-Max%20Multiple%20Traveling%20Salesman%20Problem%20%28MTSP%29%2C%0Awhere%20the%20goal%20is%20to%20find%20a%20set%20of%20tours%2C%20one%20for%20each%20agent%2C%20to%20collectively%0Avisit%20all%20the%20cities%20while%20minimizing%20the%20length%20of%20the%20longest%20tour.%20Though%0AMTSP%20has%20been%20widely%20studied%2C%20obtaining%20near-optimal%20solutions%20for%20large-scale%0Aproblems%20is%20still%20challenging%20due%20to%20its%20NP-hardness.%20Recent%20efforts%20in%0Adata-driven%20methods%20face%20challenges%20of%20the%20need%20for%20hard-to-obtain%20supervision%0Aand%20issues%20with%20high%20variance%20in%20gradient%20estimations%2C%20leading%20to%20slow%0Aconvergence%20and%20highly%20suboptimal%20solutions.%20We%20address%20these%20issues%20by%0Areformulating%20MTSP%20as%20a%20bilevel%20optimization%20problem%2C%20using%20the%20concept%20of%0Aimperative%20learning%20%28IL%29.%20This%20involves%20introducing%20an%20allocation%20network%20that%0Adecomposes%20the%20MTSP%20into%20multiple%20single-agent%20traveling%20salesman%20problems%0A%28TSPs%29.%20The%20longest%20tour%20from%20these%20TSP%20solutions%20is%20then%20used%20to%0Aself-supervise%20the%20allocation%20network%2C%20resulting%20in%20a%20new%20self-supervised%2C%0Abilevel%2C%20end-to-end%20learning%20framework%2C%20which%20we%20refer%20to%20as%20imperative%20MTSP%0A%28iMTSP%29.%20Additionally%2C%20to%20tackle%20the%20high-variance%20gradient%20issues%20during%20the%0Aoptimization%2C%20we%20introduce%20a%20control%20variate-based%20gradient%20estimation%0Aalgorithm.%20Our%20experiments%20showed%20that%20these%20innovative%20designs%20enable%20our%0Agradient%20estimator%20to%20converge%2020%25%20faster%20than%20the%20advanced%20reinforcement%0Alearning%20baseline%20and%20find%20up%20to%2080%25%20shorter%20tour%20length%20compared%20with%20Google%0AOR-Tools%20MTSP%20solver%2C%20especially%20in%20large-scale%20problems%20%28e.g.%201000%20cities%20and%0A15%20agents%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00285v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiMTSP%253A%2520Solving%2520Min-Max%2520Multiple%2520Traveling%2520Salesman%2520Problem%2520with%250A%2520%2520Imperative%2520Learning%26entry.906535625%3DYifan%2520Guo%2520and%2520Zhongqiang%2520Ren%2520and%2520Chen%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520a%2520Min-Max%2520Multiple%2520Traveling%2520Salesman%2520Problem%2520%2528MTSP%2529%252C%250Awhere%2520the%2520goal%2520is%2520to%2520find%2520a%2520set%2520of%2520tours%252C%2520one%2520for%2520each%2520agent%252C%2520to%2520collectively%250Avisit%2520all%2520the%2520cities%2520while%2520minimizing%2520the%2520length%2520of%2520the%2520longest%2520tour.%2520Though%250AMTSP%2520has%2520been%2520widely%2520studied%252C%2520obtaining%2520near-optimal%2520solutions%2520for%2520large-scale%250Aproblems%2520is%2520still%2520challenging%2520due%2520to%2520its%2520NP-hardness.%2520Recent%2520efforts%2520in%250Adata-driven%2520methods%2520face%2520challenges%2520of%2520the%2520need%2520for%2520hard-to-obtain%2520supervision%250Aand%2520issues%2520with%2520high%2520variance%2520in%2520gradient%2520estimations%252C%2520leading%2520to%2520slow%250Aconvergence%2520and%2520highly%2520suboptimal%2520solutions.%2520We%2520address%2520these%2520issues%2520by%250Areformulating%2520MTSP%2520as%2520a%2520bilevel%2520optimization%2520problem%252C%2520using%2520the%2520concept%2520of%250Aimperative%2520learning%2520%2528IL%2529.%2520This%2520involves%2520introducing%2520an%2520allocation%2520network%2520that%250Adecomposes%2520the%2520MTSP%2520into%2520multiple%2520single-agent%2520traveling%2520salesman%2520problems%250A%2528TSPs%2529.%2520The%2520longest%2520tour%2520from%2520these%2520TSP%2520solutions%2520is%2520then%2520used%2520to%250Aself-supervise%2520the%2520allocation%2520network%252C%2520resulting%2520in%2520a%2520new%2520self-supervised%252C%250Abilevel%252C%2520end-to-end%2520learning%2520framework%252C%2520which%2520we%2520refer%2520to%2520as%2520imperative%2520MTSP%250A%2528iMTSP%2529.%2520Additionally%252C%2520to%2520tackle%2520the%2520high-variance%2520gradient%2520issues%2520during%2520the%250Aoptimization%252C%2520we%2520introduce%2520a%2520control%2520variate-based%2520gradient%2520estimation%250Aalgorithm.%2520Our%2520experiments%2520showed%2520that%2520these%2520innovative%2520designs%2520enable%2520our%250Agradient%2520estimator%2520to%2520converge%252020%2525%2520faster%2520than%2520the%2520advanced%2520reinforcement%250Alearning%2520baseline%2520and%2520find%2520up%2520to%252080%2525%2520shorter%2520tour%2520length%2520compared%2520with%2520Google%250AOR-Tools%2520MTSP%2520solver%252C%2520especially%2520in%2520large-scale%2520problems%2520%2528e.g.%25201000%2520cities%2520and%250A15%2520agents%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00285v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iMTSP%3A%20Solving%20Min-Max%20Multiple%20Traveling%20Salesman%20Problem%20with%0A%20%20Imperative%20Learning&entry.906535625=Yifan%20Guo%20and%20Zhongqiang%20Ren%20and%20Chen%20Wang&entry.1292438233=%20%20This%20paper%20considers%20a%20Min-Max%20Multiple%20Traveling%20Salesman%20Problem%20%28MTSP%29%2C%0Awhere%20the%20goal%20is%20to%20find%20a%20set%20of%20tours%2C%20one%20for%20each%20agent%2C%20to%20collectively%0Avisit%20all%20the%20cities%20while%20minimizing%20the%20length%20of%20the%20longest%20tour.%20Though%0AMTSP%20has%20been%20widely%20studied%2C%20obtaining%20near-optimal%20solutions%20for%20large-scale%0Aproblems%20is%20still%20challenging%20due%20to%20its%20NP-hardness.%20Recent%20efforts%20in%0Adata-driven%20methods%20face%20challenges%20of%20the%20need%20for%20hard-to-obtain%20supervision%0Aand%20issues%20with%20high%20variance%20in%20gradient%20estimations%2C%20leading%20to%20slow%0Aconvergence%20and%20highly%20suboptimal%20solutions.%20We%20address%20these%20issues%20by%0Areformulating%20MTSP%20as%20a%20bilevel%20optimization%20problem%2C%20using%20the%20concept%20of%0Aimperative%20learning%20%28IL%29.%20This%20involves%20introducing%20an%20allocation%20network%20that%0Adecomposes%20the%20MTSP%20into%20multiple%20single-agent%20traveling%20salesman%20problems%0A%28TSPs%29.%20The%20longest%20tour%20from%20these%20TSP%20solutions%20is%20then%20used%20to%0Aself-supervise%20the%20allocation%20network%2C%20resulting%20in%20a%20new%20self-supervised%2C%0Abilevel%2C%20end-to-end%20learning%20framework%2C%20which%20we%20refer%20to%20as%20imperative%20MTSP%0A%28iMTSP%29.%20Additionally%2C%20to%20tackle%20the%20high-variance%20gradient%20issues%20during%20the%0Aoptimization%2C%20we%20introduce%20a%20control%20variate-based%20gradient%20estimation%0Aalgorithm.%20Our%20experiments%20showed%20that%20these%20innovative%20designs%20enable%20our%0Agradient%20estimator%20to%20converge%2020%25%20faster%20than%20the%20advanced%20reinforcement%0Alearning%20baseline%20and%20find%20up%20to%2080%25%20shorter%20tour%20length%20compared%20with%20Google%0AOR-Tools%20MTSP%20solver%2C%20especially%20in%20large-scale%20problems%20%28e.g.%201000%20cities%20and%0A15%20agents%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00285v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


