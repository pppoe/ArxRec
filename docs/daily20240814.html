<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240813.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting", "author": "Helisa Dhamo and Yinyu Nie and Arthur Moreau and Jifei Song and Richard Shaw and Yiren Zhou and Eduardo P\u00e9rez-Pellitero", "abstract": "  3D head animation has seen major quality and runtime improvements over the\nlast few years, particularly empowered by the advances in differentiable\nrendering and neural radiance fields. Real-time rendering is a highly desirable\ngoal for real-world applications. We propose HeadGaS, a model that uses 3D\nGaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper\nwe introduce a hybrid model that extends the explicit 3DGS representation with\na base of learnable latent features, which can be linearly blended with\nlow-dimensional parameters from parametric head models to obtain\nexpression-dependent color and opacity values. We demonstrate that HeadGaS\ndelivers state-of-the-art results in real-time inference frame rates,\nsurpassing baselines by up to 2dB, while accelerating rendering speed by over\nx10.\n", "link": "http://arxiv.org/abs/2312.02902v2", "date": "2024-08-13", "relevancy": 3.8605, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8022}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8022}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadGaS%3A%20Real-Time%20Animatable%20Head%20Avatars%20via%203D%20Gaussian%20Splatting&body=Title%3A%20HeadGaS%3A%20Real-Time%20Animatable%20Head%20Avatars%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Helisa%20Dhamo%20and%20Yinyu%20Nie%20and%20Arthur%20Moreau%20and%20Jifei%20Song%20and%20Richard%20Shaw%20and%20Yiren%20Zhou%20and%20Eduardo%20P%C3%A9rez-Pellitero%0AAbstract%3A%20%20%203D%20head%20animation%20has%20seen%20major%20quality%20and%20runtime%20improvements%20over%20the%0Alast%20few%20years%2C%20particularly%20empowered%20by%20the%20advances%20in%20differentiable%0Arendering%20and%20neural%20radiance%20fields.%20Real-time%20rendering%20is%20a%20highly%20desirable%0Agoal%20for%20real-world%20applications.%20We%20propose%20HeadGaS%2C%20a%20model%20that%20uses%203D%0AGaussian%20Splats%20%283DGS%29%20for%203D%20head%20reconstruction%20and%20animation.%20In%20this%20paper%0Awe%20introduce%20a%20hybrid%20model%20that%20extends%20the%20explicit%203DGS%20representation%20with%0Aa%20base%20of%20learnable%20latent%20features%2C%20which%20can%20be%20linearly%20blended%20with%0Alow-dimensional%20parameters%20from%20parametric%20head%20models%20to%20obtain%0Aexpression-dependent%20color%20and%20opacity%20values.%20We%20demonstrate%20that%20HeadGaS%0Adelivers%20state-of-the-art%20results%20in%20real-time%20inference%20frame%20rates%2C%0Asurpassing%20baselines%20by%20up%20to%202dB%2C%20while%20accelerating%20rendering%20speed%20by%20over%0Ax10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02902v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadGaS%253A%2520Real-Time%2520Animatable%2520Head%2520Avatars%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DHelisa%2520Dhamo%2520and%2520Yinyu%2520Nie%2520and%2520Arthur%2520Moreau%2520and%2520Jifei%2520Song%2520and%2520Richard%2520Shaw%2520and%2520Yiren%2520Zhou%2520and%2520Eduardo%2520P%25C3%25A9rez-Pellitero%26entry.1292438233%3D%2520%25203D%2520head%2520animation%2520has%2520seen%2520major%2520quality%2520and%2520runtime%2520improvements%2520over%2520the%250Alast%2520few%2520years%252C%2520particularly%2520empowered%2520by%2520the%2520advances%2520in%2520differentiable%250Arendering%2520and%2520neural%2520radiance%2520fields.%2520Real-time%2520rendering%2520is%2520a%2520highly%2520desirable%250Agoal%2520for%2520real-world%2520applications.%2520We%2520propose%2520HeadGaS%252C%2520a%2520model%2520that%2520uses%25203D%250AGaussian%2520Splats%2520%25283DGS%2529%2520for%25203D%2520head%2520reconstruction%2520and%2520animation.%2520In%2520this%2520paper%250Awe%2520introduce%2520a%2520hybrid%2520model%2520that%2520extends%2520the%2520explicit%25203DGS%2520representation%2520with%250Aa%2520base%2520of%2520learnable%2520latent%2520features%252C%2520which%2520can%2520be%2520linearly%2520blended%2520with%250Alow-dimensional%2520parameters%2520from%2520parametric%2520head%2520models%2520to%2520obtain%250Aexpression-dependent%2520color%2520and%2520opacity%2520values.%2520We%2520demonstrate%2520that%2520HeadGaS%250Adelivers%2520state-of-the-art%2520results%2520in%2520real-time%2520inference%2520frame%2520rates%252C%250Asurpassing%2520baselines%2520by%2520up%2520to%25202dB%252C%2520while%2520accelerating%2520rendering%2520speed%2520by%2520over%250Ax10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02902v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadGaS%3A%20Real-Time%20Animatable%20Head%20Avatars%20via%203D%20Gaussian%20Splatting&entry.906535625=Helisa%20Dhamo%20and%20Yinyu%20Nie%20and%20Arthur%20Moreau%20and%20Jifei%20Song%20and%20Richard%20Shaw%20and%20Yiren%20Zhou%20and%20Eduardo%20P%C3%A9rez-Pellitero&entry.1292438233=%20%203D%20head%20animation%20has%20seen%20major%20quality%20and%20runtime%20improvements%20over%20the%0Alast%20few%20years%2C%20particularly%20empowered%20by%20the%20advances%20in%20differentiable%0Arendering%20and%20neural%20radiance%20fields.%20Real-time%20rendering%20is%20a%20highly%20desirable%0Agoal%20for%20real-world%20applications.%20We%20propose%20HeadGaS%2C%20a%20model%20that%20uses%203D%0AGaussian%20Splats%20%283DGS%29%20for%203D%20head%20reconstruction%20and%20animation.%20In%20this%20paper%0Awe%20introduce%20a%20hybrid%20model%20that%20extends%20the%20explicit%203DGS%20representation%20with%0Aa%20base%20of%20learnable%20latent%20features%2C%20which%20can%20be%20linearly%20blended%20with%0Alow-dimensional%20parameters%20from%20parametric%20head%20models%20to%20obtain%0Aexpression-dependent%20color%20and%20opacity%20values.%20We%20demonstrate%20that%20HeadGaS%0Adelivers%20state-of-the-art%20results%20in%20real-time%20inference%20frame%20rates%2C%0Asurpassing%20baselines%20by%20up%20to%202dB%2C%20while%20accelerating%20rendering%20speed%20by%20over%0Ax10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02902v2&entry.124074799=Read"},
{"title": "SpectralGaussians: Semantic, spectral 3D Gaussian splatting for\n  multi-spectral scene representation, visualization and analysis", "author": "Saptarshi Neil Sinha and Holger Graf and Michael Weinmann", "abstract": "  We propose a novel cross-spectral rendering framework based on 3D Gaussian\nSplatting (3DGS) that generates realistic and semantically meaningful splats\nfrom registered multi-view spectrum and segmentation maps. This extension\nenhances the representation of scenes with multiple spectra, providing insights\ninto the underlying materials and segmentation. We introduce an improved\nphysically-based rendering approach for Gaussian splats, estimating reflectance\nand lights per spectra, thereby enhancing accuracy and realism. In a\ncomprehensive quantitative and qualitative evaluation, we demonstrate the\nsuperior performance of our approach with respect to other recent\nlearning-based spectral scene representation approaches (i.e., XNeRF and\nSpectralNeRF) as well as other non-spectral state-of-the-art learning-based\napproaches. Our work also demonstrates the potential of spectral scene\nunderstanding for precise scene editing techniques like style transfer,\ninpainting, and removal. Thereby, our contributions address challenges in\nmulti-spectral scene representation, rendering, and editing, offering new\npossibilities for diverse applications.\n", "link": "http://arxiv.org/abs/2408.06975v1", "date": "2024-08-13", "relevancy": 3.0911, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7047}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6084}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectralGaussians%3A%20Semantic%2C%20spectral%203D%20Gaussian%20splatting%20for%0A%20%20multi-spectral%20scene%20representation%2C%20visualization%20and%20analysis&body=Title%3A%20SpectralGaussians%3A%20Semantic%2C%20spectral%203D%20Gaussian%20splatting%20for%0A%20%20multi-spectral%20scene%20representation%2C%20visualization%20and%20analysis%0AAuthor%3A%20Saptarshi%20Neil%20Sinha%20and%20Holger%20Graf%20and%20Michael%20Weinmann%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20cross-spectral%20rendering%20framework%20based%20on%203D%20Gaussian%0ASplatting%20%283DGS%29%20that%20generates%20realistic%20and%20semantically%20meaningful%20splats%0Afrom%20registered%20multi-view%20spectrum%20and%20segmentation%20maps.%20This%20extension%0Aenhances%20the%20representation%20of%20scenes%20with%20multiple%20spectra%2C%20providing%20insights%0Ainto%20the%20underlying%20materials%20and%20segmentation.%20We%20introduce%20an%20improved%0Aphysically-based%20rendering%20approach%20for%20Gaussian%20splats%2C%20estimating%20reflectance%0Aand%20lights%20per%20spectra%2C%20thereby%20enhancing%20accuracy%20and%20realism.%20In%20a%0Acomprehensive%20quantitative%20and%20qualitative%20evaluation%2C%20we%20demonstrate%20the%0Asuperior%20performance%20of%20our%20approach%20with%20respect%20to%20other%20recent%0Alearning-based%20spectral%20scene%20representation%20approaches%20%28i.e.%2C%20XNeRF%20and%0ASpectralNeRF%29%20as%20well%20as%20other%20non-spectral%20state-of-the-art%20learning-based%0Aapproaches.%20Our%20work%20also%20demonstrates%20the%20potential%20of%20spectral%20scene%0Aunderstanding%20for%20precise%20scene%20editing%20techniques%20like%20style%20transfer%2C%0Ainpainting%2C%20and%20removal.%20Thereby%2C%20our%20contributions%20address%20challenges%20in%0Amulti-spectral%20scene%20representation%2C%20rendering%2C%20and%20editing%2C%20offering%20new%0Apossibilities%20for%20diverse%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectralGaussians%253A%2520Semantic%252C%2520spectral%25203D%2520Gaussian%2520splatting%2520for%250A%2520%2520multi-spectral%2520scene%2520representation%252C%2520visualization%2520and%2520analysis%26entry.906535625%3DSaptarshi%2520Neil%2520Sinha%2520and%2520Holger%2520Graf%2520and%2520Michael%2520Weinmann%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520cross-spectral%2520rendering%2520framework%2520based%2520on%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520that%2520generates%2520realistic%2520and%2520semantically%2520meaningful%2520splats%250Afrom%2520registered%2520multi-view%2520spectrum%2520and%2520segmentation%2520maps.%2520This%2520extension%250Aenhances%2520the%2520representation%2520of%2520scenes%2520with%2520multiple%2520spectra%252C%2520providing%2520insights%250Ainto%2520the%2520underlying%2520materials%2520and%2520segmentation.%2520We%2520introduce%2520an%2520improved%250Aphysically-based%2520rendering%2520approach%2520for%2520Gaussian%2520splats%252C%2520estimating%2520reflectance%250Aand%2520lights%2520per%2520spectra%252C%2520thereby%2520enhancing%2520accuracy%2520and%2520realism.%2520In%2520a%250Acomprehensive%2520quantitative%2520and%2520qualitative%2520evaluation%252C%2520we%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520our%2520approach%2520with%2520respect%2520to%2520other%2520recent%250Alearning-based%2520spectral%2520scene%2520representation%2520approaches%2520%2528i.e.%252C%2520XNeRF%2520and%250ASpectralNeRF%2529%2520as%2520well%2520as%2520other%2520non-spectral%2520state-of-the-art%2520learning-based%250Aapproaches.%2520Our%2520work%2520also%2520demonstrates%2520the%2520potential%2520of%2520spectral%2520scene%250Aunderstanding%2520for%2520precise%2520scene%2520editing%2520techniques%2520like%2520style%2520transfer%252C%250Ainpainting%252C%2520and%2520removal.%2520Thereby%252C%2520our%2520contributions%2520address%2520challenges%2520in%250Amulti-spectral%2520scene%2520representation%252C%2520rendering%252C%2520and%2520editing%252C%2520offering%2520new%250Apossibilities%2520for%2520diverse%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectralGaussians%3A%20Semantic%2C%20spectral%203D%20Gaussian%20splatting%20for%0A%20%20multi-spectral%20scene%20representation%2C%20visualization%20and%20analysis&entry.906535625=Saptarshi%20Neil%20Sinha%20and%20Holger%20Graf%20and%20Michael%20Weinmann&entry.1292438233=%20%20We%20propose%20a%20novel%20cross-spectral%20rendering%20framework%20based%20on%203D%20Gaussian%0ASplatting%20%283DGS%29%20that%20generates%20realistic%20and%20semantically%20meaningful%20splats%0Afrom%20registered%20multi-view%20spectrum%20and%20segmentation%20maps.%20This%20extension%0Aenhances%20the%20representation%20of%20scenes%20with%20multiple%20spectra%2C%20providing%20insights%0Ainto%20the%20underlying%20materials%20and%20segmentation.%20We%20introduce%20an%20improved%0Aphysically-based%20rendering%20approach%20for%20Gaussian%20splats%2C%20estimating%20reflectance%0Aand%20lights%20per%20spectra%2C%20thereby%20enhancing%20accuracy%20and%20realism.%20In%20a%0Acomprehensive%20quantitative%20and%20qualitative%20evaluation%2C%20we%20demonstrate%20the%0Asuperior%20performance%20of%20our%20approach%20with%20respect%20to%20other%20recent%0Alearning-based%20spectral%20scene%20representation%20approaches%20%28i.e.%2C%20XNeRF%20and%0ASpectralNeRF%29%20as%20well%20as%20other%20non-spectral%20state-of-the-art%20learning-based%0Aapproaches.%20Our%20work%20also%20demonstrates%20the%20potential%20of%20spectral%20scene%0Aunderstanding%20for%20precise%20scene%20editing%20techniques%20like%20style%20transfer%2C%0Ainpainting%2C%20and%20removal.%20Thereby%2C%20our%20contributions%20address%20challenges%20in%0Amulti-spectral%20scene%20representation%2C%20rendering%2C%20and%20editing%2C%20offering%20new%0Apossibilities%20for%20diverse%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06975v1&entry.124074799=Read"},
{"title": "Refractive COLMAP: Refractive Structure-from-Motion Revisited", "author": "Mengkun She and Felix Seegr\u00e4ber and David Nakath and Kevin K\u00f6ser", "abstract": "  In this paper, we present a complete refractive Structure-from-Motion (RSfM)\nframework for underwater 3D reconstruction using refractive camera setups (for\nboth, flat- and dome-port underwater housings). Despite notable achievements in\nrefractive multi-view geometry over the past decade, a robust, complete and\npublicly available solution for such tasks is not available at present, and\noften practical applications have to resort to approximating refraction effects\nby the intrinsic (distortion) parameters of a pinhole camera model. To fill\nthis gap, we have integrated refraction considerations throughout the entire\nSfM process within the state-of-the-art, open-source SfM framework COLMAP.\nNumerical simulations and reconstruction results on synthetically generated but\nphoto-realistic images with ground truth validate that enabling refraction does\nnot compromise accuracy or robustness as compared to in-air reconstructions.\nFinally, we demonstrate the capability of our approach for large-scale\nrefractive scenarios using a dataset consisting of nearly 6000 images. The\nimplementation is released as open-source at:\nhttps://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.\n", "link": "http://arxiv.org/abs/2403.08640v3", "date": "2024-08-13", "relevancy": 2.8998, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5833}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&body=Title%3A%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited%0AAuthor%3A%20Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08640v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefractive%2520COLMAP%253A%2520Refractive%2520Structure-from-Motion%2520Revisited%26entry.906535625%3DMengkun%2520She%2520and%2520Felix%2520Seegr%25C3%25A4ber%2520and%2520David%2520Nakath%2520and%2520Kevin%2520K%25C3%25B6ser%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520complete%2520refractive%2520Structure-from-Motion%2520%2528RSfM%2529%250Aframework%2520for%2520underwater%25203D%2520reconstruction%2520using%2520refractive%2520camera%2520setups%2520%2528for%250Aboth%252C%2520flat-%2520and%2520dome-port%2520underwater%2520housings%2529.%2520Despite%2520notable%2520achievements%2520in%250Arefractive%2520multi-view%2520geometry%2520over%2520the%2520past%2520decade%252C%2520a%2520robust%252C%2520complete%2520and%250Apublicly%2520available%2520solution%2520for%2520such%2520tasks%2520is%2520not%2520available%2520at%2520present%252C%2520and%250Aoften%2520practical%2520applications%2520have%2520to%2520resort%2520to%2520approximating%2520refraction%2520effects%250Aby%2520the%2520intrinsic%2520%2528distortion%2529%2520parameters%2520of%2520a%2520pinhole%2520camera%2520model.%2520To%2520fill%250Athis%2520gap%252C%2520we%2520have%2520integrated%2520refraction%2520considerations%2520throughout%2520the%2520entire%250ASfM%2520process%2520within%2520the%2520state-of-the-art%252C%2520open-source%2520SfM%2520framework%2520COLMAP.%250ANumerical%2520simulations%2520and%2520reconstruction%2520results%2520on%2520synthetically%2520generated%2520but%250Aphoto-realistic%2520images%2520with%2520ground%2520truth%2520validate%2520that%2520enabling%2520refraction%2520does%250Anot%2520compromise%2520accuracy%2520or%2520robustness%2520as%2520compared%2520to%2520in-air%2520reconstructions.%250AFinally%252C%2520we%2520demonstrate%2520the%2520capability%2520of%2520our%2520approach%2520for%2520large-scale%250Arefractive%2520scenarios%2520using%2520a%2520dataset%2520consisting%2520of%2520nearly%25206000%2520images.%2520The%250Aimplementation%2520is%2520released%2520as%2520open-source%2520at%253A%250Ahttps%253A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08640v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&entry.906535625=Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08640v3&entry.124074799=Read"},
{"title": "MLAAN: Scaling Supervised Local Learning with Multilaminar Leap\n  Augmented Auxiliary Network", "author": "Yuming Zhang and Shouxin Zhang and Peizhe Wang and Feiyu Zhu and Dongzhi Guan and Junhao Su and Jiabin Liu and Changpeng Cai", "abstract": "  Deep neural networks (DNNs) typically employ an end-to-end (E2E) training\nparadigm which presents several challenges, including high GPU memory\nconsumption, inefficiency, and difficulties in model parallelization during\ntraining. Recent research has sought to address these issues, with one\npromising approach being local learning. This method involves partitioning the\nbackbone network into gradient-isolated modules and manually designing\nauxiliary networks to train these local modules. Existing methods often neglect\nthe interaction of information between local modules, leading to myopic issues\nand a performance gap compared to E2E training. To address these limitations,\nwe propose the Multilaminar Leap Augmented Auxiliary Network (MLAAN).\nSpecifically, MLAAN comprises Multilaminar Local Modules (MLM) and Leap\nAugmented Modules (LAM). MLM captures both local and global features through\nindependent and cascaded auxiliary networks, alleviating performance issues\ncaused by insufficient global features. However, overly simplistic auxiliary\nnetworks can impede MLM's ability to capture global information. To address\nthis, we further design LAM, an enhanced auxiliary network that uses the\nExponential Moving Average (EMA) method to facilitate information exchange\nbetween local modules, thereby mitigating the shortsightedness resulting from\ninadequate interaction. The synergy between MLM and LAM has demonstrated\nexcellent performance. Our experiments on the CIFAR-10, STL-10, SVHN, and\nImageNet datasets show that MLAAN can be seamlessly integrated into existing\nlocal learning frameworks, significantly enhancing their performance and even\nsurpassing end-to-end (E2E) training methods, while also reducing GPU memory\nconsumption.\n", "link": "http://arxiv.org/abs/2406.16633v3", "date": "2024-08-13", "relevancy": 2.8744, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5927}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&body=Title%3A%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network%0AAuthor%3A%20Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Junhao%20Su%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20typically%20employ%20an%20end-to-end%20%28E2E%29%20training%0Aparadigm%20which%20presents%20several%20challenges%2C%20including%20high%20GPU%20memory%0Aconsumption%2C%20inefficiency%2C%20and%20difficulties%20in%20model%20parallelization%20during%0Atraining.%20Recent%20research%20has%20sought%20to%20address%20these%20issues%2C%20with%20one%0Apromising%20approach%20being%20local%20learning.%20This%20method%20involves%20partitioning%20the%0Abackbone%20network%20into%20gradient-isolated%20modules%20and%20manually%20designing%0Aauxiliary%20networks%20to%20train%20these%20local%20modules.%20Existing%20methods%20often%20neglect%0Athe%20interaction%20of%20information%20between%20local%20modules%2C%20leading%20to%20myopic%20issues%0Aand%20a%20performance%20gap%20compared%20to%20E2E%20training.%20To%20address%20these%20limitations%2C%0Awe%20propose%20the%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%0ASpecifically%2C%20MLAAN%20comprises%20Multilaminar%20Local%20Modules%20%28MLM%29%20and%20Leap%0AAugmented%20Modules%20%28LAM%29.%20MLM%20captures%20both%20local%20and%20global%20features%20through%0Aindependent%20and%20cascaded%20auxiliary%20networks%2C%20alleviating%20performance%20issues%0Acaused%20by%20insufficient%20global%20features.%20However%2C%20overly%20simplistic%20auxiliary%0Anetworks%20can%20impede%20MLM%27s%20ability%20to%20capture%20global%20information.%20To%20address%0Athis%2C%20we%20further%20design%20LAM%2C%20an%20enhanced%20auxiliary%20network%20that%20uses%20the%0AExponential%20Moving%20Average%20%28EMA%29%20method%20to%20facilitate%20information%20exchange%0Abetween%20local%20modules%2C%20thereby%20mitigating%20the%20shortsightedness%20resulting%20from%0Ainadequate%20interaction.%20The%20synergy%20between%20MLM%20and%20LAM%20has%20demonstrated%0Aexcellent%20performance.%20Our%20experiments%20on%20the%20CIFAR-10%2C%20STL-10%2C%20SVHN%2C%20and%0AImageNet%20datasets%20show%20that%20MLAAN%20can%20be%20seamlessly%20integrated%20into%20existing%0Alocal%20learning%20frameworks%2C%20significantly%20enhancing%20their%20performance%20and%20even%0Asurpassing%20end-to-end%20%28E2E%29%20training%20methods%2C%20while%20also%20reducing%20GPU%20memory%0Aconsumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16633v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLAAN%253A%2520Scaling%2520Supervised%2520Local%2520Learning%2520with%2520Multilaminar%2520Leap%250A%2520%2520Augmented%2520Auxiliary%2520Network%26entry.906535625%3DYuming%2520Zhang%2520and%2520Shouxin%2520Zhang%2520and%2520Peizhe%2520Wang%2520and%2520Feiyu%2520Zhu%2520and%2520Dongzhi%2520Guan%2520and%2520Junhao%2520Su%2520and%2520Jiabin%2520Liu%2520and%2520Changpeng%2520Cai%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520typically%2520employ%2520an%2520end-to-end%2520%2528E2E%2529%2520training%250Aparadigm%2520which%2520presents%2520several%2520challenges%252C%2520including%2520high%2520GPU%2520memory%250Aconsumption%252C%2520inefficiency%252C%2520and%2520difficulties%2520in%2520model%2520parallelization%2520during%250Atraining.%2520Recent%2520research%2520has%2520sought%2520to%2520address%2520these%2520issues%252C%2520with%2520one%250Apromising%2520approach%2520being%2520local%2520learning.%2520This%2520method%2520involves%2520partitioning%2520the%250Abackbone%2520network%2520into%2520gradient-isolated%2520modules%2520and%2520manually%2520designing%250Aauxiliary%2520networks%2520to%2520train%2520these%2520local%2520modules.%2520Existing%2520methods%2520often%2520neglect%250Athe%2520interaction%2520of%2520information%2520between%2520local%2520modules%252C%2520leading%2520to%2520myopic%2520issues%250Aand%2520a%2520performance%2520gap%2520compared%2520to%2520E2E%2520training.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520the%2520Multilaminar%2520Leap%2520Augmented%2520Auxiliary%2520Network%2520%2528MLAAN%2529.%250ASpecifically%252C%2520MLAAN%2520comprises%2520Multilaminar%2520Local%2520Modules%2520%2528MLM%2529%2520and%2520Leap%250AAugmented%2520Modules%2520%2528LAM%2529.%2520MLM%2520captures%2520both%2520local%2520and%2520global%2520features%2520through%250Aindependent%2520and%2520cascaded%2520auxiliary%2520networks%252C%2520alleviating%2520performance%2520issues%250Acaused%2520by%2520insufficient%2520global%2520features.%2520However%252C%2520overly%2520simplistic%2520auxiliary%250Anetworks%2520can%2520impede%2520MLM%2527s%2520ability%2520to%2520capture%2520global%2520information.%2520To%2520address%250Athis%252C%2520we%2520further%2520design%2520LAM%252C%2520an%2520enhanced%2520auxiliary%2520network%2520that%2520uses%2520the%250AExponential%2520Moving%2520Average%2520%2528EMA%2529%2520method%2520to%2520facilitate%2520information%2520exchange%250Abetween%2520local%2520modules%252C%2520thereby%2520mitigating%2520the%2520shortsightedness%2520resulting%2520from%250Ainadequate%2520interaction.%2520The%2520synergy%2520between%2520MLM%2520and%2520LAM%2520has%2520demonstrated%250Aexcellent%2520performance.%2520Our%2520experiments%2520on%2520the%2520CIFAR-10%252C%2520STL-10%252C%2520SVHN%252C%2520and%250AImageNet%2520datasets%2520show%2520that%2520MLAAN%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%250Alocal%2520learning%2520frameworks%252C%2520significantly%2520enhancing%2520their%2520performance%2520and%2520even%250Asurpassing%2520end-to-end%2520%2528E2E%2529%2520training%2520methods%252C%2520while%2520also%2520reducing%2520GPU%2520memory%250Aconsumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16633v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&entry.906535625=Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Junhao%20Su%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20typically%20employ%20an%20end-to-end%20%28E2E%29%20training%0Aparadigm%20which%20presents%20several%20challenges%2C%20including%20high%20GPU%20memory%0Aconsumption%2C%20inefficiency%2C%20and%20difficulties%20in%20model%20parallelization%20during%0Atraining.%20Recent%20research%20has%20sought%20to%20address%20these%20issues%2C%20with%20one%0Apromising%20approach%20being%20local%20learning.%20This%20method%20involves%20partitioning%20the%0Abackbone%20network%20into%20gradient-isolated%20modules%20and%20manually%20designing%0Aauxiliary%20networks%20to%20train%20these%20local%20modules.%20Existing%20methods%20often%20neglect%0Athe%20interaction%20of%20information%20between%20local%20modules%2C%20leading%20to%20myopic%20issues%0Aand%20a%20performance%20gap%20compared%20to%20E2E%20training.%20To%20address%20these%20limitations%2C%0Awe%20propose%20the%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%0ASpecifically%2C%20MLAAN%20comprises%20Multilaminar%20Local%20Modules%20%28MLM%29%20and%20Leap%0AAugmented%20Modules%20%28LAM%29.%20MLM%20captures%20both%20local%20and%20global%20features%20through%0Aindependent%20and%20cascaded%20auxiliary%20networks%2C%20alleviating%20performance%20issues%0Acaused%20by%20insufficient%20global%20features.%20However%2C%20overly%20simplistic%20auxiliary%0Anetworks%20can%20impede%20MLM%27s%20ability%20to%20capture%20global%20information.%20To%20address%0Athis%2C%20we%20further%20design%20LAM%2C%20an%20enhanced%20auxiliary%20network%20that%20uses%20the%0AExponential%20Moving%20Average%20%28EMA%29%20method%20to%20facilitate%20information%20exchange%0Abetween%20local%20modules%2C%20thereby%20mitigating%20the%20shortsightedness%20resulting%20from%0Ainadequate%20interaction.%20The%20synergy%20between%20MLM%20and%20LAM%20has%20demonstrated%0Aexcellent%20performance.%20Our%20experiments%20on%20the%20CIFAR-10%2C%20STL-10%2C%20SVHN%2C%20and%0AImageNet%20datasets%20show%20that%20MLAAN%20can%20be%20seamlessly%20integrated%20into%20existing%0Alocal%20learning%20frameworks%2C%20significantly%20enhancing%20their%20performance%20and%20even%0Asurpassing%20end-to-end%20%28E2E%29%20training%20methods%2C%20while%20also%20reducing%20GPU%20memory%0Aconsumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16633v3&entry.124074799=Read"},
{"title": "Non-convex Pose Graph Optimization in SLAM via Proximal Linearized\n  Riemannian ADMM", "author": "Xin Chen and Chunfeng Cui and Deren Han and Liqun Qi", "abstract": "  Pose graph optimization (PGO) is a well-known technique for solving the\npose-based simultaneous localization and mapping (SLAM) problem. In this paper,\nwe represent the rotation and translation by a unit quaternion and a\nthree-dimensional vector, and propose a new PGO model based on the von\nMises-Fisher distribution. The constraints derived from the unit quaternions\nare spherical manifolds, and the projection onto the constraints can be\ncalculated by normalization. Then a proximal linearized Riemannian alternating\ndirection method of multipliers (PieADMM) is developed to solve the proposed\nmodel, which not only has low memory requirements, but also can update the\nposes in parallel. Furthermore, we establish the iteration complexity of\n$O(1/\\epsilon^{2})$ of PieADMM for finding an $\\epsilon$-stationary solution of\nour model. The efficiency of our proposed algorithm is demonstrated by\nnumerical experiments on two synthetic and four 3D SLAM benchmark datasets.\n", "link": "http://arxiv.org/abs/2404.18560v2", "date": "2024-08-13", "relevancy": 2.8353, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5957}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5609}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-convex%20Pose%20Graph%20Optimization%20in%20SLAM%20via%20Proximal%20Linearized%0A%20%20Riemannian%20ADMM&body=Title%3A%20Non-convex%20Pose%20Graph%20Optimization%20in%20SLAM%20via%20Proximal%20Linearized%0A%20%20Riemannian%20ADMM%0AAuthor%3A%20Xin%20Chen%20and%20Chunfeng%20Cui%20and%20Deren%20Han%20and%20Liqun%20Qi%0AAbstract%3A%20%20%20Pose%20graph%20optimization%20%28PGO%29%20is%20a%20well-known%20technique%20for%20solving%20the%0Apose-based%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20problem.%20In%20this%20paper%2C%0Awe%20represent%20the%20rotation%20and%20translation%20by%20a%20unit%20quaternion%20and%20a%0Athree-dimensional%20vector%2C%20and%20propose%20a%20new%20PGO%20model%20based%20on%20the%20von%0AMises-Fisher%20distribution.%20The%20constraints%20derived%20from%20the%20unit%20quaternions%0Aare%20spherical%20manifolds%2C%20and%20the%20projection%20onto%20the%20constraints%20can%20be%0Acalculated%20by%20normalization.%20Then%20a%20proximal%20linearized%20Riemannian%20alternating%0Adirection%20method%20of%20multipliers%20%28PieADMM%29%20is%20developed%20to%20solve%20the%20proposed%0Amodel%2C%20which%20not%20only%20has%20low%20memory%20requirements%2C%20but%20also%20can%20update%20the%0Aposes%20in%20parallel.%20Furthermore%2C%20we%20establish%20the%20iteration%20complexity%20of%0A%24O%281/%5Cepsilon%5E%7B2%7D%29%24%20of%20PieADMM%20for%20finding%20an%20%24%5Cepsilon%24-stationary%20solution%20of%0Aour%20model.%20The%20efficiency%20of%20our%20proposed%20algorithm%20is%20demonstrated%20by%0Anumerical%20experiments%20on%20two%20synthetic%20and%20four%203D%20SLAM%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18560v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-convex%2520Pose%2520Graph%2520Optimization%2520in%2520SLAM%2520via%2520Proximal%2520Linearized%250A%2520%2520Riemannian%2520ADMM%26entry.906535625%3DXin%2520Chen%2520and%2520Chunfeng%2520Cui%2520and%2520Deren%2520Han%2520and%2520Liqun%2520Qi%26entry.1292438233%3D%2520%2520Pose%2520graph%2520optimization%2520%2528PGO%2529%2520is%2520a%2520well-known%2520technique%2520for%2520solving%2520the%250Apose-based%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520problem.%2520In%2520this%2520paper%252C%250Awe%2520represent%2520the%2520rotation%2520and%2520translation%2520by%2520a%2520unit%2520quaternion%2520and%2520a%250Athree-dimensional%2520vector%252C%2520and%2520propose%2520a%2520new%2520PGO%2520model%2520based%2520on%2520the%2520von%250AMises-Fisher%2520distribution.%2520The%2520constraints%2520derived%2520from%2520the%2520unit%2520quaternions%250Aare%2520spherical%2520manifolds%252C%2520and%2520the%2520projection%2520onto%2520the%2520constraints%2520can%2520be%250Acalculated%2520by%2520normalization.%2520Then%2520a%2520proximal%2520linearized%2520Riemannian%2520alternating%250Adirection%2520method%2520of%2520multipliers%2520%2528PieADMM%2529%2520is%2520developed%2520to%2520solve%2520the%2520proposed%250Amodel%252C%2520which%2520not%2520only%2520has%2520low%2520memory%2520requirements%252C%2520but%2520also%2520can%2520update%2520the%250Aposes%2520in%2520parallel.%2520Furthermore%252C%2520we%2520establish%2520the%2520iteration%2520complexity%2520of%250A%2524O%25281/%255Cepsilon%255E%257B2%257D%2529%2524%2520of%2520PieADMM%2520for%2520finding%2520an%2520%2524%255Cepsilon%2524-stationary%2520solution%2520of%250Aour%2520model.%2520The%2520efficiency%2520of%2520our%2520proposed%2520algorithm%2520is%2520demonstrated%2520by%250Anumerical%2520experiments%2520on%2520two%2520synthetic%2520and%2520four%25203D%2520SLAM%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18560v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-convex%20Pose%20Graph%20Optimization%20in%20SLAM%20via%20Proximal%20Linearized%0A%20%20Riemannian%20ADMM&entry.906535625=Xin%20Chen%20and%20Chunfeng%20Cui%20and%20Deren%20Han%20and%20Liqun%20Qi&entry.1292438233=%20%20Pose%20graph%20optimization%20%28PGO%29%20is%20a%20well-known%20technique%20for%20solving%20the%0Apose-based%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20problem.%20In%20this%20paper%2C%0Awe%20represent%20the%20rotation%20and%20translation%20by%20a%20unit%20quaternion%20and%20a%0Athree-dimensional%20vector%2C%20and%20propose%20a%20new%20PGO%20model%20based%20on%20the%20von%0AMises-Fisher%20distribution.%20The%20constraints%20derived%20from%20the%20unit%20quaternions%0Aare%20spherical%20manifolds%2C%20and%20the%20projection%20onto%20the%20constraints%20can%20be%0Acalculated%20by%20normalization.%20Then%20a%20proximal%20linearized%20Riemannian%20alternating%0Adirection%20method%20of%20multipliers%20%28PieADMM%29%20is%20developed%20to%20solve%20the%20proposed%0Amodel%2C%20which%20not%20only%20has%20low%20memory%20requirements%2C%20but%20also%20can%20update%20the%0Aposes%20in%20parallel.%20Furthermore%2C%20we%20establish%20the%20iteration%20complexity%20of%0A%24O%281/%5Cepsilon%5E%7B2%7D%29%24%20of%20PieADMM%20for%20finding%20an%20%24%5Cepsilon%24-stationary%20solution%20of%0Aour%20model.%20The%20efficiency%20of%20our%20proposed%20algorithm%20is%20demonstrated%20by%0Anumerical%20experiments%20on%20two%20synthetic%20and%20four%203D%20SLAM%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18560v2&entry.124074799=Read"},
{"title": "PBIR-NIE: Glossy Object Capture under Non-Distant Lighting", "author": "Guangyan Cai and Fujun Luan and Milo\u0161 Ha\u0161an and Kai Zhang and Sai Bi and Zexiang Xu and Iliyan Georgiev and Shuang Zhao", "abstract": "  Glossy objects present a significant challenge for 3D reconstruction from\nmulti-view input images under natural lighting. In this paper, we introduce\nPBIR-NIE, an inverse rendering framework designed to holistically capture the\ngeometry, material attributes, and surrounding illumination of such objects. We\npropose a novel parallax-aware non-distant environment map as a lightweight and\nefficient lighting representation, accurately modeling the near-field\nbackground of the scene, which is commonly encountered in real-world capture\nsetups. This feature allows our framework to accommodate complex parallax\neffects beyond the capabilities of standard infinite-distance environment maps.\nOur method optimizes an underlying signed distance field (SDF) through\nphysics-based differentiable rendering, seamlessly connecting surface gradients\nbetween a triangle mesh and the SDF via neural implicit evolution (NIE). To\naddress the intricacies of highly glossy BRDFs in differentiable rendering, we\nintegrate the antithetic sampling algorithm to mitigate variance in the Monte\nCarlo gradient estimator. Consequently, our framework exhibits robust\ncapabilities in handling glossy object reconstruction, showcasing superior\nquality in geometry, relighting, and material estimation.\n", "link": "http://arxiv.org/abs/2408.06878v1", "date": "2024-08-13", "relevancy": 2.8035, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5884}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5518}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PBIR-NIE%3A%20Glossy%20Object%20Capture%20under%20Non-Distant%20Lighting&body=Title%3A%20PBIR-NIE%3A%20Glossy%20Object%20Capture%20under%20Non-Distant%20Lighting%0AAuthor%3A%20Guangyan%20Cai%20and%20Fujun%20Luan%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Zexiang%20Xu%20and%20Iliyan%20Georgiev%20and%20Shuang%20Zhao%0AAbstract%3A%20%20%20Glossy%20objects%20present%20a%20significant%20challenge%20for%203D%20reconstruction%20from%0Amulti-view%20input%20images%20under%20natural%20lighting.%20In%20this%20paper%2C%20we%20introduce%0APBIR-NIE%2C%20an%20inverse%20rendering%20framework%20designed%20to%20holistically%20capture%20the%0Ageometry%2C%20material%20attributes%2C%20and%20surrounding%20illumination%20of%20such%20objects.%20We%0Apropose%20a%20novel%20parallax-aware%20non-distant%20environment%20map%20as%20a%20lightweight%20and%0Aefficient%20lighting%20representation%2C%20accurately%20modeling%20the%20near-field%0Abackground%20of%20the%20scene%2C%20which%20is%20commonly%20encountered%20in%20real-world%20capture%0Asetups.%20This%20feature%20allows%20our%20framework%20to%20accommodate%20complex%20parallax%0Aeffects%20beyond%20the%20capabilities%20of%20standard%20infinite-distance%20environment%20maps.%0AOur%20method%20optimizes%20an%20underlying%20signed%20distance%20field%20%28SDF%29%20through%0Aphysics-based%20differentiable%20rendering%2C%20seamlessly%20connecting%20surface%20gradients%0Abetween%20a%20triangle%20mesh%20and%20the%20SDF%20via%20neural%20implicit%20evolution%20%28NIE%29.%20To%0Aaddress%20the%20intricacies%20of%20highly%20glossy%20BRDFs%20in%20differentiable%20rendering%2C%20we%0Aintegrate%20the%20antithetic%20sampling%20algorithm%20to%20mitigate%20variance%20in%20the%20Monte%0ACarlo%20gradient%20estimator.%20Consequently%2C%20our%20framework%20exhibits%20robust%0Acapabilities%20in%20handling%20glossy%20object%20reconstruction%2C%20showcasing%20superior%0Aquality%20in%20geometry%2C%20relighting%2C%20and%20material%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPBIR-NIE%253A%2520Glossy%2520Object%2520Capture%2520under%2520Non-Distant%2520Lighting%26entry.906535625%3DGuangyan%2520Cai%2520and%2520Fujun%2520Luan%2520and%2520Milo%25C5%25A1%2520Ha%25C5%25A1an%2520and%2520Kai%2520Zhang%2520and%2520Sai%2520Bi%2520and%2520Zexiang%2520Xu%2520and%2520Iliyan%2520Georgiev%2520and%2520Shuang%2520Zhao%26entry.1292438233%3D%2520%2520Glossy%2520objects%2520present%2520a%2520significant%2520challenge%2520for%25203D%2520reconstruction%2520from%250Amulti-view%2520input%2520images%2520under%2520natural%2520lighting.%2520In%2520this%2520paper%252C%2520we%2520introduce%250APBIR-NIE%252C%2520an%2520inverse%2520rendering%2520framework%2520designed%2520to%2520holistically%2520capture%2520the%250Ageometry%252C%2520material%2520attributes%252C%2520and%2520surrounding%2520illumination%2520of%2520such%2520objects.%2520We%250Apropose%2520a%2520novel%2520parallax-aware%2520non-distant%2520environment%2520map%2520as%2520a%2520lightweight%2520and%250Aefficient%2520lighting%2520representation%252C%2520accurately%2520modeling%2520the%2520near-field%250Abackground%2520of%2520the%2520scene%252C%2520which%2520is%2520commonly%2520encountered%2520in%2520real-world%2520capture%250Asetups.%2520This%2520feature%2520allows%2520our%2520framework%2520to%2520accommodate%2520complex%2520parallax%250Aeffects%2520beyond%2520the%2520capabilities%2520of%2520standard%2520infinite-distance%2520environment%2520maps.%250AOur%2520method%2520optimizes%2520an%2520underlying%2520signed%2520distance%2520field%2520%2528SDF%2529%2520through%250Aphysics-based%2520differentiable%2520rendering%252C%2520seamlessly%2520connecting%2520surface%2520gradients%250Abetween%2520a%2520triangle%2520mesh%2520and%2520the%2520SDF%2520via%2520neural%2520implicit%2520evolution%2520%2528NIE%2529.%2520To%250Aaddress%2520the%2520intricacies%2520of%2520highly%2520glossy%2520BRDFs%2520in%2520differentiable%2520rendering%252C%2520we%250Aintegrate%2520the%2520antithetic%2520sampling%2520algorithm%2520to%2520mitigate%2520variance%2520in%2520the%2520Monte%250ACarlo%2520gradient%2520estimator.%2520Consequently%252C%2520our%2520framework%2520exhibits%2520robust%250Acapabilities%2520in%2520handling%2520glossy%2520object%2520reconstruction%252C%2520showcasing%2520superior%250Aquality%2520in%2520geometry%252C%2520relighting%252C%2520and%2520material%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PBIR-NIE%3A%20Glossy%20Object%20Capture%20under%20Non-Distant%20Lighting&entry.906535625=Guangyan%20Cai%20and%20Fujun%20Luan%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Zexiang%20Xu%20and%20Iliyan%20Georgiev%20and%20Shuang%20Zhao&entry.1292438233=%20%20Glossy%20objects%20present%20a%20significant%20challenge%20for%203D%20reconstruction%20from%0Amulti-view%20input%20images%20under%20natural%20lighting.%20In%20this%20paper%2C%20we%20introduce%0APBIR-NIE%2C%20an%20inverse%20rendering%20framework%20designed%20to%20holistically%20capture%20the%0Ageometry%2C%20material%20attributes%2C%20and%20surrounding%20illumination%20of%20such%20objects.%20We%0Apropose%20a%20novel%20parallax-aware%20non-distant%20environment%20map%20as%20a%20lightweight%20and%0Aefficient%20lighting%20representation%2C%20accurately%20modeling%20the%20near-field%0Abackground%20of%20the%20scene%2C%20which%20is%20commonly%20encountered%20in%20real-world%20capture%0Asetups.%20This%20feature%20allows%20our%20framework%20to%20accommodate%20complex%20parallax%0Aeffects%20beyond%20the%20capabilities%20of%20standard%20infinite-distance%20environment%20maps.%0AOur%20method%20optimizes%20an%20underlying%20signed%20distance%20field%20%28SDF%29%20through%0Aphysics-based%20differentiable%20rendering%2C%20seamlessly%20connecting%20surface%20gradients%0Abetween%20a%20triangle%20mesh%20and%20the%20SDF%20via%20neural%20implicit%20evolution%20%28NIE%29.%20To%0Aaddress%20the%20intricacies%20of%20highly%20glossy%20BRDFs%20in%20differentiable%20rendering%2C%20we%0Aintegrate%20the%20antithetic%20sampling%20algorithm%20to%20mitigate%20variance%20in%20the%20Monte%0ACarlo%20gradient%20estimator.%20Consequently%2C%20our%20framework%20exhibits%20robust%0Acapabilities%20in%20handling%20glossy%20object%20reconstruction%2C%20showcasing%20superior%0Aquality%20in%20geometry%2C%20relighting%2C%20and%20material%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06878v1&entry.124074799=Read"},
{"title": "GLGait: A Global-Local Temporal Receptive Field Network for Gait\n  Recognition in the Wild", "author": "Guozhen Peng and Yunhong Wang and Yuwei Zhao and Shaoxiong Zhang and Annan Li", "abstract": "  Gait recognition has attracted increasing attention from academia and\nindustry as a human recognition technology from a distance in non-intrusive\nways without requiring cooperation. Although advanced methods have achieved\nimpressive success in lab scenarios, most of them perform poorly in the wild.\nRecently, some Convolution Neural Networks (ConvNets) based methods have been\nproposed to address the issue of gait recognition in the wild. However, the\ntemporal receptive field obtained by convolution operations is limited for long\ngait sequences. If directly replacing convolution blocks with visual\ntransformer blocks, the model may not enhance a local temporal receptive field,\nwhich is important for covering a complete gait cycle. To address this issue,\nwe design a Global-Local Temporal Receptive Field Network (GLGait). GLGait\nemploys a Global-Local Temporal Module (GLTM) to establish a global-local\ntemporal receptive field, which mainly consists of a Pseudo Global Temporal\nSelf-Attention (PGTA) and a temporal convolution operation. Specifically, PGTA\nis used to obtain a pseudo global temporal receptive field with less memory and\ncomputation complexity compared with a multi-head self-attention (MHSA). The\ntemporal convolution operation is used to enhance the local temporal receptive\nfield. Besides, it can also aggregate pseudo global temporal receptive field to\na true holistic temporal receptive field. Furthermore, we also propose a\nCenter-Augmented Triplet Loss (CTL) in GLGait to reduce the intra-class\ndistance and expand the positive samples in the training stage. Extensive\nexperiments show that our method obtains state-of-the-art results on\nin-the-wild datasets, $i.e.$, Gait3D and GREW. The code is available at\nhttps://github.com/bgdpgz/GLGait.\n", "link": "http://arxiv.org/abs/2408.06834v1", "date": "2024-08-13", "relevancy": 2.7663, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5608}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5603}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLGait%3A%20A%20Global-Local%20Temporal%20Receptive%20Field%20Network%20for%20Gait%0A%20%20Recognition%20in%20the%20Wild&body=Title%3A%20GLGait%3A%20A%20Global-Local%20Temporal%20Receptive%20Field%20Network%20for%20Gait%0A%20%20Recognition%20in%20the%20Wild%0AAuthor%3A%20Guozhen%20Peng%20and%20Yunhong%20Wang%20and%20Yuwei%20Zhao%20and%20Shaoxiong%20Zhang%20and%20Annan%20Li%0AAbstract%3A%20%20%20Gait%20recognition%20has%20attracted%20increasing%20attention%20from%20academia%20and%0Aindustry%20as%20a%20human%20recognition%20technology%20from%20a%20distance%20in%20non-intrusive%0Aways%20without%20requiring%20cooperation.%20Although%20advanced%20methods%20have%20achieved%0Aimpressive%20success%20in%20lab%20scenarios%2C%20most%20of%20them%20perform%20poorly%20in%20the%20wild.%0ARecently%2C%20some%20Convolution%20Neural%20Networks%20%28ConvNets%29%20based%20methods%20have%20been%0Aproposed%20to%20address%20the%20issue%20of%20gait%20recognition%20in%20the%20wild.%20However%2C%20the%0Atemporal%20receptive%20field%20obtained%20by%20convolution%20operations%20is%20limited%20for%20long%0Agait%20sequences.%20If%20directly%20replacing%20convolution%20blocks%20with%20visual%0Atransformer%20blocks%2C%20the%20model%20may%20not%20enhance%20a%20local%20temporal%20receptive%20field%2C%0Awhich%20is%20important%20for%20covering%20a%20complete%20gait%20cycle.%20To%20address%20this%20issue%2C%0Awe%20design%20a%20Global-Local%20Temporal%20Receptive%20Field%20Network%20%28GLGait%29.%20GLGait%0Aemploys%20a%20Global-Local%20Temporal%20Module%20%28GLTM%29%20to%20establish%20a%20global-local%0Atemporal%20receptive%20field%2C%20which%20mainly%20consists%20of%20a%20Pseudo%20Global%20Temporal%0ASelf-Attention%20%28PGTA%29%20and%20a%20temporal%20convolution%20operation.%20Specifically%2C%20PGTA%0Ais%20used%20to%20obtain%20a%20pseudo%20global%20temporal%20receptive%20field%20with%20less%20memory%20and%0Acomputation%20complexity%20compared%20with%20a%20multi-head%20self-attention%20%28MHSA%29.%20The%0Atemporal%20convolution%20operation%20is%20used%20to%20enhance%20the%20local%20temporal%20receptive%0Afield.%20Besides%2C%20it%20can%20also%20aggregate%20pseudo%20global%20temporal%20receptive%20field%20to%0Aa%20true%20holistic%20temporal%20receptive%20field.%20Furthermore%2C%20we%20also%20propose%20a%0ACenter-Augmented%20Triplet%20Loss%20%28CTL%29%20in%20GLGait%20to%20reduce%20the%20intra-class%0Adistance%20and%20expand%20the%20positive%20samples%20in%20the%20training%20stage.%20Extensive%0Aexperiments%20show%20that%20our%20method%20obtains%20state-of-the-art%20results%20on%0Ain-the-wild%20datasets%2C%20%24i.e.%24%2C%20Gait3D%20and%20GREW.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bgdpgz/GLGait.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLGait%253A%2520A%2520Global-Local%2520Temporal%2520Receptive%2520Field%2520Network%2520for%2520Gait%250A%2520%2520Recognition%2520in%2520the%2520Wild%26entry.906535625%3DGuozhen%2520Peng%2520and%2520Yunhong%2520Wang%2520and%2520Yuwei%2520Zhao%2520and%2520Shaoxiong%2520Zhang%2520and%2520Annan%2520Li%26entry.1292438233%3D%2520%2520Gait%2520recognition%2520has%2520attracted%2520increasing%2520attention%2520from%2520academia%2520and%250Aindustry%2520as%2520a%2520human%2520recognition%2520technology%2520from%2520a%2520distance%2520in%2520non-intrusive%250Aways%2520without%2520requiring%2520cooperation.%2520Although%2520advanced%2520methods%2520have%2520achieved%250Aimpressive%2520success%2520in%2520lab%2520scenarios%252C%2520most%2520of%2520them%2520perform%2520poorly%2520in%2520the%2520wild.%250ARecently%252C%2520some%2520Convolution%2520Neural%2520Networks%2520%2528ConvNets%2529%2520based%2520methods%2520have%2520been%250Aproposed%2520to%2520address%2520the%2520issue%2520of%2520gait%2520recognition%2520in%2520the%2520wild.%2520However%252C%2520the%250Atemporal%2520receptive%2520field%2520obtained%2520by%2520convolution%2520operations%2520is%2520limited%2520for%2520long%250Agait%2520sequences.%2520If%2520directly%2520replacing%2520convolution%2520blocks%2520with%2520visual%250Atransformer%2520blocks%252C%2520the%2520model%2520may%2520not%2520enhance%2520a%2520local%2520temporal%2520receptive%2520field%252C%250Awhich%2520is%2520important%2520for%2520covering%2520a%2520complete%2520gait%2520cycle.%2520To%2520address%2520this%2520issue%252C%250Awe%2520design%2520a%2520Global-Local%2520Temporal%2520Receptive%2520Field%2520Network%2520%2528GLGait%2529.%2520GLGait%250Aemploys%2520a%2520Global-Local%2520Temporal%2520Module%2520%2528GLTM%2529%2520to%2520establish%2520a%2520global-local%250Atemporal%2520receptive%2520field%252C%2520which%2520mainly%2520consists%2520of%2520a%2520Pseudo%2520Global%2520Temporal%250ASelf-Attention%2520%2528PGTA%2529%2520and%2520a%2520temporal%2520convolution%2520operation.%2520Specifically%252C%2520PGTA%250Ais%2520used%2520to%2520obtain%2520a%2520pseudo%2520global%2520temporal%2520receptive%2520field%2520with%2520less%2520memory%2520and%250Acomputation%2520complexity%2520compared%2520with%2520a%2520multi-head%2520self-attention%2520%2528MHSA%2529.%2520The%250Atemporal%2520convolution%2520operation%2520is%2520used%2520to%2520enhance%2520the%2520local%2520temporal%2520receptive%250Afield.%2520Besides%252C%2520it%2520can%2520also%2520aggregate%2520pseudo%2520global%2520temporal%2520receptive%2520field%2520to%250Aa%2520true%2520holistic%2520temporal%2520receptive%2520field.%2520Furthermore%252C%2520we%2520also%2520propose%2520a%250ACenter-Augmented%2520Triplet%2520Loss%2520%2528CTL%2529%2520in%2520GLGait%2520to%2520reduce%2520the%2520intra-class%250Adistance%2520and%2520expand%2520the%2520positive%2520samples%2520in%2520the%2520training%2520stage.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520obtains%2520state-of-the-art%2520results%2520on%250Ain-the-wild%2520datasets%252C%2520%2524i.e.%2524%252C%2520Gait3D%2520and%2520GREW.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/bgdpgz/GLGait.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLGait%3A%20A%20Global-Local%20Temporal%20Receptive%20Field%20Network%20for%20Gait%0A%20%20Recognition%20in%20the%20Wild&entry.906535625=Guozhen%20Peng%20and%20Yunhong%20Wang%20and%20Yuwei%20Zhao%20and%20Shaoxiong%20Zhang%20and%20Annan%20Li&entry.1292438233=%20%20Gait%20recognition%20has%20attracted%20increasing%20attention%20from%20academia%20and%0Aindustry%20as%20a%20human%20recognition%20technology%20from%20a%20distance%20in%20non-intrusive%0Aways%20without%20requiring%20cooperation.%20Although%20advanced%20methods%20have%20achieved%0Aimpressive%20success%20in%20lab%20scenarios%2C%20most%20of%20them%20perform%20poorly%20in%20the%20wild.%0ARecently%2C%20some%20Convolution%20Neural%20Networks%20%28ConvNets%29%20based%20methods%20have%20been%0Aproposed%20to%20address%20the%20issue%20of%20gait%20recognition%20in%20the%20wild.%20However%2C%20the%0Atemporal%20receptive%20field%20obtained%20by%20convolution%20operations%20is%20limited%20for%20long%0Agait%20sequences.%20If%20directly%20replacing%20convolution%20blocks%20with%20visual%0Atransformer%20blocks%2C%20the%20model%20may%20not%20enhance%20a%20local%20temporal%20receptive%20field%2C%0Awhich%20is%20important%20for%20covering%20a%20complete%20gait%20cycle.%20To%20address%20this%20issue%2C%0Awe%20design%20a%20Global-Local%20Temporal%20Receptive%20Field%20Network%20%28GLGait%29.%20GLGait%0Aemploys%20a%20Global-Local%20Temporal%20Module%20%28GLTM%29%20to%20establish%20a%20global-local%0Atemporal%20receptive%20field%2C%20which%20mainly%20consists%20of%20a%20Pseudo%20Global%20Temporal%0ASelf-Attention%20%28PGTA%29%20and%20a%20temporal%20convolution%20operation.%20Specifically%2C%20PGTA%0Ais%20used%20to%20obtain%20a%20pseudo%20global%20temporal%20receptive%20field%20with%20less%20memory%20and%0Acomputation%20complexity%20compared%20with%20a%20multi-head%20self-attention%20%28MHSA%29.%20The%0Atemporal%20convolution%20operation%20is%20used%20to%20enhance%20the%20local%20temporal%20receptive%0Afield.%20Besides%2C%20it%20can%20also%20aggregate%20pseudo%20global%20temporal%20receptive%20field%20to%0Aa%20true%20holistic%20temporal%20receptive%20field.%20Furthermore%2C%20we%20also%20propose%20a%0ACenter-Augmented%20Triplet%20Loss%20%28CTL%29%20in%20GLGait%20to%20reduce%20the%20intra-class%0Adistance%20and%20expand%20the%20positive%20samples%20in%20the%20training%20stage.%20Extensive%0Aexperiments%20show%20that%20our%20method%20obtains%20state-of-the-art%20results%20on%0Ain-the-wild%20datasets%2C%20%24i.e.%24%2C%20Gait3D%20and%20GREW.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bgdpgz/GLGait.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06834v1&entry.124074799=Read"},
{"title": "Multi-Agent Continuous Control with Generative Flow Networks", "author": "Shuang Luo and Yinchuan Li and Shunyu Liu and Xu Zhang and Yunfeng Shao and Chao Wu", "abstract": "  Generative Flow Networks (GFlowNets) aim to generate diverse trajectories\nfrom a distribution in which the final states of the trajectories are\nproportional to the reward, serving as a powerful alternative to reinforcement\nlearning for exploratory control tasks. However, the individual-flow matching\nconstraint in GFlowNets limits their applications for multi-agent systems,\nespecially continuous joint-control problems. In this paper, we propose a novel\nMulti-Agent generative Continuous Flow Networks (MACFN) method to enable\nmultiple agents to perform cooperative exploration for various compositional\ncontinuous objects. Technically, MACFN trains decentralized\nindividual-flow-based policies in a centralized global-flow-based matching\nfashion. During centralized training, MACFN introduces a continuous flow\ndecomposition network to deduce the flow contributions of each agent in the\npresence of only global rewards. Then agents can deliver actions solely based\non their assigned local flow in a decentralized way, forming a joint policy\ndistribution proportional to the rewards. To guarantee the expressiveness of\ncontinuous flow decomposition, we theoretically derive a consistency condition\non the decomposition network. Experimental results demonstrate that the\nproposed method yields results superior to the state-of-the-art counterparts\nand better exploration capability. Our code is available at\nhttps://github.com/isluoshuang/MACFN.\n", "link": "http://arxiv.org/abs/2408.06920v1", "date": "2024-08-13", "relevancy": 2.679, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5536}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Continuous%20Control%20with%20Generative%20Flow%20Networks&body=Title%3A%20Multi-Agent%20Continuous%20Control%20with%20Generative%20Flow%20Networks%0AAuthor%3A%20Shuang%20Luo%20and%20Yinchuan%20Li%20and%20Shunyu%20Liu%20and%20Xu%20Zhang%20and%20Yunfeng%20Shao%20and%20Chao%20Wu%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20aim%20to%20generate%20diverse%20trajectories%0Afrom%20a%20distribution%20in%20which%20the%20final%20states%20of%20the%20trajectories%20are%0Aproportional%20to%20the%20reward%2C%20serving%20as%20a%20powerful%20alternative%20to%20reinforcement%0Alearning%20for%20exploratory%20control%20tasks.%20However%2C%20the%20individual-flow%20matching%0Aconstraint%20in%20GFlowNets%20limits%20their%20applications%20for%20multi-agent%20systems%2C%0Aespecially%20continuous%20joint-control%20problems.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AMulti-Agent%20generative%20Continuous%20Flow%20Networks%20%28MACFN%29%20method%20to%20enable%0Amultiple%20agents%20to%20perform%20cooperative%20exploration%20for%20various%20compositional%0Acontinuous%20objects.%20Technically%2C%20MACFN%20trains%20decentralized%0Aindividual-flow-based%20policies%20in%20a%20centralized%20global-flow-based%20matching%0Afashion.%20During%20centralized%20training%2C%20MACFN%20introduces%20a%20continuous%20flow%0Adecomposition%20network%20to%20deduce%20the%20flow%20contributions%20of%20each%20agent%20in%20the%0Apresence%20of%20only%20global%20rewards.%20Then%20agents%20can%20deliver%20actions%20solely%20based%0Aon%20their%20assigned%20local%20flow%20in%20a%20decentralized%20way%2C%20forming%20a%20joint%20policy%0Adistribution%20proportional%20to%20the%20rewards.%20To%20guarantee%20the%20expressiveness%20of%0Acontinuous%20flow%20decomposition%2C%20we%20theoretically%20derive%20a%20consistency%20condition%0Aon%20the%20decomposition%20network.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20yields%20results%20superior%20to%20the%20state-of-the-art%20counterparts%0Aand%20better%20exploration%20capability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/isluoshuang/MACFN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Continuous%2520Control%2520with%2520Generative%2520Flow%2520Networks%26entry.906535625%3DShuang%2520Luo%2520and%2520Yinchuan%2520Li%2520and%2520Shunyu%2520Liu%2520and%2520Xu%2520Zhang%2520and%2520Yunfeng%2520Shao%2520and%2520Chao%2520Wu%26entry.1292438233%3D%2520%2520Generative%2520Flow%2520Networks%2520%2528GFlowNets%2529%2520aim%2520to%2520generate%2520diverse%2520trajectories%250Afrom%2520a%2520distribution%2520in%2520which%2520the%2520final%2520states%2520of%2520the%2520trajectories%2520are%250Aproportional%2520to%2520the%2520reward%252C%2520serving%2520as%2520a%2520powerful%2520alternative%2520to%2520reinforcement%250Alearning%2520for%2520exploratory%2520control%2520tasks.%2520However%252C%2520the%2520individual-flow%2520matching%250Aconstraint%2520in%2520GFlowNets%2520limits%2520their%2520applications%2520for%2520multi-agent%2520systems%252C%250Aespecially%2520continuous%2520joint-control%2520problems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250AMulti-Agent%2520generative%2520Continuous%2520Flow%2520Networks%2520%2528MACFN%2529%2520method%2520to%2520enable%250Amultiple%2520agents%2520to%2520perform%2520cooperative%2520exploration%2520for%2520various%2520compositional%250Acontinuous%2520objects.%2520Technically%252C%2520MACFN%2520trains%2520decentralized%250Aindividual-flow-based%2520policies%2520in%2520a%2520centralized%2520global-flow-based%2520matching%250Afashion.%2520During%2520centralized%2520training%252C%2520MACFN%2520introduces%2520a%2520continuous%2520flow%250Adecomposition%2520network%2520to%2520deduce%2520the%2520flow%2520contributions%2520of%2520each%2520agent%2520in%2520the%250Apresence%2520of%2520only%2520global%2520rewards.%2520Then%2520agents%2520can%2520deliver%2520actions%2520solely%2520based%250Aon%2520their%2520assigned%2520local%2520flow%2520in%2520a%2520decentralized%2520way%252C%2520forming%2520a%2520joint%2520policy%250Adistribution%2520proportional%2520to%2520the%2520rewards.%2520To%2520guarantee%2520the%2520expressiveness%2520of%250Acontinuous%2520flow%2520decomposition%252C%2520we%2520theoretically%2520derive%2520a%2520consistency%2520condition%250Aon%2520the%2520decomposition%2520network.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520yields%2520results%2520superior%2520to%2520the%2520state-of-the-art%2520counterparts%250Aand%2520better%2520exploration%2520capability.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/isluoshuang/MACFN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Continuous%20Control%20with%20Generative%20Flow%20Networks&entry.906535625=Shuang%20Luo%20and%20Yinchuan%20Li%20and%20Shunyu%20Liu%20and%20Xu%20Zhang%20and%20Yunfeng%20Shao%20and%20Chao%20Wu&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20aim%20to%20generate%20diverse%20trajectories%0Afrom%20a%20distribution%20in%20which%20the%20final%20states%20of%20the%20trajectories%20are%0Aproportional%20to%20the%20reward%2C%20serving%20as%20a%20powerful%20alternative%20to%20reinforcement%0Alearning%20for%20exploratory%20control%20tasks.%20However%2C%20the%20individual-flow%20matching%0Aconstraint%20in%20GFlowNets%20limits%20their%20applications%20for%20multi-agent%20systems%2C%0Aespecially%20continuous%20joint-control%20problems.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AMulti-Agent%20generative%20Continuous%20Flow%20Networks%20%28MACFN%29%20method%20to%20enable%0Amultiple%20agents%20to%20perform%20cooperative%20exploration%20for%20various%20compositional%0Acontinuous%20objects.%20Technically%2C%20MACFN%20trains%20decentralized%0Aindividual-flow-based%20policies%20in%20a%20centralized%20global-flow-based%20matching%0Afashion.%20During%20centralized%20training%2C%20MACFN%20introduces%20a%20continuous%20flow%0Adecomposition%20network%20to%20deduce%20the%20flow%20contributions%20of%20each%20agent%20in%20the%0Apresence%20of%20only%20global%20rewards.%20Then%20agents%20can%20deliver%20actions%20solely%20based%0Aon%20their%20assigned%20local%20flow%20in%20a%20decentralized%20way%2C%20forming%20a%20joint%20policy%0Adistribution%20proportional%20to%20the%20rewards.%20To%20guarantee%20the%20expressiveness%20of%0Acontinuous%20flow%20decomposition%2C%20we%20theoretically%20derive%20a%20consistency%20condition%0Aon%20the%20decomposition%20network.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20yields%20results%20superior%20to%20the%20state-of-the-art%20counterparts%0Aand%20better%20exploration%20capability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/isluoshuang/MACFN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06920v1&entry.124074799=Read"},
{"title": "FlatFusion: Delving into Details of Sparse Transformer-based\n  Camera-LiDAR Fusion for Autonomous Driving", "author": "Yutao Zhu and Xiaosong Jia and Xinyu Yang and Junchi Yan", "abstract": "  The integration of data from diverse sensor modalities (e.g., camera and\nLiDAR) constitutes a prevalent methodology within the ambit of autonomous\ndriving scenarios. Recent advancements in efficient point cloud transformers\nhave underscored the efficacy of integrating information in sparse formats.\nWhen it comes to fusion, since image patches are dense in pixel space with\nambiguous depth, it necessitates additional design considerations for effective\nfusion. In this paper, we conduct a comprehensive exploration of design choices\nfor Transformer-based sparse cameraLiDAR fusion. This investigation encompasses\nstrategies for image-to-3D and LiDAR-to-2D mapping, attention neighbor\ngrouping, single modal tokenizer, and micro-structure of Transformer. By\namalgamating the most effective principles uncovered through our investigation,\nwe introduce FlatFusion, a carefully designed framework for sparse camera-LiDAR\nfusion. Notably, FlatFusion significantly outperforms state-of-the-art sparse\nTransformer-based methods, including UniTR, CMT, and SparseFusion, achieving\n73.7 NDS on the nuScenes validation set with 10.1 FPS with PyTorch.\n", "link": "http://arxiv.org/abs/2408.06832v1", "date": "2024-08-13", "relevancy": 2.677, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlatFusion%3A%20Delving%20into%20Details%20of%20Sparse%20Transformer-based%0A%20%20Camera-LiDAR%20Fusion%20for%20Autonomous%20Driving&body=Title%3A%20FlatFusion%3A%20Delving%20into%20Details%20of%20Sparse%20Transformer-based%0A%20%20Camera-LiDAR%20Fusion%20for%20Autonomous%20Driving%0AAuthor%3A%20Yutao%20Zhu%20and%20Xiaosong%20Jia%20and%20Xinyu%20Yang%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20The%20integration%20of%20data%20from%20diverse%20sensor%20modalities%20%28e.g.%2C%20camera%20and%0ALiDAR%29%20constitutes%20a%20prevalent%20methodology%20within%20the%20ambit%20of%20autonomous%0Adriving%20scenarios.%20Recent%20advancements%20in%20efficient%20point%20cloud%20transformers%0Ahave%20underscored%20the%20efficacy%20of%20integrating%20information%20in%20sparse%20formats.%0AWhen%20it%20comes%20to%20fusion%2C%20since%20image%20patches%20are%20dense%20in%20pixel%20space%20with%0Aambiguous%20depth%2C%20it%20necessitates%20additional%20design%20considerations%20for%20effective%0Afusion.%20In%20this%20paper%2C%20we%20conduct%20a%20comprehensive%20exploration%20of%20design%20choices%0Afor%20Transformer-based%20sparse%20cameraLiDAR%20fusion.%20This%20investigation%20encompasses%0Astrategies%20for%20image-to-3D%20and%20LiDAR-to-2D%20mapping%2C%20attention%20neighbor%0Agrouping%2C%20single%20modal%20tokenizer%2C%20and%20micro-structure%20of%20Transformer.%20By%0Aamalgamating%20the%20most%20effective%20principles%20uncovered%20through%20our%20investigation%2C%0Awe%20introduce%20FlatFusion%2C%20a%20carefully%20designed%20framework%20for%20sparse%20camera-LiDAR%0Afusion.%20Notably%2C%20FlatFusion%20significantly%20outperforms%20state-of-the-art%20sparse%0ATransformer-based%20methods%2C%20including%20UniTR%2C%20CMT%2C%20and%20SparseFusion%2C%20achieving%0A73.7%20NDS%20on%20the%20nuScenes%20validation%20set%20with%2010.1%20FPS%20with%20PyTorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlatFusion%253A%2520Delving%2520into%2520Details%2520of%2520Sparse%2520Transformer-based%250A%2520%2520Camera-LiDAR%2520Fusion%2520for%2520Autonomous%2520Driving%26entry.906535625%3DYutao%2520Zhu%2520and%2520Xiaosong%2520Jia%2520and%2520Xinyu%2520Yang%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520data%2520from%2520diverse%2520sensor%2520modalities%2520%2528e.g.%252C%2520camera%2520and%250ALiDAR%2529%2520constitutes%2520a%2520prevalent%2520methodology%2520within%2520the%2520ambit%2520of%2520autonomous%250Adriving%2520scenarios.%2520Recent%2520advancements%2520in%2520efficient%2520point%2520cloud%2520transformers%250Ahave%2520underscored%2520the%2520efficacy%2520of%2520integrating%2520information%2520in%2520sparse%2520formats.%250AWhen%2520it%2520comes%2520to%2520fusion%252C%2520since%2520image%2520patches%2520are%2520dense%2520in%2520pixel%2520space%2520with%250Aambiguous%2520depth%252C%2520it%2520necessitates%2520additional%2520design%2520considerations%2520for%2520effective%250Afusion.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520comprehensive%2520exploration%2520of%2520design%2520choices%250Afor%2520Transformer-based%2520sparse%2520cameraLiDAR%2520fusion.%2520This%2520investigation%2520encompasses%250Astrategies%2520for%2520image-to-3D%2520and%2520LiDAR-to-2D%2520mapping%252C%2520attention%2520neighbor%250Agrouping%252C%2520single%2520modal%2520tokenizer%252C%2520and%2520micro-structure%2520of%2520Transformer.%2520By%250Aamalgamating%2520the%2520most%2520effective%2520principles%2520uncovered%2520through%2520our%2520investigation%252C%250Awe%2520introduce%2520FlatFusion%252C%2520a%2520carefully%2520designed%2520framework%2520for%2520sparse%2520camera-LiDAR%250Afusion.%2520Notably%252C%2520FlatFusion%2520significantly%2520outperforms%2520state-of-the-art%2520sparse%250ATransformer-based%2520methods%252C%2520including%2520UniTR%252C%2520CMT%252C%2520and%2520SparseFusion%252C%2520achieving%250A73.7%2520NDS%2520on%2520the%2520nuScenes%2520validation%2520set%2520with%252010.1%2520FPS%2520with%2520PyTorch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlatFusion%3A%20Delving%20into%20Details%20of%20Sparse%20Transformer-based%0A%20%20Camera-LiDAR%20Fusion%20for%20Autonomous%20Driving&entry.906535625=Yutao%20Zhu%20and%20Xiaosong%20Jia%20and%20Xinyu%20Yang%20and%20Junchi%20Yan&entry.1292438233=%20%20The%20integration%20of%20data%20from%20diverse%20sensor%20modalities%20%28e.g.%2C%20camera%20and%0ALiDAR%29%20constitutes%20a%20prevalent%20methodology%20within%20the%20ambit%20of%20autonomous%0Adriving%20scenarios.%20Recent%20advancements%20in%20efficient%20point%20cloud%20transformers%0Ahave%20underscored%20the%20efficacy%20of%20integrating%20information%20in%20sparse%20formats.%0AWhen%20it%20comes%20to%20fusion%2C%20since%20image%20patches%20are%20dense%20in%20pixel%20space%20with%0Aambiguous%20depth%2C%20it%20necessitates%20additional%20design%20considerations%20for%20effective%0Afusion.%20In%20this%20paper%2C%20we%20conduct%20a%20comprehensive%20exploration%20of%20design%20choices%0Afor%20Transformer-based%20sparse%20cameraLiDAR%20fusion.%20This%20investigation%20encompasses%0Astrategies%20for%20image-to-3D%20and%20LiDAR-to-2D%20mapping%2C%20attention%20neighbor%0Agrouping%2C%20single%20modal%20tokenizer%2C%20and%20micro-structure%20of%20Transformer.%20By%0Aamalgamating%20the%20most%20effective%20principles%20uncovered%20through%20our%20investigation%2C%0Awe%20introduce%20FlatFusion%2C%20a%20carefully%20designed%20framework%20for%20sparse%20camera-LiDAR%0Afusion.%20Notably%2C%20FlatFusion%20significantly%20outperforms%20state-of-the-art%20sparse%0ATransformer-based%20methods%2C%20including%20UniTR%2C%20CMT%2C%20and%20SparseFusion%2C%20achieving%0A73.7%20NDS%20on%20the%20nuScenes%20validation%20set%20with%2010.1%20FPS%20with%20PyTorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06832v1&entry.124074799=Read"},
{"title": "Neural Architecture Search based Global-local Vision Mamba for Palm-Vein\n  Recognition", "author": "Huafeng Qin and Yuming Fu and Jing Chen and Mounim A. El-Yacoubi and Xinbo Gao and Jun Wang", "abstract": "  Due to the advantages such as high security, high privacy, and liveness\nrecognition, vein recognition has been received more and more attention in past\nyears. Recently, deep learning models, e.g., Mamba has shown robust feature\nrepresentation with linear computational complexity and successfully applied\nfor visual tasks. However, vision Manba can capture long-distance feature\ndependencies but unfortunately deteriorate local feature details. Besides,\nmanually designing a Mamba architecture based on human priori knowledge is very\ntime-consuming and error-prone. In this paper, first, we propose a hybrid\nnetwork structure named Global-local Vision Mamba (GLVM), to learn the local\ncorrelations in images explicitly and global dependencies among tokens for vein\nfeature representation. Secondly, we design a Multi-head Mamba to learn the\ndependencies along different directions, so as to improve the feature\nrepresentation ability of vision Mamba. Thirdly, to learn the complementary\nfeatures, we propose a ConvMamba block consisting of three branches, named\nMulti-head Mamba branch (MHMamba), Feature Iteration Unit branch (FIU), and\nConvolutional Neural Network (CNN) branch, where the Feature Iteration Unit\nbranch aims to fuse convolutional local features with Mamba-based global\nrepresentations. Finally, a Globallocal Alternate Neural Architecture Search\n(GLNAS) method is proposed to search the optimal architecture of GLVM\nalternately with the evolutionary algorithm, thereby improving the recognition\nperformance for vein recognition tasks. We conduct rigorous experiments on\nthree public palm-vein databases to estimate the performance. The experimental\nresults demonstrate that the proposed method outperforms the representative\napproaches and achieves state-of-the-art recognition accuracy.\n", "link": "http://arxiv.org/abs/2408.05743v2", "date": "2024-08-13", "relevancy": 2.644, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5484}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Architecture%20Search%20based%20Global-local%20Vision%20Mamba%20for%20Palm-Vein%0A%20%20Recognition&body=Title%3A%20Neural%20Architecture%20Search%20based%20Global-local%20Vision%20Mamba%20for%20Palm-Vein%0A%20%20Recognition%0AAuthor%3A%20Huafeng%20Qin%20and%20Yuming%20Fu%20and%20Jing%20Chen%20and%20Mounim%20A.%20El-Yacoubi%20and%20Xinbo%20Gao%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Due%20to%20the%20advantages%20such%20as%20high%20security%2C%20high%20privacy%2C%20and%20liveness%0Arecognition%2C%20vein%20recognition%20has%20been%20received%20more%20and%20more%20attention%20in%20past%0Ayears.%20Recently%2C%20deep%20learning%20models%2C%20e.g.%2C%20Mamba%20has%20shown%20robust%20feature%0Arepresentation%20with%20linear%20computational%20complexity%20and%20successfully%20applied%0Afor%20visual%20tasks.%20However%2C%20vision%20Manba%20can%20capture%20long-distance%20feature%0Adependencies%20but%20unfortunately%20deteriorate%20local%20feature%20details.%20Besides%2C%0Amanually%20designing%20a%20Mamba%20architecture%20based%20on%20human%20priori%20knowledge%20is%20very%0Atime-consuming%20and%20error-prone.%20In%20this%20paper%2C%20first%2C%20we%20propose%20a%20hybrid%0Anetwork%20structure%20named%20Global-local%20Vision%20Mamba%20%28GLVM%29%2C%20to%20learn%20the%20local%0Acorrelations%20in%20images%20explicitly%20and%20global%20dependencies%20among%20tokens%20for%20vein%0Afeature%20representation.%20Secondly%2C%20we%20design%20a%20Multi-head%20Mamba%20to%20learn%20the%0Adependencies%20along%20different%20directions%2C%20so%20as%20to%20improve%20the%20feature%0Arepresentation%20ability%20of%20vision%20Mamba.%20Thirdly%2C%20to%20learn%20the%20complementary%0Afeatures%2C%20we%20propose%20a%20ConvMamba%20block%20consisting%20of%20three%20branches%2C%20named%0AMulti-head%20Mamba%20branch%20%28MHMamba%29%2C%20Feature%20Iteration%20Unit%20branch%20%28FIU%29%2C%20and%0AConvolutional%20Neural%20Network%20%28CNN%29%20branch%2C%20where%20the%20Feature%20Iteration%20Unit%0Abranch%20aims%20to%20fuse%20convolutional%20local%20features%20with%20Mamba-based%20global%0Arepresentations.%20Finally%2C%20a%20Globallocal%20Alternate%20Neural%20Architecture%20Search%0A%28GLNAS%29%20method%20is%20proposed%20to%20search%20the%20optimal%20architecture%20of%20GLVM%0Aalternately%20with%20the%20evolutionary%20algorithm%2C%20thereby%20improving%20the%20recognition%0Aperformance%20for%20vein%20recognition%20tasks.%20We%20conduct%20rigorous%20experiments%20on%0Athree%20public%20palm-vein%20databases%20to%20estimate%20the%20performance.%20The%20experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%20representative%0Aapproaches%20and%20achieves%20state-of-the-art%20recognition%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05743v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Architecture%2520Search%2520based%2520Global-local%2520Vision%2520Mamba%2520for%2520Palm-Vein%250A%2520%2520Recognition%26entry.906535625%3DHuafeng%2520Qin%2520and%2520Yuming%2520Fu%2520and%2520Jing%2520Chen%2520and%2520Mounim%2520A.%2520El-Yacoubi%2520and%2520Xinbo%2520Gao%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520advantages%2520such%2520as%2520high%2520security%252C%2520high%2520privacy%252C%2520and%2520liveness%250Arecognition%252C%2520vein%2520recognition%2520has%2520been%2520received%2520more%2520and%2520more%2520attention%2520in%2520past%250Ayears.%2520Recently%252C%2520deep%2520learning%2520models%252C%2520e.g.%252C%2520Mamba%2520has%2520shown%2520robust%2520feature%250Arepresentation%2520with%2520linear%2520computational%2520complexity%2520and%2520successfully%2520applied%250Afor%2520visual%2520tasks.%2520However%252C%2520vision%2520Manba%2520can%2520capture%2520long-distance%2520feature%250Adependencies%2520but%2520unfortunately%2520deteriorate%2520local%2520feature%2520details.%2520Besides%252C%250Amanually%2520designing%2520a%2520Mamba%2520architecture%2520based%2520on%2520human%2520priori%2520knowledge%2520is%2520very%250Atime-consuming%2520and%2520error-prone.%2520In%2520this%2520paper%252C%2520first%252C%2520we%2520propose%2520a%2520hybrid%250Anetwork%2520structure%2520named%2520Global-local%2520Vision%2520Mamba%2520%2528GLVM%2529%252C%2520to%2520learn%2520the%2520local%250Acorrelations%2520in%2520images%2520explicitly%2520and%2520global%2520dependencies%2520among%2520tokens%2520for%2520vein%250Afeature%2520representation.%2520Secondly%252C%2520we%2520design%2520a%2520Multi-head%2520Mamba%2520to%2520learn%2520the%250Adependencies%2520along%2520different%2520directions%252C%2520so%2520as%2520to%2520improve%2520the%2520feature%250Arepresentation%2520ability%2520of%2520vision%2520Mamba.%2520Thirdly%252C%2520to%2520learn%2520the%2520complementary%250Afeatures%252C%2520we%2520propose%2520a%2520ConvMamba%2520block%2520consisting%2520of%2520three%2520branches%252C%2520named%250AMulti-head%2520Mamba%2520branch%2520%2528MHMamba%2529%252C%2520Feature%2520Iteration%2520Unit%2520branch%2520%2528FIU%2529%252C%2520and%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529%2520branch%252C%2520where%2520the%2520Feature%2520Iteration%2520Unit%250Abranch%2520aims%2520to%2520fuse%2520convolutional%2520local%2520features%2520with%2520Mamba-based%2520global%250Arepresentations.%2520Finally%252C%2520a%2520Globallocal%2520Alternate%2520Neural%2520Architecture%2520Search%250A%2528GLNAS%2529%2520method%2520is%2520proposed%2520to%2520search%2520the%2520optimal%2520architecture%2520of%2520GLVM%250Aalternately%2520with%2520the%2520evolutionary%2520algorithm%252C%2520thereby%2520improving%2520the%2520recognition%250Aperformance%2520for%2520vein%2520recognition%2520tasks.%2520We%2520conduct%2520rigorous%2520experiments%2520on%250Athree%2520public%2520palm-vein%2520databases%2520to%2520estimate%2520the%2520performance.%2520The%2520experimental%250Aresults%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%2520representative%250Aapproaches%2520and%2520achieves%2520state-of-the-art%2520recognition%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05743v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Architecture%20Search%20based%20Global-local%20Vision%20Mamba%20for%20Palm-Vein%0A%20%20Recognition&entry.906535625=Huafeng%20Qin%20and%20Yuming%20Fu%20and%20Jing%20Chen%20and%20Mounim%20A.%20El-Yacoubi%20and%20Xinbo%20Gao%20and%20Jun%20Wang&entry.1292438233=%20%20Due%20to%20the%20advantages%20such%20as%20high%20security%2C%20high%20privacy%2C%20and%20liveness%0Arecognition%2C%20vein%20recognition%20has%20been%20received%20more%20and%20more%20attention%20in%20past%0Ayears.%20Recently%2C%20deep%20learning%20models%2C%20e.g.%2C%20Mamba%20has%20shown%20robust%20feature%0Arepresentation%20with%20linear%20computational%20complexity%20and%20successfully%20applied%0Afor%20visual%20tasks.%20However%2C%20vision%20Manba%20can%20capture%20long-distance%20feature%0Adependencies%20but%20unfortunately%20deteriorate%20local%20feature%20details.%20Besides%2C%0Amanually%20designing%20a%20Mamba%20architecture%20based%20on%20human%20priori%20knowledge%20is%20very%0Atime-consuming%20and%20error-prone.%20In%20this%20paper%2C%20first%2C%20we%20propose%20a%20hybrid%0Anetwork%20structure%20named%20Global-local%20Vision%20Mamba%20%28GLVM%29%2C%20to%20learn%20the%20local%0Acorrelations%20in%20images%20explicitly%20and%20global%20dependencies%20among%20tokens%20for%20vein%0Afeature%20representation.%20Secondly%2C%20we%20design%20a%20Multi-head%20Mamba%20to%20learn%20the%0Adependencies%20along%20different%20directions%2C%20so%20as%20to%20improve%20the%20feature%0Arepresentation%20ability%20of%20vision%20Mamba.%20Thirdly%2C%20to%20learn%20the%20complementary%0Afeatures%2C%20we%20propose%20a%20ConvMamba%20block%20consisting%20of%20three%20branches%2C%20named%0AMulti-head%20Mamba%20branch%20%28MHMamba%29%2C%20Feature%20Iteration%20Unit%20branch%20%28FIU%29%2C%20and%0AConvolutional%20Neural%20Network%20%28CNN%29%20branch%2C%20where%20the%20Feature%20Iteration%20Unit%0Abranch%20aims%20to%20fuse%20convolutional%20local%20features%20with%20Mamba-based%20global%0Arepresentations.%20Finally%2C%20a%20Globallocal%20Alternate%20Neural%20Architecture%20Search%0A%28GLNAS%29%20method%20is%20proposed%20to%20search%20the%20optimal%20architecture%20of%20GLVM%0Aalternately%20with%20the%20evolutionary%20algorithm%2C%20thereby%20improving%20the%20recognition%0Aperformance%20for%20vein%20recognition%20tasks.%20We%20conduct%20rigorous%20experiments%20on%0Athree%20public%20palm-vein%20databases%20to%20estimate%20the%20performance.%20The%20experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%20representative%0Aapproaches%20and%20achieves%20state-of-the-art%20recognition%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05743v2&entry.124074799=Read"},
{"title": "The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye\n  Movement, Odometry, and Egocentric Video", "author": "Michelle R. Greene and Benjamin J. Balas and Mark D. Lescroart and Paul R. MacNeilage and Jennifer A. Hart and Kamran Binaee and Peter A. Hausamann and Ronald Mezile and Bharath Shankar and Christian B. Sinnott and Kaylie Capurro and Savannah Halow and Hunter Howe and Mariam Josyula and Annie Li and Abraham Mieses and Amina Mohamed and Ilya Nudnou and Ezra Parkhill and Peter Riley and Brett Schmidt and Matthew W. Shinkle and Wentao Si and Brian Szekely and Joaquin M. Torres and Eliana Weissmann", "abstract": "  We introduce the Visual Experience Dataset (VEDB), a compilation of over 240\nhours of egocentric video combined with gaze- and head-tracking data that\noffers an unprecedented view of the visual world as experienced by human\nobservers. The dataset consists of 717 sessions, recorded by 58 observers\nranging from 6-49 years old. This paper outlines the data collection,\nprocessing, and labeling protocols undertaken to ensure a representative sample\nand discusses the potential sources of error or bias within the dataset. The\nVEDB's potential applications are vast, including improving gaze tracking\nmethodologies, assessing spatiotemporal image statistics, and refining deep\nneural networks for scene and activity recognition. The VEDB is accessible\nthrough established open science platforms and is intended to be a living\ndataset with plans for expansion and community contributions. It is released\nwith an emphasis on ethical considerations, such as participant privacy and the\nmitigation of potential biases. By providing a dataset grounded in real-world\nexperiences and accompanied by extensive metadata and supporting code, the\nauthors invite the research community to utilize and contribute to the VEDB,\nfacilitating a richer understanding of visual perception and behavior in\nnaturalistic settings.\n", "link": "http://arxiv.org/abs/2404.18934v2", "date": "2024-08-13", "relevancy": 2.6249, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5399}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5274}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Visual%20Experience%20Dataset%3A%20Over%20200%20Recorded%20Hours%20of%20Integrated%20Eye%0A%20%20Movement%2C%20Odometry%2C%20and%20Egocentric%20Video&body=Title%3A%20The%20Visual%20Experience%20Dataset%3A%20Over%20200%20Recorded%20Hours%20of%20Integrated%20Eye%0A%20%20Movement%2C%20Odometry%2C%20and%20Egocentric%20Video%0AAuthor%3A%20Michelle%20R.%20Greene%20and%20Benjamin%20J.%20Balas%20and%20Mark%20D.%20Lescroart%20and%20Paul%20R.%20MacNeilage%20and%20Jennifer%20A.%20Hart%20and%20Kamran%20Binaee%20and%20Peter%20A.%20Hausamann%20and%20Ronald%20Mezile%20and%20Bharath%20Shankar%20and%20Christian%20B.%20Sinnott%20and%20Kaylie%20Capurro%20and%20Savannah%20Halow%20and%20Hunter%20Howe%20and%20Mariam%20Josyula%20and%20Annie%20Li%20and%20Abraham%20Mieses%20and%20Amina%20Mohamed%20and%20Ilya%20Nudnou%20and%20Ezra%20Parkhill%20and%20Peter%20Riley%20and%20Brett%20Schmidt%20and%20Matthew%20W.%20Shinkle%20and%20Wentao%20Si%20and%20Brian%20Szekely%20and%20Joaquin%20M.%20Torres%20and%20Eliana%20Weissmann%0AAbstract%3A%20%20%20We%20introduce%20the%20Visual%20Experience%20Dataset%20%28VEDB%29%2C%20a%20compilation%20of%20over%20240%0Ahours%20of%20egocentric%20video%20combined%20with%20gaze-%20and%20head-tracking%20data%20that%0Aoffers%20an%20unprecedented%20view%20of%20the%20visual%20world%20as%20experienced%20by%20human%0Aobservers.%20The%20dataset%20consists%20of%20717%20sessions%2C%20recorded%20by%2058%20observers%0Aranging%20from%206-49%20years%20old.%20This%20paper%20outlines%20the%20data%20collection%2C%0Aprocessing%2C%20and%20labeling%20protocols%20undertaken%20to%20ensure%20a%20representative%20sample%0Aand%20discusses%20the%20potential%20sources%20of%20error%20or%20bias%20within%20the%20dataset.%20The%0AVEDB%27s%20potential%20applications%20are%20vast%2C%20including%20improving%20gaze%20tracking%0Amethodologies%2C%20assessing%20spatiotemporal%20image%20statistics%2C%20and%20refining%20deep%0Aneural%20networks%20for%20scene%20and%20activity%20recognition.%20The%20VEDB%20is%20accessible%0Athrough%20established%20open%20science%20platforms%20and%20is%20intended%20to%20be%20a%20living%0Adataset%20with%20plans%20for%20expansion%20and%20community%20contributions.%20It%20is%20released%0Awith%20an%20emphasis%20on%20ethical%20considerations%2C%20such%20as%20participant%20privacy%20and%20the%0Amitigation%20of%20potential%20biases.%20By%20providing%20a%20dataset%20grounded%20in%20real-world%0Aexperiences%20and%20accompanied%20by%20extensive%20metadata%20and%20supporting%20code%2C%20the%0Aauthors%20invite%20the%20research%20community%20to%20utilize%20and%20contribute%20to%20the%20VEDB%2C%0Afacilitating%20a%20richer%20understanding%20of%20visual%20perception%20and%20behavior%20in%0Anaturalistic%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18934v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Visual%2520Experience%2520Dataset%253A%2520Over%2520200%2520Recorded%2520Hours%2520of%2520Integrated%2520Eye%250A%2520%2520Movement%252C%2520Odometry%252C%2520and%2520Egocentric%2520Video%26entry.906535625%3DMichelle%2520R.%2520Greene%2520and%2520Benjamin%2520J.%2520Balas%2520and%2520Mark%2520D.%2520Lescroart%2520and%2520Paul%2520R.%2520MacNeilage%2520and%2520Jennifer%2520A.%2520Hart%2520and%2520Kamran%2520Binaee%2520and%2520Peter%2520A.%2520Hausamann%2520and%2520Ronald%2520Mezile%2520and%2520Bharath%2520Shankar%2520and%2520Christian%2520B.%2520Sinnott%2520and%2520Kaylie%2520Capurro%2520and%2520Savannah%2520Halow%2520and%2520Hunter%2520Howe%2520and%2520Mariam%2520Josyula%2520and%2520Annie%2520Li%2520and%2520Abraham%2520Mieses%2520and%2520Amina%2520Mohamed%2520and%2520Ilya%2520Nudnou%2520and%2520Ezra%2520Parkhill%2520and%2520Peter%2520Riley%2520and%2520Brett%2520Schmidt%2520and%2520Matthew%2520W.%2520Shinkle%2520and%2520Wentao%2520Si%2520and%2520Brian%2520Szekely%2520and%2520Joaquin%2520M.%2520Torres%2520and%2520Eliana%2520Weissmann%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Visual%2520Experience%2520Dataset%2520%2528VEDB%2529%252C%2520a%2520compilation%2520of%2520over%2520240%250Ahours%2520of%2520egocentric%2520video%2520combined%2520with%2520gaze-%2520and%2520head-tracking%2520data%2520that%250Aoffers%2520an%2520unprecedented%2520view%2520of%2520the%2520visual%2520world%2520as%2520experienced%2520by%2520human%250Aobservers.%2520The%2520dataset%2520consists%2520of%2520717%2520sessions%252C%2520recorded%2520by%252058%2520observers%250Aranging%2520from%25206-49%2520years%2520old.%2520This%2520paper%2520outlines%2520the%2520data%2520collection%252C%250Aprocessing%252C%2520and%2520labeling%2520protocols%2520undertaken%2520to%2520ensure%2520a%2520representative%2520sample%250Aand%2520discusses%2520the%2520potential%2520sources%2520of%2520error%2520or%2520bias%2520within%2520the%2520dataset.%2520The%250AVEDB%2527s%2520potential%2520applications%2520are%2520vast%252C%2520including%2520improving%2520gaze%2520tracking%250Amethodologies%252C%2520assessing%2520spatiotemporal%2520image%2520statistics%252C%2520and%2520refining%2520deep%250Aneural%2520networks%2520for%2520scene%2520and%2520activity%2520recognition.%2520The%2520VEDB%2520is%2520accessible%250Athrough%2520established%2520open%2520science%2520platforms%2520and%2520is%2520intended%2520to%2520be%2520a%2520living%250Adataset%2520with%2520plans%2520for%2520expansion%2520and%2520community%2520contributions.%2520It%2520is%2520released%250Awith%2520an%2520emphasis%2520on%2520ethical%2520considerations%252C%2520such%2520as%2520participant%2520privacy%2520and%2520the%250Amitigation%2520of%2520potential%2520biases.%2520By%2520providing%2520a%2520dataset%2520grounded%2520in%2520real-world%250Aexperiences%2520and%2520accompanied%2520by%2520extensive%2520metadata%2520and%2520supporting%2520code%252C%2520the%250Aauthors%2520invite%2520the%2520research%2520community%2520to%2520utilize%2520and%2520contribute%2520to%2520the%2520VEDB%252C%250Afacilitating%2520a%2520richer%2520understanding%2520of%2520visual%2520perception%2520and%2520behavior%2520in%250Anaturalistic%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18934v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Visual%20Experience%20Dataset%3A%20Over%20200%20Recorded%20Hours%20of%20Integrated%20Eye%0A%20%20Movement%2C%20Odometry%2C%20and%20Egocentric%20Video&entry.906535625=Michelle%20R.%20Greene%20and%20Benjamin%20J.%20Balas%20and%20Mark%20D.%20Lescroart%20and%20Paul%20R.%20MacNeilage%20and%20Jennifer%20A.%20Hart%20and%20Kamran%20Binaee%20and%20Peter%20A.%20Hausamann%20and%20Ronald%20Mezile%20and%20Bharath%20Shankar%20and%20Christian%20B.%20Sinnott%20and%20Kaylie%20Capurro%20and%20Savannah%20Halow%20and%20Hunter%20Howe%20and%20Mariam%20Josyula%20and%20Annie%20Li%20and%20Abraham%20Mieses%20and%20Amina%20Mohamed%20and%20Ilya%20Nudnou%20and%20Ezra%20Parkhill%20and%20Peter%20Riley%20and%20Brett%20Schmidt%20and%20Matthew%20W.%20Shinkle%20and%20Wentao%20Si%20and%20Brian%20Szekely%20and%20Joaquin%20M.%20Torres%20and%20Eliana%20Weissmann&entry.1292438233=%20%20We%20introduce%20the%20Visual%20Experience%20Dataset%20%28VEDB%29%2C%20a%20compilation%20of%20over%20240%0Ahours%20of%20egocentric%20video%20combined%20with%20gaze-%20and%20head-tracking%20data%20that%0Aoffers%20an%20unprecedented%20view%20of%20the%20visual%20world%20as%20experienced%20by%20human%0Aobservers.%20The%20dataset%20consists%20of%20717%20sessions%2C%20recorded%20by%2058%20observers%0Aranging%20from%206-49%20years%20old.%20This%20paper%20outlines%20the%20data%20collection%2C%0Aprocessing%2C%20and%20labeling%20protocols%20undertaken%20to%20ensure%20a%20representative%20sample%0Aand%20discusses%20the%20potential%20sources%20of%20error%20or%20bias%20within%20the%20dataset.%20The%0AVEDB%27s%20potential%20applications%20are%20vast%2C%20including%20improving%20gaze%20tracking%0Amethodologies%2C%20assessing%20spatiotemporal%20image%20statistics%2C%20and%20refining%20deep%0Aneural%20networks%20for%20scene%20and%20activity%20recognition.%20The%20VEDB%20is%20accessible%0Athrough%20established%20open%20science%20platforms%20and%20is%20intended%20to%20be%20a%20living%0Adataset%20with%20plans%20for%20expansion%20and%20community%20contributions.%20It%20is%20released%0Awith%20an%20emphasis%20on%20ethical%20considerations%2C%20such%20as%20participant%20privacy%20and%20the%0Amitigation%20of%20potential%20biases.%20By%20providing%20a%20dataset%20grounded%20in%20real-world%0Aexperiences%20and%20accompanied%20by%20extensive%20metadata%20and%20supporting%20code%2C%20the%0Aauthors%20invite%20the%20research%20community%20to%20utilize%20and%20contribute%20to%20the%20VEDB%2C%0Afacilitating%20a%20richer%20understanding%20of%20visual%20perception%20and%20behavior%20in%0Anaturalistic%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18934v2&entry.124074799=Read"},
{"title": "Heterogeneous Space Fusion and Dual-Dimension Attention: A New Paradigm\n  for Speech Enhancement", "author": "Tao Zheng and Liejun Wang and Yinfeng Yu", "abstract": "  Self-supervised learning has demonstrated impressive performance in speech\ntasks, yet there remains ample opportunity for advancement in the realm of\nspeech enhancement research. In addressing speech tasks, confining the\nattention mechanism solely to the temporal dimension poses limitations in\neffectively focusing on critical speech features. Considering the\naforementioned issues, our study introduces a novel speech enhancement\nframework, HFSDA, which skillfully integrates heterogeneous spatial features\nand incorporates a dual-dimension attention mechanism to significantly enhance\nspeech clarity and quality in noisy environments. By leveraging self-supervised\nlearning embeddings in tandem with Short-Time Fourier Transform (STFT)\nspectrogram features, our model excels at capturing both high-level semantic\ninformation and detailed spectral data, enabling a more thorough analysis and\nrefinement of speech signals. Furthermore, we employ the innovative\nOmni-dimensional Dynamic Convolution (ODConv) technology within the spectrogram\ninput branch, enabling enhanced extraction and integration of crucial\ninformation across multiple dimensions. Additionally, we refine the Conformer\nmodel by enhancing its feature extraction capabilities not only in the temporal\ndimension but also across the spectral domain. Extensive experiments on the\nVCTK-DEMAND dataset show that HFSDA is comparable to existing state-of-the-art\nmodels, confirming the validity of our approach.\n", "link": "http://arxiv.org/abs/2408.06911v1", "date": "2024-08-13", "relevancy": 2.6241, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5349}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5276}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Space%20Fusion%20and%20Dual-Dimension%20Attention%3A%20A%20New%20Paradigm%0A%20%20for%20Speech%20Enhancement&body=Title%3A%20Heterogeneous%20Space%20Fusion%20and%20Dual-Dimension%20Attention%3A%20A%20New%20Paradigm%0A%20%20for%20Speech%20Enhancement%0AAuthor%3A%20Tao%20Zheng%20and%20Liejun%20Wang%20and%20Yinfeng%20Yu%0AAbstract%3A%20%20%20Self-supervised%20learning%20has%20demonstrated%20impressive%20performance%20in%20speech%0Atasks%2C%20yet%20there%20remains%20ample%20opportunity%20for%20advancement%20in%20the%20realm%20of%0Aspeech%20enhancement%20research.%20In%20addressing%20speech%20tasks%2C%20confining%20the%0Aattention%20mechanism%20solely%20to%20the%20temporal%20dimension%20poses%20limitations%20in%0Aeffectively%20focusing%20on%20critical%20speech%20features.%20Considering%20the%0Aaforementioned%20issues%2C%20our%20study%20introduces%20a%20novel%20speech%20enhancement%0Aframework%2C%20HFSDA%2C%20which%20skillfully%20integrates%20heterogeneous%20spatial%20features%0Aand%20incorporates%20a%20dual-dimension%20attention%20mechanism%20to%20significantly%20enhance%0Aspeech%20clarity%20and%20quality%20in%20noisy%20environments.%20By%20leveraging%20self-supervised%0Alearning%20embeddings%20in%20tandem%20with%20Short-Time%20Fourier%20Transform%20%28STFT%29%0Aspectrogram%20features%2C%20our%20model%20excels%20at%20capturing%20both%20high-level%20semantic%0Ainformation%20and%20detailed%20spectral%20data%2C%20enabling%20a%20more%20thorough%20analysis%20and%0Arefinement%20of%20speech%20signals.%20Furthermore%2C%20we%20employ%20the%20innovative%0AOmni-dimensional%20Dynamic%20Convolution%20%28ODConv%29%20technology%20within%20the%20spectrogram%0Ainput%20branch%2C%20enabling%20enhanced%20extraction%20and%20integration%20of%20crucial%0Ainformation%20across%20multiple%20dimensions.%20Additionally%2C%20we%20refine%20the%20Conformer%0Amodel%20by%20enhancing%20its%20feature%20extraction%20capabilities%20not%20only%20in%20the%20temporal%0Adimension%20but%20also%20across%20the%20spectral%20domain.%20Extensive%20experiments%20on%20the%0AVCTK-DEMAND%20dataset%20show%20that%20HFSDA%20is%20comparable%20to%20existing%20state-of-the-art%0Amodels%2C%20confirming%20the%20validity%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Space%2520Fusion%2520and%2520Dual-Dimension%2520Attention%253A%2520A%2520New%2520Paradigm%250A%2520%2520for%2520Speech%2520Enhancement%26entry.906535625%3DTao%2520Zheng%2520and%2520Liejun%2520Wang%2520and%2520Yinfeng%2520Yu%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520has%2520demonstrated%2520impressive%2520performance%2520in%2520speech%250Atasks%252C%2520yet%2520there%2520remains%2520ample%2520opportunity%2520for%2520advancement%2520in%2520the%2520realm%2520of%250Aspeech%2520enhancement%2520research.%2520In%2520addressing%2520speech%2520tasks%252C%2520confining%2520the%250Aattention%2520mechanism%2520solely%2520to%2520the%2520temporal%2520dimension%2520poses%2520limitations%2520in%250Aeffectively%2520focusing%2520on%2520critical%2520speech%2520features.%2520Considering%2520the%250Aaforementioned%2520issues%252C%2520our%2520study%2520introduces%2520a%2520novel%2520speech%2520enhancement%250Aframework%252C%2520HFSDA%252C%2520which%2520skillfully%2520integrates%2520heterogeneous%2520spatial%2520features%250Aand%2520incorporates%2520a%2520dual-dimension%2520attention%2520mechanism%2520to%2520significantly%2520enhance%250Aspeech%2520clarity%2520and%2520quality%2520in%2520noisy%2520environments.%2520By%2520leveraging%2520self-supervised%250Alearning%2520embeddings%2520in%2520tandem%2520with%2520Short-Time%2520Fourier%2520Transform%2520%2528STFT%2529%250Aspectrogram%2520features%252C%2520our%2520model%2520excels%2520at%2520capturing%2520both%2520high-level%2520semantic%250Ainformation%2520and%2520detailed%2520spectral%2520data%252C%2520enabling%2520a%2520more%2520thorough%2520analysis%2520and%250Arefinement%2520of%2520speech%2520signals.%2520Furthermore%252C%2520we%2520employ%2520the%2520innovative%250AOmni-dimensional%2520Dynamic%2520Convolution%2520%2528ODConv%2529%2520technology%2520within%2520the%2520spectrogram%250Ainput%2520branch%252C%2520enabling%2520enhanced%2520extraction%2520and%2520integration%2520of%2520crucial%250Ainformation%2520across%2520multiple%2520dimensions.%2520Additionally%252C%2520we%2520refine%2520the%2520Conformer%250Amodel%2520by%2520enhancing%2520its%2520feature%2520extraction%2520capabilities%2520not%2520only%2520in%2520the%2520temporal%250Adimension%2520but%2520also%2520across%2520the%2520spectral%2520domain.%2520Extensive%2520experiments%2520on%2520the%250AVCTK-DEMAND%2520dataset%2520show%2520that%2520HFSDA%2520is%2520comparable%2520to%2520existing%2520state-of-the-art%250Amodels%252C%2520confirming%2520the%2520validity%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Space%20Fusion%20and%20Dual-Dimension%20Attention%3A%20A%20New%20Paradigm%0A%20%20for%20Speech%20Enhancement&entry.906535625=Tao%20Zheng%20and%20Liejun%20Wang%20and%20Yinfeng%20Yu&entry.1292438233=%20%20Self-supervised%20learning%20has%20demonstrated%20impressive%20performance%20in%20speech%0Atasks%2C%20yet%20there%20remains%20ample%20opportunity%20for%20advancement%20in%20the%20realm%20of%0Aspeech%20enhancement%20research.%20In%20addressing%20speech%20tasks%2C%20confining%20the%0Aattention%20mechanism%20solely%20to%20the%20temporal%20dimension%20poses%20limitations%20in%0Aeffectively%20focusing%20on%20critical%20speech%20features.%20Considering%20the%0Aaforementioned%20issues%2C%20our%20study%20introduces%20a%20novel%20speech%20enhancement%0Aframework%2C%20HFSDA%2C%20which%20skillfully%20integrates%20heterogeneous%20spatial%20features%0Aand%20incorporates%20a%20dual-dimension%20attention%20mechanism%20to%20significantly%20enhance%0Aspeech%20clarity%20and%20quality%20in%20noisy%20environments.%20By%20leveraging%20self-supervised%0Alearning%20embeddings%20in%20tandem%20with%20Short-Time%20Fourier%20Transform%20%28STFT%29%0Aspectrogram%20features%2C%20our%20model%20excels%20at%20capturing%20both%20high-level%20semantic%0Ainformation%20and%20detailed%20spectral%20data%2C%20enabling%20a%20more%20thorough%20analysis%20and%0Arefinement%20of%20speech%20signals.%20Furthermore%2C%20we%20employ%20the%20innovative%0AOmni-dimensional%20Dynamic%20Convolution%20%28ODConv%29%20technology%20within%20the%20spectrogram%0Ainput%20branch%2C%20enabling%20enhanced%20extraction%20and%20integration%20of%20crucial%0Ainformation%20across%20multiple%20dimensions.%20Additionally%2C%20we%20refine%20the%20Conformer%0Amodel%20by%20enhancing%20its%20feature%20extraction%20capabilities%20not%20only%20in%20the%20temporal%0Adimension%20but%20also%20across%20the%20spectral%20domain.%20Extensive%20experiments%20on%20the%0AVCTK-DEMAND%20dataset%20show%20that%20HFSDA%20is%20comparable%20to%20existing%20state-of-the-art%0Amodels%2C%20confirming%20the%20validity%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06911v1&entry.124074799=Read"},
{"title": "kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually\n  Expanding Large Vocabularies", "author": "Zhongrui Gui and Shuyang Sun and Runjia Li and Jianhao Yuan and Zhaochong An and Karsten Roth and Ameya Prabhu and Philip Torr", "abstract": "  Continual segmentation has not yet tackled the challenge of improving\nopen-vocabulary segmentation models with training data for accurate\nsegmentation across large, continually expanding vocabularies. We discover that\ntraditional continual training results in severe catastrophic forgetting,\nfailing to outperform a zero-shot segmentation baseline. We introduce a novel\ntraining-free strategy, kNN-CLIP, which augments the model with a database of\ninstance embeddings for semantic and panoptic segmentation that achieves zero\nforgetting. We demonstrate that kNN-CLIP can adapt to continually growing\nvocabularies without the need for retraining or large memory costs. kNN-CLIP\nenables open-vocabulary segmentation methods to expand their vocabularies on\nany domain with a single pass through the data, while only storing compact\nembeddings. This approach minimizes both compute and memory costs. kNN-CLIP\nachieves state-of-the-art performance across large-vocabulary semantic and\npanoptic segmentation datasets. We hope kNN-CLIP represents a significant step\nforward in enabling more efficient and adaptable continual segmentation, paving\nthe way for advances in real-world large-vocabulary continual segmentation\nmethods.\n", "link": "http://arxiv.org/abs/2404.09447v3", "date": "2024-08-13", "relevancy": 2.6233, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5069}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20kNN-CLIP%3A%20Retrieval%20Enables%20Training-Free%20Segmentation%20on%20Continually%0A%20%20Expanding%20Large%20Vocabularies&body=Title%3A%20kNN-CLIP%3A%20Retrieval%20Enables%20Training-Free%20Segmentation%20on%20Continually%0A%20%20Expanding%20Large%20Vocabularies%0AAuthor%3A%20Zhongrui%20Gui%20and%20Shuyang%20Sun%20and%20Runjia%20Li%20and%20Jianhao%20Yuan%20and%20Zhaochong%20An%20and%20Karsten%20Roth%20and%20Ameya%20Prabhu%20and%20Philip%20Torr%0AAbstract%3A%20%20%20Continual%20segmentation%20has%20not%20yet%20tackled%20the%20challenge%20of%20improving%0Aopen-vocabulary%20segmentation%20models%20with%20training%20data%20for%20accurate%0Asegmentation%20across%20large%2C%20continually%20expanding%20vocabularies.%20We%20discover%20that%0Atraditional%20continual%20training%20results%20in%20severe%20catastrophic%20forgetting%2C%0Afailing%20to%20outperform%20a%20zero-shot%20segmentation%20baseline.%20We%20introduce%20a%20novel%0Atraining-free%20strategy%2C%20kNN-CLIP%2C%20which%20augments%20the%20model%20with%20a%20database%20of%0Ainstance%20embeddings%20for%20semantic%20and%20panoptic%20segmentation%20that%20achieves%20zero%0Aforgetting.%20We%20demonstrate%20that%20kNN-CLIP%20can%20adapt%20to%20continually%20growing%0Avocabularies%20without%20the%20need%20for%20retraining%20or%20large%20memory%20costs.%20kNN-CLIP%0Aenables%20open-vocabulary%20segmentation%20methods%20to%20expand%20their%20vocabularies%20on%0Aany%20domain%20with%20a%20single%20pass%20through%20the%20data%2C%20while%20only%20storing%20compact%0Aembeddings.%20This%20approach%20minimizes%20both%20compute%20and%20memory%20costs.%20kNN-CLIP%0Aachieves%20state-of-the-art%20performance%20across%20large-vocabulary%20semantic%20and%0Apanoptic%20segmentation%20datasets.%20We%20hope%20kNN-CLIP%20represents%20a%20significant%20step%0Aforward%20in%20enabling%20more%20efficient%20and%20adaptable%20continual%20segmentation%2C%20paving%0Athe%20way%20for%20advances%20in%20real-world%20large-vocabulary%20continual%20segmentation%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09447v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DkNN-CLIP%253A%2520Retrieval%2520Enables%2520Training-Free%2520Segmentation%2520on%2520Continually%250A%2520%2520Expanding%2520Large%2520Vocabularies%26entry.906535625%3DZhongrui%2520Gui%2520and%2520Shuyang%2520Sun%2520and%2520Runjia%2520Li%2520and%2520Jianhao%2520Yuan%2520and%2520Zhaochong%2520An%2520and%2520Karsten%2520Roth%2520and%2520Ameya%2520Prabhu%2520and%2520Philip%2520Torr%26entry.1292438233%3D%2520%2520Continual%2520segmentation%2520has%2520not%2520yet%2520tackled%2520the%2520challenge%2520of%2520improving%250Aopen-vocabulary%2520segmentation%2520models%2520with%2520training%2520data%2520for%2520accurate%250Asegmentation%2520across%2520large%252C%2520continually%2520expanding%2520vocabularies.%2520We%2520discover%2520that%250Atraditional%2520continual%2520training%2520results%2520in%2520severe%2520catastrophic%2520forgetting%252C%250Afailing%2520to%2520outperform%2520a%2520zero-shot%2520segmentation%2520baseline.%2520We%2520introduce%2520a%2520novel%250Atraining-free%2520strategy%252C%2520kNN-CLIP%252C%2520which%2520augments%2520the%2520model%2520with%2520a%2520database%2520of%250Ainstance%2520embeddings%2520for%2520semantic%2520and%2520panoptic%2520segmentation%2520that%2520achieves%2520zero%250Aforgetting.%2520We%2520demonstrate%2520that%2520kNN-CLIP%2520can%2520adapt%2520to%2520continually%2520growing%250Avocabularies%2520without%2520the%2520need%2520for%2520retraining%2520or%2520large%2520memory%2520costs.%2520kNN-CLIP%250Aenables%2520open-vocabulary%2520segmentation%2520methods%2520to%2520expand%2520their%2520vocabularies%2520on%250Aany%2520domain%2520with%2520a%2520single%2520pass%2520through%2520the%2520data%252C%2520while%2520only%2520storing%2520compact%250Aembeddings.%2520This%2520approach%2520minimizes%2520both%2520compute%2520and%2520memory%2520costs.%2520kNN-CLIP%250Aachieves%2520state-of-the-art%2520performance%2520across%2520large-vocabulary%2520semantic%2520and%250Apanoptic%2520segmentation%2520datasets.%2520We%2520hope%2520kNN-CLIP%2520represents%2520a%2520significant%2520step%250Aforward%2520in%2520enabling%2520more%2520efficient%2520and%2520adaptable%2520continual%2520segmentation%252C%2520paving%250Athe%2520way%2520for%2520advances%2520in%2520real-world%2520large-vocabulary%2520continual%2520segmentation%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09447v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=kNN-CLIP%3A%20Retrieval%20Enables%20Training-Free%20Segmentation%20on%20Continually%0A%20%20Expanding%20Large%20Vocabularies&entry.906535625=Zhongrui%20Gui%20and%20Shuyang%20Sun%20and%20Runjia%20Li%20and%20Jianhao%20Yuan%20and%20Zhaochong%20An%20and%20Karsten%20Roth%20and%20Ameya%20Prabhu%20and%20Philip%20Torr&entry.1292438233=%20%20Continual%20segmentation%20has%20not%20yet%20tackled%20the%20challenge%20of%20improving%0Aopen-vocabulary%20segmentation%20models%20with%20training%20data%20for%20accurate%0Asegmentation%20across%20large%2C%20continually%20expanding%20vocabularies.%20We%20discover%20that%0Atraditional%20continual%20training%20results%20in%20severe%20catastrophic%20forgetting%2C%0Afailing%20to%20outperform%20a%20zero-shot%20segmentation%20baseline.%20We%20introduce%20a%20novel%0Atraining-free%20strategy%2C%20kNN-CLIP%2C%20which%20augments%20the%20model%20with%20a%20database%20of%0Ainstance%20embeddings%20for%20semantic%20and%20panoptic%20segmentation%20that%20achieves%20zero%0Aforgetting.%20We%20demonstrate%20that%20kNN-CLIP%20can%20adapt%20to%20continually%20growing%0Avocabularies%20without%20the%20need%20for%20retraining%20or%20large%20memory%20costs.%20kNN-CLIP%0Aenables%20open-vocabulary%20segmentation%20methods%20to%20expand%20their%20vocabularies%20on%0Aany%20domain%20with%20a%20single%20pass%20through%20the%20data%2C%20while%20only%20storing%20compact%0Aembeddings.%20This%20approach%20minimizes%20both%20compute%20and%20memory%20costs.%20kNN-CLIP%0Aachieves%20state-of-the-art%20performance%20across%20large-vocabulary%20semantic%20and%0Apanoptic%20segmentation%20datasets.%20We%20hope%20kNN-CLIP%20represents%20a%20significant%20step%0Aforward%20in%20enabling%20more%20efficient%20and%20adaptable%20continual%20segmentation%2C%20paving%0Athe%20way%20for%20advances%20in%20real-world%20large-vocabulary%20continual%20segmentation%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09447v3&entry.124074799=Read"},
{"title": "Generative AI for Immersive Communication: The Next Frontier in\n  Internet-of-Senses Through 6G", "author": "Nassim Sehad and Lina Bariah and Wassim Hamidouche and Hamed Hellaoui and Riku J\u00e4ntti and M\u00e9rouane Debbah", "abstract": "  Over the past two decades, the Internet-of-Things (IoT) has become a\ntransformative concept, and as we approach 2030, a new paradigm known as the\nInternet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR),\nIoS seeks to provide multi-sensory experiences, acknowledging that in our\nphysical reality, our perception extends far beyond just sight and sound; it\nencompasses a range of senses. This article explores the existing technologies\ndriving immersive multi-sensory media, delving into their capabilities and\npotential applications. This exploration includes a comparative analysis\nbetween conventional immersive media streaming and a proposed use case that\nleverages semantic communication empowered by generative Artificial\nIntelligence (AI). The focal point of this analysis is the substantial\nreduction in bandwidth consumption by 99.93% in the proposed scheme. Through\nthis comparison, we aim to underscore the practical applications of generative\nAI for immersive media. Concurrently addressing major challenges in this field,\nsuch as temporal synchronization of multiple media, ensuring high throughput,\nminimizing the End-to-End (E2E) latency, and robustness to low bandwidth while\noutlining future trajectories.\n", "link": "http://arxiv.org/abs/2404.01713v2", "date": "2024-08-13", "relevancy": 2.6058, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5661}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5036}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Immersive%20Communication%3A%20The%20Next%20Frontier%20in%0A%20%20Internet-of-Senses%20Through%206G&body=Title%3A%20Generative%20AI%20for%20Immersive%20Communication%3A%20The%20Next%20Frontier%20in%0A%20%20Internet-of-Senses%20Through%206G%0AAuthor%3A%20Nassim%20Sehad%20and%20Lina%20Bariah%20and%20Wassim%20Hamidouche%20and%20Hamed%20Hellaoui%20and%20Riku%20J%C3%A4ntti%20and%20M%C3%A9rouane%20Debbah%0AAbstract%3A%20%20%20Over%20the%20past%20two%20decades%2C%20the%20Internet-of-Things%20%28IoT%29%20has%20become%20a%0Atransformative%20concept%2C%20and%20as%20we%20approach%202030%2C%20a%20new%20paradigm%20known%20as%20the%0AInternet%20of%20Senses%20%28IoS%29%20is%20emerging.%20Unlike%20conventional%20Virtual%20Reality%20%28VR%29%2C%0AIoS%20seeks%20to%20provide%20multi-sensory%20experiences%2C%20acknowledging%20that%20in%20our%0Aphysical%20reality%2C%20our%20perception%20extends%20far%20beyond%20just%20sight%20and%20sound%3B%20it%0Aencompasses%20a%20range%20of%20senses.%20This%20article%20explores%20the%20existing%20technologies%0Adriving%20immersive%20multi-sensory%20media%2C%20delving%20into%20their%20capabilities%20and%0Apotential%20applications.%20This%20exploration%20includes%20a%20comparative%20analysis%0Abetween%20conventional%20immersive%20media%20streaming%20and%20a%20proposed%20use%20case%20that%0Aleverages%20semantic%20communication%20empowered%20by%20generative%20Artificial%0AIntelligence%20%28AI%29.%20The%20focal%20point%20of%20this%20analysis%20is%20the%20substantial%0Areduction%20in%20bandwidth%20consumption%20by%2099.93%25%20in%20the%20proposed%20scheme.%20Through%0Athis%20comparison%2C%20we%20aim%20to%20underscore%20the%20practical%20applications%20of%20generative%0AAI%20for%20immersive%20media.%20Concurrently%20addressing%20major%20challenges%20in%20this%20field%2C%0Asuch%20as%20temporal%20synchronization%20of%20multiple%20media%2C%20ensuring%20high%20throughput%2C%0Aminimizing%20the%20End-to-End%20%28E2E%29%20latency%2C%20and%20robustness%20to%20low%20bandwidth%20while%0Aoutlining%20future%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Immersive%2520Communication%253A%2520The%2520Next%2520Frontier%2520in%250A%2520%2520Internet-of-Senses%2520Through%25206G%26entry.906535625%3DNassim%2520Sehad%2520and%2520Lina%2520Bariah%2520and%2520Wassim%2520Hamidouche%2520and%2520Hamed%2520Hellaoui%2520and%2520Riku%2520J%25C3%25A4ntti%2520and%2520M%25C3%25A9rouane%2520Debbah%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520two%2520decades%252C%2520the%2520Internet-of-Things%2520%2528IoT%2529%2520has%2520become%2520a%250Atransformative%2520concept%252C%2520and%2520as%2520we%2520approach%25202030%252C%2520a%2520new%2520paradigm%2520known%2520as%2520the%250AInternet%2520of%2520Senses%2520%2528IoS%2529%2520is%2520emerging.%2520Unlike%2520conventional%2520Virtual%2520Reality%2520%2528VR%2529%252C%250AIoS%2520seeks%2520to%2520provide%2520multi-sensory%2520experiences%252C%2520acknowledging%2520that%2520in%2520our%250Aphysical%2520reality%252C%2520our%2520perception%2520extends%2520far%2520beyond%2520just%2520sight%2520and%2520sound%253B%2520it%250Aencompasses%2520a%2520range%2520of%2520senses.%2520This%2520article%2520explores%2520the%2520existing%2520technologies%250Adriving%2520immersive%2520multi-sensory%2520media%252C%2520delving%2520into%2520their%2520capabilities%2520and%250Apotential%2520applications.%2520This%2520exploration%2520includes%2520a%2520comparative%2520analysis%250Abetween%2520conventional%2520immersive%2520media%2520streaming%2520and%2520a%2520proposed%2520use%2520case%2520that%250Aleverages%2520semantic%2520communication%2520empowered%2520by%2520generative%2520Artificial%250AIntelligence%2520%2528AI%2529.%2520The%2520focal%2520point%2520of%2520this%2520analysis%2520is%2520the%2520substantial%250Areduction%2520in%2520bandwidth%2520consumption%2520by%252099.93%2525%2520in%2520the%2520proposed%2520scheme.%2520Through%250Athis%2520comparison%252C%2520we%2520aim%2520to%2520underscore%2520the%2520practical%2520applications%2520of%2520generative%250AAI%2520for%2520immersive%2520media.%2520Concurrently%2520addressing%2520major%2520challenges%2520in%2520this%2520field%252C%250Asuch%2520as%2520temporal%2520synchronization%2520of%2520multiple%2520media%252C%2520ensuring%2520high%2520throughput%252C%250Aminimizing%2520the%2520End-to-End%2520%2528E2E%2529%2520latency%252C%2520and%2520robustness%2520to%2520low%2520bandwidth%2520while%250Aoutlining%2520future%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Immersive%20Communication%3A%20The%20Next%20Frontier%20in%0A%20%20Internet-of-Senses%20Through%206G&entry.906535625=Nassim%20Sehad%20and%20Lina%20Bariah%20and%20Wassim%20Hamidouche%20and%20Hamed%20Hellaoui%20and%20Riku%20J%C3%A4ntti%20and%20M%C3%A9rouane%20Debbah&entry.1292438233=%20%20Over%20the%20past%20two%20decades%2C%20the%20Internet-of-Things%20%28IoT%29%20has%20become%20a%0Atransformative%20concept%2C%20and%20as%20we%20approach%202030%2C%20a%20new%20paradigm%20known%20as%20the%0AInternet%20of%20Senses%20%28IoS%29%20is%20emerging.%20Unlike%20conventional%20Virtual%20Reality%20%28VR%29%2C%0AIoS%20seeks%20to%20provide%20multi-sensory%20experiences%2C%20acknowledging%20that%20in%20our%0Aphysical%20reality%2C%20our%20perception%20extends%20far%20beyond%20just%20sight%20and%20sound%3B%20it%0Aencompasses%20a%20range%20of%20senses.%20This%20article%20explores%20the%20existing%20technologies%0Adriving%20immersive%20multi-sensory%20media%2C%20delving%20into%20their%20capabilities%20and%0Apotential%20applications.%20This%20exploration%20includes%20a%20comparative%20analysis%0Abetween%20conventional%20immersive%20media%20streaming%20and%20a%20proposed%20use%20case%20that%0Aleverages%20semantic%20communication%20empowered%20by%20generative%20Artificial%0AIntelligence%20%28AI%29.%20The%20focal%20point%20of%20this%20analysis%20is%20the%20substantial%0Areduction%20in%20bandwidth%20consumption%20by%2099.93%25%20in%20the%20proposed%20scheme.%20Through%0Athis%20comparison%2C%20we%20aim%20to%20underscore%20the%20practical%20applications%20of%20generative%0AAI%20for%20immersive%20media.%20Concurrently%20addressing%20major%20challenges%20in%20this%20field%2C%0Asuch%20as%20temporal%20synchronization%20of%20multiple%20media%2C%20ensuring%20high%20throughput%2C%0Aminimizing%20the%20End-to-End%20%28E2E%29%20latency%2C%20and%20robustness%20to%20low%20bandwidth%20while%0Aoutlining%20future%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01713v2&entry.124074799=Read"},
{"title": "Structure-preserving Planar Simplification for Indoor Environments", "author": "Bishwash Khanal and Sanjay Rijal and Manish Awale and Vaghawan Ojha", "abstract": "  This paper presents a novel approach for structure-preserving planar\nsimplification of indoor scene point clouds for both simulated and real-world\nenvironments. Initially, the scene point cloud undergoes preprocessing steps,\nincluding noise reduction and Manhattan world alignment, to ensure robustness\nand coherence in subsequent analyses. We segment each captured scene into\nstructured (walls-ceiling-floor) and non-structured (indoor objects) scenes.\nLeveraging a RANSAC algorithm, we extract primitive planes from the input point\ncloud, facilitating the segmentation and simplification of the structured\nscene. The best-fitting wall meshes are then generated from the primitives,\nfollowed by adjacent mesh merging with the vertex-translation algorithm which\npreserves the mesh layout. To accurately represent ceilings and floors, we\nemploy the mesh clipping algorithm which clips the ceiling and floor meshes\nwith respect to wall normals. In the case of indoor scenes, we apply a surface\nreconstruction technique to enhance the fidelity. This paper focuses on the\nintricate steps of the proposed scene simplification methodology, addressing\ncomplex scenarios such as multi-story and slanted walls and ceilings. We also\nconduct qualitative and quantitative performance comparisons against popular\nsurface reconstruction, shape approximation, and floorplan generation\napproaches.\n", "link": "http://arxiv.org/abs/2408.06814v1", "date": "2024-08-13", "relevancy": 2.5957, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-preserving%20Planar%20Simplification%20for%20Indoor%20Environments&body=Title%3A%20Structure-preserving%20Planar%20Simplification%20for%20Indoor%20Environments%0AAuthor%3A%20Bishwash%20Khanal%20and%20Sanjay%20Rijal%20and%20Manish%20Awale%20and%20Vaghawan%20Ojha%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20structure-preserving%20planar%0Asimplification%20of%20indoor%20scene%20point%20clouds%20for%20both%20simulated%20and%20real-world%0Aenvironments.%20Initially%2C%20the%20scene%20point%20cloud%20undergoes%20preprocessing%20steps%2C%0Aincluding%20noise%20reduction%20and%20Manhattan%20world%20alignment%2C%20to%20ensure%20robustness%0Aand%20coherence%20in%20subsequent%20analyses.%20We%20segment%20each%20captured%20scene%20into%0Astructured%20%28walls-ceiling-floor%29%20and%20non-structured%20%28indoor%20objects%29%20scenes.%0ALeveraging%20a%20RANSAC%20algorithm%2C%20we%20extract%20primitive%20planes%20from%20the%20input%20point%0Acloud%2C%20facilitating%20the%20segmentation%20and%20simplification%20of%20the%20structured%0Ascene.%20The%20best-fitting%20wall%20meshes%20are%20then%20generated%20from%20the%20primitives%2C%0Afollowed%20by%20adjacent%20mesh%20merging%20with%20the%20vertex-translation%20algorithm%20which%0Apreserves%20the%20mesh%20layout.%20To%20accurately%20represent%20ceilings%20and%20floors%2C%20we%0Aemploy%20the%20mesh%20clipping%20algorithm%20which%20clips%20the%20ceiling%20and%20floor%20meshes%0Awith%20respect%20to%20wall%20normals.%20In%20the%20case%20of%20indoor%20scenes%2C%20we%20apply%20a%20surface%0Areconstruction%20technique%20to%20enhance%20the%20fidelity.%20This%20paper%20focuses%20on%20the%0Aintricate%20steps%20of%20the%20proposed%20scene%20simplification%20methodology%2C%20addressing%0Acomplex%20scenarios%20such%20as%20multi-story%20and%20slanted%20walls%20and%20ceilings.%20We%20also%0Aconduct%20qualitative%20and%20quantitative%20performance%20comparisons%20against%20popular%0Asurface%20reconstruction%2C%20shape%20approximation%2C%20and%20floorplan%20generation%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-preserving%2520Planar%2520Simplification%2520for%2520Indoor%2520Environments%26entry.906535625%3DBishwash%2520Khanal%2520and%2520Sanjay%2520Rijal%2520and%2520Manish%2520Awale%2520and%2520Vaghawan%2520Ojha%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520structure-preserving%2520planar%250Asimplification%2520of%2520indoor%2520scene%2520point%2520clouds%2520for%2520both%2520simulated%2520and%2520real-world%250Aenvironments.%2520Initially%252C%2520the%2520scene%2520point%2520cloud%2520undergoes%2520preprocessing%2520steps%252C%250Aincluding%2520noise%2520reduction%2520and%2520Manhattan%2520world%2520alignment%252C%2520to%2520ensure%2520robustness%250Aand%2520coherence%2520in%2520subsequent%2520analyses.%2520We%2520segment%2520each%2520captured%2520scene%2520into%250Astructured%2520%2528walls-ceiling-floor%2529%2520and%2520non-structured%2520%2528indoor%2520objects%2529%2520scenes.%250ALeveraging%2520a%2520RANSAC%2520algorithm%252C%2520we%2520extract%2520primitive%2520planes%2520from%2520the%2520input%2520point%250Acloud%252C%2520facilitating%2520the%2520segmentation%2520and%2520simplification%2520of%2520the%2520structured%250Ascene.%2520The%2520best-fitting%2520wall%2520meshes%2520are%2520then%2520generated%2520from%2520the%2520primitives%252C%250Afollowed%2520by%2520adjacent%2520mesh%2520merging%2520with%2520the%2520vertex-translation%2520algorithm%2520which%250Apreserves%2520the%2520mesh%2520layout.%2520To%2520accurately%2520represent%2520ceilings%2520and%2520floors%252C%2520we%250Aemploy%2520the%2520mesh%2520clipping%2520algorithm%2520which%2520clips%2520the%2520ceiling%2520and%2520floor%2520meshes%250Awith%2520respect%2520to%2520wall%2520normals.%2520In%2520the%2520case%2520of%2520indoor%2520scenes%252C%2520we%2520apply%2520a%2520surface%250Areconstruction%2520technique%2520to%2520enhance%2520the%2520fidelity.%2520This%2520paper%2520focuses%2520on%2520the%250Aintricate%2520steps%2520of%2520the%2520proposed%2520scene%2520simplification%2520methodology%252C%2520addressing%250Acomplex%2520scenarios%2520such%2520as%2520multi-story%2520and%2520slanted%2520walls%2520and%2520ceilings.%2520We%2520also%250Aconduct%2520qualitative%2520and%2520quantitative%2520performance%2520comparisons%2520against%2520popular%250Asurface%2520reconstruction%252C%2520shape%2520approximation%252C%2520and%2520floorplan%2520generation%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-preserving%20Planar%20Simplification%20for%20Indoor%20Environments&entry.906535625=Bishwash%20Khanal%20and%20Sanjay%20Rijal%20and%20Manish%20Awale%20and%20Vaghawan%20Ojha&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20structure-preserving%20planar%0Asimplification%20of%20indoor%20scene%20point%20clouds%20for%20both%20simulated%20and%20real-world%0Aenvironments.%20Initially%2C%20the%20scene%20point%20cloud%20undergoes%20preprocessing%20steps%2C%0Aincluding%20noise%20reduction%20and%20Manhattan%20world%20alignment%2C%20to%20ensure%20robustness%0Aand%20coherence%20in%20subsequent%20analyses.%20We%20segment%20each%20captured%20scene%20into%0Astructured%20%28walls-ceiling-floor%29%20and%20non-structured%20%28indoor%20objects%29%20scenes.%0ALeveraging%20a%20RANSAC%20algorithm%2C%20we%20extract%20primitive%20planes%20from%20the%20input%20point%0Acloud%2C%20facilitating%20the%20segmentation%20and%20simplification%20of%20the%20structured%0Ascene.%20The%20best-fitting%20wall%20meshes%20are%20then%20generated%20from%20the%20primitives%2C%0Afollowed%20by%20adjacent%20mesh%20merging%20with%20the%20vertex-translation%20algorithm%20which%0Apreserves%20the%20mesh%20layout.%20To%20accurately%20represent%20ceilings%20and%20floors%2C%20we%0Aemploy%20the%20mesh%20clipping%20algorithm%20which%20clips%20the%20ceiling%20and%20floor%20meshes%0Awith%20respect%20to%20wall%20normals.%20In%20the%20case%20of%20indoor%20scenes%2C%20we%20apply%20a%20surface%0Areconstruction%20technique%20to%20enhance%20the%20fidelity.%20This%20paper%20focuses%20on%20the%0Aintricate%20steps%20of%20the%20proposed%20scene%20simplification%20methodology%2C%20addressing%0Acomplex%20scenarios%20such%20as%20multi-story%20and%20slanted%20walls%20and%20ceilings.%20We%20also%0Aconduct%20qualitative%20and%20quantitative%20performance%20comparisons%20against%20popular%0Asurface%20reconstruction%2C%20shape%20approximation%2C%20and%20floorplan%20generation%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06814v1&entry.124074799=Read"},
{"title": "From NeRFs to Gaussian Splats, and Back", "author": "Siming He and Zach Osman and Pratik Chaudhari", "abstract": "  For robotics applications where there is a limited number of (typically\nego-centric) views, parametric representations such as neural radiance fields\n(NeRFs) generalize better than non-parametric ones such as Gaussian splatting\n(GS) to views that are very different from those in the training data; GS\nhowever can render much faster than NeRFs. We develop a procedure to convert\nback and forth between the two. Our approach achieves the best of both NeRFs\n(superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact\nrepresentation) and GS (real-time rendering and ability for easily modifying\nthe representation); the computational cost of these conversions is minor\ncompared to training the two from scratch.\n", "link": "http://arxiv.org/abs/2405.09717v3", "date": "2024-08-13", "relevancy": 2.5541, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.712}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6075}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20NeRFs%20to%20Gaussian%20Splats%2C%20and%20Back&body=Title%3A%20From%20NeRFs%20to%20Gaussian%20Splats%2C%20and%20Back%0AAuthor%3A%20Siming%20He%20and%20Zach%20Osman%20and%20Pratik%20Chaudhari%0AAbstract%3A%20%20%20For%20robotics%20applications%20where%20there%20is%20a%20limited%20number%20of%20%28typically%0Aego-centric%29%20views%2C%20parametric%20representations%20such%20as%20neural%20radiance%20fields%0A%28NeRFs%29%20generalize%20better%20than%20non-parametric%20ones%20such%20as%20Gaussian%20splatting%0A%28GS%29%20to%20views%20that%20are%20very%20different%20from%20those%20in%20the%20training%20data%3B%20GS%0Ahowever%20can%20render%20much%20faster%20than%20NeRFs.%20We%20develop%20a%20procedure%20to%20convert%0Aback%20and%20forth%20between%20the%20two.%20Our%20approach%20achieves%20the%20best%20of%20both%20NeRFs%0A%28superior%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20on%20dissimilar%20views%2C%20and%20a%20compact%0Arepresentation%29%20and%20GS%20%28real-time%20rendering%20and%20ability%20for%20easily%20modifying%0Athe%20representation%29%3B%20the%20computational%20cost%20of%20these%20conversions%20is%20minor%0Acompared%20to%20training%20the%20two%20from%20scratch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09717v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520NeRFs%2520to%2520Gaussian%2520Splats%252C%2520and%2520Back%26entry.906535625%3DSiming%2520He%2520and%2520Zach%2520Osman%2520and%2520Pratik%2520Chaudhari%26entry.1292438233%3D%2520%2520For%2520robotics%2520applications%2520where%2520there%2520is%2520a%2520limited%2520number%2520of%2520%2528typically%250Aego-centric%2529%2520views%252C%2520parametric%2520representations%2520such%2520as%2520neural%2520radiance%2520fields%250A%2528NeRFs%2529%2520generalize%2520better%2520than%2520non-parametric%2520ones%2520such%2520as%2520Gaussian%2520splatting%250A%2528GS%2529%2520to%2520views%2520that%2520are%2520very%2520different%2520from%2520those%2520in%2520the%2520training%2520data%253B%2520GS%250Ahowever%2520can%2520render%2520much%2520faster%2520than%2520NeRFs.%2520We%2520develop%2520a%2520procedure%2520to%2520convert%250Aback%2520and%2520forth%2520between%2520the%2520two.%2520Our%2520approach%2520achieves%2520the%2520best%2520of%2520both%2520NeRFs%250A%2528superior%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%2520on%2520dissimilar%2520views%252C%2520and%2520a%2520compact%250Arepresentation%2529%2520and%2520GS%2520%2528real-time%2520rendering%2520and%2520ability%2520for%2520easily%2520modifying%250Athe%2520representation%2529%253B%2520the%2520computational%2520cost%2520of%2520these%2520conversions%2520is%2520minor%250Acompared%2520to%2520training%2520the%2520two%2520from%2520scratch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09717v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20NeRFs%20to%20Gaussian%20Splats%2C%20and%20Back&entry.906535625=Siming%20He%20and%20Zach%20Osman%20and%20Pratik%20Chaudhari&entry.1292438233=%20%20For%20robotics%20applications%20where%20there%20is%20a%20limited%20number%20of%20%28typically%0Aego-centric%29%20views%2C%20parametric%20representations%20such%20as%20neural%20radiance%20fields%0A%28NeRFs%29%20generalize%20better%20than%20non-parametric%20ones%20such%20as%20Gaussian%20splatting%0A%28GS%29%20to%20views%20that%20are%20very%20different%20from%20those%20in%20the%20training%20data%3B%20GS%0Ahowever%20can%20render%20much%20faster%20than%20NeRFs.%20We%20develop%20a%20procedure%20to%20convert%0Aback%20and%20forth%20between%20the%20two.%20Our%20approach%20achieves%20the%20best%20of%20both%20NeRFs%0A%28superior%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20on%20dissimilar%20views%2C%20and%20a%20compact%0Arepresentation%29%20and%20GS%20%28real-time%20rendering%20and%20ability%20for%20easily%20modifying%0Athe%20representation%29%3B%20the%20computational%20cost%20of%20these%20conversions%20is%20minor%0Acompared%20to%20training%20the%20two%20from%20scratch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09717v3&entry.124074799=Read"},
{"title": "Out of Length Text Recognition with Sub-String Matching", "author": "Yongkun Du and Zhineng Chen and Caiyan Jia and Xieping Gao and Yu-Gang Jiang", "abstract": "  Scene Text Recognition (STR) methods have demonstrated robust performance in\nword-level text recognition. However, in real applications the text image is\nsometimes long due to detected with multiple horizontal words. It triggers the\nrequirement to build long text recognition models from readily available short\n(i.e., word-level) text datasets, which has been less studied previously. In\nthis paper, we term this task Out of Length (OOL) text recognition. We\nestablish the first Long Text Benchmark (LTB) to facilitate the assessment of\ndifferent methods in long text recognition. Meanwhile, we propose a novel\nmethod called OOL Text Recognition with sub-String Matching (SMTR). SMTR\ncomprises two cross-attention-based modules: one encodes a sub-string\ncontaining multiple characters into next and previous queries, and the other\nemploys the queries to attend to the image features, matching the sub-string\nand simultaneously recognizing its next and previous character. SMTR can\nrecognize text of arbitrary length by iterating the process above. To avoid\nbeing trapped in recognizing highly similar sub-strings, we introduce a\nregularization training to compel SMTR to effectively discover subtle\ndifferences between similar sub-strings for precise matching. In addition, we\npropose an inference augmentation strategy to alleviate confusion caused by\nidentical sub-strings in the same text and improve the overall recognition\nefficiency. Extensive experimental results reveal that SMTR, even when trained\nexclusively on short text, outperforms existing methods in public short text\nbenchmarks and exhibits a clear advantage on LTB. Code:\nhttps://github.com/Topdu/OpenOCR.\n", "link": "http://arxiv.org/abs/2407.12317v2", "date": "2024-08-13", "relevancy": 2.5334, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5414}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out%20of%20Length%20Text%20Recognition%20with%20Sub-String%20Matching&body=Title%3A%20Out%20of%20Length%20Text%20Recognition%20with%20Sub-String%20Matching%0AAuthor%3A%20Yongkun%20Du%20and%20Zhineng%20Chen%20and%20Caiyan%20Jia%20and%20Xieping%20Gao%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Scene%20Text%20Recognition%20%28STR%29%20methods%20have%20demonstrated%20robust%20performance%20in%0Aword-level%20text%20recognition.%20However%2C%20in%20real%20applications%20the%20text%20image%20is%0Asometimes%20long%20due%20to%20detected%20with%20multiple%20horizontal%20words.%20It%20triggers%20the%0Arequirement%20to%20build%20long%20text%20recognition%20models%20from%20readily%20available%20short%0A%28i.e.%2C%20word-level%29%20text%20datasets%2C%20which%20has%20been%20less%20studied%20previously.%20In%0Athis%20paper%2C%20we%20term%20this%20task%20Out%20of%20Length%20%28OOL%29%20text%20recognition.%20We%0Aestablish%20the%20first%20Long%20Text%20Benchmark%20%28LTB%29%20to%20facilitate%20the%20assessment%20of%0Adifferent%20methods%20in%20long%20text%20recognition.%20Meanwhile%2C%20we%20propose%20a%20novel%0Amethod%20called%20OOL%20Text%20Recognition%20with%20sub-String%20Matching%20%28SMTR%29.%20SMTR%0Acomprises%20two%20cross-attention-based%20modules%3A%20one%20encodes%20a%20sub-string%0Acontaining%20multiple%20characters%20into%20next%20and%20previous%20queries%2C%20and%20the%20other%0Aemploys%20the%20queries%20to%20attend%20to%20the%20image%20features%2C%20matching%20the%20sub-string%0Aand%20simultaneously%20recognizing%20its%20next%20and%20previous%20character.%20SMTR%20can%0Arecognize%20text%20of%20arbitrary%20length%20by%20iterating%20the%20process%20above.%20To%20avoid%0Abeing%20trapped%20in%20recognizing%20highly%20similar%20sub-strings%2C%20we%20introduce%20a%0Aregularization%20training%20to%20compel%20SMTR%20to%20effectively%20discover%20subtle%0Adifferences%20between%20similar%20sub-strings%20for%20precise%20matching.%20In%20addition%2C%20we%0Apropose%20an%20inference%20augmentation%20strategy%20to%20alleviate%20confusion%20caused%20by%0Aidentical%20sub-strings%20in%20the%20same%20text%20and%20improve%20the%20overall%20recognition%0Aefficiency.%20Extensive%20experimental%20results%20reveal%20that%20SMTR%2C%20even%20when%20trained%0Aexclusively%20on%20short%20text%2C%20outperforms%20existing%20methods%20in%20public%20short%20text%0Abenchmarks%20and%20exhibits%20a%20clear%20advantage%20on%20LTB.%20Code%3A%0Ahttps%3A//github.com/Topdu/OpenOCR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut%2520of%2520Length%2520Text%2520Recognition%2520with%2520Sub-String%2520Matching%26entry.906535625%3DYongkun%2520Du%2520and%2520Zhineng%2520Chen%2520and%2520Caiyan%2520Jia%2520and%2520Xieping%2520Gao%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Scene%2520Text%2520Recognition%2520%2528STR%2529%2520methods%2520have%2520demonstrated%2520robust%2520performance%2520in%250Aword-level%2520text%2520recognition.%2520However%252C%2520in%2520real%2520applications%2520the%2520text%2520image%2520is%250Asometimes%2520long%2520due%2520to%2520detected%2520with%2520multiple%2520horizontal%2520words.%2520It%2520triggers%2520the%250Arequirement%2520to%2520build%2520long%2520text%2520recognition%2520models%2520from%2520readily%2520available%2520short%250A%2528i.e.%252C%2520word-level%2529%2520text%2520datasets%252C%2520which%2520has%2520been%2520less%2520studied%2520previously.%2520In%250Athis%2520paper%252C%2520we%2520term%2520this%2520task%2520Out%2520of%2520Length%2520%2528OOL%2529%2520text%2520recognition.%2520We%250Aestablish%2520the%2520first%2520Long%2520Text%2520Benchmark%2520%2528LTB%2529%2520to%2520facilitate%2520the%2520assessment%2520of%250Adifferent%2520methods%2520in%2520long%2520text%2520recognition.%2520Meanwhile%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520called%2520OOL%2520Text%2520Recognition%2520with%2520sub-String%2520Matching%2520%2528SMTR%2529.%2520SMTR%250Acomprises%2520two%2520cross-attention-based%2520modules%253A%2520one%2520encodes%2520a%2520sub-string%250Acontaining%2520multiple%2520characters%2520into%2520next%2520and%2520previous%2520queries%252C%2520and%2520the%2520other%250Aemploys%2520the%2520queries%2520to%2520attend%2520to%2520the%2520image%2520features%252C%2520matching%2520the%2520sub-string%250Aand%2520simultaneously%2520recognizing%2520its%2520next%2520and%2520previous%2520character.%2520SMTR%2520can%250Arecognize%2520text%2520of%2520arbitrary%2520length%2520by%2520iterating%2520the%2520process%2520above.%2520To%2520avoid%250Abeing%2520trapped%2520in%2520recognizing%2520highly%2520similar%2520sub-strings%252C%2520we%2520introduce%2520a%250Aregularization%2520training%2520to%2520compel%2520SMTR%2520to%2520effectively%2520discover%2520subtle%250Adifferences%2520between%2520similar%2520sub-strings%2520for%2520precise%2520matching.%2520In%2520addition%252C%2520we%250Apropose%2520an%2520inference%2520augmentation%2520strategy%2520to%2520alleviate%2520confusion%2520caused%2520by%250Aidentical%2520sub-strings%2520in%2520the%2520same%2520text%2520and%2520improve%2520the%2520overall%2520recognition%250Aefficiency.%2520Extensive%2520experimental%2520results%2520reveal%2520that%2520SMTR%252C%2520even%2520when%2520trained%250Aexclusively%2520on%2520short%2520text%252C%2520outperforms%2520existing%2520methods%2520in%2520public%2520short%2520text%250Abenchmarks%2520and%2520exhibits%2520a%2520clear%2520advantage%2520on%2520LTB.%2520Code%253A%250Ahttps%253A//github.com/Topdu/OpenOCR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out%20of%20Length%20Text%20Recognition%20with%20Sub-String%20Matching&entry.906535625=Yongkun%20Du%20and%20Zhineng%20Chen%20and%20Caiyan%20Jia%20and%20Xieping%20Gao%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Scene%20Text%20Recognition%20%28STR%29%20methods%20have%20demonstrated%20robust%20performance%20in%0Aword-level%20text%20recognition.%20However%2C%20in%20real%20applications%20the%20text%20image%20is%0Asometimes%20long%20due%20to%20detected%20with%20multiple%20horizontal%20words.%20It%20triggers%20the%0Arequirement%20to%20build%20long%20text%20recognition%20models%20from%20readily%20available%20short%0A%28i.e.%2C%20word-level%29%20text%20datasets%2C%20which%20has%20been%20less%20studied%20previously.%20In%0Athis%20paper%2C%20we%20term%20this%20task%20Out%20of%20Length%20%28OOL%29%20text%20recognition.%20We%0Aestablish%20the%20first%20Long%20Text%20Benchmark%20%28LTB%29%20to%20facilitate%20the%20assessment%20of%0Adifferent%20methods%20in%20long%20text%20recognition.%20Meanwhile%2C%20we%20propose%20a%20novel%0Amethod%20called%20OOL%20Text%20Recognition%20with%20sub-String%20Matching%20%28SMTR%29.%20SMTR%0Acomprises%20two%20cross-attention-based%20modules%3A%20one%20encodes%20a%20sub-string%0Acontaining%20multiple%20characters%20into%20next%20and%20previous%20queries%2C%20and%20the%20other%0Aemploys%20the%20queries%20to%20attend%20to%20the%20image%20features%2C%20matching%20the%20sub-string%0Aand%20simultaneously%20recognizing%20its%20next%20and%20previous%20character.%20SMTR%20can%0Arecognize%20text%20of%20arbitrary%20length%20by%20iterating%20the%20process%20above.%20To%20avoid%0Abeing%20trapped%20in%20recognizing%20highly%20similar%20sub-strings%2C%20we%20introduce%20a%0Aregularization%20training%20to%20compel%20SMTR%20to%20effectively%20discover%20subtle%0Adifferences%20between%20similar%20sub-strings%20for%20precise%20matching.%20In%20addition%2C%20we%0Apropose%20an%20inference%20augmentation%20strategy%20to%20alleviate%20confusion%20caused%20by%0Aidentical%20sub-strings%20in%20the%20same%20text%20and%20improve%20the%20overall%20recognition%0Aefficiency.%20Extensive%20experimental%20results%20reveal%20that%20SMTR%2C%20even%20when%20trained%0Aexclusively%20on%20short%20text%2C%20outperforms%20existing%20methods%20in%20public%20short%20text%0Abenchmarks%20and%20exhibits%20a%20clear%20advantage%20on%20LTB.%20Code%3A%0Ahttps%3A//github.com/Topdu/OpenOCR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12317v2&entry.124074799=Read"},
{"title": "Maintaining Adversarial Robustness in Continuous Learning", "author": "Xiaolei Ru and Xiaowei Cao and Zijia Liu and Jack Murdoch Moore and Xin-Ya Zhang and Xia Zhu and Wenjia Wei and Gang Yan", "abstract": "  Adversarial robustness is essential for security and reliability of machine\nlearning systems. However, adversarial robustness enhanced by defense\nalgorithms is easily erased as the neural network's weights update to learn new\ntasks. To address this vulnerability, it is essential to improve the capability\nof neural networks in terms of robust continual learning. Specially, we propose\na novel gradient projection technique that effectively stabilizes sample\ngradients from previous data by orthogonally projecting back-propagation\ngradients onto a crucial subspace before using them for weight updates. This\ntechnique can maintaining robustness by collaborating with a class of defense\nalgorithms through sample gradient smoothing. The experimental results on four\nbenchmarks including Split-CIFAR100 and Split-miniImageNet, demonstrate that\nthe superiority of the proposed approach in mitigating rapidly degradation of\nrobustness during continual learning even when facing strong adversarial\nattacks.\n", "link": "http://arxiv.org/abs/2402.11196v2", "date": "2024-08-13", "relevancy": 2.5153, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5183}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.502}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maintaining%20Adversarial%20Robustness%20in%20Continuous%20Learning&body=Title%3A%20Maintaining%20Adversarial%20Robustness%20in%20Continuous%20Learning%0AAuthor%3A%20Xiaolei%20Ru%20and%20Xiaowei%20Cao%20and%20Zijia%20Liu%20and%20Jack%20Murdoch%20Moore%20and%20Xin-Ya%20Zhang%20and%20Xia%20Zhu%20and%20Wenjia%20Wei%20and%20Gang%20Yan%0AAbstract%3A%20%20%20Adversarial%20robustness%20is%20essential%20for%20security%20and%20reliability%20of%20machine%0Alearning%20systems.%20However%2C%20adversarial%20robustness%20enhanced%20by%20defense%0Aalgorithms%20is%20easily%20erased%20as%20the%20neural%20network%27s%20weights%20update%20to%20learn%20new%0Atasks.%20To%20address%20this%20vulnerability%2C%20it%20is%20essential%20to%20improve%20the%20capability%0Aof%20neural%20networks%20in%20terms%20of%20robust%20continual%20learning.%20Specially%2C%20we%20propose%0Aa%20novel%20gradient%20projection%20technique%20that%20effectively%20stabilizes%20sample%0Agradients%20from%20previous%20data%20by%20orthogonally%20projecting%20back-propagation%0Agradients%20onto%20a%20crucial%20subspace%20before%20using%20them%20for%20weight%20updates.%20This%0Atechnique%20can%20maintaining%20robustness%20by%20collaborating%20with%20a%20class%20of%20defense%0Aalgorithms%20through%20sample%20gradient%20smoothing.%20The%20experimental%20results%20on%20four%0Abenchmarks%20including%20Split-CIFAR100%20and%20Split-miniImageNet%2C%20demonstrate%20that%0Athe%20superiority%20of%20the%20proposed%20approach%20in%20mitigating%20rapidly%20degradation%20of%0Arobustness%20during%20continual%20learning%20even%20when%20facing%20strong%20adversarial%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaintaining%2520Adversarial%2520Robustness%2520in%2520Continuous%2520Learning%26entry.906535625%3DXiaolei%2520Ru%2520and%2520Xiaowei%2520Cao%2520and%2520Zijia%2520Liu%2520and%2520Jack%2520Murdoch%2520Moore%2520and%2520Xin-Ya%2520Zhang%2520and%2520Xia%2520Zhu%2520and%2520Wenjia%2520Wei%2520and%2520Gang%2520Yan%26entry.1292438233%3D%2520%2520Adversarial%2520robustness%2520is%2520essential%2520for%2520security%2520and%2520reliability%2520of%2520machine%250Alearning%2520systems.%2520However%252C%2520adversarial%2520robustness%2520enhanced%2520by%2520defense%250Aalgorithms%2520is%2520easily%2520erased%2520as%2520the%2520neural%2520network%2527s%2520weights%2520update%2520to%2520learn%2520new%250Atasks.%2520To%2520address%2520this%2520vulnerability%252C%2520it%2520is%2520essential%2520to%2520improve%2520the%2520capability%250Aof%2520neural%2520networks%2520in%2520terms%2520of%2520robust%2520continual%2520learning.%2520Specially%252C%2520we%2520propose%250Aa%2520novel%2520gradient%2520projection%2520technique%2520that%2520effectively%2520stabilizes%2520sample%250Agradients%2520from%2520previous%2520data%2520by%2520orthogonally%2520projecting%2520back-propagation%250Agradients%2520onto%2520a%2520crucial%2520subspace%2520before%2520using%2520them%2520for%2520weight%2520updates.%2520This%250Atechnique%2520can%2520maintaining%2520robustness%2520by%2520collaborating%2520with%2520a%2520class%2520of%2520defense%250Aalgorithms%2520through%2520sample%2520gradient%2520smoothing.%2520The%2520experimental%2520results%2520on%2520four%250Abenchmarks%2520including%2520Split-CIFAR100%2520and%2520Split-miniImageNet%252C%2520demonstrate%2520that%250Athe%2520superiority%2520of%2520the%2520proposed%2520approach%2520in%2520mitigating%2520rapidly%2520degradation%2520of%250Arobustness%2520during%2520continual%2520learning%2520even%2520when%2520facing%2520strong%2520adversarial%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maintaining%20Adversarial%20Robustness%20in%20Continuous%20Learning&entry.906535625=Xiaolei%20Ru%20and%20Xiaowei%20Cao%20and%20Zijia%20Liu%20and%20Jack%20Murdoch%20Moore%20and%20Xin-Ya%20Zhang%20and%20Xia%20Zhu%20and%20Wenjia%20Wei%20and%20Gang%20Yan&entry.1292438233=%20%20Adversarial%20robustness%20is%20essential%20for%20security%20and%20reliability%20of%20machine%0Alearning%20systems.%20However%2C%20adversarial%20robustness%20enhanced%20by%20defense%0Aalgorithms%20is%20easily%20erased%20as%20the%20neural%20network%27s%20weights%20update%20to%20learn%20new%0Atasks.%20To%20address%20this%20vulnerability%2C%20it%20is%20essential%20to%20improve%20the%20capability%0Aof%20neural%20networks%20in%20terms%20of%20robust%20continual%20learning.%20Specially%2C%20we%20propose%0Aa%20novel%20gradient%20projection%20technique%20that%20effectively%20stabilizes%20sample%0Agradients%20from%20previous%20data%20by%20orthogonally%20projecting%20back-propagation%0Agradients%20onto%20a%20crucial%20subspace%20before%20using%20them%20for%20weight%20updates.%20This%0Atechnique%20can%20maintaining%20robustness%20by%20collaborating%20with%20a%20class%20of%20defense%0Aalgorithms%20through%20sample%20gradient%20smoothing.%20The%20experimental%20results%20on%20four%0Abenchmarks%20including%20Split-CIFAR100%20and%20Split-miniImageNet%2C%20demonstrate%20that%0Athe%20superiority%20of%20the%20proposed%20approach%20in%20mitigating%20rapidly%20degradation%20of%0Arobustness%20during%20continual%20learning%20even%20when%20facing%20strong%20adversarial%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11196v2&entry.124074799=Read"},
{"title": "Oracle Bone Script Similiar Character Screening Approach Based on\n  Simsiam Contrastive Learning and Supervised Learning", "author": "Xinying Weng and Yifan Li and Shuaidong Hao and Jialiang Hou", "abstract": "  This project proposes a new method that uses fuzzy comprehensive evaluation\nmethod to integrate ResNet-50 self-supervised and RepVGG supervised learning.\nThe source image dataset HWOBC oracle is taken as input, the target image is\nselected, and finally the most similar image is output in turn without any\nmanual intervention. The same feature encoding method is not used for images of\ndifferent modalities. Before the model training, the image data is\npreprocessed, and the image is enhanced by random rotation processing,\nself-square graph equalization theory algorithm, and gamma transform, which\neffectively enhances the key feature learning. Finally, the fuzzy comprehensive\nevaluation method is used to combine the results of supervised training and\nunsupervised training, which can better solve the \"most similar\" problem that\nis difficult to quantify. At present, there are many unknown oracle-bone\ninscriptions waiting for us to crack. Contacting with the glyphs can provide\nnew ideas for cracking.\n", "link": "http://arxiv.org/abs/2408.06811v1", "date": "2024-08-13", "relevancy": 2.47, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5115}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.492}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oracle%20Bone%20Script%20Similiar%20Character%20Screening%20Approach%20Based%20on%0A%20%20Simsiam%20Contrastive%20Learning%20and%20Supervised%20Learning&body=Title%3A%20Oracle%20Bone%20Script%20Similiar%20Character%20Screening%20Approach%20Based%20on%0A%20%20Simsiam%20Contrastive%20Learning%20and%20Supervised%20Learning%0AAuthor%3A%20Xinying%20Weng%20and%20Yifan%20Li%20and%20Shuaidong%20Hao%20and%20Jialiang%20Hou%0AAbstract%3A%20%20%20This%20project%20proposes%20a%20new%20method%20that%20uses%20fuzzy%20comprehensive%20evaluation%0Amethod%20to%20integrate%20ResNet-50%20self-supervised%20and%20RepVGG%20supervised%20learning.%0AThe%20source%20image%20dataset%20HWOBC%20oracle%20is%20taken%20as%20input%2C%20the%20target%20image%20is%0Aselected%2C%20and%20finally%20the%20most%20similar%20image%20is%20output%20in%20turn%20without%20any%0Amanual%20intervention.%20The%20same%20feature%20encoding%20method%20is%20not%20used%20for%20images%20of%0Adifferent%20modalities.%20Before%20the%20model%20training%2C%20the%20image%20data%20is%0Apreprocessed%2C%20and%20the%20image%20is%20enhanced%20by%20random%20rotation%20processing%2C%0Aself-square%20graph%20equalization%20theory%20algorithm%2C%20and%20gamma%20transform%2C%20which%0Aeffectively%20enhances%20the%20key%20feature%20learning.%20Finally%2C%20the%20fuzzy%20comprehensive%0Aevaluation%20method%20is%20used%20to%20combine%20the%20results%20of%20supervised%20training%20and%0Aunsupervised%20training%2C%20which%20can%20better%20solve%20the%20%22most%20similar%22%20problem%20that%0Ais%20difficult%20to%20quantify.%20At%20present%2C%20there%20are%20many%20unknown%20oracle-bone%0Ainscriptions%20waiting%20for%20us%20to%20crack.%20Contacting%20with%20the%20glyphs%20can%20provide%0Anew%20ideas%20for%20cracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOracle%2520Bone%2520Script%2520Similiar%2520Character%2520Screening%2520Approach%2520Based%2520on%250A%2520%2520Simsiam%2520Contrastive%2520Learning%2520and%2520Supervised%2520Learning%26entry.906535625%3DXinying%2520Weng%2520and%2520Yifan%2520Li%2520and%2520Shuaidong%2520Hao%2520and%2520Jialiang%2520Hou%26entry.1292438233%3D%2520%2520This%2520project%2520proposes%2520a%2520new%2520method%2520that%2520uses%2520fuzzy%2520comprehensive%2520evaluation%250Amethod%2520to%2520integrate%2520ResNet-50%2520self-supervised%2520and%2520RepVGG%2520supervised%2520learning.%250AThe%2520source%2520image%2520dataset%2520HWOBC%2520oracle%2520is%2520taken%2520as%2520input%252C%2520the%2520target%2520image%2520is%250Aselected%252C%2520and%2520finally%2520the%2520most%2520similar%2520image%2520is%2520output%2520in%2520turn%2520without%2520any%250Amanual%2520intervention.%2520The%2520same%2520feature%2520encoding%2520method%2520is%2520not%2520used%2520for%2520images%2520of%250Adifferent%2520modalities.%2520Before%2520the%2520model%2520training%252C%2520the%2520image%2520data%2520is%250Apreprocessed%252C%2520and%2520the%2520image%2520is%2520enhanced%2520by%2520random%2520rotation%2520processing%252C%250Aself-square%2520graph%2520equalization%2520theory%2520algorithm%252C%2520and%2520gamma%2520transform%252C%2520which%250Aeffectively%2520enhances%2520the%2520key%2520feature%2520learning.%2520Finally%252C%2520the%2520fuzzy%2520comprehensive%250Aevaluation%2520method%2520is%2520used%2520to%2520combine%2520the%2520results%2520of%2520supervised%2520training%2520and%250Aunsupervised%2520training%252C%2520which%2520can%2520better%2520solve%2520the%2520%2522most%2520similar%2522%2520problem%2520that%250Ais%2520difficult%2520to%2520quantify.%2520At%2520present%252C%2520there%2520are%2520many%2520unknown%2520oracle-bone%250Ainscriptions%2520waiting%2520for%2520us%2520to%2520crack.%2520Contacting%2520with%2520the%2520glyphs%2520can%2520provide%250Anew%2520ideas%2520for%2520cracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oracle%20Bone%20Script%20Similiar%20Character%20Screening%20Approach%20Based%20on%0A%20%20Simsiam%20Contrastive%20Learning%20and%20Supervised%20Learning&entry.906535625=Xinying%20Weng%20and%20Yifan%20Li%20and%20Shuaidong%20Hao%20and%20Jialiang%20Hou&entry.1292438233=%20%20This%20project%20proposes%20a%20new%20method%20that%20uses%20fuzzy%20comprehensive%20evaluation%0Amethod%20to%20integrate%20ResNet-50%20self-supervised%20and%20RepVGG%20supervised%20learning.%0AThe%20source%20image%20dataset%20HWOBC%20oracle%20is%20taken%20as%20input%2C%20the%20target%20image%20is%0Aselected%2C%20and%20finally%20the%20most%20similar%20image%20is%20output%20in%20turn%20without%20any%0Amanual%20intervention.%20The%20same%20feature%20encoding%20method%20is%20not%20used%20for%20images%20of%0Adifferent%20modalities.%20Before%20the%20model%20training%2C%20the%20image%20data%20is%0Apreprocessed%2C%20and%20the%20image%20is%20enhanced%20by%20random%20rotation%20processing%2C%0Aself-square%20graph%20equalization%20theory%20algorithm%2C%20and%20gamma%20transform%2C%20which%0Aeffectively%20enhances%20the%20key%20feature%20learning.%20Finally%2C%20the%20fuzzy%20comprehensive%0Aevaluation%20method%20is%20used%20to%20combine%20the%20results%20of%20supervised%20training%20and%0Aunsupervised%20training%2C%20which%20can%20better%20solve%20the%20%22most%20similar%22%20problem%20that%0Ais%20difficult%20to%20quantify.%20At%20present%2C%20there%20are%20many%20unknown%20oracle-bone%0Ainscriptions%20waiting%20for%20us%20to%20crack.%20Contacting%20with%20the%20glyphs%20can%20provide%0Anew%20ideas%20for%20cracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06811v1&entry.124074799=Read"},
{"title": "GarmentCodeData: A Dataset of 3D Made-to-Measure Garments With Sewing\n  Patterns", "author": "Maria Korosteleva and Timur Levent Kesdogan and Fabian Kemper and Stephan Wenninger and Jasmin Koller and Yuhan Zhang and Mario Botsch and Olga Sorkine-Hornung", "abstract": "  Recent research interest in the learning-based processing of garments, from\nvirtual fitting to generation and reconstruction, stumbles on a scarcity of\nhigh-quality public data in the domain. We contribute to resolving this need by\npresenting the first large-scale synthetic dataset of 3D made-to-measure\ngarments with sewing patterns, as well as its generation pipeline.\nGarmentCodeData contains 115,000 data points that cover a variety of designs in\nmany common garment categories: tops, shirts, dresses, jumpsuits, skirts,\npants, etc., fitted to a variety of body shapes sampled from a custom\nstatistical body model based on CAESAR, as well as a standard reference body\nshape, applying three different textile materials. To enable the creation of\ndatasets of such complexity, we introduce a set of algorithms for automatically\ntaking tailor's measures on sampled body shapes, sampling strategies for sewing\npattern design, and propose an automatic, open-source 3D garment draping\npipeline based on a fast XPBD simulator, while contributing several solutions\nfor collision resolution and drape correctness to enable scalability.\n  Project Page: https://igl.ethz.ch/projects/GarmentCodeData/\n  Dataset: https://doi.org/10.3929/ethz-b-000673889\n", "link": "http://arxiv.org/abs/2405.17609v2", "date": "2024-08-13", "relevancy": 2.4339, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7556}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5573}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GarmentCodeData%3A%20A%20Dataset%20of%203D%20Made-to-Measure%20Garments%20With%20Sewing%0A%20%20Patterns&body=Title%3A%20GarmentCodeData%3A%20A%20Dataset%20of%203D%20Made-to-Measure%20Garments%20With%20Sewing%0A%20%20Patterns%0AAuthor%3A%20Maria%20Korosteleva%20and%20Timur%20Levent%20Kesdogan%20and%20Fabian%20Kemper%20and%20Stephan%20Wenninger%20and%20Jasmin%20Koller%20and%20Yuhan%20Zhang%20and%20Mario%20Botsch%20and%20Olga%20Sorkine-Hornung%0AAbstract%3A%20%20%20Recent%20research%20interest%20in%20the%20learning-based%20processing%20of%20garments%2C%20from%0Avirtual%20fitting%20to%20generation%20and%20reconstruction%2C%20stumbles%20on%20a%20scarcity%20of%0Ahigh-quality%20public%20data%20in%20the%20domain.%20We%20contribute%20to%20resolving%20this%20need%20by%0Apresenting%20the%20first%20large-scale%20synthetic%20dataset%20of%203D%20made-to-measure%0Agarments%20with%20sewing%20patterns%2C%20as%20well%20as%20its%20generation%20pipeline.%0AGarmentCodeData%20contains%20115%2C000%20data%20points%20that%20cover%20a%20variety%20of%20designs%20in%0Amany%20common%20garment%20categories%3A%20tops%2C%20shirts%2C%20dresses%2C%20jumpsuits%2C%20skirts%2C%0Apants%2C%20etc.%2C%20fitted%20to%20a%20variety%20of%20body%20shapes%20sampled%20from%20a%20custom%0Astatistical%20body%20model%20based%20on%20CAESAR%2C%20as%20well%20as%20a%20standard%20reference%20body%0Ashape%2C%20applying%20three%20different%20textile%20materials.%20To%20enable%20the%20creation%20of%0Adatasets%20of%20such%20complexity%2C%20we%20introduce%20a%20set%20of%20algorithms%20for%20automatically%0Ataking%20tailor%27s%20measures%20on%20sampled%20body%20shapes%2C%20sampling%20strategies%20for%20sewing%0Apattern%20design%2C%20and%20propose%20an%20automatic%2C%20open-source%203D%20garment%20draping%0Apipeline%20based%20on%20a%20fast%20XPBD%20simulator%2C%20while%20contributing%20several%20solutions%0Afor%20collision%20resolution%20and%20drape%20correctness%20to%20enable%20scalability.%0A%20%20Project%20Page%3A%20https%3A//igl.ethz.ch/projects/GarmentCodeData/%0A%20%20Dataset%3A%20https%3A//doi.org/10.3929/ethz-b-000673889%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17609v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarmentCodeData%253A%2520A%2520Dataset%2520of%25203D%2520Made-to-Measure%2520Garments%2520With%2520Sewing%250A%2520%2520Patterns%26entry.906535625%3DMaria%2520Korosteleva%2520and%2520Timur%2520Levent%2520Kesdogan%2520and%2520Fabian%2520Kemper%2520and%2520Stephan%2520Wenninger%2520and%2520Jasmin%2520Koller%2520and%2520Yuhan%2520Zhang%2520and%2520Mario%2520Botsch%2520and%2520Olga%2520Sorkine-Hornung%26entry.1292438233%3D%2520%2520Recent%2520research%2520interest%2520in%2520the%2520learning-based%2520processing%2520of%2520garments%252C%2520from%250Avirtual%2520fitting%2520to%2520generation%2520and%2520reconstruction%252C%2520stumbles%2520on%2520a%2520scarcity%2520of%250Ahigh-quality%2520public%2520data%2520in%2520the%2520domain.%2520We%2520contribute%2520to%2520resolving%2520this%2520need%2520by%250Apresenting%2520the%2520first%2520large-scale%2520synthetic%2520dataset%2520of%25203D%2520made-to-measure%250Agarments%2520with%2520sewing%2520patterns%252C%2520as%2520well%2520as%2520its%2520generation%2520pipeline.%250AGarmentCodeData%2520contains%2520115%252C000%2520data%2520points%2520that%2520cover%2520a%2520variety%2520of%2520designs%2520in%250Amany%2520common%2520garment%2520categories%253A%2520tops%252C%2520shirts%252C%2520dresses%252C%2520jumpsuits%252C%2520skirts%252C%250Apants%252C%2520etc.%252C%2520fitted%2520to%2520a%2520variety%2520of%2520body%2520shapes%2520sampled%2520from%2520a%2520custom%250Astatistical%2520body%2520model%2520based%2520on%2520CAESAR%252C%2520as%2520well%2520as%2520a%2520standard%2520reference%2520body%250Ashape%252C%2520applying%2520three%2520different%2520textile%2520materials.%2520To%2520enable%2520the%2520creation%2520of%250Adatasets%2520of%2520such%2520complexity%252C%2520we%2520introduce%2520a%2520set%2520of%2520algorithms%2520for%2520automatically%250Ataking%2520tailor%2527s%2520measures%2520on%2520sampled%2520body%2520shapes%252C%2520sampling%2520strategies%2520for%2520sewing%250Apattern%2520design%252C%2520and%2520propose%2520an%2520automatic%252C%2520open-source%25203D%2520garment%2520draping%250Apipeline%2520based%2520on%2520a%2520fast%2520XPBD%2520simulator%252C%2520while%2520contributing%2520several%2520solutions%250Afor%2520collision%2520resolution%2520and%2520drape%2520correctness%2520to%2520enable%2520scalability.%250A%2520%2520Project%2520Page%253A%2520https%253A//igl.ethz.ch/projects/GarmentCodeData/%250A%2520%2520Dataset%253A%2520https%253A//doi.org/10.3929/ethz-b-000673889%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17609v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GarmentCodeData%3A%20A%20Dataset%20of%203D%20Made-to-Measure%20Garments%20With%20Sewing%0A%20%20Patterns&entry.906535625=Maria%20Korosteleva%20and%20Timur%20Levent%20Kesdogan%20and%20Fabian%20Kemper%20and%20Stephan%20Wenninger%20and%20Jasmin%20Koller%20and%20Yuhan%20Zhang%20and%20Mario%20Botsch%20and%20Olga%20Sorkine-Hornung&entry.1292438233=%20%20Recent%20research%20interest%20in%20the%20learning-based%20processing%20of%20garments%2C%20from%0Avirtual%20fitting%20to%20generation%20and%20reconstruction%2C%20stumbles%20on%20a%20scarcity%20of%0Ahigh-quality%20public%20data%20in%20the%20domain.%20We%20contribute%20to%20resolving%20this%20need%20by%0Apresenting%20the%20first%20large-scale%20synthetic%20dataset%20of%203D%20made-to-measure%0Agarments%20with%20sewing%20patterns%2C%20as%20well%20as%20its%20generation%20pipeline.%0AGarmentCodeData%20contains%20115%2C000%20data%20points%20that%20cover%20a%20variety%20of%20designs%20in%0Amany%20common%20garment%20categories%3A%20tops%2C%20shirts%2C%20dresses%2C%20jumpsuits%2C%20skirts%2C%0Apants%2C%20etc.%2C%20fitted%20to%20a%20variety%20of%20body%20shapes%20sampled%20from%20a%20custom%0Astatistical%20body%20model%20based%20on%20CAESAR%2C%20as%20well%20as%20a%20standard%20reference%20body%0Ashape%2C%20applying%20three%20different%20textile%20materials.%20To%20enable%20the%20creation%20of%0Adatasets%20of%20such%20complexity%2C%20we%20introduce%20a%20set%20of%20algorithms%20for%20automatically%0Ataking%20tailor%27s%20measures%20on%20sampled%20body%20shapes%2C%20sampling%20strategies%20for%20sewing%0Apattern%20design%2C%20and%20propose%20an%20automatic%2C%20open-source%203D%20garment%20draping%0Apipeline%20based%20on%20a%20fast%20XPBD%20simulator%2C%20while%20contributing%20several%20solutions%0Afor%20collision%20resolution%20and%20drape%20correctness%20to%20enable%20scalability.%0A%20%20Project%20Page%3A%20https%3A//igl.ethz.ch/projects/GarmentCodeData/%0A%20%20Dataset%3A%20https%3A//doi.org/10.3929/ethz-b-000673889%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17609v2&entry.124074799=Read"},
{"title": "Prioritize Alignment in Dataset Distillation", "author": "Zekai Li and Ziyao Guo and Wangbo Zhao and Tianle Zhang and Zhi-Qi Cheng and Samir Khaki and Kaipeng Zhang and Ahmad Sajedi and Konstantinos N Plataniotis and Kai Wang and Yang You", "abstract": "  Dataset Distillation aims to compress a large dataset into a significantly\nmore compact, synthetic one without compromising the performance of the trained\nmodels. To achieve this, existing methods use the agent model to extract\ninformation from the target dataset and embed it into the distilled dataset.\nConsequently, the quality of extracted and embedded information determines the\nquality of the distilled dataset. In this work, we find that existing methods\nintroduce misaligned information in both information extraction and embedding\nstages. To alleviate this, we propose Prioritize Alignment in Dataset\nDistillation (PAD), which aligns information from the following two\nperspectives. 1) We prune the target dataset according to the compressing ratio\nto filter the information that can be extracted by the agent model. 2) We use\nonly deep layers of the agent model to perform the distillation to avoid\nexcessively introducing low-level information. This simple strategy effectively\nfilters out misaligned information and brings non-trivial improvement for\nmainstream matching-based distillation algorithms. Furthermore, built on\ntrajectory matching, \\textbf{PAD} achieves remarkable improvements on various\nbenchmarks, achieving state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2408.03360v2", "date": "2024-08-13", "relevancy": 2.4295, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4915}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4854}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prioritize%20Alignment%20in%20Dataset%20Distillation&body=Title%3A%20Prioritize%20Alignment%20in%20Dataset%20Distillation%0AAuthor%3A%20Zekai%20Li%20and%20Ziyao%20Guo%20and%20Wangbo%20Zhao%20and%20Tianle%20Zhang%20and%20Zhi-Qi%20Cheng%20and%20Samir%20Khaki%20and%20Kaipeng%20Zhang%20and%20Ahmad%20Sajedi%20and%20Konstantinos%20N%20Plataniotis%20and%20Kai%20Wang%20and%20Yang%20You%0AAbstract%3A%20%20%20Dataset%20Distillation%20aims%20to%20compress%20a%20large%20dataset%20into%20a%20significantly%0Amore%20compact%2C%20synthetic%20one%20without%20compromising%20the%20performance%20of%20the%20trained%0Amodels.%20To%20achieve%20this%2C%20existing%20methods%20use%20the%20agent%20model%20to%20extract%0Ainformation%20from%20the%20target%20dataset%20and%20embed%20it%20into%20the%20distilled%20dataset.%0AConsequently%2C%20the%20quality%20of%20extracted%20and%20embedded%20information%20determines%20the%0Aquality%20of%20the%20distilled%20dataset.%20In%20this%20work%2C%20we%20find%20that%20existing%20methods%0Aintroduce%20misaligned%20information%20in%20both%20information%20extraction%20and%20embedding%0Astages.%20To%20alleviate%20this%2C%20we%20propose%20Prioritize%20Alignment%20in%20Dataset%0ADistillation%20%28PAD%29%2C%20which%20aligns%20information%20from%20the%20following%20two%0Aperspectives.%201%29%20We%20prune%20the%20target%20dataset%20according%20to%20the%20compressing%20ratio%0Ato%20filter%20the%20information%20that%20can%20be%20extracted%20by%20the%20agent%20model.%202%29%20We%20use%0Aonly%20deep%20layers%20of%20the%20agent%20model%20to%20perform%20the%20distillation%20to%20avoid%0Aexcessively%20introducing%20low-level%20information.%20This%20simple%20strategy%20effectively%0Afilters%20out%20misaligned%20information%20and%20brings%20non-trivial%20improvement%20for%0Amainstream%20matching-based%20distillation%20algorithms.%20Furthermore%2C%20built%20on%0Atrajectory%20matching%2C%20%5Ctextbf%7BPAD%7D%20achieves%20remarkable%20improvements%20on%20various%0Abenchmarks%2C%20achieving%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrioritize%2520Alignment%2520in%2520Dataset%2520Distillation%26entry.906535625%3DZekai%2520Li%2520and%2520Ziyao%2520Guo%2520and%2520Wangbo%2520Zhao%2520and%2520Tianle%2520Zhang%2520and%2520Zhi-Qi%2520Cheng%2520and%2520Samir%2520Khaki%2520and%2520Kaipeng%2520Zhang%2520and%2520Ahmad%2520Sajedi%2520and%2520Konstantinos%2520N%2520Plataniotis%2520and%2520Kai%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Dataset%2520Distillation%2520aims%2520to%2520compress%2520a%2520large%2520dataset%2520into%2520a%2520significantly%250Amore%2520compact%252C%2520synthetic%2520one%2520without%2520compromising%2520the%2520performance%2520of%2520the%2520trained%250Amodels.%2520To%2520achieve%2520this%252C%2520existing%2520methods%2520use%2520the%2520agent%2520model%2520to%2520extract%250Ainformation%2520from%2520the%2520target%2520dataset%2520and%2520embed%2520it%2520into%2520the%2520distilled%2520dataset.%250AConsequently%252C%2520the%2520quality%2520of%2520extracted%2520and%2520embedded%2520information%2520determines%2520the%250Aquality%2520of%2520the%2520distilled%2520dataset.%2520In%2520this%2520work%252C%2520we%2520find%2520that%2520existing%2520methods%250Aintroduce%2520misaligned%2520information%2520in%2520both%2520information%2520extraction%2520and%2520embedding%250Astages.%2520To%2520alleviate%2520this%252C%2520we%2520propose%2520Prioritize%2520Alignment%2520in%2520Dataset%250ADistillation%2520%2528PAD%2529%252C%2520which%2520aligns%2520information%2520from%2520the%2520following%2520two%250Aperspectives.%25201%2529%2520We%2520prune%2520the%2520target%2520dataset%2520according%2520to%2520the%2520compressing%2520ratio%250Ato%2520filter%2520the%2520information%2520that%2520can%2520be%2520extracted%2520by%2520the%2520agent%2520model.%25202%2529%2520We%2520use%250Aonly%2520deep%2520layers%2520of%2520the%2520agent%2520model%2520to%2520perform%2520the%2520distillation%2520to%2520avoid%250Aexcessively%2520introducing%2520low-level%2520information.%2520This%2520simple%2520strategy%2520effectively%250Afilters%2520out%2520misaligned%2520information%2520and%2520brings%2520non-trivial%2520improvement%2520for%250Amainstream%2520matching-based%2520distillation%2520algorithms.%2520Furthermore%252C%2520built%2520on%250Atrajectory%2520matching%252C%2520%255Ctextbf%257BPAD%257D%2520achieves%2520remarkable%2520improvements%2520on%2520various%250Abenchmarks%252C%2520achieving%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prioritize%20Alignment%20in%20Dataset%20Distillation&entry.906535625=Zekai%20Li%20and%20Ziyao%20Guo%20and%20Wangbo%20Zhao%20and%20Tianle%20Zhang%20and%20Zhi-Qi%20Cheng%20and%20Samir%20Khaki%20and%20Kaipeng%20Zhang%20and%20Ahmad%20Sajedi%20and%20Konstantinos%20N%20Plataniotis%20and%20Kai%20Wang%20and%20Yang%20You&entry.1292438233=%20%20Dataset%20Distillation%20aims%20to%20compress%20a%20large%20dataset%20into%20a%20significantly%0Amore%20compact%2C%20synthetic%20one%20without%20compromising%20the%20performance%20of%20the%20trained%0Amodels.%20To%20achieve%20this%2C%20existing%20methods%20use%20the%20agent%20model%20to%20extract%0Ainformation%20from%20the%20target%20dataset%20and%20embed%20it%20into%20the%20distilled%20dataset.%0AConsequently%2C%20the%20quality%20of%20extracted%20and%20embedded%20information%20determines%20the%0Aquality%20of%20the%20distilled%20dataset.%20In%20this%20work%2C%20we%20find%20that%20existing%20methods%0Aintroduce%20misaligned%20information%20in%20both%20information%20extraction%20and%20embedding%0Astages.%20To%20alleviate%20this%2C%20we%20propose%20Prioritize%20Alignment%20in%20Dataset%0ADistillation%20%28PAD%29%2C%20which%20aligns%20information%20from%20the%20following%20two%0Aperspectives.%201%29%20We%20prune%20the%20target%20dataset%20according%20to%20the%20compressing%20ratio%0Ato%20filter%20the%20information%20that%20can%20be%20extracted%20by%20the%20agent%20model.%202%29%20We%20use%0Aonly%20deep%20layers%20of%20the%20agent%20model%20to%20perform%20the%20distillation%20to%20avoid%0Aexcessively%20introducing%20low-level%20information.%20This%20simple%20strategy%20effectively%0Afilters%20out%20misaligned%20information%20and%20brings%20non-trivial%20improvement%20for%0Amainstream%20matching-based%20distillation%20algorithms.%20Furthermore%2C%20built%20on%0Atrajectory%20matching%2C%20%5Ctextbf%7BPAD%7D%20achieves%20remarkable%20improvements%20on%20various%0Abenchmarks%2C%20achieving%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03360v2&entry.124074799=Read"},
{"title": "V4d: voxel for 4d novel view synthesis", "author": "Wanshui Gan and Hongbin Xu and Yi Huang and Shifeng Chen and Naoto Yokoya", "abstract": "  Neural radiance fields have made a remarkable breakthrough in the novel view\nsynthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,\ndynamic scene), the performance of the existing method is still limited by the\ncapacity of the neural network, typically in a multilayer perceptron network\n(MLP). In this paper, we utilize 3D Voxel to model the 4D neural radiance\nfield, short as V4D, where the 3D voxel has two formats. The first one is to\nregularly model the 3D space and then use the sampled local 3D feature with the\ntime index to model the density field and the texture field by a tiny MLP. The\nsecond one is in look-up tables (LUTs) format that is for the pixel-level\nrefinement, where the pseudo-surface produced by the volume rendering is\nutilized as the guidance information to learn a 2D pixel-level refinement\nmapping. The proposed LUTs-based refinement module achieves the performance\ngain with little computational cost and could serve as the plug-and-play module\nin the novel view synthesis task. Moreover, we propose a more effective\nconditional positional encoding toward the 4D data that achieves performance\ngain with negligible computational burdens. Extensive experiments demonstrate\nthat the proposed method achieves state-of-the-art performance at a low\ncomputational cost.\n", "link": "http://arxiv.org/abs/2205.14332v4", "date": "2024-08-13", "relevancy": 2.4206, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6114}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6114}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V4d%3A%20voxel%20for%204d%20novel%20view%20synthesis&body=Title%3A%20V4d%3A%20voxel%20for%204d%20novel%20view%20synthesis%0AAuthor%3A%20Wanshui%20Gan%20and%20Hongbin%20Xu%20and%20Yi%20Huang%20and%20Shifeng%20Chen%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Neural%20radiance%20fields%20have%20made%20a%20remarkable%20breakthrough%20in%20the%20novel%20view%0Asynthesis%20task%20at%20the%203D%20static%20scene.%20However%2C%20for%20the%204D%20circumstance%20%28e.g.%2C%0Adynamic%20scene%29%2C%20the%20performance%20of%20the%20existing%20method%20is%20still%20limited%20by%20the%0Acapacity%20of%20the%20neural%20network%2C%20typically%20in%20a%20multilayer%20perceptron%20network%0A%28MLP%29.%20In%20this%20paper%2C%20we%20utilize%203D%20Voxel%20to%20model%20the%204D%20neural%20radiance%0Afield%2C%20short%20as%20V4D%2C%20where%20the%203D%20voxel%20has%20two%20formats.%20The%20first%20one%20is%20to%0Aregularly%20model%20the%203D%20space%20and%20then%20use%20the%20sampled%20local%203D%20feature%20with%20the%0Atime%20index%20to%20model%20the%20density%20field%20and%20the%20texture%20field%20by%20a%20tiny%20MLP.%20The%0Asecond%20one%20is%20in%20look-up%20tables%20%28LUTs%29%20format%20that%20is%20for%20the%20pixel-level%0Arefinement%2C%20where%20the%20pseudo-surface%20produced%20by%20the%20volume%20rendering%20is%0Autilized%20as%20the%20guidance%20information%20to%20learn%20a%202D%20pixel-level%20refinement%0Amapping.%20The%20proposed%20LUTs-based%20refinement%20module%20achieves%20the%20performance%0Again%20with%20little%20computational%20cost%20and%20could%20serve%20as%20the%20plug-and-play%20module%0Ain%20the%20novel%20view%20synthesis%20task.%20Moreover%2C%20we%20propose%20a%20more%20effective%0Aconditional%20positional%20encoding%20toward%20the%204D%20data%20that%20achieves%20performance%0Again%20with%20negligible%20computational%20burdens.%20Extensive%20experiments%20demonstrate%0Athat%20the%20proposed%20method%20achieves%20state-of-the-art%20performance%20at%20a%20low%0Acomputational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.14332v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV4d%253A%2520voxel%2520for%25204d%2520novel%2520view%2520synthesis%26entry.906535625%3DWanshui%2520Gan%2520and%2520Hongbin%2520Xu%2520and%2520Yi%2520Huang%2520and%2520Shifeng%2520Chen%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Neural%2520radiance%2520fields%2520have%2520made%2520a%2520remarkable%2520breakthrough%2520in%2520the%2520novel%2520view%250Asynthesis%2520task%2520at%2520the%25203D%2520static%2520scene.%2520However%252C%2520for%2520the%25204D%2520circumstance%2520%2528e.g.%252C%250Adynamic%2520scene%2529%252C%2520the%2520performance%2520of%2520the%2520existing%2520method%2520is%2520still%2520limited%2520by%2520the%250Acapacity%2520of%2520the%2520neural%2520network%252C%2520typically%2520in%2520a%2520multilayer%2520perceptron%2520network%250A%2528MLP%2529.%2520In%2520this%2520paper%252C%2520we%2520utilize%25203D%2520Voxel%2520to%2520model%2520the%25204D%2520neural%2520radiance%250Afield%252C%2520short%2520as%2520V4D%252C%2520where%2520the%25203D%2520voxel%2520has%2520two%2520formats.%2520The%2520first%2520one%2520is%2520to%250Aregularly%2520model%2520the%25203D%2520space%2520and%2520then%2520use%2520the%2520sampled%2520local%25203D%2520feature%2520with%2520the%250Atime%2520index%2520to%2520model%2520the%2520density%2520field%2520and%2520the%2520texture%2520field%2520by%2520a%2520tiny%2520MLP.%2520The%250Asecond%2520one%2520is%2520in%2520look-up%2520tables%2520%2528LUTs%2529%2520format%2520that%2520is%2520for%2520the%2520pixel-level%250Arefinement%252C%2520where%2520the%2520pseudo-surface%2520produced%2520by%2520the%2520volume%2520rendering%2520is%250Autilized%2520as%2520the%2520guidance%2520information%2520to%2520learn%2520a%25202D%2520pixel-level%2520refinement%250Amapping.%2520The%2520proposed%2520LUTs-based%2520refinement%2520module%2520achieves%2520the%2520performance%250Again%2520with%2520little%2520computational%2520cost%2520and%2520could%2520serve%2520as%2520the%2520plug-and-play%2520module%250Ain%2520the%2520novel%2520view%2520synthesis%2520task.%2520Moreover%252C%2520we%2520propose%2520a%2520more%2520effective%250Aconditional%2520positional%2520encoding%2520toward%2520the%25204D%2520data%2520that%2520achieves%2520performance%250Again%2520with%2520negligible%2520computational%2520burdens.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520the%2520proposed%2520method%2520achieves%2520state-of-the-art%2520performance%2520at%2520a%2520low%250Acomputational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.14332v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V4d%3A%20voxel%20for%204d%20novel%20view%20synthesis&entry.906535625=Wanshui%20Gan%20and%20Hongbin%20Xu%20and%20Yi%20Huang%20and%20Shifeng%20Chen%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Neural%20radiance%20fields%20have%20made%20a%20remarkable%20breakthrough%20in%20the%20novel%20view%0Asynthesis%20task%20at%20the%203D%20static%20scene.%20However%2C%20for%20the%204D%20circumstance%20%28e.g.%2C%0Adynamic%20scene%29%2C%20the%20performance%20of%20the%20existing%20method%20is%20still%20limited%20by%20the%0Acapacity%20of%20the%20neural%20network%2C%20typically%20in%20a%20multilayer%20perceptron%20network%0A%28MLP%29.%20In%20this%20paper%2C%20we%20utilize%203D%20Voxel%20to%20model%20the%204D%20neural%20radiance%0Afield%2C%20short%20as%20V4D%2C%20where%20the%203D%20voxel%20has%20two%20formats.%20The%20first%20one%20is%20to%0Aregularly%20model%20the%203D%20space%20and%20then%20use%20the%20sampled%20local%203D%20feature%20with%20the%0Atime%20index%20to%20model%20the%20density%20field%20and%20the%20texture%20field%20by%20a%20tiny%20MLP.%20The%0Asecond%20one%20is%20in%20look-up%20tables%20%28LUTs%29%20format%20that%20is%20for%20the%20pixel-level%0Arefinement%2C%20where%20the%20pseudo-surface%20produced%20by%20the%20volume%20rendering%20is%0Autilized%20as%20the%20guidance%20information%20to%20learn%20a%202D%20pixel-level%20refinement%0Amapping.%20The%20proposed%20LUTs-based%20refinement%20module%20achieves%20the%20performance%0Again%20with%20little%20computational%20cost%20and%20could%20serve%20as%20the%20plug-and-play%20module%0Ain%20the%20novel%20view%20synthesis%20task.%20Moreover%2C%20we%20propose%20a%20more%20effective%0Aconditional%20positional%20encoding%20toward%20the%204D%20data%20that%20achieves%20performance%0Again%20with%20negligible%20computational%20burdens.%20Extensive%20experiments%20demonstrate%0Athat%20the%20proposed%20method%20achieves%20state-of-the-art%20performance%20at%20a%20low%0Acomputational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.14332v4&entry.124074799=Read"},
{"title": "EE3P3D: Event-based Estimation of Periodic Phenomena Frequency using 3D\n  Correlation", "author": "Jakub Kol\u00e1\u0159 and Radim \u0160petl\u00edk and Ji\u0159\u00ed Matas", "abstract": "  We present a novel method for measuring the frequency of periodic phenomena,\ne.g., rotation, flicker and vibration, by an event camera, a device\nasynchronously reporting brightness changes at independently operating pixels\nwith high temporal resolution. The approach assumes that for a periodic\nphenomenon, a highly similar set of events is generated within a specific\nspatio-temporal window at a time difference corresponding to the phenomenon's\nperiod. The sets of similar events are detected by 3D spatio-temporal\ncorrelation in the event stream space. The proposed method, EE3P3D, is\nevaluated on a dataset of 12 sequences of periodic phenomena, i.e. flashing\nlight and vibration, and periodic motion, e.g., rotation, ranging from 3.2 Hz\nto 2 kHz (equivalent to 192 - 120 000 RPM). EE3P3D significantly outperforms\npublished methods on this dataset, achieving a mean relative error of 0.1%.\n", "link": "http://arxiv.org/abs/2408.06899v1", "date": "2024-08-13", "relevancy": 2.4155, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4893}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4893}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EE3P3D%3A%20Event-based%20Estimation%20of%20Periodic%20Phenomena%20Frequency%20using%203D%0A%20%20Correlation&body=Title%3A%20EE3P3D%3A%20Event-based%20Estimation%20of%20Periodic%20Phenomena%20Frequency%20using%203D%0A%20%20Correlation%0AAuthor%3A%20Jakub%20Kol%C3%A1%C5%99%20and%20Radim%20%C5%A0petl%C3%ADk%20and%20Ji%C5%99%C3%AD%20Matas%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20measuring%20the%20frequency%20of%20periodic%20phenomena%2C%0Ae.g.%2C%20rotation%2C%20flicker%20and%20vibration%2C%20by%20an%20event%20camera%2C%20a%20device%0Aasynchronously%20reporting%20brightness%20changes%20at%20independently%20operating%20pixels%0Awith%20high%20temporal%20resolution.%20The%20approach%20assumes%20that%20for%20a%20periodic%0Aphenomenon%2C%20a%20highly%20similar%20set%20of%20events%20is%20generated%20within%20a%20specific%0Aspatio-temporal%20window%20at%20a%20time%20difference%20corresponding%20to%20the%20phenomenon%27s%0Aperiod.%20The%20sets%20of%20similar%20events%20are%20detected%20by%203D%20spatio-temporal%0Acorrelation%20in%20the%20event%20stream%20space.%20The%20proposed%20method%2C%20EE3P3D%2C%20is%0Aevaluated%20on%20a%20dataset%20of%2012%20sequences%20of%20periodic%20phenomena%2C%20i.e.%20flashing%0Alight%20and%20vibration%2C%20and%20periodic%20motion%2C%20e.g.%2C%20rotation%2C%20ranging%20from%203.2%20Hz%0Ato%202%20kHz%20%28equivalent%20to%20192%20-%20120%20000%20RPM%29.%20EE3P3D%20significantly%20outperforms%0Apublished%20methods%20on%20this%20dataset%2C%20achieving%20a%20mean%20relative%20error%20of%200.1%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEE3P3D%253A%2520Event-based%2520Estimation%2520of%2520Periodic%2520Phenomena%2520Frequency%2520using%25203D%250A%2520%2520Correlation%26entry.906535625%3DJakub%2520Kol%25C3%25A1%25C5%2599%2520and%2520Radim%2520%25C5%25A0petl%25C3%25ADk%2520and%2520Ji%25C5%2599%25C3%25AD%2520Matas%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520for%2520measuring%2520the%2520frequency%2520of%2520periodic%2520phenomena%252C%250Ae.g.%252C%2520rotation%252C%2520flicker%2520and%2520vibration%252C%2520by%2520an%2520event%2520camera%252C%2520a%2520device%250Aasynchronously%2520reporting%2520brightness%2520changes%2520at%2520independently%2520operating%2520pixels%250Awith%2520high%2520temporal%2520resolution.%2520The%2520approach%2520assumes%2520that%2520for%2520a%2520periodic%250Aphenomenon%252C%2520a%2520highly%2520similar%2520set%2520of%2520events%2520is%2520generated%2520within%2520a%2520specific%250Aspatio-temporal%2520window%2520at%2520a%2520time%2520difference%2520corresponding%2520to%2520the%2520phenomenon%2527s%250Aperiod.%2520The%2520sets%2520of%2520similar%2520events%2520are%2520detected%2520by%25203D%2520spatio-temporal%250Acorrelation%2520in%2520the%2520event%2520stream%2520space.%2520The%2520proposed%2520method%252C%2520EE3P3D%252C%2520is%250Aevaluated%2520on%2520a%2520dataset%2520of%252012%2520sequences%2520of%2520periodic%2520phenomena%252C%2520i.e.%2520flashing%250Alight%2520and%2520vibration%252C%2520and%2520periodic%2520motion%252C%2520e.g.%252C%2520rotation%252C%2520ranging%2520from%25203.2%2520Hz%250Ato%25202%2520kHz%2520%2528equivalent%2520to%2520192%2520-%2520120%2520000%2520RPM%2529.%2520EE3P3D%2520significantly%2520outperforms%250Apublished%2520methods%2520on%2520this%2520dataset%252C%2520achieving%2520a%2520mean%2520relative%2520error%2520of%25200.1%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EE3P3D%3A%20Event-based%20Estimation%20of%20Periodic%20Phenomena%20Frequency%20using%203D%0A%20%20Correlation&entry.906535625=Jakub%20Kol%C3%A1%C5%99%20and%20Radim%20%C5%A0petl%C3%ADk%20and%20Ji%C5%99%C3%AD%20Matas&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20measuring%20the%20frequency%20of%20periodic%20phenomena%2C%0Ae.g.%2C%20rotation%2C%20flicker%20and%20vibration%2C%20by%20an%20event%20camera%2C%20a%20device%0Aasynchronously%20reporting%20brightness%20changes%20at%20independently%20operating%20pixels%0Awith%20high%20temporal%20resolution.%20The%20approach%20assumes%20that%20for%20a%20periodic%0Aphenomenon%2C%20a%20highly%20similar%20set%20of%20events%20is%20generated%20within%20a%20specific%0Aspatio-temporal%20window%20at%20a%20time%20difference%20corresponding%20to%20the%20phenomenon%27s%0Aperiod.%20The%20sets%20of%20similar%20events%20are%20detected%20by%203D%20spatio-temporal%0Acorrelation%20in%20the%20event%20stream%20space.%20The%20proposed%20method%2C%20EE3P3D%2C%20is%0Aevaluated%20on%20a%20dataset%20of%2012%20sequences%20of%20periodic%20phenomena%2C%20i.e.%20flashing%0Alight%20and%20vibration%2C%20and%20periodic%20motion%2C%20e.g.%2C%20rotation%2C%20ranging%20from%203.2%20Hz%0Ato%202%20kHz%20%28equivalent%20to%20192%20-%20120%20000%20RPM%29.%20EE3P3D%20significantly%20outperforms%0Apublished%20methods%20on%20this%20dataset%2C%20achieving%20a%20mean%20relative%20error%20of%200.1%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06899v1&entry.124074799=Read"},
{"title": "Neural Quantile Optimization for Edge-Cloud Networking", "author": "Bin Du and He Zhang and Xiangle Cheng and Lei Zhang", "abstract": "  We seek the best traffic allocation scheme for the edge-cloud computing\nnetwork that satisfies constraints and minimizes the cost based on burstable\nbilling. First, for a fixed network topology, we formulate a family of integer\nprogramming problems with random parameters describing the various traffic\ndemands. Then, to overcome the difficulty caused by the discrete feature of the\nproblem, we generalize the Gumbel-softmax reparameterization method to induce\nan unconstrained continuous optimization problem as a regularized continuation\nof the discrete problem. Finally, we introduce the Gumbel-softmax sampling\nnetwork to solve the optimization problems via unsupervised learning. The\nnetwork structure reflects the edge-cloud computing topology and is trained to\nminimize the expectation of the cost function for unconstrained continuous\noptimization problems. The trained network works as an efficient traffic\nallocation scheme sampler, remarkably outperforming the random strategy in\nfeasibility and cost function value. Besides testing the quality of the output\nallocation scheme, we examine the generalization property of the network by\nincreasing the time steps and the number of users. We also feed the solution to\nexisting integer optimization solvers as initial conditions and verify the\nwarm-starts can accelerate the short-time iteration process. The framework is\ngeneral with solid performance, and the decoupled feature of the random neural\nnetworks is adequate for practical implementations.\n", "link": "http://arxiv.org/abs/2307.05170v2", "date": "2024-08-13", "relevancy": 2.3836, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5336}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4508}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Quantile%20Optimization%20for%20Edge-Cloud%20Networking&body=Title%3A%20Neural%20Quantile%20Optimization%20for%20Edge-Cloud%20Networking%0AAuthor%3A%20Bin%20Du%20and%20He%20Zhang%20and%20Xiangle%20Cheng%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20We%20seek%20the%20best%20traffic%20allocation%20scheme%20for%20the%20edge-cloud%20computing%0Anetwork%20that%20satisfies%20constraints%20and%20minimizes%20the%20cost%20based%20on%20burstable%0Abilling.%20First%2C%20for%20a%20fixed%20network%20topology%2C%20we%20formulate%20a%20family%20of%20integer%0Aprogramming%20problems%20with%20random%20parameters%20describing%20the%20various%20traffic%0Ademands.%20Then%2C%20to%20overcome%20the%20difficulty%20caused%20by%20the%20discrete%20feature%20of%20the%0Aproblem%2C%20we%20generalize%20the%20Gumbel-softmax%20reparameterization%20method%20to%20induce%0Aan%20unconstrained%20continuous%20optimization%20problem%20as%20a%20regularized%20continuation%0Aof%20the%20discrete%20problem.%20Finally%2C%20we%20introduce%20the%20Gumbel-softmax%20sampling%0Anetwork%20to%20solve%20the%20optimization%20problems%20via%20unsupervised%20learning.%20The%0Anetwork%20structure%20reflects%20the%20edge-cloud%20computing%20topology%20and%20is%20trained%20to%0Aminimize%20the%20expectation%20of%20the%20cost%20function%20for%20unconstrained%20continuous%0Aoptimization%20problems.%20The%20trained%20network%20works%20as%20an%20efficient%20traffic%0Aallocation%20scheme%20sampler%2C%20remarkably%20outperforming%20the%20random%20strategy%20in%0Afeasibility%20and%20cost%20function%20value.%20Besides%20testing%20the%20quality%20of%20the%20output%0Aallocation%20scheme%2C%20we%20examine%20the%20generalization%20property%20of%20the%20network%20by%0Aincreasing%20the%20time%20steps%20and%20the%20number%20of%20users.%20We%20also%20feed%20the%20solution%20to%0Aexisting%20integer%20optimization%20solvers%20as%20initial%20conditions%20and%20verify%20the%0Awarm-starts%20can%20accelerate%20the%20short-time%20iteration%20process.%20The%20framework%20is%0Ageneral%20with%20solid%20performance%2C%20and%20the%20decoupled%20feature%20of%20the%20random%20neural%0Anetworks%20is%20adequate%20for%20practical%20implementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.05170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Quantile%2520Optimization%2520for%2520Edge-Cloud%2520Networking%26entry.906535625%3DBin%2520Du%2520and%2520He%2520Zhang%2520and%2520Xiangle%2520Cheng%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520We%2520seek%2520the%2520best%2520traffic%2520allocation%2520scheme%2520for%2520the%2520edge-cloud%2520computing%250Anetwork%2520that%2520satisfies%2520constraints%2520and%2520minimizes%2520the%2520cost%2520based%2520on%2520burstable%250Abilling.%2520First%252C%2520for%2520a%2520fixed%2520network%2520topology%252C%2520we%2520formulate%2520a%2520family%2520of%2520integer%250Aprogramming%2520problems%2520with%2520random%2520parameters%2520describing%2520the%2520various%2520traffic%250Ademands.%2520Then%252C%2520to%2520overcome%2520the%2520difficulty%2520caused%2520by%2520the%2520discrete%2520feature%2520of%2520the%250Aproblem%252C%2520we%2520generalize%2520the%2520Gumbel-softmax%2520reparameterization%2520method%2520to%2520induce%250Aan%2520unconstrained%2520continuous%2520optimization%2520problem%2520as%2520a%2520regularized%2520continuation%250Aof%2520the%2520discrete%2520problem.%2520Finally%252C%2520we%2520introduce%2520the%2520Gumbel-softmax%2520sampling%250Anetwork%2520to%2520solve%2520the%2520optimization%2520problems%2520via%2520unsupervised%2520learning.%2520The%250Anetwork%2520structure%2520reflects%2520the%2520edge-cloud%2520computing%2520topology%2520and%2520is%2520trained%2520to%250Aminimize%2520the%2520expectation%2520of%2520the%2520cost%2520function%2520for%2520unconstrained%2520continuous%250Aoptimization%2520problems.%2520The%2520trained%2520network%2520works%2520as%2520an%2520efficient%2520traffic%250Aallocation%2520scheme%2520sampler%252C%2520remarkably%2520outperforming%2520the%2520random%2520strategy%2520in%250Afeasibility%2520and%2520cost%2520function%2520value.%2520Besides%2520testing%2520the%2520quality%2520of%2520the%2520output%250Aallocation%2520scheme%252C%2520we%2520examine%2520the%2520generalization%2520property%2520of%2520the%2520network%2520by%250Aincreasing%2520the%2520time%2520steps%2520and%2520the%2520number%2520of%2520users.%2520We%2520also%2520feed%2520the%2520solution%2520to%250Aexisting%2520integer%2520optimization%2520solvers%2520as%2520initial%2520conditions%2520and%2520verify%2520the%250Awarm-starts%2520can%2520accelerate%2520the%2520short-time%2520iteration%2520process.%2520The%2520framework%2520is%250Ageneral%2520with%2520solid%2520performance%252C%2520and%2520the%2520decoupled%2520feature%2520of%2520the%2520random%2520neural%250Anetworks%2520is%2520adequate%2520for%2520practical%2520implementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.05170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Quantile%20Optimization%20for%20Edge-Cloud%20Networking&entry.906535625=Bin%20Du%20and%20He%20Zhang%20and%20Xiangle%20Cheng%20and%20Lei%20Zhang&entry.1292438233=%20%20We%20seek%20the%20best%20traffic%20allocation%20scheme%20for%20the%20edge-cloud%20computing%0Anetwork%20that%20satisfies%20constraints%20and%20minimizes%20the%20cost%20based%20on%20burstable%0Abilling.%20First%2C%20for%20a%20fixed%20network%20topology%2C%20we%20formulate%20a%20family%20of%20integer%0Aprogramming%20problems%20with%20random%20parameters%20describing%20the%20various%20traffic%0Ademands.%20Then%2C%20to%20overcome%20the%20difficulty%20caused%20by%20the%20discrete%20feature%20of%20the%0Aproblem%2C%20we%20generalize%20the%20Gumbel-softmax%20reparameterization%20method%20to%20induce%0Aan%20unconstrained%20continuous%20optimization%20problem%20as%20a%20regularized%20continuation%0Aof%20the%20discrete%20problem.%20Finally%2C%20we%20introduce%20the%20Gumbel-softmax%20sampling%0Anetwork%20to%20solve%20the%20optimization%20problems%20via%20unsupervised%20learning.%20The%0Anetwork%20structure%20reflects%20the%20edge-cloud%20computing%20topology%20and%20is%20trained%20to%0Aminimize%20the%20expectation%20of%20the%20cost%20function%20for%20unconstrained%20continuous%0Aoptimization%20problems.%20The%20trained%20network%20works%20as%20an%20efficient%20traffic%0Aallocation%20scheme%20sampler%2C%20remarkably%20outperforming%20the%20random%20strategy%20in%0Afeasibility%20and%20cost%20function%20value.%20Besides%20testing%20the%20quality%20of%20the%20output%0Aallocation%20scheme%2C%20we%20examine%20the%20generalization%20property%20of%20the%20network%20by%0Aincreasing%20the%20time%20steps%20and%20the%20number%20of%20users.%20We%20also%20feed%20the%20solution%20to%0Aexisting%20integer%20optimization%20solvers%20as%20initial%20conditions%20and%20verify%20the%0Awarm-starts%20can%20accelerate%20the%20short-time%20iteration%20process.%20The%20framework%20is%0Ageneral%20with%20solid%20performance%2C%20and%20the%20decoupled%20feature%20of%20the%20random%20neural%0Anetworks%20is%20adequate%20for%20practical%20implementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.05170v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey on Synthetic Infrared Image synthesis", "author": "Avinash Upadhyay and Manoj sharma and Prerna Mukherjee and Amit Singhal and Brejesh Lall", "abstract": "  Synthetic infrared (IR) scene and target generation is an important computer\nvision problem as it allows the generation of realistic IR images and targets\nfor training and testing of various applications, such as remote sensing,\nsurveillance, and target recognition. It also helps reduce the cost and risk\nassociated with collecting real-world IR data. This survey paper aims to\nprovide a comprehensive overview of the conventional mathematical\nmodelling-based methods and deep learning-based methods used for generating\nsynthetic IR scenes and targets. The paper discusses the importance of\nsynthetic IR scene and target generation and briefly covers the mathematics of\nblackbody and grey body radiations, as well as IR image-capturing methods. The\npotential use cases of synthetic IR scenes and target generation are also\ndescribed, highlighting the significance of these techniques in various fields.\nAdditionally, the paper explores possible new ways of developing new techniques\nto enhance the efficiency and effectiveness of synthetic IR scenes and target\ngeneration while highlighting the need for further research to advance this\nfield.\n", "link": "http://arxiv.org/abs/2408.06868v1", "date": "2024-08-13", "relevancy": 2.3718, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4757}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4757}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20on%20Synthetic%20Infrared%20Image%20synthesis&body=Title%3A%20A%20Comprehensive%20Survey%20on%20Synthetic%20Infrared%20Image%20synthesis%0AAuthor%3A%20Avinash%20Upadhyay%20and%20Manoj%20sharma%20and%20Prerna%20Mukherjee%20and%20Amit%20Singhal%20and%20Brejesh%20Lall%0AAbstract%3A%20%20%20Synthetic%20infrared%20%28IR%29%20scene%20and%20target%20generation%20is%20an%20important%20computer%0Avision%20problem%20as%20it%20allows%20the%20generation%20of%20realistic%20IR%20images%20and%20targets%0Afor%20training%20and%20testing%20of%20various%20applications%2C%20such%20as%20remote%20sensing%2C%0Asurveillance%2C%20and%20target%20recognition.%20It%20also%20helps%20reduce%20the%20cost%20and%20risk%0Aassociated%20with%20collecting%20real-world%20IR%20data.%20This%20survey%20paper%20aims%20to%0Aprovide%20a%20comprehensive%20overview%20of%20the%20conventional%20mathematical%0Amodelling-based%20methods%20and%20deep%20learning-based%20methods%20used%20for%20generating%0Asynthetic%20IR%20scenes%20and%20targets.%20The%20paper%20discusses%20the%20importance%20of%0Asynthetic%20IR%20scene%20and%20target%20generation%20and%20briefly%20covers%20the%20mathematics%20of%0Ablackbody%20and%20grey%20body%20radiations%2C%20as%20well%20as%20IR%20image-capturing%20methods.%20The%0Apotential%20use%20cases%20of%20synthetic%20IR%20scenes%20and%20target%20generation%20are%20also%0Adescribed%2C%20highlighting%20the%20significance%20of%20these%20techniques%20in%20various%20fields.%0AAdditionally%2C%20the%20paper%20explores%20possible%20new%20ways%20of%20developing%20new%20techniques%0Ato%20enhance%20the%20efficiency%20and%20effectiveness%20of%20synthetic%20IR%20scenes%20and%20target%0Ageneration%20while%20highlighting%20the%20need%20for%20further%20research%20to%20advance%20this%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520on%2520Synthetic%2520Infrared%2520Image%2520synthesis%26entry.906535625%3DAvinash%2520Upadhyay%2520and%2520Manoj%2520sharma%2520and%2520Prerna%2520Mukherjee%2520and%2520Amit%2520Singhal%2520and%2520Brejesh%2520Lall%26entry.1292438233%3D%2520%2520Synthetic%2520infrared%2520%2528IR%2529%2520scene%2520and%2520target%2520generation%2520is%2520an%2520important%2520computer%250Avision%2520problem%2520as%2520it%2520allows%2520the%2520generation%2520of%2520realistic%2520IR%2520images%2520and%2520targets%250Afor%2520training%2520and%2520testing%2520of%2520various%2520applications%252C%2520such%2520as%2520remote%2520sensing%252C%250Asurveillance%252C%2520and%2520target%2520recognition.%2520It%2520also%2520helps%2520reduce%2520the%2520cost%2520and%2520risk%250Aassociated%2520with%2520collecting%2520real-world%2520IR%2520data.%2520This%2520survey%2520paper%2520aims%2520to%250Aprovide%2520a%2520comprehensive%2520overview%2520of%2520the%2520conventional%2520mathematical%250Amodelling-based%2520methods%2520and%2520deep%2520learning-based%2520methods%2520used%2520for%2520generating%250Asynthetic%2520IR%2520scenes%2520and%2520targets.%2520The%2520paper%2520discusses%2520the%2520importance%2520of%250Asynthetic%2520IR%2520scene%2520and%2520target%2520generation%2520and%2520briefly%2520covers%2520the%2520mathematics%2520of%250Ablackbody%2520and%2520grey%2520body%2520radiations%252C%2520as%2520well%2520as%2520IR%2520image-capturing%2520methods.%2520The%250Apotential%2520use%2520cases%2520of%2520synthetic%2520IR%2520scenes%2520and%2520target%2520generation%2520are%2520also%250Adescribed%252C%2520highlighting%2520the%2520significance%2520of%2520these%2520techniques%2520in%2520various%2520fields.%250AAdditionally%252C%2520the%2520paper%2520explores%2520possible%2520new%2520ways%2520of%2520developing%2520new%2520techniques%250Ato%2520enhance%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520synthetic%2520IR%2520scenes%2520and%2520target%250Ageneration%2520while%2520highlighting%2520the%2520need%2520for%2520further%2520research%2520to%2520advance%2520this%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20on%20Synthetic%20Infrared%20Image%20synthesis&entry.906535625=Avinash%20Upadhyay%20and%20Manoj%20sharma%20and%20Prerna%20Mukherjee%20and%20Amit%20Singhal%20and%20Brejesh%20Lall&entry.1292438233=%20%20Synthetic%20infrared%20%28IR%29%20scene%20and%20target%20generation%20is%20an%20important%20computer%0Avision%20problem%20as%20it%20allows%20the%20generation%20of%20realistic%20IR%20images%20and%20targets%0Afor%20training%20and%20testing%20of%20various%20applications%2C%20such%20as%20remote%20sensing%2C%0Asurveillance%2C%20and%20target%20recognition.%20It%20also%20helps%20reduce%20the%20cost%20and%20risk%0Aassociated%20with%20collecting%20real-world%20IR%20data.%20This%20survey%20paper%20aims%20to%0Aprovide%20a%20comprehensive%20overview%20of%20the%20conventional%20mathematical%0Amodelling-based%20methods%20and%20deep%20learning-based%20methods%20used%20for%20generating%0Asynthetic%20IR%20scenes%20and%20targets.%20The%20paper%20discusses%20the%20importance%20of%0Asynthetic%20IR%20scene%20and%20target%20generation%20and%20briefly%20covers%20the%20mathematics%20of%0Ablackbody%20and%20grey%20body%20radiations%2C%20as%20well%20as%20IR%20image-capturing%20methods.%20The%0Apotential%20use%20cases%20of%20synthetic%20IR%20scenes%20and%20target%20generation%20are%20also%0Adescribed%2C%20highlighting%20the%20significance%20of%20these%20techniques%20in%20various%20fields.%0AAdditionally%2C%20the%20paper%20explores%20possible%20new%20ways%20of%20developing%20new%20techniques%0Ato%20enhance%20the%20efficiency%20and%20effectiveness%20of%20synthetic%20IR%20scenes%20and%20target%0Ageneration%20while%20highlighting%20the%20need%20for%20further%20research%20to%20advance%20this%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06868v1&entry.124074799=Read"},
{"title": "Dynamic and Compressive Adaptation of Transformers From Images to Videos", "author": "Guozhen Zhang and Jingyu Liu and Shengming Cao and Xiaotong Zhao and Kevin Zhao and Kai Ma and Limin Wang", "abstract": "  Recently, the remarkable success of pre-trained Vision Transformers (ViTs)\nfrom image-text matching has sparked an interest in image-to-video adaptation.\nHowever, most current approaches retain the full forward pass for each frame,\nleading to a high computation overhead for processing entire videos. In this\npaper, we present InTI, a novel approach for compressive image-to-video\nadaptation using dynamic Inter-frame Token Interpolation. InTI aims to softly\npreserve the informative tokens without disrupting their coherent\nspatiotemporal structure. Specifically, each token pair at identical positions\nwithin neighbor frames is linearly aggregated into a new token, where the\naggregation weights are generated by a multi-scale context-aware network. In\nthis way, the information of neighbor frames can be adaptively compressed in a\npoint-by-point manner, thereby effectively reducing the number of processed\nframes by half each time. Importantly, InTI can be seamlessly integrated with\nexisting adaptation methods, achieving strong performance without extra-complex\ndesign. On Kinetics-400, InTI reaches a top-1 accuracy of 87.1 with a\nremarkable 37.5% reduction in GFLOPs compared to naive adaptation. When\ncombined with additional temporal modules, InTI achieves a top-1 accuracy of\n87.6 with a 37% reduction in GFLOPs. Similar conclusions have been verified in\nother common datasets.\n", "link": "http://arxiv.org/abs/2408.06840v1", "date": "2024-08-13", "relevancy": 2.3585, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6139}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5903}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20and%20Compressive%20Adaptation%20of%20Transformers%20From%20Images%20to%20Videos&body=Title%3A%20Dynamic%20and%20Compressive%20Adaptation%20of%20Transformers%20From%20Images%20to%20Videos%0AAuthor%3A%20Guozhen%20Zhang%20and%20Jingyu%20Liu%20and%20Shengming%20Cao%20and%20Xiaotong%20Zhao%20and%20Kevin%20Zhao%20and%20Kai%20Ma%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Recently%2C%20the%20remarkable%20success%20of%20pre-trained%20Vision%20Transformers%20%28ViTs%29%0Afrom%20image-text%20matching%20has%20sparked%20an%20interest%20in%20image-to-video%20adaptation.%0AHowever%2C%20most%20current%20approaches%20retain%20the%20full%20forward%20pass%20for%20each%20frame%2C%0Aleading%20to%20a%20high%20computation%20overhead%20for%20processing%20entire%20videos.%20In%20this%0Apaper%2C%20we%20present%20InTI%2C%20a%20novel%20approach%20for%20compressive%20image-to-video%0Aadaptation%20using%20dynamic%20Inter-frame%20Token%20Interpolation.%20InTI%20aims%20to%20softly%0Apreserve%20the%20informative%20tokens%20without%20disrupting%20their%20coherent%0Aspatiotemporal%20structure.%20Specifically%2C%20each%20token%20pair%20at%20identical%20positions%0Awithin%20neighbor%20frames%20is%20linearly%20aggregated%20into%20a%20new%20token%2C%20where%20the%0Aaggregation%20weights%20are%20generated%20by%20a%20multi-scale%20context-aware%20network.%20In%0Athis%20way%2C%20the%20information%20of%20neighbor%20frames%20can%20be%20adaptively%20compressed%20in%20a%0Apoint-by-point%20manner%2C%20thereby%20effectively%20reducing%20the%20number%20of%20processed%0Aframes%20by%20half%20each%20time.%20Importantly%2C%20InTI%20can%20be%20seamlessly%20integrated%20with%0Aexisting%20adaptation%20methods%2C%20achieving%20strong%20performance%20without%20extra-complex%0Adesign.%20On%20Kinetics-400%2C%20InTI%20reaches%20a%20top-1%20accuracy%20of%2087.1%20with%20a%0Aremarkable%2037.5%25%20reduction%20in%20GFLOPs%20compared%20to%20naive%20adaptation.%20When%0Acombined%20with%20additional%20temporal%20modules%2C%20InTI%20achieves%20a%20top-1%20accuracy%20of%0A87.6%20with%20a%2037%25%20reduction%20in%20GFLOPs.%20Similar%20conclusions%20have%20been%20verified%20in%0Aother%20common%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520and%2520Compressive%2520Adaptation%2520of%2520Transformers%2520From%2520Images%2520to%2520Videos%26entry.906535625%3DGuozhen%2520Zhang%2520and%2520Jingyu%2520Liu%2520and%2520Shengming%2520Cao%2520and%2520Xiaotong%2520Zhao%2520and%2520Kevin%2520Zhao%2520and%2520Kai%2520Ma%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520remarkable%2520success%2520of%2520pre-trained%2520Vision%2520Transformers%2520%2528ViTs%2529%250Afrom%2520image-text%2520matching%2520has%2520sparked%2520an%2520interest%2520in%2520image-to-video%2520adaptation.%250AHowever%252C%2520most%2520current%2520approaches%2520retain%2520the%2520full%2520forward%2520pass%2520for%2520each%2520frame%252C%250Aleading%2520to%2520a%2520high%2520computation%2520overhead%2520for%2520processing%2520entire%2520videos.%2520In%2520this%250Apaper%252C%2520we%2520present%2520InTI%252C%2520a%2520novel%2520approach%2520for%2520compressive%2520image-to-video%250Aadaptation%2520using%2520dynamic%2520Inter-frame%2520Token%2520Interpolation.%2520InTI%2520aims%2520to%2520softly%250Apreserve%2520the%2520informative%2520tokens%2520without%2520disrupting%2520their%2520coherent%250Aspatiotemporal%2520structure.%2520Specifically%252C%2520each%2520token%2520pair%2520at%2520identical%2520positions%250Awithin%2520neighbor%2520frames%2520is%2520linearly%2520aggregated%2520into%2520a%2520new%2520token%252C%2520where%2520the%250Aaggregation%2520weights%2520are%2520generated%2520by%2520a%2520multi-scale%2520context-aware%2520network.%2520In%250Athis%2520way%252C%2520the%2520information%2520of%2520neighbor%2520frames%2520can%2520be%2520adaptively%2520compressed%2520in%2520a%250Apoint-by-point%2520manner%252C%2520thereby%2520effectively%2520reducing%2520the%2520number%2520of%2520processed%250Aframes%2520by%2520half%2520each%2520time.%2520Importantly%252C%2520InTI%2520can%2520be%2520seamlessly%2520integrated%2520with%250Aexisting%2520adaptation%2520methods%252C%2520achieving%2520strong%2520performance%2520without%2520extra-complex%250Adesign.%2520On%2520Kinetics-400%252C%2520InTI%2520reaches%2520a%2520top-1%2520accuracy%2520of%252087.1%2520with%2520a%250Aremarkable%252037.5%2525%2520reduction%2520in%2520GFLOPs%2520compared%2520to%2520naive%2520adaptation.%2520When%250Acombined%2520with%2520additional%2520temporal%2520modules%252C%2520InTI%2520achieves%2520a%2520top-1%2520accuracy%2520of%250A87.6%2520with%2520a%252037%2525%2520reduction%2520in%2520GFLOPs.%2520Similar%2520conclusions%2520have%2520been%2520verified%2520in%250Aother%2520common%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20and%20Compressive%20Adaptation%20of%20Transformers%20From%20Images%20to%20Videos&entry.906535625=Guozhen%20Zhang%20and%20Jingyu%20Liu%20and%20Shengming%20Cao%20and%20Xiaotong%20Zhao%20and%20Kevin%20Zhao%20and%20Kai%20Ma%20and%20Limin%20Wang&entry.1292438233=%20%20Recently%2C%20the%20remarkable%20success%20of%20pre-trained%20Vision%20Transformers%20%28ViTs%29%0Afrom%20image-text%20matching%20has%20sparked%20an%20interest%20in%20image-to-video%20adaptation.%0AHowever%2C%20most%20current%20approaches%20retain%20the%20full%20forward%20pass%20for%20each%20frame%2C%0Aleading%20to%20a%20high%20computation%20overhead%20for%20processing%20entire%20videos.%20In%20this%0Apaper%2C%20we%20present%20InTI%2C%20a%20novel%20approach%20for%20compressive%20image-to-video%0Aadaptation%20using%20dynamic%20Inter-frame%20Token%20Interpolation.%20InTI%20aims%20to%20softly%0Apreserve%20the%20informative%20tokens%20without%20disrupting%20their%20coherent%0Aspatiotemporal%20structure.%20Specifically%2C%20each%20token%20pair%20at%20identical%20positions%0Awithin%20neighbor%20frames%20is%20linearly%20aggregated%20into%20a%20new%20token%2C%20where%20the%0Aaggregation%20weights%20are%20generated%20by%20a%20multi-scale%20context-aware%20network.%20In%0Athis%20way%2C%20the%20information%20of%20neighbor%20frames%20can%20be%20adaptively%20compressed%20in%20a%0Apoint-by-point%20manner%2C%20thereby%20effectively%20reducing%20the%20number%20of%20processed%0Aframes%20by%20half%20each%20time.%20Importantly%2C%20InTI%20can%20be%20seamlessly%20integrated%20with%0Aexisting%20adaptation%20methods%2C%20achieving%20strong%20performance%20without%20extra-complex%0Adesign.%20On%20Kinetics-400%2C%20InTI%20reaches%20a%20top-1%20accuracy%20of%2087.1%20with%20a%0Aremarkable%2037.5%25%20reduction%20in%20GFLOPs%20compared%20to%20naive%20adaptation.%20When%0Acombined%20with%20additional%20temporal%20modules%2C%20InTI%20achieves%20a%20top-1%20accuracy%20of%0A87.6%20with%20a%2037%25%20reduction%20in%20GFLOPs.%20Similar%20conclusions%20have%20been%20verified%20in%0Aother%20common%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06840v1&entry.124074799=Read"},
{"title": "SSHPool: The Separated Subgraph-based Hierarchical Pooling", "author": "Zhuo Xu and Lixin Cui and Ming Li and Yue Wang and Ziyu Lyu and Hangyuan Du and Lu Bai and Philip S. Yu and Edwin R. Hancock", "abstract": "  In this paper, we develop a novel local graph pooling method, namely the\nSeparated Subgraph-based Hierarchical Pooling (SSHPool), for graph\nclassification. We commence by assigning the nodes of a sample graph into\ndifferent clusters, resulting in a family of separated subgraphs. We\nindividually employ the local graph convolution units as the local structure to\nfurther compress each subgraph into a coarsened node, transforming the original\ngraph into a coarsened graph. Since these subgraphs are separated by different\nclusters and the structural information cannot be propagated between them, the\nlocal convolution operation can significantly avoid the over-smoothing problem\ncaused by message passing through edges in most existing Graph Neural Networks\n(GNNs). By hierarchically performing the proposed procedures on the resulting\ncoarsened graph, the proposed SSHPool can effectively extract the hierarchical\nglobal features of the original graph structure, encapsulating rich intrinsic\nstructural characteristics. Furthermore, we develop an end-to-end GNN framework\nassociated with the SSHPool module for graph classification. Experimental\nresults demonstrate the superior performance of the proposed model on\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2403.16133v2", "date": "2024-08-13", "relevancy": 2.355, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4846}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4652}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSHPool%3A%20The%20Separated%20Subgraph-based%20Hierarchical%20Pooling&body=Title%3A%20SSHPool%3A%20The%20Separated%20Subgraph-based%20Hierarchical%20Pooling%0AAuthor%3A%20Zhuo%20Xu%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Ziyu%20Lyu%20and%20Hangyuan%20Du%20and%20Lu%20Bai%20and%20Philip%20S.%20Yu%20and%20Edwin%20R.%20Hancock%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20a%20novel%20local%20graph%20pooling%20method%2C%20namely%20the%0ASeparated%20Subgraph-based%20Hierarchical%20Pooling%20%28SSHPool%29%2C%20for%20graph%0Aclassification.%20We%20commence%20by%20assigning%20the%20nodes%20of%20a%20sample%20graph%20into%0Adifferent%20clusters%2C%20resulting%20in%20a%20family%20of%20separated%20subgraphs.%20We%0Aindividually%20employ%20the%20local%20graph%20convolution%20units%20as%20the%20local%20structure%20to%0Afurther%20compress%20each%20subgraph%20into%20a%20coarsened%20node%2C%20transforming%20the%20original%0Agraph%20into%20a%20coarsened%20graph.%20Since%20these%20subgraphs%20are%20separated%20by%20different%0Aclusters%20and%20the%20structural%20information%20cannot%20be%20propagated%20between%20them%2C%20the%0Alocal%20convolution%20operation%20can%20significantly%20avoid%20the%20over-smoothing%20problem%0Acaused%20by%20message%20passing%20through%20edges%20in%20most%20existing%20Graph%20Neural%20Networks%0A%28GNNs%29.%20By%20hierarchically%20performing%20the%20proposed%20procedures%20on%20the%20resulting%0Acoarsened%20graph%2C%20the%20proposed%20SSHPool%20can%20effectively%20extract%20the%20hierarchical%0Aglobal%20features%20of%20the%20original%20graph%20structure%2C%20encapsulating%20rich%20intrinsic%0Astructural%20characteristics.%20Furthermore%2C%20we%20develop%20an%20end-to-end%20GNN%20framework%0Aassociated%20with%20the%20SSHPool%20module%20for%20graph%20classification.%20Experimental%0Aresults%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20model%20on%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16133v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSHPool%253A%2520The%2520Separated%2520Subgraph-based%2520Hierarchical%2520Pooling%26entry.906535625%3DZhuo%2520Xu%2520and%2520Lixin%2520Cui%2520and%2520Ming%2520Li%2520and%2520Yue%2520Wang%2520and%2520Ziyu%2520Lyu%2520and%2520Hangyuan%2520Du%2520and%2520Lu%2520Bai%2520and%2520Philip%2520S.%2520Yu%2520and%2520Edwin%2520R.%2520Hancock%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520local%2520graph%2520pooling%2520method%252C%2520namely%2520the%250ASeparated%2520Subgraph-based%2520Hierarchical%2520Pooling%2520%2528SSHPool%2529%252C%2520for%2520graph%250Aclassification.%2520We%2520commence%2520by%2520assigning%2520the%2520nodes%2520of%2520a%2520sample%2520graph%2520into%250Adifferent%2520clusters%252C%2520resulting%2520in%2520a%2520family%2520of%2520separated%2520subgraphs.%2520We%250Aindividually%2520employ%2520the%2520local%2520graph%2520convolution%2520units%2520as%2520the%2520local%2520structure%2520to%250Afurther%2520compress%2520each%2520subgraph%2520into%2520a%2520coarsened%2520node%252C%2520transforming%2520the%2520original%250Agraph%2520into%2520a%2520coarsened%2520graph.%2520Since%2520these%2520subgraphs%2520are%2520separated%2520by%2520different%250Aclusters%2520and%2520the%2520structural%2520information%2520cannot%2520be%2520propagated%2520between%2520them%252C%2520the%250Alocal%2520convolution%2520operation%2520can%2520significantly%2520avoid%2520the%2520over-smoothing%2520problem%250Acaused%2520by%2520message%2520passing%2520through%2520edges%2520in%2520most%2520existing%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529.%2520By%2520hierarchically%2520performing%2520the%2520proposed%2520procedures%2520on%2520the%2520resulting%250Acoarsened%2520graph%252C%2520the%2520proposed%2520SSHPool%2520can%2520effectively%2520extract%2520the%2520hierarchical%250Aglobal%2520features%2520of%2520the%2520original%2520graph%2520structure%252C%2520encapsulating%2520rich%2520intrinsic%250Astructural%2520characteristics.%2520Furthermore%252C%2520we%2520develop%2520an%2520end-to-end%2520GNN%2520framework%250Aassociated%2520with%2520the%2520SSHPool%2520module%2520for%2520graph%2520classification.%2520Experimental%250Aresults%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520model%2520on%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16133v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSHPool%3A%20The%20Separated%20Subgraph-based%20Hierarchical%20Pooling&entry.906535625=Zhuo%20Xu%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Ziyu%20Lyu%20and%20Hangyuan%20Du%20and%20Lu%20Bai%20and%20Philip%20S.%20Yu%20and%20Edwin%20R.%20Hancock&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20a%20novel%20local%20graph%20pooling%20method%2C%20namely%20the%0ASeparated%20Subgraph-based%20Hierarchical%20Pooling%20%28SSHPool%29%2C%20for%20graph%0Aclassification.%20We%20commence%20by%20assigning%20the%20nodes%20of%20a%20sample%20graph%20into%0Adifferent%20clusters%2C%20resulting%20in%20a%20family%20of%20separated%20subgraphs.%20We%0Aindividually%20employ%20the%20local%20graph%20convolution%20units%20as%20the%20local%20structure%20to%0Afurther%20compress%20each%20subgraph%20into%20a%20coarsened%20node%2C%20transforming%20the%20original%0Agraph%20into%20a%20coarsened%20graph.%20Since%20these%20subgraphs%20are%20separated%20by%20different%0Aclusters%20and%20the%20structural%20information%20cannot%20be%20propagated%20between%20them%2C%20the%0Alocal%20convolution%20operation%20can%20significantly%20avoid%20the%20over-smoothing%20problem%0Acaused%20by%20message%20passing%20through%20edges%20in%20most%20existing%20Graph%20Neural%20Networks%0A%28GNNs%29.%20By%20hierarchically%20performing%20the%20proposed%20procedures%20on%20the%20resulting%0Acoarsened%20graph%2C%20the%20proposed%20SSHPool%20can%20effectively%20extract%20the%20hierarchical%0Aglobal%20features%20of%20the%20original%20graph%20structure%2C%20encapsulating%20rich%20intrinsic%0Astructural%20characteristics.%20Furthermore%2C%20we%20develop%20an%20end-to-end%20GNN%20framework%0Aassociated%20with%20the%20SSHPool%20module%20for%20graph%20classification.%20Experimental%0Aresults%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20model%20on%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16133v2&entry.124074799=Read"},
{"title": "Generative AI Tools in Academic Research: Applications and Implications\n  for Qualitative and Quantitative Research Methodologies", "author": "Mike Perkins and Jasper Roe", "abstract": "  This study examines the impact of Generative Artificial Intelligence (GenAI)\non academic research, focusing on its application to qualitative and\nquantitative data analysis. As GenAI tools evolve rapidly, they offer new\npossibilities for enhancing research productivity and democratising complex\nanalytical processes. However, their integration into academic practice raises\nsignificant questions regarding research integrity and security, authorship,\nand the changing nature of scholarly work. Through an examination of current\ncapabilities and potential future applications, this study provides insights\ninto how researchers may utilise GenAI tools responsibly and ethically.\n  We present case studies that demonstrate the application of GenAI in various\nresearch methodologies, discuss the challenges of replicability and consistency\nin AI-assisted research, and consider the ethical implications of increased AI\nintegration in academia. This study explores both qualitative and quantitative\napplications of GenAI, highlighting tools for transcription, coding, thematic\nanalysis, visual analytics, and statistical analysis. By addressing these\nissues, we aim to contribute to the ongoing discourse on the role of AI in\nshaping the future of academic research and provide guidance for researchers\nexploring the rapidly evolving landscape of AI-assisted research tools and\nresearch.\n", "link": "http://arxiv.org/abs/2408.06872v1", "date": "2024-08-13", "relevancy": 2.2997, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5031}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4491}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Tools%20in%20Academic%20Research%3A%20Applications%20and%20Implications%0A%20%20for%20Qualitative%20and%20Quantitative%20Research%20Methodologies&body=Title%3A%20Generative%20AI%20Tools%20in%20Academic%20Research%3A%20Applications%20and%20Implications%0A%20%20for%20Qualitative%20and%20Quantitative%20Research%20Methodologies%0AAuthor%3A%20Mike%20Perkins%20and%20Jasper%20Roe%0AAbstract%3A%20%20%20This%20study%20examines%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%0Aon%20academic%20research%2C%20focusing%20on%20its%20application%20to%20qualitative%20and%0Aquantitative%20data%20analysis.%20As%20GenAI%20tools%20evolve%20rapidly%2C%20they%20offer%20new%0Apossibilities%20for%20enhancing%20research%20productivity%20and%20democratising%20complex%0Aanalytical%20processes.%20However%2C%20their%20integration%20into%20academic%20practice%20raises%0Asignificant%20questions%20regarding%20research%20integrity%20and%20security%2C%20authorship%2C%0Aand%20the%20changing%20nature%20of%20scholarly%20work.%20Through%20an%20examination%20of%20current%0Acapabilities%20and%20potential%20future%20applications%2C%20this%20study%20provides%20insights%0Ainto%20how%20researchers%20may%20utilise%20GenAI%20tools%20responsibly%20and%20ethically.%0A%20%20We%20present%20case%20studies%20that%20demonstrate%20the%20application%20of%20GenAI%20in%20various%0Aresearch%20methodologies%2C%20discuss%20the%20challenges%20of%20replicability%20and%20consistency%0Ain%20AI-assisted%20research%2C%20and%20consider%20the%20ethical%20implications%20of%20increased%20AI%0Aintegration%20in%20academia.%20This%20study%20explores%20both%20qualitative%20and%20quantitative%0Aapplications%20of%20GenAI%2C%20highlighting%20tools%20for%20transcription%2C%20coding%2C%20thematic%0Aanalysis%2C%20visual%20analytics%2C%20and%20statistical%20analysis.%20By%20addressing%20these%0Aissues%2C%20we%20aim%20to%20contribute%20to%20the%20ongoing%20discourse%20on%20the%20role%20of%20AI%20in%0Ashaping%20the%20future%20of%20academic%20research%20and%20provide%20guidance%20for%20researchers%0Aexploring%20the%20rapidly%20evolving%20landscape%20of%20AI-assisted%20research%20tools%20and%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Tools%2520in%2520Academic%2520Research%253A%2520Applications%2520and%2520Implications%250A%2520%2520for%2520Qualitative%2520and%2520Quantitative%2520Research%2520Methodologies%26entry.906535625%3DMike%2520Perkins%2520and%2520Jasper%2520Roe%26entry.1292438233%3D%2520%2520This%2520study%2520examines%2520the%2520impact%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%250Aon%2520academic%2520research%252C%2520focusing%2520on%2520its%2520application%2520to%2520qualitative%2520and%250Aquantitative%2520data%2520analysis.%2520As%2520GenAI%2520tools%2520evolve%2520rapidly%252C%2520they%2520offer%2520new%250Apossibilities%2520for%2520enhancing%2520research%2520productivity%2520and%2520democratising%2520complex%250Aanalytical%2520processes.%2520However%252C%2520their%2520integration%2520into%2520academic%2520practice%2520raises%250Asignificant%2520questions%2520regarding%2520research%2520integrity%2520and%2520security%252C%2520authorship%252C%250Aand%2520the%2520changing%2520nature%2520of%2520scholarly%2520work.%2520Through%2520an%2520examination%2520of%2520current%250Acapabilities%2520and%2520potential%2520future%2520applications%252C%2520this%2520study%2520provides%2520insights%250Ainto%2520how%2520researchers%2520may%2520utilise%2520GenAI%2520tools%2520responsibly%2520and%2520ethically.%250A%2520%2520We%2520present%2520case%2520studies%2520that%2520demonstrate%2520the%2520application%2520of%2520GenAI%2520in%2520various%250Aresearch%2520methodologies%252C%2520discuss%2520the%2520challenges%2520of%2520replicability%2520and%2520consistency%250Ain%2520AI-assisted%2520research%252C%2520and%2520consider%2520the%2520ethical%2520implications%2520of%2520increased%2520AI%250Aintegration%2520in%2520academia.%2520This%2520study%2520explores%2520both%2520qualitative%2520and%2520quantitative%250Aapplications%2520of%2520GenAI%252C%2520highlighting%2520tools%2520for%2520transcription%252C%2520coding%252C%2520thematic%250Aanalysis%252C%2520visual%2520analytics%252C%2520and%2520statistical%2520analysis.%2520By%2520addressing%2520these%250Aissues%252C%2520we%2520aim%2520to%2520contribute%2520to%2520the%2520ongoing%2520discourse%2520on%2520the%2520role%2520of%2520AI%2520in%250Ashaping%2520the%2520future%2520of%2520academic%2520research%2520and%2520provide%2520guidance%2520for%2520researchers%250Aexploring%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520AI-assisted%2520research%2520tools%2520and%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Tools%20in%20Academic%20Research%3A%20Applications%20and%20Implications%0A%20%20for%20Qualitative%20and%20Quantitative%20Research%20Methodologies&entry.906535625=Mike%20Perkins%20and%20Jasper%20Roe&entry.1292438233=%20%20This%20study%20examines%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%0Aon%20academic%20research%2C%20focusing%20on%20its%20application%20to%20qualitative%20and%0Aquantitative%20data%20analysis.%20As%20GenAI%20tools%20evolve%20rapidly%2C%20they%20offer%20new%0Apossibilities%20for%20enhancing%20research%20productivity%20and%20democratising%20complex%0Aanalytical%20processes.%20However%2C%20their%20integration%20into%20academic%20practice%20raises%0Asignificant%20questions%20regarding%20research%20integrity%20and%20security%2C%20authorship%2C%0Aand%20the%20changing%20nature%20of%20scholarly%20work.%20Through%20an%20examination%20of%20current%0Acapabilities%20and%20potential%20future%20applications%2C%20this%20study%20provides%20insights%0Ainto%20how%20researchers%20may%20utilise%20GenAI%20tools%20responsibly%20and%20ethically.%0A%20%20We%20present%20case%20studies%20that%20demonstrate%20the%20application%20of%20GenAI%20in%20various%0Aresearch%20methodologies%2C%20discuss%20the%20challenges%20of%20replicability%20and%20consistency%0Ain%20AI-assisted%20research%2C%20and%20consider%20the%20ethical%20implications%20of%20increased%20AI%0Aintegration%20in%20academia.%20This%20study%20explores%20both%20qualitative%20and%20quantitative%0Aapplications%20of%20GenAI%2C%20highlighting%20tools%20for%20transcription%2C%20coding%2C%20thematic%0Aanalysis%2C%20visual%20analytics%2C%20and%20statistical%20analysis.%20By%20addressing%20these%0Aissues%2C%20we%20aim%20to%20contribute%20to%20the%20ongoing%20discourse%20on%20the%20role%20of%20AI%20in%0Ashaping%20the%20future%20of%20academic%20research%20and%20provide%20guidance%20for%20researchers%0Aexploring%20the%20rapidly%20evolving%20landscape%20of%20AI-assisted%20research%20tools%20and%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06872v1&entry.124074799=Read"},
{"title": "RealGen: Retrieval Augmented Generation for Controllable Traffic\n  Scenarios", "author": "Wenhao Ding and Yulong Cao and Ding Zhao and Chaowei Xiao and Marco Pavone", "abstract": "  Simulation plays a crucial role in the development of autonomous vehicles\n(AVs) due to the potential risks associated with real-world testing. Although\nsignificant progress has been made in the visual aspects of simulators,\ngenerating complex behavior among agents remains a formidable challenge. It is\nnot only imperative to ensure realism in the scenarios generated but also\nessential to incorporate preferences and conditions to facilitate controllable\ngeneration for AV training and evaluation. Traditional methods, mainly relying\non memorizing the distribution of training datasets, often fall short in\ngenerating unseen scenarios. Inspired by the success of retrieval augmented\ngeneration in large language models, we present RealGen, a novel\nretrieval-based in-context learning framework for traffic scenario generation.\nRealGen synthesizes new scenarios by combining behaviors from multiple\nretrieved examples in a gradient-free way, which may originate from templates\nor tagged scenarios. This in-context learning framework endows versatile\ngenerative capabilities, including the ability to edit scenarios, compose\nvarious behaviors, and produce critical scenarios. Evaluations show that\nRealGen offers considerable flexibility and controllability, marking a new\ndirection in the field of controllable traffic scenario generation. Check our\nproject website for more information: https://realgen.github.io.\n", "link": "http://arxiv.org/abs/2312.13303v2", "date": "2024-08-13", "relevancy": 2.2927, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6159}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5509}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealGen%3A%20Retrieval%20Augmented%20Generation%20for%20Controllable%20Traffic%0A%20%20Scenarios&body=Title%3A%20RealGen%3A%20Retrieval%20Augmented%20Generation%20for%20Controllable%20Traffic%0A%20%20Scenarios%0AAuthor%3A%20Wenhao%20Ding%20and%20Yulong%20Cao%20and%20Ding%20Zhao%20and%20Chaowei%20Xiao%20and%20Marco%20Pavone%0AAbstract%3A%20%20%20Simulation%20plays%20a%20crucial%20role%20in%20the%20development%20of%20autonomous%20vehicles%0A%28AVs%29%20due%20to%20the%20potential%20risks%20associated%20with%20real-world%20testing.%20Although%0Asignificant%20progress%20has%20been%20made%20in%20the%20visual%20aspects%20of%20simulators%2C%0Agenerating%20complex%20behavior%20among%20agents%20remains%20a%20formidable%20challenge.%20It%20is%0Anot%20only%20imperative%20to%20ensure%20realism%20in%20the%20scenarios%20generated%20but%20also%0Aessential%20to%20incorporate%20preferences%20and%20conditions%20to%20facilitate%20controllable%0Ageneration%20for%20AV%20training%20and%20evaluation.%20Traditional%20methods%2C%20mainly%20relying%0Aon%20memorizing%20the%20distribution%20of%20training%20datasets%2C%20often%20fall%20short%20in%0Agenerating%20unseen%20scenarios.%20Inspired%20by%20the%20success%20of%20retrieval%20augmented%0Ageneration%20in%20large%20language%20models%2C%20we%20present%20RealGen%2C%20a%20novel%0Aretrieval-based%20in-context%20learning%20framework%20for%20traffic%20scenario%20generation.%0ARealGen%20synthesizes%20new%20scenarios%20by%20combining%20behaviors%20from%20multiple%0Aretrieved%20examples%20in%20a%20gradient-free%20way%2C%20which%20may%20originate%20from%20templates%0Aor%20tagged%20scenarios.%20This%20in-context%20learning%20framework%20endows%20versatile%0Agenerative%20capabilities%2C%20including%20the%20ability%20to%20edit%20scenarios%2C%20compose%0Avarious%20behaviors%2C%20and%20produce%20critical%20scenarios.%20Evaluations%20show%20that%0ARealGen%20offers%20considerable%20flexibility%20and%20controllability%2C%20marking%20a%20new%0Adirection%20in%20the%20field%20of%20controllable%20traffic%20scenario%20generation.%20Check%20our%0Aproject%20website%20for%20more%20information%3A%20https%3A//realgen.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealGen%253A%2520Retrieval%2520Augmented%2520Generation%2520for%2520Controllable%2520Traffic%250A%2520%2520Scenarios%26entry.906535625%3DWenhao%2520Ding%2520and%2520Yulong%2520Cao%2520and%2520Ding%2520Zhao%2520and%2520Chaowei%2520Xiao%2520and%2520Marco%2520Pavone%26entry.1292438233%3D%2520%2520Simulation%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520development%2520of%2520autonomous%2520vehicles%250A%2528AVs%2529%2520due%2520to%2520the%2520potential%2520risks%2520associated%2520with%2520real-world%2520testing.%2520Although%250Asignificant%2520progress%2520has%2520been%2520made%2520in%2520the%2520visual%2520aspects%2520of%2520simulators%252C%250Agenerating%2520complex%2520behavior%2520among%2520agents%2520remains%2520a%2520formidable%2520challenge.%2520It%2520is%250Anot%2520only%2520imperative%2520to%2520ensure%2520realism%2520in%2520the%2520scenarios%2520generated%2520but%2520also%250Aessential%2520to%2520incorporate%2520preferences%2520and%2520conditions%2520to%2520facilitate%2520controllable%250Ageneration%2520for%2520AV%2520training%2520and%2520evaluation.%2520Traditional%2520methods%252C%2520mainly%2520relying%250Aon%2520memorizing%2520the%2520distribution%2520of%2520training%2520datasets%252C%2520often%2520fall%2520short%2520in%250Agenerating%2520unseen%2520scenarios.%2520Inspired%2520by%2520the%2520success%2520of%2520retrieval%2520augmented%250Ageneration%2520in%2520large%2520language%2520models%252C%2520we%2520present%2520RealGen%252C%2520a%2520novel%250Aretrieval-based%2520in-context%2520learning%2520framework%2520for%2520traffic%2520scenario%2520generation.%250ARealGen%2520synthesizes%2520new%2520scenarios%2520by%2520combining%2520behaviors%2520from%2520multiple%250Aretrieved%2520examples%2520in%2520a%2520gradient-free%2520way%252C%2520which%2520may%2520originate%2520from%2520templates%250Aor%2520tagged%2520scenarios.%2520This%2520in-context%2520learning%2520framework%2520endows%2520versatile%250Agenerative%2520capabilities%252C%2520including%2520the%2520ability%2520to%2520edit%2520scenarios%252C%2520compose%250Avarious%2520behaviors%252C%2520and%2520produce%2520critical%2520scenarios.%2520Evaluations%2520show%2520that%250ARealGen%2520offers%2520considerable%2520flexibility%2520and%2520controllability%252C%2520marking%2520a%2520new%250Adirection%2520in%2520the%2520field%2520of%2520controllable%2520traffic%2520scenario%2520generation.%2520Check%2520our%250Aproject%2520website%2520for%2520more%2520information%253A%2520https%253A//realgen.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealGen%3A%20Retrieval%20Augmented%20Generation%20for%20Controllable%20Traffic%0A%20%20Scenarios&entry.906535625=Wenhao%20Ding%20and%20Yulong%20Cao%20and%20Ding%20Zhao%20and%20Chaowei%20Xiao%20and%20Marco%20Pavone&entry.1292438233=%20%20Simulation%20plays%20a%20crucial%20role%20in%20the%20development%20of%20autonomous%20vehicles%0A%28AVs%29%20due%20to%20the%20potential%20risks%20associated%20with%20real-world%20testing.%20Although%0Asignificant%20progress%20has%20been%20made%20in%20the%20visual%20aspects%20of%20simulators%2C%0Agenerating%20complex%20behavior%20among%20agents%20remains%20a%20formidable%20challenge.%20It%20is%0Anot%20only%20imperative%20to%20ensure%20realism%20in%20the%20scenarios%20generated%20but%20also%0Aessential%20to%20incorporate%20preferences%20and%20conditions%20to%20facilitate%20controllable%0Ageneration%20for%20AV%20training%20and%20evaluation.%20Traditional%20methods%2C%20mainly%20relying%0Aon%20memorizing%20the%20distribution%20of%20training%20datasets%2C%20often%20fall%20short%20in%0Agenerating%20unseen%20scenarios.%20Inspired%20by%20the%20success%20of%20retrieval%20augmented%0Ageneration%20in%20large%20language%20models%2C%20we%20present%20RealGen%2C%20a%20novel%0Aretrieval-based%20in-context%20learning%20framework%20for%20traffic%20scenario%20generation.%0ARealGen%20synthesizes%20new%20scenarios%20by%20combining%20behaviors%20from%20multiple%0Aretrieved%20examples%20in%20a%20gradient-free%20way%2C%20which%20may%20originate%20from%20templates%0Aor%20tagged%20scenarios.%20This%20in-context%20learning%20framework%20endows%20versatile%0Agenerative%20capabilities%2C%20including%20the%20ability%20to%20edit%20scenarios%2C%20compose%0Avarious%20behaviors%2C%20and%20produce%20critical%20scenarios.%20Evaluations%20show%20that%0ARealGen%20offers%20considerable%20flexibility%20and%20controllability%2C%20marking%20a%20new%0Adirection%20in%20the%20field%20of%20controllable%20traffic%20scenario%20generation.%20Check%20our%0Aproject%20website%20for%20more%20information%3A%20https%3A//realgen.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13303v2&entry.124074799=Read"},
{"title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks", "author": "Renqiang Luo and Huafei Huang and Shuo Yu and Zhuoyang Han and Estrid He and Xiuzhen Zhang and Feng Xia", "abstract": "  Fairness-aware Graph Neural Networks (GNNs) often face a challenging\ntrade-off, where prioritizing fairness may require compromising utility. In\nthis work, we re-examine fairness through the lens of spectral graph theory,\naiming to reconcile fairness and utility within the framework of spectral graph\nlearning. We explore the correlation between sensitive features and spectrum in\nGNNs, using theoretical analysis to delineate the similarity between original\nsensitive features and those after convolution under different spectra. Our\nanalysis reveals a reduction in the impact of similarity when the eigenvectors\nassociated with the largest magnitude eigenvalue exhibit directional\nsimilarity. Based on these theoretical insights, we propose FUGNN, a novel\nspectral graph learning approach that harmonizes the conflict between fairness\nand utility. FUGNN ensures algorithmic fairness and utility by truncating the\nspectrum and optimizing eigenvector distribution during the encoding process.\nThe fairness-aware eigenvector selection reduces the impact of convolution on\nsensitive features while concurrently minimizing the sacrifice of utility.\nFUGNN further optimizes the distribution of eigenvectors through a transformer\narchitecture. By incorporating the optimized spectrum into the graph\nconvolution network, FUGNN effectively learns node representations. Experiments\non six real-world datasets demonstrate the superiority of FUGNN over baseline\nmethods. The codes are available at https://github.com/yushuowiki/FUGNN.\n", "link": "http://arxiv.org/abs/2405.17034v2", "date": "2024-08-13", "relevancy": 2.2884, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4634}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4558}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FUGNN%3A%20Harmonizing%20Fairness%20and%20Utility%20in%20Graph%20Neural%20Networks&body=Title%3A%20FUGNN%3A%20Harmonizing%20Fairness%20and%20Utility%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Renqiang%20Luo%20and%20Huafei%20Huang%20and%20Shuo%20Yu%20and%20Zhuoyang%20Han%20and%20Estrid%20He%20and%20Xiuzhen%20Zhang%20and%20Feng%20Xia%0AAbstract%3A%20%20%20Fairness-aware%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20face%20a%20challenging%0Atrade-off%2C%20where%20prioritizing%20fairness%20may%20require%20compromising%20utility.%20In%0Athis%20work%2C%20we%20re-examine%20fairness%20through%20the%20lens%20of%20spectral%20graph%20theory%2C%0Aaiming%20to%20reconcile%20fairness%20and%20utility%20within%20the%20framework%20of%20spectral%20graph%0Alearning.%20We%20explore%20the%20correlation%20between%20sensitive%20features%20and%20spectrum%20in%0AGNNs%2C%20using%20theoretical%20analysis%20to%20delineate%20the%20similarity%20between%20original%0Asensitive%20features%20and%20those%20after%20convolution%20under%20different%20spectra.%20Our%0Aanalysis%20reveals%20a%20reduction%20in%20the%20impact%20of%20similarity%20when%20the%20eigenvectors%0Aassociated%20with%20the%20largest%20magnitude%20eigenvalue%20exhibit%20directional%0Asimilarity.%20Based%20on%20these%20theoretical%20insights%2C%20we%20propose%20FUGNN%2C%20a%20novel%0Aspectral%20graph%20learning%20approach%20that%20harmonizes%20the%20conflict%20between%20fairness%0Aand%20utility.%20FUGNN%20ensures%20algorithmic%20fairness%20and%20utility%20by%20truncating%20the%0Aspectrum%20and%20optimizing%20eigenvector%20distribution%20during%20the%20encoding%20process.%0AThe%20fairness-aware%20eigenvector%20selection%20reduces%20the%20impact%20of%20convolution%20on%0Asensitive%20features%20while%20concurrently%20minimizing%20the%20sacrifice%20of%20utility.%0AFUGNN%20further%20optimizes%20the%20distribution%20of%20eigenvectors%20through%20a%20transformer%0Aarchitecture.%20By%20incorporating%20the%20optimized%20spectrum%20into%20the%20graph%0Aconvolution%20network%2C%20FUGNN%20effectively%20learns%20node%20representations.%20Experiments%0Aon%20six%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20FUGNN%20over%20baseline%0Amethods.%20The%20codes%20are%20available%20at%20https%3A//github.com/yushuowiki/FUGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFUGNN%253A%2520Harmonizing%2520Fairness%2520and%2520Utility%2520in%2520Graph%2520Neural%2520Networks%26entry.906535625%3DRenqiang%2520Luo%2520and%2520Huafei%2520Huang%2520and%2520Shuo%2520Yu%2520and%2520Zhuoyang%2520Han%2520and%2520Estrid%2520He%2520and%2520Xiuzhen%2520Zhang%2520and%2520Feng%2520Xia%26entry.1292438233%3D%2520%2520Fairness-aware%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520often%2520face%2520a%2520challenging%250Atrade-off%252C%2520where%2520prioritizing%2520fairness%2520may%2520require%2520compromising%2520utility.%2520In%250Athis%2520work%252C%2520we%2520re-examine%2520fairness%2520through%2520the%2520lens%2520of%2520spectral%2520graph%2520theory%252C%250Aaiming%2520to%2520reconcile%2520fairness%2520and%2520utility%2520within%2520the%2520framework%2520of%2520spectral%2520graph%250Alearning.%2520We%2520explore%2520the%2520correlation%2520between%2520sensitive%2520features%2520and%2520spectrum%2520in%250AGNNs%252C%2520using%2520theoretical%2520analysis%2520to%2520delineate%2520the%2520similarity%2520between%2520original%250Asensitive%2520features%2520and%2520those%2520after%2520convolution%2520under%2520different%2520spectra.%2520Our%250Aanalysis%2520reveals%2520a%2520reduction%2520in%2520the%2520impact%2520of%2520similarity%2520when%2520the%2520eigenvectors%250Aassociated%2520with%2520the%2520largest%2520magnitude%2520eigenvalue%2520exhibit%2520directional%250Asimilarity.%2520Based%2520on%2520these%2520theoretical%2520insights%252C%2520we%2520propose%2520FUGNN%252C%2520a%2520novel%250Aspectral%2520graph%2520learning%2520approach%2520that%2520harmonizes%2520the%2520conflict%2520between%2520fairness%250Aand%2520utility.%2520FUGNN%2520ensures%2520algorithmic%2520fairness%2520and%2520utility%2520by%2520truncating%2520the%250Aspectrum%2520and%2520optimizing%2520eigenvector%2520distribution%2520during%2520the%2520encoding%2520process.%250AThe%2520fairness-aware%2520eigenvector%2520selection%2520reduces%2520the%2520impact%2520of%2520convolution%2520on%250Asensitive%2520features%2520while%2520concurrently%2520minimizing%2520the%2520sacrifice%2520of%2520utility.%250AFUGNN%2520further%2520optimizes%2520the%2520distribution%2520of%2520eigenvectors%2520through%2520a%2520transformer%250Aarchitecture.%2520By%2520incorporating%2520the%2520optimized%2520spectrum%2520into%2520the%2520graph%250Aconvolution%2520network%252C%2520FUGNN%2520effectively%2520learns%2520node%2520representations.%2520Experiments%250Aon%2520six%2520real-world%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520FUGNN%2520over%2520baseline%250Amethods.%2520The%2520codes%2520are%2520available%2520at%2520https%253A//github.com/yushuowiki/FUGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FUGNN%3A%20Harmonizing%20Fairness%20and%20Utility%20in%20Graph%20Neural%20Networks&entry.906535625=Renqiang%20Luo%20and%20Huafei%20Huang%20and%20Shuo%20Yu%20and%20Zhuoyang%20Han%20and%20Estrid%20He%20and%20Xiuzhen%20Zhang%20and%20Feng%20Xia&entry.1292438233=%20%20Fairness-aware%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20face%20a%20challenging%0Atrade-off%2C%20where%20prioritizing%20fairness%20may%20require%20compromising%20utility.%20In%0Athis%20work%2C%20we%20re-examine%20fairness%20through%20the%20lens%20of%20spectral%20graph%20theory%2C%0Aaiming%20to%20reconcile%20fairness%20and%20utility%20within%20the%20framework%20of%20spectral%20graph%0Alearning.%20We%20explore%20the%20correlation%20between%20sensitive%20features%20and%20spectrum%20in%0AGNNs%2C%20using%20theoretical%20analysis%20to%20delineate%20the%20similarity%20between%20original%0Asensitive%20features%20and%20those%20after%20convolution%20under%20different%20spectra.%20Our%0Aanalysis%20reveals%20a%20reduction%20in%20the%20impact%20of%20similarity%20when%20the%20eigenvectors%0Aassociated%20with%20the%20largest%20magnitude%20eigenvalue%20exhibit%20directional%0Asimilarity.%20Based%20on%20these%20theoretical%20insights%2C%20we%20propose%20FUGNN%2C%20a%20novel%0Aspectral%20graph%20learning%20approach%20that%20harmonizes%20the%20conflict%20between%20fairness%0Aand%20utility.%20FUGNN%20ensures%20algorithmic%20fairness%20and%20utility%20by%20truncating%20the%0Aspectrum%20and%20optimizing%20eigenvector%20distribution%20during%20the%20encoding%20process.%0AThe%20fairness-aware%20eigenvector%20selection%20reduces%20the%20impact%20of%20convolution%20on%0Asensitive%20features%20while%20concurrently%20minimizing%20the%20sacrifice%20of%20utility.%0AFUGNN%20further%20optimizes%20the%20distribution%20of%20eigenvectors%20through%20a%20transformer%0Aarchitecture.%20By%20incorporating%20the%20optimized%20spectrum%20into%20the%20graph%0Aconvolution%20network%2C%20FUGNN%20effectively%20learns%20node%20representations.%20Experiments%0Aon%20six%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20FUGNN%20over%20baseline%0Amethods.%20The%20codes%20are%20available%20at%20https%3A//github.com/yushuowiki/FUGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17034v2&entry.124074799=Read"},
{"title": "SceneGPT: A Language Model for 3D Scene Understanding", "author": "Shivam Chandhok", "abstract": "  Building models that can understand and reason about 3D scenes is difficult\nowing to the lack of data sources for 3D supervised training and large-scale\ntraining regimes. In this work we ask - How can the knowledge in a pre-trained\nlanguage model be leveraged for 3D scene understanding without any 3D\npre-training. The aim of this work is to establish whether pre-trained LLMs\npossess priors/knowledge required for reasoning in 3D space and how can we\nprompt them such that they can be used for general purpose spatial reasoning\nand object understanding in 3D. To this end, we present SceneGPT, an LLM based\nscene understanding system which can perform 3D spatial reasoning without\ntraining or explicit 3D supervision. The key components of our framework are -\n1) a 3D scene graph, that serves as scene representation, encoding the objects\nin the scene and their spatial relationships 2) a pre-trained LLM that can be\nadapted with in context learning for 3D spatial reasoning. We evaluate our\nframework qualitatively on object and scene understanding tasks including\nobject semantics, physical properties and affordances (object-level) and\nspatial understanding (scene-level).\n", "link": "http://arxiv.org/abs/2408.06926v1", "date": "2024-08-13", "relevancy": 2.2592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.58}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5678}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneGPT%3A%20A%20Language%20Model%20for%203D%20Scene%20Understanding&body=Title%3A%20SceneGPT%3A%20A%20Language%20Model%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Shivam%20Chandhok%0AAbstract%3A%20%20%20Building%20models%20that%20can%20understand%20and%20reason%20about%203D%20scenes%20is%20difficult%0Aowing%20to%20the%20lack%20of%20data%20sources%20for%203D%20supervised%20training%20and%20large-scale%0Atraining%20regimes.%20In%20this%20work%20we%20ask%20-%20How%20can%20the%20knowledge%20in%20a%20pre-trained%0Alanguage%20model%20be%20leveraged%20for%203D%20scene%20understanding%20without%20any%203D%0Apre-training.%20The%20aim%20of%20this%20work%20is%20to%20establish%20whether%20pre-trained%20LLMs%0Apossess%20priors/knowledge%20required%20for%20reasoning%20in%203D%20space%20and%20how%20can%20we%0Aprompt%20them%20such%20that%20they%20can%20be%20used%20for%20general%20purpose%20spatial%20reasoning%0Aand%20object%20understanding%20in%203D.%20To%20this%20end%2C%20we%20present%20SceneGPT%2C%20an%20LLM%20based%0Ascene%20understanding%20system%20which%20can%20perform%203D%20spatial%20reasoning%20without%0Atraining%20or%20explicit%203D%20supervision.%20The%20key%20components%20of%20our%20framework%20are%20-%0A1%29%20a%203D%20scene%20graph%2C%20that%20serves%20as%20scene%20representation%2C%20encoding%20the%20objects%0Ain%20the%20scene%20and%20their%20spatial%20relationships%202%29%20a%20pre-trained%20LLM%20that%20can%20be%0Aadapted%20with%20in%20context%20learning%20for%203D%20spatial%20reasoning.%20We%20evaluate%20our%0Aframework%20qualitatively%20on%20object%20and%20scene%20understanding%20tasks%20including%0Aobject%20semantics%2C%20physical%20properties%20and%20affordances%20%28object-level%29%20and%0Aspatial%20understanding%20%28scene-level%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneGPT%253A%2520A%2520Language%2520Model%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DShivam%2520Chandhok%26entry.1292438233%3D%2520%2520Building%2520models%2520that%2520can%2520understand%2520and%2520reason%2520about%25203D%2520scenes%2520is%2520difficult%250Aowing%2520to%2520the%2520lack%2520of%2520data%2520sources%2520for%25203D%2520supervised%2520training%2520and%2520large-scale%250Atraining%2520regimes.%2520In%2520this%2520work%2520we%2520ask%2520-%2520How%2520can%2520the%2520knowledge%2520in%2520a%2520pre-trained%250Alanguage%2520model%2520be%2520leveraged%2520for%25203D%2520scene%2520understanding%2520without%2520any%25203D%250Apre-training.%2520The%2520aim%2520of%2520this%2520work%2520is%2520to%2520establish%2520whether%2520pre-trained%2520LLMs%250Apossess%2520priors/knowledge%2520required%2520for%2520reasoning%2520in%25203D%2520space%2520and%2520how%2520can%2520we%250Aprompt%2520them%2520such%2520that%2520they%2520can%2520be%2520used%2520for%2520general%2520purpose%2520spatial%2520reasoning%250Aand%2520object%2520understanding%2520in%25203D.%2520To%2520this%2520end%252C%2520we%2520present%2520SceneGPT%252C%2520an%2520LLM%2520based%250Ascene%2520understanding%2520system%2520which%2520can%2520perform%25203D%2520spatial%2520reasoning%2520without%250Atraining%2520or%2520explicit%25203D%2520supervision.%2520The%2520key%2520components%2520of%2520our%2520framework%2520are%2520-%250A1%2529%2520a%25203D%2520scene%2520graph%252C%2520that%2520serves%2520as%2520scene%2520representation%252C%2520encoding%2520the%2520objects%250Ain%2520the%2520scene%2520and%2520their%2520spatial%2520relationships%25202%2529%2520a%2520pre-trained%2520LLM%2520that%2520can%2520be%250Aadapted%2520with%2520in%2520context%2520learning%2520for%25203D%2520spatial%2520reasoning.%2520We%2520evaluate%2520our%250Aframework%2520qualitatively%2520on%2520object%2520and%2520scene%2520understanding%2520tasks%2520including%250Aobject%2520semantics%252C%2520physical%2520properties%2520and%2520affordances%2520%2528object-level%2529%2520and%250Aspatial%2520understanding%2520%2528scene-level%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneGPT%3A%20A%20Language%20Model%20for%203D%20Scene%20Understanding&entry.906535625=Shivam%20Chandhok&entry.1292438233=%20%20Building%20models%20that%20can%20understand%20and%20reason%20about%203D%20scenes%20is%20difficult%0Aowing%20to%20the%20lack%20of%20data%20sources%20for%203D%20supervised%20training%20and%20large-scale%0Atraining%20regimes.%20In%20this%20work%20we%20ask%20-%20How%20can%20the%20knowledge%20in%20a%20pre-trained%0Alanguage%20model%20be%20leveraged%20for%203D%20scene%20understanding%20without%20any%203D%0Apre-training.%20The%20aim%20of%20this%20work%20is%20to%20establish%20whether%20pre-trained%20LLMs%0Apossess%20priors/knowledge%20required%20for%20reasoning%20in%203D%20space%20and%20how%20can%20we%0Aprompt%20them%20such%20that%20they%20can%20be%20used%20for%20general%20purpose%20spatial%20reasoning%0Aand%20object%20understanding%20in%203D.%20To%20this%20end%2C%20we%20present%20SceneGPT%2C%20an%20LLM%20based%0Ascene%20understanding%20system%20which%20can%20perform%203D%20spatial%20reasoning%20without%0Atraining%20or%20explicit%203D%20supervision.%20The%20key%20components%20of%20our%20framework%20are%20-%0A1%29%20a%203D%20scene%20graph%2C%20that%20serves%20as%20scene%20representation%2C%20encoding%20the%20objects%0Ain%20the%20scene%20and%20their%20spatial%20relationships%202%29%20a%20pre-trained%20LLM%20that%20can%20be%0Aadapted%20with%20in%20context%20learning%20for%203D%20spatial%20reasoning.%20We%20evaluate%20our%0Aframework%20qualitatively%20on%20object%20and%20scene%20understanding%20tasks%20including%0Aobject%20semantics%2C%20physical%20properties%20and%20affordances%20%28object-level%29%20and%0Aspatial%20understanding%20%28scene-level%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06926v1&entry.124074799=Read"},
{"title": "Weakly Supervised Video Anomaly Detection and Localization with\n  Spatio-Temporal Prompts", "author": "Peng Wu and Xuerong Zhou and Guansong Pang and Zhiwei Yang and Qingsen Yan and Peng Wang and Yanning Zhang", "abstract": "  Current weakly supervised video anomaly detection (WSVAD) task aims to\nachieve frame-level anomalous event detection with only coarse video-level\nannotations available. Existing works typically involve extracting global\nfeatures from full-resolution video frames and training frame-level classifiers\nto detect anomalies in the temporal dimension. However, most anomalous events\ntend to occur in localized spatial regions rather than the entire video frames,\nwhich implies existing frame-level feature based works may be misled by the\ndominant background information and lack the interpretation of the detected\nanomalies. To address this dilemma, this paper introduces a novel method called\nSTPrompt that learns spatio-temporal prompt embeddings for weakly supervised\nvideo anomaly detection and localization (WSVADL) based on pre-trained\nvision-language models (VLMs). Our proposed method employs a two-stream network\nstructure, with one stream focusing on the temporal dimension and the other\nprimarily on the spatial dimension. By leveraging the learned knowledge from\npre-trained VLMs and incorporating natural motion priors from raw videos, our\nmodel learns prompt embeddings that are aligned with spatio-temporal regions of\nvideos (e.g., patches of individual frames) for identify specific local regions\nof anomalies, enabling accurate video anomaly detection while mitigating the\ninfluence of background information. Without relying on detailed\nspatio-temporal annotations or auxiliary object detection/tracking, our method\nachieves state-of-the-art performance on three public benchmarks for the WSVADL\ntask.\n", "link": "http://arxiv.org/abs/2408.05905v2", "date": "2024-08-13", "relevancy": 2.2494, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6112}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5614}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Video%20Anomaly%20Detection%20and%20Localization%20with%0A%20%20Spatio-Temporal%20Prompts&body=Title%3A%20Weakly%20Supervised%20Video%20Anomaly%20Detection%20and%20Localization%20with%0A%20%20Spatio-Temporal%20Prompts%0AAuthor%3A%20Peng%20Wu%20and%20Xuerong%20Zhou%20and%20Guansong%20Pang%20and%20Zhiwei%20Yang%20and%20Qingsen%20Yan%20and%20Peng%20Wang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20Current%20weakly%20supervised%20video%20anomaly%20detection%20%28WSVAD%29%20task%20aims%20to%0Aachieve%20frame-level%20anomalous%20event%20detection%20with%20only%20coarse%20video-level%0Aannotations%20available.%20Existing%20works%20typically%20involve%20extracting%20global%0Afeatures%20from%20full-resolution%20video%20frames%20and%20training%20frame-level%20classifiers%0Ato%20detect%20anomalies%20in%20the%20temporal%20dimension.%20However%2C%20most%20anomalous%20events%0Atend%20to%20occur%20in%20localized%20spatial%20regions%20rather%20than%20the%20entire%20video%20frames%2C%0Awhich%20implies%20existing%20frame-level%20feature%20based%20works%20may%20be%20misled%20by%20the%0Adominant%20background%20information%20and%20lack%20the%20interpretation%20of%20the%20detected%0Aanomalies.%20To%20address%20this%20dilemma%2C%20this%20paper%20introduces%20a%20novel%20method%20called%0ASTPrompt%20that%20learns%20spatio-temporal%20prompt%20embeddings%20for%20weakly%20supervised%0Avideo%20anomaly%20detection%20and%20localization%20%28WSVADL%29%20based%20on%20pre-trained%0Avision-language%20models%20%28VLMs%29.%20Our%20proposed%20method%20employs%20a%20two-stream%20network%0Astructure%2C%20with%20one%20stream%20focusing%20on%20the%20temporal%20dimension%20and%20the%20other%0Aprimarily%20on%20the%20spatial%20dimension.%20By%20leveraging%20the%20learned%20knowledge%20from%0Apre-trained%20VLMs%20and%20incorporating%20natural%20motion%20priors%20from%20raw%20videos%2C%20our%0Amodel%20learns%20prompt%20embeddings%20that%20are%20aligned%20with%20spatio-temporal%20regions%20of%0Avideos%20%28e.g.%2C%20patches%20of%20individual%20frames%29%20for%20identify%20specific%20local%20regions%0Aof%20anomalies%2C%20enabling%20accurate%20video%20anomaly%20detection%20while%20mitigating%20the%0Ainfluence%20of%20background%20information.%20Without%20relying%20on%20detailed%0Aspatio-temporal%20annotations%20or%20auxiliary%20object%20detection/tracking%2C%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20three%20public%20benchmarks%20for%20the%20WSVADL%0Atask.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Video%2520Anomaly%2520Detection%2520and%2520Localization%2520with%250A%2520%2520Spatio-Temporal%2520Prompts%26entry.906535625%3DPeng%2520Wu%2520and%2520Xuerong%2520Zhou%2520and%2520Guansong%2520Pang%2520and%2520Zhiwei%2520Yang%2520and%2520Qingsen%2520Yan%2520and%2520Peng%2520Wang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520weakly%2520supervised%2520video%2520anomaly%2520detection%2520%2528WSVAD%2529%2520task%2520aims%2520to%250Aachieve%2520frame-level%2520anomalous%2520event%2520detection%2520with%2520only%2520coarse%2520video-level%250Aannotations%2520available.%2520Existing%2520works%2520typically%2520involve%2520extracting%2520global%250Afeatures%2520from%2520full-resolution%2520video%2520frames%2520and%2520training%2520frame-level%2520classifiers%250Ato%2520detect%2520anomalies%2520in%2520the%2520temporal%2520dimension.%2520However%252C%2520most%2520anomalous%2520events%250Atend%2520to%2520occur%2520in%2520localized%2520spatial%2520regions%2520rather%2520than%2520the%2520entire%2520video%2520frames%252C%250Awhich%2520implies%2520existing%2520frame-level%2520feature%2520based%2520works%2520may%2520be%2520misled%2520by%2520the%250Adominant%2520background%2520information%2520and%2520lack%2520the%2520interpretation%2520of%2520the%2520detected%250Aanomalies.%2520To%2520address%2520this%2520dilemma%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520method%2520called%250ASTPrompt%2520that%2520learns%2520spatio-temporal%2520prompt%2520embeddings%2520for%2520weakly%2520supervised%250Avideo%2520anomaly%2520detection%2520and%2520localization%2520%2528WSVADL%2529%2520based%2520on%2520pre-trained%250Avision-language%2520models%2520%2528VLMs%2529.%2520Our%2520proposed%2520method%2520employs%2520a%2520two-stream%2520network%250Astructure%252C%2520with%2520one%2520stream%2520focusing%2520on%2520the%2520temporal%2520dimension%2520and%2520the%2520other%250Aprimarily%2520on%2520the%2520spatial%2520dimension.%2520By%2520leveraging%2520the%2520learned%2520knowledge%2520from%250Apre-trained%2520VLMs%2520and%2520incorporating%2520natural%2520motion%2520priors%2520from%2520raw%2520videos%252C%2520our%250Amodel%2520learns%2520prompt%2520embeddings%2520that%2520are%2520aligned%2520with%2520spatio-temporal%2520regions%2520of%250Avideos%2520%2528e.g.%252C%2520patches%2520of%2520individual%2520frames%2529%2520for%2520identify%2520specific%2520local%2520regions%250Aof%2520anomalies%252C%2520enabling%2520accurate%2520video%2520anomaly%2520detection%2520while%2520mitigating%2520the%250Ainfluence%2520of%2520background%2520information.%2520Without%2520relying%2520on%2520detailed%250Aspatio-temporal%2520annotations%2520or%2520auxiliary%2520object%2520detection/tracking%252C%2520our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520on%2520three%2520public%2520benchmarks%2520for%2520the%2520WSVADL%250Atask.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Video%20Anomaly%20Detection%20and%20Localization%20with%0A%20%20Spatio-Temporal%20Prompts&entry.906535625=Peng%20Wu%20and%20Xuerong%20Zhou%20and%20Guansong%20Pang%20and%20Zhiwei%20Yang%20and%20Qingsen%20Yan%20and%20Peng%20Wang%20and%20Yanning%20Zhang&entry.1292438233=%20%20Current%20weakly%20supervised%20video%20anomaly%20detection%20%28WSVAD%29%20task%20aims%20to%0Aachieve%20frame-level%20anomalous%20event%20detection%20with%20only%20coarse%20video-level%0Aannotations%20available.%20Existing%20works%20typically%20involve%20extracting%20global%0Afeatures%20from%20full-resolution%20video%20frames%20and%20training%20frame-level%20classifiers%0Ato%20detect%20anomalies%20in%20the%20temporal%20dimension.%20However%2C%20most%20anomalous%20events%0Atend%20to%20occur%20in%20localized%20spatial%20regions%20rather%20than%20the%20entire%20video%20frames%2C%0Awhich%20implies%20existing%20frame-level%20feature%20based%20works%20may%20be%20misled%20by%20the%0Adominant%20background%20information%20and%20lack%20the%20interpretation%20of%20the%20detected%0Aanomalies.%20To%20address%20this%20dilemma%2C%20this%20paper%20introduces%20a%20novel%20method%20called%0ASTPrompt%20that%20learns%20spatio-temporal%20prompt%20embeddings%20for%20weakly%20supervised%0Avideo%20anomaly%20detection%20and%20localization%20%28WSVADL%29%20based%20on%20pre-trained%0Avision-language%20models%20%28VLMs%29.%20Our%20proposed%20method%20employs%20a%20two-stream%20network%0Astructure%2C%20with%20one%20stream%20focusing%20on%20the%20temporal%20dimension%20and%20the%20other%0Aprimarily%20on%20the%20spatial%20dimension.%20By%20leveraging%20the%20learned%20knowledge%20from%0Apre-trained%20VLMs%20and%20incorporating%20natural%20motion%20priors%20from%20raw%20videos%2C%20our%0Amodel%20learns%20prompt%20embeddings%20that%20are%20aligned%20with%20spatio-temporal%20regions%20of%0Avideos%20%28e.g.%2C%20patches%20of%20individual%20frames%29%20for%20identify%20specific%20local%20regions%0Aof%20anomalies%2C%20enabling%20accurate%20video%20anomaly%20detection%20while%20mitigating%20the%0Ainfluence%20of%20background%20information.%20Without%20relying%20on%20detailed%0Aspatio-temporal%20annotations%20or%20auxiliary%20object%20detection/tracking%2C%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20three%20public%20benchmarks%20for%20the%20WSVADL%0Atask.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05905v2&entry.124074799=Read"},
{"title": "Token Compensator: Altering Inference Cost of Vision Transformer without\n  Re-Tuning", "author": "Shibo Jie and Yehui Tang and Jianyuan Guo and Zhi-Hong Deng and Kai Han and Yunhe Wang", "abstract": "  Token compression expedites the training and inference of Vision Transformers\n(ViTs) by reducing the number of the redundant tokens, e.g., pruning\ninattentive tokens or merging similar tokens. However, when applied to\ndownstream tasks, these approaches suffer from significant performance drop\nwhen the compression degrees are mismatched between training and inference\nstages, which limits the application of token compression on off-the-shelf\ntrained models. In this paper, we propose a model arithmetic framework to\ndecouple the compression degrees between the two stages. In advance, we\nadditionally perform a fast parameter-efficient self-distillation stage on the\npre-trained models to obtain a small plugin, called Token Compensator (ToCom),\nwhich describes the gap between models across different compression degrees.\nDuring inference, ToCom can be directly inserted into any downstream\noff-the-shelf models with any mismatched training and inference compression\ndegrees to acquire universal performance improvements without further training.\nExperiments on over 20 downstream tasks demonstrate the effectiveness of our\nframework. On CIFAR100, fine-grained visual classification, and VTAB-1k, ToCom\ncan yield up to a maximum improvement of 2.3%, 1.5%, and 2.0% in the average\nperformance of DeiT-B, respectively. Code: https://github.com/JieShibo/ToCom\n", "link": "http://arxiv.org/abs/2408.06798v1", "date": "2024-08-13", "relevancy": 2.2478, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.618}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5295}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Compensator%3A%20Altering%20Inference%20Cost%20of%20Vision%20Transformer%20without%0A%20%20Re-Tuning&body=Title%3A%20Token%20Compensator%3A%20Altering%20Inference%20Cost%20of%20Vision%20Transformer%20without%0A%20%20Re-Tuning%0AAuthor%3A%20Shibo%20Jie%20and%20Yehui%20Tang%20and%20Jianyuan%20Guo%20and%20Zhi-Hong%20Deng%20and%20Kai%20Han%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Token%20compression%20expedites%20the%20training%20and%20inference%20of%20Vision%20Transformers%0A%28ViTs%29%20by%20reducing%20the%20number%20of%20the%20redundant%20tokens%2C%20e.g.%2C%20pruning%0Ainattentive%20tokens%20or%20merging%20similar%20tokens.%20However%2C%20when%20applied%20to%0Adownstream%20tasks%2C%20these%20approaches%20suffer%20from%20significant%20performance%20drop%0Awhen%20the%20compression%20degrees%20are%20mismatched%20between%20training%20and%20inference%0Astages%2C%20which%20limits%20the%20application%20of%20token%20compression%20on%20off-the-shelf%0Atrained%20models.%20In%20this%20paper%2C%20we%20propose%20a%20model%20arithmetic%20framework%20to%0Adecouple%20the%20compression%20degrees%20between%20the%20two%20stages.%20In%20advance%2C%20we%0Aadditionally%20perform%20a%20fast%20parameter-efficient%20self-distillation%20stage%20on%20the%0Apre-trained%20models%20to%20obtain%20a%20small%20plugin%2C%20called%20Token%20Compensator%20%28ToCom%29%2C%0Awhich%20describes%20the%20gap%20between%20models%20across%20different%20compression%20degrees.%0ADuring%20inference%2C%20ToCom%20can%20be%20directly%20inserted%20into%20any%20downstream%0Aoff-the-shelf%20models%20with%20any%20mismatched%20training%20and%20inference%20compression%0Adegrees%20to%20acquire%20universal%20performance%20improvements%20without%20further%20training.%0AExperiments%20on%20over%2020%20downstream%20tasks%20demonstrate%20the%20effectiveness%20of%20our%0Aframework.%20On%20CIFAR100%2C%20fine-grained%20visual%20classification%2C%20and%20VTAB-1k%2C%20ToCom%0Acan%20yield%20up%20to%20a%20maximum%20improvement%20of%202.3%25%2C%201.5%25%2C%20and%202.0%25%20in%20the%20average%0Aperformance%20of%20DeiT-B%2C%20respectively.%20Code%3A%20https%3A//github.com/JieShibo/ToCom%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Compensator%253A%2520Altering%2520Inference%2520Cost%2520of%2520Vision%2520Transformer%2520without%250A%2520%2520Re-Tuning%26entry.906535625%3DShibo%2520Jie%2520and%2520Yehui%2520Tang%2520and%2520Jianyuan%2520Guo%2520and%2520Zhi-Hong%2520Deng%2520and%2520Kai%2520Han%2520and%2520Yunhe%2520Wang%26entry.1292438233%3D%2520%2520Token%2520compression%2520expedites%2520the%2520training%2520and%2520inference%2520of%2520Vision%2520Transformers%250A%2528ViTs%2529%2520by%2520reducing%2520the%2520number%2520of%2520the%2520redundant%2520tokens%252C%2520e.g.%252C%2520pruning%250Ainattentive%2520tokens%2520or%2520merging%2520similar%2520tokens.%2520However%252C%2520when%2520applied%2520to%250Adownstream%2520tasks%252C%2520these%2520approaches%2520suffer%2520from%2520significant%2520performance%2520drop%250Awhen%2520the%2520compression%2520degrees%2520are%2520mismatched%2520between%2520training%2520and%2520inference%250Astages%252C%2520which%2520limits%2520the%2520application%2520of%2520token%2520compression%2520on%2520off-the-shelf%250Atrained%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520model%2520arithmetic%2520framework%2520to%250Adecouple%2520the%2520compression%2520degrees%2520between%2520the%2520two%2520stages.%2520In%2520advance%252C%2520we%250Aadditionally%2520perform%2520a%2520fast%2520parameter-efficient%2520self-distillation%2520stage%2520on%2520the%250Apre-trained%2520models%2520to%2520obtain%2520a%2520small%2520plugin%252C%2520called%2520Token%2520Compensator%2520%2528ToCom%2529%252C%250Awhich%2520describes%2520the%2520gap%2520between%2520models%2520across%2520different%2520compression%2520degrees.%250ADuring%2520inference%252C%2520ToCom%2520can%2520be%2520directly%2520inserted%2520into%2520any%2520downstream%250Aoff-the-shelf%2520models%2520with%2520any%2520mismatched%2520training%2520and%2520inference%2520compression%250Adegrees%2520to%2520acquire%2520universal%2520performance%2520improvements%2520without%2520further%2520training.%250AExperiments%2520on%2520over%252020%2520downstream%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aframework.%2520On%2520CIFAR100%252C%2520fine-grained%2520visual%2520classification%252C%2520and%2520VTAB-1k%252C%2520ToCom%250Acan%2520yield%2520up%2520to%2520a%2520maximum%2520improvement%2520of%25202.3%2525%252C%25201.5%2525%252C%2520and%25202.0%2525%2520in%2520the%2520average%250Aperformance%2520of%2520DeiT-B%252C%2520respectively.%2520Code%253A%2520https%253A//github.com/JieShibo/ToCom%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Compensator%3A%20Altering%20Inference%20Cost%20of%20Vision%20Transformer%20without%0A%20%20Re-Tuning&entry.906535625=Shibo%20Jie%20and%20Yehui%20Tang%20and%20Jianyuan%20Guo%20and%20Zhi-Hong%20Deng%20and%20Kai%20Han%20and%20Yunhe%20Wang&entry.1292438233=%20%20Token%20compression%20expedites%20the%20training%20and%20inference%20of%20Vision%20Transformers%0A%28ViTs%29%20by%20reducing%20the%20number%20of%20the%20redundant%20tokens%2C%20e.g.%2C%20pruning%0Ainattentive%20tokens%20or%20merging%20similar%20tokens.%20However%2C%20when%20applied%20to%0Adownstream%20tasks%2C%20these%20approaches%20suffer%20from%20significant%20performance%20drop%0Awhen%20the%20compression%20degrees%20are%20mismatched%20between%20training%20and%20inference%0Astages%2C%20which%20limits%20the%20application%20of%20token%20compression%20on%20off-the-shelf%0Atrained%20models.%20In%20this%20paper%2C%20we%20propose%20a%20model%20arithmetic%20framework%20to%0Adecouple%20the%20compression%20degrees%20between%20the%20two%20stages.%20In%20advance%2C%20we%0Aadditionally%20perform%20a%20fast%20parameter-efficient%20self-distillation%20stage%20on%20the%0Apre-trained%20models%20to%20obtain%20a%20small%20plugin%2C%20called%20Token%20Compensator%20%28ToCom%29%2C%0Awhich%20describes%20the%20gap%20between%20models%20across%20different%20compression%20degrees.%0ADuring%20inference%2C%20ToCom%20can%20be%20directly%20inserted%20into%20any%20downstream%0Aoff-the-shelf%20models%20with%20any%20mismatched%20training%20and%20inference%20compression%0Adegrees%20to%20acquire%20universal%20performance%20improvements%20without%20further%20training.%0AExperiments%20on%20over%2020%20downstream%20tasks%20demonstrate%20the%20effectiveness%20of%20our%0Aframework.%20On%20CIFAR100%2C%20fine-grained%20visual%20classification%2C%20and%20VTAB-1k%2C%20ToCom%0Acan%20yield%20up%20to%20a%20maximum%20improvement%20of%202.3%25%2C%201.5%25%2C%20and%202.0%25%20in%20the%20average%0Aperformance%20of%20DeiT-B%2C%20respectively.%20Code%3A%20https%3A//github.com/JieShibo/ToCom%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06798v1&entry.124074799=Read"},
{"title": "Event-Stream Super Resolution using Sigma-Delta Neural Network", "author": "Waseem Shariff and Joe Lemley and Peter Corcoran", "abstract": "  This study introduces a novel approach to enhance the spatial-temporal\nresolution of time-event pixels based on luminance changes captured by event\ncameras. These cameras present unique challenges due to their low resolution\nand the sparse, asynchronous nature of the data they collect. Current event\nsuper-resolution algorithms are not fully optimized for the distinct data\nstructure produced by event cameras, resulting in inefficiencies in capturing\nthe full dynamism and detail of visual scenes with improved computational\ncomplexity. To bridge this gap, our research proposes a method that integrates\nbinary spikes with Sigma Delta Neural Networks (SDNNs), leveraging\nspatiotemporal constraint learning mechanism designed to simultaneously learn\nthe spatial and temporal distributions of the event stream. The proposed\nnetwork is evaluated using widely recognized benchmark datasets, including\nN-MNIST, CIFAR10-DVS, ASL-DVS, and Event-NFS. A comprehensive evaluation\nframework is employed, assessing both the accuracy, through root mean square\nerror (RMSE), and the computational efficiency of our model. The findings\ndemonstrate significant improvements over existing state-of-the-art methods,\nspecifically, the proposed method outperforms state-of-the-art performance in\ncomputational efficiency, achieving a 17.04-fold improvement in event sparsity\nand a 32.28-fold increase in synaptic operation efficiency over traditional\nartificial neural networks, alongside a two-fold better performance over\nspiking neural networks.\n", "link": "http://arxiv.org/abs/2408.06968v1", "date": "2024-08-13", "relevancy": 2.233, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6008}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5628}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Stream%20Super%20Resolution%20using%20Sigma-Delta%20Neural%20Network&body=Title%3A%20Event-Stream%20Super%20Resolution%20using%20Sigma-Delta%20Neural%20Network%0AAuthor%3A%20Waseem%20Shariff%20and%20Joe%20Lemley%20and%20Peter%20Corcoran%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20novel%20approach%20to%20enhance%20the%20spatial-temporal%0Aresolution%20of%20time-event%20pixels%20based%20on%20luminance%20changes%20captured%20by%20event%0Acameras.%20These%20cameras%20present%20unique%20challenges%20due%20to%20their%20low%20resolution%0Aand%20the%20sparse%2C%20asynchronous%20nature%20of%20the%20data%20they%20collect.%20Current%20event%0Asuper-resolution%20algorithms%20are%20not%20fully%20optimized%20for%20the%20distinct%20data%0Astructure%20produced%20by%20event%20cameras%2C%20resulting%20in%20inefficiencies%20in%20capturing%0Athe%20full%20dynamism%20and%20detail%20of%20visual%20scenes%20with%20improved%20computational%0Acomplexity.%20To%20bridge%20this%20gap%2C%20our%20research%20proposes%20a%20method%20that%20integrates%0Abinary%20spikes%20with%20Sigma%20Delta%20Neural%20Networks%20%28SDNNs%29%2C%20leveraging%0Aspatiotemporal%20constraint%20learning%20mechanism%20designed%20to%20simultaneously%20learn%0Athe%20spatial%20and%20temporal%20distributions%20of%20the%20event%20stream.%20The%20proposed%0Anetwork%20is%20evaluated%20using%20widely%20recognized%20benchmark%20datasets%2C%20including%0AN-MNIST%2C%20CIFAR10-DVS%2C%20ASL-DVS%2C%20and%20Event-NFS.%20A%20comprehensive%20evaluation%0Aframework%20is%20employed%2C%20assessing%20both%20the%20accuracy%2C%20through%20root%20mean%20square%0Aerror%20%28RMSE%29%2C%20and%20the%20computational%20efficiency%20of%20our%20model.%20The%20findings%0Ademonstrate%20significant%20improvements%20over%20existing%20state-of-the-art%20methods%2C%0Aspecifically%2C%20the%20proposed%20method%20outperforms%20state-of-the-art%20performance%20in%0Acomputational%20efficiency%2C%20achieving%20a%2017.04-fold%20improvement%20in%20event%20sparsity%0Aand%20a%2032.28-fold%20increase%20in%20synaptic%20operation%20efficiency%20over%20traditional%0Aartificial%20neural%20networks%2C%20alongside%20a%20two-fold%20better%20performance%20over%0Aspiking%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Stream%2520Super%2520Resolution%2520using%2520Sigma-Delta%2520Neural%2520Network%26entry.906535625%3DWaseem%2520Shariff%2520and%2520Joe%2520Lemley%2520and%2520Peter%2520Corcoran%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520novel%2520approach%2520to%2520enhance%2520the%2520spatial-temporal%250Aresolution%2520of%2520time-event%2520pixels%2520based%2520on%2520luminance%2520changes%2520captured%2520by%2520event%250Acameras.%2520These%2520cameras%2520present%2520unique%2520challenges%2520due%2520to%2520their%2520low%2520resolution%250Aand%2520the%2520sparse%252C%2520asynchronous%2520nature%2520of%2520the%2520data%2520they%2520collect.%2520Current%2520event%250Asuper-resolution%2520algorithms%2520are%2520not%2520fully%2520optimized%2520for%2520the%2520distinct%2520data%250Astructure%2520produced%2520by%2520event%2520cameras%252C%2520resulting%2520in%2520inefficiencies%2520in%2520capturing%250Athe%2520full%2520dynamism%2520and%2520detail%2520of%2520visual%2520scenes%2520with%2520improved%2520computational%250Acomplexity.%2520To%2520bridge%2520this%2520gap%252C%2520our%2520research%2520proposes%2520a%2520method%2520that%2520integrates%250Abinary%2520spikes%2520with%2520Sigma%2520Delta%2520Neural%2520Networks%2520%2528SDNNs%2529%252C%2520leveraging%250Aspatiotemporal%2520constraint%2520learning%2520mechanism%2520designed%2520to%2520simultaneously%2520learn%250Athe%2520spatial%2520and%2520temporal%2520distributions%2520of%2520the%2520event%2520stream.%2520The%2520proposed%250Anetwork%2520is%2520evaluated%2520using%2520widely%2520recognized%2520benchmark%2520datasets%252C%2520including%250AN-MNIST%252C%2520CIFAR10-DVS%252C%2520ASL-DVS%252C%2520and%2520Event-NFS.%2520A%2520comprehensive%2520evaluation%250Aframework%2520is%2520employed%252C%2520assessing%2520both%2520the%2520accuracy%252C%2520through%2520root%2520mean%2520square%250Aerror%2520%2528RMSE%2529%252C%2520and%2520the%2520computational%2520efficiency%2520of%2520our%2520model.%2520The%2520findings%250Ademonstrate%2520significant%2520improvements%2520over%2520existing%2520state-of-the-art%2520methods%252C%250Aspecifically%252C%2520the%2520proposed%2520method%2520outperforms%2520state-of-the-art%2520performance%2520in%250Acomputational%2520efficiency%252C%2520achieving%2520a%252017.04-fold%2520improvement%2520in%2520event%2520sparsity%250Aand%2520a%252032.28-fold%2520increase%2520in%2520synaptic%2520operation%2520efficiency%2520over%2520traditional%250Aartificial%2520neural%2520networks%252C%2520alongside%2520a%2520two-fold%2520better%2520performance%2520over%250Aspiking%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Stream%20Super%20Resolution%20using%20Sigma-Delta%20Neural%20Network&entry.906535625=Waseem%20Shariff%20and%20Joe%20Lemley%20and%20Peter%20Corcoran&entry.1292438233=%20%20This%20study%20introduces%20a%20novel%20approach%20to%20enhance%20the%20spatial-temporal%0Aresolution%20of%20time-event%20pixels%20based%20on%20luminance%20changes%20captured%20by%20event%0Acameras.%20These%20cameras%20present%20unique%20challenges%20due%20to%20their%20low%20resolution%0Aand%20the%20sparse%2C%20asynchronous%20nature%20of%20the%20data%20they%20collect.%20Current%20event%0Asuper-resolution%20algorithms%20are%20not%20fully%20optimized%20for%20the%20distinct%20data%0Astructure%20produced%20by%20event%20cameras%2C%20resulting%20in%20inefficiencies%20in%20capturing%0Athe%20full%20dynamism%20and%20detail%20of%20visual%20scenes%20with%20improved%20computational%0Acomplexity.%20To%20bridge%20this%20gap%2C%20our%20research%20proposes%20a%20method%20that%20integrates%0Abinary%20spikes%20with%20Sigma%20Delta%20Neural%20Networks%20%28SDNNs%29%2C%20leveraging%0Aspatiotemporal%20constraint%20learning%20mechanism%20designed%20to%20simultaneously%20learn%0Athe%20spatial%20and%20temporal%20distributions%20of%20the%20event%20stream.%20The%20proposed%0Anetwork%20is%20evaluated%20using%20widely%20recognized%20benchmark%20datasets%2C%20including%0AN-MNIST%2C%20CIFAR10-DVS%2C%20ASL-DVS%2C%20and%20Event-NFS.%20A%20comprehensive%20evaluation%0Aframework%20is%20employed%2C%20assessing%20both%20the%20accuracy%2C%20through%20root%20mean%20square%0Aerror%20%28RMSE%29%2C%20and%20the%20computational%20efficiency%20of%20our%20model.%20The%20findings%0Ademonstrate%20significant%20improvements%20over%20existing%20state-of-the-art%20methods%2C%0Aspecifically%2C%20the%20proposed%20method%20outperforms%20state-of-the-art%20performance%20in%0Acomputational%20efficiency%2C%20achieving%20a%2017.04-fold%20improvement%20in%20event%20sparsity%0Aand%20a%2032.28-fold%20increase%20in%20synaptic%20operation%20efficiency%20over%20traditional%0Aartificial%20neural%20networks%2C%20alongside%20a%20two-fold%20better%20performance%20over%0Aspiking%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06968v1&entry.124074799=Read"},
{"title": "Unmasking the Uniqueness: A Glimpse into Age-Invariant Face Recognition\n  of Indigenous African Faces", "author": "Fakunle Ajewole and Joseph Damilola Akinyemi and Khadijat Tope Ladoja and Olufade Falade Williams Onifade", "abstract": "  The task of recognizing the age-separated faces of an individual,\nAge-Invariant Face Recognition (AIFR), has received considerable research\nefforts in Europe, America, and Asia, compared to Africa. Thus, AIFR research\nefforts have often under-represented/misrepresented the African ethnicity with\nnon-indigenous Africans. This work developed an AIFR system for indigenous\nAfrican faces to reduce the misrepresentation of African ethnicity in facial\nimage analysis research. We adopted a pre-trained deep learning model (VGGFace)\nfor AIFR on a dataset of 5,000 indigenous African faces (FAGE\\_v2) collected\nfor this study. FAGE\\_v2 was curated via Internet image searches of 500\nindividuals evenly distributed across 10 African countries. VGGFace was trained\non FAGE\\_v2 to obtain the best accuracy of 81.80\\%. We also performed\nexperiments on an African-American subset of the CACD dataset and obtained the\nbest accuracy of 91.5\\%. The results show a significant difference in the\nrecognition accuracies of indigenous versus non-indigenous Africans.\n", "link": "http://arxiv.org/abs/2408.06806v1", "date": "2024-08-13", "relevancy": 2.2322, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4747}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4345}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unmasking%20the%20Uniqueness%3A%20A%20Glimpse%20into%20Age-Invariant%20Face%20Recognition%0A%20%20of%20Indigenous%20African%20Faces&body=Title%3A%20Unmasking%20the%20Uniqueness%3A%20A%20Glimpse%20into%20Age-Invariant%20Face%20Recognition%0A%20%20of%20Indigenous%20African%20Faces%0AAuthor%3A%20Fakunle%20Ajewole%20and%20Joseph%20Damilola%20Akinyemi%20and%20Khadijat%20Tope%20Ladoja%20and%20Olufade%20Falade%20Williams%20Onifade%0AAbstract%3A%20%20%20The%20task%20of%20recognizing%20the%20age-separated%20faces%20of%20an%20individual%2C%0AAge-Invariant%20Face%20Recognition%20%28AIFR%29%2C%20has%20received%20considerable%20research%0Aefforts%20in%20Europe%2C%20America%2C%20and%20Asia%2C%20compared%20to%20Africa.%20Thus%2C%20AIFR%20research%0Aefforts%20have%20often%20under-represented/misrepresented%20the%20African%20ethnicity%20with%0Anon-indigenous%20Africans.%20This%20work%20developed%20an%20AIFR%20system%20for%20indigenous%0AAfrican%20faces%20to%20reduce%20the%20misrepresentation%20of%20African%20ethnicity%20in%20facial%0Aimage%20analysis%20research.%20We%20adopted%20a%20pre-trained%20deep%20learning%20model%20%28VGGFace%29%0Afor%20AIFR%20on%20a%20dataset%20of%205%2C000%20indigenous%20African%20faces%20%28FAGE%5C_v2%29%20collected%0Afor%20this%20study.%20FAGE%5C_v2%20was%20curated%20via%20Internet%20image%20searches%20of%20500%0Aindividuals%20evenly%20distributed%20across%2010%20African%20countries.%20VGGFace%20was%20trained%0Aon%20FAGE%5C_v2%20to%20obtain%20the%20best%20accuracy%20of%2081.80%5C%25.%20We%20also%20performed%0Aexperiments%20on%20an%20African-American%20subset%20of%20the%20CACD%20dataset%20and%20obtained%20the%0Abest%20accuracy%20of%2091.5%5C%25.%20The%20results%20show%20a%20significant%20difference%20in%20the%0Arecognition%20accuracies%20of%20indigenous%20versus%20non-indigenous%20Africans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnmasking%2520the%2520Uniqueness%253A%2520A%2520Glimpse%2520into%2520Age-Invariant%2520Face%2520Recognition%250A%2520%2520of%2520Indigenous%2520African%2520Faces%26entry.906535625%3DFakunle%2520Ajewole%2520and%2520Joseph%2520Damilola%2520Akinyemi%2520and%2520Khadijat%2520Tope%2520Ladoja%2520and%2520Olufade%2520Falade%2520Williams%2520Onifade%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520recognizing%2520the%2520age-separated%2520faces%2520of%2520an%2520individual%252C%250AAge-Invariant%2520Face%2520Recognition%2520%2528AIFR%2529%252C%2520has%2520received%2520considerable%2520research%250Aefforts%2520in%2520Europe%252C%2520America%252C%2520and%2520Asia%252C%2520compared%2520to%2520Africa.%2520Thus%252C%2520AIFR%2520research%250Aefforts%2520have%2520often%2520under-represented/misrepresented%2520the%2520African%2520ethnicity%2520with%250Anon-indigenous%2520Africans.%2520This%2520work%2520developed%2520an%2520AIFR%2520system%2520for%2520indigenous%250AAfrican%2520faces%2520to%2520reduce%2520the%2520misrepresentation%2520of%2520African%2520ethnicity%2520in%2520facial%250Aimage%2520analysis%2520research.%2520We%2520adopted%2520a%2520pre-trained%2520deep%2520learning%2520model%2520%2528VGGFace%2529%250Afor%2520AIFR%2520on%2520a%2520dataset%2520of%25205%252C000%2520indigenous%2520African%2520faces%2520%2528FAGE%255C_v2%2529%2520collected%250Afor%2520this%2520study.%2520FAGE%255C_v2%2520was%2520curated%2520via%2520Internet%2520image%2520searches%2520of%2520500%250Aindividuals%2520evenly%2520distributed%2520across%252010%2520African%2520countries.%2520VGGFace%2520was%2520trained%250Aon%2520FAGE%255C_v2%2520to%2520obtain%2520the%2520best%2520accuracy%2520of%252081.80%255C%2525.%2520We%2520also%2520performed%250Aexperiments%2520on%2520an%2520African-American%2520subset%2520of%2520the%2520CACD%2520dataset%2520and%2520obtained%2520the%250Abest%2520accuracy%2520of%252091.5%255C%2525.%2520The%2520results%2520show%2520a%2520significant%2520difference%2520in%2520the%250Arecognition%2520accuracies%2520of%2520indigenous%2520versus%2520non-indigenous%2520Africans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unmasking%20the%20Uniqueness%3A%20A%20Glimpse%20into%20Age-Invariant%20Face%20Recognition%0A%20%20of%20Indigenous%20African%20Faces&entry.906535625=Fakunle%20Ajewole%20and%20Joseph%20Damilola%20Akinyemi%20and%20Khadijat%20Tope%20Ladoja%20and%20Olufade%20Falade%20Williams%20Onifade&entry.1292438233=%20%20The%20task%20of%20recognizing%20the%20age-separated%20faces%20of%20an%20individual%2C%0AAge-Invariant%20Face%20Recognition%20%28AIFR%29%2C%20has%20received%20considerable%20research%0Aefforts%20in%20Europe%2C%20America%2C%20and%20Asia%2C%20compared%20to%20Africa.%20Thus%2C%20AIFR%20research%0Aefforts%20have%20often%20under-represented/misrepresented%20the%20African%20ethnicity%20with%0Anon-indigenous%20Africans.%20This%20work%20developed%20an%20AIFR%20system%20for%20indigenous%0AAfrican%20faces%20to%20reduce%20the%20misrepresentation%20of%20African%20ethnicity%20in%20facial%0Aimage%20analysis%20research.%20We%20adopted%20a%20pre-trained%20deep%20learning%20model%20%28VGGFace%29%0Afor%20AIFR%20on%20a%20dataset%20of%205%2C000%20indigenous%20African%20faces%20%28FAGE%5C_v2%29%20collected%0Afor%20this%20study.%20FAGE%5C_v2%20was%20curated%20via%20Internet%20image%20searches%20of%20500%0Aindividuals%20evenly%20distributed%20across%2010%20African%20countries.%20VGGFace%20was%20trained%0Aon%20FAGE%5C_v2%20to%20obtain%20the%20best%20accuracy%20of%2081.80%5C%25.%20We%20also%20performed%0Aexperiments%20on%20an%20African-American%20subset%20of%20the%20CACD%20dataset%20and%20obtained%20the%0Abest%20accuracy%20of%2091.5%5C%25.%20The%20results%20show%20a%20significant%20difference%20in%20the%0Arecognition%20accuracies%20of%20indigenous%20versus%20non-indigenous%20Africans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06806v1&entry.124074799=Read"},
{"title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM", "author": "Quandong Wang and Yuxuan Yuan and Xiaoyu Yang and Ruike Zhang and Kang Zhao and Wei Liu and Jian Luan and Daniel Povey and Bin Wang", "abstract": "  While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.\n", "link": "http://arxiv.org/abs/2406.06571v3", "date": "2024-08-13", "relevancy": 2.2129, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5613}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5596}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUBLLM%3A%20A%20Novel%20Efficient%20Architecture%20with%20Token%20Sequence%20Subsampling%0A%20%20for%20LLM&body=Title%3A%20SUBLLM%3A%20A%20Novel%20Efficient%20Architecture%20with%20Token%20Sequence%20Subsampling%0A%20%20for%20LLM%0AAuthor%3A%20Quandong%20Wang%20and%20Yuxuan%20Yuan%20and%20Xiaoyu%20Yang%20and%20Ruike%20Zhang%20and%20Kang%20Zhao%20and%20Wei%20Liu%20and%20Jian%20Luan%20and%20Daniel%20Povey%20and%20Bin%20Wang%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%0Avarious%20fields%2C%20the%20efficiency%20of%20training%20and%20inference%20remains%20a%20major%0Achallenge.%20To%20address%20this%20issue%2C%20we%20propose%20SUBLLM%2C%20short%20for%0ASubsampling-Upsampling-Bypass%20Large%20Language%20Model%2C%20an%20innovative%20architecture%0Athat%20extends%20the%20core%20decoder-only%20framework%20by%20incorporating%20subsampling%2C%0Aupsampling%2C%20and%20bypass%20modules.%20The%20subsampling%20modules%20are%20responsible%20for%0Ashortening%20the%20sequence%2C%20while%20the%20upsampling%20modules%20restore%20the%20sequence%0Alength%2C%20and%20the%20bypass%20modules%20enhance%20convergence.%20In%20comparison%20to%20LLaMA%2C%20the%0Aproposed%20SUBLLM%20exhibits%20significant%20enhancements%20in%20both%20training%20and%0Ainference%20speeds%20as%20well%20as%20memory%20usage%2C%20while%20maintaining%20competitive%0Afew-shot%20performance.%20During%20training%2C%20SUBLLM%20increases%20speeds%20by%2026%25%20and%20cuts%0Amemory%20by%2010GB%20per%20GPU.%20In%20inference%2C%20it%20boosts%20speeds%20by%20up%20to%2037%25%20and%20reduces%0Amemory%20by%201GB%20per%20GPU.%20The%20training%20and%20inference%20speeds%20can%20be%20enhanced%20by%2034%25%0Aand%2052%25%20respectively%20when%20the%20context%20window%20is%20expanded%20to%208192.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/XiaoMi/subllm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06571v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUBLLM%253A%2520A%2520Novel%2520Efficient%2520Architecture%2520with%2520Token%2520Sequence%2520Subsampling%250A%2520%2520for%2520LLM%26entry.906535625%3DQuandong%2520Wang%2520and%2520Yuxuan%2520Yuan%2520and%2520Xiaoyu%2520Yang%2520and%2520Ruike%2520Zhang%2520and%2520Kang%2520Zhao%2520and%2520Wei%2520Liu%2520and%2520Jian%2520Luan%2520and%2520Daniel%2520Povey%2520and%2520Bin%2520Wang%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%250Avarious%2520fields%252C%2520the%2520efficiency%2520of%2520training%2520and%2520inference%2520remains%2520a%2520major%250Achallenge.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520SUBLLM%252C%2520short%2520for%250ASubsampling-Upsampling-Bypass%2520Large%2520Language%2520Model%252C%2520an%2520innovative%2520architecture%250Athat%2520extends%2520the%2520core%2520decoder-only%2520framework%2520by%2520incorporating%2520subsampling%252C%250Aupsampling%252C%2520and%2520bypass%2520modules.%2520The%2520subsampling%2520modules%2520are%2520responsible%2520for%250Ashortening%2520the%2520sequence%252C%2520while%2520the%2520upsampling%2520modules%2520restore%2520the%2520sequence%250Alength%252C%2520and%2520the%2520bypass%2520modules%2520enhance%2520convergence.%2520In%2520comparison%2520to%2520LLaMA%252C%2520the%250Aproposed%2520SUBLLM%2520exhibits%2520significant%2520enhancements%2520in%2520both%2520training%2520and%250Ainference%2520speeds%2520as%2520well%2520as%2520memory%2520usage%252C%2520while%2520maintaining%2520competitive%250Afew-shot%2520performance.%2520During%2520training%252C%2520SUBLLM%2520increases%2520speeds%2520by%252026%2525%2520and%2520cuts%250Amemory%2520by%252010GB%2520per%2520GPU.%2520In%2520inference%252C%2520it%2520boosts%2520speeds%2520by%2520up%2520to%252037%2525%2520and%2520reduces%250Amemory%2520by%25201GB%2520per%2520GPU.%2520The%2520training%2520and%2520inference%2520speeds%2520can%2520be%2520enhanced%2520by%252034%2525%250Aand%252052%2525%2520respectively%2520when%2520the%2520context%2520window%2520is%2520expanded%2520to%25208192.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/XiaoMi/subllm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06571v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUBLLM%3A%20A%20Novel%20Efficient%20Architecture%20with%20Token%20Sequence%20Subsampling%0A%20%20for%20LLM&entry.906535625=Quandong%20Wang%20and%20Yuxuan%20Yuan%20and%20Xiaoyu%20Yang%20and%20Ruike%20Zhang%20and%20Kang%20Zhao%20and%20Wei%20Liu%20and%20Jian%20Luan%20and%20Daniel%20Povey%20and%20Bin%20Wang&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%0Avarious%20fields%2C%20the%20efficiency%20of%20training%20and%20inference%20remains%20a%20major%0Achallenge.%20To%20address%20this%20issue%2C%20we%20propose%20SUBLLM%2C%20short%20for%0ASubsampling-Upsampling-Bypass%20Large%20Language%20Model%2C%20an%20innovative%20architecture%0Athat%20extends%20the%20core%20decoder-only%20framework%20by%20incorporating%20subsampling%2C%0Aupsampling%2C%20and%20bypass%20modules.%20The%20subsampling%20modules%20are%20responsible%20for%0Ashortening%20the%20sequence%2C%20while%20the%20upsampling%20modules%20restore%20the%20sequence%0Alength%2C%20and%20the%20bypass%20modules%20enhance%20convergence.%20In%20comparison%20to%20LLaMA%2C%20the%0Aproposed%20SUBLLM%20exhibits%20significant%20enhancements%20in%20both%20training%20and%0Ainference%20speeds%20as%20well%20as%20memory%20usage%2C%20while%20maintaining%20competitive%0Afew-shot%20performance.%20During%20training%2C%20SUBLLM%20increases%20speeds%20by%2026%25%20and%20cuts%0Amemory%20by%2010GB%20per%20GPU.%20In%20inference%2C%20it%20boosts%20speeds%20by%20up%20to%2037%25%20and%20reduces%0Amemory%20by%201GB%20per%20GPU.%20The%20training%20and%20inference%20speeds%20can%20be%20enhanced%20by%2034%25%0Aand%2052%25%20respectively%20when%20the%20context%20window%20is%20expanded%20to%208192.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/XiaoMi/subllm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06571v3&entry.124074799=Read"},
{"title": "Regularizing Self-supervised 3D Scene Flows with Surface Awareness and\n  Cyclic Consistency", "author": "Patrik Vacek and David Hurych and Karel Zimmermann and Patrick Perez and Tomas Svoboda", "abstract": "  Learning without supervision how to predict 3D scene flows from point clouds\nis essential to many perception systems. We propose a novel learning framework\nfor this task which improves the necessary regularization. Relying on the\nassumption that scene elements are mostly rigid, current smoothness losses are\nbuilt on the definition of \"rigid clusters\" in the input point clouds. The\ndefinition of these clusters is challenging and has a significant impact on the\nquality of predicted flows. We introduce two new consistency losses that\nenlarge clusters while preventing them from spreading over distinct objects. In\nparticular, we enforce \\emph{temporal} consistency with a forward-backward\ncyclic loss and \\emph{spatial} consistency by considering surface orientation\nsimilarity in addition to spatial proximity. The proposed losses are\nmodel-independent and can thus be used in a plug-and-play fashion to\nsignificantly improve the performance of existing models, as demonstrated on\ntwo most widely used architectures. We also showcase the effectiveness and\ngeneralization capability of our framework on four standard sensor-unique\ndriving datasets, achieving state-of-the-art performance in 3D scene flow\nestimation. Our codes are available on https://github.com/ctu-vras/sac-flow.\n", "link": "http://arxiv.org/abs/2312.08879v3", "date": "2024-08-13", "relevancy": 2.2076, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularizing%20Self-supervised%203D%20Scene%20Flows%20with%20Surface%20Awareness%20and%0A%20%20Cyclic%20Consistency&body=Title%3A%20Regularizing%20Self-supervised%203D%20Scene%20Flows%20with%20Surface%20Awareness%20and%0A%20%20Cyclic%20Consistency%0AAuthor%3A%20Patrik%20Vacek%20and%20David%20Hurych%20and%20Karel%20Zimmermann%20and%20Patrick%20Perez%20and%20Tomas%20Svoboda%0AAbstract%3A%20%20%20Learning%20without%20supervision%20how%20to%20predict%203D%20scene%20flows%20from%20point%20clouds%0Ais%20essential%20to%20many%20perception%20systems.%20We%20propose%20a%20novel%20learning%20framework%0Afor%20this%20task%20which%20improves%20the%20necessary%20regularization.%20Relying%20on%20the%0Aassumption%20that%20scene%20elements%20are%20mostly%20rigid%2C%20current%20smoothness%20losses%20are%0Abuilt%20on%20the%20definition%20of%20%22rigid%20clusters%22%20in%20the%20input%20point%20clouds.%20The%0Adefinition%20of%20these%20clusters%20is%20challenging%20and%20has%20a%20significant%20impact%20on%20the%0Aquality%20of%20predicted%20flows.%20We%20introduce%20two%20new%20consistency%20losses%20that%0Aenlarge%20clusters%20while%20preventing%20them%20from%20spreading%20over%20distinct%20objects.%20In%0Aparticular%2C%20we%20enforce%20%5Cemph%7Btemporal%7D%20consistency%20with%20a%20forward-backward%0Acyclic%20loss%20and%20%5Cemph%7Bspatial%7D%20consistency%20by%20considering%20surface%20orientation%0Asimilarity%20in%20addition%20to%20spatial%20proximity.%20The%20proposed%20losses%20are%0Amodel-independent%20and%20can%20thus%20be%20used%20in%20a%20plug-and-play%20fashion%20to%0Asignificantly%20improve%20the%20performance%20of%20existing%20models%2C%20as%20demonstrated%20on%0Atwo%20most%20widely%20used%20architectures.%20We%20also%20showcase%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20our%20framework%20on%20four%20standard%20sensor-unique%0Adriving%20datasets%2C%20achieving%20state-of-the-art%20performance%20in%203D%20scene%20flow%0Aestimation.%20Our%20codes%20are%20available%20on%20https%3A//github.com/ctu-vras/sac-flow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08879v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularizing%2520Self-supervised%25203D%2520Scene%2520Flows%2520with%2520Surface%2520Awareness%2520and%250A%2520%2520Cyclic%2520Consistency%26entry.906535625%3DPatrik%2520Vacek%2520and%2520David%2520Hurych%2520and%2520Karel%2520Zimmermann%2520and%2520Patrick%2520Perez%2520and%2520Tomas%2520Svoboda%26entry.1292438233%3D%2520%2520Learning%2520without%2520supervision%2520how%2520to%2520predict%25203D%2520scene%2520flows%2520from%2520point%2520clouds%250Ais%2520essential%2520to%2520many%2520perception%2520systems.%2520We%2520propose%2520a%2520novel%2520learning%2520framework%250Afor%2520this%2520task%2520which%2520improves%2520the%2520necessary%2520regularization.%2520Relying%2520on%2520the%250Aassumption%2520that%2520scene%2520elements%2520are%2520mostly%2520rigid%252C%2520current%2520smoothness%2520losses%2520are%250Abuilt%2520on%2520the%2520definition%2520of%2520%2522rigid%2520clusters%2522%2520in%2520the%2520input%2520point%2520clouds.%2520The%250Adefinition%2520of%2520these%2520clusters%2520is%2520challenging%2520and%2520has%2520a%2520significant%2520impact%2520on%2520the%250Aquality%2520of%2520predicted%2520flows.%2520We%2520introduce%2520two%2520new%2520consistency%2520losses%2520that%250Aenlarge%2520clusters%2520while%2520preventing%2520them%2520from%2520spreading%2520over%2520distinct%2520objects.%2520In%250Aparticular%252C%2520we%2520enforce%2520%255Cemph%257Btemporal%257D%2520consistency%2520with%2520a%2520forward-backward%250Acyclic%2520loss%2520and%2520%255Cemph%257Bspatial%257D%2520consistency%2520by%2520considering%2520surface%2520orientation%250Asimilarity%2520in%2520addition%2520to%2520spatial%2520proximity.%2520The%2520proposed%2520losses%2520are%250Amodel-independent%2520and%2520can%2520thus%2520be%2520used%2520in%2520a%2520plug-and-play%2520fashion%2520to%250Asignificantly%2520improve%2520the%2520performance%2520of%2520existing%2520models%252C%2520as%2520demonstrated%2520on%250Atwo%2520most%2520widely%2520used%2520architectures.%2520We%2520also%2520showcase%2520the%2520effectiveness%2520and%250Ageneralization%2520capability%2520of%2520our%2520framework%2520on%2520four%2520standard%2520sensor-unique%250Adriving%2520datasets%252C%2520achieving%2520state-of-the-art%2520performance%2520in%25203D%2520scene%2520flow%250Aestimation.%2520Our%2520codes%2520are%2520available%2520on%2520https%253A//github.com/ctu-vras/sac-flow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08879v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularizing%20Self-supervised%203D%20Scene%20Flows%20with%20Surface%20Awareness%20and%0A%20%20Cyclic%20Consistency&entry.906535625=Patrik%20Vacek%20and%20David%20Hurych%20and%20Karel%20Zimmermann%20and%20Patrick%20Perez%20and%20Tomas%20Svoboda&entry.1292438233=%20%20Learning%20without%20supervision%20how%20to%20predict%203D%20scene%20flows%20from%20point%20clouds%0Ais%20essential%20to%20many%20perception%20systems.%20We%20propose%20a%20novel%20learning%20framework%0Afor%20this%20task%20which%20improves%20the%20necessary%20regularization.%20Relying%20on%20the%0Aassumption%20that%20scene%20elements%20are%20mostly%20rigid%2C%20current%20smoothness%20losses%20are%0Abuilt%20on%20the%20definition%20of%20%22rigid%20clusters%22%20in%20the%20input%20point%20clouds.%20The%0Adefinition%20of%20these%20clusters%20is%20challenging%20and%20has%20a%20significant%20impact%20on%20the%0Aquality%20of%20predicted%20flows.%20We%20introduce%20two%20new%20consistency%20losses%20that%0Aenlarge%20clusters%20while%20preventing%20them%20from%20spreading%20over%20distinct%20objects.%20In%0Aparticular%2C%20we%20enforce%20%5Cemph%7Btemporal%7D%20consistency%20with%20a%20forward-backward%0Acyclic%20loss%20and%20%5Cemph%7Bspatial%7D%20consistency%20by%20considering%20surface%20orientation%0Asimilarity%20in%20addition%20to%20spatial%20proximity.%20The%20proposed%20losses%20are%0Amodel-independent%20and%20can%20thus%20be%20used%20in%20a%20plug-and-play%20fashion%20to%0Asignificantly%20improve%20the%20performance%20of%20existing%20models%2C%20as%20demonstrated%20on%0Atwo%20most%20widely%20used%20architectures.%20We%20also%20showcase%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20our%20framework%20on%20four%20standard%20sensor-unique%0Adriving%20datasets%2C%20achieving%20state-of-the-art%20performance%20in%203D%20scene%20flow%0Aestimation.%20Our%20codes%20are%20available%20on%20https%3A//github.com/ctu-vras/sac-flow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08879v3&entry.124074799=Read"},
{"title": "PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot\n  Soundscape Mapping", "author": "Subash Khanal and Eric Xing and Srikumar Sastry and Aayush Dhakal and Zhexiao Xiong and Adeel Ahmad and Nathan Jacobs", "abstract": "  A soundscape is defined by the acoustic environment a person perceives at a\nlocation. In this work, we propose a framework for mapping soundscapes across\nthe Earth. Since soundscapes involve sound distributions that span varying\nspatial scales, we represent locations with multi-scale satellite imagery and\nlearn a joint representation among this imagery, audio, and text. To capture\nthe inherent uncertainty in the soundscape of a location, we design the\nrepresentation space to be probabilistic. We also fuse ubiquitous metadata\n(including geolocation, time, and data source) to enable learning of spatially\nand temporally dynamic representations of soundscapes. We demonstrate the\nutility of our framework by creating large-scale soundscape maps integrating\nboth audio and text with temporal control. To facilitate future research on\nthis task, we also introduce a large-scale dataset, GeoSound, containing over\n$300k$ geotagged audio samples paired with both low- and high-resolution\nsatellite imagery. We demonstrate that our method outperforms the existing\nstate-of-the-art on both GeoSound and the existing SoundingEarth dataset. Our\ndataset and code is available at https://github.com/mvrl/PSM.\n", "link": "http://arxiv.org/abs/2408.07050v1", "date": "2024-08-13", "relevancy": 2.207, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSM%3A%20Learning%20Probabilistic%20Embeddings%20for%20Multi-scale%20Zero-Shot%0A%20%20Soundscape%20Mapping&body=Title%3A%20PSM%3A%20Learning%20Probabilistic%20Embeddings%20for%20Multi-scale%20Zero-Shot%0A%20%20Soundscape%20Mapping%0AAuthor%3A%20Subash%20Khanal%20and%20Eric%20Xing%20and%20Srikumar%20Sastry%20and%20Aayush%20Dhakal%20and%20Zhexiao%20Xiong%20and%20Adeel%20Ahmad%20and%20Nathan%20Jacobs%0AAbstract%3A%20%20%20A%20soundscape%20is%20defined%20by%20the%20acoustic%20environment%20a%20person%20perceives%20at%20a%0Alocation.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%20mapping%20soundscapes%20across%0Athe%20Earth.%20Since%20soundscapes%20involve%20sound%20distributions%20that%20span%20varying%0Aspatial%20scales%2C%20we%20represent%20locations%20with%20multi-scale%20satellite%20imagery%20and%0Alearn%20a%20joint%20representation%20among%20this%20imagery%2C%20audio%2C%20and%20text.%20To%20capture%0Athe%20inherent%20uncertainty%20in%20the%20soundscape%20of%20a%20location%2C%20we%20design%20the%0Arepresentation%20space%20to%20be%20probabilistic.%20We%20also%20fuse%20ubiquitous%20metadata%0A%28including%20geolocation%2C%20time%2C%20and%20data%20source%29%20to%20enable%20learning%20of%20spatially%0Aand%20temporally%20dynamic%20representations%20of%20soundscapes.%20We%20demonstrate%20the%0Autility%20of%20our%20framework%20by%20creating%20large-scale%20soundscape%20maps%20integrating%0Aboth%20audio%20and%20text%20with%20temporal%20control.%20To%20facilitate%20future%20research%20on%0Athis%20task%2C%20we%20also%20introduce%20a%20large-scale%20dataset%2C%20GeoSound%2C%20containing%20over%0A%24300k%24%20geotagged%20audio%20samples%20paired%20with%20both%20low-%20and%20high-resolution%0Asatellite%20imagery.%20We%20demonstrate%20that%20our%20method%20outperforms%20the%20existing%0Astate-of-the-art%20on%20both%20GeoSound%20and%20the%20existing%20SoundingEarth%20dataset.%20Our%0Adataset%20and%20code%20is%20available%20at%20https%3A//github.com/mvrl/PSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSM%253A%2520Learning%2520Probabilistic%2520Embeddings%2520for%2520Multi-scale%2520Zero-Shot%250A%2520%2520Soundscape%2520Mapping%26entry.906535625%3DSubash%2520Khanal%2520and%2520Eric%2520Xing%2520and%2520Srikumar%2520Sastry%2520and%2520Aayush%2520Dhakal%2520and%2520Zhexiao%2520Xiong%2520and%2520Adeel%2520Ahmad%2520and%2520Nathan%2520Jacobs%26entry.1292438233%3D%2520%2520A%2520soundscape%2520is%2520defined%2520by%2520the%2520acoustic%2520environment%2520a%2520person%2520perceives%2520at%2520a%250Alocation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520framework%2520for%2520mapping%2520soundscapes%2520across%250Athe%2520Earth.%2520Since%2520soundscapes%2520involve%2520sound%2520distributions%2520that%2520span%2520varying%250Aspatial%2520scales%252C%2520we%2520represent%2520locations%2520with%2520multi-scale%2520satellite%2520imagery%2520and%250Alearn%2520a%2520joint%2520representation%2520among%2520this%2520imagery%252C%2520audio%252C%2520and%2520text.%2520To%2520capture%250Athe%2520inherent%2520uncertainty%2520in%2520the%2520soundscape%2520of%2520a%2520location%252C%2520we%2520design%2520the%250Arepresentation%2520space%2520to%2520be%2520probabilistic.%2520We%2520also%2520fuse%2520ubiquitous%2520metadata%250A%2528including%2520geolocation%252C%2520time%252C%2520and%2520data%2520source%2529%2520to%2520enable%2520learning%2520of%2520spatially%250Aand%2520temporally%2520dynamic%2520representations%2520of%2520soundscapes.%2520We%2520demonstrate%2520the%250Autility%2520of%2520our%2520framework%2520by%2520creating%2520large-scale%2520soundscape%2520maps%2520integrating%250Aboth%2520audio%2520and%2520text%2520with%2520temporal%2520control.%2520To%2520facilitate%2520future%2520research%2520on%250Athis%2520task%252C%2520we%2520also%2520introduce%2520a%2520large-scale%2520dataset%252C%2520GeoSound%252C%2520containing%2520over%250A%2524300k%2524%2520geotagged%2520audio%2520samples%2520paired%2520with%2520both%2520low-%2520and%2520high-resolution%250Asatellite%2520imagery.%2520We%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520on%2520both%2520GeoSound%2520and%2520the%2520existing%2520SoundingEarth%2520dataset.%2520Our%250Adataset%2520and%2520code%2520is%2520available%2520at%2520https%253A//github.com/mvrl/PSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSM%3A%20Learning%20Probabilistic%20Embeddings%20for%20Multi-scale%20Zero-Shot%0A%20%20Soundscape%20Mapping&entry.906535625=Subash%20Khanal%20and%20Eric%20Xing%20and%20Srikumar%20Sastry%20and%20Aayush%20Dhakal%20and%20Zhexiao%20Xiong%20and%20Adeel%20Ahmad%20and%20Nathan%20Jacobs&entry.1292438233=%20%20A%20soundscape%20is%20defined%20by%20the%20acoustic%20environment%20a%20person%20perceives%20at%20a%0Alocation.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%20mapping%20soundscapes%20across%0Athe%20Earth.%20Since%20soundscapes%20involve%20sound%20distributions%20that%20span%20varying%0Aspatial%20scales%2C%20we%20represent%20locations%20with%20multi-scale%20satellite%20imagery%20and%0Alearn%20a%20joint%20representation%20among%20this%20imagery%2C%20audio%2C%20and%20text.%20To%20capture%0Athe%20inherent%20uncertainty%20in%20the%20soundscape%20of%20a%20location%2C%20we%20design%20the%0Arepresentation%20space%20to%20be%20probabilistic.%20We%20also%20fuse%20ubiquitous%20metadata%0A%28including%20geolocation%2C%20time%2C%20and%20data%20source%29%20to%20enable%20learning%20of%20spatially%0Aand%20temporally%20dynamic%20representations%20of%20soundscapes.%20We%20demonstrate%20the%0Autility%20of%20our%20framework%20by%20creating%20large-scale%20soundscape%20maps%20integrating%0Aboth%20audio%20and%20text%20with%20temporal%20control.%20To%20facilitate%20future%20research%20on%0Athis%20task%2C%20we%20also%20introduce%20a%20large-scale%20dataset%2C%20GeoSound%2C%20containing%20over%0A%24300k%24%20geotagged%20audio%20samples%20paired%20with%20both%20low-%20and%20high-resolution%0Asatellite%20imagery.%20We%20demonstrate%20that%20our%20method%20outperforms%20the%20existing%0Astate-of-the-art%20on%20both%20GeoSound%20and%20the%20existing%20SoundingEarth%20dataset.%20Our%0Adataset%20and%20code%20is%20available%20at%20https%3A//github.com/mvrl/PSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07050v1&entry.124074799=Read"},
{"title": "DA-BEV: Unsupervised Domain Adaptation for Bird's Eye View Perception", "author": "Kai Jiang and Jiaxing Huang and Weiying Xie and Yunsong Li and Ling Shao and Shijian Lu", "abstract": "  Camera-only Bird's Eye View (BEV) has demonstrated great potential in\nenvironment perception in a 3D space. However, most existing studies were\nconducted under a supervised setup which cannot scale well while handling\nvarious new data. Unsupervised domain adaptive BEV, which effective learning\nfrom various unlabelled target data, is far under-explored. In this work, we\ndesign DA-BEV, the first domain adaptive camera-only BEV framework that\naddresses domain adaptive BEV challenges by exploiting the complementary nature\nof image-view features and BEV features. DA-BEV introduces the idea of query\ninto the domain adaptation framework to derive useful information from\nimage-view and BEV features. It consists of two query-based designs, namely,\nquery-based adversarial learning (QAL) and query-based self-training (QST),\nwhich exploits image-view features or BEV features to regularize the adaptation\nof the other. Extensive experiments show that DA-BEV achieves superior domain\nadaptive BEV perception performance consistently across multiple datasets and\ntasks such as 3D object detection and 3D scene segmentation.\n", "link": "http://arxiv.org/abs/2401.08687v2", "date": "2024-08-13", "relevancy": 2.1989, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DA-BEV%3A%20Unsupervised%20Domain%20Adaptation%20for%20Bird%27s%20Eye%20View%20Perception&body=Title%3A%20DA-BEV%3A%20Unsupervised%20Domain%20Adaptation%20for%20Bird%27s%20Eye%20View%20Perception%0AAuthor%3A%20Kai%20Jiang%20and%20Jiaxing%20Huang%20and%20Weiying%20Xie%20and%20Yunsong%20Li%20and%20Ling%20Shao%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20Camera-only%20Bird%27s%20Eye%20View%20%28BEV%29%20has%20demonstrated%20great%20potential%20in%0Aenvironment%20perception%20in%20a%203D%20space.%20However%2C%20most%20existing%20studies%20were%0Aconducted%20under%20a%20supervised%20setup%20which%20cannot%20scale%20well%20while%20handling%0Avarious%20new%20data.%20Unsupervised%20domain%20adaptive%20BEV%2C%20which%20effective%20learning%0Afrom%20various%20unlabelled%20target%20data%2C%20is%20far%20under-explored.%20In%20this%20work%2C%20we%0Adesign%20DA-BEV%2C%20the%20first%20domain%20adaptive%20camera-only%20BEV%20framework%20that%0Aaddresses%20domain%20adaptive%20BEV%20challenges%20by%20exploiting%20the%20complementary%20nature%0Aof%20image-view%20features%20and%20BEV%20features.%20DA-BEV%20introduces%20the%20idea%20of%20query%0Ainto%20the%20domain%20adaptation%20framework%20to%20derive%20useful%20information%20from%0Aimage-view%20and%20BEV%20features.%20It%20consists%20of%20two%20query-based%20designs%2C%20namely%2C%0Aquery-based%20adversarial%20learning%20%28QAL%29%20and%20query-based%20self-training%20%28QST%29%2C%0Awhich%20exploits%20image-view%20features%20or%20BEV%20features%20to%20regularize%20the%20adaptation%0Aof%20the%20other.%20Extensive%20experiments%20show%20that%20DA-BEV%20achieves%20superior%20domain%0Aadaptive%20BEV%20perception%20performance%20consistently%20across%20multiple%20datasets%20and%0Atasks%20such%20as%203D%20object%20detection%20and%203D%20scene%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDA-BEV%253A%2520Unsupervised%2520Domain%2520Adaptation%2520for%2520Bird%2527s%2520Eye%2520View%2520Perception%26entry.906535625%3DKai%2520Jiang%2520and%2520Jiaxing%2520Huang%2520and%2520Weiying%2520Xie%2520and%2520Yunsong%2520Li%2520and%2520Ling%2520Shao%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520Camera-only%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520has%2520demonstrated%2520great%2520potential%2520in%250Aenvironment%2520perception%2520in%2520a%25203D%2520space.%2520However%252C%2520most%2520existing%2520studies%2520were%250Aconducted%2520under%2520a%2520supervised%2520setup%2520which%2520cannot%2520scale%2520well%2520while%2520handling%250Avarious%2520new%2520data.%2520Unsupervised%2520domain%2520adaptive%2520BEV%252C%2520which%2520effective%2520learning%250Afrom%2520various%2520unlabelled%2520target%2520data%252C%2520is%2520far%2520under-explored.%2520In%2520this%2520work%252C%2520we%250Adesign%2520DA-BEV%252C%2520the%2520first%2520domain%2520adaptive%2520camera-only%2520BEV%2520framework%2520that%250Aaddresses%2520domain%2520adaptive%2520BEV%2520challenges%2520by%2520exploiting%2520the%2520complementary%2520nature%250Aof%2520image-view%2520features%2520and%2520BEV%2520features.%2520DA-BEV%2520introduces%2520the%2520idea%2520of%2520query%250Ainto%2520the%2520domain%2520adaptation%2520framework%2520to%2520derive%2520useful%2520information%2520from%250Aimage-view%2520and%2520BEV%2520features.%2520It%2520consists%2520of%2520two%2520query-based%2520designs%252C%2520namely%252C%250Aquery-based%2520adversarial%2520learning%2520%2528QAL%2529%2520and%2520query-based%2520self-training%2520%2528QST%2529%252C%250Awhich%2520exploits%2520image-view%2520features%2520or%2520BEV%2520features%2520to%2520regularize%2520the%2520adaptation%250Aof%2520the%2520other.%2520Extensive%2520experiments%2520show%2520that%2520DA-BEV%2520achieves%2520superior%2520domain%250Aadaptive%2520BEV%2520perception%2520performance%2520consistently%2520across%2520multiple%2520datasets%2520and%250Atasks%2520such%2520as%25203D%2520object%2520detection%2520and%25203D%2520scene%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA-BEV%3A%20Unsupervised%20Domain%20Adaptation%20for%20Bird%27s%20Eye%20View%20Perception&entry.906535625=Kai%20Jiang%20and%20Jiaxing%20Huang%20and%20Weiying%20Xie%20and%20Yunsong%20Li%20and%20Ling%20Shao%20and%20Shijian%20Lu&entry.1292438233=%20%20Camera-only%20Bird%27s%20Eye%20View%20%28BEV%29%20has%20demonstrated%20great%20potential%20in%0Aenvironment%20perception%20in%20a%203D%20space.%20However%2C%20most%20existing%20studies%20were%0Aconducted%20under%20a%20supervised%20setup%20which%20cannot%20scale%20well%20while%20handling%0Avarious%20new%20data.%20Unsupervised%20domain%20adaptive%20BEV%2C%20which%20effective%20learning%0Afrom%20various%20unlabelled%20target%20data%2C%20is%20far%20under-explored.%20In%20this%20work%2C%20we%0Adesign%20DA-BEV%2C%20the%20first%20domain%20adaptive%20camera-only%20BEV%20framework%20that%0Aaddresses%20domain%20adaptive%20BEV%20challenges%20by%20exploiting%20the%20complementary%20nature%0Aof%20image-view%20features%20and%20BEV%20features.%20DA-BEV%20introduces%20the%20idea%20of%20query%0Ainto%20the%20domain%20adaptation%20framework%20to%20derive%20useful%20information%20from%0Aimage-view%20and%20BEV%20features.%20It%20consists%20of%20two%20query-based%20designs%2C%20namely%2C%0Aquery-based%20adversarial%20learning%20%28QAL%29%20and%20query-based%20self-training%20%28QST%29%2C%0Awhich%20exploits%20image-view%20features%20or%20BEV%20features%20to%20regularize%20the%20adaptation%0Aof%20the%20other.%20Extensive%20experiments%20show%20that%20DA-BEV%20achieves%20superior%20domain%0Aadaptive%20BEV%20perception%20performance%20consistently%20across%20multiple%20datasets%20and%0Atasks%20such%20as%203D%20object%20detection%20and%203D%20scene%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08687v2&entry.124074799=Read"},
{"title": "Enhancing Diabetic Retinopathy Diagnosis: A Lightweight CNN Architecture\n  for Efficient Exudate Detection in Retinal Fundus Images", "author": "Mujadded Al Rabbani Alif", "abstract": "  Retinal fundus imaging plays an essential role in diagnosing various stages\nof diabetic retinopathy, where exudates are critical markers of early disease\nonset. Prompt detection of these exudates is pivotal for enabling optometrists\nto arrest or significantly decelerate the disease progression. This paper\nintroduces a novel, lightweight convolutional neural network architecture\ntailored for automated exudate detection, designed to identify these markers\nefficiently and accurately. To address the challenge of limited training data,\nwe have incorporated domain-specific data augmentations to enhance the model's\ngeneralizability. Furthermore, we applied a suite of regularization techniques\nwithin our custom architecture to boost diagnostic accuracy while optimizing\ncomputational efficiency. Remarkably, this streamlined model contains only 4.73\nmillion parameters a reduction of nearly 60% compared to the standard ResNet-18\nmodel, which has 11.69 million parameters. Despite its reduced complexity, our\nmodel achieves an impressive F1 score of 90%, demonstrating its efficacy in the\nearly detection of diabetic retinopathy through fundus imaging.\n", "link": "http://arxiv.org/abs/2408.06784v1", "date": "2024-08-13", "relevancy": 2.1818, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.549}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5457}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Diabetic%20Retinopathy%20Diagnosis%3A%20A%20Lightweight%20CNN%20Architecture%0A%20%20for%20Efficient%20Exudate%20Detection%20in%20Retinal%20Fundus%20Images&body=Title%3A%20Enhancing%20Diabetic%20Retinopathy%20Diagnosis%3A%20A%20Lightweight%20CNN%20Architecture%0A%20%20for%20Efficient%20Exudate%20Detection%20in%20Retinal%20Fundus%20Images%0AAuthor%3A%20Mujadded%20Al%20Rabbani%20Alif%0AAbstract%3A%20%20%20Retinal%20fundus%20imaging%20plays%20an%20essential%20role%20in%20diagnosing%20various%20stages%0Aof%20diabetic%20retinopathy%2C%20where%20exudates%20are%20critical%20markers%20of%20early%20disease%0Aonset.%20Prompt%20detection%20of%20these%20exudates%20is%20pivotal%20for%20enabling%20optometrists%0Ato%20arrest%20or%20significantly%20decelerate%20the%20disease%20progression.%20This%20paper%0Aintroduces%20a%20novel%2C%20lightweight%20convolutional%20neural%20network%20architecture%0Atailored%20for%20automated%20exudate%20detection%2C%20designed%20to%20identify%20these%20markers%0Aefficiently%20and%20accurately.%20To%20address%20the%20challenge%20of%20limited%20training%20data%2C%0Awe%20have%20incorporated%20domain-specific%20data%20augmentations%20to%20enhance%20the%20model%27s%0Ageneralizability.%20Furthermore%2C%20we%20applied%20a%20suite%20of%20regularization%20techniques%0Awithin%20our%20custom%20architecture%20to%20boost%20diagnostic%20accuracy%20while%20optimizing%0Acomputational%20efficiency.%20Remarkably%2C%20this%20streamlined%20model%20contains%20only%204.73%0Amillion%20parameters%20a%20reduction%20of%20nearly%2060%25%20compared%20to%20the%20standard%20ResNet-18%0Amodel%2C%20which%20has%2011.69%20million%20parameters.%20Despite%20its%20reduced%20complexity%2C%20our%0Amodel%20achieves%20an%20impressive%20F1%20score%20of%2090%25%2C%20demonstrating%20its%20efficacy%20in%20the%0Aearly%20detection%20of%20diabetic%20retinopathy%20through%20fundus%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Diabetic%2520Retinopathy%2520Diagnosis%253A%2520A%2520Lightweight%2520CNN%2520Architecture%250A%2520%2520for%2520Efficient%2520Exudate%2520Detection%2520in%2520Retinal%2520Fundus%2520Images%26entry.906535625%3DMujadded%2520Al%2520Rabbani%2520Alif%26entry.1292438233%3D%2520%2520Retinal%2520fundus%2520imaging%2520plays%2520an%2520essential%2520role%2520in%2520diagnosing%2520various%2520stages%250Aof%2520diabetic%2520retinopathy%252C%2520where%2520exudates%2520are%2520critical%2520markers%2520of%2520early%2520disease%250Aonset.%2520Prompt%2520detection%2520of%2520these%2520exudates%2520is%2520pivotal%2520for%2520enabling%2520optometrists%250Ato%2520arrest%2520or%2520significantly%2520decelerate%2520the%2520disease%2520progression.%2520This%2520paper%250Aintroduces%2520a%2520novel%252C%2520lightweight%2520convolutional%2520neural%2520network%2520architecture%250Atailored%2520for%2520automated%2520exudate%2520detection%252C%2520designed%2520to%2520identify%2520these%2520markers%250Aefficiently%2520and%2520accurately.%2520To%2520address%2520the%2520challenge%2520of%2520limited%2520training%2520data%252C%250Awe%2520have%2520incorporated%2520domain-specific%2520data%2520augmentations%2520to%2520enhance%2520the%2520model%2527s%250Ageneralizability.%2520Furthermore%252C%2520we%2520applied%2520a%2520suite%2520of%2520regularization%2520techniques%250Awithin%2520our%2520custom%2520architecture%2520to%2520boost%2520diagnostic%2520accuracy%2520while%2520optimizing%250Acomputational%2520efficiency.%2520Remarkably%252C%2520this%2520streamlined%2520model%2520contains%2520only%25204.73%250Amillion%2520parameters%2520a%2520reduction%2520of%2520nearly%252060%2525%2520compared%2520to%2520the%2520standard%2520ResNet-18%250Amodel%252C%2520which%2520has%252011.69%2520million%2520parameters.%2520Despite%2520its%2520reduced%2520complexity%252C%2520our%250Amodel%2520achieves%2520an%2520impressive%2520F1%2520score%2520of%252090%2525%252C%2520demonstrating%2520its%2520efficacy%2520in%2520the%250Aearly%2520detection%2520of%2520diabetic%2520retinopathy%2520through%2520fundus%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Diabetic%20Retinopathy%20Diagnosis%3A%20A%20Lightweight%20CNN%20Architecture%0A%20%20for%20Efficient%20Exudate%20Detection%20in%20Retinal%20Fundus%20Images&entry.906535625=Mujadded%20Al%20Rabbani%20Alif&entry.1292438233=%20%20Retinal%20fundus%20imaging%20plays%20an%20essential%20role%20in%20diagnosing%20various%20stages%0Aof%20diabetic%20retinopathy%2C%20where%20exudates%20are%20critical%20markers%20of%20early%20disease%0Aonset.%20Prompt%20detection%20of%20these%20exudates%20is%20pivotal%20for%20enabling%20optometrists%0Ato%20arrest%20or%20significantly%20decelerate%20the%20disease%20progression.%20This%20paper%0Aintroduces%20a%20novel%2C%20lightweight%20convolutional%20neural%20network%20architecture%0Atailored%20for%20automated%20exudate%20detection%2C%20designed%20to%20identify%20these%20markers%0Aefficiently%20and%20accurately.%20To%20address%20the%20challenge%20of%20limited%20training%20data%2C%0Awe%20have%20incorporated%20domain-specific%20data%20augmentations%20to%20enhance%20the%20model%27s%0Ageneralizability.%20Furthermore%2C%20we%20applied%20a%20suite%20of%20regularization%20techniques%0Awithin%20our%20custom%20architecture%20to%20boost%20diagnostic%20accuracy%20while%20optimizing%0Acomputational%20efficiency.%20Remarkably%2C%20this%20streamlined%20model%20contains%20only%204.73%0Amillion%20parameters%20a%20reduction%20of%20nearly%2060%25%20compared%20to%20the%20standard%20ResNet-18%0Amodel%2C%20which%20has%2011.69%20million%20parameters.%20Despite%20its%20reduced%20complexity%2C%20our%0Amodel%20achieves%20an%20impressive%20F1%20score%20of%2090%25%2C%20demonstrating%20its%20efficacy%20in%20the%0Aearly%20detection%20of%20diabetic%20retinopathy%20through%20fundus%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06784v1&entry.124074799=Read"},
{"title": "ED$^4$: Explicit Data-level Debiasing for Deepfake Detection", "author": "Jikang Cheng and Ying Zhang and Qin Zou and Zhiyuan Yan and Chao Liang and Zhongyuan Wang and Chen Li", "abstract": "  Learning intrinsic bias from limited data has been considered the main reason\nfor the failure of deepfake detection with generalizability. Apart from the\ndiscovered content and specific-forgery bias, we reveal a novel spatial bias,\nwhere detectors inertly anticipate observing structural forgery clues appearing\nat the image center, also can lead to the poor generalization of existing\nmethods. We present ED$^4$, a simple and effective strategy, to address\naforementioned biases explicitly at the data level in a unified framework\nrather than implicit disentanglement via network design. In particular, we\ndevelop ClockMix to produce facial structure preserved mixtures with arbitrary\nsamples, which allows the detector to learn from an exponentially extended data\ndistribution with much more diverse identities, backgrounds, local manipulation\ntraces, and the co-occurrence of multiple forgery artifacts. We further propose\nthe Adversarial Spatial Consistency Module (AdvSCM) to prevent extracting\nfeatures with spatial bias, which adversarially generates spatial-inconsistent\nimages and constrains their extracted feature to be consistent. As a\nmodel-agnostic debiasing strategy, ED$^4$ is plug-and-play: it can be\nintegrated with various deepfake detectors to obtain significant benefits. We\nconduct extensive experiments to demonstrate its effectiveness and superiority\nover existing deepfake detection approaches.\n", "link": "http://arxiv.org/abs/2408.06779v1", "date": "2024-08-13", "relevancy": 2.1678, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5746}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5426}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ED%24%5E4%24%3A%20Explicit%20Data-level%20Debiasing%20for%20Deepfake%20Detection&body=Title%3A%20ED%24%5E4%24%3A%20Explicit%20Data-level%20Debiasing%20for%20Deepfake%20Detection%0AAuthor%3A%20Jikang%20Cheng%20and%20Ying%20Zhang%20and%20Qin%20Zou%20and%20Zhiyuan%20Yan%20and%20Chao%20Liang%20and%20Zhongyuan%20Wang%20and%20Chen%20Li%0AAbstract%3A%20%20%20Learning%20intrinsic%20bias%20from%20limited%20data%20has%20been%20considered%20the%20main%20reason%0Afor%20the%20failure%20of%20deepfake%20detection%20with%20generalizability.%20Apart%20from%20the%0Adiscovered%20content%20and%20specific-forgery%20bias%2C%20we%20reveal%20a%20novel%20spatial%20bias%2C%0Awhere%20detectors%20inertly%20anticipate%20observing%20structural%20forgery%20clues%20appearing%0Aat%20the%20image%20center%2C%20also%20can%20lead%20to%20the%20poor%20generalization%20of%20existing%0Amethods.%20We%20present%20ED%24%5E4%24%2C%20a%20simple%20and%20effective%20strategy%2C%20to%20address%0Aaforementioned%20biases%20explicitly%20at%20the%20data%20level%20in%20a%20unified%20framework%0Arather%20than%20implicit%20disentanglement%20via%20network%20design.%20In%20particular%2C%20we%0Adevelop%20ClockMix%20to%20produce%20facial%20structure%20preserved%20mixtures%20with%20arbitrary%0Asamples%2C%20which%20allows%20the%20detector%20to%20learn%20from%20an%20exponentially%20extended%20data%0Adistribution%20with%20much%20more%20diverse%20identities%2C%20backgrounds%2C%20local%20manipulation%0Atraces%2C%20and%20the%20co-occurrence%20of%20multiple%20forgery%20artifacts.%20We%20further%20propose%0Athe%20Adversarial%20Spatial%20Consistency%20Module%20%28AdvSCM%29%20to%20prevent%20extracting%0Afeatures%20with%20spatial%20bias%2C%20which%20adversarially%20generates%20spatial-inconsistent%0Aimages%20and%20constrains%20their%20extracted%20feature%20to%20be%20consistent.%20As%20a%0Amodel-agnostic%20debiasing%20strategy%2C%20ED%24%5E4%24%20is%20plug-and-play%3A%20it%20can%20be%0Aintegrated%20with%20various%20deepfake%20detectors%20to%20obtain%20significant%20benefits.%20We%0Aconduct%20extensive%20experiments%20to%20demonstrate%20its%20effectiveness%20and%20superiority%0Aover%20existing%20deepfake%20detection%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DED%2524%255E4%2524%253A%2520Explicit%2520Data-level%2520Debiasing%2520for%2520Deepfake%2520Detection%26entry.906535625%3DJikang%2520Cheng%2520and%2520Ying%2520Zhang%2520and%2520Qin%2520Zou%2520and%2520Zhiyuan%2520Yan%2520and%2520Chao%2520Liang%2520and%2520Zhongyuan%2520Wang%2520and%2520Chen%2520Li%26entry.1292438233%3D%2520%2520Learning%2520intrinsic%2520bias%2520from%2520limited%2520data%2520has%2520been%2520considered%2520the%2520main%2520reason%250Afor%2520the%2520failure%2520of%2520deepfake%2520detection%2520with%2520generalizability.%2520Apart%2520from%2520the%250Adiscovered%2520content%2520and%2520specific-forgery%2520bias%252C%2520we%2520reveal%2520a%2520novel%2520spatial%2520bias%252C%250Awhere%2520detectors%2520inertly%2520anticipate%2520observing%2520structural%2520forgery%2520clues%2520appearing%250Aat%2520the%2520image%2520center%252C%2520also%2520can%2520lead%2520to%2520the%2520poor%2520generalization%2520of%2520existing%250Amethods.%2520We%2520present%2520ED%2524%255E4%2524%252C%2520a%2520simple%2520and%2520effective%2520strategy%252C%2520to%2520address%250Aaforementioned%2520biases%2520explicitly%2520at%2520the%2520data%2520level%2520in%2520a%2520unified%2520framework%250Arather%2520than%2520implicit%2520disentanglement%2520via%2520network%2520design.%2520In%2520particular%252C%2520we%250Adevelop%2520ClockMix%2520to%2520produce%2520facial%2520structure%2520preserved%2520mixtures%2520with%2520arbitrary%250Asamples%252C%2520which%2520allows%2520the%2520detector%2520to%2520learn%2520from%2520an%2520exponentially%2520extended%2520data%250Adistribution%2520with%2520much%2520more%2520diverse%2520identities%252C%2520backgrounds%252C%2520local%2520manipulation%250Atraces%252C%2520and%2520the%2520co-occurrence%2520of%2520multiple%2520forgery%2520artifacts.%2520We%2520further%2520propose%250Athe%2520Adversarial%2520Spatial%2520Consistency%2520Module%2520%2528AdvSCM%2529%2520to%2520prevent%2520extracting%250Afeatures%2520with%2520spatial%2520bias%252C%2520which%2520adversarially%2520generates%2520spatial-inconsistent%250Aimages%2520and%2520constrains%2520their%2520extracted%2520feature%2520to%2520be%2520consistent.%2520As%2520a%250Amodel-agnostic%2520debiasing%2520strategy%252C%2520ED%2524%255E4%2524%2520is%2520plug-and-play%253A%2520it%2520can%2520be%250Aintegrated%2520with%2520various%2520deepfake%2520detectors%2520to%2520obtain%2520significant%2520benefits.%2520We%250Aconduct%2520extensive%2520experiments%2520to%2520demonstrate%2520its%2520effectiveness%2520and%2520superiority%250Aover%2520existing%2520deepfake%2520detection%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ED%24%5E4%24%3A%20Explicit%20Data-level%20Debiasing%20for%20Deepfake%20Detection&entry.906535625=Jikang%20Cheng%20and%20Ying%20Zhang%20and%20Qin%20Zou%20and%20Zhiyuan%20Yan%20and%20Chao%20Liang%20and%20Zhongyuan%20Wang%20and%20Chen%20Li&entry.1292438233=%20%20Learning%20intrinsic%20bias%20from%20limited%20data%20has%20been%20considered%20the%20main%20reason%0Afor%20the%20failure%20of%20deepfake%20detection%20with%20generalizability.%20Apart%20from%20the%0Adiscovered%20content%20and%20specific-forgery%20bias%2C%20we%20reveal%20a%20novel%20spatial%20bias%2C%0Awhere%20detectors%20inertly%20anticipate%20observing%20structural%20forgery%20clues%20appearing%0Aat%20the%20image%20center%2C%20also%20can%20lead%20to%20the%20poor%20generalization%20of%20existing%0Amethods.%20We%20present%20ED%24%5E4%24%2C%20a%20simple%20and%20effective%20strategy%2C%20to%20address%0Aaforementioned%20biases%20explicitly%20at%20the%20data%20level%20in%20a%20unified%20framework%0Arather%20than%20implicit%20disentanglement%20via%20network%20design.%20In%20particular%2C%20we%0Adevelop%20ClockMix%20to%20produce%20facial%20structure%20preserved%20mixtures%20with%20arbitrary%0Asamples%2C%20which%20allows%20the%20detector%20to%20learn%20from%20an%20exponentially%20extended%20data%0Adistribution%20with%20much%20more%20diverse%20identities%2C%20backgrounds%2C%20local%20manipulation%0Atraces%2C%20and%20the%20co-occurrence%20of%20multiple%20forgery%20artifacts.%20We%20further%20propose%0Athe%20Adversarial%20Spatial%20Consistency%20Module%20%28AdvSCM%29%20to%20prevent%20extracting%0Afeatures%20with%20spatial%20bias%2C%20which%20adversarially%20generates%20spatial-inconsistent%0Aimages%20and%20constrains%20their%20extracted%20feature%20to%20be%20consistent.%20As%20a%0Amodel-agnostic%20debiasing%20strategy%2C%20ED%24%5E4%24%20is%20plug-and-play%3A%20it%20can%20be%0Aintegrated%20with%20various%20deepfake%20detectors%20to%20obtain%20significant%20benefits.%20We%0Aconduct%20extensive%20experiments%20to%20demonstrate%20its%20effectiveness%20and%20superiority%0Aover%20existing%20deepfake%20detection%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06779v1&entry.124074799=Read"},
{"title": "DynaSeg: A Deep Dynamic Fusion Method for Unsupervised Image\n  Segmentation Incorporating Feature Similarity and Spatial Continuity", "author": "Boujemaa Guermazi and Naimul Khan", "abstract": "  Our work tackles the fundamental challenge of image segmentation in computer\nvision, which is crucial for diverse applications. While supervised methods\ndemonstrate proficiency, their reliance on extensive pixel-level annotations\nlimits scalability. We introduce DynaSeg, an innovative unsupervised image\nsegmentation approach that overcomes the challenge of balancing feature\nsimilarity and spatial continuity without relying on extensive hyperparameter\ntuning. Unlike traditional methods, DynaSeg employs a dynamic weighting scheme\nthat automates parameter tuning, adapts flexibly to image characteristics, and\nfacilitates easy integration with other segmentation networks. By incorporating\na Silhouette Score Phase, DynaSeg prevents undersegmentation failures where the\nnumber of predicted clusters might converge to one. DynaSeg uses CNN-based and\npre-trained ResNet feature extraction, making it computationally efficient and\nmore straightforward than other complex models. Experimental results showcase\nstate-of-the-art performance, achieving a 12.2% and 14.12% mIOU improvement\nover current unsupervised segmentation approaches on COCO-All and COCO-Stuff\ndatasets, respectively. We provide qualitative and quantitative results on five\nbenchmark datasets, demonstrating the efficacy of the proposed approach.Code is\navailable at https://github.com/RyersonMultimediaLab/DynaSeg\n", "link": "http://arxiv.org/abs/2405.05477v3", "date": "2024-08-13", "relevancy": 2.1543, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5449}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5364}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaSeg%3A%20A%20Deep%20Dynamic%20Fusion%20Method%20for%20Unsupervised%20Image%0A%20%20Segmentation%20Incorporating%20Feature%20Similarity%20and%20Spatial%20Continuity&body=Title%3A%20DynaSeg%3A%20A%20Deep%20Dynamic%20Fusion%20Method%20for%20Unsupervised%20Image%0A%20%20Segmentation%20Incorporating%20Feature%20Similarity%20and%20Spatial%20Continuity%0AAuthor%3A%20Boujemaa%20Guermazi%20and%20Naimul%20Khan%0AAbstract%3A%20%20%20Our%20work%20tackles%20the%20fundamental%20challenge%20of%20image%20segmentation%20in%20computer%0Avision%2C%20which%20is%20crucial%20for%20diverse%20applications.%20While%20supervised%20methods%0Ademonstrate%20proficiency%2C%20their%20reliance%20on%20extensive%20pixel-level%20annotations%0Alimits%20scalability.%20We%20introduce%20DynaSeg%2C%20an%20innovative%20unsupervised%20image%0Asegmentation%20approach%20that%20overcomes%20the%20challenge%20of%20balancing%20feature%0Asimilarity%20and%20spatial%20continuity%20without%20relying%20on%20extensive%20hyperparameter%0Atuning.%20Unlike%20traditional%20methods%2C%20DynaSeg%20employs%20a%20dynamic%20weighting%20scheme%0Athat%20automates%20parameter%20tuning%2C%20adapts%20flexibly%20to%20image%20characteristics%2C%20and%0Afacilitates%20easy%20integration%20with%20other%20segmentation%20networks.%20By%20incorporating%0Aa%20Silhouette%20Score%20Phase%2C%20DynaSeg%20prevents%20undersegmentation%20failures%20where%20the%0Anumber%20of%20predicted%20clusters%20might%20converge%20to%20one.%20DynaSeg%20uses%20CNN-based%20and%0Apre-trained%20ResNet%20feature%20extraction%2C%20making%20it%20computationally%20efficient%20and%0Amore%20straightforward%20than%20other%20complex%20models.%20Experimental%20results%20showcase%0Astate-of-the-art%20performance%2C%20achieving%20a%2012.2%25%20and%2014.12%25%20mIOU%20improvement%0Aover%20current%20unsupervised%20segmentation%20approaches%20on%20COCO-All%20and%20COCO-Stuff%0Adatasets%2C%20respectively.%20We%20provide%20qualitative%20and%20quantitative%20results%20on%20five%0Abenchmark%20datasets%2C%20demonstrating%20the%20efficacy%20of%20the%20proposed%20approach.Code%20is%0Aavailable%20at%20https%3A//github.com/RyersonMultimediaLab/DynaSeg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05477v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaSeg%253A%2520A%2520Deep%2520Dynamic%2520Fusion%2520Method%2520for%2520Unsupervised%2520Image%250A%2520%2520Segmentation%2520Incorporating%2520Feature%2520Similarity%2520and%2520Spatial%2520Continuity%26entry.906535625%3DBoujemaa%2520Guermazi%2520and%2520Naimul%2520Khan%26entry.1292438233%3D%2520%2520Our%2520work%2520tackles%2520the%2520fundamental%2520challenge%2520of%2520image%2520segmentation%2520in%2520computer%250Avision%252C%2520which%2520is%2520crucial%2520for%2520diverse%2520applications.%2520While%2520supervised%2520methods%250Ademonstrate%2520proficiency%252C%2520their%2520reliance%2520on%2520extensive%2520pixel-level%2520annotations%250Alimits%2520scalability.%2520We%2520introduce%2520DynaSeg%252C%2520an%2520innovative%2520unsupervised%2520image%250Asegmentation%2520approach%2520that%2520overcomes%2520the%2520challenge%2520of%2520balancing%2520feature%250Asimilarity%2520and%2520spatial%2520continuity%2520without%2520relying%2520on%2520extensive%2520hyperparameter%250Atuning.%2520Unlike%2520traditional%2520methods%252C%2520DynaSeg%2520employs%2520a%2520dynamic%2520weighting%2520scheme%250Athat%2520automates%2520parameter%2520tuning%252C%2520adapts%2520flexibly%2520to%2520image%2520characteristics%252C%2520and%250Afacilitates%2520easy%2520integration%2520with%2520other%2520segmentation%2520networks.%2520By%2520incorporating%250Aa%2520Silhouette%2520Score%2520Phase%252C%2520DynaSeg%2520prevents%2520undersegmentation%2520failures%2520where%2520the%250Anumber%2520of%2520predicted%2520clusters%2520might%2520converge%2520to%2520one.%2520DynaSeg%2520uses%2520CNN-based%2520and%250Apre-trained%2520ResNet%2520feature%2520extraction%252C%2520making%2520it%2520computationally%2520efficient%2520and%250Amore%2520straightforward%2520than%2520other%2520complex%2520models.%2520Experimental%2520results%2520showcase%250Astate-of-the-art%2520performance%252C%2520achieving%2520a%252012.2%2525%2520and%252014.12%2525%2520mIOU%2520improvement%250Aover%2520current%2520unsupervised%2520segmentation%2520approaches%2520on%2520COCO-All%2520and%2520COCO-Stuff%250Adatasets%252C%2520respectively.%2520We%2520provide%2520qualitative%2520and%2520quantitative%2520results%2520on%2520five%250Abenchmark%2520datasets%252C%2520demonstrating%2520the%2520efficacy%2520of%2520the%2520proposed%2520approach.Code%2520is%250Aavailable%2520at%2520https%253A//github.com/RyersonMultimediaLab/DynaSeg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05477v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaSeg%3A%20A%20Deep%20Dynamic%20Fusion%20Method%20for%20Unsupervised%20Image%0A%20%20Segmentation%20Incorporating%20Feature%20Similarity%20and%20Spatial%20Continuity&entry.906535625=Boujemaa%20Guermazi%20and%20Naimul%20Khan&entry.1292438233=%20%20Our%20work%20tackles%20the%20fundamental%20challenge%20of%20image%20segmentation%20in%20computer%0Avision%2C%20which%20is%20crucial%20for%20diverse%20applications.%20While%20supervised%20methods%0Ademonstrate%20proficiency%2C%20their%20reliance%20on%20extensive%20pixel-level%20annotations%0Alimits%20scalability.%20We%20introduce%20DynaSeg%2C%20an%20innovative%20unsupervised%20image%0Asegmentation%20approach%20that%20overcomes%20the%20challenge%20of%20balancing%20feature%0Asimilarity%20and%20spatial%20continuity%20without%20relying%20on%20extensive%20hyperparameter%0Atuning.%20Unlike%20traditional%20methods%2C%20DynaSeg%20employs%20a%20dynamic%20weighting%20scheme%0Athat%20automates%20parameter%20tuning%2C%20adapts%20flexibly%20to%20image%20characteristics%2C%20and%0Afacilitates%20easy%20integration%20with%20other%20segmentation%20networks.%20By%20incorporating%0Aa%20Silhouette%20Score%20Phase%2C%20DynaSeg%20prevents%20undersegmentation%20failures%20where%20the%0Anumber%20of%20predicted%20clusters%20might%20converge%20to%20one.%20DynaSeg%20uses%20CNN-based%20and%0Apre-trained%20ResNet%20feature%20extraction%2C%20making%20it%20computationally%20efficient%20and%0Amore%20straightforward%20than%20other%20complex%20models.%20Experimental%20results%20showcase%0Astate-of-the-art%20performance%2C%20achieving%20a%2012.2%25%20and%2014.12%25%20mIOU%20improvement%0Aover%20current%20unsupervised%20segmentation%20approaches%20on%20COCO-All%20and%20COCO-Stuff%0Adatasets%2C%20respectively.%20We%20provide%20qualitative%20and%20quantitative%20results%20on%20five%0Abenchmark%20datasets%2C%20demonstrating%20the%20efficacy%20of%20the%20proposed%20approach.Code%20is%0Aavailable%20at%20https%3A//github.com/RyersonMultimediaLab/DynaSeg%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05477v3&entry.124074799=Read"},
{"title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty", "author": "Yongjin Yang and Haneul Yoo and Hwaran Lee", "abstract": "  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n", "link": "http://arxiv.org/abs/2408.06816v1", "date": "2024-08-13", "relevancy": 2.1506, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6093}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5254}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAQA%3A%20Evaluating%20Uncertainty%20Quantification%20in%20LLMs%20Regarding%20Data%0A%20%20Uncertainty&body=Title%3A%20MAQA%3A%20Evaluating%20Uncertainty%20Quantification%20in%20LLMs%20Regarding%20Data%0A%20%20Uncertainty%0AAuthor%3A%20Yongjin%20Yang%20and%20Haneul%20Yoo%20and%20Hwaran%20Lee%0AAbstract%3A%20%20%20Although%20large%20language%20models%20%28LLMs%29%20are%20capable%20of%20performing%20various%0Atasks%2C%20they%20still%20suffer%20from%20producing%20plausible%20but%20incorrect%20responses.%20To%0Aimprove%20the%20reliability%20of%20LLMs%2C%20recent%20research%20has%20focused%20on%20uncertainty%0Aquantification%20to%20predict%20whether%20a%20response%20is%20correct%20or%20not.%20However%2C%20most%0Auncertainty%20quantification%20methods%20have%20been%20evaluated%20on%20questions%20requiring%20a%0Asingle%20clear%20answer%2C%20ignoring%20the%20existence%20of%20data%20uncertainty%20that%20arises%0Afrom%20irreducible%20randomness.%20Instead%2C%20these%20methods%20only%20consider%20model%0Auncertainty%2C%20which%20arises%20from%20a%20lack%20of%20knowledge.%20In%20this%20paper%2C%20we%0Ainvestigate%20previous%20uncertainty%20quantification%20methods%20under%20the%20presence%20of%0Adata%20uncertainty.%20Our%20contributions%20are%20two-fold%3A%201%29%20proposing%20a%20new%0AMulti-Answer%20Question%20Answering%20dataset%2C%20MAQA%2C%20consisting%20of%20world%20knowledge%2C%0Amathematical%20reasoning%2C%20and%20commonsense%20reasoning%20tasks%20to%20evaluate%20uncertainty%0Aquantification%20regarding%20data%20uncertainty%2C%20and%202%29%20assessing%205%20uncertainty%0Aquantification%20methods%20of%20diverse%20white-%20and%20black-box%20LLMs.%20Our%20findings%20show%0Athat%20entropy%20and%20consistency-based%20methods%20estimate%20the%20model%20uncertainty%20well%0Aeven%20under%20data%20uncertainty%2C%20while%20other%20methods%20for%20white-%20and%20black-box%20LLMs%0Astruggle%20depending%20on%20the%20tasks.%20Additionally%2C%20methods%20designed%20for%20white-box%0ALLMs%20suffer%20from%20overconfidence%20in%20reasoning%20tasks%20compared%20to%20simple%20knowledge%0Aqueries.%20We%20believe%20our%20observations%20will%20pave%20the%20way%20for%20future%20work%20on%0Auncertainty%20quantification%20in%20realistic%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAQA%253A%2520Evaluating%2520Uncertainty%2520Quantification%2520in%2520LLMs%2520Regarding%2520Data%250A%2520%2520Uncertainty%26entry.906535625%3DYongjin%2520Yang%2520and%2520Haneul%2520Yoo%2520and%2520Hwaran%2520Lee%26entry.1292438233%3D%2520%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520capable%2520of%2520performing%2520various%250Atasks%252C%2520they%2520still%2520suffer%2520from%2520producing%2520plausible%2520but%2520incorrect%2520responses.%2520To%250Aimprove%2520the%2520reliability%2520of%2520LLMs%252C%2520recent%2520research%2520has%2520focused%2520on%2520uncertainty%250Aquantification%2520to%2520predict%2520whether%2520a%2520response%2520is%2520correct%2520or%2520not.%2520However%252C%2520most%250Auncertainty%2520quantification%2520methods%2520have%2520been%2520evaluated%2520on%2520questions%2520requiring%2520a%250Asingle%2520clear%2520answer%252C%2520ignoring%2520the%2520existence%2520of%2520data%2520uncertainty%2520that%2520arises%250Afrom%2520irreducible%2520randomness.%2520Instead%252C%2520these%2520methods%2520only%2520consider%2520model%250Auncertainty%252C%2520which%2520arises%2520from%2520a%2520lack%2520of%2520knowledge.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520previous%2520uncertainty%2520quantification%2520methods%2520under%2520the%2520presence%2520of%250Adata%2520uncertainty.%2520Our%2520contributions%2520are%2520two-fold%253A%25201%2529%2520proposing%2520a%2520new%250AMulti-Answer%2520Question%2520Answering%2520dataset%252C%2520MAQA%252C%2520consisting%2520of%2520world%2520knowledge%252C%250Amathematical%2520reasoning%252C%2520and%2520commonsense%2520reasoning%2520tasks%2520to%2520evaluate%2520uncertainty%250Aquantification%2520regarding%2520data%2520uncertainty%252C%2520and%25202%2529%2520assessing%25205%2520uncertainty%250Aquantification%2520methods%2520of%2520diverse%2520white-%2520and%2520black-box%2520LLMs.%2520Our%2520findings%2520show%250Athat%2520entropy%2520and%2520consistency-based%2520methods%2520estimate%2520the%2520model%2520uncertainty%2520well%250Aeven%2520under%2520data%2520uncertainty%252C%2520while%2520other%2520methods%2520for%2520white-%2520and%2520black-box%2520LLMs%250Astruggle%2520depending%2520on%2520the%2520tasks.%2520Additionally%252C%2520methods%2520designed%2520for%2520white-box%250ALLMs%2520suffer%2520from%2520overconfidence%2520in%2520reasoning%2520tasks%2520compared%2520to%2520simple%2520knowledge%250Aqueries.%2520We%2520believe%2520our%2520observations%2520will%2520pave%2520the%2520way%2520for%2520future%2520work%2520on%250Auncertainty%2520quantification%2520in%2520realistic%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAQA%3A%20Evaluating%20Uncertainty%20Quantification%20in%20LLMs%20Regarding%20Data%0A%20%20Uncertainty&entry.906535625=Yongjin%20Yang%20and%20Haneul%20Yoo%20and%20Hwaran%20Lee&entry.1292438233=%20%20Although%20large%20language%20models%20%28LLMs%29%20are%20capable%20of%20performing%20various%0Atasks%2C%20they%20still%20suffer%20from%20producing%20plausible%20but%20incorrect%20responses.%20To%0Aimprove%20the%20reliability%20of%20LLMs%2C%20recent%20research%20has%20focused%20on%20uncertainty%0Aquantification%20to%20predict%20whether%20a%20response%20is%20correct%20or%20not.%20However%2C%20most%0Auncertainty%20quantification%20methods%20have%20been%20evaluated%20on%20questions%20requiring%20a%0Asingle%20clear%20answer%2C%20ignoring%20the%20existence%20of%20data%20uncertainty%20that%20arises%0Afrom%20irreducible%20randomness.%20Instead%2C%20these%20methods%20only%20consider%20model%0Auncertainty%2C%20which%20arises%20from%20a%20lack%20of%20knowledge.%20In%20this%20paper%2C%20we%0Ainvestigate%20previous%20uncertainty%20quantification%20methods%20under%20the%20presence%20of%0Adata%20uncertainty.%20Our%20contributions%20are%20two-fold%3A%201%29%20proposing%20a%20new%0AMulti-Answer%20Question%20Answering%20dataset%2C%20MAQA%2C%20consisting%20of%20world%20knowledge%2C%0Amathematical%20reasoning%2C%20and%20commonsense%20reasoning%20tasks%20to%20evaluate%20uncertainty%0Aquantification%20regarding%20data%20uncertainty%2C%20and%202%29%20assessing%205%20uncertainty%0Aquantification%20methods%20of%20diverse%20white-%20and%20black-box%20LLMs.%20Our%20findings%20show%0Athat%20entropy%20and%20consistency-based%20methods%20estimate%20the%20model%20uncertainty%20well%0Aeven%20under%20data%20uncertainty%2C%20while%20other%20methods%20for%20white-%20and%20black-box%20LLMs%0Astruggle%20depending%20on%20the%20tasks.%20Additionally%2C%20methods%20designed%20for%20white-box%0ALLMs%20suffer%20from%20overconfidence%20in%20reasoning%20tasks%20compared%20to%20simple%20knowledge%0Aqueries.%20We%20believe%20our%20observations%20will%20pave%20the%20way%20for%20future%20work%20on%0Auncertainty%20quantification%20in%20realistic%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06816v1&entry.124074799=Read"},
{"title": "Continual Driving Policy Optimization with Closed-Loop Individualized\n  Curricula", "author": "Haoyi Niu and Yizhou Xu and Xingjian Jiang and Jianming Hu", "abstract": "  The safety of autonomous vehicles (AV) has been a long-standing top concern,\nstemming from the absence of rare and safety-critical scenarios in the\nlong-tail naturalistic driving distribution. To tackle this challenge, a surge\nof research in scenario-based autonomous driving has emerged, with a focus on\ngenerating high-risk driving scenarios and applying them to conduct\nsafety-critical testing of AV models. However, limited work has been explored\non the reuse of these extensive scenarios to iteratively improve AV models.\nMoreover, it remains intractable and challenging to filter through gigantic\nscenario libraries collected from other AV models with distinct behaviors,\nattempting to extract transferable information for current AV improvement.\nTherefore, we develop a continual driving policy optimization framework\nfeaturing Closed-Loop Individualized Curricula (CLIC), which we factorize into\na set of standardized sub-modules for flexible implementation choices: AV\nEvaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a\ncollision prediction task, where it estimates the chance of AV failures in\nthese scenarios at each iteration. Subsequently, by re-sampling from historical\nscenarios based on these failure probabilities, CLIC tailors individualized\ncurricula for downstream training, aligning them with the evaluated capability\nof AV. Accordingly, CLIC not only maximizes the utilization of the vast\npre-collected scenario library for closed-loop driving policy optimization but\nalso facilitates AV improvement by individualizing its training with more\nchallenging cases out of those poorly organized scenarios. Experimental results\nclearly indicate that CLIC surpasses other curriculum-based training\nstrategies, showing substantial improvement in managing risky scenarios, while\nstill maintaining proficiency in handling simpler cases.\n", "link": "http://arxiv.org/abs/2309.14209v4", "date": "2024-08-13", "relevancy": 2.127, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Driving%20Policy%20Optimization%20with%20Closed-Loop%20Individualized%0A%20%20Curricula&body=Title%3A%20Continual%20Driving%20Policy%20Optimization%20with%20Closed-Loop%20Individualized%0A%20%20Curricula%0AAuthor%3A%20Haoyi%20Niu%20and%20Yizhou%20Xu%20and%20Xingjian%20Jiang%20and%20Jianming%20Hu%0AAbstract%3A%20%20%20The%20safety%20of%20autonomous%20vehicles%20%28AV%29%20has%20been%20a%20long-standing%20top%20concern%2C%0Astemming%20from%20the%20absence%20of%20rare%20and%20safety-critical%20scenarios%20in%20the%0Along-tail%20naturalistic%20driving%20distribution.%20To%20tackle%20this%20challenge%2C%20a%20surge%0Aof%20research%20in%20scenario-based%20autonomous%20driving%20has%20emerged%2C%20with%20a%20focus%20on%0Agenerating%20high-risk%20driving%20scenarios%20and%20applying%20them%20to%20conduct%0Asafety-critical%20testing%20of%20AV%20models.%20However%2C%20limited%20work%20has%20been%20explored%0Aon%20the%20reuse%20of%20these%20extensive%20scenarios%20to%20iteratively%20improve%20AV%20models.%0AMoreover%2C%20it%20remains%20intractable%20and%20challenging%20to%20filter%20through%20gigantic%0Ascenario%20libraries%20collected%20from%20other%20AV%20models%20with%20distinct%20behaviors%2C%0Aattempting%20to%20extract%20transferable%20information%20for%20current%20AV%20improvement.%0ATherefore%2C%20we%20develop%20a%20continual%20driving%20policy%20optimization%20framework%0Afeaturing%20Closed-Loop%20Individualized%20Curricula%20%28CLIC%29%2C%20which%20we%20factorize%20into%0Aa%20set%20of%20standardized%20sub-modules%20for%20flexible%20implementation%20choices%3A%20AV%0AEvaluation%2C%20Scenario%20Selection%2C%20and%20AV%20Training.%20CLIC%20frames%20AV%20Evaluation%20as%20a%0Acollision%20prediction%20task%2C%20where%20it%20estimates%20the%20chance%20of%20AV%20failures%20in%0Athese%20scenarios%20at%20each%20iteration.%20Subsequently%2C%20by%20re-sampling%20from%20historical%0Ascenarios%20based%20on%20these%20failure%20probabilities%2C%20CLIC%20tailors%20individualized%0Acurricula%20for%20downstream%20training%2C%20aligning%20them%20with%20the%20evaluated%20capability%0Aof%20AV.%20Accordingly%2C%20CLIC%20not%20only%20maximizes%20the%20utilization%20of%20the%20vast%0Apre-collected%20scenario%20library%20for%20closed-loop%20driving%20policy%20optimization%20but%0Aalso%20facilitates%20AV%20improvement%20by%20individualizing%20its%20training%20with%20more%0Achallenging%20cases%20out%20of%20those%20poorly%20organized%20scenarios.%20Experimental%20results%0Aclearly%20indicate%20that%20CLIC%20surpasses%20other%20curriculum-based%20training%0Astrategies%2C%20showing%20substantial%20improvement%20in%20managing%20risky%20scenarios%2C%20while%0Astill%20maintaining%20proficiency%20in%20handling%20simpler%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14209v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Driving%2520Policy%2520Optimization%2520with%2520Closed-Loop%2520Individualized%250A%2520%2520Curricula%26entry.906535625%3DHaoyi%2520Niu%2520and%2520Yizhou%2520Xu%2520and%2520Xingjian%2520Jiang%2520and%2520Jianming%2520Hu%26entry.1292438233%3D%2520%2520The%2520safety%2520of%2520autonomous%2520vehicles%2520%2528AV%2529%2520has%2520been%2520a%2520long-standing%2520top%2520concern%252C%250Astemming%2520from%2520the%2520absence%2520of%2520rare%2520and%2520safety-critical%2520scenarios%2520in%2520the%250Along-tail%2520naturalistic%2520driving%2520distribution.%2520To%2520tackle%2520this%2520challenge%252C%2520a%2520surge%250Aof%2520research%2520in%2520scenario-based%2520autonomous%2520driving%2520has%2520emerged%252C%2520with%2520a%2520focus%2520on%250Agenerating%2520high-risk%2520driving%2520scenarios%2520and%2520applying%2520them%2520to%2520conduct%250Asafety-critical%2520testing%2520of%2520AV%2520models.%2520However%252C%2520limited%2520work%2520has%2520been%2520explored%250Aon%2520the%2520reuse%2520of%2520these%2520extensive%2520scenarios%2520to%2520iteratively%2520improve%2520AV%2520models.%250AMoreover%252C%2520it%2520remains%2520intractable%2520and%2520challenging%2520to%2520filter%2520through%2520gigantic%250Ascenario%2520libraries%2520collected%2520from%2520other%2520AV%2520models%2520with%2520distinct%2520behaviors%252C%250Aattempting%2520to%2520extract%2520transferable%2520information%2520for%2520current%2520AV%2520improvement.%250ATherefore%252C%2520we%2520develop%2520a%2520continual%2520driving%2520policy%2520optimization%2520framework%250Afeaturing%2520Closed-Loop%2520Individualized%2520Curricula%2520%2528CLIC%2529%252C%2520which%2520we%2520factorize%2520into%250Aa%2520set%2520of%2520standardized%2520sub-modules%2520for%2520flexible%2520implementation%2520choices%253A%2520AV%250AEvaluation%252C%2520Scenario%2520Selection%252C%2520and%2520AV%2520Training.%2520CLIC%2520frames%2520AV%2520Evaluation%2520as%2520a%250Acollision%2520prediction%2520task%252C%2520where%2520it%2520estimates%2520the%2520chance%2520of%2520AV%2520failures%2520in%250Athese%2520scenarios%2520at%2520each%2520iteration.%2520Subsequently%252C%2520by%2520re-sampling%2520from%2520historical%250Ascenarios%2520based%2520on%2520these%2520failure%2520probabilities%252C%2520CLIC%2520tailors%2520individualized%250Acurricula%2520for%2520downstream%2520training%252C%2520aligning%2520them%2520with%2520the%2520evaluated%2520capability%250Aof%2520AV.%2520Accordingly%252C%2520CLIC%2520not%2520only%2520maximizes%2520the%2520utilization%2520of%2520the%2520vast%250Apre-collected%2520scenario%2520library%2520for%2520closed-loop%2520driving%2520policy%2520optimization%2520but%250Aalso%2520facilitates%2520AV%2520improvement%2520by%2520individualizing%2520its%2520training%2520with%2520more%250Achallenging%2520cases%2520out%2520of%2520those%2520poorly%2520organized%2520scenarios.%2520Experimental%2520results%250Aclearly%2520indicate%2520that%2520CLIC%2520surpasses%2520other%2520curriculum-based%2520training%250Astrategies%252C%2520showing%2520substantial%2520improvement%2520in%2520managing%2520risky%2520scenarios%252C%2520while%250Astill%2520maintaining%2520proficiency%2520in%2520handling%2520simpler%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14209v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Driving%20Policy%20Optimization%20with%20Closed-Loop%20Individualized%0A%20%20Curricula&entry.906535625=Haoyi%20Niu%20and%20Yizhou%20Xu%20and%20Xingjian%20Jiang%20and%20Jianming%20Hu&entry.1292438233=%20%20The%20safety%20of%20autonomous%20vehicles%20%28AV%29%20has%20been%20a%20long-standing%20top%20concern%2C%0Astemming%20from%20the%20absence%20of%20rare%20and%20safety-critical%20scenarios%20in%20the%0Along-tail%20naturalistic%20driving%20distribution.%20To%20tackle%20this%20challenge%2C%20a%20surge%0Aof%20research%20in%20scenario-based%20autonomous%20driving%20has%20emerged%2C%20with%20a%20focus%20on%0Agenerating%20high-risk%20driving%20scenarios%20and%20applying%20them%20to%20conduct%0Asafety-critical%20testing%20of%20AV%20models.%20However%2C%20limited%20work%20has%20been%20explored%0Aon%20the%20reuse%20of%20these%20extensive%20scenarios%20to%20iteratively%20improve%20AV%20models.%0AMoreover%2C%20it%20remains%20intractable%20and%20challenging%20to%20filter%20through%20gigantic%0Ascenario%20libraries%20collected%20from%20other%20AV%20models%20with%20distinct%20behaviors%2C%0Aattempting%20to%20extract%20transferable%20information%20for%20current%20AV%20improvement.%0ATherefore%2C%20we%20develop%20a%20continual%20driving%20policy%20optimization%20framework%0Afeaturing%20Closed-Loop%20Individualized%20Curricula%20%28CLIC%29%2C%20which%20we%20factorize%20into%0Aa%20set%20of%20standardized%20sub-modules%20for%20flexible%20implementation%20choices%3A%20AV%0AEvaluation%2C%20Scenario%20Selection%2C%20and%20AV%20Training.%20CLIC%20frames%20AV%20Evaluation%20as%20a%0Acollision%20prediction%20task%2C%20where%20it%20estimates%20the%20chance%20of%20AV%20failures%20in%0Athese%20scenarios%20at%20each%20iteration.%20Subsequently%2C%20by%20re-sampling%20from%20historical%0Ascenarios%20based%20on%20these%20failure%20probabilities%2C%20CLIC%20tailors%20individualized%0Acurricula%20for%20downstream%20training%2C%20aligning%20them%20with%20the%20evaluated%20capability%0Aof%20AV.%20Accordingly%2C%20CLIC%20not%20only%20maximizes%20the%20utilization%20of%20the%20vast%0Apre-collected%20scenario%20library%20for%20closed-loop%20driving%20policy%20optimization%20but%0Aalso%20facilitates%20AV%20improvement%20by%20individualizing%20its%20training%20with%20more%0Achallenging%20cases%20out%20of%20those%20poorly%20organized%20scenarios.%20Experimental%20results%0Aclearly%20indicate%20that%20CLIC%20surpasses%20other%20curriculum-based%20training%0Astrategies%2C%20showing%20substantial%20improvement%20in%20managing%20risky%20scenarios%2C%20while%0Astill%20maintaining%20proficiency%20in%20handling%20simpler%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14209v4&entry.124074799=Read"},
{"title": "Learn2Decompose: Learning Problem Decomposition for Efficient Task and\n  Motion Planning", "author": "Yan Zhang and Amirreza Razmjoo and Sylvain Calinon", "abstract": "  We focus on designing efficient Task and Motion Planning (TAMP) approach for\nlong-horizon manipulation tasks involving multi-step manipulation of multiple\nobjects. TAMP solvers typically require exponentially longer planning time as\nthe planning horizon and the number of environmental objects increase. To\naddress this challenge, we first propose Learn2Decompose, a Learning from\nDemonstrations (LfD) approach that learns embedding task rules from\ndemonstrations and decomposes the long-horizon problem into several\nsubproblems. These subproblems require planning over shorter horizons with\nfewer objects and can be solved in parallel. We then design a parallelized\nhierarchical TAMP framework that concurrently solves the subproblems and\nconcatenates the resulting subplans for the target task, significantly\nimproving the planning efficiency of classical TAMP solvers. The effectiveness\nof our proposed methods is validated in both simulation and real-world\nexperiments.\n", "link": "http://arxiv.org/abs/2408.06843v1", "date": "2024-08-13", "relevancy": 2.1257, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5235}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn2Decompose%3A%20Learning%20Problem%20Decomposition%20for%20Efficient%20Task%20and%0A%20%20Motion%20Planning&body=Title%3A%20Learn2Decompose%3A%20Learning%20Problem%20Decomposition%20for%20Efficient%20Task%20and%0A%20%20Motion%20Planning%0AAuthor%3A%20Yan%20Zhang%20and%20Amirreza%20Razmjoo%20and%20Sylvain%20Calinon%0AAbstract%3A%20%20%20We%20focus%20on%20designing%20efficient%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approach%20for%0Along-horizon%20manipulation%20tasks%20involving%20multi-step%20manipulation%20of%20multiple%0Aobjects.%20TAMP%20solvers%20typically%20require%20exponentially%20longer%20planning%20time%20as%0Athe%20planning%20horizon%20and%20the%20number%20of%20environmental%20objects%20increase.%20To%0Aaddress%20this%20challenge%2C%20we%20first%20propose%20Learn2Decompose%2C%20a%20Learning%20from%0ADemonstrations%20%28LfD%29%20approach%20that%20learns%20embedding%20task%20rules%20from%0Ademonstrations%20and%20decomposes%20the%20long-horizon%20problem%20into%20several%0Asubproblems.%20These%20subproblems%20require%20planning%20over%20shorter%20horizons%20with%0Afewer%20objects%20and%20can%20be%20solved%20in%20parallel.%20We%20then%20design%20a%20parallelized%0Ahierarchical%20TAMP%20framework%20that%20concurrently%20solves%20the%20subproblems%20and%0Aconcatenates%20the%20resulting%20subplans%20for%20the%20target%20task%2C%20significantly%0Aimproving%20the%20planning%20efficiency%20of%20classical%20TAMP%20solvers.%20The%20effectiveness%0Aof%20our%20proposed%20methods%20is%20validated%20in%20both%20simulation%20and%20real-world%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn2Decompose%253A%2520Learning%2520Problem%2520Decomposition%2520for%2520Efficient%2520Task%2520and%250A%2520%2520Motion%2520Planning%26entry.906535625%3DYan%2520Zhang%2520and%2520Amirreza%2520Razmjoo%2520and%2520Sylvain%2520Calinon%26entry.1292438233%3D%2520%2520We%2520focus%2520on%2520designing%2520efficient%2520Task%2520and%2520Motion%2520Planning%2520%2528TAMP%2529%2520approach%2520for%250Along-horizon%2520manipulation%2520tasks%2520involving%2520multi-step%2520manipulation%2520of%2520multiple%250Aobjects.%2520TAMP%2520solvers%2520typically%2520require%2520exponentially%2520longer%2520planning%2520time%2520as%250Athe%2520planning%2520horizon%2520and%2520the%2520number%2520of%2520environmental%2520objects%2520increase.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520first%2520propose%2520Learn2Decompose%252C%2520a%2520Learning%2520from%250ADemonstrations%2520%2528LfD%2529%2520approach%2520that%2520learns%2520embedding%2520task%2520rules%2520from%250Ademonstrations%2520and%2520decomposes%2520the%2520long-horizon%2520problem%2520into%2520several%250Asubproblems.%2520These%2520subproblems%2520require%2520planning%2520over%2520shorter%2520horizons%2520with%250Afewer%2520objects%2520and%2520can%2520be%2520solved%2520in%2520parallel.%2520We%2520then%2520design%2520a%2520parallelized%250Ahierarchical%2520TAMP%2520framework%2520that%2520concurrently%2520solves%2520the%2520subproblems%2520and%250Aconcatenates%2520the%2520resulting%2520subplans%2520for%2520the%2520target%2520task%252C%2520significantly%250Aimproving%2520the%2520planning%2520efficiency%2520of%2520classical%2520TAMP%2520solvers.%2520The%2520effectiveness%250Aof%2520our%2520proposed%2520methods%2520is%2520validated%2520in%2520both%2520simulation%2520and%2520real-world%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn2Decompose%3A%20Learning%20Problem%20Decomposition%20for%20Efficient%20Task%20and%0A%20%20Motion%20Planning&entry.906535625=Yan%20Zhang%20and%20Amirreza%20Razmjoo%20and%20Sylvain%20Calinon&entry.1292438233=%20%20We%20focus%20on%20designing%20efficient%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approach%20for%0Along-horizon%20manipulation%20tasks%20involving%20multi-step%20manipulation%20of%20multiple%0Aobjects.%20TAMP%20solvers%20typically%20require%20exponentially%20longer%20planning%20time%20as%0Athe%20planning%20horizon%20and%20the%20number%20of%20environmental%20objects%20increase.%20To%0Aaddress%20this%20challenge%2C%20we%20first%20propose%20Learn2Decompose%2C%20a%20Learning%20from%0ADemonstrations%20%28LfD%29%20approach%20that%20learns%20embedding%20task%20rules%20from%0Ademonstrations%20and%20decomposes%20the%20long-horizon%20problem%20into%20several%0Asubproblems.%20These%20subproblems%20require%20planning%20over%20shorter%20horizons%20with%0Afewer%20objects%20and%20can%20be%20solved%20in%20parallel.%20We%20then%20design%20a%20parallelized%0Ahierarchical%20TAMP%20framework%20that%20concurrently%20solves%20the%20subproblems%20and%0Aconcatenates%20the%20resulting%20subplans%20for%20the%20target%20task%2C%20significantly%0Aimproving%20the%20planning%20efficiency%20of%20classical%20TAMP%20solvers.%20The%20effectiveness%0Aof%20our%20proposed%20methods%20is%20validated%20in%20both%20simulation%20and%20real-world%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06843v1&entry.124074799=Read"},
{"title": "Efficient Human-Object-Interaction (EHOI) Detection via Interaction\n  Label Coding and Conditional Decision", "author": "Tsung-Shan Yang and Yun-Cheng Wang and Chengwei Wei and Suya You and C. -C. Jay Kuo", "abstract": "  Human-Object Interaction (HOI) detection is a fundamental task in image\nunderstanding. While deep-learning-based HOI methods provide high performance\nin terms of mean Average Precision (mAP), they are computationally expensive\nand opaque in training and inference processes. An Efficient HOI (EHOI)\ndetector is proposed in this work to strike a good balance between detection\nperformance, inference complexity, and mathematical transparency. EHOI is a\ntwo-stage method. In the first stage, it leverages a frozen object detector to\nlocalize the objects and extract various features as intermediate outputs. In\nthe second stage, the first-stage outputs predict the interaction type using\nthe XGBoost classifier. Our contributions include the application of error\ncorrection codes (ECCs) to encode rare interaction cases, which reduces the\nmodel size and the complexity of the XGBoost classifier in the second stage.\nAdditionally, we provide a mathematical formulation of the relabeling and\ndecision-making process. Apart from the architecture, we present qualitative\nresults to explain the functionalities of the feedforward modules. Experimental\nresults demonstrate the advantages of ECC-coded interaction labels and the\nexcellent balance of detection performance and complexity of the proposed EHOI\nmethod.\n", "link": "http://arxiv.org/abs/2408.07018v1", "date": "2024-08-13", "relevancy": 2.1056, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5476}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5236}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Human-Object-Interaction%20%28EHOI%29%20Detection%20via%20Interaction%0A%20%20Label%20Coding%20and%20Conditional%20Decision&body=Title%3A%20Efficient%20Human-Object-Interaction%20%28EHOI%29%20Detection%20via%20Interaction%0A%20%20Label%20Coding%20and%20Conditional%20Decision%0AAuthor%3A%20Tsung-Shan%20Yang%20and%20Yun-Cheng%20Wang%20and%20Chengwei%20Wei%20and%20Suya%20You%20and%20C.%20-C.%20Jay%20Kuo%0AAbstract%3A%20%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20is%20a%20fundamental%20task%20in%20image%0Aunderstanding.%20While%20deep-learning-based%20HOI%20methods%20provide%20high%20performance%0Ain%20terms%20of%20mean%20Average%20Precision%20%28mAP%29%2C%20they%20are%20computationally%20expensive%0Aand%20opaque%20in%20training%20and%20inference%20processes.%20An%20Efficient%20HOI%20%28EHOI%29%0Adetector%20is%20proposed%20in%20this%20work%20to%20strike%20a%20good%20balance%20between%20detection%0Aperformance%2C%20inference%20complexity%2C%20and%20mathematical%20transparency.%20EHOI%20is%20a%0Atwo-stage%20method.%20In%20the%20first%20stage%2C%20it%20leverages%20a%20frozen%20object%20detector%20to%0Alocalize%20the%20objects%20and%20extract%20various%20features%20as%20intermediate%20outputs.%20In%0Athe%20second%20stage%2C%20the%20first-stage%20outputs%20predict%20the%20interaction%20type%20using%0Athe%20XGBoost%20classifier.%20Our%20contributions%20include%20the%20application%20of%20error%0Acorrection%20codes%20%28ECCs%29%20to%20encode%20rare%20interaction%20cases%2C%20which%20reduces%20the%0Amodel%20size%20and%20the%20complexity%20of%20the%20XGBoost%20classifier%20in%20the%20second%20stage.%0AAdditionally%2C%20we%20provide%20a%20mathematical%20formulation%20of%20the%20relabeling%20and%0Adecision-making%20process.%20Apart%20from%20the%20architecture%2C%20we%20present%20qualitative%0Aresults%20to%20explain%20the%20functionalities%20of%20the%20feedforward%20modules.%20Experimental%0Aresults%20demonstrate%20the%20advantages%20of%20ECC-coded%20interaction%20labels%20and%20the%0Aexcellent%20balance%20of%20detection%20performance%20and%20complexity%20of%20the%20proposed%20EHOI%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Human-Object-Interaction%2520%2528EHOI%2529%2520Detection%2520via%2520Interaction%250A%2520%2520Label%2520Coding%2520and%2520Conditional%2520Decision%26entry.906535625%3DTsung-Shan%2520Yang%2520and%2520Yun-Cheng%2520Wang%2520and%2520Chengwei%2520Wei%2520and%2520Suya%2520You%2520and%2520C.%2520-C.%2520Jay%2520Kuo%26entry.1292438233%3D%2520%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%2520is%2520a%2520fundamental%2520task%2520in%2520image%250Aunderstanding.%2520While%2520deep-learning-based%2520HOI%2520methods%2520provide%2520high%2520performance%250Ain%2520terms%2520of%2520mean%2520Average%2520Precision%2520%2528mAP%2529%252C%2520they%2520are%2520computationally%2520expensive%250Aand%2520opaque%2520in%2520training%2520and%2520inference%2520processes.%2520An%2520Efficient%2520HOI%2520%2528EHOI%2529%250Adetector%2520is%2520proposed%2520in%2520this%2520work%2520to%2520strike%2520a%2520good%2520balance%2520between%2520detection%250Aperformance%252C%2520inference%2520complexity%252C%2520and%2520mathematical%2520transparency.%2520EHOI%2520is%2520a%250Atwo-stage%2520method.%2520In%2520the%2520first%2520stage%252C%2520it%2520leverages%2520a%2520frozen%2520object%2520detector%2520to%250Alocalize%2520the%2520objects%2520and%2520extract%2520various%2520features%2520as%2520intermediate%2520outputs.%2520In%250Athe%2520second%2520stage%252C%2520the%2520first-stage%2520outputs%2520predict%2520the%2520interaction%2520type%2520using%250Athe%2520XGBoost%2520classifier.%2520Our%2520contributions%2520include%2520the%2520application%2520of%2520error%250Acorrection%2520codes%2520%2528ECCs%2529%2520to%2520encode%2520rare%2520interaction%2520cases%252C%2520which%2520reduces%2520the%250Amodel%2520size%2520and%2520the%2520complexity%2520of%2520the%2520XGBoost%2520classifier%2520in%2520the%2520second%2520stage.%250AAdditionally%252C%2520we%2520provide%2520a%2520mathematical%2520formulation%2520of%2520the%2520relabeling%2520and%250Adecision-making%2520process.%2520Apart%2520from%2520the%2520architecture%252C%2520we%2520present%2520qualitative%250Aresults%2520to%2520explain%2520the%2520functionalities%2520of%2520the%2520feedforward%2520modules.%2520Experimental%250Aresults%2520demonstrate%2520the%2520advantages%2520of%2520ECC-coded%2520interaction%2520labels%2520and%2520the%250Aexcellent%2520balance%2520of%2520detection%2520performance%2520and%2520complexity%2520of%2520the%2520proposed%2520EHOI%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Human-Object-Interaction%20%28EHOI%29%20Detection%20via%20Interaction%0A%20%20Label%20Coding%20and%20Conditional%20Decision&entry.906535625=Tsung-Shan%20Yang%20and%20Yun-Cheng%20Wang%20and%20Chengwei%20Wei%20and%20Suya%20You%20and%20C.%20-C.%20Jay%20Kuo&entry.1292438233=%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20is%20a%20fundamental%20task%20in%20image%0Aunderstanding.%20While%20deep-learning-based%20HOI%20methods%20provide%20high%20performance%0Ain%20terms%20of%20mean%20Average%20Precision%20%28mAP%29%2C%20they%20are%20computationally%20expensive%0Aand%20opaque%20in%20training%20and%20inference%20processes.%20An%20Efficient%20HOI%20%28EHOI%29%0Adetector%20is%20proposed%20in%20this%20work%20to%20strike%20a%20good%20balance%20between%20detection%0Aperformance%2C%20inference%20complexity%2C%20and%20mathematical%20transparency.%20EHOI%20is%20a%0Atwo-stage%20method.%20In%20the%20first%20stage%2C%20it%20leverages%20a%20frozen%20object%20detector%20to%0Alocalize%20the%20objects%20and%20extract%20various%20features%20as%20intermediate%20outputs.%20In%0Athe%20second%20stage%2C%20the%20first-stage%20outputs%20predict%20the%20interaction%20type%20using%0Athe%20XGBoost%20classifier.%20Our%20contributions%20include%20the%20application%20of%20error%0Acorrection%20codes%20%28ECCs%29%20to%20encode%20rare%20interaction%20cases%2C%20which%20reduces%20the%0Amodel%20size%20and%20the%20complexity%20of%20the%20XGBoost%20classifier%20in%20the%20second%20stage.%0AAdditionally%2C%20we%20provide%20a%20mathematical%20formulation%20of%20the%20relabeling%20and%0Adecision-making%20process.%20Apart%20from%20the%20architecture%2C%20we%20present%20qualitative%0Aresults%20to%20explain%20the%20functionalities%20of%20the%20feedforward%20modules.%20Experimental%0Aresults%20demonstrate%20the%20advantages%20of%20ECC-coded%20interaction%20labels%20and%20the%0Aexcellent%20balance%20of%20detection%20performance%20and%20complexity%20of%20the%20proposed%20EHOI%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07018v1&entry.124074799=Read"},
{"title": "Integrating Saliency Ranking and Reinforcement Learning for Enhanced\n  Object Detection", "author": "Matthias Bartolo and Dylan Seychell and Josef Bajada", "abstract": "  With the ever-growing variety of object detection approaches, this study\nexplores a series of experiments that combine reinforcement learning (RL)-based\nvisual attention methods with saliency ranking techniques to investigate\ntransparent and sustainable solutions. By integrating saliency ranking for\ninitial bounding box prediction and subsequently applying RL techniques to\nrefine these predictions through a finite set of actions over multiple time\nsteps, this study aims to enhance RL object detection accuracy. Presented as a\nseries of experiments, this research investigates the use of various image\nfeature extraction methods and explores diverse Deep Q-Network (DQN)\narchitectural variations for deep reinforcement learning-based localisation\nagent training. Additionally, we focus on optimising the detection pipeline at\nevery step by prioritising lightweight and faster models, while also\nincorporating the capability to classify detected objects, a feature absent in\nprevious RL approaches. We show that by evaluating the performance of these\ntrained agents using the Pascal VOC 2007 dataset, faster and more optimised\nmodels were developed. Notably, the best mean Average Precision (mAP) achieved\nin this study was 51.4, surpassing benchmarks set by RL-based single object\ndetectors in the literature.\n", "link": "http://arxiv.org/abs/2408.06803v1", "date": "2024-08-13", "relevancy": 2.1001, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5264}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Saliency%20Ranking%20and%20Reinforcement%20Learning%20for%20Enhanced%0A%20%20Object%20Detection&body=Title%3A%20Integrating%20Saliency%20Ranking%20and%20Reinforcement%20Learning%20for%20Enhanced%0A%20%20Object%20Detection%0AAuthor%3A%20Matthias%20Bartolo%20and%20Dylan%20Seychell%20and%20Josef%20Bajada%0AAbstract%3A%20%20%20With%20the%20ever-growing%20variety%20of%20object%20detection%20approaches%2C%20this%20study%0Aexplores%20a%20series%20of%20experiments%20that%20combine%20reinforcement%20learning%20%28RL%29-based%0Avisual%20attention%20methods%20with%20saliency%20ranking%20techniques%20to%20investigate%0Atransparent%20and%20sustainable%20solutions.%20By%20integrating%20saliency%20ranking%20for%0Ainitial%20bounding%20box%20prediction%20and%20subsequently%20applying%20RL%20techniques%20to%0Arefine%20these%20predictions%20through%20a%20finite%20set%20of%20actions%20over%20multiple%20time%0Asteps%2C%20this%20study%20aims%20to%20enhance%20RL%20object%20detection%20accuracy.%20Presented%20as%20a%0Aseries%20of%20experiments%2C%20this%20research%20investigates%20the%20use%20of%20various%20image%0Afeature%20extraction%20methods%20and%20explores%20diverse%20Deep%20Q-Network%20%28DQN%29%0Aarchitectural%20variations%20for%20deep%20reinforcement%20learning-based%20localisation%0Aagent%20training.%20Additionally%2C%20we%20focus%20on%20optimising%20the%20detection%20pipeline%20at%0Aevery%20step%20by%20prioritising%20lightweight%20and%20faster%20models%2C%20while%20also%0Aincorporating%20the%20capability%20to%20classify%20detected%20objects%2C%20a%20feature%20absent%20in%0Aprevious%20RL%20approaches.%20We%20show%20that%20by%20evaluating%20the%20performance%20of%20these%0Atrained%20agents%20using%20the%20Pascal%20VOC%202007%20dataset%2C%20faster%20and%20more%20optimised%0Amodels%20were%20developed.%20Notably%2C%20the%20best%20mean%20Average%20Precision%20%28mAP%29%20achieved%0Ain%20this%20study%20was%2051.4%2C%20surpassing%20benchmarks%20set%20by%20RL-based%20single%20object%0Adetectors%20in%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Saliency%2520Ranking%2520and%2520Reinforcement%2520Learning%2520for%2520Enhanced%250A%2520%2520Object%2520Detection%26entry.906535625%3DMatthias%2520Bartolo%2520and%2520Dylan%2520Seychell%2520and%2520Josef%2520Bajada%26entry.1292438233%3D%2520%2520With%2520the%2520ever-growing%2520variety%2520of%2520object%2520detection%2520approaches%252C%2520this%2520study%250Aexplores%2520a%2520series%2520of%2520experiments%2520that%2520combine%2520reinforcement%2520learning%2520%2528RL%2529-based%250Avisual%2520attention%2520methods%2520with%2520saliency%2520ranking%2520techniques%2520to%2520investigate%250Atransparent%2520and%2520sustainable%2520solutions.%2520By%2520integrating%2520saliency%2520ranking%2520for%250Ainitial%2520bounding%2520box%2520prediction%2520and%2520subsequently%2520applying%2520RL%2520techniques%2520to%250Arefine%2520these%2520predictions%2520through%2520a%2520finite%2520set%2520of%2520actions%2520over%2520multiple%2520time%250Asteps%252C%2520this%2520study%2520aims%2520to%2520enhance%2520RL%2520object%2520detection%2520accuracy.%2520Presented%2520as%2520a%250Aseries%2520of%2520experiments%252C%2520this%2520research%2520investigates%2520the%2520use%2520of%2520various%2520image%250Afeature%2520extraction%2520methods%2520and%2520explores%2520diverse%2520Deep%2520Q-Network%2520%2528DQN%2529%250Aarchitectural%2520variations%2520for%2520deep%2520reinforcement%2520learning-based%2520localisation%250Aagent%2520training.%2520Additionally%252C%2520we%2520focus%2520on%2520optimising%2520the%2520detection%2520pipeline%2520at%250Aevery%2520step%2520by%2520prioritising%2520lightweight%2520and%2520faster%2520models%252C%2520while%2520also%250Aincorporating%2520the%2520capability%2520to%2520classify%2520detected%2520objects%252C%2520a%2520feature%2520absent%2520in%250Aprevious%2520RL%2520approaches.%2520We%2520show%2520that%2520by%2520evaluating%2520the%2520performance%2520of%2520these%250Atrained%2520agents%2520using%2520the%2520Pascal%2520VOC%25202007%2520dataset%252C%2520faster%2520and%2520more%2520optimised%250Amodels%2520were%2520developed.%2520Notably%252C%2520the%2520best%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520achieved%250Ain%2520this%2520study%2520was%252051.4%252C%2520surpassing%2520benchmarks%2520set%2520by%2520RL-based%2520single%2520object%250Adetectors%2520in%2520the%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Saliency%20Ranking%20and%20Reinforcement%20Learning%20for%20Enhanced%0A%20%20Object%20Detection&entry.906535625=Matthias%20Bartolo%20and%20Dylan%20Seychell%20and%20Josef%20Bajada&entry.1292438233=%20%20With%20the%20ever-growing%20variety%20of%20object%20detection%20approaches%2C%20this%20study%0Aexplores%20a%20series%20of%20experiments%20that%20combine%20reinforcement%20learning%20%28RL%29-based%0Avisual%20attention%20methods%20with%20saliency%20ranking%20techniques%20to%20investigate%0Atransparent%20and%20sustainable%20solutions.%20By%20integrating%20saliency%20ranking%20for%0Ainitial%20bounding%20box%20prediction%20and%20subsequently%20applying%20RL%20techniques%20to%0Arefine%20these%20predictions%20through%20a%20finite%20set%20of%20actions%20over%20multiple%20time%0Asteps%2C%20this%20study%20aims%20to%20enhance%20RL%20object%20detection%20accuracy.%20Presented%20as%20a%0Aseries%20of%20experiments%2C%20this%20research%20investigates%20the%20use%20of%20various%20image%0Afeature%20extraction%20methods%20and%20explores%20diverse%20Deep%20Q-Network%20%28DQN%29%0Aarchitectural%20variations%20for%20deep%20reinforcement%20learning-based%20localisation%0Aagent%20training.%20Additionally%2C%20we%20focus%20on%20optimising%20the%20detection%20pipeline%20at%0Aevery%20step%20by%20prioritising%20lightweight%20and%20faster%20models%2C%20while%20also%0Aincorporating%20the%20capability%20to%20classify%20detected%20objects%2C%20a%20feature%20absent%20in%0Aprevious%20RL%20approaches.%20We%20show%20that%20by%20evaluating%20the%20performance%20of%20these%0Atrained%20agents%20using%20the%20Pascal%20VOC%202007%20dataset%2C%20faster%20and%20more%20optimised%0Amodels%20were%20developed.%20Notably%2C%20the%20best%20mean%20Average%20Precision%20%28mAP%29%20achieved%0Ain%20this%20study%20was%2051.4%2C%20surpassing%20benchmarks%20set%20by%20RL-based%20single%20object%0Adetectors%20in%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06803v1&entry.124074799=Read"},
{"title": "Let-It-Flow: Simultaneous Optimization of 3D Flow and Object Clustering", "author": "Patrik Vacek and David Hurych and Tom\u00e1\u0161 Svoboda and Karel Zimmermann", "abstract": "  We study the problem of self-supervised 3D scene flow estimation from real\nlarge-scale raw point cloud sequences, which is crucial to various tasks like\ntrajectory prediction or instance segmentation. In the absence of ground truth\nscene flow labels, contemporary approaches concentrate on deducing optimizing\nflow across sequential pairs of point clouds by incorporating structure based\nregularization on flow and object rigidity. The rigid objects are estimated by\na variety of 3D spatial clustering methods. While state-of-the-art methods\nsuccessfully capture overall scene motion using the Neural Prior structure,\nthey encounter challenges in discerning multi-object motions. We identified the\nstructural constraints and the use of large and strict rigid clusters as the\nmain pitfall of the current approaches and we propose a novel clustering\napproach that allows for combination of overlapping soft clusters as well as\nnon-overlapping rigid clusters representation. Flow is then jointly estimated\nwith progressively growing non-overlapping rigid clusters together with fixed\nsize overlapping soft clusters. We evaluate our method on multiple datasets\nwith LiDAR point clouds, demonstrating the superior performance over the\nself-supervised baselines reaching new state of the art results. Our method\nespecially excels in resolving flow in complicated dynamic scenes with multiple\nindependently moving objects close to each other which includes pedestrians,\ncyclists and other vulnerable road users. Our codes are publicly available on\nhttps://github.com/ctu-vras/let-it-flow.\n", "link": "http://arxiv.org/abs/2404.08363v3", "date": "2024-08-13", "relevancy": 2.1, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5439}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5216}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let-It-Flow%3A%20Simultaneous%20Optimization%20of%203D%20Flow%20and%20Object%20Clustering&body=Title%3A%20Let-It-Flow%3A%20Simultaneous%20Optimization%20of%203D%20Flow%20and%20Object%20Clustering%0AAuthor%3A%20Patrik%20Vacek%20and%20David%20Hurych%20and%20Tom%C3%A1%C5%A1%20Svoboda%20and%20Karel%20Zimmermann%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20self-supervised%203D%20scene%20flow%20estimation%20from%20real%0Alarge-scale%20raw%20point%20cloud%20sequences%2C%20which%20is%20crucial%20to%20various%20tasks%20like%0Atrajectory%20prediction%20or%20instance%20segmentation.%20In%20the%20absence%20of%20ground%20truth%0Ascene%20flow%20labels%2C%20contemporary%20approaches%20concentrate%20on%20deducing%20optimizing%0Aflow%20across%20sequential%20pairs%20of%20point%20clouds%20by%20incorporating%20structure%20based%0Aregularization%20on%20flow%20and%20object%20rigidity.%20The%20rigid%20objects%20are%20estimated%20by%0Aa%20variety%20of%203D%20spatial%20clustering%20methods.%20While%20state-of-the-art%20methods%0Asuccessfully%20capture%20overall%20scene%20motion%20using%20the%20Neural%20Prior%20structure%2C%0Athey%20encounter%20challenges%20in%20discerning%20multi-object%20motions.%20We%20identified%20the%0Astructural%20constraints%20and%20the%20use%20of%20large%20and%20strict%20rigid%20clusters%20as%20the%0Amain%20pitfall%20of%20the%20current%20approaches%20and%20we%20propose%20a%20novel%20clustering%0Aapproach%20that%20allows%20for%20combination%20of%20overlapping%20soft%20clusters%20as%20well%20as%0Anon-overlapping%20rigid%20clusters%20representation.%20Flow%20is%20then%20jointly%20estimated%0Awith%20progressively%20growing%20non-overlapping%20rigid%20clusters%20together%20with%20fixed%0Asize%20overlapping%20soft%20clusters.%20We%20evaluate%20our%20method%20on%20multiple%20datasets%0Awith%20LiDAR%20point%20clouds%2C%20demonstrating%20the%20superior%20performance%20over%20the%0Aself-supervised%20baselines%20reaching%20new%20state%20of%20the%20art%20results.%20Our%20method%0Aespecially%20excels%20in%20resolving%20flow%20in%20complicated%20dynamic%20scenes%20with%20multiple%0Aindependently%20moving%20objects%20close%20to%20each%20other%20which%20includes%20pedestrians%2C%0Acyclists%20and%20other%20vulnerable%20road%20users.%20Our%20codes%20are%20publicly%20available%20on%0Ahttps%3A//github.com/ctu-vras/let-it-flow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08363v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet-It-Flow%253A%2520Simultaneous%2520Optimization%2520of%25203D%2520Flow%2520and%2520Object%2520Clustering%26entry.906535625%3DPatrik%2520Vacek%2520and%2520David%2520Hurych%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Svoboda%2520and%2520Karel%2520Zimmermann%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520self-supervised%25203D%2520scene%2520flow%2520estimation%2520from%2520real%250Alarge-scale%2520raw%2520point%2520cloud%2520sequences%252C%2520which%2520is%2520crucial%2520to%2520various%2520tasks%2520like%250Atrajectory%2520prediction%2520or%2520instance%2520segmentation.%2520In%2520the%2520absence%2520of%2520ground%2520truth%250Ascene%2520flow%2520labels%252C%2520contemporary%2520approaches%2520concentrate%2520on%2520deducing%2520optimizing%250Aflow%2520across%2520sequential%2520pairs%2520of%2520point%2520clouds%2520by%2520incorporating%2520structure%2520based%250Aregularization%2520on%2520flow%2520and%2520object%2520rigidity.%2520The%2520rigid%2520objects%2520are%2520estimated%2520by%250Aa%2520variety%2520of%25203D%2520spatial%2520clustering%2520methods.%2520While%2520state-of-the-art%2520methods%250Asuccessfully%2520capture%2520overall%2520scene%2520motion%2520using%2520the%2520Neural%2520Prior%2520structure%252C%250Athey%2520encounter%2520challenges%2520in%2520discerning%2520multi-object%2520motions.%2520We%2520identified%2520the%250Astructural%2520constraints%2520and%2520the%2520use%2520of%2520large%2520and%2520strict%2520rigid%2520clusters%2520as%2520the%250Amain%2520pitfall%2520of%2520the%2520current%2520approaches%2520and%2520we%2520propose%2520a%2520novel%2520clustering%250Aapproach%2520that%2520allows%2520for%2520combination%2520of%2520overlapping%2520soft%2520clusters%2520as%2520well%2520as%250Anon-overlapping%2520rigid%2520clusters%2520representation.%2520Flow%2520is%2520then%2520jointly%2520estimated%250Awith%2520progressively%2520growing%2520non-overlapping%2520rigid%2520clusters%2520together%2520with%2520fixed%250Asize%2520overlapping%2520soft%2520clusters.%2520We%2520evaluate%2520our%2520method%2520on%2520multiple%2520datasets%250Awith%2520LiDAR%2520point%2520clouds%252C%2520demonstrating%2520the%2520superior%2520performance%2520over%2520the%250Aself-supervised%2520baselines%2520reaching%2520new%2520state%2520of%2520the%2520art%2520results.%2520Our%2520method%250Aespecially%2520excels%2520in%2520resolving%2520flow%2520in%2520complicated%2520dynamic%2520scenes%2520with%2520multiple%250Aindependently%2520moving%2520objects%2520close%2520to%2520each%2520other%2520which%2520includes%2520pedestrians%252C%250Acyclists%2520and%2520other%2520vulnerable%2520road%2520users.%2520Our%2520codes%2520are%2520publicly%2520available%2520on%250Ahttps%253A//github.com/ctu-vras/let-it-flow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08363v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let-It-Flow%3A%20Simultaneous%20Optimization%20of%203D%20Flow%20and%20Object%20Clustering&entry.906535625=Patrik%20Vacek%20and%20David%20Hurych%20and%20Tom%C3%A1%C5%A1%20Svoboda%20and%20Karel%20Zimmermann&entry.1292438233=%20%20We%20study%20the%20problem%20of%20self-supervised%203D%20scene%20flow%20estimation%20from%20real%0Alarge-scale%20raw%20point%20cloud%20sequences%2C%20which%20is%20crucial%20to%20various%20tasks%20like%0Atrajectory%20prediction%20or%20instance%20segmentation.%20In%20the%20absence%20of%20ground%20truth%0Ascene%20flow%20labels%2C%20contemporary%20approaches%20concentrate%20on%20deducing%20optimizing%0Aflow%20across%20sequential%20pairs%20of%20point%20clouds%20by%20incorporating%20structure%20based%0Aregularization%20on%20flow%20and%20object%20rigidity.%20The%20rigid%20objects%20are%20estimated%20by%0Aa%20variety%20of%203D%20spatial%20clustering%20methods.%20While%20state-of-the-art%20methods%0Asuccessfully%20capture%20overall%20scene%20motion%20using%20the%20Neural%20Prior%20structure%2C%0Athey%20encounter%20challenges%20in%20discerning%20multi-object%20motions.%20We%20identified%20the%0Astructural%20constraints%20and%20the%20use%20of%20large%20and%20strict%20rigid%20clusters%20as%20the%0Amain%20pitfall%20of%20the%20current%20approaches%20and%20we%20propose%20a%20novel%20clustering%0Aapproach%20that%20allows%20for%20combination%20of%20overlapping%20soft%20clusters%20as%20well%20as%0Anon-overlapping%20rigid%20clusters%20representation.%20Flow%20is%20then%20jointly%20estimated%0Awith%20progressively%20growing%20non-overlapping%20rigid%20clusters%20together%20with%20fixed%0Asize%20overlapping%20soft%20clusters.%20We%20evaluate%20our%20method%20on%20multiple%20datasets%0Awith%20LiDAR%20point%20clouds%2C%20demonstrating%20the%20superior%20performance%20over%20the%0Aself-supervised%20baselines%20reaching%20new%20state%20of%20the%20art%20results.%20Our%20method%0Aespecially%20excels%20in%20resolving%20flow%20in%20complicated%20dynamic%20scenes%20with%20multiple%0Aindependently%20moving%20objects%20close%20to%20each%20other%20which%20includes%20pedestrians%2C%0Acyclists%20and%20other%20vulnerable%20road%20users.%20Our%20codes%20are%20publicly%20available%20on%0Ahttps%3A//github.com/ctu-vras/let-it-flow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08363v3&entry.124074799=Read"},
{"title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models", "author": "Shi Lin and Rongchang Li and Xun Wang and Changting Lin and Wenpeng Xing and Meng Han", "abstract": "  The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful.\n", "link": "http://arxiv.org/abs/2407.16205v3", "date": "2024-08-13", "relevancy": 2.0854, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4216}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Figure%20it%20Out%3A%20Analyzing-based%20Jailbreak%20Attack%20on%20Large%20Language%20Models&body=Title%3A%20Figure%20it%20Out%3A%20Analyzing-based%20Jailbreak%20Attack%20on%20Large%20Language%20Models%0AAuthor%3A%20Shi%20Lin%20and%20Rongchang%20Li%20and%20Xun%20Wang%20and%20Changting%20Lin%20and%20Wenpeng%20Xing%20and%20Meng%20Han%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20brought%20remarkable%0Agenerative%20capabilities%20across%20diverse%20tasks.%20However%2C%20despite%20the%20impressive%0Aachievements%2C%20these%20LLMs%20still%20have%20numerous%20inherent%20vulnerabilities%2C%0Aparticularly%20when%20faced%20with%20jailbreak%20attacks.%20By%20investigating%20jailbreak%0Aattacks%2C%20we%20can%20uncover%20hidden%20weaknesses%20in%20LLMs%20and%20inform%20the%20development%20of%0Amore%20robust%20defense%20mechanisms%20to%20fortify%20their%20security.%20In%20this%20paper%2C%20we%0Afurther%20explore%20the%20boundary%20of%20jailbreak%20attacks%20on%20LLMs%20and%20propose%0AAnalyzing-based%20Jailbreak%20%28ABJ%29.%20This%20effective%20jailbreak%20attack%20method%20takes%0Aadvantage%20of%20LLMs%27%20growing%20analyzing%20and%20reasoning%20capability%20and%20reveals%20their%0Aunderlying%20vulnerabilities%20when%20facing%20analyzing-based%20tasks.%20We%20conduct%20a%0Adetailed%20evaluation%20of%20ABJ%20across%20various%20open-source%20and%20closed-source%20LLMs%2C%0Awhich%20achieves%2094.8%25%20attack%20success%20rate%20%28ASR%29%20and%201.06%20attack%20efficiency%20%28AE%29%0Aon%20GPT-4-turbo-0409%2C%20demonstrating%20state-of-the-art%20attack%20effectiveness%20and%0Aefficiency.%20Our%20research%20highlights%20the%20importance%20of%20prioritizing%20and%0Aenhancing%20the%20safety%20of%20LLMs%20to%20mitigate%20the%20risks%20of%20misuse.%20The%20code%20is%0Apublicly%20available%20at%20hhttps%3A//github.com/theshi-1128/ABJ-Attack.%20Warning%3A%20This%0Apaper%20contains%20examples%20of%20LLMs%20that%20might%20be%20offensive%20or%20harmful.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16205v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFigure%2520it%2520Out%253A%2520Analyzing-based%2520Jailbreak%2520Attack%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DShi%2520Lin%2520and%2520Rongchang%2520Li%2520and%2520Xun%2520Wang%2520and%2520Changting%2520Lin%2520and%2520Wenpeng%2520Xing%2520and%2520Meng%2520Han%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520brought%2520remarkable%250Agenerative%2520capabilities%2520across%2520diverse%2520tasks.%2520However%252C%2520despite%2520the%2520impressive%250Aachievements%252C%2520these%2520LLMs%2520still%2520have%2520numerous%2520inherent%2520vulnerabilities%252C%250Aparticularly%2520when%2520faced%2520with%2520jailbreak%2520attacks.%2520By%2520investigating%2520jailbreak%250Aattacks%252C%2520we%2520can%2520uncover%2520hidden%2520weaknesses%2520in%2520LLMs%2520and%2520inform%2520the%2520development%2520of%250Amore%2520robust%2520defense%2520mechanisms%2520to%2520fortify%2520their%2520security.%2520In%2520this%2520paper%252C%2520we%250Afurther%2520explore%2520the%2520boundary%2520of%2520jailbreak%2520attacks%2520on%2520LLMs%2520and%2520propose%250AAnalyzing-based%2520Jailbreak%2520%2528ABJ%2529.%2520This%2520effective%2520jailbreak%2520attack%2520method%2520takes%250Aadvantage%2520of%2520LLMs%2527%2520growing%2520analyzing%2520and%2520reasoning%2520capability%2520and%2520reveals%2520their%250Aunderlying%2520vulnerabilities%2520when%2520facing%2520analyzing-based%2520tasks.%2520We%2520conduct%2520a%250Adetailed%2520evaluation%2520of%2520ABJ%2520across%2520various%2520open-source%2520and%2520closed-source%2520LLMs%252C%250Awhich%2520achieves%252094.8%2525%2520attack%2520success%2520rate%2520%2528ASR%2529%2520and%25201.06%2520attack%2520efficiency%2520%2528AE%2529%250Aon%2520GPT-4-turbo-0409%252C%2520demonstrating%2520state-of-the-art%2520attack%2520effectiveness%2520and%250Aefficiency.%2520Our%2520research%2520highlights%2520the%2520importance%2520of%2520prioritizing%2520and%250Aenhancing%2520the%2520safety%2520of%2520LLMs%2520to%2520mitigate%2520the%2520risks%2520of%2520misuse.%2520The%2520code%2520is%250Apublicly%2520available%2520at%2520hhttps%253A//github.com/theshi-1128/ABJ-Attack.%2520Warning%253A%2520This%250Apaper%2520contains%2520examples%2520of%2520LLMs%2520that%2520might%2520be%2520offensive%2520or%2520harmful.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16205v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Figure%20it%20Out%3A%20Analyzing-based%20Jailbreak%20Attack%20on%20Large%20Language%20Models&entry.906535625=Shi%20Lin%20and%20Rongchang%20Li%20and%20Xun%20Wang%20and%20Changting%20Lin%20and%20Wenpeng%20Xing%20and%20Meng%20Han&entry.1292438233=%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20brought%20remarkable%0Agenerative%20capabilities%20across%20diverse%20tasks.%20However%2C%20despite%20the%20impressive%0Aachievements%2C%20these%20LLMs%20still%20have%20numerous%20inherent%20vulnerabilities%2C%0Aparticularly%20when%20faced%20with%20jailbreak%20attacks.%20By%20investigating%20jailbreak%0Aattacks%2C%20we%20can%20uncover%20hidden%20weaknesses%20in%20LLMs%20and%20inform%20the%20development%20of%0Amore%20robust%20defense%20mechanisms%20to%20fortify%20their%20security.%20In%20this%20paper%2C%20we%0Afurther%20explore%20the%20boundary%20of%20jailbreak%20attacks%20on%20LLMs%20and%20propose%0AAnalyzing-based%20Jailbreak%20%28ABJ%29.%20This%20effective%20jailbreak%20attack%20method%20takes%0Aadvantage%20of%20LLMs%27%20growing%20analyzing%20and%20reasoning%20capability%20and%20reveals%20their%0Aunderlying%20vulnerabilities%20when%20facing%20analyzing-based%20tasks.%20We%20conduct%20a%0Adetailed%20evaluation%20of%20ABJ%20across%20various%20open-source%20and%20closed-source%20LLMs%2C%0Awhich%20achieves%2094.8%25%20attack%20success%20rate%20%28ASR%29%20and%201.06%20attack%20efficiency%20%28AE%29%0Aon%20GPT-4-turbo-0409%2C%20demonstrating%20state-of-the-art%20attack%20effectiveness%20and%0Aefficiency.%20Our%20research%20highlights%20the%20importance%20of%20prioritizing%20and%0Aenhancing%20the%20safety%20of%20LLMs%20to%20mitigate%20the%20risks%20of%20misuse.%20The%20code%20is%0Apublicly%20available%20at%20hhttps%3A//github.com/theshi-1128/ABJ-Attack.%20Warning%3A%20This%0Apaper%20contains%20examples%20of%20LLMs%20that%20might%20be%20offensive%20or%20harmful.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16205v3&entry.124074799=Read"},
{"title": "Temporal Variability and Multi-Viewed Self-Supervised Representations to\n  Tackle the ASVspoof5 Deepfake Challenge", "author": "Yuankun Xie and Xiaopeng Wang and Zhiyong Wang and Ruibo Fu and Zhengqi Wen and Haonan Cheng and Long Ye", "abstract": "  ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest\nglobal audio security challenges. It aims to advance the development of\ncountermeasure (CM) to discriminate bonafide and spoofed speech utterances. In\nthis paper, we focus on addressing the problem of open-domain audio deepfake\ndetection, which corresponds directly to the ASVspoof5 Track1 open condition.\nAt first, we comprehensively investigate various CM on ASVspoof5, including\ndata expansion, data augmentation, and self-supervised learning (SSL) features.\nDue to the high-frequency gaps characteristic of the ASVspoof5 dataset, we\nintroduce Frequency Mask, a data augmentation method that masks specific\nfrequency bands to improve CM robustness. Combining various scale of temporal\ninformation with multiple SSL features, our experiments achieved a minDCF of\n0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.\n", "link": "http://arxiv.org/abs/2408.06922v1", "date": "2024-08-13", "relevancy": 2.0843, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5512}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5006}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Variability%20and%20Multi-Viewed%20Self-Supervised%20Representations%20to%0A%20%20Tackle%20the%20ASVspoof5%20Deepfake%20Challenge&body=Title%3A%20Temporal%20Variability%20and%20Multi-Viewed%20Self-Supervised%20Representations%20to%0A%20%20Tackle%20the%20ASVspoof5%20Deepfake%20Challenge%0AAuthor%3A%20Yuankun%20Xie%20and%20Xiaopeng%20Wang%20and%20Zhiyong%20Wang%20and%20Ruibo%20Fu%20and%20Zhengqi%20Wen%20and%20Haonan%20Cheng%20and%20Long%20Ye%0AAbstract%3A%20%20%20ASVspoof5%2C%20the%20fifth%20edition%20of%20the%20ASVspoof%20series%2C%20is%20one%20of%20the%20largest%0Aglobal%20audio%20security%20challenges.%20It%20aims%20to%20advance%20the%20development%20of%0Acountermeasure%20%28CM%29%20to%20discriminate%20bonafide%20and%20spoofed%20speech%20utterances.%20In%0Athis%20paper%2C%20we%20focus%20on%20addressing%20the%20problem%20of%20open-domain%20audio%20deepfake%0Adetection%2C%20which%20corresponds%20directly%20to%20the%20ASVspoof5%20Track1%20open%20condition.%0AAt%20first%2C%20we%20comprehensively%20investigate%20various%20CM%20on%20ASVspoof5%2C%20including%0Adata%20expansion%2C%20data%20augmentation%2C%20and%20self-supervised%20learning%20%28SSL%29%20features.%0ADue%20to%20the%20high-frequency%20gaps%20characteristic%20of%20the%20ASVspoof5%20dataset%2C%20we%0Aintroduce%20Frequency%20Mask%2C%20a%20data%20augmentation%20method%20that%20masks%20specific%0Afrequency%20bands%20to%20improve%20CM%20robustness.%20Combining%20various%20scale%20of%20temporal%0Ainformation%20with%20multiple%20SSL%20features%2C%20our%20experiments%20achieved%20a%20minDCF%20of%0A0.0158%20and%20an%20EER%20of%200.55%25%20on%20the%20ASVspoof%205%20Track%201%20evaluation%20progress%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Variability%2520and%2520Multi-Viewed%2520Self-Supervised%2520Representations%2520to%250A%2520%2520Tackle%2520the%2520ASVspoof5%2520Deepfake%2520Challenge%26entry.906535625%3DYuankun%2520Xie%2520and%2520Xiaopeng%2520Wang%2520and%2520Zhiyong%2520Wang%2520and%2520Ruibo%2520Fu%2520and%2520Zhengqi%2520Wen%2520and%2520Haonan%2520Cheng%2520and%2520Long%2520Ye%26entry.1292438233%3D%2520%2520ASVspoof5%252C%2520the%2520fifth%2520edition%2520of%2520the%2520ASVspoof%2520series%252C%2520is%2520one%2520of%2520the%2520largest%250Aglobal%2520audio%2520security%2520challenges.%2520It%2520aims%2520to%2520advance%2520the%2520development%2520of%250Acountermeasure%2520%2528CM%2529%2520to%2520discriminate%2520bonafide%2520and%2520spoofed%2520speech%2520utterances.%2520In%250Athis%2520paper%252C%2520we%2520focus%2520on%2520addressing%2520the%2520problem%2520of%2520open-domain%2520audio%2520deepfake%250Adetection%252C%2520which%2520corresponds%2520directly%2520to%2520the%2520ASVspoof5%2520Track1%2520open%2520condition.%250AAt%2520first%252C%2520we%2520comprehensively%2520investigate%2520various%2520CM%2520on%2520ASVspoof5%252C%2520including%250Adata%2520expansion%252C%2520data%2520augmentation%252C%2520and%2520self-supervised%2520learning%2520%2528SSL%2529%2520features.%250ADue%2520to%2520the%2520high-frequency%2520gaps%2520characteristic%2520of%2520the%2520ASVspoof5%2520dataset%252C%2520we%250Aintroduce%2520Frequency%2520Mask%252C%2520a%2520data%2520augmentation%2520method%2520that%2520masks%2520specific%250Afrequency%2520bands%2520to%2520improve%2520CM%2520robustness.%2520Combining%2520various%2520scale%2520of%2520temporal%250Ainformation%2520with%2520multiple%2520SSL%2520features%252C%2520our%2520experiments%2520achieved%2520a%2520minDCF%2520of%250A0.0158%2520and%2520an%2520EER%2520of%25200.55%2525%2520on%2520the%2520ASVspoof%25205%2520Track%25201%2520evaluation%2520progress%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Variability%20and%20Multi-Viewed%20Self-Supervised%20Representations%20to%0A%20%20Tackle%20the%20ASVspoof5%20Deepfake%20Challenge&entry.906535625=Yuankun%20Xie%20and%20Xiaopeng%20Wang%20and%20Zhiyong%20Wang%20and%20Ruibo%20Fu%20and%20Zhengqi%20Wen%20and%20Haonan%20Cheng%20and%20Long%20Ye&entry.1292438233=%20%20ASVspoof5%2C%20the%20fifth%20edition%20of%20the%20ASVspoof%20series%2C%20is%20one%20of%20the%20largest%0Aglobal%20audio%20security%20challenges.%20It%20aims%20to%20advance%20the%20development%20of%0Acountermeasure%20%28CM%29%20to%20discriminate%20bonafide%20and%20spoofed%20speech%20utterances.%20In%0Athis%20paper%2C%20we%20focus%20on%20addressing%20the%20problem%20of%20open-domain%20audio%20deepfake%0Adetection%2C%20which%20corresponds%20directly%20to%20the%20ASVspoof5%20Track1%20open%20condition.%0AAt%20first%2C%20we%20comprehensively%20investigate%20various%20CM%20on%20ASVspoof5%2C%20including%0Adata%20expansion%2C%20data%20augmentation%2C%20and%20self-supervised%20learning%20%28SSL%29%20features.%0ADue%20to%20the%20high-frequency%20gaps%20characteristic%20of%20the%20ASVspoof5%20dataset%2C%20we%0Aintroduce%20Frequency%20Mask%2C%20a%20data%20augmentation%20method%20that%20masks%20specific%0Afrequency%20bands%20to%20improve%20CM%20robustness.%20Combining%20various%20scale%20of%20temporal%0Ainformation%20with%20multiple%20SSL%20features%2C%20our%20experiments%20achieved%20a%20minDCF%20of%0A0.0158%20and%20an%20EER%20of%200.55%25%20on%20the%20ASVspoof%205%20Track%201%20evaluation%20progress%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06922v1&entry.124074799=Read"},
{"title": "VNet: A GAN-based Multi-Tier Discriminator Network for Speech Synthesis\n  Vocoders", "author": "Yubing Cao and Yongming Li and Liejun Wang and Yinfeng Yu", "abstract": "  Since the introduction of Generative Adversarial Networks (GANs) in speech\nsynthesis, remarkable achievements have been attained. In a thorough\nexploration of vocoders, it has been discovered that audio waveforms can be\ngenerated at speeds exceeding real-time while maintaining high fidelity,\nachieved through the utilization of GAN-based models. Typically, the inputs to\nthe vocoder consist of band-limited spectral information, which inevitably\nsacrifices high-frequency details. To address this, we adopt the full-band Mel\nspectrogram information as input, aiming to provide the vocoder with the most\ncomprehensive information possible. However, previous studies have revealed\nthat the use of full-band spectral information as input can result in the issue\nof over-smoothing, compromising the naturalness of the synthesized speech. To\ntackle this challenge, we propose VNet, a GAN-based neural vocoder network that\nincorporates full-band spectral information and introduces a Multi-Tier\nDiscriminator (MTD) comprising multiple sub-discriminators to generate\nhigh-resolution signals. Additionally, we introduce an asymptotically\nconstrained method that modifies the adversarial loss of the generator and\ndiscriminator, enhancing the stability of the training process. Through\nrigorous experiments, we demonstrate that the VNet model is capable of\ngenerating high-fidelity speech and significantly improving the performance of\nthe vocoder.\n", "link": "http://arxiv.org/abs/2408.06906v1", "date": "2024-08-13", "relevancy": 2.082, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5251}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5176}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VNet%3A%20A%20GAN-based%20Multi-Tier%20Discriminator%20Network%20for%20Speech%20Synthesis%0A%20%20Vocoders&body=Title%3A%20VNet%3A%20A%20GAN-based%20Multi-Tier%20Discriminator%20Network%20for%20Speech%20Synthesis%0A%20%20Vocoders%0AAuthor%3A%20Yubing%20Cao%20and%20Yongming%20Li%20and%20Liejun%20Wang%20and%20Yinfeng%20Yu%0AAbstract%3A%20%20%20Since%20the%20introduction%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20in%20speech%0Asynthesis%2C%20remarkable%20achievements%20have%20been%20attained.%20In%20a%20thorough%0Aexploration%20of%20vocoders%2C%20it%20has%20been%20discovered%20that%20audio%20waveforms%20can%20be%0Agenerated%20at%20speeds%20exceeding%20real-time%20while%20maintaining%20high%20fidelity%2C%0Aachieved%20through%20the%20utilization%20of%20GAN-based%20models.%20Typically%2C%20the%20inputs%20to%0Athe%20vocoder%20consist%20of%20band-limited%20spectral%20information%2C%20which%20inevitably%0Asacrifices%20high-frequency%20details.%20To%20address%20this%2C%20we%20adopt%20the%20full-band%20Mel%0Aspectrogram%20information%20as%20input%2C%20aiming%20to%20provide%20the%20vocoder%20with%20the%20most%0Acomprehensive%20information%20possible.%20However%2C%20previous%20studies%20have%20revealed%0Athat%20the%20use%20of%20full-band%20spectral%20information%20as%20input%20can%20result%20in%20the%20issue%0Aof%20over-smoothing%2C%20compromising%20the%20naturalness%20of%20the%20synthesized%20speech.%20To%0Atackle%20this%20challenge%2C%20we%20propose%20VNet%2C%20a%20GAN-based%20neural%20vocoder%20network%20that%0Aincorporates%20full-band%20spectral%20information%20and%20introduces%20a%20Multi-Tier%0ADiscriminator%20%28MTD%29%20comprising%20multiple%20sub-discriminators%20to%20generate%0Ahigh-resolution%20signals.%20Additionally%2C%20we%20introduce%20an%20asymptotically%0Aconstrained%20method%20that%20modifies%20the%20adversarial%20loss%20of%20the%20generator%20and%0Adiscriminator%2C%20enhancing%20the%20stability%20of%20the%20training%20process.%20Through%0Arigorous%20experiments%2C%20we%20demonstrate%20that%20the%20VNet%20model%20is%20capable%20of%0Agenerating%20high-fidelity%20speech%20and%20significantly%20improving%20the%20performance%20of%0Athe%20vocoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVNet%253A%2520A%2520GAN-based%2520Multi-Tier%2520Discriminator%2520Network%2520for%2520Speech%2520Synthesis%250A%2520%2520Vocoders%26entry.906535625%3DYubing%2520Cao%2520and%2520Yongming%2520Li%2520and%2520Liejun%2520Wang%2520and%2520Yinfeng%2520Yu%26entry.1292438233%3D%2520%2520Since%2520the%2520introduction%2520of%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520in%2520speech%250Asynthesis%252C%2520remarkable%2520achievements%2520have%2520been%2520attained.%2520In%2520a%2520thorough%250Aexploration%2520of%2520vocoders%252C%2520it%2520has%2520been%2520discovered%2520that%2520audio%2520waveforms%2520can%2520be%250Agenerated%2520at%2520speeds%2520exceeding%2520real-time%2520while%2520maintaining%2520high%2520fidelity%252C%250Aachieved%2520through%2520the%2520utilization%2520of%2520GAN-based%2520models.%2520Typically%252C%2520the%2520inputs%2520to%250Athe%2520vocoder%2520consist%2520of%2520band-limited%2520spectral%2520information%252C%2520which%2520inevitably%250Asacrifices%2520high-frequency%2520details.%2520To%2520address%2520this%252C%2520we%2520adopt%2520the%2520full-band%2520Mel%250Aspectrogram%2520information%2520as%2520input%252C%2520aiming%2520to%2520provide%2520the%2520vocoder%2520with%2520the%2520most%250Acomprehensive%2520information%2520possible.%2520However%252C%2520previous%2520studies%2520have%2520revealed%250Athat%2520the%2520use%2520of%2520full-band%2520spectral%2520information%2520as%2520input%2520can%2520result%2520in%2520the%2520issue%250Aof%2520over-smoothing%252C%2520compromising%2520the%2520naturalness%2520of%2520the%2520synthesized%2520speech.%2520To%250Atackle%2520this%2520challenge%252C%2520we%2520propose%2520VNet%252C%2520a%2520GAN-based%2520neural%2520vocoder%2520network%2520that%250Aincorporates%2520full-band%2520spectral%2520information%2520and%2520introduces%2520a%2520Multi-Tier%250ADiscriminator%2520%2528MTD%2529%2520comprising%2520multiple%2520sub-discriminators%2520to%2520generate%250Ahigh-resolution%2520signals.%2520Additionally%252C%2520we%2520introduce%2520an%2520asymptotically%250Aconstrained%2520method%2520that%2520modifies%2520the%2520adversarial%2520loss%2520of%2520the%2520generator%2520and%250Adiscriminator%252C%2520enhancing%2520the%2520stability%2520of%2520the%2520training%2520process.%2520Through%250Arigorous%2520experiments%252C%2520we%2520demonstrate%2520that%2520the%2520VNet%2520model%2520is%2520capable%2520of%250Agenerating%2520high-fidelity%2520speech%2520and%2520significantly%2520improving%2520the%2520performance%2520of%250Athe%2520vocoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VNet%3A%20A%20GAN-based%20Multi-Tier%20Discriminator%20Network%20for%20Speech%20Synthesis%0A%20%20Vocoders&entry.906535625=Yubing%20Cao%20and%20Yongming%20Li%20and%20Liejun%20Wang%20and%20Yinfeng%20Yu&entry.1292438233=%20%20Since%20the%20introduction%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20in%20speech%0Asynthesis%2C%20remarkable%20achievements%20have%20been%20attained.%20In%20a%20thorough%0Aexploration%20of%20vocoders%2C%20it%20has%20been%20discovered%20that%20audio%20waveforms%20can%20be%0Agenerated%20at%20speeds%20exceeding%20real-time%20while%20maintaining%20high%20fidelity%2C%0Aachieved%20through%20the%20utilization%20of%20GAN-based%20models.%20Typically%2C%20the%20inputs%20to%0Athe%20vocoder%20consist%20of%20band-limited%20spectral%20information%2C%20which%20inevitably%0Asacrifices%20high-frequency%20details.%20To%20address%20this%2C%20we%20adopt%20the%20full-band%20Mel%0Aspectrogram%20information%20as%20input%2C%20aiming%20to%20provide%20the%20vocoder%20with%20the%20most%0Acomprehensive%20information%20possible.%20However%2C%20previous%20studies%20have%20revealed%0Athat%20the%20use%20of%20full-band%20spectral%20information%20as%20input%20can%20result%20in%20the%20issue%0Aof%20over-smoothing%2C%20compromising%20the%20naturalness%20of%20the%20synthesized%20speech.%20To%0Atackle%20this%20challenge%2C%20we%20propose%20VNet%2C%20a%20GAN-based%20neural%20vocoder%20network%20that%0Aincorporates%20full-band%20spectral%20information%20and%20introduces%20a%20Multi-Tier%0ADiscriminator%20%28MTD%29%20comprising%20multiple%20sub-discriminators%20to%20generate%0Ahigh-resolution%20signals.%20Additionally%2C%20we%20introduce%20an%20asymptotically%0Aconstrained%20method%20that%20modifies%20the%20adversarial%20loss%20of%20the%20generator%20and%0Adiscriminator%2C%20enhancing%20the%20stability%20of%20the%20training%20process.%20Through%0Arigorous%20experiments%2C%20we%20demonstrate%20that%20the%20VNet%20model%20is%20capable%20of%0Agenerating%20high-fidelity%20speech%20and%20significantly%20improving%20the%20performance%20of%0Athe%20vocoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06906v1&entry.124074799=Read"},
{"title": "Interpretable Pre-Trained Transformers for Heart Time-Series Data", "author": "Harry J. Davies and James Monsen and Danilo P. Mandic", "abstract": "  Decoder-only transformers are the backbone of the popular generative\npre-trained transformer (GPT) series of large language models. In this work, we\nemploy this framework to the analysis of clinical heart time-series data, to\ncreate two pre-trained general purpose cardiac models, termed PPG-PT and\nECG-PT. We place a special emphasis on making both such pre-trained models\nfully interpretable. This is achieved firstly through aggregate attention maps\nwhich show that, in order to make predictions, the model focuses on similar\npoints in previous cardiac cycles and gradually broadens its attention in\ndeeper layers. Next, we show that tokens with the same value, which occur at\ndifferent distinct points in the electrocardiography (ECG) and\nphotoplethysmography (PPG) cycle, form separate clusters in high dimensional\nspace. The clusters form according to phase, as the tokens propagate through\nthe transformer blocks. Finally, we highlight that individual attention heads\nrespond to specific physiologically relevent features, such as the dicrotic\nnotch in PPG and the P-wave in ECG. It is also demonstrated that these\npre-trained models are straightforward to fine-tune for tasks such as\nclassification of atrial fibrillation (AF), and beat detection in\nphotoplethysmography. For the example of AF, the fine-tuning took 11 minutes of\ncomputer time, and achieved the respective leave-one-subject-out AUCs of 0.99\nand 0.93 for ECG and PPG within the MIMIC Perform AF dataset. In addition, the\nfine-tuned beat detector achieved a state-of-the-art F1 score of 98%, as well\nas uniquely providing a beat confidence level which acts as a signal quality\nestimator. Importantly, the fine-tuned models for AF screening are also fully\nexplainable, with attention shifting to regions in the context that are\nstrongly indicative of atrial fibrillation.\n", "link": "http://arxiv.org/abs/2407.20775v2", "date": "2024-08-13", "relevancy": 2.0594, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5649}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5183}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Pre-Trained%20Transformers%20for%20Heart%20Time-Series%20Data&body=Title%3A%20Interpretable%20Pre-Trained%20Transformers%20for%20Heart%20Time-Series%20Data%0AAuthor%3A%20Harry%20J.%20Davies%20and%20James%20Monsen%20and%20Danilo%20P.%20Mandic%0AAbstract%3A%20%20%20Decoder-only%20transformers%20are%20the%20backbone%20of%20the%20popular%20generative%0Apre-trained%20transformer%20%28GPT%29%20series%20of%20large%20language%20models.%20In%20this%20work%2C%20we%0Aemploy%20this%20framework%20to%20the%20analysis%20of%20clinical%20heart%20time-series%20data%2C%20to%0Acreate%20two%20pre-trained%20general%20purpose%20cardiac%20models%2C%20termed%20PPG-PT%20and%0AECG-PT.%20We%20place%20a%20special%20emphasis%20on%20making%20both%20such%20pre-trained%20models%0Afully%20interpretable.%20This%20is%20achieved%20firstly%20through%20aggregate%20attention%20maps%0Awhich%20show%20that%2C%20in%20order%20to%20make%20predictions%2C%20the%20model%20focuses%20on%20similar%0Apoints%20in%20previous%20cardiac%20cycles%20and%20gradually%20broadens%20its%20attention%20in%0Adeeper%20layers.%20Next%2C%20we%20show%20that%20tokens%20with%20the%20same%20value%2C%20which%20occur%20at%0Adifferent%20distinct%20points%20in%20the%20electrocardiography%20%28ECG%29%20and%0Aphotoplethysmography%20%28PPG%29%20cycle%2C%20form%20separate%20clusters%20in%20high%20dimensional%0Aspace.%20The%20clusters%20form%20according%20to%20phase%2C%20as%20the%20tokens%20propagate%20through%0Athe%20transformer%20blocks.%20Finally%2C%20we%20highlight%20that%20individual%20attention%20heads%0Arespond%20to%20specific%20physiologically%20relevent%20features%2C%20such%20as%20the%20dicrotic%0Anotch%20in%20PPG%20and%20the%20P-wave%20in%20ECG.%20It%20is%20also%20demonstrated%20that%20these%0Apre-trained%20models%20are%20straightforward%20to%20fine-tune%20for%20tasks%20such%20as%0Aclassification%20of%20atrial%20fibrillation%20%28AF%29%2C%20and%20beat%20detection%20in%0Aphotoplethysmography.%20For%20the%20example%20of%20AF%2C%20the%20fine-tuning%20took%2011%20minutes%20of%0Acomputer%20time%2C%20and%20achieved%20the%20respective%20leave-one-subject-out%20AUCs%20of%200.99%0Aand%200.93%20for%20ECG%20and%20PPG%20within%20the%20MIMIC%20Perform%20AF%20dataset.%20In%20addition%2C%20the%0Afine-tuned%20beat%20detector%20achieved%20a%20state-of-the-art%20F1%20score%20of%2098%25%2C%20as%20well%0Aas%20uniquely%20providing%20a%20beat%20confidence%20level%20which%20acts%20as%20a%20signal%20quality%0Aestimator.%20Importantly%2C%20the%20fine-tuned%20models%20for%20AF%20screening%20are%20also%20fully%0Aexplainable%2C%20with%20attention%20shifting%20to%20regions%20in%20the%20context%20that%20are%0Astrongly%20indicative%20of%20atrial%20fibrillation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Pre-Trained%2520Transformers%2520for%2520Heart%2520Time-Series%2520Data%26entry.906535625%3DHarry%2520J.%2520Davies%2520and%2520James%2520Monsen%2520and%2520Danilo%2520P.%2520Mandic%26entry.1292438233%3D%2520%2520Decoder-only%2520transformers%2520are%2520the%2520backbone%2520of%2520the%2520popular%2520generative%250Apre-trained%2520transformer%2520%2528GPT%2529%2520series%2520of%2520large%2520language%2520models.%2520In%2520this%2520work%252C%2520we%250Aemploy%2520this%2520framework%2520to%2520the%2520analysis%2520of%2520clinical%2520heart%2520time-series%2520data%252C%2520to%250Acreate%2520two%2520pre-trained%2520general%2520purpose%2520cardiac%2520models%252C%2520termed%2520PPG-PT%2520and%250AECG-PT.%2520We%2520place%2520a%2520special%2520emphasis%2520on%2520making%2520both%2520such%2520pre-trained%2520models%250Afully%2520interpretable.%2520This%2520is%2520achieved%2520firstly%2520through%2520aggregate%2520attention%2520maps%250Awhich%2520show%2520that%252C%2520in%2520order%2520to%2520make%2520predictions%252C%2520the%2520model%2520focuses%2520on%2520similar%250Apoints%2520in%2520previous%2520cardiac%2520cycles%2520and%2520gradually%2520broadens%2520its%2520attention%2520in%250Adeeper%2520layers.%2520Next%252C%2520we%2520show%2520that%2520tokens%2520with%2520the%2520same%2520value%252C%2520which%2520occur%2520at%250Adifferent%2520distinct%2520points%2520in%2520the%2520electrocardiography%2520%2528ECG%2529%2520and%250Aphotoplethysmography%2520%2528PPG%2529%2520cycle%252C%2520form%2520separate%2520clusters%2520in%2520high%2520dimensional%250Aspace.%2520The%2520clusters%2520form%2520according%2520to%2520phase%252C%2520as%2520the%2520tokens%2520propagate%2520through%250Athe%2520transformer%2520blocks.%2520Finally%252C%2520we%2520highlight%2520that%2520individual%2520attention%2520heads%250Arespond%2520to%2520specific%2520physiologically%2520relevent%2520features%252C%2520such%2520as%2520the%2520dicrotic%250Anotch%2520in%2520PPG%2520and%2520the%2520P-wave%2520in%2520ECG.%2520It%2520is%2520also%2520demonstrated%2520that%2520these%250Apre-trained%2520models%2520are%2520straightforward%2520to%2520fine-tune%2520for%2520tasks%2520such%2520as%250Aclassification%2520of%2520atrial%2520fibrillation%2520%2528AF%2529%252C%2520and%2520beat%2520detection%2520in%250Aphotoplethysmography.%2520For%2520the%2520example%2520of%2520AF%252C%2520the%2520fine-tuning%2520took%252011%2520minutes%2520of%250Acomputer%2520time%252C%2520and%2520achieved%2520the%2520respective%2520leave-one-subject-out%2520AUCs%2520of%25200.99%250Aand%25200.93%2520for%2520ECG%2520and%2520PPG%2520within%2520the%2520MIMIC%2520Perform%2520AF%2520dataset.%2520In%2520addition%252C%2520the%250Afine-tuned%2520beat%2520detector%2520achieved%2520a%2520state-of-the-art%2520F1%2520score%2520of%252098%2525%252C%2520as%2520well%250Aas%2520uniquely%2520providing%2520a%2520beat%2520confidence%2520level%2520which%2520acts%2520as%2520a%2520signal%2520quality%250Aestimator.%2520Importantly%252C%2520the%2520fine-tuned%2520models%2520for%2520AF%2520screening%2520are%2520also%2520fully%250Aexplainable%252C%2520with%2520attention%2520shifting%2520to%2520regions%2520in%2520the%2520context%2520that%2520are%250Astrongly%2520indicative%2520of%2520atrial%2520fibrillation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Pre-Trained%20Transformers%20for%20Heart%20Time-Series%20Data&entry.906535625=Harry%20J.%20Davies%20and%20James%20Monsen%20and%20Danilo%20P.%20Mandic&entry.1292438233=%20%20Decoder-only%20transformers%20are%20the%20backbone%20of%20the%20popular%20generative%0Apre-trained%20transformer%20%28GPT%29%20series%20of%20large%20language%20models.%20In%20this%20work%2C%20we%0Aemploy%20this%20framework%20to%20the%20analysis%20of%20clinical%20heart%20time-series%20data%2C%20to%0Acreate%20two%20pre-trained%20general%20purpose%20cardiac%20models%2C%20termed%20PPG-PT%20and%0AECG-PT.%20We%20place%20a%20special%20emphasis%20on%20making%20both%20such%20pre-trained%20models%0Afully%20interpretable.%20This%20is%20achieved%20firstly%20through%20aggregate%20attention%20maps%0Awhich%20show%20that%2C%20in%20order%20to%20make%20predictions%2C%20the%20model%20focuses%20on%20similar%0Apoints%20in%20previous%20cardiac%20cycles%20and%20gradually%20broadens%20its%20attention%20in%0Adeeper%20layers.%20Next%2C%20we%20show%20that%20tokens%20with%20the%20same%20value%2C%20which%20occur%20at%0Adifferent%20distinct%20points%20in%20the%20electrocardiography%20%28ECG%29%20and%0Aphotoplethysmography%20%28PPG%29%20cycle%2C%20form%20separate%20clusters%20in%20high%20dimensional%0Aspace.%20The%20clusters%20form%20according%20to%20phase%2C%20as%20the%20tokens%20propagate%20through%0Athe%20transformer%20blocks.%20Finally%2C%20we%20highlight%20that%20individual%20attention%20heads%0Arespond%20to%20specific%20physiologically%20relevent%20features%2C%20such%20as%20the%20dicrotic%0Anotch%20in%20PPG%20and%20the%20P-wave%20in%20ECG.%20It%20is%20also%20demonstrated%20that%20these%0Apre-trained%20models%20are%20straightforward%20to%20fine-tune%20for%20tasks%20such%20as%0Aclassification%20of%20atrial%20fibrillation%20%28AF%29%2C%20and%20beat%20detection%20in%0Aphotoplethysmography.%20For%20the%20example%20of%20AF%2C%20the%20fine-tuning%20took%2011%20minutes%20of%0Acomputer%20time%2C%20and%20achieved%20the%20respective%20leave-one-subject-out%20AUCs%20of%200.99%0Aand%200.93%20for%20ECG%20and%20PPG%20within%20the%20MIMIC%20Perform%20AF%20dataset.%20In%20addition%2C%20the%0Afine-tuned%20beat%20detector%20achieved%20a%20state-of-the-art%20F1%20score%20of%2098%25%2C%20as%20well%0Aas%20uniquely%20providing%20a%20beat%20confidence%20level%20which%20acts%20as%20a%20signal%20quality%0Aestimator.%20Importantly%2C%20the%20fine-tuned%20models%20for%20AF%20screening%20are%20also%20fully%0Aexplainable%2C%20with%20attention%20shifting%20to%20regions%20in%20the%20context%20that%20are%0Astrongly%20indicative%20of%20atrial%20fibrillation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20775v2&entry.124074799=Read"},
{"title": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2", "author": "Osher Rafaeli and Tal Svoray and Ariel Nahlieli", "abstract": "  This paper provides insight into the effectiveness of zero-shot,\nprompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, and\nthe non-promptable, conventional convolutional network (CNN), in segmenting\nsolar panels, in RGB aerial imagery, across lighting conditions, spatial\nresolutions, and prompt strategies. SAM 2 demonstrates improvements over SAM,\nparticularly in sub-optimal lighting conditions when prompted by points. Both\nSAMs, prompted by user-box, outperformed CNN, in all scenarios. Additionally,\nYOLOv9 prompting outperformed user points prompting. In high-resolution\nimagery, both in optimal and sub-optimal lighting conditions, Eff-UNet\noutperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet as\nthe appropriate model for automatic segmentation in high-resolution data. In\nlow-resolution data, user box prompts were found crucial to achieve a\nreasonable performance. This paper provides details on strengths and\nlimitations of each model and outlines robustness of user prompted image\nsegmentation models in inconsistent resolution and lighting conditions of\nremotely sensed data.\n", "link": "http://arxiv.org/abs/2408.06970v1", "date": "2024-08-13", "relevancy": 2.0447, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5436}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Based%20Segmentation%20at%20Multiple%20Resolutions%20and%20Lighting%0A%20%20Conditions%20using%20Segment%20Anything%20Model%202&body=Title%3A%20Prompt-Based%20Segmentation%20at%20Multiple%20Resolutions%20and%20Lighting%0A%20%20Conditions%20using%20Segment%20Anything%20Model%202%0AAuthor%3A%20Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Ariel%20Nahlieli%0AAbstract%3A%20%20%20This%20paper%20provides%20insight%20into%20the%20effectiveness%20of%20zero-shot%2C%0Aprompt-based%2C%20Segment%20Anything%20Model%20%28SAM%29%2C%20and%20its%20updated%20version%2C%20SAM%202%2C%20and%0Athe%20non-promptable%2C%20conventional%20convolutional%20network%20%28CNN%29%2C%20in%20segmenting%0Asolar%20panels%2C%20in%20RGB%20aerial%20imagery%2C%20across%20lighting%20conditions%2C%20spatial%0Aresolutions%2C%20and%20prompt%20strategies.%20SAM%202%20demonstrates%20improvements%20over%20SAM%2C%0Aparticularly%20in%20sub-optimal%20lighting%20conditions%20when%20prompted%20by%20points.%20Both%0ASAMs%2C%20prompted%20by%20user-box%2C%20outperformed%20CNN%2C%20in%20all%20scenarios.%20Additionally%2C%0AYOLOv9%20prompting%20outperformed%20user%20points%20prompting.%20In%20high-resolution%0Aimagery%2C%20both%20in%20optimal%20and%20sub-optimal%20lighting%20conditions%2C%20Eff-UNet%0Aoutperformed%20both%20SAM%20models%20prompted%20by%20YOLOv9%20boxes%2C%20positioning%20Eff-UNet%20as%0Athe%20appropriate%20model%20for%20automatic%20segmentation%20in%20high-resolution%20data.%20In%0Alow-resolution%20data%2C%20user%20box%20prompts%20were%20found%20crucial%20to%20achieve%20a%0Areasonable%20performance.%20This%20paper%20provides%20details%20on%20strengths%20and%0Alimitations%20of%20each%20model%20and%20outlines%20robustness%20of%20user%20prompted%20image%0Asegmentation%20models%20in%20inconsistent%20resolution%20and%20lighting%20conditions%20of%0Aremotely%20sensed%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Based%2520Segmentation%2520at%2520Multiple%2520Resolutions%2520and%2520Lighting%250A%2520%2520Conditions%2520using%2520Segment%2520Anything%2520Model%25202%26entry.906535625%3DOsher%2520Rafaeli%2520and%2520Tal%2520Svoray%2520and%2520Ariel%2520Nahlieli%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520insight%2520into%2520the%2520effectiveness%2520of%2520zero-shot%252C%250Aprompt-based%252C%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520and%2520its%2520updated%2520version%252C%2520SAM%25202%252C%2520and%250Athe%2520non-promptable%252C%2520conventional%2520convolutional%2520network%2520%2528CNN%2529%252C%2520in%2520segmenting%250Asolar%2520panels%252C%2520in%2520RGB%2520aerial%2520imagery%252C%2520across%2520lighting%2520conditions%252C%2520spatial%250Aresolutions%252C%2520and%2520prompt%2520strategies.%2520SAM%25202%2520demonstrates%2520improvements%2520over%2520SAM%252C%250Aparticularly%2520in%2520sub-optimal%2520lighting%2520conditions%2520when%2520prompted%2520by%2520points.%2520Both%250ASAMs%252C%2520prompted%2520by%2520user-box%252C%2520outperformed%2520CNN%252C%2520in%2520all%2520scenarios.%2520Additionally%252C%250AYOLOv9%2520prompting%2520outperformed%2520user%2520points%2520prompting.%2520In%2520high-resolution%250Aimagery%252C%2520both%2520in%2520optimal%2520and%2520sub-optimal%2520lighting%2520conditions%252C%2520Eff-UNet%250Aoutperformed%2520both%2520SAM%2520models%2520prompted%2520by%2520YOLOv9%2520boxes%252C%2520positioning%2520Eff-UNet%2520as%250Athe%2520appropriate%2520model%2520for%2520automatic%2520segmentation%2520in%2520high-resolution%2520data.%2520In%250Alow-resolution%2520data%252C%2520user%2520box%2520prompts%2520were%2520found%2520crucial%2520to%2520achieve%2520a%250Areasonable%2520performance.%2520This%2520paper%2520provides%2520details%2520on%2520strengths%2520and%250Alimitations%2520of%2520each%2520model%2520and%2520outlines%2520robustness%2520of%2520user%2520prompted%2520image%250Asegmentation%2520models%2520in%2520inconsistent%2520resolution%2520and%2520lighting%2520conditions%2520of%250Aremotely%2520sensed%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Based%20Segmentation%20at%20Multiple%20Resolutions%20and%20Lighting%0A%20%20Conditions%20using%20Segment%20Anything%20Model%202&entry.906535625=Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Ariel%20Nahlieli&entry.1292438233=%20%20This%20paper%20provides%20insight%20into%20the%20effectiveness%20of%20zero-shot%2C%0Aprompt-based%2C%20Segment%20Anything%20Model%20%28SAM%29%2C%20and%20its%20updated%20version%2C%20SAM%202%2C%20and%0Athe%20non-promptable%2C%20conventional%20convolutional%20network%20%28CNN%29%2C%20in%20segmenting%0Asolar%20panels%2C%20in%20RGB%20aerial%20imagery%2C%20across%20lighting%20conditions%2C%20spatial%0Aresolutions%2C%20and%20prompt%20strategies.%20SAM%202%20demonstrates%20improvements%20over%20SAM%2C%0Aparticularly%20in%20sub-optimal%20lighting%20conditions%20when%20prompted%20by%20points.%20Both%0ASAMs%2C%20prompted%20by%20user-box%2C%20outperformed%20CNN%2C%20in%20all%20scenarios.%20Additionally%2C%0AYOLOv9%20prompting%20outperformed%20user%20points%20prompting.%20In%20high-resolution%0Aimagery%2C%20both%20in%20optimal%20and%20sub-optimal%20lighting%20conditions%2C%20Eff-UNet%0Aoutperformed%20both%20SAM%20models%20prompted%20by%20YOLOv9%20boxes%2C%20positioning%20Eff-UNet%20as%0Athe%20appropriate%20model%20for%20automatic%20segmentation%20in%20high-resolution%20data.%20In%0Alow-resolution%20data%2C%20user%20box%20prompts%20were%20found%20crucial%20to%20achieve%20a%0Areasonable%20performance.%20This%20paper%20provides%20details%20on%20strengths%20and%0Alimitations%20of%20each%20model%20and%20outlines%20robustness%20of%20user%20prompted%20image%0Asegmentation%20models%20in%20inconsistent%20resolution%20and%20lighting%20conditions%20of%0Aremotely%20sensed%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06970v1&entry.124074799=Read"},
{"title": "Visual Neural Decoding via Improved Visual-EEG Semantic Consistency", "author": "Hongzhou Chen and Lianghua He and Yihang Liu and Longzhen Yang", "abstract": "  Visual neural decoding refers to the process of extracting and interpreting\noriginal visual experiences from human brain activity. Recent advances in\nmetric learning-based EEG visual decoding methods have delivered promising\nresults and demonstrated the feasibility of decoding novel visual categories\nfrom brain activity. However, methods that directly map EEG features to the\nCLIP embedding space may introduce mapping bias and cause semantic\ninconsistency among features, thereby degrading alignment and impairing\ndecoding performance. To further explore the semantic consistency between\nvisual and neural signals. In this work, we construct a joint semantic space\nand propose a Visual-EEG Semantic Decouple Framework that explicitly extracts\nthe semantic-related features of these two modalities to facilitate optimal\nalignment. Specifically, a cross-modal information decoupling module is\nintroduced to guide the extraction of semantic-related information from\nmodalities. Then, by quantifying the mutual information between visual image\nand EEG features, we observe a strong positive correlation between the decoding\nperformance and the magnitude of mutual information. Furthermore, inspired by\nthe mechanisms of visual object understanding from neuroscience, we propose an\nintra-class geometric consistency approach during the alignment process. This\nstrategy maps visual samples within the same class to consistent neural\npatterns, which further enhances the robustness and the performance of EEG\nvisual decoding. Experiments on a large Image-EEG dataset show that our method\nachieves state-of-the-art results in zero-shot neural decoding tasks.\n", "link": "http://arxiv.org/abs/2408.06788v1", "date": "2024-08-13", "relevancy": 2.0368, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Neural%20Decoding%20via%20Improved%20Visual-EEG%20Semantic%20Consistency&body=Title%3A%20Visual%20Neural%20Decoding%20via%20Improved%20Visual-EEG%20Semantic%20Consistency%0AAuthor%3A%20Hongzhou%20Chen%20and%20Lianghua%20He%20and%20Yihang%20Liu%20and%20Longzhen%20Yang%0AAbstract%3A%20%20%20Visual%20neural%20decoding%20refers%20to%20the%20process%20of%20extracting%20and%20interpreting%0Aoriginal%20visual%20experiences%20from%20human%20brain%20activity.%20Recent%20advances%20in%0Ametric%20learning-based%20EEG%20visual%20decoding%20methods%20have%20delivered%20promising%0Aresults%20and%20demonstrated%20the%20feasibility%20of%20decoding%20novel%20visual%20categories%0Afrom%20brain%20activity.%20However%2C%20methods%20that%20directly%20map%20EEG%20features%20to%20the%0ACLIP%20embedding%20space%20may%20introduce%20mapping%20bias%20and%20cause%20semantic%0Ainconsistency%20among%20features%2C%20thereby%20degrading%20alignment%20and%20impairing%0Adecoding%20performance.%20To%20further%20explore%20the%20semantic%20consistency%20between%0Avisual%20and%20neural%20signals.%20In%20this%20work%2C%20we%20construct%20a%20joint%20semantic%20space%0Aand%20propose%20a%20Visual-EEG%20Semantic%20Decouple%20Framework%20that%20explicitly%20extracts%0Athe%20semantic-related%20features%20of%20these%20two%20modalities%20to%20facilitate%20optimal%0Aalignment.%20Specifically%2C%20a%20cross-modal%20information%20decoupling%20module%20is%0Aintroduced%20to%20guide%20the%20extraction%20of%20semantic-related%20information%20from%0Amodalities.%20Then%2C%20by%20quantifying%20the%20mutual%20information%20between%20visual%20image%0Aand%20EEG%20features%2C%20we%20observe%20a%20strong%20positive%20correlation%20between%20the%20decoding%0Aperformance%20and%20the%20magnitude%20of%20mutual%20information.%20Furthermore%2C%20inspired%20by%0Athe%20mechanisms%20of%20visual%20object%20understanding%20from%20neuroscience%2C%20we%20propose%20an%0Aintra-class%20geometric%20consistency%20approach%20during%20the%20alignment%20process.%20This%0Astrategy%20maps%20visual%20samples%20within%20the%20same%20class%20to%20consistent%20neural%0Apatterns%2C%20which%20further%20enhances%20the%20robustness%20and%20the%20performance%20of%20EEG%0Avisual%20decoding.%20Experiments%20on%20a%20large%20Image-EEG%20dataset%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20results%20in%20zero-shot%20neural%20decoding%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Neural%2520Decoding%2520via%2520Improved%2520Visual-EEG%2520Semantic%2520Consistency%26entry.906535625%3DHongzhou%2520Chen%2520and%2520Lianghua%2520He%2520and%2520Yihang%2520Liu%2520and%2520Longzhen%2520Yang%26entry.1292438233%3D%2520%2520Visual%2520neural%2520decoding%2520refers%2520to%2520the%2520process%2520of%2520extracting%2520and%2520interpreting%250Aoriginal%2520visual%2520experiences%2520from%2520human%2520brain%2520activity.%2520Recent%2520advances%2520in%250Ametric%2520learning-based%2520EEG%2520visual%2520decoding%2520methods%2520have%2520delivered%2520promising%250Aresults%2520and%2520demonstrated%2520the%2520feasibility%2520of%2520decoding%2520novel%2520visual%2520categories%250Afrom%2520brain%2520activity.%2520However%252C%2520methods%2520that%2520directly%2520map%2520EEG%2520features%2520to%2520the%250ACLIP%2520embedding%2520space%2520may%2520introduce%2520mapping%2520bias%2520and%2520cause%2520semantic%250Ainconsistency%2520among%2520features%252C%2520thereby%2520degrading%2520alignment%2520and%2520impairing%250Adecoding%2520performance.%2520To%2520further%2520explore%2520the%2520semantic%2520consistency%2520between%250Avisual%2520and%2520neural%2520signals.%2520In%2520this%2520work%252C%2520we%2520construct%2520a%2520joint%2520semantic%2520space%250Aand%2520propose%2520a%2520Visual-EEG%2520Semantic%2520Decouple%2520Framework%2520that%2520explicitly%2520extracts%250Athe%2520semantic-related%2520features%2520of%2520these%2520two%2520modalities%2520to%2520facilitate%2520optimal%250Aalignment.%2520Specifically%252C%2520a%2520cross-modal%2520information%2520decoupling%2520module%2520is%250Aintroduced%2520to%2520guide%2520the%2520extraction%2520of%2520semantic-related%2520information%2520from%250Amodalities.%2520Then%252C%2520by%2520quantifying%2520the%2520mutual%2520information%2520between%2520visual%2520image%250Aand%2520EEG%2520features%252C%2520we%2520observe%2520a%2520strong%2520positive%2520correlation%2520between%2520the%2520decoding%250Aperformance%2520and%2520the%2520magnitude%2520of%2520mutual%2520information.%2520Furthermore%252C%2520inspired%2520by%250Athe%2520mechanisms%2520of%2520visual%2520object%2520understanding%2520from%2520neuroscience%252C%2520we%2520propose%2520an%250Aintra-class%2520geometric%2520consistency%2520approach%2520during%2520the%2520alignment%2520process.%2520This%250Astrategy%2520maps%2520visual%2520samples%2520within%2520the%2520same%2520class%2520to%2520consistent%2520neural%250Apatterns%252C%2520which%2520further%2520enhances%2520the%2520robustness%2520and%2520the%2520performance%2520of%2520EEG%250Avisual%2520decoding.%2520Experiments%2520on%2520a%2520large%2520Image-EEG%2520dataset%2520show%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520results%2520in%2520zero-shot%2520neural%2520decoding%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Neural%20Decoding%20via%20Improved%20Visual-EEG%20Semantic%20Consistency&entry.906535625=Hongzhou%20Chen%20and%20Lianghua%20He%20and%20Yihang%20Liu%20and%20Longzhen%20Yang&entry.1292438233=%20%20Visual%20neural%20decoding%20refers%20to%20the%20process%20of%20extracting%20and%20interpreting%0Aoriginal%20visual%20experiences%20from%20human%20brain%20activity.%20Recent%20advances%20in%0Ametric%20learning-based%20EEG%20visual%20decoding%20methods%20have%20delivered%20promising%0Aresults%20and%20demonstrated%20the%20feasibility%20of%20decoding%20novel%20visual%20categories%0Afrom%20brain%20activity.%20However%2C%20methods%20that%20directly%20map%20EEG%20features%20to%20the%0ACLIP%20embedding%20space%20may%20introduce%20mapping%20bias%20and%20cause%20semantic%0Ainconsistency%20among%20features%2C%20thereby%20degrading%20alignment%20and%20impairing%0Adecoding%20performance.%20To%20further%20explore%20the%20semantic%20consistency%20between%0Avisual%20and%20neural%20signals.%20In%20this%20work%2C%20we%20construct%20a%20joint%20semantic%20space%0Aand%20propose%20a%20Visual-EEG%20Semantic%20Decouple%20Framework%20that%20explicitly%20extracts%0Athe%20semantic-related%20features%20of%20these%20two%20modalities%20to%20facilitate%20optimal%0Aalignment.%20Specifically%2C%20a%20cross-modal%20information%20decoupling%20module%20is%0Aintroduced%20to%20guide%20the%20extraction%20of%20semantic-related%20information%20from%0Amodalities.%20Then%2C%20by%20quantifying%20the%20mutual%20information%20between%20visual%20image%0Aand%20EEG%20features%2C%20we%20observe%20a%20strong%20positive%20correlation%20between%20the%20decoding%0Aperformance%20and%20the%20magnitude%20of%20mutual%20information.%20Furthermore%2C%20inspired%20by%0Athe%20mechanisms%20of%20visual%20object%20understanding%20from%20neuroscience%2C%20we%20propose%20an%0Aintra-class%20geometric%20consistency%20approach%20during%20the%20alignment%20process.%20This%0Astrategy%20maps%20visual%20samples%20within%20the%20same%20class%20to%20consistent%20neural%0Apatterns%2C%20which%20further%20enhances%20the%20robustness%20and%20the%20performance%20of%20EEG%0Avisual%20decoding.%20Experiments%20on%20a%20large%20Image-EEG%20dataset%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20results%20in%20zero-shot%20neural%20decoding%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06788v1&entry.124074799=Read"},
{"title": "How Transformers Learn Causal Structure with Gradient Descent", "author": "Eshaan Nichani and Alex Damian and Jason D. Lee", "abstract": "  The incredible success of transformers on sequence modeling tasks can be\nlargely attributed to the self-attention mechanism, which allows information to\nbe transferred between different parts of a sequence. Self-attention allows\ntransformers to encode causal structure which makes them particularly suitable\nfor sequence modeling. However, the process by which transformers learn such\ncausal structure via gradient-based training algorithms remains poorly\nunderstood. To better understand this process, we introduce an in-context\nlearning task that requires learning latent causal structure. We prove that\ngradient descent on a simplified two-layer transformer learns to solve this\ntask by encoding the latent causal graph in the first attention layer. The key\ninsight of our proof is that the gradient of the attention matrix encodes the\nmutual information between tokens. As a consequence of the data processing\ninequality, the largest entries of this gradient correspond to edges in the\nlatent causal graph. As a special case, when the sequences are generated from\nin-context Markov chains, we prove that transformers learn an induction head\n(Olsson et al., 2022). We confirm our theoretical findings by showing that\ntransformers trained on our in-context learning task are able to recover a wide\nvariety of causal structures.\n", "link": "http://arxiv.org/abs/2402.14735v2", "date": "2024-08-13", "relevancy": 2.0316, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5387}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5041}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Transformers%20Learn%20Causal%20Structure%20with%20Gradient%20Descent&body=Title%3A%20How%20Transformers%20Learn%20Causal%20Structure%20with%20Gradient%20Descent%0AAuthor%3A%20Eshaan%20Nichani%20and%20Alex%20Damian%20and%20Jason%20D.%20Lee%0AAbstract%3A%20%20%20The%20incredible%20success%20of%20transformers%20on%20sequence%20modeling%20tasks%20can%20be%0Alargely%20attributed%20to%20the%20self-attention%20mechanism%2C%20which%20allows%20information%20to%0Abe%20transferred%20between%20different%20parts%20of%20a%20sequence.%20Self-attention%20allows%0Atransformers%20to%20encode%20causal%20structure%20which%20makes%20them%20particularly%20suitable%0Afor%20sequence%20modeling.%20However%2C%20the%20process%20by%20which%20transformers%20learn%20such%0Acausal%20structure%20via%20gradient-based%20training%20algorithms%20remains%20poorly%0Aunderstood.%20To%20better%20understand%20this%20process%2C%20we%20introduce%20an%20in-context%0Alearning%20task%20that%20requires%20learning%20latent%20causal%20structure.%20We%20prove%20that%0Agradient%20descent%20on%20a%20simplified%20two-layer%20transformer%20learns%20to%20solve%20this%0Atask%20by%20encoding%20the%20latent%20causal%20graph%20in%20the%20first%20attention%20layer.%20The%20key%0Ainsight%20of%20our%20proof%20is%20that%20the%20gradient%20of%20the%20attention%20matrix%20encodes%20the%0Amutual%20information%20between%20tokens.%20As%20a%20consequence%20of%20the%20data%20processing%0Ainequality%2C%20the%20largest%20entries%20of%20this%20gradient%20correspond%20to%20edges%20in%20the%0Alatent%20causal%20graph.%20As%20a%20special%20case%2C%20when%20the%20sequences%20are%20generated%20from%0Ain-context%20Markov%20chains%2C%20we%20prove%20that%20transformers%20learn%20an%20induction%20head%0A%28Olsson%20et%20al.%2C%202022%29.%20We%20confirm%20our%20theoretical%20findings%20by%20showing%20that%0Atransformers%20trained%20on%20our%20in-context%20learning%20task%20are%20able%20to%20recover%20a%20wide%0Avariety%20of%20causal%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14735v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Transformers%2520Learn%2520Causal%2520Structure%2520with%2520Gradient%2520Descent%26entry.906535625%3DEshaan%2520Nichani%2520and%2520Alex%2520Damian%2520and%2520Jason%2520D.%2520Lee%26entry.1292438233%3D%2520%2520The%2520incredible%2520success%2520of%2520transformers%2520on%2520sequence%2520modeling%2520tasks%2520can%2520be%250Alargely%2520attributed%2520to%2520the%2520self-attention%2520mechanism%252C%2520which%2520allows%2520information%2520to%250Abe%2520transferred%2520between%2520different%2520parts%2520of%2520a%2520sequence.%2520Self-attention%2520allows%250Atransformers%2520to%2520encode%2520causal%2520structure%2520which%2520makes%2520them%2520particularly%2520suitable%250Afor%2520sequence%2520modeling.%2520However%252C%2520the%2520process%2520by%2520which%2520transformers%2520learn%2520such%250Acausal%2520structure%2520via%2520gradient-based%2520training%2520algorithms%2520remains%2520poorly%250Aunderstood.%2520To%2520better%2520understand%2520this%2520process%252C%2520we%2520introduce%2520an%2520in-context%250Alearning%2520task%2520that%2520requires%2520learning%2520latent%2520causal%2520structure.%2520We%2520prove%2520that%250Agradient%2520descent%2520on%2520a%2520simplified%2520two-layer%2520transformer%2520learns%2520to%2520solve%2520this%250Atask%2520by%2520encoding%2520the%2520latent%2520causal%2520graph%2520in%2520the%2520first%2520attention%2520layer.%2520The%2520key%250Ainsight%2520of%2520our%2520proof%2520is%2520that%2520the%2520gradient%2520of%2520the%2520attention%2520matrix%2520encodes%2520the%250Amutual%2520information%2520between%2520tokens.%2520As%2520a%2520consequence%2520of%2520the%2520data%2520processing%250Ainequality%252C%2520the%2520largest%2520entries%2520of%2520this%2520gradient%2520correspond%2520to%2520edges%2520in%2520the%250Alatent%2520causal%2520graph.%2520As%2520a%2520special%2520case%252C%2520when%2520the%2520sequences%2520are%2520generated%2520from%250Ain-context%2520Markov%2520chains%252C%2520we%2520prove%2520that%2520transformers%2520learn%2520an%2520induction%2520head%250A%2528Olsson%2520et%2520al.%252C%25202022%2529.%2520We%2520confirm%2520our%2520theoretical%2520findings%2520by%2520showing%2520that%250Atransformers%2520trained%2520on%2520our%2520in-context%2520learning%2520task%2520are%2520able%2520to%2520recover%2520a%2520wide%250Avariety%2520of%2520causal%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14735v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Transformers%20Learn%20Causal%20Structure%20with%20Gradient%20Descent&entry.906535625=Eshaan%20Nichani%20and%20Alex%20Damian%20and%20Jason%20D.%20Lee&entry.1292438233=%20%20The%20incredible%20success%20of%20transformers%20on%20sequence%20modeling%20tasks%20can%20be%0Alargely%20attributed%20to%20the%20self-attention%20mechanism%2C%20which%20allows%20information%20to%0Abe%20transferred%20between%20different%20parts%20of%20a%20sequence.%20Self-attention%20allows%0Atransformers%20to%20encode%20causal%20structure%20which%20makes%20them%20particularly%20suitable%0Afor%20sequence%20modeling.%20However%2C%20the%20process%20by%20which%20transformers%20learn%20such%0Acausal%20structure%20via%20gradient-based%20training%20algorithms%20remains%20poorly%0Aunderstood.%20To%20better%20understand%20this%20process%2C%20we%20introduce%20an%20in-context%0Alearning%20task%20that%20requires%20learning%20latent%20causal%20structure.%20We%20prove%20that%0Agradient%20descent%20on%20a%20simplified%20two-layer%20transformer%20learns%20to%20solve%20this%0Atask%20by%20encoding%20the%20latent%20causal%20graph%20in%20the%20first%20attention%20layer.%20The%20key%0Ainsight%20of%20our%20proof%20is%20that%20the%20gradient%20of%20the%20attention%20matrix%20encodes%20the%0Amutual%20information%20between%20tokens.%20As%20a%20consequence%20of%20the%20data%20processing%0Ainequality%2C%20the%20largest%20entries%20of%20this%20gradient%20correspond%20to%20edges%20in%20the%0Alatent%20causal%20graph.%20As%20a%20special%20case%2C%20when%20the%20sequences%20are%20generated%20from%0Ain-context%20Markov%20chains%2C%20we%20prove%20that%20transformers%20learn%20an%20induction%20head%0A%28Olsson%20et%20al.%2C%202022%29.%20We%20confirm%20our%20theoretical%20findings%20by%20showing%20that%0Atransformers%20trained%20on%20our%20in-context%20learning%20task%20are%20able%20to%20recover%20a%20wide%0Avariety%20of%20causal%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14735v2&entry.124074799=Read"},
{"title": "Membership Inference Attack Against Masked Image Modeling", "author": "Zheng Li and Xinlei He and Ning Yu and Yang Zhang", "abstract": "  Masked Image Modeling (MIM) has achieved significant success in the realm of\nself-supervised learning (SSL) for visual recognition. The image encoder\npre-trained through MIM, involving the masking and subsequent reconstruction of\ninput images, attains state-of-the-art performance in various downstream vision\ntasks. However, most existing works focus on improving the performance of\nMIM.In this work, we take a different angle by studying the pre-training data\nprivacy of MIM. Specifically, we propose the first membership inference attack\nagainst image encoders pre-trained by MIM, which aims to determine whether an\nimage is part of the MIM pre-training dataset. The key design is to simulate\nthe pre-training paradigm of MIM, i.e., image masking and subsequent\nreconstruction, and then obtain reconstruction errors. These reconstruction\nerrors can serve as membership signals for achieving attack goals, as the\nencoder is more capable of reconstructing the input image in its training set\nwith lower errors. Extensive evaluations are conducted on three model\narchitectures and three benchmark datasets. Empirical results show that our\nattack outperforms baseline methods. Additionally, we undertake intricate\nablation studies to analyze multiple factors that could influence the\nperformance of the attack.\n", "link": "http://arxiv.org/abs/2408.06825v1", "date": "2024-08-13", "relevancy": 2.0159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4992}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Membership%20Inference%20Attack%20Against%20Masked%20Image%20Modeling&body=Title%3A%20Membership%20Inference%20Attack%20Against%20Masked%20Image%20Modeling%0AAuthor%3A%20Zheng%20Li%20and%20Xinlei%20He%20and%20Ning%20Yu%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20achieved%20significant%20success%20in%20the%20realm%20of%0Aself-supervised%20learning%20%28SSL%29%20for%20visual%20recognition.%20The%20image%20encoder%0Apre-trained%20through%20MIM%2C%20involving%20the%20masking%20and%20subsequent%20reconstruction%20of%0Ainput%20images%2C%20attains%20state-of-the-art%20performance%20in%20various%20downstream%20vision%0Atasks.%20However%2C%20most%20existing%20works%20focus%20on%20improving%20the%20performance%20of%0AMIM.In%20this%20work%2C%20we%20take%20a%20different%20angle%20by%20studying%20the%20pre-training%20data%0Aprivacy%20of%20MIM.%20Specifically%2C%20we%20propose%20the%20first%20membership%20inference%20attack%0Aagainst%20image%20encoders%20pre-trained%20by%20MIM%2C%20which%20aims%20to%20determine%20whether%20an%0Aimage%20is%20part%20of%20the%20MIM%20pre-training%20dataset.%20The%20key%20design%20is%20to%20simulate%0Athe%20pre-training%20paradigm%20of%20MIM%2C%20i.e.%2C%20image%20masking%20and%20subsequent%0Areconstruction%2C%20and%20then%20obtain%20reconstruction%20errors.%20These%20reconstruction%0Aerrors%20can%20serve%20as%20membership%20signals%20for%20achieving%20attack%20goals%2C%20as%20the%0Aencoder%20is%20more%20capable%20of%20reconstructing%20the%20input%20image%20in%20its%20training%20set%0Awith%20lower%20errors.%20Extensive%20evaluations%20are%20conducted%20on%20three%20model%0Aarchitectures%20and%20three%20benchmark%20datasets.%20Empirical%20results%20show%20that%20our%0Aattack%20outperforms%20baseline%20methods.%20Additionally%2C%20we%20undertake%20intricate%0Aablation%20studies%20to%20analyze%20multiple%20factors%20that%20could%20influence%20the%0Aperformance%20of%20the%20attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMembership%2520Inference%2520Attack%2520Against%2520Masked%2520Image%2520Modeling%26entry.906535625%3DZheng%2520Li%2520and%2520Xinlei%2520He%2520and%2520Ning%2520Yu%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%2520has%2520achieved%2520significant%2520success%2520in%2520the%2520realm%2520of%250Aself-supervised%2520learning%2520%2528SSL%2529%2520for%2520visual%2520recognition.%2520The%2520image%2520encoder%250Apre-trained%2520through%2520MIM%252C%2520involving%2520the%2520masking%2520and%2520subsequent%2520reconstruction%2520of%250Ainput%2520images%252C%2520attains%2520state-of-the-art%2520performance%2520in%2520various%2520downstream%2520vision%250Atasks.%2520However%252C%2520most%2520existing%2520works%2520focus%2520on%2520improving%2520the%2520performance%2520of%250AMIM.In%2520this%2520work%252C%2520we%2520take%2520a%2520different%2520angle%2520by%2520studying%2520the%2520pre-training%2520data%250Aprivacy%2520of%2520MIM.%2520Specifically%252C%2520we%2520propose%2520the%2520first%2520membership%2520inference%2520attack%250Aagainst%2520image%2520encoders%2520pre-trained%2520by%2520MIM%252C%2520which%2520aims%2520to%2520determine%2520whether%2520an%250Aimage%2520is%2520part%2520of%2520the%2520MIM%2520pre-training%2520dataset.%2520The%2520key%2520design%2520is%2520to%2520simulate%250Athe%2520pre-training%2520paradigm%2520of%2520MIM%252C%2520i.e.%252C%2520image%2520masking%2520and%2520subsequent%250Areconstruction%252C%2520and%2520then%2520obtain%2520reconstruction%2520errors.%2520These%2520reconstruction%250Aerrors%2520can%2520serve%2520as%2520membership%2520signals%2520for%2520achieving%2520attack%2520goals%252C%2520as%2520the%250Aencoder%2520is%2520more%2520capable%2520of%2520reconstructing%2520the%2520input%2520image%2520in%2520its%2520training%2520set%250Awith%2520lower%2520errors.%2520Extensive%2520evaluations%2520are%2520conducted%2520on%2520three%2520model%250Aarchitectures%2520and%2520three%2520benchmark%2520datasets.%2520Empirical%2520results%2520show%2520that%2520our%250Aattack%2520outperforms%2520baseline%2520methods.%2520Additionally%252C%2520we%2520undertake%2520intricate%250Aablation%2520studies%2520to%2520analyze%2520multiple%2520factors%2520that%2520could%2520influence%2520the%250Aperformance%2520of%2520the%2520attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Membership%20Inference%20Attack%20Against%20Masked%20Image%20Modeling&entry.906535625=Zheng%20Li%20and%20Xinlei%20He%20and%20Ning%20Yu%20and%20Yang%20Zhang&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20achieved%20significant%20success%20in%20the%20realm%20of%0Aself-supervised%20learning%20%28SSL%29%20for%20visual%20recognition.%20The%20image%20encoder%0Apre-trained%20through%20MIM%2C%20involving%20the%20masking%20and%20subsequent%20reconstruction%20of%0Ainput%20images%2C%20attains%20state-of-the-art%20performance%20in%20various%20downstream%20vision%0Atasks.%20However%2C%20most%20existing%20works%20focus%20on%20improving%20the%20performance%20of%0AMIM.In%20this%20work%2C%20we%20take%20a%20different%20angle%20by%20studying%20the%20pre-training%20data%0Aprivacy%20of%20MIM.%20Specifically%2C%20we%20propose%20the%20first%20membership%20inference%20attack%0Aagainst%20image%20encoders%20pre-trained%20by%20MIM%2C%20which%20aims%20to%20determine%20whether%20an%0Aimage%20is%20part%20of%20the%20MIM%20pre-training%20dataset.%20The%20key%20design%20is%20to%20simulate%0Athe%20pre-training%20paradigm%20of%20MIM%2C%20i.e.%2C%20image%20masking%20and%20subsequent%0Areconstruction%2C%20and%20then%20obtain%20reconstruction%20errors.%20These%20reconstruction%0Aerrors%20can%20serve%20as%20membership%20signals%20for%20achieving%20attack%20goals%2C%20as%20the%0Aencoder%20is%20more%20capable%20of%20reconstructing%20the%20input%20image%20in%20its%20training%20set%0Awith%20lower%20errors.%20Extensive%20evaluations%20are%20conducted%20on%20three%20model%0Aarchitectures%20and%20three%20benchmark%20datasets.%20Empirical%20results%20show%20that%20our%0Aattack%20outperforms%20baseline%20methods.%20Additionally%2C%20we%20undertake%20intricate%0Aablation%20studies%20to%20analyze%20multiple%20factors%20that%20could%20influence%20the%0Aperformance%20of%20the%20attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06825v1&entry.124074799=Read"},
{"title": "SE(3)-Hyena Operator for Scalable Equivariant Learning", "author": "Artem Moskalev and Mangal Prakash and Rui Liao and Tommaso Mansi", "abstract": "  Modeling global geometric context while maintaining equivariance is crucial\nfor accurate predictions in many fields such as biology, chemistry, or vision.\nYet, this is challenging due to the computational demands of processing\nhigh-dimensional data at scale. Existing approaches such as equivariant\nself-attention or distance-based message passing, suffer from quadratic\ncomplexity with respect to sequence length, while localized methods sacrifice\nglobal information. Inspired by the recent success of state-space and\nlong-convolutional models, in this work, we introduce SE(3)-Hyena operator, an\nequivariant long-convolutional model based on the Hyena operator. The\nSE(3)-Hyena captures global geometric context at sub-quadratic complexity while\nmaintaining equivariance to rotations and translations. Evaluated on\nequivariant associative recall and n-body modeling, SE(3)-Hyena matches or\noutperforms equivariant self-attention while requiring significantly less\nmemory and computational resources for long sequences. Our model processes the\ngeometric context of 20k tokens x3.5 times faster than the equivariant\ntransformer and allows x175 longer a context within the same memory budget.\n", "link": "http://arxiv.org/abs/2407.01049v2", "date": "2024-08-13", "relevancy": 2.0028, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5113}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4937}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SE%283%29-Hyena%20Operator%20for%20Scalable%20Equivariant%20Learning&body=Title%3A%20SE%283%29-Hyena%20Operator%20for%20Scalable%20Equivariant%20Learning%0AAuthor%3A%20Artem%20Moskalev%20and%20Mangal%20Prakash%20and%20Rui%20Liao%20and%20Tommaso%20Mansi%0AAbstract%3A%20%20%20Modeling%20global%20geometric%20context%20while%20maintaining%20equivariance%20is%20crucial%0Afor%20accurate%20predictions%20in%20many%20fields%20such%20as%20biology%2C%20chemistry%2C%20or%20vision.%0AYet%2C%20this%20is%20challenging%20due%20to%20the%20computational%20demands%20of%20processing%0Ahigh-dimensional%20data%20at%20scale.%20Existing%20approaches%20such%20as%20equivariant%0Aself-attention%20or%20distance-based%20message%20passing%2C%20suffer%20from%20quadratic%0Acomplexity%20with%20respect%20to%20sequence%20length%2C%20while%20localized%20methods%20sacrifice%0Aglobal%20information.%20Inspired%20by%20the%20recent%20success%20of%20state-space%20and%0Along-convolutional%20models%2C%20in%20this%20work%2C%20we%20introduce%20SE%283%29-Hyena%20operator%2C%20an%0Aequivariant%20long-convolutional%20model%20based%20on%20the%20Hyena%20operator.%20The%0ASE%283%29-Hyena%20captures%20global%20geometric%20context%20at%20sub-quadratic%20complexity%20while%0Amaintaining%20equivariance%20to%20rotations%20and%20translations.%20Evaluated%20on%0Aequivariant%20associative%20recall%20and%20n-body%20modeling%2C%20SE%283%29-Hyena%20matches%20or%0Aoutperforms%20equivariant%20self-attention%20while%20requiring%20significantly%20less%0Amemory%20and%20computational%20resources%20for%20long%20sequences.%20Our%20model%20processes%20the%0Ageometric%20context%20of%2020k%20tokens%20x3.5%20times%20faster%20than%20the%20equivariant%0Atransformer%20and%20allows%20x175%20longer%20a%20context%20within%20the%20same%20memory%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSE%25283%2529-Hyena%2520Operator%2520for%2520Scalable%2520Equivariant%2520Learning%26entry.906535625%3DArtem%2520Moskalev%2520and%2520Mangal%2520Prakash%2520and%2520Rui%2520Liao%2520and%2520Tommaso%2520Mansi%26entry.1292438233%3D%2520%2520Modeling%2520global%2520geometric%2520context%2520while%2520maintaining%2520equivariance%2520is%2520crucial%250Afor%2520accurate%2520predictions%2520in%2520many%2520fields%2520such%2520as%2520biology%252C%2520chemistry%252C%2520or%2520vision.%250AYet%252C%2520this%2520is%2520challenging%2520due%2520to%2520the%2520computational%2520demands%2520of%2520processing%250Ahigh-dimensional%2520data%2520at%2520scale.%2520Existing%2520approaches%2520such%2520as%2520equivariant%250Aself-attention%2520or%2520distance-based%2520message%2520passing%252C%2520suffer%2520from%2520quadratic%250Acomplexity%2520with%2520respect%2520to%2520sequence%2520length%252C%2520while%2520localized%2520methods%2520sacrifice%250Aglobal%2520information.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520state-space%2520and%250Along-convolutional%2520models%252C%2520in%2520this%2520work%252C%2520we%2520introduce%2520SE%25283%2529-Hyena%2520operator%252C%2520an%250Aequivariant%2520long-convolutional%2520model%2520based%2520on%2520the%2520Hyena%2520operator.%2520The%250ASE%25283%2529-Hyena%2520captures%2520global%2520geometric%2520context%2520at%2520sub-quadratic%2520complexity%2520while%250Amaintaining%2520equivariance%2520to%2520rotations%2520and%2520translations.%2520Evaluated%2520on%250Aequivariant%2520associative%2520recall%2520and%2520n-body%2520modeling%252C%2520SE%25283%2529-Hyena%2520matches%2520or%250Aoutperforms%2520equivariant%2520self-attention%2520while%2520requiring%2520significantly%2520less%250Amemory%2520and%2520computational%2520resources%2520for%2520long%2520sequences.%2520Our%2520model%2520processes%2520the%250Ageometric%2520context%2520of%252020k%2520tokens%2520x3.5%2520times%2520faster%2520than%2520the%2520equivariant%250Atransformer%2520and%2520allows%2520x175%2520longer%2520a%2520context%2520within%2520the%2520same%2520memory%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SE%283%29-Hyena%20Operator%20for%20Scalable%20Equivariant%20Learning&entry.906535625=Artem%20Moskalev%20and%20Mangal%20Prakash%20and%20Rui%20Liao%20and%20Tommaso%20Mansi&entry.1292438233=%20%20Modeling%20global%20geometric%20context%20while%20maintaining%20equivariance%20is%20crucial%0Afor%20accurate%20predictions%20in%20many%20fields%20such%20as%20biology%2C%20chemistry%2C%20or%20vision.%0AYet%2C%20this%20is%20challenging%20due%20to%20the%20computational%20demands%20of%20processing%0Ahigh-dimensional%20data%20at%20scale.%20Existing%20approaches%20such%20as%20equivariant%0Aself-attention%20or%20distance-based%20message%20passing%2C%20suffer%20from%20quadratic%0Acomplexity%20with%20respect%20to%20sequence%20length%2C%20while%20localized%20methods%20sacrifice%0Aglobal%20information.%20Inspired%20by%20the%20recent%20success%20of%20state-space%20and%0Along-convolutional%20models%2C%20in%20this%20work%2C%20we%20introduce%20SE%283%29-Hyena%20operator%2C%20an%0Aequivariant%20long-convolutional%20model%20based%20on%20the%20Hyena%20operator.%20The%0ASE%283%29-Hyena%20captures%20global%20geometric%20context%20at%20sub-quadratic%20complexity%20while%0Amaintaining%20equivariance%20to%20rotations%20and%20translations.%20Evaluated%20on%0Aequivariant%20associative%20recall%20and%20n-body%20modeling%2C%20SE%283%29-Hyena%20matches%20or%0Aoutperforms%20equivariant%20self-attention%20while%20requiring%20significantly%20less%0Amemory%20and%20computational%20resources%20for%20long%20sequences.%20Our%20model%20processes%20the%0Ageometric%20context%20of%2020k%20tokens%20x3.5%20times%20faster%20than%20the%20equivariant%0Atransformer%20and%20allows%20x175%20longer%20a%20context%20within%20the%20same%20memory%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01049v2&entry.124074799=Read"},
{"title": "Automatic Feature Recognition and Dimensional Attributes Extraction From\n  CAD Models for Hybrid Additive-Subtractive Manufacturing", "author": "Muhammad Tayyab Khan and Wenhe Feng and Lequn Chen and Ye Han Ng and Nicholas Yew Jin Tan and Seung Ki Moon", "abstract": "  The integration of Computer-Aided Design (CAD), Computer-Aided Process\nPlanning (CAPP), and Computer-Aided Manufacturing (CAM) plays a crucial role in\nmodern manufacturing, facilitating seamless transitions from digital designs to\nphysical products. However, a significant challenge within this integration is\nthe Automatic Feature Recognition (AFR) of CAD models, especially in the\ncontext of hybrid manufacturing that combines subtractive and additive\nmanufacturing processes. Traditional AFR methods, focused mainly on the\nidentification of subtractive (machined) features including holes, fillets,\nchamfers, pockets, and slots, fail to recognize features pertinent to additive\nmanufacturing. Furthermore, the traditional methods fall short in accurately\nextracting geometric dimensions and orientations, which are also key factors\nfor effective manufacturing process planning. This paper presents a novel\napproach for creating a synthetic CAD dataset that encompasses features\nrelevant to both additive and subtractive machining through Python Open\nCascade. The Hierarchical Graph Convolutional Neural Network (HGCNN) model is\nimplemented to accurately identify the composite additive-subtractive features\nwithin the synthetic CAD dataset. The key novelty and contribution of the\nproposed methodology lie in its ability to recognize a wide range of\nmanufacturing features, and precisely extracting their dimensions,\norientations, and stock sizes. The proposed model demonstrates remarkable\nfeature recognition accuracy exceeding 97% and a dimension extraction accuracy\nof 100% for identified features. Therefore, the proposed methodology enhances\nthe integration of CAD, CAPP, and CAM within hybrid manufacturing by providing\nprecise feature recognition and dimension extraction. It facilitates improved\nmanufacturing process planning, by enabling more informed decision-making.\n", "link": "http://arxiv.org/abs/2408.06891v1", "date": "2024-08-13", "relevancy": 1.9969, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.503}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.503}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Feature%20Recognition%20and%20Dimensional%20Attributes%20Extraction%20From%0A%20%20CAD%20Models%20for%20Hybrid%20Additive-Subtractive%20Manufacturing&body=Title%3A%20Automatic%20Feature%20Recognition%20and%20Dimensional%20Attributes%20Extraction%20From%0A%20%20CAD%20Models%20for%20Hybrid%20Additive-Subtractive%20Manufacturing%0AAuthor%3A%20Muhammad%20Tayyab%20Khan%20and%20Wenhe%20Feng%20and%20Lequn%20Chen%20and%20Ye%20Han%20Ng%20and%20Nicholas%20Yew%20Jin%20Tan%20and%20Seung%20Ki%20Moon%0AAbstract%3A%20%20%20The%20integration%20of%20Computer-Aided%20Design%20%28CAD%29%2C%20Computer-Aided%20Process%0APlanning%20%28CAPP%29%2C%20and%20Computer-Aided%20Manufacturing%20%28CAM%29%20plays%20a%20crucial%20role%20in%0Amodern%20manufacturing%2C%20facilitating%20seamless%20transitions%20from%20digital%20designs%20to%0Aphysical%20products.%20However%2C%20a%20significant%20challenge%20within%20this%20integration%20is%0Athe%20Automatic%20Feature%20Recognition%20%28AFR%29%20of%20CAD%20models%2C%20especially%20in%20the%0Acontext%20of%20hybrid%20manufacturing%20that%20combines%20subtractive%20and%20additive%0Amanufacturing%20processes.%20Traditional%20AFR%20methods%2C%20focused%20mainly%20on%20the%0Aidentification%20of%20subtractive%20%28machined%29%20features%20including%20holes%2C%20fillets%2C%0Achamfers%2C%20pockets%2C%20and%20slots%2C%20fail%20to%20recognize%20features%20pertinent%20to%20additive%0Amanufacturing.%20Furthermore%2C%20the%20traditional%20methods%20fall%20short%20in%20accurately%0Aextracting%20geometric%20dimensions%20and%20orientations%2C%20which%20are%20also%20key%20factors%0Afor%20effective%20manufacturing%20process%20planning.%20This%20paper%20presents%20a%20novel%0Aapproach%20for%20creating%20a%20synthetic%20CAD%20dataset%20that%20encompasses%20features%0Arelevant%20to%20both%20additive%20and%20subtractive%20machining%20through%20Python%20Open%0ACascade.%20The%20Hierarchical%20Graph%20Convolutional%20Neural%20Network%20%28HGCNN%29%20model%20is%0Aimplemented%20to%20accurately%20identify%20the%20composite%20additive-subtractive%20features%0Awithin%20the%20synthetic%20CAD%20dataset.%20The%20key%20novelty%20and%20contribution%20of%20the%0Aproposed%20methodology%20lie%20in%20its%20ability%20to%20recognize%20a%20wide%20range%20of%0Amanufacturing%20features%2C%20and%20precisely%20extracting%20their%20dimensions%2C%0Aorientations%2C%20and%20stock%20sizes.%20The%20proposed%20model%20demonstrates%20remarkable%0Afeature%20recognition%20accuracy%20exceeding%2097%25%20and%20a%20dimension%20extraction%20accuracy%0Aof%20100%25%20for%20identified%20features.%20Therefore%2C%20the%20proposed%20methodology%20enhances%0Athe%20integration%20of%20CAD%2C%20CAPP%2C%20and%20CAM%20within%20hybrid%20manufacturing%20by%20providing%0Aprecise%20feature%20recognition%20and%20dimension%20extraction.%20It%20facilitates%20improved%0Amanufacturing%20process%20planning%2C%20by%20enabling%20more%20informed%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Feature%2520Recognition%2520and%2520Dimensional%2520Attributes%2520Extraction%2520From%250A%2520%2520CAD%2520Models%2520for%2520Hybrid%2520Additive-Subtractive%2520Manufacturing%26entry.906535625%3DMuhammad%2520Tayyab%2520Khan%2520and%2520Wenhe%2520Feng%2520and%2520Lequn%2520Chen%2520and%2520Ye%2520Han%2520Ng%2520and%2520Nicholas%2520Yew%2520Jin%2520Tan%2520and%2520Seung%2520Ki%2520Moon%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Computer-Aided%2520Design%2520%2528CAD%2529%252C%2520Computer-Aided%2520Process%250APlanning%2520%2528CAPP%2529%252C%2520and%2520Computer-Aided%2520Manufacturing%2520%2528CAM%2529%2520plays%2520a%2520crucial%2520role%2520in%250Amodern%2520manufacturing%252C%2520facilitating%2520seamless%2520transitions%2520from%2520digital%2520designs%2520to%250Aphysical%2520products.%2520However%252C%2520a%2520significant%2520challenge%2520within%2520this%2520integration%2520is%250Athe%2520Automatic%2520Feature%2520Recognition%2520%2528AFR%2529%2520of%2520CAD%2520models%252C%2520especially%2520in%2520the%250Acontext%2520of%2520hybrid%2520manufacturing%2520that%2520combines%2520subtractive%2520and%2520additive%250Amanufacturing%2520processes.%2520Traditional%2520AFR%2520methods%252C%2520focused%2520mainly%2520on%2520the%250Aidentification%2520of%2520subtractive%2520%2528machined%2529%2520features%2520including%2520holes%252C%2520fillets%252C%250Achamfers%252C%2520pockets%252C%2520and%2520slots%252C%2520fail%2520to%2520recognize%2520features%2520pertinent%2520to%2520additive%250Amanufacturing.%2520Furthermore%252C%2520the%2520traditional%2520methods%2520fall%2520short%2520in%2520accurately%250Aextracting%2520geometric%2520dimensions%2520and%2520orientations%252C%2520which%2520are%2520also%2520key%2520factors%250Afor%2520effective%2520manufacturing%2520process%2520planning.%2520This%2520paper%2520presents%2520a%2520novel%250Aapproach%2520for%2520creating%2520a%2520synthetic%2520CAD%2520dataset%2520that%2520encompasses%2520features%250Arelevant%2520to%2520both%2520additive%2520and%2520subtractive%2520machining%2520through%2520Python%2520Open%250ACascade.%2520The%2520Hierarchical%2520Graph%2520Convolutional%2520Neural%2520Network%2520%2528HGCNN%2529%2520model%2520is%250Aimplemented%2520to%2520accurately%2520identify%2520the%2520composite%2520additive-subtractive%2520features%250Awithin%2520the%2520synthetic%2520CAD%2520dataset.%2520The%2520key%2520novelty%2520and%2520contribution%2520of%2520the%250Aproposed%2520methodology%2520lie%2520in%2520its%2520ability%2520to%2520recognize%2520a%2520wide%2520range%2520of%250Amanufacturing%2520features%252C%2520and%2520precisely%2520extracting%2520their%2520dimensions%252C%250Aorientations%252C%2520and%2520stock%2520sizes.%2520The%2520proposed%2520model%2520demonstrates%2520remarkable%250Afeature%2520recognition%2520accuracy%2520exceeding%252097%2525%2520and%2520a%2520dimension%2520extraction%2520accuracy%250Aof%2520100%2525%2520for%2520identified%2520features.%2520Therefore%252C%2520the%2520proposed%2520methodology%2520enhances%250Athe%2520integration%2520of%2520CAD%252C%2520CAPP%252C%2520and%2520CAM%2520within%2520hybrid%2520manufacturing%2520by%2520providing%250Aprecise%2520feature%2520recognition%2520and%2520dimension%2520extraction.%2520It%2520facilitates%2520improved%250Amanufacturing%2520process%2520planning%252C%2520by%2520enabling%2520more%2520informed%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Feature%20Recognition%20and%20Dimensional%20Attributes%20Extraction%20From%0A%20%20CAD%20Models%20for%20Hybrid%20Additive-Subtractive%20Manufacturing&entry.906535625=Muhammad%20Tayyab%20Khan%20and%20Wenhe%20Feng%20and%20Lequn%20Chen%20and%20Ye%20Han%20Ng%20and%20Nicholas%20Yew%20Jin%20Tan%20and%20Seung%20Ki%20Moon&entry.1292438233=%20%20The%20integration%20of%20Computer-Aided%20Design%20%28CAD%29%2C%20Computer-Aided%20Process%0APlanning%20%28CAPP%29%2C%20and%20Computer-Aided%20Manufacturing%20%28CAM%29%20plays%20a%20crucial%20role%20in%0Amodern%20manufacturing%2C%20facilitating%20seamless%20transitions%20from%20digital%20designs%20to%0Aphysical%20products.%20However%2C%20a%20significant%20challenge%20within%20this%20integration%20is%0Athe%20Automatic%20Feature%20Recognition%20%28AFR%29%20of%20CAD%20models%2C%20especially%20in%20the%0Acontext%20of%20hybrid%20manufacturing%20that%20combines%20subtractive%20and%20additive%0Amanufacturing%20processes.%20Traditional%20AFR%20methods%2C%20focused%20mainly%20on%20the%0Aidentification%20of%20subtractive%20%28machined%29%20features%20including%20holes%2C%20fillets%2C%0Achamfers%2C%20pockets%2C%20and%20slots%2C%20fail%20to%20recognize%20features%20pertinent%20to%20additive%0Amanufacturing.%20Furthermore%2C%20the%20traditional%20methods%20fall%20short%20in%20accurately%0Aextracting%20geometric%20dimensions%20and%20orientations%2C%20which%20are%20also%20key%20factors%0Afor%20effective%20manufacturing%20process%20planning.%20This%20paper%20presents%20a%20novel%0Aapproach%20for%20creating%20a%20synthetic%20CAD%20dataset%20that%20encompasses%20features%0Arelevant%20to%20both%20additive%20and%20subtractive%20machining%20through%20Python%20Open%0ACascade.%20The%20Hierarchical%20Graph%20Convolutional%20Neural%20Network%20%28HGCNN%29%20model%20is%0Aimplemented%20to%20accurately%20identify%20the%20composite%20additive-subtractive%20features%0Awithin%20the%20synthetic%20CAD%20dataset.%20The%20key%20novelty%20and%20contribution%20of%20the%0Aproposed%20methodology%20lie%20in%20its%20ability%20to%20recognize%20a%20wide%20range%20of%0Amanufacturing%20features%2C%20and%20precisely%20extracting%20their%20dimensions%2C%0Aorientations%2C%20and%20stock%20sizes.%20The%20proposed%20model%20demonstrates%20remarkable%0Afeature%20recognition%20accuracy%20exceeding%2097%25%20and%20a%20dimension%20extraction%20accuracy%0Aof%20100%25%20for%20identified%20features.%20Therefore%2C%20the%20proposed%20methodology%20enhances%0Athe%20integration%20of%20CAD%2C%20CAPP%2C%20and%20CAM%20within%20hybrid%20manufacturing%20by%20providing%0Aprecise%20feature%20recognition%20and%20dimension%20extraction.%20It%20facilitates%20improved%0Amanufacturing%20process%20planning%2C%20by%20enabling%20more%20informed%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06891v1&entry.124074799=Read"},
{"title": "Neural Speech and Audio Coding", "author": "Minje Kim and Jan Skoglund", "abstract": "  This paper explores the integration of model-based and data-driven approaches\nwithin the realm of neural speech and audio coding systems. It highlights the\nchallenges posed by the subjective evaluation processes of speech and audio\ncodecs and discusses the limitations of purely data-driven approaches, which\noften require inefficiently large architectures to match the performance of\nmodel-based methods. The study presents hybrid systems as a viable solution,\noffering significant improvements to the performance of conventional codecs\nthrough meticulously chosen design enhancements. Specifically, it introduces a\nneural network-based signal enhancer designed to post-process existing codecs'\noutput, along with the autoencoder-based end-to-end models and LPCNet--hybrid\nsystems that combine linear predictive coding (LPC) with neural networks.\nFurthermore, the paper delves into predictive models operating within custom\nfeature spaces (TF-Codec) or predefined transform domains (MDCTNet) and\nexamines the use of psychoacoustically calibrated loss functions to train\nend-to-end neural audio codecs. Through these investigations, the paper\ndemonstrates the potential of hybrid systems to advance the field of speech and\naudio coding by bridging the gap between traditional model-based approaches and\nmodern data-driven techniques.\n", "link": "http://arxiv.org/abs/2408.06954v1", "date": "2024-08-13", "relevancy": 1.9918, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5153}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4873}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Speech%20and%20Audio%20Coding&body=Title%3A%20Neural%20Speech%20and%20Audio%20Coding%0AAuthor%3A%20Minje%20Kim%20and%20Jan%20Skoglund%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20integration%20of%20model-based%20and%20data-driven%20approaches%0Awithin%20the%20realm%20of%20neural%20speech%20and%20audio%20coding%20systems.%20It%20highlights%20the%0Achallenges%20posed%20by%20the%20subjective%20evaluation%20processes%20of%20speech%20and%20audio%0Acodecs%20and%20discusses%20the%20limitations%20of%20purely%20data-driven%20approaches%2C%20which%0Aoften%20require%20inefficiently%20large%20architectures%20to%20match%20the%20performance%20of%0Amodel-based%20methods.%20The%20study%20presents%20hybrid%20systems%20as%20a%20viable%20solution%2C%0Aoffering%20significant%20improvements%20to%20the%20performance%20of%20conventional%20codecs%0Athrough%20meticulously%20chosen%20design%20enhancements.%20Specifically%2C%20it%20introduces%20a%0Aneural%20network-based%20signal%20enhancer%20designed%20to%20post-process%20existing%20codecs%27%0Aoutput%2C%20along%20with%20the%20autoencoder-based%20end-to-end%20models%20and%20LPCNet--hybrid%0Asystems%20that%20combine%20linear%20predictive%20coding%20%28LPC%29%20with%20neural%20networks.%0AFurthermore%2C%20the%20paper%20delves%20into%20predictive%20models%20operating%20within%20custom%0Afeature%20spaces%20%28TF-Codec%29%20or%20predefined%20transform%20domains%20%28MDCTNet%29%20and%0Aexamines%20the%20use%20of%20psychoacoustically%20calibrated%20loss%20functions%20to%20train%0Aend-to-end%20neural%20audio%20codecs.%20Through%20these%20investigations%2C%20the%20paper%0Ademonstrates%20the%20potential%20of%20hybrid%20systems%20to%20advance%20the%20field%20of%20speech%20and%0Aaudio%20coding%20by%20bridging%20the%20gap%20between%20traditional%20model-based%20approaches%20and%0Amodern%20data-driven%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Speech%2520and%2520Audio%2520Coding%26entry.906535625%3DMinje%2520Kim%2520and%2520Jan%2520Skoglund%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520integration%2520of%2520model-based%2520and%2520data-driven%2520approaches%250Awithin%2520the%2520realm%2520of%2520neural%2520speech%2520and%2520audio%2520coding%2520systems.%2520It%2520highlights%2520the%250Achallenges%2520posed%2520by%2520the%2520subjective%2520evaluation%2520processes%2520of%2520speech%2520and%2520audio%250Acodecs%2520and%2520discusses%2520the%2520limitations%2520of%2520purely%2520data-driven%2520approaches%252C%2520which%250Aoften%2520require%2520inefficiently%2520large%2520architectures%2520to%2520match%2520the%2520performance%2520of%250Amodel-based%2520methods.%2520The%2520study%2520presents%2520hybrid%2520systems%2520as%2520a%2520viable%2520solution%252C%250Aoffering%2520significant%2520improvements%2520to%2520the%2520performance%2520of%2520conventional%2520codecs%250Athrough%2520meticulously%2520chosen%2520design%2520enhancements.%2520Specifically%252C%2520it%2520introduces%2520a%250Aneural%2520network-based%2520signal%2520enhancer%2520designed%2520to%2520post-process%2520existing%2520codecs%2527%250Aoutput%252C%2520along%2520with%2520the%2520autoencoder-based%2520end-to-end%2520models%2520and%2520LPCNet--hybrid%250Asystems%2520that%2520combine%2520linear%2520predictive%2520coding%2520%2528LPC%2529%2520with%2520neural%2520networks.%250AFurthermore%252C%2520the%2520paper%2520delves%2520into%2520predictive%2520models%2520operating%2520within%2520custom%250Afeature%2520spaces%2520%2528TF-Codec%2529%2520or%2520predefined%2520transform%2520domains%2520%2528MDCTNet%2529%2520and%250Aexamines%2520the%2520use%2520of%2520psychoacoustically%2520calibrated%2520loss%2520functions%2520to%2520train%250Aend-to-end%2520neural%2520audio%2520codecs.%2520Through%2520these%2520investigations%252C%2520the%2520paper%250Ademonstrates%2520the%2520potential%2520of%2520hybrid%2520systems%2520to%2520advance%2520the%2520field%2520of%2520speech%2520and%250Aaudio%2520coding%2520by%2520bridging%2520the%2520gap%2520between%2520traditional%2520model-based%2520approaches%2520and%250Amodern%2520data-driven%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Speech%20and%20Audio%20Coding&entry.906535625=Minje%20Kim%20and%20Jan%20Skoglund&entry.1292438233=%20%20This%20paper%20explores%20the%20integration%20of%20model-based%20and%20data-driven%20approaches%0Awithin%20the%20realm%20of%20neural%20speech%20and%20audio%20coding%20systems.%20It%20highlights%20the%0Achallenges%20posed%20by%20the%20subjective%20evaluation%20processes%20of%20speech%20and%20audio%0Acodecs%20and%20discusses%20the%20limitations%20of%20purely%20data-driven%20approaches%2C%20which%0Aoften%20require%20inefficiently%20large%20architectures%20to%20match%20the%20performance%20of%0Amodel-based%20methods.%20The%20study%20presents%20hybrid%20systems%20as%20a%20viable%20solution%2C%0Aoffering%20significant%20improvements%20to%20the%20performance%20of%20conventional%20codecs%0Athrough%20meticulously%20chosen%20design%20enhancements.%20Specifically%2C%20it%20introduces%20a%0Aneural%20network-based%20signal%20enhancer%20designed%20to%20post-process%20existing%20codecs%27%0Aoutput%2C%20along%20with%20the%20autoencoder-based%20end-to-end%20models%20and%20LPCNet--hybrid%0Asystems%20that%20combine%20linear%20predictive%20coding%20%28LPC%29%20with%20neural%20networks.%0AFurthermore%2C%20the%20paper%20delves%20into%20predictive%20models%20operating%20within%20custom%0Afeature%20spaces%20%28TF-Codec%29%20or%20predefined%20transform%20domains%20%28MDCTNet%29%20and%0Aexamines%20the%20use%20of%20psychoacoustically%20calibrated%20loss%20functions%20to%20train%0Aend-to-end%20neural%20audio%20codecs.%20Through%20these%20investigations%2C%20the%20paper%0Ademonstrates%20the%20potential%20of%20hybrid%20systems%20to%20advance%20the%20field%20of%20speech%20and%0Aaudio%20coding%20by%20bridging%20the%20gap%20between%20traditional%20model-based%20approaches%20and%0Amodern%20data-driven%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06954v1&entry.124074799=Read"},
{"title": "Optimal Bound for PCA with Outliers using Higher-Degree Voronoi Diagrams", "author": "Sajjad Hashemian and Mohammad Saeed Arvenaghi and Ebrahim Ardeshir-Larijani", "abstract": "  In this paper, we introduce new algorithms for Principal Component Analysis\n(PCA) with outliers. Utilizing techniques from computational geometry,\nspecifically higher-degree Voronoi diagrams, we navigate to the optimal\nsubspace for PCA even in the presence of outliers. This approach achieves an\noptimal solution with a time complexity of\n$n^{d+\\mathcal{O}(1)}\\text{poly}(n,d)$. Additionally, we present a randomized\nalgorithm with a complexity of $2^{\\mathcal{O}(r(d-r))} \\times \\text{poly}(n,\nd)$. This algorithm samples subspaces characterized in terms of a Grassmannian\nmanifold. By employing such sampling method, we ensure a high likelihood of\ncapturing the optimal subspace, with the success probability $(1 - \\delta)^T$.\nWhere $\\delta$ represents the probability that a sampled subspace does not\ncontain the optimal solution, and $T$ is the number of subspaces sampled,\nproportional to $2^{r(d-r)}$. Our use of higher-degree Voronoi diagrams and\nGrassmannian based sampling offers a clearer conceptual pathway and practical\nadvantages, particularly in handling large datasets or higher-dimensional\nsettings.\n", "link": "http://arxiv.org/abs/2408.06867v1", "date": "2024-08-13", "relevancy": 1.9884, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4069}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3935}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Bound%20for%20PCA%20with%20Outliers%20using%20Higher-Degree%20Voronoi%20Diagrams&body=Title%3A%20Optimal%20Bound%20for%20PCA%20with%20Outliers%20using%20Higher-Degree%20Voronoi%20Diagrams%0AAuthor%3A%20Sajjad%20Hashemian%20and%20Mohammad%20Saeed%20Arvenaghi%20and%20Ebrahim%20Ardeshir-Larijani%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20new%20algorithms%20for%20Principal%20Component%20Analysis%0A%28PCA%29%20with%20outliers.%20Utilizing%20techniques%20from%20computational%20geometry%2C%0Aspecifically%20higher-degree%20Voronoi%20diagrams%2C%20we%20navigate%20to%20the%20optimal%0Asubspace%20for%20PCA%20even%20in%20the%20presence%20of%20outliers.%20This%20approach%20achieves%20an%0Aoptimal%20solution%20with%20a%20time%20complexity%20of%0A%24n%5E%7Bd%2B%5Cmathcal%7BO%7D%281%29%7D%5Ctext%7Bpoly%7D%28n%2Cd%29%24.%20Additionally%2C%20we%20present%20a%20randomized%0Aalgorithm%20with%20a%20complexity%20of%20%242%5E%7B%5Cmathcal%7BO%7D%28r%28d-r%29%29%7D%20%5Ctimes%20%5Ctext%7Bpoly%7D%28n%2C%0Ad%29%24.%20This%20algorithm%20samples%20subspaces%20characterized%20in%20terms%20of%20a%20Grassmannian%0Amanifold.%20By%20employing%20such%20sampling%20method%2C%20we%20ensure%20a%20high%20likelihood%20of%0Acapturing%20the%20optimal%20subspace%2C%20with%20the%20success%20probability%20%24%281%20-%20%5Cdelta%29%5ET%24.%0AWhere%20%24%5Cdelta%24%20represents%20the%20probability%20that%20a%20sampled%20subspace%20does%20not%0Acontain%20the%20optimal%20solution%2C%20and%20%24T%24%20is%20the%20number%20of%20subspaces%20sampled%2C%0Aproportional%20to%20%242%5E%7Br%28d-r%29%7D%24.%20Our%20use%20of%20higher-degree%20Voronoi%20diagrams%20and%0AGrassmannian%20based%20sampling%20offers%20a%20clearer%20conceptual%20pathway%20and%20practical%0Aadvantages%2C%20particularly%20in%20handling%20large%20datasets%20or%20higher-dimensional%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Bound%2520for%2520PCA%2520with%2520Outliers%2520using%2520Higher-Degree%2520Voronoi%2520Diagrams%26entry.906535625%3DSajjad%2520Hashemian%2520and%2520Mohammad%2520Saeed%2520Arvenaghi%2520and%2520Ebrahim%2520Ardeshir-Larijani%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520new%2520algorithms%2520for%2520Principal%2520Component%2520Analysis%250A%2528PCA%2529%2520with%2520outliers.%2520Utilizing%2520techniques%2520from%2520computational%2520geometry%252C%250Aspecifically%2520higher-degree%2520Voronoi%2520diagrams%252C%2520we%2520navigate%2520to%2520the%2520optimal%250Asubspace%2520for%2520PCA%2520even%2520in%2520the%2520presence%2520of%2520outliers.%2520This%2520approach%2520achieves%2520an%250Aoptimal%2520solution%2520with%2520a%2520time%2520complexity%2520of%250A%2524n%255E%257Bd%252B%255Cmathcal%257BO%257D%25281%2529%257D%255Ctext%257Bpoly%257D%2528n%252Cd%2529%2524.%2520Additionally%252C%2520we%2520present%2520a%2520randomized%250Aalgorithm%2520with%2520a%2520complexity%2520of%2520%25242%255E%257B%255Cmathcal%257BO%257D%2528r%2528d-r%2529%2529%257D%2520%255Ctimes%2520%255Ctext%257Bpoly%257D%2528n%252C%250Ad%2529%2524.%2520This%2520algorithm%2520samples%2520subspaces%2520characterized%2520in%2520terms%2520of%2520a%2520Grassmannian%250Amanifold.%2520By%2520employing%2520such%2520sampling%2520method%252C%2520we%2520ensure%2520a%2520high%2520likelihood%2520of%250Acapturing%2520the%2520optimal%2520subspace%252C%2520with%2520the%2520success%2520probability%2520%2524%25281%2520-%2520%255Cdelta%2529%255ET%2524.%250AWhere%2520%2524%255Cdelta%2524%2520represents%2520the%2520probability%2520that%2520a%2520sampled%2520subspace%2520does%2520not%250Acontain%2520the%2520optimal%2520solution%252C%2520and%2520%2524T%2524%2520is%2520the%2520number%2520of%2520subspaces%2520sampled%252C%250Aproportional%2520to%2520%25242%255E%257Br%2528d-r%2529%257D%2524.%2520Our%2520use%2520of%2520higher-degree%2520Voronoi%2520diagrams%2520and%250AGrassmannian%2520based%2520sampling%2520offers%2520a%2520clearer%2520conceptual%2520pathway%2520and%2520practical%250Aadvantages%252C%2520particularly%2520in%2520handling%2520large%2520datasets%2520or%2520higher-dimensional%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Bound%20for%20PCA%20with%20Outliers%20using%20Higher-Degree%20Voronoi%20Diagrams&entry.906535625=Sajjad%20Hashemian%20and%20Mohammad%20Saeed%20Arvenaghi%20and%20Ebrahim%20Ardeshir-Larijani&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20new%20algorithms%20for%20Principal%20Component%20Analysis%0A%28PCA%29%20with%20outliers.%20Utilizing%20techniques%20from%20computational%20geometry%2C%0Aspecifically%20higher-degree%20Voronoi%20diagrams%2C%20we%20navigate%20to%20the%20optimal%0Asubspace%20for%20PCA%20even%20in%20the%20presence%20of%20outliers.%20This%20approach%20achieves%20an%0Aoptimal%20solution%20with%20a%20time%20complexity%20of%0A%24n%5E%7Bd%2B%5Cmathcal%7BO%7D%281%29%7D%5Ctext%7Bpoly%7D%28n%2Cd%29%24.%20Additionally%2C%20we%20present%20a%20randomized%0Aalgorithm%20with%20a%20complexity%20of%20%242%5E%7B%5Cmathcal%7BO%7D%28r%28d-r%29%29%7D%20%5Ctimes%20%5Ctext%7Bpoly%7D%28n%2C%0Ad%29%24.%20This%20algorithm%20samples%20subspaces%20characterized%20in%20terms%20of%20a%20Grassmannian%0Amanifold.%20By%20employing%20such%20sampling%20method%2C%20we%20ensure%20a%20high%20likelihood%20of%0Acapturing%20the%20optimal%20subspace%2C%20with%20the%20success%20probability%20%24%281%20-%20%5Cdelta%29%5ET%24.%0AWhere%20%24%5Cdelta%24%20represents%20the%20probability%20that%20a%20sampled%20subspace%20does%20not%0Acontain%20the%20optimal%20solution%2C%20and%20%24T%24%20is%20the%20number%20of%20subspaces%20sampled%2C%0Aproportional%20to%20%242%5E%7Br%28d-r%29%7D%24.%20Our%20use%20of%20higher-degree%20Voronoi%20diagrams%20and%0AGrassmannian%20based%20sampling%20offers%20a%20clearer%20conceptual%20pathway%20and%20practical%0Aadvantages%2C%20particularly%20in%20handling%20large%20datasets%20or%20higher-dimensional%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06867v1&entry.124074799=Read"},
{"title": "BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning", "author": "Yuyang Xue and Junyu Yan and Raman Dutt and Fasih Haider and Jingshuai Liu and Steven McDonagh and Sotirios A. Tsaftaris", "abstract": "  Developing models with robust group fairness properties is paramount,\nparticularly in ethically sensitive domains such as medical diagnosis. Recent\napproaches to achieving fairness in machine learning require a substantial\namount of training data and depend on model retraining, which may not be\npractical in real-world scenarios. To mitigate these challenges, we propose\nBias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method\nthat enhances the fairness of a trained model in significantly fewer epochs\nwithout requiring access to the original training data. BMFT produces a mask\nover model parameters, which efficiently identifies the weights contributing\nthe most towards biased predictions. Furthermore, we propose a two-step\ndebiasing strategy, wherein the feature extractor undergoes initial fine-tuning\non the identified bias-influenced weights, succeeded by a fine-tuning phase on\na reinitialised classification layer to uphold discriminative performance.\nExtensive experiments across four dermatological datasets and two sensitive\nattributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA)\ntechniques in both diagnostic accuracy and fairness metrics. Our findings\nunderscore the efficacy and robustness of BMFT in advancing fairness across\nvarious out-of-distribution (OOD) settings. Our code is available at:\nhttps://github.com/vios-s/BMFT\n", "link": "http://arxiv.org/abs/2408.06890v1", "date": "2024-08-13", "relevancy": 1.9821, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5216}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5001}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BMFT%3A%20Achieving%20Fairness%20via%20Bias-based%20Weight%20Masking%20Fine-tuning&body=Title%3A%20BMFT%3A%20Achieving%20Fairness%20via%20Bias-based%20Weight%20Masking%20Fine-tuning%0AAuthor%3A%20Yuyang%20Xue%20and%20Junyu%20Yan%20and%20Raman%20Dutt%20and%20Fasih%20Haider%20and%20Jingshuai%20Liu%20and%20Steven%20McDonagh%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20%20%20Developing%20models%20with%20robust%20group%20fairness%20properties%20is%20paramount%2C%0Aparticularly%20in%20ethically%20sensitive%20domains%20such%20as%20medical%20diagnosis.%20Recent%0Aapproaches%20to%20achieving%20fairness%20in%20machine%20learning%20require%20a%20substantial%0Aamount%20of%20training%20data%20and%20depend%20on%20model%20retraining%2C%20which%20may%20not%20be%0Apractical%20in%20real-world%20scenarios.%20To%20mitigate%20these%20challenges%2C%20we%20propose%0ABias-based%20Weight%20Masking%20Fine-Tuning%20%28BMFT%29%2C%20a%20novel%20post-processing%20method%0Athat%20enhances%20the%20fairness%20of%20a%20trained%20model%20in%20significantly%20fewer%20epochs%0Awithout%20requiring%20access%20to%20the%20original%20training%20data.%20BMFT%20produces%20a%20mask%0Aover%20model%20parameters%2C%20which%20efficiently%20identifies%20the%20weights%20contributing%0Athe%20most%20towards%20biased%20predictions.%20Furthermore%2C%20we%20propose%20a%20two-step%0Adebiasing%20strategy%2C%20wherein%20the%20feature%20extractor%20undergoes%20initial%20fine-tuning%0Aon%20the%20identified%20bias-influenced%20weights%2C%20succeeded%20by%20a%20fine-tuning%20phase%20on%0Aa%20reinitialised%20classification%20layer%20to%20uphold%20discriminative%20performance.%0AExtensive%20experiments%20across%20four%20dermatological%20datasets%20and%20two%20sensitive%0Aattributes%20demonstrate%20that%20BMFT%20outperforms%20existing%20state-of-the-art%20%28SOTA%29%0Atechniques%20in%20both%20diagnostic%20accuracy%20and%20fairness%20metrics.%20Our%20findings%0Aunderscore%20the%20efficacy%20and%20robustness%20of%20BMFT%20in%20advancing%20fairness%20across%0Avarious%20out-of-distribution%20%28OOD%29%20settings.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/vios-s/BMFT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBMFT%253A%2520Achieving%2520Fairness%2520via%2520Bias-based%2520Weight%2520Masking%2520Fine-tuning%26entry.906535625%3DYuyang%2520Xue%2520and%2520Junyu%2520Yan%2520and%2520Raman%2520Dutt%2520and%2520Fasih%2520Haider%2520and%2520Jingshuai%2520Liu%2520and%2520Steven%2520McDonagh%2520and%2520Sotirios%2520A.%2520Tsaftaris%26entry.1292438233%3D%2520%2520Developing%2520models%2520with%2520robust%2520group%2520fairness%2520properties%2520is%2520paramount%252C%250Aparticularly%2520in%2520ethically%2520sensitive%2520domains%2520such%2520as%2520medical%2520diagnosis.%2520Recent%250Aapproaches%2520to%2520achieving%2520fairness%2520in%2520machine%2520learning%2520require%2520a%2520substantial%250Aamount%2520of%2520training%2520data%2520and%2520depend%2520on%2520model%2520retraining%252C%2520which%2520may%2520not%2520be%250Apractical%2520in%2520real-world%2520scenarios.%2520To%2520mitigate%2520these%2520challenges%252C%2520we%2520propose%250ABias-based%2520Weight%2520Masking%2520Fine-Tuning%2520%2528BMFT%2529%252C%2520a%2520novel%2520post-processing%2520method%250Athat%2520enhances%2520the%2520fairness%2520of%2520a%2520trained%2520model%2520in%2520significantly%2520fewer%2520epochs%250Awithout%2520requiring%2520access%2520to%2520the%2520original%2520training%2520data.%2520BMFT%2520produces%2520a%2520mask%250Aover%2520model%2520parameters%252C%2520which%2520efficiently%2520identifies%2520the%2520weights%2520contributing%250Athe%2520most%2520towards%2520biased%2520predictions.%2520Furthermore%252C%2520we%2520propose%2520a%2520two-step%250Adebiasing%2520strategy%252C%2520wherein%2520the%2520feature%2520extractor%2520undergoes%2520initial%2520fine-tuning%250Aon%2520the%2520identified%2520bias-influenced%2520weights%252C%2520succeeded%2520by%2520a%2520fine-tuning%2520phase%2520on%250Aa%2520reinitialised%2520classification%2520layer%2520to%2520uphold%2520discriminative%2520performance.%250AExtensive%2520experiments%2520across%2520four%2520dermatological%2520datasets%2520and%2520two%2520sensitive%250Aattributes%2520demonstrate%2520that%2520BMFT%2520outperforms%2520existing%2520state-of-the-art%2520%2528SOTA%2529%250Atechniques%2520in%2520both%2520diagnostic%2520accuracy%2520and%2520fairness%2520metrics.%2520Our%2520findings%250Aunderscore%2520the%2520efficacy%2520and%2520robustness%2520of%2520BMFT%2520in%2520advancing%2520fairness%2520across%250Avarious%2520out-of-distribution%2520%2528OOD%2529%2520settings.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/vios-s/BMFT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BMFT%3A%20Achieving%20Fairness%20via%20Bias-based%20Weight%20Masking%20Fine-tuning&entry.906535625=Yuyang%20Xue%20and%20Junyu%20Yan%20and%20Raman%20Dutt%20and%20Fasih%20Haider%20and%20Jingshuai%20Liu%20and%20Steven%20McDonagh%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=%20%20Developing%20models%20with%20robust%20group%20fairness%20properties%20is%20paramount%2C%0Aparticularly%20in%20ethically%20sensitive%20domains%20such%20as%20medical%20diagnosis.%20Recent%0Aapproaches%20to%20achieving%20fairness%20in%20machine%20learning%20require%20a%20substantial%0Aamount%20of%20training%20data%20and%20depend%20on%20model%20retraining%2C%20which%20may%20not%20be%0Apractical%20in%20real-world%20scenarios.%20To%20mitigate%20these%20challenges%2C%20we%20propose%0ABias-based%20Weight%20Masking%20Fine-Tuning%20%28BMFT%29%2C%20a%20novel%20post-processing%20method%0Athat%20enhances%20the%20fairness%20of%20a%20trained%20model%20in%20significantly%20fewer%20epochs%0Awithout%20requiring%20access%20to%20the%20original%20training%20data.%20BMFT%20produces%20a%20mask%0Aover%20model%20parameters%2C%20which%20efficiently%20identifies%20the%20weights%20contributing%0Athe%20most%20towards%20biased%20predictions.%20Furthermore%2C%20we%20propose%20a%20two-step%0Adebiasing%20strategy%2C%20wherein%20the%20feature%20extractor%20undergoes%20initial%20fine-tuning%0Aon%20the%20identified%20bias-influenced%20weights%2C%20succeeded%20by%20a%20fine-tuning%20phase%20on%0Aa%20reinitialised%20classification%20layer%20to%20uphold%20discriminative%20performance.%0AExtensive%20experiments%20across%20four%20dermatological%20datasets%20and%20two%20sensitive%0Aattributes%20demonstrate%20that%20BMFT%20outperforms%20existing%20state-of-the-art%20%28SOTA%29%0Atechniques%20in%20both%20diagnostic%20accuracy%20and%20fairness%20metrics.%20Our%20findings%0Aunderscore%20the%20efficacy%20and%20robustness%20of%20BMFT%20in%20advancing%20fairness%20across%0Avarious%20out-of-distribution%20%28OOD%29%20settings.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/vios-s/BMFT%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06890v1&entry.124074799=Read"},
{"title": "Deepfake Media Forensics: State of the Art and Challenges Ahead", "author": "Irene Amerini and Mauro Barni and Sebastiano Battiato and Paolo Bestagini and Giulia Boato and Tania Sari Bonaventura and Vittoria Bruni and Roberto Caldelli and Francesco De Natale and Rocco De Nicola and Luca Guarnera and Sara Mandelli and Gian Luca Marcialis and Marco Micheletto and Andrea Montibeller and Giulia Orru' and Alessandro Ortis and Pericle Perazzo and Giovanni Puglisi and Davide Salvi and Stefano Tubaro and Claudia Melis Tonti and Massimo Villari and Domenico Vitulano", "abstract": "  AI-generated synthetic media, also called Deepfakes, have significantly\ninfluenced so many domains, from entertainment to cybersecurity. Generative\nAdversarial Networks (GANs) and Diffusion Models (DMs) are the main frameworks\nused to create Deepfakes, producing highly realistic yet fabricated content.\nWhile these technologies open up new creative possibilities, they also bring\nsubstantial ethical and security risks due to their potential misuse. The rise\nof such advanced media has led to the development of a cognitive bias known as\nImpostor Bias, where individuals doubt the authenticity of multimedia due to\nthe awareness of AI's capabilities. As a result, Deepfake detection has become\na vital area of research, focusing on identifying subtle inconsistencies and\nartifacts with machine learning techniques, especially Convolutional Neural\nNetworks (CNNs). Research in forensic Deepfake technology encompasses five main\nareas: detection, attribution and recognition, passive authentication,\ndetection in realistic scenarios, and active authentication. This paper reviews\nthe primary algorithms that address these challenges, examining their\nadvantages, limitations, and future prospects.\n", "link": "http://arxiv.org/abs/2408.00388v2", "date": "2024-08-13", "relevancy": 1.9809, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5075}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4883}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deepfake%20Media%20Forensics%3A%20State%20of%20the%20Art%20and%20Challenges%20Ahead&body=Title%3A%20Deepfake%20Media%20Forensics%3A%20State%20of%20the%20Art%20and%20Challenges%20Ahead%0AAuthor%3A%20Irene%20Amerini%20and%20Mauro%20Barni%20and%20Sebastiano%20Battiato%20and%20Paolo%20Bestagini%20and%20Giulia%20Boato%20and%20Tania%20Sari%20Bonaventura%20and%20Vittoria%20Bruni%20and%20Roberto%20Caldelli%20and%20Francesco%20De%20Natale%20and%20Rocco%20De%20Nicola%20and%20Luca%20Guarnera%20and%20Sara%20Mandelli%20and%20Gian%20Luca%20Marcialis%20and%20Marco%20Micheletto%20and%20Andrea%20Montibeller%20and%20Giulia%20Orru%27%20and%20Alessandro%20Ortis%20and%20Pericle%20Perazzo%20and%20Giovanni%20Puglisi%20and%20Davide%20Salvi%20and%20Stefano%20Tubaro%20and%20Claudia%20Melis%20Tonti%20and%20Massimo%20Villari%20and%20Domenico%20Vitulano%0AAbstract%3A%20%20%20AI-generated%20synthetic%20media%2C%20also%20called%20Deepfakes%2C%20have%20significantly%0Ainfluenced%20so%20many%20domains%2C%20from%20entertainment%20to%20cybersecurity.%20Generative%0AAdversarial%20Networks%20%28GANs%29%20and%20Diffusion%20Models%20%28DMs%29%20are%20the%20main%20frameworks%0Aused%20to%20create%20Deepfakes%2C%20producing%20highly%20realistic%20yet%20fabricated%20content.%0AWhile%20these%20technologies%20open%20up%20new%20creative%20possibilities%2C%20they%20also%20bring%0Asubstantial%20ethical%20and%20security%20risks%20due%20to%20their%20potential%20misuse.%20The%20rise%0Aof%20such%20advanced%20media%20has%20led%20to%20the%20development%20of%20a%20cognitive%20bias%20known%20as%0AImpostor%20Bias%2C%20where%20individuals%20doubt%20the%20authenticity%20of%20multimedia%20due%20to%0Athe%20awareness%20of%20AI%27s%20capabilities.%20As%20a%20result%2C%20Deepfake%20detection%20has%20become%0Aa%20vital%20area%20of%20research%2C%20focusing%20on%20identifying%20subtle%20inconsistencies%20and%0Aartifacts%20with%20machine%20learning%20techniques%2C%20especially%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20Research%20in%20forensic%20Deepfake%20technology%20encompasses%20five%20main%0Aareas%3A%20detection%2C%20attribution%20and%20recognition%2C%20passive%20authentication%2C%0Adetection%20in%20realistic%20scenarios%2C%20and%20active%20authentication.%20This%20paper%20reviews%0Athe%20primary%20algorithms%20that%20address%20these%20challenges%2C%20examining%20their%0Aadvantages%2C%20limitations%2C%20and%20future%20prospects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepfake%2520Media%2520Forensics%253A%2520State%2520of%2520the%2520Art%2520and%2520Challenges%2520Ahead%26entry.906535625%3DIrene%2520Amerini%2520and%2520Mauro%2520Barni%2520and%2520Sebastiano%2520Battiato%2520and%2520Paolo%2520Bestagini%2520and%2520Giulia%2520Boato%2520and%2520Tania%2520Sari%2520Bonaventura%2520and%2520Vittoria%2520Bruni%2520and%2520Roberto%2520Caldelli%2520and%2520Francesco%2520De%2520Natale%2520and%2520Rocco%2520De%2520Nicola%2520and%2520Luca%2520Guarnera%2520and%2520Sara%2520Mandelli%2520and%2520Gian%2520Luca%2520Marcialis%2520and%2520Marco%2520Micheletto%2520and%2520Andrea%2520Montibeller%2520and%2520Giulia%2520Orru%2527%2520and%2520Alessandro%2520Ortis%2520and%2520Pericle%2520Perazzo%2520and%2520Giovanni%2520Puglisi%2520and%2520Davide%2520Salvi%2520and%2520Stefano%2520Tubaro%2520and%2520Claudia%2520Melis%2520Tonti%2520and%2520Massimo%2520Villari%2520and%2520Domenico%2520Vitulano%26entry.1292438233%3D%2520%2520AI-generated%2520synthetic%2520media%252C%2520also%2520called%2520Deepfakes%252C%2520have%2520significantly%250Ainfluenced%2520so%2520many%2520domains%252C%2520from%2520entertainment%2520to%2520cybersecurity.%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%2520and%2520Diffusion%2520Models%2520%2528DMs%2529%2520are%2520the%2520main%2520frameworks%250Aused%2520to%2520create%2520Deepfakes%252C%2520producing%2520highly%2520realistic%2520yet%2520fabricated%2520content.%250AWhile%2520these%2520technologies%2520open%2520up%2520new%2520creative%2520possibilities%252C%2520they%2520also%2520bring%250Asubstantial%2520ethical%2520and%2520security%2520risks%2520due%2520to%2520their%2520potential%2520misuse.%2520The%2520rise%250Aof%2520such%2520advanced%2520media%2520has%2520led%2520to%2520the%2520development%2520of%2520a%2520cognitive%2520bias%2520known%2520as%250AImpostor%2520Bias%252C%2520where%2520individuals%2520doubt%2520the%2520authenticity%2520of%2520multimedia%2520due%2520to%250Athe%2520awareness%2520of%2520AI%2527s%2520capabilities.%2520As%2520a%2520result%252C%2520Deepfake%2520detection%2520has%2520become%250Aa%2520vital%2520area%2520of%2520research%252C%2520focusing%2520on%2520identifying%2520subtle%2520inconsistencies%2520and%250Aartifacts%2520with%2520machine%2520learning%2520techniques%252C%2520especially%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529.%2520Research%2520in%2520forensic%2520Deepfake%2520technology%2520encompasses%2520five%2520main%250Aareas%253A%2520detection%252C%2520attribution%2520and%2520recognition%252C%2520passive%2520authentication%252C%250Adetection%2520in%2520realistic%2520scenarios%252C%2520and%2520active%2520authentication.%2520This%2520paper%2520reviews%250Athe%2520primary%2520algorithms%2520that%2520address%2520these%2520challenges%252C%2520examining%2520their%250Aadvantages%252C%2520limitations%252C%2520and%2520future%2520prospects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20Media%20Forensics%3A%20State%20of%20the%20Art%20and%20Challenges%20Ahead&entry.906535625=Irene%20Amerini%20and%20Mauro%20Barni%20and%20Sebastiano%20Battiato%20and%20Paolo%20Bestagini%20and%20Giulia%20Boato%20and%20Tania%20Sari%20Bonaventura%20and%20Vittoria%20Bruni%20and%20Roberto%20Caldelli%20and%20Francesco%20De%20Natale%20and%20Rocco%20De%20Nicola%20and%20Luca%20Guarnera%20and%20Sara%20Mandelli%20and%20Gian%20Luca%20Marcialis%20and%20Marco%20Micheletto%20and%20Andrea%20Montibeller%20and%20Giulia%20Orru%27%20and%20Alessandro%20Ortis%20and%20Pericle%20Perazzo%20and%20Giovanni%20Puglisi%20and%20Davide%20Salvi%20and%20Stefano%20Tubaro%20and%20Claudia%20Melis%20Tonti%20and%20Massimo%20Villari%20and%20Domenico%20Vitulano&entry.1292438233=%20%20AI-generated%20synthetic%20media%2C%20also%20called%20Deepfakes%2C%20have%20significantly%0Ainfluenced%20so%20many%20domains%2C%20from%20entertainment%20to%20cybersecurity.%20Generative%0AAdversarial%20Networks%20%28GANs%29%20and%20Diffusion%20Models%20%28DMs%29%20are%20the%20main%20frameworks%0Aused%20to%20create%20Deepfakes%2C%20producing%20highly%20realistic%20yet%20fabricated%20content.%0AWhile%20these%20technologies%20open%20up%20new%20creative%20possibilities%2C%20they%20also%20bring%0Asubstantial%20ethical%20and%20security%20risks%20due%20to%20their%20potential%20misuse.%20The%20rise%0Aof%20such%20advanced%20media%20has%20led%20to%20the%20development%20of%20a%20cognitive%20bias%20known%20as%0AImpostor%20Bias%2C%20where%20individuals%20doubt%20the%20authenticity%20of%20multimedia%20due%20to%0Athe%20awareness%20of%20AI%27s%20capabilities.%20As%20a%20result%2C%20Deepfake%20detection%20has%20become%0Aa%20vital%20area%20of%20research%2C%20focusing%20on%20identifying%20subtle%20inconsistencies%20and%0Aartifacts%20with%20machine%20learning%20techniques%2C%20especially%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20Research%20in%20forensic%20Deepfake%20technology%20encompasses%20five%20main%0Aareas%3A%20detection%2C%20attribution%20and%20recognition%2C%20passive%20authentication%2C%0Adetection%20in%20realistic%20scenarios%2C%20and%20active%20authentication.%20This%20paper%20reviews%0Athe%20primary%20algorithms%20that%20address%20these%20challenges%2C%20examining%20their%0Aadvantages%2C%20limitations%2C%20and%20future%20prospects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00388v2&entry.124074799=Read"},
{"title": "Enhancing Multiview Synergy: Robust Learning by Exploiting the Wave Loss\n  Function with Consensus and Complementarity Principles", "author": "A. Quadir and Mushir Akhtar and M. Tanveer", "abstract": "  Multiview learning (MvL) is an advancing domain in machine learning,\nleveraging multiple data perspectives to enhance model performance through\nview-consistency and view-discrepancy. Despite numerous successful\nmultiview-based SVM models, existing frameworks predominantly focus on the\nconsensus principle, often overlooking the complementarity principle.\nFurthermore, they exhibit limited robustness against noisy, error-prone, and\nview-inconsistent samples, prevalent in multiview datasets. To tackle the\naforementioned limitations, this paper introduces Wave-MvSVM, a novel multiview\nsupport vector machine framework leveraging the wave loss (W-loss) function,\nspecifically designed to harness both consensus and complementarity principles.\nUnlike traditional approaches that often overlook the complementary information\namong different views, the proposed Wave-MvSVM ensures a more comprehensive and\nresilient learning process by integrating both principles effectively. The\nW-loss function, characterized by its smoothness, asymmetry, and bounded\nnature, is particularly effective in mitigating the adverse effects of noisy\nand outlier data, thereby enhancing model stability. Theoretically, the W-loss\nfunction also exhibits a crucial classification-calibrated property, further\nboosting its effectiveness. Wave-MvSVM employs a between-view co-regularization\nterm to enforce view consistency and utilizes an adaptive combination weight\nstrategy to maximize the discriminative power of each view. The optimization\nproblem is efficiently solved using a combination of GD and the ADMM, ensuring\nreliable convergence to optimal solutions. Theoretical analyses, grounded in\nRademacher complexity, validate the generalization capabilities of the\nWave-MvSVM model. Extensive empirical evaluations across diverse datasets\ndemonstrate the superior performance of Wave-MvSVM in comparison to existing\nbenchmark models.\n", "link": "http://arxiv.org/abs/2408.06819v1", "date": "2024-08-13", "relevancy": 1.9809, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.502}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4923}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multiview%20Synergy%3A%20Robust%20Learning%20by%20Exploiting%20the%20Wave%20Loss%0A%20%20Function%20with%20Consensus%20and%20Complementarity%20Principles&body=Title%3A%20Enhancing%20Multiview%20Synergy%3A%20Robust%20Learning%20by%20Exploiting%20the%20Wave%20Loss%0A%20%20Function%20with%20Consensus%20and%20Complementarity%20Principles%0AAuthor%3A%20A.%20Quadir%20and%20Mushir%20Akhtar%20and%20M.%20Tanveer%0AAbstract%3A%20%20%20Multiview%20learning%20%28MvL%29%20is%20an%20advancing%20domain%20in%20machine%20learning%2C%0Aleveraging%20multiple%20data%20perspectives%20to%20enhance%20model%20performance%20through%0Aview-consistency%20and%20view-discrepancy.%20Despite%20numerous%20successful%0Amultiview-based%20SVM%20models%2C%20existing%20frameworks%20predominantly%20focus%20on%20the%0Aconsensus%20principle%2C%20often%20overlooking%20the%20complementarity%20principle.%0AFurthermore%2C%20they%20exhibit%20limited%20robustness%20against%20noisy%2C%20error-prone%2C%20and%0Aview-inconsistent%20samples%2C%20prevalent%20in%20multiview%20datasets.%20To%20tackle%20the%0Aaforementioned%20limitations%2C%20this%20paper%20introduces%20Wave-MvSVM%2C%20a%20novel%20multiview%0Asupport%20vector%20machine%20framework%20leveraging%20the%20wave%20loss%20%28W-loss%29%20function%2C%0Aspecifically%20designed%20to%20harness%20both%20consensus%20and%20complementarity%20principles.%0AUnlike%20traditional%20approaches%20that%20often%20overlook%20the%20complementary%20information%0Aamong%20different%20views%2C%20the%20proposed%20Wave-MvSVM%20ensures%20a%20more%20comprehensive%20and%0Aresilient%20learning%20process%20by%20integrating%20both%20principles%20effectively.%20The%0AW-loss%20function%2C%20characterized%20by%20its%20smoothness%2C%20asymmetry%2C%20and%20bounded%0Anature%2C%20is%20particularly%20effective%20in%20mitigating%20the%20adverse%20effects%20of%20noisy%0Aand%20outlier%20data%2C%20thereby%20enhancing%20model%20stability.%20Theoretically%2C%20the%20W-loss%0Afunction%20also%20exhibits%20a%20crucial%20classification-calibrated%20property%2C%20further%0Aboosting%20its%20effectiveness.%20Wave-MvSVM%20employs%20a%20between-view%20co-regularization%0Aterm%20to%20enforce%20view%20consistency%20and%20utilizes%20an%20adaptive%20combination%20weight%0Astrategy%20to%20maximize%20the%20discriminative%20power%20of%20each%20view.%20The%20optimization%0Aproblem%20is%20efficiently%20solved%20using%20a%20combination%20of%20GD%20and%20the%20ADMM%2C%20ensuring%0Areliable%20convergence%20to%20optimal%20solutions.%20Theoretical%20analyses%2C%20grounded%20in%0ARademacher%20complexity%2C%20validate%20the%20generalization%20capabilities%20of%20the%0AWave-MvSVM%20model.%20Extensive%20empirical%20evaluations%20across%20diverse%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20Wave-MvSVM%20in%20comparison%20to%20existing%0Abenchmark%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Multiview%2520Synergy%253A%2520Robust%2520Learning%2520by%2520Exploiting%2520the%2520Wave%2520Loss%250A%2520%2520Function%2520with%2520Consensus%2520and%2520Complementarity%2520Principles%26entry.906535625%3DA.%2520Quadir%2520and%2520Mushir%2520Akhtar%2520and%2520M.%2520Tanveer%26entry.1292438233%3D%2520%2520Multiview%2520learning%2520%2528MvL%2529%2520is%2520an%2520advancing%2520domain%2520in%2520machine%2520learning%252C%250Aleveraging%2520multiple%2520data%2520perspectives%2520to%2520enhance%2520model%2520performance%2520through%250Aview-consistency%2520and%2520view-discrepancy.%2520Despite%2520numerous%2520successful%250Amultiview-based%2520SVM%2520models%252C%2520existing%2520frameworks%2520predominantly%2520focus%2520on%2520the%250Aconsensus%2520principle%252C%2520often%2520overlooking%2520the%2520complementarity%2520principle.%250AFurthermore%252C%2520they%2520exhibit%2520limited%2520robustness%2520against%2520noisy%252C%2520error-prone%252C%2520and%250Aview-inconsistent%2520samples%252C%2520prevalent%2520in%2520multiview%2520datasets.%2520To%2520tackle%2520the%250Aaforementioned%2520limitations%252C%2520this%2520paper%2520introduces%2520Wave-MvSVM%252C%2520a%2520novel%2520multiview%250Asupport%2520vector%2520machine%2520framework%2520leveraging%2520the%2520wave%2520loss%2520%2528W-loss%2529%2520function%252C%250Aspecifically%2520designed%2520to%2520harness%2520both%2520consensus%2520and%2520complementarity%2520principles.%250AUnlike%2520traditional%2520approaches%2520that%2520often%2520overlook%2520the%2520complementary%2520information%250Aamong%2520different%2520views%252C%2520the%2520proposed%2520Wave-MvSVM%2520ensures%2520a%2520more%2520comprehensive%2520and%250Aresilient%2520learning%2520process%2520by%2520integrating%2520both%2520principles%2520effectively.%2520The%250AW-loss%2520function%252C%2520characterized%2520by%2520its%2520smoothness%252C%2520asymmetry%252C%2520and%2520bounded%250Anature%252C%2520is%2520particularly%2520effective%2520in%2520mitigating%2520the%2520adverse%2520effects%2520of%2520noisy%250Aand%2520outlier%2520data%252C%2520thereby%2520enhancing%2520model%2520stability.%2520Theoretically%252C%2520the%2520W-loss%250Afunction%2520also%2520exhibits%2520a%2520crucial%2520classification-calibrated%2520property%252C%2520further%250Aboosting%2520its%2520effectiveness.%2520Wave-MvSVM%2520employs%2520a%2520between-view%2520co-regularization%250Aterm%2520to%2520enforce%2520view%2520consistency%2520and%2520utilizes%2520an%2520adaptive%2520combination%2520weight%250Astrategy%2520to%2520maximize%2520the%2520discriminative%2520power%2520of%2520each%2520view.%2520The%2520optimization%250Aproblem%2520is%2520efficiently%2520solved%2520using%2520a%2520combination%2520of%2520GD%2520and%2520the%2520ADMM%252C%2520ensuring%250Areliable%2520convergence%2520to%2520optimal%2520solutions.%2520Theoretical%2520analyses%252C%2520grounded%2520in%250ARademacher%2520complexity%252C%2520validate%2520the%2520generalization%2520capabilities%2520of%2520the%250AWave-MvSVM%2520model.%2520Extensive%2520empirical%2520evaluations%2520across%2520diverse%2520datasets%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520Wave-MvSVM%2520in%2520comparison%2520to%2520existing%250Abenchmark%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multiview%20Synergy%3A%20Robust%20Learning%20by%20Exploiting%20the%20Wave%20Loss%0A%20%20Function%20with%20Consensus%20and%20Complementarity%20Principles&entry.906535625=A.%20Quadir%20and%20Mushir%20Akhtar%20and%20M.%20Tanveer&entry.1292438233=%20%20Multiview%20learning%20%28MvL%29%20is%20an%20advancing%20domain%20in%20machine%20learning%2C%0Aleveraging%20multiple%20data%20perspectives%20to%20enhance%20model%20performance%20through%0Aview-consistency%20and%20view-discrepancy.%20Despite%20numerous%20successful%0Amultiview-based%20SVM%20models%2C%20existing%20frameworks%20predominantly%20focus%20on%20the%0Aconsensus%20principle%2C%20often%20overlooking%20the%20complementarity%20principle.%0AFurthermore%2C%20they%20exhibit%20limited%20robustness%20against%20noisy%2C%20error-prone%2C%20and%0Aview-inconsistent%20samples%2C%20prevalent%20in%20multiview%20datasets.%20To%20tackle%20the%0Aaforementioned%20limitations%2C%20this%20paper%20introduces%20Wave-MvSVM%2C%20a%20novel%20multiview%0Asupport%20vector%20machine%20framework%20leveraging%20the%20wave%20loss%20%28W-loss%29%20function%2C%0Aspecifically%20designed%20to%20harness%20both%20consensus%20and%20complementarity%20principles.%0AUnlike%20traditional%20approaches%20that%20often%20overlook%20the%20complementary%20information%0Aamong%20different%20views%2C%20the%20proposed%20Wave-MvSVM%20ensures%20a%20more%20comprehensive%20and%0Aresilient%20learning%20process%20by%20integrating%20both%20principles%20effectively.%20The%0AW-loss%20function%2C%20characterized%20by%20its%20smoothness%2C%20asymmetry%2C%20and%20bounded%0Anature%2C%20is%20particularly%20effective%20in%20mitigating%20the%20adverse%20effects%20of%20noisy%0Aand%20outlier%20data%2C%20thereby%20enhancing%20model%20stability.%20Theoretically%2C%20the%20W-loss%0Afunction%20also%20exhibits%20a%20crucial%20classification-calibrated%20property%2C%20further%0Aboosting%20its%20effectiveness.%20Wave-MvSVM%20employs%20a%20between-view%20co-regularization%0Aterm%20to%20enforce%20view%20consistency%20and%20utilizes%20an%20adaptive%20combination%20weight%0Astrategy%20to%20maximize%20the%20discriminative%20power%20of%20each%20view.%20The%20optimization%0Aproblem%20is%20efficiently%20solved%20using%20a%20combination%20of%20GD%20and%20the%20ADMM%2C%20ensuring%0Areliable%20convergence%20to%20optimal%20solutions.%20Theoretical%20analyses%2C%20grounded%20in%0ARademacher%20complexity%2C%20validate%20the%20generalization%20capabilities%20of%20the%0AWave-MvSVM%20model.%20Extensive%20empirical%20evaluations%20across%20diverse%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20Wave-MvSVM%20in%20comparison%20to%20existing%0Abenchmark%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06819v1&entry.124074799=Read"},
{"title": "Personalized Dynamic Difficulty Adjustment -- Imitation Learning Meets\n  Reinforcement Learning", "author": "Ronja Fuchs and Robin Gieseke and Alexander Dockhorn", "abstract": "  Balancing game difficulty in video games is a key task to create interesting\ngaming experiences for players. Mismatching the game difficulty and a player's\nskill or commitment results in frustration or boredom on the player's side, and\nhence reduces time spent playing the game. In this work, we explore balancing\ngame difficulty using machine learning-based agents to challenge players based\non their current behavior. This is achieved by a combination of two agents, in\nwhich one learns to imitate the player, while the second is trained to beat the\nfirst. In our demo, we investigate the proposed framework for personalized\ndynamic difficulty adjustment of AI agents in the context of the fighting game\nAI competition.\n", "link": "http://arxiv.org/abs/2408.06818v1", "date": "2024-08-13", "relevancy": 1.9738, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4964}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Dynamic%20Difficulty%20Adjustment%20--%20Imitation%20Learning%20Meets%0A%20%20Reinforcement%20Learning&body=Title%3A%20Personalized%20Dynamic%20Difficulty%20Adjustment%20--%20Imitation%20Learning%20Meets%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ronja%20Fuchs%20and%20Robin%20Gieseke%20and%20Alexander%20Dockhorn%0AAbstract%3A%20%20%20Balancing%20game%20difficulty%20in%20video%20games%20is%20a%20key%20task%20to%20create%20interesting%0Agaming%20experiences%20for%20players.%20Mismatching%20the%20game%20difficulty%20and%20a%20player%27s%0Askill%20or%20commitment%20results%20in%20frustration%20or%20boredom%20on%20the%20player%27s%20side%2C%20and%0Ahence%20reduces%20time%20spent%20playing%20the%20game.%20In%20this%20work%2C%20we%20explore%20balancing%0Agame%20difficulty%20using%20machine%20learning-based%20agents%20to%20challenge%20players%20based%0Aon%20their%20current%20behavior.%20This%20is%20achieved%20by%20a%20combination%20of%20two%20agents%2C%20in%0Awhich%20one%20learns%20to%20imitate%20the%20player%2C%20while%20the%20second%20is%20trained%20to%20beat%20the%0Afirst.%20In%20our%20demo%2C%20we%20investigate%20the%20proposed%20framework%20for%20personalized%0Adynamic%20difficulty%20adjustment%20of%20AI%20agents%20in%20the%20context%20of%20the%20fighting%20game%0AAI%20competition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Dynamic%2520Difficulty%2520Adjustment%2520--%2520Imitation%2520Learning%2520Meets%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DRonja%2520Fuchs%2520and%2520Robin%2520Gieseke%2520and%2520Alexander%2520Dockhorn%26entry.1292438233%3D%2520%2520Balancing%2520game%2520difficulty%2520in%2520video%2520games%2520is%2520a%2520key%2520task%2520to%2520create%2520interesting%250Agaming%2520experiences%2520for%2520players.%2520Mismatching%2520the%2520game%2520difficulty%2520and%2520a%2520player%2527s%250Askill%2520or%2520commitment%2520results%2520in%2520frustration%2520or%2520boredom%2520on%2520the%2520player%2527s%2520side%252C%2520and%250Ahence%2520reduces%2520time%2520spent%2520playing%2520the%2520game.%2520In%2520this%2520work%252C%2520we%2520explore%2520balancing%250Agame%2520difficulty%2520using%2520machine%2520learning-based%2520agents%2520to%2520challenge%2520players%2520based%250Aon%2520their%2520current%2520behavior.%2520This%2520is%2520achieved%2520by%2520a%2520combination%2520of%2520two%2520agents%252C%2520in%250Awhich%2520one%2520learns%2520to%2520imitate%2520the%2520player%252C%2520while%2520the%2520second%2520is%2520trained%2520to%2520beat%2520the%250Afirst.%2520In%2520our%2520demo%252C%2520we%2520investigate%2520the%2520proposed%2520framework%2520for%2520personalized%250Adynamic%2520difficulty%2520adjustment%2520of%2520AI%2520agents%2520in%2520the%2520context%2520of%2520the%2520fighting%2520game%250AAI%2520competition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Dynamic%20Difficulty%20Adjustment%20--%20Imitation%20Learning%20Meets%0A%20%20Reinforcement%20Learning&entry.906535625=Ronja%20Fuchs%20and%20Robin%20Gieseke%20and%20Alexander%20Dockhorn&entry.1292438233=%20%20Balancing%20game%20difficulty%20in%20video%20games%20is%20a%20key%20task%20to%20create%20interesting%0Agaming%20experiences%20for%20players.%20Mismatching%20the%20game%20difficulty%20and%20a%20player%27s%0Askill%20or%20commitment%20results%20in%20frustration%20or%20boredom%20on%20the%20player%27s%20side%2C%20and%0Ahence%20reduces%20time%20spent%20playing%20the%20game.%20In%20this%20work%2C%20we%20explore%20balancing%0Agame%20difficulty%20using%20machine%20learning-based%20agents%20to%20challenge%20players%20based%0Aon%20their%20current%20behavior.%20This%20is%20achieved%20by%20a%20combination%20of%20two%20agents%2C%20in%0Awhich%20one%20learns%20to%20imitate%20the%20player%2C%20while%20the%20second%20is%20trained%20to%20beat%20the%0Afirst.%20In%20our%20demo%2C%20we%20investigate%20the%20proposed%20framework%20for%20personalized%0Adynamic%20difficulty%20adjustment%20of%20AI%20agents%20in%20the%20context%20of%20the%20fighting%20game%0AAI%20competition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06818v1&entry.124074799=Read"},
{"title": "Efficient Search for Customized Activation Functions with Gradient\n  Descent", "author": "Lukas Strack and Mahmoud Safari and Frank Hutter", "abstract": "  Different activation functions work best for different deep learning models.\nTo exploit this, we leverage recent advancements in gradient-based search\ntechniques for neural architectures to efficiently identify high-performing\nactivation functions for a given application. We propose a fine-grained search\ncell that combines basic mathematical operations to model activation functions,\nallowing for the exploration of novel activations. Our approach enables the\nidentification of specialized activations, leading to improved performance in\nevery model we tried, from image classification to language models. Moreover,\nthe identified activations exhibit strong transferability to larger models of\nthe same type, as well as new datasets. Importantly, our automated process for\ncreating customized activation functions is orders of magnitude more efficient\nthan previous approaches. It can easily be applied on top of arbitrary deep\nlearning pipelines and thus offers a promising practical avenue for enhancing\ndeep learning architectures.\n", "link": "http://arxiv.org/abs/2408.06820v1", "date": "2024-08-13", "relevancy": 1.973, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5076}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4857}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Search%20for%20Customized%20Activation%20Functions%20with%20Gradient%0A%20%20Descent&body=Title%3A%20Efficient%20Search%20for%20Customized%20Activation%20Functions%20with%20Gradient%0A%20%20Descent%0AAuthor%3A%20Lukas%20Strack%20and%20Mahmoud%20Safari%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20Different%20activation%20functions%20work%20best%20for%20different%20deep%20learning%20models.%0ATo%20exploit%20this%2C%20we%20leverage%20recent%20advancements%20in%20gradient-based%20search%0Atechniques%20for%20neural%20architectures%20to%20efficiently%20identify%20high-performing%0Aactivation%20functions%20for%20a%20given%20application.%20We%20propose%20a%20fine-grained%20search%0Acell%20that%20combines%20basic%20mathematical%20operations%20to%20model%20activation%20functions%2C%0Aallowing%20for%20the%20exploration%20of%20novel%20activations.%20Our%20approach%20enables%20the%0Aidentification%20of%20specialized%20activations%2C%20leading%20to%20improved%20performance%20in%0Aevery%20model%20we%20tried%2C%20from%20image%20classification%20to%20language%20models.%20Moreover%2C%0Athe%20identified%20activations%20exhibit%20strong%20transferability%20to%20larger%20models%20of%0Athe%20same%20type%2C%20as%20well%20as%20new%20datasets.%20Importantly%2C%20our%20automated%20process%20for%0Acreating%20customized%20activation%20functions%20is%20orders%20of%20magnitude%20more%20efficient%0Athan%20previous%20approaches.%20It%20can%20easily%20be%20applied%20on%20top%20of%20arbitrary%20deep%0Alearning%20pipelines%20and%20thus%20offers%20a%20promising%20practical%20avenue%20for%20enhancing%0Adeep%20learning%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Search%2520for%2520Customized%2520Activation%2520Functions%2520with%2520Gradient%250A%2520%2520Descent%26entry.906535625%3DLukas%2520Strack%2520and%2520Mahmoud%2520Safari%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520Different%2520activation%2520functions%2520work%2520best%2520for%2520different%2520deep%2520learning%2520models.%250ATo%2520exploit%2520this%252C%2520we%2520leverage%2520recent%2520advancements%2520in%2520gradient-based%2520search%250Atechniques%2520for%2520neural%2520architectures%2520to%2520efficiently%2520identify%2520high-performing%250Aactivation%2520functions%2520for%2520a%2520given%2520application.%2520We%2520propose%2520a%2520fine-grained%2520search%250Acell%2520that%2520combines%2520basic%2520mathematical%2520operations%2520to%2520model%2520activation%2520functions%252C%250Aallowing%2520for%2520the%2520exploration%2520of%2520novel%2520activations.%2520Our%2520approach%2520enables%2520the%250Aidentification%2520of%2520specialized%2520activations%252C%2520leading%2520to%2520improved%2520performance%2520in%250Aevery%2520model%2520we%2520tried%252C%2520from%2520image%2520classification%2520to%2520language%2520models.%2520Moreover%252C%250Athe%2520identified%2520activations%2520exhibit%2520strong%2520transferability%2520to%2520larger%2520models%2520of%250Athe%2520same%2520type%252C%2520as%2520well%2520as%2520new%2520datasets.%2520Importantly%252C%2520our%2520automated%2520process%2520for%250Acreating%2520customized%2520activation%2520functions%2520is%2520orders%2520of%2520magnitude%2520more%2520efficient%250Athan%2520previous%2520approaches.%2520It%2520can%2520easily%2520be%2520applied%2520on%2520top%2520of%2520arbitrary%2520deep%250Alearning%2520pipelines%2520and%2520thus%2520offers%2520a%2520promising%2520practical%2520avenue%2520for%2520enhancing%250Adeep%2520learning%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Search%20for%20Customized%20Activation%20Functions%20with%20Gradient%0A%20%20Descent&entry.906535625=Lukas%20Strack%20and%20Mahmoud%20Safari%20and%20Frank%20Hutter&entry.1292438233=%20%20Different%20activation%20functions%20work%20best%20for%20different%20deep%20learning%20models.%0ATo%20exploit%20this%2C%20we%20leverage%20recent%20advancements%20in%20gradient-based%20search%0Atechniques%20for%20neural%20architectures%20to%20efficiently%20identify%20high-performing%0Aactivation%20functions%20for%20a%20given%20application.%20We%20propose%20a%20fine-grained%20search%0Acell%20that%20combines%20basic%20mathematical%20operations%20to%20model%20activation%20functions%2C%0Aallowing%20for%20the%20exploration%20of%20novel%20activations.%20Our%20approach%20enables%20the%0Aidentification%20of%20specialized%20activations%2C%20leading%20to%20improved%20performance%20in%0Aevery%20model%20we%20tried%2C%20from%20image%20classification%20to%20language%20models.%20Moreover%2C%0Athe%20identified%20activations%20exhibit%20strong%20transferability%20to%20larger%20models%20of%0Athe%20same%20type%2C%20as%20well%20as%20new%20datasets.%20Importantly%2C%20our%20automated%20process%20for%0Acreating%20customized%20activation%20functions%20is%20orders%20of%20magnitude%20more%20efficient%0Athan%20previous%20approaches.%20It%20can%20easily%20be%20applied%20on%20top%20of%20arbitrary%20deep%0Alearning%20pipelines%20and%20thus%20offers%20a%20promising%20practical%20avenue%20for%20enhancing%0Adeep%20learning%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06820v1&entry.124074799=Read"},
{"title": "BSS-CFFMA: Cross-Domain Feature Fusion and Multi-Attention Speech\n  Enhancement Network based on Self-Supervised Embedding", "author": "Alimjan Mattursun and Liejun Wang and Yinfeng Yu", "abstract": "  Speech self-supervised learning (SSL) represents has achieved\nstate-of-the-art (SOTA) performance in multiple downstream tasks. However, its\napplication in speech enhancement (SE) tasks remains immature, offering\nopportunities for improvement. In this study, we introduce a novel cross-domain\nfeature fusion and multi-attention speech enhancement network, termed\nBSS-CFFMA, which leverages self-supervised embeddings. BSS-CFFMA comprises a\nmulti-scale cross-domain feature fusion (MSCFF) block and a residual hybrid\nmulti-attention (RHMA) block. The MSCFF block effectively integrates\ncross-domain features, facilitating the extraction of rich acoustic\ninformation. The RHMA block, serving as the primary enhancement module,\nutilizes three distinct attention modules to capture diverse attention\nrepresentations and estimate high-quality speech signals.\n  We evaluate the performance of the BSS-CFFMA model through comparative and\nablation studies on the VoiceBank-DEMAND dataset, achieving SOTA results.\nFurthermore, we select three types of data from the WHAMR! dataset, a\ncollection specifically designed for speech enhancement tasks, to assess the\ncapabilities of BSS-CFFMA in tasks such as denoising only, dereverberation\nonly, and simultaneous denoising and dereverberation. This study marks the\nfirst attempt to explore the effectiveness of self-supervised embedding-based\nspeech enhancement methods in complex tasks encompassing dereverberation and\nsimultaneous denoising and dereverberation. The demo implementation of\nBSS-CFFMA is available online\\footnote[2]{https://github.com/AlimMat/BSS-CFFMA.\n\\label{s1}}.\n", "link": "http://arxiv.org/abs/2408.06851v1", "date": "2024-08-13", "relevancy": 1.9669, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.49}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BSS-CFFMA%3A%20Cross-Domain%20Feature%20Fusion%20and%20Multi-Attention%20Speech%0A%20%20Enhancement%20Network%20based%20on%20Self-Supervised%20Embedding&body=Title%3A%20BSS-CFFMA%3A%20Cross-Domain%20Feature%20Fusion%20and%20Multi-Attention%20Speech%0A%20%20Enhancement%20Network%20based%20on%20Self-Supervised%20Embedding%0AAuthor%3A%20Alimjan%20Mattursun%20and%20Liejun%20Wang%20and%20Yinfeng%20Yu%0AAbstract%3A%20%20%20Speech%20self-supervised%20learning%20%28SSL%29%20represents%20has%20achieved%0Astate-of-the-art%20%28SOTA%29%20performance%20in%20multiple%20downstream%20tasks.%20However%2C%20its%0Aapplication%20in%20speech%20enhancement%20%28SE%29%20tasks%20remains%20immature%2C%20offering%0Aopportunities%20for%20improvement.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20cross-domain%0Afeature%20fusion%20and%20multi-attention%20speech%20enhancement%20network%2C%20termed%0ABSS-CFFMA%2C%20which%20leverages%20self-supervised%20embeddings.%20BSS-CFFMA%20comprises%20a%0Amulti-scale%20cross-domain%20feature%20fusion%20%28MSCFF%29%20block%20and%20a%20residual%20hybrid%0Amulti-attention%20%28RHMA%29%20block.%20The%20MSCFF%20block%20effectively%20integrates%0Across-domain%20features%2C%20facilitating%20the%20extraction%20of%20rich%20acoustic%0Ainformation.%20The%20RHMA%20block%2C%20serving%20as%20the%20primary%20enhancement%20module%2C%0Autilizes%20three%20distinct%20attention%20modules%20to%20capture%20diverse%20attention%0Arepresentations%20and%20estimate%20high-quality%20speech%20signals.%0A%20%20We%20evaluate%20the%20performance%20of%20the%20BSS-CFFMA%20model%20through%20comparative%20and%0Aablation%20studies%20on%20the%20VoiceBank-DEMAND%20dataset%2C%20achieving%20SOTA%20results.%0AFurthermore%2C%20we%20select%20three%20types%20of%20data%20from%20the%20WHAMR%21%20dataset%2C%20a%0Acollection%20specifically%20designed%20for%20speech%20enhancement%20tasks%2C%20to%20assess%20the%0Acapabilities%20of%20BSS-CFFMA%20in%20tasks%20such%20as%20denoising%20only%2C%20dereverberation%0Aonly%2C%20and%20simultaneous%20denoising%20and%20dereverberation.%20This%20study%20marks%20the%0Afirst%20attempt%20to%20explore%20the%20effectiveness%20of%20self-supervised%20embedding-based%0Aspeech%20enhancement%20methods%20in%20complex%20tasks%20encompassing%20dereverberation%20and%0Asimultaneous%20denoising%20and%20dereverberation.%20The%20demo%20implementation%20of%0ABSS-CFFMA%20is%20available%20online%5Cfootnote%5B2%5D%7Bhttps%3A//github.com/AlimMat/BSS-CFFMA.%0A%5Clabel%7Bs1%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBSS-CFFMA%253A%2520Cross-Domain%2520Feature%2520Fusion%2520and%2520Multi-Attention%2520Speech%250A%2520%2520Enhancement%2520Network%2520based%2520on%2520Self-Supervised%2520Embedding%26entry.906535625%3DAlimjan%2520Mattursun%2520and%2520Liejun%2520Wang%2520and%2520Yinfeng%2520Yu%26entry.1292438233%3D%2520%2520Speech%2520self-supervised%2520learning%2520%2528SSL%2529%2520represents%2520has%2520achieved%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520in%2520multiple%2520downstream%2520tasks.%2520However%252C%2520its%250Aapplication%2520in%2520speech%2520enhancement%2520%2528SE%2529%2520tasks%2520remains%2520immature%252C%2520offering%250Aopportunities%2520for%2520improvement.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520cross-domain%250Afeature%2520fusion%2520and%2520multi-attention%2520speech%2520enhancement%2520network%252C%2520termed%250ABSS-CFFMA%252C%2520which%2520leverages%2520self-supervised%2520embeddings.%2520BSS-CFFMA%2520comprises%2520a%250Amulti-scale%2520cross-domain%2520feature%2520fusion%2520%2528MSCFF%2529%2520block%2520and%2520a%2520residual%2520hybrid%250Amulti-attention%2520%2528RHMA%2529%2520block.%2520The%2520MSCFF%2520block%2520effectively%2520integrates%250Across-domain%2520features%252C%2520facilitating%2520the%2520extraction%2520of%2520rich%2520acoustic%250Ainformation.%2520The%2520RHMA%2520block%252C%2520serving%2520as%2520the%2520primary%2520enhancement%2520module%252C%250Autilizes%2520three%2520distinct%2520attention%2520modules%2520to%2520capture%2520diverse%2520attention%250Arepresentations%2520and%2520estimate%2520high-quality%2520speech%2520signals.%250A%2520%2520We%2520evaluate%2520the%2520performance%2520of%2520the%2520BSS-CFFMA%2520model%2520through%2520comparative%2520and%250Aablation%2520studies%2520on%2520the%2520VoiceBank-DEMAND%2520dataset%252C%2520achieving%2520SOTA%2520results.%250AFurthermore%252C%2520we%2520select%2520three%2520types%2520of%2520data%2520from%2520the%2520WHAMR%2521%2520dataset%252C%2520a%250Acollection%2520specifically%2520designed%2520for%2520speech%2520enhancement%2520tasks%252C%2520to%2520assess%2520the%250Acapabilities%2520of%2520BSS-CFFMA%2520in%2520tasks%2520such%2520as%2520denoising%2520only%252C%2520dereverberation%250Aonly%252C%2520and%2520simultaneous%2520denoising%2520and%2520dereverberation.%2520This%2520study%2520marks%2520the%250Afirst%2520attempt%2520to%2520explore%2520the%2520effectiveness%2520of%2520self-supervised%2520embedding-based%250Aspeech%2520enhancement%2520methods%2520in%2520complex%2520tasks%2520encompassing%2520dereverberation%2520and%250Asimultaneous%2520denoising%2520and%2520dereverberation.%2520The%2520demo%2520implementation%2520of%250ABSS-CFFMA%2520is%2520available%2520online%255Cfootnote%255B2%255D%257Bhttps%253A//github.com/AlimMat/BSS-CFFMA.%250A%255Clabel%257Bs1%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BSS-CFFMA%3A%20Cross-Domain%20Feature%20Fusion%20and%20Multi-Attention%20Speech%0A%20%20Enhancement%20Network%20based%20on%20Self-Supervised%20Embedding&entry.906535625=Alimjan%20Mattursun%20and%20Liejun%20Wang%20and%20Yinfeng%20Yu&entry.1292438233=%20%20Speech%20self-supervised%20learning%20%28SSL%29%20represents%20has%20achieved%0Astate-of-the-art%20%28SOTA%29%20performance%20in%20multiple%20downstream%20tasks.%20However%2C%20its%0Aapplication%20in%20speech%20enhancement%20%28SE%29%20tasks%20remains%20immature%2C%20offering%0Aopportunities%20for%20improvement.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20cross-domain%0Afeature%20fusion%20and%20multi-attention%20speech%20enhancement%20network%2C%20termed%0ABSS-CFFMA%2C%20which%20leverages%20self-supervised%20embeddings.%20BSS-CFFMA%20comprises%20a%0Amulti-scale%20cross-domain%20feature%20fusion%20%28MSCFF%29%20block%20and%20a%20residual%20hybrid%0Amulti-attention%20%28RHMA%29%20block.%20The%20MSCFF%20block%20effectively%20integrates%0Across-domain%20features%2C%20facilitating%20the%20extraction%20of%20rich%20acoustic%0Ainformation.%20The%20RHMA%20block%2C%20serving%20as%20the%20primary%20enhancement%20module%2C%0Autilizes%20three%20distinct%20attention%20modules%20to%20capture%20diverse%20attention%0Arepresentations%20and%20estimate%20high-quality%20speech%20signals.%0A%20%20We%20evaluate%20the%20performance%20of%20the%20BSS-CFFMA%20model%20through%20comparative%20and%0Aablation%20studies%20on%20the%20VoiceBank-DEMAND%20dataset%2C%20achieving%20SOTA%20results.%0AFurthermore%2C%20we%20select%20three%20types%20of%20data%20from%20the%20WHAMR%21%20dataset%2C%20a%0Acollection%20specifically%20designed%20for%20speech%20enhancement%20tasks%2C%20to%20assess%20the%0Acapabilities%20of%20BSS-CFFMA%20in%20tasks%20such%20as%20denoising%20only%2C%20dereverberation%0Aonly%2C%20and%20simultaneous%20denoising%20and%20dereverberation.%20This%20study%20marks%20the%0Afirst%20attempt%20to%20explore%20the%20effectiveness%20of%20self-supervised%20embedding-based%0Aspeech%20enhancement%20methods%20in%20complex%20tasks%20encompassing%20dereverberation%20and%0Asimultaneous%20denoising%20and%20dereverberation.%20The%20demo%20implementation%20of%0ABSS-CFFMA%20is%20available%20online%5Cfootnote%5B2%5D%7Bhttps%3A//github.com/AlimMat/BSS-CFFMA.%0A%5Clabel%7Bs1%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06851v1&entry.124074799=Read"},
{"title": "HADRON: Human-friendly Control and Artificial Intelligence for Military\n  Drone Operations", "author": "Ana M. Casado Faul\u00ed and Mario Malizia and Ken Hasselmann and Emile Le Fl\u00e9cher and Geert De Cubber and Ben Lauwens", "abstract": "  As drones are getting more and more entangled in our society, more untrained\nusers require the capability to operate them. This scenario is to be achieved\nthrough the development of artificial intelligence capabilities assisting the\nhuman operator in controlling the Unmanned Aerial System (UAS) and processing\nthe sensor data, thereby alleviating the need for extensive operator training.\nThis paper presents the HADRON project that seeks to develop and test multiple\nnovel technologies to enable human-friendly control of drone swarms. This\nproject is divided into three main parts. The first part consists of the\nintegration of different technologies for the intuitive control of drones,\nfocusing on novice or inexperienced pilots and operators. The second part\nfocuses on the development of a multi-drone system that will be controlled from\na command and control station, in which an expert pilot can supervise the\noperations of the multiple drones. The third part of the project will focus on\nreducing the cognitive load on human operators, whether they are novice or\nexpert pilots. For this, we will develop AI tools that will assist drone\noperators with semi-automated real-time data processing.\n", "link": "http://arxiv.org/abs/2408.07063v1", "date": "2024-08-13", "relevancy": 1.9636, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4996}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4929}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HADRON%3A%20Human-friendly%20Control%20and%20Artificial%20Intelligence%20for%20Military%0A%20%20Drone%20Operations&body=Title%3A%20HADRON%3A%20Human-friendly%20Control%20and%20Artificial%20Intelligence%20for%20Military%0A%20%20Drone%20Operations%0AAuthor%3A%20Ana%20M.%20Casado%20Faul%C3%AD%20and%20Mario%20Malizia%20and%20Ken%20Hasselmann%20and%20Emile%20Le%20Fl%C3%A9cher%20and%20Geert%20De%20Cubber%20and%20Ben%20Lauwens%0AAbstract%3A%20%20%20As%20drones%20are%20getting%20more%20and%20more%20entangled%20in%20our%20society%2C%20more%20untrained%0Ausers%20require%20the%20capability%20to%20operate%20them.%20This%20scenario%20is%20to%20be%20achieved%0Athrough%20the%20development%20of%20artificial%20intelligence%20capabilities%20assisting%20the%0Ahuman%20operator%20in%20controlling%20the%20Unmanned%20Aerial%20System%20%28UAS%29%20and%20processing%0Athe%20sensor%20data%2C%20thereby%20alleviating%20the%20need%20for%20extensive%20operator%20training.%0AThis%20paper%20presents%20the%20HADRON%20project%20that%20seeks%20to%20develop%20and%20test%20multiple%0Anovel%20technologies%20to%20enable%20human-friendly%20control%20of%20drone%20swarms.%20This%0Aproject%20is%20divided%20into%20three%20main%20parts.%20The%20first%20part%20consists%20of%20the%0Aintegration%20of%20different%20technologies%20for%20the%20intuitive%20control%20of%20drones%2C%0Afocusing%20on%20novice%20or%20inexperienced%20pilots%20and%20operators.%20The%20second%20part%0Afocuses%20on%20the%20development%20of%20a%20multi-drone%20system%20that%20will%20be%20controlled%20from%0Aa%20command%20and%20control%20station%2C%20in%20which%20an%20expert%20pilot%20can%20supervise%20the%0Aoperations%20of%20the%20multiple%20drones.%20The%20third%20part%20of%20the%20project%20will%20focus%20on%0Areducing%20the%20cognitive%20load%20on%20human%20operators%2C%20whether%20they%20are%20novice%20or%0Aexpert%20pilots.%20For%20this%2C%20we%20will%20develop%20AI%20tools%20that%20will%20assist%20drone%0Aoperators%20with%20semi-automated%20real-time%20data%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHADRON%253A%2520Human-friendly%2520Control%2520and%2520Artificial%2520Intelligence%2520for%2520Military%250A%2520%2520Drone%2520Operations%26entry.906535625%3DAna%2520M.%2520Casado%2520Faul%25C3%25AD%2520and%2520Mario%2520Malizia%2520and%2520Ken%2520Hasselmann%2520and%2520Emile%2520Le%2520Fl%25C3%25A9cher%2520and%2520Geert%2520De%2520Cubber%2520and%2520Ben%2520Lauwens%26entry.1292438233%3D%2520%2520As%2520drones%2520are%2520getting%2520more%2520and%2520more%2520entangled%2520in%2520our%2520society%252C%2520more%2520untrained%250Ausers%2520require%2520the%2520capability%2520to%2520operate%2520them.%2520This%2520scenario%2520is%2520to%2520be%2520achieved%250Athrough%2520the%2520development%2520of%2520artificial%2520intelligence%2520capabilities%2520assisting%2520the%250Ahuman%2520operator%2520in%2520controlling%2520the%2520Unmanned%2520Aerial%2520System%2520%2528UAS%2529%2520and%2520processing%250Athe%2520sensor%2520data%252C%2520thereby%2520alleviating%2520the%2520need%2520for%2520extensive%2520operator%2520training.%250AThis%2520paper%2520presents%2520the%2520HADRON%2520project%2520that%2520seeks%2520to%2520develop%2520and%2520test%2520multiple%250Anovel%2520technologies%2520to%2520enable%2520human-friendly%2520control%2520of%2520drone%2520swarms.%2520This%250Aproject%2520is%2520divided%2520into%2520three%2520main%2520parts.%2520The%2520first%2520part%2520consists%2520of%2520the%250Aintegration%2520of%2520different%2520technologies%2520for%2520the%2520intuitive%2520control%2520of%2520drones%252C%250Afocusing%2520on%2520novice%2520or%2520inexperienced%2520pilots%2520and%2520operators.%2520The%2520second%2520part%250Afocuses%2520on%2520the%2520development%2520of%2520a%2520multi-drone%2520system%2520that%2520will%2520be%2520controlled%2520from%250Aa%2520command%2520and%2520control%2520station%252C%2520in%2520which%2520an%2520expert%2520pilot%2520can%2520supervise%2520the%250Aoperations%2520of%2520the%2520multiple%2520drones.%2520The%2520third%2520part%2520of%2520the%2520project%2520will%2520focus%2520on%250Areducing%2520the%2520cognitive%2520load%2520on%2520human%2520operators%252C%2520whether%2520they%2520are%2520novice%2520or%250Aexpert%2520pilots.%2520For%2520this%252C%2520we%2520will%2520develop%2520AI%2520tools%2520that%2520will%2520assist%2520drone%250Aoperators%2520with%2520semi-automated%2520real-time%2520data%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HADRON%3A%20Human-friendly%20Control%20and%20Artificial%20Intelligence%20for%20Military%0A%20%20Drone%20Operations&entry.906535625=Ana%20M.%20Casado%20Faul%C3%AD%20and%20Mario%20Malizia%20and%20Ken%20Hasselmann%20and%20Emile%20Le%20Fl%C3%A9cher%20and%20Geert%20De%20Cubber%20and%20Ben%20Lauwens&entry.1292438233=%20%20As%20drones%20are%20getting%20more%20and%20more%20entangled%20in%20our%20society%2C%20more%20untrained%0Ausers%20require%20the%20capability%20to%20operate%20them.%20This%20scenario%20is%20to%20be%20achieved%0Athrough%20the%20development%20of%20artificial%20intelligence%20capabilities%20assisting%20the%0Ahuman%20operator%20in%20controlling%20the%20Unmanned%20Aerial%20System%20%28UAS%29%20and%20processing%0Athe%20sensor%20data%2C%20thereby%20alleviating%20the%20need%20for%20extensive%20operator%20training.%0AThis%20paper%20presents%20the%20HADRON%20project%20that%20seeks%20to%20develop%20and%20test%20multiple%0Anovel%20technologies%20to%20enable%20human-friendly%20control%20of%20drone%20swarms.%20This%0Aproject%20is%20divided%20into%20three%20main%20parts.%20The%20first%20part%20consists%20of%20the%0Aintegration%20of%20different%20technologies%20for%20the%20intuitive%20control%20of%20drones%2C%0Afocusing%20on%20novice%20or%20inexperienced%20pilots%20and%20operators.%20The%20second%20part%0Afocuses%20on%20the%20development%20of%20a%20multi-drone%20system%20that%20will%20be%20controlled%20from%0Aa%20command%20and%20control%20station%2C%20in%20which%20an%20expert%20pilot%20can%20supervise%20the%0Aoperations%20of%20the%20multiple%20drones.%20The%20third%20part%20of%20the%20project%20will%20focus%20on%0Areducing%20the%20cognitive%20load%20on%20human%20operators%2C%20whether%20they%20are%20novice%20or%0Aexpert%20pilots.%20For%20this%2C%20we%20will%20develop%20AI%20tools%20that%20will%20assist%20drone%0Aoperators%20with%20semi-automated%20real-time%20data%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07063v1&entry.124074799=Read"},
{"title": "AKBR: Learning Adaptive Kernel-based Representations for Graph\n  Classification", "author": "Feifei Qian and Lixin Cui and Ming Li and Yue Wang and Hangyuan Du and Lixiang Xu and Lu Bai and Philip S. Yu and Edwin R. Hancock", "abstract": "  In this paper, we propose a new model to learn Adaptive Kernel-based\nRepresentations (AKBR) for graph classification. Unlike state-of-the-art\nR-convolution graph kernels that are defined by merely counting any pair of\nisomorphic substructures between graphs and cannot provide an end-to-end\nlearning mechanism for the classifier, the proposed AKBR approach aims to\ndefine an end-to-end representation learning model to construct an adaptive\nkernel matrix for graphs. To this end, we commence by leveraging a novel\nfeature-channel attention mechanism to capture the interdependencies between\ndifferent substructure invariants of original graphs. The proposed AKBR model\ncan thus effectively identify the structural importance of different\nsubstructures, and compute the R-convolution kernel between pairwise graphs\nassociated with the more significant substructures specified by their\nstructural attentions. Since each row of the resulting kernel matrix can be\ntheoretically seen as the embedding vector of a sample graph, the proposed AKBR\nmodel is able to directly employ the resulting kernel matrix as the graph\nfeature matrix and input it into the classifier for classification (i.e., the\nSoftMax layer), naturally providing an end-to-end learning architecture between\nthe kernel computation as well as the classifier. Experimental results show\nthat the proposed AKBR model outperforms existing state-of-the-art graph\nkernels and deep learning methods on standard graph benchmarks.\n", "link": "http://arxiv.org/abs/2403.16130v2", "date": "2024-08-13", "relevancy": 1.9586, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5098}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4787}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AKBR%3A%20Learning%20Adaptive%20Kernel-based%20Representations%20for%20Graph%0A%20%20Classification&body=Title%3A%20AKBR%3A%20Learning%20Adaptive%20Kernel-based%20Representations%20for%20Graph%0A%20%20Classification%0AAuthor%3A%20Feifei%20Qian%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Hangyuan%20Du%20and%20Lixiang%20Xu%20and%20Lu%20Bai%20and%20Philip%20S.%20Yu%20and%20Edwin%20R.%20Hancock%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20model%20to%20learn%20Adaptive%20Kernel-based%0ARepresentations%20%28AKBR%29%20for%20graph%20classification.%20Unlike%20state-of-the-art%0AR-convolution%20graph%20kernels%20that%20are%20defined%20by%20merely%20counting%20any%20pair%20of%0Aisomorphic%20substructures%20between%20graphs%20and%20cannot%20provide%20an%20end-to-end%0Alearning%20mechanism%20for%20the%20classifier%2C%20the%20proposed%20AKBR%20approach%20aims%20to%0Adefine%20an%20end-to-end%20representation%20learning%20model%20to%20construct%20an%20adaptive%0Akernel%20matrix%20for%20graphs.%20To%20this%20end%2C%20we%20commence%20by%20leveraging%20a%20novel%0Afeature-channel%20attention%20mechanism%20to%20capture%20the%20interdependencies%20between%0Adifferent%20substructure%20invariants%20of%20original%20graphs.%20The%20proposed%20AKBR%20model%0Acan%20thus%20effectively%20identify%20the%20structural%20importance%20of%20different%0Asubstructures%2C%20and%20compute%20the%20R-convolution%20kernel%20between%20pairwise%20graphs%0Aassociated%20with%20the%20more%20significant%20substructures%20specified%20by%20their%0Astructural%20attentions.%20Since%20each%20row%20of%20the%20resulting%20kernel%20matrix%20can%20be%0Atheoretically%20seen%20as%20the%20embedding%20vector%20of%20a%20sample%20graph%2C%20the%20proposed%20AKBR%0Amodel%20is%20able%20to%20directly%20employ%20the%20resulting%20kernel%20matrix%20as%20the%20graph%0Afeature%20matrix%20and%20input%20it%20into%20the%20classifier%20for%20classification%20%28i.e.%2C%20the%0ASoftMax%20layer%29%2C%20naturally%20providing%20an%20end-to-end%20learning%20architecture%20between%0Athe%20kernel%20computation%20as%20well%20as%20the%20classifier.%20Experimental%20results%20show%0Athat%20the%20proposed%20AKBR%20model%20outperforms%20existing%20state-of-the-art%20graph%0Akernels%20and%20deep%20learning%20methods%20on%20standard%20graph%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16130v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAKBR%253A%2520Learning%2520Adaptive%2520Kernel-based%2520Representations%2520for%2520Graph%250A%2520%2520Classification%26entry.906535625%3DFeifei%2520Qian%2520and%2520Lixin%2520Cui%2520and%2520Ming%2520Li%2520and%2520Yue%2520Wang%2520and%2520Hangyuan%2520Du%2520and%2520Lixiang%2520Xu%2520and%2520Lu%2520Bai%2520and%2520Philip%2520S.%2520Yu%2520and%2520Edwin%2520R.%2520Hancock%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520model%2520to%2520learn%2520Adaptive%2520Kernel-based%250ARepresentations%2520%2528AKBR%2529%2520for%2520graph%2520classification.%2520Unlike%2520state-of-the-art%250AR-convolution%2520graph%2520kernels%2520that%2520are%2520defined%2520by%2520merely%2520counting%2520any%2520pair%2520of%250Aisomorphic%2520substructures%2520between%2520graphs%2520and%2520cannot%2520provide%2520an%2520end-to-end%250Alearning%2520mechanism%2520for%2520the%2520classifier%252C%2520the%2520proposed%2520AKBR%2520approach%2520aims%2520to%250Adefine%2520an%2520end-to-end%2520representation%2520learning%2520model%2520to%2520construct%2520an%2520adaptive%250Akernel%2520matrix%2520for%2520graphs.%2520To%2520this%2520end%252C%2520we%2520commence%2520by%2520leveraging%2520a%2520novel%250Afeature-channel%2520attention%2520mechanism%2520to%2520capture%2520the%2520interdependencies%2520between%250Adifferent%2520substructure%2520invariants%2520of%2520original%2520graphs.%2520The%2520proposed%2520AKBR%2520model%250Acan%2520thus%2520effectively%2520identify%2520the%2520structural%2520importance%2520of%2520different%250Asubstructures%252C%2520and%2520compute%2520the%2520R-convolution%2520kernel%2520between%2520pairwise%2520graphs%250Aassociated%2520with%2520the%2520more%2520significant%2520substructures%2520specified%2520by%2520their%250Astructural%2520attentions.%2520Since%2520each%2520row%2520of%2520the%2520resulting%2520kernel%2520matrix%2520can%2520be%250Atheoretically%2520seen%2520as%2520the%2520embedding%2520vector%2520of%2520a%2520sample%2520graph%252C%2520the%2520proposed%2520AKBR%250Amodel%2520is%2520able%2520to%2520directly%2520employ%2520the%2520resulting%2520kernel%2520matrix%2520as%2520the%2520graph%250Afeature%2520matrix%2520and%2520input%2520it%2520into%2520the%2520classifier%2520for%2520classification%2520%2528i.e.%252C%2520the%250ASoftMax%2520layer%2529%252C%2520naturally%2520providing%2520an%2520end-to-end%2520learning%2520architecture%2520between%250Athe%2520kernel%2520computation%2520as%2520well%2520as%2520the%2520classifier.%2520Experimental%2520results%2520show%250Athat%2520the%2520proposed%2520AKBR%2520model%2520outperforms%2520existing%2520state-of-the-art%2520graph%250Akernels%2520and%2520deep%2520learning%2520methods%2520on%2520standard%2520graph%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16130v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AKBR%3A%20Learning%20Adaptive%20Kernel-based%20Representations%20for%20Graph%0A%20%20Classification&entry.906535625=Feifei%20Qian%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Hangyuan%20Du%20and%20Lixiang%20Xu%20and%20Lu%20Bai%20and%20Philip%20S.%20Yu%20and%20Edwin%20R.%20Hancock&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20model%20to%20learn%20Adaptive%20Kernel-based%0ARepresentations%20%28AKBR%29%20for%20graph%20classification.%20Unlike%20state-of-the-art%0AR-convolution%20graph%20kernels%20that%20are%20defined%20by%20merely%20counting%20any%20pair%20of%0Aisomorphic%20substructures%20between%20graphs%20and%20cannot%20provide%20an%20end-to-end%0Alearning%20mechanism%20for%20the%20classifier%2C%20the%20proposed%20AKBR%20approach%20aims%20to%0Adefine%20an%20end-to-end%20representation%20learning%20model%20to%20construct%20an%20adaptive%0Akernel%20matrix%20for%20graphs.%20To%20this%20end%2C%20we%20commence%20by%20leveraging%20a%20novel%0Afeature-channel%20attention%20mechanism%20to%20capture%20the%20interdependencies%20between%0Adifferent%20substructure%20invariants%20of%20original%20graphs.%20The%20proposed%20AKBR%20model%0Acan%20thus%20effectively%20identify%20the%20structural%20importance%20of%20different%0Asubstructures%2C%20and%20compute%20the%20R-convolution%20kernel%20between%20pairwise%20graphs%0Aassociated%20with%20the%20more%20significant%20substructures%20specified%20by%20their%0Astructural%20attentions.%20Since%20each%20row%20of%20the%20resulting%20kernel%20matrix%20can%20be%0Atheoretically%20seen%20as%20the%20embedding%20vector%20of%20a%20sample%20graph%2C%20the%20proposed%20AKBR%0Amodel%20is%20able%20to%20directly%20employ%20the%20resulting%20kernel%20matrix%20as%20the%20graph%0Afeature%20matrix%20and%20input%20it%20into%20the%20classifier%20for%20classification%20%28i.e.%2C%20the%0ASoftMax%20layer%29%2C%20naturally%20providing%20an%20end-to-end%20learning%20architecture%20between%0Athe%20kernel%20computation%20as%20well%20as%20the%20classifier.%20Experimental%20results%20show%0Athat%20the%20proposed%20AKBR%20model%20outperforms%20existing%20state-of-the-art%20graph%0Akernels%20and%20deep%20learning%20methods%20on%20standard%20graph%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16130v2&entry.124074799=Read"},
{"title": "Federated Smoothing Proximal Gradient for Quantile Regression with\n  Non-Convex Penalties", "author": "Reza Mirzaeifard and Diyako Ghaderyan and Stefan Werner", "abstract": "  Distributed sensors in the internet-of-things (IoT) generate vast amounts of\nsparse data. Analyzing this high-dimensional data and identifying relevant\npredictors pose substantial challenges, especially when data is preferred to\nremain on the device where it was collected for reasons such as data integrity,\ncommunication bandwidth, and privacy. This paper introduces a federated\nquantile regression algorithm to address these challenges. Quantile regression\nprovides a more comprehensive view of the relationship between variables than\nmean regression models. However, traditional approaches face difficulties when\ndealing with nonconvex sparse penalties and the inherent non-smoothness of the\nloss function. For this purpose, we propose a federated smoothing proximal\ngradient (FSPG) algorithm that integrates a smoothing mechanism with the\nproximal gradient framework, thereby enhancing both precision and computational\nspeed. This integration adeptly handles optimization over a network of devices,\neach holding local data samples, making it particularly effective in federated\nlearning scenarios. The FSPG algorithm ensures steady progress and reliable\nconvergence in each iteration by maintaining or reducing the value of the\nobjective function. By leveraging nonconvex penalties, such as the minimax\nconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), the\nproposed method can identify and preserve key predictors within sparse models.\nComprehensive simulations validate the robust theoretical foundations of the\nproposed algorithm and demonstrate improved estimation precision and reliable\nconvergence.\n", "link": "http://arxiv.org/abs/2408.05640v2", "date": "2024-08-13", "relevancy": 1.9391, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4809}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Smoothing%20Proximal%20Gradient%20for%20Quantile%20Regression%20with%0A%20%20Non-Convex%20Penalties&body=Title%3A%20Federated%20Smoothing%20Proximal%20Gradient%20for%20Quantile%20Regression%20with%0A%20%20Non-Convex%20Penalties%0AAuthor%3A%20Reza%20Mirzaeifard%20and%20Diyako%20Ghaderyan%20and%20Stefan%20Werner%0AAbstract%3A%20%20%20Distributed%20sensors%20in%20the%20internet-of-things%20%28IoT%29%20generate%20vast%20amounts%20of%0Asparse%20data.%20Analyzing%20this%20high-dimensional%20data%20and%20identifying%20relevant%0Apredictors%20pose%20substantial%20challenges%2C%20especially%20when%20data%20is%20preferred%20to%0Aremain%20on%20the%20device%20where%20it%20was%20collected%20for%20reasons%20such%20as%20data%20integrity%2C%0Acommunication%20bandwidth%2C%20and%20privacy.%20This%20paper%20introduces%20a%20federated%0Aquantile%20regression%20algorithm%20to%20address%20these%20challenges.%20Quantile%20regression%0Aprovides%20a%20more%20comprehensive%20view%20of%20the%20relationship%20between%20variables%20than%0Amean%20regression%20models.%20However%2C%20traditional%20approaches%20face%20difficulties%20when%0Adealing%20with%20nonconvex%20sparse%20penalties%20and%20the%20inherent%20non-smoothness%20of%20the%0Aloss%20function.%20For%20this%20purpose%2C%20we%20propose%20a%20federated%20smoothing%20proximal%0Agradient%20%28FSPG%29%20algorithm%20that%20integrates%20a%20smoothing%20mechanism%20with%20the%0Aproximal%20gradient%20framework%2C%20thereby%20enhancing%20both%20precision%20and%20computational%0Aspeed.%20This%20integration%20adeptly%20handles%20optimization%20over%20a%20network%20of%20devices%2C%0Aeach%20holding%20local%20data%20samples%2C%20making%20it%20particularly%20effective%20in%20federated%0Alearning%20scenarios.%20The%20FSPG%20algorithm%20ensures%20steady%20progress%20and%20reliable%0Aconvergence%20in%20each%20iteration%20by%20maintaining%20or%20reducing%20the%20value%20of%20the%0Aobjective%20function.%20By%20leveraging%20nonconvex%20penalties%2C%20such%20as%20the%20minimax%0Aconcave%20penalty%20%28MCP%29%20and%20smoothly%20clipped%20absolute%20deviation%20%28SCAD%29%2C%20the%0Aproposed%20method%20can%20identify%20and%20preserve%20key%20predictors%20within%20sparse%20models.%0AComprehensive%20simulations%20validate%20the%20robust%20theoretical%20foundations%20of%20the%0Aproposed%20algorithm%20and%20demonstrate%20improved%20estimation%20precision%20and%20reliable%0Aconvergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Smoothing%2520Proximal%2520Gradient%2520for%2520Quantile%2520Regression%2520with%250A%2520%2520Non-Convex%2520Penalties%26entry.906535625%3DReza%2520Mirzaeifard%2520and%2520Diyako%2520Ghaderyan%2520and%2520Stefan%2520Werner%26entry.1292438233%3D%2520%2520Distributed%2520sensors%2520in%2520the%2520internet-of-things%2520%2528IoT%2529%2520generate%2520vast%2520amounts%2520of%250Asparse%2520data.%2520Analyzing%2520this%2520high-dimensional%2520data%2520and%2520identifying%2520relevant%250Apredictors%2520pose%2520substantial%2520challenges%252C%2520especially%2520when%2520data%2520is%2520preferred%2520to%250Aremain%2520on%2520the%2520device%2520where%2520it%2520was%2520collected%2520for%2520reasons%2520such%2520as%2520data%2520integrity%252C%250Acommunication%2520bandwidth%252C%2520and%2520privacy.%2520This%2520paper%2520introduces%2520a%2520federated%250Aquantile%2520regression%2520algorithm%2520to%2520address%2520these%2520challenges.%2520Quantile%2520regression%250Aprovides%2520a%2520more%2520comprehensive%2520view%2520of%2520the%2520relationship%2520between%2520variables%2520than%250Amean%2520regression%2520models.%2520However%252C%2520traditional%2520approaches%2520face%2520difficulties%2520when%250Adealing%2520with%2520nonconvex%2520sparse%2520penalties%2520and%2520the%2520inherent%2520non-smoothness%2520of%2520the%250Aloss%2520function.%2520For%2520this%2520purpose%252C%2520we%2520propose%2520a%2520federated%2520smoothing%2520proximal%250Agradient%2520%2528FSPG%2529%2520algorithm%2520that%2520integrates%2520a%2520smoothing%2520mechanism%2520with%2520the%250Aproximal%2520gradient%2520framework%252C%2520thereby%2520enhancing%2520both%2520precision%2520and%2520computational%250Aspeed.%2520This%2520integration%2520adeptly%2520handles%2520optimization%2520over%2520a%2520network%2520of%2520devices%252C%250Aeach%2520holding%2520local%2520data%2520samples%252C%2520making%2520it%2520particularly%2520effective%2520in%2520federated%250Alearning%2520scenarios.%2520The%2520FSPG%2520algorithm%2520ensures%2520steady%2520progress%2520and%2520reliable%250Aconvergence%2520in%2520each%2520iteration%2520by%2520maintaining%2520or%2520reducing%2520the%2520value%2520of%2520the%250Aobjective%2520function.%2520By%2520leveraging%2520nonconvex%2520penalties%252C%2520such%2520as%2520the%2520minimax%250Aconcave%2520penalty%2520%2528MCP%2529%2520and%2520smoothly%2520clipped%2520absolute%2520deviation%2520%2528SCAD%2529%252C%2520the%250Aproposed%2520method%2520can%2520identify%2520and%2520preserve%2520key%2520predictors%2520within%2520sparse%2520models.%250AComprehensive%2520simulations%2520validate%2520the%2520robust%2520theoretical%2520foundations%2520of%2520the%250Aproposed%2520algorithm%2520and%2520demonstrate%2520improved%2520estimation%2520precision%2520and%2520reliable%250Aconvergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Smoothing%20Proximal%20Gradient%20for%20Quantile%20Regression%20with%0A%20%20Non-Convex%20Penalties&entry.906535625=Reza%20Mirzaeifard%20and%20Diyako%20Ghaderyan%20and%20Stefan%20Werner&entry.1292438233=%20%20Distributed%20sensors%20in%20the%20internet-of-things%20%28IoT%29%20generate%20vast%20amounts%20of%0Asparse%20data.%20Analyzing%20this%20high-dimensional%20data%20and%20identifying%20relevant%0Apredictors%20pose%20substantial%20challenges%2C%20especially%20when%20data%20is%20preferred%20to%0Aremain%20on%20the%20device%20where%20it%20was%20collected%20for%20reasons%20such%20as%20data%20integrity%2C%0Acommunication%20bandwidth%2C%20and%20privacy.%20This%20paper%20introduces%20a%20federated%0Aquantile%20regression%20algorithm%20to%20address%20these%20challenges.%20Quantile%20regression%0Aprovides%20a%20more%20comprehensive%20view%20of%20the%20relationship%20between%20variables%20than%0Amean%20regression%20models.%20However%2C%20traditional%20approaches%20face%20difficulties%20when%0Adealing%20with%20nonconvex%20sparse%20penalties%20and%20the%20inherent%20non-smoothness%20of%20the%0Aloss%20function.%20For%20this%20purpose%2C%20we%20propose%20a%20federated%20smoothing%20proximal%0Agradient%20%28FSPG%29%20algorithm%20that%20integrates%20a%20smoothing%20mechanism%20with%20the%0Aproximal%20gradient%20framework%2C%20thereby%20enhancing%20both%20precision%20and%20computational%0Aspeed.%20This%20integration%20adeptly%20handles%20optimization%20over%20a%20network%20of%20devices%2C%0Aeach%20holding%20local%20data%20samples%2C%20making%20it%20particularly%20effective%20in%20federated%0Alearning%20scenarios.%20The%20FSPG%20algorithm%20ensures%20steady%20progress%20and%20reliable%0Aconvergence%20in%20each%20iteration%20by%20maintaining%20or%20reducing%20the%20value%20of%20the%0Aobjective%20function.%20By%20leveraging%20nonconvex%20penalties%2C%20such%20as%20the%20minimax%0Aconcave%20penalty%20%28MCP%29%20and%20smoothly%20clipped%20absolute%20deviation%20%28SCAD%29%2C%20the%0Aproposed%20method%20can%20identify%20and%20preserve%20key%20predictors%20within%20sparse%20models.%0AComprehensive%20simulations%20validate%20the%20robust%20theoretical%20foundations%20of%20the%0Aproposed%20algorithm%20and%20demonstrate%20improved%20estimation%20precision%20and%20reliable%0Aconvergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05640v2&entry.124074799=Read"},
{"title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs", "author": "Yushi Bai and Jiajie Zhang and Xin Lv and Linzhi Zheng and Siqi Zhu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li", "abstract": "  Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.\n", "link": "http://arxiv.org/abs/2408.07055v1", "date": "2024-08-13", "relevancy": 1.9228, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4869}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4816}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongWriter%3A%20Unleashing%2010%2C000%2B%20Word%20Generation%20from%20Long%20Context%20LLMs&body=Title%3A%20LongWriter%3A%20Unleashing%2010%2C000%2B%20Word%20Generation%20from%20Long%20Context%20LLMs%0AAuthor%3A%20Yushi%20Bai%20and%20Jiajie%20Zhang%20and%20Xin%20Lv%20and%20Linzhi%20Zheng%20and%20Siqi%20Zhu%20and%20Lei%20Hou%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Current%20long%20context%20large%20language%20models%20%28LLMs%29%20can%20process%20inputs%20up%20to%0A100%2C000%20tokens%2C%20yet%20struggle%20to%20generate%20outputs%20exceeding%20even%20a%20modest%20length%0Aof%202%2C000%20words.%20Through%20controlled%20experiments%2C%20we%20find%20that%20the%20model%27s%0Aeffective%20generation%20length%20is%20inherently%20bounded%20by%20the%20sample%20it%20has%20seen%0Aduring%20supervised%20fine-tuning%20%28SFT%29.%20In%20other%20words%2C%20their%20output%20limitation%20is%0Adue%20to%20the%20scarcity%20of%20long-output%20examples%20in%20existing%20SFT%20datasets.%20To%0Aaddress%20this%2C%20we%20introduce%20AgentWrite%2C%20an%20agent-based%20pipeline%20that%20decomposes%0Aultra-long%20generation%20tasks%20into%20subtasks%2C%20enabling%20off-the-shelf%20LLMs%20to%0Agenerate%20coherent%20outputs%20exceeding%2020%2C000%20words.%20Leveraging%20AgentWrite%2C%20we%0Aconstruct%20LongWriter-6k%2C%20a%20dataset%20containing%206%2C000%20SFT%20data%20with%20output%0Alengths%20ranging%20from%202k%20to%2032k%20words.%20By%20incorporating%20this%20dataset%20into%20model%0Atraining%2C%20we%20successfully%20scale%20the%20output%20length%20of%20existing%20models%20to%20over%0A10%2C000%20words%20while%20maintaining%20output%20quality.%20We%20also%20develop%20LongBench-Write%2C%0Aa%20comprehensive%20benchmark%20for%20evaluating%20ultra-long%20generation%20capabilities.%0AOur%209B%20parameter%20model%2C%20further%20improved%20through%20DPO%2C%20achieves%20state-of-the-art%0Aperformance%20on%20this%20benchmark%2C%20surpassing%20even%20much%20larger%20proprietary%20models.%0AIn%20general%2C%20our%20work%20demonstrates%20that%20existing%20long%20context%20LLM%20already%0Apossesses%20the%20potential%20for%20a%20larger%20output%20window--all%20you%20need%20is%20data%20with%0Aextended%20output%20during%20model%20alignment%20to%20unlock%20this%20capability.%20Our%20code%20%26%0Amodels%20are%20at%3A%20https%3A//github.com/THUDM/LongWriter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongWriter%253A%2520Unleashing%252010%252C000%252B%2520Word%2520Generation%2520from%2520Long%2520Context%2520LLMs%26entry.906535625%3DYushi%2520Bai%2520and%2520Jiajie%2520Zhang%2520and%2520Xin%2520Lv%2520and%2520Linzhi%2520Zheng%2520and%2520Siqi%2520Zhu%2520and%2520Lei%2520Hou%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Current%2520long%2520context%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520process%2520inputs%2520up%2520to%250A100%252C000%2520tokens%252C%2520yet%2520struggle%2520to%2520generate%2520outputs%2520exceeding%2520even%2520a%2520modest%2520length%250Aof%25202%252C000%2520words.%2520Through%2520controlled%2520experiments%252C%2520we%2520find%2520that%2520the%2520model%2527s%250Aeffective%2520generation%2520length%2520is%2520inherently%2520bounded%2520by%2520the%2520sample%2520it%2520has%2520seen%250Aduring%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520In%2520other%2520words%252C%2520their%2520output%2520limitation%2520is%250Adue%2520to%2520the%2520scarcity%2520of%2520long-output%2520examples%2520in%2520existing%2520SFT%2520datasets.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520AgentWrite%252C%2520an%2520agent-based%2520pipeline%2520that%2520decomposes%250Aultra-long%2520generation%2520tasks%2520into%2520subtasks%252C%2520enabling%2520off-the-shelf%2520LLMs%2520to%250Agenerate%2520coherent%2520outputs%2520exceeding%252020%252C000%2520words.%2520Leveraging%2520AgentWrite%252C%2520we%250Aconstruct%2520LongWriter-6k%252C%2520a%2520dataset%2520containing%25206%252C000%2520SFT%2520data%2520with%2520output%250Alengths%2520ranging%2520from%25202k%2520to%252032k%2520words.%2520By%2520incorporating%2520this%2520dataset%2520into%2520model%250Atraining%252C%2520we%2520successfully%2520scale%2520the%2520output%2520length%2520of%2520existing%2520models%2520to%2520over%250A10%252C000%2520words%2520while%2520maintaining%2520output%2520quality.%2520We%2520also%2520develop%2520LongBench-Write%252C%250Aa%2520comprehensive%2520benchmark%2520for%2520evaluating%2520ultra-long%2520generation%2520capabilities.%250AOur%25209B%2520parameter%2520model%252C%2520further%2520improved%2520through%2520DPO%252C%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520this%2520benchmark%252C%2520surpassing%2520even%2520much%2520larger%2520proprietary%2520models.%250AIn%2520general%252C%2520our%2520work%2520demonstrates%2520that%2520existing%2520long%2520context%2520LLM%2520already%250Apossesses%2520the%2520potential%2520for%2520a%2520larger%2520output%2520window--all%2520you%2520need%2520is%2520data%2520with%250Aextended%2520output%2520during%2520model%2520alignment%2520to%2520unlock%2520this%2520capability.%2520Our%2520code%2520%2526%250Amodels%2520are%2520at%253A%2520https%253A//github.com/THUDM/LongWriter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongWriter%3A%20Unleashing%2010%2C000%2B%20Word%20Generation%20from%20Long%20Context%20LLMs&entry.906535625=Yushi%20Bai%20and%20Jiajie%20Zhang%20and%20Xin%20Lv%20and%20Linzhi%20Zheng%20and%20Siqi%20Zhu%20and%20Lei%20Hou%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%20and%20Juanzi%20Li&entry.1292438233=%20%20Current%20long%20context%20large%20language%20models%20%28LLMs%29%20can%20process%20inputs%20up%20to%0A100%2C000%20tokens%2C%20yet%20struggle%20to%20generate%20outputs%20exceeding%20even%20a%20modest%20length%0Aof%202%2C000%20words.%20Through%20controlled%20experiments%2C%20we%20find%20that%20the%20model%27s%0Aeffective%20generation%20length%20is%20inherently%20bounded%20by%20the%20sample%20it%20has%20seen%0Aduring%20supervised%20fine-tuning%20%28SFT%29.%20In%20other%20words%2C%20their%20output%20limitation%20is%0Adue%20to%20the%20scarcity%20of%20long-output%20examples%20in%20existing%20SFT%20datasets.%20To%0Aaddress%20this%2C%20we%20introduce%20AgentWrite%2C%20an%20agent-based%20pipeline%20that%20decomposes%0Aultra-long%20generation%20tasks%20into%20subtasks%2C%20enabling%20off-the-shelf%20LLMs%20to%0Agenerate%20coherent%20outputs%20exceeding%2020%2C000%20words.%20Leveraging%20AgentWrite%2C%20we%0Aconstruct%20LongWriter-6k%2C%20a%20dataset%20containing%206%2C000%20SFT%20data%20with%20output%0Alengths%20ranging%20from%202k%20to%2032k%20words.%20By%20incorporating%20this%20dataset%20into%20model%0Atraining%2C%20we%20successfully%20scale%20the%20output%20length%20of%20existing%20models%20to%20over%0A10%2C000%20words%20while%20maintaining%20output%20quality.%20We%20also%20develop%20LongBench-Write%2C%0Aa%20comprehensive%20benchmark%20for%20evaluating%20ultra-long%20generation%20capabilities.%0AOur%209B%20parameter%20model%2C%20further%20improved%20through%20DPO%2C%20achieves%20state-of-the-art%0Aperformance%20on%20this%20benchmark%2C%20surpassing%20even%20much%20larger%20proprietary%20models.%0AIn%20general%2C%20our%20work%20demonstrates%20that%20existing%20long%20context%20LLM%20already%0Apossesses%20the%20potential%20for%20a%20larger%20output%20window--all%20you%20need%20is%20data%20with%0Aextended%20output%20during%20model%20alignment%20to%20unlock%20this%20capability.%20Our%20code%20%26%0Amodels%20are%20at%3A%20https%3A//github.com/THUDM/LongWriter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07055v1&entry.124074799=Read"},
{"title": "Low-Bitwidth Floating Point Quantization for Efficient High-Quality\n  Diffusion Models", "author": "Cheng Chen and Christina Giannoula and Andreas Moshovos", "abstract": "  Diffusion models are emerging models that generate images by iteratively\ndenoising random Gaussian noise using deep neural networks. These models\ntypically exhibit high computational and memory demands, necessitating\neffective post-training quantization for high-performance inference. Recent\nworks propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusion\nmodels, however 4-bit integer quantization typically results in low-quality\nimages. We observe that on several widely used hardware platforms, there is\nlittle or no difference in compute capability between floating-point and\ninteger arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit).\nTherefore, we propose an effective floating-point quantization method for\ndiffusion models that provides better image quality compared to integer\nquantization methods. We employ a floating-point quantization method that was\neffective for other processing tasks, specifically computer vision and natural\nlanguage tasks, and tailor it for diffusion models by integrating weight\nrounding learning during the mapping of the full-precision values to the\nquantized values in the quantization process. We comprehensively study integer\nand floating-point quantization methods in state-of-the-art diffusion models.\nOur floating-point quantization method not only generates higher-quality images\nthan that of integer quantization methods, but also shows no noticeable\ndegradation compared to full-precision models (32-bit floating-point), when\nboth weights and activations are quantized to 8-bit floating-point values,\nwhile has minimal degradation with 4-bit weights and 8-bit activations.\n", "link": "http://arxiv.org/abs/2408.06995v1", "date": "2024-08-13", "relevancy": 1.9193, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6749}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6309}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Bitwidth%20Floating%20Point%20Quantization%20for%20Efficient%20High-Quality%0A%20%20Diffusion%20Models&body=Title%3A%20Low-Bitwidth%20Floating%20Point%20Quantization%20for%20Efficient%20High-Quality%0A%20%20Diffusion%20Models%0AAuthor%3A%20Cheng%20Chen%20and%20Christina%20Giannoula%20and%20Andreas%20Moshovos%0AAbstract%3A%20%20%20Diffusion%20models%20are%20emerging%20models%20that%20generate%20images%20by%20iteratively%0Adenoising%20random%20Gaussian%20noise%20using%20deep%20neural%20networks.%20These%20models%0Atypically%20exhibit%20high%20computational%20and%20memory%20demands%2C%20necessitating%0Aeffective%20post-training%20quantization%20for%20high-performance%20inference.%20Recent%0Aworks%20propose%20low-bitwidth%20%28e.g.%2C%208-bit%20or%204-bit%29%20quantization%20for%20diffusion%0Amodels%2C%20however%204-bit%20integer%20quantization%20typically%20results%20in%20low-quality%0Aimages.%20We%20observe%20that%20on%20several%20widely%20used%20hardware%20platforms%2C%20there%20is%0Alittle%20or%20no%20difference%20in%20compute%20capability%20between%20floating-point%20and%0Ainteger%20arithmetic%20operations%20of%20the%20same%20bitwidth%20%28e.g.%2C%208-bit%20or%204-bit%29.%0ATherefore%2C%20we%20propose%20an%20effective%20floating-point%20quantization%20method%20for%0Adiffusion%20models%20that%20provides%20better%20image%20quality%20compared%20to%20integer%0Aquantization%20methods.%20We%20employ%20a%20floating-point%20quantization%20method%20that%20was%0Aeffective%20for%20other%20processing%20tasks%2C%20specifically%20computer%20vision%20and%20natural%0Alanguage%20tasks%2C%20and%20tailor%20it%20for%20diffusion%20models%20by%20integrating%20weight%0Arounding%20learning%20during%20the%20mapping%20of%20the%20full-precision%20values%20to%20the%0Aquantized%20values%20in%20the%20quantization%20process.%20We%20comprehensively%20study%20integer%0Aand%20floating-point%20quantization%20methods%20in%20state-of-the-art%20diffusion%20models.%0AOur%20floating-point%20quantization%20method%20not%20only%20generates%20higher-quality%20images%0Athan%20that%20of%20integer%20quantization%20methods%2C%20but%20also%20shows%20no%20noticeable%0Adegradation%20compared%20to%20full-precision%20models%20%2832-bit%20floating-point%29%2C%20when%0Aboth%20weights%20and%20activations%20are%20quantized%20to%208-bit%20floating-point%20values%2C%0Awhile%20has%20minimal%20degradation%20with%204-bit%20weights%20and%208-bit%20activations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Bitwidth%2520Floating%2520Point%2520Quantization%2520for%2520Efficient%2520High-Quality%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DCheng%2520Chen%2520and%2520Christina%2520Giannoula%2520and%2520Andreas%2520Moshovos%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520are%2520emerging%2520models%2520that%2520generate%2520images%2520by%2520iteratively%250Adenoising%2520random%2520Gaussian%2520noise%2520using%2520deep%2520neural%2520networks.%2520These%2520models%250Atypically%2520exhibit%2520high%2520computational%2520and%2520memory%2520demands%252C%2520necessitating%250Aeffective%2520post-training%2520quantization%2520for%2520high-performance%2520inference.%2520Recent%250Aworks%2520propose%2520low-bitwidth%2520%2528e.g.%252C%25208-bit%2520or%25204-bit%2529%2520quantization%2520for%2520diffusion%250Amodels%252C%2520however%25204-bit%2520integer%2520quantization%2520typically%2520results%2520in%2520low-quality%250Aimages.%2520We%2520observe%2520that%2520on%2520several%2520widely%2520used%2520hardware%2520platforms%252C%2520there%2520is%250Alittle%2520or%2520no%2520difference%2520in%2520compute%2520capability%2520between%2520floating-point%2520and%250Ainteger%2520arithmetic%2520operations%2520of%2520the%2520same%2520bitwidth%2520%2528e.g.%252C%25208-bit%2520or%25204-bit%2529.%250ATherefore%252C%2520we%2520propose%2520an%2520effective%2520floating-point%2520quantization%2520method%2520for%250Adiffusion%2520models%2520that%2520provides%2520better%2520image%2520quality%2520compared%2520to%2520integer%250Aquantization%2520methods.%2520We%2520employ%2520a%2520floating-point%2520quantization%2520method%2520that%2520was%250Aeffective%2520for%2520other%2520processing%2520tasks%252C%2520specifically%2520computer%2520vision%2520and%2520natural%250Alanguage%2520tasks%252C%2520and%2520tailor%2520it%2520for%2520diffusion%2520models%2520by%2520integrating%2520weight%250Arounding%2520learning%2520during%2520the%2520mapping%2520of%2520the%2520full-precision%2520values%2520to%2520the%250Aquantized%2520values%2520in%2520the%2520quantization%2520process.%2520We%2520comprehensively%2520study%2520integer%250Aand%2520floating-point%2520quantization%2520methods%2520in%2520state-of-the-art%2520diffusion%2520models.%250AOur%2520floating-point%2520quantization%2520method%2520not%2520only%2520generates%2520higher-quality%2520images%250Athan%2520that%2520of%2520integer%2520quantization%2520methods%252C%2520but%2520also%2520shows%2520no%2520noticeable%250Adegradation%2520compared%2520to%2520full-precision%2520models%2520%252832-bit%2520floating-point%2529%252C%2520when%250Aboth%2520weights%2520and%2520activations%2520are%2520quantized%2520to%25208-bit%2520floating-point%2520values%252C%250Awhile%2520has%2520minimal%2520degradation%2520with%25204-bit%2520weights%2520and%25208-bit%2520activations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Bitwidth%20Floating%20Point%20Quantization%20for%20Efficient%20High-Quality%0A%20%20Diffusion%20Models&entry.906535625=Cheng%20Chen%20and%20Christina%20Giannoula%20and%20Andreas%20Moshovos&entry.1292438233=%20%20Diffusion%20models%20are%20emerging%20models%20that%20generate%20images%20by%20iteratively%0Adenoising%20random%20Gaussian%20noise%20using%20deep%20neural%20networks.%20These%20models%0Atypically%20exhibit%20high%20computational%20and%20memory%20demands%2C%20necessitating%0Aeffective%20post-training%20quantization%20for%20high-performance%20inference.%20Recent%0Aworks%20propose%20low-bitwidth%20%28e.g.%2C%208-bit%20or%204-bit%29%20quantization%20for%20diffusion%0Amodels%2C%20however%204-bit%20integer%20quantization%20typically%20results%20in%20low-quality%0Aimages.%20We%20observe%20that%20on%20several%20widely%20used%20hardware%20platforms%2C%20there%20is%0Alittle%20or%20no%20difference%20in%20compute%20capability%20between%20floating-point%20and%0Ainteger%20arithmetic%20operations%20of%20the%20same%20bitwidth%20%28e.g.%2C%208-bit%20or%204-bit%29.%0ATherefore%2C%20we%20propose%20an%20effective%20floating-point%20quantization%20method%20for%0Adiffusion%20models%20that%20provides%20better%20image%20quality%20compared%20to%20integer%0Aquantization%20methods.%20We%20employ%20a%20floating-point%20quantization%20method%20that%20was%0Aeffective%20for%20other%20processing%20tasks%2C%20specifically%20computer%20vision%20and%20natural%0Alanguage%20tasks%2C%20and%20tailor%20it%20for%20diffusion%20models%20by%20integrating%20weight%0Arounding%20learning%20during%20the%20mapping%20of%20the%20full-precision%20values%20to%20the%0Aquantized%20values%20in%20the%20quantization%20process.%20We%20comprehensively%20study%20integer%0Aand%20floating-point%20quantization%20methods%20in%20state-of-the-art%20diffusion%20models.%0AOur%20floating-point%20quantization%20method%20not%20only%20generates%20higher-quality%20images%0Athan%20that%20of%20integer%20quantization%20methods%2C%20but%20also%20shows%20no%20noticeable%0Adegradation%20compared%20to%20full-precision%20models%20%2832-bit%20floating-point%29%2C%20when%0Aboth%20weights%20and%20activations%20are%20quantized%20to%208-bit%20floating-point%20values%2C%0Awhile%20has%20minimal%20degradation%20with%204-bit%20weights%20and%208-bit%20activations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06995v1&entry.124074799=Read"},
{"title": "Power Variable Projection for Initialization-Free Large-Scale Bundle\n  Adjustment", "author": "Simon Weber and Je Hyeong Hong and Daniel Cremers", "abstract": "  Most Bundle Adjustment (BA) solvers like the Levenberg-Marquardt algorithm\nrequire a good initialization. Instead, initialization-free BA remains a\nlargely uncharted territory. The under-explored Variable Projection algorithm\n(VarPro) exhibits a wide convergence basin even without initialization. Coupled\nwith object space error formulation, recent works have shown its ability to\nsolve small-scale initialization-free bundle adjustment problem. To make such\ninitialization-free BA approaches scalable, we introduce Power Variable\nProjection (PoVar), extending a recent inverse expansion method based on power\nseries. Importantly, we link the power series expansion to Riemannian manifold\noptimization. This projective framework is crucial to solve large-scale bundle\nadjustment problems without initialization. Using the real-world BAL dataset,\nwe experimentally demonstrate that our solver achieves state-of-the-art results\nin terms of speed and accuracy. To our knowledge, this work is the first to\naddress the scalability of BA without initialization opening new venues for\ninitialization-free structure-from-motion.\n", "link": "http://arxiv.org/abs/2405.05079v5", "date": "2024-08-13", "relevancy": 1.9174, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4915}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4727}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment&body=Title%3A%20Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment%0AAuthor%3A%20Simon%20Weber%20and%20Je%20Hyeong%20Hong%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Most%20Bundle%20Adjustment%20%28BA%29%20solvers%20like%20the%20Levenberg-Marquardt%20algorithm%0Arequire%20a%20good%20initialization.%20Instead%2C%20initialization-free%20BA%20remains%20a%0Alargely%20uncharted%20territory.%20The%20under-explored%20Variable%20Projection%20algorithm%0A%28VarPro%29%20exhibits%20a%20wide%20convergence%20basin%20even%20without%20initialization.%20Coupled%0Awith%20object%20space%20error%20formulation%2C%20recent%20works%20have%20shown%20its%20ability%20to%0Asolve%20small-scale%20initialization-free%20bundle%20adjustment%20problem.%20To%20make%20such%0Ainitialization-free%20BA%20approaches%20scalable%2C%20we%20introduce%20Power%20Variable%0AProjection%20%28PoVar%29%2C%20extending%20a%20recent%20inverse%20expansion%20method%20based%20on%20power%0Aseries.%20Importantly%2C%20we%20link%20the%20power%20series%20expansion%20to%20Riemannian%20manifold%0Aoptimization.%20This%20projective%20framework%20is%20crucial%20to%20solve%20large-scale%20bundle%0Aadjustment%20problems%20without%20initialization.%20Using%20the%20real-world%20BAL%20dataset%2C%0Awe%20experimentally%20demonstrate%20that%20our%20solver%20achieves%20state-of-the-art%20results%0Ain%20terms%20of%20speed%20and%20accuracy.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%0Aaddress%20the%20scalability%20of%20BA%20without%20initialization%20opening%20new%20venues%20for%0Ainitialization-free%20structure-from-motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05079v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPower%2520Variable%2520Projection%2520for%2520Initialization-Free%2520Large-Scale%2520Bundle%250A%2520%2520Adjustment%26entry.906535625%3DSimon%2520Weber%2520and%2520Je%2520Hyeong%2520Hong%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Most%2520Bundle%2520Adjustment%2520%2528BA%2529%2520solvers%2520like%2520the%2520Levenberg-Marquardt%2520algorithm%250Arequire%2520a%2520good%2520initialization.%2520Instead%252C%2520initialization-free%2520BA%2520remains%2520a%250Alargely%2520uncharted%2520territory.%2520The%2520under-explored%2520Variable%2520Projection%2520algorithm%250A%2528VarPro%2529%2520exhibits%2520a%2520wide%2520convergence%2520basin%2520even%2520without%2520initialization.%2520Coupled%250Awith%2520object%2520space%2520error%2520formulation%252C%2520recent%2520works%2520have%2520shown%2520its%2520ability%2520to%250Asolve%2520small-scale%2520initialization-free%2520bundle%2520adjustment%2520problem.%2520To%2520make%2520such%250Ainitialization-free%2520BA%2520approaches%2520scalable%252C%2520we%2520introduce%2520Power%2520Variable%250AProjection%2520%2528PoVar%2529%252C%2520extending%2520a%2520recent%2520inverse%2520expansion%2520method%2520based%2520on%2520power%250Aseries.%2520Importantly%252C%2520we%2520link%2520the%2520power%2520series%2520expansion%2520to%2520Riemannian%2520manifold%250Aoptimization.%2520This%2520projective%2520framework%2520is%2520crucial%2520to%2520solve%2520large-scale%2520bundle%250Aadjustment%2520problems%2520without%2520initialization.%2520Using%2520the%2520real-world%2520BAL%2520dataset%252C%250Awe%2520experimentally%2520demonstrate%2520that%2520our%2520solver%2520achieves%2520state-of-the-art%2520results%250Ain%2520terms%2520of%2520speed%2520and%2520accuracy.%2520To%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520to%250Aaddress%2520the%2520scalability%2520of%2520BA%2520without%2520initialization%2520opening%2520new%2520venues%2520for%250Ainitialization-free%2520structure-from-motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05079v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment&entry.906535625=Simon%20Weber%20and%20Je%20Hyeong%20Hong%20and%20Daniel%20Cremers&entry.1292438233=%20%20Most%20Bundle%20Adjustment%20%28BA%29%20solvers%20like%20the%20Levenberg-Marquardt%20algorithm%0Arequire%20a%20good%20initialization.%20Instead%2C%20initialization-free%20BA%20remains%20a%0Alargely%20uncharted%20territory.%20The%20under-explored%20Variable%20Projection%20algorithm%0A%28VarPro%29%20exhibits%20a%20wide%20convergence%20basin%20even%20without%20initialization.%20Coupled%0Awith%20object%20space%20error%20formulation%2C%20recent%20works%20have%20shown%20its%20ability%20to%0Asolve%20small-scale%20initialization-free%20bundle%20adjustment%20problem.%20To%20make%20such%0Ainitialization-free%20BA%20approaches%20scalable%2C%20we%20introduce%20Power%20Variable%0AProjection%20%28PoVar%29%2C%20extending%20a%20recent%20inverse%20expansion%20method%20based%20on%20power%0Aseries.%20Importantly%2C%20we%20link%20the%20power%20series%20expansion%20to%20Riemannian%20manifold%0Aoptimization.%20This%20projective%20framework%20is%20crucial%20to%20solve%20large-scale%20bundle%0Aadjustment%20problems%20without%20initialization.%20Using%20the%20real-world%20BAL%20dataset%2C%0Awe%20experimentally%20demonstrate%20that%20our%20solver%20achieves%20state-of-the-art%20results%0Ain%20terms%20of%20speed%20and%20accuracy.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%0Aaddress%20the%20scalability%20of%20BA%20without%20initialization%20opening%20new%20venues%20for%0Ainitialization-free%20structure-from-motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05079v5&entry.124074799=Read"},
{"title": "Convergence of Message Passing Graph Neural Networks with Generic\n  Aggregation On Large Random Graphs", "author": "Matthieu Cordonnier and Nicolas Keriven and Nicolas Tremblay and Samuel Vaiter", "abstract": "  We study the convergence of message passing graph neural networks on random\ngraph models to their continuous counterpart as the number of nodes tends to\ninfinity. Until now, this convergence was only known for architectures with\naggregation functions in the form of normalized means, or, equivalently, of an\napplication of classical operators like the adjacency matrix or the graph\nLaplacian. We extend such results to a large class of aggregation functions,\nthat encompasses all classically used message passing graph neural networks,\nsuch as attention-based message passing, max convolutional message passing,\n(degree-normalized) convolutional message passing, or moment-based aggregation\nmessage passing. Under mild assumptions, we give non-asymptotic bounds with\nhigh probability to quantify this convergence. Our main result is based on the\nMcDiarmid inequality. Interestingly, this result does not apply to the case\nwhere the aggregation is a coordinate-wise maximum. We treat this case\nseparately and obtain a different convergence rate.\n", "link": "http://arxiv.org/abs/2304.11140v3", "date": "2024-08-13", "relevancy": 1.9132, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4876}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20Message%20Passing%20Graph%20Neural%20Networks%20with%20Generic%0A%20%20Aggregation%20On%20Large%20Random%20Graphs&body=Title%3A%20Convergence%20of%20Message%20Passing%20Graph%20Neural%20Networks%20with%20Generic%0A%20%20Aggregation%20On%20Large%20Random%20Graphs%0AAuthor%3A%20Matthieu%20Cordonnier%20and%20Nicolas%20Keriven%20and%20Nicolas%20Tremblay%20and%20Samuel%20Vaiter%0AAbstract%3A%20%20%20We%20study%20the%20convergence%20of%20message%20passing%20graph%20neural%20networks%20on%20random%0Agraph%20models%20to%20their%20continuous%20counterpart%20as%20the%20number%20of%20nodes%20tends%20to%0Ainfinity.%20Until%20now%2C%20this%20convergence%20was%20only%20known%20for%20architectures%20with%0Aaggregation%20functions%20in%20the%20form%20of%20normalized%20means%2C%20or%2C%20equivalently%2C%20of%20an%0Aapplication%20of%20classical%20operators%20like%20the%20adjacency%20matrix%20or%20the%20graph%0ALaplacian.%20We%20extend%20such%20results%20to%20a%20large%20class%20of%20aggregation%20functions%2C%0Athat%20encompasses%20all%20classically%20used%20message%20passing%20graph%20neural%20networks%2C%0Asuch%20as%20attention-based%20message%20passing%2C%20max%20convolutional%20message%20passing%2C%0A%28degree-normalized%29%20convolutional%20message%20passing%2C%20or%20moment-based%20aggregation%0Amessage%20passing.%20Under%20mild%20assumptions%2C%20we%20give%20non-asymptotic%20bounds%20with%0Ahigh%20probability%20to%20quantify%20this%20convergence.%20Our%20main%20result%20is%20based%20on%20the%0AMcDiarmid%20inequality.%20Interestingly%2C%20this%20result%20does%20not%20apply%20to%20the%20case%0Awhere%20the%20aggregation%20is%20a%20coordinate-wise%20maximum.%20We%20treat%20this%20case%0Aseparately%20and%20obtain%20a%20different%20convergence%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.11140v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520Message%2520Passing%2520Graph%2520Neural%2520Networks%2520with%2520Generic%250A%2520%2520Aggregation%2520On%2520Large%2520Random%2520Graphs%26entry.906535625%3DMatthieu%2520Cordonnier%2520and%2520Nicolas%2520Keriven%2520and%2520Nicolas%2520Tremblay%2520and%2520Samuel%2520Vaiter%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520convergence%2520of%2520message%2520passing%2520graph%2520neural%2520networks%2520on%2520random%250Agraph%2520models%2520to%2520their%2520continuous%2520counterpart%2520as%2520the%2520number%2520of%2520nodes%2520tends%2520to%250Ainfinity.%2520Until%2520now%252C%2520this%2520convergence%2520was%2520only%2520known%2520for%2520architectures%2520with%250Aaggregation%2520functions%2520in%2520the%2520form%2520of%2520normalized%2520means%252C%2520or%252C%2520equivalently%252C%2520of%2520an%250Aapplication%2520of%2520classical%2520operators%2520like%2520the%2520adjacency%2520matrix%2520or%2520the%2520graph%250ALaplacian.%2520We%2520extend%2520such%2520results%2520to%2520a%2520large%2520class%2520of%2520aggregation%2520functions%252C%250Athat%2520encompasses%2520all%2520classically%2520used%2520message%2520passing%2520graph%2520neural%2520networks%252C%250Asuch%2520as%2520attention-based%2520message%2520passing%252C%2520max%2520convolutional%2520message%2520passing%252C%250A%2528degree-normalized%2529%2520convolutional%2520message%2520passing%252C%2520or%2520moment-based%2520aggregation%250Amessage%2520passing.%2520Under%2520mild%2520assumptions%252C%2520we%2520give%2520non-asymptotic%2520bounds%2520with%250Ahigh%2520probability%2520to%2520quantify%2520this%2520convergence.%2520Our%2520main%2520result%2520is%2520based%2520on%2520the%250AMcDiarmid%2520inequality.%2520Interestingly%252C%2520this%2520result%2520does%2520not%2520apply%2520to%2520the%2520case%250Awhere%2520the%2520aggregation%2520is%2520a%2520coordinate-wise%2520maximum.%2520We%2520treat%2520this%2520case%250Aseparately%2520and%2520obtain%2520a%2520different%2520convergence%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.11140v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20Message%20Passing%20Graph%20Neural%20Networks%20with%20Generic%0A%20%20Aggregation%20On%20Large%20Random%20Graphs&entry.906535625=Matthieu%20Cordonnier%20and%20Nicolas%20Keriven%20and%20Nicolas%20Tremblay%20and%20Samuel%20Vaiter&entry.1292438233=%20%20We%20study%20the%20convergence%20of%20message%20passing%20graph%20neural%20networks%20on%20random%0Agraph%20models%20to%20their%20continuous%20counterpart%20as%20the%20number%20of%20nodes%20tends%20to%0Ainfinity.%20Until%20now%2C%20this%20convergence%20was%20only%20known%20for%20architectures%20with%0Aaggregation%20functions%20in%20the%20form%20of%20normalized%20means%2C%20or%2C%20equivalently%2C%20of%20an%0Aapplication%20of%20classical%20operators%20like%20the%20adjacency%20matrix%20or%20the%20graph%0ALaplacian.%20We%20extend%20such%20results%20to%20a%20large%20class%20of%20aggregation%20functions%2C%0Athat%20encompasses%20all%20classically%20used%20message%20passing%20graph%20neural%20networks%2C%0Asuch%20as%20attention-based%20message%20passing%2C%20max%20convolutional%20message%20passing%2C%0A%28degree-normalized%29%20convolutional%20message%20passing%2C%20or%20moment-based%20aggregation%0Amessage%20passing.%20Under%20mild%20assumptions%2C%20we%20give%20non-asymptotic%20bounds%20with%0Ahigh%20probability%20to%20quantify%20this%20convergence.%20Our%20main%20result%20is%20based%20on%20the%0AMcDiarmid%20inequality.%20Interestingly%2C%20this%20result%20does%20not%20apply%20to%20the%20case%0Awhere%20the%20aggregation%20is%20a%20coordinate-wise%20maximum.%20We%20treat%20this%20case%0Aseparately%20and%20obtain%20a%20different%20convergence%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.11140v3&entry.124074799=Read"},
{"title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus", "author": "Eduard Poesina and Cornelia Caragea and Radu Tudor Ionescu", "abstract": "  Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.\n", "link": "http://arxiv.org/abs/2405.11877v4", "date": "2024-08-13", "relevancy": 1.9131, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4904}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4821}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Cartography-Based%20Curriculum%20Learning%20Method%20Applied%20on%20RoNLI%3A%0A%20%20The%20First%20Romanian%20Natural%20Language%20Inference%20Corpus&body=Title%3A%20A%20Novel%20Cartography-Based%20Curriculum%20Learning%20Method%20Applied%20on%20RoNLI%3A%0A%20%20The%20First%20Romanian%20Natural%20Language%20Inference%20Corpus%0AAuthor%3A%20Eduard%20Poesina%20and%20Cornelia%20Caragea%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20%20%20Natural%20language%20inference%20%28NLI%29%2C%20the%20task%20of%20recognizing%20the%20entailment%0Arelationship%20in%20sentence%20pairs%2C%20is%20an%20actively%20studied%20topic%20serving%20as%20a%20proxy%0Afor%20natural%20language%20understanding.%20Despite%20the%20relevance%20of%20the%20task%20in%0Abuilding%20conversational%20agents%20and%20improving%20text%20classification%2C%20machine%0Atranslation%20and%20other%20NLP%20tasks%2C%20to%20the%20best%20of%20our%20knowledge%2C%20there%20is%20no%0Apublicly%20available%20NLI%20corpus%20for%20the%20Romanian%20language.%20To%20this%20end%2C%20we%0Aintroduce%20the%20first%20Romanian%20NLI%20corpus%20%28RoNLI%29%20comprising%2058K%20training%0Asentence%20pairs%2C%20which%20are%20obtained%20via%20distant%20supervision%2C%20and%206K%20validation%0Aand%20test%20sentence%20pairs%2C%20which%20are%20manually%20annotated%20with%20the%20correct%20labels.%0AWe%20conduct%20experiments%20with%20multiple%20machine%20learning%20methods%20based%20on%20distant%0Alearning%2C%20ranging%20from%20shallow%20models%20based%20on%20word%20embeddings%20to%0Atransformer-based%20neural%20networks%2C%20to%20establish%20a%20set%20of%20competitive%20baselines.%0AFurthermore%2C%20we%20improve%20on%20the%20best%20model%20by%20employing%20a%20new%20curriculum%0Alearning%20strategy%20based%20on%20data%20cartography.%20Our%20dataset%20and%20code%20to%20reproduce%0Athe%20baselines%20are%20available%20at%20https%3A//github.com/Eduard6421/RONLI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11877v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Cartography-Based%2520Curriculum%2520Learning%2520Method%2520Applied%2520on%2520RoNLI%253A%250A%2520%2520The%2520First%2520Romanian%2520Natural%2520Language%2520Inference%2520Corpus%26entry.906535625%3DEduard%2520Poesina%2520and%2520Cornelia%2520Caragea%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3D%2520%2520Natural%2520language%2520inference%2520%2528NLI%2529%252C%2520the%2520task%2520of%2520recognizing%2520the%2520entailment%250Arelationship%2520in%2520sentence%2520pairs%252C%2520is%2520an%2520actively%2520studied%2520topic%2520serving%2520as%2520a%2520proxy%250Afor%2520natural%2520language%2520understanding.%2520Despite%2520the%2520relevance%2520of%2520the%2520task%2520in%250Abuilding%2520conversational%2520agents%2520and%2520improving%2520text%2520classification%252C%2520machine%250Atranslation%2520and%2520other%2520NLP%2520tasks%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520there%2520is%2520no%250Apublicly%2520available%2520NLI%2520corpus%2520for%2520the%2520Romanian%2520language.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520the%2520first%2520Romanian%2520NLI%2520corpus%2520%2528RoNLI%2529%2520comprising%252058K%2520training%250Asentence%2520pairs%252C%2520which%2520are%2520obtained%2520via%2520distant%2520supervision%252C%2520and%25206K%2520validation%250Aand%2520test%2520sentence%2520pairs%252C%2520which%2520are%2520manually%2520annotated%2520with%2520the%2520correct%2520labels.%250AWe%2520conduct%2520experiments%2520with%2520multiple%2520machine%2520learning%2520methods%2520based%2520on%2520distant%250Alearning%252C%2520ranging%2520from%2520shallow%2520models%2520based%2520on%2520word%2520embeddings%2520to%250Atransformer-based%2520neural%2520networks%252C%2520to%2520establish%2520a%2520set%2520of%2520competitive%2520baselines.%250AFurthermore%252C%2520we%2520improve%2520on%2520the%2520best%2520model%2520by%2520employing%2520a%2520new%2520curriculum%250Alearning%2520strategy%2520based%2520on%2520data%2520cartography.%2520Our%2520dataset%2520and%2520code%2520to%2520reproduce%250Athe%2520baselines%2520are%2520available%2520at%2520https%253A//github.com/Eduard6421/RONLI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11877v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Cartography-Based%20Curriculum%20Learning%20Method%20Applied%20on%20RoNLI%3A%0A%20%20The%20First%20Romanian%20Natural%20Language%20Inference%20Corpus&entry.906535625=Eduard%20Poesina%20and%20Cornelia%20Caragea%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=%20%20Natural%20language%20inference%20%28NLI%29%2C%20the%20task%20of%20recognizing%20the%20entailment%0Arelationship%20in%20sentence%20pairs%2C%20is%20an%20actively%20studied%20topic%20serving%20as%20a%20proxy%0Afor%20natural%20language%20understanding.%20Despite%20the%20relevance%20of%20the%20task%20in%0Abuilding%20conversational%20agents%20and%20improving%20text%20classification%2C%20machine%0Atranslation%20and%20other%20NLP%20tasks%2C%20to%20the%20best%20of%20our%20knowledge%2C%20there%20is%20no%0Apublicly%20available%20NLI%20corpus%20for%20the%20Romanian%20language.%20To%20this%20end%2C%20we%0Aintroduce%20the%20first%20Romanian%20NLI%20corpus%20%28RoNLI%29%20comprising%2058K%20training%0Asentence%20pairs%2C%20which%20are%20obtained%20via%20distant%20supervision%2C%20and%206K%20validation%0Aand%20test%20sentence%20pairs%2C%20which%20are%20manually%20annotated%20with%20the%20correct%20labels.%0AWe%20conduct%20experiments%20with%20multiple%20machine%20learning%20methods%20based%20on%20distant%0Alearning%2C%20ranging%20from%20shallow%20models%20based%20on%20word%20embeddings%20to%0Atransformer-based%20neural%20networks%2C%20to%20establish%20a%20set%20of%20competitive%20baselines.%0AFurthermore%2C%20we%20improve%20on%20the%20best%20model%20by%20employing%20a%20new%20curriculum%0Alearning%20strategy%20based%20on%20data%20cartography.%20Our%20dataset%20and%20code%20to%20reproduce%0Athe%20baselines%20are%20available%20at%20https%3A//github.com/Eduard6421/RONLI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11877v4&entry.124074799=Read"},
{"title": "Learning Optimal Filters Using Variational Inference", "author": "Enoch Luk and Eviatar Bach and Ricardo Baptista and Andrew Stuart", "abstract": "  Filtering - the task of estimating the conditional distribution of states of\na dynamical system given partial, noisy, observations - is important in many\nareas of science and engineering, including weather and climate prediction.\nHowever, the filtering distribution is generally intractable to obtain for\nhigh-dimensional, nonlinear systems. Filters used in practice, such as the\nensemble Kalman filter (EnKF), are biased for nonlinear systems and have\nnumerous tuning parameters. Here, we present a framework for learning a\nparameterized analysis map - the map that takes a forecast distribution and\nobservations to the filtering distribution - using variational inference. We\nshow that this methodology can be used to learn gain matrices for filtering\nlinear and nonlinear dynamical systems, as well as inflation and localization\nparameters for an EnKF. Future work will apply this framework to learn new\nfiltering algorithms.\n", "link": "http://arxiv.org/abs/2406.18066v2", "date": "2024-08-13", "relevancy": 1.9098, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5015}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4732}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Optimal%20Filters%20Using%20Variational%20Inference&body=Title%3A%20Learning%20Optimal%20Filters%20Using%20Variational%20Inference%0AAuthor%3A%20Enoch%20Luk%20and%20Eviatar%20Bach%20and%20Ricardo%20Baptista%20and%20Andrew%20Stuart%0AAbstract%3A%20%20%20Filtering%20-%20the%20task%20of%20estimating%20the%20conditional%20distribution%20of%20states%20of%0Aa%20dynamical%20system%20given%20partial%2C%20noisy%2C%20observations%20-%20is%20important%20in%20many%0Aareas%20of%20science%20and%20engineering%2C%20including%20weather%20and%20climate%20prediction.%0AHowever%2C%20the%20filtering%20distribution%20is%20generally%20intractable%20to%20obtain%20for%0Ahigh-dimensional%2C%20nonlinear%20systems.%20Filters%20used%20in%20practice%2C%20such%20as%20the%0Aensemble%20Kalman%20filter%20%28EnKF%29%2C%20are%20biased%20for%20nonlinear%20systems%20and%20have%0Anumerous%20tuning%20parameters.%20Here%2C%20we%20present%20a%20framework%20for%20learning%20a%0Aparameterized%20analysis%20map%20-%20the%20map%20that%20takes%20a%20forecast%20distribution%20and%0Aobservations%20to%20the%20filtering%20distribution%20-%20using%20variational%20inference.%20We%0Ashow%20that%20this%20methodology%20can%20be%20used%20to%20learn%20gain%20matrices%20for%20filtering%0Alinear%20and%20nonlinear%20dynamical%20systems%2C%20as%20well%20as%20inflation%20and%20localization%0Aparameters%20for%20an%20EnKF.%20Future%20work%20will%20apply%20this%20framework%20to%20learn%20new%0Afiltering%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Optimal%2520Filters%2520Using%2520Variational%2520Inference%26entry.906535625%3DEnoch%2520Luk%2520and%2520Eviatar%2520Bach%2520and%2520Ricardo%2520Baptista%2520and%2520Andrew%2520Stuart%26entry.1292438233%3D%2520%2520Filtering%2520-%2520the%2520task%2520of%2520estimating%2520the%2520conditional%2520distribution%2520of%2520states%2520of%250Aa%2520dynamical%2520system%2520given%2520partial%252C%2520noisy%252C%2520observations%2520-%2520is%2520important%2520in%2520many%250Aareas%2520of%2520science%2520and%2520engineering%252C%2520including%2520weather%2520and%2520climate%2520prediction.%250AHowever%252C%2520the%2520filtering%2520distribution%2520is%2520generally%2520intractable%2520to%2520obtain%2520for%250Ahigh-dimensional%252C%2520nonlinear%2520systems.%2520Filters%2520used%2520in%2520practice%252C%2520such%2520as%2520the%250Aensemble%2520Kalman%2520filter%2520%2528EnKF%2529%252C%2520are%2520biased%2520for%2520nonlinear%2520systems%2520and%2520have%250Anumerous%2520tuning%2520parameters.%2520Here%252C%2520we%2520present%2520a%2520framework%2520for%2520learning%2520a%250Aparameterized%2520analysis%2520map%2520-%2520the%2520map%2520that%2520takes%2520a%2520forecast%2520distribution%2520and%250Aobservations%2520to%2520the%2520filtering%2520distribution%2520-%2520using%2520variational%2520inference.%2520We%250Ashow%2520that%2520this%2520methodology%2520can%2520be%2520used%2520to%2520learn%2520gain%2520matrices%2520for%2520filtering%250Alinear%2520and%2520nonlinear%2520dynamical%2520systems%252C%2520as%2520well%2520as%2520inflation%2520and%2520localization%250Aparameters%2520for%2520an%2520EnKF.%2520Future%2520work%2520will%2520apply%2520this%2520framework%2520to%2520learn%2520new%250Afiltering%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Optimal%20Filters%20Using%20Variational%20Inference&entry.906535625=Enoch%20Luk%20and%20Eviatar%20Bach%20and%20Ricardo%20Baptista%20and%20Andrew%20Stuart&entry.1292438233=%20%20Filtering%20-%20the%20task%20of%20estimating%20the%20conditional%20distribution%20of%20states%20of%0Aa%20dynamical%20system%20given%20partial%2C%20noisy%2C%20observations%20-%20is%20important%20in%20many%0Aareas%20of%20science%20and%20engineering%2C%20including%20weather%20and%20climate%20prediction.%0AHowever%2C%20the%20filtering%20distribution%20is%20generally%20intractable%20to%20obtain%20for%0Ahigh-dimensional%2C%20nonlinear%20systems.%20Filters%20used%20in%20practice%2C%20such%20as%20the%0Aensemble%20Kalman%20filter%20%28EnKF%29%2C%20are%20biased%20for%20nonlinear%20systems%20and%20have%0Anumerous%20tuning%20parameters.%20Here%2C%20we%20present%20a%20framework%20for%20learning%20a%0Aparameterized%20analysis%20map%20-%20the%20map%20that%20takes%20a%20forecast%20distribution%20and%0Aobservations%20to%20the%20filtering%20distribution%20-%20using%20variational%20inference.%20We%0Ashow%20that%20this%20methodology%20can%20be%20used%20to%20learn%20gain%20matrices%20for%20filtering%0Alinear%20and%20nonlinear%20dynamical%20systems%2C%20as%20well%20as%20inflation%20and%20localization%0Aparameters%20for%20an%20EnKF.%20Future%20work%20will%20apply%20this%20framework%20to%20learn%20new%0Afiltering%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18066v2&entry.124074799=Read"},
{"title": "The Physics-Informed Neural Network Gravity Model: Generation III", "author": "John Martin and Hanspeter Schaub", "abstract": "  Scientific machine learning and the advent of the Physics-Informed Neural\nNetwork (PINN) have shown high potential in their ability to solve complex\ndifferential equations. One example is the use of PINNs to solve the gravity\nfield modeling problem -- learning convenient representations of the\ngravitational potential from position and acceleration data. These PINN gravity\nmodels, or PINN-GMs, have demonstrated advantages in model compactness,\nrobustness to noise, and sample efficiency when compared to popular\nalternatives; however, further investigation has revealed various failure modes\nfor these and other machine learning gravity models which this manuscript aims\nto address. Specifically, this paper introduces the third generation\nPhysics-Informed Neural Network Gravity Model (PINN-GM-III) which includes\ndesign changes that solve the problems of feature divergence, bias towards\nlow-altitude samples, numerical instability, and extrapolation error. Six\nevaluation metrics are proposed to expose these past pitfalls and illustrate\nthe PINN-GM-III's robustness to them. This study concludes by evaluating the\nPINN-GM-III modeling accuracy on a heterogeneous density asteroid, and\ncomparing its performance to other analytic and machine learning gravity\nmodels.\n", "link": "http://arxiv.org/abs/2312.10257v2", "date": "2024-08-13", "relevancy": 1.8964, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5056}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Physics-Informed%20Neural%20Network%20Gravity%20Model%3A%20Generation%20III&body=Title%3A%20The%20Physics-Informed%20Neural%20Network%20Gravity%20Model%3A%20Generation%20III%0AAuthor%3A%20John%20Martin%20and%20Hanspeter%20Schaub%0AAbstract%3A%20%20%20Scientific%20machine%20learning%20and%20the%20advent%20of%20the%20Physics-Informed%20Neural%0ANetwork%20%28PINN%29%20have%20shown%20high%20potential%20in%20their%20ability%20to%20solve%20complex%0Adifferential%20equations.%20One%20example%20is%20the%20use%20of%20PINNs%20to%20solve%20the%20gravity%0Afield%20modeling%20problem%20--%20learning%20convenient%20representations%20of%20the%0Agravitational%20potential%20from%20position%20and%20acceleration%20data.%20These%20PINN%20gravity%0Amodels%2C%20or%20PINN-GMs%2C%20have%20demonstrated%20advantages%20in%20model%20compactness%2C%0Arobustness%20to%20noise%2C%20and%20sample%20efficiency%20when%20compared%20to%20popular%0Aalternatives%3B%20however%2C%20further%20investigation%20has%20revealed%20various%20failure%20modes%0Afor%20these%20and%20other%20machine%20learning%20gravity%20models%20which%20this%20manuscript%20aims%0Ato%20address.%20Specifically%2C%20this%20paper%20introduces%20the%20third%20generation%0APhysics-Informed%20Neural%20Network%20Gravity%20Model%20%28PINN-GM-III%29%20which%20includes%0Adesign%20changes%20that%20solve%20the%20problems%20of%20feature%20divergence%2C%20bias%20towards%0Alow-altitude%20samples%2C%20numerical%20instability%2C%20and%20extrapolation%20error.%20Six%0Aevaluation%20metrics%20are%20proposed%20to%20expose%20these%20past%20pitfalls%20and%20illustrate%0Athe%20PINN-GM-III%27s%20robustness%20to%20them.%20This%20study%20concludes%20by%20evaluating%20the%0APINN-GM-III%20modeling%20accuracy%20on%20a%20heterogeneous%20density%20asteroid%2C%20and%0Acomparing%20its%20performance%20to%20other%20analytic%20and%20machine%20learning%20gravity%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Physics-Informed%2520Neural%2520Network%2520Gravity%2520Model%253A%2520Generation%2520III%26entry.906535625%3DJohn%2520Martin%2520and%2520Hanspeter%2520Schaub%26entry.1292438233%3D%2520%2520Scientific%2520machine%2520learning%2520and%2520the%2520advent%2520of%2520the%2520Physics-Informed%2520Neural%250ANetwork%2520%2528PINN%2529%2520have%2520shown%2520high%2520potential%2520in%2520their%2520ability%2520to%2520solve%2520complex%250Adifferential%2520equations.%2520One%2520example%2520is%2520the%2520use%2520of%2520PINNs%2520to%2520solve%2520the%2520gravity%250Afield%2520modeling%2520problem%2520--%2520learning%2520convenient%2520representations%2520of%2520the%250Agravitational%2520potential%2520from%2520position%2520and%2520acceleration%2520data.%2520These%2520PINN%2520gravity%250Amodels%252C%2520or%2520PINN-GMs%252C%2520have%2520demonstrated%2520advantages%2520in%2520model%2520compactness%252C%250Arobustness%2520to%2520noise%252C%2520and%2520sample%2520efficiency%2520when%2520compared%2520to%2520popular%250Aalternatives%253B%2520however%252C%2520further%2520investigation%2520has%2520revealed%2520various%2520failure%2520modes%250Afor%2520these%2520and%2520other%2520machine%2520learning%2520gravity%2520models%2520which%2520this%2520manuscript%2520aims%250Ato%2520address.%2520Specifically%252C%2520this%2520paper%2520introduces%2520the%2520third%2520generation%250APhysics-Informed%2520Neural%2520Network%2520Gravity%2520Model%2520%2528PINN-GM-III%2529%2520which%2520includes%250Adesign%2520changes%2520that%2520solve%2520the%2520problems%2520of%2520feature%2520divergence%252C%2520bias%2520towards%250Alow-altitude%2520samples%252C%2520numerical%2520instability%252C%2520and%2520extrapolation%2520error.%2520Six%250Aevaluation%2520metrics%2520are%2520proposed%2520to%2520expose%2520these%2520past%2520pitfalls%2520and%2520illustrate%250Athe%2520PINN-GM-III%2527s%2520robustness%2520to%2520them.%2520This%2520study%2520concludes%2520by%2520evaluating%2520the%250APINN-GM-III%2520modeling%2520accuracy%2520on%2520a%2520heterogeneous%2520density%2520asteroid%252C%2520and%250Acomparing%2520its%2520performance%2520to%2520other%2520analytic%2520and%2520machine%2520learning%2520gravity%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Physics-Informed%20Neural%20Network%20Gravity%20Model%3A%20Generation%20III&entry.906535625=John%20Martin%20and%20Hanspeter%20Schaub&entry.1292438233=%20%20Scientific%20machine%20learning%20and%20the%20advent%20of%20the%20Physics-Informed%20Neural%0ANetwork%20%28PINN%29%20have%20shown%20high%20potential%20in%20their%20ability%20to%20solve%20complex%0Adifferential%20equations.%20One%20example%20is%20the%20use%20of%20PINNs%20to%20solve%20the%20gravity%0Afield%20modeling%20problem%20--%20learning%20convenient%20representations%20of%20the%0Agravitational%20potential%20from%20position%20and%20acceleration%20data.%20These%20PINN%20gravity%0Amodels%2C%20or%20PINN-GMs%2C%20have%20demonstrated%20advantages%20in%20model%20compactness%2C%0Arobustness%20to%20noise%2C%20and%20sample%20efficiency%20when%20compared%20to%20popular%0Aalternatives%3B%20however%2C%20further%20investigation%20has%20revealed%20various%20failure%20modes%0Afor%20these%20and%20other%20machine%20learning%20gravity%20models%20which%20this%20manuscript%20aims%0Ato%20address.%20Specifically%2C%20this%20paper%20introduces%20the%20third%20generation%0APhysics-Informed%20Neural%20Network%20Gravity%20Model%20%28PINN-GM-III%29%20which%20includes%0Adesign%20changes%20that%20solve%20the%20problems%20of%20feature%20divergence%2C%20bias%20towards%0Alow-altitude%20samples%2C%20numerical%20instability%2C%20and%20extrapolation%20error.%20Six%0Aevaluation%20metrics%20are%20proposed%20to%20expose%20these%20past%20pitfalls%20and%20illustrate%0Athe%20PINN-GM-III%27s%20robustness%20to%20them.%20This%20study%20concludes%20by%20evaluating%20the%0APINN-GM-III%20modeling%20accuracy%20on%20a%20heterogeneous%20density%20asteroid%2C%20and%0Acomparing%20its%20performance%20to%20other%20analytic%20and%20machine%20learning%20gravity%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10257v2&entry.124074799=Read"},
{"title": "An Ensemble Score Filter for Tracking High-Dimensional Nonlinear\n  Dynamical Systems", "author": "Feng Bao and Zezhong Zhang and Guannan Zhang", "abstract": "  We propose an ensemble score filter (EnSF) for solving high-dimensional\nnonlinear filtering problems with superior accuracy. A major drawback of\nexisting filtering methods, e.g., particle filters or ensemble Kalman filters,\nis the low accuracy in handling high-dimensional and highly nonlinear problems.\nEnSF attacks this challenge by exploiting the score-based diffusion model,\ndefined in a pseudo-temporal domain, to characterizing the evolution of the\nfiltering density. EnSF stores the information of the recursively updated\nfiltering density function in the score function, instead of storing the\ninformation in a set of finite Monte Carlo samples (used in particle filters\nand ensemble Kalman filters). Unlike existing diffusion models that train\nneural networks to approximate the score function, we develop a training-free\nscore estimation that uses a mini-batch-based Monte Carlo estimator to directly\napproximate the score function at any pseudo-spatial-temporal location, which\nprovides sufficient accuracy in solving high-dimensional nonlinear problems as\nwell as saves a tremendous amount of time spent on training neural networks.\nHigh-dimensional Lorenz-96 systems are used to demonstrate the performance of\nour method. EnSF provides surprising performance, compared with the\nstate-of-the-art Local Ensemble Transform Kalman Filter method, in reliably and\nefficiently tracking extremely high-dimensional Lorenz systems (up to 1,000,000\ndimensions) with highly nonlinear observation processes.\n", "link": "http://arxiv.org/abs/2309.00983v2", "date": "2024-08-13", "relevancy": 1.8901, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4835}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4705}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Ensemble%20Score%20Filter%20for%20Tracking%20High-Dimensional%20Nonlinear%0A%20%20Dynamical%20Systems&body=Title%3A%20An%20Ensemble%20Score%20Filter%20for%20Tracking%20High-Dimensional%20Nonlinear%0A%20%20Dynamical%20Systems%0AAuthor%3A%20Feng%20Bao%20and%20Zezhong%20Zhang%20and%20Guannan%20Zhang%0AAbstract%3A%20%20%20We%20propose%20an%20ensemble%20score%20filter%20%28EnSF%29%20for%20solving%20high-dimensional%0Anonlinear%20filtering%20problems%20with%20superior%20accuracy.%20A%20major%20drawback%20of%0Aexisting%20filtering%20methods%2C%20e.g.%2C%20particle%20filters%20or%20ensemble%20Kalman%20filters%2C%0Ais%20the%20low%20accuracy%20in%20handling%20high-dimensional%20and%20highly%20nonlinear%20problems.%0AEnSF%20attacks%20this%20challenge%20by%20exploiting%20the%20score-based%20diffusion%20model%2C%0Adefined%20in%20a%20pseudo-temporal%20domain%2C%20to%20characterizing%20the%20evolution%20of%20the%0Afiltering%20density.%20EnSF%20stores%20the%20information%20of%20the%20recursively%20updated%0Afiltering%20density%20function%20in%20the%20score%20function%2C%20instead%20of%20storing%20the%0Ainformation%20in%20a%20set%20of%20finite%20Monte%20Carlo%20samples%20%28used%20in%20particle%20filters%0Aand%20ensemble%20Kalman%20filters%29.%20Unlike%20existing%20diffusion%20models%20that%20train%0Aneural%20networks%20to%20approximate%20the%20score%20function%2C%20we%20develop%20a%20training-free%0Ascore%20estimation%20that%20uses%20a%20mini-batch-based%20Monte%20Carlo%20estimator%20to%20directly%0Aapproximate%20the%20score%20function%20at%20any%20pseudo-spatial-temporal%20location%2C%20which%0Aprovides%20sufficient%20accuracy%20in%20solving%20high-dimensional%20nonlinear%20problems%20as%0Awell%20as%20saves%20a%20tremendous%20amount%20of%20time%20spent%20on%20training%20neural%20networks.%0AHigh-dimensional%20Lorenz-96%20systems%20are%20used%20to%20demonstrate%20the%20performance%20of%0Aour%20method.%20EnSF%20provides%20surprising%20performance%2C%20compared%20with%20the%0Astate-of-the-art%20Local%20Ensemble%20Transform%20Kalman%20Filter%20method%2C%20in%20reliably%20and%0Aefficiently%20tracking%20extremely%20high-dimensional%20Lorenz%20systems%20%28up%20to%201%2C000%2C000%0Adimensions%29%20with%20highly%20nonlinear%20observation%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Ensemble%2520Score%2520Filter%2520for%2520Tracking%2520High-Dimensional%2520Nonlinear%250A%2520%2520Dynamical%2520Systems%26entry.906535625%3DFeng%2520Bao%2520and%2520Zezhong%2520Zhang%2520and%2520Guannan%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520ensemble%2520score%2520filter%2520%2528EnSF%2529%2520for%2520solving%2520high-dimensional%250Anonlinear%2520filtering%2520problems%2520with%2520superior%2520accuracy.%2520A%2520major%2520drawback%2520of%250Aexisting%2520filtering%2520methods%252C%2520e.g.%252C%2520particle%2520filters%2520or%2520ensemble%2520Kalman%2520filters%252C%250Ais%2520the%2520low%2520accuracy%2520in%2520handling%2520high-dimensional%2520and%2520highly%2520nonlinear%2520problems.%250AEnSF%2520attacks%2520this%2520challenge%2520by%2520exploiting%2520the%2520score-based%2520diffusion%2520model%252C%250Adefined%2520in%2520a%2520pseudo-temporal%2520domain%252C%2520to%2520characterizing%2520the%2520evolution%2520of%2520the%250Afiltering%2520density.%2520EnSF%2520stores%2520the%2520information%2520of%2520the%2520recursively%2520updated%250Afiltering%2520density%2520function%2520in%2520the%2520score%2520function%252C%2520instead%2520of%2520storing%2520the%250Ainformation%2520in%2520a%2520set%2520of%2520finite%2520Monte%2520Carlo%2520samples%2520%2528used%2520in%2520particle%2520filters%250Aand%2520ensemble%2520Kalman%2520filters%2529.%2520Unlike%2520existing%2520diffusion%2520models%2520that%2520train%250Aneural%2520networks%2520to%2520approximate%2520the%2520score%2520function%252C%2520we%2520develop%2520a%2520training-free%250Ascore%2520estimation%2520that%2520uses%2520a%2520mini-batch-based%2520Monte%2520Carlo%2520estimator%2520to%2520directly%250Aapproximate%2520the%2520score%2520function%2520at%2520any%2520pseudo-spatial-temporal%2520location%252C%2520which%250Aprovides%2520sufficient%2520accuracy%2520in%2520solving%2520high-dimensional%2520nonlinear%2520problems%2520as%250Awell%2520as%2520saves%2520a%2520tremendous%2520amount%2520of%2520time%2520spent%2520on%2520training%2520neural%2520networks.%250AHigh-dimensional%2520Lorenz-96%2520systems%2520are%2520used%2520to%2520demonstrate%2520the%2520performance%2520of%250Aour%2520method.%2520EnSF%2520provides%2520surprising%2520performance%252C%2520compared%2520with%2520the%250Astate-of-the-art%2520Local%2520Ensemble%2520Transform%2520Kalman%2520Filter%2520method%252C%2520in%2520reliably%2520and%250Aefficiently%2520tracking%2520extremely%2520high-dimensional%2520Lorenz%2520systems%2520%2528up%2520to%25201%252C000%252C000%250Adimensions%2529%2520with%2520highly%2520nonlinear%2520observation%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Ensemble%20Score%20Filter%20for%20Tracking%20High-Dimensional%20Nonlinear%0A%20%20Dynamical%20Systems&entry.906535625=Feng%20Bao%20and%20Zezhong%20Zhang%20and%20Guannan%20Zhang&entry.1292438233=%20%20We%20propose%20an%20ensemble%20score%20filter%20%28EnSF%29%20for%20solving%20high-dimensional%0Anonlinear%20filtering%20problems%20with%20superior%20accuracy.%20A%20major%20drawback%20of%0Aexisting%20filtering%20methods%2C%20e.g.%2C%20particle%20filters%20or%20ensemble%20Kalman%20filters%2C%0Ais%20the%20low%20accuracy%20in%20handling%20high-dimensional%20and%20highly%20nonlinear%20problems.%0AEnSF%20attacks%20this%20challenge%20by%20exploiting%20the%20score-based%20diffusion%20model%2C%0Adefined%20in%20a%20pseudo-temporal%20domain%2C%20to%20characterizing%20the%20evolution%20of%20the%0Afiltering%20density.%20EnSF%20stores%20the%20information%20of%20the%20recursively%20updated%0Afiltering%20density%20function%20in%20the%20score%20function%2C%20instead%20of%20storing%20the%0Ainformation%20in%20a%20set%20of%20finite%20Monte%20Carlo%20samples%20%28used%20in%20particle%20filters%0Aand%20ensemble%20Kalman%20filters%29.%20Unlike%20existing%20diffusion%20models%20that%20train%0Aneural%20networks%20to%20approximate%20the%20score%20function%2C%20we%20develop%20a%20training-free%0Ascore%20estimation%20that%20uses%20a%20mini-batch-based%20Monte%20Carlo%20estimator%20to%20directly%0Aapproximate%20the%20score%20function%20at%20any%20pseudo-spatial-temporal%20location%2C%20which%0Aprovides%20sufficient%20accuracy%20in%20solving%20high-dimensional%20nonlinear%20problems%20as%0Awell%20as%20saves%20a%20tremendous%20amount%20of%20time%20spent%20on%20training%20neural%20networks.%0AHigh-dimensional%20Lorenz-96%20systems%20are%20used%20to%20demonstrate%20the%20performance%20of%0Aour%20method.%20EnSF%20provides%20surprising%20performance%2C%20compared%20with%20the%0Astate-of-the-art%20Local%20Ensemble%20Transform%20Kalman%20Filter%20method%2C%20in%20reliably%20and%0Aefficiently%20tracking%20extremely%20high-dimensional%20Lorenz%20systems%20%28up%20to%201%2C000%2C000%0Adimensions%29%20with%20highly%20nonlinear%20observation%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00983v2&entry.124074799=Read"},
{"title": "A Universal Flexible Near-sensor Neuromorphic Tactile System with\n  Multi-threshold strategy for Pressure Characteristic Detection", "author": "Jialin Liu and Diansheng Liao", "abstract": "  Constructing the new generation information processing system by mimicking\nbiological nervous system is a feasible way for implement of high-efficient\nintelligent sensing device and bionic robot. However, most biological nervous\nsystem, especially the tactile system, have various powerful functions. This is\na big challenge for bionic system design. Here we report a universal fully\nflexible neuromorphic tactile perception system with strong compatibility and a\nmultithreshold signal processing strategy. Like nervous system, signal in our\nsystem is transmitted as pulses and processed as threshold information. For\nfeasibility verification, recognition of three different type pressure signals\n(continuous changing signal, Morse code signal and symbol pattern) is tested\nrespectively. Our system can output trend of these signals accurately and have\na high accuracy in the recognition of symbol pattern and Morse code. Comparing\nto conventional system, consumption of our system significantly decreases in a\nsame recognition task. Meanwhile, we give the detail introduction and\ndemonstration of our system universality.\n", "link": "http://arxiv.org/abs/2408.05846v2", "date": "2024-08-13", "relevancy": 1.8433, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5344}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4534}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Flexible%20Near-sensor%20Neuromorphic%20Tactile%20System%20with%0A%20%20Multi-threshold%20strategy%20for%20Pressure%20Characteristic%20Detection&body=Title%3A%20A%20Universal%20Flexible%20Near-sensor%20Neuromorphic%20Tactile%20System%20with%0A%20%20Multi-threshold%20strategy%20for%20Pressure%20Characteristic%20Detection%0AAuthor%3A%20Jialin%20Liu%20and%20Diansheng%20Liao%0AAbstract%3A%20%20%20Constructing%20the%20new%20generation%20information%20processing%20system%20by%20mimicking%0Abiological%20nervous%20system%20is%20a%20feasible%20way%20for%20implement%20of%20high-efficient%0Aintelligent%20sensing%20device%20and%20bionic%20robot.%20However%2C%20most%20biological%20nervous%0Asystem%2C%20especially%20the%20tactile%20system%2C%20have%20various%20powerful%20functions.%20This%20is%0Aa%20big%20challenge%20for%20bionic%20system%20design.%20Here%20we%20report%20a%20universal%20fully%0Aflexible%20neuromorphic%20tactile%20perception%20system%20with%20strong%20compatibility%20and%20a%0Amultithreshold%20signal%20processing%20strategy.%20Like%20nervous%20system%2C%20signal%20in%20our%0Asystem%20is%20transmitted%20as%20pulses%20and%20processed%20as%20threshold%20information.%20For%0Afeasibility%20verification%2C%20recognition%20of%20three%20different%20type%20pressure%20signals%0A%28continuous%20changing%20signal%2C%20Morse%20code%20signal%20and%20symbol%20pattern%29%20is%20tested%0Arespectively.%20Our%20system%20can%20output%20trend%20of%20these%20signals%20accurately%20and%20have%0Aa%20high%20accuracy%20in%20the%20recognition%20of%20symbol%20pattern%20and%20Morse%20code.%20Comparing%0Ato%20conventional%20system%2C%20consumption%20of%20our%20system%20significantly%20decreases%20in%20a%0Asame%20recognition%20task.%20Meanwhile%2C%20we%20give%20the%20detail%20introduction%20and%0Ademonstration%20of%20our%20system%20universality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Universal%2520Flexible%2520Near-sensor%2520Neuromorphic%2520Tactile%2520System%2520with%250A%2520%2520Multi-threshold%2520strategy%2520for%2520Pressure%2520Characteristic%2520Detection%26entry.906535625%3DJialin%2520Liu%2520and%2520Diansheng%2520Liao%26entry.1292438233%3D%2520%2520Constructing%2520the%2520new%2520generation%2520information%2520processing%2520system%2520by%2520mimicking%250Abiological%2520nervous%2520system%2520is%2520a%2520feasible%2520way%2520for%2520implement%2520of%2520high-efficient%250Aintelligent%2520sensing%2520device%2520and%2520bionic%2520robot.%2520However%252C%2520most%2520biological%2520nervous%250Asystem%252C%2520especially%2520the%2520tactile%2520system%252C%2520have%2520various%2520powerful%2520functions.%2520This%2520is%250Aa%2520big%2520challenge%2520for%2520bionic%2520system%2520design.%2520Here%2520we%2520report%2520a%2520universal%2520fully%250Aflexible%2520neuromorphic%2520tactile%2520perception%2520system%2520with%2520strong%2520compatibility%2520and%2520a%250Amultithreshold%2520signal%2520processing%2520strategy.%2520Like%2520nervous%2520system%252C%2520signal%2520in%2520our%250Asystem%2520is%2520transmitted%2520as%2520pulses%2520and%2520processed%2520as%2520threshold%2520information.%2520For%250Afeasibility%2520verification%252C%2520recognition%2520of%2520three%2520different%2520type%2520pressure%2520signals%250A%2528continuous%2520changing%2520signal%252C%2520Morse%2520code%2520signal%2520and%2520symbol%2520pattern%2529%2520is%2520tested%250Arespectively.%2520Our%2520system%2520can%2520output%2520trend%2520of%2520these%2520signals%2520accurately%2520and%2520have%250Aa%2520high%2520accuracy%2520in%2520the%2520recognition%2520of%2520symbol%2520pattern%2520and%2520Morse%2520code.%2520Comparing%250Ato%2520conventional%2520system%252C%2520consumption%2520of%2520our%2520system%2520significantly%2520decreases%2520in%2520a%250Asame%2520recognition%2520task.%2520Meanwhile%252C%2520we%2520give%2520the%2520detail%2520introduction%2520and%250Ademonstration%2520of%2520our%2520system%2520universality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Flexible%20Near-sensor%20Neuromorphic%20Tactile%20System%20with%0A%20%20Multi-threshold%20strategy%20for%20Pressure%20Characteristic%20Detection&entry.906535625=Jialin%20Liu%20and%20Diansheng%20Liao&entry.1292438233=%20%20Constructing%20the%20new%20generation%20information%20processing%20system%20by%20mimicking%0Abiological%20nervous%20system%20is%20a%20feasible%20way%20for%20implement%20of%20high-efficient%0Aintelligent%20sensing%20device%20and%20bionic%20robot.%20However%2C%20most%20biological%20nervous%0Asystem%2C%20especially%20the%20tactile%20system%2C%20have%20various%20powerful%20functions.%20This%20is%0Aa%20big%20challenge%20for%20bionic%20system%20design.%20Here%20we%20report%20a%20universal%20fully%0Aflexible%20neuromorphic%20tactile%20perception%20system%20with%20strong%20compatibility%20and%20a%0Amultithreshold%20signal%20processing%20strategy.%20Like%20nervous%20system%2C%20signal%20in%20our%0Asystem%20is%20transmitted%20as%20pulses%20and%20processed%20as%20threshold%20information.%20For%0Afeasibility%20verification%2C%20recognition%20of%20three%20different%20type%20pressure%20signals%0A%28continuous%20changing%20signal%2C%20Morse%20code%20signal%20and%20symbol%20pattern%29%20is%20tested%0Arespectively.%20Our%20system%20can%20output%20trend%20of%20these%20signals%20accurately%20and%20have%0Aa%20high%20accuracy%20in%20the%20recognition%20of%20symbol%20pattern%20and%20Morse%20code.%20Comparing%0Ato%20conventional%20system%2C%20consumption%20of%20our%20system%20significantly%20decreases%20in%20a%0Asame%20recognition%20task.%20Meanwhile%2C%20we%20give%20the%20detail%20introduction%20and%0Ademonstration%20of%20our%20system%20universality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05846v2&entry.124074799=Read"},
{"title": "A Practical Solver for Scalar Data Topological Simplification", "author": "Mohamed Kissi and Mathieu Pont and Joshua A. Levine and Julien Tierny", "abstract": "  This paper presents a practical approach for the optimization of topological\nsimplification, a central pre-processing step for the analysis and\nvisualization of scalar data. Given an input scalar field f and a set of\n\"signal\" persistence pairs to maintain, our approach produces an output field g\nthat is close to f and which optimizes (i) the cancellation of \"non-signal\"\npairs, while (ii) preserving the \"signal\" pairs. In contrast to pre-existing\nsimplification algorithms, our approach is not restricted to persistence pairs\ninvolving extrema and can thus address a larger class of topological features,\nin particular saddle pairs in three-dimensional scalar data. Our approach\nleverages recent generic persistence optimization frameworks and extends them\nwith tailored accelerations specific to the problem of topological\nsimplification. Extensive experiments report substantial accelerations over\nthese frameworks, thereby making topological simplification optimization\npractical for real-life datasets. Our approach enables a direct visualization\nand analysis of the topologically simplified data, e.g., via isosurfaces of\nsimplified topology (fewer components and handles). We apply our approach to\nthe extraction of prominent filament structures in three-dimensional data.\nSpecifically, we show that our pre-simplification of the data leads to\npractical improvements over standard topological techniques for removing\nfilament loops. We also show how our approach can be used to repair genus\ndefects in surface processing. Finally, we provide a C++ implementation for\nreproducibility purposes.\n", "link": "http://arxiv.org/abs/2407.12399v2", "date": "2024-08-13", "relevancy": 1.8356, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4722}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4559}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Practical%20Solver%20for%20Scalar%20Data%20Topological%20Simplification&body=Title%3A%20A%20Practical%20Solver%20for%20Scalar%20Data%20Topological%20Simplification%0AAuthor%3A%20Mohamed%20Kissi%20and%20Mathieu%20Pont%20and%20Joshua%20A.%20Levine%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20practical%20approach%20for%20the%20optimization%20of%20topological%0Asimplification%2C%20a%20central%20pre-processing%20step%20for%20the%20analysis%20and%0Avisualization%20of%20scalar%20data.%20Given%20an%20input%20scalar%20field%20f%20and%20a%20set%20of%0A%22signal%22%20persistence%20pairs%20to%20maintain%2C%20our%20approach%20produces%20an%20output%20field%20g%0Athat%20is%20close%20to%20f%20and%20which%20optimizes%20%28i%29%20the%20cancellation%20of%20%22non-signal%22%0Apairs%2C%20while%20%28ii%29%20preserving%20the%20%22signal%22%20pairs.%20In%20contrast%20to%20pre-existing%0Asimplification%20algorithms%2C%20our%20approach%20is%20not%20restricted%20to%20persistence%20pairs%0Ainvolving%20extrema%20and%20can%20thus%20address%20a%20larger%20class%20of%20topological%20features%2C%0Ain%20particular%20saddle%20pairs%20in%20three-dimensional%20scalar%20data.%20Our%20approach%0Aleverages%20recent%20generic%20persistence%20optimization%20frameworks%20and%20extends%20them%0Awith%20tailored%20accelerations%20specific%20to%20the%20problem%20of%20topological%0Asimplification.%20Extensive%20experiments%20report%20substantial%20accelerations%20over%0Athese%20frameworks%2C%20thereby%20making%20topological%20simplification%20optimization%0Apractical%20for%20real-life%20datasets.%20Our%20approach%20enables%20a%20direct%20visualization%0Aand%20analysis%20of%20the%20topologically%20simplified%20data%2C%20e.g.%2C%20via%20isosurfaces%20of%0Asimplified%20topology%20%28fewer%20components%20and%20handles%29.%20We%20apply%20our%20approach%20to%0Athe%20extraction%20of%20prominent%20filament%20structures%20in%20three-dimensional%20data.%0ASpecifically%2C%20we%20show%20that%20our%20pre-simplification%20of%20the%20data%20leads%20to%0Apractical%20improvements%20over%20standard%20topological%20techniques%20for%20removing%0Afilament%20loops.%20We%20also%20show%20how%20our%20approach%20can%20be%20used%20to%20repair%20genus%0Adefects%20in%20surface%20processing.%20Finally%2C%20we%20provide%20a%20C%2B%2B%20implementation%20for%0Areproducibility%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Practical%2520Solver%2520for%2520Scalar%2520Data%2520Topological%2520Simplification%26entry.906535625%3DMohamed%2520Kissi%2520and%2520Mathieu%2520Pont%2520and%2520Joshua%2520A.%2520Levine%2520and%2520Julien%2520Tierny%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520practical%2520approach%2520for%2520the%2520optimization%2520of%2520topological%250Asimplification%252C%2520a%2520central%2520pre-processing%2520step%2520for%2520the%2520analysis%2520and%250Avisualization%2520of%2520scalar%2520data.%2520Given%2520an%2520input%2520scalar%2520field%2520f%2520and%2520a%2520set%2520of%250A%2522signal%2522%2520persistence%2520pairs%2520to%2520maintain%252C%2520our%2520approach%2520produces%2520an%2520output%2520field%2520g%250Athat%2520is%2520close%2520to%2520f%2520and%2520which%2520optimizes%2520%2528i%2529%2520the%2520cancellation%2520of%2520%2522non-signal%2522%250Apairs%252C%2520while%2520%2528ii%2529%2520preserving%2520the%2520%2522signal%2522%2520pairs.%2520In%2520contrast%2520to%2520pre-existing%250Asimplification%2520algorithms%252C%2520our%2520approach%2520is%2520not%2520restricted%2520to%2520persistence%2520pairs%250Ainvolving%2520extrema%2520and%2520can%2520thus%2520address%2520a%2520larger%2520class%2520of%2520topological%2520features%252C%250Ain%2520particular%2520saddle%2520pairs%2520in%2520three-dimensional%2520scalar%2520data.%2520Our%2520approach%250Aleverages%2520recent%2520generic%2520persistence%2520optimization%2520frameworks%2520and%2520extends%2520them%250Awith%2520tailored%2520accelerations%2520specific%2520to%2520the%2520problem%2520of%2520topological%250Asimplification.%2520Extensive%2520experiments%2520report%2520substantial%2520accelerations%2520over%250Athese%2520frameworks%252C%2520thereby%2520making%2520topological%2520simplification%2520optimization%250Apractical%2520for%2520real-life%2520datasets.%2520Our%2520approach%2520enables%2520a%2520direct%2520visualization%250Aand%2520analysis%2520of%2520the%2520topologically%2520simplified%2520data%252C%2520e.g.%252C%2520via%2520isosurfaces%2520of%250Asimplified%2520topology%2520%2528fewer%2520components%2520and%2520handles%2529.%2520We%2520apply%2520our%2520approach%2520to%250Athe%2520extraction%2520of%2520prominent%2520filament%2520structures%2520in%2520three-dimensional%2520data.%250ASpecifically%252C%2520we%2520show%2520that%2520our%2520pre-simplification%2520of%2520the%2520data%2520leads%2520to%250Apractical%2520improvements%2520over%2520standard%2520topological%2520techniques%2520for%2520removing%250Afilament%2520loops.%2520We%2520also%2520show%2520how%2520our%2520approach%2520can%2520be%2520used%2520to%2520repair%2520genus%250Adefects%2520in%2520surface%2520processing.%2520Finally%252C%2520we%2520provide%2520a%2520C%252B%252B%2520implementation%2520for%250Areproducibility%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Practical%20Solver%20for%20Scalar%20Data%20Topological%20Simplification&entry.906535625=Mohamed%20Kissi%20and%20Mathieu%20Pont%20and%20Joshua%20A.%20Levine%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20paper%20presents%20a%20practical%20approach%20for%20the%20optimization%20of%20topological%0Asimplification%2C%20a%20central%20pre-processing%20step%20for%20the%20analysis%20and%0Avisualization%20of%20scalar%20data.%20Given%20an%20input%20scalar%20field%20f%20and%20a%20set%20of%0A%22signal%22%20persistence%20pairs%20to%20maintain%2C%20our%20approach%20produces%20an%20output%20field%20g%0Athat%20is%20close%20to%20f%20and%20which%20optimizes%20%28i%29%20the%20cancellation%20of%20%22non-signal%22%0Apairs%2C%20while%20%28ii%29%20preserving%20the%20%22signal%22%20pairs.%20In%20contrast%20to%20pre-existing%0Asimplification%20algorithms%2C%20our%20approach%20is%20not%20restricted%20to%20persistence%20pairs%0Ainvolving%20extrema%20and%20can%20thus%20address%20a%20larger%20class%20of%20topological%20features%2C%0Ain%20particular%20saddle%20pairs%20in%20three-dimensional%20scalar%20data.%20Our%20approach%0Aleverages%20recent%20generic%20persistence%20optimization%20frameworks%20and%20extends%20them%0Awith%20tailored%20accelerations%20specific%20to%20the%20problem%20of%20topological%0Asimplification.%20Extensive%20experiments%20report%20substantial%20accelerations%20over%0Athese%20frameworks%2C%20thereby%20making%20topological%20simplification%20optimization%0Apractical%20for%20real-life%20datasets.%20Our%20approach%20enables%20a%20direct%20visualization%0Aand%20analysis%20of%20the%20topologically%20simplified%20data%2C%20e.g.%2C%20via%20isosurfaces%20of%0Asimplified%20topology%20%28fewer%20components%20and%20handles%29.%20We%20apply%20our%20approach%20to%0Athe%20extraction%20of%20prominent%20filament%20structures%20in%20three-dimensional%20data.%0ASpecifically%2C%20we%20show%20that%20our%20pre-simplification%20of%20the%20data%20leads%20to%0Apractical%20improvements%20over%20standard%20topological%20techniques%20for%20removing%0Afilament%20loops.%20We%20also%20show%20how%20our%20approach%20can%20be%20used%20to%20repair%20genus%0Adefects%20in%20surface%20processing.%20Finally%2C%20we%20provide%20a%20C%2B%2B%20implementation%20for%0Areproducibility%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12399v2&entry.124074799=Read"},
{"title": "A Novel Computational and Modeling Foundation for Automatic Coherence\n  Assessment", "author": "Aviya Maimon and Reut Tsarfaty", "abstract": "  Coherence is an essential property of well-written texts, that refers to the\nway textual units relate to one another. In the era of generative AI, coherence\nassessment is essential for many NLP tasks; summarization, generation,\nlong-form question-answering, and more. However, in NLP {coherence} is an\nill-defined notion, not having a formal definition or evaluation metrics, that\nwould allow for large-scale automatic and systematic coherence assessment. To\nbridge this gap, in this work we employ the formal linguistic definition of\n\\citet{Reinhart:1980} of what makes a discourse coherent, consisting of three\nconditions -- {\\em cohesion, consistency} and {\\em relevance} -- and formalize\nthese conditions as respective computational tasks. We hypothesize that (i) a\nmodel trained on all of these tasks will learn the features required for\ncoherence detection, and that (ii) a joint model for all tasks will exceed the\nperformance of models trained on each task individually. On two benchmarks for\ncoherence scoring rated by humans, one containing 500 automatically-generated\nshort stories and another containing 4k real-world texts, our experiments\nconfirm that jointly training on the proposed tasks leads to better performance\non each task compared with task-specific models, and to better performance on\nassessing coherence overall, compared with strong baselines. We conclude that\nthe formal and computational setup of coherence as proposed here provides a\nsolid foundation for advanced methods of large-scale automatic assessment of\ncoherence.\n", "link": "http://arxiv.org/abs/2310.00598v2", "date": "2024-08-13", "relevancy": 1.8302, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4632}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Computational%20and%20Modeling%20Foundation%20for%20Automatic%20Coherence%0A%20%20Assessment&body=Title%3A%20A%20Novel%20Computational%20and%20Modeling%20Foundation%20for%20Automatic%20Coherence%0A%20%20Assessment%0AAuthor%3A%20Aviya%20Maimon%20and%20Reut%20Tsarfaty%0AAbstract%3A%20%20%20Coherence%20is%20an%20essential%20property%20of%20well-written%20texts%2C%20that%20refers%20to%20the%0Away%20textual%20units%20relate%20to%20one%20another.%20In%20the%20era%20of%20generative%20AI%2C%20coherence%0Aassessment%20is%20essential%20for%20many%20NLP%20tasks%3B%20summarization%2C%20generation%2C%0Along-form%20question-answering%2C%20and%20more.%20However%2C%20in%20NLP%20%7Bcoherence%7D%20is%20an%0Aill-defined%20notion%2C%20not%20having%20a%20formal%20definition%20or%20evaluation%20metrics%2C%20that%0Awould%20allow%20for%20large-scale%20automatic%20and%20systematic%20coherence%20assessment.%20To%0Abridge%20this%20gap%2C%20in%20this%20work%20we%20employ%20the%20formal%20linguistic%20definition%20of%0A%5Ccitet%7BReinhart%3A1980%7D%20of%20what%20makes%20a%20discourse%20coherent%2C%20consisting%20of%20three%0Aconditions%20--%20%7B%5Cem%20cohesion%2C%20consistency%7D%20and%20%7B%5Cem%20relevance%7D%20--%20and%20formalize%0Athese%20conditions%20as%20respective%20computational%20tasks.%20We%20hypothesize%20that%20%28i%29%20a%0Amodel%20trained%20on%20all%20of%20these%20tasks%20will%20learn%20the%20features%20required%20for%0Acoherence%20detection%2C%20and%20that%20%28ii%29%20a%20joint%20model%20for%20all%20tasks%20will%20exceed%20the%0Aperformance%20of%20models%20trained%20on%20each%20task%20individually.%20On%20two%20benchmarks%20for%0Acoherence%20scoring%20rated%20by%20humans%2C%20one%20containing%20500%20automatically-generated%0Ashort%20stories%20and%20another%20containing%204k%20real-world%20texts%2C%20our%20experiments%0Aconfirm%20that%20jointly%20training%20on%20the%20proposed%20tasks%20leads%20to%20better%20performance%0Aon%20each%20task%20compared%20with%20task-specific%20models%2C%20and%20to%20better%20performance%20on%0Aassessing%20coherence%20overall%2C%20compared%20with%20strong%20baselines.%20We%20conclude%20that%0Athe%20formal%20and%20computational%20setup%20of%20coherence%20as%20proposed%20here%20provides%20a%0Asolid%20foundation%20for%20advanced%20methods%20of%20large-scale%20automatic%20assessment%20of%0Acoherence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Computational%2520and%2520Modeling%2520Foundation%2520for%2520Automatic%2520Coherence%250A%2520%2520Assessment%26entry.906535625%3DAviya%2520Maimon%2520and%2520Reut%2520Tsarfaty%26entry.1292438233%3D%2520%2520Coherence%2520is%2520an%2520essential%2520property%2520of%2520well-written%2520texts%252C%2520that%2520refers%2520to%2520the%250Away%2520textual%2520units%2520relate%2520to%2520one%2520another.%2520In%2520the%2520era%2520of%2520generative%2520AI%252C%2520coherence%250Aassessment%2520is%2520essential%2520for%2520many%2520NLP%2520tasks%253B%2520summarization%252C%2520generation%252C%250Along-form%2520question-answering%252C%2520and%2520more.%2520However%252C%2520in%2520NLP%2520%257Bcoherence%257D%2520is%2520an%250Aill-defined%2520notion%252C%2520not%2520having%2520a%2520formal%2520definition%2520or%2520evaluation%2520metrics%252C%2520that%250Awould%2520allow%2520for%2520large-scale%2520automatic%2520and%2520systematic%2520coherence%2520assessment.%2520To%250Abridge%2520this%2520gap%252C%2520in%2520this%2520work%2520we%2520employ%2520the%2520formal%2520linguistic%2520definition%2520of%250A%255Ccitet%257BReinhart%253A1980%257D%2520of%2520what%2520makes%2520a%2520discourse%2520coherent%252C%2520consisting%2520of%2520three%250Aconditions%2520--%2520%257B%255Cem%2520cohesion%252C%2520consistency%257D%2520and%2520%257B%255Cem%2520relevance%257D%2520--%2520and%2520formalize%250Athese%2520conditions%2520as%2520respective%2520computational%2520tasks.%2520We%2520hypothesize%2520that%2520%2528i%2529%2520a%250Amodel%2520trained%2520on%2520all%2520of%2520these%2520tasks%2520will%2520learn%2520the%2520features%2520required%2520for%250Acoherence%2520detection%252C%2520and%2520that%2520%2528ii%2529%2520a%2520joint%2520model%2520for%2520all%2520tasks%2520will%2520exceed%2520the%250Aperformance%2520of%2520models%2520trained%2520on%2520each%2520task%2520individually.%2520On%2520two%2520benchmarks%2520for%250Acoherence%2520scoring%2520rated%2520by%2520humans%252C%2520one%2520containing%2520500%2520automatically-generated%250Ashort%2520stories%2520and%2520another%2520containing%25204k%2520real-world%2520texts%252C%2520our%2520experiments%250Aconfirm%2520that%2520jointly%2520training%2520on%2520the%2520proposed%2520tasks%2520leads%2520to%2520better%2520performance%250Aon%2520each%2520task%2520compared%2520with%2520task-specific%2520models%252C%2520and%2520to%2520better%2520performance%2520on%250Aassessing%2520coherence%2520overall%252C%2520compared%2520with%2520strong%2520baselines.%2520We%2520conclude%2520that%250Athe%2520formal%2520and%2520computational%2520setup%2520of%2520coherence%2520as%2520proposed%2520here%2520provides%2520a%250Asolid%2520foundation%2520for%2520advanced%2520methods%2520of%2520large-scale%2520automatic%2520assessment%2520of%250Acoherence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Computational%20and%20Modeling%20Foundation%20for%20Automatic%20Coherence%0A%20%20Assessment&entry.906535625=Aviya%20Maimon%20and%20Reut%20Tsarfaty&entry.1292438233=%20%20Coherence%20is%20an%20essential%20property%20of%20well-written%20texts%2C%20that%20refers%20to%20the%0Away%20textual%20units%20relate%20to%20one%20another.%20In%20the%20era%20of%20generative%20AI%2C%20coherence%0Aassessment%20is%20essential%20for%20many%20NLP%20tasks%3B%20summarization%2C%20generation%2C%0Along-form%20question-answering%2C%20and%20more.%20However%2C%20in%20NLP%20%7Bcoherence%7D%20is%20an%0Aill-defined%20notion%2C%20not%20having%20a%20formal%20definition%20or%20evaluation%20metrics%2C%20that%0Awould%20allow%20for%20large-scale%20automatic%20and%20systematic%20coherence%20assessment.%20To%0Abridge%20this%20gap%2C%20in%20this%20work%20we%20employ%20the%20formal%20linguistic%20definition%20of%0A%5Ccitet%7BReinhart%3A1980%7D%20of%20what%20makes%20a%20discourse%20coherent%2C%20consisting%20of%20three%0Aconditions%20--%20%7B%5Cem%20cohesion%2C%20consistency%7D%20and%20%7B%5Cem%20relevance%7D%20--%20and%20formalize%0Athese%20conditions%20as%20respective%20computational%20tasks.%20We%20hypothesize%20that%20%28i%29%20a%0Amodel%20trained%20on%20all%20of%20these%20tasks%20will%20learn%20the%20features%20required%20for%0Acoherence%20detection%2C%20and%20that%20%28ii%29%20a%20joint%20model%20for%20all%20tasks%20will%20exceed%20the%0Aperformance%20of%20models%20trained%20on%20each%20task%20individually.%20On%20two%20benchmarks%20for%0Acoherence%20scoring%20rated%20by%20humans%2C%20one%20containing%20500%20automatically-generated%0Ashort%20stories%20and%20another%20containing%204k%20real-world%20texts%2C%20our%20experiments%0Aconfirm%20that%20jointly%20training%20on%20the%20proposed%20tasks%20leads%20to%20better%20performance%0Aon%20each%20task%20compared%20with%20task-specific%20models%2C%20and%20to%20better%20performance%20on%0Aassessing%20coherence%20overall%2C%20compared%20with%20strong%20baselines.%20We%20conclude%20that%0Athe%20formal%20and%20computational%20setup%20of%20coherence%20as%20proposed%20here%20provides%20a%0Asolid%20foundation%20for%20advanced%20methods%20of%20large-scale%20automatic%20assessment%20of%0Acoherence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00598v2&entry.124074799=Read"},
{"title": "Interpretable Graph Neural Networks for Tabular Data", "author": "Amr Alkhatib and Sofiane Ennadir and Henrik Bostr\u00f6m and Michalis Vazirgiannis", "abstract": "  Data in tabular format is frequently occurring in real-world applications.\nGraph Neural Networks (GNNs) have recently been extended to effectively handle\nsuch data, allowing feature interactions to be captured through representation\nlearning. However, these approaches essentially produce black-box models, in\nthe form of deep neural networks, precluding users from following the logic\nbehind the model predictions. We propose an approach, called IGNNet\n(Interpretable Graph Neural Network for tabular data), which constrains the\nlearning algorithm to produce an interpretable model, where the model shows how\nthe predictions are exactly computed from the original input features. A\nlarge-scale empirical investigation is presented, showing that IGNNet is\nperforming on par with state-of-the-art machine-learning algorithms that target\ntabular data, including XGBoost, Random Forests, and TabNet. At the same time,\nthe results show that the explanations obtained from IGNNet are aligned with\nthe true Shapley values of the features without incurring any additional\ncomputational overhead.\n", "link": "http://arxiv.org/abs/2308.08945v3", "date": "2024-08-13", "relevancy": 1.8274, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4739}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.445}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Graph%20Neural%20Networks%20for%20Tabular%20Data&body=Title%3A%20Interpretable%20Graph%20Neural%20Networks%20for%20Tabular%20Data%0AAuthor%3A%20Amr%20Alkhatib%20and%20Sofiane%20Ennadir%20and%20Henrik%20Bostr%C3%B6m%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Data%20in%20tabular%20format%20is%20frequently%20occurring%20in%20real-world%20applications.%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20been%20extended%20to%20effectively%20handle%0Asuch%20data%2C%20allowing%20feature%20interactions%20to%20be%20captured%20through%20representation%0Alearning.%20However%2C%20these%20approaches%20essentially%20produce%20black-box%20models%2C%20in%0Athe%20form%20of%20deep%20neural%20networks%2C%20precluding%20users%20from%20following%20the%20logic%0Abehind%20the%20model%20predictions.%20We%20propose%20an%20approach%2C%20called%20IGNNet%0A%28Interpretable%20Graph%20Neural%20Network%20for%20tabular%20data%29%2C%20which%20constrains%20the%0Alearning%20algorithm%20to%20produce%20an%20interpretable%20model%2C%20where%20the%20model%20shows%20how%0Athe%20predictions%20are%20exactly%20computed%20from%20the%20original%20input%20features.%20A%0Alarge-scale%20empirical%20investigation%20is%20presented%2C%20showing%20that%20IGNNet%20is%0Aperforming%20on%20par%20with%20state-of-the-art%20machine-learning%20algorithms%20that%20target%0Atabular%20data%2C%20including%20XGBoost%2C%20Random%20Forests%2C%20and%20TabNet.%20At%20the%20same%20time%2C%0Athe%20results%20show%20that%20the%20explanations%20obtained%20from%20IGNNet%20are%20aligned%20with%0Athe%20true%20Shapley%20values%20of%20the%20features%20without%20incurring%20any%20additional%0Acomputational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08945v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Graph%2520Neural%2520Networks%2520for%2520Tabular%2520Data%26entry.906535625%3DAmr%2520Alkhatib%2520and%2520Sofiane%2520Ennadir%2520and%2520Henrik%2520Bostr%25C3%25B6m%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Data%2520in%2520tabular%2520format%2520is%2520frequently%2520occurring%2520in%2520real-world%2520applications.%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520recently%2520been%2520extended%2520to%2520effectively%2520handle%250Asuch%2520data%252C%2520allowing%2520feature%2520interactions%2520to%2520be%2520captured%2520through%2520representation%250Alearning.%2520However%252C%2520these%2520approaches%2520essentially%2520produce%2520black-box%2520models%252C%2520in%250Athe%2520form%2520of%2520deep%2520neural%2520networks%252C%2520precluding%2520users%2520from%2520following%2520the%2520logic%250Abehind%2520the%2520model%2520predictions.%2520We%2520propose%2520an%2520approach%252C%2520called%2520IGNNet%250A%2528Interpretable%2520Graph%2520Neural%2520Network%2520for%2520tabular%2520data%2529%252C%2520which%2520constrains%2520the%250Alearning%2520algorithm%2520to%2520produce%2520an%2520interpretable%2520model%252C%2520where%2520the%2520model%2520shows%2520how%250Athe%2520predictions%2520are%2520exactly%2520computed%2520from%2520the%2520original%2520input%2520features.%2520A%250Alarge-scale%2520empirical%2520investigation%2520is%2520presented%252C%2520showing%2520that%2520IGNNet%2520is%250Aperforming%2520on%2520par%2520with%2520state-of-the-art%2520machine-learning%2520algorithms%2520that%2520target%250Atabular%2520data%252C%2520including%2520XGBoost%252C%2520Random%2520Forests%252C%2520and%2520TabNet.%2520At%2520the%2520same%2520time%252C%250Athe%2520results%2520show%2520that%2520the%2520explanations%2520obtained%2520from%2520IGNNet%2520are%2520aligned%2520with%250Athe%2520true%2520Shapley%2520values%2520of%2520the%2520features%2520without%2520incurring%2520any%2520additional%250Acomputational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.08945v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Graph%20Neural%20Networks%20for%20Tabular%20Data&entry.906535625=Amr%20Alkhatib%20and%20Sofiane%20Ennadir%20and%20Henrik%20Bostr%C3%B6m%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Data%20in%20tabular%20format%20is%20frequently%20occurring%20in%20real-world%20applications.%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20been%20extended%20to%20effectively%20handle%0Asuch%20data%2C%20allowing%20feature%20interactions%20to%20be%20captured%20through%20representation%0Alearning.%20However%2C%20these%20approaches%20essentially%20produce%20black-box%20models%2C%20in%0Athe%20form%20of%20deep%20neural%20networks%2C%20precluding%20users%20from%20following%20the%20logic%0Abehind%20the%20model%20predictions.%20We%20propose%20an%20approach%2C%20called%20IGNNet%0A%28Interpretable%20Graph%20Neural%20Network%20for%20tabular%20data%29%2C%20which%20constrains%20the%0Alearning%20algorithm%20to%20produce%20an%20interpretable%20model%2C%20where%20the%20model%20shows%20how%0Athe%20predictions%20are%20exactly%20computed%20from%20the%20original%20input%20features.%20A%0Alarge-scale%20empirical%20investigation%20is%20presented%2C%20showing%20that%20IGNNet%20is%0Aperforming%20on%20par%20with%20state-of-the-art%20machine-learning%20algorithms%20that%20target%0Atabular%20data%2C%20including%20XGBoost%2C%20Random%20Forests%2C%20and%20TabNet.%20At%20the%20same%20time%2C%0Athe%20results%20show%20that%20the%20explanations%20obtained%20from%20IGNNet%20are%20aligned%20with%0Athe%20true%20Shapley%20values%20of%20the%20features%20without%20incurring%20any%20additional%0Acomputational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08945v3&entry.124074799=Read"},
{"title": "Learning Minimal Neural Specifications", "author": "Chuqin Geng and Zhaoyue Wang and Haolin Ye and Saifei Liao and Xujie Si", "abstract": "  Formal verification is only as good as the specification of a system, which\nis also true for neural network verification. Existing specifications follow\nthe paradigm of data as specification, where the local neighborhood around a\nreference data point is considered correct or robust. While these\nspecifications provide a fair testbed for assessing model robustness, they are\ntoo restrictive for verifying unseen test data-a challenging task with\nsignificant real-world implications. Recent work shows great promise through a\nnew paradigm, neural representation as specification, which uses neural\nactivation patterns (NAPs) for this purpose. However, it computes the most\nrefined NAPs, which include many redundant neurons. In this paper, we study the\nfollowing problem: Given a neural network, find a minimal (general) NAP\nspecification that is sufficient for formal verification of the network's\nrobustness. Finding the minimal NAP specification not only expands verifiable\nbounds but also provides insights into which neurons contribute to the model's\nrobustness. To address this problem, we propose several exact and approximate\napproaches. Our exact approaches leverage the verification tool to find minimal\nNAP specifications in either a deterministic or statistical manner. Whereas the\napproximate methods efficiently estimate minimal NAPs using adversarial\nexamples and local gradients, without making calls to the verification tool.\nThis allows us to inspect potential causal links between neurons and the\nrobustness of state-of-the art neural networks, a task for which existing\nverification frameworks fail to scale. Our experimental results suggest that\nminimal NAP specifications require much smaller fractions of neurons compared\nto the most refined NAP specifications computed by previous work, yet they can\nsignificantly expand the verifiable boundaries to several orders of magnitude\nlarger.\n", "link": "http://arxiv.org/abs/2404.04662v3", "date": "2024-08-13", "relevancy": 1.8234, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4662}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4633}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Minimal%20Neural%20Specifications&body=Title%3A%20Learning%20Minimal%20Neural%20Specifications%0AAuthor%3A%20Chuqin%20Geng%20and%20Zhaoyue%20Wang%20and%20Haolin%20Ye%20and%20Saifei%20Liao%20and%20Xujie%20Si%0AAbstract%3A%20%20%20Formal%20verification%20is%20only%20as%20good%20as%20the%20specification%20of%20a%20system%2C%20which%0Ais%20also%20true%20for%20neural%20network%20verification.%20Existing%20specifications%20follow%0Athe%20paradigm%20of%20data%20as%20specification%2C%20where%20the%20local%20neighborhood%20around%20a%0Areference%20data%20point%20is%20considered%20correct%20or%20robust.%20While%20these%0Aspecifications%20provide%20a%20fair%20testbed%20for%20assessing%20model%20robustness%2C%20they%20are%0Atoo%20restrictive%20for%20verifying%20unseen%20test%20data-a%20challenging%20task%20with%0Asignificant%20real-world%20implications.%20Recent%20work%20shows%20great%20promise%20through%20a%0Anew%20paradigm%2C%20neural%20representation%20as%20specification%2C%20which%20uses%20neural%0Aactivation%20patterns%20%28NAPs%29%20for%20this%20purpose.%20However%2C%20it%20computes%20the%20most%0Arefined%20NAPs%2C%20which%20include%20many%20redundant%20neurons.%20In%20this%20paper%2C%20we%20study%20the%0Afollowing%20problem%3A%20Given%20a%20neural%20network%2C%20find%20a%20minimal%20%28general%29%20NAP%0Aspecification%20that%20is%20sufficient%20for%20formal%20verification%20of%20the%20network%27s%0Arobustness.%20Finding%20the%20minimal%20NAP%20specification%20not%20only%20expands%20verifiable%0Abounds%20but%20also%20provides%20insights%20into%20which%20neurons%20contribute%20to%20the%20model%27s%0Arobustness.%20To%20address%20this%20problem%2C%20we%20propose%20several%20exact%20and%20approximate%0Aapproaches.%20Our%20exact%20approaches%20leverage%20the%20verification%20tool%20to%20find%20minimal%0ANAP%20specifications%20in%20either%20a%20deterministic%20or%20statistical%20manner.%20Whereas%20the%0Aapproximate%20methods%20efficiently%20estimate%20minimal%20NAPs%20using%20adversarial%0Aexamples%20and%20local%20gradients%2C%20without%20making%20calls%20to%20the%20verification%20tool.%0AThis%20allows%20us%20to%20inspect%20potential%20causal%20links%20between%20neurons%20and%20the%0Arobustness%20of%20state-of-the%20art%20neural%20networks%2C%20a%20task%20for%20which%20existing%0Averification%20frameworks%20fail%20to%20scale.%20Our%20experimental%20results%20suggest%20that%0Aminimal%20NAP%20specifications%20require%20much%20smaller%20fractions%20of%20neurons%20compared%0Ato%20the%20most%20refined%20NAP%20specifications%20computed%20by%20previous%20work%2C%20yet%20they%20can%0Asignificantly%20expand%20the%20verifiable%20boundaries%20to%20several%20orders%20of%20magnitude%0Alarger.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04662v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Minimal%2520Neural%2520Specifications%26entry.906535625%3DChuqin%2520Geng%2520and%2520Zhaoyue%2520Wang%2520and%2520Haolin%2520Ye%2520and%2520Saifei%2520Liao%2520and%2520Xujie%2520Si%26entry.1292438233%3D%2520%2520Formal%2520verification%2520is%2520only%2520as%2520good%2520as%2520the%2520specification%2520of%2520a%2520system%252C%2520which%250Ais%2520also%2520true%2520for%2520neural%2520network%2520verification.%2520Existing%2520specifications%2520follow%250Athe%2520paradigm%2520of%2520data%2520as%2520specification%252C%2520where%2520the%2520local%2520neighborhood%2520around%2520a%250Areference%2520data%2520point%2520is%2520considered%2520correct%2520or%2520robust.%2520While%2520these%250Aspecifications%2520provide%2520a%2520fair%2520testbed%2520for%2520assessing%2520model%2520robustness%252C%2520they%2520are%250Atoo%2520restrictive%2520for%2520verifying%2520unseen%2520test%2520data-a%2520challenging%2520task%2520with%250Asignificant%2520real-world%2520implications.%2520Recent%2520work%2520shows%2520great%2520promise%2520through%2520a%250Anew%2520paradigm%252C%2520neural%2520representation%2520as%2520specification%252C%2520which%2520uses%2520neural%250Aactivation%2520patterns%2520%2528NAPs%2529%2520for%2520this%2520purpose.%2520However%252C%2520it%2520computes%2520the%2520most%250Arefined%2520NAPs%252C%2520which%2520include%2520many%2520redundant%2520neurons.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%250Afollowing%2520problem%253A%2520Given%2520a%2520neural%2520network%252C%2520find%2520a%2520minimal%2520%2528general%2529%2520NAP%250Aspecification%2520that%2520is%2520sufficient%2520for%2520formal%2520verification%2520of%2520the%2520network%2527s%250Arobustness.%2520Finding%2520the%2520minimal%2520NAP%2520specification%2520not%2520only%2520expands%2520verifiable%250Abounds%2520but%2520also%2520provides%2520insights%2520into%2520which%2520neurons%2520contribute%2520to%2520the%2520model%2527s%250Arobustness.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520several%2520exact%2520and%2520approximate%250Aapproaches.%2520Our%2520exact%2520approaches%2520leverage%2520the%2520verification%2520tool%2520to%2520find%2520minimal%250ANAP%2520specifications%2520in%2520either%2520a%2520deterministic%2520or%2520statistical%2520manner.%2520Whereas%2520the%250Aapproximate%2520methods%2520efficiently%2520estimate%2520minimal%2520NAPs%2520using%2520adversarial%250Aexamples%2520and%2520local%2520gradients%252C%2520without%2520making%2520calls%2520to%2520the%2520verification%2520tool.%250AThis%2520allows%2520us%2520to%2520inspect%2520potential%2520causal%2520links%2520between%2520neurons%2520and%2520the%250Arobustness%2520of%2520state-of-the%2520art%2520neural%2520networks%252C%2520a%2520task%2520for%2520which%2520existing%250Averification%2520frameworks%2520fail%2520to%2520scale.%2520Our%2520experimental%2520results%2520suggest%2520that%250Aminimal%2520NAP%2520specifications%2520require%2520much%2520smaller%2520fractions%2520of%2520neurons%2520compared%250Ato%2520the%2520most%2520refined%2520NAP%2520specifications%2520computed%2520by%2520previous%2520work%252C%2520yet%2520they%2520can%250Asignificantly%2520expand%2520the%2520verifiable%2520boundaries%2520to%2520several%2520orders%2520of%2520magnitude%250Alarger.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04662v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Minimal%20Neural%20Specifications&entry.906535625=Chuqin%20Geng%20and%20Zhaoyue%20Wang%20and%20Haolin%20Ye%20and%20Saifei%20Liao%20and%20Xujie%20Si&entry.1292438233=%20%20Formal%20verification%20is%20only%20as%20good%20as%20the%20specification%20of%20a%20system%2C%20which%0Ais%20also%20true%20for%20neural%20network%20verification.%20Existing%20specifications%20follow%0Athe%20paradigm%20of%20data%20as%20specification%2C%20where%20the%20local%20neighborhood%20around%20a%0Areference%20data%20point%20is%20considered%20correct%20or%20robust.%20While%20these%0Aspecifications%20provide%20a%20fair%20testbed%20for%20assessing%20model%20robustness%2C%20they%20are%0Atoo%20restrictive%20for%20verifying%20unseen%20test%20data-a%20challenging%20task%20with%0Asignificant%20real-world%20implications.%20Recent%20work%20shows%20great%20promise%20through%20a%0Anew%20paradigm%2C%20neural%20representation%20as%20specification%2C%20which%20uses%20neural%0Aactivation%20patterns%20%28NAPs%29%20for%20this%20purpose.%20However%2C%20it%20computes%20the%20most%0Arefined%20NAPs%2C%20which%20include%20many%20redundant%20neurons.%20In%20this%20paper%2C%20we%20study%20the%0Afollowing%20problem%3A%20Given%20a%20neural%20network%2C%20find%20a%20minimal%20%28general%29%20NAP%0Aspecification%20that%20is%20sufficient%20for%20formal%20verification%20of%20the%20network%27s%0Arobustness.%20Finding%20the%20minimal%20NAP%20specification%20not%20only%20expands%20verifiable%0Abounds%20but%20also%20provides%20insights%20into%20which%20neurons%20contribute%20to%20the%20model%27s%0Arobustness.%20To%20address%20this%20problem%2C%20we%20propose%20several%20exact%20and%20approximate%0Aapproaches.%20Our%20exact%20approaches%20leverage%20the%20verification%20tool%20to%20find%20minimal%0ANAP%20specifications%20in%20either%20a%20deterministic%20or%20statistical%20manner.%20Whereas%20the%0Aapproximate%20methods%20efficiently%20estimate%20minimal%20NAPs%20using%20adversarial%0Aexamples%20and%20local%20gradients%2C%20without%20making%20calls%20to%20the%20verification%20tool.%0AThis%20allows%20us%20to%20inspect%20potential%20causal%20links%20between%20neurons%20and%20the%0Arobustness%20of%20state-of-the%20art%20neural%20networks%2C%20a%20task%20for%20which%20existing%0Averification%20frameworks%20fail%20to%20scale.%20Our%20experimental%20results%20suggest%20that%0Aminimal%20NAP%20specifications%20require%20much%20smaller%20fractions%20of%20neurons%20compared%0Ato%20the%20most%20refined%20NAP%20specifications%20computed%20by%20previous%20work%2C%20yet%20they%20can%0Asignificantly%20expand%20the%20verifiable%20boundaries%20to%20several%20orders%20of%20magnitude%0Alarger.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04662v3&entry.124074799=Read"},
{"title": "Defining and Measuring Disentanglement for non-Independent Factors of\n  Variation", "author": "Antonio Almud\u00e9var and Alfonso Ortega and Luis Vicente and Antonio Miguel and Eduardo Lleida", "abstract": "  Representation learning is an approach that allows to discover and extract\nthe factors of variation from the data. Intuitively, a representation is said\nto be disentangled if it separates the different factors of variation in a way\nthat is understandable to humans. Definitions of disentanglement and metrics to\nmeasure it usually assume that the factors of variation are independent of each\nother. However, this is generally false in the real world, which limits the use\nof these definitions and metrics to very specific and unrealistic scenarios. In\nthis paper we give a definition of disentanglement based on information theory\nthat is also valid when the factors of variation are not independent.\nFurthermore, we relate this definition to the Information Bottleneck Method.\nFinally, we propose a method to measure the degree of disentanglement from the\ngiven definition that works when the factors of variation are not independent.\nWe show through different experiments that the method proposed in this paper\ncorrectly measures disentanglement with non-independent factors of variation,\nwhile other methods fail in this scenario.\n", "link": "http://arxiv.org/abs/2408.07016v1", "date": "2024-08-13", "relevancy": 1.8186, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.457}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4553}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defining%20and%20Measuring%20Disentanglement%20for%20non-Independent%20Factors%20of%0A%20%20Variation&body=Title%3A%20Defining%20and%20Measuring%20Disentanglement%20for%20non-Independent%20Factors%20of%0A%20%20Variation%0AAuthor%3A%20Antonio%20Almud%C3%A9var%20and%20Alfonso%20Ortega%20and%20Luis%20Vicente%20and%20Antonio%20Miguel%20and%20Eduardo%20Lleida%0AAbstract%3A%20%20%20Representation%20learning%20is%20an%20approach%20that%20allows%20to%20discover%20and%20extract%0Athe%20factors%20of%20variation%20from%20the%20data.%20Intuitively%2C%20a%20representation%20is%20said%0Ato%20be%20disentangled%20if%20it%20separates%20the%20different%20factors%20of%20variation%20in%20a%20way%0Athat%20is%20understandable%20to%20humans.%20Definitions%20of%20disentanglement%20and%20metrics%20to%0Ameasure%20it%20usually%20assume%20that%20the%20factors%20of%20variation%20are%20independent%20of%20each%0Aother.%20However%2C%20this%20is%20generally%20false%20in%20the%20real%20world%2C%20which%20limits%20the%20use%0Aof%20these%20definitions%20and%20metrics%20to%20very%20specific%20and%20unrealistic%20scenarios.%20In%0Athis%20paper%20we%20give%20a%20definition%20of%20disentanglement%20based%20on%20information%20theory%0Athat%20is%20also%20valid%20when%20the%20factors%20of%20variation%20are%20not%20independent.%0AFurthermore%2C%20we%20relate%20this%20definition%20to%20the%20Information%20Bottleneck%20Method.%0AFinally%2C%20we%20propose%20a%20method%20to%20measure%20the%20degree%20of%20disentanglement%20from%20the%0Agiven%20definition%20that%20works%20when%20the%20factors%20of%20variation%20are%20not%20independent.%0AWe%20show%20through%20different%20experiments%20that%20the%20method%20proposed%20in%20this%20paper%0Acorrectly%20measures%20disentanglement%20with%20non-independent%20factors%20of%20variation%2C%0Awhile%20other%20methods%20fail%20in%20this%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefining%2520and%2520Measuring%2520Disentanglement%2520for%2520non-Independent%2520Factors%2520of%250A%2520%2520Variation%26entry.906535625%3DAntonio%2520Almud%25C3%25A9var%2520and%2520Alfonso%2520Ortega%2520and%2520Luis%2520Vicente%2520and%2520Antonio%2520Miguel%2520and%2520Eduardo%2520Lleida%26entry.1292438233%3D%2520%2520Representation%2520learning%2520is%2520an%2520approach%2520that%2520allows%2520to%2520discover%2520and%2520extract%250Athe%2520factors%2520of%2520variation%2520from%2520the%2520data.%2520Intuitively%252C%2520a%2520representation%2520is%2520said%250Ato%2520be%2520disentangled%2520if%2520it%2520separates%2520the%2520different%2520factors%2520of%2520variation%2520in%2520a%2520way%250Athat%2520is%2520understandable%2520to%2520humans.%2520Definitions%2520of%2520disentanglement%2520and%2520metrics%2520to%250Ameasure%2520it%2520usually%2520assume%2520that%2520the%2520factors%2520of%2520variation%2520are%2520independent%2520of%2520each%250Aother.%2520However%252C%2520this%2520is%2520generally%2520false%2520in%2520the%2520real%2520world%252C%2520which%2520limits%2520the%2520use%250Aof%2520these%2520definitions%2520and%2520metrics%2520to%2520very%2520specific%2520and%2520unrealistic%2520scenarios.%2520In%250Athis%2520paper%2520we%2520give%2520a%2520definition%2520of%2520disentanglement%2520based%2520on%2520information%2520theory%250Athat%2520is%2520also%2520valid%2520when%2520the%2520factors%2520of%2520variation%2520are%2520not%2520independent.%250AFurthermore%252C%2520we%2520relate%2520this%2520definition%2520to%2520the%2520Information%2520Bottleneck%2520Method.%250AFinally%252C%2520we%2520propose%2520a%2520method%2520to%2520measure%2520the%2520degree%2520of%2520disentanglement%2520from%2520the%250Agiven%2520definition%2520that%2520works%2520when%2520the%2520factors%2520of%2520variation%2520are%2520not%2520independent.%250AWe%2520show%2520through%2520different%2520experiments%2520that%2520the%2520method%2520proposed%2520in%2520this%2520paper%250Acorrectly%2520measures%2520disentanglement%2520with%2520non-independent%2520factors%2520of%2520variation%252C%250Awhile%2520other%2520methods%2520fail%2520in%2520this%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defining%20and%20Measuring%20Disentanglement%20for%20non-Independent%20Factors%20of%0A%20%20Variation&entry.906535625=Antonio%20Almud%C3%A9var%20and%20Alfonso%20Ortega%20and%20Luis%20Vicente%20and%20Antonio%20Miguel%20and%20Eduardo%20Lleida&entry.1292438233=%20%20Representation%20learning%20is%20an%20approach%20that%20allows%20to%20discover%20and%20extract%0Athe%20factors%20of%20variation%20from%20the%20data.%20Intuitively%2C%20a%20representation%20is%20said%0Ato%20be%20disentangled%20if%20it%20separates%20the%20different%20factors%20of%20variation%20in%20a%20way%0Athat%20is%20understandable%20to%20humans.%20Definitions%20of%20disentanglement%20and%20metrics%20to%0Ameasure%20it%20usually%20assume%20that%20the%20factors%20of%20variation%20are%20independent%20of%20each%0Aother.%20However%2C%20this%20is%20generally%20false%20in%20the%20real%20world%2C%20which%20limits%20the%20use%0Aof%20these%20definitions%20and%20metrics%20to%20very%20specific%20and%20unrealistic%20scenarios.%20In%0Athis%20paper%20we%20give%20a%20definition%20of%20disentanglement%20based%20on%20information%20theory%0Athat%20is%20also%20valid%20when%20the%20factors%20of%20variation%20are%20not%20independent.%0AFurthermore%2C%20we%20relate%20this%20definition%20to%20the%20Information%20Bottleneck%20Method.%0AFinally%2C%20we%20propose%20a%20method%20to%20measure%20the%20degree%20of%20disentanglement%20from%20the%0Agiven%20definition%20that%20works%20when%20the%20factors%20of%20variation%20are%20not%20independent.%0AWe%20show%20through%20different%20experiments%20that%20the%20method%20proposed%20in%20this%20paper%0Acorrectly%20measures%20disentanglement%20with%20non-independent%20factors%20of%20variation%2C%0Awhile%20other%20methods%20fail%20in%20this%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07016v1&entry.124074799=Read"},
{"title": "Stabilizer bootstrapping: A recipe for efficient agnostic tomography and\n  magic estimation", "author": "Sitan Chen and Weiyuan Gong and Qi Ye and Zhihan Zhang", "abstract": "  We study the task of agnostic tomography: given copies of an unknown\n$n$-qubit state $\\rho$ which has fidelity $\\tau$ with some state in a given\nclass $C$, find a state which has fidelity $\\ge \\tau - \\epsilon$ with $\\rho$.\nWe give a new framework, stabilizer bootstrapping, for designing\ncomputationally efficient protocols for this task, and use this to get new\nagnostic tomography protocols for the following classes:\n  Stabilizer states: We give a protocol that runs in time\n$\\mathrm{poly}(n,1/\\epsilon)\\cdot (1/\\tau)^{O(\\log(1/\\tau))}$, answering an\nopen question posed by Grewal, Iyer, Kretschmer, Liang [40] and Anshu and\nArunachalam [6]. Previous protocols ran in time $\\mathrm{exp}(\\Theta(n))$ or\nrequired $\\tau>\\cos^2(\\pi/8)$.\n  States with stabilizer dimension $n - t$: We give a protocol that runs in\ntime $n^3\\cdot(2^t/\\tau)^{O(\\log(1/\\epsilon))}$, extending recent work on\nlearning quantum states prepared by circuits with few non-Clifford gates, which\nonly applied in the realizable setting where $\\tau = 1$ [30, 37, 46, 61].\n  Discrete product states: If $C = K^{\\otimes n}$ for some $\\mu$-separated\ndiscrete set $K$ of single-qubit states, we give a protocol that runs in time\n$(n/\\mu)^{O((1 + \\log (1/\\tau))/\\mu)}/\\epsilon^2$. This strictly generalizes a\nprior guarantee which applied to stabilizer product states [39]. For stabilizer\nproduct states, we give a further improved protocol that runs in time\n$(n^2/\\epsilon^2)\\cdot (1/\\tau)^{O(\\log(1/\\tau))}$.\n  As a corollary, we give the first protocol for estimating stabilizer\nfidelity, a standard measure of magic for quantum states, to error $\\epsilon$\nin $n^3 \\mathrm{quasipoly}(1/\\epsilon)$ time.\n", "link": "http://arxiv.org/abs/2408.06967v1", "date": "2024-08-13", "relevancy": 1.8086, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4848}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4369}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizer%20bootstrapping%3A%20A%20recipe%20for%20efficient%20agnostic%20tomography%20and%0A%20%20magic%20estimation&body=Title%3A%20Stabilizer%20bootstrapping%3A%20A%20recipe%20for%20efficient%20agnostic%20tomography%20and%0A%20%20magic%20estimation%0AAuthor%3A%20Sitan%20Chen%20and%20Weiyuan%20Gong%20and%20Qi%20Ye%20and%20Zhihan%20Zhang%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20agnostic%20tomography%3A%20given%20copies%20of%20an%20unknown%0A%24n%24-qubit%20state%20%24%5Crho%24%20which%20has%20fidelity%20%24%5Ctau%24%20with%20some%20state%20in%20a%20given%0Aclass%20%24C%24%2C%20find%20a%20state%20which%20has%20fidelity%20%24%5Cge%20%5Ctau%20-%20%5Cepsilon%24%20with%20%24%5Crho%24.%0AWe%20give%20a%20new%20framework%2C%20stabilizer%20bootstrapping%2C%20for%20designing%0Acomputationally%20efficient%20protocols%20for%20this%20task%2C%20and%20use%20this%20to%20get%20new%0Aagnostic%20tomography%20protocols%20for%20the%20following%20classes%3A%0A%20%20Stabilizer%20states%3A%20We%20give%20a%20protocol%20that%20runs%20in%20time%0A%24%5Cmathrm%7Bpoly%7D%28n%2C1/%5Cepsilon%29%5Ccdot%20%281/%5Ctau%29%5E%7BO%28%5Clog%281/%5Ctau%29%29%7D%24%2C%20answering%20an%0Aopen%20question%20posed%20by%20Grewal%2C%20Iyer%2C%20Kretschmer%2C%20Liang%20%5B40%5D%20and%20Anshu%20and%0AArunachalam%20%5B6%5D.%20Previous%20protocols%20ran%20in%20time%20%24%5Cmathrm%7Bexp%7D%28%5CTheta%28n%29%29%24%20or%0Arequired%20%24%5Ctau%3E%5Ccos%5E2%28%5Cpi/8%29%24.%0A%20%20States%20with%20stabilizer%20dimension%20%24n%20-%20t%24%3A%20We%20give%20a%20protocol%20that%20runs%20in%0Atime%20%24n%5E3%5Ccdot%282%5Et/%5Ctau%29%5E%7BO%28%5Clog%281/%5Cepsilon%29%29%7D%24%2C%20extending%20recent%20work%20on%0Alearning%20quantum%20states%20prepared%20by%20circuits%20with%20few%20non-Clifford%20gates%2C%20which%0Aonly%20applied%20in%20the%20realizable%20setting%20where%20%24%5Ctau%20%3D%201%24%20%5B30%2C%2037%2C%2046%2C%2061%5D.%0A%20%20Discrete%20product%20states%3A%20If%20%24C%20%3D%20K%5E%7B%5Cotimes%20n%7D%24%20for%20some%20%24%5Cmu%24-separated%0Adiscrete%20set%20%24K%24%20of%20single-qubit%20states%2C%20we%20give%20a%20protocol%20that%20runs%20in%20time%0A%24%28n/%5Cmu%29%5E%7BO%28%281%20%2B%20%5Clog%20%281/%5Ctau%29%29/%5Cmu%29%7D/%5Cepsilon%5E2%24.%20This%20strictly%20generalizes%20a%0Aprior%20guarantee%20which%20applied%20to%20stabilizer%20product%20states%20%5B39%5D.%20For%20stabilizer%0Aproduct%20states%2C%20we%20give%20a%20further%20improved%20protocol%20that%20runs%20in%20time%0A%24%28n%5E2/%5Cepsilon%5E2%29%5Ccdot%20%281/%5Ctau%29%5E%7BO%28%5Clog%281/%5Ctau%29%29%7D%24.%0A%20%20As%20a%20corollary%2C%20we%20give%20the%20first%20protocol%20for%20estimating%20stabilizer%0Afidelity%2C%20a%20standard%20measure%20of%20magic%20for%20quantum%20states%2C%20to%20error%20%24%5Cepsilon%24%0Ain%20%24n%5E3%20%5Cmathrm%7Bquasipoly%7D%281/%5Cepsilon%29%24%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizer%2520bootstrapping%253A%2520A%2520recipe%2520for%2520efficient%2520agnostic%2520tomography%2520and%250A%2520%2520magic%2520estimation%26entry.906535625%3DSitan%2520Chen%2520and%2520Weiyuan%2520Gong%2520and%2520Qi%2520Ye%2520and%2520Zhihan%2520Zhang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520agnostic%2520tomography%253A%2520given%2520copies%2520of%2520an%2520unknown%250A%2524n%2524-qubit%2520state%2520%2524%255Crho%2524%2520which%2520has%2520fidelity%2520%2524%255Ctau%2524%2520with%2520some%2520state%2520in%2520a%2520given%250Aclass%2520%2524C%2524%252C%2520find%2520a%2520state%2520which%2520has%2520fidelity%2520%2524%255Cge%2520%255Ctau%2520-%2520%255Cepsilon%2524%2520with%2520%2524%255Crho%2524.%250AWe%2520give%2520a%2520new%2520framework%252C%2520stabilizer%2520bootstrapping%252C%2520for%2520designing%250Acomputationally%2520efficient%2520protocols%2520for%2520this%2520task%252C%2520and%2520use%2520this%2520to%2520get%2520new%250Aagnostic%2520tomography%2520protocols%2520for%2520the%2520following%2520classes%253A%250A%2520%2520Stabilizer%2520states%253A%2520We%2520give%2520a%2520protocol%2520that%2520runs%2520in%2520time%250A%2524%255Cmathrm%257Bpoly%257D%2528n%252C1/%255Cepsilon%2529%255Ccdot%2520%25281/%255Ctau%2529%255E%257BO%2528%255Clog%25281/%255Ctau%2529%2529%257D%2524%252C%2520answering%2520an%250Aopen%2520question%2520posed%2520by%2520Grewal%252C%2520Iyer%252C%2520Kretschmer%252C%2520Liang%2520%255B40%255D%2520and%2520Anshu%2520and%250AArunachalam%2520%255B6%255D.%2520Previous%2520protocols%2520ran%2520in%2520time%2520%2524%255Cmathrm%257Bexp%257D%2528%255CTheta%2528n%2529%2529%2524%2520or%250Arequired%2520%2524%255Ctau%253E%255Ccos%255E2%2528%255Cpi/8%2529%2524.%250A%2520%2520States%2520with%2520stabilizer%2520dimension%2520%2524n%2520-%2520t%2524%253A%2520We%2520give%2520a%2520protocol%2520that%2520runs%2520in%250Atime%2520%2524n%255E3%255Ccdot%25282%255Et/%255Ctau%2529%255E%257BO%2528%255Clog%25281/%255Cepsilon%2529%2529%257D%2524%252C%2520extending%2520recent%2520work%2520on%250Alearning%2520quantum%2520states%2520prepared%2520by%2520circuits%2520with%2520few%2520non-Clifford%2520gates%252C%2520which%250Aonly%2520applied%2520in%2520the%2520realizable%2520setting%2520where%2520%2524%255Ctau%2520%253D%25201%2524%2520%255B30%252C%252037%252C%252046%252C%252061%255D.%250A%2520%2520Discrete%2520product%2520states%253A%2520If%2520%2524C%2520%253D%2520K%255E%257B%255Cotimes%2520n%257D%2524%2520for%2520some%2520%2524%255Cmu%2524-separated%250Adiscrete%2520set%2520%2524K%2524%2520of%2520single-qubit%2520states%252C%2520we%2520give%2520a%2520protocol%2520that%2520runs%2520in%2520time%250A%2524%2528n/%255Cmu%2529%255E%257BO%2528%25281%2520%252B%2520%255Clog%2520%25281/%255Ctau%2529%2529/%255Cmu%2529%257D/%255Cepsilon%255E2%2524.%2520This%2520strictly%2520generalizes%2520a%250Aprior%2520guarantee%2520which%2520applied%2520to%2520stabilizer%2520product%2520states%2520%255B39%255D.%2520For%2520stabilizer%250Aproduct%2520states%252C%2520we%2520give%2520a%2520further%2520improved%2520protocol%2520that%2520runs%2520in%2520time%250A%2524%2528n%255E2/%255Cepsilon%255E2%2529%255Ccdot%2520%25281/%255Ctau%2529%255E%257BO%2528%255Clog%25281/%255Ctau%2529%2529%257D%2524.%250A%2520%2520As%2520a%2520corollary%252C%2520we%2520give%2520the%2520first%2520protocol%2520for%2520estimating%2520stabilizer%250Afidelity%252C%2520a%2520standard%2520measure%2520of%2520magic%2520for%2520quantum%2520states%252C%2520to%2520error%2520%2524%255Cepsilon%2524%250Ain%2520%2524n%255E3%2520%255Cmathrm%257Bquasipoly%257D%25281/%255Cepsilon%2529%2524%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizer%20bootstrapping%3A%20A%20recipe%20for%20efficient%20agnostic%20tomography%20and%0A%20%20magic%20estimation&entry.906535625=Sitan%20Chen%20and%20Weiyuan%20Gong%20and%20Qi%20Ye%20and%20Zhihan%20Zhang&entry.1292438233=%20%20We%20study%20the%20task%20of%20agnostic%20tomography%3A%20given%20copies%20of%20an%20unknown%0A%24n%24-qubit%20state%20%24%5Crho%24%20which%20has%20fidelity%20%24%5Ctau%24%20with%20some%20state%20in%20a%20given%0Aclass%20%24C%24%2C%20find%20a%20state%20which%20has%20fidelity%20%24%5Cge%20%5Ctau%20-%20%5Cepsilon%24%20with%20%24%5Crho%24.%0AWe%20give%20a%20new%20framework%2C%20stabilizer%20bootstrapping%2C%20for%20designing%0Acomputationally%20efficient%20protocols%20for%20this%20task%2C%20and%20use%20this%20to%20get%20new%0Aagnostic%20tomography%20protocols%20for%20the%20following%20classes%3A%0A%20%20Stabilizer%20states%3A%20We%20give%20a%20protocol%20that%20runs%20in%20time%0A%24%5Cmathrm%7Bpoly%7D%28n%2C1/%5Cepsilon%29%5Ccdot%20%281/%5Ctau%29%5E%7BO%28%5Clog%281/%5Ctau%29%29%7D%24%2C%20answering%20an%0Aopen%20question%20posed%20by%20Grewal%2C%20Iyer%2C%20Kretschmer%2C%20Liang%20%5B40%5D%20and%20Anshu%20and%0AArunachalam%20%5B6%5D.%20Previous%20protocols%20ran%20in%20time%20%24%5Cmathrm%7Bexp%7D%28%5CTheta%28n%29%29%24%20or%0Arequired%20%24%5Ctau%3E%5Ccos%5E2%28%5Cpi/8%29%24.%0A%20%20States%20with%20stabilizer%20dimension%20%24n%20-%20t%24%3A%20We%20give%20a%20protocol%20that%20runs%20in%0Atime%20%24n%5E3%5Ccdot%282%5Et/%5Ctau%29%5E%7BO%28%5Clog%281/%5Cepsilon%29%29%7D%24%2C%20extending%20recent%20work%20on%0Alearning%20quantum%20states%20prepared%20by%20circuits%20with%20few%20non-Clifford%20gates%2C%20which%0Aonly%20applied%20in%20the%20realizable%20setting%20where%20%24%5Ctau%20%3D%201%24%20%5B30%2C%2037%2C%2046%2C%2061%5D.%0A%20%20Discrete%20product%20states%3A%20If%20%24C%20%3D%20K%5E%7B%5Cotimes%20n%7D%24%20for%20some%20%24%5Cmu%24-separated%0Adiscrete%20set%20%24K%24%20of%20single-qubit%20states%2C%20we%20give%20a%20protocol%20that%20runs%20in%20time%0A%24%28n/%5Cmu%29%5E%7BO%28%281%20%2B%20%5Clog%20%281/%5Ctau%29%29/%5Cmu%29%7D/%5Cepsilon%5E2%24.%20This%20strictly%20generalizes%20a%0Aprior%20guarantee%20which%20applied%20to%20stabilizer%20product%20states%20%5B39%5D.%20For%20stabilizer%0Aproduct%20states%2C%20we%20give%20a%20further%20improved%20protocol%20that%20runs%20in%20time%0A%24%28n%5E2/%5Cepsilon%5E2%29%5Ccdot%20%281/%5Ctau%29%5E%7BO%28%5Clog%281/%5Ctau%29%29%7D%24.%0A%20%20As%20a%20corollary%2C%20we%20give%20the%20first%20protocol%20for%20estimating%20stabilizer%0Afidelity%2C%20a%20standard%20measure%20of%20magic%20for%20quantum%20states%2C%20to%20error%20%24%5Cepsilon%24%0Ain%20%24n%5E3%20%5Cmathrm%7Bquasipoly%7D%281/%5Cepsilon%29%24%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06967v1&entry.124074799=Read"},
{"title": "Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation", "author": "Yanjie Dong and Haijun Zhang and Gang Wang and Shisheng Cui and Xiping Hu", "abstract": "  By using an parametric value function to replace the Monte-Carlo rollouts for\nvalue estimation, the actor-critic (AC) algorithms can reduce the variance of\nstochastic policy gradient so that to improve the convergence rate. While\nexisting works mainly focus on analyzing convergence rate of AC algorithms\nunder Markovian noise, the impacts of momentum on AC algorithms remain largely\nunexplored. In this work, we first propose a heavy-ball momentum based\nadvantage actor-critic (\\mbox{HB-A2C}) algorithm by integrating the heavy-ball\nmomentum into the critic recursion that is parameterized by a linear function.\nWhen the sample trajectory follows a Markov decision process, we quantitatively\ncertify the acceleration capability of the proposed HB-A2C algorithm. Our\ntheoretical results demonstrate that the proposed HB-A2C finds an\n$\\epsilon$-approximate stationary point with $\\oo{\\epsilon^{-2}}$ iterations\nfor reinforcement learning tasks with Markovian noise. Moreover, we also reveal\nthe dependence of learning rates on the length of the sample trajectory. By\ncarefully selecting the momentum factor of the critic recursion, the proposed\nHB-A2C can balance the errors introduced by the initialization and the\nstoschastic approximation.\n", "link": "http://arxiv.org/abs/2408.06945v1", "date": "2024-08-13", "relevancy": 1.8021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5075}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heavy-Ball%20Momentum%20Accelerated%20Actor-Critic%20With%20Function%20Approximation&body=Title%3A%20Heavy-Ball%20Momentum%20Accelerated%20Actor-Critic%20With%20Function%20Approximation%0AAuthor%3A%20Yanjie%20Dong%20and%20Haijun%20Zhang%20and%20Gang%20Wang%20and%20Shisheng%20Cui%20and%20Xiping%20Hu%0AAbstract%3A%20%20%20By%20using%20an%20parametric%20value%20function%20to%20replace%20the%20Monte-Carlo%20rollouts%20for%0Avalue%20estimation%2C%20the%20actor-critic%20%28AC%29%20algorithms%20can%20reduce%20the%20variance%20of%0Astochastic%20policy%20gradient%20so%20that%20to%20improve%20the%20convergence%20rate.%20While%0Aexisting%20works%20mainly%20focus%20on%20analyzing%20convergence%20rate%20of%20AC%20algorithms%0Aunder%20Markovian%20noise%2C%20the%20impacts%20of%20momentum%20on%20AC%20algorithms%20remain%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20first%20propose%20a%20heavy-ball%20momentum%20based%0Aadvantage%20actor-critic%20%28%5Cmbox%7BHB-A2C%7D%29%20algorithm%20by%20integrating%20the%20heavy-ball%0Amomentum%20into%20the%20critic%20recursion%20that%20is%20parameterized%20by%20a%20linear%20function.%0AWhen%20the%20sample%20trajectory%20follows%20a%20Markov%20decision%20process%2C%20we%20quantitatively%0Acertify%20the%20acceleration%20capability%20of%20the%20proposed%20HB-A2C%20algorithm.%20Our%0Atheoretical%20results%20demonstrate%20that%20the%20proposed%20HB-A2C%20finds%20an%0A%24%5Cepsilon%24-approximate%20stationary%20point%20with%20%24%5Coo%7B%5Cepsilon%5E%7B-2%7D%7D%24%20iterations%0Afor%20reinforcement%20learning%20tasks%20with%20Markovian%20noise.%20Moreover%2C%20we%20also%20reveal%0Athe%20dependence%20of%20learning%20rates%20on%20the%20length%20of%20the%20sample%20trajectory.%20By%0Acarefully%20selecting%20the%20momentum%20factor%20of%20the%20critic%20recursion%2C%20the%20proposed%0AHB-A2C%20can%20balance%20the%20errors%20introduced%20by%20the%20initialization%20and%20the%0Astoschastic%20approximation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeavy-Ball%2520Momentum%2520Accelerated%2520Actor-Critic%2520With%2520Function%2520Approximation%26entry.906535625%3DYanjie%2520Dong%2520and%2520Haijun%2520Zhang%2520and%2520Gang%2520Wang%2520and%2520Shisheng%2520Cui%2520and%2520Xiping%2520Hu%26entry.1292438233%3D%2520%2520By%2520using%2520an%2520parametric%2520value%2520function%2520to%2520replace%2520the%2520Monte-Carlo%2520rollouts%2520for%250Avalue%2520estimation%252C%2520the%2520actor-critic%2520%2528AC%2529%2520algorithms%2520can%2520reduce%2520the%2520variance%2520of%250Astochastic%2520policy%2520gradient%2520so%2520that%2520to%2520improve%2520the%2520convergence%2520rate.%2520While%250Aexisting%2520works%2520mainly%2520focus%2520on%2520analyzing%2520convergence%2520rate%2520of%2520AC%2520algorithms%250Aunder%2520Markovian%2520noise%252C%2520the%2520impacts%2520of%2520momentum%2520on%2520AC%2520algorithms%2520remain%2520largely%250Aunexplored.%2520In%2520this%2520work%252C%2520we%2520first%2520propose%2520a%2520heavy-ball%2520momentum%2520based%250Aadvantage%2520actor-critic%2520%2528%255Cmbox%257BHB-A2C%257D%2529%2520algorithm%2520by%2520integrating%2520the%2520heavy-ball%250Amomentum%2520into%2520the%2520critic%2520recursion%2520that%2520is%2520parameterized%2520by%2520a%2520linear%2520function.%250AWhen%2520the%2520sample%2520trajectory%2520follows%2520a%2520Markov%2520decision%2520process%252C%2520we%2520quantitatively%250Acertify%2520the%2520acceleration%2520capability%2520of%2520the%2520proposed%2520HB-A2C%2520algorithm.%2520Our%250Atheoretical%2520results%2520demonstrate%2520that%2520the%2520proposed%2520HB-A2C%2520finds%2520an%250A%2524%255Cepsilon%2524-approximate%2520stationary%2520point%2520with%2520%2524%255Coo%257B%255Cepsilon%255E%257B-2%257D%257D%2524%2520iterations%250Afor%2520reinforcement%2520learning%2520tasks%2520with%2520Markovian%2520noise.%2520Moreover%252C%2520we%2520also%2520reveal%250Athe%2520dependence%2520of%2520learning%2520rates%2520on%2520the%2520length%2520of%2520the%2520sample%2520trajectory.%2520By%250Acarefully%2520selecting%2520the%2520momentum%2520factor%2520of%2520the%2520critic%2520recursion%252C%2520the%2520proposed%250AHB-A2C%2520can%2520balance%2520the%2520errors%2520introduced%2520by%2520the%2520initialization%2520and%2520the%250Astoschastic%2520approximation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heavy-Ball%20Momentum%20Accelerated%20Actor-Critic%20With%20Function%20Approximation&entry.906535625=Yanjie%20Dong%20and%20Haijun%20Zhang%20and%20Gang%20Wang%20and%20Shisheng%20Cui%20and%20Xiping%20Hu&entry.1292438233=%20%20By%20using%20an%20parametric%20value%20function%20to%20replace%20the%20Monte-Carlo%20rollouts%20for%0Avalue%20estimation%2C%20the%20actor-critic%20%28AC%29%20algorithms%20can%20reduce%20the%20variance%20of%0Astochastic%20policy%20gradient%20so%20that%20to%20improve%20the%20convergence%20rate.%20While%0Aexisting%20works%20mainly%20focus%20on%20analyzing%20convergence%20rate%20of%20AC%20algorithms%0Aunder%20Markovian%20noise%2C%20the%20impacts%20of%20momentum%20on%20AC%20algorithms%20remain%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20first%20propose%20a%20heavy-ball%20momentum%20based%0Aadvantage%20actor-critic%20%28%5Cmbox%7BHB-A2C%7D%29%20algorithm%20by%20integrating%20the%20heavy-ball%0Amomentum%20into%20the%20critic%20recursion%20that%20is%20parameterized%20by%20a%20linear%20function.%0AWhen%20the%20sample%20trajectory%20follows%20a%20Markov%20decision%20process%2C%20we%20quantitatively%0Acertify%20the%20acceleration%20capability%20of%20the%20proposed%20HB-A2C%20algorithm.%20Our%0Atheoretical%20results%20demonstrate%20that%20the%20proposed%20HB-A2C%20finds%20an%0A%24%5Cepsilon%24-approximate%20stationary%20point%20with%20%24%5Coo%7B%5Cepsilon%5E%7B-2%7D%7D%24%20iterations%0Afor%20reinforcement%20learning%20tasks%20with%20Markovian%20noise.%20Moreover%2C%20we%20also%20reveal%0Athe%20dependence%20of%20learning%20rates%20on%20the%20length%20of%20the%20sample%20trajectory.%20By%0Acarefully%20selecting%20the%20momentum%20factor%20of%20the%20critic%20recursion%2C%20the%20proposed%0AHB-A2C%20can%20balance%20the%20errors%20introduced%20by%20the%20initialization%20and%20the%0Astoschastic%20approximation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06945v1&entry.124074799=Read"},
{"title": "Diagnosis extraction from unstructured Dutch echocardiogram reports\n  using span- and document-level characteristic classification", "author": "Bauke Arends and Melle Vessies and Dirk van Osch and Arco Teske and Pim van der Harst and Ren\u00e9 van Es and Bram van Es", "abstract": "  Clinical machine learning research and AI driven clinical decision support\nmodels rely on clinically accurate labels. Manually extracting these labels\nwith the help of clinical specialists is often time-consuming and expensive.\nThis study tests the feasibility of automatic span- and document-level\ndiagnosis extraction from unstructured Dutch echocardiogram reports.\n  We included 115,692 unstructured echocardiogram reports from the UMCU a large\nuniversity hospital in the Netherlands. A randomly selected subset was manually\nannotated for the occurrence and severity of eleven commonly described cardiac\ncharacteristics. We developed and tested several automatic labelling techniques\nat both span and document levels, using weighted and macro F1-score, precision,\nand recall for performance evaluation. We compared the performance of span\nlabelling against document labelling methods, which included both direct\ndocument classifiers and indirect document classifiers that rely on span\nclassification results.\n  The SpanCategorizer and MedRoBERTa.nl models outperformed all other span and\ndocument classifiers, respectively. The weighted F1-score varied between\ncharacteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98\nin MedRoBERTa.nl. Direct document classification was superior to indirect\ndocument classification using span classifiers. SetFit achieved competitive\ndocument classification performance using only 10\\% of the training data.\nUtilizing a reduced label set yielded near-perfect document classification\nresults.\n  We recommend using our published SpanCategorizer and MedRoBERTa.nl models for\nspan- and document-level diagnosis extraction from Dutch echocardiography\nreports. For settings with limited training data, SetFit may be a promising\nalternative for document classification.\n", "link": "http://arxiv.org/abs/2408.06930v1", "date": "2024-08-13", "relevancy": 1.7953, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4454}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports%0A%20%20using%20span-%20and%20document-level%20characteristic%20classification&body=Title%3A%20Diagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports%0A%20%20using%20span-%20and%20document-level%20characteristic%20classification%0AAuthor%3A%20Bauke%20Arends%20and%20Melle%20Vessies%20and%20Dirk%20van%20Osch%20and%20Arco%20Teske%20and%20Pim%20van%20der%20Harst%20and%20Ren%C3%A9%20van%20Es%20and%20Bram%20van%20Es%0AAbstract%3A%20%20%20Clinical%20machine%20learning%20research%20and%20AI%20driven%20clinical%20decision%20support%0Amodels%20rely%20on%20clinically%20accurate%20labels.%20Manually%20extracting%20these%20labels%0Awith%20the%20help%20of%20clinical%20specialists%20is%20often%20time-consuming%20and%20expensive.%0AThis%20study%20tests%20the%20feasibility%20of%20automatic%20span-%20and%20document-level%0Adiagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports.%0A%20%20We%20included%20115%2C692%20unstructured%20echocardiogram%20reports%20from%20the%20UMCU%20a%20large%0Auniversity%20hospital%20in%20the%20Netherlands.%20A%20randomly%20selected%20subset%20was%20manually%0Aannotated%20for%20the%20occurrence%20and%20severity%20of%20eleven%20commonly%20described%20cardiac%0Acharacteristics.%20We%20developed%20and%20tested%20several%20automatic%20labelling%20techniques%0Aat%20both%20span%20and%20document%20levels%2C%20using%20weighted%20and%20macro%20F1-score%2C%20precision%2C%0Aand%20recall%20for%20performance%20evaluation.%20We%20compared%20the%20performance%20of%20span%0Alabelling%20against%20document%20labelling%20methods%2C%20which%20included%20both%20direct%0Adocument%20classifiers%20and%20indirect%20document%20classifiers%20that%20rely%20on%20span%0Aclassification%20results.%0A%20%20The%20SpanCategorizer%20and%20MedRoBERTa.nl%20models%20outperformed%20all%20other%20span%20and%0Adocument%20classifiers%2C%20respectively.%20The%20weighted%20F1-score%20varied%20between%0Acharacteristics%2C%20ranging%20from%200.60%20to%200.93%20in%20SpanCategorizer%20and%200.96%20to%200.98%0Ain%20MedRoBERTa.nl.%20Direct%20document%20classification%20was%20superior%20to%20indirect%0Adocument%20classification%20using%20span%20classifiers.%20SetFit%20achieved%20competitive%0Adocument%20classification%20performance%20using%20only%2010%5C%25%20of%20the%20training%20data.%0AUtilizing%20a%20reduced%20label%20set%20yielded%20near-perfect%20document%20classification%0Aresults.%0A%20%20We%20recommend%20using%20our%20published%20SpanCategorizer%20and%20MedRoBERTa.nl%20models%20for%0Aspan-%20and%20document-level%20diagnosis%20extraction%20from%20Dutch%20echocardiography%0Areports.%20For%20settings%20with%20limited%20training%20data%2C%20SetFit%20may%20be%20a%20promising%0Aalternative%20for%20document%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagnosis%2520extraction%2520from%2520unstructured%2520Dutch%2520echocardiogram%2520reports%250A%2520%2520using%2520span-%2520and%2520document-level%2520characteristic%2520classification%26entry.906535625%3DBauke%2520Arends%2520and%2520Melle%2520Vessies%2520and%2520Dirk%2520van%2520Osch%2520and%2520Arco%2520Teske%2520and%2520Pim%2520van%2520der%2520Harst%2520and%2520Ren%25C3%25A9%2520van%2520Es%2520and%2520Bram%2520van%2520Es%26entry.1292438233%3D%2520%2520Clinical%2520machine%2520learning%2520research%2520and%2520AI%2520driven%2520clinical%2520decision%2520support%250Amodels%2520rely%2520on%2520clinically%2520accurate%2520labels.%2520Manually%2520extracting%2520these%2520labels%250Awith%2520the%2520help%2520of%2520clinical%2520specialists%2520is%2520often%2520time-consuming%2520and%2520expensive.%250AThis%2520study%2520tests%2520the%2520feasibility%2520of%2520automatic%2520span-%2520and%2520document-level%250Adiagnosis%2520extraction%2520from%2520unstructured%2520Dutch%2520echocardiogram%2520reports.%250A%2520%2520We%2520included%2520115%252C692%2520unstructured%2520echocardiogram%2520reports%2520from%2520the%2520UMCU%2520a%2520large%250Auniversity%2520hospital%2520in%2520the%2520Netherlands.%2520A%2520randomly%2520selected%2520subset%2520was%2520manually%250Aannotated%2520for%2520the%2520occurrence%2520and%2520severity%2520of%2520eleven%2520commonly%2520described%2520cardiac%250Acharacteristics.%2520We%2520developed%2520and%2520tested%2520several%2520automatic%2520labelling%2520techniques%250Aat%2520both%2520span%2520and%2520document%2520levels%252C%2520using%2520weighted%2520and%2520macro%2520F1-score%252C%2520precision%252C%250Aand%2520recall%2520for%2520performance%2520evaluation.%2520We%2520compared%2520the%2520performance%2520of%2520span%250Alabelling%2520against%2520document%2520labelling%2520methods%252C%2520which%2520included%2520both%2520direct%250Adocument%2520classifiers%2520and%2520indirect%2520document%2520classifiers%2520that%2520rely%2520on%2520span%250Aclassification%2520results.%250A%2520%2520The%2520SpanCategorizer%2520and%2520MedRoBERTa.nl%2520models%2520outperformed%2520all%2520other%2520span%2520and%250Adocument%2520classifiers%252C%2520respectively.%2520The%2520weighted%2520F1-score%2520varied%2520between%250Acharacteristics%252C%2520ranging%2520from%25200.60%2520to%25200.93%2520in%2520SpanCategorizer%2520and%25200.96%2520to%25200.98%250Ain%2520MedRoBERTa.nl.%2520Direct%2520document%2520classification%2520was%2520superior%2520to%2520indirect%250Adocument%2520classification%2520using%2520span%2520classifiers.%2520SetFit%2520achieved%2520competitive%250Adocument%2520classification%2520performance%2520using%2520only%252010%255C%2525%2520of%2520the%2520training%2520data.%250AUtilizing%2520a%2520reduced%2520label%2520set%2520yielded%2520near-perfect%2520document%2520classification%250Aresults.%250A%2520%2520We%2520recommend%2520using%2520our%2520published%2520SpanCategorizer%2520and%2520MedRoBERTa.nl%2520models%2520for%250Aspan-%2520and%2520document-level%2520diagnosis%2520extraction%2520from%2520Dutch%2520echocardiography%250Areports.%2520For%2520settings%2520with%2520limited%2520training%2520data%252C%2520SetFit%2520may%2520be%2520a%2520promising%250Aalternative%2520for%2520document%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports%0A%20%20using%20span-%20and%20document-level%20characteristic%20classification&entry.906535625=Bauke%20Arends%20and%20Melle%20Vessies%20and%20Dirk%20van%20Osch%20and%20Arco%20Teske%20and%20Pim%20van%20der%20Harst%20and%20Ren%C3%A9%20van%20Es%20and%20Bram%20van%20Es&entry.1292438233=%20%20Clinical%20machine%20learning%20research%20and%20AI%20driven%20clinical%20decision%20support%0Amodels%20rely%20on%20clinically%20accurate%20labels.%20Manually%20extracting%20these%20labels%0Awith%20the%20help%20of%20clinical%20specialists%20is%20often%20time-consuming%20and%20expensive.%0AThis%20study%20tests%20the%20feasibility%20of%20automatic%20span-%20and%20document-level%0Adiagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports.%0A%20%20We%20included%20115%2C692%20unstructured%20echocardiogram%20reports%20from%20the%20UMCU%20a%20large%0Auniversity%20hospital%20in%20the%20Netherlands.%20A%20randomly%20selected%20subset%20was%20manually%0Aannotated%20for%20the%20occurrence%20and%20severity%20of%20eleven%20commonly%20described%20cardiac%0Acharacteristics.%20We%20developed%20and%20tested%20several%20automatic%20labelling%20techniques%0Aat%20both%20span%20and%20document%20levels%2C%20using%20weighted%20and%20macro%20F1-score%2C%20precision%2C%0Aand%20recall%20for%20performance%20evaluation.%20We%20compared%20the%20performance%20of%20span%0Alabelling%20against%20document%20labelling%20methods%2C%20which%20included%20both%20direct%0Adocument%20classifiers%20and%20indirect%20document%20classifiers%20that%20rely%20on%20span%0Aclassification%20results.%0A%20%20The%20SpanCategorizer%20and%20MedRoBERTa.nl%20models%20outperformed%20all%20other%20span%20and%0Adocument%20classifiers%2C%20respectively.%20The%20weighted%20F1-score%20varied%20between%0Acharacteristics%2C%20ranging%20from%200.60%20to%200.93%20in%20SpanCategorizer%20and%200.96%20to%200.98%0Ain%20MedRoBERTa.nl.%20Direct%20document%20classification%20was%20superior%20to%20indirect%0Adocument%20classification%20using%20span%20classifiers.%20SetFit%20achieved%20competitive%0Adocument%20classification%20performance%20using%20only%2010%5C%25%20of%20the%20training%20data.%0AUtilizing%20a%20reduced%20label%20set%20yielded%20near-perfect%20document%20classification%0Aresults.%0A%20%20We%20recommend%20using%20our%20published%20SpanCategorizer%20and%20MedRoBERTa.nl%20models%20for%0Aspan-%20and%20document-level%20diagnosis%20extraction%20from%20Dutch%20echocardiography%0Areports.%20For%20settings%20with%20limited%20training%20data%2C%20SetFit%20may%20be%20a%20promising%0Aalternative%20for%20document%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06930v1&entry.124074799=Read"},
{"title": "HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge\n  Representation", "author": "Zhiwei Hu and V\u00edctor Guti\u00e9rrez-Basulto and Zhiliang Xiang and Ru Li and Jeff Z. Pan", "abstract": "  In a hyper-relational knowledge graph (HKG), each fact is composed of a main\ntriple associated with attribute-value qualifiers, which express additional\nfactual knowledge. The hyper-relational knowledge graph completion (HKGC) task\naims at inferring plausible missing links in a HKG. Most existing approaches to\nHKGC focus on enhancing the communication between qualifier pairs and main\ntriples, while overlooking two important properties that emerge from the\nmonotonicity of the hyper-relational graphs representation regime. Stage\nReasoning allows for a two-step reasoning process, facilitating the integration\nof coarse-grained inference results derived solely from main triples and\nfine-grained inference results obtained from hyper-relational facts with\nqualifiers. In the initial stage, coarse-grained results provide an upper bound\nfor correct predictions, which are subsequently refined in the fine-grained\nstep. More generally, Qualifier Monotonicity implies that by attaching more\nqualifier pairs to a main triple, we may only narrow down the answer set, but\nnever enlarge it. This paper proposes the HyperMono model for hyper-relational\nknowledge graph completion, which realizes stage reasoning and qualifier\nmonotonicity. To implement qualifier monotonicity HyperMono resorts to cone\nembeddings. Experiments on three real-world datasets with three different\nscenario conditions demonstrate the strong performance of HyperMono when\ncompared to the SoTA.\n", "link": "http://arxiv.org/abs/2404.09848v2", "date": "2024-08-13", "relevancy": 1.7811, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4585}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4463}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperMono%3A%20A%20Monotonicity-aware%20Approach%20to%20Hyper-Relational%20Knowledge%0A%20%20Representation&body=Title%3A%20HyperMono%3A%20A%20Monotonicity-aware%20Approach%20to%20Hyper-Relational%20Knowledge%0A%20%20Representation%0AAuthor%3A%20Zhiwei%20Hu%20and%20V%C3%ADctor%20Guti%C3%A9rrez-Basulto%20and%20Zhiliang%20Xiang%20and%20Ru%20Li%20and%20Jeff%20Z.%20Pan%0AAbstract%3A%20%20%20In%20a%20hyper-relational%20knowledge%20graph%20%28HKG%29%2C%20each%20fact%20is%20composed%20of%20a%20main%0Atriple%20associated%20with%20attribute-value%20qualifiers%2C%20which%20express%20additional%0Afactual%20knowledge.%20The%20hyper-relational%20knowledge%20graph%20completion%20%28HKGC%29%20task%0Aaims%20at%20inferring%20plausible%20missing%20links%20in%20a%20HKG.%20Most%20existing%20approaches%20to%0AHKGC%20focus%20on%20enhancing%20the%20communication%20between%20qualifier%20pairs%20and%20main%0Atriples%2C%20while%20overlooking%20two%20important%20properties%20that%20emerge%20from%20the%0Amonotonicity%20of%20the%20hyper-relational%20graphs%20representation%20regime.%20Stage%0AReasoning%20allows%20for%20a%20two-step%20reasoning%20process%2C%20facilitating%20the%20integration%0Aof%20coarse-grained%20inference%20results%20derived%20solely%20from%20main%20triples%20and%0Afine-grained%20inference%20results%20obtained%20from%20hyper-relational%20facts%20with%0Aqualifiers.%20In%20the%20initial%20stage%2C%20coarse-grained%20results%20provide%20an%20upper%20bound%0Afor%20correct%20predictions%2C%20which%20are%20subsequently%20refined%20in%20the%20fine-grained%0Astep.%20More%20generally%2C%20Qualifier%20Monotonicity%20implies%20that%20by%20attaching%20more%0Aqualifier%20pairs%20to%20a%20main%20triple%2C%20we%20may%20only%20narrow%20down%20the%20answer%20set%2C%20but%0Anever%20enlarge%20it.%20This%20paper%20proposes%20the%20HyperMono%20model%20for%20hyper-relational%0Aknowledge%20graph%20completion%2C%20which%20realizes%20stage%20reasoning%20and%20qualifier%0Amonotonicity.%20To%20implement%20qualifier%20monotonicity%20HyperMono%20resorts%20to%20cone%0Aembeddings.%20Experiments%20on%20three%20real-world%20datasets%20with%20three%20different%0Ascenario%20conditions%20demonstrate%20the%20strong%20performance%20of%20HyperMono%20when%0Acompared%20to%20the%20SoTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperMono%253A%2520A%2520Monotonicity-aware%2520Approach%2520to%2520Hyper-Relational%2520Knowledge%250A%2520%2520Representation%26entry.906535625%3DZhiwei%2520Hu%2520and%2520V%25C3%25ADctor%2520Guti%25C3%25A9rrez-Basulto%2520and%2520Zhiliang%2520Xiang%2520and%2520Ru%2520Li%2520and%2520Jeff%2520Z.%2520Pan%26entry.1292438233%3D%2520%2520In%2520a%2520hyper-relational%2520knowledge%2520graph%2520%2528HKG%2529%252C%2520each%2520fact%2520is%2520composed%2520of%2520a%2520main%250Atriple%2520associated%2520with%2520attribute-value%2520qualifiers%252C%2520which%2520express%2520additional%250Afactual%2520knowledge.%2520The%2520hyper-relational%2520knowledge%2520graph%2520completion%2520%2528HKGC%2529%2520task%250Aaims%2520at%2520inferring%2520plausible%2520missing%2520links%2520in%2520a%2520HKG.%2520Most%2520existing%2520approaches%2520to%250AHKGC%2520focus%2520on%2520enhancing%2520the%2520communication%2520between%2520qualifier%2520pairs%2520and%2520main%250Atriples%252C%2520while%2520overlooking%2520two%2520important%2520properties%2520that%2520emerge%2520from%2520the%250Amonotonicity%2520of%2520the%2520hyper-relational%2520graphs%2520representation%2520regime.%2520Stage%250AReasoning%2520allows%2520for%2520a%2520two-step%2520reasoning%2520process%252C%2520facilitating%2520the%2520integration%250Aof%2520coarse-grained%2520inference%2520results%2520derived%2520solely%2520from%2520main%2520triples%2520and%250Afine-grained%2520inference%2520results%2520obtained%2520from%2520hyper-relational%2520facts%2520with%250Aqualifiers.%2520In%2520the%2520initial%2520stage%252C%2520coarse-grained%2520results%2520provide%2520an%2520upper%2520bound%250Afor%2520correct%2520predictions%252C%2520which%2520are%2520subsequently%2520refined%2520in%2520the%2520fine-grained%250Astep.%2520More%2520generally%252C%2520Qualifier%2520Monotonicity%2520implies%2520that%2520by%2520attaching%2520more%250Aqualifier%2520pairs%2520to%2520a%2520main%2520triple%252C%2520we%2520may%2520only%2520narrow%2520down%2520the%2520answer%2520set%252C%2520but%250Anever%2520enlarge%2520it.%2520This%2520paper%2520proposes%2520the%2520HyperMono%2520model%2520for%2520hyper-relational%250Aknowledge%2520graph%2520completion%252C%2520which%2520realizes%2520stage%2520reasoning%2520and%2520qualifier%250Amonotonicity.%2520To%2520implement%2520qualifier%2520monotonicity%2520HyperMono%2520resorts%2520to%2520cone%250Aembeddings.%2520Experiments%2520on%2520three%2520real-world%2520datasets%2520with%2520three%2520different%250Ascenario%2520conditions%2520demonstrate%2520the%2520strong%2520performance%2520of%2520HyperMono%2520when%250Acompared%2520to%2520the%2520SoTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperMono%3A%20A%20Monotonicity-aware%20Approach%20to%20Hyper-Relational%20Knowledge%0A%20%20Representation&entry.906535625=Zhiwei%20Hu%20and%20V%C3%ADctor%20Guti%C3%A9rrez-Basulto%20and%20Zhiliang%20Xiang%20and%20Ru%20Li%20and%20Jeff%20Z.%20Pan&entry.1292438233=%20%20In%20a%20hyper-relational%20knowledge%20graph%20%28HKG%29%2C%20each%20fact%20is%20composed%20of%20a%20main%0Atriple%20associated%20with%20attribute-value%20qualifiers%2C%20which%20express%20additional%0Afactual%20knowledge.%20The%20hyper-relational%20knowledge%20graph%20completion%20%28HKGC%29%20task%0Aaims%20at%20inferring%20plausible%20missing%20links%20in%20a%20HKG.%20Most%20existing%20approaches%20to%0AHKGC%20focus%20on%20enhancing%20the%20communication%20between%20qualifier%20pairs%20and%20main%0Atriples%2C%20while%20overlooking%20two%20important%20properties%20that%20emerge%20from%20the%0Amonotonicity%20of%20the%20hyper-relational%20graphs%20representation%20regime.%20Stage%0AReasoning%20allows%20for%20a%20two-step%20reasoning%20process%2C%20facilitating%20the%20integration%0Aof%20coarse-grained%20inference%20results%20derived%20solely%20from%20main%20triples%20and%0Afine-grained%20inference%20results%20obtained%20from%20hyper-relational%20facts%20with%0Aqualifiers.%20In%20the%20initial%20stage%2C%20coarse-grained%20results%20provide%20an%20upper%20bound%0Afor%20correct%20predictions%2C%20which%20are%20subsequently%20refined%20in%20the%20fine-grained%0Astep.%20More%20generally%2C%20Qualifier%20Monotonicity%20implies%20that%20by%20attaching%20more%0Aqualifier%20pairs%20to%20a%20main%20triple%2C%20we%20may%20only%20narrow%20down%20the%20answer%20set%2C%20but%0Anever%20enlarge%20it.%20This%20paper%20proposes%20the%20HyperMono%20model%20for%20hyper-relational%0Aknowledge%20graph%20completion%2C%20which%20realizes%20stage%20reasoning%20and%20qualifier%0Amonotonicity.%20To%20implement%20qualifier%20monotonicity%20HyperMono%20resorts%20to%20cone%0Aembeddings.%20Experiments%20on%20three%20real-world%20datasets%20with%20three%20different%0Ascenario%20conditions%20demonstrate%20the%20strong%20performance%20of%20HyperMono%20when%0Acompared%20to%20the%20SoTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09848v2&entry.124074799=Read"},
{"title": "Divide and Conquer: Improving Multi-Camera 3D Perception with 2D\n  Semantic-Depth Priors and Input-Dependent Queries", "author": "Qi Song and Qingyong Hu and Chi Zhang and Yongquan Chen and Rui Huang", "abstract": "  3D perception tasks, such as 3D object detection and Bird's-Eye-View (BEV)\nsegmentation using multi-camera images, have drawn significant attention\nrecently. Despite the fact that accurately estimating both semantic and 3D\nscene layouts are crucial for this task, existing techniques often neglect the\nsynergistic effects of semantic and depth cues, leading to the occurrence of\nclassification and position estimation errors. Additionally, the\ninput-independent nature of initial queries also limits the learning capacity\nof Transformer-based models. To tackle these challenges, we propose an\ninput-aware Transformer framework that leverages Semantics and Depth as priors\n(named SDTR). Our approach involves the use of an S-D Encoder that explicitly\nmodels semantic and depth priors, thereby disentangling the learning process of\nobject categorization and position estimation. Moreover, we introduce a\nPrior-guided Query Builder that incorporates the semantic prior into the\ninitial queries of the Transformer, resulting in more effective input-aware\nqueries. Extensive experiments on the nuScenes and Lyft benchmarks demonstrate\nthe state-of-the-art performance of our method in both 3D object detection and\nBEV segmentation tasks.\n", "link": "http://arxiv.org/abs/2408.06901v1", "date": "2024-08-13", "relevancy": 1.7755, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6016}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5888}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide%20and%20Conquer%3A%20Improving%20Multi-Camera%203D%20Perception%20with%202D%0A%20%20Semantic-Depth%20Priors%20and%20Input-Dependent%20Queries&body=Title%3A%20Divide%20and%20Conquer%3A%20Improving%20Multi-Camera%203D%20Perception%20with%202D%0A%20%20Semantic-Depth%20Priors%20and%20Input-Dependent%20Queries%0AAuthor%3A%20Qi%20Song%20and%20Qingyong%20Hu%20and%20Chi%20Zhang%20and%20Yongquan%20Chen%20and%20Rui%20Huang%0AAbstract%3A%20%20%203D%20perception%20tasks%2C%20such%20as%203D%20object%20detection%20and%20Bird%27s-Eye-View%20%28BEV%29%0Asegmentation%20using%20multi-camera%20images%2C%20have%20drawn%20significant%20attention%0Arecently.%20Despite%20the%20fact%20that%20accurately%20estimating%20both%20semantic%20and%203D%0Ascene%20layouts%20are%20crucial%20for%20this%20task%2C%20existing%20techniques%20often%20neglect%20the%0Asynergistic%20effects%20of%20semantic%20and%20depth%20cues%2C%20leading%20to%20the%20occurrence%20of%0Aclassification%20and%20position%20estimation%20errors.%20Additionally%2C%20the%0Ainput-independent%20nature%20of%20initial%20queries%20also%20limits%20the%20learning%20capacity%0Aof%20Transformer-based%20models.%20To%20tackle%20these%20challenges%2C%20we%20propose%20an%0Ainput-aware%20Transformer%20framework%20that%20leverages%20Semantics%20and%20Depth%20as%20priors%0A%28named%20SDTR%29.%20Our%20approach%20involves%20the%20use%20of%20an%20S-D%20Encoder%20that%20explicitly%0Amodels%20semantic%20and%20depth%20priors%2C%20thereby%20disentangling%20the%20learning%20process%20of%0Aobject%20categorization%20and%20position%20estimation.%20Moreover%2C%20we%20introduce%20a%0APrior-guided%20Query%20Builder%20that%20incorporates%20the%20semantic%20prior%20into%20the%0Ainitial%20queries%20of%20the%20Transformer%2C%20resulting%20in%20more%20effective%20input-aware%0Aqueries.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Lyft%20benchmarks%20demonstrate%0Athe%20state-of-the-art%20performance%20of%20our%20method%20in%20both%203D%20object%20detection%20and%0ABEV%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide%2520and%2520Conquer%253A%2520Improving%2520Multi-Camera%25203D%2520Perception%2520with%25202D%250A%2520%2520Semantic-Depth%2520Priors%2520and%2520Input-Dependent%2520Queries%26entry.906535625%3DQi%2520Song%2520and%2520Qingyong%2520Hu%2520and%2520Chi%2520Zhang%2520and%2520Yongquan%2520Chen%2520and%2520Rui%2520Huang%26entry.1292438233%3D%2520%25203D%2520perception%2520tasks%252C%2520such%2520as%25203D%2520object%2520detection%2520and%2520Bird%2527s-Eye-View%2520%2528BEV%2529%250Asegmentation%2520using%2520multi-camera%2520images%252C%2520have%2520drawn%2520significant%2520attention%250Arecently.%2520Despite%2520the%2520fact%2520that%2520accurately%2520estimating%2520both%2520semantic%2520and%25203D%250Ascene%2520layouts%2520are%2520crucial%2520for%2520this%2520task%252C%2520existing%2520techniques%2520often%2520neglect%2520the%250Asynergistic%2520effects%2520of%2520semantic%2520and%2520depth%2520cues%252C%2520leading%2520to%2520the%2520occurrence%2520of%250Aclassification%2520and%2520position%2520estimation%2520errors.%2520Additionally%252C%2520the%250Ainput-independent%2520nature%2520of%2520initial%2520queries%2520also%2520limits%2520the%2520learning%2520capacity%250Aof%2520Transformer-based%2520models.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520an%250Ainput-aware%2520Transformer%2520framework%2520that%2520leverages%2520Semantics%2520and%2520Depth%2520as%2520priors%250A%2528named%2520SDTR%2529.%2520Our%2520approach%2520involves%2520the%2520use%2520of%2520an%2520S-D%2520Encoder%2520that%2520explicitly%250Amodels%2520semantic%2520and%2520depth%2520priors%252C%2520thereby%2520disentangling%2520the%2520learning%2520process%2520of%250Aobject%2520categorization%2520and%2520position%2520estimation.%2520Moreover%252C%2520we%2520introduce%2520a%250APrior-guided%2520Query%2520Builder%2520that%2520incorporates%2520the%2520semantic%2520prior%2520into%2520the%250Ainitial%2520queries%2520of%2520the%2520Transformer%252C%2520resulting%2520in%2520more%2520effective%2520input-aware%250Aqueries.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520and%2520Lyft%2520benchmarks%2520demonstrate%250Athe%2520state-of-the-art%2520performance%2520of%2520our%2520method%2520in%2520both%25203D%2520object%2520detection%2520and%250ABEV%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide%20and%20Conquer%3A%20Improving%20Multi-Camera%203D%20Perception%20with%202D%0A%20%20Semantic-Depth%20Priors%20and%20Input-Dependent%20Queries&entry.906535625=Qi%20Song%20and%20Qingyong%20Hu%20and%20Chi%20Zhang%20and%20Yongquan%20Chen%20and%20Rui%20Huang&entry.1292438233=%20%203D%20perception%20tasks%2C%20such%20as%203D%20object%20detection%20and%20Bird%27s-Eye-View%20%28BEV%29%0Asegmentation%20using%20multi-camera%20images%2C%20have%20drawn%20significant%20attention%0Arecently.%20Despite%20the%20fact%20that%20accurately%20estimating%20both%20semantic%20and%203D%0Ascene%20layouts%20are%20crucial%20for%20this%20task%2C%20existing%20techniques%20often%20neglect%20the%0Asynergistic%20effects%20of%20semantic%20and%20depth%20cues%2C%20leading%20to%20the%20occurrence%20of%0Aclassification%20and%20position%20estimation%20errors.%20Additionally%2C%20the%0Ainput-independent%20nature%20of%20initial%20queries%20also%20limits%20the%20learning%20capacity%0Aof%20Transformer-based%20models.%20To%20tackle%20these%20challenges%2C%20we%20propose%20an%0Ainput-aware%20Transformer%20framework%20that%20leverages%20Semantics%20and%20Depth%20as%20priors%0A%28named%20SDTR%29.%20Our%20approach%20involves%20the%20use%20of%20an%20S-D%20Encoder%20that%20explicitly%0Amodels%20semantic%20and%20depth%20priors%2C%20thereby%20disentangling%20the%20learning%20process%20of%0Aobject%20categorization%20and%20position%20estimation.%20Moreover%2C%20we%20introduce%20a%0APrior-guided%20Query%20Builder%20that%20incorporates%20the%20semantic%20prior%20into%20the%0Ainitial%20queries%20of%20the%20Transformer%2C%20resulting%20in%20more%20effective%20input-aware%0Aqueries.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Lyft%20benchmarks%20demonstrate%0Athe%20state-of-the-art%20performance%20of%20our%20method%20in%20both%203D%20object%20detection%20and%0ABEV%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06901v1&entry.124074799=Read"},
{"title": "AuToMATo: A Parameter-Free Persistence-Based Clustering Algorithm", "author": "Marius Huber and Sara Kalisnik and Patrick Schnider", "abstract": "  We present AuToMATo, a novel parameter-free clustering algorithm based on\npersistent homology. AuToMATo combines the existing ToMATo clustering algorithm\nwith a bootstrapping procedure in order to separate significant peaks of an\nestimated density function from non-significant ones. We perform a thorough\ncomparison of AuToMATo against many other state-of-the-art clustering\nalgorithms. We find that not only that AuToMATo compares favorably against\nother parameter-free clustering algorithms, but in many instances also\nsignificantly outperforms even the best selection of parameters for other\nalgorithms. AuToMATo is motivated by applications in topological data analysis,\nin particular the Mapper algorithm, where it is desirable to work with a\nparameter-free clustering algorithm. Indeed, we provide evidence that AuToMATo\nperforms well when used with Mapper. Finally, we provide an open-source\nimplementation of AuToMATo in Python that is fully compatible with the\nstandardscikit-learn architecture.\n", "link": "http://arxiv.org/abs/2408.06958v1", "date": "2024-08-13", "relevancy": 1.766, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4621}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuToMATo%3A%20A%20Parameter-Free%20Persistence-Based%20Clustering%20Algorithm&body=Title%3A%20AuToMATo%3A%20A%20Parameter-Free%20Persistence-Based%20Clustering%20Algorithm%0AAuthor%3A%20Marius%20Huber%20and%20Sara%20Kalisnik%20and%20Patrick%20Schnider%0AAbstract%3A%20%20%20We%20present%20AuToMATo%2C%20a%20novel%20parameter-free%20clustering%20algorithm%20based%20on%0Apersistent%20homology.%20AuToMATo%20combines%20the%20existing%20ToMATo%20clustering%20algorithm%0Awith%20a%20bootstrapping%20procedure%20in%20order%20to%20separate%20significant%20peaks%20of%20an%0Aestimated%20density%20function%20from%20non-significant%20ones.%20We%20perform%20a%20thorough%0Acomparison%20of%20AuToMATo%20against%20many%20other%20state-of-the-art%20clustering%0Aalgorithms.%20We%20find%20that%20not%20only%20that%20AuToMATo%20compares%20favorably%20against%0Aother%20parameter-free%20clustering%20algorithms%2C%20but%20in%20many%20instances%20also%0Asignificantly%20outperforms%20even%20the%20best%20selection%20of%20parameters%20for%20other%0Aalgorithms.%20AuToMATo%20is%20motivated%20by%20applications%20in%20topological%20data%20analysis%2C%0Ain%20particular%20the%20Mapper%20algorithm%2C%20where%20it%20is%20desirable%20to%20work%20with%20a%0Aparameter-free%20clustering%20algorithm.%20Indeed%2C%20we%20provide%20evidence%20that%20AuToMATo%0Aperforms%20well%20when%20used%20with%20Mapper.%20Finally%2C%20we%20provide%20an%20open-source%0Aimplementation%20of%20AuToMATo%20in%20Python%20that%20is%20fully%20compatible%20with%20the%0Astandardscikit-learn%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuToMATo%253A%2520A%2520Parameter-Free%2520Persistence-Based%2520Clustering%2520Algorithm%26entry.906535625%3DMarius%2520Huber%2520and%2520Sara%2520Kalisnik%2520and%2520Patrick%2520Schnider%26entry.1292438233%3D%2520%2520We%2520present%2520AuToMATo%252C%2520a%2520novel%2520parameter-free%2520clustering%2520algorithm%2520based%2520on%250Apersistent%2520homology.%2520AuToMATo%2520combines%2520the%2520existing%2520ToMATo%2520clustering%2520algorithm%250Awith%2520a%2520bootstrapping%2520procedure%2520in%2520order%2520to%2520separate%2520significant%2520peaks%2520of%2520an%250Aestimated%2520density%2520function%2520from%2520non-significant%2520ones.%2520We%2520perform%2520a%2520thorough%250Acomparison%2520of%2520AuToMATo%2520against%2520many%2520other%2520state-of-the-art%2520clustering%250Aalgorithms.%2520We%2520find%2520that%2520not%2520only%2520that%2520AuToMATo%2520compares%2520favorably%2520against%250Aother%2520parameter-free%2520clustering%2520algorithms%252C%2520but%2520in%2520many%2520instances%2520also%250Asignificantly%2520outperforms%2520even%2520the%2520best%2520selection%2520of%2520parameters%2520for%2520other%250Aalgorithms.%2520AuToMATo%2520is%2520motivated%2520by%2520applications%2520in%2520topological%2520data%2520analysis%252C%250Ain%2520particular%2520the%2520Mapper%2520algorithm%252C%2520where%2520it%2520is%2520desirable%2520to%2520work%2520with%2520a%250Aparameter-free%2520clustering%2520algorithm.%2520Indeed%252C%2520we%2520provide%2520evidence%2520that%2520AuToMATo%250Aperforms%2520well%2520when%2520used%2520with%2520Mapper.%2520Finally%252C%2520we%2520provide%2520an%2520open-source%250Aimplementation%2520of%2520AuToMATo%2520in%2520Python%2520that%2520is%2520fully%2520compatible%2520with%2520the%250Astandardscikit-learn%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuToMATo%3A%20A%20Parameter-Free%20Persistence-Based%20Clustering%20Algorithm&entry.906535625=Marius%20Huber%20and%20Sara%20Kalisnik%20and%20Patrick%20Schnider&entry.1292438233=%20%20We%20present%20AuToMATo%2C%20a%20novel%20parameter-free%20clustering%20algorithm%20based%20on%0Apersistent%20homology.%20AuToMATo%20combines%20the%20existing%20ToMATo%20clustering%20algorithm%0Awith%20a%20bootstrapping%20procedure%20in%20order%20to%20separate%20significant%20peaks%20of%20an%0Aestimated%20density%20function%20from%20non-significant%20ones.%20We%20perform%20a%20thorough%0Acomparison%20of%20AuToMATo%20against%20many%20other%20state-of-the-art%20clustering%0Aalgorithms.%20We%20find%20that%20not%20only%20that%20AuToMATo%20compares%20favorably%20against%0Aother%20parameter-free%20clustering%20algorithms%2C%20but%20in%20many%20instances%20also%0Asignificantly%20outperforms%20even%20the%20best%20selection%20of%20parameters%20for%20other%0Aalgorithms.%20AuToMATo%20is%20motivated%20by%20applications%20in%20topological%20data%20analysis%2C%0Ain%20particular%20the%20Mapper%20algorithm%2C%20where%20it%20is%20desirable%20to%20work%20with%20a%0Aparameter-free%20clustering%20algorithm.%20Indeed%2C%20we%20provide%20evidence%20that%20AuToMATo%0Aperforms%20well%20when%20used%20with%20Mapper.%20Finally%2C%20we%20provide%20an%20open-source%0Aimplementation%20of%20AuToMATo%20in%20Python%20that%20is%20fully%20compatible%20with%20the%0Astandardscikit-learn%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06958v1&entry.124074799=Read"},
{"title": "CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset\n  Augmentation using Diffusion Models", "author": "Kushal Kumar Jain and Steve Grosz and Anoop M. Namboodiri and Anil K. Jain", "abstract": "  Forensic sketch-to-mugshot matching is a challenging task in face\nrecognition, primarily hindered by the scarcity of annotated forensic sketches\nand the modality gap between sketches and photographs. To address this, we\npropose CLIP4Sketch, a novel approach that leverages diffusion models to\ngenerate a large and diverse set of sketch images, which helps in enhancing the\nperformance of face recognition systems in sketch-to-mugshot matching. Our\nmethod utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate\nsketches with explicit control over identity and style. We combine CLIP and\nAdaface embeddings of a reference mugshot, along with textual descriptions of\nstyle, as the conditions to the diffusion model. We demonstrate the efficacy of\nour approach by generating a comprehensive dataset of sketches corresponding to\nmugshots and training a face recognition model on our synthetic data. Our\nresults show significant improvements in sketch-to-mugshot matching accuracy\nover training on an existing, limited amount of real face sketch data,\nvalidating the potential of diffusion models in enhancing the performance of\nface recognition systems across modalities. We also compare our dataset with\ndatasets generated using GAN-based methods to show its superiority.\n", "link": "http://arxiv.org/abs/2408.01233v2", "date": "2024-08-13", "relevancy": 1.7314, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6044}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.587}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP4Sketch%3A%20Enhancing%20Sketch%20to%20Mugshot%20Matching%20through%20Dataset%0A%20%20Augmentation%20using%20Diffusion%20Models&body=Title%3A%20CLIP4Sketch%3A%20Enhancing%20Sketch%20to%20Mugshot%20Matching%20through%20Dataset%0A%20%20Augmentation%20using%20Diffusion%20Models%0AAuthor%3A%20Kushal%20Kumar%20Jain%20and%20Steve%20Grosz%20and%20Anoop%20M.%20Namboodiri%20and%20Anil%20K.%20Jain%0AAbstract%3A%20%20%20Forensic%20sketch-to-mugshot%20matching%20is%20a%20challenging%20task%20in%20face%0Arecognition%2C%20primarily%20hindered%20by%20the%20scarcity%20of%20annotated%20forensic%20sketches%0Aand%20the%20modality%20gap%20between%20sketches%20and%20photographs.%20To%20address%20this%2C%20we%0Apropose%20CLIP4Sketch%2C%20a%20novel%20approach%20that%20leverages%20diffusion%20models%20to%0Agenerate%20a%20large%20and%20diverse%20set%20of%20sketch%20images%2C%20which%20helps%20in%20enhancing%20the%0Aperformance%20of%20face%20recognition%20systems%20in%20sketch-to-mugshot%20matching.%20Our%0Amethod%20utilizes%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20to%20generate%0Asketches%20with%20explicit%20control%20over%20identity%20and%20style.%20We%20combine%20CLIP%20and%0AAdaface%20embeddings%20of%20a%20reference%20mugshot%2C%20along%20with%20textual%20descriptions%20of%0Astyle%2C%20as%20the%20conditions%20to%20the%20diffusion%20model.%20We%20demonstrate%20the%20efficacy%20of%0Aour%20approach%20by%20generating%20a%20comprehensive%20dataset%20of%20sketches%20corresponding%20to%0Amugshots%20and%20training%20a%20face%20recognition%20model%20on%20our%20synthetic%20data.%20Our%0Aresults%20show%20significant%20improvements%20in%20sketch-to-mugshot%20matching%20accuracy%0Aover%20training%20on%20an%20existing%2C%20limited%20amount%20of%20real%20face%20sketch%20data%2C%0Avalidating%20the%20potential%20of%20diffusion%20models%20in%20enhancing%20the%20performance%20of%0Aface%20recognition%20systems%20across%20modalities.%20We%20also%20compare%20our%20dataset%20with%0Adatasets%20generated%20using%20GAN-based%20methods%20to%20show%20its%20superiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01233v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP4Sketch%253A%2520Enhancing%2520Sketch%2520to%2520Mugshot%2520Matching%2520through%2520Dataset%250A%2520%2520Augmentation%2520using%2520Diffusion%2520Models%26entry.906535625%3DKushal%2520Kumar%2520Jain%2520and%2520Steve%2520Grosz%2520and%2520Anoop%2520M.%2520Namboodiri%2520and%2520Anil%2520K.%2520Jain%26entry.1292438233%3D%2520%2520Forensic%2520sketch-to-mugshot%2520matching%2520is%2520a%2520challenging%2520task%2520in%2520face%250Arecognition%252C%2520primarily%2520hindered%2520by%2520the%2520scarcity%2520of%2520annotated%2520forensic%2520sketches%250Aand%2520the%2520modality%2520gap%2520between%2520sketches%2520and%2520photographs.%2520To%2520address%2520this%252C%2520we%250Apropose%2520CLIP4Sketch%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520diffusion%2520models%2520to%250Agenerate%2520a%2520large%2520and%2520diverse%2520set%2520of%2520sketch%2520images%252C%2520which%2520helps%2520in%2520enhancing%2520the%250Aperformance%2520of%2520face%2520recognition%2520systems%2520in%2520sketch-to-mugshot%2520matching.%2520Our%250Amethod%2520utilizes%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520to%2520generate%250Asketches%2520with%2520explicit%2520control%2520over%2520identity%2520and%2520style.%2520We%2520combine%2520CLIP%2520and%250AAdaface%2520embeddings%2520of%2520a%2520reference%2520mugshot%252C%2520along%2520with%2520textual%2520descriptions%2520of%250Astyle%252C%2520as%2520the%2520conditions%2520to%2520the%2520diffusion%2520model.%2520We%2520demonstrate%2520the%2520efficacy%2520of%250Aour%2520approach%2520by%2520generating%2520a%2520comprehensive%2520dataset%2520of%2520sketches%2520corresponding%2520to%250Amugshots%2520and%2520training%2520a%2520face%2520recognition%2520model%2520on%2520our%2520synthetic%2520data.%2520Our%250Aresults%2520show%2520significant%2520improvements%2520in%2520sketch-to-mugshot%2520matching%2520accuracy%250Aover%2520training%2520on%2520an%2520existing%252C%2520limited%2520amount%2520of%2520real%2520face%2520sketch%2520data%252C%250Avalidating%2520the%2520potential%2520of%2520diffusion%2520models%2520in%2520enhancing%2520the%2520performance%2520of%250Aface%2520recognition%2520systems%2520across%2520modalities.%2520We%2520also%2520compare%2520our%2520dataset%2520with%250Adatasets%2520generated%2520using%2520GAN-based%2520methods%2520to%2520show%2520its%2520superiority.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01233v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP4Sketch%3A%20Enhancing%20Sketch%20to%20Mugshot%20Matching%20through%20Dataset%0A%20%20Augmentation%20using%20Diffusion%20Models&entry.906535625=Kushal%20Kumar%20Jain%20and%20Steve%20Grosz%20and%20Anoop%20M.%20Namboodiri%20and%20Anil%20K.%20Jain&entry.1292438233=%20%20Forensic%20sketch-to-mugshot%20matching%20is%20a%20challenging%20task%20in%20face%0Arecognition%2C%20primarily%20hindered%20by%20the%20scarcity%20of%20annotated%20forensic%20sketches%0Aand%20the%20modality%20gap%20between%20sketches%20and%20photographs.%20To%20address%20this%2C%20we%0Apropose%20CLIP4Sketch%2C%20a%20novel%20approach%20that%20leverages%20diffusion%20models%20to%0Agenerate%20a%20large%20and%20diverse%20set%20of%20sketch%20images%2C%20which%20helps%20in%20enhancing%20the%0Aperformance%20of%20face%20recognition%20systems%20in%20sketch-to-mugshot%20matching.%20Our%0Amethod%20utilizes%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20to%20generate%0Asketches%20with%20explicit%20control%20over%20identity%20and%20style.%20We%20combine%20CLIP%20and%0AAdaface%20embeddings%20of%20a%20reference%20mugshot%2C%20along%20with%20textual%20descriptions%20of%0Astyle%2C%20as%20the%20conditions%20to%20the%20diffusion%20model.%20We%20demonstrate%20the%20efficacy%20of%0Aour%20approach%20by%20generating%20a%20comprehensive%20dataset%20of%20sketches%20corresponding%20to%0Amugshots%20and%20training%20a%20face%20recognition%20model%20on%20our%20synthetic%20data.%20Our%0Aresults%20show%20significant%20improvements%20in%20sketch-to-mugshot%20matching%20accuracy%0Aover%20training%20on%20an%20existing%2C%20limited%20amount%20of%20real%20face%20sketch%20data%2C%0Avalidating%20the%20potential%20of%20diffusion%20models%20in%20enhancing%20the%20performance%20of%0Aface%20recognition%20systems%20across%20modalities.%20We%20also%20compare%20our%20dataset%20with%0Adatasets%20generated%20using%20GAN-based%20methods%20to%20show%20its%20superiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01233v2&entry.124074799=Read"},
{"title": "Who's asking? User personas and the mechanics of latent misalignment", "author": "Asma Ghandeharioun and Ann Yuan and Marius Guerard and Emily Reif and Michael A. Lepori and Lucas Dixon", "abstract": "  Despite investments in improving model safety, studies show that misaligned\ncapabilities remain latent in safety-tuned models. In this work, we shed light\non the mechanics of this phenomenon. First, we show that even when model\ngenerations are safe, harmful content can persist in hidden representations and\ncan be extracted by decoding from earlier layers. Then, we show that whether\nthe model divulges such content depends significantly on its perception of who\nit is talking to, which we refer to as user persona. In fact, we find\nmanipulating user persona to be even more effective for eliciting harmful\ncontent than direct attempts to control model refusal. We study both natural\nlanguage prompting and activation steering as control methods and show that\nactivation steering is significantly more effective at bypassing safety\nfilters. We investigate why certain personas break model safeguards and find\nthat they enable the model to form more charitable interpretations of otherwise\ndangerous queries. Finally, we show we can predict a persona's effect on\nrefusal given only the geometry of its steering vector.\n", "link": "http://arxiv.org/abs/2406.12094v2", "date": "2024-08-13", "relevancy": 1.4639, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%27s%20asking%3F%20User%20personas%20and%20the%20mechanics%20of%20latent%20misalignment&body=Title%3A%20Who%27s%20asking%3F%20User%20personas%20and%20the%20mechanics%20of%20latent%20misalignment%0AAuthor%3A%20Asma%20Ghandeharioun%20and%20Ann%20Yuan%20and%20Marius%20Guerard%20and%20Emily%20Reif%20and%20Michael%20A.%20Lepori%20and%20Lucas%20Dixon%0AAbstract%3A%20%20%20Despite%20investments%20in%20improving%20model%20safety%2C%20studies%20show%20that%20misaligned%0Acapabilities%20remain%20latent%20in%20safety-tuned%20models.%20In%20this%20work%2C%20we%20shed%20light%0Aon%20the%20mechanics%20of%20this%20phenomenon.%20First%2C%20we%20show%20that%20even%20when%20model%0Agenerations%20are%20safe%2C%20harmful%20content%20can%20persist%20in%20hidden%20representations%20and%0Acan%20be%20extracted%20by%20decoding%20from%20earlier%20layers.%20Then%2C%20we%20show%20that%20whether%0Athe%20model%20divulges%20such%20content%20depends%20significantly%20on%20its%20perception%20of%20who%0Ait%20is%20talking%20to%2C%20which%20we%20refer%20to%20as%20user%20persona.%20In%20fact%2C%20we%20find%0Amanipulating%20user%20persona%20to%20be%20even%20more%20effective%20for%20eliciting%20harmful%0Acontent%20than%20direct%20attempts%20to%20control%20model%20refusal.%20We%20study%20both%20natural%0Alanguage%20prompting%20and%20activation%20steering%20as%20control%20methods%20and%20show%20that%0Aactivation%20steering%20is%20significantly%20more%20effective%20at%20bypassing%20safety%0Afilters.%20We%20investigate%20why%20certain%20personas%20break%20model%20safeguards%20and%20find%0Athat%20they%20enable%20the%20model%20to%20form%20more%20charitable%20interpretations%20of%20otherwise%0Adangerous%20queries.%20Finally%2C%20we%20show%20we%20can%20predict%20a%20persona%27s%20effect%20on%0Arefusal%20given%20only%20the%20geometry%20of%20its%20steering%20vector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2527s%2520asking%253F%2520User%2520personas%2520and%2520the%2520mechanics%2520of%2520latent%2520misalignment%26entry.906535625%3DAsma%2520Ghandeharioun%2520and%2520Ann%2520Yuan%2520and%2520Marius%2520Guerard%2520and%2520Emily%2520Reif%2520and%2520Michael%2520A.%2520Lepori%2520and%2520Lucas%2520Dixon%26entry.1292438233%3D%2520%2520Despite%2520investments%2520in%2520improving%2520model%2520safety%252C%2520studies%2520show%2520that%2520misaligned%250Acapabilities%2520remain%2520latent%2520in%2520safety-tuned%2520models.%2520In%2520this%2520work%252C%2520we%2520shed%2520light%250Aon%2520the%2520mechanics%2520of%2520this%2520phenomenon.%2520First%252C%2520we%2520show%2520that%2520even%2520when%2520model%250Agenerations%2520are%2520safe%252C%2520harmful%2520content%2520can%2520persist%2520in%2520hidden%2520representations%2520and%250Acan%2520be%2520extracted%2520by%2520decoding%2520from%2520earlier%2520layers.%2520Then%252C%2520we%2520show%2520that%2520whether%250Athe%2520model%2520divulges%2520such%2520content%2520depends%2520significantly%2520on%2520its%2520perception%2520of%2520who%250Ait%2520is%2520talking%2520to%252C%2520which%2520we%2520refer%2520to%2520as%2520user%2520persona.%2520In%2520fact%252C%2520we%2520find%250Amanipulating%2520user%2520persona%2520to%2520be%2520even%2520more%2520effective%2520for%2520eliciting%2520harmful%250Acontent%2520than%2520direct%2520attempts%2520to%2520control%2520model%2520refusal.%2520We%2520study%2520both%2520natural%250Alanguage%2520prompting%2520and%2520activation%2520steering%2520as%2520control%2520methods%2520and%2520show%2520that%250Aactivation%2520steering%2520is%2520significantly%2520more%2520effective%2520at%2520bypassing%2520safety%250Afilters.%2520We%2520investigate%2520why%2520certain%2520personas%2520break%2520model%2520safeguards%2520and%2520find%250Athat%2520they%2520enable%2520the%2520model%2520to%2520form%2520more%2520charitable%2520interpretations%2520of%2520otherwise%250Adangerous%2520queries.%2520Finally%252C%2520we%2520show%2520we%2520can%2520predict%2520a%2520persona%2527s%2520effect%2520on%250Arefusal%2520given%2520only%2520the%2520geometry%2520of%2520its%2520steering%2520vector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%27s%20asking%3F%20User%20personas%20and%20the%20mechanics%20of%20latent%20misalignment&entry.906535625=Asma%20Ghandeharioun%20and%20Ann%20Yuan%20and%20Marius%20Guerard%20and%20Emily%20Reif%20and%20Michael%20A.%20Lepori%20and%20Lucas%20Dixon&entry.1292438233=%20%20Despite%20investments%20in%20improving%20model%20safety%2C%20studies%20show%20that%20misaligned%0Acapabilities%20remain%20latent%20in%20safety-tuned%20models.%20In%20this%20work%2C%20we%20shed%20light%0Aon%20the%20mechanics%20of%20this%20phenomenon.%20First%2C%20we%20show%20that%20even%20when%20model%0Agenerations%20are%20safe%2C%20harmful%20content%20can%20persist%20in%20hidden%20representations%20and%0Acan%20be%20extracted%20by%20decoding%20from%20earlier%20layers.%20Then%2C%20we%20show%20that%20whether%0Athe%20model%20divulges%20such%20content%20depends%20significantly%20on%20its%20perception%20of%20who%0Ait%20is%20talking%20to%2C%20which%20we%20refer%20to%20as%20user%20persona.%20In%20fact%2C%20we%20find%0Amanipulating%20user%20persona%20to%20be%20even%20more%20effective%20for%20eliciting%20harmful%0Acontent%20than%20direct%20attempts%20to%20control%20model%20refusal.%20We%20study%20both%20natural%0Alanguage%20prompting%20and%20activation%20steering%20as%20control%20methods%20and%20show%20that%0Aactivation%20steering%20is%20significantly%20more%20effective%20at%20bypassing%20safety%0Afilters.%20We%20investigate%20why%20certain%20personas%20break%20model%20safeguards%20and%20find%0Athat%20they%20enable%20the%20model%20to%20form%20more%20charitable%20interpretations%20of%20otherwise%0Adangerous%20queries.%20Finally%2C%20we%20show%20we%20can%20predict%20a%20persona%27s%20effect%20on%0Arefusal%20given%20only%20the%20geometry%20of%20its%20steering%20vector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12094v2&entry.124074799=Read"},
{"title": "Exploring Domain Shift on Radar-Based 3D Object Detection Amidst Diverse\n  Environmental Conditions", "author": "Miao Zhang and Sherif Abdulatif and Benedikt Loesch and Marco Altmann and Marius Schwarz and Bin Yang", "abstract": "  The rapid evolution of deep learning and its integration with autonomous\ndriving systems have led to substantial advancements in 3D perception using\nmultimodal sensors. Notably, radar sensors show greater robustness compared to\ncameras and lidar under adverse weather and varying illumination conditions.\nThis study delves into the often-overlooked yet crucial issue of domain shift\nin 4D radar-based object detection, examining how varying environmental\nconditions, such as different weather patterns and road types, impact 3D object\ndetection performance. Our findings highlight distinct domain shifts across\nvarious weather scenarios, revealing unique dataset sensitivities that\nunderscore the critical role of radar point cloud generation. Additionally, we\ndemonstrate that transitioning between different road types, especially from\nhighways to urban settings, introduces notable domain shifts, emphasizing the\nnecessity for diverse data collection across varied road environments. To the\nbest of our knowledge, this is the first comprehensive analysis of domain shift\neffects on 4D radar-based object detection. We believe this empirical study\ncontributes to understanding the complex nature of domain shifts in radar data\nand suggests paths forward for data collection strategy in the face of\nenvironmental variability.\n", "link": "http://arxiv.org/abs/2408.06772v1", "date": "2024-08-13", "relevancy": 1.5928, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Domain%20Shift%20on%20Radar-Based%203D%20Object%20Detection%20Amidst%20Diverse%0A%20%20Environmental%20Conditions&body=Title%3A%20Exploring%20Domain%20Shift%20on%20Radar-Based%203D%20Object%20Detection%20Amidst%20Diverse%0A%20%20Environmental%20Conditions%0AAuthor%3A%20Miao%20Zhang%20and%20Sherif%20Abdulatif%20and%20Benedikt%20Loesch%20and%20Marco%20Altmann%20and%20Marius%20Schwarz%20and%20Bin%20Yang%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20deep%20learning%20and%20its%20integration%20with%20autonomous%0Adriving%20systems%20have%20led%20to%20substantial%20advancements%20in%203D%20perception%20using%0Amultimodal%20sensors.%20Notably%2C%20radar%20sensors%20show%20greater%20robustness%20compared%20to%0Acameras%20and%20lidar%20under%20adverse%20weather%20and%20varying%20illumination%20conditions.%0AThis%20study%20delves%20into%20the%20often-overlooked%20yet%20crucial%20issue%20of%20domain%20shift%0Ain%204D%20radar-based%20object%20detection%2C%20examining%20how%20varying%20environmental%0Aconditions%2C%20such%20as%20different%20weather%20patterns%20and%20road%20types%2C%20impact%203D%20object%0Adetection%20performance.%20Our%20findings%20highlight%20distinct%20domain%20shifts%20across%0Avarious%20weather%20scenarios%2C%20revealing%20unique%20dataset%20sensitivities%20that%0Aunderscore%20the%20critical%20role%20of%20radar%20point%20cloud%20generation.%20Additionally%2C%20we%0Ademonstrate%20that%20transitioning%20between%20different%20road%20types%2C%20especially%20from%0Ahighways%20to%20urban%20settings%2C%20introduces%20notable%20domain%20shifts%2C%20emphasizing%20the%0Anecessity%20for%20diverse%20data%20collection%20across%20varied%20road%20environments.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20comprehensive%20analysis%20of%20domain%20shift%0Aeffects%20on%204D%20radar-based%20object%20detection.%20We%20believe%20this%20empirical%20study%0Acontributes%20to%20understanding%20the%20complex%20nature%20of%20domain%20shifts%20in%20radar%20data%0Aand%20suggests%20paths%20forward%20for%20data%20collection%20strategy%20in%20the%20face%20of%0Aenvironmental%20variability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Domain%2520Shift%2520on%2520Radar-Based%25203D%2520Object%2520Detection%2520Amidst%2520Diverse%250A%2520%2520Environmental%2520Conditions%26entry.906535625%3DMiao%2520Zhang%2520and%2520Sherif%2520Abdulatif%2520and%2520Benedikt%2520Loesch%2520and%2520Marco%2520Altmann%2520and%2520Marius%2520Schwarz%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520deep%2520learning%2520and%2520its%2520integration%2520with%2520autonomous%250Adriving%2520systems%2520have%2520led%2520to%2520substantial%2520advancements%2520in%25203D%2520perception%2520using%250Amultimodal%2520sensors.%2520Notably%252C%2520radar%2520sensors%2520show%2520greater%2520robustness%2520compared%2520to%250Acameras%2520and%2520lidar%2520under%2520adverse%2520weather%2520and%2520varying%2520illumination%2520conditions.%250AThis%2520study%2520delves%2520into%2520the%2520often-overlooked%2520yet%2520crucial%2520issue%2520of%2520domain%2520shift%250Ain%25204D%2520radar-based%2520object%2520detection%252C%2520examining%2520how%2520varying%2520environmental%250Aconditions%252C%2520such%2520as%2520different%2520weather%2520patterns%2520and%2520road%2520types%252C%2520impact%25203D%2520object%250Adetection%2520performance.%2520Our%2520findings%2520highlight%2520distinct%2520domain%2520shifts%2520across%250Avarious%2520weather%2520scenarios%252C%2520revealing%2520unique%2520dataset%2520sensitivities%2520that%250Aunderscore%2520the%2520critical%2520role%2520of%2520radar%2520point%2520cloud%2520generation.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520transitioning%2520between%2520different%2520road%2520types%252C%2520especially%2520from%250Ahighways%2520to%2520urban%2520settings%252C%2520introduces%2520notable%2520domain%2520shifts%252C%2520emphasizing%2520the%250Anecessity%2520for%2520diverse%2520data%2520collection%2520across%2520varied%2520road%2520environments.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520comprehensive%2520analysis%2520of%2520domain%2520shift%250Aeffects%2520on%25204D%2520radar-based%2520object%2520detection.%2520We%2520believe%2520this%2520empirical%2520study%250Acontributes%2520to%2520understanding%2520the%2520complex%2520nature%2520of%2520domain%2520shifts%2520in%2520radar%2520data%250Aand%2520suggests%2520paths%2520forward%2520for%2520data%2520collection%2520strategy%2520in%2520the%2520face%2520of%250Aenvironmental%2520variability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Domain%20Shift%20on%20Radar-Based%203D%20Object%20Detection%20Amidst%20Diverse%0A%20%20Environmental%20Conditions&entry.906535625=Miao%20Zhang%20and%20Sherif%20Abdulatif%20and%20Benedikt%20Loesch%20and%20Marco%20Altmann%20and%20Marius%20Schwarz%20and%20Bin%20Yang&entry.1292438233=%20%20The%20rapid%20evolution%20of%20deep%20learning%20and%20its%20integration%20with%20autonomous%0Adriving%20systems%20have%20led%20to%20substantial%20advancements%20in%203D%20perception%20using%0Amultimodal%20sensors.%20Notably%2C%20radar%20sensors%20show%20greater%20robustness%20compared%20to%0Acameras%20and%20lidar%20under%20adverse%20weather%20and%20varying%20illumination%20conditions.%0AThis%20study%20delves%20into%20the%20often-overlooked%20yet%20crucial%20issue%20of%20domain%20shift%0Ain%204D%20radar-based%20object%20detection%2C%20examining%20how%20varying%20environmental%0Aconditions%2C%20such%20as%20different%20weather%20patterns%20and%20road%20types%2C%20impact%203D%20object%0Adetection%20performance.%20Our%20findings%20highlight%20distinct%20domain%20shifts%20across%0Avarious%20weather%20scenarios%2C%20revealing%20unique%20dataset%20sensitivities%20that%0Aunderscore%20the%20critical%20role%20of%20radar%20point%20cloud%20generation.%20Additionally%2C%20we%0Ademonstrate%20that%20transitioning%20between%20different%20road%20types%2C%20especially%20from%0Ahighways%20to%20urban%20settings%2C%20introduces%20notable%20domain%20shifts%2C%20emphasizing%20the%0Anecessity%20for%20diverse%20data%20collection%20across%20varied%20road%20environments.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20comprehensive%20analysis%20of%20domain%20shift%0Aeffects%20on%204D%20radar-based%20object%20detection.%20We%20believe%20this%20empirical%20study%0Acontributes%20to%20understanding%20the%20complex%20nature%20of%20domain%20shifts%20in%20radar%20data%0Aand%20suggests%20paths%20forward%20for%20data%20collection%20strategy%20in%20the%20face%20of%0Aenvironmental%20variability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06772v1&entry.124074799=Read"},
{"title": "WRDScore: New Metric for Evaluation of Natural Language Generation\n  Models", "author": "Ravil Mussabayev", "abstract": "  Evaluating natural language generation models, particularly for method name\nprediction, poses significant challenges. A robust metric must account for the\nversatility of method naming, considering both semantic and syntactic\nvariations. Traditional overlap-based metrics, such as ROUGE, fail to capture\nthese nuances. Existing embedding-based metrics often suffer from imbalanced\nprecision and recall, lack normalized scores, or make unrealistic assumptions\nabout sequences. To address these limitations, we leverage the theory of\noptimal transport and construct WRDScore, a novel metric that strikes a balance\nbetween simplicity and effectiveness. In the WRDScore framework, we define\nprecision as the maximum degree to which the predicted sequence's tokens are\nincluded in the reference sequence, token by token. Recall is calculated as the\ntotal cost of the optimal transport plan that maps the reference sequence to\nthe predicted one. Finally, WRDScore is computed as the harmonic mean of\nprecision and recall, balancing these two complementary metrics. Our metric is\nlightweight, normalized, and precision-recall-oriented, avoiding unrealistic\nassumptions while aligning well with human judgments. Experiments on a\nhuman-curated dataset confirm the superiority of WRDScore over other available\ntext metrics.\n", "link": "http://arxiv.org/abs/2405.19220v5", "date": "2024-08-13", "relevancy": 1.3766, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5054}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4529}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WRDScore%3A%20New%20Metric%20for%20Evaluation%20of%20Natural%20Language%20Generation%0A%20%20Models&body=Title%3A%20WRDScore%3A%20New%20Metric%20for%20Evaluation%20of%20Natural%20Language%20Generation%0A%20%20Models%0AAuthor%3A%20Ravil%20Mussabayev%0AAbstract%3A%20%20%20Evaluating%20natural%20language%20generation%20models%2C%20particularly%20for%20method%20name%0Aprediction%2C%20poses%20significant%20challenges.%20A%20robust%20metric%20must%20account%20for%20the%0Aversatility%20of%20method%20naming%2C%20considering%20both%20semantic%20and%20syntactic%0Avariations.%20Traditional%20overlap-based%20metrics%2C%20such%20as%20ROUGE%2C%20fail%20to%20capture%0Athese%20nuances.%20Existing%20embedding-based%20metrics%20often%20suffer%20from%20imbalanced%0Aprecision%20and%20recall%2C%20lack%20normalized%20scores%2C%20or%20make%20unrealistic%20assumptions%0Aabout%20sequences.%20To%20address%20these%20limitations%2C%20we%20leverage%20the%20theory%20of%0Aoptimal%20transport%20and%20construct%20WRDScore%2C%20a%20novel%20metric%20that%20strikes%20a%20balance%0Abetween%20simplicity%20and%20effectiveness.%20In%20the%20WRDScore%20framework%2C%20we%20define%0Aprecision%20as%20the%20maximum%20degree%20to%20which%20the%20predicted%20sequence%27s%20tokens%20are%0Aincluded%20in%20the%20reference%20sequence%2C%20token%20by%20token.%20Recall%20is%20calculated%20as%20the%0Atotal%20cost%20of%20the%20optimal%20transport%20plan%20that%20maps%20the%20reference%20sequence%20to%0Athe%20predicted%20one.%20Finally%2C%20WRDScore%20is%20computed%20as%20the%20harmonic%20mean%20of%0Aprecision%20and%20recall%2C%20balancing%20these%20two%20complementary%20metrics.%20Our%20metric%20is%0Alightweight%2C%20normalized%2C%20and%20precision-recall-oriented%2C%20avoiding%20unrealistic%0Aassumptions%20while%20aligning%20well%20with%20human%20judgments.%20Experiments%20on%20a%0Ahuman-curated%20dataset%20confirm%20the%20superiority%20of%20WRDScore%20over%20other%20available%0Atext%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19220v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWRDScore%253A%2520New%2520Metric%2520for%2520Evaluation%2520of%2520Natural%2520Language%2520Generation%250A%2520%2520Models%26entry.906535625%3DRavil%2520Mussabayev%26entry.1292438233%3D%2520%2520Evaluating%2520natural%2520language%2520generation%2520models%252C%2520particularly%2520for%2520method%2520name%250Aprediction%252C%2520poses%2520significant%2520challenges.%2520A%2520robust%2520metric%2520must%2520account%2520for%2520the%250Aversatility%2520of%2520method%2520naming%252C%2520considering%2520both%2520semantic%2520and%2520syntactic%250Avariations.%2520Traditional%2520overlap-based%2520metrics%252C%2520such%2520as%2520ROUGE%252C%2520fail%2520to%2520capture%250Athese%2520nuances.%2520Existing%2520embedding-based%2520metrics%2520often%2520suffer%2520from%2520imbalanced%250Aprecision%2520and%2520recall%252C%2520lack%2520normalized%2520scores%252C%2520or%2520make%2520unrealistic%2520assumptions%250Aabout%2520sequences.%2520To%2520address%2520these%2520limitations%252C%2520we%2520leverage%2520the%2520theory%2520of%250Aoptimal%2520transport%2520and%2520construct%2520WRDScore%252C%2520a%2520novel%2520metric%2520that%2520strikes%2520a%2520balance%250Abetween%2520simplicity%2520and%2520effectiveness.%2520In%2520the%2520WRDScore%2520framework%252C%2520we%2520define%250Aprecision%2520as%2520the%2520maximum%2520degree%2520to%2520which%2520the%2520predicted%2520sequence%2527s%2520tokens%2520are%250Aincluded%2520in%2520the%2520reference%2520sequence%252C%2520token%2520by%2520token.%2520Recall%2520is%2520calculated%2520as%2520the%250Atotal%2520cost%2520of%2520the%2520optimal%2520transport%2520plan%2520that%2520maps%2520the%2520reference%2520sequence%2520to%250Athe%2520predicted%2520one.%2520Finally%252C%2520WRDScore%2520is%2520computed%2520as%2520the%2520harmonic%2520mean%2520of%250Aprecision%2520and%2520recall%252C%2520balancing%2520these%2520two%2520complementary%2520metrics.%2520Our%2520metric%2520is%250Alightweight%252C%2520normalized%252C%2520and%2520precision-recall-oriented%252C%2520avoiding%2520unrealistic%250Aassumptions%2520while%2520aligning%2520well%2520with%2520human%2520judgments.%2520Experiments%2520on%2520a%250Ahuman-curated%2520dataset%2520confirm%2520the%2520superiority%2520of%2520WRDScore%2520over%2520other%2520available%250Atext%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19220v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WRDScore%3A%20New%20Metric%20for%20Evaluation%20of%20Natural%20Language%20Generation%0A%20%20Models&entry.906535625=Ravil%20Mussabayev&entry.1292438233=%20%20Evaluating%20natural%20language%20generation%20models%2C%20particularly%20for%20method%20name%0Aprediction%2C%20poses%20significant%20challenges.%20A%20robust%20metric%20must%20account%20for%20the%0Aversatility%20of%20method%20naming%2C%20considering%20both%20semantic%20and%20syntactic%0Avariations.%20Traditional%20overlap-based%20metrics%2C%20such%20as%20ROUGE%2C%20fail%20to%20capture%0Athese%20nuances.%20Existing%20embedding-based%20metrics%20often%20suffer%20from%20imbalanced%0Aprecision%20and%20recall%2C%20lack%20normalized%20scores%2C%20or%20make%20unrealistic%20assumptions%0Aabout%20sequences.%20To%20address%20these%20limitations%2C%20we%20leverage%20the%20theory%20of%0Aoptimal%20transport%20and%20construct%20WRDScore%2C%20a%20novel%20metric%20that%20strikes%20a%20balance%0Abetween%20simplicity%20and%20effectiveness.%20In%20the%20WRDScore%20framework%2C%20we%20define%0Aprecision%20as%20the%20maximum%20degree%20to%20which%20the%20predicted%20sequence%27s%20tokens%20are%0Aincluded%20in%20the%20reference%20sequence%2C%20token%20by%20token.%20Recall%20is%20calculated%20as%20the%0Atotal%20cost%20of%20the%20optimal%20transport%20plan%20that%20maps%20the%20reference%20sequence%20to%0Athe%20predicted%20one.%20Finally%2C%20WRDScore%20is%20computed%20as%20the%20harmonic%20mean%20of%0Aprecision%20and%20recall%2C%20balancing%20these%20two%20complementary%20metrics.%20Our%20metric%20is%0Alightweight%2C%20normalized%2C%20and%20precision-recall-oriented%2C%20avoiding%20unrealistic%0Aassumptions%20while%20aligning%20well%20with%20human%20judgments.%20Experiments%20on%20a%0Ahuman-curated%20dataset%20confirm%20the%20superiority%20of%20WRDScore%20over%20other%20available%0Atext%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19220v5&entry.124074799=Read"},
{"title": "Neural networks can detect model-free static arbitrage strategies", "author": "Ariel Neufeld and Julian Sester", "abstract": "  In this paper we demonstrate both theoretically as well as numerically that\nneural networks can detect model-free static arbitrage opportunities whenever\nthe market admits some. Due to the use of neural networks, our method can be\napplied to financial markets with a high number of traded securities and\nensures almost immediate execution of the corresponding trading strategies. To\ndemonstrate its tractability, effectiveness, and robustness we provide examples\nusing real financial data. From a technical point of view, we prove that a\nsingle neural network can approximately solve a class of convex semi-infinite\nprograms, which is the key result in order to derive our theoretical results\nthat neural networks can detect model-free static arbitrage strategies whenever\nthe financial market admits such opportunities.\n", "link": "http://arxiv.org/abs/2306.16422v2", "date": "2024-08-13", "relevancy": 1.7248, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4762}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4384}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20networks%20can%20detect%20model-free%20static%20arbitrage%20strategies&body=Title%3A%20Neural%20networks%20can%20detect%20model-free%20static%20arbitrage%20strategies%0AAuthor%3A%20Ariel%20Neufeld%20and%20Julian%20Sester%0AAbstract%3A%20%20%20In%20this%20paper%20we%20demonstrate%20both%20theoretically%20as%20well%20as%20numerically%20that%0Aneural%20networks%20can%20detect%20model-free%20static%20arbitrage%20opportunities%20whenever%0Athe%20market%20admits%20some.%20Due%20to%20the%20use%20of%20neural%20networks%2C%20our%20method%20can%20be%0Aapplied%20to%20financial%20markets%20with%20a%20high%20number%20of%20traded%20securities%20and%0Aensures%20almost%20immediate%20execution%20of%20the%20corresponding%20trading%20strategies.%20To%0Ademonstrate%20its%20tractability%2C%20effectiveness%2C%20and%20robustness%20we%20provide%20examples%0Ausing%20real%20financial%20data.%20From%20a%20technical%20point%20of%20view%2C%20we%20prove%20that%20a%0Asingle%20neural%20network%20can%20approximately%20solve%20a%20class%20of%20convex%20semi-infinite%0Aprograms%2C%20which%20is%20the%20key%20result%20in%20order%20to%20derive%20our%20theoretical%20results%0Athat%20neural%20networks%20can%20detect%20model-free%20static%20arbitrage%20strategies%20whenever%0Athe%20financial%20market%20admits%20such%20opportunities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520networks%2520can%2520detect%2520model-free%2520static%2520arbitrage%2520strategies%26entry.906535625%3DAriel%2520Neufeld%2520and%2520Julian%2520Sester%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520demonstrate%2520both%2520theoretically%2520as%2520well%2520as%2520numerically%2520that%250Aneural%2520networks%2520can%2520detect%2520model-free%2520static%2520arbitrage%2520opportunities%2520whenever%250Athe%2520market%2520admits%2520some.%2520Due%2520to%2520the%2520use%2520of%2520neural%2520networks%252C%2520our%2520method%2520can%2520be%250Aapplied%2520to%2520financial%2520markets%2520with%2520a%2520high%2520number%2520of%2520traded%2520securities%2520and%250Aensures%2520almost%2520immediate%2520execution%2520of%2520the%2520corresponding%2520trading%2520strategies.%2520To%250Ademonstrate%2520its%2520tractability%252C%2520effectiveness%252C%2520and%2520robustness%2520we%2520provide%2520examples%250Ausing%2520real%2520financial%2520data.%2520From%2520a%2520technical%2520point%2520of%2520view%252C%2520we%2520prove%2520that%2520a%250Asingle%2520neural%2520network%2520can%2520approximately%2520solve%2520a%2520class%2520of%2520convex%2520semi-infinite%250Aprograms%252C%2520which%2520is%2520the%2520key%2520result%2520in%2520order%2520to%2520derive%2520our%2520theoretical%2520results%250Athat%2520neural%2520networks%2520can%2520detect%2520model-free%2520static%2520arbitrage%2520strategies%2520whenever%250Athe%2520financial%2520market%2520admits%2520such%2520opportunities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.16422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20networks%20can%20detect%20model-free%20static%20arbitrage%20strategies&entry.906535625=Ariel%20Neufeld%20and%20Julian%20Sester&entry.1292438233=%20%20In%20this%20paper%20we%20demonstrate%20both%20theoretically%20as%20well%20as%20numerically%20that%0Aneural%20networks%20can%20detect%20model-free%20static%20arbitrage%20opportunities%20whenever%0Athe%20market%20admits%20some.%20Due%20to%20the%20use%20of%20neural%20networks%2C%20our%20method%20can%20be%0Aapplied%20to%20financial%20markets%20with%20a%20high%20number%20of%20traded%20securities%20and%0Aensures%20almost%20immediate%20execution%20of%20the%20corresponding%20trading%20strategies.%20To%0Ademonstrate%20its%20tractability%2C%20effectiveness%2C%20and%20robustness%20we%20provide%20examples%0Ausing%20real%20financial%20data.%20From%20a%20technical%20point%20of%20view%2C%20we%20prove%20that%20a%0Asingle%20neural%20network%20can%20approximately%20solve%20a%20class%20of%20convex%20semi-infinite%0Aprograms%2C%20which%20is%20the%20key%20result%20in%20order%20to%20derive%20our%20theoretical%20results%0Athat%20neural%20networks%20can%20detect%20model-free%20static%20arbitrage%20strategies%20whenever%0Athe%20financial%20market%20admits%20such%20opportunities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16422v2&entry.124074799=Read"},
{"title": "Hierarchical Quantum Control Gates for Functional MRI Understanding", "author": "Xuan-Bac Nguyen and Hoang-Quan Nguyen and Hugh Churchill and Samee U. Khan and Khoa Luu", "abstract": "  Quantum computing has emerged as a powerful tool for solving complex problems\nintractable for classical computers, particularly in popular fields such as\ncryptography, optimization, and neurocomputing. In this paper, we present a new\nquantum-based approach named the Hierarchical Quantum Control Gates (HQCG)\nmethod for efficient understanding of Functional Magnetic Resonance Imaging\n(fMRI) data. This approach includes two novel modules: the Local Quantum\nControl Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are\ndesigned to extract local and global features of fMRI signals, respectively.\nOur method operates end-to-end on a quantum machine, leveraging quantum\nmechanics to learn patterns within extremely high-dimensional fMRI signals,\nsuch as 30,000 samples which is a challenge for classical computers. Empirical\nresults demonstrate that our approach significantly outperforms classical\nmethods. Additionally, we found that the proposed quantum model is more stable\nand less prone to overfitting than the classical methods.\n", "link": "http://arxiv.org/abs/2408.03596v2", "date": "2024-08-13", "relevancy": 1.3138, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.438}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Quantum%20Control%20Gates%20for%20Functional%20MRI%20Understanding&body=Title%3A%20Hierarchical%20Quantum%20Control%20Gates%20for%20Functional%20MRI%20Understanding%0AAuthor%3A%20Xuan-Bac%20Nguyen%20and%20Hoang-Quan%20Nguyen%20and%20Hugh%20Churchill%20and%20Samee%20U.%20Khan%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Quantum%20computing%20has%20emerged%20as%20a%20powerful%20tool%20for%20solving%20complex%20problems%0Aintractable%20for%20classical%20computers%2C%20particularly%20in%20popular%20fields%20such%20as%0Acryptography%2C%20optimization%2C%20and%20neurocomputing.%20In%20this%20paper%2C%20we%20present%20a%20new%0Aquantum-based%20approach%20named%20the%20Hierarchical%20Quantum%20Control%20Gates%20%28HQCG%29%0Amethod%20for%20efficient%20understanding%20of%20Functional%20Magnetic%20Resonance%20Imaging%0A%28fMRI%29%20data.%20This%20approach%20includes%20two%20novel%20modules%3A%20the%20Local%20Quantum%0AControl%20Gate%20%28LQCG%29%20and%20the%20Global%20Quantum%20Control%20Gate%20%28GQCG%29%2C%20which%20are%0Adesigned%20to%20extract%20local%20and%20global%20features%20of%20fMRI%20signals%2C%20respectively.%0AOur%20method%20operates%20end-to-end%20on%20a%20quantum%20machine%2C%20leveraging%20quantum%0Amechanics%20to%20learn%20patterns%20within%20extremely%20high-dimensional%20fMRI%20signals%2C%0Asuch%20as%2030%2C000%20samples%20which%20is%20a%20challenge%20for%20classical%20computers.%20Empirical%0Aresults%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20classical%0Amethods.%20Additionally%2C%20we%20found%20that%20the%20proposed%20quantum%20model%20is%20more%20stable%0Aand%20less%20prone%20to%20overfitting%20than%20the%20classical%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Quantum%2520Control%2520Gates%2520for%2520Functional%2520MRI%2520Understanding%26entry.906535625%3DXuan-Bac%2520Nguyen%2520and%2520Hoang-Quan%2520Nguyen%2520and%2520Hugh%2520Churchill%2520and%2520Samee%2520U.%2520Khan%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Quantum%2520computing%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520solving%2520complex%2520problems%250Aintractable%2520for%2520classical%2520computers%252C%2520particularly%2520in%2520popular%2520fields%2520such%2520as%250Acryptography%252C%2520optimization%252C%2520and%2520neurocomputing.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%250Aquantum-based%2520approach%2520named%2520the%2520Hierarchical%2520Quantum%2520Control%2520Gates%2520%2528HQCG%2529%250Amethod%2520for%2520efficient%2520understanding%2520of%2520Functional%2520Magnetic%2520Resonance%2520Imaging%250A%2528fMRI%2529%2520data.%2520This%2520approach%2520includes%2520two%2520novel%2520modules%253A%2520the%2520Local%2520Quantum%250AControl%2520Gate%2520%2528LQCG%2529%2520and%2520the%2520Global%2520Quantum%2520Control%2520Gate%2520%2528GQCG%2529%252C%2520which%2520are%250Adesigned%2520to%2520extract%2520local%2520and%2520global%2520features%2520of%2520fMRI%2520signals%252C%2520respectively.%250AOur%2520method%2520operates%2520end-to-end%2520on%2520a%2520quantum%2520machine%252C%2520leveraging%2520quantum%250Amechanics%2520to%2520learn%2520patterns%2520within%2520extremely%2520high-dimensional%2520fMRI%2520signals%252C%250Asuch%2520as%252030%252C000%2520samples%2520which%2520is%2520a%2520challenge%2520for%2520classical%2520computers.%2520Empirical%250Aresults%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520classical%250Amethods.%2520Additionally%252C%2520we%2520found%2520that%2520the%2520proposed%2520quantum%2520model%2520is%2520more%2520stable%250Aand%2520less%2520prone%2520to%2520overfitting%2520than%2520the%2520classical%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Quantum%20Control%20Gates%20for%20Functional%20MRI%20Understanding&entry.906535625=Xuan-Bac%20Nguyen%20and%20Hoang-Quan%20Nguyen%20and%20Hugh%20Churchill%20and%20Samee%20U.%20Khan%20and%20Khoa%20Luu&entry.1292438233=%20%20Quantum%20computing%20has%20emerged%20as%20a%20powerful%20tool%20for%20solving%20complex%20problems%0Aintractable%20for%20classical%20computers%2C%20particularly%20in%20popular%20fields%20such%20as%0Acryptography%2C%20optimization%2C%20and%20neurocomputing.%20In%20this%20paper%2C%20we%20present%20a%20new%0Aquantum-based%20approach%20named%20the%20Hierarchical%20Quantum%20Control%20Gates%20%28HQCG%29%0Amethod%20for%20efficient%20understanding%20of%20Functional%20Magnetic%20Resonance%20Imaging%0A%28fMRI%29%20data.%20This%20approach%20includes%20two%20novel%20modules%3A%20the%20Local%20Quantum%0AControl%20Gate%20%28LQCG%29%20and%20the%20Global%20Quantum%20Control%20Gate%20%28GQCG%29%2C%20which%20are%0Adesigned%20to%20extract%20local%20and%20global%20features%20of%20fMRI%20signals%2C%20respectively.%0AOur%20method%20operates%20end-to-end%20on%20a%20quantum%20machine%2C%20leveraging%20quantum%0Amechanics%20to%20learn%20patterns%20within%20extremely%20high-dimensional%20fMRI%20signals%2C%0Asuch%20as%2030%2C000%20samples%20which%20is%20a%20challenge%20for%20classical%20computers.%20Empirical%0Aresults%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20classical%0Amethods.%20Additionally%2C%20we%20found%20that%20the%20proposed%20quantum%20model%20is%20more%20stable%0Aand%20less%20prone%20to%20overfitting%20than%20the%20classical%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03596v2&entry.124074799=Read"},
{"title": "Faster Private Minimum Spanning Trees", "author": "Rasmus Pagh and Lukas Retschmeier", "abstract": "  Motivated by applications in clustering and synthetic data generation, we\nconsider the problem of releasing a minimum spanning tree (MST) under\nedge-weight differential privacy constraints where a graph topology $G=(V,E)$\nwith $n$ vertices and $m$ edges is public, the weight matrix $\\vec{W}\\in\n\\mathbb{R}^{n \\times n}$ is private, and we wish to release an approximate MST\nunder $\\rho$-zero-concentrated differential privacy. Weight matrices are\nconsidered neighboring if they differ by at most $\\Delta_\\infty$ in each entry,\ni.e., we consider an $\\ell_\\infty$ neighboring relationship. Existing private\nMST algorithms either add noise to each entry in $\\vec{W}$ and estimate the MST\nby post-processing or add noise to weights in-place during the execution of a\nspecific MST algorithm. Using the post-processing approach with an efficient\nMST algorithm takes $O(n^2)$ time on dense graphs but results in an additive\nerror on the weight of the MST of magnitude $O(n^2\\log n)$. In-place algorithms\ngive asymptotically better utility, but the running time of existing in-place\nalgorithms is $O(n^3)$ for dense graphs. Our main result is a new\ndifferentially private MST algorithm that matches the utility of existing\nin-place methods while running in time $O(m + n^{3/2}\\log n)$ for fixed privacy\nparameter $\\rho$. The technical core of our algorithm is an efficient sublinear\ntime simulation of Report-Noisy-Max that works by discretizing all edge weights\nto a multiple of $\\Delta_\\infty$ and forming groups of edges with identical\nweights. Specifically, we present a data structure that allows us to sample a\nnoisy minimum weight edge among at most $O(n^2)$ cut edges in $O(\\sqrt{n} \\log\nn)$ time. Experimental evaluations support our claims that our algorithm\nsignificantly improves previous algorithms either in utility or running time.\n", "link": "http://arxiv.org/abs/2408.06997v1", "date": "2024-08-13", "relevancy": 1.622, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4101}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4022}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Private%20Minimum%20Spanning%20Trees&body=Title%3A%20Faster%20Private%20Minimum%20Spanning%20Trees%0AAuthor%3A%20Rasmus%20Pagh%20and%20Lukas%20Retschmeier%0AAbstract%3A%20%20%20Motivated%20by%20applications%20in%20clustering%20and%20synthetic%20data%20generation%2C%20we%0Aconsider%20the%20problem%20of%20releasing%20a%20minimum%20spanning%20tree%20%28MST%29%20under%0Aedge-weight%20differential%20privacy%20constraints%20where%20a%20graph%20topology%20%24G%3D%28V%2CE%29%24%0Awith%20%24n%24%20vertices%20and%20%24m%24%20edges%20is%20public%2C%20the%20weight%20matrix%20%24%5Cvec%7BW%7D%5Cin%0A%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D%24%20is%20private%2C%20and%20we%20wish%20to%20release%20an%20approximate%20MST%0Aunder%20%24%5Crho%24-zero-concentrated%20differential%20privacy.%20Weight%20matrices%20are%0Aconsidered%20neighboring%20if%20they%20differ%20by%20at%20most%20%24%5CDelta_%5Cinfty%24%20in%20each%20entry%2C%0Ai.e.%2C%20we%20consider%20an%20%24%5Cell_%5Cinfty%24%20neighboring%20relationship.%20Existing%20private%0AMST%20algorithms%20either%20add%20noise%20to%20each%20entry%20in%20%24%5Cvec%7BW%7D%24%20and%20estimate%20the%20MST%0Aby%20post-processing%20or%20add%20noise%20to%20weights%20in-place%20during%20the%20execution%20of%20a%0Aspecific%20MST%20algorithm.%20Using%20the%20post-processing%20approach%20with%20an%20efficient%0AMST%20algorithm%20takes%20%24O%28n%5E2%29%24%20time%20on%20dense%20graphs%20but%20results%20in%20an%20additive%0Aerror%20on%20the%20weight%20of%20the%20MST%20of%20magnitude%20%24O%28n%5E2%5Clog%20n%29%24.%20In-place%20algorithms%0Agive%20asymptotically%20better%20utility%2C%20but%20the%20running%20time%20of%20existing%20in-place%0Aalgorithms%20is%20%24O%28n%5E3%29%24%20for%20dense%20graphs.%20Our%20main%20result%20is%20a%20new%0Adifferentially%20private%20MST%20algorithm%20that%20matches%20the%20utility%20of%20existing%0Ain-place%20methods%20while%20running%20in%20time%20%24O%28m%20%2B%20n%5E%7B3/2%7D%5Clog%20n%29%24%20for%20fixed%20privacy%0Aparameter%20%24%5Crho%24.%20The%20technical%20core%20of%20our%20algorithm%20is%20an%20efficient%20sublinear%0Atime%20simulation%20of%20Report-Noisy-Max%20that%20works%20by%20discretizing%20all%20edge%20weights%0Ato%20a%20multiple%20of%20%24%5CDelta_%5Cinfty%24%20and%20forming%20groups%20of%20edges%20with%20identical%0Aweights.%20Specifically%2C%20we%20present%20a%20data%20structure%20that%20allows%20us%20to%20sample%20a%0Anoisy%20minimum%20weight%20edge%20among%20at%20most%20%24O%28n%5E2%29%24%20cut%20edges%20in%20%24O%28%5Csqrt%7Bn%7D%20%5Clog%0An%29%24%20time.%20Experimental%20evaluations%20support%20our%20claims%20that%20our%20algorithm%0Asignificantly%20improves%20previous%20algorithms%20either%20in%20utility%20or%20running%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Private%2520Minimum%2520Spanning%2520Trees%26entry.906535625%3DRasmus%2520Pagh%2520and%2520Lukas%2520Retschmeier%26entry.1292438233%3D%2520%2520Motivated%2520by%2520applications%2520in%2520clustering%2520and%2520synthetic%2520data%2520generation%252C%2520we%250Aconsider%2520the%2520problem%2520of%2520releasing%2520a%2520minimum%2520spanning%2520tree%2520%2528MST%2529%2520under%250Aedge-weight%2520differential%2520privacy%2520constraints%2520where%2520a%2520graph%2520topology%2520%2524G%253D%2528V%252CE%2529%2524%250Awith%2520%2524n%2524%2520vertices%2520and%2520%2524m%2524%2520edges%2520is%2520public%252C%2520the%2520weight%2520matrix%2520%2524%255Cvec%257BW%257D%255Cin%250A%255Cmathbb%257BR%257D%255E%257Bn%2520%255Ctimes%2520n%257D%2524%2520is%2520private%252C%2520and%2520we%2520wish%2520to%2520release%2520an%2520approximate%2520MST%250Aunder%2520%2524%255Crho%2524-zero-concentrated%2520differential%2520privacy.%2520Weight%2520matrices%2520are%250Aconsidered%2520neighboring%2520if%2520they%2520differ%2520by%2520at%2520most%2520%2524%255CDelta_%255Cinfty%2524%2520in%2520each%2520entry%252C%250Ai.e.%252C%2520we%2520consider%2520an%2520%2524%255Cell_%255Cinfty%2524%2520neighboring%2520relationship.%2520Existing%2520private%250AMST%2520algorithms%2520either%2520add%2520noise%2520to%2520each%2520entry%2520in%2520%2524%255Cvec%257BW%257D%2524%2520and%2520estimate%2520the%2520MST%250Aby%2520post-processing%2520or%2520add%2520noise%2520to%2520weights%2520in-place%2520during%2520the%2520execution%2520of%2520a%250Aspecific%2520MST%2520algorithm.%2520Using%2520the%2520post-processing%2520approach%2520with%2520an%2520efficient%250AMST%2520algorithm%2520takes%2520%2524O%2528n%255E2%2529%2524%2520time%2520on%2520dense%2520graphs%2520but%2520results%2520in%2520an%2520additive%250Aerror%2520on%2520the%2520weight%2520of%2520the%2520MST%2520of%2520magnitude%2520%2524O%2528n%255E2%255Clog%2520n%2529%2524.%2520In-place%2520algorithms%250Agive%2520asymptotically%2520better%2520utility%252C%2520but%2520the%2520running%2520time%2520of%2520existing%2520in-place%250Aalgorithms%2520is%2520%2524O%2528n%255E3%2529%2524%2520for%2520dense%2520graphs.%2520Our%2520main%2520result%2520is%2520a%2520new%250Adifferentially%2520private%2520MST%2520algorithm%2520that%2520matches%2520the%2520utility%2520of%2520existing%250Ain-place%2520methods%2520while%2520running%2520in%2520time%2520%2524O%2528m%2520%252B%2520n%255E%257B3/2%257D%255Clog%2520n%2529%2524%2520for%2520fixed%2520privacy%250Aparameter%2520%2524%255Crho%2524.%2520The%2520technical%2520core%2520of%2520our%2520algorithm%2520is%2520an%2520efficient%2520sublinear%250Atime%2520simulation%2520of%2520Report-Noisy-Max%2520that%2520works%2520by%2520discretizing%2520all%2520edge%2520weights%250Ato%2520a%2520multiple%2520of%2520%2524%255CDelta_%255Cinfty%2524%2520and%2520forming%2520groups%2520of%2520edges%2520with%2520identical%250Aweights.%2520Specifically%252C%2520we%2520present%2520a%2520data%2520structure%2520that%2520allows%2520us%2520to%2520sample%2520a%250Anoisy%2520minimum%2520weight%2520edge%2520among%2520at%2520most%2520%2524O%2528n%255E2%2529%2524%2520cut%2520edges%2520in%2520%2524O%2528%255Csqrt%257Bn%257D%2520%255Clog%250An%2529%2524%2520time.%2520Experimental%2520evaluations%2520support%2520our%2520claims%2520that%2520our%2520algorithm%250Asignificantly%2520improves%2520previous%2520algorithms%2520either%2520in%2520utility%2520or%2520running%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Private%20Minimum%20Spanning%20Trees&entry.906535625=Rasmus%20Pagh%20and%20Lukas%20Retschmeier&entry.1292438233=%20%20Motivated%20by%20applications%20in%20clustering%20and%20synthetic%20data%20generation%2C%20we%0Aconsider%20the%20problem%20of%20releasing%20a%20minimum%20spanning%20tree%20%28MST%29%20under%0Aedge-weight%20differential%20privacy%20constraints%20where%20a%20graph%20topology%20%24G%3D%28V%2CE%29%24%0Awith%20%24n%24%20vertices%20and%20%24m%24%20edges%20is%20public%2C%20the%20weight%20matrix%20%24%5Cvec%7BW%7D%5Cin%0A%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D%24%20is%20private%2C%20and%20we%20wish%20to%20release%20an%20approximate%20MST%0Aunder%20%24%5Crho%24-zero-concentrated%20differential%20privacy.%20Weight%20matrices%20are%0Aconsidered%20neighboring%20if%20they%20differ%20by%20at%20most%20%24%5CDelta_%5Cinfty%24%20in%20each%20entry%2C%0Ai.e.%2C%20we%20consider%20an%20%24%5Cell_%5Cinfty%24%20neighboring%20relationship.%20Existing%20private%0AMST%20algorithms%20either%20add%20noise%20to%20each%20entry%20in%20%24%5Cvec%7BW%7D%24%20and%20estimate%20the%20MST%0Aby%20post-processing%20or%20add%20noise%20to%20weights%20in-place%20during%20the%20execution%20of%20a%0Aspecific%20MST%20algorithm.%20Using%20the%20post-processing%20approach%20with%20an%20efficient%0AMST%20algorithm%20takes%20%24O%28n%5E2%29%24%20time%20on%20dense%20graphs%20but%20results%20in%20an%20additive%0Aerror%20on%20the%20weight%20of%20the%20MST%20of%20magnitude%20%24O%28n%5E2%5Clog%20n%29%24.%20In-place%20algorithms%0Agive%20asymptotically%20better%20utility%2C%20but%20the%20running%20time%20of%20existing%20in-place%0Aalgorithms%20is%20%24O%28n%5E3%29%24%20for%20dense%20graphs.%20Our%20main%20result%20is%20a%20new%0Adifferentially%20private%20MST%20algorithm%20that%20matches%20the%20utility%20of%20existing%0Ain-place%20methods%20while%20running%20in%20time%20%24O%28m%20%2B%20n%5E%7B3/2%7D%5Clog%20n%29%24%20for%20fixed%20privacy%0Aparameter%20%24%5Crho%24.%20The%20technical%20core%20of%20our%20algorithm%20is%20an%20efficient%20sublinear%0Atime%20simulation%20of%20Report-Noisy-Max%20that%20works%20by%20discretizing%20all%20edge%20weights%0Ato%20a%20multiple%20of%20%24%5CDelta_%5Cinfty%24%20and%20forming%20groups%20of%20edges%20with%20identical%0Aweights.%20Specifically%2C%20we%20present%20a%20data%20structure%20that%20allows%20us%20to%20sample%20a%0Anoisy%20minimum%20weight%20edge%20among%20at%20most%20%24O%28n%5E2%29%24%20cut%20edges%20in%20%24O%28%5Csqrt%7Bn%7D%20%5Clog%0An%29%24%20time.%20Experimental%20evaluations%20support%20our%20claims%20that%20our%20algorithm%0Asignificantly%20improves%20previous%20algorithms%20either%20in%20utility%20or%20running%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06997v1&entry.124074799=Read"},
{"title": "On a Scale-Invariant Approach to Bundle Recommendations in Candy Crush\n  Saga", "author": "Styliani Katsarou and Francesca Carminati and Martin Dlask and Marta Braojos and Lavena Patra and Richard Perkins and Carlos Garcia Ling and Maria Paskevich", "abstract": "  A good understanding of player preferences is crucial for increasing content\nrelevancy, especially in mobile games. This paper illustrates the use of\nattentive models for producing item recommendations in a mobile game scenario.\nThe methodology comprises a combination of supervised and unsupervised\napproaches to create user-level recommendations while introducing a novel\nscale-invariant approach to the prediction. The methodology is subsequently\napplied to a bundle recommendation in Candy Crush Saga. The strategy of\ndeployment, maintenance, and monitoring of ML models that are scaled up to\nserve millions of users is presented, along with the best practices and design\npatterns adopted to minimize technical debt typical of ML systems. The\nrecommendation approach is evaluated both offline and online, with a focus on\nunderstanding the increase in engagement, click- and take rates, novelty\neffects, recommendation diversity, and the impact of degenerate feedback loops.\nWe have demonstrated that the recommendation enhances user engagement by 30%\nconcerning click rate and by more than 40% concerning take rate. In addition,\nwe empirically quantify the diminishing effects of recommendation accuracy on\nuser engagement.\n", "link": "http://arxiv.org/abs/2408.06799v1", "date": "2024-08-13", "relevancy": 1.3513, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4794}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4453}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20a%20Scale-Invariant%20Approach%20to%20Bundle%20Recommendations%20in%20Candy%20Crush%0A%20%20Saga&body=Title%3A%20On%20a%20Scale-Invariant%20Approach%20to%20Bundle%20Recommendations%20in%20Candy%20Crush%0A%20%20Saga%0AAuthor%3A%20Styliani%20Katsarou%20and%20Francesca%20Carminati%20and%20Martin%20Dlask%20and%20Marta%20Braojos%20and%20Lavena%20Patra%20and%20Richard%20Perkins%20and%20Carlos%20Garcia%20Ling%20and%20Maria%20Paskevich%0AAbstract%3A%20%20%20A%20good%20understanding%20of%20player%20preferences%20is%20crucial%20for%20increasing%20content%0Arelevancy%2C%20especially%20in%20mobile%20games.%20This%20paper%20illustrates%20the%20use%20of%0Aattentive%20models%20for%20producing%20item%20recommendations%20in%20a%20mobile%20game%20scenario.%0AThe%20methodology%20comprises%20a%20combination%20of%20supervised%20and%20unsupervised%0Aapproaches%20to%20create%20user-level%20recommendations%20while%20introducing%20a%20novel%0Ascale-invariant%20approach%20to%20the%20prediction.%20The%20methodology%20is%20subsequently%0Aapplied%20to%20a%20bundle%20recommendation%20in%20Candy%20Crush%20Saga.%20The%20strategy%20of%0Adeployment%2C%20maintenance%2C%20and%20monitoring%20of%20ML%20models%20that%20are%20scaled%20up%20to%0Aserve%20millions%20of%20users%20is%20presented%2C%20along%20with%20the%20best%20practices%20and%20design%0Apatterns%20adopted%20to%20minimize%20technical%20debt%20typical%20of%20ML%20systems.%20The%0Arecommendation%20approach%20is%20evaluated%20both%20offline%20and%20online%2C%20with%20a%20focus%20on%0Aunderstanding%20the%20increase%20in%20engagement%2C%20click-%20and%20take%20rates%2C%20novelty%0Aeffects%2C%20recommendation%20diversity%2C%20and%20the%20impact%20of%20degenerate%20feedback%20loops.%0AWe%20have%20demonstrated%20that%20the%20recommendation%20enhances%20user%20engagement%20by%2030%25%0Aconcerning%20click%20rate%20and%20by%20more%20than%2040%25%20concerning%20take%20rate.%20In%20addition%2C%0Awe%20empirically%20quantify%20the%20diminishing%20effects%20of%20recommendation%20accuracy%20on%0Auser%20engagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520a%2520Scale-Invariant%2520Approach%2520to%2520Bundle%2520Recommendations%2520in%2520Candy%2520Crush%250A%2520%2520Saga%26entry.906535625%3DStyliani%2520Katsarou%2520and%2520Francesca%2520Carminati%2520and%2520Martin%2520Dlask%2520and%2520Marta%2520Braojos%2520and%2520Lavena%2520Patra%2520and%2520Richard%2520Perkins%2520and%2520Carlos%2520Garcia%2520Ling%2520and%2520Maria%2520Paskevich%26entry.1292438233%3D%2520%2520A%2520good%2520understanding%2520of%2520player%2520preferences%2520is%2520crucial%2520for%2520increasing%2520content%250Arelevancy%252C%2520especially%2520in%2520mobile%2520games.%2520This%2520paper%2520illustrates%2520the%2520use%2520of%250Aattentive%2520models%2520for%2520producing%2520item%2520recommendations%2520in%2520a%2520mobile%2520game%2520scenario.%250AThe%2520methodology%2520comprises%2520a%2520combination%2520of%2520supervised%2520and%2520unsupervised%250Aapproaches%2520to%2520create%2520user-level%2520recommendations%2520while%2520introducing%2520a%2520novel%250Ascale-invariant%2520approach%2520to%2520the%2520prediction.%2520The%2520methodology%2520is%2520subsequently%250Aapplied%2520to%2520a%2520bundle%2520recommendation%2520in%2520Candy%2520Crush%2520Saga.%2520The%2520strategy%2520of%250Adeployment%252C%2520maintenance%252C%2520and%2520monitoring%2520of%2520ML%2520models%2520that%2520are%2520scaled%2520up%2520to%250Aserve%2520millions%2520of%2520users%2520is%2520presented%252C%2520along%2520with%2520the%2520best%2520practices%2520and%2520design%250Apatterns%2520adopted%2520to%2520minimize%2520technical%2520debt%2520typical%2520of%2520ML%2520systems.%2520The%250Arecommendation%2520approach%2520is%2520evaluated%2520both%2520offline%2520and%2520online%252C%2520with%2520a%2520focus%2520on%250Aunderstanding%2520the%2520increase%2520in%2520engagement%252C%2520click-%2520and%2520take%2520rates%252C%2520novelty%250Aeffects%252C%2520recommendation%2520diversity%252C%2520and%2520the%2520impact%2520of%2520degenerate%2520feedback%2520loops.%250AWe%2520have%2520demonstrated%2520that%2520the%2520recommendation%2520enhances%2520user%2520engagement%2520by%252030%2525%250Aconcerning%2520click%2520rate%2520and%2520by%2520more%2520than%252040%2525%2520concerning%2520take%2520rate.%2520In%2520addition%252C%250Awe%2520empirically%2520quantify%2520the%2520diminishing%2520effects%2520of%2520recommendation%2520accuracy%2520on%250Auser%2520engagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20a%20Scale-Invariant%20Approach%20to%20Bundle%20Recommendations%20in%20Candy%20Crush%0A%20%20Saga&entry.906535625=Styliani%20Katsarou%20and%20Francesca%20Carminati%20and%20Martin%20Dlask%20and%20Marta%20Braojos%20and%20Lavena%20Patra%20and%20Richard%20Perkins%20and%20Carlos%20Garcia%20Ling%20and%20Maria%20Paskevich&entry.1292438233=%20%20A%20good%20understanding%20of%20player%20preferences%20is%20crucial%20for%20increasing%20content%0Arelevancy%2C%20especially%20in%20mobile%20games.%20This%20paper%20illustrates%20the%20use%20of%0Aattentive%20models%20for%20producing%20item%20recommendations%20in%20a%20mobile%20game%20scenario.%0AThe%20methodology%20comprises%20a%20combination%20of%20supervised%20and%20unsupervised%0Aapproaches%20to%20create%20user-level%20recommendations%20while%20introducing%20a%20novel%0Ascale-invariant%20approach%20to%20the%20prediction.%20The%20methodology%20is%20subsequently%0Aapplied%20to%20a%20bundle%20recommendation%20in%20Candy%20Crush%20Saga.%20The%20strategy%20of%0Adeployment%2C%20maintenance%2C%20and%20monitoring%20of%20ML%20models%20that%20are%20scaled%20up%20to%0Aserve%20millions%20of%20users%20is%20presented%2C%20along%20with%20the%20best%20practices%20and%20design%0Apatterns%20adopted%20to%20minimize%20technical%20debt%20typical%20of%20ML%20systems.%20The%0Arecommendation%20approach%20is%20evaluated%20both%20offline%20and%20online%2C%20with%20a%20focus%20on%0Aunderstanding%20the%20increase%20in%20engagement%2C%20click-%20and%20take%20rates%2C%20novelty%0Aeffects%2C%20recommendation%20diversity%2C%20and%20the%20impact%20of%20degenerate%20feedback%20loops.%0AWe%20have%20demonstrated%20that%20the%20recommendation%20enhances%20user%20engagement%20by%2030%25%0Aconcerning%20click%20rate%20and%20by%20more%20than%2040%25%20concerning%20take%20rate.%20In%20addition%2C%0Awe%20empirically%20quantify%20the%20diminishing%20effects%20of%20recommendation%20accuracy%20on%0Auser%20engagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06799v1&entry.124074799=Read"},
{"title": "DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs", "author": "Dongyuan Li and Shiyin Tan and Ying Zhang and Ming Jin and Shirui Pan and Manabu Okumura and Renhe Jiang", "abstract": "  Dynamic graph learning aims to uncover evolutionary laws in real-world\nsystems, enabling accurate social recommendation (link prediction) or early\ndetection of cancer cells (classification). Inspired by the success of state\nspace models, e.g., Mamba, for efficiently capturing long-term dependencies in\nlanguage modeling, we propose DyG-Mamba, a new continuous state space model\n(SSM) for dynamic graph learning. Specifically, we first found that using\ninputs as control signals for SSM is not suitable for continuous-time dynamic\nnetwork data with irregular sampling intervals, resulting in models being\ninsensitive to time information and lacking generalization properties. Drawing\ninspiration from the Ebbinghaus forgetting curve, which suggests that memory of\npast events is strongly correlated with time intervals rather than specific\ndetails of the events themselves, we directly utilize irregular time spans as\ncontrol signals for SSM to achieve significant robustness and generalization.\nThrough exhaustive experiments on 12 datasets for dynamic link prediction and\ndynamic node classification tasks, we found that DyG-Mamba achieves\nstate-of-the-art performance on most of the datasets, while also demonstrating\nsignificantly improved computation and memory efficiency.\n", "link": "http://arxiv.org/abs/2408.06966v1", "date": "2024-08-13", "relevancy": 1.6149, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5601}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5116}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyG-Mamba%3A%20Continuous%20State%20Space%20Modeling%20on%20Dynamic%20Graphs&body=Title%3A%20DyG-Mamba%3A%20Continuous%20State%20Space%20Modeling%20on%20Dynamic%20Graphs%0AAuthor%3A%20Dongyuan%20Li%20and%20Shiyin%20Tan%20and%20Ying%20Zhang%20and%20Ming%20Jin%20and%20Shirui%20Pan%20and%20Manabu%20Okumura%20and%20Renhe%20Jiang%0AAbstract%3A%20%20%20Dynamic%20graph%20learning%20aims%20to%20uncover%20evolutionary%20laws%20in%20real-world%0Asystems%2C%20enabling%20accurate%20social%20recommendation%20%28link%20prediction%29%20or%20early%0Adetection%20of%20cancer%20cells%20%28classification%29.%20Inspired%20by%20the%20success%20of%20state%0Aspace%20models%2C%20e.g.%2C%20Mamba%2C%20for%20efficiently%20capturing%20long-term%20dependencies%20in%0Alanguage%20modeling%2C%20we%20propose%20DyG-Mamba%2C%20a%20new%20continuous%20state%20space%20model%0A%28SSM%29%20for%20dynamic%20graph%20learning.%20Specifically%2C%20we%20first%20found%20that%20using%0Ainputs%20as%20control%20signals%20for%20SSM%20is%20not%20suitable%20for%20continuous-time%20dynamic%0Anetwork%20data%20with%20irregular%20sampling%20intervals%2C%20resulting%20in%20models%20being%0Ainsensitive%20to%20time%20information%20and%20lacking%20generalization%20properties.%20Drawing%0Ainspiration%20from%20the%20Ebbinghaus%20forgetting%20curve%2C%20which%20suggests%20that%20memory%20of%0Apast%20events%20is%20strongly%20correlated%20with%20time%20intervals%20rather%20than%20specific%0Adetails%20of%20the%20events%20themselves%2C%20we%20directly%20utilize%20irregular%20time%20spans%20as%0Acontrol%20signals%20for%20SSM%20to%20achieve%20significant%20robustness%20and%20generalization.%0AThrough%20exhaustive%20experiments%20on%2012%20datasets%20for%20dynamic%20link%20prediction%20and%0Adynamic%20node%20classification%20tasks%2C%20we%20found%20that%20DyG-Mamba%20achieves%0Astate-of-the-art%20performance%20on%20most%20of%20the%20datasets%2C%20while%20also%20demonstrating%0Asignificantly%20improved%20computation%20and%20memory%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyG-Mamba%253A%2520Continuous%2520State%2520Space%2520Modeling%2520on%2520Dynamic%2520Graphs%26entry.906535625%3DDongyuan%2520Li%2520and%2520Shiyin%2520Tan%2520and%2520Ying%2520Zhang%2520and%2520Ming%2520Jin%2520and%2520Shirui%2520Pan%2520and%2520Manabu%2520Okumura%2520and%2520Renhe%2520Jiang%26entry.1292438233%3D%2520%2520Dynamic%2520graph%2520learning%2520aims%2520to%2520uncover%2520evolutionary%2520laws%2520in%2520real-world%250Asystems%252C%2520enabling%2520accurate%2520social%2520recommendation%2520%2528link%2520prediction%2529%2520or%2520early%250Adetection%2520of%2520cancer%2520cells%2520%2528classification%2529.%2520Inspired%2520by%2520the%2520success%2520of%2520state%250Aspace%2520models%252C%2520e.g.%252C%2520Mamba%252C%2520for%2520efficiently%2520capturing%2520long-term%2520dependencies%2520in%250Alanguage%2520modeling%252C%2520we%2520propose%2520DyG-Mamba%252C%2520a%2520new%2520continuous%2520state%2520space%2520model%250A%2528SSM%2529%2520for%2520dynamic%2520graph%2520learning.%2520Specifically%252C%2520we%2520first%2520found%2520that%2520using%250Ainputs%2520as%2520control%2520signals%2520for%2520SSM%2520is%2520not%2520suitable%2520for%2520continuous-time%2520dynamic%250Anetwork%2520data%2520with%2520irregular%2520sampling%2520intervals%252C%2520resulting%2520in%2520models%2520being%250Ainsensitive%2520to%2520time%2520information%2520and%2520lacking%2520generalization%2520properties.%2520Drawing%250Ainspiration%2520from%2520the%2520Ebbinghaus%2520forgetting%2520curve%252C%2520which%2520suggests%2520that%2520memory%2520of%250Apast%2520events%2520is%2520strongly%2520correlated%2520with%2520time%2520intervals%2520rather%2520than%2520specific%250Adetails%2520of%2520the%2520events%2520themselves%252C%2520we%2520directly%2520utilize%2520irregular%2520time%2520spans%2520as%250Acontrol%2520signals%2520for%2520SSM%2520to%2520achieve%2520significant%2520robustness%2520and%2520generalization.%250AThrough%2520exhaustive%2520experiments%2520on%252012%2520datasets%2520for%2520dynamic%2520link%2520prediction%2520and%250Adynamic%2520node%2520classification%2520tasks%252C%2520we%2520found%2520that%2520DyG-Mamba%2520achieves%250Astate-of-the-art%2520performance%2520on%2520most%2520of%2520the%2520datasets%252C%2520while%2520also%2520demonstrating%250Asignificantly%2520improved%2520computation%2520and%2520memory%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyG-Mamba%3A%20Continuous%20State%20Space%20Modeling%20on%20Dynamic%20Graphs&entry.906535625=Dongyuan%20Li%20and%20Shiyin%20Tan%20and%20Ying%20Zhang%20and%20Ming%20Jin%20and%20Shirui%20Pan%20and%20Manabu%20Okumura%20and%20Renhe%20Jiang&entry.1292438233=%20%20Dynamic%20graph%20learning%20aims%20to%20uncover%20evolutionary%20laws%20in%20real-world%0Asystems%2C%20enabling%20accurate%20social%20recommendation%20%28link%20prediction%29%20or%20early%0Adetection%20of%20cancer%20cells%20%28classification%29.%20Inspired%20by%20the%20success%20of%20state%0Aspace%20models%2C%20e.g.%2C%20Mamba%2C%20for%20efficiently%20capturing%20long-term%20dependencies%20in%0Alanguage%20modeling%2C%20we%20propose%20DyG-Mamba%2C%20a%20new%20continuous%20state%20space%20model%0A%28SSM%29%20for%20dynamic%20graph%20learning.%20Specifically%2C%20we%20first%20found%20that%20using%0Ainputs%20as%20control%20signals%20for%20SSM%20is%20not%20suitable%20for%20continuous-time%20dynamic%0Anetwork%20data%20with%20irregular%20sampling%20intervals%2C%20resulting%20in%20models%20being%0Ainsensitive%20to%20time%20information%20and%20lacking%20generalization%20properties.%20Drawing%0Ainspiration%20from%20the%20Ebbinghaus%20forgetting%20curve%2C%20which%20suggests%20that%20memory%20of%0Apast%20events%20is%20strongly%20correlated%20with%20time%20intervals%20rather%20than%20specific%0Adetails%20of%20the%20events%20themselves%2C%20we%20directly%20utilize%20irregular%20time%20spans%20as%0Acontrol%20signals%20for%20SSM%20to%20achieve%20significant%20robustness%20and%20generalization.%0AThrough%20exhaustive%20experiments%20on%2012%20datasets%20for%20dynamic%20link%20prediction%20and%0Adynamic%20node%20classification%20tasks%2C%20we%20found%20that%20DyG-Mamba%20achieves%0Astate-of-the-art%20performance%20on%20most%20of%20the%20datasets%2C%20while%20also%20demonstrating%0Asignificantly%20improved%20computation%20and%20memory%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06966v1&entry.124074799=Read"},
{"title": "Active Learning for Control-Oriented Identification of Nonlinear Systems", "author": "Bruce D. Lee and Ingvar Ziemann and George J. Pappas and Nikolai Matni", "abstract": "  Model-based reinforcement learning is an effective approach for controlling\nan unknown system. It is based on a longstanding pipeline familiar to the\ncontrol community in which one performs experiments on the environment to\ncollect a dataset, uses the resulting dataset to identify a model of the\nsystem, and finally performs control synthesis using the identified model. As\ninteracting with the system may be costly and time consuming, targeted\nexploration is crucial for developing an effective control-oriented model with\nminimal experimentation. Motivated by this challenge, recent work has begun to\nstudy finite sample data requirements and sample efficient algorithms for the\nproblem of optimal exploration in model-based reinforcement learning. However,\nexisting theory and algorithms are limited to model classes which are linear in\nthe parameters. Our work instead focuses on models with nonlinear parameter\ndependencies, and presents the first finite sample analysis of an active\nlearning algorithm suitable for a general class of nonlinear dynamics. In\ncertain settings, the excess control cost of our algorithm achieves the optimal\nrate, up to logarithmic factors. We validate our approach in simulation,\nshowcasing the advantage of active, control-oriented exploration for\ncontrolling nonlinear systems.\n", "link": "http://arxiv.org/abs/2404.09030v2", "date": "2024-08-13", "relevancy": 1.6584, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20for%20Control-Oriented%20Identification%20of%20Nonlinear%20Systems&body=Title%3A%20Active%20Learning%20for%20Control-Oriented%20Identification%20of%20Nonlinear%20Systems%0AAuthor%3A%20Bruce%20D.%20Lee%20and%20Ingvar%20Ziemann%20and%20George%20J.%20Pappas%20and%20Nikolai%20Matni%0AAbstract%3A%20%20%20Model-based%20reinforcement%20learning%20is%20an%20effective%20approach%20for%20controlling%0Aan%20unknown%20system.%20It%20is%20based%20on%20a%20longstanding%20pipeline%20familiar%20to%20the%0Acontrol%20community%20in%20which%20one%20performs%20experiments%20on%20the%20environment%20to%0Acollect%20a%20dataset%2C%20uses%20the%20resulting%20dataset%20to%20identify%20a%20model%20of%20the%0Asystem%2C%20and%20finally%20performs%20control%20synthesis%20using%20the%20identified%20model.%20As%0Ainteracting%20with%20the%20system%20may%20be%20costly%20and%20time%20consuming%2C%20targeted%0Aexploration%20is%20crucial%20for%20developing%20an%20effective%20control-oriented%20model%20with%0Aminimal%20experimentation.%20Motivated%20by%20this%20challenge%2C%20recent%20work%20has%20begun%20to%0Astudy%20finite%20sample%20data%20requirements%20and%20sample%20efficient%20algorithms%20for%20the%0Aproblem%20of%20optimal%20exploration%20in%20model-based%20reinforcement%20learning.%20However%2C%0Aexisting%20theory%20and%20algorithms%20are%20limited%20to%20model%20classes%20which%20are%20linear%20in%0Athe%20parameters.%20Our%20work%20instead%20focuses%20on%20models%20with%20nonlinear%20parameter%0Adependencies%2C%20and%20presents%20the%20first%20finite%20sample%20analysis%20of%20an%20active%0Alearning%20algorithm%20suitable%20for%20a%20general%20class%20of%20nonlinear%20dynamics.%20In%0Acertain%20settings%2C%20the%20excess%20control%20cost%20of%20our%20algorithm%20achieves%20the%20optimal%0Arate%2C%20up%20to%20logarithmic%20factors.%20We%20validate%20our%20approach%20in%20simulation%2C%0Ashowcasing%20the%20advantage%20of%20active%2C%20control-oriented%20exploration%20for%0Acontrolling%20nonlinear%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09030v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520for%2520Control-Oriented%2520Identification%2520of%2520Nonlinear%2520Systems%26entry.906535625%3DBruce%2520D.%2520Lee%2520and%2520Ingvar%2520Ziemann%2520and%2520George%2520J.%2520Pappas%2520and%2520Nikolai%2520Matni%26entry.1292438233%3D%2520%2520Model-based%2520reinforcement%2520learning%2520is%2520an%2520effective%2520approach%2520for%2520controlling%250Aan%2520unknown%2520system.%2520It%2520is%2520based%2520on%2520a%2520longstanding%2520pipeline%2520familiar%2520to%2520the%250Acontrol%2520community%2520in%2520which%2520one%2520performs%2520experiments%2520on%2520the%2520environment%2520to%250Acollect%2520a%2520dataset%252C%2520uses%2520the%2520resulting%2520dataset%2520to%2520identify%2520a%2520model%2520of%2520the%250Asystem%252C%2520and%2520finally%2520performs%2520control%2520synthesis%2520using%2520the%2520identified%2520model.%2520As%250Ainteracting%2520with%2520the%2520system%2520may%2520be%2520costly%2520and%2520time%2520consuming%252C%2520targeted%250Aexploration%2520is%2520crucial%2520for%2520developing%2520an%2520effective%2520control-oriented%2520model%2520with%250Aminimal%2520experimentation.%2520Motivated%2520by%2520this%2520challenge%252C%2520recent%2520work%2520has%2520begun%2520to%250Astudy%2520finite%2520sample%2520data%2520requirements%2520and%2520sample%2520efficient%2520algorithms%2520for%2520the%250Aproblem%2520of%2520optimal%2520exploration%2520in%2520model-based%2520reinforcement%2520learning.%2520However%252C%250Aexisting%2520theory%2520and%2520algorithms%2520are%2520limited%2520to%2520model%2520classes%2520which%2520are%2520linear%2520in%250Athe%2520parameters.%2520Our%2520work%2520instead%2520focuses%2520on%2520models%2520with%2520nonlinear%2520parameter%250Adependencies%252C%2520and%2520presents%2520the%2520first%2520finite%2520sample%2520analysis%2520of%2520an%2520active%250Alearning%2520algorithm%2520suitable%2520for%2520a%2520general%2520class%2520of%2520nonlinear%2520dynamics.%2520In%250Acertain%2520settings%252C%2520the%2520excess%2520control%2520cost%2520of%2520our%2520algorithm%2520achieves%2520the%2520optimal%250Arate%252C%2520up%2520to%2520logarithmic%2520factors.%2520We%2520validate%2520our%2520approach%2520in%2520simulation%252C%250Ashowcasing%2520the%2520advantage%2520of%2520active%252C%2520control-oriented%2520exploration%2520for%250Acontrolling%2520nonlinear%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09030v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20for%20Control-Oriented%20Identification%20of%20Nonlinear%20Systems&entry.906535625=Bruce%20D.%20Lee%20and%20Ingvar%20Ziemann%20and%20George%20J.%20Pappas%20and%20Nikolai%20Matni&entry.1292438233=%20%20Model-based%20reinforcement%20learning%20is%20an%20effective%20approach%20for%20controlling%0Aan%20unknown%20system.%20It%20is%20based%20on%20a%20longstanding%20pipeline%20familiar%20to%20the%0Acontrol%20community%20in%20which%20one%20performs%20experiments%20on%20the%20environment%20to%0Acollect%20a%20dataset%2C%20uses%20the%20resulting%20dataset%20to%20identify%20a%20model%20of%20the%0Asystem%2C%20and%20finally%20performs%20control%20synthesis%20using%20the%20identified%20model.%20As%0Ainteracting%20with%20the%20system%20may%20be%20costly%20and%20time%20consuming%2C%20targeted%0Aexploration%20is%20crucial%20for%20developing%20an%20effective%20control-oriented%20model%20with%0Aminimal%20experimentation.%20Motivated%20by%20this%20challenge%2C%20recent%20work%20has%20begun%20to%0Astudy%20finite%20sample%20data%20requirements%20and%20sample%20efficient%20algorithms%20for%20the%0Aproblem%20of%20optimal%20exploration%20in%20model-based%20reinforcement%20learning.%20However%2C%0Aexisting%20theory%20and%20algorithms%20are%20limited%20to%20model%20classes%20which%20are%20linear%20in%0Athe%20parameters.%20Our%20work%20instead%20focuses%20on%20models%20with%20nonlinear%20parameter%0Adependencies%2C%20and%20presents%20the%20first%20finite%20sample%20analysis%20of%20an%20active%0Alearning%20algorithm%20suitable%20for%20a%20general%20class%20of%20nonlinear%20dynamics.%20In%0Acertain%20settings%2C%20the%20excess%20control%20cost%20of%20our%20algorithm%20achieves%20the%20optimal%0Arate%2C%20up%20to%20logarithmic%20factors.%20We%20validate%20our%20approach%20in%20simulation%2C%0Ashowcasing%20the%20advantage%20of%20active%2C%20control-oriented%20exploration%20for%0Acontrolling%20nonlinear%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09030v2&entry.124074799=Read"},
{"title": "A Survey on Model MoErging: Recycling and Routing Among Specialized\n  Experts for Collaborative Learning", "author": "Prateek Yadav and Colin Raffel and Mohammed Muqeeth and Lucas Caccia and Haokun Liu and Tianlong Chen and Mohit Bansal and Leshem Choshen and Alessandro Sordoni", "abstract": "  The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field.\n", "link": "http://arxiv.org/abs/2408.07057v1", "date": "2024-08-13", "relevancy": 1.4241, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5041}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4663}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Model%20MoErging%3A%20Recycling%20and%20Routing%20Among%20Specialized%0A%20%20Experts%20for%20Collaborative%20Learning&body=Title%3A%20A%20Survey%20on%20Model%20MoErging%3A%20Recycling%20and%20Routing%20Among%20Specialized%0A%20%20Experts%20for%20Collaborative%20Learning%0AAuthor%3A%20Prateek%20Yadav%20and%20Colin%20Raffel%20and%20Mohammed%20Muqeeth%20and%20Lucas%20Caccia%20and%20Haokun%20Liu%20and%20Tianlong%20Chen%20and%20Mohit%20Bansal%20and%20Leshem%20Choshen%20and%20Alessandro%20Sordoni%0AAbstract%3A%20%20%20The%20availability%20of%20performant%20pre-trained%20models%20has%20led%20to%20a%20proliferation%0Aof%20fine-tuned%20expert%20models%20that%20are%20specialized%20to%20a%20particular%20domain%20or%0Atask.%20Model%20MoErging%20methods%20aim%20to%20recycle%20expert%20models%20to%20create%20an%0Aaggregate%20system%20with%20improved%20performance%20or%20generalization.%20A%20key%20component%0Aof%20MoErging%20methods%20is%20the%20creation%20of%20a%20router%20that%20decides%20which%20expert%0Amodel%28s%29%20to%20use%20for%20a%20particular%20input%20or%20application.%20The%20promise%2C%0Aeffectiveness%2C%20and%20large%20design%20space%20of%20MoErging%20has%20spurred%20the%20development%0Aof%20many%20new%20methods%20over%20the%20past%20few%20years.%20This%20rapid%20pace%20of%20development%20has%0Amade%20it%20challenging%20to%20compare%20different%20MoErging%20methods%2C%20which%20are%20rarely%0Acompared%20to%20one%20another%20and%20are%20often%20validated%20in%20different%20experimental%0Asetups.%20To%20remedy%20such%20gaps%2C%20we%20present%20a%20comprehensive%20survey%20of%20MoErging%0Amethods%20that%20includes%20a%20novel%20taxonomy%20for%20cataloging%20key%20design%20choices%20and%0Aclarifying%20suitable%20applications%20for%20each%20method.%20Apart%20from%20surveying%20MoErging%0Aresearch%2C%20we%20inventory%20software%20tools%20and%20applications%20that%20make%20use%20of%0AMoErging.%20We%20additionally%20discuss%20related%20fields%20of%20study%20such%20as%20model%0Amerging%2C%20multitask%20learning%2C%20and%20mixture-of-experts%20models.%20Taken%20as%20a%20whole%2C%0Aour%20survey%20provides%20a%20unified%20overview%20of%20existing%20MoErging%20methods%20and%20creates%0Aa%20solid%20foundation%20for%20future%20work%20in%20this%20burgeoning%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Model%2520MoErging%253A%2520Recycling%2520and%2520Routing%2520Among%2520Specialized%250A%2520%2520Experts%2520for%2520Collaborative%2520Learning%26entry.906535625%3DPrateek%2520Yadav%2520and%2520Colin%2520Raffel%2520and%2520Mohammed%2520Muqeeth%2520and%2520Lucas%2520Caccia%2520and%2520Haokun%2520Liu%2520and%2520Tianlong%2520Chen%2520and%2520Mohit%2520Bansal%2520and%2520Leshem%2520Choshen%2520and%2520Alessandro%2520Sordoni%26entry.1292438233%3D%2520%2520The%2520availability%2520of%2520performant%2520pre-trained%2520models%2520has%2520led%2520to%2520a%2520proliferation%250Aof%2520fine-tuned%2520expert%2520models%2520that%2520are%2520specialized%2520to%2520a%2520particular%2520domain%2520or%250Atask.%2520Model%2520MoErging%2520methods%2520aim%2520to%2520recycle%2520expert%2520models%2520to%2520create%2520an%250Aaggregate%2520system%2520with%2520improved%2520performance%2520or%2520generalization.%2520A%2520key%2520component%250Aof%2520MoErging%2520methods%2520is%2520the%2520creation%2520of%2520a%2520router%2520that%2520decides%2520which%2520expert%250Amodel%2528s%2529%2520to%2520use%2520for%2520a%2520particular%2520input%2520or%2520application.%2520The%2520promise%252C%250Aeffectiveness%252C%2520and%2520large%2520design%2520space%2520of%2520MoErging%2520has%2520spurred%2520the%2520development%250Aof%2520many%2520new%2520methods%2520over%2520the%2520past%2520few%2520years.%2520This%2520rapid%2520pace%2520of%2520development%2520has%250Amade%2520it%2520challenging%2520to%2520compare%2520different%2520MoErging%2520methods%252C%2520which%2520are%2520rarely%250Acompared%2520to%2520one%2520another%2520and%2520are%2520often%2520validated%2520in%2520different%2520experimental%250Asetups.%2520To%2520remedy%2520such%2520gaps%252C%2520we%2520present%2520a%2520comprehensive%2520survey%2520of%2520MoErging%250Amethods%2520that%2520includes%2520a%2520novel%2520taxonomy%2520for%2520cataloging%2520key%2520design%2520choices%2520and%250Aclarifying%2520suitable%2520applications%2520for%2520each%2520method.%2520Apart%2520from%2520surveying%2520MoErging%250Aresearch%252C%2520we%2520inventory%2520software%2520tools%2520and%2520applications%2520that%2520make%2520use%2520of%250AMoErging.%2520We%2520additionally%2520discuss%2520related%2520fields%2520of%2520study%2520such%2520as%2520model%250Amerging%252C%2520multitask%2520learning%252C%2520and%2520mixture-of-experts%2520models.%2520Taken%2520as%2520a%2520whole%252C%250Aour%2520survey%2520provides%2520a%2520unified%2520overview%2520of%2520existing%2520MoErging%2520methods%2520and%2520creates%250Aa%2520solid%2520foundation%2520for%2520future%2520work%2520in%2520this%2520burgeoning%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Model%20MoErging%3A%20Recycling%20and%20Routing%20Among%20Specialized%0A%20%20Experts%20for%20Collaborative%20Learning&entry.906535625=Prateek%20Yadav%20and%20Colin%20Raffel%20and%20Mohammed%20Muqeeth%20and%20Lucas%20Caccia%20and%20Haokun%20Liu%20and%20Tianlong%20Chen%20and%20Mohit%20Bansal%20and%20Leshem%20Choshen%20and%20Alessandro%20Sordoni&entry.1292438233=%20%20The%20availability%20of%20performant%20pre-trained%20models%20has%20led%20to%20a%20proliferation%0Aof%20fine-tuned%20expert%20models%20that%20are%20specialized%20to%20a%20particular%20domain%20or%0Atask.%20Model%20MoErging%20methods%20aim%20to%20recycle%20expert%20models%20to%20create%20an%0Aaggregate%20system%20with%20improved%20performance%20or%20generalization.%20A%20key%20component%0Aof%20MoErging%20methods%20is%20the%20creation%20of%20a%20router%20that%20decides%20which%20expert%0Amodel%28s%29%20to%20use%20for%20a%20particular%20input%20or%20application.%20The%20promise%2C%0Aeffectiveness%2C%20and%20large%20design%20space%20of%20MoErging%20has%20spurred%20the%20development%0Aof%20many%20new%20methods%20over%20the%20past%20few%20years.%20This%20rapid%20pace%20of%20development%20has%0Amade%20it%20challenging%20to%20compare%20different%20MoErging%20methods%2C%20which%20are%20rarely%0Acompared%20to%20one%20another%20and%20are%20often%20validated%20in%20different%20experimental%0Asetups.%20To%20remedy%20such%20gaps%2C%20we%20present%20a%20comprehensive%20survey%20of%20MoErging%0Amethods%20that%20includes%20a%20novel%20taxonomy%20for%20cataloging%20key%20design%20choices%20and%0Aclarifying%20suitable%20applications%20for%20each%20method.%20Apart%20from%20surveying%20MoErging%0Aresearch%2C%20we%20inventory%20software%20tools%20and%20applications%20that%20make%20use%20of%0AMoErging.%20We%20additionally%20discuss%20related%20fields%20of%20study%20such%20as%20model%0Amerging%2C%20multitask%20learning%2C%20and%20mixture-of-experts%20models.%20Taken%20as%20a%20whole%2C%0Aour%20survey%20provides%20a%20unified%20overview%20of%20existing%20MoErging%20methods%20and%20creates%0Aa%20solid%20foundation%20for%20future%20work%20in%20this%20burgeoning%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07057v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


