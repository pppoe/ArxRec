<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250430.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Garment3DGen: 3D Garment Stylization and Texture Generation", "author": "Nikolaos Sarafianos and Tuur Stuyck and Xiaoyu Xiang and Yilei Li and Jovan Popovic and Rakesh Ranjan", "abstract": "  We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. We leverage the recent progress of\nimage-to-3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nCarefully designed losses allow the base mesh to freely deform towards the\ndesired target, yet preserve mesh quality and topology such that they can be\nsimulated. Finally, we generate high-fidelity texture maps that are globally\nand locally consistent and faithfully capture the input guidance, allowing us\nto render the generated 3D assets. With Garment3DGen users can generate the\nsimulation-ready 3D garment of their choice without the need of artist\nintervention. We present a plethora of quantitative and qualitative comparisons\non various assets and demonstrate that Garment3DGen unlocks key applications\nranging from sketch-to-simulated garments or interacting with the garments in\nVR. Code is publicly available.\n", "link": "http://arxiv.org/abs/2403.18816v3", "date": "2025-04-30", "relevancy": 3.6784, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.8065}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7785}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Garment3DGen%3A%203D%20Garment%20Stylization%20and%20Texture%20Generation&body=Title%3A%20Garment3DGen%3A%203D%20Garment%20Stylization%20and%20Texture%20Generation%0AAuthor%3A%20Nikolaos%20Sarafianos%20and%20Tuur%20Stuyck%20and%20Xiaoyu%20Xiang%20and%20Yilei%20Li%20and%20Jovan%20Popovic%20and%20Rakesh%20Ranjan%0AAbstract%3A%20%20%20We%20introduce%20Garment3DGen%20a%20new%20method%20to%20synthesize%203D%20garment%20assets%20from%20a%0Abase%20mesh%20given%20a%20single%20input%20image%20as%20guidance.%20Our%20proposed%20approach%20allows%0Ausers%20to%20generate%203D%20textured%20clothes%20based%20on%20both%20real%20and%20synthetic%20images%2C%0Asuch%20as%20those%20generated%20by%20text%20prompts.%20The%20generated%20assets%20can%20be%20directly%0Adraped%20and%20simulated%20on%20human%20bodies.%20We%20leverage%20the%20recent%20progress%20of%0Aimage-to-3D%20diffusion%20methods%20to%20generate%203D%20garment%20geometries.%20However%2C%20since%0Athese%20geometries%20cannot%20be%20utilized%20directly%20for%20downstream%20tasks%2C%20we%20propose%0Ato%20use%20them%20as%20pseudo%20ground-truth%20and%20set%20up%20a%20mesh%20deformation%20optimization%0Aprocedure%20that%20deforms%20a%20base%20template%20mesh%20to%20match%20the%20generated%203D%20target.%0ACarefully%20designed%20losses%20allow%20the%20base%20mesh%20to%20freely%20deform%20towards%20the%0Adesired%20target%2C%20yet%20preserve%20mesh%20quality%20and%20topology%20such%20that%20they%20can%20be%0Asimulated.%20Finally%2C%20we%20generate%20high-fidelity%20texture%20maps%20that%20are%20globally%0Aand%20locally%20consistent%20and%20faithfully%20capture%20the%20input%20guidance%2C%20allowing%20us%0Ato%20render%20the%20generated%203D%20assets.%20With%20Garment3DGen%20users%20can%20generate%20the%0Asimulation-ready%203D%20garment%20of%20their%20choice%20without%20the%20need%20of%20artist%0Aintervention.%20We%20present%20a%20plethora%20of%20quantitative%20and%20qualitative%20comparisons%0Aon%20various%20assets%20and%20demonstrate%20that%20Garment3DGen%20unlocks%20key%20applications%0Aranging%20from%20sketch-to-simulated%20garments%20or%20interacting%20with%20the%20garments%20in%0AVR.%20Code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18816v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarment3DGen%253A%25203D%2520Garment%2520Stylization%2520and%2520Texture%2520Generation%26entry.906535625%3DNikolaos%2520Sarafianos%2520and%2520Tuur%2520Stuyck%2520and%2520Xiaoyu%2520Xiang%2520and%2520Yilei%2520Li%2520and%2520Jovan%2520Popovic%2520and%2520Rakesh%2520Ranjan%26entry.1292438233%3D%2520%2520We%2520introduce%2520Garment3DGen%2520a%2520new%2520method%2520to%2520synthesize%25203D%2520garment%2520assets%2520from%2520a%250Abase%2520mesh%2520given%2520a%2520single%2520input%2520image%2520as%2520guidance.%2520Our%2520proposed%2520approach%2520allows%250Ausers%2520to%2520generate%25203D%2520textured%2520clothes%2520based%2520on%2520both%2520real%2520and%2520synthetic%2520images%252C%250Asuch%2520as%2520those%2520generated%2520by%2520text%2520prompts.%2520The%2520generated%2520assets%2520can%2520be%2520directly%250Adraped%2520and%2520simulated%2520on%2520human%2520bodies.%2520We%2520leverage%2520the%2520recent%2520progress%2520of%250Aimage-to-3D%2520diffusion%2520methods%2520to%2520generate%25203D%2520garment%2520geometries.%2520However%252C%2520since%250Athese%2520geometries%2520cannot%2520be%2520utilized%2520directly%2520for%2520downstream%2520tasks%252C%2520we%2520propose%250Ato%2520use%2520them%2520as%2520pseudo%2520ground-truth%2520and%2520set%2520up%2520a%2520mesh%2520deformation%2520optimization%250Aprocedure%2520that%2520deforms%2520a%2520base%2520template%2520mesh%2520to%2520match%2520the%2520generated%25203D%2520target.%250ACarefully%2520designed%2520losses%2520allow%2520the%2520base%2520mesh%2520to%2520freely%2520deform%2520towards%2520the%250Adesired%2520target%252C%2520yet%2520preserve%2520mesh%2520quality%2520and%2520topology%2520such%2520that%2520they%2520can%2520be%250Asimulated.%2520Finally%252C%2520we%2520generate%2520high-fidelity%2520texture%2520maps%2520that%2520are%2520globally%250Aand%2520locally%2520consistent%2520and%2520faithfully%2520capture%2520the%2520input%2520guidance%252C%2520allowing%2520us%250Ato%2520render%2520the%2520generated%25203D%2520assets.%2520With%2520Garment3DGen%2520users%2520can%2520generate%2520the%250Asimulation-ready%25203D%2520garment%2520of%2520their%2520choice%2520without%2520the%2520need%2520of%2520artist%250Aintervention.%2520We%2520present%2520a%2520plethora%2520of%2520quantitative%2520and%2520qualitative%2520comparisons%250Aon%2520various%2520assets%2520and%2520demonstrate%2520that%2520Garment3DGen%2520unlocks%2520key%2520applications%250Aranging%2520from%2520sketch-to-simulated%2520garments%2520or%2520interacting%2520with%2520the%2520garments%2520in%250AVR.%2520Code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18816v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Garment3DGen%3A%203D%20Garment%20Stylization%20and%20Texture%20Generation&entry.906535625=Nikolaos%20Sarafianos%20and%20Tuur%20Stuyck%20and%20Xiaoyu%20Xiang%20and%20Yilei%20Li%20and%20Jovan%20Popovic%20and%20Rakesh%20Ranjan&entry.1292438233=%20%20We%20introduce%20Garment3DGen%20a%20new%20method%20to%20synthesize%203D%20garment%20assets%20from%20a%0Abase%20mesh%20given%20a%20single%20input%20image%20as%20guidance.%20Our%20proposed%20approach%20allows%0Ausers%20to%20generate%203D%20textured%20clothes%20based%20on%20both%20real%20and%20synthetic%20images%2C%0Asuch%20as%20those%20generated%20by%20text%20prompts.%20The%20generated%20assets%20can%20be%20directly%0Adraped%20and%20simulated%20on%20human%20bodies.%20We%20leverage%20the%20recent%20progress%20of%0Aimage-to-3D%20diffusion%20methods%20to%20generate%203D%20garment%20geometries.%20However%2C%20since%0Athese%20geometries%20cannot%20be%20utilized%20directly%20for%20downstream%20tasks%2C%20we%20propose%0Ato%20use%20them%20as%20pseudo%20ground-truth%20and%20set%20up%20a%20mesh%20deformation%20optimization%0Aprocedure%20that%20deforms%20a%20base%20template%20mesh%20to%20match%20the%20generated%203D%20target.%0ACarefully%20designed%20losses%20allow%20the%20base%20mesh%20to%20freely%20deform%20towards%20the%0Adesired%20target%2C%20yet%20preserve%20mesh%20quality%20and%20topology%20such%20that%20they%20can%20be%0Asimulated.%20Finally%2C%20we%20generate%20high-fidelity%20texture%20maps%20that%20are%20globally%0Aand%20locally%20consistent%20and%20faithfully%20capture%20the%20input%20guidance%2C%20allowing%20us%0Ato%20render%20the%20generated%203D%20assets.%20With%20Garment3DGen%20users%20can%20generate%20the%0Asimulation-ready%203D%20garment%20of%20their%20choice%20without%20the%20need%20of%20artist%0Aintervention.%20We%20present%20a%20plethora%20of%20quantitative%20and%20qualitative%20comparisons%0Aon%20various%20assets%20and%20demonstrate%20that%20Garment3DGen%20unlocks%20key%20applications%0Aranging%20from%20sketch-to-simulated%20garments%20or%20interacting%20with%20the%20garments%20in%0AVR.%20Code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18816v3&entry.124074799=Read"},
{"title": "MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric\n  Guidance", "author": "Mengting Wei and Yante Li and Tuomas Varanka and Yan Jiang and Licai Sun and Guoying Zhao", "abstract": "  In this paper, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nenables precise extraction of detailed face geometry and motion features from\ndriving videos. Specifically, we enhance the latent diffusion model with rich\n3D expression and detailed pose information by incorporating depth maps, normal\nmaps, and rendering maps derived from FLAME sequences. A multi-layer face\nmovements fusion module with integrated self-attention mechanisms is used to\ncombine identity and motion latent features within the spatial domain. By\nutilizing the 3D face parametric model as motion guidance, our method enables\nparametric alignment of face identity between the reference image and the\nmotion captured from the driving video. Experimental results on benchmark\ndatasets show that our method excels at generating high-quality face animations\nwith precise expression and head pose variation modeling. In addition, it\ndemonstrates strong generalization performance on out-of-domain images. Code is\npublicly available at https://github.com/weimengting/MagicPortrait.\n", "link": "http://arxiv.org/abs/2504.21497v1", "date": "2025-04-30", "relevancy": 3.363, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7016}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6789}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicPortrait%3A%20Temporally%20Consistent%20Face%20Reenactment%20with%203D%20Geometric%0A%20%20Guidance&body=Title%3A%20MagicPortrait%3A%20Temporally%20Consistent%20Face%20Reenactment%20with%203D%20Geometric%0A%20%20Guidance%0AAuthor%3A%20Mengting%20Wei%20and%20Yante%20Li%20and%20Tuomas%20Varanka%20and%20Yan%20Jiang%20and%20Licai%20Sun%20and%20Guoying%20Zhao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20video%20face%20reenactment%20that%20integrates%0Aa%203D%20face%20parametric%20model%20into%20a%20latent%20diffusion%20framework%2C%20aiming%20to%20improve%0Ashape%20consistency%20and%20motion%20control%20in%20existing%20video-based%20face%20generation%0Aapproaches.%20Our%20approach%20employs%20the%20FLAME%20%28Faces%20Learned%20with%20an%20Articulated%0AModel%20and%20Expressions%29%20model%20as%20the%203D%20face%20parametric%20representation%2C%0Aproviding%20a%20unified%20framework%20for%20modeling%20face%20expressions%20and%20head%20pose.%20This%0Aenables%20precise%20extraction%20of%20detailed%20face%20geometry%20and%20motion%20features%20from%0Adriving%20videos.%20Specifically%2C%20we%20enhance%20the%20latent%20diffusion%20model%20with%20rich%0A3D%20expression%20and%20detailed%20pose%20information%20by%20incorporating%20depth%20maps%2C%20normal%0Amaps%2C%20and%20rendering%20maps%20derived%20from%20FLAME%20sequences.%20A%20multi-layer%20face%0Amovements%20fusion%20module%20with%20integrated%20self-attention%20mechanisms%20is%20used%20to%0Acombine%20identity%20and%20motion%20latent%20features%20within%20the%20spatial%20domain.%20By%0Autilizing%20the%203D%20face%20parametric%20model%20as%20motion%20guidance%2C%20our%20method%20enables%0Aparametric%20alignment%20of%20face%20identity%20between%20the%20reference%20image%20and%20the%0Amotion%20captured%20from%20the%20driving%20video.%20Experimental%20results%20on%20benchmark%0Adatasets%20show%20that%20our%20method%20excels%20at%20generating%20high-quality%20face%20animations%0Awith%20precise%20expression%20and%20head%20pose%20variation%20modeling.%20In%20addition%2C%20it%0Ademonstrates%20strong%20generalization%20performance%20on%20out-of-domain%20images.%20Code%20is%0Apublicly%20available%20at%20https%3A//github.com/weimengting/MagicPortrait.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicPortrait%253A%2520Temporally%2520Consistent%2520Face%2520Reenactment%2520with%25203D%2520Geometric%250A%2520%2520Guidance%26entry.906535625%3DMengting%2520Wei%2520and%2520Yante%2520Li%2520and%2520Tuomas%2520Varanka%2520and%2520Yan%2520Jiang%2520and%2520Licai%2520Sun%2520and%2520Guoying%2520Zhao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520for%2520video%2520face%2520reenactment%2520that%2520integrates%250Aa%25203D%2520face%2520parametric%2520model%2520into%2520a%2520latent%2520diffusion%2520framework%252C%2520aiming%2520to%2520improve%250Ashape%2520consistency%2520and%2520motion%2520control%2520in%2520existing%2520video-based%2520face%2520generation%250Aapproaches.%2520Our%2520approach%2520employs%2520the%2520FLAME%2520%2528Faces%2520Learned%2520with%2520an%2520Articulated%250AModel%2520and%2520Expressions%2529%2520model%2520as%2520the%25203D%2520face%2520parametric%2520representation%252C%250Aproviding%2520a%2520unified%2520framework%2520for%2520modeling%2520face%2520expressions%2520and%2520head%2520pose.%2520This%250Aenables%2520precise%2520extraction%2520of%2520detailed%2520face%2520geometry%2520and%2520motion%2520features%2520from%250Adriving%2520videos.%2520Specifically%252C%2520we%2520enhance%2520the%2520latent%2520diffusion%2520model%2520with%2520rich%250A3D%2520expression%2520and%2520detailed%2520pose%2520information%2520by%2520incorporating%2520depth%2520maps%252C%2520normal%250Amaps%252C%2520and%2520rendering%2520maps%2520derived%2520from%2520FLAME%2520sequences.%2520A%2520multi-layer%2520face%250Amovements%2520fusion%2520module%2520with%2520integrated%2520self-attention%2520mechanisms%2520is%2520used%2520to%250Acombine%2520identity%2520and%2520motion%2520latent%2520features%2520within%2520the%2520spatial%2520domain.%2520By%250Autilizing%2520the%25203D%2520face%2520parametric%2520model%2520as%2520motion%2520guidance%252C%2520our%2520method%2520enables%250Aparametric%2520alignment%2520of%2520face%2520identity%2520between%2520the%2520reference%2520image%2520and%2520the%250Amotion%2520captured%2520from%2520the%2520driving%2520video.%2520Experimental%2520results%2520on%2520benchmark%250Adatasets%2520show%2520that%2520our%2520method%2520excels%2520at%2520generating%2520high-quality%2520face%2520animations%250Awith%2520precise%2520expression%2520and%2520head%2520pose%2520variation%2520modeling.%2520In%2520addition%252C%2520it%250Ademonstrates%2520strong%2520generalization%2520performance%2520on%2520out-of-domain%2520images.%2520Code%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/weimengting/MagicPortrait.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicPortrait%3A%20Temporally%20Consistent%20Face%20Reenactment%20with%203D%20Geometric%0A%20%20Guidance&entry.906535625=Mengting%20Wei%20and%20Yante%20Li%20and%20Tuomas%20Varanka%20and%20Yan%20Jiang%20and%20Licai%20Sun%20and%20Guoying%20Zhao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20video%20face%20reenactment%20that%20integrates%0Aa%203D%20face%20parametric%20model%20into%20a%20latent%20diffusion%20framework%2C%20aiming%20to%20improve%0Ashape%20consistency%20and%20motion%20control%20in%20existing%20video-based%20face%20generation%0Aapproaches.%20Our%20approach%20employs%20the%20FLAME%20%28Faces%20Learned%20with%20an%20Articulated%0AModel%20and%20Expressions%29%20model%20as%20the%203D%20face%20parametric%20representation%2C%0Aproviding%20a%20unified%20framework%20for%20modeling%20face%20expressions%20and%20head%20pose.%20This%0Aenables%20precise%20extraction%20of%20detailed%20face%20geometry%20and%20motion%20features%20from%0Adriving%20videos.%20Specifically%2C%20we%20enhance%20the%20latent%20diffusion%20model%20with%20rich%0A3D%20expression%20and%20detailed%20pose%20information%20by%20incorporating%20depth%20maps%2C%20normal%0Amaps%2C%20and%20rendering%20maps%20derived%20from%20FLAME%20sequences.%20A%20multi-layer%20face%0Amovements%20fusion%20module%20with%20integrated%20self-attention%20mechanisms%20is%20used%20to%0Acombine%20identity%20and%20motion%20latent%20features%20within%20the%20spatial%20domain.%20By%0Autilizing%20the%203D%20face%20parametric%20model%20as%20motion%20guidance%2C%20our%20method%20enables%0Aparametric%20alignment%20of%20face%20identity%20between%20the%20reference%20image%20and%20the%0Amotion%20captured%20from%20the%20driving%20video.%20Experimental%20results%20on%20benchmark%0Adatasets%20show%20that%20our%20method%20excels%20at%20generating%20high-quality%20face%20animations%0Awith%20precise%20expression%20and%20head%20pose%20variation%20modeling.%20In%20addition%2C%20it%0Ademonstrates%20strong%20generalization%20performance%20on%20out-of-domain%20images.%20Code%20is%0Apublicly%20available%20at%20https%3A//github.com/weimengting/MagicPortrait.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21497v1&entry.124074799=Read"},
{"title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry\n  Monocular Video", "author": "Minh-Quan Viet Bui and Jongmin Park and Juan Luis Gonzalez Bello and Jaeho Moon and Jihyong Oh and Munchurl Kim", "abstract": "  We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\nframework capable of reconstructing sharp and high-quality novel\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\nmotion blur in casually captured videos, resulting in significant degradation\nof rendering quality. While recent approaches address motion-blurred inputs for\nNVS, they primarily focus on static scene reconstruction and lack dedicated\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\neffective latent camera trajectory estimation, improving global camera motion\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\nconsistency of unseen latent timestamps and robust motion decomposition of\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\nand real-world blurry videos show that our MoBGS significantly outperforms the\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\nstate-of-the-art performance for dynamic NVS under motion blur.\n", "link": "http://arxiv.org/abs/2504.15122v2", "date": "2025-04-30", "relevancy": 3.2167, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.684}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6447}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%0A%20%20Monocular%20Video&body=Title%3A%20MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%0A%20%20Monocular%20Video%0AAuthor%3A%20Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Juan%20Luis%20Gonzalez%20Bello%20and%20Jaeho%20Moon%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim%0AAbstract%3A%20%20%20We%20present%20MoBGS%2C%20a%20novel%20deblurring%20dynamic%203D%20Gaussian%20Splatting%20%283DGS%29%0Aframework%20capable%20of%20reconstructing%20sharp%20and%20high-quality%20novel%0Aspatio-temporal%20views%20from%20blurry%20monocular%20videos%20in%20an%20end-to-end%20manner.%0AExisting%20dynamic%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20highly%20sensitive%20to%0Amotion%20blur%20in%20casually%20captured%20videos%2C%20resulting%20in%20significant%20degradation%0Aof%20rendering%20quality.%20While%20recent%20approaches%20address%20motion-blurred%20inputs%20for%0ANVS%2C%20they%20primarily%20focus%20on%20static%20scene%20reconstruction%20and%20lack%20dedicated%0Amotion%20modeling%20for%20dynamic%20objects.%20To%20overcome%20these%20limitations%2C%20our%20MoBGS%0Aintroduces%20a%20novel%20Blur-adaptive%20Latent%20Camera%20Estimation%20%28BLCE%29%20method%20for%0Aeffective%20latent%20camera%20trajectory%20estimation%2C%20improving%20global%20camera%20motion%0Adeblurring.%20In%20addition%2C%20we%20propose%20a%20physically-inspired%20Latent%20Camera-induced%0AExposure%20Estimation%20%28LCEE%29%20method%20to%20ensure%20consistent%20deblurring%20of%20both%0Aglobal%20camera%20and%20local%20object%20motion.%20Our%20MoBGS%20framework%20ensures%20the%20temporal%0Aconsistency%20of%20unseen%20latent%20timestamps%20and%20robust%20motion%20decomposition%20of%0Astatic%20and%20dynamic%20regions.%20Extensive%20experiments%20on%20the%20Stereo%20Blur%20dataset%0Aand%20real-world%20blurry%20videos%20show%20that%20our%20MoBGS%20significantly%20outperforms%20the%0Avery%20recent%20advanced%20methods%20%28DyBluRF%20and%20Deblur4DGS%29%2C%20achieving%0Astate-of-the-art%20performance%20for%20dynamic%20NVS%20under%20motion%20blur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15122v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoBGS%253A%2520Motion%2520Deblurring%2520Dynamic%25203D%2520Gaussian%2520Splatting%2520for%2520Blurry%250A%2520%2520Monocular%2520Video%26entry.906535625%3DMinh-Quan%2520Viet%2520Bui%2520and%2520Jongmin%2520Park%2520and%2520Juan%2520Luis%2520Gonzalez%2520Bello%2520and%2520Jaeho%2520Moon%2520and%2520Jihyong%2520Oh%2520and%2520Munchurl%2520Kim%26entry.1292438233%3D%2520%2520We%2520present%2520MoBGS%252C%2520a%2520novel%2520deblurring%2520dynamic%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Aframework%2520capable%2520of%2520reconstructing%2520sharp%2520and%2520high-quality%2520novel%250Aspatio-temporal%2520views%2520from%2520blurry%2520monocular%2520videos%2520in%2520an%2520end-to-end%2520manner.%250AExisting%2520dynamic%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520methods%2520are%2520highly%2520sensitive%2520to%250Amotion%2520blur%2520in%2520casually%2520captured%2520videos%252C%2520resulting%2520in%2520significant%2520degradation%250Aof%2520rendering%2520quality.%2520While%2520recent%2520approaches%2520address%2520motion-blurred%2520inputs%2520for%250ANVS%252C%2520they%2520primarily%2520focus%2520on%2520static%2520scene%2520reconstruction%2520and%2520lack%2520dedicated%250Amotion%2520modeling%2520for%2520dynamic%2520objects.%2520To%2520overcome%2520these%2520limitations%252C%2520our%2520MoBGS%250Aintroduces%2520a%2520novel%2520Blur-adaptive%2520Latent%2520Camera%2520Estimation%2520%2528BLCE%2529%2520method%2520for%250Aeffective%2520latent%2520camera%2520trajectory%2520estimation%252C%2520improving%2520global%2520camera%2520motion%250Adeblurring.%2520In%2520addition%252C%2520we%2520propose%2520a%2520physically-inspired%2520Latent%2520Camera-induced%250AExposure%2520Estimation%2520%2528LCEE%2529%2520method%2520to%2520ensure%2520consistent%2520deblurring%2520of%2520both%250Aglobal%2520camera%2520and%2520local%2520object%2520motion.%2520Our%2520MoBGS%2520framework%2520ensures%2520the%2520temporal%250Aconsistency%2520of%2520unseen%2520latent%2520timestamps%2520and%2520robust%2520motion%2520decomposition%2520of%250Astatic%2520and%2520dynamic%2520regions.%2520Extensive%2520experiments%2520on%2520the%2520Stereo%2520Blur%2520dataset%250Aand%2520real-world%2520blurry%2520videos%2520show%2520that%2520our%2520MoBGS%2520significantly%2520outperforms%2520the%250Avery%2520recent%2520advanced%2520methods%2520%2528DyBluRF%2520and%2520Deblur4DGS%2529%252C%2520achieving%250Astate-of-the-art%2520performance%2520for%2520dynamic%2520NVS%2520under%2520motion%2520blur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15122v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%0A%20%20Monocular%20Video&entry.906535625=Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Juan%20Luis%20Gonzalez%20Bello%20and%20Jaeho%20Moon%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim&entry.1292438233=%20%20We%20present%20MoBGS%2C%20a%20novel%20deblurring%20dynamic%203D%20Gaussian%20Splatting%20%283DGS%29%0Aframework%20capable%20of%20reconstructing%20sharp%20and%20high-quality%20novel%0Aspatio-temporal%20views%20from%20blurry%20monocular%20videos%20in%20an%20end-to-end%20manner.%0AExisting%20dynamic%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20highly%20sensitive%20to%0Amotion%20blur%20in%20casually%20captured%20videos%2C%20resulting%20in%20significant%20degradation%0Aof%20rendering%20quality.%20While%20recent%20approaches%20address%20motion-blurred%20inputs%20for%0ANVS%2C%20they%20primarily%20focus%20on%20static%20scene%20reconstruction%20and%20lack%20dedicated%0Amotion%20modeling%20for%20dynamic%20objects.%20To%20overcome%20these%20limitations%2C%20our%20MoBGS%0Aintroduces%20a%20novel%20Blur-adaptive%20Latent%20Camera%20Estimation%20%28BLCE%29%20method%20for%0Aeffective%20latent%20camera%20trajectory%20estimation%2C%20improving%20global%20camera%20motion%0Adeblurring.%20In%20addition%2C%20we%20propose%20a%20physically-inspired%20Latent%20Camera-induced%0AExposure%20Estimation%20%28LCEE%29%20method%20to%20ensure%20consistent%20deblurring%20of%20both%0Aglobal%20camera%20and%20local%20object%20motion.%20Our%20MoBGS%20framework%20ensures%20the%20temporal%0Aconsistency%20of%20unseen%20latent%20timestamps%20and%20robust%20motion%20decomposition%20of%0Astatic%20and%20dynamic%20regions.%20Extensive%20experiments%20on%20the%20Stereo%20Blur%20dataset%0Aand%20real-world%20blurry%20videos%20show%20that%20our%20MoBGS%20significantly%20outperforms%20the%0Avery%20recent%20advanced%20methods%20%28DyBluRF%20and%20Deblur4DGS%29%2C%20achieving%0Astate-of-the-art%20performance%20for%20dynamic%20NVS%20under%20motion%20blur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15122v2&entry.124074799=Read"},
{"title": "3D Stylization via Large Reconstruction Model", "author": "Ipek Oztas and Duygu Ceylan and Aysegul Dundar", "abstract": "  With the growing success of text or image guided 3D generators, users demand\nmore control over the generation process, appearance stylization being one of\nthem. Given a reference image, this requires adapting the appearance of a\ngenerated 3D asset to reflect the visual style of the reference while\nmaintaining visual consistency from multiple viewpoints. To tackle this\nproblem, we draw inspiration from the success of 2D stylization methods that\nleverage the attention mechanisms in large image generation models to capture\nand transfer visual style. In particular, we probe if large reconstruction\nmodels, commonly used in the context of 3D generation, has a similar\ncapability. We discover that the certain attention blocks in these models\ncapture the appearance specific features. By injecting features from a visual\nstyle image to such blocks, we develop a simple yet effective 3D appearance\nstylization method. Our method does not require training or test time\noptimization. Through both quantitative and qualitative evaluations, we\ndemonstrate that our approach achieves superior results in terms of 3D\nappearance stylization, significantly improving efficiency while maintaining\nhigh-quality visual outcomes.\n", "link": "http://arxiv.org/abs/2504.21836v1", "date": "2025-04-30", "relevancy": 3.0976, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6244}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6244}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Stylization%20via%20Large%20Reconstruction%20Model&body=Title%3A%203D%20Stylization%20via%20Large%20Reconstruction%20Model%0AAuthor%3A%20Ipek%20Oztas%20and%20Duygu%20Ceylan%20and%20Aysegul%20Dundar%0AAbstract%3A%20%20%20With%20the%20growing%20success%20of%20text%20or%20image%20guided%203D%20generators%2C%20users%20demand%0Amore%20control%20over%20the%20generation%20process%2C%20appearance%20stylization%20being%20one%20of%0Athem.%20Given%20a%20reference%20image%2C%20this%20requires%20adapting%20the%20appearance%20of%20a%0Agenerated%203D%20asset%20to%20reflect%20the%20visual%20style%20of%20the%20reference%20while%0Amaintaining%20visual%20consistency%20from%20multiple%20viewpoints.%20To%20tackle%20this%0Aproblem%2C%20we%20draw%20inspiration%20from%20the%20success%20of%202D%20stylization%20methods%20that%0Aleverage%20the%20attention%20mechanisms%20in%20large%20image%20generation%20models%20to%20capture%0Aand%20transfer%20visual%20style.%20In%20particular%2C%20we%20probe%20if%20large%20reconstruction%0Amodels%2C%20commonly%20used%20in%20the%20context%20of%203D%20generation%2C%20has%20a%20similar%0Acapability.%20We%20discover%20that%20the%20certain%20attention%20blocks%20in%20these%20models%0Acapture%20the%20appearance%20specific%20features.%20By%20injecting%20features%20from%20a%20visual%0Astyle%20image%20to%20such%20blocks%2C%20we%20develop%20a%20simple%20yet%20effective%203D%20appearance%0Astylization%20method.%20Our%20method%20does%20not%20require%20training%20or%20test%20time%0Aoptimization.%20Through%20both%20quantitative%20and%20qualitative%20evaluations%2C%20we%0Ademonstrate%20that%20our%20approach%20achieves%20superior%20results%20in%20terms%20of%203D%0Aappearance%20stylization%2C%20significantly%20improving%20efficiency%20while%20maintaining%0Ahigh-quality%20visual%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Stylization%2520via%2520Large%2520Reconstruction%2520Model%26entry.906535625%3DIpek%2520Oztas%2520and%2520Duygu%2520Ceylan%2520and%2520Aysegul%2520Dundar%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520success%2520of%2520text%2520or%2520image%2520guided%25203D%2520generators%252C%2520users%2520demand%250Amore%2520control%2520over%2520the%2520generation%2520process%252C%2520appearance%2520stylization%2520being%2520one%2520of%250Athem.%2520Given%2520a%2520reference%2520image%252C%2520this%2520requires%2520adapting%2520the%2520appearance%2520of%2520a%250Agenerated%25203D%2520asset%2520to%2520reflect%2520the%2520visual%2520style%2520of%2520the%2520reference%2520while%250Amaintaining%2520visual%2520consistency%2520from%2520multiple%2520viewpoints.%2520To%2520tackle%2520this%250Aproblem%252C%2520we%2520draw%2520inspiration%2520from%2520the%2520success%2520of%25202D%2520stylization%2520methods%2520that%250Aleverage%2520the%2520attention%2520mechanisms%2520in%2520large%2520image%2520generation%2520models%2520to%2520capture%250Aand%2520transfer%2520visual%2520style.%2520In%2520particular%252C%2520we%2520probe%2520if%2520large%2520reconstruction%250Amodels%252C%2520commonly%2520used%2520in%2520the%2520context%2520of%25203D%2520generation%252C%2520has%2520a%2520similar%250Acapability.%2520We%2520discover%2520that%2520the%2520certain%2520attention%2520blocks%2520in%2520these%2520models%250Acapture%2520the%2520appearance%2520specific%2520features.%2520By%2520injecting%2520features%2520from%2520a%2520visual%250Astyle%2520image%2520to%2520such%2520blocks%252C%2520we%2520develop%2520a%2520simple%2520yet%2520effective%25203D%2520appearance%250Astylization%2520method.%2520Our%2520method%2520does%2520not%2520require%2520training%2520or%2520test%2520time%250Aoptimization.%2520Through%2520both%2520quantitative%2520and%2520qualitative%2520evaluations%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520superior%2520results%2520in%2520terms%2520of%25203D%250Aappearance%2520stylization%252C%2520significantly%2520improving%2520efficiency%2520while%2520maintaining%250Ahigh-quality%2520visual%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Stylization%20via%20Large%20Reconstruction%20Model&entry.906535625=Ipek%20Oztas%20and%20Duygu%20Ceylan%20and%20Aysegul%20Dundar&entry.1292438233=%20%20With%20the%20growing%20success%20of%20text%20or%20image%20guided%203D%20generators%2C%20users%20demand%0Amore%20control%20over%20the%20generation%20process%2C%20appearance%20stylization%20being%20one%20of%0Athem.%20Given%20a%20reference%20image%2C%20this%20requires%20adapting%20the%20appearance%20of%20a%0Agenerated%203D%20asset%20to%20reflect%20the%20visual%20style%20of%20the%20reference%20while%0Amaintaining%20visual%20consistency%20from%20multiple%20viewpoints.%20To%20tackle%20this%0Aproblem%2C%20we%20draw%20inspiration%20from%20the%20success%20of%202D%20stylization%20methods%20that%0Aleverage%20the%20attention%20mechanisms%20in%20large%20image%20generation%20models%20to%20capture%0Aand%20transfer%20visual%20style.%20In%20particular%2C%20we%20probe%20if%20large%20reconstruction%0Amodels%2C%20commonly%20used%20in%20the%20context%20of%203D%20generation%2C%20has%20a%20similar%0Acapability.%20We%20discover%20that%20the%20certain%20attention%20blocks%20in%20these%20models%0Acapture%20the%20appearance%20specific%20features.%20By%20injecting%20features%20from%20a%20visual%0Astyle%20image%20to%20such%20blocks%2C%20we%20develop%20a%20simple%20yet%20effective%203D%20appearance%0Astylization%20method.%20Our%20method%20does%20not%20require%20training%20or%20test%20time%0Aoptimization.%20Through%20both%20quantitative%20and%20qualitative%20evaluations%2C%20we%0Ademonstrate%20that%20our%20approach%20achieves%20superior%20results%20in%20terms%20of%203D%0Aappearance%20stylization%2C%20significantly%20improving%20efficiency%20while%20maintaining%0Ahigh-quality%20visual%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21836v1&entry.124074799=Read"},
{"title": "A Survey of Interactive Generative Video", "author": "Jiwen Yu and Yiran Qin and Haoxuan Che and Quande Liu and Xintao Wang and Pengfei Wan and Di Zhang and Kun Gai and Hao Chen and Xihui Liu", "abstract": "  Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.\n", "link": "http://arxiv.org/abs/2504.21853v1", "date": "2025-04-30", "relevancy": 3.0835, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6462}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6239}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Interactive%20Generative%20Video&body=Title%3A%20A%20Survey%20of%20Interactive%20Generative%20Video%0AAuthor%3A%20Jiwen%20Yu%20and%20Yiran%20Qin%20and%20Haoxuan%20Che%20and%20Quande%20Liu%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Kun%20Gai%20and%20Hao%20Chen%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Interactive%20Generative%20Video%20%28IGV%29%20has%20emerged%20as%20a%20crucial%20technology%20in%0Aresponse%20to%20the%20growing%20demand%20for%20high-quality%2C%20interactive%20video%20content%0Aacross%20various%20domains.%20In%20this%20paper%2C%20we%20define%20IGV%20as%20a%20technology%20that%0Acombines%20generative%20capabilities%20to%20produce%20diverse%20high-quality%20video%20content%0Awith%20interactive%20features%20that%20enable%20user%20engagement%20through%20control%20signals%0Aand%20responsive%20feedback.%20We%20survey%20the%20current%20landscape%20of%20IGV%20applications%2C%0Afocusing%20on%20three%20major%20domains%3A%201%29%20gaming%2C%20where%20IGV%20enables%20infinite%0Aexploration%20in%20virtual%20worlds%3B%202%29%20embodied%20AI%2C%20where%20IGV%20serves%20as%20a%0Aphysics-aware%20environment%20synthesizer%20for%20training%20agents%20in%20multimodal%0Ainteraction%20with%20dynamically%20evolving%20scenes%3B%20and%203%29%20autonomous%20driving%2C%20where%0AIGV%20provides%20closed-loop%20simulation%20capabilities%20for%20safety-critical%20testing%0Aand%20validation.%20To%20guide%20future%20development%2C%20we%20propose%20a%20comprehensive%0Aframework%20that%20decomposes%20an%20ideal%20IGV%20system%20into%20five%20essential%20modules%3A%0AGeneration%2C%20Control%2C%20Memory%2C%20Dynamics%2C%20and%20Intelligence.%20Furthermore%2C%20we%0Asystematically%20analyze%20the%20technical%20challenges%20and%20future%20directions%20in%0Arealizing%20each%20component%20for%20an%20ideal%20IGV%20system%2C%20such%20as%20achieving%20real-time%0Ageneration%2C%20enabling%20open-domain%20control%2C%20maintaining%20long-term%20coherence%2C%0Asimulating%20accurate%20physics%2C%20and%20integrating%20causal%20reasoning.%20We%20believe%20that%0Athis%20systematic%20analysis%20will%20facilitate%20future%20research%20and%20development%20in%20the%0Afield%20of%20IGV%2C%20ultimately%20advancing%20the%20technology%20toward%20more%20sophisticated%20and%0Apractical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Interactive%2520Generative%2520Video%26entry.906535625%3DJiwen%2520Yu%2520and%2520Yiran%2520Qin%2520and%2520Haoxuan%2520Che%2520and%2520Quande%2520Liu%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Kun%2520Gai%2520and%2520Hao%2520Chen%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Interactive%2520Generative%2520Video%2520%2528IGV%2529%2520has%2520emerged%2520as%2520a%2520crucial%2520technology%2520in%250Aresponse%2520to%2520the%2520growing%2520demand%2520for%2520high-quality%252C%2520interactive%2520video%2520content%250Aacross%2520various%2520domains.%2520In%2520this%2520paper%252C%2520we%2520define%2520IGV%2520as%2520a%2520technology%2520that%250Acombines%2520generative%2520capabilities%2520to%2520produce%2520diverse%2520high-quality%2520video%2520content%250Awith%2520interactive%2520features%2520that%2520enable%2520user%2520engagement%2520through%2520control%2520signals%250Aand%2520responsive%2520feedback.%2520We%2520survey%2520the%2520current%2520landscape%2520of%2520IGV%2520applications%252C%250Afocusing%2520on%2520three%2520major%2520domains%253A%25201%2529%2520gaming%252C%2520where%2520IGV%2520enables%2520infinite%250Aexploration%2520in%2520virtual%2520worlds%253B%25202%2529%2520embodied%2520AI%252C%2520where%2520IGV%2520serves%2520as%2520a%250Aphysics-aware%2520environment%2520synthesizer%2520for%2520training%2520agents%2520in%2520multimodal%250Ainteraction%2520with%2520dynamically%2520evolving%2520scenes%253B%2520and%25203%2529%2520autonomous%2520driving%252C%2520where%250AIGV%2520provides%2520closed-loop%2520simulation%2520capabilities%2520for%2520safety-critical%2520testing%250Aand%2520validation.%2520To%2520guide%2520future%2520development%252C%2520we%2520propose%2520a%2520comprehensive%250Aframework%2520that%2520decomposes%2520an%2520ideal%2520IGV%2520system%2520into%2520five%2520essential%2520modules%253A%250AGeneration%252C%2520Control%252C%2520Memory%252C%2520Dynamics%252C%2520and%2520Intelligence.%2520Furthermore%252C%2520we%250Asystematically%2520analyze%2520the%2520technical%2520challenges%2520and%2520future%2520directions%2520in%250Arealizing%2520each%2520component%2520for%2520an%2520ideal%2520IGV%2520system%252C%2520such%2520as%2520achieving%2520real-time%250Ageneration%252C%2520enabling%2520open-domain%2520control%252C%2520maintaining%2520long-term%2520coherence%252C%250Asimulating%2520accurate%2520physics%252C%2520and%2520integrating%2520causal%2520reasoning.%2520We%2520believe%2520that%250Athis%2520systematic%2520analysis%2520will%2520facilitate%2520future%2520research%2520and%2520development%2520in%2520the%250Afield%2520of%2520IGV%252C%2520ultimately%2520advancing%2520the%2520technology%2520toward%2520more%2520sophisticated%2520and%250Apractical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Interactive%20Generative%20Video&entry.906535625=Jiwen%20Yu%20and%20Yiran%20Qin%20and%20Haoxuan%20Che%20and%20Quande%20Liu%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Kun%20Gai%20and%20Hao%20Chen%20and%20Xihui%20Liu&entry.1292438233=%20%20Interactive%20Generative%20Video%20%28IGV%29%20has%20emerged%20as%20a%20crucial%20technology%20in%0Aresponse%20to%20the%20growing%20demand%20for%20high-quality%2C%20interactive%20video%20content%0Aacross%20various%20domains.%20In%20this%20paper%2C%20we%20define%20IGV%20as%20a%20technology%20that%0Acombines%20generative%20capabilities%20to%20produce%20diverse%20high-quality%20video%20content%0Awith%20interactive%20features%20that%20enable%20user%20engagement%20through%20control%20signals%0Aand%20responsive%20feedback.%20We%20survey%20the%20current%20landscape%20of%20IGV%20applications%2C%0Afocusing%20on%20three%20major%20domains%3A%201%29%20gaming%2C%20where%20IGV%20enables%20infinite%0Aexploration%20in%20virtual%20worlds%3B%202%29%20embodied%20AI%2C%20where%20IGV%20serves%20as%20a%0Aphysics-aware%20environment%20synthesizer%20for%20training%20agents%20in%20multimodal%0Ainteraction%20with%20dynamically%20evolving%20scenes%3B%20and%203%29%20autonomous%20driving%2C%20where%0AIGV%20provides%20closed-loop%20simulation%20capabilities%20for%20safety-critical%20testing%0Aand%20validation.%20To%20guide%20future%20development%2C%20we%20propose%20a%20comprehensive%0Aframework%20that%20decomposes%20an%20ideal%20IGV%20system%20into%20five%20essential%20modules%3A%0AGeneration%2C%20Control%2C%20Memory%2C%20Dynamics%2C%20and%20Intelligence.%20Furthermore%2C%20we%0Asystematically%20analyze%20the%20technical%20challenges%20and%20future%20directions%20in%0Arealizing%20each%20component%20for%20an%20ideal%20IGV%20system%2C%20such%20as%20achieving%20real-time%0Ageneration%2C%20enabling%20open-domain%20control%2C%20maintaining%20long-term%20coherence%2C%0Asimulating%20accurate%20physics%2C%20and%20integrating%20causal%20reasoning.%20We%20believe%20that%0Athis%20systematic%20analysis%20will%20facilitate%20future%20research%20and%20development%20in%20the%0Afield%20of%20IGV%2C%20ultimately%20advancing%20the%20technology%20toward%20more%20sophisticated%20and%0Apractical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21853v1&entry.124074799=Read"},
{"title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common\n  Objects in Neural Feature Space", "author": "Leonhard Sommer and Olaf D\u00fcnkel and Christian Theobalt and Adam Kortylewski", "abstract": "  3D morphable models (3DMMs) are a powerful tool to represent the possible\nshapes and appearances of an object category. Given a single test image, 3DMMs\ncan be used to solve various tasks, such as predicting the 3D shape, pose,\nsemantic correspondence, and instance segmentation of an object. Unfortunately,\n3DMMs are only available for very few object categories that are of particular\ninterest, like faces or human bodies, as they require a demanding 3D data\nacquisition and category-specific training process. In contrast, we introduce a\nnew method, Common3D, that learns 3DMMs of common objects in a fully\nself-supervised manner from a collection of object-centric videos. For this\npurpose, our model represents objects as a learned 3D template mesh and a\ndeformation field that is parameterized as an image-conditioned neural network.\nDifferent from prior works, Common3D represents the object appearance with\nneural features instead of RGB colors, which enables the learning of more\ngeneralizable representations through an abstraction from pixel intensities.\nImportantly, we train the appearance features using a contrastive objective by\nexploiting the correspondences defined through the deformable template mesh.\nThis leads to higher quality correspondence features compared to related works\nand a significantly improved model performance at estimating 3D object pose and\nsemantic correspondence. Common3D is the first completely self-supervised\nmethod that can solve various vision tasks in a zero-shot manner.\n", "link": "http://arxiv.org/abs/2504.21749v1", "date": "2025-04-30", "relevancy": 3.0676, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6396}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6015}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Common3D%3A%20Self-Supervised%20Learning%20of%203D%20Morphable%20Models%20for%20Common%0A%20%20Objects%20in%20Neural%20Feature%20Space&body=Title%3A%20Common3D%3A%20Self-Supervised%20Learning%20of%203D%20Morphable%20Models%20for%20Common%0A%20%20Objects%20in%20Neural%20Feature%20Space%0AAuthor%3A%20Leonhard%20Sommer%20and%20Olaf%20D%C3%BCnkel%20and%20Christian%20Theobalt%20and%20Adam%20Kortylewski%0AAbstract%3A%20%20%203D%20morphable%20models%20%283DMMs%29%20are%20a%20powerful%20tool%20to%20represent%20the%20possible%0Ashapes%20and%20appearances%20of%20an%20object%20category.%20Given%20a%20single%20test%20image%2C%203DMMs%0Acan%20be%20used%20to%20solve%20various%20tasks%2C%20such%20as%20predicting%20the%203D%20shape%2C%20pose%2C%0Asemantic%20correspondence%2C%20and%20instance%20segmentation%20of%20an%20object.%20Unfortunately%2C%0A3DMMs%20are%20only%20available%20for%20very%20few%20object%20categories%20that%20are%20of%20particular%0Ainterest%2C%20like%20faces%20or%20human%20bodies%2C%20as%20they%20require%20a%20demanding%203D%20data%0Aacquisition%20and%20category-specific%20training%20process.%20In%20contrast%2C%20we%20introduce%20a%0Anew%20method%2C%20Common3D%2C%20that%20learns%203DMMs%20of%20common%20objects%20in%20a%20fully%0Aself-supervised%20manner%20from%20a%20collection%20of%20object-centric%20videos.%20For%20this%0Apurpose%2C%20our%20model%20represents%20objects%20as%20a%20learned%203D%20template%20mesh%20and%20a%0Adeformation%20field%20that%20is%20parameterized%20as%20an%20image-conditioned%20neural%20network.%0ADifferent%20from%20prior%20works%2C%20Common3D%20represents%20the%20object%20appearance%20with%0Aneural%20features%20instead%20of%20RGB%20colors%2C%20which%20enables%20the%20learning%20of%20more%0Ageneralizable%20representations%20through%20an%20abstraction%20from%20pixel%20intensities.%0AImportantly%2C%20we%20train%20the%20appearance%20features%20using%20a%20contrastive%20objective%20by%0Aexploiting%20the%20correspondences%20defined%20through%20the%20deformable%20template%20mesh.%0AThis%20leads%20to%20higher%20quality%20correspondence%20features%20compared%20to%20related%20works%0Aand%20a%20significantly%20improved%20model%20performance%20at%20estimating%203D%20object%20pose%20and%0Asemantic%20correspondence.%20Common3D%20is%20the%20first%20completely%20self-supervised%0Amethod%20that%20can%20solve%20various%20vision%20tasks%20in%20a%20zero-shot%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommon3D%253A%2520Self-Supervised%2520Learning%2520of%25203D%2520Morphable%2520Models%2520for%2520Common%250A%2520%2520Objects%2520in%2520Neural%2520Feature%2520Space%26entry.906535625%3DLeonhard%2520Sommer%2520and%2520Olaf%2520D%25C3%25BCnkel%2520and%2520Christian%2520Theobalt%2520and%2520Adam%2520Kortylewski%26entry.1292438233%3D%2520%25203D%2520morphable%2520models%2520%25283DMMs%2529%2520are%2520a%2520powerful%2520tool%2520to%2520represent%2520the%2520possible%250Ashapes%2520and%2520appearances%2520of%2520an%2520object%2520category.%2520Given%2520a%2520single%2520test%2520image%252C%25203DMMs%250Acan%2520be%2520used%2520to%2520solve%2520various%2520tasks%252C%2520such%2520as%2520predicting%2520the%25203D%2520shape%252C%2520pose%252C%250Asemantic%2520correspondence%252C%2520and%2520instance%2520segmentation%2520of%2520an%2520object.%2520Unfortunately%252C%250A3DMMs%2520are%2520only%2520available%2520for%2520very%2520few%2520object%2520categories%2520that%2520are%2520of%2520particular%250Ainterest%252C%2520like%2520faces%2520or%2520human%2520bodies%252C%2520as%2520they%2520require%2520a%2520demanding%25203D%2520data%250Aacquisition%2520and%2520category-specific%2520training%2520process.%2520In%2520contrast%252C%2520we%2520introduce%2520a%250Anew%2520method%252C%2520Common3D%252C%2520that%2520learns%25203DMMs%2520of%2520common%2520objects%2520in%2520a%2520fully%250Aself-supervised%2520manner%2520from%2520a%2520collection%2520of%2520object-centric%2520videos.%2520For%2520this%250Apurpose%252C%2520our%2520model%2520represents%2520objects%2520as%2520a%2520learned%25203D%2520template%2520mesh%2520and%2520a%250Adeformation%2520field%2520that%2520is%2520parameterized%2520as%2520an%2520image-conditioned%2520neural%2520network.%250ADifferent%2520from%2520prior%2520works%252C%2520Common3D%2520represents%2520the%2520object%2520appearance%2520with%250Aneural%2520features%2520instead%2520of%2520RGB%2520colors%252C%2520which%2520enables%2520the%2520learning%2520of%2520more%250Ageneralizable%2520representations%2520through%2520an%2520abstraction%2520from%2520pixel%2520intensities.%250AImportantly%252C%2520we%2520train%2520the%2520appearance%2520features%2520using%2520a%2520contrastive%2520objective%2520by%250Aexploiting%2520the%2520correspondences%2520defined%2520through%2520the%2520deformable%2520template%2520mesh.%250AThis%2520leads%2520to%2520higher%2520quality%2520correspondence%2520features%2520compared%2520to%2520related%2520works%250Aand%2520a%2520significantly%2520improved%2520model%2520performance%2520at%2520estimating%25203D%2520object%2520pose%2520and%250Asemantic%2520correspondence.%2520Common3D%2520is%2520the%2520first%2520completely%2520self-supervised%250Amethod%2520that%2520can%2520solve%2520various%2520vision%2520tasks%2520in%2520a%2520zero-shot%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Common3D%3A%20Self-Supervised%20Learning%20of%203D%20Morphable%20Models%20for%20Common%0A%20%20Objects%20in%20Neural%20Feature%20Space&entry.906535625=Leonhard%20Sommer%20and%20Olaf%20D%C3%BCnkel%20and%20Christian%20Theobalt%20and%20Adam%20Kortylewski&entry.1292438233=%20%203D%20morphable%20models%20%283DMMs%29%20are%20a%20powerful%20tool%20to%20represent%20the%20possible%0Ashapes%20and%20appearances%20of%20an%20object%20category.%20Given%20a%20single%20test%20image%2C%203DMMs%0Acan%20be%20used%20to%20solve%20various%20tasks%2C%20such%20as%20predicting%20the%203D%20shape%2C%20pose%2C%0Asemantic%20correspondence%2C%20and%20instance%20segmentation%20of%20an%20object.%20Unfortunately%2C%0A3DMMs%20are%20only%20available%20for%20very%20few%20object%20categories%20that%20are%20of%20particular%0Ainterest%2C%20like%20faces%20or%20human%20bodies%2C%20as%20they%20require%20a%20demanding%203D%20data%0Aacquisition%20and%20category-specific%20training%20process.%20In%20contrast%2C%20we%20introduce%20a%0Anew%20method%2C%20Common3D%2C%20that%20learns%203DMMs%20of%20common%20objects%20in%20a%20fully%0Aself-supervised%20manner%20from%20a%20collection%20of%20object-centric%20videos.%20For%20this%0Apurpose%2C%20our%20model%20represents%20objects%20as%20a%20learned%203D%20template%20mesh%20and%20a%0Adeformation%20field%20that%20is%20parameterized%20as%20an%20image-conditioned%20neural%20network.%0ADifferent%20from%20prior%20works%2C%20Common3D%20represents%20the%20object%20appearance%20with%0Aneural%20features%20instead%20of%20RGB%20colors%2C%20which%20enables%20the%20learning%20of%20more%0Ageneralizable%20representations%20through%20an%20abstraction%20from%20pixel%20intensities.%0AImportantly%2C%20we%20train%20the%20appearance%20features%20using%20a%20contrastive%20objective%20by%0Aexploiting%20the%20correspondences%20defined%20through%20the%20deformable%20template%20mesh.%0AThis%20leads%20to%20higher%20quality%20correspondence%20features%20compared%20to%20related%20works%0Aand%20a%20significantly%20improved%20model%20performance%20at%20estimating%203D%20object%20pose%20and%0Asemantic%20correspondence.%20Common3D%20is%20the%20first%20completely%20self-supervised%0Amethod%20that%20can%20solve%20various%20vision%20tasks%20in%20a%20zero-shot%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21749v1&entry.124074799=Read"},
{"title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal\n  Diffusion Transformers", "author": "Xinyu Li and Qi Yao and Yuanda Wang", "abstract": "  Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/.\n", "link": "http://arxiv.org/abs/2504.21476v1", "date": "2025-04-30", "relevancy": 3.0426, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.8073}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7501}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GarmentDiffusion%3A%203D%20Garment%20Sewing%20Pattern%20Generation%20with%20Multimodal%0A%20%20Diffusion%20Transformers&body=Title%3A%20GarmentDiffusion%3A%203D%20Garment%20Sewing%20Pattern%20Generation%20with%20Multimodal%0A%20%20Diffusion%20Transformers%0AAuthor%3A%20Xinyu%20Li%20and%20Qi%20Yao%20and%20Yuanda%20Wang%0AAbstract%3A%20%20%20Garment%20sewing%20patterns%20are%20fundamental%20design%20elements%20that%20bridge%20the%20gap%0Abetween%20design%20concepts%20and%20practical%20manufacturing.%20The%20generative%20modeling%20of%0Asewing%20patterns%20is%20crucial%20for%20creating%20diversified%20garments.%20However%2C%20existing%0Aapproaches%20are%20limited%20either%20by%20reliance%20on%20a%20single%20input%20modality%20or%20by%0Asuboptimal%20generation%20efficiency.%20In%20this%20work%2C%20we%20present%0A%5Ctextbf%7B%5Ctextit%7BGarmentDiffusion%7D%7D%2C%20a%20new%20generative%20model%20capable%20of%20producing%0Acentimeter-precise%2C%20vectorized%203D%20sewing%20patterns%20from%20multimodal%20inputs%20%28text%2C%0Aimage%2C%20and%20incomplete%20sewing%20pattern%29.%20Our%20method%20efficiently%20encodes%203D%20sewing%0Apattern%20parameters%20into%20compact%20edge%20token%20representations%2C%20achieving%20a%0Asequence%20length%20that%20is%20%24%5Ctextbf%7B10%7D%5Ctimes%24%20shorter%20than%20that%20of%20the%0Aautoregressive%20SewingGPT%20in%20DressCode.%20By%20employing%20a%20diffusion%20transformer%2C%20we%0Asimultaneously%20denoise%20all%20edge%20tokens%20along%20the%20temporal%20axis%2C%20while%0Amaintaining%20a%20constant%20number%20of%20denoising%20steps%20regardless%20of%20dataset-specific%0Aedge%20and%20panel%20statistics.%20With%20all%20combination%20of%20designs%20of%20our%20model%2C%20the%0Asewing%20pattern%20generation%20speed%20is%20accelerated%20by%20%24%5Ctextbf%7B100%7D%5Ctimes%24%20compared%0Ato%20SewingGPT.%20We%20achieve%20new%20state-of-the-art%20results%20on%20DressCodeData%2C%20as%20well%0Aas%20on%20the%20largest%20sewing%20pattern%20dataset%2C%20namely%20GarmentCodeData.%20The%20project%0Awebsite%20is%20available%20at%20https%3A//shenfu-research.github.io/Garment-Diffusion/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarmentDiffusion%253A%25203D%2520Garment%2520Sewing%2520Pattern%2520Generation%2520with%2520Multimodal%250A%2520%2520Diffusion%2520Transformers%26entry.906535625%3DXinyu%2520Li%2520and%2520Qi%2520Yao%2520and%2520Yuanda%2520Wang%26entry.1292438233%3D%2520%2520Garment%2520sewing%2520patterns%2520are%2520fundamental%2520design%2520elements%2520that%2520bridge%2520the%2520gap%250Abetween%2520design%2520concepts%2520and%2520practical%2520manufacturing.%2520The%2520generative%2520modeling%2520of%250Asewing%2520patterns%2520is%2520crucial%2520for%2520creating%2520diversified%2520garments.%2520However%252C%2520existing%250Aapproaches%2520are%2520limited%2520either%2520by%2520reliance%2520on%2520a%2520single%2520input%2520modality%2520or%2520by%250Asuboptimal%2520generation%2520efficiency.%2520In%2520this%2520work%252C%2520we%2520present%250A%255Ctextbf%257B%255Ctextit%257BGarmentDiffusion%257D%257D%252C%2520a%2520new%2520generative%2520model%2520capable%2520of%2520producing%250Acentimeter-precise%252C%2520vectorized%25203D%2520sewing%2520patterns%2520from%2520multimodal%2520inputs%2520%2528text%252C%250Aimage%252C%2520and%2520incomplete%2520sewing%2520pattern%2529.%2520Our%2520method%2520efficiently%2520encodes%25203D%2520sewing%250Apattern%2520parameters%2520into%2520compact%2520edge%2520token%2520representations%252C%2520achieving%2520a%250Asequence%2520length%2520that%2520is%2520%2524%255Ctextbf%257B10%257D%255Ctimes%2524%2520shorter%2520than%2520that%2520of%2520the%250Aautoregressive%2520SewingGPT%2520in%2520DressCode.%2520By%2520employing%2520a%2520diffusion%2520transformer%252C%2520we%250Asimultaneously%2520denoise%2520all%2520edge%2520tokens%2520along%2520the%2520temporal%2520axis%252C%2520while%250Amaintaining%2520a%2520constant%2520number%2520of%2520denoising%2520steps%2520regardless%2520of%2520dataset-specific%250Aedge%2520and%2520panel%2520statistics.%2520With%2520all%2520combination%2520of%2520designs%2520of%2520our%2520model%252C%2520the%250Asewing%2520pattern%2520generation%2520speed%2520is%2520accelerated%2520by%2520%2524%255Ctextbf%257B100%257D%255Ctimes%2524%2520compared%250Ato%2520SewingGPT.%2520We%2520achieve%2520new%2520state-of-the-art%2520results%2520on%2520DressCodeData%252C%2520as%2520well%250Aas%2520on%2520the%2520largest%2520sewing%2520pattern%2520dataset%252C%2520namely%2520GarmentCodeData.%2520The%2520project%250Awebsite%2520is%2520available%2520at%2520https%253A//shenfu-research.github.io/Garment-Diffusion/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GarmentDiffusion%3A%203D%20Garment%20Sewing%20Pattern%20Generation%20with%20Multimodal%0A%20%20Diffusion%20Transformers&entry.906535625=Xinyu%20Li%20and%20Qi%20Yao%20and%20Yuanda%20Wang&entry.1292438233=%20%20Garment%20sewing%20patterns%20are%20fundamental%20design%20elements%20that%20bridge%20the%20gap%0Abetween%20design%20concepts%20and%20practical%20manufacturing.%20The%20generative%20modeling%20of%0Asewing%20patterns%20is%20crucial%20for%20creating%20diversified%20garments.%20However%2C%20existing%0Aapproaches%20are%20limited%20either%20by%20reliance%20on%20a%20single%20input%20modality%20or%20by%0Asuboptimal%20generation%20efficiency.%20In%20this%20work%2C%20we%20present%0A%5Ctextbf%7B%5Ctextit%7BGarmentDiffusion%7D%7D%2C%20a%20new%20generative%20model%20capable%20of%20producing%0Acentimeter-precise%2C%20vectorized%203D%20sewing%20patterns%20from%20multimodal%20inputs%20%28text%2C%0Aimage%2C%20and%20incomplete%20sewing%20pattern%29.%20Our%20method%20efficiently%20encodes%203D%20sewing%0Apattern%20parameters%20into%20compact%20edge%20token%20representations%2C%20achieving%20a%0Asequence%20length%20that%20is%20%24%5Ctextbf%7B10%7D%5Ctimes%24%20shorter%20than%20that%20of%20the%0Aautoregressive%20SewingGPT%20in%20DressCode.%20By%20employing%20a%20diffusion%20transformer%2C%20we%0Asimultaneously%20denoise%20all%20edge%20tokens%20along%20the%20temporal%20axis%2C%20while%0Amaintaining%20a%20constant%20number%20of%20denoising%20steps%20regardless%20of%20dataset-specific%0Aedge%20and%20panel%20statistics.%20With%20all%20combination%20of%20designs%20of%20our%20model%2C%20the%0Asewing%20pattern%20generation%20speed%20is%20accelerated%20by%20%24%5Ctextbf%7B100%7D%5Ctimes%24%20compared%0Ato%20SewingGPT.%20We%20achieve%20new%20state-of-the-art%20results%20on%20DressCodeData%2C%20as%20well%0Aas%20on%20the%20largest%20sewing%20pattern%20dataset%2C%20namely%20GarmentCodeData.%20The%20project%0Awebsite%20is%20available%20at%20https%3A//shenfu-research.github.io/Garment-Diffusion/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21476v1&entry.124074799=Read"},
{"title": "Enhancing Self-Supervised Fine-Grained Video Object Tracking with\n  Dynamic Memory Prediction", "author": "Zihan Zhou and Changrui Dai and Aibo Song and Xiaolin Fang", "abstract": "  Successful video analysis relies on accurate recognition of pixels across\nframes, and frame reconstruction methods based on video correspondence learning\nare popular due to their efficiency. Existing frame reconstruction methods,\nwhile efficient, neglect the value of direct involvement of multiple reference\nframes for reconstruction and decision-making aspects, especially in complex\nsituations such as occlusion or fast movement. In this paper, we introduce a\nDynamic Memory Prediction (DMP) framework that innovatively utilizes multiple\nreference frames to concisely and directly enhance frame reconstruction. Its\ncore component is a Reference Frame Memory Engine that dynamically selects\nframes based on object pixel features to improve tracking accuracy. In\naddition, a Bidirectional Target Prediction Network is built to utilize\nmultiple reference frames to improve the robustness of the model. Through\nexperiments, our algorithm outperforms the state-of-the-art self-supervised\ntechniques on two fine-grained video object tracking tasks: object segmentation\nand keypoint tracking.\n", "link": "http://arxiv.org/abs/2504.21692v1", "date": "2025-04-30", "relevancy": 2.9593, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6039}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5977}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Self-Supervised%20Fine-Grained%20Video%20Object%20Tracking%20with%0A%20%20Dynamic%20Memory%20Prediction&body=Title%3A%20Enhancing%20Self-Supervised%20Fine-Grained%20Video%20Object%20Tracking%20with%0A%20%20Dynamic%20Memory%20Prediction%0AAuthor%3A%20Zihan%20Zhou%20and%20Changrui%20Dai%20and%20Aibo%20Song%20and%20Xiaolin%20Fang%0AAbstract%3A%20%20%20Successful%20video%20analysis%20relies%20on%20accurate%20recognition%20of%20pixels%20across%0Aframes%2C%20and%20frame%20reconstruction%20methods%20based%20on%20video%20correspondence%20learning%0Aare%20popular%20due%20to%20their%20efficiency.%20Existing%20frame%20reconstruction%20methods%2C%0Awhile%20efficient%2C%20neglect%20the%20value%20of%20direct%20involvement%20of%20multiple%20reference%0Aframes%20for%20reconstruction%20and%20decision-making%20aspects%2C%20especially%20in%20complex%0Asituations%20such%20as%20occlusion%20or%20fast%20movement.%20In%20this%20paper%2C%20we%20introduce%20a%0ADynamic%20Memory%20Prediction%20%28DMP%29%20framework%20that%20innovatively%20utilizes%20multiple%0Areference%20frames%20to%20concisely%20and%20directly%20enhance%20frame%20reconstruction.%20Its%0Acore%20component%20is%20a%20Reference%20Frame%20Memory%20Engine%20that%20dynamically%20selects%0Aframes%20based%20on%20object%20pixel%20features%20to%20improve%20tracking%20accuracy.%20In%0Aaddition%2C%20a%20Bidirectional%20Target%20Prediction%20Network%20is%20built%20to%20utilize%0Amultiple%20reference%20frames%20to%20improve%20the%20robustness%20of%20the%20model.%20Through%0Aexperiments%2C%20our%20algorithm%20outperforms%20the%20state-of-the-art%20self-supervised%0Atechniques%20on%20two%20fine-grained%20video%20object%20tracking%20tasks%3A%20object%20segmentation%0Aand%20keypoint%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Self-Supervised%2520Fine-Grained%2520Video%2520Object%2520Tracking%2520with%250A%2520%2520Dynamic%2520Memory%2520Prediction%26entry.906535625%3DZihan%2520Zhou%2520and%2520Changrui%2520Dai%2520and%2520Aibo%2520Song%2520and%2520Xiaolin%2520Fang%26entry.1292438233%3D%2520%2520Successful%2520video%2520analysis%2520relies%2520on%2520accurate%2520recognition%2520of%2520pixels%2520across%250Aframes%252C%2520and%2520frame%2520reconstruction%2520methods%2520based%2520on%2520video%2520correspondence%2520learning%250Aare%2520popular%2520due%2520to%2520their%2520efficiency.%2520Existing%2520frame%2520reconstruction%2520methods%252C%250Awhile%2520efficient%252C%2520neglect%2520the%2520value%2520of%2520direct%2520involvement%2520of%2520multiple%2520reference%250Aframes%2520for%2520reconstruction%2520and%2520decision-making%2520aspects%252C%2520especially%2520in%2520complex%250Asituations%2520such%2520as%2520occlusion%2520or%2520fast%2520movement.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250ADynamic%2520Memory%2520Prediction%2520%2528DMP%2529%2520framework%2520that%2520innovatively%2520utilizes%2520multiple%250Areference%2520frames%2520to%2520concisely%2520and%2520directly%2520enhance%2520frame%2520reconstruction.%2520Its%250Acore%2520component%2520is%2520a%2520Reference%2520Frame%2520Memory%2520Engine%2520that%2520dynamically%2520selects%250Aframes%2520based%2520on%2520object%2520pixel%2520features%2520to%2520improve%2520tracking%2520accuracy.%2520In%250Aaddition%252C%2520a%2520Bidirectional%2520Target%2520Prediction%2520Network%2520is%2520built%2520to%2520utilize%250Amultiple%2520reference%2520frames%2520to%2520improve%2520the%2520robustness%2520of%2520the%2520model.%2520Through%250Aexperiments%252C%2520our%2520algorithm%2520outperforms%2520the%2520state-of-the-art%2520self-supervised%250Atechniques%2520on%2520two%2520fine-grained%2520video%2520object%2520tracking%2520tasks%253A%2520object%2520segmentation%250Aand%2520keypoint%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Self-Supervised%20Fine-Grained%20Video%20Object%20Tracking%20with%0A%20%20Dynamic%20Memory%20Prediction&entry.906535625=Zihan%20Zhou%20and%20Changrui%20Dai%20and%20Aibo%20Song%20and%20Xiaolin%20Fang&entry.1292438233=%20%20Successful%20video%20analysis%20relies%20on%20accurate%20recognition%20of%20pixels%20across%0Aframes%2C%20and%20frame%20reconstruction%20methods%20based%20on%20video%20correspondence%20learning%0Aare%20popular%20due%20to%20their%20efficiency.%20Existing%20frame%20reconstruction%20methods%2C%0Awhile%20efficient%2C%20neglect%20the%20value%20of%20direct%20involvement%20of%20multiple%20reference%0Aframes%20for%20reconstruction%20and%20decision-making%20aspects%2C%20especially%20in%20complex%0Asituations%20such%20as%20occlusion%20or%20fast%20movement.%20In%20this%20paper%2C%20we%20introduce%20a%0ADynamic%20Memory%20Prediction%20%28DMP%29%20framework%20that%20innovatively%20utilizes%20multiple%0Areference%20frames%20to%20concisely%20and%20directly%20enhance%20frame%20reconstruction.%20Its%0Acore%20component%20is%20a%20Reference%20Frame%20Memory%20Engine%20that%20dynamically%20selects%0Aframes%20based%20on%20object%20pixel%20features%20to%20improve%20tracking%20accuracy.%20In%0Aaddition%2C%20a%20Bidirectional%20Target%20Prediction%20Network%20is%20built%20to%20utilize%0Amultiple%20reference%20frames%20to%20improve%20the%20robustness%20of%20the%20model.%20Through%0Aexperiments%2C%20our%20algorithm%20outperforms%20the%20state-of-the-art%20self-supervised%0Atechniques%20on%20two%20fine-grained%20video%20object%20tracking%20tasks%3A%20object%20segmentation%0Aand%20keypoint%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21692v1&entry.124074799=Read"},
{"title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video\n  Games", "author": "Lukas Sch\u00e4fer and Logan Jones and Anssi Kanervisto and Yuhan Cao and Tabish Rashid and Raluca Georgescu and Dave Bignell and Siddhartha Sen and Andrea Trevi\u00f1o Gavito and Sam Devlin", "abstract": "  Video games have served as useful benchmarks for the decision-making\ncommunity, but going beyond Atari games towards modern games has been\nprohibitively expensive for the vast majority of the research community. Prior\nwork in modern video games typically relied on game-specific integration to\nobtain game features and enable online training, or on existing large datasets.\nAn alternative approach is to train agents using imitation learning to play\nvideo games purely from images. However, this setting poses a fundamental\nquestion: which visual encoders obtain representations that retain information\ncritical for decision making? To answer this question, we conduct a systematic\nstudy of imitation learning with publicly available pre-trained visual encoders\ncompared to the typical task-specific end-to-end training approach in\nMinecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our\nresults show that end-to-end training can be effective with comparably\nlow-resolution images and only minutes of demonstrations, but significant\nimprovements can be gained by utilising pre-trained encoders such as DINOv2\ndepending on the game. In addition to enabling effective decision making, we\nshow that pre-trained encoders can make decision-making research in video games\nmore accessible by significantly reducing the cost of training.\n", "link": "http://arxiv.org/abs/2312.02312v2", "date": "2025-04-30", "relevancy": 2.9532, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Encoders%20for%20Data-Efficient%20Imitation%20Learning%20in%20Modern%20Video%0A%20%20Games&body=Title%3A%20Visual%20Encoders%20for%20Data-Efficient%20Imitation%20Learning%20in%20Modern%20Video%0A%20%20Games%0AAuthor%3A%20Lukas%20Sch%C3%A4fer%20and%20Logan%20Jones%20and%20Anssi%20Kanervisto%20and%20Yuhan%20Cao%20and%20Tabish%20Rashid%20and%20Raluca%20Georgescu%20and%20Dave%20Bignell%20and%20Siddhartha%20Sen%20and%20Andrea%20Trevi%C3%B1o%20Gavito%20and%20Sam%20Devlin%0AAbstract%3A%20%20%20Video%20games%20have%20served%20as%20useful%20benchmarks%20for%20the%20decision-making%0Acommunity%2C%20but%20going%20beyond%20Atari%20games%20towards%20modern%20games%20has%20been%0Aprohibitively%20expensive%20for%20the%20vast%20majority%20of%20the%20research%20community.%20Prior%0Awork%20in%20modern%20video%20games%20typically%20relied%20on%20game-specific%20integration%20to%0Aobtain%20game%20features%20and%20enable%20online%20training%2C%20or%20on%20existing%20large%20datasets.%0AAn%20alternative%20approach%20is%20to%20train%20agents%20using%20imitation%20learning%20to%20play%0Avideo%20games%20purely%20from%20images.%20However%2C%20this%20setting%20poses%20a%20fundamental%0Aquestion%3A%20which%20visual%20encoders%20obtain%20representations%20that%20retain%20information%0Acritical%20for%20decision%20making%3F%20To%20answer%20this%20question%2C%20we%20conduct%20a%20systematic%0Astudy%20of%20imitation%20learning%20with%20publicly%20available%20pre-trained%20visual%20encoders%0Acompared%20to%20the%20typical%20task-specific%20end-to-end%20training%20approach%20in%0AMinecraft%2C%20Counter-Strike%3A%20Global%20Offensive%2C%20and%20Minecraft%20Dungeons.%20Our%0Aresults%20show%20that%20end-to-end%20training%20can%20be%20effective%20with%20comparably%0Alow-resolution%20images%20and%20only%20minutes%20of%20demonstrations%2C%20but%20significant%0Aimprovements%20can%20be%20gained%20by%20utilising%20pre-trained%20encoders%20such%20as%20DINOv2%0Adepending%20on%20the%20game.%20In%20addition%20to%20enabling%20effective%20decision%20making%2C%20we%0Ashow%20that%20pre-trained%20encoders%20can%20make%20decision-making%20research%20in%20video%20games%0Amore%20accessible%20by%20significantly%20reducing%20the%20cost%20of%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Encoders%2520for%2520Data-Efficient%2520Imitation%2520Learning%2520in%2520Modern%2520Video%250A%2520%2520Games%26entry.906535625%3DLukas%2520Sch%25C3%25A4fer%2520and%2520Logan%2520Jones%2520and%2520Anssi%2520Kanervisto%2520and%2520Yuhan%2520Cao%2520and%2520Tabish%2520Rashid%2520and%2520Raluca%2520Georgescu%2520and%2520Dave%2520Bignell%2520and%2520Siddhartha%2520Sen%2520and%2520Andrea%2520Trevi%25C3%25B1o%2520Gavito%2520and%2520Sam%2520Devlin%26entry.1292438233%3D%2520%2520Video%2520games%2520have%2520served%2520as%2520useful%2520benchmarks%2520for%2520the%2520decision-making%250Acommunity%252C%2520but%2520going%2520beyond%2520Atari%2520games%2520towards%2520modern%2520games%2520has%2520been%250Aprohibitively%2520expensive%2520for%2520the%2520vast%2520majority%2520of%2520the%2520research%2520community.%2520Prior%250Awork%2520in%2520modern%2520video%2520games%2520typically%2520relied%2520on%2520game-specific%2520integration%2520to%250Aobtain%2520game%2520features%2520and%2520enable%2520online%2520training%252C%2520or%2520on%2520existing%2520large%2520datasets.%250AAn%2520alternative%2520approach%2520is%2520to%2520train%2520agents%2520using%2520imitation%2520learning%2520to%2520play%250Avideo%2520games%2520purely%2520from%2520images.%2520However%252C%2520this%2520setting%2520poses%2520a%2520fundamental%250Aquestion%253A%2520which%2520visual%2520encoders%2520obtain%2520representations%2520that%2520retain%2520information%250Acritical%2520for%2520decision%2520making%253F%2520To%2520answer%2520this%2520question%252C%2520we%2520conduct%2520a%2520systematic%250Astudy%2520of%2520imitation%2520learning%2520with%2520publicly%2520available%2520pre-trained%2520visual%2520encoders%250Acompared%2520to%2520the%2520typical%2520task-specific%2520end-to-end%2520training%2520approach%2520in%250AMinecraft%252C%2520Counter-Strike%253A%2520Global%2520Offensive%252C%2520and%2520Minecraft%2520Dungeons.%2520Our%250Aresults%2520show%2520that%2520end-to-end%2520training%2520can%2520be%2520effective%2520with%2520comparably%250Alow-resolution%2520images%2520and%2520only%2520minutes%2520of%2520demonstrations%252C%2520but%2520significant%250Aimprovements%2520can%2520be%2520gained%2520by%2520utilising%2520pre-trained%2520encoders%2520such%2520as%2520DINOv2%250Adepending%2520on%2520the%2520game.%2520In%2520addition%2520to%2520enabling%2520effective%2520decision%2520making%252C%2520we%250Ashow%2520that%2520pre-trained%2520encoders%2520can%2520make%2520decision-making%2520research%2520in%2520video%2520games%250Amore%2520accessible%2520by%2520significantly%2520reducing%2520the%2520cost%2520of%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Encoders%20for%20Data-Efficient%20Imitation%20Learning%20in%20Modern%20Video%0A%20%20Games&entry.906535625=Lukas%20Sch%C3%A4fer%20and%20Logan%20Jones%20and%20Anssi%20Kanervisto%20and%20Yuhan%20Cao%20and%20Tabish%20Rashid%20and%20Raluca%20Georgescu%20and%20Dave%20Bignell%20and%20Siddhartha%20Sen%20and%20Andrea%20Trevi%C3%B1o%20Gavito%20and%20Sam%20Devlin&entry.1292438233=%20%20Video%20games%20have%20served%20as%20useful%20benchmarks%20for%20the%20decision-making%0Acommunity%2C%20but%20going%20beyond%20Atari%20games%20towards%20modern%20games%20has%20been%0Aprohibitively%20expensive%20for%20the%20vast%20majority%20of%20the%20research%20community.%20Prior%0Awork%20in%20modern%20video%20games%20typically%20relied%20on%20game-specific%20integration%20to%0Aobtain%20game%20features%20and%20enable%20online%20training%2C%20or%20on%20existing%20large%20datasets.%0AAn%20alternative%20approach%20is%20to%20train%20agents%20using%20imitation%20learning%20to%20play%0Avideo%20games%20purely%20from%20images.%20However%2C%20this%20setting%20poses%20a%20fundamental%0Aquestion%3A%20which%20visual%20encoders%20obtain%20representations%20that%20retain%20information%0Acritical%20for%20decision%20making%3F%20To%20answer%20this%20question%2C%20we%20conduct%20a%20systematic%0Astudy%20of%20imitation%20learning%20with%20publicly%20available%20pre-trained%20visual%20encoders%0Acompared%20to%20the%20typical%20task-specific%20end-to-end%20training%20approach%20in%0AMinecraft%2C%20Counter-Strike%3A%20Global%20Offensive%2C%20and%20Minecraft%20Dungeons.%20Our%0Aresults%20show%20that%20end-to-end%20training%20can%20be%20effective%20with%20comparably%0Alow-resolution%20images%20and%20only%20minutes%20of%20demonstrations%2C%20but%20significant%0Aimprovements%20can%20be%20gained%20by%20utilising%20pre-trained%20encoders%20such%20as%20DINOv2%0Adepending%20on%20the%20game.%20In%20addition%20to%20enabling%20effective%20decision%20making%2C%20we%0Ashow%20that%20pre-trained%20encoders%20can%20make%20decision-making%20research%20in%20video%20games%0Amore%20accessible%20by%20significantly%20reducing%20the%20cost%20of%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02312v2&entry.124074799=Read"},
{"title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning", "author": "Xindi Wu and Hee Seung Hwang and Polina Kirichenko and Olga Russakovsky", "abstract": "  Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.\n", "link": "http://arxiv.org/abs/2504.21850v1", "date": "2025-04-30", "relevancy": 2.7458, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMPACT%3A%20COMPositional%20Atomic-to-Complex%20Visual%20Capability%20Tuning&body=Title%3A%20COMPACT%3A%20COMPositional%20Atomic-to-Complex%20Visual%20Capability%20Tuning%0AAuthor%3A%20Xindi%20Wu%20and%20Hee%20Seung%20Hwang%20and%20Polina%20Kirichenko%20and%20Olga%20Russakovsky%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20simple%20vision-language%0Atasks%20but%20struggle%20when%20faced%20with%20complex%20tasks%20that%20require%20multiple%0Acapabilities%2C%20such%20as%20simultaneously%20recognizing%20objects%2C%20counting%20them%2C%20and%0Aunderstanding%20their%20spatial%20relationships.%20This%20might%20be%20partially%20the%20result%0Aof%20the%20fact%20that%20Visual%20Instruction%20Tuning%20%28VIT%29%2C%20a%20critical%20training%20step%20for%0AMLLMs%2C%20has%20traditionally%20focused%20on%20scaling%20data%20volume%2C%20but%20not%20the%0Acompositional%20complexity%20of%20training%20examples.%20We%20propose%20COMPACT%0A%28COMPositional%20Atomic-to-complex%20visual%20Capability%20Tuning%29%2C%20which%20generates%20a%0Atraining%20dataset%20explicitly%20controlling%20for%20the%20compositional%20complexity%20of%20the%0Atraining%20examples.%20The%20data%20from%20COMPACT%20allows%20MLLMs%20to%20train%20on%20combinations%0Aof%20atomic%20capabilities%20to%20learn%20complex%20capabilities%20more%20efficiently.%20Across%0Aall%20benchmarks%2C%20COMPACT%20achieves%20comparable%20performance%20to%20the%20LLaVA-665k%20VIT%0Awhile%20using%20less%20than%2010%25%20of%20its%20data%20budget%2C%20and%20even%20outperforms%20it%20on%0Aseveral%2C%20especially%20those%20involving%20complex%20multi-capability%20tasks.%20For%0Aexample%2C%20COMPACT%20achieves%20substantial%2083.3%25%20improvement%20on%20MMStar%20and%2094.0%25%0Aimprovement%20on%20MM-Vet%20compared%20to%20the%20full-scale%20VIT%20on%20particularly%20complex%0Aquestions%20that%20require%20four%20or%20more%20atomic%20capabilities.%20COMPACT%20offers%20a%0Ascalable%2C%20data-efficient%2C%20visual%20compositional%20tuning%20recipe%20to%20improve%20on%0Acomplex%20visual-language%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMPACT%253A%2520COMPositional%2520Atomic-to-Complex%2520Visual%2520Capability%2520Tuning%26entry.906535625%3DXindi%2520Wu%2520and%2520Hee%2520Seung%2520Hwang%2520and%2520Polina%2520Kirichenko%2520and%2520Olga%2520Russakovsky%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520simple%2520vision-language%250Atasks%2520but%2520struggle%2520when%2520faced%2520with%2520complex%2520tasks%2520that%2520require%2520multiple%250Acapabilities%252C%2520such%2520as%2520simultaneously%2520recognizing%2520objects%252C%2520counting%2520them%252C%2520and%250Aunderstanding%2520their%2520spatial%2520relationships.%2520This%2520might%2520be%2520partially%2520the%2520result%250Aof%2520the%2520fact%2520that%2520Visual%2520Instruction%2520Tuning%2520%2528VIT%2529%252C%2520a%2520critical%2520training%2520step%2520for%250AMLLMs%252C%2520has%2520traditionally%2520focused%2520on%2520scaling%2520data%2520volume%252C%2520but%2520not%2520the%250Acompositional%2520complexity%2520of%2520training%2520examples.%2520We%2520propose%2520COMPACT%250A%2528COMPositional%2520Atomic-to-complex%2520visual%2520Capability%2520Tuning%2529%252C%2520which%2520generates%2520a%250Atraining%2520dataset%2520explicitly%2520controlling%2520for%2520the%2520compositional%2520complexity%2520of%2520the%250Atraining%2520examples.%2520The%2520data%2520from%2520COMPACT%2520allows%2520MLLMs%2520to%2520train%2520on%2520combinations%250Aof%2520atomic%2520capabilities%2520to%2520learn%2520complex%2520capabilities%2520more%2520efficiently.%2520Across%250Aall%2520benchmarks%252C%2520COMPACT%2520achieves%2520comparable%2520performance%2520to%2520the%2520LLaVA-665k%2520VIT%250Awhile%2520using%2520less%2520than%252010%2525%2520of%2520its%2520data%2520budget%252C%2520and%2520even%2520outperforms%2520it%2520on%250Aseveral%252C%2520especially%2520those%2520involving%2520complex%2520multi-capability%2520tasks.%2520For%250Aexample%252C%2520COMPACT%2520achieves%2520substantial%252083.3%2525%2520improvement%2520on%2520MMStar%2520and%252094.0%2525%250Aimprovement%2520on%2520MM-Vet%2520compared%2520to%2520the%2520full-scale%2520VIT%2520on%2520particularly%2520complex%250Aquestions%2520that%2520require%2520four%2520or%2520more%2520atomic%2520capabilities.%2520COMPACT%2520offers%2520a%250Ascalable%252C%2520data-efficient%252C%2520visual%2520compositional%2520tuning%2520recipe%2520to%2520improve%2520on%250Acomplex%2520visual-language%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMPACT%3A%20COMPositional%20Atomic-to-Complex%20Visual%20Capability%20Tuning&entry.906535625=Xindi%20Wu%20and%20Hee%20Seung%20Hwang%20and%20Polina%20Kirichenko%20and%20Olga%20Russakovsky&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20simple%20vision-language%0Atasks%20but%20struggle%20when%20faced%20with%20complex%20tasks%20that%20require%20multiple%0Acapabilities%2C%20such%20as%20simultaneously%20recognizing%20objects%2C%20counting%20them%2C%20and%0Aunderstanding%20their%20spatial%20relationships.%20This%20might%20be%20partially%20the%20result%0Aof%20the%20fact%20that%20Visual%20Instruction%20Tuning%20%28VIT%29%2C%20a%20critical%20training%20step%20for%0AMLLMs%2C%20has%20traditionally%20focused%20on%20scaling%20data%20volume%2C%20but%20not%20the%0Acompositional%20complexity%20of%20training%20examples.%20We%20propose%20COMPACT%0A%28COMPositional%20Atomic-to-complex%20visual%20Capability%20Tuning%29%2C%20which%20generates%20a%0Atraining%20dataset%20explicitly%20controlling%20for%20the%20compositional%20complexity%20of%20the%0Atraining%20examples.%20The%20data%20from%20COMPACT%20allows%20MLLMs%20to%20train%20on%20combinations%0Aof%20atomic%20capabilities%20to%20learn%20complex%20capabilities%20more%20efficiently.%20Across%0Aall%20benchmarks%2C%20COMPACT%20achieves%20comparable%20performance%20to%20the%20LLaVA-665k%20VIT%0Awhile%20using%20less%20than%2010%25%20of%20its%20data%20budget%2C%20and%20even%20outperforms%20it%20on%0Aseveral%2C%20especially%20those%20involving%20complex%20multi-capability%20tasks.%20For%0Aexample%2C%20COMPACT%20achieves%20substantial%2083.3%25%20improvement%20on%20MMStar%20and%2094.0%25%0Aimprovement%20on%20MM-Vet%20compared%20to%20the%20full-scale%20VIT%20on%20particularly%20complex%0Aquestions%20that%20require%20four%20or%20more%20atomic%20capabilities.%20COMPACT%20offers%20a%0Ascalable%2C%20data-efficient%2C%20visual%20compositional%20tuning%20recipe%20to%20improve%20on%0Acomplex%20visual-language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21850v1&entry.124074799=Read"},
{"title": "Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue\n  Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP)\n  Assessment", "author": "Zixue Zeng and Xiaoyan Zhao and Matthew Cartier and Tong Yu and Jing Wang and Xin Meng and Zhiyu Sheng and Maryam Satarpour and John M Cormack and Allison Bean and Ryan Nussbaum and Maya Maurer and Emily Landis-Walkenhorst and Dinesh Kumbhare and Kang Kim and Ajay Wasan and Jiantao Pu", "abstract": "  We introduce a novel segmentation-aware joint training framework called\ngenerative reinforcement network (GRN) that integrates segmentation loss\nfeedback to optimize both image generation and segmentation performance in a\nsingle stage. An image enhancement technique called segmentation-guided\nenhancement (SGE) is also developed, where the generator produces images\ntailored specifically for the segmentation model. Two variants of GRN were also\ndeveloped, including GRN for sample-efficient learning (GRN-SEL) and GRN for\nsemi-supervised learning (GRN-SSL). GRN's performance was evaluated using a\ndataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The\nannotations included six anatomical structures: dermis, superficial fat,\nsuperficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and\nmuscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up\nto 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient\n(DSC) compared to models trained on fully labeled datasets. GRN-SEL alone\nreduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling\nrequirements by 70%, and GRN-SSL alone by 60%, all while maintaining\nperformance comparable to fully supervised models. These findings suggest the\neffectiveness of the GRN framework in optimizing segmentation performance with\nsignificantly less labeled data, offering a scalable and efficient solution for\nultrasound image analysis and reducing the burdens associated with data\nannotation.\n", "link": "http://arxiv.org/abs/2501.17690v2", "date": "2025-04-30", "relevancy": 2.7363, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5607}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5477}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation-Aware%20Generative%20Reinforcement%20Network%20%28GRN%29%20for%20Tissue%0A%20%20Layer%20Segmentation%20in%203-D%20Ultrasound%20Images%20for%20Chronic%20Low-back%20Pain%20%28cLBP%29%0A%20%20Assessment&body=Title%3A%20Segmentation-Aware%20Generative%20Reinforcement%20Network%20%28GRN%29%20for%20Tissue%0A%20%20Layer%20Segmentation%20in%203-D%20Ultrasound%20Images%20for%20Chronic%20Low-back%20Pain%20%28cLBP%29%0A%20%20Assessment%0AAuthor%3A%20Zixue%20Zeng%20and%20Xiaoyan%20Zhao%20and%20Matthew%20Cartier%20and%20Tong%20Yu%20and%20Jing%20Wang%20and%20Xin%20Meng%20and%20Zhiyu%20Sheng%20and%20Maryam%20Satarpour%20and%20John%20M%20Cormack%20and%20Allison%20Bean%20and%20Ryan%20Nussbaum%20and%20Maya%20Maurer%20and%20Emily%20Landis-Walkenhorst%20and%20Dinesh%20Kumbhare%20and%20Kang%20Kim%20and%20Ajay%20Wasan%20and%20Jiantao%20Pu%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20segmentation-aware%20joint%20training%20framework%20called%0Agenerative%20reinforcement%20network%20%28GRN%29%20that%20integrates%20segmentation%20loss%0Afeedback%20to%20optimize%20both%20image%20generation%20and%20segmentation%20performance%20in%20a%0Asingle%20stage.%20An%20image%20enhancement%20technique%20called%20segmentation-guided%0Aenhancement%20%28SGE%29%20is%20also%20developed%2C%20where%20the%20generator%20produces%20images%0Atailored%20specifically%20for%20the%20segmentation%20model.%20Two%20variants%20of%20GRN%20were%20also%0Adeveloped%2C%20including%20GRN%20for%20sample-efficient%20learning%20%28GRN-SEL%29%20and%20GRN%20for%0Asemi-supervised%20learning%20%28GRN-SSL%29.%20GRN%27s%20performance%20was%20evaluated%20using%20a%0Adataset%20of%2069%20fully%20annotated%203D%20ultrasound%20scans%20from%2029%20subjects.%20The%0Aannotations%20included%20six%20anatomical%20structures%3A%20dermis%2C%20superficial%20fat%2C%0Asuperficial%20fascial%20membrane%20%28SFM%29%2C%20deep%20fat%2C%20deep%20fascial%20membrane%20%28DFM%29%2C%20and%0Amuscle.%20Our%20results%20show%20that%20GRN-SEL%20with%20SGE%20reduces%20labeling%20efforts%20by%20up%0Ato%2070%25%20while%20achieving%20a%201.98%25%20improvement%20in%20the%20Dice%20Similarity%20Coefficient%0A%28DSC%29%20compared%20to%20models%20trained%20on%20fully%20labeled%20datasets.%20GRN-SEL%20alone%0Areduces%20labeling%20efforts%20by%2060%25%2C%20GRN-SSL%20with%20SGE%20decreases%20labeling%0Arequirements%20by%2070%25%2C%20and%20GRN-SSL%20alone%20by%2060%25%2C%20all%20while%20maintaining%0Aperformance%20comparable%20to%20fully%20supervised%20models.%20These%20findings%20suggest%20the%0Aeffectiveness%20of%20the%20GRN%20framework%20in%20optimizing%20segmentation%20performance%20with%0Asignificantly%20less%20labeled%20data%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%0Aultrasound%20image%20analysis%20and%20reducing%20the%20burdens%20associated%20with%20data%0Aannotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation-Aware%2520Generative%2520Reinforcement%2520Network%2520%2528GRN%2529%2520for%2520Tissue%250A%2520%2520Layer%2520Segmentation%2520in%25203-D%2520Ultrasound%2520Images%2520for%2520Chronic%2520Low-back%2520Pain%2520%2528cLBP%2529%250A%2520%2520Assessment%26entry.906535625%3DZixue%2520Zeng%2520and%2520Xiaoyan%2520Zhao%2520and%2520Matthew%2520Cartier%2520and%2520Tong%2520Yu%2520and%2520Jing%2520Wang%2520and%2520Xin%2520Meng%2520and%2520Zhiyu%2520Sheng%2520and%2520Maryam%2520Satarpour%2520and%2520John%2520M%2520Cormack%2520and%2520Allison%2520Bean%2520and%2520Ryan%2520Nussbaum%2520and%2520Maya%2520Maurer%2520and%2520Emily%2520Landis-Walkenhorst%2520and%2520Dinesh%2520Kumbhare%2520and%2520Kang%2520Kim%2520and%2520Ajay%2520Wasan%2520and%2520Jiantao%2520Pu%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520segmentation-aware%2520joint%2520training%2520framework%2520called%250Agenerative%2520reinforcement%2520network%2520%2528GRN%2529%2520that%2520integrates%2520segmentation%2520loss%250Afeedback%2520to%2520optimize%2520both%2520image%2520generation%2520and%2520segmentation%2520performance%2520in%2520a%250Asingle%2520stage.%2520An%2520image%2520enhancement%2520technique%2520called%2520segmentation-guided%250Aenhancement%2520%2528SGE%2529%2520is%2520also%2520developed%252C%2520where%2520the%2520generator%2520produces%2520images%250Atailored%2520specifically%2520for%2520the%2520segmentation%2520model.%2520Two%2520variants%2520of%2520GRN%2520were%2520also%250Adeveloped%252C%2520including%2520GRN%2520for%2520sample-efficient%2520learning%2520%2528GRN-SEL%2529%2520and%2520GRN%2520for%250Asemi-supervised%2520learning%2520%2528GRN-SSL%2529.%2520GRN%2527s%2520performance%2520was%2520evaluated%2520using%2520a%250Adataset%2520of%252069%2520fully%2520annotated%25203D%2520ultrasound%2520scans%2520from%252029%2520subjects.%2520The%250Aannotations%2520included%2520six%2520anatomical%2520structures%253A%2520dermis%252C%2520superficial%2520fat%252C%250Asuperficial%2520fascial%2520membrane%2520%2528SFM%2529%252C%2520deep%2520fat%252C%2520deep%2520fascial%2520membrane%2520%2528DFM%2529%252C%2520and%250Amuscle.%2520Our%2520results%2520show%2520that%2520GRN-SEL%2520with%2520SGE%2520reduces%2520labeling%2520efforts%2520by%2520up%250Ato%252070%2525%2520while%2520achieving%2520a%25201.98%2525%2520improvement%2520in%2520the%2520Dice%2520Similarity%2520Coefficient%250A%2528DSC%2529%2520compared%2520to%2520models%2520trained%2520on%2520fully%2520labeled%2520datasets.%2520GRN-SEL%2520alone%250Areduces%2520labeling%2520efforts%2520by%252060%2525%252C%2520GRN-SSL%2520with%2520SGE%2520decreases%2520labeling%250Arequirements%2520by%252070%2525%252C%2520and%2520GRN-SSL%2520alone%2520by%252060%2525%252C%2520all%2520while%2520maintaining%250Aperformance%2520comparable%2520to%2520fully%2520supervised%2520models.%2520These%2520findings%2520suggest%2520the%250Aeffectiveness%2520of%2520the%2520GRN%2520framework%2520in%2520optimizing%2520segmentation%2520performance%2520with%250Asignificantly%2520less%2520labeled%2520data%252C%2520offering%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%250Aultrasound%2520image%2520analysis%2520and%2520reducing%2520the%2520burdens%2520associated%2520with%2520data%250Aannotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation-Aware%20Generative%20Reinforcement%20Network%20%28GRN%29%20for%20Tissue%0A%20%20Layer%20Segmentation%20in%203-D%20Ultrasound%20Images%20for%20Chronic%20Low-back%20Pain%20%28cLBP%29%0A%20%20Assessment&entry.906535625=Zixue%20Zeng%20and%20Xiaoyan%20Zhao%20and%20Matthew%20Cartier%20and%20Tong%20Yu%20and%20Jing%20Wang%20and%20Xin%20Meng%20and%20Zhiyu%20Sheng%20and%20Maryam%20Satarpour%20and%20John%20M%20Cormack%20and%20Allison%20Bean%20and%20Ryan%20Nussbaum%20and%20Maya%20Maurer%20and%20Emily%20Landis-Walkenhorst%20and%20Dinesh%20Kumbhare%20and%20Kang%20Kim%20and%20Ajay%20Wasan%20and%20Jiantao%20Pu&entry.1292438233=%20%20We%20introduce%20a%20novel%20segmentation-aware%20joint%20training%20framework%20called%0Agenerative%20reinforcement%20network%20%28GRN%29%20that%20integrates%20segmentation%20loss%0Afeedback%20to%20optimize%20both%20image%20generation%20and%20segmentation%20performance%20in%20a%0Asingle%20stage.%20An%20image%20enhancement%20technique%20called%20segmentation-guided%0Aenhancement%20%28SGE%29%20is%20also%20developed%2C%20where%20the%20generator%20produces%20images%0Atailored%20specifically%20for%20the%20segmentation%20model.%20Two%20variants%20of%20GRN%20were%20also%0Adeveloped%2C%20including%20GRN%20for%20sample-efficient%20learning%20%28GRN-SEL%29%20and%20GRN%20for%0Asemi-supervised%20learning%20%28GRN-SSL%29.%20GRN%27s%20performance%20was%20evaluated%20using%20a%0Adataset%20of%2069%20fully%20annotated%203D%20ultrasound%20scans%20from%2029%20subjects.%20The%0Aannotations%20included%20six%20anatomical%20structures%3A%20dermis%2C%20superficial%20fat%2C%0Asuperficial%20fascial%20membrane%20%28SFM%29%2C%20deep%20fat%2C%20deep%20fascial%20membrane%20%28DFM%29%2C%20and%0Amuscle.%20Our%20results%20show%20that%20GRN-SEL%20with%20SGE%20reduces%20labeling%20efforts%20by%20up%0Ato%2070%25%20while%20achieving%20a%201.98%25%20improvement%20in%20the%20Dice%20Similarity%20Coefficient%0A%28DSC%29%20compared%20to%20models%20trained%20on%20fully%20labeled%20datasets.%20GRN-SEL%20alone%0Areduces%20labeling%20efforts%20by%2060%25%2C%20GRN-SSL%20with%20SGE%20decreases%20labeling%0Arequirements%20by%2070%25%2C%20and%20GRN-SSL%20alone%20by%2060%25%2C%20all%20while%20maintaining%0Aperformance%20comparable%20to%20fully%20supervised%20models.%20These%20findings%20suggest%20the%0Aeffectiveness%20of%20the%20GRN%20framework%20in%20optimizing%20segmentation%20performance%20with%0Asignificantly%20less%20labeled%20data%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%0Aultrasound%20image%20analysis%20and%20reducing%20the%20burdens%20associated%20with%20data%0Aannotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17690v2&entry.124074799=Read"},
{"title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation", "author": "Haiyang Zhou and Wangbo Yu and Jiawen Guan and Xinhua Cheng and Yonghong Tian and Li Yuan", "abstract": "  The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.\n", "link": "http://arxiv.org/abs/2504.21650v1", "date": "2025-04-30", "relevancy": 2.7237, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6853}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6853}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloTime%3A%20Taming%20Video%20Diffusion%20Models%20for%20Panoramic%204D%20Scene%0A%20%20Generation&body=Title%3A%20HoloTime%3A%20Taming%20Video%20Diffusion%20Models%20for%20Panoramic%204D%20Scene%0A%20%20Generation%0AAuthor%3A%20Haiyang%20Zhou%20and%20Wangbo%20Yu%20and%20Jiawen%20Guan%20and%20Xinhua%20Cheng%20and%20Yonghong%20Tian%20and%20Li%20Yuan%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20diffusion%20models%20holds%20the%20promise%20of%0Arevolutionizing%20the%20application%20of%20VR%20and%20AR%20technologies%2C%20which%20typically%0Arequire%20scene-level%204D%20assets%20for%20user%20experience.%20Nonetheless%2C%20existing%0Adiffusion%20models%20predominantly%20concentrate%20on%20modeling%20static%203D%20scenes%20or%0Aobject-level%20dynamics%2C%20constraining%20their%20capacity%20to%20provide%20truly%20immersive%0Aexperiences.%20To%20address%20this%20issue%2C%20we%20propose%20HoloTime%2C%20a%20framework%20that%0Aintegrates%20video%20diffusion%20models%20to%20generate%20panoramic%20videos%20from%20a%20single%0Aprompt%20or%20reference%20image%2C%20along%20with%20a%20360-degree%204D%20scene%20reconstruction%0Amethod%20that%20seamlessly%20transforms%20the%20generated%20panoramic%20video%20into%204D%20assets%2C%0Aenabling%20a%20fully%20immersive%204D%20experience%20for%20users.%20Specifically%2C%20to%20tame%20video%0Adiffusion%20models%20for%20generating%20high-fidelity%20panoramic%20videos%2C%20we%20introduce%0Athe%20360World%20dataset%2C%20the%20first%20comprehensive%20collection%20of%20panoramic%20videos%0Asuitable%20for%20downstream%204D%20scene%20reconstruction%20tasks.%20With%20this%20curated%0Adataset%2C%20we%20propose%20Panoramic%20Animator%2C%20a%20two-stage%20image-to-video%20diffusion%0Amodel%20that%20can%20convert%20panoramic%20images%20into%20high-quality%20panoramic%20videos.%0AFollowing%20this%2C%20we%20present%20Panoramic%20Space-Time%20Reconstruction%2C%20which%20leverages%0Aa%20space-time%20depth%20estimation%20method%20to%20transform%20the%20generated%20panoramic%0Avideos%20into%204D%20point%20clouds%2C%20enabling%20the%20optimization%20of%20a%20holistic%204D%0AGaussian%20Splatting%20representation%20to%20reconstruct%20spatially%20and%20temporally%0Aconsistent%204D%20scenes.%20To%20validate%20the%20efficacy%20of%20our%20method%2C%20we%20conducted%20a%0Acomparative%20analysis%20with%20existing%20approaches%2C%20revealing%20its%20superiority%20in%0Aboth%20panoramic%20video%20generation%20and%204D%20scene%20reconstruction.%20This%20demonstrates%0Aour%20method%27s%20capability%20to%20create%20more%20engaging%20and%20realistic%20immersive%0Aenvironments%2C%20thereby%20enhancing%20user%20experiences%20in%20VR%20and%20AR%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloTime%253A%2520Taming%2520Video%2520Diffusion%2520Models%2520for%2520Panoramic%25204D%2520Scene%250A%2520%2520Generation%26entry.906535625%3DHaiyang%2520Zhou%2520and%2520Wangbo%2520Yu%2520and%2520Jiawen%2520Guan%2520and%2520Xinhua%2520Cheng%2520and%2520Yonghong%2520Tian%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520diffusion%2520models%2520holds%2520the%2520promise%2520of%250Arevolutionizing%2520the%2520application%2520of%2520VR%2520and%2520AR%2520technologies%252C%2520which%2520typically%250Arequire%2520scene-level%25204D%2520assets%2520for%2520user%2520experience.%2520Nonetheless%252C%2520existing%250Adiffusion%2520models%2520predominantly%2520concentrate%2520on%2520modeling%2520static%25203D%2520scenes%2520or%250Aobject-level%2520dynamics%252C%2520constraining%2520their%2520capacity%2520to%2520provide%2520truly%2520immersive%250Aexperiences.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520HoloTime%252C%2520a%2520framework%2520that%250Aintegrates%2520video%2520diffusion%2520models%2520to%2520generate%2520panoramic%2520videos%2520from%2520a%2520single%250Aprompt%2520or%2520reference%2520image%252C%2520along%2520with%2520a%2520360-degree%25204D%2520scene%2520reconstruction%250Amethod%2520that%2520seamlessly%2520transforms%2520the%2520generated%2520panoramic%2520video%2520into%25204D%2520assets%252C%250Aenabling%2520a%2520fully%2520immersive%25204D%2520experience%2520for%2520users.%2520Specifically%252C%2520to%2520tame%2520video%250Adiffusion%2520models%2520for%2520generating%2520high-fidelity%2520panoramic%2520videos%252C%2520we%2520introduce%250Athe%2520360World%2520dataset%252C%2520the%2520first%2520comprehensive%2520collection%2520of%2520panoramic%2520videos%250Asuitable%2520for%2520downstream%25204D%2520scene%2520reconstruction%2520tasks.%2520With%2520this%2520curated%250Adataset%252C%2520we%2520propose%2520Panoramic%2520Animator%252C%2520a%2520two-stage%2520image-to-video%2520diffusion%250Amodel%2520that%2520can%2520convert%2520panoramic%2520images%2520into%2520high-quality%2520panoramic%2520videos.%250AFollowing%2520this%252C%2520we%2520present%2520Panoramic%2520Space-Time%2520Reconstruction%252C%2520which%2520leverages%250Aa%2520space-time%2520depth%2520estimation%2520method%2520to%2520transform%2520the%2520generated%2520panoramic%250Avideos%2520into%25204D%2520point%2520clouds%252C%2520enabling%2520the%2520optimization%2520of%2520a%2520holistic%25204D%250AGaussian%2520Splatting%2520representation%2520to%2520reconstruct%2520spatially%2520and%2520temporally%250Aconsistent%25204D%2520scenes.%2520To%2520validate%2520the%2520efficacy%2520of%2520our%2520method%252C%2520we%2520conducted%2520a%250Acomparative%2520analysis%2520with%2520existing%2520approaches%252C%2520revealing%2520its%2520superiority%2520in%250Aboth%2520panoramic%2520video%2520generation%2520and%25204D%2520scene%2520reconstruction.%2520This%2520demonstrates%250Aour%2520method%2527s%2520capability%2520to%2520create%2520more%2520engaging%2520and%2520realistic%2520immersive%250Aenvironments%252C%2520thereby%2520enhancing%2520user%2520experiences%2520in%2520VR%2520and%2520AR%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloTime%3A%20Taming%20Video%20Diffusion%20Models%20for%20Panoramic%204D%20Scene%0A%20%20Generation&entry.906535625=Haiyang%20Zhou%20and%20Wangbo%20Yu%20and%20Jiawen%20Guan%20and%20Xinhua%20Cheng%20and%20Yonghong%20Tian%20and%20Li%20Yuan&entry.1292438233=%20%20The%20rapid%20advancement%20of%20diffusion%20models%20holds%20the%20promise%20of%0Arevolutionizing%20the%20application%20of%20VR%20and%20AR%20technologies%2C%20which%20typically%0Arequire%20scene-level%204D%20assets%20for%20user%20experience.%20Nonetheless%2C%20existing%0Adiffusion%20models%20predominantly%20concentrate%20on%20modeling%20static%203D%20scenes%20or%0Aobject-level%20dynamics%2C%20constraining%20their%20capacity%20to%20provide%20truly%20immersive%0Aexperiences.%20To%20address%20this%20issue%2C%20we%20propose%20HoloTime%2C%20a%20framework%20that%0Aintegrates%20video%20diffusion%20models%20to%20generate%20panoramic%20videos%20from%20a%20single%0Aprompt%20or%20reference%20image%2C%20along%20with%20a%20360-degree%204D%20scene%20reconstruction%0Amethod%20that%20seamlessly%20transforms%20the%20generated%20panoramic%20video%20into%204D%20assets%2C%0Aenabling%20a%20fully%20immersive%204D%20experience%20for%20users.%20Specifically%2C%20to%20tame%20video%0Adiffusion%20models%20for%20generating%20high-fidelity%20panoramic%20videos%2C%20we%20introduce%0Athe%20360World%20dataset%2C%20the%20first%20comprehensive%20collection%20of%20panoramic%20videos%0Asuitable%20for%20downstream%204D%20scene%20reconstruction%20tasks.%20With%20this%20curated%0Adataset%2C%20we%20propose%20Panoramic%20Animator%2C%20a%20two-stage%20image-to-video%20diffusion%0Amodel%20that%20can%20convert%20panoramic%20images%20into%20high-quality%20panoramic%20videos.%0AFollowing%20this%2C%20we%20present%20Panoramic%20Space-Time%20Reconstruction%2C%20which%20leverages%0Aa%20space-time%20depth%20estimation%20method%20to%20transform%20the%20generated%20panoramic%0Avideos%20into%204D%20point%20clouds%2C%20enabling%20the%20optimization%20of%20a%20holistic%204D%0AGaussian%20Splatting%20representation%20to%20reconstruct%20spatially%20and%20temporally%0Aconsistent%204D%20scenes.%20To%20validate%20the%20efficacy%20of%20our%20method%2C%20we%20conducted%20a%0Acomparative%20analysis%20with%20existing%20approaches%2C%20revealing%20its%20superiority%20in%0Aboth%20panoramic%20video%20generation%20and%204D%20scene%20reconstruction.%20This%20demonstrates%0Aour%20method%27s%20capability%20to%20create%20more%20engaging%20and%20realistic%20immersive%0Aenvironments%2C%20thereby%20enhancing%20user%20experiences%20in%20VR%20and%20AR%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21650v1&entry.124074799=Read"},
{"title": "SAM4EM: Efficient memory-based two stage prompt-free segment anything\n  model adapter for complex 3D neuroscience electron microscopy stacks", "author": "Uzair Shah and Marco Agus and Daniya Boges and Vanessa Chiappini and Mahmood Alzubaidi and Jens Schneider and Markus Hadwiger and Pierre J. Magistretti and Mowafa Househ and Corrado Cal\u0131", "abstract": "  We present SAM4EM, a novel approach for 3D segmentation of complex neural\nstructures in electron microscopy (EM) data by leveraging the Segment Anything\nModel (SAM) alongside advanced fine-tuning strategies. Our contributions\ninclude the development of a prompt-free adapter for SAM using two stage mask\ndecoding to automatically generate prompt embeddings, a dual-stage fine-tuning\nmethod based on Low-Rank Adaptation (LoRA) for enhancing segmentation with\nlimited annotated data, and a 3D memory attention mechanism to ensure\nsegmentation consistency across 3D stacks. We further release a unique\nbenchmark dataset for the segmentation of astrocytic processes and synapses. We\nevaluated our method on challenging neuroscience segmentation benchmarks,\nspecifically targeting mitochondria, glia, and synapses, with significant\naccuracy improvements over state-of-the-art (SOTA) methods, including recent\nSAM-based adapters developed for the medical domain and other vision\ntransformer-based approaches. Experimental results indicate that our approach\noutperforms existing solutions in the segmentation of complex processes like\nglia and post-synaptic densities. Our code and models are available at\nhttps://github.com/Uzshah/SAM4EM.\n", "link": "http://arxiv.org/abs/2504.21544v1", "date": "2025-04-30", "relevancy": 2.7228, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM4EM%3A%20Efficient%20memory-based%20two%20stage%20prompt-free%20segment%20anything%0A%20%20model%20adapter%20for%20complex%203D%20neuroscience%20electron%20microscopy%20stacks&body=Title%3A%20SAM4EM%3A%20Efficient%20memory-based%20two%20stage%20prompt-free%20segment%20anything%0A%20%20model%20adapter%20for%20complex%203D%20neuroscience%20electron%20microscopy%20stacks%0AAuthor%3A%20Uzair%20Shah%20and%20Marco%20Agus%20and%20Daniya%20Boges%20and%20Vanessa%20Chiappini%20and%20Mahmood%20Alzubaidi%20and%20Jens%20Schneider%20and%20Markus%20Hadwiger%20and%20Pierre%20J.%20Magistretti%20and%20Mowafa%20Househ%20and%20Corrado%20Cal%C4%B1%0AAbstract%3A%20%20%20We%20present%20SAM4EM%2C%20a%20novel%20approach%20for%203D%20segmentation%20of%20complex%20neural%0Astructures%20in%20electron%20microscopy%20%28EM%29%20data%20by%20leveraging%20the%20Segment%20Anything%0AModel%20%28SAM%29%20alongside%20advanced%20fine-tuning%20strategies.%20Our%20contributions%0Ainclude%20the%20development%20of%20a%20prompt-free%20adapter%20for%20SAM%20using%20two%20stage%20mask%0Adecoding%20to%20automatically%20generate%20prompt%20embeddings%2C%20a%20dual-stage%20fine-tuning%0Amethod%20based%20on%20Low-Rank%20Adaptation%20%28LoRA%29%20for%20enhancing%20segmentation%20with%0Alimited%20annotated%20data%2C%20and%20a%203D%20memory%20attention%20mechanism%20to%20ensure%0Asegmentation%20consistency%20across%203D%20stacks.%20We%20further%20release%20a%20unique%0Abenchmark%20dataset%20for%20the%20segmentation%20of%20astrocytic%20processes%20and%20synapses.%20We%0Aevaluated%20our%20method%20on%20challenging%20neuroscience%20segmentation%20benchmarks%2C%0Aspecifically%20targeting%20mitochondria%2C%20glia%2C%20and%20synapses%2C%20with%20significant%0Aaccuracy%20improvements%20over%20state-of-the-art%20%28SOTA%29%20methods%2C%20including%20recent%0ASAM-based%20adapters%20developed%20for%20the%20medical%20domain%20and%20other%20vision%0Atransformer-based%20approaches.%20Experimental%20results%20indicate%20that%20our%20approach%0Aoutperforms%20existing%20solutions%20in%20the%20segmentation%20of%20complex%20processes%20like%0Aglia%20and%20post-synaptic%20densities.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/Uzshah/SAM4EM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM4EM%253A%2520Efficient%2520memory-based%2520two%2520stage%2520prompt-free%2520segment%2520anything%250A%2520%2520model%2520adapter%2520for%2520complex%25203D%2520neuroscience%2520electron%2520microscopy%2520stacks%26entry.906535625%3DUzair%2520Shah%2520and%2520Marco%2520Agus%2520and%2520Daniya%2520Boges%2520and%2520Vanessa%2520Chiappini%2520and%2520Mahmood%2520Alzubaidi%2520and%2520Jens%2520Schneider%2520and%2520Markus%2520Hadwiger%2520and%2520Pierre%2520J.%2520Magistretti%2520and%2520Mowafa%2520Househ%2520and%2520Corrado%2520Cal%25C4%25B1%26entry.1292438233%3D%2520%2520We%2520present%2520SAM4EM%252C%2520a%2520novel%2520approach%2520for%25203D%2520segmentation%2520of%2520complex%2520neural%250Astructures%2520in%2520electron%2520microscopy%2520%2528EM%2529%2520data%2520by%2520leveraging%2520the%2520Segment%2520Anything%250AModel%2520%2528SAM%2529%2520alongside%2520advanced%2520fine-tuning%2520strategies.%2520Our%2520contributions%250Ainclude%2520the%2520development%2520of%2520a%2520prompt-free%2520adapter%2520for%2520SAM%2520using%2520two%2520stage%2520mask%250Adecoding%2520to%2520automatically%2520generate%2520prompt%2520embeddings%252C%2520a%2520dual-stage%2520fine-tuning%250Amethod%2520based%2520on%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520for%2520enhancing%2520segmentation%2520with%250Alimited%2520annotated%2520data%252C%2520and%2520a%25203D%2520memory%2520attention%2520mechanism%2520to%2520ensure%250Asegmentation%2520consistency%2520across%25203D%2520stacks.%2520We%2520further%2520release%2520a%2520unique%250Abenchmark%2520dataset%2520for%2520the%2520segmentation%2520of%2520astrocytic%2520processes%2520and%2520synapses.%2520We%250Aevaluated%2520our%2520method%2520on%2520challenging%2520neuroscience%2520segmentation%2520benchmarks%252C%250Aspecifically%2520targeting%2520mitochondria%252C%2520glia%252C%2520and%2520synapses%252C%2520with%2520significant%250Aaccuracy%2520improvements%2520over%2520state-of-the-art%2520%2528SOTA%2529%2520methods%252C%2520including%2520recent%250ASAM-based%2520adapters%2520developed%2520for%2520the%2520medical%2520domain%2520and%2520other%2520vision%250Atransformer-based%2520approaches.%2520Experimental%2520results%2520indicate%2520that%2520our%2520approach%250Aoutperforms%2520existing%2520solutions%2520in%2520the%2520segmentation%2520of%2520complex%2520processes%2520like%250Aglia%2520and%2520post-synaptic%2520densities.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/Uzshah/SAM4EM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM4EM%3A%20Efficient%20memory-based%20two%20stage%20prompt-free%20segment%20anything%0A%20%20model%20adapter%20for%20complex%203D%20neuroscience%20electron%20microscopy%20stacks&entry.906535625=Uzair%20Shah%20and%20Marco%20Agus%20and%20Daniya%20Boges%20and%20Vanessa%20Chiappini%20and%20Mahmood%20Alzubaidi%20and%20Jens%20Schneider%20and%20Markus%20Hadwiger%20and%20Pierre%20J.%20Magistretti%20and%20Mowafa%20Househ%20and%20Corrado%20Cal%C4%B1&entry.1292438233=%20%20We%20present%20SAM4EM%2C%20a%20novel%20approach%20for%203D%20segmentation%20of%20complex%20neural%0Astructures%20in%20electron%20microscopy%20%28EM%29%20data%20by%20leveraging%20the%20Segment%20Anything%0AModel%20%28SAM%29%20alongside%20advanced%20fine-tuning%20strategies.%20Our%20contributions%0Ainclude%20the%20development%20of%20a%20prompt-free%20adapter%20for%20SAM%20using%20two%20stage%20mask%0Adecoding%20to%20automatically%20generate%20prompt%20embeddings%2C%20a%20dual-stage%20fine-tuning%0Amethod%20based%20on%20Low-Rank%20Adaptation%20%28LoRA%29%20for%20enhancing%20segmentation%20with%0Alimited%20annotated%20data%2C%20and%20a%203D%20memory%20attention%20mechanism%20to%20ensure%0Asegmentation%20consistency%20across%203D%20stacks.%20We%20further%20release%20a%20unique%0Abenchmark%20dataset%20for%20the%20segmentation%20of%20astrocytic%20processes%20and%20synapses.%20We%0Aevaluated%20our%20method%20on%20challenging%20neuroscience%20segmentation%20benchmarks%2C%0Aspecifically%20targeting%20mitochondria%2C%20glia%2C%20and%20synapses%2C%20with%20significant%0Aaccuracy%20improvements%20over%20state-of-the-art%20%28SOTA%29%20methods%2C%20including%20recent%0ASAM-based%20adapters%20developed%20for%20the%20medical%20domain%20and%20other%20vision%0Atransformer-based%20approaches.%20Experimental%20results%20indicate%20that%20our%20approach%0Aoutperforms%20existing%20solutions%20in%20the%20segmentation%20of%20complex%20processes%20like%0Aglia%20and%20post-synaptic%20densities.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/Uzshah/SAM4EM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21544v1&entry.124074799=Read"},
{"title": "VividListener: Expressive and Controllable Listener Dynamics Modeling\n  for Multi-Modal Responsive Interaction", "author": "Shiying Li and Xingqun Qi and Bingkun Yang and Chen Weile and Zezhao Tian and Muyi Sun and Qifeng Liu and Man Zhang and Zhenan Sun", "abstract": "  Generating responsive listener head dynamics with nuanced emotions and\nexpressive reactions is crucial for practical dialogue modeling in various\nvirtual avatar animations. Previous studies mainly focus on the direct\nshort-term production of listener behavior. They overlook the fine-grained\ncontrol over motion variations and emotional intensity, especially in\nlong-sequence modeling. Moreover, the lack of long-term and large-scale paired\nspeaker-listener corpora including head dynamics and fine-grained\nmulti-modality annotations (e.g., text-based expression descriptions, emotional\nintensity) also limits the application of dialogue modeling.Therefore, we first\nnewly collect a large-scale multi-turn dataset of 3D dyadic conversation\ncontaining more than 1.4M valid frames for multi-modal responsive interaction,\ndubbed ListenerX. Additionally, we propose VividListener, a novel framework\nenabling fine-grained, expressive and controllable listener dynamics modeling.\nThis framework leverages multi-modal conditions as guiding principles for\nfostering coherent interactions between speakers and listeners.Specifically, we\ndesign the Responsive Interaction Module (RIM) to adaptively represent the\nmulti-modal interactive embeddings. RIM ensures the listener dynamics achieve\nfine-grained semantic coordination with textual descriptions and adjustments,\nwhile preserving expressive reaction with speaker behavior. Meanwhile, we\ndesign the Emotional Intensity Tags (EIT) for emotion intensity editing with\nmulti-modal information integration, applying to both text descriptions and\nlistener motion amplitude.Extensive experiments conducted on our newly\ncollected ListenerX dataset demonstrate that VividListener achieves\nstate-of-the-art performance, realizing expressive and controllable listener\ndynamics.\n", "link": "http://arxiv.org/abs/2504.21718v1", "date": "2025-04-30", "relevancy": 2.717, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VividListener%3A%20Expressive%20and%20Controllable%20Listener%20Dynamics%20Modeling%0A%20%20for%20Multi-Modal%20Responsive%20Interaction&body=Title%3A%20VividListener%3A%20Expressive%20and%20Controllable%20Listener%20Dynamics%20Modeling%0A%20%20for%20Multi-Modal%20Responsive%20Interaction%0AAuthor%3A%20Shiying%20Li%20and%20Xingqun%20Qi%20and%20Bingkun%20Yang%20and%20Chen%20Weile%20and%20Zezhao%20Tian%20and%20Muyi%20Sun%20and%20Qifeng%20Liu%20and%20Man%20Zhang%20and%20Zhenan%20Sun%0AAbstract%3A%20%20%20Generating%20responsive%20listener%20head%20dynamics%20with%20nuanced%20emotions%20and%0Aexpressive%20reactions%20is%20crucial%20for%20practical%20dialogue%20modeling%20in%20various%0Avirtual%20avatar%20animations.%20Previous%20studies%20mainly%20focus%20on%20the%20direct%0Ashort-term%20production%20of%20listener%20behavior.%20They%20overlook%20the%20fine-grained%0Acontrol%20over%20motion%20variations%20and%20emotional%20intensity%2C%20especially%20in%0Along-sequence%20modeling.%20Moreover%2C%20the%20lack%20of%20long-term%20and%20large-scale%20paired%0Aspeaker-listener%20corpora%20including%20head%20dynamics%20and%20fine-grained%0Amulti-modality%20annotations%20%28e.g.%2C%20text-based%20expression%20descriptions%2C%20emotional%0Aintensity%29%20also%20limits%20the%20application%20of%20dialogue%20modeling.Therefore%2C%20we%20first%0Anewly%20collect%20a%20large-scale%20multi-turn%20dataset%20of%203D%20dyadic%20conversation%0Acontaining%20more%20than%201.4M%20valid%20frames%20for%20multi-modal%20responsive%20interaction%2C%0Adubbed%20ListenerX.%20Additionally%2C%20we%20propose%20VividListener%2C%20a%20novel%20framework%0Aenabling%20fine-grained%2C%20expressive%20and%20controllable%20listener%20dynamics%20modeling.%0AThis%20framework%20leverages%20multi-modal%20conditions%20as%20guiding%20principles%20for%0Afostering%20coherent%20interactions%20between%20speakers%20and%20listeners.Specifically%2C%20we%0Adesign%20the%20Responsive%20Interaction%20Module%20%28RIM%29%20to%20adaptively%20represent%20the%0Amulti-modal%20interactive%20embeddings.%20RIM%20ensures%20the%20listener%20dynamics%20achieve%0Afine-grained%20semantic%20coordination%20with%20textual%20descriptions%20and%20adjustments%2C%0Awhile%20preserving%20expressive%20reaction%20with%20speaker%20behavior.%20Meanwhile%2C%20we%0Adesign%20the%20Emotional%20Intensity%20Tags%20%28EIT%29%20for%20emotion%20intensity%20editing%20with%0Amulti-modal%20information%20integration%2C%20applying%20to%20both%20text%20descriptions%20and%0Alistener%20motion%20amplitude.Extensive%20experiments%20conducted%20on%20our%20newly%0Acollected%20ListenerX%20dataset%20demonstrate%20that%20VividListener%20achieves%0Astate-of-the-art%20performance%2C%20realizing%20expressive%20and%20controllable%20listener%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVividListener%253A%2520Expressive%2520and%2520Controllable%2520Listener%2520Dynamics%2520Modeling%250A%2520%2520for%2520Multi-Modal%2520Responsive%2520Interaction%26entry.906535625%3DShiying%2520Li%2520and%2520Xingqun%2520Qi%2520and%2520Bingkun%2520Yang%2520and%2520Chen%2520Weile%2520and%2520Zezhao%2520Tian%2520and%2520Muyi%2520Sun%2520and%2520Qifeng%2520Liu%2520and%2520Man%2520Zhang%2520and%2520Zhenan%2520Sun%26entry.1292438233%3D%2520%2520Generating%2520responsive%2520listener%2520head%2520dynamics%2520with%2520nuanced%2520emotions%2520and%250Aexpressive%2520reactions%2520is%2520crucial%2520for%2520practical%2520dialogue%2520modeling%2520in%2520various%250Avirtual%2520avatar%2520animations.%2520Previous%2520studies%2520mainly%2520focus%2520on%2520the%2520direct%250Ashort-term%2520production%2520of%2520listener%2520behavior.%2520They%2520overlook%2520the%2520fine-grained%250Acontrol%2520over%2520motion%2520variations%2520and%2520emotional%2520intensity%252C%2520especially%2520in%250Along-sequence%2520modeling.%2520Moreover%252C%2520the%2520lack%2520of%2520long-term%2520and%2520large-scale%2520paired%250Aspeaker-listener%2520corpora%2520including%2520head%2520dynamics%2520and%2520fine-grained%250Amulti-modality%2520annotations%2520%2528e.g.%252C%2520text-based%2520expression%2520descriptions%252C%2520emotional%250Aintensity%2529%2520also%2520limits%2520the%2520application%2520of%2520dialogue%2520modeling.Therefore%252C%2520we%2520first%250Anewly%2520collect%2520a%2520large-scale%2520multi-turn%2520dataset%2520of%25203D%2520dyadic%2520conversation%250Acontaining%2520more%2520than%25201.4M%2520valid%2520frames%2520for%2520multi-modal%2520responsive%2520interaction%252C%250Adubbed%2520ListenerX.%2520Additionally%252C%2520we%2520propose%2520VividListener%252C%2520a%2520novel%2520framework%250Aenabling%2520fine-grained%252C%2520expressive%2520and%2520controllable%2520listener%2520dynamics%2520modeling.%250AThis%2520framework%2520leverages%2520multi-modal%2520conditions%2520as%2520guiding%2520principles%2520for%250Afostering%2520coherent%2520interactions%2520between%2520speakers%2520and%2520listeners.Specifically%252C%2520we%250Adesign%2520the%2520Responsive%2520Interaction%2520Module%2520%2528RIM%2529%2520to%2520adaptively%2520represent%2520the%250Amulti-modal%2520interactive%2520embeddings.%2520RIM%2520ensures%2520the%2520listener%2520dynamics%2520achieve%250Afine-grained%2520semantic%2520coordination%2520with%2520textual%2520descriptions%2520and%2520adjustments%252C%250Awhile%2520preserving%2520expressive%2520reaction%2520with%2520speaker%2520behavior.%2520Meanwhile%252C%2520we%250Adesign%2520the%2520Emotional%2520Intensity%2520Tags%2520%2528EIT%2529%2520for%2520emotion%2520intensity%2520editing%2520with%250Amulti-modal%2520information%2520integration%252C%2520applying%2520to%2520both%2520text%2520descriptions%2520and%250Alistener%2520motion%2520amplitude.Extensive%2520experiments%2520conducted%2520on%2520our%2520newly%250Acollected%2520ListenerX%2520dataset%2520demonstrate%2520that%2520VividListener%2520achieves%250Astate-of-the-art%2520performance%252C%2520realizing%2520expressive%2520and%2520controllable%2520listener%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VividListener%3A%20Expressive%20and%20Controllable%20Listener%20Dynamics%20Modeling%0A%20%20for%20Multi-Modal%20Responsive%20Interaction&entry.906535625=Shiying%20Li%20and%20Xingqun%20Qi%20and%20Bingkun%20Yang%20and%20Chen%20Weile%20and%20Zezhao%20Tian%20and%20Muyi%20Sun%20and%20Qifeng%20Liu%20and%20Man%20Zhang%20and%20Zhenan%20Sun&entry.1292438233=%20%20Generating%20responsive%20listener%20head%20dynamics%20with%20nuanced%20emotions%20and%0Aexpressive%20reactions%20is%20crucial%20for%20practical%20dialogue%20modeling%20in%20various%0Avirtual%20avatar%20animations.%20Previous%20studies%20mainly%20focus%20on%20the%20direct%0Ashort-term%20production%20of%20listener%20behavior.%20They%20overlook%20the%20fine-grained%0Acontrol%20over%20motion%20variations%20and%20emotional%20intensity%2C%20especially%20in%0Along-sequence%20modeling.%20Moreover%2C%20the%20lack%20of%20long-term%20and%20large-scale%20paired%0Aspeaker-listener%20corpora%20including%20head%20dynamics%20and%20fine-grained%0Amulti-modality%20annotations%20%28e.g.%2C%20text-based%20expression%20descriptions%2C%20emotional%0Aintensity%29%20also%20limits%20the%20application%20of%20dialogue%20modeling.Therefore%2C%20we%20first%0Anewly%20collect%20a%20large-scale%20multi-turn%20dataset%20of%203D%20dyadic%20conversation%0Acontaining%20more%20than%201.4M%20valid%20frames%20for%20multi-modal%20responsive%20interaction%2C%0Adubbed%20ListenerX.%20Additionally%2C%20we%20propose%20VividListener%2C%20a%20novel%20framework%0Aenabling%20fine-grained%2C%20expressive%20and%20controllable%20listener%20dynamics%20modeling.%0AThis%20framework%20leverages%20multi-modal%20conditions%20as%20guiding%20principles%20for%0Afostering%20coherent%20interactions%20between%20speakers%20and%20listeners.Specifically%2C%20we%0Adesign%20the%20Responsive%20Interaction%20Module%20%28RIM%29%20to%20adaptively%20represent%20the%0Amulti-modal%20interactive%20embeddings.%20RIM%20ensures%20the%20listener%20dynamics%20achieve%0Afine-grained%20semantic%20coordination%20with%20textual%20descriptions%20and%20adjustments%2C%0Awhile%20preserving%20expressive%20reaction%20with%20speaker%20behavior.%20Meanwhile%2C%20we%0Adesign%20the%20Emotional%20Intensity%20Tags%20%28EIT%29%20for%20emotion%20intensity%20editing%20with%0Amulti-modal%20information%20integration%2C%20applying%20to%20both%20text%20descriptions%20and%0Alistener%20motion%20amplitude.Extensive%20experiments%20conducted%20on%20our%20newly%0Acollected%20ListenerX%20dataset%20demonstrate%20that%20VividListener%20achieves%0Astate-of-the-art%20performance%2C%20realizing%20expressive%20and%20controllable%20listener%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21718v1&entry.124074799=Read"},
{"title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction", "author": "Qihao Liu and Ju He and Qihang Yu and Liang-Chieh Chen and Alan Yuille", "abstract": "  In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.\n", "link": "http://arxiv.org/abs/2504.21855v1", "date": "2025-04-30", "relevancy": 2.6944, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7034}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6558}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReVision%3A%20High-Quality%2C%20Low-Cost%20Video%20Generation%20with%20Explicit%203D%0A%20%20Physics%20Modeling%20for%20Complex%20Motion%20and%20Interaction&body=Title%3A%20ReVision%3A%20High-Quality%2C%20Low-Cost%20Video%20Generation%20with%20Explicit%203D%0A%20%20Physics%20Modeling%20for%20Complex%20Motion%20and%20Interaction%0AAuthor%3A%20Qihao%20Liu%20and%20Ju%20He%20and%20Qihang%20Yu%20and%20Liang-Chieh%20Chen%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20In%20recent%20years%2C%20video%20generation%20has%20seen%20significant%20advancements.%20However%2C%0Achallenges%20still%20persist%20in%20generating%20complex%20motions%20and%20interactions.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20ReVision%2C%20a%20plug-and-play%20framework%20that%0Aexplicitly%20integrates%20parameterized%203D%20physical%20knowledge%20into%20a%20pretrained%0Aconditional%20video%20generation%20model%2C%20significantly%20enhancing%20its%20ability%20to%0Agenerate%20high-quality%20videos%20with%20complex%20motion%20and%20interactions.%0ASpecifically%2C%20ReVision%20consists%20of%20three%20stages.%20First%2C%20a%20video%20diffusion%20model%0Ais%20used%20to%20generate%20a%20coarse%20video.%20Next%2C%20we%20extract%20a%20set%20of%202D%20and%203D%0Afeatures%20from%20the%20coarse%20video%20to%20construct%20a%203D%20object-centric%20representation%2C%0Awhich%20is%20then%20refined%20by%20our%20proposed%20parameterized%20physical%20prior%20model%20to%0Aproduce%20an%20accurate%203D%20motion%20sequence.%20Finally%2C%20this%20refined%20motion%20sequence%0Ais%20fed%20back%20into%20the%20same%20video%20diffusion%20model%20as%20additional%20conditioning%2C%0Aenabling%20the%20generation%20of%20motion-consistent%20videos%2C%20even%20in%20scenarios%0Ainvolving%20complex%20actions%20and%20interactions.%20We%20validate%20the%20effectiveness%20of%0Aour%20approach%20on%20Stable%20Video%20Diffusion%2C%20where%20ReVision%20significantly%20improves%0Amotion%20fidelity%20and%20coherence.%20Remarkably%2C%20with%20only%201.5B%20parameters%2C%20it%20even%0Aoutperforms%20a%20state-of-the-art%20video%20generation%20model%20with%20over%2013B%20parameters%0Aon%20complex%20video%20generation%20by%20a%20substantial%20margin.%20Our%20results%20suggest%20that%2C%0Aby%20incorporating%203D%20physical%20knowledge%2C%20even%20a%20relatively%20small%20video%20diffusion%0Amodel%20can%20generate%20complex%20motions%20and%20interactions%20with%20greater%20realism%20and%0Acontrollability%2C%20offering%20a%20promising%20solution%20for%20physically%20plausible%20video%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReVision%253A%2520High-Quality%252C%2520Low-Cost%2520Video%2520Generation%2520with%2520Explicit%25203D%250A%2520%2520Physics%2520Modeling%2520for%2520Complex%2520Motion%2520and%2520Interaction%26entry.906535625%3DQihao%2520Liu%2520and%2520Ju%2520He%2520and%2520Qihang%2520Yu%2520and%2520Liang-Chieh%2520Chen%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520video%2520generation%2520has%2520seen%2520significant%2520advancements.%2520However%252C%250Achallenges%2520still%2520persist%2520in%2520generating%2520complex%2520motions%2520and%2520interactions.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520ReVision%252C%2520a%2520plug-and-play%2520framework%2520that%250Aexplicitly%2520integrates%2520parameterized%25203D%2520physical%2520knowledge%2520into%2520a%2520pretrained%250Aconditional%2520video%2520generation%2520model%252C%2520significantly%2520enhancing%2520its%2520ability%2520to%250Agenerate%2520high-quality%2520videos%2520with%2520complex%2520motion%2520and%2520interactions.%250ASpecifically%252C%2520ReVision%2520consists%2520of%2520three%2520stages.%2520First%252C%2520a%2520video%2520diffusion%2520model%250Ais%2520used%2520to%2520generate%2520a%2520coarse%2520video.%2520Next%252C%2520we%2520extract%2520a%2520set%2520of%25202D%2520and%25203D%250Afeatures%2520from%2520the%2520coarse%2520video%2520to%2520construct%2520a%25203D%2520object-centric%2520representation%252C%250Awhich%2520is%2520then%2520refined%2520by%2520our%2520proposed%2520parameterized%2520physical%2520prior%2520model%2520to%250Aproduce%2520an%2520accurate%25203D%2520motion%2520sequence.%2520Finally%252C%2520this%2520refined%2520motion%2520sequence%250Ais%2520fed%2520back%2520into%2520the%2520same%2520video%2520diffusion%2520model%2520as%2520additional%2520conditioning%252C%250Aenabling%2520the%2520generation%2520of%2520motion-consistent%2520videos%252C%2520even%2520in%2520scenarios%250Ainvolving%2520complex%2520actions%2520and%2520interactions.%2520We%2520validate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520on%2520Stable%2520Video%2520Diffusion%252C%2520where%2520ReVision%2520significantly%2520improves%250Amotion%2520fidelity%2520and%2520coherence.%2520Remarkably%252C%2520with%2520only%25201.5B%2520parameters%252C%2520it%2520even%250Aoutperforms%2520a%2520state-of-the-art%2520video%2520generation%2520model%2520with%2520over%252013B%2520parameters%250Aon%2520complex%2520video%2520generation%2520by%2520a%2520substantial%2520margin.%2520Our%2520results%2520suggest%2520that%252C%250Aby%2520incorporating%25203D%2520physical%2520knowledge%252C%2520even%2520a%2520relatively%2520small%2520video%2520diffusion%250Amodel%2520can%2520generate%2520complex%2520motions%2520and%2520interactions%2520with%2520greater%2520realism%2520and%250Acontrollability%252C%2520offering%2520a%2520promising%2520solution%2520for%2520physically%2520plausible%2520video%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReVision%3A%20High-Quality%2C%20Low-Cost%20Video%20Generation%20with%20Explicit%203D%0A%20%20Physics%20Modeling%20for%20Complex%20Motion%20and%20Interaction&entry.906535625=Qihao%20Liu%20and%20Ju%20He%20and%20Qihang%20Yu%20and%20Liang-Chieh%20Chen%20and%20Alan%20Yuille&entry.1292438233=%20%20In%20recent%20years%2C%20video%20generation%20has%20seen%20significant%20advancements.%20However%2C%0Achallenges%20still%20persist%20in%20generating%20complex%20motions%20and%20interactions.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20ReVision%2C%20a%20plug-and-play%20framework%20that%0Aexplicitly%20integrates%20parameterized%203D%20physical%20knowledge%20into%20a%20pretrained%0Aconditional%20video%20generation%20model%2C%20significantly%20enhancing%20its%20ability%20to%0Agenerate%20high-quality%20videos%20with%20complex%20motion%20and%20interactions.%0ASpecifically%2C%20ReVision%20consists%20of%20three%20stages.%20First%2C%20a%20video%20diffusion%20model%0Ais%20used%20to%20generate%20a%20coarse%20video.%20Next%2C%20we%20extract%20a%20set%20of%202D%20and%203D%0Afeatures%20from%20the%20coarse%20video%20to%20construct%20a%203D%20object-centric%20representation%2C%0Awhich%20is%20then%20refined%20by%20our%20proposed%20parameterized%20physical%20prior%20model%20to%0Aproduce%20an%20accurate%203D%20motion%20sequence.%20Finally%2C%20this%20refined%20motion%20sequence%0Ais%20fed%20back%20into%20the%20same%20video%20diffusion%20model%20as%20additional%20conditioning%2C%0Aenabling%20the%20generation%20of%20motion-consistent%20videos%2C%20even%20in%20scenarios%0Ainvolving%20complex%20actions%20and%20interactions.%20We%20validate%20the%20effectiveness%20of%0Aour%20approach%20on%20Stable%20Video%20Diffusion%2C%20where%20ReVision%20significantly%20improves%0Amotion%20fidelity%20and%20coherence.%20Remarkably%2C%20with%20only%201.5B%20parameters%2C%20it%20even%0Aoutperforms%20a%20state-of-the-art%20video%20generation%20model%20with%20over%2013B%20parameters%0Aon%20complex%20video%20generation%20by%20a%20substantial%20margin.%20Our%20results%20suggest%20that%2C%0Aby%20incorporating%203D%20physical%20knowledge%2C%20even%20a%20relatively%20small%20video%20diffusion%0Amodel%20can%20generate%20complex%20motions%20and%20interactions%20with%20greater%20realism%20and%0Acontrollability%2C%20offering%20a%20promising%20solution%20for%20physically%20plausible%20video%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21855v1&entry.124074799=Read"},
{"title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video\n  Summarization", "author": "Anas Anwarul Haq Khan and Utkarsh Verma and Prateek Chanda and Ganesh Ramakrishnan", "abstract": "  We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research.\n", "link": "http://arxiv.org/abs/2504.21831v1", "date": "2025-04-30", "relevancy": 2.6939, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Exit%20and%20Multi%20Stage%20Knowledge%20Distillation%20in%20VLMs%20for%20Video%0A%20%20Summarization&body=Title%3A%20Early%20Exit%20and%20Multi%20Stage%20Knowledge%20Distillation%20in%20VLMs%20for%20Video%0A%20%20Summarization%0AAuthor%3A%20Anas%20Anwarul%20Haq%20Khan%20and%20Utkarsh%20Verma%20and%20Prateek%20Chanda%20and%20Ganesh%20Ramakrishnan%0AAbstract%3A%20%20%20We%20introduce%20DEEVISum%20%28Distilled%20Early%20Exit%20Vision%20language%20model%20for%0ASummarization%29%2C%20a%20lightweight%2C%20efficient%2C%20and%20scalable%20vision%20language%20model%0Adesigned%20for%20segment%20wise%20video%20summarization.%20Leveraging%20multi%20modal%20prompts%0Athat%20combine%20textual%20and%20audio%20derived%20signals%2C%20DEEVISum%20incorporates%20Multi%0AStage%20Knowledge%20Distillation%20%28MSKD%29%20and%20Early%20Exit%20%28EE%29%20to%20strike%20a%20balance%0Abetween%20performance%20and%20efficiency.%20MSKD%20offers%20a%201.33%25%20absolute%20F1%20improvement%0Aover%20baseline%20distillation%20%280.5%25%29%2C%20while%20EE%20reduces%20inference%20time%20by%0Aapproximately%2021%25%20with%20a%201.3%20point%20drop%20in%20F1.%20Evaluated%20on%20the%20TVSum%20dataset%2C%0Aour%20best%20model%20PaLI%20Gemma2%203B%20%2B%20MSKD%20achieves%20an%20F1%20score%20of%2061.1%2C%20competing%0Athe%20performance%20of%20significantly%20larger%20models%2C%20all%20while%20maintaining%20a%20lower%0Acomputational%20footprint.%20We%20publicly%20release%20our%20code%20and%20processed%20dataset%20to%0Asupport%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Exit%2520and%2520Multi%2520Stage%2520Knowledge%2520Distillation%2520in%2520VLMs%2520for%2520Video%250A%2520%2520Summarization%26entry.906535625%3DAnas%2520Anwarul%2520Haq%2520Khan%2520and%2520Utkarsh%2520Verma%2520and%2520Prateek%2520Chanda%2520and%2520Ganesh%2520Ramakrishnan%26entry.1292438233%3D%2520%2520We%2520introduce%2520DEEVISum%2520%2528Distilled%2520Early%2520Exit%2520Vision%2520language%2520model%2520for%250ASummarization%2529%252C%2520a%2520lightweight%252C%2520efficient%252C%2520and%2520scalable%2520vision%2520language%2520model%250Adesigned%2520for%2520segment%2520wise%2520video%2520summarization.%2520Leveraging%2520multi%2520modal%2520prompts%250Athat%2520combine%2520textual%2520and%2520audio%2520derived%2520signals%252C%2520DEEVISum%2520incorporates%2520Multi%250AStage%2520Knowledge%2520Distillation%2520%2528MSKD%2529%2520and%2520Early%2520Exit%2520%2528EE%2529%2520to%2520strike%2520a%2520balance%250Abetween%2520performance%2520and%2520efficiency.%2520MSKD%2520offers%2520a%25201.33%2525%2520absolute%2520F1%2520improvement%250Aover%2520baseline%2520distillation%2520%25280.5%2525%2529%252C%2520while%2520EE%2520reduces%2520inference%2520time%2520by%250Aapproximately%252021%2525%2520with%2520a%25201.3%2520point%2520drop%2520in%2520F1.%2520Evaluated%2520on%2520the%2520TVSum%2520dataset%252C%250Aour%2520best%2520model%2520PaLI%2520Gemma2%25203B%2520%252B%2520MSKD%2520achieves%2520an%2520F1%2520score%2520of%252061.1%252C%2520competing%250Athe%2520performance%2520of%2520significantly%2520larger%2520models%252C%2520all%2520while%2520maintaining%2520a%2520lower%250Acomputational%2520footprint.%2520We%2520publicly%2520release%2520our%2520code%2520and%2520processed%2520dataset%2520to%250Asupport%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Exit%20and%20Multi%20Stage%20Knowledge%20Distillation%20in%20VLMs%20for%20Video%0A%20%20Summarization&entry.906535625=Anas%20Anwarul%20Haq%20Khan%20and%20Utkarsh%20Verma%20and%20Prateek%20Chanda%20and%20Ganesh%20Ramakrishnan&entry.1292438233=%20%20We%20introduce%20DEEVISum%20%28Distilled%20Early%20Exit%20Vision%20language%20model%20for%0ASummarization%29%2C%20a%20lightweight%2C%20efficient%2C%20and%20scalable%20vision%20language%20model%0Adesigned%20for%20segment%20wise%20video%20summarization.%20Leveraging%20multi%20modal%20prompts%0Athat%20combine%20textual%20and%20audio%20derived%20signals%2C%20DEEVISum%20incorporates%20Multi%0AStage%20Knowledge%20Distillation%20%28MSKD%29%20and%20Early%20Exit%20%28EE%29%20to%20strike%20a%20balance%0Abetween%20performance%20and%20efficiency.%20MSKD%20offers%20a%201.33%25%20absolute%20F1%20improvement%0Aover%20baseline%20distillation%20%280.5%25%29%2C%20while%20EE%20reduces%20inference%20time%20by%0Aapproximately%2021%25%20with%20a%201.3%20point%20drop%20in%20F1.%20Evaluated%20on%20the%20TVSum%20dataset%2C%0Aour%20best%20model%20PaLI%20Gemma2%203B%20%2B%20MSKD%20achieves%20an%20F1%20score%20of%2061.1%2C%20competing%0Athe%20performance%20of%20significantly%20larger%20models%2C%20all%20while%20maintaining%20a%20lower%0Acomputational%20footprint.%20We%20publicly%20release%20our%20code%20and%20processed%20dataset%20to%0Asupport%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21831v1&entry.124074799=Read"},
{"title": "ObjectFinder: An Open-Vocabulary Assistive System for Interactive Object\n  Search by Blind People", "author": "Ruiping Liu and Jiaming Zhang and Angela Sch\u00f6n and Karin M\u00fcller and Junwei Zheng and Kailun Yang and Anhong Guo and Kathrin Gerling and Rainer Stiefelhagen", "abstract": "  Searching for objects in unfamiliar scenarios is a challenging task for blind\npeople. It involves specifying the target object, detecting it, and then\ngathering detailed information according to the user's intent. However,\nexisting description- and detection-based assistive technologies do not\nsufficiently support the multifaceted nature of interactive object search\ntasks. We present ObjectFinder, an open-vocabulary wearable assistive system\nfor interactive object search by blind people. ObjectFinder allows users to\nquery target objects using flexible wording. Once the target object is\ndetected, it provides egocentric localization information in real-time,\nincluding distance and direction. Users can then initiate different branches to\ngather detailed information based on their intent towards the target object,\nsuch as navigating to it or perceiving its surroundings. ObjectFinder is\npowered by a seamless combination of open-vocabulary models, namely an\nopen-vocabulary object detector and a multimodal large language model. The\nObjectFinder design concept and its development were carried out in\ncollaboration with a blind co-designer. To evaluate ObjectFinder, we conducted\nan exploratory user study with eight blind participants. We compared\nObjectFinder to BeMyAI and Google Lookout, popular description- and\ndetection-based assistive applications. Our findings indicate that most\nparticipants felt more independent with ObjectFinder and preferred it for\nobject search, as it enhanced scene context gathering and navigation, and\nallowed for active target identification. Finally, we discuss the implications\nfor future assistive systems to support interactive object search.\n", "link": "http://arxiv.org/abs/2412.03118v2", "date": "2025-04-30", "relevancy": 2.677, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjectFinder%3A%20An%20Open-Vocabulary%20Assistive%20System%20for%20Interactive%20Object%0A%20%20Search%20by%20Blind%20People&body=Title%3A%20ObjectFinder%3A%20An%20Open-Vocabulary%20Assistive%20System%20for%20Interactive%20Object%0A%20%20Search%20by%20Blind%20People%0AAuthor%3A%20Ruiping%20Liu%20and%20Jiaming%20Zhang%20and%20Angela%20Sch%C3%B6n%20and%20Karin%20M%C3%BCller%20and%20Junwei%20Zheng%20and%20Kailun%20Yang%20and%20Anhong%20Guo%20and%20Kathrin%20Gerling%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20Searching%20for%20objects%20in%20unfamiliar%20scenarios%20is%20a%20challenging%20task%20for%20blind%0Apeople.%20It%20involves%20specifying%20the%20target%20object%2C%20detecting%20it%2C%20and%20then%0Agathering%20detailed%20information%20according%20to%20the%20user%27s%20intent.%20However%2C%0Aexisting%20description-%20and%20detection-based%20assistive%20technologies%20do%20not%0Asufficiently%20support%20the%20multifaceted%20nature%20of%20interactive%20object%20search%0Atasks.%20We%20present%20ObjectFinder%2C%20an%20open-vocabulary%20wearable%20assistive%20system%0Afor%20interactive%20object%20search%20by%20blind%20people.%20ObjectFinder%20allows%20users%20to%0Aquery%20target%20objects%20using%20flexible%20wording.%20Once%20the%20target%20object%20is%0Adetected%2C%20it%20provides%20egocentric%20localization%20information%20in%20real-time%2C%0Aincluding%20distance%20and%20direction.%20Users%20can%20then%20initiate%20different%20branches%20to%0Agather%20detailed%20information%20based%20on%20their%20intent%20towards%20the%20target%20object%2C%0Asuch%20as%20navigating%20to%20it%20or%20perceiving%20its%20surroundings.%20ObjectFinder%20is%0Apowered%20by%20a%20seamless%20combination%20of%20open-vocabulary%20models%2C%20namely%20an%0Aopen-vocabulary%20object%20detector%20and%20a%20multimodal%20large%20language%20model.%20The%0AObjectFinder%20design%20concept%20and%20its%20development%20were%20carried%20out%20in%0Acollaboration%20with%20a%20blind%20co-designer.%20To%20evaluate%20ObjectFinder%2C%20we%20conducted%0Aan%20exploratory%20user%20study%20with%20eight%20blind%20participants.%20We%20compared%0AObjectFinder%20to%20BeMyAI%20and%20Google%20Lookout%2C%20popular%20description-%20and%0Adetection-based%20assistive%20applications.%20Our%20findings%20indicate%20that%20most%0Aparticipants%20felt%20more%20independent%20with%20ObjectFinder%20and%20preferred%20it%20for%0Aobject%20search%2C%20as%20it%20enhanced%20scene%20context%20gathering%20and%20navigation%2C%20and%0Aallowed%20for%20active%20target%20identification.%20Finally%2C%20we%20discuss%20the%20implications%0Afor%20future%20assistive%20systems%20to%20support%20interactive%20object%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjectFinder%253A%2520An%2520Open-Vocabulary%2520Assistive%2520System%2520for%2520Interactive%2520Object%250A%2520%2520Search%2520by%2520Blind%2520People%26entry.906535625%3DRuiping%2520Liu%2520and%2520Jiaming%2520Zhang%2520and%2520Angela%2520Sch%25C3%25B6n%2520and%2520Karin%2520M%25C3%25BCller%2520and%2520Junwei%2520Zheng%2520and%2520Kailun%2520Yang%2520and%2520Anhong%2520Guo%2520and%2520Kathrin%2520Gerling%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520Searching%2520for%2520objects%2520in%2520unfamiliar%2520scenarios%2520is%2520a%2520challenging%2520task%2520for%2520blind%250Apeople.%2520It%2520involves%2520specifying%2520the%2520target%2520object%252C%2520detecting%2520it%252C%2520and%2520then%250Agathering%2520detailed%2520information%2520according%2520to%2520the%2520user%2527s%2520intent.%2520However%252C%250Aexisting%2520description-%2520and%2520detection-based%2520assistive%2520technologies%2520do%2520not%250Asufficiently%2520support%2520the%2520multifaceted%2520nature%2520of%2520interactive%2520object%2520search%250Atasks.%2520We%2520present%2520ObjectFinder%252C%2520an%2520open-vocabulary%2520wearable%2520assistive%2520system%250Afor%2520interactive%2520object%2520search%2520by%2520blind%2520people.%2520ObjectFinder%2520allows%2520users%2520to%250Aquery%2520target%2520objects%2520using%2520flexible%2520wording.%2520Once%2520the%2520target%2520object%2520is%250Adetected%252C%2520it%2520provides%2520egocentric%2520localization%2520information%2520in%2520real-time%252C%250Aincluding%2520distance%2520and%2520direction.%2520Users%2520can%2520then%2520initiate%2520different%2520branches%2520to%250Agather%2520detailed%2520information%2520based%2520on%2520their%2520intent%2520towards%2520the%2520target%2520object%252C%250Asuch%2520as%2520navigating%2520to%2520it%2520or%2520perceiving%2520its%2520surroundings.%2520ObjectFinder%2520is%250Apowered%2520by%2520a%2520seamless%2520combination%2520of%2520open-vocabulary%2520models%252C%2520namely%2520an%250Aopen-vocabulary%2520object%2520detector%2520and%2520a%2520multimodal%2520large%2520language%2520model.%2520The%250AObjectFinder%2520design%2520concept%2520and%2520its%2520development%2520were%2520carried%2520out%2520in%250Acollaboration%2520with%2520a%2520blind%2520co-designer.%2520To%2520evaluate%2520ObjectFinder%252C%2520we%2520conducted%250Aan%2520exploratory%2520user%2520study%2520with%2520eight%2520blind%2520participants.%2520We%2520compared%250AObjectFinder%2520to%2520BeMyAI%2520and%2520Google%2520Lookout%252C%2520popular%2520description-%2520and%250Adetection-based%2520assistive%2520applications.%2520Our%2520findings%2520indicate%2520that%2520most%250Aparticipants%2520felt%2520more%2520independent%2520with%2520ObjectFinder%2520and%2520preferred%2520it%2520for%250Aobject%2520search%252C%2520as%2520it%2520enhanced%2520scene%2520context%2520gathering%2520and%2520navigation%252C%2520and%250Aallowed%2520for%2520active%2520target%2520identification.%2520Finally%252C%2520we%2520discuss%2520the%2520implications%250Afor%2520future%2520assistive%2520systems%2520to%2520support%2520interactive%2520object%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjectFinder%3A%20An%20Open-Vocabulary%20Assistive%20System%20for%20Interactive%20Object%0A%20%20Search%20by%20Blind%20People&entry.906535625=Ruiping%20Liu%20and%20Jiaming%20Zhang%20and%20Angela%20Sch%C3%B6n%20and%20Karin%20M%C3%BCller%20and%20Junwei%20Zheng%20and%20Kailun%20Yang%20and%20Anhong%20Guo%20and%20Kathrin%20Gerling%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20Searching%20for%20objects%20in%20unfamiliar%20scenarios%20is%20a%20challenging%20task%20for%20blind%0Apeople.%20It%20involves%20specifying%20the%20target%20object%2C%20detecting%20it%2C%20and%20then%0Agathering%20detailed%20information%20according%20to%20the%20user%27s%20intent.%20However%2C%0Aexisting%20description-%20and%20detection-based%20assistive%20technologies%20do%20not%0Asufficiently%20support%20the%20multifaceted%20nature%20of%20interactive%20object%20search%0Atasks.%20We%20present%20ObjectFinder%2C%20an%20open-vocabulary%20wearable%20assistive%20system%0Afor%20interactive%20object%20search%20by%20blind%20people.%20ObjectFinder%20allows%20users%20to%0Aquery%20target%20objects%20using%20flexible%20wording.%20Once%20the%20target%20object%20is%0Adetected%2C%20it%20provides%20egocentric%20localization%20information%20in%20real-time%2C%0Aincluding%20distance%20and%20direction.%20Users%20can%20then%20initiate%20different%20branches%20to%0Agather%20detailed%20information%20based%20on%20their%20intent%20towards%20the%20target%20object%2C%0Asuch%20as%20navigating%20to%20it%20or%20perceiving%20its%20surroundings.%20ObjectFinder%20is%0Apowered%20by%20a%20seamless%20combination%20of%20open-vocabulary%20models%2C%20namely%20an%0Aopen-vocabulary%20object%20detector%20and%20a%20multimodal%20large%20language%20model.%20The%0AObjectFinder%20design%20concept%20and%20its%20development%20were%20carried%20out%20in%0Acollaboration%20with%20a%20blind%20co-designer.%20To%20evaluate%20ObjectFinder%2C%20we%20conducted%0Aan%20exploratory%20user%20study%20with%20eight%20blind%20participants.%20We%20compared%0AObjectFinder%20to%20BeMyAI%20and%20Google%20Lookout%2C%20popular%20description-%20and%0Adetection-based%20assistive%20applications.%20Our%20findings%20indicate%20that%20most%0Aparticipants%20felt%20more%20independent%20with%20ObjectFinder%20and%20preferred%20it%20for%0Aobject%20search%2C%20as%20it%20enhanced%20scene%20context%20gathering%20and%20navigation%2C%20and%0Aallowed%20for%20active%20target%20identification.%20Finally%2C%20we%20discuss%20the%20implications%0Afor%20future%20assistive%20systems%20to%20support%20interactive%20object%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03118v2&entry.124074799=Read"},
{"title": "BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection\n  between Point and Text Prompts", "author": "Suzhe Xu and Jialin Peng and Chengyuan Zhang", "abstract": "  Segmentation is a fundamental task in computer vision, with prompt-driven\nmethods gaining prominence due to their flexibility. The Segment Anything Model\n(SAM) excels at point-prompted segmentation, while text-based models, often\nleveraging powerful multimodal encoders like BEIT-3, provide rich semantic\nunderstanding. However, effectively combining these complementary modalities\nremains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal\nprompt segmentation framework employing an explicit selection mechanism. We\nleverage SAM's ability to generate multiple mask candidates from a single point\nprompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select\nthe point-generated mask that best aligns spatially, measured by Intersection\nover Union (IoU). This approach, interpretable as a simplified Mixture of\nExperts (MoE), effectively fuses spatial precision and semantic context without\ncomplex model modifications. Notably, our method achieves strong zero-shot\nperformance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using\nonly a single point prompt per instance. This significantly reduces annotation\nburden compared to bounding boxes and aligns better with practical clinical\nworkflows, demonstrating the method's effectiveness without domain-specific\ntraining. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8%\nIoU, significantly outperforming existing approaches. Experiments show\nBiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic\ndisambiguation, offering a simple, effective, and interpretable perspective on\nmulti-modal prompt fusion.\n", "link": "http://arxiv.org/abs/2503.19769v2", "date": "2025-04-30", "relevancy": 2.655, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiPrompt-SAM%3A%20Enhancing%20Image%20Segmentation%20via%20Explicit%20Selection%0A%20%20between%20Point%20and%20Text%20Prompts&body=Title%3A%20BiPrompt-SAM%3A%20Enhancing%20Image%20Segmentation%20via%20Explicit%20Selection%0A%20%20between%20Point%20and%20Text%20Prompts%0AAuthor%3A%20Suzhe%20Xu%20and%20Jialin%20Peng%20and%20Chengyuan%20Zhang%0AAbstract%3A%20%20%20Segmentation%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20with%20prompt-driven%0Amethods%20gaining%20prominence%20due%20to%20their%20flexibility.%20The%20Segment%20Anything%20Model%0A%28SAM%29%20excels%20at%20point-prompted%20segmentation%2C%20while%20text-based%20models%2C%20often%0Aleveraging%20powerful%20multimodal%20encoders%20like%20BEIT-3%2C%20provide%20rich%20semantic%0Aunderstanding.%20However%2C%20effectively%20combining%20these%20complementary%20modalities%0Aremains%20a%20challenge.%20This%20paper%20introduces%20BiPrompt-SAM%2C%20a%20novel%20dual-modal%0Aprompt%20segmentation%20framework%20employing%20an%20explicit%20selection%20mechanism.%20We%0Aleverage%20SAM%27s%20ability%20to%20generate%20multiple%20mask%20candidates%20from%20a%20single%20point%0Aprompt%20and%20use%20a%20text-guided%20mask%20%28generated%20via%20EVF-SAM%20with%20BEIT-3%29%20to%20select%0Athe%20point-generated%20mask%20that%20best%20aligns%20spatially%2C%20measured%20by%20Intersection%0Aover%20Union%20%28IoU%29.%20This%20approach%2C%20interpretable%20as%20a%20simplified%20Mixture%20of%0AExperts%20%28MoE%29%2C%20effectively%20fuses%20spatial%20precision%20and%20semantic%20context%20without%0Acomplex%20model%20modifications.%20Notably%2C%20our%20method%20achieves%20strong%20zero-shot%0Aperformance%20on%20the%20Endovis17%20medical%20dataset%20%2889.55%25%20mDice%2C%2081.46%25%20mIoU%29%20using%0Aonly%20a%20single%20point%20prompt%20per%20instance.%20This%20significantly%20reduces%20annotation%0Aburden%20compared%20to%20bounding%20boxes%20and%20aligns%20better%20with%20practical%20clinical%0Aworkflows%2C%20demonstrating%20the%20method%27s%20effectiveness%20without%20domain-specific%0Atraining.%20On%20the%20RefCOCO%20series%2C%20BiPrompt-SAM%20attained%2087.1%25%2C%2086.5%25%2C%20and%2085.8%25%0AIoU%2C%20significantly%20outperforming%20existing%20approaches.%20Experiments%20show%0ABiPrompt-SAM%20excels%20in%20scenarios%20requiring%20both%20spatial%20accuracy%20and%20semantic%0Adisambiguation%2C%20offering%20a%20simple%2C%20effective%2C%20and%20interpretable%20perspective%20on%0Amulti-modal%20prompt%20fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiPrompt-SAM%253A%2520Enhancing%2520Image%2520Segmentation%2520via%2520Explicit%2520Selection%250A%2520%2520between%2520Point%2520and%2520Text%2520Prompts%26entry.906535625%3DSuzhe%2520Xu%2520and%2520Jialin%2520Peng%2520and%2520Chengyuan%2520Zhang%26entry.1292438233%3D%2520%2520Segmentation%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520with%2520prompt-driven%250Amethods%2520gaining%2520prominence%2520due%2520to%2520their%2520flexibility.%2520The%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%2520excels%2520at%2520point-prompted%2520segmentation%252C%2520while%2520text-based%2520models%252C%2520often%250Aleveraging%2520powerful%2520multimodal%2520encoders%2520like%2520BEIT-3%252C%2520provide%2520rich%2520semantic%250Aunderstanding.%2520However%252C%2520effectively%2520combining%2520these%2520complementary%2520modalities%250Aremains%2520a%2520challenge.%2520This%2520paper%2520introduces%2520BiPrompt-SAM%252C%2520a%2520novel%2520dual-modal%250Aprompt%2520segmentation%2520framework%2520employing%2520an%2520explicit%2520selection%2520mechanism.%2520We%250Aleverage%2520SAM%2527s%2520ability%2520to%2520generate%2520multiple%2520mask%2520candidates%2520from%2520a%2520single%2520point%250Aprompt%2520and%2520use%2520a%2520text-guided%2520mask%2520%2528generated%2520via%2520EVF-SAM%2520with%2520BEIT-3%2529%2520to%2520select%250Athe%2520point-generated%2520mask%2520that%2520best%2520aligns%2520spatially%252C%2520measured%2520by%2520Intersection%250Aover%2520Union%2520%2528IoU%2529.%2520This%2520approach%252C%2520interpretable%2520as%2520a%2520simplified%2520Mixture%2520of%250AExperts%2520%2528MoE%2529%252C%2520effectively%2520fuses%2520spatial%2520precision%2520and%2520semantic%2520context%2520without%250Acomplex%2520model%2520modifications.%2520Notably%252C%2520our%2520method%2520achieves%2520strong%2520zero-shot%250Aperformance%2520on%2520the%2520Endovis17%2520medical%2520dataset%2520%252889.55%2525%2520mDice%252C%252081.46%2525%2520mIoU%2529%2520using%250Aonly%2520a%2520single%2520point%2520prompt%2520per%2520instance.%2520This%2520significantly%2520reduces%2520annotation%250Aburden%2520compared%2520to%2520bounding%2520boxes%2520and%2520aligns%2520better%2520with%2520practical%2520clinical%250Aworkflows%252C%2520demonstrating%2520the%2520method%2527s%2520effectiveness%2520without%2520domain-specific%250Atraining.%2520On%2520the%2520RefCOCO%2520series%252C%2520BiPrompt-SAM%2520attained%252087.1%2525%252C%252086.5%2525%252C%2520and%252085.8%2525%250AIoU%252C%2520significantly%2520outperforming%2520existing%2520approaches.%2520Experiments%2520show%250ABiPrompt-SAM%2520excels%2520in%2520scenarios%2520requiring%2520both%2520spatial%2520accuracy%2520and%2520semantic%250Adisambiguation%252C%2520offering%2520a%2520simple%252C%2520effective%252C%2520and%2520interpretable%2520perspective%2520on%250Amulti-modal%2520prompt%2520fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiPrompt-SAM%3A%20Enhancing%20Image%20Segmentation%20via%20Explicit%20Selection%0A%20%20between%20Point%20and%20Text%20Prompts&entry.906535625=Suzhe%20Xu%20and%20Jialin%20Peng%20and%20Chengyuan%20Zhang&entry.1292438233=%20%20Segmentation%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20with%20prompt-driven%0Amethods%20gaining%20prominence%20due%20to%20their%20flexibility.%20The%20Segment%20Anything%20Model%0A%28SAM%29%20excels%20at%20point-prompted%20segmentation%2C%20while%20text-based%20models%2C%20often%0Aleveraging%20powerful%20multimodal%20encoders%20like%20BEIT-3%2C%20provide%20rich%20semantic%0Aunderstanding.%20However%2C%20effectively%20combining%20these%20complementary%20modalities%0Aremains%20a%20challenge.%20This%20paper%20introduces%20BiPrompt-SAM%2C%20a%20novel%20dual-modal%0Aprompt%20segmentation%20framework%20employing%20an%20explicit%20selection%20mechanism.%20We%0Aleverage%20SAM%27s%20ability%20to%20generate%20multiple%20mask%20candidates%20from%20a%20single%20point%0Aprompt%20and%20use%20a%20text-guided%20mask%20%28generated%20via%20EVF-SAM%20with%20BEIT-3%29%20to%20select%0Athe%20point-generated%20mask%20that%20best%20aligns%20spatially%2C%20measured%20by%20Intersection%0Aover%20Union%20%28IoU%29.%20This%20approach%2C%20interpretable%20as%20a%20simplified%20Mixture%20of%0AExperts%20%28MoE%29%2C%20effectively%20fuses%20spatial%20precision%20and%20semantic%20context%20without%0Acomplex%20model%20modifications.%20Notably%2C%20our%20method%20achieves%20strong%20zero-shot%0Aperformance%20on%20the%20Endovis17%20medical%20dataset%20%2889.55%25%20mDice%2C%2081.46%25%20mIoU%29%20using%0Aonly%20a%20single%20point%20prompt%20per%20instance.%20This%20significantly%20reduces%20annotation%0Aburden%20compared%20to%20bounding%20boxes%20and%20aligns%20better%20with%20practical%20clinical%0Aworkflows%2C%20demonstrating%20the%20method%27s%20effectiveness%20without%20domain-specific%0Atraining.%20On%20the%20RefCOCO%20series%2C%20BiPrompt-SAM%20attained%2087.1%25%2C%2086.5%25%2C%20and%2085.8%25%0AIoU%2C%20significantly%20outperforming%20existing%20approaches.%20Experiments%20show%0ABiPrompt-SAM%20excels%20in%20scenarios%20requiring%20both%20spatial%20accuracy%20and%20semantic%0Adisambiguation%2C%20offering%20a%20simple%2C%20effective%2C%20and%20interpretable%20perspective%20on%0Amulti-modal%20prompt%20fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19769v2&entry.124074799=Read"},
{"title": "Recursive KL Divergence Optimization: A Dynamic Framework for\n  Representation Learning", "author": "Anthony D Martin", "abstract": "  We propose a generalization of modern representation learning objectives by\nreframing them as recursive divergence alignment processes over localized\nconditional distributions While recent frameworks like Information Contrastive\nLearning I-Con unify multiple learning paradigms through KL divergence between\nfixed neighborhood conditionals we argue this view underplays a crucial\nrecursive structure inherent in the learning process. We introduce Recursive KL\nDivergence Optimization RKDO a dynamic formalism where representation learning\nis framed as the evolution of KL divergences across data neighborhoods. This\nformulation captures contrastive clustering and dimensionality reduction\nmethods as static slices while offering a new path to model stability and local\nadaptation. Our experiments demonstrate that RKDO offers dual efficiency\nadvantages approximately 30 percent lower loss values compared to static\napproaches across three different datasets and 60 to 80 percent reduction in\ncomputational resources needed to achieve comparable results. This suggests\nthat RKDOs recursive updating mechanism provides a fundamentally more efficient\noptimization landscape for representation learning with significant\nimplications for resource constrained applications.\n", "link": "http://arxiv.org/abs/2504.21707v1", "date": "2025-04-30", "relevancy": 2.6162, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20KL%20Divergence%20Optimization%3A%20A%20Dynamic%20Framework%20for%0A%20%20Representation%20Learning&body=Title%3A%20Recursive%20KL%20Divergence%20Optimization%3A%20A%20Dynamic%20Framework%20for%0A%20%20Representation%20Learning%0AAuthor%3A%20Anthony%20D%20Martin%0AAbstract%3A%20%20%20We%20propose%20a%20generalization%20of%20modern%20representation%20learning%20objectives%20by%0Areframing%20them%20as%20recursive%20divergence%20alignment%20processes%20over%20localized%0Aconditional%20distributions%20While%20recent%20frameworks%20like%20Information%20Contrastive%0ALearning%20I-Con%20unify%20multiple%20learning%20paradigms%20through%20KL%20divergence%20between%0Afixed%20neighborhood%20conditionals%20we%20argue%20this%20view%20underplays%20a%20crucial%0Arecursive%20structure%20inherent%20in%20the%20learning%20process.%20We%20introduce%20Recursive%20KL%0ADivergence%20Optimization%20RKDO%20a%20dynamic%20formalism%20where%20representation%20learning%0Ais%20framed%20as%20the%20evolution%20of%20KL%20divergences%20across%20data%20neighborhoods.%20This%0Aformulation%20captures%20contrastive%20clustering%20and%20dimensionality%20reduction%0Amethods%20as%20static%20slices%20while%20offering%20a%20new%20path%20to%20model%20stability%20and%20local%0Aadaptation.%20Our%20experiments%20demonstrate%20that%20RKDO%20offers%20dual%20efficiency%0Aadvantages%20approximately%2030%20percent%20lower%20loss%20values%20compared%20to%20static%0Aapproaches%20across%20three%20different%20datasets%20and%2060%20to%2080%20percent%20reduction%20in%0Acomputational%20resources%20needed%20to%20achieve%20comparable%20results.%20This%20suggests%0Athat%20RKDOs%20recursive%20updating%20mechanism%20provides%20a%20fundamentally%20more%20efficient%0Aoptimization%20landscape%20for%20representation%20learning%20with%20significant%0Aimplications%20for%20resource%20constrained%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520KL%2520Divergence%2520Optimization%253A%2520A%2520Dynamic%2520Framework%2520for%250A%2520%2520Representation%2520Learning%26entry.906535625%3DAnthony%2520D%2520Martin%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520generalization%2520of%2520modern%2520representation%2520learning%2520objectives%2520by%250Areframing%2520them%2520as%2520recursive%2520divergence%2520alignment%2520processes%2520over%2520localized%250Aconditional%2520distributions%2520While%2520recent%2520frameworks%2520like%2520Information%2520Contrastive%250ALearning%2520I-Con%2520unify%2520multiple%2520learning%2520paradigms%2520through%2520KL%2520divergence%2520between%250Afixed%2520neighborhood%2520conditionals%2520we%2520argue%2520this%2520view%2520underplays%2520a%2520crucial%250Arecursive%2520structure%2520inherent%2520in%2520the%2520learning%2520process.%2520We%2520introduce%2520Recursive%2520KL%250ADivergence%2520Optimization%2520RKDO%2520a%2520dynamic%2520formalism%2520where%2520representation%2520learning%250Ais%2520framed%2520as%2520the%2520evolution%2520of%2520KL%2520divergences%2520across%2520data%2520neighborhoods.%2520This%250Aformulation%2520captures%2520contrastive%2520clustering%2520and%2520dimensionality%2520reduction%250Amethods%2520as%2520static%2520slices%2520while%2520offering%2520a%2520new%2520path%2520to%2520model%2520stability%2520and%2520local%250Aadaptation.%2520Our%2520experiments%2520demonstrate%2520that%2520RKDO%2520offers%2520dual%2520efficiency%250Aadvantages%2520approximately%252030%2520percent%2520lower%2520loss%2520values%2520compared%2520to%2520static%250Aapproaches%2520across%2520three%2520different%2520datasets%2520and%252060%2520to%252080%2520percent%2520reduction%2520in%250Acomputational%2520resources%2520needed%2520to%2520achieve%2520comparable%2520results.%2520This%2520suggests%250Athat%2520RKDOs%2520recursive%2520updating%2520mechanism%2520provides%2520a%2520fundamentally%2520more%2520efficient%250Aoptimization%2520landscape%2520for%2520representation%2520learning%2520with%2520significant%250Aimplications%2520for%2520resource%2520constrained%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20KL%20Divergence%20Optimization%3A%20A%20Dynamic%20Framework%20for%0A%20%20Representation%20Learning&entry.906535625=Anthony%20D%20Martin&entry.1292438233=%20%20We%20propose%20a%20generalization%20of%20modern%20representation%20learning%20objectives%20by%0Areframing%20them%20as%20recursive%20divergence%20alignment%20processes%20over%20localized%0Aconditional%20distributions%20While%20recent%20frameworks%20like%20Information%20Contrastive%0ALearning%20I-Con%20unify%20multiple%20learning%20paradigms%20through%20KL%20divergence%20between%0Afixed%20neighborhood%20conditionals%20we%20argue%20this%20view%20underplays%20a%20crucial%0Arecursive%20structure%20inherent%20in%20the%20learning%20process.%20We%20introduce%20Recursive%20KL%0ADivergence%20Optimization%20RKDO%20a%20dynamic%20formalism%20where%20representation%20learning%0Ais%20framed%20as%20the%20evolution%20of%20KL%20divergences%20across%20data%20neighborhoods.%20This%0Aformulation%20captures%20contrastive%20clustering%20and%20dimensionality%20reduction%0Amethods%20as%20static%20slices%20while%20offering%20a%20new%20path%20to%20model%20stability%20and%20local%0Aadaptation.%20Our%20experiments%20demonstrate%20that%20RKDO%20offers%20dual%20efficiency%0Aadvantages%20approximately%2030%20percent%20lower%20loss%20values%20compared%20to%20static%0Aapproaches%20across%20three%20different%20datasets%20and%2060%20to%2080%20percent%20reduction%20in%0Acomputational%20resources%20needed%20to%20achieve%20comparable%20results.%20This%20suggests%0Athat%20RKDOs%20recursive%20updating%20mechanism%20provides%20a%20fundamentally%20more%20efficient%0Aoptimization%20landscape%20for%20representation%20learning%20with%20significant%0Aimplications%20for%20resource%20constrained%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21707v1&entry.124074799=Read"},
{"title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos", "author": "Linyi Jin and Richard Tucker and Zhengqi Li and David Fouhey and Noah Snavely and Aleksander Holynski", "abstract": "  Learning to understand dynamic 3D scenes from imagery is crucial for\napplications ranging from robotics to scene reconstruction. Yet, unlike other\nproblems where large-scale supervised training has enabled rapid progress,\ndirectly supervising methods for recovering 3D motion remains challenging due\nto the fundamental difficulty of obtaining ground truth annotations. We present\na system for mining high-quality 4D reconstructions from internet stereoscopic,\nwide-angle videos. Our system fuses and filters the outputs of camera pose\nestimation, stereo depth estimation, and temporal tracking methods into\nhigh-quality dynamic 3D reconstructions. We use this method to generate\nlarge-scale data in the form of world-consistent, pseudo-metric 3D point clouds\nwith long-term motion trajectories. We demonstrate the utility of this data by\ntraining a variant of DUSt3R to predict structure and 3D motion from real-world\nimage pairs, showing that training on our reconstructed data enables\ngeneralization to diverse real-world scenes. Project page and data at:\nhttps://stereo4d.github.io\n", "link": "http://arxiv.org/abs/2412.09621v2", "date": "2025-04-30", "relevancy": 2.5999, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stereo4D%3A%20Learning%20How%20Things%20Move%20in%203D%20from%20Internet%20Stereo%20Videos&body=Title%3A%20Stereo4D%3A%20Learning%20How%20Things%20Move%20in%203D%20from%20Internet%20Stereo%20Videos%0AAuthor%3A%20Linyi%20Jin%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20David%20Fouhey%20and%20Noah%20Snavely%20and%20Aleksander%20Holynski%0AAbstract%3A%20%20%20Learning%20to%20understand%20dynamic%203D%20scenes%20from%20imagery%20is%20crucial%20for%0Aapplications%20ranging%20from%20robotics%20to%20scene%20reconstruction.%20Yet%2C%20unlike%20other%0Aproblems%20where%20large-scale%20supervised%20training%20has%20enabled%20rapid%20progress%2C%0Adirectly%20supervising%20methods%20for%20recovering%203D%20motion%20remains%20challenging%20due%0Ato%20the%20fundamental%20difficulty%20of%20obtaining%20ground%20truth%20annotations.%20We%20present%0Aa%20system%20for%20mining%20high-quality%204D%20reconstructions%20from%20internet%20stereoscopic%2C%0Awide-angle%20videos.%20Our%20system%20fuses%20and%20filters%20the%20outputs%20of%20camera%20pose%0Aestimation%2C%20stereo%20depth%20estimation%2C%20and%20temporal%20tracking%20methods%20into%0Ahigh-quality%20dynamic%203D%20reconstructions.%20We%20use%20this%20method%20to%20generate%0Alarge-scale%20data%20in%20the%20form%20of%20world-consistent%2C%20pseudo-metric%203D%20point%20clouds%0Awith%20long-term%20motion%20trajectories.%20We%20demonstrate%20the%20utility%20of%20this%20data%20by%0Atraining%20a%20variant%20of%20DUSt3R%20to%20predict%20structure%20and%203D%20motion%20from%20real-world%0Aimage%20pairs%2C%20showing%20that%20training%20on%20our%20reconstructed%20data%20enables%0Ageneralization%20to%20diverse%20real-world%20scenes.%20Project%20page%20and%20data%20at%3A%0Ahttps%3A//stereo4d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereo4D%253A%2520Learning%2520How%2520Things%2520Move%2520in%25203D%2520from%2520Internet%2520Stereo%2520Videos%26entry.906535625%3DLinyi%2520Jin%2520and%2520Richard%2520Tucker%2520and%2520Zhengqi%2520Li%2520and%2520David%2520Fouhey%2520and%2520Noah%2520Snavely%2520and%2520Aleksander%2520Holynski%26entry.1292438233%3D%2520%2520Learning%2520to%2520understand%2520dynamic%25203D%2520scenes%2520from%2520imagery%2520is%2520crucial%2520for%250Aapplications%2520ranging%2520from%2520robotics%2520to%2520scene%2520reconstruction.%2520Yet%252C%2520unlike%2520other%250Aproblems%2520where%2520large-scale%2520supervised%2520training%2520has%2520enabled%2520rapid%2520progress%252C%250Adirectly%2520supervising%2520methods%2520for%2520recovering%25203D%2520motion%2520remains%2520challenging%2520due%250Ato%2520the%2520fundamental%2520difficulty%2520of%2520obtaining%2520ground%2520truth%2520annotations.%2520We%2520present%250Aa%2520system%2520for%2520mining%2520high-quality%25204D%2520reconstructions%2520from%2520internet%2520stereoscopic%252C%250Awide-angle%2520videos.%2520Our%2520system%2520fuses%2520and%2520filters%2520the%2520outputs%2520of%2520camera%2520pose%250Aestimation%252C%2520stereo%2520depth%2520estimation%252C%2520and%2520temporal%2520tracking%2520methods%2520into%250Ahigh-quality%2520dynamic%25203D%2520reconstructions.%2520We%2520use%2520this%2520method%2520to%2520generate%250Alarge-scale%2520data%2520in%2520the%2520form%2520of%2520world-consistent%252C%2520pseudo-metric%25203D%2520point%2520clouds%250Awith%2520long-term%2520motion%2520trajectories.%2520We%2520demonstrate%2520the%2520utility%2520of%2520this%2520data%2520by%250Atraining%2520a%2520variant%2520of%2520DUSt3R%2520to%2520predict%2520structure%2520and%25203D%2520motion%2520from%2520real-world%250Aimage%2520pairs%252C%2520showing%2520that%2520training%2520on%2520our%2520reconstructed%2520data%2520enables%250Ageneralization%2520to%2520diverse%2520real-world%2520scenes.%2520Project%2520page%2520and%2520data%2520at%253A%250Ahttps%253A//stereo4d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stereo4D%3A%20Learning%20How%20Things%20Move%20in%203D%20from%20Internet%20Stereo%20Videos&entry.906535625=Linyi%20Jin%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20David%20Fouhey%20and%20Noah%20Snavely%20and%20Aleksander%20Holynski&entry.1292438233=%20%20Learning%20to%20understand%20dynamic%203D%20scenes%20from%20imagery%20is%20crucial%20for%0Aapplications%20ranging%20from%20robotics%20to%20scene%20reconstruction.%20Yet%2C%20unlike%20other%0Aproblems%20where%20large-scale%20supervised%20training%20has%20enabled%20rapid%20progress%2C%0Adirectly%20supervising%20methods%20for%20recovering%203D%20motion%20remains%20challenging%20due%0Ato%20the%20fundamental%20difficulty%20of%20obtaining%20ground%20truth%20annotations.%20We%20present%0Aa%20system%20for%20mining%20high-quality%204D%20reconstructions%20from%20internet%20stereoscopic%2C%0Awide-angle%20videos.%20Our%20system%20fuses%20and%20filters%20the%20outputs%20of%20camera%20pose%0Aestimation%2C%20stereo%20depth%20estimation%2C%20and%20temporal%20tracking%20methods%20into%0Ahigh-quality%20dynamic%203D%20reconstructions.%20We%20use%20this%20method%20to%20generate%0Alarge-scale%20data%20in%20the%20form%20of%20world-consistent%2C%20pseudo-metric%203D%20point%20clouds%0Awith%20long-term%20motion%20trajectories.%20We%20demonstrate%20the%20utility%20of%20this%20data%20by%0Atraining%20a%20variant%20of%20DUSt3R%20to%20predict%20structure%20and%203D%20motion%20from%20real-world%0Aimage%20pairs%2C%20showing%20that%20training%20on%20our%20reconstructed%20data%20enables%0Ageneralization%20to%20diverse%20real-world%20scenes.%20Project%20page%20and%20data%20at%3A%0Ahttps%3A//stereo4d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09621v2&entry.124074799=Read"},
{"title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud\n  De-raining", "author": "Abu Mohammed Raisuddin and Jesper Holmblad and Hamed Haghighi and Yuri Poledna and Maikol Funk Drechsler and Valentina Donzella and Eren Erdal Aksoy", "abstract": "  Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D.\n", "link": "http://arxiv.org/abs/2504.21699v1", "date": "2025-04-30", "relevancy": 2.5833, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5254}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5123}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REHEARSE-3D%3A%20A%20Multi-modal%20Emulated%20Rain%20Dataset%20for%203D%20Point%20Cloud%0A%20%20De-raining&body=Title%3A%20REHEARSE-3D%3A%20A%20Multi-modal%20Emulated%20Rain%20Dataset%20for%203D%20Point%20Cloud%0A%20%20De-raining%0AAuthor%3A%20Abu%20Mohammed%20Raisuddin%20and%20Jesper%20Holmblad%20and%20Hamed%20Haghighi%20and%20Yuri%20Poledna%20and%20Maikol%20Funk%20Drechsler%20and%20Valentina%20Donzella%20and%20Eren%20Erdal%20Aksoy%0AAbstract%3A%20%20%20Sensor%20degradation%20poses%20a%20significant%20challenge%20in%20autonomous%20driving.%0ADuring%20heavy%20rainfall%2C%20the%20interference%20from%20raindrops%20can%20adversely%20affect%20the%0Aquality%20of%20LiDAR%20point%20clouds%2C%20resulting%20in%2C%20for%20instance%2C%20inaccurate%20point%0Ameasurements.%20This%2C%20in%20turn%2C%20can%20potentially%20lead%20to%20safety%20concerns%20if%0Aautonomous%20driving%20systems%20are%20not%20weather-aware%2C%20i.e.%2C%20if%20they%20are%20unable%20to%0Adiscern%20such%20changes.%20In%20this%20study%2C%20we%20release%20a%20new%2C%20large-scale%2C%20multi-modal%0Aemulated%20rain%20dataset%2C%20REHEARSE-3D%2C%20to%20promote%20research%20advancements%20in%203D%0Apoint%20cloud%20de-raining.%20Distinct%20from%20the%20most%20relevant%20competitors%2C%20our%0Adataset%20is%20unique%20in%20several%20respects.%20First%2C%20it%20is%20the%20largest%20point-wise%0Aannotated%20dataset%2C%20and%20second%2C%20it%20is%20the%20only%20one%20with%20high-resolution%20LiDAR%0Adata%20%28LiDAR-256%29%20enriched%20with%204D%20Radar%20point%20clouds%20logged%20in%20both%20daytime%20and%0Anighttime%20conditions%20in%20a%20controlled%20weather%20environment.%20Furthermore%2C%0AREHEARSE-3D%20involves%20rain-characteristic%20information%2C%20which%20is%20of%20significant%0Avalue%20not%20only%20for%20sensor%20noise%20modeling%20but%20also%20for%20analyzing%20the%20impact%20of%0Aweather%20at%20a%20point%20level.%20Leveraging%20REHEARSE-3D%2C%20we%20benchmark%20raindrop%0Adetection%20and%20removal%20in%20fused%20LiDAR%20and%204D%20Radar%20point%20clouds.%20Our%0Acomprehensive%20study%20further%20evaluates%20the%20performance%20of%20various%20statistical%0Aand%20deep-learning%20models.%20Upon%20publication%2C%20the%20dataset%20and%20benchmark%20models%0Awill%20be%20made%20publicly%20available%20at%3A%20https%3A//sporsho.github.io/REHEARSE3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREHEARSE-3D%253A%2520A%2520Multi-modal%2520Emulated%2520Rain%2520Dataset%2520for%25203D%2520Point%2520Cloud%250A%2520%2520De-raining%26entry.906535625%3DAbu%2520Mohammed%2520Raisuddin%2520and%2520Jesper%2520Holmblad%2520and%2520Hamed%2520Haghighi%2520and%2520Yuri%2520Poledna%2520and%2520Maikol%2520Funk%2520Drechsler%2520and%2520Valentina%2520Donzella%2520and%2520Eren%2520Erdal%2520Aksoy%26entry.1292438233%3D%2520%2520Sensor%2520degradation%2520poses%2520a%2520significant%2520challenge%2520in%2520autonomous%2520driving.%250ADuring%2520heavy%2520rainfall%252C%2520the%2520interference%2520from%2520raindrops%2520can%2520adversely%2520affect%2520the%250Aquality%2520of%2520LiDAR%2520point%2520clouds%252C%2520resulting%2520in%252C%2520for%2520instance%252C%2520inaccurate%2520point%250Ameasurements.%2520This%252C%2520in%2520turn%252C%2520can%2520potentially%2520lead%2520to%2520safety%2520concerns%2520if%250Aautonomous%2520driving%2520systems%2520are%2520not%2520weather-aware%252C%2520i.e.%252C%2520if%2520they%2520are%2520unable%2520to%250Adiscern%2520such%2520changes.%2520In%2520this%2520study%252C%2520we%2520release%2520a%2520new%252C%2520large-scale%252C%2520multi-modal%250Aemulated%2520rain%2520dataset%252C%2520REHEARSE-3D%252C%2520to%2520promote%2520research%2520advancements%2520in%25203D%250Apoint%2520cloud%2520de-raining.%2520Distinct%2520from%2520the%2520most%2520relevant%2520competitors%252C%2520our%250Adataset%2520is%2520unique%2520in%2520several%2520respects.%2520First%252C%2520it%2520is%2520the%2520largest%2520point-wise%250Aannotated%2520dataset%252C%2520and%2520second%252C%2520it%2520is%2520the%2520only%2520one%2520with%2520high-resolution%2520LiDAR%250Adata%2520%2528LiDAR-256%2529%2520enriched%2520with%25204D%2520Radar%2520point%2520clouds%2520logged%2520in%2520both%2520daytime%2520and%250Anighttime%2520conditions%2520in%2520a%2520controlled%2520weather%2520environment.%2520Furthermore%252C%250AREHEARSE-3D%2520involves%2520rain-characteristic%2520information%252C%2520which%2520is%2520of%2520significant%250Avalue%2520not%2520only%2520for%2520sensor%2520noise%2520modeling%2520but%2520also%2520for%2520analyzing%2520the%2520impact%2520of%250Aweather%2520at%2520a%2520point%2520level.%2520Leveraging%2520REHEARSE-3D%252C%2520we%2520benchmark%2520raindrop%250Adetection%2520and%2520removal%2520in%2520fused%2520LiDAR%2520and%25204D%2520Radar%2520point%2520clouds.%2520Our%250Acomprehensive%2520study%2520further%2520evaluates%2520the%2520performance%2520of%2520various%2520statistical%250Aand%2520deep-learning%2520models.%2520Upon%2520publication%252C%2520the%2520dataset%2520and%2520benchmark%2520models%250Awill%2520be%2520made%2520publicly%2520available%2520at%253A%2520https%253A//sporsho.github.io/REHEARSE3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REHEARSE-3D%3A%20A%20Multi-modal%20Emulated%20Rain%20Dataset%20for%203D%20Point%20Cloud%0A%20%20De-raining&entry.906535625=Abu%20Mohammed%20Raisuddin%20and%20Jesper%20Holmblad%20and%20Hamed%20Haghighi%20and%20Yuri%20Poledna%20and%20Maikol%20Funk%20Drechsler%20and%20Valentina%20Donzella%20and%20Eren%20Erdal%20Aksoy&entry.1292438233=%20%20Sensor%20degradation%20poses%20a%20significant%20challenge%20in%20autonomous%20driving.%0ADuring%20heavy%20rainfall%2C%20the%20interference%20from%20raindrops%20can%20adversely%20affect%20the%0Aquality%20of%20LiDAR%20point%20clouds%2C%20resulting%20in%2C%20for%20instance%2C%20inaccurate%20point%0Ameasurements.%20This%2C%20in%20turn%2C%20can%20potentially%20lead%20to%20safety%20concerns%20if%0Aautonomous%20driving%20systems%20are%20not%20weather-aware%2C%20i.e.%2C%20if%20they%20are%20unable%20to%0Adiscern%20such%20changes.%20In%20this%20study%2C%20we%20release%20a%20new%2C%20large-scale%2C%20multi-modal%0Aemulated%20rain%20dataset%2C%20REHEARSE-3D%2C%20to%20promote%20research%20advancements%20in%203D%0Apoint%20cloud%20de-raining.%20Distinct%20from%20the%20most%20relevant%20competitors%2C%20our%0Adataset%20is%20unique%20in%20several%20respects.%20First%2C%20it%20is%20the%20largest%20point-wise%0Aannotated%20dataset%2C%20and%20second%2C%20it%20is%20the%20only%20one%20with%20high-resolution%20LiDAR%0Adata%20%28LiDAR-256%29%20enriched%20with%204D%20Radar%20point%20clouds%20logged%20in%20both%20daytime%20and%0Anighttime%20conditions%20in%20a%20controlled%20weather%20environment.%20Furthermore%2C%0AREHEARSE-3D%20involves%20rain-characteristic%20information%2C%20which%20is%20of%20significant%0Avalue%20not%20only%20for%20sensor%20noise%20modeling%20but%20also%20for%20analyzing%20the%20impact%20of%0Aweather%20at%20a%20point%20level.%20Leveraging%20REHEARSE-3D%2C%20we%20benchmark%20raindrop%0Adetection%20and%20removal%20in%20fused%20LiDAR%20and%204D%20Radar%20point%20clouds.%20Our%0Acomprehensive%20study%20further%20evaluates%20the%20performance%20of%20various%20statistical%0Aand%20deep-learning%20models.%20Upon%20publication%2C%20the%20dataset%20and%20benchmark%20models%0Awill%20be%20made%20publicly%20available%20at%3A%20https%3A//sporsho.github.io/REHEARSE3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21699v1&entry.124074799=Read"},
{"title": "Towards Understanding Depth Perception in Foveated Rendering", "author": "Sophie Kerga\u00dfner and Taimoor Tariq and Piotr Didyk", "abstract": "  The true vision for real-time virtual and augmented reality is reproducing\nour visual reality in its entirety on immersive displays. To this end, foveated\nrendering leverages the limitations of spatial acuity in human peripheral\nvision to allocate computational resources to the fovea while reducing quality\nin the periphery. Such methods are often derived from studies on the spatial\nresolution of the human visual system and its ability to perceive blur in the\nperiphery, enabling the potential for high spatial quality in real-time.\nHowever, the effects of blur on other visual cues that depend on luminance\ncontrast, such as depth, remain largely unexplored. It is critical to\nunderstand this interplay, as accurate depth representation is a fundamental\naspect of visual realism. In this paper, we present the first evaluation\nexploring the effects of foveated rendering on stereoscopic depth perception.\nWe design a psychovisual experiment to quantitatively study the effects of\nperipheral blur on depth perception. Our analysis demonstrates that\nstereoscopic acuity remains unaffected (or even improves) by high levels of\nperipheral blur. Based on our studies, we derive a simple perceptual model that\ndetermines the amount of foveation that does not affect stereoacuity.\nFurthermore, we analyze the model in the context of common foveation practices\nreported in literature. The findings indicate that foveated rendering does not\nimpact stereoscopic depth perception, and stereoacuity remains unaffected with\nup to 2x stronger foveation than commonly used. Finally, we conduct a\nvalidation experiment and show that our findings hold for complex natural\nstimuli.\n", "link": "http://arxiv.org/abs/2501.18635v2", "date": "2025-04-30", "relevancy": 2.5575, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Depth%20Perception%20in%20Foveated%20Rendering&body=Title%3A%20Towards%20Understanding%20Depth%20Perception%20in%20Foveated%20Rendering%0AAuthor%3A%20Sophie%20Kerga%C3%9Fner%20and%20Taimoor%20Tariq%20and%20Piotr%20Didyk%0AAbstract%3A%20%20%20The%20true%20vision%20for%20real-time%20virtual%20and%20augmented%20reality%20is%20reproducing%0Aour%20visual%20reality%20in%20its%20entirety%20on%20immersive%20displays.%20To%20this%20end%2C%20foveated%0Arendering%20leverages%20the%20limitations%20of%20spatial%20acuity%20in%20human%20peripheral%0Avision%20to%20allocate%20computational%20resources%20to%20the%20fovea%20while%20reducing%20quality%0Ain%20the%20periphery.%20Such%20methods%20are%20often%20derived%20from%20studies%20on%20the%20spatial%0Aresolution%20of%20the%20human%20visual%20system%20and%20its%20ability%20to%20perceive%20blur%20in%20the%0Aperiphery%2C%20enabling%20the%20potential%20for%20high%20spatial%20quality%20in%20real-time.%0AHowever%2C%20the%20effects%20of%20blur%20on%20other%20visual%20cues%20that%20depend%20on%20luminance%0Acontrast%2C%20such%20as%20depth%2C%20remain%20largely%20unexplored.%20It%20is%20critical%20to%0Aunderstand%20this%20interplay%2C%20as%20accurate%20depth%20representation%20is%20a%20fundamental%0Aaspect%20of%20visual%20realism.%20In%20this%20paper%2C%20we%20present%20the%20first%20evaluation%0Aexploring%20the%20effects%20of%20foveated%20rendering%20on%20stereoscopic%20depth%20perception.%0AWe%20design%20a%20psychovisual%20experiment%20to%20quantitatively%20study%20the%20effects%20of%0Aperipheral%20blur%20on%20depth%20perception.%20Our%20analysis%20demonstrates%20that%0Astereoscopic%20acuity%20remains%20unaffected%20%28or%20even%20improves%29%20by%20high%20levels%20of%0Aperipheral%20blur.%20Based%20on%20our%20studies%2C%20we%20derive%20a%20simple%20perceptual%20model%20that%0Adetermines%20the%20amount%20of%20foveation%20that%20does%20not%20affect%20stereoacuity.%0AFurthermore%2C%20we%20analyze%20the%20model%20in%20the%20context%20of%20common%20foveation%20practices%0Areported%20in%20literature.%20The%20findings%20indicate%20that%20foveated%20rendering%20does%20not%0Aimpact%20stereoscopic%20depth%20perception%2C%20and%20stereoacuity%20remains%20unaffected%20with%0Aup%20to%202x%20stronger%20foveation%20than%20commonly%20used.%20Finally%2C%20we%20conduct%20a%0Avalidation%20experiment%20and%20show%20that%20our%20findings%20hold%20for%20complex%20natural%0Astimuli.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Depth%2520Perception%2520in%2520Foveated%2520Rendering%26entry.906535625%3DSophie%2520Kerga%25C3%259Fner%2520and%2520Taimoor%2520Tariq%2520and%2520Piotr%2520Didyk%26entry.1292438233%3D%2520%2520The%2520true%2520vision%2520for%2520real-time%2520virtual%2520and%2520augmented%2520reality%2520is%2520reproducing%250Aour%2520visual%2520reality%2520in%2520its%2520entirety%2520on%2520immersive%2520displays.%2520To%2520this%2520end%252C%2520foveated%250Arendering%2520leverages%2520the%2520limitations%2520of%2520spatial%2520acuity%2520in%2520human%2520peripheral%250Avision%2520to%2520allocate%2520computational%2520resources%2520to%2520the%2520fovea%2520while%2520reducing%2520quality%250Ain%2520the%2520periphery.%2520Such%2520methods%2520are%2520often%2520derived%2520from%2520studies%2520on%2520the%2520spatial%250Aresolution%2520of%2520the%2520human%2520visual%2520system%2520and%2520its%2520ability%2520to%2520perceive%2520blur%2520in%2520the%250Aperiphery%252C%2520enabling%2520the%2520potential%2520for%2520high%2520spatial%2520quality%2520in%2520real-time.%250AHowever%252C%2520the%2520effects%2520of%2520blur%2520on%2520other%2520visual%2520cues%2520that%2520depend%2520on%2520luminance%250Acontrast%252C%2520such%2520as%2520depth%252C%2520remain%2520largely%2520unexplored.%2520It%2520is%2520critical%2520to%250Aunderstand%2520this%2520interplay%252C%2520as%2520accurate%2520depth%2520representation%2520is%2520a%2520fundamental%250Aaspect%2520of%2520visual%2520realism.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520evaluation%250Aexploring%2520the%2520effects%2520of%2520foveated%2520rendering%2520on%2520stereoscopic%2520depth%2520perception.%250AWe%2520design%2520a%2520psychovisual%2520experiment%2520to%2520quantitatively%2520study%2520the%2520effects%2520of%250Aperipheral%2520blur%2520on%2520depth%2520perception.%2520Our%2520analysis%2520demonstrates%2520that%250Astereoscopic%2520acuity%2520remains%2520unaffected%2520%2528or%2520even%2520improves%2529%2520by%2520high%2520levels%2520of%250Aperipheral%2520blur.%2520Based%2520on%2520our%2520studies%252C%2520we%2520derive%2520a%2520simple%2520perceptual%2520model%2520that%250Adetermines%2520the%2520amount%2520of%2520foveation%2520that%2520does%2520not%2520affect%2520stereoacuity.%250AFurthermore%252C%2520we%2520analyze%2520the%2520model%2520in%2520the%2520context%2520of%2520common%2520foveation%2520practices%250Areported%2520in%2520literature.%2520The%2520findings%2520indicate%2520that%2520foveated%2520rendering%2520does%2520not%250Aimpact%2520stereoscopic%2520depth%2520perception%252C%2520and%2520stereoacuity%2520remains%2520unaffected%2520with%250Aup%2520to%25202x%2520stronger%2520foveation%2520than%2520commonly%2520used.%2520Finally%252C%2520we%2520conduct%2520a%250Avalidation%2520experiment%2520and%2520show%2520that%2520our%2520findings%2520hold%2520for%2520complex%2520natural%250Astimuli.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Depth%20Perception%20in%20Foveated%20Rendering&entry.906535625=Sophie%20Kerga%C3%9Fner%20and%20Taimoor%20Tariq%20and%20Piotr%20Didyk&entry.1292438233=%20%20The%20true%20vision%20for%20real-time%20virtual%20and%20augmented%20reality%20is%20reproducing%0Aour%20visual%20reality%20in%20its%20entirety%20on%20immersive%20displays.%20To%20this%20end%2C%20foveated%0Arendering%20leverages%20the%20limitations%20of%20spatial%20acuity%20in%20human%20peripheral%0Avision%20to%20allocate%20computational%20resources%20to%20the%20fovea%20while%20reducing%20quality%0Ain%20the%20periphery.%20Such%20methods%20are%20often%20derived%20from%20studies%20on%20the%20spatial%0Aresolution%20of%20the%20human%20visual%20system%20and%20its%20ability%20to%20perceive%20blur%20in%20the%0Aperiphery%2C%20enabling%20the%20potential%20for%20high%20spatial%20quality%20in%20real-time.%0AHowever%2C%20the%20effects%20of%20blur%20on%20other%20visual%20cues%20that%20depend%20on%20luminance%0Acontrast%2C%20such%20as%20depth%2C%20remain%20largely%20unexplored.%20It%20is%20critical%20to%0Aunderstand%20this%20interplay%2C%20as%20accurate%20depth%20representation%20is%20a%20fundamental%0Aaspect%20of%20visual%20realism.%20In%20this%20paper%2C%20we%20present%20the%20first%20evaluation%0Aexploring%20the%20effects%20of%20foveated%20rendering%20on%20stereoscopic%20depth%20perception.%0AWe%20design%20a%20psychovisual%20experiment%20to%20quantitatively%20study%20the%20effects%20of%0Aperipheral%20blur%20on%20depth%20perception.%20Our%20analysis%20demonstrates%20that%0Astereoscopic%20acuity%20remains%20unaffected%20%28or%20even%20improves%29%20by%20high%20levels%20of%0Aperipheral%20blur.%20Based%20on%20our%20studies%2C%20we%20derive%20a%20simple%20perceptual%20model%20that%0Adetermines%20the%20amount%20of%20foveation%20that%20does%20not%20affect%20stereoacuity.%0AFurthermore%2C%20we%20analyze%20the%20model%20in%20the%20context%20of%20common%20foveation%20practices%0Areported%20in%20literature.%20The%20findings%20indicate%20that%20foveated%20rendering%20does%20not%0Aimpact%20stereoscopic%20depth%20perception%2C%20and%20stereoacuity%20remains%20unaffected%20with%0Aup%20to%202x%20stronger%20foveation%20than%20commonly%20used.%20Finally%2C%20we%20conduct%20a%0Avalidation%20experiment%20and%20show%20that%20our%20findings%20hold%20for%20complex%20natural%0Astimuli.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18635v2&entry.124074799=Read"},
{"title": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for\n  Knowledge Graph Completion", "author": "Muzhi Li and Cehao Yang and Chengjin Xu and Xuhui Jiang and Yiyan Qi and Jian Guo and Ho-fung Leung and Irwin King", "abstract": "  The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets.\n", "link": "http://arxiv.org/abs/2411.08165v2", "date": "2025-04-30", "relevancy": 2.5233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%2C%20Reasoning%2C%20Re-ranking%3A%20A%20Context-Enriched%20Framework%20for%0A%20%20Knowledge%20Graph%20Completion&body=Title%3A%20Retrieval%2C%20Reasoning%2C%20Re-ranking%3A%20A%20Context-Enriched%20Framework%20for%0A%20%20Knowledge%20Graph%20Completion%0AAuthor%3A%20Muzhi%20Li%20and%20Cehao%20Yang%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Yiyan%20Qi%20and%20Jian%20Guo%20and%20Ho-fung%20Leung%20and%20Irwin%20King%0AAbstract%3A%20%20%20The%20Knowledge%20Graph%20Completion~%28KGC%29%20task%20aims%20to%20infer%20the%20missing%20entity%0Afrom%20an%20incomplete%20triple.%20Existing%20embedding-based%20methods%20rely%20solely%20on%0Atriples%20in%20the%20KG%2C%20which%20is%20vulnerable%20to%20specious%20relation%20patterns%20and%0Along-tail%20entities.%20On%20the%20other%20hand%2C%20text-based%20methods%20struggle%20with%20the%0Asemantic%20gap%20between%20KG%20triples%20and%20natural%20language.%20Apart%20from%20triples%2C%0Aentity%20contexts%20%28e.g.%2C%20labels%2C%20descriptions%2C%20aliases%29%20also%20play%20a%20significant%0Arole%20in%20augmenting%20KGs.%20To%20address%20these%20limitations%2C%20we%20propose%20KGR3%2C%20a%0Acontext-enriched%20framework%20for%20KGC.%20KGR3%20is%20composed%20of%20three%20modules.%20Firstly%2C%0Athe%20Retrieval%20module%20gathers%20supporting%20triples%20from%20the%20KG%2C%20collects%20plausible%0Acandidate%20answers%20from%20a%20base%20embedding%20model%2C%20and%20retrieves%20context%20for%20each%0Arelated%20entity.%20Then%2C%20the%20Reasoning%20module%20employs%20a%20large%20language%20model%20to%0Agenerate%20potential%20answers%20for%20each%20query%20triple.%20Finally%2C%20the%20Re-ranking%0Amodule%20combines%20candidate%20answers%20from%20the%20two%20modules%20mentioned%20above%2C%20and%0Afine-tunes%20an%20LLM%20to%20provide%20the%20best%20answer.%20Extensive%20experiments%20on%20widely%0Aused%20datasets%20demonstrate%20that%20KGR3%20consistently%20improves%20various%20KGC%20methods.%0ASpecifically%2C%20the%20best%20variant%20of%20KGR3%20achieves%20absolute%20Hits%401%20improvements%20of%0A12.3%25%20and%205.6%25%20on%20the%20FB15k237%20and%20WN18RR%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08165v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%252C%2520Reasoning%252C%2520Re-ranking%253A%2520A%2520Context-Enriched%2520Framework%2520for%250A%2520%2520Knowledge%2520Graph%2520Completion%26entry.906535625%3DMuzhi%2520Li%2520and%2520Cehao%2520Yang%2520and%2520Chengjin%2520Xu%2520and%2520Xuhui%2520Jiang%2520and%2520Yiyan%2520Qi%2520and%2520Jian%2520Guo%2520and%2520Ho-fung%2520Leung%2520and%2520Irwin%2520King%26entry.1292438233%3D%2520%2520The%2520Knowledge%2520Graph%2520Completion~%2528KGC%2529%2520task%2520aims%2520to%2520infer%2520the%2520missing%2520entity%250Afrom%2520an%2520incomplete%2520triple.%2520Existing%2520embedding-based%2520methods%2520rely%2520solely%2520on%250Atriples%2520in%2520the%2520KG%252C%2520which%2520is%2520vulnerable%2520to%2520specious%2520relation%2520patterns%2520and%250Along-tail%2520entities.%2520On%2520the%2520other%2520hand%252C%2520text-based%2520methods%2520struggle%2520with%2520the%250Asemantic%2520gap%2520between%2520KG%2520triples%2520and%2520natural%2520language.%2520Apart%2520from%2520triples%252C%250Aentity%2520contexts%2520%2528e.g.%252C%2520labels%252C%2520descriptions%252C%2520aliases%2529%2520also%2520play%2520a%2520significant%250Arole%2520in%2520augmenting%2520KGs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520KGR3%252C%2520a%250Acontext-enriched%2520framework%2520for%2520KGC.%2520KGR3%2520is%2520composed%2520of%2520three%2520modules.%2520Firstly%252C%250Athe%2520Retrieval%2520module%2520gathers%2520supporting%2520triples%2520from%2520the%2520KG%252C%2520collects%2520plausible%250Acandidate%2520answers%2520from%2520a%2520base%2520embedding%2520model%252C%2520and%2520retrieves%2520context%2520for%2520each%250Arelated%2520entity.%2520Then%252C%2520the%2520Reasoning%2520module%2520employs%2520a%2520large%2520language%2520model%2520to%250Agenerate%2520potential%2520answers%2520for%2520each%2520query%2520triple.%2520Finally%252C%2520the%2520Re-ranking%250Amodule%2520combines%2520candidate%2520answers%2520from%2520the%2520two%2520modules%2520mentioned%2520above%252C%2520and%250Afine-tunes%2520an%2520LLM%2520to%2520provide%2520the%2520best%2520answer.%2520Extensive%2520experiments%2520on%2520widely%250Aused%2520datasets%2520demonstrate%2520that%2520KGR3%2520consistently%2520improves%2520various%2520KGC%2520methods.%250ASpecifically%252C%2520the%2520best%2520variant%2520of%2520KGR3%2520achieves%2520absolute%2520Hits%25401%2520improvements%2520of%250A12.3%2525%2520and%25205.6%2525%2520on%2520the%2520FB15k237%2520and%2520WN18RR%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08165v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%2C%20Reasoning%2C%20Re-ranking%3A%20A%20Context-Enriched%20Framework%20for%0A%20%20Knowledge%20Graph%20Completion&entry.906535625=Muzhi%20Li%20and%20Cehao%20Yang%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Yiyan%20Qi%20and%20Jian%20Guo%20and%20Ho-fung%20Leung%20and%20Irwin%20King&entry.1292438233=%20%20The%20Knowledge%20Graph%20Completion~%28KGC%29%20task%20aims%20to%20infer%20the%20missing%20entity%0Afrom%20an%20incomplete%20triple.%20Existing%20embedding-based%20methods%20rely%20solely%20on%0Atriples%20in%20the%20KG%2C%20which%20is%20vulnerable%20to%20specious%20relation%20patterns%20and%0Along-tail%20entities.%20On%20the%20other%20hand%2C%20text-based%20methods%20struggle%20with%20the%0Asemantic%20gap%20between%20KG%20triples%20and%20natural%20language.%20Apart%20from%20triples%2C%0Aentity%20contexts%20%28e.g.%2C%20labels%2C%20descriptions%2C%20aliases%29%20also%20play%20a%20significant%0Arole%20in%20augmenting%20KGs.%20To%20address%20these%20limitations%2C%20we%20propose%20KGR3%2C%20a%0Acontext-enriched%20framework%20for%20KGC.%20KGR3%20is%20composed%20of%20three%20modules.%20Firstly%2C%0Athe%20Retrieval%20module%20gathers%20supporting%20triples%20from%20the%20KG%2C%20collects%20plausible%0Acandidate%20answers%20from%20a%20base%20embedding%20model%2C%20and%20retrieves%20context%20for%20each%0Arelated%20entity.%20Then%2C%20the%20Reasoning%20module%20employs%20a%20large%20language%20model%20to%0Agenerate%20potential%20answers%20for%20each%20query%20triple.%20Finally%2C%20the%20Re-ranking%0Amodule%20combines%20candidate%20answers%20from%20the%20two%20modules%20mentioned%20above%2C%20and%0Afine-tunes%20an%20LLM%20to%20provide%20the%20best%20answer.%20Extensive%20experiments%20on%20widely%0Aused%20datasets%20demonstrate%20that%20KGR3%20consistently%20improves%20various%20KGC%20methods.%0ASpecifically%2C%20the%20best%20variant%20of%20KGR3%20achieves%20absolute%20Hits%401%20improvements%20of%0A12.3%25%20and%205.6%25%20on%20the%20FB15k237%20and%20WN18RR%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08165v2&entry.124074799=Read"},
{"title": "CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge\n  Distillation", "author": "Zherui Zhang and Changwei Wang and Rongtao Xu and Wenhao Xu and Shibiao Xu and Yu Zhang and Li Guo", "abstract": "  Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from\nthe given pre-trained teacher network to the target student model without\naccess to the real training data. Existing DFKD methods focus primarily on\nimproving image recognition performance on associated datasets, often\nneglecting the crucial aspect of the transferability of learned\nrepresentations. In this paper, we propose Category-Aware Embedding Data-Free\nKnowledge Distillation (CAE-DFKD), which addresses at the embedding level the\nlimitations of previous rely on image-level methods to improve model\ngeneralization but fail when directly applied to DFKD. The superiority and\nflexibility of CAE-DFKD are extensively evaluated, including:\n\\textit{\\textbf{i.)}} Significant efficiency advantages resulting from altering\nthe generator training paradigm; \\textit{\\textbf{ii.)}} Competitive performance\nwith existing DFKD state-of-the-art methods on image recognition tasks;\n\\textit{\\textbf{iii.)}} Remarkable transferability of data-free learned\nrepresentations demonstrated in downstream tasks.\n", "link": "http://arxiv.org/abs/2504.21478v1", "date": "2025-04-30", "relevancy": 2.491, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5011}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4979}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAE-DFKD%3A%20Bridging%20the%20Transferability%20Gap%20in%20Data-Free%20Knowledge%0A%20%20Distillation&body=Title%3A%20CAE-DFKD%3A%20Bridging%20the%20Transferability%20Gap%20in%20Data-Free%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Zherui%20Zhang%20and%20Changwei%20Wang%20and%20Rongtao%20Xu%20and%20Wenhao%20Xu%20and%20Shibiao%20Xu%20and%20Yu%20Zhang%20and%20Li%20Guo%0AAbstract%3A%20%20%20Data-Free%20Knowledge%20Distillation%20%28DFKD%29%20enables%20the%20knowledge%20transfer%20from%0Athe%20given%20pre-trained%20teacher%20network%20to%20the%20target%20student%20model%20without%0Aaccess%20to%20the%20real%20training%20data.%20Existing%20DFKD%20methods%20focus%20primarily%20on%0Aimproving%20image%20recognition%20performance%20on%20associated%20datasets%2C%20often%0Aneglecting%20the%20crucial%20aspect%20of%20the%20transferability%20of%20learned%0Arepresentations.%20In%20this%20paper%2C%20we%20propose%20Category-Aware%20Embedding%20Data-Free%0AKnowledge%20Distillation%20%28CAE-DFKD%29%2C%20which%20addresses%20at%20the%20embedding%20level%20the%0Alimitations%20of%20previous%20rely%20on%20image-level%20methods%20to%20improve%20model%0Ageneralization%20but%20fail%20when%20directly%20applied%20to%20DFKD.%20The%20superiority%20and%0Aflexibility%20of%20CAE-DFKD%20are%20extensively%20evaluated%2C%20including%3A%0A%5Ctextit%7B%5Ctextbf%7Bi.%29%7D%7D%20Significant%20efficiency%20advantages%20resulting%20from%20altering%0Athe%20generator%20training%20paradigm%3B%20%5Ctextit%7B%5Ctextbf%7Bii.%29%7D%7D%20Competitive%20performance%0Awith%20existing%20DFKD%20state-of-the-art%20methods%20on%20image%20recognition%20tasks%3B%0A%5Ctextit%7B%5Ctextbf%7Biii.%29%7D%7D%20Remarkable%20transferability%20of%20data-free%20learned%0Arepresentations%20demonstrated%20in%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAE-DFKD%253A%2520Bridging%2520the%2520Transferability%2520Gap%2520in%2520Data-Free%2520Knowledge%250A%2520%2520Distillation%26entry.906535625%3DZherui%2520Zhang%2520and%2520Changwei%2520Wang%2520and%2520Rongtao%2520Xu%2520and%2520Wenhao%2520Xu%2520and%2520Shibiao%2520Xu%2520and%2520Yu%2520Zhang%2520and%2520Li%2520Guo%26entry.1292438233%3D%2520%2520Data-Free%2520Knowledge%2520Distillation%2520%2528DFKD%2529%2520enables%2520the%2520knowledge%2520transfer%2520from%250Athe%2520given%2520pre-trained%2520teacher%2520network%2520to%2520the%2520target%2520student%2520model%2520without%250Aaccess%2520to%2520the%2520real%2520training%2520data.%2520Existing%2520DFKD%2520methods%2520focus%2520primarily%2520on%250Aimproving%2520image%2520recognition%2520performance%2520on%2520associated%2520datasets%252C%2520often%250Aneglecting%2520the%2520crucial%2520aspect%2520of%2520the%2520transferability%2520of%2520learned%250Arepresentations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Category-Aware%2520Embedding%2520Data-Free%250AKnowledge%2520Distillation%2520%2528CAE-DFKD%2529%252C%2520which%2520addresses%2520at%2520the%2520embedding%2520level%2520the%250Alimitations%2520of%2520previous%2520rely%2520on%2520image-level%2520methods%2520to%2520improve%2520model%250Ageneralization%2520but%2520fail%2520when%2520directly%2520applied%2520to%2520DFKD.%2520The%2520superiority%2520and%250Aflexibility%2520of%2520CAE-DFKD%2520are%2520extensively%2520evaluated%252C%2520including%253A%250A%255Ctextit%257B%255Ctextbf%257Bi.%2529%257D%257D%2520Significant%2520efficiency%2520advantages%2520resulting%2520from%2520altering%250Athe%2520generator%2520training%2520paradigm%253B%2520%255Ctextit%257B%255Ctextbf%257Bii.%2529%257D%257D%2520Competitive%2520performance%250Awith%2520existing%2520DFKD%2520state-of-the-art%2520methods%2520on%2520image%2520recognition%2520tasks%253B%250A%255Ctextit%257B%255Ctextbf%257Biii.%2529%257D%257D%2520Remarkable%2520transferability%2520of%2520data-free%2520learned%250Arepresentations%2520demonstrated%2520in%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAE-DFKD%3A%20Bridging%20the%20Transferability%20Gap%20in%20Data-Free%20Knowledge%0A%20%20Distillation&entry.906535625=Zherui%20Zhang%20and%20Changwei%20Wang%20and%20Rongtao%20Xu%20and%20Wenhao%20Xu%20and%20Shibiao%20Xu%20and%20Yu%20Zhang%20and%20Li%20Guo&entry.1292438233=%20%20Data-Free%20Knowledge%20Distillation%20%28DFKD%29%20enables%20the%20knowledge%20transfer%20from%0Athe%20given%20pre-trained%20teacher%20network%20to%20the%20target%20student%20model%20without%0Aaccess%20to%20the%20real%20training%20data.%20Existing%20DFKD%20methods%20focus%20primarily%20on%0Aimproving%20image%20recognition%20performance%20on%20associated%20datasets%2C%20often%0Aneglecting%20the%20crucial%20aspect%20of%20the%20transferability%20of%20learned%0Arepresentations.%20In%20this%20paper%2C%20we%20propose%20Category-Aware%20Embedding%20Data-Free%0AKnowledge%20Distillation%20%28CAE-DFKD%29%2C%20which%20addresses%20at%20the%20embedding%20level%20the%0Alimitations%20of%20previous%20rely%20on%20image-level%20methods%20to%20improve%20model%0Ageneralization%20but%20fail%20when%20directly%20applied%20to%20DFKD.%20The%20superiority%20and%0Aflexibility%20of%20CAE-DFKD%20are%20extensively%20evaluated%2C%20including%3A%0A%5Ctextit%7B%5Ctextbf%7Bi.%29%7D%7D%20Significant%20efficiency%20advantages%20resulting%20from%20altering%0Athe%20generator%20training%20paradigm%3B%20%5Ctextit%7B%5Ctextbf%7Bii.%29%7D%7D%20Competitive%20performance%0Awith%20existing%20DFKD%20state-of-the-art%20methods%20on%20image%20recognition%20tasks%3B%0A%5Ctextit%7B%5Ctextbf%7Biii.%29%7D%7D%20Remarkable%20transferability%20of%20data-free%20learned%0Arepresentations%20demonstrated%20in%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21478v1&entry.124074799=Read"},
{"title": "Gaussian process surrogate model to approximate power grid simulators --\n  An application to the certification of a congestion management controller", "author": "Pierre Houdouin and Lucas Saludjian", "abstract": "  With the digitalization of power grids, physical equations become\ninsufficient to describe the network's behavior, and realistic but\ntime-consuming simulators must be used. Numerical experiments, such as safety\nvalidation, that involve simulating a large number of scenarios become\ncomputationally intractable. A popular solution to reduce the computational\nburden is to learn a surrogate model of the simulator with Machine Learning\n(ML) and then conduct the experiment directly on the fast-to-evaluate surrogate\nmodel. Among the various ML possibilities for building surrogate models,\nGaussian processes (GPs) emerged as a popular solution due to their\nflexibility, data efficiency, and interpretability. Their probabilistic nature\nenables them to provide both predictions and uncertainty quantification (UQ).\nThis paper starts with a discussion on the interest of using GPs to approximate\npower grid simulators and fasten numerical experiments. Such simulators,\nhowever, often violate the GP's underlying Gaussian assumption, leading to poor\napproximations. To address this limitation, an approach that consists in adding\nan adaptive residual uncertainty term to the UQ is proposed. It enables the GP\nto remain accurate and reliable despite the simulator's non-Gaussian behaviors.\nThis approach is successfully applied to the certification of the proper\nfunctioning of a congestion management controller, with over 98% of simulations\navoided.\n", "link": "http://arxiv.org/abs/2503.00094v2", "date": "2025-04-30", "relevancy": 2.4171, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4891}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20process%20surrogate%20model%20to%20approximate%20power%20grid%20simulators%20--%0A%20%20An%20application%20to%20the%20certification%20of%20a%20congestion%20management%20controller&body=Title%3A%20Gaussian%20process%20surrogate%20model%20to%20approximate%20power%20grid%20simulators%20--%0A%20%20An%20application%20to%20the%20certification%20of%20a%20congestion%20management%20controller%0AAuthor%3A%20Pierre%20Houdouin%20and%20Lucas%20Saludjian%0AAbstract%3A%20%20%20With%20the%20digitalization%20of%20power%20grids%2C%20physical%20equations%20become%0Ainsufficient%20to%20describe%20the%20network%27s%20behavior%2C%20and%20realistic%20but%0Atime-consuming%20simulators%20must%20be%20used.%20Numerical%20experiments%2C%20such%20as%20safety%0Avalidation%2C%20that%20involve%20simulating%20a%20large%20number%20of%20scenarios%20become%0Acomputationally%20intractable.%20A%20popular%20solution%20to%20reduce%20the%20computational%0Aburden%20is%20to%20learn%20a%20surrogate%20model%20of%20the%20simulator%20with%20Machine%20Learning%0A%28ML%29%20and%20then%20conduct%20the%20experiment%20directly%20on%20the%20fast-to-evaluate%20surrogate%0Amodel.%20Among%20the%20various%20ML%20possibilities%20for%20building%20surrogate%20models%2C%0AGaussian%20processes%20%28GPs%29%20emerged%20as%20a%20popular%20solution%20due%20to%20their%0Aflexibility%2C%20data%20efficiency%2C%20and%20interpretability.%20Their%20probabilistic%20nature%0Aenables%20them%20to%20provide%20both%20predictions%20and%20uncertainty%20quantification%20%28UQ%29.%0AThis%20paper%20starts%20with%20a%20discussion%20on%20the%20interest%20of%20using%20GPs%20to%20approximate%0Apower%20grid%20simulators%20and%20fasten%20numerical%20experiments.%20Such%20simulators%2C%0Ahowever%2C%20often%20violate%20the%20GP%27s%20underlying%20Gaussian%20assumption%2C%20leading%20to%20poor%0Aapproximations.%20To%20address%20this%20limitation%2C%20an%20approach%20that%20consists%20in%20adding%0Aan%20adaptive%20residual%20uncertainty%20term%20to%20the%20UQ%20is%20proposed.%20It%20enables%20the%20GP%0Ato%20remain%20accurate%20and%20reliable%20despite%20the%20simulator%27s%20non-Gaussian%20behaviors.%0AThis%20approach%20is%20successfully%20applied%20to%20the%20certification%20of%20the%20proper%0Afunctioning%20of%20a%20congestion%20management%20controller%2C%20with%20over%2098%25%20of%20simulations%0Aavoided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520process%2520surrogate%2520model%2520to%2520approximate%2520power%2520grid%2520simulators%2520--%250A%2520%2520An%2520application%2520to%2520the%2520certification%2520of%2520a%2520congestion%2520management%2520controller%26entry.906535625%3DPierre%2520Houdouin%2520and%2520Lucas%2520Saludjian%26entry.1292438233%3D%2520%2520With%2520the%2520digitalization%2520of%2520power%2520grids%252C%2520physical%2520equations%2520become%250Ainsufficient%2520to%2520describe%2520the%2520network%2527s%2520behavior%252C%2520and%2520realistic%2520but%250Atime-consuming%2520simulators%2520must%2520be%2520used.%2520Numerical%2520experiments%252C%2520such%2520as%2520safety%250Avalidation%252C%2520that%2520involve%2520simulating%2520a%2520large%2520number%2520of%2520scenarios%2520become%250Acomputationally%2520intractable.%2520A%2520popular%2520solution%2520to%2520reduce%2520the%2520computational%250Aburden%2520is%2520to%2520learn%2520a%2520surrogate%2520model%2520of%2520the%2520simulator%2520with%2520Machine%2520Learning%250A%2528ML%2529%2520and%2520then%2520conduct%2520the%2520experiment%2520directly%2520on%2520the%2520fast-to-evaluate%2520surrogate%250Amodel.%2520Among%2520the%2520various%2520ML%2520possibilities%2520for%2520building%2520surrogate%2520models%252C%250AGaussian%2520processes%2520%2528GPs%2529%2520emerged%2520as%2520a%2520popular%2520solution%2520due%2520to%2520their%250Aflexibility%252C%2520data%2520efficiency%252C%2520and%2520interpretability.%2520Their%2520probabilistic%2520nature%250Aenables%2520them%2520to%2520provide%2520both%2520predictions%2520and%2520uncertainty%2520quantification%2520%2528UQ%2529.%250AThis%2520paper%2520starts%2520with%2520a%2520discussion%2520on%2520the%2520interest%2520of%2520using%2520GPs%2520to%2520approximate%250Apower%2520grid%2520simulators%2520and%2520fasten%2520numerical%2520experiments.%2520Such%2520simulators%252C%250Ahowever%252C%2520often%2520violate%2520the%2520GP%2527s%2520underlying%2520Gaussian%2520assumption%252C%2520leading%2520to%2520poor%250Aapproximations.%2520To%2520address%2520this%2520limitation%252C%2520an%2520approach%2520that%2520consists%2520in%2520adding%250Aan%2520adaptive%2520residual%2520uncertainty%2520term%2520to%2520the%2520UQ%2520is%2520proposed.%2520It%2520enables%2520the%2520GP%250Ato%2520remain%2520accurate%2520and%2520reliable%2520despite%2520the%2520simulator%2527s%2520non-Gaussian%2520behaviors.%250AThis%2520approach%2520is%2520successfully%2520applied%2520to%2520the%2520certification%2520of%2520the%2520proper%250Afunctioning%2520of%2520a%2520congestion%2520management%2520controller%252C%2520with%2520over%252098%2525%2520of%2520simulations%250Aavoided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20process%20surrogate%20model%20to%20approximate%20power%20grid%20simulators%20--%0A%20%20An%20application%20to%20the%20certification%20of%20a%20congestion%20management%20controller&entry.906535625=Pierre%20Houdouin%20and%20Lucas%20Saludjian&entry.1292438233=%20%20With%20the%20digitalization%20of%20power%20grids%2C%20physical%20equations%20become%0Ainsufficient%20to%20describe%20the%20network%27s%20behavior%2C%20and%20realistic%20but%0Atime-consuming%20simulators%20must%20be%20used.%20Numerical%20experiments%2C%20such%20as%20safety%0Avalidation%2C%20that%20involve%20simulating%20a%20large%20number%20of%20scenarios%20become%0Acomputationally%20intractable.%20A%20popular%20solution%20to%20reduce%20the%20computational%0Aburden%20is%20to%20learn%20a%20surrogate%20model%20of%20the%20simulator%20with%20Machine%20Learning%0A%28ML%29%20and%20then%20conduct%20the%20experiment%20directly%20on%20the%20fast-to-evaluate%20surrogate%0Amodel.%20Among%20the%20various%20ML%20possibilities%20for%20building%20surrogate%20models%2C%0AGaussian%20processes%20%28GPs%29%20emerged%20as%20a%20popular%20solution%20due%20to%20their%0Aflexibility%2C%20data%20efficiency%2C%20and%20interpretability.%20Their%20probabilistic%20nature%0Aenables%20them%20to%20provide%20both%20predictions%20and%20uncertainty%20quantification%20%28UQ%29.%0AThis%20paper%20starts%20with%20a%20discussion%20on%20the%20interest%20of%20using%20GPs%20to%20approximate%0Apower%20grid%20simulators%20and%20fasten%20numerical%20experiments.%20Such%20simulators%2C%0Ahowever%2C%20often%20violate%20the%20GP%27s%20underlying%20Gaussian%20assumption%2C%20leading%20to%20poor%0Aapproximations.%20To%20address%20this%20limitation%2C%20an%20approach%20that%20consists%20in%20adding%0Aan%20adaptive%20residual%20uncertainty%20term%20to%20the%20UQ%20is%20proposed.%20It%20enables%20the%20GP%0Ato%20remain%20accurate%20and%20reliable%20despite%20the%20simulator%27s%20non-Gaussian%20behaviors.%0AThis%20approach%20is%20successfully%20applied%20to%20the%20certification%20of%20the%20proper%0Afunctioning%20of%20a%20congestion%20management%20controller%2C%20with%20over%2098%25%20of%20simulations%0Aavoided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00094v2&entry.124074799=Read"},
{"title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via\n  Adaptive Radial Fusion", "author": "Shuhao Kang and Martin Y. Liao and Yan Xia and Olaf Wysocki and Boris Jutzi and Daniel Cremers", "abstract": "  LiDAR place recognition is a critical capability for autonomous navigation\nand cross-modal localization in large-scale outdoor environments. Existing\napproaches predominantly depend on pre-built 3D dense maps or aerial imagery,\nwhich impose significant storage overhead and lack real-time adaptability. In\nthis paper, we propose OPAL, a novel network for LiDAR place recognition that\nleverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key\ninnovation lies in bridging the domain disparity between sparse LiDAR scans and\nstructured OSM data through two carefully designed components. First, a\ncross-modal visibility mask that identifies maximal observable regions from\nboth modalities to guide feature learning. Second, an adaptive radial fusion\nmodule that dynamically consolidates radial features into discriminative global\ndescriptors. Extensive experiments on the KITTI and KITTI-360 datasets\ndemonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold\nfor top-1 retrieved matches, along with 12x faster inference speed compared to\nthe state-of-the-art approach. Code and datasets will be publicly available.\n", "link": "http://arxiv.org/abs/2504.19258v2", "date": "2025-04-30", "relevancy": 2.3982, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.654}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5664}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPAL%3A%20Visibility-aware%20LiDAR-to-OpenStreetMap%20Place%20Recognition%20via%0A%20%20Adaptive%20Radial%20Fusion&body=Title%3A%20OPAL%3A%20Visibility-aware%20LiDAR-to-OpenStreetMap%20Place%20Recognition%20via%0A%20%20Adaptive%20Radial%20Fusion%0AAuthor%3A%20Shuhao%20Kang%20and%20Martin%20Y.%20Liao%20and%20Yan%20Xia%20and%20Olaf%20Wysocki%20and%20Boris%20Jutzi%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20LiDAR%20place%20recognition%20is%20a%20critical%20capability%20for%20autonomous%20navigation%0Aand%20cross-modal%20localization%20in%20large-scale%20outdoor%20environments.%20Existing%0Aapproaches%20predominantly%20depend%20on%20pre-built%203D%20dense%20maps%20or%20aerial%20imagery%2C%0Awhich%20impose%20significant%20storage%20overhead%20and%20lack%20real-time%20adaptability.%20In%0Athis%20paper%2C%20we%20propose%20OPAL%2C%20a%20novel%20network%20for%20LiDAR%20place%20recognition%20that%0Aleverages%20OpenStreetMap%20%28OSM%29%20as%20a%20lightweight%20and%20up-to-date%20prior.%20Our%20key%0Ainnovation%20lies%20in%20bridging%20the%20domain%20disparity%20between%20sparse%20LiDAR%20scans%20and%0Astructured%20OSM%20data%20through%20two%20carefully%20designed%20components.%20First%2C%20a%0Across-modal%20visibility%20mask%20that%20identifies%20maximal%20observable%20regions%20from%0Aboth%20modalities%20to%20guide%20feature%20learning.%20Second%2C%20an%20adaptive%20radial%20fusion%0Amodule%20that%20dynamically%20consolidates%20radial%20features%20into%20discriminative%20global%0Adescriptors.%20Extensive%20experiments%20on%20the%20KITTI%20and%20KITTI-360%20datasets%0Ademonstrate%20OPAL%27s%20superiority%2C%20achieving%2015.98%25%20higher%20recall%20at%20%401m%20threshold%0Afor%20top-1%20retrieved%20matches%2C%20along%20with%2012x%20faster%20inference%20speed%20compared%20to%0Athe%20state-of-the-art%20approach.%20Code%20and%20datasets%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPAL%253A%2520Visibility-aware%2520LiDAR-to-OpenStreetMap%2520Place%2520Recognition%2520via%250A%2520%2520Adaptive%2520Radial%2520Fusion%26entry.906535625%3DShuhao%2520Kang%2520and%2520Martin%2520Y.%2520Liao%2520and%2520Yan%2520Xia%2520and%2520Olaf%2520Wysocki%2520and%2520Boris%2520Jutzi%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520LiDAR%2520place%2520recognition%2520is%2520a%2520critical%2520capability%2520for%2520autonomous%2520navigation%250Aand%2520cross-modal%2520localization%2520in%2520large-scale%2520outdoor%2520environments.%2520Existing%250Aapproaches%2520predominantly%2520depend%2520on%2520pre-built%25203D%2520dense%2520maps%2520or%2520aerial%2520imagery%252C%250Awhich%2520impose%2520significant%2520storage%2520overhead%2520and%2520lack%2520real-time%2520adaptability.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520OPAL%252C%2520a%2520novel%2520network%2520for%2520LiDAR%2520place%2520recognition%2520that%250Aleverages%2520OpenStreetMap%2520%2528OSM%2529%2520as%2520a%2520lightweight%2520and%2520up-to-date%2520prior.%2520Our%2520key%250Ainnovation%2520lies%2520in%2520bridging%2520the%2520domain%2520disparity%2520between%2520sparse%2520LiDAR%2520scans%2520and%250Astructured%2520OSM%2520data%2520through%2520two%2520carefully%2520designed%2520components.%2520First%252C%2520a%250Across-modal%2520visibility%2520mask%2520that%2520identifies%2520maximal%2520observable%2520regions%2520from%250Aboth%2520modalities%2520to%2520guide%2520feature%2520learning.%2520Second%252C%2520an%2520adaptive%2520radial%2520fusion%250Amodule%2520that%2520dynamically%2520consolidates%2520radial%2520features%2520into%2520discriminative%2520global%250Adescriptors.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%2520and%2520KITTI-360%2520datasets%250Ademonstrate%2520OPAL%2527s%2520superiority%252C%2520achieving%252015.98%2525%2520higher%2520recall%2520at%2520%25401m%2520threshold%250Afor%2520top-1%2520retrieved%2520matches%252C%2520along%2520with%252012x%2520faster%2520inference%2520speed%2520compared%2520to%250Athe%2520state-of-the-art%2520approach.%2520Code%2520and%2520datasets%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPAL%3A%20Visibility-aware%20LiDAR-to-OpenStreetMap%20Place%20Recognition%20via%0A%20%20Adaptive%20Radial%20Fusion&entry.906535625=Shuhao%20Kang%20and%20Martin%20Y.%20Liao%20and%20Yan%20Xia%20and%20Olaf%20Wysocki%20and%20Boris%20Jutzi%20and%20Daniel%20Cremers&entry.1292438233=%20%20LiDAR%20place%20recognition%20is%20a%20critical%20capability%20for%20autonomous%20navigation%0Aand%20cross-modal%20localization%20in%20large-scale%20outdoor%20environments.%20Existing%0Aapproaches%20predominantly%20depend%20on%20pre-built%203D%20dense%20maps%20or%20aerial%20imagery%2C%0Awhich%20impose%20significant%20storage%20overhead%20and%20lack%20real-time%20adaptability.%20In%0Athis%20paper%2C%20we%20propose%20OPAL%2C%20a%20novel%20network%20for%20LiDAR%20place%20recognition%20that%0Aleverages%20OpenStreetMap%20%28OSM%29%20as%20a%20lightweight%20and%20up-to-date%20prior.%20Our%20key%0Ainnovation%20lies%20in%20bridging%20the%20domain%20disparity%20between%20sparse%20LiDAR%20scans%20and%0Astructured%20OSM%20data%20through%20two%20carefully%20designed%20components.%20First%2C%20a%0Across-modal%20visibility%20mask%20that%20identifies%20maximal%20observable%20regions%20from%0Aboth%20modalities%20to%20guide%20feature%20learning.%20Second%2C%20an%20adaptive%20radial%20fusion%0Amodule%20that%20dynamically%20consolidates%20radial%20features%20into%20discriminative%20global%0Adescriptors.%20Extensive%20experiments%20on%20the%20KITTI%20and%20KITTI-360%20datasets%0Ademonstrate%20OPAL%27s%20superiority%2C%20achieving%2015.98%25%20higher%20recall%20at%20%401m%20threshold%0Afor%20top-1%20retrieved%20matches%2C%20along%20with%2012x%20faster%20inference%20speed%20compared%20to%0Athe%20state-of-the-art%20approach.%20Code%20and%20datasets%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19258v2&entry.124074799=Read"},
{"title": "Leveraging Motion Information for Better Self-Supervised Video\n  Correspondence Learning", "author": "Zihan Zhou and Changrui Dai and Aibo Song and Xiaolin Fang", "abstract": "  Self-supervised video correspondence learning depends on the ability to\naccurately associate pixels between video frames that correspond to the same\nvisual object. However, achieving reliable pixel matching without supervision\nremains a major challenge. To address this issue, recent research has focused\non feature learning techniques that aim to encode unique pixel representations\nfor matching. Despite these advances, existing methods still struggle to\nachieve exact pixel correspondences and often suffer from false matches,\nlimiting their effectiveness in self-supervised settings.\n  To this end, we explore an efficient self-supervised Video Correspondence\nLearning framework (MER) that aims to accurately extract object details from\nunlabeled videos. First, we design a dedicated Motion Enhancement Engine that\nemphasizes capturing the dynamic motion of objects in videos. In addition, we\nintroduce a flexible sampling strategy for inter-pixel correspondence\ninformation (Multi-Cluster Sampler) that enables the model to pay more\nattention to the pixel changes of important objects in motion. Through\nexperiments, our algorithm outperforms the state-of-the-art competitors on\nvideo correspondence learning tasks such as video object segmentation and video\nobject keypoint tracking.\n", "link": "http://arxiv.org/abs/2503.12026v2", "date": "2025-04-30", "relevancy": 2.3955, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.618}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5881}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Motion%20Information%20for%20Better%20Self-Supervised%20Video%0A%20%20Correspondence%20Learning&body=Title%3A%20Leveraging%20Motion%20Information%20for%20Better%20Self-Supervised%20Video%0A%20%20Correspondence%20Learning%0AAuthor%3A%20Zihan%20Zhou%20and%20Changrui%20Dai%20and%20Aibo%20Song%20and%20Xiaolin%20Fang%0AAbstract%3A%20%20%20Self-supervised%20video%20correspondence%20learning%20depends%20on%20the%20ability%20to%0Aaccurately%20associate%20pixels%20between%20video%20frames%20that%20correspond%20to%20the%20same%0Avisual%20object.%20However%2C%20achieving%20reliable%20pixel%20matching%20without%20supervision%0Aremains%20a%20major%20challenge.%20To%20address%20this%20issue%2C%20recent%20research%20has%20focused%0Aon%20feature%20learning%20techniques%20that%20aim%20to%20encode%20unique%20pixel%20representations%0Afor%20matching.%20Despite%20these%20advances%2C%20existing%20methods%20still%20struggle%20to%0Aachieve%20exact%20pixel%20correspondences%20and%20often%20suffer%20from%20false%20matches%2C%0Alimiting%20their%20effectiveness%20in%20self-supervised%20settings.%0A%20%20To%20this%20end%2C%20we%20explore%20an%20efficient%20self-supervised%20Video%20Correspondence%0ALearning%20framework%20%28MER%29%20that%20aims%20to%20accurately%20extract%20object%20details%20from%0Aunlabeled%20videos.%20First%2C%20we%20design%20a%20dedicated%20Motion%20Enhancement%20Engine%20that%0Aemphasizes%20capturing%20the%20dynamic%20motion%20of%20objects%20in%20videos.%20In%20addition%2C%20we%0Aintroduce%20a%20flexible%20sampling%20strategy%20for%20inter-pixel%20correspondence%0Ainformation%20%28Multi-Cluster%20Sampler%29%20that%20enables%20the%20model%20to%20pay%20more%0Aattention%20to%20the%20pixel%20changes%20of%20important%20objects%20in%20motion.%20Through%0Aexperiments%2C%20our%20algorithm%20outperforms%20the%20state-of-the-art%20competitors%20on%0Avideo%20correspondence%20learning%20tasks%20such%20as%20video%20object%20segmentation%20and%20video%0Aobject%20keypoint%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Motion%2520Information%2520for%2520Better%2520Self-Supervised%2520Video%250A%2520%2520Correspondence%2520Learning%26entry.906535625%3DZihan%2520Zhou%2520and%2520Changrui%2520Dai%2520and%2520Aibo%2520Song%2520and%2520Xiaolin%2520Fang%26entry.1292438233%3D%2520%2520Self-supervised%2520video%2520correspondence%2520learning%2520depends%2520on%2520the%2520ability%2520to%250Aaccurately%2520associate%2520pixels%2520between%2520video%2520frames%2520that%2520correspond%2520to%2520the%2520same%250Avisual%2520object.%2520However%252C%2520achieving%2520reliable%2520pixel%2520matching%2520without%2520supervision%250Aremains%2520a%2520major%2520challenge.%2520To%2520address%2520this%2520issue%252C%2520recent%2520research%2520has%2520focused%250Aon%2520feature%2520learning%2520techniques%2520that%2520aim%2520to%2520encode%2520unique%2520pixel%2520representations%250Afor%2520matching.%2520Despite%2520these%2520advances%252C%2520existing%2520methods%2520still%2520struggle%2520to%250Aachieve%2520exact%2520pixel%2520correspondences%2520and%2520often%2520suffer%2520from%2520false%2520matches%252C%250Alimiting%2520their%2520effectiveness%2520in%2520self-supervised%2520settings.%250A%2520%2520To%2520this%2520end%252C%2520we%2520explore%2520an%2520efficient%2520self-supervised%2520Video%2520Correspondence%250ALearning%2520framework%2520%2528MER%2529%2520that%2520aims%2520to%2520accurately%2520extract%2520object%2520details%2520from%250Aunlabeled%2520videos.%2520First%252C%2520we%2520design%2520a%2520dedicated%2520Motion%2520Enhancement%2520Engine%2520that%250Aemphasizes%2520capturing%2520the%2520dynamic%2520motion%2520of%2520objects%2520in%2520videos.%2520In%2520addition%252C%2520we%250Aintroduce%2520a%2520flexible%2520sampling%2520strategy%2520for%2520inter-pixel%2520correspondence%250Ainformation%2520%2528Multi-Cluster%2520Sampler%2529%2520that%2520enables%2520the%2520model%2520to%2520pay%2520more%250Aattention%2520to%2520the%2520pixel%2520changes%2520of%2520important%2520objects%2520in%2520motion.%2520Through%250Aexperiments%252C%2520our%2520algorithm%2520outperforms%2520the%2520state-of-the-art%2520competitors%2520on%250Avideo%2520correspondence%2520learning%2520tasks%2520such%2520as%2520video%2520object%2520segmentation%2520and%2520video%250Aobject%2520keypoint%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Motion%20Information%20for%20Better%20Self-Supervised%20Video%0A%20%20Correspondence%20Learning&entry.906535625=Zihan%20Zhou%20and%20Changrui%20Dai%20and%20Aibo%20Song%20and%20Xiaolin%20Fang&entry.1292438233=%20%20Self-supervised%20video%20correspondence%20learning%20depends%20on%20the%20ability%20to%0Aaccurately%20associate%20pixels%20between%20video%20frames%20that%20correspond%20to%20the%20same%0Avisual%20object.%20However%2C%20achieving%20reliable%20pixel%20matching%20without%20supervision%0Aremains%20a%20major%20challenge.%20To%20address%20this%20issue%2C%20recent%20research%20has%20focused%0Aon%20feature%20learning%20techniques%20that%20aim%20to%20encode%20unique%20pixel%20representations%0Afor%20matching.%20Despite%20these%20advances%2C%20existing%20methods%20still%20struggle%20to%0Aachieve%20exact%20pixel%20correspondences%20and%20often%20suffer%20from%20false%20matches%2C%0Alimiting%20their%20effectiveness%20in%20self-supervised%20settings.%0A%20%20To%20this%20end%2C%20we%20explore%20an%20efficient%20self-supervised%20Video%20Correspondence%0ALearning%20framework%20%28MER%29%20that%20aims%20to%20accurately%20extract%20object%20details%20from%0Aunlabeled%20videos.%20First%2C%20we%20design%20a%20dedicated%20Motion%20Enhancement%20Engine%20that%0Aemphasizes%20capturing%20the%20dynamic%20motion%20of%20objects%20in%20videos.%20In%20addition%2C%20we%0Aintroduce%20a%20flexible%20sampling%20strategy%20for%20inter-pixel%20correspondence%0Ainformation%20%28Multi-Cluster%20Sampler%29%20that%20enables%20the%20model%20to%20pay%20more%0Aattention%20to%20the%20pixel%20changes%20of%20important%20objects%20in%20motion.%20Through%0Aexperiments%2C%20our%20algorithm%20outperforms%20the%20state-of-the-art%20competitors%20on%0Avideo%20correspondence%20learning%20tasks%20such%20as%20video%20object%20segmentation%20and%20video%0Aobject%20keypoint%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12026v2&entry.124074799=Read"},
{"title": "T2VEval: Benchmark Dataset and Objective Evaluation Method for\n  T2V-generated Videos", "author": "Zelu Qi and Ping Shi and Shuqi Wang and Chaoyang Zhang and Fei Zhao and Zefeng Ying and Da Pan and Xi Yang and Zheqi He and Teng Dai", "abstract": "  Recent advances in text-to-video (T2V) technology, as demonstrated by models\nsuch as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the\napplicability and popularity of the technology. This progress has created a\ngrowing demand for accurate quality assessment metrics to evaluate the\nperceptual quality of T2V-generated videos and optimize video generation\nmodels. However, assessing the quality of text-to-video outputs remain\nchallenging due to the presence of highly complex distortions, such as\nunnatural actions and phenomena that defy human cognition. To address these\nchallenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset\nfor text-to-video quality evaluation, which contains 148 textual prompts and\n1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation,\nwe scored each video on four dimensions in the subjective experiment, which are\noverall impression, text-video consistency, realness, and technical quality.\nBased on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for\nT2V quality evaluation. T2VEval assesses videos across three branches:\ntext-video consistency, realness, and technical quality. Using an\nattention-based fusion module, T2VEval effectively integrates features from\neach branch and predicts scores with the aid of a large language model.\nAdditionally, we implemented a divide-and-conquer training strategy, enabling\neach branch to learn targeted knowledge while maintaining synergy with the\nothers. Experimental results demonstrate that T2VEval achieves state-of-the-art\nperformance across multiple metrics.\n", "link": "http://arxiv.org/abs/2501.08545v6", "date": "2025-04-30", "relevancy": 2.3955, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6287}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6136}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2VEval%3A%20Benchmark%20Dataset%20and%20Objective%20Evaluation%20Method%20for%0A%20%20T2V-generated%20Videos&body=Title%3A%20T2VEval%3A%20Benchmark%20Dataset%20and%20Objective%20Evaluation%20Method%20for%0A%20%20T2V-generated%20Videos%0AAuthor%3A%20Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Chaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan%20and%20Xi%20Yang%20and%20Zheqi%20He%20and%20Teng%20Dai%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20which%20contains%20148%20textual%20prompts%20and%0A1%2C783%20videos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%0Awe%20scored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20divide-and-conquer%20training%20strategy%2C%20enabling%0Aeach%20branch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%0Aothers.%20Experimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08545v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2VEval%253A%2520Benchmark%2520Dataset%2520and%2520Objective%2520Evaluation%2520Method%2520for%250A%2520%2520T2V-generated%2520Videos%26entry.906535625%3DZelu%2520Qi%2520and%2520Ping%2520Shi%2520and%2520Shuqi%2520Wang%2520and%2520Chaoyang%2520Zhang%2520and%2520Fei%2520Zhao%2520and%2520Zefeng%2520Ying%2520and%2520Da%2520Pan%2520and%2520Xi%2520Yang%2520and%2520Zheqi%2520He%2520and%2520Teng%2520Dai%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-video%2520%2528T2V%2529%2520technology%252C%2520as%2520demonstrated%2520by%2520models%250Asuch%2520as%2520Runway%2520Gen-3%252C%2520Pika%252C%2520Sora%252C%2520and%2520Kling%252C%2520have%2520significantly%2520broadened%2520the%250Aapplicability%2520and%2520popularity%2520of%2520the%2520technology.%2520This%2520progress%2520has%2520created%2520a%250Agrowing%2520demand%2520for%2520accurate%2520quality%2520assessment%2520metrics%2520to%2520evaluate%2520the%250Aperceptual%2520quality%2520of%2520T2V-generated%2520videos%2520and%2520optimize%2520video%2520generation%250Amodels.%2520However%252C%2520assessing%2520the%2520quality%2520of%2520text-to-video%2520outputs%2520remain%250Achallenging%2520due%2520to%2520the%2520presence%2520of%2520highly%2520complex%2520distortions%252C%2520such%2520as%250Aunnatural%2520actions%2520and%2520phenomena%2520that%2520defy%2520human%2520cognition.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520constructed%2520T2VEval-Bench%252C%2520a%2520multi-dimensional%2520benchmark%2520dataset%250Afor%2520text-to-video%2520quality%2520evaluation%252C%2520which%2520contains%2520148%2520textual%2520prompts%2520and%250A1%252C783%2520videos%2520generated%2520by%252013%2520T2V%2520models.%2520To%2520ensure%2520a%2520comprehensive%2520evaluation%252C%250Awe%2520scored%2520each%2520video%2520on%2520four%2520dimensions%2520in%2520the%2520subjective%2520experiment%252C%2520which%2520are%250Aoverall%2520impression%252C%2520text-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%250ABased%2520on%2520T2VEval-Bench%252C%2520we%2520developed%2520T2VEval%252C%2520a%2520multi-branch%2520fusion%2520scheme%2520for%250AT2V%2520quality%2520evaluation.%2520T2VEval%2520assesses%2520videos%2520across%2520three%2520branches%253A%250Atext-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%2520Using%2520an%250Aattention-based%2520fusion%2520module%252C%2520T2VEval%2520effectively%2520integrates%2520features%2520from%250Aeach%2520branch%2520and%2520predicts%2520scores%2520with%2520the%2520aid%2520of%2520a%2520large%2520language%2520model.%250AAdditionally%252C%2520we%2520implemented%2520a%2520divide-and-conquer%2520training%2520strategy%252C%2520enabling%250Aeach%2520branch%2520to%2520learn%2520targeted%2520knowledge%2520while%2520maintaining%2520synergy%2520with%2520the%250Aothers.%2520Experimental%2520results%2520demonstrate%2520that%2520T2VEval%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520multiple%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08545v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2VEval%3A%20Benchmark%20Dataset%20and%20Objective%20Evaluation%20Method%20for%0A%20%20T2V-generated%20Videos&entry.906535625=Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Chaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan%20and%20Xi%20Yang%20and%20Zheqi%20He%20and%20Teng%20Dai&entry.1292438233=%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20which%20contains%20148%20textual%20prompts%20and%0A1%2C783%20videos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%0Awe%20scored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20divide-and-conquer%20training%20strategy%2C%20enabling%0Aeach%20branch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%0Aothers.%20Experimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08545v6&entry.124074799=Read"},
{"title": "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable\n  and Intelligent Embodied Systems", "author": " AgiBot-World-Contributors and Qingwen Bu and Jisong Cai and Li Chen and Xiuqi Cui and Yan Ding and Siyuan Feng and Shenyuan Gao and Xindong He and Xuan Hu and Xu Huang and Shu Jiang and Yuxin Jiang and Cheng Jing and Hongyang Li and Jialu Li and Chiming Liu and Yi Liu and Yuxiang Lu and Jianlan Luo and Ping Luo and Yao Mu and Yuehan Niu and Yixuan Pan and Jiangmiao Pang and Yu Qiao and Guanghui Ren and Cheng Ruan and Jiaqi Shan and Yongjian Shen and Chengshi Shi and Mingkang Shi and Modi Shi and Chonghao Sima and Jianheng Song and Huijie Wang and Wenhao Wang and Dafeng Wei and Chengen Xie and Guo Xu and Junchi Yan and Cunbiao Yang and Lei Yang and Shukai Yang and Maoqing Yao and Jia Zeng and Chi Zhang and Qinglin Zhang and Bin Zhao and Chengyue Zhao and Jiaqi Zhao and Jianchao Zhu", "abstract": "  We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence.\n", "link": "http://arxiv.org/abs/2503.06669v3", "date": "2025-04-30", "relevancy": 2.3761, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.654}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5829}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgiBot%20World%20Colosseo%3A%20A%20Large-scale%20Manipulation%20Platform%20for%20Scalable%0A%20%20and%20Intelligent%20Embodied%20Systems&body=Title%3A%20AgiBot%20World%20Colosseo%3A%20A%20Large-scale%20Manipulation%20Platform%20for%20Scalable%0A%20%20and%20Intelligent%20Embodied%20Systems%0AAuthor%3A%20%20AgiBot-World-Contributors%20and%20Qingwen%20Bu%20and%20Jisong%20Cai%20and%20Li%20Chen%20and%20Xiuqi%20Cui%20and%20Yan%20Ding%20and%20Siyuan%20Feng%20and%20Shenyuan%20Gao%20and%20Xindong%20He%20and%20Xuan%20Hu%20and%20Xu%20Huang%20and%20Shu%20Jiang%20and%20Yuxin%20Jiang%20and%20Cheng%20Jing%20and%20Hongyang%20Li%20and%20Jialu%20Li%20and%20Chiming%20Liu%20and%20Yi%20Liu%20and%20Yuxiang%20Lu%20and%20Jianlan%20Luo%20and%20Ping%20Luo%20and%20Yao%20Mu%20and%20Yuehan%20Niu%20and%20Yixuan%20Pan%20and%20Jiangmiao%20Pang%20and%20Yu%20Qiao%20and%20Guanghui%20Ren%20and%20Cheng%20Ruan%20and%20Jiaqi%20Shan%20and%20Yongjian%20Shen%20and%20Chengshi%20Shi%20and%20Mingkang%20Shi%20and%20Modi%20Shi%20and%20Chonghao%20Sima%20and%20Jianheng%20Song%20and%20Huijie%20Wang%20and%20Wenhao%20Wang%20and%20Dafeng%20Wei%20and%20Chengen%20Xie%20and%20Guo%20Xu%20and%20Junchi%20Yan%20and%20Cunbiao%20Yang%20and%20Lei%20Yang%20and%20Shukai%20Yang%20and%20Maoqing%20Yao%20and%20Jia%20Zeng%20and%20Chi%20Zhang%20and%20Qinglin%20Zhang%20and%20Bin%20Zhao%20and%20Chengyue%20Zhao%20and%20Jiaqi%20Zhao%20and%20Jianchao%20Zhu%0AAbstract%3A%20%20%20We%20explore%20how%20scalable%20robot%20data%20can%20address%20real-world%20challenges%20for%0Ageneralized%20robotic%20manipulation.%20Introducing%20AgiBot%20World%2C%20a%20large-scale%0Aplatform%20comprising%20over%201%20million%20trajectories%20across%20217%20tasks%20in%20five%0Adeployment%20scenarios%2C%20we%20achieve%20an%20order-of-magnitude%20increase%20in%20data%20scale%0Acompared%20to%20existing%20datasets.%20Accelerated%20by%20a%20standardized%20collection%0Apipeline%20with%20human-in-the-loop%20verification%2C%20AgiBot%20World%20guarantees%0Ahigh-quality%20and%20diverse%20data%20distribution.%20It%20is%20extensible%20from%20grippers%20to%0Adexterous%20hands%20and%20visuo-tactile%20sensors%20for%20fine-grained%20skill%20acquisition.%0ABuilding%20on%20top%20of%20data%2C%20we%20introduce%20Genie%20Operator-1%20%28GO-1%29%2C%20a%20novel%0Ageneralist%20policy%20that%20leverages%20latent%20action%20representations%20to%20maximize%20data%0Autilization%2C%20demonstrating%20predictable%20performance%20scaling%20with%20increased%20data%0Avolume.%20Policies%20pre-trained%20on%20our%20dataset%20achieve%20an%20average%20performance%0Aimprovement%20of%2030%25%20over%20those%20trained%20on%20Open%20X-Embodiment%2C%20both%20in%20in-domain%0Aand%20out-of-distribution%20scenarios.%20GO-1%20exhibits%20exceptional%20capability%20in%0Areal-world%20dexterous%20and%20long-horizon%20tasks%2C%20achieving%20over%2060%25%20success%20rate%20on%0Acomplex%20tasks%20and%20outperforming%20prior%20RDT%20approach%20by%2032%25.%20By%20open-sourcing%20the%0Adataset%2C%20tools%2C%20and%20models%2C%20we%20aim%20to%20democratize%20access%20to%20large-scale%2C%0Ahigh-quality%20robot%20data%2C%20advancing%20the%20pursuit%20of%20scalable%20and%20general-purpose%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06669v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgiBot%2520World%2520Colosseo%253A%2520A%2520Large-scale%2520Manipulation%2520Platform%2520for%2520Scalable%250A%2520%2520and%2520Intelligent%2520Embodied%2520Systems%26entry.906535625%3D%2520AgiBot-World-Contributors%2520and%2520Qingwen%2520Bu%2520and%2520Jisong%2520Cai%2520and%2520Li%2520Chen%2520and%2520Xiuqi%2520Cui%2520and%2520Yan%2520Ding%2520and%2520Siyuan%2520Feng%2520and%2520Shenyuan%2520Gao%2520and%2520Xindong%2520He%2520and%2520Xuan%2520Hu%2520and%2520Xu%2520Huang%2520and%2520Shu%2520Jiang%2520and%2520Yuxin%2520Jiang%2520and%2520Cheng%2520Jing%2520and%2520Hongyang%2520Li%2520and%2520Jialu%2520Li%2520and%2520Chiming%2520Liu%2520and%2520Yi%2520Liu%2520and%2520Yuxiang%2520Lu%2520and%2520Jianlan%2520Luo%2520and%2520Ping%2520Luo%2520and%2520Yao%2520Mu%2520and%2520Yuehan%2520Niu%2520and%2520Yixuan%2520Pan%2520and%2520Jiangmiao%2520Pang%2520and%2520Yu%2520Qiao%2520and%2520Guanghui%2520Ren%2520and%2520Cheng%2520Ruan%2520and%2520Jiaqi%2520Shan%2520and%2520Yongjian%2520Shen%2520and%2520Chengshi%2520Shi%2520and%2520Mingkang%2520Shi%2520and%2520Modi%2520Shi%2520and%2520Chonghao%2520Sima%2520and%2520Jianheng%2520Song%2520and%2520Huijie%2520Wang%2520and%2520Wenhao%2520Wang%2520and%2520Dafeng%2520Wei%2520and%2520Chengen%2520Xie%2520and%2520Guo%2520Xu%2520and%2520Junchi%2520Yan%2520and%2520Cunbiao%2520Yang%2520and%2520Lei%2520Yang%2520and%2520Shukai%2520Yang%2520and%2520Maoqing%2520Yao%2520and%2520Jia%2520Zeng%2520and%2520Chi%2520Zhang%2520and%2520Qinglin%2520Zhang%2520and%2520Bin%2520Zhao%2520and%2520Chengyue%2520Zhao%2520and%2520Jiaqi%2520Zhao%2520and%2520Jianchao%2520Zhu%26entry.1292438233%3D%2520%2520We%2520explore%2520how%2520scalable%2520robot%2520data%2520can%2520address%2520real-world%2520challenges%2520for%250Ageneralized%2520robotic%2520manipulation.%2520Introducing%2520AgiBot%2520World%252C%2520a%2520large-scale%250Aplatform%2520comprising%2520over%25201%2520million%2520trajectories%2520across%2520217%2520tasks%2520in%2520five%250Adeployment%2520scenarios%252C%2520we%2520achieve%2520an%2520order-of-magnitude%2520increase%2520in%2520data%2520scale%250Acompared%2520to%2520existing%2520datasets.%2520Accelerated%2520by%2520a%2520standardized%2520collection%250Apipeline%2520with%2520human-in-the-loop%2520verification%252C%2520AgiBot%2520World%2520guarantees%250Ahigh-quality%2520and%2520diverse%2520data%2520distribution.%2520It%2520is%2520extensible%2520from%2520grippers%2520to%250Adexterous%2520hands%2520and%2520visuo-tactile%2520sensors%2520for%2520fine-grained%2520skill%2520acquisition.%250ABuilding%2520on%2520top%2520of%2520data%252C%2520we%2520introduce%2520Genie%2520Operator-1%2520%2528GO-1%2529%252C%2520a%2520novel%250Ageneralist%2520policy%2520that%2520leverages%2520latent%2520action%2520representations%2520to%2520maximize%2520data%250Autilization%252C%2520demonstrating%2520predictable%2520performance%2520scaling%2520with%2520increased%2520data%250Avolume.%2520Policies%2520pre-trained%2520on%2520our%2520dataset%2520achieve%2520an%2520average%2520performance%250Aimprovement%2520of%252030%2525%2520over%2520those%2520trained%2520on%2520Open%2520X-Embodiment%252C%2520both%2520in%2520in-domain%250Aand%2520out-of-distribution%2520scenarios.%2520GO-1%2520exhibits%2520exceptional%2520capability%2520in%250Areal-world%2520dexterous%2520and%2520long-horizon%2520tasks%252C%2520achieving%2520over%252060%2525%2520success%2520rate%2520on%250Acomplex%2520tasks%2520and%2520outperforming%2520prior%2520RDT%2520approach%2520by%252032%2525.%2520By%2520open-sourcing%2520the%250Adataset%252C%2520tools%252C%2520and%2520models%252C%2520we%2520aim%2520to%2520democratize%2520access%2520to%2520large-scale%252C%250Ahigh-quality%2520robot%2520data%252C%2520advancing%2520the%2520pursuit%2520of%2520scalable%2520and%2520general-purpose%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06669v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgiBot%20World%20Colosseo%3A%20A%20Large-scale%20Manipulation%20Platform%20for%20Scalable%0A%20%20and%20Intelligent%20Embodied%20Systems&entry.906535625=%20AgiBot-World-Contributors%20and%20Qingwen%20Bu%20and%20Jisong%20Cai%20and%20Li%20Chen%20and%20Xiuqi%20Cui%20and%20Yan%20Ding%20and%20Siyuan%20Feng%20and%20Shenyuan%20Gao%20and%20Xindong%20He%20and%20Xuan%20Hu%20and%20Xu%20Huang%20and%20Shu%20Jiang%20and%20Yuxin%20Jiang%20and%20Cheng%20Jing%20and%20Hongyang%20Li%20and%20Jialu%20Li%20and%20Chiming%20Liu%20and%20Yi%20Liu%20and%20Yuxiang%20Lu%20and%20Jianlan%20Luo%20and%20Ping%20Luo%20and%20Yao%20Mu%20and%20Yuehan%20Niu%20and%20Yixuan%20Pan%20and%20Jiangmiao%20Pang%20and%20Yu%20Qiao%20and%20Guanghui%20Ren%20and%20Cheng%20Ruan%20and%20Jiaqi%20Shan%20and%20Yongjian%20Shen%20and%20Chengshi%20Shi%20and%20Mingkang%20Shi%20and%20Modi%20Shi%20and%20Chonghao%20Sima%20and%20Jianheng%20Song%20and%20Huijie%20Wang%20and%20Wenhao%20Wang%20and%20Dafeng%20Wei%20and%20Chengen%20Xie%20and%20Guo%20Xu%20and%20Junchi%20Yan%20and%20Cunbiao%20Yang%20and%20Lei%20Yang%20and%20Shukai%20Yang%20and%20Maoqing%20Yao%20and%20Jia%20Zeng%20and%20Chi%20Zhang%20and%20Qinglin%20Zhang%20and%20Bin%20Zhao%20and%20Chengyue%20Zhao%20and%20Jiaqi%20Zhao%20and%20Jianchao%20Zhu&entry.1292438233=%20%20We%20explore%20how%20scalable%20robot%20data%20can%20address%20real-world%20challenges%20for%0Ageneralized%20robotic%20manipulation.%20Introducing%20AgiBot%20World%2C%20a%20large-scale%0Aplatform%20comprising%20over%201%20million%20trajectories%20across%20217%20tasks%20in%20five%0Adeployment%20scenarios%2C%20we%20achieve%20an%20order-of-magnitude%20increase%20in%20data%20scale%0Acompared%20to%20existing%20datasets.%20Accelerated%20by%20a%20standardized%20collection%0Apipeline%20with%20human-in-the-loop%20verification%2C%20AgiBot%20World%20guarantees%0Ahigh-quality%20and%20diverse%20data%20distribution.%20It%20is%20extensible%20from%20grippers%20to%0Adexterous%20hands%20and%20visuo-tactile%20sensors%20for%20fine-grained%20skill%20acquisition.%0ABuilding%20on%20top%20of%20data%2C%20we%20introduce%20Genie%20Operator-1%20%28GO-1%29%2C%20a%20novel%0Ageneralist%20policy%20that%20leverages%20latent%20action%20representations%20to%20maximize%20data%0Autilization%2C%20demonstrating%20predictable%20performance%20scaling%20with%20increased%20data%0Avolume.%20Policies%20pre-trained%20on%20our%20dataset%20achieve%20an%20average%20performance%0Aimprovement%20of%2030%25%20over%20those%20trained%20on%20Open%20X-Embodiment%2C%20both%20in%20in-domain%0Aand%20out-of-distribution%20scenarios.%20GO-1%20exhibits%20exceptional%20capability%20in%0Areal-world%20dexterous%20and%20long-horizon%20tasks%2C%20achieving%20over%2060%25%20success%20rate%20on%0Acomplex%20tasks%20and%20outperforming%20prior%20RDT%20approach%20by%2032%25.%20By%20open-sourcing%20the%0Adataset%2C%20tools%2C%20and%20models%2C%20we%20aim%20to%20democratize%20access%20to%20large-scale%2C%0Ahigh-quality%20robot%20data%2C%20advancing%20the%20pursuit%20of%20scalable%20and%20general-purpose%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06669v3&entry.124074799=Read"},
{"title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy\n  Protection", "author": "Liqin Wang and Qianyue Hu and Wei Lu and Xiangyang Luo", "abstract": "  The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun.\n", "link": "http://arxiv.org/abs/2504.21646v1", "date": "2025-04-30", "relevancy": 2.3696, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6295}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5716}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Adversarial%20Identity%20Manipulation%20for%20Facial%20Privacy%0A%20%20Protection&body=Title%3A%20Diffusion-based%20Adversarial%20Identity%20Manipulation%20for%20Facial%20Privacy%0A%20%20Protection%0AAuthor%3A%20Liqin%20Wang%20and%20Qianyue%20Hu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%0AAbstract%3A%20%20%20The%20success%20of%20face%20recognition%20%28FR%29%20systems%20has%20led%20to%20serious%20privacy%0Aconcerns%20due%20to%20potential%20unauthorized%20surveillance%20and%20user%20tracking%20on%20social%0Anetworks.%20Existing%20methods%20for%20enhancing%20privacy%20fail%20to%20generate%20natural%20face%0Aimages%20that%20can%20protect%20facial%20privacy.%20In%20this%20paper%2C%20we%20propose%0Adiffusion-based%20adversarial%20identity%20manipulation%20%28DiffAIM%29%20to%20generate%20natural%0Aand%20highly%20transferable%20adversarial%20faces%20against%20malicious%20FR%20systems.%20To%20be%0Aspecific%2C%20we%20manipulate%20facial%20identity%20within%20the%20low-dimensional%20latent%20space%0Aof%20a%20diffusion%20model.%20This%20involves%20iteratively%20injecting%20gradient-based%0Aadversarial%20identity%20guidance%20during%20the%20reverse%20diffusion%20process%2C%0Aprogressively%20steering%20the%20generation%20toward%20the%20desired%20adversarial%20faces.%20The%0Aguidance%20is%20optimized%20for%20identity%20convergence%20towards%20a%20target%20while%20promoting%0Asemantic%20divergence%20from%20the%20source%2C%20facilitating%20effective%20impersonation%20while%0Amaintaining%20visual%20naturalness.%20We%20further%20incorporate%20structure-preserving%0Aregularization%20to%20preserve%20facial%20structure%20consistency%20during%20manipulation.%0AExtensive%20experiments%20on%20both%20face%20verification%20and%20identification%20tasks%0Ademonstrate%20that%20compared%20with%20the%20state-of-the-art%2C%20DiffAIM%20achieves%20stronger%0Ablack-box%20attack%20transferability%20while%20maintaining%20superior%20visual%20quality.%20We%0Aalso%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20for%20commercial%20FR%0AAPIs%2C%20including%20Face%2B%2B%20and%20Aliyun.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Adversarial%2520Identity%2520Manipulation%2520for%2520Facial%2520Privacy%250A%2520%2520Protection%26entry.906535625%3DLiqin%2520Wang%2520and%2520Qianyue%2520Hu%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520face%2520recognition%2520%2528FR%2529%2520systems%2520has%2520led%2520to%2520serious%2520privacy%250Aconcerns%2520due%2520to%2520potential%2520unauthorized%2520surveillance%2520and%2520user%2520tracking%2520on%2520social%250Anetworks.%2520Existing%2520methods%2520for%2520enhancing%2520privacy%2520fail%2520to%2520generate%2520natural%2520face%250Aimages%2520that%2520can%2520protect%2520facial%2520privacy.%2520In%2520this%2520paper%252C%2520we%2520propose%250Adiffusion-based%2520adversarial%2520identity%2520manipulation%2520%2528DiffAIM%2529%2520to%2520generate%2520natural%250Aand%2520highly%2520transferable%2520adversarial%2520faces%2520against%2520malicious%2520FR%2520systems.%2520To%2520be%250Aspecific%252C%2520we%2520manipulate%2520facial%2520identity%2520within%2520the%2520low-dimensional%2520latent%2520space%250Aof%2520a%2520diffusion%2520model.%2520This%2520involves%2520iteratively%2520injecting%2520gradient-based%250Aadversarial%2520identity%2520guidance%2520during%2520the%2520reverse%2520diffusion%2520process%252C%250Aprogressively%2520steering%2520the%2520generation%2520toward%2520the%2520desired%2520adversarial%2520faces.%2520The%250Aguidance%2520is%2520optimized%2520for%2520identity%2520convergence%2520towards%2520a%2520target%2520while%2520promoting%250Asemantic%2520divergence%2520from%2520the%2520source%252C%2520facilitating%2520effective%2520impersonation%2520while%250Amaintaining%2520visual%2520naturalness.%2520We%2520further%2520incorporate%2520structure-preserving%250Aregularization%2520to%2520preserve%2520facial%2520structure%2520consistency%2520during%2520manipulation.%250AExtensive%2520experiments%2520on%2520both%2520face%2520verification%2520and%2520identification%2520tasks%250Ademonstrate%2520that%2520compared%2520with%2520the%2520state-of-the-art%252C%2520DiffAIM%2520achieves%2520stronger%250Ablack-box%2520attack%2520transferability%2520while%2520maintaining%2520superior%2520visual%2520quality.%2520We%250Aalso%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520for%2520commercial%2520FR%250AAPIs%252C%2520including%2520Face%252B%252B%2520and%2520Aliyun.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Adversarial%20Identity%20Manipulation%20for%20Facial%20Privacy%0A%20%20Protection&entry.906535625=Liqin%20Wang%20and%20Qianyue%20Hu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo&entry.1292438233=%20%20The%20success%20of%20face%20recognition%20%28FR%29%20systems%20has%20led%20to%20serious%20privacy%0Aconcerns%20due%20to%20potential%20unauthorized%20surveillance%20and%20user%20tracking%20on%20social%0Anetworks.%20Existing%20methods%20for%20enhancing%20privacy%20fail%20to%20generate%20natural%20face%0Aimages%20that%20can%20protect%20facial%20privacy.%20In%20this%20paper%2C%20we%20propose%0Adiffusion-based%20adversarial%20identity%20manipulation%20%28DiffAIM%29%20to%20generate%20natural%0Aand%20highly%20transferable%20adversarial%20faces%20against%20malicious%20FR%20systems.%20To%20be%0Aspecific%2C%20we%20manipulate%20facial%20identity%20within%20the%20low-dimensional%20latent%20space%0Aof%20a%20diffusion%20model.%20This%20involves%20iteratively%20injecting%20gradient-based%0Aadversarial%20identity%20guidance%20during%20the%20reverse%20diffusion%20process%2C%0Aprogressively%20steering%20the%20generation%20toward%20the%20desired%20adversarial%20faces.%20The%0Aguidance%20is%20optimized%20for%20identity%20convergence%20towards%20a%20target%20while%20promoting%0Asemantic%20divergence%20from%20the%20source%2C%20facilitating%20effective%20impersonation%20while%0Amaintaining%20visual%20naturalness.%20We%20further%20incorporate%20structure-preserving%0Aregularization%20to%20preserve%20facial%20structure%20consistency%20during%20manipulation.%0AExtensive%20experiments%20on%20both%20face%20verification%20and%20identification%20tasks%0Ademonstrate%20that%20compared%20with%20the%20state-of-the-art%2C%20DiffAIM%20achieves%20stronger%0Ablack-box%20attack%20transferability%20while%20maintaining%20superior%20visual%20quality.%20We%0Aalso%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20for%20commercial%20FR%0AAPIs%2C%20including%20Face%2B%2B%20and%20Aliyun.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21646v1&entry.124074799=Read"},
{"title": "ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force\n  Control in VR Hand Manipulation", "author": "DongHeun Han and Byungmin Kim and RoUn Lee and KyeongMin Kim and Hyoseok Hwang and HyeongYeop Kang", "abstract": "  Realistic Hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. Demo videos are available as supplementary material and the code is\nprovided at https://han-dongheun.github.io/ForceGrip.\n", "link": "http://arxiv.org/abs/2503.08061v3", "date": "2025-04-30", "relevancy": 2.3625, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6244}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ForceGrip%3A%20Reference-Free%20Curriculum%20Learning%20for%20Realistic%20Grip%20Force%0A%20%20Control%20in%20VR%20Hand%20Manipulation&body=Title%3A%20ForceGrip%3A%20Reference-Free%20Curriculum%20Learning%20for%20Realistic%20Grip%20Force%0A%20%20Control%20in%20VR%20Hand%20Manipulation%0AAuthor%3A%20DongHeun%20Han%20and%20Byungmin%20Kim%20and%20RoUn%20Lee%20and%20KyeongMin%20Kim%20and%20Hyoseok%20Hwang%20and%20HyeongYeop%20Kang%0AAbstract%3A%20%20%20Realistic%20Hand%20manipulation%20is%20a%20key%20component%20of%20immersive%20virtual%20reality%0A%28VR%29%2C%20yet%20existing%20methods%20often%20rely%20on%20kinematic%20approach%20or%20motion-capture%0Adatasets%20that%20omit%20crucial%20physical%20attributes%20such%20as%20contact%20forces%20and%0Afinger%20torques.%20Consequently%2C%20these%20approaches%20prioritize%20tight%2C%0Aone-size-fits-all%20grips%20rather%20than%20reflecting%20users%27%20intended%20force%20levels.%20We%0Apresent%20ForceGrip%2C%20a%20deep%20learning%20agent%20that%20synthesizes%20realistic%20hand%0Amanipulation%20motions%2C%20faithfully%20reflecting%20the%20user%27s%20grip%20force%20intention.%0AInstead%20of%20mimicking%20predefined%20motion%20datasets%2C%20ForceGrip%20uses%20generated%0Atraining%20scenarios-randomizing%20object%20shapes%2C%20wrist%20movements%2C%20and%20trigger%0Ainput%20flows-to%20challenge%20the%20agent%20with%20a%20broad%20spectrum%20of%20physical%0Ainteractions.%20To%20effectively%20learn%20from%20these%20complex%20tasks%2C%20we%20employ%20a%0Athree-phase%20curriculum%20learning%20framework%20comprising%20Finger%20Positioning%2C%0AIntention%20Adaptation%2C%20and%20Dynamic%20Stabilization.%20This%20progressive%20strategy%0Aensures%20stable%20hand-object%20contact%2C%20adaptive%20force%20control%20based%20on%20user%0Ainputs%2C%20and%20robust%20handling%20under%20dynamic%20conditions.%20Additionally%2C%20a%20proximity%0Areward%20function%20enhances%20natural%20finger%20motions%20and%20accelerates%20training%0Aconvergence.%20Quantitative%20and%20qualitative%20evaluations%20reveal%20ForceGrip%27s%0Asuperior%20force%20controllability%20and%20plausibility%20compared%20to%20state-of-the-art%0Amethods.%20Demo%20videos%20are%20available%20as%20supplementary%20material%20and%20the%20code%20is%0Aprovided%20at%20https%3A//han-dongheun.github.io/ForceGrip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForceGrip%253A%2520Reference-Free%2520Curriculum%2520Learning%2520for%2520Realistic%2520Grip%2520Force%250A%2520%2520Control%2520in%2520VR%2520Hand%2520Manipulation%26entry.906535625%3DDongHeun%2520Han%2520and%2520Byungmin%2520Kim%2520and%2520RoUn%2520Lee%2520and%2520KyeongMin%2520Kim%2520and%2520Hyoseok%2520Hwang%2520and%2520HyeongYeop%2520Kang%26entry.1292438233%3D%2520%2520Realistic%2520Hand%2520manipulation%2520is%2520a%2520key%2520component%2520of%2520immersive%2520virtual%2520reality%250A%2528VR%2529%252C%2520yet%2520existing%2520methods%2520often%2520rely%2520on%2520kinematic%2520approach%2520or%2520motion-capture%250Adatasets%2520that%2520omit%2520crucial%2520physical%2520attributes%2520such%2520as%2520contact%2520forces%2520and%250Afinger%2520torques.%2520Consequently%252C%2520these%2520approaches%2520prioritize%2520tight%252C%250Aone-size-fits-all%2520grips%2520rather%2520than%2520reflecting%2520users%2527%2520intended%2520force%2520levels.%2520We%250Apresent%2520ForceGrip%252C%2520a%2520deep%2520learning%2520agent%2520that%2520synthesizes%2520realistic%2520hand%250Amanipulation%2520motions%252C%2520faithfully%2520reflecting%2520the%2520user%2527s%2520grip%2520force%2520intention.%250AInstead%2520of%2520mimicking%2520predefined%2520motion%2520datasets%252C%2520ForceGrip%2520uses%2520generated%250Atraining%2520scenarios-randomizing%2520object%2520shapes%252C%2520wrist%2520movements%252C%2520and%2520trigger%250Ainput%2520flows-to%2520challenge%2520the%2520agent%2520with%2520a%2520broad%2520spectrum%2520of%2520physical%250Ainteractions.%2520To%2520effectively%2520learn%2520from%2520these%2520complex%2520tasks%252C%2520we%2520employ%2520a%250Athree-phase%2520curriculum%2520learning%2520framework%2520comprising%2520Finger%2520Positioning%252C%250AIntention%2520Adaptation%252C%2520and%2520Dynamic%2520Stabilization.%2520This%2520progressive%2520strategy%250Aensures%2520stable%2520hand-object%2520contact%252C%2520adaptive%2520force%2520control%2520based%2520on%2520user%250Ainputs%252C%2520and%2520robust%2520handling%2520under%2520dynamic%2520conditions.%2520Additionally%252C%2520a%2520proximity%250Areward%2520function%2520enhances%2520natural%2520finger%2520motions%2520and%2520accelerates%2520training%250Aconvergence.%2520Quantitative%2520and%2520qualitative%2520evaluations%2520reveal%2520ForceGrip%2527s%250Asuperior%2520force%2520controllability%2520and%2520plausibility%2520compared%2520to%2520state-of-the-art%250Amethods.%2520Demo%2520videos%2520are%2520available%2520as%2520supplementary%2520material%2520and%2520the%2520code%2520is%250Aprovided%2520at%2520https%253A//han-dongheun.github.io/ForceGrip.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ForceGrip%3A%20Reference-Free%20Curriculum%20Learning%20for%20Realistic%20Grip%20Force%0A%20%20Control%20in%20VR%20Hand%20Manipulation&entry.906535625=DongHeun%20Han%20and%20Byungmin%20Kim%20and%20RoUn%20Lee%20and%20KyeongMin%20Kim%20and%20Hyoseok%20Hwang%20and%20HyeongYeop%20Kang&entry.1292438233=%20%20Realistic%20Hand%20manipulation%20is%20a%20key%20component%20of%20immersive%20virtual%20reality%0A%28VR%29%2C%20yet%20existing%20methods%20often%20rely%20on%20kinematic%20approach%20or%20motion-capture%0Adatasets%20that%20omit%20crucial%20physical%20attributes%20such%20as%20contact%20forces%20and%0Afinger%20torques.%20Consequently%2C%20these%20approaches%20prioritize%20tight%2C%0Aone-size-fits-all%20grips%20rather%20than%20reflecting%20users%27%20intended%20force%20levels.%20We%0Apresent%20ForceGrip%2C%20a%20deep%20learning%20agent%20that%20synthesizes%20realistic%20hand%0Amanipulation%20motions%2C%20faithfully%20reflecting%20the%20user%27s%20grip%20force%20intention.%0AInstead%20of%20mimicking%20predefined%20motion%20datasets%2C%20ForceGrip%20uses%20generated%0Atraining%20scenarios-randomizing%20object%20shapes%2C%20wrist%20movements%2C%20and%20trigger%0Ainput%20flows-to%20challenge%20the%20agent%20with%20a%20broad%20spectrum%20of%20physical%0Ainteractions.%20To%20effectively%20learn%20from%20these%20complex%20tasks%2C%20we%20employ%20a%0Athree-phase%20curriculum%20learning%20framework%20comprising%20Finger%20Positioning%2C%0AIntention%20Adaptation%2C%20and%20Dynamic%20Stabilization.%20This%20progressive%20strategy%0Aensures%20stable%20hand-object%20contact%2C%20adaptive%20force%20control%20based%20on%20user%0Ainputs%2C%20and%20robust%20handling%20under%20dynamic%20conditions.%20Additionally%2C%20a%20proximity%0Areward%20function%20enhances%20natural%20finger%20motions%20and%20accelerates%20training%0Aconvergence.%20Quantitative%20and%20qualitative%20evaluations%20reveal%20ForceGrip%27s%0Asuperior%20force%20controllability%20and%20plausibility%20compared%20to%20state-of-the-art%0Amethods.%20Demo%20videos%20are%20available%20as%20supplementary%20material%20and%20the%20code%20is%0Aprovided%20at%20https%3A//han-dongheun.github.io/ForceGrip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08061v3&entry.124074799=Read"},
{"title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos", "author": "Prithviraj Banerjee and Sindi Shkodrani and Pierre Moulon and Shreyas Hampali and Shangchen Han and Fan Zhang and Linguang Zhang and Jade Fountain and Edward Miller and Selen Basol and Richard Newcombe and Robert Wang and Jakob Julian Engel and Tomas Hodan", "abstract": "  We introduce HOT3D, a publicly available dataset for egocentric hand and\nobject tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of\nrecordings that feature 19 subjects interacting with 33 diverse rigid objects.\nIn addition to simple pick-up, observe, and put-down actions, the subjects\nperform actions typical for a kitchen, office, and living room environment. The\nrecordings include multiple synchronized data streams containing egocentric\nmulti-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D\nposes of cameras, hands, and objects. The dataset is recorded with two headsets\nfrom Meta: Project Aria, which is a research prototype of AI glasses, and Quest\n3, a virtual-reality headset that has shipped millions of units. Ground-truth\nposes were obtained by a motion-capture system using small optical markers\nattached to hands and objects. Hand annotations are provided in the UmeTrack\nand MANO formats, and objects are represented by 3D meshes with PBR materials\nobtained by an in-house scanner. In our experiments, we demonstrate the\neffectiveness of multi-view egocentric data for three popular tasks: 3D hand\ntracking, model-based 6DoF object pose estimation, and 3D lifting of unknown\nin-hand objects. The evaluated multi-view methods, whose benchmarking is\nuniquely enabled by HOT3D, significantly outperform their single-view\ncounterparts.\n", "link": "http://arxiv.org/abs/2411.19167v2", "date": "2025-04-30", "relevancy": 2.3422, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5885}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5885}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOT3D%3A%20Hand%20and%20Object%20Tracking%20in%203D%20from%20Egocentric%20Multi-View%20Videos&body=Title%3A%20HOT3D%3A%20Hand%20and%20Object%20Tracking%20in%203D%20from%20Egocentric%20Multi-View%20Videos%0AAuthor%3A%20Prithviraj%20Banerjee%20and%20Sindi%20Shkodrani%20and%20Pierre%20Moulon%20and%20Shreyas%20Hampali%20and%20Shangchen%20Han%20and%20Fan%20Zhang%20and%20Linguang%20Zhang%20and%20Jade%20Fountain%20and%20Edward%20Miller%20and%20Selen%20Basol%20and%20Richard%20Newcombe%20and%20Robert%20Wang%20and%20Jakob%20Julian%20Engel%20and%20Tomas%20Hodan%0AAbstract%3A%20%20%20We%20introduce%20HOT3D%2C%20a%20publicly%20available%20dataset%20for%20egocentric%20hand%20and%0Aobject%20tracking%20in%203D.%20The%20dataset%20offers%20over%20833%20minutes%20%283.7M%2B%20images%29%20of%0Arecordings%20that%20feature%2019%20subjects%20interacting%20with%2033%20diverse%20rigid%20objects.%0AIn%20addition%20to%20simple%20pick-up%2C%20observe%2C%20and%20put-down%20actions%2C%20the%20subjects%0Aperform%20actions%20typical%20for%20a%20kitchen%2C%20office%2C%20and%20living%20room%20environment.%20The%0Arecordings%20include%20multiple%20synchronized%20data%20streams%20containing%20egocentric%0Amulti-view%20RGB/monochrome%20images%2C%20eye%20gaze%20signal%2C%20scene%20point%20clouds%2C%20and%203D%0Aposes%20of%20cameras%2C%20hands%2C%20and%20objects.%20The%20dataset%20is%20recorded%20with%20two%20headsets%0Afrom%20Meta%3A%20Project%20Aria%2C%20which%20is%20a%20research%20prototype%20of%20AI%20glasses%2C%20and%20Quest%0A3%2C%20a%20virtual-reality%20headset%20that%20has%20shipped%20millions%20of%20units.%20Ground-truth%0Aposes%20were%20obtained%20by%20a%20motion-capture%20system%20using%20small%20optical%20markers%0Aattached%20to%20hands%20and%20objects.%20Hand%20annotations%20are%20provided%20in%20the%20UmeTrack%0Aand%20MANO%20formats%2C%20and%20objects%20are%20represented%20by%203D%20meshes%20with%20PBR%20materials%0Aobtained%20by%20an%20in-house%20scanner.%20In%20our%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20multi-view%20egocentric%20data%20for%20three%20popular%20tasks%3A%203D%20hand%0Atracking%2C%20model-based%206DoF%20object%20pose%20estimation%2C%20and%203D%20lifting%20of%20unknown%0Ain-hand%20objects.%20The%20evaluated%20multi-view%20methods%2C%20whose%20benchmarking%20is%0Auniquely%20enabled%20by%20HOT3D%2C%20significantly%20outperform%20their%20single-view%0Acounterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19167v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOT3D%253A%2520Hand%2520and%2520Object%2520Tracking%2520in%25203D%2520from%2520Egocentric%2520Multi-View%2520Videos%26entry.906535625%3DPrithviraj%2520Banerjee%2520and%2520Sindi%2520Shkodrani%2520and%2520Pierre%2520Moulon%2520and%2520Shreyas%2520Hampali%2520and%2520Shangchen%2520Han%2520and%2520Fan%2520Zhang%2520and%2520Linguang%2520Zhang%2520and%2520Jade%2520Fountain%2520and%2520Edward%2520Miller%2520and%2520Selen%2520Basol%2520and%2520Richard%2520Newcombe%2520and%2520Robert%2520Wang%2520and%2520Jakob%2520Julian%2520Engel%2520and%2520Tomas%2520Hodan%26entry.1292438233%3D%2520%2520We%2520introduce%2520HOT3D%252C%2520a%2520publicly%2520available%2520dataset%2520for%2520egocentric%2520hand%2520and%250Aobject%2520tracking%2520in%25203D.%2520The%2520dataset%2520offers%2520over%2520833%2520minutes%2520%25283.7M%252B%2520images%2529%2520of%250Arecordings%2520that%2520feature%252019%2520subjects%2520interacting%2520with%252033%2520diverse%2520rigid%2520objects.%250AIn%2520addition%2520to%2520simple%2520pick-up%252C%2520observe%252C%2520and%2520put-down%2520actions%252C%2520the%2520subjects%250Aperform%2520actions%2520typical%2520for%2520a%2520kitchen%252C%2520office%252C%2520and%2520living%2520room%2520environment.%2520The%250Arecordings%2520include%2520multiple%2520synchronized%2520data%2520streams%2520containing%2520egocentric%250Amulti-view%2520RGB/monochrome%2520images%252C%2520eye%2520gaze%2520signal%252C%2520scene%2520point%2520clouds%252C%2520and%25203D%250Aposes%2520of%2520cameras%252C%2520hands%252C%2520and%2520objects.%2520The%2520dataset%2520is%2520recorded%2520with%2520two%2520headsets%250Afrom%2520Meta%253A%2520Project%2520Aria%252C%2520which%2520is%2520a%2520research%2520prototype%2520of%2520AI%2520glasses%252C%2520and%2520Quest%250A3%252C%2520a%2520virtual-reality%2520headset%2520that%2520has%2520shipped%2520millions%2520of%2520units.%2520Ground-truth%250Aposes%2520were%2520obtained%2520by%2520a%2520motion-capture%2520system%2520using%2520small%2520optical%2520markers%250Aattached%2520to%2520hands%2520and%2520objects.%2520Hand%2520annotations%2520are%2520provided%2520in%2520the%2520UmeTrack%250Aand%2520MANO%2520formats%252C%2520and%2520objects%2520are%2520represented%2520by%25203D%2520meshes%2520with%2520PBR%2520materials%250Aobtained%2520by%2520an%2520in-house%2520scanner.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520multi-view%2520egocentric%2520data%2520for%2520three%2520popular%2520tasks%253A%25203D%2520hand%250Atracking%252C%2520model-based%25206DoF%2520object%2520pose%2520estimation%252C%2520and%25203D%2520lifting%2520of%2520unknown%250Ain-hand%2520objects.%2520The%2520evaluated%2520multi-view%2520methods%252C%2520whose%2520benchmarking%2520is%250Auniquely%2520enabled%2520by%2520HOT3D%252C%2520significantly%2520outperform%2520their%2520single-view%250Acounterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19167v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOT3D%3A%20Hand%20and%20Object%20Tracking%20in%203D%20from%20Egocentric%20Multi-View%20Videos&entry.906535625=Prithviraj%20Banerjee%20and%20Sindi%20Shkodrani%20and%20Pierre%20Moulon%20and%20Shreyas%20Hampali%20and%20Shangchen%20Han%20and%20Fan%20Zhang%20and%20Linguang%20Zhang%20and%20Jade%20Fountain%20and%20Edward%20Miller%20and%20Selen%20Basol%20and%20Richard%20Newcombe%20and%20Robert%20Wang%20and%20Jakob%20Julian%20Engel%20and%20Tomas%20Hodan&entry.1292438233=%20%20We%20introduce%20HOT3D%2C%20a%20publicly%20available%20dataset%20for%20egocentric%20hand%20and%0Aobject%20tracking%20in%203D.%20The%20dataset%20offers%20over%20833%20minutes%20%283.7M%2B%20images%29%20of%0Arecordings%20that%20feature%2019%20subjects%20interacting%20with%2033%20diverse%20rigid%20objects.%0AIn%20addition%20to%20simple%20pick-up%2C%20observe%2C%20and%20put-down%20actions%2C%20the%20subjects%0Aperform%20actions%20typical%20for%20a%20kitchen%2C%20office%2C%20and%20living%20room%20environment.%20The%0Arecordings%20include%20multiple%20synchronized%20data%20streams%20containing%20egocentric%0Amulti-view%20RGB/monochrome%20images%2C%20eye%20gaze%20signal%2C%20scene%20point%20clouds%2C%20and%203D%0Aposes%20of%20cameras%2C%20hands%2C%20and%20objects.%20The%20dataset%20is%20recorded%20with%20two%20headsets%0Afrom%20Meta%3A%20Project%20Aria%2C%20which%20is%20a%20research%20prototype%20of%20AI%20glasses%2C%20and%20Quest%0A3%2C%20a%20virtual-reality%20headset%20that%20has%20shipped%20millions%20of%20units.%20Ground-truth%0Aposes%20were%20obtained%20by%20a%20motion-capture%20system%20using%20small%20optical%20markers%0Aattached%20to%20hands%20and%20objects.%20Hand%20annotations%20are%20provided%20in%20the%20UmeTrack%0Aand%20MANO%20formats%2C%20and%20objects%20are%20represented%20by%203D%20meshes%20with%20PBR%20materials%0Aobtained%20by%20an%20in-house%20scanner.%20In%20our%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20multi-view%20egocentric%20data%20for%20three%20popular%20tasks%3A%203D%20hand%0Atracking%2C%20model-based%206DoF%20object%20pose%20estimation%2C%20and%203D%20lifting%20of%20unknown%0Ain-hand%20objects.%20The%20evaluated%20multi-view%20methods%2C%20whose%20benchmarking%20is%0Auniquely%20enabled%20by%20HOT3D%2C%20significantly%20outperform%20their%20single-view%0Acounterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19167v2&entry.124074799=Read"},
{"title": "An Underwater, Fault-Tolerant, Laser-Aided Robotic Multi-Modal Dense\n  SLAM System for Continuous Underwater In-Situ Observation", "author": "Yaming Ou and Junfeng Fan and Chao Zhou and Pengju Zhang and Zongyuan Shen and Yichen Fu and Xiaoyan Liu and Zengguang Hou", "abstract": "  Existing underwater SLAM systems are difficult to work effectively in\ntexture-sparse and geometrically degraded underwater environments, resulting in\nintermittent tracking and sparse mapping. Therefore, we present Water-DSLAM, a\nnovel laser-aided multi-sensor fusion system that can achieve uninterrupted,\nfault-tolerant dense SLAM capable of continuous in-situ observation in diverse\ncomplex underwater scenarios through three key innovations: Firstly, we develop\nWater-Scanner, a multi-sensor fusion robotic platform featuring a self-designed\nUnderwater Binocular Structured Light (UBSL) module that enables high-precision\n3D perception. Secondly, we propose a fault-tolerant triple-subsystem\narchitecture combining: 1) DP-INS (DVL- and Pressure-aided Inertial Navigation\nSystem): fusing inertial measurement unit, doppler velocity log, and pressure\nsensor based Error-State Kalman Filter (ESKF) to provide high-frequency\nabsolute odometry 2) Water-UBSL: a novel Iterated ESKF (IESKF)-based tight\ncoupling between UBSL and DP-INS to mitigate UBSL's degeneration issues 3)\nWater-Stereo: a fusion of DP-INS and stereo camera for accurate initialization\nand tracking. Thirdly, we introduce a multi-modal factor graph back-end that\ndynamically fuses heterogeneous sensor data. The proposed multi-sensor factor\ngraph maintenance strategy efficiently addresses issues caused by asynchronous\nsensor frequencies and partial data loss. Experimental results demonstrate\nWater-DSLAM achieves superior robustness (0.039 m trajectory RMSE and 100\\%\ncontinuity ratio during partial sensor dropout) and dense mapping (6922.4\npoints/m^3 in 750 m^3 water volume, approximately 10 times denser than existing\nmethods) in various challenging environments, including pools, dark underwater\nscenes, 16-meter-deep sinkholes, and field rivers. Our project is available at\nhttps://water-scanner.github.io/.\n", "link": "http://arxiv.org/abs/2504.21826v1", "date": "2025-04-30", "relevancy": 2.3386, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5997}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Underwater%2C%20Fault-Tolerant%2C%20Laser-Aided%20Robotic%20Multi-Modal%20Dense%0A%20%20SLAM%20System%20for%20Continuous%20Underwater%20In-Situ%20Observation&body=Title%3A%20An%20Underwater%2C%20Fault-Tolerant%2C%20Laser-Aided%20Robotic%20Multi-Modal%20Dense%0A%20%20SLAM%20System%20for%20Continuous%20Underwater%20In-Situ%20Observation%0AAuthor%3A%20Yaming%20Ou%20and%20Junfeng%20Fan%20and%20Chao%20Zhou%20and%20Pengju%20Zhang%20and%20Zongyuan%20Shen%20and%20Yichen%20Fu%20and%20Xiaoyan%20Liu%20and%20Zengguang%20Hou%0AAbstract%3A%20%20%20Existing%20underwater%20SLAM%20systems%20are%20difficult%20to%20work%20effectively%20in%0Atexture-sparse%20and%20geometrically%20degraded%20underwater%20environments%2C%20resulting%20in%0Aintermittent%20tracking%20and%20sparse%20mapping.%20Therefore%2C%20we%20present%20Water-DSLAM%2C%20a%0Anovel%20laser-aided%20multi-sensor%20fusion%20system%20that%20can%20achieve%20uninterrupted%2C%0Afault-tolerant%20dense%20SLAM%20capable%20of%20continuous%20in-situ%20observation%20in%20diverse%0Acomplex%20underwater%20scenarios%20through%20three%20key%20innovations%3A%20Firstly%2C%20we%20develop%0AWater-Scanner%2C%20a%20multi-sensor%20fusion%20robotic%20platform%20featuring%20a%20self-designed%0AUnderwater%20Binocular%20Structured%20Light%20%28UBSL%29%20module%20that%20enables%20high-precision%0A3D%20perception.%20Secondly%2C%20we%20propose%20a%20fault-tolerant%20triple-subsystem%0Aarchitecture%20combining%3A%201%29%20DP-INS%20%28DVL-%20and%20Pressure-aided%20Inertial%20Navigation%0ASystem%29%3A%20fusing%20inertial%20measurement%20unit%2C%20doppler%20velocity%20log%2C%20and%20pressure%0Asensor%20based%20Error-State%20Kalman%20Filter%20%28ESKF%29%20to%20provide%20high-frequency%0Aabsolute%20odometry%202%29%20Water-UBSL%3A%20a%20novel%20Iterated%20ESKF%20%28IESKF%29-based%20tight%0Acoupling%20between%20UBSL%20and%20DP-INS%20to%20mitigate%20UBSL%27s%20degeneration%20issues%203%29%0AWater-Stereo%3A%20a%20fusion%20of%20DP-INS%20and%20stereo%20camera%20for%20accurate%20initialization%0Aand%20tracking.%20Thirdly%2C%20we%20introduce%20a%20multi-modal%20factor%20graph%20back-end%20that%0Adynamically%20fuses%20heterogeneous%20sensor%20data.%20The%20proposed%20multi-sensor%20factor%0Agraph%20maintenance%20strategy%20efficiently%20addresses%20issues%20caused%20by%20asynchronous%0Asensor%20frequencies%20and%20partial%20data%20loss.%20Experimental%20results%20demonstrate%0AWater-DSLAM%20achieves%20superior%20robustness%20%280.039%20m%20trajectory%20RMSE%20and%20100%5C%25%0Acontinuity%20ratio%20during%20partial%20sensor%20dropout%29%20and%20dense%20mapping%20%286922.4%0Apoints/m%5E3%20in%20750%20m%5E3%20water%20volume%2C%20approximately%2010%20times%20denser%20than%20existing%0Amethods%29%20in%20various%20challenging%20environments%2C%20including%20pools%2C%20dark%20underwater%0Ascenes%2C%2016-meter-deep%20sinkholes%2C%20and%20field%20rivers.%20Our%20project%20is%20available%20at%0Ahttps%3A//water-scanner.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Underwater%252C%2520Fault-Tolerant%252C%2520Laser-Aided%2520Robotic%2520Multi-Modal%2520Dense%250A%2520%2520SLAM%2520System%2520for%2520Continuous%2520Underwater%2520In-Situ%2520Observation%26entry.906535625%3DYaming%2520Ou%2520and%2520Junfeng%2520Fan%2520and%2520Chao%2520Zhou%2520and%2520Pengju%2520Zhang%2520and%2520Zongyuan%2520Shen%2520and%2520Yichen%2520Fu%2520and%2520Xiaoyan%2520Liu%2520and%2520Zengguang%2520Hou%26entry.1292438233%3D%2520%2520Existing%2520underwater%2520SLAM%2520systems%2520are%2520difficult%2520to%2520work%2520effectively%2520in%250Atexture-sparse%2520and%2520geometrically%2520degraded%2520underwater%2520environments%252C%2520resulting%2520in%250Aintermittent%2520tracking%2520and%2520sparse%2520mapping.%2520Therefore%252C%2520we%2520present%2520Water-DSLAM%252C%2520a%250Anovel%2520laser-aided%2520multi-sensor%2520fusion%2520system%2520that%2520can%2520achieve%2520uninterrupted%252C%250Afault-tolerant%2520dense%2520SLAM%2520capable%2520of%2520continuous%2520in-situ%2520observation%2520in%2520diverse%250Acomplex%2520underwater%2520scenarios%2520through%2520three%2520key%2520innovations%253A%2520Firstly%252C%2520we%2520develop%250AWater-Scanner%252C%2520a%2520multi-sensor%2520fusion%2520robotic%2520platform%2520featuring%2520a%2520self-designed%250AUnderwater%2520Binocular%2520Structured%2520Light%2520%2528UBSL%2529%2520module%2520that%2520enables%2520high-precision%250A3D%2520perception.%2520Secondly%252C%2520we%2520propose%2520a%2520fault-tolerant%2520triple-subsystem%250Aarchitecture%2520combining%253A%25201%2529%2520DP-INS%2520%2528DVL-%2520and%2520Pressure-aided%2520Inertial%2520Navigation%250ASystem%2529%253A%2520fusing%2520inertial%2520measurement%2520unit%252C%2520doppler%2520velocity%2520log%252C%2520and%2520pressure%250Asensor%2520based%2520Error-State%2520Kalman%2520Filter%2520%2528ESKF%2529%2520to%2520provide%2520high-frequency%250Aabsolute%2520odometry%25202%2529%2520Water-UBSL%253A%2520a%2520novel%2520Iterated%2520ESKF%2520%2528IESKF%2529-based%2520tight%250Acoupling%2520between%2520UBSL%2520and%2520DP-INS%2520to%2520mitigate%2520UBSL%2527s%2520degeneration%2520issues%25203%2529%250AWater-Stereo%253A%2520a%2520fusion%2520of%2520DP-INS%2520and%2520stereo%2520camera%2520for%2520accurate%2520initialization%250Aand%2520tracking.%2520Thirdly%252C%2520we%2520introduce%2520a%2520multi-modal%2520factor%2520graph%2520back-end%2520that%250Adynamically%2520fuses%2520heterogeneous%2520sensor%2520data.%2520The%2520proposed%2520multi-sensor%2520factor%250Agraph%2520maintenance%2520strategy%2520efficiently%2520addresses%2520issues%2520caused%2520by%2520asynchronous%250Asensor%2520frequencies%2520and%2520partial%2520data%2520loss.%2520Experimental%2520results%2520demonstrate%250AWater-DSLAM%2520achieves%2520superior%2520robustness%2520%25280.039%2520m%2520trajectory%2520RMSE%2520and%2520100%255C%2525%250Acontinuity%2520ratio%2520during%2520partial%2520sensor%2520dropout%2529%2520and%2520dense%2520mapping%2520%25286922.4%250Apoints/m%255E3%2520in%2520750%2520m%255E3%2520water%2520volume%252C%2520approximately%252010%2520times%2520denser%2520than%2520existing%250Amethods%2529%2520in%2520various%2520challenging%2520environments%252C%2520including%2520pools%252C%2520dark%2520underwater%250Ascenes%252C%252016-meter-deep%2520sinkholes%252C%2520and%2520field%2520rivers.%2520Our%2520project%2520is%2520available%2520at%250Ahttps%253A//water-scanner.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Underwater%2C%20Fault-Tolerant%2C%20Laser-Aided%20Robotic%20Multi-Modal%20Dense%0A%20%20SLAM%20System%20for%20Continuous%20Underwater%20In-Situ%20Observation&entry.906535625=Yaming%20Ou%20and%20Junfeng%20Fan%20and%20Chao%20Zhou%20and%20Pengju%20Zhang%20and%20Zongyuan%20Shen%20and%20Yichen%20Fu%20and%20Xiaoyan%20Liu%20and%20Zengguang%20Hou&entry.1292438233=%20%20Existing%20underwater%20SLAM%20systems%20are%20difficult%20to%20work%20effectively%20in%0Atexture-sparse%20and%20geometrically%20degraded%20underwater%20environments%2C%20resulting%20in%0Aintermittent%20tracking%20and%20sparse%20mapping.%20Therefore%2C%20we%20present%20Water-DSLAM%2C%20a%0Anovel%20laser-aided%20multi-sensor%20fusion%20system%20that%20can%20achieve%20uninterrupted%2C%0Afault-tolerant%20dense%20SLAM%20capable%20of%20continuous%20in-situ%20observation%20in%20diverse%0Acomplex%20underwater%20scenarios%20through%20three%20key%20innovations%3A%20Firstly%2C%20we%20develop%0AWater-Scanner%2C%20a%20multi-sensor%20fusion%20robotic%20platform%20featuring%20a%20self-designed%0AUnderwater%20Binocular%20Structured%20Light%20%28UBSL%29%20module%20that%20enables%20high-precision%0A3D%20perception.%20Secondly%2C%20we%20propose%20a%20fault-tolerant%20triple-subsystem%0Aarchitecture%20combining%3A%201%29%20DP-INS%20%28DVL-%20and%20Pressure-aided%20Inertial%20Navigation%0ASystem%29%3A%20fusing%20inertial%20measurement%20unit%2C%20doppler%20velocity%20log%2C%20and%20pressure%0Asensor%20based%20Error-State%20Kalman%20Filter%20%28ESKF%29%20to%20provide%20high-frequency%0Aabsolute%20odometry%202%29%20Water-UBSL%3A%20a%20novel%20Iterated%20ESKF%20%28IESKF%29-based%20tight%0Acoupling%20between%20UBSL%20and%20DP-INS%20to%20mitigate%20UBSL%27s%20degeneration%20issues%203%29%0AWater-Stereo%3A%20a%20fusion%20of%20DP-INS%20and%20stereo%20camera%20for%20accurate%20initialization%0Aand%20tracking.%20Thirdly%2C%20we%20introduce%20a%20multi-modal%20factor%20graph%20back-end%20that%0Adynamically%20fuses%20heterogeneous%20sensor%20data.%20The%20proposed%20multi-sensor%20factor%0Agraph%20maintenance%20strategy%20efficiently%20addresses%20issues%20caused%20by%20asynchronous%0Asensor%20frequencies%20and%20partial%20data%20loss.%20Experimental%20results%20demonstrate%0AWater-DSLAM%20achieves%20superior%20robustness%20%280.039%20m%20trajectory%20RMSE%20and%20100%5C%25%0Acontinuity%20ratio%20during%20partial%20sensor%20dropout%29%20and%20dense%20mapping%20%286922.4%0Apoints/m%5E3%20in%20750%20m%5E3%20water%20volume%2C%20approximately%2010%20times%20denser%20than%20existing%0Amethods%29%20in%20various%20challenging%20environments%2C%20including%20pools%2C%20dark%20underwater%0Ascenes%2C%2016-meter-deep%20sinkholes%2C%20and%20field%20rivers.%20Our%20project%20is%20available%20at%0Ahttps%3A//water-scanner.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21826v1&entry.124074799=Read"},
{"title": "Real Time Semantic Segmentation of High Resolution Automotive LiDAR\n  Scans", "author": "Hannes Reichert and Benjamin Serfling and Elijah Sch\u00fcssler and Kerim Turacan and Konrad Doll and Bernhard Sick", "abstract": "  In recent studies, numerous previous works emphasize the importance of\nsemantic segmentation of LiDAR data as a critical component to the development\nof driver-assistance systems and autonomous vehicles. However, many\nstate-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors\nand struggle with real-time constraints. This study introduces a novel semantic\nsegmentation framework tailored for modern high-resolution LiDAR sensors that\naddresses both accuracy and real-time processing demands. We propose a novel\nLiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban\ntraffic scenes. Furthermore, we propose a semantic segmentation method\nutilizing surface normals as strong input features. Our approach is bridging\nthe gap between cutting-edge research and practical automotive applications.\nAdditionaly, we provide a Robot Operating System (ROS2) implementation that we\noperate on our research vehicle. Our dataset and code are publicly available:\nhttps://github.com/kav-institute/SemanticLiDAR.\n", "link": "http://arxiv.org/abs/2504.21602v1", "date": "2025-04-30", "relevancy": 2.338, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6024}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5823}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real%20Time%20Semantic%20Segmentation%20of%20High%20Resolution%20Automotive%20LiDAR%0A%20%20Scans&body=Title%3A%20Real%20Time%20Semantic%20Segmentation%20of%20High%20Resolution%20Automotive%20LiDAR%0A%20%20Scans%0AAuthor%3A%20Hannes%20Reichert%20and%20Benjamin%20Serfling%20and%20Elijah%20Sch%C3%BCssler%20and%20Kerim%20Turacan%20and%20Konrad%20Doll%20and%20Bernhard%20Sick%0AAbstract%3A%20%20%20In%20recent%20studies%2C%20numerous%20previous%20works%20emphasize%20the%20importance%20of%0Asemantic%20segmentation%20of%20LiDAR%20data%20as%20a%20critical%20component%20to%20the%20development%0Aof%20driver-assistance%20systems%20and%20autonomous%20vehicles.%20However%2C%20many%0Astate-of-the-art%20methods%20are%20tested%20on%20outdated%2C%20lower-resolution%20LiDAR%20sensors%0Aand%20struggle%20with%20real-time%20constraints.%20This%20study%20introduces%20a%20novel%20semantic%0Asegmentation%20framework%20tailored%20for%20modern%20high-resolution%20LiDAR%20sensors%20that%0Aaddresses%20both%20accuracy%20and%20real-time%20processing%20demands.%20We%20propose%20a%20novel%0ALiDAR%20dataset%20collected%20by%20a%20cutting-edge%20automotive%20128%20layer%20LiDAR%20in%20urban%0Atraffic%20scenes.%20Furthermore%2C%20we%20propose%20a%20semantic%20segmentation%20method%0Autilizing%20surface%20normals%20as%20strong%20input%20features.%20Our%20approach%20is%20bridging%0Athe%20gap%20between%20cutting-edge%20research%20and%20practical%20automotive%20applications.%0AAdditionaly%2C%20we%20provide%20a%20Robot%20Operating%20System%20%28ROS2%29%20implementation%20that%20we%0Aoperate%20on%20our%20research%20vehicle.%20Our%20dataset%20and%20code%20are%20publicly%20available%3A%0Ahttps%3A//github.com/kav-institute/SemanticLiDAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal%2520Time%2520Semantic%2520Segmentation%2520of%2520High%2520Resolution%2520Automotive%2520LiDAR%250A%2520%2520Scans%26entry.906535625%3DHannes%2520Reichert%2520and%2520Benjamin%2520Serfling%2520and%2520Elijah%2520Sch%25C3%25BCssler%2520and%2520Kerim%2520Turacan%2520and%2520Konrad%2520Doll%2520and%2520Bernhard%2520Sick%26entry.1292438233%3D%2520%2520In%2520recent%2520studies%252C%2520numerous%2520previous%2520works%2520emphasize%2520the%2520importance%2520of%250Asemantic%2520segmentation%2520of%2520LiDAR%2520data%2520as%2520a%2520critical%2520component%2520to%2520the%2520development%250Aof%2520driver-assistance%2520systems%2520and%2520autonomous%2520vehicles.%2520However%252C%2520many%250Astate-of-the-art%2520methods%2520are%2520tested%2520on%2520outdated%252C%2520lower-resolution%2520LiDAR%2520sensors%250Aand%2520struggle%2520with%2520real-time%2520constraints.%2520This%2520study%2520introduces%2520a%2520novel%2520semantic%250Asegmentation%2520framework%2520tailored%2520for%2520modern%2520high-resolution%2520LiDAR%2520sensors%2520that%250Aaddresses%2520both%2520accuracy%2520and%2520real-time%2520processing%2520demands.%2520We%2520propose%2520a%2520novel%250ALiDAR%2520dataset%2520collected%2520by%2520a%2520cutting-edge%2520automotive%2520128%2520layer%2520LiDAR%2520in%2520urban%250Atraffic%2520scenes.%2520Furthermore%252C%2520we%2520propose%2520a%2520semantic%2520segmentation%2520method%250Autilizing%2520surface%2520normals%2520as%2520strong%2520input%2520features.%2520Our%2520approach%2520is%2520bridging%250Athe%2520gap%2520between%2520cutting-edge%2520research%2520and%2520practical%2520automotive%2520applications.%250AAdditionaly%252C%2520we%2520provide%2520a%2520Robot%2520Operating%2520System%2520%2528ROS2%2529%2520implementation%2520that%2520we%250Aoperate%2520on%2520our%2520research%2520vehicle.%2520Our%2520dataset%2520and%2520code%2520are%2520publicly%2520available%253A%250Ahttps%253A//github.com/kav-institute/SemanticLiDAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real%20Time%20Semantic%20Segmentation%20of%20High%20Resolution%20Automotive%20LiDAR%0A%20%20Scans&entry.906535625=Hannes%20Reichert%20and%20Benjamin%20Serfling%20and%20Elijah%20Sch%C3%BCssler%20and%20Kerim%20Turacan%20and%20Konrad%20Doll%20and%20Bernhard%20Sick&entry.1292438233=%20%20In%20recent%20studies%2C%20numerous%20previous%20works%20emphasize%20the%20importance%20of%0Asemantic%20segmentation%20of%20LiDAR%20data%20as%20a%20critical%20component%20to%20the%20development%0Aof%20driver-assistance%20systems%20and%20autonomous%20vehicles.%20However%2C%20many%0Astate-of-the-art%20methods%20are%20tested%20on%20outdated%2C%20lower-resolution%20LiDAR%20sensors%0Aand%20struggle%20with%20real-time%20constraints.%20This%20study%20introduces%20a%20novel%20semantic%0Asegmentation%20framework%20tailored%20for%20modern%20high-resolution%20LiDAR%20sensors%20that%0Aaddresses%20both%20accuracy%20and%20real-time%20processing%20demands.%20We%20propose%20a%20novel%0ALiDAR%20dataset%20collected%20by%20a%20cutting-edge%20automotive%20128%20layer%20LiDAR%20in%20urban%0Atraffic%20scenes.%20Furthermore%2C%20we%20propose%20a%20semantic%20segmentation%20method%0Autilizing%20surface%20normals%20as%20strong%20input%20features.%20Our%20approach%20is%20bridging%0Athe%20gap%20between%20cutting-edge%20research%20and%20practical%20automotive%20applications.%0AAdditionaly%2C%20we%20provide%20a%20Robot%20Operating%20System%20%28ROS2%29%20implementation%20that%20we%0Aoperate%20on%20our%20research%20vehicle.%20Our%20dataset%20and%20code%20are%20publicly%20available%3A%0Ahttps%3A//github.com/kav-institute/SemanticLiDAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21602v1&entry.124074799=Read"},
{"title": "Visual Text Processing: A Comprehensive Review and Unified Evaluation", "author": "Yan Shu and Weichao Zeng and Fangmin Zhao and Zeyu Chen and Zhenhang Li and Xiaomeng Yang and Yu Zhou and Paolo Rota and Xiang Bai and Lianwen Jin and Xu-Cheng Yin and Nicu Sebe", "abstract": "  Visual text is a crucial component in both document and scene images,\nconveying rich semantic information and attracting significant attention in the\ncomputer vision community. Beyond traditional tasks such as text detection and\nrecognition, visual text processing has witnessed rapid advancements driven by\nthe emergence of foundation models, including text image reconstruction and\ntext image manipulation. Despite significant progress, challenges remain due to\nthe unique properties that differentiate text from general objects. Effectively\ncapturing and leveraging these distinct textual characteristics is essential\nfor developing robust visual text processing models. In this survey, we present\na comprehensive, multi-perspective analysis of recent advancements in visual\ntext processing, focusing on two key questions: (1) What textual features are\nmost suitable for different visual text processing tasks? (2) How can these\ndistinctive text features be effectively incorporated into processing\nframeworks? Furthermore, we introduce VTPBench, a new benchmark that\nencompasses a broad range of visual text processing datasets. Leveraging the\nadvanced visual quality assessment capabilities of multimodal large language\nmodels (MLLMs), we propose VTPScore, a novel evaluation metric designed to\nensure fair and reliable evaluation. Our empirical study with more than 20\nspecific models reveals substantial room for improvement in the current\ntechniques. Our aim is to establish this work as a fundamental resource that\nfosters future exploration and innovation in the dynamic field of visual text\nprocessing. The relevant repository is available at\nhttps://github.com/shuyansy/Visual-Text-Processing-survey.\n", "link": "http://arxiv.org/abs/2504.21682v1", "date": "2025-04-30", "relevancy": 2.3174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5926}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Text%20Processing%3A%20A%20Comprehensive%20Review%20and%20Unified%20Evaluation&body=Title%3A%20Visual%20Text%20Processing%3A%20A%20Comprehensive%20Review%20and%20Unified%20Evaluation%0AAuthor%3A%20Yan%20Shu%20and%20Weichao%20Zeng%20and%20Fangmin%20Zhao%20and%20Zeyu%20Chen%20and%20Zhenhang%20Li%20and%20Xiaomeng%20Yang%20and%20Yu%20Zhou%20and%20Paolo%20Rota%20and%20Xiang%20Bai%20and%20Lianwen%20Jin%20and%20Xu-Cheng%20Yin%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Visual%20text%20is%20a%20crucial%20component%20in%20both%20document%20and%20scene%20images%2C%0Aconveying%20rich%20semantic%20information%20and%20attracting%20significant%20attention%20in%20the%0Acomputer%20vision%20community.%20Beyond%20traditional%20tasks%20such%20as%20text%20detection%20and%0Arecognition%2C%20visual%20text%20processing%20has%20witnessed%20rapid%20advancements%20driven%20by%0Athe%20emergence%20of%20foundation%20models%2C%20including%20text%20image%20reconstruction%20and%0Atext%20image%20manipulation.%20Despite%20significant%20progress%2C%20challenges%20remain%20due%20to%0Athe%20unique%20properties%20that%20differentiate%20text%20from%20general%20objects.%20Effectively%0Acapturing%20and%20leveraging%20these%20distinct%20textual%20characteristics%20is%20essential%0Afor%20developing%20robust%20visual%20text%20processing%20models.%20In%20this%20survey%2C%20we%20present%0Aa%20comprehensive%2C%20multi-perspective%20analysis%20of%20recent%20advancements%20in%20visual%0Atext%20processing%2C%20focusing%20on%20two%20key%20questions%3A%20%281%29%20What%20textual%20features%20are%0Amost%20suitable%20for%20different%20visual%20text%20processing%20tasks%3F%20%282%29%20How%20can%20these%0Adistinctive%20text%20features%20be%20effectively%20incorporated%20into%20processing%0Aframeworks%3F%20Furthermore%2C%20we%20introduce%20VTPBench%2C%20a%20new%20benchmark%20that%0Aencompasses%20a%20broad%20range%20of%20visual%20text%20processing%20datasets.%20Leveraging%20the%0Aadvanced%20visual%20quality%20assessment%20capabilities%20of%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20we%20propose%20VTPScore%2C%20a%20novel%20evaluation%20metric%20designed%20to%0Aensure%20fair%20and%20reliable%20evaluation.%20Our%20empirical%20study%20with%20more%20than%2020%0Aspecific%20models%20reveals%20substantial%20room%20for%20improvement%20in%20the%20current%0Atechniques.%20Our%20aim%20is%20to%20establish%20this%20work%20as%20a%20fundamental%20resource%20that%0Afosters%20future%20exploration%20and%20innovation%20in%20the%20dynamic%20field%20of%20visual%20text%0Aprocessing.%20The%20relevant%20repository%20is%20available%20at%0Ahttps%3A//github.com/shuyansy/Visual-Text-Processing-survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Text%2520Processing%253A%2520A%2520Comprehensive%2520Review%2520and%2520Unified%2520Evaluation%26entry.906535625%3DYan%2520Shu%2520and%2520Weichao%2520Zeng%2520and%2520Fangmin%2520Zhao%2520and%2520Zeyu%2520Chen%2520and%2520Zhenhang%2520Li%2520and%2520Xiaomeng%2520Yang%2520and%2520Yu%2520Zhou%2520and%2520Paolo%2520Rota%2520and%2520Xiang%2520Bai%2520and%2520Lianwen%2520Jin%2520and%2520Xu-Cheng%2520Yin%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Visual%2520text%2520is%2520a%2520crucial%2520component%2520in%2520both%2520document%2520and%2520scene%2520images%252C%250Aconveying%2520rich%2520semantic%2520information%2520and%2520attracting%2520significant%2520attention%2520in%2520the%250Acomputer%2520vision%2520community.%2520Beyond%2520traditional%2520tasks%2520such%2520as%2520text%2520detection%2520and%250Arecognition%252C%2520visual%2520text%2520processing%2520has%2520witnessed%2520rapid%2520advancements%2520driven%2520by%250Athe%2520emergence%2520of%2520foundation%2520models%252C%2520including%2520text%2520image%2520reconstruction%2520and%250Atext%2520image%2520manipulation.%2520Despite%2520significant%2520progress%252C%2520challenges%2520remain%2520due%2520to%250Athe%2520unique%2520properties%2520that%2520differentiate%2520text%2520from%2520general%2520objects.%2520Effectively%250Acapturing%2520and%2520leveraging%2520these%2520distinct%2520textual%2520characteristics%2520is%2520essential%250Afor%2520developing%2520robust%2520visual%2520text%2520processing%2520models.%2520In%2520this%2520survey%252C%2520we%2520present%250Aa%2520comprehensive%252C%2520multi-perspective%2520analysis%2520of%2520recent%2520advancements%2520in%2520visual%250Atext%2520processing%252C%2520focusing%2520on%2520two%2520key%2520questions%253A%2520%25281%2529%2520What%2520textual%2520features%2520are%250Amost%2520suitable%2520for%2520different%2520visual%2520text%2520processing%2520tasks%253F%2520%25282%2529%2520How%2520can%2520these%250Adistinctive%2520text%2520features%2520be%2520effectively%2520incorporated%2520into%2520processing%250Aframeworks%253F%2520Furthermore%252C%2520we%2520introduce%2520VTPBench%252C%2520a%2520new%2520benchmark%2520that%250Aencompasses%2520a%2520broad%2520range%2520of%2520visual%2520text%2520processing%2520datasets.%2520Leveraging%2520the%250Aadvanced%2520visual%2520quality%2520assessment%2520capabilities%2520of%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%252C%2520we%2520propose%2520VTPScore%252C%2520a%2520novel%2520evaluation%2520metric%2520designed%2520to%250Aensure%2520fair%2520and%2520reliable%2520evaluation.%2520Our%2520empirical%2520study%2520with%2520more%2520than%252020%250Aspecific%2520models%2520reveals%2520substantial%2520room%2520for%2520improvement%2520in%2520the%2520current%250Atechniques.%2520Our%2520aim%2520is%2520to%2520establish%2520this%2520work%2520as%2520a%2520fundamental%2520resource%2520that%250Afosters%2520future%2520exploration%2520and%2520innovation%2520in%2520the%2520dynamic%2520field%2520of%2520visual%2520text%250Aprocessing.%2520The%2520relevant%2520repository%2520is%2520available%2520at%250Ahttps%253A//github.com/shuyansy/Visual-Text-Processing-survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Text%20Processing%3A%20A%20Comprehensive%20Review%20and%20Unified%20Evaluation&entry.906535625=Yan%20Shu%20and%20Weichao%20Zeng%20and%20Fangmin%20Zhao%20and%20Zeyu%20Chen%20and%20Zhenhang%20Li%20and%20Xiaomeng%20Yang%20and%20Yu%20Zhou%20and%20Paolo%20Rota%20and%20Xiang%20Bai%20and%20Lianwen%20Jin%20and%20Xu-Cheng%20Yin%20and%20Nicu%20Sebe&entry.1292438233=%20%20Visual%20text%20is%20a%20crucial%20component%20in%20both%20document%20and%20scene%20images%2C%0Aconveying%20rich%20semantic%20information%20and%20attracting%20significant%20attention%20in%20the%0Acomputer%20vision%20community.%20Beyond%20traditional%20tasks%20such%20as%20text%20detection%20and%0Arecognition%2C%20visual%20text%20processing%20has%20witnessed%20rapid%20advancements%20driven%20by%0Athe%20emergence%20of%20foundation%20models%2C%20including%20text%20image%20reconstruction%20and%0Atext%20image%20manipulation.%20Despite%20significant%20progress%2C%20challenges%20remain%20due%20to%0Athe%20unique%20properties%20that%20differentiate%20text%20from%20general%20objects.%20Effectively%0Acapturing%20and%20leveraging%20these%20distinct%20textual%20characteristics%20is%20essential%0Afor%20developing%20robust%20visual%20text%20processing%20models.%20In%20this%20survey%2C%20we%20present%0Aa%20comprehensive%2C%20multi-perspective%20analysis%20of%20recent%20advancements%20in%20visual%0Atext%20processing%2C%20focusing%20on%20two%20key%20questions%3A%20%281%29%20What%20textual%20features%20are%0Amost%20suitable%20for%20different%20visual%20text%20processing%20tasks%3F%20%282%29%20How%20can%20these%0Adistinctive%20text%20features%20be%20effectively%20incorporated%20into%20processing%0Aframeworks%3F%20Furthermore%2C%20we%20introduce%20VTPBench%2C%20a%20new%20benchmark%20that%0Aencompasses%20a%20broad%20range%20of%20visual%20text%20processing%20datasets.%20Leveraging%20the%0Aadvanced%20visual%20quality%20assessment%20capabilities%20of%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20we%20propose%20VTPScore%2C%20a%20novel%20evaluation%20metric%20designed%20to%0Aensure%20fair%20and%20reliable%20evaluation.%20Our%20empirical%20study%20with%20more%20than%2020%0Aspecific%20models%20reveals%20substantial%20room%20for%20improvement%20in%20the%20current%0Atechniques.%20Our%20aim%20is%20to%20establish%20this%20work%20as%20a%20fundamental%20resource%20that%0Afosters%20future%20exploration%20and%20innovation%20in%20the%20dynamic%20field%20of%20visual%20text%0Aprocessing.%20The%20relevant%20repository%20is%20available%20at%0Ahttps%3A//github.com/shuyansy/Visual-Text-Processing-survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21682v1&entry.124074799=Read"},
{"title": "Self-Supervised Monocular Visual Drone Model Identification through\n  Improved Occlusion Handling", "author": "Stavrow A. Bahnam and Christophe De Wagter and Guido C. H. E. de Croon", "abstract": "  Ego-motion estimation is vital for drones when flying in GPS-denied\nenvironments. Vision-based methods struggle when flight speed increases and\nclose-by objects lead to difficult visual conditions with considerable motion\nblur and large occlusions. To tackle this, vision is typically complemented by\nstate estimation filters that combine a drone model with inertial measurements.\nHowever, these drone models are currently learned in a supervised manner with\nground-truth data from external motion capture systems, limiting scalability to\ndifferent environments and drones. In this work, we propose a self-supervised\nlearning scheme to train a neural-network-based drone model using only onboard\nmonocular video and flight controller data (IMU and motor feedback). We achieve\nthis by first training a self-supervised relative pose estimation model, which\nthen serves as a teacher for the drone model. To allow this to work at high\nspeed close to obstacles, we propose an improved occlusion handling method for\ntraining self-supervised pose estimation models. Due to this method, the root\nmean squared error of resulting odometry estimates is reduced by an average of\n15%. Moreover, the student neural drone model can be successfully obtained from\nthe onboard data. It even becomes more accurate at higher speeds compared to\nits teacher, the self-supervised vision-based model. We demonstrate the value\nof the neural drone model by integrating it into a traditional filter-based VIO\nsystem (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing\ntrajectories near obstacles. Self-supervised learning of ego-motion estimation\nrepresents a significant step toward bridging the gap between flying in\ncontrolled, expensive lab environments and real-world drone applications. The\nfusion of vision and drone models will enable higher-speed flight and improve\nstate estimation, on any drone in any environment.\n", "link": "http://arxiv.org/abs/2504.21695v1", "date": "2025-04-30", "relevancy": 2.3125, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5867}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5801}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Monocular%20Visual%20Drone%20Model%20Identification%20through%0A%20%20Improved%20Occlusion%20Handling&body=Title%3A%20Self-Supervised%20Monocular%20Visual%20Drone%20Model%20Identification%20through%0A%20%20Improved%20Occlusion%20Handling%0AAuthor%3A%20Stavrow%20A.%20Bahnam%20and%20Christophe%20De%20Wagter%20and%20Guido%20C.%20H.%20E.%20de%20Croon%0AAbstract%3A%20%20%20Ego-motion%20estimation%20is%20vital%20for%20drones%20when%20flying%20in%20GPS-denied%0Aenvironments.%20Vision-based%20methods%20struggle%20when%20flight%20speed%20increases%20and%0Aclose-by%20objects%20lead%20to%20difficult%20visual%20conditions%20with%20considerable%20motion%0Ablur%20and%20large%20occlusions.%20To%20tackle%20this%2C%20vision%20is%20typically%20complemented%20by%0Astate%20estimation%20filters%20that%20combine%20a%20drone%20model%20with%20inertial%20measurements.%0AHowever%2C%20these%20drone%20models%20are%20currently%20learned%20in%20a%20supervised%20manner%20with%0Aground-truth%20data%20from%20external%20motion%20capture%20systems%2C%20limiting%20scalability%20to%0Adifferent%20environments%20and%20drones.%20In%20this%20work%2C%20we%20propose%20a%20self-supervised%0Alearning%20scheme%20to%20train%20a%20neural-network-based%20drone%20model%20using%20only%20onboard%0Amonocular%20video%20and%20flight%20controller%20data%20%28IMU%20and%20motor%20feedback%29.%20We%20achieve%0Athis%20by%20first%20training%20a%20self-supervised%20relative%20pose%20estimation%20model%2C%20which%0Athen%20serves%20as%20a%20teacher%20for%20the%20drone%20model.%20To%20allow%20this%20to%20work%20at%20high%0Aspeed%20close%20to%20obstacles%2C%20we%20propose%20an%20improved%20occlusion%20handling%20method%20for%0Atraining%20self-supervised%20pose%20estimation%20models.%20Due%20to%20this%20method%2C%20the%20root%0Amean%20squared%20error%20of%20resulting%20odometry%20estimates%20is%20reduced%20by%20an%20average%20of%0A15%25.%20Moreover%2C%20the%20student%20neural%20drone%20model%20can%20be%20successfully%20obtained%20from%0Athe%20onboard%20data.%20It%20even%20becomes%20more%20accurate%20at%20higher%20speeds%20compared%20to%0Aits%20teacher%2C%20the%20self-supervised%20vision-based%20model.%20We%20demonstrate%20the%20value%0Aof%20the%20neural%20drone%20model%20by%20integrating%20it%20into%20a%20traditional%20filter-based%20VIO%0Asystem%20%28ROVIO%29%2C%20resulting%20in%20superior%20odometry%20accuracy%20on%20aggressive%203D%20racing%0Atrajectories%20near%20obstacles.%20Self-supervised%20learning%20of%20ego-motion%20estimation%0Arepresents%20a%20significant%20step%20toward%20bridging%20the%20gap%20between%20flying%20in%0Acontrolled%2C%20expensive%20lab%20environments%20and%20real-world%20drone%20applications.%20The%0Afusion%20of%20vision%20and%20drone%20models%20will%20enable%20higher-speed%20flight%20and%20improve%0Astate%20estimation%2C%20on%20any%20drone%20in%20any%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Monocular%2520Visual%2520Drone%2520Model%2520Identification%2520through%250A%2520%2520Improved%2520Occlusion%2520Handling%26entry.906535625%3DStavrow%2520A.%2520Bahnam%2520and%2520Christophe%2520De%2520Wagter%2520and%2520Guido%2520C.%2520H.%2520E.%2520de%2520Croon%26entry.1292438233%3D%2520%2520Ego-motion%2520estimation%2520is%2520vital%2520for%2520drones%2520when%2520flying%2520in%2520GPS-denied%250Aenvironments.%2520Vision-based%2520methods%2520struggle%2520when%2520flight%2520speed%2520increases%2520and%250Aclose-by%2520objects%2520lead%2520to%2520difficult%2520visual%2520conditions%2520with%2520considerable%2520motion%250Ablur%2520and%2520large%2520occlusions.%2520To%2520tackle%2520this%252C%2520vision%2520is%2520typically%2520complemented%2520by%250Astate%2520estimation%2520filters%2520that%2520combine%2520a%2520drone%2520model%2520with%2520inertial%2520measurements.%250AHowever%252C%2520these%2520drone%2520models%2520are%2520currently%2520learned%2520in%2520a%2520supervised%2520manner%2520with%250Aground-truth%2520data%2520from%2520external%2520motion%2520capture%2520systems%252C%2520limiting%2520scalability%2520to%250Adifferent%2520environments%2520and%2520drones.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520self-supervised%250Alearning%2520scheme%2520to%2520train%2520a%2520neural-network-based%2520drone%2520model%2520using%2520only%2520onboard%250Amonocular%2520video%2520and%2520flight%2520controller%2520data%2520%2528IMU%2520and%2520motor%2520feedback%2529.%2520We%2520achieve%250Athis%2520by%2520first%2520training%2520a%2520self-supervised%2520relative%2520pose%2520estimation%2520model%252C%2520which%250Athen%2520serves%2520as%2520a%2520teacher%2520for%2520the%2520drone%2520model.%2520To%2520allow%2520this%2520to%2520work%2520at%2520high%250Aspeed%2520close%2520to%2520obstacles%252C%2520we%2520propose%2520an%2520improved%2520occlusion%2520handling%2520method%2520for%250Atraining%2520self-supervised%2520pose%2520estimation%2520models.%2520Due%2520to%2520this%2520method%252C%2520the%2520root%250Amean%2520squared%2520error%2520of%2520resulting%2520odometry%2520estimates%2520is%2520reduced%2520by%2520an%2520average%2520of%250A15%2525.%2520Moreover%252C%2520the%2520student%2520neural%2520drone%2520model%2520can%2520be%2520successfully%2520obtained%2520from%250Athe%2520onboard%2520data.%2520It%2520even%2520becomes%2520more%2520accurate%2520at%2520higher%2520speeds%2520compared%2520to%250Aits%2520teacher%252C%2520the%2520self-supervised%2520vision-based%2520model.%2520We%2520demonstrate%2520the%2520value%250Aof%2520the%2520neural%2520drone%2520model%2520by%2520integrating%2520it%2520into%2520a%2520traditional%2520filter-based%2520VIO%250Asystem%2520%2528ROVIO%2529%252C%2520resulting%2520in%2520superior%2520odometry%2520accuracy%2520on%2520aggressive%25203D%2520racing%250Atrajectories%2520near%2520obstacles.%2520Self-supervised%2520learning%2520of%2520ego-motion%2520estimation%250Arepresents%2520a%2520significant%2520step%2520toward%2520bridging%2520the%2520gap%2520between%2520flying%2520in%250Acontrolled%252C%2520expensive%2520lab%2520environments%2520and%2520real-world%2520drone%2520applications.%2520The%250Afusion%2520of%2520vision%2520and%2520drone%2520models%2520will%2520enable%2520higher-speed%2520flight%2520and%2520improve%250Astate%2520estimation%252C%2520on%2520any%2520drone%2520in%2520any%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Monocular%20Visual%20Drone%20Model%20Identification%20through%0A%20%20Improved%20Occlusion%20Handling&entry.906535625=Stavrow%20A.%20Bahnam%20and%20Christophe%20De%20Wagter%20and%20Guido%20C.%20H.%20E.%20de%20Croon&entry.1292438233=%20%20Ego-motion%20estimation%20is%20vital%20for%20drones%20when%20flying%20in%20GPS-denied%0Aenvironments.%20Vision-based%20methods%20struggle%20when%20flight%20speed%20increases%20and%0Aclose-by%20objects%20lead%20to%20difficult%20visual%20conditions%20with%20considerable%20motion%0Ablur%20and%20large%20occlusions.%20To%20tackle%20this%2C%20vision%20is%20typically%20complemented%20by%0Astate%20estimation%20filters%20that%20combine%20a%20drone%20model%20with%20inertial%20measurements.%0AHowever%2C%20these%20drone%20models%20are%20currently%20learned%20in%20a%20supervised%20manner%20with%0Aground-truth%20data%20from%20external%20motion%20capture%20systems%2C%20limiting%20scalability%20to%0Adifferent%20environments%20and%20drones.%20In%20this%20work%2C%20we%20propose%20a%20self-supervised%0Alearning%20scheme%20to%20train%20a%20neural-network-based%20drone%20model%20using%20only%20onboard%0Amonocular%20video%20and%20flight%20controller%20data%20%28IMU%20and%20motor%20feedback%29.%20We%20achieve%0Athis%20by%20first%20training%20a%20self-supervised%20relative%20pose%20estimation%20model%2C%20which%0Athen%20serves%20as%20a%20teacher%20for%20the%20drone%20model.%20To%20allow%20this%20to%20work%20at%20high%0Aspeed%20close%20to%20obstacles%2C%20we%20propose%20an%20improved%20occlusion%20handling%20method%20for%0Atraining%20self-supervised%20pose%20estimation%20models.%20Due%20to%20this%20method%2C%20the%20root%0Amean%20squared%20error%20of%20resulting%20odometry%20estimates%20is%20reduced%20by%20an%20average%20of%0A15%25.%20Moreover%2C%20the%20student%20neural%20drone%20model%20can%20be%20successfully%20obtained%20from%0Athe%20onboard%20data.%20It%20even%20becomes%20more%20accurate%20at%20higher%20speeds%20compared%20to%0Aits%20teacher%2C%20the%20self-supervised%20vision-based%20model.%20We%20demonstrate%20the%20value%0Aof%20the%20neural%20drone%20model%20by%20integrating%20it%20into%20a%20traditional%20filter-based%20VIO%0Asystem%20%28ROVIO%29%2C%20resulting%20in%20superior%20odometry%20accuracy%20on%20aggressive%203D%20racing%0Atrajectories%20near%20obstacles.%20Self-supervised%20learning%20of%20ego-motion%20estimation%0Arepresents%20a%20significant%20step%20toward%20bridging%20the%20gap%20between%20flying%20in%0Acontrolled%2C%20expensive%20lab%20environments%20and%20real-world%20drone%20applications.%20The%0Afusion%20of%20vision%20and%20drone%20models%20will%20enable%20higher-speed%20flight%20and%20improve%0Astate%20estimation%2C%20on%20any%20drone%20in%20any%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21695v1&entry.124074799=Read"},
{"title": "Learning Heterogeneous Performance-Fairness Trade-offs in Federated\n  Learning", "author": "Rongguang Ye and Ming Tang", "abstract": "  Recent methods leverage a hypernet to handle the performance-fairness\ntrade-offs in federated learning. This hypernet maps the clients' preferences\nbetween model performance and fairness to preference-specifc models on the\ntrade-off curve, known as local Pareto front. However, existing methods\ntypically adopt a uniform preference sampling distribution to train the\nhypernet across clients, neglecting the inherent heterogeneity of their local\nPareto fronts. Meanwhile, from the perspective of generalization, they do not\nconsider the gap between local and global Pareto fronts on the global dataset.\nTo address these limitations, we propose HetPFL to effectively learn both local\nand global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)\nand Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the\noptimal preference sampling distribution for each client to accommodate\nheterogeneous local Pareto fronts. While PHF performs preference-aware fusion\nof clients' hypernets to ensure the performance of the global Pareto front. We\nprove that HetPFL converges linearly with respect to the number of rounds,\nunder weaker assumptions than existing methods. Extensive experiments on four\ndatasets show that HetPFL significantly outperforms seven baselines in terms of\nthe quality of learned local and global Pareto fronts.\n", "link": "http://arxiv.org/abs/2504.21775v1", "date": "2025-04-30", "relevancy": 2.2917, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4725}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4583}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Heterogeneous%20Performance-Fairness%20Trade-offs%20in%20Federated%0A%20%20Learning&body=Title%3A%20Learning%20Heterogeneous%20Performance-Fairness%20Trade-offs%20in%20Federated%0A%20%20Learning%0AAuthor%3A%20Rongguang%20Ye%20and%20Ming%20Tang%0AAbstract%3A%20%20%20Recent%20methods%20leverage%20a%20hypernet%20to%20handle%20the%20performance-fairness%0Atrade-offs%20in%20federated%20learning.%20This%20hypernet%20maps%20the%20clients%27%20preferences%0Abetween%20model%20performance%20and%20fairness%20to%20preference-specifc%20models%20on%20the%0Atrade-off%20curve%2C%20known%20as%20local%20Pareto%20front.%20However%2C%20existing%20methods%0Atypically%20adopt%20a%20uniform%20preference%20sampling%20distribution%20to%20train%20the%0Ahypernet%20across%20clients%2C%20neglecting%20the%20inherent%20heterogeneity%20of%20their%20local%0APareto%20fronts.%20Meanwhile%2C%20from%20the%20perspective%20of%20generalization%2C%20they%20do%20not%0Aconsider%20the%20gap%20between%20local%20and%20global%20Pareto%20fronts%20on%20the%20global%20dataset.%0ATo%20address%20these%20limitations%2C%20we%20propose%20HetPFL%20to%20effectively%20learn%20both%20local%0Aand%20global%20Pareto%20fronts.%20HetPFL%20comprises%20Preference%20Sampling%20Adaptation%20%28PSA%29%0Aand%20Preference-aware%20Hypernet%20Fusion%20%28PHF%29.%20PSA%20adaptively%20determines%20the%0Aoptimal%20preference%20sampling%20distribution%20for%20each%20client%20to%20accommodate%0Aheterogeneous%20local%20Pareto%20fronts.%20While%20PHF%20performs%20preference-aware%20fusion%0Aof%20clients%27%20hypernets%20to%20ensure%20the%20performance%20of%20the%20global%20Pareto%20front.%20We%0Aprove%20that%20HetPFL%20converges%20linearly%20with%20respect%20to%20the%20number%20of%20rounds%2C%0Aunder%20weaker%20assumptions%20than%20existing%20methods.%20Extensive%20experiments%20on%20four%0Adatasets%20show%20that%20HetPFL%20significantly%20outperforms%20seven%20baselines%20in%20terms%20of%0Athe%20quality%20of%20learned%20local%20and%20global%20Pareto%20fronts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Heterogeneous%2520Performance-Fairness%2520Trade-offs%2520in%2520Federated%250A%2520%2520Learning%26entry.906535625%3DRongguang%2520Ye%2520and%2520Ming%2520Tang%26entry.1292438233%3D%2520%2520Recent%2520methods%2520leverage%2520a%2520hypernet%2520to%2520handle%2520the%2520performance-fairness%250Atrade-offs%2520in%2520federated%2520learning.%2520This%2520hypernet%2520maps%2520the%2520clients%2527%2520preferences%250Abetween%2520model%2520performance%2520and%2520fairness%2520to%2520preference-specifc%2520models%2520on%2520the%250Atrade-off%2520curve%252C%2520known%2520as%2520local%2520Pareto%2520front.%2520However%252C%2520existing%2520methods%250Atypically%2520adopt%2520a%2520uniform%2520preference%2520sampling%2520distribution%2520to%2520train%2520the%250Ahypernet%2520across%2520clients%252C%2520neglecting%2520the%2520inherent%2520heterogeneity%2520of%2520their%2520local%250APareto%2520fronts.%2520Meanwhile%252C%2520from%2520the%2520perspective%2520of%2520generalization%252C%2520they%2520do%2520not%250Aconsider%2520the%2520gap%2520between%2520local%2520and%2520global%2520Pareto%2520fronts%2520on%2520the%2520global%2520dataset.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520HetPFL%2520to%2520effectively%2520learn%2520both%2520local%250Aand%2520global%2520Pareto%2520fronts.%2520HetPFL%2520comprises%2520Preference%2520Sampling%2520Adaptation%2520%2528PSA%2529%250Aand%2520Preference-aware%2520Hypernet%2520Fusion%2520%2528PHF%2529.%2520PSA%2520adaptively%2520determines%2520the%250Aoptimal%2520preference%2520sampling%2520distribution%2520for%2520each%2520client%2520to%2520accommodate%250Aheterogeneous%2520local%2520Pareto%2520fronts.%2520While%2520PHF%2520performs%2520preference-aware%2520fusion%250Aof%2520clients%2527%2520hypernets%2520to%2520ensure%2520the%2520performance%2520of%2520the%2520global%2520Pareto%2520front.%2520We%250Aprove%2520that%2520HetPFL%2520converges%2520linearly%2520with%2520respect%2520to%2520the%2520number%2520of%2520rounds%252C%250Aunder%2520weaker%2520assumptions%2520than%2520existing%2520methods.%2520Extensive%2520experiments%2520on%2520four%250Adatasets%2520show%2520that%2520HetPFL%2520significantly%2520outperforms%2520seven%2520baselines%2520in%2520terms%2520of%250Athe%2520quality%2520of%2520learned%2520local%2520and%2520global%2520Pareto%2520fronts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Heterogeneous%20Performance-Fairness%20Trade-offs%20in%20Federated%0A%20%20Learning&entry.906535625=Rongguang%20Ye%20and%20Ming%20Tang&entry.1292438233=%20%20Recent%20methods%20leverage%20a%20hypernet%20to%20handle%20the%20performance-fairness%0Atrade-offs%20in%20federated%20learning.%20This%20hypernet%20maps%20the%20clients%27%20preferences%0Abetween%20model%20performance%20and%20fairness%20to%20preference-specifc%20models%20on%20the%0Atrade-off%20curve%2C%20known%20as%20local%20Pareto%20front.%20However%2C%20existing%20methods%0Atypically%20adopt%20a%20uniform%20preference%20sampling%20distribution%20to%20train%20the%0Ahypernet%20across%20clients%2C%20neglecting%20the%20inherent%20heterogeneity%20of%20their%20local%0APareto%20fronts.%20Meanwhile%2C%20from%20the%20perspective%20of%20generalization%2C%20they%20do%20not%0Aconsider%20the%20gap%20between%20local%20and%20global%20Pareto%20fronts%20on%20the%20global%20dataset.%0ATo%20address%20these%20limitations%2C%20we%20propose%20HetPFL%20to%20effectively%20learn%20both%20local%0Aand%20global%20Pareto%20fronts.%20HetPFL%20comprises%20Preference%20Sampling%20Adaptation%20%28PSA%29%0Aand%20Preference-aware%20Hypernet%20Fusion%20%28PHF%29.%20PSA%20adaptively%20determines%20the%0Aoptimal%20preference%20sampling%20distribution%20for%20each%20client%20to%20accommodate%0Aheterogeneous%20local%20Pareto%20fronts.%20While%20PHF%20performs%20preference-aware%20fusion%0Aof%20clients%27%20hypernets%20to%20ensure%20the%20performance%20of%20the%20global%20Pareto%20front.%20We%0Aprove%20that%20HetPFL%20converges%20linearly%20with%20respect%20to%20the%20number%20of%20rounds%2C%0Aunder%20weaker%20assumptions%20than%20existing%20methods.%20Extensive%20experiments%20on%20four%0Adatasets%20show%20that%20HetPFL%20significantly%20outperforms%20seven%20baselines%20in%20terms%20of%0Athe%20quality%20of%20learned%20local%20and%20global%20Pareto%20fronts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21775v1&entry.124074799=Read"},
{"title": "Multi-view Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition of Autonomous Vehicles", "author": "Younggun Kim and Beomsik Cho and Seonghoon Ryoo and Soomok Lee", "abstract": "  Point cloud representation has recently become a research hotspot in the\nfield of computer vision and has been utilized for autonomous vehicles.\nHowever, adapting deep learning networks for point cloud data recognition is\nchallenging due to the variability in datasets and sensor technologies. This\nvariability underscores the necessity for adaptive techniques to maintain\naccuracy under different conditions. In this paper, we present the Multi-View\nStructural Convolution Network (MSCN) designed for domain-invariant point cloud\nrecognition. MSCN comprises Structural Convolution Layers (SCL) that extract\nlocal context geometric features from point clouds and Structural Aggregation\nLayers (SAL) that extract and aggregate both local and overall context features\nfrom point clouds. Additionally, our MSCN enhances feature representation\nrobustness by training with unseen domain point clouds derived from source\ndomain point clouds. This method acquires domain-invariant features and\nexhibits robust, consistent performance across various point cloud datasets,\nensuring compatibility with diverse sensor configurations without the need for\nparameter adjustments. This highlights MSCN's potential to significantly\nimprove the reliability and domain invariant features in different\nenvironments. Our code is available at https://github.com/MLMLab/MSCN.\n", "link": "http://arxiv.org/abs/2501.16289v3", "date": "2025-04-30", "relevancy": 2.2873, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.585}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5683}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&body=Title%3A%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles%0AAuthor%3A%20Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee%0AAbstract%3A%20%20%20Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%0Afield%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%0AHowever%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%0Achallenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%0Avariability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%0AStructural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%0Arecognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%0Alocal%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%0ALayers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%0Afrom%20point%20clouds.%20Additionally%2C%20our%20MSCN%20enhances%20feature%20representation%0Arobustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20derived%20from%20source%0Adomain%20point%20clouds.%20This%20method%20acquires%20domain-invariant%20features%20and%0Aexhibits%20robust%2C%20consistent%20performance%20across%20various%20point%20cloud%20datasets%2C%0Aensuring%20compatibility%20with%20diverse%20sensor%20configurations%20without%20the%20need%20for%0Aparameter%20adjustments.%20This%20highlights%20MSCN%27s%20potential%20to%20significantly%0Aimprove%20the%20reliability%20and%20domain%20invariant%20features%20in%20different%0Aenvironments.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16289v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Structural%2520Convolution%2520Network%2520for%2520Domain-Invariant%2520Point%250A%2520%2520Cloud%2520Recognition%2520of%2520Autonomous%2520Vehicles%26entry.906535625%3DYounggun%2520Kim%2520and%2520Beomsik%2520Cho%2520and%2520Seonghoon%2520Ryoo%2520and%2520Soomok%2520Lee%26entry.1292438233%3D%2520%2520Point%2520cloud%2520representation%2520has%2520recently%2520become%2520a%2520research%2520hotspot%2520in%2520the%250Afield%2520of%2520computer%2520vision%2520and%2520has%2520been%2520utilized%2520for%2520autonomous%2520vehicles.%250AHowever%252C%2520adapting%2520deep%2520learning%2520networks%2520for%2520point%2520cloud%2520data%2520recognition%2520is%250Achallenging%2520due%2520to%2520the%2520variability%2520in%2520datasets%2520and%2520sensor%2520technologies.%2520This%250Avariability%2520underscores%2520the%2520necessity%2520for%2520adaptive%2520techniques%2520to%2520maintain%250Aaccuracy%2520under%2520different%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Multi-View%250AStructural%2520Convolution%2520Network%2520%2528MSCN%2529%2520designed%2520for%2520domain-invariant%2520point%2520cloud%250Arecognition.%2520MSCN%2520comprises%2520Structural%2520Convolution%2520Layers%2520%2528SCL%2529%2520that%2520extract%250Alocal%2520context%2520geometric%2520features%2520from%2520point%2520clouds%2520and%2520Structural%2520Aggregation%250ALayers%2520%2528SAL%2529%2520that%2520extract%2520and%2520aggregate%2520both%2520local%2520and%2520overall%2520context%2520features%250Afrom%2520point%2520clouds.%2520Additionally%252C%2520our%2520MSCN%2520enhances%2520feature%2520representation%250Arobustness%2520by%2520training%2520with%2520unseen%2520domain%2520point%2520clouds%2520derived%2520from%2520source%250Adomain%2520point%2520clouds.%2520This%2520method%2520acquires%2520domain-invariant%2520features%2520and%250Aexhibits%2520robust%252C%2520consistent%2520performance%2520across%2520various%2520point%2520cloud%2520datasets%252C%250Aensuring%2520compatibility%2520with%2520diverse%2520sensor%2520configurations%2520without%2520the%2520need%2520for%250Aparameter%2520adjustments.%2520This%2520highlights%2520MSCN%2527s%2520potential%2520to%2520significantly%250Aimprove%2520the%2520reliability%2520and%2520domain%2520invariant%2520features%2520in%2520different%250Aenvironments.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/MLMLab/MSCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16289v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&entry.906535625=Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee&entry.1292438233=%20%20Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%0Afield%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%0AHowever%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%0Achallenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%0Avariability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%0AStructural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%0Arecognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%0Alocal%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%0ALayers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%0Afrom%20point%20clouds.%20Additionally%2C%20our%20MSCN%20enhances%20feature%20representation%0Arobustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20derived%20from%20source%0Adomain%20point%20clouds.%20This%20method%20acquires%20domain-invariant%20features%20and%0Aexhibits%20robust%2C%20consistent%20performance%20across%20various%20point%20cloud%20datasets%2C%0Aensuring%20compatibility%20with%20diverse%20sensor%20configurations%20without%20the%20need%20for%0Aparameter%20adjustments.%20This%20highlights%20MSCN%27s%20potential%20to%20significantly%0Aimprove%20the%20reliability%20and%20domain%20invariant%20features%20in%20different%0Aenvironments.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16289v3&entry.124074799=Read"},
{"title": "LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end\n  Learning", "author": "Yiyang Shao and Xiaoyu Huang and Bike Zhang and Qiayuan Liao and Yuman Gao and Yufeng Chi and Zhongyu Li and Sophia Shao and Koushil Sreenath", "abstract": "  General-purpose humanoid robots are expected to interact intuitively with\nhumans, enabling seamless integration into daily life. Natural language\nprovides the most accessible medium for this purpose. However, translating\nlanguage into humanoid whole-body motion remains a significant challenge,\nprimarily due to the gap between linguistic understanding and physical actions.\nIn this work, we present an end-to-end, language-directed policy for real-world\nhumanoid whole-body control. Our approach combines reinforcement learning with\npolicy distillation, allowing a single neural network to interpret language\ncommands and execute corresponding physical actions directly. To enhance motion\ndiversity and compositionality, we incorporate a Conditional Variational\nAutoencoder (CVAE) structure. The resulting policy achieves agile and versatile\nwhole-body behaviors conditioned on language inputs, with smooth transitions\nbetween various motions, enabling adaptation to linguistic variations and the\nemergence of novel motions. We validate the efficacy and generalizability of\nour method through extensive simulations and real-world experiments,\ndemonstrating robust whole-body control. Please see our website at\nLangWBC.github.io for more information.\n", "link": "http://arxiv.org/abs/2504.21738v1", "date": "2025-04-30", "relevancy": 2.2792, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5877}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5745}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangWBC%3A%20Language-directed%20Humanoid%20Whole-Body%20Control%20via%20End-to-end%0A%20%20Learning&body=Title%3A%20LangWBC%3A%20Language-directed%20Humanoid%20Whole-Body%20Control%20via%20End-to-end%0A%20%20Learning%0AAuthor%3A%20Yiyang%20Shao%20and%20Xiaoyu%20Huang%20and%20Bike%20Zhang%20and%20Qiayuan%20Liao%20and%20Yuman%20Gao%20and%20Yufeng%20Chi%20and%20Zhongyu%20Li%20and%20Sophia%20Shao%20and%20Koushil%20Sreenath%0AAbstract%3A%20%20%20General-purpose%20humanoid%20robots%20are%20expected%20to%20interact%20intuitively%20with%0Ahumans%2C%20enabling%20seamless%20integration%20into%20daily%20life.%20Natural%20language%0Aprovides%20the%20most%20accessible%20medium%20for%20this%20purpose.%20However%2C%20translating%0Alanguage%20into%20humanoid%20whole-body%20motion%20remains%20a%20significant%20challenge%2C%0Aprimarily%20due%20to%20the%20gap%20between%20linguistic%20understanding%20and%20physical%20actions.%0AIn%20this%20work%2C%20we%20present%20an%20end-to-end%2C%20language-directed%20policy%20for%20real-world%0Ahumanoid%20whole-body%20control.%20Our%20approach%20combines%20reinforcement%20learning%20with%0Apolicy%20distillation%2C%20allowing%20a%20single%20neural%20network%20to%20interpret%20language%0Acommands%20and%20execute%20corresponding%20physical%20actions%20directly.%20To%20enhance%20motion%0Adiversity%20and%20compositionality%2C%20we%20incorporate%20a%20Conditional%20Variational%0AAutoencoder%20%28CVAE%29%20structure.%20The%20resulting%20policy%20achieves%20agile%20and%20versatile%0Awhole-body%20behaviors%20conditioned%20on%20language%20inputs%2C%20with%20smooth%20transitions%0Abetween%20various%20motions%2C%20enabling%20adaptation%20to%20linguistic%20variations%20and%20the%0Aemergence%20of%20novel%20motions.%20We%20validate%20the%20efficacy%20and%20generalizability%20of%0Aour%20method%20through%20extensive%20simulations%20and%20real-world%20experiments%2C%0Ademonstrating%20robust%20whole-body%20control.%20Please%20see%20our%20website%20at%0ALangWBC.github.io%20for%20more%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangWBC%253A%2520Language-directed%2520Humanoid%2520Whole-Body%2520Control%2520via%2520End-to-end%250A%2520%2520Learning%26entry.906535625%3DYiyang%2520Shao%2520and%2520Xiaoyu%2520Huang%2520and%2520Bike%2520Zhang%2520and%2520Qiayuan%2520Liao%2520and%2520Yuman%2520Gao%2520and%2520Yufeng%2520Chi%2520and%2520Zhongyu%2520Li%2520and%2520Sophia%2520Shao%2520and%2520Koushil%2520Sreenath%26entry.1292438233%3D%2520%2520General-purpose%2520humanoid%2520robots%2520are%2520expected%2520to%2520interact%2520intuitively%2520with%250Ahumans%252C%2520enabling%2520seamless%2520integration%2520into%2520daily%2520life.%2520Natural%2520language%250Aprovides%2520the%2520most%2520accessible%2520medium%2520for%2520this%2520purpose.%2520However%252C%2520translating%250Alanguage%2520into%2520humanoid%2520whole-body%2520motion%2520remains%2520a%2520significant%2520challenge%252C%250Aprimarily%2520due%2520to%2520the%2520gap%2520between%2520linguistic%2520understanding%2520and%2520physical%2520actions.%250AIn%2520this%2520work%252C%2520we%2520present%2520an%2520end-to-end%252C%2520language-directed%2520policy%2520for%2520real-world%250Ahumanoid%2520whole-body%2520control.%2520Our%2520approach%2520combines%2520reinforcement%2520learning%2520with%250Apolicy%2520distillation%252C%2520allowing%2520a%2520single%2520neural%2520network%2520to%2520interpret%2520language%250Acommands%2520and%2520execute%2520corresponding%2520physical%2520actions%2520directly.%2520To%2520enhance%2520motion%250Adiversity%2520and%2520compositionality%252C%2520we%2520incorporate%2520a%2520Conditional%2520Variational%250AAutoencoder%2520%2528CVAE%2529%2520structure.%2520The%2520resulting%2520policy%2520achieves%2520agile%2520and%2520versatile%250Awhole-body%2520behaviors%2520conditioned%2520on%2520language%2520inputs%252C%2520with%2520smooth%2520transitions%250Abetween%2520various%2520motions%252C%2520enabling%2520adaptation%2520to%2520linguistic%2520variations%2520and%2520the%250Aemergence%2520of%2520novel%2520motions.%2520We%2520validate%2520the%2520efficacy%2520and%2520generalizability%2520of%250Aour%2520method%2520through%2520extensive%2520simulations%2520and%2520real-world%2520experiments%252C%250Ademonstrating%2520robust%2520whole-body%2520control.%2520Please%2520see%2520our%2520website%2520at%250ALangWBC.github.io%2520for%2520more%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangWBC%3A%20Language-directed%20Humanoid%20Whole-Body%20Control%20via%20End-to-end%0A%20%20Learning&entry.906535625=Yiyang%20Shao%20and%20Xiaoyu%20Huang%20and%20Bike%20Zhang%20and%20Qiayuan%20Liao%20and%20Yuman%20Gao%20and%20Yufeng%20Chi%20and%20Zhongyu%20Li%20and%20Sophia%20Shao%20and%20Koushil%20Sreenath&entry.1292438233=%20%20General-purpose%20humanoid%20robots%20are%20expected%20to%20interact%20intuitively%20with%0Ahumans%2C%20enabling%20seamless%20integration%20into%20daily%20life.%20Natural%20language%0Aprovides%20the%20most%20accessible%20medium%20for%20this%20purpose.%20However%2C%20translating%0Alanguage%20into%20humanoid%20whole-body%20motion%20remains%20a%20significant%20challenge%2C%0Aprimarily%20due%20to%20the%20gap%20between%20linguistic%20understanding%20and%20physical%20actions.%0AIn%20this%20work%2C%20we%20present%20an%20end-to-end%2C%20language-directed%20policy%20for%20real-world%0Ahumanoid%20whole-body%20control.%20Our%20approach%20combines%20reinforcement%20learning%20with%0Apolicy%20distillation%2C%20allowing%20a%20single%20neural%20network%20to%20interpret%20language%0Acommands%20and%20execute%20corresponding%20physical%20actions%20directly.%20To%20enhance%20motion%0Adiversity%20and%20compositionality%2C%20we%20incorporate%20a%20Conditional%20Variational%0AAutoencoder%20%28CVAE%29%20structure.%20The%20resulting%20policy%20achieves%20agile%20and%20versatile%0Awhole-body%20behaviors%20conditioned%20on%20language%20inputs%2C%20with%20smooth%20transitions%0Abetween%20various%20motions%2C%20enabling%20adaptation%20to%20linguistic%20variations%20and%20the%0Aemergence%20of%20novel%20motions.%20We%20validate%20the%20efficacy%20and%20generalizability%20of%0Aour%20method%20through%20extensive%20simulations%20and%20real-world%20experiments%2C%0Ademonstrating%20robust%20whole-body%20control.%20Please%20see%20our%20website%20at%0ALangWBC.github.io%20for%20more%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21738v1&entry.124074799=Read"},
{"title": "LRBO2: Improved 3D Vision Based Hand-Eye Calibration for Collaborative\n  Robot Arm", "author": "Leihui Li and Lixuepiao Wan and Volker Krueger and Xuping Zhang", "abstract": "  Hand-eye calibration is a common problem in the field of collaborative\nrobotics, involving the determination of the transformation matrix between the\nvisual sensor and the robot flange to enable vision-based robotic tasks.\nHowever, this process typically requires multiple movements of the robot arm\nand an external calibration object, making it both time-consuming and\ninconvenient, especially in scenarios where frequent recalibration is\nnecessary. In this work, we extend our previous method, Look at Robot Base Once\n(LRBO), which eliminates the need for external calibration objects such as a\nchessboard. We propose a generic dataset generation approach for point cloud\nregistration, focusing on aligning the robot base point cloud with the scanned\ndata. Furthermore, a more detailed simulation study is conducted involving\nseveral different collaborative robot arms, followed by real-world experiments\nin an industrial setting. Our improved method is simulated and evaluated using\na total of 14 robotic arms from 9 different brands, including KUKA, Universal\nRobots, UFACTORY, and Franka Emika, all of which are widely used in the field\nof collaborative robotics. Physical experiments demonstrate that our extended\napproach achieves performance comparable to existing commercial hand-eye\ncalibration solutions, while completing the entire calibration procedure in\njust a few seconds. In addition, we provide a user-friendly hand-eye\ncalibration solution, with the code publicly available at\ngithub.com/leihui6/LRBO2.\n", "link": "http://arxiv.org/abs/2504.21619v1", "date": "2025-04-30", "relevancy": 2.2638, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5801}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5583}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LRBO2%3A%20Improved%203D%20Vision%20Based%20Hand-Eye%20Calibration%20for%20Collaborative%0A%20%20Robot%20Arm&body=Title%3A%20LRBO2%3A%20Improved%203D%20Vision%20Based%20Hand-Eye%20Calibration%20for%20Collaborative%0A%20%20Robot%20Arm%0AAuthor%3A%20Leihui%20Li%20and%20Lixuepiao%20Wan%20and%20Volker%20Krueger%20and%20Xuping%20Zhang%0AAbstract%3A%20%20%20Hand-eye%20calibration%20is%20a%20common%20problem%20in%20the%20field%20of%20collaborative%0Arobotics%2C%20involving%20the%20determination%20of%20the%20transformation%20matrix%20between%20the%0Avisual%20sensor%20and%20the%20robot%20flange%20to%20enable%20vision-based%20robotic%20tasks.%0AHowever%2C%20this%20process%20typically%20requires%20multiple%20movements%20of%20the%20robot%20arm%0Aand%20an%20external%20calibration%20object%2C%20making%20it%20both%20time-consuming%20and%0Ainconvenient%2C%20especially%20in%20scenarios%20where%20frequent%20recalibration%20is%0Anecessary.%20In%20this%20work%2C%20we%20extend%20our%20previous%20method%2C%20Look%20at%20Robot%20Base%20Once%0A%28LRBO%29%2C%20which%20eliminates%20the%20need%20for%20external%20calibration%20objects%20such%20as%20a%0Achessboard.%20We%20propose%20a%20generic%20dataset%20generation%20approach%20for%20point%20cloud%0Aregistration%2C%20focusing%20on%20aligning%20the%20robot%20base%20point%20cloud%20with%20the%20scanned%0Adata.%20Furthermore%2C%20a%20more%20detailed%20simulation%20study%20is%20conducted%20involving%0Aseveral%20different%20collaborative%20robot%20arms%2C%20followed%20by%20real-world%20experiments%0Ain%20an%20industrial%20setting.%20Our%20improved%20method%20is%20simulated%20and%20evaluated%20using%0Aa%20total%20of%2014%20robotic%20arms%20from%209%20different%20brands%2C%20including%20KUKA%2C%20Universal%0ARobots%2C%20UFACTORY%2C%20and%20Franka%20Emika%2C%20all%20of%20which%20are%20widely%20used%20in%20the%20field%0Aof%20collaborative%20robotics.%20Physical%20experiments%20demonstrate%20that%20our%20extended%0Aapproach%20achieves%20performance%20comparable%20to%20existing%20commercial%20hand-eye%0Acalibration%20solutions%2C%20while%20completing%20the%20entire%20calibration%20procedure%20in%0Ajust%20a%20few%20seconds.%20In%20addition%2C%20we%20provide%20a%20user-friendly%20hand-eye%0Acalibration%20solution%2C%20with%20the%20code%20publicly%20available%20at%0Agithub.com/leihui6/LRBO2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLRBO2%253A%2520Improved%25203D%2520Vision%2520Based%2520Hand-Eye%2520Calibration%2520for%2520Collaborative%250A%2520%2520Robot%2520Arm%26entry.906535625%3DLeihui%2520Li%2520and%2520Lixuepiao%2520Wan%2520and%2520Volker%2520Krueger%2520and%2520Xuping%2520Zhang%26entry.1292438233%3D%2520%2520Hand-eye%2520calibration%2520is%2520a%2520common%2520problem%2520in%2520the%2520field%2520of%2520collaborative%250Arobotics%252C%2520involving%2520the%2520determination%2520of%2520the%2520transformation%2520matrix%2520between%2520the%250Avisual%2520sensor%2520and%2520the%2520robot%2520flange%2520to%2520enable%2520vision-based%2520robotic%2520tasks.%250AHowever%252C%2520this%2520process%2520typically%2520requires%2520multiple%2520movements%2520of%2520the%2520robot%2520arm%250Aand%2520an%2520external%2520calibration%2520object%252C%2520making%2520it%2520both%2520time-consuming%2520and%250Ainconvenient%252C%2520especially%2520in%2520scenarios%2520where%2520frequent%2520recalibration%2520is%250Anecessary.%2520In%2520this%2520work%252C%2520we%2520extend%2520our%2520previous%2520method%252C%2520Look%2520at%2520Robot%2520Base%2520Once%250A%2528LRBO%2529%252C%2520which%2520eliminates%2520the%2520need%2520for%2520external%2520calibration%2520objects%2520such%2520as%2520a%250Achessboard.%2520We%2520propose%2520a%2520generic%2520dataset%2520generation%2520approach%2520for%2520point%2520cloud%250Aregistration%252C%2520focusing%2520on%2520aligning%2520the%2520robot%2520base%2520point%2520cloud%2520with%2520the%2520scanned%250Adata.%2520Furthermore%252C%2520a%2520more%2520detailed%2520simulation%2520study%2520is%2520conducted%2520involving%250Aseveral%2520different%2520collaborative%2520robot%2520arms%252C%2520followed%2520by%2520real-world%2520experiments%250Ain%2520an%2520industrial%2520setting.%2520Our%2520improved%2520method%2520is%2520simulated%2520and%2520evaluated%2520using%250Aa%2520total%2520of%252014%2520robotic%2520arms%2520from%25209%2520different%2520brands%252C%2520including%2520KUKA%252C%2520Universal%250ARobots%252C%2520UFACTORY%252C%2520and%2520Franka%2520Emika%252C%2520all%2520of%2520which%2520are%2520widely%2520used%2520in%2520the%2520field%250Aof%2520collaborative%2520robotics.%2520Physical%2520experiments%2520demonstrate%2520that%2520our%2520extended%250Aapproach%2520achieves%2520performance%2520comparable%2520to%2520existing%2520commercial%2520hand-eye%250Acalibration%2520solutions%252C%2520while%2520completing%2520the%2520entire%2520calibration%2520procedure%2520in%250Ajust%2520a%2520few%2520seconds.%2520In%2520addition%252C%2520we%2520provide%2520a%2520user-friendly%2520hand-eye%250Acalibration%2520solution%252C%2520with%2520the%2520code%2520publicly%2520available%2520at%250Agithub.com/leihui6/LRBO2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LRBO2%3A%20Improved%203D%20Vision%20Based%20Hand-Eye%20Calibration%20for%20Collaborative%0A%20%20Robot%20Arm&entry.906535625=Leihui%20Li%20and%20Lixuepiao%20Wan%20and%20Volker%20Krueger%20and%20Xuping%20Zhang&entry.1292438233=%20%20Hand-eye%20calibration%20is%20a%20common%20problem%20in%20the%20field%20of%20collaborative%0Arobotics%2C%20involving%20the%20determination%20of%20the%20transformation%20matrix%20between%20the%0Avisual%20sensor%20and%20the%20robot%20flange%20to%20enable%20vision-based%20robotic%20tasks.%0AHowever%2C%20this%20process%20typically%20requires%20multiple%20movements%20of%20the%20robot%20arm%0Aand%20an%20external%20calibration%20object%2C%20making%20it%20both%20time-consuming%20and%0Ainconvenient%2C%20especially%20in%20scenarios%20where%20frequent%20recalibration%20is%0Anecessary.%20In%20this%20work%2C%20we%20extend%20our%20previous%20method%2C%20Look%20at%20Robot%20Base%20Once%0A%28LRBO%29%2C%20which%20eliminates%20the%20need%20for%20external%20calibration%20objects%20such%20as%20a%0Achessboard.%20We%20propose%20a%20generic%20dataset%20generation%20approach%20for%20point%20cloud%0Aregistration%2C%20focusing%20on%20aligning%20the%20robot%20base%20point%20cloud%20with%20the%20scanned%0Adata.%20Furthermore%2C%20a%20more%20detailed%20simulation%20study%20is%20conducted%20involving%0Aseveral%20different%20collaborative%20robot%20arms%2C%20followed%20by%20real-world%20experiments%0Ain%20an%20industrial%20setting.%20Our%20improved%20method%20is%20simulated%20and%20evaluated%20using%0Aa%20total%20of%2014%20robotic%20arms%20from%209%20different%20brands%2C%20including%20KUKA%2C%20Universal%0ARobots%2C%20UFACTORY%2C%20and%20Franka%20Emika%2C%20all%20of%20which%20are%20widely%20used%20in%20the%20field%0Aof%20collaborative%20robotics.%20Physical%20experiments%20demonstrate%20that%20our%20extended%0Aapproach%20achieves%20performance%20comparable%20to%20existing%20commercial%20hand-eye%0Acalibration%20solutions%2C%20while%20completing%20the%20entire%20calibration%20procedure%20in%0Ajust%20a%20few%20seconds.%20In%20addition%2C%20we%20provide%20a%20user-friendly%20hand-eye%0Acalibration%20solution%2C%20with%20the%20code%20publicly%20available%20at%0Agithub.com/leihui6/LRBO2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21619v1&entry.124074799=Read"},
{"title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination\n  in Large Vision Language Models", "author": "Sangmin Woo and Kang Zhou and Yun Zhou and Shuai Wang and Sheng Guan and Haibo Ding and Lin Lee Cheong", "abstract": "  Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination.\n", "link": "http://arxiv.org/abs/2504.21559v1", "date": "2025-04-30", "relevancy": 2.2523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Black-Box%20Visual%20Prompt%20Engineering%20for%20Mitigating%20Object%20Hallucination%0A%20%20in%20Large%20Vision%20Language%20Models&body=Title%3A%20Black-Box%20Visual%20Prompt%20Engineering%20for%20Mitigating%20Object%20Hallucination%0A%20%20in%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Sangmin%20Woo%20and%20Kang%20Zhou%20and%20Yun%20Zhou%20and%20Shuai%20Wang%20and%20Sheng%20Guan%20and%20Haibo%20Ding%20and%20Lin%20Lee%20Cheong%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%0Awhich%20undermines%20their%20reliability.%20Surprisingly%2C%20we%20find%20that%20simple%0Aobject-based%20visual%20prompting%20--%20overlaying%20visual%20cues%20%28e.g.%2C%20bounding%20box%2C%0Acircle%29%20on%20images%20--%20can%20significantly%20mitigate%20such%20hallucination%3B%20however%2C%0Adifferent%20visual%20prompts%20%28VPs%29%20vary%20in%20effectiveness.%20To%20address%20this%2C%20we%0Apropose%20Black-Box%20Visual%20Prompt%20Engineering%20%28BBVPE%29%2C%20a%20framework%20to%20identify%0Aoptimal%20VPs%20that%20enhance%20LVLM%20responses%20without%20needing%20access%20to%20model%0Ainternals.%20Our%20approach%20employs%20a%20pool%20of%20candidate%20VPs%20and%20trains%20a%20router%0Amodel%20to%20dynamically%20select%20the%20most%20effective%20VP%20for%20a%20given%20input%20image.%20This%0Ablack-box%20approach%20is%20model-agnostic%2C%20making%20it%20applicable%20to%20both%20open-source%0Aand%20proprietary%20LVLMs.%20Evaluations%20on%20benchmarks%20such%20as%20POPE%20and%20CHAIR%0Ademonstrate%20that%20BBVPE%20effectively%20reduces%20object%20hallucination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlack-Box%2520Visual%2520Prompt%2520Engineering%2520for%2520Mitigating%2520Object%2520Hallucination%250A%2520%2520in%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DSangmin%2520Woo%2520and%2520Kang%2520Zhou%2520and%2520Yun%2520Zhou%2520and%2520Shuai%2520Wang%2520and%2520Sheng%2520Guan%2520and%2520Haibo%2520Ding%2520and%2520Lin%2520Lee%2520Cheong%26entry.1292438233%3D%2520%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520often%2520suffer%2520from%2520object%2520hallucination%252C%250Awhich%2520undermines%2520their%2520reliability.%2520Surprisingly%252C%2520we%2520find%2520that%2520simple%250Aobject-based%2520visual%2520prompting%2520--%2520overlaying%2520visual%2520cues%2520%2528e.g.%252C%2520bounding%2520box%252C%250Acircle%2529%2520on%2520images%2520--%2520can%2520significantly%2520mitigate%2520such%2520hallucination%253B%2520however%252C%250Adifferent%2520visual%2520prompts%2520%2528VPs%2529%2520vary%2520in%2520effectiveness.%2520To%2520address%2520this%252C%2520we%250Apropose%2520Black-Box%2520Visual%2520Prompt%2520Engineering%2520%2528BBVPE%2529%252C%2520a%2520framework%2520to%2520identify%250Aoptimal%2520VPs%2520that%2520enhance%2520LVLM%2520responses%2520without%2520needing%2520access%2520to%2520model%250Ainternals.%2520Our%2520approach%2520employs%2520a%2520pool%2520of%2520candidate%2520VPs%2520and%2520trains%2520a%2520router%250Amodel%2520to%2520dynamically%2520select%2520the%2520most%2520effective%2520VP%2520for%2520a%2520given%2520input%2520image.%2520This%250Ablack-box%2520approach%2520is%2520model-agnostic%252C%2520making%2520it%2520applicable%2520to%2520both%2520open-source%250Aand%2520proprietary%2520LVLMs.%2520Evaluations%2520on%2520benchmarks%2520such%2520as%2520POPE%2520and%2520CHAIR%250Ademonstrate%2520that%2520BBVPE%2520effectively%2520reduces%2520object%2520hallucination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Black-Box%20Visual%20Prompt%20Engineering%20for%20Mitigating%20Object%20Hallucination%0A%20%20in%20Large%20Vision%20Language%20Models&entry.906535625=Sangmin%20Woo%20and%20Kang%20Zhou%20and%20Yun%20Zhou%20and%20Shuai%20Wang%20and%20Sheng%20Guan%20and%20Haibo%20Ding%20and%20Lin%20Lee%20Cheong&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%0Awhich%20undermines%20their%20reliability.%20Surprisingly%2C%20we%20find%20that%20simple%0Aobject-based%20visual%20prompting%20--%20overlaying%20visual%20cues%20%28e.g.%2C%20bounding%20box%2C%0Acircle%29%20on%20images%20--%20can%20significantly%20mitigate%20such%20hallucination%3B%20however%2C%0Adifferent%20visual%20prompts%20%28VPs%29%20vary%20in%20effectiveness.%20To%20address%20this%2C%20we%0Apropose%20Black-Box%20Visual%20Prompt%20Engineering%20%28BBVPE%29%2C%20a%20framework%20to%20identify%0Aoptimal%20VPs%20that%20enhance%20LVLM%20responses%20without%20needing%20access%20to%20model%0Ainternals.%20Our%20approach%20employs%20a%20pool%20of%20candidate%20VPs%20and%20trains%20a%20router%0Amodel%20to%20dynamically%20select%20the%20most%20effective%20VP%20for%20a%20given%20input%20image.%20This%0Ablack-box%20approach%20is%20model-agnostic%2C%20making%20it%20applicable%20to%20both%20open-source%0Aand%20proprietary%20LVLMs.%20Evaluations%20on%20benchmarks%20such%20as%20POPE%20and%20CHAIR%0Ademonstrate%20that%20BBVPE%20effectively%20reduces%20object%20hallucination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21559v1&entry.124074799=Read"},
{"title": "Is Intermediate Fusion All You Need for UAV-based Collaborative\n  Perception?", "author": "Jiuwu Hao and Liguo Sun and Yuting Wan and Yueyang Wu and Ti Xiang and Haolin Song and Pin Lv", "abstract": "  Collaborative perception enhances environmental awareness through inter-agent\ncommunication and is regarded as a promising solution to intelligent\ntransportation systems. However, existing collaborative methods for Unmanned\nAerial Vehicles (UAVs) overlook the unique characteristics of the UAV\nperspective, resulting in substantial communication overhead. To address this\nissue, we propose a novel communication-efficient collaborative perception\nframework based on late-intermediate fusion, dubbed LIF. The core concept is to\nexchange informative and compact detection results and shift the fusion stage\nto the feature representation level. In particular, we leverage vision-guided\npositional embedding (VPE) and box-based virtual augmented feature (BoBEV) to\neffectively integrate complementary information from various agents.\nAdditionally, we innovatively introduce an uncertainty-driven communication\nmechanism that uses uncertainty evaluation to select high-quality and reliable\nshared areas. Experimental results demonstrate that our LIF achieves superior\nperformance with minimal communication bandwidth, proving its effectiveness and\npracticality. Code and models are available at https://github.com/uestchjw/LIF.\n", "link": "http://arxiv.org/abs/2504.21774v1", "date": "2025-04-30", "relevancy": 2.2465, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Intermediate%20Fusion%20All%20You%20Need%20for%20UAV-based%20Collaborative%0A%20%20Perception%3F&body=Title%3A%20Is%20Intermediate%20Fusion%20All%20You%20Need%20for%20UAV-based%20Collaborative%0A%20%20Perception%3F%0AAuthor%3A%20Jiuwu%20Hao%20and%20Liguo%20Sun%20and%20Yuting%20Wan%20and%20Yueyang%20Wu%20and%20Ti%20Xiang%20and%20Haolin%20Song%20and%20Pin%20Lv%0AAbstract%3A%20%20%20Collaborative%20perception%20enhances%20environmental%20awareness%20through%20inter-agent%0Acommunication%20and%20is%20regarded%20as%20a%20promising%20solution%20to%20intelligent%0Atransportation%20systems.%20However%2C%20existing%20collaborative%20methods%20for%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20overlook%20the%20unique%20characteristics%20of%20the%20UAV%0Aperspective%2C%20resulting%20in%20substantial%20communication%20overhead.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20communication-efficient%20collaborative%20perception%0Aframework%20based%20on%20late-intermediate%20fusion%2C%20dubbed%20LIF.%20The%20core%20concept%20is%20to%0Aexchange%20informative%20and%20compact%20detection%20results%20and%20shift%20the%20fusion%20stage%0Ato%20the%20feature%20representation%20level.%20In%20particular%2C%20we%20leverage%20vision-guided%0Apositional%20embedding%20%28VPE%29%20and%20box-based%20virtual%20augmented%20feature%20%28BoBEV%29%20to%0Aeffectively%20integrate%20complementary%20information%20from%20various%20agents.%0AAdditionally%2C%20we%20innovatively%20introduce%20an%20uncertainty-driven%20communication%0Amechanism%20that%20uses%20uncertainty%20evaluation%20to%20select%20high-quality%20and%20reliable%0Ashared%20areas.%20Experimental%20results%20demonstrate%20that%20our%20LIF%20achieves%20superior%0Aperformance%20with%20minimal%20communication%20bandwidth%2C%20proving%20its%20effectiveness%20and%0Apracticality.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/uestchjw/LIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Intermediate%2520Fusion%2520All%2520You%2520Need%2520for%2520UAV-based%2520Collaborative%250A%2520%2520Perception%253F%26entry.906535625%3DJiuwu%2520Hao%2520and%2520Liguo%2520Sun%2520and%2520Yuting%2520Wan%2520and%2520Yueyang%2520Wu%2520and%2520Ti%2520Xiang%2520and%2520Haolin%2520Song%2520and%2520Pin%2520Lv%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520enhances%2520environmental%2520awareness%2520through%2520inter-agent%250Acommunication%2520and%2520is%2520regarded%2520as%2520a%2520promising%2520solution%2520to%2520intelligent%250Atransportation%2520systems.%2520However%252C%2520existing%2520collaborative%2520methods%2520for%2520Unmanned%250AAerial%2520Vehicles%2520%2528UAVs%2529%2520overlook%2520the%2520unique%2520characteristics%2520of%2520the%2520UAV%250Aperspective%252C%2520resulting%2520in%2520substantial%2520communication%2520overhead.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520novel%2520communication-efficient%2520collaborative%2520perception%250Aframework%2520based%2520on%2520late-intermediate%2520fusion%252C%2520dubbed%2520LIF.%2520The%2520core%2520concept%2520is%2520to%250Aexchange%2520informative%2520and%2520compact%2520detection%2520results%2520and%2520shift%2520the%2520fusion%2520stage%250Ato%2520the%2520feature%2520representation%2520level.%2520In%2520particular%252C%2520we%2520leverage%2520vision-guided%250Apositional%2520embedding%2520%2528VPE%2529%2520and%2520box-based%2520virtual%2520augmented%2520feature%2520%2528BoBEV%2529%2520to%250Aeffectively%2520integrate%2520complementary%2520information%2520from%2520various%2520agents.%250AAdditionally%252C%2520we%2520innovatively%2520introduce%2520an%2520uncertainty-driven%2520communication%250Amechanism%2520that%2520uses%2520uncertainty%2520evaluation%2520to%2520select%2520high-quality%2520and%2520reliable%250Ashared%2520areas.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520LIF%2520achieves%2520superior%250Aperformance%2520with%2520minimal%2520communication%2520bandwidth%252C%2520proving%2520its%2520effectiveness%2520and%250Apracticality.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/uestchjw/LIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Intermediate%20Fusion%20All%20You%20Need%20for%20UAV-based%20Collaborative%0A%20%20Perception%3F&entry.906535625=Jiuwu%20Hao%20and%20Liguo%20Sun%20and%20Yuting%20Wan%20and%20Yueyang%20Wu%20and%20Ti%20Xiang%20and%20Haolin%20Song%20and%20Pin%20Lv&entry.1292438233=%20%20Collaborative%20perception%20enhances%20environmental%20awareness%20through%20inter-agent%0Acommunication%20and%20is%20regarded%20as%20a%20promising%20solution%20to%20intelligent%0Atransportation%20systems.%20However%2C%20existing%20collaborative%20methods%20for%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20overlook%20the%20unique%20characteristics%20of%20the%20UAV%0Aperspective%2C%20resulting%20in%20substantial%20communication%20overhead.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20communication-efficient%20collaborative%20perception%0Aframework%20based%20on%20late-intermediate%20fusion%2C%20dubbed%20LIF.%20The%20core%20concept%20is%20to%0Aexchange%20informative%20and%20compact%20detection%20results%20and%20shift%20the%20fusion%20stage%0Ato%20the%20feature%20representation%20level.%20In%20particular%2C%20we%20leverage%20vision-guided%0Apositional%20embedding%20%28VPE%29%20and%20box-based%20virtual%20augmented%20feature%20%28BoBEV%29%20to%0Aeffectively%20integrate%20complementary%20information%20from%20various%20agents.%0AAdditionally%2C%20we%20innovatively%20introduce%20an%20uncertainty-driven%20communication%0Amechanism%20that%20uses%20uncertainty%20evaluation%20to%20select%20high-quality%20and%20reliable%0Ashared%20areas.%20Experimental%20results%20demonstrate%20that%20our%20LIF%20achieves%20superior%0Aperformance%20with%20minimal%20communication%20bandwidth%2C%20proving%20its%20effectiveness%20and%0Apracticality.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/uestchjw/LIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21774v1&entry.124074799=Read"},
{"title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors", "author": "Derong Jin and Ruohan Gao", "abstract": "  An immersive acoustic experience enabled by spatial audio is just as crucial\nas the visual aspect in creating realistic virtual environments. However,\nexisting methods for room impulse response estimation rely either on\ndata-demanding learning-based models or computationally expensive physics-based\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\nRendering (AV-DAR), a framework that leverages visual cues extracted from\nmulti-view images and acoustic beam tracing for physics-based room acoustic\nrendering. Experiments across six real-world environments from two datasets\ndemonstrate that our multimodal, physics-based approach is efficient,\ninterpretable, and accurate, significantly outperforming a series of prior\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\ncomparable performance to models trained on 10 times more data while delivering\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale.\n", "link": "http://arxiv.org/abs/2504.21847v1", "date": "2025-04-30", "relevancy": 2.2421, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5635}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5635}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Room%20Acoustic%20Rendering%20with%20Multi-View%20Vision%20Priors&body=Title%3A%20Differentiable%20Room%20Acoustic%20Rendering%20with%20Multi-View%20Vision%20Priors%0AAuthor%3A%20Derong%20Jin%20and%20Ruohan%20Gao%0AAbstract%3A%20%20%20An%20immersive%20acoustic%20experience%20enabled%20by%20spatial%20audio%20is%20just%20as%20crucial%0Aas%20the%20visual%20aspect%20in%20creating%20realistic%20virtual%20environments.%20However%2C%0Aexisting%20methods%20for%20room%20impulse%20response%20estimation%20rely%20either%20on%0Adata-demanding%20learning-based%20models%20or%20computationally%20expensive%20physics-based%0Amodeling.%20In%20this%20work%2C%20we%20introduce%20Audio-Visual%20Differentiable%20Room%20Acoustic%0ARendering%20%28AV-DAR%29%2C%20a%20framework%20that%20leverages%20visual%20cues%20extracted%20from%0Amulti-view%20images%20and%20acoustic%20beam%20tracing%20for%20physics-based%20room%20acoustic%0Arendering.%20Experiments%20across%20six%20real-world%20environments%20from%20two%20datasets%0Ademonstrate%20that%20our%20multimodal%2C%20physics-based%20approach%20is%20efficient%2C%0Ainterpretable%2C%20and%20accurate%2C%20significantly%20outperforming%20a%20series%20of%20prior%0Amethods.%20Notably%2C%20on%20the%20Real%20Acoustic%20Field%20dataset%2C%20AV-DAR%20achieves%0Acomparable%20performance%20to%20models%20trained%20on%2010%20times%20more%20data%20while%20delivering%0Arelative%20gains%20ranging%20from%2016.6%25%20to%2050.9%25%20when%20trained%20at%20the%20same%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Room%2520Acoustic%2520Rendering%2520with%2520Multi-View%2520Vision%2520Priors%26entry.906535625%3DDerong%2520Jin%2520and%2520Ruohan%2520Gao%26entry.1292438233%3D%2520%2520An%2520immersive%2520acoustic%2520experience%2520enabled%2520by%2520spatial%2520audio%2520is%2520just%2520as%2520crucial%250Aas%2520the%2520visual%2520aspect%2520in%2520creating%2520realistic%2520virtual%2520environments.%2520However%252C%250Aexisting%2520methods%2520for%2520room%2520impulse%2520response%2520estimation%2520rely%2520either%2520on%250Adata-demanding%2520learning-based%2520models%2520or%2520computationally%2520expensive%2520physics-based%250Amodeling.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Audio-Visual%2520Differentiable%2520Room%2520Acoustic%250ARendering%2520%2528AV-DAR%2529%252C%2520a%2520framework%2520that%2520leverages%2520visual%2520cues%2520extracted%2520from%250Amulti-view%2520images%2520and%2520acoustic%2520beam%2520tracing%2520for%2520physics-based%2520room%2520acoustic%250Arendering.%2520Experiments%2520across%2520six%2520real-world%2520environments%2520from%2520two%2520datasets%250Ademonstrate%2520that%2520our%2520multimodal%252C%2520physics-based%2520approach%2520is%2520efficient%252C%250Ainterpretable%252C%2520and%2520accurate%252C%2520significantly%2520outperforming%2520a%2520series%2520of%2520prior%250Amethods.%2520Notably%252C%2520on%2520the%2520Real%2520Acoustic%2520Field%2520dataset%252C%2520AV-DAR%2520achieves%250Acomparable%2520performance%2520to%2520models%2520trained%2520on%252010%2520times%2520more%2520data%2520while%2520delivering%250Arelative%2520gains%2520ranging%2520from%252016.6%2525%2520to%252050.9%2525%2520when%2520trained%2520at%2520the%2520same%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Room%20Acoustic%20Rendering%20with%20Multi-View%20Vision%20Priors&entry.906535625=Derong%20Jin%20and%20Ruohan%20Gao&entry.1292438233=%20%20An%20immersive%20acoustic%20experience%20enabled%20by%20spatial%20audio%20is%20just%20as%20crucial%0Aas%20the%20visual%20aspect%20in%20creating%20realistic%20virtual%20environments.%20However%2C%0Aexisting%20methods%20for%20room%20impulse%20response%20estimation%20rely%20either%20on%0Adata-demanding%20learning-based%20models%20or%20computationally%20expensive%20physics-based%0Amodeling.%20In%20this%20work%2C%20we%20introduce%20Audio-Visual%20Differentiable%20Room%20Acoustic%0ARendering%20%28AV-DAR%29%2C%20a%20framework%20that%20leverages%20visual%20cues%20extracted%20from%0Amulti-view%20images%20and%20acoustic%20beam%20tracing%20for%20physics-based%20room%20acoustic%0Arendering.%20Experiments%20across%20six%20real-world%20environments%20from%20two%20datasets%0Ademonstrate%20that%20our%20multimodal%2C%20physics-based%20approach%20is%20efficient%2C%0Ainterpretable%2C%20and%20accurate%2C%20significantly%20outperforming%20a%20series%20of%20prior%0Amethods.%20Notably%2C%20on%20the%20Real%20Acoustic%20Field%20dataset%2C%20AV-DAR%20achieves%0Acomparable%20performance%20to%20models%20trained%20on%2010%20times%20more%20data%20while%20delivering%0Arelative%20gains%20ranging%20from%2016.6%25%20to%2050.9%25%20when%20trained%20at%20the%20same%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21847v1&entry.124074799=Read"},
{"title": "Exploring Acoustic Similarity in Emotional Speech and Music via\n  Self-Supervised Representations", "author": "Yujia Sun and Zeyu Zhao and Korin Richmond and Yuanchao Li", "abstract": "  Emotion recognition from speech and music shares similarities due to their\nacoustic overlap, which has led to interest in transferring knowledge between\nthese domains. However, the shared acoustic cues between speech and music,\nparticularly those encoded by Self-Supervised Learning (SSL) models, remain\nlargely unexplored, given the fact that SSL models for speech and music have\nrarely been applied in cross-domain research. In this work, we revisit the\nacoustic similarity between emotion speech and music, starting with an analysis\nof the layerwise behavior of SSL models for Speech Emotion Recognition (SER)\nand Music Emotion Recognition (MER). Furthermore, we perform cross-domain\nadaptation by comparing several approaches in a two-stage fine-tuning process,\nexamining effective ways to utilize music for SER and speech for MER. Lastly,\nwe explore the acoustic similarities between emotional speech and music using\nFrechet audio distance for individual emotions, uncovering the issue of emotion\nbias in both speech and music SSL models. Our findings reveal that while speech\nand music SSL models do capture shared acoustic features, their behaviors can\nvary depending on different emotions due to their training strategies and\ndomain-specificities. Additionally, parameter-efficient fine-tuning can enhance\nSER and MER performance by leveraging knowledge from each other. This study\nprovides new insights into the acoustic similarity between emotional speech and\nmusic, and highlights the potential for cross-domain generalization to improve\nSER and MER systems.\n", "link": "http://arxiv.org/abs/2409.17899v2", "date": "2025-04-30", "relevancy": 2.236, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Acoustic%20Similarity%20in%20Emotional%20Speech%20and%20Music%20via%0A%20%20Self-Supervised%20Representations&body=Title%3A%20Exploring%20Acoustic%20Similarity%20in%20Emotional%20Speech%20and%20Music%20via%0A%20%20Self-Supervised%20Representations%0AAuthor%3A%20Yujia%20Sun%20and%20Zeyu%20Zhao%20and%20Korin%20Richmond%20and%20Yuanchao%20Li%0AAbstract%3A%20%20%20Emotion%20recognition%20from%20speech%20and%20music%20shares%20similarities%20due%20to%20their%0Aacoustic%20overlap%2C%20which%20has%20led%20to%20interest%20in%20transferring%20knowledge%20between%0Athese%20domains.%20However%2C%20the%20shared%20acoustic%20cues%20between%20speech%20and%20music%2C%0Aparticularly%20those%20encoded%20by%20Self-Supervised%20Learning%20%28SSL%29%20models%2C%20remain%0Alargely%20unexplored%2C%20given%20the%20fact%20that%20SSL%20models%20for%20speech%20and%20music%20have%0Ararely%20been%20applied%20in%20cross-domain%20research.%20In%20this%20work%2C%20we%20revisit%20the%0Aacoustic%20similarity%20between%20emotion%20speech%20and%20music%2C%20starting%20with%20an%20analysis%0Aof%20the%20layerwise%20behavior%20of%20SSL%20models%20for%20Speech%20Emotion%20Recognition%20%28SER%29%0Aand%20Music%20Emotion%20Recognition%20%28MER%29.%20Furthermore%2C%20we%20perform%20cross-domain%0Aadaptation%20by%20comparing%20several%20approaches%20in%20a%20two-stage%20fine-tuning%20process%2C%0Aexamining%20effective%20ways%20to%20utilize%20music%20for%20SER%20and%20speech%20for%20MER.%20Lastly%2C%0Awe%20explore%20the%20acoustic%20similarities%20between%20emotional%20speech%20and%20music%20using%0AFrechet%20audio%20distance%20for%20individual%20emotions%2C%20uncovering%20the%20issue%20of%20emotion%0Abias%20in%20both%20speech%20and%20music%20SSL%20models.%20Our%20findings%20reveal%20that%20while%20speech%0Aand%20music%20SSL%20models%20do%20capture%20shared%20acoustic%20features%2C%20their%20behaviors%20can%0Avary%20depending%20on%20different%20emotions%20due%20to%20their%20training%20strategies%20and%0Adomain-specificities.%20Additionally%2C%20parameter-efficient%20fine-tuning%20can%20enhance%0ASER%20and%20MER%20performance%20by%20leveraging%20knowledge%20from%20each%20other.%20This%20study%0Aprovides%20new%20insights%20into%20the%20acoustic%20similarity%20between%20emotional%20speech%20and%0Amusic%2C%20and%20highlights%20the%20potential%20for%20cross-domain%20generalization%20to%20improve%0ASER%20and%20MER%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Acoustic%2520Similarity%2520in%2520Emotional%2520Speech%2520and%2520Music%2520via%250A%2520%2520Self-Supervised%2520Representations%26entry.906535625%3DYujia%2520Sun%2520and%2520Zeyu%2520Zhao%2520and%2520Korin%2520Richmond%2520and%2520Yuanchao%2520Li%26entry.1292438233%3D%2520%2520Emotion%2520recognition%2520from%2520speech%2520and%2520music%2520shares%2520similarities%2520due%2520to%2520their%250Aacoustic%2520overlap%252C%2520which%2520has%2520led%2520to%2520interest%2520in%2520transferring%2520knowledge%2520between%250Athese%2520domains.%2520However%252C%2520the%2520shared%2520acoustic%2520cues%2520between%2520speech%2520and%2520music%252C%250Aparticularly%2520those%2520encoded%2520by%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520models%252C%2520remain%250Alargely%2520unexplored%252C%2520given%2520the%2520fact%2520that%2520SSL%2520models%2520for%2520speech%2520and%2520music%2520have%250Ararely%2520been%2520applied%2520in%2520cross-domain%2520research.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%250Aacoustic%2520similarity%2520between%2520emotion%2520speech%2520and%2520music%252C%2520starting%2520with%2520an%2520analysis%250Aof%2520the%2520layerwise%2520behavior%2520of%2520SSL%2520models%2520for%2520Speech%2520Emotion%2520Recognition%2520%2528SER%2529%250Aand%2520Music%2520Emotion%2520Recognition%2520%2528MER%2529.%2520Furthermore%252C%2520we%2520perform%2520cross-domain%250Aadaptation%2520by%2520comparing%2520several%2520approaches%2520in%2520a%2520two-stage%2520fine-tuning%2520process%252C%250Aexamining%2520effective%2520ways%2520to%2520utilize%2520music%2520for%2520SER%2520and%2520speech%2520for%2520MER.%2520Lastly%252C%250Awe%2520explore%2520the%2520acoustic%2520similarities%2520between%2520emotional%2520speech%2520and%2520music%2520using%250AFrechet%2520audio%2520distance%2520for%2520individual%2520emotions%252C%2520uncovering%2520the%2520issue%2520of%2520emotion%250Abias%2520in%2520both%2520speech%2520and%2520music%2520SSL%2520models.%2520Our%2520findings%2520reveal%2520that%2520while%2520speech%250Aand%2520music%2520SSL%2520models%2520do%2520capture%2520shared%2520acoustic%2520features%252C%2520their%2520behaviors%2520can%250Avary%2520depending%2520on%2520different%2520emotions%2520due%2520to%2520their%2520training%2520strategies%2520and%250Adomain-specificities.%2520Additionally%252C%2520parameter-efficient%2520fine-tuning%2520can%2520enhance%250ASER%2520and%2520MER%2520performance%2520by%2520leveraging%2520knowledge%2520from%2520each%2520other.%2520This%2520study%250Aprovides%2520new%2520insights%2520into%2520the%2520acoustic%2520similarity%2520between%2520emotional%2520speech%2520and%250Amusic%252C%2520and%2520highlights%2520the%2520potential%2520for%2520cross-domain%2520generalization%2520to%2520improve%250ASER%2520and%2520MER%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Acoustic%20Similarity%20in%20Emotional%20Speech%20and%20Music%20via%0A%20%20Self-Supervised%20Representations&entry.906535625=Yujia%20Sun%20and%20Zeyu%20Zhao%20and%20Korin%20Richmond%20and%20Yuanchao%20Li&entry.1292438233=%20%20Emotion%20recognition%20from%20speech%20and%20music%20shares%20similarities%20due%20to%20their%0Aacoustic%20overlap%2C%20which%20has%20led%20to%20interest%20in%20transferring%20knowledge%20between%0Athese%20domains.%20However%2C%20the%20shared%20acoustic%20cues%20between%20speech%20and%20music%2C%0Aparticularly%20those%20encoded%20by%20Self-Supervised%20Learning%20%28SSL%29%20models%2C%20remain%0Alargely%20unexplored%2C%20given%20the%20fact%20that%20SSL%20models%20for%20speech%20and%20music%20have%0Ararely%20been%20applied%20in%20cross-domain%20research.%20In%20this%20work%2C%20we%20revisit%20the%0Aacoustic%20similarity%20between%20emotion%20speech%20and%20music%2C%20starting%20with%20an%20analysis%0Aof%20the%20layerwise%20behavior%20of%20SSL%20models%20for%20Speech%20Emotion%20Recognition%20%28SER%29%0Aand%20Music%20Emotion%20Recognition%20%28MER%29.%20Furthermore%2C%20we%20perform%20cross-domain%0Aadaptation%20by%20comparing%20several%20approaches%20in%20a%20two-stage%20fine-tuning%20process%2C%0Aexamining%20effective%20ways%20to%20utilize%20music%20for%20SER%20and%20speech%20for%20MER.%20Lastly%2C%0Awe%20explore%20the%20acoustic%20similarities%20between%20emotional%20speech%20and%20music%20using%0AFrechet%20audio%20distance%20for%20individual%20emotions%2C%20uncovering%20the%20issue%20of%20emotion%0Abias%20in%20both%20speech%20and%20music%20SSL%20models.%20Our%20findings%20reveal%20that%20while%20speech%0Aand%20music%20SSL%20models%20do%20capture%20shared%20acoustic%20features%2C%20their%20behaviors%20can%0Avary%20depending%20on%20different%20emotions%20due%20to%20their%20training%20strategies%20and%0Adomain-specificities.%20Additionally%2C%20parameter-efficient%20fine-tuning%20can%20enhance%0ASER%20and%20MER%20performance%20by%20leveraging%20knowledge%20from%20each%20other.%20This%20study%0Aprovides%20new%20insights%20into%20the%20acoustic%20similarity%20between%20emotional%20speech%20and%0Amusic%2C%20and%20highlights%20the%20potential%20for%20cross-domain%20generalization%20to%20improve%0ASER%20and%20MER%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17899v2&entry.124074799=Read"},
{"title": "Designing Control Barrier Function via Probabilistic Enumeration for\n  Safe Reinforcement Learning Navigation", "author": "Luca Marzari and Francesco Trotti and Enrico Marchesini and Alessandro Farinelli", "abstract": "  Achieving safe autonomous navigation systems is critical for deploying robots\nin dynamic and uncertain real-world environments. In this paper, we propose a\nhierarchical control framework leveraging neural network verification\ntechniques to design control barrier functions (CBFs) and policy correction\nmechanisms that ensure safe reinforcement learning navigation policies. Our\napproach relies on probabilistic enumeration to identify unsafe regions of\noperation, which are then used to construct a safe CBF-based control layer\napplicable to arbitrary policies. We validate our framework both in simulation\nand on a real robot, using a standard mobile robot benchmark and a highly\ndynamic aquatic environmental monitoring task. These experiments demonstrate\nthe ability of the proposed solution to correct unsafe actions while preserving\nefficient navigation behavior. Our results show the promise of developing\nhierarchical verification-based systems to enable safe and robust navigation\nbehaviors in complex scenarios.\n", "link": "http://arxiv.org/abs/2504.21643v1", "date": "2025-04-30", "relevancy": 2.2252, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5582}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20Control%20Barrier%20Function%20via%20Probabilistic%20Enumeration%20for%0A%20%20Safe%20Reinforcement%20Learning%20Navigation&body=Title%3A%20Designing%20Control%20Barrier%20Function%20via%20Probabilistic%20Enumeration%20for%0A%20%20Safe%20Reinforcement%20Learning%20Navigation%0AAuthor%3A%20Luca%20Marzari%20and%20Francesco%20Trotti%20and%20Enrico%20Marchesini%20and%20Alessandro%20Farinelli%0AAbstract%3A%20%20%20Achieving%20safe%20autonomous%20navigation%20systems%20is%20critical%20for%20deploying%20robots%0Ain%20dynamic%20and%20uncertain%20real-world%20environments.%20In%20this%20paper%2C%20we%20propose%20a%0Ahierarchical%20control%20framework%20leveraging%20neural%20network%20verification%0Atechniques%20to%20design%20control%20barrier%20functions%20%28CBFs%29%20and%20policy%20correction%0Amechanisms%20that%20ensure%20safe%20reinforcement%20learning%20navigation%20policies.%20Our%0Aapproach%20relies%20on%20probabilistic%20enumeration%20to%20identify%20unsafe%20regions%20of%0Aoperation%2C%20which%20are%20then%20used%20to%20construct%20a%20safe%20CBF-based%20control%20layer%0Aapplicable%20to%20arbitrary%20policies.%20We%20validate%20our%20framework%20both%20in%20simulation%0Aand%20on%20a%20real%20robot%2C%20using%20a%20standard%20mobile%20robot%20benchmark%20and%20a%20highly%0Adynamic%20aquatic%20environmental%20monitoring%20task.%20These%20experiments%20demonstrate%0Athe%20ability%20of%20the%20proposed%20solution%20to%20correct%20unsafe%20actions%20while%20preserving%0Aefficient%20navigation%20behavior.%20Our%20results%20show%20the%20promise%20of%20developing%0Ahierarchical%20verification-based%20systems%20to%20enable%20safe%20and%20robust%20navigation%0Abehaviors%20in%20complex%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520Control%2520Barrier%2520Function%2520via%2520Probabilistic%2520Enumeration%2520for%250A%2520%2520Safe%2520Reinforcement%2520Learning%2520Navigation%26entry.906535625%3DLuca%2520Marzari%2520and%2520Francesco%2520Trotti%2520and%2520Enrico%2520Marchesini%2520and%2520Alessandro%2520Farinelli%26entry.1292438233%3D%2520%2520Achieving%2520safe%2520autonomous%2520navigation%2520systems%2520is%2520critical%2520for%2520deploying%2520robots%250Ain%2520dynamic%2520and%2520uncertain%2520real-world%2520environments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ahierarchical%2520control%2520framework%2520leveraging%2520neural%2520network%2520verification%250Atechniques%2520to%2520design%2520control%2520barrier%2520functions%2520%2528CBFs%2529%2520and%2520policy%2520correction%250Amechanisms%2520that%2520ensure%2520safe%2520reinforcement%2520learning%2520navigation%2520policies.%2520Our%250Aapproach%2520relies%2520on%2520probabilistic%2520enumeration%2520to%2520identify%2520unsafe%2520regions%2520of%250Aoperation%252C%2520which%2520are%2520then%2520used%2520to%2520construct%2520a%2520safe%2520CBF-based%2520control%2520layer%250Aapplicable%2520to%2520arbitrary%2520policies.%2520We%2520validate%2520our%2520framework%2520both%2520in%2520simulation%250Aand%2520on%2520a%2520real%2520robot%252C%2520using%2520a%2520standard%2520mobile%2520robot%2520benchmark%2520and%2520a%2520highly%250Adynamic%2520aquatic%2520environmental%2520monitoring%2520task.%2520These%2520experiments%2520demonstrate%250Athe%2520ability%2520of%2520the%2520proposed%2520solution%2520to%2520correct%2520unsafe%2520actions%2520while%2520preserving%250Aefficient%2520navigation%2520behavior.%2520Our%2520results%2520show%2520the%2520promise%2520of%2520developing%250Ahierarchical%2520verification-based%2520systems%2520to%2520enable%2520safe%2520and%2520robust%2520navigation%250Abehaviors%2520in%2520complex%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20Control%20Barrier%20Function%20via%20Probabilistic%20Enumeration%20for%0A%20%20Safe%20Reinforcement%20Learning%20Navigation&entry.906535625=Luca%20Marzari%20and%20Francesco%20Trotti%20and%20Enrico%20Marchesini%20and%20Alessandro%20Farinelli&entry.1292438233=%20%20Achieving%20safe%20autonomous%20navigation%20systems%20is%20critical%20for%20deploying%20robots%0Ain%20dynamic%20and%20uncertain%20real-world%20environments.%20In%20this%20paper%2C%20we%20propose%20a%0Ahierarchical%20control%20framework%20leveraging%20neural%20network%20verification%0Atechniques%20to%20design%20control%20barrier%20functions%20%28CBFs%29%20and%20policy%20correction%0Amechanisms%20that%20ensure%20safe%20reinforcement%20learning%20navigation%20policies.%20Our%0Aapproach%20relies%20on%20probabilistic%20enumeration%20to%20identify%20unsafe%20regions%20of%0Aoperation%2C%20which%20are%20then%20used%20to%20construct%20a%20safe%20CBF-based%20control%20layer%0Aapplicable%20to%20arbitrary%20policies.%20We%20validate%20our%20framework%20both%20in%20simulation%0Aand%20on%20a%20real%20robot%2C%20using%20a%20standard%20mobile%20robot%20benchmark%20and%20a%20highly%0Adynamic%20aquatic%20environmental%20monitoring%20task.%20These%20experiments%20demonstrate%0Athe%20ability%20of%20the%20proposed%20solution%20to%20correct%20unsafe%20actions%20while%20preserving%0Aefficient%20navigation%20behavior.%20Our%20results%20show%20the%20promise%20of%20developing%0Ahierarchical%20verification-based%20systems%20to%20enable%20safe%20and%20robust%20navigation%0Abehaviors%20in%20complex%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21643v1&entry.124074799=Read"},
{"title": "A simple and effective approach for body part recognition on CT scans\n  based on projection estimation", "author": "Franko Hrzic and Mohammadreza Movahhedi and Ophelie Lavoie-Gagne and Ata Kiapour", "abstract": "  It is well known that machine learning models require a high amount of\nannotated data to obtain optimal performance. Labelling Computed Tomography\n(CT) data can be a particularly challenging task due to its volumetric nature\nand often missing and$/$or incomplete associated meta-data. Even inspecting one\nCT scan requires additional computer software, or in the case of programming\nlanguages $-$ additional programming libraries. This study proposes a simple,\nyet effective approach based on 2D X-ray-like estimation of 3D CT scans for\nbody region identification. Although body region is commonly associated with\nthe CT scan, it often describes only the focused major body region neglecting\nother anatomical regions present in the observed CT. In the proposed approach,\nestimated 2D images were utilized to identify 14 distinct body regions,\nproviding valuable information for constructing a high-quality medical dataset.\nTo evaluate the effectiveness of the proposed method, it was compared against\n2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed\nthe others, where it came on top with statistical significance and F1-Score for\nthe best-performing model EffNet-B0 of 0.980 $\\pm$ 0.016 in comparison to the\n0.840 $\\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\\pm$ 0.096 (3D VoxCNN), and 0.852\n$\\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three\ndifferent clinical centers and counted 15,622 CT scans (44,135 labels).\n", "link": "http://arxiv.org/abs/2504.21810v1", "date": "2025-04-30", "relevancy": 2.2216, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20simple%20and%20effective%20approach%20for%20body%20part%20recognition%20on%20CT%20scans%0A%20%20based%20on%20projection%20estimation&body=Title%3A%20A%20simple%20and%20effective%20approach%20for%20body%20part%20recognition%20on%20CT%20scans%0A%20%20based%20on%20projection%20estimation%0AAuthor%3A%20Franko%20Hrzic%20and%20Mohammadreza%20Movahhedi%20and%20Ophelie%20Lavoie-Gagne%20and%20Ata%20Kiapour%0AAbstract%3A%20%20%20It%20is%20well%20known%20that%20machine%20learning%20models%20require%20a%20high%20amount%20of%0Aannotated%20data%20to%20obtain%20optimal%20performance.%20Labelling%20Computed%20Tomography%0A%28CT%29%20data%20can%20be%20a%20particularly%20challenging%20task%20due%20to%20its%20volumetric%20nature%0Aand%20often%20missing%20and%24/%24or%20incomplete%20associated%20meta-data.%20Even%20inspecting%20one%0ACT%20scan%20requires%20additional%20computer%20software%2C%20or%20in%20the%20case%20of%20programming%0Alanguages%20%24-%24%20additional%20programming%20libraries.%20This%20study%20proposes%20a%20simple%2C%0Ayet%20effective%20approach%20based%20on%202D%20X-ray-like%20estimation%20of%203D%20CT%20scans%20for%0Abody%20region%20identification.%20Although%20body%20region%20is%20commonly%20associated%20with%0Athe%20CT%20scan%2C%20it%20often%20describes%20only%20the%20focused%20major%20body%20region%20neglecting%0Aother%20anatomical%20regions%20present%20in%20the%20observed%20CT.%20In%20the%20proposed%20approach%2C%0Aestimated%202D%20images%20were%20utilized%20to%20identify%2014%20distinct%20body%20regions%2C%0Aproviding%20valuable%20information%20for%20constructing%20a%20high-quality%20medical%20dataset.%0ATo%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20it%20was%20compared%20against%0A2.5D%2C%203D%20and%20foundation%20model%20%28MI2%29%20based%20approaches.%20Our%20approach%20outperformed%0Athe%20others%2C%20where%20it%20came%20on%20top%20with%20statistical%20significance%20and%20F1-Score%20for%0Athe%20best-performing%20model%20EffNet-B0%20of%200.980%20%24%5Cpm%24%200.016%20in%20comparison%20to%20the%0A0.840%20%24%5Cpm%24%200.114%20%282.5D%20DenseNet-161%29%2C%200.854%20%24%5Cpm%24%200.096%20%283D%20VoxCNN%29%2C%20and%200.852%0A%24%5Cpm%24%200.104%20%28MI2%20foundation%20model%29.%20The%20utilized%20dataset%20comprised%20three%0Adifferent%20clinical%20centers%20and%20counted%2015%2C622%20CT%20scans%20%2844%2C135%20labels%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520simple%2520and%2520effective%2520approach%2520for%2520body%2520part%2520recognition%2520on%2520CT%2520scans%250A%2520%2520based%2520on%2520projection%2520estimation%26entry.906535625%3DFranko%2520Hrzic%2520and%2520Mohammadreza%2520Movahhedi%2520and%2520Ophelie%2520Lavoie-Gagne%2520and%2520Ata%2520Kiapour%26entry.1292438233%3D%2520%2520It%2520is%2520well%2520known%2520that%2520machine%2520learning%2520models%2520require%2520a%2520high%2520amount%2520of%250Aannotated%2520data%2520to%2520obtain%2520optimal%2520performance.%2520Labelling%2520Computed%2520Tomography%250A%2528CT%2529%2520data%2520can%2520be%2520a%2520particularly%2520challenging%2520task%2520due%2520to%2520its%2520volumetric%2520nature%250Aand%2520often%2520missing%2520and%2524/%2524or%2520incomplete%2520associated%2520meta-data.%2520Even%2520inspecting%2520one%250ACT%2520scan%2520requires%2520additional%2520computer%2520software%252C%2520or%2520in%2520the%2520case%2520of%2520programming%250Alanguages%2520%2524-%2524%2520additional%2520programming%2520libraries.%2520This%2520study%2520proposes%2520a%2520simple%252C%250Ayet%2520effective%2520approach%2520based%2520on%25202D%2520X-ray-like%2520estimation%2520of%25203D%2520CT%2520scans%2520for%250Abody%2520region%2520identification.%2520Although%2520body%2520region%2520is%2520commonly%2520associated%2520with%250Athe%2520CT%2520scan%252C%2520it%2520often%2520describes%2520only%2520the%2520focused%2520major%2520body%2520region%2520neglecting%250Aother%2520anatomical%2520regions%2520present%2520in%2520the%2520observed%2520CT.%2520In%2520the%2520proposed%2520approach%252C%250Aestimated%25202D%2520images%2520were%2520utilized%2520to%2520identify%252014%2520distinct%2520body%2520regions%252C%250Aproviding%2520valuable%2520information%2520for%2520constructing%2520a%2520high-quality%2520medical%2520dataset.%250ATo%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520it%2520was%2520compared%2520against%250A2.5D%252C%25203D%2520and%2520foundation%2520model%2520%2528MI2%2529%2520based%2520approaches.%2520Our%2520approach%2520outperformed%250Athe%2520others%252C%2520where%2520it%2520came%2520on%2520top%2520with%2520statistical%2520significance%2520and%2520F1-Score%2520for%250Athe%2520best-performing%2520model%2520EffNet-B0%2520of%25200.980%2520%2524%255Cpm%2524%25200.016%2520in%2520comparison%2520to%2520the%250A0.840%2520%2524%255Cpm%2524%25200.114%2520%25282.5D%2520DenseNet-161%2529%252C%25200.854%2520%2524%255Cpm%2524%25200.096%2520%25283D%2520VoxCNN%2529%252C%2520and%25200.852%250A%2524%255Cpm%2524%25200.104%2520%2528MI2%2520foundation%2520model%2529.%2520The%2520utilized%2520dataset%2520comprised%2520three%250Adifferent%2520clinical%2520centers%2520and%2520counted%252015%252C622%2520CT%2520scans%2520%252844%252C135%2520labels%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20simple%20and%20effective%20approach%20for%20body%20part%20recognition%20on%20CT%20scans%0A%20%20based%20on%20projection%20estimation&entry.906535625=Franko%20Hrzic%20and%20Mohammadreza%20Movahhedi%20and%20Ophelie%20Lavoie-Gagne%20and%20Ata%20Kiapour&entry.1292438233=%20%20It%20is%20well%20known%20that%20machine%20learning%20models%20require%20a%20high%20amount%20of%0Aannotated%20data%20to%20obtain%20optimal%20performance.%20Labelling%20Computed%20Tomography%0A%28CT%29%20data%20can%20be%20a%20particularly%20challenging%20task%20due%20to%20its%20volumetric%20nature%0Aand%20often%20missing%20and%24/%24or%20incomplete%20associated%20meta-data.%20Even%20inspecting%20one%0ACT%20scan%20requires%20additional%20computer%20software%2C%20or%20in%20the%20case%20of%20programming%0Alanguages%20%24-%24%20additional%20programming%20libraries.%20This%20study%20proposes%20a%20simple%2C%0Ayet%20effective%20approach%20based%20on%202D%20X-ray-like%20estimation%20of%203D%20CT%20scans%20for%0Abody%20region%20identification.%20Although%20body%20region%20is%20commonly%20associated%20with%0Athe%20CT%20scan%2C%20it%20often%20describes%20only%20the%20focused%20major%20body%20region%20neglecting%0Aother%20anatomical%20regions%20present%20in%20the%20observed%20CT.%20In%20the%20proposed%20approach%2C%0Aestimated%202D%20images%20were%20utilized%20to%20identify%2014%20distinct%20body%20regions%2C%0Aproviding%20valuable%20information%20for%20constructing%20a%20high-quality%20medical%20dataset.%0ATo%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20it%20was%20compared%20against%0A2.5D%2C%203D%20and%20foundation%20model%20%28MI2%29%20based%20approaches.%20Our%20approach%20outperformed%0Athe%20others%2C%20where%20it%20came%20on%20top%20with%20statistical%20significance%20and%20F1-Score%20for%0Athe%20best-performing%20model%20EffNet-B0%20of%200.980%20%24%5Cpm%24%200.016%20in%20comparison%20to%20the%0A0.840%20%24%5Cpm%24%200.114%20%282.5D%20DenseNet-161%29%2C%200.854%20%24%5Cpm%24%200.096%20%283D%20VoxCNN%29%2C%20and%200.852%0A%24%5Cpm%24%200.104%20%28MI2%20foundation%20model%29.%20The%20utilized%20dataset%20comprised%20three%0Adifferent%20clinical%20centers%20and%20counted%2015%2C622%20CT%20scans%20%2844%2C135%20labels%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21810v1&entry.124074799=Read"},
{"title": "Type-R: Automatically Retouching Typos for Text-to-Image Generation", "author": "Wataru Shimoda and Naoto Inoue and Daichi Haraguchi and Hayato Mitani and Seiichi Uchida and Kota Yamaguchi", "abstract": "  While recent text-to-image models can generate photorealistic images from\ntext prompts that reflect detailed instructions, they still face significant\nchallenges in accurately rendering words in the image. In this paper, we\npropose to retouch erroneous text renderings in the post-processing pipeline.\nOur approach, called Type-R, identifies typographical errors in the generated\nimage, erases the erroneous text, regenerates text boxes for missing words, and\nfinally corrects typos in the rendered words. Through extensive experiments, we\nshow that Type-R, in combination with the latest text-to-image models such as\nStable Diffusion or Flux, achieves the highest text rendering accuracy while\nmaintaining image quality and also outperforms text-focused generation\nbaselines in terms of balancing text accuracy and image quality.\n", "link": "http://arxiv.org/abs/2411.18159v2", "date": "2025-04-30", "relevancy": 2.2154, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.601}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Type-R%3A%20Automatically%20Retouching%20Typos%20for%20Text-to-Image%20Generation&body=Title%3A%20Type-R%3A%20Automatically%20Retouching%20Typos%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Wataru%20Shimoda%20and%20Naoto%20Inoue%20and%20Daichi%20Haraguchi%20and%20Hayato%20Mitani%20and%20Seiichi%20Uchida%20and%20Kota%20Yamaguchi%0AAbstract%3A%20%20%20While%20recent%20text-to-image%20models%20can%20generate%20photorealistic%20images%20from%0Atext%20prompts%20that%20reflect%20detailed%20instructions%2C%20they%20still%20face%20significant%0Achallenges%20in%20accurately%20rendering%20words%20in%20the%20image.%20In%20this%20paper%2C%20we%0Apropose%20to%20retouch%20erroneous%20text%20renderings%20in%20the%20post-processing%20pipeline.%0AOur%20approach%2C%20called%20Type-R%2C%20identifies%20typographical%20errors%20in%20the%20generated%0Aimage%2C%20erases%20the%20erroneous%20text%2C%20regenerates%20text%20boxes%20for%20missing%20words%2C%20and%0Afinally%20corrects%20typos%20in%20the%20rendered%20words.%20Through%20extensive%20experiments%2C%20we%0Ashow%20that%20Type-R%2C%20in%20combination%20with%20the%20latest%20text-to-image%20models%20such%20as%0AStable%20Diffusion%20or%20Flux%2C%20achieves%20the%20highest%20text%20rendering%20accuracy%20while%0Amaintaining%20image%20quality%20and%20also%20outperforms%20text-focused%20generation%0Abaselines%20in%20terms%20of%20balancing%20text%20accuracy%20and%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18159v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DType-R%253A%2520Automatically%2520Retouching%2520Typos%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DWataru%2520Shimoda%2520and%2520Naoto%2520Inoue%2520and%2520Daichi%2520Haraguchi%2520and%2520Hayato%2520Mitani%2520and%2520Seiichi%2520Uchida%2520and%2520Kota%2520Yamaguchi%26entry.1292438233%3D%2520%2520While%2520recent%2520text-to-image%2520models%2520can%2520generate%2520photorealistic%2520images%2520from%250Atext%2520prompts%2520that%2520reflect%2520detailed%2520instructions%252C%2520they%2520still%2520face%2520significant%250Achallenges%2520in%2520accurately%2520rendering%2520words%2520in%2520the%2520image.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520to%2520retouch%2520erroneous%2520text%2520renderings%2520in%2520the%2520post-processing%2520pipeline.%250AOur%2520approach%252C%2520called%2520Type-R%252C%2520identifies%2520typographical%2520errors%2520in%2520the%2520generated%250Aimage%252C%2520erases%2520the%2520erroneous%2520text%252C%2520regenerates%2520text%2520boxes%2520for%2520missing%2520words%252C%2520and%250Afinally%2520corrects%2520typos%2520in%2520the%2520rendered%2520words.%2520Through%2520extensive%2520experiments%252C%2520we%250Ashow%2520that%2520Type-R%252C%2520in%2520combination%2520with%2520the%2520latest%2520text-to-image%2520models%2520such%2520as%250AStable%2520Diffusion%2520or%2520Flux%252C%2520achieves%2520the%2520highest%2520text%2520rendering%2520accuracy%2520while%250Amaintaining%2520image%2520quality%2520and%2520also%2520outperforms%2520text-focused%2520generation%250Abaselines%2520in%2520terms%2520of%2520balancing%2520text%2520accuracy%2520and%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18159v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Type-R%3A%20Automatically%20Retouching%20Typos%20for%20Text-to-Image%20Generation&entry.906535625=Wataru%20Shimoda%20and%20Naoto%20Inoue%20and%20Daichi%20Haraguchi%20and%20Hayato%20Mitani%20and%20Seiichi%20Uchida%20and%20Kota%20Yamaguchi&entry.1292438233=%20%20While%20recent%20text-to-image%20models%20can%20generate%20photorealistic%20images%20from%0Atext%20prompts%20that%20reflect%20detailed%20instructions%2C%20they%20still%20face%20significant%0Achallenges%20in%20accurately%20rendering%20words%20in%20the%20image.%20In%20this%20paper%2C%20we%0Apropose%20to%20retouch%20erroneous%20text%20renderings%20in%20the%20post-processing%20pipeline.%0AOur%20approach%2C%20called%20Type-R%2C%20identifies%20typographical%20errors%20in%20the%20generated%0Aimage%2C%20erases%20the%20erroneous%20text%2C%20regenerates%20text%20boxes%20for%20missing%20words%2C%20and%0Afinally%20corrects%20typos%20in%20the%20rendered%20words.%20Through%20extensive%20experiments%2C%20we%0Ashow%20that%20Type-R%2C%20in%20combination%20with%20the%20latest%20text-to-image%20models%20such%20as%0AStable%20Diffusion%20or%20Flux%2C%20achieves%20the%20highest%20text%20rendering%20accuracy%20while%0Amaintaining%20image%20quality%20and%20also%20outperforms%20text-focused%20generation%0Abaselines%20in%20terms%20of%20balancing%20text%20accuracy%20and%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18159v2&entry.124074799=Read"},
{"title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via\n  Reinforcement Learning for Subgoal Decomposition", "author": "Z. Z. Ren and Zhihong Shao and Junxiao Song and Huajian Xin and Haocheng Wang and Wanjia Zhao and Liyue Zhang and Zhe Fu and Qihao Zhu and Dejian Yang and Z. F. Wu and Zhibin Gou and Shirong Ma and Hongxuan Tang and Yuxuan Liu and Wenjun Gao and Daya Guo and Chong Ruan", "abstract": "  We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing.\n", "link": "http://arxiv.org/abs/2504.21801v1", "date": "2025-04-30", "relevancy": 2.2028, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSeek-Prover-V2%3A%20Advancing%20Formal%20Mathematical%20Reasoning%20via%0A%20%20Reinforcement%20Learning%20for%20Subgoal%20Decomposition&body=Title%3A%20DeepSeek-Prover-V2%3A%20Advancing%20Formal%20Mathematical%20Reasoning%20via%0A%20%20Reinforcement%20Learning%20for%20Subgoal%20Decomposition%0AAuthor%3A%20Z.%20Z.%20Ren%20and%20Zhihong%20Shao%20and%20Junxiao%20Song%20and%20Huajian%20Xin%20and%20Haocheng%20Wang%20and%20Wanjia%20Zhao%20and%20Liyue%20Zhang%20and%20Zhe%20Fu%20and%20Qihao%20Zhu%20and%20Dejian%20Yang%20and%20Z.%20F.%20Wu%20and%20Zhibin%20Gou%20and%20Shirong%20Ma%20and%20Hongxuan%20Tang%20and%20Yuxuan%20Liu%20and%20Wenjun%20Gao%20and%20Daya%20Guo%20and%20Chong%20Ruan%0AAbstract%3A%20%20%20We%20introduce%20DeepSeek-Prover-V2%2C%20an%20open-source%20large%20language%20model%20designed%0Afor%20formal%20theorem%20proving%20in%20Lean%204%2C%20with%20initialization%20data%20collected%0Athrough%20a%20recursive%20theorem%20proving%20pipeline%20powered%20by%20DeepSeek-V3.%20The%0Acold-start%20training%20procedure%20begins%20by%20prompting%20DeepSeek-V3%20to%20decompose%0Acomplex%20problems%20into%20a%20series%20of%20subgoals.%20The%20proofs%20of%20resolved%20subgoals%20are%0Asynthesized%20into%20a%20chain-of-thought%20process%2C%20combined%20with%20DeepSeek-V3%27s%0Astep-by-step%20reasoning%2C%20to%20create%20an%20initial%20cold%20start%20for%20reinforcement%0Alearning.%20This%20process%20enables%20us%20to%20integrate%20both%20informal%20and%20formal%0Amathematical%20reasoning%20into%20a%20unified%20model.%20The%20resulting%20model%2C%0ADeepSeek-Prover-V2-671B%2C%20achieves%20state-of-the-art%20performance%20in%20neural%0Atheorem%20proving%2C%20reaching%2088.9%25%20pass%20ratio%20on%20the%20MiniF2F-test%20and%20solving%2049%0Aout%20of%20658%20problems%20from%20PutnamBench.%20In%20addition%20to%20standard%20benchmarks%2C%20we%0Aintroduce%20ProverBench%2C%20a%20collection%20of%20325%20formalized%20problems%2C%20to%20enrich%20our%0Aevaluation%2C%20including%2015%20selected%20problems%20from%20the%20recent%20AIME%20competitions%0A%28years%2024-25%29.%20Further%20evaluation%20on%20these%2015%20AIME%20problems%20shows%20that%20the%0Amodel%20successfully%20solves%206%20of%20them.%20In%20comparison%2C%20DeepSeek-V3%20solves%208%20of%0Athese%20problems%20using%20majority%20voting%2C%20highlighting%20that%20the%20gap%20between%20formal%0Aand%20informal%20mathematical%20reasoning%20in%20large%20language%20models%20is%20substantially%0Anarrowing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSeek-Prover-V2%253A%2520Advancing%2520Formal%2520Mathematical%2520Reasoning%2520via%250A%2520%2520Reinforcement%2520Learning%2520for%2520Subgoal%2520Decomposition%26entry.906535625%3DZ.%2520Z.%2520Ren%2520and%2520Zhihong%2520Shao%2520and%2520Junxiao%2520Song%2520and%2520Huajian%2520Xin%2520and%2520Haocheng%2520Wang%2520and%2520Wanjia%2520Zhao%2520and%2520Liyue%2520Zhang%2520and%2520Zhe%2520Fu%2520and%2520Qihao%2520Zhu%2520and%2520Dejian%2520Yang%2520and%2520Z.%2520F.%2520Wu%2520and%2520Zhibin%2520Gou%2520and%2520Shirong%2520Ma%2520and%2520Hongxuan%2520Tang%2520and%2520Yuxuan%2520Liu%2520and%2520Wenjun%2520Gao%2520and%2520Daya%2520Guo%2520and%2520Chong%2520Ruan%26entry.1292438233%3D%2520%2520We%2520introduce%2520DeepSeek-Prover-V2%252C%2520an%2520open-source%2520large%2520language%2520model%2520designed%250Afor%2520formal%2520theorem%2520proving%2520in%2520Lean%25204%252C%2520with%2520initialization%2520data%2520collected%250Athrough%2520a%2520recursive%2520theorem%2520proving%2520pipeline%2520powered%2520by%2520DeepSeek-V3.%2520The%250Acold-start%2520training%2520procedure%2520begins%2520by%2520prompting%2520DeepSeek-V3%2520to%2520decompose%250Acomplex%2520problems%2520into%2520a%2520series%2520of%2520subgoals.%2520The%2520proofs%2520of%2520resolved%2520subgoals%2520are%250Asynthesized%2520into%2520a%2520chain-of-thought%2520process%252C%2520combined%2520with%2520DeepSeek-V3%2527s%250Astep-by-step%2520reasoning%252C%2520to%2520create%2520an%2520initial%2520cold%2520start%2520for%2520reinforcement%250Alearning.%2520This%2520process%2520enables%2520us%2520to%2520integrate%2520both%2520informal%2520and%2520formal%250Amathematical%2520reasoning%2520into%2520a%2520unified%2520model.%2520The%2520resulting%2520model%252C%250ADeepSeek-Prover-V2-671B%252C%2520achieves%2520state-of-the-art%2520performance%2520in%2520neural%250Atheorem%2520proving%252C%2520reaching%252088.9%2525%2520pass%2520ratio%2520on%2520the%2520MiniF2F-test%2520and%2520solving%252049%250Aout%2520of%2520658%2520problems%2520from%2520PutnamBench.%2520In%2520addition%2520to%2520standard%2520benchmarks%252C%2520we%250Aintroduce%2520ProverBench%252C%2520a%2520collection%2520of%2520325%2520formalized%2520problems%252C%2520to%2520enrich%2520our%250Aevaluation%252C%2520including%252015%2520selected%2520problems%2520from%2520the%2520recent%2520AIME%2520competitions%250A%2528years%252024-25%2529.%2520Further%2520evaluation%2520on%2520these%252015%2520AIME%2520problems%2520shows%2520that%2520the%250Amodel%2520successfully%2520solves%25206%2520of%2520them.%2520In%2520comparison%252C%2520DeepSeek-V3%2520solves%25208%2520of%250Athese%2520problems%2520using%2520majority%2520voting%252C%2520highlighting%2520that%2520the%2520gap%2520between%2520formal%250Aand%2520informal%2520mathematical%2520reasoning%2520in%2520large%2520language%2520models%2520is%2520substantially%250Anarrowing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSeek-Prover-V2%3A%20Advancing%20Formal%20Mathematical%20Reasoning%20via%0A%20%20Reinforcement%20Learning%20for%20Subgoal%20Decomposition&entry.906535625=Z.%20Z.%20Ren%20and%20Zhihong%20Shao%20and%20Junxiao%20Song%20and%20Huajian%20Xin%20and%20Haocheng%20Wang%20and%20Wanjia%20Zhao%20and%20Liyue%20Zhang%20and%20Zhe%20Fu%20and%20Qihao%20Zhu%20and%20Dejian%20Yang%20and%20Z.%20F.%20Wu%20and%20Zhibin%20Gou%20and%20Shirong%20Ma%20and%20Hongxuan%20Tang%20and%20Yuxuan%20Liu%20and%20Wenjun%20Gao%20and%20Daya%20Guo%20and%20Chong%20Ruan&entry.1292438233=%20%20We%20introduce%20DeepSeek-Prover-V2%2C%20an%20open-source%20large%20language%20model%20designed%0Afor%20formal%20theorem%20proving%20in%20Lean%204%2C%20with%20initialization%20data%20collected%0Athrough%20a%20recursive%20theorem%20proving%20pipeline%20powered%20by%20DeepSeek-V3.%20The%0Acold-start%20training%20procedure%20begins%20by%20prompting%20DeepSeek-V3%20to%20decompose%0Acomplex%20problems%20into%20a%20series%20of%20subgoals.%20The%20proofs%20of%20resolved%20subgoals%20are%0Asynthesized%20into%20a%20chain-of-thought%20process%2C%20combined%20with%20DeepSeek-V3%27s%0Astep-by-step%20reasoning%2C%20to%20create%20an%20initial%20cold%20start%20for%20reinforcement%0Alearning.%20This%20process%20enables%20us%20to%20integrate%20both%20informal%20and%20formal%0Amathematical%20reasoning%20into%20a%20unified%20model.%20The%20resulting%20model%2C%0ADeepSeek-Prover-V2-671B%2C%20achieves%20state-of-the-art%20performance%20in%20neural%0Atheorem%20proving%2C%20reaching%2088.9%25%20pass%20ratio%20on%20the%20MiniF2F-test%20and%20solving%2049%0Aout%20of%20658%20problems%20from%20PutnamBench.%20In%20addition%20to%20standard%20benchmarks%2C%20we%0Aintroduce%20ProverBench%2C%20a%20collection%20of%20325%20formalized%20problems%2C%20to%20enrich%20our%0Aevaluation%2C%20including%2015%20selected%20problems%20from%20the%20recent%20AIME%20competitions%0A%28years%2024-25%29.%20Further%20evaluation%20on%20these%2015%20AIME%20problems%20shows%20that%20the%0Amodel%20successfully%20solves%206%20of%20them.%20In%20comparison%2C%20DeepSeek-V3%20solves%208%20of%0Athese%20problems%20using%20majority%20voting%2C%20highlighting%20that%20the%20gap%20between%20formal%0Aand%20informal%20mathematical%20reasoning%20in%20large%20language%20models%20is%20substantially%0Anarrowing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21801v1&entry.124074799=Read"},
{"title": "Uncovering Bias in Large Vision-Language Models at Scale with\n  Counterfactuals", "author": "Phillip Howard and Kathleen C. Fraser and Anahita Bhiwandiwalla and Svetlana Kiritchenko", "abstract": "  With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images,\nproducing over 57 million responses from popular models. Our multi-dimensional\nbias evaluation framework reveals that social attributes such as perceived\nrace, gender, and physical characteristics depicted in images can significantly\ninfluence the generation of toxic content, competency-associated words, harmful\nstereotypes, and numerical ratings of individuals.\n", "link": "http://arxiv.org/abs/2405.20152v2", "date": "2025-04-30", "relevancy": 2.1995, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Bias%20in%20Large%20Vision-Language%20Models%20at%20Scale%20with%0A%20%20Counterfactuals&body=Title%3A%20Uncovering%20Bias%20in%20Large%20Vision-Language%20Models%20at%20Scale%20with%0A%20%20Counterfactuals%0AAuthor%3A%20Phillip%20Howard%20and%20Kathleen%20C.%20Fraser%20and%20Anahita%20Bhiwandiwalla%20and%20Svetlana%20Kiritchenko%0AAbstract%3A%20%20%20With%20the%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20possessing%20increasingly%0Aimpressive%20capabilities%2C%20a%20number%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%0Abeen%20proposed%20to%20augment%20LLMs%20with%20visual%20inputs.%20Such%20models%20condition%0Agenerated%20text%20on%20both%20an%20input%20image%20and%20a%20text%20prompt%2C%20enabling%20a%20variety%20of%0Ause%20cases%20such%20as%20visual%20question%20answering%20and%20multimodal%20chat.%20While%20prior%0Astudies%20have%20examined%20the%20social%20biases%20contained%20in%20text%20generated%20by%20LLMs%2C%0Athis%20topic%20has%20been%20relatively%20unexplored%20in%20LVLMs.%20Examining%20social%20biases%20in%0ALVLMs%20is%20particularly%20challenging%20due%20to%20the%20confounding%20contributions%20of%20bias%0Ainduced%20by%20information%20contained%20across%20the%20text%20and%20visual%20modalities.%20To%0Aaddress%20this%20challenging%20problem%2C%20we%20conduct%20a%20large-scale%20study%20of%20text%0Agenerated%20by%20different%20LVLMs%20under%20counterfactual%20changes%20to%20input%20images%2C%0Aproducing%20over%2057%20million%20responses%20from%20popular%20models.%20Our%20multi-dimensional%0Abias%20evaluation%20framework%20reveals%20that%20social%20attributes%20such%20as%20perceived%0Arace%2C%20gender%2C%20and%20physical%20characteristics%20depicted%20in%20images%20can%20significantly%0Ainfluence%20the%20generation%20of%20toxic%20content%2C%20competency-associated%20words%2C%20harmful%0Astereotypes%2C%20and%20numerical%20ratings%20of%20individuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20152v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Bias%2520in%2520Large%2520Vision-Language%2520Models%2520at%2520Scale%2520with%250A%2520%2520Counterfactuals%26entry.906535625%3DPhillip%2520Howard%2520and%2520Kathleen%2520C.%2520Fraser%2520and%2520Anahita%2520Bhiwandiwalla%2520and%2520Svetlana%2520Kiritchenko%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520possessing%2520increasingly%250Aimpressive%2520capabilities%252C%2520a%2520number%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%250Abeen%2520proposed%2520to%2520augment%2520LLMs%2520with%2520visual%2520inputs.%2520Such%2520models%2520condition%250Agenerated%2520text%2520on%2520both%2520an%2520input%2520image%2520and%2520a%2520text%2520prompt%252C%2520enabling%2520a%2520variety%2520of%250Ause%2520cases%2520such%2520as%2520visual%2520question%2520answering%2520and%2520multimodal%2520chat.%2520While%2520prior%250Astudies%2520have%2520examined%2520the%2520social%2520biases%2520contained%2520in%2520text%2520generated%2520by%2520LLMs%252C%250Athis%2520topic%2520has%2520been%2520relatively%2520unexplored%2520in%2520LVLMs.%2520Examining%2520social%2520biases%2520in%250ALVLMs%2520is%2520particularly%2520challenging%2520due%2520to%2520the%2520confounding%2520contributions%2520of%2520bias%250Ainduced%2520by%2520information%2520contained%2520across%2520the%2520text%2520and%2520visual%2520modalities.%2520To%250Aaddress%2520this%2520challenging%2520problem%252C%2520we%2520conduct%2520a%2520large-scale%2520study%2520of%2520text%250Agenerated%2520by%2520different%2520LVLMs%2520under%2520counterfactual%2520changes%2520to%2520input%2520images%252C%250Aproducing%2520over%252057%2520million%2520responses%2520from%2520popular%2520models.%2520Our%2520multi-dimensional%250Abias%2520evaluation%2520framework%2520reveals%2520that%2520social%2520attributes%2520such%2520as%2520perceived%250Arace%252C%2520gender%252C%2520and%2520physical%2520characteristics%2520depicted%2520in%2520images%2520can%2520significantly%250Ainfluence%2520the%2520generation%2520of%2520toxic%2520content%252C%2520competency-associated%2520words%252C%2520harmful%250Astereotypes%252C%2520and%2520numerical%2520ratings%2520of%2520individuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20152v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Bias%20in%20Large%20Vision-Language%20Models%20at%20Scale%20with%0A%20%20Counterfactuals&entry.906535625=Phillip%20Howard%20and%20Kathleen%20C.%20Fraser%20and%20Anahita%20Bhiwandiwalla%20and%20Svetlana%20Kiritchenko&entry.1292438233=%20%20With%20the%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20possessing%20increasingly%0Aimpressive%20capabilities%2C%20a%20number%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%0Abeen%20proposed%20to%20augment%20LLMs%20with%20visual%20inputs.%20Such%20models%20condition%0Agenerated%20text%20on%20both%20an%20input%20image%20and%20a%20text%20prompt%2C%20enabling%20a%20variety%20of%0Ause%20cases%20such%20as%20visual%20question%20answering%20and%20multimodal%20chat.%20While%20prior%0Astudies%20have%20examined%20the%20social%20biases%20contained%20in%20text%20generated%20by%20LLMs%2C%0Athis%20topic%20has%20been%20relatively%20unexplored%20in%20LVLMs.%20Examining%20social%20biases%20in%0ALVLMs%20is%20particularly%20challenging%20due%20to%20the%20confounding%20contributions%20of%20bias%0Ainduced%20by%20information%20contained%20across%20the%20text%20and%20visual%20modalities.%20To%0Aaddress%20this%20challenging%20problem%2C%20we%20conduct%20a%20large-scale%20study%20of%20text%0Agenerated%20by%20different%20LVLMs%20under%20counterfactual%20changes%20to%20input%20images%2C%0Aproducing%20over%2057%20million%20responses%20from%20popular%20models.%20Our%20multi-dimensional%0Abias%20evaluation%20framework%20reveals%20that%20social%20attributes%20such%20as%20perceived%0Arace%2C%20gender%2C%20and%20physical%20characteristics%20depicted%20in%20images%20can%20significantly%0Ainfluence%20the%20generation%20of%20toxic%20content%2C%20competency-associated%20words%2C%20harmful%0Astereotypes%2C%20and%20numerical%20ratings%20of%20individuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20152v2&entry.124074799=Read"},
{"title": "Consistency-aware Fake Videos Detection on Short Video Platforms", "author": "Junxi Wang and Jize liu and Na Zhang and Yaxiong Wang", "abstract": "  This paper focuses to detect the fake news on the short video platforms.\nWhile significant research efforts have been devoted to this task with notable\nprogress in recent years, current detection accuracy remains suboptimal due to\nthe rapid evolution of content manipulation and generation technologies.\nExisting approaches typically employ a cross-modal fusion strategy that\ndirectly combines raw video data with metadata inputs before applying a\nclassification layer. However, our empirical observations reveal a critical\noversight: manipulated content frequently exhibits inter-modal inconsistencies\nthat could serve as valuable discriminative features, yet remain underutilized\nin contemporary detection frameworks. Motivated by this insight, we propose a\nnovel detection paradigm that explicitly identifies and leverages cross-modal\ncontradictions as discriminative cues. Our approach consists of two core\nmodules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative\nDiagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal\nConsistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used\nto generate pseudo-labels for evaluating cross-modal semantic consistency.\nThen, CMCD extracts [CLS] tokens and computes cosine loss to quantify\ncross-modal inconsistencies. MMCD further integrates multimodal features\nthrough Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).\nMFF employs a co-attention mechanism to enhance semantic interactions across\ndifferent modalities, while a Transformer is utilized for comprehensive feature\nfusion. Meanwhile, PSF further integrates the fake news probability scores\nobtained in the previous step. Extensive experiments on established benchmarks\n(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in\nFake videos detection.\n", "link": "http://arxiv.org/abs/2504.21495v1", "date": "2025-04-30", "relevancy": 2.1913, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5603}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5545}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistency-aware%20Fake%20Videos%20Detection%20on%20Short%20Video%20Platforms&body=Title%3A%20Consistency-aware%20Fake%20Videos%20Detection%20on%20Short%20Video%20Platforms%0AAuthor%3A%20Junxi%20Wang%20and%20Jize%20liu%20and%20Na%20Zhang%20and%20Yaxiong%20Wang%0AAbstract%3A%20%20%20This%20paper%20focuses%20to%20detect%20the%20fake%20news%20on%20the%20short%20video%20platforms.%0AWhile%20significant%20research%20efforts%20have%20been%20devoted%20to%20this%20task%20with%20notable%0Aprogress%20in%20recent%20years%2C%20current%20detection%20accuracy%20remains%20suboptimal%20due%20to%0Athe%20rapid%20evolution%20of%20content%20manipulation%20and%20generation%20technologies.%0AExisting%20approaches%20typically%20employ%20a%20cross-modal%20fusion%20strategy%20that%0Adirectly%20combines%20raw%20video%20data%20with%20metadata%20inputs%20before%20applying%20a%0Aclassification%20layer.%20However%2C%20our%20empirical%20observations%20reveal%20a%20critical%0Aoversight%3A%20manipulated%20content%20frequently%20exhibits%20inter-modal%20inconsistencies%0Athat%20could%20serve%20as%20valuable%20discriminative%20features%2C%20yet%20remain%20underutilized%0Ain%20contemporary%20detection%20frameworks.%20Motivated%20by%20this%20insight%2C%20we%20propose%20a%0Anovel%20detection%20paradigm%20that%20explicitly%20identifies%20and%20leverages%20cross-modal%0Acontradictions%20as%20discriminative%20cues.%20Our%20approach%20consists%20of%20two%20core%0Amodules%3A%20Cross-modal%20Consistency%20Learning%20%28CMCL%29%20and%20Multi-modal%20Collaborative%0ADiagnosis%20%28MMCD%29.%20CMCL%20includes%20Pseudo-label%20Generation%20%28PLG%29%20and%20Cross-modal%0AConsistency%20Diagnosis%20%28CMCD%29.%20In%20PLG%2C%20a%20Multimodal%20Large%20Language%20Model%20is%20used%0Ato%20generate%20pseudo-labels%20for%20evaluating%20cross-modal%20semantic%20consistency.%0AThen%2C%20CMCD%20extracts%20%5BCLS%5D%20tokens%20and%20computes%20cosine%20loss%20to%20quantify%0Across-modal%20inconsistencies.%20MMCD%20further%20integrates%20multimodal%20features%0Athrough%20Multimodal%20Feature%20Fusion%20%28MFF%29%20and%20Probability%20Scores%20Fusion%20%28PSF%29.%0AMFF%20employs%20a%20co-attention%20mechanism%20to%20enhance%20semantic%20interactions%20across%0Adifferent%20modalities%2C%20while%20a%20Transformer%20is%20utilized%20for%20comprehensive%20feature%0Afusion.%20Meanwhile%2C%20PSF%20further%20integrates%20the%20fake%20news%20probability%20scores%0Aobtained%20in%20the%20previous%20step.%20Extensive%20experiments%20on%20established%20benchmarks%0A%28FakeSV%20and%20FakeTT%29%20demonstrate%20our%20model%20exhibits%20outstanding%20performance%20in%0AFake%20videos%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistency-aware%2520Fake%2520Videos%2520Detection%2520on%2520Short%2520Video%2520Platforms%26entry.906535625%3DJunxi%2520Wang%2520and%2520Jize%2520liu%2520and%2520Na%2520Zhang%2520and%2520Yaxiong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520to%2520detect%2520the%2520fake%2520news%2520on%2520the%2520short%2520video%2520platforms.%250AWhile%2520significant%2520research%2520efforts%2520have%2520been%2520devoted%2520to%2520this%2520task%2520with%2520notable%250Aprogress%2520in%2520recent%2520years%252C%2520current%2520detection%2520accuracy%2520remains%2520suboptimal%2520due%2520to%250Athe%2520rapid%2520evolution%2520of%2520content%2520manipulation%2520and%2520generation%2520technologies.%250AExisting%2520approaches%2520typically%2520employ%2520a%2520cross-modal%2520fusion%2520strategy%2520that%250Adirectly%2520combines%2520raw%2520video%2520data%2520with%2520metadata%2520inputs%2520before%2520applying%2520a%250Aclassification%2520layer.%2520However%252C%2520our%2520empirical%2520observations%2520reveal%2520a%2520critical%250Aoversight%253A%2520manipulated%2520content%2520frequently%2520exhibits%2520inter-modal%2520inconsistencies%250Athat%2520could%2520serve%2520as%2520valuable%2520discriminative%2520features%252C%2520yet%2520remain%2520underutilized%250Ain%2520contemporary%2520detection%2520frameworks.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520a%250Anovel%2520detection%2520paradigm%2520that%2520explicitly%2520identifies%2520and%2520leverages%2520cross-modal%250Acontradictions%2520as%2520discriminative%2520cues.%2520Our%2520approach%2520consists%2520of%2520two%2520core%250Amodules%253A%2520Cross-modal%2520Consistency%2520Learning%2520%2528CMCL%2529%2520and%2520Multi-modal%2520Collaborative%250ADiagnosis%2520%2528MMCD%2529.%2520CMCL%2520includes%2520Pseudo-label%2520Generation%2520%2528PLG%2529%2520and%2520Cross-modal%250AConsistency%2520Diagnosis%2520%2528CMCD%2529.%2520In%2520PLG%252C%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520is%2520used%250Ato%2520generate%2520pseudo-labels%2520for%2520evaluating%2520cross-modal%2520semantic%2520consistency.%250AThen%252C%2520CMCD%2520extracts%2520%255BCLS%255D%2520tokens%2520and%2520computes%2520cosine%2520loss%2520to%2520quantify%250Across-modal%2520inconsistencies.%2520MMCD%2520further%2520integrates%2520multimodal%2520features%250Athrough%2520Multimodal%2520Feature%2520Fusion%2520%2528MFF%2529%2520and%2520Probability%2520Scores%2520Fusion%2520%2528PSF%2529.%250AMFF%2520employs%2520a%2520co-attention%2520mechanism%2520to%2520enhance%2520semantic%2520interactions%2520across%250Adifferent%2520modalities%252C%2520while%2520a%2520Transformer%2520is%2520utilized%2520for%2520comprehensive%2520feature%250Afusion.%2520Meanwhile%252C%2520PSF%2520further%2520integrates%2520the%2520fake%2520news%2520probability%2520scores%250Aobtained%2520in%2520the%2520previous%2520step.%2520Extensive%2520experiments%2520on%2520established%2520benchmarks%250A%2528FakeSV%2520and%2520FakeTT%2529%2520demonstrate%2520our%2520model%2520exhibits%2520outstanding%2520performance%2520in%250AFake%2520videos%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistency-aware%20Fake%20Videos%20Detection%20on%20Short%20Video%20Platforms&entry.906535625=Junxi%20Wang%20and%20Jize%20liu%20and%20Na%20Zhang%20and%20Yaxiong%20Wang&entry.1292438233=%20%20This%20paper%20focuses%20to%20detect%20the%20fake%20news%20on%20the%20short%20video%20platforms.%0AWhile%20significant%20research%20efforts%20have%20been%20devoted%20to%20this%20task%20with%20notable%0Aprogress%20in%20recent%20years%2C%20current%20detection%20accuracy%20remains%20suboptimal%20due%20to%0Athe%20rapid%20evolution%20of%20content%20manipulation%20and%20generation%20technologies.%0AExisting%20approaches%20typically%20employ%20a%20cross-modal%20fusion%20strategy%20that%0Adirectly%20combines%20raw%20video%20data%20with%20metadata%20inputs%20before%20applying%20a%0Aclassification%20layer.%20However%2C%20our%20empirical%20observations%20reveal%20a%20critical%0Aoversight%3A%20manipulated%20content%20frequently%20exhibits%20inter-modal%20inconsistencies%0Athat%20could%20serve%20as%20valuable%20discriminative%20features%2C%20yet%20remain%20underutilized%0Ain%20contemporary%20detection%20frameworks.%20Motivated%20by%20this%20insight%2C%20we%20propose%20a%0Anovel%20detection%20paradigm%20that%20explicitly%20identifies%20and%20leverages%20cross-modal%0Acontradictions%20as%20discriminative%20cues.%20Our%20approach%20consists%20of%20two%20core%0Amodules%3A%20Cross-modal%20Consistency%20Learning%20%28CMCL%29%20and%20Multi-modal%20Collaborative%0ADiagnosis%20%28MMCD%29.%20CMCL%20includes%20Pseudo-label%20Generation%20%28PLG%29%20and%20Cross-modal%0AConsistency%20Diagnosis%20%28CMCD%29.%20In%20PLG%2C%20a%20Multimodal%20Large%20Language%20Model%20is%20used%0Ato%20generate%20pseudo-labels%20for%20evaluating%20cross-modal%20semantic%20consistency.%0AThen%2C%20CMCD%20extracts%20%5BCLS%5D%20tokens%20and%20computes%20cosine%20loss%20to%20quantify%0Across-modal%20inconsistencies.%20MMCD%20further%20integrates%20multimodal%20features%0Athrough%20Multimodal%20Feature%20Fusion%20%28MFF%29%20and%20Probability%20Scores%20Fusion%20%28PSF%29.%0AMFF%20employs%20a%20co-attention%20mechanism%20to%20enhance%20semantic%20interactions%20across%0Adifferent%20modalities%2C%20while%20a%20Transformer%20is%20utilized%20for%20comprehensive%20feature%0Afusion.%20Meanwhile%2C%20PSF%20further%20integrates%20the%20fake%20news%20probability%20scores%0Aobtained%20in%20the%20previous%20step.%20Extensive%20experiments%20on%20established%20benchmarks%0A%28FakeSV%20and%20FakeTT%29%20demonstrate%20our%20model%20exhibits%20outstanding%20performance%20in%0AFake%20videos%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21495v1&entry.124074799=Read"},
{"title": "Why Compress What You Can Generate? When GPT-4o Generation Ushers in\n  Image Compression Fields", "author": "Yixin Gao and Xiaohan Pan and Xin Li and Zhibo Chen", "abstract": "  The rapid development of AIGC foundation models has revolutionized the\nparadigm of image compression, which paves the way for the abandonment of most\npixel-level transform and coding, compelling us to ask: why compress what you\ncan generate if the AIGC foundation model is powerful enough to faithfully\ngenerate intricate structure and fine-grained details from nothing more than\nsome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o\nimage generation of OpenAI has achieved impressive cross-modality generation,\nediting, and design capabilities, which motivates us to answer the above\nquestion by exploring its potential in image compression fields. In this work,\nwe investigate two typical compression paradigms: textual coding and multimodal\ncoding (i.e., text + extremely low-resolution image), where all/most\npixel-level information is generated instead of compressing via the advanced\nGPT-4o image generation function. The essential challenge lies in how to\nmaintain semantic and structure consistency during the decoding process. To\novercome this, we propose a structure raster-scan prompt engineering mechanism\nto transform the image into textual space, which is compressed as the condition\nof GPT-4o image generation. Extensive experiments have shown that the\ncombination of our designed structural raster-scan prompts and GPT-4o's image\ngeneration function achieved the impressive performance compared with recent\nmultimodal/generative image compression at ultra-low bitrate, further\nindicating the potential of AIGC generation in image compression fields.\n", "link": "http://arxiv.org/abs/2504.21814v1", "date": "2025-04-30", "relevancy": 2.1801, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5744}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5412}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Compress%20What%20You%20Can%20Generate%3F%20When%20GPT-4o%20Generation%20Ushers%20in%0A%20%20Image%20Compression%20Fields&body=Title%3A%20Why%20Compress%20What%20You%20Can%20Generate%3F%20When%20GPT-4o%20Generation%20Ushers%20in%0A%20%20Image%20Compression%20Fields%0AAuthor%3A%20Yixin%20Gao%20and%20Xiaohan%20Pan%20and%20Xin%20Li%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20AIGC%20foundation%20models%20has%20revolutionized%20the%0Aparadigm%20of%20image%20compression%2C%20which%20paves%20the%20way%20for%20the%20abandonment%20of%20most%0Apixel-level%20transform%20and%20coding%2C%20compelling%20us%20to%20ask%3A%20why%20compress%20what%20you%0Acan%20generate%20if%20the%20AIGC%20foundation%20model%20is%20powerful%20enough%20to%20faithfully%0Agenerate%20intricate%20structure%20and%20fine-grained%20details%20from%20nothing%20more%20than%0Asome%20compact%20descriptors%2C%20i.e.%2C%20texts%2C%20or%20cues.%20Fortunately%2C%20recent%20GPT-4o%0Aimage%20generation%20of%20OpenAI%20has%20achieved%20impressive%20cross-modality%20generation%2C%0Aediting%2C%20and%20design%20capabilities%2C%20which%20motivates%20us%20to%20answer%20the%20above%0Aquestion%20by%20exploring%20its%20potential%20in%20image%20compression%20fields.%20In%20this%20work%2C%0Awe%20investigate%20two%20typical%20compression%20paradigms%3A%20textual%20coding%20and%20multimodal%0Acoding%20%28i.e.%2C%20text%20%2B%20extremely%20low-resolution%20image%29%2C%20where%20all/most%0Apixel-level%20information%20is%20generated%20instead%20of%20compressing%20via%20the%20advanced%0AGPT-4o%20image%20generation%20function.%20The%20essential%20challenge%20lies%20in%20how%20to%0Amaintain%20semantic%20and%20structure%20consistency%20during%20the%20decoding%20process.%20To%0Aovercome%20this%2C%20we%20propose%20a%20structure%20raster-scan%20prompt%20engineering%20mechanism%0Ato%20transform%20the%20image%20into%20textual%20space%2C%20which%20is%20compressed%20as%20the%20condition%0Aof%20GPT-4o%20image%20generation.%20Extensive%20experiments%20have%20shown%20that%20the%0Acombination%20of%20our%20designed%20structural%20raster-scan%20prompts%20and%20GPT-4o%27s%20image%0Ageneration%20function%20achieved%20the%20impressive%20performance%20compared%20with%20recent%0Amultimodal/generative%20image%20compression%20at%20ultra-low%20bitrate%2C%20further%0Aindicating%20the%20potential%20of%20AIGC%20generation%20in%20image%20compression%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Compress%2520What%2520You%2520Can%2520Generate%253F%2520When%2520GPT-4o%2520Generation%2520Ushers%2520in%250A%2520%2520Image%2520Compression%2520Fields%26entry.906535625%3DYixin%2520Gao%2520and%2520Xiaohan%2520Pan%2520and%2520Xin%2520Li%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520AIGC%2520foundation%2520models%2520has%2520revolutionized%2520the%250Aparadigm%2520of%2520image%2520compression%252C%2520which%2520paves%2520the%2520way%2520for%2520the%2520abandonment%2520of%2520most%250Apixel-level%2520transform%2520and%2520coding%252C%2520compelling%2520us%2520to%2520ask%253A%2520why%2520compress%2520what%2520you%250Acan%2520generate%2520if%2520the%2520AIGC%2520foundation%2520model%2520is%2520powerful%2520enough%2520to%2520faithfully%250Agenerate%2520intricate%2520structure%2520and%2520fine-grained%2520details%2520from%2520nothing%2520more%2520than%250Asome%2520compact%2520descriptors%252C%2520i.e.%252C%2520texts%252C%2520or%2520cues.%2520Fortunately%252C%2520recent%2520GPT-4o%250Aimage%2520generation%2520of%2520OpenAI%2520has%2520achieved%2520impressive%2520cross-modality%2520generation%252C%250Aediting%252C%2520and%2520design%2520capabilities%252C%2520which%2520motivates%2520us%2520to%2520answer%2520the%2520above%250Aquestion%2520by%2520exploring%2520its%2520potential%2520in%2520image%2520compression%2520fields.%2520In%2520this%2520work%252C%250Awe%2520investigate%2520two%2520typical%2520compression%2520paradigms%253A%2520textual%2520coding%2520and%2520multimodal%250Acoding%2520%2528i.e.%252C%2520text%2520%252B%2520extremely%2520low-resolution%2520image%2529%252C%2520where%2520all/most%250Apixel-level%2520information%2520is%2520generated%2520instead%2520of%2520compressing%2520via%2520the%2520advanced%250AGPT-4o%2520image%2520generation%2520function.%2520The%2520essential%2520challenge%2520lies%2520in%2520how%2520to%250Amaintain%2520semantic%2520and%2520structure%2520consistency%2520during%2520the%2520decoding%2520process.%2520To%250Aovercome%2520this%252C%2520we%2520propose%2520a%2520structure%2520raster-scan%2520prompt%2520engineering%2520mechanism%250Ato%2520transform%2520the%2520image%2520into%2520textual%2520space%252C%2520which%2520is%2520compressed%2520as%2520the%2520condition%250Aof%2520GPT-4o%2520image%2520generation.%2520Extensive%2520experiments%2520have%2520shown%2520that%2520the%250Acombination%2520of%2520our%2520designed%2520structural%2520raster-scan%2520prompts%2520and%2520GPT-4o%2527s%2520image%250Ageneration%2520function%2520achieved%2520the%2520impressive%2520performance%2520compared%2520with%2520recent%250Amultimodal/generative%2520image%2520compression%2520at%2520ultra-low%2520bitrate%252C%2520further%250Aindicating%2520the%2520potential%2520of%2520AIGC%2520generation%2520in%2520image%2520compression%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Compress%20What%20You%20Can%20Generate%3F%20When%20GPT-4o%20Generation%20Ushers%20in%0A%20%20Image%20Compression%20Fields&entry.906535625=Yixin%20Gao%20and%20Xiaohan%20Pan%20and%20Xin%20Li%20and%20Zhibo%20Chen&entry.1292438233=%20%20The%20rapid%20development%20of%20AIGC%20foundation%20models%20has%20revolutionized%20the%0Aparadigm%20of%20image%20compression%2C%20which%20paves%20the%20way%20for%20the%20abandonment%20of%20most%0Apixel-level%20transform%20and%20coding%2C%20compelling%20us%20to%20ask%3A%20why%20compress%20what%20you%0Acan%20generate%20if%20the%20AIGC%20foundation%20model%20is%20powerful%20enough%20to%20faithfully%0Agenerate%20intricate%20structure%20and%20fine-grained%20details%20from%20nothing%20more%20than%0Asome%20compact%20descriptors%2C%20i.e.%2C%20texts%2C%20or%20cues.%20Fortunately%2C%20recent%20GPT-4o%0Aimage%20generation%20of%20OpenAI%20has%20achieved%20impressive%20cross-modality%20generation%2C%0Aediting%2C%20and%20design%20capabilities%2C%20which%20motivates%20us%20to%20answer%20the%20above%0Aquestion%20by%20exploring%20its%20potential%20in%20image%20compression%20fields.%20In%20this%20work%2C%0Awe%20investigate%20two%20typical%20compression%20paradigms%3A%20textual%20coding%20and%20multimodal%0Acoding%20%28i.e.%2C%20text%20%2B%20extremely%20low-resolution%20image%29%2C%20where%20all/most%0Apixel-level%20information%20is%20generated%20instead%20of%20compressing%20via%20the%20advanced%0AGPT-4o%20image%20generation%20function.%20The%20essential%20challenge%20lies%20in%20how%20to%0Amaintain%20semantic%20and%20structure%20consistency%20during%20the%20decoding%20process.%20To%0Aovercome%20this%2C%20we%20propose%20a%20structure%20raster-scan%20prompt%20engineering%20mechanism%0Ato%20transform%20the%20image%20into%20textual%20space%2C%20which%20is%20compressed%20as%20the%20condition%0Aof%20GPT-4o%20image%20generation.%20Extensive%20experiments%20have%20shown%20that%20the%0Acombination%20of%20our%20designed%20structural%20raster-scan%20prompts%20and%20GPT-4o%27s%20image%0Ageneration%20function%20achieved%20the%20impressive%20performance%20compared%20with%20recent%0Amultimodal/generative%20image%20compression%20at%20ultra-low%20bitrate%2C%20further%0Aindicating%20the%20potential%20of%20AIGC%20generation%20in%20image%20compression%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21814v1&entry.124074799=Read"},
{"title": "RDF-Based Structured Quality Assessment Representation of Multilingual\n  LLM Evaluations", "author": "Jonas Gwozdz and Andreas Both", "abstract": "  Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy.\n", "link": "http://arxiv.org/abs/2504.21605v1", "date": "2025-04-30", "relevancy": 2.1668, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RDF-Based%20Structured%20Quality%20Assessment%20Representation%20of%20Multilingual%0A%20%20LLM%20Evaluations&body=Title%3A%20RDF-Based%20Structured%20Quality%20Assessment%20Representation%20of%20Multilingual%0A%20%20LLM%20Evaluations%0AAuthor%3A%20Jonas%20Gwozdz%20and%20Andreas%20Both%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20serve%20as%20knowledge%20interfaces%2C%20yet%0Asystematically%20assessing%20their%20reliability%20with%20conflicting%20information%20remains%0Adifficult.%20We%20propose%20an%20RDF-based%20framework%20to%20assess%20multilingual%20LLM%0Aquality%2C%20focusing%20on%20knowledge%20conflicts.%20Our%20approach%20captures%20model%20responses%0Aacross%20four%20distinct%20context%20conditions%20%28complete%2C%20incomplete%2C%20conflicting%2C%20and%0Ano-context%20information%29%20in%20German%20and%20English.%20This%20structured%20representation%0Aenables%20the%20comprehensive%20analysis%20of%20knowledge%20leakage-where%20models%20favor%0Atraining%20data%20over%20provided%20context-error%20detection%2C%20and%20multilingual%0Aconsistency.%20We%20demonstrate%20the%20framework%20through%20a%20fire%20safety%20domain%0Aexperiment%2C%20revealing%20critical%20patterns%20in%20context%20prioritization%20and%0Alanguage-specific%20performance%2C%20and%20demonstrating%20that%20our%20vocabulary%20was%0Asufficient%20to%20express%20every%20assessment%20facet%20encountered%20in%20the%2028-question%0Astudy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRDF-Based%2520Structured%2520Quality%2520Assessment%2520Representation%2520of%2520Multilingual%250A%2520%2520LLM%2520Evaluations%26entry.906535625%3DJonas%2520Gwozdz%2520and%2520Andreas%2520Both%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520increasingly%2520serve%2520as%2520knowledge%2520interfaces%252C%2520yet%250Asystematically%2520assessing%2520their%2520reliability%2520with%2520conflicting%2520information%2520remains%250Adifficult.%2520We%2520propose%2520an%2520RDF-based%2520framework%2520to%2520assess%2520multilingual%2520LLM%250Aquality%252C%2520focusing%2520on%2520knowledge%2520conflicts.%2520Our%2520approach%2520captures%2520model%2520responses%250Aacross%2520four%2520distinct%2520context%2520conditions%2520%2528complete%252C%2520incomplete%252C%2520conflicting%252C%2520and%250Ano-context%2520information%2529%2520in%2520German%2520and%2520English.%2520This%2520structured%2520representation%250Aenables%2520the%2520comprehensive%2520analysis%2520of%2520knowledge%2520leakage-where%2520models%2520favor%250Atraining%2520data%2520over%2520provided%2520context-error%2520detection%252C%2520and%2520multilingual%250Aconsistency.%2520We%2520demonstrate%2520the%2520framework%2520through%2520a%2520fire%2520safety%2520domain%250Aexperiment%252C%2520revealing%2520critical%2520patterns%2520in%2520context%2520prioritization%2520and%250Alanguage-specific%2520performance%252C%2520and%2520demonstrating%2520that%2520our%2520vocabulary%2520was%250Asufficient%2520to%2520express%2520every%2520assessment%2520facet%2520encountered%2520in%2520the%252028-question%250Astudy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RDF-Based%20Structured%20Quality%20Assessment%20Representation%20of%20Multilingual%0A%20%20LLM%20Evaluations&entry.906535625=Jonas%20Gwozdz%20and%20Andreas%20Both&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20serve%20as%20knowledge%20interfaces%2C%20yet%0Asystematically%20assessing%20their%20reliability%20with%20conflicting%20information%20remains%0Adifficult.%20We%20propose%20an%20RDF-based%20framework%20to%20assess%20multilingual%20LLM%0Aquality%2C%20focusing%20on%20knowledge%20conflicts.%20Our%20approach%20captures%20model%20responses%0Aacross%20four%20distinct%20context%20conditions%20%28complete%2C%20incomplete%2C%20conflicting%2C%20and%0Ano-context%20information%29%20in%20German%20and%20English.%20This%20structured%20representation%0Aenables%20the%20comprehensive%20analysis%20of%20knowledge%20leakage-where%20models%20favor%0Atraining%20data%20over%20provided%20context-error%20detection%2C%20and%20multilingual%0Aconsistency.%20We%20demonstrate%20the%20framework%20through%20a%20fire%20safety%20domain%0Aexperiment%2C%20revealing%20critical%20patterns%20in%20context%20prioritization%20and%0Alanguage-specific%20performance%2C%20and%20demonstrating%20that%20our%20vocabulary%20was%0Asufficient%20to%20express%20every%20assessment%20facet%20encountered%20in%20the%2028-question%0Astudy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21605v1&entry.124074799=Read"},
{"title": "LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature\n  Transforms", "author": "Ayman A. Ameen and Thomas Richter and Andr\u00e9 Kaup", "abstract": "  Current learned image compression models typically exhibit high complexity,\nwhich demands significant computational resources. To overcome these\nchallenges, we propose an innovative approach that employs hierarchical feature\nextraction transforms to significantly reduce complexity while preserving bit\nrate reduction efficiency. Our novel architecture achieves this by using fewer\nchannels for high spatial resolution inputs/feature maps. On the other hand,\nfeature maps with a large number of channels have reduced spatial dimensions,\nthereby cutting down on computational load without sacrificing performance.\nThis strategy effectively reduces the forward pass complexity from \\(1256 \\,\n\\text{kMAC/Pixel}\\) to just \\(270 \\, \\text{kMAC/Pixel}\\). As a result, the\nreduced complexity model can open the way for learned image compression models\nto operate efficiently across various devices and pave the way for the\ndevelopment of new architectures in image compression technology.\n", "link": "http://arxiv.org/abs/2504.21778v1", "date": "2025-04-30", "relevancy": 2.1628, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5659}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5387}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoC-LIC%3A%20Low%20Complexity%20Learned%20Image%20Coding%20Using%20Hierarchical%20Feature%0A%20%20Transforms&body=Title%3A%20LoC-LIC%3A%20Low%20Complexity%20Learned%20Image%20Coding%20Using%20Hierarchical%20Feature%0A%20%20Transforms%0AAuthor%3A%20Ayman%20A.%20Ameen%20and%20Thomas%20Richter%20and%20Andr%C3%A9%20Kaup%0AAbstract%3A%20%20%20Current%20learned%20image%20compression%20models%20typically%20exhibit%20high%20complexity%2C%0Awhich%20demands%20significant%20computational%20resources.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20an%20innovative%20approach%20that%20employs%20hierarchical%20feature%0Aextraction%20transforms%20to%20significantly%20reduce%20complexity%20while%20preserving%20bit%0Arate%20reduction%20efficiency.%20Our%20novel%20architecture%20achieves%20this%20by%20using%20fewer%0Achannels%20for%20high%20spatial%20resolution%20inputs/feature%20maps.%20On%20the%20other%20hand%2C%0Afeature%20maps%20with%20a%20large%20number%20of%20channels%20have%20reduced%20spatial%20dimensions%2C%0Athereby%20cutting%20down%20on%20computational%20load%20without%20sacrificing%20performance.%0AThis%20strategy%20effectively%20reduces%20the%20forward%20pass%20complexity%20from%20%5C%281256%20%5C%2C%0A%5Ctext%7BkMAC/Pixel%7D%5C%29%20to%20just%20%5C%28270%20%5C%2C%20%5Ctext%7BkMAC/Pixel%7D%5C%29.%20As%20a%20result%2C%20the%0Areduced%20complexity%20model%20can%20open%20the%20way%20for%20learned%20image%20compression%20models%0Ato%20operate%20efficiently%20across%20various%20devices%20and%20pave%20the%20way%20for%20the%0Adevelopment%20of%20new%20architectures%20in%20image%20compression%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoC-LIC%253A%2520Low%2520Complexity%2520Learned%2520Image%2520Coding%2520Using%2520Hierarchical%2520Feature%250A%2520%2520Transforms%26entry.906535625%3DAyman%2520A.%2520Ameen%2520and%2520Thomas%2520Richter%2520and%2520Andr%25C3%25A9%2520Kaup%26entry.1292438233%3D%2520%2520Current%2520learned%2520image%2520compression%2520models%2520typically%2520exhibit%2520high%2520complexity%252C%250Awhich%2520demands%2520significant%2520computational%2520resources.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520an%2520innovative%2520approach%2520that%2520employs%2520hierarchical%2520feature%250Aextraction%2520transforms%2520to%2520significantly%2520reduce%2520complexity%2520while%2520preserving%2520bit%250Arate%2520reduction%2520efficiency.%2520Our%2520novel%2520architecture%2520achieves%2520this%2520by%2520using%2520fewer%250Achannels%2520for%2520high%2520spatial%2520resolution%2520inputs/feature%2520maps.%2520On%2520the%2520other%2520hand%252C%250Afeature%2520maps%2520with%2520a%2520large%2520number%2520of%2520channels%2520have%2520reduced%2520spatial%2520dimensions%252C%250Athereby%2520cutting%2520down%2520on%2520computational%2520load%2520without%2520sacrificing%2520performance.%250AThis%2520strategy%2520effectively%2520reduces%2520the%2520forward%2520pass%2520complexity%2520from%2520%255C%25281256%2520%255C%252C%250A%255Ctext%257BkMAC/Pixel%257D%255C%2529%2520to%2520just%2520%255C%2528270%2520%255C%252C%2520%255Ctext%257BkMAC/Pixel%257D%255C%2529.%2520As%2520a%2520result%252C%2520the%250Areduced%2520complexity%2520model%2520can%2520open%2520the%2520way%2520for%2520learned%2520image%2520compression%2520models%250Ato%2520operate%2520efficiently%2520across%2520various%2520devices%2520and%2520pave%2520the%2520way%2520for%2520the%250Adevelopment%2520of%2520new%2520architectures%2520in%2520image%2520compression%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoC-LIC%3A%20Low%20Complexity%20Learned%20Image%20Coding%20Using%20Hierarchical%20Feature%0A%20%20Transforms&entry.906535625=Ayman%20A.%20Ameen%20and%20Thomas%20Richter%20and%20Andr%C3%A9%20Kaup&entry.1292438233=%20%20Current%20learned%20image%20compression%20models%20typically%20exhibit%20high%20complexity%2C%0Awhich%20demands%20significant%20computational%20resources.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20an%20innovative%20approach%20that%20employs%20hierarchical%20feature%0Aextraction%20transforms%20to%20significantly%20reduce%20complexity%20while%20preserving%20bit%0Arate%20reduction%20efficiency.%20Our%20novel%20architecture%20achieves%20this%20by%20using%20fewer%0Achannels%20for%20high%20spatial%20resolution%20inputs/feature%20maps.%20On%20the%20other%20hand%2C%0Afeature%20maps%20with%20a%20large%20number%20of%20channels%20have%20reduced%20spatial%20dimensions%2C%0Athereby%20cutting%20down%20on%20computational%20load%20without%20sacrificing%20performance.%0AThis%20strategy%20effectively%20reduces%20the%20forward%20pass%20complexity%20from%20%5C%281256%20%5C%2C%0A%5Ctext%7BkMAC/Pixel%7D%5C%29%20to%20just%20%5C%28270%20%5C%2C%20%5Ctext%7BkMAC/Pixel%7D%5C%29.%20As%20a%20result%2C%20the%0Areduced%20complexity%20model%20can%20open%20the%20way%20for%20learned%20image%20compression%20models%0Ato%20operate%20efficiently%20across%20various%20devices%20and%20pave%20the%20way%20for%20the%0Adevelopment%20of%20new%20architectures%20in%20image%20compression%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21778v1&entry.124074799=Read"},
{"title": "Active Light Modulation to Counter Manipulation of Speech Visual Content", "author": "Hadleigh Schwartz and Xiaofeng Yan and Charles J. Carver and Xia Zhou", "abstract": "  High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies.\n", "link": "http://arxiv.org/abs/2504.21846v1", "date": "2025-04-30", "relevancy": 2.1429, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5642}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Light%20Modulation%20to%20Counter%20Manipulation%20of%20Speech%20Visual%20Content&body=Title%3A%20Active%20Light%20Modulation%20to%20Counter%20Manipulation%20of%20Speech%20Visual%20Content%0AAuthor%3A%20Hadleigh%20Schwartz%20and%20Xiaofeng%20Yan%20and%20Charles%20J.%20Carver%20and%20Xia%20Zhou%0AAbstract%3A%20%20%20High-profile%20speech%20videos%20are%20prime%20targets%20for%20falsification%2C%20owing%20to%0Atheir%20accessibility%20and%20influence.%20This%20work%20proposes%20Spotlight%2C%20a%20low-overhead%0Aand%20unobtrusive%20system%20for%20protecting%20live%20speech%20videos%20from%20visual%0Afalsification%20of%20speaker%20identity%20and%20lip%20and%20facial%20motion.%20Unlike%20predominant%0Afalsification%20detection%20methods%20operating%20in%20the%20digital%20domain%2C%20Spotlight%0Acreates%20dynamic%20physical%20signatures%20at%20the%20event%20site%20and%20embeds%20them%20into%20all%0Avideo%20recordings%20via%20imperceptible%20modulated%20light.%20These%20physical%20signatures%0Aencode%20semantically-meaningful%20features%20unique%20to%20the%20speech%20event%2C%20including%0Athe%20speaker%27s%20identity%20and%20facial%20motion%2C%20and%20are%20cryptographically-secured%20to%0Aprevent%20spoofing.%20The%20signatures%20can%20be%20extracted%20from%20any%20video%20downstream%20and%0Avalidated%20against%20the%20portrayed%20speech%20content%20to%20check%20its%20integrity.%20Key%0Aelements%20of%20Spotlight%20include%20%281%29%20a%20framework%20for%20generating%20extremely%20compact%0A%28i.e.%2C%20150-bit%29%2C%20pose-invariant%20speech%20video%20features%2C%20based%20on%0Alocality-sensitive%20hashing%3B%20and%20%282%29%20an%20optical%20modulation%20scheme%20that%20embeds%0A%3E200%20bps%20into%20video%20while%20remaining%20imperceptible%20both%20in%20video%20and%20live.%0APrototype%20experiments%20on%20extensive%20video%20datasets%20show%20Spotlight%20achieves%20AUCs%0A%24%5Cgeq%24%200.99%20and%20an%20overall%20true%20positive%20rate%20of%20100%25%20in%20detecting%20falsified%0Avideos.%20Further%2C%20Spotlight%20is%20highly%20robust%20across%20recording%20conditions%2C%20video%0Apost-processing%20techniques%2C%20and%20white-box%20adversarial%20attacks%20on%20its%20video%0Afeature%20extraction%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Light%2520Modulation%2520to%2520Counter%2520Manipulation%2520of%2520Speech%2520Visual%2520Content%26entry.906535625%3DHadleigh%2520Schwartz%2520and%2520Xiaofeng%2520Yan%2520and%2520Charles%2520J.%2520Carver%2520and%2520Xia%2520Zhou%26entry.1292438233%3D%2520%2520High-profile%2520speech%2520videos%2520are%2520prime%2520targets%2520for%2520falsification%252C%2520owing%2520to%250Atheir%2520accessibility%2520and%2520influence.%2520This%2520work%2520proposes%2520Spotlight%252C%2520a%2520low-overhead%250Aand%2520unobtrusive%2520system%2520for%2520protecting%2520live%2520speech%2520videos%2520from%2520visual%250Afalsification%2520of%2520speaker%2520identity%2520and%2520lip%2520and%2520facial%2520motion.%2520Unlike%2520predominant%250Afalsification%2520detection%2520methods%2520operating%2520in%2520the%2520digital%2520domain%252C%2520Spotlight%250Acreates%2520dynamic%2520physical%2520signatures%2520at%2520the%2520event%2520site%2520and%2520embeds%2520them%2520into%2520all%250Avideo%2520recordings%2520via%2520imperceptible%2520modulated%2520light.%2520These%2520physical%2520signatures%250Aencode%2520semantically-meaningful%2520features%2520unique%2520to%2520the%2520speech%2520event%252C%2520including%250Athe%2520speaker%2527s%2520identity%2520and%2520facial%2520motion%252C%2520and%2520are%2520cryptographically-secured%2520to%250Aprevent%2520spoofing.%2520The%2520signatures%2520can%2520be%2520extracted%2520from%2520any%2520video%2520downstream%2520and%250Avalidated%2520against%2520the%2520portrayed%2520speech%2520content%2520to%2520check%2520its%2520integrity.%2520Key%250Aelements%2520of%2520Spotlight%2520include%2520%25281%2529%2520a%2520framework%2520for%2520generating%2520extremely%2520compact%250A%2528i.e.%252C%2520150-bit%2529%252C%2520pose-invariant%2520speech%2520video%2520features%252C%2520based%2520on%250Alocality-sensitive%2520hashing%253B%2520and%2520%25282%2529%2520an%2520optical%2520modulation%2520scheme%2520that%2520embeds%250A%253E200%2520bps%2520into%2520video%2520while%2520remaining%2520imperceptible%2520both%2520in%2520video%2520and%2520live.%250APrototype%2520experiments%2520on%2520extensive%2520video%2520datasets%2520show%2520Spotlight%2520achieves%2520AUCs%250A%2524%255Cgeq%2524%25200.99%2520and%2520an%2520overall%2520true%2520positive%2520rate%2520of%2520100%2525%2520in%2520detecting%2520falsified%250Avideos.%2520Further%252C%2520Spotlight%2520is%2520highly%2520robust%2520across%2520recording%2520conditions%252C%2520video%250Apost-processing%2520techniques%252C%2520and%2520white-box%2520adversarial%2520attacks%2520on%2520its%2520video%250Afeature%2520extraction%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Light%20Modulation%20to%20Counter%20Manipulation%20of%20Speech%20Visual%20Content&entry.906535625=Hadleigh%20Schwartz%20and%20Xiaofeng%20Yan%20and%20Charles%20J.%20Carver%20and%20Xia%20Zhou&entry.1292438233=%20%20High-profile%20speech%20videos%20are%20prime%20targets%20for%20falsification%2C%20owing%20to%0Atheir%20accessibility%20and%20influence.%20This%20work%20proposes%20Spotlight%2C%20a%20low-overhead%0Aand%20unobtrusive%20system%20for%20protecting%20live%20speech%20videos%20from%20visual%0Afalsification%20of%20speaker%20identity%20and%20lip%20and%20facial%20motion.%20Unlike%20predominant%0Afalsification%20detection%20methods%20operating%20in%20the%20digital%20domain%2C%20Spotlight%0Acreates%20dynamic%20physical%20signatures%20at%20the%20event%20site%20and%20embeds%20them%20into%20all%0Avideo%20recordings%20via%20imperceptible%20modulated%20light.%20These%20physical%20signatures%0Aencode%20semantically-meaningful%20features%20unique%20to%20the%20speech%20event%2C%20including%0Athe%20speaker%27s%20identity%20and%20facial%20motion%2C%20and%20are%20cryptographically-secured%20to%0Aprevent%20spoofing.%20The%20signatures%20can%20be%20extracted%20from%20any%20video%20downstream%20and%0Avalidated%20against%20the%20portrayed%20speech%20content%20to%20check%20its%20integrity.%20Key%0Aelements%20of%20Spotlight%20include%20%281%29%20a%20framework%20for%20generating%20extremely%20compact%0A%28i.e.%2C%20150-bit%29%2C%20pose-invariant%20speech%20video%20features%2C%20based%20on%0Alocality-sensitive%20hashing%3B%20and%20%282%29%20an%20optical%20modulation%20scheme%20that%20embeds%0A%3E200%20bps%20into%20video%20while%20remaining%20imperceptible%20both%20in%20video%20and%20live.%0APrototype%20experiments%20on%20extensive%20video%20datasets%20show%20Spotlight%20achieves%20AUCs%0A%24%5Cgeq%24%200.99%20and%20an%20overall%20true%20positive%20rate%20of%20100%25%20in%20detecting%20falsified%0Avideos.%20Further%2C%20Spotlight%20is%20highly%20robust%20across%20recording%20conditions%2C%20video%0Apost-processing%20techniques%2C%20and%20white-box%20adversarial%20attacks%20on%20its%20video%0Afeature%20extraction%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21846v1&entry.124074799=Read"},
{"title": "Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation", "author": "Alessia Hu and Regina Beets-Tan and Lishan Cai and Eduardo Pooch", "abstract": "  Magnetic Resonance Imaging (MRI) plays an important role in identifying\nclinically significant prostate cancer (csPCa), yet automated methods face\nchallenges such as data imbalance, variable tumor sizes, and a lack of\nannotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which\nincorporates anomaly maps derived from biparametric MRI sequences into a deep\nlearning-based segmentation framework to improve csPCa identification. We\nconduct a comparative analysis of anomaly detection methods and evaluate the\nintegration of anomaly maps into the segmentation pipeline. Anomaly maps,\ngenerated using Fixed-Point GAN reconstruction, highlight deviations from\nnormal prostate tissue, guiding the segmentation model to potential cancerous\nregions. We compare the performance by using the average score, computed as the\nmean of the AUROC and Average Precision (AP). On the external test set, adU-Net\nachieves the best average score of 0.618, outperforming the baseline nnU-Net\nmodel (0.605). The results demonstrate that incorporating anomaly detection\ninto segmentation improves generalization and performance, particularly with\nADC-based anomaly maps, offering a promising direction for automated csPCa\nidentification.\n", "link": "http://arxiv.org/abs/2504.21789v1", "date": "2025-04-30", "relevancy": 2.142, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5611}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5317}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomaly-Driven%20Approach%20for%20Enhanced%20Prostate%20Cancer%20Segmentation&body=Title%3A%20Anomaly-Driven%20Approach%20for%20Enhanced%20Prostate%20Cancer%20Segmentation%0AAuthor%3A%20Alessia%20Hu%20and%20Regina%20Beets-Tan%20and%20Lishan%20Cai%20and%20Eduardo%20Pooch%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20plays%20an%20important%20role%20in%20identifying%0Aclinically%20significant%20prostate%20cancer%20%28csPCa%29%2C%20yet%20automated%20methods%20face%0Achallenges%20such%20as%20data%20imbalance%2C%20variable%20tumor%20sizes%2C%20and%20a%20lack%20of%0Aannotated%20data.%20This%20study%20introduces%20Anomaly-Driven%20U-Net%20%28adU-Net%29%2C%20which%0Aincorporates%20anomaly%20maps%20derived%20from%20biparametric%20MRI%20sequences%20into%20a%20deep%0Alearning-based%20segmentation%20framework%20to%20improve%20csPCa%20identification.%20We%0Aconduct%20a%20comparative%20analysis%20of%20anomaly%20detection%20methods%20and%20evaluate%20the%0Aintegration%20of%20anomaly%20maps%20into%20the%20segmentation%20pipeline.%20Anomaly%20maps%2C%0Agenerated%20using%20Fixed-Point%20GAN%20reconstruction%2C%20highlight%20deviations%20from%0Anormal%20prostate%20tissue%2C%20guiding%20the%20segmentation%20model%20to%20potential%20cancerous%0Aregions.%20We%20compare%20the%20performance%20by%20using%20the%20average%20score%2C%20computed%20as%20the%0Amean%20of%20the%20AUROC%20and%20Average%20Precision%20%28AP%29.%20On%20the%20external%20test%20set%2C%20adU-Net%0Aachieves%20the%20best%20average%20score%20of%200.618%2C%20outperforming%20the%20baseline%20nnU-Net%0Amodel%20%280.605%29.%20The%20results%20demonstrate%20that%20incorporating%20anomaly%20detection%0Ainto%20segmentation%20improves%20generalization%20and%20performance%2C%20particularly%20with%0AADC-based%20anomaly%20maps%2C%20offering%20a%20promising%20direction%20for%20automated%20csPCa%0Aidentification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaly-Driven%2520Approach%2520for%2520Enhanced%2520Prostate%2520Cancer%2520Segmentation%26entry.906535625%3DAlessia%2520Hu%2520and%2520Regina%2520Beets-Tan%2520and%2520Lishan%2520Cai%2520and%2520Eduardo%2520Pooch%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520plays%2520an%2520important%2520role%2520in%2520identifying%250Aclinically%2520significant%2520prostate%2520cancer%2520%2528csPCa%2529%252C%2520yet%2520automated%2520methods%2520face%250Achallenges%2520such%2520as%2520data%2520imbalance%252C%2520variable%2520tumor%2520sizes%252C%2520and%2520a%2520lack%2520of%250Aannotated%2520data.%2520This%2520study%2520introduces%2520Anomaly-Driven%2520U-Net%2520%2528adU-Net%2529%252C%2520which%250Aincorporates%2520anomaly%2520maps%2520derived%2520from%2520biparametric%2520MRI%2520sequences%2520into%2520a%2520deep%250Alearning-based%2520segmentation%2520framework%2520to%2520improve%2520csPCa%2520identification.%2520We%250Aconduct%2520a%2520comparative%2520analysis%2520of%2520anomaly%2520detection%2520methods%2520and%2520evaluate%2520the%250Aintegration%2520of%2520anomaly%2520maps%2520into%2520the%2520segmentation%2520pipeline.%2520Anomaly%2520maps%252C%250Agenerated%2520using%2520Fixed-Point%2520GAN%2520reconstruction%252C%2520highlight%2520deviations%2520from%250Anormal%2520prostate%2520tissue%252C%2520guiding%2520the%2520segmentation%2520model%2520to%2520potential%2520cancerous%250Aregions.%2520We%2520compare%2520the%2520performance%2520by%2520using%2520the%2520average%2520score%252C%2520computed%2520as%2520the%250Amean%2520of%2520the%2520AUROC%2520and%2520Average%2520Precision%2520%2528AP%2529.%2520On%2520the%2520external%2520test%2520set%252C%2520adU-Net%250Aachieves%2520the%2520best%2520average%2520score%2520of%25200.618%252C%2520outperforming%2520the%2520baseline%2520nnU-Net%250Amodel%2520%25280.605%2529.%2520The%2520results%2520demonstrate%2520that%2520incorporating%2520anomaly%2520detection%250Ainto%2520segmentation%2520improves%2520generalization%2520and%2520performance%252C%2520particularly%2520with%250AADC-based%2520anomaly%2520maps%252C%2520offering%2520a%2520promising%2520direction%2520for%2520automated%2520csPCa%250Aidentification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly-Driven%20Approach%20for%20Enhanced%20Prostate%20Cancer%20Segmentation&entry.906535625=Alessia%20Hu%20and%20Regina%20Beets-Tan%20and%20Lishan%20Cai%20and%20Eduardo%20Pooch&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20plays%20an%20important%20role%20in%20identifying%0Aclinically%20significant%20prostate%20cancer%20%28csPCa%29%2C%20yet%20automated%20methods%20face%0Achallenges%20such%20as%20data%20imbalance%2C%20variable%20tumor%20sizes%2C%20and%20a%20lack%20of%0Aannotated%20data.%20This%20study%20introduces%20Anomaly-Driven%20U-Net%20%28adU-Net%29%2C%20which%0Aincorporates%20anomaly%20maps%20derived%20from%20biparametric%20MRI%20sequences%20into%20a%20deep%0Alearning-based%20segmentation%20framework%20to%20improve%20csPCa%20identification.%20We%0Aconduct%20a%20comparative%20analysis%20of%20anomaly%20detection%20methods%20and%20evaluate%20the%0Aintegration%20of%20anomaly%20maps%20into%20the%20segmentation%20pipeline.%20Anomaly%20maps%2C%0Agenerated%20using%20Fixed-Point%20GAN%20reconstruction%2C%20highlight%20deviations%20from%0Anormal%20prostate%20tissue%2C%20guiding%20the%20segmentation%20model%20to%20potential%20cancerous%0Aregions.%20We%20compare%20the%20performance%20by%20using%20the%20average%20score%2C%20computed%20as%20the%0Amean%20of%20the%20AUROC%20and%20Average%20Precision%20%28AP%29.%20On%20the%20external%20test%20set%2C%20adU-Net%0Aachieves%20the%20best%20average%20score%20of%200.618%2C%20outperforming%20the%20baseline%20nnU-Net%0Amodel%20%280.605%29.%20The%20results%20demonstrate%20that%20incorporating%20anomaly%20detection%0Ainto%20segmentation%20improves%20generalization%20and%20performance%2C%20particularly%20with%0AADC-based%20anomaly%20maps%2C%20offering%20a%20promising%20direction%20for%20automated%20csPCa%0Aidentification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21789v1&entry.124074799=Read"},
{"title": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for\n  Arbitrary Chinese Dishes", "author": "Huijie Liu and Bingcan Wang and Jie Hu and Xiaoming Wei and Guoliang Kang", "abstract": "  Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods.\n", "link": "http://arxiv.org/abs/2504.09948v2", "date": "2025-04-30", "relevancy": 2.1391, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5383}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Dish%3A%20Photorealistic%20and%20Faithful%20Image%20Generation%20and%20Editing%20for%0A%20%20Arbitrary%20Chinese%20Dishes&body=Title%3A%20Omni-Dish%3A%20Photorealistic%20and%20Faithful%20Image%20Generation%20and%20Editing%20for%0A%20%20Arbitrary%20Chinese%20Dishes%0AAuthor%3A%20Huijie%20Liu%20and%20Bingcan%20Wang%20and%20Jie%20Hu%20and%20Xiaoming%20Wei%20and%20Guoliang%20Kang%0AAbstract%3A%20%20%20Dish%20images%20play%20a%20crucial%20role%20in%20the%20digital%20era%2C%20with%20the%20demand%20for%0Aculturally%20distinctive%20dish%20images%20continuously%20increasing%20due%20to%20the%0Adigitization%20of%20the%20food%20industry%20and%20e-commerce.%20In%20general%20cases%2C%20existing%0Atext-to-image%20generation%20models%20excel%20in%20producing%20high-quality%20images%3B%0Ahowever%2C%20they%20struggle%20to%20capture%20diverse%20characteristics%20and%20faithful%20details%0Aof%20specific%20domains%2C%20particularly%20Chinese%20dishes.%20To%20address%20this%20limitation%2C%0Awe%20propose%20Omni-Dish%2C%20the%20first%20text-to-image%20generation%20model%20specifically%0Atailored%20for%20Chinese%20dishes.%20We%20develop%20a%20comprehensive%20dish%20curation%20pipeline%2C%0Abuilding%20the%20largest%20dish%20dataset%20to%20date.%20Additionally%2C%20we%20introduce%20a%0Arecaption%20strategy%20and%20employ%20a%20coarse-to-fine%20training%20scheme%20to%20help%20the%0Amodel%20better%20learn%20fine-grained%20culinary%20nuances.%20During%20inference%2C%20we%20enhance%0Athe%20user%27s%20textual%20input%20using%20a%20pre-constructed%20high-quality%20caption%20library%0Aand%20a%20large%20language%20model%2C%20enabling%20more%20photorealistic%20and%20faithful%20image%0Ageneration.%20Furthermore%2C%20to%20extend%20our%20model%27s%20capability%20for%20dish%20editing%0Atasks%2C%20we%20propose%20Concept-Enhanced%20P2P.%20Based%20on%20this%20approach%2C%20we%20build%20a%20dish%0Aediting%20dataset%20and%20train%20a%20specialized%20editing%20model.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20our%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09948v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Dish%253A%2520Photorealistic%2520and%2520Faithful%2520Image%2520Generation%2520and%2520Editing%2520for%250A%2520%2520Arbitrary%2520Chinese%2520Dishes%26entry.906535625%3DHuijie%2520Liu%2520and%2520Bingcan%2520Wang%2520and%2520Jie%2520Hu%2520and%2520Xiaoming%2520Wei%2520and%2520Guoliang%2520Kang%26entry.1292438233%3D%2520%2520Dish%2520images%2520play%2520a%2520crucial%2520role%2520in%2520the%2520digital%2520era%252C%2520with%2520the%2520demand%2520for%250Aculturally%2520distinctive%2520dish%2520images%2520continuously%2520increasing%2520due%2520to%2520the%250Adigitization%2520of%2520the%2520food%2520industry%2520and%2520e-commerce.%2520In%2520general%2520cases%252C%2520existing%250Atext-to-image%2520generation%2520models%2520excel%2520in%2520producing%2520high-quality%2520images%253B%250Ahowever%252C%2520they%2520struggle%2520to%2520capture%2520diverse%2520characteristics%2520and%2520faithful%2520details%250Aof%2520specific%2520domains%252C%2520particularly%2520Chinese%2520dishes.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520Omni-Dish%252C%2520the%2520first%2520text-to-image%2520generation%2520model%2520specifically%250Atailored%2520for%2520Chinese%2520dishes.%2520We%2520develop%2520a%2520comprehensive%2520dish%2520curation%2520pipeline%252C%250Abuilding%2520the%2520largest%2520dish%2520dataset%2520to%2520date.%2520Additionally%252C%2520we%2520introduce%2520a%250Arecaption%2520strategy%2520and%2520employ%2520a%2520coarse-to-fine%2520training%2520scheme%2520to%2520help%2520the%250Amodel%2520better%2520learn%2520fine-grained%2520culinary%2520nuances.%2520During%2520inference%252C%2520we%2520enhance%250Athe%2520user%2527s%2520textual%2520input%2520using%2520a%2520pre-constructed%2520high-quality%2520caption%2520library%250Aand%2520a%2520large%2520language%2520model%252C%2520enabling%2520more%2520photorealistic%2520and%2520faithful%2520image%250Ageneration.%2520Furthermore%252C%2520to%2520extend%2520our%2520model%2527s%2520capability%2520for%2520dish%2520editing%250Atasks%252C%2520we%2520propose%2520Concept-Enhanced%2520P2P.%2520Based%2520on%2520this%2520approach%252C%2520we%2520build%2520a%2520dish%250Aediting%2520dataset%2520and%2520train%2520a%2520specialized%2520editing%2520model.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09948v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Dish%3A%20Photorealistic%20and%20Faithful%20Image%20Generation%20and%20Editing%20for%0A%20%20Arbitrary%20Chinese%20Dishes&entry.906535625=Huijie%20Liu%20and%20Bingcan%20Wang%20and%20Jie%20Hu%20and%20Xiaoming%20Wei%20and%20Guoliang%20Kang&entry.1292438233=%20%20Dish%20images%20play%20a%20crucial%20role%20in%20the%20digital%20era%2C%20with%20the%20demand%20for%0Aculturally%20distinctive%20dish%20images%20continuously%20increasing%20due%20to%20the%0Adigitization%20of%20the%20food%20industry%20and%20e-commerce.%20In%20general%20cases%2C%20existing%0Atext-to-image%20generation%20models%20excel%20in%20producing%20high-quality%20images%3B%0Ahowever%2C%20they%20struggle%20to%20capture%20diverse%20characteristics%20and%20faithful%20details%0Aof%20specific%20domains%2C%20particularly%20Chinese%20dishes.%20To%20address%20this%20limitation%2C%0Awe%20propose%20Omni-Dish%2C%20the%20first%20text-to-image%20generation%20model%20specifically%0Atailored%20for%20Chinese%20dishes.%20We%20develop%20a%20comprehensive%20dish%20curation%20pipeline%2C%0Abuilding%20the%20largest%20dish%20dataset%20to%20date.%20Additionally%2C%20we%20introduce%20a%0Arecaption%20strategy%20and%20employ%20a%20coarse-to-fine%20training%20scheme%20to%20help%20the%0Amodel%20better%20learn%20fine-grained%20culinary%20nuances.%20During%20inference%2C%20we%20enhance%0Athe%20user%27s%20textual%20input%20using%20a%20pre-constructed%20high-quality%20caption%20library%0Aand%20a%20large%20language%20model%2C%20enabling%20more%20photorealistic%20and%20faithful%20image%0Ageneration.%20Furthermore%2C%20to%20extend%20our%20model%27s%20capability%20for%20dish%20editing%0Atasks%2C%20we%20propose%20Concept-Enhanced%20P2P.%20Based%20on%20this%20approach%2C%20we%20build%20a%20dish%0Aediting%20dataset%20and%20train%20a%20specialized%20editing%20model.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20our%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09948v2&entry.124074799=Read"},
{"title": "SDWPF: A Dataset for Spatial Dynamic Wind Power Forecasting Challenge at\n  KDD Cup 2022", "author": "Jingbo Zhou and Xinjiang Lu and Yixiong Xiao and Jiantao Su and Junfu Lyu and Yanjun Ma and Dejing Dou", "abstract": "  The variability of wind power supply can present substantial challenges to\nincorporating wind power into a grid system. Thus, Wind Power Forecasting (WPF)\nhas been widely recognized as one of the most critical issues in wind power\nintegration and operation. There has been an explosion of studies on wind power\nforecasting problems in the past decades. Nevertheless, how to well handle the\nWPF problem is still challenging, since high prediction accuracy is always\ndemanded to ensure grid stability and security of supply. We present a unique\nSpatial Dynamic Wind Power Forecasting dataset: SDWPF, which includes the\nspatial distribution of wind turbines, as well as the dynamic context factors.\nWhereas, most of the existing datasets have only a small number of wind\nturbines without knowing the locations and context information of wind turbines\nat a fine-grained time scale. By contrast, SDWPF provides the wind power data\nof 134 wind turbines from a wind farm over half a year with their relative\npositions and internal statuses. We use this dataset to launch the Baidu KDD\nCup 2022 to examine the limit of current WPF solutions. The dataset is released\nat https://aistudio.baidu.com/aistudio/competition/detail/152/0/datasets.\n", "link": "http://arxiv.org/abs/2208.04360v2", "date": "2025-04-30", "relevancy": 2.1345, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4331}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4271}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDWPF%3A%20A%20Dataset%20for%20Spatial%20Dynamic%20Wind%20Power%20Forecasting%20Challenge%20at%0A%20%20KDD%20Cup%202022&body=Title%3A%20SDWPF%3A%20A%20Dataset%20for%20Spatial%20Dynamic%20Wind%20Power%20Forecasting%20Challenge%20at%0A%20%20KDD%20Cup%202022%0AAuthor%3A%20Jingbo%20Zhou%20and%20Xinjiang%20Lu%20and%20Yixiong%20Xiao%20and%20Jiantao%20Su%20and%20Junfu%20Lyu%20and%20Yanjun%20Ma%20and%20Dejing%20Dou%0AAbstract%3A%20%20%20The%20variability%20of%20wind%20power%20supply%20can%20present%20substantial%20challenges%20to%0Aincorporating%20wind%20power%20into%20a%20grid%20system.%20Thus%2C%20Wind%20Power%20Forecasting%20%28WPF%29%0Ahas%20been%20widely%20recognized%20as%20one%20of%20the%20most%20critical%20issues%20in%20wind%20power%0Aintegration%20and%20operation.%20There%20has%20been%20an%20explosion%20of%20studies%20on%20wind%20power%0Aforecasting%20problems%20in%20the%20past%20decades.%20Nevertheless%2C%20how%20to%20well%20handle%20the%0AWPF%20problem%20is%20still%20challenging%2C%20since%20high%20prediction%20accuracy%20is%20always%0Ademanded%20to%20ensure%20grid%20stability%20and%20security%20of%20supply.%20We%20present%20a%20unique%0ASpatial%20Dynamic%20Wind%20Power%20Forecasting%20dataset%3A%20SDWPF%2C%20which%20includes%20the%0Aspatial%20distribution%20of%20wind%20turbines%2C%20as%20well%20as%20the%20dynamic%20context%20factors.%0AWhereas%2C%20most%20of%20the%20existing%20datasets%20have%20only%20a%20small%20number%20of%20wind%0Aturbines%20without%20knowing%20the%20locations%20and%20context%20information%20of%20wind%20turbines%0Aat%20a%20fine-grained%20time%20scale.%20By%20contrast%2C%20SDWPF%20provides%20the%20wind%20power%20data%0Aof%20134%20wind%20turbines%20from%20a%20wind%20farm%20over%20half%20a%20year%20with%20their%20relative%0Apositions%20and%20internal%20statuses.%20We%20use%20this%20dataset%20to%20launch%20the%20Baidu%20KDD%0ACup%202022%20to%20examine%20the%20limit%20of%20current%20WPF%20solutions.%20The%20dataset%20is%20released%0Aat%20https%3A//aistudio.baidu.com/aistudio/competition/detail/152/0/datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.04360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDWPF%253A%2520A%2520Dataset%2520for%2520Spatial%2520Dynamic%2520Wind%2520Power%2520Forecasting%2520Challenge%2520at%250A%2520%2520KDD%2520Cup%25202022%26entry.906535625%3DJingbo%2520Zhou%2520and%2520Xinjiang%2520Lu%2520and%2520Yixiong%2520Xiao%2520and%2520Jiantao%2520Su%2520and%2520Junfu%2520Lyu%2520and%2520Yanjun%2520Ma%2520and%2520Dejing%2520Dou%26entry.1292438233%3D%2520%2520The%2520variability%2520of%2520wind%2520power%2520supply%2520can%2520present%2520substantial%2520challenges%2520to%250Aincorporating%2520wind%2520power%2520into%2520a%2520grid%2520system.%2520Thus%252C%2520Wind%2520Power%2520Forecasting%2520%2528WPF%2529%250Ahas%2520been%2520widely%2520recognized%2520as%2520one%2520of%2520the%2520most%2520critical%2520issues%2520in%2520wind%2520power%250Aintegration%2520and%2520operation.%2520There%2520has%2520been%2520an%2520explosion%2520of%2520studies%2520on%2520wind%2520power%250Aforecasting%2520problems%2520in%2520the%2520past%2520decades.%2520Nevertheless%252C%2520how%2520to%2520well%2520handle%2520the%250AWPF%2520problem%2520is%2520still%2520challenging%252C%2520since%2520high%2520prediction%2520accuracy%2520is%2520always%250Ademanded%2520to%2520ensure%2520grid%2520stability%2520and%2520security%2520of%2520supply.%2520We%2520present%2520a%2520unique%250ASpatial%2520Dynamic%2520Wind%2520Power%2520Forecasting%2520dataset%253A%2520SDWPF%252C%2520which%2520includes%2520the%250Aspatial%2520distribution%2520of%2520wind%2520turbines%252C%2520as%2520well%2520as%2520the%2520dynamic%2520context%2520factors.%250AWhereas%252C%2520most%2520of%2520the%2520existing%2520datasets%2520have%2520only%2520a%2520small%2520number%2520of%2520wind%250Aturbines%2520without%2520knowing%2520the%2520locations%2520and%2520context%2520information%2520of%2520wind%2520turbines%250Aat%2520a%2520fine-grained%2520time%2520scale.%2520By%2520contrast%252C%2520SDWPF%2520provides%2520the%2520wind%2520power%2520data%250Aof%2520134%2520wind%2520turbines%2520from%2520a%2520wind%2520farm%2520over%2520half%2520a%2520year%2520with%2520their%2520relative%250Apositions%2520and%2520internal%2520statuses.%2520We%2520use%2520this%2520dataset%2520to%2520launch%2520the%2520Baidu%2520KDD%250ACup%25202022%2520to%2520examine%2520the%2520limit%2520of%2520current%2520WPF%2520solutions.%2520The%2520dataset%2520is%2520released%250Aat%2520https%253A//aistudio.baidu.com/aistudio/competition/detail/152/0/datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.04360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDWPF%3A%20A%20Dataset%20for%20Spatial%20Dynamic%20Wind%20Power%20Forecasting%20Challenge%20at%0A%20%20KDD%20Cup%202022&entry.906535625=Jingbo%20Zhou%20and%20Xinjiang%20Lu%20and%20Yixiong%20Xiao%20and%20Jiantao%20Su%20and%20Junfu%20Lyu%20and%20Yanjun%20Ma%20and%20Dejing%20Dou&entry.1292438233=%20%20The%20variability%20of%20wind%20power%20supply%20can%20present%20substantial%20challenges%20to%0Aincorporating%20wind%20power%20into%20a%20grid%20system.%20Thus%2C%20Wind%20Power%20Forecasting%20%28WPF%29%0Ahas%20been%20widely%20recognized%20as%20one%20of%20the%20most%20critical%20issues%20in%20wind%20power%0Aintegration%20and%20operation.%20There%20has%20been%20an%20explosion%20of%20studies%20on%20wind%20power%0Aforecasting%20problems%20in%20the%20past%20decades.%20Nevertheless%2C%20how%20to%20well%20handle%20the%0AWPF%20problem%20is%20still%20challenging%2C%20since%20high%20prediction%20accuracy%20is%20always%0Ademanded%20to%20ensure%20grid%20stability%20and%20security%20of%20supply.%20We%20present%20a%20unique%0ASpatial%20Dynamic%20Wind%20Power%20Forecasting%20dataset%3A%20SDWPF%2C%20which%20includes%20the%0Aspatial%20distribution%20of%20wind%20turbines%2C%20as%20well%20as%20the%20dynamic%20context%20factors.%0AWhereas%2C%20most%20of%20the%20existing%20datasets%20have%20only%20a%20small%20number%20of%20wind%0Aturbines%20without%20knowing%20the%20locations%20and%20context%20information%20of%20wind%20turbines%0Aat%20a%20fine-grained%20time%20scale.%20By%20contrast%2C%20SDWPF%20provides%20the%20wind%20power%20data%0Aof%20134%20wind%20turbines%20from%20a%20wind%20farm%20over%20half%20a%20year%20with%20their%20relative%0Apositions%20and%20internal%20statuses.%20We%20use%20this%20dataset%20to%20launch%20the%20Baidu%20KDD%0ACup%202022%20to%20examine%20the%20limit%20of%20current%20WPF%20solutions.%20The%20dataset%20is%20released%0Aat%20https%3A//aistudio.baidu.com/aistudio/competition/detail/152/0/datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.04360v2&entry.124074799=Read"},
{"title": "Provably-Safe, Online System Identification", "author": "Bohao Zhang and Zichang Zhou and Ram Vasudevan", "abstract": "  Precise manipulation tasks require accurate knowledge of payload inertial\nparameters. Unfortunately, identifying these parameters for unknown payloads\nwhile ensuring that the robotic system satisfies its input and state\nconstraints while avoiding collisions with the environment remains a\nsignificant challenge. This paper presents an integrated framework that enables\nrobotic manipulators to safely and automatically identify payload parameters\nwhile maintaining operational safety guarantees. The framework consists of two\nsynergistic components: an online trajectory planning and control framework\nthat generates provably-safe exciting trajectories for system identification\nthat can be tracked while respecting robot constraints and avoiding obstacles\nand a robust system identification method that computes rigorous\noverapproximative bounds on end-effector inertial parameters assuming bounded\nsensor noise. Experimental validation on a robotic manipulator performing\nchallenging tasks with various unknown payloads demonstrates the framework's\neffectiveness in establishing accurate parameter bounds while maintaining\nsafety throughout the identification process. The code is available at our\nproject webpage: https://roahmlab.github.io/OnlineSafeSysID/.\n", "link": "http://arxiv.org/abs/2504.21486v1", "date": "2025-04-30", "relevancy": 2.131, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.584}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5289}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably-Safe%2C%20Online%20System%20Identification&body=Title%3A%20Provably-Safe%2C%20Online%20System%20Identification%0AAuthor%3A%20Bohao%20Zhang%20and%20Zichang%20Zhou%20and%20Ram%20Vasudevan%0AAbstract%3A%20%20%20Precise%20manipulation%20tasks%20require%20accurate%20knowledge%20of%20payload%20inertial%0Aparameters.%20Unfortunately%2C%20identifying%20these%20parameters%20for%20unknown%20payloads%0Awhile%20ensuring%20that%20the%20robotic%20system%20satisfies%20its%20input%20and%20state%0Aconstraints%20while%20avoiding%20collisions%20with%20the%20environment%20remains%20a%0Asignificant%20challenge.%20This%20paper%20presents%20an%20integrated%20framework%20that%20enables%0Arobotic%20manipulators%20to%20safely%20and%20automatically%20identify%20payload%20parameters%0Awhile%20maintaining%20operational%20safety%20guarantees.%20The%20framework%20consists%20of%20two%0Asynergistic%20components%3A%20an%20online%20trajectory%20planning%20and%20control%20framework%0Athat%20generates%20provably-safe%20exciting%20trajectories%20for%20system%20identification%0Athat%20can%20be%20tracked%20while%20respecting%20robot%20constraints%20and%20avoiding%20obstacles%0Aand%20a%20robust%20system%20identification%20method%20that%20computes%20rigorous%0Aoverapproximative%20bounds%20on%20end-effector%20inertial%20parameters%20assuming%20bounded%0Asensor%20noise.%20Experimental%20validation%20on%20a%20robotic%20manipulator%20performing%0Achallenging%20tasks%20with%20various%20unknown%20payloads%20demonstrates%20the%20framework%27s%0Aeffectiveness%20in%20establishing%20accurate%20parameter%20bounds%20while%20maintaining%0Asafety%20throughout%20the%20identification%20process.%20The%20code%20is%20available%20at%20our%0Aproject%20webpage%3A%20https%3A//roahmlab.github.io/OnlineSafeSysID/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably-Safe%252C%2520Online%2520System%2520Identification%26entry.906535625%3DBohao%2520Zhang%2520and%2520Zichang%2520Zhou%2520and%2520Ram%2520Vasudevan%26entry.1292438233%3D%2520%2520Precise%2520manipulation%2520tasks%2520require%2520accurate%2520knowledge%2520of%2520payload%2520inertial%250Aparameters.%2520Unfortunately%252C%2520identifying%2520these%2520parameters%2520for%2520unknown%2520payloads%250Awhile%2520ensuring%2520that%2520the%2520robotic%2520system%2520satisfies%2520its%2520input%2520and%2520state%250Aconstraints%2520while%2520avoiding%2520collisions%2520with%2520the%2520environment%2520remains%2520a%250Asignificant%2520challenge.%2520This%2520paper%2520presents%2520an%2520integrated%2520framework%2520that%2520enables%250Arobotic%2520manipulators%2520to%2520safely%2520and%2520automatically%2520identify%2520payload%2520parameters%250Awhile%2520maintaining%2520operational%2520safety%2520guarantees.%2520The%2520framework%2520consists%2520of%2520two%250Asynergistic%2520components%253A%2520an%2520online%2520trajectory%2520planning%2520and%2520control%2520framework%250Athat%2520generates%2520provably-safe%2520exciting%2520trajectories%2520for%2520system%2520identification%250Athat%2520can%2520be%2520tracked%2520while%2520respecting%2520robot%2520constraints%2520and%2520avoiding%2520obstacles%250Aand%2520a%2520robust%2520system%2520identification%2520method%2520that%2520computes%2520rigorous%250Aoverapproximative%2520bounds%2520on%2520end-effector%2520inertial%2520parameters%2520assuming%2520bounded%250Asensor%2520noise.%2520Experimental%2520validation%2520on%2520a%2520robotic%2520manipulator%2520performing%250Achallenging%2520tasks%2520with%2520various%2520unknown%2520payloads%2520demonstrates%2520the%2520framework%2527s%250Aeffectiveness%2520in%2520establishing%2520accurate%2520parameter%2520bounds%2520while%2520maintaining%250Asafety%2520throughout%2520the%2520identification%2520process.%2520The%2520code%2520is%2520available%2520at%2520our%250Aproject%2520webpage%253A%2520https%253A//roahmlab.github.io/OnlineSafeSysID/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably-Safe%2C%20Online%20System%20Identification&entry.906535625=Bohao%20Zhang%20and%20Zichang%20Zhou%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Precise%20manipulation%20tasks%20require%20accurate%20knowledge%20of%20payload%20inertial%0Aparameters.%20Unfortunately%2C%20identifying%20these%20parameters%20for%20unknown%20payloads%0Awhile%20ensuring%20that%20the%20robotic%20system%20satisfies%20its%20input%20and%20state%0Aconstraints%20while%20avoiding%20collisions%20with%20the%20environment%20remains%20a%0Asignificant%20challenge.%20This%20paper%20presents%20an%20integrated%20framework%20that%20enables%0Arobotic%20manipulators%20to%20safely%20and%20automatically%20identify%20payload%20parameters%0Awhile%20maintaining%20operational%20safety%20guarantees.%20The%20framework%20consists%20of%20two%0Asynergistic%20components%3A%20an%20online%20trajectory%20planning%20and%20control%20framework%0Athat%20generates%20provably-safe%20exciting%20trajectories%20for%20system%20identification%0Athat%20can%20be%20tracked%20while%20respecting%20robot%20constraints%20and%20avoiding%20obstacles%0Aand%20a%20robust%20system%20identification%20method%20that%20computes%20rigorous%0Aoverapproximative%20bounds%20on%20end-effector%20inertial%20parameters%20assuming%20bounded%0Asensor%20noise.%20Experimental%20validation%20on%20a%20robotic%20manipulator%20performing%0Achallenging%20tasks%20with%20various%20unknown%20payloads%20demonstrates%20the%20framework%27s%0Aeffectiveness%20in%20establishing%20accurate%20parameter%20bounds%20while%20maintaining%0Asafety%20throughout%20the%20identification%20process.%20The%20code%20is%20available%20at%20our%0Aproject%20webpage%3A%20https%3A//roahmlab.github.io/OnlineSafeSysID/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21486v1&entry.124074799=Read"},
{"title": "A Library for Learning Neural Operators", "author": "Jean Kossaifi and Nikola Kovachki and Zongyi Li and David Pitt and Miguel Liu-Schiaffini and Robert Joseph George and Boris Bonev and Kamyar Azizzadenesheli and Julius Berner and Valentin Duruisseaux and Anima Anandkumar", "abstract": "  We present NeuralOperator, an open-source Python library for operator\nlearning. Neural operators generalize neural networks to maps between function\nspaces instead of finite-dimensional Euclidean spaces. They can be trained and\ninferenced on input and output functions given at various discretizations,\nsatisfying a discretization convergence properties. Built on top of PyTorch,\nNeuralOperator provides all the tools for training and deploying neural\noperator models, as well as developing new ones, in a high-quality, tested,\nopen-source package. It combines cutting-edge models and customizability with a\ngentle learning curve and simple user interface for newcomers.\n", "link": "http://arxiv.org/abs/2412.10354v3", "date": "2025-04-30", "relevancy": 2.1304, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4365}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4217}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.42}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Library%20for%20Learning%20Neural%20Operators&body=Title%3A%20A%20Library%20for%20Learning%20Neural%20Operators%0AAuthor%3A%20Jean%20Kossaifi%20and%20Nikola%20Kovachki%20and%20Zongyi%20Li%20and%20David%20Pitt%20and%20Miguel%20Liu-Schiaffini%20and%20Robert%20Joseph%20George%20and%20Boris%20Bonev%20and%20Kamyar%20Azizzadenesheli%20and%20Julius%20Berner%20and%20Valentin%20Duruisseaux%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20We%20present%20NeuralOperator%2C%20an%20open-source%20Python%20library%20for%20operator%0Alearning.%20Neural%20operators%20generalize%20neural%20networks%20to%20maps%20between%20function%0Aspaces%20instead%20of%20finite-dimensional%20Euclidean%20spaces.%20They%20can%20be%20trained%20and%0Ainferenced%20on%20input%20and%20output%20functions%20given%20at%20various%20discretizations%2C%0Asatisfying%20a%20discretization%20convergence%20properties.%20Built%20on%20top%20of%20PyTorch%2C%0ANeuralOperator%20provides%20all%20the%20tools%20for%20training%20and%20deploying%20neural%0Aoperator%20models%2C%20as%20well%20as%20developing%20new%20ones%2C%20in%20a%20high-quality%2C%20tested%2C%0Aopen-source%20package.%20It%20combines%20cutting-edge%20models%20and%20customizability%20with%20a%0Agentle%20learning%20curve%20and%20simple%20user%20interface%20for%20newcomers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10354v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Library%2520for%2520Learning%2520Neural%2520Operators%26entry.906535625%3DJean%2520Kossaifi%2520and%2520Nikola%2520Kovachki%2520and%2520Zongyi%2520Li%2520and%2520David%2520Pitt%2520and%2520Miguel%2520Liu-Schiaffini%2520and%2520Robert%2520Joseph%2520George%2520and%2520Boris%2520Bonev%2520and%2520Kamyar%2520Azizzadenesheli%2520and%2520Julius%2520Berner%2520and%2520Valentin%2520Duruisseaux%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520We%2520present%2520NeuralOperator%252C%2520an%2520open-source%2520Python%2520library%2520for%2520operator%250Alearning.%2520Neural%2520operators%2520generalize%2520neural%2520networks%2520to%2520maps%2520between%2520function%250Aspaces%2520instead%2520of%2520finite-dimensional%2520Euclidean%2520spaces.%2520They%2520can%2520be%2520trained%2520and%250Ainferenced%2520on%2520input%2520and%2520output%2520functions%2520given%2520at%2520various%2520discretizations%252C%250Asatisfying%2520a%2520discretization%2520convergence%2520properties.%2520Built%2520on%2520top%2520of%2520PyTorch%252C%250ANeuralOperator%2520provides%2520all%2520the%2520tools%2520for%2520training%2520and%2520deploying%2520neural%250Aoperator%2520models%252C%2520as%2520well%2520as%2520developing%2520new%2520ones%252C%2520in%2520a%2520high-quality%252C%2520tested%252C%250Aopen-source%2520package.%2520It%2520combines%2520cutting-edge%2520models%2520and%2520customizability%2520with%2520a%250Agentle%2520learning%2520curve%2520and%2520simple%2520user%2520interface%2520for%2520newcomers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10354v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Library%20for%20Learning%20Neural%20Operators&entry.906535625=Jean%20Kossaifi%20and%20Nikola%20Kovachki%20and%20Zongyi%20Li%20and%20David%20Pitt%20and%20Miguel%20Liu-Schiaffini%20and%20Robert%20Joseph%20George%20and%20Boris%20Bonev%20and%20Kamyar%20Azizzadenesheli%20and%20Julius%20Berner%20and%20Valentin%20Duruisseaux%20and%20Anima%20Anandkumar&entry.1292438233=%20%20We%20present%20NeuralOperator%2C%20an%20open-source%20Python%20library%20for%20operator%0Alearning.%20Neural%20operators%20generalize%20neural%20networks%20to%20maps%20between%20function%0Aspaces%20instead%20of%20finite-dimensional%20Euclidean%20spaces.%20They%20can%20be%20trained%20and%0Ainferenced%20on%20input%20and%20output%20functions%20given%20at%20various%20discretizations%2C%0Asatisfying%20a%20discretization%20convergence%20properties.%20Built%20on%20top%20of%20PyTorch%2C%0ANeuralOperator%20provides%20all%20the%20tools%20for%20training%20and%20deploying%20neural%0Aoperator%20models%2C%20as%20well%20as%20developing%20new%20ones%2C%20in%20a%20high-quality%2C%20tested%2C%0Aopen-source%20package.%20It%20combines%20cutting-edge%20models%20and%20customizability%20with%20a%0Agentle%20learning%20curve%20and%20simple%20user%20interface%20for%20newcomers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10354v3&entry.124074799=Read"},
{"title": "Lossless data compression by large models", "author": "Ziguang Li and Chao Huang and Xuliang Wang and Haibo Hu and Cole Wyeth and Dongbo Bu and Quan Yu and Wen Gao and Xingwu Liu and Ming Li", "abstract": "  Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression. We have previously shown all\nunderstanding or learning are compression, under reasonable assumptions. Large\nlanguage models (LLMs) understand data better than ever before. Can they help\nus to compress data? The LLMs may be seen to approximate the uncomputable\nSolomonoff induction. Therefore, under this new uncomputable paradigm, we\npresent LMCompress. LMCompress shatters all previous lossless compression\nalgorithms, doubling the lossless compression ratios of JPEG-XL for images,\nFLAC for audios, and H.264 for videos, and quadrupling the compression ratio of\nbz2 for texts. The better a large model understands the data, the better\nLMCompress compresses.\n", "link": "http://arxiv.org/abs/2407.07723v3", "date": "2025-04-30", "relevancy": 2.1291, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lossless%20data%20compression%20by%20large%20models&body=Title%3A%20Lossless%20data%20compression%20by%20large%20models%0AAuthor%3A%20Ziguang%20Li%20and%20Chao%20Huang%20and%20Xuliang%20Wang%20and%20Haibo%20Hu%20and%20Cole%20Wyeth%20and%20Dongbo%20Bu%20and%20Quan%20Yu%20and%20Wen%20Gao%20and%20Xingwu%20Liu%20and%20Ming%20Li%0AAbstract%3A%20%20%20Modern%20data%20compression%20methods%20are%20slowly%20reaching%20their%20limits%20after%2080%0Ayears%20of%20research%2C%20millions%20of%20papers%2C%20and%20wide%20range%20of%20applications.%20Yet%2C%20the%0Aextravagant%206G%20communication%20speed%20requirement%20raises%20a%20major%20open%20question%20for%0Arevolutionary%20new%20ideas%20of%20data%20compression.%20We%20have%20previously%20shown%20all%0Aunderstanding%20or%20learning%20are%20compression%2C%20under%20reasonable%20assumptions.%20Large%0Alanguage%20models%20%28LLMs%29%20understand%20data%20better%20than%20ever%20before.%20Can%20they%20help%0Aus%20to%20compress%20data%3F%20The%20LLMs%20may%20be%20seen%20to%20approximate%20the%20uncomputable%0ASolomonoff%20induction.%20Therefore%2C%20under%20this%20new%20uncomputable%20paradigm%2C%20we%0Apresent%20LMCompress.%20LMCompress%20shatters%20all%20previous%20lossless%20compression%0Aalgorithms%2C%20doubling%20the%20lossless%20compression%20ratios%20of%20JPEG-XL%20for%20images%2C%0AFLAC%20for%20audios%2C%20and%20H.264%20for%20videos%2C%20and%20quadrupling%20the%20compression%20ratio%20of%0Abz2%20for%20texts.%20The%20better%20a%20large%20model%20understands%20the%20data%2C%20the%20better%0ALMCompress%20compresses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07723v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLossless%2520data%2520compression%2520by%2520large%2520models%26entry.906535625%3DZiguang%2520Li%2520and%2520Chao%2520Huang%2520and%2520Xuliang%2520Wang%2520and%2520Haibo%2520Hu%2520and%2520Cole%2520Wyeth%2520and%2520Dongbo%2520Bu%2520and%2520Quan%2520Yu%2520and%2520Wen%2520Gao%2520and%2520Xingwu%2520Liu%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%2520Modern%2520data%2520compression%2520methods%2520are%2520slowly%2520reaching%2520their%2520limits%2520after%252080%250Ayears%2520of%2520research%252C%2520millions%2520of%2520papers%252C%2520and%2520wide%2520range%2520of%2520applications.%2520Yet%252C%2520the%250Aextravagant%25206G%2520communication%2520speed%2520requirement%2520raises%2520a%2520major%2520open%2520question%2520for%250Arevolutionary%2520new%2520ideas%2520of%2520data%2520compression.%2520We%2520have%2520previously%2520shown%2520all%250Aunderstanding%2520or%2520learning%2520are%2520compression%252C%2520under%2520reasonable%2520assumptions.%2520Large%250Alanguage%2520models%2520%2528LLMs%2529%2520understand%2520data%2520better%2520than%2520ever%2520before.%2520Can%2520they%2520help%250Aus%2520to%2520compress%2520data%253F%2520The%2520LLMs%2520may%2520be%2520seen%2520to%2520approximate%2520the%2520uncomputable%250ASolomonoff%2520induction.%2520Therefore%252C%2520under%2520this%2520new%2520uncomputable%2520paradigm%252C%2520we%250Apresent%2520LMCompress.%2520LMCompress%2520shatters%2520all%2520previous%2520lossless%2520compression%250Aalgorithms%252C%2520doubling%2520the%2520lossless%2520compression%2520ratios%2520of%2520JPEG-XL%2520for%2520images%252C%250AFLAC%2520for%2520audios%252C%2520and%2520H.264%2520for%2520videos%252C%2520and%2520quadrupling%2520the%2520compression%2520ratio%2520of%250Abz2%2520for%2520texts.%2520The%2520better%2520a%2520large%2520model%2520understands%2520the%2520data%252C%2520the%2520better%250ALMCompress%2520compresses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07723v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lossless%20data%20compression%20by%20large%20models&entry.906535625=Ziguang%20Li%20and%20Chao%20Huang%20and%20Xuliang%20Wang%20and%20Haibo%20Hu%20and%20Cole%20Wyeth%20and%20Dongbo%20Bu%20and%20Quan%20Yu%20and%20Wen%20Gao%20and%20Xingwu%20Liu%20and%20Ming%20Li&entry.1292438233=%20%20Modern%20data%20compression%20methods%20are%20slowly%20reaching%20their%20limits%20after%2080%0Ayears%20of%20research%2C%20millions%20of%20papers%2C%20and%20wide%20range%20of%20applications.%20Yet%2C%20the%0Aextravagant%206G%20communication%20speed%20requirement%20raises%20a%20major%20open%20question%20for%0Arevolutionary%20new%20ideas%20of%20data%20compression.%20We%20have%20previously%20shown%20all%0Aunderstanding%20or%20learning%20are%20compression%2C%20under%20reasonable%20assumptions.%20Large%0Alanguage%20models%20%28LLMs%29%20understand%20data%20better%20than%20ever%20before.%20Can%20they%20help%0Aus%20to%20compress%20data%3F%20The%20LLMs%20may%20be%20seen%20to%20approximate%20the%20uncomputable%0ASolomonoff%20induction.%20Therefore%2C%20under%20this%20new%20uncomputable%20paradigm%2C%20we%0Apresent%20LMCompress.%20LMCompress%20shatters%20all%20previous%20lossless%20compression%0Aalgorithms%2C%20doubling%20the%20lossless%20compression%20ratios%20of%20JPEG-XL%20for%20images%2C%0AFLAC%20for%20audios%2C%20and%20H.264%20for%20videos%2C%20and%20quadrupling%20the%20compression%20ratio%20of%0Abz2%20for%20texts.%20The%20better%20a%20large%20model%20understands%20the%20data%2C%20the%20better%0ALMCompress%20compresses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07723v3&entry.124074799=Read"},
{"title": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using\n  Unsupervised Backbones", "author": "Giacomo Pacini and Lorenzo Bianchi and Luca Ciampi and Nicola Messina and Giuseppe Amato and Fabrizio Falchi", "abstract": "  Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we consistently outperform a baseline based on an SOTA\nunsupervised object detector under the same label- and training-free setting.\nAdditionally, we achieve competitive results -- and in some cases surpass --\ntraining-free methods that rely on supervised backbones, non-training-free\nunsupervised methods, as well as several fully supervised SOTA approaches. This\ndemonstrates that label- and training-free CAC can be both scalable and\neffective. Code: https://lorebianchi98.github.io/CountingDINO/.\n", "link": "http://arxiv.org/abs/2504.16570v2", "date": "2025-04-30", "relevancy": 2.1249, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5558}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5141}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CountingDINO%3A%20A%20Training-free%20Pipeline%20for%20Class-Agnostic%20Counting%20using%0A%20%20Unsupervised%20Backbones&body=Title%3A%20CountingDINO%3A%20A%20Training-free%20Pipeline%20for%20Class-Agnostic%20Counting%20using%0A%20%20Unsupervised%20Backbones%0AAuthor%3A%20Giacomo%20Pacini%20and%20Lorenzo%20Bianchi%20and%20Luca%20Ciampi%20and%20Nicola%20Messina%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi%0AAbstract%3A%20%20%20Class-agnostic%20counting%20%28CAC%29%20aims%20to%20estimate%20the%20number%20of%20objects%20in%0Aimages%20without%20being%20restricted%20to%20predefined%20categories.%20However%2C%20while%0Acurrent%20exemplar-based%20CAC%20methods%20offer%20flexibility%20at%20inference%20time%2C%20they%0Astill%20rely%20heavily%20on%20labeled%20data%20for%20training%2C%20which%20limits%20scalability%20and%0Ageneralization%20to%20many%20downstream%20use%20cases.%20In%20this%20paper%2C%20we%20introduce%0ACountingDINO%2C%20the%20first%20training-free%20exemplar-based%20CAC%20framework%20that%0Aexploits%20a%20fully%20unsupervised%20feature%20extractor.%20Specifically%2C%20our%20approach%0Aemploys%20self-supervised%20vision-only%20backbones%20to%20extract%20object-aware%20features%2C%0Aand%20it%20eliminates%20the%20need%20for%20annotated%20data%20throughout%20the%20entire%20proposed%0Apipeline.%20At%20inference%20time%2C%20we%20extract%20latent%20object%20prototypes%20via%20ROI-Align%0Afrom%20DINO%20features%20and%20use%20them%20as%20convolutional%20kernels%20to%20generate%20similarity%0Amaps.%20These%20are%20then%20transformed%20into%20density%20maps%20through%20a%20simple%20yet%0Aeffective%20normalization%20scheme.%20We%20evaluate%20our%20approach%20on%20the%20FSC-147%0Abenchmark%2C%20where%20we%20consistently%20outperform%20a%20baseline%20based%20on%20an%20SOTA%0Aunsupervised%20object%20detector%20under%20the%20same%20label-%20and%20training-free%20setting.%0AAdditionally%2C%20we%20achieve%20competitive%20results%20--%20and%20in%20some%20cases%20surpass%20--%0Atraining-free%20methods%20that%20rely%20on%20supervised%20backbones%2C%20non-training-free%0Aunsupervised%20methods%2C%20as%20well%20as%20several%20fully%20supervised%20SOTA%20approaches.%20This%0Ademonstrates%20that%20label-%20and%20training-free%20CAC%20can%20be%20both%20scalable%20and%0Aeffective.%20Code%3A%20https%3A//lorebianchi98.github.io/CountingDINO/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCountingDINO%253A%2520A%2520Training-free%2520Pipeline%2520for%2520Class-Agnostic%2520Counting%2520using%250A%2520%2520Unsupervised%2520Backbones%26entry.906535625%3DGiacomo%2520Pacini%2520and%2520Lorenzo%2520Bianchi%2520and%2520Luca%2520Ciampi%2520and%2520Nicola%2520Messina%2520and%2520Giuseppe%2520Amato%2520and%2520Fabrizio%2520Falchi%26entry.1292438233%3D%2520%2520Class-agnostic%2520counting%2520%2528CAC%2529%2520aims%2520to%2520estimate%2520the%2520number%2520of%2520objects%2520in%250Aimages%2520without%2520being%2520restricted%2520to%2520predefined%2520categories.%2520However%252C%2520while%250Acurrent%2520exemplar-based%2520CAC%2520methods%2520offer%2520flexibility%2520at%2520inference%2520time%252C%2520they%250Astill%2520rely%2520heavily%2520on%2520labeled%2520data%2520for%2520training%252C%2520which%2520limits%2520scalability%2520and%250Ageneralization%2520to%2520many%2520downstream%2520use%2520cases.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ACountingDINO%252C%2520the%2520first%2520training-free%2520exemplar-based%2520CAC%2520framework%2520that%250Aexploits%2520a%2520fully%2520unsupervised%2520feature%2520extractor.%2520Specifically%252C%2520our%2520approach%250Aemploys%2520self-supervised%2520vision-only%2520backbones%2520to%2520extract%2520object-aware%2520features%252C%250Aand%2520it%2520eliminates%2520the%2520need%2520for%2520annotated%2520data%2520throughout%2520the%2520entire%2520proposed%250Apipeline.%2520At%2520inference%2520time%252C%2520we%2520extract%2520latent%2520object%2520prototypes%2520via%2520ROI-Align%250Afrom%2520DINO%2520features%2520and%2520use%2520them%2520as%2520convolutional%2520kernels%2520to%2520generate%2520similarity%250Amaps.%2520These%2520are%2520then%2520transformed%2520into%2520density%2520maps%2520through%2520a%2520simple%2520yet%250Aeffective%2520normalization%2520scheme.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520FSC-147%250Abenchmark%252C%2520where%2520we%2520consistently%2520outperform%2520a%2520baseline%2520based%2520on%2520an%2520SOTA%250Aunsupervised%2520object%2520detector%2520under%2520the%2520same%2520label-%2520and%2520training-free%2520setting.%250AAdditionally%252C%2520we%2520achieve%2520competitive%2520results%2520--%2520and%2520in%2520some%2520cases%2520surpass%2520--%250Atraining-free%2520methods%2520that%2520rely%2520on%2520supervised%2520backbones%252C%2520non-training-free%250Aunsupervised%2520methods%252C%2520as%2520well%2520as%2520several%2520fully%2520supervised%2520SOTA%2520approaches.%2520This%250Ademonstrates%2520that%2520label-%2520and%2520training-free%2520CAC%2520can%2520be%2520both%2520scalable%2520and%250Aeffective.%2520Code%253A%2520https%253A//lorebianchi98.github.io/CountingDINO/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CountingDINO%3A%20A%20Training-free%20Pipeline%20for%20Class-Agnostic%20Counting%20using%0A%20%20Unsupervised%20Backbones&entry.906535625=Giacomo%20Pacini%20and%20Lorenzo%20Bianchi%20and%20Luca%20Ciampi%20and%20Nicola%20Messina%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi&entry.1292438233=%20%20Class-agnostic%20counting%20%28CAC%29%20aims%20to%20estimate%20the%20number%20of%20objects%20in%0Aimages%20without%20being%20restricted%20to%20predefined%20categories.%20However%2C%20while%0Acurrent%20exemplar-based%20CAC%20methods%20offer%20flexibility%20at%20inference%20time%2C%20they%0Astill%20rely%20heavily%20on%20labeled%20data%20for%20training%2C%20which%20limits%20scalability%20and%0Ageneralization%20to%20many%20downstream%20use%20cases.%20In%20this%20paper%2C%20we%20introduce%0ACountingDINO%2C%20the%20first%20training-free%20exemplar-based%20CAC%20framework%20that%0Aexploits%20a%20fully%20unsupervised%20feature%20extractor.%20Specifically%2C%20our%20approach%0Aemploys%20self-supervised%20vision-only%20backbones%20to%20extract%20object-aware%20features%2C%0Aand%20it%20eliminates%20the%20need%20for%20annotated%20data%20throughout%20the%20entire%20proposed%0Apipeline.%20At%20inference%20time%2C%20we%20extract%20latent%20object%20prototypes%20via%20ROI-Align%0Afrom%20DINO%20features%20and%20use%20them%20as%20convolutional%20kernels%20to%20generate%20similarity%0Amaps.%20These%20are%20then%20transformed%20into%20density%20maps%20through%20a%20simple%20yet%0Aeffective%20normalization%20scheme.%20We%20evaluate%20our%20approach%20on%20the%20FSC-147%0Abenchmark%2C%20where%20we%20consistently%20outperform%20a%20baseline%20based%20on%20an%20SOTA%0Aunsupervised%20object%20detector%20under%20the%20same%20label-%20and%20training-free%20setting.%0AAdditionally%2C%20we%20achieve%20competitive%20results%20--%20and%20in%20some%20cases%20surpass%20--%0Atraining-free%20methods%20that%20rely%20on%20supervised%20backbones%2C%20non-training-free%0Aunsupervised%20methods%2C%20as%20well%20as%20several%20fully%20supervised%20SOTA%20approaches.%20This%0Ademonstrates%20that%20label-%20and%20training-free%20CAC%20can%20be%20both%20scalable%20and%0Aeffective.%20Code%3A%20https%3A//lorebianchi98.github.io/CountingDINO/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16570v2&entry.124074799=Read"},
{"title": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers", "author": "Dylan Bouchard and Mohit Singh Chauhan", "abstract": "  Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.\n", "link": "http://arxiv.org/abs/2504.19254v2", "date": "2025-04-30", "relevancy": 2.1176, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5986}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20for%20Language%20Models%3A%20A%20Suite%20of%20Black-Box%2C%0A%20%20White-Box%2C%20LLM%20Judge%2C%20and%20Ensemble%20Scorers&body=Title%3A%20Uncertainty%20Quantification%20for%20Language%20Models%3A%20A%20Suite%20of%20Black-Box%2C%0A%20%20White-Box%2C%20LLM%20Judge%2C%20and%20Ensemble%20Scorers%0AAuthor%3A%20Dylan%20Bouchard%20and%20Mohit%20Singh%20Chauhan%0AAbstract%3A%20%20%20Hallucinations%20are%20a%20persistent%20problem%20with%20Large%20Language%20Models%20%28LLMs%29.%20As%0Athese%20models%20become%20increasingly%20used%20in%20high-stakes%20domains%2C%20such%20as%0Ahealthcare%20and%20finance%2C%20the%20need%20for%20effective%20hallucination%20detection%20is%0Acrucial.%20To%20this%20end%2C%20we%20propose%20a%20versatile%20framework%20for%20zero-resource%0Ahallucination%20detection%20that%20practitioners%20can%20apply%20to%20real-world%20use%20cases.%0ATo%20achieve%20this%2C%20we%20adapt%20a%20variety%20of%20existing%20uncertainty%20quantification%20%28UQ%29%0Atechniques%2C%20including%20black-box%20UQ%2C%20white-box%20UQ%2C%20and%20LLM-as-a-Judge%2C%0Atransforming%20them%20as%20necessary%20into%20standardized%20response-level%20confidence%0Ascores%20ranging%20from%200%20to%201.%20To%20enhance%20flexibility%2C%20we%20introduce%20a%20tunable%0Aensemble%20approach%20that%20incorporates%20any%20combination%20of%20the%20individual%0Aconfidence%20scores.%20This%20approach%20enables%20practitioners%20to%20optimize%20the%20ensemble%0Afor%20a%20specific%20use%20case%20for%20improved%20performance.%20To%20streamline%20implementation%2C%0Athe%20full%20suite%20of%20scorers%20is%20offered%20in%20this%20paper%27s%20companion%20Python%20toolkit%2C%0AUQLM.%20To%20evaluate%20the%20performance%20of%20the%20various%20scorers%2C%20we%20conduct%20an%0Aextensive%20set%20of%20experiments%20using%20several%20LLM%20question-answering%20benchmarks.%0AWe%20find%20that%20our%20tunable%20ensemble%20typically%20surpasses%20its%20individual%20components%0Aand%20outperforms%20existing%20hallucination%20detection%20methods.%20Our%20results%0Ademonstrate%20the%20benefits%20of%20customized%20hallucination%20detection%20strategies%20for%0Aimproving%20the%20accuracy%20and%20reliability%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520for%2520Language%2520Models%253A%2520A%2520Suite%2520of%2520Black-Box%252C%250A%2520%2520White-Box%252C%2520LLM%2520Judge%252C%2520and%2520Ensemble%2520Scorers%26entry.906535625%3DDylan%2520Bouchard%2520and%2520Mohit%2520Singh%2520Chauhan%26entry.1292438233%3D%2520%2520Hallucinations%2520are%2520a%2520persistent%2520problem%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520As%250Athese%2520models%2520become%2520increasingly%2520used%2520in%2520high-stakes%2520domains%252C%2520such%2520as%250Ahealthcare%2520and%2520finance%252C%2520the%2520need%2520for%2520effective%2520hallucination%2520detection%2520is%250Acrucial.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520versatile%2520framework%2520for%2520zero-resource%250Ahallucination%2520detection%2520that%2520practitioners%2520can%2520apply%2520to%2520real-world%2520use%2520cases.%250ATo%2520achieve%2520this%252C%2520we%2520adapt%2520a%2520variety%2520of%2520existing%2520uncertainty%2520quantification%2520%2528UQ%2529%250Atechniques%252C%2520including%2520black-box%2520UQ%252C%2520white-box%2520UQ%252C%2520and%2520LLM-as-a-Judge%252C%250Atransforming%2520them%2520as%2520necessary%2520into%2520standardized%2520response-level%2520confidence%250Ascores%2520ranging%2520from%25200%2520to%25201.%2520To%2520enhance%2520flexibility%252C%2520we%2520introduce%2520a%2520tunable%250Aensemble%2520approach%2520that%2520incorporates%2520any%2520combination%2520of%2520the%2520individual%250Aconfidence%2520scores.%2520This%2520approach%2520enables%2520practitioners%2520to%2520optimize%2520the%2520ensemble%250Afor%2520a%2520specific%2520use%2520case%2520for%2520improved%2520performance.%2520To%2520streamline%2520implementation%252C%250Athe%2520full%2520suite%2520of%2520scorers%2520is%2520offered%2520in%2520this%2520paper%2527s%2520companion%2520Python%2520toolkit%252C%250AUQLM.%2520To%2520evaluate%2520the%2520performance%2520of%2520the%2520various%2520scorers%252C%2520we%2520conduct%2520an%250Aextensive%2520set%2520of%2520experiments%2520using%2520several%2520LLM%2520question-answering%2520benchmarks.%250AWe%2520find%2520that%2520our%2520tunable%2520ensemble%2520typically%2520surpasses%2520its%2520individual%2520components%250Aand%2520outperforms%2520existing%2520hallucination%2520detection%2520methods.%2520Our%2520results%250Ademonstrate%2520the%2520benefits%2520of%2520customized%2520hallucination%2520detection%2520strategies%2520for%250Aimproving%2520the%2520accuracy%2520and%2520reliability%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20for%20Language%20Models%3A%20A%20Suite%20of%20Black-Box%2C%0A%20%20White-Box%2C%20LLM%20Judge%2C%20and%20Ensemble%20Scorers&entry.906535625=Dylan%20Bouchard%20and%20Mohit%20Singh%20Chauhan&entry.1292438233=%20%20Hallucinations%20are%20a%20persistent%20problem%20with%20Large%20Language%20Models%20%28LLMs%29.%20As%0Athese%20models%20become%20increasingly%20used%20in%20high-stakes%20domains%2C%20such%20as%0Ahealthcare%20and%20finance%2C%20the%20need%20for%20effective%20hallucination%20detection%20is%0Acrucial.%20To%20this%20end%2C%20we%20propose%20a%20versatile%20framework%20for%20zero-resource%0Ahallucination%20detection%20that%20practitioners%20can%20apply%20to%20real-world%20use%20cases.%0ATo%20achieve%20this%2C%20we%20adapt%20a%20variety%20of%20existing%20uncertainty%20quantification%20%28UQ%29%0Atechniques%2C%20including%20black-box%20UQ%2C%20white-box%20UQ%2C%20and%20LLM-as-a-Judge%2C%0Atransforming%20them%20as%20necessary%20into%20standardized%20response-level%20confidence%0Ascores%20ranging%20from%200%20to%201.%20To%20enhance%20flexibility%2C%20we%20introduce%20a%20tunable%0Aensemble%20approach%20that%20incorporates%20any%20combination%20of%20the%20individual%0Aconfidence%20scores.%20This%20approach%20enables%20practitioners%20to%20optimize%20the%20ensemble%0Afor%20a%20specific%20use%20case%20for%20improved%20performance.%20To%20streamline%20implementation%2C%0Athe%20full%20suite%20of%20scorers%20is%20offered%20in%20this%20paper%27s%20companion%20Python%20toolkit%2C%0AUQLM.%20To%20evaluate%20the%20performance%20of%20the%20various%20scorers%2C%20we%20conduct%20an%0Aextensive%20set%20of%20experiments%20using%20several%20LLM%20question-answering%20benchmarks.%0AWe%20find%20that%20our%20tunable%20ensemble%20typically%20surpasses%20its%20individual%20components%0Aand%20outperforms%20existing%20hallucination%20detection%20methods.%20Our%20results%0Ademonstrate%20the%20benefits%20of%20customized%20hallucination%20detection%20strategies%20for%0Aimproving%20the%20accuracy%20and%20reliability%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19254v2&entry.124074799=Read"},
{"title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability", "author": "Xiaoxi Li and Jiajie Jin and Guanting Dong and Hongjin Qian and Yutao Zhu and Yongkang Wu and Ji-Rong Wen and Zhicheng Dou", "abstract": "  Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.\n", "link": "http://arxiv.org/abs/2504.21776v1", "date": "2025-04-30", "relevancy": 2.1069, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WebThinker%3A%20Empowering%20Large%20Reasoning%20Models%20with%20Deep%20Research%0A%20%20Capability&body=Title%3A%20WebThinker%3A%20Empowering%20Large%20Reasoning%20Models%20with%20Deep%20Research%0A%20%20Capability%0AAuthor%3A%20Xiaoxi%20Li%20and%20Jiajie%20Jin%20and%20Guanting%20Dong%20and%20Hongjin%20Qian%20and%20Yutao%20Zhu%20and%20Yongkang%20Wu%20and%20Ji-Rong%20Wen%20and%20Zhicheng%20Dou%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%2C%20such%20as%20OpenAI-o1%20and%20DeepSeek-R1%2C%20demonstrate%0Aimpressive%20long-horizon%20reasoning%20capabilities.%20However%2C%20their%20reliance%20on%0Astatic%20internal%20knowledge%20limits%20their%20performance%20on%20complex%2C%0Aknowledge-intensive%20tasks%20and%20hinders%20their%20ability%20to%20produce%20comprehensive%0Aresearch%20reports%20requiring%20synthesis%20of%20diverse%20web%20information.%20To%20address%0Athis%2C%20we%20propose%20%5Ctextbf%7BWebThinker%7D%2C%20a%20deep%20research%20agent%20that%20empowers%20LRMs%0Ato%20autonomously%20search%20the%20web%2C%20navigate%20web%20pages%2C%20and%20draft%20research%20reports%0Aduring%20the%20reasoning%20process.%20WebThinker%20integrates%20a%20%5Ctextbf%7BDeep%20Web%0AExplorer%7D%20module%2C%20enabling%20LRMs%20to%20dynamically%20search%2C%20navigate%2C%20and%20extract%0Ainformation%20from%20the%20web%20when%20encountering%20knowledge%20gaps.%20It%20also%20employs%20an%0A%5Ctextbf%7BAutonomous%20Think-Search-and-Draft%20strategy%7D%2C%20allowing%20the%20model%20to%0Aseamlessly%20interleave%20reasoning%2C%20information%20gathering%2C%20and%20report%20writing%20in%0Areal%20time.%20To%20further%20enhance%20research%20tool%20utilization%2C%20we%20introduce%20an%0A%5Ctextbf%7BRL-based%20training%20strategy%7D%20via%20iterative%20online%20Direct%20Preference%0AOptimization%20%28DPO%29.%20Extensive%20experiments%20on%20complex%20reasoning%20benchmarks%0A%28GPQA%2C%20GAIA%2C%20WebWalkerQA%2C%20HLE%29%20and%20scientific%20report%20generation%20tasks%20%28Glaive%29%0Ademonstrate%20that%20WebThinker%20significantly%20outperforms%20existing%20methods%20and%0Astrong%20proprietary%20systems.%20Our%20approach%20enhances%20LRM%20reliability%20and%0Aapplicability%20in%20complex%20scenarios%2C%20paving%20the%20way%20for%20more%20capable%20and%0Aversatile%20deep%20research%20systems.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/RUC-NLPIR/WebThinker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWebThinker%253A%2520Empowering%2520Large%2520Reasoning%2520Models%2520with%2520Deep%2520Research%250A%2520%2520Capability%26entry.906535625%3DXiaoxi%2520Li%2520and%2520Jiajie%2520Jin%2520and%2520Guanting%2520Dong%2520and%2520Hongjin%2520Qian%2520and%2520Yutao%2520Zhu%2520and%2520Yongkang%2520Wu%2520and%2520Ji-Rong%2520Wen%2520and%2520Zhicheng%2520Dou%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%252C%2520such%2520as%2520OpenAI-o1%2520and%2520DeepSeek-R1%252C%2520demonstrate%250Aimpressive%2520long-horizon%2520reasoning%2520capabilities.%2520However%252C%2520their%2520reliance%2520on%250Astatic%2520internal%2520knowledge%2520limits%2520their%2520performance%2520on%2520complex%252C%250Aknowledge-intensive%2520tasks%2520and%2520hinders%2520their%2520ability%2520to%2520produce%2520comprehensive%250Aresearch%2520reports%2520requiring%2520synthesis%2520of%2520diverse%2520web%2520information.%2520To%2520address%250Athis%252C%2520we%2520propose%2520%255Ctextbf%257BWebThinker%257D%252C%2520a%2520deep%2520research%2520agent%2520that%2520empowers%2520LRMs%250Ato%2520autonomously%2520search%2520the%2520web%252C%2520navigate%2520web%2520pages%252C%2520and%2520draft%2520research%2520reports%250Aduring%2520the%2520reasoning%2520process.%2520WebThinker%2520integrates%2520a%2520%255Ctextbf%257BDeep%2520Web%250AExplorer%257D%2520module%252C%2520enabling%2520LRMs%2520to%2520dynamically%2520search%252C%2520navigate%252C%2520and%2520extract%250Ainformation%2520from%2520the%2520web%2520when%2520encountering%2520knowledge%2520gaps.%2520It%2520also%2520employs%2520an%250A%255Ctextbf%257BAutonomous%2520Think-Search-and-Draft%2520strategy%257D%252C%2520allowing%2520the%2520model%2520to%250Aseamlessly%2520interleave%2520reasoning%252C%2520information%2520gathering%252C%2520and%2520report%2520writing%2520in%250Areal%2520time.%2520To%2520further%2520enhance%2520research%2520tool%2520utilization%252C%2520we%2520introduce%2520an%250A%255Ctextbf%257BRL-based%2520training%2520strategy%257D%2520via%2520iterative%2520online%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529.%2520Extensive%2520experiments%2520on%2520complex%2520reasoning%2520benchmarks%250A%2528GPQA%252C%2520GAIA%252C%2520WebWalkerQA%252C%2520HLE%2529%2520and%2520scientific%2520report%2520generation%2520tasks%2520%2528Glaive%2529%250Ademonstrate%2520that%2520WebThinker%2520significantly%2520outperforms%2520existing%2520methods%2520and%250Astrong%2520proprietary%2520systems.%2520Our%2520approach%2520enhances%2520LRM%2520reliability%2520and%250Aapplicability%2520in%2520complex%2520scenarios%252C%2520paving%2520the%2520way%2520for%2520more%2520capable%2520and%250Aversatile%2520deep%2520research%2520systems.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/RUC-NLPIR/WebThinker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WebThinker%3A%20Empowering%20Large%20Reasoning%20Models%20with%20Deep%20Research%0A%20%20Capability&entry.906535625=Xiaoxi%20Li%20and%20Jiajie%20Jin%20and%20Guanting%20Dong%20and%20Hongjin%20Qian%20and%20Yutao%20Zhu%20and%20Yongkang%20Wu%20and%20Ji-Rong%20Wen%20and%20Zhicheng%20Dou&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%2C%20such%20as%20OpenAI-o1%20and%20DeepSeek-R1%2C%20demonstrate%0Aimpressive%20long-horizon%20reasoning%20capabilities.%20However%2C%20their%20reliance%20on%0Astatic%20internal%20knowledge%20limits%20their%20performance%20on%20complex%2C%0Aknowledge-intensive%20tasks%20and%20hinders%20their%20ability%20to%20produce%20comprehensive%0Aresearch%20reports%20requiring%20synthesis%20of%20diverse%20web%20information.%20To%20address%0Athis%2C%20we%20propose%20%5Ctextbf%7BWebThinker%7D%2C%20a%20deep%20research%20agent%20that%20empowers%20LRMs%0Ato%20autonomously%20search%20the%20web%2C%20navigate%20web%20pages%2C%20and%20draft%20research%20reports%0Aduring%20the%20reasoning%20process.%20WebThinker%20integrates%20a%20%5Ctextbf%7BDeep%20Web%0AExplorer%7D%20module%2C%20enabling%20LRMs%20to%20dynamically%20search%2C%20navigate%2C%20and%20extract%0Ainformation%20from%20the%20web%20when%20encountering%20knowledge%20gaps.%20It%20also%20employs%20an%0A%5Ctextbf%7BAutonomous%20Think-Search-and-Draft%20strategy%7D%2C%20allowing%20the%20model%20to%0Aseamlessly%20interleave%20reasoning%2C%20information%20gathering%2C%20and%20report%20writing%20in%0Areal%20time.%20To%20further%20enhance%20research%20tool%20utilization%2C%20we%20introduce%20an%0A%5Ctextbf%7BRL-based%20training%20strategy%7D%20via%20iterative%20online%20Direct%20Preference%0AOptimization%20%28DPO%29.%20Extensive%20experiments%20on%20complex%20reasoning%20benchmarks%0A%28GPQA%2C%20GAIA%2C%20WebWalkerQA%2C%20HLE%29%20and%20scientific%20report%20generation%20tasks%20%28Glaive%29%0Ademonstrate%20that%20WebThinker%20significantly%20outperforms%20existing%20methods%20and%0Astrong%20proprietary%20systems.%20Our%20approach%20enhances%20LRM%20reliability%20and%0Aapplicability%20in%20complex%20scenarios%2C%20paving%20the%20way%20for%20more%20capable%20and%0Aversatile%20deep%20research%20systems.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/RUC-NLPIR/WebThinker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21776v1&entry.124074799=Read"},
{"title": "Sionna RT: Technical Report", "author": "Fay\u00e7al A\u00eft Aoudia and Jakob Hoydis and Merlin Nimier-David and Sebastian Cammerer and Alexander Keller", "abstract": "  Sionna is an open-source, GPU-accelerated library that, as of version 0.14,\nincorporates a ray tracer for simulating radio wave propagation. A unique\nfeature of Sionna RT is differentiability, enabling the calculation of\ngradients for the channel impulse responses (CIRs), radio maps, and other\nrelated metrics with respect to system and environmental parameters, such as\nmaterial properties, antenna patterns, and array geometries. The release of\nSionna 1.0 provides a complete overhaul of the ray tracer, significantly\nimproving its speed, memory efficiency, and extensibility. This document\ndetails the algorithms employed by Sionna RT to simulate radio wave propagation\nefficiently, while also addressing their current limitations. Given that the\ncomputation of CIRs and radio maps requires distinct algorithms, these are\ndetailed in separate sections. For CIRs, Sionna RT integrates shooting and\nbouncing of rays (SBR) with the image method and uses a hashing-based mechanism\nto efficiently eliminate duplicate paths. Radio maps are computed using a\npurely SBR-based approach.\n", "link": "http://arxiv.org/abs/2504.21719v1", "date": "2025-04-30", "relevancy": 2.1064, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.457}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4034}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sionna%20RT%3A%20Technical%20Report&body=Title%3A%20Sionna%20RT%3A%20Technical%20Report%0AAuthor%3A%20Fay%C3%A7al%20A%C3%AFt%20Aoudia%20and%20Jakob%20Hoydis%20and%20Merlin%20Nimier-David%20and%20Sebastian%20Cammerer%20and%20Alexander%20Keller%0AAbstract%3A%20%20%20Sionna%20is%20an%20open-source%2C%20GPU-accelerated%20library%20that%2C%20as%20of%20version%200.14%2C%0Aincorporates%20a%20ray%20tracer%20for%20simulating%20radio%20wave%20propagation.%20A%20unique%0Afeature%20of%20Sionna%20RT%20is%20differentiability%2C%20enabling%20the%20calculation%20of%0Agradients%20for%20the%20channel%20impulse%20responses%20%28CIRs%29%2C%20radio%20maps%2C%20and%20other%0Arelated%20metrics%20with%20respect%20to%20system%20and%20environmental%20parameters%2C%20such%20as%0Amaterial%20properties%2C%20antenna%20patterns%2C%20and%20array%20geometries.%20The%20release%20of%0ASionna%201.0%20provides%20a%20complete%20overhaul%20of%20the%20ray%20tracer%2C%20significantly%0Aimproving%20its%20speed%2C%20memory%20efficiency%2C%20and%20extensibility.%20This%20document%0Adetails%20the%20algorithms%20employed%20by%20Sionna%20RT%20to%20simulate%20radio%20wave%20propagation%0Aefficiently%2C%20while%20also%20addressing%20their%20current%20limitations.%20Given%20that%20the%0Acomputation%20of%20CIRs%20and%20radio%20maps%20requires%20distinct%20algorithms%2C%20these%20are%0Adetailed%20in%20separate%20sections.%20For%20CIRs%2C%20Sionna%20RT%20integrates%20shooting%20and%0Abouncing%20of%20rays%20%28SBR%29%20with%20the%20image%20method%20and%20uses%20a%20hashing-based%20mechanism%0Ato%20efficiently%20eliminate%20duplicate%20paths.%20Radio%20maps%20are%20computed%20using%20a%0Apurely%20SBR-based%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSionna%2520RT%253A%2520Technical%2520Report%26entry.906535625%3DFay%25C3%25A7al%2520A%25C3%25AFt%2520Aoudia%2520and%2520Jakob%2520Hoydis%2520and%2520Merlin%2520Nimier-David%2520and%2520Sebastian%2520Cammerer%2520and%2520Alexander%2520Keller%26entry.1292438233%3D%2520%2520Sionna%2520is%2520an%2520open-source%252C%2520GPU-accelerated%2520library%2520that%252C%2520as%2520of%2520version%25200.14%252C%250Aincorporates%2520a%2520ray%2520tracer%2520for%2520simulating%2520radio%2520wave%2520propagation.%2520A%2520unique%250Afeature%2520of%2520Sionna%2520RT%2520is%2520differentiability%252C%2520enabling%2520the%2520calculation%2520of%250Agradients%2520for%2520the%2520channel%2520impulse%2520responses%2520%2528CIRs%2529%252C%2520radio%2520maps%252C%2520and%2520other%250Arelated%2520metrics%2520with%2520respect%2520to%2520system%2520and%2520environmental%2520parameters%252C%2520such%2520as%250Amaterial%2520properties%252C%2520antenna%2520patterns%252C%2520and%2520array%2520geometries.%2520The%2520release%2520of%250ASionna%25201.0%2520provides%2520a%2520complete%2520overhaul%2520of%2520the%2520ray%2520tracer%252C%2520significantly%250Aimproving%2520its%2520speed%252C%2520memory%2520efficiency%252C%2520and%2520extensibility.%2520This%2520document%250Adetails%2520the%2520algorithms%2520employed%2520by%2520Sionna%2520RT%2520to%2520simulate%2520radio%2520wave%2520propagation%250Aefficiently%252C%2520while%2520also%2520addressing%2520their%2520current%2520limitations.%2520Given%2520that%2520the%250Acomputation%2520of%2520CIRs%2520and%2520radio%2520maps%2520requires%2520distinct%2520algorithms%252C%2520these%2520are%250Adetailed%2520in%2520separate%2520sections.%2520For%2520CIRs%252C%2520Sionna%2520RT%2520integrates%2520shooting%2520and%250Abouncing%2520of%2520rays%2520%2528SBR%2529%2520with%2520the%2520image%2520method%2520and%2520uses%2520a%2520hashing-based%2520mechanism%250Ato%2520efficiently%2520eliminate%2520duplicate%2520paths.%2520Radio%2520maps%2520are%2520computed%2520using%2520a%250Apurely%2520SBR-based%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sionna%20RT%3A%20Technical%20Report&entry.906535625=Fay%C3%A7al%20A%C3%AFt%20Aoudia%20and%20Jakob%20Hoydis%20and%20Merlin%20Nimier-David%20and%20Sebastian%20Cammerer%20and%20Alexander%20Keller&entry.1292438233=%20%20Sionna%20is%20an%20open-source%2C%20GPU-accelerated%20library%20that%2C%20as%20of%20version%200.14%2C%0Aincorporates%20a%20ray%20tracer%20for%20simulating%20radio%20wave%20propagation.%20A%20unique%0Afeature%20of%20Sionna%20RT%20is%20differentiability%2C%20enabling%20the%20calculation%20of%0Agradients%20for%20the%20channel%20impulse%20responses%20%28CIRs%29%2C%20radio%20maps%2C%20and%20other%0Arelated%20metrics%20with%20respect%20to%20system%20and%20environmental%20parameters%2C%20such%20as%0Amaterial%20properties%2C%20antenna%20patterns%2C%20and%20array%20geometries.%20The%20release%20of%0ASionna%201.0%20provides%20a%20complete%20overhaul%20of%20the%20ray%20tracer%2C%20significantly%0Aimproving%20its%20speed%2C%20memory%20efficiency%2C%20and%20extensibility.%20This%20document%0Adetails%20the%20algorithms%20employed%20by%20Sionna%20RT%20to%20simulate%20radio%20wave%20propagation%0Aefficiently%2C%20while%20also%20addressing%20their%20current%20limitations.%20Given%20that%20the%0Acomputation%20of%20CIRs%20and%20radio%20maps%20requires%20distinct%20algorithms%2C%20these%20are%0Adetailed%20in%20separate%20sections.%20For%20CIRs%2C%20Sionna%20RT%20integrates%20shooting%20and%0Abouncing%20of%20rays%20%28SBR%29%20with%20the%20image%20method%20and%20uses%20a%20hashing-based%20mechanism%0Ato%20efficiently%20eliminate%20duplicate%20paths.%20Radio%20maps%20are%20computed%20using%20a%0Apurely%20SBR-based%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21719v1&entry.124074799=Read"},
{"title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with\n  Multi-Modality Observation Data", "author": "Qi Wang and Xiao Zhang and Mingyi Li and Yuan Yuan and Mengbai Xiao and Fuzhen Zhuang and Dongxiao Yu", "abstract": "  With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness.\n", "link": "http://arxiv.org/abs/2504.20462v2", "date": "2025-04-30", "relevancy": 2.096, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5614}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAMO%3AFine-Grained%20Root%20Cause%20Analysis%20via%20Tool-Assisted%20LLM%20Agent%20with%0A%20%20Multi-Modality%20Observation%20Data&body=Title%3A%20TAMO%3AFine-Grained%20Root%20Cause%20Analysis%20via%20Tool-Assisted%20LLM%20Agent%20with%0A%20%20Multi-Modality%20Observation%20Data%0AAuthor%3A%20Qi%20Wang%20and%20Xiao%20Zhang%20and%20Mingyi%20Li%20and%20Yuan%20Yuan%20and%20Mengbai%20Xiao%20and%20Fuzhen%20Zhuang%20and%20Dongxiao%20Yu%0AAbstract%3A%20%20%20With%20the%20development%20of%20distributed%20systems%2C%20microservices%20and%20cloud%20native%0Atechnologies%20have%20become%20central%20to%20modern%20enterprise%20software%20development.%0ADespite%20bringing%20significant%20advantages%2C%20these%20technologies%20also%20increase%0Asystem%20complexity%20and%20operational%20challenges.%20Traditional%20root%20cause%20analysis%0A%28RCA%29%20struggles%20to%20achieve%20automated%20fault%20response%2C%20heavily%20relying%20on%20manual%0Aintervention.%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20made%0Abreakthroughs%20in%20contextual%20inference%20and%20domain%20knowledge%20integration%2C%0Aproviding%20new%20solutions%20for%20Artificial%20Intelligence%20for%20Operations%20%28AIOps%29.%0AHowever%2C%20Existing%20LLM-based%20approaches%20face%20three%20key%20challenges%3A%20text%20input%0Aconstraints%2C%20dynamic%20service%20dependency%20hallucinations%2C%20and%20context%20window%0Alimitations.%20To%20address%20these%20issues%2C%20we%20propose%20a%20tool-assisted%20LLM%20agent%20with%0Amulti-modality%20observation%20data%2C%20namely%20TAMO%2C%20for%20fine-grained%20RCA.%20It%20unifies%0Amulti-modal%20observational%20data%20into%20time-aligned%20representations%20to%20extract%0Aconsistent%20features%20and%20employs%20specialized%20root%20cause%20localization%20and%20fault%0Aclassification%20tools%20for%20perceiving%20the%20contextual%20environment.%20This%20approach%0Aovercomes%20the%20limitations%20of%20LLM%20in%20handling%20real-time%20changing%20service%0Adependencies%20and%20raw%20observational%20data%20and%20guides%20LLM%20to%20generate%20repair%0Astrategies%20aligned%20with%20system%20contexts%20by%20structuring%20key%20information%20into%20a%0Aprompt.%20Experimental%20results%20show%20that%20TAMO%20performs%20well%20in%20root%20cause%0Aanalysis%20when%20dealing%20with%20public%20datasets%20characterized%20by%20heterogeneity%20and%0Acommon%20fault%20types%2C%20demonstrating%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20462v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAMO%253AFine-Grained%2520Root%2520Cause%2520Analysis%2520via%2520Tool-Assisted%2520LLM%2520Agent%2520with%250A%2520%2520Multi-Modality%2520Observation%2520Data%26entry.906535625%3DQi%2520Wang%2520and%2520Xiao%2520Zhang%2520and%2520Mingyi%2520Li%2520and%2520Yuan%2520Yuan%2520and%2520Mengbai%2520Xiao%2520and%2520Fuzhen%2520Zhuang%2520and%2520Dongxiao%2520Yu%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520distributed%2520systems%252C%2520microservices%2520and%2520cloud%2520native%250Atechnologies%2520have%2520become%2520central%2520to%2520modern%2520enterprise%2520software%2520development.%250ADespite%2520bringing%2520significant%2520advantages%252C%2520these%2520technologies%2520also%2520increase%250Asystem%2520complexity%2520and%2520operational%2520challenges.%2520Traditional%2520root%2520cause%2520analysis%250A%2528RCA%2529%2520struggles%2520to%2520achieve%2520automated%2520fault%2520response%252C%2520heavily%2520relying%2520on%2520manual%250Aintervention.%2520In%2520recent%2520years%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%250Abreakthroughs%2520in%2520contextual%2520inference%2520and%2520domain%2520knowledge%2520integration%252C%250Aproviding%2520new%2520solutions%2520for%2520Artificial%2520Intelligence%2520for%2520Operations%2520%2528AIOps%2529.%250AHowever%252C%2520Existing%2520LLM-based%2520approaches%2520face%2520three%2520key%2520challenges%253A%2520text%2520input%250Aconstraints%252C%2520dynamic%2520service%2520dependency%2520hallucinations%252C%2520and%2520context%2520window%250Alimitations.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520tool-assisted%2520LLM%2520agent%2520with%250Amulti-modality%2520observation%2520data%252C%2520namely%2520TAMO%252C%2520for%2520fine-grained%2520RCA.%2520It%2520unifies%250Amulti-modal%2520observational%2520data%2520into%2520time-aligned%2520representations%2520to%2520extract%250Aconsistent%2520features%2520and%2520employs%2520specialized%2520root%2520cause%2520localization%2520and%2520fault%250Aclassification%2520tools%2520for%2520perceiving%2520the%2520contextual%2520environment.%2520This%2520approach%250Aovercomes%2520the%2520limitations%2520of%2520LLM%2520in%2520handling%2520real-time%2520changing%2520service%250Adependencies%2520and%2520raw%2520observational%2520data%2520and%2520guides%2520LLM%2520to%2520generate%2520repair%250Astrategies%2520aligned%2520with%2520system%2520contexts%2520by%2520structuring%2520key%2520information%2520into%2520a%250Aprompt.%2520Experimental%2520results%2520show%2520that%2520TAMO%2520performs%2520well%2520in%2520root%2520cause%250Aanalysis%2520when%2520dealing%2520with%2520public%2520datasets%2520characterized%2520by%2520heterogeneity%2520and%250Acommon%2520fault%2520types%252C%2520demonstrating%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20462v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAMO%3AFine-Grained%20Root%20Cause%20Analysis%20via%20Tool-Assisted%20LLM%20Agent%20with%0A%20%20Multi-Modality%20Observation%20Data&entry.906535625=Qi%20Wang%20and%20Xiao%20Zhang%20and%20Mingyi%20Li%20and%20Yuan%20Yuan%20and%20Mengbai%20Xiao%20and%20Fuzhen%20Zhuang%20and%20Dongxiao%20Yu&entry.1292438233=%20%20With%20the%20development%20of%20distributed%20systems%2C%20microservices%20and%20cloud%20native%0Atechnologies%20have%20become%20central%20to%20modern%20enterprise%20software%20development.%0ADespite%20bringing%20significant%20advantages%2C%20these%20technologies%20also%20increase%0Asystem%20complexity%20and%20operational%20challenges.%20Traditional%20root%20cause%20analysis%0A%28RCA%29%20struggles%20to%20achieve%20automated%20fault%20response%2C%20heavily%20relying%20on%20manual%0Aintervention.%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20made%0Abreakthroughs%20in%20contextual%20inference%20and%20domain%20knowledge%20integration%2C%0Aproviding%20new%20solutions%20for%20Artificial%20Intelligence%20for%20Operations%20%28AIOps%29.%0AHowever%2C%20Existing%20LLM-based%20approaches%20face%20three%20key%20challenges%3A%20text%20input%0Aconstraints%2C%20dynamic%20service%20dependency%20hallucinations%2C%20and%20context%20window%0Alimitations.%20To%20address%20these%20issues%2C%20we%20propose%20a%20tool-assisted%20LLM%20agent%20with%0Amulti-modality%20observation%20data%2C%20namely%20TAMO%2C%20for%20fine-grained%20RCA.%20It%20unifies%0Amulti-modal%20observational%20data%20into%20time-aligned%20representations%20to%20extract%0Aconsistent%20features%20and%20employs%20specialized%20root%20cause%20localization%20and%20fault%0Aclassification%20tools%20for%20perceiving%20the%20contextual%20environment.%20This%20approach%0Aovercomes%20the%20limitations%20of%20LLM%20in%20handling%20real-time%20changing%20service%0Adependencies%20and%20raw%20observational%20data%20and%20guides%20LLM%20to%20generate%20repair%0Astrategies%20aligned%20with%20system%20contexts%20by%20structuring%20key%20information%20into%20a%0Aprompt.%20Experimental%20results%20show%20that%20TAMO%20performs%20well%20in%20root%20cause%0Aanalysis%20when%20dealing%20with%20public%20datasets%20characterized%20by%20heterogeneity%20and%0Acommon%20fault%20types%2C%20demonstrating%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20462v2&entry.124074799=Read"},
{"title": "WARP-LCA: Efficient Convolutional Sparse Coding with Locally Competitive\n  Algorithm", "author": "Geoffrey Kasenbacher and Felix Ehret and Gerrit Ecke and Sebastian Otte", "abstract": "  The locally competitive algorithm (LCA) can solve sparse coding problems\nacross a wide range of use cases. Recently, convolution-based LCA approaches\nhave been shown to be highly effective for enhancing robustness for image\nrecognition tasks in vision pipelines. To additionally maximize\nrepresentational sparsity, LCA with hard-thresholding can be applied. While\nthis combination often yields very good solutions satisfying an $\\ell_0$\nsparsity criterion, it comes with significant drawbacks for practical\napplication: (i) LCA is very inefficient, typically requiring hundreds of\noptimization cycles for convergence; (ii) the use of hard-thresholding results\nin a non-convex loss function, which might lead to suboptimal minima. To\naddress these issues, we propose the Locally Competitive Algorithm with State\nWarm-up via Predictive Priming (WARP-LCA), which leverages a predictor network\nto provide a suitable initial guess of the LCA state based on the current\ninput. Our approach significantly improves both convergence speed and the\nquality of solutions, while maintaining and even enhancing the overall\nstrengths of LCA. We demonstrate that WARP-LCA converges faster by orders of\nmagnitude and reaches better minima compared to conventional LCA. Moreover, the\nlearned representations are more sparse and exhibit superior properties in\nterms of reconstruction and denoising quality as well as robustness when\napplied in deep recognition pipelines. Furthermore, we apply WARP-LCA to image\ndenoising tasks, showcasing its robustness and practical effectiveness. Our\nfindings confirm that the naive use of LCA with hard-thresholding results in\nsuboptimal minima, whereas initializing LCA with a predictive guess results in\nbetter outcomes. This research advances the field of biologically inspired deep\nlearning by providing a novel approach to convolutional sparse coding.\n", "link": "http://arxiv.org/abs/2410.18794v2", "date": "2025-04-30", "relevancy": 2.0898, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5284}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WARP-LCA%3A%20Efficient%20Convolutional%20Sparse%20Coding%20with%20Locally%20Competitive%0A%20%20Algorithm&body=Title%3A%20WARP-LCA%3A%20Efficient%20Convolutional%20Sparse%20Coding%20with%20Locally%20Competitive%0A%20%20Algorithm%0AAuthor%3A%20Geoffrey%20Kasenbacher%20and%20Felix%20Ehret%20and%20Gerrit%20Ecke%20and%20Sebastian%20Otte%0AAbstract%3A%20%20%20The%20locally%20competitive%20algorithm%20%28LCA%29%20can%20solve%20sparse%20coding%20problems%0Aacross%20a%20wide%20range%20of%20use%20cases.%20Recently%2C%20convolution-based%20LCA%20approaches%0Ahave%20been%20shown%20to%20be%20highly%20effective%20for%20enhancing%20robustness%20for%20image%0Arecognition%20tasks%20in%20vision%20pipelines.%20To%20additionally%20maximize%0Arepresentational%20sparsity%2C%20LCA%20with%20hard-thresholding%20can%20be%20applied.%20While%0Athis%20combination%20often%20yields%20very%20good%20solutions%20satisfying%20an%20%24%5Cell_0%24%0Asparsity%20criterion%2C%20it%20comes%20with%20significant%20drawbacks%20for%20practical%0Aapplication%3A%20%28i%29%20LCA%20is%20very%20inefficient%2C%20typically%20requiring%20hundreds%20of%0Aoptimization%20cycles%20for%20convergence%3B%20%28ii%29%20the%20use%20of%20hard-thresholding%20results%0Ain%20a%20non-convex%20loss%20function%2C%20which%20might%20lead%20to%20suboptimal%20minima.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Locally%20Competitive%20Algorithm%20with%20State%0AWarm-up%20via%20Predictive%20Priming%20%28WARP-LCA%29%2C%20which%20leverages%20a%20predictor%20network%0Ato%20provide%20a%20suitable%20initial%20guess%20of%20the%20LCA%20state%20based%20on%20the%20current%0Ainput.%20Our%20approach%20significantly%20improves%20both%20convergence%20speed%20and%20the%0Aquality%20of%20solutions%2C%20while%20maintaining%20and%20even%20enhancing%20the%20overall%0Astrengths%20of%20LCA.%20We%20demonstrate%20that%20WARP-LCA%20converges%20faster%20by%20orders%20of%0Amagnitude%20and%20reaches%20better%20minima%20compared%20to%20conventional%20LCA.%20Moreover%2C%20the%0Alearned%20representations%20are%20more%20sparse%20and%20exhibit%20superior%20properties%20in%0Aterms%20of%20reconstruction%20and%20denoising%20quality%20as%20well%20as%20robustness%20when%0Aapplied%20in%20deep%20recognition%20pipelines.%20Furthermore%2C%20we%20apply%20WARP-LCA%20to%20image%0Adenoising%20tasks%2C%20showcasing%20its%20robustness%20and%20practical%20effectiveness.%20Our%0Afindings%20confirm%20that%20the%20naive%20use%20of%20LCA%20with%20hard-thresholding%20results%20in%0Asuboptimal%20minima%2C%20whereas%20initializing%20LCA%20with%20a%20predictive%20guess%20results%20in%0Abetter%20outcomes.%20This%20research%20advances%20the%20field%20of%20biologically%20inspired%20deep%0Alearning%20by%20providing%20a%20novel%20approach%20to%20convolutional%20sparse%20coding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWARP-LCA%253A%2520Efficient%2520Convolutional%2520Sparse%2520Coding%2520with%2520Locally%2520Competitive%250A%2520%2520Algorithm%26entry.906535625%3DGeoffrey%2520Kasenbacher%2520and%2520Felix%2520Ehret%2520and%2520Gerrit%2520Ecke%2520and%2520Sebastian%2520Otte%26entry.1292438233%3D%2520%2520The%2520locally%2520competitive%2520algorithm%2520%2528LCA%2529%2520can%2520solve%2520sparse%2520coding%2520problems%250Aacross%2520a%2520wide%2520range%2520of%2520use%2520cases.%2520Recently%252C%2520convolution-based%2520LCA%2520approaches%250Ahave%2520been%2520shown%2520to%2520be%2520highly%2520effective%2520for%2520enhancing%2520robustness%2520for%2520image%250Arecognition%2520tasks%2520in%2520vision%2520pipelines.%2520To%2520additionally%2520maximize%250Arepresentational%2520sparsity%252C%2520LCA%2520with%2520hard-thresholding%2520can%2520be%2520applied.%2520While%250Athis%2520combination%2520often%2520yields%2520very%2520good%2520solutions%2520satisfying%2520an%2520%2524%255Cell_0%2524%250Asparsity%2520criterion%252C%2520it%2520comes%2520with%2520significant%2520drawbacks%2520for%2520practical%250Aapplication%253A%2520%2528i%2529%2520LCA%2520is%2520very%2520inefficient%252C%2520typically%2520requiring%2520hundreds%2520of%250Aoptimization%2520cycles%2520for%2520convergence%253B%2520%2528ii%2529%2520the%2520use%2520of%2520hard-thresholding%2520results%250Ain%2520a%2520non-convex%2520loss%2520function%252C%2520which%2520might%2520lead%2520to%2520suboptimal%2520minima.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520the%2520Locally%2520Competitive%2520Algorithm%2520with%2520State%250AWarm-up%2520via%2520Predictive%2520Priming%2520%2528WARP-LCA%2529%252C%2520which%2520leverages%2520a%2520predictor%2520network%250Ato%2520provide%2520a%2520suitable%2520initial%2520guess%2520of%2520the%2520LCA%2520state%2520based%2520on%2520the%2520current%250Ainput.%2520Our%2520approach%2520significantly%2520improves%2520both%2520convergence%2520speed%2520and%2520the%250Aquality%2520of%2520solutions%252C%2520while%2520maintaining%2520and%2520even%2520enhancing%2520the%2520overall%250Astrengths%2520of%2520LCA.%2520We%2520demonstrate%2520that%2520WARP-LCA%2520converges%2520faster%2520by%2520orders%2520of%250Amagnitude%2520and%2520reaches%2520better%2520minima%2520compared%2520to%2520conventional%2520LCA.%2520Moreover%252C%2520the%250Alearned%2520representations%2520are%2520more%2520sparse%2520and%2520exhibit%2520superior%2520properties%2520in%250Aterms%2520of%2520reconstruction%2520and%2520denoising%2520quality%2520as%2520well%2520as%2520robustness%2520when%250Aapplied%2520in%2520deep%2520recognition%2520pipelines.%2520Furthermore%252C%2520we%2520apply%2520WARP-LCA%2520to%2520image%250Adenoising%2520tasks%252C%2520showcasing%2520its%2520robustness%2520and%2520practical%2520effectiveness.%2520Our%250Afindings%2520confirm%2520that%2520the%2520naive%2520use%2520of%2520LCA%2520with%2520hard-thresholding%2520results%2520in%250Asuboptimal%2520minima%252C%2520whereas%2520initializing%2520LCA%2520with%2520a%2520predictive%2520guess%2520results%2520in%250Abetter%2520outcomes.%2520This%2520research%2520advances%2520the%2520field%2520of%2520biologically%2520inspired%2520deep%250Alearning%2520by%2520providing%2520a%2520novel%2520approach%2520to%2520convolutional%2520sparse%2520coding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WARP-LCA%3A%20Efficient%20Convolutional%20Sparse%20Coding%20with%20Locally%20Competitive%0A%20%20Algorithm&entry.906535625=Geoffrey%20Kasenbacher%20and%20Felix%20Ehret%20and%20Gerrit%20Ecke%20and%20Sebastian%20Otte&entry.1292438233=%20%20The%20locally%20competitive%20algorithm%20%28LCA%29%20can%20solve%20sparse%20coding%20problems%0Aacross%20a%20wide%20range%20of%20use%20cases.%20Recently%2C%20convolution-based%20LCA%20approaches%0Ahave%20been%20shown%20to%20be%20highly%20effective%20for%20enhancing%20robustness%20for%20image%0Arecognition%20tasks%20in%20vision%20pipelines.%20To%20additionally%20maximize%0Arepresentational%20sparsity%2C%20LCA%20with%20hard-thresholding%20can%20be%20applied.%20While%0Athis%20combination%20often%20yields%20very%20good%20solutions%20satisfying%20an%20%24%5Cell_0%24%0Asparsity%20criterion%2C%20it%20comes%20with%20significant%20drawbacks%20for%20practical%0Aapplication%3A%20%28i%29%20LCA%20is%20very%20inefficient%2C%20typically%20requiring%20hundreds%20of%0Aoptimization%20cycles%20for%20convergence%3B%20%28ii%29%20the%20use%20of%20hard-thresholding%20results%0Ain%20a%20non-convex%20loss%20function%2C%20which%20might%20lead%20to%20suboptimal%20minima.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Locally%20Competitive%20Algorithm%20with%20State%0AWarm-up%20via%20Predictive%20Priming%20%28WARP-LCA%29%2C%20which%20leverages%20a%20predictor%20network%0Ato%20provide%20a%20suitable%20initial%20guess%20of%20the%20LCA%20state%20based%20on%20the%20current%0Ainput.%20Our%20approach%20significantly%20improves%20both%20convergence%20speed%20and%20the%0Aquality%20of%20solutions%2C%20while%20maintaining%20and%20even%20enhancing%20the%20overall%0Astrengths%20of%20LCA.%20We%20demonstrate%20that%20WARP-LCA%20converges%20faster%20by%20orders%20of%0Amagnitude%20and%20reaches%20better%20minima%20compared%20to%20conventional%20LCA.%20Moreover%2C%20the%0Alearned%20representations%20are%20more%20sparse%20and%20exhibit%20superior%20properties%20in%0Aterms%20of%20reconstruction%20and%20denoising%20quality%20as%20well%20as%20robustness%20when%0Aapplied%20in%20deep%20recognition%20pipelines.%20Furthermore%2C%20we%20apply%20WARP-LCA%20to%20image%0Adenoising%20tasks%2C%20showcasing%20its%20robustness%20and%20practical%20effectiveness.%20Our%0Afindings%20confirm%20that%20the%20naive%20use%20of%20LCA%20with%20hard-thresholding%20results%20in%0Asuboptimal%20minima%2C%20whereas%20initializing%20LCA%20with%20a%20predictive%20guess%20results%20in%0Abetter%20outcomes.%20This%20research%20advances%20the%20field%20of%20biologically%20inspired%20deep%0Alearning%20by%20providing%20a%20novel%20approach%20to%20convolutional%20sparse%20coding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18794v2&entry.124074799=Read"},
{"title": "On Advancements of the Forward-Forward Algorithm", "author": "Mauricio Ortiz Torres and Markus Lange and Arne P. Raulf", "abstract": "  The Forward-Forward algorithm has evolved in machine learning research,\ntackling more complex tasks that mimic real-life applications. In the last\nyears, it has been improved by several techniques to perform better than its\noriginal version, handling a challenging dataset like CIFAR10 without losing\nits flexibility and low memory usage. We have shown in our results that\nimprovements are achieved through a combination of convolutional channel\ngrouping, learning rate schedules, and independent block structures during\ntraining that lead to a 20\\% decrease in test error percentage. Additionally,\nto approach further implementations on low-capacity hardware projects we have\npresented a series of lighter models that achieve low test error percentages\nwithin (21$\\pm$6)\\% and number of trainable parameters between 164,706 and\n754,386. This serving also as a basis for our future study on complete\nverification and validation of these kinds of neural networks.\n", "link": "http://arxiv.org/abs/2504.21662v1", "date": "2025-04-30", "relevancy": 2.082, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.535}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5134}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Advancements%20of%20the%20Forward-Forward%20Algorithm&body=Title%3A%20On%20Advancements%20of%20the%20Forward-Forward%20Algorithm%0AAuthor%3A%20Mauricio%20Ortiz%20Torres%20and%20Markus%20Lange%20and%20Arne%20P.%20Raulf%0AAbstract%3A%20%20%20The%20Forward-Forward%20algorithm%20has%20evolved%20in%20machine%20learning%20research%2C%0Atackling%20more%20complex%20tasks%20that%20mimic%20real-life%20applications.%20In%20the%20last%0Ayears%2C%20it%20has%20been%20improved%20by%20several%20techniques%20to%20perform%20better%20than%20its%0Aoriginal%20version%2C%20handling%20a%20challenging%20dataset%20like%20CIFAR10%20without%20losing%0Aits%20flexibility%20and%20low%20memory%20usage.%20We%20have%20shown%20in%20our%20results%20that%0Aimprovements%20are%20achieved%20through%20a%20combination%20of%20convolutional%20channel%0Agrouping%2C%20learning%20rate%20schedules%2C%20and%20independent%20block%20structures%20during%0Atraining%20that%20lead%20to%20a%2020%5C%25%20decrease%20in%20test%20error%20percentage.%20Additionally%2C%0Ato%20approach%20further%20implementations%20on%20low-capacity%20hardware%20projects%20we%20have%0Apresented%20a%20series%20of%20lighter%20models%20that%20achieve%20low%20test%20error%20percentages%0Awithin%20%2821%24%5Cpm%246%29%5C%25%20and%20number%20of%20trainable%20parameters%20between%20164%2C706%20and%0A754%2C386.%20This%20serving%20also%20as%20a%20basis%20for%20our%20future%20study%20on%20complete%0Averification%20and%20validation%20of%20these%20kinds%20of%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Advancements%2520of%2520the%2520Forward-Forward%2520Algorithm%26entry.906535625%3DMauricio%2520Ortiz%2520Torres%2520and%2520Markus%2520Lange%2520and%2520Arne%2520P.%2520Raulf%26entry.1292438233%3D%2520%2520The%2520Forward-Forward%2520algorithm%2520has%2520evolved%2520in%2520machine%2520learning%2520research%252C%250Atackling%2520more%2520complex%2520tasks%2520that%2520mimic%2520real-life%2520applications.%2520In%2520the%2520last%250Ayears%252C%2520it%2520has%2520been%2520improved%2520by%2520several%2520techniques%2520to%2520perform%2520better%2520than%2520its%250Aoriginal%2520version%252C%2520handling%2520a%2520challenging%2520dataset%2520like%2520CIFAR10%2520without%2520losing%250Aits%2520flexibility%2520and%2520low%2520memory%2520usage.%2520We%2520have%2520shown%2520in%2520our%2520results%2520that%250Aimprovements%2520are%2520achieved%2520through%2520a%2520combination%2520of%2520convolutional%2520channel%250Agrouping%252C%2520learning%2520rate%2520schedules%252C%2520and%2520independent%2520block%2520structures%2520during%250Atraining%2520that%2520lead%2520to%2520a%252020%255C%2525%2520decrease%2520in%2520test%2520error%2520percentage.%2520Additionally%252C%250Ato%2520approach%2520further%2520implementations%2520on%2520low-capacity%2520hardware%2520projects%2520we%2520have%250Apresented%2520a%2520series%2520of%2520lighter%2520models%2520that%2520achieve%2520low%2520test%2520error%2520percentages%250Awithin%2520%252821%2524%255Cpm%25246%2529%255C%2525%2520and%2520number%2520of%2520trainable%2520parameters%2520between%2520164%252C706%2520and%250A754%252C386.%2520This%2520serving%2520also%2520as%2520a%2520basis%2520for%2520our%2520future%2520study%2520on%2520complete%250Averification%2520and%2520validation%2520of%2520these%2520kinds%2520of%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Advancements%20of%20the%20Forward-Forward%20Algorithm&entry.906535625=Mauricio%20Ortiz%20Torres%20and%20Markus%20Lange%20and%20Arne%20P.%20Raulf&entry.1292438233=%20%20The%20Forward-Forward%20algorithm%20has%20evolved%20in%20machine%20learning%20research%2C%0Atackling%20more%20complex%20tasks%20that%20mimic%20real-life%20applications.%20In%20the%20last%0Ayears%2C%20it%20has%20been%20improved%20by%20several%20techniques%20to%20perform%20better%20than%20its%0Aoriginal%20version%2C%20handling%20a%20challenging%20dataset%20like%20CIFAR10%20without%20losing%0Aits%20flexibility%20and%20low%20memory%20usage.%20We%20have%20shown%20in%20our%20results%20that%0Aimprovements%20are%20achieved%20through%20a%20combination%20of%20convolutional%20channel%0Agrouping%2C%20learning%20rate%20schedules%2C%20and%20independent%20block%20structures%20during%0Atraining%20that%20lead%20to%20a%2020%5C%25%20decrease%20in%20test%20error%20percentage.%20Additionally%2C%0Ato%20approach%20further%20implementations%20on%20low-capacity%20hardware%20projects%20we%20have%0Apresented%20a%20series%20of%20lighter%20models%20that%20achieve%20low%20test%20error%20percentages%0Awithin%20%2821%24%5Cpm%246%29%5C%25%20and%20number%20of%20trainable%20parameters%20between%20164%2C706%20and%0A754%2C386.%20This%20serving%20also%20as%20a%20basis%20for%20our%20future%20study%20on%20complete%0Averification%20and%20validation%20of%20these%20kinds%20of%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21662v1&entry.124074799=Read"},
{"title": "Cascade Detector Analysis and Application to Biomedical Microscopy", "author": "Thomas L. Athey and Shashata Sawmya and Nir Shavit", "abstract": "  As both computer vision models and biomedical datasets grow in size, there is\nan increasing need for efficient inference algorithms. We utilize cascade\ndetectors to efficiently identify sparse objects in multiresolution images.\nGiven an object's prevalence and a set of detectors at different resolutions\nwith known accuracies, we derive the accuracy, and expected number of\nclassifier calls by a cascade detector. These results generalize across number\nof dimensions and number of cascade levels. Finally, we compare one- and\ntwo-level detectors in fluorescent cell detection, organelle segmentation, and\ntissue segmentation across various microscopy modalities. We show that the\nmulti-level detector achieves comparable performance in 30-75% less time. Our\nwork is compatible with a variety of computer vision models and data domains.\n", "link": "http://arxiv.org/abs/2504.21598v1", "date": "2025-04-30", "relevancy": 2.0719, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5181}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5181}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascade%20Detector%20Analysis%20and%20Application%20to%20Biomedical%20Microscopy&body=Title%3A%20Cascade%20Detector%20Analysis%20and%20Application%20to%20Biomedical%20Microscopy%0AAuthor%3A%20Thomas%20L.%20Athey%20and%20Shashata%20Sawmya%20and%20Nir%20Shavit%0AAbstract%3A%20%20%20As%20both%20computer%20vision%20models%20and%20biomedical%20datasets%20grow%20in%20size%2C%20there%20is%0Aan%20increasing%20need%20for%20efficient%20inference%20algorithms.%20We%20utilize%20cascade%0Adetectors%20to%20efficiently%20identify%20sparse%20objects%20in%20multiresolution%20images.%0AGiven%20an%20object%27s%20prevalence%20and%20a%20set%20of%20detectors%20at%20different%20resolutions%0Awith%20known%20accuracies%2C%20we%20derive%20the%20accuracy%2C%20and%20expected%20number%20of%0Aclassifier%20calls%20by%20a%20cascade%20detector.%20These%20results%20generalize%20across%20number%0Aof%20dimensions%20and%20number%20of%20cascade%20levels.%20Finally%2C%20we%20compare%20one-%20and%0Atwo-level%20detectors%20in%20fluorescent%20cell%20detection%2C%20organelle%20segmentation%2C%20and%0Atissue%20segmentation%20across%20various%20microscopy%20modalities.%20We%20show%20that%20the%0Amulti-level%20detector%20achieves%20comparable%20performance%20in%2030-75%25%20less%20time.%20Our%0Awork%20is%20compatible%20with%20a%20variety%20of%20computer%20vision%20models%20and%20data%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascade%2520Detector%2520Analysis%2520and%2520Application%2520to%2520Biomedical%2520Microscopy%26entry.906535625%3DThomas%2520L.%2520Athey%2520and%2520Shashata%2520Sawmya%2520and%2520Nir%2520Shavit%26entry.1292438233%3D%2520%2520As%2520both%2520computer%2520vision%2520models%2520and%2520biomedical%2520datasets%2520grow%2520in%2520size%252C%2520there%2520is%250Aan%2520increasing%2520need%2520for%2520efficient%2520inference%2520algorithms.%2520We%2520utilize%2520cascade%250Adetectors%2520to%2520efficiently%2520identify%2520sparse%2520objects%2520in%2520multiresolution%2520images.%250AGiven%2520an%2520object%2527s%2520prevalence%2520and%2520a%2520set%2520of%2520detectors%2520at%2520different%2520resolutions%250Awith%2520known%2520accuracies%252C%2520we%2520derive%2520the%2520accuracy%252C%2520and%2520expected%2520number%2520of%250Aclassifier%2520calls%2520by%2520a%2520cascade%2520detector.%2520These%2520results%2520generalize%2520across%2520number%250Aof%2520dimensions%2520and%2520number%2520of%2520cascade%2520levels.%2520Finally%252C%2520we%2520compare%2520one-%2520and%250Atwo-level%2520detectors%2520in%2520fluorescent%2520cell%2520detection%252C%2520organelle%2520segmentation%252C%2520and%250Atissue%2520segmentation%2520across%2520various%2520microscopy%2520modalities.%2520We%2520show%2520that%2520the%250Amulti-level%2520detector%2520achieves%2520comparable%2520performance%2520in%252030-75%2525%2520less%2520time.%2520Our%250Awork%2520is%2520compatible%2520with%2520a%2520variety%2520of%2520computer%2520vision%2520models%2520and%2520data%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascade%20Detector%20Analysis%20and%20Application%20to%20Biomedical%20Microscopy&entry.906535625=Thomas%20L.%20Athey%20and%20Shashata%20Sawmya%20and%20Nir%20Shavit&entry.1292438233=%20%20As%20both%20computer%20vision%20models%20and%20biomedical%20datasets%20grow%20in%20size%2C%20there%20is%0Aan%20increasing%20need%20for%20efficient%20inference%20algorithms.%20We%20utilize%20cascade%0Adetectors%20to%20efficiently%20identify%20sparse%20objects%20in%20multiresolution%20images.%0AGiven%20an%20object%27s%20prevalence%20and%20a%20set%20of%20detectors%20at%20different%20resolutions%0Awith%20known%20accuracies%2C%20we%20derive%20the%20accuracy%2C%20and%20expected%20number%20of%0Aclassifier%20calls%20by%20a%20cascade%20detector.%20These%20results%20generalize%20across%20number%0Aof%20dimensions%20and%20number%20of%20cascade%20levels.%20Finally%2C%20we%20compare%20one-%20and%0Atwo-level%20detectors%20in%20fluorescent%20cell%20detection%2C%20organelle%20segmentation%2C%20and%0Atissue%20segmentation%20across%20various%20microscopy%20modalities.%20We%20show%20that%20the%0Amulti-level%20detector%20achieves%20comparable%20performance%20in%2030-75%25%20less%20time.%20Our%0Awork%20is%20compatible%20with%20a%20variety%20of%20computer%20vision%20models%20and%20data%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21598v1&entry.124074799=Read"},
{"title": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks", "author": "William Sutcliffe and Marta Calvi and Simone Capelli and Jonas Eschle and Juli\u00e1n Garc\u00eda Pardi\u00f1as and Abhijit Mathad and Azusa Uzuki and Nicola Serra", "abstract": "  The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme.\n", "link": "http://arxiv.org/abs/2504.21844v1", "date": "2025-04-30", "relevancy": 2.0711, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5381}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5179}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Multi-Task%20Learning%20for%20Particle%20Collision%20Event%20Reconstruction%0A%20%20with%20Heterogeneous%20Graph%20Neural%20Networks&body=Title%3A%20Scalable%20Multi-Task%20Learning%20for%20Particle%20Collision%20Event%20Reconstruction%0A%20%20with%20Heterogeneous%20Graph%20Neural%20Networks%0AAuthor%3A%20William%20Sutcliffe%20and%20Marta%20Calvi%20and%20Simone%20Capelli%20and%20Jonas%20Eschle%20and%20Juli%C3%A1n%20Garc%C3%ADa%20Pardi%C3%B1as%20and%20Abhijit%20Mathad%20and%20Azusa%20Uzuki%20and%20Nicola%20Serra%0AAbstract%3A%20%20%20The%20growing%20luminosity%20frontier%20at%20the%20Large%20Hadron%20Collider%20is%20challenging%0Athe%20reconstruction%20and%20analysis%20of%20particle%20collision%20events.%20Increased%0Aparticle%20multiplicities%20are%20straining%20latency%20and%20storage%20requirements%20at%20the%0Adata%20acquisition%20stage%2C%20while%20new%20complications%20are%20emerging%2C%20including%20higher%0Abackground%20levels%20and%20more%20frequent%20particle%20vertex%20misassociations.%20This%20in%0Aturn%20necessitates%20the%20development%20of%20more%20holistic%20and%20scalable%20reconstruction%0Amethods%20that%20take%20advantage%20of%20recent%20advances%20in%20machine%20learning.%20We%20propose%0Aa%20novel%20Heterogeneous%20Graph%20Neural%20Network%20%28HGNN%29%20architecture%20featuring%20unique%0Arepresentations%20for%20diverse%20particle%20collision%20relationships%20and%20integrated%0Agraph%20pruning%20layers%20for%20scalability.%20Trained%20with%20a%20multi-task%20paradigm%20in%20an%0Aenvironment%20mimicking%20the%20LHCb%20experiment%2C%20this%20HGNN%20significantly%20improves%0Abeauty%20hadron%20reconstruction%20performance.%20Notably%2C%20it%20concurrently%20performs%0Aparticle%20vertex%20association%20and%20graph%20pruning%20within%20a%20single%20framework.%20We%0Aquantify%20reconstruction%20and%20pruning%20performance%2C%20demonstrate%20enhanced%20inference%0Atime%20scaling%20with%20event%20complexity%2C%20and%20mitigate%20potential%20performance%20loss%0Ausing%20a%20weighted%20message%20passing%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Multi-Task%2520Learning%2520for%2520Particle%2520Collision%2520Event%2520Reconstruction%250A%2520%2520with%2520Heterogeneous%2520Graph%2520Neural%2520Networks%26entry.906535625%3DWilliam%2520Sutcliffe%2520and%2520Marta%2520Calvi%2520and%2520Simone%2520Capelli%2520and%2520Jonas%2520Eschle%2520and%2520Juli%25C3%25A1n%2520Garc%25C3%25ADa%2520Pardi%25C3%25B1as%2520and%2520Abhijit%2520Mathad%2520and%2520Azusa%2520Uzuki%2520and%2520Nicola%2520Serra%26entry.1292438233%3D%2520%2520The%2520growing%2520luminosity%2520frontier%2520at%2520the%2520Large%2520Hadron%2520Collider%2520is%2520challenging%250Athe%2520reconstruction%2520and%2520analysis%2520of%2520particle%2520collision%2520events.%2520Increased%250Aparticle%2520multiplicities%2520are%2520straining%2520latency%2520and%2520storage%2520requirements%2520at%2520the%250Adata%2520acquisition%2520stage%252C%2520while%2520new%2520complications%2520are%2520emerging%252C%2520including%2520higher%250Abackground%2520levels%2520and%2520more%2520frequent%2520particle%2520vertex%2520misassociations.%2520This%2520in%250Aturn%2520necessitates%2520the%2520development%2520of%2520more%2520holistic%2520and%2520scalable%2520reconstruction%250Amethods%2520that%2520take%2520advantage%2520of%2520recent%2520advances%2520in%2520machine%2520learning.%2520We%2520propose%250Aa%2520novel%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520%2528HGNN%2529%2520architecture%2520featuring%2520unique%250Arepresentations%2520for%2520diverse%2520particle%2520collision%2520relationships%2520and%2520integrated%250Agraph%2520pruning%2520layers%2520for%2520scalability.%2520Trained%2520with%2520a%2520multi-task%2520paradigm%2520in%2520an%250Aenvironment%2520mimicking%2520the%2520LHCb%2520experiment%252C%2520this%2520HGNN%2520significantly%2520improves%250Abeauty%2520hadron%2520reconstruction%2520performance.%2520Notably%252C%2520it%2520concurrently%2520performs%250Aparticle%2520vertex%2520association%2520and%2520graph%2520pruning%2520within%2520a%2520single%2520framework.%2520We%250Aquantify%2520reconstruction%2520and%2520pruning%2520performance%252C%2520demonstrate%2520enhanced%2520inference%250Atime%2520scaling%2520with%2520event%2520complexity%252C%2520and%2520mitigate%2520potential%2520performance%2520loss%250Ausing%2520a%2520weighted%2520message%2520passing%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Multi-Task%20Learning%20for%20Particle%20Collision%20Event%20Reconstruction%0A%20%20with%20Heterogeneous%20Graph%20Neural%20Networks&entry.906535625=William%20Sutcliffe%20and%20Marta%20Calvi%20and%20Simone%20Capelli%20and%20Jonas%20Eschle%20and%20Juli%C3%A1n%20Garc%C3%ADa%20Pardi%C3%B1as%20and%20Abhijit%20Mathad%20and%20Azusa%20Uzuki%20and%20Nicola%20Serra&entry.1292438233=%20%20The%20growing%20luminosity%20frontier%20at%20the%20Large%20Hadron%20Collider%20is%20challenging%0Athe%20reconstruction%20and%20analysis%20of%20particle%20collision%20events.%20Increased%0Aparticle%20multiplicities%20are%20straining%20latency%20and%20storage%20requirements%20at%20the%0Adata%20acquisition%20stage%2C%20while%20new%20complications%20are%20emerging%2C%20including%20higher%0Abackground%20levels%20and%20more%20frequent%20particle%20vertex%20misassociations.%20This%20in%0Aturn%20necessitates%20the%20development%20of%20more%20holistic%20and%20scalable%20reconstruction%0Amethods%20that%20take%20advantage%20of%20recent%20advances%20in%20machine%20learning.%20We%20propose%0Aa%20novel%20Heterogeneous%20Graph%20Neural%20Network%20%28HGNN%29%20architecture%20featuring%20unique%0Arepresentations%20for%20diverse%20particle%20collision%20relationships%20and%20integrated%0Agraph%20pruning%20layers%20for%20scalability.%20Trained%20with%20a%20multi-task%20paradigm%20in%20an%0Aenvironment%20mimicking%20the%20LHCb%20experiment%2C%20this%20HGNN%20significantly%20improves%0Abeauty%20hadron%20reconstruction%20performance.%20Notably%2C%20it%20concurrently%20performs%0Aparticle%20vertex%20association%20and%20graph%20pruning%20within%20a%20single%20framework.%20We%0Aquantify%20reconstruction%20and%20pruning%20performance%2C%20demonstrate%20enhanced%20inference%0Atime%20scaling%20with%20event%20complexity%2C%20and%20mitigate%20potential%20performance%20loss%0Ausing%20a%20weighted%20message%20passing%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21844v1&entry.124074799=Read"},
{"title": "Low-rank computation of the posterior mean in Multi-Output Gaussian\n  Processes", "author": "Sebastian Esche and Martin Stoll", "abstract": "  Gaussian processes (GP) are a versatile tool in machine learning and\ncomputational science. We here consider the case of multi-output Gaussian\nprocesses (MOGP) and present low-rank approaches for efficiently computing the\nposterior mean of a MOGP. Starting from low-rank spatio-temporal data we\nconsider a structured covariance function, assuming separability across space\nand time. This separability, in turn, gives a decomposition of the covariance\nmatrix into a Kronecker product of individual covariance matrices.\nIncorporating the typical noise term to the model then requires the solution of\na large-scale Stein equation for computing the posterior mean. For this, we\npropose efficient low-rank methods based on a combination of a LRPCG method\nwith the Sylvester equation solver KPIK adjusted for solving Stein equations.\nWe test the developed method on real world street network graphs by using graph\nfilters as covariance matrices. Moreover, we propose a degree-weighted average\ncovariance matrix, which can be employed under specific assumptions to achieve\nmore efficient convergence.\n", "link": "http://arxiv.org/abs/2504.21527v1", "date": "2025-04-30", "relevancy": 2.0704, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5229}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-rank%20computation%20of%20the%20posterior%20mean%20in%20Multi-Output%20Gaussian%0A%20%20Processes&body=Title%3A%20Low-rank%20computation%20of%20the%20posterior%20mean%20in%20Multi-Output%20Gaussian%0A%20%20Processes%0AAuthor%3A%20Sebastian%20Esche%20and%20Martin%20Stoll%0AAbstract%3A%20%20%20Gaussian%20processes%20%28GP%29%20are%20a%20versatile%20tool%20in%20machine%20learning%20and%0Acomputational%20science.%20We%20here%20consider%20the%20case%20of%20multi-output%20Gaussian%0Aprocesses%20%28MOGP%29%20and%20present%20low-rank%20approaches%20for%20efficiently%20computing%20the%0Aposterior%20mean%20of%20a%20MOGP.%20Starting%20from%20low-rank%20spatio-temporal%20data%20we%0Aconsider%20a%20structured%20covariance%20function%2C%20assuming%20separability%20across%20space%0Aand%20time.%20This%20separability%2C%20in%20turn%2C%20gives%20a%20decomposition%20of%20the%20covariance%0Amatrix%20into%20a%20Kronecker%20product%20of%20individual%20covariance%20matrices.%0AIncorporating%20the%20typical%20noise%20term%20to%20the%20model%20then%20requires%20the%20solution%20of%0Aa%20large-scale%20Stein%20equation%20for%20computing%20the%20posterior%20mean.%20For%20this%2C%20we%0Apropose%20efficient%20low-rank%20methods%20based%20on%20a%20combination%20of%20a%20LRPCG%20method%0Awith%20the%20Sylvester%20equation%20solver%20KPIK%20adjusted%20for%20solving%20Stein%20equations.%0AWe%20test%20the%20developed%20method%20on%20real%20world%20street%20network%20graphs%20by%20using%20graph%0Afilters%20as%20covariance%20matrices.%20Moreover%2C%20we%20propose%20a%20degree-weighted%20average%0Acovariance%20matrix%2C%20which%20can%20be%20employed%20under%20specific%20assumptions%20to%20achieve%0Amore%20efficient%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-rank%2520computation%2520of%2520the%2520posterior%2520mean%2520in%2520Multi-Output%2520Gaussian%250A%2520%2520Processes%26entry.906535625%3DSebastian%2520Esche%2520and%2520Martin%2520Stoll%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520%2528GP%2529%2520are%2520a%2520versatile%2520tool%2520in%2520machine%2520learning%2520and%250Acomputational%2520science.%2520We%2520here%2520consider%2520the%2520case%2520of%2520multi-output%2520Gaussian%250Aprocesses%2520%2528MOGP%2529%2520and%2520present%2520low-rank%2520approaches%2520for%2520efficiently%2520computing%2520the%250Aposterior%2520mean%2520of%2520a%2520MOGP.%2520Starting%2520from%2520low-rank%2520spatio-temporal%2520data%2520we%250Aconsider%2520a%2520structured%2520covariance%2520function%252C%2520assuming%2520separability%2520across%2520space%250Aand%2520time.%2520This%2520separability%252C%2520in%2520turn%252C%2520gives%2520a%2520decomposition%2520of%2520the%2520covariance%250Amatrix%2520into%2520a%2520Kronecker%2520product%2520of%2520individual%2520covariance%2520matrices.%250AIncorporating%2520the%2520typical%2520noise%2520term%2520to%2520the%2520model%2520then%2520requires%2520the%2520solution%2520of%250Aa%2520large-scale%2520Stein%2520equation%2520for%2520computing%2520the%2520posterior%2520mean.%2520For%2520this%252C%2520we%250Apropose%2520efficient%2520low-rank%2520methods%2520based%2520on%2520a%2520combination%2520of%2520a%2520LRPCG%2520method%250Awith%2520the%2520Sylvester%2520equation%2520solver%2520KPIK%2520adjusted%2520for%2520solving%2520Stein%2520equations.%250AWe%2520test%2520the%2520developed%2520method%2520on%2520real%2520world%2520street%2520network%2520graphs%2520by%2520using%2520graph%250Afilters%2520as%2520covariance%2520matrices.%2520Moreover%252C%2520we%2520propose%2520a%2520degree-weighted%2520average%250Acovariance%2520matrix%252C%2520which%2520can%2520be%2520employed%2520under%2520specific%2520assumptions%2520to%2520achieve%250Amore%2520efficient%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-rank%20computation%20of%20the%20posterior%20mean%20in%20Multi-Output%20Gaussian%0A%20%20Processes&entry.906535625=Sebastian%20Esche%20and%20Martin%20Stoll&entry.1292438233=%20%20Gaussian%20processes%20%28GP%29%20are%20a%20versatile%20tool%20in%20machine%20learning%20and%0Acomputational%20science.%20We%20here%20consider%20the%20case%20of%20multi-output%20Gaussian%0Aprocesses%20%28MOGP%29%20and%20present%20low-rank%20approaches%20for%20efficiently%20computing%20the%0Aposterior%20mean%20of%20a%20MOGP.%20Starting%20from%20low-rank%20spatio-temporal%20data%20we%0Aconsider%20a%20structured%20covariance%20function%2C%20assuming%20separability%20across%20space%0Aand%20time.%20This%20separability%2C%20in%20turn%2C%20gives%20a%20decomposition%20of%20the%20covariance%0Amatrix%20into%20a%20Kronecker%20product%20of%20individual%20covariance%20matrices.%0AIncorporating%20the%20typical%20noise%20term%20to%20the%20model%20then%20requires%20the%20solution%20of%0Aa%20large-scale%20Stein%20equation%20for%20computing%20the%20posterior%20mean.%20For%20this%2C%20we%0Apropose%20efficient%20low-rank%20methods%20based%20on%20a%20combination%20of%20a%20LRPCG%20method%0Awith%20the%20Sylvester%20equation%20solver%20KPIK%20adjusted%20for%20solving%20Stein%20equations.%0AWe%20test%20the%20developed%20method%20on%20real%20world%20street%20network%20graphs%20by%20using%20graph%0Afilters%20as%20covariance%20matrices.%20Moreover%2C%20we%20propose%20a%20degree-weighted%20average%0Acovariance%20matrix%2C%20which%20can%20be%20employed%20under%20specific%20assumptions%20to%20achieve%0Amore%20efficient%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21527v1&entry.124074799=Read"},
{"title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense", "author": "Ting Qiao and Yingjia Wang and Xing Liu and Sixing Wu and Jianbing Li and Yiming Li", "abstract": "  Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB.\n", "link": "http://arxiv.org/abs/2504.21730v1", "date": "2025-04-30", "relevancy": 2.0607, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5253}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.523}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cert-SSB%3A%20Toward%20Certified%20Sample-Specific%20Backdoor%20Defense&body=Title%3A%20Cert-SSB%3A%20Toward%20Certified%20Sample-Specific%20Backdoor%20Defense%0AAuthor%3A%20Ting%20Qiao%20and%20Yingjia%20Wang%20and%20Xing%20Liu%20and%20Sixing%20Wu%20and%20Jianbing%20Li%20and%20Yiming%20Li%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20backdoor%20attacks%2C%20where%20an%0Aattacker%20manipulates%20a%20small%20portion%20of%20the%20training%20data%20to%20implant%20hidden%0Abackdoors%20into%20the%20model.%20The%20compromised%20model%20behaves%20normally%20on%20clean%0Asamples%20but%20misclassifies%20backdoored%20samples%20into%20the%20attacker-specified%20target%0Aclass%2C%20posing%20a%20significant%20threat%20to%20real-world%20DNN%20applications.%20Currently%2C%0Aseveral%20empirical%20defense%20methods%20have%20been%20proposed%20to%20mitigate%20backdoor%0Aattacks%2C%20but%20they%20are%20often%20bypassed%20by%20more%20advanced%20backdoor%20techniques.%20In%0Acontrast%2C%20certified%20defenses%20based%20on%20randomized%20smoothing%20have%20shown%20promise%0Aby%20adding%20random%20noise%20to%20training%20and%20testing%20samples%20to%20counteract%20backdoor%0Aattacks.%20In%20this%20paper%2C%20we%20reveal%20that%20existing%20randomized%20smoothing%20defenses%0Aimplicitly%20assume%20that%20all%20samples%20are%20equidistant%20from%20the%20decision%20boundary.%0AHowever%2C%20it%20may%20not%20hold%20in%20practice%2C%20leading%20to%20suboptimal%20certification%0Aperformance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20sample-specific%20certified%0Abackdoor%20defense%20method%2C%20termed%20Cert-SSB.%20Cert-SSB%20first%20employs%20stochastic%0Agradient%20ascent%20to%20optimize%20the%20noise%20magnitude%20for%20each%20sample%2C%20ensuring%20a%0Asample-specific%20noise%20level%20that%20is%20then%20applied%20to%20multiple%20poisoned%20training%0Asets%20to%20retrain%20several%20smoothed%20models.%20After%20that%2C%20Cert-SSB%20aggregates%20the%0Apredictions%20of%20multiple%20smoothed%20models%20to%20generate%20the%20final%20robust%0Aprediction.%20In%20particular%2C%20in%20this%20case%2C%20existing%20certification%20methods%20become%0Ainapplicable%20since%20the%20optimized%20noise%20varies%20across%20different%20samples.%20To%0Aconquer%20this%20challenge%2C%20we%20introduce%20a%20storage-update-based%20certification%0Amethod%2C%20which%20dynamically%20adjusts%20each%20sample%27s%20certification%20region%20to%20improve%0Acertification%20performance.%20We%20conduct%20extensive%20experiments%20on%20multiple%0Abenchmark%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20our%20proposed%20method.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/NcepuQiaoTing/Cert-SSB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCert-SSB%253A%2520Toward%2520Certified%2520Sample-Specific%2520Backdoor%2520Defense%26entry.906535625%3DTing%2520Qiao%2520and%2520Yingjia%2520Wang%2520and%2520Xing%2520Liu%2520and%2520Sixing%2520Wu%2520and%2520Jianbing%2520Li%2520and%2520Yiming%2520Li%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520vulnerable%2520to%2520backdoor%2520attacks%252C%2520where%2520an%250Aattacker%2520manipulates%2520a%2520small%2520portion%2520of%2520the%2520training%2520data%2520to%2520implant%2520hidden%250Abackdoors%2520into%2520the%2520model.%2520The%2520compromised%2520model%2520behaves%2520normally%2520on%2520clean%250Asamples%2520but%2520misclassifies%2520backdoored%2520samples%2520into%2520the%2520attacker-specified%2520target%250Aclass%252C%2520posing%2520a%2520significant%2520threat%2520to%2520real-world%2520DNN%2520applications.%2520Currently%252C%250Aseveral%2520empirical%2520defense%2520methods%2520have%2520been%2520proposed%2520to%2520mitigate%2520backdoor%250Aattacks%252C%2520but%2520they%2520are%2520often%2520bypassed%2520by%2520more%2520advanced%2520backdoor%2520techniques.%2520In%250Acontrast%252C%2520certified%2520defenses%2520based%2520on%2520randomized%2520smoothing%2520have%2520shown%2520promise%250Aby%2520adding%2520random%2520noise%2520to%2520training%2520and%2520testing%2520samples%2520to%2520counteract%2520backdoor%250Aattacks.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520that%2520existing%2520randomized%2520smoothing%2520defenses%250Aimplicitly%2520assume%2520that%2520all%2520samples%2520are%2520equidistant%2520from%2520the%2520decision%2520boundary.%250AHowever%252C%2520it%2520may%2520not%2520hold%2520in%2520practice%252C%2520leading%2520to%2520suboptimal%2520certification%250Aperformance.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520sample-specific%2520certified%250Abackdoor%2520defense%2520method%252C%2520termed%2520Cert-SSB.%2520Cert-SSB%2520first%2520employs%2520stochastic%250Agradient%2520ascent%2520to%2520optimize%2520the%2520noise%2520magnitude%2520for%2520each%2520sample%252C%2520ensuring%2520a%250Asample-specific%2520noise%2520level%2520that%2520is%2520then%2520applied%2520to%2520multiple%2520poisoned%2520training%250Asets%2520to%2520retrain%2520several%2520smoothed%2520models.%2520After%2520that%252C%2520Cert-SSB%2520aggregates%2520the%250Apredictions%2520of%2520multiple%2520smoothed%2520models%2520to%2520generate%2520the%2520final%2520robust%250Aprediction.%2520In%2520particular%252C%2520in%2520this%2520case%252C%2520existing%2520certification%2520methods%2520become%250Ainapplicable%2520since%2520the%2520optimized%2520noise%2520varies%2520across%2520different%2520samples.%2520To%250Aconquer%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520storage-update-based%2520certification%250Amethod%252C%2520which%2520dynamically%2520adjusts%2520each%2520sample%2527s%2520certification%2520region%2520to%2520improve%250Acertification%2520performance.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520multiple%250Abenchmark%2520datasets%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/NcepuQiaoTing/Cert-SSB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cert-SSB%3A%20Toward%20Certified%20Sample-Specific%20Backdoor%20Defense&entry.906535625=Ting%20Qiao%20and%20Yingjia%20Wang%20and%20Xing%20Liu%20and%20Sixing%20Wu%20and%20Jianbing%20Li%20and%20Yiming%20Li&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20backdoor%20attacks%2C%20where%20an%0Aattacker%20manipulates%20a%20small%20portion%20of%20the%20training%20data%20to%20implant%20hidden%0Abackdoors%20into%20the%20model.%20The%20compromised%20model%20behaves%20normally%20on%20clean%0Asamples%20but%20misclassifies%20backdoored%20samples%20into%20the%20attacker-specified%20target%0Aclass%2C%20posing%20a%20significant%20threat%20to%20real-world%20DNN%20applications.%20Currently%2C%0Aseveral%20empirical%20defense%20methods%20have%20been%20proposed%20to%20mitigate%20backdoor%0Aattacks%2C%20but%20they%20are%20often%20bypassed%20by%20more%20advanced%20backdoor%20techniques.%20In%0Acontrast%2C%20certified%20defenses%20based%20on%20randomized%20smoothing%20have%20shown%20promise%0Aby%20adding%20random%20noise%20to%20training%20and%20testing%20samples%20to%20counteract%20backdoor%0Aattacks.%20In%20this%20paper%2C%20we%20reveal%20that%20existing%20randomized%20smoothing%20defenses%0Aimplicitly%20assume%20that%20all%20samples%20are%20equidistant%20from%20the%20decision%20boundary.%0AHowever%2C%20it%20may%20not%20hold%20in%20practice%2C%20leading%20to%20suboptimal%20certification%0Aperformance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20sample-specific%20certified%0Abackdoor%20defense%20method%2C%20termed%20Cert-SSB.%20Cert-SSB%20first%20employs%20stochastic%0Agradient%20ascent%20to%20optimize%20the%20noise%20magnitude%20for%20each%20sample%2C%20ensuring%20a%0Asample-specific%20noise%20level%20that%20is%20then%20applied%20to%20multiple%20poisoned%20training%0Asets%20to%20retrain%20several%20smoothed%20models.%20After%20that%2C%20Cert-SSB%20aggregates%20the%0Apredictions%20of%20multiple%20smoothed%20models%20to%20generate%20the%20final%20robust%0Aprediction.%20In%20particular%2C%20in%20this%20case%2C%20existing%20certification%20methods%20become%0Ainapplicable%20since%20the%20optimized%20noise%20varies%20across%20different%20samples.%20To%0Aconquer%20this%20challenge%2C%20we%20introduce%20a%20storage-update-based%20certification%0Amethod%2C%20which%20dynamically%20adjusts%20each%20sample%27s%20certification%20region%20to%20improve%0Acertification%20performance.%20We%20conduct%20extensive%20experiments%20on%20multiple%0Abenchmark%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20our%20proposed%20method.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/NcepuQiaoTing/Cert-SSB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21730v1&entry.124074799=Read"},
{"title": "Explorations of the Softmax Space: Knowing When the Neural Network\n  Doesn't Know", "author": "Daniel Sikar and Artur d'Avila Garcez and Tillman Weyde", "abstract": "  Ensuring the reliability of automated decision-making based on neural\nnetworks will be crucial as Artificial Intelligence systems are deployed more\nwidely in critical situations. This paper proposes a new approach for measuring\nconfidence in the predictions of any neural network that relies on the\npredictions of a softmax layer. We identify that a high-accuracy trained\nnetwork may have certain outputs for which there should be low confidence. In\nsuch cases, decisions should be deferred and it is more appropriate for the\nnetwork to provide a \\textit{not known} answer to a corresponding\nclassification task. Our approach clusters the vectors in the softmax layer to\nmeasure distances between cluster centroids and network outputs. We show that a\ncluster with centroid calculated simply as the mean softmax output for all\ncorrect predictions can serve as a suitable proxy in the evaluation of\nconfidence. Defining a distance threshold for a class as the smallest distance\nfrom an incorrect prediction to the given class centroid offers a simple\napproach to adding \\textit{not known} answers to any network classification\nfalling outside of the threshold. We evaluate the approach on the MNIST and\nCIFAR-10 datasets using a Convolutional Neural Network and a Vision\nTransformer, respectively. The results show that our approach is consistent\nacross datasets and network models, and indicate that the proposed distance\nmetric can offer an efficient way of determining when automated predictions are\nacceptable and when they should be deferred to human operators.\n", "link": "http://arxiv.org/abs/2502.00456v2", "date": "2025-04-30", "relevancy": 2.0441, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5264}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5162}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explorations%20of%20the%20Softmax%20Space%3A%20Knowing%20When%20the%20Neural%20Network%0A%20%20Doesn%27t%20Know&body=Title%3A%20Explorations%20of%20the%20Softmax%20Space%3A%20Knowing%20When%20the%20Neural%20Network%0A%20%20Doesn%27t%20Know%0AAuthor%3A%20Daniel%20Sikar%20and%20Artur%20d%27Avila%20Garcez%20and%20Tillman%20Weyde%0AAbstract%3A%20%20%20Ensuring%20the%20reliability%20of%20automated%20decision-making%20based%20on%20neural%0Anetworks%20will%20be%20crucial%20as%20Artificial%20Intelligence%20systems%20are%20deployed%20more%0Awidely%20in%20critical%20situations.%20This%20paper%20proposes%20a%20new%20approach%20for%20measuring%0Aconfidence%20in%20the%20predictions%20of%20any%20neural%20network%20that%20relies%20on%20the%0Apredictions%20of%20a%20softmax%20layer.%20We%20identify%20that%20a%20high-accuracy%20trained%0Anetwork%20may%20have%20certain%20outputs%20for%20which%20there%20should%20be%20low%20confidence.%20In%0Asuch%20cases%2C%20decisions%20should%20be%20deferred%20and%20it%20is%20more%20appropriate%20for%20the%0Anetwork%20to%20provide%20a%20%5Ctextit%7Bnot%20known%7D%20answer%20to%20a%20corresponding%0Aclassification%20task.%20Our%20approach%20clusters%20the%20vectors%20in%20the%20softmax%20layer%20to%0Ameasure%20distances%20between%20cluster%20centroids%20and%20network%20outputs.%20We%20show%20that%20a%0Acluster%20with%20centroid%20calculated%20simply%20as%20the%20mean%20softmax%20output%20for%20all%0Acorrect%20predictions%20can%20serve%20as%20a%20suitable%20proxy%20in%20the%20evaluation%20of%0Aconfidence.%20Defining%20a%20distance%20threshold%20for%20a%20class%20as%20the%20smallest%20distance%0Afrom%20an%20incorrect%20prediction%20to%20the%20given%20class%20centroid%20offers%20a%20simple%0Aapproach%20to%20adding%20%5Ctextit%7Bnot%20known%7D%20answers%20to%20any%20network%20classification%0Afalling%20outside%20of%20the%20threshold.%20We%20evaluate%20the%20approach%20on%20the%20MNIST%20and%0ACIFAR-10%20datasets%20using%20a%20Convolutional%20Neural%20Network%20and%20a%20Vision%0ATransformer%2C%20respectively.%20The%20results%20show%20that%20our%20approach%20is%20consistent%0Aacross%20datasets%20and%20network%20models%2C%20and%20indicate%20that%20the%20proposed%20distance%0Ametric%20can%20offer%20an%20efficient%20way%20of%20determining%20when%20automated%20predictions%20are%0Aacceptable%20and%20when%20they%20should%20be%20deferred%20to%20human%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplorations%2520of%2520the%2520Softmax%2520Space%253A%2520Knowing%2520When%2520the%2520Neural%2520Network%250A%2520%2520Doesn%2527t%2520Know%26entry.906535625%3DDaniel%2520Sikar%2520and%2520Artur%2520d%2527Avila%2520Garcez%2520and%2520Tillman%2520Weyde%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520reliability%2520of%2520automated%2520decision-making%2520based%2520on%2520neural%250Anetworks%2520will%2520be%2520crucial%2520as%2520Artificial%2520Intelligence%2520systems%2520are%2520deployed%2520more%250Awidely%2520in%2520critical%2520situations.%2520This%2520paper%2520proposes%2520a%2520new%2520approach%2520for%2520measuring%250Aconfidence%2520in%2520the%2520predictions%2520of%2520any%2520neural%2520network%2520that%2520relies%2520on%2520the%250Apredictions%2520of%2520a%2520softmax%2520layer.%2520We%2520identify%2520that%2520a%2520high-accuracy%2520trained%250Anetwork%2520may%2520have%2520certain%2520outputs%2520for%2520which%2520there%2520should%2520be%2520low%2520confidence.%2520In%250Asuch%2520cases%252C%2520decisions%2520should%2520be%2520deferred%2520and%2520it%2520is%2520more%2520appropriate%2520for%2520the%250Anetwork%2520to%2520provide%2520a%2520%255Ctextit%257Bnot%2520known%257D%2520answer%2520to%2520a%2520corresponding%250Aclassification%2520task.%2520Our%2520approach%2520clusters%2520the%2520vectors%2520in%2520the%2520softmax%2520layer%2520to%250Ameasure%2520distances%2520between%2520cluster%2520centroids%2520and%2520network%2520outputs.%2520We%2520show%2520that%2520a%250Acluster%2520with%2520centroid%2520calculated%2520simply%2520as%2520the%2520mean%2520softmax%2520output%2520for%2520all%250Acorrect%2520predictions%2520can%2520serve%2520as%2520a%2520suitable%2520proxy%2520in%2520the%2520evaluation%2520of%250Aconfidence.%2520Defining%2520a%2520distance%2520threshold%2520for%2520a%2520class%2520as%2520the%2520smallest%2520distance%250Afrom%2520an%2520incorrect%2520prediction%2520to%2520the%2520given%2520class%2520centroid%2520offers%2520a%2520simple%250Aapproach%2520to%2520adding%2520%255Ctextit%257Bnot%2520known%257D%2520answers%2520to%2520any%2520network%2520classification%250Afalling%2520outside%2520of%2520the%2520threshold.%2520We%2520evaluate%2520the%2520approach%2520on%2520the%2520MNIST%2520and%250ACIFAR-10%2520datasets%2520using%2520a%2520Convolutional%2520Neural%2520Network%2520and%2520a%2520Vision%250ATransformer%252C%2520respectively.%2520The%2520results%2520show%2520that%2520our%2520approach%2520is%2520consistent%250Aacross%2520datasets%2520and%2520network%2520models%252C%2520and%2520indicate%2520that%2520the%2520proposed%2520distance%250Ametric%2520can%2520offer%2520an%2520efficient%2520way%2520of%2520determining%2520when%2520automated%2520predictions%2520are%250Aacceptable%2520and%2520when%2520they%2520should%2520be%2520deferred%2520to%2520human%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explorations%20of%20the%20Softmax%20Space%3A%20Knowing%20When%20the%20Neural%20Network%0A%20%20Doesn%27t%20Know&entry.906535625=Daniel%20Sikar%20and%20Artur%20d%27Avila%20Garcez%20and%20Tillman%20Weyde&entry.1292438233=%20%20Ensuring%20the%20reliability%20of%20automated%20decision-making%20based%20on%20neural%0Anetworks%20will%20be%20crucial%20as%20Artificial%20Intelligence%20systems%20are%20deployed%20more%0Awidely%20in%20critical%20situations.%20This%20paper%20proposes%20a%20new%20approach%20for%20measuring%0Aconfidence%20in%20the%20predictions%20of%20any%20neural%20network%20that%20relies%20on%20the%0Apredictions%20of%20a%20softmax%20layer.%20We%20identify%20that%20a%20high-accuracy%20trained%0Anetwork%20may%20have%20certain%20outputs%20for%20which%20there%20should%20be%20low%20confidence.%20In%0Asuch%20cases%2C%20decisions%20should%20be%20deferred%20and%20it%20is%20more%20appropriate%20for%20the%0Anetwork%20to%20provide%20a%20%5Ctextit%7Bnot%20known%7D%20answer%20to%20a%20corresponding%0Aclassification%20task.%20Our%20approach%20clusters%20the%20vectors%20in%20the%20softmax%20layer%20to%0Ameasure%20distances%20between%20cluster%20centroids%20and%20network%20outputs.%20We%20show%20that%20a%0Acluster%20with%20centroid%20calculated%20simply%20as%20the%20mean%20softmax%20output%20for%20all%0Acorrect%20predictions%20can%20serve%20as%20a%20suitable%20proxy%20in%20the%20evaluation%20of%0Aconfidence.%20Defining%20a%20distance%20threshold%20for%20a%20class%20as%20the%20smallest%20distance%0Afrom%20an%20incorrect%20prediction%20to%20the%20given%20class%20centroid%20offers%20a%20simple%0Aapproach%20to%20adding%20%5Ctextit%7Bnot%20known%7D%20answers%20to%20any%20network%20classification%0Afalling%20outside%20of%20the%20threshold.%20We%20evaluate%20the%20approach%20on%20the%20MNIST%20and%0ACIFAR-10%20datasets%20using%20a%20Convolutional%20Neural%20Network%20and%20a%20Vision%0ATransformer%2C%20respectively.%20The%20results%20show%20that%20our%20approach%20is%20consistent%0Aacross%20datasets%20and%20network%20models%2C%20and%20indicate%20that%20the%20proposed%20distance%0Ametric%20can%20offer%20an%20efficient%20way%20of%20determining%20when%20automated%20predictions%20are%0Aacceptable%20and%20when%20they%20should%20be%20deferred%20to%20human%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00456v2&entry.124074799=Read"},
{"title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic\n  Segmentation of Remote Sensing Imagery", "author": "Qinfeng Zhu and Yunxi Jiang and Lei Fan", "abstract": "  We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.\n", "link": "http://arxiv.org/abs/2504.21491v1", "date": "2025-04-30", "relevancy": 2.0415, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5093}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClassWise-CRF%3A%20Category-Specific%20Fusion%20for%20Enhanced%20Semantic%0A%20%20Segmentation%20of%20Remote%20Sensing%20Imagery&body=Title%3A%20ClassWise-CRF%3A%20Category-Specific%20Fusion%20for%20Enhanced%20Semantic%0A%20%20Segmentation%20of%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Qinfeng%20Zhu%20and%20Yunxi%20Jiang%20and%20Lei%20Fan%0AAbstract%3A%20%20%20We%20propose%20a%20result-level%20category-specific%20fusion%20architecture%20called%0AClassWise-CRF.%20This%20architecture%20employs%20a%20two-stage%20process%3A%20first%2C%20it%20selects%0Aexpert%20networks%20that%20perform%20well%20in%20specific%20categories%20from%20a%20pool%20of%0Acandidate%20networks%20using%20a%20greedy%20algorithm%3B%20second%2C%20it%20integrates%20the%0Asegmentation%20predictions%20of%20these%20selected%20networks%20by%20adaptively%20weighting%0Atheir%20contributions%20based%20on%20their%20segmentation%20performance%20in%20each%20category.%0AInspired%20by%20Conditional%20Random%20Field%20%28CRF%29%2C%20the%20ClassWise-CRF%20architecture%0Atreats%20the%20segmentation%20predictions%20from%20multiple%20networks%20as%20confidence%20vector%0Afields.%20It%20leverages%20segmentation%20metrics%20%28such%20as%20Intersection%20over%20Union%29%0Afrom%20the%20validation%20set%20as%20priors%20and%20employs%20an%20exponential%20weighting%20strategy%0Ato%20fuse%20the%20category-specific%20confidence%20scores%20predicted%20by%20each%20network.%20This%0Afusion%20method%20dynamically%20adjusts%20the%20weights%20of%20each%20network%20for%20different%0Acategories%2C%20achieving%20category-specific%20optimization.%20Building%20on%20this%2C%20the%0Aarchitecture%20further%20optimizes%20the%20fused%20results%20using%20unary%20and%20pairwise%0Apotentials%20in%20CRF%20to%20ensure%20spatial%20consistency%20and%20boundary%20accuracy.%20To%0Avalidate%20the%20effectiveness%20of%20ClassWise-CRF%2C%20we%20conducted%20experiments%20on%20two%0Aremote%20sensing%20datasets%2C%20LoveDA%20and%20Vaihingen%2C%20using%20eight%20classic%20and%20advanced%0Asemantic%20segmentation%20networks.%20The%20results%20show%20that%20the%20ClassWise-CRF%0Aarchitecture%20significantly%20improves%20segmentation%20performance%3A%20on%20the%20LoveDA%0Adataset%2C%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20metric%20increased%20by%201.00%25%20on%0Athe%20validation%20set%20and%20by%200.68%25%20on%20the%20test%20set%3B%20on%20the%20Vaihingen%20dataset%2C%20the%0AmIoU%20improved%20by%200.87%25%20on%20the%20validation%20set%20and%20by%200.91%25%20on%20the%20test%20set.%0AThese%20results%20fully%20demonstrate%20the%20effectiveness%20and%20generality%20of%20the%0AClassWise-CRF%20architecture%20in%20semantic%20segmentation%20of%20remote%20sensing%20images.%0AThe%20full%20code%20is%20available%20at%20https%3A//github.com/zhuqinfeng1999/ClassWise-CRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassWise-CRF%253A%2520Category-Specific%2520Fusion%2520for%2520Enhanced%2520Semantic%250A%2520%2520Segmentation%2520of%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DQinfeng%2520Zhu%2520and%2520Yunxi%2520Jiang%2520and%2520Lei%2520Fan%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520result-level%2520category-specific%2520fusion%2520architecture%2520called%250AClassWise-CRF.%2520This%2520architecture%2520employs%2520a%2520two-stage%2520process%253A%2520first%252C%2520it%2520selects%250Aexpert%2520networks%2520that%2520perform%2520well%2520in%2520specific%2520categories%2520from%2520a%2520pool%2520of%250Acandidate%2520networks%2520using%2520a%2520greedy%2520algorithm%253B%2520second%252C%2520it%2520integrates%2520the%250Asegmentation%2520predictions%2520of%2520these%2520selected%2520networks%2520by%2520adaptively%2520weighting%250Atheir%2520contributions%2520based%2520on%2520their%2520segmentation%2520performance%2520in%2520each%2520category.%250AInspired%2520by%2520Conditional%2520Random%2520Field%2520%2528CRF%2529%252C%2520the%2520ClassWise-CRF%2520architecture%250Atreats%2520the%2520segmentation%2520predictions%2520from%2520multiple%2520networks%2520as%2520confidence%2520vector%250Afields.%2520It%2520leverages%2520segmentation%2520metrics%2520%2528such%2520as%2520Intersection%2520over%2520Union%2529%250Afrom%2520the%2520validation%2520set%2520as%2520priors%2520and%2520employs%2520an%2520exponential%2520weighting%2520strategy%250Ato%2520fuse%2520the%2520category-specific%2520confidence%2520scores%2520predicted%2520by%2520each%2520network.%2520This%250Afusion%2520method%2520dynamically%2520adjusts%2520the%2520weights%2520of%2520each%2520network%2520for%2520different%250Acategories%252C%2520achieving%2520category-specific%2520optimization.%2520Building%2520on%2520this%252C%2520the%250Aarchitecture%2520further%2520optimizes%2520the%2520fused%2520results%2520using%2520unary%2520and%2520pairwise%250Apotentials%2520in%2520CRF%2520to%2520ensure%2520spatial%2520consistency%2520and%2520boundary%2520accuracy.%2520To%250Avalidate%2520the%2520effectiveness%2520of%2520ClassWise-CRF%252C%2520we%2520conducted%2520experiments%2520on%2520two%250Aremote%2520sensing%2520datasets%252C%2520LoveDA%2520and%2520Vaihingen%252C%2520using%2520eight%2520classic%2520and%2520advanced%250Asemantic%2520segmentation%2520networks.%2520The%2520results%2520show%2520that%2520the%2520ClassWise-CRF%250Aarchitecture%2520significantly%2520improves%2520segmentation%2520performance%253A%2520on%2520the%2520LoveDA%250Adataset%252C%2520the%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520metric%2520increased%2520by%25201.00%2525%2520on%250Athe%2520validation%2520set%2520and%2520by%25200.68%2525%2520on%2520the%2520test%2520set%253B%2520on%2520the%2520Vaihingen%2520dataset%252C%2520the%250AmIoU%2520improved%2520by%25200.87%2525%2520on%2520the%2520validation%2520set%2520and%2520by%25200.91%2525%2520on%2520the%2520test%2520set.%250AThese%2520results%2520fully%2520demonstrate%2520the%2520effectiveness%2520and%2520generality%2520of%2520the%250AClassWise-CRF%2520architecture%2520in%2520semantic%2520segmentation%2520of%2520remote%2520sensing%2520images.%250AThe%2520full%2520code%2520is%2520available%2520at%2520https%253A//github.com/zhuqinfeng1999/ClassWise-CRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClassWise-CRF%3A%20Category-Specific%20Fusion%20for%20Enhanced%20Semantic%0A%20%20Segmentation%20of%20Remote%20Sensing%20Imagery&entry.906535625=Qinfeng%20Zhu%20and%20Yunxi%20Jiang%20and%20Lei%20Fan&entry.1292438233=%20%20We%20propose%20a%20result-level%20category-specific%20fusion%20architecture%20called%0AClassWise-CRF.%20This%20architecture%20employs%20a%20two-stage%20process%3A%20first%2C%20it%20selects%0Aexpert%20networks%20that%20perform%20well%20in%20specific%20categories%20from%20a%20pool%20of%0Acandidate%20networks%20using%20a%20greedy%20algorithm%3B%20second%2C%20it%20integrates%20the%0Asegmentation%20predictions%20of%20these%20selected%20networks%20by%20adaptively%20weighting%0Atheir%20contributions%20based%20on%20their%20segmentation%20performance%20in%20each%20category.%0AInspired%20by%20Conditional%20Random%20Field%20%28CRF%29%2C%20the%20ClassWise-CRF%20architecture%0Atreats%20the%20segmentation%20predictions%20from%20multiple%20networks%20as%20confidence%20vector%0Afields.%20It%20leverages%20segmentation%20metrics%20%28such%20as%20Intersection%20over%20Union%29%0Afrom%20the%20validation%20set%20as%20priors%20and%20employs%20an%20exponential%20weighting%20strategy%0Ato%20fuse%20the%20category-specific%20confidence%20scores%20predicted%20by%20each%20network.%20This%0Afusion%20method%20dynamically%20adjusts%20the%20weights%20of%20each%20network%20for%20different%0Acategories%2C%20achieving%20category-specific%20optimization.%20Building%20on%20this%2C%20the%0Aarchitecture%20further%20optimizes%20the%20fused%20results%20using%20unary%20and%20pairwise%0Apotentials%20in%20CRF%20to%20ensure%20spatial%20consistency%20and%20boundary%20accuracy.%20To%0Avalidate%20the%20effectiveness%20of%20ClassWise-CRF%2C%20we%20conducted%20experiments%20on%20two%0Aremote%20sensing%20datasets%2C%20LoveDA%20and%20Vaihingen%2C%20using%20eight%20classic%20and%20advanced%0Asemantic%20segmentation%20networks.%20The%20results%20show%20that%20the%20ClassWise-CRF%0Aarchitecture%20significantly%20improves%20segmentation%20performance%3A%20on%20the%20LoveDA%0Adataset%2C%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20metric%20increased%20by%201.00%25%20on%0Athe%20validation%20set%20and%20by%200.68%25%20on%20the%20test%20set%3B%20on%20the%20Vaihingen%20dataset%2C%20the%0AmIoU%20improved%20by%200.87%25%20on%20the%20validation%20set%20and%20by%200.91%25%20on%20the%20test%20set.%0AThese%20results%20fully%20demonstrate%20the%20effectiveness%20and%20generality%20of%20the%0AClassWise-CRF%20architecture%20in%20semantic%20segmentation%20of%20remote%20sensing%20images.%0AThe%20full%20code%20is%20available%20at%20https%3A//github.com/zhuqinfeng1999/ClassWise-CRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21491v1&entry.124074799=Read"},
{"title": "Whleaper: A 10-DOF Flexible Bipedal Wheeled Robot", "author": "Yinglei Zhu and Sixiao He and Zhenghao Qi and Zhuoyuan Yong and Yihua Qin and Jianyu Chen", "abstract": "  Wheel-legged robots combine the advantages of both wheeled robots and legged\nrobots, offering versatile locomotion capabilities with excellent stability on\nchallenging terrains and high efficiency on flat surfaces. However, existing\nwheel-legged robots typically have limited hip joint mobility compared to\nhumans, while hip joint plays a crucial role in locomotion. In this paper, we\nintroduce Whleaper, a novel 10-degree-of-freedom (DOF) bipedal wheeled robot,\nwith 3 DOFs at the hip of each leg. Its humanoid joint design enables adaptable\nmotion in complex scenarios, ensuring stability and flexibility. This paper\nintroduces the details of Whleaper, with a focus on innovative mechanical\ndesign, control algorithms and system implementation. Firstly, stability stems\nfrom the increased DOFs at the hip, which expand the range of possible postures\nand improve the robot's foot-ground contact. Secondly, the extra DOFs also\naugment its mobility. During walking or sliding, more complex movements can be\nadopted to execute obstacle avoidance tasks. Thirdly, we utilize two control\nalgorithms to implement multimodal motion for walking and sliding. By\ncontrolling specific DOFs of the robot, we conducted a series of simulations\nand practical experiments, demonstrating that a high-DOF hip joint design can\neffectively enhance the stability and flexibility of wheel-legged robots.\nWhleaper shows its capability to perform actions such as squatting, obstacle\navoidance sliding, and rapid turning in real-world scenarios.\n", "link": "http://arxiv.org/abs/2504.21767v1", "date": "2025-04-30", "relevancy": 2.0383, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5431}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whleaper%3A%20A%2010-DOF%20Flexible%20Bipedal%20Wheeled%20Robot&body=Title%3A%20Whleaper%3A%20A%2010-DOF%20Flexible%20Bipedal%20Wheeled%20Robot%0AAuthor%3A%20Yinglei%20Zhu%20and%20Sixiao%20He%20and%20Zhenghao%20Qi%20and%20Zhuoyuan%20Yong%20and%20Yihua%20Qin%20and%20Jianyu%20Chen%0AAbstract%3A%20%20%20Wheel-legged%20robots%20combine%20the%20advantages%20of%20both%20wheeled%20robots%20and%20legged%0Arobots%2C%20offering%20versatile%20locomotion%20capabilities%20with%20excellent%20stability%20on%0Achallenging%20terrains%20and%20high%20efficiency%20on%20flat%20surfaces.%20However%2C%20existing%0Awheel-legged%20robots%20typically%20have%20limited%20hip%20joint%20mobility%20compared%20to%0Ahumans%2C%20while%20hip%20joint%20plays%20a%20crucial%20role%20in%20locomotion.%20In%20this%20paper%2C%20we%0Aintroduce%20Whleaper%2C%20a%20novel%2010-degree-of-freedom%20%28DOF%29%20bipedal%20wheeled%20robot%2C%0Awith%203%20DOFs%20at%20the%20hip%20of%20each%20leg.%20Its%20humanoid%20joint%20design%20enables%20adaptable%0Amotion%20in%20complex%20scenarios%2C%20ensuring%20stability%20and%20flexibility.%20This%20paper%0Aintroduces%20the%20details%20of%20Whleaper%2C%20with%20a%20focus%20on%20innovative%20mechanical%0Adesign%2C%20control%20algorithms%20and%20system%20implementation.%20Firstly%2C%20stability%20stems%0Afrom%20the%20increased%20DOFs%20at%20the%20hip%2C%20which%20expand%20the%20range%20of%20possible%20postures%0Aand%20improve%20the%20robot%27s%20foot-ground%20contact.%20Secondly%2C%20the%20extra%20DOFs%20also%0Aaugment%20its%20mobility.%20During%20walking%20or%20sliding%2C%20more%20complex%20movements%20can%20be%0Aadopted%20to%20execute%20obstacle%20avoidance%20tasks.%20Thirdly%2C%20we%20utilize%20two%20control%0Aalgorithms%20to%20implement%20multimodal%20motion%20for%20walking%20and%20sliding.%20By%0Acontrolling%20specific%20DOFs%20of%20the%20robot%2C%20we%20conducted%20a%20series%20of%20simulations%0Aand%20practical%20experiments%2C%20demonstrating%20that%20a%20high-DOF%20hip%20joint%20design%20can%0Aeffectively%20enhance%20the%20stability%20and%20flexibility%20of%20wheel-legged%20robots.%0AWhleaper%20shows%20its%20capability%20to%20perform%20actions%20such%20as%20squatting%2C%20obstacle%0Aavoidance%20sliding%2C%20and%20rapid%20turning%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhleaper%253A%2520A%252010-DOF%2520Flexible%2520Bipedal%2520Wheeled%2520Robot%26entry.906535625%3DYinglei%2520Zhu%2520and%2520Sixiao%2520He%2520and%2520Zhenghao%2520Qi%2520and%2520Zhuoyuan%2520Yong%2520and%2520Yihua%2520Qin%2520and%2520Jianyu%2520Chen%26entry.1292438233%3D%2520%2520Wheel-legged%2520robots%2520combine%2520the%2520advantages%2520of%2520both%2520wheeled%2520robots%2520and%2520legged%250Arobots%252C%2520offering%2520versatile%2520locomotion%2520capabilities%2520with%2520excellent%2520stability%2520on%250Achallenging%2520terrains%2520and%2520high%2520efficiency%2520on%2520flat%2520surfaces.%2520However%252C%2520existing%250Awheel-legged%2520robots%2520typically%2520have%2520limited%2520hip%2520joint%2520mobility%2520compared%2520to%250Ahumans%252C%2520while%2520hip%2520joint%2520plays%2520a%2520crucial%2520role%2520in%2520locomotion.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Whleaper%252C%2520a%2520novel%252010-degree-of-freedom%2520%2528DOF%2529%2520bipedal%2520wheeled%2520robot%252C%250Awith%25203%2520DOFs%2520at%2520the%2520hip%2520of%2520each%2520leg.%2520Its%2520humanoid%2520joint%2520design%2520enables%2520adaptable%250Amotion%2520in%2520complex%2520scenarios%252C%2520ensuring%2520stability%2520and%2520flexibility.%2520This%2520paper%250Aintroduces%2520the%2520details%2520of%2520Whleaper%252C%2520with%2520a%2520focus%2520on%2520innovative%2520mechanical%250Adesign%252C%2520control%2520algorithms%2520and%2520system%2520implementation.%2520Firstly%252C%2520stability%2520stems%250Afrom%2520the%2520increased%2520DOFs%2520at%2520the%2520hip%252C%2520which%2520expand%2520the%2520range%2520of%2520possible%2520postures%250Aand%2520improve%2520the%2520robot%2527s%2520foot-ground%2520contact.%2520Secondly%252C%2520the%2520extra%2520DOFs%2520also%250Aaugment%2520its%2520mobility.%2520During%2520walking%2520or%2520sliding%252C%2520more%2520complex%2520movements%2520can%2520be%250Aadopted%2520to%2520execute%2520obstacle%2520avoidance%2520tasks.%2520Thirdly%252C%2520we%2520utilize%2520two%2520control%250Aalgorithms%2520to%2520implement%2520multimodal%2520motion%2520for%2520walking%2520and%2520sliding.%2520By%250Acontrolling%2520specific%2520DOFs%2520of%2520the%2520robot%252C%2520we%2520conducted%2520a%2520series%2520of%2520simulations%250Aand%2520practical%2520experiments%252C%2520demonstrating%2520that%2520a%2520high-DOF%2520hip%2520joint%2520design%2520can%250Aeffectively%2520enhance%2520the%2520stability%2520and%2520flexibility%2520of%2520wheel-legged%2520robots.%250AWhleaper%2520shows%2520its%2520capability%2520to%2520perform%2520actions%2520such%2520as%2520squatting%252C%2520obstacle%250Aavoidance%2520sliding%252C%2520and%2520rapid%2520turning%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whleaper%3A%20A%2010-DOF%20Flexible%20Bipedal%20Wheeled%20Robot&entry.906535625=Yinglei%20Zhu%20and%20Sixiao%20He%20and%20Zhenghao%20Qi%20and%20Zhuoyuan%20Yong%20and%20Yihua%20Qin%20and%20Jianyu%20Chen&entry.1292438233=%20%20Wheel-legged%20robots%20combine%20the%20advantages%20of%20both%20wheeled%20robots%20and%20legged%0Arobots%2C%20offering%20versatile%20locomotion%20capabilities%20with%20excellent%20stability%20on%0Achallenging%20terrains%20and%20high%20efficiency%20on%20flat%20surfaces.%20However%2C%20existing%0Awheel-legged%20robots%20typically%20have%20limited%20hip%20joint%20mobility%20compared%20to%0Ahumans%2C%20while%20hip%20joint%20plays%20a%20crucial%20role%20in%20locomotion.%20In%20this%20paper%2C%20we%0Aintroduce%20Whleaper%2C%20a%20novel%2010-degree-of-freedom%20%28DOF%29%20bipedal%20wheeled%20robot%2C%0Awith%203%20DOFs%20at%20the%20hip%20of%20each%20leg.%20Its%20humanoid%20joint%20design%20enables%20adaptable%0Amotion%20in%20complex%20scenarios%2C%20ensuring%20stability%20and%20flexibility.%20This%20paper%0Aintroduces%20the%20details%20of%20Whleaper%2C%20with%20a%20focus%20on%20innovative%20mechanical%0Adesign%2C%20control%20algorithms%20and%20system%20implementation.%20Firstly%2C%20stability%20stems%0Afrom%20the%20increased%20DOFs%20at%20the%20hip%2C%20which%20expand%20the%20range%20of%20possible%20postures%0Aand%20improve%20the%20robot%27s%20foot-ground%20contact.%20Secondly%2C%20the%20extra%20DOFs%20also%0Aaugment%20its%20mobility.%20During%20walking%20or%20sliding%2C%20more%20complex%20movements%20can%20be%0Aadopted%20to%20execute%20obstacle%20avoidance%20tasks.%20Thirdly%2C%20we%20utilize%20two%20control%0Aalgorithms%20to%20implement%20multimodal%20motion%20for%20walking%20and%20sliding.%20By%0Acontrolling%20specific%20DOFs%20of%20the%20robot%2C%20we%20conducted%20a%20series%20of%20simulations%0Aand%20practical%20experiments%2C%20demonstrating%20that%20a%20high-DOF%20hip%20joint%20design%20can%0Aeffectively%20enhance%20the%20stability%20and%20flexibility%20of%20wheel-legged%20robots.%0AWhleaper%20shows%20its%20capability%20to%20perform%20actions%20such%20as%20squatting%2C%20obstacle%0Aavoidance%20sliding%2C%20and%20rapid%20turning%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21767v1&entry.124074799=Read"},
{"title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets\n  and an Audio Restoration Deep Learning Pipeline", "author": "Minwoo Oh and Minsu Park and Eunil Park", "abstract": "  Short video platforms like YouTube Shorts and TikTok face significant\ncopyright compliance challenges, as infringers frequently embed arbitrary\nbackground music (BGM) to obscure original soundtracks (OST) and evade content\noriginality detection. To tackle this issue, we propose a novel pipeline that\nintegrates Music Source Separation (MSS) and cross-modal video-music retrieval\n(CMVMR). Our approach effectively separates arbitrary BGM from the original\nOST, enabling the restoration of authentic video audio tracks. To support this\nwork, we introduce two domain-specific datasets: OASD-20K for audio separation\nand OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips\nfeaturing mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset\ncomprising 1,121 video and mixed-audio pairs, specifically designed for short\nvideo restoration tasks. Experimental results demonstrate that our pipeline not\nonly removes arbitrary BGM with high accuracy but also restores OSTs, ensuring\ncontent integrity. This approach provides an ethical and scalable solution to\ncopyright challenges in user-generated content on short video platforms.\n", "link": "http://arxiv.org/abs/2504.21772v1", "date": "2025-04-30", "relevancy": 2.0335, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5025}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Copyright%20Infringement%20on%20Short%20Video%20Platforms%3A%20Novel%20Datasets%0A%20%20and%20an%20Audio%20Restoration%20Deep%20Learning%20Pipeline&body=Title%3A%20Solving%20Copyright%20Infringement%20on%20Short%20Video%20Platforms%3A%20Novel%20Datasets%0A%20%20and%20an%20Audio%20Restoration%20Deep%20Learning%20Pipeline%0AAuthor%3A%20Minwoo%20Oh%20and%20Minsu%20Park%20and%20Eunil%20Park%0AAbstract%3A%20%20%20Short%20video%20platforms%20like%20YouTube%20Shorts%20and%20TikTok%20face%20significant%0Acopyright%20compliance%20challenges%2C%20as%20infringers%20frequently%20embed%20arbitrary%0Abackground%20music%20%28BGM%29%20to%20obscure%20original%20soundtracks%20%28OST%29%20and%20evade%20content%0Aoriginality%20detection.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20pipeline%20that%0Aintegrates%20Music%20Source%20Separation%20%28MSS%29%20and%20cross-modal%20video-music%20retrieval%0A%28CMVMR%29.%20Our%20approach%20effectively%20separates%20arbitrary%20BGM%20from%20the%20original%0AOST%2C%20enabling%20the%20restoration%20of%20authentic%20video%20audio%20tracks.%20To%20support%20this%0Awork%2C%20we%20introduce%20two%20domain-specific%20datasets%3A%20OASD-20K%20for%20audio%20separation%0Aand%20OSVAR-160%20for%20pipeline%20evaluation.%20OASD-20K%20contains%2020%2C000%20audio%20clips%0Afeaturing%20mixed%20BGM%20and%20OST%20pairs%2C%20while%20OSVAR160%20is%20a%20unique%20benchmark%20dataset%0Acomprising%201%2C121%20video%20and%20mixed-audio%20pairs%2C%20specifically%20designed%20for%20short%0Avideo%20restoration%20tasks.%20Experimental%20results%20demonstrate%20that%20our%20pipeline%20not%0Aonly%20removes%20arbitrary%20BGM%20with%20high%20accuracy%20but%20also%20restores%20OSTs%2C%20ensuring%0Acontent%20integrity.%20This%20approach%20provides%20an%20ethical%20and%20scalable%20solution%20to%0Acopyright%20challenges%20in%20user-generated%20content%20on%20short%20video%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Copyright%2520Infringement%2520on%2520Short%2520Video%2520Platforms%253A%2520Novel%2520Datasets%250A%2520%2520and%2520an%2520Audio%2520Restoration%2520Deep%2520Learning%2520Pipeline%26entry.906535625%3DMinwoo%2520Oh%2520and%2520Minsu%2520Park%2520and%2520Eunil%2520Park%26entry.1292438233%3D%2520%2520Short%2520video%2520platforms%2520like%2520YouTube%2520Shorts%2520and%2520TikTok%2520face%2520significant%250Acopyright%2520compliance%2520challenges%252C%2520as%2520infringers%2520frequently%2520embed%2520arbitrary%250Abackground%2520music%2520%2528BGM%2529%2520to%2520obscure%2520original%2520soundtracks%2520%2528OST%2529%2520and%2520evade%2520content%250Aoriginality%2520detection.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520pipeline%2520that%250Aintegrates%2520Music%2520Source%2520Separation%2520%2528MSS%2529%2520and%2520cross-modal%2520video-music%2520retrieval%250A%2528CMVMR%2529.%2520Our%2520approach%2520effectively%2520separates%2520arbitrary%2520BGM%2520from%2520the%2520original%250AOST%252C%2520enabling%2520the%2520restoration%2520of%2520authentic%2520video%2520audio%2520tracks.%2520To%2520support%2520this%250Awork%252C%2520we%2520introduce%2520two%2520domain-specific%2520datasets%253A%2520OASD-20K%2520for%2520audio%2520separation%250Aand%2520OSVAR-160%2520for%2520pipeline%2520evaluation.%2520OASD-20K%2520contains%252020%252C000%2520audio%2520clips%250Afeaturing%2520mixed%2520BGM%2520and%2520OST%2520pairs%252C%2520while%2520OSVAR160%2520is%2520a%2520unique%2520benchmark%2520dataset%250Acomprising%25201%252C121%2520video%2520and%2520mixed-audio%2520pairs%252C%2520specifically%2520designed%2520for%2520short%250Avideo%2520restoration%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520pipeline%2520not%250Aonly%2520removes%2520arbitrary%2520BGM%2520with%2520high%2520accuracy%2520but%2520also%2520restores%2520OSTs%252C%2520ensuring%250Acontent%2520integrity.%2520This%2520approach%2520provides%2520an%2520ethical%2520and%2520scalable%2520solution%2520to%250Acopyright%2520challenges%2520in%2520user-generated%2520content%2520on%2520short%2520video%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Copyright%20Infringement%20on%20Short%20Video%20Platforms%3A%20Novel%20Datasets%0A%20%20and%20an%20Audio%20Restoration%20Deep%20Learning%20Pipeline&entry.906535625=Minwoo%20Oh%20and%20Minsu%20Park%20and%20Eunil%20Park&entry.1292438233=%20%20Short%20video%20platforms%20like%20YouTube%20Shorts%20and%20TikTok%20face%20significant%0Acopyright%20compliance%20challenges%2C%20as%20infringers%20frequently%20embed%20arbitrary%0Abackground%20music%20%28BGM%29%20to%20obscure%20original%20soundtracks%20%28OST%29%20and%20evade%20content%0Aoriginality%20detection.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20pipeline%20that%0Aintegrates%20Music%20Source%20Separation%20%28MSS%29%20and%20cross-modal%20video-music%20retrieval%0A%28CMVMR%29.%20Our%20approach%20effectively%20separates%20arbitrary%20BGM%20from%20the%20original%0AOST%2C%20enabling%20the%20restoration%20of%20authentic%20video%20audio%20tracks.%20To%20support%20this%0Awork%2C%20we%20introduce%20two%20domain-specific%20datasets%3A%20OASD-20K%20for%20audio%20separation%0Aand%20OSVAR-160%20for%20pipeline%20evaluation.%20OASD-20K%20contains%2020%2C000%20audio%20clips%0Afeaturing%20mixed%20BGM%20and%20OST%20pairs%2C%20while%20OSVAR160%20is%20a%20unique%20benchmark%20dataset%0Acomprising%201%2C121%20video%20and%20mixed-audio%20pairs%2C%20specifically%20designed%20for%20short%0Avideo%20restoration%20tasks.%20Experimental%20results%20demonstrate%20that%20our%20pipeline%20not%0Aonly%20removes%20arbitrary%20BGM%20with%20high%20accuracy%20but%20also%20restores%20OSTs%2C%20ensuring%0Acontent%20integrity.%20This%20approach%20provides%20an%20ethical%20and%20scalable%20solution%20to%0Acopyright%20challenges%20in%20user-generated%20content%20on%20short%20video%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21772v1&entry.124074799=Read"},
{"title": "VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector\n  Fonts via Signed Distance Functions", "author": "Zeqing Xia and Bojun Xiong and Zhouhui Lian", "abstract": "  Font design is of vital importance in the digital content design and modern\nprinting industry. Developing algorithms capable of automatically synthesizing\nvector fonts can significantly facilitate the font design process. However,\nexisting methods mainly concentrate on raster image generation, and only a few\napproaches can directly synthesize vector fonts. This paper proposes an\nend-to-end trainable method, VecFontSDF, to reconstruct and synthesize\nhigh-quality vector fonts using signed distance functions (SDFs). Specifically,\nbased on the proposed SDF-based implicit shape representation, VecFontSDF\nlearns to model each glyph as shape primitives enclosed by several parabolic\ncurves, which can be precisely converted to quadratic B\\'ezier curves that are\nwidely used in vector font products. In this manner, most image generation\nmethods can be easily extended to synthesize vector fonts. Qualitative and\nquantitative experiments conducted on a publicly-available dataset demonstrate\nthat our method obtains high-quality results on several tasks, including vector\nfont reconstruction, interpolation, and few-shot vector font synthesis,\nmarkedly outperforming the state of the art.\n", "link": "http://arxiv.org/abs/2303.12675v2", "date": "2025-04-30", "relevancy": 2.0261, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5603}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5017}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VecFontSDF%3A%20Learning%20to%20Reconstruct%20and%20Synthesize%20High-quality%20Vector%0A%20%20Fonts%20via%20Signed%20Distance%20Functions&body=Title%3A%20VecFontSDF%3A%20Learning%20to%20Reconstruct%20and%20Synthesize%20High-quality%20Vector%0A%20%20Fonts%20via%20Signed%20Distance%20Functions%0AAuthor%3A%20Zeqing%20Xia%20and%20Bojun%20Xiong%20and%20Zhouhui%20Lian%0AAbstract%3A%20%20%20Font%20design%20is%20of%20vital%20importance%20in%20the%20digital%20content%20design%20and%20modern%0Aprinting%20industry.%20Developing%20algorithms%20capable%20of%20automatically%20synthesizing%0Avector%20fonts%20can%20significantly%20facilitate%20the%20font%20design%20process.%20However%2C%0Aexisting%20methods%20mainly%20concentrate%20on%20raster%20image%20generation%2C%20and%20only%20a%20few%0Aapproaches%20can%20directly%20synthesize%20vector%20fonts.%20This%20paper%20proposes%20an%0Aend-to-end%20trainable%20method%2C%20VecFontSDF%2C%20to%20reconstruct%20and%20synthesize%0Ahigh-quality%20vector%20fonts%20using%20signed%20distance%20functions%20%28SDFs%29.%20Specifically%2C%0Abased%20on%20the%20proposed%20SDF-based%20implicit%20shape%20representation%2C%20VecFontSDF%0Alearns%20to%20model%20each%20glyph%20as%20shape%20primitives%20enclosed%20by%20several%20parabolic%0Acurves%2C%20which%20can%20be%20precisely%20converted%20to%20quadratic%20B%5C%27ezier%20curves%20that%20are%0Awidely%20used%20in%20vector%20font%20products.%20In%20this%20manner%2C%20most%20image%20generation%0Amethods%20can%20be%20easily%20extended%20to%20synthesize%20vector%20fonts.%20Qualitative%20and%0Aquantitative%20experiments%20conducted%20on%20a%20publicly-available%20dataset%20demonstrate%0Athat%20our%20method%20obtains%20high-quality%20results%20on%20several%20tasks%2C%20including%20vector%0Afont%20reconstruction%2C%20interpolation%2C%20and%20few-shot%20vector%20font%20synthesis%2C%0Amarkedly%20outperforming%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.12675v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVecFontSDF%253A%2520Learning%2520to%2520Reconstruct%2520and%2520Synthesize%2520High-quality%2520Vector%250A%2520%2520Fonts%2520via%2520Signed%2520Distance%2520Functions%26entry.906535625%3DZeqing%2520Xia%2520and%2520Bojun%2520Xiong%2520and%2520Zhouhui%2520Lian%26entry.1292438233%3D%2520%2520Font%2520design%2520is%2520of%2520vital%2520importance%2520in%2520the%2520digital%2520content%2520design%2520and%2520modern%250Aprinting%2520industry.%2520Developing%2520algorithms%2520capable%2520of%2520automatically%2520synthesizing%250Avector%2520fonts%2520can%2520significantly%2520facilitate%2520the%2520font%2520design%2520process.%2520However%252C%250Aexisting%2520methods%2520mainly%2520concentrate%2520on%2520raster%2520image%2520generation%252C%2520and%2520only%2520a%2520few%250Aapproaches%2520can%2520directly%2520synthesize%2520vector%2520fonts.%2520This%2520paper%2520proposes%2520an%250Aend-to-end%2520trainable%2520method%252C%2520VecFontSDF%252C%2520to%2520reconstruct%2520and%2520synthesize%250Ahigh-quality%2520vector%2520fonts%2520using%2520signed%2520distance%2520functions%2520%2528SDFs%2529.%2520Specifically%252C%250Abased%2520on%2520the%2520proposed%2520SDF-based%2520implicit%2520shape%2520representation%252C%2520VecFontSDF%250Alearns%2520to%2520model%2520each%2520glyph%2520as%2520shape%2520primitives%2520enclosed%2520by%2520several%2520parabolic%250Acurves%252C%2520which%2520can%2520be%2520precisely%2520converted%2520to%2520quadratic%2520B%255C%2527ezier%2520curves%2520that%2520are%250Awidely%2520used%2520in%2520vector%2520font%2520products.%2520In%2520this%2520manner%252C%2520most%2520image%2520generation%250Amethods%2520can%2520be%2520easily%2520extended%2520to%2520synthesize%2520vector%2520fonts.%2520Qualitative%2520and%250Aquantitative%2520experiments%2520conducted%2520on%2520a%2520publicly-available%2520dataset%2520demonstrate%250Athat%2520our%2520method%2520obtains%2520high-quality%2520results%2520on%2520several%2520tasks%252C%2520including%2520vector%250Afont%2520reconstruction%252C%2520interpolation%252C%2520and%2520few-shot%2520vector%2520font%2520synthesis%252C%250Amarkedly%2520outperforming%2520the%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.12675v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VecFontSDF%3A%20Learning%20to%20Reconstruct%20and%20Synthesize%20High-quality%20Vector%0A%20%20Fonts%20via%20Signed%20Distance%20Functions&entry.906535625=Zeqing%20Xia%20and%20Bojun%20Xiong%20and%20Zhouhui%20Lian&entry.1292438233=%20%20Font%20design%20is%20of%20vital%20importance%20in%20the%20digital%20content%20design%20and%20modern%0Aprinting%20industry.%20Developing%20algorithms%20capable%20of%20automatically%20synthesizing%0Avector%20fonts%20can%20significantly%20facilitate%20the%20font%20design%20process.%20However%2C%0Aexisting%20methods%20mainly%20concentrate%20on%20raster%20image%20generation%2C%20and%20only%20a%20few%0Aapproaches%20can%20directly%20synthesize%20vector%20fonts.%20This%20paper%20proposes%20an%0Aend-to-end%20trainable%20method%2C%20VecFontSDF%2C%20to%20reconstruct%20and%20synthesize%0Ahigh-quality%20vector%20fonts%20using%20signed%20distance%20functions%20%28SDFs%29.%20Specifically%2C%0Abased%20on%20the%20proposed%20SDF-based%20implicit%20shape%20representation%2C%20VecFontSDF%0Alearns%20to%20model%20each%20glyph%20as%20shape%20primitives%20enclosed%20by%20several%20parabolic%0Acurves%2C%20which%20can%20be%20precisely%20converted%20to%20quadratic%20B%5C%27ezier%20curves%20that%20are%0Awidely%20used%20in%20vector%20font%20products.%20In%20this%20manner%2C%20most%20image%20generation%0Amethods%20can%20be%20easily%20extended%20to%20synthesize%20vector%20fonts.%20Qualitative%20and%0Aquantitative%20experiments%20conducted%20on%20a%20publicly-available%20dataset%20demonstrate%0Athat%20our%20method%20obtains%20high-quality%20results%20on%20several%20tasks%2C%20including%20vector%0Afont%20reconstruction%2C%20interpolation%2C%20and%20few-shot%20vector%20font%20synthesis%2C%0Amarkedly%20outperforming%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.12675v2&entry.124074799=Read"},
{"title": "Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary\n  Variables", "author": "Yaru Liu and Yiqi Gu and Michael K. Ng", "abstract": "  In this paper, we develop a new optimization framework for the least squares\nlearning problem via fully connected neural networks or physics-informed neural\nnetworks. The gradient descent sometimes behaves inefficiently in deep learning\nbecause of the high non-convexity of loss functions and the vanishing gradient\nissue. Our idea is to introduce auxiliary variables to separate the layers of\nthe deep neural networks and reformulate the loss functions for ease of\noptimization. We design the self-adaptive weights to preserve the consistency\nbetween the reformulated loss and the original mean squared loss, which\nguarantees that optimizing the new loss helps optimize the original problem.\nNumerical experiments are presented to verify the consistency and show the\neffectiveness and robustness of our models over gradient descent.\n", "link": "http://arxiv.org/abs/2504.21501v1", "date": "2025-04-30", "relevancy": 2.0218, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5209}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4992}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Optimization%20Using%20Self-Adaptive%20Weighted%20Auxiliary%0A%20%20Variables&body=Title%3A%20Deep%20Learning%20Optimization%20Using%20Self-Adaptive%20Weighted%20Auxiliary%0A%20%20Variables%0AAuthor%3A%20Yaru%20Liu%20and%20Yiqi%20Gu%20and%20Michael%20K.%20Ng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20a%20new%20optimization%20framework%20for%20the%20least%20squares%0Alearning%20problem%20via%20fully%20connected%20neural%20networks%20or%20physics-informed%20neural%0Anetworks.%20The%20gradient%20descent%20sometimes%20behaves%20inefficiently%20in%20deep%20learning%0Abecause%20of%20the%20high%20non-convexity%20of%20loss%20functions%20and%20the%20vanishing%20gradient%0Aissue.%20Our%20idea%20is%20to%20introduce%20auxiliary%20variables%20to%20separate%20the%20layers%20of%0Athe%20deep%20neural%20networks%20and%20reformulate%20the%20loss%20functions%20for%20ease%20of%0Aoptimization.%20We%20design%20the%20self-adaptive%20weights%20to%20preserve%20the%20consistency%0Abetween%20the%20reformulated%20loss%20and%20the%20original%20mean%20squared%20loss%2C%20which%0Aguarantees%20that%20optimizing%20the%20new%20loss%20helps%20optimize%20the%20original%20problem.%0ANumerical%20experiments%20are%20presented%20to%20verify%20the%20consistency%20and%20show%20the%0Aeffectiveness%20and%20robustness%20of%20our%20models%20over%20gradient%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Optimization%2520Using%2520Self-Adaptive%2520Weighted%2520Auxiliary%250A%2520%2520Variables%26entry.906535625%3DYaru%2520Liu%2520and%2520Yiqi%2520Gu%2520and%2520Michael%2520K.%2520Ng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520new%2520optimization%2520framework%2520for%2520the%2520least%2520squares%250Alearning%2520problem%2520via%2520fully%2520connected%2520neural%2520networks%2520or%2520physics-informed%2520neural%250Anetworks.%2520The%2520gradient%2520descent%2520sometimes%2520behaves%2520inefficiently%2520in%2520deep%2520learning%250Abecause%2520of%2520the%2520high%2520non-convexity%2520of%2520loss%2520functions%2520and%2520the%2520vanishing%2520gradient%250Aissue.%2520Our%2520idea%2520is%2520to%2520introduce%2520auxiliary%2520variables%2520to%2520separate%2520the%2520layers%2520of%250Athe%2520deep%2520neural%2520networks%2520and%2520reformulate%2520the%2520loss%2520functions%2520for%2520ease%2520of%250Aoptimization.%2520We%2520design%2520the%2520self-adaptive%2520weights%2520to%2520preserve%2520the%2520consistency%250Abetween%2520the%2520reformulated%2520loss%2520and%2520the%2520original%2520mean%2520squared%2520loss%252C%2520which%250Aguarantees%2520that%2520optimizing%2520the%2520new%2520loss%2520helps%2520optimize%2520the%2520original%2520problem.%250ANumerical%2520experiments%2520are%2520presented%2520to%2520verify%2520the%2520consistency%2520and%2520show%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520our%2520models%2520over%2520gradient%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Optimization%20Using%20Self-Adaptive%20Weighted%20Auxiliary%0A%20%20Variables&entry.906535625=Yaru%20Liu%20and%20Yiqi%20Gu%20and%20Michael%20K.%20Ng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20a%20new%20optimization%20framework%20for%20the%20least%20squares%0Alearning%20problem%20via%20fully%20connected%20neural%20networks%20or%20physics-informed%20neural%0Anetworks.%20The%20gradient%20descent%20sometimes%20behaves%20inefficiently%20in%20deep%20learning%0Abecause%20of%20the%20high%20non-convexity%20of%20loss%20functions%20and%20the%20vanishing%20gradient%0Aissue.%20Our%20idea%20is%20to%20introduce%20auxiliary%20variables%20to%20separate%20the%20layers%20of%0Athe%20deep%20neural%20networks%20and%20reformulate%20the%20loss%20functions%20for%20ease%20of%0Aoptimization.%20We%20design%20the%20self-adaptive%20weights%20to%20preserve%20the%20consistency%0Abetween%20the%20reformulated%20loss%20and%20the%20original%20mean%20squared%20loss%2C%20which%0Aguarantees%20that%20optimizing%20the%20new%20loss%20helps%20optimize%20the%20original%20problem.%0ANumerical%20experiments%20are%20presented%20to%20verify%20the%20consistency%20and%20show%20the%0Aeffectiveness%20and%20robustness%20of%20our%20models%20over%20gradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21501v1&entry.124074799=Read"},
{"title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey", "author": "Saber Mehdipour and Seyed Abolghasem Mirroshandel and Seyed Amirhossein Tabatabaei", "abstract": "  Detecting plant diseases is a crucial aspect of modern agriculture - it plays\na key role in maintaining crop health and increasing overall yield. Traditional\napproaches, though still valuable, often rely on manual inspection or\nconventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering benefits such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This survey\nexplores the application of ViTs in precision agriculture, covering tasks from\nclassification to detection and segmentation. We begin by introducing the\nfoundational architecture of ViTs and discuss their transition from Natural\nLanguage Processing (NLP) to computer vision. The discussion includes the\nconcept of inductive bias in traditional models like Convolutional Neural\nNetworks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive\nreview of recent literature, focusing on key methodologies, datasets, and\nperformance metrics. The survey also includes a comparative analysis of CNNs\nand ViTs, with a look at hybrid models and performance enhancements. Technical\nchallenges - such as data requirements, computational demands, and model\ninterpretability - are addressed alongside potential solutions. Finally, we\noutline potential research directions and technological advancements that could\nfurther support the integration of ViTs in real-world agricultural settings.\nOur goal with this study is to offer practitioners and researchers a deeper\nunderstanding of how ViTs are poised to transform smart and precision\nagriculture.\n", "link": "http://arxiv.org/abs/2504.21706v1", "date": "2025-04-30", "relevancy": 2.0182, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformers%20in%20Precision%20Agriculture%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Vision%20Transformers%20in%20Precision%20Agriculture%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Saber%20Mehdipour%20and%20Seyed%20Abolghasem%20Mirroshandel%20and%20Seyed%20Amirhossein%20Tabatabaei%0AAbstract%3A%20%20%20Detecting%20plant%20diseases%20is%20a%20crucial%20aspect%20of%20modern%20agriculture%20-%20it%20plays%0Aa%20key%20role%20in%20maintaining%20crop%20health%20and%20increasing%20overall%20yield.%20Traditional%0Aapproaches%2C%20though%20still%20valuable%2C%20often%20rely%20on%20manual%20inspection%20or%0Aconventional%20machine%20learning%20techniques%2C%20both%20of%20which%20face%20limitations%20in%0Ascalability%20and%20accuracy.%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%20have%20emerged%20as%0Aa%20promising%20alternative%2C%20offering%20benefits%20such%20as%20improved%20handling%20of%0Along-range%20dependencies%20and%20better%20scalability%20for%20visual%20tasks.%20This%20survey%0Aexplores%20the%20application%20of%20ViTs%20in%20precision%20agriculture%2C%20covering%20tasks%20from%0Aclassification%20to%20detection%20and%20segmentation.%20We%20begin%20by%20introducing%20the%0Afoundational%20architecture%20of%20ViTs%20and%20discuss%20their%20transition%20from%20Natural%0ALanguage%20Processing%20%28NLP%29%20to%20computer%20vision.%20The%20discussion%20includes%20the%0Aconcept%20of%20inductive%20bias%20in%20traditional%20models%20like%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%2C%20and%20how%20ViTs%20mitigate%20these%20biases.%20We%20provide%20a%20comprehensive%0Areview%20of%20recent%20literature%2C%20focusing%20on%20key%20methodologies%2C%20datasets%2C%20and%0Aperformance%20metrics.%20The%20survey%20also%20includes%20a%20comparative%20analysis%20of%20CNNs%0Aand%20ViTs%2C%20with%20a%20look%20at%20hybrid%20models%20and%20performance%20enhancements.%20Technical%0Achallenges%20-%20such%20as%20data%20requirements%2C%20computational%20demands%2C%20and%20model%0Ainterpretability%20-%20are%20addressed%20alongside%20potential%20solutions.%20Finally%2C%20we%0Aoutline%20potential%20research%20directions%20and%20technological%20advancements%20that%20could%0Afurther%20support%20the%20integration%20of%20ViTs%20in%20real-world%20agricultural%20settings.%0AOur%20goal%20with%20this%20study%20is%20to%20offer%20practitioners%20and%20researchers%20a%20deeper%0Aunderstanding%20of%20how%20ViTs%20are%20poised%20to%20transform%20smart%20and%20precision%0Aagriculture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformers%2520in%2520Precision%2520Agriculture%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DSaber%2520Mehdipour%2520and%2520Seyed%2520Abolghasem%2520Mirroshandel%2520and%2520Seyed%2520Amirhossein%2520Tabatabaei%26entry.1292438233%3D%2520%2520Detecting%2520plant%2520diseases%2520is%2520a%2520crucial%2520aspect%2520of%2520modern%2520agriculture%2520-%2520it%2520plays%250Aa%2520key%2520role%2520in%2520maintaining%2520crop%2520health%2520and%2520increasing%2520overall%2520yield.%2520Traditional%250Aapproaches%252C%2520though%2520still%2520valuable%252C%2520often%2520rely%2520on%2520manual%2520inspection%2520or%250Aconventional%2520machine%2520learning%2520techniques%252C%2520both%2520of%2520which%2520face%2520limitations%2520in%250Ascalability%2520and%2520accuracy.%2520Recently%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520emerged%2520as%250Aa%2520promising%2520alternative%252C%2520offering%2520benefits%2520such%2520as%2520improved%2520handling%2520of%250Along-range%2520dependencies%2520and%2520better%2520scalability%2520for%2520visual%2520tasks.%2520This%2520survey%250Aexplores%2520the%2520application%2520of%2520ViTs%2520in%2520precision%2520agriculture%252C%2520covering%2520tasks%2520from%250Aclassification%2520to%2520detection%2520and%2520segmentation.%2520We%2520begin%2520by%2520introducing%2520the%250Afoundational%2520architecture%2520of%2520ViTs%2520and%2520discuss%2520their%2520transition%2520from%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%2520to%2520computer%2520vision.%2520The%2520discussion%2520includes%2520the%250Aconcept%2520of%2520inductive%2520bias%2520in%2520traditional%2520models%2520like%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%252C%2520and%2520how%2520ViTs%2520mitigate%2520these%2520biases.%2520We%2520provide%2520a%2520comprehensive%250Areview%2520of%2520recent%2520literature%252C%2520focusing%2520on%2520key%2520methodologies%252C%2520datasets%252C%2520and%250Aperformance%2520metrics.%2520The%2520survey%2520also%2520includes%2520a%2520comparative%2520analysis%2520of%2520CNNs%250Aand%2520ViTs%252C%2520with%2520a%2520look%2520at%2520hybrid%2520models%2520and%2520performance%2520enhancements.%2520Technical%250Achallenges%2520-%2520such%2520as%2520data%2520requirements%252C%2520computational%2520demands%252C%2520and%2520model%250Ainterpretability%2520-%2520are%2520addressed%2520alongside%2520potential%2520solutions.%2520Finally%252C%2520we%250Aoutline%2520potential%2520research%2520directions%2520and%2520technological%2520advancements%2520that%2520could%250Afurther%2520support%2520the%2520integration%2520of%2520ViTs%2520in%2520real-world%2520agricultural%2520settings.%250AOur%2520goal%2520with%2520this%2520study%2520is%2520to%2520offer%2520practitioners%2520and%2520researchers%2520a%2520deeper%250Aunderstanding%2520of%2520how%2520ViTs%2520are%2520poised%2520to%2520transform%2520smart%2520and%2520precision%250Aagriculture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformers%20in%20Precision%20Agriculture%3A%20A%20Comprehensive%20Survey&entry.906535625=Saber%20Mehdipour%20and%20Seyed%20Abolghasem%20Mirroshandel%20and%20Seyed%20Amirhossein%20Tabatabaei&entry.1292438233=%20%20Detecting%20plant%20diseases%20is%20a%20crucial%20aspect%20of%20modern%20agriculture%20-%20it%20plays%0Aa%20key%20role%20in%20maintaining%20crop%20health%20and%20increasing%20overall%20yield.%20Traditional%0Aapproaches%2C%20though%20still%20valuable%2C%20often%20rely%20on%20manual%20inspection%20or%0Aconventional%20machine%20learning%20techniques%2C%20both%20of%20which%20face%20limitations%20in%0Ascalability%20and%20accuracy.%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%20have%20emerged%20as%0Aa%20promising%20alternative%2C%20offering%20benefits%20such%20as%20improved%20handling%20of%0Along-range%20dependencies%20and%20better%20scalability%20for%20visual%20tasks.%20This%20survey%0Aexplores%20the%20application%20of%20ViTs%20in%20precision%20agriculture%2C%20covering%20tasks%20from%0Aclassification%20to%20detection%20and%20segmentation.%20We%20begin%20by%20introducing%20the%0Afoundational%20architecture%20of%20ViTs%20and%20discuss%20their%20transition%20from%20Natural%0ALanguage%20Processing%20%28NLP%29%20to%20computer%20vision.%20The%20discussion%20includes%20the%0Aconcept%20of%20inductive%20bias%20in%20traditional%20models%20like%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%2C%20and%20how%20ViTs%20mitigate%20these%20biases.%20We%20provide%20a%20comprehensive%0Areview%20of%20recent%20literature%2C%20focusing%20on%20key%20methodologies%2C%20datasets%2C%20and%0Aperformance%20metrics.%20The%20survey%20also%20includes%20a%20comparative%20analysis%20of%20CNNs%0Aand%20ViTs%2C%20with%20a%20look%20at%20hybrid%20models%20and%20performance%20enhancements.%20Technical%0Achallenges%20-%20such%20as%20data%20requirements%2C%20computational%20demands%2C%20and%20model%0Ainterpretability%20-%20are%20addressed%20alongside%20potential%20solutions.%20Finally%2C%20we%0Aoutline%20potential%20research%20directions%20and%20technological%20advancements%20that%20could%0Afurther%20support%20the%20integration%20of%20ViTs%20in%20real-world%20agricultural%20settings.%0AOur%20goal%20with%20this%20study%20is%20to%20offer%20practitioners%20and%20researchers%20a%20deeper%0Aunderstanding%20of%20how%20ViTs%20are%20poised%20to%20transform%20smart%20and%20precision%0Aagriculture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21706v1&entry.124074799=Read"},
{"title": "Fine-tuning Is a Surprisingly Effective Domain Adaptation Baseline in\n  Handwriting Recognition", "author": "Jan Koh\u00fat and Michal Hradi\u0161", "abstract": "  In many machine learning tasks, a large general dataset and a small\nspecialized dataset are available. In such situations, various domain\nadaptation methods can be used to adapt a general model to the target dataset.\nWe show that in the case of neural networks trained for handwriting recognition\nusing CTC, simple fine-tuning with data augmentation works surprisingly well in\nsuch scenarios and that it is resistant to overfitting even for very small\ntarget domain datasets. We evaluated the behavior of fine-tuning with respect\nto augmentation, training data size, and quality of the pre-trained network,\nboth in writer-dependent and writer-independent settings. On a large real-world\ndataset, fine-tuning on new writers provided an average relative CER\nimprovement of 25 % for 16 text lines and 50 % for 256 text lines.\n", "link": "http://arxiv.org/abs/2302.06308v2", "date": "2025-04-30", "relevancy": 2.0142, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5085}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5055}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20Is%20a%20Surprisingly%20Effective%20Domain%20Adaptation%20Baseline%20in%0A%20%20Handwriting%20Recognition&body=Title%3A%20Fine-tuning%20Is%20a%20Surprisingly%20Effective%20Domain%20Adaptation%20Baseline%20in%0A%20%20Handwriting%20Recognition%0AAuthor%3A%20Jan%20Koh%C3%BAt%20and%20Michal%20Hradi%C5%A1%0AAbstract%3A%20%20%20In%20many%20machine%20learning%20tasks%2C%20a%20large%20general%20dataset%20and%20a%20small%0Aspecialized%20dataset%20are%20available.%20In%20such%20situations%2C%20various%20domain%0Aadaptation%20methods%20can%20be%20used%20to%20adapt%20a%20general%20model%20to%20the%20target%20dataset.%0AWe%20show%20that%20in%20the%20case%20of%20neural%20networks%20trained%20for%20handwriting%20recognition%0Ausing%20CTC%2C%20simple%20fine-tuning%20with%20data%20augmentation%20works%20surprisingly%20well%20in%0Asuch%20scenarios%20and%20that%20it%20is%20resistant%20to%20overfitting%20even%20for%20very%20small%0Atarget%20domain%20datasets.%20We%20evaluated%20the%20behavior%20of%20fine-tuning%20with%20respect%0Ato%20augmentation%2C%20training%20data%20size%2C%20and%20quality%20of%20the%20pre-trained%20network%2C%0Aboth%20in%20writer-dependent%20and%20writer-independent%20settings.%20On%20a%20large%20real-world%0Adataset%2C%20fine-tuning%20on%20new%20writers%20provided%20an%20average%20relative%20CER%0Aimprovement%20of%2025%20%25%20for%2016%20text%20lines%20and%2050%20%25%20for%20256%20text%20lines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.06308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520Is%2520a%2520Surprisingly%2520Effective%2520Domain%2520Adaptation%2520Baseline%2520in%250A%2520%2520Handwriting%2520Recognition%26entry.906535625%3DJan%2520Koh%25C3%25BAt%2520and%2520Michal%2520Hradi%25C5%25A1%26entry.1292438233%3D%2520%2520In%2520many%2520machine%2520learning%2520tasks%252C%2520a%2520large%2520general%2520dataset%2520and%2520a%2520small%250Aspecialized%2520dataset%2520are%2520available.%2520In%2520such%2520situations%252C%2520various%2520domain%250Aadaptation%2520methods%2520can%2520be%2520used%2520to%2520adapt%2520a%2520general%2520model%2520to%2520the%2520target%2520dataset.%250AWe%2520show%2520that%2520in%2520the%2520case%2520of%2520neural%2520networks%2520trained%2520for%2520handwriting%2520recognition%250Ausing%2520CTC%252C%2520simple%2520fine-tuning%2520with%2520data%2520augmentation%2520works%2520surprisingly%2520well%2520in%250Asuch%2520scenarios%2520and%2520that%2520it%2520is%2520resistant%2520to%2520overfitting%2520even%2520for%2520very%2520small%250Atarget%2520domain%2520datasets.%2520We%2520evaluated%2520the%2520behavior%2520of%2520fine-tuning%2520with%2520respect%250Ato%2520augmentation%252C%2520training%2520data%2520size%252C%2520and%2520quality%2520of%2520the%2520pre-trained%2520network%252C%250Aboth%2520in%2520writer-dependent%2520and%2520writer-independent%2520settings.%2520On%2520a%2520large%2520real-world%250Adataset%252C%2520fine-tuning%2520on%2520new%2520writers%2520provided%2520an%2520average%2520relative%2520CER%250Aimprovement%2520of%252025%2520%2525%2520for%252016%2520text%2520lines%2520and%252050%2520%2525%2520for%2520256%2520text%2520lines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.06308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20Is%20a%20Surprisingly%20Effective%20Domain%20Adaptation%20Baseline%20in%0A%20%20Handwriting%20Recognition&entry.906535625=Jan%20Koh%C3%BAt%20and%20Michal%20Hradi%C5%A1&entry.1292438233=%20%20In%20many%20machine%20learning%20tasks%2C%20a%20large%20general%20dataset%20and%20a%20small%0Aspecialized%20dataset%20are%20available.%20In%20such%20situations%2C%20various%20domain%0Aadaptation%20methods%20can%20be%20used%20to%20adapt%20a%20general%20model%20to%20the%20target%20dataset.%0AWe%20show%20that%20in%20the%20case%20of%20neural%20networks%20trained%20for%20handwriting%20recognition%0Ausing%20CTC%2C%20simple%20fine-tuning%20with%20data%20augmentation%20works%20surprisingly%20well%20in%0Asuch%20scenarios%20and%20that%20it%20is%20resistant%20to%20overfitting%20even%20for%20very%20small%0Atarget%20domain%20datasets.%20We%20evaluated%20the%20behavior%20of%20fine-tuning%20with%20respect%0Ato%20augmentation%2C%20training%20data%20size%2C%20and%20quality%20of%20the%20pre-trained%20network%2C%0Aboth%20in%20writer-dependent%20and%20writer-independent%20settings.%20On%20a%20large%20real-world%0Adataset%2C%20fine-tuning%20on%20new%20writers%20provided%20an%20average%20relative%20CER%0Aimprovement%20of%2025%20%25%20for%2016%20text%20lines%20and%2050%20%25%20for%20256%20text%20lines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.06308v2&entry.124074799=Read"},
{"title": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb", "author": "Fotis I. Giasemis and Vladimir Lon\u010dar and Bertrand Granado and Vladimir Vava Gligorov", "abstract": "  In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power.\n", "link": "http://arxiv.org/abs/2502.02304v4", "date": "2025-04-30", "relevancy": 1.9772, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4982}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4927}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20FPGA%20and%20GPU%20Performance%20for%20Machine%0A%20%20Learning-Based%20Track%20Reconstruction%20at%20LHCb&body=Title%3A%20Comparative%20Analysis%20of%20FPGA%20and%20GPU%20Performance%20for%20Machine%0A%20%20Learning-Based%20Track%20Reconstruction%20at%20LHCb%0AAuthor%3A%20Fotis%20I.%20Giasemis%20and%20Vladimir%20Lon%C4%8Dar%20and%20Bertrand%20Granado%20and%20Vladimir%20Vava%20Gligorov%0AAbstract%3A%20%20%20In%20high-energy%20physics%2C%20the%20increasing%20luminosity%20and%20detector%20granularity%20at%0Athe%20Large%20Hadron%20Collider%20are%20driving%20the%20need%20for%20more%20efficient%20data%0Aprocessing%20solutions.%20Machine%20Learning%20has%20emerged%20as%20a%20promising%20tool%20for%0Areconstructing%20charged%20particle%20tracks%2C%20due%20to%20its%20potentially%20linear%0Acomputational%20scaling%20with%20detector%20hits.%20The%20recent%20implementation%20of%20a%20graph%0Aneural%20network-based%20track%20reconstruction%20pipeline%20in%20the%20first%20level%20trigger%0Aof%20the%20LHCb%20experiment%20on%20GPUs%20serves%20as%20a%20platform%20for%20comparative%20studies%0Abetween%20computational%20architectures%20in%20the%20context%20of%20high-energy%20physics.%20This%0Apaper%20presents%20a%20novel%20comparison%20of%20the%20throughput%20of%20ML%20model%20inference%0Abetween%20FPGAs%20and%20GPUs%2C%20focusing%20on%20the%20first%20step%20of%20the%20track%20reconstruction%0Apipeline%24%5Cunicode%7Bx2013%7D%24an%20implementation%20of%20a%20multilayer%20perceptron.%20Using%0AHLS4ML%20for%20FPGA%20deployment%2C%20we%20benchmark%20its%20performance%20against%20the%20GPU%0Aimplementation%20and%20demonstrate%20the%20potential%20of%20FPGAs%20for%20high-throughput%2C%0Alow-latency%20inference%20without%20the%20need%20for%20an%20expertise%20in%20FPGA%20development%20and%0Awhile%20consuming%20significantly%20less%20power.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02304v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520FPGA%2520and%2520GPU%2520Performance%2520for%2520Machine%250A%2520%2520Learning-Based%2520Track%2520Reconstruction%2520at%2520LHCb%26entry.906535625%3DFotis%2520I.%2520Giasemis%2520and%2520Vladimir%2520Lon%25C4%258Dar%2520and%2520Bertrand%2520Granado%2520and%2520Vladimir%2520Vava%2520Gligorov%26entry.1292438233%3D%2520%2520In%2520high-energy%2520physics%252C%2520the%2520increasing%2520luminosity%2520and%2520detector%2520granularity%2520at%250Athe%2520Large%2520Hadron%2520Collider%2520are%2520driving%2520the%2520need%2520for%2520more%2520efficient%2520data%250Aprocessing%2520solutions.%2520Machine%2520Learning%2520has%2520emerged%2520as%2520a%2520promising%2520tool%2520for%250Areconstructing%2520charged%2520particle%2520tracks%252C%2520due%2520to%2520its%2520potentially%2520linear%250Acomputational%2520scaling%2520with%2520detector%2520hits.%2520The%2520recent%2520implementation%2520of%2520a%2520graph%250Aneural%2520network-based%2520track%2520reconstruction%2520pipeline%2520in%2520the%2520first%2520level%2520trigger%250Aof%2520the%2520LHCb%2520experiment%2520on%2520GPUs%2520serves%2520as%2520a%2520platform%2520for%2520comparative%2520studies%250Abetween%2520computational%2520architectures%2520in%2520the%2520context%2520of%2520high-energy%2520physics.%2520This%250Apaper%2520presents%2520a%2520novel%2520comparison%2520of%2520the%2520throughput%2520of%2520ML%2520model%2520inference%250Abetween%2520FPGAs%2520and%2520GPUs%252C%2520focusing%2520on%2520the%2520first%2520step%2520of%2520the%2520track%2520reconstruction%250Apipeline%2524%255Cunicode%257Bx2013%257D%2524an%2520implementation%2520of%2520a%2520multilayer%2520perceptron.%2520Using%250AHLS4ML%2520for%2520FPGA%2520deployment%252C%2520we%2520benchmark%2520its%2520performance%2520against%2520the%2520GPU%250Aimplementation%2520and%2520demonstrate%2520the%2520potential%2520of%2520FPGAs%2520for%2520high-throughput%252C%250Alow-latency%2520inference%2520without%2520the%2520need%2520for%2520an%2520expertise%2520in%2520FPGA%2520development%2520and%250Awhile%2520consuming%2520significantly%2520less%2520power.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02304v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20FPGA%20and%20GPU%20Performance%20for%20Machine%0A%20%20Learning-Based%20Track%20Reconstruction%20at%20LHCb&entry.906535625=Fotis%20I.%20Giasemis%20and%20Vladimir%20Lon%C4%8Dar%20and%20Bertrand%20Granado%20and%20Vladimir%20Vava%20Gligorov&entry.1292438233=%20%20In%20high-energy%20physics%2C%20the%20increasing%20luminosity%20and%20detector%20granularity%20at%0Athe%20Large%20Hadron%20Collider%20are%20driving%20the%20need%20for%20more%20efficient%20data%0Aprocessing%20solutions.%20Machine%20Learning%20has%20emerged%20as%20a%20promising%20tool%20for%0Areconstructing%20charged%20particle%20tracks%2C%20due%20to%20its%20potentially%20linear%0Acomputational%20scaling%20with%20detector%20hits.%20The%20recent%20implementation%20of%20a%20graph%0Aneural%20network-based%20track%20reconstruction%20pipeline%20in%20the%20first%20level%20trigger%0Aof%20the%20LHCb%20experiment%20on%20GPUs%20serves%20as%20a%20platform%20for%20comparative%20studies%0Abetween%20computational%20architectures%20in%20the%20context%20of%20high-energy%20physics.%20This%0Apaper%20presents%20a%20novel%20comparison%20of%20the%20throughput%20of%20ML%20model%20inference%0Abetween%20FPGAs%20and%20GPUs%2C%20focusing%20on%20the%20first%20step%20of%20the%20track%20reconstruction%0Apipeline%24%5Cunicode%7Bx2013%7D%24an%20implementation%20of%20a%20multilayer%20perceptron.%20Using%0AHLS4ML%20for%20FPGA%20deployment%2C%20we%20benchmark%20its%20performance%20against%20the%20GPU%0Aimplementation%20and%20demonstrate%20the%20potential%20of%20FPGAs%20for%20high-throughput%2C%0Alow-latency%20inference%20without%20the%20need%20for%20an%20expertise%20in%20FPGA%20development%20and%0Awhile%20consuming%20significantly%20less%20power.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02304v4&entry.124074799=Read"},
{"title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness", "author": "Junsheng Huang and Zhitao He and Sandeep Polisetty and Qingyun Wang and May Fung", "abstract": "  With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.\n", "link": "http://arxiv.org/abs/2504.21773v1", "date": "2025-04-30", "relevancy": 1.9765, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5103}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAC-Tuning%3A%20LLM%20Multi-Compositional%20Problem%20Reasoning%20with%20Enhanced%0A%20%20Knowledge%20Boundary%20Awareness&body=Title%3A%20MAC-Tuning%3A%20LLM%20Multi-Compositional%20Problem%20Reasoning%20with%20Enhanced%0A%20%20Knowledge%20Boundary%20Awareness%0AAuthor%3A%20Junsheng%20Huang%20and%20Zhitao%20He%20and%20Sandeep%20Polisetty%20and%20Qingyun%20Wang%20and%20May%20Fung%0AAbstract%3A%20%20%20With%20the%20widespread%20application%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20issue%20of%0Agenerating%20non-existing%20facts%2C%20known%20as%20hallucination%2C%20has%20garnered%20increasing%0Aattention.%20Previous%20research%20in%20enhancing%20LLM%20confidence%20estimation%20mainly%0Afocuses%20on%20the%20single%20problem%20setting.%20However%2C%20LLM%20awareness%20of%20its%20internal%0Aparameterized%20knowledge%20boundary%20under%20the%20more%20challenging%20multi-problem%0Asetting%2C%20which%20requires%20answering%20multiple%20problems%20accurately%20simultaneously%2C%0Aremains%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20method%2C%0AMultiple%20Answers%20and%20Confidence%20Stepwise%20Tuning%20%28MAC-Tuning%29%2C%20that%20separates%0Athe%20learning%20of%20answer%20prediction%20and%20confidence%20estimation%20during%20fine-tuning%0Aon%20instruction%20data.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20baselines%20by%20up%20to%2025%25%20in%20average%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAC-Tuning%253A%2520LLM%2520Multi-Compositional%2520Problem%2520Reasoning%2520with%2520Enhanced%250A%2520%2520Knowledge%2520Boundary%2520Awareness%26entry.906535625%3DJunsheng%2520Huang%2520and%2520Zhitao%2520He%2520and%2520Sandeep%2520Polisetty%2520and%2520Qingyun%2520Wang%2520and%2520May%2520Fung%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520application%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520issue%2520of%250Agenerating%2520non-existing%2520facts%252C%2520known%2520as%2520hallucination%252C%2520has%2520garnered%2520increasing%250Aattention.%2520Previous%2520research%2520in%2520enhancing%2520LLM%2520confidence%2520estimation%2520mainly%250Afocuses%2520on%2520the%2520single%2520problem%2520setting.%2520However%252C%2520LLM%2520awareness%2520of%2520its%2520internal%250Aparameterized%2520knowledge%2520boundary%2520under%2520the%2520more%2520challenging%2520multi-problem%250Asetting%252C%2520which%2520requires%2520answering%2520multiple%2520problems%2520accurately%2520simultaneously%252C%250Aremains%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520method%252C%250AMultiple%2520Answers%2520and%2520Confidence%2520Stepwise%2520Tuning%2520%2528MAC-Tuning%2529%252C%2520that%2520separates%250Athe%2520learning%2520of%2520answer%2520prediction%2520and%2520confidence%2520estimation%2520during%2520fine-tuning%250Aon%2520instruction%2520data.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520baselines%2520by%2520up%2520to%252025%2525%2520in%2520average%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAC-Tuning%3A%20LLM%20Multi-Compositional%20Problem%20Reasoning%20with%20Enhanced%0A%20%20Knowledge%20Boundary%20Awareness&entry.906535625=Junsheng%20Huang%20and%20Zhitao%20He%20and%20Sandeep%20Polisetty%20and%20Qingyun%20Wang%20and%20May%20Fung&entry.1292438233=%20%20With%20the%20widespread%20application%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20issue%20of%0Agenerating%20non-existing%20facts%2C%20known%20as%20hallucination%2C%20has%20garnered%20increasing%0Aattention.%20Previous%20research%20in%20enhancing%20LLM%20confidence%20estimation%20mainly%0Afocuses%20on%20the%20single%20problem%20setting.%20However%2C%20LLM%20awareness%20of%20its%20internal%0Aparameterized%20knowledge%20boundary%20under%20the%20more%20challenging%20multi-problem%0Asetting%2C%20which%20requires%20answering%20multiple%20problems%20accurately%20simultaneously%2C%0Aremains%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20method%2C%0AMultiple%20Answers%20and%20Confidence%20Stepwise%20Tuning%20%28MAC-Tuning%29%2C%20that%20separates%0Athe%20learning%20of%20answer%20prediction%20and%20confidence%20estimation%20during%20fine-tuning%0Aon%20instruction%20data.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20baselines%20by%20up%20to%2025%25%20in%20average%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21773v1&entry.124074799=Read"},
{"title": "Towards proactive self-adaptive AI for non-stationary environments with\n  dataset shifts", "author": "David Fern\u00e1ndez Narro and Pablo Ferri and Juan M. Garc\u00eda-G\u00f3mez and Carlos S\u00e1ez", "abstract": "  Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health.\n", "link": "http://arxiv.org/abs/2504.21565v1", "date": "2025-04-30", "relevancy": 1.965, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.492}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20proactive%20self-adaptive%20AI%20for%20non-stationary%20environments%20with%0A%20%20dataset%20shifts&body=Title%3A%20Towards%20proactive%20self-adaptive%20AI%20for%20non-stationary%20environments%20with%0A%20%20dataset%20shifts%0AAuthor%3A%20David%20Fern%C3%A1ndez%20Narro%20and%20Pablo%20Ferri%20and%20Juan%20M.%20Garc%C3%ADa-G%C3%B3mez%20and%20Carlos%20S%C3%A1ez%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20models%20deployed%20in%20production%20frequently%20face%0Achallenges%20in%20maintaining%20their%20performance%20in%20non-stationary%20environments.%0AThis%20issue%20is%20particularly%20noticeable%20in%20medical%20settings%2C%20where%20temporal%0Adataset%20shifts%20often%20occur.%20These%20shifts%20arise%20when%20the%20distributions%20of%0Atraining%20data%20differ%20from%20those%20of%20the%20data%20encountered%20during%20deployment%20over%0Atime.%20Further%2C%20new%20labeled%20data%20to%20continuously%20retrain%20AI%20is%20not%20typically%0Aavailable%20in%20a%20timely%20manner%20due%20to%20data%20access%20limitations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20proactive%20self-adaptive%20AI%20approach%2C%20or%20pro-adaptive%2C%0Awhere%20we%20model%20the%20temporal%20trajectory%20of%20AI%20parameters%2C%20allowing%20us%20to%0Ashort-term%20forecast%20parameter%20values.%20To%20this%20end%2C%20we%20use%20polynomial%20spline%0Abases%2C%20within%20an%20extensible%20Functional%20Data%20Analysis%20framework.%20We%20validate%20our%0Amethodology%20with%20a%20logistic%20regression%20model%20addressing%20prior%20probability%0Ashift%2C%20covariate%20shift%2C%20and%20concept%20shift.%20This%20validation%20is%20conducted%20on%20both%0Aa%20controlled%20simulated%20dataset%20and%20a%20publicly%20available%20real-world%20COVID-19%0Adataset%20from%20Mexico%2C%20with%20various%20shifts%20occurring%20between%202020%20and%202024.%20Our%0Aresults%20indicate%20that%20this%20approach%20enhances%20the%20performance%20of%20AI%20against%0Ashifts%20compared%20to%20baseline%20stable%20models%20trained%20at%20different%20time%20distances%0Afrom%20the%20present%2C%20without%20requiring%20updated%20training%20data.%20This%20work%20lays%20the%0Afoundation%20for%20pro-adaptive%20AI%20research%20against%20dynamic%2C%20non-stationary%0Aenvironments%2C%20being%20compatible%20with%20data%20protection%2C%20in%20resilient%20AI%20production%0Aenvironments%20for%20health.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520proactive%2520self-adaptive%2520AI%2520for%2520non-stationary%2520environments%2520with%250A%2520%2520dataset%2520shifts%26entry.906535625%3DDavid%2520Fern%25C3%25A1ndez%2520Narro%2520and%2520Pablo%2520Ferri%2520and%2520Juan%2520M.%2520Garc%25C3%25ADa-G%25C3%25B3mez%2520and%2520Carlos%2520S%25C3%25A1ez%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520models%2520deployed%2520in%2520production%2520frequently%2520face%250Achallenges%2520in%2520maintaining%2520their%2520performance%2520in%2520non-stationary%2520environments.%250AThis%2520issue%2520is%2520particularly%2520noticeable%2520in%2520medical%2520settings%252C%2520where%2520temporal%250Adataset%2520shifts%2520often%2520occur.%2520These%2520shifts%2520arise%2520when%2520the%2520distributions%2520of%250Atraining%2520data%2520differ%2520from%2520those%2520of%2520the%2520data%2520encountered%2520during%2520deployment%2520over%250Atime.%2520Further%252C%2520new%2520labeled%2520data%2520to%2520continuously%2520retrain%2520AI%2520is%2520not%2520typically%250Aavailable%2520in%2520a%2520timely%2520manner%2520due%2520to%2520data%2520access%2520limitations.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520proactive%2520self-adaptive%2520AI%2520approach%252C%2520or%2520pro-adaptive%252C%250Awhere%2520we%2520model%2520the%2520temporal%2520trajectory%2520of%2520AI%2520parameters%252C%2520allowing%2520us%2520to%250Ashort-term%2520forecast%2520parameter%2520values.%2520To%2520this%2520end%252C%2520we%2520use%2520polynomial%2520spline%250Abases%252C%2520within%2520an%2520extensible%2520Functional%2520Data%2520Analysis%2520framework.%2520We%2520validate%2520our%250Amethodology%2520with%2520a%2520logistic%2520regression%2520model%2520addressing%2520prior%2520probability%250Ashift%252C%2520covariate%2520shift%252C%2520and%2520concept%2520shift.%2520This%2520validation%2520is%2520conducted%2520on%2520both%250Aa%2520controlled%2520simulated%2520dataset%2520and%2520a%2520publicly%2520available%2520real-world%2520COVID-19%250Adataset%2520from%2520Mexico%252C%2520with%2520various%2520shifts%2520occurring%2520between%25202020%2520and%25202024.%2520Our%250Aresults%2520indicate%2520that%2520this%2520approach%2520enhances%2520the%2520performance%2520of%2520AI%2520against%250Ashifts%2520compared%2520to%2520baseline%2520stable%2520models%2520trained%2520at%2520different%2520time%2520distances%250Afrom%2520the%2520present%252C%2520without%2520requiring%2520updated%2520training%2520data.%2520This%2520work%2520lays%2520the%250Afoundation%2520for%2520pro-adaptive%2520AI%2520research%2520against%2520dynamic%252C%2520non-stationary%250Aenvironments%252C%2520being%2520compatible%2520with%2520data%2520protection%252C%2520in%2520resilient%2520AI%2520production%250Aenvironments%2520for%2520health.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20proactive%20self-adaptive%20AI%20for%20non-stationary%20environments%20with%0A%20%20dataset%20shifts&entry.906535625=David%20Fern%C3%A1ndez%20Narro%20and%20Pablo%20Ferri%20and%20Juan%20M.%20Garc%C3%ADa-G%C3%B3mez%20and%20Carlos%20S%C3%A1ez&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20models%20deployed%20in%20production%20frequently%20face%0Achallenges%20in%20maintaining%20their%20performance%20in%20non-stationary%20environments.%0AThis%20issue%20is%20particularly%20noticeable%20in%20medical%20settings%2C%20where%20temporal%0Adataset%20shifts%20often%20occur.%20These%20shifts%20arise%20when%20the%20distributions%20of%0Atraining%20data%20differ%20from%20those%20of%20the%20data%20encountered%20during%20deployment%20over%0Atime.%20Further%2C%20new%20labeled%20data%20to%20continuously%20retrain%20AI%20is%20not%20typically%0Aavailable%20in%20a%20timely%20manner%20due%20to%20data%20access%20limitations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20proactive%20self-adaptive%20AI%20approach%2C%20or%20pro-adaptive%2C%0Awhere%20we%20model%20the%20temporal%20trajectory%20of%20AI%20parameters%2C%20allowing%20us%20to%0Ashort-term%20forecast%20parameter%20values.%20To%20this%20end%2C%20we%20use%20polynomial%20spline%0Abases%2C%20within%20an%20extensible%20Functional%20Data%20Analysis%20framework.%20We%20validate%20our%0Amethodology%20with%20a%20logistic%20regression%20model%20addressing%20prior%20probability%0Ashift%2C%20covariate%20shift%2C%20and%20concept%20shift.%20This%20validation%20is%20conducted%20on%20both%0Aa%20controlled%20simulated%20dataset%20and%20a%20publicly%20available%20real-world%20COVID-19%0Adataset%20from%20Mexico%2C%20with%20various%20shifts%20occurring%20between%202020%20and%202024.%20Our%0Aresults%20indicate%20that%20this%20approach%20enhances%20the%20performance%20of%20AI%20against%0Ashifts%20compared%20to%20baseline%20stable%20models%20trained%20at%20different%20time%20distances%0Afrom%20the%20present%2C%20without%20requiring%20updated%20training%20data.%20This%20work%20lays%20the%0Afoundation%20for%20pro-adaptive%20AI%20research%20against%20dynamic%2C%20non-stationary%0Aenvironments%2C%20being%20compatible%20with%20data%20protection%2C%20in%20resilient%20AI%20production%0Aenvironments%20for%20health.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21565v1&entry.124074799=Read"},
{"title": "Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised\n  Models", "author": "Zhichen Han and Tianqi Geng and Hui Feng and Jiahong Yuan and Korin Richmond and Yuanchao Li", "abstract": "  Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception.\n", "link": "http://arxiv.org/abs/2409.16920v2", "date": "2025-04-30", "relevancy": 1.9576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Lingual%20Speech%20Emotion%20Recognition%3A%20Humans%20vs.%20Self-Supervised%0A%20%20Models&body=Title%3A%20Cross-Lingual%20Speech%20Emotion%20Recognition%3A%20Humans%20vs.%20Self-Supervised%0A%20%20Models%0AAuthor%3A%20Zhichen%20Han%20and%20Tianqi%20Geng%20and%20Hui%20Feng%20and%20Jiahong%20Yuan%20and%20Korin%20Richmond%20and%20Yuanchao%20Li%0AAbstract%3A%20%20%20Utilizing%20Self-Supervised%20Learning%20%28SSL%29%20models%20for%20Speech%20Emotion%0ARecognition%20%28SER%29%20has%20proven%20effective%2C%20yet%20limited%20research%20has%20explored%0Across-lingual%20scenarios.%20This%20study%20presents%20a%20comparative%20analysis%20between%0Ahuman%20performance%20and%20SSL%20models%2C%20beginning%20with%20a%20layer-wise%20analysis%20and%20an%0Aexploration%20of%20parameter-efficient%20fine-tuning%20strategies%20in%20monolingual%2C%0Across-lingual%2C%20and%20transfer%20learning%20contexts.%20We%20further%20compare%20the%20SER%0Aability%20of%20models%20and%20humans%20at%20both%20utterance-%20and%20segment-levels.%0AAdditionally%2C%20we%20investigate%20the%20impact%20of%20dialect%20on%20cross-lingual%20SER%20through%0Ahuman%20evaluation.%20Our%20findings%20reveal%20that%20models%2C%20with%20appropriate%20knowledge%0Atransfer%2C%20can%20adapt%20to%20the%20target%20language%20and%20achieve%20performance%20comparable%0Ato%20native%20speakers.%20We%20also%20demonstrate%20the%20significant%20effect%20of%20dialect%20on%0ASER%20for%20individuals%20without%20prior%20linguistic%20and%20paralinguistic%20background.%0AMoreover%2C%20both%20humans%20and%20models%20exhibit%20distinct%20behaviors%20across%20different%0Aemotions.%20These%20results%20offer%20new%20insights%20into%20the%20cross-lingual%20SER%0Acapabilities%20of%20SSL%20models%2C%20underscoring%20both%20their%20similarities%20to%20and%0Adifferences%20from%20human%20emotion%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Lingual%2520Speech%2520Emotion%2520Recognition%253A%2520Humans%2520vs.%2520Self-Supervised%250A%2520%2520Models%26entry.906535625%3DZhichen%2520Han%2520and%2520Tianqi%2520Geng%2520and%2520Hui%2520Feng%2520and%2520Jiahong%2520Yuan%2520and%2520Korin%2520Richmond%2520and%2520Yuanchao%2520Li%26entry.1292438233%3D%2520%2520Utilizing%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520models%2520for%2520Speech%2520Emotion%250ARecognition%2520%2528SER%2529%2520has%2520proven%2520effective%252C%2520yet%2520limited%2520research%2520has%2520explored%250Across-lingual%2520scenarios.%2520This%2520study%2520presents%2520a%2520comparative%2520analysis%2520between%250Ahuman%2520performance%2520and%2520SSL%2520models%252C%2520beginning%2520with%2520a%2520layer-wise%2520analysis%2520and%2520an%250Aexploration%2520of%2520parameter-efficient%2520fine-tuning%2520strategies%2520in%2520monolingual%252C%250Across-lingual%252C%2520and%2520transfer%2520learning%2520contexts.%2520We%2520further%2520compare%2520the%2520SER%250Aability%2520of%2520models%2520and%2520humans%2520at%2520both%2520utterance-%2520and%2520segment-levels.%250AAdditionally%252C%2520we%2520investigate%2520the%2520impact%2520of%2520dialect%2520on%2520cross-lingual%2520SER%2520through%250Ahuman%2520evaluation.%2520Our%2520findings%2520reveal%2520that%2520models%252C%2520with%2520appropriate%2520knowledge%250Atransfer%252C%2520can%2520adapt%2520to%2520the%2520target%2520language%2520and%2520achieve%2520performance%2520comparable%250Ato%2520native%2520speakers.%2520We%2520also%2520demonstrate%2520the%2520significant%2520effect%2520of%2520dialect%2520on%250ASER%2520for%2520individuals%2520without%2520prior%2520linguistic%2520and%2520paralinguistic%2520background.%250AMoreover%252C%2520both%2520humans%2520and%2520models%2520exhibit%2520distinct%2520behaviors%2520across%2520different%250Aemotions.%2520These%2520results%2520offer%2520new%2520insights%2520into%2520the%2520cross-lingual%2520SER%250Acapabilities%2520of%2520SSL%2520models%252C%2520underscoring%2520both%2520their%2520similarities%2520to%2520and%250Adifferences%2520from%2520human%2520emotion%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Lingual%20Speech%20Emotion%20Recognition%3A%20Humans%20vs.%20Self-Supervised%0A%20%20Models&entry.906535625=Zhichen%20Han%20and%20Tianqi%20Geng%20and%20Hui%20Feng%20and%20Jiahong%20Yuan%20and%20Korin%20Richmond%20and%20Yuanchao%20Li&entry.1292438233=%20%20Utilizing%20Self-Supervised%20Learning%20%28SSL%29%20models%20for%20Speech%20Emotion%0ARecognition%20%28SER%29%20has%20proven%20effective%2C%20yet%20limited%20research%20has%20explored%0Across-lingual%20scenarios.%20This%20study%20presents%20a%20comparative%20analysis%20between%0Ahuman%20performance%20and%20SSL%20models%2C%20beginning%20with%20a%20layer-wise%20analysis%20and%20an%0Aexploration%20of%20parameter-efficient%20fine-tuning%20strategies%20in%20monolingual%2C%0Across-lingual%2C%20and%20transfer%20learning%20contexts.%20We%20further%20compare%20the%20SER%0Aability%20of%20models%20and%20humans%20at%20both%20utterance-%20and%20segment-levels.%0AAdditionally%2C%20we%20investigate%20the%20impact%20of%20dialect%20on%20cross-lingual%20SER%20through%0Ahuman%20evaluation.%20Our%20findings%20reveal%20that%20models%2C%20with%20appropriate%20knowledge%0Atransfer%2C%20can%20adapt%20to%20the%20target%20language%20and%20achieve%20performance%20comparable%0Ato%20native%20speakers.%20We%20also%20demonstrate%20the%20significant%20effect%20of%20dialect%20on%0ASER%20for%20individuals%20without%20prior%20linguistic%20and%20paralinguistic%20background.%0AMoreover%2C%20both%20humans%20and%20models%20exhibit%20distinct%20behaviors%20across%20different%0Aemotions.%20These%20results%20offer%20new%20insights%20into%20the%20cross-lingual%20SER%0Acapabilities%20of%20SSL%20models%2C%20underscoring%20both%20their%20similarities%20to%20and%0Adifferences%20from%20human%20emotion%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16920v2&entry.124074799=Read"},
{"title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization", "author": "Haotian Luo and Haiying He and Yibo Wang and Jinluan Yang and Rui Liu and Naiqiang Tan and Xiaochun Cao and Dacheng Tao and Li Shen", "abstract": "  Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1\n", "link": "http://arxiv.org/abs/2504.21659v1", "date": "2025-04-30", "relevancy": 1.946, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaR1%3A%20From%20Long-CoT%20to%20Hybrid-CoT%20via%20Bi-Level%20Adaptive%20Reasoning%0A%20%20Optimization&body=Title%3A%20AdaR1%3A%20From%20Long-CoT%20to%20Hybrid-CoT%20via%20Bi-Level%20Adaptive%20Reasoning%0A%20%20Optimization%0AAuthor%3A%20Haotian%20Luo%20and%20Haiying%20He%20and%20Yibo%20Wang%20and%20Jinluan%20Yang%20and%20Rui%20Liu%20and%20Naiqiang%20Tan%20and%20Xiaochun%20Cao%20and%20Dacheng%20Tao%20and%20Li%20Shen%0AAbstract%3A%20%20%20Recently%2C%20long-thought%20reasoning%20models%20achieve%20strong%20performance%20on%20complex%0Areasoning%20tasks%2C%20but%20often%20incur%20substantial%20inference%20overhead%2C%20making%0Aefficiency%20a%20critical%20concern.%20Our%20empirical%20analysis%20reveals%20that%20the%20benefit%0Aof%20using%20Long-CoT%20varies%20across%20problems%3A%20while%20some%20problems%20require%20elaborate%0Areasoning%2C%20others%20show%20no%20improvement%2C%20or%20even%20degraded%20accuracy.%20This%0Amotivates%20adaptive%20reasoning%20strategies%20that%20tailor%20reasoning%20depth%20to%20the%0Ainput.%20However%2C%20prior%20work%20primarily%20reduces%20redundancy%20within%20long%20reasoning%0Apaths%2C%20limiting%20exploration%20of%20more%20efficient%20strategies%20beyond%20the%20Long-CoT%0Aparadigm.%20To%20address%20this%2C%20we%20propose%20a%20novel%20two-stage%20framework%20for%20adaptive%0Aand%20efficient%20reasoning.%20First%2C%20we%20construct%20a%20hybrid%20reasoning%20model%20by%0Amerging%20long%20and%20short%20CoT%20models%20to%20enable%20diverse%20reasoning%20styles.%20Second%2C%0Awe%20apply%20bi-level%20preference%20training%20to%20guide%20the%20model%20to%20select%20suitable%0Areasoning%20styles%20%28group-level%29%2C%20and%20prefer%20concise%20and%20correct%20reasoning%20within%0Aeach%20style%20group%20%28instance-level%29.%20Experiments%20demonstrate%20that%20our%20method%0Asignificantly%20reduces%20inference%20costs%20compared%20to%20other%20baseline%20approaches%2C%0Awhile%20maintaining%20performance.%20Notably%2C%20on%20five%20mathematical%20datasets%2C%20the%0Aaverage%20length%20of%20reasoning%20is%20reduced%20by%20more%20than%2050%25%2C%20highlighting%20the%0Apotential%20of%20adaptive%20strategies%20to%20optimize%20reasoning%20efficiency%20in%20large%0Alanguage%20models.%20Our%20code%20is%20coming%20soon%20at%20https%3A//github.com/StarDewXXX/AdaR1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaR1%253A%2520From%2520Long-CoT%2520to%2520Hybrid-CoT%2520via%2520Bi-Level%2520Adaptive%2520Reasoning%250A%2520%2520Optimization%26entry.906535625%3DHaotian%2520Luo%2520and%2520Haiying%2520He%2520and%2520Yibo%2520Wang%2520and%2520Jinluan%2520Yang%2520and%2520Rui%2520Liu%2520and%2520Naiqiang%2520Tan%2520and%2520Xiaochun%2520Cao%2520and%2520Dacheng%2520Tao%2520and%2520Li%2520Shen%26entry.1292438233%3D%2520%2520Recently%252C%2520long-thought%2520reasoning%2520models%2520achieve%2520strong%2520performance%2520on%2520complex%250Areasoning%2520tasks%252C%2520but%2520often%2520incur%2520substantial%2520inference%2520overhead%252C%2520making%250Aefficiency%2520a%2520critical%2520concern.%2520Our%2520empirical%2520analysis%2520reveals%2520that%2520the%2520benefit%250Aof%2520using%2520Long-CoT%2520varies%2520across%2520problems%253A%2520while%2520some%2520problems%2520require%2520elaborate%250Areasoning%252C%2520others%2520show%2520no%2520improvement%252C%2520or%2520even%2520degraded%2520accuracy.%2520This%250Amotivates%2520adaptive%2520reasoning%2520strategies%2520that%2520tailor%2520reasoning%2520depth%2520to%2520the%250Ainput.%2520However%252C%2520prior%2520work%2520primarily%2520reduces%2520redundancy%2520within%2520long%2520reasoning%250Apaths%252C%2520limiting%2520exploration%2520of%2520more%2520efficient%2520strategies%2520beyond%2520the%2520Long-CoT%250Aparadigm.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520two-stage%2520framework%2520for%2520adaptive%250Aand%2520efficient%2520reasoning.%2520First%252C%2520we%2520construct%2520a%2520hybrid%2520reasoning%2520model%2520by%250Amerging%2520long%2520and%2520short%2520CoT%2520models%2520to%2520enable%2520diverse%2520reasoning%2520styles.%2520Second%252C%250Awe%2520apply%2520bi-level%2520preference%2520training%2520to%2520guide%2520the%2520model%2520to%2520select%2520suitable%250Areasoning%2520styles%2520%2528group-level%2529%252C%2520and%2520prefer%2520concise%2520and%2520correct%2520reasoning%2520within%250Aeach%2520style%2520group%2520%2528instance-level%2529.%2520Experiments%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520reduces%2520inference%2520costs%2520compared%2520to%2520other%2520baseline%2520approaches%252C%250Awhile%2520maintaining%2520performance.%2520Notably%252C%2520on%2520five%2520mathematical%2520datasets%252C%2520the%250Aaverage%2520length%2520of%2520reasoning%2520is%2520reduced%2520by%2520more%2520than%252050%2525%252C%2520highlighting%2520the%250Apotential%2520of%2520adaptive%2520strategies%2520to%2520optimize%2520reasoning%2520efficiency%2520in%2520large%250Alanguage%2520models.%2520Our%2520code%2520is%2520coming%2520soon%2520at%2520https%253A//github.com/StarDewXXX/AdaR1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaR1%3A%20From%20Long-CoT%20to%20Hybrid-CoT%20via%20Bi-Level%20Adaptive%20Reasoning%0A%20%20Optimization&entry.906535625=Haotian%20Luo%20and%20Haiying%20He%20and%20Yibo%20Wang%20and%20Jinluan%20Yang%20and%20Rui%20Liu%20and%20Naiqiang%20Tan%20and%20Xiaochun%20Cao%20and%20Dacheng%20Tao%20and%20Li%20Shen&entry.1292438233=%20%20Recently%2C%20long-thought%20reasoning%20models%20achieve%20strong%20performance%20on%20complex%0Areasoning%20tasks%2C%20but%20often%20incur%20substantial%20inference%20overhead%2C%20making%0Aefficiency%20a%20critical%20concern.%20Our%20empirical%20analysis%20reveals%20that%20the%20benefit%0Aof%20using%20Long-CoT%20varies%20across%20problems%3A%20while%20some%20problems%20require%20elaborate%0Areasoning%2C%20others%20show%20no%20improvement%2C%20or%20even%20degraded%20accuracy.%20This%0Amotivates%20adaptive%20reasoning%20strategies%20that%20tailor%20reasoning%20depth%20to%20the%0Ainput.%20However%2C%20prior%20work%20primarily%20reduces%20redundancy%20within%20long%20reasoning%0Apaths%2C%20limiting%20exploration%20of%20more%20efficient%20strategies%20beyond%20the%20Long-CoT%0Aparadigm.%20To%20address%20this%2C%20we%20propose%20a%20novel%20two-stage%20framework%20for%20adaptive%0Aand%20efficient%20reasoning.%20First%2C%20we%20construct%20a%20hybrid%20reasoning%20model%20by%0Amerging%20long%20and%20short%20CoT%20models%20to%20enable%20diverse%20reasoning%20styles.%20Second%2C%0Awe%20apply%20bi-level%20preference%20training%20to%20guide%20the%20model%20to%20select%20suitable%0Areasoning%20styles%20%28group-level%29%2C%20and%20prefer%20concise%20and%20correct%20reasoning%20within%0Aeach%20style%20group%20%28instance-level%29.%20Experiments%20demonstrate%20that%20our%20method%0Asignificantly%20reduces%20inference%20costs%20compared%20to%20other%20baseline%20approaches%2C%0Awhile%20maintaining%20performance.%20Notably%2C%20on%20five%20mathematical%20datasets%2C%20the%0Aaverage%20length%20of%20reasoning%20is%20reduced%20by%20more%20than%2050%25%2C%20highlighting%20the%0Apotential%20of%20adaptive%20strategies%20to%20optimize%20reasoning%20efficiency%20in%20large%0Alanguage%20models.%20Our%20code%20is%20coming%20soon%20at%20https%3A//github.com/StarDewXXX/AdaR1%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21659v1&entry.124074799=Read"},
{"title": "Preference-centric Bandits: Optimality of Mixtures and Regret-efficient\n  Algorithms", "author": "Meltem Tatl\u0131 and Arpan Mukherjee and Prashanth L. A. and Karthikeyan Shanmugam and Ali Tajer", "abstract": "  The objective of canonical multi-armed bandits is to identify and repeatedly\nselect an arm with the largest reward, often in the form of the expected value\nof the arm's probability distribution. Such a utilitarian perspective and focus\non the probability models' first moments, however, is agnostic to the\ndistributions' tail behavior and their implications for variability and risks\nin decision-making. This paper introduces a principled framework for shifting\nfrom expectation-based evaluation to an alternative reward formulation, termed\na preference metric (PM). The PMs can place the desired emphasis on different\nreward realization and can encode a richer modeling of preferences that\nincorporate risk aversion, robustness, or other desired attitudes toward\nuncertainty. A fundamentally distinct observation in such a PM-centric\nperspective is that designing bandit algorithms will have a significantly\ndifferent principle: as opposed to the reward-based models in which the optimal\nsampling policy converges to repeatedly sampling from the single best arm, in\nthe PM-centric framework the optimal policy converges to selecting a mix of\narms based on specific mixing weights. Designing such mixture policies departs\nfrom the principles for designing bandit algorithms in significant ways,\nprimarily because of uncountable mixture possibilities. The paper formalizes\nthe PM-centric framework and presents two algorithm classes (horizon-dependent\nand anytime) that learn and track mixtures in a regret-efficient fashion. These\nalgorithms have two distinctions from their canonical counterparts: (i) they\ninvolve an estimation routine to form reliable estimates of optimal mixtures,\nand (ii) they are equipped with tracking mechanisms to navigate arm selection\nfractions to track the optimal mixtures. These algorithms' regret guarantees\nare investigated under various algebraic forms of the PMs.\n", "link": "http://arxiv.org/abs/2504.20877v2", "date": "2025-04-30", "relevancy": 1.9422, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5299}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4915}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preference-centric%20Bandits%3A%20Optimality%20of%20Mixtures%20and%20Regret-efficient%0A%20%20Algorithms&body=Title%3A%20Preference-centric%20Bandits%3A%20Optimality%20of%20Mixtures%20and%20Regret-efficient%0A%20%20Algorithms%0AAuthor%3A%20Meltem%20Tatl%C4%B1%20and%20Arpan%20Mukherjee%20and%20Prashanth%20L.%20A.%20and%20Karthikeyan%20Shanmugam%20and%20Ali%20Tajer%0AAbstract%3A%20%20%20The%20objective%20of%20canonical%20multi-armed%20bandits%20is%20to%20identify%20and%20repeatedly%0Aselect%20an%20arm%20with%20the%20largest%20reward%2C%20often%20in%20the%20form%20of%20the%20expected%20value%0Aof%20the%20arm%27s%20probability%20distribution.%20Such%20a%20utilitarian%20perspective%20and%20focus%0Aon%20the%20probability%20models%27%20first%20moments%2C%20however%2C%20is%20agnostic%20to%20the%0Adistributions%27%20tail%20behavior%20and%20their%20implications%20for%20variability%20and%20risks%0Ain%20decision-making.%20This%20paper%20introduces%20a%20principled%20framework%20for%20shifting%0Afrom%20expectation-based%20evaluation%20to%20an%20alternative%20reward%20formulation%2C%20termed%0Aa%20preference%20metric%20%28PM%29.%20The%20PMs%20can%20place%20the%20desired%20emphasis%20on%20different%0Areward%20realization%20and%20can%20encode%20a%20richer%20modeling%20of%20preferences%20that%0Aincorporate%20risk%20aversion%2C%20robustness%2C%20or%20other%20desired%20attitudes%20toward%0Auncertainty.%20A%20fundamentally%20distinct%20observation%20in%20such%20a%20PM-centric%0Aperspective%20is%20that%20designing%20bandit%20algorithms%20will%20have%20a%20significantly%0Adifferent%20principle%3A%20as%20opposed%20to%20the%20reward-based%20models%20in%20which%20the%20optimal%0Asampling%20policy%20converges%20to%20repeatedly%20sampling%20from%20the%20single%20best%20arm%2C%20in%0Athe%20PM-centric%20framework%20the%20optimal%20policy%20converges%20to%20selecting%20a%20mix%20of%0Aarms%20based%20on%20specific%20mixing%20weights.%20Designing%20such%20mixture%20policies%20departs%0Afrom%20the%20principles%20for%20designing%20bandit%20algorithms%20in%20significant%20ways%2C%0Aprimarily%20because%20of%20uncountable%20mixture%20possibilities.%20The%20paper%20formalizes%0Athe%20PM-centric%20framework%20and%20presents%20two%20algorithm%20classes%20%28horizon-dependent%0Aand%20anytime%29%20that%20learn%20and%20track%20mixtures%20in%20a%20regret-efficient%20fashion.%20These%0Aalgorithms%20have%20two%20distinctions%20from%20their%20canonical%20counterparts%3A%20%28i%29%20they%0Ainvolve%20an%20estimation%20routine%20to%20form%20reliable%20estimates%20of%20optimal%20mixtures%2C%0Aand%20%28ii%29%20they%20are%20equipped%20with%20tracking%20mechanisms%20to%20navigate%20arm%20selection%0Afractions%20to%20track%20the%20optimal%20mixtures.%20These%20algorithms%27%20regret%20guarantees%0Aare%20investigated%20under%20various%20algebraic%20forms%20of%20the%20PMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreference-centric%2520Bandits%253A%2520Optimality%2520of%2520Mixtures%2520and%2520Regret-efficient%250A%2520%2520Algorithms%26entry.906535625%3DMeltem%2520Tatl%25C4%25B1%2520and%2520Arpan%2520Mukherjee%2520and%2520Prashanth%2520L.%2520A.%2520and%2520Karthikeyan%2520Shanmugam%2520and%2520Ali%2520Tajer%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520canonical%2520multi-armed%2520bandits%2520is%2520to%2520identify%2520and%2520repeatedly%250Aselect%2520an%2520arm%2520with%2520the%2520largest%2520reward%252C%2520often%2520in%2520the%2520form%2520of%2520the%2520expected%2520value%250Aof%2520the%2520arm%2527s%2520probability%2520distribution.%2520Such%2520a%2520utilitarian%2520perspective%2520and%2520focus%250Aon%2520the%2520probability%2520models%2527%2520first%2520moments%252C%2520however%252C%2520is%2520agnostic%2520to%2520the%250Adistributions%2527%2520tail%2520behavior%2520and%2520their%2520implications%2520for%2520variability%2520and%2520risks%250Ain%2520decision-making.%2520This%2520paper%2520introduces%2520a%2520principled%2520framework%2520for%2520shifting%250Afrom%2520expectation-based%2520evaluation%2520to%2520an%2520alternative%2520reward%2520formulation%252C%2520termed%250Aa%2520preference%2520metric%2520%2528PM%2529.%2520The%2520PMs%2520can%2520place%2520the%2520desired%2520emphasis%2520on%2520different%250Areward%2520realization%2520and%2520can%2520encode%2520a%2520richer%2520modeling%2520of%2520preferences%2520that%250Aincorporate%2520risk%2520aversion%252C%2520robustness%252C%2520or%2520other%2520desired%2520attitudes%2520toward%250Auncertainty.%2520A%2520fundamentally%2520distinct%2520observation%2520in%2520such%2520a%2520PM-centric%250Aperspective%2520is%2520that%2520designing%2520bandit%2520algorithms%2520will%2520have%2520a%2520significantly%250Adifferent%2520principle%253A%2520as%2520opposed%2520to%2520the%2520reward-based%2520models%2520in%2520which%2520the%2520optimal%250Asampling%2520policy%2520converges%2520to%2520repeatedly%2520sampling%2520from%2520the%2520single%2520best%2520arm%252C%2520in%250Athe%2520PM-centric%2520framework%2520the%2520optimal%2520policy%2520converges%2520to%2520selecting%2520a%2520mix%2520of%250Aarms%2520based%2520on%2520specific%2520mixing%2520weights.%2520Designing%2520such%2520mixture%2520policies%2520departs%250Afrom%2520the%2520principles%2520for%2520designing%2520bandit%2520algorithms%2520in%2520significant%2520ways%252C%250Aprimarily%2520because%2520of%2520uncountable%2520mixture%2520possibilities.%2520The%2520paper%2520formalizes%250Athe%2520PM-centric%2520framework%2520and%2520presents%2520two%2520algorithm%2520classes%2520%2528horizon-dependent%250Aand%2520anytime%2529%2520that%2520learn%2520and%2520track%2520mixtures%2520in%2520a%2520regret-efficient%2520fashion.%2520These%250Aalgorithms%2520have%2520two%2520distinctions%2520from%2520their%2520canonical%2520counterparts%253A%2520%2528i%2529%2520they%250Ainvolve%2520an%2520estimation%2520routine%2520to%2520form%2520reliable%2520estimates%2520of%2520optimal%2520mixtures%252C%250Aand%2520%2528ii%2529%2520they%2520are%2520equipped%2520with%2520tracking%2520mechanisms%2520to%2520navigate%2520arm%2520selection%250Afractions%2520to%2520track%2520the%2520optimal%2520mixtures.%2520These%2520algorithms%2527%2520regret%2520guarantees%250Aare%2520investigated%2520under%2520various%2520algebraic%2520forms%2520of%2520the%2520PMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preference-centric%20Bandits%3A%20Optimality%20of%20Mixtures%20and%20Regret-efficient%0A%20%20Algorithms&entry.906535625=Meltem%20Tatl%C4%B1%20and%20Arpan%20Mukherjee%20and%20Prashanth%20L.%20A.%20and%20Karthikeyan%20Shanmugam%20and%20Ali%20Tajer&entry.1292438233=%20%20The%20objective%20of%20canonical%20multi-armed%20bandits%20is%20to%20identify%20and%20repeatedly%0Aselect%20an%20arm%20with%20the%20largest%20reward%2C%20often%20in%20the%20form%20of%20the%20expected%20value%0Aof%20the%20arm%27s%20probability%20distribution.%20Such%20a%20utilitarian%20perspective%20and%20focus%0Aon%20the%20probability%20models%27%20first%20moments%2C%20however%2C%20is%20agnostic%20to%20the%0Adistributions%27%20tail%20behavior%20and%20their%20implications%20for%20variability%20and%20risks%0Ain%20decision-making.%20This%20paper%20introduces%20a%20principled%20framework%20for%20shifting%0Afrom%20expectation-based%20evaluation%20to%20an%20alternative%20reward%20formulation%2C%20termed%0Aa%20preference%20metric%20%28PM%29.%20The%20PMs%20can%20place%20the%20desired%20emphasis%20on%20different%0Areward%20realization%20and%20can%20encode%20a%20richer%20modeling%20of%20preferences%20that%0Aincorporate%20risk%20aversion%2C%20robustness%2C%20or%20other%20desired%20attitudes%20toward%0Auncertainty.%20A%20fundamentally%20distinct%20observation%20in%20such%20a%20PM-centric%0Aperspective%20is%20that%20designing%20bandit%20algorithms%20will%20have%20a%20significantly%0Adifferent%20principle%3A%20as%20opposed%20to%20the%20reward-based%20models%20in%20which%20the%20optimal%0Asampling%20policy%20converges%20to%20repeatedly%20sampling%20from%20the%20single%20best%20arm%2C%20in%0Athe%20PM-centric%20framework%20the%20optimal%20policy%20converges%20to%20selecting%20a%20mix%20of%0Aarms%20based%20on%20specific%20mixing%20weights.%20Designing%20such%20mixture%20policies%20departs%0Afrom%20the%20principles%20for%20designing%20bandit%20algorithms%20in%20significant%20ways%2C%0Aprimarily%20because%20of%20uncountable%20mixture%20possibilities.%20The%20paper%20formalizes%0Athe%20PM-centric%20framework%20and%20presents%20two%20algorithm%20classes%20%28horizon-dependent%0Aand%20anytime%29%20that%20learn%20and%20track%20mixtures%20in%20a%20regret-efficient%20fashion.%20These%0Aalgorithms%20have%20two%20distinctions%20from%20their%20canonical%20counterparts%3A%20%28i%29%20they%0Ainvolve%20an%20estimation%20routine%20to%20form%20reliable%20estimates%20of%20optimal%20mixtures%2C%0Aand%20%28ii%29%20they%20are%20equipped%20with%20tracking%20mechanisms%20to%20navigate%20arm%20selection%0Afractions%20to%20track%20the%20optimal%20mixtures.%20These%20algorithms%27%20regret%20guarantees%0Aare%20investigated%20under%20various%20algebraic%20forms%20of%20the%20PMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20877v2&entry.124074799=Read"},
{"title": "Hexcute: A Tile-based Programming Language with Automatic Layout and\n  Task-Mapping Synthesis", "author": "Xiao Zhang and Yaoyao Ding and Yang Hu and Gennady Pekhimenko", "abstract": "  Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL\nquantization techniques demand a new matrix multiplication operator with mixed\ninput data types, further complicating GPU optimization. Prior high-level\ncompilers like Triton lack the expressiveness to implement key optimizations\nlike fine-grained data pipelines and hardware-friendly memory layouts for these\noperators, while low-level programming models, such as Hidet, Graphene, and\nCUTLASS, require significant programming efforts. To balance expressiveness\nwith engineering effort, we propose Hexcute, a tile-based programming language\nthat exposes shared memory and register abstractions to enable fine-grained\noptimization for these operators. Additionally, Hexcute leverages task mapping\nto schedule the GPU program, and to reduce programming efforts, it automates\nlayout and task mapping synthesis with a novel type-inference-based algorithm.\nOur evaluation shows that Hexcute generalizes to a wide range of DL operators,\nachieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type\noperators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation.\n", "link": "http://arxiv.org/abs/2504.16214v2", "date": "2025-04-30", "relevancy": 1.9351, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4814}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hexcute%3A%20A%20Tile-based%20Programming%20Language%20with%20Automatic%20Layout%20and%0A%20%20Task-Mapping%20Synthesis&body=Title%3A%20Hexcute%3A%20A%20Tile-based%20Programming%20Language%20with%20Automatic%20Layout%20and%0A%20%20Task-Mapping%20Synthesis%0AAuthor%3A%20Xiao%20Zhang%20and%20Yaoyao%20Ding%20and%20Yang%20Hu%20and%20Gennady%20Pekhimenko%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20workloads%20mainly%20run%20on%20accelerators%20like%20GPUs.%20Recent%20DL%0Aquantization%20techniques%20demand%20a%20new%20matrix%20multiplication%20operator%20with%20mixed%0Ainput%20data%20types%2C%20further%20complicating%20GPU%20optimization.%20Prior%20high-level%0Acompilers%20like%20Triton%20lack%20the%20expressiveness%20to%20implement%20key%20optimizations%0Alike%20fine-grained%20data%20pipelines%20and%20hardware-friendly%20memory%20layouts%20for%20these%0Aoperators%2C%20while%20low-level%20programming%20models%2C%20such%20as%20Hidet%2C%20Graphene%2C%20and%0ACUTLASS%2C%20require%20significant%20programming%20efforts.%20To%20balance%20expressiveness%0Awith%20engineering%20effort%2C%20we%20propose%20Hexcute%2C%20a%20tile-based%20programming%20language%0Athat%20exposes%20shared%20memory%20and%20register%20abstractions%20to%20enable%20fine-grained%0Aoptimization%20for%20these%20operators.%20Additionally%2C%20Hexcute%20leverages%20task%20mapping%0Ato%20schedule%20the%20GPU%20program%2C%20and%20to%20reduce%20programming%20efforts%2C%20it%20automates%0Alayout%20and%20task%20mapping%20synthesis%20with%20a%20novel%20type-inference-based%20algorithm.%0AOur%20evaluation%20shows%20that%20Hexcute%20generalizes%20to%20a%20wide%20range%20of%20DL%20operators%2C%0Aachieves%201.7-11.28%24%5Ctimes%24%20speedup%20over%20existing%20DL%20compilers%20for%20mixed-type%0Aoperators%2C%20and%20brings%20up%20to%202.91%24%5Ctimes%24%20speedup%20in%20the%20end-to-end%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHexcute%253A%2520A%2520Tile-based%2520Programming%2520Language%2520with%2520Automatic%2520Layout%2520and%250A%2520%2520Task-Mapping%2520Synthesis%26entry.906535625%3DXiao%2520Zhang%2520and%2520Yaoyao%2520Ding%2520and%2520Yang%2520Hu%2520and%2520Gennady%2520Pekhimenko%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520workloads%2520mainly%2520run%2520on%2520accelerators%2520like%2520GPUs.%2520Recent%2520DL%250Aquantization%2520techniques%2520demand%2520a%2520new%2520matrix%2520multiplication%2520operator%2520with%2520mixed%250Ainput%2520data%2520types%252C%2520further%2520complicating%2520GPU%2520optimization.%2520Prior%2520high-level%250Acompilers%2520like%2520Triton%2520lack%2520the%2520expressiveness%2520to%2520implement%2520key%2520optimizations%250Alike%2520fine-grained%2520data%2520pipelines%2520and%2520hardware-friendly%2520memory%2520layouts%2520for%2520these%250Aoperators%252C%2520while%2520low-level%2520programming%2520models%252C%2520such%2520as%2520Hidet%252C%2520Graphene%252C%2520and%250ACUTLASS%252C%2520require%2520significant%2520programming%2520efforts.%2520To%2520balance%2520expressiveness%250Awith%2520engineering%2520effort%252C%2520we%2520propose%2520Hexcute%252C%2520a%2520tile-based%2520programming%2520language%250Athat%2520exposes%2520shared%2520memory%2520and%2520register%2520abstractions%2520to%2520enable%2520fine-grained%250Aoptimization%2520for%2520these%2520operators.%2520Additionally%252C%2520Hexcute%2520leverages%2520task%2520mapping%250Ato%2520schedule%2520the%2520GPU%2520program%252C%2520and%2520to%2520reduce%2520programming%2520efforts%252C%2520it%2520automates%250Alayout%2520and%2520task%2520mapping%2520synthesis%2520with%2520a%2520novel%2520type-inference-based%2520algorithm.%250AOur%2520evaluation%2520shows%2520that%2520Hexcute%2520generalizes%2520to%2520a%2520wide%2520range%2520of%2520DL%2520operators%252C%250Aachieves%25201.7-11.28%2524%255Ctimes%2524%2520speedup%2520over%2520existing%2520DL%2520compilers%2520for%2520mixed-type%250Aoperators%252C%2520and%2520brings%2520up%2520to%25202.91%2524%255Ctimes%2524%2520speedup%2520in%2520the%2520end-to-end%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hexcute%3A%20A%20Tile-based%20Programming%20Language%20with%20Automatic%20Layout%20and%0A%20%20Task-Mapping%20Synthesis&entry.906535625=Xiao%20Zhang%20and%20Yaoyao%20Ding%20and%20Yang%20Hu%20and%20Gennady%20Pekhimenko&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20workloads%20mainly%20run%20on%20accelerators%20like%20GPUs.%20Recent%20DL%0Aquantization%20techniques%20demand%20a%20new%20matrix%20multiplication%20operator%20with%20mixed%0Ainput%20data%20types%2C%20further%20complicating%20GPU%20optimization.%20Prior%20high-level%0Acompilers%20like%20Triton%20lack%20the%20expressiveness%20to%20implement%20key%20optimizations%0Alike%20fine-grained%20data%20pipelines%20and%20hardware-friendly%20memory%20layouts%20for%20these%0Aoperators%2C%20while%20low-level%20programming%20models%2C%20such%20as%20Hidet%2C%20Graphene%2C%20and%0ACUTLASS%2C%20require%20significant%20programming%20efforts.%20To%20balance%20expressiveness%0Awith%20engineering%20effort%2C%20we%20propose%20Hexcute%2C%20a%20tile-based%20programming%20language%0Athat%20exposes%20shared%20memory%20and%20register%20abstractions%20to%20enable%20fine-grained%0Aoptimization%20for%20these%20operators.%20Additionally%2C%20Hexcute%20leverages%20task%20mapping%0Ato%20schedule%20the%20GPU%20program%2C%20and%20to%20reduce%20programming%20efforts%2C%20it%20automates%0Alayout%20and%20task%20mapping%20synthesis%20with%20a%20novel%20type-inference-based%20algorithm.%0AOur%20evaluation%20shows%20that%20Hexcute%20generalizes%20to%20a%20wide%20range%20of%20DL%20operators%2C%0Aachieves%201.7-11.28%24%5Ctimes%24%20speedup%20over%20existing%20DL%20compilers%20for%20mixed-type%0Aoperators%2C%20and%20brings%20up%20to%202.91%24%5Ctimes%24%20speedup%20in%20the%20end-to-end%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16214v2&entry.124074799=Read"},
{"title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for\n  Automated Subject Indexing", "author": "Lisa Kluge and Maximilian K\u00e4hler", "abstract": "  This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts.\n", "link": "http://arxiv.org/abs/2504.21589v1", "date": "2025-04-30", "relevancy": 1.9347, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNB-AI-Project%20at%20SemEval-2025%20Task%205%3A%20An%20LLM-Ensemble%20Approach%20for%0A%20%20Automated%20Subject%20Indexing&body=Title%3A%20DNB-AI-Project%20at%20SemEval-2025%20Task%205%3A%20An%20LLM-Ensemble%20Approach%20for%0A%20%20Automated%20Subject%20Indexing%0AAuthor%3A%20Lisa%20Kluge%20and%20Maximilian%20K%C3%A4hler%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20system%20developed%20for%20the%20SemEval-2025%20Task%205%3A%0ALLMs4Subjects%3A%20LLM-based%20Automated%20Subject%20Tagging%20for%20a%20National%20Technical%0ALibrary%27s%20Open-Access%20Catalog.%20Our%20system%20relies%20on%20prompting%20a%20selection%20of%0ALLMs%20with%20varying%20examples%20of%20intellectually%20annotated%20records%20and%20asking%20the%0ALLMs%20to%20similarly%20suggest%20keywords%20for%20new%20records.%20This%20few-shot%20prompting%0Atechnique%20is%20combined%20with%20a%20series%20of%20post-processing%20steps%20that%20map%20the%0Agenerated%20keywords%20to%20the%20target%20vocabulary%2C%20aggregate%20the%20resulting%20subject%0Aterms%20to%20an%20ensemble%20vote%20and%2C%20finally%2C%20rank%20them%20as%20to%20their%20relevance%20to%20the%0Arecord.%20Our%20system%20is%20fourth%20in%20the%20quantitative%20ranking%20in%20the%20all-subjects%0Atrack%2C%20but%20achieves%20the%20best%20result%20in%20the%20qualitative%20ranking%20conducted%20by%0Asubject%20indexing%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNB-AI-Project%2520at%2520SemEval-2025%2520Task%25205%253A%2520An%2520LLM-Ensemble%2520Approach%2520for%250A%2520%2520Automated%2520Subject%2520Indexing%26entry.906535625%3DLisa%2520Kluge%2520and%2520Maximilian%2520K%25C3%25A4hler%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520our%2520system%2520developed%2520for%2520the%2520SemEval-2025%2520Task%25205%253A%250ALLMs4Subjects%253A%2520LLM-based%2520Automated%2520Subject%2520Tagging%2520for%2520a%2520National%2520Technical%250ALibrary%2527s%2520Open-Access%2520Catalog.%2520Our%2520system%2520relies%2520on%2520prompting%2520a%2520selection%2520of%250ALLMs%2520with%2520varying%2520examples%2520of%2520intellectually%2520annotated%2520records%2520and%2520asking%2520the%250ALLMs%2520to%2520similarly%2520suggest%2520keywords%2520for%2520new%2520records.%2520This%2520few-shot%2520prompting%250Atechnique%2520is%2520combined%2520with%2520a%2520series%2520of%2520post-processing%2520steps%2520that%2520map%2520the%250Agenerated%2520keywords%2520to%2520the%2520target%2520vocabulary%252C%2520aggregate%2520the%2520resulting%2520subject%250Aterms%2520to%2520an%2520ensemble%2520vote%2520and%252C%2520finally%252C%2520rank%2520them%2520as%2520to%2520their%2520relevance%2520to%2520the%250Arecord.%2520Our%2520system%2520is%2520fourth%2520in%2520the%2520quantitative%2520ranking%2520in%2520the%2520all-subjects%250Atrack%252C%2520but%2520achieves%2520the%2520best%2520result%2520in%2520the%2520qualitative%2520ranking%2520conducted%2520by%250Asubject%2520indexing%2520experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNB-AI-Project%20at%20SemEval-2025%20Task%205%3A%20An%20LLM-Ensemble%20Approach%20for%0A%20%20Automated%20Subject%20Indexing&entry.906535625=Lisa%20Kluge%20and%20Maximilian%20K%C3%A4hler&entry.1292438233=%20%20This%20paper%20presents%20our%20system%20developed%20for%20the%20SemEval-2025%20Task%205%3A%0ALLMs4Subjects%3A%20LLM-based%20Automated%20Subject%20Tagging%20for%20a%20National%20Technical%0ALibrary%27s%20Open-Access%20Catalog.%20Our%20system%20relies%20on%20prompting%20a%20selection%20of%0ALLMs%20with%20varying%20examples%20of%20intellectually%20annotated%20records%20and%20asking%20the%0ALLMs%20to%20similarly%20suggest%20keywords%20for%20new%20records.%20This%20few-shot%20prompting%0Atechnique%20is%20combined%20with%20a%20series%20of%20post-processing%20steps%20that%20map%20the%0Agenerated%20keywords%20to%20the%20target%20vocabulary%2C%20aggregate%20the%20resulting%20subject%0Aterms%20to%20an%20ensemble%20vote%20and%2C%20finally%2C%20rank%20them%20as%20to%20their%20relevance%20to%20the%0Arecord.%20Our%20system%20is%20fourth%20in%20the%20quantitative%20ranking%20in%20the%20all-subjects%0Atrack%2C%20but%20achieves%20the%20best%20result%20in%20the%20qualitative%20ranking%20conducted%20by%0Asubject%20indexing%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21589v1&entry.124074799=Read"},
{"title": "Meta knowledge assisted Evolutionary Neural Architecture Search", "author": "Yangyang Li and Guanlong Liu and Ronghua Shang and Licheng Jiao", "abstract": "  Evolutionary computation (EC)-based neural architecture search (NAS) has\nachieved remarkable performance in the automatic design of neural\narchitectures. However, the high computational cost associated with evaluating\nsearched architectures poses a challenge for these methods, and a fixed form of\nlearning rate (LR) schedule means greater information loss on diverse searched\narchitectures. This paper introduces an efficient EC-based NAS method to solve\nthese problems via an innovative meta-learning framework. Specifically, a\nmeta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a\nsuitable LR schedule, which guides the training process with lower information\nloss when evaluating each individual. An adaptive surrogate model is designed\nthrough an adaptive threshold to select the potential architectures in a few\nepochs and then evaluate the potential architectures with complete epochs.\nAdditionally, a periodic mutation operator is proposed to increase the\ndiversity of the population, which enhances the generalizability and\nrobustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets\ndemonstrate that the proposed method achieves high performance comparable to\nthat of many state-of-the-art peer methods, with lower computational cost and\ngreater robustness.\n", "link": "http://arxiv.org/abs/2504.21545v1", "date": "2025-04-30", "relevancy": 1.9279, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4944}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4805}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta%20knowledge%20assisted%20Evolutionary%20Neural%20Architecture%20Search&body=Title%3A%20Meta%20knowledge%20assisted%20Evolutionary%20Neural%20Architecture%20Search%0AAuthor%3A%20Yangyang%20Li%20and%20Guanlong%20Liu%20and%20Ronghua%20Shang%20and%20Licheng%20Jiao%0AAbstract%3A%20%20%20Evolutionary%20computation%20%28EC%29-based%20neural%20architecture%20search%20%28NAS%29%20has%0Aachieved%20remarkable%20performance%20in%20the%20automatic%20design%20of%20neural%0Aarchitectures.%20However%2C%20the%20high%20computational%20cost%20associated%20with%20evaluating%0Asearched%20architectures%20poses%20a%20challenge%20for%20these%20methods%2C%20and%20a%20fixed%20form%20of%0Alearning%20rate%20%28LR%29%20schedule%20means%20greater%20information%20loss%20on%20diverse%20searched%0Aarchitectures.%20This%20paper%20introduces%20an%20efficient%20EC-based%20NAS%20method%20to%20solve%0Athese%20problems%20via%20an%20innovative%20meta-learning%20framework.%20Specifically%2C%20a%0Ameta-learning-rate%20%28Meta-LR%29%20scheme%20is%20used%20through%20pretraining%20to%20obtain%20a%0Asuitable%20LR%20schedule%2C%20which%20guides%20the%20training%20process%20with%20lower%20information%0Aloss%20when%20evaluating%20each%20individual.%20An%20adaptive%20surrogate%20model%20is%20designed%0Athrough%20an%20adaptive%20threshold%20to%20select%20the%20potential%20architectures%20in%20a%20few%0Aepochs%20and%20then%20evaluate%20the%20potential%20architectures%20with%20complete%20epochs.%0AAdditionally%2C%20a%20periodic%20mutation%20operator%20is%20proposed%20to%20increase%20the%0Adiversity%20of%20the%20population%2C%20which%20enhances%20the%20generalizability%20and%0Arobustness.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet1K%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20high%20performance%20comparable%20to%0Athat%20of%20many%20state-of-the-art%20peer%20methods%2C%20with%20lower%20computational%20cost%20and%0Agreater%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta%2520knowledge%2520assisted%2520Evolutionary%2520Neural%2520Architecture%2520Search%26entry.906535625%3DYangyang%2520Li%2520and%2520Guanlong%2520Liu%2520and%2520Ronghua%2520Shang%2520and%2520Licheng%2520Jiao%26entry.1292438233%3D%2520%2520Evolutionary%2520computation%2520%2528EC%2529-based%2520neural%2520architecture%2520search%2520%2528NAS%2529%2520has%250Aachieved%2520remarkable%2520performance%2520in%2520the%2520automatic%2520design%2520of%2520neural%250Aarchitectures.%2520However%252C%2520the%2520high%2520computational%2520cost%2520associated%2520with%2520evaluating%250Asearched%2520architectures%2520poses%2520a%2520challenge%2520for%2520these%2520methods%252C%2520and%2520a%2520fixed%2520form%2520of%250Alearning%2520rate%2520%2528LR%2529%2520schedule%2520means%2520greater%2520information%2520loss%2520on%2520diverse%2520searched%250Aarchitectures.%2520This%2520paper%2520introduces%2520an%2520efficient%2520EC-based%2520NAS%2520method%2520to%2520solve%250Athese%2520problems%2520via%2520an%2520innovative%2520meta-learning%2520framework.%2520Specifically%252C%2520a%250Ameta-learning-rate%2520%2528Meta-LR%2529%2520scheme%2520is%2520used%2520through%2520pretraining%2520to%2520obtain%2520a%250Asuitable%2520LR%2520schedule%252C%2520which%2520guides%2520the%2520training%2520process%2520with%2520lower%2520information%250Aloss%2520when%2520evaluating%2520each%2520individual.%2520An%2520adaptive%2520surrogate%2520model%2520is%2520designed%250Athrough%2520an%2520adaptive%2520threshold%2520to%2520select%2520the%2520potential%2520architectures%2520in%2520a%2520few%250Aepochs%2520and%2520then%2520evaluate%2520the%2520potential%2520architectures%2520with%2520complete%2520epochs.%250AAdditionally%252C%2520a%2520periodic%2520mutation%2520operator%2520is%2520proposed%2520to%2520increase%2520the%250Adiversity%2520of%2520the%2520population%252C%2520which%2520enhances%2520the%2520generalizability%2520and%250Arobustness.%2520Experiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520ImageNet1K%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520high%2520performance%2520comparable%2520to%250Athat%2520of%2520many%2520state-of-the-art%2520peer%2520methods%252C%2520with%2520lower%2520computational%2520cost%2520and%250Agreater%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%20knowledge%20assisted%20Evolutionary%20Neural%20Architecture%20Search&entry.906535625=Yangyang%20Li%20and%20Guanlong%20Liu%20and%20Ronghua%20Shang%20and%20Licheng%20Jiao&entry.1292438233=%20%20Evolutionary%20computation%20%28EC%29-based%20neural%20architecture%20search%20%28NAS%29%20has%0Aachieved%20remarkable%20performance%20in%20the%20automatic%20design%20of%20neural%0Aarchitectures.%20However%2C%20the%20high%20computational%20cost%20associated%20with%20evaluating%0Asearched%20architectures%20poses%20a%20challenge%20for%20these%20methods%2C%20and%20a%20fixed%20form%20of%0Alearning%20rate%20%28LR%29%20schedule%20means%20greater%20information%20loss%20on%20diverse%20searched%0Aarchitectures.%20This%20paper%20introduces%20an%20efficient%20EC-based%20NAS%20method%20to%20solve%0Athese%20problems%20via%20an%20innovative%20meta-learning%20framework.%20Specifically%2C%20a%0Ameta-learning-rate%20%28Meta-LR%29%20scheme%20is%20used%20through%20pretraining%20to%20obtain%20a%0Asuitable%20LR%20schedule%2C%20which%20guides%20the%20training%20process%20with%20lower%20information%0Aloss%20when%20evaluating%20each%20individual.%20An%20adaptive%20surrogate%20model%20is%20designed%0Athrough%20an%20adaptive%20threshold%20to%20select%20the%20potential%20architectures%20in%20a%20few%0Aepochs%20and%20then%20evaluate%20the%20potential%20architectures%20with%20complete%20epochs.%0AAdditionally%2C%20a%20periodic%20mutation%20operator%20is%20proposed%20to%20increase%20the%0Adiversity%20of%20the%20population%2C%20which%20enhances%20the%20generalizability%20and%0Arobustness.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet1K%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20high%20performance%20comparable%20to%0Athat%20of%20many%20state-of-the-art%20peer%20methods%2C%20with%20lower%20computational%20cost%20and%0Agreater%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21545v1&entry.124074799=Read"},
{"title": "Balancing Interpretability and Flexibility in Modeling Diagnostic\n  Trajectories with an Embedded Neural Hawkes Process Model", "author": "Yuankang Zhao and Matthew Engelhard", "abstract": "  The Hawkes process (HP) is commonly used to model event sequences with\nself-reinforcing dynamics, including electronic health records (EHRs).\nTraditional HPs capture self-reinforcement via parametric impact functions that\ncan be inspected to understand how each event modulates the intensity of\nothers. Neural network-based HPs offer greater flexibility, resulting in\nimproved fit and prediction performance, but at the cost of interpretability,\nwhich is often critical in healthcare. In this work, we aim to understand and\nimprove upon this tradeoff. We propose a novel HP formulation in which impact\nfunctions are modeled by defining a flexible impact kernel, instantiated as a\nneural network, in event embedding space, which allows us to model large-scale\nevent sequences with many event types. This approach is more flexible than\ntraditional HPs yet more interpretable than other neural network approaches,\nand allows us to explicitly trade flexibility for interpretability by adding\ntransformer encoder layers to further contextualize the event embeddings.\nResults show that our method accurately recovers impact functions in\nsimulations, achieves competitive performance on MIMIC-IV procedure dataset,\nand gains clinically meaningful interpretation on XX-EHR with children\ndiagnosis dataset even without transformer layers. This suggests that our\nflexible impact kernel is often sufficient to capture self-reinforcing dynamics\nin EHRs and other data effectively, implying that interpretability can be\nmaintained without loss of performance.\n", "link": "http://arxiv.org/abs/2504.21795v1", "date": "2025-04-30", "relevancy": 1.9248, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4805}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Interpretability%20and%20Flexibility%20in%20Modeling%20Diagnostic%0A%20%20Trajectories%20with%20an%20Embedded%20Neural%20Hawkes%20Process%20Model&body=Title%3A%20Balancing%20Interpretability%20and%20Flexibility%20in%20Modeling%20Diagnostic%0A%20%20Trajectories%20with%20an%20Embedded%20Neural%20Hawkes%20Process%20Model%0AAuthor%3A%20Yuankang%20Zhao%20and%20Matthew%20Engelhard%0AAbstract%3A%20%20%20The%20Hawkes%20process%20%28HP%29%20is%20commonly%20used%20to%20model%20event%20sequences%20with%0Aself-reinforcing%20dynamics%2C%20including%20electronic%20health%20records%20%28EHRs%29.%0ATraditional%20HPs%20capture%20self-reinforcement%20via%20parametric%20impact%20functions%20that%0Acan%20be%20inspected%20to%20understand%20how%20each%20event%20modulates%20the%20intensity%20of%0Aothers.%20Neural%20network-based%20HPs%20offer%20greater%20flexibility%2C%20resulting%20in%0Aimproved%20fit%20and%20prediction%20performance%2C%20but%20at%20the%20cost%20of%20interpretability%2C%0Awhich%20is%20often%20critical%20in%20healthcare.%20In%20this%20work%2C%20we%20aim%20to%20understand%20and%0Aimprove%20upon%20this%20tradeoff.%20We%20propose%20a%20novel%20HP%20formulation%20in%20which%20impact%0Afunctions%20are%20modeled%20by%20defining%20a%20flexible%20impact%20kernel%2C%20instantiated%20as%20a%0Aneural%20network%2C%20in%20event%20embedding%20space%2C%20which%20allows%20us%20to%20model%20large-scale%0Aevent%20sequences%20with%20many%20event%20types.%20This%20approach%20is%20more%20flexible%20than%0Atraditional%20HPs%20yet%20more%20interpretable%20than%20other%20neural%20network%20approaches%2C%0Aand%20allows%20us%20to%20explicitly%20trade%20flexibility%20for%20interpretability%20by%20adding%0Atransformer%20encoder%20layers%20to%20further%20contextualize%20the%20event%20embeddings.%0AResults%20show%20that%20our%20method%20accurately%20recovers%20impact%20functions%20in%0Asimulations%2C%20achieves%20competitive%20performance%20on%20MIMIC-IV%20procedure%20dataset%2C%0Aand%20gains%20clinically%20meaningful%20interpretation%20on%20XX-EHR%20with%20children%0Adiagnosis%20dataset%20even%20without%20transformer%20layers.%20This%20suggests%20that%20our%0Aflexible%20impact%20kernel%20is%20often%20sufficient%20to%20capture%20self-reinforcing%20dynamics%0Ain%20EHRs%20and%20other%20data%20effectively%2C%20implying%20that%20interpretability%20can%20be%0Amaintained%20without%20loss%20of%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Interpretability%2520and%2520Flexibility%2520in%2520Modeling%2520Diagnostic%250A%2520%2520Trajectories%2520with%2520an%2520Embedded%2520Neural%2520Hawkes%2520Process%2520Model%26entry.906535625%3DYuankang%2520Zhao%2520and%2520Matthew%2520Engelhard%26entry.1292438233%3D%2520%2520The%2520Hawkes%2520process%2520%2528HP%2529%2520is%2520commonly%2520used%2520to%2520model%2520event%2520sequences%2520with%250Aself-reinforcing%2520dynamics%252C%2520including%2520electronic%2520health%2520records%2520%2528EHRs%2529.%250ATraditional%2520HPs%2520capture%2520self-reinforcement%2520via%2520parametric%2520impact%2520functions%2520that%250Acan%2520be%2520inspected%2520to%2520understand%2520how%2520each%2520event%2520modulates%2520the%2520intensity%2520of%250Aothers.%2520Neural%2520network-based%2520HPs%2520offer%2520greater%2520flexibility%252C%2520resulting%2520in%250Aimproved%2520fit%2520and%2520prediction%2520performance%252C%2520but%2520at%2520the%2520cost%2520of%2520interpretability%252C%250Awhich%2520is%2520often%2520critical%2520in%2520healthcare.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520understand%2520and%250Aimprove%2520upon%2520this%2520tradeoff.%2520We%2520propose%2520a%2520novel%2520HP%2520formulation%2520in%2520which%2520impact%250Afunctions%2520are%2520modeled%2520by%2520defining%2520a%2520flexible%2520impact%2520kernel%252C%2520instantiated%2520as%2520a%250Aneural%2520network%252C%2520in%2520event%2520embedding%2520space%252C%2520which%2520allows%2520us%2520to%2520model%2520large-scale%250Aevent%2520sequences%2520with%2520many%2520event%2520types.%2520This%2520approach%2520is%2520more%2520flexible%2520than%250Atraditional%2520HPs%2520yet%2520more%2520interpretable%2520than%2520other%2520neural%2520network%2520approaches%252C%250Aand%2520allows%2520us%2520to%2520explicitly%2520trade%2520flexibility%2520for%2520interpretability%2520by%2520adding%250Atransformer%2520encoder%2520layers%2520to%2520further%2520contextualize%2520the%2520event%2520embeddings.%250AResults%2520show%2520that%2520our%2520method%2520accurately%2520recovers%2520impact%2520functions%2520in%250Asimulations%252C%2520achieves%2520competitive%2520performance%2520on%2520MIMIC-IV%2520procedure%2520dataset%252C%250Aand%2520gains%2520clinically%2520meaningful%2520interpretation%2520on%2520XX-EHR%2520with%2520children%250Adiagnosis%2520dataset%2520even%2520without%2520transformer%2520layers.%2520This%2520suggests%2520that%2520our%250Aflexible%2520impact%2520kernel%2520is%2520often%2520sufficient%2520to%2520capture%2520self-reinforcing%2520dynamics%250Ain%2520EHRs%2520and%2520other%2520data%2520effectively%252C%2520implying%2520that%2520interpretability%2520can%2520be%250Amaintained%2520without%2520loss%2520of%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Interpretability%20and%20Flexibility%20in%20Modeling%20Diagnostic%0A%20%20Trajectories%20with%20an%20Embedded%20Neural%20Hawkes%20Process%20Model&entry.906535625=Yuankang%20Zhao%20and%20Matthew%20Engelhard&entry.1292438233=%20%20The%20Hawkes%20process%20%28HP%29%20is%20commonly%20used%20to%20model%20event%20sequences%20with%0Aself-reinforcing%20dynamics%2C%20including%20electronic%20health%20records%20%28EHRs%29.%0ATraditional%20HPs%20capture%20self-reinforcement%20via%20parametric%20impact%20functions%20that%0Acan%20be%20inspected%20to%20understand%20how%20each%20event%20modulates%20the%20intensity%20of%0Aothers.%20Neural%20network-based%20HPs%20offer%20greater%20flexibility%2C%20resulting%20in%0Aimproved%20fit%20and%20prediction%20performance%2C%20but%20at%20the%20cost%20of%20interpretability%2C%0Awhich%20is%20often%20critical%20in%20healthcare.%20In%20this%20work%2C%20we%20aim%20to%20understand%20and%0Aimprove%20upon%20this%20tradeoff.%20We%20propose%20a%20novel%20HP%20formulation%20in%20which%20impact%0Afunctions%20are%20modeled%20by%20defining%20a%20flexible%20impact%20kernel%2C%20instantiated%20as%20a%0Aneural%20network%2C%20in%20event%20embedding%20space%2C%20which%20allows%20us%20to%20model%20large-scale%0Aevent%20sequences%20with%20many%20event%20types.%20This%20approach%20is%20more%20flexible%20than%0Atraditional%20HPs%20yet%20more%20interpretable%20than%20other%20neural%20network%20approaches%2C%0Aand%20allows%20us%20to%20explicitly%20trade%20flexibility%20for%20interpretability%20by%20adding%0Atransformer%20encoder%20layers%20to%20further%20contextualize%20the%20event%20embeddings.%0AResults%20show%20that%20our%20method%20accurately%20recovers%20impact%20functions%20in%0Asimulations%2C%20achieves%20competitive%20performance%20on%20MIMIC-IV%20procedure%20dataset%2C%0Aand%20gains%20clinically%20meaningful%20interpretation%20on%20XX-EHR%20with%20children%0Adiagnosis%20dataset%20even%20without%20transformer%20layers.%20This%20suggests%20that%20our%0Aflexible%20impact%20kernel%20is%20often%20sufficient%20to%20capture%20self-reinforcing%20dynamics%0Ain%20EHRs%20and%20other%20data%20effectively%2C%20implying%20that%20interpretability%20can%20be%0Amaintained%20without%20loss%20of%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21795v1&entry.124074799=Read"},
{"title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System", "author": "Weihang Su and Baoqing Yue and Qingyao Ai and Yiran Hu and Jiaqi Li and Changyue Wang and Kaiyuan Zhang and Yueyue Wu and Yiqun Liu", "abstract": "  This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.\n", "link": "http://arxiv.org/abs/2503.14258v3", "date": "2025-04-30", "relevancy": 1.919, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4914}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4768}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JuDGE%3A%20Benchmarking%20Judgment%20Document%20Generation%20for%20Chinese%20Legal%0A%20%20System&body=Title%3A%20JuDGE%3A%20Benchmarking%20Judgment%20Document%20Generation%20for%20Chinese%20Legal%0A%20%20System%0AAuthor%3A%20Weihang%20Su%20and%20Baoqing%20Yue%20and%20Qingyao%20Ai%20and%20Yiran%20Hu%20and%20Jiaqi%20Li%20and%20Changyue%20Wang%20and%20Kaiyuan%20Zhang%20and%20Yueyue%20Wu%20and%20Yiqun%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20JuDGE%20%28Judgment%20Document%20Generation%20Evaluation%29%2C%20a%0Anovel%20benchmark%20for%20evaluating%20the%20performance%20of%20judgment%20document%20generation%0Ain%20the%20Chinese%20legal%20system.%20We%20define%20the%20task%20as%20generating%20a%20complete%20legal%0Ajudgment%20document%20from%20the%20given%20factual%20description%20of%20the%20case.%20To%20facilitate%0Athis%20benchmark%2C%20we%20construct%20a%20comprehensive%20dataset%20consisting%20of%20factual%0Adescriptions%20from%20real%20legal%20cases%2C%20paired%20with%20their%20corresponding%20full%0Ajudgment%20documents%2C%20which%20serve%20as%20the%20ground%20truth%20for%20evaluating%20the%20quality%0Aof%20generated%20documents.%20This%20dataset%20is%20further%20augmented%20by%20two%20external%20legal%0Acorpora%20that%20provide%20additional%20legal%20knowledge%20for%20the%20task%3A%20one%20comprising%0Astatutes%20and%20regulations%2C%20and%20the%20other%20consisting%20of%20a%20large%20collection%20of%0Apast%20judgment%20documents.%20In%20collaboration%20with%20legal%20professionals%2C%20we%0Aestablish%20a%20comprehensive%20automated%20evaluation%20framework%20to%20assess%20the%20quality%0Aof%20generated%20judgment%20documents%20across%20various%20dimensions.%20We%20evaluate%20various%0Abaseline%20approaches%2C%20including%20few-shot%20in-context%20learning%2C%20fine-tuning%2C%20and%20a%0Amulti-source%20retrieval-augmented%20generation%20%28RAG%29%20approach%2C%20using%20both%20general%0Aand%20legal-domain%20LLMs.%20The%20experimental%20results%20demonstrate%20that%2C%20while%20RAG%0Aapproaches%20can%20effectively%20improve%20performance%20in%20this%20task%2C%20there%20is%20still%0Asubstantial%20room%20for%20further%20improvement.%20All%20the%20codes%20and%20datasets%20are%0Aavailable%20at%3A%20https%3A//github.com/oneal2000/JuDGE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14258v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJuDGE%253A%2520Benchmarking%2520Judgment%2520Document%2520Generation%2520for%2520Chinese%2520Legal%250A%2520%2520System%26entry.906535625%3DWeihang%2520Su%2520and%2520Baoqing%2520Yue%2520and%2520Qingyao%2520Ai%2520and%2520Yiran%2520Hu%2520and%2520Jiaqi%2520Li%2520and%2520Changyue%2520Wang%2520and%2520Kaiyuan%2520Zhang%2520and%2520Yueyue%2520Wu%2520and%2520Yiqun%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520JuDGE%2520%2528Judgment%2520Document%2520Generation%2520Evaluation%2529%252C%2520a%250Anovel%2520benchmark%2520for%2520evaluating%2520the%2520performance%2520of%2520judgment%2520document%2520generation%250Ain%2520the%2520Chinese%2520legal%2520system.%2520We%2520define%2520the%2520task%2520as%2520generating%2520a%2520complete%2520legal%250Ajudgment%2520document%2520from%2520the%2520given%2520factual%2520description%2520of%2520the%2520case.%2520To%2520facilitate%250Athis%2520benchmark%252C%2520we%2520construct%2520a%2520comprehensive%2520dataset%2520consisting%2520of%2520factual%250Adescriptions%2520from%2520real%2520legal%2520cases%252C%2520paired%2520with%2520their%2520corresponding%2520full%250Ajudgment%2520documents%252C%2520which%2520serve%2520as%2520the%2520ground%2520truth%2520for%2520evaluating%2520the%2520quality%250Aof%2520generated%2520documents.%2520This%2520dataset%2520is%2520further%2520augmented%2520by%2520two%2520external%2520legal%250Acorpora%2520that%2520provide%2520additional%2520legal%2520knowledge%2520for%2520the%2520task%253A%2520one%2520comprising%250Astatutes%2520and%2520regulations%252C%2520and%2520the%2520other%2520consisting%2520of%2520a%2520large%2520collection%2520of%250Apast%2520judgment%2520documents.%2520In%2520collaboration%2520with%2520legal%2520professionals%252C%2520we%250Aestablish%2520a%2520comprehensive%2520automated%2520evaluation%2520framework%2520to%2520assess%2520the%2520quality%250Aof%2520generated%2520judgment%2520documents%2520across%2520various%2520dimensions.%2520We%2520evaluate%2520various%250Abaseline%2520approaches%252C%2520including%2520few-shot%2520in-context%2520learning%252C%2520fine-tuning%252C%2520and%2520a%250Amulti-source%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520approach%252C%2520using%2520both%2520general%250Aand%2520legal-domain%2520LLMs.%2520The%2520experimental%2520results%2520demonstrate%2520that%252C%2520while%2520RAG%250Aapproaches%2520can%2520effectively%2520improve%2520performance%2520in%2520this%2520task%252C%2520there%2520is%2520still%250Asubstantial%2520room%2520for%2520further%2520improvement.%2520All%2520the%2520codes%2520and%2520datasets%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/oneal2000/JuDGE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14258v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JuDGE%3A%20Benchmarking%20Judgment%20Document%20Generation%20for%20Chinese%20Legal%0A%20%20System&entry.906535625=Weihang%20Su%20and%20Baoqing%20Yue%20and%20Qingyao%20Ai%20and%20Yiran%20Hu%20and%20Jiaqi%20Li%20and%20Changyue%20Wang%20and%20Kaiyuan%20Zhang%20and%20Yueyue%20Wu%20and%20Yiqun%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20JuDGE%20%28Judgment%20Document%20Generation%20Evaluation%29%2C%20a%0Anovel%20benchmark%20for%20evaluating%20the%20performance%20of%20judgment%20document%20generation%0Ain%20the%20Chinese%20legal%20system.%20We%20define%20the%20task%20as%20generating%20a%20complete%20legal%0Ajudgment%20document%20from%20the%20given%20factual%20description%20of%20the%20case.%20To%20facilitate%0Athis%20benchmark%2C%20we%20construct%20a%20comprehensive%20dataset%20consisting%20of%20factual%0Adescriptions%20from%20real%20legal%20cases%2C%20paired%20with%20their%20corresponding%20full%0Ajudgment%20documents%2C%20which%20serve%20as%20the%20ground%20truth%20for%20evaluating%20the%20quality%0Aof%20generated%20documents.%20This%20dataset%20is%20further%20augmented%20by%20two%20external%20legal%0Acorpora%20that%20provide%20additional%20legal%20knowledge%20for%20the%20task%3A%20one%20comprising%0Astatutes%20and%20regulations%2C%20and%20the%20other%20consisting%20of%20a%20large%20collection%20of%0Apast%20judgment%20documents.%20In%20collaboration%20with%20legal%20professionals%2C%20we%0Aestablish%20a%20comprehensive%20automated%20evaluation%20framework%20to%20assess%20the%20quality%0Aof%20generated%20judgment%20documents%20across%20various%20dimensions.%20We%20evaluate%20various%0Abaseline%20approaches%2C%20including%20few-shot%20in-context%20learning%2C%20fine-tuning%2C%20and%20a%0Amulti-source%20retrieval-augmented%20generation%20%28RAG%29%20approach%2C%20using%20both%20general%0Aand%20legal-domain%20LLMs.%20The%20experimental%20results%20demonstrate%20that%2C%20while%20RAG%0Aapproaches%20can%20effectively%20improve%20performance%20in%20this%20task%2C%20there%20is%20still%0Asubstantial%20room%20for%20further%20improvement.%20All%20the%20codes%20and%20datasets%20are%0Aavailable%20at%3A%20https%3A//github.com/oneal2000/JuDGE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14258v3&entry.124074799=Read"},
{"title": "Quantitative Clustering in Mean-Field Transformer Models", "author": "Shi Chen and Zhengjiang Lin and Yury Polyanskiy and Philippe Rigollet", "abstract": "  The evolution of tokens through a deep transformer models can be modeled as\nan interacting particle system that has been shown to exhibit an asymptotic\nclustering behavior akin to the synchronization phenomenon in Kuramoto models.\nIn this work, we investigate the long-time clustering of mean-field transformer\nmodels. More precisely, we establish exponential rates of contraction to a\nDirac point mass for any suitably regular initialization under some assumptions\non the parameters of transformer models, any suitably regular mean-field\ninitialization synchronizes exponentially fast with some quantitative rates.\n", "link": "http://arxiv.org/abs/2504.14697v2", "date": "2025-04-30", "relevancy": 1.7826, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5128}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.443}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantitative%20Clustering%20in%20Mean-Field%20Transformer%20Models&body=Title%3A%20Quantitative%20Clustering%20in%20Mean-Field%20Transformer%20Models%0AAuthor%3A%20Shi%20Chen%20and%20Zhengjiang%20Lin%20and%20Yury%20Polyanskiy%20and%20Philippe%20Rigollet%0AAbstract%3A%20%20%20The%20evolution%20of%20tokens%20through%20a%20deep%20transformer%20models%20can%20be%20modeled%20as%0Aan%20interacting%20particle%20system%20that%20has%20been%20shown%20to%20exhibit%20an%20asymptotic%0Aclustering%20behavior%20akin%20to%20the%20synchronization%20phenomenon%20in%20Kuramoto%20models.%0AIn%20this%20work%2C%20we%20investigate%20the%20long-time%20clustering%20of%20mean-field%20transformer%0Amodels.%20More%20precisely%2C%20we%20establish%20exponential%20rates%20of%20contraction%20to%20a%0ADirac%20point%20mass%20for%20any%20suitably%20regular%20initialization%20under%20some%20assumptions%0Aon%20the%20parameters%20of%20transformer%20models%2C%20any%20suitably%20regular%20mean-field%0Ainitialization%20synchronizes%20exponentially%20fast%20with%20some%20quantitative%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantitative%2520Clustering%2520in%2520Mean-Field%2520Transformer%2520Models%26entry.906535625%3DShi%2520Chen%2520and%2520Zhengjiang%2520Lin%2520and%2520Yury%2520Polyanskiy%2520and%2520Philippe%2520Rigollet%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520tokens%2520through%2520a%2520deep%2520transformer%2520models%2520can%2520be%2520modeled%2520as%250Aan%2520interacting%2520particle%2520system%2520that%2520has%2520been%2520shown%2520to%2520exhibit%2520an%2520asymptotic%250Aclustering%2520behavior%2520akin%2520to%2520the%2520synchronization%2520phenomenon%2520in%2520Kuramoto%2520models.%250AIn%2520this%2520work%252C%2520we%2520investigate%2520the%2520long-time%2520clustering%2520of%2520mean-field%2520transformer%250Amodels.%2520More%2520precisely%252C%2520we%2520establish%2520exponential%2520rates%2520of%2520contraction%2520to%2520a%250ADirac%2520point%2520mass%2520for%2520any%2520suitably%2520regular%2520initialization%2520under%2520some%2520assumptions%250Aon%2520the%2520parameters%2520of%2520transformer%2520models%252C%2520any%2520suitably%2520regular%2520mean-field%250Ainitialization%2520synchronizes%2520exponentially%2520fast%2520with%2520some%2520quantitative%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantitative%20Clustering%20in%20Mean-Field%20Transformer%20Models&entry.906535625=Shi%20Chen%20and%20Zhengjiang%20Lin%20and%20Yury%20Polyanskiy%20and%20Philippe%20Rigollet&entry.1292438233=%20%20The%20evolution%20of%20tokens%20through%20a%20deep%20transformer%20models%20can%20be%20modeled%20as%0Aan%20interacting%20particle%20system%20that%20has%20been%20shown%20to%20exhibit%20an%20asymptotic%0Aclustering%20behavior%20akin%20to%20the%20synchronization%20phenomenon%20in%20Kuramoto%20models.%0AIn%20this%20work%2C%20we%20investigate%20the%20long-time%20clustering%20of%20mean-field%20transformer%0Amodels.%20More%20precisely%2C%20we%20establish%20exponential%20rates%20of%20contraction%20to%20a%0ADirac%20point%20mass%20for%20any%20suitably%20regular%20initialization%20under%20some%20assumptions%0Aon%20the%20parameters%20of%20transformer%20models%2C%20any%20suitably%20regular%20mean-field%0Ainitialization%20synchronizes%20exponentially%20fast%20with%20some%20quantitative%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14697v2&entry.124074799=Read"},
{"title": "Path Planning on Multi-level Point Cloud with a Weighted Traversability\n  Graph", "author": "Yujie Tang and Quan Li and Hao Geng and Yangmin Xie and Hang Shi and Yusheng Yang", "abstract": "  This article proposes a new path planning method for addressing multi-level\nterrain situations. The proposed method includes innovations in three aspects:\n1) the pre-processing of point cloud maps with a multi-level skip-list\nstructure and data-slimming algorithm for well-organized and simplified map\nformalization and management, 2) the direct acquisition of local traversability\nindexes through vehicle and point cloud interaction analysis, which saves work\nin surface fitting, and 3) the assignment of traversability indexes on a\nmulti-level connectivity graph to generate a weighted traversability graph for\ngenerally search-based path planning. The A* algorithm is modified to utilize\nthe traversability graph to generate a short and safe path. The effectiveness\nand reliability of the proposed method are verified through indoor and outdoor\nexperiments conducted in various environments, including multi-floor buildings,\nwoodland, and rugged mountainous regions. The results demonstrate that the\nproposed method can properly address 3D path planning problems for ground\nvehicles in a wide range of situations.\n", "link": "http://arxiv.org/abs/2504.21622v1", "date": "2025-04-30", "relevancy": 1.5044, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5163}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4959}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Path%20Planning%20on%20Multi-level%20Point%20Cloud%20with%20a%20Weighted%20Traversability%0A%20%20Graph&body=Title%3A%20Path%20Planning%20on%20Multi-level%20Point%20Cloud%20with%20a%20Weighted%20Traversability%0A%20%20Graph%0AAuthor%3A%20Yujie%20Tang%20and%20Quan%20Li%20and%20Hao%20Geng%20and%20Yangmin%20Xie%20and%20Hang%20Shi%20and%20Yusheng%20Yang%0AAbstract%3A%20%20%20This%20article%20proposes%20a%20new%20path%20planning%20method%20for%20addressing%20multi-level%0Aterrain%20situations.%20The%20proposed%20method%20includes%20innovations%20in%20three%20aspects%3A%0A1%29%20the%20pre-processing%20of%20point%20cloud%20maps%20with%20a%20multi-level%20skip-list%0Astructure%20and%20data-slimming%20algorithm%20for%20well-organized%20and%20simplified%20map%0Aformalization%20and%20management%2C%202%29%20the%20direct%20acquisition%20of%20local%20traversability%0Aindexes%20through%20vehicle%20and%20point%20cloud%20interaction%20analysis%2C%20which%20saves%20work%0Ain%20surface%20fitting%2C%20and%203%29%20the%20assignment%20of%20traversability%20indexes%20on%20a%0Amulti-level%20connectivity%20graph%20to%20generate%20a%20weighted%20traversability%20graph%20for%0Agenerally%20search-based%20path%20planning.%20The%20A%2A%20algorithm%20is%20modified%20to%20utilize%0Athe%20traversability%20graph%20to%20generate%20a%20short%20and%20safe%20path.%20The%20effectiveness%0Aand%20reliability%20of%20the%20proposed%20method%20are%20verified%20through%20indoor%20and%20outdoor%0Aexperiments%20conducted%20in%20various%20environments%2C%20including%20multi-floor%20buildings%2C%0Awoodland%2C%20and%20rugged%20mountainous%20regions.%20The%20results%20demonstrate%20that%20the%0Aproposed%20method%20can%20properly%20address%203D%20path%20planning%20problems%20for%20ground%0Avehicles%20in%20a%20wide%20range%20of%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPath%2520Planning%2520on%2520Multi-level%2520Point%2520Cloud%2520with%2520a%2520Weighted%2520Traversability%250A%2520%2520Graph%26entry.906535625%3DYujie%2520Tang%2520and%2520Quan%2520Li%2520and%2520Hao%2520Geng%2520and%2520Yangmin%2520Xie%2520and%2520Hang%2520Shi%2520and%2520Yusheng%2520Yang%26entry.1292438233%3D%2520%2520This%2520article%2520proposes%2520a%2520new%2520path%2520planning%2520method%2520for%2520addressing%2520multi-level%250Aterrain%2520situations.%2520The%2520proposed%2520method%2520includes%2520innovations%2520in%2520three%2520aspects%253A%250A1%2529%2520the%2520pre-processing%2520of%2520point%2520cloud%2520maps%2520with%2520a%2520multi-level%2520skip-list%250Astructure%2520and%2520data-slimming%2520algorithm%2520for%2520well-organized%2520and%2520simplified%2520map%250Aformalization%2520and%2520management%252C%25202%2529%2520the%2520direct%2520acquisition%2520of%2520local%2520traversability%250Aindexes%2520through%2520vehicle%2520and%2520point%2520cloud%2520interaction%2520analysis%252C%2520which%2520saves%2520work%250Ain%2520surface%2520fitting%252C%2520and%25203%2529%2520the%2520assignment%2520of%2520traversability%2520indexes%2520on%2520a%250Amulti-level%2520connectivity%2520graph%2520to%2520generate%2520a%2520weighted%2520traversability%2520graph%2520for%250Agenerally%2520search-based%2520path%2520planning.%2520The%2520A%252A%2520algorithm%2520is%2520modified%2520to%2520utilize%250Athe%2520traversability%2520graph%2520to%2520generate%2520a%2520short%2520and%2520safe%2520path.%2520The%2520effectiveness%250Aand%2520reliability%2520of%2520the%2520proposed%2520method%2520are%2520verified%2520through%2520indoor%2520and%2520outdoor%250Aexperiments%2520conducted%2520in%2520various%2520environments%252C%2520including%2520multi-floor%2520buildings%252C%250Awoodland%252C%2520and%2520rugged%2520mountainous%2520regions.%2520The%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520can%2520properly%2520address%25203D%2520path%2520planning%2520problems%2520for%2520ground%250Avehicles%2520in%2520a%2520wide%2520range%2520of%2520situations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Path%20Planning%20on%20Multi-level%20Point%20Cloud%20with%20a%20Weighted%20Traversability%0A%20%20Graph&entry.906535625=Yujie%20Tang%20and%20Quan%20Li%20and%20Hao%20Geng%20and%20Yangmin%20Xie%20and%20Hang%20Shi%20and%20Yusheng%20Yang&entry.1292438233=%20%20This%20article%20proposes%20a%20new%20path%20planning%20method%20for%20addressing%20multi-level%0Aterrain%20situations.%20The%20proposed%20method%20includes%20innovations%20in%20three%20aspects%3A%0A1%29%20the%20pre-processing%20of%20point%20cloud%20maps%20with%20a%20multi-level%20skip-list%0Astructure%20and%20data-slimming%20algorithm%20for%20well-organized%20and%20simplified%20map%0Aformalization%20and%20management%2C%202%29%20the%20direct%20acquisition%20of%20local%20traversability%0Aindexes%20through%20vehicle%20and%20point%20cloud%20interaction%20analysis%2C%20which%20saves%20work%0Ain%20surface%20fitting%2C%20and%203%29%20the%20assignment%20of%20traversability%20indexes%20on%20a%0Amulti-level%20connectivity%20graph%20to%20generate%20a%20weighted%20traversability%20graph%20for%0Agenerally%20search-based%20path%20planning.%20The%20A%2A%20algorithm%20is%20modified%20to%20utilize%0Athe%20traversability%20graph%20to%20generate%20a%20short%20and%20safe%20path.%20The%20effectiveness%0Aand%20reliability%20of%20the%20proposed%20method%20are%20verified%20through%20indoor%20and%20outdoor%0Aexperiments%20conducted%20in%20various%20environments%2C%20including%20multi-floor%20buildings%2C%0Awoodland%2C%20and%20rugged%20mountainous%20regions.%20The%20results%20demonstrate%20that%20the%0Aproposed%20method%20can%20properly%20address%203D%20path%20planning%20problems%20for%20ground%0Avehicles%20in%20a%20wide%20range%20of%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21622v1&entry.124074799=Read"},
{"title": "Variational Offline Multi-agent Skill Discovery", "author": "Jiayu Chen and Tian Lan and Vaneet Aggarwal", "abstract": "  Skills are effective temporal abstractions established for sequential\ndecision making, which enable efficient hierarchical learning for long-horizon\ntasks and facilitate multi-task learning through their transferability. Despite\nextensive research, research gaps remain in multi-agent scenarios, particularly\nfor automatically extracting subgroup coordination patterns in a multi-agent\ntask. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and\nVO-MASD-Hier, to simultaneously capture subgroup- and temporal-level\nabstractions and form multi-agent skills, which firstly solves the\naforementioned challenge. An essential algorithm component of these schemes is\na dynamic grouping function that can automatically detect latent subgroups\nbased on agent interactions in a task. Further, our method can be applied to\noffline multi-task data, and the discovered subgroup skills can be transferred\nacross relevant tasks without retraining. Empirical evaluations on StarCraft\ntasks indicate that our approach significantly outperforms existing\nhierarchical multi-agent reinforcement learning (MARL) methods. Moreover,\nskills discovered using our method can effectively reduce the learning\ndifficulty in MARL scenarios with delayed and sparse reward signals. The\ncodebase is available at https://github.com/LucasCJYSDL/VOMASD.\n", "link": "http://arxiv.org/abs/2405.16386v3", "date": "2025-04-30", "relevancy": 1.5931, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.537}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Offline%20Multi-agent%20Skill%20Discovery&body=Title%3A%20Variational%20Offline%20Multi-agent%20Skill%20Discovery%0AAuthor%3A%20Jiayu%20Chen%20and%20Tian%20Lan%20and%20Vaneet%20Aggarwal%0AAbstract%3A%20%20%20Skills%20are%20effective%20temporal%20abstractions%20established%20for%20sequential%0Adecision%20making%2C%20which%20enable%20efficient%20hierarchical%20learning%20for%20long-horizon%0Atasks%20and%20facilitate%20multi-task%20learning%20through%20their%20transferability.%20Despite%0Aextensive%20research%2C%20research%20gaps%20remain%20in%20multi-agent%20scenarios%2C%20particularly%0Afor%20automatically%20extracting%20subgroup%20coordination%20patterns%20in%20a%20multi-agent%0Atask.%20In%20this%20case%2C%20we%20propose%20two%20novel%20auto-encoder%20schemes%3A%20VO-MASD-3D%20and%0AVO-MASD-Hier%2C%20to%20simultaneously%20capture%20subgroup-%20and%20temporal-level%0Aabstractions%20and%20form%20multi-agent%20skills%2C%20which%20firstly%20solves%20the%0Aaforementioned%20challenge.%20An%20essential%20algorithm%20component%20of%20these%20schemes%20is%0Aa%20dynamic%20grouping%20function%20that%20can%20automatically%20detect%20latent%20subgroups%0Abased%20on%20agent%20interactions%20in%20a%20task.%20Further%2C%20our%20method%20can%20be%20applied%20to%0Aoffline%20multi-task%20data%2C%20and%20the%20discovered%20subgroup%20skills%20can%20be%20transferred%0Aacross%20relevant%20tasks%20without%20retraining.%20Empirical%20evaluations%20on%20StarCraft%0Atasks%20indicate%20that%20our%20approach%20significantly%20outperforms%20existing%0Ahierarchical%20multi-agent%20reinforcement%20learning%20%28MARL%29%20methods.%20Moreover%2C%0Askills%20discovered%20using%20our%20method%20can%20effectively%20reduce%20the%20learning%0Adifficulty%20in%20MARL%20scenarios%20with%20delayed%20and%20sparse%20reward%20signals.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/LucasCJYSDL/VOMASD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16386v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Offline%2520Multi-agent%2520Skill%2520Discovery%26entry.906535625%3DJiayu%2520Chen%2520and%2520Tian%2520Lan%2520and%2520Vaneet%2520Aggarwal%26entry.1292438233%3D%2520%2520Skills%2520are%2520effective%2520temporal%2520abstractions%2520established%2520for%2520sequential%250Adecision%2520making%252C%2520which%2520enable%2520efficient%2520hierarchical%2520learning%2520for%2520long-horizon%250Atasks%2520and%2520facilitate%2520multi-task%2520learning%2520through%2520their%2520transferability.%2520Despite%250Aextensive%2520research%252C%2520research%2520gaps%2520remain%2520in%2520multi-agent%2520scenarios%252C%2520particularly%250Afor%2520automatically%2520extracting%2520subgroup%2520coordination%2520patterns%2520in%2520a%2520multi-agent%250Atask.%2520In%2520this%2520case%252C%2520we%2520propose%2520two%2520novel%2520auto-encoder%2520schemes%253A%2520VO-MASD-3D%2520and%250AVO-MASD-Hier%252C%2520to%2520simultaneously%2520capture%2520subgroup-%2520and%2520temporal-level%250Aabstractions%2520and%2520form%2520multi-agent%2520skills%252C%2520which%2520firstly%2520solves%2520the%250Aaforementioned%2520challenge.%2520An%2520essential%2520algorithm%2520component%2520of%2520these%2520schemes%2520is%250Aa%2520dynamic%2520grouping%2520function%2520that%2520can%2520automatically%2520detect%2520latent%2520subgroups%250Abased%2520on%2520agent%2520interactions%2520in%2520a%2520task.%2520Further%252C%2520our%2520method%2520can%2520be%2520applied%2520to%250Aoffline%2520multi-task%2520data%252C%2520and%2520the%2520discovered%2520subgroup%2520skills%2520can%2520be%2520transferred%250Aacross%2520relevant%2520tasks%2520without%2520retraining.%2520Empirical%2520evaluations%2520on%2520StarCraft%250Atasks%2520indicate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%250Ahierarchical%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520methods.%2520Moreover%252C%250Askills%2520discovered%2520using%2520our%2520method%2520can%2520effectively%2520reduce%2520the%2520learning%250Adifficulty%2520in%2520MARL%2520scenarios%2520with%2520delayed%2520and%2520sparse%2520reward%2520signals.%2520The%250Acodebase%2520is%2520available%2520at%2520https%253A//github.com/LucasCJYSDL/VOMASD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16386v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Offline%20Multi-agent%20Skill%20Discovery&entry.906535625=Jiayu%20Chen%20and%20Tian%20Lan%20and%20Vaneet%20Aggarwal&entry.1292438233=%20%20Skills%20are%20effective%20temporal%20abstractions%20established%20for%20sequential%0Adecision%20making%2C%20which%20enable%20efficient%20hierarchical%20learning%20for%20long-horizon%0Atasks%20and%20facilitate%20multi-task%20learning%20through%20their%20transferability.%20Despite%0Aextensive%20research%2C%20research%20gaps%20remain%20in%20multi-agent%20scenarios%2C%20particularly%0Afor%20automatically%20extracting%20subgroup%20coordination%20patterns%20in%20a%20multi-agent%0Atask.%20In%20this%20case%2C%20we%20propose%20two%20novel%20auto-encoder%20schemes%3A%20VO-MASD-3D%20and%0AVO-MASD-Hier%2C%20to%20simultaneously%20capture%20subgroup-%20and%20temporal-level%0Aabstractions%20and%20form%20multi-agent%20skills%2C%20which%20firstly%20solves%20the%0Aaforementioned%20challenge.%20An%20essential%20algorithm%20component%20of%20these%20schemes%20is%0Aa%20dynamic%20grouping%20function%20that%20can%20automatically%20detect%20latent%20subgroups%0Abased%20on%20agent%20interactions%20in%20a%20task.%20Further%2C%20our%20method%20can%20be%20applied%20to%0Aoffline%20multi-task%20data%2C%20and%20the%20discovered%20subgroup%20skills%20can%20be%20transferred%0Aacross%20relevant%20tasks%20without%20retraining.%20Empirical%20evaluations%20on%20StarCraft%0Atasks%20indicate%20that%20our%20approach%20significantly%20outperforms%20existing%0Ahierarchical%20multi-agent%20reinforcement%20learning%20%28MARL%29%20methods.%20Moreover%2C%0Askills%20discovered%20using%20our%20method%20can%20effectively%20reduce%20the%20learning%0Adifficulty%20in%20MARL%20scenarios%20with%20delayed%20and%20sparse%20reward%20signals.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/LucasCJYSDL/VOMASD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16386v3&entry.124074799=Read"},
{"title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner\n  for Subject Tagging", "author": "Hadi Bayrami Asl Tekanlou and Jafar Razmara and Mahsa Sanaei and Mostafa Rahgouy and Hamed Babaei Giglou", "abstract": "  This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries.\n", "link": "http://arxiv.org/abs/2504.21474v1", "date": "2025-04-30", "relevancy": 1.5927, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4257}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3794}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Homa%20at%20SemEval-2025%20Task%205%3A%20Aligning%20Librarian%20Records%20with%20OntoAligner%0A%20%20for%20Subject%20Tagging&body=Title%3A%20Homa%20at%20SemEval-2025%20Task%205%3A%20Aligning%20Librarian%20Records%20with%20OntoAligner%0A%20%20for%20Subject%20Tagging%0AAuthor%3A%20Hadi%20Bayrami%20Asl%20Tekanlou%20and%20Jafar%20Razmara%20and%20Mahsa%20Sanaei%20and%20Mostafa%20Rahgouy%20and%20Hamed%20Babaei%20Giglou%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20system%2C%20Homa%2C%20for%20SemEval-2025%20Task%205%3A%20Subject%0ATagging%2C%20which%20focuses%20on%20automatically%20assigning%20subject%20labels%20to%20technical%0Arecords%20from%20TIBKAT%20using%20the%20Gemeinsame%20Normdatei%20%28GND%29%20taxonomy.%20We%20leverage%0AOntoAligner%2C%20a%20modular%20ontology%20alignment%20toolkit%2C%20to%20address%20this%20task%20by%0Aintegrating%20retrieval-augmented%20generation%20%28RAG%29%20techniques.%20Our%20approach%0Aformulates%20the%20subject%20tagging%20problem%20as%20an%20alignment%20task%2C%20where%20records%20are%0Amatched%20to%20GND%20categories%20based%20on%20semantic%20similarity.%20We%20evaluate%0AOntoAligner%27s%20adaptability%20for%20subject%20indexing%20and%20analyze%20its%20effectiveness%0Ain%20handling%20multilingual%20records.%20Experimental%20results%20demonstrate%20the%0Astrengths%20and%20limitations%20of%20this%20method%2C%20highlighting%20the%20potential%20of%0Aalignment%20techniques%20for%20improving%20subject%20tagging%20in%20digital%20libraries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoma%2520at%2520SemEval-2025%2520Task%25205%253A%2520Aligning%2520Librarian%2520Records%2520with%2520OntoAligner%250A%2520%2520for%2520Subject%2520Tagging%26entry.906535625%3DHadi%2520Bayrami%2520Asl%2520Tekanlou%2520and%2520Jafar%2520Razmara%2520and%2520Mahsa%2520Sanaei%2520and%2520Mostafa%2520Rahgouy%2520and%2520Hamed%2520Babaei%2520Giglou%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520our%2520system%252C%2520Homa%252C%2520for%2520SemEval-2025%2520Task%25205%253A%2520Subject%250ATagging%252C%2520which%2520focuses%2520on%2520automatically%2520assigning%2520subject%2520labels%2520to%2520technical%250Arecords%2520from%2520TIBKAT%2520using%2520the%2520Gemeinsame%2520Normdatei%2520%2528GND%2529%2520taxonomy.%2520We%2520leverage%250AOntoAligner%252C%2520a%2520modular%2520ontology%2520alignment%2520toolkit%252C%2520to%2520address%2520this%2520task%2520by%250Aintegrating%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520techniques.%2520Our%2520approach%250Aformulates%2520the%2520subject%2520tagging%2520problem%2520as%2520an%2520alignment%2520task%252C%2520where%2520records%2520are%250Amatched%2520to%2520GND%2520categories%2520based%2520on%2520semantic%2520similarity.%2520We%2520evaluate%250AOntoAligner%2527s%2520adaptability%2520for%2520subject%2520indexing%2520and%2520analyze%2520its%2520effectiveness%250Ain%2520handling%2520multilingual%2520records.%2520Experimental%2520results%2520demonstrate%2520the%250Astrengths%2520and%2520limitations%2520of%2520this%2520method%252C%2520highlighting%2520the%2520potential%2520of%250Aalignment%2520techniques%2520for%2520improving%2520subject%2520tagging%2520in%2520digital%2520libraries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homa%20at%20SemEval-2025%20Task%205%3A%20Aligning%20Librarian%20Records%20with%20OntoAligner%0A%20%20for%20Subject%20Tagging&entry.906535625=Hadi%20Bayrami%20Asl%20Tekanlou%20and%20Jafar%20Razmara%20and%20Mahsa%20Sanaei%20and%20Mostafa%20Rahgouy%20and%20Hamed%20Babaei%20Giglou&entry.1292438233=%20%20This%20paper%20presents%20our%20system%2C%20Homa%2C%20for%20SemEval-2025%20Task%205%3A%20Subject%0ATagging%2C%20which%20focuses%20on%20automatically%20assigning%20subject%20labels%20to%20technical%0Arecords%20from%20TIBKAT%20using%20the%20Gemeinsame%20Normdatei%20%28GND%29%20taxonomy.%20We%20leverage%0AOntoAligner%2C%20a%20modular%20ontology%20alignment%20toolkit%2C%20to%20address%20this%20task%20by%0Aintegrating%20retrieval-augmented%20generation%20%28RAG%29%20techniques.%20Our%20approach%0Aformulates%20the%20subject%20tagging%20problem%20as%20an%20alignment%20task%2C%20where%20records%20are%0Amatched%20to%20GND%20categories%20based%20on%20semantic%20similarity.%20We%20evaluate%0AOntoAligner%27s%20adaptability%20for%20subject%20indexing%20and%20analyze%20its%20effectiveness%0Ain%20handling%20multilingual%20records.%20Experimental%20results%20demonstrate%20the%0Astrengths%20and%20limitations%20of%20this%20method%2C%20highlighting%20the%20potential%20of%0Aalignment%20techniques%20for%20improving%20subject%20tagging%20in%20digital%20libraries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21474v1&entry.124074799=Read"},
{"title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering", "author": "Zhonghao Li and Kunpeng Zhang and Jinghuai Ou and Shuliang Liu and Xuming Hu", "abstract": "  Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop-RAG.\n", "link": "http://arxiv.org/abs/2504.20114v2", "date": "2025-04-30", "relevancy": 1.8743, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4738}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TreeHop%3A%20Generate%20and%20Filter%20Next%20Query%20Embeddings%20Efficiently%20for%0A%20%20Multi-hop%20Question%20Answering&body=Title%3A%20TreeHop%3A%20Generate%20and%20Filter%20Next%20Query%20Embeddings%20Efficiently%20for%0A%20%20Multi-hop%20Question%20Answering%0AAuthor%3A%20Zhonghao%20Li%20and%20Kunpeng%20Zhang%20and%20Jinghuai%20Ou%20and%20Shuliang%20Liu%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20systems%20face%20significant%20challenges%20in%0Amulti-hop%20question%20answering%20%28MHQA%29%2C%20where%20complex%20queries%20require%20synthesizing%0Ainformation%20across%20multiple%20document%20chunks.%20Existing%20approaches%20typically%20rely%0Aon%20iterative%20LLM-based%20query%20rewriting%20and%20routing%2C%20resulting%20in%20high%0Acomputational%20costs%20due%20to%20repeated%20LLM%20invocations%20and%20multi-stage%20processes.%0ATo%20address%20these%20limitations%2C%20we%20propose%20TreeHop%2C%20an%20embedding-level%20framework%0Awithout%20the%20need%20for%20LLMs%20in%20query%20refinement.%20TreeHop%20dynamically%20updates%0Aquery%20embeddings%20by%20fusing%20semantic%20information%20from%20prior%20queries%20and%0Aretrieved%20documents%2C%20enabling%20iterative%20retrieval%20through%20embedding-space%0Aoperations%20alone.%20This%20method%20replaces%20the%20traditional%0A%22Retrieve-Rewrite-Vectorize-Retrieve%22%20cycle%20with%20a%20streamlined%0A%22Retrieve-Embed-Retrieve%22%20loop%2C%20significantly%20reducing%20computational%20overhead.%0AMoreover%2C%20a%20rule-based%20stop%20criterion%20is%20introduced%20to%20further%20prune%20redundant%0Aretrievals%2C%20balancing%20efficiency%20and%20recall%20rate.%20Experimental%20results%20show%0Athat%20TreeHop%20rivals%20advanced%20RAG%20methods%20across%20three%20open-domain%20MHQA%0Adatasets%2C%20achieving%20comparable%20performance%20with%20only%205%5C%25-0.4%5C%25%20of%20the%20model%0Aparameter%20size%20and%20reducing%20the%20query%20latency%20by%20approximately%2099%5C%25%20compared%20to%0Aconcurrent%20approaches.%20This%20makes%20TreeHop%20a%20faster%20and%20more%20cost-effective%0Asolution%20for%20deployment%20in%20a%20range%20of%20knowledge-intensive%20applications.%20For%0Areproducibility%20purposes%2C%20codes%20and%20data%20are%20available%20here%3A%0Ahttps%3A//github.com/allen-li1231/TreeHop-RAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreeHop%253A%2520Generate%2520and%2520Filter%2520Next%2520Query%2520Embeddings%2520Efficiently%2520for%250A%2520%2520Multi-hop%2520Question%2520Answering%26entry.906535625%3DZhonghao%2520Li%2520and%2520Kunpeng%2520Zhang%2520and%2520Jinghuai%2520Ou%2520and%2520Shuliang%2520Liu%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520systems%2520face%2520significant%2520challenges%2520in%250Amulti-hop%2520question%2520answering%2520%2528MHQA%2529%252C%2520where%2520complex%2520queries%2520require%2520synthesizing%250Ainformation%2520across%2520multiple%2520document%2520chunks.%2520Existing%2520approaches%2520typically%2520rely%250Aon%2520iterative%2520LLM-based%2520query%2520rewriting%2520and%2520routing%252C%2520resulting%2520in%2520high%250Acomputational%2520costs%2520due%2520to%2520repeated%2520LLM%2520invocations%2520and%2520multi-stage%2520processes.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520TreeHop%252C%2520an%2520embedding-level%2520framework%250Awithout%2520the%2520need%2520for%2520LLMs%2520in%2520query%2520refinement.%2520TreeHop%2520dynamically%2520updates%250Aquery%2520embeddings%2520by%2520fusing%2520semantic%2520information%2520from%2520prior%2520queries%2520and%250Aretrieved%2520documents%252C%2520enabling%2520iterative%2520retrieval%2520through%2520embedding-space%250Aoperations%2520alone.%2520This%2520method%2520replaces%2520the%2520traditional%250A%2522Retrieve-Rewrite-Vectorize-Retrieve%2522%2520cycle%2520with%2520a%2520streamlined%250A%2522Retrieve-Embed-Retrieve%2522%2520loop%252C%2520significantly%2520reducing%2520computational%2520overhead.%250AMoreover%252C%2520a%2520rule-based%2520stop%2520criterion%2520is%2520introduced%2520to%2520further%2520prune%2520redundant%250Aretrievals%252C%2520balancing%2520efficiency%2520and%2520recall%2520rate.%2520Experimental%2520results%2520show%250Athat%2520TreeHop%2520rivals%2520advanced%2520RAG%2520methods%2520across%2520three%2520open-domain%2520MHQA%250Adatasets%252C%2520achieving%2520comparable%2520performance%2520with%2520only%25205%255C%2525-0.4%255C%2525%2520of%2520the%2520model%250Aparameter%2520size%2520and%2520reducing%2520the%2520query%2520latency%2520by%2520approximately%252099%255C%2525%2520compared%2520to%250Aconcurrent%2520approaches.%2520This%2520makes%2520TreeHop%2520a%2520faster%2520and%2520more%2520cost-effective%250Asolution%2520for%2520deployment%2520in%2520a%2520range%2520of%2520knowledge-intensive%2520applications.%2520For%250Areproducibility%2520purposes%252C%2520codes%2520and%2520data%2520are%2520available%2520here%253A%250Ahttps%253A//github.com/allen-li1231/TreeHop-RAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TreeHop%3A%20Generate%20and%20Filter%20Next%20Query%20Embeddings%20Efficiently%20for%0A%20%20Multi-hop%20Question%20Answering&entry.906535625=Zhonghao%20Li%20and%20Kunpeng%20Zhang%20and%20Jinghuai%20Ou%20and%20Shuliang%20Liu%20and%20Xuming%20Hu&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20systems%20face%20significant%20challenges%20in%0Amulti-hop%20question%20answering%20%28MHQA%29%2C%20where%20complex%20queries%20require%20synthesizing%0Ainformation%20across%20multiple%20document%20chunks.%20Existing%20approaches%20typically%20rely%0Aon%20iterative%20LLM-based%20query%20rewriting%20and%20routing%2C%20resulting%20in%20high%0Acomputational%20costs%20due%20to%20repeated%20LLM%20invocations%20and%20multi-stage%20processes.%0ATo%20address%20these%20limitations%2C%20we%20propose%20TreeHop%2C%20an%20embedding-level%20framework%0Awithout%20the%20need%20for%20LLMs%20in%20query%20refinement.%20TreeHop%20dynamically%20updates%0Aquery%20embeddings%20by%20fusing%20semantic%20information%20from%20prior%20queries%20and%0Aretrieved%20documents%2C%20enabling%20iterative%20retrieval%20through%20embedding-space%0Aoperations%20alone.%20This%20method%20replaces%20the%20traditional%0A%22Retrieve-Rewrite-Vectorize-Retrieve%22%20cycle%20with%20a%20streamlined%0A%22Retrieve-Embed-Retrieve%22%20loop%2C%20significantly%20reducing%20computational%20overhead.%0AMoreover%2C%20a%20rule-based%20stop%20criterion%20is%20introduced%20to%20further%20prune%20redundant%0Aretrievals%2C%20balancing%20efficiency%20and%20recall%20rate.%20Experimental%20results%20show%0Athat%20TreeHop%20rivals%20advanced%20RAG%20methods%20across%20three%20open-domain%20MHQA%0Adatasets%2C%20achieving%20comparable%20performance%20with%20only%205%5C%25-0.4%5C%25%20of%20the%20model%0Aparameter%20size%20and%20reducing%20the%20query%20latency%20by%20approximately%2099%5C%25%20compared%20to%0Aconcurrent%20approaches.%20This%20makes%20TreeHop%20a%20faster%20and%20more%20cost-effective%0Asolution%20for%20deployment%20in%20a%20range%20of%20knowledge-intensive%20applications.%20For%0Areproducibility%20purposes%2C%20codes%20and%20data%20are%20available%20here%3A%0Ahttps%3A//github.com/allen-li1231/TreeHop-RAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20114v2&entry.124074799=Read"},
{"title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in\n  Prolonged Exposure Dialogues", "author": "Suhas BN and Dominik Mattioli and Saeed Abdullah and Rosa I. Arriaga and Chris W. Wiese and Andrew M. Sherrill", "abstract": "  The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\nhowever, synthetic interactions do not adequately reflect key fidelity markers\n(e.g., distress monitoring). We highlight gaps in existing evaluation\nframeworks and advocate for fidelity-aware metrics that go beyond surface\nfluency to uncover clinically significant failures. Our findings clarify where\nsynthetic data can effectively complement real-world datasets -- and where\ncritical limitations remain.\n", "link": "http://arxiv.org/abs/2504.21800v1", "date": "2025-04-30", "relevancy": 1.7017, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4326}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Real%20Are%20Synthetic%20Therapy%20Conversations%3F%20Evaluating%20Fidelity%20in%0A%20%20Prolonged%20Exposure%20Dialogues&body=Title%3A%20How%20Real%20Are%20Synthetic%20Therapy%20Conversations%3F%20Evaluating%20Fidelity%20in%0A%20%20Prolonged%20Exposure%20Dialogues%0AAuthor%3A%20Suhas%20BN%20and%20Dominik%20Mattioli%20and%20Saeed%20Abdullah%20and%20Rosa%20I.%20Arriaga%20and%20Chris%20W.%20Wiese%20and%20Andrew%20M.%20Sherrill%0AAbstract%3A%20%20%20The%20growing%20adoption%20of%20synthetic%20data%20in%20healthcare%20is%20driven%20by%20privacy%0Aconcerns%2C%20limited%20access%20to%20real-world%20data%2C%20and%20the%20high%20cost%20of%20annotation.%0AThis%20work%20explores%20the%20use%20of%20synthetic%20Prolonged%20Exposure%20%28PE%29%20therapeutic%0Aconversations%20for%20Post-Traumatic%20Stress%20Disorder%20%28PTSD%29%20as%20a%20scalable%0Aalternative%20for%20training%20and%20evaluating%20clinical%20models.%20We%20systematically%0Acompare%20real%20and%20synthetic%20dialogues%20using%20linguistic%2C%20structural%2C%20and%0Aprotocol-specific%20metrics%2C%20including%20turn-taking%20patterns%20and%20treatment%0Afidelity.%20We%20also%20introduce%20and%20evaluate%20PE-specific%20metrics%20derived%20from%0Alinguistic%20analysis%20and%20semantic%20modeling%2C%20offering%20a%20novel%20framework%20for%0Aassessing%20clinical%20fidelity%20beyond%20surface%20fluency.%20Our%20findings%20show%20that%0Aalthough%20synthetic%20data%20holds%20promise%20for%20mitigating%20data%20scarcity%20and%0Aprotecting%20patient%20privacy%2C%20it%20can%20struggle%20to%20capture%20the%20subtle%20dynamics%20of%0Atherapeutic%20interactions.%20In%20our%20dataset%2C%20synthetic%20dialogues%20match%20structural%0Afeatures%20of%20real-world%20dialogues%20%28e.g.%2C%20speaker%20switch%20ratio%3A%200.98%20vs.%200.99%29%2C%0Ahowever%2C%20synthetic%20interactions%20do%20not%20adequately%20reflect%20key%20fidelity%20markers%0A%28e.g.%2C%20distress%20monitoring%29.%20We%20highlight%20gaps%20in%20existing%20evaluation%0Aframeworks%20and%20advocate%20for%20fidelity-aware%20metrics%20that%20go%20beyond%20surface%0Afluency%20to%20uncover%20clinically%20significant%20failures.%20Our%20findings%20clarify%20where%0Asynthetic%20data%20can%20effectively%20complement%20real-world%20datasets%20--%20and%20where%0Acritical%20limitations%20remain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Real%2520Are%2520Synthetic%2520Therapy%2520Conversations%253F%2520Evaluating%2520Fidelity%2520in%250A%2520%2520Prolonged%2520Exposure%2520Dialogues%26entry.906535625%3DSuhas%2520BN%2520and%2520Dominik%2520Mattioli%2520and%2520Saeed%2520Abdullah%2520and%2520Rosa%2520I.%2520Arriaga%2520and%2520Chris%2520W.%2520Wiese%2520and%2520Andrew%2520M.%2520Sherrill%26entry.1292438233%3D%2520%2520The%2520growing%2520adoption%2520of%2520synthetic%2520data%2520in%2520healthcare%2520is%2520driven%2520by%2520privacy%250Aconcerns%252C%2520limited%2520access%2520to%2520real-world%2520data%252C%2520and%2520the%2520high%2520cost%2520of%2520annotation.%250AThis%2520work%2520explores%2520the%2520use%2520of%2520synthetic%2520Prolonged%2520Exposure%2520%2528PE%2529%2520therapeutic%250Aconversations%2520for%2520Post-Traumatic%2520Stress%2520Disorder%2520%2528PTSD%2529%2520as%2520a%2520scalable%250Aalternative%2520for%2520training%2520and%2520evaluating%2520clinical%2520models.%2520We%2520systematically%250Acompare%2520real%2520and%2520synthetic%2520dialogues%2520using%2520linguistic%252C%2520structural%252C%2520and%250Aprotocol-specific%2520metrics%252C%2520including%2520turn-taking%2520patterns%2520and%2520treatment%250Afidelity.%2520We%2520also%2520introduce%2520and%2520evaluate%2520PE-specific%2520metrics%2520derived%2520from%250Alinguistic%2520analysis%2520and%2520semantic%2520modeling%252C%2520offering%2520a%2520novel%2520framework%2520for%250Aassessing%2520clinical%2520fidelity%2520beyond%2520surface%2520fluency.%2520Our%2520findings%2520show%2520that%250Aalthough%2520synthetic%2520data%2520holds%2520promise%2520for%2520mitigating%2520data%2520scarcity%2520and%250Aprotecting%2520patient%2520privacy%252C%2520it%2520can%2520struggle%2520to%2520capture%2520the%2520subtle%2520dynamics%2520of%250Atherapeutic%2520interactions.%2520In%2520our%2520dataset%252C%2520synthetic%2520dialogues%2520match%2520structural%250Afeatures%2520of%2520real-world%2520dialogues%2520%2528e.g.%252C%2520speaker%2520switch%2520ratio%253A%25200.98%2520vs.%25200.99%2529%252C%250Ahowever%252C%2520synthetic%2520interactions%2520do%2520not%2520adequately%2520reflect%2520key%2520fidelity%2520markers%250A%2528e.g.%252C%2520distress%2520monitoring%2529.%2520We%2520highlight%2520gaps%2520in%2520existing%2520evaluation%250Aframeworks%2520and%2520advocate%2520for%2520fidelity-aware%2520metrics%2520that%2520go%2520beyond%2520surface%250Afluency%2520to%2520uncover%2520clinically%2520significant%2520failures.%2520Our%2520findings%2520clarify%2520where%250Asynthetic%2520data%2520can%2520effectively%2520complement%2520real-world%2520datasets%2520--%2520and%2520where%250Acritical%2520limitations%2520remain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Real%20Are%20Synthetic%20Therapy%20Conversations%3F%20Evaluating%20Fidelity%20in%0A%20%20Prolonged%20Exposure%20Dialogues&entry.906535625=Suhas%20BN%20and%20Dominik%20Mattioli%20and%20Saeed%20Abdullah%20and%20Rosa%20I.%20Arriaga%20and%20Chris%20W.%20Wiese%20and%20Andrew%20M.%20Sherrill&entry.1292438233=%20%20The%20growing%20adoption%20of%20synthetic%20data%20in%20healthcare%20is%20driven%20by%20privacy%0Aconcerns%2C%20limited%20access%20to%20real-world%20data%2C%20and%20the%20high%20cost%20of%20annotation.%0AThis%20work%20explores%20the%20use%20of%20synthetic%20Prolonged%20Exposure%20%28PE%29%20therapeutic%0Aconversations%20for%20Post-Traumatic%20Stress%20Disorder%20%28PTSD%29%20as%20a%20scalable%0Aalternative%20for%20training%20and%20evaluating%20clinical%20models.%20We%20systematically%0Acompare%20real%20and%20synthetic%20dialogues%20using%20linguistic%2C%20structural%2C%20and%0Aprotocol-specific%20metrics%2C%20including%20turn-taking%20patterns%20and%20treatment%0Afidelity.%20We%20also%20introduce%20and%20evaluate%20PE-specific%20metrics%20derived%20from%0Alinguistic%20analysis%20and%20semantic%20modeling%2C%20offering%20a%20novel%20framework%20for%0Aassessing%20clinical%20fidelity%20beyond%20surface%20fluency.%20Our%20findings%20show%20that%0Aalthough%20synthetic%20data%20holds%20promise%20for%20mitigating%20data%20scarcity%20and%0Aprotecting%20patient%20privacy%2C%20it%20can%20struggle%20to%20capture%20the%20subtle%20dynamics%20of%0Atherapeutic%20interactions.%20In%20our%20dataset%2C%20synthetic%20dialogues%20match%20structural%0Afeatures%20of%20real-world%20dialogues%20%28e.g.%2C%20speaker%20switch%20ratio%3A%200.98%20vs.%200.99%29%2C%0Ahowever%2C%20synthetic%20interactions%20do%20not%20adequately%20reflect%20key%20fidelity%20markers%0A%28e.g.%2C%20distress%20monitoring%29.%20We%20highlight%20gaps%20in%20existing%20evaluation%0Aframeworks%20and%20advocate%20for%20fidelity-aware%20metrics%20that%20go%20beyond%20surface%0Afluency%20to%20uncover%20clinically%20significant%20failures.%20Our%20findings%20clarify%20where%0Asynthetic%20data%20can%20effectively%20complement%20real-world%20datasets%20--%20and%20where%0Acritical%20limitations%20remain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21800v1&entry.124074799=Read"},
{"title": "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies", "author": "Shaibal Saha and Lanyu Xu", "abstract": "  In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\n", "link": "http://arxiv.org/abs/2503.02891v2", "date": "2025-04-30", "relevancy": 1.6488, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5504}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformers%20on%20the%20Edge%3A%20A%20Comprehensive%20Survey%20of%20Model%0A%20%20Compression%20and%20Acceleration%20Strategies&body=Title%3A%20Vision%20Transformers%20on%20the%20Edge%3A%20A%20Comprehensive%20Survey%20of%20Model%0A%20%20Compression%20and%20Acceleration%20Strategies%0AAuthor%3A%20Shaibal%20Saha%20and%20Lanyu%20Xu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20vision%20transformers%20%28ViTs%29%20have%20emerged%20as%20powerful%20and%0Apromising%20techniques%20for%20computer%20vision%20tasks%20such%20as%20image%20classification%2C%0Aobject%20detection%2C%20and%20segmentation.%20Unlike%20convolutional%20neural%20networks%0A%28CNNs%29%2C%20which%20rely%20on%20hierarchical%20feature%20extraction%2C%20ViTs%20treat%20images%20as%0Asequences%20of%20patches%20and%20leverage%20self-attention%20mechanisms.%20However%2C%20their%0Ahigh%20computational%20complexity%20and%20memory%20demands%20pose%20significant%20challenges%0Afor%20deployment%20on%20resource-constrained%20edge%20devices.%20To%20address%20these%0Alimitations%2C%20extensive%20research%20has%20focused%20on%20model%20compression%20techniques%20and%0Ahardware-aware%20acceleration%20strategies.%20Nonetheless%2C%20a%20comprehensive%20review%0Athat%20systematically%20categorizes%20these%20techniques%20and%20their%20trade-offs%20in%0Aaccuracy%2C%20efficiency%2C%20and%20hardware%20adaptability%20for%20edge%20deployment%20remains%0Alacking.%20This%20survey%20bridges%20this%20gap%20by%20providing%20a%20structured%20analysis%20of%0Amodel%20compression%20techniques%2C%20software%20tools%20for%20inference%20on%20edge%2C%20and%0Ahardware%20acceleration%20strategies%20for%20ViTs.%20We%20discuss%20their%20impact%20on%20accuracy%2C%0Aefficiency%2C%20and%20hardware%20adaptability%2C%20highlighting%20key%20challenges%20and%20emerging%0Aresearch%20directions%20to%20advance%20ViT%20deployment%20on%20edge%20platforms%2C%20including%0Agraphics%20processing%20units%20%28GPUs%29%2C%20application-specific%20integrated%20circuit%0A%28ASICs%29%2C%20and%20field-programmable%20gate%20arrays%20%28FPGAs%29.%20The%20goal%20is%20to%20inspire%0Afurther%20research%20with%20a%20contemporary%20guide%20on%20optimizing%20ViTs%20for%20efficient%0Adeployment%20on%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformers%2520on%2520the%2520Edge%253A%2520A%2520Comprehensive%2520Survey%2520of%2520Model%250A%2520%2520Compression%2520and%2520Acceleration%2520Strategies%26entry.906535625%3DShaibal%2520Saha%2520and%2520Lanyu%2520Xu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520vision%2520transformers%2520%2528ViTs%2529%2520have%2520emerged%2520as%2520powerful%2520and%250Apromising%2520techniques%2520for%2520computer%2520vision%2520tasks%2520such%2520as%2520image%2520classification%252C%250Aobject%2520detection%252C%2520and%2520segmentation.%2520Unlike%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%252C%2520which%2520rely%2520on%2520hierarchical%2520feature%2520extraction%252C%2520ViTs%2520treat%2520images%2520as%250Asequences%2520of%2520patches%2520and%2520leverage%2520self-attention%2520mechanisms.%2520However%252C%2520their%250Ahigh%2520computational%2520complexity%2520and%2520memory%2520demands%2520pose%2520significant%2520challenges%250Afor%2520deployment%2520on%2520resource-constrained%2520edge%2520devices.%2520To%2520address%2520these%250Alimitations%252C%2520extensive%2520research%2520has%2520focused%2520on%2520model%2520compression%2520techniques%2520and%250Ahardware-aware%2520acceleration%2520strategies.%2520Nonetheless%252C%2520a%2520comprehensive%2520review%250Athat%2520systematically%2520categorizes%2520these%2520techniques%2520and%2520their%2520trade-offs%2520in%250Aaccuracy%252C%2520efficiency%252C%2520and%2520hardware%2520adaptability%2520for%2520edge%2520deployment%2520remains%250Alacking.%2520This%2520survey%2520bridges%2520this%2520gap%2520by%2520providing%2520a%2520structured%2520analysis%2520of%250Amodel%2520compression%2520techniques%252C%2520software%2520tools%2520for%2520inference%2520on%2520edge%252C%2520and%250Ahardware%2520acceleration%2520strategies%2520for%2520ViTs.%2520We%2520discuss%2520their%2520impact%2520on%2520accuracy%252C%250Aefficiency%252C%2520and%2520hardware%2520adaptability%252C%2520highlighting%2520key%2520challenges%2520and%2520emerging%250Aresearch%2520directions%2520to%2520advance%2520ViT%2520deployment%2520on%2520edge%2520platforms%252C%2520including%250Agraphics%2520processing%2520units%2520%2528GPUs%2529%252C%2520application-specific%2520integrated%2520circuit%250A%2528ASICs%2529%252C%2520and%2520field-programmable%2520gate%2520arrays%2520%2528FPGAs%2529.%2520The%2520goal%2520is%2520to%2520inspire%250Afurther%2520research%2520with%2520a%2520contemporary%2520guide%2520on%2520optimizing%2520ViTs%2520for%2520efficient%250Adeployment%2520on%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformers%20on%20the%20Edge%3A%20A%20Comprehensive%20Survey%20of%20Model%0A%20%20Compression%20and%20Acceleration%20Strategies&entry.906535625=Shaibal%20Saha%20and%20Lanyu%20Xu&entry.1292438233=%20%20In%20recent%20years%2C%20vision%20transformers%20%28ViTs%29%20have%20emerged%20as%20powerful%20and%0Apromising%20techniques%20for%20computer%20vision%20tasks%20such%20as%20image%20classification%2C%0Aobject%20detection%2C%20and%20segmentation.%20Unlike%20convolutional%20neural%20networks%0A%28CNNs%29%2C%20which%20rely%20on%20hierarchical%20feature%20extraction%2C%20ViTs%20treat%20images%20as%0Asequences%20of%20patches%20and%20leverage%20self-attention%20mechanisms.%20However%2C%20their%0Ahigh%20computational%20complexity%20and%20memory%20demands%20pose%20significant%20challenges%0Afor%20deployment%20on%20resource-constrained%20edge%20devices.%20To%20address%20these%0Alimitations%2C%20extensive%20research%20has%20focused%20on%20model%20compression%20techniques%20and%0Ahardware-aware%20acceleration%20strategies.%20Nonetheless%2C%20a%20comprehensive%20review%0Athat%20systematically%20categorizes%20these%20techniques%20and%20their%20trade-offs%20in%0Aaccuracy%2C%20efficiency%2C%20and%20hardware%20adaptability%20for%20edge%20deployment%20remains%0Alacking.%20This%20survey%20bridges%20this%20gap%20by%20providing%20a%20structured%20analysis%20of%0Amodel%20compression%20techniques%2C%20software%20tools%20for%20inference%20on%20edge%2C%20and%0Ahardware%20acceleration%20strategies%20for%20ViTs.%20We%20discuss%20their%20impact%20on%20accuracy%2C%0Aefficiency%2C%20and%20hardware%20adaptability%2C%20highlighting%20key%20challenges%20and%20emerging%0Aresearch%20directions%20to%20advance%20ViT%20deployment%20on%20edge%20platforms%2C%20including%0Agraphics%20processing%20units%20%28GPUs%29%2C%20application-specific%20integrated%20circuit%0A%28ASICs%29%2C%20and%20field-programmable%20gate%20arrays%20%28FPGAs%29.%20The%20goal%20is%20to%20inspire%0Afurther%20research%20with%20a%20contemporary%20guide%20on%20optimizing%20ViTs%20for%20efficient%0Adeployment%20on%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02891v2&entry.124074799=Read"},
{"title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From\n  Vulnerability to Defense", "author": "Yuchen Ding and Hongli Peng and Xiaoqi Li", "abstract": "  With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures.\n", "link": "http://arxiv.org/abs/2504.21480v1", "date": "2025-04-30", "relevancy": 0.924, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Study%20of%20Exploitable%20Patterns%20in%20Smart%20Contracts%3A%20From%0A%20%20Vulnerability%20to%20Defense&body=Title%3A%20A%20Comprehensive%20Study%20of%20Exploitable%20Patterns%20in%20Smart%20Contracts%3A%20From%0A%20%20Vulnerability%20to%20Defense%0AAuthor%3A%20Yuchen%20Ding%20and%20Hongli%20Peng%20and%20Xiaoqi%20Li%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20blockchain%20technology%2C%20smart%20contracts%20have%0Aenabled%20the%20implementation%20of%20increasingly%20complex%20functionalities.%20However%2C%0Aensuring%20the%20security%20of%20smart%20contracts%20remains%20a%20persistent%20challenge%20across%0Athe%20stages%20of%20development%2C%20compilation%2C%20and%20execution.%20Vulnerabilities%20within%0Asmart%20contracts%20not%20only%20undermine%20the%20security%20of%20individual%20applications%20but%0Aalso%20pose%20significant%20risks%20to%20the%20broader%20blockchain%20ecosystem%2C%20as%0Ademonstrated%20by%20the%20growing%20frequency%20of%20attacks%20since%202016%2C%20resulting%20in%0Asubstantial%20financial%20losses.%20This%20paper%20provides%20a%20comprehensive%20analysis%20of%0Akey%20security%20risks%20in%20Ethereum%20smart%20contracts%2C%20specifically%20those%20written%20in%0ASolidity%20and%20executed%20on%20the%20Ethereum%20Virtual%20Machine%20%28EVM%29.%20We%20focus%20on%20two%0Aprevalent%20and%20critical%20vulnerability%20types%20%28reentrancy%20and%20integer%20overflow%29%20by%0Aexamining%20their%20underlying%20mechanisms%2C%20replicating%20attack%20scenarios%2C%20and%0Aassessing%20effective%20countermeasures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Study%2520of%2520Exploitable%2520Patterns%2520in%2520Smart%2520Contracts%253A%2520From%250A%2520%2520Vulnerability%2520to%2520Defense%26entry.906535625%3DYuchen%2520Ding%2520and%2520Hongli%2520Peng%2520and%2520Xiaoqi%2520Li%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520blockchain%2520technology%252C%2520smart%2520contracts%2520have%250Aenabled%2520the%2520implementation%2520of%2520increasingly%2520complex%2520functionalities.%2520However%252C%250Aensuring%2520the%2520security%2520of%2520smart%2520contracts%2520remains%2520a%2520persistent%2520challenge%2520across%250Athe%2520stages%2520of%2520development%252C%2520compilation%252C%2520and%2520execution.%2520Vulnerabilities%2520within%250Asmart%2520contracts%2520not%2520only%2520undermine%2520the%2520security%2520of%2520individual%2520applications%2520but%250Aalso%2520pose%2520significant%2520risks%2520to%2520the%2520broader%2520blockchain%2520ecosystem%252C%2520as%250Ademonstrated%2520by%2520the%2520growing%2520frequency%2520of%2520attacks%2520since%25202016%252C%2520resulting%2520in%250Asubstantial%2520financial%2520losses.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520analysis%2520of%250Akey%2520security%2520risks%2520in%2520Ethereum%2520smart%2520contracts%252C%2520specifically%2520those%2520written%2520in%250ASolidity%2520and%2520executed%2520on%2520the%2520Ethereum%2520Virtual%2520Machine%2520%2528EVM%2529.%2520We%2520focus%2520on%2520two%250Aprevalent%2520and%2520critical%2520vulnerability%2520types%2520%2528reentrancy%2520and%2520integer%2520overflow%2529%2520by%250Aexamining%2520their%2520underlying%2520mechanisms%252C%2520replicating%2520attack%2520scenarios%252C%2520and%250Aassessing%2520effective%2520countermeasures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Study%20of%20Exploitable%20Patterns%20in%20Smart%20Contracts%3A%20From%0A%20%20Vulnerability%20to%20Defense&entry.906535625=Yuchen%20Ding%20and%20Hongli%20Peng%20and%20Xiaoqi%20Li&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20blockchain%20technology%2C%20smart%20contracts%20have%0Aenabled%20the%20implementation%20of%20increasingly%20complex%20functionalities.%20However%2C%0Aensuring%20the%20security%20of%20smart%20contracts%20remains%20a%20persistent%20challenge%20across%0Athe%20stages%20of%20development%2C%20compilation%2C%20and%20execution.%20Vulnerabilities%20within%0Asmart%20contracts%20not%20only%20undermine%20the%20security%20of%20individual%20applications%20but%0Aalso%20pose%20significant%20risks%20to%20the%20broader%20blockchain%20ecosystem%2C%20as%0Ademonstrated%20by%20the%20growing%20frequency%20of%20attacks%20since%202016%2C%20resulting%20in%0Asubstantial%20financial%20losses.%20This%20paper%20provides%20a%20comprehensive%20analysis%20of%0Akey%20security%20risks%20in%20Ethereum%20smart%20contracts%2C%20specifically%20those%20written%20in%0ASolidity%20and%20executed%20on%20the%20Ethereum%20Virtual%20Machine%20%28EVM%29.%20We%20focus%20on%20two%0Aprevalent%20and%20critical%20vulnerability%20types%20%28reentrancy%20and%20integer%20overflow%29%20by%0Aexamining%20their%20underlying%20mechanisms%2C%20replicating%20attack%20scenarios%2C%20and%0Aassessing%20effective%20countermeasures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21480v1&entry.124074799=Read"},
{"title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and\n  Regulation Support", "author": "Justin B. Bullock and Janet V. T. Pauketat and Hsini Huang and Yi-Fan Wang and Jacy Reese Anthis", "abstract": "  Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape.\n", "link": "http://arxiv.org/abs/2504.21849v1", "date": "2025-04-30", "relevancy": 1.1833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.421}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Public%20Opinion%20and%20The%20Rise%20of%20Digital%20Minds%3A%20Perceived%20Risk%2C%20Trust%2C%20and%0A%20%20Regulation%20Support&body=Title%3A%20Public%20Opinion%20and%20The%20Rise%20of%20Digital%20Minds%3A%20Perceived%20Risk%2C%20Trust%2C%20and%0A%20%20Regulation%20Support%0AAuthor%3A%20Justin%20B.%20Bullock%20and%20Janet%20V.%20T.%20Pauketat%20and%20Hsini%20Huang%20and%20Yi-Fan%20Wang%20and%20Jacy%20Reese%20Anthis%0AAbstract%3A%20%20%20Governance%20institutions%20must%20respond%20to%20societal%20risks%2C%20including%20those%20posed%0Aby%20generative%20AI.%20This%20study%20empirically%20examines%20how%20public%20trust%20in%0Ainstitutions%20and%20AI%20technologies%2C%20along%20with%20perceived%20risks%2C%20shape%20preferences%0Afor%20AI%20regulation.%20Using%20the%20nationally%20representative%202023%20Artificial%0AIntelligence%2C%20Morality%2C%20and%20Sentience%20%28AIMS%29%20survey%2C%20we%20assess%20trust%20in%0Agovernment%2C%20AI%20companies%2C%20and%20AI%20technologies%2C%20as%20well%20as%20public%20support%20for%0Aregulatory%20measures%20such%20as%20slowing%20AI%20development%20or%20outright%20bans%20on%20advanced%0AAI.%20Our%20findings%20reveal%20broad%20public%20support%20for%20AI%20regulation%2C%20with%20risk%0Aperception%20playing%20a%20significant%20role%20in%20shaping%20policy%20preferences.%0AIndividuals%20with%20higher%20trust%20in%20government%20favor%20regulation%2C%20while%20those%20with%0Agreater%20trust%20in%20AI%20companies%20and%20AI%20technologies%20are%20less%20inclined%20to%20support%0Arestrictions.%20Trust%20in%20government%20and%20perceived%20risks%20significantly%20predict%0Apreferences%20for%20both%20soft%20%28e.g.%2C%20slowing%20development%29%20and%20strong%20%28e.g.%2C%20banning%0AAI%20systems%29%20regulatory%20interventions.%20These%20results%20highlight%20the%20importance%20of%0Apublic%20opinion%20in%20AI%20governance.%20As%20AI%20capabilities%20advance%2C%20effective%0Aregulation%20will%20require%20balancing%20public%20concerns%20about%20risks%20with%20trust%20in%0Ainstitutions.%20This%20study%20provides%20a%20foundational%20empirical%20baseline%20for%0Apolicymakers%20navigating%20AI%20governance%20and%20underscores%20the%20need%20for%20further%0Aresearch%20into%20public%20trust%2C%20risk%20perception%2C%20and%20regulatory%20strategies%20in%20the%0Aevolving%20AI%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPublic%2520Opinion%2520and%2520The%2520Rise%2520of%2520Digital%2520Minds%253A%2520Perceived%2520Risk%252C%2520Trust%252C%2520and%250A%2520%2520Regulation%2520Support%26entry.906535625%3DJustin%2520B.%2520Bullock%2520and%2520Janet%2520V.%2520T.%2520Pauketat%2520and%2520Hsini%2520Huang%2520and%2520Yi-Fan%2520Wang%2520and%2520Jacy%2520Reese%2520Anthis%26entry.1292438233%3D%2520%2520Governance%2520institutions%2520must%2520respond%2520to%2520societal%2520risks%252C%2520including%2520those%2520posed%250Aby%2520generative%2520AI.%2520This%2520study%2520empirically%2520examines%2520how%2520public%2520trust%2520in%250Ainstitutions%2520and%2520AI%2520technologies%252C%2520along%2520with%2520perceived%2520risks%252C%2520shape%2520preferences%250Afor%2520AI%2520regulation.%2520Using%2520the%2520nationally%2520representative%25202023%2520Artificial%250AIntelligence%252C%2520Morality%252C%2520and%2520Sentience%2520%2528AIMS%2529%2520survey%252C%2520we%2520assess%2520trust%2520in%250Agovernment%252C%2520AI%2520companies%252C%2520and%2520AI%2520technologies%252C%2520as%2520well%2520as%2520public%2520support%2520for%250Aregulatory%2520measures%2520such%2520as%2520slowing%2520AI%2520development%2520or%2520outright%2520bans%2520on%2520advanced%250AAI.%2520Our%2520findings%2520reveal%2520broad%2520public%2520support%2520for%2520AI%2520regulation%252C%2520with%2520risk%250Aperception%2520playing%2520a%2520significant%2520role%2520in%2520shaping%2520policy%2520preferences.%250AIndividuals%2520with%2520higher%2520trust%2520in%2520government%2520favor%2520regulation%252C%2520while%2520those%2520with%250Agreater%2520trust%2520in%2520AI%2520companies%2520and%2520AI%2520technologies%2520are%2520less%2520inclined%2520to%2520support%250Arestrictions.%2520Trust%2520in%2520government%2520and%2520perceived%2520risks%2520significantly%2520predict%250Apreferences%2520for%2520both%2520soft%2520%2528e.g.%252C%2520slowing%2520development%2529%2520and%2520strong%2520%2528e.g.%252C%2520banning%250AAI%2520systems%2529%2520regulatory%2520interventions.%2520These%2520results%2520highlight%2520the%2520importance%2520of%250Apublic%2520opinion%2520in%2520AI%2520governance.%2520As%2520AI%2520capabilities%2520advance%252C%2520effective%250Aregulation%2520will%2520require%2520balancing%2520public%2520concerns%2520about%2520risks%2520with%2520trust%2520in%250Ainstitutions.%2520This%2520study%2520provides%2520a%2520foundational%2520empirical%2520baseline%2520for%250Apolicymakers%2520navigating%2520AI%2520governance%2520and%2520underscores%2520the%2520need%2520for%2520further%250Aresearch%2520into%2520public%2520trust%252C%2520risk%2520perception%252C%2520and%2520regulatory%2520strategies%2520in%2520the%250Aevolving%2520AI%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Public%20Opinion%20and%20The%20Rise%20of%20Digital%20Minds%3A%20Perceived%20Risk%2C%20Trust%2C%20and%0A%20%20Regulation%20Support&entry.906535625=Justin%20B.%20Bullock%20and%20Janet%20V.%20T.%20Pauketat%20and%20Hsini%20Huang%20and%20Yi-Fan%20Wang%20and%20Jacy%20Reese%20Anthis&entry.1292438233=%20%20Governance%20institutions%20must%20respond%20to%20societal%20risks%2C%20including%20those%20posed%0Aby%20generative%20AI.%20This%20study%20empirically%20examines%20how%20public%20trust%20in%0Ainstitutions%20and%20AI%20technologies%2C%20along%20with%20perceived%20risks%2C%20shape%20preferences%0Afor%20AI%20regulation.%20Using%20the%20nationally%20representative%202023%20Artificial%0AIntelligence%2C%20Morality%2C%20and%20Sentience%20%28AIMS%29%20survey%2C%20we%20assess%20trust%20in%0Agovernment%2C%20AI%20companies%2C%20and%20AI%20technologies%2C%20as%20well%20as%20public%20support%20for%0Aregulatory%20measures%20such%20as%20slowing%20AI%20development%20or%20outright%20bans%20on%20advanced%0AAI.%20Our%20findings%20reveal%20broad%20public%20support%20for%20AI%20regulation%2C%20with%20risk%0Aperception%20playing%20a%20significant%20role%20in%20shaping%20policy%20preferences.%0AIndividuals%20with%20higher%20trust%20in%20government%20favor%20regulation%2C%20while%20those%20with%0Agreater%20trust%20in%20AI%20companies%20and%20AI%20technologies%20are%20less%20inclined%20to%20support%0Arestrictions.%20Trust%20in%20government%20and%20perceived%20risks%20significantly%20predict%0Apreferences%20for%20both%20soft%20%28e.g.%2C%20slowing%20development%29%20and%20strong%20%28e.g.%2C%20banning%0AAI%20systems%29%20regulatory%20interventions.%20These%20results%20highlight%20the%20importance%20of%0Apublic%20opinion%20in%20AI%20governance.%20As%20AI%20capabilities%20advance%2C%20effective%0Aregulation%20will%20require%20balancing%20public%20concerns%20about%20risks%20with%20trust%20in%0Ainstitutions.%20This%20study%20provides%20a%20foundational%20empirical%20baseline%20for%0Apolicymakers%20navigating%20AI%20governance%20and%20underscores%20the%20need%20for%20further%0Aresearch%20into%20public%20trust%2C%20risk%20perception%2C%20and%20regulatory%20strategies%20in%20the%0Aevolving%20AI%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21849v1&entry.124074799=Read"},
{"title": "One Net to Rule Them All: Domain Randomization in Quadcopter Racing\n  Across Different Platforms", "author": "Robin Ferede and Till Blaha and Erin Lucassen and Christophe De Wagter and Guido C. H. E. de Croon", "abstract": "  In high-speed quadcopter racing, finding a single controller that works well\nacross different platforms remains challenging. This work presents the first\nneural network controller for drone racing that generalizes across physically\ndistinct quadcopters. We demonstrate that a single network, trained with domain\nrandomization, can robustly control various types of quadcopters. The network\nrelies solely on the current state to directly compute motor commands. The\neffectiveness of this generalized controller is validated through real-world\ntests on two substantially different crafts (3-inch and 5-inch race\nquadcopters). We further compare the performance of this generalized controller\nwith controllers specifically trained for the 3-inch and 5-inch drone, using\ntheir identified model parameters with varying levels of domain randomization\n(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower\nspeeds compared to the fine-tuned models, it excels in adaptability across\ndifferent platforms. Our results show that no randomization fails sim-to-real\ntransfer while increasing randomization improves robustness but reduces speed.\nDespite this trade-off, our findings highlight the potential of domain\nrandomization for generalizing controllers, paving the way for universal AI\ncontrollers that can adapt to any platform.\n", "link": "http://arxiv.org/abs/2504.21586v1", "date": "2025-04-30", "relevancy": 1.502, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5029}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Net%20to%20Rule%20Them%20All%3A%20Domain%20Randomization%20in%20Quadcopter%20Racing%0A%20%20Across%20Different%20Platforms&body=Title%3A%20One%20Net%20to%20Rule%20Them%20All%3A%20Domain%20Randomization%20in%20Quadcopter%20Racing%0A%20%20Across%20Different%20Platforms%0AAuthor%3A%20Robin%20Ferede%20and%20Till%20Blaha%20and%20Erin%20Lucassen%20and%20Christophe%20De%20Wagter%20and%20Guido%20C.%20H.%20E.%20de%20Croon%0AAbstract%3A%20%20%20In%20high-speed%20quadcopter%20racing%2C%20finding%20a%20single%20controller%20that%20works%20well%0Aacross%20different%20platforms%20remains%20challenging.%20This%20work%20presents%20the%20first%0Aneural%20network%20controller%20for%20drone%20racing%20that%20generalizes%20across%20physically%0Adistinct%20quadcopters.%20We%20demonstrate%20that%20a%20single%20network%2C%20trained%20with%20domain%0Arandomization%2C%20can%20robustly%20control%20various%20types%20of%20quadcopters.%20The%20network%0Arelies%20solely%20on%20the%20current%20state%20to%20directly%20compute%20motor%20commands.%20The%0Aeffectiveness%20of%20this%20generalized%20controller%20is%20validated%20through%20real-world%0Atests%20on%20two%20substantially%20different%20crafts%20%283-inch%20and%205-inch%20race%0Aquadcopters%29.%20We%20further%20compare%20the%20performance%20of%20this%20generalized%20controller%0Awith%20controllers%20specifically%20trained%20for%20the%203-inch%20and%205-inch%20drone%2C%20using%0Atheir%20identified%20model%20parameters%20with%20varying%20levels%20of%20domain%20randomization%0A%280%25%2C%2010%25%2C%2020%25%2C%2030%25%29.%20While%20the%20generalized%20controller%20shows%20slightly%20slower%0Aspeeds%20compared%20to%20the%20fine-tuned%20models%2C%20it%20excels%20in%20adaptability%20across%0Adifferent%20platforms.%20Our%20results%20show%20that%20no%20randomization%20fails%20sim-to-real%0Atransfer%20while%20increasing%20randomization%20improves%20robustness%20but%20reduces%20speed.%0ADespite%20this%20trade-off%2C%20our%20findings%20highlight%20the%20potential%20of%20domain%0Arandomization%20for%20generalizing%20controllers%2C%20paving%20the%20way%20for%20universal%20AI%0Acontrollers%20that%20can%20adapt%20to%20any%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Net%2520to%2520Rule%2520Them%2520All%253A%2520Domain%2520Randomization%2520in%2520Quadcopter%2520Racing%250A%2520%2520Across%2520Different%2520Platforms%26entry.906535625%3DRobin%2520Ferede%2520and%2520Till%2520Blaha%2520and%2520Erin%2520Lucassen%2520and%2520Christophe%2520De%2520Wagter%2520and%2520Guido%2520C.%2520H.%2520E.%2520de%2520Croon%26entry.1292438233%3D%2520%2520In%2520high-speed%2520quadcopter%2520racing%252C%2520finding%2520a%2520single%2520controller%2520that%2520works%2520well%250Aacross%2520different%2520platforms%2520remains%2520challenging.%2520This%2520work%2520presents%2520the%2520first%250Aneural%2520network%2520controller%2520for%2520drone%2520racing%2520that%2520generalizes%2520across%2520physically%250Adistinct%2520quadcopters.%2520We%2520demonstrate%2520that%2520a%2520single%2520network%252C%2520trained%2520with%2520domain%250Arandomization%252C%2520can%2520robustly%2520control%2520various%2520types%2520of%2520quadcopters.%2520The%2520network%250Arelies%2520solely%2520on%2520the%2520current%2520state%2520to%2520directly%2520compute%2520motor%2520commands.%2520The%250Aeffectiveness%2520of%2520this%2520generalized%2520controller%2520is%2520validated%2520through%2520real-world%250Atests%2520on%2520two%2520substantially%2520different%2520crafts%2520%25283-inch%2520and%25205-inch%2520race%250Aquadcopters%2529.%2520We%2520further%2520compare%2520the%2520performance%2520of%2520this%2520generalized%2520controller%250Awith%2520controllers%2520specifically%2520trained%2520for%2520the%25203-inch%2520and%25205-inch%2520drone%252C%2520using%250Atheir%2520identified%2520model%2520parameters%2520with%2520varying%2520levels%2520of%2520domain%2520randomization%250A%25280%2525%252C%252010%2525%252C%252020%2525%252C%252030%2525%2529.%2520While%2520the%2520generalized%2520controller%2520shows%2520slightly%2520slower%250Aspeeds%2520compared%2520to%2520the%2520fine-tuned%2520models%252C%2520it%2520excels%2520in%2520adaptability%2520across%250Adifferent%2520platforms.%2520Our%2520results%2520show%2520that%2520no%2520randomization%2520fails%2520sim-to-real%250Atransfer%2520while%2520increasing%2520randomization%2520improves%2520robustness%2520but%2520reduces%2520speed.%250ADespite%2520this%2520trade-off%252C%2520our%2520findings%2520highlight%2520the%2520potential%2520of%2520domain%250Arandomization%2520for%2520generalizing%2520controllers%252C%2520paving%2520the%2520way%2520for%2520universal%2520AI%250Acontrollers%2520that%2520can%2520adapt%2520to%2520any%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Net%20to%20Rule%20Them%20All%3A%20Domain%20Randomization%20in%20Quadcopter%20Racing%0A%20%20Across%20Different%20Platforms&entry.906535625=Robin%20Ferede%20and%20Till%20Blaha%20and%20Erin%20Lucassen%20and%20Christophe%20De%20Wagter%20and%20Guido%20C.%20H.%20E.%20de%20Croon&entry.1292438233=%20%20In%20high-speed%20quadcopter%20racing%2C%20finding%20a%20single%20controller%20that%20works%20well%0Aacross%20different%20platforms%20remains%20challenging.%20This%20work%20presents%20the%20first%0Aneural%20network%20controller%20for%20drone%20racing%20that%20generalizes%20across%20physically%0Adistinct%20quadcopters.%20We%20demonstrate%20that%20a%20single%20network%2C%20trained%20with%20domain%0Arandomization%2C%20can%20robustly%20control%20various%20types%20of%20quadcopters.%20The%20network%0Arelies%20solely%20on%20the%20current%20state%20to%20directly%20compute%20motor%20commands.%20The%0Aeffectiveness%20of%20this%20generalized%20controller%20is%20validated%20through%20real-world%0Atests%20on%20two%20substantially%20different%20crafts%20%283-inch%20and%205-inch%20race%0Aquadcopters%29.%20We%20further%20compare%20the%20performance%20of%20this%20generalized%20controller%0Awith%20controllers%20specifically%20trained%20for%20the%203-inch%20and%205-inch%20drone%2C%20using%0Atheir%20identified%20model%20parameters%20with%20varying%20levels%20of%20domain%20randomization%0A%280%25%2C%2010%25%2C%2020%25%2C%2030%25%29.%20While%20the%20generalized%20controller%20shows%20slightly%20slower%0Aspeeds%20compared%20to%20the%20fine-tuned%20models%2C%20it%20excels%20in%20adaptability%20across%0Adifferent%20platforms.%20Our%20results%20show%20that%20no%20randomization%20fails%20sim-to-real%0Atransfer%20while%20increasing%20randomization%20improves%20robustness%20but%20reduces%20speed.%0ADespite%20this%20trade-off%2C%20our%20findings%20highlight%20the%20potential%20of%20domain%0Arandomization%20for%20generalizing%20controllers%2C%20paving%20the%20way%20for%20universal%20AI%0Acontrollers%20that%20can%20adapt%20to%20any%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21586v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


